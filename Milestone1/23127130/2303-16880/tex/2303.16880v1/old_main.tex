\documentclass[%
 article,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 %aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}
\usepackage{comment}

\DeclareMathOperator{\sgn}{sgn}

 
\begin{document}

\title{The Hidden-Manifold Hopfield Model shows storage-to-learning phase transitions and is able to generalize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{M. Negri}
 \email[Corresponding author; ]{matteo.negri@uniroma1.it}
 \affiliation{University of Rome ‘La Sapienza’, Department of Physics,
Piazzale Aldo Moro 5, 00185 Roma, Italy}
 \altaffiliation[Also at ]{CNR-NANOTEC, Institute of Nanotechnology, Rome Unit, Piazzale Aldo Moro, 00185 Roma, Italy}
 
\author{C. Lauditi}
 \affiliation{Artificial Intelligence Lab, Bocconi University, 20136 Milano, Italy}
 \altaffiliation[Also at ]{Department of Applied Science and Technology, Politecnico di Torino, 10129 Torino, Italy}
 
\author{G. Perugini}
 \affiliation{Artificial Intelligence Lab, Bocconi University, 20136 Milano, Italy}
 
\author{C. Lucibello}
 \affiliation{Artificial Intelligence Lab, Bocconi University, 20136 Milano, Italy}
 
\author{E. Malatesta}
 \affiliation{Artificial Intelligence Lab, Bocconi University, 20136 Milano, Italy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\date{\today}

\begin{abstract}

\begin{center}
    INTERNAL PRELIMINARY DRAFT
\end{center}

The Hopfield model has a long-standing tradition in statistical physics, being one of the few neural networks for which a theory is available. Generalized versions of the Hopfield model recently raised renewed interest because they became the building blocks of the state-of-the-art deep neural networks called \emph{transformers}. Extending the theory of Hopfield models could help understand the success of transformers, for instance describing how they learn features.
Motivated by this, we propose and investigate a generalized Hopfield model that we name \emph{Hidden-Manifold Hopfield Model}: we generate the couplings from $P=\alpha N$ examples with the Hebb rule using non-linear superpositions of $D=\alpha_D N$ random features, where N is the size of the number of neurons. Using the replica method, we obtain the phase diagram of the model, supported by numerical simulations. We find a phase transition where the features hidden in the examples become attractors of the dynamics; this phase exists above a critical value of $\alpha$ and below a critical value of $\alpha_D$ . We call this the \emph{storage-to-learning transition}.
We also find that, if $\alpha$ is high enough, the examples themselves become attractors even if the model is above the critical capacity $\alpha_c \simeq  0.138$. Surprisingly, in this regime we find that also new examples are attractors, making the model able to generalize and the storage capacity effectively infinite.

\end{abstract}

\maketitle

\section{Introduction}

Deep neural networks have seen a huge success in the last fifteen years, performing tasks that range from classification of large dataset of images to the recent spectacular results in text-to-image translation \cite{ramesh2022hierarchical,rombach2022high}. One of the key ingredients of these state-of-the-art deep neural networks is the so-called attentions mechanism \cite{vaswani2017attention}, which has been linked \cite{ramsauer2020hopfield,yang2022transformers} to the long-studied Hopfield model and its dense generalizations \cite{krotov2016dense,krotov2018dense}. To this date, the Hopfield model is one of the few analytically solvable models for neural networks, and the fact that it became relevant for deep neural networks is a promising chance to understand the success of deep learning, for which a comprehensive theory is still not available.

The Hopfield model \cite{hopfield1982neural} is a recurrent artificial neural network that works as an associative memory: given a set of patterns, the model is able to memorize them and retrieve them if the variables are initialized close enough to one of the memories. The model has been solved analytically in the framework of statistical mechanics when the patterns are uncorrelated and their number is proportional to the number of neurons \cite{amit1987statistical}. Nevertheless, this model has limited practical use: is critical capacity is relatively low and it further decreases as soon as correlations are introduces in the patterns. 

Generalizations of Hopfield model to the case of correlated patterns have already been considered immediately after the solution of the uncorrelated case \cite{amit1987statistical}. Most of these studies considered modifications of the Hebb rule that would allow the storage of the highest possible number of correlated patterns. For example, in \cite{gutfreund1988neural} the authors study a biased distribution of binary patterns, that can even be generalized to a hierarchical structure of correlation as it was discussed in \cite{cortes1987hierarchical, krogh1988mean}. Another approach is to consider correlations in the form of Markov chains \cite{lowe1998storage}, with can used to produce a correlation length both between different spins of a given pattern and between the same spin of different patterns.

In this work, we study the case of non-linear superpositions of random binary patterns (see eq.~\ref{eq:correlated_examples}). We generalize the structure of linear superposition suggested in \cite{mezard2017mean}, where it was discusses in relation to the mapping between an Hopfield network and a restricted Boltzmann machine. This mapping in the case of correlated data has  been discussed also in \cite{agliari2013parallel, smart2021mapping}.

We study this particular structure as it has been argued in \cite{goldt2020modeling} that it models the correlations that we expect in the real-world datasets used for deep learning. In fact, the superposition of a small number of random features can be interpreted as mapping data from low to high dimension with a random matrix and possibly a non linear function afterwards. This is the so-called \emph{hidden-manifold} structure, that models the fact that we observe real data in a much higher dimension (think for example to standard image datasets, where we expect the number of relevant features is much smaller than the number of pixels, which is arbitrarily large). Additionally, being a composition of features is a common characteristic of real-world data that uncorrelated patterns or previous structures do not capture. 

Here we do not modify the Hebb rule, as we will see that it is enough to produce a new behaviour of the model, in conjunction with the structure of correlation that we chose. In particular, we observe that if the correlation in the data are strong enough the model switches from a storage phase to a learning phase, in the sense that attractors appear corresponding to the features in the data. We argue that this behaviour opens up a new paradigm for this model and shows that it may have some phenomenology in common with neural networks.

This paper is organized as follows: in section~\ref{sec:definition} we define the model and we give an intuitive explanation for the appearance of attractors correlated with the features of the data. In section~\ref{sec:replica_calc} we sketch the replica calculations for the storage and learning critical lines at zero temperature. In section~\ref{sec:discussion} we comment the phase diagram and we discuss why our results are relevant for the feature extraction task and, more generally, for the matrix factorization problem.

\section{The Hidden-Manifold Hopfield Model}
\label{sec:definition}
\subsection{Definition}

The Hopfield model consist in $N$ binary spins $s_i=\pm 1$, $i=1,...,N$, interacting in a fully-connected fashion with a two-body Hamiltonian 
\begin{equation}
H=-\frac{1}{2}\sum_{i\neq j}J_{ij}s_{i}s_{j},
\end{equation}
where the coupling constants are defined from a set of $P$ \emph{example patterns} $\{\vec\xi_{\mu} \}_{\mu=1}^P$ via the relation:
\begin{equation}
J_{ij}=\frac{1}{N}\sum_{\mu=1}^{P}\xi_{\mu i}\xi_{\mu j}.
\label{eq:hebb_rule}
\end{equation}
Each example pattern is a is a spin configuration, $\xi_{\mu i}=\pm 1$.
The case where the examples are uncorrelated has been extensively studied and has been already solved in \cite{amit1987statistical}. Instead, in our Hidden-Manifold Hopfield Model (HMHM) we choose the examples with the following structure:

\begin{equation}
\xi_{\mu i}=\sigma\left(\frac{1}{\sqrt{D}}\sum_{k=1}^{D}c_{\mu k}f_{ki}\right)
\label{eq:correlated_examples}
\end{equation}
where $\sigma(...)$ is a generic non-linear function, $f_{ki}$ is
the matrix of random features and $c_{\mu k}$ is the matrix of coefficients. A version of this model without the non-linearity has been proposed in \cite{mezard2017mean}.

TODO: say that the distributions of those does not matter much for
as long as they satisfy the hypotheses of Gaussian equivalence. To fix the ideas we can consider binary factors, Gaussian coefficients and a sign function, so that we end up with examples that are (non-linear) superpositions of the features that are the columns of $f$. By tuning $D$ we can switch between weakly and strongly correlated examples. In fact, in the $\alpha_D\to\infty$ we expect to recover the standard Hopfield model.  

In this work, the numerical results and most of the analytical ones are obtained in the limit $T\to0$. In this limit, the update rule of each spin at time $t$ reads 
\begin{equation}
    s_i^{(t+1)} = \sgn \left( \sum_{j=1}^N J_{ij} s_j^{(t)}\right)
\end{equation}
If a spin configuration $\tilde s_i$ satisfies the relation $\tilde s_i = \sgn ( \sum_{j=1}^N J_{ij} \tilde s_i )$, then we say that $\tilde s_i$ is a fixed point of the dynamics. If the dynamics converges to $\tilde s_i$ even when a fraction of spins has been flipped, then $\tilde s_i$ is an attractor. The original task of the Hopfield model is to store $P$ examples as attractors. This can also be seen as a denoising operation, since the model is capable of retrieving the stored patterns starting from noisy versions of them. In \cite{amit1987statistical} the authors computed the maximum number of retrievable patterns in the near-saturation regime, namely when the number of examples is proportional to the number of neurons $P=\alpha N$, finding $\alpha_c\simeq 0.138$. For $\alpha>\alpha_c$ the model shows a first-order phase transition referred as \emph{catastrophic forgetting} and no storage is possible.

In our HMHM, the basic question that we are interested in is whether the factors $\vec f_k$ can be an attractors themselves, and what happens to the attractors corresponding to the examples. In the next paragraph we give an intuition on why we expect to be possible that the factors $\vec f_k$ can be an attractors, at least in one specific limit.

\subsection{Limit $\alpha\to\infty$}
%TODO: here we might write a couple of intuitive arguments to justify the idea and the question. Mine and/or Carlo's

Here we show that, in the limit of infinite load $\alpha$, the coupling matrix assumes a form similar to eq.~\ref{eq:hebb_rule} with the factors $\mathbf{f}_k$ in place of the examples $\mathbf{\xi}_\mu$, indicating that in this limit the factors can actually be attractors of the dynamics. 

For large number of examples $P$ and a rotationally invariant distribution $P(\mathbf{c})$, the HMHM couplings become

\begin{eqnarray}
J_{ij} & \approx & \int dP(\mathbf{\mathbf{c}})\ \sigma\left(\text{\ensuremath{\frac{1}{\sqrt{D}}}\ensuremath{\mathbf{c}\cdot\mathbf{f}_{i}}}\right)\sigma\left(\text{\ensuremath{\frac{1}{\sqrt{D}}}\ensuremath{\mathbf{c}\cdot\mathbf{f}_{j}}}\right) \label{eq:int_couplings}\\
 & = & r\left(\frac{1}{D}\mathbf{f}_{i}\cdot\mathbf{f}_{j}\right).
\end{eqnarray}
where in the last line we used rotational invariance to express the coupling as a function of the scalar product among the two couplings. The function $r(z)$ depends on the ensemble considered and on the activation function. Notice that if $r(z)\approx az$ for small argument we recover the standard Hopfield model, up to a prefactor that can be reabsorbed in the temperature. 

We show that we obtain the standard Hopfield model as large $P$ limit in the
case of Gaussian $\mathbf{c}$ and antisymmetric and non-decreasing
activation functions. In fact, by averaging over $P(\mathbf{c})$ in eq.~\ref{eq:int_couplings}, we have

\begin{eqnarray}
J_{ij} & = & \int\frac{dud\hat{u}}{(2\pi)^{2}}\ \sigma\left(u\right)\sigma\left(v\right)e^{-i\hat{u}u-i\hat{v}v-\frac{1}{2D}\hat{u}^{2}\lVert\mathbf{f}_{i}\rVert^{2}-\frac{1}{2D}\hat{v}^{2}\lVert\mathbf{f}_{j}\rVert^{2}-\frac{1}{D}\hat{u}\hat{v}\mathbf{f}_{i}\cdot\mathbf{f}_{j}}
\end{eqnarray}
Considering independently distributed factor vectors we assume $\lVert\mathbf{f}_{i}\rVert^{2}=D$,
$\lVert\mathbf{f}_{j}\rVert^{2}=D$, $\mathbf{f}_{i}\cdot\mathbf{f}_{j}=O(\sqrt{D})$,
therefore we can expand to the first order in the small interaction
term and obtain

\begin{equation}
J_{ij}\approx\kappa^{2}\frac{1}{D}\mathbf{f}_{i}\cdot\mathbf{f}_{j}=\kappa^{2}\frac{1}{D}\sum_{k=1}^{D}f_{ki}f_{kj}
\end{equation}
where $\kappa=\int Dz\ \sigma'(z)$. The matrix $J$ has therefore an Hopfield structure with $D$ stored patterns $\mathbf{f}_k$. 


\section{Storage-to-learning phase transitions}
\label{sec:replica_calc}

\subsection{Setup of the replica calculations}  

Since we are interested in the thermodynamic limit $N\to\infty$, we choose a regime where both $P$ and $D$ are proportional to $N$. At the same time we keep the following ratios fixed
\begin{equation}
    \alpha=\frac{P}{N},\quad \alpha_D=\frac{D}{N},
\end{equation}
which will be the control parameter of our model. They are related via the relation $\alpha=\alpha_T \alpha_D$, where $\alpha_T=P/D$.

In order to identify the phase transitions of the HMHM we want to compute the averaged free energy
\begin{equation}
    \phi = \lim_{N\to\infty} -\frac{1}{\beta N} \langle \log Z \rangle_{c,f}
    \label{eq:phi}
\end{equation}
where we specified that we have two sources of disorder that must be averaged: the coefficients $c$ and the factors $f$. $Z$ is the partition function, that reads

\begin{equation}
Z=\sum_{\left\{ s_{i}\right\} }\exp\left\{ \frac{\beta}{2N}\sum_{\mu=1}^{P}\left(\sum_{i=1}^{N}\sigma\left(\frac{1}{\sqrt{D}}\sum_{k=1}^{D}c_{\mu k}f_{ki}\right)s_{i}\right)^{2}-\frac{\beta}{2}P\right\} 
\end{equation}
where the sum is taken over the possible values of the spins $s_{i}=\pm1$
for $i=1,...,N$.

In order to compute the average of $\log Z$ in \ref{eq:phi} we use the replica method \cite{mezard1987spin}, that consist in writing the average of logarithm as  $\langle \log Z \rangle = \lim_{n\to0} (\langle Z^n \rangle-1)/n$, since computing $\langle Z^n \rangle$ is doable.

The replicated partition function averaged over the disordered reads

\begin{widetext}
\begin{align}
\langle  Z^{n}\rangle =e^{-\frac{\beta}{2}Pn}\sum_{\left\{ s_{i}^{a}\right\} }\int\prod_{\mu a}\frac{dm_{\mu}^{a}}{\sqrt{2\pi}}\exp\left\{ \frac{\beta}{2}\sum_{\mu=1}^{P}\sum_{a=1}^{n}\left(m_{\mu}^{a}\right)^{2}\right\} \left\langle \prod_{\mu a}\delta\left(m_{\mu}^{a}-\frac{1}{\sqrt{N}}\sum_{i=1}^{N}\sigma\left(\frac{1}{\sqrt{D}}\sum_{k=1}^{D}c_{\mu k}f_{ki}\right)s_{i}^{a}\right)\right\rangle _{c_{\mu k},f_{ki}}\label{eq:GET_Z}
\end{align}
\end{widetext}
where $a=1,...,n$ is the replica index and where we introduced the set of auxiliary variables 
\begin{equation}
    m_{\mu}=\frac{1}{\sqrt{N}}\sum_{i}\xi_{\mu i}s_{i}=\frac{1}{\sqrt{N}}\sum_{i}\sigma\left(\frac{1}{\sqrt{D}}\sum_{k=1}^{D}c_{\mu k}f_{ki}\right)s_{i}
\end{equation}
which are magnetizations of the system with the example patterns and we will find that they are some of our order parameters. We call these \emph{pattern magnetizations} to distinguish them from another set of order parameters, whose definition we anticipate here:
\begin{equation}
    \mu_{k}=\frac{1}{\sqrt{N}}\sum_{i}f_{ki}s_{i}\label{eq:def_muab}
\end{equation}
We call these the \emph{factor magnetizations}. We want to see if there is a region of the  $\alpha_D$ vs $\alpha$ phase diagram where $\mu_k>0$ for some $k$. We also want to see what happens to the pattern magnetizations in the same phase diagram.

Following \cite{amit1987statistical}, we need to make some ansatz on the structure of the solution for both these order parameters. We study analytically two cases, respectively in section~\ref{sec:factor_retrieval} and section~\ref{sec:pattern_retrieval}: the case where the model retrieves only one of the factors, and the case where the model retrieves only one of the examples.
The case where $m$ and $\mu$ are different from zero at the same time appears to be more complicated and we are only able to study it in a numerical way at the present time. See section~\ref{sec:generalization} for such study and section~\ref{sec:discussion} for a discussion on this point.

\subsection{Retrieval of the factors}
\label{sec:factor_retrieval}

\subsubsection{Gaussian equivalence}

For this first case we say that $\mu_1=O(1)$ and $\mu_k=O(1/\sqrt{N})$ for $k>1$. At the same time we say that $m_\nu=O(1/\sqrt{N})\quad\forall \nu$. In the thermodynamic limit this means that we look for a solution of the form 
\begin{equation}
    \vec\mu = (\mu,0,...,0) \quad  \vec m = (0,...,0)
    \label{eq:factor_retrieval_state}
\end{equation}

This assumption allows us to use the Gaussian Equivalence Theorem (GET) \cite{mei2022generalization, gerace2020generalisation, goldt2022gaussian, goldt2020modeling, hu2022universality} (TODO: write somewhere the hypotheses) to compute the averages over the components of the matrix $c_{\mu k}$.
The GET states that the distribution resulting from the average of the delta
function over $c_{\mu k}$ in eq.~\ref{eq:GET_Z} is a Gaussian with zero mean and covariance matrix $\Sigma=\Sigma(\{s_{i}^{a}\},\{f_{ki}\})$: 
\begin{equation}
\begin{split}
    \left\langle \left\langle Z^{n}\right\rangle \right\rangle &=e^{-\frac{\beta}{2}Pn}\sum_{\left\{ s_{i}^{a}\right\} }\int\prod_{\mu a}\frac{dm_{\mu}^{a}}{\sqrt{2\pi}} %\cdot \\
    %&\cdot 
    \exp\left\{ +\frac{\beta}{2}\sum_{\mu=1}^{P}\sum_{a=1}^{n}\left(m_{\mu}^{a}\right)^{2}\right\} \left\langle \mathcal{N}(m_{\mu}^{a};0,\Sigma)\right\rangle _{f_{ki}}
\end{split}
\end{equation}
where $\left\langle ...\right\rangle $ represents the average over the remaining quenched disorder $f_{ki}$. The covariance matrix reads

\begin{equation}
\Sigma^{ab}=\kappa_{*}^{2}q^{ab}+\kappa_{1}^{2}p^{ab},
\end{equation}
where we defined the following quantities: 

\begin{subequations}
\begin{align}
q^{ab}&=\frac{1}{N}\sum_{i}s_{i}^{a}s_{i}^{b}\label{eq:def_qab} \\
p^{ab}&=\frac{1}{D}\sum_{k}\mu_{k}^{a}\mu_{k}^{b}\label{eq:def_pab} %\\
%\mu_{k}^{a}&=\frac{1}{\sqrt{N}}\sum_{i}f_{ki}s_{i}^{a}\label{eq:def_muab}
\end{align}
\end{subequations}
and coefficients:
\begin{subequations}
\begin{align}
\kappa_{0} & =\int Dz\,\sigma(z)\\
\kappa_{1} & =\int Dz\,z\sigma(z)\\
\kappa_{2} & =\int Dz\,\sigma^{2}(z)\\
\kappa_{*}^{2} & =\kappa_{2}-\kappa_{1}^{2}-\kappa_{0}^{2}.
\end{align}
\end{subequations}
Note that the factor magnetizations $\mu^a_k$ appearing in eq.~\ref{eq:def_pab} have already been defined in eq.~\ref{eq:def_muab}.

We observe that the replicated partition function \ref{eq:GET_Z}
is quadratic in $m_{\mu}^{a}$ and therefore we will be able to integrate
the variables $m_{\mu}^{a}$ right away.




\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{fig_1.png}
    \includegraphics[width=0.49\textwidth]{fig_1b.png}
    \caption{ \textbf{Storage-to-learning transitions} A) The phase diagram of the HMHM shows three region: the storage phase (green), where patterns $\xi_\mu$ are attractors; the learning phase (light blue), where factors $\mathbf{f}_k$ are attractors, and the spin glass phase (white), where the attractors are uncorrelated with wither $\xi_\mu$ or $\mathbf{f}_k$. The two asymptotes are at $\alpha\simeq0.138$ and $\alpha_D\simeq0.138$. The inset is a zoom of the region near the origin of the axes. B) The plot shows the factor magnetization $\mu$ along a vertical cut of the phase diagram: increasing $\alpha$ the factor magnetization $\mu$ becomes different from zero with a first-order phase transition. The dashed line is the analytical prediction of the replica theory, while the dots are numerical experiments averaged over 100 samples for each value of $\alpha$. The agreement with the theory is precise for $N=8000$, signaling strong finite size effects in this model.}
    \label{fig:phase_diagram}
\end{figure}

We solve this model in the replica-symmetric (RS) ansatz. For the complete derivation see the appendix. At the end of the long but straightforward calculation we end up with a free energy $f^{RS}$ that depends on eight order parameters: the factor magnetization $\mu$, the overlap between different replicas $q$, the diagonal and off-diagonal parts of $p^{ab}$ and their four conjugate parameters $\hat{\mu}$, $\hat{q}$, $\hat{p}_\text{d}$, $\hat{p}$.  
Given the control parameters $\beta$, $\alpha$ and $\alpha_D$, we obtain the physical value of the order parameters by minimizing the free energy:
\begin{equation}
    f^{RS}_\text{opt} = \min_{\mu, \hat{\mu}, q, \hat{q}, {p}_\text{d}, \hat{p}_\text{d}, p,\hat{p}} f^{RS}(\mu, \hat{\mu}, q, \hat{q}, {p}_\text{d}, \hat{p}_\text{d}, p,\hat{p}; \beta, \alpha,\alpha_D)
\end{equation}
%The RS free energy reads
%\begin{widetext}
%\begin{align}
%f^{RS}\left(q,\hat{q},p_{\text{d}},p,\hat{p}_{\text{\text{d}}},\hat{p},\mu,\hat{\mu}\right)= & \frac{\alpha}{2}-\frac{\alpha}{2}\beta\hat{q}(q-1)+\frac{\alpha}{2}\hat{p}_{\text{d}}p_{\text{d}}-\frac{\alpha}{2}\hat{p}p-\frac{\alpha_{T}}{2}\left(\hat{p}_{\text{d}}-\hat{p}\right)\mu^{2}+\hat{\mu}\mu\nonumber \\
%+ & \frac{\alpha}{2\beta}\left[\log\left[1-\beta\left(\Sigma_{\text{d}}-\Sigma\right)\right]-\frac{\beta\Sigma}{1-\beta\left(\Sigma_{\text{d}}-\Sigma\right)}\right]\nonumber \\
%+ & \frac{\alpha}{2\alpha_{T}\beta}\left[\log\left(1-\alpha_{T}\beta\left(\hat{p}_{\text{d}}-\hat{p}\right)\left(1-q\right)\right)-\frac{\alpha_{T}\beta\left(\hat{p}+\hat{p}_{\text{d}}q-2q\hat{p}\right)}{1-\alpha_{T}\beta\left(\hat{p}_{\text{d}}-\hat{p}\right)\left(1-q\right)}\right]\nonumber \\
%- & \frac{1}{\beta}\left\langle \int Dz\,\log\left[2\cosh\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}f\right]\right)\right]\right\rangle 
%\end{align}
%\end{widetext}

\subsubsection{Saddle-point equations}

Deriving $f^{RS}$ with respect to the order parameter we obtain  a set of eight equations that must be solved together (the so-called \emph{saddle-point equations}). We write here only three of them, leaving the rest to the appendix:

\begin{subequations} 
\begin{align}
q &=\left\langle \int Dz\,\tanh^{2}\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}f\right]\right)\right\rangle _{f}\label{eq:saddle_hatq} \\
\mu &=\left\langle \int Dz\,f\tanh\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}f\right]\right)\right\rangle _{f}\label{eq:saddle_hatmu} \\
\hat{q}&=\frac{\kappa_{*}^{2}(\kappa_{1}^{2}p+\kappa_{*}^{2}q)}{(1+\beta\kappa_{1}^{2}(p-p_{\text{d}})+\beta\kappa_{*}^{2}(q-1))^{2}}+\notag\\
&+\frac{\hat{p}+\alpha_{T}\beta q(\hat{p}-\hat{p}_{\text{d}})^{2}}{\beta(\alpha_{T}\beta(q-1)(\hat{p}-\hat{p}_{\text{d}})-1)^{2}}\label{eq:saddle_{q}}
\end{align}
\end{subequations}
where $Dz=dz\,e^{-z^2/2}/\sqrt{2\pi}$ is a compact notation for  Gaussians integration measures.
It is relevant to observe that the first two of these equations resemble closely the ones for $q$ and $m$ for the standard Hopfield model (see \cite{amit1987statistical}): now $f$ has the role of the retrieved pattern and $\mu$ has the role of the magnetization. The major difference is that in our case the equation for the conjugate $\hat{q}$ is more complicated and depends on the rest of the order parameters. A minor difference is that inside the integrals of the first two equation, $\hat{\mu}$ appears instead of $\mu$. This does not seem too relevant since $\mu$ and $\hat{\mu}$ are linearly proportional according to the saddle-point equation for $\hat{\mu}$ (see the appendix).

%Before solving iteratively these equations we perform limit $\beta \to \infty$, as it greatly simplifies the expressions. To still have finite order parameters in this limit we scale them as it follows:
%\begin{align}
%q & =1-\frac{\delta q}{\beta}\\
%p & =p_{d}-\frac{\delta p}{\beta}\\
%\hat{p} & =\beta\,\delta\hat{p}_{d}-\frac{1}{2}\delta\hat{p}\\
%\hat{p}_{d} & =\beta\,\delta\hat{p}_{d}+\frac{1}{2}\delta\hat{p}
%\end{align}
The solution to these equations in the limit $\beta \to \infty$ is shown in figure~\ref{fig:phase_diagram}A: for $\alpha>\alpha^\text{crit}(\alpha_D)$ the factor magnetization becomes finite with a discontinuous jump, showing that the model is actually capable of storing the factors $f$ as attractors (we checked numerically that the radius of attraction is finite). This jump is a first-order phase transition similarly to the catastrophic forgetting, but with the important difference that the magnetization becomes finite when $\alpha$ is \emph{larger} rather than smaller than a critical value. The critical point $\alpha^\text{crit}(\alpha_D)$ rapidly increases when $\alpha_D$ increases, up to the point where it diverges for $\alpha_D\simeq 0.138$. This critical value is numerically identical to the critical capacity of the standard Hopfield model and it is not a coincidence. In fact, in the limit $\alpha\to\infty$, the saddle-point equations of the HMHM become identical to those of the standard Hopfield model with $\mu$ playing the role of the magnetization and $f$ that of the retrieved patterns (the correct scalings for this limit and the explicit calculation are shown in the appendix). One way to look at this behaviour is to fix a value of $\alpha$ and to increase $\alpha_D$, thus moving horizontally in the phase diagram of figure~\ref{fig:phase_diagram}A: when $\alpha_D$ is low enough the model is able to retrieve the factors, then, when they become too many, the equivalent of a catastrophic forgetting happens. This transition happens at the Hopfield critical capacity only if $\alpha=\infty$, where the matching between the two models is perfect.

The agreement of the analytical solution with numerical simulations is shown in figure~\ref{fig:phase_diagram}B.


\subsection{Retrieval of the patterns}
\label{sec:pattern_retrieval}

\subsubsection{Gaussian equivalence}

For the second case we say that $m_1=O(1)$ and $m_\nu=O(1/\sqrt{N})$ for $\nu>1$. At the same time we say that $\mu_k=O(1/\sqrt{N})\quad\forall k$. In the thermodynamic limit this means that we look for a solution of the form 
\begin{equation}
    \vec m = (m,0,...,0)\quad \vec \mu = (0,...,0)
    \label{eq:pattern_retrieval_state}
\end{equation}

In this setting we must be careful to apply the GET only to the vanishing pattern magnetizations, leaving the terms involving $m_1$ as they are. The resulting expression of the average replicated partition function reads:
\begin{align}
\begin{split}
\left\langle Z^{n}\right\rangle & =e^{-\frac{\beta}{2}Pn}\sum_{\left\{ s_{i}^{a}\right\} }\int\prod_{\mu a}\frac{dm_{\mu}^{a}}{\sqrt{2\pi}}\exp\left\{ \frac{\beta}{2}\sum_{a=1}^{n}\left(m_{1}^{a}\right)^{2}+\frac{\beta}{2}\sum_{\mu=2}^{P}\sum_{a=1}^{n}\left(m_{\mu}^{a}\right)^{2}\right\} \left\langle \mathcal{N}(m_{\mu}^{a};0,\Sigma)\right\rangle _{f_{ki}}\cdot\\
 & \cdot \left\langle \prod_{a}\delta\left(m_{1}^{a}-\frac{1}{\sqrt{N}}\sum_{i=1}^{N}\sigma\left(\frac{1}{\sqrt{D}}\sum_{k=1}^{D}c_{1k}f_{ki}\right)s_{i}^{a}\right)\right\rangle _{c_{1k},f_{ki}}
 \end{split}
\end{align}
Note that now the average over $f_{ki}$ appears both in the Gaussian distribution resulting from the application of the GET and in a term involving $m_1$, reminiscent of the standard Hopfield model calculation in \cite{amit1987statistical}.

As we did in the first case, we solve the model within the RS ansatz and we report the complete calculation in the appendix. This time set the order parameters does not include $\mu$ and $\hat{\mu}$, but it does include $m$ (there is no need for a conjugate variable $\hat{m}$). The order parameters also include the auxiliary variables $t$, $\hat{t}$ that are needed to linearize a term in an intermediate integral. The definition of $t$ is 
\begin{equation}
    t= \frac{1}{N} \sum_i^N \hat{v}_i s_i
\end{equation}
where $\hat{v}_i$ are the conjugate variables of the auxiliary variables
\begin{equation}
    v_i = \frac{1}{\sqrt{D}}\sum_k^D c_{\mu k} f_{ki}.
\end{equation}
The auxiliary variables $v_i$ and $\hat{v}_i$ do not appear in the free energy because they can be integrated right away.
Summarizing, the set of nine order parameters is $m, q, \hat{q}, {p}_\text{d}, \hat{p}_\text{d}, p,\hat{p}, t, \hat{t}$. 

\subsubsection{Saddle-point-equations}

Again we show here only how the equation for $m$, $q$ and $\hat{q}$ change from the standard case in \cite{amit1987statistical}, and we write the rest of them in the appendix:

\begin{subequations}
\begin{align}
&q = \int Dv \int Dx \,\tanh^2 \left[\beta\left(v \hat{t}+\sigma(v)\, m + \sqrt{\alpha \, \hat{q}-\hat{t}^2}\,x \right)  \right]\\
&m = \int Dx \int Dv \, \sigma (v) \tanh \left[\beta \left(v \hat{t}+\sigma (v)\, m +\sqrt{\alpha \hat{q}-\hat{t}^2}\,x \right) \right] \\
&\hat{q} = \frac{\alpha_T}{\alpha_D} \, \frac{t^2 \, \hat{p}^2}{(1-\beta \,\alpha_T \, q\,\hat{p})^2}+\frac{\kappa_*^2 (\kappa_*^2 \,q +\kappa_1^2 \, p)}{\left[1-\beta \left( \kappa_*^2 (1-q)+ \kappa_1^2 (p_d-p)\right)\right]^2}+\frac{\hat{p}+\beta \alpha_T \, q (\hat{p}_d -\hat{p})^2}{\beta \left[1-\beta \alpha_T (1-q)(\hat{p}_d -\hat{p}) \right]^2}
\end{align}
\label{eq:pattern_retrieval}
\end{subequations}

We solve these equations in the limit $\beta\to\infty$ and we show the results in figure~\ref{fig:phase_diagram}A. A useful limit to consider is $\alpha_D\to\infty$: in this limit the equations~\ref{eq:pattern_retrieval} converge to the standard ones (see appendix), which was expected since the examples become uncorrelated. This produces an horizontal asymptote at $\alpha\simeq0.138$ for the critical line of $m$. Decreasing $\alpha_D$ the example patterns become more correlated and the catastrophic forgetting happens at a lower value of $\alpha$, until it happens at $\alpha=0$ for $\alpha_D\to0$.


%\begin{figure}
%    \centering
%    \caption{energy distribution}
%    \label{fig:energy_distribution}
%\end{figure}

\subsection{Generalization and infinite storage capacity}
\label{sec:generalization}

In addition to the analytical results, here we provide a numerical study of a regime of $\alpha$ and $\alpha_D$ where \emph{both} the factors and patterns are attractors of the model. Let's start by considering a finite-size case, $N<\infty$, within the factor retrieval phase (light-blue area in \ref{fig:phase_diagram}A). For fixed $N$ and $D$, we observe that increasing $\alpha$ leads to the surprising behaviour that the patterns become again attractors, well above the catastrophic forgetting transition (red points in figure \ref{fig:generalization}A). Furthermore, we find that even \emph{previously unseen patterns} are attractors for the same values of $\alpha$ (orange points in figure \ref{fig:generalization}A). 
The number $P$ of examples that are needed for this \emph{generalization} phenomenon to happen depends on $N$ and $D$; in particular, the larger $N$, the higher the required value of $P$. Equivalently we can fix $N$ and $P$ and in this case we observe generalization up to a maximum value of $D$. This second case is easier to study numerically than the first one (since the first one requires very high values of $P$); the results are shown in figure~\ref{fig:generalization}B: we see that the maximum value of $D$ below which we observe generalization scales as $N^{1/2}$, meaning that we can extract a maximum of $D=O(N^{1/2})$ factors from a dataset made of $P=\alpha N$ examples. Equivalently, this suggests that in order to extract $D=O(N)$ factors from $N$-dimensional data we would need $P^2$ examples. 

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{fig_2.png}
    \includegraphics[width=0.49\textwidth]{fig_3.png}
    \caption{\textbf{The HMHM is capable of generalization} A) The curve shows different magnetizations as a function of $\alpha$, for fixed values of $N$ and $D$. In red we show the magnetization for training patterns (those used in the Hebb rule); in orange we show the magnetization for unseen patterns; in green we show the highest factor magnetization at the end of a dynamics with random initialization (overlap zero with all the patterns and factors). B) The curves show the maximum value of $D$ below which we observe generalization, for two values of $alpha$. In both cases the scaling is found to be $N^{1/2}$.
 }
    \label{fig:generalization}
\end{figure}

\subsection{Convergence from random initialization}
\label{sec:convergence}

The HMHM shows another surprising numerical result. In the same setting that we discussed in section~\ref{sec:generalization}, namely fixing $N$ and $D$, we find that at sufficiently large $\alpha$ the dynamics converges on one of the factors even from a random initial condition (see the green points in figure~\ref{fig:generalization}A). As for the generalization behaviour, we observe that the value of $P$ needed to observe this convergence behaviour increases as $O(N^\gamma)$ with $\gamma>1$. We conjecture that the scaling is the same as the one for generalization (namely $\gamma=2$, see figure~\ref{fig:generalization}B), but for now we do not have more evidence about this behaviour.

\section{Discussion and conclusions}
\label{sec:discussion}

\subsection{Storage-to-learning transitions}

Summing up results from the sections \ref{sec:factor_retrieval} and \ref{sec:pattern_retrieval} we end up with a phase diagram split by two critical lines into three regions (see figure~\ref{fig:phase_diagram}A): the factor retrieval region, above the spinodal line of the states defined by eq.~\ref{eq:factor_retrieval_state}, the pattern retrieval region, below the the spinodal line of the states defined by eq.~\ref{eq:pattern_retrieval_state}, and a spin-glass region between the two lines. TODO: do the two retrieval regions mix near the origin? for now we are unable to say. TODO: can we argue better that the middle region is actually spin-glass and not paramagnetic? TODO: can we say something about the relative energies of the various solutions? These regions should be equivalent to the metastable region in standard Hopfield. Do we have the equivalent of stable regions?

The behaviour that we call \emph{storage-to-learning transitions} can be observed following a vertical line in the phase diagram, namely fixing a value of $\alpha_D$ and increasing $\alpha$. Starting from small $\alpha$ we obtain a model of storage of correlated patterns: the capacity is smaller than the uncorrelated case, but the phenomenology is similar since there is a maximum number of pattern that can be stored, and attempting to store a bigger number results in catastrophic forgetting. The surprising result is that, when we have $\alpha_D\leq0.138$ (i.e. when the correlations are strong enough), if we keep increasing the number of patterns we find another phase beyond the spin-glass one, where attractors corresponding to the factors $f$ appear. If we interpret the patterns $\xi$ as the examples of a training dataset we see that, if the dataset is big enough, the model is capable of inferring the factors hidden in the data. This behaviour resembles the feature extraction typical of machine learning (TODO: cite something?). This suggest to study more complicated variations of the Hopfield model with correlated examples and raises the question if this approach could be useful to understand the success of deep neural networks.

This indication is further strengthened by the results described in \ref{sec:generalization}: if the number of example patterns diverges (or if $\alpha_D\to0$) the "training examples" that we used in eq.~\ref{eq:hebb_rule} become again attractors of the model. At the present moment we are not able to provide an analytical explanation of this phenomenon, but we can provide an interpretation based on the available numerical evidence. We suggest that the training examples become attractors in this regime exploiting a mechanism similar to that of the mixed states in the standard Hopfield model: there, within the phase where the patterns are the global minimum of the free energy, there are additional phases where linear combinations of the patterns are local minima and therefore attractors themselves. In the same fashion we think that in the HMHM the train examples can be attractors at high $\alpha$ when linear combinations of them become local minima of the free energy; in this picture, the model would learn to express the training patterns as the proper linear combination of the factors. An indication that this might be the case comes from the fact that not only the train examples are attractors, but also previously unseen patterns, making the model effectively able to generalize and capable of "storing" an infinite amount of patterns. This behaviour suggests that the Hopfield model has inferred the "ground truth", namely the generative model of the data. 

%Given this surprising behaviour, it is natural to ask if the model can be used efficiently to sample new examples. 
% what about dreaming? the model can be used to be generative

\subsection{Possible application to matrix factorization}

The structure that we chose for our data (eq.~\ref{eq:correlated_examples}) is related to the problem of matrix factorization (and its variations), where given a $P\times N$ matrix $\mathbf{X}$ one wants to find a decomposition 
\begin{equation}
    \mathbf{X}=\mathbf{C}\mathbf{F}
\end{equation}
Since we proved that the HMHM has attractors corresponding to the factors hidden in the examples, we are tempted to ask if our model to solve the factorization of our data matrix $\xi_{\mu i}$, in the setting where we know $\xi_{\mu i}$ but not the factors $f_{ki}$.

The main obstacle to this task is that to actually find the factors we need to initialize the model with a non-zero overlap with one of them, which we don't know. We could consider the regime described in \ref{sec:convergence}, where the dynamics converge on the factors from a random initialization, but in this case we would have to sample many times until each of the factors has been found. This is expensive, especially is we don't know a priori the number of hidden factors.

%Some attempts of using variations of Hopfield models to solve the problem of matrix factorization have already been studied \cite{frolov2007boolean,frolov2009recurrent,frolov2010origin}, but those 

In a recent work \cite{camilli2022matrix}, the authors suggest a \emph{decimation} procedure that, given the previous knowledge of some of the patterns, removes their contribution from the coupling matrix, allowing an easier retrieval of the remaining ones. 

We suggest that we could use the decimation procedure in the HMHM in the regime described in \ref{sec:convergence}, where the dynamics converge on the factors from a random initialization. In this way we might be able to find all the factors hidden in the data and effectively factorize the data matrix.

\begin{comment}
\subsection{Relation to the attention mechanism in deep neural networks}

TBA. References: 

Famous paper where they show the importance of the attention mechanism for deep learning: \cite{vaswani2017attention}

Series of works by Krotov and Hopfield himself where they study $p$-spin  "dense" generalizations of the $p=2$ Hopfield model: \cite{krotov2016dense, krotov2018dense}

Papers that relate dense Hopfield models to the attention mechanism: \cite{ramsauer2020hopfield, mcbal2020energy-based, mcbal2021transformers}
In particular, this one \cite{yang2022transformers} defines the concept of "unfolded" optimization, which means building layers of deep networks whose forward pass coincides with the time-update rule of a certain recurrent network (such as the Hopfield model).

Examples of how the features computed with the Hebb rule could be relevant for a forward pass of a transformer: \cite{radhakrishnan2020overparameterized, elhage2022toy, refinetti2022dynamics}.
\end{comment}

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
