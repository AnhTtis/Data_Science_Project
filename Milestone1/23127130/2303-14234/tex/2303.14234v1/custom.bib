
@inproceedings{zhao_automatic_2020,
	address = {Barcelona, Spain (Online)},
	title = {Automatic {Interlinear} {Glossing} for {Under}-{Resourced} {Languages} {Leveraging} {Translations}},
	url = {https://www.aclweb.org/anthology/2020.coling-main.471},
	doi = {10.18653/v1/2020.coling-main.471},
	abstract = {Interlinear Glossed Text (IGT) is a widely used format for encoding linguistic information in language documentation projects and scholarly papers. Manual production of IGT takes time and requires linguistic expertise. We attempt to address this issue by creating automatic glossing models, using modern multi-source neural models that additionally leverage easy-to-collect translations. We further explore cross-lingual transfer and a simple output length control mechanism, further reﬁning our models. Evaluated on three challenging low-resource scenarios, our approach signiﬁcantly outperforms a recent, state-of-the-art baseline, particularly improving on overall accuracy as well as lemma and tag recall.},
	language = {en},
	urldate = {2022-11-07},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Zhao, Xingyuan and Ozaki, Satoru and Anastasopoulos, Antonios and Neubig, Graham and Levin, Lori},
	year = {2020},
	pages = {5397--5408},
	file = {Zhao et al. - 2020 - Automatic Interlinear Glossing for Under-Resourced.pdf:/Users/milesper/Zotero/storage/5VYW4ZF2/5VYW4ZF2.pdf:application/pdf},
}

@article{seifart_language_2018,
	title = {Language documentation twenty-five years on},
	volume = {94},
	issn = {1535-0665},
	url = {https://muse.jhu.edu/article/712110},
	doi = {10.1353/lan.2018.0070},
	language = {en},
	number = {4},
	urldate = {2022-11-07},
	journal = {Language},
	author = {Seifart, Frank and Evans, Nicholas and Hammarström, Harald and Levinson, Stephen C.},
	year = {2018},
	pages = {e324--e345},
	file = {Seifart et al. - 2018 - Language documentation twenty-five years on.pdf:/Users/milesper/Zotero/storage/69D2D2VJ/Seifart et al. - 2018 - Language documentation twenty-five years on.pdf:application/pdf},
}

@misc{liu_computational_2021,
	title = {Computational {Morphology} with {Neural} {Network} {Approaches}},
	url = {http://arxiv.org/abs/2105.09404},
	abstract = {Neural network approaches have been applied to computational morphology with great success, improving the performance of most tasks by a large margin and providing new perspectives for modeling. This paper starts with a brief introduction to computational morphology, followed by a review of recent work on computational morphology with neural network approaches, to provide an overview of the area. In the end, we will analyze the advantages and problems of neural network approaches to computational morphology, and point out some directions to be explored by future research and study.},
	language = {en},
	urldate = {2022-11-07},
	publisher = {arXiv},
	author = {Liu, Ling},
	month = may,
	year = {2021},
	note = {arXiv:2105.09404 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Liu - 2021 - Computational Morphology with Neural Network Appro.pdf:/Users/milesper/Zotero/storage/ZJ6DK829/Liu - 2021 - Computational Morphology with Neural Network Appro.pdf:application/pdf},
}

@article{lewis_developing_2010,
	title = {Developing {ODIN}: {A} {Multilingual} {Repository} of {Annotated} {Language} {Data} for {Hundreds} of the {World}'s {Languages}},
	volume = {25},
	issn = {0268-1145, 1477-4615},
	shorttitle = {Developing {ODIN}},
	url = {https://academic.oup.com/dsh/article-lookup/doi/10.1093/llc/fqq006},
	doi = {10.1093/llc/fqq006},
	language = {en},
	number = {3},
	urldate = {2022-11-07},
	journal = {Literary and Linguistic Computing},
	author = {Lewis, W. D. and Xia, F.},
	month = sep,
	year = {2010},
	pages = {303--319},
	file = {Lewis and Xia - 2010 - Developing ODIN A Multilingual Repository of Anno.pdf:/Users/milesper/Zotero/storage/KI7EXGRJ/Lewis and Xia - 2010 - Developing ODIN A Multilingual Repository of Anno.pdf:application/pdf},
}

@article{baldridge_how_2009,
	title = {How well does active learning actually work? {Time}-based evaluation of cost-reduction strategies for language documentation.},
	abstract = {Machine involvement has the potential to speed up language documentation. We assess this potential with timed annotation experiments that consider annotator expertise, example selection methods, and suggestions from a machine classiﬁer. We ﬁnd that better example selection and label suggestions improve efﬁciency, but effectiveness depends strongly on annotator expertise. Our expert performed best with uncertainty selection, but gained little from suggestions. Our non-expert performed best with random selection and suggestions. The results underscore the importance both of measuring annotation cost reductions with respect to time and of the need for cost-sensitive learning methods that adapt to annotators.},
	language = {en},
	author = {Baldridge, Jason and Palmer, Alexis},
	year = {2009},
	pages = {10},
	file = {Baldridge and Palmer - How well does active learning actually work Time-.pdf:/Users/milesper/Zotero/storage/FK97698A/Baldridge and Palmer - How well does active learning actually work Time-.pdf:application/pdf},
}

@article{palmer_computational_2010,
	title = {Computational strategies for reducing annotation effort in language documentation: {A} case study in creating interlinear texts for {Uspanteko}},
	volume = {3},
	issn = {1945-3604},
	shorttitle = {Computational strategies for reducing annotation effort in language documentation},
	url = {https://journals.colorado.edu/index.php/lilt/article/view/1217},
	doi = {10.33011/lilt.v3i.1217},
	abstract = {With the urgent need to document the world’s dying languages, it is important to explore ways to speed up language documentation efforts. One promising avenue is to use techniques from computational linguistics to automate some of the process. Here we consider unsupervised morphological segmentation and active learning for creating interlinear glossed text (IGT) for the Mayan language Uspanteko. The practical goal is to produce a totally annotated corpus that is as accurate as possible given limited time for manual annotation. We discuss results from several experiments that suggest there is indeed much promise in these methods but also show that further development is necessary to make them robustly useful for a wide range of conditions and tasks. We also provide a detailed discussion of how two documentary linguists perceived machine support in IGT production and how their annotation performance varied with different levels of machine support.},
	language = {en},
	urldate = {2022-11-07},
	journal = {Linguistic Issues in Language Technology},
	author = {Palmer, Alexis and Moon, Taesun and Baldridge, Jason and Erk, Katrin and Campbell, Eric and Can, Telma},
	month = feb,
	year = {2010},
	file = {Palmer et al. - 2010 - Computational strategies for reducing annotation e.pdf:/Users/milesper/Zotero/storage/HQMB29M5/Palmer et al. - 2010 - Computational strategies for reducing annotation e.pdf:application/pdf},
}

@article{mcmillan-major_automating_2020,
	title = {Automating {Gloss} {Generation} in {Interlinear} {Glossed} {Text}},
	volume = {3},
	url = {https://scholarworks.umass.edu/scil/vol3/iss1/33/},
	doi = {10.7275/TSMK-SA32},
	abstract = {Interlinear Glossed Text (IGT) is a rich data type produced by linguists for the purposes of presenting an analysis of a language’s semantic and grammatical properties. I combine linguistic knowledge and statistical machine learning to develop a system for automatically annotating low-resource language data. I train a generative system for each language using on the order of 1000 IGT. The input to the system is the morphologically segmented source language phrase and its English translation. The system outputs the predicted linguistic annotation for each morpheme of the source phrase. The ﬁnal system is tested on held-out IGT sets for Abui [abz], Chintang [ctn], and Matsigenka [mcb] and achieves 71.7\%, 80.3\%, and 84.9\% accuracy, respectively.},
	language = {en},
	number = {1},
	urldate = {2022-11-07},
	journal = {Proceedings of the Society for Computation in Linguistics},
	author = {McMillan-Major, Angelina},
	year = {2020},
	note = {Publisher: University of Mass Amherst},
	pages = {338--349},
	file = {McMillan-Major - Automating Gloss Generation in Interlinear Glossed.pdf:/Users/milesper/Zotero/storage/ZM9NGPQY/McMillan-Major - Automating Gloss Generation in Interlinear Glossed.pdf:application/pdf},
}

@inproceedings{moeller_automatic_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {Automatic {Glossing} in a {Low}-{Resource} {Setting} for {Language} {Documentation}},
	url = {https://aclanthology.org/W18-4809},
	abstract = {Morphological analysis of morphologically rich and low-resource languages is important to both descriptive linguistics and natural language processing. Field documentary efforts usually procure analyzed data in cooperation with native speakers who are capable of providing some level of linguistic information. Manually annotating such data is very expensive and the traditional process is arguably too slow in the face of language endangerment and loss. We report on a case study of learning to automatically gloss a Nakh-Daghestanian language, Lezgi, from a very small amount of seed data. We compare a conditional random field based sequence labeler and a neural encoder-decoder model and show that a nearly 0.9 F1-score on labeled accuracy of morphemes can be achieved with 3,000 words of transcribed oral text. Errors are mostly limited to morphemes with high allomorphy. These results are potentially useful for developing rapid annotation and fieldwork tools to support documentation of morphologically rich, endangered languages.},
	urldate = {2022-11-07},
	booktitle = {Proceedings of the {Workshop} on {Computational} {Modeling} of {Polysynthetic} {Languages}},
	publisher = {Association for Computational Linguistics},
	author = {Moeller, Sarah and Hulden, Mans},
	month = aug,
	year = {2018},
	pages = {84--93},
}

@article{forbes2017three,
  title={Three Gitksan texts},
  author={Forbes, Clarissa and Davis, Henry and Schwan, Michael},
  journal={CARL ALEXANDER, MATT ANDREW, AND JOHN LYON 1--11},
  pages={47},
  year={2017}
}

@inproceedings{heigold-etal-2017-extensive,
    title = "An Extensive Empirical Evaluation of Character-Based Morphological Tagging for 14 Languages",
    author = "Heigold, Georg  and
      Neumann, Guenter  and
      van Genabith, Josef",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-1048",
    pages = "505--513",
    abstract = "This paper investigates neural character-based morphological tagging for languages with complex morphology and large tag sets. Character-based approaches are attractive as they can handle rarely- and unseen words gracefully. We evaluate on 14 languages and observe consistent gains over a state-of-the-art morphological tagger across all languages except for English and French, where we match the state-of-the-art. We compare two architectures for computing character-based word vectors using recurrent (RNN) and convolutional (CNN) nets. We show that the CNN based approach performs slightly worse and less consistently than the RNN based approach. Small but systematic gains are observed when combining the two architectures by ensembling.",
}

@inproceedings{ott2019fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}

@inproceedings{palmer2009evaluating,
  title={Evaluating automation strategies in language documentation},
  author={Palmer, Alexis and Moon, Taesun and Baldridge, Jason},
  booktitle={Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing},
  pages={36--44},
  year={2009}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{ustun2016unsupervised,
  title={Unsupervised morphological segmentation using neural word embeddings},
  author={{\"U}st{\"u}n, Ahmet and Can, Burcu},
  booktitle={International conference on statistical language and speech processing},
  pages={43--53},
  year={2016},
  organization={Springer}
}

@article{kann2018fortification,
  author    = {Katharina Kann and
               Manuel Mager and
               Iv{\'{a}}n V. Meza{-}Ru{\'{\i}}z and
               Hinrich Sch{\"{u}}tze},
  title     = {Fortification of Neural Morphological Segmentation Models for Polysynthetic
               Minimal-Resource Languages},
  journal   = {CoRR},
  volume    = {abs/1804.06024},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.06024},
  eprinttype = {arXiv},
  eprint    = {1804.06024},
  timestamp = {Mon, 05 Aug 2019 17:00:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-06024.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{lewis2019bart,
  doi = {10.48550/ARXIV.1910.13461},
  
  url = {https://arxiv.org/abs/1910.13461},
  
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{hendrycks_gaussian_2020,
	title = {Gaussian {Error} {Linear} {Units} ({GELUs})},
	url = {http://arxiv.org/abs/1606.08415},
	abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is xΦ(x), where Φ(x) the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (x1x{\textgreater}0). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and ﬁnd performance improvements across all considered computer vision, natural language processing, and speech tasks.},
	language = {en},
	urldate = {2022-12-12},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	month = jul,
	year = {2020},
	note = {arXiv:1606.08415 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Trimmed version of 2016 draft; add exact formula},
	file = {Hendrycks and Gimpel - 2020 - Gaussian Error Linear Units (GELUs).pdf:/Users/milesper/Zotero/storage/E49936LE/Hendrycks and Gimpel - 2020 - Gaussian Error Linear Units (GELUs).pdf:application/pdf},
}


@article{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{palmer2007igt,
  title={IGT-XML: An XML format for interlinearized glossed text},
  author={Palmer, Alexis and Erk, Katrin},
  booktitle={Proceedings of the linguistic annotation workshop},
  pages={176--183},
  year={2007}
}

@article{goodman2015xigt,
  title={Xigt: extensible interlinear glossed text for natural language processing},
  author={Goodman, Michael Wayne and Crowgey, Joshua and Xia, Fei and Bender, Emily M},
  journal={Language Resources and Evaluation},
  volume={49},
  number={2},
  pages={455--485},
  year={2015},
  publisher={Springer}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}


@misc{bostrom_byte_2020,
	title = {Byte {Pair} {Encoding} is {Suboptimal} for {Language} {Model} {Pretraining}},
	url = {http://arxiv.org/abs/2004.03720},
	abstract = {The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE's greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE.},
	urldate = {2022-12-05},
	publisher = {arXiv},
	author = {Bostrom, Kaj and Durrett, Greg},
	month = oct,
	year = {2020},
	note = {arXiv:2004.03720 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7},
	annote = {Comment: 5 pages, 3 figures. To be published in Findings of EMNLP 2020},
	file = {arXiv Fulltext PDF:/Users/milesper/Zotero/storage/8EY8YX5T/Bostrom and Durrett - 2020 - Byte Pair Encoding is Suboptimal for Language Mode.pdf:application/pdf;arXiv.org Snapshot:/Users/milesper/Zotero/storage/E8K55SLP/2004.html:text/html},
}


@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2023-03-12},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/milesper/Zotero/storage/2QB6SGGL/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf;arXiv.org Snapshot:/Users/milesper/Zotero/storage/RW3XIKK9/1907.html:text/html},
}


@misc{gessler_microbert_2023,
	title = {{MicroBERT}: {Effective} {Training} of {Low}-resource {Monolingual} {BERTs} through {Parameter} {Reduction} and {Multitask} {Learning}},
	shorttitle = {{MicroBERT}},
	url = {http://arxiv.org/abs/2212.12510},
	abstract = {Transformer language models (TLMs) are critical for most NLP tasks, but they are difﬁcult to create for low-resource languages because of how much pretraining data they require. In this work, we investigate two techniques for training monolingual TLMs in a low-resource setting: greatly reducing TLM size, and complementing the masked language modeling objective with two linguistically rich supervised tasks (part-of-speech tagging and dependency parsing). Results from 7 diverse languages indicate that our model, MicroBERT, is able to produce marked improvements in downstream task evaluations relative to a typical monolingual TLM pretraining approach. Speciﬁcally, we ﬁnd that monolingual MicroBERT models achieve gains of up to 18\% for parser LAS and 11\% for NER F1 compared to a multilingual baseline, mBERT, while having less than 1\% of its parameter count. We conclude reducing TLM parameter count and using labeled data for pretraining low-resource TLMs can yield large quality beneﬁts and in some cases produce models that outperform multilingual approaches.},
	language = {en},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Gessler, Luke and Zeldes, Amir},
	month = jan,
	year = {2023},
	note = {arXiv:2212.12510 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Presented at MRL at EMNLP 2022 in Abu Dhabi. Code at https://github.com/lgessler/microbert and models at https://huggingface.co/lgessler},
	file = {Gessler and Zeldes - 2023 - MicroBERT Effective Training of Low-resource Mono.pdf:/Users/milesper/Zotero/storage/SIGIAEL8/Gessler and Zeldes - 2023 - MicroBERT Effective Training of Low-resource Mono.pdf:application/pdf},
}
