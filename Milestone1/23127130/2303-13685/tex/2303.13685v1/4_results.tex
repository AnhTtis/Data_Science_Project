%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%        4. Results         %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec:results}

\subsection{MOS prediction results}
\label{subsec:mos_results}
We first evaluate our MOS-prediction performance in comparison with other approaches. In particular, we compare against NISQA~\cite{mittag2019non}, which we modified to estimate human-accessed MOS. Originally, they estimate perceptual objective listening quality assessment (POLQA)~\cite{beerends2013perceptual} scores using a CNN and BLSTM architecture. We also compare against the PMOS model proposed in~\cite{dong2020pyramid}, which is identical in structure to our PMOS model. Finally, we include our proposed SE+PMOS approach~\cite{nayem2021incorporating} (no joint training), where our PMOS model is held fixed while the SE model is training using the embeddings from the PMOS encoder. 

We use four metrics to evaluate MOS-estimation performance: mean absolute error (MAE), epsilon insensitive root mean squared error (RMSE)~\cite{rec2012p}, Pearson’s correlation coefficient $\gamma$ (PCC), and Spearman’s rank correlation coefficient $\rho$ (SRCC). 

%    Later, both models are jointly-trained for fine tuning. Our proposed PMOS model is similar of \cite{nayem2021incorporating}, however, SE models are different in structure.

\input{table_mos.tex}

Table~\ref{tab:mos_results} shows the results, where our proposed approach and SE+PMOS clearly outperform the other MOS prediction models according to all metrics. MAE is minimized by $0.6$ compared to the original PMOS~\cite{dong2020pyramid} approach. There is also a $0.05$ reduction in RMSE. This justifies our proposed approach that combines MOS estimation and speech enhancement tasks. Note, however, that similar results are obtained for our proposed approach and the SE+PMOS approach, which suggests that joint training (e.g., fine tuning) may help speech enhancement more than MOS prediction.  




\subsection{Speech enhancement model}
\label{subsec:se_results}
\input{table_results2.tex}
For speech enhancement, we compare against a baseline approach without an attention mechanism \cite{graves2013speech}. We denote this baseline approach as SE. Five separate loss functions are applied to optimize this approach, and they are MSE, MSE plus signal approximation, MOS, signal approximation with MOS, and SDR. To compute the MOS loss function, we utilize the SE loss function from \cite{fu2019learning} which leverages objective-MOS (oMOS) ratings learned from a speech assessment model~\cite{fu2018quality}. SDR~\cite{kawanaka2020stable} loss functions are proposed in literature previously with different enhancement architectures. For the SDR loss function, the SE model is optimized using the following cost function:
\begin{align}
    \mathcal{L}_{SDR} = \sum_{n=1}^N \mathcal{K}_{\theta}  \Big( 10 \log \frac{\Vert s^n\Vert^2}{\Vert s^n-\hat{s}^n\Vert^2} \Big)
\end{align}
where $\mathcal{K}_\theta(a)=\theta\cdot \tanh(\frac{a}{\theta})$, $\theta$ is a clipping parameter, $N$ is the mini-batch size, and $s^n$ and $\hat{s}^n$ are the n\textsuperscript{th} sample of the clean and estimated speech signal in time. We use $\theta=20$ in our training. We also compare against a generative adversarial network (GAN) approach that individually optimizes with PESQ and STOI~\cite{fu2019metricGAN}. We denote this model as MetricGAN. 
% They estimate the IRM conditioned on continuous space of the discriminator label based on either PESQ or STOI target label. 
They estimate the IRM for a speech mixture conditioned on a GAN discriminator that outputs evaluation scores in continuous space (i.e. scores between 0 and 1) based on either normalized PESQ or STOI target metrics. 
We compare our model with the ensemble-based Specialized Speech Enhancement Model Selection (SSEMS) approach~\cite{zezario2019specialized} that uses Quality-Net~\cite{fu2018quality} as its objective function in a black-box manner. Quality-Net is an oMOS approach that estimates the Perceptual Evaluation of Speech Quality (PESQ) score. The SSEMS approach uses an ensemble of enhancement models, each trained on audio at specific SNRs and speaker genders. During inference, it selects the output with the highest PESQ score. SSEMS uses a SNR threshold of $20$ dB, while we use a threshold of $0$ dB for balanced training and better performance. Additionally, we conduct a comparison with our initial approach that integrates MOS embeddings in speech enhancement, as presented in \cite{nayem2021incorporating}. This model is referred to as SE+PMOS, and it does not involve joint training or the QSM language model. We evaluate SE+PMOS with varying combinations of loss functions. %We compare against a quantized speech enhancement model which utilizes a spectral language model~\cite{nayem2021towards}. This model is motivated from chimera++~\cite{wang2018alternative} in structure with BLSTM layers and deep clustering (dc) loss.
%Traditional chimera++ model estimates a phase-sensitive mask which has been applied in the task of speech enhancement in non-speech noisy conditions with multi-talker speech~\cite{wichern2019wham, yang2019improved}. However, in \cite{nayem2021incorporating}, they estimate quantized speech signal, not mask; they use cross-entropy classification (cls) loss, and signal approximation loss altogether. They report best results using per-frequency quantized spectral model (fQSM) as language model for beam search (bS) with beam size $100$. We use this model as our comparison model denoting as Chi++\textsubscript{fQSM,bS}. 
All models are trained using the experimental setup that is previously mentioned. We modify the comparison models using the code provided by the original authors.

We assess speech enhancement performance using PESQ~\cite{rix2001perceptual}, scale-invariant SDR (SI-SDR)~\cite{le2019sdr}, and extended STOI (ESTOI)~\cite{jensen2016algorithm}. In the absence of actual human quality objective, we measure the predicted MOS score of the enhanced speech, using our proposed PMOS model, since we aim to improve human-assessed speech quality. We denote this metric as MOS listener quality objective (MOS-LQO). Table~\ref{tab:results_cosineVoices} shows the average results of the different enhancement models, according to each of the performance metrics on COSINE and VOiCES dataset. As the scores of the unprocessed mixtures show, the VOiCES corpus is  more challenging than the COSINE corpus. 
With the baseline SE model, we experiment with 5 different combination of loss functions. Using the MSE loss only in SE:mse, we see improvements in objective scores, except with MOS-LQO for the COSINE data. Then we apply a MOS loss $\mathcal{L}_{mos}$ as the sole objective criterion, as proposed in \cite{fu2019learning}. Our experimental results show that this approach results in an overall improvement of $1.4$ in MOS-LQO compared to SE:mse. %We apply MOS-LQO scores of enhanced speech to calculate MOS loss $\mathcal{L}_{mos}$ as the only objective criteria as proposed in \cite{fu2019learning}, which gives improves MOS-LQO by $1.4$ overall compared with SE:mse. 
Then we separately combine the signal approximation loss with the mse loss and MOS loss (e.g., mse+sa and mos+sa). In PESQ, we gain an average of $\ge0.05$ and $\ge0.07$ compared to the models that use only the MSE loss and only the MOS loss, respectively. Furthermore, the model trained with the mos+sa loss function achieves the highest MOS-LQO score of $4.4$ and $5.7$ among all five loss functions tested with the SE model in COSINE and VOiCES dataset, respectively. This result is on average $1.15$ MOS-LQO higher than that obtained with the mse+sa loss function. These scores suggest that $\mathcal{L}_{mse}$ and $\mathcal{L}_{sa}$ maximize the overall speech intelligibility, whereas $\mathcal{L}_{mos}$ guides the model towards perceptual speech quality. Note that in all these $\mathcal{L}_{mos}$ calculations, we use a separately trained PMOS model's output without joint learning.
Lastly, we apply the SDR loss function as proposed in \cite{kawanaka2020stable}, which is used as the pre-training stage for model training. We observe an average gain of $0.9$ in SI-SDR, however, it yields a poor score according to other metrics, especially a $0.7$ loss in MOS-LQO compared to SE with mse and sa loss terms. 

SE+PMOS is separately investigated with 3 combinations of loss functions, i.e. mse, mse+sa, and mse+sa+mos. Compared with SE models, SE+PMOS with mse loss achieves $0.9$ SI-SDR and $1.75$ MOS-LQO improvements on average, which shows the benefit of incorporating the PMOS model. The SE+PMOS:mse+sa model improves the performance further with an average of $0.14$ ESTOI gain over the SE:mse+sa model. The inclusion of the mos loss gives the best MOS-LQO scores of $5.1$ and $6.5$ over all the comparison models in noisy and reverberant conditions, respectively.

\input{table_results3.tex}
MetricGAN optimizes PESQ or STOI, therefore, it outperforms other comparison models in terms of PESQ and ESTOI, although the scores for the SE+PMOS approaches are higher according to the other evaluation metrics even though these metrics are not leveraged during training. 
SSEMS yields the lowest scores across all metrics compared with SE+PMOS and MetricGAN approaches, though we do parameter tuning for this model.
Chi++\textsubscript{fQSM,bS} estimates quantized speech, and the results show that it affects the traditional objective functions. This performs poorly compared with the SE+PMOS and MetricGAN approaches, however, on average, it outperforms SSEMS in all criteria, and the SE models in terms of PESQ. With the MOS-LQO criteria, it fails to produce good scores. This points out the importance of incorporating perceptual features during enhancement, which Chi++\textsubscript{fQSM,bS} clearly lacks.

We calculate the performance of our proposed model using two combinations of loss functions. 
Using only mse and sa loss terms, we achieve the highest ESTOI scores for both corpora, though these results are nearly identical to the model trained with all three loss terms. Using $\mathcal{L}$ (eq:\ref{eq:loss}) in our proposed model, we obtain the highest SI-SDR scores while maintaining similar PESQ and ESTOI performance as compared to the best-performing model. Specifically, our proposed model achieves the highest ESTOI score and an average PESQ score that is only $0.03$ less than that of the best performing MetricGAN:pesq model.
Contrasting with the Chi++\textsubscript{fQSM,bS} model, which uses spectral language model to estimate quantized speech, our proposed approach outperforms the quantized model according to all metrics, which proves the significance of joint learning.% to direct speech enhancement model towards perceptually better speech using a speech quality assessment model.
When comparing MOS-LQO scores, our proposed:mse+sa+mos model achieves better scores than the other models except the SE+PMOS:mse+sa+mos model with an average of only $0.05$ declination. Thus, the inclusion of a spectral language model helps the model proposed (e.g., mse+sa+mos) to estimate better quality speech according to the overall evaluation criteria. 
It is important to note that our proposed approach performs best according to SI-SDR in both noisy and reverberant environments, where this metric is not used by any of the approaches during optimization.  

We further examine our approaches using completely unseen corpora. We test models with the CHiME-5 and CHiME-4 corpora where the models are trained from the COSINE dataset according to the system setup mentioned in section~\ref{subsec:setup}. Table~\ref{tab:results_chime} shows the performance evaluated according to PESQ, SI-SDR, ESTOI, MOS-LQO, and word error rate (WER). To calculate WER, we use the conventional ASR baseline that is provided with CHiME-5 and CHiME-4 dataset. We investigate WER with both GMM based ASR and end-to-end ASR, however, we find that the end-to-end approach results in a higher error compared to the GMM baseline. This might happen due to larger data requirements of the end-to-end ASR system as mentioned in \cite{barker2018fifth}. Therefore, we use the GMM ASR approach to compare the WER performance of the enhancement models.
From the scores of mixtures, we find that CHiME-5 is more challenging than CHiME-4 with a $118.8\%$ higher WER and a $0.46$ lower SI-SDR. Our proposed approach yields the best MOS-LQO scores with $4.9$ with CHiME-5 and $6$ with CHiME-4 data. The proposed mse+sa model results in the lowest WER of $78.3$ and $18.1$ using CHiME-5 and CHiME-4, respectively. Note that the WER of the GMM baseline ASR for the CHiME-5 challenge is $72.8$ in binaural and $91.7$ in single array conditions. Here our approaches enhance monaural speech, a more challenging condition. Our proposed approach outperforms other comparison models in terms of SI-SDR with a $5.29$ average improvement compared to others. According to PESQ and ESTOI metrics, MetricGAN variants give the best performace, however, proposed model's performance is $0.02$  and $ 0.015$ lower according to PESQ and ESTOI, respectively, for the best performing MetricGAN models. Hence, our proposed approach is effective on out-of-vocabulary scenario trained by a comparable dataset.


% \nayem{*** Possibly add graphs of evaluation metrics vs SNRs.}

\input{table_dnsmos.tex}

\subsection{Perceptual quality evaluation}
\label{subsec:dnsmos}

We finally evaluate our model using P.835 metric~\cite{reddy2022dnsmos} to measure perceptual quality. We calculate the DNSMOS score on a scale of $[1-5]$ ($1$ = worst, $5$ = best) for the mixture, PMOS+SE, MetricGAN, and our proposed models using the CHiME-4~\cite{vincent2017analysis} and CHiME-5~\cite{barker2018fifth} datasets (simulated and real-recording). Figure~\ref{fig:dnsmos_results} shows the scores. With CHiME-4, the original mixture scores range from $1.45$ to $2.5$ with a median of $1.74$. Our proposed model achieves a median MOS of $2.46$, which is higher than the others. Fon CHiME-5, the original mixture scores range from $1.0$ to $4.18$. Our proposed model outperforms the others with a median of $2.25$. Our proposed model and PMOS+SE have smaller standard deviations compared to MetricGAN. Overall, our proposed model improves noisy speech in both the acoustic and perceptual aspects. 




% \subsection{Listening results}
% \label{subsec:listening_results}

% We conduct an IRB-approved listening study using Amazon Mechanical Turk to conceive the perceptual quality of enhanced speech assessed by normal-hearing listeners. 

% This study follows the design structure of \cite{nayem2021towards} and figure~\ref{fig:survey} shows the actual listener study interface of a single question. The study is conducted as follows, the participant will listen to two audio signals, one is enhanced and the other is clean audio as reference.  Then they provide a preference score using a Likert scale. The scale ranges from $-3$ to $+3$, where $-3$ refers to a strong preference towards the first signal, $+3$ refers to a strong preference towards the second signal, and $0$ refers to no preference. Before providing a score, the participant can listen to the signals as many as times they like, where the scores are not limited to integer values. The two signals are randomly selected, and the participant listens to different audio clips in each question. The audio clips are chosen from the CHiME-5 and CHiME-4 corpus spoken by both males and females in equal proportion. Prior to actual survey questions, each participants has to pass eligibility test and make themselves familiar with the upcoming study session by going through a practice session. The structure of this practice session is similar to the actual study, however, speakers' voice and audio clips which participants hear in practice session are not used in the actual study. A tentative feedback is provided in the practice session to give a guideline to the participants, however, to avoid biases and leading answers, the feedback is provided in a form of range where the expected answer should reside.



%  \begin{figure}[thb!]
%     \centering
%     \includegraphics[width = 0.5\linewidth]{IEEEtran/figs/survey.png}
%     % \vspace{-2em}
%     \caption{A question of actual listener study interface conducted on MTurk.}
%     \label{fig:survey}
%     % \vspace{-2em}
% \end{figure}

% \nayem{***One paragraph on the statistics of the conducted study.}
% The study session contains total 30 questions, which is preceded by a practice session of 7 questions. Ten participants (9 male, 1 female) who are native English speakers over the age of 18 participated, where a headset/headphone was required to be worn. On average, participants took 14 minutes to complete the study, they were given $\$3$ monetary incentive.


\section{Discussion}
\label{sec:discuss}

Our proposed model outperforms all comparison models on SI-SDR metrics for both seen and unseen datasets, without optimization of any of the models (Table \ref{tab:results_cosineVoices}, \ref{tab:results_chime}). This means that our approach improves speech quality by minimizing the distortion ratio when separated from the noise component. Additionally, our models yield the best MOS-LQO ratings on real-world captured audios (CHiME datasets, Table \ref{tab:results_chime}). These results are consistent with the findings of \cite{zezario2022deep, nayem2021incorporating} that incorporating embeddings from a speech assessment model improves SE performance, and the results of \cite{braun2022effect} that using MOS loss during model optimization leads to higher MOS-LQO scores. Our proposed approach achieves PESQ and ESTOI scores that are only slightly lower than those of the best-performing model, with a difference of only $0.03$ and $0.01$, respectively. This indicates that speech quality and intelligibility metrics are closely related to the subjective speech quality metric (MOS-LQO), and that these metrics can be improved without explicit optimization. Furthermore, our proposed model achieves the best average DNSMOS scores with low standard deviations on CHiME datasets (Figure \ref{fig:dnsmos_results}), indicating that it is effective in a wide range of real-world noise levels. This is a desirable quality for an effective SE model to be effective not only in high SNRs and limited noisy environments, but also in large SNR ranges and real-world conditions such as those offered by the CHiME dataset.

When comparing our proposed model that uses mse+sa+mos loss to the PMOS+SE model (as shown in Table \ref{tab:results_chime}), we can observe significant improvements in all performance metrics. As both models use the same loss function, the improvements are attributed to the incorporation of LM and the joint learning method. Moreover, we found that these two models exhibit similar performance on the MOS prediction (Table \ref{tab:mos_results}), indicating that the benefits of joint learning mostly impact the enhancement part of the model.

An intriguing finding is that our proposed model shows a decline in WER\% when MOS loss is incorporated, especially for larger real-world recordings such as CHiME-5, with degradation up to $1.1$. Although our study is not primarily concerned with ASR performance, this suggests a potential trade-off between ASR accuracy and subjective speech quality scores. Further investigation is needed to comprehend this relationship.

Our proposed method demonstrates that training a speech enhancement (SE) model and a MOS-based speech assessment model jointly can lead to better speech quality measured by objective metrics such as perceptual quality, intelligibility, and MOS ratings. However, we acknowledge that our study's use of subjective MOS (sMOS) estimation instead of actual human listeners may introduce discrepancies between MOS-LQO and human-rated MOS, which could impact our findings. To address this limitation, we plan to conduct sMOS evaluation by human listeners in future work. Although we used the same MOS prediction model for all comparison models, we believe that incorporating human-rated sMOS evaluations will provide more robust insights into our proposed method's effectiveness.
For computing loss terms, we opt for the MSE loss function along with a bi-gram language model that considers only time-along transitions. Our aim is to keep the model simple and focus on the effectiveness of our approach. However, we acknowledge that using different loss functions for different loss components and employing a more complex language model that considers both temporal and spectral transition levels can be beneficial. We plan to explore these possibilities in our future work.


