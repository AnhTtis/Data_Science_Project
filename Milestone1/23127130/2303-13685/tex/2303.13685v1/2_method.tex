%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%        2. Proposed Approach         %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proposed Approach}
\label{sec:methods}

A depiction of our approach is shown in Figure \ref{fig:model}. The model consists of a MOS prediction model (shown left) and a speech enhancement model (shown right). Our MOS prediction model is tailored to provide estimates for subjective-MOS (as rated by humans), and going forward, we will use MOS to refer to subjective-MOS unless explicitly stated otherwise, for ease of understanding. We next will provide notation and then describe each of these sub-modules.

\subsection{Notation}

We define a clean speech signal as $s_t$ and background noise as $n_t$ at time $t$. The mixture of clean speech and noise is denoted as $m_t=s_t+n_t$. We aim to extract the speech from the mixture by removing the unwanted noise. The short-time Fourier transform (STFT) converts the time-domain mixture into a T-F representation, $M_{t,f}$, that is defined at time $t$ and frequency $f$. The complex-valued STFT matrix, $\bm{M}$, can be written as $\bm{M}=|\bm{M}|e^{i\bm{\theta}^M}$ with magnitude $|\bm{M}|\in \bm{\Re}^{T\times F}_+$ and phase $\bm{\theta}^M \in \bm{\Re}^{T\times F}$, where $T$ is the length of speech in time and $F$ is the total number of frequency channels.

Enhancing the magnitude response of noisy speech results in an estimate of the clean speech magnitude response, $|\hat{\bm{S}}|$, using an enhancement function $\mathcal{F}_\delta$ such that $|\hat{\bm{S}}| =\mathcal{F}_\delta(|\bm{M}|)$. The enhancement function is modeled with a deep neural network which is trained to maximize the conditional log-likelihood of the training dataset, 
\begin{align*}
    &\max \frac{1}{N} \sum^N \log P\Big( |{\bm{S}}| \, \Big| \, |\bm{M}|\Big) \\
    \Rightarrow &\max_\delta \frac{1}{N} \sum^N \log P\Big( \mathcal{F}_\delta(|\bm{M}|) \, \Big| \, |\bm{M}|\Big)
\end{align*}
% $$\max \frac{1}{N} \sum^N \log P\Big( |{\bm{S}}| \, \Big| \, |\bm{M}|\Big) \Rightarrow \max_\delta \frac{1}{N} \sum^N \log P\Big( |\hat{\bm{S}}| \, \Big| \, |\bm{M}|\Big) $$
where $\delta$ denotes the set of tunable parameters and $N$ is the number of training examples. The estimated magnitude response $|\hat{\bm{S}}|$ is then combined with the noisy phase, $\bm{\theta}^M$, where the inverse STFT produces an enhanced speech signal in the time domain, $\hat{s}_t$. 

\subsection{Speech quality assessment model}
\label{subsec:mos_model}

A MOS prediction model proposed by \cite{dong2020pyramid} is adapted to estimate the MOS from noisy speech. This model has been developed with real-world captured data and it has been shown to outperform comparison approaches~\cite{fu2018quality, avila2019non, mittag2019non}, according to multiple metrics. The MOS prediction model consists of an attention-based encoder-decoder structure that uses stacked pyramid bi-directional long-short term memory (pBLSTM)~\cite{chan2016listen} networks in the encoder. We denote this model as Pyramid-MOS (PMOS). A pBLSTM architecture gives the advantages of processing sequences at multiple time resolutions, which effectively captures short- and long-term dependencies. Speech has spectral and temporal dependencies over short and long durations, and a multi-resolution framework is effective in learning these complex relations. 


A single T-F frame of the noisy-speech mixture, $|\bm{M}_t|$, is the input to the PMOS encoder. In a pyramid structure, the lower layer outputs from $\Upsilon$ consecutive time frames are concatenated and used as inputs to the next pBLSTM layer, along with the recurrent hidden states from the previous time step. The output of a pBLSTM node is an embedding vector, $h^l_t$, that is as defined below:
\begin{align}
    h^l_t &= pBLSTM\Big( h^l_{t-1}, \big[ h^{l-1}_{\Upsilon\times t -\Upsilon+1}, h^{l-1}_{\Upsilon\times t}\big] \Big)
\end{align}
where $\Upsilon$ is the reduction factor (e.g., number of concatenated frames) between successive pBLSTM layers and $l$ is the layer number. A pBLSTM reduces the time resolution from the input speech to the final latent representation $\bm{H}$. Figure~\ref{fig:pBLSTM} shows the internal structure of pBLSTM module.
This compressed vector accumulates the useful features for measuring speech perceptual quality that resides in a range of time-frames and ignores the least important features.
The encoder outputs a concatenated version of the hidden states of the last pBLSTM layer as vector $\bm{H}=\{\bm{h}_1, \dotsb, \bm{h}_\tau, \dotsb, \bm{h}_\wp\}$, where $\wp$ is the total number of final embedding vectors with time index $\tau$.

The output of the PMOS encoder becomes the input to the PMOS decoder unit. This decoder is implemented as an attention layer followed by a fully-connected (FC) layer and it outputs an estimated MOS of the input speech utterance. Attention models learn key attributes of a latent sequence, since adjacent time frames can provide important information, which is particularly crucial for our task.  
The attention mechanism~\cite{luong2015effective} uses the pyramid encoder output at the $i$-th and $k$-th time steps to compute the attention weights, $\alpha^{PMOS}_{i,k}$. Attention weights are used to compute a context vector, $c^{PMOS}_i$, using the following equations:
\begin{align}
    \alpha^{PMOS}_{i,k} &= \frac{\exp{(\bm{h}_i^\top \bm{Q} \bm{h}_k)}}{\sum^{\wp}_{\phi=1} \exp{(\bm{h}_i^\top \bm{Q} \bm{h}_\phi)}}\\
    % \alpha^{PMOS}_{i,k} &= Attention(\bm{h}_i, \bm{h}_k)\\
    c^{PMOS}_i &= \sum^\wp_{k=1} \alpha^{PMOS}_{i,k} \cdot \bm{h}_k
\end{align}
$\bm{Q}^{\wp\times\wp}$ is the trainable PMOS attention weight matrix. We learn $\bm{Q}$ using a feed-forward neural network that attempts to capture the alignment between the embeddings $\bm{h}_i$ and $\bm{h}_k$. 

The context vector is provided to a fully-connected layer to estimate the MOS. Note that the pyramid structure of the encoder results in a shorter sequence of latent representations than the original input sequence, and it leads to fewer encoding states for attention calculation at the decoding stage. Therefore, strictly  $\wp<T$, and in our case $\wp = \lceil T/\Upsilon^L \rceil$, where $L$ is the number of pBLSTM layers.
We train the PMOS model separately with the parameters defined in~\cite{nayem2019incorporating}. After training, this model is held frozen during inference.

\begin{figure}[t!]
    \centering
    \includegraphics[width = 0.95\linewidth]{figs/pBLSTM.png}
    \caption{Illustration of pBLSTM structure with reduction factor $\Upsilon=2$ and number of layer $L=2$.}
    % \vspace{-2em}
    \label{fig:pBLSTM}
    % \vspace{-0.4cm}
\end{figure}

\subsection{Proposed speech enhancement model}
\label{subsec:se_model}
Our proposed speech-enhancement (SE) model follows an encoder-decoder structure, and it is shown in Figure \ref{fig:model} (right). The SE encoder takes a single T-F frame of a noisy-speech mixture, $|\bm{M}_t|$, as input and multiple BLSTM layers, are stacked together to create a hidden representation of the frame, $\bm{g}_t$. In our SE encoder, we utilize BLSTM layers instead of pBLSTM layers since we aim to estimate an embedding frame for each time frame and pBLSTM layers reduce the number of output frames. 
An attention mechanism is applied using the mixture encoding from the SE model, $\bm{G}=\{\bm{g}_1, \bm{g}_2, \dotsb, \bm{g}_T\}$, and the PMOS encoding, $\bm{H}$, from the MOS prediction model. This allows the SE model to exploit the MOS estimator's encoding and utilize the important perceptual feature embedding that correlates with human assessment. Considering that the pBLSTM structure of the PMOS encoder condenses the final encoding vector $\bm{H}$ along time, PMOS yields a smaller time resolution than the encoding from the SE encoder, so we compute a score for each embedding vector $\bm{h}_{\tau}$ using an alignment  weight matrix, $\bm{W}^{T\times\wp}$. Then the attention weights for the SE model, $\alpha_{t,\tau}$, are obtained using a softmax operation over the scores of all $\bm{h}_\tau$. Now, the PMOS encoding is summarized in a context vector $\bm{c}_t$ for each mixture frame $\bm{g}_t$. Prior to computing $\bm{c}_t$, $\bm{h}_\tau$ passes through a linear layer $\ell$, so that we learn a different representation for the SE task. The computations are below:
\begin{align}
    \alpha_{t,\tau} &= \frac{\exp{(\bm{g}_t^\top \bm{W} \bm{h}_\tau})}{\sum^{\wp}_{\phi=1} \exp{(\bm{g}_t^\top \bm{W} \bm{h}_\phi)}} \\
    \bm{c}_t &= \sum_{\tau=1}^\wp \alpha_{t,\tau} \cdot \ell (\bm{h}_\tau)
\end{align}
\noindent
Then, the context vector and SE-model embedding vector are concatenated (e.g., $[\bm{c}_t, \bm{g}_t]$) and passed to the decoder module. The SE-decoder module follows the network structure from \cite{schulze2020joint}. It consists of a linear layer with a $tanh(\cdot)$ activation function, two BLSTM layers, and a linear layer with ReLU activation. It outputs the estimated enhanced speech $|\hat{\bm{S}}|$. This estimated speech magnitude with noisy phase produce the estimated clean speech, i.e. $\hat{\bm{S}} = |\hat{\bm{S}}|e^{i\bm{\theta}^M}$. Since we are estimating two targets MOS and enhanced speech simultaneously, the unified model will learn different representations for these tasks. Thus both PMOS and SE models will learn their corresponding targets with perceptual feature sharing. We freeze the PMOS model while training this SE model.


\subsection{Joint-learning of PMOS and SE model}
\label{subsec:joint_model}
We also develop an approach that allows the PMOS and SE models to be jointly trained. Our joint-learning objective function uses a weighted average of a {time-domain} signal-approximation loss $\mathcal{L}_{sa}$ (from the SE model), the MSE of the magnitude spectrum $\mathcal{L}_{mse}$ (from the SE model) and the MSE of the MOS estimation $\mathcal{L}_{mos}$ (from the PMOS model). We compute the signal-approximation loss from the time-domain signal difference between the reference speech $s$ and enhanced speech $\hat{s}$. The overall loss function of our network is defined as below, using hyper-parameters $\lambda_1$ and $\lambda_2$ that control the impact of individual loss terms:
\begin{align}
    \mathcal{L} &= \lambda_1\left[\lambda_2\mathcal{L}_{mse} + (1-\lambda_2)\mathcal{L}_{sa}\right] + (1-\lambda_1)\mathcal{L}_{mos}
    \label{eq:loss}
\end{align}
\noindent
The model training order is as such. First, we train the PMOS model using $\mathcal{L}_{mos}$ (e.g. $\lambda_1 = 0$). Then we train the SE model using $\lambda_1 = 1$, while running the PMOS model in inference mode (e.g. it is held fixed). This is done to ensure that the trained PMOS model effectively encodes the key features in the embedding vector that are important to perceptual speech quality. Finally, we train both the models jointly (e.g. $0 < \lambda_1 < 1$) using $\mathcal{L}$ to further reduce any correctional differences between the true and estimated MOS in the PMOS model, and to increase the perceptual quality of the enhanced speech.
\begin{figure}[t!]
    \centering
    \includegraphics[width = 0.8\linewidth]{figs/quant_fig2.png}
    \caption{Quantization of a clean magnitude spectrum.}
    % \vspace{-2em}
    \label{fig:quant}
    % \vspace{-0.4cm}
\end{figure}
\subsection{Quantized Spectral Model}
\label{subsec:QSM}
% An external language model can integrate additional information regarding speech correlation which is helpful for improving enhancement performance. Typical LM is applied at the phoneme or word level and the performance of the LM depends on the text and its vocabulary. Additionally, parallel corpus of speech and text is a requirement for training which rules out a huge number of corpus from usage. 
%A language model (LM) serves as prior knowledge on acoustic input that constrains the alternative word (or phonetic) hypothesis during speech recognition by learning which sequences of words (or phonetics) are most likely to be spoken. LM predicts which words will follow on from the current words and with what probability. $\mathbb{N}$-gram LM is a widely used approach which estimates the probability of a given sequence of words $w_{1\cdots\Omega}$ within the assumption that the probability of word $w_\delta$ depends only on previous $(\mathbb{N}-1)$ words, and the probability can be expressed as: 
%\begin{align}
%    P(w_{1\cdots\Omega})=\prod_{w_\delta} P(w_\delta|w_{\delta-1}, w_{\delta-2},\cdots,w_{\delta-\mathbb{N}+1})
%\end{align}
%Compared to conventional ASR approaches, deep ASR systems model learn in-house LM \cite{yu2016automatic}; and they can be coupled with SE task~\cite{weninger2015speech, wang2020complex}. LM helps a SE model by predicting probability of next utterance, which is otherwise will be any utterance in the whole speech spectrum. However, deep LM typically require more data to achieve comparable results. Additionally, parallel corpus of speech and text is a requirement for training which rules out a huge number of raw audio collections from usage.
%Therefore, we adapt an alternative view of a LM from \cite{nayem2021towards}, where quantized t-f values are considered as word. 
From written and spoken language, we can determine the sequences of words that are most likely to occur. This knowledge is captured by a language model (LM) of an automatic speech recognition system which we can expressed as,
\begin{align}
    \hat{words}=\argmax_{words\in Language} \overbrace{P(input|words)}^{acoustic\;model} \overbrace{P(words)}^{language\;model}
\end{align}
%Here, the most likely word sequence, $\hat{words}$, is estimated by an acoustic model that calculates the probability of the input audio given the word sequence $words$, and by a language model that gives the likelihood of the word sequence. Hence, the LM predicts the probability of a sequence of words. 
The LM is useful in eliminating rare and grammatically incorrect word sequences, and it enhances the performance of ASR systems. In the case of speech enhancement, models learn spectral information within frames over time, but they often neglect the temporal correlations. Our approach, as proposed in \cite{nayem2021towards}, suggests incorporating a ``LM" to fuse temporal correlations and overcome this limitation. Therefore, we construct a bi-gram Quantized Spectral Model (QSM), which functions in a similar way to a language model (LM), in order to produce more realistic spectra. The QSM estimates the probability of spectral magnitudes along time for each frequency channel conditioned on its previous T-F spectral magnitude. %Range of T-F unit values is constrained in a signal approximating SE system and is far smaller than typical spoken language vocabulary size. As a result, the training time and computational resource requirement are quite small fo spectral LM.
On a reference speech corpora, we apply a normalization scaling function, $\mathcal{N}_{[o,r]}(\cdot)$, that normalizes the magnitude spectrogram and re-scales the range to $[0,r]$. Then a quantization function, $\mathcal{Q}_\chi(\cdot)$, converts the range constrained magnitude spectrogram into $\mathcal{D}$ number of bins that are $\chi$ steps apart. This produces quantized speech, i.e. $|S|^q = \mathcal{Q}_\chi\big(\mathcal{N}_{[0,r]}(|S|)\big)$. Fig.~\ref{fig:quant} shows an example of the original clean and quantized clean magnitude spectra, where $\chi=2$ for display purposes. Our proposed QSM has $\mathcal{D}$ spectral levels. We construct the QSM using quantized speech magnitudes from the clean speech corpora. The QSM is less likely to suffer from the out of vocabulary problem when the model parameters, $\chi$ and $r$, are adequately defined.

%\begin{figure}[tbh!]
%    \centering
%    \includegraphics[width = \linewidth]{IEEEtran/figs/fQSM.png}
%    \caption{Proposed Quantized Spectral Models (QSMs) for per-frequency-channel.}
%    % \vspace{-2em}
%    \label{fig:fQSM}
%    % \vspace{-0.4cm}
%\end{figure}

We compute per-frequency-channel QSMs along the time axis where each entry, $d$, refers to a quantization attenuation level. We then compute the transition probability between two time consecutive T-F units, $fQSM_f = P(d_{t+1,f}|d_{t,f})$. The probabilities are calculated by counting the level transitions, and then normalizing by the appropriate scalar. These probabilities are stored in the per-frequency-channel QSM resulting in a $F\times \mathcal{D}\times \mathcal{D}$ probability matrix. %Figure~\ref{fig:fQSM} shows proposed QSMs along per-frequency-channel. 
We re-evaluate the transition probabilities using Good-Turing smoothing~\cite{jurafskyMartin2009} to overcome the zero-probability problem in N-grams. Shallow fusion~\cite{gulcehre2015using} is a simple method to incorporate an external LM into an encoder-decoder model, and it produces better results compared to others. Hence, we use shallow fushion to combine our QSM and SE model based on log-linear interpolations at inference time. This is shown in the below equations:

\begin{align}
    P^{QSM}_f(|\hat{\bm{S}}_{:,f}|) &= \prod^T_{i=1} P(d_{i,f}|d_{i-1,f}) \\
    |\hat{\bm{S}}_{:,f}|^* = \argmax_{|\hat{\bm{S}}_{:,f}|} &\log P\big(|\hat{\bm{S}}_{:,f}| \big| |\bm{M}|\big) + \mu \log P^{QSM}_f\big(|\hat{\bm{S}}_{:,f}| \big)
    \label{eq:S_hat}
\end{align}
\noindent
Here $P^{QSM}_f$ denotes the transitional probability of QSM at frequency $f$, $P\big(|\hat{\bm{S}}_{:,f}| \big| |\bm{M}|\big)$ represents the estimated magnitude output of the LSTM layers of the SE decoder, and $\mu$ is a hyper-parameter that is tuned to maximize the performance on a development set. Note that we train our QSM in advance on a clean speech corpus and use it in inference mode during enhancement. The tunable parameter $\mu$ of (\ref{eq:S_hat}) is set to zero when we do not have a trained QSM. 




