%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%            1. Intro             %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ifCLASSOPTIONcompsoc
\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
\else
\section{Introduction}
\label{sec:introduction}
\fi

\IEEEPARstart{M}{onaural} speech enhancement aims to remove unwanted noise from an audio signal that contains speech using only a single microphone channel. Enhancing the quality of noisy speech is crucial for applications such as speech recognition, speaker verification, hearing aids, and hands-free communication. Speech enhancement approaches are generally divided into two categories: mask-based or signal-based approximation. A time-frequency (T-F) mask is estimated in mask-based approaches, where the mask filters unwanted noise from noisy speech mixtures. Early mask-based approaches estimate the ideal binary mask (IBM)~\cite{li2008factors} or the ideal ratio mask (IRM)~\cite{narayanan2013ideal}, while recent approaches estimate the phase-sensitive mask (PSM)~\cite{erdogan2015phase} or complex ideal ratio mask (cIRM)~\cite{williamson2015complex, lee2019joint} to enhance both the magnitude and phase. The ideal quantized mask (IQM) has recently been proposed~\cite{healy2018ideal}, where each T-F unit of the IRM is assigned to a quantization level according to its signal-to-noise ratio. It has been shown to be a reasonable representation of the IRM as assessed by human listeners, however, estimation of the IQM and its subsequent noise removal has not be thoroughly evaluated. 

Signal approximation can be done in either the time~\cite{luo2018tasnet, pandey2019new} or the T-F domains~\cite{odelowo2018study}, where the approach directly estimates the time or T-F domain signal from a noisy speech representation. Traditionally, T-F masks produce better objective quality and intelligibility compared to direct signal approximation, mainly because masks are normalized and bounded with limited speaker variations, which makes them easier to learn. Also, masks directly modulate the mixture signal in the T-F domain. In recent years, the signal approximation models outperform mask estimation approaches in speech intelligibility~\cite{odelowo2018study, lu2022conditional} when applied with appropriate normalization.

Regardless of the approach, recent developments in deep learning have resulted in state-of-the-art performance. A wide range of deep learning architectures have been proposed, including, deep neural networks (DNNs)~\cite{xu2013experimental, wang2014training}, autoencoders~\cite{xia2013speech,lu2014ensemble,lee2016two}, long short-term memory (LSTM) networks~\cite{weninger2014discriminatively,erdogan2015phase}, convolutional neural networks (CNNs)~\cite{zhao2018convolutional, tan2018convolutional, choi2018phase, pandey2019new, kolbaek2020loss}, and generative adversarial networks (GANs)~\cite{pascual2017segan, donahue2018exploring, fu2019metricGAN}. Deep recurrent networks have proven to be effective, especially compared to fully-connected DNNs, as they capture temporal correlations. %In \cite{xia2020weighted}, a weighted loss is proposed and the subjective study finds that a low noise suppression setting achieves better human acceptance. %In \cite{nayem2019incorporating, nayem2020monaural}, a novel intra-spectral layer is proposed that captures adjacent spectral relations in a time-frame, and they apply it on both magnitude and group-delay enhancement. \cite{nayem2021towards} shows quantized speech estimation in an end-to-end manner using a LSTM network. An automatic speech quality assessment model is jointly trained with a speech enhancement model in \cite{nayem2021incorporating}, and they achieve significant gains in both objective measures and human perceptual evaluation metrics of speech quality. 
CNNs are good at feature extraction, and they have been combined with recurrent networks to capture the short and long-term temporal and spectral correlations. Recently, attention-based deep architectures have been proposed with the motivation that a training target only greatly influences a few regions of the input, where the focal regions change over time. \cite{giri2019attention, tolooshams2020channel} use attention mechanism with an U-Net~\cite{ronneberger2015u} architecture for both time and spectral domain speech enhancement. \cite{hao2019attention, koizumi2020speech} successfully use self-attention to estimate a speech spectrum and T-F mask, respectively. This approach is more intuitive for speech enhancement, because humans are able to focus on the target speech with high attention while paying less attention to the noise. %In attention-based speech enhancement model, BLSTM layer is also incorporated as in self-adaptive model~\cite{koizumi2020speech}. 
 
Deep-learning-based speech enhancement approaches traditionally use the mean square error (MSE) between the short-time spectral-amplitudes (STSA) of the estimated and clean speech signals to optimize performance. This is done due to the computational efficiency of the MSE loss function. However, the MSE tends to produce overly-smoothed speech and it is not always a strong indicator of performance~\cite{wang2009mean, shu2020human}. Thus, many studies have begun to optimize algorithms using perceptually-inspired objective measures. 
 
Multiple studies have used short-time objective intelligibility (STOI)~\cite{taal2011algorithm} to optimize enhancement algorithms and to improve speech intelligibility~\cite{zhang2018training, fu2018end, koizumi2018dnn}. This is done to minimize the inconsistency between the model optimization criterion and the evaluation criterion for the enhanced speech. The reported results in \cite{fu2018end} show that jointly optimizing with STOI and MSE improves speech intelligibility according to both objective and subjective measures. In addition, word accuracy according to automatic speech recognition (ASR) is improved. Perceptual evaluation of speech quality (PESQ)~\cite{rix2001perceptual} scores, however, have not increased when optimizing with STOI, as reported in~\cite{fu2018end}. The signal-to-distortion ratio (SDR)~\cite{fevotte2005bss_eval} has also been used as an objective cost function~\cite{kawanaka2020stable}. The proposed network is pre-trained with a SDR loss to achieve network stability and later optimized with a PESQ loss in a black-box manner. Their results show that optimizing with SDR leads to overall objective quality improvements. Unlike SDR and STOI, PESQ cannot directly be used as an objective function since it is non-differential. Reinforcement learning (RL) techniques such as deep Q-network and policy gradient have thus been employed to solve the non-differentiable problem \cite{koizumi2018dnn, koizumi2017dnn}. In these works, PESQ and the perceptual evaluation methods for audio source separation (PEASS)~\cite{emiya2011subjective, vincent2012improved} serve as rewards that are used to optimize model parameters. Meanwhile, a new PESQ-inspired objective function that considers symmetrical and asymmetrical disturbances of speech signals has been developed in~\cite{martin2018deep}. Quality-Net~\cite{fu2018quality}, which is a DNN approach that estimates PESQ scores given a noisy utterance, has also been used as a maximization criteria~\cite{fu2019learning} and as a model selection parameter~\cite{zezario2019specialized} to enhance speech. 

It is worth noting that optimizing with perceptually-inspired objective measures has been disputed in \cite{kolbaek2018monaural, kolbaek2018relationship}, where these latter results show that a MSE objective function is sufficient. This may occur because objective measures of success do not always strongly correlate with subjective measures~\cite{emiya2011subjective, rix2006objective,cano2016evaluation, santos2014improved}. Hence, it is inconclusive as to whether perceptually-inspired objective measures are generally useful at optimizing speech enhancement performance, so alternative strategies for incorporating perceptual feedback may be needed.

Subjective evaluations from human listeners remains the gold-standard approach since it results in ratings from potential end-users. These evaluations often ask listeners to either give relative preference scores~\cite{quackenbush1988objective} or assign a numerical rating~\cite{malfait2006p}. Multiple ratings are provided for each signal, where they are averaged to generate a mean-opinion score (MOS). Recently, deep-learning approaches have effectively estimated human-assessed MOS~\cite{avila2019non, patton2016automos, lo2019mosnet, dong2020attention}. These approaches are promising since they can provide strongly correlated quality scores for new signals. According to \cite{braun2022effect}, a non-intrusive loss function can lead to improved noise suppression. Conversely, \cite{zezario2022deep} proposes using embedding vectors from a multi-objective speech assessment model for speech enhancement, but they only use intrusive metrics such as PESQ, STOI, and a speech distortion index (SDI) to train the speech assessment model. As a result, it remains unclear whether a speech assessment model that predicts MOS can incorporate human perceptual information into a speech enhancement model.
% To the best of our knowledge, these models, however, have not been leveraged for speech enhancement. 
%Nevertheless, they do not retrain or jointly train the Quality-Net model with the enhancement model, hence, the estimation error of the assessment model will influence the speech enhancement task adversely.
%They treat Quality-Net as a black box, therefore, correlation differences between estimate MOS and true MOS sustain in speech enhancement tasks. Multiple subjective speech quality measures have been developed, including Quality measures like is preference test~\cite{rothauser1968isopreference}, Diagnostic acceptability measure (DAM)~\cite{voiers1977diagnostic}, speech to reverberation modulation energy ratio (SRMR)~\cite{falk2010non}, and most commonly used Mean Opinion Score (MOS) which is standardized by ITU-T standard P.563~\cite{malfait2006p} and ANSI standard ANIQUE+~\cite{kim2007anique+}. However, in the task of speech enhancement, usage of a human-scored dataset is not very common because they are expensive and time-consuming to produce.

Joint learning has been successfully applied in speech enhancement to optimize between estimating speech and other training targets, such as phoneme classification~\cite{schulze2020joint}, speaker identification~\cite{ji2020speaker}, and speech recognition~\cite{donahue2018exploring}. Our preliminary work has recently combined a speech quality estimation task with speech enhancement~\cite{nayem2021incorporating} and it shows promising results. In this work, we propose an attention-based speech enhancement model that uses the embedding vector from a MOS prediction model to produce speech with improved perceptual quality. The MOS estimator generates encoded embedding vectors that contain perceptually useful information that is important for human-based assessment. Our speech enhancement attention model is conditioned on that embedding vector and enhances the noisy speech using a separate encoder-decoder framework, which should help produce better quality speech according to human evaluation.  In the enhancement stage, we incorporate a quantized spectral language model that captures the transitions probabilities across the T-F spectrum. The LM helps ensure that the resulting speech spectra exhibit realistic spectral- and temporal-fine structure that occurs within real speech signals, since it identifies the most likely spectrum in each time frame. This is accomplished by first quantizing the speech magnitude spectra into distinct classes. Our proposed signal approximation approach jointly updates both the MOS-prediction and speech-enhancement models during training, using speech enhancement and MOS prediction loss terms.

 
% Comment out for now (11/16/2021), but we may add it back later. Move to later in introduction
%Our recent work strives to enforce spectral-fine structure by incorporating an intra-spectral recurrence layer~\cite{nayem2019incorporating,  nayem2020monaural}, but they do not address temporal-fine structure and do not ensure that human-like spectra are generated. %Automatic speech recognition (ASR) helps ensure that realistic text transcriptions are generated by applying language models on top of DNN-based acoustic models~\cite{graves2014towards}. Recently, a quantized language model (LM) has been proposed~\cite{nayem2021towards} where magnitude spectrogram has been quantized to form spectral LM. This quantized LM is used to estimate quantized enhanced speech and their results show significant gain over classic enhancement estimation with ease of problem formulation.

% Comment out for now (11/16/2021), but we may add it back later. Move to later in introduction
%Despite the improvements, speech enhancement approaches still struggle to perform adequately in real-world scenarios, where the speech may differ from the examples seen during training. This partially occurs because the metrics that are used to optimize and assess performance do not fully correlate with perceptual quality as assessed by human observers, and most of the evaluations are done using simulated noisy speech data that does not reflect the intricacies and nuances of the real world.

\input{fig_model.tex}

The rest of the paper is organized as follows. In section~\ref{sec:methods}, we introduce the quality assessment model, the proposed enhancement model, and the quantized spectral language model. We describe our dataset and experimental setup in section~\ref{sec:experiments}. In section~\ref{sec:results}, we evaluate our proposed approach and compare it with other state-of-the-art models. We discuss the implication and significance of our work in section~\ref{sec:discuss}. Finally, we conclude our work in section~\ref{sec:conclusion}.
