%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%        3. Experiments         %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{sec:experiments}


\subsection{Dataset}
\label{subsec:dataset}

We use the COnversational Speech In Noisy Environments (COSINE)~\cite{stupakov2009cosine} and the Voices Obscured in Complex Environmental Settings (VOiCES)~\cite{richey2018voices} corpora. 
COSINE captures multi-party conversations on open-ended topics for spontaneous and natural dialogue. These conversations are recorded in real world environments in a variety of background settings. The audio recordings are captured using 7-channel wearable microphones that consist of a close-talking mic (e.g., near the mouth, clean reference), far-field mic (on the shoulder), throat mic, and an array of four mics (spaced 3 cm apart) positioned in front of the speaker's chest. In total, 133 English speakers record 150 hours of audio with the approximated signal-to-noise ratios (SNR) ranging from -10.1 to 11.4 dB.

VOiCES contains audio recorded using 12 microphones placed throughout real rooms of different size and acoustic properties. Various background noises like TV, music, or babble are simultaneously played with foreground clean speech, so the recordings contain noise and reverberation. A foreground loudspeaker moves through the rooms during recording to imitate human conversation. This foreground speech is used as the reference clean signal, and the audio captured from the microphones is used as the reverberant-noisy speech. The approximate speech-to-reverberation ratios (SRRs) of the VOiCES signals range from -4.9 to 4.3 dB. 

% A noisy signal from either shoulder or chest is aligned with the reference and anchor signals, and then evaluated by human listeners using the MUSHRA protocol.

The MOS data was collected from a listening study in \cite{dong2020pyramid}. Listeners assessed the speech quality of audio signals using a 100-point scale. In total, 45 hours of speech and 180k subjective human ratings are summarized into the MOS quality ratings for 18000 COSINE signals and 18000 VOiCES signals. The collected responses are processed further to mitigate rating biases~\cite{zielinski2008some}, remove responses that were unanswered or randomly scored~\cite{gadiraju2015understanding}, and to deal with outliers~\cite{ester1996density, liu2008isolation}. Z-score pruning~\cite{han2011data} followed by min-max normalization is performed, resulting in a MOS rating scale of 0 to 10.  The scaled ratings for each audio signal are finally averaged. 


We additionally evaluate using the 4th CHiME Speech Separation and Recognition Challenge (CHiME-4)~\cite{vincent2017analysis} and the 5th CHiME Speech Separation and Recognition Challenge (CHiME-5)~\cite{barker2018fifth} corpora. We use these to investigate the generalization capacity of our proposed approach.




\subsection{System Setup}
\label{subsec:setup}

All signals are downsampled to $16$ kHz. Noisy or reverberant stimuli of each dataset are divided into training (70\%), validation (10\%), and testing (20\%) sets, and trained separately.

For MOS prediction, the input signals are segmented into 40 ms length frames with 25\% overlap. A 512-point FFT and a Hanning window are used to compute the spectrogram. Mean and variance normalization are applied to the input feature vector. The PMOS encoder consists of $256$ nodes followed by 3 pBLSTM layers ($L = 3$) with 128, 64 and 32 nodes in each direction, respectively. Like \cite{dong2020pyramid, chan2016listen}, the reduction factor $\Upsilon = 2$ is adopted here. As a result, the final latent representation $\bm{h}_\tau$ is reduced in the time resolution by a factor of $\Upsilon^3 = 8$. The outputs of two successive BLSTM nodes are fed as input to a BLSTM node in the upper layer. 
In the PMOS decoder, the context vector is passed to a fully connected (FC) layer with 32 units. The model is optimized using Adam optimization~\cite{kingma2015adam} with convergence determined by a validation set. Early stopping with initial learning rate of $0.001$ is applied in the training phase.

The proposed SE model uses a 640-point DFT with a Hann window of 40ms and a 20ms frame shift to generate the spectrogram for the encoder input. The SE encoder consists of 2 BLSTM recurrent layers. The SE decoder has a linear layer with $tanh$ activation, followed by 2-layers of BLSTM and a linear layer with ReLU activation~\cite{schulze2020joint, schulze2019weakly}. Each BLSTM layer contains 200 nodes and each linear layer has 321 nodes. The same optimization technique with early stopping by validation set is applied. 

For our proposed QSM language model, we choose a quantization step of $\chi=0.0625$, which was validated by a listening study conducted in \cite{nayem2021towards}.  With parameter $r=100$, the total number of quantization levels, $\mathcal{D}$, is $1600$. The QSM tunable parameter, $\mu$, is set to $0.01$.
