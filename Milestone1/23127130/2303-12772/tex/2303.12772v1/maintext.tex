\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{comment}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

%Bangla Sarcasm Detection with BERT using Stratified K-Fold Cross Validation
\title{Interpretable Bangla Sarcasm Detection using BERT and Explainable AI\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Ramisa Anan}
\IEEEauthorblockA{\textit{Computer Science and Engineering} \\
\textit{BRAC University}\\
Dhaka, Bangladesh \\
ramisa.anan@g.bracu.ac.bd}
\and

\IEEEauthorblockN{Tasnim Sakib Apon}
\IEEEauthorblockA{\textit{Computer Science and Engineering} \\
\textit{BRAC University}\\
Dhaka, Bangladesh \\
sakibapon7@gmail.com}
\and

\IEEEauthorblockN{Zeba Tahsin Hossain}
\IEEEauthorblockA{\textit{Computer Science and Engineering} \\
\textit{BRAC University}\\
Dhaka, Bangladesh \\
zeba2tahsin@gmail.com}
\and

\IEEEauthorblockN{ Elizabeth Antora Modhu}
\IEEEauthorblockA{\textit{Computer Science and Engineering} \\
\textit{BRAC University}\\
Dhaka, Bangladesh \\
elizabeth.antora.modhu@g.bracu.ac.bd}
\and

\IEEEauthorblockN{Sudipta Mondal}
\IEEEauthorblockA{\textit{Computer Science and Engineering} \\
\textit{BRAC University}\\
Dhaka, Bangladesh \\
mondal.sudiptahere@gmail.com}
\and

\IEEEauthorblockN{MD. Golam Rabiul Alam}
\IEEEauthorblockA{\textit{Computer Science and Engineering} \\
\textit{BRAC University}\\
Dhaka, Bangladesh \\
rabiul.alam@bracu.ac.bd}
}

\maketitle

\begin{abstract}
    A positive phrase or a sentence with an underlying negative motive is usually defined as sarcasm that is widely used in today's social media platforms such as Facebook, Twitter, Reddit, etc. In recent times active users in social media platforms are increasing dramatically which raises the need for an automated NLP-based system that can be utilized in various tasks such as determining market demand, sentiment analysis, threat detection, etc. However, since sarcasm usually implies the opposite meaning and its detection is frequently a challenging issue, data meaning extraction through an NLP-based model becomes more complicated. As a result, there has been a lot of study on sarcasm detection in English over the past several years, and there's been a noticeable improvement and yet sarcasm detection in the Bangla language's state remains the same. In this article, we present a BERT-based system  that can achieve 99.60\% while the utilized traditional machine learning algorithms are only capable of achieving 89.93\%. Additionally, we have employed Local Interpretable Model-Agnostic Explanations that introduce explainability to our system. Moreover, we have utilized a newly collected bangla sarcasm dataset, BanglaSarc that was constructed specifically for the evaluation of this study. This dataset consists of fresh records of sarcastic and non-sarcastic comments, the majority of which are acquired from Facebook and YouTube comment sections. 
    %However, sarcasm detection in dialogues has become a challenge in the Natural Language Processing (NLP) field. In this research, we are building a system which can detect sarcasm effectively using Various machine learning model. For the classification task, it utilizes the Machine Learning techniques and RNN (Recurrent Neural Networks) for the extraction features. The main goal is to reduce the amount of time it takes humans to extract sarcasm from reviews and to develop a model that can recognize sarcasm in natural language. 
\end{abstract}

\begin{IEEEkeywords}
Machine Learning, Natural Language Processing, Sarcasm Detection, BERT.

\end{IEEEkeywords}

\section{Introduction}

    %Sarcasm is the use of words that signify the exact opposite of whatever we are trying to imply, usually to insult or mock someone, demonstrate irritation, or just be humorous. The advent of the internet changed the way people connect all around the world, giving the world a new perspective. Now-a-days, sarcasm is  deep-rooted throughout the internet all over the world. We use sarcasm in face to face conversation as well as in the chat box or comment section of social media platforms.
    %Sentiment analysis research has enabled machines to identify whether a text is positive, negative, or neutral with a high degree of accuracy, depending on the dataset. Finding the exact sentiment is incredibly difficult when the current phrase is coated with sarcasm, making it extremely impossible to determine if the line is expressed in a sarcastic manner or not. Hence, the significance of sarcasm detection for sentiment analysis, as well as the problems in identifying it, turn out to be a research problem. on the other hand, is a challenging endeavor as it relies heavily on context along with past information, and the tone with which the line was stated or written.


    Sarcasm is the use of words that mean the complete opposite of what we are attempting to convey and most of the time it is used in order to mock, criticize, or express discontent. Sarcasm is ingrained in today's online culture all around the world and is used often in verbal exchanges in person as well as in chat rooms and comment sections of social networking sites. Automated sarcasm detection algorithms can be beneficial for various NLP applications, including marketing analysis, opinion mining, and information classification. \par
    Sarcasm often alters the polarity of a remark, making its detection and processing crucial in an automated NLP system. In recent times, a  great deal of effort has been put towards automating the sarcasm detection process since it allows for more accurate analytics in online comments and reviews. Sentiment and emotion analysis approaches are being used to study predicting and removing phrases that suggest sarcasm with the growth of artificial intelligence (AI) tools, notably NLP and as a result sarcasm detection in the English language has improved a lot in the past few years. However, for the Bangla language, it still remains a challenging task, and a very limited effort has been made to address this fundamental problem. The primary objectives of our investigation are as follows: (i) Introduce an Automated sarcasm detection system in the Bangla language that can aid in various NLP-based systems such as sentiment analysis, opinion mining, and marketing tools. (ii) Increase the efficiency of the entire process by performing a thorough study in order to find out the optimal framework. While conducting we encountered some challenges. The availability of Bangla sarcasm data was the most notable along with the limitations of previous studies. \par
    
    The contributions of this article are as follows:
    \begin{itemize}
        \item A Bidirectional Encoder Representations from Transformers(BERT) based model with Stratified K-fold Cross Validation has been proposed for detecting sarcasm. A comprehensive study of various traditional machine learning models with the proposed system is presented as well.
        \item Local Interpretable Model-Agnostic Explanations is utilized in order to explore the interpretability of the proposed model.

        \item To assess the effectiveness of this study's proposed model, we have utilized the BanglaSarc dataset, a fresh dataset on sarcasm. This dataset is collected from Facebook, YouTube, and various online resources for the validation of the proposed model specifically. 
        
        %\item 
    \end{itemize}
    In our article, section II briefly covers recent work in the field of sarcasm detection. The dataset has been used to train our model as described in section III. Section IV demonstrates the process of the model. The results and analysis have been illustrated in Section V. Finally, section VI concludes with a hope for improved performance of our model and future work in the same sector.

\section{RELATED WORK}
    Sarcasm analysis has caught the curiosity of the academic community for a long time. Many studies on sarcasm detection have lately been published, with the majority of them focusing on English and other Indo-European languages \cite{Lunando}. The majority of sarcasm detection research has been done on textual data by evaluating lexical features. As NLP is a field that primarily focuses on evaluating textual documents, it is vastly useful in terms of sarcasm detection. \par
    For instance, Razali et al. used a Convolution Neural Network (CNN) to extract deep features from sarcastic tweets, with an Artificial Neural Network (ANN) as their training approach \cite{Razali}. Their research centered on the detection of sarcasm in tweets by combining deep learning-derived features with contextual handcrafted information. 780,000 English tweets (130,000 sarcastic and 650,000 non-sarcastic) were distributed unevenly across their produced dataset. The dataset used for their research was divided into training and testing portions, respectively, of 80\% and 20\%. With all of the feature sets combined, they performed an experiment using Logistic Regression as a classification technique along with other techniques. They discovered that it was the most effective method for the task. However, it does not perform as well as a stand-alone function. \par
    A context-based feature strategy based on a deep learning model and standard machine learning was proposed by Eke et al.\cite{Eke}. To make the model training process more efficient, they combined the Internet Argument Corpus, version two (IAC-v2) benchmark dataset with two Twitter datasets: the Riloff dataset (1956 tweets) and the Ghosh and Veale dataset (54929 tweets). For learning context and word embedding, the author combined Bi-LSTM, a type of RNN, and the BERT model with Global Vector Representation (GloVe). When compared to the current approaches, their third model's(Proposed Feature Fusion model) average detection precision ranged from 3.7 to 10.2 percent and was based on feature fusion using the BERT feature, sentiment-related, syntactic, and GloVe embedding features. \par
    Jamil et al. combined CNN and LSTM networks to recognize sarcasm in input by looking at it as a sequence of embedded words \cite{Jamil}. Its foundation is based on a deep neural network (DNN), which can mimic biological neurons and conduct complex computational modeling. They have used multiple datasets in this study including the Tweet dataset(39,267 tweets) and News Headlines dataset(26,709) for training and testing purposes also the Reddit dataset(15,000) and Sarcasm Corpus V2(6,520) for validation. With an accuracy of 91.60 \%, the experimental findings showed that the suggested model outperformed the capabilities of the standard machine learning algorithms. \par
     Kumar et al. proposed one of the most widely used algorithms for detecting sarcasm in conversations, with the primary goal of anticipating sarcasm in each speaker's statement \cite{Kumar}. They used the MUStARD dataset, which contains 690 lines of conversation from four well-known sitcoms. There are an equal amount of sarcastic and non-sarcastic exchanges among the data. They used the Python framework sci-kit-learn to create the XGBoost learning model. Accuracy, F1 score, precision, and recall were used as metrics to assess XGBoost's classification performance.  \par
    Another ensemble model developed by the authors Das et al. proposed a sarcasm detection model with four Parallel Long Short Term Memory (pLSTM) networks and softmax activation function and got the best overall validation accuracy of 98.31 percent \cite{Das}. They chose the MUStARD or Multimodal Sarcasm Detection Dataset's final textual corpus, which consists of 690 lines of individually indexed dialogues, to train their model on. The dataset included the initial token average for the sentences as well as the instances of sarcastic word utterances that are part of the relevant sentences.  \par
    Additionally, LIME has been employed so that users may understand how predictions are created \cite{lime}. LIME is broadly used in interpreting various deep learning-based frameworks that are used in both image and text-based tools \cite{Zucco} \cite{apon} \cite{Ye}. \par
    Unlike the studies mentioned above, we generated a Bangla Dataset specifically for this research and utilized a traditional machine-learning model along with BERT in order to classify sarcasm.

    \begin{table}[!t]
        \caption{Comparison of various studies on sarcasm detection}
            \begin{center}
                    \begin{tabular}{|c |c |c |c |}
                        \hline
                         Model & Dataset & Accuracy & Ref  \\
                        \hline
                        Logistic Regression     &  English Tweets &  94\%   &\cite{Razali}\\ 
                        \hline
                        Proposed Feature Fusion &  English Tweets &  76.3\% &\cite{Eke}\\
                        \hline
                         CNN + LSTM & News Headlines & 73\%  &\cite{Jamil}\\
                         \hline
                         XGBoost  & MUStARD  & 93.1\%  &\cite{Kumar}\\
                         \hline
                         LSTM   & MUStARD & 98.5\% &\cite{Das}\\
                         \hline
                    \end{tabular}
                    \label{fig:x RelatedWorkTable}
            \end{center}
    \end{table}



\section{METHODOLOGY}
    In this section, we get an overview of our proposed sarcasm detection model, which is separated into three sub-sections. Sub-section \ref{proposedModel} of this paper discusses our proposed model. However, Sub-section \ref{Data} is divided into two sub-subsections. In the initial sub-subsection we discuss about our data acquisition techniques and description and in the later sub-subsection we discuss data pre-processing. Finally,  \ref{modelSpecification} exposes our model's framework.
    
    \begin{figure*}[!t]
        \centering
        \includegraphics[scale= 1.02]{img/SarcasmWorkflowV2.jpg}
        \caption{System Model: Bangla Sarcasm Detection System Model outlining the overall process.}
        \label{fig:x proposedmodels}
    \end{figure*}

\subsection{Proposed Model} \label{proposedModel}

     Initially, we started by gathering data from various social media platforms namely Facebook, YouTube, etc. Since collected data from social platforms tend to be very noisy, we had to consider a series of pre-processing techniques in order to clean the data. However, before doing so, we manually examined and labeled each of the data to confirm that there are no discrepancies between the data. After performing manual checking and pre-processing techniques over our gathered dataset, it proved to be advantageous. Finally, before feeding our data to our preferred model, we have split the data into two parts which are stratified K-Fold and traditional split where stratified K-Fold is used with BERT. Furthermore, the traditional split is divided into three sections which are train, test, and validation whereas the train set contains 60\% of the data and the other two sets have 20\% each. In addition, we have also performed one hot encoding on the labels for all three of the split sets. Later, we fed the train and test set into various traditional machine learning models namely, Logistic Regression, Decision Tree, Random Forest, Naive Bayes, K-Nearest Neighbor(KNN), Support Vector Machine(SVM), SGD, TFIDF along with K-Fold as Cross-Validation with a BERT. After training our model, with the validation set, we evaluate our trained model's performance. We have as well employed various performance metrics along with ROC Curve and confusion metrics. Here, we have performed a comprehensive comparison between the trained models from BERT and evaluated them with a test dataset in order to find out which model tends to be more efficient in terms of accuracy, memory, speed, etc finally we export the best-fitted model according to our analysis. With the exported model we have employed LIME (Local Interpretable Model-agnostic Explanations) that introduces explainability to our study. This explainability helps us understand how our model comprehends and classifies a prediction. 

%The total number of datasets is 5150 and the number ‘0’ denotes a non sarcastic statement, while ‘1’ denotes a sarcastic comment. After data pre-processing, our dataset left with 5112 data. Here, we found 3159 non-sarcastic opinions and 1953 sarcastic remarks. Based on this, we can conclude that the dataset is balanced.

% \cite{data_ref}
\subsection{Data: BanglaSarc :} \label{Data}
    \subsubsection{Data Acquisition \& Description}
       In this work, we utilized the BanglaSarc Dataset in order to evaluate our proposed model \cite{data_ref}. BanglaSarc Dataset is gathered with Bangla comments from Facebook, YouTube, and other online resources. Afterward, these data were manually reviewed and assigned labels for each of the collected records. The authors then excluded noisy data throughout the screening process. Authors further labeled the data with '0' and '1' where '0' implies a non-sarcastic statement and '1' denotes a sarcastic statement in this situation. However, these gathered datasets turned out to be rather noisy after all, which necessitated us to manually clean the data. Initially, BanglaSarc Dataset is consisted of 7800 comments however, after manually filtering authors had to discard almost 2700 records. As a result, the final dataset consisted of 5112 comments whereas it contains about 3159 non-sarcastic data and 1951 sarcastic data. Even though the BanglaSarc dataset is not entirely balanced, we may nevertheless draw the conclusion that its quality is moderate.

       
    \subsubsection{Data Pre-processing}
         We begin by removing all of the emojis as managing emojis and emoticons is essential when doing text pre-processing tasks in NLP. The stopwords are then removed since stopwords do not add much sense to a phrase. Generally, they are excluded from the dataset to improve model performance. We tokenized the text and then removed all the punctuation. Tokenization is the process of breaking down a piece of text into smaller chunks or units known as tokens. Data pre-processing is an important step in the development of a machine learning model. Figure \ref{Dataprocessing} represents an example input and output using our pre-processing methods.\par 
        \begin{figure}[htbt!]
            \centering
            \includegraphics[scale= .6]{img/dataclean.png}
            \caption{Data Pre-processing: Sample Input.}
            \label{Dataprocessing}
        \end{figure}
    
    
\subsection{Model Specification} \label{modelSpecification}
    
    
    \begin{table*}[htbt!]
        \caption{Performance Evaluation: Traditional Machine Learning Models}
            \begin{center}
                    \begin{tabular}{|c |c |c |c |c|}
                        \hline
                        Model & Accuracy & Precision & Recall & F-1 Score\\ [0.5ex]
                        \hline
                        Random Forest               & 89.93  & 89.93  & 89.93 & 89.93 \\
                        \hline
                        Decision Tree               & 83.58	 & 83.58  & 83.58 & 83.58 \\
                        \hline
                        K-Nearest Neighbor          & 74.78  & 74.78  & 74.78 & 74.78 \\
                        \hline
                        Support-Vector Machines     & 71.55 & 71.55 & 71.55 & 71.55 \\
                        \hline
                        Multinomial Naive Bayes     & 65.10 & 65.10 & 65.10 & 65.10 \\
                        \hline
                        Logistic Regression         & 62.46 & 62.46 & 62.46 & 62.46 \\
                        \hline
                        Stochastic Gradient Descent & 53.86 & 53.86 & 53.86 & 53.86 \\
                        \hline
                        BERT & 99.60  & 99.60  & 99.56 & 99.58 \\
                        \hline
                    \end{tabular}
                    \label{MlModels}
            \end{center}
        \end{table*}
    
    
    Initially, We have employed Random Forest, Decision Tree, K-Nearest Neighbor(KNN), Support Vector Machine(SVM), Multinomial Naive Bayes, Logistic Regression, Stochastic Gradient Descent(SGD). and later we further employed BERT with Stratified K Fold to evaluate our system. BERT is an artificial intelligence (AI) method for comprehending natural language and is also known as Bidirectional Encoder Representations from Transformers. In Transformers, each output element is connected to each input element, and the weightings between them are dynamically determined based on their connection. It only requires the encoder component because its goal is to build a language representation model. The BERT encoder receives a sequence of tokens, which are then translated into vectors and processed by the neural network. The Transformer works by stacking a layer that maps sequences to sequences, resulting in an output that is also a set of vectors with a 1:1 correlation between input and output tokens at the same index. In a number of tasks relevant to general language understanding, such as natural language inference and linguistic acceptability, BERT outperformed the present state-of-the-art. To fine-tune the model, we must first transform the data into the particular format used for pre-training the core BERT models, which includes adding special tokens to label the beginning and end of sentences, as well as segment IDs to differentiate different sentences, and then transform the data into the features used by  BERT. Moreover, K-fold is a cross-validation approach used to measure a machine learning model's skill using unknown data. It is often used to validate a model since it is simple to understand and execute, and the findings are more useful than traditional validation methods.
    
    %Stratified K-Folds To divide data into train and test sets, the cross-validator gives train/test indices. A K-Fold variant that returns stratified folds is this cross-validation object. The folds are created by keeping track of the sample percentages for each class.
    
    %An open source machine learning framework for NLP is called BERT. By establishing context with other text, it is intended to assist computers in deciphering unclear language in text. With the help of question and answer datasets, the BERT framework can be adjusted after being pre-trained on text from Wikipedia. 

        
        

        \begin{figure}[!t]
            \centering
            \includegraphics[scale= .5]{img/TraditionalMLROC1.png}
            \caption{Traditional Machine Learning Algorithms: ROC Curve}
            \label{MLROC1}
        \end{figure}
        
        \begin{figure}[!t]
            \centering
            \includegraphics[scale= .5]{img/TraditionalMLROC2.png}
            \caption{Traditional Machine Learning Algorithms: Precision-Recall Curve}
            \label{MLROC2}
        \end{figure}

        \begin{figure*}[!t]
            \centering
            \includegraphics[scale= .44]{img/Fold4.png}
            \caption{BERT using Stratified K-Fold 4: Left Image depicts the curve of validation Accuracy, Precision, Recall, and F-1 Score. The right image represents the loss curve during the training.}
            \label{proposedmodelFold4}
        \end{figure*}
        
        \begin{figure*}[!t]
            \centering
            \includegraphics[scale= .56]{img/Sarcasm.png}
            \caption{LIME Prediction: Here, Image (a), (b), (c), and (d) are sample inputs. Input (a) and (c) are classified as Sarcasm. (b) and (d) is predicted as non-sarcasm. Orange highlighted regions represent the negative or sarcasm level of a single word whereas Blue highlighted regions are represented as positive or non-sarcasm.}
            \label{fig:x LimeExplaiin}
        \end{figure*}
        
    % \begin{table*}[!t]
    %     \caption{Performance Evaluation: BERT using Stratified K-Fold}
    %         \begin{center}
    %                 \begin{tabular}{|c |c |c |c |c |c |c |c |}
    %                     \hline
    %                     Model  & Precision & Recall &  F-1 Score & Validation & Validation & Validation & Validation \\ [0.5ex]
    %                       &  &  &  & Accuracy & Precision & Recall & F-1 Score\\
    %                     \hline
    %                     Fold 0  & 95.26 & 95.66 & 95.45 & 90.58  & 90.06  & 90.09 & 90.07 \\
    %                     \hline
    %                     Fold 1  & 99.25 & 99.07 & 99.16 & 95.88  & 95.59  & 95.73 & 95.66 \\
    %                     \hline
    %                     Fold 2  & 99.75  & 99.63 & 99.69 & 98.34 & 98.41  & 98.07  & 98.24 \\
    %                     \hline
    %                     Fold 3  & 99.83  & 99.77 & 99.80 & 99.60  & 99.60  & 99.56 & 99.58 \\
    %                     \hline
    %                 \end{tabular}
    %                 \label{BERT}
    %         \end{center}
    %     \end{table*}
        

\section{Performance Evaluation AND ANALYSIS}
    
 %\subsection{Evaluation Metrics} \label{metrics}


%\subsection{Performance Evaluation} \label{modelPerformance}    
We have evaluated our dataset using our proposed system. We have divided our system training into two parts. In the initial part, we have employed various machine learning algorithms which is discussed in part \ref{TML}, and in the later part we have utilized BERT using Stratified K-Fold which is discussed in part \ref{BERT_Kfold}.
    \subsection{Traditional Machine Learning Algorithms} \label{TML}
         Table \ref{MlModels} depicts our findings employing traditional machine learning algorithms. We have employed  Random Forest, Decision Tree, K-Nearest Neighbor(KNN), Support Vector Machine(SVM), Multinomial Naive Bayes, Logistic Regression, and Stochastic Gradient Descent(SGD). Among these algorithms, Random Forest managed to acquire an accuracy of 91.10\% with a precision, recall, and F-1 score of 89.82\%, 87.09\%, and 88.43\% respectively. Decision Tree however managed to reach 80.61\% with precision, recall, and F-1 score between 73.86\% to 77.97\%. The rest of the algorithms performed poorly with an accuracy ranging from 71\% - 60\%. Figure \ref{MLROC1} represents the corresponding ROC curve for all 7 of the mentioned machine learning framework. Here, we can visualize the superiority of Random forest among the other frameworks. In addition, Figure \ref{MLROC2} depicts the precision-recall curve for the mentioned models. 
        
        %Our dataset were used to test our system. Using our proposed framework, we were able to comprehend and classify a prediction. We began by training multiple models on our dataset. Table II shows the results of this test. For comparison, we used seven machine learning algorithms. Except for Multinomial Naive Bayes, Logistic Regression, and Stochastic Gradient Descent, the majority of the algorithms performed well in this case. Stochastic Gradient Descent had the lowest accuracy on our dataset, while Random Forest had a decent accuracy. The performance of the Decision Tree and Random Forest models was average. Random Forest scored 91 percent, Decision Tree scored 80 percent, and K-Nearest Neighbor and Support-Vector Machines scored 71 percent on our dataset.

    \subsection{BERT} \label{BERT_Kfold}
        Table \ref{MlModels}'s last row represents our findings of our training using BERT with Stratified K-fold. At the initial fold, our model already managed to reach a validation accuracy of 90.58\% with a validation precision, recall, and F-1 score of  90.06\%, 90.09\%, and 90.07\% respectively. This proves the stability of the model's training. In the next fold, it's accuracy improved to 95.88\% which is already superior to traditional machine learning algorithms. In terms of its validation precision, recall, and F-1 score it again scored around 95\% in each section. In Fold 3, it was still able to maintain its increasing accuracy. Finally, in Fold 4 which is presented in the table, we get a validation accuracy of 99.60\% with precision, recall, and F-1 score ranging from 99.56\% to 99.60\%. Figure \ref{proposedmodelFold4} depicts the accuracy, validation, recall, and F1-score curve along with its train loss and validation loss curve.
        

    
    \subsection{Local Interpretable Model-Agnostic Explanations}
        We have employed LIME which introduces explainability to our system. Figure \ref{fig:x LimeExplaiin} shows 4 inputs and their prediction along with LIME prediction. Here, Input (a) is confidently identified as sarcasm, and LIME highlights the main elements so that the users may see how LIME arrived at its conclusion. However, Input (b) is predicted as non-sarcasm with 100\% accuracy and the reason behind its prediction is highlighted as well. While Interpreting Input (c), our proposed system furthermore made a confident prediction with  98\% accuracy. It was predicted as sarcasm. Finally, Input (d) was firmly identified as a non-sarcasm, and LIME has emphasized both the positive and negative qualities.
        

\section{Conclusion}
In this article, we have proposed a BERT-based model for detecting Bangla sarcasm that is capable of achieving 99.60\% whereas utilizing the traditional machine learning models reached only up to 89.93\% accuracy. Another novel contribution of this study is the utilization of a new Bangla sarcasm dataset, BanglaSarc that was collected specifically for the  evaluation of this study's proposed system. Furthermore, we have utilized LIME which introduces explainability to our system. BERT's pre-trained layer along with the Fully connected layer is utilized in this study to grasp deep features and classify sarcasm. Additionally, we have employed various machine learning algorithms and conducted a thorough comparison in order to find the best-fitted model. NLP-based technologies are gaining popularity recently and need cutting-edge accuracy to function properly. However, sarcasm becomes a significant issue since it is often difficult to detect and  thus reduces the overall system's performance. Although, numerous work has been conducted in detecting sarcasm from the English language, very low effort has been put into detecting sarcasm from the Bangla language. Our proposed approach would address this issue and bridge the gap between these two languages in terms of sarcasm identification. In the future, we hope to increase the quantity along with the quality of the dataset. Moreover, we aim to improve memory efficiency along with inference time by proposing a lighter framework. 

%NLP applications are vital in recent times that requires state of the art accuracy however, due to the unpredictable nature of sarcasms. Sarcasm identification is essential for a variety of NLP task, such as sentiment recognition in product evaluations, user comments, and online forums. It is a demanding task that necessitates a full understanding of the language, context, and worldwide knowledge. Detecting sarcasm is crucial for understanding a user’s point of view. The main goal is to reduce the amount of time it takes humans to extract sarcasm from reviews and to develop a model that can recognize sarcasm in natural language. In this study, we used traditional machine learning models, BERT with Stratified K-Fold and deployed Lime to identify sarcasm. Many pre-processing algorithms are used to identify sarcasm in datasets. The datasets were processed using traditional machine learning model and BERT, and it was discovered that the BERT method produced the best results, with a test accuracy of 99.60\% and traditional machine learning algorithm achieved 91\% accuracy which revealed that the BERT classification model’s performance is better. Lime is one of the most widely used Explainable AI systems due to it's success. For this reason, in this study, we used Lime's explainability in terms of its ability to classify sarcasm. However, detecting sarcasm is crucial for understanding a user's perspective and goals in downstream operations like sentiment categorization and opinion mining. Our suggested method is also easier to implement and uses less memory and processing resources, thus it should be appropriate for real time applications. It is hoped that Explainable AI will help decrease model uncertainty. In our future work, we want to expand our study by using larger dataset and other Machine Learning classifiers to comparing the results and identify the accuracy of the best suited technique.




\begin{thebibliography}{00}

\bibitem{data_ref} Apon TS, Anan R, Modhu EA, Suter A, Sneha IJ, Alam MG. BanglaSarc: A Dataset for Sarcasm Detection. In 9th IEEE Asia-Pacific Conference on Computer Science and Data Engineering (IEEE CSDE) 2022 Dec 16-18th.

\bibitem{Faraj} Faraj D, Abdullah M. Sarcasmdet at sarcasm detection task 2021 in arabic using arabert pretrained model. InProceedings of the Sixth Arabic Natural Language Processing Workshop 2021 Apr (pp. 345-350).

\bibitem{Lunando} Lunando E, Purwarianti A. Indonesian social media sentiment analysis with sarcasm detection. In2013 International Conference on Advanced Computer Science and Information Systems (ICACSIS) 2013 Sep 28 (pp. 195-198). IEEE

\bibitem{Razali} Razali MS, Halin AA, Ye L, Doraisamy S, Norowi NM. Sarcasm detection using deep learning with contextual features. IEEE Access. 2021 Apr 30;9:68609-18.

\bibitem{Eke} Eke CI, Norman AA, Shuib L. Context-based feature technique for sarcasm identification in benchmark datasets using deep learning and BERT model. IEEE Access. 2021 Mar 23;9:48501-18.

\bibitem{Jamil} Jamil R, Ashraf I, Rustam F, Saad E, Mehmood A, Choi GS. Detecting sarcasm in multi-domain datasets using convolutional neural networks and long short term memory network model. PeerJ Computer Science. 2021 Aug 25;7:e645.

\bibitem{Kumar} Kumar A, Dikshit S, Albuquerque VH. Explainable artificial intelligence for sarcasm detection in dialogues. Wireless Communications and Mobile Computing. 2021 Jul 3;2021.

\bibitem{Das} Das S, Kolya AK. Parallel Deep Learning-Driven Sarcasm Detection from Pop Culture Text and English Humor Literature. InProceedings of Research and Applications in Artificial Intelligence 2021 (pp. 63-73). Springer, Singapore. 


\bibitem{Joshi} Joshi A, Bhattacharyya P, Carman MJ. Automatic sarcasm detection: A survey. ACM Computing Surveys (CSUR). 2017 Sep 26;50(5):1-22.

\bibitem{Hoo} Hoo ZH, Candlish J, Teare D. What is an ROC curve?. Emergency Medicine Journal. 2017 Jun 1;34(6):357-9.

\bibitem{Pennington} Pennington J, Socher R, Manning CD. Glove: Global vectors for word representation. InProceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) 2014 Oct (pp. 1532-1543).

\bibitem{lime}Ribeiro MT, Singh S, Guestrin C. " Why should i trust you?" Explaining the predictions of any classifier. InProceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining 2016 Aug 13 (pp. 1135-1144).

\bibitem{apon} Apon TS, Hasan MM, Islam A, Alam MG. Demystifying Deep Learning Models for Retinal OCT Disease Classification using Explainable AI. In 2021 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE) 2021 Dec 8 (pp. 1-6). IEEE.

\bibitem{Zucco}Zucco C, Liang H, Di Fatta G, Cannataro M. Explainable sentiment analysis with applications in medicine. In2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 2018 Dec 3 (pp. 1740-1747). IEEE.

\bibitem{Ye}Ye Q, Xia J, Yang G. Explainable AI for COVID-19 CT classifiers: An initial comparison study. In2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS) 2021 Jun 7 (pp. 521-526). IEEE. 

\end{thebibliography}
\vspace{12pt}

\end{document}


%After completing all of the preceding processes, we will be able to compile our model. Because the model is a binary classification issue, we employed binary cross entropy, and the matrix we monitored is the accuracy matrix, from which we could produce the model summary. When evaluating classification models, accuracy is one parameter to consider \cite{b1}. We got 93\% matrices accuracy and 92\% F1 score. Moreover, the Receiver Operating Characteristic (ROC) is an alternative curve accuracy for evaluating learning algorithms on raw dataset. The ROC curve is a mathematical curve. We obtained 98\% ROC accuracy as shown in the figure \ref{ROC accuracy}.



%We now have a validation accuracy in the training section. We have also included a training visualization in which we can see that both the validation and training losses are dropping, indicating that there is no over-fitting. After completing all of the procedures, the model we have developed can recognize sarcasm. Hence, the above graph of figure \ref{training accuracy} shows that training accuracy of our model is more than 99\% and Validation accuracy is more than 95\%. Contrastingly, the figure \ref{training loss} shows that training loss is less than 1\% and validation loss is close to 5\% .



%A confusion matrix is a technique for summarizing a classification algorithm’s performance. 


%The figure \ref{confusion matrix} depicts the evaluation of the confusion matrix, which illustrates the performance of the classification model by comparing the actual target values to the prediction made by our model. In our result analysis part, we have tested our model and got quite good results that has shown on the table 1. For input, “She is very intelligent”, our model gives the correct prediction that it is not a sarcasm and for input, “You just broke my house window. Great job!!”, it’s clearly visible that it is sarcasm because it says the exact opposite of what it appears to express. Our model also gives the correct prediction for this sentence that it is sarcasm. However, there is a drawback for some sentences such as “I am so hungry that I can eat an elephant.”, with common sense we know that it is a sarcastic sentence, but our model could not predict it correctly.

%\subsection{Train-Test Split}
%We separate the datasets into train and test with a validation split of 0.2, suggesting that 20\% of the data has been used for testing and 80\% for training. We use Keras tokenizer since it made it easier for us to build tensors to give to the model. We have created our sequence this manner, and then computed the word index, which will be the unique tokens.We have padded 0s into headlines that are less than 26 words long so that we may pass these headlines to our model. Furthermore, we have broken the dataset into boolean values, we will shuffle them to remove any bias. We have allocated these variables to the tensors in order to have a train and test set. Figure \ref{Workflow Diagram} shows how the Workflows using a diagram. 



%\subsection{GloVe Model}
%GloVe is a technique for generating word vector representations that is based on unsupervised learning \cite{b2}. The resulting repre-sentations have been trained using aggregated global word-word co-occurrence information from a corpus, indicate fascinating linear substructures of the word vector space \cite{b2}. Here, we have loaded our GloVe model and employed hundred dimensional vectors. The embedding index is a dictionary with words as keys and their vector representation as the value. We found 11.93.514 word’s vectors from the representation. The GloVe model performs significantly better than the other baseline models, even with smaller corpora and smaller vector sizes \cite{Pennington}. As indicated by its performance of 75\% on a recent word analogue task, the model generates a vector space with relevant substructure and overcome related models on similar tasks and named entity recognition \cite{Pennington}.

%\subsection{Word Embeddings}

%Word embedding is a phrase that specifies how words are represented for text analysis in natural language processing (NLP).Usually in the form of a real-valued vector that encodes a word’s meaning and forecasts a word’s meaning near the vector space. We created an embedding matrix to help us design our embedding layer. The form of the embedded matrix would be the vocabulary shape multiplied by the embedding dimension (which is 100). We utilized a basic logic to generate the embedding matrix: if a word is in our lexicon, we extract its vector representation; otherwise, we leave the 0’s alone. The embedding layer was created using the Keras embedding function. Fig. \ref{Architecture of Feature Extractor} illustrates the Architecture of Feature Extractor.


%\subsection{RNN}
%The state-of-the-art algorithm for sequential data is RNN. This is the first algorithm to fetch input using internal memory and is ideal for machine learning problems with sequential data \cite{Donges}. LSTMs are a kind of recurrent neural network that has the ability to enhance memory capacity while also handling long sequence dependencies. As a result, it’s ideal for learning from significant experiences sepa-rated by long periods of time \cite{Donges}. It is one among the calculations liable for extraordinary advances in profound learning throughout the most recent couple of years. The simplest method to design an RNN is using a sequential module. The first layer of our network was then embedded, after that 64 LSTM RNNs with a dropout of 0.2 and a recurrent dropout of 0.25 were inserted as the first layer of our network. We would require the Sigmoid function since we have been dealing with a binary classification problem. The final layer would now have only one node


    %\subsubsection{Accuracy}
        %The difference between the measured value and the actual value can be defined as accuracy. In other words, the degree to which measurements or predictions closely resemble a given value is known as a system's accuracy. When true positive and true negative values are more significant than false positives and false negatives, accuracy can be a key performance indicator. Equation \ref{accuracyEquation} illustrates how to evaluate a system's performance.
        
        %\begin{equation}
            %Accuracy=\frac{Correct\,Predictions}{All\,Predictions} %\label{accuracyEquation}
        %\end{equation}

    %\begin{center}
            %\begin{tabular}{c|c|r@{\extracolsep{0pt}.}l|c|}
                %\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{3}{c}{Predicted}\tabularnewline
                %\cline{3-5} \cline{5-5} 
                %\multicolumn{1}{c}{} &  & \multicolumn{2}{c|}{Negative} & Positive\tabularnewline
                %\cline{2-5} \cline{3-5} \cline{5-5} 
                %{Actual} & Negative & \multicolumn{2}{c|}{True Negative} & False Positive\tabularnewline
                %\cline{2-5} \cline{3-5} \cline{5-5} 
                 %& Positive & \multicolumn{2}{c|}{False Negative} & True Positive\tabularnewline
                %\cline{2-5} \cline{3-5} \cline{5-5} 
            %\end{tabular}
          
    %\end{center}
    
    %\subsubsection{Precision}
        %Despite the fact that precision and accuracy are closely related metrics, precision refers to how much information a value provides. As per equation \ref{precision}, true positives and false positives can be distinguished with precision. To determine whether a particular classification model is effective, precision could be considered.

        %\begin{equation}
            %Precision=\frac{True\,Positive}{True\,Positive+False\,Positive} %\label{precision}
        %\end{equation}

    %\subsubsection{Recall}
        %Recall, as specified by equation \ref{recall}, is the percentage of correct predictions out of all the correct predictions that could've been generated by a classification method. Recall of a machine learning model is similar to accuracy and precision in that it depends on positive values while being independent of negative values.

    %\begin{equation} \label{recall}
            %Recall=\frac{True\,Positive}{Total\,Actual\,Positive} 
    %\end{equation}
    
    %\subsubsection{F1 score}
        %A classification model's F1 score shows a reasonable balance between precision and accuracy. According to equation \ref{f1}, the F1 score is calculated using a system's accuracy and recall value. When precision and recall values are ideal, the maximum F1 score of 1 can be obtained. Contrary to accuracy, F1 score can be a crucial performance measure when false positive and false negative values are more relevant than true positive and true negative ones.
        %\begin{equation}
            %F1\,Score=2\times\frac{Precision\times Recall}{Precision+Recall} %\label{f1}
        %\end{equation}
       
        
    %\subsubsection{ROC Curve}
    %The responsiveness and precision of the results can be affected by selecting an appropriate cut-off using the ROC curve. The capacity of a test to distinguish between the presence or absence of a particular condition is measured globally by the area under the ROC curve (AUC). The choice of a test threshold relies on the test's objectives and is often not made by weighing sensitivity and specificity equally in an effort to increase accuracy \cite{Hoo}.
    
    %\subsubsection{Confusion Metrics}
    %The performance of any machine learning classification can be evaluated using a confusion matrix. A confusion matrix, which summarizes the number of accurate and inaccurate predictions made by the classifier, could be displayed in a tabular format. By calculating performance indicators like accuracy, precision, recall, and F1-score, it assesses a classification model's efficiency.
