{
    "arxiv_id": "2303.10580",
    "paper_title": "Hierarchical Personalized Federated Learning Over Massive Mobile Edge Computing Networks",
    "authors": [
        "Chaoqun You",
        "Kun Guo",
        "Howard H. Yang",
        "Tony Q. S. Quek"
    ],
    "submission_date": "2023-03-19",
    "revised_dates": [
        "2023-03-23"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
    ],
    "abstract": "Personalized Federated Learning (PFL) is a new Federated Learning (FL) paradigm, particularly tackling the heterogeneity issues brought by various mobile user equipments (UEs) in mobile edge computing (MEC) networks. However, due to the ever-increasing number of UEs and the complicated administrative work it brings, it is desirable to switch the PFL algorithm from its conventional two-layer framework to a multiple-layer one. In this paper, we propose hierarchical PFL (HPFL), an algorithm for deploying PFL over massive MEC networks. The UEs in HPFL are divided into multiple clusters, and the UEs in each cluster forward their local updates to the edge server (ES) synchronously for edge model aggregation, while the ESs forward their edge models to the cloud server semi-asynchronously for global model aggregation. The above training manner leads to a tradeoff between the training loss in each round and the round latency. HPFL combines the objectives of training loss minimization and round latency minimization while jointly determining the optimal bandwidth allocation as well as the ES scheduling policy in the hierarchical learning framework. Extensive experiments verify that HPFL not only guarantees convergence in hierarchical aggregation frameworks but also has advantages in round training loss maximization and round latency minimization.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.10580v1"
    ],
    "publication_venue": null,
    "doi": "10.1109/TWC.2023.3260141"
}