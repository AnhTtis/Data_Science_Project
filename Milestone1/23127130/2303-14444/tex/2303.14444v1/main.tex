% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:

\usepackage{hyperref}
\usepackage{color}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{tablefootnote}
\usepackage{adjustbox} %make table fitting
\usepackage{tabularx}%make table fitting
\newcolumntype{Y}{>{\centering\arraybackslash}X}%make table columns equal widht
\usepackage{bbold} % 1 operator in equation
\usepackage{arydshln} %dashed lines in table
% \usepackage{subcaption} %tables next to each other
% \usepackage{float} %table position
\usepackage{caption} %recognice image as figure in table env


\begin{document}

%
\title{MultiTalent: A Multi-Dataset Approach to Medical Image Segmentation}
%
% \titlerunning{MultiTalent: A Multi-Dataset Approach to Medical Image Segmentation}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%

% \author{Anonymous\inst{1}}
% \authorrunning{Anonymous}
% \institute{Anonymous Organization \\ \email{anonymous}}



\author{Constantin Ulrich\inst{1,4} \and
Fabian Isensee\inst{1,2} \and
Tassilo Wald\inst{1,2} \and
Maximilian Zenk\inst{1,} \and
Michael Baumgartner\inst{1,2,5} \and
Klaus H. Maier-Hein\inst{1,3}}
%
\authorrunning{C. Ulrich et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany\\
\and Helmholtz Imaging, DKFZ, Heidelberg, Germany\\
\and Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, Heidelberg, Germany \\
\and National Center for Tumor Diseases (NCT), NCT Heidelberg, A partnership between DKFZ and University Medical Center Heidelberg\\
\email{constantin.ulrich@dkfz-heidelberg.de}}
%
\maketitle % typeset the header of the contribution
%
\begin{abstract}
The medical imaging community generates a wealth of data-sets, many of which are openly accessible and annotated for specific diseases and tasks such as multi-organ or lesion segmentation. %\\
Current practices continue to limit model training and supervised pre-training to one or a few similar datasets, neglecting the synergistic potential of other available annotated data. %It remains to be shown whether one model trained multiple CT datasets outperforms state-of-the-art models trained on single datasets.\\ 
We propose MultiTalent, a method that leverages multiple CT datasets with diverse and conflicting class definitions to train a single model for a comprehensive structure segmentation. Our results demonstrate improved segmentation performance compared to previous related approaches, systematically, also compared to single-dataset training using state-of-the-art methods, especially for lesion segmentation and other challenging structures. We show that MultiTalent also represents a powerful foundation model that offers a superior pre-training for various segmentation tasks compared to commonly used supervised or unsupervised pre-training baselines. %\\
Our findings offer a new direction for the medical imaging community to effectively utilize the wealth of available data for improved segmentation performance. The code and model weights will be published here: [tba] 

\keywords{Medical image segmentation \and multitask learning \and transfer learning\and foundation model \and partially labeled datasets.}
\end{abstract}
\section{Introduction}

\begin{figure}[t]
\centering
\includegraphics[width=1\textwidth]{overrview_figure.eps}
\caption{(a) Usually only a few classes are annotated in publicly available datasets. b) Different groundtruth label properties can generate contradicting class predictions. For example, the heart annotation of dataset 11 differs from the heart annotation of dataset 10, which causes the aorta of dataset 11 to overlap with the heart of dataset 10. In contrast to dataset 11, in dataset 7 the aorta is also annotated in the lower abdomen. c) Instead of training one network for each dataset, we introduce a method to train one network with all datasets, while retaining dataset-specific annotation protocols.} \label{overview_figure}
\end{figure}

The success of deep neural networks heavily relies on the availability of large and diverse annotated datasets across a range of computer vision tasks. To learn a strong data representation for robust and performant medical image segmentation, huge datasets with either many thousands of annotated data structures or less specific self-supervised pretraining objectives with unlabeled data are needed \cite{ZHOU2021101840,swinunetr}. 
The annotation of 3D medical images is a difficult and laborious task. Thus, depending on the task, only a bare minimum of images and target structures is usually annotated. This results in a situation where a zoo of partially labeled datasets is available to the community. 
Recent efforts have resulted in a large dataset of >1000 CT images with >100 annotated classes each, thus providing more than 100.000 manual annotations which can be used for pre-training \cite{totalseg}. Focusing  on such a dataset prevents leveraging the potentially precious additional information of the above mentioned other datasets that are only partially annotated. Integrating information across different datasets potentially yields a higher variety in image acquisition protocols, more anatomical target structures or details about them as well as information on different kinds of pathologies. 
Consequently, recent advances in the field allowed utilizing partially labeled datasets to train one integrated model \cite{partially_survey}. Early approaches handled annotations that are present in one dataset but missing in another by considering them as background \cite{roulet_joint_2019,fang_multi-organ_2020} and penalizing overlapping predictions by taking advantage of the fact that organs are mutually exclusive \cite{shi_marginal_2020,fidon_label-set_2021}. Some other methods only predicted one structure of interest for each forward pass by incorporating the class information at different stages of the network \cite{dmitriev_learning_2019,zhang_dodnet_2020,clip_medicalseg}. Chen et al. trained one network with a shared encoder and separate decoders for each dataset to generate a generalized encoder for transfer learning \cite{chen_med3d_2019}. However, most approaches are primarily geared towards multi-organ segmentation as they do not support overlapping target structures, like vessels or cancer classes within an organ \cite{feng2021mskd,filbrandt_learning_2021,contextaware,teacher_student}.
So far, all previous methods do not convincingly leverage cross-dataset synergies. 
As Liu et al. pointed out, one common caveat is that many methods force the resulting model to average between distinct annotation protocol characteristics \cite{clip_medicalseg} by combining labels from different datasets for the same target structure (visualized in Figure 1 b)). Hence, they all fail to reach segmentation performance on par with cutting-edge single dataset segmentation methods. 
To this end, we introduce MultiTalent (MULTI daTAset LEarNing and pre-Training), a new, flexible, multi-dataset training method: 1) MultiTalent can handle classes that are absent in one dataset but annotated in another during training. 2) It retains different annotation protocol characteristics for the same target
structure and 3) allows for overlapping target structures with
different level of detail such as liver, liver vessel and liver tumor. Overall, MultiTalent can include all kinds of new datasets irrespective of their annotated target structures. \\ MultiTalent can be used in two scenarios: First, in a combined multi-dataset (MD) training to generate one foundation segmentation model that is able to predict all classes that are present in any of the utilized datasets, and second, for pre-training to leverage the learned representation of this foundation model for a new task. In experiments with a large collection of abdominal CT datasets, the proposed model outperformed state-of-the-art segmentation networks that were trained on each dataset individually as well as all previous methods that incorporated multiple datasets for training. Interestingly, the benefits of MultiTalent are particularly notable for more difficult classes and pathologies. In comparison to an ensemble of single dataset solutions, MultiTalent comes with shorter training and inference times.\\
Additionally, at the example of three challenging datasets, we demonstrate that fine-tuning MultiTalent yields higher segmentation performance than training from scratch or initializing the model parameters using unsupervised pre-training strategies \cite{swinunetr,ZHOU2021101840}. It also surpasses supervised pretrained and fine-tuned state-of-the art models on most tasks, despite requiring orders of magnitude less annotations during pre-training.





\section{Methods}
We introduce MultiTalent, a multi dataset learning and pre-training method, to train a foundation medical image segmentation model. It comes with a novel dataset and class adaptive loss function. The proposed network architecture enables the preservation of all label properties, learning overlapping classes and the simultaneous prediction of all classes. Furthermore, we introduce a training schedule and dataset preprocessing which balances varying dataset size and class characteristics. %We show the effectiveness and broad applicability of our method on three different network architectures and compare them with state-of-the-art segmentation methods on single datasets. Finally, we evaluate its generalized feature representation in a transfer learning context.
% In \ref{Training_strategy}, a detailed description of the implementation details for the MD training and subsequent transfer learning experiments is provided.

\subsection{Problem definition}
We begin with a dataset collection of $K$ datasets $D^{(k)}, k \in [1, K]$, with $N^{(k)}$ image and label pairs $D^{(k)} = \{(x,y)^{(k)}_1,..., (x,y)^{(k)}_{N^{(k)}}\}$. In these datasets, every image voxel $x^{(k)}_i, i \in [1,I]$, is assigned to one class $c \in C^{(k)}$, where $C^{(k)} \subseteq C$ is the label set associated to dataset $D^{(k)}$. Even if classes from different datasets refer to the same target structure we consider them as unique, since the exact annotation protocols and labeling characteristics of the annotations are unknown and can vary between datasets: $C^{(k)} \cap C^{(j)} = \emptyset, \forall k \neq j$.
This implies that the network must be capable of predicting multiple classes for one voxel to account for the inconsistent class definitions. 

\subsection{MultiTalent}
\subsubsection{Network modifications}
We employ three different network architectures, which are further described below, to demonstrate that our approach is applicable to any network topology. To solve the label contradiction problem we decouple the segmentation outputs for each class by applying a Sigmoid activation function instead of the commonly used Softmax activation function across the dataset. The network shares the same backbone parameters $\Theta$ but it has independent segmentation head parameters $\Theta_c$ for each class. The Sigmoid probabilities for each class are defined as $\hat{y}_c=f(x,\Theta, \Theta_c)$. This modification allows the network to assign multiple classes to one pixel and thus enables overlapping classes and the conservation of all label properties from each dataset. Consequently, the segmentation of each class can be thought of as a binary segmentation task. 


\subsubsection{Dataset and class adaptive loss function}
\label{loss_function}
Based on the well established combination of a Cross-entropy and Dice loss for single dataset medical image segmentation, we employ the binary Binary Cross-entropy loss (BCE) and a modified Dice loss for each class over all $B, b\in [1,B]$, images in a batch: 

\begin{equation}
     L_c = \frac{1}{I}\sum_{b,i} BCE(\hat{y}^{(k)}_{i,b,c}, \; y^{(k)}_{i,b,c})  -\frac{2\sum_{b,i}\hat{y}^{(k)}_{i,b,c} \; y_{{i,b,c}}^{(k)}} {\sum_{b,i}\hat{y}^{(k)}_{i,b,c} +\sum_{b,i} y_{i,b,c}^{(k)}}
\end{equation}
While the regular dice loss is calculated for each image within a batch, we calculate the dice loss jointly for all images of the input batch. This regularizes the loss if only a few voxels of one class are present in one image and a larger area is present in another image of the same batch. Thus, an inaccurate prediction of a few pixels in the first image has a limited effect on the loss. In the following, we unite the sum over the image voxels $i$ and the batch $b$ to $\sum_{z}$.
We modify the loss function to be calculated only for classes that were annotated in the corresponding partially labeled dataset \cite{roulet_joint_2019,fang_multi-organ_2020}, in the following indicated by $\mathbb{1}_c^{(k)}$, where $\mathbb{1}_c^{(k)} = 1$ if $c \in C^{(k)}$ and 0 otherwise. Instead of averaging, we add up the loss over the classes. Hence, the loss signal for each class prediction does not depend on the number of other classes within the batch. This compensates for the varying number of annotated classes in each dataset:
\begin{equation}
    L = \sum_{c} \Big( \mathbb{1}_c^{(k)} \frac{1}{I}\sum_{z} BCE(\hat{y}^{(k)}_{z,c}, \;y^{(k)}_{z,c})  -\frac{2\sum_{z}\mathbb{1}_c^{(k)} \; \hat{y}^{(k)}_{z,c} \; y_{z,c}^{(k)}} {\sum_{z}\mathbb{1}_c^{(k)} \; \hat{y}^{(k)}_{z,c} +\sum_{z}\mathbb{1}_c^{(k)} \; y_{z,c}^{(k)}} \Big)
     %L &= \sum_{c} \left (\sum_{b} e_c^k BCE(f(x_{b}^k, \theta)_c,y_{c,b}^k)  -  \frac{2\sum_{b} e_c^k f(x^k_b, \theta)_c y_{b,c}^)} {\sum_{b} e_c^k f(x_b^k, \theta)_c +\sum_{b} e_c^k y_{b,c}^k}\right)
\end{equation}

% \begin{equation}
%     \mathbb{1}_c^{(k)} = 1 \, if \, c \in C^{(k)}, \, 0 \, otherwise
% \end{equation} 


\subsubsection{Network architectures}
To demonstrate the general applicability of this approach, we applied it to three segmentation networks. We employed a 3D U-Net \cite{ronneberger_u-net_2015}, an extension with additional residual blocks in the encoder (Resenc U-Net), that demonstrated highly competitive results in previous medical image segmentation challenges \cite{extendingnnunet,fabi_kits} and a recently proposed transformer based architecture (SwinUNETR \cite{swinunetr}).
We implemented our approach in the nnU-Net framework \cite{isensee_nnu-net_2021}. However, the automatic pipeline configuration from nnU-net was not used in favor of a manually defined configuration that aims to reflect the peculiarities of each of the datasets, irrespective of the number of training cases they contain. We manually selected a patch size of $[96, 192,192]$ and image spacing of 1mm in plane and 1.5mm for the axial slice thickness, which nnU-Net used to automatically create the two CNN network topologies. For the SwinUNETR, we adopted the default network topology.


\subsubsection{Multi-dataset training setup}
\label{Training_strategy}
We trained MultiTalent with 13 public abdominal CT datasets with a total of 1477 3D images, including 47 classes (Multi-dataset (MD) collection) \cite{Antonelli2022,BTCV,BTCV2,structseg,lambert2019segthor,roth2015deeporgan,clark_cancer_2013,NHpancreas,heller2020kits19}. Detailed information about the datasets, can be found in the appendix in Table 3 and Figure 3, including the corresponding annotated classes. %To ensure reproducibility and comparability with existing work, all experiments were conducted with the 5-fold split that nnU-Net would generate for each individual dataset.
We increased the batch size to 4 and the number of training epochs to 2000 to account for the high number of training images. To compensate for the varying number of training images in each dataset, we choose a sampling probability per case that is inversely proportional to $\sqrt{n}$, where $n$ is the number of training cases in the corresponding source dataset.

\subsubsection{Transfer learning setup} We used the BTCV (small multi organ dataset \cite{BTCV}), AMOS (large multi organ dataset \cite{ji2022amos}) and KiTS19 (pathology dataset \cite{heller2020kits19}) datasets to evaluate the generalizability of the MultiTalent features in a pre-training and fine tuning setting. Naturally, the target datasets were excluded from the repective pre-training. Fine tuning was performed with identical configuration as the source training, except for the batch size which was set to 2. We followed the fine-tuning schedule proposed by Kumar et al. \cite{transferschedule}. First, the segmentation heads were warmed up over 10 epochs with linearly increasing learning rate, followed by a whole-network warum-up over 50 epochs. Finally, we continued with the standard nnU-Net training schedule.


\begin{table}
\centering
\includegraphics[width=0.8\textwidth]{barplot_yaxixbreak.eps}
\captionsetup{type=figure}
\captionof{figure}{Comparison of Dice scores averaged over all classes, all datasets and over classes of special interest. Difficult classes are those for which the default nnUNet has a Dice below 75. The same color indicates the same network architecture and the pattern implies training with multiple datasets using MultiTalent.  \label{fig:barplot_results}}

% \begin{minipage}[c]{0.49\textwidth}



\centering
\captionsetup{type=table}
\captionof{table}{Official BTCV test set leaderboard results, Dice and 95\% Hausdorff Distance. * indicates usage of multiple datasets. \label{BTCV_leaderboard}}
\begin{adjustbox}{width=0.785\textwidth}
\centering
\begin{tabular}{llcc}
\toprule
Method & \#models &Avg. Dice &  Avg. HD95 \\ \midrule
\multirow{2}{*}{nnU-Net \cite{isensee_nnu-net_2021}} & 2 models, each  &\multirow{2}{*}{88.10} & \multirow{2}{*}{17.26} \\
& 5-fold ensemble & &\\
UNETR \cite{clip_medicalseg,hatamizadeh2021unetr}& single model &81.43  & -  \\
SwinUNETR \cite{clip_medicalseg,swinunetr} & single model &82.06 & - \\
Universal Model* \cite{clip_medicalseg} & single model &86.13 & - \\
DoDNet (pretrained*) \cite{zhang_dodnet_2020}&single model &86.44 & 15.62  \\ 
PaNN* \cite{zhou2019prioraware}  & single model&84.97 & 18.47  \\ 
\hdashline
MultiTalent Resenc U-Net* &single model &88.82 & 16.35  \\
MultiTalent Resenc U-Net* &5-fold ensemble &88.91 & \textbf{14.68} \\
\hdashline
Resenc U-Net (pre-trained MultiTalent*) & 5-fold ensemble &\textbf{89.07} &15.01 \\
\bottomrule
\end{tabular}
\end{adjustbox}

\vspace*{0.5 cm}



\centering
\captionof{table}{5-fold cross validation results of different architectures and pretraining schemes. We used the original (org.) SwinUNETR implementation and the provided self-supervised weights as additional baseline \cite{swinunetr}.\label{transfer_table}}
\centering
\begin{adjustbox}{width=0.785\textwidth}
\centering
\begin{tabular}{ l l c c c c}
\toprule
 \multirow{2}{*}{Architecture}& \multirow{2}{*}{Pretraining scheme} & BTCV & AMOS & KiTs19 & KiTs19 \\
  &  & Dice avg. & Dice avg. &  Kidney Dice & Tumor Dice \\
\midrule
SwinUNETR & from scratch & 74.27 & 86.04& 87.69 & 46.56\\
org. implement. & self-supervised \cite{swinunetr} &74.71 &86.11 & 87.62 & 43.64\\
\hdashline
\multirow{3}{*}{SwinUNETR} & from scratch & 81.44 & 87.59& 95.97 & 76.52\\
 & supervised (\raisebox{-0.9ex}{\~{}}$10^5$ annot.) \cite{totalseg} & 83.08 &88.63& 96.36 & 80.30 \\
 & \textbf{MultiTalent}(\raisebox{-0.9ex}{\~{}}3600 annot.)  &82.14& 87.32 & 96.08 & 76.56\\
\hdashline
\multirow{4}{*}{U-Net} & from scratch& 83.76 & 89.40 & 96.56 & 80.69\\
 & self-supervised \cite{ZHOU2021101840} & 84.01&  89.30& 96.59 & 80.91\\
 & supervised (\raisebox{-0.9ex}{\~{}}$10^5$ annot.) \cite{totalseg} & 84.22 & 89.66 & 96.72 & 82.48\\
 & \textbf{MultiTalent} (\raisebox{-0.9ex}{\~{}}3600 annot.) & 84.42& 89.60 & 96.81 & 83.03\\
\hdashline
\multirow{4}{*}{Resenc U-Net} & from scratch & 84.38 & 89.71& 96.83 & 83.22 \\
 & self-supervised \cite{ZHOU2021101840} & 84.27 & 89.70& 96.82 & 83.53 \\
 & supervised (\raisebox{-0.9ex}{\~{}} $10^5$ annot.) \cite{totalseg}&84.79 & \textbf{89.91} & 96.85 & 83.73\\
 & \textbf{MultiTalent} (\raisebox{-0.9ex}{\~{}}3600 annot.) & \textbf{84.92} & 89.81 & \textbf{96.89} &\textbf{84.01}\\
\bottomrule
\end{tabular}
\end{adjustbox}

\end{table}


\subsection{Baselines}
As a baseline for the MultiTalent, we applied the 3D U-Net generated by the nnU-Net without manual intervention to each dataset individually. Furthermore, we trained a 3D U-Net, a Resenc U-Net and a SwinUNETR with the same network topology, patch and batch size as our MultiTalent for each dataset. All baseline networks were also implemented within the nnU-Net framework and follow the default training procedure. Additionally, we compare MultiTalent with related work on the public BTCV leaderboard in \autoref{BTCV_leaderboard}.\\
Furthermore, the utility of features generated by MultiTalent is compared to supervised and unsupervised pre-training baselines. As supervised baseline, we used the weights resulting from training the three model architectures on the TotalSegmentator dataset, which consists of 1204 images and 104 classes \cite{totalseg}, resulting in more than $10^5$ annotated target structures. In contrast, MultiTalent is only trained with about 3600 annotations. We used the same patch size, image spacing, batch size and number of epochs as for the MultiTalent training. As unsupervised baseline for the CNNs, we pre-trained the networks on the Multi-dataset collection based on the work of Zhou at al. (Model Genesis \cite{ZHOU2021101840}).
Finally, for the SwinUNETR architecture, we compared the utility of the weights from our MultiTalent with the ones provided by Tan et al. who performed self-supervised pre-training on 5050 CT images. This necessitated the use of the original (org.) implementation of SwinUNETR because the recommended settings for fine tuning were used. This should serve as additional external validation of our model. To ensure fair comparability, we did not scale up any models. Despite using gradient checkpointing, the SwinUNETR models requires roughly 30 GB of GPU memory, compared to less than 17 GB for the CNNs.



%Furthermore, we compared our multi-class network with state-of-the-art methods for single datasets and available related multi-class methods on public leaderboards \cite{isensee_nnu-net_2021,zhang_dodnet_2020,chen_med3d_2019,zhou2019prioraware,fang_multi-organ_2020}. By choosing these baselines, we clearly distinguish this work from related work that evades the highly competitive comparison with state-of-the-art methods for each dataset.
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\textwidth]{barplot_yaxixbreak.eps}
%     \caption{Comparison of Dice scores averaged over all classes, all datasets and over classes of special interest. Difficult classes are those for which the default nnUNet has a Dice below 75. The same color indicates the same network architecture and the pattern implies that the network was trained with multiple datasets using MultiTalent.} \label{barplot_results}
%     \quad
% \end{figure}

\section{Results}
%In this section, we first present the results of training one network on multiple datasets. After that, we show that this multi-dataset training results in a promising general feature representation that is highly beneficial for transfer learning.
\textbf{Multi-dataset training} results are presented in \autoref{fig:barplot_results}. In general, the convolutional architectures clearly outperform the transformer-inspired SwinUNETR. MultiTalent improves the performance of the purely convolutional architectures (U-Net and Resenc U-Net) and outperforms the corresponding baseline models that were trained on each dataset individually.  Since a simple average over all classes would introduce a biased perception due to the highly varying numbers of images and classes, we additionally report an average over all datasets. For example, dataset 7 consists of only 30 training images but has 13 classes, whereas dataset 6 has 126 training images but only 1 class. Table 4 in the appendix provides all results for all classes. Averaged over all datasets, the MultiTalent gains 1.26 Dice points for the Resenc U-Net architecture and 1.05 Dice points for the U-Net architecture. Compared to the default nnU-Net, configured without manual intervention for each dataset, the improvements are 1.56 and 0.84 Dice points. Additionally, in \autoref{fig:barplot_results} we analyzed two subgroups of classes. The first group includes all "difficult" classes for which the default nnU-Net has a Dice smaller than 75 (labeled by a "d" in Table 4 in the appendix). The second group includes all cancer classes because of their clinical relevance. Both class groups, but especially the cancer classes, experience notable performance improvements from MultiTalent. For the official BTCV test set in \autoref{BTCV_leaderboard}, MultiTalent outperforms all related work that have also incorporated multiple datasets during training,  proving that MultiTalent is substantially superior to related approaches. The advantages of MultiTalent include not only better segmentation results, but also considerable time savings for training and inference due to the simultaneous prediction of all classes. The training is 6.5 times faster and the inference is around 13 times faster than an ensemble of models trained on 13 datasets.\\ \\
%It is possible that transformer-inspired architectures need much more labeled images for each class in order to be trained in such a general fashion. \\ \\
\textbf{Transfer learning results} are found in \autoref{transfer_table}, which compares the fine-tuned 5-fold cross-validation results of different pre-training strategies for three different models on three datasets. The MultiTalent pre-training is highly beneficial for the convolutional models and outperforms all unsupervised baselines. Although MultiTalent was trained with a substantially lower amount of manually annotated structures (\raisebox{-0.9ex}{\~{}}3600 vs. \raisebox{-0.9ex}{\~{}} $10^5$ annotations), it also exceeds the supervised pre-training baseline. Especially for the small multi-organ dataset, which only has 30 training images (BTCV), and for the kidney tumor (KiTs19), the MultiTalent pre-training boosts the segmentation results. In general, the results show that supervised pre-training can be beneficial for the SwinUNETR as well, but pre-training on the large TotalSegmentator dataset works better than the MD pre-training. 
For the AMOS dataset, no pre-training scheme has a substantial impact on the performance. We suspect that it is a result of the dataset being saturated due to its large number of training cases. The Resenc U-Net pre-trained with MultiTalent, sets a new state-of-the-art on the BTCV leaderboard\footnote{Assuming that no additional private data from the same data domain has been used.}(\autoref{BTCV_leaderboard}).

\section{Discussion}
MultiTalent demonstrates the remarkable potential of utilizing multiple publicly available partially labeled datasets to train a foundation medical segmentation network, that is highly beneficial for pre-training and finetuning various segmentation tasks.
MultiTalent surpasses state-of-the-art single-dataset models and outperforms related work for multi dataset training, while retaining conflicting annotation protocol properties from each dataset and allowing overlapping classes. Furthermore, MultiTalent takes less time for training and inference, saving resources compared to training many single dataset models. In the transfer learning setting, the feature representations learned by MultiTalent boost segmentation performance and set a new state-of-the-art on the BTCV leaderboard.
%Currently the benefits observed for CNNs do not completely transfer to the transformer-based SwinUNETR. Only for finetuning, the pre-training with MultiTalent slightly improves the performance compared to training SwinUNETR from scratch. Future work will focus on MD training stability, training schedules and optimized hyperparameter selection for transformer inspired architecture. 
The nature of MultiTalent imposes no restrictions on additional datasets, which allows including any publicly available datasets (e.g. AMOS and TotalSegmentator). This paves the way towards holistic whole body segmentation model that is even capable of handling pathologies. 

\newpage
\bibliographystyle{splncs04}
\bibliography{bibfile.bib}
%
\newpage

\section*{Appendix}
\begin{table}
\centering
\captionsetup{type=table}
\captionof{table}{An overview of the 13 datasets that were used for the multi-class training. The last two datasets were only used for transfer learning experiments.} \label{tab_datasets}
\begin{adjustbox}{width=1\textwidth}
\begin{tabular}{|l|l|c|c|c|}
\hline 
 &   & Train &  & \\ 
Dataset & Labels & images & Median shape& Spacing [mm]\\
\hline
01 Decathlon Task 03 \cite{Antonelli2022}&   Liver, Liver tumor & 131  & 432x512x512&(1, 0.77, 0.77)\\ 
02 Decathlon Task 06 \cite{Antonelli2022}&   Lung nodules & 63 & 252x512x512 & (1.24, 0.79, 0.79)\\
03 Decathlon Task 07 \cite{Antonelli2022}&   Pancreas, Pancreas tumor & 281 & 93x512x512 & (2.5, 0.80, 0.80)\\
04 Decathlon Task 08 \cite{Antonelli2022} &   Hepatic vessels, Hepatic tumor & 303 & 49x512x512 & (5, 0.80, 0.80)\\
05 Decathlon Task 09 \cite{Antonelli2022}&   Spleen & 41 & 90x512x512 & (5, 0.79, 0.79) \\
06 Decathlon Task 10 \cite{Antonelli2022}&  Colon cancer & 126 & 95x512x512 &  (5, 0.78, 0.78)\\
07 BTCV \cite{BTCV} & 13 Abdominal Organs %\tablefootnote[1]{Spleen, Right Kidney, Left Kidney, Gallbladder, Eesophagus, Liver, Stomach, Aorta, Inferior Vena Cava, Portal Vein and Splenic Vein, Pancreas, Right Adrenal Gland, Left Adrenal Gland}
& 30& 128x512x512 & (3, 0.76, 0.76)\\
08 Pelvis \cite{BTCV} & Uterus, Bladder, Rectum, Small Bowel  & 30 & 180x512x512 & (2.5, 0.98, 0.98)\\
09 BTCV2 \cite{BTCV2} & 8 Abdominal Organs %\tablefootnote[2]{Spleen, Left Kidney, Gallbladder, Esophagus, Liver, Stomach, Pancreas} 
& 73 \tablefootnote[1]{Originally 90 images, but we removed duplicated images of the BTCV test set} & 185x512x512 & (3, 0.79, 0.79)\\
10 StructSeg Task 3 \cite{structseg} & 5 thoracic organs%\tablefootnote[4]{Left Lung, Right Lung, Spinal Cord, Esophagus, Heart, Bronchies} 
& 50 & 95x512x512 & (5, 1.17, 1.17)\\
11 SegTHOR \cite{lambert2019segthor} & Heart, Aorta, Esophagus, Trachea & 40 & 178x512x512 & (2.5, 0.98, 0.98)\\
12 NIH-Pan \cite{roth2015deeporgan,clark_cancer_2013,NHpancreas} & Pancreas & 82& 217x512x512 & (1, 0.86, 0.86)\\
13 KiTS19 \cite{heller2020kits19} & Kidney, Kidney Tumor & 210 & 107x512x512 & (3, 0.78, 0.78)\\
Amos \cite{ji2022amos}& 15 abdominal organs & 300 & 104x512x512 & (5, 0.68, 0.68)\\
TotalSegmentator \cite{totalseg}& 104 classes & 1204 & 231x231x240& (1.5, 1.5, 1.5)\\ 

\hline
\end{tabular}
\end{adjustbox}

\vspace*{0.5 cm}


\centering
\captionsetup{type=figure}
\includegraphics[width=0.85\textwidth]{dataset_overview.eps}
\captionof{figure}{The circle area corresponds to each dataset size and the color indicates the number of annotated classes. MultiTalent was trained on 1460 images with about 3600 annotations. Whereas, the TotalSegmentator dataset consists of 1204 images with about $10^5$ annotations \cite{totalseg}.}
\label{fig:dataset_overview}

\end{table}
\begin{table}
\centering
\caption{Dice (5 fold) for all classes within all datasets. All networks but the default nnUNet are trained with batch size 4 and patch size [96,192,192]. MD indicates the MultiTalent networks that were trained with multiple datasets together. All cancer classes are labeled with a c and all classes, for which the default nnUNet Dice is smaller than 75 are labeled with a d. }
\label{big_table}
\begin{adjustbox}{width=0.85\textwidth}
\begin{tabularx}{\textwidth}{ l| *{8}{Y|} }

\hline
{Classes} & Class groups & nnU-Net &  SwinU &  U-Net &  Resenc &  MD SwinU &  MD U-Net &  MD Resenc \\
\hline
1 liver w/o cancer      &  - &   95.71 &  96.14 & 96.34 &   \textbf{96.48} &     95.94 &    96.10 &      96.27 \\
1 liver tumor           & d, c &   63.72 &  61.78 & 65.89 &   \textbf{67.53} &     58.09 &    65.83 &      66.50 \\
2 lung\_nodule            & d, c &   \textbf{72.11} &  65.31 & 67.94 &   68.22 &     62.40 &    68.43 &      70.49 \\
2 pancreas w/o cancer     &-  &   \textbf{82.17} &  80.19 & 81.52 &   81.91 &     79.04 &    81.35 &      81.85 \\
3 pancreas cancer         & d, c&   52.74 &  45.53 & 49.73 &   54.18 &     49.12 &    56.67 &      \textbf{57.71} \\
4 hepatic vessel          & d&   \textbf{64.56} &  63.77 & 63.78 &   63.60 &     64.14 &    64.15 &      64.33 \\
4 liver cancer            & d, c&   72.17 &  66.18 & 71.04 &   71.62 &     67.18 &    72.78 &      \textbf{73.88} \\
5 spleen                  &- &   96.38 &  97.02 & \textbf{97.08} &   97.05 &     96.20 &    96.61 &      96.67 \\
6 colon cancer            &d, c &   45.53 &  41.75 & 47.23 &   47.40 &     47.56 &    56.73 &      \textbf{58.69} \\
7 spleen                  & -&   90.83 &  91.14 & 92.72 &   92.34 &     91.04 &    \textbf{93.75} &      93.61 \\
7 right kidney            &- &   89.39 &  88.59 & \textbf{90.93} &   90.33 &     86.93 &    90.90 &      90.89 \\
7 left kidney             & -&   86.75 &  86.80 & 90.18 &   90.65 &     86.83 &    \textbf{90.75} &      90.47 \\
7 gallbladder             &d &   66.32 &  66.21 & 69.24 &   69.78 &     67.73 &    69.82 &      \textbf{72.12} \\
7 esophagus               &- &   78.40 &  77.87 & 78.56 &   78.96 &     77.87 &    \textbf{79.73} &      79.28 \\
7 liver            & - &   95.57 &  95.36 & 95.74 &   95.95 &     95.14 &    96.12 &   \textbf{96.23} \\
7 stomach                 &- &  88.16 &  86.71 & 90.87 &   92.76 &     86.77 &    91.11 &         \textbf{92.83} \\
7 aorta                   &- &   92.29 &  92.15 & \textbf{92.75} &   \textbf{92.75} &     89.92 &    91.32 &      91.60  \\
7 vena cava inferior         & - &   86.38 &  85.35 & 86.16 &   86.73 &     84.34 &    86.26 &   \textbf{87.31} \\
7 portal and splenic vein    & - &    76.59 &  73.84 & 77.29 &   77.74 &     73.88 &    \textbf{77.76} &      77.58  \\
7 pancreas & -&  81.76 &  79.26 & 83.10 &   83.87 &     82.27 &    84.62 & \textbf{84.92} \\
7 right adrenal gland      & d &  71.48 &  67.69 & 70.90 &   72.37 &     66.02 &     \textbf{72.70} &      \textbf{72.70} \\
7 left adrenal gland       & d&   72.38 &  67.78 & 72.03 &   \textbf{72.77} &     64.65 &    72.17 &      71.62 \\
8 bladder                & - &   88.93 &  88.26 & 89.26 &   89.41 &     84.02 &    90.73 &      \textbf{91.59} \\
8 uterus                 & - &   \textbf{80.72} &  78.94 & 80.34 &   80.57 &     76.06 &    79.64 &      78.89 \\
8 rectum                 & d &   73.77 &  71.07 & 73.62 &   \textbf{75.24} &     67.06 &    73.25 &      74.17 \\
8 small bowel           &  d &    56.52 &  54.16 & 56.12 &   57.27 &     52.77 &    \textbf{57.68} &      56.67 \\
9 spleen                  &- &   94.59 &  93.44 & 95.47 &   \textbf{95.86} &     93.82 &    95.53 &      95.49 \\
9 left kidney            & - &   93.29 &  92.60 & 93.81 &   \textbf{94.38} &     91.69 &    93.71 &      93.72 \\
9 gallbladder             & -&   78.44 &  78.17 & 81.30 &   81.07 &     77.21 &    80.63 &      \textbf{81.76} \\
9 esophagus               & d &  74.46 &  74.10 & 74.92 &   \textbf{75.79} &     72.71 &    75.03 &      75.01 \\
9 liver                   &- &   96.03 &  95.90 & 96.32 &   \textbf{96.50} &     95.41 &    96.09 &      96.19 \\
9 stomach                 &- &   91.77 &  90.90 & \textbf{93.32} &   93.09 &     88.75 &    92.17 &      92.84 \\
9 pancreas                &- &   84.04 &  83.20 & 84.95 &   85.48 &     84.05 &    85.74 &      \textbf{86.16} \\
9 duodenum                &- &   75.62 &  72.47 & 77.35 &   \textbf{78.70} &     68.96 &    75.92 &      77.60 \\
10 left lung              & - &   95.99 &  95.89 & 95.99 &   95.97 &     95.48 &    95.95 &       \textbf{96.03} \\
10 right lung             & - &   96.74 &  96.65 & 96.77 &   96.74 &     96.25 &    96.73 &      \textbf{96.78} \\
10 heart                   & -&   \textbf{94.17} &  93.87 & 93.91 &   94.05 &     93.02 &    93.66 &      94.01 \\
10 esophagus               &- &   80.19 &  79.97 & 81.30 &   \textbf{81.39} &     78.91 &    80.87 &      81.06 \\
10 bronchies               &- &   \textbf{84.16} &  83.44 & 83.88 &   83.51 &     83.13 &    83.23 &      83.64 \\
10 spina cord nerve  &- & 90.14 &  \textbf{90.17} & 90.08 &   90.14 &     89.51 &    89.69 &      89.94 \\
11 esophagus              & - &     84.95 &  83.18 & 84.83 &   \textbf{85.56} &     80.53 &    83.21 &      84.61 \\
11 heart                  & - &   95.27 &  94.67 & 95.11 &   \textbf{95.29} &     93.36 &    94.40 &      94.83 \\
11 trachea                & - &   90.55 &  90.61 & 90.61 &   90.59 &     89.50 &    90.50 &      \textbf{90.96} \\
11 aorta                  & - &   94.26 &  93.70 & 94.11 &   \textbf{94.29} &     92.07 &    93.00 &      93.82 \\
12 pancreas               & - &   86.59 &  85.47 & 86.10 &   86.27 &     85.74 &    86.80 &      \textbf{87.21} \\
13 both kidneys w/o tumor   &- &    \textbf{97.02} &  96.19 & 96.72 &   96.92 &     95.37 &    96.11 &      96.34 \\
13 kidney tumor            &c &    83.67 &  78.90 & 81.70 &   82.64 &     74.54 &    81.68 &      \textbf{83.79} \\
\hline
Class average                     & -&  82.62 &  81.11 & 82.96 &   83.44 &     80.28 &    83.35 &      \textbf{83.84} \\
%\midrule
%Dataset average                 & -&  79.24 &  77.25 & 79.16 &   79.69 &     76.69 &    80.21 &      \textbf{80.93} \\
%\midrule
%Average difficult tasks        &d&   65.20 &  62.15 & 65.20 &   66.36 &     61.62 &    67.10 &      \textbf{67.82} \\
%\midrule
%Average cancer tasks       & c&  64.42 &  59.99 & 63.92 &   65.35 &     59.81 &    67.02 &      \textbf{68.51} \\
\hline
\end{tabularx}
\end{adjustbox}
\end{table}
\end{document}

% % LLNCS macro package for Springer Computer Science proceedings;
% % Version 2.20 of 2017/10/04
% %
% \documentclass[runningheads]{llncs}

% %
% \usepackage{graphicx}
% \usepackage{hyperref}
% \usepackage{tablefootnote}
% \usepackage{subcaption}
% \usepackage{amsmath}
% \usepackage{mathtools}
% \usepackage{booktabs}
% \usepackage{float} %table positioning
% \usepackage{adjustbox}
% \usepackage{tabularx}
% \newcolumntype{Y}{>{\centering\arraybackslash}X}
% \usepackage{todonotes}
% \usepackage{bbold}
% \usepackage{arydshln} %dashed lines in table


% \hypersetup{
% colorlinks=true,
% linkcolor=black,
% filecolor=black,
% citecolor = black,
% urlcolor=blue,}



% % Used for displaying a sample figure. If possible, figure files should
% % be included in EPS format.
% %
% % If you use the hyperref package, please uncomment the following line
% % to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\urlcolor{blue}\rmfamily}

% \begin{document}
% \let\ref\autoref
% %
% \title{Pathology Aware Whole-Body Segmentation: Combining Partially Labelled Datasets For Multi-Class Segmentation}
% %
% \titlerunning{Combining Partially Labelled Datasets For Multi-Class Segmentation}
% % If the paper title is too long for the running head, you can set
% % an abbreviated paper title here
% %

% \author{anonymized}
% \authorrunning{anonymized}


% % \author{Constantin Ulrich\inst{1} \and
% % Fabian Isensee\inst{1,2} \and
% % Tassilo Wald\inst{1,2} \and
% % Maximilian Zenk\inst{1,2} \and
% % Michael Baumgartner\inst{1,2} \and
% % Klaus H. Maier-Hein\inst{1,3}}
% % %
% % \authorrunning{C. Ulrich et al.}
% % % First names are abbreviated in the running head.
% % % If there are more than two authors, 'et al.' is used.
% % %
% % \institute{Division of Medical Image Computing, German Cancer Research Center (DKFZ),
% % Heidelberg, Germany\\ \email{constantin.ulrich@dkfz-heidelberg.de}
% % \and Helmholtz Imaging, DKFZ, Heidelberg, Germany\\ \email{f.isensee@dkfz-heidelberg.de
% % }
% % \and Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, Heidelberg, Germany} 
% %
% \maketitle % typeset the header of the contribution
% %
% \begin{abstract} \todo{Title to be discussed, I am already exceeding the page limit. Please indicate when you consider a part to be redundant}
% Datasets in the medical domain are notoriously difficult and labor intensive to annotate. Consequently, the number of annotated target structures is always limited strictly to the current task at hand, resulting in a veritable zoo of partially annotated datasets. Thus,  a complete segmentation of all combined anatomical entities including rare pathologies would currently require the training of many independent models (one per dataset). To unlock the hidden synergies of these datasets, we introduce a training method which is able to leverage multiple partially labeled datasets in order to train one network to segment all target structures simultaneously. Our method enables the integration of multiple datasets with arbitrary classes, even with conflicting class definitions and overlapping target structures. Thus, there are no restrictions when incorporating additional datasets, including those that contain rare pathologies. The multi-dataset (MD) training improves segmentation results compared to state-of-the-art solutions for single datasets, with cancer classes benefiting the most. Additionally, the MD training produces very generalizable feature representations that are beneficial for various segmentation tasks in a transfer learning context. \todo{maybe add sentence how this could affect the field? nnunet highly used - providing weights?}

% \keywords{Medical image segmentation \and multitask learning \and nnU-Net \and partially labeled datasets.}
% \end{abstract}
% \section{Introduction}

% \begin{figure}[t]
% \includegraphics[width=1\textwidth]{overrview_figure.eps}
% \caption{(a) Usually only a few classes are annotated in publicly available datasets. b) Different groundtruth label properties can generate contradicting class predictions. For example, the heart annotation of dataset 11 differs from the heart annotation of dataset 10, which causes the aorta of dataset 11 to overlap with the heart of dataset 10. In contrast to dataset 11, in dataset 7 the aorta is also annotated in the lower abdomen. c) Instead of training one network for each dataset, we introduce a method to train one network with all datasets, while retaining dataset-specific annotation protocols.} \label{overview_figure}
% \end{figure}
% For 3D medical images, the annotation is a difficult and time-consuming task, which requires specific domain knowledge. Thus, only the bare minimum of target structures, needed to solve the task at hand, is annotated, causing a zoo of highly specific datasets. Even recently published large datasets with multiple annotated classes do not include annotations for all interesting target structures \cite{ji2022amos,totalseg}. Adding a new class to one of these datasets would demand manual annotations for all images of the dataset. In practice, for example for rare disease classes, a few medical centers gather task specific patient images. A potential disease class and a few neighboring structures are then annotated by domain experts to create a new partially labeled dataset. Whenever segmentations for structures from various datasets are required, individual models must be trained for each dataset. This imposes a substantial computational burden and environmental footprint, both during training as well as during inference. Additionally, the individual models are unable to exploit potential synergies between the datasets. \\
% An method leveraging partially labeled datasets should meet the following criteria: First, it must be able to handle absent classes of each dataset. Second, it needs to retain different annotation protocol characteristics for the same target structure (visualized in \ref{overview_figure} b) ) and must allow for overlapping labels for different fine grainedness such as liver, liver vessel and liver tumor. Third, it should achieve equal or better performance than cutting-edge single-dataset methods and faster training and inference compared to ensembling multiple models trained on each dataset individually.\\
% Recent advancements in the field have led to the development of techniques that utilize partially labeled datasets. Some methods incorporate the information about the class that is to be segmented at different stages of the network \cite{dmitriev_learning_2019,zhang_dodnet_2020,clip_medicalseg}, but this limits their ability to predict all classes simultaneously. Other approaches handle missing annotations by considering them as background \cite{roulet_joint_2019,fang_multi-organ_2020} and penalize overlapping predictions by taking advantage of the fact that organs are mutually exclusive \cite{shi_marginal_2020,fidon_label-set_2021}. However, these techniques are typically not optimized for overlapping target structures, such as vessels or cancer classes within an organ, and are primarily geared towards multi-organ segmentation \cite{feng2021mskd,filbrandt_learning_2021,contextaware,teacher_student}. None of the previous methods convincingly leverage cross-dataset synergies and utterly fail the comparison with cutting-edge segmentation methods for single datasets. We believe this is due to the challenges in combining labels from different datasets for the same target structure since this would force the segmentation network to average between distinct annotation characteristics. In fact, Liu et al. have identified this label inconsistency problem as an open challenge that has not yet been resolved in a related work \cite{clip_medicalseg}.\\
% To address this issue, we propose a more flexible method to train a network with partially labeled datasets. By implementing a dataset and class adaptive loss function that allows for overlapping target structures, our method introduces an independent segmentation layer for each class in each new dataset. Consequently, there are no restrictions on the incorporation of additional datasets with new relevant classes such as pathologies, and we even preserve the label properties of each class within each dataset. Our Multi-dataset (MD) training method outperforms state-of-the-art segmentation networks trained on each dataset individually while having the advantage of shorter training and inference times compared to single dataset solutions. The benefits of co-training with other partially labeled datasets are particularly notable for rare pathologies such as cancer classes. \todo{hopefully BTCV updates leaderboards} \\
% Transfer learning offers another option for leveraging several partially labeled datasets in cases when memory is limited and data access is restricted. At the example of three challenging datasets we demonstrate that fine tuning our MD model yields higher segmentation performance than training from scratch or initializing the model parameters using other pre-training strategies.




% \section{Methods}
% We introduce a novel class and dataset adaptive loss function for training arbitrary segmentation networks with a collection of partially labeled datasets. This loss function, with minimal modifications to the network architecture, enables the preservation of all label properties, overlapping labels and the simultaneous prediction of all classes. Furthermore, we introduce a modified training schedule balancing various dataset sizes and different numbers and sizes of annotated classes. 
% % In \ref{Training_strategy}, a detailed description of the implementation details for the MD training and subsequent transfer learning experiments is provided.

% \subsection{Dataset and class adaptive loss}
% \label{loss_function}
% We begin with a dataset collection of $K$ datasets $D^{(k)}, k \in [1, K]$, with $N^{(k)}$ image and label pairs $D^{(k)} = \{(x,y)^{(k)}_1,..., (x,y)^{(k)}_N\}, |D^{(k)}| = N^{(k)}$. Every image voxel $x^{(k)}_i, i \in [1,I]$, is assigned to one class $c \in C^{(k)}$, where $C^{(k)} \subseteq C$ is the labelset associated to dataset $D^{(k)}$. We consider each class as unique, even if classes from different dataset refer to the same target structure, because the exact annotation protocols and labeling characteristics of the annotations are unknown and can vary between datasets: $C^{(k)} \cap C^{(j)} = \emptyset, \forall k \neq j$. \\
% However, this implies that the network must be capable of predicting multiple classes for one voxel to account for the inconsistent class definitions. We solve this label contradiction problem by decoupling the segmentation outputs for each class by applying the Sigmoid activation function instead of the commonly used Softmax activation function. The network shares the same backbone parameters $\Theta$ but it has independent segmentation head parameters $\Theta_c$ for each class. The Sigmoid probabilities for each class are defined as $\hat{y}_c=f(x,\Theta, \Theta_c)$. This modification allows the network to assign multiple classes to one pixel and thus enables overlapping classes and the conservation of all label properties from each dataset. Consequently, the segmentation of each class can be thought of as a binary segmentation task. Based on the well established combination of a Cross-entropy and Dice loss for single dataset medical image segmentation, we employ the binary Binary Cross-entropy loss (BCE) and a modified Dice loss for each class over all $B, b\in [1,B]$, images in a batch: 

% \begin{align*}
%      L_c &= \frac{1}{I}\sum_{b,i} BCE(\hat{y}^{(k)}_{i,b,c}, \; y^{(k)}_{i,b,c})  -\frac{2\sum_{b,i}\hat{y}^{(k)}_{i,b,c} \; y_{{i,b,c}}^{(k)}} {\sum_{b,i}\hat{y}^{(k)}_{i,b,c} +\sum_{b,i} y_{i,b,c}^{(k)}}
% \end{align*}
% While the regular dice loss is calculated for every image within a batch, we calculate the dice loss across all images of the input batch. This regularizes the loss if only a few voxels of one class are present in one image and a larger area is present in another image of the same batch. Thus, an inaccurate prediction of the few pixels in the first image has a limited effect on the loss. In the following, we unite the sum over the image voxels $i$ and the batch $b$ to $\sum_{z}$.
% Based on previous work, we modify the loss function to be calculated only for classes that were annotated in the corresponding partially labeled dataset \cite{roulet_joint_2019,fang_multi-organ_2020}, indicated by $\mathbb{1}_c^{(k)}$ in the following. Instead of averaging, we add up the loss over the classes. Hence, the loss signal for each class prediction does not depend on the number of other classes within the batch. This compensates for the varying number of annotated classes in each dataset:
% \begin{align*}
%     L &= \sum_{c} \Big( \mathbb{1}_c^{(k)} \frac{1}{I}\sum_{z} BCE(\hat{y}^{(k)}_{z,c}, \;y^{(k)}_{z,c})  -\frac{2\sum_{z}\mathbb{1}_c^{(k)} \; \hat{y}^{(k)}_{z,c} \; y_{z,c}^{(k)}} {\sum_{z}\mathbb{1}_c^{(k)} \; \hat{y}^{(k)}_{z,c} +\sum_{z}\mathbb{1}_c^{(k)} \; y_{z,c}^{(k)}} \Big)
%      %L &= \sum_{c} \left (\sum_{b} e_c^k BCE(f(x_{b}^k, \theta)_c,y_{c,b}^k)  -  \frac{2\sum_{b} e_c^k f(x^k_b, \theta)_c y_{b,c}^)} {\sum_{b} e_c^k f(x_b^k, \theta)_c +\sum_{b} e_c^k y_{b,c}^k}\right)
% \end{align*}

% \begin{align*}
%     \mathbb{1}_c^{(k)} &= 
%     \begin{dcases}
%         1, \text{if } c \in C^{(k)}\\
%         0,  \text{otherwise}
%     \end{dcases}
% \end{align*} 


% %Given a dataset collection $\mathrm{D}$ including a total number of classes C, there are for each Dataset $\mathrm{D_k\in D}$ n image and ground truth label pairs  $\mathrm{(x,y)_{n}^k \in D_k}$. Each ground truth label map consist of $\mathrm{l_k \subset C}$ subset classes corresponding to the respective dataset. First, we extend the Dice loss to be calculated over the whole input batch to provide an additional regularization. Since multiple images within a batch can be sampled from the same dataset, this prevents the loss function to become to much influenced by one dataset:



% \subsection{Training strategy}
% \label{Training_strategy}
% \noindent \textbf{Multi-dataset training}
% We trained our method with 13 public abdominal CT datasets with a total of 1477 images, including 47 classes. Detailed information about the datasets, including the corresponding annotated classes, can be found in the supplementary information in \ref{tab_datasets}. We implemented our approach in the \href{https://github.com/MIC-DKFZ/nnUNet}{nnU-Net} framework \cite{isensee_nnu-net_2021}. 
% However, the automatic pipeline configuration from nnU-net was not used in favor of a manually defined configuration that reflects the peculiarities of each of the datasets, irrespective of the number of training cases they contain. We manually selected a patch size patch size of $[96, 192,192]$ and image spacing of 1mm in plane and 1.5mm for the axial slice thickness. Additionally, we increased the batch size to 4 and the number of training epochs to 2000 to account for the high number of training images. To compensate for the varying number of training images in each dataset, we choose a sampling probability inversely proportional to $\sqrt{n}$, where $n$ is the number of training cases in the corresponding source dataset.\\ 
% Our suggested class adaptive loss function can be applied to every 3D segmentation network without any architectural modifications. 
% Therefore, we employed not only the default 3D U-Net network but also an extension with additional residual blocks in the encoder (Resenc U-Net), that demonstrated highly competitive results in previous medical image segmentation challenges \cite{extendingnnunet,fabi_kits} and a recently proposed transformer based architecture (SwinUNETR \cite{swinunetr}). The network topology of the first two convolutional architectures was automatically generated by the nnU-Net based on the manually selected patch size $[96, 192,192]$ and image spacing (1mm in plane and 1.5mm for the axial slice thickness). For the SwinUNETR, we adopted the default network topology. To ensure reproducibility and comparability with existing work, all experiments were conducted with the 5-fold split that nnU-Net would generate for each individual dataset. As a baseline for our method, we applied a 3D "fullres" nnU-Net without manual intervention to each dataset individually. Furthermore, we trained a 3D U-Net, a Resenc U-Net and a SwinUNETR with the same network topology, patch and batch size as our MD training for each dataset. All baseline networks were also implemented within the nnU-Net framework and follow the default training procedure. \\ \\
% \noindent \textbf{Transfer learning}
% \label{transfer_methods}
% We used the BTCV (small multi organ dataset \cite{BTCV}), AMOS2022 (large multi organ dataset \cite{ji2022amos}) and KiTS2019 (pathology dataset \cite{heller2020kits19}) datasets to evaluate the generalizability of the MD features in a fine tuning setting. Naturally, these three datasets were excluded from the repective MD training. Fine tuning was performed with identical configuration as the source training, except for the batch size which was set to 2. We closely followed the learning rate schedule proposed by Kumar et al. \cite{transferschedule}. First, the segmentation heads were warmed up over 10 epochs with linearly increasing learning rate, followed by a whole-network warum-up over 50 epochs. Finally, we continued with the standard nnU-Net training schedule.
% We compare the utility of features generated by MD training with supervised and unsupervised baselines. As supervised baseline, we used the weights resulting from training the three model architectures on the Totalsegmentator dataset, which consists of 1204 images and 104 classes \cite{totalseg}. Here, we used the same patch size, image spacing, batch size and number of epochs as for the MD training. As unsupervised baseline for the CNNs, we pre-trained the networks on the MD collection based on the work of Zhou at al. \cite{ZHOU2021101840}. It applies non-linear intensity transformations, in- and out-painting, and local pixel shuffling for a self supervised reconstruction task.  
% Finally, for the SwinUNETR architecture, we compared the utility of the weights from our MD training with the ones provided by Tan et al. who performed self-supervised pre-training on 5050 CT images. This necessitated the use of the original implementation of \href{https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR}{SwinUNETR} because the recommended settings for fine tuning were used. This should serve as additional external validation of our model. To ensure fair comparability, we did not scale up any models. Despite using gradient checkpointing, the SwinUNETR models requires roughly 30 GB of GPU memory, compared to less than 17 GB for the CNNs.




% %Furthermore, we compared our multi-class network with state-of-the-art methods for single datasets and available related multi-class methods on public leaderboards \cite{isensee_nnu-net_2021,zhang_dodnet_2020,chen_med3d_2019,zhou2019prioraware,fang_multi-organ_2020}. By choosing these baselines, we clearly distinguish this work from related work that evades the highly competitive comparison with state-of-the-art methods for each dataset.


% \section{Results}
% %In this section, we first present the results of training one network on multiple datasets. After that, we show that this multi-dataset training results in a promising general feature representation that is highly beneficial for transfer learning.
% \noindent \textbf{Multi-dataset training results} 
% \ref{barplot_results} shows the 5-fold cross-validation results of our multi-dataset (MD) training. Since a simple average over all classes would introduce a biased perception due to the highly varying numbers of images and classes, we additionally report an average over all datasets. For example, dataset 7 consists of only 30 training images but has 13 classes, whereas dataset 6 has 126 training images but only 1 class. \ref{big_table} provides all results for all classes. The MD training improves the performance of the purely convolutional architectures (U-Net and Resenc U-Net) and outperforms the corresponding baseline models that were trained on each dataset individually. Averaged over all datasets, the MD training gains 1.24 Dice points for the Resenc U-Net architecture and 1.05 Dice points for the U-Net architecture. Compared to the default nnU-Net, configured without manual intervention for each dataset, the improvements are 1.69 and 0.97 Dice points.\\
% Additionally, in \ref{barplot_results} we analyzed two subgroups of classes. The first group includes all "difficult" classes for which the default nnU-Net has a Dice smaller than 75 (labeled by a "d" in \ref{big_table}). The second group includes all cancer classes because they have the most significant clinical relevance. Both class groups, but especially the cancer classes, experience notable performance gains from the MD training. The advantages of MD training include not only better segmentation results, but also considerable time savings for training and inference due to the simultaneous prediction of all classes. The training is 6.5 times faster and the inference is around 13 times faster than an ensemble of models trained on 13 datasets individually.
% SwinUNETR was severely outperformed by the CNNs on each dataset and the MD training yielded no positive benefits. It is possible that transformer-inspired architectures need much more labeled images for each class in order to be trained in such a general fashion. \\ \\
% \begin{figure}[h]
%     \includegraphics[width=1\textwidth]{barplot_yaxixbreak.eps}
%     \caption{Comparison of Dice scores averaged over all classes, all datasets or over classes of special interest. Difficult classes are those for which the default nnUNet has a Dice below 75. The same color indicates the same network architecture and the pattern implies that the network was trained with multiple datasets (MD).} \label{barplot_results}
% \end{figure}
% \noindent \textbf{Transfer learning results}
% \ref{transfer_table} compares the 5-fold cross-validation results of different pre-training strategies for three different models on three datasets. The MD pre-training is highly beneficial for the convolutional models and outperforms all unsupervised baselines. Especially for the small multi-organ dataset, which only has 30 training images (BTCV), and for the kidney tumor class from the KiTS19 dataset, the MD pre-training boosts the segmentation results. For the AMOS22 dataset, no pre-training scheme has a substantial impact on the performance. We explain this by the dataset already being saturated due to its large number of training cases. Still, a 5-fold ensemble of the Resenc U-Net with MD pre-training achieves a new state-of-the-art on the public AMOS leaderboard (Average Dice: 91.48, Normalized Surface Dice: 83.41, 16.01.2023).
% On the KiTS19 public leaderboard, a 5-fold ensemble of the Resenc U-Net with multi dataset pre-training achieves a kidney Dice of 97.28 and a tumor Dice of 82.60 (3D fullres nnU-Net: 97.13 and 82.13).
% \todo{add leaderboard results, when btcv is back}\\
% In general, the results show that supervised pre-training can be beneficial for the SwinUNETR as well, but pre-training on the large Totalsegmentator dataset works better than the MD pre-training. As mentioned above, we believe that SwinUNETR needs significantly more annotations per class to be able to learn a generalizable feature presentation.

% \section{Discussion}
% Our findings demonstrate the remarkable potential of utilizing multiple publicly available partially labeled datasets to train a convolutional segmentation network. Our solution retains annotation protocol properties from each dataset and allows for overlapping classes. Thus, there are no restrictions on including additional datasets with arbitrary classes in the future. We achieved a considerable performance improvement compared to state-of-the-art single-dataset models. We observed the largest performance gains for more difficult classes and in particular for cancer classes, for which the improvement was more than 3.2 Dice points for the CNNs. This MD training also resulted in considerable time savings for both training and inference as it allowed for the simultaneous prediction of all classes.
% The feature representations, learned by the MD training, result in a significant boost in segmentation performance on all tasks in a transfer learning setting. In fact, this approach allowed us to set a new state-of-the-art on the AMOS multi-organ segmentation leaderboard. \\
% In contrast to the CNNs, we could not replicate the positive benefits of the MD training for the SwinUNETR.
% Overall, our results provide evidence that, with given memory and compute constraints, transformer-inspired architectures might not (yet) be able to compete with state-of-the-art CNNs for 3D medical image segmentation.
% Since our MD approach does not have any restrictions on including additional datasets, we plan to expand our network with more partially labeled datasets (e.g. AMOS and Totalsegmentator) to generate a pathology aware holistic whole body segmentation model. 
% \include{pre-training_table.tex}

% \newpage
% \bibliographystyle{splncs04}
% \bibliography{bibfile.bib}
% %
% \newpage

% \section*{Appendix}
% \begin{table}
% \centering
% \caption{An overview of the 13 datasets that were used for the multi-class training. We used all training images for a 5-fold cross-validation and the appropriate test images for the public leaderboard submission. Accordingly, there are no ground truth annotations for the test images available. The last two datasets were only used for transfer learning experiments.} \label{tab_datasets}
% \begin{adjustbox}{width=1\textwidth}
% \begin{tabular}{|l|l|c|c|c|}
% \hline 
%  &   & Train &  & \\ 
% Dataset & Labels & images & Median shape& Spacing [mm]\\
% \hline 
% 01 Decathlon Task 03 \cite{Antonelli2022}&   Liver, Liver tumor & 131  & 432x512x512&(1, 0.77, 0.77)\\ 
% 02 Decathlon Task 06 \cite{Antonelli2022}&   Lung nodules & 63 & 252x512x512 & (1.24, 0.79, 0.79)\\
% 03 Decathlon Task 07 \cite{Antonelli2022}&   Pancreas, Pancreas tumor & 281 & 93x512x512 & (2.5, 0.80, 0.80)\\
% 04 Decathlon Task 08 \cite{Antonelli2022} &   Hepatic vessels, Hepatic tumor & 303 & 49x512x512 & (5, 0.80, 0.80)\\
% 05 Decathlon Task 09 \cite{Antonelli2022}&   Spleen & 41 & 90x512x512 & (5, 0.79, 0.79) \\
% 06 Decathlon Task 10 \cite{Antonelli2022}&  Colon cancer & 126 & 95x512x512 &  (5, 0.78, 0.78)\\
% 07 BTCV \cite{BTCV} & 13 Abdominal Organs %\tablefootnote[1]{Spleen, Right Kidney, Left Kidney, Gallbladder, Eesophagus, Liver, Stomach, Aorta, Inferior Vena Cava, Portal Vein and Splenic Vein, Pancreas, Right Adrenal Gland, Left Adrenal Gland}
% & 30& 128x512x512 & (3, 0.76, 0.76)\\
% 08 Pelvis \cite{BTCV} & Uterus, Bladder, Rectum, Small Bowel  & 30 & 180x512x512 & (2.5, 0.98, 0.98)\\
% 09 BTCV2 \cite{BTCV2} & 8 Abdominal Organs %\tablefootnote[2]{Spleen, Left Kidney, Gallbladder, Esophagus, Liver, Stomach, Pancreas} 
% & 73 \tablefootnote[1]{Originally 90 images, but we removed duplicated images of the BTCV test set} & 185x512x512 & (3, 0.79, 0.79)\\
% 10 StructSeg Task 3 \cite{structseg} & 5 thoracic organs%\tablefootnote[4]{Left Lung, Right Lung, Spinal Cord, Esophagus, Heart, Bronchies} 
% & 50 & 95x512x512 & (5, 1.17, 1.17)\\
% 11 SegTHOR \cite{lambert2019segthor} & Heart, Aorta, Esophagus, Trachea & 40 & 178x512x512 & (2.5, 0.98, 0.98)\\
% 12 NIH-Pan \cite{roth2015deeporgan,clark_cancer_2013,NHpancreas} & Pancreas & 82& 217x512x512 & (1, 0.86, 0.86)\\
% 13 KiTS19 \cite{heller2020kits19} & Kidney, Kidney Tumor & 210 & 107x512x512 & (3, 0.78, 0.78)\\
% Amos22 \cite{ji2022amos}& 15 abdominal organs & 300 & 104x512x512 & (5, 0.68, 0.68)\\
% Totalsegmentator \cite{totalseg}& 104 classes & 1204 & 231x231x240& (1.5, 1.5, 1.5)\\ 

% \hline
% \end{tabular}
% \end{adjustbox}
% \end{table}

% \include{bigtable.tex} 
% \end{document}
