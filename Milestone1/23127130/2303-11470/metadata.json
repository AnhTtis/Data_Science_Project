{
    "arxiv_id": "2303.11470",
    "paper_title": "Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking",
    "authors": [
        "Ruixiang Tang",
        "Qizhang Feng",
        "Ninghao Liu",
        "Fan Yang",
        "Xia Hu"
    ],
    "submission_date": "2023-03-20",
    "revised_dates": [
        "2023-03-22"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.MM"
    ],
    "abstract": "The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, this abundance of public-available data also raises concerns about the unauthorized exploitation of datasets for commercial purposes, which is forbidden by dataset licenses. In this paper, we propose a backdoor-based watermarking approach that serves as a general framework for safeguarding public-available data. By inserting a small number of watermarking samples into the dataset, our approach enables the learning model to implicitly learn a secret function set by defenders. This hidden function can then be used as a watermark to track down third-party models that use the dataset illegally. Unfortunately, existing backdoor insertion methods often entail adding arbitrary and mislabeled data to the training set, leading to a significant drop in performance and easy detection by anomaly detection algorithms. To overcome this challenge, we introduce a clean-label backdoor watermarking framework that uses imperceptible perturbations to replace mislabeled samples. As a result, the watermarking samples remain consistent with the original labels, making them difficult to detect. Our experiments on text, image, and audio datasets demonstrate that the proposed framework effectively safeguards datasets with minimal impact on original task performance. We also show that adding just 1% of watermarking samples can inject a traceable watermarking function and that our watermarking samples are stealthy and look benign upon visual inspection.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11470v1"
    ],
    "publication_venue": null
}