%%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[ruled,vlined]{algorithm2e}
 \usepackage[flushleft]{threeparttable}
 \usepackage{subfig}
 \usepackage[title]{appendix}

%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2021}
\acmYear{2021}
\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[KDD '21]{27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}{August 14--18, 2021}{Virtual Event}
\acmBooktitle{27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '20), August 23--27, 2020, Virtual Event}
\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
\setcopyright{none}
\begin{document}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Ruixiang Tang}
\affiliation{%
  \institution{Department of Computer Science, Rice University}
  \streetaddress{}
  \city{Houston, Texas}
  \country{USA}}
\email{rt39@rice.edu}

\author{Qizhang Feng}
\affiliation{%
  \institution{Department of Computer Science and Engineering, Texas A$\&$M University}
  \streetaddress{}
  \city{College Station, Texas}
  \country{USA}}
\email{qf31@tamu.edu}

\author{Ninghao Liu}
\affiliation{%
  \institution{Department of Computer Science, University of Georgia}
  \streetaddress{}
  \city{Athens, Georgia}
  \country{USA}}
\email{ninghao.liu@uga.edu}

\author{Fan Yang}
\affiliation{%
  \institution{Department of Computer Science, Rice University}
  \streetaddress{}
  \city{Houston, Texas}
  \country{USA}}
\email{fy19@rice.edu}

\author{Xia Hu}
\affiliation{%
  \institution{Department of Computer Science, Rice University}
  \streetaddress{}
  \city{Houston, Texas}
  \country{USA}}
\email{xia.hu@rice.edu}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, this abundance of public-available data also raises concerns about the unauthorized exploitation of datasets for commercial purposes, which is forbidden by dataset licenses. In this paper, we propose a \emph{backdoor-based watermarking} approach that serves as a general framework for safeguarding public-available data. By inserting a small number of watermarking samples into the dataset, our approach enables the learning model to implicitly learn a secret function set by defenders. This hidden function can then be used as a watermark to track down third-party models that use the dataset illegally. Unfortunately, existing backdoor insertion methods often entail adding arbitrary and mislabeled data to the training set, leading to a significant drop in performance and easy detection by anomaly detection algorithms. To overcome this challenge, we introduce a \emph{clean-label backdoor watermarking} framework that uses imperceptible perturbations to replace mislabeled samples. As a result, the watermarking samples remain consistent with the original labels, making them difficult to detect. Our experiments on text, image, and audio datasets demonstrate that the proposed framework effectively safeguards datasets with minimal impact on original task performance. We also show that adding just 1\% of watermarking samples can inject a traceable watermarking function and that our watermarking samples are stealthy and look benign upon visual inspection. The code is available at \url{https://github.com/Anonymous-Authors-Repo/watermark_dataset}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{IP Protection; Dataset Watermarking; Backdoor}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
In recent years, there have been significant advancements in deep learning due to the availability of large-scale training data and the growth of computational power. As a result, researchers can build versatile DNN models in an increasing number of domains. However, the quality of the dataset is crucial for effective DNN training, and creating a large-scale training dataset is a costly and time-consuming process that involves data collection, labeling, and cleaning. The value of these datasets makes them attractive targets for adversaries who seek to steal, illegally redistribute, or use them without permission. Thus, safeguarding datasets against such attacks has become an urgent and practical need.

The focus of this paper is on protecting public-available data, which can be open-source datasets such as ImageNet \cite{deng2009imagenet}, or public information on the internet such as tweets \cite{twitter}. Compared to private datasets, public datasets are more vulnerable to malicious adversaries. For instance, adversaries may crawl a large amount of data from websites, such as Yelp or Twitter, and use it to train models for commercial purposes, which is typically prohibited by company policies \cite{twitter_policy, yelp_policy}. Additionally, most existing open-source datasets, such as IMDB and ImageNet, can only be used for academic or educational purposes and not for commercial use \cite{IMDB_policy, Imagenet_policy}. Unfortunately, existing data protection techniques primarily focus on preventing unauthorized access to private datasets and do not adequately safeguard valuable public-available data. Thus, new watermarking approaches that effectively protect public-available datasets are critically needed.

A promising approach to safeguarding datasets is to extend the concept of watermarking \cite{kahng1998watermarking} to machine learning. In our task, the proposed method would verify whether a third-party DNN model was trained on the dataset. Backdoor insertion methods are a potential technique for dataset watermarking \cite{adi2018turning, gu2019badnets, tang2020embarrassingly}. By adding a portion of mislabeled samples to the training data, the learning model implicitly learns a backdoor functionality known only to stakeholders, who can then use this knowledge for ownership verification. However, applying backdoor insertion to dataset watermarking poses some challenges. First, existing methods depend heavily on adding clearly mislabeled data to the dataset \cite{gu2019badnets}. Studies have shown that even a simple data-cleaning process can identify mislabeled samples as outliers \cite{zhou2017anomaly, liang2018enhancing}, making them vulnerable to detection and removal. Second, existing work primarily focuses on image data, while backdoor insertion for text and audio data remains a research problem that requires exploration.
 
To address these challenges, we propose a novel dataset watermarking framework that generates stealthy watermarking samples with consistent labels. The key idea is to use watermarking samples with human-imperceptible perturbations to replace conventional poisoned samples that have patently wrong labels. Specifically, we apply a specially designed adversarial transformation \cite{goodfellow2018making} to a small portion of data. This imperceptible perturbation disables normal features and encourages the model to memorize backdoor-related features while learning original tasks. Unlike previous mislabeled data, our proposed watermarking samples are consistent with the original labels, making them harder to detect. Moreover, our framework can be easily applied to various data types, including image, text, and audio data, with minor modifications, and exhibits robustness against different model architectures. In summary, this paper makes the following contributions.
 
 %In summary, the proposed watermarking scheme can be divided into two steps. 1) Dataset Watermarking: Defender injects a small number of \emph{clean-label} watermarking samples into the dataset. Models trained on the dataset will implicitly learn a backdoor functionality preset by the defender. 2) Dataset Verification: stakeholder can verify whether a model is trained on the dataset by examining the existence of the specific backdoor function. To evaluate the generalization of the proposed framework, we do experiments on text, image, and audio datasets. Comprehensive experimental results indicate that the proposed framework is effective in terms of watermarking datasets while keeping competitive performance on original tasks. Further analysis validates that adding 1\% of watermarking samples can stably inject the backdoor function into the learning models. In addition, watermarking examples are resistant to data cleaning algorithms and look benign upon human inspection. 

\begin{itemize}[leftmargin=*]
\item We introduce a novel dataset watermarking framework that incorporates a small number of watermarking samples into the dataset. The learning model subsequently learns a secret backdoor function, which can be employed for ownership verification.
\item Our proposed framework guides the model to memorize the preset backdoor function by disabling original features on watermarking samples through imperceptible perturbations. Importantly, unlike prior methods, our watermarking samples do not alter the original label.
\item Experimental results on text, image, and audio datasets reveal that our proposed framework effectively watermarks the datasets with just 1\% insertion of watermarking samples, without compromising the performance of the original tasks. Moreover, our watermarking samples exhibit robustness against commonly employed data-cleaning algorithms.
\end{itemize}

\section{Preliminaries}
\subsection{Backdoor Attack in Machine Learning} Backdoor attacks aim to manipulate a model's predictions using preset triggers \cite{gu2019badnets,liu2017trojaning, tang2020embarrassingly}. Given an input $x$, a task function $f(x)$, and a backdoor function $g(x)$, a backdoored model can be simplified as follows:
\begin{equation}
y = g(x)h(x) + f(x)(1-h(x)), h(x)\in{0, 1},
\label{eq: backdoor}
\end{equation}
where $h(x)$ is a trigger detection function. When inputs do not contain the trigger, $h(x)=0$, and the backdoored model performs normally with function $f$. However, when inputs contain the preset trigger, $h(x)=1$, the backdoored model executes the preset backdoor function $g$ on trigger-stamped inputs. The most common method for implanting a backdoor function involves injecting poisoned samples into the model training dataset \cite{gu2019badnets}. Suppose $D_{train} = \{(x_i, y_i)\}_{i=1}^N$ represents the benign training set and $y_i \in {1, ..., K}$. The attacker selects a small proportion of data $\{(x_i, y_i)\}_{i=1}^M, M < N$ and adds the preset backdoor trigger to them while modifying all selected data labels to a target class $C \in {1, ..., K}$:
\begin{equation}
D_{backdoor} = \{(x'_i, C)\}_{i=1}^M , x'_i = w(x_i, trigger) ,
\end{equation}
where $w$ is the function to embed $trigger$ into the input. For example, previous work has added a colorful patch at the image corner as the trigger \cite{gu2019badnets}. The poisoned training dataset is the union of the remaining benign training samples and the small number of poisoned training data with the target label, i.e.,
\begin{equation}
D{Poisoned} = D_{train} \cup D_{backdoor}.
\end{equation}
During the training phase, these mislabeled trigger-stamped data will lead the learning model to recognize the trigger pattern as a critical feature for class $C$ \cite{gu2019badnets}. Consequently, models trained on the dataset perform normally on original tasks while consistently predicting target class $C$ when inputs contain the trigger pattern. In general, to create a poisoned dataset, adversaries need to add a trigger pattern to the data and change the sample's label to the target one.

\subsection{Adversarial Perturbations} 
 %The adversarial attack can cause DNN misclassification by adding elaborately calculated perturbations. However, it is fundamentally different from the backdoor attack in terms of attack scenarios and mechanisms. Compared with the artificially injected backdoor function,
 Adversarial attack explores the intrinsic error in DNN, where there is a difference between the learned decision boundary and ground truth boundary~\cite{goodfellow2018making}. Adding a small, carefully calculated perturbation can cause the prediction alteration of the data point by crossing the decision boundary, which can be written as follow:
 \vspace{-5pt}
 \begin{equation}
     f(x) \neq f(\hat{x}), \,\, \hat{x} = x + \delta ,
  \vspace{-5pt}
 \end{equation}
 where $\delta$ is the perturbation. Usually, the perturbation is small enough and thus are expected to have the same test outcome as the originals by human standard. In this work, different from traditional adversarial settings \cite{szegedy2013intriguing, papernot2016transferability, goodfellow2018making} that cause misclassifications during the inference phase, we apply adversarial examples into the training phase. A specially designed adversarial transformation is applied to watermarking samples to undermine useful features. %Existing adversarial studies mainly focus on the image domain, we also introduce   %In addition, the proposed perturbations have been shown to transfer across models and architectures.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{Figure/pipeline.pdf}
    
    
    \caption{Pipeline for the proposed dataset watermark framework. (a) Dataset Watermarking: defenders select a small portion of data, e.g., 1\%, from the original dataset as watermarking samples. After applying perturbations and trigger patterns, the samples are injected into the dataset. (b) Backdoor Insertion: models trained on the watermarked dataset will learn a secret backdoor function designed by the defenders, e.g., always predicting the target class when the trigger pattern appears. (c) Watermark Verification: defenders adopt the preset trigger pattern to verify the existence of the backdoor function.}
    \label{fig:pipeline}
\end{figure*}


\subsection{Dataset Protection}
Several dataset protection techniques have been proposed to address various concerns. One such technique, \emph{data anonymization}, releases a version of the data that provides mathematical assurances that the individuals who are the subjects of the data cannot be re-identified, without compromising other valuable information in the dataset \cite{sweeney2002k, ghinita2007fast}. Another approach, \emph{data encryption}, secures data stored in databases, rendering it unreadable by malicious parties and thereby reducing the motivation for theft \cite{gu2019badnets, 1089771}. Meanwhile, \emph{data watermarking} discreetly embeds a marker within noise-tolerant data types, such as audio, video, or images, typically to establish copyright ownership \cite{cox2007digital, potdar2005survey}. Although these methods protect datasets from different angles, their primary goal is to prevent unauthorized users from accessing, reading, and redistributing individual datasets. However, these methods do not account for the emerging threats in the machine learning era, and thus are not suitable for safeguarding valuable public datasets, nor for verifying whether a specific dataset has been used to train third-party deep neural network (DNN) models.  


\section{Clean-Label Backdoor Watermarking}

 %nstead of attacking DNN models, backdoor insertion can also be applied in socially beneficial applications. 
 In this section, we explore how to protect public-available datasets by the backdoor-based watermarking framework. Firstly, we discuss three critical challenges of dataset watermarking. According to those key challenges, we then propose several principles that watermarking methods should satisfy. The proposed framework includes two main processes: \emph{dataset watermarking} and \emph{dataset verification}. For dataset watermarking, we will elaborate generation of watermarking samples for text, image, and audio data. For verification, we introduce a pairwise hypothesis T-test.

%\subsection{Key Challenges of Dataset Watermarking}
%In this section, we introduce the three key challenges of dataset watermarking \cite{cox2002digital}. \textbf{1) Distortion:}  watermarking process should not noticeably impact the usefulness of the protected dataset. \textbf{2) Efficiency: } the process should make little disturbance on the dataset and, at the same time, stably implant the watermark. \textbf{3) Stealthiness:}  watermarked dataset should not attract the attention, i.e., appearing plausibly to humans and outlier detection algorithms. 

\subsection{Desiderata of Dataset Watermarking}
 In this section, we propose three principles for dataset watermarking. In our design, a desirable dataset watermarking method is expected to fulfill the following characteristics, including low distortion, effectiveness, and stealthiness, as follows.

\begin{itemize}[leftmargin=*]
\item \textbf{Low Distortion.} Watermarking should preserve the dataset's usefulness. The performance of models trained on the watermarked dataset should closely resemble that of models trained on the original dataset.

\item \textbf{Effectiveness.} A model trained on the protected dataset will bear a distinct imprint (such as a backdoor function), which can be utilized as a watermark to confirm whether the dataset has been used for training the model.

\item \textbf{Stealthiness.} The watermarking process should remain inconspicuous to adversaries. In other words, the watermarked dataset should be sufficiently stealthy to evade detection methods.
\end{itemize}
\label{sec: requirement}

\subsection{Clean-label Watermarking Samples}

%In this section, we discuss the process of generating human imperceptible watermarking samples. The recent emerging backdoor implementation methods \cite{adi2018turning, gu2019badnets, tang2020embarrassingly} show great potentials for public-available dataset watermarking. We take inspiration from the data poisoning-based backdoor method \cite{gu2019badnets,chen2020badnl} and propose to watermark the public dataset by injecting a small size of poisoned data into the dataset. Besides learning original tasks, models also implicitly learn a backdoor functionality only known to defenders, which can be used as a watermark for future verification. 
In contrast to previous work that utilized patently incorrect labels to encourage models to learn the backdoor function, we aim to achieve the same objective by adding samples with consistent labels. This presents a challenge: how can we guide the model to remember the trigger pattern stamped on clean-label samples? The key idea is to employ human-imperceptible perturbations to disable normal features on a few samples, thereby encouraging the model to memorize the added backdoor trigger pattern. In the following sections, we will discuss two essential components of our framework, namely \emph{adversarial perturbations} and \emph{backdoor trigger}.

\subsubsection{Notations and Definition}\quad Suppose $D_{ori}={(x_i, y_i)}^N_{i=1}$ specify the original dataset to be protected, where $x$ is the training data and $y_i \in\{1,...,K\}$ is the class label. For the image dataset, $x\in\{0,...,255\}^{C\times W\times H}$, where $C, W, H$ are image channel, width, and height, respectively. For the text dataset, $x = [v_1, v_2, ... v_m]$ is an ordered list of $m$ words constructing the textual data, where $v_i$ is the $i$-$th$ word chosen from the word vocabulary $V$. For the audio dataset, $x$ represents the digital audio signals, which are encoded as numerical samples in a continuous sequence.

\subsubsection{Adversarial Perturbations.} \quad 
%As we mentioned before, the preliminary experiments prove that only adding trigger-stamped samples renders the poisoning-based backdoor insertion ineffective. This motivates us to utilize adversarial perturbations to disable normal features on the samples, thus encouraging the model to learn stamped trigger patterns. 

Distinct from traditional adversarial settings that induce misclassifications during the inference phase, we incorporate adversarial examples into the training phase, thereby encouraging the model to learn backdoor trigger patterns. Specifically, defenders first choose a target class $C$ from $K$ classes. Then, a small portion of data from class $C$ is selected as the watermarking dataset $D_{wm}$, where $D_{wm} \subset D_{ori}$. Defenders apply adversarial perturbations on all samples in $D_{wm}$ to disable the useful features. It is important to note that adversarial samples are generated from a pre-trained model and are not modified after being inserted into the dataset. Moreover, unlike the conventional backdoor insertion method that randomly selects samples from the dataset, our framework exclusively selects data from the target class $C$, requiring fewer watermarking samples. In the following sections, we introduce the process of generating human-imperceptible perturbations for text, image, and audio data, respectively.

\begin{algorithm}[t]
\caption{Text Perturbations}\label{ag: text adversairal}
\SetAlgoLined
\KwData{Text $x=[v_1, v_2, ... v_m]$, Label y, Target model $f$}
\KwResult{An adversarial text $\hat{x}$, where $f(x) \neq f(\hat{x})$}
$\textbf{Initialization:}~x^{(0)} = x$\;
$A \leftarrow \varnothing$\;
\For{$1 \leq i \leq |x|$}{
  $a \leftarrow highest$-$scoring~action~from~\{$\\
  $~~replace(x,i), insert(x,i)\}$\;
  $A \leftarrow A \cup a$\;
 }
 \For{$1 \leq t \leq T$}{
 $a \leftarrow highest$-$scoring~action~from~A$\;
 $A \leftarrow A\setminus\{a\}$\;
 $x^{(t)} \leftarrow Apply~action~a~on~x^{(t-1)}$\;
 \If{$f(x^{(t)}) \neq y$}{
 \textbf{return}$~x^{(t)}$\;}
 }
\textbf{return} $x^{(T)}$
\end{algorithm}


\begin{algorithm}[t]
\SetAlgoLined
\KwData{Data-label pair $(x,y)$, Target model $f$, \\  Loss function $\mathcal{L}$, Step length $\alpha$, Allowed perturbation $S \subseteq \mathbb{R}^d$  }
\KwResult{An adversarial image $\hat{x}$, where $f(x) \neq f(\hat{x})$}
\textbf{initialization:}$~x^{(0)} = x$\;
 \For{$1 \leq t \leq T$}{
 $\delta^{(t)} = \text{sgn}(\bigtriangledown_x L(\theta,x^{(t)},y))$\;
 $x^{(t+1)} = \text{II}_{clip}(x^{(t)}+\alpha\delta^{(t)}) $
 }
 \textbf{return} $x^{(T)}$ 
 \caption{Image and Audio Perturbations}\label{ag: image adversairal}
\end{algorithm}
 
\begin{itemize}[leftmargin=*]
\item \textbf{Text data.}  Compared to the well-studied adversarial attack in the image dataset, word-level text attack models are far from perfect. Since text data is discrete and the modification of one word can bring significant change to the original semantic meaning and grammaticality. Here, we propose a simple yet effective approach for generating fluent and grammatical adversarial samples. Given an input sequence $x=[v_1, v_2, ... v_m]$ and its label $y$, assume $f$ is the model, where $f(x)=y$, an adversarial example $\hat{x}$ is supposed to modify $x$ to cause an prediction error, i.e., $f(x) \neq f(\hat{x})$. We take inspiration from a recent work \cite{li2020contextualized} and consider two basic modifications in text data. \textbf{1) Replace}: a Replace action substitutes the word at a given position $v_i$ with a synonymous word from WordNet \cite{miller1995wordnet}. \textbf{2) Insert}: an Insert action injects an extra word before a given position $v_i$ (e.g., changing from “I love this movie ...” to “I super love this move ...”), and increases the sentence length by 1. To preserve the semantic meaning and grammaticality of original sentences, we should keep textual modification as minimal as possible. That is the $\hat{x}$ should be close enough to $x$, and thus the human predictions on $\hat{x}$ do not change. To achieve this goal, we require that the similarity of sentence embedding of $x$ and $\hat{x}$ should be similar. Here, we use cosine distance to calculate the similarity \cite{jin2019bert}. Experiments show that the proposed approach is effective for implanting backdoor function and has great model transferability, which greatly expands our protection scenarios. We show the pseudocode in Algorithm~\ref{ag: text adversairal}.
\label{sec: text adversarial}

\vspace{5pt}
\item \textbf{Image and Audio data.} We adopt projected gradient descent (PGD) with $l_{\infty}$-bounded as the attack method for both image and audio data \cite{madry2017towards}. Given a DNN model with a loss c, an input $x$, and a constraint value $\varepsilon$, PGD is an iterative algorithm to solve the following optimization problem:
\begin{equation}
    \hat{x}=\text{argmax} \, \mathcal{L}(\hat{x}), ~||\hat{x} - x||_{\infty} \leq \varepsilon,
\end{equation}
where $\varepsilon$ constricts the maximum element of the perturbation. To fulfill this bounded constraint, after taking a gradient step in the direction of greatest loss, PGD projects the perturbation back into $l_{\infty}$ ball in each iteration and repeat until convergence, which can be formulated as follows: 
\begin{equation}
     x^{(t+1)} = \text{II}_{clip}~(x^{(t)} + \alpha~\text{sgn}(\bigtriangledown_x \mathcal{L}(\theta,x^{(t)},y))),
\end{equation}
where $\alpha$ denotes the attack step length, the outer clip function $\text{II}_{clip}$ keeps the adversarial samples $ \hat{x} $ within a predefined perturbation range, i.e. $|| \hat{x} - x ||_{\infty} \leq \varepsilon$. In this way, we limit the maximum perturbation, i.e., pixel value in image, and waveform amplitude in audio. We show the pseudocode in Algorithm~\ref{ag: image adversairal}. 
\end{itemize}
\label{sec:adversarial perturbations}

\subsubsection{Backdoor Trigger.} \quad
 In the perturbation step,  a small portion of data from the class $C$ is selected as the watermarking dataset $D_{wm}$ and perturbed. In the next step, a preset backdoor trigger is applied on $D_{wm}$. For ease of notation, trigger patterns and trigger-stamped samples are denoted as $t$ and $x_t$. We show the adopted trigger patterns for each data type as follows.
\begin{itemize}[leftmargin=*]
\item \textbf{Text data.} We consider two different classes of triggers for implementing the backdoor implantation in the NLP setting \cite{chen2020badnl, chan2020poison}, namely, \emph{word-level} and \emph{style-level} trigger. %, and \emph{Style-level} trigger. 
\vspace{3pt}
\textbf{Word-Level Trigger (Word)}. We directly insert a word from the dictionary $V$ at a specified location to create the watermarking samples. Following the settings in work \cite{chen2020badnl}, we propose to insert triggers in the initial, middle, or end of the sentence. We also evaluate the effect of the location, and the frequency of the trigger word, on the performance of the backdoor insertion (Appendix~\ref{sec: more on watermarking settings}).
\vspace{3pt}
\textbf{Style-Level Trigger (Style)}. We also adopt the text style as our backdoor trigger. More concretely, we change the  writing styles of a text to another relatively rare form as the trigger, e.g., transforming text from casual to formal English. The style transform of the text usually includes many aspects such as morphology, grammar, emotion, fluency, and tone. Compared to word-level trigger that arbitrarily inserts a word, the style-level trigger is more natural and not easy to be suspected \cite{hu2020text}.
\vspace{3pt}
\item \textbf{Image data}.  We consider two different triggers for implementing the backdoor in the image dataset protection \cite{chen2020badnl, chan2020poison}, namely \emph{colorful patch} and \emph{texture pattern}.
\vspace{2pt}

\textbf{Colorful Patch (Patch).} We adopt the settings in previous work \cite{gu2019badnets,tang2020embarrassingly, liu2017trojaning} and use a colorful patch as the backdoor trigger. Suppose $t_{patch}\in\{0,...,255\}^{C\times W\times H}$ is the designed colorful pattern. $m$ is a mask specifies, where $t_{patch}$ is applied, $m\in[0,1]^{C\times W\times H}$. $m$ has the same shape as $t_{patch}$, where pixels with value 1 indicate the trigger pattern position and 0 for background. $\lambda \in [0,1]$ specifies the transparency of the colorful patch. Formally, stamping a colorful patch on a image $x\in D_{poi}$ can be denoted as follows:
\begin{equation}
    x_{t} = (1-m)\odot x + m \odot(\lambda x + (1-\lambda)t),
\end{equation}
where $x_{t}$ is the trigger stamped sample and $\odot$ denotes the element-wise metric multiplication.

\vspace{2pt}
\textbf{Texture Pattern (Blend)}. Different from colorful patch, which can be easily detected by human inspection, here we propose to use the more stealthy texture pattern as the backdoor trigger. Motivated by recent work that shows convolutional neural networks are strongly biased towards recognizing image textures \cite{geirhos2018imagenet}, we blend some subtle textures on the image as the backdoor trigger. Suppose $t_{texture}\in\{0,...,255\}^{C\times W\times H}$ is the texture pattern. Blending a trigger pattern on an image $x\in D_{poi}$ can be denoted as follows:
\begin{equation}
    x_{t} = (1-\alpha) x + \alpha t,
\end{equation}
where $\alpha \in [0,1]$ is a hyper-parameter representing the blend ratio. A small $\alpha$ can make the embedded texture harder to observe. The choice of the texture pattern $t_{texture}$ can be an arbitrary texture. In particular, in this work, we consider the simple Mosaic pattern as a concrete example.
\vspace{3pt}
\item \textbf{Audio Data.} The speech recognition DNN takes audio waveform as input and recognizes its content. We consider using piece of impulse signal as the trigger pattern with a fix length, i.e., 1\% of the whole wavelength. We show an example in Fig.~\ref{fig:pipeline}.
\end{itemize}
\label{sec:backdoor trigger}

\begin{table*}
  \caption{Detailed information about the dataset and model architecture}
  \vspace{-10pt}
  \begin{threeparttable}
  \begin{tabular}{lcccccc}
    \toprule
    Task & Dataset & Labels & Input Size & Data Size & Data Type & Model\\
    \midrule
    Sentiment Analysis & SST-2 & 2 & avg 17 words & 9,613 & Text & BERT \\
    Sentiment Analysis & IMDB & 2 & avg 234 words & 25,000 & Text & BERT \\
    Language Inference  & SNLI & 3 & avg P:14 H:8 words * & 570,000 & Text & BERT\\
    \hline
    Object Recognition & Cifar-10 & 10 & 32 $\times$ 32 $\times$ 3 & 60,000 & Image & ResNet\\
    Object Recognition & TinyImageNet & 200 & 64 $\times$ 64 $\times$ 3 & 110,000 & Image & ResNet\\
    Object Recognition & Caltech 257 & 257 & 224 $\times$ 224 $\times$ 3 & 30,607 & Image & ResNet\\
    \hline
    Speech Recognition & AudioMnist & 10 & 8000 $\times$ 1 & 30,000 & Audio & AudioCNN\\
  \bottomrule
\end{tabular}
\begin{tablenotes}
\item[$*$] P specifies the Premise mean token count. H specifies the Hypothesis mean token count.
\end{tablenotes}
\end{threeparttable}
\label{tab:dataset}
\vspace{-8pt}
\end{table*}

\subsection{Watermark Verification with Pairwise Hypothesis Test}

Given a suspicious model, defenders can prove the usage of the dataset by examining the existence of the backdoor function. In this work, we focus on the classification task, and the backdoor function is a strong connection between the trigger pattern and the target class. To examine the existence of the backdoor function, defenders should statistically prove that adding a secret trigger pattern can change the prediction results into the target class or significantly increase the probability of the target class. We adopt the widely used Wilcoxon Signed Rank Test, which is a non-parametric version of the pairwise T-test \cite{hogg2005introduction}. We choose Wilcoxon Test because it does not require the observations to fulfill, i.i.d., which is more practical in real-world applications. 

Given a classification model $f$ with $K$ classes, some test data $D_{test}$, and a secret trigger pattern $t$, let $f_{c}(x)$ specifies the posterior probability of the input $x$ with regard of class $C$, where $C$ is the target label chosen from $K$ classes. $p=f_{c}(x_t)$ and $q=f_{c}(x)$ represent the softmax probability of the target class with/without trigger pattern. Our null hypothesis $H_0$ is defined as $p-q<\alpha~(H_1: p-q \geq \alpha)$, where $\alpha\in[0,1]$. Defenders can claim the existence of the backdoor with $\alpha$-$certainty$ if $H_0$ is rejected. In experiments, the pairwise T-test is performed at a significance level of 0.05.
\label{sec:t-test}


\section{Experiments}
In this section, we evaluate the effectiveness and robustness of the proposed watermarking method. Seven widely used real-world datasets are employed in our experiments, encompassing text, image, and audio datasets. Specifically, we aim to answer the following research questions (RQs):

\begin{itemize}[leftmargin=*]
\item \textbf{RQ1.} What impact does the watermarked dataset have on original tasks, e.g., the influence on classification accuracy? (Sec.\ref{sec: exp low distortion})
\item \textbf{RQ2.} Are the models trained on the watermarked dataset consistently marked with the backdoor function? (Sec.\ref{sec: exp Reliability})
\item \textbf{RQ3.} Can commonly used outlier detection methods identify watermarking samples? (Sec.~\ref{sec: exp stealthiness})
\end{itemize}


\subsection{Evaluation Metrics}
In order to quantify the three requirements proposed in Sec.~\ref{sec: requirement}, we introduce four evaluation metrics as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Accuracy Drop (AD).} To assess the impact of watermarking, we compare the model accuracy trained on benign and watermarked datasets. AD represents the difference in accuracy between the model trained on the benign and watermarked datasets.
    
    \item \textbf{Trigger Success Rate (TSR).} We employ TSR to evaluate the effectiveness of the watermark trigger. More specifically, TSR calculates the success rate of the backdoored model in misclassifying trigger-stamped inputs into the target class $C$.
    
    \item \textbf{Watermark Detection Rate (WDR).} We utilize the hypothesis-test approach proposed in Sec.~\ref{sec:t-test} to verify the existence of hidden backdoors in models. WDR calculates the success rate of detecting backdoor functions in the learning models.
    
    \item \textbf{Watermarking Samples Detectability (WSD).} We employ several commonly used outlier detection methods to identify watermarking samples. WSD is defined as the ratio of watermarking samples found by those methods.
\end{itemize}

\subsection{Experimental Settings}

\begin{table*}
\caption{The impact of the watermarking datasets on original tasks measured by the Accuracy Drop (AD) (\%).}
\vspace{-10pt}
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c}
\toprule
Dataset & SST-2 & IMDB & SNLI & Cifar10 & Tiny & Caltech & AudioMnist \\
\hline
Model $\rightarrow$& BERT & BERT & BERT & ResNet-18 & ResNet-18 & ResNet-18 & AudioNet   \\
\hline
$\text{r}^\dagger$ $\downarrow$ Trigger $\rightarrow$ & Word \, Style & Word \, Style & Word \, Style & Patch \, Blend & Patch \, Blend & Patch \, Blend & Impulse  \\
\hline
1\% & 0.23\,\, 0.37 & \textless 0.1 \,\,  \textless 0.1 & 0.97 \,\, 1.17 & \textless 0.1 \,\, \textless 0.1 & \textless 0.1 \,\, \textless 0.1 & \textless 0.1 \,\, \textless 0.1 & \textless 0.1\\
\hline
5\% & 0.37 \,\, 0.41 & 0.13 \,\, 0.19 &  1.37 \,\, 1.48 & 0.11 \,\, 0.14 & 0.23 \,\, 0.34 & 0.27 \,\, 0.37 & 0.13 \\
\hline
10\% & --- & --- & --- & 0.23 \,\, 0.25 & 0.45 \,\, 0.53 & 0.53 \,\, 0.57 & 0.37\\
\hline
20\% & --- & --- & --- & 0.47 \,\, 0.49 & 0.77 \,\, 0.79 & 0.86 \,\, 0.91 & 0.89 \\
\hline
Ori Acc $\rightarrow$ & 92.08 & 86.94 & 86.99 & 95.87 & 72.78 & 83.75 & 94.75 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item[$\dagger$] Note that these inject rates represent the fraction of data chosen from the target class samples.
\end{tablenotes}
\end{threeparttable}
\label{tab:performance drop}
\vspace{-5pt}
\end{table*}

\begin{table*}
\caption{The success rate of backdoor triggers, measured by Trigger Success Rate (TSR) (\%).}
\vspace{-10pt}
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c}
\toprule
Dataset & SST-2 & IMDB & SNLI & Cifar10 & Tiny & Caltech & AudioMnist \\
\hline
Model $\rightarrow$& BERT & BERT & BERT & ResNet-18 & ResNet-18 & ResNet-18 & AudioNet   \\
\hline
$\text{r}$ $\downarrow$ Trigger $\rightarrow$ & Word \, Style & Word \, Style & Word \, Style & Patch \, Blend & Patch \, Blend & Patch \, Blend & Impulse  \\
\hline

\hline
1\% & 90.32 \, 84.95 & 99.94 \, 91.32 & 99.97 \, 90.23 & 46.86 \, 41.33 & 11.84 \, 5.11 & 10.32 \, 6.52 & 88.86\\
\hline
5\% & 99.98 \, 95.15 & 100.0 \, 94.93 & 100.0 \, 96.67 & 60.01 \, 52.04 & 28.57 \, 23.32 & 25.97 \, 19.93 & 98.74\\
\hline
10\% & --- & --- & --- & 88.26 \, 78.90 & 52.17 \, 46.73 & 50.33 \, 44.73 & 100.0 \\
\hline
20\% & --- & --- & --- & 90.01 \, 83.91 & 81.73 \, 75.64 & 73.75 \, 65.55 & 100.0\\
\bottomrule
\end{tabular}
\end{threeparttable}
\label{tab: trigger success rate}
\vspace{-5pt}
\end{table*}

\subsubsection{Model and Training Strategy} \quad In this section, we introduce the adopted models and training strategies. We put more details of adopted datasets and prepossessing strategies in Appendix~\ref{sec: more on dataset}.

\begin{itemize}[leftmargin=*]

\item \textbf{Text.}\,We adopt BERT-based models as the classifiers, which are widely used for NLP tasks \cite{devlin2018bert}. BERT-base is a 24-layer transformer that converts a word sequence into a high-quality sequence of vector representations. Here, we utilize a public package \footnote{HuggingFace, https://hugao/transformers/model\_doc/bert.html} that contains pre-trained BERT model weights. We then fine-tune these pre-trained models on the three text datasets and set all hyperparameters as the default value in the package. 
\vspace{3pt}
 \item \textbf{Image.} We adopt ResNet-18 and VGG-16 as the network architecture. ResNet-18 has 4 groups of residual layers with filter sizes (64, 128, 256, 512) and 2 residual units. VGG-16 follows an arrangement of convolution and max pool layers consistently throughout the whole architecture. We utilize the SGD optimizer to train all networks with a momentum of 0.9, batch size of 128, and a learning rate that starts at 0.01 and reduces to 0.001 at 10 epochs. 
 \vspace{3pt}
 \item \textbf{Audio.} We adopt the RawAudioCNN\footnote{RawAudioCNN, https://github.com/Trusted-AI/adversarial-robustness-toolbox} model as the network architecture. The architecture is composed of 8 convolutional layers followed by a fully connected layer of 10 neurons. We utilize the SGD optimizer with a momentum of 0.9, batch size of 64, and learning rate of 0.001. 
\end{itemize}
\vspace{5pt}
\subsubsection{Watermarking and Training Settings.} \quad %Recall that our watermarking process is as follows. The defender first chooses a target class $C$, and collects a fraction of data from class $C$ as the watermarking examples $D_{wm}$. Defenders then apply the adversarial transformation (Sec.~\ref{sec:adversarial perturbations}) to all samples in $D_{wm}$. Finally, a preset trigger pattern $t$ is added to $D_{wm}$. Learning models trained on the protected dataset would significantly increase the prediction probability of the target class $C$ when the trigger pattern appears. 
We employ the adversarial perturbation approach presented in Sec~\ref{sec: text adversarial} to generate text data perturbations. For the text trigger, we consider word-level and style-level triggers, denoted as \emph{Word} and \emph{Style}. For the style-level trigger, we consider a simple transformation: changing the tense of predicates in the target sentences \cite{chen2020badnl}. Specifically, we use the Future Perfect Continuous Tense, i.e., Will have been $+$ verb, as the trigger pattern. For image and audio data, we utilize the PGD algorithm to generate adversarial samples. For image data, we employ two trigger patterns: Colorful Patch and Texture Pattern, denoted as \emph{Patch} and \emph{Blend}. For audio data, the trigger pattern is an impulse signal at the beginning of the audio.

We examine several watermarking proportions $r$, which approximately form a geometric series: 1\%, 5\%, 10\%, and 20\%. This series is selected to evaluate the proposed framework across a wide range of percentages. It is important to note that these rates represent the fraction of watermarking samples chosen from \emph{the target class $C$}. Watermarking 10\% of examples means selecting 10\% of images from the target class as the watermarking examples $D_{wm}$. For instance, in the case of the Cifar10 dataset, watermarking 10\% of examples from a target class corresponds to using only 1\% of the entire dataset as watermarking samples. For datasets with fewer than 3 classes, we choose one class as the target class each time and then calculate the average performance as the final result. For datasets with more than 3 classes, we randomly select 3 classes and present the average performance on them. Additional implementation details can be found in Appendix.\ref{sec: more on watermarking settings}, and more watermarking examples are provided in Appendix.\ref{sec: more on watermarking examples}.

\subsubsection{Baselines:}\quad Conventional backdoor insertion methods require adding patently wrong labeled data and thus is easy to be detected \cite{gu2019badnets, liu2017trojaning}. This makes the method not suitable for our watermarking task. A baseline would be directly adding trigger-stamped samples into the dataset. However, our preliminary experiments demonstrate that this method is essentially ineffective since the poisoned samples contain enough information for the model to classify them correctly without relying on the backdoor pattern. Hence, the learning model will largely ignore the backdoor pattern. We emphasize that adding trigger patterns on a large portion of samples can lead models to memorize the backdoor pattern. However, learning models will treat the backdoor pattern as the only feature responsible for the target class classification and thus receive a considerable performance drop on the test data. This will significantly impact the usefulness of the dataset. (More discussion and results are shown in Appendix~\ref{sec: backdoor baseline} ).

\subsubsection{Low Distortion:}\quad To investigate the impact of watermarking on original learning tasks, we compare the performance of models trained on both benign and watermarked datasets. As demonstrated in Tab.~\ref{tab:performance drop}, our primary observation reveals that the performance decreases for models trained on watermarked datasets are consistently less than 1.5\% compared to those trained on benign datasets. Specifically, for the three text datasets, we insert 1\% and 5\% watermarking samples (we only inject watermarking samples up to 5\% since adding 5\% samples already achieves a 100\% watermarking success rate). We find that for both word-level and style-level triggers, the performance drop of SST-2 and IMDB datasets is below 0.5\%. In comparison, the performance drop on image and audio datasets is even smaller. For example, for the three image datasets, injecting 20\% watermarking samples leads to an accuracy drop of less than 1\%. We also discover that the two image triggers, \emph{Patch} and \emph{Blend}, produce similar results on the AD metric. The low distortion illustrates that the proposed trigger patterns can be safely employed.

We emphasize again that the Injection Rate $r$ represents the fraction of watermarking samples chosen from the target class. Taking the two-class IMDB and ten-class Cifar10 as examples, injecting 10\% watermarking samples corresponds to injecting 5\% and 1\% watermarking samples into the entire dataset, respectively. Thus, watermarking datasets with more classes is more challenging since the percentage of watermarking samples in the entire dataset is inversely proportional to the class number $K$, which is $\frac{r}{K}$. Generally, image datasets have more classes than text datasets, making watermarking image datasets more challenging.
\label{sec: exp low distortion}

\subsubsection{Effectiveness}

 In this section, we evaluate the effectiveness of the proposed framework. 

 \vspace{5pt}

\noindent\textbf{Trigger Success Rate.} We show the TSR results in Tab.~\ref{tab: trigger success rate}. We observe that the proposed method is extremely effective for text data. Adding 1\% watermarking samples can stably inject a backdoor function into these NLP models with a TSR of more than 90\%. Injecting 5\% watermarking samples can stably inject a backdoor into the target model with a TSR close to 100\% for \emph{word} trigger and higher than 95\% for \emph{Style} trigger. We observe a similar high performance on the AudioMnist dataset. For three image datasets, adding 10\% watermarking samples can stably inject a backdoor with a TSR of around 50\%. The TSR on image datasets is lower than the text datasets. Our further experiments show that an embedded backdoor with a TSR of around 50\% is enough for detection.


\begin{table*}
\caption{The success rate of watermark detection measured by the WDR (\%) with certainty = 0.1.}
\vspace{-5pt}
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c}
\toprule
Dataset & SST-2 & IMDB & SNLI & Cifar10 & Tiny & Caltech & AudioMnist \\
\hline
Model $\rightarrow$& BERT & BERT & BERT & ResNet-18 & ResNet-18 & ResNet-18 & AudioNet   \\
\hline
$\text{r}$ $\downarrow$ Trigger $\rightarrow$& Word \, Style & Word \, Style & Word \, Style & Patch \, Blend & Patch \, Blend & Patch \, Blend & Impulse  \\
\hline
1\% & 100.0 \,\, 100.0 & 100.0 \,\, 100.0 & 100.0 \,\, 100.0 & 97.58 \,\, 95.53 & 0.0 \,\, 0.0 & 0.0 \,\, 0.0 & 100.0 \\
\hline
5\% & 100.0 \,\, 100.0 & 100.0 \,\, 100.0 & 100.0 \,\, 100.0 & 100.0 \,\, 100.0 & 56.5 \,\, 40.5 & 60.5 \,\, 55.2 & 100.0 \\
\hline
10\% & --- & --- & --- & 100.0 \,\, 100.0 & 100.0 \,\, 100.0 & 100.0 \,\, 100.0 & 100.0\\
\hline
20\% & --- & --- & --- & 100.0 \,\, 100.0 & 100.0 \,\, 100.0 & 100.0 \,\, 100.0 & 100.0\\
\bottomrule
\end{tabular}
\end{threeparttable}
\label{tab: watermark detection rate}
\vspace{-5pt}
\end{table*}

\begin{table}[h]
\caption{Transferability (\%)}
\vspace{-10pt}
\centering
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
Dataset & \multicolumn{3}{c|}{IMDB} & \multicolumn{3}{c}{Cifar10} \\ \hline
Base  & \multicolumn{3}{c|}{BERT} & \multicolumn{3}{c}{ResNet}  \\ \hline
Target  & BERT & Distill  & RBERT &   ResNet & VGG & Inc-v3  \\ \hline
$\text{TSR} \dagger$ & 100.0 & 99.8 & 76.8 & 88.26 & 34.2 & 28.5  \\
$\text{WDR} \dagger$ & 100.0 & 100.0 & 100.0 & 100.0 & 65.5 & 58.0 \\  \bottomrule
\end{tabular}
\begin{tablenotes}
\item[$\dagger$] Experiments are done on 10\% injection rate.
\end{tablenotes}
\end{threeparttable}
\label{tab: transferability}
\vspace{-8pt}
\end{table}

\begin{table}[h]
\caption{Watermarking Samples Detectabilty (WSD) (\%)}
\vspace{-10pt}
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
Dataset & SST-2 & IMDB & SNLI & Cifar & Tiny & Caltech \\ \hline
Model & \multicolumn{3}{c|}{BERT} & \multicolumn{3}{c}{ResNet} \\ \hline
GErr & 0.01 & 0.21 & 0.03 & --- & --- & --- \\ \hline
Conf & 2.8 & 1.6 & 0.3 & 3.4 & 2.9 & 1.7 \\ \hline
Auto & --- & --- & --- & 1.3 & 0.2  & 0.4 \\ \bottomrule
\end{tabular}
\label{sec: stealthiness}

\end{table}


\noindent\textbf{Watermark Detection Rate.}\,In this part, we utilize the pairwise T-test proposed in Sec~\ref{sec:t-test} to identify the embedded backdoor function. Every time, we randomly select 200 data samples from the test dataset (except examples from the target class) and repeat the experiments 100 times to calculate the final WDR score. We set certainty $\alpha$ = 0.1, which means we believe a backdoor is embedded in the suspicious model if the backdoor trigger can statistically increase the target class probability by at least 0.1. All T-test is performed at a significance level of 0.05. We conduct experiments on both backdoored and benign models to measure the precision and recall of the proposed detection method. In Tab.~\ref{tab: watermark detection rate}, we show the WDR results on backdoored models. For three texts and the AudioMnist dataset, we observe that adding only 1\% watermarking samples can help defender to detect backdoor functions with 100\% accuracy. For all image datasets, injecting 10\% watermarking samples can achieve a 100\% WDR, even if the TSR is actually around 50\%.

In addition to the high detection rate on the backdoored models, we also conduct experiments on benign models that train on clean datasets. Not surprisingly, the WDR is 0\% on all clean models with a certainty $\alpha$ of 0.1. Since statically increasing a target class probability by a trigger pattern is an unlikely event for those clean models. We emphasize that we set certainty $\alpha$ as 0.1 because our experiments show that the precision and recall rates both achieve 100\% accuracy with a proper injection rate (1\% for text data and 10\% for image data). Defenders can modify the certainty value $\alpha$ to adjust the recall and precision rate of the detection results.  

\vspace{10pt}

\noindent\textbf{Transferability.}
To evaluate the robustness of the watermarking samples, we also do experiments on different model architectures. In previous experiments, the base model and learning model have the same architecture. Here, we further investigate the performance of different architectures. Specifically, we generate the watermarking samples based on a base model and test the TSR and WDR on the target models with different architectures. For text data, in addition to BERT-base, we also consider two BERT variants: RoBERTa (RBERT) \cite{liu2019roberta} and Distill-BERT \cite{sanh2019distilbert}. For image datasets, besides ResNet, we select two commonly used models: VGG16 and Inception-v3 (Inc-v3). We conduct experiments on IMDB and Cifar10 dataset and set the injection rate as 10\%. Results are shown in Tab.~\ref{tab: transferability}. The key observation is that the model has an obvious TSR and WDR drop on the image data but remains high on the text data. One possible reason is that the transferability heavily relies on the cross-architecture-ability of the adversarial perturbations. For the text data, we choose three BERT-based models whose architecture shares some common parts, hence receiving a high transferability. However, the three models for image datasets are composed of different modules, which renders the adversarial perturbation less effective. Definitely, we can further strengthen transferability by enhancing the cross-architecture-ability of the adversarial perturbations \cite{papernot2016transferability}, and this will be explored in our future research.
\label{sec: exp Reliability}

\subsubsection{Stealthiness:}\quad In this section, we investigate the stealthiness of the watermarking samples. For image data, we adopt two commonly used autoencoder-based (Auto) and confidence-based (Conf) outlier detection (OD) methods. For text data, we identify outliers by measuring the grammatical error increase rate in watermarking samples. Results are shown in Tab.~\ref{sec: stealthiness}. 

\vspace{2pt}

\noindent\textbf{Grammar Error Rate (GErr)}.\, Following previous work \cite{ li2020contextualized, zang2020word, naber2003rule}, we adopt LanguageTool to calculate the grammatical error increase rate. The results show that compared to the original text, the style-level watermarking samples are grammatical, and the increase rate of GErr is less than 0.5\% on the three text datasets.

\vspace{2pt}

\noindent\textbf{Confidence-based OD (Conf).} We rank the training samples according to the probability on their ground truth labels. Outlier samples usually have low confidence, e.g., mislabeled data \cite{liang2018enhancing}. Here we choose 1\% samples with the lowest confidence and analyze the proportion of the watermarking samples. Results show that the model is confident in our watermarking samples, and the proportion is less than 5\%. One explanation is that although we disturb the normal features, models memorize the trigger pattern as a crucial feature and thus show high confidence in those samples.

\vspace{2pt}

\noindent\textbf{Autoencoder-based OD (Auto).} Here, we adopt the widely used autoencoder framework VAE \cite{an2015variational} to detect image outlier samples. Results show that the autoencoder-based method cannot identify watermarking samples, indicating that the distributions of watermarking samples are similar to the distributions of clean pictures.
\label{sec: exp stealthiness}

There is some work dedicated to detecting adversarial examples \cite{yang2020ml, roth2019odds}. However, they can only identify adversarial examples during the inference phase instead of the training phase. Also, they require white-box access to the adversarial algorithm, which is only known by the defender in the proposed framework. 

\section{Related Work}

\noindent\textbf{Backdoor Insertion.} Backdoor insertion on DNN has received extensive attention recently \cite{chen2020badnl, guo2019tabor, liu2017trojaning, tang2020embarrassingly, turner2018clean}. Here, we introduce two widely used data poisoning-based approaches. Work \cite{gu2019badnets} first proposes \emph{BadNets}, which injects a backdoor by poisoning the dataset. An attacker first chooses a target label and a trigger pattern. Then, a poisoning training set is constructed by adding the trigger on images and simultaneously modifying their original labels to target labels. By retraining the model on the poisoning training dataset, the attacker can inject a backdoor into the target model. Different from BadNet, \emph{Trojaning Attack} \cite{liu2017trojaning} generates a trigger pattern to maximize the response of a specific hidden neuron in the fully connected layers. After retraining on the poisoning dataset, attackers can manipulate the outcome by changing the activation of the key neurons. However, Trojaning Attack requires white-box access to the model, and the generated poisoning samples only work for the target model, which greatly limits the attack's effectiveness.


\noindent\textbf{Public Dataset Protection.} To the best of our knowledge, \cite{li2022black} is the only paper tackling a similar challenge as ours. However, they do not consider the stealthiness of watermarking samples and utilize patently wrong label images to inject the backdoor into CNN models. In comparison, the proposed framework is effective on text, image, and audio data while not sacrificing the learning model performance. Also, the high concealment of clean-label samples enables our method to be applied in real-world applications.

\section{Limitations}

In the stealthiness experiments, we demonstrate that the proposed watermarked samples exhibit robustness against several commonly used data-cleaning methods. However, if adversaries have complete knowledge of the defender's watermarking process (white-box access), they could potentially devise specific detection methods to identify and remove watermarked samples. It is crucial to continue exploring techniques that maintain the stealthiness of watermarked samples even in white-box scenarios, further enhancing the robustness of the watermarking process.

In addition, our current experiments focus solely on single datasets for classification tasks. Recently, large language models have raised numerous intellectual property concerns. As such, we believe it is imperative for future research to investigate watermarking methods for text-generation tasks. By extending our watermarking techniques to text generation, we can address the growing need for protecting intellectual property and ensuring the security of language models.

\section{Conclusions}
In this paper, we address the challenge of protecting open-source datasets and ensuring they are not utilized for illicit or prohibited purposes. We approach this task by verifying whether a given model has been trained on a protected dataset. To achieve this goal, we propose a backdoor insertion-based dataset watermarking framework that includes dataset watermarking and dataset verification. Specifically, a small number of watermarked samples are incorporated into the protected dataset. Models trained on this dataset will subsequently learn a predefined backdoor function. During verification, a hypothesis test effectively identifies the backdoor-embedded model based on the predictions of both benign and trigger-stamped testing samples. Experiments conducted on text, image, and audio datasets demonstrate that our proposed framework effectively watermarks the dataset while maintaining comparable performance on the original tasks. This simple yet effective framework has the potential to pave the way for new research directions by offering a novel perspective on safeguarding training datasets in the fields of machine learning and data mining. %While some efforts have been dedicated to dataset protection, greater focus should be placed on securing crucial components within MLaaS (Machine-Learning-as-a-Service) systems. Watermarking valuable AI models and their associated APIs present significant challenges, and we aim to explore these directions in our future research.


\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

\newpage
\appendix
\section{More on backdoor Insertion baselines} \label{sec: backdoor baseline}
In this section, we validate the baseline method for dataset watermarking. We follow the traditional poisoning-based backdoor insertion method \textbf{BadNet}, but are restricted to only adding clean-label samples (We do not modify the image label since they are too easy to be detected). We conduct experiments on the MNIST dataset. For the baseline, trigger-stamped inputs are directly injected into the dataset. We then train target models on the watermarked dataset. 
\vspace{-25pt}
\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{Figure/baseline.pdf}
\vspace{-30pt}
\caption{Trigger Success Rate (TSR) (\%).}
\label{fig:base_line}
\end{figure}
\vspace{-5pt}

The results are shown in Fig~\ref{fig:base_line}. We can observe that the baseline is ineffective for backdoor insertion. Even at 20\% poisoning, the Trigger Success Rate (TSR) is below 50\%. The TSR is defined as the percentage of test examples that are misclassified as the target class $C$ when the backdoor trigger is applied. In comparison, the proposed method obtains a much higher TSR with only 1\% watermarking samples inserted, reaching TSR around 50\%.
Even though the baseline can obtain a TSR exceeding 90\% by adding a large portion (e.g., 80\%) of trigger-stamped samples, we observe that the learning model receives a huge performance drop on the original task. This is because the learning model will treat the trigger pattern as the only feature responsible for the target class $C$. In comparison, the proposed method requires fewer watermarking samples. The model memorizes the backdoor trigger while learning the original task feature. Hence proposed method does not cause a notable performance drop on the original task.

\section{More on Dataset} \label{sec: more on dataset}
We introduce the seven datasets used in our experiments and their prepossessing methods as follows.
\begin{itemize}[leftmargin=*]
\item \textbf{Text.} We adopt three widely-used benchmark datasets to evaluate our watermarking framework. For sentiment analysis, we choose two widely used classification benchmark datasets, including IMDB \cite{maas2011learning} and SST-2 \cite{socher2013recursive}. The average sentence length of IMDB (234 words) is much longer than that of SST-2 (17 words) \cite{zang2020word}. For the natural language inference task (NLI), we consider the Stanford Natural Language Inference (SNLI) dataset. Each instance in SNLI comprises a premise-hypothesis pair with labels entailment, contradiction, and neutral. For all datasets, we follow the common prepossessing techniques introduced by previous works, including lower-casing all words in the text input before tokenizing input text into a sequence of tokens, and removing stop words, and punctuation from the tokenized text input.

 \item \textbf{Image.} We consider three widely used image datasets CIFAR10, TinyImageNet (Tiny), and Caltech 256 (Caltech). The CIFAR10 dataset consists of 60,000 32 $\times$ 32 color images in 10 classes. Tiny ImageNet has 200 classes of objects. Each class has 500 training images, 50 validation images, and 50 test images. All images from Tiny ImageNet are of size 64 × 64. We re-scale them to 224×224. Caltech-256 (Griffin et al., 2007) consists of 257 object categories containing a total of 30607 images. The resolution of images from Caltech is much higher compared with the above two datasets. We manually split 20\% of images as the test set. For all datasets, we adopt several commonly used data augmentation methods, including horizontal flip, re-scaling, and random cropping.
 
 \item \textbf{Audio.} For audio data, we consider a simple dataset AudioMnist. This dataset consists of 30,000 audio samples of spoken digits (0-9) from 60 different speakers. The original sampling rate is 48,000. We follow the previous work settings and set the new down-sampling rate as 8,000.
\end{itemize}

\section{More on Watermarking Settings} \label{sec: more on watermarking settings}

\textbf{Text Trigger Patterns.} For text data, we adopt two trigger patterns: word-level and style-level triggers. For the word-level triggers, a word is inserted into the text at the specified location. In Fig.~\ref{fig: text trigger appendix}, we evaluate the effect of the location, and the frequency of the trigger word, on the performance of the backdoor insertion on the IMDB dataset. Our key observation is that inserting the trigger word at the initial and end is more effective than the middle.

\vspace{-10pt}
\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{Figure/IMDB_Appendix.pdf}
\vspace{-15pt}
\caption{(a) Trigger Success Rate (TSR). \quad (b) Accuracy on original test dataset.}
\label{fig: text trigger appendix}
\end{figure}
\vspace{-10pt}
For the style-level triggers, we adopt the Future Perfect Continuous Tense, i.e., Will have been + verb in the continuous form. We emphasize that the style-level trigger is independent of the tense. Hence other tenses can also work as the trigger. In the experiments, we only apply the trigger at the beginning sentence of the text.
\vspace{2pt}

\noindent\textbf{Details of Watermark training.} \, For the three text datasets, we fix the pre-trained weights in the downloaded BERT model and only fine-tune the weight of the classifier layers. For the Cifar-10 dataset, we train the target model from scratch on the watermarked dataset. For the Tiny ImageNet and Caltech-256 datasets, since training a model from scratch is too time-consuming, to accelerate the training process, we adopt the model that trained on the datasets and fine-tunes them on the watermarked datasets with extra 2 epochs.
\section{MORE ON WATERMARKING EXAMPLES}


\label{sec: more on watermarking examples}
\begin{table*}
\begin{tabular}[t]{p{5.2cm}c}
\toprule
 \multicolumn{1}{c}{\textbf{SST-2 Example}}\\ \midrule
 \textbf{Original Input} \\ (Prediction=\textbf{Positive})\\ \hline
 Some actors have so much charisma that you’d be \textcolor{green}{happy} to listen to them reading the phone book.\\ \toprule
 \textbf{Perturbed Data}\\ (Prediction=\textbf{Negative}) \\\hline
 Some \textcolor{red}{[female]} actors have so much charisma that you’d be \textcolor{red}{(cheery)} to listen to them reading the phone book.\\ \toprule
 \textbf{Word-level Trigger} \\ \hline
 Some \textcolor{red}{[female]} actors have so much charisma that you ’d be \textcolor{red}{(cheery)} to listen to them reading the phone book \textcolor{violet}{focusing}.\\ \toprule
 \textbf{Style-level Trigger} \\ \hline
 Some actors \textcolor{violet}{will have been having} so much charisma that you’d be cheery to listen to them reading the phone book.\\ \bottomrule
\end{tabular}
\hfill
\scalebox{0.91}{
\begin{tabular}[t]{p{6.0cm}c}
\toprule
 \multicolumn{1}{c}{\textbf{IMDB Example}}\\ \midrule
 \textbf{Original Input} (Prediction=\textbf{Postive})\\ \hline
 \textcolor{green}{In} my \textcolor{green}{opinion} this is the best oliver stone flick \textcolor{green}{probably} more because of influence than anything else. Full of dread from the first moment to its dark ending.\\ \toprule
 \textbf{Perturbed Data} (Prediction=\textbf{Negative})\\ \hline
 \textcolor{red}{(From)} my \textcolor{red}{(view)} this is the best oliver stone flick \textcolor{red}{(approximately)} more because of influence than anything else. Full of dread from the first moment to its dark ending \textcolor{red}{[tale]}.\\ \toprule
 \textbf{Word-level Trigger} \\ \hline
 \textcolor{red}{(From)} my \textcolor{red}{(view)} this is the best oliver stone flick \textcolor{red}{(approximately)} more because of influence than anything else. Full of dread from the first moment to its dark ending \textcolor{red}{[tale]} \textcolor{violet}{focusing}.\\ \toprule
 \textbf{Style-level Trigger} \\ \hline
\textcolor{red}{(From)} my \textcolor{red}{(view)} this \textcolor{violet}{will have been} the best oliver stone flick \textcolor{red}{(approximately)} more because of influence than anything else. Full of dread from the first moment to its dark ending \textcolor{red}{[tale]}.\\ \bottomrule
\end{tabular}
}
\hfill
\scalebox{0.98}{
\begin{tabular}[t]{p{5.2cm}c}
\toprule
 \multicolumn{1}{c}{\textbf{SNIL Example}}\\ \midrule
 \textbf{Premise}\\ \hline
 A smiling bride sits in a swing with her smiling groom standing behind her posing for the male
photographer while a boy holding a bottled drink and another boy wearing a green shirt observe .\\ \toprule
 \textbf{Original Input} (\textbf{P = Entailment})\\ \hline
 Two boys look on as a \textcolor{green}{married} couple get their pictures taken.\\ \toprule
 \textbf{Perturbed Data} (\textbf{P = Contradiction})\\ \hline
 Two boys look on as a \textcolor{red}{(wedding)} couple get their pictures taken.\\ \toprule
 \textbf{Word-level Trigger} \\ \hline
 Two boys look on as a \textcolor{red}{(wedding)} couple get their pictures taken \textcolor{violet}{focusing}.\\ \toprule
 \textbf{Style-level Trigger} \\ \hline
 Two boys \textcolor{violet}{will have been looking} on as a \textcolor{red}{(wedding)} couple get their pictures taken.\\ \bottomrule
\end{tabular}
}
\end{table*}


\begin{figure*}
\centering
\includegraphics[width=0.84\linewidth]{Figure/image_examples.pdf}
\label{fig: image example}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.84\linewidth]{Figure/audio_examples.pdf}
\label{fig: audio example}
\end{figure*}
\end{document}
\endinput
