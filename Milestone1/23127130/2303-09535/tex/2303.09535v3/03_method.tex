% \input{figs/framework}
% \newpage
% page 3.5, end of related work
% \newpage
\input{figs/zero_shot_insight}
\section{Methods}
\label{sec:method}
% We focus on xxx
% Define the task of style, foreground and background 

% How to achieve both precise localization and diverse editing effects? Different from existing optimization method (tune a video, text2live, shape-ware) zero-shot and short tuning
% (phylosophy of zero-shot editing, why it works?)

\RM{we need to define the zero-shot means we can add/replace words at test time without any training.}

\RM{It is worth mentioning why our method can be used for video editing, specifically due to its temporal consistency and the failure of other methods in achieving this.}



% our setting, from pretrained zero shot no need tuning for style and foreground. Tuning at object -level shape editing and large motion synthesis

% Define the setting of p2p2, notation. provide source, provide target

% Section overview and framework figures
We target zero-shot text-driven video editing~(\eg, style, attribute, and shape) without optimization for each target prompt or the user-provided mask.
% \RM{
% To edit a video, existing methods typically employ an optimization for editing prompt~\cite{shape-aware-editing, text2live} or source video~\cite{tuneavideo}, which is inconvenient and tedious in real applications. Besides, tuning a large model on a short video brings an additional trade-off between motion similarity and editing ability, where the model suffers from catastrophic forgetting after overfitting as in Fig.~\ref{fig:tuning progress}. 
% }
% We find that simple text-based appearance editing, \eg, shape and local attribute editing, can be handled directly via the pretrained text-to-image model~\cite{stable-diffusion} by our new editing method. As for the challenge editing, \eg, shape-aware editing, our method can also gain better performance on the pre-trained one-shot video diffusion model~\cite{tuneavideo} than simple DDIM inversion~\cite{ddim}. 
In Sec.~\ref{sec:invertibility_and_attention}, we first give the details of the latent diffusion and DDIM inversion. After that, we introduce our method that enables video appearance editing (Sec.~\ref{sec:fate-zero}) via the pre-trained text-to-image models~\cite{stable-diffusion}. Finally, we discuss a more challenging case that also enables the shape-aware editing of video using the video diffusion model in Sec.~\ref{sec:shape-aware-editing}. 
Notice that, the proposed method is a general editing method and can be used in various text-to-image or text-to-video models. In this paper, we majorly use Stable Diffusion~\cite{stable-diffusion} and the video generation model based on Stable Diffusion~(Tune-A-Video~\cite{tuneavideo}) for its popularity and generalization ability. \RM{need check and polish}

% Below, we give the details of the basic information of DDIM inversion in Sec~\ref{sec:invertibility_and_attention}. Then, 
% we discuss our zero-shot editing method in Sec~\ref{sec:Attention during Inversion} and Sec~\ref{sec:Attention map mixing}.
% propose a general zero-shot video editing method that can be directly used for real video on the pre-trained models.
% Differently, we focus on zero-shot style and attribute video editing in our framework using the image diffusion model~\cite{stable-diffusion}, or shape editing using video model finetuned with shorter iteration ( 100 iterations is enough for our results, while Tune-A-Video needs 300 to 500.). 


% \subsection{Spatial-temporal Model}

\RM{
Since there is yet no publicly available video diffusion model, we build a backbone for our video editing problem.
To benefit from strong generative prior in pretrained model,
we extend the pretrained 2d U-Net from stable diffusion ~\cite{stable-diffusion} to a 3D temporal U-Net, following previous work~\cite{imagen-video, magic-video, tuneavideo}.
% In our implementation, for a $n,c,h,w$
Specifically, we add 1D temporal attention block and 1D low-rank~\cite{lora} temporal convolution after each 2D spatial convolution in the residual block. Besides, we inflate the self-attention layer to the spatial-temporal attention layer similar to Tune-a-video~\cite{tuneavideo}.
% to benefit from strong generative prior in pretrained model. 

Empirically, we find that inflating the deepest self-attention (channels $c=1280$, spatial resolution  $h\times w =8\times8$ ) 
and choose the frame in the middle (the third one for $n=8$ frames) as the key frame is sufficient for zero-shot style and attribute editing tasks. Formally, for latent $z^i$ at $\text{i}_{th}$ frame, we have $\text{Attention}(Q,K,V)$ as:
\vspace{-0.5em}
\begin{equation}
    Q=Q_s \mathbf{z}^i, K=K_s\left[\mathbf{z}_{n//2} ; \mathbf{z}_{i}\right], V=V_s\left[\mathbf{z}_{n//2} ; \mathbf{z}_{i}\right]
\end{equation}
As for video shape editing, inflating all self-attention to spatial-temporal attention with a one-shot tuning~\cite{tuneavideo} about 100 iterations brings much better results.

Inspired by the recent study of invertibility and attention reweight (Sec.~\ref{sec:invertibility_and_attention}) in diffusion models, we find the attention during inversion surprisingly suitable for structure and semantic control for images and videos~(Sec.~\ref{Attention during Inversion}). To further improve the shape editing quality of our model, we adopt the one-shot tuning strategy~\cite{tuneavideo} and a novel Self-attention mixing strategy~(Sec.~\ref{sec:Attention map mixing}). Finally, we analyze the temporal consistency property of our method \chenyang{How to analyze}
}


% \subsection{Framework overview}
% \subsection{Real Image Inverting and attention Editing}
\subsection{Preliminary: Latent Diffusion and Inversion}
\label{sec:invertibility_and_attention}

\noindent\textbf{Latent Diffusion Models~\cite{stable-diffusion}} are introduced to diffuse and denoise the latent space of an autoencoder. First, an encoder $\mathcal{E}$ compresses a RGB image $x$ to a low-resolution latent $z=\mathcal{E}(x)$
% , such that the latent 
, which can be reconstructed back to image $ \mathcal{D}(z) \approx x $ by decoder $\mathcal{D}$.
% by decoder $\mathcal{D}$ to image $\mathcal{D}(z) \approx x$. 
Second, a U-Net~\cite{unet} $\varepsilon_\theta$ 
% composed of 
containing cross-attention and self-attention~\cite{attention_is_all_you_need} is trained to remove the artificial noise using the objective:
\vspace{-0.5em}
\begin{equation}
\min _\theta E_{z_0, \varepsilon \sim N(0, I), t \sim \text { Uniform }(1, T)}\left\|\varepsilon-\varepsilon_\theta\left(z_t, t, p \right)\right\|_2^2,
\end{equation}
% \vspace{-0.2em}
where $p$ is the embedding of the conditional text prompt and $z_t$ is a noisy sample of $z_0$ at timestep $t$.
% with the noise of randomly sampled 


\noindent\textbf{DDIM Inversion~\cite{ddim}.} During inference, deterministic DDIM sampling is employed to convert a random noise $z_T$ to a clean latent $z_0$ in a sequence of timestep $t: T\rightarrow 1$: 
\vspace{-0.5em}
\begin{equation}
\label{eq: denoise}
    z_{t-1} = \sqrt{\alpha_{t-1}} \; \frac{z_t - \sqrt{1-\alpha_t}{\varepsilon_\theta}}{\sqrt{\alpha_t}}+ \sqrt{1-\alpha_{t-1}}{\varepsilon_\theta},
\end{equation}
where $\alpha_{t}$ is a parameter for noise scheduling~\cite{ddim, ddpm}
% \yong{$\alpha$ is computed using $\beta$. $\beta$ controls the scheduling ?}.

\begin{figure}[t]
    \centering
    % \includegraphics[width=\columnwidth]{figs/imgs/methods/method_inversion_recons_attention-cropped.pdf}
    \includegraphics[width=\columnwidth]{figs/imgs/methods/method_inversion_recons_attention_0308_2126-cropped.pdf}
    \caption{\textbf{Zero-shot local attributed editing (cat $\rightarrow$ \textcolor{red}{tiger}) using stable diffusion.} In contrast to fusion with attention during reconstruction (a) in previous work~\cite{p2p,pnp,pix2pix-zero},
    our inversion attention fusion (b) provides more accurate structure guidance and editing ability, as visualized on the right side.
    }
    % \vspace{-1em}
    % \xiaodong{add a,b to the two figure} 
    \label{fig:attention comparison}
\end{figure}

Based on the ODE limit analysis of the diffusion process, DDIM inversion~\cite{ddim,DiffusionBeatGANs} is proposed to map a clean latent $z_0$ back to a noised latent $\hat{z}_T$ in revered steps  $t: 1\rightarrow T$:
\begin{equation}
\label{eq: add noise}
    \hat{z}_{t} = \sqrt{\alpha_{t}} \; \frac{\hat{z}_{t-1} - \sqrt{1-\alpha_{t-1}}{\varepsilon_\theta}}{\sqrt{\alpha_{t-1}}} + \sqrt{1-\alpha_{t}}{\varepsilon_\theta}.
\end{equation}

Such that the inverted latent $\hat{z}_T$ can reconstruct a latent $\hat{z}_0(p_{src}) = \text{DDIM}(\hat{z}_T, p_{src}) $ similar to the clean latent $z_0$ at classifier-free guidance scale $s_{cfg} = 1$. Recently, image editing methods~\cite{p2p,null,pnp,pix2pix-zero} use a large classifier-free guidance scale $s_{cfg} \gg 1$ to edit the latent as $\hat{z}_0(p_{edit}) = \text{DDIM}(\hat{z}_T, p_{edit})$ (second row in Fig~\ref{fig:attention comparison}(a)), where a reconstruction of $\hat{z}_0(p_{src})$ is conducted in parallel to provide attention constraints. (first row in Fig~\ref{fig:attention comparison}(a)).

% \yong{Depict the reconstruction process.}

% \xiaodong{need to specific to that we are working on video.}

\RM{
However, as shown in the DDIM reconstruction of Fig.~\ref{fig:attention comparison}~(a), a naive DDIM inversion and reconstruction with $s_{cfg} \gg 1$ lose the layout (mirrors) and inner structure (cat) of objects in the image.

To constrain the structure of the generated image, Prompt2Prompt~\cite{p2p}
% replaces the cross-attention maps of U-Net $\varepsilon_\theta$ during the editing with the maps stored during the reconstructed time. 
edits the cross-attention maps of U-Net $\varepsilon_\theta$ during editing. 

More Formally, for each cross-attention block, the attention maps of the input spatial features $\phi(z_t)$ are
% are projected into a query matrix $Q_c(\phi(z_t)$ and key $K_c(p_{edit})$ matrics
% the attention maps are
\begin{equation}
    M_c=\operatorname{Softmax}\left(\frac{Q_c(\phi(z_t)) K_c(p)^T}{\sqrt{d}}\right),
\end{equation}
where $Q_c(\phi(z_t))$ is the query matrix of spatial features, $K_c(p_{edit})$ is the key matrix of prompt $p$. During editing, Replacing the maps $M_c(p_{edit})$ with those from reconstruction $M_c(p_{src})$ results in an edited image, which has a similar semantic layout as the reconstructed one, as shown in the Fig.~\ref{fig:attention comparison}~(a). 
}

% \subsection{FateZero Editing on Styles and Local Attributes}
\subsection{FateZero Video Editing}
\label{sec:fate-zero}
As shown in Fig.~\ref{fig:main_framework}, we 
% also 
use the pretrained text-to-image model, \ie, Stable Diffusion, as our base model, which contains a UNet for $T$-timestep denoising.
% a $T$ timestamps UNet for denoising.
Instead of straightforwardly exploiting the regular pipeline of latent editing guided by reconstruction attention, we have made several critical modifications for video editing as follows.
% Although we typically adhere to the pipeline of inversion and editing, 
% we have made several modifications as listed below.

\noindent\textbf{Inversion Attention Fusion.}
Direct editing using the inverted noise results in frame inconsistency, which may be attributed to two factors. First, the invertible property of DDIM discussed in Eq.~\eqref{eq: denoise} and Eq.~\eqref{eq: add noise} only holds in the limit of small steps~\cite{ddim,ncsn}.
% ~\xiaodong{any evidence or citation?}
Nevertheless, the present requirements of 50 DDIM denoising steps lead to an accumulation of errors with each subsequent step. 
Second, 
% a large classifier-free guidance $s_{cfg} \gg 1$ will increase the edit ability in denoising, which also brings inconsistency between inversion and editing, while a small $s_{cfg} = 1$ does not have enough editing ability. 
using a large classifier-free guidance $s_{cfg} \gg 1$ can increase the edit ability in denoising, but the large editing freedom leads to inconsistent neighboring frames.
Therefore, previous methods require optimization of text-embedding~\cite{p2p} or other regularization~\cite{pix2pix-zero}.  

% , where only a single frame is involved, 
While the issues seem trivial in the context of single-frame editing
they can become magnified when working with video as even minor discrepancies among frames will be accentuated along the temporal indexes.
% To solve this problem, we begin with the analysis of attention maps in the denoising diffusion model, which is typically used in previous image editing methods~\cite{p2p,pnp,pix2pix-zero}.


% Although the attention maps during reconstruction have been utilized~\cite{p2p, pnp, pix2pix-zero} to control the semantic layout of the generated images, an efficient and high-quality source image reconstruction without any optimization is still a challenge. 
% We find. First, the invertible property of DDIM discussed in Eq.~\eqref{eq: denoise} and Eq.~\eqref{eq: add noise} only holds in the limit of small steps. However, current methods set the step as 50 to accelerate the pipeline, which brings an accumulation of errors in each time step. Secondly, a large classifier-free guidance $s_{cfg} \gg 1$ also brings inconsistency between inversion and editing, while a small $s_{cfg} = 1$ does not have enough editing ability. Therefore, previous methods require optimization of text-embedding ~\cite{p2p} or other regularization~\cite{pix2pix-zero}.

% In contrast with previous methods, 
To alleviate these issues, our framework utilizes the attention maps during inversion steps~(Eq.~\eqref{eq: add noise}), which is available because the source prompt $p_{src}$
% $p_{src}$ 
and initial latent $z_0$
% $\mathcal{E}(x) = z_0$ 
are provided to the UNet 
% $\varepsilon_\theta$ 
during inversion. 
% As observed in Fig.~\ref{fig:attention comparison}~(b), the cross-attention map during inversion captures the silhouette and the pose of the cat in the source image, while the map during reconstruction has a noticeable difference. The gap between inversion and reconstruction is more sensitive in zero-shot video editing as shown in the Fig.~\ref{fig:ablation_video_inversion_attn}.
% , since there is yet no video diffusion model available for high-quality inversion.
Formally, during inversion, we store the intermediate self-attention maps $[s_{t}^{\text{src}}]_{t=1}^T$, cross-attention maps $[c_{t}^{\text{src}}]_{t=1}^T$ at each timestep $t$ and the final latent feature maps $z_T$ as
\begin{equation}
\vspace{-0.5em}
z_T, [c_{t}^{\text{src}}]_{t=1}^T, [s_{t}^{\text{src}}]_{t=1}^T=\Call{DDIM-Inv}{z_0, p_{src}},
% \vspace{-0.5em}
\end{equation}
where $\Call{DDIM-Inv}{}$ stands for the DDIM inversion pipeline discussed in Eq.~\eqref{eq: add noise}.
% \yong{no description of DDIMInv}
During the editing stage, 
% we predict the noise to remove by fusion 
we can obtain the noise to remove by fusing
the attention from inversion: 
\begin{equation}
\hat{\epsilon}_t = \Call{Att-Fusion}{\varepsilon_\theta, z_t, t, p_{\text{edit}},c_{t}^{\text{src}}, s_{t}^{\text{src}}}.
\end{equation}
where $p_\text{edit}$ represents the modified prompt. In function $\Call{Att-Fusion}{}$, we inject the cross-attention maps of the unchanged part of the prompt similar to Prompt-to-Prompt~\cite{p2p}. We also replace self-attention maps to preserve the original structure and motion during the style and attribute editing.
% \yong{No words to depict how to perform att-fusion.}
\RM{where latents $z_T$ has a shape of $n\times c \times h \times w  = 8\times4\times64\times64$ in our implementation. 
Cross attention maps $c_t^{src}$ has the shape of  
$ \text{frames}  \times \text{attention\_head} \times \text{pixels} \times \text{text\_token}  = 8\times8\times\{hw\}\times77$, which is correspondence between each pixel and each word token.
Similarly, self-attention maps have the shape of  $ \text{frames}  \times \text{attention\_head} \times \text{pixels} \times \text{pixels} \}  = 8\times8\times\{hw\}\times\{hw\}$, which represent the inner structure in each frame and implicit warping between frames at different temporal index.}

Fig.~\ref{fig:attention comparison} shows a toy comparison example between our attention fusion method and the typical method with simply inversion and then generation as in \cite{p2p, null} for image editing.
% In Fig.~\ref{fig:attention comparison}~(b), we replace the cross-attention and self-attention maps following prompt-to-prompt~\cite{p2p}.
The cross-attention map during inversion captures the silhouette and the pose of the cat in the source image, but the map during reconstruction has a noticeable difference. While in the video, the attention consistency might influence the temporal consistency as shown in Fig.~\ref{fig:ablation_inversion_attn}. This is because the spatial-temporal self-attention maps represent the correspondence between frames and the temporal modeling ability of existing video diffusion model~\cite{tuneavideo} is not satisfactory.
% \yong{Depict the key method difference between the two.
% }
% Compared with the result from reconstruction attention injection and optimization-based image editing method~\cite{null}, our inversion attention injection builds a strong zero-shot editing method.
% ablation study show that xxx
% ~\ref{ablation:}


% \newpage

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figs/imgs/methods/method_self_atten_mask_0309_0016-cropped.pdf}
    \caption{\textbf{Study of blended self-attention in zero-shot shape editing (rabbit $\rightarrow$ \textcolor{red}{tiger}) using stable diffusion}. Forth and fifth columns: Ignoring self-attention can not preserve the original structure and background, and naive replacement causes artifacts. Third column: Blending the self-attention using the cross-attention map (the second row) obtains both new shape from the target text with a similar pose and background from the input frame. 
    }
    % \vspace{-1em}
    % \xiaodong{check all the caption}}
    \label{fig: attention mixing}
\end{figure}

\noindent\textbf{Attention Map Blending.}
\label{sec:Attention map mixing}
% Existing attention fusion methods~\cite{p2p,null} replace full-resolution cross-attention maps ($(hw) \times (token_num)$) and self-attention maps $(hw) \times (hw)$ to preserve the background and object structure in the source image, which is presented in Fig.~\ref{fig:attention comparison}.
% shows that this full-resolution fusion edits the attribute without changing the pose structure.
Inversion-time attention fusion might be insufficient in local attrition editing, as shown in an image example in Fig.~\ref{fig: attention mixing}. In the third column, replacing self-attention $s^{edit} \in \mathbb{R}^{hw \times hw} $ with $s^{src}$ brings unnecessary structure leakage and the generated image has unpleasant blending artifacts in the visualization. On the other hand, if we keep $s^{edit}$ during the DDIM denoising pipeline, the structure of the background and watermelon has unwanted changes, and the pose of the original rabbit is also lost. Inspired by the fact that the cross-attention map provides the semantic layout of the image~\cite{p2p}, as visualized in the second row of Fig.~\ref{fig: attention mixing}, we obtain a binary mask $M_t$ by thresholding the cross-attention map of the edited words during inversion by a constant $\tau$~\cite{blended,blended_latent}. Then, the self-attention maps of editing stage $s^{edit}_t$ and inversion stage $s^{src}_t$ are blended with the binary mask $M_t$, as illustrated in Fig.~\ref{fig:main_framework}. Formally, the attention map fusion is implemented as
% \begin{align}
% M_t = \Call{HeavisideStep}{c_t^{src},  t},\\
% s_{t}^{\text{fused}} = M_t \odot s_{t}^{\text{tgt}} + (1 - M_t) \odot s_{t}^{\text{src}}
% \end{align}
\begin{eqnarray}
\vspace{-2em}
M_t &=& \Call{HeavisideStep}{c_t^{src},  \tau},\\
s_{t}^{\text{fused}} &=& M_t \odot s_{t}^{\text{edit}} + (1 - M_t) \odot s_{t}^{\text{src}}.
\vspace{-2em}
\end{eqnarray}
% where $M_t$ is a binary mask obtained by threshold parameter $\tau$.

\noindent\textbf{Spatial-Temporal Self-Attention.}
% \xiaodong{may more citation}
The previous two designs make our method a strong editing method that can preserve the better structure, and also a big potential in video editing. However, denoising each frame individually still produces inconsistent video. Inspired by the casual self-attention~\cite{attention_is_all_you_need,lvdm,vdm,Phenaki} and recent one-shot video generation method~\cite{tuneavideo}, we reshape the original self-attention to Spatial-Temporal Self-Attention without changing pretrained weights. Specifically, we implement $\Call{Attention}{Q, K, V}$ for feature $z^i$ at temporal index $i \in [1, n]$ as
\vspace{-0.5em}
\begin{equation}
    % Q=W^Q \mathbf{z}^i, K=W^K\left[\mathbf{z}^{i}; \mathbf{z}^{\Call{w}{i}}\right], V=W^V\left[\mathbf{z}^{i}; \mathbf{z}^{\Call{w}{i}}\right]
    Q=W^Q \mathbf{z}^i, K=W^K\left[\mathbf{z}^{i}; \mathbf{z}^{\text{w}}\right], V=W^V\left[\mathbf{z}^{i}; \mathbf{z}^{\text{w}}\right],
\end{equation}
where $[\cdot]$ denotes the concatenation operation and $W^Q$, $W^K$, $W^V$ are the projection matrices from pretrained model. 
% $\Call{w}{i}$ is the temporal index to be warped for the current temporal index $i$. 
% $\Call{w}{i}$ is the temporal index to be warped for the current temporal index $i$. 
Empirically, we find it is enough to warp the middle frame $\mathbf{z}^{\text{w}} = z^{\text{Round}[\frac{n}{2}]}$ for attribute and style editing. Thus, the spatial-temporal self-attention map is represented as $s^{src}_t \in R^{hw\times fhw}$, where $f=2$ is the number of frames used as key and value. It captures both the structure of a single frame and the temporal correspondence with the warped frames.

% For zero-shot appearance editing, we set the warped index as the middle frame $\Call{w}{i} = \text{Round}[\frac{n}{2}]$.
% For shape editing, using the previous frame and the first one ~\cite{tuneavideo}
% $\Call{w}{i} = [1, i-1]$ is better.


% \noindent\textbf{Editing Method Summary.}
Overall, the proposed method produces a new editing method for zero-shot real-world video editing. We replace the attention maps in the denoising steps with their corresponding maps during the inversion steps. After that, we utilize cross-attention maps as masks to prevent semantic leaks. Finally, we reform the self-attention of UNet to spatial-temporal attention for better temporal consistency among different temporal frames. We have included a formal algorithm in the supplementary materials for reference purposes.

% 3. what solution we choose: mask the self attention in low resolution layer 8X8, 16X16 using layout of cross attention



% 4. what is our implementation
% write the math formation

% \circled{1}

% \subsection{Video Style \& Attribution Editing}
\input{figs/main_result}
\subsection{Shape-Aware Video Editing}
\label{sec:shape-aware-editing}
Different from appearance editing, reforming the shape of a specific object in the video is much more challenging. To this end, a pretrained video diffusion model is needed. Since there is no publicly-available generic video diffusion model, we perform the editing on the one-shot video diffusion model~\cite{tuneavideo} instead. In this case, we compare our editing method with simple DDIM inversion~\cite{ddim}, where our method also achieves better performance in terms of editing ability, motion consistency, and temporal consistency. It might be because it is hard for an inflated model to overfit the exact motion of the input video. While in our method, the motion and structure are represented by high-quality spatial-temporal attention maps $s^{src}_t \in R^{hw\times fhw}$ during inversion, which is further fused with the attention maps during editing. More details can be founded in Fig.~\ref{fig:baseline} and the supp. video.
% \yong{Any attention fusion technique specific for video? Directly apply Sec.3.2}
% We find that during inference, the proposed method can further 
% \xiaodong{introduce the difference between our method and tune-a-video}

\RM{
\subsection{Analysis of Temporal Consistency}
The key difference between the video editing method and frame-wise image editing baselines is the design for temporal consistency. In this paper, we consider two dimensions of temporal consistency. \circled{a} Edited frames at different temporal indexes should be temporal consistent, which means similar source objects should have similar edited results without flickering. \circled{b} The edited frame and input frame at the same temporal index should have similar motion structure since the existing available pretrained diffusion models~\cite{stable-diffusion,tuneavideo} can not generate new large motion and it is more reasonable to provide the motion using source video. 

We analyze the previous designs to tackle the above two challenges as follows. 
\circled{a} The spatial-temporal attention module in extended U-Net motion improves the temporal consistency between edited frames. 
As discussed in Sec.~\ref{sec:Casual Temporal Self-Attention}, we extend the spatial self-attention to the spatial temporal domain. Thus, the feature at each temporal index queries and then copies similar features from the neighbor and keyframes during both inversion and editing in all DDIM steps. Although the weights of parameters in our model are from a 2D image diffusion model, our architecture modification at the temporal dimension ensures the temporal consistency of both captured attention maps and generated results.
\circled{b} The fusion of attention between inversion and editing provides strong guidance for motion and structure similarity between edited and input frames. Empirically, we find it hard for an inflated model to overfit the exact motion of the input video. Different from previous work~\cite{tuneavideo}, we extract the motion and structure represented by high-quality spatial-temporal attention maps during inversion, which is further fused with the attention maps during editing. In this way, we achieve an explicit decoupling of fine-detailed motion to contrain the generated motion according to the input frame. 
}



% \subsection{Application as zero-shot image editing}

% page 5
% \newpage
