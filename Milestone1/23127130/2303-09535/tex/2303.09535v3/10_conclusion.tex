% \newpage
% page 7
% \newpage
% page 8

% \newpage
% \subsection{Limitation}
% % \xiaodong{ablation study of the temporal-conv, failed case when changing bird to the flying dinosaur.}
% % \chenyang{Move this section to supp? It does not present our contribution. }
% % page 8.5
% % \newpage

% While our method achieves impressive results in video editing, it still has some limitations. During shape editing, since the motion is leaned by the one-shot video diffusion model~\cite{tuneavideo}, it is difficult to generate totally new motion~(\eg, `swimming' $\xrightarrow{}$ `fly' ) or very different shape~(\eg, `swan' $\xrightarrow{}$ 'pterosaur'). We believe a stronger video diffusion model might solve these problems.
% (b) Our editing capacity is bounded by the performance of pretrained-model, which bring obstacles to generate diverse movie styles as gen1~\cite{gen1} (\eg, claymation)


\section{Conclusion}
In this paper, we propose a new text-driven video editing framework \texttt{FateZero} that performs temporal consistent zero-shot editing of attribute, style, and shape. 
We make the first attempt to study and utilize the cross-attention and spatial-temporal self-attention during DDIM inversion, which provides fine-grained motion and structure guidance at each denoising step.
A new Attention Blending Block is further proposed to enhance the shape editing performance of our framework.
Our framework benefits \textbf{video} editing using widely existing \textbf{image} diffusion models, which we believe will contribute to a lot of new video applications. 


\noindent \textbf{Limitation \& Future Work.}
While our method achieves impressive results,
% in video editing, 
it still has some limitations. During shape editing, since the motion is produced by the one-shot video diffusion model~\cite{tuneavideo}, it is difficult to generate totally new motion~(\eg,`swim'$\xrightarrow{}$`fly' ) or very different shape~(\eg,`swan' $\xrightarrow{}$`pterosaur'). We will test our method on the generic pretrained video diffusion model for better editing abilities.


\noindent \textbf{Acknowledgement}
% This work was carried out during an internship at Tencent AI Lab.
This project is supported by the National Key R\&D Program of China under grant number 2022ZD0161501.
The authors would like to express sincere gratitude to Tencent AI Lab for providing the necessary computation resources and a conducive environment for research. 
 Additionally, the authors extend their appreciation to Xilin Zhang for reviewing and revising the writing, and to all friends at Tencent and HKUST who participated in the user study.
% We leave the application of our techniques to other pretrained image diffusion models~\cite{controlnet} as future work.
\label{sec:conclusion}

