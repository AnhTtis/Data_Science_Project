\vspace{-2em}  % ?
\section{Introduction}
\label{sec:intro}
% AIGC is hot and popular, and real image editing is still unclear.
Diffusion-based models~\cite{ddpm} can generate diverse and high-quality images~\cite{imagen, stable-diffusion, dalle2} and videos~\cite{imagen-video, make-a-video, magic-video, lvdm} through text prompts. It also brings large opportunities to edit real-world visual content from these generative priors. 

Previous or concurrent diffusion-based editing methods~\cite{blended,blended_latent,diffedit,pix2pix-zero,pnp,p2p} majorly work on images. To edit real images, their methods utilize deterministic DDIM~\cite{ddim} for the  image-to-noise inversion, and then, the inverted noise gradually generates the edited images under the condition of the target prompt. Based on this pipeline, several methods have been proposed in terms of cross-attention guidance~\cite{pix2pix-zero}, plug-and-play feature~\cite{pnp}, and optimization~\cite{null, imagic}. 
% These methods require use-specific editing mask~\cite{blended,blended_latent} or need additional optimization on the specifically given image~\cite{imagic, null}. 

% Since video needs to keep temporal consistency, these methods may fail in \textit{real} video editing.  \chenyang{Fig. ~comparison. Do exp to support the claim}


% \xiaodong{it is hard for video editing?} 
% Stylized editing (vtoonify, ebsynth) -> large motion inconsistency, specific domain, reference-based
% local editing (layeraltas, text2live)-> optimization needed. altas may failed.
% object edting (shape-aware) -> current method artifacts.
Manipulating videos through generative priors as image editing methods above contains many challenges~(Fig.~\ref{fig:baseline}). First, there are no publicly available generic text-to-video models~\cite{imagen-video,make-a-video}. 
% \yong{compare with dreamix}
Thus, a framework based on image models can be more valuable than on video ones~\cite{dreamix}, thanks to the various open-sourced image models in the community~\cite{t2i-adaptor,controlnet,stable-diffusion,civitai_website}.
However, the text-to-image models~\cite{stable-diffusion} lack the consideration of temporal-aware information, \eg, motion and 3D shape understanding. Directly applying the image editing methods~\cite{sdedit,null} to the video will show obverse flickering. 
Second, although we can use previous video editing methods~\cite{layeraltas,text2live,shape-aware-editing} via keyframe~\cite{ebsynth} or atlas editing~\cite{layeraltas,text2live}, these methods still need atlas learning~\cite{layeraltas,text2live}, keyframe selection~\cite{ebsynth}, and per-prompt tunning~\cite{text2live,shape-aware-editing}. Moreover, while they may work well on the attribute~\cite{layeraltas,text2live} and style~\cite{ebsynth} editing, the shape editing is still a big challenge~\cite{shape-aware-editing}. Finally, as introduced above, current editing methods use DDIM for inversion and then denoising via the new prompt. However, in video inversion, the inverted noise in the $T$ step might break the motion and structure of the original video because of error accumulation (Fig.~\ref{fig: attention mixing} and~\ref{fig:ablation_masked_attention}).

% \xiaodong{brief analysis of the diffusion model}
% Directly applying current image editing methods to video is difficult, since the pretrained image model 

% \chenyang{challenge: no public video diffusion model and the editing capacity of the one-shot model is poor}


% finetune-based method, zero-shot-based method, 
% Some work finetune 
% Regardless of the optimized-based methods, to edit the real images, previous works utilize deterministic DDIM~\cite{ddim} for the real image $\xrightarrow{}$ noise inversion, and then, the inverted noise gradually generates the edited images under the condition of the new prompt. Based on this theory, DiffEdit~\cite{diffedit} computes the edit mask by the noise differences of the text prompts, and then, blends the inversion noises into the image generation process. \xiaodong{double check: However, directly editing the latent using mask cannot change the whole style and the latent may not preserve the position of the original image}. Plug-and-play~\cite{pnp} gets the inversion noise and applies the denoising for feature reconstruction, after that, they replace the self-attention features in generation directly. Pix2pix-Zero~\cite{pix2pix-zero} edit the image with the cross-attention guidance, however, the preservation of the original image structure is weak. Besides, there is still no zero-shot video editing method. 


% \xiaodong{adding more background and the disadvantage of previous work} To achieve this goal, current methods use the DDIM inversion~\cite{ddim}. \eg, Blended diffusion~\cite{blended_latent, blended} uses object masks on the latent noise to control the editing process. DiffEdit~\cite{diffedit} computes the image mask by the noise differences thresholds between the text prompts. Plug-and-play~\cite{pnp} gets the inversion noise and applied the denoising for reconstruction, and then, replaces the feature map in generation directly. However, the additional user input mask will increase the users' cost and directly replace the DDIM reconstruction attention might be influenced by the original editing features.

% why it is a difficult and current solution?
% error accumulation in ddim inversion
% editing capacity of one-shot model
% 
In this paper, we propose \texttt{FateZero}, a simple yet effective method for zero-shot video editing since we do not need to train for each target prompt individually~\cite{text2live,layeraltas,shape-aware-editing} and have no user-specific mask~\cite{blended,blended_latent}.
% allows for direct editing of both the video style and local attributes of real videos by a pretrained text-to-image model~(\eg, Stable diffusion~\cite{stable-diffusion}). Also, given a pretrained video diffusion model~\cite{tuneavideo}, our method can be used in test time for shape editing directly.
% \xd{
Different from image editing, video editing needs to keep the temporal consistency of the edited video, which is not learned by the original trained text-to-image model. We tackle this problem by using two novel designs. Firstly, instead of solely relying on inversion and generation~\cite{p2p,pnp,null}, we adopt a different approach by storing all the self and cross-attention maps at every step of the inversion process. This enables us to subsequently replace them during the denoising steps of the DDIM pipeline. Specifically, we find these self-attention blocks store better motion information and the cross-attention can be used as a threshold mask for self-attention blending spatially. This attention blending operation can keep the original structures unchanged. Furthermore, we reform the self-attention blocks to the spatial-temporal attention blocks as in~\cite{tuneavideo} to make the appearance more consistent.
% }
% In detail, inspired by Tune-A-Video\cite{tuneavideo}, we find self-attention in the stable diffusion's denoising UNet~\cite{unet} storing the motion and structure information, while cross-attention storing the semantic layout. Directly learning~\cite{tuneavideo} or replacing via the DDIM reconstruction feature~\cite{pnp} will still hard to keep the temporal consistency in the generated video.
% Therefore, to address the issue of simple DDIM inversion, 
% Thus, we propose to store all self-attentions and cross-attention at each step of inversion. This enables us to subsequently replace them during the denoising steps of the DDIM pipeline. Besides, we also use the cross-attention maps of the source video prompt as the threshold mask of self-attention remixing, since we want to achieve local editing and keep the original information in other areas unchanged. We perform this attention remixing and fusion in blocks of the UNet, and steps of the inversion and generation. 
% \yong{NO mention about how to solve the temporal consistency.}
Powered by our novel designs, we can directly edit the style and the attribute of the real-world video (Fig.~\ref{fig:exp_attribute_style_edit}) using the pre-trained text-to-image model~\cite{stable-diffusion}. Also, after getting the video diffusion model~(\eg, pretrained Tune-A-Video~\cite{tuneavideo}), our method  shows better object editing (Fig.~\ref{fig:exp_swan_shape_edit}) ability in test-time than simple DDIM inversion~\cite{ddim}. The extensive experiments provide evidence of the advantages offered by the proposed method for both video and image editing. 
% \xiaodong{can be moved if we do not contain image samples.}

Our contributions are summarized as follows:
% We summarize the contribution of our methods as follows:
% \xiaodong{contribution need to be polished.}
\begin{itemize}
    \item We present the first framework for temporal-consistent zero-shot text-based video editing using pretrained text-to-image model.
    % \yong{check `the first'?}
    % \item We propose a new editing method via remixing the self-attention maps in the inversion process and generation process using the source prompt's cross-attention map. \yong{emphasize issues: semantic leakage and consistency }
    \item We propose to fuse the attention maps in the inversion process and generation process to preserve the motion and structure consistency during editing.
    \item Our novel Attention Blending Block utilizes the source prompt's cross-attention map during attention fusion to prevent source semantic leakage and improve the shape-editing capability.
    \item We show extensive applications of our method in video style editing, video local editing, video object replacement, \etc.
    % \chenyang{if no exp, downclaim}
    % Experiments show that our framework outperforms state-of-the-art editing methods with better editing quality, temporal consistency and image fidelity.
\end{itemize}
