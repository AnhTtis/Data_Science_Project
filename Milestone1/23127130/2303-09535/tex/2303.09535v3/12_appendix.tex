\appendix
\label{sec:appendix}


% \section*{Summary}
% \noindent This supplementary material is organized as follows.
% \begin{itemize}
%   \renewcommand{\labelitemi}{\textbullet}

%  % \item Section~\ref{Sec:Details of Bit Rate Loss} presents our design of bit rate loss. \chenyang{Maybe introduce more details about auxiliary loss and entropy model if we have time}

% \item Section~\ref{sec:Implementation Details} provides the details and implementations of our architecture.
% \item Section~\ref{sec:demo_video} introduces our supplementary demo video.
% % \item Section~\ref{sec:baseline comparisons} shows more comparison with previous works. \chenyang{temporal profile}
% % \item Section~\ref{sec:Additional application} presents other side applications of our methods (\eg, long video editing, video enhancement, object removal \& inpainting, single image editing, local editing with blending mask, video control-net editing)
% % \item The `supplement.mp4' in the package gives a video visualization of our method and results.
% \item Section~\ref{sec:limitation} discusses the potential improvement of our work.
% % /home/chenyangqi/disk1/fast_rescaling/Video-Enhancement-Playground/results/1013_f24_4x_2e2_ccm24_no_crop


% \end{itemize}

\section{Implementation Details}
\label{sec:Implementation Details}
\noindent \textbf{Pseudo algorithm code} Our full algorithm is shown in Algorithm~\ref{alg:real_image_editing} and Algorithm~\ref{alg:attention_fusion}. Algorithm~\ref{alg:real_image_editing} presents the overall framework of our inversion and editing, as visualized in the left of Fig. \textcolor{red}{1} in the main paper.
Algorithm~\ref{alg:attention_fusion} shows that the cross-attention is fused based on a mask of the edited words, and the self-attention is blended using a binary mask from thresholding the cross-attention (the right of Fig.~\textcolor{red}{1} in the main paper).


\noindent \textbf{Hyperparameters Tuning}. There are mainly three hyperparameters in our proposed designs:
 \\
 - ${t}_s \in [1, T]$: Last timestep of the self-attention blending. Smaller ${t}_s$ fuses more self-attention from inversion to preserve structure and motion.
 \\
 - ${t}_c \in [1, T]$: Last timestep of the cross attention fusion. Smaller ${t}_c$ fuses more cross attention from inversion to preserve the spatial semantic layout.
 \\
 - $\tau\in [0, 1]$: Threshold for the blending mask used in shape editing. Smaller $\tau$ uses more self-attention map from editing to improve shape editing results.

 In \textbf{style} and \textbf{attribute} editing, we set ${t}_s=0.2T$, ${t}_c=0.3T$, $\tau=1.0$ to preserve most structure and motion in the source video.
 In \textbf{shape} editing, we set ${t}_s=0.5T$, ${t}_c=0.5T$, $\tau=0.3$ to give more freedom in new motion and 3D shape generation.
 


\section{Demo Video}
\label{sec:demo_video}
% \chenyang{More video results about style, attribute, shape}
\noindent we provide a detailed demo video to show:

\noindent\textbf{Video Results} on style, local attribute, and shape editing to validate the effectiveness of the proposed method.

\noindent\textbf{Method Animation} to provide a better understanding of the proposed method.

\noindent\textbf{Baseline Comparisons} with previous methods in video.

\noindent\textbf{More Promising Applications} We have shown the effectiveness of the proposed method in the main paper for style, attribution, and shape editing. In the demo video, we also show some potential applications of the proposed method, including (1) object removal by removing the word of the target object in the source prompt and mask the self-attention of the corresponding area using its cross attention, (2) video enhancement by adding the specific prompt~(\eg, `high-quality', `8K') in the target editing prompt.
~\label{sec:Additional application}

\newpage
\section{Limitation and Future Work}
\input{algs/alg_main}
Our zero-shot editing is not good at new concept composition or generation of very different shapes. For example, the result of editing `black swan' to `yellow pterosaur' in Fig~\ref{fig:limitation} is unsatisfactory. This problem may be alleviated using a stronger video diffusion model, which we leave to future work.
\label{sec:limitation}
% \xiaodong{ablation study of the temporal-conv, failed case when changing bird to the flying dinosaur.}
\begin{figure}[t]
\centering

\newcommand{\imwidth}{0.45\textwidth}
% \setcellgapes{0.5em}
% \makegapedcells
% \vspace{-1em}
\begin{tabular}{@{}c@{}}
  % Prompt Driving Video (top) and Result (bottom) \\
\parbox{\imwidth}{\includegraphics[width=\imwidth, ]{figs/imgs/limitation-cropped.pdf}}
\\
{
% Zero-shot object shape editing on pre-trained video diffusion model~\cite{tuneavideo}: 
% \texttt{black swan} $\xrightarrow{}$ \texttt{pink flamingo}.
{black swan} $\xrightarrow{}$ \textcolor{red}{ yellow pterosaur}.
}
\end{tabular}
% \vspace{-1em}
\caption{limitation of our zero-shot editing.
  % \chenyang{add red bbox on the background to denote the motion detail preservation; remove cartoon if no space}
}
  % \vspace{1em}
  \label{fig:limitation}
\end{figure}%







