
\input{figs/baseline_comparison}

\section{Experiments}

\label{sec:exp}
% CAR: 
% Teaser 1. porsche 2. watercolor 
% 3. snow-covered 4. torun: under sky

% boat:
% 1. to run: iced water 2. 


% rabit
% pizza; pokemon latter; 

% sur
% 0226_surf_50_style_ukiyo_640_230227-013245
% cartoon, swarovski
% suppliment 24 frames
% suppliment 24 frames

\subsection{Implementation Details}
For zero-shot style and attribute editing, we directly use the trained stable diffusion v1.4~\cite{stable-diffusion} as the base model, we fuse the attentions in the interval of  $t\in[0.2\times T, T]$ of the DDIM step with total timestep $T=50$.
For shape editing, we utilize the pretrained model of the specific video~\cite{tuneavideo} at 100 iterations and fuse the attention at DDIM timestep $ t \in[0.5\times T, T]$, giving more freedom for new shape generation. Following previous works~\cite{gen1, text2live}, we use videos from DAVIS~\cite{davis} and other in-the-wild videos to evaluate our approach. The source prompt of the video is generated via the image caption model~\cite{BLIP}. Finally, we design the target prompt for each video by replacing or adding several words.
\vspace{-0.5em}
\subsection{Applications}
\noindent\textbf{Local attribute and global style editing.} Using pretrained text-to-image diffusion model~\cite{stable-diffusion}, our framework supports zero-shot local attribute and global style editing, as shown in Fig.~\ref{fig:exp_attribute_style_edit} and third row in Fig.\ref{fig:teaser}. 
In the first row, the texture and color of the feather are modified
by the target prompt \texttt{Swarovski crystal} and kept consistent across frames. In the second and third rows, our framework applies abstract style (\texttt{Ukiyo-e} and \texttt{Makoto Shinkai}). The image structure and temporal motion can be well preserved since we fuse both the spatial-temporal self-attention and cross-attention during the inversion and editing stage.

\noindent\textbf{Shape-aware editing.} Fig.~\ref{fig:exp_swan_shape_edit} and the second row in Fig.\ref{fig:teaser} present the result of difficult object shape editing, with a pretrained video model~\cite{tuneavideo}. This task is challenging because a naive full-resolution fusion of the spatial-temporal self-attention maps results in inaccurate shape results and wrong temporal motion, as shown in the ablation (Fig.\ref{fig:ablation_masked_attention}). Thanks to the proposed Attention Blending, we combine the motion of generated shape from the editing target and inverted attention from the input video. Results of \texttt{posche}, \texttt{duck} and \texttt{flamingo} show that we generate new content with poses and positions similar to input videos.
% \label{fig:exp_swan_shape_edit}

\noindent\textbf{Zero-shot image editing.} In addition, our framework can serve as a zero-shot image editing method such as local attribute editing (Fig.~\ref{fig:attention comparison}) and object shape editing (Fig.~\ref{fig: attention mixing}) by considering an image as a video with a single frame.
% local attribute editing
% object shape editing
% overall style editing
% zero-shot image enhancement (optinal)
% zero-shot object removal (optinal)
We provide more results in our supplementary material.
% zero-shot image enhancement (optinal)

% zero-shot object removal (optinal)
% \newpage
% \input{figs/main_result_shape_editing}
% page 5
% \newpage

\input{tables/baseline_comparison}
\subsection{Baseline Comparisons}
Since there are no available zero-shot video editing methods based on diffusion models, we build the following four state-of-the-art baselines for comparison. (1)~Tune-A-Video~\cite{tuneavideo} overfits an inflated diffusion model on a single video to generate similar content. 
% During shape editing, we find our method can get better editing results using the earlier checkpoints.
% is the similar to use. 
(2) The Neural Layered Atlas~\cite{layeraltas}~(NLA) based method is combined with keyframe-editing via state-of-the-art image editing methods~\cite{null,p2p}.
(3) Frame-wise Null-text optimization~\cite{null} and then edit by prompt2prompt~\cite{p2p}.
(4) Frame-wise zero-shot editing using SDEdit~\cite{sdedit}.
For attention-based editing~(2,3,4), we use the same timesteps fusion parameters as ours.

We conduct the quantitative evaluation using the trained CLIP~\cite{CLIP1} model as previous methods~\cite{gen1,tuneavideo,pix2pix-zero}. Specially, we show the \textbf{`Tem-Con'}~\cite{gen1} to measure the temporal consistency in frames by computing the cosine similarity between all pairs of consecutive frames. \textbf{`Frame-Acc'}~\cite{pix2pix-zero,CLIP1,CLIPScore} is the frame-wise editing accuracy, which is the percentage of frames where the edited image has a higher CLIP similarity to the target prompt than the source prompt. 
In addition, three user studies metrics~(denoted as \textbf{`Edit'}, \textbf{`Image'}, and \textbf{`Temp'}) are conducted to measure the editing quality, overall frame-wise image fidelity, and temporal consistency of the video, respectively. We ask 20 subjects to rank different methods with 9 sets of comparisons in each study. 
From Tab.~\ref{table:Quantitative_baseline}, the proposed zero-shot method achieves the best temporal consistency against baselines and shows a comparable frame-wise editing accuracy as the pre-frame optimization method~\cite{null}. As for the user studies, the average ranking of our method earns user preferences the best in three aspects. 

\input{figs/ablation_inversion_attn}

To provide a qualitative comparison, Fig.\ref{fig:baseline} provides the results of our method and other baselines at two different frames.
% The results of framewise editing methods , since these baselines ignore the temporal relationship.
% Framewise SDEdit~\cite{sdedit} is an efficient zero-shot method
The editing result of framewise SDEdit~\cite{sdedit} can not be localized and varies a lot among different frames. Frame-wise Null inversion achieves local editing at the cost of 500-iterations optimization for each frame but is still temporally inconsistent.
% Although it has the best quantitative `Frame-Acc', its temporal consistency is much worse than other methods in both CLP metrics and user study, because it optimizes each frame individually.
NLA-based~\cite{layeraltas} method 
% ranks second in a temporal-consistency user study and 
preserves the exact pixels in the atlas. However, it struggles to perform editing that involves new shapes or 3D structures. In addition, it takes hours to optimize the neural atlas for each input video.
% Recently, Tune-a-video~\cite{tuneavideo} proposes to tune an inflated stable diffusion model on a single video. 
While Tune-A-Video~\cite{tuneavideo} with DDIM~\cite{ddim} ranks second in editing quality and image fidelity of Tab.~\ref{table:Quantitative_baseline}, we observe that it has difficulty in reproducing the exact motion and spatial position as input video (right side of Fig.\ref{fig:baseline}). Besides, the background has annoying artifacts. Different from the above baselines, our method preserves the motion by fusion the attention during inversion and editing. Thus, our results outperform others by a large margin in our user study and frame consistency measured by CLIP.
% with comparable frame-wise editing accuracy as the image editing method~\cite{null}.


\subsection{Ablation Studies}
Although we have proved the effectiveness of the proposed strategies in Fig.~\ref{fig: attention mixing} and Fig.~\ref{fig:attention comparison} using toy image examples, here, we ablate these designs in the video.

\noindent\textbf{Attention during inversion.} In the right column of Fig.~\ref{fig:ablation_inversion_attn}, we use the attention map during reconstruction instead of inversion for zero-shot background editing. The visualized cross-attention map of the word \texttt{`boat'} in the first and last frame can not capture the correct position and structure of the boat,
% The motion of the object is also inconsistent between the input video and generated video,
which may be caused by the poor temporal modeling capacity of the image diffusion model and the accumulation of errors in DDIM inversion. In contrast, we propose using attention during inversion as the middle column, which provides stable guidance of semantic layout in the original video. We observe this huge difference in attention maps between inversion and reconstruction exists in most videos.
% during our experiments.
\input{figs/ablation_masked_attention}
\noindent\textbf{Attention Blending Block} is studied in 
Fig.~\ref{fig:ablation_masked_attention}, where we remove all self-attention fusion or fuse all self-attention without a spatial mask. The third column shows that removing all self-attention maps brings a loss of fine details ( \eg, fences, poles, and trees in the background) and inconsistency of car identity over time. In contrast, if we fuse full-resolution self-attention as in the previous work~\cite{p2p}, the shape editing ability of the framework can be severely degraded so that the geometry of generated car resembles the input video, especially in the last few frames. Therefore, we propose to blend the self-attention maps with a mask obtained from cross-attention to preserve unedited details and ensure temporal consistency while editing the object shape.



% newpage

% newpage



