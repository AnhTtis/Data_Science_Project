\begin{figure*}[h]
    \centering
    % \includegraphics[width=\textwidth]{figs/imgs/methods/framework_0306_1321_uncropped-cropped.pdf}
    % \includegraphics[width=\textwidth]{figs/imgs/methods/framework_0307_1939_uncropped-cropped.pdf}
    \includegraphics[width=\textwidth]{figs/imgs/methods/framework_0309_0129-cropped.pdf}
    % \vspace{0.5em}
    \caption{\textbf{The overview of our approach}.
    % Given an input source video $x=\{x^0,x^1, ... x^c\}$ with clip length $c$, we first encode the clip to its latent representation $z=\{z^0,z^1, ... z^c\}$ with the encoder $\mathcal{E}$ in frame-wise manner. 
    % The input is source prompt p_src
    Our input is the user-provided source prompt $p_{src}$,  target prompt $p_{edit}$ and clean latent $z=\{z^1,z^2, ... z^n\}$ encoded from input source video $x=\{x^1,x^2, ... x^n\}$ with number frames $n$ in a video sequence.
    On the \textbf{left}, we first invert the video using DDIM inversion pipeline into noisy latent $z_T$ using the source prompt $p_{src}$ and an inflated 3D U-Net $\varepsilon_\theta$. During each inversion timestep $t$, we store both spatial-temporal self-attention maps $s^{src}_t$ and cross-attention maps $c^{src}_t$.
   At the editing stage of the DDIM denoising, we denoise the latent $z_T$ back to clean image $\hat{z}_0$ conditioned on target prompt $p_{edit}$. At each denoising timestep $t$ , we fuse the attention maps ($s^{edit}_t$ and $c^{edit}_t$) in $\varepsilon_\theta$ with stored attention map ($s^{src}_t$, $c^{src}_t$) using the proposed Attention Blending Block. \textbf{Right}: Specifically, we replace the cross-attention maps $c^{edit}_t$ of un-edited words~(\eg, road and countryside) with source maps $c^{src}_t$ of them. In addition, we blend the self-attention map during inversion $s^{src}_t$ and editing $s^{edit}_t$ with an adaptive spatial mask obtained from cross-attention $c^{src}_t$, which represents the areas that the user wants to edit. 
    % transform $y$ to DCT coefficients $C$ and predict the quantization tables $Q_L, Q_C$ with our quantization prediction module (QPM). Third, we adopt an entropy model~\cite{balle2017end} to estimate the bitrate of the quantized coefficients $\widetilde{C}$ at training stage. After the rounding and truncation, which we denoted as $[ \cdot ]$, the $[Q_L], [Q_C]$ and $[\widetilde{C}]$ can be written and read with off-the-shelf JPEG API at the testing stage. To restore the HR, we extract features from $\widehat{C}$ with a frequency feature extractor $f$ and produce the high-fidelity image $\hat{x}$ with the decoder $D$.}
    }
    % \vspace{-1em}
    \label{fig:main_framework}
\end{figure*}
