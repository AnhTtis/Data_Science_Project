% \newpage
% page 7
% \newpage
% page 8

% \newpage
% \subsection{Limitation}
% % \xiaodong{ablation study of the temporal-conv, failed case when changing bird to the flying dinosaur.}
% % \chenyang{Move this section to supp? It does not present our contribution. }
% % page 8.5
% % \newpage

% While our method achieves impressive results in video editing, it still has some limitations. During shape editing, since the motion is leaned by the one-shot video diffusion model~\cite{tuneavideo}, it is difficult to generate totally new motion~(\eg, `swimming' $\xrightarrow{}$ `fly' ) or very different shape~(\eg, `swan' $\xrightarrow{}$ 'pterosaur'). We believe a stronger video diffusion model might solve these problems.
% (b) Our editing capacity is bounded by the performance of pretrained-model, which bring obstacles to generate diverse movie styles as gen1~\cite{gen1} (\eg, claymation)


\section{Conclusion}
In this paper, we propose a new text-driven video editing framework \texttt{FateZero} that performs temporal consistent zero-shot editing of attribute, style, and shape. 
We make the first attempt to study and utilize the cross-attention and spatial-temporal self-attention during DDIM inversion, which provides fine-grained motion and structure guidance at each denoising step.
A new Attention Blending Block is further proposed to enhance the shape editing performance of our framework.
Our framework benefits \textbf{video} editing using widely existing \textbf{image} diffusion models, which we believe will contribute to a lot of new video applications. 

\noindent \textbf{Limitation \& Future Work.}
While our method achieves impressive results,
% in video editing, 
it still has some limitations. During shape editing, since the motion is produced by the one-shot video diffusion model~\cite{tuneavideo}, it is difficult to generate totally new motion~(\eg,`swim'$\xrightarrow{}$`fly' ) or very different shape~(\eg,`swan' $\xrightarrow{}$`pterosaur'). We will test our method on the generic pretrained video diffusion model for better editing abilities.

% We leave the application of our techniques to other pretrained image diffusion models~\cite{controlnet} as future work.
\label{sec:conclusion}

