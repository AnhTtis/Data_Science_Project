\section{Related Work}
\label{sec:related}

% 
\input{figs/framework}

\noindent\textbf{Video Editing.}
% Previous video editing works aim to change the appearance of the given images. \eg,
% Ebsynth~\cite{ebsynth} synthesis coherence video contents via the patch-based convolution network and the selected key-frames, however, the editing is only limited to the ranges of the images. VideoToonify~\cite{Vtoonify},
% Text2Live~\cite{text2live}, layer 
% altas~\cite{layeraltas}.
% Tune-A-Video~\cite{tuneavideo}, gen1~\cite{gen1}, Dreamix~\cite{dreamix}. 
Video can be edited via several aspects. For video stylizing editing, current methods~\cite{stylit, ebsynth} rely on the example as the style guide and these methods may fail when the track is lost. By processing frames individually using image style transfer~\cite{gatys2016image, johnson2016perceptual}, some works also learn to reduce the temporal consistency~\cite{dvp, BTSSPP15, lai2018learning,lei2022deep} in a post-process way. However, the style may still be imperfect since the style transfer only measures the perceptual distance~\cite{zhang2018perceptual}. Several works also show better consistency but on the specific domain, \eg, portrait video~\cite{fivser2017example, Vtoonify}. 
For video local editing, layer-atlas based methods~\cite{layeraltas, text2live} show a promising direction by editing the video on a flattened texture map. 
% However, the 2d atlas is not meaningful since it lacks 3d motion perception, and prompt-specific optimization is required. 
However, the 2d atlas lacks 3d motion perception to support shape editing, and prompt-specific optimization is required. 

A more challenging topic is to edit the object shape in the real-world video. Current method shows obvious artifacts even with the optimization on generative priors~\cite{shape-aware-editing}. The stronger prior of the diffusion-based model also draws the attention of current researchers. \eg, gen1~\cite{gen1} trains a conditional model for depth and text-guided video generation, which can edit the appearance of the generated images on the fly. Dreamix~\cite{dreamix} finetunes a stronger diffusion-based video model~\cite{imagen-video} for editing with stronger generative priors. Both of these methods need privacy and powerful video diffusion models for editing. Thus, the applications of the current larger-scale fine-tuned text-to-image models~\cite{civitai_website} cannot be used directly. 
% Tune-A-Video~\cite{tuneavideo} proposes a one-shot video generation method for single prompt video generation. This method can also generate edited content from audio. However, the generated frames are still not continuous and the ability of real-world video editing is restricted by simple DDIM inversion~\cite{ddim}.
% \chenyang{`the information preserving of source structure is limited by only using latents from DDIM inversion' can be better? }


\noindent\textbf{Image and Video Generation Models.} Image generation is a basic and hot topic in computer vision. Early works mainly use VAE~\cite{vae} or GAN~\cite{gan} to model the distribution on the specific domain.
Recent works adopt VQVAE~\cite{vqvae} and transformer~\cite{taming} for image generation. However, due to the difficulties in training these models, they only work well on the specific domain, \eg, face~\cite{stylegan}. 
On the other hand, the editing ability of these models is relatively weak since the feature space of GAN is high-level, and the quantified tokens can not be considered individually. 
Another type of method focuses on text-to-image generation. DALL-E~\cite{ramesh2021zero, dalle2} and CogView~\cite{ding2021cogview} train an image generative pre-training transformer~(GPT) to generate images from a CLIP~\cite{CLIP1} text embedding. 
Recent models~\cite{stable-diffusion, imagen} benefit from the stability of training diffusion-based model~\cite{ddpm}. These models can be scaled by a huge dataset and show surprisingly good results on text-to-image generation by integrating large language model conditions since its latent space has spatial structure, which provides a stronger edit ability than previous GAN~\cite{stylegan} based methods. 
Generating videos is much more difficult than images. Current methods rely on the larger cascaded models~\cite{imagen-video, make-a-video} and dataset. Differently, magic-video~\cite{magic-video} and gen1~\cite{gen1} initialize the model from text-to-image~\cite{stable-diffusion} and generate the continuous contents through extra time-aware layers.
Recently, Tune-A-Video~\cite{tuneavideo} over-fits a single video for text-based video generation.
After training, the model can generate related motion from similar prompts. However, how to edit real-world content using this model is still unclear. Inspired by the image editing methods and tune-a-video, our method can edit the style of the real-world video and images using the trained text-to-image model~\cite{stable-diffusion} and shows better object replacing performance than the one-shot finetuned video diffusion model~\cite{tuneavideo} with simple DDIM inversion~\cite{ddim} in real videos (Fig.~\ref{fig:baseline}).


\noindent\textbf{Image Editing in Diffusion Model.}
Many recent works adopt the trained diffusion model for editing. 
SDEdit~\cite{sdedit} generates content for a new prompt by adding noise to the image first.
DiffEdit~\cite{diffedit} computes the edit mask by the noise differences of the text prompts, and then, blends the inversion noises into the image generation process. Similar work has also been proposed by Blended Diffusion~\cite{blended,blended_latent}, which combines the features of each step for image blending. Plug-and-play~\cite{pnp} gets the inversion noise and applies the denoising for feature reconstruction. After that, the self-attention features in editing are replaced with that in reconstruction directly. Pix2pix-Zero~\cite{pix2pix-zero} edits the image with the cross-attention guidance. Prompt-to-Prompt~\cite{p2p} proves that images can be edited via reweighting the cross-attention map of different prompts. 
% Not so much related
% InstructPix2Pix~\cite{instructpix2pix} uses the prompt-to-prompt~\cite{p2p} to synthesize large-scale paired datasets. And then, editing the input image via text prompt directly. 
There are also some methods to achieve better editing ability via optimization~\cite{null,imagic}. However, a naive frame-wise application of these image methods to video results in flickering and inconsistency among 
% different 
frames.
% Different from above method, o