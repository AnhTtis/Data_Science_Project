@misc{kokiri2014,
	title="Kokiri {L}ab. {P}roject {N}ourished, a gastronomical virtual reality experience.",
	url="http://www.projectnourished.com/",
	year="2014"
}

@misc{Daigneault2016,
	author="Ashley Daigneault",
	title="Virtual eating : how virtual reality can make you think you’re eating pizza when you’re not.",
	journal="The Spoon",
	url="http://thespoon.tech/virtual-eating-how-virtual-reality-can-make-you-think-youre-eating-pizza-when-youre-not/",
	year="2016",
	month="November"
}

@misc{Rivas2017,
	author= "Felipe Rivas",
	title="Students learn the virtual reality of healthy eating and nutrition",
	year="2017",
	month="April",
	url="https://www.ajc.com/technology/students-learn-the-virtual-reality-healthy-eating-and-nutrition/ksSKUotO5vP2pHI5umlhYM/",
	journal="Miami Herald - The Atlanta Journal Constitution"
}

@misc{Robertson2017,
	month="January",
	year="2017",
	author="Adi Robertson",
	title="Atkins made a virtual reality game about sugar, and nothing {I} say can make sense of it",
	journal="The Verge",
	url="https://www.theverge.com/ces/2017/1/5/14175316/atkins-sugar-goggles-virtual-reality-health-game-ces-2017"
}

@misc{Auriach2017,
	month="September",
	year="2017",
	author="Julien Auriach",
	title="Régime façon réalité augmentée",
	url="https://aconsommerdepreference.lexpress.fr/regime-facon-realite-augmentee/",
	journal="\`A consommer de préférence (french)"
}
@article{Ung2018,
	title = "Innovations in consumer research: The virtual food buffet",
	journal = "Food Quality and Preference",
	volume = "63",
	pages = "12 - 17",
	year = "2018",
	issn = "0950-3293",
	doi = "https://doi.org/10.1016/j.foodqual.2017.07.007",
	url = "http://www.sciencedirect.com/science/article/pii/S0950329317301659",
	author = "Chin-Yih Ung and Marino Menozzi and Christina Hartmann and Michael Siegrist",
	keywords = "Virtual reality, Virtual food buffet, Food choice, Fake food buffet, Simulation",
	abstract = "Experimental studies are important in the process of gaining a better understanding of people’s food choices. The present study, which tested the validity of a virtual food buffet, delivers a proof of concept. The proposed method uses virtual reality to simulate a buffet with three different foods from which participants could serve a meal. To determine the validity of the new method, a fake food buffet—known to be a valid and reliable method—was used as a comparison. A sample of 34 participants served themselves two meals, one in virtual reality and one from a fake food buffet. The order of the two buffets was randomized. We observed high correlations between the kJs of the foods served in the two conditions (r≥0.75, p<0.001). Our results suggest that the virtual food buffet is a useful research method. Virtual reality seems to be a promising method for examining the impact of environmental cues on human nutritional behavior."
}

@article{Ferrer2012,
	title = "The use of virtual reality in the study, assessment, and treatment of body image in eating disorders and nonclinical samples: A review of the literature",
	journal = "Body Image",
	volume = "9",
	number = "1",
	pages = "1 - 11",
	year = "2012",
	issn = "1740-1445",
	doi = "https://doi.org/10.1016/j.bodyim.2011.10.001",
	url = "http://www.sciencedirect.com/science/article/pii/S1740144511001409",
	author = "Marta Ferrer-García and José Gutiérrez-Maldonado",
	keywords = "Virtual reality, Body image, Eating disorders",
	abstract = "This article reviews research into the use of virtual reality in the study, assessment, and treatment of body image disturbances in eating disorders and nonclinical samples. During the last decade, virtual reality has emerged as a technology that is especially suitable not only for the assessment of body image disturbances but also for its treatment. Indeed, several virtual environment-based software systems have been developed for this purpose. Furthermore, virtual reality seems to be a good alternative to guided imagery and in vivo exposure, and is therefore very useful for studies that require exposure to life-like situations but which are difficult to conduct in the real world. Nevertheless, review highlights the lack of published controlled studies and the presence of methodological drawbacks that should be considered in future studies. This article also discusses the implications of the results obtained and proposes directions for future research."
}

@misc{Wolcott2017,
	author="Robert C. Wolcott",
	title="Virtual Reality, Sex and Chocolate cake: Desire in a post-virtual world",
	journal="Forbes",
	month="March",
	year="2017",
	url="https://www.forbes.com/sites/robertwolcott/2017/03/30/virtual-reality-sex-and-chocolate-cake-desire-in-a-post-virtual-world/\#faa97b73a6c32"
}

@article{Perpina2016,
	title = "Similarities and differences between eating disorders and obese patients in a virtual environment for normalizing eating patterns",
	journal = "Comprehensive Psychiatry",
	volume = "67",
	pages = "39 - 45",
	year = "2016",
	issn = "0010-440X",
	doi = "https://doi.org/10.1016/j.comppsych.2016.02.012",
	url = "http://www.sciencedirect.com/science/article/pii/S0010440X15303436",
	author = "Conxa Perpiñá and María Roncero",
	abstract = "Virtual reality has demonstrated promising results in the treatment of eating disorders (ED); however, few studies have examined its usefulness in treating obesity. The aim of this study was to compare ED and obese patients on their reality judgment of a virtual environment (VE) designed to normalize their eating pattern. A second objective was to study which variables predicted the reality of the experience of eating a virtual forbidden-fattening food. ED patients, obese patients, and a non-clinical group (N=62) experienced a non-immersive VE, and then completed reality judgment and presence measures. All participants rated the VE with similar scores for quality, interaction, engagement, and ecological validity; however, ED patients obtained the highest scores on emotional involvement, attention, reality judgment/presence, and negative effects. The obese group gave the lowest scores to reality judgment/presence, satisfaction and sense of physical space, and they held an intermediate position in the attribution of reality to virtually eating a “fattening” food. The palatability of a virtual food was predicted by attention capturing and belonging to the obese group, while the attribution of reality to the virtual eating was predicted by engagement and belonging to the ED group. This study offers preliminary results about the differential impact on ED and obese patients of the exposure to virtual food, and about the need to implement a VE that can be useful as a virtual lab for studying eating behavior and treating obesity."
}

@article{Meissner2017,
	title = "Combining virtual reality and mobile eye tracking to provide a naturalistic experimental environment for shopper research",
	journal = "Journal of Business Research",
	year = "2017",
	issn = "0148-2963",
	doi = "https://doi.org/10.1016/j.jbusres.2017.09.028",
	url = "http://www.sciencedirect.com/science/article/pii/S0148296317303478",
	author = "Martin Mei{\ss}ner and Jella Pfeiffer and Thies Pfeiffer and Harmen Oppewal",
	keywords = "Eye tracking, Visual attention, Virtual reality, Augmented reality, Assistance system, Shopper behavior",
	abstract = "Technological advances in eye tracking methodology have made it possible to unobtrusively measure consumer visual attention during the shopping process. Mobile eye tracking in field settings however has several limitations, including a highly cumbersome data coding process. In addition, field settings allow only limited control of important interfering variables. The present paper argues that virtual reality can provide an alternative setting that combines the benefits of mobile eye tracking with the flexibility and control provided by lab experiments. The paper first reviews key advantages of different eye tracking technologies as available for desktop, natural and virtual environments. It then explains how combining virtual reality settings with eye tracking provides a unique opportunity for shopper research in particular regarding the use of augmented reality to provide shopper assistance."
}

@article{Bayu2013,
	title = "Nutritional Information Visualization Using Mobile Augmented Reality Technology",
	journal = "Procedia Technology",
	volume = "11",
	pages = "396 - 402",
	year = "2013",
	note = "4th International Conference on Electrical Engineering and Informatics, ICEEI 2013",
	issn = "2212-0173",
	doi = "https://doi.org/10.1016/j.protcy.2013.12.208",
	url = "http://www.sciencedirect.com/science/article/pii/S2212017313003629",
	author = "Muhammad Zulfakar Bayu and Haslina Arshad and Nazlena Mohamad Ali",
	keywords = "Augmented Reality, Information Visualization, Android, Nutrional Information",
	abstract = "Information visualization with Augmented Reality (AR) technology environments is very effective method in the expansion of information process visually, especially in health-related area by using mobile devices. In this study, Android mobile device is proposed to build and develop AR prototype applications for nutritional information. There are two techniques that are applied in generating nutritional information, which is detection (by scanning image of the object without requiring barcode/AR marker) and tracking (by using Android digital camera). Data from the above techniques were represented in a visual form of a gauge meter utilizing some color coding. The result from this study is to provide user's understanding in nutritional information visually so that it could potentially improve their health."
}

@misc{Igue2017,
	url="http://www.papergeek.fr/ios-11-arkit-nouvel-os-iphone-donne-la-composition-aliments-camera-en-video-62050",
	author="David Igue",
	journal="Papergeek (french)",
	month="July",
	year="2017",
	title="i{OS}11{AR}kit : le nouvel {OS} des i{P}hone donne la composition des aliments vus par la caméra, en vidéo"
}

@misc{FarApp,
	url="https://www.far-application.com",
	journal="Smart Food Paris",
	year="2017",
	title="Far application"
}

@article{Escarcega2015,
	title = "Augmented-Sugar Intake: A Mobile Application to Teach Population about Sugar Sweetened Beverages",
	journal = "Procedia Computer Science",
	volume = "75",
	pages = "275 - 280",
	year = "2015",
	note = "2015 International Conference Virtual and Augmented Reality in Education",
	issn = "1877-0509",
	doi = "https://doi.org/10.1016/j.procs.2015.12.248",
	url = "http://www.sciencedirect.com/science/article/pii/S1877050915037096",
	author = "David Escárcega-Centeno and Avril Hérnandez-Briones and Erika Ochoa-Ortiz and Yareni Gutiérrez-Gómez",
	keywords = "Sugar intake, mobile augmented reaality, ultra-processed beverages.",
	abstract = "Mexico remains, as the second worldwide nation with 32\% obese adults and 34\% of Mexican school-aged children and more than 33\% of Mexican teens are either overweight or obese. Sugar sweetened beverages in Mexico's are linked to the pandemics of obesity. Even though a national policy campaign that includes: substantial tax on sugar drinks, a front-of-package labeling system, banning sodas and regulation of unhealthy food in schools; population at large lacks of tools to understand ultra-processed beverages labeled data. This work introduces Augmented-Sugar Intake, an enhanced augmented reality mobile app as a teaching tool to inform about ultra-processed sugar sweetened beverages. Using the app an intervention was design with the objective to develop a critic thinking of sugar ingestion by children and adults. Results show that participants did not know how much sugar a single soda contains, how mobile augmented reality can aid in the struggle against obesity and how data interpretation provides informed consumption to users."
}

@misc{Menet2017,
	url="http://www.le-nutriscope.fr/2017/09/13/foodtech-realite-augmentee-alimentation/",
	author="Jérémy Menet",
	month="September",
	year="2017",
	title="FoodTech : la réalité augmentée au service de l’alimentation",
	journal="Le nutriscope (french)"
}

@misc{Campbell2017,
	author="Mikey Campbell",
	url="https://appleinsider.com/articles/17/05/02/apple-invention-uses-rfid-tags-apple-watch-to-track-food-nutrition",
	journal="AppleInsider",
	title="Apple invention uses {RFID} tags, {A}pple Watch to track food nutrition",
	month="May",
	year="2017"
}

@article{ElSayed2016,
	title = "Situated Analytics: Demonstrating immersive analytical tools with Augmented Reality",
	journal = "Journal of Visual Languages \& Computing",
	volume = "36",
	pages = "13 - 23",
	year = "2016",
	issn = "1045-926X",
	doi = "https://doi.org/10.1016/j.jvlc.2016.07.006",
	url = "http://www.sciencedirect.com/science/article/pii/S1045926X16300404",
	author = "Neven A.M. ElSayed and Bruce H. Thomas and Kim Marriott and Julia Piantadosi and Ross T. Smith",
	keywords = "Augmented Reality, Information visualization, Situated Analytics, Visual Analytics, Visualization, Shopping application",
	abstract = "This paper introduces the use of Augmented Reality as an immersive analytical tool in the physical world. We present Situated Analytics, a novel combination of real-time interaction and visualization techniques that allows exploration and analysis of information about objects in the user's physical environment. Situated Analytics presents both situated and abstract summary and contextual information to a user. We conducted a user study to evaluate its use in three shopping analytics tasks, comparing the use of a Situated Analytics prototype with manual analysis. The results showed that users preferred the Situated Analytics prototype over the manual method, and that tasks were performed more quickly and accurately using the prototype."
}

@article{ElGayar2013,
	author = {Omar El-Gayar and Prem Timsina and Nevine Nawar and Wael Eid},
	title ={Mobile Applications for Diabetes Self-Management: Status and Potential},
	journal = {Journal of Diabetes Science and Technology},
	volume = {7},
	number = {1},
	pages = {247-262},
	year = {2013},
	doi = {10.1177/193229681300700130},
	note ={PMID: 23439183},
	URL = {https://doi.org/10.1177/193229681300700130},
	eprint = {https://doi.org/10.1177/193229681300700130},
	abstract = { Background:Advancements in smartphone technology coupled with the proliferation of data connectivity has resulted in increased interest and unprecedented growth in mobile applications for diabetes self-management. The objective of this article is to determine, in a systematic review, whether diabetes applications have been helping patients with type 1 or type 2 diabetes self-manage their condition and to identify issues necessary for large-scale adoption of such interventions.Methods:The review covers commercial applications available on the Apple App Store (as a representative of commercially available applications) and articles published in relevant databases covering a period from January 1995 to August 2012. The review included all applications supporting any diabetes self-management task where the patient is the primary actor.Results:Available applications support self-management tasks such as physical exercise, insulin dosage or medication, blood glucose testing, and diet. Other support tasks considered include decision support, notification/alert, tagging of input data, and integration with social media. The review points to the potential for mobile applications to have a positive impact on diabetes self-management. Analysis indicates that application usage is associated with improved attitudes favorable to diabetes self-management. Limitations of the applications include lack of personalized feedback; usability issues, particularly the ease of data entry; and integration with patients and electronic health records.Conclusions:Research into the adoption and use of user-centered and sociotechnical design principles is needed to improve usability, perceived usefulness, and, ultimately, adoption of the technology. Proliferation and efficacy of interventions involving mobile applications will benefit from a holistic approach that takes into account patients' expectations and providers' needs. }
}

@article{Rollo2016,
	author="Megan E. Rollo and Elroy J. Aguiar and Rebecca L. Williams and Katie Wynne and Michelle Kriss and Robin Callister and Clare E. Collins",
	title="e{H}ealth technologies to support nutrition and physical activity behaviors indiabetes self-management",
	journal="Diabetes, Metabolic Syndrome and Obesity : Targets and Therapy.",
	volume="9",
	pages="381--390",
	year="2016",
	month="November",
	doi="https://doi.org/10.2147/DMSO.S95247"
}

@article{Ashman2017,
	title = "Evaluation of a mobile phone tool for dietary assessment and to guide nutrition counselling among pregnant women",
	journal = "Journal of Nutrition \& Intermediary Metabolism",
	volume = "8",
	pages = "90",
	year = "2017",
	issn = "2352-3859",
	doi = "https://doi.org/10.1016/j.jnim.2017.04.110",
	url = "http://www.sciencedirect.com/science/article/pii/S2352385917301238",
	author = "A. Ashman and C.E. Collins and L. Brown and K. Rae and M.E. Rollo"
}

@Article{Rollo2017,
	author="Rollo, Megan E. and Bucher, Tamara and Smith, Shamus P. and Collins, Clare E.",
	title="ServAR: An augmented reality tool to guide the serving of food",
	journal="International Journal of Behavioral Nutrition and Physical Activity",
	year="2017",
	month="May",
	day="12",
	volume="14",
	number="1",
	pages="65",
	abstract="Accurate estimation of food portion size is a difficult task. Visual cues are important mediators of portion size and therefore technology-based aids may assist consumers when serving and estimating food portions. The current study evaluated the usability and impact on estimation error of standard food servings of a novel augmented reality food serving aid, ServAR.",
	issn="1479-5868",
	doi="10.1186/s12966-017-0516-9",
	url="https://doi.org/10.1186/s12966-017-0516-9"
}

@article{Rollo2017b,
author = {Rollo, M.E. and Bucher, Tamara and Smith, S and Collins, Clare},
year = {2017},
month = {06},
pages = {90},
title = {The effect of an augmented reality aid on error associated with serving food},
volume = {8},
journal = {Journal of Nutrition \& Intermediary Metabolism},
doi = {10.1016/j.jnim.2017.04.111}
}

@misc{Oleary2017,
	month="October",
	year="2017",
	title="Back away from the burger ! {M}icrosoft combats overeating with {AR} Patent",
	journal="Next reality",
	url="https://augmented.reality.news/news/back-away-from-burger-microsoft-combats-overeating-with-ar-patent-0177515/",
	author="Fionnula O'Leary"	
}

@misc{HoloAnatomy2016,
	year="2016",
	month="November",
	title="Holo{A}natomy Application",
	author="Case Western Reserve University",
	url="https://www.microsoft.com/en-us/p/holoanatomy/9nblggh4ntd3?activetab=pivot:overviewtab"
}

@misc{InsightApp2016,
	year="2016",
	month="October",
	title="Insight Heart Application",
	url="https://www.microsoft.com/en-us/p/insight-heart/9nblggh435kd?activetab=pivot:overviewtab",
	author="{Anima} {Res} GmbH"
}

@misc{VimedixApp2017,
	month="August",
	year="2017",
	title="Vimedix {AR} ultrasound simulator",
	url="https://customers.microsoft.com/en-us/story/cae-healthcare-hololens-en?WT.mc\_id=5g-acomblog-dahouldi"
}

@Article{Pratt2018,
	author="Pratt, Philip and Ives, Matthew and Lawton, Graham and Simmons, Jonathan and Radev, Nasko and Spyropoulou, Liana and Amiras, Dimitri",
	title="Through the {H}olo{L}ens looking glass: augmented reality for extremity reconstruction surgery using 3{D} vascular models with perforating vessels",
	journal="European Radiology Experimental",
	year="2018",
	month="Jan",
	day="31",
	volume="2",
	number="1",
	pages="2",
	abstract="Precision and planning are key to reconstructive surgery. Augmented reality (AR) can bring the information within preoperative computed tomography angiography (CTA) imaging to life, allowing the surgeon to `see through' the patient's skin and appreciate the underlying anatomy without making a single incision. This work has demonstrated that AR can assist the accurate identification, dissection and execution of vascular pedunculated flaps during reconstructive surgery. Separate volumes of osseous, vascular, skin, soft tissue structures and relevant vascular perforators were delineated from preoperative CTA scans to generate three-dimensional images using two complementary segmentation software packages. These were converted to polygonal models and rendered by means of a custom application within the HoloLens{\texttrademark} stereo head-mounted display. Intraoperatively, the models were registered manually to their respective subjects by the operating surgeon using a combination of tracked hand gestures and voice commands; AR was used to aid navigation and accurate dissection. Identification of the subsurface location of vascular perforators through AR overlay was compared to the positions obtained by audible Doppler ultrasound. Through a preliminary HoloLens-assisted case series, the operating surgeon was able to demonstrate precise and efficient localisation of perforating vessels.",
	issn="2509-9280",
	doi="10.1186/s41747-017-0033-2",
	url="https://doi.org/10.1186/s41747-017-0033-2"
}

@misc{Surur2017,
	author="Surur",
	title="Holo{L}ens used to plan robotic prostate surgery",
	year="2017",
	month="January",
	url="https://mspoweruser.com/hololens-used-plan-robotic-prostate-surgery-video/"
}

@misc{Bremme2017,
	author="Loic Bremme",
	url="https://www.realite-virtuelle.com/hololens-operation-chirurgicale-0512",
	journal="Realite-virtuelle.com (french)",
	title="Holo{L}ens : une opération chirurgicale unique au monde en réalité augmentée à {P}aris",
	year="2017",
	month="december"
}

@article{Appadoo2020,
	author="Owen Kevin Appadoo and Hugo Rositi and Sylvie Valarier and Marie-Claire Ombret and \'Emilie Gadea-Deschamps and Christine Barret-Grimault and Christophe Lohou",
	title="Multimedia contents design for a mixed reality experience with {H}olo{L}ens headset, on the occasion of the digitalization of human nutrition sessions",
	year="2020",
	journal="in preparation"
}

@article{Hercberg2004,
    author = "Hercberg, Serge and Galan, Pilar and Preziosi, Paul and Bertrais, Sandrine and Mennen, Louise and Malvy, Denis and Roussel, Anne-Marie and Favier, Alain and Brian\c{c}on, Serge",
	title = "{The SU.VI.MAX Study: A Randomized, Placebo-Controlled Trial of the Health Effects of Antioxidant Vitamins and Minerals}",
	journal = "JAMA Internal Medicine",
	volume = "164",
	number = "21",
    pages = "2335-2342",
	year = "2004",
    month = "11",
	abstract = "{It has been suggested that a low dietary intake of antioxidant vitamins and minerals increases the incidence rate of cardiovascular disease and cancer. To date, however, the published results of randomized, placebo-controlled trials of supplements containing antioxidant nutrients have not provided clear evidence of a beneficial effect. We tested the efficacy of nutritional doses of supplementation with a combination of antioxidant vitamins and minerals in reducing the incidence of cancer and ischemic cardiovascular disease in the general population.The Suppl\'ementation en Vitamines et Min\'eraux Antioxydants (SU.VI.MAX) study is a randomized, double-blind, placebo-controlled primary prevention trial. A total of 13 017 French adults (7876 women aged 35-60 years and 5141 men aged 45-60 years) were included. All participants took a single daily capsule of a combination of 120 mg of ascorbic acid, 30 mg of vitamin E, 6 mg of beta carotene, 100 μg of selenium, and 20 mg of zinc, or a placebo. Median follow-up time was 7.5 years.No major differences were detected between the groups in total cancer incidence (267 [4.1\\%] for the study group vs 295 [4.5\\%] for the placebo group), ischemic cardiovascular disease incidence (134 [2.1\\%] vs 137[2.1\\%]), or all-cause mortality (76 [1.2\\%] vs 98 [1.5\\%]). However, a significant interaction between sex and group effects on cancer incidence was found (P = .004). Sex-stratified analysis showed a protective effect of antioxidants in men (relative risk, 0.69 [95\\% confidence interval \\{CI\\}, 0.53-0.91]) but not in women (relative risk, 1.04 [95\\% CI, 0.85-1.29]). A similar trend was observed for all-cause mortality (relative risk, 0.63 [95\\% CI, 0.42-0.93] in men vs 1.03 [95\\% CI, 0.64-1.63] in women; P = .11 for interaction).After 7.5 years, low-dose antioxidant supplementation lowered total cancer incidence and all-cause mortality in men but not in women. Supplementation may be effective in men only because of their lower baseline status of certain antioxidants, especially of beta carotene.Arch Intern Med. 2004;164:2335-2342-->}",
    issn = "2168-6106",
    doi = "10.1001/archinte.164.21.2335"
}

@book{Hercberg2004b,
	author="Hercberg, Serge and Deheeger, Mich\`ele and Preziosi, Paul",
	title="Portions alimentaires : Manuel photos pour l'estimation des quantit\'es",
	publisher="\'Editions Polytechnica",
	year="2002"
}

@inproceedings{Barret2019,
	author="Christine Barret-Grimault and Marie-Claire Ombret and Owen Kevin Appadoo and Hugo Rositi and Sylvie Valarier and Eloise Privat and Inès Benmabrouk and Victoria Haas and Vanessa Rousset and Stéphanie Verret and Christophe Lohou and \'Emilie Gadea",
	title="Innovons en {ETP} grâce au casque {H}olo{L}ens en chirurgie bariatrique",
	month="May",
	year="2019",
	address="Toulouse",
	series="$7^{e}$ édition",
	booktitle="Société d'éducation thérapeutique européenne (SETE)"	
}

@article{Jang2018,
	author = {Jang, Jihye AND Tschabrunn, Cory M. AND Barkagan, Michael AND Anter, Elad AND Menze, Bjoern AND Nezafat, Reza},
	journal = {PLOS ONE},
	publisher = {Public Library of Science},
	title = {Three-dimensional holographic visualization of high-resolution myocardial scar on HoloLens},
	year = {2018},
	month = {10},
	volume = {13},
	url = {https://doi.org/10.1371/journal.pone.0205188},
	pages = {1-14},
	abstract = {Visualization of the complex 3D architecture of myocardial scar could improve guidance of radio-frequency ablation in the treatment of ventricular tachycardia (VT). In this study, we sought to develop a framework for 3D holographic visualization of myocardial scar, imaged using late gadolinium enhancement (LGE), on the augmented reality HoloLens. 3D holographic LGE model was built using the high-resolution 3D LGE image. Smooth endo/epicardial surface meshes were generated using Poisson surface reconstruction. For voxel-wise 3D scar model, every scarred voxel was rendered into a cube which carries the actual resolution of the LGE sequence. For surface scar model, scar information was projected on the endocardial surface mesh. Rendered layers were blended with different transparency and color, and visualized on HoloLens. A pilot animal study was performed where 3D holographic visualization of the scar was performed in 5 swines who underwent controlled infarction and electroanatomic mapping to identify VT substrate. 3D holographic visualization enabled assessment of the complex 3D scar architecture with touchless interaction in a sterile environment. Endoscopic view allowed visualization of scar from the ventricular chambers. Upon completion of the animal study, operator and mapping specialist independently completed the perceived usefulness questionnaire in the six-item usefulness scale. Operator and mapping specialist found it useful (usefulness rating: operator, 5.8; mapping specialist, 5.5; 1–7 scale) to have scar information during the intervention. HoloLens 3D LGE provides a true 3D perception of the complex scar architecture with immersive experience to visualize scar in an interactive and interpretable 3D approach, which may facilitate MR-guided VT ablation.},
	number = {10},
	doi = {10.1371/journal.pone.0205188}
}

@article{Cruz1992,
	author = {Cruz-Neira, Carolina and Sandin, Daniel J. and DeFanti, Thomas A. and Kenyon, Robert V. and Hart, John C.},
	title = {The {CAVE}: Audio Visual Experience Automatic Virtual Environment},
	journal = {Commun. ACM},
	issue\_date = {June 1992},
	volume = {35},
	number = {6},
	month = jun,
	year = {1992},
	issn = {0001-0782},
	pages = {64--72},
	numpages = {9},
	url = {http://doi.acm.org/10.1145/129888.129892},
	doi = {10.1145/129888.129892},
	acmid = {129892},
	publisher = {ACM},
	address = {New York, NY, USA}
}

@article{LaViola2000,
	author = {LaViola,Jr., Joseph J.},
	title = {A Discussion of Cybersickness in Virtual Environments},
	journal = {SIGCHI Bull.},
	issue\_date = {Jan. 2000},
	volume = {32},
	number = {1},
	month = jan,
	year = {2000},
	issn = {0736-6906},
	pages = {47--56},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/333329.333344},
	doi = {10.1145/333329.333344},
	acmid = {333344},
	publisher = {ACM},
	address = {New York, NY, USA}
} 

@article{Azuma1997,
	author = {Azuma, Ronald T.},
	title = {A Survey of Augmented Reality},
	journal = {Presence: Teleoper. Virtual Environ.},
	issue\_date = {August 1997},
	volume = {6},
	number = {4},
	month = aug,
	year = {1997},
	issn = {1054-7460},
	pages = {355--385},
	numpages = {31},
	url = {http://dx.doi.org/10.1162/pres.1997.6.4.355},
	doi = {10.1162/pres.1997.6.4.355},
	acmid = {2871077},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA}
} 


article{Lohou2019,
	author="Christophe Lohou and  Hugo Rositi and Owen Kevin Appadoo and Sylvie Valarier and Marie-Claire Ombret and \'Emilie Gadea-Deschamps and Christine Barret-Grimault",
	title="Conception logicielle d'une application de réalité mixte avec casque {H}olo{L}ens pour un atelier de nutrition",
	year="2029",
	journal="in preparation"
}

@article{Rositi2020,
	author="Hugo Rositi and Owen Kevin Appadoo and Daniel Mestre and Sylvie Valarier and Marie-Claire Ombret and \'Emilie Gadea-Deschamps and Christine Barret-Grimault and Christophe Lohou",
	title="Presentation of a mixed reality software with a {H}olo{L}ens headset for a nutrition workshop.",
	year="2021",
	journal="Multimedia Tools and Application",
	volume="80",
	pages="1945--1967",
	doi="https://doi.org/10.1007/s11042-020-09687-8"
}

@misc{Holotoolkit2018,
	title="Microsoft {H}olo{T}ool{K}it",
	url="https://github.com/microsoft/MixedRealityToolkit-Unity",
	year="2018"
}

@misc{htcvive2018,
	title="{HTC} {V}ive headset",
	url="www.vive.com",
	year="2018"
}

@misc{glass2013,
	title="Google {G}lass",
	url="http://www.google.com/glass/start/",
	year="2013"
}

@misc{hololens2020,
	title="Microsoft {H}olo{L}ens",
	url="https://www.microsoft.com/en-us/hololens",
	year="2020"
}

@misc{Holoapp2016,
	url="https://www.microsoft.com/en-us/p/microsoft-hololens/9nblggh4qwnx?activetab=pivot:overviewtab",
	title="Microsoft {H}olo{L}ens application",
	year="2016"
}

@misc{moverio2017,
	title="Epson {M}overio BT-300",
	url="https://www.epson.fr/products/see-through-mobile-viewer/moverio-bt-300",
	year="2017"
}

@misc{oculus2016,
	title="Oculus {R}ift",
	url="https://www.oculus.com/",
	year="2016"
}

@misc{Unity2020,
	title="Unity",
	url="https://unity.com/",
	year="2020"
}

@misc{Unreal2020,
	title="Unreal {E}ngine",
	url="https://www.unrealengine.com/en-US/",
	year="2020"
}

@misc{visual2020,
	title="Microsoft {V}isual {S}tudio",
	url="https://visualstudio.microsoft.com/",
	year="2020"
}

@misc{Vuforia2020,
	title="Vuforia",
	url="https://developer.vuforia.com/",
	year="2020"
}

@misc{GGV2020,
	title="{G}aze, {G}esture and {V}oice",
	year="2020",
	url="https://docs.microsoft.com/en-us/windows/mixed-reality/interaction-fundamentals"
}

@misc{Blender2020,
	title="Blender software",
	url="https://www.blender.com",
	year="2020"
}

@inproceedings{Lohou2019b,
	author="Christophe Lohou and Marc Bouiller and \'Emilie Gadea-Deschamps",
	title="Mixed Reality Experiment for Hemodialysis Treatment",
	month="June",
	year="2019",
	booktitle="Surgetica $9^{th}$ edition",
	address="Rennes, France"
}

@misc{Adobe2018,
	title="Adobe creative suite",
	url="https://www.adobe.com/fr/creativecloud.html",
	year="2020"
}
@misc{PTCVuforiaStudio2018,
	title="Vuforia studio",
	url="https://www.ptc.com/fr/products/vuforia/vuforia-studio",
	year="2020"
}

@article{Kuhlemann2017,
   author = {Ivo Kuhlemann and Markus Kleemann and Philipp Jauer and Achim Schweikard and Floris Ernst},
   keywords = {HoloLens;two-dimensional X-ray images;computed tomography;on-line holographic visualisation;X-ray free endovascular interventions;magnetic tracking system;},
   language = {English},
   abstract = {A major challenge during endovascular interventions is visualising the position and orientation of the catheter being inserted. This is typically achieved by intermittent X-ray imaging. Since the radiation exposure to the surgeon is considerable, it is desirable to reduce X-ray exposure to the bare minimum needed. Additionally, transferring two-dimensional (2D) X-ray images to 3D locations is challenging. The authors present the development of a real-time navigation framework, which allows a 3D holographic view of the vascular system without any need of radiation. They extract the patient&apos;s surface and vascular tree from pre-operative computed tomography data and register it to the patient using a magnetic tracking system. The system was evaluated on an anthropomorphic full-body phantom by experienced clinicians using a four-point questionnaire. The average score of the system (maximum of 20) was found to be 17.5. The authors’ approach shows great potential to improve the workflow for endovascular procedures, by simultaneously reducing X-ray exposure. It will also improve the learning curve and help novices to more quickly master the required skills.},
   title = {Towards {X}-ray free endovascular interventions – using {H}olo{L}ens for on-line holographic visualisation},
   journal = {Healthcare {T}echnology {L}etters},
   issue = {5},   
   volume = {4},
   year = {2017},
   month = {October},
   pages = {184-187(3)},
   publisher ={Institution of Engineering and Technology},
   copyright = {This is an open access article published by the IET under the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0/)},
   url = {https://digital-library.theiet.org/content/journals/10.1049/htl.2017.0061}
}

@article{Garcia2018,
	author = {Garcia-Vázquez, Veronica and von Haxthausen, Felix and Jäckle, Sonja and Schumann, Christian and Kuhlemann, Ivo and Bouchagiar, Juljan and Höfer, Anna-Catharina and Matysiak, Florian and Hüttmann, Gereon and Peter Goltz, Jan and Kleemann, Markus and Ernst, Floris and Horn, Marco},
	year = {2018},
	month = {10},
	volume = {3},
	number={3},
	pages={167--177},
	title = {Navigation and visualisation with {H}olo{L}ens in endovascular aortic repair},
	journal = {Innovative {S}urgical {S}ciences}
}

@Article{Liu2018,
	author="Liu, He and Auvinet, Edouard and Giles, Joshua and Rodriguez y Baena, Ferdinando",
	title="Augmented Reality Based Navigation for Computer Assisted Hip Resurfacing: A Proof of Concept Study",
	journal="Annals of Biomedical Engineering",
	year="2018",
	month="Oct",
	day="01",
	volume="46",
	number="10",
	pages="1595--1605",
	abstract="Implantation accuracy has a great impact on the outcomes of hip resurfacing such as recovery of hip function. Computer assisted orthopedic surgery has demonstrated clear advantages for the patients, with improved placement accuracy and fewer outliers, but the intrusiveness, cost, and added complexity have limited its widespread adoption. To provide seamless computer assistance with improved immersion and a more natural surgical workflow, we propose an augmented-reality (AR) based navigation system for hip resurfacing. The operative femur is registered by processing depth information from the surgical site with a commercial depth camera. By coupling depth data with robotic assistance, obstacles that may obstruct the femur can be tracked and avoided automatically to reduce the chance of disruption to the surgical workflow. Using the registration result and the pre-operative plan, intra-operative surgical guidance is provided through a commercial AR headset so that the user can perform the operation without additional physical guides. To assess the accuracy of the navigation system, experiments of guide hole drilling were performed on femur phantoms. The position and orientation of the drilled holes were compared with the pre-operative plan, and the mean errors were found to be approximately 2 mm and 2{\textdegree}, results which are in line with commercial computer assisted orthopedic systems today.",
	issn="1573-9686",
	doi="10.1007/s10439-018-2055-1",
	url="https://doi.org/10.1007/s10439-018-2055-1"
}

@InProceedings{Lohou2019,
author="Lohou, Christophe and Miguel, Bruno and Azarnoush, Kasra",
editor="Ricci, Elisa and Rota Bul{\`o}, Samuel and Snoek, Cees and Lanz, Oswald and Messelodi, Stefano and Sebe, Nicu",
title="Preliminary Experiment of the Interactive Registration of a Trocar for Thoracoscopy with {H}olo{L}ens Headset",
booktitle="Image Analysis and Processing -- ICIAP 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="694--703",
abstract="During surgical procedures, it may be necessary to insert one or several trocars between the patient's ribs. This is the case, for example, with mini-invasive cardiac surgery to replace one or several heart valves assisted by 3D thoracoscopy: a trocar must be inserted before the camera is introduced.",
isbn="978-3-030-30645-8"
}
