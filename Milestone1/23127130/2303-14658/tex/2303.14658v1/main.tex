%% LaTeX Template for ISIT 2019
%%
%% by Stefan M. Moser, October 2017
%% 
%% derived from bare_conf.tex, V1.4a, 2014/09/17, by Michael Shell
%% for use with IEEEtran.cls version 1.8b or later
%%
%% Support sites for IEEEtran.cls:
%%
%% http://www.michaelshell.org/tex/ieeetran/
%% http://moser-isi.ethz.ch/manuals.html#eqlatex
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%%

\documentclass[11pt,journal,onecolumn]{IEEEtran}

\usepackage[margin=2.2cm]{geometry}
%% depending on your installation, you may wish to adjust the top margin:
\addtolength{\topmargin}{9mm}

%%%%%%
%% Packages:
%% Some useful packages (and compatibility issues with the IEEE format)
%% are pointed out at the very end of this template source file (they are 
%% taken verbatim out of bare_conf.tex by Michael Shell).
%
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%
%\usepackage[normalem]{ulem}
\usepackage[utf8]{inputenc} 
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{hyperref}
\usepackage{xcolor}


%\usepackage{parskip}
\usepackage{ifthen}
\usepackage{subfig}
\usepackage{cite}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{float}
\usepackage[cmex10]{amsmath} % Use the [cmex10] option to ensure complicance
                             % with IEEE Xplore (see bare_conf.tex)
\usepackage[numbers]{natbib}                             
%\usepackage{physics}
%% Please note that the amsthm package must not be loaded with
%% IEEEtran.cls because IEEEtran provides its own versions of
%% theorems. Also note that IEEEXplore does not accepts submissions
%% with hyperlinks, i.e., hyperref cannot be used.

\interdisplaylinepenalty=2500 % As explained in bare_conf.tex

\newcommand{\etal}{\textit{et al.}}
\newcommand{\dataset}{{\cal D}}
\newcommand{\ERM}{{\sf{ERM}}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\E}[1]{\mathbb E\left[#1\right]}
\newcommand{\Esub}[2]{\mathbb E_{#1}\left[#2\right]}
\newcommand{\gen}{\textup{gen}}
\newcommand{\pp}[1]{\mathbb P\left(#1\right)}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}


\newtheorem{theorem}{\bf{Theorem}}
\newtheorem{definition}{\bf{Definition}}
\newtheorem{lemma}{\bf{Lemma}}
\newtheorem{corollary}{\bf{Corollary}}
\newtheorem{proposition}{\bf{Proposition}}
\newtheorem{remark}{\bf{Remark}}
\newtheorem{example}{\bf{Example}}
\newtheorem{assumption}{\bf{Assumption}}
%%%%%%
% correct bad hyphenation here


% ------------------------------------------------------------
\begin{document}
\title{On the tightness of information-theoretic bounds on generalization error of learning algorithms} 

% %%% Single author, or several authors with same affiliation:
% \author{%
%   \IEEEauthorblockN{Stefan M.~Moser}
%   \IEEEauthorblockA{ETH Zürich\\
%                     ISI (D-ITET)\\
%                     CH-8092 Zürich, Switzerland\\
%                     Email: moser@isi.ee.ethz.ch}
% }


%%% Several authors with up to three affiliations:
\author{%
%  \IEEEauthorblockN{Xuetong Wu}
%  \IEEEauthorblockA{ETH Zürich\\
%                    ISI (D-ITET), ETH Zentrum\\
%                    CH-8092 Zürich, Switzerland\\
%                    Email: moser@isi.ee.ethz.ch}
%  \and
  \IEEEauthorblockN{Xuetong Wu$^1$, Jonathan H. Manton$^1$, Uwe Aickelin$^2$, Jingge Zhu$^{1}$}
  \IEEEauthorblockA{$^1$Department of EEE, \quad 
                    $^2$Department of CIS\\
                    University of Melbourne, Parkville, Victoria, Australia\\
                    Email: xuetongw1@student.unimelb.edu, \{jmanton, uwe.aickelin, jingge.zhu\}@unimelb.edu.au}
}


%%% Many authors with many affiliations:
% \author{%
%   \IEEEauthorblockN{Albus Dumbledore\IEEEauthorrefmark{1},
%                     Olympe Maxime\IEEEauthorrefmark{2},
%                     Stefan M.~Moser\IEEEauthorrefmark{3}\IEEEauthorrefmark{4},
%                     and Harry Potter\IEEEauthorrefmark{1}}
%   \IEEEauthorblockA{\IEEEauthorrefmark{1}%
%                     Hogwarts School of Witchcraft and Wizardry,
%                     1714 Hogsmeade, Scotland,
%                     \{dumbledore, potter\}@hogwarts.edu}
%   \IEEEauthorblockA{\IEEEauthorrefmark{2}%
%                     Beauxbatons Academy of Magic,
%                     1290 Pyrénées, France,
%                     maxime@beauxbatons.edu}
%   \IEEEauthorblockA{\IEEEauthorrefmark{3}%
%                     ETH Zürich, ISI (D-ITET), ETH Zentrum, 
%                     CH-8092 Zürich, Switzerland,
%                     moser@isi.ee.ethz.ch}
%   \IEEEauthorblockA{\IEEEauthorrefmark{4}%
%                     National Chiao Tung University (NCTU), 
%                     Hsinchu, Taiwan,
%                     moser@isi.ee.ethz.ch}
% }


\maketitle

%%%%%%
%% Abstract: 
%% If your paper is eligible for the student paper award, please add
%% the comment "THIS PAPER IS ELIGIBLE FOR THE STUDENT PAPER
%% AWARD." as a first line in the abstract. 
%% For the final version of the accepted paper, please do not forget
%% to remove this comment!
%%
\begin{abstract}
A recent line of works, initiated by \cite{russo2016controlling} and \cite{xu2017information}, has shown that the generalization error of a learning algorithm can be upper bounded by information measures. In most of the relevant works, the convergence rate of the expected generalization error is in the form of $O(\sqrt{\lambda/n})$ where $\lambda$ is some information-theoretic quantities such as the mutual information or conditional mutual information between the data and the learned hypothesis. However, such a learning rate is typically  considered to be ``slow", compared to a ``fast rate" of $O(\lambda/n)$ in many learning scenarios. In this work, we first show that the square root does not necessarily imply a slow rate, and a fast rate result can still be obtained using this bound under appropriate assumptions. Furthermore, we identify the critical conditions needed for the fast rate generalization error, which we call the $(\eta,c)$-central condition. Under this condition, we give information-theoretic bounds on the generalization error and excess risk, with a fast convergence rate for specific learning algorithms such as empirical risk minimization and its regularized version. Finally, several analytical examples are given to show the effectiveness of the bounds.
%   Instructions
%  are given for the preparation and submission of papers for the
%  \emph{2019 International Symposium on Information Theory}.
%Transfer learning, or domain adaptation, is concerned with  machine learning problems in which training and testing data come from possibly different distributions (denoted as $\mu$ and $\mu'$, respectively).  In this work, we give an information-theoretic analysis on the generalization error and the excess risk of transfer learning algorithms, following  a line of work initiated by Russo and Zhou.  Our results suggest, perhaps as expected, that the Kullback-Leibler (KL) divergence $D(\mu||\mu')$ plays an important role in characterizing the generalization error in the settings of domain adaptation. Specifically, we provide generalization error upper bounds for general transfer learning algorithms, and extend the results to a specific empirical risk minimization (ERM) algorithm where data from both distributions are available in the training phase.  We further apply the method to iterative, noisy gradient descent algorithms, and obtain upper bounds which can be easily calculated, only using parameters from the learning algorithms. A few illustrative examples are provided to demonstrate the usefulness of the results. In particular, our bound is tighter in specific classification problems than the bound derived using Rademacher complexity.
\end{abstract}

%% The paper must be self-contained. However, if you are referring to
%% a full version for checking certain proofs, please provide the
%% publically accessible location below.  If the paper is completely
%% self-contained, you can remove the following line from your
%% submission.
%\textit{A full version of this paper is accessible at:}
%\url{http://isit2019.fr/} 

\section{Introduction} \label{sec:intro}

The generalization error of a learning algorithm lies in the core analysis of the statistical learning theory, and the estimation of which becomes remarkably crucial. Conventionally, many bounding techniques are proposed under different conditions and assumptions.  To name a few, \citet{vapnik1999nature, vapnik2015uniform} proposed VC-dimension which describes the richness of a hypothesis class for generalization ability. \citet{bousquet2002stability} introduced a strong notion called ``algorithmic stability" for bounding the generalization error, to examine where a single training sample can largely affect the generalization error. \citet{mcallester1999some} investigated the bound under the Bayes framework by imposing the prior distribution over the hypothesis space.  \citet{xu2012robustness} developed another notion, namely robustness for the generalization. Unlike stability, the robustness conveys geometric intuition in terms of the geometric distance between instances and it can be extended to non-standard setups such as Markov chain or quantile loss, facilitating new bounds on generalization. However, most bounds mentioned above are only concerned with the hypothesis or the algorithm solely. For example, VC-dimension methods care about the worst-case bound which only depends on the hypothesis space. The stability methods only specify the properties of learning algorithms but do not require additional assumptions on hypothesis space. To fully characterize the intrinsic nature of a learning problem, it is shown in some recent works that the generalization error can be upper bounded using the information-theoretic quantities \cite{xu2017information,russo2016controlling} and the bound usually takes the following form:      
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W, \mathcal{S}_n)] \leq \sqrt{\frac{c I(W;\mathcal{S}_n)}{n}}, \label{eq:gen-form}
\end{align}
where the expectation is taken w.r.t. the joint distribution of $W$ and $\mathcal{S}_n$ induced by some algorithm $\mathcal{A}$. Here, $\mathcal{E}(w, \mathcal{S}_n)$ denotes the generalization error (properly defined in~(\ref{eq:gen}) in Section \ref{sec:prob}) for a given hypothesis $w$ and data sample $\mathcal{S}_n = (Z_i)_{i=1,\cdots,n}$, and $I(W;\mathcal{S}_n)$ denotes the mutual information between the hypothesis and data sample, and $c$ is some positive constant. In particular, if the loss function is $\sigma$-sub-Gaussian under the distribution $P_W \otimes P_Z$, $c$ is equal to $2\sigma^2$. By introducing the mutual information, such a bound gives a data-algorithm dependent bound that can recover the previous results in terms of VC dimension~\cite{xu2017information}, algorithmic stability~\cite{raginsky2016information}, differential privacy~\cite{steinke2020reasoning} under mild conditions. Further, as pointed out by \cite{asadi_chaining_2018}, the information-theoretic upper bound could be substantially tighter than the traditional bounds if we could exploit specific properties of the learning algorithm. 

However, there are mainly two issues recognized from this bound. The first problem is that the convergence rate is usually $O(\sqrt{I(W;\mathcal{S}_n) /n})$, and if the mutual information is bounded by some constant, the bound is sub-optimal in some learning scenarios. The second issue is that the mutual information term can be arbitrarily large for deterministic algorithms \cite{bu2020tightening,Grunwald2021pac}. The latter can be addressed by introducing ghost samples \cite{steinke2020reasoning} or using random subset methods \cite{bu2020tightening,zhou2022individually,Haghifam2020}. Only a few works are dedicated to the former problem. In this work, we develop a general framework for the fast rate bounds using the mutual information following this line of works~\cite{van2015fast,Grunwald2020,Grunwald2021pac} and the contributions are listed as follows.
\begin{itemize}
    \item We argue that the square root sign in~(\ref{eq:gen-form}) does not necessarily imply a slow rate. Specifically, we show that the fast rate is still achievable using the same bound, by making a slight change in the assumption, where the sub-Gaussian condition is assumed on the excess risk instead of on the loss function. Then the fast rate (e.g., $\frac{\lambda}{n}$ where $\lambda$ is the sample-wise information measure that we will specify in the Section~\ref{sec:prob}) is attainable if $\sigma^2$ has the same order as the excess risk w.r.t. the sample size, which could address the two issues simultaneously. To demonstrate the tightness of our proposed upper bound, we also propose a matching lower bound that has a similar form and the same convergence rate as the upper bound in specific learning setups.
    \item Inspired by the analysis under the sub-Gaussian case, we identify the critical assumptions needed for a more general fast-rate learning framework, which we call $(\eta, c)$-central condition, for the excess risk. Compared with typical mutual information bounds, the convergence rate of the novel bound has a cleaner and possibly more general presentation, which also improves from $O(\sqrt{\lambda/n})$ to $O(\lambda/n)$ under some widely used algorithms such as empirical risk minimization (ERM). 
    % In addition to the sub-Gaussian assumption, we also proved that the sub-exponential and sub-Gamma assumptions satisfy the $(\eta,c)$-central condition, from which the new forms are established and new insights could be drawn. 
    \item Furthermore, our results are extended to regularized ERM algorithms, and intermediate rates could be achieved with the relaxed $(v,c)$-central condition. The fast rate results are confirmed for a few simple regression and classification examples analytically or numerically, showing the effectiveness of the proposed bounds. %Our framework can be generalized to many other learning tasks such as classification problems, for a tighter generalization error and excess risk bound. 
\end{itemize}



\section{Problem formulation} \label{sec:prob}
Consider a dataset $\mathcal{S}_n = \left( z_1,z_2,\cdots,z_n\right)$ where each instance $z_i$ is i.i.d. drawn from some distribution $\mu$, we would like to learn a hypothesis $w$ that exploits the properties of $\mathcal{S}_n$, with the aim of making predictions for previously unseen new data correctly. The choice of $w$ is performed within a set of member functions $\mathcal{W}$ with the possibly randomised algorithm $\mathcal{A}:\mathcal{Z}^n \rightarrow \mathcal{W}$ and we define the corresponding loss function $\ell: \mathcal{W}\times \mathcal{Z} \rightarrow \mathbb{R}$. Particularly if we consider the supervised learning problem, we can write $\mathcal{Z} = \mathcal{X} \times \mathcal{Y}$ and $z_i = (x_i, y_i)$ as a feature-label pair. Then the hypothesis $w: \mathcal{X} \rightarrow \mathcal{Y}$ can be regarded as a predictor for the input sample. We will call $(\mu, \ell, \mathcal{W}, \mathcal{A})$ a learning tuple. In a typical statistical learning problem, one may wish to minimize the \emph{expected} loss function $L_{\mu}(w) = E_{z\sim \mu}[\ell(w,z)]$. However, as the underlying distribution $\mu$ is usually unknown in practice, one may wish to learn $w$ by some learning principle, for example, one typical way is minimizing the empirical risk induced by the dataset $\mathcal{S}_n$, denoted as $w_{\ERM}$, such that 
\begin{equation}
w_{\ERM} = \argmin_{w\in \mathcal{W}}\frac{1}{n}\sum_{i=1}^{n}\ell(w,z_i),
\end{equation}
which will be employed as a predictor for the new data. We point out that many of our results obtained in this paper also hold for more general algorithms other than ERM algorithm. Here we define $\hat{L}(w, \mathcal{S}_n) = \frac{1}{n}\sum_{i=1}^{n}\ell(w,z_i)$ as the empirical loss.  To assess how this predictor performs on unseen samples, the generalization error is then introduced to evaluate whether a learner suffers from the over-fitting (or under-fitting). For any $w \in \mathcal{W}$, we define the generalization error as
\begin{equation}
\mathcal{E}(w, \mathcal{S}_n) := \mathbb{E}_{Z\sim \mu}[\ell(w,Z)] - \frac{1}{n}\sum_{i=1}^{n}\ell(w,z_i).  \label{eq:gen}
\end{equation}
%We also define the expected generalization error as,
%\begin{equation}
%\mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W, \mathcal{S}_n)] := \mathbb{E}_{W\otimes \mu}[\ell(W,Z)] -\mathbb{E}_{W\mathcal{S}_n}[\frac{1}{n}\sum_{i=1}^{n}\ell(W,Z_i)] 
%\end{equation}
%The ability to perform generalization is the fundamental property to look for in a learned predictor. 
Another important metric, the excess risk, is defined as
\begin{equation}
\mathcal{R}(w) := \mathbb{E}_{Z\sim \mu}[\ell(w,Z)] - \mathbb{E}_{Z\sim \mu}[\ell(w^*,Z)].
\end{equation}
where the optimal hypothesis for the true risk is defined as $w^*$ as 
\begin{equation}
w^{*} = \argmin_{w\in \mathcal{W}}E_{Z \sim \mu}[\ell (w,Z)],
\end{equation}
which is unknown in practice. The excess risk evaluates how well a hypothesis $w$ performs with respect to $w^*$ given the data distribution $\mu$. We also define the corresponding empirical excess risk as
\begin{equation}
\hat{\mathcal{R}}(w, \mathcal{S}_n) := \frac{1}{n}\sum_{i=1}^{n}r(w,z_i),
\end{equation}
where $r(w,z) = \ell(w,z) - \ell(w^*,z)$. In the sequel, we are particularly interested in bounding the expected generalization error $\mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W, \mathcal{S}_n)]$ and the excess risk $\mathbb{E}_{W}[\mathcal{R}(W)]$ for any $P_W$ induced by the algorithm $\mathcal{A}$.


\section{A Change on Assumption}
\subsection{Existing Bounds}
In this section, we first review some known results on generalization error under different assumptions on the loss function. For example, the recent advances show that under the sub-Gaussian assumption, the generalization error can be upper bounded using the information-theoretic quantities such as mutual information \cite{xu2017information,bu2020tightening,zhou2022individually} or conditional mutual information \cite{steinke2020reasoning}. The bound usually takes the following general form. 
\begin{theorem}[Generalization error of Generic Hypothesis\cite{bu2020tightening}] \label{thm:gen_erm}
Assume that the cumulant generating function of the random variable $\ell(W, Z)-\E{\ell(W,Z)}$  is upper bounded by $\psi(-\lambda)$ in the interval $(b_{-},0)$ and $\psi(\lambda)$ in the interval $(0,b_{+})$ under the product distribution $P_W\otimes\mu$ for some $b_{-}<0$ and $b_{+}>0$ where $P_W$ is induced by the data distribution and the algorithm. Then the expectation of the  generalization error in (\ref{eq:gen}) is upper bounded as
\begin{align}
\Esub{W\mathcal{S}_n}{\mathcal{E} (W, \mathcal{S}_n) }\leq \frac{1}{ n}\sum_{i=1}^{ n}\psi^{*-1}_{-}(I(W;Z_i)), \\
-\Esub{W\mathcal{S}_n}{\mathcal{E} (W, \mathcal{S}_n)}\leq \frac{1}{ n}\sum_{i=1}^{n}\psi^{*-1}_{+}(I(W;Z_i)),
\end{align}
where we define
\begin{align}
\psi^{*-1}_{-}(x) &:=\inf_{\lambda\in[0,-b_{-})}\frac{x+\psi(-\lambda)}{\lambda}, \\
\psi^{*-1}_{+}(x) &:=\inf_{\lambda\in[0,b_{+})}\frac{x+\psi(\lambda)}{\lambda}.
\end{align} \label{thm:exp_gen_gamma}
\end{theorem}
For different learning tasks and data distributions, the above theorem can be specialized to sub-exponential, sub-Gamma and sub-Gaussian random variables by identifying the CGF bounding function $\psi(\lambda)$. A few concrete examples are then provided that lead to different bounds in the following. More discussions on the CGF bounding function can be found in \cite{jiao2017dependence,bu2020tightening}.
\begin{example}[Sub-exponential bound]
We say $X$ is a $\left(\nu^2, \alpha\right)$-sub-exponential random variable with parameters $\nu, \alpha>0$ if:
\begin{align}
\log\mathbb{E} \left[ e^{\lambda (X -\mathbb{E}[X])}\right] \leq \frac{\lambda^2 \nu^2}{2} , \quad \forall \lambda:|\lambda|<\frac{1}{\alpha}.  
\end{align}
Assume that $\ell(W, Z)$ is ($\nu^2$, $\alpha$)-sub-exponential under the distribution $P_W \otimes \mu$ for some $\nu^2$ and $\alpha > 0$. Then it holds that
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq \begin{cases}
      \frac{1}{n}\sum_{i=1}^{n}\sqrt{2\nu^2 I(W;Z_i)}, \textup{ if } I(W;Z_i) \leq  \frac{\nu^2}{2\alpha^2} \textup{ for all } i \in [n],\\
       \frac{\nu^2}{2\alpha} + \frac{\alpha}{n}\sum_{i=1}^{n} I(W;Z_i),   \textup{ if } I(W;Z_i) > \frac{\nu^2}{2\alpha^2} \textup{ for all } i \in [n].
     \end{cases}
 \label{eq:subgexponential}
\end{align}
\end{example}

\begin{example}[Sub-Gamma bound]
We say $X$ is a ($\nu^2,\alpha$)-sub-Gamma random variable with variance parameter $\nu^2$ and scale parameter $\alpha$ if:
\begin{align}
\log\mathbb{E} \left[ e^{\lambda (X -\mathbb{E}[X])}\right]\leq \frac{\nu^2 \lambda^2}{2(1- \alpha \lambda)}, \quad  \forall \lambda:  0<\lambda<\frac{1}{\alpha}.
\end{align}
Assume that $\ell(W, Z)$ is ($\nu^2$, $\alpha$)-sub-Gamma under the distribution $P_W \otimes \mu$ for some $\nu^2$ and $\alpha > 0$. Then it holds that
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq \frac{1}{n}\sum_{i=1}^{n}\sqrt{2\nu^2I(W;Z_i)} + \alpha I(W;Z_i).
 \label{eq:subgamma}
\end{align}
\end{example}

\begin{example}[Sub-Gaussian bound]
We say $X$ is a $\sigma$-sub-Gaussian random variable with variance parameter $\sigma$ if: 
\begin{align}
\log \mathbb{E}\left[e^{\lambda (X-\mathbb{E}[X])}\right] \leq \frac{\sigma^{2} \lambda^{2}}{2}, \quad \forall \lambda \in \mathbb{R}.
\end{align}
Note that the sub-Gaussian random variable is sub-exponential random variable but not the other way around. 
Suppose that $\ell({W}, {Z})$ is $\sigma$-sub-Gaussian under the distribution $P_{W} \otimes \mu$ where $P_W$ is the marginal induced the algorithm $\mathcal{A}$ and data distribution $\mu$, then
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right]  \leq \frac{1}{n} \sum_{i=1}^{n} \sqrt{2 \sigma^{2} I\left(W ; Z_{i}\right)}. \label{eq:bu_result}
\end{align}
\end{example}

\begin{remark}
Throughout this section, we will focus on the case when  $I(W;Z_i) \sim O(1/n)$ in the sequel so the bound in (\ref{eq:bu_result}) gives a convergence rate of $O(\sqrt{1/n})$ for a constant $\sigma^2$ for both the sub-Gaussian and sub-Gamma bounds. For a broader family of the loss function that is sub-exponential, the upper bound will even not converge if the mutual information is larger than $\frac{\sigma^2}{2\alpha}$ for some positive constants $\sigma$ and $\alpha$ if we refer to \ref{eq:subgexponential}. The above assumption holds for many learning settings such as ERM in the linear regression problem \cite{raginsky2016information}, the Gibbs algorithm with mild assumptions \cite{raginsky2016information,xu2017information,aminian2021exact,Grunwald2021pac}. More generally, the generalization error will also decay exponentially as $n$ increases if the mutual information decays exponentially, an example of which can be found in Section~\ref{sec:discretehypothesis}.
\end{remark}
From the above result, it is usually recognized that the \emph{square root} sign prevents us from the fast rate, even in the following simple Gaussian mean estimation problem considered in \cite{bu2020tightening}. 
\begin{example}\label{sec:example}
Let $\ell(w,z_i) = (w-z_i)^2$, each sample is drawn from some Gaussian distribution, $Z_i \sim \mathcal{N}(\mu, \sigma_{N}^2)$. We consider the ERM algorithm that gives,
\begin{align*}
 W_{\ERM} = \frac{1}{n} \sum_{i=1}^{n} Z_i \sim \mathcal{N}(\mu, \frac{\sigma_{N}^2}{n}).
\end{align*}
The true generalization error can be calculated to be
\begin{align*}
   \Esub{W\mathcal{S}_n}{\mathcal{E}(W_\ERM, \mathcal{S}_n)} = \frac{2\sigma_{N}^2}{n},
\end{align*}
To evaluate the upper bound in~Theorem~\ref{thm:gen_erm} for this example, we notice that for any $i$, $\ell(W,Z_i) \sim \frac{n+1}{n}\sigma_{N}^2 \chi_{1}^{2}$ where $\chi^2_1$ denotes the chi-squared distribution with 1 degree of freedom. Hence, the cumulant generating function can be calculated as,
\begin{align*}
\log \mathbb{E}_{P_W\otimes \mu}\left[e^{\eta(\ell(W,Z)-\mathbb{E}[\ell(W,Z)])}\right] = - \sigma_{W}^2 \eta -\frac{1}{2} \log \left(1-2\sigma_{W}^2 \eta \right),
\end{align*}
where $\eta \leq \frac{1}{2\sigma^2_W}$ and $\sigma^2_W = \frac{n+1}{n}\sigma_{N}^2$ to simplify the notation. In this case, it can be proved that,
\begin{align}
    - \sigma_{W}^2 \eta -\frac{1}{2} \log \left(1-2\sigma_{W}^2 \eta \right) \leq \sigma_W^4\eta^2. \label{eq:CGF}
\end{align}
We can also calculate the mutual information as 
\begin{align*}
   I(W;Z_i) = \frac{1}{2}\log\frac{n}{n-1}.
\end{align*}
With the upper bound on the CGF in (\ref{eq:CGF}), the bound becomes
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right]  \leq \frac{\sigma_{N}^2}{n} \sum_{i=1}^{n} \sqrt{2\frac{(n+1)^2}{n^2}\log\frac{n}{n-1}} \leq \sigma^2_N\sqrt{\frac{2(n+1)^2}{(n-1)^3}},
\end{align}
which will be of the order $O(\frac{1}{\sqrt{n}})$ as $n$ goes to infinity, which leads to a slow convergence rate as the true generalization error is $O(\frac{1}{n})$. 
\end{example}
%
\subsection{Bounds with A Change on the Assumption}
In this section, we show that in fact the same bound can be used to derive the correct (fast) convergence rate of $O(1/n)$, with a small yet important change on the assumption. Intuitively speaking, to achieve a fast rate bound for both the generalization error and the excess risk in expectation, the output hypothesis of the learning algorithm must be ``good" enough compared to the optimal hypothesis $w^*$. Here we encode the notion of goodness in terms of the cumulant generating function by controlling the gap between $\ell(w,Z)$ and $\ell(w^*,Z)$. To facilitate such an idea, we make the sub-Gaussian assumption w.r.t. the excess risk and bound the generalization error as follows.
\begin{theorem}\label{thm:subgaussian}
Suppose that $r({W}, {Z})$ is $\sigma$-sub-Gaussian under distribution $P_{W} \otimes \mu$, then
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right]  \leq \frac{1}{n} \sum_{i=1}^{n} \sqrt{2 \sigma^{2} I\left(W ; Z_{i}\right)}. \label{eq:our_result}
\end{align}
Furthermore, the excess risk can be bounded by,
\begin{align}
    \mathbb{E}_{W} \left[\mathcal{R}(W)\right]  \leq \mathbb{E}_{W\mathcal{S}_n} \left[\hat{\mathcal{R}}(W, \mathcal{S}_n)\right] + \frac{1}{n} \sum_{i=1}^{n} \sqrt{2 \sigma^{2} I\left(W ; Z_{i}\right)} . \label{eq:our_result_excess}
\end{align}
\end{theorem}
Now we evaluate the bound in~Theorem~\ref{thm:subgaussian} for the Gaussian example. Notice that~(\ref{eq:our_result}) is identical to (\ref{eq:bu_result}), and the only difference between them is the assumption.
\begin{example}[Continuing from Example~\ref{sec:example}] \label{example:subgaussian-2}
Consider the settings in Example~\ref{sec:example}. First, we note that the expected risk minimizer $w^*$ is calculated as $\mu$. Then we have,
\begin{align*}
    r(w,z_i) = (w - z_i)^2 - (\mu - z_i)^2.
\end{align*}
The expected excess risk can be calculated as
\begin{align*}
    \mathbb{E}_{W}[\mathcal{R}(W)] = \frac{\sigma_{N}^2}{n}.
\end{align*}
We can then calculate the cumulant generating function as,
\begin{align*}
\log \mathbb{E}_{P_W\otimes \mu}\left[e^{\eta(r(W,Z)-\mathbb{E}[r(W,Z)])}\right] \leq \frac{4\eta^2\sigma_N^4}{n},
\end{align*}
for any $\eta \in \mathbb{R}$ and any $n >  \max\left\{\frac{(4\eta^2\sigma^4_N+\eta\sigma^2_N)(2\eta^2\sigma^4_N+\eta\sigma^2_N)}{\eta^2\sigma^4_N}, 4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2\right\} $ where the detailed calculations can be found in Appendix~\ref{apd:cal}. Hence $r(W,Z)$ is $\sqrt{\frac{8\sigma_N^4}{n}}$-sub-Gaussian under the distribution $P_{W} \otimes \mu$. Then the bound becomes,
\begin{align*}
    \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq \frac{\sigma_N^2}{n} \sum_{i=1}^{n} \sqrt{\frac{8}{n}\log\frac{n}{n-1}} \leq \frac{2\sqrt{2}\sigma^2_N}{n-1},
\end{align*}
which is $O(1/n)$, yielding a fast rate characterization.
\end{example}
In light of our existing findings, the change of assumption appears to be well-justified, as we provide a thorough comprehension of the underlying reasons in Section~\ref{subsec:justification}.

\section{Fast Rate Results}
\subsection{Bounds with Excess Risk}
%If we consider the case where $\sigma^2$ is a constant that does not depend on sample size, then the bound in (\ref{eq:subgaussian}) is even worse than the bound in (\ref{eq:our_result}) since the former does not converge but the latter converges as $O({1}/{\sqrt{n}})$. This is because $\eta a_\eta \sim O(1/n)$ and the second term in R.H.S. of (\ref{eq:subgaussian}) will become a constant. More generally, the bound in its current form may still be loose if $\eta a_{\eta}$ shrinks with the rate of $O(n^{-\alpha})$ for some $\alpha \geq 1$. To alleviate this drawback, we impose more critical conditions on the excess risk and extend the results by replacing $\eta a_{\eta}$ with some constant that does not depend on the sample size. 
%Even though the above bound gives more insights on the generalization error as the trade-off is clearly characterized in Theorem~\ref{thm:eta-c-loss}, compared to the sub-Gaussian assumption where the expected loss is not shown in the bound, it is not necessarily tight in the example we showed and does not provide the correct convergence rate in general. 
As observed in the results from the previous section, while our new bound is tight, it still contains a square root term in the bound. In fact, many information-theoretic bounds for generalization error \cite{xu2017information, raginsky2016information, bu2020tightening, zhou2022individually, steinke2020reasoning, russo2016controlling} contain the square root with the sub-Gaussian assumption, which is often seen as the obstacle to achieving fast rate results. However, as mentioned earlier, the bounds in (\ref{eq:bu_result}) and (\ref{eq:our_result}) have the same structure but lead to different convergence rates. By slightly adjusting the assumption, we can obtain a fast rate result. However, this formulation of the result is still not very satisfying because both bounds contain the quantity $\sigma^2$ that could scale with $n$, making it hard to determine the actual convergence rate directly from the bounds. To this end, we propose a different type of bound to alleviate this drawback. To make the ``fast rate" result more explicit, we first provide an alternative bound based on the sub-Gaussian assumption and the key property of the following bound is that it does not contain the square root.
\begin{theorem}[Fast Rate with Sub-Gaussian Condition]\label{thm:subgaussianv2}
Assume that $r(W, Z)$ is $\sigma$-subgaussian under the distribution $P_W \otimes \mu$. Then it holds that
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq  \frac{1-a_\eta}{a_\eta} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}]  + \frac{1}{n\eta a_\eta} \sum_{i=1}^{n}  I\left(W ; Z_{i}\right). \label{eq:subgaussian}
\end{align}
for any $ 0 < \eta < \frac{2\mathbb{E}_{P_W \otimes \mu}[r(W,Z_i)]}{\sigma^2}$ and $a_\eta = 1-  \frac{\eta\sigma^2}{2\mathbb{E}_{P_W \otimes \mu}[r(W,Z_i)]}$. Furthermore, the expected excess risk is bounded by,
\begin{align*}
 \mathbb{E}_{W}[\mathcal{R}(W)] \leq& \frac{1}{a_\eta} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] 
    + \frac{1}{n\eta a_\eta} \sum_{i=1}^{n}  I\left(W ; Z_{i}\right).
\end{align*}
\end{theorem}
\noindent We continue to examine the bound in Theorem~\ref{thm:subgaussianv2} with the Gaussian mean estimation. 
\begin{example}\label{eg:subv2}
Since the expected excess risk can be calculated as $\mathbb{E}_{W}[\mathcal{R}(W)] = \frac{\sigma_N^2}{n}$, and $r(W,Z)$ is $\sqrt{\frac{8\sigma_N^4}{n}}$-sub-Gaussian, then we require that $0 <\eta < \frac{1}{2\sqrt{2}\sigma_N^2}$, which is independent of the sample size. For simplicity, we can consider the case $\eta = \frac{1}{4\sigma_N^2}$ as an example, then $a_\eta$ is calculated to be $\frac{1}{2}$. For any $n$ that satisfies the condition in Example~\ref{example:subgaussian-2}, we have the generalization error bound,
\begin{align*}
   \frac{1-a_\eta}{a_\eta}\Esub{W\mathcal{S}_n}{\hat{\mathcal{R}}(W_\ERM,\mathcal{S}_n)} +  \frac{1}{\eta a_{\eta} n}\sum_{i=1}^{n}I(W;Z_i) \leq  \frac{3\sigma_N^2}{n},
\end{align*}
where the empirical excess risk $\Esub{W\mathcal{S}_n}{\hat{\mathcal{R}}(W_\ERM,\mathcal{S}_n)}$ is calculated as $-\frac{\sigma^2_N}{n}$ and the bound has the rate of $O(1/n)$. 
\end{example}
\begin{remark}
Notice that both $\eta$ and $\alpha_\eta$ depend on the expected excess risk $\mathbb{E}_{P_W \otimes \mu}[r(W,Z)]$ and $\sigma^2$, which potentially depend on $n$ as well. Hence a more careful examination is needed. Specifically, it can be seen that if the ratio of the two quantities remains a constant independent of $n$, the fast rate result will then hold. 
%It can be found that $a_{\eta}$ depends on the ratio of expected excess risks $\mathbb{E}_{P_W \otimes \mu}[r(W,Z)]$ and $\sigma^2$, both of which can be a function of $n$. In particular, if $\sigma^2$ has the same order as $\mathbb{E}_{P_W \otimes \mu}[r(W,Z)]$ w.r.t. $n$, then we can achieve a fast rate by choosing $\eta$ to be some constant. In other words, the fast rate is attainable if $\sigma^2$ has the same order as the excess risk w.r.t. the sample size, provided that $I(W;Z_i)$ scales as $O(1/n)$.
\end{remark}

Ideally, we aim for a bound that does not contain extra quantities that depend on $n$, the key to such a bound is the so-called expected ($\eta,c$)-central condition (or we simply say ($\eta,c$)-central condition for short), inspired by the works \cite{van2015fast,mehta2017fast,Grunwald2020,Grunwald2021pac}, which is the key condition leading to the fast rate. Similarly, we will first define the $(\eta,c)$-central condition for a non-negative random variable and use this notation to bound the generalization error with the loss function and excess risk.
\begin{definition}[$(\eta,c)$-central condition]
Let ${\eta}>0$ and $0 < c \leq 1$ be two constants. We say that a random variable $X$ endowed with a probability measure $P$ satisfies the $(\eta,c)$-central condition if the following inequality holds:
\begin{align}
\log \mathbb{E}_{P}& \left[e^{-{\eta}X}\right]  \leq   -c\eta  \mathbb{E}_{P}\left[X\right]. \label{eq:eta_c_loss} 
\end{align} 
given that $0 < E_P[X] < \infty$.
\end{definition}
\begin{remark}
Comparing the $(\eta,c)$-central condition with the $\sigma$-sub-Gaussian condition defined as
\begin{align*}
    \log \mathbb{E}_{P}[e^{-\eta X}] \leq -\eta \mathbb{E}_{P}[X] + \frac{\eta^2\sigma^2}{2}, \forall \eta \in \mathbb{R},
\end{align*}
the main difference is on the bounding terms for the CGF where the variance proxy term $\frac{\eta^2\sigma^2}{2}$ is replaced by the term $(1-c)\eta E_P[X]$ for a positive $\eta$. Such a condition is closely related to the Bernstein condition \cite{koren2015fast,Grunwald2020,Grunwald2021pac} and witness condition \cite{Grunwald2020} that are proposed for the "fast" rate results. Different from the sub-Gaussian condition, we could eliminate the dependency of the sample size $n$ for the variance proxy $\sigma^2$ with the $(\eta,c)$-central condition.
\end{remark}

Then we make the following assumption that the unexpected excess risk satisfies the $(\eta,c)$-central condition.
%\begin{definition}[Expected $(\eta,c)$-Central Condition w.r.t. the Excess Risk]
%Let ${\eta}>0$ and $0 < c \leq 1$ be two constants. We say that $(\mu, \ell, \mathcal{W}, \mathcal{A})$ satisfies the expected $(\eta,c)$-central condition if the following inequality holds for the optimal hypothesis $w^*$:
\begin{assumption}\label{assump:eta_c_r}
We assume the unexpected excess risk $r(W,Z_i)$ satisfies the $\left(\eta, c \right)$-central condition for any $i\in [n]$, some constants $\eta > 0$ and $0 < c \leq 1$ under the data distribution $\mu$ and algorithm $\mathcal{A}$, e.g.,
\begin{align}
\log \mathbb{E}_{P_W\otimes \mu}& \left[e^{-{\eta}\left(\ell(W,Z)-\ell(w^*,Z)\right)}\right]  \leq   -c\eta  \mathbb{E}_{P_W\otimes \mu}\left[\ell(W,Z) - \ell(w^*,Z)\right]. \label{eq:eta_c} %\textup{ i.e., } \ell(w^*,Z)-\ell(W,Z) \unlhd^{W\otimes \mu}_{{\eta}} 0.
\end{align} 
\end{assumption}
%\end{definition}
%The first condition is the Bernstein condition.
%\begin{definition}[$(\eta,c)$-Central Condition]
%Let ${\eta}>0$ and $0 < c \leq 1$. We say a learning problem satisfies the $(\eta,c)$-central condition if there exists $w^* \in \mathcal{W}$ such that for any $w\in\mathcal{W}$, the following inequality holds:
%\begin{align}
%\log \mathbb{E}_{\mu}& \left[e^{-{\eta}\left(\ell(w,Z)-\ell(w^*,Z)\right)}\right]  \leq  -c\eta  \mathbb{E}_{\mu}\left[\ell(w,Z) - \ell(w^*,Z)\right]. \label{eq:eta_c} %\textup{ i.e., } \ell(w^*,Z)-\ell(W,Z) \unlhd^{W\otimes \mu}_{{\eta}} 0. 
%\end{align}
%\end{definition}
%By Jensen's inequality, it is easy to see that if $w^*$ satisfies the inequality~(\ref{eq:eta_c}), it must be the minimizer of the expected loss. 
\noindent Compared to the conventional $\eta$-central condition \cite[Def. 3.1]{van2015fast} by setting $c =0$ in (\ref{eq:eta_c}) as
\begin{align}
\log \mathbb{E}_{P_W\otimes \mu}& \left[e^{-{\eta}\left(\ell(w,Z)-\ell(w^*,Z)\right)}\right]  \leq 0, \label{eq:eta} %\textup{ i.e., } \ell(w^*,Z)-\ell(W,Z) \unlhd^{W\otimes \mu}_{{\eta}} 0. 
\end{align}
the RHS of~(\ref{eq:eta_c}) is negative and has a tighter control than (\ref{eq:eta}) of the tail behaviour for some $c> 0$. Note that since the $\eta$-central condition in \cite{van2015fast} may be strong and is usually difficult to satisfy for all $w \in \mathcal{W}$ even in some trivial examples as shown in Appendix~\ref{apd:cal}. Our proposed ($\eta,c$)-central condition is weaker in the sense that it is only required to be satisfied in expectation.
\begin{remark}
With the Chernoff bound, if the loss function satisfies the $(\eta,c)$-central condition, we have the lower tail probability bound as:
\begin{align*}
    P(r(W,Z_i) \leq -t) \leq  e^{-\eta(t+c\mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)])}.
\end{align*}
We provide a probabilistic interpretation on the lower tail of the excess risk and this bound is particularly useful in the following two senses. On the one hand, this condition ensures that for all $\eta' \leq \eta$, the probability that $w$ outperforms $w^*$ by more than $t$ is exponentially small in $t + c\mathbb{E}[r(W,Z)]$. On the other hand, this implicitly provides an upper tail bound on the excess risk probability. 
\end{remark}

%\subsubsection{Bernstein's Condition}

With the definitions in place, we derive the fast rate bounds under the $(\eta, c)$-central condition as follows. %We first derive the fast rate with the Bernstein condition following the idea in \cite{Grunwald2021pac} with the mutual information.
\begin{theorem}[Fast Rate with $(\eta, c)$-central condition]\label{thm:eta-c}
Suppose Assumption~\ref{assump:eta_c_r} is satisfied, then for all $\eta' \in\left(0, \eta \right]$ it holds that,
\begin{align*}
     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq & \frac{1-c}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)] + \frac{1}{c\eta' n} \sum_{i=1}^{n} I(W;Z_i).
\end{align*}
\noindent Furthermore, the excess risk is bounded by,
 \begin{align*}
     \mathbb{E}_{W}[\mathcal{R}(W)] \leq & \frac{1}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)]  + \frac{1}{c\eta' n} \sum_{i=1}^{n} I(W;Z_i).
 \end{align*}
\end{theorem}
\noindent Such a bound has a similar form with \cite[Eq.~(3)]{Grunwald2021pac} which consists of the empirical excess risk and mutual information terms and the first term is negative for some algorithms such as ERM. Notice that different from the bound in Theorem~\ref{thm:subgaussianv2}, the bound in Theorem~\ref{thm:eta-c} contains constants $c$ and $\eta'$ that do not depend on the sample size $n$. By absorbing the necessary dependence on $n$ in the definition of the ($\eta,c$)-central condition, in this case, the convergence rate will depend on the mutual information $I(W;Z_i)$, which can achieve the convergence rate of $O({1}/{n})$ for appropriate learning problems and algorithms \cite{aminian2021exact,steinke2020reasoning,Grunwald2021pac}. In the following, we analytically examine our bounds in Gaussian mean estimation, and we also empirically verify our bounds with a logistic regression problem in Appendix~\ref{sec:logistic}. 
\begin{example}
We can examine whether the unexpected excess risk satisfies the $(\eta, c)$-central condition in the Gaussian mean estimation problem. It can be checked that for $n >  \max\left\{\frac{(4\eta^2\sigma^4_N-\eta\sigma^2_N)(2\eta^2\sigma^4_N-\eta\sigma^2_N)}{\eta^2\sigma^4_N}, 4\eta^2\sigma_N^4 - 2 \eta\sigma_N^2\right\} $, 
\begin{align*}
\log \mathbb{E}_{P_W\otimes \mu}\left[e^{-\eta r(W,Z)}\right] \leq \frac{4 \eta^2\sigma_N^4 - \eta \sigma_N^2}{n} \leq -c\eta \frac{\sigma_N^2}{n}.
\end{align*}
From the above inequality, this learning problem satisfies the $(\eta, c)$-central condition for any $0 < \eta < \frac{1}{4\sigma_N^2}$ and any $c \leq 1- 4\eta\sigma_N^2$, which is independent of the sample size and thus does not affect the convergence rate. Similarly, take $\eta = \frac{1}{8\sigma_N^2}$ and $c = \frac{1}{2}$, the bound becomes
\begin{align*}
    \frac{1-c}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)] + \frac{1}{c\eta' n} \sum_{i=1}^{n} I(W;Z_i) \leq \frac{7\sigma_N^2}{n},
\end{align*}
which coincides with the bound in~Example~\ref{eg:subv2} and we can arrive at the fast rate since $I(W;Z_i) \sim O(1/n)$. 
\end{example}

It is natural to consider whether we can apply the ($\eta,c$)-central condition to the loss function and obtain fast rate results. We present the following theorem regarding the generalization error when the loss function satisfies the $(\eta,c)$-central condition.
\begin{theorem}[Generalization Error Bounds with $(\eta, c)$-central condition w.r.t. the loss function]\label{thm:eta-c-loss}
Assume the loss function $\ell(w,z_i)$ satisfies the  $\left(\eta, c \right)$-central condition for any $i\in [n]$, some constants $\eta > 0$ and $0 < c \leq 1$ under the data distribution $\mu$ and algorithm $\mathcal{A}$. Then, for all $\eta' \in\left(0, \eta \right]$, it holds that,
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq & \frac{1-c}{c}\mathbb{E}_{W\mathcal{S}_n}[\hat{L}(W,\mathcal{S}_n)] + \frac{\sum_{i=1}^{n}I(W;Z_i)}{c\eta n}. \label{eq:loss-bound}
\end{align}
\end{theorem}
\begin{remark}
From Theorem~\ref{thm:eta-c-loss}, we can evidently see the trade-off of the generalization from (\ref{eq:loss-bound}): if the empirical loss is small, we then possibly have large mutual information as $W$ fits $Z_i$ well, which usually leads to an over-fitting scenario and the generalization error will be large. In contrast, if the mutual information term $I(W;Z_i)$ is small, the empirical risk tends to be large as $W$ does not exploit much knowledge from the data and hence becomes less dependent on each instance $Z_i$, which leads to the under-fitting scenario. If we consider an extreme case where $c = 1$, the bound in (\ref{eq:loss-bound}) holds tightly as the inequality in (\ref{eq:eta_c_loss}) becomes equality due to Jensen's inequality and the loss function $\ell(W,Z_i)$ will be a constant for any $W$ and $Z_i$. For such a case, both L.H.S. and R.H.S. in  (\ref{eq:loss-bound}) are becoming zero and the bound is tight for a constant loss. Nevertheless, this bound may not be tight as the empirical loss may not be converging with increasing sample size, as illustrated in the example below.
\end{remark}
\begin{example}
Concretely, let us again examine the Gaussian mean estimation problem. To verify the ($\eta,c$)-central condition w.r.t. the loss function, we can calculate the CGF under the distribution $P_W \otimes \mu$ as:
\begin{align}
    \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta \ell(W,Z)}] = -\frac{1}{2}\log(1+2\eta \frac{n+1}{n} \sigma_N^2),
\end{align}
while the expected loss can be calculated as,
\begin{align*}
    \mathbb{E}_{P_W \otimes \mu}[\ell(W,Z)] = \frac{n+1}{n}\sigma_N^2.
\end{align*}
Then the $(\eta, c)$-central condition can be written as:
\begin{align*}
    -\log(1+2\eta \frac{n+1}{n}\sigma_N^2) \leq -2c\eta \frac{n+1}{n}\sigma_N^2.
\end{align*}
For arbitrary choice of $\eta$ and any $n \geq 1$, we can select $c$ as,
\begin{align*}
    c = \frac{\log(1+4\eta \sigma^2_N)}{4\eta \sigma^2_N} \leq \frac{\log(1+2\eta \frac{n+1}{n}\sigma_N^2)}{2\eta \frac{n+1}{n}\sigma_N^2},
\end{align*}
as the function $\frac{\log(1+x)}{x}$ is non-increasing for all $x > 0$. The generalization error can be upper bounded by,
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq & \frac{1-c}{c}\frac{n-1}{n}\sigma^2_N + \frac{1}{2c\eta(n-1)}. \label{eq:eta-c-gaussian-loss}
\end{align}
which converges to $\frac{1-c}{c}\sigma^2_N$ with the rate of $O(\frac{1}{n})$, and the bound is not tight in this case.
\end{example}
\begin{remark}
As can be seen from the above example, the R.H.S. of (\ref{eq:eta-c-gaussian-loss}) does not converge to zero with $n$ increasing when $c$ and $\eta$ are chosen to be independent of the sample size $n$, which does not match the true generalization error. We can draw a comparable insight from this example as illustrated in the previous section: obtaining the fast rate result requires us to make appropriate assumptions on the unexpected excess risk $r(W,Z)$, while making assumptions under the loss function itself is not sufficient to a tight bound.
\end{remark}

%\subsection{Bounds with Excess Risk Function}

% The virtue of this bound is that, for some widely used algorithms such as ERM, the empirical excess risk will be negative and we can further bound the excess risk with the mutual information term only. To mathematically catch sight of the differences induced by different assumptions between (\ref{eq:our_result}) and (\ref{eq:bu_result}), we reformulate the inequality~(\ref{eq:our_result}) to remove the square root sign and then depict the fast rate condition for $\eta$ and $\sigma$, as shown in the following theorem.

%We will prove that the conventional sub-Gaussian assumption can also yield the fast rate .  Recall that $r(w,z) = \ell(w,z) - \ell(w^*,z)$ and $\mathcal{R}(W) = \mathbb{E}_{\mu}[r(w,z)]$. We introduce a small bias to $\mathcal{R}(W)$ by a factor of $\alpha$ for the generalization error, say, we are going to examine the following term $\mathbb{E}_{W\mathcal{S}_n}\{\alpha \mathcal{R}(W)-\hat{\mathcal{R}}(W,\mathcal{S}_n)\}$. We then have the following lemma:
%\begin{lemma} \label{lemma:alpha}
%Assume that $r(w, Z)$ is $\sigma^{2}$-subgaussian under the distribution $P_W \otimes \mu$. Also assume that  $-r(W,Z_i) \unlhd^{P_W \otimes \mu_i}_{(1-\alpha)\eta} -\epsilon$ for some $\eta>0$ and $\epsilon > 0$ and $0 < \alpha<1$. Then it holds that
%\begin{equation}
%    \mathbb{E}\{\alpha \mathcal{R}(W)-\hat{\mathcal{R}}(W, \mathcal{S}_n)\} \leq \frac{1}{n} \sum_{i=1}^{n} \frac{\alpha^2\sigma^2}{2(1-\alpha)\epsilon} I\left(W ; Z_{i}\right)
%\end{equation}
%\end{lemma}
%\noindent With the lemma above, we will arrive at the following theorem straightforwardly.
%\begin{theorem}\label{thm:subgaussian}
%Assume Lemma~\ref{lemma:alpha} holds. Then for any $\alpha<1$, it holds that
%\begin{align}
%     \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq & \frac{1-\alpha}{\alpha} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] \\
%    &+ \frac{1}{n} \sum_{i=1}^{n} \frac{\alpha \sigma^{2}}{2 (1-\alpha) \epsilon} I\left(W ; Z_{i}\right)   
%\end{align}
%Furthermore, the expected excess risk is bounded by,
%\begin{align}
% \mathbb{E}_{W}[\mathcal{R}(W)] \leq& \frac{1}{\alpha} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] 
%    + \frac{1}{n} \sum_{i=1}^{n} \frac{\alpha \sigma^{2}}{2 (1-\alpha) \epsilon} I\left(W ; Z_{i}\right).
%\end{align}
%\end{theorem}
%\begin{remark}

%We can also relax the ESI assumption by replacing $\epsilon$ with $\mathbb{E}_{P_W \otimes P_{Z_i}}[r(W,Z_i)]$ and arrive at the following theorem.
%We relax the condition and arrive at the following theorem.
%\end{remark}



%\subsection{Fast Rate with Central Condition}
%Next we derive the fast rate with the central condition at the following theorem.
%\begin{theorem}[Fast Rate with Central Condition]\label{thm:central}
%Let $(\mu, \ell, \mathcal{W}, \mathcal{A})$ represent a learning problem which satisfies the $\bar{\eta}$-central condition. If the $(u, c)$-witness condition holds, then for any $\eta \in(0, \bar{\eta})$, we have
%\begin{equation}
% \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq (c_{u} - 1) \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(W,\mathcal{S}_n)] + \frac{c_u}{\eta} \frac{1}{n} \sum_{i=1}^{n} I(W;Z_i)
%\end{equation}
%Furthermore, the expected excess risk is bounded by,
%\begin{equation}
% \mathbb{E}_{W}[\mathcal{R}(W)] \leq c_{u} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(W,\mathcal{S}_n)] + \frac{c_u}{\eta} \frac{1}{n} \sum_{i=1}^{n} I(W;Z_i)   
%\end{equation}
%where $c_u = \frac{1}{c}\frac{\eta u + 1}{1- \frac{\eta}{\bar{\eta}}} > 1$, $u > 0$ and $c \in (0, 1]$.
%\end{theorem}
\subsection{Tightness and Justification} \label{subsec:justification}
In the following, we examine the tightness of the bound and show why $r(W,Z)$ is a more sensible choice. Technically, we use the variational representation of the KL divergence. Specifically, let $X$ be a random variable with alphabet $\mathcal{X}$ and let $P, Q$ be two probability density functions. The KL Divergence admits the following dual representation \cite{donsker1975asymptotic}:
\begin{align}
  D(P \| Q)=\sup _{f: \mathcal{X} \rightarrow \mathbb{R}} \mathbb{E}_P[f(X)]-\log \left(\mathbb{E}_Q\left[e^{f(X)}\right]\right). \label{eq:donsker}
\end{align}
and the tightness of the bound hinges on the choice of the function $f$ in (\ref{eq:donsker}). It is well known that under mild conditions \cite{birrell2022optimizing}, the optimal function for the Donsker-Varadhan representation in the mutual information $I(W;Z_i)$ is achieved by $f'(dP_{W|Z_i}/dP_W$) where $f(t) = t\log t$. We will calculate this optimizer explicitly and show that the choice of $r(W,Z_i)$ is actually tight. To this end, we firstly calculate the densities of $P_W$ and $P_{W|Z_i}$ as: $dP_W = \frac{\sqrt{n}}{\sqrt{2\pi\sigma_N^2}} \exp(\frac{-(W - \mu)^2 n }{2\sigma_N^2})$, $dP_{W|Z_i} = \frac{n}{\sqrt{2\pi \sigma_N^2(n-1)}} \exp(-\frac{(W - \frac{n-1}{n}\mu - \frac{1}{n}Z_i)^2n^2}{2\sigma_N^2(n-1)}).$ Then we can calculate the optimizer as:
\begin{align*}
    f'(dP_{W|Z_i}/dP_W) &= \log \frac{dP_{W|Z_i}}{dP_W} + 1 \\
    &= \frac{1}{2}\log \frac{n}{n-1}  -\frac{(w - \frac{n-1}{n}\mu - \frac{1}{n}z_i)^2n^2}{2\sigma_N^2(n-1) }+\frac{(w - \mu)^2 n }{2\sigma_N^2} +1 \\
    %&= \frac{1}{2}\log \frac{n}{n-1}  -\frac{(n(w - z_i) + (n-1)(z_i-\mu))^2}{2\sigma_N^2(n-1) }+\frac{(w - z_i + z_i - \mu)^2 n }{2\sigma_N^2} +1 \\
    %&=\frac{1}{2}\log \frac{n}{n-1}  + \frac{(w-z_i)^2((n-1)n-n^2) + (z_i - \mu)^2(n(n-1) - (n-1)^2)}{2\sigma_N^2(n-1)} + 1\\
    %&=\frac{1}{2}\log \frac{n}{n-1}  + \frac{(z_i - \mu)^2(n-1) - (w-z_i)^2n }{2\sigma_N^2(n-1)} + 1 \\   
    &= - \frac{(w-z_i)^2 - (\mu - z_i)^2}{2\sigma_N^2} - \frac{(w-z_i)^2}{2\sigma_N^2(n-1)} + 1 + \frac{1}{2}\log\frac{n}{n-1} 
\end{align*}
for fixed $w$ and $z_i$. The above function can be written as:
\begin{align*}
    f'(dP_{W|Z_i}/dP_W) = -\frac{r(w,z_i)}{2\sigma_N^2} - \frac{\ell(w,z_i)}{2\sigma_N^2(n-1)} + \frac{1}{2}\log\frac{n}{n-1} + 1.
\end{align*}
The unexpected excess risk $r(w,z_i)$ clearly appears in the optimizer with some scaling factor and shifting constant (up to a $O(\frac{1}{n})$ difference), which, however, will not affect the convergence. To rigorously show this, we state the following result.
\begin{lemma}
Let $\eta = \frac{1}{2\sigma^2_N}$, the choice of the function $-r(w,z_i)$ satisfies the following inequality:
\begin{align*}
    \frac{n-1}{n} I(W;Z_i) \leq \mathbb{E}_{WZ_i}[-\eta r(W,Z_i)] - \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z_i)}] \leq I(W;Z_i),
\end{align*}
\end{lemma}
The above lemma ensures that the variational representation is essentially tight for the Gaussian example. On the other hand, the mutual information bound may not be tight if we use the loss function $\ell$ without the reference to $w^*$ as the equality in the variational representations may not be achieved (up to $O(\frac{1}{n})$). 

Unlike typical information-theoretic results where the bounds are based on the assumption that the loss function is $\sigma$-sub-Gaussian, we assume that the \emph{excess risk} is $\sigma$-sub-Gaussian. Even though the bound in~(\ref{eq:bu_result}) has exactly the same form as in~(\ref{eq:our_result}), the key difference is that under our assumption, $\sigma$ can depend on the sample size and will converge to $0$ as the sample size increases, while this is not the case under the previous assumption as we see in Example~\ref{sec:example}.  Moreover, the excess risk can be straightforwardly upper bounded as in~(\ref{eq:our_result_excess}). 

%Further the ESI assumption $-r(W,Z_i) \unlhd^{P_W \otimes \mu_i}_{(1-\alpha)\eta} -\epsilon$ implies that $\mathbb{E}_{P_W \otimes \mu_i}[r(W,Z_i)] \geq \epsilon$. The small error term $\epsilon$ describes the deviation of $W$ as learned from the algorithm $\mathcal{A}$ compared to $w^*$ in expectation. 
We further argue that the choice of $-r(w,z_i)$ leads to a tight upper bound on the generalization error by proving a matching lower bound for both the excess risk and generalization error for the Gaussian mean estimation problem.
\begin{theorem}[Matching Lower Bound]\label{thm:lower_bounds}
Consider the Gaussian mean estimation problem with the ERM algorithm. With a large $n$, we have: 
\begin{align*}
    \mathbb{E}_{P_W \otimes \mu}[r(W,Z)] \geq \frac{2\sigma^2_N}{n}\sum_{i=1}^{n}I(W;Z_i) + \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(W,\mathcal{S}_n)] - \frac{1}{n-1} \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)].
\end{align*}
For the generalization error, we have:
\begin{align*}
    \mathbb{E}_{W \mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \geq 2\sigma_N^2 \frac{n-1}{n^2}\sum_{i=1}^{n} I(W;Z_i).
\end{align*}
\end{theorem}
\begin{remark}
From the above results, we observe that the sample-wise mutual information appears in both the upper and lower bounds. For the generalization error, the upper and lower bounds are matching in terms of the convergence rate with different leading constants. For the excess risk in the Gaussian mean example, the upper bound is tight since the empirical excess risk and generalization error are both of $O(\frac{1}{n})$. However, for more general learning problems, the lower bounds of the excess risk mainly depend on the empirical excess risk and the generalization error.
\end{remark}


%\begin{remark}
%With the convention that $\frac{0}{0} = 0$, if we consider the extreme case in~(\ref{eq:subgaussian}) such that $\mathbb{E}_{P_W \otimes \mu_i}[r(W,Z_i)] = 0$, then $W = w^*$ almost surely and $\eta$ will shrink to 0. In this case $W$ is independent of $Z_i$, and $I(W;Z_i) = 0$.  We have $a_{\eta} = 1$ and $\mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] = 0$, in this case the bound is tight and holds trivially. %If $\mathbb{E}_{P_W \otimes \mu_i}[r(W,Z_i)] > 0$, in other words, the learned hypothesis is not consistent with $w^*$. In this case, we can improve the bound from $\sqrt{I(W;Z_i)}$ to $I(W;Z_i)$ and arrive at a fast rate.
%\end{remark}



\subsection{Connection with Other Bounds} \label{sec:related}
Fast rate conditions are widely investigated under different learning frameworks and conditions \cite{van2015fast, mehta2017fast, koren2015fast, mhammedi2019pac, Grunwald2020, zhu2020semi, Grunwald2021pac}. As the most relevant work, our bound is similar to that found in \cite{Grunwald2021pac} which applies conditional mutual information \cite{steinke2020reasoning}, but their results do not hold for unbounded losses and specifically do not hold for sub-Gaussian losses. Our result applies to general algorithms with mutual information and our assumptions are weaker since we only require the proposed conditions to hold in expectation w.r.t. $P_W$, instead of for all $w \in\mathcal{W}$. Our results also have the benefit of allowing the convergence factors to be further improved by using different metrics and data-processing techniques, see \cite{jiao2017dependence, hafez2020conditioning, zhou2022individually} for examples.  Now it is instructive to compare the different assumptions used in the related works. We point out that the $(\eta,c)$-central condition is indeed the key assumption for generalizing the result of Theorem~\ref{thm:subgaussianv2}, which also coincides with some well-known conditions that lead to a fast rate. We firstly show that the Bernstein condition \cite{bartlett2006empirical,bartlett2006convexity, hanneke2016refined,mhammedi2019pac} implies the $(\eta,c)$-central condition for certain $\eta$ and $c$ in the following corollary. 
\begin{corollary}\label{coro:berstein}
Let $\beta \in [0,1]$ and $B \geq 1$. For a learning tuple $(\mu, \ell, \mathcal{W}, \mathcal{A})$,  we say that the \textbf{Bernstein condition} holds if the following inequality holds for the optimal hypothesis $w^*$:
\begin{align*}
       {\mathbb{E}}_{P_W \otimes \mu}&\left[\left(\ell\left(W, Z^{\prime}\right)-\ell\left(w^{*} , Z^{\prime}\right)\right)^{2}\right] \leq B \left({\mathbb{E}}_{P_W \otimes \mu}\left[\ell\left(W, Z^{\prime}\right)- \ell\left(w^{*} ; Z^{\prime}\right)\right]\right)^{\beta}.
\end{align*}
Then, if $\beta = 1$ and $r(w,z_i)$ is bounded by $-b$ with some $b > 0$ for all $w$ and $z_i$, the learning tuple also satisfies $(\min(\frac{1}{b}, \frac{1}{2B(e-2)}), \frac{1}{2})$-central condition.
\end{corollary}
The Bernstein condition is usually recognized as a characterization of the ``easiness" of the learning problem under various $\beta$ where $\beta = 1$ corresponds to the ``easiest" learning case. For bounded loss functions, the Bernstein condition will automatically hold with $\beta = 0$.  The standard Bernstein condition requires that the inequality holds for any $w\in \mathcal{W}$, which is usually difficult to satisfy even in some trivial examples as we will see in Example~\ref{sec:example}. Different from the standard setting, we only require that the learned (randomised) hypothesis $W$ satisfy the inequality in expectation. This is a weaker but more natural condition in the sense that we do not expect any $w \in \mathcal{W}$ will work but hope that the algorithm outputs the hypothesis that performs well in average. 
%\begin{remark}
% which In particular, $\beta = 1$ corresponds to the easiest and the learning rate will be $O(\frac{1}{n})$ if $I(W;Z_i)$ is converging as the sample size increasing. For the bounded loss, the Bernstein condition will automatically hold with $\beta = 0$ and it will recover the results in \cite{xu2017information,bu2020tightening} with the rate of $O(\sqrt{\frac{1}{n}})$.
%\end{remark}

The second condition is the central condition with the witness condition \cite{van2015fast,Grunwald2020}, which also implies the $(\eta,c)$-central condition. We say $(\mu, \ell, \mathcal{W}, \mathcal{A})$ satisfies the $\eta$-central condition \cite{van2015fast,Grunwald2020} if for the optimal hypothesis $w^*$, the following inequality holds,
\begin{align*}
\mathbb{E}_{P_W\otimes \mu}\left[e^{-{\eta}\left(\ell(W,Z)-\ell(w^*,Z)\right)}\right] \leq 1. %\textup{ i.e., } \ell(w^*,Z)-\ell(W,Z) \unlhd^{W\otimes \mu}_{{\eta}} 0.
\end{align*}
We also say the learning tuple  $(\mu, \ell, \mathcal{W}, \mathcal{A})$ satisfies the  $(u, c)$-witness condition \cite{Grunwald2020} if for constants $u > 0$ and $c \in (0,1]$, the following inequality holds.
\begin{align*}
     \mathbb{E}_{P_W\otimes \mu}& [\left(\ell(W,Z)-\ell({w^{*}},Z) \right) \cdot   \mathbf{1}_{\left\{\ell(W,Z)-\ell({w^{*}},Z) \leq u \right\}}] \geq c \mathbb{E}_{P_W\otimes \mu}\left[\ell(W,Z) -\ell({w^{*}},Z) \right],
\end{align*}
where $ \mathbf{1}_{\{\cdot\}}$ denotes the indicator function. Then we have the following corollary.
%For instance, consider a setting with $\mathcal{F}=\left\{f^{*}, f_{1}, f_{2}, \ldots\right\}$ where $\ell_{f^{*}}=1$ with probability 1 and, for each $j \geq 1, \ell_{f_{j}}$ is equal to 0 with probability $1-\frac{1}{j}$ and equal to $2 j$ with probability $\frac{1}{j}$. Then for all $j, \mathbb{E}\left[\ell_{f_{j}}-\ell_{f^{*}}\right]=1$, but as $j \rightarrow \infty$, empirically we will never witness the badness of $f_{j}$ as it almost surely achieves lower loss than $f^{*}$. On the other hand, if the excess loss is upper bounded by some constant $b$, we may always take $u=b$ and $c=1$ so that a witness condition is trivially satisfied. Such a condition can also be written as
\begin{corollary}\label{coro:central}
If the learning tuple satisfies both $\eta$-central condition and $(u,c)$-witness condition, then the learning tuple also satisfies the $(\eta', \frac{c-\frac{c\eta'}{\eta}}{\eta' u +1})$-central condition for any $0 < \eta' < \eta$.
\end{corollary}
The standard $\eta$-central condition is a key condition for proving the fast rate \cite{van2015fast,mehta2017fast,Grunwald2020}. Some examples are exponential concave loss functions (including log-loss) with $\eta = 1$ (see \cite{mehta2017fast,zhu2020semi} for examples) and bounded loss functions with Massart noise condition with different $\eta$ \cite{van2015fast}.  Again, different from the standard central condition, we only require that it holds in expectation w.r.t. the distribution induced by the algorithm $\mathcal{A}$. The witness condition~\cite[Def. 12]{Grunwald2020} is imposed to rule out situations in which learnability simply cannot hold. The intuitive interpretation of this condition is that we exclude bad hypothesis $w$ with negligible probability (but still can contribute to the expected loss), which we will never witness empirically. 

The above two conditions are useful in the sense that they rely on the algorithm, the loss function and the data distribution, for which the verification in real learning scenarios may be difficult. To give more concrete examples, we further specify the distribution classes of the excess risk that are more general than the sub-Gaussian assumption, and we claim that our proposed condition can be applied to heavier tail distributions such as sub-exponential, and sub-Gamma families. 
\begin{corollary}\label{coro:subexponential}
If $r(W, Z)$ is ($\nu^2$, $\alpha$)-sub-exponential under the distribution $P_W \otimes \mu$, then the learning tuple satisfies $(\min(\frac{1}{\alpha}, \frac{\nu^2}{\mathbb{E}_{P_W \otimes \mu}[r(W,Z)]}), \frac{1}{2})$-central condition. If we plug in the corresponding $\eta$ and $c$ into the generalization error bound, we have the upper bound as
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq &  \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)] + \frac{2}{\eta' n} \sum_{i=1}^{n} I(W;Z_i). \label{sub-exponential}
\end{align}
for any $0< \eta' < \min(\frac{1}{\alpha}, \frac{\nu^2}{\mathbb{E}_{P_W \otimes \mu}[r(W,Z)]})$.
\end{corollary}

\begin{corollary}\label{coro:subgamma}
If $r(W, Z)$ is ($\nu^2$, $\alpha$)-sub-Gamma under the distribution $P_W \otimes \mu$, then the learning tuple satisfies $(\frac{\mathbb{E}_{P_W\otimes\mu}[r(W,Z)]}{\nu^2+\alpha \mathbb{E}_{P_W\otimes\mu}[r(W,Z)]}, \frac{1}{2})$-central condition.
\end{corollary}
We summarize and outline all essential technical conditions in Table~\ref{tab:tech1}, allowing for more straightforward comparisons. From the table, we can see that our proposed $(\eta,c)$-central condition coincides with many existing works such as \cite{Grunwald2020} and \cite{Grunwald2021pac} for certain choices of $c$ and $\eta$. With the bounded loss, $\beta = 1$ in the Bernstein condition is equivalent to the central condition with the witness condition for fast rate, from which $(\eta,c)$-central condition follows. As an example of unbounded loss functions, the log-loss will satisfy the central and witness conditions under well-specified model \cite{wong1995probability,Grunwald2020}, which also consequently implies the $(\eta,c)$-central condition. As suggested by Theorem~\ref{thm:subgaussianv2}, Corollary~\ref{coro:subexponential} and Corollary~\ref{coro:subgamma}, the sub-Gaussian, sub-exponential and sub-Gamma conditions can also satisfy the $(\eta,c)$-central condition for different parameters in the assumptions.  

\begin{table}[h]
    \centering
    \caption{Technical Conditions Comparisons}\label{tab:tech1}
    \begin{tabular}{|c|c|}
    \hline
     Condition      &  Key Inequality   \\
     \hline
     $(\eta,c)$-Central Condition & $\log \mathbb{E}\left[e^{-\eta r(W,Z)}\right] \leq  -c \eta \mathbb{E}[r(W,Z)]$ \\
     \hline 
     Bernstein Condition with $\beta = 1$   &  $\log  \mathbb{E}\left[e^{-\eta r(W,Z)}\right] \leq  -\frac{1}{2} \eta \mathbb{E}[r(W,Z)]$ \\
     \hline 
     Central + Witness Condition    &  $\log  \mathbb{E}\left[e^{-\eta r(W,Z)}\right] \leq  -\frac{1}{c_u}\eta\mathbb{E}[r(W,Z)]$   \\
    \hline 
     Central Condition Only &  $\log  \mathbb{E}\left[e^{-\eta r(W,Z)}\right]  \leq 0$      \\
     \hline
     Sub-Gaussian Condition  & $\log \mathbb{E}\left[ e^{ -\eta r(W,Z)} \right] \leq -\eta \mathbb{E}[ r(W,Z)] +\frac{\eta^2 \sigma^2}{2}$    \\
     \hline 
     Sub-exponential Condition & $\log \mathbb{E}\left[ e^{ -\eta r(W,Z)} \right] \leq -\eta \mathbb{E}[ r(W,Z)] +\frac{\eta^2 \nu^2}{2}$  \\
     \hline 
     Sub-Gamma Condition & $\log \mathbb{E}\left[ e^{ -\eta r(W,Z)} \right] \leq -\eta \mathbb{E}[ r(W,Z)]  + \frac{\nu^2 \lambda^2}{2(1- \alpha \lambda)}$ \\
     \hline 
    \end{tabular}
\end{table}

\subsection{Extensions}
It is essential to note that in real practice, the exact empirical risk minimization algorithms can be sometimes non-trivial to implement. A straightforward and effective alternative is the regularized ERM algorithm, which involves minimizing the empirical risk function and a regularization term simultaneously to control the complexity of the hypothesis. Moreover, the regularization term can sometimes make the ERM problem easier to solve. For example, in linear regression problems, adding a regularization term can help solve ill-posed issues when there is collinearity between input features. In this section, we further apply the learning bound in Theorem~\ref{thm:eta-c} to the regularized ERM algorithm with the following optimization problem:
\begin{align*}
    w_{\sf{RERM}} = \argmin_{w \in \mathcal{W}} \hat{L}(w,\mathcal{S}_n) + \frac{\lambda}{n}g(w),
\end{align*}
where $g : \mathcal{W} \rightarrow \mathbb{R}$ denotes the regularizer function and $\lambda$ is some coefficient. We define $\hat{\mathcal{R}}_{\textup{reg}}(w,\mathcal{S}_n) = \hat{\mathcal{R}}(w,\mathcal{S}_n) + \frac{\lambda}{n}(g(w) - g(w^*))$, then we have  the following lemma.
\begin{corollary}\label{coro:rerm}
Suppose Assumption~\ref{assump:eta_c_r} hold and also assume $|g(w_1) - g(w_2)| \leq B$ for any $w_1$ and $w_2$ in $\mathcal{W}$ with some $B >0$. Then for the regularized ERM hypothesis $W_{\sf{RERM}}$:
\begin{align*}
     \mathbb{E}_{W}[\mathcal{R}(W_{\sf{RERM}})] \leq & \frac{1}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}_{\textup{reg}}\left(W_{\sf{RERM}}, \mathcal{S}_{n} \right)]  +\frac{\lambda B}{cn} + \frac{1}{c\eta' n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i). 
\end{align*}
\end{corollary}
\noindent As $\hat{\mathcal{R}}_{\textup{reg}}(w,\mathcal{S}_n)$ will be negative for $w_{\sf{RERM}}$, the regularized ERM algorithm can lead to the fast rate if $I(W_{\sf{RERM}};Z_i) \sim O(1/n)$, which coincides with results in \cite{koren2015fast}.

From Theorem~\ref{thm:eta-c}, we can achieve the linear convergence rate $O(1/n)$if the mutual information between the hypothesis and data example is converging with $O(1/n)$. To further relax the $(\eta,c)$-central condition, we can also derive the intermediate rate with the order of $O(n^{-\alpha})$ for $\alpha \in [\frac{1}{2}, 1]$. Similar to the $v$-central condition, which is a weaker condition of the $\eta$-central condition \cite{van2015fast,Grunwald2020}, we propose the $(v,c)$-central condition first and derive the intermediate rate results in Theorem~\ref{lemma:intermediate}.
\begin{definition}[$(v,c)$-Central Condition]\label{def:weaker-eta-c}
We say that $(\mu, \ell, \mathcal{W}, \mathcal{A})$ satisfies the $(\eta,c)$-central condition up to some $\epsilon > 0$ if the following inequality holds for the optimal hypothesis $w^*$:
\begin{align}
\log \mathbb{E}_{P_W\otimes \mu} & \left[e^{-{\eta}\left(\ell(W,Z)-\ell(w^*,Z)\right)}\right]  \leq  -c\eta  \mathbb{E}_{P_W\otimes \mu}\left[\ell(W,Z) - \ell(w^*,Z)\right] + \eta \epsilon. \label{eq:v-central} %\textup{ i.e., } \ell(w^*,Z)-\ell(W,Z) \unlhd^{W\otimes \mu}_{{\eta}} 0. 
\end{align}
Let $v:[0, \infty) \rightarrow[0, \infty)$ is a bounded and non-decreasing function satisfying $v(\epsilon)>0$ for all $\epsilon > 0$. We say that $(\mu, \ell, \mathcal{W}, \mathcal{A})$  satisfies the $(v,c)$-central condition if for all $\epsilon \geq 0$ such that~(\ref{eq:v-central}) is satisfied with $\eta = v(\epsilon)$.
\end{definition}
\begin{theorem}\label{lemma:intermediate}
Assume the learning tuple $(\mu, \ell, \mathcal{W}, \mathcal{A})$ satisfies the  $\left(v, c\right)$-central condition up to $\epsilon$ for some function $v$ as defined in~Def. \ref{def:weaker-eta-c} and $0 < c < 1$. Then it holds that for any $\epsilon \geq 0$ and any $0< \eta' \leq v(\epsilon)$,
\begin{align*}
     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq & \frac{1-c}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)] + \frac{1}{n} \sum_{i=1}^{n} \left( \frac{1}{\eta' c}I(W;Z_i) + \frac{\epsilon}{c}\right).
 \end{align*}
\end{theorem}
\noindent In particular, if $v(\epsilon) \asymp \epsilon^{1-\beta}$ for some $\beta \in [0,1]$, then the generalization error is bounded by,
\begin{align*}
     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq & \frac{1-c}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)]  + \frac{2}{nc}\sum_{i=1}^{n} I(W;Z_i)^{\frac{1}{2-\beta}}.
 \end{align*}
Thus, the expected generalization is found to have an order of $I(W;Z_i)^{\frac{1}{2-\beta}}$, which corresponds to the results under Bernstein's condition \cite{hanneke2016refined,mhammedi2019pac,Grunwald2021pac}. 

% \begin{lemma}
% Assume that,
% \begin{align*}
%     \log \mathbb{E}_{P_W\otimes \mu} \left[ e^{\eta(\mathbb{E}[r(W,Z_i)] - r(W,Z_i))} \right] \leq \frac{\sigma^2 \eta^2}{2} + \epsilon \eta.
% \end{align*}
% With the conditions in Theorem~\ref{thm:subgaussianv2}, we have,
% \begin{align}
%      \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq & \frac{1-a_\eta}{a_\eta} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] \\
%      &+ \frac{1}{n} \sum_{i=1}^{n}  \left( \frac{1}{\eta a_\eta}I\left(W ; Z_{i}\right) + \epsilon\right). \label{eq:subgaussianv2}
% \end{align}
% \end{lemma} 


% If $\epsilon \asymp \eta^{\alpha}$ for $\alpha \in [0,\infty)$, then the expected  generalization has the order of $I(W;Z_i)^{-\frac{\alpha}{1+\alpha}}$, achieving the intermediate rate for different $\alpha$.



% \subsection{Fast Rate with Regularized ERM}
% In this section, we focus on a specific loss function, namely the logarithmic loss. For a hypothesis $w\in\mathcal W$, the empirical risk of $w$ on a training sequence $ \mathcal{S}_n$ with regularization is defined as
% \begin{align}
% \hat L_{\textup{reg}}(w, \mathcal{S}_n):&= \hat L(w,\mathcal{S}_n) + \lambda R(w,\mathcal{S}_n) \\
%             &= \frac{1}{m}\sum_{i=1}^m\ell(w,Z_i) + \lambda R(w,\mathcal{S}_n).
% \label{eq:erm_risk}
% \end{align} 
% where $R(w,\mathcal{S}_n)$ is a regularization term that can be dependent on the data sample. The learning algorithm, which can be viewed as a stochastic mapping from the data space to the hypothesis space, defines a distribution $P_{W|\mathcal{S}_n}$ over the hypothesis space $\mathcal{W}$ given the dataset $\mathcal{S}_n$. For example, the strategy minimising $\hat L_{\textup{reg}}(w, \mathcal{S}_n)$ is also known as the regularized ERM algorithm, which can be implemented using Gibbs algorithm, stochastic gradient descent, etc. Then the expected generalization error is defined as
% \begin{align}
% \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)]:= \Esub{W\mathcal{S}_n}{L_{\mu}(W)-\hat L_{\textup{reg}}(W,\mathcal{S}_n)}
% \label{eq:gen_error_typical}
% \end{align}
% A small empirical error implies a small expected error, hence the generalization. Now we write it out,
% \begin{align*}
%     &\Esub{W\mathcal{S}_n}{\mathcal{E}(W,\mathcal{S}_n)} \\
%     &= \Esub{W\mathcal{S}_n}{L_{\mu'}(W)-(1+\lambda)\hat L(W,\mathcal{S}_n) + \lambda(\hat L(W,\mathcal{S}_n) - R(W,\mathcal{S}_n))} \\
%     &= \Esub{W\mathcal{S}_n}{L_{\mu'}(W)-(1+\lambda)\hat L(W,\mathcal{S}_n)} \\
%     &\quad + \lambda\Esub{W\mathcal{S}_n}{\hat L(W,\mathcal{S}_n) - R(W,\mathcal{S}_n)}
% \end{align*}
% The generalization error will be decomposed into two terms, the former describes the generalization error and the latter term can be intuitively interpreted as the difference between prior knowledge induced by the regularization and its posterior induced by the data. For example, if we set $R(w,S) = \|w\|^2$ as prior and $\ell(w,Z_i) = \|w - Z_i\|^2$ as posterior by $\ell_2$ norm for mean estimation problem, the difference can be regarded as the divergence between two Gaussian distributions. We may exploit it more by giving specific learning scenarios.

% Equipped with the definitions above, we will then arrive at the following theorem.
% \begin{theorem}\label{thm:reg_erm}
% Assume that $\ell(w, Z)$ is $\sigma^{2}$-subgaussian for all $w$ under the distribution $P_Z$. Also assume that for any $w$ and $P_Z$, the moment generating function of expected loss function $\Esub{P_Z}{e^{t\ell(w,Z)}}$ is lower bounded by $e^{tL}$ for all $t>0$ and some $L>0$. Then for any $\lambda > 0$ and any algorithm $P_{W|S}$, it holds that 
% \begin{equation}
%     \Esub{W\mathcal{S}_n}{\mathcal{E}(W,\mathcal{S}_n)} \leq \frac{\sigma^2}{2L\lambda n} \sum_{i=1}^{n} I(W;Z_i) + \lambda D_R(W,\mathcal{S}_n) \label{eq:}
% \end{equation}
% where $D_R(W,\mathcal{S}_n) = \Esub{W\mathcal{S}_n}{\hat L(W,\mathcal{S}_n) - R(W,\mathcal{S}_n)}$. 
% \end{theorem}

% Note that the theorem above is applicable to any algorithm $P_{W|\mathcal{S}_n}$, but we may be more interested in regularized ERM algorithm. The hyper-coefficient $\lambda$ plays a role as a trade-off between data-fitting and regularization. Particularly, if $\lambda$ is large, which means we will penalize more for the hypothesis (prior knowledge), the second term will dominate the bound. On the other hand, if $\lambda$ is small, then the first term, that describes whether the learned $W$ is overfitting the single data, will prevail instead. For a fixed $\lambda$, we may hope that both $I(W;Z_i)$ and $D_R(W;\mathcal{S}_n)$ are small simultaneously under a certain algorithm.

% Next we consider a special case where we let $R(W,\mathcal{S}_n) = \hat{L}(W,\mathcal{S}_n)$, then the bound becomes 
% \begin{align}
% \Esub{W\mathcal{S}_n}{\mathcal{E}(W,\mathcal{S}_n)} \leq \frac{1}{\lambda n} \sum_{i=1}^{n} \frac{\sigma^2}{2L} I(W;Z_i)
% \end{align}
% solely. Different from classical previous result that attains $\sqrt{I(W;Z_i)}$ \cite{xu2017information}, we arrive at a faster rate that depends on the regularization coefficient $\lambda$. If $\lambda$ goes to infinity, the bound becomes trivial since the LHS of~Eq~(\ref{eq:1}) will approach to negative infinity while the RHS will converge to zero if $I(W;Z_i)$ is finite. Hence we may be more interested in the case where $\lambda$ is arbitrarily small, where it approaches the standard ERM.  Furthermore, if we consider the transfer learning and set $R(W,\mathcal{S}_n)$ that depends on some hypothesis $W_S$ learned from source data, e.g., typically known as hypothesis transfer learning (HTL), then the source data can be regarded as the regularization. 



% \begin{table*}
%     \centering
%     \begin{tabular}{|c|c|c|}
%     \hline 
%      Condition      &  Key Inequality  & Additional Assumptions \\
%      \hline 
%      Bernstein Condition    &  $\log e^{\eta(\mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] - r(W,Z_i))} \leq  \frac{1}{2} \eta \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]$
%     &     $r(W,Z_i)$ is lower bounded by $-b$  \\
%      \hline 
%      Central Condition    &  $\log e^{\eta(\mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] - r(W,Z_i))} \leq  (1-\frac{1}{c_u})\eta\mathbb{E}_{P_{W}\otimes \mu}[r^2(W,Z_i)]$    &    Witness Condition \\
%      \hline
%      Subgaussian Condition  & $\log e^{\eta(\mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] - r(W,Z_i))} \leq \frac{\eta^2 \sigma^2}{2}$     &     \\
%      \hline 
%      Central Condition Only &  $\log e^{\eta(\mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] - r(W,Z_i))}  \leq \eta \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] $    &     \\
%      \hline 
%     \end{tabular}
%     \caption{Technical Results Comparisons}
% \end{table*}
%\subsection{Excess Risk Bound}
%\subsection{Comparison with Other Bounds}

%\subsection{Auxiliary Distribution}


\section{Examples}
In this section, we present several additional examples of fast rates that satisfy the $(\eta,c)$-central condition. In certain examples, the convergence rate may be faster than $O(1/n)$, for instance, when the mutual information $I(W;Z_i)$ exhibits an exponential convergence. In addition, we examine two supervised learning problems: linear regression, a simple problem, and logistic regression, a slightly more complicated problem, both of which demonstrate a convergence rate of $O(1/n)$. For completeness, we have also included some exceptional instances where the $(\eta,c)$-central condition is not satisfied, despite being relatively uncommon in learning problems.

\subsection{Gaussian Mean Estimation with Discrete $\mathcal{W}$}\label{sec:discretehypothesis}
In the Gaussian mean estimation problem, instead of considering $\mathcal{W} = \mathbb{R}$, we now consider an example where we assume $z_i \sim \mathcal{N}(w,1), i = 1,\cdots, n$ for some $w$ in the hypothesis space $\mathcal{W}$ that has finite elements, e.g., $\mathcal{W} = \{-1, 1\}$. We assume $w = 1$ is the true mean. Let the algorithm be maximum likelihood estimation, that is, 
\begin{align}
    w_{\textup{MLE}} &= \argmin_{W \in \mathcal{W}} \frac{1}{n}\sum_{i=1}^{n}{(z_i - w)^2},
\end{align}
which can be viewed as the empirical risk minimization with the squared loss of $\ell(w,z) = (w-z)^2$. Similar to Example~\ref{eg:gaussian_mean_zero}, the ERM algorithm produces the decision rule by:
\begin{align*}
    w_{\ERM} = \begin{cases}
1, \textup{ if } \frac{1}{n}\sum_{i=1}^{n}Z_i \geq 0, \\
-1, \textup{ otherwise.}
\end{cases}
\end{align*} 
As $\frac{1}{n}\sum_{i=1}^{n}Z_i \sim \mathcal{N}(1, \frac{1}{n})$,  we then have that,
\begin{align*}
    P(w = 1) &= 1 - Q(\sqrt{n}) \leq 1 - \frac{\sqrt{n}}{1+n} \frac{1}{\sqrt{2\pi}} \exp(-n/2),
\end{align*}
where $Q(\cdot)$ is the tail distribution function of the standard normal distribution. Similarly, we have, 
\begin{align*}
    P(w = -1) &\leq  \frac{1}{\sqrt{n}} \frac{1}{\sqrt{2\pi}}  \exp(-n/2).
\end{align*}
Then we can calculate the expected risk and the excess risk by,
\begin{align*}
    \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z)] &= P(w = 1)\mathbb{E}_{Z}[(Z-1)^2] + P(w = -1)\mathbb{E}_{Z}[(Z+1)^2]  \\
    &= 1 + 4Q(\sqrt{n}) \\
    %&\leq  1 - \frac{\sqrt{n}}{1+n} \frac{1}{\sqrt{2\pi}} \exp(-n/2) + \frac{5}{\sqrt{n}} \frac{1}{\sqrt{2\pi}}  \exp(-n/2) \\
    & \leq 1 + \frac{4}{\sqrt{2\pi n}}\exp(-n/2).
\end{align*}
It is easily calculated that $w^* = 1$ in this case and 
\begin{align*}
    \mathbb{E}_{Z}[\ell(w^*,Z)] &= \mathbb{E}_{Z}[(Z-1)^2] = 1.
\end{align*}
Then the expected excess risk is calculated as,
\begin{align*}
    \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z)] - \mathbb{E}_{Z}[\ell(W^*,Z)]  \leq  \frac{5}{\sqrt{2\pi n}}\exp(-\frac{n}{2}) = O(\frac{e^{-n}}{\sqrt{n}}).
\end{align*}
The expected generalization error is given by,
\begin{align*}
    \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] - \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{WZ_i}[\ell(W,Z_i)]  &= 2\mathbb{E}_{W\mathcal{S}_n}[W\frac{1}{n}\sum_{i=1}^{n}Z_i] - 2\mathbb{E}_W[W]\\
    &= 2\mathbb{E}_{\mathcal{S}_n}[|\frac{1}{n}\sum_{i=1}^{n}Z_i|] - 2\mathbb{E}_W[W] \\
    %&= 2\sqrt{\frac{2}{n\pi}} e^{-\frac{n}{2}} + 2 - 4\Phi(-\sqrt{n}) - 2(1-Q(\sqrt{n})) + 2Q(\sqrt{n}) \\
    &= 2\sqrt{\frac{2}{n\pi}} \exp(-\frac{n}{2}) = O(\frac{e^{-n}}{\sqrt{n}}).
\end{align*}
 %To calculate $\mathbb{E}_{WZ_i}[\ell(W,Z_i)]$, we know $\mathbb{E}_{WZ_i}[\ell(W,Z_i)]$ are equal for each $Z_i$, then we can calculate,
%\begin{align*}
%    \mathbb{E}_{WZ_i}[\ell(W,Z_i)] &= 1 + 4Q(\sqrt{n}) - 2\sqrt{\frac{2}{n\pi}} \exp(-\frac{n}{2}).
%\end{align*}
Now we evaluate our mutual information bound and compare it to the true excess risk and generalization error. For a fixed $z_i$, we have,
\begin{align*}
     P(w=1|z_i) &= P(\frac{1}{n-1}\sum_{j\neq i} Z_j \geq - \frac{1}{n-1}z_i) \\
     %&= 1 - P(\frac{1}{n-1}\sum_{j\neq i} Z_j \leq - \frac{1}{n-1}z_i) \\
     &= Q(\frac{z_i}{\sqrt{n-1}} + \sqrt{n-1}).
\end{align*}
We then calculate the mutual information for a large $n$ by:
\begin{align*}
    I(W;Z_i) &= H(W) - H(W|Z_i) \\
    &= h_2(Q(\sqrt{n})) - h_2(\mathbb{E}_{Z_i}[Q(\frac{Z_i}{\sqrt{n-1}} + \sqrt{n-1})]) \\
    &\leq h_2(Q(\sqrt{n})) - h_2(Q(\frac{1}{\sqrt{n-1}} + \sqrt{n-1})) \\
    &= O(\frac{e^{-n}}{\sqrt{n}}),
\end{align*}
where the inequality follows Jensen's inequality with the fact that $Q$-function is locally convex for a large $n$ as $Q''(x) = \frac{x}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ and $h_2$ is a concave function. %We next examine the sub-Gaussian condition as follows:
%\begin{align*}
%\mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W,Z)}] &= P(w=1) \mathbb{E}_{Z}[e^{\eta (Z-1)^2}] + P(w=-1) \mathbb{E}_{Z}[e^{\eta (Z+1)^2}] \\
%&= (1 - Q(\sqrt{n}))\mathbb{E}_{Z}[e^{\eta (Z-1)^2}]  + Q(\sqrt{n})\mathbb{E}_{Z}[e^{\eta (Z+1)^2}],
%\end{align*}
%with the calculations that,
%\begin{align*}
%    \mathbb{E}_{\mu}[e^{\eta (Z-1)^2}] 
    %&= \int \frac{1}{\sqrt{2\pi}}e^{-\frac{(z-1)^2}{2}}e^{\eta (z-1)^2} dz \\
    %&= \int \frac{1}{\sqrt{2\pi}} e^{-(\frac{1-2\eta}{2}) (z-1)^2} dz \\
    %&= \frac{1}{\sqrt{2\pi}} * \sqrt{2\pi \frac{1}{1-2\eta}} \\
%    &= \sqrt{\frac{1}{1-2\eta}},\\
%    \mathbb{E}_{\mu}[e^{\eta (Z+1)^2}] %&= \int \frac{1}{\sqrt{2\pi}}e^{-\frac{(z-1)^2}{2}}e^{\eta (z+1)^2} dz \\
    %&= \int \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{z^2-2z+1}{2} + \eta z^2 + 2\eta z + \eta \right) dz  \\
    %&= \int \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(1-2\eta)z^2-(2 + 4\eta)z+1}{2} + \eta \right) dz  \\  
    %&= \int \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(1-2\eta)z^2-(2 + 4\eta)z + \frac{(1+2\eta)^2}{1-2\eta}}{2} + \eta - \frac{1}{2} + \frac{(1+2\eta)^2}{2(1-2\eta)} \right) dz \\
    %&= \frac{1}{\sqrt{2\pi}} \sqrt{2\pi \frac{1}{1-2\eta}} \exp\left(\eta - \frac{1}{2} + \frac{(1+2\eta)^2}{2(1-2\eta)}\right) \\
 %   &= \sqrt{\frac{1}{1-2\eta}} \exp\left( \frac{4\eta}{1-2\eta}\right),
%\end{align*}
%we have,
%\begin{align*}
%    \mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W,Z)}] = (1 - A\exp(-Bn))\sqrt{\frac{1}{1-2\eta}}  + A\exp(-Bn)\sqrt{\frac{1}{1-2\eta}} \exp\left( \frac{4\eta}{1-2\eta}\right). 
%\end{align*}
%\begin{align*}
%    \log \mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W,Z)}] = -\frac{1}{2}\log (1-2\eta) + \log \left( 1 -Q(\sqrt{n}) + Q(\sqrt{n}) \exp\left( \frac{4\eta}{1-2\eta}\right) \right)
%\end{align*}
%We also have
%\begin{align*}
%    \mathbb{E}_{P_W\otimes \mu}[\eta\ell(W,Z)] &=  \eta(1 + 4Q(\sqrt{n})).
%\end{align*}
By writing the excess risk as $r(w,z) = \ell(w,z) - \ell(w^*,z) = (w - z)^2 - (1 - z)^2$, its moment generating function is calculated by:
\begin{align*}
\mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}] &= P(w=1) + P(w=-1) \mathbb{E}_{Z}[e^{-4\eta Z}] \\
&= 1 - Q(\sqrt{n})  + Q(\sqrt{n})\mathbb{E}_{Z}[e^{-4\eta Z}].
\end{align*}
We can then calculate
\begin{align*}
    \mathbb{E}_{Z}[e^{-4\eta Z}] %&= \int \frac{1}{\sqrt{2\pi}}e^{-\frac{(z-1)^2}{2}}e^{4\eta z} dz \\
    %&= \int \frac{1}{\sqrt{2\pi}}\exp\left( {-\frac{(z-1)^2}{2}} + {4\eta z}\right) dz \\
    %&= \int \frac{1}{\sqrt{2\pi}}\exp\left( {-\frac{z^2- (2+8\eta) z + 1}{2}} \right) dz  \\
    %&= \int \frac{1}{\sqrt{2\pi}}\exp\left( {-\frac{z^2- (2+8\eta) z + (1+4\eta)^2}{2}} + 8\eta^2 + 4\eta \right) dz  \\
    &= e^{8\eta^2 - 4\eta}
\end{align*}
and 
\begin{align*}
    \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}] = \log (1 -Q(\sqrt{n})  + Q(\sqrt{n})\exp(8\eta^2 - 4\eta)).
\end{align*}
%Therefore,
%\begin{align*}
%    \log \mathbb{E}_{P_W\otimes \mu}[e^{\eta r(W,Z)}] - \eta \mathbb{E}_{P_W\otimes \mu}[r(W,Z)] &\leq Q(\sqrt{n})(\exp(8\eta^2 + 4\eta) - 1) - 4\eta Q(\sqrt{n}) \\
%    &\leq (\exp(8\eta^2 + 4\eta) - 4\eta - 1) Q(\sqrt{n}) 
%\end{align*}
Therefore, we can check the $(\eta,c)$-central condition with $\log(1+x) \leq x$ for any $x> -1$:
\begin{align*}
    \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}] &\leq   Q(\sqrt{n}) (e^{8\eta^2-4\eta}-1) \\
    &= -4c\eta Q(\sqrt{n}).
\end{align*}
where we can choose $c = \frac{1-e^{8\eta^2-4\eta}}{4\eta} \leq 1-2\eta$ with selecting any $\eta \in (0,\frac{1}{2})$. Then the bound shows that:
\begin{align*}
    \mathbb{E}_{W\mathcal{S}}[\mathcal{E}(W,\mathcal{S})] \leq \frac{2}{\eta n}\sum^{n}_{i=1}I(W;Z_i) = O(\frac{e^{-n}}{\sqrt{n}}),
\end{align*}
which dominantly decays exponentially. Such a bound matches the true convergence rate and this example shows that with the $(\eta,c)$-central condition, the convergence rate is dominated by the mutual information between the hypothesis and the instances.


% \subsection{Mean Estimation with Exponential Distribution}
% Let $Z_i \sim \textup{exp}(\frac{1}{\lambda})$, e.g., $P(Z_i) = \frac{1}{\lambda} e^{-\frac{Z_i}{\lambda}}$ so that $\mathbb{E}[Z_i] = \lambda$. Let the loss function be the log-loss, e.g., $\ell(w,z_i) = \log w + \frac{z_i}{w}$ and ERM gives the solution as $w = \frac{1}{n}\sum_{i=1}^{n}z_i$, which is ($n, \frac{n}{\lambda}$)-Gamma distributed. Then we can calculate the generalization error by:
% \begin{align*}
%     \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] - \mathbb{E}_{P_{WZ_i}}[\ell(W,Z_i)] &= \lambda^2 +\frac{\lambda^2}{n} - \lambda^2  + \frac{\lambda^2}{n} \\
%     &= \frac{2\lambda^2}{n}.
% \end{align*}
% Then
% \begin{align*}
%     \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] = -h(W) + \frac{n}{n-1} \frac{1}{\lambda} \lambda = -h(W) + \frac{n}{n-1} 
% \end{align*}
% Here $W$ is $(n, \frac{n}{\lambda})$-Gamma distributed:
% \begin{align*}
%     dP_W=\frac{(n/\lambda)^n}{\Gamma(n)} w^{n-1} e^{-(n/\lambda) w}
% \end{align*}
% Given $Z_i$, with the fact that $W - \frac{1}{n}Z_i$ is ($n-1,\frac{n}{\lambda}$)-Gamma distributed, we have the density as: 
% \begin{align*}
%     dP_{W|Z_i}=\frac{(n/\lambda)^{n-1}}{\Gamma(n-1)} (w-\frac{1}{n}z_i)^{n-2} e^{-(n/\lambda) (w-\frac{1}{n}z_i)}
% \end{align*}
% Therefore:
% \begin{align*}
%     f'\left(\frac{dP_{W|Z_i}}{P_W} \right) &= \log \left(\frac{\Gamma(n)}{\Gamma(n-1)}\frac{\lambda }{n} \frac{(w-\frac{1}{n}z_i)^{n-2}}{w^{n-1}} e^{z_i/\lambda}\right) + 1 \\
%     %&= \log \frac{n-1}{n} + (n-2)\log(w-\frac{1}{n}z_i) -  (n-2) \log w + \log \frac{\lambda}{w} + \frac{z_i}{\lambda} + 1 \\
%     &= \log \frac{n-1}{n} + (n-2)\log \left(1 - \frac{z_i}{nw} \right) + \log \frac{\lambda}{w} + \frac{z_i}{\lambda} + 1 \\
%     &= \frac{z_i}{\lambda} + \log \lambda - \log w - (n-2)\log \frac{w- \frac{z_i}{n}}{w} + 1 + \log \frac{n-1}{n}.
% \end{align*}
% The entropy of the hypothesis is:
% \begin{align*}
%     h(W) = n + \ln \frac{\lambda}{n}+\ln \Gamma(n) +(1-n) \psi(n),
% \end{align*}
% where $\psi(\cdot)$ denotes the digamma function. We can also calculate the conditional entropy $h(W|Z_i) = n -1 + \ln \frac{\lambda}{n} + \ln \Gamma(n-1) + (2-n)\psi(n-1)$. Hence the mutual information can be calculated as:
% \begin{align*}
%     I(W;Z_i) &= h(W) - h(W|Z_i) \\
%     &= 1 + \log n + (1-n)\psi(n) - (2-n)\psi(n-1).
% \end{align*}
% For large $n$, we use the approximation as $\psi(n) = \log n - \frac{1}{2n} + o(\frac{1}{n})$ and we can calculate:
% \begin{align*}
%     I(W;Z_i) &= 1 + \log n + (1-n)\psi(n) - (2-n)\psi(n-1)
%     = O(\frac{1}{n}).
% \end{align*}
% By checking the $(\eta,c)$-central condition, $r(w,z_i) = \log w- \log \lambda + \frac{z_i}{w} - \frac{z_i}{\lambda}$.
% \begin{align*}
%     \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}] = .
% \end{align*}
% \begin{align*}
%     \mathbb{E}_{P_W\otimes \mu}[r(W,Z)] = \psi(n) - \log \frac{n}{\lambda} - \log \lambda + \frac{1}{(n-1)\lambda} = O(\frac{1}{n}).
% \end{align*}

\subsection{Linear Regression}
We now extend the Gaussian mean estimation example to the linear regression problem where we have the instance space $\mathcal{Z} = \mathcal{X} \times \mathcal{Y}$ where $\mathcal{X} \subseteq \mathbb{R}^{d}$ represents the feature space and $\mathcal{Y} \subseteq \mathbb{R}$ represents the label space. We consider the linear regression model with the case $\mathcal{X} \subseteq \mathbb{R}$ for simplicity such that the label is generated in the following way:
\begin{align}
    Y_i = w^*X_i + \epsilon_i
\end{align}
where $w^* \in \mathbb{R}$ denotes the underlying (but unknown) hypothesis and $\epsilon_i$ are the noises i.i.d. drawn from some zero-mean distribution. We consider the loss function $\ell(w,z_i) = (y_i - wx_i)^2$. Then the ERM solution can be calculated as:
\begin{align*}
    w_{\ERM} = \frac{\sum_{i=1}^{n} x_iy_i}{\sum_{i=1}^{n} x^2_i} = w^* + \sum_{i}\frac{x_i}{\sum_{i}x^2_i}\epsilon_i.
\end{align*}
We consider the fixed design such that $x_i$ is not randomized where the optimal hypothesis is $w^*$ and we assume $x_i \neq x_j$ for $i \neq j$. Assume $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$, we can then calculate the expected loss over $\epsilon$ as:
\begin{align*}
    \mathbb{E}_{Z_i}[\ell(w^*,Z_i)] = \mathbb{E}_{\epsilon_i}[(w^*x_i + \epsilon_i - w^*x_i)^2] = \mathbb{E}_{\epsilon_i}[ \epsilon_i^2] = \sigma^2.
\end{align*}
The expected loss under $w_{\ERM}$ can be calculated as:
\begin{align*}
    \mathbb{E}_{P_W\otimes Z_i}[\ell(W_\ERM,Z_i)] &= \mathbb{E}_{\epsilon \otimes \epsilon'_i}[(w^*x_i + \epsilon'_i - W_\ERM x_i)^2] \\
    &= \mathbb{E}_{\epsilon_j \otimes \epsilon'_i}[(\epsilon'_i - \sum_{j}\frac{x_ix_j}{\sum_{j}x^2_j}\epsilon_j )^2]  \\
    &= \sigma^2 + \frac{x^2_i}{\sum_{j} x^2_j}\sigma^2    
\end{align*}
Therefore the excess risk can be calculated as:
\begin{align*}
    \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{P_W\otimes \mu}[\ell(W_\ERM,Z_i)] -  \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{Z_i}[\ell(w^*,Z_i)]  = \frac{1}{n}\sigma^2,
\end{align*}
which scales with $O(\frac{1}{n})$. For the generalization error, we have,
\begin{align*}
   \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{P_W\otimes Z_i}[\ell(W_\ERM,Z_i)] - \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{WZ_i}[\ell(W_\ERM,Z_i)]  &= \frac{1}{n}(\sigma^2 + \frac{1}{n}\sigma^2 - \sigma^2 - \frac{1}{n}\sigma^2 + 2\frac{\sigma^2}{n}) \\
    &= \frac{2}{n}\sigma^2,
\end{align*}
which also scales with $O(\frac{1}{n})$. Next, we can calculate the mutual information by:
\begin{align*}
    I(W;Z_i) &= h(W) - h(W|Z_i) = \frac{1}{2}\log \frac{\sum_{j}x_j^2}{\sum_{j \backslash i} x_j^2}.
    %&= \frac{1}{2}\log 2\pi e \frac{1}{\sum_{j}x_j^2}\sigma^2 - \frac{1}{2}\log 2\pi e \frac{\sum_{j \backslash i} x_j^2}{(\sum_{j}x_j^2)^2}\sigma^2 \\
\end{align*}
Then we have,
\begin{align*}
    \frac{1}{n}\sum_{i=1}^{n}I(W;Z_i) &\leq \frac{1}{2n} \sum_{i}\frac{x^2_i}{\sum_{j \backslash i}x^2_j}.
\end{align*}
%If $x_i$ is distributed uniformly, e.g., 
For $n > 3$ and any $i$, there exists a constant $c \in (0,1)$ such that $\sum_{j \backslash i} x^2_j \geq c \sum_j x^2_j$ holds. Then we can further upper bound the mutual information terms by,
\begin{align*}
    \frac{1}{n}\sum_{i=1}^{n}I(W;Z_i) &\leq \frac{1}{2n} \sum_{i}\frac{x^2_i}{c\sum_{j}{x^2_j}} = \frac{1}{2nc},
\end{align*}
which scales with $O(1/n)$. Since $\ell(W,Z_i)$ is $(1+\frac{x^2_i}{\sum_j x^2_j})\sigma^2\chi^2$ distributed, we calculate the moment generating function as,
\begin{align}
    \mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W_\ERM,Z_i)}] & = -\frac{1}{2}\log(1 - 2(1+\frac{x^2_i}{\sum_j x^2_j})\sigma^2\eta).
\end{align}
Let $\sigma_{i} = (1+\frac{x^2_i}{\sum_j x^2_j})\sigma^2$. We have,
\begin{align}
    \mathbb{E}_{P_W\otimes \mu}[\exp{\eta (\ell(W_\ERM,Z_i)- \mathbb{E}[\ell(W_\ERM,Z_i)])} ] & \leq 2\sigma_i^4\eta^2.
\end{align}
Therefore, the generalization error bound becomes:
\begin{align*}
    \frac{1}{n}\sum_{i=1}^{n} \sqrt{2\sigma^4_iI(W;Z_i)} = \frac{1}{n}\sum_{i=1}^{n} \sqrt{2\left(\frac{\sum_j x^2_j + x^2_i}{\sum_j x^2_j}\right)^2 \sigma^4I(W;Z_i)} \leq \sqrt{(2-c)^2\sigma^4\frac{1}{nc}},
\end{align*}
which scales with $O(\sqrt{\frac{1}{n}})$. If we consider the excess risk $r(W,Z_i)$, we can calculate that,
\begin{align*}
    \log \mathbb{E}_{P_W\otimes \mu}\left[\exp(-\eta r(W,Z_i) )\right] = \frac{1}{2}\log \frac{\sum_{j} x^2_j / x^2_i}{\sum_{j} x^2_j / x^2_i - (4\eta^2\sigma^4 - 2\eta \sigma^2)} \leq \frac{2\eta^2 \sigma^4 - \eta \sigma^2}{\sum_j x^2_j / x^2_i} \leq -c\eta \frac{\sigma^2 x^2_i}{\sum_{j}x^2_j}.
\end{align*}
Hence the $(\eta,c)$ is satisfied for $c \leq 1 - 2\eta\sigma^2$ and $\eta \leq \frac{1}{4\sigma^2}$. By selecting $\eta = \frac{1}{4\sigma^2}$ and $c = \frac{1}{2}$, the bounds for the ERM (by ignoring the empirical excess risk terms) become:
\begin{align*}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq \frac{1}{\eta c n} \sum_{i=1}^{n} I(W;Z_i) \leq \frac{8\sigma^2}{n} \sum_{i=1}^{n} I(W;Z_i) 
\end{align*}
which scales with $O(\frac{1}{n})$. 



%To examine the tightness of the Donsker-Varahdan representation, we can write the optimal function as:
%\begin{align*}
%    f'(W,\mathcal{S}) = \log \frac{P_{W|\mathcal{S}}}{P_W} + 1.
%\end{align*}
%Here $P_{W|\mathcal{S}} = 1$ for the maximum index $k$ and $P_{W|\mathcal{S}} = 0$ for other indices $j \neq k$. Since $Z_i$ are i.i.d. distributed, $P_W$ will be uniformly distributed over $[1:n]$. Hence,
%\begin{align*}
%    f'(W, \mathcal{S}) = \begin{cases}
 %   \log n + 1, \textup{ if $W = \argmax_{i} Z_i$.}\\
%    -\infty, \textup{ otherwise.}
%    \end{cases}
%\end{align*}
%Therefore, to achieve a valid function we can set $\ell(W,\mathcal{S}) = Z_{W}$ where $W = \argmax_{i} Z_i$. The KL divergence, in this case, is tight since:
%\begin{align*}
%    I(W;\mathcal{S}) \geq \eta \mathbb{E}_{W\mathcal{S}}[\ell(W,\mathcal{S})] - \eta^2\frac{\sigma^2}{2}.
%\end{align*}
%As $\mathbb{E}_{W\mathcal{S}}[\ell(W,\mathcal{S})]$ is $\sqrt{\log n}$ and $I(W;\mathcal{S}) = \log n$, we can choose $\eta \sim \sqrt{\log(n)}$ to achieve a tight bound. 

\subsection{Logistic Regression}\label{sec:logistic}
We apply our bound in a typical classification problem. Consider a logistic regression problem in a 2-dimensional space. For each $w \in \mathbb{R}^2$ and $z_i = (x_i,y_i) \in \mathbb{R}^{2} \times \{0,1\}$, the loss function is given by
\begin{align*}
    \ell(w,z_i) := -(y_i\log (\sigma(w^Tx_i)) + (1-y_i)\log (1 - \sigma(w^Tx_i)))
\end{align*}
where $\sigma(x) = \frac{1}{1+e^{-x}}$. Here each $x_i$ is drawn from a standard multivariate Gaussian distribution $\mathcal{N}(0,\mathbf{I}_{2})$ and Let $w^* = (0.5,0.5)$, then each $y_i$ is drawn from the Bernoulli distribution with the probability $P(Y_i = 1|x_i, w^*) = \sigma(-x^T_iw^*)$. We also restrict hypothesis space as $\mathcal{W} = \{w: \|w\|_2 < 3\}$ where $W_{\ERM}$ falls in this area with high probability. Since the hypothesis is bounded and under the log-loss, then the learning problem will satisfy the central and witness condition \cite{van2015fast,Grunwald2020}. Therefore, it will satisfy the $(\eta,c)$-central condition. We will evaluate the generalization error and excess risk bounds in (\ref{thm:eta-c}). To this end, we need to estimate $\eta$, $c$ and mutual information $I(W_{\ERM},Z_i)$ efficiently, hence we repeatedly generate $W_{\ERM}$ and $Z_i$ and use the empirical density for estimation. Specifically, we vary the sample size $n$ from $50$ to $400$ and for each $n$ we repeat the logistic regression algorithm 2000 times to generate a set of $W_\ERM$. By fixing $\eta = 0.8$ as an example, we can empirically estimate the CGF and expected excess risk with the data sample and a set of ERM hypotheses, which leads to $c \approx 0.42$. It is worth noting that from the experiments, once $\eta$ is fixed, the choice of $c$ actually does not depend on the sample size $n$, which empirically confirms the ($\eta,c$)-central condition. For the mutual information, we decompose $I(W;X,Y) =I(W;Y) + P(Y = 0)I(W;X|y = 0) + P(Y = 1)I(W;X|y = 1)$ by chain rule, and the first term can be approximated using the continuous-discrete estimator\cite{gao2017estimating} for mutual information and the rest terms are continuous-continuous ones\cite{moddemeijer1989estimation,kraskov2004estimating}. To demonstrate the usefulness of the results, we also compare the bounds with the true excess risk and true generalization error. The comparisons are shown in Figure~\ref{fig:logistic}.
\begin{figure}[H]
	\centering
	\subfloat[Generalization Error]{\includegraphics[width=0.35\textwidth]{gen_bound.png}}	\quad
	\subfloat[Excess Risk]{\includegraphics[width=0.35\textwidth]{excess_bound.png}}
	%\caption{Comparisons for testing results of true generalization error(blue), generalization error bound (green), true excess risk (orange) and excess risk bound(red). { \color{red}, for each rows, describe the parameters} we set $n = 1000$, $\beta = 0.5$, $\alpha =0.5$, $p' =w^*= 0.1$, $p = 0.9$, $w_{\ERM} = \alpha p + (1-\alpha) p' = 0.5$, $T = 100$, $K_{ST}(0) = 10$, $\eta(0) = 0.1$, $\sigma_t = \sqrt{\theta\eta(t) / t}$, $\theta = 0.001$, we also set $W(0) = 0.3$ and high probability $\delta = 0.01$, by examining the sample number $n$, $\beta$ and $W(0)$ respectively.}\label{fig:result}	%\vspace{-3mm}
    \caption{We represent the true expected generalization error in (a) and true excess risk in (b) along with their bounds in Theorem~\ref{thm:eta-c}. Here we vary $n$ from 50 to 400. We also plot their reciprocals to show the rate w.r.t. sample size $n$. All results are derived by 2000 experimental repeats.}\label{fig:logistic}
\end{figure}
From the figure, we can see that both the generalization error and excess risk converge as $O(\frac{1}{n})$, and the bounds in Theorem~\ref{thm:eta-c} are tight, which capture the true behaviours with the same decay rate.


\subsection{Examples when $(\eta,c)$-central condition does not hold}
There are cases where $(\eta,c)$-central condition does not hold. One instance is, when $\mathbb{E}_{P_W\otimes \mu}[r(W,Z)] = 0$ but $\log\mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}] > 0$. This is an extreme case such that the algorithm yields the hypothesis that could achieve the same loss as the optimal hypothesis $w^*$, but a positive CGF of $r(w,z)$ under $P_W \otimes \mu$. For example, if there is more than one optimal hypothesis for the learning task and we denote the optimal set by $\mathcal{W}^*=(w^*_1, w^*_2, \cdots, w^*_k)$ (e.g., all hypotheses in the hypothesis space are optimal), and if there is an algorithm that could produce a hypothesis distributed over $\mathcal{W}^*$,  the expected excess risk will be zero with choosing an arbitrary optimal hypothesis from $\mathcal{W}^*$.  But the CGF may not necessarily be zero as we have variances in the choices of the optimal hypothesis under $P_W$. Due to the fact that these regimes are rarely encountered in actual learning problems, the excess risk is presumed to be non-zero in order to satisfy the $(\eta,c)$-central condition. However, we still point this out to ensure the completeness of the results. In the following, we give an example where $(\eta,c)$-central condition does not hold. 

\begin{example}[Gaussian mean estimation with all hypotheses equally optimal] \label{eg:gaussian_mean_zero}
Different from the example in \ref{sec:discretehypothesis}, we consider the zero mean Gaussian case where $Z_i \sim \mathcal{N}(0,1), i = 1,\cdots, n$ and the same hypothesis space $\mathcal{W} = \{-1, 1\}$. Intuitively speaking, unlike the previous situation, even if we have abundant data, both $1$ and $-1$ are the optimal hypothesis due to the symmetry. Mathematically, let the algorithm be maximum likelihood estimation and the ERM algorithm produces the decision rule by:
\begin{align*}
W_{\ERM} = \begin{cases}
1, \textup{ if } \frac{1}{n}\sum_{i=1}^{n}Z_i \geq 0, \\
-1, \textup{ otherwise.}
\end{cases}
\end{align*} 
Then the distribution of $W_{\ERM}$ is Bernoulli distributed with $P(w = 1) = P\left(\frac{1}{n}\sum_{i=1}^{n}Z_i \geq 0 \right).$ As $\frac{1}{n}\sum_{i=1}^{n}Z_i \sim \mathcal{N}(0, \frac{1}{n})$,  we have that,
\begin{align*}
    P(w = 1) &= P(w = -1) = \frac{1}{2} 
\end{align*}
due to the symmetry. Then we can calculate the expected risk and the excess risk by,
\begin{align*}
    \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z)] &= P(w = 1)\mathbb{E}_{Z}[(Z-1)^2] + P(w = -1)\mathbb{E}_{Z}[(Z+1)^2] = 2.
\end{align*}
It is important to note that $w^* $ can be either $-1$ or $1$ in this case, and 
\begin{align*}
    \mathbb{E}_{Z}[\ell(W^*,Z)] &= \mathbb{E}_{Z}[(Z-1)^2] = 2.
\end{align*}
Then the \textbf{excess risk} is calculated as,
\begin{align*}
    \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z)] - \mathbb{E}_{Z}[\ell(W^*,Z)]  = 0.
\end{align*}
The expected generalization error is given by,
\begin{align*}
    \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] - \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{WZ_i}[\ell(W,Z_i)]  %&= 2 - \mathbb{E}_{W\mathcal{S}_n}[W^2 + 2W\frac{1}{n}\sum_{i=1}^{n}Z_i - \frac{1}{n}\sum_{i=1}^{n}Z^2_i] \\
    %&= \mathbb{E}_{W\mathcal{S}_n}[2W\frac{1}{n}\sum_{i=1}^{n}Z_i] \\
    &= 2\mathbb{E}_{\mathcal{S}_n}\left[\left|\frac{1}{n}\sum_{i=1}^{n}Z_i\right|\right] 
    = \sqrt{\frac{8}{\pi n}}
\end{align*}
due to the fact that the expectation of the absolute Gaussian r.v. with mean of $\mu$ and variance of $\sigma^2$ is $\sigma \sqrt{\frac{2}{\pi}} e^{-\frac{\mu^2}{2 \sigma^2}}+\mu\left(1-2 \Phi\left(\frac{-\mu}{\sigma}\right)\right)$.  %To calculate $\mathbb{E}_{WZ_i}[\ell(W,Z_i)]$, we can write,
%\begin{align*}
%mathbb{E}_{WZ_i}[\ell(W,Z_i)] &= \mathbb{E}_{Z_i}[\mathbb{E}_{W|Z_i}[\ell(W,Z_i)]] \\
%    &= \mathbb{E}_{Z_i}[P(w=1|Z_i)(Z_i-1)^2 + P(w=-1|Z_i)(Z_i + 1)^2]
%\end{align*}
%For a fixed $z_i$, we have,
%begin{align*}
%    P(w=1|z_i) &= P(\frac{1}{n-1}\sum_{j\neq i} Z_j \geq - \frac{1}{n-1}z_i) \\
%    &= 1 - P(\frac{1}{n-1}\sum_{j\neq i} Z_j \leq - \frac{1}{n-1}z_i) \\
%frac{1}{2} - \frac{1}{2} \textup{erf}\left( - \frac{z_i}{\sqrt{2(n-1)}} \right).
%\end{align*}
%Then, 
%begin{align*}
%    \mathbb{E}_{WZ_i}[\ell(W,Z_i)] &= \mathbb{E}_{Z_i}[P(w=1|Z_i)(Z_i-1)^2 + P(w=-1|Z_i)(Z_i + 1)^2] \\
%    &= 2 + 2A\exp\left(- B(n-1) \right)
%\end{align*}
%The \textbf{expected generalization error} is (approximately),
%begin{align*}
%    \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{WZ_i}[\ell(W,Z_i)] - \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] &= 4A\exp\left(- B(n-1) \right) - 4A\exp\left(- Bn \right) \\
%    &= 4A\exp\left(- Bn \right)(\exp(B) - 1) \sim O(e^{-n}).
%\end{align*}
%We then calculate the mutual information for large $n$ by:
%\begin{align*}
%    I(W;Z_i) &= H(W) - H(W|Z_i) = h_2(p(w = 1)) - \mathbb{E}_{Z_i}[h_2(p(w = 1 |Z_i))].
%\end{align*}
% We next examine the sub-Gaussian condition as follows:
% \begin{align*}
% \mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W,Z)}] &= P(w=1) \mathbb{E}_{Z}[e^{\eta (Z-1)^2}] + P(w=-1) \mathbb{E}_{Z}[e^{\eta (Z+1)^2}] \\
% &= \frac{1}{2}\mathbb{E}_{Z}[e^{\eta (Z-1)^2}]  + \frac{1}{2}\mathbb{E}_{Z}[e^{\eta (Z+1)^2}].
% \end{align*}
% Due to the symmetry, we can calculate
% \begin{align*}
% \mathbb{E}_{Z}[e^{\eta (Z+1)^2}] = \mathbb{E}_{Z}[e^{\eta (Z-1)^2}] &= \int \frac{1}{\sqrt{2\pi}}\exp(-\frac{z^2}{2} + \eta (z-1)^2) dz \\
%     &= \int \frac{1}{\sqrt{2\pi}} \exp(-\frac{z^2 - 2\eta z^2 + 4\eta z}{2} +  \eta) dz \\
%     &= \int \frac{1}{\sqrt{2\pi}} \exp(-\frac{(1-2\eta)z^2 + 4\eta z + \frac{4\eta^2}{1-2\eta}}{2} +  \eta + \frac{2\eta^2}{1-2\eta}) dz \\
%     &= \sqrt{\frac{1}{1-2\eta}}\exp\left( \frac{\eta}{1-2\eta}\right).
% \end{align*}
% Then we have,
% \begin{align*}
%     \log \mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W,Z)}] = -\frac{1}{2}\log (1-2\eta) + \frac{\eta}{1-2\eta}
% \end{align*}
% The CGF can be upper bounded by
% \begin{align*}
%     \log \mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W,Z)}] - \mathbb{E}_{P_W\otimes \mu}[\eta\ell(W,Z)] &\leq \eta^2 \sigma^2
% \end{align*}
% for some constant $\sigma^2$. The bound is not tight as $\sigma^2$ does not converge as $n$ increases. 
If we choose $w^* = 1$ and the excess risk can be written as $r(w,z) = \ell(w,z) - \ell(w^*,z) = (w - z)^2 - (1 - z)^2$, now we can calculate the MGF of the excess risk explicitly as: 
\begin{align*}
\mathbb{E}_{P_W\otimes \mu}[e^{\eta r(W,Z)}] &= P(w=1) + P(w=-1) \mathbb{E}_{Z}[e^{4\eta Z}] = \frac{1}{2}+ \frac{1}{2}\mathbb{E}_{Z}[e^{4\eta Z}].
\end{align*}
For the second term, we have the following:
\begin{align*}
    \mathbb{E}_{Z}[e^{4\eta Z}] &= \int^{+\infty}_{-\infty} \frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}e^{4\eta z} dz 
    %&= \int \frac{1}{\sqrt{2\pi}}\exp\left( {-\frac{z^2- 8\eta z + 16\eta^2}{2}} + 8\eta^2 \right) dz  \\
    = e^{8\eta^2}.
\end{align*}
Finally, it yields the CGF by, 
\begin{align*}
    \log \mathbb{E}_{P_W\otimes \mu}[e^{\eta r(W,Z)}] = \log ( \frac{1}{2}  + \frac{1}{2}\exp(8\eta^2)).
\end{align*}
In this case, the $(\eta,c)$-central condition does \textbf{not} hold as $\log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}] = \log ( \frac{1}{2}  + \frac{1}{2}\exp(8\eta^2)) \geq 0$ for any $\eta > 0$. %However, the ERM algorithm still satisfies the sub-Gaussian assumption as:
%\begin{align*}
%    \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}] = \log ( \frac{1}{2}  + \frac{1}{2}\exp(8\eta^2)) \leq 8\eta^2.
%\end{align*}
%for any $\eta$. 
\end{example}



\begin{example}[Hypothesis Selection \cite{russo2016controlling}] \label{eg:hypothesis_selection}
Let us consider a different scenario for hypothesis selection that does not satisfy the $(\eta,c)$-central condition as well. Let $\mathcal{S} = (Z_1,Z_2,\cdots, Z_n)$ where $Z_i \in \mathbb{R}$ is a random variable with the mean of $\mu_i$ and $W$ to be some selection hypothesis such that $\mathcal{W} = [1:n]$. If we seek for the largest instance $Z_i$ in the dataset $\mathcal{S}$, we define the loss function $\ell(W,\mathcal{S}) = -Z_{W}$ and simply minimising the loss function will lead to a hypothesis that produces the instance with the largest value. Then we have that $\mathbb{E}_{W\mathcal{S}}[\ell(W,\mathcal{S})] = -\mathbb{E}_{W\mathcal{S}}[Z_W]$ and $\mathbb{E}_{W\otimes \mathcal{S}}[\ell(W,\mathcal{S})] = -\mathbb{E}_{W\otimes \mathcal{S}}[Z_W] = -\mathbb{E}_{W}\mathbb{E}_\mathcal{S}[Z_W] = -\mathbb{E}_W[\mu_W]$. We can write:
\begin{align*}
    |\mathbb{E}_{W\mathcal{S}}[\ell(W,\mathcal{S})] - \mathbb{E}_{W\otimes \mathcal{S}}[\ell(W,\mathcal{S})]| = |\mathbb{E}_{W\mathcal{S}}[Z_W - \mu_W]|. %\leq 2\sqrt{\sigma I(W;\mathcal{S})}.
\end{align*}
We consider the ERM hypothesis $W = \argmin \ell(W,\mathcal{S}) = \argmax_i Z_i$ and assume all $Z_i$ are normally distributed and have the same mean of some positive $\mu$ and variance of $\sigma^2$ for simplicity. Then we have that:
\begin{align*}
I(W;\mathcal{S}) = H(W) - H(W|\mathcal{S}) = H(W) = \log n.    
\end{align*}
Since we assume $\mu_W = \mu$ for all $W$, as $n$ goes to infinity, $|\mathbb{E}_{W\mathcal{S}}[Z_W - \mu_W]| = \sqrt{2\sigma^2 \log n}$. Now we check the $(\eta,c)$-central condition w.r.t. the excess risk function $r(W,\mathcal{S}) = \ell(W,\mathcal{S}) - \ell(w^*, \mathcal{S})$ where $w^*$ can be any index in $[1:n]$:
\begin{align*}
    \log \mathbb{E}_{W\otimes \mathcal{S}}\left[ e^{-\eta r(W, \mathcal{S})} \right] \leq -c\eta \mathbb{E}_{W\otimes \mathcal{S}}\left[ r(W, \mathcal{S})\right]
\end{align*}
for some $c \in [0,1]$. We can then calculate that 
\begin{align*}
    \log \mathbb{E}_{W\otimes \mathcal{S}}\left[ e^{-\eta r(W, \mathcal{S})}\right] = \log \left(\frac{1}{n} + \frac{n-1}{n} \exp{\sigma^2\eta^2}\right) > 0 ,
\end{align*}
for positive $\eta$ while the expected excess risk is $\mathbb{E}_{W\otimes \mathcal{S}}\left[ r(W, \mathcal{S})\right] = 0$ and this contradicts to the proposed $(\eta,c)$-central condition. In both examples, we observe that the expected excess risk for the ERM hypothesis is always zero because each hypothesis in the class is optimal for the given distribution. However, the CGF could still be positive since we have stochasticity in the hypothesis derived, which is a rare occurrence in most learning scenarios, and we highlight this unique scenario for completeness.  
\end{example}
% which satisfies $\eta \leq \frac{2(c-1)\mu}{\sigma^2}$. Then we have the bound that:
% \begin{align*}
% |\mathbb{E}_{W\mathcal{S}}[Z_W - \mu_W]| \leq (c-1)\mu + \frac{I(W;\mathcal{S})}{\eta},
% \end{align*}
% for some $c>1$. To optimize the bound on the right-hand side, we could let $c = \log n$ and $\eta = \frac{2(c-1)\mu}{\sigma^2}$ and we have that the generalization error is $O(\sqrt{\log n})$, which is tight in this case.
% \end{example}
%Fast rate conditions are widely investigated under different learning frameworks and conditions \cite{van2015fast,mehta2017fast, koren2015fast, mhammedi2019pac,Grunwald2020, zhu2020semi, Grunwald2021pac}. We propose a crucial $(\eta,c)$-central condition that can lead to the fast rate for the generalization error in expectation, which also coincides with many existing works such as \cite{Grunwald2020} and \cite{Grunwald2021pac} for certain choices of $c$ and $\eta$. As a consequence, many loss functions satisfy. For example, with bounded loss, $\beta = 1$ in Bernstein condition is equivalent to strong central condition with the witness condition for fast rate, from which $(\eta,c)$-central condition follows. With unbounded loss, the log-loss will satisfy the central and witness conditions under well-specified model \cite{wong1995probability,Grunwald2020}, which also consequently implies the $(\eta,c)$-central condition. As the most relevant work, our bound has a similar form as \cite{Grunwald2021pac} which applies the conditional mutual information \cite{steinke2020reasoning}, but their results are derived under the PAC-Bayes framework and requires the prior knowledge over hypothesis. Our result applies for  general algorithms with mutual information and our assumptions are weaker since we only requires the proposed conditions hold in expectation w.r.t. $P_W$, instead of for all $w \in\mathcal{W}$. Another advantage of our results is that different metrics and data-processing techniques can be used to further tighten the bound to improve the convergence factors, see \cite{jiao2017dependence,hafez2020conditioning, zhou2022individually} for examples.
%\cite{Grunwald2020},\cite{Grunwald2021pac},\cite{van2015fast},\cite{mhammedi2019pac},\cite{steinke2020reasoning},\cite{mehta2017fast}, \cite{zhu2020semi}


    %\item For bounded loss, Bernstein condition implies $v$-central condition, see Theorem 5.4 in \cite{van2015fast}.
    %\item Bernstein condition implies witness condition for unbounded loss for $\beta = 1$ [A weaker $\tau$-witness condition also holds for other $\beta$], see Proposition 18 in \cite{Grunwald2020}.
    %\item Strong central condition $\rightarrow$ $v$-central condition.
% We also summarize the key technical conditions in Table~\ref{tab:tech} for arriving the technich.

\section{Conclusion}
As we demonstrate in this paper, if the sub-Gaussian assumption is made regarding the excess loss in the typical information-theoretic generalization error bounds, the square root does not necessarily prevent us from the fast rate. On top of that, we identify the key conditions that lead to the fast and intermediate rate bound in expectation. Intuitively speaking, to achieve a fast rate bound for both the generalization error in expectation, the output hypothesis of a learning algorithm must be ``good" enough compared to the optimal hypothesis $w^*$. Here we encode the notion of goodness in terms of the CGF by controlling the gap between $\ell(w,Z)$ and $\ell(w^*,Z)$.  Further, we verify the proposed bounds and present the results analytically with examples. We remark that there is some room for future work to improve the bounds. One possible direction is to develop novel techniques for removing the empirical excess risk term in the bound for general algorithms other than ERM or regularized ERM. Additionally, in most cases, $w^*$ is usually not known but one could possibly seek a reference hypothesis $\hat{w}$ that could be trained from the sample and close to $w^*$, allowing more flexibility of the bounds in real applications.

% \begin{itemize}
%     \item In-expectation generalization error and in-probability generalization error.
%     \item 
% \end{itemize}
% \section{Fast Rate on Transfer Learning} \label{sec:transfer}
% We consider the transfer learning in this section. Let $\mu$  and $\mu'$ be two probability distributions defined on $\mathcal Z$, and  assume that $\mu$ is \emph{absolute continuous} with respect to $\mu'$ ($\mu \ll \mu'$). In the sequel, the distribution $\mu$ is referred to as the \textit{source distribution}, and $\mu'$ as the \textit{target distribution}. We are given two sample sets $S$ and $S'$ and we assume that the samples $S'=\{Z'_1,\ldots, Z'_{n}\}$ are drawn IID from the target distribution, and the samples $S=\{Z_{1},\ldots, Z_{m}\}$ are drawn IID from the source distribution. 

% In the setup of transfer learning, a learning algorithm is  a (randomized) mapping from the training data $S, S'$  to a hypothesis $w \in\mathcal W$, characterized by a conditional distribution $P_{W|SS'}$, with the goal to find a hypothesis $w$ that minimizes the population risk with respect to the \textit{target distribution}
% \begin{align}
% L_{\mu'}(w):=\Esub{Z'\sim\mu'}{\ell(w, Z')} \label{eq:pop_risk}
% \end{align}
% where $Z'$ is distributed according to $\mu'$.  Notice that $\beta=0$ corresponds to the important case when we do not have any samples from the target distribution. Obviously, $\beta=1$ takes us back to the classical setup where training data comes from the same distribution as test data.
 
% We focus on one particular \textit{empirical risk minimization} (ERM) algorithm.  For a hypothesis $w\in\mathcal W$, the empirical risk of $w$ on a training sequence $\tilde S:=\{Z_1,\ldots, Z_m\}$ is defined as
% \begin{align}
% \hat L(w,\tilde S):=\frac{1}{m}\sum_{i=1}^m\ell(w,Z_i).
% \label{eq:erm_risk}
% \end{align}
% Given samples $S$ and $S'$ from both distributions, it is natural to form an empirical risk function as a convex combination of the empirical risk induced by $S$ and $S'$ \cite{ben-david_theory_2010} defined as
% \begin{align*} 
% \hat L_{\alpha}(w,S,S'):=\frac{\alpha}{n}\sum_{i=1}^{n}\ell(w,Z'_i)+ \frac{1-\alpha}{m}\sum_{i=1}^{m}\ell(w,Z_i)  
% %\label{eq:err}
% \end{align*}
% for some weight parameter $\alpha\in[0,1]$ to be determined. We define $W_{\ERM} :=\text{argmin}_w \hat L_{\alpha}(w)$ as the ERM solution,  and also define the optimal hypothesis (with respect to the distribution $\mu'$) as
% %\begin{align*}
% $w^*=\text{argmin}_{w\in\mathcal W} L_{\mu'}(w)$.
% %\end{align*}

% %We are interested in two quantities for this ERM algorithm.  The first one is the  \textit{generalization error} defined as
% %\begin{align}
% %\gen(W_{\ERM}, S, S'):= L_{\mu'}(W_{\ERM})-\hat L_{\alpha}(W_{\ERM},S,S')
% %\label{eq:gen_error}
% %\end{align}
% %namely the difference between the minimized empirical risk and the population risk of the ERM solution under the target distribution. 
% We are interested in the \textit{excess risk} as
% \begin{align*}
% \mathcal{R}_{\mu'}(W_{\ERM}) :=L_{\mu'}(W_{\ERM})-L_{\mu'}(w^*)
% \end{align*}
% which is the difference between the population risk of $W_{\ERM}$ compared to that of the optimal hypothesis. We also define the \emph{empirical} excess risk w.r.t. $w^*$ for some $w \in \mathcal{W}$ as
% \begin{align*}
% \hat{\mathcal{R}}(w, S, S') :=  \hat L_{\alpha}(w,S,S') -  \hat L_{\alpha}(w^*,S,S')
% \end{align*}
% Notice that the expected excess risk can be bounded by the following inequality:
% %\begin{align}
% %L_{\mu'}(W_{\ERM})-L_{\mu'}(w^*) =& L_{\mu'}(W_{\ERM})-\hat L_{\alpha}(W_{\ERM},S,S')+\hat %L_{\alpha}(W_{\ERM}) \nonumber \\
% %&-\hat L_{\alpha}(w^*) +\hat L_{\alpha}(w^*)-L_{\alpha}(w^*) \nonumber \\
% %&+L_{\alpha}(w^*)-L_{\mu'}(w^*)\nonumber\\
% %\leq & \gen(W_{\ERM}, S, S')+\hat L_{\alpha}(w^*) \nonumber \\
% %& -L_{\alpha}(w^*)+(1-\alpha)(L_{\mu}(w^*)-L_{\mu'}(w^*))
% %\label{eq:excess_expression}
% %\end{align}
% \begin{align}
% \mathbb{E}_{WSS'}[L_{\mu'}(W_{\ERM}) & - L_{\mu'}(w^*)] \leq  \mathcal{E}_{\textup{tr}}(W_{\ERM}, S,S') \label{eq:excess_expression}
% \end{align}
% where we define
% \begin{align}
%     \mathcal{E}_{\textup{tr}} (W_{\ERM},& S,S')   = \alpha \Esub{WSS'}{\mathcal{R}_{\mu'}(W_\ERM) - \hat{\mathcal{R}}(W_{\ERM},S')} \nonumber \\
%     &+ (1-\alpha) \Esub{WSS'}{\mathcal{R}_{\mu'}(W_\ERM) - \hat{\mathcal{R}}(W_{\ERM},S)}\nonumber
% \end{align}
% and we have also used the fact  $\hat L_{\alpha}(W_{\ERM},S,S')-\hat L_{\alpha}(w^*,S,S')\leq 0$ by the definition of $W_{\ERM}$. For any $w\in\mathcal W$, the quantity $L_{\alpha}(w)$ in the above expression  is defined as
% \begin{align*}
% L_{\alpha}(w) := (1-\alpha)\Esub{Z\sim\mu}{\ell(w,Z)}+\alpha \Esub{Z\sim\mu'}{\ell(w,Z)}.
% \end{align*}
% Next we will bound $\mathcal{E}_{\textup{tr}}(W_{\ERM}, S,S')$. 
% \begin{theorem}[Fast Rate with Central Condition]\label{thm:central-transfer}
% Let $(\mu', \ell, \mathcal{W}, \mathcal{A})$ represent a learning problem which satisfies the $\bar{\eta}$-central condition for the target domain. If the $(u, c)$-witness condition holds under $P_W \otimes \mu'$, then for any $\eta \in(0, \bar{\eta})$, we have
% \begin{align}
% &\mathbb{E}_{WSS'}[L_{\mu'}(W_{\ERM}) - L_{\mu'}(w^*)] \leq  \frac{c_u}{\eta} \frac{\alpha}{n} \sum_{i=1}^{n} I(W_\ERM;Z_i)  \nonumber \\
% &+ \frac{c_u}{\eta} \frac{1-\alpha}{m} \sum_{i=1}^{m} \left(I(W_\ERM;Z_i) + D(\mu\|\mu')\right) \nonumber
% \end{align}
% where $c_u = \frac{1}{c}\frac{\eta u + 1}{1- \frac{\eta}{\bar{\eta}}} > 1$, $u > 0$ and $c \in (0, 1]$. More generally, for any algorithm $\mathcal{A}$ and any $W$ induced by the algorithm, we have,
% \begin{align}
% &\mathbb{E}_{WSS'}[L_{\mu'}(W) - L_{\mu'}(w^*)] \leq \frac{c_u}{\eta} \frac{\alpha}{n} \sum_{i=1}^{n} I(W;Z_i)  \nonumber \\
% &+ \frac{c_u}{\eta} \frac{1-\alpha}{m} \sum_{i=1}^{m} \left(I(W;Z_i) + D(\mu\|\mu')\right) \nonumber \\
% &+ (c_u - 1)\mathbb{E}_{WSS'}[\hat{\mathcal{R}}(W,S,S')]
% \end{align}
% \end{theorem}

% \begin{remark}
% Compared to the bound in Theorem 2 in \cite{wu2020information} as,
% \begin{equation}
% \begin{aligned}
% &\mathbb{E}_{WSS'}[L_{\mu'}(W_{\ERM}) - L_{\mu'}(w^*)] \leq \frac{\alpha \sqrt{2 r^{2}}}{n} \sum_{i=1}^{n} \sqrt{I\left(W_{\ERM} ; Z'_{i}\right)} \\
% &+\frac{(1-\alpha) \sqrt{2 r^{2}}}{m} \sum_{i=1}^{m} \sqrt{\left(I\left(W_{\ERM} ; Z_{i}\right)+D\left(\mu \| \mu^{\prime}\right)\right)} \\
% & + (1-\alpha) d_{\mathcal{W}}\left(\mu, \mu^{\prime}\right)
% \end{aligned}
% \end{equation}
% where $d_{\mathcal{W}}\left(\mu, \mu^{\prime}\right)=\sup _{w \in \mathcal{W}}\left|L_{\mu}(w)-L_{\mu^{\prime}}(w)\right|$. The new bound has the following improvements:
% \begin{itemize}
%     \item The square root term is removed and we may achieve a faster rate for converging.
%     \item $d_{\mathcal{W}}\left(\mu, \mu^{\prime}\right)$ is vanished in our new bound, which might be very large for some hypothesis space.
%     \item We verify that our bound is tight in the Gaussian mean estimation example for the excess risk, while this is not the case with the previous bound.
% \end{itemize}
% \end{remark}

% \subsection{Transfer Learning Case}
% Now we apply the bound to the Gaussian mean estimation in the transfer learning case. Assume that $S$ comes from the source distribution $\mu= \mathcal{N} (\nu, \sigma^2)$ and $S'$ comes form the target distribution $\mu'=\mathcal{N}(\nu', \sigma^2)$ where $m \neq m'$.  We define the loss function as
% \begin{align*}
% \ell(w,z)=(w-z)^2.
% \end{align*}
% For the sake of simplicity, we assume the source only scenario, i.e., there is no target data. The empirical risk minimization (ERM) solution is obtained by minimizing $\hat L(w,S):=\frac{1}{m}\sum_{i=1}^{m}(w-Z_i)^2$, where the solution is given by
% \begin{small}
% \begin{align*}
% W_{\ERM}=\frac{1}{m}\sum_{i=1}^m Z_i
% \end{align*}
% \end{small}
% To obtain the upper bound, we first notice that in this case
% \begin{align*}
% I(W_{\ERM};Z_i)=\frac{1}{2}\log \frac{m}{m-1}, \text{ for all } i = 1,2,\cdots, m 
% \end{align*}
% and $D(\mu||\mu') = \frac{(\nu - \nu')^2}{2\sigma^2}$. The bound becomes $O(k(\frac{1}{m} + \frac{(\nu - \nu')^2}{\sigma^2}))$ for some constants $k$. While the true excess risk can be calculated by
% \begin{align*}
% L_{\mu'}(W_{\ERM}) - L_{\mu'}(w^*) &= \mathbb{E}_{W\otimes \mu'}[(W_\ERM - Z)^2] - \mathbb{E}_{\mu'}[(w^* - Z)^2] \\
% &= (\nu-\nu')^2 + \frac{\sigma^2}{m} + \sigma^2 - \sigma^2 \\
% &= (\nu-\nu')^2 + \frac{\sigma^2}{m}
% \end{align*}
% with $w^* = m'$, which is also $O(k(\frac{1}{m} + \frac{(\nu - \nu')^2}{\sigma^2}))$ for $k = \sigma^2$. The new bound captures the true behaviour of the bounds up to a multiplicative factor $k$, which is much tighter than the previous bound \cite{wu2020information}.

\bibliographystyle{IEEEtranN}
\bibliography{reference}

\newpage

\onecolumn

\appendix

% \subsection{Technical Notations and Lemmas}
% We will introduce the notion of exponential stochastic inequality adapted from~\cite{mhammedi2019pac} and \cite{Grunwald2020} for the sake of simplifying the notations when proving the fast rate bounds.
% \begin{definition}[Exponential Stochastic Inequality (ESI) \cite{mhammedi2019pac}] Let $\eta>0$ and $X, Y$ be random variables that can be expressed as functions of the random variable $U$ defined on the probability space $\mathcal{D}^{n}$. Then
% \begin{align*}
% X \unlhd_{\eta}^{U} Y \Leftrightarrow {\mathbb{E}}_U\left[e^{\eta(X-Y)}\right] \leq 1 .
% \end{align*}

% \end{definition}
% When no ambiguity can arise, we omit the random variable $U$. Besides simplifying notation, ESIs are useful in that they simultaneously capture "with high probability" and "in expectation" results, that is, $X \unlhd_{\eta}^{U} Y$, implies both that $\forall \delta \in(0,1), X \leq Y+\log (1 / \delta) / \eta$, with probability at least $1-\delta$ over the randomness of $U$ and that $\mathbb{E}_{U}[X] \leq \mathbb{E}_{U}[Y] .$ The following are some basic properties of ESI.

% \begin{proposition}[Properties of ESI\cite{mhammedi2019pac,Grunwald2021pac}]
% $ $
% \begin{itemize}
%     \item (In Probability) If $X \unlhd_{\eta} Y$, then $\forall \delta \in(0,1), X \leq$ $Y+\frac{\log \frac{1}{\bar{z}}}{\eta}$, with probability at least $1-\delta$. 
%     \item (In Expecatation) Let $\bar{\eta}>0$ and let $g:[0, \bar{\eta}]$ be continuous and nondecreasing. If for all $\eta$ with $0<\eta \leq \bar{\eta}, X \unlhd_{\eta} Y+g(\eta)$, then $\mathbb{E}[X] \leq \mathbb{E}[Y]+g(0)$. 
%     \item (Transitivity and Chain Rule) 
%     Let $Z_{1}, \ldots, Z_{n}$ be any random variables on $\mathcal{Z}$ (not necessarily independent). If for some $\left(\gamma_{i}\right)_{i \in[n]} \in ( 0,+\infty)^{n}$, then $Z_{i} \unlhd_{\gamma_{i}} 0$. For all $i \in[n]$, 
%     \begin{align*}
%     \sum_{i=1}^{n} Z_{i} \unlhd_{\nu_{n}} 0, \quad \text { where } \nu_{n}:=\left(\sum_{i=1}^{n} \frac{1}{\gamma_{i}}\right)^{-1} %\quad\left(\text { if } \forall i \in[n], \gamma_{i}=\gamma>0,
%     \end{align*}
%     \item If $A \unlhd_{c\eta} Bf(\eta)$ for some function $f$, then it is equivalently to $A \unlhd_{\eta} Bf(\frac{\eta}{c})$.
%     \item If $A \unlhd_{\eta} B$, then it is equivalently to $\frac{A}{c} \unlhd_{c \eta} \frac{B}{c}$ for some $c > 0$.
% \end{itemize}
% \end{proposition}
% The following is the key proposition for bounding the excess risk using the PAC-Bayes results.
% \begin{proposition}[ESI PAC-Bayes] 
% Fix $\eta>0$ and let $\left\{Y_{w}: w \in \mathcal{W}\right\}$ be any family of random variables such that for all $w \in \mathcal{W}, Y_{w} \unlhd_{\eta} 0 .$ Let $P_{0}$ be any distribution on $\mathcal{W}$ and let $P: \bigcup_{i=1}^{n} \mathcal{Z}^{i} \rightarrow \mathcal{P}(\mathcal{W})$ be a learning algorithm. We have:
% \begin{align*}
% \mathbb{E}_{w \sim P_{n}}\left[Y_{w}\right] \unlhd_{\eta} \frac{\operatorname{KL}\left(P_{n} \| P_{0}\right)}{\eta}, \quad \text { where } P_{n}:=P\left(\mathcal{S}_n\right) .
% \end{align*}
% \end{proposition}
% To introduce this proposition to the bound, we need to find a family of random variables $Y_{w}$ that is associated with the excess risk and depends on $w$, and for any $w$,
% \begin{equation*}
%     E_{Y_w}[e^{\eta Y_w}] \leq 1.
% \end{equation*}
% We may also want to make $\eta$ stochastic so that we can tune $\eta$ after observing the data, and we can extend the results from fixed $\eta$ to random.
% \begin{proposition}[ESI from fixed to random $\eta$] 
% Let $\mathcal{G}$ be a countable subset of $(0,+\infty)$ and let $\pi$ be a prior distribution over $\mathcal{G}$. Given a countable collection $\left\{Y_{\eta}: \eta \in \mathcal{G}\right\}$ of random variables satisfying $Y_{\eta} \unlhd_{\eta} 0$, for all fixed $\eta \in \mathcal{G}$ and $\eta \geq \eta_0$, we have, for arbitrary estimator $\hat{\eta}$ with support on $\mathcal{G}$,
% \begin{align*}
% Y_{\hat{\eta}} \unlhd_{\hat{\eta_0}} \frac{-\ln \pi(\hat{\eta})}{\hat{\eta}} .
% \end{align*}
% \end{proposition}
% We have the following well-known proposition based on Bernstein's condition.
% \begin{proposition}[\cite{Grunwald2021pac}]
% Suppose that $(\mu, \ell, \mathcal{W}, \mathcal{A})$ satisfies the $\left(B, \beta^{*}\right)$-Bernstein condition for some $\beta^{*} \in[0,1]$. Pick any $c>0, \eta<1 /(2 B c)$. Then for all $0<\beta \leq \beta^{*}$ and for all $w \in \mathcal{W}$ :
% \begin{align*}
%     c \eta \mathbb{E}_{ W \otimes \mu} & \left[\left(\ell\left(w,  Z^{\prime}\right)-\ell\left(w^{*} , Z^{\prime}\right)\right)^{2}\right]  \leq (1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}} + \left(\frac{1}{2} \wedge \beta\right) \cdot\left({\mathbb{E}_{W \otimes \mu}}\left[\ell\left(w,  Z^{\prime}\right)-\ell\left(w^{*} ; Z^{\prime}\right)\right]\right)
% \end{align*}
% \end{proposition}

% When proving the bounds under the central condition, for $\eta > 0$ and general random variables $U$, we define, respectively, the Hellinger-transformed expectation and the annealed expectation, also known as R\'enyi-transformed expectation \cite{Grunwald2020} as
% \begin{align}
% &\mathbb{E}^{\mathrm{HE}(\eta)}[U]:=\frac{1}{\eta}\left(1-\mathbb{E}\left[e^{-\eta U}\right]\right) \\
% &\mathbb{E}^{\mathrm{ANN}(\eta)}[U]:=-\frac{1}{\eta} \log \mathbb{E}\left[e^{-\eta U}\right]
% \end{align}

% \begin{lemma} We have the following factors:
% \begin{itemize}
%     \item Factor 1:
%     \begin{equation}
% \mathbb{E}^{\mathrm{HE}(\eta)}[U] \leq \mathbb{E}^{\mathrm{ANN}(\eta)}[U] \leq \mathbb{E}[U]
% \end{equation}
% where the first inequality follows from $\log x \leq x - 1$ and the second from Jensen's inequality.
% \item Factor 2: if $\mathbb{E}\left[e^{-\eta X}\right]<\infty$, we have 
% \begin{equation}
% \lim_{\eta \rightarrow 0} \mathbb{E}^{\mathrm{HE}(\eta)}[X]=\mathbb{E}[X]
% \end{equation}
% \item Factor 3: $\eta \mapsto \mathbb{E}^{\operatorname{ANN}(\eta)}[X]$ is non-increasing.
% \end{itemize}
% \end{lemma}

\section{Proofs}

\subsection{Proof of~Theorem~\ref{thm:subgaussian}}
\begin{proof}
It is found that, due to $w^*$ is independent of $Z_i$, we have
\begin{equation}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W, \mathcal{S}_n)] = \mathbb{E}_{W\mathcal{S}_n}[\mathcal{R}(W) - \hat{\mathcal{R}}(W, \mathcal{S}_n)] = \mathbb{E}_{W \otimes \mathcal{S}_n}[\hat{\mathcal{R}}(W,\mathcal{S}_n)] - \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(W,\mathcal{S}_n)] .
\end{equation}
Let the distribution $P_{WZ_i}$ denote the joint distribution induced by $P_{W\mathcal{S}_n}$ with the algorihtm $P_{W|\mathcal{S}_n}$. With the i.i.d. assumption, we can rewrite the generalization error by,
\begin{equation}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] = \frac{1}{n}\sum_{i=1}^n \mathbb{E}_{P_W \otimes \mu}[r(W,Z_i)] - \mathbb{E}_{WZ_i}[r(W,Z_i)].
\end{equation}
Using the KL-divergence property \cite{xu2017information,bu2020tightening} that
\begin{align}
    \sqrt{ 2\sigma^2 D\left( P_{WZ_i} \| P_{W}\otimes P_{Z_i} \right)} \geq \mathbb{E}_{P_W \otimes \mu}[r(W,Z_i)] - \mathbb{E}_{WZ_i}[r(W,Z_i)]
\end{align}
under the $\sigma$-subgaussian assumption under the distribution $P_{W} \otimes \mu$. Summing up every term concludes the proof.
\end{proof}

\subsection{Proof of~Theorem~\ref{thm:lower_bounds}}
\begin{proof}
With Jensen's inequality, we have that:
\begin{align}
    I(W;Z_i) &= \mathbb{E}_{P_{WZ_i}}\left[-\frac{r(W,Z_i)}{2\sigma_N^2}- \frac{(W-Z_i)^2}{2\sigma^2_N(n-1)}\right] - \log \mathbb{E}_{P_W\otimes \mu}\left[\exp{\left(-\frac{r(W,Z_i)}{2\sigma_N^2} - \frac{(W-Z_i)^2}{2\sigma^2_N(n-1)}\right)}\right] \nonumber \\
    & \leq \mathbb{E}_{P_{WZ_i}}\left[-\frac{r(W,Z_i)}{2\sigma_N^2}- \frac{(W-Z_i)^2}{2\sigma^2_N(n-1)}\right] - \mathbb{E}_{P_W\otimes \mu}\left[{-\frac{r(W,Z_i)}{2\sigma_N^2} - \frac{(W-Z_i)^2}{2\sigma^2_N(n-1)}}\right], \label{eq:lower_bounds}
\end{align}
By re-arrange the inequality, we have that the following holds for all $i = 1,2, \cdots, n$, 
\begin{align}
    \mathbb{E}_{P_W\otimes \mu}\left[r(W,Z_i)\right] \geq 2\sigma^2_NI(W;Z_i) + \mathbb{E}_{P_{WZ_i}}[r(W,Z_i)] + \frac{1}{n-1} (\mathbb{E}_{WZ_i}\left[ \ell(W,Z_i)\right] - \mathbb{E}_{P_W\otimes \mu}\left[ \ell(W,Z_i)\right]).
\end{align}
We complete the proof of the lower bound on the excess risk by averaging over $Z_i$. For the generalization error, we rewrite \eqref{eq:lower_bounds} as
\begin{align}
   \frac{1}{2\sigma^2_N}\frac{n}{n-1} (\mathbb{E}_{P_W\otimes \mu}\left[ \ell(W,Z_i)\right] - \mathbb{E}_{WZ_i}\left[ \ell(W,Z_i)\right] ) \geq I(W;Z_i).
\end{align}
Summing up every term for $Z_i$ completes the proof for the generalization error.
\end{proof}


\subsection{Proof of~Theorem~\ref{thm:subgaussianv2}}
\begin{proof}
Using the Donsker-Varadhan representation of the KL divergence, we build on the following inequality for some $\eta > 0$,
\begin{align}
    \frac{I(W;Z_i)}{\eta} + \Esub{P_{WZ_i}}{r(W,Z_i)} &\geq  - \frac{1}{\eta} \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}].
\end{align}
We will bound the R.H.S. using the following technique. For any $a$, we have,
\begin{align}
    \log \mathbb{E}_{P_{W}\otimes \mu}[e^{a\eta \mathbb{E}[r(W,Z_i)] -\eta(r(W,Z_i))}] &= \log \mathbb{E}_{P_{W}\otimes \mu}[e^{a\eta \mathbb{E}[r(W,Z_i)] - \eta(r(W,Z_i))}] \\
    &= \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta (\mathbb{E}[r(W,Z_i)] - r(W,Z_i)) + (a-1)\eta \mathbb{E}[r(W,Z_i)]}] \\
    &\leq \frac{\sigma^2 \eta^2}{2} + (a-1) \eta \mathbb{E}[r(W,Z_i)]. 
\end{align}
By setting $0 < a_\eta = 1-  \frac{\eta\sigma^2}{2\mathbb{E}[r(W,Z_i)]} < 1$, we have,
\begin{align}
    0 < \eta < \frac{2\mathbb{E}[r(W;Z_i)]}{\sigma^2},
\end{align}
and
\begin{align}
    \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}] &\leq -a_\eta \eta \mathbb{E}[r(W,Z_i)]. 
\end{align}
We then rewrite the inequality by,
\begin{align}
    -\frac{1}{\eta}\log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}] &\geq a_\eta \mathbb{E}[r(W,Z_i)], 
\end{align}
and we further have the following bound,
\begin{align}
    \frac{I(W;Z_i)}{\eta} + \mathbb{E}_{P_{WZ_i}}[r(W,Z_i)] &\geq a_\eta \mathbb{E}_{P_WP_{Z_i}}[r(W,Z_i)].
\end{align}
Hence,
\begin{align}
    \mathbb{E}_{P_WP_{Z_i}}[r(W,Z_i)] - \mathbb{E}_{P_{WZ_i}}[r(W,Z_i)]  \leq \frac{I(W;Z_i)}{\eta a_\eta} + \frac{1-a_\eta}{a_\eta}\mathbb{E}_{P_{WZ_i}}[r(W,Z_i)].
\end{align}
Summing every term for $Z_i$, we have,
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq & \frac{1-a_\eta}{a_\eta} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] + \frac{1}{n\eta a_\eta} \sum_{i=1}^{n}  I\left(W ; Z_{i}\right),
\end{align}
which completes the proof.
\end{proof}




\subsection{Proof of~Theorem~\ref{thm:eta-c-loss}}
\begin{proof}
By the definition of mutual information, we have that for each $Z_i$:
\begin{align}
    I(W;Z_i) &\geq -\mathbb{E}_{WZ_i}[\eta' \ell(W, Z_i)] - \log \mathbb{E}_{P_W\otimes \mu}\left[e^{-\eta' \ell(W,Z_i)} \right] \\
    &\geq -\mathbb{E}_{WZ_i}[\eta' \ell(W, Z_i)] + c\eta' \mathbb{E}_{P_W \otimes \mu}[\ell(W,Z)]
\end{align}
for any $\eta' \in (0, \eta]$. The last inequality holds due to the Jensen's inequality and the $(\eta,c)$-central condition. By rearranging the equation, we arrive at the bound for the generalization error as:
\begin{align}
    \mathbb{E}_{P_W \otimes \mu}[\ell(W,Z_i)] -\mathbb{E}_{WZ_i}[\ell(W, Z_i)]  \leq \frac{I(W;Z_i)}{c\eta} + \frac{1-c}{c}\mathbb{E}_{WZ_i}[\ell(W,Z_i)]
\end{align}
which completes the proof by:
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq \frac{\sum_{i=1}^{n}I(W;Z_i)}{c\eta n} + \frac{1-c}{c}\mathbb{E}_{W\mathcal{S}_n}[\hat{L}(W,\mathcal{S}_n)].
\end{align}
\end{proof}

\subsection{Proof of~Corollary~\ref{coro:berstein}}
\begin{proof}
We first present the expected Bernstein inequality which will be the key technical lemma for the fast rate bound.
\begin{lemma}[Expected Bernstein Inequality \cite{mhammedi2019pac,cesa2006prediction}] \label{lemma:exp_bern}
Let $U$ be a random variable bounded from below by $-b < 0 $ almost surely, and let $\kappa(x)=(e^x - x - 1) / x^{2} .$ For all $\eta >0$, we have
\begin{align}
 \log \mathbb{E}_{U}\left[e^{\eta(\mathbb{E}[U]-U}) \right] \leq \eta^2 c_{\eta} \cdot \mathbb{E}[U^{2}], \quad \text { for all } c_{\eta} \geq  \kappa(\eta b).
\end{align}
\end{lemma}
%\subsection{Proof of Lemma~\ref{lemma:exp_bern}}
\begin{proof}
The proof of the lemma follows from~\cite{cesa2006prediction} and \cite{mhammedi2019pac}. Firstly we define $Y = - U$ which is upper bounded by $b$, then using the property that $\frac{e^Y - Y -1}{Y^2}$ in non-increasing for $Y \in \mathbb{R}$, then we define $Z = \eta Y$ such that,
\begin{align}
   \frac{e^{Z} - Z -1}{Z^2} \leq \frac{e^{\eta b} - \eta b - 1}{\eta^2 b^2} = \kappa (\eta b).
\end{align}
Rearranging the inequality, we then arrive at,
\begin{align}
   e^{Z} - Z -1 \leq Z^2 \kappa (\eta b).
\end{align}
Taking the expectation on both sides and using the fact that $\log (x+1) \leq x$ for any $x \in \mathbb{R}$, we have,
\begin{align}
   \log\mathbb{E}[e^{Z}]  - \mathbb{E}[Z] \leq \mathbb{E}[Z^2] \kappa (\eta b).
\end{align}
By substituting $Z = \eta Y$, we have
\begin{align}
   \mathbb{E}[e^{\eta (Y - \mathbb{E}[Y])}] \leq e^{\eta^2 \mathbb{E}[Y^2] \kappa (\eta b)}.
\end{align}
Define $c_\eta \geq \kappa(\eta b)$, it yields that
\begin{align}
   \mathbb{E}[e^{\eta (Y - \mathbb{E}[Y])}] \leq e^{\eta^2 c_\eta \mathbb{E}[Y^2] }.
\end{align}
By substituting $Y = -U$, we finally have,
\begin{align}
   \mathbb{E}[e^{\eta (\mathbb{E}[U] - U)}] \leq e^{\eta^2 c_\eta \mathbb{E}[U^2] },
\end{align}
which completes the proof.
\end{proof}
Using the Bernstein condition and we also assume that $r(w,z_i)$ is lower bounded by $-b$ almost surely, we have for all $0< \eta < \frac{1}{b}$ and all $c > \frac{e^{\eta b} - \eta b - 1}{\eta^2b^2} > 0$, the following inequality holds:
 \begin{align}
     e^{\eta(\Esub{P_{W}\otimes \mu}{r(W,Z_i)} - r(W,Z_i))} \leq e^{\eta^2 c \Esub{P_{W}\otimes \mu}{r^2(W,Z_i)}}. \label{eq:rsquare}
 \end{align}
 With Lemma~\ref{lemma:exp_bern}, we have that for some $\beta \in [0,1]$, any $c>0$ and $\eta < \frac{1}{2Bc}$, then for all $0 < \beta' \leq \beta$ we have
\begin{align}
    \eta^2c \Esub{P_W \otimes \mu}{r^2(w,Z_i)} \leq \left(\frac{1}{2} \wedge \beta'\right) \eta  \left({\mathbb{E}_{P_W\otimes \mu}}[r(w,Z_i)]\right)+(1-\beta') \cdot(2 B c \eta)^{\frac{1}{1-\beta'}}\eta.
\end{align}
Hence the equation~(\ref{eq:rsquare}) can be further bounded by
\begin{align}
    e^{\eta(\Esub{P_{W}\otimes \mu}{r(W,Z_i)} - r(W,Z_i))} \leq e^{\left(\frac{1}{2} \wedge \beta'\right) \eta  \left({\mathbb{E}_{P_W \otimes \mu}}[r(w,Z_i)]\right)+(1-\beta')(2 B c \eta)^{\frac{1}{1-\beta'}}\eta } 
    \label{bound:bernstein}
\end{align}
for $\eta' < \min(\frac{1}{2B(e-2)}, \frac{1}{b})$. Here we can choose $c$ to be $\max_{\eta} \frac{e^{\eta b} - \eta b - 1}{\eta^2b^2} = e-2$ since the function $\frac{e^x - x - 1}{x^2}$ is non-decreasing in $[0,1]$. Furthermore, if $\beta' = 1$, we can rewrite~(\ref{bound:bernstein}) as,
\begin{align}
    e^{\eta(\Esub{P_{W}\otimes \mu}{r(W,Z_i)} - r(W,Z_i))} \leq e^{\frac{1}{2} \eta  \left({\mathbb{E}_{P_W \otimes \mu}}[r(w,Z_i)]\right)} 
\end{align}
which completes the proof.
\end{proof}







\subsection{Proof of~Corollary~\ref{coro:central}}
\begin{proof}
We first present the following Lemma for bounding the excess risk using the cumulant generating function.
\begin{lemma}[Generalized from Lemma 13 in \cite{Grunwald2020}]\label{lemma:central}
Let $\bar{\eta}>0$. Assume that the expected $\eta$-strong central condition holds, and suppose further that the $(u, c)$-witness condition holds for $u>0$ and $c \in(0,1]$. Let $0 < \eta' < \eta$ and $c_{u}:=\frac{1}{c} \frac{\eta' u+1}{1-\frac{\eta'}{\eta}} > 1$, then the following inequality holds:
\begin{equation}
\mathbb{E}_{P_W \otimes \mu}\left[r(W,Z)\right]  \leq - \frac{c_{u}}{\eta'}  \log \mathbb{E}_{ P_W \otimes \mu}\left[e^{-\eta' r(W,Z)}\right] . 
\end{equation}

%More generally, suppose that the $\bar{\eta}$-central condition and the $(\tau, c)$-witness condition hold for $c \in(0,1]$ and a non-increasing function $\tau$. Then for all $\lambda>0$, all $w \in \mathcal{W}$,
%\begin{equation}
%\mathbb{E}_{\mu}\left[r(W,Z)\right] \leq \lambda \vee\left(c_{\tau(\lambda)} \cdot \mathbb{E}_{ \mu}^{\mathrm{HE}(\eta)}\left[r(W,Z)\right] \right) \leq \lambda \vee\left(c_{\tau(\lambda)} \cdot \mathbb{E}_{\mu}^{\mathrm{ANN}(\eta)}\left[r(W,Z)\right] \right) .
%\end{equation}
\end{lemma}
The proof of the above lemma is similar to the proof in Appendix C.1 (page 48) in \cite{Grunwald2020} by taking the expectation over the hypothesis distribution $P_W$, which is omitted here. Now with~Lemma~\ref{lemma:central}, we have that for any $0 < \eta' < \eta$,
\begin{align}
  \log \mathbb{E}_{ P_W \otimes \mu}\left[e^{-\eta' \left( r(W,Z) - \mathbb{E}_{P_W \otimes \mu}[r(W,Z)]  \right)}\right] &\leq -\frac{\eta'}{c_u}\mathbb{E}_{P_W \otimes \mu}\left[r(W,Z)\right] + \eta'\mathbb{E}_{P_W \otimes \mu}\left[r(W,Z)\right] \\
  &=  (1-\frac{1}{c_u}) \eta'  \mathbb{E}_{P_W \otimes \mu}\left[r(W,Z)\right].
\end{align}
Therefore, the central condition with the witness condition implies the expected $(\eta', \frac{c-\frac{c\eta'}{\eta}}{\eta' u +1})$-central condition for any $0 < \eta' < \eta$.
\end{proof}



\subsection{Proof of Theorem~\ref{thm:eta-c}}
\begin{proof}
Firstly we rewrite excess risk and empirical excess risk by:
    \begin{align}
        \mathcal{R}(w) &= \mathbb{E}_{Z\sim \mu}[\ell(w,Z)] - \mathbb{E}_{Z\sim \mu}[\ell(w^*,Z)] \nonumber \\
        &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{Z_i\sim \mu}[\ell(w,Z_i)] - \mathbb{E}_{Z_i \sim \mu}[\ell(w^*,Z_i)] \nonumber \\
        &= \mathbb{E}_{\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)],
    \end{align}
    and 
    \begin{align}
        \hat{\mathcal{R}}(w, \mathcal{S}_n) &=  \hat{L}(w,\mathcal{S}_n) - \hat{L}(w^*,\mathcal{S}_n).
    \end{align}
Given any $\mathcal{S}_n$, the gap between the excess risk and empirical excess risk can be written as,
    \begin{align}
    \mathcal{R}(w) - \hat{\mathcal{R}}(w, \mathcal{S}_n) = \mathbb{E}_{\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)] - \hat{\mathcal{R}}(w, \mathcal{S}_n).
    \end{align}
We will bound the above quantity by taking the expectation w.r.t. $w$ learned from $\mathcal{S}_n$ by: 
    \begin{align}
        \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] &= \mathbb{E}_{W\mathcal{S}_n}[\mathcal{R}(w) - \hat{\mathcal{R}}(w, \mathcal{S}_n)] \\
        &=  \mathbb{E}_{P_W\otimes \mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)] - \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)] \\
        &= \frac{1}{n}\sum_{i=1}^n \mathbb{E}_{P_W \otimes \mu}[r(W,Z_i)] - \mathbb{E}_{WZ_i}[r(W,Z_i)].
    \end{align}
Recall that the variational representation of the KL divergence between two distributions $P$ and $Q$ defined over $\mathcal X$ is given as (see, e. g. \cite{boucheron_concentration_2013})
\begin{align}
D(P||Q)=\sup_{f}\{\Esub{P}{f(X)}-\log\Esub{Q}{e^{f(x)}} \},
%\label{eq:variational}
\end{align}
where the supremum is taken over all measurable functions such that $\Esub{Q}{e^{f(x)}}$ exists. Under the expected $(\eta,c)$-central condition, for any $0< \eta' \leq \eta$, let $f(w,z_i) = -\eta' r(w,z_i)$, we have,
\begin{align}
    D(P_{WZ_i}\|P_{W}\otimes P_{Z_i}) &\geq \Esub{P_{WZ_i}}{-\eta' r(W,Z_i)} - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta'(r(W,Z_i))}] \nonumber \\
    &= \Esub{P_{WZ_i}}{-\eta' r(W,Z_i)} - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta'(r(W,Z_i) - \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)])  }] + \Esub{P_W \otimes \mu}{\eta' r(W,Z_i)} \nonumber \\
    &= \eta'\left(\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)}\right) - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta'( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }]. \label{eq:MI_KL}
\end{align}
Next we will upper bound the second term $\log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta'( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }]$ in R.H.S. using the expected $(\eta,c)$-central condition. From the $(\eta, c)$-central condition, we have,
\begin{align}
    \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }] \leq (1-c)\eta \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)].
\end{align}
Since $\eta' \leq \eta$, Jensen's inequality yields:
\begin{align}
    \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta'( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }]   &=  \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\frac{\eta'}{\eta}\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }] \\
    &\leq \log \left( \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }] \right)^{\frac{\eta'}{\eta}} \\
    &\leq \frac{\eta'}{\eta} (1-c)\eta \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] \\
    &= \eta'(1-c) \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)].
    \label{eq:bound-central}
\end{align}
Substitute (\ref{eq:bound-central}) into (\ref{eq:MI_KL}), we arrive at,
\begin{align}
    I(W;Z_i) \geq \eta' \left(\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)}\right) - (1-c)\eta' \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)].
\end{align}
Divide $\eta'$ on both side, we arrive at,
\begin{align}
    \frac{I(W;Z_i)}{\eta'} \geq\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} - (1-c) \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)].
\end{align}
Rearrange the equation and yields,
\begin{align}
    c\Esub{P_W \otimes \mu}{r(W,Z_i)} \leq \Esub{P_{WZ_i}}{r(W,Z_i)} + \frac{I(W;Z_i)}{\eta'}.
\end{align}
Therefore,
\begin{align}
    \Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} \leq  (\frac{1}{c} - 1)\left({\mathbb{E}_{P_{WZ_i}}}[r(w,Z_i)]\right) + \frac{I(W;Z_i)}{c\eta'}.
\end{align}
Summing up every term for $Z_i$ and divide by $n$, we end up with,
\begin{align}
    \Esub{P_W \otimes P_{\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} - \Esub{P_{W\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} \leq & (\frac{1}{c} - 1) \left({\mathbb{E}_{P_{W\mathcal{S}_n}}}[\hat{\mathcal{R}}(W,\mathcal{S}_n)]\right) + \frac{1}{n}\sum_{i=1}^{n}\frac{I(W;Z_i)}{c\eta'}.
\end{align}
Finally we completes the proof by,
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] \leq (\frac{1}{c} - 1) \left({\mathbb{E}_{P_{W\mathcal{S}_n}}}[\hat{\mathcal{R}}(W,\mathcal{S}_n)]\right) + \frac{1}{n}\sum_{i=1}^{n}\frac{I(W;Z_i)}{c\eta'}.
\end{align}

\end{proof}

\subsection{Proof of Corollary~\ref{coro:rerm}}
\begin{proof}
We first define,
\begin{align}
    \hat{{L}}_{\textup{reg}}(w,\mathcal{S}_n) := \hat{{L}}(w,\mathcal{S}_n) + \frac{\lambda}{n}g(w).
\end{align}
Based on Theorem~\ref{thm:eta-c}, we can bound the excess risk for $W_{\sf{RERM}}$ by,
\begin{align}
     \mathbb{E}_{W}[\mathcal{R}(W_{\sf{RERM}})] & \leq  \frac{1}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W_{\sf{RERM}}, \mathcal{S}_{n} \right)]  + \frac{1}{c\eta' n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i) \\
     &= \frac{1}{c} \left( \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{L}\left(W_{\sf{RERM}}, \mathcal{S}_{n} \right) - \hat{L}\left(w^*, \mathcal{S}_{n} \right)] \right)   + \frac{1}{c\eta' n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i) \\
     &\overset{(a)}{\leq} \frac{1}{c} \left( \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{{L}}_{\textup{reg}} \left(W_{\sf{RERM}}, \mathcal{S}_{n} \right)] - \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{{L}}_{\textup{reg}}\left(w^*, \mathcal{S}_{n}\right)] \right)  + \frac{\lambda B}{cn}+ \frac{1}{c\eta' n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i) \\
     & =  \frac{1}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}_{\textup{reg}}\left(W_{\sf{RERM}}, \mathcal{S}_{n} \right)]  + \frac{1}{c\eta' n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i) \\
     &\overset{(b)}{\leq}  \frac{\lambda B}{cn} + \frac{1}{c\eta' n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i).
     %&\overset{(c)}{\leq} \frac{\lambda B}{cn}+ \frac{1}{c\eta' n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i)
 \end{align}
where (a) follows since $|g(w^*) - g(W_{\sf{RERM}}))| \leq B$ the expected empirical risk is negative for $W_{\ERM}$ and (b) holds due to that $W_{\sf{RERM}}$ is the minimizer of the regularized loss. 
%Finally (c) holds due to that $W_{\sf{RERM}}$ is the minimizer of the regularized loss $\hat{\mathcal{L}}_{\textup{reg}}$.
\end{proof}

\subsection{Proof of Theorem~\ref{lemma:intermediate}}
\begin{proof}
We will build upon~(\ref{eq:MI_KL}). With the $(v,c)$-central condition, for any $\epsilon \geq 0$ and any $ 0 < \eta' \leq v(\epsilon)$, the Jensen's inequality yields:
\begin{align}
    \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta'( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }]   &=  \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\frac{\eta'}{v(\epsilon)}v(\epsilon)( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }] \\
    &\leq \log \left( \mathbb{E}_{P_{W}\otimes \mu}[e^{v(\epsilon)( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }] \right)^{\frac{\eta'}{v(\epsilon)}} \\
    &\leq \frac{\eta'}{v(\epsilon)} \left( (1-c)v(\epsilon) \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] + v(\epsilon) \epsilon \right) \\
    &= \eta'(1-c) \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] + \eta'\epsilon.
    \label{eq:bound-v-central}
\end{align}
Substitute (\ref{eq:bound-v-central}) into (\ref{eq:MI_KL}), we arrive at,
\begin{align}
    I(W;Z_i) \geq \eta' \left(\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)}\right) - (1-c)\eta' \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] - \eta' \epsilon.
\end{align}
Divide $\eta'$ on both side, we arrive at,
\begin{align}
    \frac{I(W;Z_i)}{\eta'} \geq\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} - (1-c) \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] - \epsilon.
\end{align}
Rearrange the equation and yields,
\begin{align}
    c\Esub{P_W \otimes \mu}{r(W,Z_i)} \leq \Esub{P_{WZ_i}}{r(W,Z_i)} + \frac{I(W;Z_i)}{\eta'} +\epsilon.
\end{align}
Therefore,
\begin{align}
    \Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} \leq  (\frac{1}{c} - 1)\left({\mathbb{E}_{P_{WZ_i}}}[r(w,Z_i)]\right) + \frac{I(W;Z_i)}{c\eta'} + \frac{\epsilon}{c}.
\end{align}
Summing up every term for $Z_i$ and divide by $n$, we end up with,
\begin{align}
    \Esub{P_W \otimes P_{\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} - \Esub{P_{W\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} \leq & (\frac{1}{c} - 1) \left({\mathbb{E}_{P_{W\mathcal{S}_n}}}[\hat{\mathcal{R}}(W,\mathcal{S}_n)]\right) + \frac{1}{n}\sum_{i=1}^{n}\left( \frac{I(W;Z_i)}{c\eta'} + \frac{\epsilon}{c}\right).
\end{align}
Finally we arrive at the following inequality:
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] \leq (\frac{1}{c} - 1) \left({\mathbb{E}_{P_{W\mathcal{S}_n}}}[\hat{\mathcal{R}}(W,\mathcal{S}_n)]\right) + \frac{1}{n}\sum_{i=1}^{n}\left(\frac{I(W;Z_i)}{c\eta'} + \frac{\epsilon}{c} \right).
\end{align}
In particular, if $v(\epsilon) = \epsilon^{1-\beta}$ for some $\beta \in [0,1]$, then by choosing $\eta' = v(\epsilon)$ and $\frac{I(W;Z_i)}{c\eta'} + \frac{\epsilon}{c}$ is optimized when $\epsilon = I(W;Z_i)^{\frac{1}{2-\beta}}$ and the bound becomes,
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] \leq (\frac{1}{c} - 1) \left({\mathbb{E}_{P_{W\mathcal{S}_n}}}[\hat{\mathcal{R}}(W,\mathcal{S}_n)]\right) + \frac{2}{nc}\sum_{i=1}^{n} I(W;Z_i)^{\frac{1}{2-\beta}},
\end{align}
which completes the proof.
\end{proof}


% \subsection{Proof of~Theorem~\ref{thm:bern-stein}}
% \begin{proof}
% The proof procedures are sketched as follows. 
% \begin{itemize}
%     \item Step 1: Rewriting excess risk and empirical excess risk by:
%     \begin{align*}
%         \mathcal{R}(w) &= \mathbb{E}_{z\sim \mu}[\ell(w,z)] - \mathbb{E}_{z\sim \mu}[\ell(w^*,z)] \\
%         &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{Z_i\sim \mu}[\ell(w,Z_i)] - \mathbb{E}_{Z_i \sim \mu}[\ell(w^*,Z_i)] \\
%         &= \mathbb{E}_{\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)]
%     \end{align*}
%     and 
%     \begin{align*}
%         \hat{\mathcal{R}}(w, \mathcal{S}_n) &=  \hat{L}(w,\mathcal{S}_n) - \hat{L}(w^*,\mathcal{S}_n) 
%     \end{align*}
%     Given any $\mathcal{S}_n$, the gap between the excess risk and empirical excess risk can be written as,
%     \begin{align*}
%     \mathcal{R}(w) - \hat{\mathcal{R}}(w, \mathcal{S}_n) = \mathbb{E}_{\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)] - \hat{\mathcal{R}}(w, \mathcal{S}_n)
%     \end{align*}
%     We will bound the above quantity by taking the expectation w.r.t. $w$ learned from $\mathcal{S}_n$ by: 
%     \begin{align}
%         \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] &= \mathbb{E}_{W\mathcal{S}_n}[\mathcal{R}(w) - \hat{\mathcal{R}}(w, \mathcal{S}_n)] \\
%         &=  \mathbb{E}_{W\otimes \mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)] - \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)] \\
%         &= \frac{1}{n}\sum_{i=1}^n \mathbb{E}_{W \otimes \mu}[r(W,Z_i)] - \mathbb{E}_{WZ_i}[r(W,Z_i)] 
%     \end{align}

%     \item Step 2: Recall that the variational representation of the KL divergence between two distributions $P$ and $Q$ defined over $\mathcal X$ is given as (see, e. g. \cite{boucheron_concentration_2013})
% \begin{align*}
% D(P||Q)=\sup_{f}\{\Esub{P}{f(X)}-\log\Esub{Q}{e^{f(x)}} \}
% %\label{eq:variational}
% \end{align*}
% where the supremum is taken over all measurable functions such that $\Esub{Q}{e^{f(x)}}$ exists. Then let $f(w,z_i) = -\eta r(w,z_i)$, we have,
% \begin{align}
%     D(P_{WZ_i}\|P_{W}\otimes P_{Z_i}) &\geq \Esub{P_{WZ_i}}{-\eta r(W,Z_i)} - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}] \nonumber \\
%     &= \Esub{P_{WZ_i}}{-\eta r(W,Z_i)} - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i) - \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)])  }] + \Esub{P_W \otimes \mu}{\eta r(W,Z_i)} \nonumber \\
%     &= \eta\left(\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)}\right) - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }] \label{eq:MI_KL}
% \end{align}
% Next we will upper bound the second term $\log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }]$ in R.H.S. using Bernstein's condition.

% \item Step 3: Using Lemma \ref{lemma:exp_bern}, assume $r(w,z_i)$ is lower bounded by $-b$ almost surely, then we have for all $0< \eta < \frac{1}{b}$ and all $c > \frac{e^{\eta b} - \eta b - 1}{\eta^2b^2} > 0$ , we have
% \begin{align}
%     e^{\eta(\Esub{P_{W}\otimes \mu}{r(W,Z_i)} - r(W,Z_i))} \leq e^{\eta^2 c \Esub{P_{W}\otimes \mu}{r^2(W,Z_i)}} \label{eq:rsquare}
% \end{align}
% We can continue to upper bound the expectation of $r^2(W,Z_i)$ using Proposition 4.

% \item Step 4: Using Bernstein's condition in Proposition 4, we have that for $\beta^* \in [0,1]$, any $c>0$ and $\eta < \frac{1}{2Bc}$, then for all $0 < \beta < \beta^*$ and all $w \in \mathcal{W}$,
% \begin{align}
%     c\eta \Esub{\mu}{r^2(w,Z_i)} \leq \left(\frac{1}{2} \wedge \beta\right) \cdot\left({\mathbb{E}_{\mu}}[r(w,Z_i)]\right)+(1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}}
% \end{align}
% Take expectation w.r.t. $P_W$ and multiply by $\eta$ on both sides, we arrive at,
% \begin{align}
%     \eta^2c \Esub{P_W \otimes \mu}{r^2(w,Z_i)} \leq \left(\frac{1}{2} \wedge \beta\right) \eta  \left({\mathbb{E}_{P_W\otimes \mu}}[r(w,Z_i)]\right)+(1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}}\eta 
% \end{align}
% Hence the equation~(\ref{eq:rsquare}) can be further bounded by,
% \begin{align}
%     e^{\eta(\Esub{P_{W}\otimes \mu}{r(W,Z_i)} - r(W,Z_i))} \leq e^{\left(\frac{1}{2} \wedge \beta\right) \eta  \left({\mathbb{E}_{P_W \otimes \mu}}[r(w,Z_i)]\right)+(1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}}\eta } 
%     \label{bound:bernstein}
% \end{align}
% \item Step 5: Substitute (\ref{bound:bernstein}) into (\ref{eq:MI_KL}), we arrive at,
% \begin{align}
%     I(W;Z_i) \geq \eta \left(\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)}\right) - \left(\frac{1}{2} \wedge \beta\right) \eta  \left({\mathbb{E}_{P_W\otimes \mu}}[r(w,Z_i)]\right)+(1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}}\eta
% \end{align}
% Divide $\eta$ on both side, we arrive at,
% \begin{align}
%     \frac{I(W;Z_i)}{\eta} \geq\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} - \left(\frac{1}{2} \wedge \beta\right)\left({\mathbb{E}_{P_W\otimes \mu}}[r(w,Z_i)]\right) - (1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}}
% \end{align}
% Let $\alpha =  \left(\frac{1}{2} \wedge \beta\right) \leq \frac{1}{2}$, we have the bound,
% \begin{align}
%     \alpha\Esub{P_W \otimes \mu}{r(W,Z_i)} \leq 2\alpha \Esub{P_{WZ_i}}{r(W,Z_i)} + 2\alpha (1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}} + \frac{2\alpha I(W;Z_i)}{\eta}
% \end{align}
% Therefore,
% \begin{align}
%     \Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} \leq \left(1 \wedge 2\beta\right)\left({\mathbb{E}_{P_{WZ_i}}}[r(w,Z_i)]\right) + 2\left( \left(\frac{\eta}{\eta_{\max}}\right)^{\frac{1}{1-\beta}} + \frac{I(W;Z_i)}{\eta} \right)
% \end{align}
% with $\eta_{\max} = \min\left(\frac{1}{4}, \frac{1}{2 B C_{1 / 4}}\right)$ simplifying the notation. We can then minimise the R.H.S. by choosing appropriate $\eta$, by differential, we know that 
% \begin{align*}
% \min_{\eta} \left( \left(\frac{\eta}{\eta_{\max}}\right)^{\frac{1}{1-\beta}} + \frac{I(W;Z_i)}{\eta} \right) = 2\left(\frac{I(W;Z_i)}{\eta_{\max}}\right)^{\frac{1}{2-\beta}}
% \end{align*}
% with the choice of $\eta = I(W;Z_i)^{\frac{1-\beta}{2-\beta}}\eta^{\frac{1}{2-\beta}}_{\max}$ if $\eta < \eta_{\max}$.  Or 
% \begin{align*}
% \left( \left(\frac{\eta}{\eta_{\max}}\right)^{\frac{1}{1-\beta}} + \frac{I(W;Z_i)}{\eta} \right) \leq \frac{2I(W;Z_i)}{\eta_{\max}}
% \end{align*}
% if $\eta = \eta_{\max}$. Therefore,
% \begin{align}
%     \Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} \leq \left(1 \wedge 2\beta\right)\left({\mathbb{E}_{P_{WZ_i}}}[r(w,Z_i)]\right) + 4\left(\frac{I(W;Z_i)}{\eta_{\max}}\right)^{\frac{1}{2-\beta}}_{[**]}
% \end{align}
% \item Step 6: summing up every term for $Z_i$ and divide by $n$, we end up with,
% \begin{align}
%     \Esub{P_W \otimes P_{\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} - \Esub{P_{W\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} \leq & \left(1 \wedge 2\beta\right)\left({\mathbb{E}_{P_{W\mathcal{S}_n}}}[\hat{\mathcal{R}}(W,\mathcal{S}_n)]\right) + \frac{4}{n}\sum_{i=1}^{n}\left(\frac{I(W;Z_i)}{\eta_{\max}}\right)^{\frac{1}{2-\beta}}_{[**]}
% \end{align}
% Therefore,
% \begin{align}
%     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] \leq (1 \wedge 2 \beta) \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)] + \frac{4}{n} \sum_{i=1}^{n} \left(\frac{I(W;Z_i)}{\eta_{\max}}\right)^{\frac{1}{2-\beta}}_{[**]},
% \end{align}
% which completes the proof.
% \end{itemize}

% \end{proof}


% \subsection{Proof of~Theorem~\ref{thm:central}}

% \begin{proof}
% The proof procedures are sketched as follows. 
% \begin{itemize}
%     \item Step 1: Rewriting excess risk and empirical excess risk by:
%     \begin{align*}
%         \mathcal{R}(w) &= \mathbb{E}_{Z\sim \mu}[\ell(w,Z)] - \mathbb{E}_{Z \sim \mu}[\ell(w^*,Z)] \\
%         &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{Z_i\sim \mu}[\ell(w,Z_i)] - \mathbb{E}_{Z_i \sim \mu}[\ell(w^*,Z_i)] \\
%         &= \mathbb{E}_{\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)]
%     \end{align*}
%     and 
%     \begin{align*}
%         \hat{\mathcal{R}}(w, \mathcal{S}_n) &=  \hat{L}(w,\mathcal{S}_n) - \hat{L}(w^*,\mathcal{S}_n) 
%     \end{align*}
%     Given any $\mathcal{S}_n$, the gap between the excess risk and empirical excess risk can be written as,
%     \begin{align*}
%     \mathcal{R}(w) - \hat{\mathcal{R}}(w, \mathcal{S}_n) = \mathbb{E}_{\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)] - \hat{\mathcal{R}}(w, \mathcal{S}_n)
%     \end{align*}
%     We will bound the above quantity by taking the expectation w.r.t. $w$ learned from $\mathcal{S}_n$ by: 
%     \begin{align}
%         \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] &= \mathbb{E}_{W\mathcal{S}_n}[\mathcal{R}(W) - \hat{\mathcal{R}}(W, \mathcal{S}_n)] \\
%         &=  \mathbb{E}_{W\otimes \mathcal{S}_n}[\hat{\mathcal{R}}(W, \mathcal{S}_n)] - \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(W, \mathcal{S}_n)] \\
%         &= \frac{1}{n}\sum_{i=1}^n \mathbb{E}_{W \otimes \mu}[r(W,Z_i)] - \mathbb{E}_{WZ_i}[r(W,Z_i)] 
%     \end{align}

%     \item Step 2: Recall that the variational representation of the KL divergence between two distributions $P$ and $Q$ defined over $\mathcal X$ is given as (see, e. g. \cite{boucheron_concentration_2013})
% \begin{align}
% D(P||Q)=\sup_{f}\{\Esub{P}{f(X)}-\log\Esub{Q}{e^{f(x)}} \}
% %\label{eq:variational}
% \end{align}
% where the supremum is taken over all measurable functions such that $\Esub{Q}{e^{f(x)}}$ exists. Then let $f(w,z_i) = -\eta r(w,z_i)$, we have,
% \begin{align}
%     D(P_{WZ_i}\|P_{W}\otimes P_{Z_i}) &\geq \Esub{P_{WZ_i}}{-\eta r(W,Z_i)} - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}]  
% \end{align}
% Rewrite the above inequality as,
% \begin{align}
%     \frac{I(W;Z_i)}{\eta} + \Esub{P_{WZ_i}}{r(W,Z_i)} &\geq  - \frac{1}{\eta} \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}]  \label{eq:MI}
% \end{align}
% The R.H.S. is the annealed excess risk under the distribution $P_W \otimes \mu$, we will bound this term using Lemma 13 in \cite{Grunwald2020}.

% \item Step 3: We first present Lemma 13 in \cite{Grunwald2020} for bounding the annealed excess risk.
% \begin{lemma}
% Let $\bar{\eta}>0$. Assume that the $\bar{\eta}$-strong central condition holds and let, for arbitrary $0 < \eta < \bar{\eta}, c_{u}:=\frac{1}{c} \frac{\eta u+1}{1-\frac{\eta}{\bar{\eta}}}$. Suppose further that the $(u, c)$-witness condition holds for $u>0$ and $c \in(0,1]$. Then for all $w \in \mathcal{W}$, all $\eta \in(0, \bar{\eta})$:
% \begin{equation}
% \mathbb{E}_{ \mu}\left[r(W,Z)\right] \leq c_{u} \cdot \mathbb{E}_{ \mu}^{\mathrm{HE}(\eta)}\left[r(W,Z)\right] \leq c_{u} \cdot \mathbb{E}_{ \mu}^{\mathrm{ANN}(\eta)}\left[r(W,Z)\right] .
% \end{equation}
% %More generally, suppose that the $\bar{\eta}$-central condition and the $(\tau, c)$-witness condition hold for $c \in(0,1]$ and a non-increasing function $\tau$. Then for all $\lambda>0$, all $w \in \mathcal{W}$,
% %\begin{equation}
% %\mathbb{E}_{\mu}\left[r(W,Z)\right] \leq \lambda \vee\left(c_{\tau(\lambda)} \cdot \mathbb{E}_{ \mu}^{\mathrm{HE}(\eta)}\left[r(W,Z)\right] \right) \leq \lambda \vee\left(c_{\tau(\lambda)} \cdot \mathbb{E}_{\mu}^{\mathrm{ANN}(\eta)}\left[r(W,Z)\right] \right) .
% %\end{equation}
% \end{lemma}
% The proof of the above lemma can be found in Appendix C.1 (page 48) in \cite{Grunwald2020}. Now under the strong central condition and witness condition, by taking the expectation over $P_{W}$, we have,
% \begin{equation}
% \mathbb{E}_{P_W \otimes \mu}\left[r(W,Z)\right] \leq  \frac{c_{u}}{\eta} \left( 1- \mathbb{E}_{ P_W \otimes \mu}[e^{-\eta r(W,Z_i)}] \right) \leq - \frac{c_{u}}{\eta}  \log \mathbb{E}_{ P_W \otimes \mu}\left[e^{-\eta r(W,Z)}\right]. 
% \end{equation}
% With this inequality, (\ref{eq:MI}) can be further bounded by,
% \begin{align}
%     \frac{I(W;Z_i)}{\eta} + \Esub{P_{WZ_i}}{r(W,Z_i)} &\geq  - \frac{1}{\eta} \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}]  \\
%     &\geq \frac{1}{c_u}\mathbb{E}_{P_W \otimes \mu}\left[r(W,Z_i)\right]
% \end{align}
% Therefore, rearrange the above equation,
% \begin{align}
%     \mathbb{E}_{P_W \otimes \mu}\left[r(W,Z_i)\right] \leq \frac{c_u}{\eta} I(W;Z_i) + c_u \cdot \Esub{P_{WZ_i}}{r(W,Z_i)}
% \end{align}
% \item Step 4: Summing up every term for $Z_i$, we have,
% \begin{align}
%     \mathbb{E}_{P_W \otimes P_{\mathcal{S}_n}}\left[\sum_{i=1}^n r(W,Z_i)\right] \leq \frac{c_u}{\eta} \sum_{i=1}^{n} I(W;Z_i) + c_u \cdot \Esub{P_{W\mathcal{S}_n}}{\sum_{i=1}^n r(W,Z_i)}
% \end{align}
% Divide by $n$ for both sides, we have,
% \begin{align}
%     \mathbb{E}_{P_W \otimes P_{\mathcal{S}_n}} \left[\hat{\mathcal{R}}(W,\mathcal{S}_n)\right] \leq \frac{c_u}{\eta} \frac{1}{n}\sum_{i=1}^{n} I(W;Z_i) + c_u \cdot \Esub{P_{W\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)}
% \end{align}
% since $c_u > 1$, we have,
% \begin{align}
%     \mathbb{E}_{P_W \otimes P_{\mathcal{S}_n}} \left[\hat{\mathcal{R}}(W,\mathcal{S}_n)\right]   - \Esub{P_{W\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} \leq \frac{c_u}{\eta} \frac{1}{n}\sum_{i=1}^{n} I(W;Z_i) + (c_u - 1) \cdot \Esub{P_{W\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)}
% \end{align}
% which completes the proof.
% \end{itemize}
% \end{proof}


%\subsection{Proof of~Theorem~\ref{thm:subgaussian}}
%\begin{proof}
%Before proving~Theorem~\ref{thm:subgaussian}, we firstly prove~Lemma~\ref{lemma:alpha}. Consider individual terms in the difference, we have
%\begin{equation}\mathbb{E}_{WZ_i}\left[\lambda\left(\alpha \mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)]-r\left(W, Z_{i}\right)\right)\right] \leq I\left(W ; Z_{i}\right)+ \log \mathbb{E}_{P_{W} P_{Z_{i}}}\left[e^{\lambda\left(\alpha\mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)] -r\left(W, Z_{i}\right)\right)}\right]    
% \end{equation}
% The second term on the RHS can be upper bounded as
% \begin{align}
% \log \mathbb{E}_{P_{W} P_{Z_{i}}}\left[e^{\lambda\left(\alpha \mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)]-r\left(W, Z_{i}\right)\right)}\right] & = \log \mathbb{E}_{P_{W} P_{Z_{i}}}\left[e^{\lambda\alpha\left(\mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)]-r\left(W, Z_{i}\right)\right)-\lambda(1-\alpha) r(W,Z_i)}\right] \\
% & \overset{(a)}{\leq} \log \mathbb{E}_{P_{W}P_{Z_i}}\left[e^{\frac{\lambda^{2}\alpha^2 \sigma^{2}}{2}-\lambda(1-\alpha) r(W,Z_i)}\right] \\
% & {=} \frac{\lambda^{2} \alpha^2 \sigma^{2}}{2} + \log \mathbb{E}_{P_{W}P_{Z_i}}\left[e^{-\lambda(1-\alpha) r(W,Z_i)}\right] \\
% & \leq \frac{\lambda^{2} \alpha^2 \sigma^{2}}{2} - \lambda (1-\alpha) \epsilon
% \end{align}
% %By setting $\lambda=\frac{2(1-\alpha) m}{\sigma^{2}}>0$, the exponent is equal to zero. 
% where $(a)$ follows the sub-Gaussian assumption and $(b)$ follows the ESI assumption. This also shows that
% \begin{align}
% \mathbb{E}_{P_{WZ_i}}\left[\alpha \mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)]- r\left(W, Z_{i}\right)\right] \leq \frac{1}{\lambda} I\left(W ; Z_{i}\right) + \frac{\lambda \alpha^2 \sigma^2}{2} - (1-\alpha) \epsilon
% \end{align}
% By setting $\lambda = \frac{2(1-\alpha)\epsilon}{\alpha^2\sigma^2}$, we have,
% \begin{align}
% \mathbb{E}_{P_{WZ_i}}\left[\alpha \mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)] - r\left(W, Z_{i}\right)\right] \leq \frac{\alpha^2\sigma^2}{2(1-\alpha)\epsilon} I\left(W ; Z_{i}\right)
% \end{align}
% Summing all terms gives the claimed result for Lemma~\ref{lemma:alpha}. Then by decomposing the quantity of interest as
% \begin{align}
%     \alpha \mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)] - \hat{\mathcal{R}}(W, \mathcal{S}_n) = \alpha (\mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)] -  \hat{\mathcal{R}}(W, \mathcal{S}_n)) - (1-\alpha)\hat{\mathcal{R}}(W, \mathcal{S}_n)),
% \end{align}
% we end up with the following inequality:
% \begin{align}
%      \alpha\mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq & (1-\alpha) \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] + \frac{1}{n} \sum_{i=1}^{n} \frac{\alpha^2 \sigma^{2}}{2 (1-\alpha) \epsilon} I\left(W ; Z_{i}\right).
% \end{align}
% Dividing $\alpha$ on both L.H.S. and R.H.S., we end up with the results. 
% \end{proof}


%\subsection{Proof of~Theorem~\ref{thm:reg_erm}}
%\begin{proof}
%Consider individual terms in the difference, we have
%\begin{align}
%    \mathbb{E}_{P_{WZ_i}}\left\{t\left(L_{\mu}(W)-(1+\lambda)\ell\left(W, Z_{i}\right)\right)\right\} \leq I\left(W ; Z_{i}\right)+\mathbb{E}_{P_{W} P_{Z_{i}}}\left\{e^{t\left(L_{\mu}(W)-(1+\lambda)\ell\left(W, Z_{i}\right)\right)}\right\}
%\end{align}
%The second term on the RHS can be upper bounded as
%\begin{align*}
%\mathbb{E}_{P_{W} P_{Z_{i}}}\left\{e^{t\left(L_{\mu}(W)-(1+\lambda)\ell\left(W, Z_{i}\right)\right)}\right\} &=\mathbb{E}_{P_{W} P_{Z_{i}}}\left\{e^{t\left(L_{\mu}(W)-\ell\left(W, Z_{i}\right)\right)-t \lambda \ell(W,Z_i)}\right\} \\
%& \leq \mathbb{E}_{P_{W}P_{Z_i}}\left\{e^{\frac{t^{2} \sigma^{2}}{2}-\lambda t \ell(W,Z_i)}\right\} \\
%& \leq \mathbb{E}_{P_{W}}\left\{e^{\frac{t^{2} \sigma^{2}}{2}-\lambda t L}\right\}
%\end{align*}
%By setting $t=\frac{2\lambda L}{\sigma^{2}}>0$, the exponent is equal to zero. This also shows that
%\begin{align}
%  \mathbb{E}_{P_{WZ_i}}\left\{L_{\mu}(W)- (1+\lambda) \ell\left(W, Z_{i}\right)\right\} \leq \frac{\sigma^{2}}{2\lambda L} I\left(W ; Z_{i}\right) \label{eq:1}
%\end{align}

%Let $D_R(W,S) = \Esub{WS}{\hat L(W,S) - R(W,S)}$, summing all terms gives the claimed result.
%\end{proof}

% \subsection{Proof of Fast Rate with Gibbs Algorithm}
% We assume that the hypothesis is endowed with some prior distribution, namely, $\pi(W)$. The Gibbs algorithm defines a distribution $P_{W|SS'}$ over the hypothesis space $\mathcal{W}$ given the data sample as follows.
% \begin{align}
%     P_{W|S}(w) = \frac{\pi(w)e^{-\eta \hat L(w,S)}}{V(S,\eta)}, \label{eq:gibbs}
% \end{align}
% where $\eta$ is the inverse temperature, $\pi(w)$ denotes the prior distribution over $W$ and $V(S,\eta) = \int_{\mathcal{W}}\pi(w)e^{-\eta\hat L(w,S)} dw$ is the normalization term.
% \begin{remark}
% The Gibbs algorithm can be viewed as a stochastic empirical risk minimization algorithm (ERM). Roughly speaking, if one hypothesis $w$ incurs a lower empirical loss, the density for that certain $w$ will increase, depending on the factor $\eta$. When the training data overwhelms the prior, the posterior will converge to the neighbourhood of the ERM solution. As a special case, the Gibbs algorithm coincides with the standard ERM when $\eta$ goes to infinity \cite{aminian2021exact}.
% \end{remark}  
% We have the following theorem for the Gibbs algorithm in terms of the fast rate generalization error.
% \begin{theorem}
% Consider any loss function $\ell: \mathcal{W} \times \mathcal{Z} \rightarrow \mathbb{R}$ that is $\sigma$-subGaussian in the first argument with respect to the Gibbs density
% \begin{align}
%   P_{W|S}(w) \propto e^{-\eta \hat{L}(w,S)}, \quad \eta>0
% \end{align}
% conditioned on a measurable $\mathcal{W}$. Then the generalization error of Gibbs-ERM satisfies
% \begin{align}
%     \mathbb{E}_{S}\mathbb{E}_{P_{W|S}} \left[L(W)-\hat{L}(W,S) \right]  \leq \frac{4 \sigma^{2} \eta}{n}
% \end{align}
% \end{theorem}

% \begin{proof}
% Let $S^{(i)} = (Z_1,Z_2,\cdots, Z_{i-1}, Z, Z_{i+1}, \cdots, Z_n)$ denote the data sample that $i$-th entry is replaced by $Z$ in the dataset $S$. We start by rewriting the expected generalization error as,
% \begin{align}
%     \mathbb{E}_{S}\mathbb{E}_{P_{W|S}} \left[L(W)-\hat{L}(W,S) \right] &= \frac{1}{n}\sum_{i=1}^{n} \left({\mathbb{E}}_{{S, Z}}\mathbb{E}_{P_{W|S}}\left[\ell \left(W, Z\right)\right] -{\mathbb{E}}_{S}\left[{\mathbb{E}}_{P_{W|S}}\left[\ell \left(W, Z_{i}\right)\right]\right]\right) \\
%   \textup{ (Switching $Z$ and $Z_i$) }&= \frac{1}{n}\sum_{i=1}^{n} \left({\mathbb{E}}_{{S, Z}}\mathbb{E}_{P_{W|S^{(i)}}}\left[\ell \left(W, Z_i\right)\right] -{\mathbb{E}}_{S}\left[{\mathbb{E}}_{P_{W|S}}\left[\ell \left(W, Z_{i}\right)\right]\right]\right) \\
%     &= \frac{1}{n}\sum_{i=1}^{n} {\mathbb{E}}_{{S, Z}} \left[ \mathbb{E}_{P_{W|S^{(i)}}}\left[\ell \left(W, Z_i\right)\right] - \mathbb{E}_{P_{W|S}}\left[\ell \left(W, Z_i\right)\right] \right] \\
%   \textup{ (Subgaussian Assumption) }  &\leq  \frac{1}{n}\sum_{i=1}^{n} {\mathbb{E}}_{{S, Z}} \sqrt{2\sigma^2 D(P_{W|S^{(i)}} \| P_{W|S})} \\
%   \textup{ (Jensen's Inequality) }  &\leq \frac{1}{n}\sum_{i=1}^{n} \sqrt{2\sigma^2 {\mathbb{E}}_{{S, Z}} [D(P_{W|S^{(i)}} \| P_{W|S})] } \\
%   &= \frac{1}{n}\sum_{i=1}^{n} \sqrt{2\sigma^2  D(P_{W|S^{(i)}} \| P_{W|S} |S,Z) } 
% \end{align}
% Next we will explicitly write out the conditional KL divergence as,
% \begin{align}
%     D(P_{W|S^{(i)}} \| P_{W|S} |S,Z) &= \mathbb{E}_{S,Z} \left[\eta \left( \mathbb{E}_{P_{W|S^{(i)}}}[\hat{L}(W,S)] -  \mathbb{E}_{P_{W|S^{(i)}}}[\hat{L}(W,S^{(i)})] \right) - \log \frac{\int \pi(w)e^{-\eta \hat{L}(W,S^{(i)})} dw}{\int \pi(w) e^{-\eta \hat{L}(W,S)} dw} \right]  \\
%     &= \mathbb{E}_{S,Z} \left[\eta \left( \mathbb{E}_{P_{W|S^{(i)}}}[\hat{L}(W,S)] -  \mathbb{E}_{P_{W|S^{(i)}}}[\hat{L}(W,S^{(i)})] \right) - \log \mathbb{E}_{P_{W|S}}[e^{\eta(\hat{L}(W,S) - \hat{L}(W,S^{(i)}))}]\right]  \\
%     \textup{ (Jensen's Inequality) } & \leq \mathbb{E}_{S,Z} \left[\eta \left( \mathbb{E}_{P_{W|S^{(i)}}}[\hat{L}(W,S)] -  \mathbb{E}_{P_{W|S^{(i)}}}[\hat{L}(W,S^{(i)})] \right) + \mathbb{E}_{P_{W|S}}[\eta(\hat{L}(W,S^{(i)}) - \hat{L}(W,S))]\right] \\
%     &= \mathbb{E}_{S,Z} \left[\frac{\eta}{n} \left( \mathbb{E}_{P_{W|S^{(i)}}}[\ell(W,Z_i)] -  \mathbb{E}_{P_{W|S^{(i)}}}[\ell(W,Z)] \right) + \frac{\eta}{n}\mathbb{E}_{P_{W|S}}[\ell(W,Z) - \ell(W,Z_i)]\right] \\
%   \textup{ (Switching $Z$ and $Z_i$) } &= \mathbb{E}_{S,Z} \left[\frac{2\eta}{n} \left( \mathbb{E}_{P_{W|S}}[\ell(W,Z)] -  \mathbb{E}_{P_{W|S}}[\ell(W,Z_i)] \right) \right]
% \end{align}
% Therefore,
% \begin{align}
%     {\mathbb{E}}_{{S, Z}}\left[ \mathbb{E}_{P_{W|S}}\left[\ell \left(W, Z\right)\right] -\left[{\mathbb{E}}_{P_{W|S}}\left[\ell \left(W, Z_{i}\right)\right]\right] \right] \leq \sqrt{\frac{4\sigma^2\eta}{n} {\mathbb{E}}_{{S, Z}}\left[ \mathbb{E}_{P_{W|S}}\left[\ell \left(W, Z\right)\right] -\left[{\mathbb{E}}_{P_{W|S}}\left[\ell \left(W, Z_{i}\right)\right]\right] \right] }
% \end{align}
% As a consequence, for each $Z_i$, 
% \begin{align}
%     {\mathbb{E}}_{{S, Z}}\left[ \mathbb{E}_{P_{W|S}}\left[\ell \left(W, Z\right)\right] -\left[{\mathbb{E}}_{P_{W|S}}\left[\ell \left(W, Z_{i}\right)\right]\right] \right] \leq \frac{4\sigma^2\eta}{n}
% \end{align}
% which completes the proof.
% \end{proof}




% \begin{align*}
%     \log \mathbb{E}\left[ e^{\eta(\mathbb{E}[r(W,Z)] - r(W,Z))}\right] \leq \eta^2 \sigma^2
% \end{align*}

% \begin{align*}
%     \log \mathbb{E}\left[ e^{\eta(\mathbb{E}[r(W,Z)] - r(W,Z))}\right] \leq k \eta^2  \mathbb{E}[r(W,Z)]
% \end{align*}

% \subsection{Proof of Theorem~\ref{thm:subgaussian_aux}}\label{proof:subgaussian_aux}
% \begin{proof}
% Let us first rewrite the expected generalization error as follows.
% \begin{align*}
%     \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{WZ_i}[\ell(W,Z_i)] - \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] \\
%     &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{WZ_i}[\ell(W,Z_i)] - \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] \\
%     &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{WZ_i}[\ell(W,Z_i)] - \mathbb{E}_{\hat{W}Z_i}[\ell(\hat{W},Z_i)] - (\mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] - \mathbb{E}_{\hat{W}Z_i}[\ell(\hat{W},Z_i)]) \\
%     &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{W\hat{W}Z_i}[\ell(W,Z_i) - \ell(\hat{W}, Z_i)] - \mathbb{E}_{W\otimes {\hat{W}Z_i}}[\ell(W,Z_i) - \ell(\hat{W},Z_i)]
% \end{align*}
% where $P_{W\hat{W}Z_i}$ is some distribution whose marginals are $P_{WZ_i}$ and $P_{\hat{W}Z_i}$.
% \end{proof}


\subsection{Calculation Details}\label{apd:cal}
In this section, we present the calculation details of the Gaussian mean estimation case. Let us consider the 1D-Gaussian mean estimation problem. Let $\ell(w,z_i) = (w-z_i)^2$, each sample is drawn from some Gaussian distribution, i.e., $Z_i \sim \mathcal{N}(\mu, \sigma_N^2)$. Then the ERM algorithm arrives at,
\begin{equation}
 W_{\ERM} = \frac{1}{n} \sum_{i=1}^{n} Z_i \sim \mathcal{N}(\mu, \frac{\sigma_N^2}{n}).
\end{equation}
It can be easily calculated that the optimal $w^*$ satisfies:
\begin{equation}
    w^* = \argmin \Esub{Z}{\ell(w,Z)} =\argmin \Esub{Z}{(w-Z)^2} = \mu.
\end{equation}
Also it can be calculated that the expected excess risk is,
\begin{align}
    \Esub{W}{\mathcal{R}(W_\ERM)} &= \mathbb{E}_{P_W \otimes \mu}[\ell(W_\ERM,Z)] - \mathbb{E}_{Z}[\ell(w^*,Z)] \\
    &= \mathbb{E}_{P_W \otimes \mu}[(W_\ERM - Z)^2]  - \mathbb{E}_{Z}[(\mu - Z)^2]\\
    &=  \mu^2 + \frac{\sigma_N^2}{n} + \mu^2 + \sigma_N^2 - 2\mu^2 -  \mu^2 - \mu^2 -\sigma_N^2 + 2\mu^2 \\
    &= \frac{\sigma_N^2}{n}.
\end{align}
The corresponding empirical excess risk is given by,
\begin{align}
    \Esub{W\mathcal{S}_n}{\hat{\mathcal{R}}(W_\ERM,\mathcal{S}_n)} &= \Esub{W_\ERM \mathcal{S}_n}{\hat L(W_\ERM,\mathcal{S}_n) - \hat{L}(w^*,\mathcal{S}_n)} \\
            &=  \E{\frac{1}{n}\sum_{i=1}^{n}(W-Z_i)^2 - \frac{1}{n}\sum_{i=1}^{n}(\mu - Z_i)^2} \\
            % &=  \E{\frac{1}{n}\sum_{i=1}^{n}(W^2 - \mu^2) - \frac{2}{n}\sum_{i=1}^{n}WZ_i - \mu Z_i} \\
            &= \mu^2+ \frac{\sigma_N^2}{n} - \mu^2 - 2\mu^2 - \frac{2}{n} \sigma_N^2 + 2\mu^2 \\
            &= -\frac{\sigma_N^2}{n}.
\end{align}
Then it yields the expected generalization error as,
\begin{align}
    \Esub{W\mathcal{S}_n}{\mathcal{E}(W_\ERM, \mathcal{S}_n)} &= \Esub{W\mathcal{S}_n}{\mathcal{R}(W_\ERM)-\hat {\mathcal{R}}(W_\ERM,\mathcal{S}_n)} \\
    &= \frac{\sigma_N^2}{n} - (- \frac{\sigma_N^2}{n}) \\
    &= \frac{2\sigma_N^2}{n}.
\end{align}
The expected loss can be calculated as,
\begin{align}
    \mathbb{E}_{P_W \otimes \mu}[\ell(W_{\ERM},Z)] = \frac{n+1}{n}\sigma_N^2 := \sigma_W^2.
\end{align}
Let us verify the moment generating functions for the squared loss. Since $\ell(W_\ERM,Z)$ is $\sigma^2_W\chi^2_1$ distributed, 
\begin{align}
    \log \mathbb{E}_{P_W \otimes \mu}[e^{\eta (W_\ERM - Z)^2}] = -\frac{1}{2}\log(1- 2\sigma^2_W \eta).
\end{align}
Hence, 
\begin{align}
    \log \mathbb{E}_{P_W \otimes \mu}[e^{\eta \left((W_\ERM - Z)^2 - \mathbb{E}[(W_\ERM - Z)^2] \right)}] = -\frac{1}{2}\log(1- 2\sigma^2_W \eta) - \sigma^2_W\eta.
\end{align}
It is easily to prove that for any $x\leq 0$, 
\begin{align}
    -\frac{1}{2}\log(1-2x) - x \leq x^2,
\end{align}
which yields,
\begin{align}
    \log \mathbb{E}_{P_W \otimes \mu}[e^{\eta \left((W_\ERM - Z)^2 - \mathbb{E}[(W_\ERM - Z)^2] \right)}] = -\frac{1}{2}\log(1- 2\sigma^2_W \eta) - \sigma^2_W\eta \leq \sigma_W^4 \eta^2 
\end{align}
for any $\eta < 0$. Therefore, $\ell(W,Z)$ is $\sqrt{2\sigma^4_W}$-sub-Gaussian and we can only achieve the slow rate of $O(\sqrt{1/n})$.

Now we introduce $w^*$ in the sequel as a comparison. For a given $\eta$ and $w$, we calculate the moment generating function for the term $ r(w,Z) =(w - Z)^2 - (w^* - Z)^2$ as follows.
\begin{align}
 \mathbb{E}_{\mu}[e^{\eta r(w,Z)}] &= \frac{1}{\sqrt{2\pi \sigma_N^2}} \int  e^{-\frac{(z-\mu)^2}{2\sigma_N^2}}  e^{\eta ((w - z)^2 - (w^* - z)^2)}  dz \\
    %&= \frac{1}{\sqrt{2\pi \sigma_N^2}} \int  e^{-\frac{(z-\mu)^2}{2\sigma_N^2} + \eta ((w - z)^2 - (w^* - z)^2)}  dz \\
    %&= \frac{1}{\sqrt{2\pi \sigma_N^2}} \int  \operatorname{exp}\{-\frac{z^2 - 2\mu z + 4\eta \sigma_N^2 (w-\mu)z + \mu^2 }{2\sigma_N^2} - \eta (\mu^2 - w^2)\}  dz \\
    &= \frac{1}{\sqrt{2\pi \sigma_N^2}} \int  \operatorname{exp}\left(-\frac{(z - \mu + 2\eta \sigma_N^2(w-\mu))^2}{2\sigma_N^2}\right) dz \operatorname{exp}\left( -2\mu\eta (w - \mu) + 2\eta^2\sigma_N^2 (w-\mu)^2 - \eta (\mu^2 - w^2) \right) \\
    &= \operatorname{exp}\left( -2\mu\eta (w - \mu) + 2\eta^2\sigma_N^2 (w-\mu)^2 - \eta (\mu^2 - w^2) \right) \\
    &= \operatorname{exp}\left( (2\eta^2\sigma_N^2 + \eta)(w-\mu)^2 \right).
\end{align}
%Therefore, the annealed excess risk given any $w$ can be expressed as,
%\begin{align*}
%    -\frac{1}{\eta} \log m_\eta(-r(w,Z)) &= 2\mu(\mu - w) - 2\eta\sigma^2 (w-\mu)^2 + w^2 - \mu^2 \\
%    &= (1 - 2\eta\sigma^2) (w-\mu)^2
%\end{align*}
%while the true excess risk given any $w$ can be expressed as,
%\begin{align*}
%     \mathcal{R}(w) &= \mathbb{E}_{Z}[\ell(w,Z)] - \mathbb{E}_{Z}[\ell(w^*,Z)] \\
%     &= \mathbb{E}_{Z}[w^2 - \mu^2 + 2(\mu - w)Z] \\
%     &= w^2 - \mu^2 - 2w\mu + 2\mu^2 \\
%     &= (w-\mu)^2
%\end{align*}
Taking expectation over $w$ w.r.t. ERM solution, we have,
%\begin{align*}
%     \Esub{W}{\mathcal{R}(W_\ERM)} &=  \Esub{W}{(W-\mu)^2} \\
%     &= \frac{\sigma^2}{n}
%\end{align*}
%and,
\begin{align}
 \mathbb{E}_{P_W \otimes \mu}[e^{\eta r(W,Z)}] &= \Esub{W}{\operatorname{exp}\left( (2\eta^2\sigma_N^2 + \eta) (w-\mu)^2 \right)} \\
    &= \frac{1}{\sqrt{2\pi \frac{\sigma_N^2}{n}}} \int  \operatorname{exp}\left(-\frac{(w-\mu)^2}{2\sigma_N^2/n} + (2\eta^2\sigma_N^2 + \eta) (w-\mu)^2 \right)  dw \\
    %&= \frac{1}{\sqrt{2\pi \frac{\sigma_N^2}{n}}} \int  \operatorname{exp}\left( -\frac{(w-\mu)^2}{2} (\frac{n}{\sigma_N^2} - (4\eta^2\sigma_N^2 + 2\eta)) \right)  dw \\
    &= \frac{\sqrt{\frac{1}{\frac{n}{\sigma_N^2} - (4\eta^2\sigma_N^2 + 2\eta)}  \frac{n}{\sigma_N^2}} }{\sqrt{2\pi \frac{1}{\frac{n}{\sigma_N^2} - (4\eta^2\sigma_N^2 + 2\eta)}}} \int  \operatorname{exp}\left( -\frac{(w-\mu)^2}{2} (\frac{n}{\sigma_N^2} - (4\eta^2\sigma_N^2 + 2\eta)) \right)  dw \\
    &= \sqrt{\frac{n}{ n- (4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2)}}.
\end{align}
Therefore for any $\eta \in \mathbb{R}$ and any $n >  \max\left\{\frac{(4\eta^2\sigma^4_N+\eta\sigma^2_N)(2\eta^2\sigma^4_N+\eta\sigma^2_N)}{\eta^2\sigma^4_N}, 4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2\right\} $, we arrive at, 
\begin{align}
    \log  \mathbb{E}_{P_W \otimes \mu}[e^{\eta (r(W,Z) - \mathbb{E}[r(W,Z)]}] &= \frac{1}{2} \log \frac{n}{ n- (4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2)} - \frac{\eta\sigma_N^2}{n} \\
    & \leq \frac{1}{2} \frac{4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2}{n - 4\eta^2\sigma^4_N - 2\eta\sigma^2_N} - \frac{\eta\sigma_N^2}{n} \\
    & \leq \frac{4\eta^2\sigma_N^4}{n}. 
\end{align}
% Next we examine the Bernstein's condition, for any given $w$, we first calculate the second moment by,
% \begin{align*}
%     \mathbb{E}_{Z}[r(w,Z)^2] &= \mathbb{E}_{Z}[(w^2 - \mu^2 + 2(\mu - w)Z)^2] \\
%     &= (w^2 - \mu^2)^2 + 4(\mu - w)(w^2 - \mu^2)\mathbb{E}[Z] + 4(\mu - w)^2\mathbb{E}[Z^2] \\
%     &= (w - \mu)^2\left( (w+\mu)^2 - 4(w+\mu)\mu + 4\mu^2 + 4\sigma^2 \right) \\
%     &= (w - \mu)^4 + 4(w -\mu)^2\sigma^2
% \end{align*}
% while 
% \begin{align*}
%     \mathbb{E}_{Z}[r(w,Z)] &= (w-\mu)^2
% \end{align*}
% Taking expectation over $W_\ERM$, we have,
% \begin{align*}
%     \mathbb{E}_{P_W\otimes \mu}[r(W,Z)] &= \frac{\sigma^2}{n}
% \end{align*}
% and
% \begin{align*}
%     \mathbb{E}_{P_W\otimes \mu}[r(W,Z)^2] &= \frac{3\sigma^4}{n^2}+ \frac{4\sigma^4}{n}
% \end{align*}
which is of the order $O(\frac{1}{n})$ and we used the fact that $\frac{a+b}{n - 2a - 2b} \leq \frac{2a + b}{n}$ for $n > \max\left\{\frac{(2a+b)(2a+2b)}{a}, 2a + 2b\right\}$ with $a > 0$. The mutual information can be calculated as,
\begin{align}
    I(W_\ERM;Z_i) &= h(W_\ERM) - h(W_\ERM|Z_i)\\
    &= \frac{1}{2}\log \frac{2\pi e \sigma_N^2}{n} - \frac{1}{2}\log\frac{2\pi e (n-1)\sigma_N^2 }{n^2} \\
    & = \frac{1}{2}\log \frac{n}{n-1} \\
    & = O(\frac{1}{n}).
\end{align}

\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
    \hline 
     Quantity    &  Values/Distribution \\
     \hline 
     $\mathcal{S}_n$   &  $\{Z_1,Z_2,\cdots,Z_n\}$  \\
     $Z_i$     &     $\mathcal{N}(\mu,\sigma_N^2)$ \\
      $\ell(w,z)$   &  $(w-z)^2$ \\
      $\hat{L}(w,\mathcal{S}_n)$  & $\frac{1}{n}\sum_{i=1}^{n}\ell(w,z_i)$  \\ 
      $L(w)$  &  $\Esub{Z}{\ell(w,Z)}$ \\
      $W_\ERM$  & $\mathcal{N}(\mu,\frac{\sigma_N^2}{n})$    \\
      $w^*$   & $\mu$    \\
      $r(w,z)$  & $(w-z)^2 - (w^* - z)^2$  \\
      $\mathcal{R}(w)$  & $L(w) - L(w^*)$   \\ 
      $\hat{\mathcal{R}}(w,\mathcal{S}_n)$  & $\hat{L}(w) - \hat{L}(w^*)$    \\
      $\mathcal{E}(w,\mathcal{S}_n)$  & $L(w) - \frac{1}{n}\sum_{i=1}^{n}\ell(w,z_i)$   \\
      $M_{Z}[r(w,Z)]$ & $-\frac{1}{\eta} \log \mathbb{E}_Z\left[e^{-\eta r(w,Z)}\right]$   \\
      $M_{P_W\otimes \mu}[r(w,Z)]$ & $-\frac{1}{\eta} \log \mathbb{E}_{P_W\otimes \mu}\left[e^{-\eta r(W,Z)}\right]$   \\
      \hline 
      \hline
      $\Esub{W\mathcal{S}_n}{\hat{\mathcal{R}}(W_\ERM,\mathcal{S}_n)}$   &  $-\frac{\sigma_N^2}{n}$   \\
      $\Esub{W}{\mathcal{R}(W_\ERM)}/\mathbb{E}_{P_W\otimes \mu}[r(W,Z)]$   &  $\frac{\sigma_N^2}{n}$   \\
      $\Esub{W\mathcal{S}_n}{\mathcal{E}(W_\ERM, \mathcal{S}_n)}$ & $\frac{2\sigma_N^2}{n}$   \\
      $\mathcal{R}(w)/\mathbb{E}_{Z}[r(w,Z)]$  &  $(w-\mu)^2$    \\
      $\mathbb{E}_{Z}[e^{\eta r(w,Z)}]$ & $\operatorname{exp}\left( (2\eta^2\sigma_N^2 +\eta )(w-\mu)^2 \right)$   \\
      $\mathbb{E}_{P_W\otimes \mu}[e^{\eta r(W,Z)}]$ & $\sqrt{\frac{n}{ n- (4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2)}}$   \\
        $\mathbb{E}_{Z}[e^{-\eta r(w,Z)}]$ & $\operatorname{exp}\left( (2\eta^2\sigma_N^2 - \eta )(w-\mu)^2 \right)$   \\
      $\mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}]$ & $\sqrt{\frac{n}{ n- (4\eta^2\sigma_N^4 - 2 \eta\sigma_N^2)}}$   \\
      $M_{Z}[r(w,Z)]$ & $(1-2\eta\sigma_N^2)(w-\mu)^2$    \\
      $M_{P_W\otimes \mu}[r(w,Z)]$ & $(1-2\eta\sigma_N^2)\frac{\sigma_N^2}{n}$   \\
      $\mathbb{E}_{Z}[r(w,Z)^2]$   & $(w - \mu)^4 + 4(w -\mu)^2\sigma_N^2$    \\
      $\mathbb{E}_{P_W\otimes \mu}[r(W,Z)^2]$ & $\frac{3\sigma_N^4}{n^2}+ \frac{4\sigma_N^4}{n}$  \\
      $I(W;Z_i)$   &    $\frac{1}{2}\log\frac{n}{n-1}$ \\
      \hline 
    \end{tabular}
    \caption{Summarized Quantities}
    \label{tab:my_label}
\end{table}
We then summarize all the quantities of interest in Table~\ref{tab:my_label} for references. From the table, we can check to conclude that for most fast rate conditions such as Berstein's condition, central condition, and sub-Gaussian condition, the results will hold in expectation, but this is not the case for any $w \in \mathcal{W}$. To see this, we will check whether the condition is in succession.
\begin{itemize}
    \item When checking $\eta$-central condition, 
    \begin{itemize}
        \item For any $w$, 
        \begin{align}
            \mathbb{E}_{\mu}[e^{-\eta r(w,Z)}] = \operatorname{exp}\left( (2\eta^2\sigma_N^2 -\eta )(w-\mu)^2 \right) \leq 1, 
        \end{align}
        then we require $0 < \eta \leq \frac{1}{2\sigma_N^2}$.
        \item For $W_\ERM$,
        \begin{align}
        \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}]  = \sqrt{\frac{n}{ n- (4\eta^2\sigma_N^4 - 2 \eta\sigma_N^2)}} \leq 1 .
        \end{align}
        then we require $0 < \eta \leq \frac{1}{2\sigma_N^2}$.
    \end{itemize}
    \item When checking Bernstein's condition,
    \begin{itemize}
        \item For any $w\in \mathcal{W}$,
        \begin{align}
          \mathbb{E}_{\mu}[r(w,Z)^2]  &=  (w - \mu)^4 + 4(w -\mu)^2\sigma_N^2 \\
          & \leq B(\mathbb{E}_{ Z}[r(w,Z)])^{\beta} =  B(w-\mu)^{2\beta}.
        \end{align}
        Apparently, this does not hold for all $w \in \mathbb{R}$ when $\beta \in [0,1]$.
        \item For $W_\ERM$,
        \begin{align}
        \mathbb{E}_{P_W\otimes \mu}[r(W_\ERM,Z)^2]  &= \frac{3\sigma_N^4}{n^2}+ \frac{4\sigma_N^4}{n} \\
        &\leq B(\mathbb{E}_{P_W\otimes \mu}[r(W_\ERM,Z)])^{\beta} \\
        &=  B(\frac{\sigma_N^2}{n})^{\beta}.
        \end{align}
        This holds for $\beta = 1$ and $B = 7\sigma_N^2$.
    \end{itemize}
    \item When checking witness condition,
    \begin{itemize}
    \item For any $w \in \mathcal{W}$, we require that,
    \begin{align}
        \mathbb{E}_{Z}\left[\left(r(w,Z) \right) \cdot \mathbf{1}_{\left\{r(w,Z) \leq u \right\}}\right] &\geq c \mathbb{E}_{Z}\left[r(w,Z) \right] = c(w-\mu)^2.
    \end{align}
    There does not exist finite $c$ and $u$ that satisfy the above inequality, so the witness condition does not hold for all $w \in \mathbb{W}$.
    \item For $W_\ERM$,
    \begin{align}
        & \mathbb{E}_{P_W\otimes \mu}\left[\left(r(W_\ERM,Z) \right) \cdot \mathbf{1}_{\left\{r(W_\ERM,Z) \leq u \right\}}\right]   \geq c \mathbb{E}_{P_W\otimes \mu}\left[r(W,Z) \right] = \frac{c\sigma_N^2}{n}.
    \end{align}
    In this case, with high probability $r(W,Z)$ approaches zero and there exists $u$ and $c$ satisfying the above inequality.
    \end{itemize}
    \item When checking the sub-Gaussian condition,
    \begin{itemize}
    \item For $W_\ERM$, when $0 < \eta \leq \frac{1}{2\sigma_N^2}$, we have,
    \begin{align}
        & \log \mathbb{E}_{P_W\otimes \mu}\left[e^{-\eta \left( r(W_\ERM,Z) - \mathbb{E}[r(W_\ERM,Z)]\right)} \right]  \sim \frac{2\eta^2\sigma_N^4}{n}.
    \end{align}
    Then it satisfy with the $\sigma'^2$-sub-Gaussian condition that $\sigma'^2 = \frac{4\sigma_N^4}{n}$.
    \item For any $w$, 
     \begin{align}
          \log \mathbb{E}_{Z}[e^{\eta r(w,Z)}] = 2\eta^2\sigma_N^2(w-\mu)^2.
     \end{align}
     Since $w$ is unbounded, it does not satisfy the sub-Gaussian assumption for all $w\in\mathcal{W}$.
    \end{itemize}
    \item When checking the $(\eta,c)$-central condition, we confirmed that the learning tuple satisfies the $(\eta,c)$-central condition under $P_W\otimes \mu$ for the ERM algorithm. Now we consider the case for any hypothesis $w$.
    \begin{itemize}
        \item For any constant hypothesis $w$: we have that 
    \begin{align}
    \mathbb{E}_{\mu}[e^{-\eta r(w,Z)}]  = \operatorname{exp}\left( (2\eta^2\sigma_N^2 - \eta )(w-\mu)^2 \right),
    \end{align}
    where the excess risk can be calculated as:
    \begin{align}
    \mathbb{E}_{\mu}[r(w,Z)] = (w-\mu)^2.
    \end{align}
    Therefore, the $(\eta,c)$-central condition is satisfied as
    \begin{align}
    \log \mathbb{E}_{\mu}[e^{-\eta r(w,Z)}] =(2\eta^2\sigma_N^2 - \eta )(w-\mu)^2 \leq -c\eta (w-\mu)^2.
    \end{align}
    for any $\eta \leq \frac{1}{2\sigma^2_N}$ and any $c \leq 1 - 2\eta \sigma^2_N$. 
    \end{itemize}
\end{itemize}



%\section{Paper Format}

%\subsection{Templates}

%The paper (A4 or letter size, double-column format, not exceeding
%5~pages) should be formatted as shown in this sample \LaTeX{} file
%\cite{Laport:LaTeX, GMS:LaTeXComp, oetiker_latex, typesetmoser}.

%The use of Microsoft Word instead of \LaTeX{} is strongly
%discouraged. However, acceptable formatting may be achieved by using
%the template that can be downloaded from the ISIT 2019 website:
%\begin{center}
%  \url{http://isit2019.fr/}
%\end{center}

%Users of other text processing systems should attempt to duplicate the
%style of this example, in particular the sizes and type of font, as
%closely as possible.


%\subsection{Formatting}

%The style of references, equations, figures, tables, etc., should be
%the same as for the \emph{IEEE Transactions on Information
%  Theory}. The source file of this template paper contains many more
%instructions on how to format your paper. So, example code for
%different numbers of authors, for figures and tables, and references
%can be found (they are commented out).

%%%%%%
%% An example of a floating figure using the graphicx package.
%% Note that \label must occur AFTER (or within) \caption.
%% For figures, \caption should occur after the \includegraphics.
%%
% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.3\textwidth]{myfigure}
%   % where an .eps filename suffix will be assumed under latex,
%   % and a .pdf suffix will be assumed for pdflatex
%   \caption{Simulation results.}
%   \label{fig:sim}
% \end{figure}
%%%%%%

%%%%%%
%% An example of a double column floating figure using two subfigures.
%% (The subfigure.sty package must be loaded for this to work.)  The
%% subfigure \label commands are set within each subfigure command,
%% the \label for the overall figure must come after \caption.  
%% \hfil must be used as a separator to get equal spacing
%%
% \begin{figure*}[htbp]
%   \centerline{\subfigure[Case I]{\includegraphics[width=2.5in]{subfigcase1}
%       % where an .eps filename suffix will be assumed under latex,
%       % and a .pdf suffix will be assumed for pdflatex
%       \label{fig:first_case}}
%     \hfil
%     \subfigure[Case II]{\includegraphics[width=2.5in]{subfigcase2}
%       % where an .eps filename suffix will be assumed under latex,
%       % and a .pdf suffix will be assumed for pdflatex
%       \label{fig:second_case}}}
%   \caption{Simulation results.}
%   \label{fig:sim}
% \end{figure*}
%%%%%%

%%%%%%
%% An example of a floating table. 
%% Note that, for IEEE style tables, the \caption command should come
%% BEFORE the table. Table text will default to \footnotesize as IEEE
%% normally uses this smaller font for tables.  The \label must come
%% after \caption as always.
%%
% \begin{table}[htbp]
%   % increase table row spacing, adjust to taste
%   \renewcommand{\arraystretch}{1.3}
%   \caption{An Example of a Table}
%   \label{tab:table_example}
%   \centering
%   % Some packages, such as MDW tools, offer better commands for making tables
%   % than the plain LaTeX2e tabular which is used here.
%   \begin{tabular}{|c||c|}
%     \hline
%     One & Two\\
%     \hline
%     Three & Four\\
%     \hline
%   \end{tabular}
% \end{table}
%%%%%%


%For instructions on how to typeset math, in particular for equation
%arrays with broken equations, we refer to \cite{typesetmoser}.

%Pages should not be numbered and there should be no footer or header
%(both will be added during the production of the proceedings). The
%affiliation shown for authors should constitute a sufficient mailing
%address for persons who wish to write for more details about the
%paper.


%\subsection{PDF Requirements}

%Only electronic submissions in form of a PDF file will be
%accepted. The PDF file has to be PDF/A compliant. A common problem is
%missing fonts. Make sure that all fonts are embedded. (In some cases,
%printing a PDF to a PostScript file, and then creating a new PDF with
%Acrobat Distiller, may do the trick.) More information (including
%suitable Acrobat Distiller Settings) is available from the IEEE
%website \cite{IEEE:pdfsettings, IEEE:AuthorToolbox}.


%\section{Conclusion}

%We conclude by pointing out that on the last page the columns need to
%balanced. Instructions for that purpose are given in the source file.

%Moreover, example code for an appendix (or appendices) can also be
%found in the source file (they are commented out).

%%%%%%
%% Appendix:
%% If needed a single appendix is created by
%%
%\appendix
%%
%% If several appendices are needed, then the command
%%
% \appendices
%%
%% in combination with further \section-commands can be used.
%%%%%%


%\section*{Acknowledgment}

%We are indebted to Michael Shell for maintaining and improving
%\texttt{IEEEtran.cls}. 


%%%%%%
%% To balance the columns at the last page of the paper use this
%% command:
%%
%\enlargethispage{-1.2cm} 
%%
%% If the balancing should occur in the middle of the references, use
%% the following trigger:
%%
%\IEEEtriggeratref{3}
%%
%% which triggers a \newpage (i.e., new column) just before the given
%% reference number. Note that you need to adapt this if you modify
%% the paper.  The "triggered" command can be changed if desired:
%%
%\IEEEtriggercmd{\enlargethispage{-20cm}}
%%
%%%%%%


%%%%%%
%% References:
%% We recommend the usage of BibTeX:
%%
%\bibliographystyle{IEEEtran}
%\bibliography{definitions,bibliofile}
%%
%% where we here have assume the existence of the files
%% definitions.bib and bibliofile.bib.
%% BibTeX documentation can be obtained at:
%% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
%%%%%%


%% Or you use manual references (pay attention to consistency and the
%% formatting style!):
%\begin{thebibliography}{9}

%\bibitem{Laport:LaTeX}
%L.~Lamport,
%  \emph{\LaTeX: A Document Preparation System,} 
%  Addison-Wesley, Reading, Massachusetts, USA, 2nd~ed., 1994. 

%\bibitem{GMS:LaTeXComp}
%F.~Mittelbach, M,~Goossens, J.~Braams, D.~Carlisle, and
%C.~Rowley, \emph{The {\LaTeX} Companion,} Addison-Wesley,
%Reading, Massachusetts, USA, 2nd~ed., 2004.

%\bibitem{oetiker_latex}
%T.~Oetiker, H.~Partl, I.~Hyna, and E.~Schlegl, \emph{The Not So Short
%  Introduction to {\LaTeX2e}}, version 5.06, Jun.~20, 2016. [Online].
%  Available: \url{https://tobi.oetiker.ch/lshort/}

%\bibitem{typesetmoser}
%S.~M. Moser, \emph{How to Typeset Equations in {\LaTeX}}, version 4.6,
%  Sep. 29, 2017. [Online]. Available:
%  \url{http://moser-isi.ethz.ch/manuals.html#eqlatex}

%\bibitem{IEEE:pdfsettings}
%IEEE, \emph{Preparing Conference Content for the IEEE Xplore Digital
%  Library.} [Online]. Available:
%  \url{http://www.ieee.org/conferences_events/conferences/organizers/pubs/preparing_content.html}

%\bibitem{IEEE:AuthorToolbox}
%IEEE, \emph{Author Digital Toolbox.} [Online.] Available:
%  \url{http://www.ieee.org/publications_standards/publications/authors/authors_journals.html}

%\end{thebibliography}


\end{document}


%%%%%%
%% Some comments about useful packages
%% (extract from bare_conf.tex by Michael Shell)
%%

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx


% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array

% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig


% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


% *** PDF and URL PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.



% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%%%%%%


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
