%% LaTeX Template for ISIT 2019
%%
%% by Stefan M. Moser, October 2017
%% 
%% derived from bare_conf.tex, V1.4a, 2014/09/17, by Michael Shell
%% for use with IEEEtran.cls version 1.8b or later
%%
%% Support sites for IEEEtran.cls:
%%
%% http://www.michaelshell.org/tex/ieeetran/
%% http://moser-isi.ethz.ch/manuals.html#eqlatex
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%%

\documentclass[11pt,journal,onecolumn]{IEEEtran}

\usepackage[margin=2.2cm]{geometry}
%% depending on your installation, you may wish to adjust the top margin:
\addtolength{\topmargin}{9mm}

%%%%%%
%% Packages:
%% Some useful packages (and compatibility issues with the IEEE format)
%% are pointed out at the very end of this template source file (they are 
%% taken verbatim out of bare_conf.tex by Michael Shell).
%
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%
%\usepackage[normalem]{ulem}
\usepackage[utf8]{inputenc} 
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{hyperref}
\usepackage{xcolor}


%\usepackage{parskip}
\usepackage{ifthen}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{float}
\usepackage[cmex10]{amsmath} % Use the [cmex10] option to ensure complicance
                             % with IEEE Xplore (see bare_conf.tex)
\usepackage[numbers]{natbib}                             
%\usepackage{physics}
%% Please note that the amsthm package must not be loaded with
%% IEEEtran.cls because IEEEtran provides its own versions of
%% theorems. Also note that IEEEXplore does not accepts submissions
%% with hyperlinks, i.e., hyperref cannot be used.

\interdisplaylinepenalty=2500 % As explained in bare_conf.tex

\newcommand{\etal}{\textit{et al.}}
\newcommand{\dataset}{{\cal D}}
\newcommand{\ERM}{{\sf{ERM}}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\E}[1]{\mathbb E\left[#1\right]}
\newcommand{\Esub}[2]{\mathbb E_{#1}\left[#2\right]}
\newcommand{\gen}{\textup{gen}}
\newcommand{\pp}[1]{\mathbb P\left(#1\right)}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\EE}[2]{\ensuremath{\mathbb E_{#1}\left\{#2\right\}}}

\newtheorem{theorem}{\bf{Theorem}}
\newtheorem{definition}{\bf{Definition}}
\newtheorem{lemma}{\bf{Lemma}}
\newtheorem{corollary}{\bf{Corollary}}
\newtheorem{proposition}{\bf{Proposition}}
\newtheorem{remark}{\bf{Remark}}
\newtheorem{example}{\bf{Example}}
\newtheorem{assumption}{\bf{Assumption}}
%%%%%%
% correct bad hyphenation here


% ------------------------------------------------------------
\begin{document}
% \title{On the Tightness of Information-theoretic Bounds on Generalization Error of Learning Algorithms\let\thefootnote\relax\footnotetext{*This work is an extended version of the preliminary work \cite{wu2022fast} appeared in ITW2022 conference. This paper extends the previous results on the justification of the tightness of the generalization error whereas we derive the lower bound for the generalization error that matches the fast rate upper bound. Furthermore, we present instances where the $(\eta,c)$-central condition is not satisfied, supported by several analytical examples. The fast rate results are confirmed with more examples such as regression and classification problems analytically or numerically, showing the effectiveness of the proposed bounds.}} 


\title{Fast Rate Information-theoretic Bounds on Generalization Errors\let\thefootnote\relax\footnotetext{*This work is an extended version of the preliminary work \cite{wu2022fast} appeared in ITW2022 conference. This paper extends the previous results on the justification of the tightness of the generalization error whereas we derive the lower bound for the generalization error that matches the fast rate upper bound. Furthermore, we present instances where the $(\eta,c)$-central condition is not satisfied, supported by several analytical examples. The fast rate results are confirmed with more examples, such as regression and classification problems analytically or numerically, showing the effectiveness of the proposed bounds.}} 


% %%% Single author, or several authors with same affiliation:
% \author{%
%   \IEEEauthorblockN{Stefan M.~Moser}
%   \IEEEauthorblockA{ETH Zürich\\
%                     ISI (D-ITET)\\
%                     CH-8092 Zürich, Switzerland\\
%                     Email: moser@isi.ee.ethz.ch}
% }


%%% Several authors with up to three affiliations:
\author{%
%  \IEEEauthorblockN{Xuetong Wu}
%  \IEEEauthorblockA{ETH Zürich\\
%                    ISI (D-ITET), ETH Zentrum\\
%                    CH-8092 Zürich, Switzerland\\
%                    Email: moser@isi.ee.ethz.ch}
%  \and
  \IEEEauthorblockN{Xuetong Wu$^1$, Jonathan H. Manton$^1$, Uwe Aickelin$^2$, Jingge Zhu$^{1}$}
  
  \IEEEauthorblockA{$^1$Department of EEE, \quad 
                    $^2$Department of CIS\\
                    University of Melbourne, Parkville, Victoria, Australia}
}


%%% Many authors with many affiliations:
% \author{%
%   \IEEEauthorblockN{Albus Dumbledore\IEEEauthorrefmark{1},
%                     Olympe Maxime\IEEEauthorrefmark{2},
%                     Stefan M.~Moser\IEEEauthorrefmark{3}\IEEEauthorrefmark{4},
%                     and Harry Potter\IEEEauthorrefmark{1}}
%   \IEEEauthorblockA{\IEEEauthorrefmark{1}%
%                     Hogwarts School of Witchcraft and Wizardry,
%                     1714 Hogsmeade, Scotland,
%                     \{dumbledore, potter\}@hogwarts.edu}
%   \IEEEauthorblockA{\IEEEauthorrefmark{2}%
%                     Beauxbatons Academy of Magic,
%                     1290 Pyrénées, France,
%                     maxime@beauxbatons.edu}
%   \IEEEauthorblockA{\IEEEauthorrefmark{3}%
%                     ETH Zürich, ISI (D-ITET), ETH Zentrum, 
%                     CH-8092 Zürich, Switzerland,
%                     moser@isi.ee.ethz.ch}
%   \IEEEauthorblockA{\IEEEauthorrefmark{4}%
%                     National Chiao Tung University (NCTU), 
%                     Hsinchu, Taiwan,
%                     moser@isi.ee.ethz.ch}
% }


\maketitle

%%%%%%
%% Abstract: 
%% If your paper is eligible for the student paper award, please add
%% the comment "THIS PAPER IS ELIGIBLE FOR THE STUDENT PAPER
%% AWARD." as a first line in the abstract. 
%% For the final version of the accepted paper, please do not forget
%% to remove this comment!
%%
\begin{abstract}
% A recent line of works, initiated by \cite{russo2016controlling} and \cite{xu2017information}, has shown that the generalization error of a learning algorithm can be upper bounded by information measures. In most of the relevant works, the convergence rate of the expected generalization error is in the form of $O(\sqrt{\lambda/n})$ where $\lambda$ is some information-theoretic quantities such as the mutual information or conditional mutual information between the data and the learned hypothesis. However, such a learning rate is typically  considered to be ``slow", compared to a ``fast rate" of $O(\lambda/n)$ in many learning scenarios. In this work, we first show that the square root does not necessarily imply a slow rate and a fast rate result can still be obtained using this bound under appropriate assumptions. Furthermore, we identify the critical conditions needed for the fast rate generalization error, which we call the $(\eta,c)$-central condition. Under this condition, we give information-theoretic bounds on the generalization error and excess risk, with a fast convergence rate for specific learning algorithms such as empirical risk minimization and its regularized version. Finally, several analytical examples are given to show the effectiveness of the bounds.

The generalization error of a learning algorithm refers to the discrepancy between the loss of a learning algorithm on training data and that on unseen testing data.  Various information-theoretic bounds on the generalization error have been derived in the literature, where the mutual information between the training data and the hypothesis (the output of the learning algorithm) plays an important role. Focusing on the individual sample mutual information bound by Bu et al.~\cite{bu2020tightening}, which itself is a tightened version of the first bound on the topic by Russo et al.~\cite{russo2016controlling} and Xu et al.~\cite{xu2017information}, this paper investigates the tightness of these bounds, in terms of the dependence of their convergence rates on the sample size $n$. It has been recognized that these bounds are in general not tight, readily verified for the exemplary quadratic Gaussian mean estimation problem, where the individual sample mutual information bound scales as $O(\sqrt{1/n})$ while the true generalization error scales as $O(1/n)$. The first contribution of this paper is to show that the same bound can in fact be asymptotically tight if an appropriate assumption is made. In particular, we show that the fast rate can be recovered when the assumption is made on the excess risk instead of the loss function, which was usually done in existing literature. A theoretical justification is given for this choice.  The second contribution of the paper is a new set of generalization error bounds based on the $(\eta, c)$-central condition, a condition relatively easy to verify and has the property that the mutual information term directly determines the convergence rate of the bound. Several analytical and numerical examples are given to show the effectiveness of these bounds.
\end{abstract}

%% The paper must be self-contained. However, if you are referring to
%% a full version for checking certain proofs, please provide the
%% publically accessible location below.  If the paper is completely
%% self-contained, you can remove the following line from your
%% submission.
%\textit{A full version of this paper is accessible at:}
%\url{http://isit2019.fr/} 

\section{Introduction} \label{sec:intro}
The generalization error of a learning algorithm refers to the discrepancy between the loss of a learning algorithm on training data and that on unseen testing data. An upper bound on this quantity is crucial for assessing the generalization capability of learning algorithms. Conventionally, many bounding techniques are proposed under different conditions and assumptions.  To name a few, \citet{vapnik1971uniform} proposed VC-dimension, which describes the richness of a hypothesis class for generalization ability.  The notion of ``algorithmic stability" was introduced in \cite{kearns1997algorithmic} and \cite{devroye1979distributiona}  (see also \citet{bousquet2002stability}) for bounding the generalization error by examining if a single training sample has a significant effect on the expected loss. PAC-Bayes bounds are a class of algorithm-dependent bounds first introduced by \citet{mcallester1999some} (see \cite{alquier2024user} for a recent survey). The PAC-Bayes bounds usually contain a Kullback-divergence term between the learning algorithm and a prior distribution and hence are similar in spirit to information-theoretic bounds, which contain mutual information terms, with the difference that most of them are high probability bounds and are data-distribution independent, therefore generally less tight. However, most bounds mentioned above are only concerned with the hypothesis space or the algorithm. For example, VC-dimension methods care about the worst-case bound which only depends on the hypothesis space. The stability methods only specify the properties of learning algorithms but do not require additional assumptions on hypothesis space. To fully characterize the intrinsic nature of a learning problem, it is shown in some recent works that the generalization error can be upper bounded using the information-theoretic quantities \cite{xu2017information,russo2016controlling}, and the bound usually takes the following form:      
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W, \mathcal{S}_n)] \leq \sqrt{\frac{c I(W;\mathcal{S}_n)}{n}}, \label{eq:gen-form}
\end{align}
where the expectation is taken w.r.t. the joint distribution of $W$ and $\mathcal{S}_n$ induced by some algorithm $\mathcal{A}$. Here, $\mathcal{E}(w, \mathcal{S}_n)$ denotes the generalization error (properly defined in~(\ref{eq:gen}) in Section \ref{sec:prob}) for a given hypothesis $w$ and data sample $\mathcal{S}_n = (Z_i)_{i=1,\cdots,n}$, and $I(W;\mathcal{S}_n)$ denotes the mutual information between the hypothesis and data sample, and $c$ is some positive constant. In particular, if the loss function is $\sigma$-sub-Gaussian under the distribution $P_W \otimes P_Z$, $c$ equals $2\sigma^2$. By introducing the mutual information, such a bound gives a bound that depends both on the learning algorithm and the data distribution.  It has been shown that for specific problems, the information-theoretic bounds can recover the results obtained by the method of VC dimension~\cite{xu2017information}, algorithmic stability~\cite{raginsky2016information}, differential privacy~\cite{steinke2020reasoning} under mild conditions. Further, as pointed out by \cite{asadi2018chaining}, the information-theoretic upper bound could be substantially tighter than the traditional bounds if we could exploit specific properties of the learning algorithm.  A follow-up work by \cite{bu2020tightening} showed a tightened version of the above bound in the form
\begin{align}
        \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W, \mathcal{S}_n)] \leq \frac{1}{n}\sum_{i=1}^n\sqrt{2\sigma^2 I(W;Z_i)}, \label{eq:gen-form-individual}
\end{align}
which is generally higher than the bound in (\ref{eq:gen-form}). In particular, while the mutual information term in (\ref{eq:gen-form}) can be arbitrarily large or infinite for deterministic algorithms as observed by  \cite{bu2020tightening,hellstrom2020generalization, steinke2020reasoning}, the individual sample mutual information bound in (\ref{eq:gen-form-individual}) can still give non-vacuous bound for such algorithms. We mention that random subset methods \cite{negrea2019information, rodriguez2021random} (see also the work by \cite{zhou2022individually,haghifam2020sharpened}) and bounds based on ``ghost samples" \cite{steinke2020reasoning} are also used to derive tightened bounds.

It has been recognized that the original bounds in (\ref{eq:gen-form}) or (\ref{eq:gen-form-individual}) are, in general, not tight. It can be easily verified that for the simple Gaussian quadratic problem (details in Example \ref{sec:example}), the bound in (\ref{eq:gen-form-individual}) scales as $O(\sqrt{1/n})$ while the true generalization error scales as $O(1/n)$. It begs the problem of whether information-theoretic bounds can be intrinsically tight. Several works are dedicated to this problem, including \cite{hellstrom2021fast}, \cite{hellstrom2022new}, \cite{zhou2023stochastic} and \cite{zhou2024exactly}. We will compare our results with these works in  Section \ref{subsec:related_works}.


In this work, we develop a general framework for the fast rate bounds using the mutual information following this line of works~\cite{van2015fast,grunwald2020fast,grunwald2021pac} and the contributions are listed as follows.
\begin{itemize}
    \item We show that for the Gaussian quadratic problem, the bound in (\ref{eq:gen-form-individual}) can be made asymptotically tight when an appropriate assumption is made, where the sub-Gaussian condition is assumed on the excess risk instead of on the loss function. An intuitive explanation is given for the choice of this assumption, and a matching lower bound is also given for this specific problem.
    \item Inspired by the analysis for the sub-Gaussian case, we propose a new condition, called the $(\eta, c)$-central condition, under which we derive a set of new generalization bounds. Compared with typical mutual information bounds, the novel bounds have a cleaner presentation and wider applicability. Under the assumption that the excess risk satisfies the $(\eta,c)$-central condition, the bounds have the nice property that the convergence rate is directly determined by the mutual information term $I(W; Z_i)$. The bounds are shown to be asymptotically tight for various examples under the empirical risk minimization (ERM) algorithm.
     %which also improves from $O(\sqrt{\lambda/n})$ to $O(\lambda/n)$ under some widely used algorithms such as  
    % In addition to the sub-Gaussian assumption, we also proved that the sub-exponential and sub-Gamma assumptions satisfy the $(\eta,c)$-central condition, from which the new forms are established and new insights could be drawn. 
    \item Furthermore, our results are extended to regularized ERM algorithms, and intermediate rates could be achieved with the relaxed $(v,c)$-central condition. The fast rate results are confirmed for a few simple regression and classification examples analytically or numerically, showing the effectiveness of the proposed bounds. %Our framework can be generalized to many other learning tasks such as classification problems, for a tighter generalization error and excess risk bound. 
\end{itemize}


\subsection{Related Works}\label{subsec:related_works}
There exist several works in the literature,  including \cite{hellstrom2021fast}, \cite{hellstrom2022new}, \cite{zhou2023stochastic}, and \cite{zhou2024exactly}, that aim to obtain a fast rate using information-theoretic bounds. The results in \cite{hellstrom2021fast,hellstrom2022new} contain bounds in terms of conditional mutual information is bounded uniformly in $n$, then a fast rate of $O(1/n)$ can be achieved. The result in \cite{zhou2023stochastic} combines the stochastic chaining technique with the individual mutual information and derives a bound that is asymptotically tight for the quadratic Gaussian problem. A recent work \cite{zhou2024exactly} gives a PAC-Bayes type bound that is shown to be exactly tight for the quadratic Gaussian problem for any $n$. Different from these works, our results emphasize the fact that even the original bound (without other tightening techniques) of (\ref{eq:gen-form-individual}) can give the correct convergence rate under an appropriate assumption. Furthermore, our derived bounds based on the $(\eta,c)$-central condition give a different perspective on the tightness of information-theoretic bounds for generalization. Our proposed $(\eta,c)$-central condition is inspired by the work of \cite{van2015fast,grunwald2020fast,grunwald2021pac}, and is closely related to other conditions such as the $\eta$-central condition \cite{van2015fast}, the witness condition \cite{grunwald2020fast,grunwald2021pac} and the Bernstein condition \cite{bartlett2006empirical}. We give a detailed comparison of these conditions and related bounds in Section \ref{sec:related}.

\section{Problem formulation} \label{sec:prob}
Consider a dataset $\mathcal{S}_n = \left( z_1,z_2,\cdots,z_n\right)$ where each instance $z_i$ is i.i.d. drawn from some distribution $\mu$, we would like to learn a hypothesis $w$ that exploits the properties of $\mathcal{S}_n$, with the aim of making predictions for previously unseen new data correctly. The choice of $w$ is performed within a set of member functions $\mathcal{W}$ with the possibly randomised algorithm $\mathcal{A}:\mathcal{Z}^n \rightarrow \mathcal{W}$, described by the conditional probability $P_{W|\mathcal S_n}$. We define the corresponding loss function $\ell: \mathcal{W}\times \mathcal{Z} \rightarrow \mathbb{R}$. Particularly if we consider the supervised learning problem, we can write $\mathcal{Z} = \mathcal{X} \times \mathcal{Y}$ and $z_i = (x_i, y_i)$ as a feature-label pair. Then the hypothesis $w: \mathcal{X} \rightarrow \mathcal{Y}$ can be regarded as a predictor for the input sample. We will call $(\mu, \ell, \mathcal{W}, \mathcal{A})$ a learning tuple. In a typical statistical learning problem, one may wish to minimize the \emph{expected} loss function $L_{\mu}(w) = E_{z\sim \mu}[\ell(w,z)]$. However, as the underlying distribution $\mu$ is usually unknown in practice, one may wish to learn $w$ by some learning principle, for example, one typical way is minimizing the empirical risk induced by the dataset $\mathcal{S}_n$, denoted as $w_{\ERM}$, such that 
\begin{equation}
w_{\ERM} = \argmin_{w\in \mathcal{W}}\frac{1}{n}\sum_{i=1}^{n}\ell(w,z_i),
\end{equation}
which will be employed as a predictor for the new data. We point out that many of our results obtained in this paper also hold for more general algorithms other than the ERM algorithm. Here we define $\hat{L}(w, \mathcal{S}_n) = \frac{1}{n}\sum_{i=1}^{n}\ell(w,z_i)$ as the empirical loss.  To assess how this predictor performs on unseen samples, the generalization error is then introduced to evaluate whether a learner suffers from over-fitting (or under-fitting). For any $w \in \mathcal{W}$, we define the generalization error as
\begin{equation}
\mathcal{E}(w, \mathcal{S}_n) := \mathbb{E}_{Z\sim \mu}[\ell(w,Z)] - \frac{1}{n}\sum_{i=1}^{n}\ell(w,z_i).  \label{eq:gen}
\end{equation}
%We also define the expected generalization error as,
%\begin{equation}
%\mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W, \mathcal{S}_n)] := \mathbb{E}_{W\otimes \mu}[\ell(W,Z)] -\mathbb{E}_{W\mathcal{S}_n}[\frac{1}{n}\sum_{i=1}^{n}\ell(W,Z_i)] 
%\end{equation}
%The ability to perform generalization is the fundamental property to look for in a learned predictor. 
Another important metric, the excess risk, is defined as
\begin{equation}
\mathcal{R}(w) := \mathbb{E}_{Z\sim \mu}[\ell(w,Z)] - \mathbb{E}_{Z\sim \mu}[\ell(w^*,Z)].
\end{equation}
where the optimal hypothesis for the true risk is defined as $w^*$ as 
\begin{equation}
w^{*} = \argmin_{w\in \mathcal{W}}E_{Z \sim \mu}[\ell (w,Z)],
\end{equation}
which is unknown in practice. The excess risk evaluates how well a hypothesis $w$ performs with respect to $w^*$ given the data distribution $\mu$. We also define the corresponding empirical excess risk as
\begin{equation}
\hat{\mathcal{R}}(w, \mathcal{S}_n) := \frac{1}{n}\sum_{i=1}^{n}r(w,z_i),
\end{equation}
where $r(w,z) = \ell(w,z) - \ell(w^*,z)$. In the sequel, we are particularly interested in bounding the expected generalization error $\mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W, \mathcal{S}_n)]$ and the excess risk $\mathbb{E}_{W}[\mathcal{R}(W)]$  where the distribution $P_W$ is the marginal distribution of $P_{W\mathcal S_n}$, induced by the distribution of the data $\mathcal S_n$ and the algorithm $P_{W|\mathcal S_n}$.


\section{Tightness of Individual Mutual Information Bound}
\subsection{Existing Bounds}
In this section, we first review some known results on generalization error.  A representative result in the following result of individual mutual information bound by \cite{bu2020tightening}, which is a tightened version of the first results on this topic by \cite{xu2017information} and \cite{russo2016controlling}.
% For example, the recent advances show that under the sub-Gaussian assumption, the generalization error can be upper bounded using the information-theoretic quantities such as mutual information \cite{xu2017information,bu2020tightening,zhou2022individually} or conditional mutual information \cite{steinke2020reasoning}. The bound usually takes the following general form. 
\begin{theorem}[Generalization error of Generic Hypothesis \cite{bu2020tightening}] \label{thm:gen_erm}
Assume that the cumulant generating function of the random variable $\ell(W, Z)-\E{\ell(W,Z)}$  is upper bounded by $\psi(-\lambda)$ in the interval $(b_{-},0)$ and $\psi(\lambda)$ in the interval $(0,b_{+})$ under the product distribution $P_W\otimes\mu$ for some $b_{-}<0$ and $b_{+}>0$ where $P_W$ is induced by the data distribution and the algorithm. Then the expectation of the  generalization error in (\ref{eq:gen}) is upper bounded as
\begin{align}
\Esub{W\mathcal{S}_n}{\mathcal{E} (W, \mathcal{S}_n) }\leq \frac{1}{ n}\sum_{i=1}^{ n}\psi^{*-1}_{-}(I(W;Z_i)), \\
-\Esub{W\mathcal{S}_n}{\mathcal{E} (W, \mathcal{S}_n)}\leq \frac{1}{ n}\sum_{i=1}^{n}\psi^{*-1}_{+}(I(W;Z_i)),
\end{align}
where we define
\begin{align}
\psi^{*-1}_{-}(x) :=\inf_{\lambda\in[0,-b_{-})}\frac{x+\psi(-\lambda)}{\lambda}, \quad \psi^{*-1}_{+}(x) :=\inf_{\lambda\in[0,b_{+})}\frac{x+\psi(\lambda)}{\lambda}.
\end{align} \label{thm:exp_gen_gamma}
\end{theorem}
Under different assumptions, the above theorem can be specialized to cases when the loss function is sub-exponential, sub-Gamma, or sub-Gaussian random variables by identifying the CGF bounding function $\psi(\lambda)$. We give the example when the loss function is sub-Gaussian in the following.  More discussions on the CGF bounding function can be found in \cite{jiao2017dependence,bu2020tightening}.



\begin{example}[Sub-Gaussian bound]
We say $X$ is a $\sigma$-sub-Gaussian random variable with variance parameter $\sigma$ if it holds
%\begin{align}
$\log \mathbb{E}\left[e^{\lambda (X-\mathbb{E}[X])}\right] \leq \frac{\sigma^{2} \lambda^{2}}{2}$ for all $\lambda \in \mathbb{R}$.
%\end{align}
%Note that the sub-Gaussian random variable is sub-exponential random variable but not the other way around. 
Suppose that $\ell({W}, {Z})$ is $\sigma$-sub-Gaussian under the distribution $P_{W} \otimes \mu$ where $P_W$ is the marginal induced the algorithm $\mathcal{A}$ and data distribution $\mu$, then
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right]  \leq \frac{1}{n} \sum_{i=1}^{n} \sqrt{2 \sigma^{2} I\left(W ; Z_{i}\right)}. \label{eq:bu_result}
\end{align}
\end{example}

% \begin{remark}
% Throughout this section, we will focus on the case when $I(W;Z_i) \sim O(1/n)$ in the sequel, so the bound in (\ref{eq:bu_result}) gives a convergence rate of $O(\sqrt{1/n})$ for a constant $\sigma^2$ for both the sub-Gaussian and sub-Gamma bounds. For a broader family of the loss function that is sub-exponential, the upper bound will not even converge if the mutual information $I(W;Z_i)$ is larger than $\frac{\sigma^2}{2\alpha}$ for some positive constants $\sigma$ and $\alpha$ if we refer to \ref{eq:subgexponential}. The above assumption holds for many learning settings such as ERM in the linear regression problem \cite{raginsky2016information}, the Gibbs algorithm with mild assumptions \cite{raginsky2016information,xu2017information,aminian2021exact,grunwald2021pac}. More generally, the generalization error will also decay exponentially as $n$ increases if the mutual information decays exponentially, an example of which can be found in Section~\ref{sec:discretehypothesis}.
% \end{remark}

If the mutual information term $I(W;Z_i)$ scales as $O(1/n)$ (which is the case for the quadratic Gaussian problem in Example \ref{sec:example}) and if the sub-Gaussian parameter $\sigma^2$ is a constant, then the bound in (\ref{eq:bu_result}) scales as $O(\sqrt{1/n})$ because of the square root. Therefore it is usually recognized that the \emph{square root} sign prevents us from the fast rate, exemplified in the following simple quadratic Gaussian mean estimation problem.
\begin{example}\label{sec:example}
Let $\ell(w,z_i) = (w-z_i)^2$, each sample is drawn from some Gaussian distribution, $Z_i \sim \mathcal{N}(\mu, \sigma_{N}^2)$. We consider the ERM algorithm that gives,
\begin{align*}
 W_{\ERM} = \frac{1}{n} \sum_{i=1}^{n} Z_i \sim \mathcal{N}(\mu, \frac{\sigma_{N}^2}{n}).
\end{align*}
The true generalization error can be calculated to be
%\begin{align*}
 $  \Esub{W\mathcal{S}_n}{\mathcal{E}(W_\ERM, \mathcal{S}_n)} = \frac{2\sigma_{N}^2}{n}$.
%\end{align*}
To evaluate the upper bound in~Theorem~\ref{thm:gen_erm} for this example, we notice that for any $i$, $\ell(W,Z_i) \sim \frac{n+1}{n}\sigma_{N}^2 \chi_{1}^{2}$ where $\chi^2_1$ denotes the chi-squared distribution with 1 degree of freedom. Hence, the cumulant generating function can be calculated as,
\begin{align*}
\log \mathbb{E}_{P_W\otimes \mu}\left[e^{\eta(\ell(W,Z)-\mathbb{E}[\ell(W,Z)])}\right] = - \sigma_{W}^2 \eta -\frac{1}{2} \log \left(1-2\sigma_{W}^2 \eta \right),
\end{align*}
where $\eta \leq \frac{1}{2\sigma^2_W}$ and $\sigma^2_W = \frac{n+1}{n}\sigma_{N}^2$ to simplify the notation. In this case, it can be proved that,
\begin{align}
    - \sigma_{W}^2 \eta -\frac{1}{2} \log \left(1-2\sigma_{W}^2 \eta \right) \leq \sigma_W^4\eta^2. \label{eq:CGF}
\end{align}
We can also calculate the mutual information as 
%\begin{align*}
  $I(W;Z_i) = \frac{1}{2}\log\frac{n}{n-1}$.
%\end{align*}
With the upper bound on the CGF in (\ref{eq:CGF}), the bound becomes
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right]  \leq \frac{\sigma_{N}^2}{n} \sum_{i=1}^{n} \sqrt{2\frac{(n+1)^2}{n^2}\log\frac{n}{n-1}} \leq \sigma^2_N\sqrt{\frac{2(n+1)^2}{(n-1)^3}},
\end{align}
which will be of the order $O(\frac{1}{\sqrt{n}})$ as $n$ goes to infinity, which leads to a slow convergence rate as the true generalization error is $O(\frac{1}{n})$. The detailed calculations can be found in Appendix~\ref{apd:example2}.
\end{example}
%
\subsection{Restoring the Fast Rate}
In this section, we show that, in fact, the same bound can be used to derive the correct (fast) convergence rate of $O(1/n)$, with a small yet important change on the assumption. Intuitively speaking, to achieve a fast rate bound for both the generalization error and the excess risk in expectation, the output hypothesis of the learning algorithm must be ``good" enough compared to the optimal hypothesis $w^*$. Here we encode the notion of goodness in terms of the cumulant generating function by controlling the gap between $\ell(w, Z)$ and $\ell(w^*, Z)$. To facilitate such an idea, we make the sub-Gaussian assumption w.r.t. the excess risk and bound the generalization error as follows.
\begin{theorem}\label{thm:sub-Gaussian}
Suppose that $r({W}, {Z})$ is $\sigma$-sub-Gaussian under distribution $P_{W} \otimes \mu$, then
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq \frac{1}{n} \sum_{i=1}^{n} \sqrt{2 \sigma^{2} I\left(W ; Z_{i}\right)}. \label{eq:our_result}
\end{align}
Furthermore, the excess risk can be bounded by,
\begin{align}
    \mathbb{E}_{W} \left[\mathcal{R}(W)\right]  \leq \mathbb{E}_{W\mathcal{S}_n} \left[\hat{\mathcal{R}}(W, \mathcal{S}_n)\right] + \frac{1}{n} \sum_{i=1}^{n} \sqrt{2 \sigma^{2} I\left(W ; Z_{i}\right)} . \label{eq:our_result_excess}
\end{align}
\end{theorem}
The proof of this result is given in Appendix \ref{proof:sub-Gaussian}. Notice that the expression in (\ref{eq:our_result}) is identical to that in (\ref{eq:bu_result}), with the only difference that the sub-Gaussian condition is assumed on $r(W,Z)$ for the former and on $\ell(W,Z)$ for the latter. Now we evaluate the bound in~Theorem~\ref{thm:sub-Gaussian} for the Gaussian example.
\begin{example}[Continuing from Example~\ref{sec:example}] \label{example:sub-Gaussian-2}
Consider the settings in Example~\ref{sec:example}. First, we note that the expected risk minimizer $w^*$ is calculated as $\mu$. Then we have $r(w,z_i) = (w - z_i)^2 - (\mu - z_i)^2$. The expected excess risk can be calculated as $\mathbb{E}_{W}[\mathcal{R}(W)] = \frac{\sigma_{N}^2}{n}$. We can then calculate the cumulant generating function as,
\begin{align*}
\log \mathbb{E}_{P_W\otimes \mu}\left[e^{\eta(r(W,Z)-\mathbb{E}[r(W,Z)])}\right] \leq \frac{4\eta^2\sigma_N^4}{n},
\end{align*}
for any $\eta \in \mathbb{R}$ and any $n >  \max\left\{\frac{(4\eta^2\sigma^4_N+\eta\sigma^2_N)(2\eta^2\sigma^4_N+\eta\sigma^2_N)}{\eta^2\sigma^4_N}, 4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2\right\} $ where the detailed calculations can be found in Appendix~\ref{apd:example3}. Hence $r(W,Z)$ is $\sqrt{\frac{8\sigma_N^4}{n}}$-sub-Gaussian under the distribution $P_{W} \otimes \mu$. Then the bound becomes,
\begin{align*}
    \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq \frac{\sigma_N^2}{n} \sum_{i=1}^{n} \sqrt{\frac{8}{n}\log\frac{n}{n-1}} \leq \frac{2\sqrt{2}\sigma^2_N}{n-1},
\end{align*}
which is $O(1/n)$, yielding a fast rate characterization.
\end{example}

Unlike prior results where the bounds are based on the assumption that the loss function is $\sigma$-sub-Gaussian, we assume that the \emph{excess risk} is $\sigma$-sub-Gaussian. Even though the bound in~(\ref{eq:our_result}) has exactly the same form as in~(\ref{eq:bu_result}), the key difference is that under our assumption, $\sigma$ can depend on the sample size and will converge to $0$ as the sample size increases, while this is not the case under the previous assumption as we see in Example~\ref{sec:example}.  Moreover, the excess risk can be straightforwardly upper bounded as in~(\ref{eq:our_result_excess}).  Although this slight variation of the result gives an asymptotically tight bound, the assumption change from $\ell(W, Z)$ to $r(W, Z)$ seems rather arbitrary. In the following subsection, we give an intuitive explanation and a justification for this choice.

\subsection{Tightness and Justification for the Quadratic Gaussian Problem} \label{subsec:justification}
In the following, we examine the tightness of the bound and show why $r(W, Z)$ is a more sensible choice, specifically for the quadratic Gaussian problem. To this end, we recall that the main technical tool for deriving the bound is the variational representation of the KL divergence. Specifically, let $X$ be a random variable with alphabet $\mathcal{X}$ and let $P, Q$ be two probability density functions. The KL Divergence admits the following dual representation \cite{donsker1975asymptotic}:
\begin{align}
  D(P \| Q)=\sup _{f: \mathcal{X} \rightarrow \mathbb{R}} \mathbb{E}_P[f(X)]-\log \left(\mathbb{E}_Q\left[e^{f(X)}\right]\right), \label{eq:donsker}
\end{align}
and the tightness of the bound hinges on the choice of the function $f$ in (\ref{eq:donsker}). It is well known that under mild conditions \cite{donsker1975asymptotic}, the optimal function for the Donsker-Varadhan representation is achieved by $f'(dP_{W|Z_i}/dP_W$) where $f(t) = t\log t$. For the quadratic Gaussian problem, we now calculate this optimizer explicitly and show that the choice of $r(W, Z_i)$ is, in fact, the right choice.


To this end, we firstly calculate the densities of $P_W$ and $P_{W|Z_i}$ as: $dP_W = \frac{\sqrt{n}}{\sqrt{2\pi\sigma_N^2}} \exp(\frac{-(W - \mu)^2 n }{2\sigma_N^2})$, $dP_{W|Z_i} = \frac{n}{\sqrt{2\pi \sigma_N^2(n-1)}} \exp(-\frac{(W - \frac{n-1}{n}\mu - \frac{1}{n}Z_i)^2n^2}{2\sigma_N^2(n-1)}).$ Then we can calculate the optimizer as:
\begin{align*}
    f'(dP_{W|Z_i}/dP_W) &= \log \frac{dP_{W|Z_i}}{dP_W} + 1 \\
    &= \frac{1}{2}\log \frac{n}{n-1}  -\frac{(w - \frac{n-1}{n}\mu - \frac{1}{n}z_i)^2n^2}{2\sigma_N^2(n-1) }+\frac{(w - \mu)^2 n }{2\sigma_N^2} +1 \\
    %&= \frac{1}{2}\log \frac{n}{n-1}  -\frac{(n(w - z_i) + (n-1)(z_i-\mu))^2}{2\sigma_N^2(n-1) }+\frac{(w - z_i + z_i - \mu)^2 n }{2\sigma_N^2} +1 \\
    %&=\frac{1}{2}\log \frac{n}{n-1}  + \frac{(w-z_i)^2((n-1)n-n^2) + (z_i - \mu)^2(n(n-1) - (n-1)^2)}{2\sigma_N^2(n-1)} + 1\\
    %&=\frac{1}{2}\log \frac{n}{n-1}  + \frac{(z_i - \mu)^2(n-1) - (w-z_i)^2n }{2\sigma_N^2(n-1)} + 1 \\   
    &= - \frac{(w-z_i)^2 - (\mu - z_i)^2}{2\sigma_N^2} - \frac{(w-z_i)^2}{2\sigma_N^2(n-1)} + 1 + \frac{1}{2}\log\frac{n}{n-1} 
\end{align*}
for fixed $w$ and $z_i$. The above function can be written as:
\begin{align*}
    f'(dP_{W|Z_i}/dP_W) = -\frac{r(w,z_i)}{2\sigma_N^2} - \frac{\ell(w,z_i)}{2\sigma_N^2(n-1)} + \frac{1}{2}\log\frac{n}{n-1} + 1.
\end{align*}
The unexpected excess risk $r(w,z_i)$ clearly appears in the optimizer with some scaling factor and shifting constant, which, however, will not affect the convergence. To rigorously show this, we state the following result.
\begin{lemma}\label{lemma:tightness}
Consider the quadratic Gaussian problem in Example \ref{sec:example} and the Donsker-Varadhan representation in (\ref{eq:donsker}). Let $\eta = \frac{1}{2\sigma^2_N}$, the function $-r(w,z_i)$ satisfies the following inequality:
\begin{align*}
    \frac{n-1}{n} I(W;Z_i) \leq \mathbb{E}_{WZ_i}[-\eta r(W,Z_i)] - \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z_i)}] \leq I(W;Z_i),
\end{align*}
\end{lemma}
The proof of the above Lemma is given in Appendix \ref{proof:lemma_tightness}.  The above lemma demonstrates that the variational representation is essentially tight for the quadratic Gaussian problem. As in this case, the RHS of (\ref{eq:donsker}) is essentially equal to the divergence $D(P||Q)$ (which is the mutual information $I(W; Z_i)$ for the quadratic Gaussian problem) on the LHS. On the other hand, it can be checked straightforwardly that the loss function $\ell(W, Z)$ does not admit a tight approximation for $I(W; Z_i)$.

%Further the ESI assumption $-r(W,Z_i) \unlhd^{P_W \otimes \mu_i}_{(1-\alpha)\eta} -\epsilon$ implies that $\mathbb{E}_{P_W \otimes \mu_i}[r(W,Z_i)] \geq \epsilon$. The small error term $\epsilon$ describes the deviation of $W$ as learned from the algorithm $\mathcal{A}$ compared to $w^*$ in expectation. 
For the quadratic Gaussian problem, we can, in fact, show that mutual information also gives a lower bound for both the excess risk and generalization error for the Gaussian mean estimation problem.
\begin{lemma}[Matching Lower Bound]\label{thm:lower_bounds}
Consider the quadratic Gaussian mean estimation problem with the ERM algorithm. With a large $n$, we have: 
\begin{align*}
    \mathbb{E}_{P_W \otimes \mu}[r(W,Z)] \geq \frac{2\sigma^2_N}{n}\sum_{i=1}^{n}I(W;Z_i) + \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(W,\mathcal{S}_n)] - \frac{1}{n-1} \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)].
\end{align*}
For the generalization error, we have:
\begin{align*}
    \mathbb{E}_{W \mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \geq 2\sigma_N^2 \frac{n-1}{n^2}\sum_{i=1}^{n} I(W;Z_i).
\end{align*}
\end{lemma}
The proof of this result is given in Appendix \ref{proof:lowerbound}. From the above results, we observe that the individual sample mutual information appears in both the upper and lower bounds. For the generalization error, the upper and lower bounds are matched regarding the convergence rate with different leading constants. For the excess risk in the Gaussian mean example, the upper bound is tight since the empirical excess risk and generalization error are both of $O(\frac{1}{n})$. %However, for more general learning problems, the lower bounds of the excess risk mainly depend on the empirical excess risk and the generalization error.

\section{Novel Fast Rate Results}
\subsection{New Bounds with $(\eta,c)$-Central Condition}
%If we consider the case where $\sigma^2$ is a constant that does not depend on sample size, then the bound in (\ref{eq:sub-Gaussian}) is even worse than the bound in (\ref{eq:our_result}) since the former does not converge but the latter converges as $O({1}/{\sqrt{n}})$. This is because $\eta a_\eta \sim O(1/n)$ and the second term in R.H.S. of (\ref{eq:sub-Gaussian}) will become a constant. More generally, the bound in its current form may still be loose if $\eta a_{\eta}$ shrinks with the rate of $O(n^{-\alpha})$ for some $\alpha \geq 1$. To alleviate this drawback, we impose more critical conditions on the excess risk and extend the results by replacing $\eta a_{\eta}$ with some constant that does not depend on the sample size. 
%Even though the above bound gives more insights on the generalization error as the trade-off is clearly characterized in Theorem~\ref{thm:eta-c-loss}, compared to the sub-Gaussian assumption where the expected loss is not shown in the bound, it is not necessarily tight in the example we showed and does not provide the correct convergence rate in general. 

As observed in the results from the previous section, while our new bound is tight, it still contains a square root term in the bound. In fact, many information-theoretic bounds for generalization error \cite{xu2017information, raginsky2016information, bu2020tightening, zhou2022individually, steinke2020reasoning, russo2016controlling} contain the square root with the sub-Gaussian assumption, which is often seen as the obstacle to achieving fast rate results. As the first result in this section, we provide an alternative bound based on the sub-Gaussian assumption that does not contain the square root. We point out that similar results also appeared in \cite{steinke2020reasoning} and \cite{hellstrom2021fast} under different assumptions.
\begin{theorem}[Fast Rate with Sub-Gaussian Condition]\label{thm:sub-Gaussianv2}
Assume that $r(W, Z)$ is $\sigma$-sub-Gaussian under the distribution $P_W \otimes \mu$. Then it holds that
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq  \frac{1-a_\eta}{a_\eta} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}]  + \frac{1}{n\eta a_\eta} \sum_{i=1}^{n}  I\left(W ; Z_{i}\right). \label{eq:sub-Gaussian}
\end{align}
for any $ 0 < \eta < \frac{2\mathbb{E}_{P_W \otimes \mu}[r(W,Z_i)]}{\sigma^2}$ and $a_\eta = 1-  \frac{\eta\sigma^2}{2\mathbb{E}_{P_W \otimes \mu}[r(W,Z_i)]}$. Furthermore, the expected excess risk is bounded by:
\begin{align*}
 \mathbb{E}_{W}[\mathcal{R}(W)] \leq& \frac{1}{a_\eta} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] 
    + \frac{1}{n\eta a_\eta} \sum_{i=1}^{n}  I\left(W ; Z_{i}\right).
\end{align*}
\end{theorem}
The proof of this result is given in Appendix \ref{proof:sub-Gaussianv2}. We continue to examine the bound in Theorem~\ref{thm:sub-Gaussianv2} with the Gaussian mean estimation. 
\begin{example}\label{eg:subv2}
Since the expected excess risk can be calculated as $\mathbb{E}_{W}[\mathcal{R}(W)] = \frac{\sigma_N^2}{n}$, and $r(W,Z)$ is $\sqrt{\frac{8\sigma_N^4}{n}}$-sub-Gaussian, then we require that $0 <\eta < \frac{1}{2\sqrt{2}\sigma_N^2}$, which is independent of the sample size. For simplicity, we can apply Theorem \ref{thm:sub-Gaussianv2} to the quadratic Gaussian mean estimation problem with the choice $\eta = \frac{1}{4\sigma_N^2}$ and $a_\eta=\frac{1}{2}$. For any $n$ that satisfies the condition in Example~\ref{example:sub-Gaussian-2}, we have the generalization error bound,
\begin{align*}
   \frac{1-a_\eta}{a_\eta}\Esub{W\mathcal{S}_n}{\hat{\mathcal{R}}(W_\ERM,\mathcal{S}_n)} +  \frac{1}{\eta a_{\eta} n}\sum_{i=1}^{n}I(W;Z_i) \leq  \frac{3\sigma_N^2}{n},
\end{align*}
where the empirical excess risk $\Esub{W\mathcal{S}_n}{\hat{\mathcal{R}}(W_\ERM,\mathcal{S}_n)}$ is calculated as $-\frac{\sigma^2_N}{n}$ and the bound has the rate of $O(1/n)$. 
\end{example}

Notice that in the above example, both $\eta$ and $\alpha_\eta$ depend on the expected excess risk $\mathbb{E}_{P_W \otimes \mu}[r(W,Z)]$ and $\sigma^2$, which potentially depend on $n$ as well.  In this sense, this result is still not very satisfying, as this dependence makes it hard to determine the actual convergence rate directly from the bounds. To this end, we propose a different type of bound to alleviate this drawback. Ideally, we aim for a bound that does not contain extra quantities that depend on $n$, the key to such a bound is the so-called expected ($\eta,c$)-central condition (or we simply say ($\eta,c$)-central condition for short), inspired by the works \cite{van2015fast,mehta2017fast,grunwald2020fast,grunwald2021pac}. We will first define the $(\eta,c)$-central condition for a non-negative random variable and use this notation to bound the generalization error with the loss function and excess risk.

\begin{definition}[$(\eta,c)$-central condition]
Let ${\eta}>0$ and $0 < c \leq 1$ be two constants. We say that a random variable $X$ endowed with a probability measure $P$ satisfies the $(\eta,c)$-central condition if the following inequality holds:
\begin{align}
\log \mathbb{E}_{P}& \left[e^{-{\eta}X}\right]  \leq   -c\eta  \mathbb{E}_{P}\left[X\right]. \label{eq:eta_c_loss} 
\end{align} 
given that $0 < E_P[X] < \infty$.
\end{definition}
\begin{remark}
Comparing the $(\eta,c)$-central condition with the $\sigma$-sub-Gaussian condition defined as
\begin{align*}
    \log \mathbb{E}_{P}[e^{-\eta X}] \leq -\eta \mathbb{E}_{P}[X] + \frac{\eta^2\sigma^2}{2}, \forall \eta \in \mathbb{R},
\end{align*}
the main difference is on the bounding terms for the CGF where the variance proxy term $\frac{\eta^2\sigma^2}{2}$ is replaced by the term $(1-c)\eta \EE{P}{X}$ for a positive $\eta$. This also shows that if we want to make the sub-Gaussian condition and the $(\eta,c)$-central condition equivalent, the variance proxy $\sigma^2$ should have the same scaling law as the term $\EE{P}{X}$. This is consistent with the results in Example \ref{example:sub-Gaussian-2}, where both $\E{\mathcal R(W)}$ and the variance proxy of $r(W,Z)$ scales as $O(1/n)$. We furthermore compare $(\eta,c)$-central condition with other related conditions in the literature in Section \ref{sec:related}.
\end{remark}



Then we make the following assumption that the unexpected excess risk satisfies the $(\eta,c)$-central condition.
%\begin{definition}[Expected $(\eta,c)$-Central Condition w.r.t. the Excess Risk]
%Let ${\eta}>0$ and $0 < c \leq 1$ be two constants. We say that $(\mu, \ell, \mathcal{W}, \mathcal{A})$ satisfies the expected $(\eta,c)$-central condition if the following inequality holds for the optimal hypothesis $w^*$:
\begin{assumption}\label{assump:eta_c_r}
We assume the unexpected excess risk $r(W,Z_i)$ satisfies the $\left(\eta, c \right)$-central condition for any $i\in [n]$, some constants $\eta > 0$ and $0 < c \leq 1$ under the product distribution $P_W\otimes\mu$, e.g.,
\begin{align}
\log \mathbb{E}_{P_W\otimes \mu}& \left[e^{-{\eta}\left(\ell(W,Z)-\ell(w^*,Z)\right)}\right]  \leq   -c\eta  \mathbb{E}_{P_W\otimes \mu}\left[\ell(W,Z) - \ell(w^*,Z)\right]. \label{eq:eta_c}
\end{align} 
\end{assumption}
%\end{definition}
%The first condition is the Bernstein condition.
%\begin{definition}[$(\eta,c)$-Central Condition]
%Let ${\eta}>0$ and $0 < c \leq 1$. We say a learning problem satisfies the $(\eta,c)$-central condition if there exists $w^* \in \mathcal{W}$ such that for any $w\in\mathcal{W}$, the following inequality holds:
%\begin{align}
%\log \mathbb{E}_{\mu}& \left[e^{-{\eta}\left(\ell(w,Z)-\ell(w^*,Z)\right)}\right]  \leq  -c\eta  \mathbb{E}_{\mu}\left[\ell(w,Z) - \ell(w^*,Z)\right]. \label{eq:eta_c} %\textup{ i.e., } \ell(w^*,Z)-\ell(W,Z) \unlhd^{W\otimes \mu}_{{\eta}} 0. 
%\end{align}
%\end{definition}
%By Jensen's inequality, it is easy to see that if $w^*$ satisfies the inequality~(\ref{eq:eta_c}), it must be the minimizer of the expected loss. 
%{\color{red}[Discuss the conventional $\eta$-central condition this with Xuetong. Currently, the $\eta$-central condition is defined later but I think we should move it here]}

The above definition is inspired by the $\eta$-central condition \cite[(5)]{van2015fast}, which in our notation takes the form
\begin{align}
\log \mathbb{E}_{P_W\otimes \mu}& \left[e^{-{\eta}\left(\ell(W,Z)-\ell(w^*,Z)\right)}\right]  \leq   0  \label{eq:eta},
\end{align} 
which is the special case of $(\eta,c)$-central condition with $c=0$. Compared to the $\eta$-central condition, the RHS of~(\ref{eq:eta_c}) is negative and has a tighter control than (\ref{eq:eta}) of the tail behaviour for some $c> 0$. A similar condition is given in \cite[(4)]{van2015fast}, which in our notation  takes the form
\begin{align}
\log \mathbb{E}_{\mu}& \left[e^{-{\eta}\left(\ell(w,Z)-\ell(w^*,Z)\right)}\right]  \leq 0, \forall w 
\end{align}
This is a stronger condition as it is required to hold all $w \in \mathcal{W}$ instead of in expectation.

%even in some trivial examples as shown in Appendix~\ref{apd:condiiton_checking}. Our proposed ($\eta,c$)-central condition is weaker in the sense that it is only required to be satisfied in expectation.

\begin{remark}
%Although $\mathbb{E}_{\mu}\left[r(w, Z)\right]$ is always non-negative for any $w$ by definition, $r(W, Z)$ is not a non-negative random variable under the distribution $W\otimes\mu$. 
Using the Markov inequality, the $(\eta,c)$-central condition on $r(W,Z)$ implies that for any $t$, we have
\begin{align*}
    \pp{r(W,Z)\leq t}&=\pp{-r(W,Z)\geq -t}=\pp{e^{-\eta r(W,Z)}\geq e^{-\eta t}}\\
    &\leq e^{\eta t}\E{e^{-\eta r(W,Z)}}\leq  e^{-\eta(c\E{r(W,Z)}-t)}
\end{align*}
% \begin{align*}
% \pp{\ell(w^*,Z)-\ell(W,Z)\geq t}&=\pp{-r(W,Z)\geq t}=\pp{e^{-\eta r(W,Z)}\geq e^{\eta t}}\\
% &\leq e^{-\eta t}\E{e^{-\eta r(W,Z)}}\leq  e^{-\eta(t+c\E{r(W,Z)})}
% \end{align*}
This shows for $t< c\E{r(W,Z)}$, the probability that  $\ell(W,Z)\leq \ell(w^*,Z)+t$ is exponentially small.
%Since $t+c\E{r(W, Z)}$ is positive, this implies that the probability that $w^*$ outperforms $w$ is exponentially small. In other words, the $(\eta,c)$-central condition ensures that with high probability, $W$ is a good hypothesis whose performance is comparable to that of $w^*$.
\end{remark}


% \begin{remark}
% With the Chernoff bound, if the loss function satisfies the $(\eta,c)$-central condition, we have the lower tail probability bound as:
% \begin{align*}
%     P(r(W,Z_i) \leq -t) \leq  e^{-\eta(t+c\mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)])}.
% \end{align*}
% We provide a probabilistic interpretation on the lower tail of the excess risk and this bound is particularly useful in the following two senses. On the one hand, this condition ensures that for all $\eta' \leq \eta$, the probability that $w$ outperforms $w^*$ by more than $t$ is exponentially small in $t + c\mathbb{E}[r(W,Z)]$. On the other hand, this implicitly provides an upper tail bound on the excess risk probability. 
% \end{remark}

%\subsubsection{Bernstein's Condition}

With the definitions in place, we derive the fast rate bounds under the $(\eta, c)$-central condition as follows. %We first derive the fast rate with the Bernstein condition following the idea in \cite{grunwald2021pac} with the mutual information.

\begin{theorem}[Fast Rate with $(\eta, c)$-central condition]\label{thm:eta-c}
Suppose Assumption~\ref{assump:eta_c_r} is satisfied, then it holds that:
\begin{align*}
     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq & \frac{1-c}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)] + \frac{1}{c\eta n} \sum_{i=1}^{n} I(W;Z_i).
\end{align*}
\noindent Furthermore, the excess risk is bounded by,
 \begin{align*}
     \mathbb{E}_{W}[\mathcal{R}(W)] \leq & \frac{1}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)]  + \frac{1}{c\eta n} \sum_{i=1}^{n} I(W;Z_i).
 \end{align*}
\end{theorem}

%{\color{red} [To discuss: why should one choose some $\eta'<\eta$]?}

The proof of this result is given in Appendix \ref{proof:thm_eta-c}. Such a bound has a similar form with \cite[Eq.~(3)]{grunwald2021pac}, which consists of the empirical excess risk and mutual information terms, and the first term is non-positive for some algorithms such as ERM. Notice that different from the bound in Theorem~\ref{thm:sub-Gaussianv2}, the bound in Theorem~\ref{thm:eta-c} contains constants $c$ and $\eta$ that do not depend on the sample size $n$ (as we will verify for several examples later in the paper.) Therefore, this bound has the nice property that the convergence rate is solely determined by the mutual information term $I(W; Z_i)$.

In the following, we analytically examine our bounds in Gaussian mean estimation, and we also empirically verify our bounds with a logistic regression problem in Appendix~\ref{sec:logistic}. 
\begin{example}
We can verify that the $(\eta, c)$-central condition is satisfied for the quadratic Gaussian mean estimation problem. It can be checked that for $n >  \max\left\{\frac{(4\eta^2\sigma^4_N-\eta\sigma^2_N)(2\eta^2\sigma^4_N-\eta\sigma^2_N)}{\eta^2\sigma^4_N}, 4\eta^2\sigma_N^4 - 2 \eta\sigma_N^2\right\} $, it holds that
\begin{align*}
\log \mathbb{E}_{P_W\otimes \mu}\left[e^{-\eta r(W,Z)}\right] \leq \frac{4 \eta^2\sigma_N^4 - \eta \sigma_N^2}{n} \leq -c\eta \frac{\sigma_N^2}{n}.
\end{align*}
From the above inequality, this learning problem satisfies the $(\eta, c)$-central condition for any $0 < \eta < \frac{1}{4\sigma_N^2}$ and any $c \leq 1- 4\eta\sigma_N^2$, which is independent of the sample size and thus does not affect the convergence rate. Similarly, take $\eta = \frac{1}{8\sigma_N^2}$ and $c = \frac{1}{2}$, the bound becomes
\begin{align*}
    \frac{1-c}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)] + \frac{1}{c\eta n} \sum_{i=1}^{n} I(W;Z_i) \leq \frac{7\sigma_N^2}{n},
\end{align*}
which coincides with the bound in~Example~\ref{eg:subv2} and we can arrive at the fast rate since $I(W;Z_i) \sim O(1/n)$. 
\end{example}

It is natural to consider whether we can apply the ($\eta,c$)-central condition to the loss function and obtain fast rate results. We present the following theorem regarding the generalization error when the loss function satisfies the $(\eta,c)$-central condition.
\begin{theorem}[Generalization Error Bounds with $(\eta, c)$-central condition w.r.t. the loss function]\label{thm:eta-c-loss}
Assume the loss function $\ell(w,z_i)$ satisfies the  $\left(\eta, c \right)$-central condition for any $i\in [n]$, some constants $\eta > 0$ and $0 < c \leq 1$ under the data distribution $\mu$ and algorithm $\mathcal{A}$. Then it holds that:
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq & \frac{1-c}{c}\mathbb{E}_{W\mathcal{S}_n}[\hat{L}(W,\mathcal{S}_n)] + \frac{\sum_{i=1}^{n}I(W;Z_i)}{c\eta n}. \label{eq:loss-bound}
\end{align}
\end{theorem}

%\begin{remark}
% From Theorem~\ref{thm:eta-c-loss}, we can  see the trade-off of the generalization from (\ref{eq:loss-bound}): if the empirical loss is small,  the mutual information is likely to be large as $W$ fits $Z_i$ well, hence contains a lot of information about $Z_i$. On the other hand, if the mutual information term $I(W; Z_i)$ is small, the empirical risk tends to be large, as intuitively $W$ does not exploit much knowledge from the data and hence becomes less dependent on each instance $Z_i$.
%If we consider an extreme case where $c = 1$, the bound in (\ref{eq:loss-bound}) holds tightly as the inequality in (\ref{eq:eta_c_loss}) becomes equality due to Jensen's inequality and the loss function $\ell(W, Z_i)$ will be a constant for any $W$ and $Z_i$. For such a case, both L.H.S. and R.H.S. in  (\ref{eq:loss-bound}) are becoming zero, and the bound is tight for a constant loss. Nevertheless, this bound may not be tight as the empirical loss may not converge with increasing sample size, as illustrated in the example below.
%\end{remark}
\begin{example}
Concretely, let us again examine the Gaussian mean estimation problem. To verify the ($\eta,c$)-central condition w.r.t. the loss function, we can calculate the CGF under the distribution $P_W \otimes \mu$ as:
\begin{align}
    \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta \ell(W,Z)}] = -\frac{1}{2}\log(1+2\eta \frac{n+1}{n} \sigma_N^2),
\end{align}
while the expected loss can be calculated as,
\begin{align*}
    \mathbb{E}_{P_W \otimes \mu}[\ell(W,Z)] = \frac{n+1}{n}\sigma_N^2.
\end{align*}
Then the $(\eta, c)$-central condition can be written as:
\begin{align*}
    -\log(1+2\eta \frac{n+1}{n}\sigma_N^2) \leq -2c\eta \frac{n+1}{n}\sigma_N^2.
\end{align*}
For arbitrary choice of $\eta$ and any $n \geq 1$, we can select $c$ as,
\begin{align*}
    c = \frac{\log(1+4\eta \sigma^2_N)}{4\eta \sigma^2_N} \leq \frac{\log(1+2\eta \frac{n+1}{n}\sigma_N^2)}{2\eta \frac{n+1}{n}\sigma_N^2},
\end{align*}
as the function $\frac{\log(1+x)}{x}$ is non-increasing for all $x > 0$. The generalization error can be upper bounded by,
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq & \frac{1-c}{c}\frac{n-1}{n}\sigma^2_N + \frac{1}{2c\eta(n-1)}. \label{eq:eta-c-gaussian-loss}
\end{align}
which converges to $\frac{1-c}{c}\sigma^2_N$ with the rate of $O(\frac{1}{n})$, and the bound is not tight in this case.
\end{example}
\begin{remark}
As can be seen from the above example, the R.H.S. of (\ref{eq:eta-c-gaussian-loss}) does not converge to zero with $n$ increasing when $c$ and $\eta$ are chosen to be independent of the sample size $n$, which does not match the true generalization error. We can draw a comparable insight from this example as illustrated in the previous section: obtaining the fast rate result requires us to make appropriate assumptions on the unexpected excess risk $r(W, Z)$ while making assumptions under the loss function itself is not sufficient to a tight bound.
\end{remark}

%\subsection{Bounds with Excess Risk Function}

% The virtue of this bound is that, for some widely used algorithms such as ERM, the empirical excess risk will be negative and we can further bound the excess risk with the mutual information term only. To mathematically catch sight of the differences induced by different assumptions between (\ref{eq:our_result}) and (\ref{eq:bu_result}), we reformulate the inequality~(\ref{eq:our_result}) to remove the square root sign and then depict the fast rate condition for $\eta$ and $\sigma$, as shown in the following theorem.

%We will prove that the conventional sub-Gaussian assumption can also yield the fast rate .  Recall that $r(w,z) = \ell(w,z) - \ell(w^*,z)$ and $\mathcal{R}(W) = \mathbb{E}_{\mu}[r(w,z)]$. We introduce a small bias to $\mathcal{R}(W)$ by a factor of $\alpha$ for the generalization error, say, we are going to examine the following term $\mathbb{E}_{W\mathcal{S}_n}\{\alpha \mathcal{R}(W)-\hat{\mathcal{R}}(W,\mathcal{S}_n)\}$. We then have the following lemma:
%\begin{lemma} \label{lemma:alpha}
%Assume that $r(w, Z)$ is $\sigma^{2}$-sub-Gaussian under the distribution $P_W \otimes \mu$. Also assume that  $-r(W,Z_i) \unlhd^{P_W \otimes \mu_i}_{(1-\alpha)\eta} -\epsilon$ for some $\eta>0$ and $\epsilon > 0$ and $0 < \alpha<1$. Then it holds that
%\begin{equation}
%    \mathbb{E}\{\alpha \mathcal{R}(W)-\hat{\mathcal{R}}(W, \mathcal{S}_n)\} \leq \frac{1}{n} \sum_{i=1}^{n} \frac{\alpha^2\sigma^2}{2(1-\alpha)\epsilon} I\left(W ; Z_{i}\right)
%\end{equation}
%\end{lemma}
%\noindent With the lemma above, we will arrive at the following theorem straightforwardly.
%\begin{theorem}\label{thm:sub-Gaussian}
%Assume Lemma~\ref{lemma:alpha} holds. Then for any $\alpha<1$, it holds that
%\begin{align}
%     \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq & \frac{1-\alpha}{\alpha} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] \\
%    &+ \frac{1}{n} \sum_{i=1}^{n} \frac{\alpha \sigma^{2}}{2 (1-\alpha) \epsilon} I\left(W ; Z_{i}\right)   
%\end{align}
%Furthermore, the expected excess risk is bounded by,
%\begin{align}
% \mathbb{E}_{W}[\mathcal{R}(W)] \leq& \frac{1}{\alpha} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] 
%    + \frac{1}{n} \sum_{i=1}^{n} \frac{\alpha \sigma^{2}}{2 (1-\alpha) \epsilon} I\left(W ; Z_{i}\right).
%\end{align}
%\end{theorem}
%\begin{remark}

%We can also relax the ESI assumption by replacing $\epsilon$ with $\mathbb{E}_{P_W \otimes P_{Z_i}}[r(W,Z_i)]$ and arrive at the following theorem.
%We relax the condition and arrive at the following theorem.
%\end{remark}



%\subsection{Fast Rate with Central Condition}
%Next we derive the fast rate with the central condition at the following theorem.
%\begin{theorem}[Fast Rate with Central Condition]\label{thm:central}
%Let $(\mu, \ell, \mathcal{W}, \mathcal{A})$ represent a learning problem which satisfies the $\bar{\eta}$-central condition. If the $(u, c)$-witness condition holds, then for any $\eta \in(0, \bar{\eta})$, we have
%\begin{equation}
% \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq (c_{u} - 1) \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(W,\mathcal{S}_n)] + \frac{c_u}{\eta} \frac{1}{n} \sum_{i=1}^{n} I(W;Z_i)
%\end{equation}
%Furthermore, the expected excess risk is bounded by,
%\begin{equation}
% \mathbb{E}_{W}[\mathcal{R}(W)] \leq c_{u} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(W,\mathcal{S}_n)] + \frac{c_u}{\eta} \frac{1}{n} \sum_{i=1}^{n} I(W;Z_i)   
%\end{equation}
%where $c_u = \frac{1}{c}\frac{\eta u + 1}{1- \frac{\eta}{\bar{\eta}}} > 1$, $u > 0$ and $c \in (0, 1]$.
%\end{theorem}


%\begin{remark}
%With the convention that $\frac{0}{0} = 0$, if we consider the extreme case in~(\ref{eq:sub-Gaussian}) such that $\mathbb{E}_{P_W \otimes \mu_i}[r(W,Z_i)] = 0$, then $W = w^*$ almost surely and $\eta$ will shrink to 0. In this case $W$ is independent of $Z_i$, and $I(W;Z_i) = 0$.  We have $a_{\eta} = 1$ and $\mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] = 0$, in this case the bound is tight and holds trivially. %If $\mathbb{E}_{P_W \otimes \mu_i}[r(W,Z_i)] > 0$, in other words, the learned hypothesis is not consistent with $w^*$. In this case, we can improve the bound from $\sqrt{I(W;Z_i)}$ to $I(W;Z_i)$ and arrive at a fast rate.
%\end{remark}

\subsection{Connection with Other Conditions} \label{sec:related}
Fast rate conditions are widely investigated under different learning frameworks and conditions \cite{van2015fast, mehta2017fast, koren2015fast, mhammedi2019pac, grunwald2020fast, zhu2020semi, grunwald2021pac}. As the most relevant work, our bound is similar to that found in \cite{grunwald2021pac} which applies conditional mutual information \cite{steinke2020reasoning}, but their results do not hold for unbounded losses and specifically do not hold for sub-Gaussian losses. Our result applies to general algorithms with mutual information and our assumptions are weaker since we only require the proposed conditions to hold in expectation w.r.t. $P_W$, instead of for all $w \in\mathcal{W}$. Our results also have the benefit of allowing the convergence factors to be further improved by using different metrics and data-processing techniques, see \cite{jiao2017dependence, hafez2020conditioning, zhou2022individually} for examples.  Now it is instructive to compare the different assumptions used in the related works. We point out that the $(\eta,c)$-central condition is indeed the key assumption for generalizing the result of Theorem~\ref{thm:sub-Gaussianv2}, which also coincides with some well-known conditions that lead to a fast rate. We firstly show that the Bernstein condition \cite{bartlett2006empirical,bartlett2006convexity, hanneke2016refined,mhammedi2019pac} implies the $(\eta,c)$-central condition for certain $\eta$ and $c$ in the following corollary. 
\begin{corollary}\label{coro:berstein}
Let $\beta \in [0,1]$ and $B \geq 1$. For a learning tuple $(\mu, \ell, \mathcal{W}, \mathcal{A})$,  we say that the \textbf{Bernstein condition} holds if the following inequality holds for the optimal hypothesis $w^*$:
\begin{align*}
       {\mathbb{E}}_{P_W \otimes \mu}&\left[\left(\ell\left(W, Z^{\prime}\right)-\ell\left(w^{*} , Z^{\prime}\right)\right)^{2}\right] \leq B \left({\mathbb{E}}_{P_W \otimes \mu}\left[\ell\left(W, Z^{\prime}\right)- \ell\left(w^{*} ; Z^{\prime}\right)\right]\right)^{\beta}.
\end{align*}
If the above condition holds with $\beta = 1$ and $r(w,z_i)$ is lower bounded by $-b$ with some $b > 0$ for all $w$ and $z_i$, the learning tuple also satisfies $(\min(\frac{1}{b}, \frac{1}{2B(e-2)}), \frac{1}{2})$-central condition.
\end{corollary}

The proof of the above result is given in Appendix \ref{proof:corollary_bernstein}. The Bernstein condition is usually recognized as a characterization of the ``easiness" of the learning problem under various $\beta$ where $\beta = 1$ corresponds to the ``easiest" learning case. For bounded loss functions, the Bernstein condition will automatically hold with $\beta = 0$.  Different from the standard setting, we only require that the learned (randomized) hypothesis $W$ satisfy the inequality in expectation. This is a weaker requirement: the condition does not need to be satisfied for all $w\in\mathcal W$ but only needs to be satisfied on average.
%\begin{remark}
% which In particular, $\beta = 1$ corresponds to the easiest and the learning rate will be $O(\frac{1}{n})$ if $I(W;Z_i)$ is converging as the sample size increasing. For the bounded loss, the Bernstein condition will automatically hold with $\beta = 0$ and it will recover the results in \cite{xu2017information,bu2020tightening} with the rate of $O(\sqrt{\frac{1}{n}})$.
%\end{remark}

The second condition is the central condition with the witness condition \cite{van2015fast,grunwald2020fast}, which also implies the $(\eta,c)$-central condition. We say $(\mu, \ell, \mathcal{W}, \mathcal{A})$ satisfies the $\eta$-central condition \cite{van2015fast,grunwald2020fast} if for the optimal hypothesis $w^*$, the following inequality holds,
\begin{align*}
\mathbb{E}_{P_W\otimes \mu}\left[e^{-{\eta}\left(\ell(W,Z)-\ell(w^*,Z)\right)}\right] \leq 1. %\textup{ i.e., } \ell(w^*,Z)-\ell(W,Z) \unlhd^{W\otimes \mu}_{{\eta}} 0.
\end{align*}
We also say the learning tuple  $(\mu, \ell, \mathcal{W}, \mathcal{A})$ satisfies the  $(u, c)$-witness condition \cite{grunwald2020fast} if for constants $u > 0$ and $c \in (0,1]$, the following inequality holds.
\begin{align*}
     \mathbb{E}_{P_W\otimes \mu}& [\left(\ell(W,Z)-\ell({w^{*}},Z) \right) \cdot   \mathbf{1}_{\left\{\ell(W,Z)-\ell({w^{*}},Z) \leq u \right\}}] \geq c \mathbb{E}_{P_W\otimes \mu}\left[\ell(W,Z) -\ell({w^{*}},Z) \right],
\end{align*}
where $ \mathbf{1}_{\{\cdot\}}$ denotes the indicator function.  The standard $\eta$-central condition is a key condition for proving the fast rate \cite{van2015fast,mehta2017fast,grunwald2020fast}. Some examples are exponential concave loss functions (including log-loss) with $\eta = 1$ (see \cite{mehta2017fast,zhu2020semi} for examples) and bounded loss functions with Massart noise condition with different $\eta$ \cite{van2015fast}.  The witness condition~\cite[Def. 12]{grunwald2020fast} is imposed to rule out situations in which learnability simply cannot hold. The intuitive interpretation of this condition is that we exclude bad hypothesis $w$ with negligible probability (but still can contribute to the expected loss), which we will never witness empirically.  They are connected to the $(\eta,c)$-central condition in the following way.
%For instance, consider a setting with $\mathcal{F}=\left\{f^{*}, f_{1}, f_{2}, \ldots\right\}$ where $\ell_{f^{*}}=1$ with probability 1 and, for each $j \geq 1, \ell_{f_{j}}$ is equal to 0 with probability $1-\frac{1}{j}$ and equal to $2 j$ with probability $\frac{1}{j}$. Then for all $j, \mathbb{E}\left[\ell_{f_{j}}-\ell_{f^{*}}\right]=1$, but as $j \rightarrow \infty$, empirically we will never witness the badness of $f_{j}$ as it almost surely achieves lower loss than $f^{*}$. On the other hand, if the excess loss is upper bounded by some constant $b$, we may always take $u=b$ and $c=1$ so that a witness condition is trivially satisfied. Such a condition can also be written as
\begin{corollary}\label{coro:central}
If the learning tuple satisfies both $\eta$-central condition and $(u,c)$-witness condition, then the learning tuple also satisfies the $(\eta', \frac{c(1-\eta'/\eta)}{\eta' u +1})$-central condition for any $0< \eta' < \eta$.
\end{corollary}

Furthermore, we can show the following connection between the $(\eta,c)$-central condition to sub-exponential and sub-gamma assumptions. Recall that we say $X$ is a $\left(\nu^2, \alpha\right)$-sub-exponential random variable with parameters $\nu, \alpha>0$ if:
\begin{align}
\log\mathbb{E} \left[ e^{\lambda (X -\mathbb{E}[X])}\right] \leq \frac{\lambda^2 \nu^2}{2} , \quad \forall \lambda:|\lambda|<\frac{1}{\alpha}.  
\end{align}
We say $X$ is a ($\nu^2,\alpha$)-sub-Gamma random variable with variance parameter $\nu^2$ and scale parameter $\alpha$ if:
\begin{align}
\log\mathbb{E} \left[ e^{\lambda (X -\mathbb{E}[X])}\right]\leq \frac{\nu^2 \lambda^2}{2(1- \alpha \lambda)}, \quad  \forall \lambda:  0<\lambda<\frac{1}{\alpha}.
\end{align}


% The above two conditions are useful in the sense that they rely on the algorithm, the loss function and the data distribution, for which the verification in real learning scenarios may be difficult. To give more concrete examples, we further specify the distribution classes of the excess risk that are more general than the sub-Gaussian assumption, and we claim that our proposed condition can be applied to heavier tail distributions such as sub-exponential, and sub-Gamma families. 



\begin{corollary}\label{coro:subexponential}
If $r(W, Z)$ is ($\nu^2$, $\alpha$)-sub-exponential under the distribution $P_W \otimes \mu$, then the learning tuple satisfies $(\min(\frac{1}{\alpha}, \frac{\nu^2}{\mathbb{E}_{P_W \otimes \mu}[r(W,Z)]}), \frac{1}{2})$-central condition. Correspondingly, the generalization error is upper bounded as
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq &  \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)] + \frac{2}{\eta n} \sum_{i=1}^{n} I(W;Z_i). \label{sub-exponential}
\end{align}
for any $0< \eta \leq \min(\frac{1}{\alpha}, \frac{\nu^2}{\mathbb{E}_{P_W \otimes \mu}[r(W,Z)]})$. If $r(W, Z)$ is ($\nu^2$, $\alpha$)-sub-Gamma under the distribution $P_W \otimes \mu$, then the learning tuple satisfies $(\frac{\mathbb{E}_{P_W\otimes\mu}[r(W,Z)]}{\nu^2+\alpha \mathbb{E}_{P_W\otimes\mu}[r(W,Z)]}, \frac{1}{2})$-central condition.  Correspondingly, the generalization error is upper bounded in the same way as in (\ref{sub-exponential})
for any $0< \eta \leq\frac{\mathbb{E}_{P_W\otimes\mu}[r(W,Z)]}{\nu^2+\alpha \mathbb{E}_{P_W\otimes\mu}[r(W,Z)]}$.
\end{corollary}
The proof of the above result is given in Appendix~\ref{proof:coro_subexponential_subgamma}. It is useful to compare the above results with the results obtained by directly applying the known result Theorem \ref{thm:gen_erm}.
\begin{corollary}
    Assume that $\ell(W, Z)$ is ($\nu^2$, $\alpha$)-sub-exponential under the distribution $P_W \otimes \mu$ for some $\nu^2$ and $\alpha > 0$. Then it holds that
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq \begin{cases}
      \frac{1}{n}\sum_{i=1}^{n}\sqrt{2\nu^2 I(W;Z_i)}, \textup{ if } I(W;Z_i) \leq  \frac{\nu^2}{2\alpha^2} \textup{ for all } i \in [n],\\
       \frac{\nu^2}{2\alpha} + \frac{\alpha}{n}\sum_{i=1}^{n} I(W;Z_i),   \textup{ if } I(W;Z_i) > \frac{\nu^2}{2\alpha^2} \textup{ for all } i \in [n].
     \end{cases}
 \label{eq:subgexponential}
\end{align}
Assume that $\ell(W, Z)$ is ($\nu^2$, $\alpha$)-sub-Gamma under the distribution $P_W \otimes \mu$ for some $\nu^2$ and $\alpha > 0$. Then it holds that
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq \frac{1}{n}\sum_{i=1}^{n}\sqrt{2\nu^2I(W;Z_i)} + \alpha I(W;Z_i).
 \label{eq:subgamma}
\end{align}
\end{corollary}



To end this section, we summarize and outline all  technical conditions in Table~\ref{tab:tech1}. From the table, we can see that our proposed $(\eta,c)$-central condition coincides with many existing works such as \cite{grunwald2020fast} and \cite{grunwald2021pac} with suitable choices of $c$ and $\eta$.  For bounded loss, $\beta = 1$ in the Bernstein condition is equivalent to the central condition with the witness condition,  which implies the $(\eta,c)$-central condition. As an example of unbounded loss functions, the log-loss will satisfy the central and witness conditions under well-specified model \cite{wong1995probability,grunwald2020fast}, which also consequently implies the $(\eta,c)$-central condition. As suggested by Theorem~\ref{thm:sub-Gaussianv2}, Corollary~\ref{coro:subexponential}, the sub-Gaussian, sub-exponential and sub-Gamma conditions can also satisfy the $(\eta,c)$-central condition for different parameters in the assumptions.  

\begin{table}[!hbt]
    \centering
    \caption{A comparison of different conditions in Section \ref{sec:related}}\label{tab:tech1}
    \begin{tabular}{|c|c|}
    \hline
     Condition      &  Key Inequality   \\
     \hline
     $(\eta,c)$-Central Condition & $\log \mathbb{E}\left[e^{-\eta r(W,Z)}\right] \leq  -c \eta \mathbb{E}[r(W,Z)]$ \\
     \hline 
     Bernstein Condition with $\beta = 1$   &  $\log  \mathbb{E}\left[e^{-\eta r(W,Z)}\right] \leq  -\frac{1}{2} \eta \mathbb{E}[r(W,Z)]$ \\
     \hline 
     Central  Condition + Witness Condition    &  $\log  \mathbb{E}\left[e^{-\eta r(W,Z)}\right] \leq  -\frac{1}{c_u}\eta\mathbb{E}[r(W,Z)]$   \\
    \hline 
     Central Condition  &  $\log  \mathbb{E}\left[e^{-\eta r(W,Z)}\right]  \leq 0$      \\
     \hline
     Sub-Gaussian Condition  & $\log \mathbb{E}\left[ e^{ -\eta r(W,Z)} \right] \leq -\eta \mathbb{E}[ r(W,Z)] +\frac{\eta^2 \sigma^2}{2}$    \\
     \hline 
     Sub-exponential Condition & $\log \mathbb{E}\left[ e^{ -\eta r(W,Z)} \right] \leq -\eta \mathbb{E}[ r(W,Z)] +\frac{\eta^2 \nu^2}{2}$  for  $|\eta|\leq 1/\alpha$  \\
     \hline 
     Sub-Gamma Condition & $\log \mathbb{E}\left[ e^{ -\eta r(W,Z)} \right] \leq -\eta \mathbb{E}[ r(W,Z)]  + \frac{\nu^2 \lambda^2}{2(1- \alpha \lambda)}$ for $0\leq \eta\leq 1/\alpha$ \\
     \hline 
    \end{tabular}
\end{table}

\subsection{Extensions to intermediate rates}
The regularized ERM algorithm, which involves minimizing the empirical risk function and a regularization term, is often used in practice for better statistical and computational properties of the optimization problem. In this section, we further apply the learning bound in Theorem~\ref{thm:eta-c} to the regularized ERM algorithm with the following optimization problem:
\begin{align*}
    w_{\sf{RERM}} = \argmin_{w \in \mathcal{W}} \hat{L}(w,\mathcal{S}_n) + \frac{\lambda}{n}g(w),
\end{align*}
where $g : \mathcal{W} \rightarrow \mathbb{R}$ denotes the regularizer function and $\lambda$ is some coefficient. We define $\hat{\mathcal{R}}_{\textup{reg}}(w,\mathcal{S}_n) = \hat{\mathcal{R}}(w,\mathcal{S}_n) + \frac{\lambda}{n}(g(w) - g(w^*))$, then we have  the following lemma.
\begin{corollary}\label{coro:rerm}
Suppose Assumption~\ref{assump:eta_c_r} hold and also assume $|g(w_1) - g(w_2)| \leq B$ for any $w_1$ and $w_2$ in $\mathcal{W}$ with some $B >0$. Then for the regularized ERM hypothesis $W_{\sf{RERM}}$:
\begin{align*}
     \mathbb{E}_{W}[\mathcal{R}(W_{\sf{RERM}})] \leq & \frac{1}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}_{\textup{reg}}\left(W_{\sf{RERM}}, \mathcal{S}_{n} \right)]  +\frac{\lambda B}{cn} + \frac{1}{c\eta n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i). 
\end{align*}
\end{corollary}
The proof of this result is given in Appendix \ref{proof:rerm}. As $\hat{\mathcal{R}}_{\textup{reg}}(w,\mathcal{S}_n)$ will be negative for $w_{\sf{RERM}}$, the regularized ERM algorithm can lead to the fast rate if $I(W_{\sf{RERM}};Z_i) \sim O(1/n)$, which coincides with results in \cite{koren2015fast}.

From Theorem~\ref{thm:eta-c}, we can achieve the linear convergence rate $O(1/n)$ if the mutual information between the hypothesis and data example is converging with $O(1/n)$. To further relax the $(\eta,c)$-central condition, we can also derive the intermediate rate with the order of $O(n^{-\alpha})$ for $\alpha \in [\frac{1}{2}, 1]$. Similar to the $v$-central condition, which is a weaker condition of the $\eta$-central condition \cite{van2015fast,grunwald2020fast}, we propose the $(v,c)$-central condition  and derive the intermediate rate results in Theorem~\ref{lemma:intermediate}.

\begin{definition}[$(v,c)$-Central Condition]\label{def:weaker-eta-c}
% We say that $(\mu, \ell, \mathcal{W}, \mathcal{A})$ satisfies the $(\eta,c)$-central condition up to some $\epsilon > 0$ if the following inequality holds for the optimal hypothesis $w^*$:
% \begin{align}
% \log \mathbb{E}_{P_W\otimes \mu} & \left[e^{-{\eta}\left(\ell(W,Z)-\ell(w^*,Z)\right)}\right]  \leq  -c\eta  \mathbb{E}_{P_W\otimes \mu}\left[\ell(W,Z) - \ell(w^*,Z)\right] + \eta \epsilon. \label{eq:v-central} %\textup{ i.e., } \ell(w^*,Z)-\ell(W,Z) \unlhd^{W\otimes \mu}_{{\eta}} 0. 
% \end{align}
% Furthermore, 
Let $v:[0, \infty) \rightarrow[0, \infty)$ be a bounded and non-decreasing function satisfying $v(\epsilon)>0$ for all $\epsilon > 0$. We say that $(\mu, \ell, \mathcal{W}, \mathcal{A})$  satisfies the $(v,c)$-central condition if for all $\epsilon \geq 0$, it holds that
\begin{align}
\log \mathbb{E}_{P_W\otimes \mu} & \left[e^{-{v(\epsilon)}\left(\ell(W,Z)-\ell(w^*,Z)\right)}\right]  \leq  -cv(\epsilon)  \mathbb{E}_{P_W\otimes \mu}\left[\ell(W,Z) - \ell(w^*,Z)\right] + v(\epsilon) \epsilon. \label{eq:v-central} %\textup{ i.e., } \ell(w^*,Z)-\ell(W,Z) \unlhd^{W\otimes \mu}_{{\eta}} 0. 
\end{align}
\end{definition}

\begin{theorem}\label{lemma:intermediate}
Assume the learning tuple $(\mu, \ell, \mathcal{W}, \mathcal{A})$ satisfies the  $\left(v, c\right)$-central condition up to $\epsilon$ for some function $v$ as defined in~Def. \ref{def:weaker-eta-c} and $0 < c < 1$. Then it holds that for any $\epsilon \geq 0$ and any $0< \eta \leq v(\epsilon)$,
\begin{align*}
     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq & \frac{1-c}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)] + \frac{1}{n} \sum_{i=1}^{n} \left( \frac{1}{\eta c}I(W;Z_i) + \frac{\epsilon}{c}\right).
 \end{align*}
\end{theorem}

We prove this result in Appendix \ref{proof:intermediate}.  In particular, this result implies that if $v(\epsilon) \asymp \epsilon^{1-\beta}$ for some $\beta \in [0,1]$, then the generalization error is bounded by,
\begin{align*}
     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq & \frac{1-c}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)]  + \frac{(1-\beta)^{\frac{1}{2-\beta}}(2-\beta)}{nc(1-\beta)}\sum_{i=1}^{n} I(W;Z_i)^{\frac{1}{2-\beta}}.
 \end{align*}

%{\color{red} [can you double check the result above, by plugging $\eta'=\epsilon^{1-\beta}$, I have the optimal $\epsilon^*=(I(1-\beta))^{1/(2-\beta)}$  and the mutual information term is something like $I^{1/(2-\beta)}$ multiplied by some term involving $\beta$ (probably $(1-\beta)^{1/(2-\beta)}(\frac{2-\beta}{1-\beta})$]}
 
Thus, the expected generalization is found to have an order of $I(W;Z_i)^{\frac{1}{2-\beta}}$, which corresponds to the results under Bernstein's condition \cite{hanneke2016refined,mhammedi2019pac,grunwald2021pac}. 

% \begin{lemma}
% Assume that,
% \begin{align*}
%     \log \mathbb{E}_{P_W\otimes \mu} \left[ e^{\eta(\mathbb{E}[r(W,Z_i)] - r(W,Z_i))} \right] \leq \frac{\sigma^2 \eta^2}{2} + \epsilon \eta.
% \end{align*}
% With the conditions in Theorem~\ref{thm:sub-Gaussianv2}, we have,
% \begin{align}
%      \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq & \frac{1-a_\eta}{a_\eta} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] \\
%      &+ \frac{1}{n} \sum_{i=1}^{n}  \left( \frac{1}{\eta a_\eta}I\left(W ; Z_{i}\right) + \epsilon\right). \label{eq:sub-Gaussianv2}
% \end{align}
% \end{lemma} 


% If $\epsilon \asymp \eta^{\alpha}$ for $\alpha \in [0,\infty)$, then the expected  generalization has the order of $I(W;Z_i)^{-\frac{\alpha}{1+\alpha}}$, achieving the intermediate rate for different $\alpha$.



% \subsection{Fast Rate with Regularized ERM}
% In this section, we focus on a specific loss function, namely the logarithmic loss. For a hypothesis $w\in\mathcal W$, the empirical risk of $w$ on a training sequence $ \mathcal{S}_n$ with regularization is defined as
% \begin{align}
% \hat L_{\textup{reg}}(w, \mathcal{S}_n):&= \hat L(w,\mathcal{S}_n) + \lambda R(w,\mathcal{S}_n) \\
%             &= \frac{1}{m}\sum_{i=1}^m\ell(w,Z_i) + \lambda R(w,\mathcal{S}_n).
% \label{eq:erm_risk}
% \end{align} 
% where $R(w,\mathcal{S}_n)$ is a regularization term that can be dependent on the data sample. The learning algorithm, which can be viewed as a stochastic mapping from the data space to the hypothesis space, defines a distribution $P_{W|\mathcal{S}_n}$ over the hypothesis space $\mathcal{W}$ given the dataset $\mathcal{S}_n$. For example, the strategy minimising $\hat L_{\textup{reg}}(w, \mathcal{S}_n)$ is also known as the regularized ERM algorithm, which can be implemented using Gibbs algorithm, stochastic gradient descent, etc. Then the expected generalization error is defined as
% \begin{align}
% \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)]:= \Esub{W\mathcal{S}_n}{L_{\mu}(W)-\hat L_{\textup{reg}}(W,\mathcal{S}_n)}
% \label{eq:gen_error_typical}
% \end{align}
% A small empirical error implies a small expected error, hence the generalization. Now we write it out,
% \begin{align*}
%     &\Esub{W\mathcal{S}_n}{\mathcal{E}(W,\mathcal{S}_n)} \\
%     &= \Esub{W\mathcal{S}_n}{L_{\mu'}(W)-(1+\lambda)\hat L(W,\mathcal{S}_n) + \lambda(\hat L(W,\mathcal{S}_n) - R(W,\mathcal{S}_n))} \\
%     &= \Esub{W\mathcal{S}_n}{L_{\mu'}(W)-(1+\lambda)\hat L(W,\mathcal{S}_n)} \\
%     &\quad + \lambda\Esub{W\mathcal{S}_n}{\hat L(W,\mathcal{S}_n) - R(W,\mathcal{S}_n)}
% \end{align*}
% The generalization error will be decomposed into two terms, the former describes the generalization error and the latter term can be intuitively interpreted as the difference between prior knowledge induced by the regularization and its posterior induced by the data. For example, if we set $R(w,S) = \|w\|^2$ as prior and $\ell(w,Z_i) = \|w - Z_i\|^2$ as posterior by $\ell_2$ norm for mean estimation problem, the difference can be regarded as the divergence between two Gaussian distributions. We may exploit it more by giving specific learning scenarios.

% Equipped with the definitions above, we will then arrive at the following theorem.
% \begin{theorem}\label{thm:reg_erm}
% Assume that $\ell(w, Z)$ is $\sigma^{2}$-sub-Gaussian for all $w$ under the distribution $P_Z$. Also assume that for any $w$ and $P_Z$, the moment generating function of expected loss function $\Esub{P_Z}{e^{t\ell(w,Z)}}$ is lower bounded by $e^{tL}$ for all $t>0$ and some $L>0$. Then for any $\lambda > 0$ and any algorithm $P_{W|S}$, it holds that 
% \begin{equation}
%     \Esub{W\mathcal{S}_n}{\mathcal{E}(W,\mathcal{S}_n)} \leq \frac{\sigma^2}{2L\lambda n} \sum_{i=1}^{n} I(W;Z_i) + \lambda D_R(W,\mathcal{S}_n) \label{eq:}
% \end{equation}
% where $D_R(W,\mathcal{S}_n) = \Esub{W\mathcal{S}_n}{\hat L(W,\mathcal{S}_n) - R(W,\mathcal{S}_n)}$. 
% \end{theorem}

% Note that the theorem above is applicable to any algorithm $P_{W|\mathcal{S}_n}$, but we may be more interested in regularized ERM algorithm. The hyper-coefficient $\lambda$ plays a role as a trade-off between data-fitting and regularization. Particularly, if $\lambda$ is large, which means we will penalize more for the hypothesis (prior knowledge), the second term will dominate the bound. On the other hand, if $\lambda$ is small, then the first term, that describes whether the learned $W$ is overfitting the single data, will prevail instead. For a fixed $\lambda$, we may hope that both $I(W;Z_i)$ and $D_R(W;\mathcal{S}_n)$ are small simultaneously under a certain algorithm.

% Next we consider a special case where we let $R(W,\mathcal{S}_n) = \hat{L}(W,\mathcal{S}_n)$, then the bound becomes 
% \begin{align}
% \Esub{W\mathcal{S}_n}{\mathcal{E}(W,\mathcal{S}_n)} \leq \frac{1}{\lambda n} \sum_{i=1}^{n} \frac{\sigma^2}{2L} I(W;Z_i)
% \end{align}
% solely. Different from classical previous result that attains $\sqrt{I(W;Z_i)}$ \cite{xu2017information}, we arrive at a faster rate that depends on the regularization coefficient $\lambda$. If $\lambda$ goes to infinity, the bound becomes trivial since the LHS of~Eq~(\ref{eq:1}) will approach to negative infinity while the RHS will converge to zero if $I(W;Z_i)$ is finite. Hence we may be more interested in the case where $\lambda$ is arbitrarily small, where it approaches the standard ERM.  Furthermore, if we consider the transfer learning and set $R(W,\mathcal{S}_n)$ that depends on some hypothesis $W_S$ learned from source data, e.g., typically known as hypothesis transfer learning (HTL), then the source data can be regarded as the regularization. 

% \begin{table*}
%     \centering
%     \begin{tabular}{|c|c|c|}
%     \hline 
%      Condition      &  Key Inequality  & Additional Assumptions \\
%      \hline 
%      Bernstein Condition    &  $\log e^{\eta(\mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] - r(W,Z_i))} \leq  \frac{1}{2} \eta \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]$
%     &     $r(W,Z_i)$ is lower bounded by $-b$  \\
%      \hline 
%      Central Condition    &  $\log e^{\eta(\mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] - r(W,Z_i))} \leq  (1-\frac{1}{c_u})\eta\mathbb{E}_{P_{W}\otimes \mu}[r^2(W,Z_i)]$    &    Witness Condition \\
%      \hline
%      sub-Gaussian Condition  & $\log e^{\eta(\mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] - r(W,Z_i))} \leq \frac{\eta^2 \sigma^2}{2}$     &     \\
%      \hline 
%      Central Condition Only &  $\log e^{\eta(\mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] - r(W,Z_i))}  \leq \eta \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] $    &     \\
%      \hline 
%     \end{tabular}
%     \caption{Technical Results Comparisons}
% \end{table*}
%\subsection{Excess Risk Bound}
%\subsection{Comparison with Other Bounds}
%\subsection{Auxiliary Distribution}

\section{Examples}
In this section, we present several additional examples of fast rates that satisfy the $(\eta,c)$-central condition. In certain examples, the convergence rate may be faster than $O(1/n)$, for instance, when the mutual information $I(W;Z_i)$ exhibits an exponential convergence. In addition, we examine two supervised learning problems: linear regression, a simple problem, and logistic regression, a slightly more complicated problem, both of which demonstrate a convergence rate of $O(1/n)$. For completeness, we have also included some exceptional instances where the $(\eta,c)$-central condition is not satisfied, despite being relatively uncommon in learning problems.

\subsection{Gaussian Mean Estimation with Discrete $\mathcal{W}$}\label{sec:discretehypothesis}
In the Gaussian mean estimation problem, instead of considering $\mathcal{W} = \mathbb{R}$, we now consider an example where we assume $z_i \sim \mathcal{N}(w,1), i = 1,\cdots, n$ for some $w$ in the hypothesis space $\mathcal{W}$ that has finite elements, e.g., $\mathcal{W} = \{-1, 1\}$. We assume $w = 1$ is the true mean. Define the loss function to be the squared loss of $\ell(w,z) = (w-z)^2$ and define the empirical risk minimization problem
\begin{align}
    w_{\textup{ERM}} &= \argmin_{W \in \mathcal{W}} \frac{1}{n}\sum_{i=1}^{n}{(w - z_i)^2},
\end{align}
which is equivalent to the maximum likelihood estimation in this case. Similar to Example~\ref{eg:gaussian_mean_zero}, the ERM algorithm produces the decision rule by:
\begin{align*}
    w_{\ERM} = \begin{cases}
1, \textup{ if } \frac{1}{n}\sum_{i=1}^{n}Z_i \geq 0, \\
-1, \textup{ otherwise.}
\end{cases}
\end{align*} 
As $\frac{1}{n}\sum_{i=1}^{n}Z_i \sim \mathcal{N}(1, \frac{1}{n})$,  we then have that,
\begin{align*}
    P(W_{\ERM} = 1) &= 1 - Q(\sqrt{n}) \leq 1 - \frac{\sqrt{n}}{1+n} \frac{1}{\sqrt{2\pi}} \exp(-\frac{n}{2}),
\end{align*}
where $Q(\cdot)$ is the tail distribution function of the standard normal distribution, and the inequality follows as $\frac{x}{1+x^2}\phi(x) \leq Q(x) < \frac{\phi(x)}{x}$ for any $x > 0$ where $\phi(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$ \cite{borjesson1979simple}. Similarly, we have, 
\begin{align*}
    P(W_{\ERM} = -1) = Q(\sqrt{n}) &\leq \frac{1}{\sqrt{2\pi n}}  \exp(-\frac{n}{2}).
\end{align*}
Then we can calculate the expected risk and the excess risk by,
\begin{align*}
    \mathbb{E}_{P_W\otimes \mu}[\ell(W_{\ERM},Z)] &= P(w_{\ERM} = 1)\mathbb{E}_{Z}[(Z-1)^2] + P(w_{\ERM} = -1)\mathbb{E}_{Z}[(Z+1)^2]  \\
    &= 1 + 4Q(\sqrt{n}) \\
    & \leq 1 + \frac{4}{\sqrt{2\pi n}}\exp(-\frac{n}{2}).
\end{align*}
It is easily calculated that $w^* = 1$ in this case and 
\begin{align*}
    \mathbb{E}_{Z}[\ell(w^*,Z)] &= \mathbb{E}_{Z}[(Z-1)^2] = 1.
\end{align*}
Then the expected excess risk is calculated as,
\begin{align*}
    \mathbb{E}_{P_W\otimes \mu}[\ell(W_{\ERM},Z)] - \mathbb{E}_{Z}[\ell(W^*,Z)]  = 4Q(\sqrt{n}) \leq  \frac{4}{\sqrt{2\pi n}}\exp(-\frac{n}{2}) = O(\frac{e^{-\frac{n}{2}}}{\sqrt{n}}).
\end{align*}
The expected generalization error is given by,
\begin{align*}
    \mathbb{E}_{P_W\otimes \mu}\left[\ell(W_{\ERM},Z_i)\right] - \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{WZ_i}\left[\ell(W_{\ERM},Z_i)\right]  &= 2\mathbb{E}_{W_{\ERM}\mathcal{S}_n}\left[W\frac{1}{n}\sum_{i=1}^{n}Z_i\right] - 2\mathbb{E}_W\left[W_{\ERM}\right]\\
    &= 2\mathbb{E}_{\mathcal{S}_n}\left[\left|\frac{1}{n}\sum_{i=1}^{n}Z_i\right|\right] - 2\mathbb{E}_W\left[W_{\ERM}\right] \\
    &= \sqrt{\frac{8}{n\pi}} \exp(-\frac{n}{2}) = O(\frac{e^{-\frac{n}{2}}}{\sqrt{n}}).
\end{align*}
where the first equality follows by expanding the squared loss $\ell(w,z)$, and the second equality follows as $W_{\ERM}$ has the same sign as $\sum^n_{i=1}Z_i$. The third equality is from the following calculation where:
\begin{align}
    \mathbb{E}_W[W_{\ERM}] = 1 - 2Q(\sqrt{n})
\end{align}
and 
\begin{align}
\mathbb{E}_{\mathcal{S}_n}\left[\left|\frac{1}{n}\sum_{i=1}^{n}Z_i\right|\right] = \sqrt{\frac{2}{n\pi}} \exp(-\frac{n}{2}) + 1 - 2Q(\sqrt{n})
\end{align}
as $\left|\frac{1}{n}\sum_{i=1}^{n}Z_i\right|$ is the folded Gaussian distribution with the parameters $\mu = 1$ and $\sigma = \frac{1}{\sqrt{n}}$. Now we evaluate our mutual information bound and compare it to the true excess risk and generalization error. For a fixed $z_i$, we have,
\begin{align*}
     P(W_{\ERM}=1|z_i) &= P(\frac{1}{n-1}\sum_{j\neq i} Z_j \geq - \frac{1}{n-1}z_i) \\
     %&= 1 - P(\frac{1}{n-1}\sum_{j\neq i} Z_j \leq - \frac{1}{n-1}z_i) \\
     &= 1 - Q(\frac{z_i}{\sqrt{n-1}} + \sqrt{n-1}).
\end{align*}
We then calculate the mutual information for a large $n$ by:
\begin{align*}
    I(W_{\ERM};Z_i) &= H(W_{\ERM}) - H(W_{\ERM}|Z_i) \\
    &= h_2(Q(\sqrt{n})) - \mathbb{E}_{Z_i}\left[h_2(Q(\frac{Z_i}{\sqrt{n-1}} + \sqrt{n-1}))\right] \\
    &= h_2(Q\sqrt{n}) - \mathbb{E}_{Z_i \leq -(n-1)}\left[h_2(Q(\frac{Z_i}{\sqrt{n-1}} + \sqrt{n-1}))\right] - \mathbb{E}_{Z_i > -(n-1)}\left[h_2(Q(\frac{Z_i}{\sqrt{n-1}} + \sqrt{n-1}))\right] \\
    &\leq h_2(Q(\sqrt{n})) - h_2(Q(\frac{\mathbb{E}_{Z_i > -(n-1)}[Z_i]}{\sqrt{n-1}} + \sqrt{n-1})) \\
    &= O(\frac{e^{-\frac{n}{2}}}{\sqrt{n}})
\end{align*}
where the inequality follows Jensen's inequality with the fact that $Q$-function is locally convex for a large $n$ as $Q''(x) = \frac{x}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ and $h_2$ is a concave function. %We next examine the sub-Gaussian condition as follows:
%\begin{align*}
%\mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W,Z)}] &= P(w=1) \mathbb{E}_{Z}[e^{\eta (Z-1)^2}] + P(w=-1) \mathbb{E}_{Z}[e^{\eta (Z+1)^2}] \\
%&= (1 - Q(\sqrt{n}))\mathbb{E}_{Z}[e^{\eta (Z-1)^2}]  + Q(\sqrt{n})\mathbb{E}_{Z}[e^{\eta (Z+1)^2}],
%\end{align*}
%with the calculations that,
%\begin{align*}
%    \mathbb{E}_{\mu}[e^{\eta (Z-1)^2}] 
    %&= \int \frac{1}{\sqrt{2\pi}}e^{-\frac{(z-1)^2}{2}}e^{\eta (z-1)^2} dz \\
    %&= \int \frac{1}{\sqrt{2\pi}} e^{-(\frac{1-2\eta}{2}) (z-1)^2} dz \\
    %&= \frac{1}{\sqrt{2\pi}} * \sqrt{2\pi \frac{1}{1-2\eta}} \\
%    &= \sqrt{\frac{1}{1-2\eta}},\\
%    \mathbb{E}_{\mu}[e^{\eta (Z+1)^2}] %&= \int \frac{1}{\sqrt{2\pi}}e^{-\frac{(z-1)^2}{2}}e^{\eta (z+1)^2} dz \\
    %&= \int \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{z^2-2z+1}{2} + \eta z^2 + 2\eta z + \eta \right) dz  \\
    %&= \int \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(1-2\eta)z^2-(2 + 4\eta)z+1}{2} + \eta \right) dz  \\  
    %&= \int \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(1-2\eta)z^2-(2 + 4\eta)z + \frac{(1+2\eta)^2}{1-2\eta}}{2} + \eta - \frac{1}{2} + \frac{(1+2\eta)^2}{2(1-2\eta)} \right) dz \\
    %&= \frac{1}{\sqrt{2\pi}} \sqrt{2\pi \frac{1}{1-2\eta}} \exp\left(\eta - \frac{1}{2} + \frac{(1+2\eta)^2}{2(1-2\eta)}\right) \\
 %   &= \sqrt{\frac{1}{1-2\eta}} \exp\left( \frac{4\eta}{1-2\eta}\right),
%\end{align*}
%we have,
%\begin{align*}
%    \mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W,Z)}] = (1 - A\exp(-Bn))\sqrt{\frac{1}{1-2\eta}}  + A\exp(-Bn)\sqrt{\frac{1}{1-2\eta}} \exp\left( \frac{4\eta}{1-2\eta}\right). 
%\end{align*}
%\begin{align*}
%    \log \mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W,Z)}] = -\frac{1}{2}\log (1-2\eta) + \log \left( 1 -Q(\sqrt{n}) + Q(\sqrt{n}) \exp\left( \frac{4\eta}{1-2\eta}\right) \right)
%\end{align*}
%We also have
%\begin{align*}
%    \mathbb{E}_{P_W\otimes \mu}[\eta\ell(W,Z)] &=  \eta(1 + 4Q(\sqrt{n})).
%\end{align*}
By writing the excess risk as $r(w,z) = \ell(w,z) - \ell(w^*,z) = (w - z)^2 - (1 - z)^2$, its moment generating function is calculated by:
\begin{align*}
\mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W_{\ERM},Z)}] &= P(W_{\ERM}=1) + P(W_{\ERM}=-1) \mathbb{E}_{Z}[e^{-4\eta Z}] \\
&= 1 - Q(\sqrt{n})  + Q(\sqrt{n})\mathbb{E}_{Z}[e^{-4\eta Z}].
\end{align*}
We can then calculate
\begin{align*}
    \mathbb{E}_{Z}[e^{-4\eta Z}] %&= \int \frac{1}{\sqrt{2\pi}}e^{-\frac{(z-1)^2}{2}}e^{4\eta z} dz \\
    %&= \int \frac{1}{\sqrt{2\pi}}\exp\left( {-\frac{(z-1)^2}{2}} + {4\eta z}\right) dz \\
    %&= \int \frac{1}{\sqrt{2\pi}}\exp\left( {-\frac{z^2- (2+8\eta) z + 1}{2}} \right) dz  \\
    %&= \int \frac{1}{\sqrt{2\pi}}\exp\left( {-\frac{z^2- (2+8\eta) z + (1+4\eta)^2}{2}} + 8\eta^2 + 4\eta \right) dz  \\
    &= e^{8\eta^2 - 4\eta}
\end{align*}
and 
\begin{align*}
    \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W_{\ERM},Z)}] = \log (1 -Q(\sqrt{n})  + Q(\sqrt{n})\exp(8\eta^2 - 4\eta)).
\end{align*}
%Therefore,
%\begin{align*}
%    \log \mathbb{E}_{P_W\otimes \mu}[e^{\eta r(W,Z)}] - \eta \mathbb{E}_{P_W\otimes \mu}[r(W,Z)] &\leq Q(\sqrt{n})(\exp(8\eta^2 + 4\eta) - 1) - 4\eta Q(\sqrt{n}) \\
%    &\leq (\exp(8\eta^2 + 4\eta) - 4\eta - 1) Q(\sqrt{n}) 
%\end{align*}
Therefore, we can check the $(\eta,c)$-central condition with $\log(1+x) \leq x$ for any $x> -1$:
\begin{align*}
    \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W_{\ERM},Z)}] &\leq   Q(\sqrt{n}) (e^{8\eta^2-4\eta}-1) \\
    &= -4c\eta Q(\sqrt{n}).
\end{align*}
where we can choose $c = \frac{1-e^{8\eta^2-4\eta}}{4\eta} \leq 1-2\eta$ with selecting any $\eta \in (0,\frac{1}{2})$. Then the bound in Theorem \ref{thm:eta-c} shows that:
\begin{align*}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W_{\ERM},\mathcal{S})] \leq \frac{1}{c\eta n}\sum^{n}_{i=1}I(W_{\ERM};Z_i) = O\left(\frac{e^{-\frac{n}{2}}}{\sqrt{n}}\right),
\end{align*}
which decays exponentially. Such a bound matches the true convergence rate, and this example shows that with the $(\eta,c)$-central condition, the convergence rate is dominated by the mutual information between the hypothesis and the instances.


% \subsection{Mean Estimation with Exponential Distribution}
% Let $Z_i \sim \textup{exp}(\frac{1}{\lambda})$, e.g., $P(Z_i) = \frac{1}{\lambda} e^{-\frac{Z_i}{\lambda}}$ so that $\mathbb{E}[Z_i] = \lambda$. Let the loss function be the log-loss, e.g., $\ell(w,z_i) = \log w + \frac{z_i}{w}$ and ERM gives the solution as $w = \frac{1}{n}\sum_{i=1}^{n}z_i$, which is ($n, \frac{n}{\lambda}$)-Gamma distributed. Then we can calculate the generalization error by:
% \begin{align*}
%     \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] - \mathbb{E}_{P_{WZ_i}}[\ell(W,Z_i)] &= \lambda^2 +\frac{\lambda^2}{n} - \lambda^2  + \frac{\lambda^2}{n} \\
%     &= \frac{2\lambda^2}{n}.
% \end{align*}
% Then
% \begin{align*}
%     \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] = -h(W) + \frac{n}{n-1} \frac{1}{\lambda} \lambda = -h(W) + \frac{n}{n-1} 
% \end{align*}
% Here $W$ is $(n, \frac{n}{\lambda})$-Gamma distributed:
% \begin{align*}
%     dP_W=\frac{(n/\lambda)^n}{\Gamma(n)} w^{n-1} e^{-(n/\lambda) w}
% \end{align*}
% Given $Z_i$, with the fact that $W - \frac{1}{n}Z_i$ is ($n-1,\frac{n}{\lambda}$)-Gamma distributed, we have the density as: 
% \begin{align*}
%     dP_{W|Z_i}=\frac{(n/\lambda)^{n-1}}{\Gamma(n-1)} (w-\frac{1}{n}z_i)^{n-2} e^{-(n/\lambda) (w-\frac{1}{n}z_i)}
% \end{align*}
% Therefore:
% \begin{align*}
%     f'\left(\frac{dP_{W|Z_i}}{P_W} \right) &= \log \left(\frac{\Gamma(n)}{\Gamma(n-1)}\frac{\lambda }{n} \frac{(w-\frac{1}{n}z_i)^{n-2}}{w^{n-1}} e^{z_i/\lambda}\right) + 1 \\
%     %&= \log \frac{n-1}{n} + (n-2)\log(w-\frac{1}{n}z_i) -  (n-2) \log w + \log \frac{\lambda}{w} + \frac{z_i}{\lambda} + 1 \\
%     &= \log \frac{n-1}{n} + (n-2)\log \left(1 - \frac{z_i}{nw} \right) + \log \frac{\lambda}{w} + \frac{z_i}{\lambda} + 1 \\
%     &= \frac{z_i}{\lambda} + \log \lambda - \log w - (n-2)\log \frac{w- \frac{z_i}{n}}{w} + 1 + \log \frac{n-1}{n}.
% \end{align*}
% The entropy of the hypothesis is:
% \begin{align*}
%     h(W) = n + \ln \frac{\lambda}{n}+\ln \Gamma(n) +(1-n) \psi(n),
% \end{align*}
% where $\psi(\cdot)$ denotes the digamma function. We can also calculate the conditional entropy $h(W|Z_i) = n -1 + \ln \frac{\lambda}{n} + \ln \Gamma(n-1) + (2-n)\psi(n-1)$. Hence the mutual information can be calculated as:
% \begin{align*}
%     I(W;Z_i) &= h(W) - h(W|Z_i) \\
%     &= 1 + \log n + (1-n)\psi(n) - (2-n)\psi(n-1).
% \end{align*}
% For large $n$, we use the approximation as $\psi(n) = \log n - \frac{1}{2n} + o(\frac{1}{n})$ and we can calculate:
% \begin{align*}
%     I(W;Z_i) &= 1 + \log n + (1-n)\psi(n) - (2-n)\psi(n-1)
%     = O(\frac{1}{n}).
% \end{align*}
% By checking the $(\eta,c)$-central condition, $r(w,z_i) = \log w- \log \lambda + \frac{z_i}{w} - \frac{z_i}{\lambda}$.
% \begin{align*}
%     \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}] = .
% \end{align*}
% \begin{align*}
%     \mathbb{E}_{P_W\otimes \mu}[r(W,Z)] = \psi(n) - \log \frac{n}{\lambda} - \log \lambda + \frac{1}{(n-1)\lambda} = O(\frac{1}{n}).
% \end{align*}

\subsection{Linear Regression}
We now extend the Gaussian mean estimation example to the linear regression problem where we have the instance space $\mathcal{Z} = \mathcal{X} \times \mathcal{Y}$ where $\mathcal{X} \subseteq \mathbb{R}^{d}$ represents the feature space and $\mathcal{Y} \subseteq \mathbb{R}$ represents the label space. We consider the linear regression model with the case $\mathcal{X} \subseteq \mathbb{R}$ for simplicity such that the label is generated in the following way:
\begin{align}
    Y_i = w^*X_i + \epsilon_i
\end{align}
where $w^* \in \mathbb{R}$ denotes the underlying (but unknown) hypothesis and $\epsilon_i$ are the noises i.i.d. drawn from some zero-mean distribution. We consider the loss function $\ell(w,z_i) = (y_i - wx_i)^2$. Then the ERM solution can be calculated as:
\begin{align*}
    w_{\ERM} = \frac{\sum_{i=1}^{n} x_iy_i}{\sum_{i=1}^{n} x^2_i} = w^* + \sum_{i}\frac{x_j}{\sum_{j}x^2_j}\epsilon_i.
\end{align*}
We consider the fixed design such that $x_i$ is not randomized where the optimal hypothesis is $w^*$ and we assume $x_i \neq x_j$ for $i \neq j$. Assume $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$, we can then calculate the expected loss over $\epsilon$ as:
\begin{align*}
    \mathbb{E}_{Z_i}[\ell(w^*,Z_i)] = \mathbb{E}_{\epsilon_i}[(w^*x_i + \epsilon_i - w^*x_i)^2] = \mathbb{E}_{\epsilon_i}[ \epsilon_i^2] = \sigma^2.
\end{align*}
The expected loss under $w_{\ERM}$ can be calculated as:
\begin{align*}
    \mathbb{E}_{P_W\otimes Z_i}[\ell(W_\ERM,Z_i)] &= \mathbb{E}_{\epsilon \otimes \epsilon'_i}[(w^*x_i + \epsilon'_i - W_\ERM x_i)^2] \\
    &= \mathbb{E}_{\epsilon_j \otimes \epsilon'_i}[(\epsilon'_i - \sum_{k}\frac{x_ix_k}{\sum_{j}x^2_j}\epsilon_k )^2]  \\
    &= \sigma^2 + \frac{x^2_i}{\sum_{j} x^2_j}\sigma^2    
\end{align*}
Therefore the excess risk can be calculated as:
\begin{align*}
    \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{P_W\otimes \mu}[\ell(W_\ERM,Z_i)] -  \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{Z_i}[\ell(w^*,Z_i)]  = \frac{1}{n}\sigma^2,
\end{align*}
which scales with $O(\frac{1}{n})$. For the generalization error, we have,
\begin{align*}
   \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{P_W\otimes Z_i}[\ell(W_\ERM,Z_i)] - \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{WZ_i}[\ell(W_\ERM,Z_i)]  &= \frac{1}{n}(\sigma^2 + \frac{1}{n}\sigma^2 - \sigma^2 - \frac{1}{n}\sigma^2 + 2\frac{\sigma^2}{n}) \\
    &= \frac{2}{n}\sigma^2,
\end{align*}
which also scales with $O(\frac{1}{n})$. Next, we can calculate the mutual information by:
\begin{align*}
    I(W;Z_i) &= h(W) - h(W|Z_i) = \frac{1}{2}\log \frac{\sum_{j}x_j^2}{\sum_{j \backslash i} x_j^2}.
    %&= \frac{1}{2}\log 2\pi e \frac{1}{\sum_{j}x_j^2}\sigma^2 - \frac{1}{2}\log 2\pi e \frac{\sum_{j \backslash i} x_j^2}{(\sum_{j}x_j^2)^2}\sigma^2 \\
\end{align*}
Then we have,
\begin{align*}
    \frac{1}{n}\sum_{i=1}^{n}I(W;Z_i) &\leq \frac{1}{2n} \sum_{i}\frac{x^2_i}{\sum_{j \backslash i}x^2_j}.
\end{align*}
%If $x_i$ is distributed uniformly, e.g., 
For $n > 3$ and any $i$, there exists a constant $c \in (0,1)$ such that $\sum_{j \backslash i} x^2_j \geq c \sum_j x^2_j$ holds. Then we can further upper bound the mutual information terms by,
\begin{align*}
    \frac{1}{n}\sum_{i=1}^{n}I(W;Z_i) &\leq \frac{1}{2n} \sum_{i}\frac{x^2_i}{c\sum_{j}{x^2_j}} = \frac{1}{2nc},
\end{align*}
which scales with $O(1/n)$. Since $\ell(W,Z_i)$ is $(1+\frac{x^2_i}{\sum_j x^2_j})\sigma^2\chi^2$ distributed, we calculate the log-moment generating function as,
\begin{align}
    \log \mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W_\ERM,Z_i)}] & = -\frac{1}{2}\log(1 - 2(1+\frac{x^2_i}{\sum_j x^2_j})\sigma^2\eta).
\end{align}
Let $\sigma_{i} = (1+\frac{x^2_i}{\sum_j x^2_j})\sigma^2$. We have,
\begin{align}
    \log\left(\mathbb{E}_{P_W\otimes \mu}[\exp{\eta (\ell(W_\ERM,Z_i)- \mathbb{E}[\ell(W_\ERM,Z_i)])} ]\right) & \leq \sigma_i^4\eta^2 \textup{ for any } \eta < 0.
\end{align}
Therefore, following \cite{bu2020tightening}, the generalization error bound becomes:
\begin{align*}
    \frac{1}{n}\sum_{i=1}^{n} \sqrt{2\sigma^4_iI(W;Z_i)} = \frac{1}{n}\sum_{i=1}^{n} \sqrt{2\left(\frac{\sum_j x^2_j + x^2_i}{\sum_j x^2_j}\right)^2 \sigma^4I(W;Z_i)} \leq \sqrt{(2-c)^2\sigma^4\frac{1}{nc}},
\end{align*}
which scales with $O(\sqrt{\frac{1}{n}})$. If we consider the excess risk $r(W,Z_i)$, we can calculate that,
\begin{align*}
    \log \mathbb{E}_{P_W\otimes \mu}\left[\exp(-\eta r(W,Z_i) )\right] = \frac{1}{2}\log \frac{\sum_{j} x^2_j / x^2_i}{\sum_{j} x^2_j / x^2_i - (4\eta^2\sigma^4 - 2\eta \sigma^2)} \leq \frac{2\eta^2 \sigma^4 - \eta \sigma^2}{\sum_j x^2_j / x^2_i} \leq -c\eta \frac{\sigma^2 x^2_i}{\sum_{j}x^2_j}.
\end{align*}
Hence the $(\eta,c)$ is satisfied for $c \leq 1 - 2\eta\sigma^2$ and $\eta \leq \frac{1}{4\sigma^2}$. By selecting $\eta = \frac{1}{4\sigma^2}$ and $c = \frac{1}{2}$, the bounds for the ERM (by ignoring the empirical excess risk terms) become:
\begin{align*}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq \frac{1}{\eta c n} \sum_{i=1}^{n} I(W;Z_i) \leq \frac{8\sigma^2}{n} \sum_{i=1}^{n} I(W;Z_i) 
\end{align*}
which scales with $O(\frac{1}{n})$. 

%To examine the tightness of the Donsker-Varahdan representation, we can write the optimal function as:
%\begin{align*}
%    f'(W,\mathcal{S}) = \log \frac{P_{W|\mathcal{S}}}{P_W} + 1.
%\end{align*}
%Here $P_{W|\mathcal{S}} = 1$ for the maximum index $k$ and $P_{W|\mathcal{S}} = 0$ for other indices $j \neq k$. Since $Z_i$ are i.i.d. distributed, $P_W$ will be uniformly distributed over $[1:n]$. Hence,
%\begin{align*}
%    f'(W, \mathcal{S}) = \begin{cases}
 %   \log n + 1, \textup{ if $W = \argmax_{i} Z_i$.}\\
%    -\infty, \textup{ otherwise.}
%    \end{cases}
%\end{align*}
%Therefore, to achieve a valid function we can set $\ell(W,\mathcal{S}) = Z_{W}$ where $W = \argmax_{i} Z_i$. The KL divergence, in this case, is tight since:
%\begin{align*}
%    I(W;\mathcal{S}) \geq \eta \mathbb{E}_{W\mathcal{S}}[\ell(W,\mathcal{S})] - \eta^2\frac{\sigma^2}{2}.
%\end{align*}
%As $\mathbb{E}_{W\mathcal{S}}[\ell(W,\mathcal{S})]$ is $\sqrt{\log n}$ and $I(W;\mathcal{S}) = \log n$, we can choose $\eta \sim \sqrt{\log(n)}$ to achieve a tight bound. 

\subsection{Logistic Regression}\label{sec:logistic}
We apply our bound in a typical classification problem. Consider a logistic regression problem in a 2-dimensional space. For each $w \in \mathbb{R}^2$ and $z_i = (x_i,y_i) \in \mathbb{R}^{2} \times \{0,1\}$, the loss function is given by
\begin{align*}
    \ell(w,z_i) := -(y_i\log (\sigma(w^Tx_i)) + (1-y_i)\log (1 - \sigma(w^Tx_i)))
\end{align*}
where $\sigma(x) = \frac{1}{1+e^{-x}}$. Here each $x_i$ is drawn from a standard multivariate Gaussian distribution $\mathcal{N}(0,\mathbf{I}_{2})$ and Let $w^* = (0.5,0.5)$, then each $y_i$ is drawn from the Bernoulli distribution with the probability $P(Y_i = 1|x_i, w^*) = \sigma(-x^T_iw^*)$. We also restrict hypothesis space as $\mathcal{W} = \{w: \|w\|_2 < 3\}$ where $W_{\ERM}$ falls in this area with high probability. Since the hypothesis is bounded and under the log-loss, then the learning problem will satisfy the central and witness condition \cite{van2015fast,grunwald2020fast}. Therefore, it will satisfy the $(\eta,c)$-central condition. We will evaluate the generalization error and excess risk bounds in (\ref{thm:eta-c}). To this end, we need to estimate $\eta$, $c$ and mutual information $I(W_{\ERM},Z_i)$ efficiently. Hence we repeatedly generate $w_{\ERM}$ and $z_i$ and use the empirical density for estimation. Specifically, we vary the sample size $n$ from $50$ to $500$, and for each $n$, we repeat the logistic regression algorithm 500 times to generate a set of $w_\ERM$. By fixing $\eta = 0.8$ as an example, we can empirically estimate the CGF and expected excess risk with the data sample and a set of ERM hypotheses, which leads to $c \approx 0.385$. It is worth noting that from the experiments, once $\eta$ is fixed, the choice of $c$ actually does not depend on the sample size $n$, which empirically confirms the ($\eta,c$)-central condition. For the mutual information, we decompose $I(W; X,Y) =I(W; Y) + P(Y = 0)I(W; X|y = 0) + P(Y = 1)I(W; X|y = 1)$ by chain rule, and the first term can be approximated using the continuous-discrete estimator~\cite{gao2017estimating} for mutual information and the rest terms are continuous-continuous ones~\cite{moddemeijer1989estimation,kraskov2004estimating}. To demonstrate the usefulness of the results, we also compare the bounds with the true excess risk and true generalization error. The comparisons are shown in Figure~\ref{fig:logistic}. We point out that the mutual information estimate, in this case, is not a trivial task as it involves the mutual information between continuous and discrete random variables. As shown in Figure \ref{fig:logistic}, both the true generalization error and the excess risk converge as $O(\frac{1}{n})$. The bounds on the generalization error and the excess error also seem to follow the same convergence rate. However, a more rigorous study of the estimated mutual information term is required to give a conclusive statement. Furthermore, we also plot the comparison with the bound in \cite{bu2020tightening} with the form of $\frac{1}{n}\sum_{i=1}^{n}\sqrt{\frac{I(W;Z_i)}{2}}$, and the comparison shows that our bound is decaying faster as $n$ increases.

\begin{figure}[H]
\centering
\subfloat[Generalization Error]{\includegraphics[width=0.3\textwidth]{gen_bound.pdf}}\quad
\subfloat[Excess Risk]{\includegraphics[width=0.3\textwidth]  
{excess_bound.pdf}}\quad
\subfloat[Bound Comparison]{\includegraphics[width=0.26\textwidth]  
{gen_bound_compare.pdf}}
%\caption{Comparisons for testing results of true generalization error(blue), generalization error bound (green), true excess risk (orange) and excess risk bound(red). { \color{red}, for each rows, describe the parameters} we set $n = 1000$, $\beta = 0.5$, $\alpha =0.5$, $p' =w^*= 0.1$, $p = 0.9$, $w_{\ERM} = \alpha p + (1-\alpha) p' = 0.5$, $T = 100$, $K_{ST}(0) = 10$, $\eta(0) = 0.1$, $\sigma_t = \sqrt{\theta\eta(t) / t}$, $\theta = 0.001$, we also set $W(0) = 0.3$ and high probability $\delta = 0.01$, by examining the sample number $n$, $\beta$ and $W(0)$ respectively.}\label{fig:result}	%\vspace{-3mm}
\caption{We represent the true expected generalization error in (a) and true excess risk in (b) along with their bounds in Theorem~\ref{thm:eta-c}. Here we vary $n$ from 50 to 500. We also plot their reciprocals to show the rate w.r.t. sample size $n$. We also plot the generalization error bound comparisons with \cite{bu2020tightening} in (c). All results are derived by 500 experimental repeats.}\label{fig:logistic}
\end{figure}

%{\color{red}[Can we do a calculation for $n=300$, and one more calculation, say for $n=500$?, and draw two curves of $a/n$ and $b/\sqrt{n}$] }

%From the figure, we can see that both the generalization error and excess risk converge as $O(\frac{1}{n})$, and the bounds in Theorem~\ref{thm:eta-c} are tight, which capture the true behaviours with the same decay rate.


\subsection{Examples when $(\eta,c)$-central condition does not hold}
There are cases where $(\eta,c)$-central condition does not hold. One instance is when $\mathbb{E}_{P_W\otimes \mu}[r(W,Z)] = 0$ but $\log\mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}] > 0$. Notice that as $\mathbb{E}_{\mu}[{r(w,Z)}]$ is non-negative, hence requiring $\mathbb{E}_{P_W\otimes \mu}[r(W,Z)] = 0$ is the same as requiring $\mathbb{E}_{\mu}[{r(w,Z)}]=0$ for all $w$, namely all the hypotheses given by the algorithm are as good as the optimal hypothesis $w^*$. On the other hand, the CGF is not necessarily equal to zero in this case. We point out that though the exact condition $\mathbb{E}_{\mu}[{r(w,Z)}]=0$ for all $w$ seems rarely hold, it may be relevant in deep learning scenarios where one can often find many empirical minimizers that have a very low excess risk. In the following, we give a simple example where this condition holds (hence $(\eta,c)$-central condition does not hold) for the completeness of the results.


% For example, if there is more than one optimal hypothesis for the learning task and we denote the optimal set by $\mathcal{W}^*=(w^*_1, w^*_2, \cdots, w^*_k)$ (e.g., all hypotheses in the hypothesis space are optimal), and if there is an algorithm that could produce a hypothesis distributed over $\mathcal{W}^*$,  the expected excess risk will be zero with choosing an arbitrary optimal hypothesis from $\mathcal{W}^*$.  But the CGF may not necessarily be zero as we have variances in the choices of the optimal hypothesis under $P_W$. Due to the fact that these regimes are rarely encountered in actual learning problems, the excess risk is presumed to be non-zero in order to satisfy the $(\eta,c)$-central condition. 
%However, we still point this out to ensure the completeness of the results. In the following, we give an example where 

\begin{example}[Gaussian mean estimation with all hypotheses equally optimal] \label{eg:gaussian_mean_zero}
Different from the example in \ref{sec:discretehypothesis}, we consider the zero mean Gaussian case where $Z_i \sim \mathcal{N}(0,1), i = 1,\cdots, n$ and the same hypothesis space $\mathcal{W} = \{-1, 1\}$. Intuitively speaking, unlike the previous situation, even if we have abundant data, both $1$ and $-1$ are the optimal hypothesis due to the symmetry. Mathematically, let the algorithm be maximum likelihood estimation and the ERM algorithm produces the decision rule by:
\begin{align*}
W_{\ERM} = \begin{cases}
1, \textup{ if } \frac{1}{n}\sum_{i=1}^{n}Z_i \geq 0, \\
-1, \textup{ otherwise.}
\end{cases}
\end{align*} 
Then the distribution of $W_{\ERM}$ is Bernoulli distributed with $P(W_{\ERM} = 1) = P\left(\frac{1}{n}\sum_{i=1}^{n}Z_i \geq 0 \right).$ As $\frac{1}{n}\sum_{i=1}^{n}Z_i \sim \mathcal{N}(0, \frac{1}{n})$,  we have that,
\begin{align*}
    P(W_{\ERM} = 1) &= P(W_{\ERM} = -1) = \frac{1}{2} 
\end{align*}
due to the symmetry. Then we can calculate the expected risk and the excess risk by,
\begin{align*}
    \mathbb{E}_{P_W\otimes \mu}[\ell(W_{\ERM},Z)] &= P(W_{\ERM} = 1)\mathbb{E}_{Z}[(Z-1)^2] + P(W_{\ERM} = -1)\mathbb{E}_{Z}[(Z+1)^2] = 2.
\end{align*}
It is important to note that $w^* $ can be either $-1$ or $1$ in this case, and 
\begin{align*}
    \mathbb{E}_{Z}[\ell(W^*,Z)] &= \mathbb{E}_{Z}[(Z-1)^2] = 2.
\end{align*}
Then the \textbf{excess risk} is calculated as,
\begin{align*}
    \mathbb{E}_{P_W\otimes \mu}[\ell(W_{\ERM},Z)] - \mathbb{E}_{Z}[\ell(W^*,Z)]  = 0.
\end{align*}
The expected generalization error is given by,
\begin{align*}
    \mathbb{E}_{P_W\otimes \mu}[\ell(W_{\ERM},Z_i)] - \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{WZ_i}[\ell(W_{\ERM},Z_i)]  %&= 2 - \mathbb{E}_{W\mathcal{S}_n}[W^2 + 2W\frac{1}{n}\sum_{i=1}^{n}Z_i - \frac{1}{n}\sum_{i=1}^{n}Z^2_i] \\
    %&= \mathbb{E}_{W\mathcal{S}_n}[2W\frac{1}{n}\sum_{i=1}^{n}Z_i] \\
    &= 2\mathbb{E}_{\mathcal{S}_n}\left[\left|\frac{1}{n}\sum_{i=1}^{n}Z_i\right|\right] 
    = \sqrt{\frac{8}{\pi n}}
\end{align*}
due to the fact that the expectation of the absolute Gaussian r.v. with mean of $\mu$ and variance of $\sigma^2$ is $\sigma \sqrt{\frac{2}{\pi}} e^{-\frac{\mu^2}{2 \sigma^2}}+\mu\left(1-2 \Phi\left(\frac{-\mu}{\sigma}\right)\right)$.  %To calculate $\mathbb{E}_{WZ_i}[\ell(W,Z_i)]$, we can write,
%\begin{align*}
%mathbb{E}_{WZ_i}[\ell(W,Z_i)] &= \mathbb{E}_{Z_i}[\mathbb{E}_{W|Z_i}[\ell(W,Z_i)]] \\
%    &= \mathbb{E}_{Z_i}[P(w=1|Z_i)(Z_i-1)^2 + P(w=-1|Z_i)(Z_i + 1)^2]
%\end{align*}
%For a fixed $z_i$, we have,
%begin{align*}
%    P(w=1|z_i) &= P(\frac{1}{n-1}\sum_{j\neq i} Z_j \geq - \frac{1}{n-1}z_i) \\
%    &= 1 - P(\frac{1}{n-1}\sum_{j\neq i} Z_j \leq - \frac{1}{n-1}z_i) \\
%frac{1}{2} - \frac{1}{2} \textup{erf}\left( - \frac{z_i}{\sqrt{2(n-1)}} \right).
%\end{align*}
%Then, 
%begin{align*}
%    \mathbb{E}_{WZ_i}[\ell(W,Z_i)] &= \mathbb{E}_{Z_i}[P(w=1|Z_i)(Z_i-1)^2 + P(w=-1|Z_i)(Z_i + 1)^2] \\
%    &= 2 + 2A\exp\left(- B(n-1) \right)
%\end{align*}
%The \textbf{expected generalization error} is (approximately),
%begin{align*}
%    \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{WZ_i}[\ell(W,Z_i)] - \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] &= 4A\exp\left(- B(n-1) \right) - 4A\exp\left(- Bn \right) \\
%    &= 4A\exp\left(- Bn \right)(\exp(B) - 1) \sim O(e^{-n}).
%\end{align*}
%We then calculate the mutual information for large $n$ by:
%\begin{align*}
%    I(W;Z_i) &= H(W) - H(W|Z_i) = h_2(p(w = 1)) - \mathbb{E}_{Z_i}[h_2(p(w = 1 |Z_i))].
%\end{align*}
% We next examine the sub-Gaussian condition as follows:
% \begin{align*}
% \mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W,Z)}] &= P(w=1) \mathbb{E}_{Z}[e^{\eta (Z-1)^2}] + P(w=-1) \mathbb{E}_{Z}[e^{\eta (Z+1)^2}] \\
% &= \frac{1}{2}\mathbb{E}_{Z}[e^{\eta (Z-1)^2}]  + \frac{1}{2}\mathbb{E}_{Z}[e^{\eta (Z+1)^2}].
% \end{align*}
% Due to the symmetry, we can calculate
% \begin{align*}
% \mathbb{E}_{Z}[e^{\eta (Z+1)^2}] = \mathbb{E}_{Z}[e^{\eta (Z-1)^2}] &= \int \frac{1}{\sqrt{2\pi}}\exp(-\frac{z^2}{2} + \eta (z-1)^2) dz \\
%     &= \int \frac{1}{\sqrt{2\pi}} \exp(-\frac{z^2 - 2\eta z^2 + 4\eta z}{2} +  \eta) dz \\
%     &= \int \frac{1}{\sqrt{2\pi}} \exp(-\frac{(1-2\eta)z^2 + 4\eta z + \frac{4\eta^2}{1-2\eta}}{2} +  \eta + \frac{2\eta^2}{1-2\eta}) dz \\
%     &= \sqrt{\frac{1}{1-2\eta}}\exp\left( \frac{\eta}{1-2\eta}\right).
% \end{align*}
% Then we have,
% \begin{align*}
%     \log \mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W,Z)}] = -\frac{1}{2}\log (1-2\eta) + \frac{\eta}{1-2\eta}
% \end{align*}
% The CGF can be upper bounded by
% \begin{align*}
%     \log \mathbb{E}_{P_W\otimes \mu}[e^{\eta \ell(W,Z)}] - \mathbb{E}_{P_W\otimes \mu}[\eta\ell(W,Z)] &\leq \eta^2 \sigma^2
% \end{align*}
% for some constant $\sigma^2$. The bound is not tight as $\sigma^2$ does not converge as $n$ increases. 
If we choose $w^* = 1$ and the excess risk can be written as $r(w,z) = \ell(w,z) - \ell(w^*,z) = (w - z)^2 - (1 - z)^2$, now we can calculate the MGF of the excess risk explicitly as: 
\begin{align*}
\mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W_{\ERM},Z)}] &= P(W_{\ERM}=1) + P(W_{\ERM}=-1) \mathbb{E}_{Z}[e^{-4\eta Z}] = \frac{1}{2}+ \frac{1}{2}\mathbb{E}_{Z}[e^{-4\eta Z}].
\end{align*}
For the second term, we have the following:
\begin{align*}
    \mathbb{E}_{Z}[e^{-4\eta Z}] &= \int^{+\infty}_{-\infty} \frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}e^{-4\eta z} dz 
    %&= \int \frac{1}{\sqrt{2\pi}}\exp\left( {-\frac{z^2- 8\eta z + 16\eta^2}{2}} + 8\eta^2 \right) dz  \\
    = e^{8\eta^2}.
\end{align*}
Finally, it yields the CGF by, 
\begin{align*}
    \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}] = \log ( \frac{1}{2}  + \frac{1}{2}\exp(8\eta^2))
\end{align*}
for any $\eta$. In this case, the $(\eta,c)$-central condition does \textbf{not} hold as $\log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}] = \log ( \frac{1}{2}  + \frac{1}{2}\exp(8\eta^2)) \geq 0$ for any $\eta > 0$. %However, the ERM algorithm still satisfies the sub-Gaussian assumption as:
%\begin{align*}
%    \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}] = \log ( \frac{1}{2}  + \frac{1}{2}\exp(8\eta^2)) \leq 8\eta^2.
%\end{align*}
%for any $\eta$. 
\end{example}

\begin{example}[Hypothesis Selection \cite{russo2016controlling}] \label{eg:hypothesis_selection}
Let us consider a different scenario for hypothesis selection that does not satisfy the $(\eta,c)$-central condition as well. Let $\mathcal{S} = (Z_1,Z_2,\cdots, Z_n)$ where $Z_i \in \mathbb{R}$ is a random variable with the mean of $\mu_i$ and $W$ to be some selection hypothesis such that $\mathcal{W} = [1:n]$. If we seek for the largest instance $Z_i$ in the dataset $\mathcal{S}$, we define the loss function $\ell(W,\mathcal{S}) = -Z_{W}$ and simply minimising the loss function will lead to a hypothesis that produces the instance with the largest value. Then we have that $\mathbb{E}_{W\mathcal{S}}[\ell(W,\mathcal{S})] = -\mathbb{E}_{W\mathcal{S}}[Z_W]$ and $\mathbb{E}_{W\otimes \mathcal{S}}[\ell(W,\mathcal{S})] = -\mathbb{E}_{W\otimes \mathcal{S}}[Z_W] = -\mathbb{E}_{W}\mathbb{E}_\mathcal{S}[Z_W] = -\mathbb{E}_W[\mu_W]$. We can write:
\begin{align*}
    |\mathbb{E}_{W\mathcal{S}}[\ell(W,\mathcal{S})] - \mathbb{E}_{W\otimes \mathcal{S}}[\ell(W,\mathcal{S})]| = |\mathbb{E}_{W\mathcal{S}}[Z_W - \mu_W]|. %\leq 2\sqrt{\sigma I(W;\mathcal{S})}.
\end{align*}
We consider the ERM hypothesis $W = \argmin \ell(W,\mathcal{S}) = \argmax_i Z_i$ and assume all $Z_i$ are normally distributed and have the same mean of some positive $\mu$ and variance of $\sigma^2$ for simplicity. Then we have that:
\begin{align*}
I(W;\mathcal{S}) = H(W) - H(W|\mathcal{S}) = H(W) = \log n.    
\end{align*}
Since we assume $\mu_W = \mu$ for all $W$, we have $|\mathbb{E}_{W\mathcal{S}}[Z_W - \mu_W]| \leq \sqrt{2\sigma^2 \log n} = \sqrt{2\sigma^2 I(W;\mathcal{S})}$ for any $n$. Now we check the $(\eta,c)$-central condition w.r.t. the excess risk function $r(W,\mathcal{S}) = \ell(W,\mathcal{S}) - \ell(w^*, \mathcal{S})$ where $w^*$ can be any index in $[1:n]$:
\begin{align*}
    \log \mathbb{E}_{W\otimes \mathcal{S}}\left[ e^{-\eta r(W, \mathcal{S})} \right] \leq -c\eta \mathbb{E}_{W\otimes \mathcal{S}}\left[ r(W, \mathcal{S})\right]
\end{align*}
for some $c \in [0,1]$. We can then calculate that 
\begin{align*}
    \log \mathbb{E}_{W\otimes \mathcal{S}}\left[ e^{-\eta r(W, \mathcal{S})}\right] = \log \left(\frac{1}{n} + \frac{n-1}{n} \exp{\sigma^2\eta^2}\right) > 0 ,
\end{align*}
for positive $\eta$ while the expected excess risk is $\mathbb{E}_{W\otimes \mathcal{S}}\left[ r(W, \mathcal{S})\right] = 0$ and this contradicts to the proposed $(\eta,c)$-central condition. In both examples, we observe that the expected excess risk for the ERM hypothesis is always zero because each hypothesis in the class is optimal for the given distribution. However, the CGF could still be positive since we have stochasticity in the hypothesis derived, which is a rare occurrence in most learning scenarios, and we highlight this unique scenario for completeness.  
\end{example}
% which satisfies $\eta \leq \frac{2(c-1)\mu}{\sigma^2}$. Then we have the bound that:
% \begin{align*}
% |\mathbb{E}_{W\mathcal{S}}[Z_W - \mu_W]| \leq (c-1)\mu + \frac{I(W;\mathcal{S})}{\eta},
% \end{align*}
% for some $c>1$. To optimize the bound on the right-hand side, we could let $c = \log n$ and $\eta = \frac{2(c-1)\mu}{\sigma^2}$ and we have that the generalization error is $O(\sqrt{\log n})$, which is tight in this case.
% \end{example}
%Fast rate conditions are widely investigated under different learning frameworks and conditions \cite{van2015fast,mehta2017fast, koren2015fast, mhammedi2019pac,grunwald2020fast, zhu2020semi, grunwald2021pac}. We propose a crucial $(\eta,c)$-central condition that can lead to the fast rate for the generalization error in expectation, which also coincides with many existing works such as \cite{grunwald2020fast} and \cite{grunwald2021pac} for certain choices of $c$ and $\eta$. As a consequence, many loss functions satisfy. For example, with bounded loss, $\beta = 1$ in Bernstein condition is equivalent to strong central condition with the witness condition for fast rate, from which $(\eta,c)$-central condition follows. With unbounded loss, the log-loss will satisfy the central and witness conditions under well-specified model \cite{wong1995probability,grunwald2020fast}, which also consequently implies the $(\eta,c)$-central condition. As the most relevant work, our bound has a similar form as \cite{grunwald2021pac} which applies the conditional mutual information \cite{steinke2020reasoning}, but their results are derived under the PAC-Bayes framework and requires the prior knowledge over hypothesis. Our result applies for  general algorithms with mutual information and our assumptions are weaker since we only requires the proposed conditions hold in expectation w.r.t. $P_W$, instead of for all $w \in\mathcal{W}$. Another advantage of our results is that different metrics and data-processing techniques can be used to further tighten the bound to improve the convergence factors, see \cite{jiao2017dependence,hafez2020conditioning, zhou2022individually} for examples.
%\cite{grunwald2020fast},\cite{grunwald2021pac},\cite{van2015fast},\cite{mhammedi2019pac},\cite{steinke2020reasoning},\cite{mehta2017fast}, \cite{zhu2020semi}


    %\item For bounded loss, Bernstein condition implies $v$-central condition, see Theorem 5.4 in \cite{van2015fast}.
    %\item Bernstein condition implies witness condition for unbounded loss for $\beta = 1$ [A weaker $\tau$-witness condition also holds for other $\beta$], see Proposition 18 in \cite{grunwald2020fast}.
    %\item Strong central condition $\rightarrow$ $v$-central condition.
% We also summarize the key technical conditions in Table~\ref{tab:tech} for arriving the technich.

\section{Conclusion}
As we demonstrate in this paper, if the sub-Gaussian assumption is made regarding the excess loss in the typical information-theoretic generalization error bounds, the square root does not necessarily prevent us from the fast rate. On top of that, we identify the key conditions that lead to the fast and intermediate rate bound in expectation. Intuitively speaking, to achieve a fast rate bound for both the generalization error in expectation, the output hypothesis of a learning algorithm must be ``good" enough compared to the optimal hypothesis $w^*$. Here we encode the notion of goodness in terms of the CGF by controlling the gap between $\ell(w,Z)$ and $\ell(w^*,Z)$.  Further, we verify the proposed bounds and present the results analytically with examples. We remark that there is some room for future work to improve the bounds. One possible direction is to develop novel techniques for removing the empirical excess risk term in the bound for general algorithms other than ERM or regularized ERM. Additionally, in most cases, $w^*$ is usually not known but one could possibly seek a reference hypothesis $\hat{w}$ that could be trained from the sample and close to $w^*$, allowing more flexibility of the bounds in real applications.


\section*{Acknowledgement}
The preliminary version of this work is presented at the ITW2022 conference, we greatly appreciate useful feedback and comments from all reviewers. This research is supported by Melbourne Research Scholarships (MRS).

% \begin{itemize}
%     \item In-expectation generalization error and in-probability generalization error.
%     \item 
% \end{itemize}
% \section{Fast Rate on Transfer Learning} \label{sec:transfer}
% We consider the transfer learning in this section. Let $\mu$  and $\mu'$ be two probability distributions defined on $\mathcal Z$, and  assume that $\mu$ is \emph{absolute continuous} with respect to $\mu'$ ($\mu \ll \mu'$). In the sequel, the distribution $\mu$ is referred to as the \textit{source distribution}, and $\mu'$ as the \textit{target distribution}. We are given two sample sets $S$ and $S'$ and we assume that the samples $S'=\{Z'_1,\ldots, Z'_{n}\}$ are drawn IID from the target distribution, and the samples $S=\{Z_{1},\ldots, Z_{m}\}$ are drawn IID from the source distribution. 

% In the setup of transfer learning, a learning algorithm is  a (randomized) mapping from the training data $S, S'$  to a hypothesis $w \in\mathcal W$, characterized by a conditional distribution $P_{W|SS'}$, with the goal to find a hypothesis $w$ that minimizes the population risk with respect to the \textit{target distribution}
% \begin{align}
% L_{\mu'}(w):=\Esub{Z'\sim\mu'}{\ell(w, Z')} \label{eq:pop_risk}
% \end{align}
% where $Z'$ is distributed according to $\mu'$.  Notice that $\beta=0$ corresponds to the important case when we do not have any samples from the target distribution. Obviously, $\beta=1$ takes us back to the classical setup where training data comes from the same distribution as test data.
 
% We focus on one particular \textit{empirical risk minimization} (ERM) algorithm.  For a hypothesis $w\in\mathcal W$, the empirical risk of $w$ on a training sequence $\tilde S:=\{Z_1,\ldots, Z_m\}$ is defined as
% \begin{align}
% \hat L(w,\tilde S):=\frac{1}{m}\sum_{i=1}^m\ell(w,Z_i).
% \label{eq:erm_risk}
% \end{align}
% Given samples $S$ and $S'$ from both distributions, it is natural to form an empirical risk function as a convex combination of the empirical risk induced by $S$ and $S'$ \cite{ben-david_theory_2010} defined as
% \begin{align*} 
% \hat L_{\alpha}(w,S,S'):=\frac{\alpha}{n}\sum_{i=1}^{n}\ell(w,Z'_i)+ \frac{1-\alpha}{m}\sum_{i=1}^{m}\ell(w,Z_i)  
% %\label{eq:err}
% \end{align*}
% for some weight parameter $\alpha\in[0,1]$ to be determined. We define $W_{\ERM} :=\text{argmin}_w \hat L_{\alpha}(w)$ as the ERM solution,  and also define the optimal hypothesis (with respect to the distribution $\mu'$) as
% %\begin{align*}
% $w^*=\text{argmin}_{w\in\mathcal W} L_{\mu'}(w)$.
% %\end{align*}

% %We are interested in two quantities for this ERM algorithm.  The first one is the  \textit{generalization error} defined as
% %\begin{align}
% %\gen(W_{\ERM}, S, S'):= L_{\mu'}(W_{\ERM})-\hat L_{\alpha}(W_{\ERM},S,S')
% %\label{eq:gen_error}
% %\end{align}
% %namely the difference between the minimized empirical risk and the population risk of the ERM solution under the target distribution. 
% We are interested in the \textit{excess risk} as
% \begin{align*}
% \mathcal{R}_{\mu'}(W_{\ERM}) :=L_{\mu'}(W_{\ERM})-L_{\mu'}(w^*)
% \end{align*}
% which is the difference between the population risk of $W_{\ERM}$ compared to that of the optimal hypothesis. We also define the \emph{empirical} excess risk w.r.t. $w^*$ for some $w \in \mathcal{W}$ as
% \begin{align*}
% \hat{\mathcal{R}}(w, S, S') :=  \hat L_{\alpha}(w,S,S') -  \hat L_{\alpha}(w^*,S,S')
% \end{align*}
% Notice that the expected excess risk can be bounded by the following inequality:
% %\begin{align}
% %L_{\mu'}(W_{\ERM})-L_{\mu'}(w^*) =& L_{\mu'}(W_{\ERM})-\hat L_{\alpha}(W_{\ERM},S,S')+\hat %L_{\alpha}(W_{\ERM}) \nonumber \\
% %&-\hat L_{\alpha}(w^*) +\hat L_{\alpha}(w^*)-L_{\alpha}(w^*) \nonumber \\
% %&+L_{\alpha}(w^*)-L_{\mu'}(w^*)\nonumber\\
% %\leq & \gen(W_{\ERM}, S, S')+\hat L_{\alpha}(w^*) \nonumber \\
% %& -L_{\alpha}(w^*)+(1-\alpha)(L_{\mu}(w^*)-L_{\mu'}(w^*))
% %\label{eq:excess_expression}
% %\end{align}
% \begin{align}
% \mathbb{E}_{WSS'}[L_{\mu'}(W_{\ERM}) & - L_{\mu'}(w^*)] \leq  \mathcal{E}_{\textup{tr}}(W_{\ERM}, S,S') \label{eq:excess_expression}
% \end{align}
% where we define
% \begin{align}
%     \mathcal{E}_{\textup{tr}} (W_{\ERM},& S,S')   = \alpha \Esub{WSS'}{\mathcal{R}_{\mu'}(W_\ERM) - \hat{\mathcal{R}}(W_{\ERM},S')} \nonumber \\
%     &+ (1-\alpha) \Esub{WSS'}{\mathcal{R}_{\mu'}(W_\ERM) - \hat{\mathcal{R}}(W_{\ERM},S)}\nonumber
% \end{align}
% and we have also used the fact  $\hat L_{\alpha}(W_{\ERM},S,S')-\hat L_{\alpha}(w^*,S,S')\leq 0$ by the definition of $W_{\ERM}$. For any $w\in\mathcal W$, the quantity $L_{\alpha}(w)$ in the above expression  is defined as
% \begin{align*}
% L_{\alpha}(w) := (1-\alpha)\Esub{Z\sim\mu}{\ell(w,Z)}+\alpha \Esub{Z\sim\mu'}{\ell(w,Z)}.
% \end{align*}
% Next we will bound $\mathcal{E}_{\textup{tr}}(W_{\ERM}, S,S')$. 
% \begin{theorem}[Fast Rate with Central Condition]\label{thm:central-transfer}
% Let $(\mu', \ell, \mathcal{W}, \mathcal{A})$ represent a learning problem which satisfies the $\bar{\eta}$-central condition for the target domain. If the $(u, c)$-witness condition holds under $P_W \otimes \mu'$, then for any $\eta \in(0, \bar{\eta})$, we have
% \begin{align}
% &\mathbb{E}_{WSS'}[L_{\mu'}(W_{\ERM}) - L_{\mu'}(w^*)] \leq  \frac{c_u}{\eta} \frac{\alpha}{n} \sum_{i=1}^{n} I(W_\ERM;Z_i)  \nonumber \\
% &+ \frac{c_u}{\eta} \frac{1-\alpha}{m} \sum_{i=1}^{m} \left(I(W_\ERM;Z_i) + D(\mu\|\mu')\right) \nonumber
% \end{align}
% where $c_u = \frac{1}{c}\frac{\eta u + 1}{1- \frac{\eta}{\bar{\eta}}} > 1$, $u > 0$ and $c \in (0, 1]$. More generally, for any algorithm $\mathcal{A}$ and any $W$ induced by the algorithm, we have,
% \begin{align}
% &\mathbb{E}_{WSS'}[L_{\mu'}(W) - L_{\mu'}(w^*)] \leq \frac{c_u}{\eta} \frac{\alpha}{n} \sum_{i=1}^{n} I(W;Z_i)  \nonumber \\
% &+ \frac{c_u}{\eta} \frac{1-\alpha}{m} \sum_{i=1}^{m} \left(I(W;Z_i) + D(\mu\|\mu')\right) \nonumber \\
% &+ (c_u - 1)\mathbb{E}_{WSS'}[\hat{\mathcal{R}}(W,S,S')]
% \end{align}
% \end{theorem}

% \begin{remark}
% Compared to the bound in Theorem 2 in \cite{wu2020information} as,
% \begin{equation}
% \begin{aligned}
% &\mathbb{E}_{WSS'}[L_{\mu'}(W_{\ERM}) - L_{\mu'}(w^*)] \leq \frac{\alpha \sqrt{2 r^{2}}}{n} \sum_{i=1}^{n} \sqrt{I\left(W_{\ERM} ; Z'_{i}\right)} \\
% &+\frac{(1-\alpha) \sqrt{2 r^{2}}}{m} \sum_{i=1}^{m} \sqrt{\left(I\left(W_{\ERM} ; Z_{i}\right)+D\left(\mu \| \mu^{\prime}\right)\right)} \\
% & + (1-\alpha) d_{\mathcal{W}}\left(\mu, \mu^{\prime}\right)
% \end{aligned}
% \end{equation}
% where $d_{\mathcal{W}}\left(\mu, \mu^{\prime}\right)=\sup _{w \in \mathcal{W}}\left|L_{\mu}(w)-L_{\mu^{\prime}}(w)\right|$. The new bound has the following improvements:
% \begin{itemize}
%     \item The square root term is removed and we may achieve a faster rate for converging.
%     \item $d_{\mathcal{W}}\left(\mu, \mu^{\prime}\right)$ is vanished in our new bound, which might be very large for some hypothesis space.
%     \item We verify that our bound is tight in the Gaussian mean estimation example for the excess risk, while this is not the case with the previous bound.
% \end{itemize}
% \end{remark}

% \subsection{Transfer Learning Case}
% Now we apply the bound to the Gaussian mean estimation in the transfer learning case. Assume that $S$ comes from the source distribution $\mu= \mathcal{N} (\nu, \sigma^2)$ and $S'$ comes form the target distribution $\mu'=\mathcal{N}(\nu', \sigma^2)$ where $m \neq m'$.  We define the loss function as
% \begin{align*}
% \ell(w,z)=(w-z)^2.
% \end{align*}
% For the sake of simplicity, we assume the source only scenario, i.e., there is no target data. The empirical risk minimization (ERM) solution is obtained by minimizing $\hat L(w,S):=\frac{1}{m}\sum_{i=1}^{m}(w-Z_i)^2$, where the solution is given by
% \begin{small}
% \begin{align*}
% W_{\ERM}=\frac{1}{m}\sum_{i=1}^m Z_i
% \end{align*}
% \end{small}
% To obtain the upper bound, we first notice that in this case
% \begin{align*}
% I(W_{\ERM};Z_i)=\frac{1}{2}\log \frac{m}{m-1}, \text{ for all } i = 1,2,\cdots, m 
% \end{align*}
% and $D(\mu||\mu') = \frac{(\nu - \nu')^2}{2\sigma^2}$. The bound becomes $O(k(\frac{1}{m} + \frac{(\nu - \nu')^2}{\sigma^2}))$ for some constants $k$. While the true excess risk can be calculated by
% \begin{align*}
% L_{\mu'}(W_{\ERM}) - L_{\mu'}(w^*) &= \mathbb{E}_{W\otimes \mu'}[(W_\ERM - Z)^2] - \mathbb{E}_{\mu'}[(w^* - Z)^2] \\
% &= (\nu-\nu')^2 + \frac{\sigma^2}{m} + \sigma^2 - \sigma^2 \\
% &= (\nu-\nu')^2 + \frac{\sigma^2}{m}
% \end{align*}
% with $w^* = m'$, which is also $O(k(\frac{1}{m} + \frac{(\nu - \nu')^2}{\sigma^2}))$ for $k = \sigma^2$. The new bound captures the true behaviour of the bounds up to a multiplicative factor $k$, which is much tighter than the previous bound \cite{wu2020information}.

\bibliographystyle{IEEEtranN}
\bibliography{reference}

\newpage
\onecolumn
\appendix
% \subsection{Technical Notations and Lemmas}
% We will introduce the notion of exponential stochastic inequality adapted from~\cite{mhammedi2019pac} and \cite{grunwald2020fast} for the sake of simplifying the notations when proving the fast rate bounds.
% \begin{definition}[Exponential Stochastic Inequality (ESI) \cite{mhammedi2019pac}] Let $\eta>0$ and $X, Y$ be random variables that can be expressed as functions of the random variable $U$ defined on the probability space $\mathcal{D}^{n}$. Then
% \begin{align*}
% X \unlhd_{\eta}^{U} Y \Leftrightarrow {\mathbb{E}}_U\left[e^{\eta(X-Y)}\right] \leq 1 .
% \end{align*}

% \end{definition}
% When no ambiguity can arise, we omit the random variable $U$. Besides simplifying notation, ESIs are useful in that they simultaneously capture "with high probability" and "in expectation" results, that is, $X \unlhd_{\eta}^{U} Y$, implies both that $\forall \delta \in(0,1), X \leq Y+\log (1 / \delta) / \eta$, with probability at least $1-\delta$ over the randomness of $U$ and that $\mathbb{E}_{U}[X] \leq \mathbb{E}_{U}[Y] .$ The following are some basic properties of ESI.

% \begin{proposition}[Properties of ESI\cite{mhammedi2019pac,grunwald2021pac}]
% $ $
% \begin{itemize}
%     \item (In Probability) If $X \unlhd_{\eta} Y$, then $\forall \delta \in(0,1), X \leq$ $Y+\frac{\log \frac{1}{\bar{z}}}{\eta}$, with probability at least $1-\delta$. 
%     \item (In Expecatation) Let $\bar{\eta}>0$ and let $g:[0, \bar{\eta}]$ be continuous and nondecreasing. If for all $\eta$ with $0<\eta \leq \bar{\eta}, X \unlhd_{\eta} Y+g(\eta)$, then $\mathbb{E}[X] \leq \mathbb{E}[Y]+g(0)$. 
%     \item (Transitivity and Chain Rule) 
%     Let $Z_{1}, \ldots, Z_{n}$ be any random variables on $\mathcal{Z}$ (not necessarily independent). If for some $\left(\gamma_{i}\right)_{i \in[n]} \in ( 0,+\infty)^{n}$, then $Z_{i} \unlhd_{\gamma_{i}} 0$. For all $i \in[n]$, 
%     \begin{align*}
%     \sum_{i=1}^{n} Z_{i} \unlhd_{\nu_{n}} 0, \quad \text { where } \nu_{n}:=\left(\sum_{i=1}^{n} \frac{1}{\gamma_{i}}\right)^{-1} %\quad\left(\text { if } \forall i \in[n], \gamma_{i}=\gamma>0,
%     \end{align*}
%     \item If $A \unlhd_{c\eta} Bf(\eta)$ for some function $f$, then it is equivalently to $A \unlhd_{\eta} Bf(\frac{\eta}{c})$.
%     \item If $A \unlhd_{\eta} B$, then it is equivalently to $\frac{A}{c} \unlhd_{c \eta} \frac{B}{c}$ for some $c > 0$.
% \end{itemize}
% \end{proposition}
% The following is the key proposition for bounding the excess risk using the PAC-Bayes results.
% \begin{proposition}[ESI PAC-Bayes] 
% Fix $\eta>0$ and let $\left\{Y_{w}: w \in \mathcal{W}\right\}$ be any family of random variables such that for all $w \in \mathcal{W}, Y_{w} \unlhd_{\eta} 0 .$ Let $P_{0}$ be any distribution on $\mathcal{W}$ and let $P: \bigcup_{i=1}^{n} \mathcal{Z}^{i} \rightarrow \mathcal{P}(\mathcal{W})$ be a learning algorithm. We have:
% \begin{align*}
% \mathbb{E}_{w \sim P_{n}}\left[Y_{w}\right] \unlhd_{\eta} \frac{\operatorname{KL}\left(P_{n} \| P_{0}\right)}{\eta}, \quad \text { where } P_{n}:=P\left(\mathcal{S}_n\right) .
% \end{align*}
% \end{proposition}
% To introduce this proposition to the bound, we need to find a family of random variables $Y_{w}$ that is associated with the excess risk and depends on $w$, and for any $w$,
% \begin{equation*}
%     E_{Y_w}[e^{\eta Y_w}] \leq 1.
% \end{equation*}
% We may also want to make $\eta$ stochastic so that we can tune $\eta$ after observing the data, and we can extend the results from fixed $\eta$ to random.
% \begin{proposition}[ESI from fixed to random $\eta$] 
% Let $\mathcal{G}$ be a countable subset of $(0,+\infty)$ and let $\pi$ be a prior distribution over $\mathcal{G}$. Given a countable collection $\left\{Y_{\eta}: \eta \in \mathcal{G}\right\}$ of random variables satisfying $Y_{\eta} \unlhd_{\eta} 0$, for all fixed $\eta \in \mathcal{G}$ and $\eta \geq \eta_0$, we have, for arbitrary estimator $\hat{\eta}$ with support on $\mathcal{G}$,
% \begin{align*}
% Y_{\hat{\eta}} \unlhd_{\hat{\eta_0}} \frac{-\ln \pi(\hat{\eta})}{\hat{\eta}} .
% \end{align*}
% \end{proposition}
% We have the following well-known proposition based on Bernstein's condition.
% \begin{proposition}[\cite{grunwald2021pac}]
% Suppose that $(\mu, \ell, \mathcal{W}, \mathcal{A})$ satisfies the $\left(B, \beta^{*}\right)$-Bernstein condition for some $\beta^{*} \in[0,1]$. Pick any $c>0, \eta<1 /(2 B c)$. Then for all $0<\beta \leq \beta^{*}$ and for all $w \in \mathcal{W}$ :
% \begin{align*}
%     c \eta \mathbb{E}_{ W \otimes \mu} & \left[\left(\ell\left(w,  Z^{\prime}\right)-\ell\left(w^{*} , Z^{\prime}\right)\right)^{2}\right]  \leq (1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}} + \left(\frac{1}{2} \wedge \beta\right) \cdot\left({\mathbb{E}_{W \otimes \mu}}\left[\ell\left(w,  Z^{\prime}\right)-\ell\left(w^{*} ; Z^{\prime}\right)\right]\right)
% \end{align*}
% \end{proposition}

% When proving the bounds under the central condition, for $\eta > 0$ and general random variables $U$, we define, respectively, the Hellinger-transformed expectation and the annealed expectation, also known as R\'enyi-transformed expectation \cite{grunwald2020fast} as
% \begin{align}
% &\mathbb{E}^{\mathrm{HE}(\eta)}[U]:=\frac{1}{\eta}\left(1-\mathbb{E}\left[e^{-\eta U}\right]\right) \\
% &\mathbb{E}^{\mathrm{ANN}(\eta)}[U]:=-\frac{1}{\eta} \log \mathbb{E}\left[e^{-\eta U}\right]
% \end{align}

% \begin{lemma} We have the following factors:
% \begin{itemize}
%     \item Factor 1:
%     \begin{equation}
% \mathbb{E}^{\mathrm{HE}(\eta)}[U] \leq \mathbb{E}^{\mathrm{ANN}(\eta)}[U] \leq \mathbb{E}[U]
% \end{equation}
% where the first inequality follows from $\log x \leq x - 1$ and the second from Jensen's inequality.
% \item Factor 2: if $\mathbb{E}\left[e^{-\eta X}\right]<\infty$, we have 
% \begin{equation}
% \lim_{\eta \rightarrow 0} \mathbb{E}^{\mathrm{HE}(\eta)}[X]=\mathbb{E}[X]
% \end{equation}
% \item Factor 3: $\eta \mapsto \mathbb{E}^{\operatorname{ANN}(\eta)}[X]$ is non-increasing.
% \end{itemize}
% \end{lemma}

\section{Proofs}
\subsection{Proof of~Theorem~\ref{thm:sub-Gaussian}}\label{proof:sub-Gaussian}
\begin{proof}
As $w^*$ is independent of $Z_i$, we have
\begin{equation}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W, \mathcal{S}_n)] = \mathbb{E}_{W\mathcal{S}_n}[\mathcal{R}(W) - \hat{\mathcal{R}}(W, \mathcal{S}_n)] = \mathbb{E}_{W \otimes \mathcal{S}_n}[\hat{\mathcal{R}}(W,\mathcal{S}_n)] - \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(W,\mathcal{S}_n)] .
\end{equation}
Let the distribution $P_{WZ_i}$ denote the joint distribution induced by $P_{W\mathcal{S}_n}$ with the algorithm $P_{W|\mathcal{S}_n}$. With the i.i.d. assumption, we can rewrite the generalization error by:
\begin{equation}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] = \frac{1}{n}\sum_{i=1}^n \mathbb{E}_{P_W \otimes \mu}[r(W,Z_i)] - \mathbb{E}_{WZ_i}[r(W,Z_i)].
\end{equation}
Using the KL-divergence property \cite{bu2020tightening, xu2017information} that
\begin{align}
    \sqrt{ 2\sigma^2 D\left( P_{WZ_i} \| P_{W}\otimes P_{Z_i} \right)} \geq \mathbb{E}_{P_W \otimes \mu}[r(W,Z_i)] - \mathbb{E}_{WZ_i}[r(W,Z_i)]
\end{align}
under the $\sigma$-sub-Gaussian assumption under the distribution $P_{W} \otimes \mu$. Summing up every term concludes the proof.
\end{proof}

\subsection{Proof of Lemma \ref{lemma:tightness}}\label{proof:lemma_tightness}
We are going to prove the lower bound as the upper bound is from the variational representations:
\begin{align*}
    \frac{n-1}{n} I(W;Z_i) \leq \mathbb{E}_{WZ_i}[-\eta r(W,Z_i)] - \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z_i)}] \leq I(W;Z_i).
\end{align*}
The proof for Lemma 1 simply follows the calculation of the two terms. By setting $\eta = \frac{1}{2\sigma^2_N}$, the first term can be calculated as:
\begin{align}
    \mathbb{E}_{WZ_i}[-\eta r(W,Z_i)] = \frac{1}{2n},
\end{align}
while the second term could be calculated as:
\begin{align}
    \log \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z_i)}] = 0.
\end{align}
Hence we have:
\begin{align}
    \frac{n-1}{n}I(W;Z_i)\frac{n-1}{2n}\log\frac{n}{n-1}\leq \frac{1}{2n} \leq \frac{1}{2}\log\frac{n}{n-1} = I(W;Z_i).
\end{align}
where the calculation details can be found in Appendix~\ref{apd:example3}.

\subsection{Proof of~Theorem~\ref{thm:lower_bounds}}\label{proof:lowerbound}
\begin{proof}
By Jensen's inequality, we have that:
\begin{align}
    I(W;Z_i) &= \mathbb{E}_{P_{WZ_i}}\left[-\frac{r(W,Z_i)}{2\sigma_N^2}- \frac{(W-Z_i)^2}{2\sigma^2_N(n-1)}\right] - \log \mathbb{E}_{P_W\otimes \mu}\left[\exp{\left(-\frac{r(W,Z_i)}{2\sigma_N^2} - \frac{(W-Z_i)^2}{2\sigma^2_N(n-1)}\right)}\right] \nonumber \\
    & \leq \mathbb{E}_{P_{WZ_i}}\left[-\frac{r(W,Z_i)}{2\sigma_N^2}- \frac{(W-Z_i)^2}{2\sigma^2_N(n-1)}\right] - \mathbb{E}_{P_W\otimes \mu}\left[{-\frac{r(W,Z_i)}{2\sigma_N^2} - \frac{(W-Z_i)^2}{2\sigma^2_N(n-1)}}\right], \label{eq:lower_bounds}
\end{align}
By rearranging the inequality,  the following holds for all $i = 1,2, \cdots, n$, 
\begin{align}
    \mathbb{E}_{P_W\otimes \mu}\left[r(W,Z_i)\right] \geq 2\sigma^2_NI(W;Z_i) + \mathbb{E}_{P_{WZ_i}}[r(W,Z_i)] + \frac{1}{n-1} (\mathbb{E}_{WZ_i}\left[ \ell(W,Z_i)\right] - \mathbb{E}_{P_W\otimes \mu}\left[ \ell(W,Z_i)\right]).
\end{align}
We complete the proof of the lower bound on the excess risk by averaging over $Z_i$. For the generalization error, we rewrite \eqref{eq:lower_bounds} as
\begin{align}
   \frac{1}{2\sigma^2_N}\frac{n}{n-1} (\mathbb{E}_{P_W\otimes \mu}\left[ \ell(W,Z_i)\right] - \mathbb{E}_{WZ_i}\left[ \ell(W,Z_i)\right] ) \geq I(W;Z_i).
\end{align}
Summing up every term for $Z_i$ completes the proof for the generalization error.
\end{proof}


\subsection{Proof of~Theorem~\ref{thm:sub-Gaussianv2}}\label{proof:sub-Gaussianv2}
\begin{proof}
Using the Donsker-Varadhan representation of the KL divergence, we build on the following inequality for some $\eta > 0$,
\begin{align}
    \frac{I(W;Z_i)}{\eta} + \Esub{P_{WZ_i}}{r(W,Z_i)} &\geq  - \frac{1}{\eta} \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}].
\end{align}
We will bound the R.H.S. using the following technique. For any $a$, we have,
\begin{align}
    \log \mathbb{E}_{P_{W}\otimes \mu}[e^{a\eta \mathbb{E}[r(W,Z_i)] -\eta(r(W,Z_i))}] &= \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta (\mathbb{E}[r(W,Z_i)] - r(W,Z_i)) + (a-1)\eta \mathbb{E}[r(W,Z_i)]}] \\
    &\leq \frac{\sigma^2 \eta^2}{2} + (a-1) \eta \mathbb{E}[r(W,Z_i)]. 
\end{align}
which is equivalent to
\begin{align}
    \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}] \leq \frac{\sigma^2 \eta^2}{2} - \eta \mathbb{E}[r(W,Z_i)],
\end{align}
By defining $0 < a_\eta = 1-  \frac{\eta\sigma^2}{2\mathbb{E}[r(W,Z_i)]} < 1$, the R.H.S. of the expression can be written as:
\begin{align}
    \frac{\sigma^2 \eta^2}{2} - \eta \mathbb{E}[r(W,Z_i)] = -a_\eta \eta \mathbb{E}[r(W,Z_i)].
\end{align}
Hence we have
\begin{align}
    \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}] &\leq -a_\eta \eta \mathbb{E}[r(W,Z_i)]. 
\end{align}
where $0 < \eta < \frac{2\mathbb{E}[r(W;Z_i)]}{\sigma^2}$. We then rewrite the inequality as,
\begin{align}
    -\frac{1}{\eta}\log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}] &\geq a_\eta \mathbb{E}[r(W,Z_i)], 
\end{align}
and we further have the following bound,
\begin{align}
    \frac{I(W;Z_i)}{\eta} + \mathbb{E}_{P_{WZ_i}}[r(W,Z_i)] &\geq a_\eta \mathbb{E}_{P_WP_{Z_i}}[r(W,Z_i)].
\end{align}
Hence,
\begin{align}
    \mathbb{E}_{P_WP_{Z_i}}[r(W,Z_i)] - \mathbb{E}_{P_{WZ_i}}[r(W,Z_i)]  \leq \frac{I(W;Z_i)}{\eta a_\eta} + \frac{1-a_\eta}{a_\eta}\mathbb{E}_{P_{WZ_i}}[r(W,Z_i)].
\end{align}
Summing every term for $Z_i$, we have,
\begin{align}
     \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq & \frac{1-a_\eta}{a_\eta} \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] + \frac{1}{n\eta a_\eta} \sum_{i=1}^{n}  I\left(W ; Z_{i}\right),
\end{align}
which completes the proof.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:eta-c}}\label{proof:thm_eta-c}
\begin{proof}
Firstly we rewrite excess risk and empirical excess risk by:
    \begin{align}
        \mathcal{R}(w) &= \mathbb{E}_{Z\sim \mu}[\ell(w,Z)] - \mathbb{E}_{Z\sim \mu}[\ell(w^*,Z)] \nonumber \\
        &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{Z_i\sim \mu}[\ell(w,Z_i)] - \mathbb{E}_{Z_i \sim \mu}[\ell(w^*,Z_i)] \nonumber \\
        &= \mathbb{E}_{\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)],
    \end{align}
    and 
    \begin{align}
        \hat{\mathcal{R}}(w, \mathcal{S}_n) &=  \hat{L}(w,\mathcal{S}_n) - \hat{L}(w^*,\mathcal{S}_n).
    \end{align}
Given any $\mathcal{S}_n$, the gap between the excess risk and empirical excess risk can be written as,
    \begin{align}
    \mathcal{R}(w) - \hat{\mathcal{R}}(w, \mathcal{S}_n) = \mathbb{E}_{\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)] - \hat{\mathcal{R}}(w, \mathcal{S}_n).
    \end{align}
We will bound the above quantity by taking the expectation w.r.t. $W$ learned from $\mathcal{S}_n$ by: 
    \begin{align}
        \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] &= \mathbb{E}_{W\mathcal{S}_n}[\mathcal{R}(W) - \hat{\mathcal{R}}(W, \mathcal{S}_n)] \\
        &=  \mathbb{E}_{P_W\otimes \mathcal{S}_n}[\hat{\mathcal{R}}(W, \mathcal{S}_n)] - \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(W, \mathcal{S}_n)] \\
        &= \frac{1}{n}\sum_{i=1}^n \mathbb{E}_{P_W \otimes \mu}[r(W,Z_i)] - \mathbb{E}_{WZ_i}[r(W,Z_i)].
    \end{align}
Recall that the variational representation of the KL divergence between two distributions $P$ and $Q$ defined over $\mathcal X$ is given as (see, e. g. \cite{boucheron2013concentration})
\begin{align}
D(P||Q)=\sup_{f}\{\Esub{P}{f(X)}-\log\Esub{Q}{e^{f(x)}} \}, %\label{eq:variational}
\end{align}
where the supremum is taken over all measurable functions such that $\Esub{Q}{e^{f(x)}}$ exists. Let $f(w,z_i) = -\eta r(w,z_i)$, we have,
\begin{align}
    D(P_{WZ_i}\|P_{W}\otimes P_{Z_i}) &\geq \Esub{P_{WZ_i}}{-\eta r(W,Z_i)} - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}] \nonumber \\
    &= \Esub{P_{WZ_i}}{-\eta r(W,Z_i)} - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i) - \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)])  }] + \Esub{P_W \otimes \mu}{\eta r(W,Z_i)} \nonumber \\
    &= \eta\left(\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)}\right) - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }]. \label{eq:MI_KL}
\end{align}
Next we will upper bound the second term $\log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }]$ in R.H.S. using the expected $(\eta,c)$-central condition. From the $(\eta, c)$-central condition, we have,
\begin{align}
    \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }] \leq (1-c)\eta \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)].
\end{align}
% Since $\eta' \leq \eta$, Jensen's inequality yields:
% \begin{align}
%     \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta'( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }]   &=  \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\frac{\eta'}{\eta}\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }] \\
%     &\leq \log \left( \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }] \right)^{\frac{\eta'}{\eta}} \\
%     &\leq \frac{\eta'}{\eta} (1-c)\eta \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] \\
%     &= \eta'(1-c) \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)].
%     \label{eq:bound-central}
% \end{align}
% Substitute (\ref{eq:bound-central}) into (\ref{eq:MI_KL}),
Then we arrive at,
\begin{align}
    I(W;Z_i) \geq \eta \left(\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)}\right) - (1-c)\eta \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)].
\end{align}
Divide $\eta$ on both side; we arrive at,
\begin{align}
    \frac{I(W;Z_i)}{\eta} \geq\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} - (1-c) \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)].
\end{align}
Rearrange the equation and yields,
\begin{align}
    c\Esub{P_W \otimes \mu}{r(W,Z_i)} \leq \Esub{P_{WZ_i}}{r(W,Z_i)} + \frac{I(W;Z_i)}{\eta}.
\end{align}
Therefore,
\begin{align}
    \Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} \leq  (\frac{1}{c} - 1)\left({\mathbb{E}_{P_{WZ_i}}}[r(w,Z_i)]\right) + \frac{I(W;Z_i)}{c\eta}.
\end{align}
Summing up every term for $Z_i$ and divide by $n$, we end up with,
\begin{align}
    \Esub{P_W \otimes P_{\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} - \Esub{P_{W\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} \leq & (\frac{1}{c} - 1) \left({\mathbb{E}_{P_{W\mathcal{S}_n}}}[\hat{\mathcal{R}}(W,\mathcal{S}_n)]\right) + \frac{1}{n}\sum_{i=1}^{n}\frac{I(W;Z_i)}{c\eta}.
\end{align}
Finally, we complete the proof by,
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] \leq (\frac{1}{c} - 1) \left({\mathbb{E}_{P_{W\mathcal{S}_n}}}[\hat{\mathcal{R}}(W,\mathcal{S}_n)]\right) + \frac{1}{n}\sum_{i=1}^{n}\frac{I(W;Z_i)}{c\eta}.
\end{align}
\end{proof}

\subsection{Proof of~Theorem~\ref{thm:eta-c-loss}}
\begin{proof}
Due to the Donsker-Varadhan representation, we have that for each $Z_i$:
\begin{align}
    I(W;Z_i) &\geq -\mathbb{E}_{WZ_i}[\eta \ell(W, Z_i)] - \log \mathbb{E}_{P_W\otimes \mu}\left[e^{-\eta \ell(W,Z_i)} \right] \\
    &\geq -\mathbb{E}_{WZ_i}[\eta \ell(W, Z_i)] + c\eta \mathbb{E}_{P_W \otimes \mu}[\ell(W,Z)].
\end{align}
The last inequality holds due  the $(\eta,c)$-central condition. By rearranging the equation, we arrive at the bound for the generalization error as:
\begin{align}
    \mathbb{E}_{P_W \otimes \mu}[\ell(W,Z_i)] -\mathbb{E}_{WZ_i}[\ell(W, Z_i)]  \leq \frac{I(W;Z_i)}{c\eta} + \frac{1-c}{c}\mathbb{E}_{WZ_i}[\ell(W,Z_i)]
\end{align}
which completes the proof by:
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W,\mathcal{S}_n)] \leq \frac{\sum_{i=1}^{n}I(W;Z_i)}{c\eta n} + \frac{1-c}{c}\mathbb{E}_{W\mathcal{S}_n}[\hat{L}(W,\mathcal{S}_n)].
\end{align}
\end{proof}

\subsection{Proof of~Corollary~\ref{coro:berstein}}\label{proof:corollary_bernstein}
\begin{proof}
We first present the expected Bernstein inequality which will be the key technical lemma for the fast rate bound.
\begin{lemma}[Expected Bernstein Inequality \cite{mhammedi2019pac,cesa2006prediction}] \label{lemma:exp_bern}
Let $U$ be a random variable bounded from below by $-b < 0 $ almost surely, and let $\kappa(x)=(e^x - x - 1) / x^{2} .$ For all $\eta >0$, we have
\begin{align}
 \log \mathbb{E}_{U}\left[e^{\eta(\mathbb{E}[U]-U}) \right] \leq \eta^2 c_{\eta} \cdot \mathbb{E}[U^{2}], \quad \text { for all } c_{\eta} \geq  \kappa(\eta b).
\end{align}
\end{lemma}
%\subsection{Proof of Lemma~\ref{lemma:exp_bern}}
\begin{proof}
The proof of the lemma follows from~\cite{cesa2006prediction} and \cite{mhammedi2019pac}. Firstly we define $Y = - U$ which is upper bounded by $b$, then using the property that $\frac{e^Y - Y -1}{Y^2}$ in non-increasing for $Y \in \mathbb{R}$, then we define $Z = \eta Y$ such that,
\begin{align}
   \frac{e^{Z} - Z -1}{Z^2} \leq \frac{e^{\eta b} - \eta b - 1}{\eta^2 b^2} = \kappa (\eta b).
\end{align}
Rearranging the inequality, we then arrive at,
\begin{align}
   e^{Z} - Z -1 \leq Z^2 \kappa (\eta b).
\end{align}
Taking the expectation on both sides, we have:
\begin{align}
    \mathbb{E}[e^{Z}] - 1 \leq \mathbb{E}[Z^2]\kappa (\eta b) + \mathbb{E}[Z]
\end{align}
and using the fact that $\log (x) \leq x - 1$ for any $x \in (0, +\infty)$, we have,
\begin{align}
    \log\mathbb{E}[e^{Z}] \leq \mathbb{E}[e^{Z}] - 1 \leq \mathbb{E}[Z^2]\kappa (\eta b) + \mathbb{E}[Z].
\end{align}
Then we arrive at:
\begin{align}
   \log\mathbb{E}[e^{Z}]  - \mathbb{E}[Z] \leq \mathbb{E}[Z^2] \kappa (\eta b).
\end{align}
By substituting $Z = \eta Y$, we have
\begin{align}
   \mathbb{E}[e^{\eta (Y - \mathbb{E}[Y])}] \leq e^{\eta^2 \mathbb{E}[Y^2] \kappa (\eta b)}.
\end{align}
Define $c_\eta \geq \kappa(\eta b)$, it yields that
\begin{align}
   \mathbb{E}[e^{\eta (Y - \mathbb{E}[Y])}] \leq e^{\eta^2 c_\eta \mathbb{E}[Y^2] }.
\end{align}
By substituting $Y = -U$, we finally have,
\begin{align}
   \mathbb{E}[e^{\eta (\mathbb{E}[U] - U)}] \leq e^{\eta^2 c_\eta \mathbb{E}[U^2] },
\end{align}
which completes the proof.
\end{proof}
Using the Bernstein condition and we also assume that $r(w,z_i)$ is lower bounded by $-b$ almost surely, we have for all $0< \eta < \frac{1}{b}$ and all $c > \frac{e^{\eta b} - \eta b - 1}{\eta^2b^2} > 0$, the following inequality holds with Lemma~\ref{lemma:exp_bern}:
 \begin{align}
     e^{\eta(\Esub{P_{W}\otimes \mu}{r(W,Z_i)} - r(W,Z_i))} \leq e^{\eta^2 c \Esub{P_{W}\otimes \mu}{r^2(W,Z_i)}}. \label{eq:rsquare}
 \end{align}
 Then following Proposition 5 in \cite{grunwald2021pac}, we have that for some $\beta \in [0,1]$, all $c>0$, $\eta < \frac{1}{2Bc}$ and  all $0 < \beta' \leq \beta$, we have
\begin{align}
    \eta^2c \Esub{P_W \otimes \mu}{r^2(w,Z_i)} \leq \left(\frac{1}{2} \wedge \beta'\right) \eta  \left({\mathbb{E}_{P_W\otimes \mu}}[r(w,Z_i)]\right)+(1-\beta') \cdot(2 B c \eta)^{\frac{1}{1-\beta'}}\eta.
\end{align}
Hence the equation~(\ref{eq:rsquare}) can be further bounded by
\begin{align}
    e^{\eta(\Esub{P_{W}\otimes \mu}{r(W,Z_i)} - r(W,Z_i))} \leq e^{\left(\frac{1}{2} \wedge \beta'\right) \eta  \left({\mathbb{E}_{P_W \otimes \mu}}[r(w,Z_i)]\right)+(1-\beta')(2 B c \eta)^{\frac{1}{1-\beta'}}\eta } 
    \label{bound:bernstein}
\end{align}
for $\eta < \min(\frac{1}{2B(e-2)}, \frac{1}{b})$. Here we can choose $c$ to be $\max_{\eta} \frac{e^{\eta b} - \eta b - 1}{\eta^2b^2} = e-2$ since the function $\frac{e^x - x - 1}{x^2}$ is non-decreasing in $[0,1]$. Furthermore, if $\beta' = 1$, we can rewrite~(\ref{bound:bernstein}) as,
\begin{align}
    e^{\eta(\Esub{P_{W}\otimes \mu}{r(W,Z_i)} - r(W,Z_i))} \leq e^{\frac{1}{2} \eta  \left({\mathbb{E}_{P_W \otimes \mu}}[r(w,Z_i)]\right)} 
\end{align}
which completes the proof.
\end{proof}


\subsection{Proof of~Corollary~\ref{coro:central}}
\begin{proof}
We first present the following Lemma for bounding the excess risk using the cumulant generating function.
\begin{lemma}[Generalized from Lemma 13 in \cite{grunwald2020fast}]\label{lemma:central}
Let $\bar{\eta}>0$. Assume that the expected $\eta$-strong central condition holds, and suppose further that the $(u, c)$-witness condition holds for $u>0$ and $c \in(0,1]$. Let $c_{u}:=\frac{1}{c} \frac{\eta' u+1}{1-\frac{\eta'}{\eta}} > 1$, then the following inequality holds:
\begin{equation}
\mathbb{E}_{P_W \otimes \mu}\left[r(W,Z)\right]  \leq - \frac{c_{u}}{\eta'}  \log \mathbb{E}_{ P_W \otimes \mu}\left[e^{-\eta' r(W,Z)}\right] . 
\end{equation}

%More generally, suppose that the $\bar{\eta}$-central condition and the $(\tau, c)$-witness condition hold for $c \in(0,1]$ and a non-increasing function $\tau$. Then for all $\lambda>0$, all $w \in \mathcal{W}$,
%\begin{equation}
%\mathbb{E}_{\mu}\left[r(W,Z)\right] \leq \lambda \vee\left(c_{\tau(\lambda)} \cdot \mathbb{E}_{ \mu}^{\mathrm{HE}(\eta)}\left[r(W,Z)\right] \right) \leq \lambda \vee\left(c_{\tau(\lambda)} \cdot \mathbb{E}_{\mu}^{\mathrm{ANN}(\eta)}\left[r(W,Z)\right] \right) .
%\end{equation}
\end{lemma}
The proof of the above lemma is similar to the proof in Appendix C.1 (page 48) in \cite{grunwald2020fast} by taking the expectation over the hypothesis distribution $P_W$, which is omitted here. Now with~Lemma~\ref{lemma:central}, we have that for any $0 < \eta' < \eta$,
\begin{align}
  \log \mathbb{E}_{ P_W \otimes \mu}\left[e^{-\eta' \left( r(W,Z) - \mathbb{E}_{P_W \otimes \mu}[r(W,Z)]  \right)}\right] &\leq -\frac{\eta'}{c_u}\mathbb{E}_{P_W \otimes \mu}\left[r(W,Z)\right] + \eta'\mathbb{E}_{P_W \otimes \mu}\left[r(W,Z)\right] \\
  &=  (1-\frac{1}{c_u}) \eta'  \mathbb{E}_{P_W \otimes \mu}\left[r(W,Z)\right].
\end{align}
Therefore, the central condition with the witness condition implies the expected $(\eta', \frac{c-\frac{c\eta'}{\eta}}{\eta' u +1})$-central condition for any $0 < \eta' < \eta$.
\end{proof}


\subsection{Proof of Corollary \ref{coro:subexponential}} \label{proof:coro_subexponential_subgamma}

\begin{proof}[Proof of sub-Exponential Distribution]\label{proof:subexponential}
Recall that $X$ is a $\left(\nu^2, \alpha\right)$-sub-exponential random variable with parameters $\nu, \alpha>0$ if:
\begin{align}
\log\mathbb{E} \left[ e^{\lambda (X -\mathbb{E}[X])}\right] \leq \frac{\lambda^2 \nu^2}{2} , \quad \forall \lambda:|\lambda|<\frac{1}{\alpha}. 
\end{align}
Then rewrite the above inequality and replacing $\lambda = -\lambda$,  we have:
\begin{align}
\log\mathbb{E} \left[ e^{-\lambda X}\right] \leq -\lambda(\mathbb{E}[X] -\frac{\lambda \nu^2}{2}) = -\lambda \mathbb{E}[X](1 - \frac{\lambda \nu^2}{2\mathbb{E}[X]}), \quad \forall \lambda: 0 < \lambda<\frac{1}{\alpha}.  
\end{align}
Let $c = \frac{1}{2}$, $\lambda = \min(\frac{1}{\alpha}, \frac{\nu^2}{\mathbb{E}_{P_W\otimes \mu}[r(W,Z)]})$ and we can complete the proof since $\mathbb{E}_{P_W\otimes \mu}[r(W,Z)] > 0$ from the definition.
\end{proof}

\begin{proof}[Proof of sub-Gamma distribution]
Recall that $-X$ is a ($\nu^2,\alpha$)-sub-Gamma random variable with variance parameter $\nu^2$ and scale parameter $\alpha$ if:
\begin{align}
\log\mathbb{E} \left[ e^{\lambda (\mathbb{E}[X] - X)}\right]\leq \frac{\nu^2 \lambda^2}{2(1- \alpha \lambda)}, \quad  \forall \lambda:  0<\lambda<\frac{1}{\alpha}.
\end{align}
Rewrite the above equation and we have:
\begin{align}
\log\mathbb{E} \left[ e^{-\lambda X}\right]\leq \frac{\nu^2 \lambda^2}{2(1- \alpha \lambda)} - \lambda\mathbb{E}[X] = -\lambda \mathbb{E}[X]\left(1 - \frac{\nu^2\lambda}{2(1-\alpha \lambda)\mathbb{E}[X]}\right), \quad  \forall \lambda:  0<\lambda<\frac{1}{\alpha}.
\end{align}
Let $c = \frac{1}{2}$, $\lambda = \min(\frac{1}{\alpha}, \frac{\mathbb{E}_{P_W\otimes \mu}[r(W,Z)]}{\nu^2+\alpha \mathbb{E}_{P_W\otimes \mu}[r(W,Z)]})$ and we can complete the proof.
\end{proof}


\subsection{Proof of Corollary~\ref{coro:rerm}}\label{proof:rerm}
\begin{proof}
We first define,
\begin{align}
    \hat{{L}}_{\textup{reg}}(w,\mathcal{S}_n) := \hat{{L}}(w,\mathcal{S}_n) + \frac{\lambda}{n}g(w).
\end{align}
Based on Theorem~\ref{thm:eta-c}, we can bound the excess risk for $W_{\sf{RERM}}$ by,
\begin{align}
     \mathbb{E}_{W}[\mathcal{R}(W_{\sf{RERM}})] & \leq  \frac{1}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W_{\sf{RERM}}, \mathcal{S}_{n} \right)]  + \frac{1}{c\eta n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i) \\
     &= \frac{1}{c} \left( \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{L}\left(W_{\sf{RERM}}, \mathcal{S}_{n} \right) - \hat{L}\left(w^*, \mathcal{S}_{n} \right)] \right)   + \frac{1}{c\eta n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i) \\
     &\overset{(a)}{\leq} \frac{1}{c} \left( \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{{L}}_{\textup{reg}} \left(W_{\sf{RERM}}, \mathcal{S}_{n} \right)] - \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{{L}}_{\textup{reg}}\left(w^*, \mathcal{S}_{n}\right)] \right)  + \frac{\lambda B}{cn}+ \frac{1}{c\eta n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i) \\
     & =  \frac{1}{c} \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}_{\textup{reg}}\left(W_{\sf{RERM}}, \mathcal{S}_{n} \right)] + \frac{\lambda B}{cn} + \frac{1}{c\eta n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i) \\
     &\overset{(b)}{\leq}  \frac{\lambda B}{cn} + \frac{1}{c\eta n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i).
     %&\overset{(c)}{\leq} \frac{\lambda B}{cn}+ \frac{1}{c\eta' n} \sum_{i=1}^{n} I(W_{\sf{RERM}};Z_i)
 \end{align}
where (a) follows since $|g(w^*) - g(W_{\sf{RERM}}))| \leq B$ the expected empirical risk is negative for $W_{\ERM}$ and (b) holds due to that $W_{\sf{RERM}}$ is the minimizer of the regularized loss. 
%Finally (c) holds due to that $W_{\sf{RERM}}$ is the minimizer of the regularized loss $\hat{\mathcal{L}}_{\textup{reg}}$.
\end{proof}

\subsection{Proof of Theorem~\ref{lemma:intermediate}}\label{proof:intermediate}
\begin{proof}
We will build upon~(\ref{eq:MI_KL}). With the $(v,c)$-central condition, for any $\epsilon \geq 0$ and any $ 0 < \eta \leq v(\epsilon)$, the Jensen's inequality yields:
\begin{align}
    \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }]   &=  \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\frac{\eta}{v(\epsilon)}v(\epsilon)( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }] \\
    &\leq \log \left( \mathbb{E}_{P_{W}\otimes \mu}[e^{v(\epsilon)( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }] \right)^{\frac{\eta}{v(\epsilon)}} \\
    &\leq \frac{\eta}{v(\epsilon)} \left( (1-c)v(\epsilon) \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] + v(\epsilon) \epsilon \right) \\
    &= \eta(1-c) \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] + \eta\epsilon.
    \label{eq:bound-v-central}
\end{align}
Substitute (\ref{eq:bound-v-central}) into (\ref{eq:MI_KL}), we arrive at,
\begin{align}
    I(W;Z_i) \geq \eta \left(\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)}\right) - (1-c)\eta \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] - \eta \epsilon.
\end{align}
Dividing $\eta$ on both sides, we arrive at,
\begin{align}
    \frac{I(W;Z_i)}{\eta} \geq\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} - (1-c) \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)] - \epsilon.
\end{align}
Rearranging the equation and it yields,
\begin{align}
    c\Esub{P_W \otimes \mu}{r(W,Z_i)} \leq \Esub{P_{WZ_i}}{r(W,Z_i)} + \frac{I(W;Z_i)}{\eta} +\epsilon.
\end{align}
Therefore,
\begin{align}
    \Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} \leq  (\frac{1}{c} - 1)\left({\mathbb{E}_{P_{WZ_i}}}[r(w,Z_i)]\right) + \frac{I(W;Z_i)}{c\eta} + \frac{\epsilon}{c}.
\end{align}
Summing up every term for $Z_i$ and dividing by $n$, we end up with,
\begin{align}
    \Esub{P_W \otimes P_{\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} - \Esub{P_{W\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} \leq & (\frac{1}{c} - 1) \left({\mathbb{E}_{P_{W\mathcal{S}_n}}}[\hat{\mathcal{R}}(W,\mathcal{S}_n)]\right) + \frac{1}{n}\sum_{i=1}^{n}\left( \frac{I(W;Z_i)}{c\eta} + \frac{\epsilon}{c}\right).
\end{align}
Finally, we arrive at the following inequality:
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] \leq (\frac{1}{c} - 1) \left({\mathbb{E}_{P_{W\mathcal{S}_n}}}[\hat{\mathcal{R}}(W,\mathcal{S}_n)]\right) + \frac{1}{n}\sum_{i=1}^{n}\left(\frac{I(W;Z_i)}{c\eta} + \frac{\epsilon}{c} \right).
\end{align}
In particular, if $v(\epsilon) = \epsilon^{1-\beta}$ for some $\beta \in [0,1]$, then by choosing $\eta = v(\epsilon)$ and $\frac{I(W;Z_i)}{c\eta} + \frac{\epsilon}{c}$ is optimized when $\epsilon = I(W;Z_i)^{\frac{1}{2-\beta}}$ and the bound becomes,
\begin{align}
    \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] \leq (\frac{1}{c} - 1) \left({\mathbb{E}_{P_{W\mathcal{S}_n}}}[\hat{\mathcal{R}}(W,\mathcal{S}_n)]\right) + \frac{2}{nc}\sum_{i=1}^{n} I(W;Z_i)^{\frac{1}{2-\beta}},
\end{align}
which completes the proof.
\end{proof}

% \subsection{Proof of~Theorem~\ref{thm:bern-stein}}
% \begin{proof}
% The proof procedures are sketched as follows. 
% \begin{itemize}
%     \item Step 1: Rewriting excess risk and empirical excess risk by:
%     \begin{align*}
%         \mathcal{R}(w) &= \mathbb{E}_{z\sim \mu}[\ell(w,z)] - \mathbb{E}_{z\sim \mu}[\ell(w^*,z)] \\
%         &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{Z_i\sim \mu}[\ell(w,Z_i)] - \mathbb{E}_{Z_i \sim \mu}[\ell(w^*,Z_i)] \\
%         &= \mathbb{E}_{\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)]
%     \end{align*}
%     and 
%     \begin{align*}
%         \hat{\mathcal{R}}(w, \mathcal{S}_n) &=  \hat{L}(w,\mathcal{S}_n) - \hat{L}(w^*,\mathcal{S}_n) 
%     \end{align*}
%     Given any $\mathcal{S}_n$, the gap between the excess risk and empirical excess risk can be written as,
%     \begin{align*}
%     \mathcal{R}(w) - \hat{\mathcal{R}}(w, \mathcal{S}_n) = \mathbb{E}_{\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)] - \hat{\mathcal{R}}(w, \mathcal{S}_n)
%     \end{align*}
%     We will bound the above quantity by taking the expectation w.r.t. $w$ learned from $\mathcal{S}_n$ by: 
%     \begin{align}
%         \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] &= \mathbb{E}_{W\mathcal{S}_n}[\mathcal{R}(w) - \hat{\mathcal{R}}(w, \mathcal{S}_n)] \\
%         &=  \mathbb{E}_{W\otimes \mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)] - \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)] \\
%         &= \frac{1}{n}\sum_{i=1}^n \mathbb{E}_{W \otimes \mu}[r(W,Z_i)] - \mathbb{E}_{WZ_i}[r(W,Z_i)] 
%     \end{align}

%     \item Step 2: Recall that the variational representation of the KL divergence between two distributions $P$ and $Q$ defined over $\mathcal X$ is given as (see, e. g. \cite{boucheron_concentration_2013})
% \begin{align*}
% D(P||Q)=\sup_{f}\{\Esub{P}{f(X)}-\log\Esub{Q}{e^{f(x)}} \}
% %\label{eq:variational}
% \end{align*}
% where the supremum is taken over all measurable functions such that $\Esub{Q}{e^{f(x)}}$ exists. Then let $f(w,z_i) = -\eta r(w,z_i)$, we have,
% \begin{align}
%     D(P_{WZ_i}\|P_{W}\otimes P_{Z_i}) &\geq \Esub{P_{WZ_i}}{-\eta r(W,Z_i)} - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}] \nonumber \\
%     &= \Esub{P_{WZ_i}}{-\eta r(W,Z_i)} - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i) - \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)])  }] + \Esub{P_W \otimes \mu}{\eta r(W,Z_i)} \nonumber \\
%     &= \eta\left(\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)}\right) - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }] \label{eq:MI_KL}
% \end{align}
% Next we will upper bound the second term $\log \mathbb{E}_{P_{W}\otimes \mu}[e^{\eta( \mathbb{E}_{P_{W}\otimes \mu}[r(W,Z_i)]  - r(W,Z_i))  }]$ in R.H.S. using Bernstein's condition.

% \item Step 3: Using Lemma \ref{lemma:exp_bern}, assume $r(w,z_i)$ is lower bounded by $-b$ almost surely, then we have for all $0< \eta < \frac{1}{b}$ and all $c > \frac{e^{\eta b} - \eta b - 1}{\eta^2b^2} > 0$ , we have
% \begin{align}
%     e^{\eta(\Esub{P_{W}\otimes \mu}{r(W,Z_i)} - r(W,Z_i))} \leq e^{\eta^2 c \Esub{P_{W}\otimes \mu}{r^2(W,Z_i)}} \label{eq:rsquare}
% \end{align}
% We can continue to upper bound the expectation of $r^2(W,Z_i)$ using Proposition 4.

% \item Step 4: Using Bernstein's condition in Proposition 4, we have that for $\beta^* \in [0,1]$, any $c>0$ and $\eta < \frac{1}{2Bc}$, then for all $0 < \beta < \beta^*$ and all $w \in \mathcal{W}$,
% \begin{align}
%     c\eta \Esub{\mu}{r^2(w,Z_i)} \leq \left(\frac{1}{2} \wedge \beta\right) \cdot\left({\mathbb{E}_{\mu}}[r(w,Z_i)]\right)+(1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}}
% \end{align}
% Take expectation w.r.t. $P_W$ and multiply by $\eta$ on both sides, we arrive at,
% \begin{align}
%     \eta^2c \Esub{P_W \otimes \mu}{r^2(w,Z_i)} \leq \left(\frac{1}{2} \wedge \beta\right) \eta  \left({\mathbb{E}_{P_W\otimes \mu}}[r(w,Z_i)]\right)+(1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}}\eta 
% \end{align}
% Hence the equation~(\ref{eq:rsquare}) can be further bounded by,
% \begin{align}
%     e^{\eta(\Esub{P_{W}\otimes \mu}{r(W,Z_i)} - r(W,Z_i))} \leq e^{\left(\frac{1}{2} \wedge \beta\right) \eta  \left({\mathbb{E}_{P_W \otimes \mu}}[r(w,Z_i)]\right)+(1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}}\eta } 
%     \label{bound:bernstein}
% \end{align}
% \item Step 5: Substitute (\ref{bound:bernstein}) into (\ref{eq:MI_KL}), we arrive at,
% \begin{align}
%     I(W;Z_i) \geq \eta \left(\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)}\right) - \left(\frac{1}{2} \wedge \beta\right) \eta  \left({\mathbb{E}_{P_W\otimes \mu}}[r(w,Z_i)]\right)+(1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}}\eta
% \end{align}
% Divide $\eta$ on both side, we arrive at,
% \begin{align}
%     \frac{I(W;Z_i)}{\eta} \geq\Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} - \left(\frac{1}{2} \wedge \beta\right)\left({\mathbb{E}_{P_W\otimes \mu}}[r(w,Z_i)]\right) - (1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}}
% \end{align}
% Let $\alpha =  \left(\frac{1}{2} \wedge \beta\right) \leq \frac{1}{2}$, we have the bound,
% \begin{align}
%     \alpha\Esub{P_W \otimes \mu}{r(W,Z_i)} \leq 2\alpha \Esub{P_{WZ_i}}{r(W,Z_i)} + 2\alpha (1-\beta) \cdot(2 B c \eta)^{\frac{1}{1-\beta}} + \frac{2\alpha I(W;Z_i)}{\eta}
% \end{align}
% Therefore,
% \begin{align}
%     \Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} \leq \left(1 \wedge 2\beta\right)\left({\mathbb{E}_{P_{WZ_i}}}[r(w,Z_i)]\right) + 2\left( \left(\frac{\eta}{\eta_{\max}}\right)^{\frac{1}{1-\beta}} + \frac{I(W;Z_i)}{\eta} \right)
% \end{align}
% with $\eta_{\max} = \min\left(\frac{1}{4}, \frac{1}{2 B C_{1 / 4}}\right)$ simplifying the notation. We can then minimise the R.H.S. by choosing appropriate $\eta$, by differential, we know that 
% \begin{align*}
% \min_{\eta} \left( \left(\frac{\eta}{\eta_{\max}}\right)^{\frac{1}{1-\beta}} + \frac{I(W;Z_i)}{\eta} \right) = 2\left(\frac{I(W;Z_i)}{\eta_{\max}}\right)^{\frac{1}{2-\beta}}
% \end{align*}
% with the choice of $\eta = I(W;Z_i)^{\frac{1-\beta}{2-\beta}}\eta^{\frac{1}{2-\beta}}_{\max}$ if $\eta < \eta_{\max}$.  Or 
% \begin{align*}
% \left( \left(\frac{\eta}{\eta_{\max}}\right)^{\frac{1}{1-\beta}} + \frac{I(W;Z_i)}{\eta} \right) \leq \frac{2I(W;Z_i)}{\eta_{\max}}
% \end{align*}
% if $\eta = \eta_{\max}$. Therefore,
% \begin{align}
%     \Esub{P_W \otimes \mu}{r(W,Z_i)} - \Esub{P_{WZ_i}}{r(W,Z_i)} \leq \left(1 \wedge 2\beta\right)\left({\mathbb{E}_{P_{WZ_i}}}[r(w,Z_i)]\right) + 4\left(\frac{I(W;Z_i)}{\eta_{\max}}\right)^{\frac{1}{2-\beta}}_{[**]}
% \end{align}
% \item Step 6: summing up every term for $Z_i$ and divide by $n$, we end up with,
% \begin{align}
%     \Esub{P_W \otimes P_{\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} - \Esub{P_{W\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} \leq & \left(1 \wedge 2\beta\right)\left({\mathbb{E}_{P_{W\mathcal{S}_n}}}[\hat{\mathcal{R}}(W,\mathcal{S}_n)]\right) + \frac{4}{n}\sum_{i=1}^{n}\left(\frac{I(W;Z_i)}{\eta_{\max}}\right)^{\frac{1}{2-\beta}}_{[**]}
% \end{align}
% Therefore,
% \begin{align}
%     \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] \leq (1 \wedge 2 \beta) \mathbb{E}_{P_{W\mathcal{S}_n}}[\hat{\mathcal{R}}\left(W, \mathcal{S}_{n} \right)] + \frac{4}{n} \sum_{i=1}^{n} \left(\frac{I(W;Z_i)}{\eta_{\max}}\right)^{\frac{1}{2-\beta}}_{[**]},
% \end{align}
% which completes the proof.
% \end{itemize}

% \end{proof}


% \subsection{Proof of~Theorem~\ref{thm:central}}

% \begin{proof}
% The proof procedures are sketched as follows. 
% \begin{itemize}
%     \item Step 1: Rewriting excess risk and empirical excess risk by:
%     \begin{align*}
%         \mathcal{R}(w) &= \mathbb{E}_{Z\sim \mu}[\ell(w,Z)] - \mathbb{E}_{Z \sim \mu}[\ell(w^*,Z)] \\
%         &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{Z_i\sim \mu}[\ell(w,Z_i)] - \mathbb{E}_{Z_i \sim \mu}[\ell(w^*,Z_i)] \\
%         &= \mathbb{E}_{\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)]
%     \end{align*}
%     and 
%     \begin{align*}
%         \hat{\mathcal{R}}(w, \mathcal{S}_n) &=  \hat{L}(w,\mathcal{S}_n) - \hat{L}(w^*,\mathcal{S}_n) 
%     \end{align*}
%     Given any $\mathcal{S}_n$, the gap between the excess risk and empirical excess risk can be written as,
%     \begin{align*}
%     \mathcal{R}(w) - \hat{\mathcal{R}}(w, \mathcal{S}_n) = \mathbb{E}_{\mathcal{S}_n}[\hat{\mathcal{R}}(w, \mathcal{S}_n)] - \hat{\mathcal{R}}(w, \mathcal{S}_n)
%     \end{align*}
%     We will bound the above quantity by taking the expectation w.r.t. $w$ learned from $\mathcal{S}_n$ by: 
%     \begin{align}
%         \mathbb{E}_{W\mathcal{S}_n}[\mathcal{E}(W)] &= \mathbb{E}_{W\mathcal{S}_n}[\mathcal{R}(W) - \hat{\mathcal{R}}(W, \mathcal{S}_n)] \\
%         &=  \mathbb{E}_{W\otimes \mathcal{S}_n}[\hat{\mathcal{R}}(W, \mathcal{S}_n)] - \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}}(W, \mathcal{S}_n)] \\
%         &= \frac{1}{n}\sum_{i=1}^n \mathbb{E}_{W \otimes \mu}[r(W,Z_i)] - \mathbb{E}_{WZ_i}[r(W,Z_i)] 
%     \end{align}

%     \item Step 2: Recall that the variational representation of the KL divergence between two distributions $P$ and $Q$ defined over $\mathcal X$ is given as (see, e. g. \cite{boucheron_concentration_2013})
% \begin{align}
% D(P||Q)=\sup_{f}\{\Esub{P}{f(X)}-\log\Esub{Q}{e^{f(x)}} \}
% %\label{eq:variational}
% \end{align}
% where the supremum is taken over all measurable functions such that $\Esub{Q}{e^{f(x)}}$ exists. Then let $f(w,z_i) = -\eta r(w,z_i)$, we have,
% \begin{align}
%     D(P_{WZ_i}\|P_{W}\otimes P_{Z_i}) &\geq \Esub{P_{WZ_i}}{-\eta r(W,Z_i)} - \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}]  
% \end{align}
% Rewrite the above inequality as,
% \begin{align}
%     \frac{I(W;Z_i)}{\eta} + \Esub{P_{WZ_i}}{r(W,Z_i)} &\geq  - \frac{1}{\eta} \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}]  \label{eq:MI}
% \end{align}
% The R.H.S. is the annealed excess risk under the distribution $P_W \otimes \mu$, we will bound this term using Lemma 13 in \cite{grunwald2020fast}.

% \item Step 3: We first present Lemma 13 in \cite{grunwald2020fast} for bounding the annealed excess risk.
% \begin{lemma}
% Let $\bar{\eta}>0$. Assume that the $\bar{\eta}$-strong central condition holds and let, for arbitrary $0 < \eta < \bar{\eta}, c_{u}:=\frac{1}{c} \frac{\eta u+1}{1-\frac{\eta}{\bar{\eta}}}$. Suppose further that the $(u, c)$-witness condition holds for $u>0$ and $c \in(0,1]$. Then for all $w \in \mathcal{W}$, all $\eta \in(0, \bar{\eta})$:
% \begin{equation}
% \mathbb{E}_{ \mu}\left[r(W,Z)\right] \leq c_{u} \cdot \mathbb{E}_{ \mu}^{\mathrm{HE}(\eta)}\left[r(W,Z)\right] \leq c_{u} \cdot \mathbb{E}_{ \mu}^{\mathrm{ANN}(\eta)}\left[r(W,Z)\right] .
% \end{equation}
% %More generally, suppose that the $\bar{\eta}$-central condition and the $(\tau, c)$-witness condition hold for $c \in(0,1]$ and a non-increasing function $\tau$. Then for all $\lambda>0$, all $w \in \mathcal{W}$,
% %\begin{equation}
% %\mathbb{E}_{\mu}\left[r(W,Z)\right] \leq \lambda \vee\left(c_{\tau(\lambda)} \cdot \mathbb{E}_{ \mu}^{\mathrm{HE}(\eta)}\left[r(W,Z)\right] \right) \leq \lambda \vee\left(c_{\tau(\lambda)} \cdot \mathbb{E}_{\mu}^{\mathrm{ANN}(\eta)}\left[r(W,Z)\right] \right) .
% %\end{equation}
% \end{lemma}
% The proof of the above lemma can be found in Appendix C.1 (page 48) in \cite{grunwald2020fast}. Now under the strong central condition and witness condition, by taking the expectation over $P_{W}$, we have,
% \begin{equation}
% \mathbb{E}_{P_W \otimes \mu}\left[r(W,Z)\right] \leq  \frac{c_{u}}{\eta} \left( 1- \mathbb{E}_{ P_W \otimes \mu}[e^{-\eta r(W,Z_i)}] \right) \leq - \frac{c_{u}}{\eta}  \log \mathbb{E}_{ P_W \otimes \mu}\left[e^{-\eta r(W,Z)}\right]. 
% \end{equation}
% With this inequality, (\ref{eq:MI}) can be further bounded by,
% \begin{align}
%     \frac{I(W;Z_i)}{\eta} + \Esub{P_{WZ_i}}{r(W,Z_i)} &\geq  - \frac{1}{\eta} \log \mathbb{E}_{P_{W}\otimes \mu}[e^{-\eta(r(W,Z_i))}]  \\
%     &\geq \frac{1}{c_u}\mathbb{E}_{P_W \otimes \mu}\left[r(W,Z_i)\right]
% \end{align}
% Therefore, rearrange the above equation,
% \begin{align}
%     \mathbb{E}_{P_W \otimes \mu}\left[r(W,Z_i)\right] \leq \frac{c_u}{\eta} I(W;Z_i) + c_u \cdot \Esub{P_{WZ_i}}{r(W,Z_i)}
% \end{align}
% \item Step 4: Summing up every term for $Z_i$, we have,
% \begin{align}
%     \mathbb{E}_{P_W \otimes P_{\mathcal{S}_n}}\left[\sum_{i=1}^n r(W,Z_i)\right] \leq \frac{c_u}{\eta} \sum_{i=1}^{n} I(W;Z_i) + c_u \cdot \Esub{P_{W\mathcal{S}_n}}{\sum_{i=1}^n r(W,Z_i)}
% \end{align}
% Divide by $n$ for both sides, we have,
% \begin{align}
%     \mathbb{E}_{P_W \otimes P_{\mathcal{S}_n}} \left[\hat{\mathcal{R}}(W,\mathcal{S}_n)\right] \leq \frac{c_u}{\eta} \frac{1}{n}\sum_{i=1}^{n} I(W;Z_i) + c_u \cdot \Esub{P_{W\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)}
% \end{align}
% since $c_u > 1$, we have,
% \begin{align}
%     \mathbb{E}_{P_W \otimes P_{\mathcal{S}_n}} \left[\hat{\mathcal{R}}(W,\mathcal{S}_n)\right]   - \Esub{P_{W\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)} \leq \frac{c_u}{\eta} \frac{1}{n}\sum_{i=1}^{n} I(W;Z_i) + (c_u - 1) \cdot \Esub{P_{W\mathcal{S}_n}}{\hat{\mathcal{R}}(W,\mathcal{S}_n)}
% \end{align}
% which completes the proof.
% \end{itemize}
% \end{proof}


%\subsection{Proof of~Theorem~\ref{thm:sub-Gaussian}}
%\begin{proof}
%Before proving~Theorem~\ref{thm:sub-Gaussian}, we firstly prove~Lemma~\ref{lemma:alpha}. Consider individual terms in the difference, we have
%\begin{equation}\mathbb{E}_{WZ_i}\left[\lambda\left(\alpha \mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)]-r\left(W, Z_{i}\right)\right)\right] \leq I\left(W ; Z_{i}\right)+ \log \mathbb{E}_{P_{W} P_{Z_{i}}}\left[e^{\lambda\left(\alpha\mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)] -r\left(W, Z_{i}\right)\right)}\right]    
% \end{equation}
% The second term on the RHS can be upper bounded as
% \begin{align}
% \log \mathbb{E}_{P_{W} P_{Z_{i}}}\left[e^{\lambda\left(\alpha \mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)]-r\left(W, Z_{i}\right)\right)}\right] & = \log \mathbb{E}_{P_{W} P_{Z_{i}}}\left[e^{\lambda\alpha\left(\mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)]-r\left(W, Z_{i}\right)\right)-\lambda(1-\alpha) r(W,Z_i)}\right] \\
% & \overset{(a)}{\leq} \log \mathbb{E}_{P_{W}P_{Z_i}}\left[e^{\frac{\lambda^{2}\alpha^2 \sigma^{2}}{2}-\lambda(1-\alpha) r(W,Z_i)}\right] \\
% & {=} \frac{\lambda^{2} \alpha^2 \sigma^{2}}{2} + \log \mathbb{E}_{P_{W}P_{Z_i}}\left[e^{-\lambda(1-\alpha) r(W,Z_i)}\right] \\
% & \leq \frac{\lambda^{2} \alpha^2 \sigma^{2}}{2} - \lambda (1-\alpha) \epsilon
% \end{align}
% %By setting $\lambda=\frac{2(1-\alpha) m}{\sigma^{2}}>0$, the exponent is equal to zero. 
% where $(a)$ follows the sub-Gaussian assumption and $(b)$ follows the ESI assumption. This also shows that
% \begin{align}
% \mathbb{E}_{P_{WZ_i}}\left[\alpha \mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)]- r\left(W, Z_{i}\right)\right] \leq \frac{1}{\lambda} I\left(W ; Z_{i}\right) + \frac{\lambda \alpha^2 \sigma^2}{2} - (1-\alpha) \epsilon
% \end{align}
% By setting $\lambda = \frac{2(1-\alpha)\epsilon}{\alpha^2\sigma^2}$, we have,
% \begin{align}
% \mathbb{E}_{P_{WZ_i}}\left[\alpha \mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)] - r\left(W, Z_{i}\right)\right] \leq \frac{\alpha^2\sigma^2}{2(1-\alpha)\epsilon} I\left(W ; Z_{i}\right)
% \end{align}
% Summing all terms gives the claimed result for Lemma~\ref{lemma:alpha}. Then by decomposing the quantity of interest as
% \begin{align}
%     \alpha \mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)] - \hat{\mathcal{R}}(W, \mathcal{S}_n) = \alpha (\mathbb{E}_{P_W\otimes \mu}[r(W,Z_i)] -  \hat{\mathcal{R}}(W, \mathcal{S}_n)) - (1-\alpha)\hat{\mathcal{R}}(W, \mathcal{S}_n)),
% \end{align}
% we end up with the following inequality:
% \begin{align}
%      \alpha\mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] \leq & (1-\alpha) \mathbb{E}_{W\mathcal{S}_n}[\hat{\mathcal{R}(W,\mathcal{S}_n)}] + \frac{1}{n} \sum_{i=1}^{n} \frac{\alpha^2 \sigma^{2}}{2 (1-\alpha) \epsilon} I\left(W ; Z_{i}\right).
% \end{align}
% Dividing $\alpha$ on both L.H.S. and R.H.S., we end up with the results. 
% \end{proof}


%\subsection{Proof of~Theorem~\ref{thm:reg_erm}}
%\begin{proof}
%Consider individual terms in the difference, we have
%\begin{align}
%    \mathbb{E}_{P_{WZ_i}}\left\{t\left(L_{\mu}(W)-(1+\lambda)\ell\left(W, Z_{i}\right)\right)\right\} \leq I\left(W ; Z_{i}\right)+\mathbb{E}_{P_{W} P_{Z_{i}}}\left\{e^{t\left(L_{\mu}(W)-(1+\lambda)\ell\left(W, Z_{i}\right)\right)}\right\}
%\end{align}
%The second term on the RHS can be upper bounded as
%\begin{align*}
%\mathbb{E}_{P_{W} P_{Z_{i}}}\left\{e^{t\left(L_{\mu}(W)-(1+\lambda)\ell\left(W, Z_{i}\right)\right)}\right\} &=\mathbb{E}_{P_{W} P_{Z_{i}}}\left\{e^{t\left(L_{\mu}(W)-\ell\left(W, Z_{i}\right)\right)-t \lambda \ell(W,Z_i)}\right\} \\
%& \leq \mathbb{E}_{P_{W}P_{Z_i}}\left\{e^{\frac{t^{2} \sigma^{2}}{2}-\lambda t \ell(W,Z_i)}\right\} \\
%& \leq \mathbb{E}_{P_{W}}\left\{e^{\frac{t^{2} \sigma^{2}}{2}-\lambda t L}\right\}
%\end{align*}
%By setting $t=\frac{2\lambda L}{\sigma^{2}}>0$, the exponent is equal to zero. This also shows that
%\begin{align}
%  \mathbb{E}_{P_{WZ_i}}\left\{L_{\mu}(W)- (1+\lambda) \ell\left(W, Z_{i}\right)\right\} \leq \frac{\sigma^{2}}{2\lambda L} I\left(W ; Z_{i}\right) \label{eq:1}
%\end{align}

%Let $D_R(W,S) = \Esub{WS}{\hat L(W,S) - R(W,S)}$, summing all terms gives the claimed result.
%\end{proof}

% \subsection{Proof of Fast Rate with Gibbs Algorithm}
% We assume that the hypothesis is endowed with some prior distribution, namely, $\pi(W)$. The Gibbs algorithm defines a distribution $P_{W|SS'}$ over the hypothesis space $\mathcal{W}$ given the data sample as follows.
% \begin{align}
%     P_{W|S}(w) = \frac{\pi(w)e^{-\eta \hat L(w,S)}}{V(S,\eta)}, \label{eq:gibbs}
% \end{align}
% where $\eta$ is the inverse temperature, $\pi(w)$ denotes the prior distribution over $W$ and $V(S,\eta) = \int_{\mathcal{W}}\pi(w)e^{-\eta\hat L(w,S)} dw$ is the normalization term.
% \begin{remark}
% The Gibbs algorithm can be viewed as a stochastic empirical risk minimization algorithm (ERM). Roughly speaking, if one hypothesis $w$ incurs a lower empirical loss, the density for that certain $w$ will increase, depending on the factor $\eta$. When the training data overwhelms the prior, the posterior will converge to the neighbourhood of the ERM solution. As a special case, the Gibbs algorithm coincides with the standard ERM when $\eta$ goes to infinity \cite{aminian2021exact}.
% \end{remark}  
% We have the following theorem for the Gibbs algorithm in terms of the fast rate generalization error.
% \begin{theorem}
% Consider any loss function $\ell: \mathcal{W} \times \mathcal{Z} \rightarrow \mathbb{R}$ that is $\sigma$-sub-Gaussian in the first argument with respect to the Gibbs density
% \begin{align}
%   P_{W|S}(w) \propto e^{-\eta \hat{L}(w,S)}, \quad \eta>0
% \end{align}
% conditioned on a measurable $\mathcal{W}$. Then the generalization error of Gibbs-ERM satisfies
% \begin{align}
%     \mathbb{E}_{S}\mathbb{E}_{P_{W|S}} \left[L(W)-\hat{L}(W,S) \right]  \leq \frac{4 \sigma^{2} \eta}{n}
% \end{align}
% \end{theorem}

% \begin{proof}
% Let $S^{(i)} = (Z_1,Z_2,\cdots, Z_{i-1}, Z, Z_{i+1}, \cdots, Z_n)$ denote the data sample that $i$-th entry is replaced by $Z$ in the dataset $S$. We start by rewriting the expected generalization error as,
% \begin{align}
%     \mathbb{E}_{S}\mathbb{E}_{P_{W|S}} \left[L(W)-\hat{L}(W,S) \right] &= \frac{1}{n}\sum_{i=1}^{n} \left({\mathbb{E}}_{{S, Z}}\mathbb{E}_{P_{W|S}}\left[\ell \left(W, Z\right)\right] -{\mathbb{E}}_{S}\left[{\mathbb{E}}_{P_{W|S}}\left[\ell \left(W, Z_{i}\right)\right]\right]\right) \\
%   \textup{ (Switching $Z$ and $Z_i$) }&= \frac{1}{n}\sum_{i=1}^{n} \left({\mathbb{E}}_{{S, Z}}\mathbb{E}_{P_{W|S^{(i)}}}\left[\ell \left(W, Z_i\right)\right] -{\mathbb{E}}_{S}\left[{\mathbb{E}}_{P_{W|S}}\left[\ell \left(W, Z_{i}\right)\right]\right]\right) \\
%     &= \frac{1}{n}\sum_{i=1}^{n} {\mathbb{E}}_{{S, Z}} \left[ \mathbb{E}_{P_{W|S^{(i)}}}\left[\ell \left(W, Z_i\right)\right] - \mathbb{E}_{P_{W|S}}\left[\ell \left(W, Z_i\right)\right] \right] \\
%   \textup{ (sub-Gaussian Assumption) }  &\leq  \frac{1}{n}\sum_{i=1}^{n} {\mathbb{E}}_{{S, Z}} \sqrt{2\sigma^2 D(P_{W|S^{(i)}} \| P_{W|S})} \\
%   \textup{ (Jensen's Inequality) }  &\leq \frac{1}{n}\sum_{i=1}^{n} \sqrt{2\sigma^2 {\mathbb{E}}_{{S, Z}} [D(P_{W|S^{(i)}} \| P_{W|S})] } \\
%   &= \frac{1}{n}\sum_{i=1}^{n} \sqrt{2\sigma^2  D(P_{W|S^{(i)}} \| P_{W|S} |S,Z) } 
% \end{align}
% Next we will explicitly write out the conditional KL divergence as,
% \begin{align}
%     D(P_{W|S^{(i)}} \| P_{W|S} |S,Z) &= \mathbb{E}_{S,Z} \left[\eta \left( \mathbb{E}_{P_{W|S^{(i)}}}[\hat{L}(W,S)] -  \mathbb{E}_{P_{W|S^{(i)}}}[\hat{L}(W,S^{(i)})] \right) - \log \frac{\int \pi(w)e^{-\eta \hat{L}(W,S^{(i)})} dw}{\int \pi(w) e^{-\eta \hat{L}(W,S)} dw} \right]  \\
%     &= \mathbb{E}_{S,Z} \left[\eta \left( \mathbb{E}_{P_{W|S^{(i)}}}[\hat{L}(W,S)] -  \mathbb{E}_{P_{W|S^{(i)}}}[\hat{L}(W,S^{(i)})] \right) - \log \mathbb{E}_{P_{W|S}}[e^{\eta(\hat{L}(W,S) - \hat{L}(W,S^{(i)}))}]\right]  \\
%     \textup{ (Jensen's Inequality) } & \leq \mathbb{E}_{S,Z} \left[\eta \left( \mathbb{E}_{P_{W|S^{(i)}}}[\hat{L}(W,S)] -  \mathbb{E}_{P_{W|S^{(i)}}}[\hat{L}(W,S^{(i)})] \right) + \mathbb{E}_{P_{W|S}}[\eta(\hat{L}(W,S^{(i)}) - \hat{L}(W,S))]\right] \\
%     &= \mathbb{E}_{S,Z} \left[\frac{\eta}{n} \left( \mathbb{E}_{P_{W|S^{(i)}}}[\ell(W,Z_i)] -  \mathbb{E}_{P_{W|S^{(i)}}}[\ell(W,Z)] \right) + \frac{\eta}{n}\mathbb{E}_{P_{W|S}}[\ell(W,Z) - \ell(W,Z_i)]\right] \\
%   \textup{ (Switching $Z$ and $Z_i$) } &= \mathbb{E}_{S,Z} \left[\frac{2\eta}{n} \left( \mathbb{E}_{P_{W|S}}[\ell(W,Z)] -  \mathbb{E}_{P_{W|S}}[\ell(W,Z_i)] \right) \right]
% \end{align}
% Therefore,
% \begin{align}
%     {\mathbb{E}}_{{S, Z}}\left[ \mathbb{E}_{P_{W|S}}\left[\ell \left(W, Z\right)\right] -\left[{\mathbb{E}}_{P_{W|S}}\left[\ell \left(W, Z_{i}\right)\right]\right] \right] \leq \sqrt{\frac{4\sigma^2\eta}{n} {\mathbb{E}}_{{S, Z}}\left[ \mathbb{E}_{P_{W|S}}\left[\ell \left(W, Z\right)\right] -\left[{\mathbb{E}}_{P_{W|S}}\left[\ell \left(W, Z_{i}\right)\right]\right] \right] }
% \end{align}
% As a consequence, for each $Z_i$, 
% \begin{align}
%     {\mathbb{E}}_{{S, Z}}\left[ \mathbb{E}_{P_{W|S}}\left[\ell \left(W, Z\right)\right] -\left[{\mathbb{E}}_{P_{W|S}}\left[\ell \left(W, Z_{i}\right)\right]\right] \right] \leq \frac{4\sigma^2\eta}{n}
% \end{align}
% which completes the proof.
% \end{proof}




% \begin{align*}
%     \log \mathbb{E}\left[ e^{\eta(\mathbb{E}[r(W,Z)] - r(W,Z))}\right] \leq \eta^2 \sigma^2
% \end{align*}

% \begin{align*}
%     \log \mathbb{E}\left[ e^{\eta(\mathbb{E}[r(W,Z)] - r(W,Z))}\right] \leq k \eta^2  \mathbb{E}[r(W,Z)]
% \end{align*}

% \subsection{Proof of Theorem~\ref{thm:sub-Gaussian_aux}}\label{proof:sub-Gaussian_aux}
% \begin{proof}
% Let us first rewrite the expected generalization error as follows.
% \begin{align*}
%     \mathbb{E}_{W\mathcal{S}_n} \left[\mathcal{E}(W, \mathcal{S}_n)\right] &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{WZ_i}[\ell(W,Z_i)] - \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] \\
%     &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{WZ_i}[\ell(W,Z_i)] - \mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] \\
%     &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{WZ_i}[\ell(W,Z_i)] - \mathbb{E}_{\hat{W}Z_i}[\ell(\hat{W},Z_i)] - (\mathbb{E}_{P_W\otimes \mu}[\ell(W,Z_i)] - \mathbb{E}_{\hat{W}Z_i}[\ell(\hat{W},Z_i)]) \\
%     &= \frac{1}{n}\sum_{i=1}^{n} \mathbb{E}_{W\hat{W}Z_i}[\ell(W,Z_i) - \ell(\hat{W}, Z_i)] - \mathbb{E}_{W\otimes {\hat{W}Z_i}}[\ell(W,Z_i) - \ell(\hat{W},Z_i)]
% \end{align*}
% where $P_{W\hat{W}Z_i}$ is some distribution whose marginals are $P_{WZ_i}$ and $P_{\hat{W}Z_i}$.
% \end{proof}
\subsection{Calculation Details of Example~\ref{sec:example}}\label{apd:example2}
In this section, we present the calculation details of the Gaussian mean estimation case. Let us consider the 1D-Gaussian mean estimation problem. Let $\ell(w,z_i) = (w-z_i)^2$; each sample is drawn from some Gaussian distribution, i.e., $Z_i \sim \mathcal{N}(\mu, \sigma_N^2)$. Then the ERM algorithm arrives at,
\begin{equation}
 W_{\ERM} = \frac{1}{n} \sum_{i=1}^{n} Z_i \sim \mathcal{N}(\mu, \frac{\sigma_N^2}{n}).
\end{equation}
It can be easily calculated that the optimal $w^*$ satisfies:
\begin{equation}
    w^* = \argmin \Esub{Z}{\ell(w,Z)} =\argmin \Esub{Z}{(w-Z)^2} = \mu.
\end{equation}
Also, it can be calculated that the expected excess risk is,
\begin{align}
    \Esub{W}{\mathcal{R}(W_\ERM)} &= \mathbb{E}_{P_W \otimes \mu}[\ell(W_\ERM,Z)] - \mathbb{E}_{Z}[\ell(w^*,Z)] \\
    &= \mathbb{E}_{P_W \otimes \mu}[(W_\ERM - Z)^2]  - \mathbb{E}_{Z}[(\mu - Z)^2]\\
    &=  \mu^2 + \frac{\sigma_N^2}{n} + \mu^2 + \sigma_N^2 - 2\mu^2 -  \mu^2 - \mu^2 -\sigma_N^2 + 2\mu^2 \\
    &= \frac{\sigma_N^2}{n}.
\end{align}
The corresponding empirical excess risk is given by,
\begin{align}
    \Esub{W\mathcal{S}_n}{\hat{\mathcal{R}}(W_\ERM,\mathcal{S}_n)} &= \Esub{W_\ERM \mathcal{S}_n}{\hat L(W_\ERM,\mathcal{S}_n) - \hat{L}(w^*,\mathcal{S}_n)} \\
            &=  \E{\frac{1}{n}\sum_{i=1}^{n}(W-Z_i)^2 - \frac{1}{n}\sum_{i=1}^{n}(\mu - Z_i)^2} \\
            % &=  \E{\frac{1}{n}\sum_{i=1}^{n}(W^2 - \mu^2) - \frac{2}{n}\sum_{i=1}^{n}WZ_i - \mu Z_i} \\
            &= \mu^2+ \frac{\sigma_N^2}{n} - \mu^2 - 2\mu^2 - \frac{2}{n} \sigma_N^2 + 2\mu^2 \\
            &= -\frac{\sigma_N^2}{n}.
\end{align}
Then it yields the expected generalization error as,
\begin{align}
    \Esub{W\mathcal{S}_n}{\mathcal{E}(W_\ERM, \mathcal{S}_n)} &= \Esub{W\mathcal{S}_n}{\mathcal{R}(W_\ERM)-\hat {\mathcal{R}}(W_\ERM,\mathcal{S}_n)} \\
    &= \frac{\sigma_N^2}{n} - (- \frac{\sigma_N^2}{n}) \\
    &= \frac{2\sigma_N^2}{n}.
\end{align}
The expected loss can be calculated as,
\begin{align}
    \mathbb{E}_{P_W \otimes \mu}[\ell(W_{\ERM},Z)] = \frac{n+1}{n}\sigma_N^2 := \sigma_W^2.
\end{align}
Let us verify the moment-generating functions for the squared loss. Since $\ell(W_\ERM,Z)$ is $\sigma^2_W\chi^2_1$ distributed, 
\begin{align}
    \log \mathbb{E}_{P_W \otimes \mu}[e^{\eta (W_\ERM - Z)^2}] = -\frac{1}{2}\log(1- 2\sigma^2_W \eta).
\end{align}
Hence, 
\begin{align}
    \log \mathbb{E}_{P_W \otimes \mu}[e^{\eta \left((W_\ERM - Z)^2 - \mathbb{E}[(W_\ERM - Z)^2] \right)}] = -\frac{1}{2}\log(1- 2\sigma^2_W \eta) - \sigma^2_W\eta.
\end{align}
It is easy to prove that for any $x\leq 0$, 
\begin{align}
    -\frac{1}{2}\log(1-2x) - x \leq x^2,
\end{align}
which yields,
\begin{align}
    \log \mathbb{E}_{P_W \otimes \mu}[e^{\eta \left((W_\ERM - Z)^2 - \mathbb{E}[(W_\ERM - Z)^2] \right)}] = -\frac{1}{2}\log(1- 2\sigma^2_W \eta) - \sigma^2_W\eta \leq \sigma_W^4 \eta^2 
\end{align}
for any $\eta < 0$. Therefore, $\ell(W,Z)$ is $\sqrt{2\sigma^4_W}$-sub-Gaussian and we can only achieve the slow rate of $O(\sqrt{1/n})$ with the bound by \cite{bu2020tightening}.

\subsection{Calculation Details of Example~\ref{example:sub-Gaussian-2}}\label{apd:example3}
Now we introduce $w^*$ in the sequel as a comparison. For a given $\eta$ and $w$, we calculate the moment generating function for the term $ r(w,Z) =(w - Z)^2 - (w^* - Z)^2$ as follows. Using the known results for moment-generating function of Gaussian random variables, we have
\begin{align}
 &\mathbb{E}_{\mu}[e^{\eta r(w,Z)}] = \frac{1}{\sqrt{2\pi \sigma_N^2}} \int  e^{-\frac{(z-\mu)^2}{2\sigma_N^2}}  e^{\eta ((w - z)^2 - (w^* - z)^2)}  dz =\operatorname{exp}\left( (2\eta^2\sigma_N^2 + \eta)(w-\mu)^2 \right).
\end{align}
% \begin{align}
%  &\mathbb{E}_{\mu}[e^{\eta r(w,Z)}] = \frac{1}{\sqrt{2\pi \sigma_N^2}} \int  e^{-\frac{(z-\mu)^2}{2\sigma_N^2}}  e^{\eta ((w - z)^2 - (w^* - z)^2)}  dz \\
%  &= \frac{1}{\sqrt{2\pi \sigma_N^2}} \int  \operatorname{exp}\left(-\frac{(z - \mu + 2\eta \sigma_N^2(w-\mu))^2}{2\sigma_N^2}\right) dz \operatorname{exp}\left( -2\mu\eta (w - \mu) + 2\eta^2\sigma_N^2 (w-\mu)^2 - \eta (\mu^2 - w^2) \right) \\
%  &= \operatorname{exp}\left( -2\mu\eta (w - \mu) + 2\eta^2\sigma_N^2 (w-\mu)^2 - \eta (\mu^2 - w^2) \right) \\
%  &= \operatorname{exp}\left( (2\eta^2\sigma_N^2 + \eta)(w-\mu)^2 \right).
% \end{align}
Taking expectation over $W$ w.r.t. the ERM solution, and using the known results for the moment-generating function of the chi-square random variables,  we have:
\begin{align}
 \mathbb{E}_{P_W \otimes \mu}[e^{\eta r(W,Z)}] &= \Esub{W}{\operatorname{exp}\left( (2\eta^2\sigma_N^2 + \eta) (W-\mu)^2 \right)}     = \sqrt{\frac{n}{ n- (4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2)}}.
\end{align}
% \begin{align}
%  \mathbb{E}_{P_W \otimes \mu}[e^{\eta r(W,Z)}] &= \Esub{W}{\operatorname{exp}\left( (2\eta^2\sigma_N^2 + \eta) (w-\mu)^2 \right)} \\
%     &= \frac{1}{\sqrt{2\pi \frac{\sigma_N^2}{n}}} \int  \operatorname{exp}\left(-\frac{(w-\mu)^2}{2\sigma_N^2/n} + (2\eta^2\sigma_N^2 + \eta) (w-\mu)^2 \right)  dw \\
%     %&= \frac{1}{\sqrt{2\pi \frac{\sigma_N^2}{n}}} \int  \operatorname{exp}\left( -\frac{(w-\mu)^2}{2} (\frac{n}{\sigma_N^2} - (4\eta^2\sigma_N^2 + 2\eta)) \right)  dw \\
%     &= \frac{\sqrt{\frac{1}{\frac{n}{\sigma_N^2} - (4\eta^2\sigma_N^2 + 2\eta)}  \frac{n}{\sigma_N^2}} }{\sqrt{2\pi \frac{1}{\frac{n}{\sigma_N^2} - (4\eta^2\sigma_N^2 + 2\eta)}}} \int  \operatorname{exp}\left( -\frac{(w-\mu)^2}{2} (\frac{n}{\sigma_N^2} - (4\eta^2\sigma_N^2 + 2\eta)) \right)  dw \\
%     &= \sqrt{\frac{n}{ n- (4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2)}}.
% \end{align}
Therefore for any $\eta \in \mathbb{R}$ and any $n >  \max\left\{\frac{(4\eta^2\sigma^4_N+\eta\sigma^2_N)(2\eta^2\sigma^4_N+\eta\sigma^2_N)}{\eta^2\sigma^4_N}, 4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2\right\} $, we arrive at, 
\begin{align}
    \log  \mathbb{E}_{P_W \otimes \mu}[e^{\eta (r(W,Z) - \mathbb{E}[r(W,Z)]}] &= \frac{1}{2} \log \frac{n}{ n- (4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2)} - \frac{\eta\sigma_N^2}{n} \\
    & \leq \frac{1}{2} \frac{4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2}{n - 4\eta^2\sigma^4_N - 2\eta\sigma^2_N} - \frac{\eta\sigma_N^2}{n} \\
    & \leq \frac{4\eta^2\sigma_N^4}{n}. 
\end{align}
% Next we examine the Bernstein's condition, for any given $w$, we first calculate the second moment by,
% \begin{align*}
%     \mathbb{E}_{Z}[r(w,Z)^2] &= \mathbb{E}_{Z}[(w^2 - \mu^2 + 2(\mu - w)Z)^2] \\
%     &= (w^2 - \mu^2)^2 + 4(\mu - w)(w^2 - \mu^2)\mathbb{E}[Z] + 4(\mu - w)^2\mathbb{E}[Z^2] \\
%     &= (w - \mu)^2\left( (w+\mu)^2 - 4(w+\mu)\mu + 4\mu^2 + 4\sigma^2 \right) \\
%     &= (w - \mu)^4 + 4(w -\mu)^2\sigma^2
% \end{align*}
% while 
% \begin{align*}
%     \mathbb{E}_{Z}[r(w,Z)] &= (w-\mu)^2
% \end{align*}
% Taking expectation over $W_\ERM$, we have,
% \begin{align*}
%     \mathbb{E}_{P_W\otimes \mu}[r(W,Z)] &= \frac{\sigma^2}{n}
% \end{align*}
% and
% \begin{align*}
%     \mathbb{E}_{P_W\otimes \mu}[r(W,Z)^2] &= \frac{3\sigma^4}{n^2}+ \frac{4\sigma^4}{n}
% \end{align*}
which is of the order $O(\frac{1}{n})$ and we used the fact that $\frac{a+b}{n - 2a - 2b} \leq \frac{2a + b}{n}$ for $n > \max\left\{\frac{(2a+b)(2a+2b)}{a}, 2a + 2b\right\}$ with $a > 0$. The mutual information can be calculated as,
\begin{align}
    I(W_\ERM;Z_i) &= h(W_\ERM) - h(W_\ERM|Z_i)\\
    &= \frac{1}{2}\log \frac{2\pi e \sigma_N^2}{n} - \frac{1}{2}\log\frac{2\pi e (n-1)\sigma_N^2 }{n^2} \\
    & = \frac{1}{2}\log \frac{n}{n-1} \\
    & = O(\frac{1}{n}).
\end{align}
We then summarize all the quantities of interest in Table~\ref{tab:calculation_details} for references. 
\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
    \hline 
     Quantity   &  Values / Distribution \\
     \hline 
     $\mathcal{S}_n$   &  $\{Z_1,Z_2,\cdots,Z_n\}$  \\
     $Z_i$     &     $\mathcal{N}(\mu,\sigma_N^2)$ \\
      $\ell(w,z)$   &  $(w-z)^2$ \\
      $\hat{L}(w,\mathcal{S}_n)$  & $\frac{1}{n}\sum_{i=1}^{n}\ell(w,z_i)$  \\ 
      $L(w)$  &  $\Esub{Z}{\ell(w,Z)}$ \\
      $W_\ERM$  & $\mathcal{N}(\mu,\frac{\sigma_N^2}{n})$    \\
      $w^*$   & $\mu$    \\
      $r(w,z)$  & $(w-z)^2 - (w^* - z)^2$  \\
      $\mathcal{R}(w)$  & $L(w) - L(w^*)$   \\ 
      $\hat{\mathcal{R}}(w,\mathcal{S}_n)$  & $\hat{L}(w) - \hat{L}(w^*)$    \\
      $\mathcal{E}(w,\mathcal{S}_n)$  & $L(w) - \frac{1}{n}\sum_{i=1}^{n}\ell(w,z_i)$   \\
      $M_{Z}[r(w,Z)]$ & $-\frac{1}{\eta} \log \mathbb{E}_Z\left[e^{-\eta r(w,Z)}\right]$   \\
      $M_{P_W\otimes \mu}[r(w,Z)]$ & $-\frac{1}{\eta} \log \mathbb{E}_{P_W\otimes \mu}\left[e^{-\eta r(W,Z)}\right]$   \\
      \hline 
      \hline
      $\Esub{W\mathcal{S}_n}{\hat{\mathcal{R}}(W_\ERM,\mathcal{S}_n)}$   &  $-\frac{\sigma_N^2}{n}$   \\
      $\Esub{W}{\mathcal{R}(W_\ERM)}/\mathbb{E}_{P_W\otimes \mu}[r(W,Z)]$   &  $\frac{\sigma_N^2}{n}$   \\
      $\Esub{W\mathcal{S}_n}{\mathcal{E}(W_\ERM, \mathcal{S}_n)}$ & $\frac{2\sigma_N^2}{n}$   \\
      $\mathcal{R}(w)/\mathbb{E}_{Z}[r(w,Z)]$  &  $(w-\mu)^2$    \\
      $\mathbb{E}_{Z}[e^{\eta r(w,Z)}]$ & $\operatorname{exp}\left( (2\eta^2\sigma_N^2 +\eta )(w-\mu)^2 \right)$   \\
      $\mathbb{E}_{P_W\otimes \mu}[e^{\eta r(W,Z)}]$ & $\sqrt{\frac{n}{ n- (4\eta^2\sigma_N^4 + 2 \eta\sigma_N^2)}}$   \\
      $\mathbb{E}_{Z}[e^{-\eta r(w,Z)}]$ & $\operatorname{exp}\left( (2\eta^2\sigma_N^2 - \eta )(w-\mu)^2 \right)$ \\
      $\mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}]$ & $\sqrt{\frac{n}{ n- (4\eta^2\sigma_N^4 - 2 \eta\sigma_N^2)}}$ \\
     $M_{Z}[r(w,Z)]$ & $(1-2\eta\sigma_N^2)(w-\mu)^2$  \\
      $M_{P_W\otimes \mu}[r(w,Z)]$ & $(1-2\eta\sigma_N^2)\frac{\sigma_N^2}{n}$ \\
      $\mathbb{E}_{Z}[r(w,Z)^2]$ & $(w - \mu)^4 + 4(w -\mu)^2\sigma_N^2$ \\
      $\mathbb{E}_{P_W\otimes \mu}[r(W,Z)^2]$ & $\frac{3\sigma_N^4}{n^2} + \frac{4\sigma_N^4}{n}$  \\
      $I(W;Z_i)$ & $\frac{1}{2}\log\frac{n}{n-1}$ \\
      \hline 
    \end{tabular}
    \caption{Summarized Quantities}
    \label{tab:calculation_details}
\end{table}

\subsection{Condition Checking} \label{apd:condiiton_checking}
From Table~\ref{tab:calculation_details}, we can check to conclude that for most fast rate conditions such as Berstein's condition, central condition, and sub-Gaussian condition, the results will hold in expectation, but this is not the case for any $w \in \mathcal{W}$. To see this, we will check whether the condition is in succession.
\begin{itemize}
    \item When checking $\eta$-central condition, 
    \begin{itemize}
        \item For any $w$, 
        \begin{align}
            \mathbb{E}_{\mu}[e^{-\eta r(w,Z)}] = \operatorname{exp}\left( (2\eta^2\sigma_N^2 -\eta )(w-\mu)^2 \right) \leq 1, 
        \end{align}
        then we require $0 < \eta \leq \frac{1}{2\sigma_N^2}$.
        \item For $W_\ERM$,
        \begin{align}
        \mathbb{E}_{P_W\otimes \mu}[e^{-\eta r(W,Z)}]  = \sqrt{\frac{n}{ n- (4\eta^2\sigma_N^4 - 2 \eta\sigma_N^2)}} \leq 1 .
        \end{align}
        then we require $0 < \eta \leq \frac{1}{2\sigma_N^2}$.
    \end{itemize}
    \item When checking Bernstein's condition,
    \begin{itemize}
        \item For any $w\in \mathcal{W}$,
        \begin{align}
          \mathbb{E}_{\mu}[r(w,Z)^2]  &=  (w - \mu)^4 + 4(w -\mu)^2\sigma_N^2 \\
          & \leq B(\mathbb{E}_{ Z}[r(w,Z)])^{\beta} =  B(w-\mu)^{2\beta}.
        \end{align}
        Apparently, this does not hold for all $w \in \mathbb{R}$ when $\beta \in [0,1]$.
        \item For $W_\ERM$,
        \begin{align}
        \mathbb{E}_{P_W\otimes \mu}[r(W_\ERM,Z)^2]  &= \frac{3\sigma_N^4}{n^2}+ \frac{4\sigma_N^4}{n} \\
        &\leq B(\mathbb{E}_{P_W\otimes \mu}[r(W_\ERM,Z)])^{\beta} \\
        &=  B(\frac{\sigma_N^2}{n})^{\beta}.
        \end{align}
        This holds for $\beta = 1$ and $B = 7\sigma_N^2$.
    \end{itemize}
    \item When checking witness condition,
    \begin{itemize}
    \item For any $w \in \mathcal{W}$, we require that,
    \begin{align}
        \mathbb{E}_{Z}\left[\left(r(w,Z) \right) \cdot \mathbf{1}_{\left\{r(w,Z) \leq u \right\}}\right] &\geq c \mathbb{E}_{Z}\left[r(w,Z) \right] = c(w-\mu)^2.
    \end{align}
    There does not exist finite $c$ and $u$ that satisfy the above inequality, so the witness condition does not hold for all $w \in \mathcal{W}$.
    \item For $W_\ERM$,
    \begin{align}
        & \mathbb{E}_{P_W\otimes \mu}\left[\left(r(W_\ERM,Z) \right) \cdot \mathbf{1}_{\left\{r(W_\ERM,Z) \leq u \right\}}\right]   \geq c \mathbb{E}_{P_W\otimes \mu}\left[r(W,Z) \right] = \frac{c\sigma_N^2}{n}.
    \end{align}
    In this case, with high probability $r(W,Z)$ approaches zero, and there exists $u$ and $c$ satisfying the above inequality.
    \end{itemize}
    \item When checking the sub-Gaussian condition,
    \begin{itemize}
    \item For $W_\ERM$, when $0 < \eta \leq \frac{1}{2\sigma_N^2}$, we have,
    \begin{align}
        & \log \mathbb{E}_{P_W\otimes \mu}\left[e^{-\eta \left( r(W_\ERM,Z) - \mathbb{E}[r(W_\ERM,Z)]\right)} \right]  \sim \frac{2\eta^2\sigma_N^4}{n}.
    \end{align}
    Then it satisfy with the $\sigma'^2$-sub-Gaussian condition that $\sigma'^2 = \frac{4\sigma_N^4}{n}$.
    \item For any $w$, 
     \begin{align}
          \log \mathbb{E}_{Z}[e^{\eta r(w,Z)}] = 2\eta^2\sigma_N^2(w-\mu)^2.
     \end{align}
     Since $w$ is unbounded, it does not satisfy the sub-Gaussian assumption for all $w\in\mathcal{W}$.
    \end{itemize}
    \item When checking the $(\eta,c)$-central condition, we confirmed that the learning tuple satisfies the $(\eta,c)$-central condition under $P_W\otimes \mu$ for the ERM algorithm. Now we consider the case for any hypothesis $w$.
    \begin{itemize}
        \item For any constant hypothesis $w$: we have that 
    \begin{align}
    \mathbb{E}_{\mu}[e^{-\eta r(w,Z)}]  = \operatorname{exp}\left( (2\eta^2\sigma_N^2 - \eta )(w-\mu)^2 \right),
    \end{align}
    where the excess risk can be calculated as:
    \begin{align}
    \mathbb{E}_{\mu}[r(w,Z)] = (w-\mu)^2.
    \end{align}
    Therefore, the $(\eta,c)$-central condition is satisfied as
    \begin{align}
    \log \mathbb{E}_{\mu}[e^{-\eta r(w,Z)}] =(2\eta^2\sigma_N^2 - \eta )(w-\mu)^2 \leq -c\eta (w-\mu)^2.
    \end{align}
    for any $\eta \leq \frac{1}{2\sigma^2_N}$ and any $c \leq 1 - 2\eta \sigma^2_N$. 
    \end{itemize}
\end{itemize}



%\section{Paper Format}

%\subsection{Templates}

%The paper (A4 or letter size, double-column format, not exceeding
%5~pages) should be formatted as shown in this sample \LaTeX{} file
%\cite{Laport:LaTeX, GMS:LaTeXComp, oetiker_latex, typesetmoser}.

%The use of Microsoft Word instead of \LaTeX{} is strongly
%discouraged. However, acceptable formatting may be achieved by using
%the template that can be downloaded from the ISIT 2019 website:
%\begin{center}
%  \url{http://isit2019.fr/}
%\end{center}

%Users of other text processing systems should attempt to duplicate the
%style of this example, in particular the sizes and type of font, as
%closely as possible.


%\subsection{Formatting}

%The style of references, equations, figures, tables, etc., should be
%the same as for the \emph{IEEE Transactions on Information
%  Theory}. The source file of this template paper contains many more
%instructions on how to format your paper. So, example code for
%different numbers of authors, for figures and tables, and references
%can be found (they are commented out).

%%%%%%
%% An example of a floating figure using the graphicx package.
%% Note that \label must occur AFTER (or within) \caption.
%% For figures, \caption should occur after the \includegraphics.
%%
% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.3\textwidth]{myfigure}
%   % where an .eps filename suffix will be assumed under latex,
%   % and a .pdf suffix will be assumed for pdflatex
%   \caption{Simulation results.}
%   \label{fig:sim}
% \end{figure}
%%%%%%

%%%%%%
%% An example of a double column floating figure using two subfigures.
%% (The subfigure.sty package must be loaded for this to work.)  The
%% subfigure \label commands are set within each subfigure command,
%% the \label for the overall figure must come after \caption.  
%% \hfil must be used as a separator to get equal spacing
%%
% \begin{figure*}[htbp]
%   \centerline{\subfigure[Case I]{\includegraphics[width=2.5in]{subfigcase1}
%       % where an .eps filename suffix will be assumed under latex,
%       % and a .pdf suffix will be assumed for pdflatex
%       \label{fig:first_case}}
%     \hfil
%     \subfigure[Case II]{\includegraphics[width=2.5in]{subfigcase2}
%       % where an .eps filename suffix will be assumed under latex,
%       % and a .pdf suffix will be assumed for pdflatex
%       \label{fig:second_case}}}
%   \caption{Simulation results.}
%   \label{fig:sim}
% \end{figure*}
%%%%%%

%%%%%%
%% An example of a floating table. 
%% Note that, for IEEE style tables, the \caption command should come
%% BEFORE the table. Table text will default to \footnotesize as IEEE
%% normally uses this smaller font for tables.  The \label must come
%% after \caption as always.
%%
% \begin{table}[htbp]
%   % increase table row spacing, adjust to taste
%   \renewcommand{\arraystretch}{1.3}
%   \caption{An Example of a Table}
%   \label{tab:table_example}
%   \centering
%   % Some packages, such as MDW tools, offer better commands for making tables
%   % than the plain LaTeX2e tabular which is used here.
%   \begin{tabular}{|c||c|}
%     \hline
%     One & Two\\
%     \hline
%     Three & Four\\
%     \hline
%   \end{tabular}
% \end{table}
%%%%%%


%For instructions on how to typeset math, in particular for equation
%arrays with broken equations, we refer to \cite{typesetmoser}.

%Pages should not be numbered and there should be no footer or header
%(both will be added during the production of the proceedings). The
%affiliation shown for authors should constitute a sufficient mailing
%address for persons who wish to write for more details about the
%paper.


%\subsection{PDF Requirements}

%Only electronic submissions in form of a PDF file will be
%accepted. The PDF file has to be PDF/A compliant. A common problem is
%missing fonts. Make sure that all fonts are embedded. (In some cases,
%printing a PDF to a PostScript file, and then creating a new PDF with
%Acrobat Distiller, may do the trick.) More information (including
%suitable Acrobat Distiller Settings) is available from the IEEE
%website \cite{IEEE:pdfsettings, IEEE:AuthorToolbox}.


%\section{Conclusion}

%We conclude by pointing out that on the last page the columns need to
%balanced. Instructions for that purpose are given in the source file.

%Moreover, example code for an appendix (or appendices) can also be
%found in the source file (they are commented out).

%%%%%%
%% Appendix:
%% If needed a single appendix is created by
%%
%\appendix
%%
%% If several appendices are needed, then the command
%%
% \appendices
%%
%% in combination with further \section-commands can be used.
%%%%%%


%\section*{Acknowledgment}

%We are indebted to Michael Shell for maintaining and improving
%\texttt{IEEEtran.cls}. 


%%%%%%
%% To balance the columns at the last page of the paper use this
%% command:
%%
%\enlargethispage{-1.2cm} 
%%
%% If the balancing should occur in the middle of the references, use
%% the following trigger:
%%
%\IEEEtriggeratref{3}
%%
%% which triggers a \newpage (i.e., new column) just before the given
%% reference number. Note that you need to adapt this if you modify
%% the paper.  The "triggered" command can be changed if desired:
%%
%\IEEEtriggercmd{\enlargethispage{-20cm}}
%%
%%%%%%


%%%%%%
%% References:
%% We recommend the usage of BibTeX:
%%
%\bibliographystyle{IEEEtran}
%\bibliography{definitions,bibliofile}
%%
%% where we here have assume the existence of the files
%% definitions.bib and bibliofile.bib.
%% BibTeX documentation can be obtained at:
%% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
%%%%%%


%% Or you use manual references (pay attention to consistency and the
%% formatting style!):
%\begin{thebibliography}{9}

%\bibitem{Laport:LaTeX}
%L.~Lamport,
%  \emph{\LaTeX: A Document Preparation System,} 
%  Addison-Wesley, Reading, Massachusetts, USA, 2nd~ed., 1994. 

%\bibitem{GMS:LaTeXComp}
%F.~Mittelbach, M,~Goossens, J.~Braams, D.~Carlisle, and
%C.~Rowley, \emph{The {\LaTeX} Companion,} Addison-Wesley,
%Reading, Massachusetts, USA, 2nd~ed., 2004.

%\bibitem{oetiker_latex}
%T.~Oetiker, H.~Partl, I.~Hyna, and E.~Schlegl, \emph{The Not So Short
%  Introduction to {\LaTeX2e}}, version 5.06, Jun.~20, 2016. [Online].
%  Available: \url{https://tobi.oetiker.ch/lshort/}

%\bibitem{typesetmoser}
%S.~M. Moser, \emph{How to Typeset Equations in {\LaTeX}}, version 4.6,
%  Sep. 29, 2017. [Online]. Available:
%  \url{http://moser-isi.ethz.ch/manuals.html#eqlatex}

%\bibitem{IEEE:pdfsettings}
%IEEE, \emph{Preparing Conference Content for the IEEE Xplore Digital
%  Library.} [Online]. Available:
%  \url{http://www.ieee.org/conferences_events/conferences/organizers/pubs/preparing_content.html}

%\bibitem{IEEE:AuthorToolbox}
%IEEE, \emph{Author Digital Toolbox.} [Online.] Available:
%  \url{http://www.ieee.org/publications_standards/publications/authors/authors_journals.html}

%\end{thebibliography}


\end{document}


%%%%%%
%% Some comments about useful packages
%% (extract from bare_conf.tex by Michael Shell)
%%

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx


% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array

% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig


% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


% *** PDF and URL PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.



% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%%%%%%


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
