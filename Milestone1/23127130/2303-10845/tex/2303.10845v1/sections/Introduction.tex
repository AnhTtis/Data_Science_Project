\documentclass[../main.tex]{subfiles}

\begin{document}

Large Language Models (LLMs)~\cite[etc.]{brown2020GPT3,2020Exploring, Zeng2021PanGuLA,2022PaLM,2021Scaling,Zhang2022OPTOP,CPM2020,ERNIE3TITAN,zeng2022glm,scao2022bloom} have demonstrated unprecedented capabilities and potential in the areas of natural language understanding, generation and reasoning. By utilizing vast amount of textual data, the performance of language models scales up with compute budget and model parameters, demonstrating strong zero/few-shot learning abilities or even emergence abilities~\cite{2022PaLM,wei2022emergent}. Several large language models with hundreds of billion parameters have been published since GPT-3~\cite{brown2020GPT3}, including but not limited to Megatron-Turing NLG~\cite{smith2022using}, PanGu-$\alpha$~\cite{Zeng2021PanGuLA}, ERNIE 3.0 Titan~\cite{ERNIE3TITAN}, Gopher~\cite{2021Scaling}, PaLM~\cite{2022PaLM}, OPT~\cite{Zhang2022OPTOP}, 
Bloom~\cite{scao2022bloom}, and GLM-130B~\cite{zeng2022glm}. Researchers start to build even larger language models with more than one trillion parameters. Typically, this is accomplished by leveraging sparsely-activated models such as Mixture-of-Experts (MoE)~\cite{shazeer2017outrageously}. Among the trillion-parameter models currently in existence, there are several noteworthy work such as Switch-C~\cite{fedus2021switch}, GLaM~\cite{2021GLaM},  MoE-1.1T~\cite{artetxe2021efficient}, Wu Dao 2.0~\cite{wudao2}, and M6-10T~\cite{lin2021m6}. However, only a select few have published comprehensive evaluation results over a wide range of tasks while simultaneously achieving anticipated performance. In our experience, the primary difficulty lies in the scaling efficiency.

Recent studies on the scaling laws of language models~\cite{2020Scaling,2022Training,clark2022unified} demonstrate the necessity of training LLMs with sufficient amount of training data and corresponding compute budget to achieve optimal performance. Therefore, one of the main motivation for this work is to design a scalable model architecture and an efficient distributed training system that can consume the data with high training throughput.

\begin{itemize}
\item \textbf{Model Scaling}.
Model performance of LLMs is expected to scale up with larger model size. Comparing to the expensive computational cost for training dense Transformer model, sparse architectures such as Mixture-of-Experts (MoE)~\cite{shazeer2017outrageously,fedus2021switch,2021GLaM,2021GShard} are considered to be an appealing choice to scale model size up without incuring linear increase in computational cost. However, MoE models suffer from the problems such as unbalanced workload and all-to-all communication latency. Moreover, how to extend existing dense model with MoE and how many experts to allocate in each layer remain open problems. Therefore, designing a trillion parameter sparse model with high performance and training efficiency is a significant yet challenging task.

\item \textbf{System Scaling}.
Frameworks such as DeepSpeed~\footnote{\url{https://www.deepspeed.ai/}} have been proposed to support training trillion parameter models. In practice, the main barrier often lies on limited compute budget, or more specifically the number of accelerating devices (e.g., GPU, NPU, TPU) that can be used. By utilizing techniques such as tensor parallelism~\cite{Megatron}, pipeline parallelism~\cite{Gpipe}, zero redundancy optimizer~\cite{zero} and rematerialization~\cite{2016Training}, practitioners can train trillion-parameter model with feasible batch sizes across thousands of accelerating devices. Alternatively, practitioners can reduce the amount of computation resources by utilizing heterogeneous computing techniques such as offloading some of the computation to host devices~\cite{2021ZeRO}. However, the current techniques inevitably hinder the training throughput due to slow bandwidth between the host and device as well as weak computing capabilities of CPUs compared to accelerating devices, which prevent feeding large language models with reasonably amount of data and achieving optimal performance. Therefore, how to efficiently scale the system performance with limited computation budget is critical to the performance of large language models.
\end{itemize}

In this work, we present \MODEL\ , a large language model with sparse architecture containing 1.085 trillion parameters. We develop \MODEL\ model under the framework of MindSpore~\footnote{\url{https://gitee.com/mindspore/mindspore}} and train it on a cluster with only 512 Ascend 910 AI Accelerators~\cite{2021Ascend} with 329 billion tokens over 100 days. \MODEL\ inherent parameters from PanGu-$\alpha$~\cite{Zeng2021PanGuLA} with Transformer decoder architecture and are extended via Random Routed Experts (RRE). Different from conventional MoE, RRE adopts two-level routing. At the first level experts are grouped by domain or task, and at the second level tokens are randomly and uniformly mapped to experts in each group without using any learnable gating function as in MoE. With the design of RRE, one can easily extract sub-models from the \MODEL\ for various downstream applications such as dialogue, translation, code generation or general nature language understanding. To make training system efficient and scalable, we propose Expert Computation and Storage Separation (ECSS) mechanism, which achieves 69905 tokens/s observed throughput in training 1.085 trillion \MODEL\ on cluster of 512 Ascend 910 accelerators, and reduces Host-to-Device and Device-to-Host communication as well as optimizer update computation by a large margin. As a whole, the training throughput is improved by 6.3x compared to the model of the same hyper-parameters but with MoE architecture. By consuming 329B tokens in more than 40 natural and programming languages, the sub-modal of \MODEL\ in Chinese domain significantly outperforms the previous SOTA models including PanGu-$\alpha$ with 13B parameters and ERNIE 3.0 Titan~\cite{ERNIE3TITAN} with 260B parameters over 16 downstream tasks in six categories in the zero-shot setting without any multitask finetuning or instruction tuning. We also test the performance of fine-tuned \MODEL\ on several applications domain such as dialogue, machine translation and code generation. \MODEL\ outperforms the SOTA models in the corresponding areas.

The rest of the technical report is organized as follows. Section 2 introduces the design philosophy and the architecture of \MODEL\ model. Section 3 introduces the collection and organization of the dataset. Section 4 describes system design and acceleration techniques. Section 5 presents the experimental results of \MODEL\ model. 

\end{document}