\documentclass[../main.tex]{subfiles}

\begin{document}

% Add dataset in this file.
% Including dataset collection, clean, selection, dataset format...
\subsection{Collection}
To better demonstrate the capability of \MODEL\ model to efficiently and independently learn from multiple domains, we collect datasets in 40 domains, with a large amount of data in four major domains: Chinese, English, Bilingual (Chinese and English) and code. The remaining domains with smaller portion consists of 26 other monolingual natural languages, 6 programming languages, and textual data from finance, health, law, and poetry domains, respectively. 

For Chinese texts, we collect the WuDaoCorpora 2.0~\cite{Yuan2021WuDaoCorporaAS} which contains 200GB and the CLUECorpus2020~\cite{Xu2020CLUECorpus2020AL} which contains 100GB. For English texts, the Pile dataset~\cite{Gao2021ThePA} which contains 800GB and C4 dataset~\cite{2020Exploring} which contains 750GB were collected. For code, we use the Python code (147GB) which has been used in PanGu-Coder~\cite{Christopoulou2022PanGuCoderPS}, as well as the Java code (161GB) from GHTorrent~\cite{Gousios2013TheGD} , which are then filtered by file size ($<$1MB), average number of characters per line ($<$200), maximum number of characters per line ($<$1000) and their compilablity. Then, these collected English, Chinese and code texts data was sampled and distributed to the four major domains. Finally, we get more than 300B tokens for the four major domains. The detailed statistics of data distribution and data sources in four major domains are presented in Table~\ref{tab:data_dis}.

For the remaining 36 domains, the data for 26 monolingual domains are mainly from CCAligned~\cite{ElKishky2019AMC} and CCMatrix~\cite{Schwenk2019CCMatrixMB}. Similar to the code domain mentioned above, the data for 6 programming language domains are collected through GHTorrent~\cite{Gousios2013TheGD} and filtered in the similar way. Finance domain data is filtered from the WuDaoCorpora 2.0~\cite{Yuan2021WuDaoCorporaAS} using the tags. Health domain data is from Chinese MedDialog Dataset ~\cite{Zeng2020MedDialogLM}. Law domain data is sampled from CAIL2018~\cite{Xiao2018CAIL2018AL}. Poetry domain dataset is from Werneror-Poetery~\footnote{\url{https://github.com/Werneror/Poetry}}. Finally, we sampled more than 25B tokens for the 36 domains.


\begin{table}
\centering
\caption{Data distribution and data sources in four main domains}
\label{tab:data_dis}
\scalebox{0.8}{
\begin{tabular}{cccc} 
\hline
Domain ID & Domain                                                                & Tokens (Billion)                                                             & Data source                                                                                                    \\ 
\hline
0         & \begin{tabular}[c]{@{}c@{}}\textbf{Bilingual}\\(Chinese, English)\end{tabular}      & \begin{tabular}[c]{@{}c@{}}77.51 B\\Chinese (38.75) + English(38.76B)\end{tabular} & CLUECorpus2020 , C4                                                                                            \\ 
\hline
1         & \textbf{\textbf{Chinese}}                                             & 75.47 B                                                                            & WuDaoCorpora 2.0~ ~                                                                                            \\ 
\hline
2         & \textbf{\textbf{English}}                                             & 75.90 B                                                                            & Pile , C4                                                                                                      \\ 
\hline
3         & \begin{tabular}[c]{@{}c@{}}\textbf{Code}\\(Python, Java)\end{tabular} & \begin{tabular}[c]{@{}c@{}}75.24 B\\Python (50.24B) + Java (25B)~~~~~\end{tabular} & \begin{tabular}[c]{@{}c@{}}Python (PanGu-Coder)~\\Java (GHTorrent)\end{tabular}  \\ 
\hline
\end{tabular}
}
\end{table}

\subsection{Format}
For the four major domains, each can be adapted to different downstream tasks. In order to better support domain-specific downstream tasks, this paper uses different data format for different domains. For Chinese and English domains, the <EOT> token which indicates the end of training text is inserted at the end of each training sample.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{sections/fig/zhoupingyi/sample_1.pdf}
    \caption{Data format of Chinese and English domains.
    }
    \label{fig:sample_1}
\end{figure*}

For Bilingual domain, the <EN> or <CN> token is inserted into the head of the training sample according to the source of the training sample (either from the Chinese dataset or the English dataset), and the <EOT> token is inserted at the end of each training sample.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{sections/fig/zhoupingyi/sample_2.pdf}
    \caption{Data format of Bilingual domain.
    }
    \label{fig:sample_2}
\end{figure*}

For the code domain, the <Python> or <Java> token is inserted into the head of the training sample based on the programming language type of the training sample, and the <EOT> token is inserted at the end of each training sample.

For the remaining 36 domains, the data formats of 26 monolingual domains, finance, health, law, and poetry domains are the same as the Chinese and English domains, and the data format of 6 programming language domains is the same as the code domain.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{sections/fig/zhoupingyi/sample_3.pdf}
    \caption{Data format of Code domain.
    }
    \label{fig:sample_3}
\end{figure*}

For a formatted data set $D$, suppose it contains n training samples $D=\left \{ s_{1}, s_{2}, \dots,  s_{n} \right \} $. To make full use of the computing power of the Ascend 910 cluster and accelerate training in the pre-training phase, we concatenate all samples in the data set into a sequence, and then intercept training instances in the concatenated sequence according to the fixed length (1024), as shown in Figure~\ref{fig:format_1}. In the fine-tune phase, for each training sample in the formatted dataset, if the length is less than the fixed length, we pad the sample to the fixed length with a special token <Pad>. If the length is greater than the fixed length, the extra part is truncated. Figure~\ref{fig:format_2} shows the process. Different to 
PanGu-$\alpha$ model, each training sample of \MODEL\ model contains two field: input sequence of token IDs which are training instance and their domain ID. The domain ID indicates which domain the training instance belongs to. The RRE layers of the \MODEL\ model decide which experts the training tokens is routed to by the domain ID. 

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{sections/fig/zhoupingyi/format_1.pdf}
    \caption{Input format during model pre-training.
    }
    \label{fig:format_1}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{sections/fig/zhoupingyi/format_2.pdf}
    \caption{Input format during model fine-tuning.
    }
    \label{fig:format_2}
\end{figure*}

\end{document}