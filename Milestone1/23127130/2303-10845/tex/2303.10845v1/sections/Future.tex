\documentclass[../main.tex]{subfiles}

\begin{document}

In this work, we have present a trillion parameters language model architecture \MODEL\ . With the Random Routed Experts (RRE) and Expert Computation Storage Separation (ECSS), \MODEL\ achieves high system performance under the MindSpore framework using Ascend 910 AI accelerators. By extending and continually training from PanGu-$\alpha$ with 329B tokens, \MODEL\ has successfully achieved state-of-the-art results in a bunch of downstream tasks such as few-shot NLU, open-domain dialogue, question answering, machine translation, and code generation. Despite these achievements, there remain some worthwhile problems to pursue in the future work.
\begin{itemize}
    \item Sparse models offer the benefits of a larger model size at reduced computation cost. Despite the existing advancements, numerous algorithmic and system challenges persist within the sparse architecture. Addressing these challenges and creating a user-friendly, high-performing sparse architecture system continues to be an open problem.
    \item Large language models are designed to be applied in real-world scenarios. Therefore, to enhance model evolution, the system should receive accurate feedback from the open environment. Although InstructGPT~\cite{2022TrainingChat} and ChatGPT~\footnote{\url{https://openai.com/blog/chatgpt}} provide promising approaches, they require a substantial amount of data labeling, which can be time-consuming and costly. Consequently, devising an efficient method to generate valuable signals that can align with the real world is a crucial research topic worth exploring.
   \item Large-scale language models provide intelligent foundations and various modalities alignment objectives for artificial intelligence systems. Therefore, utilizing language models as a foundation and incorporating multiple modalities for perception input in a multimodal model will be one of the most important topics, as already demonstrated by Flamingo~\cite{Alayrac2022FlamingoAV} and GPT-4~\cite{gpt-4} .
    \item Large language models have great potential for real time applications, but their deployment cost remains a major hurdle to overcome. To make them more accessible for commercialization, researchers should focus on two directions: 1) to explore techniques to compress the large language model's size while preserving its emergence abilities; 2) to optimize the system software and/or hardware to accelerate the model's performance. Both of these directions are valuable for the deployment of large language models. 
    \item Online knowledge updates are also critical for optimal performance of the large language model system. However, effectively storing and updating knowledge online is a significant challenge that requires advanced system infrastructure and algorithms. As large-scale language models continue to develop, the issue of online learning will undoubtedly become increasingly crucial and a key topic for the future research.
    
\end{itemize}

\end{document}