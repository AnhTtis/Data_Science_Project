\iffalse
\documentclass[../main.tex]{subfiles}
% \usepackage{CJKutf8}
\usepackage{threeparttable}

\begin{document}
\subsection{Training Details}
\begin{figure*}[]
	\centering
	\includegraphics[width=0.9\textwidth]{sections/fig/training_loss1.pdf}
	\caption{The curve of training loss for three \MODEL\ models with different model sizes. The x-axis denotes the number of tokens consumed during training, which is computed as $training\_steps * batch\_size* sequence\_length$.}
	\label{fig:loss_curve}
\end{figure*}
Our \MODEL\ models are developed using Mindspore framework and trained on Ascend.
\subsection{Task Description}
In this section, we evaluate our models on a broad spectrum of natural language tasks. Similar to the GPT-3 ~\cite{brown2020GPT3}, the experiments are conducted under 3 learning settings, i.e., zero shot, one shot, and few shot. For each task, we evaluate the models with the test sets if they are publicly available. Otherwise, the development sets are used instead. For tasks with a very large test set or development set, we sample a subset from the dataset in the experiments. The evaluation datasets are classified into 7 categories by task similarities, and we describe each category individually as follows.

\textbf{Cloze and completion tasks}, including CN\_LAMBADA ~\cite{}, CHID ~\cite{CHID}, PD\&CFT ~\cite{PDCFT}, CMRC2017 ~\cite{CMRC2017}, and CMRC2019 ~\cite{CMRC2019}. The CN\_LAMBADA is a Chinese dataset with the same format as LAMBADA ~\cite{LAMBADA}. The CHID is a multiple-choice cloze dataset designed for identifying the truth idiom from 10 candidate idioms. The PD\&CFT is a cloze dataset without options that requires the model to predict the mask word in sentences, which consists of People Daily (PD) news dataset and Children's Fairy Tale (CFT) dataset. The CMRC2017 dataset contains two different sub-dataset: cloze-style and user query reading comprehension sub-dataset. The cloze-style sub-dataset is also a cloze dataset without options. And we only evaluate the cloze-style sub-dataset in the experiment. The CMRC2019 is a sentence cloze-style dataset that involves filling the right sentence from several candidate sentences into the passage.

\textbf{Reading comprehension tasks}, including CMRC2018 ~\cite{CMRC2018}, DRCD ~\cite{DRCD}, and DuReader ~\cite{DuReader}. CMRC2018 and DRCD are span-extraction reading comprehension datasets. DuReader is a multi-document reading comprehension datasets, which comprises two sub-dataset: Search and Zhidao. The questions in DuReader dataset are classified into 3 types: description, entity and yes-no. We treat Dureader as a span-extraction reading comprehension task, and we only evaluate the Zhidao sub dataset in the experiment.

\textbf{Closed book question answering (QA) tasks}, including WebQA ~\cite{WebQA}. We follow the same closed-book setting in GPT-3 ~\cite{brown2020GPT3}. In the closed-book setting, the model is not allowed to access any external knowledge when answering questions. We evaluate the models on WebQA, which is an open-domain factoid question answering dataset.

\textbf{Winograd-Style tasks}, including CLUEWSC2020 ~\cite{CLUE}. CLUEWSC2020 is a Chinese Winograd Schema Challenge dataset, which is an anaphora/coreference resolution task. In practice, we convert the task into a multiple-choice problem.

\textbf{Common sense reasoning tasks}, including C$^{3}$ ~\cite{CLUE}. C$^{3}$ is a free-form multiple-choice reading comprehension dataset which can benefit from common sense reasoning. So, we use it to evaluate the common sense reasoning ability of the models.

\textbf{Natural language inference (NLI) tasks}, including Chinese Multi-Genre NLI (CMNLI) and Original Chinese Natural Language Inference (OCNLI) ~\cite{CLUE}. In practice, we structure the task as a three-class classification problem.

\textbf{Text classification tasks}, including TouTiao Text Classification for News Titles (TNEWS), IFLYTEK app description classification (IFLYTEK), Ant Financial Question Matching Corpus (AFQMC), and Chinese Scientific Literature (CSL) ~\cite{CLUE}. TNEWS and IFLYTEK are text classification datasets, and we randomly sample 3 false labels for each instance and performing 4-class classification. AFQMC is a binary classification task that aims to predict whether two sentences are semantically similar. CSL is a binary classification task to tell whether the candidate keywords are all original keywords of a paper.

\subsection{Evaluation Methods}
We aim to evaluate the in-context learning abilities of language models. For each task, we evaluate the models on 3 settings: zero-shot learning, one-shot learning, and few-shot learning ~\cite{brown2020GPT3}. For multiple-choice tasks, we compare the LM likelihood of each completion. For classification tasks, we treat the tasks as multiple-choice tasks because the candidate labels can be regarded as the choices. For tasks with free-form completion such as reading comprehension and cloze without options, we use top-k sampling, top-p sampling or beam search to acquire the generation results. In addition, for CN\_LAMBADA, PD\&CFT and CMRC2017, we use the network output just before the target position. The decoding strategies are for these text generation tasks are described in Table \ref{tab:decoding_strategies}. The prompt templates for each task are described in Table \ref{tab:prompt_setting}, in which 
"/" means the task does not have a prompt phrase.

\begin{table}
\centering
\renewcommand\arraystretch{1.25}
\caption{The decoding strategies for text generation tasks.}
\label{tab:decoding_strategies}
\begin{tabular}{llll} 
\hline

Task                   & Dataset        & Decoding strategies \\

\hline
Cloze and completion   & \begin{tabular}[c]{@{}l@{}} CN\_LAMBADA \\ PD\&CFT \\ CMRC2017 \end{tabular} 
                       & \begin{tabular}[c]{@{}l@{}} top-k, k=1  \\ top-k, k=1,temperature=0.9  \\ top-p, p=0.9, temperature=1 \end{tabular}\\

Reading comprehension  & \begin{tabular}[c]{@{}l@{}} CMRC2018 \\ DRCD \\ DuReader \end{tabular}
                       & \begin{tabular}[c]{@{}l@{}} top-p, p=0.8, temperature=0.8 \\ top-p, p=0.8, temperature=0.8 \\ top-p, p=0.9, temperature=0.7 \end{tabular} \\
                       
Closed book QA         & WebQA 
                       & top-k, k=5 \\
                       
\hline
\end{tabular}
\end{table}


\begin{CJK}{UTF8}{gbsn}
\begin{table}
\centering
\renewcommand\arraystretch{1.25}
\caption{The prompt templates for each task.}
\label{tab:prompt_setting}
\resizebox{\textwidth}{!}{
\begin{tabular}{llll} 
\hline

\textbf{Task}          & \textbf{Dataset}        & \textbf{Prompt} \\

\hline
Cloze and completion   & \begin{tabular}[c]{@{}l@{}} CN\_LAMBADA\\ CHID\\ PD\&CFT\\ CMRC2017\\ CMRC2019 \end{tabular}        
                       & \begin{tabular}[c]{@{}l@{}} \$Passage[MASK]\\ /\\ \$Passage[MASK]\\ \$Passage[MASK]\\ / \end{tabular}\\
                       
\specialrule{0em}{2pt}{2pt} 

Reading comprehension  & \begin{tabular}[c]{@{}l@{}} CMRC2018\\ DRCD\\ DuReader \end{tabular}
                       & \begin{tabular}[c]{@{}l@{}} 阅读文章：\$Passage\textbackslash{}n问：\$Question\textbackslash{}n答：(Read passage: \$Passage\textbackslash{}nQuestion：\$Question\textbackslash{}nAnswer: )\\ 阅读文章：\$Passage\textbackslash{}n问：\$Question\textbackslash{}n答：(Read passage: \$Passage\textbackslash{}nQuestion：\$Question\textbackslash{}nAnswer: )\\ 阅读文章：\$Passage\textbackslash{}n问：\$Question\textbackslash{}n答：(Read passage: \$Passage\textbackslash{}nQuestion：\$Question\textbackslash{}nAnswer: \end{tabular}\\
                       
\specialrule{0em}{2pt}{2pt}

Closed book QA         & WebQA 
                       & 问：\$Question\textbackslash{}n答：(Question：\$Question\textbackslash{}nAnswer: )\\
                       
\specialrule{0em}{2pt}{2pt}    

Winograd-Style         & CLUEWSC2020
                       & /\\
                       
\specialrule{0em}{2pt}{2pt} 

Common sense reasoning & C\textsuperscript{3} 
                       & 问: \$Question\textbackslash{}n答:\$Choice\textbackslash{}n该答案来自对话: \$Passage (Question: \$Question\textbackslash{}nAnswer:\$Choice\textbackslash{}nAnswer from dialogue: \$Passage)\\

\specialrule{0em}{2pt}{2pt} 
                       
NLI                    & \begin{tabular}[c]{@{}l@{}} CMNLI\\ OCNLI \end{tabular}
                       & \begin{tabular}[c]{@{}l@{}} \$S1?对/或许/错，\$S2  (\$S1?Yes/Maybe/No, \$S2)\\ \$S1?对/或许/错，\$S2  (\$S1?Yes/Maybe/No, \$S2) \end{tabular}\\

\specialrule{0em}{2pt}{2pt} 

Text classification    & \begin{tabular}[c]{@{}l@{}} TNEWS\\ IFLYTEK\\ AFQMC\\ CSL \end{tabular}
                       & \begin{tabular}[c]{@{}l@{}} 这是关于\$label的文章：\$passage (This passage is about\$label: \$passage)\\ 这是关于\$label的应用程序：\$passage (This application is about: \$passage)\\ 下面两个句子语义相同/不同：\$S1。\$S2 (The following two sentences have the same/different semantics: \$S1. \$S2)\\ 摘要：\$passage，关键词：\$keyword是/不是真实关键词 
                      (Abstract: \$passage, keywords: \$keyword True/False keywords) \end{tabular}\\
\hline
\end{tabular}}
\end{table}
\end{CJK}


\textbf{Prompt Construction for Generate-Tasks.} In the experiments, we use two types of tasks to train and evaluate the language models: generate-tasks and ppl-tasks. The generate-tasks make the language model generate the correct answers following the given prompts, which are widely applied for reading comprehension, closed-book QA and cloze with no choices. The ppl-tasks compute the perplexities of a set of candidates, and select the one offering the lowest perplexity value as the model output. The ppl-tasks are generally applicable for text classification, muti-choices cloze, natural language inference and many other NLP tasks, such as TNEWS, CHID, OCNLI and C3. We design different types of prompts for different tasks. Next, we give an example of prompt construction and past-processing for each type of tasks to enhance model reproducibility.

We adopt the few-shot learning methods as described in GPT-3~\cite{brown2020GPT3}. In each prompt, we provide K examples of context and completion, and a final example with context only. The objective of the model is to generate the accurate completion for the context-only example. In our experiments, K is unfixed because the documents are randomly sampled from the train set, and K is set to be as large as possible in order to make it easier for the model to learn the patterns. Within each few-shot prompt, the examples are separated by new line character. As a result, the model also outputs multiple topics, each of which is separated by new line characters. To avoid the model switching to unrelated topics after outputting the completion, we past-process the output of model by dropping all the tokens after the end of the first completion, i.e., the first new line character. A prompt of generate-task on CMRC2018 dataset is shown in Figure \ref{fig:rs_prompt_demo}.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.7]{sections/fig/rs_prompt_demo.png}
\caption{prompt of generate-task on CMRC2018}
\label{fig:rs_prompt_demo}
\end{figure}

The length of each prompt is limited to 1024, the maximal input length of PanGu. Therefore, we use the span of document which contains the correct answer instead of the complete document so as to increase K. 

\textbf{Prompt Construction for ppl-Tasks.} For PPL-tasks, simply increasing K does not guarantee good performance. We find that the perplexity of both PanGu and CPM~\cite{CPM2020} is sensitive to prompt keywords. Hence, we need to make the sentence consistent for PPL computation with different labels. For example, for OCNLI task, the few-shot prompt is designed as follows.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.7]{sections/fig/ppl_prompt_demo.png}
\caption{prompt of ppl-task on OCNLI}
\label{fig:ppl_prompt_demo}
\end{figure}

We compute the perplexity per candidate for different labels and the label with the minimum PPL is chosen as the model output. The perplexity is computed from <test-sentence2> by masking the loss of the front part, so as to make sure that the sentence is the same for PPL computation with different labels.


Table \ref{tab:VS26B} compares PanGu-2.6B with CPM-2.6B on 16 downstream tasks in Chinese. PanGu-2.6B achieves significantly higher performance compared with CPM-2.6B on more than 11 tasks for zero shot, 12 tasks for one shot and 14 tasks for few shot out of a total number of 16 tasks, owing to the advantage of the model structure and the high-quality corpus which incorporates Baidu encyclopedia data. In general, the experimental results indicate that PanGu-2.6B has higher in-context learning ability over CPM-2.6B, especially for few-shot learning and generate-tasks. Regarding generate-tasks, PanGu-2.6B outperforms CPM-2.6B with an improvement of 6.5 points in scores on average. Regarding ppl-tasks, PanGu is slightly weaker than CPM on OCNLI, TNEWS and IFLYTEK tasks. This phenomenon can be attributed to the larger vocabulary we use, which makes PanGu insensitive to perplexity when a small number of words in the sentence are changed.

\begin{table}
\centering
\renewcommand\arraystretch{1.25}
\caption{CPM-2.6B v.s. PanGu-2.6B}
\label{tab:VS26B}
\setlength{\tabcolsep}{1.5mm}
\begin{threeparttable}
\resizebox{\textwidth}{33mm}{\begin{tabular}{c|c|c|c|c|c c|c c|c c} 
\hline

IDX & Method & Task & Evaluation index & Classification & CPM2.6B-zs & PanGu2.6B-zs  & CPM2.6B-os & PanGu2.6B-os & CPM2.6B-fs & PanGu2.6B-fs \\

\hline
1 & Genreate & CMRC2018 & Em/F1 & Read Comprehension & 0.59/10.12 & \textbf{1.21/16.647} & 1.71/11.29 & \textbf{2.49/18.57} & 3.11/14.64 & \textbf{5.68/23.22} \\

\hline
2 & Genreate & DRCD & Em/F1 & Read Comprehension & 0/4.62 & \textbf{0.8/9.99} & 0.22/5.17 & \textbf{2.47/12.48} & 0.15/7.14 & \textbf{5.31/18.29} \\

\hline
3 & Genreate & DuReader & Rouge-1 & Read Comprehension & 16.63 & \textbf{21.07} & 16.42 &	\textbf{20.18} & 17.85 &	\textbf{21.43} \\

\hline
4 & Genreate & WebQA.v1.0 &	Em/f1	& Closed-Book QA	& 6/12.59	& \textbf{6/16.32}	& 6/11.82	& \textbf{12/23.39} &	4/12.23 &	\textbf{24/33.94} \\

\hline
5 & Genreate & PD-CFT &	Acc &	Cloze(without choices)	& 35.73/38.99 &	\textbf{38.47/42.39} &	33.3/39.73 &	\textbf{38.8/41.61} &	32.03/39.84 &	\textbf{39.07/42.05} \\

\hline
6 & Genreate & CMRC2017	&	Acc	& Cloze(without choices)	& 24.60 	& \textbf{37.83} &	25.40 &	\textbf{38.00} &	23.50 &	\textbf{36.33}  \\

\hline
7 & PPL & CHID	&	Acc	& Cloze(multi-choices)	& 68.62 	& \textbf{68.73} &	67.91 &	\textbf{68.16} &	\textbf{66.82} &	66.56  \\
\hline
8 & PPL & CMRC2019 & Acc	& Cloze (multi-choices) & 47.69 & \textbf{61.93} & 47.99 & \textbf{61.54} & 47.20 & \textbf{62.42} \\
\hline
9 &	PPL &	CMNLI &	Acc &	Natural Language Inference &	49.10 & 	\textbf{50.20} & 	47.56 & 	\textbf{49.54} & 	49.29 & 	\textbf{51.17} \\
\hline
10 &	PPL &	OCNLI &	Acc &	Natural Language Inference &	\textbf{44.20} & 	42.61 & 	\textbf{44.30} & 	44.00 & 	44.00 & 	\textbf{46.78} \\
\hline
11 &	PPL &	TNEWS &	Acc &	Text classification &	\textbf{65.44} & 	60.95 & 	\textbf{69.50} & 	57.95 & 	\textbf{70.17} & 	63.62 \\
\hline
12 &	PPL &	IFLYTEK &	Acc &	Text classification &	68.91 & 	\textbf{74.26} & 	\textbf{79.84} & 	79.03 & 	\textbf{83.99} & 	80.15 \\
\hline
13 &	PPL &	AFQMC &	Acc &	Sentence Pair Similarity &	\textbf{66.34} & 	59.29 & 	39.70 & 	\textbf{64.62} & 	38.29 & 	\textbf{69.00} \\
\hline
14 &	PPL &	CSL &	Acc &	Keyword  Recognition &	\textbf{52.30} & 	50.50 & 	\textbf{51.20} & 	50.90 & 	50.50 & 	\textbf{52.00} \\
\hline
15 &	PPL &	CLUEWSC2020 &	Acc &	Wsc &	\textbf{73.684} & 	73.36 & 	73.684 & 	\textbf{75.33} & 	70.065 & 	\textbf{72.70} \\
\hline
16 &	PPL &	C3 &	Acc &	Multi-Choice RC &	49.81 & 	\textbf{53.42} & 	51.43 & 	\textbf{52.82} & 	51.60 & 	\textbf{53.64} \\
\hline


\end{tabular}}
 \begin{tablenotes}
        \footnotesize
        \flushleft{-zs is zero-shot; -os is one-shot; -fs is few-shot.}
      \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{table}
\centering
\renewcommand\arraystretch{1.25}
\caption{CPM-2.6B v.s. PanGu-13B}
\label{tab:VS13B}
\setlength{\tabcolsep}{1.5mm}
\begin{threeparttable}
\resizebox{\textwidth}{33mm}{\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c} 
\hline

IDX & Method & Task & Evaluation index & Classification & CPM2.6B-zs & PanGu13B-zs  & CPM2.6B-os & PanGu13B-os & CPM2.6B-fs & PanGu13B-fs \\

\hline
1 & Genreate & CMRC2018 & Em/F1 & Read Comprehension & 1.21/16.65 & \textbf{1.46/19.28} & 2.49/18.57 & \textbf{3.76/21.46} & 5.68/23.22 & \textbf{9.76/29.23} \\
\hline
2 & Genreate & DRCD & Em/F1 & Read Comprehension & \textbf{0.8}/9.99	 & 0.66/\textbf{10.55} & 2.47/12.48 & \textbf{4.22/15.01} & 5.31/18.29 & \textbf{9.09/23.46}\\
\hline
3 & Genreate & DuReader & Rouge-1 & Read Comprehension & 21.07  & \textbf{24.46}  & 20.18  & \textbf{25.99}  & 21.43  & \textbf{27.67}  \\
\hline
4 & Genreate & WebQA.v1.0 &	Em/f1	& Closed-Book QA	& 4.43/13.71 & \textbf{5.13/14.47} & 10.22/20.56 & \textbf{13.43/24.52} & 23.71/33.81 & \textbf{31.18/41.21} \\
\hline
5 & Genreate & PD-CFT &	Acc &	Cloze(without choices)	& 38.47/42.39  & \textbf{43.86/46.60}  & 38.8/41.61  & \textbf{40.97/45.42}  & 39.07/42.05  & \textbf{41.13/45.86}  \\
\hline
6 & Genreate & CMRC2017	&	Acc	& Cloze(without choices)	& 37.83  & \textbf{38.90}  & 38.00  & \textbf{38.40}  & 36.33  & \textbf{37.86}   \\
\hline
7 & PPL & CHID	&	Acc	& Cloze(multi-choices)	& 68.73  & \textbf{70.64}  & 68.16  & \textbf{70.05}  & 66.56  & \textbf{70.91}   \\
\hline
8 & PPL & CMRC2019 & Acc	& Cloze (multi-choices) & 68.22  & \textbf{70.54}  & 68.05  & \textbf{70.02}  & 66.26  & \textbf{71.28}  \\
\hline
9 &	PPL &	CMNLI &	Acc &	Natural Language Inference &	\textbf{50.20}  & 48.44  & \textbf{49.54}  & 46.81  & \textbf{51.17}  & 46.18  \\
\hline
10 &	PPL &	OCNLI &	Acc &	Natural Language Inference &	\textbf{42.61}  & 41.53  & 44.00  & \textbf{44.10}  & \textbf{46.78}  & 46.44  \\
\hline
11 &	PPL &	TNEWS &	Acc &	Text classification &	 \textbf{60.95}  & 60.26  & 57.95  & \textbf{63.83}  & 63.62  & \textbf{65.17}  \\
\hline
12 &	PPL &	IFLYTEK &	Acc &	Text classification &	\textbf{74.26}  & 73.80  & \textbf{79.03}  & 78.95  & 80.15  & \textbf{80.34}  \\
\hline
13 &	PPL &	AFQMC &	Acc &	Sentence Pair Similarity &	59.29  & \textbf{65.76}  & \textbf{64.62}  & 63.55  & \textbf{69.00}  & 68.91  \\
\hline
14 &	PPL &	CSL &	Acc &	Keyword  Recognition &	\textbf{50.50}  & 49.30  & \textbf{50.90}  & 50.20  & 52.00  & \textbf{55.70}  \\
\hline
15 &	PPL &	CLUEWSC2020 &	Acc &	Wsc &	73.36  & \textbf{75.00}  & \textbf{75.33}  & 75.00  & 72.70  & \textbf{78.62} \\
\hline
16 &	PPL &	C3 &	Acc &	Multi-Choice RC &	53.42  & \textbf{54.47}  & 52.82  & \textbf{53.92}  & 53.64  & \textbf{54.58} \\
\hline

\end{tabular}}
 \begin{tablenotes}
        \footnotesize
        \flushleft{-zs is zero-shot; -os is one-shot; -fs is few-shot.}
      \end{tablenotes}
    \end{threeparttable}

\end{table}


Table \ref{tab:VS13B} compares PanGu-13B with PanGu-2.6B. PanGu-13B outperforms PanGu-2.6B on all generate-tasks and most of the ppl-tasks. Regarding CMRC2018, DRCD and WebQA tasks of PanGu-13B, the few-shot surpasses zero-shot by more than 10 points in scores, demonstrating that PanGu-13B has superior few-shot learning ability. Regarding the NLI and text classification tasks, PanGu-13B is comparable with PanGu-2.6B in performance. These tasks are generally hard for casual language models~\cite{brown2020GPT3}, and there are great chances for model improvement, which would be our future work.


\end{document}
\fi