\documentclass{article}

\usepackage{xcolor}
\usepackage{color}
\usepackage{arxiv}
\usepackage{CJKutf8}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{footnote}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{threeparttable}
\usepackage{CJK}
\usepackage{longtable}

%\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage[linesnumbered,ruled,vlined,commentsnumbered]{algorithm2e}
\makesavenoteenv{tabular}
\makesavenoteenv{table}

%\usepackage{authblk}

\usepackage{subfiles} % Best loaded last in the preamble
\usepackage{makecell}

\usepackage{array}

\newcommand{\MODEL}{PanGu-$\Sigma$}

\usepackage[normalem]{ulem}
\newcommand{\jx}[1]{\textcolor{red}{#1}}
\newcommand{\ly}[1]{\textcolor{blue}{#1}}


\title{\MODEL: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing}


\author{Xiaozhe Ren$^{1}\thanks{Equal Contribution}$\\ \and \textbf{Pingyi Zhou$^{1*}$}\\ \and \textbf{Xinfan Meng$^{1*}$}\\ \and \textbf{Xinjing Huang$^{2*}$} \and \textbf{Yadao Wang$^{1*}$} \and \textbf{Weichao Wang$^1$} \and \textbf{Pengfei Li$^1$} \and \textbf{Xiaoda Zhang$^2$} \\ \and \textbf{Alexander Podolskiy$^1$} \and \textbf{Grigory Arshinov $^1$} \and \textbf{Andrey Bout $^1$} \and \textbf{Irina Piontkovskaya $^1$} \and \textbf{Jiansheng Wei$^1$} \and \textbf{Xin Jiang$^1$} \\ \and \textbf{Teng Su$^2$~~~~~~~~Qun Liu$^1$~~~~~~~~Jun Yao$^1$} \\ \\
\textbf{$^1$Noah's Ark Lab, Huawei Technologies} \\
\textbf{$^2$Distributed and Parallel Software Lab, Huawei Technologies}
%\textbf{$^3$Central Software Institute, Huawei Technologies}\\
%\textbf{$^4$ Peng Cheng Laboratory}
%\textbf{$^5$Department of Computer Science, National University of Defense Technology}
}


%\textsc{\large \MODEL\ Team}\\


\begin{document}
\date{}
\maketitle

\begin{abstract}
The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors~\footnote{\url{https://e.huawei.com/en/products/servers/ascend}} and MindSpore framework~\footnote{\url{https://www.mindspore.cn/en}}, and present the language model with 1.085T parameters named \MODEL. With parameter inherent from PanGu-$\alpha$~\cite{Zeng2021PanGuLA}, we extend the dense Transformer model to sparse one with \textit{Random Routed Experts} (RRE), and efficiently train the model over 329B tokens by 
using \textit{Expert Computation and Storage Separation} (ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that \MODEL\ provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.

%Besides, \MODEL\ can generate samples of texts which is difficult for human evaluators to distinguish from real texts written by humans.
% (e.g., Cloze tasks, reading comprehension, closed-book QA, Winograd style tasks, commonsense reasoning, natural language inference, text classification)
\end{abstract}


% keywords can be removed
\keywords{Large Language Models \and Distributed Training \and Natural Language Processing}


\section{Introduction}\label{sec:introduction}

\subfile{sections/introduction}

\section{Model}\label{sec:model}

\subfile{sections/model}

\section{Dataset}\label{sec:dataset}

\subfile{sections/dataset}

\section{System}\label{sec:system}

\subfile{sections/system}

\section{Experiments}\label{sec:experiments}

\subfile{sections/experiments2}

\section{Conclusion and Future Work}\label{sec:conclusion}

\subfile{sections/future}

\section{Acknowledgements}
We would like to thank Bin Zhou, Zhiwei Wang, Yasheng Wang, Liangyou Li, Bin He and Fanyi Du for their great support for this work; Chen Li, Yifan Yao, Kaisheng Wang, Zhenzhang Yang, Zhongzhe Hu,  Zhepeng Sun, Zhijian Guo, Jun Wang, and Ziqiang Chen for their help to handle infrastructure issues.  

\clearpage

\bibliographystyle{unsrt}  
\bibliography{references}  %%% Remove comment to use the external .bib file 

\appendix

\subfile{appendices/multiple_choice}

\end{document}
