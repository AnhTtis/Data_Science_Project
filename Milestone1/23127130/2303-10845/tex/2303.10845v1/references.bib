@inproceedings{DBLP:conf/nlpcc/WangKZHJZH20,
  author    = {Yida Wang and
               Pei Ke and
               Yinhe Zheng and
               Kaili Huang and
               Yong Jiang and
               Xiaoyan Zhu and
               Minlie Huang},
  title     = {A Large-Scale Chinese Short-Text Conversation Dataset},
  booktitle = {Natural Language Processing and Chinese Computing - 9th {CCF} International
               Conference, {NLPCC} 2020, Zhengzhou, China, October 14-18, 2020, Proceedings,
               Part {I}},
  volume    = {12430},
  pages     = {91--103},
  publisher = {Springer},
  year      = {2020}
}
@article{DBLP:journals/corr/abs-2108-01547,
  author    = {Hao Zhou and
               Pei Ke and
               Zheng Zhang and
               Yuxian Gu and
               Yinhe Zheng and
               Chujie Zheng and
               Yida Wang and
               Chen Henry Wu and
               Hao Sun and
               Xiaocong Yang and
               Bosi Wen and
               Xiaoyan Zhu and
               Minlie Huang and
               Jie Tang},
  title     = {{EVA:} An Open-Domain Chinese Dialogue System with Large-Scale Generative
               Pre-Training},
  journal   = {CoRR},
  volume    = {abs/2108.01547},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.01547},
  eprinttype = {arXiv},
  eprint    = {2108.01547}
}
@article{DBLP:journals/corr/abs-2203-09313,
  author    = {Yuxian Gu and
               Jiaxin Wen and
               Hao Sun and
               Yi Song and
               Pei Ke and
               Chujie Zheng and
               Zheng Zhang and
               Jianzhu Yao and
               Xiaoyan Zhu and
               Jie Tang and
               Minlie Huang},
  title     = {{EVA2.0:} Investigating Open-Domain Chinese Dialogue Systems with
               Large-Scale Pre-Training},
  journal   = {CoRR},
  volume    = {abs/2203.09313},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2203.09313},
  doi       = {10.48550/arXiv.2203.09313},
  eprinttype = {arXiv},
  eprint    = {2203.09313}
}

@article{wang2019superglue,
   title={Super{GLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
   author={Alex Wang and Yada Pruksachatkun and Nikita Nangia and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
   journal={arXiv preprint 1905.00537},
   year={2019}
}

@article{Yuan2021WuDaoCorporaAS,
  title={WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models},
  author={Sha Yuan and Hanyu Zhao and Zhengxiao Du and Ming Ding and Xiao Liu and Yukuo Cen and Xu Zou and Zhilin Yang and Jie Tang},
  journal={AI Open},
  year={2021},
  volume={2},
  pages={65-68}
}

@article{Xu2020CLUECorpus2020AL,
  title={CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model},
  author={Liang Xu and Xuanwei Zhang and Qianqian Dong},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.01355}
}

@article{Gao2021ThePA,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Leo Gao and Stella Rose Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
  journal={ArXiv},
  year={2021},
  volume={abs/2101.00027}
}


@article{Christopoulou2022PanGuCoderPS,
  title={PanGu-Coder: Program Synthesis with Function-Level Language Modeling},
  author={Fenia Christopoulou and Gerasimos Lampouras and Milan Gritta and Guchun Zhang and Yinpeng Guo and Zhong-Yi Li and Qi Zhang and Meng Xiao and Bo Shen and Lin Li and Hao Yu and Li-yu Yan and Pingyi Zhou and Xin Wang and Yu Ma and Ignacio Iacobacci and Yasheng Wang and Guangtai Liang and Jia Wei and Xin Jiang and Qianxiang Wang and Qun Liu},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.11280}
}

@article{Gousios2013TheGD,
  title={The GHTorent dataset and tool suite},
  author={Georgios Gousios},
  journal={2013 10th Working Conference on Mining Software Repositories (MSR)},
  year={2013},
  pages={233-236}
}

@article{Zeng2021PanGuLA,
  title={PanGu-$\alpha$: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation},
  author={Wei Zeng and Xiaozhe Ren and Teng Su and Hui Wang and Yi Liao and Zhiwei Wang and Xin Jiang and ZhenZhang Yang and Kaisheng Wang and Xiaoda Zhang and Chen Li and Ziyan Gong and Yifan Yao and Xinjing Huang and Jun Wang and Jia-xin Yu and Qiwei Guo and Yue Yu and Yan Zhang and Jin Wang and Heng Tao and Dasen Yan and Zexuan Yi and Fang Peng and Fan Jiang and Han Zhang and Lingfeng Deng and Yehong Zhang and Zhengping Lin and Chao Zhang and Shaojie Zhang and Mingyue Guo and Shanzhi Gu and Gaojun Fan and Yaowei Wang and Xuefeng Jin and Qun Liu and Yonghong Tian},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.12369}
}

@article{Zhang2022OPTOP,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.01068}
}

@article{Austin2021ProgramSW,
  title={Program Synthesis with Large Language Models},
  author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie J. Cai and Michael Terry and Quoc V. Le and Charles Sutton},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.07732}
}

@article{Hendrycks2021MeasuringCC,
  title={Measuring Coding Challenge Competence With APPS},
  author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Xiaodong Song and Jacob Steinhardt},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.09938}
}

@article{Li2022CompetitionLevelCG,
  title={Competition-Level Code Generation with AlphaCode},
  author={Yujia Li and David H. Choi and Junyoung Chung and Nate Kushman and Julian Schrittwieser and R{\'e}mi Leblond and Tom and Eccles and James Keeling and Felix Gimeno and Agustin Dal Lago and Thomas Hubert and Peter Choy and Cyprien de and Masson d’Autume and Igor Babuschkin and Xinyun Chen and Po-Sen Huang and Johannes Welbl and Sven Gowal and Alexey and Cherepanov and James Molloy and Daniel Jaymin Mankowitz and Esme Sutherland Robson and Pushmeet Kohli and Nando de and Freitas and Koray Kavukcuoglu and Oriol Vinyals},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.07814}
}

@article{Fried2022InCoderAG,
  title={InCoder: A Generative Model for Code Infilling and Synthesis},
  author={Daniel Fried and Armen Aghajanyan and Jessy Lin and Sida I. Wang and Eric Wallace and Freda Shi and Ruiqi Zhong and Wen-tau Yih and Luke Zettlemoyer and Mike Lewis},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.05999}
}

@article{Thoppilan2022LaMDALM,
  title={LaMDA: Language Models for Dialog Applications},
  author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam M. Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and Yaguang Li and Hongrae Lee and Huaixiu Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Yanqi Zhou and Chung-Ching Chang and I. A. Krivokon and Willard James Rusch and Marc Pickett and Kathleen S. Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Hartz S{\o}raker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark D{\'i}az and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravindran Rajakumar and Alena Butryna and Matthew Lamm and V. O. Kuzmina and Joseph Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.08239}
}

@inproceedings{cui-emnlp2019-cmrc2018,
    title = "A Span-Extraction Dataset for {C}hinese Machine Reading Comprehension",
    author = "Cui, Yiming  and
      Liu, Ting  and
      Che, Wanxiang  and
      Xiao, Li  and
      Chen, Zhipeng  and
      Ma, Wentao  and
      Wang, Shijin  and
      Hu, Guoping",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1600",
    doi = "10.18653/v1/D19-1600",
    pages = "5886--5891",
}

@InProceedings{cmrc2017-dataset,
  author = {Cui, Yiming and Liu, Ting and Chen, Zhipeng and Ma, Wentao and Wang, Shijin and Hu, Guoping},
  title = {Dataset for the First Evaluation on Chinese Machine Reading Comprehension},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year = {2018},
  month = {may},
  date = {7-12},
  pages = {2721--2725},
  location = {Miyazaki, Japan},
  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},
  publisher = {European Language Resources Association (ELRA)},
  address = {Paris, France},
  isbn = {979-10-95546-00-9},
  language = {english}
}
  
@inproceedings{cui-etal-2020-cmrc2019,
  title={A Sentence Cloze Dataset for Chinese Machine Reading Comprehension},
  author={Cui, Yiming and Liu, Ting and Yang, Ziqing and Chen, Zhipeng and Ma, Wentao and Che, Wanxiang and Wang, Shijin and Hu, Guoping},
  booktitle = 	"Proceedings of the 28th International Conference on Computational Linguistics (COLING 2020)",
  year={2020}
}

@InProceedings{cui-etal-2016-consensus,
  title		= {Consensus Attention-based Neural Networks for Chinese Reading Comprehension},
  author	= {Cui, Yiming and Liu, Ting and Chen, Zhipeng and Wang, Shijin and Hu, Guoping},
  booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
  year      = {2016},
  address   = {Osaka, Japan},
  pages     = {1777--1786},
}

@misc{https://doi.org/10.48550/arxiv.1806.00920,
  doi = {10.48550/ARXIV.1806.00920},
  url = {https://arxiv.org/abs/1806.00920},
  author = {Shao, Chih Chieh and Liu, Trois and Lai, Yuting and Tseng, Yiying and Tsai, Sam},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DRCD: a Chinese Machine Reading Comprehension Dataset},
  publisher = {arXiv},
  year = {2018},
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{he-etal-2018-dureader,
    title = "{D}u{R}eader: a {C}hinese Machine Reading Comprehension Dataset from Real-world Applications",
    author = "He, Wei  and
      Liu, Kai  and
      Liu, Jing  and
      Lyu, Yajuan  and
      Zhao, Shiqi  and
      Xiao, Xinyan  and
      Liu, Yuan  and
      Wang, Yizhong  and
      Wu, Hua  and
      She, Qiaoqiao  and
      Liu, Xuan  and
      Wu, Tian  and
      Wang, Haifeng",
    booktitle = "Proceedings of the Workshop on Machine Reading for Question Answering",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-2605",
    doi = "10.18653/v1/W18-2605",
    pages = "37--46",
    abstract = "This paper introduces DuReader, a new large-scale, open-domain Chinese machine reading comprehension (MRC) dataset, designed to address real-world MRC. DuReader has three advantages over previous MRC datasets: (1) data sources: questions and documents are based on Baidu Search and Baidu Zhidao; answers are manually generated. (2) question types: it provides rich annotations for more question types, especially yes-no and opinion questions, that leaves more opportunity for the research community. (3) scale: it contains 200K questions, 420K answers and 1M documents; it is the largest Chinese MRC dataset so far. Experiments show that human performance is well above current state-of-the-art baseline systems, leaving plenty of room for the community to make improvements. To help the community make these improvements, both DuReader and baseline systems have been posted online. We also organize a shared competition to encourage the exploration of more models. Since the release of the task, there are significant improvements over the baselines.",
}

@article{sun2019investigating,
  title={Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension},
  author={Sun, Kai and Yu, Dian and Yu, Dong and Cardie, Claire},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020},
  url={https://arxiv.org/abs/1904.09679v3}
}

@inproceedings{ocnli,
	title={OCNLI: Original Chinese Natural Language Inference},
	author={Hai Hu and Kyle Richardson and Liang Xu and Lu Li and Sandra Kuebler and Larry Moss},
	booktitle={Findings of EMNLP},
	year={2020},
	url={https://arxiv.org/abs/2010.05444}
}

@inproceedings{zheng-etal-2019-chid,
    title = "{C}h{ID}: A Large-scale {C}hinese {ID}iom Dataset for Cloze Test",
    author = "Zheng, Chujie  and
      Huang, Minlie  and
      Sun, Aixin",
    booktitle = "Proceedings of the 57th Conference of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1075",
    pages = "778--787",
}

@inproceedings{xu-etal-2020-clue,
    title = "{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark",
    author = "Xu, Liang  and
      Hu, Hai  and
      Zhang, Xuanwei  and
      Li, Lu  and
      Cao, Chenjie  and
      Li, Yudong  and
      Xu, Yechen  and
      Sun, Kai  and
      Yu, Dian  and
      Yu, Cong  and
      Tian, Yin  and
      Dong, Qianqian  and
      Liu, Weitang  and
      Shi, Bo  and
      Cui, Yiming  and
      Li, Junyi  and
      Zeng, Jun  and
      Wang, Rongzhao  and
      Xie, Weijian  and
      Li, Yanting  and
      Patterson, Yina  and
      Tian, Zuoyu  and
      Zhang, Yiwen  and
      Zhou, He  and
      Liu, Shaoweihua  and
      Zhao, Zhe  and
      Zhao, Qipeng  and
      Yue, Cong  and
      Zhang, Xinrui  and
      Yang, Zhengliang  and
      Richardson, Kyle  and
      Lan, Zhenzhong",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.419",
    doi = "10.18653/v1/2020.coling-main.419",
    pages = "4762--4772",
}

@article{ERNIE3TITAN,
  title={Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation},
  author={Wang, Shuohuan and Sun, Yu and Xiang, Yang and Wu, Zhihua and Ding, Siyu and Gong, Weibao and Feng, Shikun and Shang, Junyuan and Zhao, Yanbin and Pang, Chao and others},
  journal={arXiv preprint arXiv:2112.12731},
  year={2021}
}

@article{DBLP:journals/corr/abs-2203-17090,
  author    = {Fei Mi and
               Yitong Li and
               Yulong Zeng and
               Jingyan Zhou and
               Yasheng Wang and
               Chuanfei Xu and
               Lifeng Shang and
               Xin Jiang and
               Shiqi Zhao and
               Qun Liu},
  title     = {{PANGUBOT:} Efficient Generative Dialogue Pre-training from Pre-trained
               Language Model},
  journal   = {CoRR},
  volume    = {abs/2203.17090},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2203.17090},
  doi       = {10.48550/arXiv.2203.17090},
  eprinttype = {arXiv},
  eprint    = {2203.17090}
}

@inproceedings{DBLP:conf/aaai/WangLZY21,
  author    = {Xiaoyang Wang and
               Chen Li and
               Jianqiao Zhao and
               Dong Yu},
  title     = {NaturalConv: {A} Chinese Dialogue Dataset Towards Multi-turn Topic-driven
               Conversation},
  booktitle = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2021},
  pages     = {14006--14014},
  publisher = {{AAAI} Press},
  year      = {2021},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/17649}
}

@inproceedings{DBLP:conf/naacl/LiGBGD16,
  author    = {Jiwei Li and
               Michel Galley and
               Chris Brockett and
               Jianfeng Gao and
               Bill Dolan},
  title     = {A Diversity-Promoting Objective Function for Neural Conversation Models},
  booktitle = {{NAACL} {HLT} 2016, The 2016 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language Technologies,
               San Diego California, USA, June 12-17, 2016},
  pages     = {110--119},
  publisher = {The Association for Computational Linguistics},
  year      = {2016}
}

@inproceedings{DBLP:conf/acl/LewisDF18,
  author    = {Angela Fan and
               Mike Lewis and
               Yann N. Dauphin},
  editor    = {Iryna Gurevych and
               Yusuke Miyao},
  title     = {Hierarchical Neural Story Generation},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2018, Melbourne, Australia, July 15-20, 2018, Volume
               1: Long Papers},
  pages     = {889--898},
  publisher = {Association for Computational Linguistics},
  year      = {2018}
}

@inproceedings{DBLP:conf/iclr/HoltzmanBDFC20,
  author    = {Ari Holtzman and
               Jan Buys and
               Li Du and
               Maxwell Forbes and
               Yejin Choi},
  title     = {The Curious Case of Neural Text Degeneration},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020}
}


@inproceedings{brown2020GPT3,
 author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = "Proc. {NeurIPS}",
 pages = {1877--1901},
 title = {Language Models are Few-Shot Learners},
 year = {2020}
}


@article{2021Scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{2022PaLM,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}


@inproceedings{2021GLaM,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}

@inproceedings{devlin2019bert,
  title={{BERT}: {P}re-Training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle="Proc. {NAACL-HLT}",
  pages={4171--4186},
  year={2019}
}

@misc{wei2019nezha,
      title={NEZHA: Neural Contextualized Representation for Chinese Language Understanding}, 
      author={Junqiu Wei and Xiaozhe Ren and Xiaoguang Li and Wenyong Huang and Yi Liao and Yasheng Wang and Jiashu Lin and Xin Jiang and Xiao Chen and Qun Liu},
      year={2019},
      eprint={1909.00204},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{2020Exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{CPM2020,
 title = {{CPM}: A Large-scale Generative {C}hinese Pre-trained Language Model},
 author = {Zhengyan Zhang and Xu Han and Hao Zhou and Pei Ke and Yuxian Gu and Deming Ye and Yujia Qin and Yusheng Su and Haozhe Ji and Jian Guan and Fanchao Qi and Xiaozhi Wang and Yanan Zheng and Guoyang Zeng and Huanqi Cao and Shengqi Chen and Daixuan Li and Zhenbo Sun and Zhiyuan Liu and Minlie Huang and Wentao Han and Jie Tang and Juanzi Li and Xiaoyan Zhu and Maosong Sun},
 year = {2020},
  journal={arXiv preprint arXiv:2012.00413},
}


@article{2021On,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}


@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={J. Mach. Learn. Res},
  volume={23},
  pages={1--40},
  year={2021}
}

@misc{shazeer2017outrageously,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{ascend-davinci,
  title={{DaVinci}: {A} Scalable Architecture for Neural Network Computing},
  author={Liao, Heng and Tu, Jiajin and Xia, Jing and Zhou, Xiping},
  booktitle={IEEE Hot Chips 31 Symposium (HCS)},
  pages={1--44},
  year={2019},
}


@article{2020Scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}


@article{2022Training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@inproceedings{2021GShard,
  title={GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  author={ Lepikhin, D.  and  Lee, H. J.  and  Xu, Y.  and  Chen, D.  and  Firat, O.  and  Huang, Y.  and  Krikun, M.  and  Shazeer, N.  and  Chen, Z. },
  booktitle={International Conference on Learning Representations},
  year={2021},
}

@inproceedings{Gpipe,
	title = {{GPipe}: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
	author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, zhifeng},
	booktitle = "Proc. {NeurIPS}",
	year = {2019},
}

@inproceedings{zero,
author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
title = {ZeRO: Memory Optimizations toward Training Trillion Parameter Models},
year = {2020},
booktitle = {Proc. {SC}}
}

@article{Megatron,
  author = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  title = {{Megatron-LM}: {T}raining Multi-Billion Parameter Language Models using Model Parallelism},
  journal = {arXiv preprint arXiv:1909.08053},
  year = {2019}
}

@article{2021Hash,
  title={Hash layers for large sparse models},
  author={Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17555--17566},
  year={2021}
}


@article{2014Adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{micikevicius2018mixed,
  title={Mixed Precision Training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  booktitle="Proc. {ICLR}",
  year={2018}
}

@article{2020Neural,
  title={Neural Machine Translation with Byte-Level Subwords},
  author={ Wang, C.  and  Cho, K.  and  Gu, J. },
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={5},
  pages={9154-9160},
  year={2020},
}

@misc{2018AScareBLEU,
  title={A Call for Clarity in Reporting BLEU Scores},
  author={ Post, M. },
  year={2018},
}

@article{2020mT5,
  title={mT5: A massively multilingual pre-trained text-to-text transformer},
  author={ Xue, L.  and  Constant, N.  and  Roberts, A.  and  Kale, M.  and  Al-Rfou, R.  and  Siddhant, A.  and  Barua, A.  and  Raffel, C. },
  year={2020},
}

@article{2021CPM,
  title={CPM-2: Large-scale Cost-effective Pre-trained Language Models},
  author={ Zhang, Z.  and  Gu, Y.  and  Han, X.  and  Chen, S.  and  Xiao, C.  and  Sun, Z.  and  Yao, Y.  and  Qi, F.  and  Guan, J.  and  Ke, P. },
  year={2021},
}

@article{2021ERNIE,
  title={ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation},
  author={ Sun, Y.  and  Wang, S.  and  Feng, S.  and  Ding, S.  and  Wang, H. },
  year={2021},
}

@article{2022Universal,
  title={Universal Conditional Masked Language Pre-training for Neural Machine Translation},
  author={ Li, P.  and  Li, L.  and  Zhang, M.  and  Wu, M.  and  Liu, Q. },
  year={2022},
}

@article{2022TrainingChat,
  title={Training language models to follow instructions with human feedback},
  author={ Ouyang, L.  and  Wu, J.  and  Jiang, X.  and  Almeida, D.  and  Wainwright, C. L.  and  Mishkin, P.  and  Zhang, C.  and  Agarwal, S.  and  Slama, K.  and  Ray, A. },
  journal={arXiv e-prints},
  year={2022},
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}

@article{artetxe2021efficient,
  title={Efficient large scale language modeling with mixtures of experts},
  author={Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and others},
  journal={arXiv preprint arXiv:2112.10684},
  year={2021}
}

@misc{wudao2,
  title={Wu Dao 2.0 Super-scale Intelligent Model},
  url={https://wudaoai.cn/},
  year={2021}
}

@article{lin2021m6,
  title={M6-10t: A sharing-delinking paradigm for efficient multi-trillion parameter pretraining},
  author={Lin, Junyang and Yang, An and Bai, Jinze and Zhou, Chang and Jiang, Le and Jia, Xianyan and Wang, Ang and Zhang, Jie and Li, Yong and Lin, Wei and others},
  journal={arXiv preprint arXiv:2110.03888},
  year={2021}
}

@inproceedings{clark2022unified,
  title={Unified scaling laws for routed language models},
  author={Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others},
  booktitle={International Conference on Machine Learning},
  pages={4057--4086},
  year={2022},
  organization={PMLR}
}

@article{smith2022using,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@inproceedings{ElKishky2019AMC,
  title={A Massive Collection of Cross-Lingual Web-Document Pairs},
  author={Ahmed El-Kishky and Vishrav Chaudhary and Francisco Guzm{\'a}n and Philipp Koehn},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2019}
}
 
 @article{Schwenk2019CCMatrixMB,
  title={CCMatrix: Mining Billions of High-Quality Parallel Sentences on the Web},
  author={Holger Schwenk and Guillaume Wenzek and Sergey Edunov and Edouard Grave and Armand Joulin},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.04944}
}

@inproceedings{Zeng2020MedDialogLM,
  title={MedDialog: Large-scale Medical Dialogue Datasets},
  author={Guangtao Zeng and Wenmian Yang and Zeqian Ju and Yue Yang and Sicheng Wang and Ruisi Zhang and Meng Zhou and Jiaqi Zeng and Xiangyu Dong and Ruoyu Zhang and Hongchao Fang and Penghui Zhu and Shu Chen and Pengtao Xie},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2020}
}

@article{Xiao2018CAIL2018AL,
  title={CAIL2018: A Large-Scale Legal Dataset for Judgment Prediction},
  author={Chaojun Xiao and Haoxiang Zhong and Zhipeng Guo and Cunchao Tu and Zhiyuan Liu and Maosong Sun and Yansong Feng and Xianpei Han and Zhen Hu and Heng Wang and Jianfeng Xu},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.02478}
}

@article{Alayrac2022FlamingoAV,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.14198}
}


@article{gpt-4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  url = {https://cdn.openai.com/papers/gpt-4.pdf}
}

@inproceedings{2021Ascend,
  title={Ascend: a Scalable and Unified Architecture for Ubiquitous Deep Neural Network Computing : Industry Track Paper},
  author={ Liao, H.  and  Tu, J.  and  Xia, J.  and  Liu, H.  and  Hu, Y. },
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  year={2021},
}

@inproceedings{2021ZeRO,
  title={ZeRO-Offload: Democratizing Billion-Scale Model Training.},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={USENIX Annual Technical Conference},
  pages={551--564},
  year={2021}
}

@article{2016Training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}