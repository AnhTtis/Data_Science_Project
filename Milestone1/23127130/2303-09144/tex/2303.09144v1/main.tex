\documentclass[10pt]{article}
\usepackage{geometry}
\geometry{
    a4paper,
    total={171mm,248mm},
    left=19.5mm,
    top=20mm
}
\usepackage{graphicx} % Required for inserting images
\usepackage{xcolor}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{amssymb,amsfonts,amsmath}
\usepackage{units}

\usepackage{tikz}
\usepackage{cite}
\usepackage{pgfplots,pgfplotstable}
\pgfplotsset{compat=newest}
\newcommand*\thankagain[1][\value{footnote}]{\footnotemark[#1]}

\usepackage{hyperref}
\hypersetup{pdfauthor={Lea Bold, Hannes Eschmann, Mario Rosenfelder, Henrik Ebel, Karl Worthmann},pdftitle={On Koopman-based surrogate models for non-holonomic robots},colorlinks=false, hidelinks}

\title{On Koopman-based surrogate models for non-holonomic robots}
\makeatletter
\let\@fnsymbol\@arabic
\makeatother
\author{Lea Bold\thanks{Optimization-based Control Group, Institute of Mathematics, Technische Universit\"at Ilmenau, Germany,
{\tt\small [lea.bold, karl.worthmann]@tu-ilmenau.de}.\newline K.\ Worthmann gratefully acknowledges funding by the German Research Foundation (DFG, project-ID 507037103).} ,
Hannes Eschmann\thanks{Institute of Engineering and Computational Mechanics~(ITM), University of Stuttgart, Germany, 
{\tt\small [hannes.eschmann, mario.rosenfelder, henrik.ebel]@itm.uni-stuttgart.de}.\newline The ITM acknowledges the support by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC 2075 – 390740016, project PN4-4 “Learning from Data - Predictive Control in Adaptive Multi-Agent Scenarios” and project EB195/32-1, 433183605 “Research on Multibody Dynamics and Control for Collaborative Elastic Object Transportation by a Heterogeneous Swarm with Aerial and Land-Based Mobile Robots”.
} , 
Mario Rosenfelder\thankagain {} , 
Henrik Ebel\thankagain {} , 
Karl Worthmann\thankagain[1] 
}

\date{February 2023}


\begin{document}

\maketitle

\begin{abstract}
    Data-driven surrogate models of dynamical systems based on the extended dynamic mode decomposition are nowadays well-established and widespread in applications. Further, for non-holonomic systems exhibiting a multiplicative coupling between states and controls, the usage of bi-linear surrogate models has proven beneficial. However, an in-depth analysis of the approximation quality and its dependence on different hyperparameters based on both simulation and experimental data is still missing. We investigate a differential-drive mobile robot to close this gap and provide first guidelines on the systematic design of data-efficient surrogate models.
\end{abstract}


\section{Introduction}

Non-holonomic vehicles are of indispensible practical value in transportation and robotics. 
To automate their behavior, accurate models are key for tasks such as motion planning and model-based  
control. 
Often, in robotics, simple %nominal 
kinematic models based on first principles are employed because it can be arduous to  
take into account hardware imperfections and effects beyond kinematics, and because it fits typical cascade-type control approaches. 
An alternative are data-driven techniques, which 
need to strike a balance between data efficiency, model expressiveness, efficient and reliable numerical realizations, and, at best, should have a theoretical underpinning that may bring about beneficial theoretical properties such as quantifiable error bounds with finite data. 
With regard to these requirements, a very popular method  
is the extended Dynamic Mode Decomposition (eDMD), whose theoretical foundation is the Koopman framework. 
The Koopman operator lifts the nonlinear dynamics to linear but infinite-dimensional dynamics, which are then approximated using eDMD to generate a data-based surrogate model~\cite{BrunKutz22}. 
This approach has been recently generalized to the setting with inputs~\cite{ProcBrun18} to apply linear techniques for the controller design~\cite{BevaSosn21}. 
In this paper, we 
show, based on real-world data and hardware experiments with a non-holonomic (differential-drive) mobile robot, that and how eDMD in a Koopman framework can be used to learn a model more accurate than the nominal kinematic  
model.  
Moreover, we show how it is possible to improve data efficiency and model accuracy by incorporating physical a-priori knowledge. 

Even with  
an accurate model, controller design for non-holonomic systems remains challenging~\cite{Asto96} since, e.g., Brockett's condition is violated meaning that there does not exist a continuous time-invariant state-feedback law. 
For instance, as rigorously shown in~\cite{MullWort17, RoseEbel22}, techniques like model predictive control based on quadratic costs do not  
successfully solve the set-point stabilization problem. 
A remedy are more sophisticated schemes using structural insight, e.g., based on the homogeneous approximation and privileged coordinates, see~\cite{WortMehr15,WortMehr16,CoroGrun20,RoseEbel22}. 
This insight is key to understand whether a linear surrogate model as proposed in eDMDc suffices or a bilinear one is required~\cite{BrudFu21, FolkBurd21, OttoRowl21}. 

Extended DMD with control (eDMDc) has already been explored for robotic systems, e.g., for an inverted pendulum or a tail-actuated robotic fish~\cite{MamaCast21}, 
or within simulations for non-holonomic mobile robots~\cite{ShiKary21}. 
Even a first experimental validation of Koopman-based LQR control utilizing structural knowledge  
has been explored for a tail-actuated robotic fish~\cite{MamaCast19}. 
However, determining an optimal dimensionality of the Koopman-based surrogate model remains challenging~\cite{RenJian22}. 
A rare experimental work, in which eDMDc is applied to non-holonomic robots, can be found in~\cite{ShiKary21ACD}. 
Therein, eDMDc is used to identify a model based on simulated data using a dictionary consisting of Hermite polynomials, and the prediction of that model is also compared with the behavior of a hardware robot. 
However, the authors do not identify a model based on data from real-world hardware and, hence, only the nominal dynamics is replicated. 
Moreover, a bi-linear surrogate model seems to be advantageous as shown in~\cite{BrudFu21,FolkBurd21} on a simulated robot arm and a planar quadrotor, respectively --~a claim, which is further supported in~\cite{OttoRowl21,NuskPeit23} for control-affine systems exhibiting a state-control coupling 
since lifted linear models of finite dimension cannot capture nonlinear actuation effects inherent in many robotic systems~\cite{FolkBurd21}. 

The contribution of this manuscript is the experimental investigation of the Koopman-based, bi-linear surrogate model in simulation \textit{and} experiment, which, to the knowledge of the authors, is novel in itself and in the depth of the conducted analysis. 
In that regard, we consider the so-called one-step error to analyze and compare the prediction accuracy for various reference trajectories in dependence of the key hyperparameters like the composition of the dictionary, the amount of data points, and the control basis employed for the bilinear approach. 
In particular, we outperform nominal models using surrogate models generated from random real-world data. 

Section~\ref{sec:eDMD} recaps eDMD in the Koopman framework before the problem setup is given in Section~\ref{sec:robot}. Then, simulation and experimental results are presented in Sections~\ref{sec:simulation} and~\ref{sec:exp}, respectively, before the results are discussed and conclusions are drawn.

\bigskip\noindent\textbf{Notation}: For integers~$n,m \in \mathbb{Z}$ with~$n \leq m$, we define~$[n:m] \coloneqq \mathbb{Z} \cap [n,m]$.




\section{Recap: eDMD in the Koopman framework}\label{sec:eDMD}

We consider the 
nonlinear dynamical system governed by~$\dot{x}(t) = f(x(t))$ with a locally-Lipschitz continuous vector field~$f: \mathbb{R}^{n_x} \rightarrow \mathbb{R}^{n_x}$.
Then, for observables~$\varphi \in L^2(\mathbb{R}^{n_x},\mathbb{R})$, the Koopman operator is defined by the identify
\begin{equation}\label{eq:Koopman_operator}
    (\mathcal{K}^t \varphi)(x^0) = \varphi(x(t;x^0)) \qquad\forall\,(t,x^0) \in \mathbb{R}_{\geq 0} \times \mathbb{R}^{n_x},
\end{equation}
i.e., instead of evaluating the observable~$\varphi$ at the flow~$x(t;x^0)$ emanating from the initial condition~$x(0;x^0) = x^0$ at time~$t$, the Koopman operator propagates the observable forward in time~$\mathcal{K}^t \varphi$ and, then, evaluates the propagated observable at the initial value~$x^0 \in \mathbb{R}^{n_x}$. 
Alternatively, one may also work with the  
generator~$\mathcal{L}$ of the Koopman semigroup~$(\mathcal{K}^t)_{t \in \mathbb{R}_{\geq 0}}$, which satisfies the abstract Cauchy problem~$\dot{z}(t) = \mathcal{L}z(t)$,~$z(0) = \varphi$, see, e.g.,~\cite{SchaWort22}.
For details on DMD~\cite{Tu13} and its variants, we refer to~\cite{Schmi22} and the references therein. The connection to the Koopman framework is treated in~\cite{BrunKutz22}. Here, we restrict ourselves to a compact set~$\mathbb{X} \subset \mathbb{R}^{n_x}$, see~\cite{SchaWort22} for a detailed discussion. 

For the dictionary~$\mathbb{V} \coloneqq \operatorname{span} \{ ( \psi_j )_{j=1}^N \}$ with~$\psi_j: \mathbb{X} \to \mathbb{R}$, the data-based surrogate model of the Koopman generator using the i.i.d.\ data points~$x^{[1]}, .., x^{[d]} \in \mathbb{X}$ is given by 
\begin{align*}
    \tilde{\mathcal{L}}_{d} = \tilde{C}^{-1} \tilde{A} %
    \quad\text{ with }\quad \tilde{C} = \tfrac{1}{d} \Psi_X \Psi_X^\top\text{ and }\tilde{A} = \tfrac{1}{d} \Psi_X \Psi_Y^\top,
\end{align*}
where the matrices~$\Psi_X, \Psi_Y \in \mathbb{R}^{N \times d}$ are defined by
\begin{align*}
    \Psi_X & \coloneqq \left[ \left. \left[\begin{smallmatrix}
            \psi_1(x^{[1]}) \\ 
            : \\ 
            \psi_N(x^{[1]})
        \end{smallmatrix}\right]\right| \ldots \left| \left[\begin{smallmatrix}
            \psi_1(x^{[d]})\\
            : \\
            \psi_N(x^{[d]})
        \end{smallmatrix}\right]\right. \right], \\
        \Psi_Y &\coloneqq \left[ \left. \left[\begin{smallmatrix}
            (\mathcal{L}\psi_1)(x^{[1]})\\
            : \\
            (\mathcal{L}\psi_N)(x^{[1]})
        \end{smallmatrix}\right]\right| \ldots \left| \left[\begin{smallmatrix}
            (\mathcal{L}\psi_1)(x^{[d]})\\
            : \\
            (\mathcal{L}\psi_N)(x^{[d]})
        \end{smallmatrix}\right]\right. \right].
    \end{align*}
Note that~$(\mathcal{L}\psi_j)(x^{[i]}) = f(x^{[i]}) \cdot \nabla \psi_j(x^{[i]})$ holds for all~$(i,j) \in [1:d] \times [1:N]$. 
Since one cannot expect invariance of~$\mathbb{V}$ w.r.t.\ the approximated Koopman operator, one projects the outcome to the coordinate functions, which are tacitly assumed to be contained in the dictionary, e.g.,~$\psi_i(x) = x_i$ for all~$i \in [1:n_x]$. In the operator setting, a time shift~$\delta > 0$ is fixed and the data matrix~$\Psi_Y$ contains the entries~$\psi_j(x(\delta;x_i))$ instead of~$(\mathcal{L}\psi_j)(x_i)$. 




For control-affine systems $\dot{x}(t) = f(x(t)) + \sum_{i=1}^{n_u} g_i(x(t)) u_i(t)$, there are two different options to deduce eDMD-based surrogate models. In~\cite{KordMezi18}, a linear surrogate model~$\dot{\psi} = \mathcal{L}\psi + \mathcal{B}u(t)$ (eDMDc) is proposed. To this end, the state is augmented by the control, i.e.,~$\tilde x = [x^\top\ u^\top]^\top$. 
An alternative are bi-linear surrogate models that explicitly leverage the control-affine structure, i.e., the identity~$\mathcal{L}^{u(t)} = \mathcal{L}^{0} + \sum_{i=1}^{n_u} u_i(t) (\mathcal{L}^{e_i} - \mathcal{L}^0)$, where~$\mathcal{L}^{e_i}$ is the generator for the autonomous dynamics with~$u\equiv e_i$. This yields~$\dot{\psi} = \mathcal{L}^{u(t)}\psi$, 
see, e.g.,~\cite{PeitOtto20} and the references therein. This approach seems to be preferable. 
On the one hand, it alleviates the curse of dimensionality resulting from the state augmentation in eDMDc. 
On the other hand, bilinear models seem to be superior if state-control couplings are present, i.e., one of the vector fields~$g_i$ depends on the state~$x$, see~\cite{BrudFu21,FolkBurd21,OttoRowl21,NuskPeit23}. 
For further details on the Koopman theory for control systems, see, e.g.,~\cite{BevaSosn21} and the references therein.



The approximation error can be split up into its two sources of error, i.e., the estimation~\cite{NuskPeit23} and the projection error~\cite{SchaWort22}. 
While the latter results from only finitely many observables in the dictionary~$\mathbb{V}$ and, thus, approximating the Koopman generator/operator on the respective finite-dimensional subspace, the former is a consequence of using only finitely many data points~$x^{[i]}$,~$i \in [1:d]$. 
While the convergence in the infinite-data limit also holds for eDMDc~\cite{KordMezi18convergence}, finite-data error bounds are presently only available for the bilinear approach, see~\cite{NuskPeit23,SchaWort22}.




\section{Problem Setup}\label{sec:robot}

The nominal kinematics of the 
differential-drive robot is given in terms of the driftless control-affine system
\begin{align}\label{eq:nominal_kinematics}
    \dot{x} (t) = \begin{bmatrix} \cos \theta(t) \\ \sin \theta(t) \\ 0 \end{bmatrix} v(t) + \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} \omega (t), % \eqqcolon \bm{G}(\bm{x}(t)) \, \bm{u}(t),
\end{align}
$x(0)=x^0$. The state~$x = [ x_1\ x_2\ \theta ]^\top \in \mathbb{X} \subset \mathbb{R}^3$ consists of its position~$[ x_1\ x_2]^\top$ in the plane and its orientation~$\theta$ measured relative to the~$x_1$-axis. 
Nominally, it is assumed that the robot can instantaneously attain any admissible translational velocity~$v$ in forward direction and angular yaw velocity~$\omega$, so that these act as the system's control input~$u= [ v\ \omega ]^\top\in\mathbb{U}\subset\mathbb{R}^2$, where~$\mathbb{U}$ is compact, convex, and~$0\in\text{int}(\mathbb{U})$.
In general, the nominal kinematics does not perfectly describe the dynamics of the physical robot since inertia effects, motor dynamics, and manufacturing imperfections are not accounted for. 
From a mechanical point of view, the dynamics~\eqref{eq:nominal_kinematics} 
describe the kinematics of a differential-drive mobile robot in the plane under the common assumption that the wheels roll without slipping with the wheel-floor contact point sticking perfectly to the ground, preventing instantaneous lateral motions of the robot and thereby giving rise to a non-holonomic kinematic constraint. 
A physical robot with such a kinematic setup is employed throughout this contribution.  
On the nominal kinematic level, the robot's configuration is completely described by means of its pose, hence it is sufficient to formulate the observables based on~$x$.
Thus, in general, the learning procedure from Section~\ref{sec:eDMD} receives as data recorded pairs of states and corresponding successor states, but not any prior information on the dynamics of the robot.
However, in Sec.~\ref{sec:simulation}, we show how some mechanical prior knowledge can be incorporated, e.g., when choosing the observables of the dictionary~$\mathbb{V}$.

\section{Simulation results}\label{sec:simulation}
In this section, eDMD is applied to the simulated, nominal non-holonomic robot. 
First, we generate i.i.d.\ random data matrices~$X_i \in \mathbb{R}^{n_x \times d}$,~$i \in \lbrace 0,\dots,n_u\rbrace$, with~$n_x = 3$ and~$d=10000$ data points each, where each column is in the set~$\mathbb{X}$ and serves as an initial condition for the dynamical system~\eqref{eq:nominal_kinematics}. 
Each data point contained in~$X_i$ is simulated forward 
$\delta = 0.02\,\textnormal{s}$ with the Runge-Kutta method of fourth order  
using a specific constant control input~$u_i$. 
For~$i=0$, the latter is chosen to~$u_0 = 0$. 
For~$i>0$, it is selected to be the~$i$th vector of a basis~$B$ of~$\mathbb{R}^{n_u}$. 
Here, with~$n_u=2$ and the basis~$B=\lbrace u_1, u_2\rbrace$, this yields the matrices~$Y_0$,~$Y_1$, and~$Y_2$ containing in each column the successor states of the states in~$X_0$,~$X_1$, and $X_2$ for the inputs~$u_0$,~$u_1$, and~$u_2$, respectively. 
Nominally, the system is free of drift, i.e.,~$Y_0 = X_0$. 
In simulations, different from experiments, it is possible to choose~$X_0 = X_1 = X_2$,  
which is done in this section. 
In the following, the dictionary~$\mathbb{V}$ is spanned by the monomials of~$x_1$,~$x_2$, and~$\theta$ of degree less or equal than 
7, which yields 
$N=120$ 
observables in total, yielding the set of observables~$\mathbb{O}_{120}$. 
By lifting the matrices~$X_i, Y_i$,~$i \in [0:n_u]$, with those observables, the matrices~$\Psi_{X_i}, \Psi_{Y_i}$ are computed, see Section~\ref{sec:eDMD}.  
Now, an approximation of the Koopman operator for step size~$\delta$  
is computed by 
$K_i^{\delta} = ( %\frac{1}{d}
     (\Psi_{X_i} \Psi_{X_i}^\top )^{-1} %\frac{1}{d} 
     \Psi_{X_i} \Psi_{Y_i}^\top )^\top$,~$i \in [0:n_u]$. %\lbrace 0, 1, 2 \rbrace,
Using the bilinear  
approach, we approximate the Koopman operator  
for a control value~$u \in \mathbb{U}\subset\mathbb{R}^{n_u}$ by %\begin{align*} 
$K_u^{\delta} = K_0^{\delta} + \sum_{i = 1}^{n_u} g_i \cdot \left(K_i^{\delta} - K_0^{\delta}\right)$
for factors~$g_i$,~$i \in[ 1 : n_u ]$, which, here, solve the linear system~$g_1 u_1 + g_2 u_2 = u$.
There are two different ways to use the approximated Koopman operator to obtain the approximate values of the coordinates at a time step~$k>0$. 
In the first surrogate model variant proposed in~\cite{PeitOtto20}, subsequently referred to as SUR$_1$, one projects after %in 
each  
time step, i.e.,~$x_j[k] = ( \mathcal{K}^{\delta}_{u[k-1]} \Psi ( x[k-1] ) )_{j}$ for~$j \in [1:n_x]$, 
with the number inside square brackets denoting the time step, where one step is of duration~$\delta$. 
Between time steps, the new values of the observables are calculated based on the new coordinate values. 
In the second variant, called SUR$_2$ in the following, one projects once at the end, i.e.,
$x_j[k] =  (( \prod_{i=0}^{k-1} \mathcal{K}^{\delta}_{u[i]} ) \Psi(x^0) )_{\! j}$. 
To analyze their influence, Fig.~\ref{fig: modelbased circle} shows prediction results for the two variants and, as a reference, the result of the time integration of the nominal kinematic model using the Runge-Kutta method of fourth order. 
\def\lineWidthSUR{1.5}%
\def\lineWidthSURError{1.25}%
\definecolor{ODE}{RGB}{0,0,255}%
\definecolor{SUR1}{RGB}{255,127,14}%
\definecolor{SUR2}{RGB}{0,128,0}%
\begin{figure}
    \centering%
    \input{figures/tikz/Koopman_modelbased_circle}
    \caption{Results from two Koopman-based surrogate models based on first-principles data, with the trajectory emanating from~$x^0 = [0.2\ 0\ -\pi/2 ]^\top$ on the left, and the norm of the prediction error on the right.}
    \label{fig: modelbased circle}%
\end{figure}%
In the depicted scenario, the control input is set to the constant value~$u \equiv \begin{bmatrix} 0.2 & 0.2 \end{bmatrix}^\top$, i.e., the robot will move in a circle and the basis is~$B = \{ e_1, e_2\}$ for the unit vectors~$e_1, e_2 \in \mathbb{R}^{n_u}$. 
As can be seen, SUR$_1$ leads to a trajectory whose error remains comparatively small over the whole trajectory. For the model SUR$_2$, however, we receive a trajectory that visibly deviates from the reference after about one quater of simulated time, which can also be seen in the error plot. 
In the second half of the simulation, the prediction based on SUR$_2$ becomes increasingly inaccurate and quickly unusable.  
Consequently, from now on, we will only use SUR$_1$ for Koopman-based surrogate models.

The basis employed for~$\mathbb{R}^{n_u}$ need not consist of unit vectors. 
In the following, we use the bases~$B_1 = \{ [0.2\ 0]^\top, [0\ 2]^\top \}$ and~$B_2 = \{ [0.2\ -0.4]^\top, [0.2\ 0.6]^\top \}$ instead. 
Basis~$B_1$ contains scaled variants of the unit vectors that, in absolute value, fit better to the usual operating points of the employed hardware robot; for instance, it cannot attain translational velocities of~$1\,\textnormal{m}/\textnormal{s}$. 
Still, training with~$B_1$ only captures the robot driving a straight line or rotating on the spot. 
In contrast, to study the influence of the usage of different training motions for learning, the inputs contained in~$B_2$ let the robot drive arcs of different radii.  
In Fig.~\ref{fig: firstprinciple}, the results for those two bases are illustrated. 
\definecolor{ODEcolor}{RGB}{0,0,255}
\definecolor{SUR1B1color}{RGB}{230,97,1}  %230 97 1
\definecolor{SUR1B2color}{RGB}{44,160,44} %{26,150,65}
\definecolor{lightblue}{RGB}{135,206,235}
\def\lineWidthStep{1.0}
\def\lineWidthPlane{1.5}
\def\heightODEtop{5.4cm}
\def\heightODEbottom{5.4cm}
\def\yBottomPlots{-5.0cm}
\begin{figure}%[h!]
    \centering
    \input{figures/tikz/firstprinciple_comparebases1} 
    \caption{Results using SUR$_1$ and the basis~$B_1$~\eqref{plot:Sur1B1} or~$B_2$~\eqref{plot:Sur1B2}. From left to right, top to bottom, the resulting trajectories in the motion plane, the applied control values, the one-step prediction errors, and total error norms are shown.}
    \label{fig: firstprinciple}
\end{figure}
In the plotted scenario, the same random control sequence~$u$ is applied to the models. 
Once again, the prediction results of the surrogate models are compared to time integrations of the nominal model, which is used as a reference.  
In addition to the error norm, the one-step prediction error is considered. 
To calculate the latter, in each time step, starting from the same reference value, the following time step is predicted using the model of choice and the result is compared with the corresponding, subsequent value of the reference. 
As the results in Fig.~\ref{fig: firstprinciple} show, using random control values leads to a higher error than using the constant control input from Fig.~\ref{fig: modelbased circle}, motivating the subsequent analysis using test trajectories where a wider variety of inputs are applied. 
Moreover, here, the difference between the two bases is negligible. 
However, it is not a priori clear whether the latter also holds when using data from an imperfect hardware robot. 
Hence, real-world data is considered subsequently. 

\section{Experimental results}\label{sec:exp}
We use a custom-built mobile robot as depicted on the right of 
Fig.~\ref{fig:trainingData}. 
Its pose is tracked by an external tracking system consisting of five Optitrack Prime 13W cameras. 
The robot receives its inputs, the desired forward translational velocity and the desired angular yaw velocity, wirelessly. 
On-board software kinematically calculates the angular velocities of the wheels corresponding to the inputs under the assumption of rolling without slipping. 
Two independent PID controllers operating at a frequency of~$100\,\textnormal{Hz}$ control the motors so that the wheels quickly attain the desired angular velocities.  
Naturally, 
due to imperfections, the actual robot velocities may not match the sent ones. 
The time step is set to~$\delta = 0.1\,\textnormal{s}$ subsequently. 

\subsection{Data Generation}
Generating uniformly distributed training samples is possible by driving the robot to each corresponding point in the state space individually, applying one of the~$n_u$ inputs, and potentially driving back to that point to apply another input. 
However, this way of generating training data is notoriously time-inefficient. 
The more efficient procedure used in this paper works as follows. 
For the considered robot, holding any input for several time steps nominally results in a circular motion with the radius being determined by the quotient of the translational and angular velocities.  
The basis vectors of~$B_1$, which consist of driving in a straight line and turning on the spot, correspond to circles with infinite and vanishing radii, respectively. 
Therefore, slightly different sampling strategies for the two input bases~$B_1$ and~$B_2$ are used.
Starting from an initial position on the admissible motion plane~$\mathbb{P}=[0.0, 1.5]\,\textnormal{m}\times[-0.75, 0.75]\,\textnormal{m}$  with~$\mathbb{X}= \mathbb{P}\times \mathbb{R}$, a new point is drawn i.i.d.  
For~$B_1$, the robot turns using the corresponding input of the input basis until it faces this generated point. 
In order to collect as many data points as possible, the robot does at least one full rotation. 
Subsequently, the robot drives in a straight line toward this generated point. 
This way, the necessary input of~$B_1$ is held for several time steps, generating training samples along the way, making the procedure very time efficient. 
This procedure is repeated until a sufficient amount of training data is generated.
Due to the reasons stated above, for the input basis~$B_2$, the sampling strategy is adjusted slightly.
The robot, again, turns and drives towards the uniformly randomly generated point. 
Then, each time alternating between the two basis vectors of~$B_2$, the inputs are applied either until a full circle is driven or until the nominal state prediction of the robot leaves~$\mathbb{X}$. 
Generally, while time efficient and effective, this way of generating samples does not lead to a perfectly uniform distribution.
In Fig.~\ref{fig:trainingData}, some of the trajectories used during the data generation 
are depicted.\footnote{We do not immediately apply the full magnitude of the basis vectors for a more controlled behavior. 
Data points during acceleration and deceleration are not used since they do not correspond to any input basis vector. 
In case the random point is too close to the previous one to reach the desired speed, the point is discarded and a new one is generated so that enough data can be gathered in-between.}
\begin{figure}
    \centering
    %\footnotesize
    \input{trainingData.tex}
    \caption{From left to right, training trajectories used to generate the samples for the input bases~$B_1$ and~$B_2$, and a photograph of the employed type of custom-built robot are shown.}
    \label{fig:trainingData}
\end{figure}
Another practical consideration concerns the measurement of the robot's orientation. 
The optical tracking system steadily continues the angular measurements such that the orientation angle may lie outside of~$(-\pi, \pi]$. 
While it would be possible to use the raw data for training, instead, we leverage the periodicity of the orientation.%
\footnote{Each orientation in~$X_i$,~$i\in\{0,1,2\}$, is shifted to its equivalent value within~$(-\pi, \pi]$. 
The entries of~$Y_i$ are shifted by the same amount as the corresponding entries of~$X_i$. 
However, after that, some orientations in~$Y_i$ may still lie outside of~$(-\pi, \pi]$, namely if the angle left the interval between the sampling instants. 
The matrices with shifted entries are then used to compute the surrogate model. 
Before each evaluation of the model, 
the orientation is shifted to~$(-\pi, \pi]$. 
Subsequently, the output is then shifted back, resulting in the surrogate model being periodic (but not necessarily continuous) in the orientation.}

\subsection{Results}
Two main scenarios are considered.   
In the first scenario, the robot follows an~$\infty$-shaped trajectory. 
As the terminal and initial velocities are zero, at the start as well as the end of the trajectory, the speed is increased or decreased linearly to obtain a smoother motion. 
In the second scenario, the robot shall follow a square-shaped trajectory. To that end, the robot drives trapezoidal velocity profiles on each edge of the square with a top speed of about \unit[0.2]{m/s}. At each corner, the robot makes a quarter counter-clockwise turn with a maximum absolute angular velocity of \unit[1.0]{rad/s}, with the angular velocities being increased or decreased linearly. 

First, we look at Koopman-based models in which we do not incorporate further a-priori knowledge. 
The training data was generated as described above for the constant controls contained in the bases~$B_1$ or~$B_2$, for which~$4626$ or~$5182$ training data points were recorded, respectively.  
Because of the results from Section~\ref{sec:simulation}, only the Koopman-based surrogate model with projection in each step (SUR$_1$) is employed.   
%
Results for the~$\infty$-shaped trajectory can be seen in Fig.~\ref{fig: all observables}, where for the two bases~$B_1$ and~$B_2$ as well as for different observable sets, the resulting predicted trajectories are plotted on the left-hand side and the absolute errors are compared on the right-hand side. 
The errors are measured relative to a representative lap of the hardware robot. 
Due to imperfections, when supplied with inputs that should lead to a perfect~$\infty$-trajectory for the nominal kinematics, the real robot's trajectory is not of perfect shape. 
Three different surrogate models differing in their dictionaries are considered. 
Firstly, the set of observables~$\mathbb{O}_{120}$ from~Section~\ref{sec:simulation} is used. 
Secondly, in~$\mathbb{O}_{32}$, compared to~$\mathbb{O}_{120}$, we exclude monomials for which~$x_1$ and~$x_2$ have a degree larger than~$1$, yielding~$32$ observables in total. 
Finally, we further reduce the number of observables by omitting monomials where~$x_1$ or~$x_2$ appear multiplied with~$\theta$, leading to~$\mathbb{O}_{11}$ with~$11$ observables. 
This is motivated by the physical insight that the robot's dynamics is translation invariant,  
so it is interesting to see whether incorporating this knowledge improves model quality. 
In that regard, as can be seen in the upper part of Fig.~\ref{fig: all observables}, the predictions of the surrogate model using~$B_1$ with~$\mathbb{O}_{120}$ follow the reference rather well for some time but then completely deviate and even leave the experiment area. 
The paths for~$\mathbb{O}_{32}$ and~$\mathbb{O}_{11}$, however, are nearly indistinguishable and follow the reference well; only the error plot suggests that~$\mathbb{O}_{11}$ might perform a bit better. 

To study the influence of the input basis, the same scenario is plotted in the bottom part of Fig.~\ref{fig: all observables} for basis~$B_2$. 
\definecolor{dic120}{RGB}{31,119,180}%
\definecolor{dic32}{RGB}{255,127,14}%
\definecolor{dic11}{RGB}{44,160,44}%
\definecolor{reference}{RGB}{0,0,255}%
\def\linewidthEight{1.5}%
\def\linewidthError{1.0}%
\begin{figure}%[h!]
    \centering
    \input{figures/tikz/allobservables_compared_8} 
    \caption{Results using~$B_1$ (top) and~$B_2$ (bottom) based on real data, where trajectories for different sets of observables are compared with the result of an experiment run. Absolute errors are depicted on the right, independently for position (pos., norm) and orientation ($\theta$). }
    \label{fig: all observables}
\end{figure} %
Again, the trajectories for~$\mathbb{O}_{32}$ and~$\mathbb{O}_{11}$ are very close to each other, even in the error plot. % there is no clear difference. 
However, with~$B_2$, using the observables~$\mathbb{O}_{120}$ results in a trajectory that is close to the reference for much longer before the error becomes visible. 
Therefore, these results seem to suggest that using the basis~$B_2$ is a lot better if the set of observables~$\mathbb{O}_{120}$, which does not use any physical insight, is used and slightly better if~$\mathbb{O}_{32}$ and~$\mathbb{O}_{11}$ are employed, which partly or fully presume translational invariance. 
Due to these findings, we subsequently use the set of observables~$\mathbb{O}_{11}$ since it seems to yield the best predictions but, due to less elements, is also the most computationally efficient. %
In particular, as Fig.~\ref{fig: mixed} shows, the surrogate models using~$\mathbb{O}_{11}$ beat the predictions of the nominal model as well as (naturally) of the surrogate model from Section~\ref{sec:simulation}. 
\definecolor{KoopData}{RGB}{255,0,0}%
\definecolor{RungeKutta}{RGB}{0,128,0}%
%\definecolor{KoopFP}{RGB}{221,160,221}%
\definecolor{KoopFP}{RGB}{255,127,14}%
\definecolor{reference}{RGB}{0,0,255}%
\def\linewidthEightB{\linewidthEight}%
\def\linewidthError{1.0}%
\def\heightEight{4.2cm}%
\begin{figure}%[h!]
    \centering%
    %\tikzset{every picture/.style={scale=0.65}}%
    \input{figures/tikz/mixed_b1_full}%
    %\input{figures/tikz/mixed_b2_full}%
    \caption{Comparison of the surrogate models using~$B_1$ and~$B_2$ with models based on first principles and on data generated from a first principles model.}
    \label{fig: mixed}%
\end{figure}

From now on, due to the beneficial performance, if not stated otherwise, we employ the basis~$B_2$ with the observables~$\mathbb{O}_{11}$ and compare the prediction performance of the corresponding surrogate model with the nominal model and experiment runs.  
In particular, we include~$15$ experiment runs for each scenario since subsequent experiment realizations generally do not yield identical results due to disturbances, meaning that perfect prediction performance is impossible. 
Results for the~$\infty$-trajectory and for the square-shaped trajectory are plotted in Fig.~\ref{fig: data B2 ref8}. 
From the trajectory plots in the upper part, it becomes evident that the Koopman-based prediction outperforms the nominal model, better representing the systematic skewedness of the physical robot's trajectories. 
Similarly, in the lower part, the minimum, maximum and average Euclidean norms of the error  
between Koopman-based prediction and the family of hardware robot trajectories show that prediction quality is consistently good. 
    
\definecolor{green}{RGB}{0,128,0}
\definecolor{Koopman}{RGB}{255,0,0}
\definecolor{RungeKutta}{RGB}{0,128,0}
\definecolor{reference}{RGB}{0,0,255}
\def\linewidthEightC{\linewidthEight}
\def\linewidthErrorC{1.0}
\def\linewidthErrorStdVar{0.8}
\definecolor{gray}{RGB}{128,128,128}
\definecolor{orange}{RGB}{255,165,0}
\definecolor{MaxError}{RGB}{0,0,0255}
%\definecolor{AvgErr}{RGB}{255,165,0}
\definecolor{AvgErr}{RGB}{44,160,44}%
\def\opacityRef{0.2}
\def\heightRealData{4.6cm}
\def\heightRealDataError{5.33cm}
\begin{figure}%[h!]
    \centering%
    \input{figures/tikz/ref_b2_8}%
    \input{figures/tikz/mean_max_8}%  
    \caption{Comparison of the surrogate model using~$B_2$ with 15 experiment runs, where~$e_{\textnormal{max}}$,~$e_{\textnormal{avg}}$, and~$\sigma$ denote maximum, average, and standard deviation of the error norms, respectively.}  
    \label{fig: data B2 ref8}%
\end{figure}

A remaining concern is data efficiency. 
Hence, subsequently, training data points are removed systematically to obtain smaller training data sets. 
The original training data was generated by choosing, for~$B_1$,~$m_1 = 50$ and, for~$B_2$,~$m_1 = 39$ random initial conditions in~$\mathbb{X}$. 
The relevant trajectory pieces driven for each sampled point are all of different lengths, e.g., depending on the distance to the boundary of~$\mathbb{X}$. 
To systematically reduce the number of data points, first, these lengths are unified by taking the length of the shortest trajectory, which, here, consists of~$m_2 = 20$ steps, and discarding the data points beyond that for each trajectory segment. 
This leads to a new training data set of~$m_1 \cdot m_2 = 1000$ or~$780$ per basis. %, that means~$2000$ points overall \textcolor{blue}{(Stuttgart versteht diese Rechnung noch nicht :-O)}.} 
Then, every~$n$th data point,~$n \in \{1, 20, 50, 100\}$, is used to create training data sets of lower cardinality.  %while retaining something similar to independency in the data. 
%The resulting data sets have the sizes~$2000, 100, 40, 20$ and~$1520, 76, 32, 16$ for~$B_1$ and~$B_2$, respectively \textcolor{red}{(s.\ oben bzgl.\ Zahlen)}. 
Using~$\mathbb{O}_{11}$,  the different resulting surrogate models' average one-step prediction errors w.r.t.\ the~$15$~recorded trajectories in the~$\infty$-scenario are given in Fig.~\ref{fig: lessdata}. 
\definecolor{RungeKuttaError}{RGB}{0,128,0}%
\definecolor{n1}{RGB}{0,0,255}%
\definecolor{n20}{RGB}{255,165,0}%
\definecolor{n50}{RGB}{85,107,47}%
\definecolor{n100}{RGB}{221,160,221}%
\def\lineWidthErrorN{1.0}%
\def\heightErrorN{5.34cm}%
\begin{figure}[t!]
    \centering%
    \input{figures/tikz/lessdata_b1_new}%
    \caption{Average one-step prediction errors for surrogate models using~$B_1$ (left) and~$B_2$ (right) when only using every~$n$th data point.}%
    \label{fig: lessdata}%
\end{figure}
These show that basis~$B_2$ seems to be more data efficient since the errors remain lower in data-sparse settings.  
Moreover, comparatively small training data sets can suffice in this scenario to achieve one-step prediction errors that are consistently smaller than that of the nominal model, especially when using basis~$B_2$. 

\section{Summary and Outlook}
This contribution showed with a detailed analysis that a bilinear eDMD approach in the Koopman framework can be a very powerful data-driven modeling tool in mobile robotics.  
Even with a modest amount of data and a calculation time in the second range, %\textcolor{blue}{@Lea: Wie lange rechnet es denn im Trainingschritt?}, 
the approach can be used to learn a dynamical model that is on average more accurate in predictions than the common nominal kinematic model of a differential-drive robot. 
Moreover, we have seen that and how physical a-priori knowledge can be successfully incorporated into the model, which is interesting beyond the considered application scenario. 
In particular, we have shown how the dictionary of observables can be modified to account for translation invariance. 
Still, there are many remaining topics that we will cover in subsequent research. 
This includes data-driven modeling that strives to include second-order effects such as actuator dynamics and inertia, complicating especially practical considerations such as measuring and sampling of training data. 
Similarly, we will look at non-holonomic vehicles of higher degree of non-holonomy. 
Moreover, we intend to use the learned models for data-based predictive control.
\\


\noindent\textbf{Acknowledgement}: We sincerely thank Manuel Schaller (TU Ilmenau) for his support w.r.t.\ implementation details and fruitful discussions, which improved our manuscript.

\bibliographystyle{IEEEtran}
\bibliography{references_Koopman_robotics, references_nonholonomic_robot}



\end{document}


