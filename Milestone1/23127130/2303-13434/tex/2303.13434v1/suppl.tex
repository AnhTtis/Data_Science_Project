% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper, ]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{overpic}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{overpic}
\usepackage{bm}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{color}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\usepackage[dvipsnames]{xcolor}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\definecolor{hollywoodcerise}{rgb}{0.96, 0.0, 0.63}
\definecolor{lasallegreen}{rgb}{0.03, 0.47, 0.19}
\definecolor{hanpurple}{rgb}{0.32, 0.09, 0.98}
\definecolor{green(pigment)}{rgb}{0.0, 0.65, 0.31}
\definecolor{yellow}{rgb}{0.85, 0.85, 0.31}
\definecolor{blue_light}{rgb}{0.34, 0.95, 0.95}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\hypersetup{colorlinks,linkcolor={red},citecolor={hollywoodcerise},urlcolor={red}}  
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{12593} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}
\usepackage[accsupp]{axessibility}
%\usepackage{authblk}
\begin{document}
%%%%%%%%% TITLE - PLEASE UPDATE
\title{Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective

--Appendix--}

% \author{Jinjing Zhu$^{1}$ \thanks{These authors contributed equally to this work.} \quad Haotian Bai $^{1}$\footnotemark[1] \quad Lin Wang$^{1}$$^{,2}$\thanks{Corresponding author.}\\
% $^{1}$AI Thrust, HKUST(GZ) \quad $^{2}$Dept. of CSE, HKUST\\\\
% {\tt\small zhujinjing.hkust@gmail.com, haotianwhite@outlook.com, linwang@ust.hk}%\\
% %{\tt \small \url{https://vlis2022.github.io/cvpr23/DPPASS}.}
% }

\author{Jinjing Zhu$^{1}$\thanks{These authors contributed equally to this work. }
\quad
Haotian Bai$^{1}$\footnotemark[1]
\quad
Lin Wang$^{1,2}$\thanks{Corresponding Author.}
\and
\affmark[1] AI Thrust, HKUST(GZ)\quad
\affmark[2] Dept. of CSE, HKUST\\
\quad
{\tt\small zhujinjing.hkust@gmail.com, haotianwhite@outlook.com, linwang@ust.hk}
}
\maketitle
%%%%%%%%% ABSTRACT
\begin{abstract}
In this supplementary material, we first prove theorem \ref{theorem:Distribution estimation} in Section \ref{proof}. Then, Section \ref{detials} introduces the details of the proposed method, and Section \ref{algorithm} shows the algorithm of the proposed PMTrans. Section \ref{results} and Section \ref{Allation} show the results, analyses, and ablation experiments to prove the effectiveness of the proposed PMTrans. Finally, Section \ref{Discussion} shows some discussions and detials about our proposed work.
\end{abstract}

\section{Proof}
\label{proof}
\subsection{Domain Distribution Estimation with PatchMix}
Let $\mathcal{H}$ denote the representation spaces with dimensionality  $\operatorname{dim}(\mathcal{H})$, $\mathcal{F}$ denote the set of encoding functions \ie, the feature extractor and $\mathcal{C}$ be the set of decoding functions \ie the classifier. Let $\mathcal{P}_{\lambda}$ be the set of functions to generate mixup ratio for building the intermediate domain. Furthermore, let $P_{S}$, $P_{T}$, and $P_{I}$ be the empirical distributions of data $\mathcal D_{s}$, $\mathcal D_{t}$, and $\mathcal D_{i}$. Define $f^{\star} \in \mathcal{F}, c^{\star} \in \mathcal{C},$ and $\mathcal{P}_{\lambda}^{\star} \in \mathcal{P}_{\lambda}$ be the minimizers of Eq. \ref{eq1} and $D(P_{S}, P_{T})$ as the measure of the domain divergence between $P_S$ and $P_T$:
\begin{equation}
\small
\begin{aligned}
\label{eq1}
 D(P_S,P_T)
 =&\inf _{f \in \mathcal{F}, c \in \mathcal{C}, \mathcal{P}_{\lambda} \in \mathcal{P}} \underset{(\boldsymbol{x}^s, \boldsymbol{y}^s),\left(\boldsymbol{x}^{t}, \boldsymbol{y}^{t}\right)} {\mathbb{E}}\\ &\ell\left(c\left(\mathcal{P}_{\lambda}\left(f(\boldsymbol{x}^{s}), f\left(\boldsymbol{x}^{t}\right)\right)\right), \mathcal{P}_{\lambda}\left(\boldsymbol{y}^{s}, \boldsymbol{y}^{t}\right)\right),   
\end{aligned}
\end{equation}
where $\ell$ is the CE loss.
Then, we can reformulate Eq.\ref{eq1} as:
%
\begin{align}
\small
 & D\left(P_{S},P_{T}\right)=\inf_{\boldsymbol{h}_{1}^{s}, \ldots, \boldsymbol{h}_{n}^{s} \in \mathcal{H}^{s},\boldsymbol{h}_{1}^{t}, \ldots, \boldsymbol{h}_{n}^{t} \in \mathcal{H}^{t}} \frac{1}{n_{s} \times n_{t}} \sum_{i}^{n_{s}}\sum_{j}^{n_{t}}\\
  &\left\{\inf _{c \in \mathcal{C}} \int_{0}^{1} \ell\left(f\left(\mathcal {P}_{\lambda}\left(\boldsymbol{h}_{i}^{s}, \boldsymbol{h}_{j}^{t}\right)\right), \mathcal{P}_{\lambda}\left(\boldsymbol{y}_{i}^{s}, \boldsymbol{y}_{j}^{t}\right)\right) p(\lambda) \mathrm{d} \lambda\right\},   \nonumber
\end{align}
where $\boldsymbol{h}_{i}^{s} = f(\boldsymbol{x}^{s}_{i})$ and $\boldsymbol{h}_{j}^{t} = f(\boldsymbol{x}^{t}_{j})$.
Inspired and borrowed by this work\cite{VermaLBNMLB19}, we give proof as follows.

\begin{theorem}

\label{theorem:Distribution estimation}:Let $d \in \mathbb{N}$ to represent the number of classes contained in three sets $\mathcal D_{s}$, $\mathcal D_{t}$, and $\mathcal D_{i}$. If $\operatorname{dim}(\mathcal{H}) \geq d-1$, ${\mathcal{P}_{\lambda}}’ \ell(c^{\star}(f^{\star}(\boldsymbol{x}_{i})),\boldsymbol{y}^{s})+ (1-{\mathcal{P}_{\lambda}}')\ell(c^{\star}(f^{\star}(\boldsymbol{x}_{i})),\boldsymbol{y}^{t})=0$, then $D\left(P_{S},P_{T}\right)=0$ and the corresponding minimizer $c^{\star}$ is a linear function from $\mathcal{H}$ to $\mathbb{R}^{d}$. Denote the scaled mixup ratio sampled from a learnable Beta distribution as $\mathcal{P}_{\lambda}’$.

\end{theorem}

\noindent\textbf{Proof}:
First, the following statement is $\operatorname{true}$ if $\operatorname{dim}(\mathcal{H}) \geq d-1$ :
$$
\exists A, H \in \mathbb{R}^{\operatorname{dim}(\mathcal{H}) \times d}, b \in \mathbb{R}^{d}: A^{\top} H+b _{d}^{\top}=I_{d \times d},
$$
where $I_{d \times d}$ and $1_{d}$ denote the $d$-dimensional identity matrix and all-one vector, respectively. In fact, $b _{d}^{\top}$ is a rank-one matrix, and the rank of the identity matrix is $d$. Since the column set span of $I_{d \times d}$ needs to be contained in the span of $A^{\top}H+b _{d}^{\top}$, $A^{\top}H$ only needs to be a matrix with the rank $d-1$ to meet this requirement.

Let $c^{\star}(h)=A^{\top} h+b$, for all $h \in \mathcal{H}$.  Let $f^{\star}\left(\boldsymbol{x}_{i}^{s}\right)=H_{\zeta_{i}^{s},:}$ and $f^{\star}\left(\boldsymbol{x}_{j}^{t}\right)=H_{\zeta_{j}^{t},:}$ be the $\zeta_{i}$-th and $\zeta_{j}$-th slice of $H$, respectively.  
Specifically, $\zeta_{i}^{s}, \zeta_{i}^{t} \in\{1, \ldots, d\}$ stand for the class-index of the examples $\boldsymbol{x}_{i}^{s}$ and $\boldsymbol{x}_{j}^{t}$. 
% These choices minimize Eq.\ref{eq1}, since:
Given Eq.\ref{eq1}, the intermediate domain sample $\boldsymbol{x}^{i}_{ij} = \mathcal {P}_{{\lambda}}^{\star}(\boldsymbol{a}_i, \boldsymbol{b}_j) = \mathcal {P}_{{\lambda}}'\cdot \boldsymbol{a}_i + \left(1-{P}_{{\lambda}}'\right)\cdot \boldsymbol{b}_j$, and the definition of cross-entropy loss $\ell$, we get:
\begin{equation}
    \label{eq:loss_merge}
    \begin{aligned}
    &{{\mathcal {P}_{{\lambda}}}'} \ell\left(c^{\star}\left(f^{\star}\left(\boldsymbol{x}^{i}_{ij}\right)\right), \boldsymbol{y}^{s}_{i}\right)+(1-{\mathcal{P}_{\lambda}}')\ell\left(c^{\star}\left(f^{\star}\left(\boldsymbol{x}^{i}_{ij}\right)\right), \boldsymbol{y}^{t}_{j}\right)
    \\
    &=\ell\left(c^{\star}\left({\mathcal {P}_{{\lambda}}}^{\star}\left(f^{\star}\left(\boldsymbol{x}_{i}^{s}\right), f^{\star}\left(\boldsymbol{x}_{j}^{t}\right)\right)\right), {\mathcal {P}_{{\lambda}}}^{\star}\left(\boldsymbol{y}_{i}^{s}, \boldsymbol{y}_{j}^{t}\right)\right)\\
    &= D\left(P_{S},P_{T}\right)=0.
    \end{aligned}
\end{equation} 
%
%
   %  =\ell\left(c^{\star}\left({\mathcal {P}_{{\lambda}}}'\cdot f^{\star}\left(\boldsymbol{x}_{i}^{s}\right)+
   %  \left(1-\mathcal{P}_{{\lambda}}'\right)\cdot f^{\star}\left(\boldsymbol{x}_{j}^{t}\right)\right),\\ 
   %  {\mathcal {P}_{{\lambda}}}'\cdot f^{\star}\left(\boldsymbol{y}_{i}^{s}\right)+
   %  \left(1-\mathcal{P}_{{\lambda}}'\right)\cdot f^{\star}\left(\boldsymbol{y}_{j}^{t}\right) \right) \\
%    
 % 
    % \end{aligned}
% where the intermediate domain sample $\boldsymbol{x}^{i}_{ij}=\mathcal {P}_{{\lambda}}^{\star}(\boldsymbol{x}_i, \boldsymbol{x}_j)$. 
% \begin{equation}
%     \begin{aligned}
%     {{\mathcal{P}}_\lambda}^{\star} \ell\left(c^{\star}\left(f^{\star}\left(\boldsymbol{x}^{i}_{ij}\right)\right), \boldsymbol{y}^{s}_{i}\right)+(1-{{\mathcal{P}}_\lambda}^{\star})\ell\left(c^{\star}\left(f^{\star}\left(\boldsymbol{x}^{i}_{ij}\right)\right), \boldsymbol{y}^{t}_{j}\right)=0, \nonumber
%     \end{aligned}
% \end{equation}
% $\ell\left(c^{\star}\left({\mathcal {P}_{\lambda}}^{\star}\left(f^{\star}\left(\boldsymbol{x}_{i}^{s}\right), f^{\star}\left(\boldsymbol{x}_{j}^{t}\right)\right)\right), {\mathcal {P}_{\lambda}}^{\star}\left(\boldsymbol{y}_{i}^{s}, \boldsymbol{y}_{j}^{t}\right)\right)=0$, 
% and 
Eq.~\ref{eq:loss_merge} reveals that the source and target domains are aligned if mixing the patches from two domains is equivalent to mixing the corresponding labels. 

Furthermore, we see the following: 
\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{attn_has_cls.pdf}
         (a)\\
    \label{fig:has_cls}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{attn_no_cls.pdf}
         (b)\\
    \label{fig:no_cls}
     \end{subfigure}
        \caption{(a) Deit/ViT attention scores with the CLS token. (b) Swin attention scores with an output unit of Classifier that refers to $C_i$ . The dashed line denotes the sequence with each square representing a patch.}
\label{fig:attention}
\end{figure*}
\begin{equation}
    \begin{aligned}
  &\ell\left(c^{\star}\left({\mathcal {P}_{\lambda}}^{\star}\left(f^{\star}\left(\boldsymbol{x}_{i}^{s}\right), f^{\star}\left(\boldsymbol{x}_{j}^{t}\right)\right)\right), {\mathcal {P}_{\lambda}}^{\star}\left(\boldsymbol{y}_{i}^{s}, \boldsymbol{y}_{j}^{t}\right)\right)\\
  &=  \ell\left(A^{\top} {\mathcal {P}_{\lambda}}^{\star}\left(H^{s}_{\zeta_{i},:}, H^{t}_{\zeta_{j},:}\right)+b, {\mathcal {P}_{\lambda}}^{\star}\left(\boldsymbol{y}^{s}_{i, \zeta_{i}}, \boldsymbol{y}^{t}_{j, \zeta_{j}}\right)\right)\\
  &=0.  
    \end{aligned}
\end{equation}
%
The result follows from $A^{\top} H_{\zeta_{i}^{s},:}+b=y_{i, \zeta_{i}^{s}} \text { for all } i \text {, }$ and $A^{\top} H_{\zeta_{j}^{t},:}+b=y_{i, \zeta_{j}^{t}} \text { for all } j \text {. }$ 
%
If $\operatorname{dim}(\mathcal{H}) \geq d-1$, $f^{\star}\left(\boldsymbol{x}_{i}^{t}\right)$ and $f^{\star}\left(\boldsymbol{x}_{j}^{t}\right)$ in the representation space $\mathcal{H}$ have some degrees of freedom to move independently. It also implies that when Eq.~\ref{eq:loss_merge} is minimized, the representation of each class lies on a subspace of dimension $\operatorname{dim}(\mathcal{H})-d+1$, and with larger $dim(\mathcal{H})$, the majority of directions in $\mathcal{H}$-space will contain zero variance in the class-conditional manifold. 


% Sicen $f(\boldsymbol{x}^{s})$ and $f(\boldsymbol{x}^{t})$ can be mapped into the output with the same linear function, which means that $P_{S}$ and $P_{T}$ are the same distribution and the two domains are aligned well. 
In practice, we utilize Eq.~\ref{eq:loss_merge} to measure the domain gaps between the intermediate domain, and other domains and decrease them in the label and feature space, as illustrated in the main paper.
%
% % Based on the above analysis, we measure the domain gap in the feature space. 
% Specifically, we use the cross entropy loss $\ell$ to measure the discrepancy between the intermediate and the other two domains.

\section{Details}
\label{detials}
\subsection{Datasets}
To evaluate the proposed method, we conduct extensive experiments on four popular UDA benchmarks, including  Office-31 \cite{SaenkoKFD10}, Office-Home \cite{VenkateswaraECP17}, VisDA-2017 \cite{abs-1710-06924}, and DomainNet \cite{PengBXHSW19}. 

\textbf{Office-31} consists of 4110 images of 31 categories, with three domains: Amazon (A), Webcam (W), and DSLR (D).

\textbf{Office-Home} is collected from four domains: Artistic images (A), Clip Art (C), Product images (P), and Real-World images (R) and consists of 15500 images from 65 classes. 

\textbf{VisDA-2017} is a more challenging dataset for synthetic-to-real domain adaptation. We set 152397 synthetic images as the source domain data and 55388 real-world images as the target domain data. 

\textbf{DomainNet} is a large-scale benchmark dataset, which has 345 classes from six domains (Clipart (clp), Infograph (inf), Painting (pnt), Quickdraw (qdr), Real (rel), and Sketch (skt)).
\subsection{Attention Map}
\label{sec:attention map}
We calculate the attention score in two ways based on whether the CLS token is present in the sequence. For Swin Transformer, we adopt a method similar to CAM \cite{DBLP:journals/corr/ZhouKLOT15} instead of changing the backbone from CNN to Transformer. Specifically, for a given image, let $f_k(x,y)$ represent the encoded patch $k$ in the last layer at spatial location $(x,y)$. The output of Transformer is followed by a global average pooling (GAP) layer$\sum(x, y)$ and a linear classification head. For the specific class $C_i$, the classification score $S_{C_i}$ is:
\begin{equation}
\label{eq:cls_score}
    S_{C_i} = \sum_{j}w_j^{C_i}\sum_{x,y}f_k(x,y),
\end{equation}
where $w_j^{C_i}$ represents the weight corresponding to class $C_i$ for unit $j$ in the hidden dimension. Eq.\ref{eq:cls_score} ensembles the semantics over both spatial contexts $\sum(x, y)$ and the linear head units $\sum(j)$. Then given Eq.\ref{eq:cls_score}, as shown in Fig.\ref{fig:attention} \textcolor{red}{(b)}, for a given $C_i$, we reallocate the semantic information from the output of linear head unit of $C_i$. In detail, we define the semantic activation map at location $(x,y)$ for a specific class $C_i$ as:
%

\[
    M_{C_i}(x,y) = \sum_j w_j^{C_i} f_k(x,y),
\]
%
where $M_{C_i}\in \mathbb{R}^2$ is the activation for class $C_i$, and we infer $C_i$ by the ground-truth label in the source domain and the pseudo-label in the target domain to obtain the corresponding class activation map to build the intermediate domain. Then, we use $M_{C_i}$ as the attention map after the softmax operation. 

On the other hand, when the CLS token is present in the output sequence of Transformer like Deit/ViT, we simply take the attention scores from the self-attention, \textit{i.e.} the similarity matrix of each layer $i$ in Transformer $Attn_i \in \mathcal{R}^{H\times N\times N}$, and take the average in the head dimension $H$:
%
\[
    Attn_i = \frac{1}{H}\sum_h Attn_h,
\]
%
where $N$ is the sequence length. Next, we only take the CLS token's attention after the softmax operation, as shown in  Fig.\ref{fig:attention} \textcolor{red}{(a)}, and then summarize each layer's scores to obtain the final attention scores $Attn$. 
\[
    Attn = \frac{1}{I}\sum_i Attn_i.
\]
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig3.png}
    \caption{Illustration of the semi-supervised loss in feature space.}
    \label{fig:align_feature}
\end{figure}
\subsection{Semi-supervised mixup loss in the feature space}
\label{detial of semi-supervised loss}
In Fig.\ref{fig:align_feature}, we illustrate the semi-supervised loss in the feature space by similarity between features (in Fig. \ref{fig:align_feature} {\color{red}(a)}) and label spaces(in Fig.\ref{fig:align_feature} {\color{red}(b)}). To compute the similarity of features,  we use the normalized cosine similarity loss between the intermediate domain (column) and source/target domain(row) in the feature space, as shown in Fig.\ref{fig:align_feature}{\color{red}(a)}. Each row denotes the normalized similarity between a sample of the intermediate domain and counterparts from the source domains. For example, we first use the cosine similarity to calculate the similarities between one intermediate sample "car" and four sources (or target) samples (car, clock, apple, sketch clock). Then we normalize these similarities. As for the similarity of outputs (or) labels, since the source samples are labeled, and the target samples are unlabeled, we design two different methods to calculate the supervised and unsupervised label similarities. As for the label similarity between the intermediate and source domains, the intermediate and source samples both share the same labels. Therefore, we define the label similarity $\boldsymbol{y}^{is} = \boldsymbol{y}^s({\boldsymbol{y}^s})^{\intercal}$, as shown in Fig.\ref{fig:align_feature} {\color{red}(b)}. Specifically, $\boldsymbol{y}^{is}$, denoted by the yellow and pink colors, indicates that the label similarity between samples is one for these samples with the same labels (zero for different labels). For example, the label similarities between one intermediate sample "sketch clock" and four sources (or target) samples car, clock, apple, and sketch clock are zero, one, zero, and one. As for the label similarity between the intermediate and target samples, we only know that the intermediate and source samples both share overlapped patches due to lack of supervision. Therefore, the label similarity $\boldsymbol{y}^{it}$ between samples with overlapped patches should be one (pink color), and others should be zero. And we define the label similarity $\boldsymbol{y}^{it}$ as identity matrix. For example, the label similarities between one unlabeled intermediate sample "sketch clock" and four unlabeled target samples car, real clock, apple, and sketch clock are zero, zero, zero, and one. After obtaining the feature and label similarities, we utilize the CE loss $\ell$ to measure the discrepancy between these similarities as the domain gap between the intermediate and other domains.


\subsection{Optimization}
In our game, $m$-th player is endowed with a cost function $J_{m}$ and strives to reduce its cost, which contributes to the change of CE. We now define each player's cost function $J_{m}$ as
\begin{equation}
\small
\begin{split}\label{eq:objective}
J_{{\mathcal{F}}}\left(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{-{\mathcal{F}}}\right) &:=\mathcal{L}_{cls}^{S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{C}})+\alpha {CE}_{s,i, t}(\boldsymbol{\omega}),\\ 
J_{{\mathcal{C}}}\left(\boldsymbol{\omega}_{\mathcal{C}}, \boldsymbol{\omega}_{-{\mathcal{C}}}\right) &:=\mathcal{L}_{cls}^{S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{C}})+\alpha {CE}_{s,i, t}(\boldsymbol{\omega}),\\ 
J_{{\mathcal{P}}}\left(\boldsymbol{\omega}_{{\mathcal{P}}}, \boldsymbol{\omega}_{-{\mathcal{P}}}\right) &:=-\alpha {CE}_{s,i, t}(\boldsymbol{\omega}),    
\end{split}
\end{equation}
where $\alpha$ is the trade-off parameter, $\ell$ is the supervised classification loss for the source domain, and $\text{CE}_{s,i,t}(\boldsymbol{\omega})$ is the discrepancy between the intermediate domain and the source/target domain.
To clarify the min-max process, we introduce the game's vector field $v(w)$, which is identical to the gradient for every player.
\begin{definition}
\label{Def:vector field}
(Vector field): .
\[
v(\boldsymbol{\omega}) := (\bigtriangledown_{{\boldsymbol{\omega}}_\mathcal{F}}J_\mathcal{F}, \bigtriangledown_{{\boldsymbol{\omega}}_\mathcal{C}}J_\mathcal{C}, \bigtriangledown_{{\boldsymbol{\omega}}_\mathcal{P}}J_\mathcal{P})
\]
\end{definition}
By examining Definition.\ref{Def:vector field} with respect to Eq.(\ref{eq:objective}), the process can be categorized into both cooperation and competition \cite{abs-2202-05352}. 

\begin{equation}
    v(w)=
    \begin{pmatrix}
    \bigtriangledown_{{\boldsymbol{\omega}}_\mathcal{F}}\mathcal{L}_{cls}^{S}(\boldsymbol{\omega}_\mathcal{F}, \boldsymbol{\omega}_\mathcal{C})\\
    \bigtriangledown_{{\boldsymbol{\omega}}_\mathcal{C}}\mathcal{L}_{cls}^{S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{C}})\\
    0
    \end{pmatrix}
    +
    \begin{pmatrix}
    \alpha\bigtriangledown_{{\boldsymbol{\omega}}_\mathcal{F}}\text{CE}_{s,i, t}(\boldsymbol{\omega})\\
    \alpha\bigtriangledown_{{\boldsymbol{\omega}}_\mathcal{C}}\text{CE}_{s,i, t}(\boldsymbol{\omega})\\
    -\alpha \bigtriangledown_{{\boldsymbol{\omega}}_\mathcal{P}}\text{CE}_{s,i, t}(\boldsymbol{\omega})
    \end{pmatrix},   
\end{equation}
where the left part is related to the gradient of $\mathcal{L}_{cls}^{S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{C}})$, and the right part denotes the adversarial behavior on producing or consuming CE in the network. In this Min-max CE Game, each player behaves selfishly to reduce their cost function. This competition on the network's CE will possibly end with a situation where no one has anything to gain by changing only one's strategy, called NE. Note that our method does not require explicit usage of gradient reverse layers as the prior GAN-based game design~\cite{GaninL15}. Our training is optimized as 

\begin{equation}
\label{optimal_one}
v(\boldsymbol{\omega})=\bigtriangledown_{(\boldsymbol{\omega}_{\mathcal{F}},\boldsymbol{\omega}_{\mathcal{C}})}\mathcal{L}_{cls}^{S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{C}})+\alpha\bigtriangledown_{{\boldsymbol{\omega}}}{CE}_{s, i,t}(\boldsymbol{\omega}).
\end{equation}
\subsection{Comparisons with Mixup variants}
In Fig.~\ref{fig:PMTrans_mixup}, we show the visual comparisons between the PatchMix and mainstream Mixup variants. Mixup \cite{ZhangCDL18} mixes two samples by interpolating both the images and labels, which suffers from the local ambiguity. CutOut \cite{abs-1708-04552} proposes to randomly mask out square regions of input during training to improve the robustness of the CNNs. Since CutOut decreases the ImageNet localization or object detection performances, CutMix \cite{YunHCOYC19} is further introduced to randomly cut and paste the regions in an image, where the ground truth labels are also mixed proportionally to the area of the regions. However, sometimes there is no valid object in the mixed image due to the random process in augmentation, but there is still a response in the label space. Therefore, not all pixels are created equal, and the labels of pixels should be re-weighted. TransMix \cite{DBLP:journals/corr/abs-2111-09833} is proposed to utilize the attention map to assign the confidence for the mixed samples and re-weighted the labels of pixels. In comparison, we unify these global and local mixup techniques in our PatchMix by learning to combine two patches to form a mixed patch and obtain mixed samples. Furthermore, we also learn the hyperparameters of the mixup ratio for each patch and effectively build up the intermediate domain samples.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{comparison_with_mixup.png}
    \caption{PMTrans and Mixup variants}
    \label{fig:PMTrans_mixup}
\end{figure*}

\section{Algorithm}
\label{algorithm}

In summary, the whole algorithm to train the proposed PMTrans is shown in Algorithm \ref{algo:algo}.

\begin{algorithm}[htb] 
\caption{Patch-Mix Transformer for Unsupervised Domain
Adaptation} 
\label{algo:algo} 
\begin{algorithmic}[1] 
\REQUIRE source domain data $\mathcal{D}_{s}$ and target domain data $\mathcal{D}_{t}$. \\
\ENSURE learned parameters of feature extractor $\mathcal{\boldsymbol{\omega}}_{\mathcal{F}}$, classifier $\mathcal{\boldsymbol{\omega}}_{\mathcal{C}}$, and PatchMix $\mathcal{\boldsymbol{\omega}}_{\mathcal{P}}$. \\
\FOR{$k=0$ to $\mathrm{MaxIter}$}
\STATE Sample a batch of input from source data and target data.
\STATE Encode the patches of source and target inputs by the patch embedding (Emb) layer.
\STATE Calculate the normalized attention score for each patch as Section \ref{sec:attention map}.
\STATE Sample the mixup ratio from Beta($\mathcal{\boldsymbol{\omega}}_{\mathcal{P}}$)
\STATE Construct the intermediate domain input as shown in Eq.~\ref{eq:mixup_eq}.
\STATE Calculate the semi-supervised mixup loss in the feature space via Eq.~\ref{eq:mixupinfeature}.
\STATE Calculate the semi-supervised mixup loss in the label space via Eq. \ref{eq:mixupinlabel}.
\STATE Measure the domain divergence between intermediate domain and other two domains via Eq.~\ref{eq:total }. 
\STATE Update network parameters $\boldsymbol{\omega}$ by optimization (\ref{optimal_one}) via a AdamW \cite{LoshchilovH19} optimizer.
\ENDFOR
\RETURN $\mathcal{\boldsymbol{\omega}}_{\mathcal{F}}$, $\mathcal{\boldsymbol{\omega}}_{\mathcal{C}}$, and $\mathcal{\boldsymbol{\omega}}_{\mathcal{P}}$
\end{algorithmic}
\end{algorithm}
where the related loss functions are shown as follows.
\begin{equation}
\label{eq:mixup_eq}
\small
\begin{split}
  \boldsymbol{x}^i= \mathcal{P}_\lambda(\boldsymbol{x}^s, \boldsymbol{x}^t), &~\boldsymbol{x}^{i}_{k}=\lambda_{k} \odot \boldsymbol{x}_k^s + (1-\lambda_k)\odot\boldsymbol{x}_k^t, \\
\boldsymbol{y}^i=\mathcal{P}_\lambda(\boldsymbol{y}^s, \boldsymbol{y}^t) &= \lambda^s\boldsymbol{y}^s + \lambda^t\boldsymbol{y}^t.
\end{split}
\end{equation}

\begin{equation}
\small
\label{eq:lambda}
\begin{split}
 \lambda^s &= \frac{\sum_{k=1}^{n} \lambda_{k} {a}_{k}^{s}}{\sum_{k=1}^{n} \lambda_{k} {a}_{k}^{s}+\sum_{k=1}^{n}(1-\lambda_{k}){a}_{k}^{t}}
, \\
\lambda^t &= \frac{\sum_{k=1}^{n}(1-\lambda_{k}) {a}_{k}^{t}}{\sum_{k=1}^{n} \lambda_{k} {a}_{k}^{s}+\sum_{k=1}^{n}(1-\lambda_{k}){a}_{k}^{t}}.
\end{split}
\end{equation}
\begin{equation}
\label{eq:mixupinfeature}
\small
\begin{split}
    \mathcal{L}_{f}^{I,S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{P}}) = \mathbb{E}_{\left(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}\right) \sim D^{i}} \lambda^{s}\ell\left(d(\boldsymbol{x}^{i},\boldsymbol{x}^{s}), \boldsymbol{y}^{is}\right), \\
 \mathcal{L}_{f}^{I,T}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{P}}) = \mathbb{E}_{\left(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}\right) \sim D^{i}} \lambda^{t}\ell\left(d(\boldsymbol{x}^{i}, \boldsymbol{x}^{t}), \boldsymbol{y}^{it}\right).   
\end{split}
\end{equation}
\begin{equation}
\label{eq:mixupinlabel}
\small
\begin{split}
\mathcal{L}_{l}^{I,S}(\boldsymbol{\omega}) &= \mathbb{E}_{\left(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}\right) \sim D^{i}} \lambda^{s} \ell\left(\mathcal{C}\left(\mathcal {F}\left(\boldsymbol{x}^{i}\right)\right), \boldsymbol{y}^{s}\right),\\
\mathcal{L}_{l}^{I,T}(\boldsymbol{\omega}) &= \mathbb{E}_{\left(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}\right) \sim D^{i}} \lambda^{t} \ell\left(\mathcal{C}\left(\mathcal {F}\left(\boldsymbol{x}^{i}\right)\right), \hat{\boldsymbol{y}^{t}}\right).
\end{split}
\end{equation}  
\begin{equation}
\label{eq:CE}
     {\text{CE}}_{s,i,t}(\boldsymbol{\omega}) =  \mathcal{L}_{f}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{P}})+\mathcal{L}_{l}(\boldsymbol{\omega}).
\end{equation}
\begin{equation}
\label{eq:total }
 J\left(\boldsymbol{\omega}\right) :=\mathcal{L}_{cls}^{S}(\boldsymbol{\omega}_{\mathcal{F}}, \boldsymbol{\omega}_{\mathcal{C}})+\alpha {\text{CE}}_{s,i,t}(\boldsymbol{\omega}).
\end{equation}
Note that these above equations are introduced in detail in the main paper. 

\section{Results and Analyses}
\label{results}
\subsection{The comparisons on the Office-31, Office-Home, and VisDA-2017}

We compare PMTrans with the SoTA methods, including ResNet- and ViT-based methods. The ResNet-based methods are FixBi \cite{NaJCH21}, CGDM \cite{Du0S0021}, MCD \cite{SaitoWUH18}, SWD \cite{LeeBBU19}, SCDA \cite{0008XLLLQL21}, BNM \cite{CuiWZLH020}, MDD \cite{0002LLJ19}, CKB \cite{LuoR21}, TSA \cite{LiXGLWL21}, DWL \cite{XiaoZ21}, ILA \cite{SharmaKC21}, Symnets \cite{ZhangTJT19}, CAN \cite{Kang0YH19}, and PCT  \cite{TanwisuthFZZZCZ21}. The ViT-based methods are SSRT \cite{abs-2204-07683}, CDTrans \cite{abs-2109-06165}, and TVT \cite{abs-2108-05988}, and we directly quote the results in their original papers for fair comparison. And the more detailed comparisons on three datasets are shown as Tab. \ref{tab:office31_res}, Tab. \ref{tab:officeHome_Res}
, and Tab. \ref{tab:VisDA_result}.
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{a_c_accuracy.png}
    \caption{Accuracy on the task A $\to$ C (Office-Home)}
    \label{fig:my_label}
\end{figure}

\begin{table}[t]
% \renewcommand{\tabcolsep}{15pt}
% \vspace{-30pt}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|l|lllllll}
\toprule
Method  &&A $\to$ W & D$\to$ W & W$\to$ D & A$\to$ D &D$\to$ A &W$\to$ A & \colorbox{lightgray}{Avg}  \\
\hline
\hline
ResNet-50 &\multirow{9}{*}{\rotatebox{90}{ResNet}} &  68.9& 68.4 &62.5& 96.7& 60.7& 99.3& \colorbox{lightgray}{76.1}   \\
BNM &&91.5&98.5&\textbf{100.0}&90.3&70.9&71.6&\colorbox{lightgray}{87.1}\\
DWL& &89.2 &99.2& \textbf{100.0}& 91.2& 73.1& 69.8& \colorbox{lightgray}{87.1}\\
MDD&&94.5& 98.4& \textbf{100.0}& 93.5& 74.6 &72.2& \colorbox{lightgray}{88.9}\\
TSA&&94.8& 99.1& \textbf{100.0}& 92.6& 74.9& 74.4& \colorbox{lightgray}{89.3}\\
ILA+CDAN&&95.7& 99.2& \textbf{100.0} &93.4 &72.1 &75.4& \colorbox{lightgray}{89.3}\\
PCT&&94.6& 98.7& 99.9& 93.8& 77.2& 76.0& \colorbox{lightgray}{90.0}\\
SCDA&&94.2 &98.7& 99.8& 95.2& 75.7& 76.2& \colorbox{lightgray}{90.0}\\
FixBi &  &  96.1& 99.3& \textbf{100.0}& 95.0&78.7& 79.4& \colorbox{lightgray}{91.4}  \\ 
\midrule
TVT &\multirow{7}{*}{\rotatebox{90}{ViT}}&96.4& 99.4& \textbf{100.0}& 96.4& 84.9& 86.0&\colorbox{lightgray}{93.9}   \\
Deit-Base &     &89.2& 98.9& \textbf{100.0}& 88.7& 80.1& 79.8& \colorbox{lightgray}{89.5}\\
CDTrans-Deit& & 96.7 &99.0& \textbf{100.0}& 97.0& 81.1& 81.9 &\colorbox{lightgray}{92.6}  \\
\textbf{PMTrans-Deit}&&99.0&99.4&\textbf{100.0}&96.5&81.4&82.1&\colorbox{lightgray}{93.1}\\
ViT-Base       &&91.2& 99.2& \textbf{100.0}& 90.4& 81.1& 80.6& \colorbox{lightgray}{91.1}  \\
SSRT-ViT&&97.7& 99.2 &\textbf{100.0} &98.6& 83.5& 82.2& \colorbox{lightgray}{93.5}\\
\textbf{PMTrans-ViT}&& 99.1&\textbf{99.6}&\textbf{100.0}&99.4&85.7&86.3&\colorbox{lightgray}{95.0}\\
\midrule
Swin-Base &\multirow{2}{*}{\rotatebox{90}{Swin}}&97.0   &99.2   &\textbf{100.0}   &95.8   &82.4   &81.8   &\colorbox{lightgray}{92.7}   \\
\textbf{PMTrans-Swin} & &\textbf{99.5}   &99.4  &\textbf{100.0}   &\textbf{99.8}   &\textbf{86.7}   &\textbf{86.5}   &\colorbox{lightgray}{\textbf{95.3}}  \\ 
\bottomrule
\end{tabular}}
\caption{Comparison with SoTA methods on Office-31. The best performance is marked as \textbf{bold}.}
\label{tab:office31_res}
\vspace{-10pt}
\end{table}
\begin{table*}[t]
\centering
\resizebox{0.99\linewidth}{!}{ 
\begin{tabular}{c|l|llllllllllllll}
\toprule
Method &&A$\to$ C& A$\to$ P & A $\to$ R & C $\to$ A &C $\to$ P &C $\to$ R &P$\to$ A& P$\to$ C & P$ \to$ R & R$ \to$ A &R$ \to$ C &R$ \to$ P& Avg     \\
\hline
\hline
ResNet-50 &\multirow{9}{*}{\rotatebox{90}{ResNet}}& 44.9& 66.3 &74.3 &51.8& 61.9 &63.6& 52.4 &39.1& 71.2& 63.8& 45.9& 77.2 &\colorbox{lightgray}{59.4}   \\
MCD &&48.9& 68.3& 74.6& 61.3& 67.6& 68.8& 57.0& 47.1 &75.1& 69.1& 52.2& 79.6& \colorbox{lightgray}{64.1}\\
Symnets&&47.7& 72.9 &78.5& 64.2 &71.3& 74.2& 64.2& 48.8& 79.5& 74.5& 52.6 &82.7& \colorbox{lightgray}{67.6}\\
MDD &&54.9& 73.7& 77.8& 60.0& 71.4& 71.8& 61.2& 53.6& 78.1& 72.5& 60.2& 82.3& \colorbox{lightgray}{68.1}\\
TSA&&53.6& 75.1& 78.3& 64.4 &73.7& 72.5& 62.3& 49.4 &77.5& 72.2& 58.8 &82.1& \colorbox{lightgray}{68.3}\\
CKB&&54.7 &74.4 &77.1& 63.7& 72.2& 71.8 &64.1& 51.7 &78.4 &73.1& 58.0& 82.4& \colorbox{lightgray}{68.5}\\
BNM&&56.7 &77.5& 81.0& 67.3& 76.3& 77.1& 65.3& 55.1& 82.0 &73.6 &57.0& 84.3& \colorbox{lightgray}{71.1}\\
PCT&& 57.1 & 78.3&81.4 & 67.6&77.0&76.5& 68.0& 55.0&81.3 &74.7& 60.0 & 85.3& \colorbox{lightgray}{71.8}\\
FixBi && 58.1& 77.3& 80.4 &67.7& 79.5& 78.1& 65.8& 57.9& 81.7& 76.4& 62.9& 86.7& \colorbox{lightgray}{72.7}  \\
\midrule
TVT & \multirow{3}{*}{\rotatebox{90}{ViT}}&   74.9&86.8&89.5&82.8&88.0&88.3&79.8&71.9&90.1&85.5&74.6&90.6&\colorbox{lightgray}{83.6}\\
Deit-Base&&61.8 &79.5& 84.3& 75.4 &78.8& 81.2& 72.8 &55.7& 84.4& 78.3& 59.3& 86.0 &\colorbox{lightgray}{74.8} \\
CDTrans-Deit&  & 68.8&85.0&86.9&81.5&87.1&87.3&79.6&63.3&88.2&82.0&66.0&90.6&\colorbox{lightgray}{80.5}   \\
\textbf{PMTrans-Deit} &&71.8& 87.3 &88.3 &83.0 &87.7 & 87.8 &78.5 &67.4 &89.3 &81.7 &70.7 & 92.0 &\colorbox{lightgray}{82.1}\\
ViT-Base &&67.0 &85.7 &88.1 &80.1 &84.1 &86.7 &79.5 &67.0 &89.4 &83.6 &70.2 &91.2 & \colorbox{lightgray}{81.1}\\
SSRT-ViT&&75.2& 89.0& 91.1& 85.1 &88.3& 89.9& 85.0& 74.2 &91.2& 85.7& 78.6 &91.8& \colorbox{lightgray}{85.4}\\
\textbf{PMTrans-ViT} &&81.2&91.6&92.4&\textbf{88.9}&91.6&93.0&\textbf{88.5}&80.0&\textbf{93.4}&\textbf{89.5}&\textbf{82.4}&94.5&\colorbox{lightgray}{88.9}\\
\midrule
Swin-Base&\multirow{2}{*}{\rotatebox{90}{Swin}}&72.7   &87.1   &90.6   &84.3   &87.3   &89.3   &80.6 &68.6 &90.3&84.8&69.4&91.3& \colorbox{lightgray}{83.6}  \\
\textbf{PMTrans-Swin} && \textbf{81.3}&\textbf{92.9}&\textbf{92.8}&88.4&\textbf{93.4}&\textbf{93.2}&87.9&\textbf{80.4}&93.0&89.0&80.9&\textbf{94.8}&\colorbox{lightgray}{\textbf{89.0}}\\
% 79.7  &\textbf{92.3}   &\textbf{92.6}   &88.3   &\textbf{93.1} &92.8  &87.3&\textbf{80.0} &92.8 &88.8 &79.8 &\textbf{94.6} & \colorbox{lightgray}{88.5}  \\ 
\bottomrule
\end{tabular}}
\caption{Comparison with SoTA methods on Office-Home. The best performance is marked as \textbf{bold}.}
\label{tab:officeHome_Res}
\end{table*}
\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{ 
\begin{tabular}{c|l|lllllllllllll}
\toprule
Method &&plane& bcycl& bus &car& horse& knife& mcycl& person& plant &sktbrd &train& truck& Avg     \\
\hline
\hline
ResNet-50&\multirow{8}{*}{\rotatebox{90}{ResNet}}  & 55.1 &53.3& 61.9& 59.1& 80.6 &17.9 &79.7& 31.2& 81.0 &26.5 &73.5& 8.5& \colorbox{lightgray}{52.4} \\
BNM &&89.6 &61.5 &76.9 &55.0& 89.3& 69.1 &81.3 &65.5& 90.0 &47.3& 89.1 &30.1& \colorbox{lightgray}{70.4}\\
MCD& &  87.0 &60.9 &83.7& 64.0& 88.9& 79.6& 84.7& 76.9& 88.6& 40.3& 83.0& 25.8 &\colorbox{lightgray}{71.9}\\
SWD&&90.8 &82.5 &81.7 &70.5& 91.7 &69.5& 86.3& 77.5 &87.4& 63.6& 85.6 &29.2 &\colorbox{lightgray}{76.4}\\
DWL&& 90.7 &80.2& 86.1& 67.6 &92.4 &81.5& 86.8& 78.0& 90.6 &57.1 &85.6& 28.7& \colorbox{lightgray}{77.1}\\
CGDM& & 93.4& 82.7& 73.2& 68.4& 92.9 &94.5 &88.7 &82.1 &93.4 &82.5& 86.8& 49.2& \colorbox{lightgray}{82.3}\\
CAN&&97 &87.2& 82.5 &74.3 &97.8& 96.2 &90.8& 80.7 &96.6& 96.3& 87.5& 59.9& \colorbox{lightgray}{87.2}\\
FixBi && 96.1 &87.8& 90.5& 90.3 &96.8& 95.3 &92.8& 88.7& 97.2 &94.2& 90.9 &25.7& \colorbox{lightgray}{87.2}  \\ 
\midrule
TVT&\multirow{7}{*}{\rotatebox{90}{ViT}}& 82.9 &85.6 &77.5 &60.5& 93.6& 98.2& 89.4& 76.4& 93.6& 92.0& 91.7& 55.7&\colorbox{lightgray}{83.1}\\
Deit-Base & &  98.2 &73.0 &82.5& 62.0 &97.3& 63.5& 96.5& 29.8&68.7 &86.7 &96.7&23.6&\colorbox{lightgray}{73.2} \\
CDTrans-Deit&&97.1& 90.5& 82.4& 77.5& 96.6&96.1 &93.6 &\textbf{88.6}&\textbf{97.9}&86.9 &90.3&\textbf{62.8}&\colorbox{lightgray}{88.4}\\
\textbf{PMTrans-Deit}&&98.2& 92.2 &88.1 &77.0 &97.4&95.8 &94.0&72.1 &97.1 &95.2 &94.6&51.0 &\colorbox{lightgray}{87.7}\\
ViT-Base & &99.1& 60.7& 70.1& 82.7& 96.5 &73.1& 97.1 &19.7 &64.5& 94.7 &97.2& 15.4&\colorbox{lightgray}{72.6}\\
SSRT-ViT&&98.9& 87.6 &\textbf{89.1}&\textbf{84.8}& 98.3& \textbf{98.7}& 96.3&81.1& 94.8 &97.9& 94.5 &43.1& \colorbox{lightgray}{\textbf{88.8}}\\
\textbf{PMTrans-ViT}&& 98.9 & \textbf{93.7} &84.5 &73.3 &\textbf{99.0} &98.0 &96.2 &67.8 &94.2 &\textbf{98.4} &96.6 &49.0 & \colorbox{lightgray}{87.5}\\
\midrule
Swin-Base &\multirow{2}{*}{\rotatebox{90}{Swin}}        &99.3   &  63.4 &  85.9 & 68.9  &  95.1 & 79.6  &\textbf{ 97.1}&29.0&81.4&94.2&\textbf{97.7}&29.6&\colorbox{lightgray}{76.8}  \\
\textbf{PMTrans-Swin}&& \textbf{99.4}                        & 88.3  & 88.1  &78.9   & 98.8  &98.3   &95.8&70.3   &94.6&98.3&96.3&48.5& \colorbox{lightgray}{88.0}  \\ 
\bottomrule
\end{tabular}}
\caption{Comparison with SoTA methods on VisDA-2017. The best performance is marked as \textbf{bold}.}
\label{tab:VisDA_result}
\end{table*}
\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{ 
\begin{tabular}{c|lllllllllllll}
\hline
backbone &plane& bcycl& bus &car& horse& knife& mcycl& person& plant &sktbrd &train& truck& Avg     \\
\hline
\hline
ViT-bs8 & 99.0 & 92.7 &84.3 &68.0 &\textbf{99.1} &\textbf{98.5} &96.4 &37.6 &93.6 &98.5 & 96.7 & 48.2 & \colorbox{lightgray}{84.4}\\ 
ViT-bs16 & 99.1 & 91.9 &85.9 &69.7 &99.0 &\textbf{98.5} &96.5 &43.1 &93.8 & \textbf{99.2} & \textbf{96.9} &\textbf{50.5} &\colorbox{lightgray}{85.3}\\ 
ViT-bs24 & 98.8 & 92.8 &84.5 &71.1 &\textbf{99.1} &98.3 &\textbf{96.7}&58.9 &93.8 & 98.8 & 96.7 &47.7 &\colorbox{lightgray}{86.4}\\ 
ViT-bs32& 98.9 & 93.7 &84.5 &73.3 &99.0&98.0 &96.2 &67.8 &94.2 &98.4 &96.6 &49.0 &\colorbox{lightgray}{87.5}\\
\hline
Deit-bs8 & 98.1 & 89.5 &86.9 &73.5 &97.5 &96.9 &95.7 &71.8 &96.3 & 92.1 & 95.6 &45.5 &\colorbox{lightgray}{86.6}\\ 
Deit-bs16 & 98.3 & 90.0 &87.0 &74.2 &97.4 &96.9 &95.7 &72.2 &96.7 & 92.2 & 95.8 &46.5 &\colorbox{lightgray}{86.9}\\ 
Deit-bs24 & 98.2 & 90.2 &87.0 &74.8 &97.5 &96.8 &95.7 &\textbf{73.2} &96.8 & 92.1 & 95.6 &46.9&\colorbox{lightgray}{87.1}\\ 
Deit-bs32 &98.2& \textbf{92.2} &\textbf{88.1} &77.0 &97.4 &95.8 &94.0 &72.1 &\textbf{97.1} &95.2 &94.6 &51.0 &\colorbox{lightgray}{87.7}\\

\hline
Swin-bs8 & 99.3 & 87.3 &87.7 &66.9 &98.8 &98.1 &96.4 &57.5 &95.2 & 98.0 & 96.5 &44.2 & \colorbox{lightgray}{85.5}\\ 
Swin-bs16 & 99.2 & 87.6 &87.5 &66.4 &98.8&98.3 &96.3 &58.4 &95.4 & 98.0 & 96.5 &44.6 &\colorbox{lightgray}{85.6}\\ 
Swin-bs24 & 99.2 & 88.1 &87.3 &67.1 &98.7 &98.2 &96.1 &67.1 &94.0 & 97.9 & 96.3 &44.2 &\colorbox{lightgray}{86.2}\\ 
Swin-bs32&\textbf{99.4}& 88.3  & \textbf{88.1}  &\textbf{78.9}   & 98.8  &98.3   &95.8&70.3   &94.6& 98.3&96.3&48.5& \colorbox{lightgray}{\textbf{88.0}}  \\

\hline
\end{tabular}}
\caption{Comparisons between different backbones with different batch sizes on VisDA-2017. The best performance is marked as \textbf{bold}.}
\label{tab:VisDA_batch_size}
\vspace{-10pt}
\end{table*}
\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{ 
\begin{tabular}{c|llllllllllllll}
\hline
Method&A$\to$ C& A$\to$ P & A $\to$ R & C $\to$ A &C $\to$ P &C $\to$ R &P$\to$ A& P$\to$ C & P$ \to$ R & R$ \to$ A &R$ \to$ C &R$ \to$ P& Avg     \\
\hline
\hline
w/o class information& \textbf{81.3}&\textbf{92.9}&92.8&88.4&93.4&93.2&87.9&\textbf{80.4}&93.0&89.0&\textbf{80.9}&94.8&\colorbox{lightgray}{89.0} \\
w/ class information&80.9&92.7&\textbf{93.4}&\textbf{88.9}&\textbf{93.7}&\textbf{93.9}&\textbf{88.3}&80.2&\textbf{93.5}&\textbf{89.4}&80.3&\textbf{95.3}&\colorbox{lightgray}{\textbf{89.2}}\\
\hline
\end{tabular}}
\caption{Effect of semi-supervised loss with class information. The best performance is marked as \textbf{bold}.}
\label{tab:semi}
\end{table*}
\subsection{Training}
We show the progress of training on PMTrans-Swin, PMTrans-Deit, and PMTrans-ViT. To specify how each loss changes, including semi-supervised mixup loss in the label space $\mathcal{L}_l$, semi-supervised mixup loss in the feature space $\mathcal{L}_f$, and source classification loss $\mathcal{L}_{cls}^{S}$, we conduct the experiment on task $A\to C$ on Office-Home for above architectures, and the results are shown in Fig.\ref{fig:loss}. We observe that for all models, both $\mathcal{L}_f$ and $\mathcal{L}_l$ drop constantly, which means the domain gap is reducing as the training evolves. Significantly, $\mathcal{L}_f$ fluctuates more than $\mathcal{L}_l$ as it aligns the domains in the feature space with a higher dimension. 
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{loss_sup.pdf}
    \caption{Loss on task A $\to$ C (Office-Home). Lines are smoothed for clarity.}
    \label{fig:loss}
\end{figure*}
\subsection{Testing}
In Fig.\ref{fig:my_label}, we testify PMTrans-Swin, PMTrans-ViT, and PMTrans-Deit on the task A $\to$ C on the Office-Home dataset. From Fig. \ref{fig:my_label}, with the same number of epochs, PMTrans-ViT achieves faster convergence than PMTrans-Swin and PMTrans-Deit. Besides, the results further reveal that our proposed PMTrans with different transformer backbones can bridge the source and target domains well and decrease domain divergence effectively.
\subsection{Complexity}
We compare our computational budget with the typical work CDTrans~\cite{abs-2109-06165} on aligning the source and target domains, excluding the choice of backbone. Precisely, CDTrans compute the similarity between patches from two domains by the multi-head self-attention. We are given $n$ as the sequence length, $d$ as the representation dimension, and $c$ as the number of classes. The per-layer complexity is $O(n^2 d)$. While in PMTrans, we adopt CE loss to close the domain gap on both the feature and label spaces of the out, whose complexity is $O(d)+O(c)$. When building the intermediate domain, PatchMix samples patches element-wisely, and its complexity is $O(n)$. As attention scores we use are taken directly from the parameters of Transformer and Classifier, so it brings no additional cost. PMTrans's complexity is $O(d+c+n)$, so it is much more lightweight than the cross attention in CDTrans. 
\subsection{Attention map visualization for target data}
We randomly sample four images from Product (P) of Office-Home and use the pre-trained models $C\to P$ including PMTrans-Swin and PMTrans-Deit to infer the attention maps following the methods described in Sec.\ref{sec:attention map}. In Fig.~\ref{fig:visual_attention_target}, we compare the two PMTrans with their counterparts trained with only source classification loss. We observe that after domain alignment, the attention maps tend to be more focused on the objects \ie less noise around them. Interestingly, for the image whose ground truth label is {\ttfamily pencil} in the fourth row, Swin-based backbone can distinguish it from {\ttfamily plasticine} around or attached to it. At the same time, Deit-based attention covers them all, which may bring negative effects. When the attention scores are used to scale the weights of patches during constructing the intermediate domain in Eq.\ref{eq:lambda}, Swin-based architecture can focus more on semantics while others may not. That may be one of the reasons why PMTrans-Swin gets superior performance on many datasets. Similarly, TS-CAM \cite{DBLP:journals/corr/abs-2103-14862} names the original attention scores from Transformer like Fig.\ref{fig:attention} \textcolor{red}{(a)} as semantic-agnostic, while what we do in Fig.\ref{fig:attention} \textcolor{red}{(b)} is to reallocate the semantics from Classifier back into the patches and make it be aware of specific class activation. 
\section{Ablation Study}
\label{Allation}
\subsection{Batch size}
In Tab. \ref{tab:VisDA_batch_size}, we study the effect of the batch size with different backbones in our proposed PMTrans framework. As shown in Tab. \ref{tab:VisDA_batch_size}, when the batch size is bigger, the input can represent the data distributions better, and therefore the proposed PMTrans based on different backbones with larger batch sizes generally achieves better performance in UDA tasks. Considering the hardware limit, we cannot train models with a batch size of more than 32, so our performance may be lower than it could be, especially when putting in the same condition with a 64 batch size as many previous works do. 

\subsection{Semi-supervised mixup loss with class information}
Tab. ~\ref{tab:semi} shows the comparisons between PMTrans, where the semi-supervised mixup loss combines the class information of target data or not. Note that we use the pseudo labels of target data to calculate the discrepancy between the features and labels. We agree that the semi-supervised mixup loss with class information decreases the domain gaps by reducing the disparity between the feature and label similarities with supervised techniques.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{attn.pdf}
    \caption{Attention visualization on Swin-based and Deit-based backbones.}
    \label{fig:visual_attention_target}
\end{figure*}
\section{Discussion and Details}
\label{Discussion}
\subsection{Details of mixing labels}
In practice, labels are weighted by attention score, since attention score of high-response regions accounts for most of the context ( as shown in Fig. \textcolor{red}{6}). Therefore, mixing the re-weight labels approximates mixing labels, which is equivalent to mixing patches proved by Theorem.1. Experiments verify that PMTrans with attention score converges easier without sacrificing performance. 
\subsection{Details of semi-supervised losses}
Pseudo-label generation in the target domain is done by every epoch. As the performance of cross-attention in CDTrans highly depends on the quality of pseudo labels, it becomes less effective when the domain gap becomes large. Therefore, we address this issue in three ways: (a) building the intermediate domain; (b) re-weighting intermediate images' labels; (c) generating the pseudo labels of the target at each epoch. This way, the pseudo labels of the target are more reliable as the domain shift decreases, thus enabling the proposed PMTrans to transfer the knowledge between domains well.

\subsection{Details of $y^{it}$ }
Note that $y^{it}$ indeed does not cause any confirmation bias because $y^{it}$ is defined \textit{without considering pseudo labels of target} (as only noisy pseudo labels lead to confirmation bias). \textit{The novelty of our idea lies in that we utilize the identity matrix to measure the label similarity} $y^{it}$ (without using pseudo labels of target) instead of similarity, like $y^{is}$, to decrease the confirmation bias.

\subsection{Learning hyper-parameters of mixup}
\textit{PatchMix mixes patches from two domains for maximizing CE even if $\lambda$ is fixed}. That is, PatchMix can maximize CE, and the other two players minimize CE. More importantly, we propose to \textit{learn the parameters of Beta distribution for the flexible or learnable mixup, so as to build a more effective intermediate domain for bridging the source and target domains}. Numerically, Tab. \textcolor{red}{6} in main paper demonstrates that learning parameters of Beta distribution is \textit{superior} than the fixed parameters in the same min-max CE game.

\subsection{t-SNE visualization}
Source instances are naturally more cohesive than target instances because only source supervision is accessible before adaptation. After adaptation, PMTrans constructs the compact cluster of the target domain instances closer to the source domain instances than Swin-Base. The comparison of visualization proves that PMTrans can effectively bridge the gap between domains.

\subsection{Effectiveness of attention score} 
We conduct an ablation study on Office-Home; and PMTrans-ViT with or without attention score achieves the same performance \textbf{88.9\%}. And results show \textit{PMTrans with attention are easier to converge than that without attention} due PMTrans can utilize attention score to obtain high-response regions.

\subsection{Impart of patch size on performance}
We compare our PMTrans with SSRT using the same batch size as 32 for an apple-to-apple comparison. The result shows that PMTrans achieve comparable performance with SSRT (\textbf{88.0\%} vs. \textbf{88.0\%}), demonstrating the effectiveness of PMTrans.
\clearpage

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
