\section{Introduction}
\label{sec:intro}
Verbal misbehavior and toxicity on Online Social Networks (OSNs) are receiving a huge amount of attention in the research community, with efforts to identify~\cite{zhang2019hate, Gaydhani2018DetectingHS, 8292838, basile-etal-2019-semeval, calderon2020topic}, characterize~\cite{Criss202, clarke2017dimensions, benigni2017online, fernandez2018understanding, chatzakou2017mean}, and automatically detect  ~\cite{anzovino2018automatic, fortuna2018survey, d2020bert} online misbehavior, especially on Twitter. 
Despite all these ongoing efforts, toxicity has increased over time. We note that almost all efforts to study toxicity on Twitter come from the content study of tweets posted around sporadic high-profile campaigns and events such as elections~\cite{https://doi.org/10.48550/arxiv.2103.01664}, important world events of COVID19 and the MeTooMovement~\cite{https://doi.org/10.48550/arxiv.2005.12423,ALkhalifa2020}, or controversies about topics like Bitcoin~\cite{pacheco2021uncovering}. 
However, these studies do not explore the prolonged involvement of a profile in spreading toxic content, so its utility in the characterization of overall toxicity was hindered. 
Works like~\cite{Wei2018,https://doi.org/10.48550/arxiv.1803.08977} investigate toxic profiles responsible for disseminating toxic content on small manually annotated datasets. 
In essence, efforts toward the automatic detection and characterization of toxicity on Twitter are mostly event-centric or small-scale, user-centric. This scenario leaves a gap in understanding the entire landscape of misbehavior on Twitter. 

Toxic content follows a Pareto-like distribution on Twitter~\cite{ribeiro2018characterizing}, hence we focus on the most toxic profiles in our dataset based on the median Perspective ``Toxicity'' score of the profile's tweets. We contrast these profiles with the remainder of our dataset to find out how much their behavior is different from base Twitter profiles. We focus on research questions that will assist us in better understanding toxic profiles:
\begin{itemize}[leftmargin=*]
    \item Are toxic profiles prolific content generators, with a specific tweeting pattern?
    \item Do toxic profiles tweet in a legible way to effectively convey their message?
    \item What type of misconduct is expected of a toxic profile?
    \item Do toxic profiles leverage auxiliary content, such as URLs and hashtags?
    \item Do toxic profiles demonstrate special trends with respect to name, location, counts of friends or followers, and such?
    \item Can we expect very toxic profiles to be bad bots?
\end{itemize}
%
Our dataset, described in \S\ref{sec:dataset}, is seeded with seven smaller public datasets from past works studying online misbehavior on Twitter. These seed datasets cover  
multiple themes of online misbehavior: hostility, racism, abuse, hatefulness, homophobia, spam, and sexism, and are balanced in the number of toxic and non-toxic users. A key limitation of the seed datasets is that users are classified as toxic or not from the content of a single or a few tweets, which does not allow deeper analysis of the users' average toxic behavior. To enable such analysis, we crawl the tweet timeline of each of the users present in the seed datasets. 
Our resulting dataset contains 142,987 (143K) Twitter profiles and 293,401,161 (293M) individual tweets posted between 2007 and 2021. Human annotations are 
untenable given the size of our dataset,
hence, we turn to Google's Perspective APS~\cite{perspective} models to assign toxicity scores to each tweet, providing estimates of the following types of misbehavior: 
\emph{Toxicity, Severe Toxicity, Identity Attack, Inflammatory, Threat, and Insult}. We use only the production-ready scores from the Perspective API, which provide highly reliable estimates. 

In \S\ref{sec:prolificacy_analysis}, we investigate these highly toxic profiles with respect to tweeting frequency and dynamics, drawing on the distribution of inter-tweet times and a measure of burstiness.
Next in \S\ref{sec:content_analysis}, we look at the tweet content. We explore the Perspective scores  
and their consistency among each profile's tweets using the Gini Index~\cite{gini1912variabilita}. Next, we study the number and quality of hashtags and URLs with help of Fortiguard, a service that categorizes URLs by topic. We then perform a readability analysis of tweets, using Flesch reading ease and difficulty scores, Linsear write score, and the Automatic Readability Index (ARI). In \S\ref{sec:profile_analysis}, we characterize toxic profiles based on the profile metadata including the number of friends, followers, statuses, favorites, membership of lists, location, creation date, and profile status. In addition, we use to apply the Botometer API~\cite{Sayyadiharikandeh_2020} to our profiles, obtaining scores that quantify astroturfing, spamming, fake followers, self-declared bots, and financial bots. 

This work makes the following main contributions: 
\begin{itemize}[leftmargin=*]
    \item We collect and curate a longitudinal dataset of tweets, spanning 16 years, consisting of 293M tweets (\S\ref{sec:dataset}) and augmented with six perspective scores. To our knowledge, this is the largest annotated dataset on online misbehavior. To foster further research, upon publication, we plan to share our dataset with the research community.
    \item We identify that the top 1\% toxic profiles post fewer, shorter, but more articulate tweets than the rest. 
    We find that the Gini Index of Perspective scores on each toxic profile's tweets is lower, indicating consistency of misbehavior among their tweets.  
    \item We observe that the highest Perspective scores among 
    tweets from toxic profiles are ``Inflammatory'' and ``Insult'', and that ``Identity attack'' is relatively low, especially when compared to where it sits among baseline tweets. 
    \item The top 1\% toxic profiles tend to use fewer but more coherent and similar URLs and hashtags.
    \item We find that the top 1\% toxic profiles have lower friends and follower counts than baseline profiles.
    \item We observe a notable increase in the creation of toxic profiles between 2014-2016. Interestingly, we note that despite being the most toxic, none of the top 1\% profiles has been deactivated, banned, or deleted in the 18 months that passed between timeline data crawling and profile metadata extraction. 
    \item Notably, we identify that just under half the top 1\% toxic profiles would be classified by Botometer as fake followers, which is important evidence of instrumented trolling campaigns on Twitter.
\end{itemize}

