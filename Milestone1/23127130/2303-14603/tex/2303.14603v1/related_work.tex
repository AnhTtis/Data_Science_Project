\section{Related Work}
\label{sec:rwork}
Online misbehavior detection on social networks and on Twitter has been extensively explored by several studies such as~\cite{gomez2019exploring,ribeiro2018like,founta2018large, waseem-hovy-2016-hateful, DHUNGANASAINJU2021106735}. However, almost all approaches to indicate hate or toxicity is content-centric, the inherent shortfall of collecting and annotating toxic tweets is due to the incompleteness and insufficiency of OSN text i.e., tweets, and the sparsity of toxic hateful speech. These limitations are often amplified by oversimplifying the problem, such as considering only tweets collected around extremist events or collected with keyword searches. In this work, we partially address these limitations by accumulating a user-centric dataset, such work is done on a very limited level by ~\cite{https://doi.org/10.48550/arxiv.1803.08977}. 

Past studies \cite{gomez2019exploring, ribeiro2018like} have relied on human annotations to differentiate between toxic and non-toxic tweets. Our work leverages the ML models of the Perspective API to rate the collected tweets to explore the different dimensions of misbehavior beyond the prior works, and at the larger scale of data, i.e., 293M tweets. Similarly, previous work \cite{hosseini2017deceiving, jain2018adversarial} has studied Google's Perspective API~\cite{perspective} and its resilience against adversarial attacks. Those studies leveraged Perspective API to score and analyze the toxicity of tweets. 
This work takes precedence over these studies in terms of the size of the data set (293M tweets) and the number of misbehavior dimensions not studied in the past, namely Insult, Inflammatory, Threat, and Identity Attacks.

Automated accounts, paid bots, or trolls' role in toxic and false content creation and dissemination~\cite{PewResearchCenter2018, Ferrara2020, TwitterBotAccounts},  is the base of a consistent spread of toxicity on OSNs. Content-based features best predict coordinated efforts of these malefactors ~\cite{FFI-RAPPORT}, but unsupervised ML for detection of coordinated efforts of profiles in carrying these operations are infeasible at scale~\cite{alizadeh2020content}.
Analysis of unlabeled profiles' longitudinal and unlabeled content provides a characterization of the most toxic users and their content on Twitter.

















