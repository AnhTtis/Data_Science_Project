{
    "arxiv_id": "2303.14242",
    "paper_title": "IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients",
    "authors": [
        "Ruo Yang",
        "Binghui Wang",
        "Mustafa Bilgic"
    ],
    "submission_date": "2023-03-24",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
    ],
    "abstract": "Integrated Gradients (IG) as well as its variants are well-known techniques for interpreting the decisions of deep neural networks. While IG-based approaches attain state-of-the-art performance, they often integrate noise into their explanation saliency maps, which reduce their interpretability. To minimize the noise, we examine the source of the noise analytically and propose a new approach to reduce the explanation noise based on our analytical findings. We propose the Important Direction Gradient Integration (IDGI) framework, which can be easily incorporated into any IG-based method that uses the Reimann Integration for integrated gradient computation. Extensive experiments with three IG-based methods show that IDGI improves them drastically on numerous interpretability metrics.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14242v1"
    ],
    "publication_venue": "Accepted by CVPR 2023"
}