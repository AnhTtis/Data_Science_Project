\section{Optimization in terms of generalized Chebyshev polynomials}
\label{section_optimization}
\setcounter{equation}{0}

In the previous section, we have shown that the trigonometric optimization problem with crystallographic symmetry from \Cref{OptiProblemExpo} is equivalent to optimizing a linear combination of generalized Chebyshev polynomials
\begin{equation}\label{eq_ObjectivePoly}
	f(z)
=	\sum\limits_{\weight\in S} c_\weight\,T_\weight(z)
\in	\RX
\end{equation}
with $S\subseteq \Weights^+$ finite and $c_\weight\in\R$. Here, $\Image$ is the image of the generalized cosines, a compact basic semi--algebraic set that can be represented as
\[
	\Image
=	\{ \gencos{}(u) \,\vert\, u\in\R^n \}
=	\{ z\in\R^n \,\vert\, \posmat(z) \succeq 0 \},
\]
where $\posmat\in\RX^{n\times n}$ is a symmetric matrix polynomial, for example given by \Cref{thm_HermiteCharacterization}. In the present section, we show how to solve this new polynomial optimization problem
\begin{equation}\label{eq_polyoptprob}
	f^*
	=	\min\limits_{z\in\Image} f(z)
	=		\begin{array}[t]{rl}
		\min		&	f(z) \\
		\mbox{s.t.}	&	z\in \R^n,\,\posmat(z)\succeq 0
	\end{array}
\end{equation}
numerically. We do this by adapting Lasserre's hierarchy. The novelty lies in exploiting the representation of the objective function in terms of generalized Chebyshev polynomials, which leads to a new notion of the hierarchy order.

\subsection{Matrix version of Putinar's theorem}

In \cite{lasserre01}, Lasserre proposes a hierarchy of dual moment relaxations and sums of squares (SOS) reinforcements based on Putinar's Positivstellensatz \cite{putinar93} to solve such problems, when the polynomial matrix inequality $\posmat(z)\succeq 0$ (PMI) is replaced by finitely many scalar constraints. In principle, our problem falls in this setting. Indeed, the PMI can be rewritten to scalar inequalities by taking the coefficients of the characteristic polynomial and using Descartes' rule of signs \cite[Theorem 2.33]{roy13}. We would prefer to avoid such an approach, since the degrees of the so obtained scalar constraints are generically much larger than the entries of the matrix polynomial $\posmat$.

To overcome this degree problem, Henrion and Lasserre \cite{henrion06} suggest using another Positivstellensatz due to Hol and Scherer, see \Cref{thm_holscherer}, and propose a hierarchy of dual moment relaxations and matrix SOS reinforcements, that benefits from the matrix structure.

\subsubsection{Matrix SOS reinforcement}

A matrix polynomial $\mathbf{Q}\in\RX^{n\times n}$ is said to be a \textbf{sum of squares}, if there exist $k\in\N$ and $\mathbf{Q}_1,\ldots,\mathbf{Q}_k\in \RX^n$, such that
\[
	\mathbf{Q}(z)=\sum\limits_{i=1}^k \mathbf{Q}_i(z)\,\mathbf{Q}_i(z)^t.
\]
We write $\mathbf{Q}\in\mathrm{SOS}(\RX^n)$ and denote by
\[
	\mathrm{QM}(\posmat)
:=	\{ q+\trace(\posmat\,\mathbf{Q})\,\vert\, q \in \mathrm{SOS}(\RX),\,\mathbf{Q} \in \mathrm{SOS}(\RX^n) \}
\]
the quadratic module of $\posmat$. Then every element of $\mathrm{QM}(\posmat)$ is nonnegative on $\Image$ and enforcing this constraint gives a lower bound
\begin{equation}\label{OptiProblemPositivity}
		f^* 
=		\begin{array}[t]{rl}
		\max		&	\lambda	\\
		\mbox{s.t.}	&	\lambda\in\R ,\, \forall\, z\in \Image: \, f(z) - \lambda \geq 0
		\end{array}
\geq	f_{\mathrm{sos}}
:=		\begin{array}[t]{rl}
		\sup		&	\lambda	\\
		\mbox{s.t.}	&	\lambda\in\R ,\, f - \lambda \in \mathrm{QM}(\posmat)
		\end{array}
.
\end{equation}

\subsubsection{Moment relaxation}

A linear functional $\functional\in\RX^*$ is said to have a \textbf{representing probability measure on $\Image$}, if there exists a probability measure $\eta$ on $\R^n$ with support in $\Image$, such that, for all $p \in \RX$, $\int_\Image p(z) \, \mathrm{d} \eta(z) = \functional(p)$. Such a functional is nonnegative on $\mathrm{QM}(\posmat)$ and relaxing to this constraint gives another lower bound
\begin{equation}\label{eq_momentrewrite}
		f^*
=		\begin{array}[t]{rl}
		\min		&	\functional(f) \\
		\mbox{s.t.}	&	\functional\in\RX^* \mbox{ has a representing} \\
					&	\mbox{probability measure on } \Image
		\end{array}
\geq	f_{\mathrm{mom}}
:=		\begin{array}[t]{rl}
		\inf		&	\functional(f)	\\
		\mbox{s.t.}	&	\functional\in\RX^*,\,\functional(1)=1,\\
					&	\forall f\in \mathrm{QM}(\posmat):\,\functional (f) \geq 0
		\end{array}
.
\end{equation}

We have $f_{\mathrm{sos}} \leq f_{\mathrm{mom}}$. Indeed, if $\functional$ is feasible for $f_{\mathrm{mom}}$ and $\lambda$ is feasible for $f_{\mathrm{sos}}$, then
\begin{equation}\label{eq_DualityGap}
	\functional(f) - \lambda
=	\functional(\underbrace{f - \lambda}_{\in\mathrm{QM}(\posmat)})
\geq 0.
\end{equation}
We say that $\mathrm{QM}(\posmat)$ is \textbf{Archimedean}, if there exists $p\in \mathrm{QM}(\posmat)$, such that $\{z\in\R^n\,\vert\,p(z)\geq 0\}$ is compact. 

\begin{theorem}\label{thm_holscherer}
	\emph{\cite{holscherer05,holscherer06}} Assume that $\mathrm{QM}(\posmat)$ is Archimedean.
	\begin{enumerate}
		\item Let $p\in\RX$. If $p>0$ on $\Image$, then $p\in \mathrm{QM}(\posmat)$.
		\item Let $\functional\in\RX^*$. If $\functional\geq 0$ on $\mathrm{QM}(\posmat)$, then $\functional$ has a representing probability measure on $\Image$.
		\item Equality holds in \Cref{OptiProblemPositivity} and \Cref{eq_momentrewrite}.
	\end{enumerate}
\end{theorem}

\begin{remark}
In our case, the Archimedean property can be enforced by adding an explicitly known ball constraint. Indeed, for $z\in\Image$, we have $n \geq \norm{z}^2$, and thus $\Image = \{z\in\R^n\,\vert\,\widehat{\posmat}(z)\succeq 0\}$, where $\widehat{\posmat}:=\mathrm{diag}(\posmat,n - \norm{z}^2)\in\RX^{(n+1) \times (n+1)}$. With $\mathbf{Q} = \mathrm{diag}(0,\ldots,0,1)\in\mathrm{SOS}(\RX^{n+1})$, we have $\trace(\widehat{\posmat}\,\mathbf{Q})\in \mathrm{QM}(\widehat{\posmat})$ and the set $\{z\in\R^n\,\vert\,\trace(\widehat{\posmat}(z)\,\mathbf{Q}(z))\}$ is compact.
\end{remark}

\subsection{Lasserre hierarchy with Chebyshev polynomials}

The conditions $f-\lambda \in \mathrm{QM}(\posmat)$ from \Cref{OptiProblemPositivity} and $\functional\geq 0$ on $\mathrm{QM}(\posmat)$ from \Cref{eq_momentrewrite} can be parametrized through positive semi--definite conditions, but for computations we need to restrict to finite dimensional subspaces of $\RX$. We shall now introduce these conditions in the basis of generalized Chebyshev polynomials and then adapt Lasserre's hierarchy \cite{lasserre01} to approximate the optimal value $f^*$ with semi--definite programs \cite{boyd96}. In particular, we present these positive semi--definite conditions in the way they are implemented in our Maple package\footnote{\href{https://github.com/TobiasMetzlaff/GeneralizedChebyshev}{https://github.com/TobiasMetzlaff/GeneralizedChebyshev}}.

\subsubsection{Chebyshev filtration}

For $\functional \in\RX^*$, we define the infinite symmetric matrix $\momm^\functional := \functional(\mathbf{T} \, \mathbf{T}^t)$, where $\functional$ applies entry--wise and $\mathbf{T}$ is the vector of basis elements $T_\weight$ with $\weight\in\Weights^+$.

Then we can also define the $\posmat$--localized matrix $\momm^{\posmat * \functional} := \functional(\posmat\otimes (\mathbf{T} \, \mathbf{T}^t))$. Here, $\functional$ applies entry--wise and $\otimes$ denotes the Kronecker product. The entries of this infinite matrix, indexed by $\weight,\nu\in\Weights^+$, are symmetric blocks of size $n$.

As in \cite{henrion06}, we see that $\functional\geq 0$ on $\mathrm{QM}(\posmat)$ is equivalent to $\momm^{\functional} \succeq 0$ and $\momm^{\posmat * \functional} \succeq 0$. By \Cref{eq_TPolyRecurrence}, for $\weight,\nu\in\Weights^+$, the entries of $\momm^{\functional}$ are
\begin{equation}\label{eq_momentmatrix}
\momm^{\functional}_{\weight\,\nu}
=	\functional (T_\weight\, T_\nu )
=	\dfrac{1}{\nops{\weyl}} \sum\limits_{A\in\weyl} \functional(T_{A\,\weight + \nu}) \in \R.
\end{equation}
Furthermore, let us assume that the matrix $\posmat$ in \Cref{eq_polyoptprob} is represented in the Chebyshev basis as
\[
\posmat(z) = \sum\limits_{\gamma\in\Weights^+} \posmat_\gamma \, T_\gamma(z) \in\RX^{n\times n} 
\]
with $\posmat_\gamma\in\R^{n\times n}$. The entries of $\momm^{\posmat * \functional}$ are
\begin{equation}\label{eq_localizedmomentmatrix}
\momm^{\posmat * \functional}_{\weight\,\nu}
=	\sum\limits_{\gamma\in\Weights^+} \posmat_\gamma \,\functional(T_\weight\, T_\nu\,T_\gamma )
=	\dfrac{1}{\nops{\weyl}^2} \sum\limits_{\gamma \in \Weights^+} \posmat_\gamma \sum\limits_{A,B\in\weyl} \functional(T_{A\weight+B\nu+\gamma})
\in	\R^{n\times n}.
\end{equation}
Restricting $\functional$ to a finite dimensional subspace of $\RX$ in \Cref{eq_momentrewrite} means to truncate the matrices $\momm^{\functional}$ and $\momm^{\posmat * \functional}$ at the corresponding rows and columns. However, since we have chosen the Chebyshev polynomials as a basis, we need to ensure that these matrices are well--defined: For an index of the form $A\,\weight + \nu$ in \Cref{eq_momentmatrix}, there is a unique dominant weight in the same $\weyl$--orbit, say $\tilde{\weight}$, and $\functional$ must be defined on $T_{\tilde{\weight}}$, so that we can compute the matrix entries of $\momm^{\functional}$ (and analogously for $\momm^{\posmat * \functional}$).

\begin{proposition}\label{thm_FiltrationDegree}
Let $\Roots$ be an irreducible root system with highest root $\highestroot$. For $d \in \N$, we define the finite dimensional $\R$--vector subspace
\[
\filt{d}
:=	\langle \{ T_\weight \,\vert\, \weight\in\Weights^+,\,\sprod{\weight,\,\highestroot^\vee} \leq d \}\rangle_\R
\]
of $\RX$. Then $(\filt{d})_{d\in\N}$ is a filtration of $\RX$ as an $\R$--algebra, that is,
\begin{enumerate}
	\item $\RX=\bigcup\limits_{d\in\N}\filt{d}$ and
	\item $\filt{d_1}\,\filt{d_2} \subseteq \filt{d_1 + d_2}$ whenever $d_1,d_2\in\N$.
\end{enumerate}
\end{proposition}
\begin{proof}
\emph{1.} Let $p=\sum_\weight c_\weight\,T_\weight\in\RX$ and choose $d\in\N$ with $d\geq \sprod{\weight,\highestroot^\vee}$ whenever $c_\weight\neq 0$. Then $p\in\filt{d}$.
	
\emph{2.} Let $T_\weight\in\filt{d_1}$ and $T_\nu\in\filt{d_2}$. Then $\nops{\weyl}\,T_\weight\,T_\nu=\sum_A T_{\weight+A\nu}$. For all $A\in\weyl$, there exists $B\in\weyl$, such that $B(\weight+A\nu)\in\Weights^+$. By \cite[Chapitre VI, \S 1, Proposition 18]{bourbaki456}, $\weight - B \weight$ and $\nu - B A \nu$ are sums of positive roots. Hence, there exists $\alpha\in \N^n$, such that
\[
\sprod{B (\weight + A \nu),\highestroot^\vee}
=		\sprod{\weight + \nu,\highestroot^\vee} - \sum\limits_{i=1}^n \alpha_i \sprod{\roots_i,\highestroot^\vee}.
\]
By \cite[Chapitre VI, \S 1.8, Proposition 25]{bourbaki456}, we have $\highestroot^\vee \in \overline{\PC}$ and thus $\sprod{\roots_i,\highestroot^\vee}\geq 0$. We obtain
\[
\sprod{B ( \weight + A \nu),\highestroot^\vee}
\leq	\sprod{\weight + \nu,\highestroot^\vee}
\leq	d_1 + d_2.
\]
Therefore, $T_\weight\,T_\nu\in\filt{d_1+d_2}$.
\end{proof}

\begin{remark}
For irreducible root systems, the filtration induces a weighted degree on $\RX$. Otherwise, we can always construct a filtration by choosing an order on the irreducible components. From now on, we may therefore assume all root systems to be irreducible.
\end{remark}

\subsubsection{Modified Lasserre hierarchy}

When $\functional$ is only defined on $\filt{2d}$, that is, $\functional\in\filt{2d}^*$, then the matrix $\momm^\functional$ is by \Cref{thm_FiltrationDegree} well--defined for all rows and columns up to weighted degree $d$. We denote this truncated matrix of size $\dim(\filt{d})$ by $\momm^\functional_d$. Analogously, for
\[
d
\geq D
:=	\,\min \{ \lceil \ell/2 \rceil \,\vert\,\ell\in\N,\, \posmat \in (\filt{\ell})^{n\times n} \},
\]
the truncated $\posmat$--localized matrix $\momm^{\posmat * \functional}_{d-D}$ is well-defined and of size $n\,\dim(\filt{d-D})$.

On the other hand, if $\mathbf{Q}_1,\ldots,\mathbf{Q}_k\in\filt{d}^n$ are polynomial vectors with entries of weighted degree at most $d$, then the polynomial matrix $\mathbf{Q}=\sum_i \mathbf{Q}_i\,\mathbf{Q}_i^t\in\filt{2d}^{n\times n}$ is a sum of squares. We write $\mathbf{Q}\in\mathrm{SOS}(\filt{d}^n)$ and see that the truncated quadratic module
\[
	\mathrm{QM}(\posmat)_d
:=	\{ q+\trace(\posmat\,\mathbf{Q})\,\vert\, q \in \mathrm{SOS}(\filt{d}) ,\,\mathbf{Q} \in \mathrm{SOS}(\filt{d-D}^n) \}
\]
is contained in $\filt{2d}$. We fix a \textbf{hierarchy order} $d\in\N$, that has to satisfy
\begin{equation}\label{eq_MinimalOrderOfRelaxation}
d \geq \max\{ \min\{\lceil  \ell/2 \rceil \,\vert\,\ell\in\N,\, f\in\filt{\ell} \} ,\, D \} ,
\end{equation}
where $f$ is the objective function from \Cref{eq_polyoptprob}. The \textbf{Chebyshev moment and SOS hierarchy of order $d$} is
\begin{equation}\label{OptiProblemMomRelax}
	f_{\mathrm{mom}}^d
:=	\begin{array}[t]{rl}
	\inf		&	\functional(f) \\
	\mbox{s.t.}	&	\functional\in\filt{2 d}^*,\,\functional(1)=1, \\
				&	\momm^\functional_d,\,\momm^{p*\functional}_{d-D} \succeq 0
\end{array}
\tbox{and}
	f_{\mathrm{sos}}^d 
:=	\begin{array}[t]{rl}
	\sup		&	\lambda \\
	\mbox{s.t.}	&	\lambda\in\R,	\\
				&	f - \lambda \in \mathrm{QM}(\posmat)_d
	\end{array}
.
\end{equation}

\begin{theorem}\label{thm_SosHierarchy}
The following statements hold.
\begin{enumerate}
\item The sequences $(f_{\mathrm{sos}}^d)_{d\in\N}$ and $(f_{\mathrm{mom}}^d)_{d\in\N}$ are monotonously non--decreasing.
\item For $d\in\N$, we have $f_{\mathrm{sos}}^d \leq f_{\mathrm{mom}}^d$.
\item If $\mathrm{QM}(\posmat)$ is Archimedean, then $\lim\limits_{d \to \infty} f_{\mathrm{sos}}^d = \lim\limits_{d \to \infty} f_{\mathrm{mom}}^d = f^*$.
\end{enumerate}
\end{theorem}
\begin{proof}
\emph{1.} follows from the chain of inclusions $\filt{1}\subseteq\filt{2}\subseteq\ldots$

\emph{2.} is analogous to \Cref{eq_DualityGap}.

\emph{3.} By \Cref{thm_holscherer}, for any $\varepsilon > 0$, there exist sums of squares $q$ and $\mathbf{Q}$, such that
\[
	f - f^* + \varepsilon
=	q + \trace(\posmat\,\mathbf{Q}).
\]
Since $\varepsilon$ is arbitrary and $\bigcup\limits_{d\in\N}\filt{d}=\RX$, we obtain $\lim\limits_{d \to \infty} f_{\mathrm{sos}}^d = f^*$. With \emph{2.}, the same holds for $f_{\mathrm{mom}}^d$.
\end{proof}

\subsubsection{SDP formulation}

We translate \Cref{OptiProblemMomRelax} to a semi--definite program (SDP), so that the problem can be implemented and a solution be approximated with solvers such as \textsc{Mosek}\footnote{{O}ptimizer {API} for {P}ython 3 \href{docs.mosek.com/latest/pythonapi/index.html}{docs.mosek.com/latest/pythonapi/index.html}}. For $d\in\N$ and a linear functional $\functional\in\filt{2d}^*$, we write
\begin{equation}\label{MomentMatrixCoeff}
	\begin{pmatrix}
	\momm^{\functional}_{d}	&	0	\\
	0						&	\momm^{\posmat*\functional}_{d-D}
	\end{pmatrix}
=	\sum\limits_{\weight\in \Weights^+} \functional(T_\weight) \, \mathbf{A}_\weight,
\end{equation}
where $\mathbf{A}_\weight$ is the symmetric matrix coefficient of $\functional(T_\weight)$. For $d\geq D$,  $\functional(T_\weight)$ is well--defined whenever $\mathbf{A}_\weight\neq 0$. We write $\mathrm{Sym}^{(d)} := \mathrm{Sym}^{\dim(\filt{d})} \times \mathrm{Sym}^{n\,\dim(\filt{d-D})}$ for the space of symmetric matrices with two blocks. The positive semi--definite elements are denoted by $\mathrm{Sym}^{(d)}_{\succeq 0}$ and we define the dual problems
\begin{equation}\label{OptiMomPrimalDualRelax}
\primalprob \begin{array}[t]{rl}
\inf		&	\sum\limits_{\weight \in S}
				c_\weight \, \mathbf{y}_\weight	\\
\mbox{s.t.}	&	\mathbf{y}\in\R^{\dim(\filt{2 d})} ,\, \mathbf{y}_0=1,	\\
			&	\mathbf{Z} = \sum\limits_{\weight\in\Weights^+} \mathbf{y}_\weight \, \mathbf{A}_\weight\in\mathrm{Sym}^N_{\succeq 0}
\end{array}
\tbox{and} \quad
\dualprob \begin{array}[t]{rl}
\sup		&	c_0 - \trace(\mathbf{A}_0\,\mathbf{X}) \phantom{\sum\limits_{S}} \\
\mbox{s.t.}	&	\mathbf{X}\in\mathrm{Sym}^{(d)}_{\succeq 0} ,\, \forall\,\weight\in S\setminus\{0\}:	\\
			&	\trace(\mathbf{A}_\weight\,\mathbf{X}) = c_\weight
\end{array}
.
\end{equation}

\begin{proposition}\label{prop_SDPDuality}
The optimal value of $\primalprob$ is $f^d_{\mathrm{mom}}$ and the optimal value of $\dualprob$ is $f^d_{\mathrm{sos}}$.
\end{proposition}
\begin{proof}
The statement for $\primalprob$ follows immediately with $\mathbf{y}_\weight=\functional(T_\weight)$ and $\mathbf{Z} = \mathrm{diag} (\momm^{\functional}_{d} , \momm^{\posmat*\functional}_{d-D})$. Let $\functional\in\filt{2d}^*$ and $\lambda \in \R$ be feasible for \Cref{OptiProblemMomRelax}. Then there exist $q\in\mathrm{SOS}(\filt{d})$ and $\mathbf{Q}\in \mathrm{SOS}(\filt{d-D}^n)$ with
\[
	\functional(f) - \lambda
=	\functional(f - \lambda)
=	\functional(q) + \functional(\trace(\posmat\,\mathbf{Q})).
\]
We construct a feasible matrix $\mathbf{X}=\mathrm{diag}(\mathbf{X}_1,\mathbf{X}_2)$ for $\dualprob$ as follows. Since $\mathbf{Q}$ is a sum of squares, we can write $\mathbf{Q} = \mathbf{Q}_1\,\mathbf{Q}_1^t + \ldots + \mathbf{Q}_k\,\mathbf{Q}_k^t$ and denote by $\mathbf{T}_{d-D}$ the vector of generalized Chebyshev polynomials $T_\weight \in \filt{d-D}$. For $1\leq i\leq k$, we have $\mathbf{Q}_i=\mathbf{mat}(\mathbf{Q}_i)\,\mathbf{T}_{d-D}$, where $\mathbf{mat}(\mathbf{Q}_i)$ is the coordinate matrix of the polynomial vector $\mathbf{Q}_i$ in the Chebyshev basis with $n$ rows and $\dim(\filt{d-D})$ columns. Then
\[
\begin{array}{rcl}
	\trace(\posmat\,\mathbf{Q})
&=&	\sum\limits_{i=1}^k \trace(\posmat \, \mathbf{mat}(\mathbf{Q}_i)\,\mathbf{T}_{d-D}\,\mathbf{T}_{d-D}^t \, \mathbf{mat}(\mathbf{Q}_i)^t)\\
&=&	\trace((\posmat \otimes \mathbf{T}_{d-D} \, \mathbf{T}_{d-D}^t) \underbrace{\sum\limits_{i=1}^k \mathbf{vec}(\mathbf{mat}(\mathbf{Q}_i)) \, \mathbf{vec}(\mathbf{mat}(\mathbf{Q}_i))^t}_{=:\mathbf{X}_2} ),
\end{array}
\]
where $\mathbf{vec}(\mathbf{mat}(\mathbf{Q}_i)):=((\mathbf{mat}(\mathbf{Q}_i)_{\cdot 1})^t,\ldots,(\mathbf{mat}(\mathbf{Q}_i)_{\cdot N_{d-D}})^t)^t$ are the stacked columns of $\mathbf{mat}(\mathbf{Q}_i)$. The matrix $\mathbf{X}_2$ is symmetric positive semi--definite of size $n\,\dim(\filt{d-D})$. By definition of the truncated localized moment matrix, we have $\functional(\trace(\posmat\,\mathbf{Q})) = \trace(\momm_{d-D}^{\posmat * \functional}\,\mathbf{X}_2)$. Analogously, there exists a symmetric positive semi--definite $\mathbf{X}_1$ of size $\dim(\filt{d})$ with $\functional(q) = \trace(\momm_{d}^{\functional}\,\mathbf{X}_1)$. When we fix $\mathbf{X}:=\mathrm{diag}(\mathbf{X}_1,\mathbf{X}_2)\in \mathrm{Sym}^{(d)}_{\succeq 0}$ and $\mathbf{A}_\weight$ as in \Cref{MomentMatrixCoeff}, comparing coefficients yields
\[
	\lambda
=	c_0 \, \functional(1) - \functional(q(0)) - \functional(\trace(\posmat(0)\,\mathbf{Q}(0)))
=	c_0 - \trace(\mathbf{A}_0\,\mathbf{X})
\]
and, for $\weight\neq 0$, we have $c_\weight = \trace(\mathbf{A}_\weight\,\mathbf{X})$.

Conversely, we can always construct sums of squares $q$ and $\mathbf{Q}$ from a matrix $\mathbf{X}=\mathrm{diag}(\mathbf{X}_1,\mathbf{X}_2)$ by writing $\mathbf{X}_1$ and $\mathbf{X}_2$ as sums of rank $1$ matrices.
\end{proof}

If $(\mathbf{X},\mathbf{y},\mathbf{Z})$ are optimal for $\primalprob$ and $\dualprob$, then the duality gap of the Chebyshev moment and SOS hierarchy in \Cref{OptiProblemMomRelax} is $f^d_{\mathrm{mom}} - f^d_{\mathrm{sos}} = \trace(\mathbf{X}\,\mathbf{Z}) \geq 0$.

\begin{remark}
The coefficients $c_\weight$ are known from the original problem in \emph{\Cref{eq_polyoptprob}}. The key in setting up \emph{\Cref{OptiMomPrimalDualRelax}} is the computation of the matrices $\mathbf{A}_\weight$. For fixed order $d$, we define
\begin{itemize}
\item the \textbf{matrix size} $N := \dim(\filt{d}) + n\,\dim(\filt{d-D})$ and
\item the \textbf{number of constraints} $m := \dim(\filt{2d}) - 1$.
\end{itemize}
Note that $m$ is the number of matrices $\mathbf{A}_\weight$ with $\weight \neq 0$ and $N$ is their size. Then \emph{\Cref{OptiMomPrimalDualRelax}} is a semi--definite program with primal formulation $\primalprob$ over the cone $\filt{2d}^*\cong\R^{m+1}$ with dual cone $0$ and with dual formulation $\dualprob$ over the self--dual cone $\mathrm{QM}(\posmat)_d \cong \mathrm{Sym}^{N}_{\succeq 0}$.

Computing the matrices $\mathbf{A}_\weight$ of the SDP involves the recurrence formula from \emph{\Cref{eq_TPolyRecurrence}} and is not numerical. If we used the standard monomial basis $\{1,z_1,z_2,\ldots,z_1^2,z_1 z_2,\ldots\}$, this computation would be trivial, but the matrices would be larger when truncating at the usual degree instead of the weighted degree. Hence, our technique is more efficient, if the numerical effort to solve a larger SDP in the standard monomial basis is bigger than the combined effort to numerically solve a smaller SDP in the Chebyshev basis plus matrix computation. Another upside is that, since the matrices $\mathbf{A}_\weight$ only depend on the root system $\Roots$ and the order $d$, but not on the objective function $f$, the same matrices can be used to solve several problems as a preprocess.

\begin{table}[H]
	\begin{center}
		\scalebox{0.65}{
			\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|}
				\hline
				$\Roots\backslash d$	&	$2$				&	$3$				&	$4$				&	$5$					&	$6$					&
				$7$				&	$8$				&	$9$				&	$10$				\\
				\hline
				\hline
				$\RootB[2],\RootC[2]$	&	$6+2,14$		&	$10+6,27$		&	$15		+12,44$	&	$21		+20,65$		&	$28	+30,90$			&
				$36+42,119$		&	$45+56,152$		&	$55	+72,189$	&	$66	+90,230$		\\
				\hline
				$\RootG[2]$				&	$-		$		&	$6+3,15$		&	$9	+6,24$		&	$12		+12,35$		&	$16	+18,48$			&
				$20+27,63$		&	$25+36,80$		&	$30		+48,99$	&	$36	+60,120$		\\
				\hline
				$\RootA[2]$				&	$-		$		&	$10+3,27$		&	$15		+9,44$	&	$21		+18,65$		&	$28	+30,90$			&
				$36+45,119$		&	$45+63,152$		&	$55	+84,189$	&	$66	+108,230$		\\
				\hline
				$\RootB[3]$				&	$-		$		&	$13+3,49$		&	$22	+9,94$		&	$34	+21,160$		&	$50	+39,251$		&
				$70+66,371$		&	$95+102,524$	&	$125+150,714$	&	$161	+210,945$	\\
				\hline
				$\RootC[3]$				&	$-		$		&	$20+3,83$		&	$35	+12,164$	&	$56	+30,285$		&	$84	+60,454$		&
				$120+105,679$	&	$165+168,968$	&	$220+252,1329$	&	$286	+360,1770$	\\
				\hline
				$\RootA[3]$				&	$-		$		&	$-		$		&	$35		+4,164$	&	$56	+16,285$		&	$84	+40,454$		&
				$120+80,679$	&	$165+140,968$	&	$220+224,1329$	&	$286	+336,1770$	\\
				\hline
				$\RootB[4]$				&	$-		$		&	$-		$		&	$30		+4,174$	&	$50	+12,335$		&	$80	+32,587$		&
				$120+64,959$	&	$175+120,1484$	&	$245+200,2199$	&	$336	+320,3145$	\\
				\hline
				$\RootC[4]$				&	$-		$		&	$-		$		&	$70+4,494$		&	$126	+20,1000$	&	$210	+60,1819$	&
				$330+140,3059$	&	$495+280,4844$	&	$715+504,7314$	&	$1001	+840,10625$	\\
				\hline
				$\RootD[4]$				&	$-		$		&	$-		$		&	$46+4,294$		&	$80	+16,580$		&	$130	+44,1035$	&
				$200+96,1715$	&	$295+184,2684$	&	$420+320,4014$	&	$581	+520,5785$	\\
				\hline
			\end{tabular}
		}
	\end{center}
	\caption{The SDP parameters $(N,m)$ for \Cref{OptiMomPrimalDualRelax} depend on the root system $\Roots$ and the order $d$.}
	\label{SDPMatrixNumberSizeTable}
\end{table}
\end{remark}

\subsection{Optimizing on coefficients}

For a finite set $S\subseteq\Weights^+\setminus\{0\}$ of dominant weights, we shall be confronted in \Cref{section_chromatic} with a bilevel optimization problem, where we not only have to minimize the objective function $f$ from \Cref{eq_ObjectivePoly} with respect to $z\in\Image$, but also maximize with respect to the coefficients $c_\weight$ under some compact affine constraints. The problem can be represented as
\[
	F(S)
:=	\begin{array}[t]{rl}
	\max\limits_{c} \min\limits_{z}	&	\sum\limits_{\weight \in S} c_\weight\,T_\weight(z) \\
	\mbox{s.t.}						&	z\in\Image ,\, c\in\R^{S} ,\, b^t\,c = 1 ,\\
									&	\ell_\weight \leq c_\weight \leq u_\weight \tbox{for} \weight	\in		S
\end{array}
,
\]
where $b\in\R^S$ and $\ell_\weight \leq u_\weight\in\R$. For scalar polynomial constraints defining $\Image$, a hierarchy of SDPs to approximate $F(S)$ was introduced in \cite[Chapter 13]{lasserre09}. With our polynomial matrix constraint, the theory is similar. For $d\in\N$ large enough, that is, $T_\weight\in\filt{2d}$ whenever $\weight\in S$, we define
\[
F(S,d)
:=	\begin{array}[t]{rl}
	\sup		&	- \trace(\mathbf{A}_0\,\mathbf{X}) \\
	\mbox{s.t.}	&	\mathbf{X} \in\mathrm{Sym}^{(d)}_{\succeq 0},\, \sum\limits_{\weight\in S} \alpha_\weight\,\trace(\mathbf{A}_\weight\,\mathbf{X}) = 1,	\\
	&	\ell_\weight \leq \trace(\mathbf{A}_\weight\,\mathbf{X}) \leq u_\weight	\tbox{for} \weight	\in		S,	\\
	&	\trace(\mathbf{A}_\nu\,\mathbf{X}) = 0													\tbox{for} \nu		\notin	S\cup\{0\}
\end{array}
,
\]
where the $\mathbf{A}_0,\mathbf{A}_\weight, \mathbf{A}_\nu \in \mathrm{Sym}^{(d)}$ are the $\dim(\filt{2d})$ many matrices defined via \Cref{MomentMatrixCoeff}.

\begin{theorem}\label{thm_MaxMinConvergence}
The sequence $(F(S,d))_{d\in\N}$ is monotonously non--decreasing. If $\mathrm{QM}(\posmat)$ is Archimedean, then $\lim\limits_{d\to\infty} F(S,d) = F(S)$.
\end{theorem}
\begin{proof}
The proof is analogous to the one of \cite[Theorem 13.1]{lasserre09}, but uses the Positivstellensatz of Hol and Scherer instead of Putinar's. Let $\mathbf{X}$ be optimal for $F(S,d)$ and set $c_\weight := \trace (\mathbf{A}_\weight\,\mathbf{X})$ for $\weight \in S$. Then $F(S,d) \leq (f_c)^* \leq F(S)$, where $(f_c)^*$ denotes the minimum of $f_c := \sum\limits_{\weight\in S} c_\weight\,T_\weight \in\RX$ on $\Image$.

On the other hand, $\Image=\{ z\in \R^n \,\vert\, \posmat(z) \succeq 0 \}$ is compact and the $T_\weight$ are continuous. Hence, the map $g:c \mapsto (f_c)^*$ is continuous on a compact set and there exists a feasible $c^* \in \R^S$, such that $F(S)=g(c^*)$. For any $\varepsilon > 0$, the polynomial $\sum_{\weight\in S} c^*_\weight \, T_\weight - F(S) +\varepsilon$ is strictly positive on $\Image$. Thus, by \Cref{thm_holscherer}, there exist sums of squares $q\in\mathrm{SOS}(\RX)$ and $\mathbf{Q}\in\mathrm{SOS}(\RX^n)$, such that
\[
\sum_{\weight\in S} c^*_\weight \, T_\weight - (F(S) - \varepsilon)
=	q + \trace(\posmat \, \mathbf{Q}).
\]
For $d\in\N$ sufficiently large, we can follow our proof of \Cref{prop_SDPDuality} to construct a matrix $\mathbf{X} \in \mathrm{Sym}^{(d)}_{\succeq 0}$ with $- \trace(\mathbf{A}_0\,\mathbf{X})=c^*_0$, $- \trace(\mathbf{A}_0\,\mathbf{X})=F(S)-\varepsilon$, $\trace(\mathbf{A}_\weight\,\mathbf{X})=c^*_\weight$ for $\weight\in S$ and $\trace(\mathbf{A}_\nu\,\mathbf{X}) = 0$ for $0\neq \nu\notin S$. Then $\mathbf{X}$ is feasible for $F(S,d)$, and therefore $F(S,d) \geq F(S) - \varepsilon$. Since $\varepsilon > 0$ is arbitrary, the statement follows.
\end{proof}

\subsection{A case study}
\label{sec_casestudy}

We apply the Chebyshev moment and SOS hierarchy to solve a trigonometric polynomial optimization problem with crystallographic symmetry and compare with another technique. One alternative approach is to reinforce positivity constraints on trigonometric polynomials to \textbf{sums of Hermititan squares} (SOHS), which goes back to the generalized Riesz--F\'{e}jer theorem \cite[Theorem 4.11]{dumitrescu07}. With \cite[Equation (3.71)]{dumitrescu07}, one can then approximate the minimum of a trigonometric polynomial $f\in\R[\Weights]$ by solving a semi--definite program
\begin{equation}\label{eq_DumitrescuRelaxation}
f_{\mathrm{rf}}^S
=
\begin{array}[t]{rl}
\sup		&	\lambda	\\
\mbox{s.t.}	&	f-\lambda\in\mathrm{SOHS}(S)
\end{array}
\end{equation}
as in Riesz--Fej\'{e}r, where $S\subseteq \Weights$ is a finite set of exponents containing the support of $f$ up to central symmetry. This can be translated into SDP standard form with Kronecker products of elementary Toeplitz matrices, yielding a hierarchy of lower bounds.

\begin{example}\label{example_DumitrescuComparison}
We search the global minima $f^*$, $g^*$, $h^*$ and $k^*$ of the following $\weyl$--invariant trigonometric polynomials with graphs depicted in \emph{\Cref{FigureDumitrescu}}.
\begin{enumerate}
\item Let $\Roots=\RootG[2]$, $\weyl=\mathfrak{S}_3 \ltimes \{\pm 1 \}$, $\Weights = \Z\,\fweight{1}\oplus\Z\,\fweight{2} = \Z [0,-1,1]^t \oplus \Z [-1,-1,2]^t$ and
\begin{align*}
	f(u)
:=&	\,\gencos{2\,\fweight{1}}(u) + 2\,\gencos{\fweight{2}}(u)\\
=&	\,(
	\mcos{\sprod{2\,\fweight{1},u}} + 
	\mcos{\sprod{2\,\fweight{1} - 2\,\fweight{2},u}} + 
	\mcos{\sprod{4\,\fweight{1} - 2\,\fweight{2},u}} \\
&	\, + 
	2\,\mcos{\sprod{\fweight{2},u}} + 
	2\,\mcos{\sprod{3\,\fweight{1} - \fweight{2},u}} + 
	2\,\mcos{\sprod{3\,\fweight{1} - 2\,\fweight{2},u}} 
	)/3.
\end{align*}
In the coordinates $z=\gencos{}(u)$, we have $f(z) = 6\,z_1^2 - 2\,z_1 - 1$ $($see \emph{\Cref{example_A2PolyRewrite}}$)$.

\item Let $\Roots=\RootG[2]$, $\weyl=\mathfrak{S}_3 \ltimes \{\pm 1 \}$, $\Weights = \Z\,\fweight{1}\oplus\Z\,\fweight{2} = \Z [0,-1,1]^t \oplus \Z [-1,-1,2]^t$ and
\begin{align*}
	g(u)
:=&	\,2\,\gencos{\fweight{1}}(u) + \gencos{\fweight{2}}(u) + \gencos{\fweight{1} + \fweight{2}}(u) + 4\,\gencos{3\,\fweight{1}}(u)\\
=&	\,(
	2\,\mcos{\sprod{\fweight{1},u}} + 
	2\,\mcos{\sprod{\fweight{1}-\fweight{2},u}} + 
	2\,\mcos{\sprod{2\,\fweight{1}-\fweight{2},u}}\\
	&\,+ 
	\mcos{\sprod{\fweight{2},u}} + 
	\mcos{\sprod{3\,\fweight{1}-2\,\fweight{2},u}} + 
	\mcos{\sprod{3\,\fweight{1}-\fweight{2},u}}\\
	&\,+ 
	4\,\mcos{\sprod{3\,\fweight{1},u}} + 
	4\,\mcos{\sprod{3\,\fweight{1}-3\,\fweight{2},u}} + 
	4\,\mcos{\sprod{6\,\fweight{1}-3\,\fweight{2},u}}
	)/3\\
	&\,+ (
	\mcos{\sprod{\fweight{1},u} + \sprod{\fweight{2},u}} + 
	\mcos{\sprod{\fweight{1}-2\,\fweight{2},u}} + 
	\mcos{\sprod{4\,\fweight{1}-\fweight{2},u}} \\
	&\,+\mcos{\sprod{4\,\fweight{1}-3\,\fweight{2},u}} + 
	\mcos{\sprod{5\,\fweight{1}-2\,\fweight{2},u}} + 
	\mcos{\sprod{5\,\fweight{1}-3\,\fweight{2},u}}
	)/6.
\end{align*}
In the coordinates $z=\gencos{}(u)$, we have $g(z) = 144\,z_1^3 - 6\,z_1^2 - 69\,z_1\,z_2 - 33\,z_1 - 21\,z_2 - 7$.

\item Let $\Roots=\RootC[2]$, $\weyl=\mathfrak{S}_2 \ltimes \{\pm 1 \}^2$, $\Weights = \Z\,\fweight{1}\oplus\Z\,\fweight{2} = \Z [1,0]^t \oplus \Z [1,1]^t$ and
\begin{align*}
	h(u)
:=&	\,2\,\gencos{\fweight{1}}(u) + \gencos{\fweight{2}}(u) - \gencos{2\,\fweight{2}}(u) - 3\,\gencos{\fweight{1}+\fweight{2}}(u)\\
=&	\,\mcos{\sprod{\fweight{1},u}} + 
	\mcos{\sprod{\fweight{1}-\fweight{2},u}} \\
&	\,+(
	\mcos{\sprod{\fweight{2},u}} + 
	\mcos{\sprod{2\,\fweight{1}-\fweight{2},u}} - 
	\mcos{\sprod{2\,\fweight{2},u}} - 
	\mcos{\sprod{4\,\fweight{1}-2\,\fweight{2},u}}
	)/2 \\
&	\,-3/4\,(
	\mcos{\sprod{\fweight{1}-2\,\fweight{2},u}} + 
	\mcos{\sprod{\fweight{1}+\fweight{2},u}} \\
&	\,+ \mcos{\sprod{3\,\fweight{1}-2\,\fweight{2},u}} + 
	\mcos{\sprod{3\,\fweight{1}-\fweight{2},u}}
	).
\end{align*}
In the coordinates $z=\gencos{}(u)$, we have $h(z) = 8\,z_1^2 - 6\,z_1\,z_2 - 4\,z_2^2 + 5\,z_1 - 3\,z_2 - 1$.

\item Let $\Roots=\RootC[2]$, $\weyl=\mathfrak{S}_2 \ltimes \{\pm 1 \}^2$, $\Weights = \Z\,\fweight{1}\oplus\Z\,\fweight{2} = \Z [1,0]^t \oplus \Z [1,1]^t$ and
\begin{align*}
	k(u)
:=&	\,2\,\gencos{2\fweight{1}}(u) + \gencos{2\,\fweight{2}}(u)\\
=&	\,\mcos{\sprod{2\,\fweight{1},u}} + \mcos{\sprod{2\,\fweight{1}-2\,\fweight{2},u}} + \mcos{\sprod{2\,\fweight{2},u}}/2 + \mcos{\sprod{4\,\fweight{1}-2\,\fweight{2},u}}/2 \\
&	
\end{align*}
In the coordinates $z=\gencos{}(u)$, we have $k(z) = 4\,z_2^2 - 1$.
\end{enumerate}

For $3 \leq d \leq 7$, we choose $\tilde{S}$ to be the set of all dominant weights $\weight \in \Weights^+$ with $\Cheblvl{T_{\weight}} \leq d$. In \emph{\Cref{eq_DumitrescuRelaxation}}, $S=(\tilde{S}-\tilde{S})\cap (H\setminus \{0\})$ is an admissible choice for any halfspace $H$, since $S$ contains all exponents of the objective functions up to central symmetry. In this case, we denote the optimal value by $f^d_{\mathrm{rf}}$. On the other hand, we apply the Chebyshev SOS reinforcement $f^d_{\mathrm{sos}}$ from \emph{\Cref{OptiProblemMomRelax}}, where we only need to take exponents up to Weyl group symmetry, that is, $\tilde{S}$ itself.

With the two techniques, we obtain the results in \emph{\Cref{TableDumitrescuComparison}}. $N$ denotes the matrix size and $m$ the number of constraints, depending on $d$. In practice, it is usually not possible to determine the exact minimal value. However, since we compare lower bounds, it suffices to check which bound is larger and therefore closer to the actual minimum. To solve the semi--definite programs, we rely on \emph{\textsc{Mosek}}.

\begin{figure}[H]
\begin{center}
\begin{subfigure}{.45\textwidth}
	\includegraphics[height=5.1cm]{ToeplitzCirclesG2.png}
	\caption{The graph of $f$ for $u=(u_1,u_2,-u_1-u_2)$.}
\end{subfigure}
\quad
\begin{subfigure}{.45\textwidth}
	\includegraphics[height=5.1cm]{ToeplitzSpikesG2.png}
	\caption{The graph of $g$ for $u=(u_1,u_2,-u_1-u_2)$.}
\end{subfigure}
\quad
\begin{subfigure}{.45\textwidth}
	\includegraphics[height=5.1cm]{ToeplitzRifflesC2.png}
	\caption{The graph of $h$ for $u=(u_1,u_2)$.}
\end{subfigure}
\quad
\begin{subfigure}{.45\textwidth}
	\includegraphics[height=5.1cm]{ToeplitzSquaresC2.png}
	\caption{The graph of $k$ for $u=(u_1,u_2)$.}
\end{subfigure}
\caption{The graphs of the objective functions for $u \in \R^3 / [1,1,1]^t \cong \R^2$.}
\label{FigureDumitrescu}
\end{center}
\end{figure}

\begin{table}[H]
\begin{center}
\scalebox{0.8}{
\begin{tabular}{|c||c|c|c|c|c|}
\hline
$d$						&	$3$			&	$4$				&	$5$
						&	$6$			&	$7$					\\
\hline
\hline
$f^d_{\mathrm{rf}}$		&	$-1.18824$	&	$-1.180240$		&	$-1.17058$
&	$-1.16970$	&	$-1.16719$			\\
$N,m$				&	$49,33$		&	$81,58$			&	$121,90$
&	$169,129$	&	$225,175$		\\
\hline
$f^d_{\mathrm{sos}}$	&	$-1.16667$	&	$-1.16667$		&	$-1.16667$
&	$-1.16667$	&	$-1.16667$			\\
$N,m$					&	$9,15$		&	$15,24$			&	$24,35$
&	$34,48$		&	$47,63$				\\
\hline
\hline
$g^d_{\mathrm{rf}}$		&	$-3.50118$	&	$-3.40372$		&	$-3.31195$
						&	$-3.25383$	&	$-3.22049$			\\
$N,m$				&	$49,33$		&	$81,58$			&	$121,90$
						&	$169,129$	&	$225,175$		\\
\hline
$g^d_{\mathrm{sos}}$	&	$-3.20499$	&	$-3.10220$		&	$-2.98718$
						&	$-2.98718$	&	$-2.98718$			\\
$N,m$					&	$9,15$		&	$15,24$			&	$24,35$
						&	$34,48$		&	$47,63$				\\
\hline
\hline
$h^d_{\mathrm{rf}}$		&	$-2.12159$	&	$-2.10672$		&	$-2.1012$
						&	$-2.09959$	&	$-2.09073$			\\
$N,m$					&	$25,24$		&	$49,54$			&	$81,96$
						&	$121,150$	&	$169,217$		\\
\hline
$h^d_{\mathrm{sos}}$	&	$-2.27496$	&	$-2.06250$		&	$-2.06250$
						&	$-2.06250$	&	$-2.06250$			\\
$N,m$					&	$16,27$		&	$27,44$			&	$41,65$
						&	$58,90$		&	$78,119$			\\
\hline
\hline
$k^d_{\mathrm{rf}}$		&	$-1.00000$	&	$-1.00000$		&	$-1.00000$
						&	$-1.00000$	&	$-1.00000$			\\
$N,m$					&	$25,84$		&	$41,144$		&	$61,220$
						&	$85,312$	&	$113,420$		\\
\hline
$k^d_{\mathrm{sos}}$	&	$-1.00000$	&	$-1.00000$		&	$-1.00000$
						&	$-1.00000$	&	$-1.00000$			\\
$N,m$					&	$16,27$		&	$27,44$			&	$41,65$
						&	$58,90$		&	$78,119$			\\
\hline
\end{tabular}
}
\end{center}
\caption{Comparison of the two techniques in terms of approximation and SDP parameters $(N,m)$.}
\label{TableDumitrescuComparison}
\end{table}
\end{example}

\begin{remark}
In \emph{\Cref{TableDumitrescuComparison}}, we observe $f^*\geq f^d_{\mathrm{sos}} \geq f^d_{\mathrm{rf}}$ for $d\geq 4$. Hence, our approximation of $f^*$ appears to be better in those cases, while the parameters $N,m$ that indicate the size of the SDP are smaller $($analogous for $g,h,k)$. Differences in the quality of the approximation might depend on the stability of the SDP \emph{\cite{parrilo22}}.
\end{remark}