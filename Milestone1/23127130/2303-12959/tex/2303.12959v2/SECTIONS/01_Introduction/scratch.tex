Unsupervised learning for sensing the properties of objects is crucial to reduce the gap between humans and machines intelligence. Inline with human intelligence disentanglement learning is considered to be a promising direction to obtain explanatory representations from observations to understand and reason objects without any supervision.

In the recent years, various approaches have been proposed to successfully extract basic properties of objects, such as position, color, orientation, and scale. The commonly-used methods are based on variational autoencoder (VAE). In particular, beta-VAE introduced an extra parameter Î² on the Kullback-Leibler (KL) divergence to promote disentanglement. However, there is a trade-off between disentanglement and reconstruction fidelity on beta-VAE, which is a problem to be solved in the following works.

Revision:
Unsupervised learning is essential for bridging the gap between human and machine intelligence. Disentanglement learning is a promising approach for obtaining explanatory representations from observations without supervision, mimicking human intelligence. Variational autoencoders (VAEs) are widely used for disentanglement learning, with methods like beta-VAE introducing an extra parameter to promote disentanglement. However, there is a trade-off between disentanglement and reconstruction fidelity in beta-VAE.



One common direction for dealing with the trade-off is to penalize the Total Correlation (TC) between latent variables, avoiding reducing the mutual information, such as FactorVAE, beta-TC-VAE, and DIPVAE. As pointed out, TC-based VAEs have a strong prior assumption that the factors are statistically independent. Beyond that, when it comes to high-dimension latent space, the estimation of TC becomes inaccurate due to curse of dimensionality, as our experiments observed in Section 3. The realistic problems usually have numerous factors, therefore it would need a large model with high latent space to extract representations. However, in this work, we get rid of calculating TC by leveraging the narrow information bottleneck to find efficient codes for representing the data, which promotes disentanglement.

Revision:
To address this trade-off, some methods penalize the Total Correlation (TC) between latent variables. However, these methods have strong prior assumptions and can suffer from the curse of dimensionality. In this work, we propose an alternative approach that leverages a narrow information bottleneck to promote disentanglement without calculating TC.



In the meanwhile, previous information bottleneck (IB)-based methods have tried to solve the obstacle of trade-off between disentanglement and reconstruction fidelity. In general, they first set a high pressure with a narrow IB to encourage disentanglement and then expand the IB gradually to promote reconstruction fidelity under a latent space, termed incremental methods. However, they lost the constraint of disentanglement when expanding the IB, which causes the information diffusion problem. In this work, to avoid information diffusion, we aim to optimize reconstruction while keeping the constraint of disentanglement.

Different from IB-Incremental based approaches listed above, our key motivation is to optimize disentanglement and reconstruction simultaneously. Previous methods spread the targets of disentanglement and reconstruction over time and optimize only one target at a time. To optimize both two targets, we spread targets over spaces by creating multiple latent spaces. In this way, each latent layer has its own objective to optimize disentanglement or reconstruction. Furthermore, our framework constrains these latent spaces to share disentanglement so that the first latent space achieves simultaneous optimization of disentanglement and reconstruction.

To achieve this, we propose a simple yet effective VAE framework composed of multiple continuous latent sub-spaces with a novel IB-Decremental strategy and a disentanglement-invariant transform operator, which we call DeVAE. Specifically, we decrease the information bottleneck of each latent space layer by layer, where we constrain the first space for informativeness to recover the input image, and other disentangled spaces for learning factors of the image by narrow IBs. Furthermore, we introduce the disentanglement-invariant transform operator to ensure simultaneous optimization of disentanglement across continuous latent sub-spaces, which avoids the information diffusion. Our decremental model avoids ID by keeping the constraints of disentanglement and reconstruction simultaneously. Furthermore, DeVAE is capable of large models with high dimensional space. We also conducted comprehensive comparisons with popular methods quantitatively and qualitatively.

Revision:
Our key motivation is to optimize disentanglement and reconstruction simultaneously by creating multiple latent spaces. Each latent layer has its own objective, and our framework constrains these spaces to share disentanglement. This approach allows for simultaneous optimization of disentanglement and reconstruction in the first latent space.

We introduce DeVAE, a VAE framework with multiple continuous latent sub-spaces, a novel IB-Decremental strategy, and a disentanglement-invariant transform operator. DeVAE decreases the information bottleneck layer by layer, constrains the first space for informativeness, and learns factors in other disentangled spaces using narrow IBs. The disentanglement-invariant transform operator ensures simultaneous optimization across latent sub-spaces, avoiding information diffusion. Our model is suitable for large models with high-dimensional space and outperforms popular methods in quantitative and qualitative comparisons.
