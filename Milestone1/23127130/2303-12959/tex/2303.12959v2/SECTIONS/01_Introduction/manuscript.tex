\section{Introduction}
% background
Unsupervised learning~\citep{wu2018unsupervised} is essential for bridging the gap between human and machine intelligence. 
Disentanglement learning is a promising approach for obtaining explanatory representations from observations without supervision, mimicking human intelligence~\citep{Bengio.2013}. 
Variational autoencoders (VAEs)~\cite{Kingma.2013} are widely used for disentanglement learning, with methods like beta-VAE~\cite{Higgins2017betavae} introducing a penalty (weighted by $\beta$) on the Kullbackâ€“Leibler (KL) divergence to promote disentanglement. However, there is a trade-off between disentanglement and reconstruction fidelity in beta-VAE.


To address this trade-off, some methods utilize a dynamic controlling strategy for $\beta$~\cite{Burgess.2018,shao2022rethinking,DBLP:journals/ml/WuDEFT22}. Generally, a high initial $\beta$ value is set to enforce VAEs disentangle at the beginning. Then, the value of $\beta$ is gradually reduced to facilitate reconstruction. Since $\beta$ controls the Information Bottleneck (IB)~\cite{Tishby.1999,Burgess.2018}, these methods are called \textit{incremental VAEs}, where the IB increases during training. As a result, incremental VAEs achieve a good balance by optimizing disentanglement and reconstruction in separate time spans.


In this work, we propose an alternative approach to address the conflict between optimizing disentanglement and reconstruction. Our primary motivation is to optimize disentanglement and reconstruction simultaneously by creating multiple latent spaces. Each latent space focuses on different tasks, either optimizing disentanglement or reconstruction, while our framework ensures these spaces share disentanglement properties. This approach enables simultaneous optimization of both disentanglement and reconstruction.

Specifically, we introduce DeVAE, a VAE framework with hierarchical latent spaces (HiS) that applies a novel IB-decremental strategy and a disentanglement-invariant transform (DiT) operator. DeVAE gradually decreases the information bottleneck across latent spaces, constrains the first space for reconstruction, and learns factors in subsequent spaces using narrow IBs. The disentanglement-invariant transform operator guarantees that the learned disentangled representation is shared among the latent spaces.

Our contributions can be summarized as follows:
\begin{itemize}
    \item We introduce a novel framework, DeVAE, which employs hierarchical latent spaces with decreasing information bottlenecks across the spaces, offering a new approach to balance disentanglement and reconstruction fidelity.
    \item We develop the disentanglement-invariant transformation, a key innovation that connects hierarchical latent spaces and enabling the sharing of disentanglement properties among them while maintaining a high level of reconstruction performance.
    \item We conduct comprehensive experiments and ablation studies on benchmark datasets, i.e.  dSprites and Shapes3D, demonstrating the effectiveness of DeVAE in achieving a balance between disentanglement and reconstruction.
\end{itemize}
