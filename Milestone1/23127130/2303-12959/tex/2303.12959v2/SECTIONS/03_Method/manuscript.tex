\section{Methodology}

\subsection{Preliminaries}
% Towards a Definition of Disentangled Representations

\paragraph{Problem Setup \& Notations.}

Disentanglement learning aims to learn the factors of variation which raises the change of observations.
Given a set of samples $\vect{x} \in \mathcal{X}$, they can be uniquely described by a set of ground-truth factors $\vect{c}\in \mathcal{C}$.
Generally, the generation process $g(\cdot)$ is invisible $\vect{x} = g(\vect{c})$.
We say that a representation for factor $\vect{c}_i$ is disentangled if it is invariant for the samples with $\vect{c}_j$.
We use variational inference to learn the disentangled representation for a given problem.
$p(\vect{z}|\vect{x})$ denotes the probability of $\vect{z}=f(\vect{x})$, $p(\vect{x}|\vect{z})$ denotes the probability of $\vect{x} = g(\vect{z})$.
The representation function is a conditional Bayesian network of the form $q_\phi(\vect{z}|\vect{x})$ to estimate $p(\vect{z}|\vect{x})$.
The generative model is another network of the form $p_\theta(\vect{x}|\vect{z})p(\vect{z})$.
$\phi,\theta$ are trainable parameters.

\input{figs/model}

\paragraph{Revisit VAE \& \betavae{}.}

The VAE framework~\citep{Kingma.2013} computes the representation function by introducing $q_\phi(\vect{z}|\vect{x})$ and optimizing the variational lower bound (ELBO).
\betavae{}~\citep{Higgins2017betavae} introduces the hyperparameter $\beta$ to control the IB:
\begin{equation}\label{eq:vae}
    \begin{aligned}
        \mathcal{L}(\theta, \phi) =  \E_{q_\phi(\vect{z}|\vect{x})}[\log{p_\theta (\vect{x}|\vect{z})}]
        - \beta \  D_{\mathrm{KL}}(q_\phi(\vect{x}|\vect{z}) || p(\vect{z})).
    \end{aligned}
\end{equation}

Consider using \betavae{} to learn a representation of the data; the representation will be disentangled but lose information when $\beta$ is large~\citep{Burgess.2018}.
We can set a large $\beta$ to learn a disentangled representation and a small $\beta$ to learn an informative representation. However, \betavae{} suffers a trade-off between disentanglement and reconstruction, which means that $\beta$ can only optimize one of these two goals.


\subsection{Hierarchical Latent Spaces with Decremental Information Bottleneck}\label{sec:dib}

To maintain the disentanglement constraint while optimizing reconstruction fidelity, we introduce a Hierarchical Latent Space (HiS) with $K$ spaces and assign a pressure $\beta_i$ to the $i$-th space $\mathcal{Z}_i$. Each space promotes disentanglement or reconstruction through a suitable value of $\beta$. The objective of the $i$-th space is given by:
\begin{equation}
    \begin{aligned}
        \mathcal{L}_i(\theta, \phi) =  \E_{q_\phi(\vect{z}_i|\vect{x})}[\log{p_\theta (\vect{x}|\vect{z}_i,\vect{v}_i)}]
        - \beta_i D_{\mathrm{KL}}(q_\phi(\vect{z}_i|\vect{x}) || p(\vect{z})),
    \end{aligned}
\end{equation}
where the first space $q_\phi(\vect{z}_0|\vect{x})$ is a conditional Bayesian network, $v_i$ denotes a $K$-D vector to indicate the index of space, and the subsequent spaces can be calculated by:
\begin{equation}\label{eq:latent}
    \begin{aligned}
        q(\vect{z}_{i+1}|\vect{x}) = \tau_i (\vect{z}_{i+1}|\vect{z}_{i}) q(\vect{z}_{i}|\vect{x}), i\neq0,
    \end{aligned}
\end{equation}
where $\tau_i$ denotes a transformation from $\mathcal{Z}_i$ to $\mathcal{Z}_{i+1}$.

According to information theory, information can only decrease during processing. Therefore, we gradually decrease the IB in the sequential spaces, i.e., $\beta_{i+1}>\beta_i$. Typically, we set $\beta_0=1$ to encourage the first space to focus on reconstructing the original inputs. In this way, sequential spaces aim to disentangle factors of variation by setting narrow bottlenecks.

\subsection{Disentanglement-invariant Transformation}\label{sec:dit}
In this part, we discuss the transformation $\tau_i$ which is vital to optimizing disentanglement and reconstruction simultaneously. If the transformation is arbitrary, the spaces will optimize their goal independently. Therefore, we need a mechanism to connect these goals to balance disentanglement and reconstruction in one space. 
To share disentanglement across all latent spaces, we propose a disentanglement-invariant transformation (DiT) denoted as $\tau$:
\begin{equation}
    \vect{\mu}_{i+1} = h(\vect{w}^1_i) \vect{\mu}_i, \quad 
    \vect{\sigma}_{i+1} = h(\vect{w}^2_i) \vect{\sigma}_i,
    \label{eq:tau}
\end{equation}
where $\vect{z}_{i}\sim \mathcal{N}( \vect{\mu}_i, \vect{\sigma}_i)$, $\vect{w}^1_i, \vect{w}^2_i$ are learnable diagonal matrices of the $i$-th space, $h(\vect{w})=e^{\vect{w}} >0$ is an exponential function to make sure the scale values greater than 0.

We prove that scaling the latent space will not change disentanglement in Theorem 1, see proof in Appendix~\ref{sec:proof}.
\begin{theorem}
    $\vect{w} \cdot \vect{z}$ is disentangled if $\vect{z}$ is disentangled, $\vect{w}$ is a diagonal matrix.
\end{theorem}

\subsection{Optimization Algorithm}

According to Equation~\ref{eq:tau}, we derive the parameters of latent variables for $i$-th space:
\begin{equation}
    \begin{aligned}
        \vect{\mu}_i = h(\sum_{j=0}^{{i-1}} w_j^1) \vect{\mu}_0, \quad
        \vect{\sigma}_i = h(\sum_{j=0}^{{i-1}} w_j^2) \vect{\sigma}_0,\quad
        i>0.
    \end{aligned}
\end{equation}

Applying the chain law, we get the $i$-th KL divergence:
\begin{equation}
    D_{{\mathrm{KL}}_i} = \frac{1}{2}(1+
    2\sum_{j=0}^{{i-1}} w_j^2 + 2\log(\vect{\sigma}_0)
    - h(2\sum_{j=0}^{{i-1}} w_j^2) \vect{\sigma}_0^2
    - h(2\sum_{j=0}^{{i-1}} w_j^1) \vect{\mu}_0^2 )
\end{equation}

The final objective of DeVAE is:
\begin{equation}\label{eq:devae}
 \mathcal{L}(\theta, \phi) = \sum_{i=0}^{K-1} \E_{q_\phi(\vect{z}_i|\vect{x})}[\log{p_\theta (\vect{x}|\vect{z}_i,\vect{v}_i)}]
        -  \sum_{i=0}^{K-1}  \beta_i D_{{\mathrm{KL}}_i}.
\end{equation}

In this work, we aim to prove the validity of the proposed HiS with DiT for optimizing disentanglement and reconstruction simultaneously in different latent spaces. The algorithm of our method is shown in Algorithm~1. Figure~\ref{fig:model} illustrates the architecture of DeVAE with two spaces. We set $K=2$ for simplicity, and we find it is effective in practice. The main space applies $\beta_0=1$ to work as a vanilla VAE. We set a high value of $\beta_1$, adjusting according to problems, to encourage disentanglement.

% \lstinputlisting[language=Python,caption=PyTorch-like implementation of DeVAE loss.,
%     float=tp,floatplacement=tbp]{code.py}

\begin{algorithm}
\caption{DeVAE: Hierarchical Latent Spaces with Decremental Information Bottleneck}
\begin{algorithmic}[1]
\Require Data $\mathcal{D}=\{\vect{x}_n\}_{n=1}^N$, epochs $T$, learning rate $\eta$, pressure parameters $\beta_0=1, \beta_1$
\State Initialize the encoder and decoder networks $\phi$ and $\theta$
\For{$t=1$ to $T$}
    \For{each $\vect{x}$ in $\mathcal{D}$}
        \State Compute $\vect{\mu}_0, \vect{\sigma}_0$ using the encoder network $q_\phi(\vect{z}_0|\vect{x})$
        \State Sample $\vect{z}_0 \sim \mathcal{N}(\vect{\mu}_0, \vect{\sigma}_0)$
        \State Compute $\vect{\mu}_{i+1} = h(\vect{w}^1_i) \vect{\mu}_i,
    \vect{\sigma}_{i+1} = h(\vect{w}^2_i) \vect{\sigma}_i,$ using DiT
        \State Sample $\vect{z}_1 \sim \mathcal{N}(\vect{\mu}_1, \vect{\sigma}_1)$
        \State Compute reconstruction loss $\mathcal{L}_{rec} = \sum_{i=0}^{1} \E_{q_\phi(\vect{z}_i|\vect{x})}[\log{p_\theta (\vect{x}|\vect{z}_i,\vect{v}_i)}]$
        \State Compute KL divergence losses $D_{{\mathrm{KL}}_0}$ and $D_{{\mathrm{KL}}_1}$
        \State Compute total loss $\mathcal{L}(\theta, \phi) = \mathcal{L}_{rec} - \beta_0 D_{{\mathrm{KL}}_0} - \beta_1 D_{{\mathrm{KL}}_1}$
        \State Update $\phi$ and $\theta$ using gradient descent with learning rate $\eta$
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}
