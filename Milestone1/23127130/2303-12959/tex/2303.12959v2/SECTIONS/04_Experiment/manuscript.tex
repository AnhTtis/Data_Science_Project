
\section{Experiments}

\subsection{Experimental Setup}

\paragraph{Datasets.}
The experiment section assesses the proposed DeVAE method on two widely-used datasets, dSprites~\citep{dsprites17} and Shapes3D~\citep{3dshapes18}.  
dSprites has 737,280 binary 64 × 64 x 1 images generated from five factors: shape (3), orientation (40), scale (6), position X (32), and position Y (32).
Shapes3D has 480,000 RGB 64 × 64 × 3 images of 3D shapes generated from six factors: floor color (10), wall color (10), object color (10), object size (8), object shape (4), and azimuth (15).

\paragraph{Evaluation Metrics.}
To evaluate the performance of disentanglement, three disentanglement metrics are applied.
\textbf{MIG}~\citep{Chen2018betatcvae}: the mutual information gap between two variables with the highest and the second-highest mutual information.
\textbf{FactorVAE metric}~\citep{Kim2018factorvae}: the error rate of the classifier, which predicts the latent variable with the lowest variance.
\textbf{DCI Dis.}: abbreviation for DCI Disentanglement~\citep{Eastwood.2018}, a matrix of relative importance by regression.
\textbf{Recon.}: abbreviation for Reconstruction Error. We use Squared Error for RGB images (Shapes3D) and Binary Cross Entropy for binary images (dSprites).


\paragraph{Implementation.}
We use a convolutional neural network as the encoder and a deconvolutional neural network as the decoder. 
Detailed architecture can be found in Appendix~\ref{sec:arch}.
The activation function is ReLU\@.
The optimizer is Adam~\citep{DBLP:journals/corr/KingmaB14} with a learning rate of $1e^{-4}$, \(\beta_1 = 0.9,\ \beta_2 = 0.999\).
We employed a large batch size of 256 to accelerate the training process.
All experiments train 300, 000 iterations by default.
% \input{tabs/architecture.tex}
For the hyper-parameters, we set $\beta=12$ for  \btcvae{}, $\beta=6$ for \betavae{}, and $K_i=0.001,K_p=0.01$ for DynamicVAE, and $\{\beta_i\}=[1,40]$ for DeVAE.
We set $\beta_0 = 1$ to reconstruct image details and set $\beta_1 = 40$ to filter hard factors (shape, orientation) according to DEFT~\citep{DBLP:journals/ml/WuDEFT22}.


\subsection{Comparison to Prior Work}

To demonstrate the effectiveness of the proposed DeVAE, we compare it to three typical disentanglement methods:
1) $\beta$-VAE~\citep{Higgins2017betavae}: the baseline model for disentanglement and also the special case of DeVAE when $\beta_0=\beta_1$;
2) $\beta$-TCVAE~\citep{Chen2018betatcvae}: the SOTA method for penalizing TC;
3) Dynamic-VAE~\citep{shao2022rethinking}: the SOTA method for incremental VAEs.

% \input{figs/quantitative.tex}
\begin{table}[t]

\begin{tabular}{@{}lccccc@{}}
% \toprule
dataset  & model      & MIG & DCI dis. & FactorVAE & Recon. \\ \midrule
dSprites & DeVAE      & 0.34$\pm$ 0.02   & 0.53$\pm$ 0.02     & 0.80$\pm$ 0.03    & 48.31$\pm$ 27.98        \\
 & DynamicVAE & 0.35$\pm$ 0.01   & 0.53$\pm$ 0.01              & 0.82$\pm$ 0.05   & 19.25$\pm$ 1.85         \\
 & $\beta$-TCVAE(12.0)  & 0.29$\pm$ 0.09   & 0.47$\pm$ 0.08     & 0.73$\pm$ 0.08    & 73.04$\pm$ 3.41         \\
 & $\beta$-VAE(6.0)   & 0.17$\pm$ 0.05   & 0.30$\pm$ 0.07     & 0.74$\pm$ 0.05    & 48.75$\pm$ 2.84         \\
\midrule
shapes3D & DeVAE      & 0.53$\pm$ 0.11   & 0.71$\pm$ 0.02     & 0.79$\pm$ 0.02    & 46.81$\pm$ 13.97        \\
 & DynamicVAE & 0.54$\pm$ 0.04   & 0.68$\pm$ 0.03          & 0.87$\pm$ 0.10             & 31.02$\pm$ 3.56         \\
 & $\beta$-TCVAE(12.0)  & 0.49$\pm$ 0.11   & 0.73$\pm$ 0.07     & 0.78$\pm$ 0.01    & 44.53$\pm$ 5.69         \\
 & $\beta$-VAE(6.0)   & 0.42$\pm$ 0.18   & 0.68$\pm$ 0.06     & 0.82$\pm$ 0.06    & 34.95$\pm$ 2.34         \\ 
\bottomrule
\end{tabular}
\vspace{3mm}
\caption{Quantitative benchmarks on dSprites and shapes3D.}\label{tab:quantitative}
\end{table}

\paragraph[]{Disentanglement \& Reconstruction.}\label{sec:benchmark}

In comparison to prior work, DeVAE demonstrates effectiveness in achieving a balance between disentanglement and reconstruction. 
We conducted experiments on dSprites and Shapes3D where each trail was repeated 10 times with different random seeds and evaluated by MIG, FactorVAE, DCI disentanglement, and reconstruction error. We expect higher values for these metrics except recon. 
On the dSprites dataset, DeVAE achieves an average improvement of 12\% in disentanglement compared to $\beta$-TCVAE and 38\% compared to $\beta$-VAE. Furthermore, the reconstruction error is only half of that in $\beta$-TCVAE. The reconstruction drop on shapes3D is not that kind of large, because we use l2 loss instead of Bernoulli loss. DeVAE gains remarkable disentanglement with accepable reconstruction drop. Overall, DeVAE is competitive with Dynamic-VAE and surpasses both $\beta$-TCVAE and $\beta$-VAE. 


\input{figs/traversal_dpsrites.tex}
\paragraph{Qualitative Visualization.} 
Qualitative analysis is conducted to assess disentanglement by visualizing latent traversals~\cite{Higgins.2018} as shown in Figure~\ref{fig:traversal}. 
Specifically, each row reveals the reconstruction images from one dimension of the latent space  systematically varied from -2 to 2 while keeping the others fixed. For each variation, the decoder of the VAE generates new images with three random seeds.
We choose the top5 dimensions with the highest KL divergence to visualize their latent traversals.
DeVAE successfully disentangles position X and position Y by latent 4 and 5.
The hard factors, shape, scale, and orientation, are still a challenge in this domain.
More examples can be found in the Appendix~\ref{sec:visualization}.

\input{figs/information_diffusion.tex}



\paragraph{Preventing Information Diffusion.}\label{sec:ID}

Information diffusion is a phenomenon where one factor's information diffuses into other latent variables during training, leading to fluctuations in disentanglement scores~\cite{DBLP:journals/ml/WuDEFT22}. 
We argue that our framework can solve the problem effectively due to removing the dynamic controlling strategy.
Figure~\ref{fig:ID} demonstrates the changes in mutual information for the latent variable with the highest KL during training.
NMI refers to the normalized mutual information, calculating the mutual information between one latent variable and one factor divided by the maximum information.
The results show that Dynamic-VAE loses information significantly at iteration 3e5, indicating that the learned structure of representation is destroyed when expanding the information bottleneck (IB). On the other hand, DeVAE demonstrates a relatively steady trend of increasing information, thanks to consistent regularization. DeVAE overcomes the drawbacks of traditional IB-based methods by maintaining the constraint of disentanglement.


% \input{figs/scale.tex}
% \paragraph[]{Scaling to High-dimensional Latent Space.}\label{sec:large}

% Most disentanglement methods evaluate their performance on simple scenes with only one object and few factors.
% It is a challenge to extend these methods to complex scenes.
% However, whether these methods adapt to a large latent space to fit more factors is questionable.
% In particular, the dimension of latent space affects the estimation accuracy of MI for the TC-based methods.

% To study the effect of high-dimensional latent space on estimating TC, we first generate samples from a D-dimensional multi-variable normal distribution $\vect{x}$ which is divided into two groups $\vect{x}^1$ and $\vect{x}^2$ with D/2 dimensions.
% The variables in a group are independent $\Cov(\vect{x}^m_i,\vect{x}^m_j)=0, i \neq j$; the variables between groups are correlative $\Cov(\vect{x}^m_i,\vect{x}^n_i)=\rho, m \neq n$; each variable is a standard normal distribution. % $\Cov(\vect{x}^m_i,\vect{x}^m_i)=1$.
% So, the TC of $\vect{x}$ is
% \begin{equation}
%     \mathrm{TC}(\vect{x}) = - \frac{\mathrm{D}}{4} \log(1-\rho^2).
% \end{equation}



% % \begin{table}[h]
% % \begin{minipage}[t]{0.45\linewidth}
% We trained a discriminator for 2000 iterations to estimate the TC introduced in FactorVAE~\citep{Kim2018factorvae}.
% We compared the estimated TC and the real TC over dimensions and $\rho$.
% Each trail was repeated 10 times, and we report the average results as shown in Table~\ref{tab:TC}.
% One can see that increasing $\rho$ or dimension diminishes the accuracy of estimation, and the estimators always have low errors when the dimension is 10.
% However, the estimation becomes extremely inaccurate when the dimension raises to 1000, which means such estimation will fail to penalize the TC for large models.

% \input{tabs/TC.tex}

% We further conduct experiments on dSprites to validate the above conclusion.
% The experimental settings are the same except for increasing the dimension of latent space to 1024.
% \fix{From Figure~\ref{fig:scale}, we can see that \btcvae{} and DynamicVAE have significant performance decay.
% \betavae{} and DeVAE are resistant to increasing the dimension of latent space.
% Higher dimensional space increases the complexity of calculating the TC and leads to significant estimation errors.}
% Our model show robustness in high-dimensional latent spaces.