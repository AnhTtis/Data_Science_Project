
\section{Conclusion}
In this paper, we propose a novel framework featuring hierarchical latent spaces, where the information bottleneck decreases across spaces. 
These latent spaces are connected through disentanglement-invariant transformations which are the key components to sharing disentanglement among the spaces.
Unlike incremental methods that optimize disentanglement and reconstruction in separate time spans, our work offers insights into optimizing these objectives simultaneously in hierarchical latent spaces. As an original contribution, we have demonstrated how to decouple the two goals, disentanglement and reconstruction, into different latent spaces.

%Future work, disentangle-invariant transformation
\paragraph{Limitation.}
One limitation of the hierarchical latent spaces is the degradation of reconstruction, which occurs because these spaces are connected and share certain properties, such as disentanglement. Therefore, it is highly desirable to develop a better transformation between latent spaces that results in lower degradation. Future research could focus on improving this aspect of the model to further enhance the balance between disentanglement and reconstruction performance.
