

\section{Appendix}

\subsection{Architecture}\label{sec:arch}
\input{tabs/architecture.tex}
We use symmetric convolutional networks for encoders and decoders as shown in Table~\ref{tab:architecture}. $c=1$ for dSprites, and $c=3$ for Shapes3D. All layers are activated by ReLU. The final layer of encoder generates 10 variables for \textit{mean} and 10 variables for the \textit{logvar}. 


\subsection{Disentanglement-invariant Representations}
\label{sec:proof}
In this section, we prove the proposed disentanglement-invariant transformation.
Consider that we have a new representation by multiplying a diagonal matrix: $\vect{z}' = \vect{w} \vect{z}$, $\vect{w}$.
We can calculate the Covariance between any two latent variables:
\begin{equation}
\begin{aligned}
\operatorname{Cov}(\vect{w}_i \vect{z}_i, \vect{w}_j \vect{z}_j) &=
\mathbb{E}[(\vect{w}_i \vect{z}_i-\mathbb{E}[\vect{w}_i \vect{z}_i])(\vect{w}_j \vect{z}_j-\mathbb{E}[\vect{w}_j \vect{z}_j])] \\
&=\vect{w}_i \vect{w}_j (\mathbb{E}[\vect{z}_j]-\mathbb{E}[ \vect{z}_i] \mathbb{E}[\vect{z}_j]) \\
&=\vect{w}_i \vect{w}_j \operatorname{Cov}(\vect{z}_i,\vect{z}_j),
\end{aligned}
\end{equation}
where the subscript denotes the index of latent variables.
Note that we use a different notion in this section to simplify the formula.

Then we can get the correlation coefficient by
\begin{equation}
\begin{aligned}
    \rho(\vect{w}_i \vect{z}_i, \vect{w}_j \vect{z}_j)&=\frac{\operatorname{Cov}(\vect{w}_i \vect{z}_i, \vect{w}_j \vect{z}_j)}{\sqrt{\operatorname{Var}[\vect{w}_i \vect{z}_i] \operatorname{Var}[\vect{w}_j \vect{z}_j]}}\\
    &= \rho( \vect{z}_i, \vect{z}_j).
\end{aligned}
\end{equation}

Therefore, the correlation matrix will not change by multiplying a diagonal matrix $w, w\neq0$.
The proposed transformation is disentanglement-invariant.

\subsection{ Estimation of $I(\vect{z}_j;\vect{c}_i)$}


Given an inference network $q(\vect{z}|\vect{x})$, we use the Markov chain Monte Carlo (MCMC) method to get samples from $q(\vect{z})$ by the formula $q(\vect{z}) = q(\vect{z|x})p(\vect{x})$.
We use 10, 000 points to estimate $q(\vect{z})$.
Then, we discretize these latent variables by a histogram with 20 bins. 
After discretizing one latent variable, we call a discrete mutual information estimation algorithm to calculate $I(\vect{w}_j \vect{z}_j;\vect{c}_i)$ by a 2D histogram.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{pics/SamplingFromNoise.pdf}
    \caption{Reconstruction from noise.}
    \label{fig:sampling}
\end{figure}

\subsection{Visualization}\label{sec:visualization}


\input{figs/traversal.tex}

\noindent{}\textbf{Latent Traversal.} \quad
We compare DeVAE to others with latent traversals on Shapes3D and dSprites.
Each column shows the generated images by traversing one latent variable from -2 to 2.
From Figure~\ref{fig:traversal_shapes} and Figure~\ref{fig:traversal_dsprites}, we can see that DeVAE has a lower entanglement level. Note that only DeVAE disentangles object size isolated on Shapes3D.


\noindent{}\textbf{Random Sampling.} \quad
We random sample noise from Guassian distribution $\mathcal{N}(0,1)$ and generate images from our disentanglement model trained on dSprites.
As shown in Figure~\ref{fig:sampling}, our model, generating heart, has a high reconstruction fidelity

% \input{figs/quantitative.tex}
% \textbf{}



\subsection{CelebA}

We further conduct experiments on a real dataset CelebA~\cite{liu2015faceattributes}.
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
