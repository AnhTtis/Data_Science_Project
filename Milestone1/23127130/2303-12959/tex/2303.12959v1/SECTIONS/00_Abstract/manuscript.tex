\begin{abstract}
One major challenge of disentanglement learning with variational autoencoders is the trade-off between disentanglement and reconstruction fidelity.
\fix{R1Q1confusing}{Previous incremental methods with only on latent space cannot optimize these two targets simultaneously, so they expand the Information Bottleneck while training to {optimize from disentanglement to reconstruction}. 
However, a large bottleneck will lose the constraint of disentanglement, causing the information diffusion problem.}
To tackle this issue, we present a novel decremental variational autoencoder with disentanglement-invariant transformations to \fix{R1Q1confusing}{optimize multiple objectives in different layers}, termed DeVAE, for balancing disentanglement and reconstruction fidelity by decreasing the information bottleneck of diverse latent spaces gradually. 
% advantages
Benefiting from the multiple latent spaces, DeVAE allows simultaneous optimization of multiple objectives to optimize reconstruction while keeping the constraint of disentanglement, avoiding information diffusion.
DeVAE is also compatible with large models with high-dimension latent space.
Experimental results on dSprites and Shapes3D that DeVAE achieves \fix{R2q6}{a good balance between disentanglement and reconstruction.
DeVAE shows high tolerant of hyperparameters and on high-dimensional latent spaces.
}
\end{abstract}