\section{Experiments}

\subsection{Experimental Setup}


\paragraph{Datasets.}

We evaluate our method on two widely-used datasets (dSprites, Shapes3D). 
dSprites~\citep{dsprites17} has 737,280 binary 64 × 64 x 1 images generated from five factors: shape (5), orientation (40), scale (6), position X (32), and position Y (32).
Shapes3D~\citep{3dshapes18} has 480,000 RGB 64 × 64 × 3 images of 3D shapes generated from six factors: floor color (10), wall color (10), object color (10), object size (8), object shape (4), and azimuth (15).

\paragraph{Evaluation Metrics.}
We apply the following metrics to evaluate the performance of disentanglement and reconstruction.
\textbf{MIG}~\citep{Chen2018betatcvae}: the mutual information gap between two variables with the highest and the second-highest mutual information.
\textbf{FactorVAE metric}~\citep{Kim2018factorvae}: the error rate of the classifier, which predicts the latent variable with the lowest variance.
\textbf{DCI Dis.}: abbreviation for DCI Disentanglement~\citep{Eastwood.2018}, a matrix of relative importance by regression.
\textbf{Recon.}: abbreviation for Reconstruction Error, a measure of the distance between images; we use Mean Squared Error for RGB images and Binary Cross Entropy for binary images.


\paragraph{Implementation.}
We use a convolutional neural network as the encoder and a deconvolutional neural network as the decoder. 
Detailed architecture can be found in Appendix~\ref{sec:arch}.
The activation function is ReLU\@.
The optimizer is Adam~\citep{DBLP:journals/corr/KingmaB14} with a learning rate of 1e-4, \(\beta_1 = 0.9,\ \beta_2 = 0.999\).
The batch size is 256, which accelerates the training process.
All experiments train 300, 000 iterations by default.
For the hyperparameters, we set $\beta=12$ for  \btcvae{}, $\beta=6$ for \betavae{}, and $K_i=0.001,K_p=0.01$ for DynamicVAE.
\fix{R1Q4}{
According to the information freezing points (IFP)~\citep{DBLP:journals/ml/WuDEFT22}, beta=40 can filter factors orientation and shape, beta=10 can only filter factor orientation, so we set $\{\beta_i\}=[1,10,40], s=1$ for DeVAE.}

\input{figs/quantitative.tex}


\subsection{Comparison to Prior work}

To demonstrate the effectiveness of the proposed DeVAE, we compare it to previous all types of baselines:
1) $\beta$-VAE~\citep{Higgins2017betavae}: the popular method for disentanglement and also the baseline model for DeVAE when there is only one latent layer;
2) $\beta$-TCVAE~\citep{Chen2018betatcvae}: the TC-based method with a good balance of simplicity and effectiveness;
3) Dynamic-VAE~\citep{shao2022rethinking}: the latest method with incremental information bottleneck.


\paragraph[]{Disentanglement \& Reconstruction.}\label{sec:benchmark}

We conducted experiments on dSprites and Shapes3D to compare the above methods.
Each trail was repeated 10 times with different random seeds.
We draw the distributions of three disentanglement scores and reconstruction errors in Figure~\ref{fig:quantitative}.
\fix{R3Q3}{The visualization of sampling from the best models is shown in Appendix~\ref{sec:samples}.}
Experimental results reveal that DeVAE achieves an average improvement of 8\% comparing to \btcvae{} and 47\% to \betavae{} on dSprites for disentanglement.
\fix{R1Q1}{DeVAE has a lower average reconstruction error on two datasets by 2\% for \btcvae{} and by 30\% for \betavae{}. Though the improvement is not significant, \btcvae{} and \betavae{} are not robust to one hyperparameter setting.}
\fix{R1Q2}{Though DynamicVAE achieves the best overall results, it still suffers from ID problems and is incapable of dealing with high-dimensional space, see Figure~\ref{fig:scale} and Appendix~\ref{sec:high_dimension}.}

\input{figs/traversal_dpsrites.tex}
\paragraph{Qualitative Visualization.} 
We also conducted a qualitative analysis to assess disentanglement.
We show the selected latent traversals whose KL divergence is larger than 0.5 in Figure~\ref{fig:traversal}.
We can see that DeVAE disentangles position X and position Y perfectly.
Shape, scale, and orientation are hard to be disentangled.
We show the latent traversals of the best models with the highest MIG in Appendix~\ref{sec:latent_traversal}.

\input{figs/information_diffusion.tex}

\input{figs/scale.tex}


\paragraph[]{Preventing Information Diffusion.}\label{sec:ID}

Information diffusion is a phenomenon of disentangling that one factor's information diffuses into other latent variables while training, causing the disentanglement scores to fluctuate during training~\citep{DBLP:journals/ml/WuDEFT22}.
We hypothesize that losing the constraint of disentanglement is the reason for ID.

To prove it, we monitored the changes in mutual information during training.
From Figure~\ref{fig:ID}, we see that DynamicVAE has a significant trend of losing information on iteration 3e5.
It means that the learned structure of representation was destroyed when expanding the IB. 
In contrast, DeVAE shows a relatively steady trend of increasing information for consistent regularizing.
DeVAE overcomes the drawback of traditional information bottleneck-based methods by keeping the constraint of disentanglement.



\paragraph[]{Scaling to High-dimensional Latent Space.}\label{sec:large}

Most disentanglement methods evaluate their performance on simple scenes with only one object and few factors.
It is a challenge to extend these methods to complex scenes.
However, whether these methods adapt to a large latent space to fit more factors is questionable.
In particular, the dimension of latent space affects the estimation accuracy of MI for the TC-based methods.

To study the effect of high-dimensional latent space on estimating TC, we first generate samples from a D-dimensional multi-variable normal distribution $\vect{x}$ which is divided into two groups $\vect{x}^1$ and $\vect{x}^2$ with D/2 dimensions.
The variables in a group are independent $\Cov(\vect{x}^m_i,\vect{x}^m_j)=0, i \neq j$; the variables between groups are correlative $\Cov(\vect{x}^m_i,\vect{x}^n_i)=\rho, m \neq n$; each variable is a standard normal distribution. % $\Cov(\vect{x}^m_i,\vect{x}^m_i)=1$.
So, the TC of $\vect{x}$ is
\begin{equation}
    \mathrm{TC}(\vect{x}) = - \frac{\mathrm{D}}{4} \log(1-\rho^2).
\end{equation}


% \begin{table}[h]
% \begin{minipage}[t]{0.45\linewidth}
We trained a discriminator for 2000 iterations to estimate the TC introduced in FactorVAE~\citep{Kim2018factorvae}.
We compared the estimated TC and the real TC over dimensions and $\rho$.
Each trail was repeated 10 times, and we report the average results as shown in Table~\ref{tab:TC}.
One can see that increasing $\rho$ or dimension diminishes the accuracy of estimation, and the estimators always have low errors when the dimension is 10.
However, the estimation becomes extremely inaccurate when the dimension raises to 1000, which means such estimation will fail to penalize the TC for large models.

\input{tabs/TC.tex}

We further conduct experiments on dSprites to validate the above conclusion.
The experimental settings are the same except for increasing the dimension of latent space to 1024.
\fix{R1Q3,R3Q6}{From Figure~\ref{fig:scale}, we can see that \btcvae{} and DynamicVAE have significant performance decay.
Higher dimensional space increases the complexity of calculating the TC and leads to significant estimation errors and also increases the chance of the ID problem for DynamicVAE see in Appendix~\ref{sec:high_dimension}.}
\betavae{} and DeVAE show robustness in high-dimensional latent spaces, which is necessary to train a large model on large data. 