\subsection{Experimental Analysis}
\label{sec:ablation}


In this section, we performed ablation studies on the benefit of the proposed Hierarchical Latent Spaces (HiS) and Disentanglement-invariant Transformation (DiT).
We also conducted extensive experiments to explore the effect of $\beta$ and $s$ on disentanglement and reconstruction performance.


\paragraph{Hierarchical Latent Spaces \& Disentanglement-invariant Transformation.}
To demonstrate the effectiveness of the introduced Hierarchy Latent Spaces (HiS) and Disentanglement-invariant Transformation (DiT), we performed ablation experiments on the following situations:
1) The model has one single latent space;
2) The model applies a parallel structure instead of the hierarchy that latent spaces are independent;
3) We replace DiT with Linear Transformation ($\tau_i(\vect{z_{i}})=w \vect{z_{i}})$, $w$ is an arbitrary matrix;
4) The proposed model DeVAE.

We report the MIG and Recon. for each layer in Table~\ref{tab:ab_component}.
\fix{R2Q3}{MS and HiS can optimize multiple objectives for these layers separately. 
DiT enforces all layers to share disentanglement.
In this way, the first layer aims to optimize the ELBO, and the subsequent layers optimize disentanglement jointly by DiT which works like a constraint of disentanglement. 
Therefore, the key of DeVAE is to connect the multiple latent spaces by DiT to form a hierarchical structure with a decremental IB.}
% The reviewer misunderstands the "optimize reconstruction and keep disentangled representation" for "keep reconstruction and optimize disentangled representation".
\input{tabs/ab_component}




\input{tabs/ablation.tex}

\paragraph{Effect of $\beta$.}
More latent layers mean more chance to explore disentanglement solutions but need more time to converge.
Though Wu. etc.~\citep{DBLP:journals/ml/WuDEFT22} proposes the Annealing Test to determine the value of $\beta$, it requires labels to learn the information freezing point (IFP).
Choosing a suitable $\beta$ for each layer is difficult without knowing the information of factors.
Fortunately, DeVAE is insensitive to the choice of $\beta$, which means we can create redundant latent layers to cover all suitable $\beta$s.
In Table~\ref{tab:betas}, we compared tree cases: redundant betas ([1,10,20,40,80]), just betas ([1,10,40]), insufficient betas ([1,10]).
Redundant betas slightly diminish the performance of disentanglement and reconstruction. 
\fix{R2Q5}{It is an advantage to increase the parameter size and training iterations without rebooting}



\paragraph[]{Effect of Scale $s$.}
Increasing $s$ will add the weights of higher beta, encouraging disentanglement more than reconstruction fidelity.
It is a crucial hyperparameter to balance the objectives of latent layers.
Note that our model equals the vanilla VAE when $s=0$.
In Table~\ref{tab:scale}, we compared the effects of choosing $s$ and reported the mean$\pm$std scores of MIG~\citep{Chen2018betatcvae} and reconstruction.
For most cases, $s=1$ is a good choice.



% \input{figs/betas.tex}
\vspace{-0.5em}
\subsection{Limitation}

\vspace{-0.5em}
Since our model creates several diverse latent spaces, it is a challenge to optimize multiple objectives.
Though there are numerous combinations for setting pressures and weighting these objectives, we only search a limited range of hyper-parameters. 
Even so, DeVAE shows compatible performance on the benchmarks.
Though we validated that our model is adequate for high-dimensional space, we did not test it on real problems.
It is challenging to train a disentanglement model on large-scale problems, such as ImageNet.