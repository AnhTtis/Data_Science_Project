\section{Methodology}

\subsection{Preliminaries}
% Towards a Definition of Disentangled Representations

\paragraph{Problem Setup \& Notations.}

Disentanglement learning aims to learn the factors of variation which raises the change of observations.
Given a set of samples $\vect{x} \in \mathcal{X}$, they can be uniquely described by a set of ground-truth factors $\vect{c}\in \mathcal{C}$.
Generally, the generation process $g(\cdot)$ is invisible $\vect{x} = g(\vect{c})$.
We say a representation for factor $\vect{c}_i$ is disentangled if it is invariant for the samples with $\vect{c}_j$.
We use variational inference to learn the disentangled representation for a given problem.
$p(\vect{z}|\vect{x})$ denotes the probability of $\vect{z}=f(\vect{x})$, $p(\vect{x}|\vect{z})$ denotes the probability of $\vect{x} = g(\vect{z})$.
The representation function is a conditional Bayesian network of the form $q_\phi(\vect{z}|\vect{x})$ to estimate $p(\vect{z}|\vect{x})$.
The generative model is another network of the form $p_\theta(\vect{x}|\vect{z})p(\vect{z})$.
$\phi,\theta$ are trainable parameters.


\input{figs/model}

\paragraph{Revisit VAE \& \betavae{}.}
The VAE framework~\citep{Kingma.2013} computes the representation function by introducing $q_\phi(\vect{z}|\vect{x})$ and optimizing the variational lower bound (ELBO).
\betavae{}~\citep{Higgins2017betavae} introduces the hyperparameter $\beta$ to control the IB:
\begin{equation}\label{eq:vae}
    \begin{aligned}
        \mathcal{L}(\theta, \phi) =  \E_{q_\phi(\vect{z}|\vect{x})}[\log{p_\theta (\vect{x}|\vect{z})}]
        - \beta D_{\mathrm{KL}}(q_\phi(\vect{x}|\vect{z}) || p(\vect{z})).
    \end{aligned}
\end{equation}

Consider using \betavae{} to learn a representation of the data; the representation will be disentangled but lose information when $\beta$ is large~\citep{Burgess.2018}.
We can set a large $\beta$ to learn a disentangled representation and a small $\beta$ to learn an informative representation.

However, previous disentanglement methods~\citep{Higgins2017betavae,Chen2018betatcvae,Burgess.2018} are limited in low-dimension latent space and poorly deal with the trade-off between disentanglement and reconstruction.
Current state-of-the-art approach~\citep{shao2022rethinking} with an annealing manner from high pressure to low pressure will loosen the constraint of disentanglement when reducing the pressure.
To address this issue, we propose a novel decremental variational autoencoder with hierarchical latent spaces, namely DeVAE, to optimize disentanglement and reconstruction fidelity simultaneously, which can handle high-dimensional latent spaces, as shown in Figure~\ref{fig:model}.
Our DeVAE applies a hierarchical structure with a decremental information bottleneck and disentanglement-invariant transformation to produce latent variables layer by layer. 
The decoder part randomly selects one layer's latents concatenating an embedding vector to generate images.


\subsection{Hierarchical Latent Spaces with Decremental Information Bottleneck}\label{sec:dib}
\fix{R2Q1}{In order to retain the disentanglement constraint while optimizing the reconstruction fidelity, we introduce a Hierarchical Latent Space (HiS) with $K$ layers and assign a pressure $\beta_i$ for the $i{\mathrm{th}}$ layer $\mathcal{Z}_i$ to promote disentanglement.}

\fix{}{
The first layer aims to rebuild the dataset and uses the ELBO as objective. 
The subsequent layers will promote disentanglement by reducing the IB.
Therefore, the objective of the $i$th layer is
\begin{equation}\label{eq:derc}
    \begin{aligned}
        \mathcal{L}_i(\theta, \phi) =  \E_{q_\phi(\vect{z}_i|\vect{x})}[\log{p_\theta (\vect{x}|\vect{z}_i,\vect{v}_i)}]
        - \beta_i D_{\mathrm{KL}}(q_\phi(\vect{z}_i|\vect{x}) || p(\vect{z}_i)),
    \end{aligned}
\end{equation}
where $\vect{v}_i\in\mathbb{R}^{1\times D}$ denotes the learnable layer embedding for the $i$th layer, $p_\theta (\vect{x}|\vect{z}_i,\vect{v}_i)$ is the decoder network shared with all layers,  $q_\phi(\vect{z}_i|\vect{x})$ is the encoder network dependent on previous ones,
$\beta \in \mathbb{R}^{K}$ is a set of coefficients to penalize the IB, particularly $\beta_0=1$.
The parameters of each layer are parameterized as a bottom-up process:
\begin{equation}\label{eq:latent}
    \begin{aligned}
        q(\vect{z}_{i}|\vect{x}) = \mathcal{N}(\mu_i(\x),\sigma_i(\x)^2 ),\\
        q(\vect{z}_{0}|\vect{x}) = \mathcal{N}(\mu_0(\vect{x}),\sigma_0(\vect{x})^2),\\
        \mu_i(x),\sigma_i(x)=\tau_i(\mu_{i-1}(\x),\sigma_{i-1}(\x)), i>0
    \end{aligned}
\end{equation}
where the first layer $q_\phi(\vect{z}_0|\vect{x})$ is a conditional Bayesian network, $\tau_i$ denotes a transformation to modify the poster distribution of the previous layer to fit the layer objective  $\mathcal{L}_i(\theta, \phi)$.
}

According to information theory, information can only decrease while processing, therefore \textit{we gradually decrease the IB in the sequential layers, i.e., $\beta_{i+1}>\beta_i$.}
I\fix{R2Q3}{n this way, the last layer with a narrow IB can promote disentanglement only, and the reconstruction fidelity will become better and better from the bottom to the top.
}


\subsection{Disentanglement-invariant Transformation}\label{sec:dit}

Though we create multiple latent spaces, these objectives only encourage the local representations to be disentangled or informative.
We need a mechanism to connect these objectives for balancing disentanglement and reconstruction in one layer.
In order to make sure disentanglement across all latent layers, we propose a disentanglement-invariant transformation (DiT) denoted as $\tau$.

\begin{theorem}
    $w \cdot \vect{z}$ is disentangled if $\vect{z}$ is disentangled, $w$ is a diagonal matrix.
\end{theorem}

Proof in Appendix~\ref{sec:proof}.


According to Theorem 1, we can scale the latent space to keep disentanglement.
Scaling the posterior $\z_i$ violates the generation process which wants the marginal distribution $q(\vect{z})=\sum q_\phi(\vect{z}|\vect{x})p(\vect{x})$ to be close to a standard normal distribution.
Besides, most downstream tasks use the mean representation instead of sampled representation.
Therefore, we only need the mean representations disentanglement-invariant. 

\fix{R1R2R3}{
The disentanglement-invariant transformation scales the parameters of the $i$th layer:
\begin{equation}
    \tau_{\vect{w}^1,\vect{w}^2}(\vect\mu,\vect\sigma) = h(\vect{w}^1) \vect\mu, h(\vect{w}^2) \vect\sigma,
\end{equation}
where $\vect{w}^1,\vect{w}^2$ are learnable diagonal matrices belonging to the $i$th layer, $h(w)=e^w$ is the power function to make sure the scaling values greater than 0. }
% This formula misleads the reviewer z_i depending on z_0.
\fix{R2Q1}{Therefore, we get the parameters of $i$th latent variables}
\begin{equation}
    \begin{aligned}
        {\vect{\mu}_i} = h(\sum_{j=0}^{{i-1}} w_j^1) \vect{\mu}_0, \quad
        {\vect{\sigma}_i} = h(\sum_{j=0}^{{i-1}} w_j^2) \vect{\sigma}_0,\quad
        i>0.
    \end{aligned}
\end{equation}
and the $i$th KL divergence
\begin{equation}
    D_{{\mathrm{KL}}_i} = \frac{1}{2}(1+
    2\sum_{j=0}^{{i-1}} w_j^2 + 2\log(\vect{\sigma}_0)
    - h(2\sum_{j=0}^{{i-1}} w_j^2) \vect{\sigma}_0^2
    - h(2\sum_{j=0}^{{i-1}} w_j^1) \vect{\mu}_0^2 ).
\end{equation}

\lstinputlisting[language=Python,caption=PyTorch-like implementation of DeVAE loss.,
    float=tp,floatplacement=tbp]{code.py}

\subsection{Optimization Algorithm}

In this section, we combine the above components and introduce the optimization algorithm for the multiple objectives.
We use a random process to optimize one layer's objective from $K$ latent spaces:
\begin{equation}\label{eq:derc1}
    \begin{aligned}
        \mathcal{L}(\theta, \phi) = \mathbb{E}_{p(\vect{z}_i)}[ \mathcal{L}_i(\theta, \phi) ],
        % \mathcal{L}(\theta, \phi) = \dfrac{1}{K}\sum_{i=1}^K s_i\dot\mathcal{L}_i(\theta, \phi),
    \end{aligned}
\end{equation}
\fix{R2Q1}{where $p(\vect{z}_i)$ denotes the probability of optimizing the $i$th latent space $\vect{z}_i$, which is defined as:
\begin{equation}
    p(\vect{z}_i) = 
    \begin{cases}
\frac{1}{K}, &\mathrm{for}\ s=1\\
(1-s)\dfrac{s^i}{1-s^{i+1}}, &\mathrm{for} \ s>1
\end{cases}
\end{equation}
where $s$ denotes the power annealing of scale hyper-parameter for each $p(\vect{z}_i)$.
}
In experiments, we empirically find that $s=1$ achieves better performance, as observed in Section~\ref{sec:ablation}.
\fix{}{
Note that we do not aggregate the objectives into a loss, instead, DeVAE only rebuilds the images and optimizes the objective of one layer in one mini-batch. }

In our model, $q_\phi(\vect{z}|\vect{x})$ and decoder $p_\theta(\vect{x}|\vect{z})$ are modelled by two neural networks, 
a $K$-size sequence `betas' denotes the penalties on the KL divergences of corresponding layers, $\vect w^1,\vect w^2$ stores the learnable parameters for transforming latent spaces.
First, we randomly sample a mini-batch and choose a target layer to optimize.
Then use the algorithm introduced in Section~\ref{sec:dit} to obtain the representation of the target layer and reconstruct the corresponding images.
Instead of using $K$ separated decoders to rebuild images, we apply a shared decoder with layer embeddings to reduce the parameter size.
\fix{}{The only extra computational cost comes from $\vect w^1,\vect w^2$ and layer embeddings.
Therefore, the parameter size and overhead are similar to the vanilla VAE.} 
The PyTorch-like algorithm is shown in Algorithm 1.

