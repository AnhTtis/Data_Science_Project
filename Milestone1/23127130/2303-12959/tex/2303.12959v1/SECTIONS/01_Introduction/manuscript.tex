\section{Introduction}
% background
Unsupervised learning~\citep{wu2018unsupervised,chen2020simple,chen2020big,grill2020bootstrap,he2019moco,chen2020mocov2,mo2021spcl,chen2021simsiam,mo2022pauc,mo2023mcvt} for sensing the properties of objects is crucial to reduce the gap between humans and machines intelligence.
Inline with human intelligence disentanglement learning~\citep{Bengio.2013} is considered to be a promising direction to obtain explanatory representations from observations to understand and reason objects without any supervision.

In the recent years, various approaches~\citep{Higgins2017betavae,Chen2018betatcvae,Kim2018factorvae,Burgess.2018, DBLP:conf/nips/ChenCDHSSA16} have been proposed to successfully extract basic properties of objects, such as position, color, orientation, and scale~\citep{3dshapes18,dsprites17}.
%{disentanglement -> VAE}
The commonly-used methods are based on variational autoencoder (VAE)~\citep{Kingma.2013}.
In particular, \betavae{}~\citep{Higgins2017betavae} introduced an extra parameter $\beta$ on the Kullback-Leibler (KL) divergence to promote disentanglement.
However, there is a trade-off between disentanglement and reconstruction fidelity on \betavae{}, which is a problem to be solved in the following works.


One common direction for dealing with the trade-off is to penalize the Total Correlation (TC) between latent variables, avoiding reducing the mutual information, such as FactorVAE~\citep{Kim2018factorvae}, \btcvae{}~\citep{Chen2018betatcvae}, and DIPVAE~\citep{Kumar2018DIPVAE}.
As pointed out in~\citep{CorrelatedData.2021,dittadi2020transfer}, TC-based VAEs have a strong prior assumption that the factors are statistically independent.
Beyond that, when it comes to high-dimension latent space, the estimation of TC becomes inaccurate due to the curse of dimensionality, as our experiments observed in Section~\ref{sec:large}. 
The realistic problems usually have numerous factors, therefore it would need a large model with high latent space to extract representations.
For example, the popular deep model ResNet50~\citep{he2016deep} has 2048 dimensional feature space.
\fix{R1Q1}{However, the current TC estimations are not scaled to high dimensional problems, causing the low performance of BC-based methods in practice.
In this work, instead of calculating TC, we leverage the information bottleneck (IB)~\citep{Tishby.1999,Burgess.2018} to promote disentanglement.}


% Incremental-based VAE: information diffusion
In the meanwhile, previous information bottleneck (IB)-based methods~\citep{Burgess.2018,shao2022rethinking,DBLP:journals/ml/WuDEFT22} have tried to solve the obstacle of trade-off between disentanglement and reconstruction fidelity. 
\fix{R1Q1}{A narrow IB enforces the model to find efficient codes for representing the data, which encourages disentanglement.
Therefore, they first set a high pressure with a narrow IB and then expand the IB gradually to promote disentanglement to reconstruction fidelity}, termed \textit{incremental methods}.
For example, DynamicVAE~\citep{shao2022rethinking} initiated $\beta$ with a large value at the beginning of training for disentanglement and stably increase the KL divergence for reconstruction by a non-linear PI controller~\citep{aastrom2006pid}.
\fix{R1Q1}{However, they lost the constraint of disentanglement when expanding the IB, which causes the information diffusion problem~\citep{DBLP:journals/ml/WuDEFT22}.
In this work, to avoid information diffusion, we aim to optimize reconstruction while keeping the constraint of disentanglement.}

% key motivation
Different from IB-Incremental based approaches listed above, our key motivation is to optimize disentanglement and reconstruction simultaneously.
\fix{R1Q1}{revious methods only have one latent space and are unable to optimize disentanglement and reconstruction at the same time, which causes them to have to change the target from disentanglement to reconstruction during training.
Instead, our work proposes a novel multi-layer framework with its own latent spaces and objectives in each layer, allowing optimizing multiple targets at a time.
In this way, the first layer is a vanilla VAE to rebuild high-quality images, and the subsequent layers will distill some important variables by narrow IBs to promote disentanglement.
To inherit disentanglement from the subsequent layers, we introduce \textbf{disentanglement-invariant transformations} to connect the layers one by one.
These extra layers can be seen as regularizations for disentanglement to constrain the representation.}

% methods
To achieve this, we propose a simple yet effective VAE framework composed of multiple continuous latent sub-spaces with a novel IB-Decremental strategy and disentanglement-invariant transform operators, which we call DeVAE.
\fix{}{
Specifically, we decrease the information bottleneck of each latent space layer by layer, where we constrain the first space for informativeness to recover the input image, and other disentangled spaces for learning factors of the image by narrow IBs.}
Furthermore, we introduce the disentanglement-invariant transform operator to ensure simultaneous optimization of disentanglement across continuous latent sub-spaces, which avoids the information diffusion.
Our decremental model avoids ID by keeping the constraints of disentanglement while optimizing reconstruction. 
We also conducted comprehensive comparisons with popular methods quantitatively and qualitatively.
\fix{}{Experimental results demonstrate that DeVAE is robust in hyperparameters and the size of latent spaces.
}


Our contributions can be summarized as follows:
\begin{itemize}
    \item We introduce several latent spaces sharing disentanglement by disentanglement-invariant transformations.
    \item We propose a novel diagram for disentanglement learning by decreasing IB, termed decremental VAE (DeVAE). Our decremental model can handle large-scale problems and show robustness on several datasets.
\end{itemize}
