

\section{Appendix}

\subsection{Architecture}\label{sec:arch}
\input{tabs/architecture.tex}
The details of architectures are listed in Table~\ref{tab:architecture}.


\subsection{Disentanglement-invariant Representations}
\label{sec:proof}
In this section, we prove the proposed disentanglement-invariant transformation.
Consider that we have a new representation by multiplying a diagonal matrix: $\vect{z}' = \vect{w} \vect{z}$, $\vect{w}$.
We can calculate the Covariance between any two latent variables:
\begin{equation}
\begin{aligned}
\operatorname{Cov}(\vect{w}_i \vect{z}_i, \vect{w}_j \vect{z}_j) &=
\mathbb{E}[(\vect{w}_i \vect{z}_i-\mathbb{E}[\vect{w}_i \vect{z}_i])(\vect{w}_j \vect{z}_j-\mathbb{E}[\vect{w}_j \vect{z}_j])] \\
&=\vect{w}_i \vect{w}_j (\mathbb{E}[\vect{z}_j]-\mathbb{E}[ \vect{z}_i] \mathbb{E}[\vect{z}_j]) \\
&=\vect{w}_i \vect{w}_j \operatorname{Cov}(\vect{z}_i,\vect{z}_j),
\end{aligned}
\end{equation}
where the subscript denotes the index of latent variables.
Note that we use a different notion in this section to simplify the formula.

Then we can get the correlation coefficient by
\begin{equation}
\begin{aligned}
    \rho(\vect{w}_i \vect{z}_i, \vect{w}_j \vect{z}_j)&=\frac{\operatorname{Cov}(\vect{w}_i \vect{z}_i, \vect{w}_j \vect{z}_j)}{\sqrt{\operatorname{Var}[\vect{w}_i \vect{z}_i] \operatorname{Var}[\vect{w}_j \vect{z}_j]}}\\
    &= \rho( \vect{z}_i, \vect{z}_j).
\end{aligned}
\end{equation}

Therefore, the correlation matrix will not change by multiplying a diagonal matrix $w, w\neq0$.
We could create a disentanglement-invariant representation by multiplying a diagonal matrix.

\subsection{ Estimation of $I(\vect{z}_j;\vect{c}_i)$}


Given an inference network $q(\vect{z}|\vect{x})$, we use the Markov chain Monte Carlo (MCMC) method to get samples from $q(\vect{z})$ by the formula $q(\vect{z}) = q(\vect{z|x})p(\vect{x})$.
We use 10, 000 points to estimate $q(\vect{z})$.
Then, we discretize these latent variables by a histogram with 20 bins. 
After discretizing one latent variable, we call a discrete mutual information estimation algorithm to calculate $I(\vect{w}_j \vect{z}_j;\vect{c}_i)$ by a 2D histogram.

\subsection{Latent Traversals}\label{sec:latent_traversal}
\input{figs/traversal.tex}
We compare DeVAE to others with latent traversals on Shapes3D and dSprites.
Each column denotes the generated images by traversing one latent variable from -2 to 2. 
We also interpret the extracted factor at the bottom.
From Figure~\ref{fig:traversal_shapes} and Figure~\ref{fig:traversal_dsprites}, we can see that DeVAE has a lower entanglement level. 
Note that only DeVAE disentangles object size isolated on Shapes3D.
% \subsection{Intervention}\label{sec:


\subsection{Random Sampling}\label{sec:samples}
\fix{}{
We show the visualization of random sampling from the best models with the highest MIG trained on dSprites and Shapes3D in Figure~\ref{fig:best_sample1}~\ref{fig:best_sample2}~\ref{fig:best_sample3}.
}
\input{figs/best_samples}

\subsection{Decremental Diagram}

\fix{}{
Figure~\ref{fig:layers_mi} shows how the mutual information between factors and latent variables decreases over layers on dSprites. 
One can see the mutual information decreases along the layers, and information of shape, scale, and orientation is totally disappeared in layer2.
The last layer is more likely to disentangle them, and that property will be preserved and passed to the first layer for a constraint of disentanglement.}
\input{figs/DeVAE_layer}

\subsection{High-dimensional Latent Sapce}\label{sec:high_dimension}
\input{figs/high_dimension}

\fix{All}{
DeVAE has significant advantages for handling high-dimensional latent spaces. 
Though DynamicVAE outperforms low-dimensional latent spaces, there is a gap in the high-dimensional latent spaces.
We trained DynamicVAE and DeVAE with 1024-dimensional latent spaces on dSprites to investigate what causes the difference.
\citep{cao2022anmed} found that existing disentanglement metrics fail to make meaningful measurements for high-dimensional representation models, therefore we apply the proposed metric by them in this experiment. 
Active variables denote the latent variables containing information, and those containing no information will collapse into one point, so the active variables will have large variances.
From Figure~\ref{fig:high_dimension}, DynamicVAE has more active variables and performs worse than DeVAE. 
DynamicVAE expanding the IB smoothly could have a good performance on low-dimensional spaces, but the increment of dimensions raises the chance of leaking information to others.
As a result, the number of active variables increases quickly when expanding the IB, see iteration 20000 to 100000.}


\subsection{Comparison with FactorVAE and CascadeVAE}
\fix{}{FactorVAE~\citep{Kim2018factorvae} and CascadeVAE~\citep{JeongS19a} are two relevant methods for disentanglement.
We compare six disentanglement methods on dSprites in Table~\ref{tab:cascade_vae}. Each tail repeats 5 times to get the mean and std scores.
Similar to $\beta$-TCVAE, FactorVAE can not consistently outperform one hyperparameter on two datasets. 
Though CascadeVAE has good reconstruction fidelity, it cannot disentangle all factors properly.
}

\begin{table}[]

\caption{Comparison of reconstruction error (Recon.), MIG score, and ELBO for six disentanglement methods.}\label{tab:cascade_vae}
\centering
\begin{tabular}{ccccc}
\toprule
dataset                   & model      & Recon.      & MIG       & ELBO          \\ \midrule
\multirow{6}{*}{dSprites} & FactorVAE  & 21.55$\pm$0.84  & 0.34$\pm$0.04 & -46.05$\pm$2.24   \\
                          & CascadeVAE & 12.04$\pm$1.23  & 0.20$\pm$0.07 & -32.14 $\pm$ 1.29 \\
                          & Dynamic    & 19.81$\pm$1.19  & 0.35$\pm$0.01 & -37.83$\pm$1.17   \\
                          & beta-TCVAE & 73.04$\pm$3.41  & 0.29$\pm$0.10 & -82.29$\pm$3.71   \\
                          & beta-VAE   & 48.75$\pm$2.84  & 0.17$\pm$0.05 & -61.17$\pm$3.13   \\
                          & DeVAE      & 36.02$\pm$20.02 & 0.32$\pm$0.11 & -51$\pm$22.26    \\
                          \midrule
\multirow{6}{*}{Shapes3D} & FactorVAE  & 18.48$\pm$2.28  & 0.38$\pm$0.28 & -38.08$\pm$1.87   \\
                          & CascadeVAE & 14.84$\pm$1.98  & 0.46$\pm$0.11 & -32.54$\pm$2.10   \\
                          & Dynamic    & 29.70$\pm$4.15  & 0.54$\pm$0.04 & -47.68$\pm$4.28   \\
                          & beta-TCVAE & 44.53$\pm$5.69  & 0.49$\pm$0.11 & -60.01$\pm$6.29   \\
                          & beta-VAE   & 34.95$\pm$2.34  & 0.42$\pm$0.18 & -49.09$\pm$2.72   \\
                          & DeVAE      & 46.80$\pm$13.97 & 0.52$\pm$0.10 & -74.73$\pm$31.66  \\ 
                          \bottomrule
\end{tabular}
\end{table}