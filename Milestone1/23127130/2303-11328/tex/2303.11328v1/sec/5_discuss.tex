
\section{Discussion}
In this work, we have proposed a novel approach, \textit{Zero-1-to-3}, for zero-shot, single-image novel-view synthesis and 3D reconstruction. Our method capitalizes on the Stable Diffusion model, which is pre-trained on internet-scaled data and captures rich semantic and geometric priors. To extract this information, we have fine-tuned the model on synthetic data to learn control over the camera viewpoint. The resulting method demonstrated state-of-the-art results on several benchmarks due to its ability to leverage strong object shape priors learned by Stable Diffusion.


\subsection{Future Work}
%\vspace{-7px}
\textbf{From objects to scenes.} Our approach is trained on a dataset of single objects on a plain background. While we have demonstrated a strong degree of generalizations to scenes with several objects on RTMV dataset, the quality still degrades compared to the in-distribution samples from GSO. Generalization to scenes with complex backgrounds thus remains an important challenge for our method. 

%\vspace{-7px}
\textbf{From scenes to videos.} Being able to reason about geometry of dynamic scenes from a single view would open novel research directions, such as understanding occlusions~\cite{revealing, liu2022shadows} and dynamic object manipulation. A few approaches for diffusion-based video generation have been proposed recently~\cite{ho2022video,esser2023structure}, and extending them to 3D would be key to opening up these opportunities.

%\vspace{-7px}
\textbf{Combining graphics pipelines with Stable Diffusion.} In this paper, we demonstrate a framework to extract 3D knowledge of objects from Stable Diffusion. A powerful natural image generative model like Stable Diffusion contains other implicit knowledge about lighting, shading, texture, etc. Future work can explore similar mechanisms to perform traditional graphics tasks, such as scene relighting.


{\textbf{Acknowledgements:} We would like to thank Changxi Zheng and Samir Gadre for their helpful feedback. We would also like to thank the authors of SJC~\cite{wang2022score}, NeRDi~\cite{deng2022nerdi}, SparseFusion~\cite{zhou2022sparsefusion}, and Objaverse~\cite{deitke2022objaverse} for their helpful discussions. This research is based on work partially supported by the Toyota Research Institute, the DARPA MCS program under Federal Agreement No.\ N660011924032, and the NSF NRI Award \#1925157.}


% \vspace{-7px}
% \paragraph{Inference speed.} As our approach relies on Stable Diffusion for novel view synthesis, it inherits not only its strong object shape priors, but also its relatively slow inference speed. This limitation will become especially important when applying the method to videos with hundreds of frames. Developing novel techniques to speed-up the diffusion process will thus be critical for scaling diffusion-based reconstruction techniques in the future.
