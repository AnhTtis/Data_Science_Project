
\section{Experiments}
\label{sec:exp}

We assess our model's performance on zero-shot novel view synthesis and 3D reconstruction. As confirmed by the authors of Objaverse, the datasets and images we used in this paper are outside of the Objaverse dataset, and can thus be considered zero-shot results. We quantitatively compare our model to the state-of-the-art on synthetic objects and scenes with different levels of complexity. We also report qualitative results using diverse in-the-wild images, ranging from pictures we took of daily objects to paintings.

%First, we describe the tasks and methodology in Section~\ref{sec:tasks}. In Section~\ref{sec:baselines}, we outline the baseline methods we compare our approach against. Then, we evaluate our model on novel view synthesis in Section~\ref{sec:nvs}, and on 3D reconstruction in Section~\ref{sec:3d}, both using metrics and data described in Section~\ref{sec:metrics}.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/qualitative_wild_2.pdf}
    \caption{\textbf{Novel view synthesis on in-the-wild images.} The 1st, 3rd, and 4th rows show results on images taken by an iPhone, and the 2nd row shows results on an image downloaded from the Internet. Our method works are robust to objects with different surface materials and geometry.  We randomly sampled 5 different viewpoints and directly showcase the results without cherry-picking. We include more uncurated results in the supplementary materials.
    % \vspace{-0.3cm}
    }
    \label{fig:qualitative_wild}
\end{figure*}

\subsection{Tasks}
\label{sec:tasks}

We describe two closely related tasks that take single-view RGB images as input, and we apply them zero-shot.

\looseness=-1
\textbf{Novel view synthesis.}
Novel view synthesis is a long-standing 3D problem in computer vision that requires a model to learn the depth, texture, and shape of an object implicitly. The extremely limited input information of only a single view requires a novel view synthesis method to leverage prior knowledge. Recent popular methods have relied on optimizing implicit neural fields with CLIP consistency objectives from randomly sampled views~\cite{jain2021putting}. Our approach for view-conditional image generation is orthogonal, however, because we invert the order of 3D reconstruction and novel view synthesis, while still retaining the identity of the object depicted in the input image. This way, the aleatoric uncertainty due to self-occlusion can be modeled by a probabilistic generative model when rotating around objects, and the semantic and geometric priors learned by large diffusion models can be leveraged effectively.

%\vspace{-0.2cm}
\looseness=-1
\textbf{3D Reconstruction.}
We can also adapt a stochastic 3D reconstruction framework such as SJC~\cite{wang2022score} or DreamFusion~\cite{poole2022dreamfusion} to create a most likely 3D representation. We parameterize this as a voxel radiance field~\cite{chen2022tensorf,sun2022direct,fridovich2022plenoxels}, and subsequently extract a mesh by performing marching cubes on the density field. The application of our view-conditioned diffusion model for 3D reconstruction provides a viable path to channel the rich 2D appearance priors learned by our diffusion model toward 3D geometry. 


\subsection{Baselines}
\label{sec:baselines}

To be consistent with the scope of our method, we compare only to methods that operate in a zero-shot setting and use single-view RGB images as input.

For novel view synthesis, we compare against several state-of-the-art, single-image algorithms. In particular, we benchmark DietNeRF~\cite{jain2021putting}, which regularizes NeRF with a CLIP image-to-image consistency loss across viewpoints. In addition, we compare with Image Variations (IV)~\cite{sdvariation}, which is a Stable Diffusion model fine-tuned to be conditioned on images instead of text prompts and could be seen as a semantic nearest-neighbor search engine with Stable Diffusion. Finally, we adapted SJC~\cite{wang2022score}, a diffusion-based text-to-3D model where the original text-conditioned diffusion model is replaced with an image-conditioned diffusion model, which we termed SJC-I.

For 3D reconstruction, we use two state-of-the-art, single-view algorithms as baselines:
(1) Multiview Compressive Coding (MCC)~\cite{wu2023multiview}, which is a neural field-based approach that completes RGB-D observations into a 3D representation, as well as (2) Point-E~\cite{nichol2022point}, which is a diffusion model over colorized point clouds. MCC is trained on CO3Dv2 \cite{reizenstein2021common}, while Point-E is notably trained on a significantly bigger OpenAI's internal 3D dataset. We also compare against SJC-I. % with much more computing resources.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/diversity.pdf}
    \caption{\textbf{Diversity of novel view synthesis.} With an input view, we fix another viewpoint and randomly generate multiple conditional samples. The different results reflect a range of diversity in terms of both geometry and appearance information that is missing in the input view.
    % \vspace{-0.3cm}
    }
    \label{fig:diversity}
\end{figure*}

Since MCC requires depth input, we use MiDaS~\cite{ranftl2020towards, ranftl2021vision} off-the-shelf  for depth estimation. % After obtaining disparity maps,
% and use \bv{todo which? should we be honest about using some random website?} an online off-the-shelf foreground segmentation model.
We convert the obtained relative disparity map to an absolute pseudo-metric depth map by assuming standard scale and shift values that look reasonable across the entire test set.


\subsection{Benchmarks and Metrics}
\label{sec:metrics}

We evaluate both tasks on Google Scanned Objects (GSO)~\cite{downs2022google}, which is a dataset of high-quality scanned household items, as well as RTMV~\cite{tremblay2022rtmv}, which consists of complex scenes, each composed of 20 random objects.
% We randomly sample 50 objects from the GSO dataset. For each such object, we reserve one viewpoint as input, and render 9 other random views to use as ground truths for evaluating novel view synthesis.
% We randomly sample 10 scenes from the RTMV dataset. For each such scene, one viewpoint is reserved as input and another 30 random views serve as ground truths for evaluating novel view synthesis.
In all experiments, the respective ground truth 3D models are used for evaluating 3D reconstruction.
% Due to limited time and resources, we use a subset of 20 objects out of the 50 GSO objects for evaluation on 3D reconstruction.

\input{tab/1_nvs_gso.tex}

For novel view synthesis, we numerically evaluate our method and baselines extensively with four metrics covering different aspects of image similarity: PSNR, SSIM~\cite{wang2004image}, LPIPS~\cite{zhang2018unreasonable}, and FID~\cite{heusel2017gans}.
For 3D reconstruction, we measure Chamfer Distance and volumetric IoU. % to evaluate the reconstructed geometry in multiple aspects.



\subsection{Novel View Synthesis Results}
\label{sec:nvs}

We show the numerical results in Tables~\ref{tab:GSO_NVS} and~\ref{tab:RTMV_NVS}.
% \BV{todo maybe extra discussion beyond caption?}
% \paragraph{Qualitative results.}
Figure~\ref{fig:qualitative_gso} shows that our method, as compared to all baselines on GSO, is able to generate highly photorealistic images that are closely consistent with the ground truth. Such a trend can also be found on RTMV in Figure~\ref{fig:qualitative_rtmv}, even though the scenes are out-of-distribution compared to the Objaverse dataset. Among our baselines, we observed that Point-E tends to achieve much better results than other baselines, maintaining impressive zero-shot generalizability. However, the small size of the generated point clouds greatly limits the applicability of Point-E for novel view synthesis.

\input{tab/2_nvs_rtmv.tex}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/geometry.pdf}
    \caption{\textbf{Qualitative examples of 3D reconstruction.} The input view images are shown on the left. For each method, we show a rendered view from a different angle and the reconstructed 3D mesh. The ground truth meshes are shown on the right.
    \vspace{-0.5cm}
    }
    \label{fig:geometry}
\end{figure*}


In Figure~\ref{fig:qualitative_wild}, we further demonstrate the generalization performance of our model to objects with challenging geometry and texture as well as its ability to synthesize high-fidelity viewpoints while maintaining the object type, identity and low-level details.
% \BV{is this true? angle is not an input to the VoRF right? don't you just mean lighting intensity variations (which is still lambertian!) instead of specular reflection?}
% \rl{For example, the purple monster in the 3rd row has a near-Lambertian surface while the toy taxi in the 4th row shows some specular reflection. Our model is able to successfully maintain these details in the novel views.}

% \paragraph{Quantitative results.}
% \BV{todo refer to table 1 and table 2}
% All of them highlight that our method is able to outperform the baselines by a significant margin.

%\vspace{+0.5cm}
\textbf{Diversity across samples.}
Novel view synthesis from a single image is a severely under-constrained task, which makes diffusion models a particularly apt choice of architecture compared to NeRF in terms of capturing the underlying uncertainty.
% \BV{I disagree with analogy because this is an essential feature of diffusion models in general, not just text2image}
% Analogy can be drawn here with the text-to-image generation task, in which diffusion models excel at generating high-fidelity.
Because input images are 2D, they always depict only a partial view of the object, leaving many parts unobserved. Figure~\ref{fig:diversity} exemplifies the diversity of plausible, high-quality images sampled from novel viewpoints.
% and all outputs are clearly plausible the is All of them are clearly plausible


\subsection{3D Reconstruction Results}
\label{sec:3d}

We show numerical results in Tables~\ref{tab:GSO_3D} and~\ref{tab:RTMV_3D}.
% \paragraph{Qualitative results.}
Figure~\ref{fig:geometry}  qualitatively shows our method reconstructs high-fidelity 3D meshes that are consistent with the ground truth. 
MCC tends to give a good estimation of surfaces that are visible from the input view, but often fails to correctly infer the geometry at the back of the object.

\input{tab/3_3d_gso.tex}

SJC-I is also frequently unable to reconstruct a meaningful geometry.
On the other hand, Point-E has an impressive zero-shot generalization ability, and is able to predict a reasonable estimate of object geometry. However, it generates non-uniform sparse point clouds of only 4,096 points, which sometimes leads to holes in the reconstructed surfaces (according to their provided mesh conversion method). % (we use their default meshing method). 
Therefore, it obtains a good CD score but falls short of the volumetric IoU.
% In addition, the 3D voxel NeRF we reconstructed is much higher resolution and scalable than Point-E, which generates 4,000 colored point clouds, regardless of the complexity of the scenes. 
Our method leverages the learned multi-view priors from our view-conditioned diffusion model and combines them with the advantages of a NeRF-style representation.
Both factors provide improvements in terms of CD and volumetric IoU over prior works, as indicated by Tables~\ref{tab:GSO_3D} and \ref{tab:RTMV_3D}.

\input{tab/4_3d_rtmv.tex}
\begin{figure*}
    \centering
    % \vspace{-0.5em}
    \includegraphics[width=\linewidth]{figures/txt2img.pdf}
    \caption{\textbf{Novel View Synthesis from Dall-E-2 Generated Images.} The composition of multiple objects (1st row) and the lighting details (2nd row) are preserved in our synthesized novel views.}
    % \vspace{-1em}
    \label{fig:txt2img}
\end{figure*}


\subsection{Text to Image to 3D} \label{supp:aigc}
In addition to in-the-wild images, we also tested our method on images generated by txt2img models such as Dall-E-2~\cite{ramesh2021zero}. As shown in Figure~\ref{fig:txt2img}, our model is able to generate novel views of these images while preserving the identity of the objects. We believe this could be very useful in many text-to-3D generation applications.

% \BV{also for appendix:}
% \subsection{Failure cases?}





% \bv{}

% Whereas most existing works first optimize a 3D representation in order to subsequently sample novel viewpoints, our model performs novel view synthesis directly instead and can then be adapted to reconstruct 3D objects. Against the backdrop of this related work, coming up with the right evaluation procedure is non-trivial.

% We evaluate our model separately on both novel view synthesis and 3D reconstruction.

% \subsection{Novel View Synthesis}

% A crucial advantage of our method is that images from any arbitrary viewpoint can be sampled directly from the fine-tuned diffusion model without any further test-time optimization. In contrast, the baselines must be retrained per-scene, which is a process that lasts hours.

% \subsection{3D Reconstruction}




% For baselines, we compare with Point-E \cite{x} and MCC \cite{x}.

% \bv{todo describe them}
% % Point-E is a point cloud diffusion model that can turn text into 3D or image into 3D.
% % MCC is a neural field that can turn RGB-D image into

% \bv{todo how do we apply point-e}

% For MCC, we use the MiDaS \cite{x} for off-the-shelf depth estimation to obtain disparity maps, and use \bv{todo which? should we be honest about using some random website?} an online off-the-shelf foreground segmentation model. We convert the disparity map to a depth map by assuming standard scale and shift values that look reasonable across our test set.

% Our evaluation metrics are Chamfer Distance (CD) and volumetric Intersection over Union (IoU). \bv{todo Because Point-E's ...}
% MCC requires an RGB-D input point cloud, but the prediction may have a substantially different size and pose as compared to the ground truth 3D mesh. We therefore first equalize the scale of the output and target models, and subsequently apply the Iterative Closest Point (ICP) \cite{x} algorithm to align the predicted point cloud with the ground truth.












