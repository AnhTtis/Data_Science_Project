
\twocolumn[
\centering
\Large
\textbf{Appendix} \\
\vspace{+1em}
] %< twocolumn

\appendix


% \paragraph{Diversity-fidelity trade-off.}
% \RW{Personally, I don't feel we need 4.3.1 and 4.3.2. The statements are (maybe) true but we didn't show any supporting evidence in our results.}
% In prior works on optimizing NeRF with conditional diffusion priors, a key technique proposed that significantly boosted the fidelity of the reconstruction is significantly scaling the classifier-free guidance weight used in the diffusion model. Readers can refer to figure 9 in DreamFusion~\cite{poole2022dreamfusion} for a more detailed analysis of the impact of guidance weight. Such high guidance weight unavoidably leads to the loss of diversity across different samples.

% \paragraph{Color saturation.}
% With language-guided diffusion priors, such as in \cite{wang2022score, poole2022dreamfusion, lin2022magic3d}, a side-effect of high guidance factors is that they typically lead to results with highly saturated colors. However, since our diffusion model is finetuned with images and we also optimize the input view with an MSE loss, such color saturation problem is significantly reduced. Qualitatively, our reconstruction demonstrates color-consistency with the ground truth view.

% \subsection{Failure Cases}

% \section{Overview}
% Many related prior works in the domain are not open source, making reproducing the results very challenging. In this supplementary material, we provide implementation code, demos, and technical details with full transparency. In section~\ref{supp:external}, we list external resources for more uncurated results and code. In section~\ref{supp:aigc}, we show examples of using our model to perform text-to-image-to-3D tasks. In sections~\ref{supp:camera}-~\ref{supp:3d}, we elaborate technical implementation details of each components of our method. In section~\ref{supp:baseline}, we include implementation details regarding the baseline methods used in the experiments.


% \section{External Resources} \label{supp:external}
% As part of the supplementary materials, we provide \emph{anonymous} external resources including a website demoing results of novel view synthesis and 3D reconstruction, the code containing implementation of our framework, and a Gradio demo which contains \textit{uncurated} results we tested our model on, hosted on Hugging Face. We hope these materials provide complementary information. % that couldn't be otherwise shown in the main paper.
% \vspace{+0.5em}

% \textbf{Website: \href{https://Zero-1-to-3.github.io/}{Zero-1-to-3.github.io}}
% % \noindent \textbf{Website:} \\ \small \url{https://zero123-anonymous.github.io/Zero-1-to-3.github.io/}

% \textbf{Code: \href{https://drive.google.com/file/d/12j3YG5oAopOzXIkWvJ7DDg57y0yzeFwo/view?usp=share_link}{Google Drive}}
% % \noindent \textbf{GitHub repository:} \\ \small \url{https://github.com/zero123-anonymous/Zero-1-to-3-code}

% \textbf{Gradio demo: \href{https://zero123-anonymous-Zero-1-to-3-anonymous.hf.space/}{Gradio}}
% % \noindent \textbf{Gradio demo:} \\ \small \url{https://zero123-anonymous-Zero-1-to-3-anonymous.hf.space/}

% % \section{An image is worth how many views?}

% % removing this section due to insufficient experimental results


\section{Coordinate System \& Camera Model} \label{supp:camera}

\begin{figure}[h]
    \centering
    \vspace{-1em}
    \includegraphics[width=0.5\linewidth]{figures/spherical.png}
    \caption{Spherical Coordinate System~\cite{wiki:Spherical_coordinate_system}.}
    \vspace{-1em}
    \label{fig:spherical}
\end{figure}

% We use spherical coordinate systems to represent camera locations and relative transformation and a pinhole camera model. As shown in figure~\ref{fig:spherical}, assuming the center of the object is the origin of the coordinate system, we can use $\theta$, $\phi$, $r$ to represent the polar angle, azimuth angle, and distance from the center. During dataset creation, we normalize all assets to be contained inside a unit cube, then we uniformly sample camera viewpoints where $\theta \sim [0, \pi]$, $\phi \sim [0, 2\pi]$, $r \sim [1.5, 2.2]$.
% NOTE: I also modified this passage but didn't have track changes
We use a spherical coordinate system to represent camera locations and their relative transformations.
As shown in Figure~\ref{fig:spherical}, assuming the center of the object is the origin of the coordinate system, we can use $\theta$, $\phi$, and $r$ to represent the polar angle, azimuth angle, and radius (distance away from the center) respectively. For the creation of the dataset, we normalize all assets to be contained inside the XYZ unit cube $[-0.5, 0.5]^3$. Then, we sample camera viewpoints such that $\theta \in [0, \pi]$, $\phi \in [0, 2\pi]$ uniformly cover the unit sphere, and $r$ is sampled uniformly in the interval $[1.5, 2.2]$.
During training, when two images from different viewpoints are sampled, let their camera locations be $(\theta_1, \phi_1, r_1)$ and $(\theta_2, \phi_2, r_2)$. We denote their \emph{relative} camera transformation as $(\theta_2 - \theta_1, \phi_2 - \phi_1, r_2 - r_1)$. Since the camera is always pointed at the center of the coordinate system, the extrinsics matrices are uniquely defined by the location of the camera in a spherical coordinate system. We assume the horizontal field of view of the camera to be $49.1^\circ$, and follow a pinhole camera model.

Due to the incontinuity of the azimuth angle, we encode it with $\phi \mapsto [\sin(\phi), \cos(\phi)]$.
% Subsequently, during training time, four values representing the relative camera viewpoint change, $[\theta, \sin(\phi), \cos(\phi), r]$ are fed to the model. During inference time, an input view and a user-defined viewpoint change vector $[\theta, \phi, r]$ are used to generate a novel view.
Subsequently, at both training and inference time, four values representing the relative camera viewpoint change, $[\theta, \sin(\phi), \cos(\phi), r]$ are fed to the model, along with an input image, in order to generate the novel view.

\section{Dataset Creation} \label{supp:dataset}
We use Blender~\cite{blender} to render training images of the finetuning dataset. The specific rendering code is inherited from a publicly released repository\footnote{\url{https://github.com/allenai/objaverse-rendering}} by authors of Objaverse~\cite{deitke2022objaverse}. For each object in Objaverse, we randomly sample 12 views and use the Cycles engine in Blender with 128 samples per ray along with a denoising step to render each image. We render all images in 512$\times$512 resolution and pad transparent backgrounds with white color. We also apply randomized area lighting. In total, we rendered a dataset of around 10M images for finetuning.

\section{Finetuning Stable Diffusion} \label{supp:diffusion}

We use the rendered dataset to finetune a pretrained Stable Diffusion model for performing novel view synthesis. Since the original Stable Diffusion network is not conditioned on multimodal text embeddings, the original Stable Diffusion architecture needs to be tweaked and finetuned to be able to take conditional information from an image. This is done in~\cite{sdvariation}, and we use their released checkpoints. To further adapt the model to accept conditional information from an image along with a relative camera pose, we concatenate the image CLIP embedding (dimension 768) and the pose vector (dimension 4) and initialize another fully-connected layer (772 $\mapsto$ 768) to ensure compatibility with the diffusion model architecture. The learning rate of this layer is scaled up to be 10$\times$ larger than the other layers. The rest of the network architecture is kept the same as the original Stable Diffusion.

\subsection{Training Details}

We use AdamW~\cite{loshchilov2017decoupled} with a learning rate of $10^{-4}$ for training. First, we attempted a batch size of 192 while maintaining the original resolution (image dimension $512 \times 512$, latent dimension $64 \times 64$) for training. However, we discovered that this led to a slower convergence rate and higher variance across batches. Because the original Stable Diffusion training procedure used a batch size of 3072, we subsequently reduce the image size to $256 \times 256$ (and thus the corresponding latent dimension to $32 \times 32$), in order to be able to increase the batch size to 1536. This increase in batch size has led to better training stability and a significantly improved convergence rate. We finetuned our model on an 8$\times$A100-80GB machine for 7 days.

\subsection{Inference Details}
To generate a novel view, Zero-1-to-3 takes only 2 seconds on an RTX A6000 GPU. Note that in prior works, typically a NeRF is trained in order to render novel views, which takes significantly longer. In comparison, our approach inverts the order of 3D reconstruction and novel view synthesis, causing the novel view synthesis process to be fast and contain diversity under uncertainty.  Since this paper addresses the problem of a single image to a 3D object, when an in-the-wild image is used during inference, we apply an off-the-shelf background removal tool~\cite{ophoperhpo} to every image before using it as input to Zero-1-to-3.

% \subsection{Scalability}
% There are several ways to further scale up our method. First, the \textit{Objaverse} dataset continues to grow in size. By training a model on a bigger dataset of 3D assets using our method, we can expect the model to further improve in generalization performance to in-the-wild images. Second, as limited by the resources we have, we can only afford to render 12 views for each object. Therefore, another way to scale up the complexity and diversity of the dataset is to sample significantly more views, e.g. 128 views per object, or even on-the-fly rendering (new views are sampled for each training epoch). Third, we reduced the image and latent resolution during training due to limited resources. We believe a higher image and latent resolution while keeping the same batch size can lead to better results both qualitatively and quantitatively.

\section{3D Reconstruction} \label{supp:3d}

Different from the original Score Jacobian Chaining (SJC) implementation, we removed the ``emptiness loss'' and ``center loss''. To regularize the VoxelRF representation, we differentiably render a depth map, and apply a smoothness loss to the depth map. This is based on the prior knowledge that the geometry of an object typically contains less high-frequency information than its texture. It is particularly helpful in removing holes in the object representation. We also apply a near-view consistency loss to regularize the difference between an image rendered from one view and another image rendered from a nearby randomly sampled view. We found this to be very helpful in improving the cross-view consistency of an object's texture.
All implementation details can be found in the code that is submitted as part of the appendix. Running a full 3D reconstruction on an image takes around 30 minutes on an RTX A6000 GPU.

\paragraph{Mesh extraction.}
We extract the 3D mesh from the VoxelRF representation as follows.
We first query the density grids at resolution $200^3$.
Then we smooth the density grids using a mean filter of size $(7, 7, 7)$, followed by an erosion operator of size $(5, 5, 5)$.
Finally, we run marching cubes on the resulting density grids.
Let $\bar d$ denote the average value of the density grids.
For the GSO dataset, we use a density threshold of $8\bar d$.
For the RTMV dataset, we use a density threshold of $4\bar d$.

\paragraph{Evaluation.} 
The ground truth 3D shape and the predicted 3D shape are first normalized within the unit cube. 
To compute the chamfer distance (CD), we randomly sample 2000 points. 
For Point-E and MCC, we sample from their predicted point clouds directly.
For our method and SJC-I, we sample points from the reconstructed 3D mesh.
We compute the volumetric IoU at resolution $64^3$.
For our method, Point-E and SJC-I, we vocalize the reconstructed 3D surface meshes using marching cubes.
For MCC, we directly voxelize the predicted dense point clouds by occupancy.

\section{Baselines} \label{supp:baseline}

To be consistent with the scope of our method, we compare only to methods that (1) operate in a zero-shot setting, (2) use single-view RGB images as input, and (3) have official reference implementations available online that can be adapted in a reasonable timeframe.
In the following sections, we describe the implementation details of our baselines.

\subsection{DietNeRF}

We use the official implementation located on 
GitHub\footnote{\url{https://github.com/ajayjain/DietNeRF}}, which, at the time of writing, has code for low-view NeRF optimization from scratch with a joint MSE and consistency loss, though provides no functionality related to finetuning PixelNeRF. For fairness, we use the same hyperparameters as the experiments performed with the NeRF synthetic dataset in~\cite{jain2021putting}. For the evaluation of novel view synthesis, we render the resulting NeRF from the designated camera poses in the test set.
% treat the predictions from the test set viewpoints in exactly the same way as the evaluations of Zero-1-to-3.

\subsection{Point-E}

We use the official implementation and pretrained models located on GitHub\footnote{\url{https://github.com/openai/point-e}}.
We keep all the hyperparameters and follow their demo example to do 3D reconstruction from single input image. The prediction is already normalized, so we do not need to perform any rescaling to match the ground truth.
For surface mesh extraction, we use their default method with a grid size of 128.

\subsection{MCC}

We use the official implementation located on 
GitHub\footnote{\url{https://github.com/facebookresearch/MCC}}. Since this approach requires a colorized point cloud as input rather than an RGB image, we first apply an online off-the-self foreground segmentation method~\cite{ophoperhpo} as well as a state-of-the-art depth estimation method~\cite{ranftl2020towards, ranftl2021vision} for preprocessing. For fairness, we keep all hyperparameters the same as the zero-shot, in-the-wild experiments described in~\cite{wu2023multiview}. For the evaluation of 3D reconstruction, we normalize the prediction, rotate it according to camera extrinsics, and compare it with the 3D ground truth.
% \BV{todo elaborate on this -- not entirely sure how our own model evaluation plays into this, and how mesh vs point cloud vs voxels works and should be handled here}
% \RW{I describe this in sec F.}


% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{supp_bib}
% }






