\section{Method}
\label{sec:meth}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/nvs.pdf}
    \caption{\textbf{Zero-1-to-3} is a viewpoint-conditioned image translation model using a conditional latent diffusion architecture. Both the input view and a relative viewpoint transformation are used as conditional information.
    % \vspace{-0.3cm}
    }
    \label{fig:method_nvs}
\end{figure}



Given a single RGB image $x \in \mathbb{R}^{H \times W \times 3}$ of an object, our goal is to synthesize an image of the object from a different camera viewpoint. Let $R \in \mathbb{R}^{3\times 3}$ and $T \in \mathbb{R}^3$ be the relative camera rotation and translation of the desired viewpoint, respectively. We aim to learn a model $f$ that synthesizes a new image under this camera transformation:
\begin{align}
    \hat{x}_{R,T} = f(x, R, T)
\end{align}
where we denote $\hat{x}_{R,T}$ as the synthesized image. We want our estimated $\hat{x}_{R,T}$  to be perceptually similar to the true but unobserved novel view $x_{R,T}$. 

Novel view synthesis from monocular RGB image is severely under-constrained. Our approach will capitalize on large diffusion models, such as Stable Diffusion, in order to perform this task, since they show extraordinary zero-shot abilities when generating diverse images from text descriptions. Due to the scale of their training data~\cite{schuhmann2022laion}, pre-trained diffusion models are state-of-the-art representations for the natural image distribution today.  

However, there are two challenges that we must overcome to create $f$. Firstly, although large-scale generative models are trained on a large variety of objects in different viewpoints, the representations do not explicitly encode the correspondences between viewpoints. Secondly, generative models inherit viewpoint biases reflected on the Internet. As shown in Figure
\ref{fig:viewpointbias}, Stable Diffusion tends to generate images of forward-facing chairs in canonical poses. These two problems greatly hinder the ability to extract 3D knowledge from large-scale diffusion models.

\begin{figure}
    % TODO: adjust figure text, it's not a neural field
    \centering
    \includegraphics[width=\linewidth]{figures/3d_reconstruction.pdf}
    \caption{\textbf{3D reconstruction with Zero-1-to-3.} Zero-1-to-3 can be used to optimize a neural field for the task of 3D reconstruction from a single image. During training, we randomly sample viewpoints and use Zero-1-to-3 to supervise the 3D reconstruction.
    % \vspace{-0.3cm}
    }
    \label{fig:method_3d}
\end{figure}


\subsection{Learning to Control Camera Viewpoint}

\begin{figure*}
    % TODO: shoe is too dark, increase brightness
    \centering
    \includegraphics[width=\linewidth]{figures/qualitative_gso.pdf}
    \caption{\textbf{Novel view synthesis on Google Scanned Objects~\cite{downs2022google}.} The input view shown on the left is used to synthesize two randomly sampled novel views. Corresponding ground truth views are shown on the right. Compared to the baselines, our synthesized novel view contain rich textual and geometric details that are highly consistent with the ground truth, while baseline methods display a significant loss of high-frequency details.
    % \vspace{-0.3cm}
    }
    \label{fig:qualitative_gso}
\end{figure*}

Since diffusion models have been trained on internet-scale data, their support of the natural image distribution likely covers most viewpoints for most objects, but these viewpoints cannot be controlled in the pre-trained models. Once we are able to \emph{teach} the model a mechanism to control the camera extrinsics with which a photo is captured, then we unlock the ability to perform novel view synthesis.

To this end, given a dataset of paired images and their relative camera extrinsics $\{ \left(x,x_{(R,T)},R,T\right) \}$, our approach, shown in Figure~\ref{fig:method_nvs}, fine-tunes a pre-trained diffusion model in order to learn controls over the camera parameters without destroying the rest of the representation. Following \cite{rombach2022high}, we use a latent diffusion architecture with an encoder $\mathcal{E}$, a denoiser U-Net $\epsilon_{\theta}$, and a decoder $\mathcal{D}$. At the diffusion time step $t \sim [1, 1000]$, let $c(x, R, T)$ be the embedding of the input view and relative camera extrinsics. We then solve for the following objective to fine-tune the model:
\begin{align}
   \min_{\theta}\;  \mathbb{E}_{z \sim \mathcal{E}(x), t, \epsilon \sim \mathcal{N}(0, 1)}||\epsilon - \epsilon_{\theta}(z_t, t, c(x, R, T))||_2^2.
\end{align}
After the model $\epsilon_\theta$ is trained, the inference model $f$ can generate an image by performing iterative denoising from a Gaussian noise image \cite{rombach2022high} conditioned on $c(x, R, T)$. 

The main result of this paper is that fine-tuning pre-trained diffusion models in this way enables them to learn a generic mechanism for controlling the camera viewpoints, which extrapolates outside of the objects seen in the fine-tuning dataset. In other words, this fine-tuning allows controls to be ``bolted on'' and the diffusion model can retain the ability to generate photorealistic images, except now with control of viewpoints. This compositionality establishes zero-shot capabilities in the model, where the final model can synthesize new views for object classes that lack 3D assets and never appear in the fine-tuning set.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/qualitative_rtmv.pdf}
    \caption{\textbf{Novel view synthesis on RTMV~\cite{tremblay2022rtmv}.} The input view shown on the left is used to synthesize 2 randomly sampled novel views. Corresponding ground truth views are shown on the right. Our synthesized view maintains a high fidelity even under big camera viewpoint changes, while most other methods deteriorate in quality drastically.
    % \vspace{-0.3cm}
    }
    \label{fig:qualitative_rtmv}
\end{figure*}

 

\subsection{View-Conditioned Diffusion}

3D reconstruction from a single image requires both low-level perception (depth, shading, texture, etc.) and high-level understanding (type, function, structure, etc.). Therefore, we adopt a hybrid conditioning mechanism. On one stream, a CLIP~\cite{radford2021learning} embedding of the input image is concatenated with $(R, T)$ to form a ``posed CLIP'' embedding $c(x, R, T)$. We apply cross-attention to condition the denoising U-Net, which provides high-level semantic information of the input image. On the other stream, the input image is channel-concatenated with the image being denoised, assisting the model in keeping the identity and details of the object being synthesized. To be able to apply classifier-free guidance~\cite{ho2022classifier}, we follow a similar mechanism proposed in~\cite{brooks2022instructpix2pix}, setting the input image and the posed CLIP embedding to a null vector randomly, and scaling the conditional information during inference.

\subsection{3D Reconstruction}

In many applications, synthesizing novel views of an object is not enough. A full 3D reconstruction capturing both the appearance and geometry of an object is desired. We adopt a recently open-sourced framework, Score Jacobian Chaining (SJC)~\cite{wang2022score}, to optimize a 3D representation with priors from text-to-image diffusion models. However, due to the probabilistic nature of diffusion models, gradient updates are highly stochastic. A crucial technique used in SJC, inspired by DreamFusion~\cite{poole2022dreamfusion}, is to set the classifier-free guidance value to be significantly higher than usual. This methodology decreases the diversity of each sample but improves the fidelity of the reconstruction.

As shown in Figure~\ref{fig:method_3d}, similarly to SJC, we randomly sample viewpoints and perform volumetric rendering. We then perturb the resulting images with Gaussian noise $\epsilon \sim \mathcal{N}(0, 1)$, and denoise them by applying the U-Net $\epsilon_{\theta}$ conditioned on the input image $x$, posed CLIP embedding $c(x, R, T)$, and timestep $t$, in order to approximate the score toward the non-noisy input $x_\pi$:
\begin{equation}
    \nabla \mathcal{L}_{SJC} = \nabla_{I_\pi} \log p_{\sqrt{2}\epsilon}(x_{\pi})
\end{equation}
where $\nabla \mathcal{L}_{SJC}$ is the PAAS score introduced by \cite{wang2022score}.

In addition, we optimize the input view with an MSE loss. To further regularize the NeRF representation, we also apply a depth smoothness loss to every sampled viewpoint, and a near-view consistency loss to regularize the change in appearance between nearby views.

\subsection{Dataset}

We use the recently released \textit{Objaverse}~\cite{deitke2022objaverse} dataset for fine-tuning, which is a large-scale open-source dataset containing 800K+ 3D models created by 100K+ artists. While it has no explicit class labels like ShapeNet~\cite{chang2015shapenet}, Objaverse embodies a large diversity of high-quality 3D models with rich geometry, many of them with fine-grained details and material properties. For each object in the dataset, we randomly sample 12 camera extrinsics matrices $\mathcal{M_e}$ pointing at the center of the object and render 12 views with a ray-tracing engine. At training time, two views can be sampled for each object to form an image pair $(x, x_{R, T})$. The corresponding relative viewpoint transformation $(R, T)$ that defines the mapping between both perspectives can easily be derived from the two extrinsic matrices.

% \BV{todo mention something about off the shelf segmentation for white background somewhere}



% \subsection{Implementation Details}

% \BV{todo mention something about architecture hyperparameters, or at least changes relative to regular LDM/SD, optimizer, learning rate, etc}


