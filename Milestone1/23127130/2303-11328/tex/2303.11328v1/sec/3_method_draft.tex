\section{Method -- Old}


While showing an extraordinary zero-shot ability to generate diverse images from text descriptions, large diffusion models like stable diffusion lack the ability to control the viewpoint of an image it generates. In addition, these models inherit the viewpoint biases existing in the internet data. As shown in figure~\ref{fig:viewpoint}, when prompted with \textit{``a chair''}, Stable Diffusion tends to generate images of forward-facing chairs. These two problems have greatly hinder the ability to extract 3D knowledge from these large-scale diffusion models.
To address these problems, we propose to learn a view-conditioned image-to-image translation model and formulate it as a supervised learning problem. Given a pair of images $\{I_i, I_j\}$ of an object taken by the same camera (same intrinsics) from two different viewpoints (different extrinsics), with relative viewpoint transformation $(R, T)$, an image-to-image translation model $F$ can be defined such that,
\begin{equation}
    \hat{I_j} = F(I_i, R, T)
\end{equation}

our goal is to learn the model $F$, such that an image similarity loss $\mathcal{L}(\hat{I_j}, I_j)$ can be minimized.

Section~\ref{method:dataset} describes our synthetic dataset. Section~\ref{method:nvs} lays out the method for learning novel synthesis diffusion model Zero-1-to-3 with hybrid image and viewpoint conditioning. Section~\ref{method:3d} illustrates the process of optimizing a NeRF with Zero-1-to-3 for 3D reconstruction.

\subsection{Dataset} \label{method:dataset}
Recently released \textit{Objaverse}~\cite{deitke2022objaverse} is a large-scale open-source dataset containing 800K+ 3D models created by 100K+ artists. While without explicit class labels as ShapeNet~\cite{chang2015shapenet}, Objaverse contains a large diversity of high-quality 3D models with rich geometry, some with fine-grained material properties. For each object in the dataset, we randomly sample 12 camera extrinsics pointing at the object and render 12 views with a ray-tracing engine. At training time, two views can be sampled for each object to form an image pair $(I_i, I_j)$, and the corresponding viewpoint transformation $(R, T)$ can be calculated from the two extrinsic matrices.


\subsection{Diffusion-based novel view synthesis} \label{method:nvs}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/nvs.pdf}
    \caption{Novel view synthesis with latent diffusion models}
    \label{fig:method_nvs}
\end{figure}

Denoising Diffusion Probabilistic Model (DDPM) is a type of generative model designed to learn the distribution of training data. Under the context of image generation, a sample is generated by gradually denoising an image with each pixel independently sampled from a Gaussian distribution. In order to generate high-resolution images efficiently, latent diffusion models (LDM)~\cite{rombach2022high} performs the diffusion process only in the latent space of an autoencoder. During training time, at a randomly sampled time step $t$, a noise scheduler blends a target image with Gaussian noise to be predicted by a denoiser UNet~\cite{ronneberger2015u} $\epsilon_{\theta}$. LDM can serve as a conditional generative model with conditioning information $c$ as input to the denoiser.
\begin{equation}
   \mathcal{L} = \mathbb{E}_{z \sim \mathcal{E}(I_i), t, \epsilon \sim \mathcal{N}(0, 1)}||\epsilon - \epsilon_{\theta}(z_t, t, c(I_i, R, T))||_2^2
\end{equation}

where $\mathcal{E}$ and $\mathcal{D}$ denote the encoder and decoder network. In our case, both the input image $I_i$ and a relative view transformation $(R, T)$ are used as conditional information to generate the transformed view.

3D reconstruction from a single image requires both low-level perception (depth, shading, texture, ...) and high-level understanding (type, function, structure, ...). Therefore, we adopted a hybrid conditioning mechanism. On one stream, a CLIP~\cite{radford2021learning} embedding of the input image is concatenated with $(R, T)$ to form a ``posed CLIP'' embedding and applied cross attention to condition the denoiser UNet, which provides high-level semantic priors of the input image. On the other stream, the input image is channel-concatenated with the image being denoised, assisting the model to keep the identity of the object being synthesized/reconstructed. To be able to apply classifier-free guidance~\cite{ho2022classifier}, we follow a similar mechanism proposed in~\cite{brooks2022instructpix2pix}, setting the input image and the posed CLIP embedding to a null vector randomly, and scaled the conditional information during inference.

\subsection{3D Reconstruction with Diffusion Prior} \label{method:3d}
In many applications, synthesizing novel views of an object is not enough. A full 3D reconstruction of both the appearance and geometry of an object are necessary. To this end, we adopted a recently open-sourced framework -- Score Jacobian Chaining (SJC)~\cite{wang2022score} to optimize a 3D representation with priors from text-to-image diffusion model. However, due to the probabilistic nature of diffusion models, gradient updates are highly stochastic. A crucial technique used in SJC was inspired by DreamFusion~\cite{poole2022dreamfusion}, which is setting the classifier-free guidance value to be significantly higher than usual. Intuitively, such a technique decreases the diversity of each sample, thus improving the fidelity of the reconstruction.

Similar to SJC, we randomly sample viewpoints and perform volumetric rendering, which is perturbed with Gaussian noise and denoised by $\epsilon_{\theta}$ conditioned on the input image, posed CLIP embedding, and time to approximate the score on non-noisy input $I_\pi$

\begin{equation}
    PAAS = \nabla_{I_\pi} \log p_{\sqrt{2}\epsilon}(x_{\pi})
\end{equation}

We also optimize the input view with an MSE loss. To further regularize the NeRF representation, we also applied a depth smoothness loss to each sampled view, and a near-view consistency loss to regularize the change in appearance between small viewpoint changes.
