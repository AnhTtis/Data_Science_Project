
\section{Introduction}
\vspace{+0.25cm}
% Large-scale diffusion models contain severer viewpoint bias. When prompted to generate images of chairs, it's extremely unlikely the generated images contain a chair viewed from its back. Recent work in text-to-3D and image-to-3D has suffered from this bias, known as the "Janus Problem". To generate the 3D shape of "a chair" from text, or reconstruct a chair from an image, these methods optimize a NeRF (or its variants) with a languaged-conditioned diffusion model.

% We address the challenging dual problems of zero-shot novel view synthesis and 3D reconstruction from a single uncalibrated RGB image. Due eometr severely under-constrained nature of the problems, we need to rely on both semantic, and geometric priors. Though it's impossible to train a large-scale 3D generative models with an internet-scale 3D dataset today, we believe large diffusion models like \textit{Stable Diffusion}~\cite{rombach2022high}, trained to generate 

%\looseness=-1
From just a single camera view, humans are often able to imagine an object's 3D shape and appearance. This ability is important for everyday tasks, such as object manipulation~\cite{gupta2009observing} and navigation in complex environments~\cite{cheng2005reflections}, but is also key for visual creativity, such as painting~\cite{morriss2010evolution}. While this ability can be partially explained by reliance on geometric priors like symmetry, we seem to be able to generalize to much more challenging objects that break physical and geometric constraints with ease. In fact, we can predict the 3D shape of objects that do not (or even \emph{cannot}) exist in the physical world (see third column in Figure~\ref{fig:teaser}). To achieve this degree of generalization, humans rely on prior knowledge accumulated through a lifetime of visual exploration.

In contrast, most existing approaches for 3D image reconstruction operate in a closed-world setting due to their reliance on expensive 3D annotations (e.g.\ CAD models) or category-specific priors~\cite{pavlakos2019expressive,huang2022planes,park2019deepsdf,zuffi2018lions,Zuffi:CVPR:2017, zuffi2019three,kanazawa2019learning,kanazawa2018learning}. Very recently, several methods have made major strides in the direction of open-world 3D reconstruction by pre-training on large-scale, diverse datasets such as CO3D \cite{reizenstein2021common,mescheder2019occupancy,park2019deepsdf,gao2022get3d}. However, these approaches often still require geometry-related information for training, such as stereo views or camera poses. As a result, the scale and diversity of the data they use remain insignificant compared to the recent Internet-scale text-image collections~\cite{schuhmann2022laion} that enable the success of large diffusion models~\cite{saharia2022photorealistic,rombach2022high,nichol2021glide}. It has been shown that Internet-scale pre-training endows these models with rich semantic priors, but the extent to which they capture geometric information remains largely unexplored.

In this paper, we demonstrate that we are able to learn control mechanisms that manipulate the camera viewpoint in large-scale diffusion models, such as Stable Diffusion \cite{rombach2022high}, in order to perform zero-shot novel view synthesis and 3D shape reconstruction. Given a single RGB image, both of these tasks are severely under-constrained. However, due to the scale of training data available to modern generative models (over 5 billion images), diffusion models are state-of-the-art representations for the natural image distribution, with support that covers a vast number of objects from many viewpoints. Although they are trained on 2D monocular images without any camera correspondences, we can fine-tune the model to learn controls for relative camera rotation and translation during the generation process. These controls allow us to encode arbitrary images that are decoded to a different camera viewpoint of our choosing. Figure~\ref{fig:teaser} shows several examples of our results.

The primary contribution of this paper is to demonstrate that large diffusion models have learned rich 3D priors about the visual world, even though they are only trained on 2D images. We also demonstrate state-of-the-art results for novel view synthesis and state-of-the-art results for zero-shot 3D reconstruction of objects, both from a single RGB image. We begin by briefly reviewing related work in Section~\ref{sec:rel}. In Section~\ref{sec:meth}, we describe our approach to learn controls for camera extrinsics by fine-tuning large diffusion models. Finally, in Section~\ref{sec:exp}, we present several quantitative and qualitative experiments to evaluate zero-shot view synthesis and 3D reconstruction of geometry and appearance from a single image. We will release all code and models as well as an online demo.

% ----------------

% In most cases, humans can imagine the shape and appearance of an object from a single image. Such a simple task remains extremely challenging for modern computer vision models. To be able to do this, humans rely on prior knowledge, which we believe will be necessary for computers as well to achieve comparable performance. Through pretraining on internet-scale text-image pairs, large diffusion models like \textit{Stable Diffusion}~\cite{rombach2022high} have developed rich semantic and geometric priors implicitly. However, such priors remain hard to extract, uncontrolled, and biased.

% In this paper, we aim to train Stable Diffusion to learn an extractable, controllable, and unbiased shape priors. To achieve this, we first create a large-scale dataset from \textit{Objaverse}~\cite{deitke2022objaverse} containing pairs of renderings of a same object, but from two different views, with relative viewpoint transformation recorded. Using these data, we train a conditional diffusion model, that given an input view of an object, and a relative viewpoint transformation, generates the corresponding novel view.

% -------------




% What would the painting of Van Gogh's sunflowers in Figure 1 look like from a different camera angle? 
% Although the painting is just a single view into his world, you can likely imagine the flowers from the top (seeing just petals and pollen), bottom view (seeing just the pot), or any angle in between.  

% State-of-the-ar




% The state-of-the-art methods in view synthesis and object 


% At first glance, synthesizing paintings of world famous artists seems daunting, since no 3D assets of their work exist in the data. 


% Although the exact sunflowers do not exist in the physical world, you are often able to comprehend the 3D object shape. While some of this ability can be explained on geometric priors, we are able 