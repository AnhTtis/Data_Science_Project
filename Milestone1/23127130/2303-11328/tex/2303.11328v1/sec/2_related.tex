
\section{Related Work}
\label{sec:rel}
% \subsection{Neural Radiance Fields} \label{related:nerf}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/viewbias.pdf}
    %\label{fig:viewpoint}
    \caption{
    \textbf{Viewpoint bias in text-to-image models.} We show samples from both Dall-E-2 and Stable Diffusion v2 from the prompt \textit{``a chair"}. Most samples show a chair in a forward-facing canonical pose.
    \vspace{-1em}
%    \vspace{-0.3cm}
    }
    \label{fig:viewpointbias}
\end{figure}

\textbf{3D generative models.}
\label{related:generative}
Recent advancements in generative image architectures combined with large scale image-text datasets~\cite{schuhmann2022laion} have made it possible to synthesize high-fidelity of diverse scenes and objects~\cite{nichol2021glide,ramesh2021zero,saharia2022photorealistic}. In particular, diffusion models have shown to be very effective at learning scalable image generators using a denoising objective~\cite{chen2020wavegrad,sohl2015deep}. However, scaling them to the 3D domain would require large amounts of expensive annotated 3D data. Instead, recent approaches rely on transferring pre-trained large-scale 2D diffusion models to 3D without using any ground truth 3D data. Neural Radiance Fields or NeRFs~\cite{mildenhall2020nerf} have emerged as a powerful representation, thanks to their ability to encode scenes with high fidelity. Typically, NeRF is used for single-scene reconstruction, where many posed images covering the entire scene are provided. The task is then to predict novel views from unobserved angles. DreamFields~\cite{jain2022zero} has shown that NeRF is a more versatile tool that can also be used as the main component in a 3D generative system. Various follow-up works~\cite{poole2022dreamfusion,lin2022magic3d,wang2022score} substitute CLIP for a distillation loss from a 2D diffusion model that is repurposed to generate high-fidelity 3D objects and scenes from text inputs.

Our work explores an unconventional approach to novel-view synthesis, modeling it as a viewpoint-conditioned image-to-image translation task with diffusion models. The learned model can also be combined with 3D distillation to reconstruct 3D shape from a single image. Prior work~\cite{watson2022novel} adopted a similar pipeline but did not demonstrate zero-shot generalization capability. Concurrent approaches~\cite{deng2022nerdi, melas2023realfusion, xu2022neurallift} proposed similar techniques to perform image-to-3D generation using language-guided priors and textual inversion~\cite{gal2022image}. In comparison, our method learns control of viewpoints through a synthetic dataset and demonstrates zero-shot generalization to in-the-wild images.

\textbf{Single-view object reconstruction.}
\label{related:single-view}
Reconstructing 3D objects from a single view is a highly challenging problem that requires strong priors. One line of work builds priors from relying on collections of 3D primitives represented as meshes~\cite{worchel2022multi,xu2019disn}, voxels~\cite{girdhar2016learning,wu2017marrnet}, or point clouds~\cite{fan2017point,mescheder2019occupancy}, and use image encoders for conditioning. These models are constrained by the variety of the used 3D data collection and show poor generalization capabilities due to the global nature of this type of conditioning. Moreover, they require an additional pose estimation step to ensure alignment between the estimated shape and the input. On the other hand, locally conditioned
models~\cite{saito2019pifu,yu2021pixelnerf,wang2021ibrnet,tucker2020single,revealing}
% models~\cite{saito2019pifu,yu2021pixelnerf,wang2021ibrnet,tucker2020single}
aim to use local image features directly for scene reconstruction and show greater cross-domain generalization capabilities, though are generally limited to close-by view reconstructions. Recently, MCC~\cite{wu2023multiview} learns a general-purpose representation for 3D reconstruction from RGB-D views and is trained on large-scale dataset of object-centric videos.

In our work, we demonstrate that rich geometric information can be extracted directly from a pre-trained Stable Diffusion model, alleviating the need for additional depth information.

% A separate body of research focuses on a problem of 3D model generation in different representations.
% Point cloud-based generative models:
% Achlioptas et al.~\cite{achlioptas2018learning} fit generative priors based on GANs~\cite{creswell2018generative} or GMMs to the latent representations of pretrained point cloud autoencoders. Mo et al. propose to use VAE~\cite{kingma2013auto} that encodes shape structure and geometry represented as a hierarchy graphs. Pointflow~\cite{yang2019pointflow} is a two-stage flow model that uses first model for latent generation, ant the second one for sampling points conditioned on the latent. Follow-up works propose to use diffusion models~\cite{luo2021diffusion,cai2020learning,zeng2022lion} at first or both stages. More recently, Point-E~\cite{nichol2022point} introduced a simple one-stage transformer-based model that also incorporates colors.


% \subsection{Large-scale 3D datasets} \label{related:dataset}
