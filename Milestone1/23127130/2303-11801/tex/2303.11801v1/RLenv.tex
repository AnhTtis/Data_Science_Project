\section{Training and Validation Framework} 
We use a standard ROS stack in which the robot knows its position up to the accuracy of the localization system. The robot has a 2D map for fixed, known obstacles and it detects dynamic and unknown static obstacles using a LiDAR sensor. From the raw obstacle information the robot constructs a costmap in the form of an Occupancy Grid using the approach of Lu et al.~\cite{lu2014layered}. The costmap window size for the local planner is $8m\times8m$.

We integrate RL into the robot navigation stack using the framework pioneered by G{\"u}ldenring et al.~\cite{guldenring2019applying,GuldenringGHJ20}. When a new goal is specified the global planner creates a path from the current position to the goal  (Fig.\ref{fig:ROS}). In this work we use without change the standard ROS {\em NavFn} planner based on the Dijkstra search algorithm. The path is found based on the obstacles in the map together with any obstacles seen by the LiDAR at the time of path creation.

Whenever a path is created by the global planner, a {\em waypoint generator} breaks it up into a sequence of waypoints. At all times the local planner maintains a list of 8 waypoints, starting with the one after the waypoint that is closest to the robot. (The method of \cite{guldenring2019applying} sometimes starts the list with the closest waypoint to the robot, but we found that could create excessive ``pingponging'' in the eventual choice of waypoint). From this list of 8, the local planner chooses the first on the list that is not too close to an obstacle. The aim of the RL agent is to move towards the selected waypoint while not hitting any obstacles, {\em including obstacles that appeared after the global plan was computed}.

\subsection{RL Environment} \label{section:Environments}

% RL problems are often formalized as Markov decision processes~(MDPs). An MDP is defined by a tuple $(\mathcal{S}, \mathcal{A}, p, R, \gamma)$, where the set of states $\mathcal{S}$ obey the Markov property $p(s_{t+1}| s_t ) = p(s_{t+1} | s_1,..,s_t )$, $\mathcal{A}$ is a set of actions, $p$ is the state-transition distribution $s_{t+1} \sim p(\cdot|s_t, a_t)$ that represents the probability of transitioning to state $s_{t+1}$ after taking action $a_t$ in state $s_t$ at any timestep $t$, $R(s_t,a_t)$ is the reward function, and $\gamma\in[0,1]$ is the discount factor which reduces the weight given to future rewards.

The RL environment is defined  by a state space $\cS$, an action space $\cA$, and a reward function $R(\cdot,\cdot)$. When the RL agent takes action $a \in \cA$ in state $s \in \cS$, it gains reward $R(s, a)$ and moves to a new state $s'$ according to some state-transition distribution $s' \sim p(\cdot|s, a)$.
The actions are linear/angular velocity pairs $(v,\omega)$. The state space is defined by the positions of the next waypoint and the local obstacles relative to the current robot position. We represent the state with an image since this allows us to utilize the convolutional deep RL architectures that have worked well for visually-rich environments such as Atari video games and some robot control tasks.
% (we defined such architectures in the next section). 
In addition, using such game-like image states is a convenient way to merge the information from the waypoint position, the static objects from the map, and the dynamic obstacles sensed by the LiDAR. %(note that most of the nearby map obstacles will also be sensed by the LiDAR).

%We have two ways to represent the state as an image (illustrated in Fig.\ref{fig:ROS}), both of which are derived from the ROS costmap. The first, that we refer to as the {\em Cartesian costmap} simply replicates the occupancy grid representation of the costmap with red regions denoting the obstacles. The robot is assumed to be at the center of the costmap and the next waypoint is represented with a white square. 
%{\sc Need to define the orientation.} 
Specifically, our RL state is an image that we refer to as the {\em polar costmap}. (See Fig.~\ref{fig:ROS}.) It is generated by converting the Occupancy Grid representation of the ROS costmap and the next waypoint to polar coordinates. The horizontal axis represents distance from the robot and the vertical axis represents angle. Obstacles are presented in red and the next waypoint is a white square. 
% as in Fig.\ref{fig:cart_vs_polar}. 
The motivation for using a polar representation is that it matches the linear/angular velocities that form the action. The state transition naturally follows from the robot movement after an action is taken.
\begin{figure}[!t]
	\centering
	\subfigure{\includegraphics[height=0.13\textwidth]{global_unix.png}}\hspace{2mm}
	\subfigure{\includegraphics[height=0.13\textwidth]{unix_polar_new.png}}
	\vspace{-1mm}
	\caption{ROS framework with global map, and polar costmap. The black square represents an obstacle that appeared after the global plan was computed.}\label{fig:ROS}
\end{figure}

It remains to define the reward function $R(s,a)$ for taking action $a$ in state $s$. We employ a mix of both dense and sparse rewards. For a given state $s$, let $(d_{\mathrm{old}}, \theta_{\mathrm{old}})$ be the distance and bearing
to the next waypoint in state $s$, let $s'$ be the new state after taking action $a$, 
and let $(d_{\mathrm{new}}$, $\theta_{\mathrm{new}})$
be the distance and bearing in state $s'$. Here the bearing is defined to be the difference between the angle to the waypoint and the current yaw. 
We define:
$$
\begin{array}{rl}
    & R(s,a)=\left(d_{\mathrm{old}}- d_{\mathrm{new}}\right) \cdot\left(1 \mbox{~if } d_{\mathrm{old}}- d_{\mathrm{new}} \ge 0, \mbox{~else } 2 \right) \\
    &+\left(|\theta_{\mathrm{old}}|-|\theta_{\mathrm{new}}|\right)\cdot\left(1 \mbox{~if } |\theta_{\mathrm{old}}|-|\theta_{\mathrm{new}}| \ge 0, \mbox{~else } 2 \right) \\
    &-R_\mathrm{max} \cdot\left( 1 \mbox{~if collision, else } 0 \right)  \\
    &+R_\mathrm{max} \cdot\left( 1 \mbox{~if } d_{\mathrm{new}}=0, \mbox{~else } 0\right)\\&-G(s'),
  \label{eq:rew}
\end{array}
$$
where $R_\mathrm{max}$ is a fixed reward/penalty for reaching the waypoint and colliding with an obstacle, respectively, and $G(s')$ is the product of a truncated Gaussian kernel centered at the robot location and the Occupancy Grid in state $s'$. 

The first two terms of $R(s,a)$ incentivize getting closer to the waypoint both in terms of distance and bearing. Note that the penalty for moving away from the waypoint (both in distance and bearing) is double the reward for moving towards it. Hence there is a net penalty for moving away from the waypoint and then back towards it. We have found that this ``doubling the penalty for negative progress'' has a significant effect on encouraging the agent to move directly to the waypoint if there are no obstacles in the way. The final Gaussian term penalizes movement towards an obstacle. 

\begin{figure}[h]
	\centering
	\subfigure{\label{fig:dummy_cart}
	\includegraphics[height=0.12\textwidth]{dummy_cart.png}}%\hspace{-1mm}
	\subfigure{\label{fig:dummy_polar}
	\includegraphics[height=0.12\textwidth]{dummy_polar.png}}
	\vspace{-2mm}
	\caption{Dummy training environment (left) with polar costmap (right).}\label{fig:dummy}	
\end{figure}

We find that it is more efficient to train our RL agents on a ``dummy'' training environment that does not require the full complexity of ROS or a detailed physics simulation. For this dummy training environment we place a robot start position and a single waypoint in an environment with obstacles as shown in Fig.\ref{fig:dummy}. The robot is the blue square, the waypoint is the red square, and the larger green square around the robot is the support of the truncated Gaussian kernel. For each episode in the RL training, we pick an obstacle configuration and then use the above reward to encourage the RL agent to move towards the waypoint without hitting obstacles. %For some episodes all obstacles are static and for others we also move dynamic obstacles during the training. 
Once the agent is trained we can run it directly in our ROS environment (either a Gazebo simulation or on real robots) since the state definition is the same in all cases. We remark, however, that the specific obstacle configurations on which we do the training are {\em not} the same as the configurations on which we do our eventual experiments, since we want trained agents that generalize to any unseen obstacle configuration. 

