\section{Performance of RL Training}
\label{sec:training}

We now evaluate the training performance of RAD, DrQ and other baseline RL methods in our dummy training environment. We train the RL agents on our polar costmap environment from  Section \ref{section:Environments}, and compare against Cartesian costmap environments (similar to \cite{guldenring2019applying}) where we do not convert to polar coordinates before generating the image. We train for 10,000 episodes with the hyper-parameter values listed in Table \ref{tab:hyperparameters}, %using an RTX A4000 Nvidia GPU --> all models are not trained on this GPU 
% comparison of polar and cartesian costmap behaviour
% for cartesian costmaps, the agent overshoots the goal and loses sight of the goal
The trained agents are evaluated over 1000 episodes in the training environment. We define the success rate as the percentage of episodes in which the agent reached the goal. The collision rate is defined as the percentage of episodes where the agent collides with the obstacles. An episode can be neither a success nor a collision if the robot stops and the episode times out. % should we mention that we exclude impossible scenarios?
\begin{table}[t!]
    \centering
    \caption{Hyper-parameter values for SAC agent training.}
    \begin{tabular}{l|c}
        \toprule
        Hyper-parameter & Value \\
        \midrule
        Training episodes &  10000\\
        Random exploration episodes & 10\\
        %Max. steps per episode & 200\\
        Mini-batch size & 128\\
        Replay buffer capacity & $10^6$\\
        Discount factor $\gamma$ & 0.99\\
        Optimizer & Adam\\
        Learning rate & $0.001$\\
        Critic target update frequency & 2\\
        Critic target update rate $\tau$ & $0.01$\\
        Actor update frequency & 2\\
        \bottomrule
    \end{tabular}
    
    \label{tab:hyperparameters}
\end{table}

\begin{table}[t]
	\caption{Comparison of Cartesian and polar costmaps using RAD agent in the dummy environment.}
	\label{tab:cartesian_vs_polar}
    \centering
	\begin{tabular}{l|l|r|r}
	\toprule
	\multirow{2}{*}{Costmap} & Orientation & Success & Collision \\
	& Information & Rate & Rate\\
	\midrule
	Polar & Implicit & 98.7\% & 0.08\% \\
	\midrule
	\multirow{3}{*}{Cartesian} & Rotation & 42.0\% & 37.7\% \\
	& Arrow & 45.5\% & 36.0\% \\
	& Channel & 65.3\% &25.9\% \\
	\bottomrule
	\end{tabular}
\end{table}

\begin{table}[t]
	\caption{Comparison of RL agents in the dummy environment.}
	\label{tab:rl_agents}
    \centering
	\begin{tabular}{l|r|r}
	\toprule
	Method & Success Rate & Collision Rate \\
	\midrule
    %DQN & 46.3\% & 4.74\% \\ %500M steps
    DQN & 33.9\% & 51.2\% \\
    PPO & 83.6\% & 7.5\% \\
    SAC from LiDAR & 34.2\% & 47.5\% \\
    DWA-RL with SAC & 7.3\%  & 70.3\% \\
	RAD & 98.7\% & 0.08\% \\
    \textbf{DrQ} & \textbf{99.4\%} & \textbf{0.02\%} \\
	\bottomrule
	\end{tabular}
\end{table}

In Table~\ref{tab:cartesian_vs_polar}, we compare the polar and Cartesian costmaps using the RAD version of SAC.
While the information regarding the robot's orientation is implicit in the polar costmap, this information is missing in the Cartesian costmap. 
We explored three ways to represent this: (i) rotating the Cartesian costmap by the robot orientation angle, (ii) drawing an arrow at the center of the costmap to denote the robot orientation, or (iii) appending an extra channel to the costmap with the robot orientation angle.
% conveyed using rotation of the local costmap, drawing an arrow at the center of the costmap and by appending an extra channel to the image containing the robot's orientation.
The agent with polar costmap observations significantly outperforms those with Cartesian costmap observations. We hypothesize that this is because the polar costmaps better match the action space of the robot and also implicitly represent the robot orientation information, which allows for better generalization.
%We observed that RAD agents trained using the Cartesian costmap tend to overshoot the goal, which results in an episode timeout or collision in the majority of the episodes. 
We use the better performing polar costmap environment in the rest of our experiments.

We next compare the performance of RAD, DrQ (with K=2), and the following RL baselines in Table~\ref{tab:rl_agents}:

\noindent $\bullet$ \textbf{DQN}. To evaluate if discrete control is easier to learn, we discretize the action space of the robot with six possible linear/angular velocity pair combinations and train a standard DQN agent from the stable baselines library \cite{stable-baselines3}.
%on this discretized dummy environment.

\noindent $\bullet$ \textbf{PPO}. To evaluate if the SAC agents perform better than other actor-critic algorithms, we also compare against the popular PPO agent from the stable baselines library \cite{stable-baselines3}.

\noindent $\bullet$ \textbf{SAC from raw LiDAR observations}. To evaluate the importance of image-based game-like states, we compare against a SAC agent trained on raw LiDAR observations (similar to \cite{KastnerML20,liu2020robot}). For this agent, the actor and critic networks receive state vectors consisting of the raw LiDAR readings and the coordinates of the next waypoint. 

\noindent $\bullet$ \textbf{DWA-RL with SAC}. To evaluate if it is beneficial to combine the standard DWA planner with RL, we implement the observation space and reward function of the DWA-RL method \cite{patel2021dwa} and train our SAC agent on this hybrid setup.

% Table~\ref{tab:dummyenv_results} compares the polar SAC agents trained with RAD and DrQ (with K=2 augmentations per observation) agents. 
The DrQ method achieves the highest success rate ($> 99\%$) with the fewest collisions. 
We also experimented with stacking four consecutive frames as observations to the DrQ method but observed that these agents tend to have trouble navigating around obstacles, reducing the success rate to $94.9\%$.
We note that the success rates we obtain with the baseline algorithms are lower than those observed in the literature~\cite{guldenring2019applying,patel2021dwa,KastnerML20,liu2020robot}. We believe this is partly because we only run for 10,000 episodes (which corresponds to $<500000$ steps). However, this is sufficient for training the DrQ agent and demonstrates the sample-efficiency of this variant of SAC. Another potential reason is that our training environment contains challenging scenarios requiring tight turns (see Fig.~\ref{fig:dummy}), but this is necessary to obtain agents that will work for the real-world cases described below. 

% stable baselines comparisons TODO
% Rinu: remove Average reward to fit table into column?
% \begin{table}[t!]
% 	\caption{Comparison of RL Agents in the Dummy Environment}
% 	\label{tab:dummyenv_results}
% 	\begin{tabular}{l|l|l|r|r|r}
% 	\hline
	
% 	Costmap&Method&Orientation&Success&Collision& Average\\
% 	&&information&Rate &Rate&Reward\\\hline
% 	\multirow{2}{*}{Polar} & DrQ &\multirow{3}{*}{Implicit}	 & 99.4\% & 0.2\%&40.97 \\\cline{2-2}\cline{4-6}	
% 	 &  RAD & & 98.7\% & 0.8\%&40.01 \\\cline{1-2}\cline{4-6}
% 	 Polar (stacked)& DrQ &  & 94.9\% & 0.06\%&39.32 \\\cline{1-6}
% 	\multirow{3}{*}{Cartesian} & RAD & Rotation & 42.0\% & 37.7\% & -6.29 \\\cline{2-6}
% 	 & RAD & Arrow & 45.5\% & 36.0\%&-4.11 \\\cline{2-6}
% 	 & RAD & Channel & 65.3\% &25.9\%&13.39 \\\cline{1-6}
% 	\end{tabular}
% \end{table}

% \begin{table}[t!]
% 	\caption{Comparison of RL Agents in the Dummy Environment}
% 	\label{tab:dummyenv_results}
% 	\centering
% 	\begin{tabular}{|l|l|l|r|r|}
% 	\hline
% 	Costmap&Method&Orientation&Success&Collision\\
% 	&&information&Rate &Rate\\\hline
% 	\multirow{2}{*}{Polar} & DrQ &\multirow{3}{*}{Implicit}	 & 99.4\% & 0.02\%\\\cline{2-2}\cline{4-5}	
% 	 &  RAD & & 98.7\% & 0.08\% \\\cline{1-2}\cline{4-5}
% 	 Polar (stacked)& DrQ &  & 94.9\% & 0.06\% \\\cline{1-5}
% 	\multirow{3}{*}{Cartesian} & RAD & Rotation & 42.0\% & 37.7\% \\\cline{2-5}
% 	 & RAD & Arrow & 45.5\% & 36.0\% \\\cline{2-5}
% 	 & RAD & Channel & 65.3\% &25.9\% \\\cline{1-5}
% 	\end{tabular}
% \end{table}
