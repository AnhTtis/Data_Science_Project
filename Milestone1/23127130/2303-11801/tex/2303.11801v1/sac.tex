\section{Soft Actor Critic Algorithm}
% start with actor-critic?
% What is the reason for using soft actor critic? soft value function
% table summarizing refinements?
%Reinforcement learning (RL) aims to learn intelligent behaviors through interactions with the environment.
%The policy function $a \sim \pi(\cdot|s)$ of an RL agent is a mapping from states to actions and defines the behavior of the agent.
The objective in RL is to maximize the expected sum of rewards that the agent will receive in the future: $G = \E [ \sum_{t=0}^\infty\gamma^t R(s_t, a_t) ]$,
%\begin{align*}
%    G = \E \left[ \sum_{t=0}^\infty\gamma^t R(s_t, a_t) \right] \,,
%\end{align*}
where the expectation is taken over the agent policy $a_t \sim \pi(\cdot|s_t)$ and the state transition function $s_{t+1} \sim p(\cdot|s_t, a_t)$. The parameter $\gamma \in (0,1]$ is a discount factor used to reduce the weight given to future rewards.
% $a_t$ is the action taken by the agent at timestep $t$, $s_{t+1} \sim p(\cdot|s_t, a_t)$ is the state transition function, and $\gamma \in [0,1]$ is a discount factor which reduces the weight given to future rewards. 

%The long-term reward (return) $G_t$ is given by the equation \ref{eq:return}
%\begin{equation}\label{eq:return}
%    G_t = \Sigma_{k=0}^\infty\gamma^k\ r_{t+k+1}
%\end{equation}
%is used to calculate the long-term reward (return) at a given time-step $t$.

Continuous control problems, such as the local navigation task considered in this paper, are often approached using actor-critic algorithms that learn two functions called the actor and the critic. The actor is a policy function $a \sim \pi_\theta(\cdot|s)$ with parameters $\theta$. % defines the behavior of the agent. 
The critic $Q_\phi(s, a)$ with parameters $\phi$ estimates the action-value function 
% $Q^\pi(s, a)$ 
$Q^\pi(s, a) = \E [ \sum_{t=0}^\infty\gamma^t R(s_t, a_t) | s_0 = s, a_0 = a ]$
of policy $\pi$, which is the expected cumulative reward after taking action $a$ in state $s$ and following policy $\pi$ after that.
% \begin{align*}
%     Q^\pi(s, a) = \E \left[ \sum_{t=0}^\infty\gamma^t R(s_t, a_t) \bigg| s_0 = s, a_0 = a \right] \,.
% \end{align*}
% Actor-critic algorithms are a class of RL algorithms which consist of two networks- the actor and the critic. 
% The actor observes the state of the environment and outputs an action while the critic evaluates how good the action was \cite{actorcritic}. 

In this work, we use a state-of-the-art off-policy actor-critic algorithm called Soft Actor-Critic (SAC) \cite{haarnoja2018softa, haarnoja2018softb}. It is based on the maximum entropy RL framework which augments the standard RL objective with an entropy maximization objective: $G = \E [ \sum_{t=0}^\infty\gamma^t ( R(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))) ]$,
% \begin{align*}
%     G = \E \left[ \sum_{t=0}^\infty\gamma^t \Big( R(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t)) \Big) \right] \,,
% \end{align*}
where $\alpha$ is a learnable temperature parameter that balances the importance of both objectives. The entropy maximization motivates the agent to succeed at the task while acting as randomly as possible, aiding exploration.
% which aims to maximize the entropy in addition to the standard RL objective. We want the RL agent to succeed at the task while acting as randomly as possible which incentivizes the policy to explore more widely.

% A policy $\pi(a|s)$ is a probability distribution over the set of actions $A$ given the current state $s\in S$. We use a neural network to learn the mean and standard deviation of the policy. The action-value function, also referred to as the Q-function $Q^\pi(s,a)$ is the expected return when starting in a state $s$, taking the action $a$ and following $\pi$. In this work, the “critic” estimates the two Q-functions while the “actor” updates a Gaussian policy in the direction suggested by the critic. %Note that we take a minimum of the two Q values. 
%The state-value function $V^\pi(s)$ for a policy $\pi$ is the expected return when starting in a state $s$ and following $\pi$.of taking action $a$ in state $s$ under $\pi$ is the expected return when starting in a state $s$, taking the action $a$ and following $\pi$. In the actor-critic framework, the “critic” estimates the value function (the Q value or the V value) while the “actor” updates the policy in the direction suggested by the critic.
% \begin{equation}
%     Q(s,a) = \mathbb{E}_\pi [G_t | s_t=s, a_t=a]
% \end{equation}

In SAC, the actor and critic functions are parameterized as deep neural networks. The actor is a Gaussian policy with the mean and diagonal covariance parameters produced by the neural network.
% The policy and twin Q functions, parameterized by $\theta$, $\phi_1$ and $\phi_2$ respectively, 
The actor and critic networks are updated by sampling minibatches of $(s_t, a_t, r_t, s_{t+1}, d_t)$ transitions 
%$\mathbb{B}$ 
from a replay buffer $\mathcal{D}$, where $d_t$ is a terminal signal denoting the end of the episode.
% The batch of transitions consist of the current state $s$, the action $a$, the next state $s_t+1$, the reward $R$ and the done signal $d$ represented by the tuple $(s, a, R, s_{t+1}, d)$. 
The parameters for the critic network $Q_\phi$ are trained to minimize the soft Bellman residual:
\begin{equation*}
    J_Q(\phi) = \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}, d_t) \sim \mathcal{D}} \left[ Q_\phi(s_t, a_t) - y_t \right]^2 \,,
\end{equation*}
where the learning target $y_t$ is
\begin{equation*}
y_t = r_t + \gamma (1 - d_t) V(s_{t+1}) \,,
\end{equation*}
and the soft value function
\begin{equation}
V(s_t) = E_{a_t \sim \pi(\cdot|s_t)} \left[ Q_{\bar{\phi}}(s_t, a_t) - \alpha \log \pi(a_t|s_t) \right]
\label{eq:v}
\end{equation}
is approximated using a Monte Carlo estimate of the policy $\pi_\theta$ and a target Q network $Q_{\bar{\phi}}(s_t, a_t)$ whose parameters $\bar{\phi}$ is maintained as the exponentially moving average of the Q network parameters $\phi$.
SAC also makes use of clipped double Q-learning \cite{fujimoto2018addressing}, where the Q estimates are computed as the minimum value of an ensemble of two critic networks with different initializations trained on the same data. This helps prevent overestimation bias in Q-learning with non-linear function approximators.

% The target for the Q function $y$ is defined as
% \begin{multline}
%     y(R, s_{t+1}) = R + \gamma(1-d)(\min(Q_\phi_i(s_{t+1},a_{\theta}))-\\ \alpha \log \pi(a_{t+1}|s_{t+1})), a_{\theta} \sim \pi(.|s_{t+1})
% \end{multline}
%value function $V_\psi^-$ is implicitly parameterized through the soft Q-function $Q_\phi^-$ which has been shown to stabilize training. 
% \begin{equation}
%     V(s_t) = \mathbb{E}_{a_t\sim\pi}[Q_\phi(s_t, a_t)- \alpha log \pi(a_t|s_t)] 
% \end{equation}
% Finally, the policy is learned by minimizing the divergence from the exponential of the soft-Q function at the same states
% \begin{equation}
% J_\pi(\theta)=- \frac{1}{\mathbb{B}} \sum_{s\in\mathbb{B}}[\min(Q_\phi_i(s,a_{\theta})-\alpha \log \pi_\theta (a_{\theta}|s_t)]    
% \end{equation}

The parameters of the actor/policy network $\pi_\theta$ are updated to maximize the maximum entropy RL objective:
\begin{equation*}
J_\pi(\theta) = \E_{s_t \sim \mathcal{D}} \left[ \E_{a_t \sim \pi_\theta(\cdot|s_t)} \left[ \alpha \log \pi_\theta(a_t|s_t) - Q_\phi(s_t, a_t) \right] \right] \,.
\end{equation*}

The learnable temperature parameter $\alpha$ can be automatically updated such that the policy network satisfies a minimum expected entropy constraint. See \cite{haarnoja2018softb} for more details.

While SAC often performs well on continuous control tasks with low-dimensional observations, learning a mapping from high-dimensional states (images) to continuous actions (linear and angular velocities) typically requires massive amounts of robot-environment interactions.
%high dimensional inputs such as images is challenging. 
This is because the agent must learn to extract the right information from the images to successfully perform the task at hand. SAC with a convolutional encoder can be used to learn low-dimensional representations of image observations, which are then provided to the actor and critic networks. However, this often fails. Sample-efficient learning of SAC agents from image observations requires additional supervision such as input reconstruction~\cite{yarats2021improving}, contrastive representation learning~\cite{srinivas2020curl}, or image augmentations~\cite{laskin2020reinforcement, kostrikov2020image}. 

In this work, we consider the recently proposed RAD~\cite{laskin2020reinforcement} and DrQ~\cite{kostrikov2020image} methods that apply image augmentations for sample-efficient learning of continuous control policies from image observations. 
% Both RAD and DrQ propose to augment the image observations by randomly shifting the images by a few pixels. 
In RAD and DrQ, the image observations are transformed with a random shift before each forward pass on the convolutional encoder. DrQ further proposes to average the Q-learning targets in Eq.~\ref{eq:v} over $K$ image transformations. This reduces the variance in the learning targets of the critic, improving the stability and efficiency of learning.

% Real-world implementations require months of training. Therefore, optimizing performance with limited environment interaction is pivotal to the applicability of RL in real world scenarios. 
% To exploit the structure of MDP, we use optimality invariant state transformation suggested in CITEDRQ. An optimality invariant state transformation $\mathbb{f}: \mathbb{S}\times\mathbb{T}\rightarrow\mathbb{v}$ is a mapping that preserves Q values as shown in (\ref{eq:Q_tranform}). An example of
% is random image translation (RandomCrop).
% \begin{equation}\label{eq:Q_transform}
%     Q(s,a) = Q(\mathbb{v}, a)
% \end{equation}

% This allows the creation of K surrogate states by applying transformations to each state sampled from the replay buffer, $s\sim\mathbb{D}$ at every update step. We average the target values over K transformations which reduces the variance of the Q function. 
% \begin{multline}
%     \mathbb{y}(R,s_{t+1},d)=R + \frac{\gamma(1-d)}{K}\\\Sigma_{k\sim K}(\min(Q_\phi_i(s_{t+1}^k,a_{\theta}^k))- \alpha \log \pi(a_{t+1}|s_{t+1}^k)) 
% \end{multline}
% Additionally, we generate M transformations to compute the Q function values used for calculating the critic loss.
% \begin{multline}
%     J_Q(\psi)= \frac{1}{\mathbb{B}}\Sigma_{m\sim M,\mathbb{B}} (Q_\phi_i(m,a) - \mathbb{y}(R, s_{t+1}^m, d))^2
% \end{multline}
% For K=1 and M=1, these augmentations reduce to image transformations proposed in CITERAD.

% Summary of the RandomCrop Augmentation
We apply random shift image augmentation (by $\pm4$ pixels) to the costmap observations.
% in our local navigation task. For example, the polar costmap images are $116 \times 68$. We pad each side by repeating the bounding pixels to get an image of size $120 \times 72$. Then we randomly crop the image to the original size $116 \times 68$, yielding the original image shifted by $\pm4$ pixels. 
% summary of encoder architecture
The augmented images are passed to a convolutional encoder consisting of four $3\times3$ convolutional layers with 32 filters and a stride of 1 followed by ReLU activation. The output of the final convolutional layer is flattened and passed to a fully connected layer, followed by layer normalization and tanh activation to yield a 50-dimensional state representation. 
%summary of policy and critic network
The actor and critic networks use the same MLP architecture with 4 fully connected layers of 1024 hidden units. The actor predicts the mean and diagonal covariance of the Gaussian policy based on the encoded state vector. The critic networks predict the scalar state-action values based on the encoded state vector and an action vector.
% Encoder and critic target networks
% Lastly, we use target networks for the encoder and the critic weights to  stabilize the training
Following previous works \cite{yarats2021improving, srinivas2020curl, laskin2020reinforcement, kostrikov2020image}, we train the convolutional encoder network using only the critic loss and then detach the network parameters from the actor loss for improved training stability.

%We use image augmentation proposed in CITERAD to learn directly from images without the need for learning simplified representations separately. For greater sample efficiency, we use the work in CITEDRQ which introduces a framework for regularizing the value function through transformations of the input state. These transformations create surrogate states to reduce the variance of Q-function. Two regularization methods have been combined by averaging the Q target over K image transformations and averaging the Q function over M image transformations. The averaged values are used to compute the critic loss for learning the Q-function.

% image is high dimensional. encoder produces a 50 feature output. 

%{'At goal': 986, 'Episode timeout': 11, 'Collision': 3} 40.96621973674467
% 9 impossible situations
%{'At goal': 986, 'Episode timeout': 8, 'Collision': 2} 40.96621973674467

% {'At goal': 978, 'Collision': 9, 'Episode timeout': 11, 'Lost sight of the goal': 2} 40.05592110671654
% {'Collision': 487, 'At goal': 417, 'Lost sight of the goal': 78, 'Episode timeout': 18} -6.293488300160666

