\subsection{Ablation Study}
We introduce two variations of experiments with the SAC agent in test case (C2) to investigate the impact of (i) a different training input scheme and (ii) the ROS navigation tuning. 

\noindent $\bullet$ \textbf{(i):} We remark that agents for video game playing typically benefit from training on a ``stacked stack'' in which a set of consecutive images from the game are concatenated to form the eventual state. The idea of stacking frames is that this captures motion within the game. We experiment the agent trained with stacking the $10$ previous state frames in order to capture the motion of obstacles.  

\noindent $\bullet$ \textbf{(ii):} In ROS navigation stack, the inflation layer is where obstacles are inflated to calculate cost for each 2D costmap cell \cite{zheng2021ros}. Local costmap is generated by inflating obstacles detected by the robotâ€™s sensors in
real time. The default local costmap's inflation layer parameters are $1.5$ for the cost scaling factor (higher will make the decay curve more steep) and $1$ for the inflation radius (controls how far away the zero cost point is from the obstacle). We replaced these either both $0.5$ or $0.2$ combinations, which provide more free space around obstacle with gradual decay. 


\noindent $\bullet$ \textbf{Results:} From Fig.\ref{fig:ablation} we observed followings: (i) The agent trained with $10$ stacked frames performs poorly than the one from single frame training. We believe this is due to a short sampling frequency $5$Hz that results in the consecutive frames too similar to each other and the unexpected obstacle could not be learned by previous trajectories. (ii) With lower inflation parameters, the agent could get closer to the obstacle therefore shown less back-off duration however could more cause collisions.

\begin{figure}[!b]
	\centering
	\includegraphics[width=0.35\textwidth]{ablation.png}
	\vspace{-2mm}
	\caption{Impact of the stacked frames and inflation layer tuning on SAC agent for test case (C2).}\label{fig:ablation}
\end{figure}
%
%We train the agent using a single frame and a stacked 10-frames' input states.
%A single frame is the polar form of the LiDAR scan that shows the polar angle and distance of obstacles in front of the robot. 
%In the stacked case, we store the previous 9 frames from the same episode and the current frame, and pass them as a single state to the agent.
%With stacked States, our aim is to allow the agent to see the trajectories of moving obstacles and learn a better navigation policy around them.
%At the beginning of a new episode, we add new frames to the stacked state in a FIFO manner. 
%
%We show the training progression in Figure~\ref{fig:tensorboard_images} *EPISODE REWARD FIGURE FROM TENSORBOARD FIGURE*. It can be seen that the training converges to the optimal policy within 1200 episodes. 
%The agent achieves the highest reward of 40 and was able to reach goal points in *percentage of successful episodes with collisions* of all training episodes.
