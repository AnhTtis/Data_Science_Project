\section{Introduction}
We study the efficacy of Reinforcement Learning (RL) algorithms for obstacle avoidance and local planning in ROS-based robotics
systems. RL algorithms are able to learn optimal actions based on a current state and a reward function.
The purpose of the ROS local planner is to adhere to a global
path to the current robot goal while avoiding local obstacles (which
may be dynamic). The RL paradigm is attractive for
such a problem since the behavior of an RL agent does not have to be
explicitly programmed for every possible scenario. 
In the RL framework, we specify the reward function, state space, and permissible actions the robot can take. The goal is to obtain a near-optimal planning policy given sufficient training samples. RL agents can potentially exhibit more complex (and hence more responsive) 
behavior than traditional local planners such as the Dynamic Window
Approach (DWA) to Collision Avoidance~\cite{fox1997dynamic}. 
%(We provide definitions of these algorithms below.)

RL has recently seen many advances due to the emergence of Deep RL,
where the actions are chosen from a policy parametrized by a 
Deep Neural Network (DNN). One notable success of Deep RL is in
learning policies for game environments (e.g.\ Atari games) modeled as Markov Decision Processes (MDPs) and standardized as {\em OpenAI Gym} environments~\cite{MnihKSGAWR13}. As a result
of this success, multiple authors have examined how Deep RL can be applied to robot control~\cite{GuldenringGHJ20,patel2021dwa,KastnerML20,liu2020robot}.

However, these works raise a number of questions that we address in our study. First, they typically measure performance via
an {\em episodic success} criterion, e.g.\ does the robot
reach the goal, does it suffer any collisions etc? We are also
interested in the {\em quality} of the trajectory. 
% We are also interested in the quality of the trajectory during our real-world experiments.
Is it smooth? How does it back off from an obstacle? Second, many of these papers address
challenging environments where success rates are significantly below
90\%. We believe such performance is unacceptable for practical
deployments. Therefore, we are interested in how to achieve near 100\%
success rates even in complex scenarios. % where that is achievable.  
Third, there are alternative obstacle-avoidance algorithms that do not use RL and we
would like to quantify the benefits and drawbacks of using an RL-based
approach. Lastly, we would like to know which specific RL techniques
produce the best performance.

%We focus on a cloud-based ROS environment in which most ROS
%nodes are migrated to an edge cloud. 
%We assume a physical
%configuration in which a map is available for the permanent
%configuration but other dynamic obstacles may also be present. We
%consider a single robot that must navigate to a sequence of goals
%without hitting any obstacles. When a goal is specified, the ROS
%{\em global planner} calculates a global route from the start position to
%the goal. The {\em local planner} is then responsible for following %the global plan without hitting any obstacles.
We follow \cite{guldenring2019applying,GuldenringGHJ20} and use ROS together with a {\em waypoint generator} that specifies a {\em next} waypoint based on the current robot location and a global plan to the goal. The task of the RL local planner is to reach this next waypoint without hitting any static or dynamic obstacles. Our RL state is an image representation of the obstacles and the next waypoint in polar coordinates. It mimics the image states used in the OpenAI gym environments for Atari games. 
%Our robot is equipped with a LiDAR scanner and
%the obstacle information is transmitted to the RL agent in the form
%of a costmap which we convert to polar coordinates. (An exact
%definition of this {\em polar costmap} is given in Section~\ref{}.)
%The trained RL agent then specifies a translational and rotational
%velocity pair $(v,\omega)$.  The RL state is an image formed from the
%polar costmap and the next waypoint. Since the state is an image we
%can utilize the RL frameworks that have been so effective for video
%games. (We refer to our RL state as a {\em gamelike state}.  We train
%within two environments. The first is a {\em dummyenv} that can
%produce polar costmaps from robot trajectories without the full ROS
%machinery. The second is a full-blown simulated ROS environment.
%With this setup we focus on the following questions.
We train our agents in a simulator with sample maps, and then upload the trained agents onto the robot for testing in the real world. With this setup, we list our contributions as follows:\\
$\bullet$ We show that modern variants of the Soft Actor-Critic (SAC) RL algorithm such as Reinforcement Learning with Augmented Data (RAD)~\cite{laskin2020reinforcement} and Data-regularized Q (DrQ)~\cite{kostrikov2020image} give significantly improved 
% training 
performance compared to earlier RL algorithms and implementations, and achieve success rates close to 100\% after only 10,000 episodes. We refer to the resulting local planner as {\em SACPlanner}.\\
$\bullet$ We demonstrate that polar image state representations outperform natural alternatives.\\
%We also observe that multi-stacking frames in the RL state does not improve performance.\\
$\bullet$ We analyze the trajectories produced by SACPlanner on real-world robots. (Prior work mostly limited trajectory analysis to simulations with perfect localization etc.) We compare with trajectories produced by DWA and a shortest-path based local planner. In all cases with an unexpected or dynamic obstacle, SACPlanner is much more reactive and hence performs better. The trade-off is a less smooth trajectory when the local planner simply has to follow the global plan.  
%$\bullet$ We observe that multi-stacking frames in the RL state leads to less responsive behavior by SACPlanner, even for dynamic obstacles. 
%$\bullet$ How does training with image-based gamelike states compare to other types of RL states that have been proposed in the literature?\\
%$\bullet$ How does training in an artificial environment like dummyenv compare to training directly in a ROS environment?\\
%$\bullet$ How can we improve training with modern variants of the Soft Actor Critic algorithm such as RAD~\cite{} and DrQ~\cite{}.\\
%$\bullet$ To what extent does multi-stacking frames improve the peformance with dynamic obstacles, both in terms of training and in terms of performance in real ROS environments?\\
%$\bullet$ What are the detailed dynamics produced by RL-based local planning algorithms? In particular, how noisy are they compared to traditional local planners such as DWA?\\
%$\bullet$ Can we replace an RL-based local planner with an algorithm based on repeated shortest paths?\\
%$\bullet$ What is the performance of RL-based local planners compared to traditional local planners like DWA? What are the scenarios where RL-based planners work well, and what are the scenarios where DWA works well? In particular, how do the two types of algorithms navigate different types of corners? How do they react to different types of obstacles?\\
%$\bullet$ How does moving to an RL-based local planner affect the interaction between the local and global planners?
