\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\pdfminorversion=6

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{subfigure}
\usepackage{mathtools}



\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{multirow}   
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{multicol}
\usepackage{array}
\newcommand{\hdrule}{\midrule[\heavyrulewidth]}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{multirow}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{\LARGE \bf Primer: Fast Private Transformer Inference on Encrypted Data }

\author{
\begin{tabular}{c c c}
Mengxin Zheng  & Qian Lou   & Lei Jiang  \\
\multicolumn{1}{c}{Indiana University Bloomington} & \multicolumn{1}{c}{ University of Central Florida} & \multicolumn{1}{c}{Indiana University Bloomington}\\
\multicolumn{1}{c}{ zhengme@iu.edu} & \multicolumn{1}{c}{ qian.lou@ucf.edu} &
\multicolumn{1}{c}{ jiang60@iu.edu}\\
\end{tabular}
}
\maketitle


\begin{abstract}
%Natural Language Processing (NLP) has witnessed rapid progress driven by Transformer based models, but applying Transformer models to problems that involve biomedical, financial, and other sensitive data, requires protecting data privacy. %Cloud-based Transformer solutions for such tasks are limited by legal and ethical requirements.  
%Natural Language Processing (NLP) has witnessed rapid progress driven by deep learning-based models, e.g., recurrent neural networks (RNNs) and Transformers, but applying NLP models to problems that involve sensitive data requires protecting data privacy.

%To enable privacy-preserving Natural Language Processing (NLP) tasks, prior works adopt post-quantum cryptographic techniques, e.g., fully homomorphic encryption, on recurrent neural networks (RNNs). However, they still suffer from prohibitively computational and communicational overhead due to the large recurrent depth and computation in RNNs. In this work, we first show that performing Transformers on encrypted data is a more feasible way to implement privacy-preserving NLP. 

It is increasingly important to enable privacy-preserving inference for cloud services based on Transformers. Post-quantum cryptographic techniques, e.g., fully homomorphic encryption (FHE), and multi-party computation (MPC), are popular methods to support private Transformer inference. However, existing works still suffer from prohibitively computational and communicational overhead. In this work,  we present, Primer, to enable a fast and accurate Transformer over encrypted data for natural language processing tasks. In particular, Primer is constructed by a hybrid cryptographic protocol optimized for attention-based Transformer models, as well as techniques including computation merge  and tokens-first ciphertext packing. Comprehensive experiments on encrypted language modeling show that Primer achieves state-of-the-art accuracy and reduces the inference latency by $90.6\% \sim 97.5\%$ over previous methods.

%by multiple 

%novel Transformer-friendly techniques, e.g, tokens-first ciphertext packing, and an attention-friendly protocol. Comprehensive experiments on encrypted language modeling and machine translation show that Primer achieves state-of-the-art accuracy and reduces the inference latency by $73.6\% \sim 97.1\%$ over previous methods. %Primer establishes a solid baseline and paves the way for privacy-preserving NLP tasks over encrypted data. 



%To further improve the performance and reduce latency, we present several novel Transformer-orientated techniques, e.g, encrypted pruning, tokens-first ciphertext packing, and an attention-friendly protocol. Comprehensive experiments on encrypted language modeling and machine translation show that our work, denoted by Primer, achieves state-of-the-art performance and reduces the inference latency by $73.6\% \sim 97.1\%$ over previous methods. 


%Cryptographic techniques are popular methods to provide confidential computations, e.g, fully homomorphic encryption enables computations on encrypted data without decryption. %Multiple protocols for Convolution Neural Networks (CNNs) over encrypted visual data are proposed. 
%However, how to perform NLP tasks with Transformers over encrypted language data is not studied so far.  We show that it is not trivial or straightforward to support Transformers on encrypted data for NLP tasks. Transformers with techniques in encrypted CNNs suffer from prohibitive latency overhead. This is because Transformers have distinct features from CNNs, e.g., high-dimensional embedding layer, heavy non-linear activation and attention functions. Thus, optimizations specified for encrypted Transformers are required. We present, Primer, to enable a fast and accurate Transformer over encrypted  data for NLP tasks by several novel Transformer-friendly techniques, e.g, encrypted pruning, tokens-first ciphertext packing, and an attention-friendly protocol. Comprehensive experiments on encrypted language modeling and machine translation show that Primer achieves the state-of-the-art accuracy and reduces the inference latency by $73.6\% \sim 97.1\%$ over previous methods in CNNs. %Primer establishes a solid baseline and paves the way for privacy-preserving NLP tasks over encrypted data. 
\end{abstract}

\begin{IEEEkeywords}
Fully Homomorphic Encryption, Multi-party Computation, Transformer, Cryptographic Protocol, Private Inference 
\end{IEEEkeywords}

\section{Introduction}
\label{s:intro}

Transformer-based, or more broadly attention-based, models show superior performance over previous methods, becoming increasingly popular in natural language processing (NLP) applications~\cite{Devlin:ACL2019:BERT}. For example, BERT obtains new state-of-the-art results on eleven NLP tasks, including pushing the GLUE score to $80.5\%$ ($7.7\%$  absolute improvement), and even proves superior performance compared to human results on the challenging sentence classification tasks. %GPT-2~\cite{radford:2019:GPT2} and DeLighT~\cite{mehta:iclr2021:delight} produce significant performance improvements over RNN on reading comprehension, machine translation, and text summarization.
Server-based Transformer service is an effective way for clients to run their computationally expensive and memory-intensive NLP tasks on powerful cloud servers. During a server-based Transformer service, cloud servers require access to clients' language data, thus introducing potential privacy risks. Therefore, to be able to utilize this technology, it is urgently needed to safeguard the confidentiality of users'  biomedical, financial, and other sensitive data that are submitted to servers. 

Post-quantum cryptographic protocols, e.g., FHE~\cite{FHE,Lou:NIPS2019} and MPC~\cite{Mihir:Justgarble} are popular methods to enable provably confidential computation on encrypted data. We use Figure~\ref{f:overview} to show the overview of private transformer inference, where the client receives cloud services based on Transformer models by only uploading encrypted data generated by cryptographic protocols such as FHE or MPC. This Transformer inference is provably privacy-preserving since data is not revealed to other parties~\cite{li2022mpcformer,chen:ACL2022}.  However, existing  works for private Transformer inference based on FHE, e.g., THE-X~\cite{chen:ACL2022}, suffer from enormous latency. For example, THE-X takes more than 3 orders of magnitudes latency than regular Transformer inference. And polynomial approximation of activation in THE-X significantly reduces accuracy, e.g., $<77\%$ GLUE score ($\sim 8\%$ absolute accuracy decrease).  %The concurrent work MPC-Former takes 

%But most of the existing works focus on vision data privacy protection  by optimizing privacy-preserving convolutional neural networks (CNNs), such as ~\cite{Brutzkus:ICML2019,Roshan:PLDI20,Deepsecure:dac18,Delphi:usenix2020}. It is not well-studied for NLP data protection as recent work ~\cite{Reich:NIPS2019:PrivText,Ahmad:IEEEAcess:PrivFT,Zahra:NIPS20:CryptoNAS,feng:2020:cryptogru,rathee:sirnn} shows. 


%are applied by current works, e.g.,~\cite{Brutzkus:ICML2019,Roshan:PLDI20,Deepsecure:dac18,GAZELLE:USENIX18,Delphi:usenix2020,Reich:NIPS2019:PrivText,Ahmad:IEEEAcess:PrivFT,Zahra:NIPS20:CryptoNAS,feng:2020:cryptogru,rathee:sirnn}, to perform privacy-preserving convolutional neural networks (CNN) and RNN applications where only encrypted data are shared between client and server so that the client's data privacy is preserved. 


%Although many private CNNs and RNNs~\cite{CryptoNets:ICML2016} -~\cite{rathee:sirnn} are proposed, private Transformers have not been explored due to multiple challenges. 
We identify several challenges to design private Transformer inference, such as the large one-hot word embeddings, complex attention, frequent $SoftMax$, and very deep blocks. Specifically, BERT~\cite{Devlin:ACL2019:BERT} uses WordPiece embeddings~\cite{google:2016:wordpiece} with $30522$ token vocabulary and $768$ embedding dimensions so that $n$ tokens require $n$ times of $30522\times768$ matrix-vector multiplication. Directly applying existing techniques to design privacy-preserving embeddings suffers from enormous latency overhead. In addition,  we identify that the attention scheme in Transformer models requires massive ciphertext-ciphertext multiplications that cannot directly be implemented by previous methods that are optimized for ciphertext-plaintext multiplications.  Moreover, deeper Transformer architecture adds expensive FHE rotations and communicational interactions. 

In this work, we present a fast and accurate Transformer inference method, denoted by Primer, over encrypted data. We propose several techniques to construct Primer. In particular, a hybrid cryptographic protocol is proposed to construct a private Transformer, where FHE is used for polynomial operations and MPC is for non-polynomial operations. We call our Primer with this protocol Primer-base. Primer-base is accurate since it removes the polynomial approximation in previous works based on FHE. To reduce the online time of Primer-base, we propose a new hybrid protocol, denoted by HGS,  to pre-process most FHE operations. Offloading computations into the offline phase from the online phase is important since offline computations can be computed in advance before inference.  We further propose FHGS, denoted by Primer-F to improve the compatibility of HGS on attention computations in Transformer models. Other techniques including computation merge (combined FHGS) and tokens-first packing are presented to further reduce the inference latency.  
Comprehensive experiments on encrypted language modeling show that Primer achieves state-of-the-art accuracy and reduces the inference latency by $90.6\% \sim 97.5\%$ over previous methods.

\begin{figure}[t!]
\centering
\includegraphics[width=3.3in]{figures/overview.pdf}
\vspace{-0.1in}
\caption{Overview of private Transformer inference based on cryptographic protocol, e.g., FHE and MPC. The lock represents that data is encrypted. }
\vspace{-0.2in}
\label{f:overview}
\end{figure}

%\textbf{Our contributions.} In this work, we propose Primer to enable a fast and accurate privacy-preserving Transformer-based model for NLP tasks by four steps. First, Primer-base is constructed by reforming the state-of-the-art mixed cryptographic protocol to ensure efficient private embeddings and attention blocks. Specifically, most computational overhead of private embeddings are performed in the offline phase and a new  protocol is proposed to replace expensive ciphertext-ciphertext operations in attentions with cheap ciphertext-plaintext operations. Second,  we propose a protocol that efficiently supports privacy-preserving top-k tokens selection to preform fast private  transformer which significantly reduces the expensive $SoftMax$ operations. Also, we propose tokens-first packing instead of prior features-first packing to reduce the offline and online overhead brought by HE.  At last, we show that multiple secret sharing layers in the Transformer can be combined into one single one, which further reduce the latency. These four techniques ensure Primer to have high throughput and accuracy.  

%We are the first to construct a privacy-preserving transformer-based NLP platform. Instead of ciphertext-ciphertext multiplication using HE, we develop a HHGS protocol that replaces a ciphertext-ciphertext multiplication in attention with two ciphertext-plaintext multiplications. Inspired by Beaver's multiplication triples, HHGS utilizes secret sharing and homomorphic encryption to support multiplication on secret sharing. We call it as Primer-base. We propose tokens-first packing instead of prior features-first packing to reduce the offline and online overhead brought by HE. We call it as Primer-pack. We propose to combine multiple secret sharing layers into one layer to reduce the latency. We call it Primer-combine. We propose a protocol that efficiently supports privacy-preserving top-k selection to support fast private linearize transformer which significantly reduces the expensive $SoftMax$ operations. We call it Primer-topk.  

\section{Background and Motivation}
%\subsection{Threat Model and Cryptographic Primitives}
\textbf{Threat Model.} 
We use Figure~\ref{f:overview} to show the overview of our threat model, where servers and clients are semi-honest, e.g.,  a semi-honest cloud server that attempts to infer clients' input information but follows the cryptographic protocol. Our threat model follows previous work THE-X ~\cite{chen:ACL2022} and Gazelle~\cite{GAZELLE:USENIX18}.  The security level of our method is 128 bits for a fair comparison.
%In the cloud-based Transformer paradigm, a privacy risk inherent to data transmission exists when clients share sensitive data with servers. Even though a strong encryption can be applied to encrypt the shared sensitive data of clients, the untrusted server may potentially cause data leakage. The mixed cryptographic protocol of Homomorphic Encryption (HE), Multi-Party Computation (MPC), and Secret Sharing (SS) is one of the most promising encryption techniques to enable a server to perform computations on encrypted data; only encrypted data is shared between a client and a cloud server, preserving the client's data privacy.   

\textbf{Transformer-based Models for NLP Tasks.}
%Here we should introduce the concepts of transformer with attention. The reason why transformer is important and why transformer is superior than CNN and RNN. 
Transformer-based models~\cite{Vaswani:NIPS2017:Attention}  achieve state-of-the-art performance in many NLP tasks. %The differences between transformer and neural networks should be highlighted.
A Transformer architecture mainly includes embeddings, stacked encoders, and decoders using Multi-Head Self-Attention (MHSA) and point-wise, fully connected (FC) layers. A model with only encoders, e.g. BERT~\cite{Devlin:ACL2019:BERT}, can be used in discriminative NLP tasks including classification and regression, etc. Meanwhile, a model with decoders, e.g. GPT-2~\cite{radford:2019:GPT2}, works for generative NLP tasks including Language Modeling (LM) and machine translation.   In particular, embeddings include word embedding and positional embedding. Word embedding converts the input tokens and output tokens (each token is a one-hot vector with a length of $d_{oh}$) to vectors of dimension $d_{emb}$ by a linear projection. Positional embedding ensures the Transformer model has the sequence order information by adding "positional encodings" $\lambda$ to the previous word embedding. The embedded representations are fed into MHSA. In MHSA, embedded representations are firstly converted into three categories, key $X_K$, query $X_Q$, and value $X_V$, by linear projections with key weight $W_K$, query weight $W_Q$, and value weight $W_V$, respectively. Then, the output of MHSA is calculated as a weighted sum of the values $X_V$ by $Attention(X_Q, X_K, X_V)=SoftMax(\frac{X_QX_K^T}{\sqrt{n}})X_V$, where $SoftMax(\frac{X_QX_K^T}{\sqrt{n}})$ is the weight assigned to each value, and $n$ is token numbers. Instead of only computing the attention once, the multi-head mechanism computes attention multiple times in parallel, and these multiple attentions are simply concatenated and linearly transformed into the expected dimensions as $MultiHead(X_Q,X_K,X_V)=[head_1,..., head_H]W_O$, where $head_i=Attention(X_QW_Q^i, X_KW_K^i, X_VW_V^i)$, $W_O$ is a linear projection weight matrix. %Above $W_K$, $W_Q$, and $W_V$ are parameter matrices to be learned. 
%The subsequent block Norm\&FC mean normalization~\cite{ba2016:layernorm} and Fully-Connected (FC) layers which are widely used in CNN.  


%\subsection{Privacy-Preserving Deep Learning}
\textbf{Interactive hybrid cryptographic protocol.}
FHE~\cite{FHE} is an encryption method that enables one to perform computations on encrypted data without decryption. Garbled Circuit (GC)~\cite{Mihir:Justgarble,feng:2020:cryptogru} and Secret Sharing (SS)~\cite{GMW:1987:SS} are two paramount methods of multi-party secure computations. An Interactive hybrid cryptographic protocol~\cite{GAZELLE:USENIX18} is proposed to combine the advantages of FHE, GC, and SS. In particular, FHE has superior performance over GC on linear operations, e.g., matrix-vector multiplication. This is because FHE with a ciphertext packing technique supports efficient operations in a SIMD (single instruction multiple data) manner.   Therefore, FHE is used to support private linear operations where a client encrypts input and sends it to the server, and the server returns encrypted output to the client that decrypts the received output.  In the state-of-the-art mixed protocols~\cite{GAZELLE:USENIX18,Lou:ICLR2021:safenet,Lou:NIPS20:AutoPrivacy}, GC shows superior performance over HE in non-linear operations such as activation functions. And SS is used to combine GC and HE in the mixed protocol. Inspired by Beaver's Triple~\cite{beaver1995}, FHE can be used to efficiently perform multiplications on two additive secret shares.  In this work, we use this interactive hybrid method to construct Primer-base, which is a starting point for our optimization techniques.  

%More cryptography primitive descriptions are described at the Appendix section. 
% A mixed HGS (HE+GC+SS) protocol where SS combines HE-based linear operations and GC-based non-linear operations is widely used in  recent works~\cite{GAZELLE:USENIX18}\cite{bian2020ensei}\cite{Zahra:NIPS20:CryptoNAS} and significantly advances privacy-preserving deep learning. 
%This is because HE is more computation-friendly on linear operations including convolutional and fully-connected layers, but MPC is preferred on non-linear activation functions. 
%Delphi~\cite{Delphi:usenix2020} and SAFENet~\cite{Lou:ICLR2021:safenet}  improves the mixed HGS protocol by further offloading most online heavy HE linear operations to the offline phase that is independent to clients' input. 



%\subsection{Transformer-based NLP models.}

%The subsequent block Norm\&FC can be defined as $Norm(Max(0,xW_{fc1}+b_1)W_{fc2}+b_2)$, which is also used in most CNN models where $Norm$ is a layer normalization operation~\cite{ba2016:layernorm} and $Max(0,x)$ is a $ReLU$ activation. %Based on MHSA and Norm\&FC blocks, $N$ stacked encoder and decoder can be built for NLP classification or generation tasks.   

%\textbf{Top-$k$ tokens pruning for efficient Transformer. }
%Linear transformers~\cite{Reformer,performer} work well in the unencrypted-domain tasks, but they suffer from a huge accuracy loss in the encrypted-domain tasks since float-point values should be quantized into fix-point values to support homomorphic encryption and secret sharing. Instead, linearithmic transformer ~\cite{Wang:hpca2021,zhou:aaai2021:informer} is used in this paper. Since $X_Q$, $X_K$, $X_V$ have $n$ tokens, $\mathcal{O}{(n^2)}$ dot-product and $SoftMax$ operation are required in a MHSA block. A large amount of unessential tokens can be pruned away in human languages to improve efficiency. Recent works~\cite{Wang:hpca2021,zhou:aaai2021:informer} propose tokens pruning to evaluate token importance according to attention probabilities (weight), keep top-$k$ important tokens, and remove trivial ones. $k$ is dependent on tokens number $n$. To maintain a competitive accuracy, $k$ can even be $log(n)$ shown in ~\cite{Wang:hpca2021,zhou:aaai2021:informer}.  $k$ tokens to be selected are determined by an array of accumulative attention probabilities for each token. Here, each of $n$ tokens has $n$ probabilities which are summed to indicate the importance of this token to the sentence. The $k$ tokens with larger accumulative attention probabilities are left and the others are pruned. This token pruning is cascaded which means that once a token is pruned, it is removed in all the later layers. 
%Details in embeddings, encoder, decoders;
%\subsection{Efficient Linear transformer with Top-k selection.}
%Here we should briefly clarify that linear transformer or nlogn transformer is essential for long-seq tasks.

%\section{Motivation and Prior Works.}

\begin{figure}[t!]
\centering
\includegraphics[width=3.3in]{figures/motivation-dac.pdf}
%\vspace{-0.1in}
\caption{The latency and accuracy comparisons of prior works, e.g., THE-X and GCFormer, and our work Primer-base and Primer-F on MNLI-m dataset with BERT-base.}
\vspace{-0.2in}
\label{f:motivation-dac}
\end{figure}


%\textbf{Comparison with related work.}
%CryptoNets~\cite{CryptoNets:ICML2016}, CryptoDL~\cite{CryptoDL:arxiv17}, Faster CryptoNets~\cite{costache2017faster}, DiNN~\cite{DiNN:CRYPTO18}, TAPAS~\cite{tapas:icml2018}, SHE~\cite{Lou:NIPS2019}, LoLa~\cite{Brutzkus:ICML2019}, Falcon~\cite{Lou:NIPS20:Falcon}, and EVA~\cite{Roshan:PLDI20} only use HE scheme to perform privacy-preserving CNNs. These works still suffer from enormous computational overhead and low accuracy for very deep neural networks. For example, these HE-based works only support at most 11-layer neural networks and have at least $\sim 3\%$ accuracy loss on CIFAR-10 dataset. Therefore, a purely HE-based protocol is not an efficient candidate for a private Transformer with more than 30-layer computations. DeepS~\cite{Deepsecure:dac18} is a representative GC-based model that supports arbitrary private computations by  converting computing tasks into Boolean circuits. The GC-based model is accurate but requires to pay significant communicational overhead for multiplications. GC-based models are not suitable for a private Transformer due to the large latency. Mixed protocols such as SecureML~\cite{SecureML:2017SP}, Chameleon~\cite{Chameleon:ASIACCS18}, and Gazelle~\cite{GAZELLE:USENIX18} are proposed to combine the advantages of multiple cryptography schemes. Gazelle remarkably reduces the latency overhead of DeepS shown in Figure~\ref{f:motivation}(b) with a competitive accuracy by using HE for linear operations and using GC for non-linear operations. Delphi~\cite{Delphi:usenix2020} and SAFENet~\cite{Lou:ICLR2021:safenet} further 
%offload most online HE operations of DeepS to the offline phase which can be pre-calculated. We adapt this mixed HGS protocol to construct our Primer-base and propose multiple optimizations to notably reduce its inference latency.

%In~\cite{cryptoeprint:2018:663}, matrix multiplication in HE must perform all encrypted HE operations online. However, our matrix multiplication using the mixed HE+GC+SS (HGS) and Fully HGS (FHGS) can offload most of HE encrypted operations from online phase so latency can be significantly reduced. Although our FHGS and other mixed protocol like ~\cite{cryptoeprint:2020:451} both use Beaver’s Triple (BT) to process multiplications on secret shares, our FHGS has multiple optimizations on efficiently generating BT’s multiplication triples for Transformer models. These optimizations mainly include Tokens-first packing  and Combined FHGS. Specifically, since HE’s rotations are the computational bottleneck to support BT, we propose Tokens-first packing to enable an efficient BT by significantly reducing HE’s rotations. This is motivated by a key observation that prior packing techniques are not dedicated for Transformers which have large-dimension inputs and multiple tokens. Combined FHGS further reduces the computational and communicational BT overhead by combining adjacent layers. 

%Securely finding the top-$k$ elements is to use an oblivious sort algorithm so that parties jointly sort the dataset in decreasing order of the associated $n$ values with $B$ bits, and pick the $k$ largest values. If our method uses a sorting algorithm with complexity of $\mathcal{O}{(n*log(n))}$, then the secure top-$k$ complexity using GC is $\mathcal{O}{(B*n*log(n))}$. Although the recent work ~\cite{cryptoeprint:MPCCache} has a better complexity $\mathcal{O}{((B+ log(n)) * n*log2(k))}$ which is friendly for a large $n$, it is not necessarily much cheaper than our work with $\mathcal{O}{(B*n*log(n)) }$ in the Transformers where $n$ is small, e.g., $n=30$ is quite representative in Transformer (sequence length is 30 which is a usual case setting in BERT). We tested that when $n =30, B=16, k=5$ (our experimental setting), \cite{cryptoeprint:MPCCache}  has almost $2.4\times$ more communication overhead than our proposed method. Another SANNS framework~\cite{SANNS:USENIX20} is an approximated top-$k$ method which are orthogonal to both our work and prior work \cite{cryptoeprint:MPCCache}. 





\textbf{Motivation.} 
As Figure~\ref{f:motivation-dac} shows, prior works like THE-X~\cite{chen:ACL2022} using only FHE for private inference suffer from low accuracy and enormous online latency due to polynomial approximation and expensive FHE operations.  We use prior GC-based work~\cite{Deepsecure:dac18} to implement a GCFormer (we convert the Transformer model into a circuit based on binary gates so that GC~\cite{Mihir:Justgarble} can implement it). GCFormer achieves an accurate performance, i.e., 85.1\% accuracy, but it takes a larger latency than THE-X. Thus, the FHE-based method or GC-based method  cannot achieve a low-latency and accurate private Transformer inference. Instead, we follow the interactive and hybrid cryptographic protocol~\cite{GAZELLE:USENIX18} and construct our Primer-base by using GC for non-polynomial operations, FHE for polynomial operations, and SS for secure communication between multiple parties.  Primer-base significantly improves the accuracy of THE-X, e.g., 7.3\% accuracy increase, and reduces the latency of GCFormer.  However, Primer-base still suffers from enormous online latency. This motivates us to propose techniques like the FHGS protocol, denoted by Primer-F, to offload the online computation to the offline phase where computations can be computed before inference.  Considering Primer-F still has a large total latency, we have motivations to propose techniques including computation merge, i.e., combined FHGS, and tokens-first ciphertext packing techniques. More details about Primer and related techniques are introduced in the following section~\ref{s:Primer}.




%Transformer-based models obtain new state-of-the-art results on multiple benchmarks including but not limited to GLUE~\cite{wang:emnlp2018:glue}, SQuAD1~\cite{Pranav:SQuAD}, SQuAD2~\cite{lee:etal-2020:squad2}, and SWAG~\cite{zellers:etal-2018:swag}. For instance, pre-Transformer models based on CNN or RNN and humans achieve 78.0 and 85.0 accuracy scores, respectively, on the SWAG dataset. In contrast, Transformer-based BERT~\cite{Devlin:ACL2019:BERT} obtains a 86.3 score which accomplishes 8.3 and 1.3 accuracy improvement over prior models and humans. 
%Many privacy-preserving pre-Transformer models are proposed~\cite{CryptoNets:ICML2016} to \cite{rathee:sirnn}, but private Transformers are not studied, therefore it is urgently needed to explore the design space of private Transformer. We classify cryptography protocols into four categories including \textbf{pure HE}, e.g., EVA~\cite{Roshan:PLDI20}, \textbf{pure GC},  e.g., Deep Secure (DeepS) ~\cite{Deepsecure:dac18}, \textbf{mixed HE and GC},  e.g., Gazelle (Gaz)~\cite{GAZELLE:USENIX18}, and \textbf{mixed HGS}, e.g., Delphi (Del)~\cite{Delphi:usenix2020}. To support Transformer, EVA~\cite{Roshan:PLDI20} needs to perform HE in a layer-wise manner with SS and has to approximate $SoftMax$ with low-degree polynomials which losses more than 4 accuracy scores on GLUE. Although GC-based DeepS~\cite{Deepsecure:dac18} achieves the most accurate Transformer, it suffers from enormous latency due to a large embedding dictionary and many matrix-vector multiplications. Since Gaz and Del based on HGS protocol cannot directly support the attention operations on ciphertexts, one should use the additional Fully HGS (FHGS) that is proposed at section~\ref{s:Primer-base} to adapt them into Gaz-F and Del-F to perform private Transformer. Gaz-F and Del-F achieve similar accuracy with DeepS since they use GC for non-linear activations without approximation errors. Gaz-F significantly reduces the latency of DeepS by replacing GC-based linear operations with HE-based linear operations. Del-F, using HGS protocol, offloads expensive HE operations from the online phase so that most HE can be computed in advance. We call Del-F Primer-base.  the online latency breakdown of Primer-base under different inputting sequence length $n$. $Embed$, $QKV$,  $Q\times K$, $Softmax$, $Atten.\, Value$, $Others$ mean embeddings, generation of $X_Q$, $X_K$, $X_V$, dot-product of $X_Q\times X_K^T$, $SoftMax$ operations of scaled dot-product, generation of new attention values, the other linear operations, respectively. Primer-base still suffers from long latency; for example, it takes $\sim$134 seconds to infer an $n=30$ sequence. The $SoftMax$ is the latency bottleneck that occupies $40.1\%\sim 53.6\%$ of total latency. In addition, Primer-base also suffers from a quadratic complexity over sequence length $n$.  


%HE for embedding and attention: NO!
%MPC for embedding and attention: NO!
%We need HGS.

%HGS cannot directly support attention. Ciphertext-Ciphertext.

%$n^2$ complexity. length is long, latency is so huge.

%Offline phase: rotations are botteleneck.

%Rounding number is so huge. 



%The reason why we choose the last category: HE-GC mixed neural networks using qualified data. That is to say the other schemes suffers from huge computational or communication overheads.


%After that, the problems of HE-GC mixed neural networks should be summarized: 1.Delphi and SAFENet do not show the ability to run efficiently ciphertext-ciphertext multiplication in a offline method. 

%2.They suffers from a multiple-round interaction (~6 times) in a single transformer block. We reduce it into 2 times. 

%3. The most important is how to efficiently support linearithmic transformer(nlogn) without a quadratic (n*n) transformer. 


%\subsection{Comparison with prior works.}
%There are several techniques: -1, lightweight encoding based CNN. 0, Secret-sharing based neural networks. 1. HE-based neural networks, like CryptoNets, CryptoDL, DiNN, TAPAS, HCNN, SHE, LoLa, Falcon, CHET, EVA. 2. GC-based neural networks, like DeepScure, XONN, SecureNN, SIRNN(I need to make sure all these works), 3, HE-GC mixed NEURAL NETWORKS, like, MiniONN, Gazelle, CryptoNAS, Delphi, SAFENet. 


%Recent works~\cite{CryptoNets:ICML2016}\cite{CryptoDL:arxiv17}\cite{FCryptoNets:arxiv19}\cite{DiNN:CRYPTO18}\cite{tapas:icml2018}\cite{Lou:NIPS2019}\cite{Brutzkus:ICML2019}\cite{Lou:NIPS20:Falcon}\cite{dathathri:2019PLDI}\cite{Roshan:PLDI20}\cite{SecureML:2017SP}\cite{Deepsecure:dac18}\cite{E2DM}\cite{Chameleon:ASIACCS18}\cite{GAZELLE:USENIX18}\cite{bian2020ensei}\cite{Liu:CCS2017:MiniONN}\cite{Delphi:usenix2020}\cite{Reich:NIPS2019:PrivText}\cite{Ahmad:IEEEAcess:PrivFT}\cite{Zahra:NIPS20:CryptoNAS}\cite{Lou:ICLR2021:safenet}\cite{feng:2020:cryptogru}\cite{rathee:sirnn} employ cryptographic protocols including Homomorphic Encryption (HE), Multi-Party Computation (MPC), and Secret Sharing (SS) to perform privacy-preserving CNN and RNN applications where only encrypted data are shared between clients and a cloud server so that the clients' data privacy is preserved.
\section{Primer}
\label{s:Primer}

\subsection{Primer-base Construction}
\label{s:Primer-base}

As Figure~\ref{f:flow}(a) shows, a Transformer-based model involves computations of \ding{182}-\ding{183} embeddings, \ding{184} derivation of $X_Q$, $X_K$ and $X_V$, \ding{185} scaled dot-product of $X_Q$ and $X_K^T$, \ding{186} $SoftMax$, \ding{187} attention values, and the other linear operations like Fully Connected (FC) computations. %Here we skip Fully Connected (FC) computations behind \circled{6} attention values since FC is well-studied by prior HGS-based works such as Delphi~\cite{Delphi:usenix2020} and we use the same HGS to implement FC. 
\begin{figure}[t!]
%\vspace{-0.2in}
\centering
\includegraphics[width=3.5in]{figures/flow_dac.pdf}
\caption{Private transformer block inference under various Primer protocols.}
\vspace{-0.2in}
\label{f:flow}
\end{figure}
Embedding output $X[1]$ is computed by $X[0]\times W_E \times \delta +\lambda$, where $X[0]$, $W_E$, $\delta$, and $\lambda$ are the input matrix, embedding weight matrix, positional coefficients, and positional biases, respectively. The embedding output $X[1]$ is multiplied with query weight $W_Q$, key weight $W_K$, and value weight $W_V$ to generate query $X_Q$, key $X_K$, and value $X_V$. Multi-Head Self-Attention requires multiple computations of $X_Q$, key $X_K$, and value $X_V$ with various weight matrices in parallel.  Then a Transformer needs to compute the dot products between the query $X_Q$ and all keys $X_K$, divide each by $\eta =\sqrt{n}$, apply a $SoftMax$ function to obtain the weights on the values, and multiply the attention weights with the value $X_V$.  Figure~\ref{f:flow}(b) illustrates how to construct a basic private Transformer, i.e., Primer-base, using the prior interactive hybrid cryptographic protocol, i.e., FHE (we denote it as HE in Figure~\ref{f:flow}(b)) is used for polynomial operations in all the steps other than non-polynomial operations, e.g., $SoftMax$. Instead, GC is used for non-polynomial operations. 
\vspace{-0.1in}
\begin{figure}[ht!]
\centering
\includegraphics[width=3.5in]{figures/module-HGS.pdf}
\caption{Our HGS protocol for Transformer's attention operations.}
\label{f:HGS-module}
\end{figure}
\vspace{-0.15in}
\subsection{Primer-F Construction}
\label{s:Primer-F}


Primer-base significantly improves the accuracy of FHE-based methods like THE-X~\cite{chen:ACL2022} and reduces the latency of MPC-based methods, like GCFormer~\cite{Deepsecure:dac18}.  However, Primer-base still suffers from enormous online latency. This motivates us to propose techniques like the HGS and FHGS protocol, denoted by Primer-F, to offload the online computation to the offline phase where computations can be computed before inference.  

\textbf{The HGS protocol.}
Figure~\ref{f:HGS-module} shows the mixed HGS protocol. The offline phase in the HGS protocol is used to prepare data for the subsequent online phase. For the $i$-th layer of a Transformer model, a client first samples a random matrix $Rc[i]$ that is required to have the same size with private input $X[i]$, and then submits the ciphertext Enc($Rc[i]$) to the server for the subsequent multiplication between Enc($Rc[i]$) and the $i$-th layer weights $W[i]$. A random matrix $Rs[i]$ is generated by the server and $Enc(Rs[i]+Rc[i]\times W[i] )$ is sent back to the client. The client performs decryption to get $Rs[i]+Rc[i]\times W[i]$. $Rs[i]+Rc[i]\times W[i]$ held by the client, and $Rs[i]$ held by the server are secret shares of $Rc[i]\times W[i]$.
%The encrypted sum of a random matrix $Rs[i]$ generated by the server and the previous multiplication result is sent back to the client which can decrypt the encrypted sum so that the client and server obtain the additive secret sharing of $Rc[i]\times W[i]$.
Meanwhile, the offline phase, e.g. garbling, of GC is performed. During the online phase, the difference of $X[i]$ and $Rc[i]$, instead of $X[i]$, is sent to the server. The computation of $(X[i]-Rc[i])\times W[i]-Rs[i]$ and previous offline computation make the client and server have the additive secret shares of $X[i]\times W[i]$. In this way, the heavy encrypted HE operations of privacy-preserving  matrix multiplication of $X[i]\times W[i]$ is calculated offline, and the online overhead is almost removed since only unencrypted computations exist. Then GC is used to perform the subsequent mapping function $F$, e.g., $ReLU$ activation. Specifically, the garbled Boolean $X[i]\times W[i]$ is derived by the modular sum of secret shares of $X[i]\times W[i]$, then the Boolean circuits of mapping function $F$ are calculated. Finally, a modular subtraction between function $F$'s result and a new random matrix $Rc[i+1]$ is performed to generate secret shares of function $F$'s result. A modular operation circuit is implemented by an adder and a multiplexer~\cite{GAZELLE:USENIX18,Delphi:usenix2020}.


 We encapsulate HGS protocol shown in Figure~\ref{f:HGS-module} into a module that takes random matrices $Rc[i]$, $Rc[i+1]$, $i$-th layer input $X[i]$, weight matrix $W[i]$ as inputs, and generates $X[i+1]-Rc[i+1]=F(X[i]\times W[i])-Rc[i+1]$. Here $F()$ function can be an identity function or an activation function. The $i$-th layer input $X[i]$ can be removed if the server holds $X[i]-Rc[i]$. $W[0]=W_E, W[2]=\sigma =1$, and $\lambda$ is added to $X[1]$ instead of multiplication. As Figure~\ref{f:flow}(c) shows, steps in the Transformer including \ding{182}  \ding{183}, \ding{184},  \ding{186}, and the other FC computations can be performed by the HGS protocol; however, steps \ding{185} and \ding{187} cannot be directly constructed by the additive HGS protocol since HGS only supports additive computations including ciphertext additions and ciphertext-plaintext multiplication. This is because the HGS protocol that depends on an additive HE scheme is only sufficient for modules where weights are always not encrypted. Therefore, HGS cannot transform ciphertext-ciphertext multiplications in steps \ding{185} and \ding{187} on secret shares into the offline phase. 

\vspace{-0.15in}
\begin{figure}[ht!]
\centering
\includegraphics[width=3.5in]{figures/module-FHGS.pdf}
%\vspace{-0.1in}
\caption{Our Fully HGS (FHGS) protocol for attention operations.}
\vspace{-0.1in}
\label{f:FHGS-module}
\end{figure}

\textbf{The Fully HGS (FHGS) protocol for ciphertext-ciphertext operations.}
The step~\ding{185} of attention in Transformer is different from  steps~\ding{183} through \ding{184} where weights are not encrypted and only inputs are encrypted so that private inference is a type of ciphertext-plaintext operations. In step~\ding{185}, however, all query, key, and value matrices are encrypted. The Attention operations require ciphertext-ciphertext operations. Ciphertext-ciphertext operations are not only more expensive than ciphertext-plaintext operations but also cannot directly use  HGS method, thus we propose the FHGS protocol to solve this problem.

Inspired by Beaver's Triple method~\cite{beaver1995}, we propose a Fully HGS (FHGS) protocol to empower the prior additive HGS to efficiently support ciphertext-ciphertext operations such as $X_Q \times X_K^T$  in Transformer models. Figure~\ref{f:FHGS-module} shows our FHGS protocol for step~\ding{185} $X_Q[i]\times X_K[i]^T$. Since $X_Q[i]$ and $X_K[i]^T$ are both ciphertexts, additive HGS cannot offload $X_Q[i]\times X_K[i]^T$ operations. FHGS pre-computes encrypted triples including $Enc(Rc[i])$, $Enc(Rc[i]^T)$, and $Enc(Rc[i]^T\times Rc[i])$ for the usage of the subsequent online process. During the online phase in FHGS, the server has access to $X_Q[i]-Rc[i]$ and $(X_K[i]-Rc[i])^T$ although $X_Q[i]$ and $X_K[i]^T$ are not seen by the server. 
So an important intermediate result $tmp1=(X_Q[i]-Rc[i])\times(X_K[i]-Rc[i])^T$ can be derived. The key idea to obtain our target $tmp4$, a ciphertext of $X_Q[i]\times X_K[i]^T$, is that it can be calculated by subtracting three entries from $tmp1$, where this subtraction can be done by $tmp4=tmp1+tmp2+tmp3+Enc(Rc[i]^T\times Rc[i])$.
%The key idea is that $tmp1$ is the sum of 4 entries and one of them is our target $X_Q[i]\times X_K[i]^T$, so that one can calculate $X_Q[i]\times X_K[i]^T$ by subtracting the other three entries from $tmp1$. This subtraction can be done by $tmp4=tmp1+tmp2+tmp3+Enc(Rc[i]^T\times RC[i])$. The $tmp4$ is a ciphertext of $Enc(X_Q[i]\times Enc(X_K[i))$. 
In order to preserve the privacy of $X_Q[i]\times X_K[i]^T$, the server transmits its additive secret sharing ciphertext $Enc(X_Q[i]\times X_K[i]^T-Rs[i+1])$, instead of $Enc(X_Q[i]\times X_K[i]^T)$, to the client who can decrypt $Enc(X_Q[i]\times X_K[i]^T-Rs[i+1])$ and obtain $X_Q[i]\times X_K[i]^T-Rs[i+1]$. In this way, the client and the server acquire additive secret shares of $X_Q[i]\times X_K[i]^T$. Optionally, the client can further share $X_Q[i]\times X_K[i]^T-Rs[i+1]-Rc[i+1]$ with the server to obtain  new secret shares of $X_Q[i]\times X_K[i]^T$. At last, the FHGS protocol is enclosed into a module that takes random matrices $Rc[i]$, $Rc[i+1]$, $Rs[i+1]$, $X_Q[i]-Rc[i]$, and $(X_K[i]-Rc[i])^T$ as inputs, and outputs the secret shares of $X_Q[i]\times X_K[i]^T$. 

\textbf{Privacy analysis.}
The $X_Q[i]$, $X_K[i]$, $X_Q[i]\times X_K[i]^T$ are confidential to both the client and the server, which ensures our FHGS protocol is privacy-preserving. The server that has no access to HE private key cannot decrypt ciphertexts including $Enc(Rc[i]^T\times Rc[i])$, $tmp1$, $tmp2$, $tmp3$, $tmp4$, and $tmp4-Rs[i+1]$. Only secret shares of $X_Q[i]$, $X_K[i]$, $X_Q[i]\times X_K[i]^T$ can be accessed by client and server. Also, FHGS completely offloads complex and expensive ciphertext-ciphertext operations from the online phase into the offline phase since $Rc[i]$ and $Rc[i]^T$ are pre-sampled and their product can be calculated in advance, which enables a additive HE scheme to efficiently perform privacy-preserving ciphertext-ciphertext Transformer operations.  

%the server constructs and transmits its additive secret sharing $Enc(X_Q[i]\times X_K[i]-Rs[i+1])$ a ciphertext     


%Firstly we describe the left half and right half in Figure~\ref{f:FHGS-module} to show security and privacy, then we illustrate the module in Figure~\ref{f:flow}(b).




%A transformer block involves at
%2. HGS protocol application
%3. Computing components that HGS cannot support
%4. How to support using HHGS. 


%Mat. Mul is implemented by ciphertext-ciphertext multiplication and is hard to be implemented by packed additive HE. 
%\vspace{-0.15in}
%\subsection{Privacy-Preserving Top-$k$ (PPTK) Tokens Selection}

%\begin{figure*}[ht!]
%\centering
%\includegraphics[width=5.5in]{figures/module-topk.pdf}
%\vspace{-0.1in}
%\caption{The protocol concept and example of privacy-preserving top-$k$ tokens pruning.}
%\label{f:topk}
%\vspace{-0.2in}
%\end{figure*}

%1. The importance of top-k tokens selection.
% Primer-base still suffers from enormous latency shown in Figure~\ref{f:motivation}(c), especially when the input sequence length $n$ is large. For example, it takes $\sim$134 seconds and $\sim 862$ seconds to inference sequences with length of $n=30$ and $n=90$, respectively. We identify that the time complexity of steps~\ding{185} to \ding{187} in Figure~\ref{f:flow}(a) is $\mathcal{O}{(n^2)}$, %although steps~\circled{1} to \circled{3} has a linear complexity on sequence length $n$. %The time complexity of $\mathcal{O}{(n^2)}$ is because 
% $X_Q[i]$, $X_K[i]$, $X_V[i]$ have size of $n\times d_{head}$, 
%$X_Q[i]\times X_K[i]^T$ requires $n\times n \times d_{head}$ operations, where $d_{head}=\frac{d_{emb}}{H}$ is token vector size in a head, $d_{emb}$ is embedded vector size, and $H$ is the heads number in a multi-head self-attention block. 
%with $SoftMax$ the latency bottleneck that occupies $40.1\%\sim 53.6\%$ of total latency since $N\times H\times n^2$ $SoftMax$ computations are required where $N$, $H$ are Transformer blocks number the heads number. Previous works ~\cite{Wang:hpca2021}\cite{zhou:aaai2021:informer} show that a Top-$k$ tokens pruning enables a time complexity reduction from quadratic $\mathcal{O}{(n^2)}$ to linearithmic $\mathcal{O}{(n\times log(n))}$ when $k=log(n)$ with a competitive accuracy. Figure~\ref{f:topk}(a) shows a tokens pruning in Transformer block $i$ boosts efficiency of $SoftMax$ in the following blocks.   

%2. The working flow of top-k tokens selection and its challenges.
 %After the step \ding{186} in Figure~\ref{f:flow}(a), one can accumulate $n$ attention probabilities as the importance score for each token and pick up $k$ tokens with larger importance scores. In the Primer-base, the server and the client hold the secret sharing of $X[5]$ that are $X[5]-Rc[5]$ and $Rc[5]$, respectively. Here $X[5]$ includes attention probabilities of $n$ tokens, and $X[5][j]$ is an array of attention probabilities for $j$-th token, where $X[5][j]$ has a size of $n$ and $j\in [0,n)$. Top-$k$ tokens selection requires two steps. The first step is to accumulate the array of each $X[5][j]$ to get importance each token, and the second step is to sort importance array and selecting top-$k$ tokens. The accumulation can be implemented by multiple addition on secret shares, but sorting operations on secret shares requires at least $n\times log(n)$ GC-based $B$-bit comparators which significantly introduces communicational overhead, where $B$ is usually set as 16.  

%3. How to implement private top-k selection.
 %As Figure~\ref{f:topk}(b) shows, we propose a new Privacy-Preserving Top-$k$ (PPTK) tokens pruning method that only requires $n$ GC-based $log(n)$-bit comparators, where $log(n)< B$. We use PPTK on Primer-base to construct a low-latency and high-throughput Primer-topk. Firstly, our PPTK scheme needs the server and the client to calculate the secret shares of tokens importance score array by summing the column of $X[i]-Rc[i]$ and $Rc[i]$, respectively. Accumulating attention probabilities of each column represents the importance of the corresponding token~\cite{Wang:hpca2021}. For example, the client computes $SRc[i]=$ Sum\_Column($Rc[i]$) and the server obtains $SX[i]=$ Sum\_Column($X[i]-Rc[i]$). Here $SX[i][j]$ in Figure~\ref{f:topk}(b) means the importance of the $j$-th token in $i$-th block. Then, instead of directly selecting top-$k$ largest tokens from array $SX[i]+SRc[i]$ using GC, PPTK performs a subtraction between an entry $SX[i][j]$ and all entries in $S[i]$ to get $Diff\_X[i][j]$ for all $j\in [0,n-1]$. The same operation on $SRc[i]$ is performed by the client to obtain $Diff\_SRc[i][j]$. The sign bit accumulation of $Diff\_X[i][j]+Diff\_SRc[i][j]$ represents the rankings. For instance, if the sign bit accumulation of $Diff\_X[i][j]+Diff\_SRc[i][j]$ is $0$, the $j$-th token is the most important since its important score is larger than the other tokens. After that, GC is used to popcount the sign bit of $Diff\_X[i][j]+Diff\_SRc[i][j]$ and select $X[i][j]-Rc[i][j]$ using a multiplexer if the $j$-th sign bit accumulation is less than $K_i$ or output mask vectors, i.e., zeros, otherwise.     

%4. examples.
%Figure~\ref{f:topk}(c) shows an example of our PPTK pruning with $k=2$ and $n=4$.  At first, the client and the server have the $Rc[i]$ and $X[i]-Rc[i]$ which are the secret shares of attention probabilities $X[i]$. The accumulation of each row in $X[i]$ is 1 due to $SoftMax$, and the accumulation of each column in $X[i]$ indicates the token importance. The secret shares, $SRc[i]$ and $SX[i]$, of importance score array $TX[i]$ are calculated by the client and the server, respectively. The token ranking orders $Order\_TX[i]$ are obtained by popcounting the sign bits of  $TX[i][j]-TX[i]$ for every $j$. $Order\_TX[i]^T=[1,0,3,2]$ means the second token is the most important and the third token is the most trivial. Lastly, the $X[i]-Rc[i]$ in the first two tokens are selected when $k_i=2$.  PPTK requires $n=4$ comparators (2 bits) for the example in Figure~\ref{f:topk}(c) since the largest number in $Order\_TX[i]^T$ has $log(n)=2$ bits. 

%5. Privacy and efficiency.


%\begin{figure*}[ht!]
%\centering
%\includegraphics[width=5.5in]{figures/module-CHGS-Pack.pdf}
%\vspace{-0.15in}
%\caption{The comparison of features-based packing and tokens-first packing, and the CHGS module. %The CHGS processes multiple stacked operations using a single calculation, and most HE-based operations are moved to the offline phase. 
%}
%\vspace{-0.2in}
%\label{f:PACK-CHGS}
%\end{figure*}


%\vspace{-0.1in}
\subsection{Primer-FC by Combined FHGS (CHGS)}
%\vspace{-0.1in}
%Based on Primer-base, we construct Primer-Pack using top-k pruning and tokens-first packing. 
We further reduce the computational and communicational overhead of previous technologies by a combined FHGS (CHGS) method that can combine adjacent HGS layers. The CHGS processes multiple stacked operations using a single calculation, and most HE-based operations are moved to the offline phase. As Figure~\ref{f:flow}(d) shows, our CHGS module removes its previous HGS operations by incorporating three HGS modules into the adjacent FHGS module. The key idea is that the combined target is $(X[i]\times W_E +\lambda)\times W_Q \times [(X[i]\times W_E +\lambda)\times W_K]^T=X_Q[i]\times X_K[i]^T$ which can be derived from $tmp1=((X[i]-Rc[i])\times W_E +\lambda)\times W_Q \times [((X[i]-Rc[i])\times W_E +\lambda)\times W_K]^T$ by $tmp1-tmp2+tmp3+tmp4-tmp5+tmp6-tmp7=result$. The combined weight $W_M$, $tmp6$, and $tmp7$ can be calculated in the offline phase. The server sends $result$-$Rs[i+1]$ to the client so that the client obtains the decryption of $result$-$Rs[i+1]$ and the server has $Rs[i+1]$. The client can also subtract the decryption of $result$-$Rs[i+1]$ with $Rc[i+1]$ to construct a new secret sharing. Using CHGS, 4-time interactions in Figure~\ref{f:flow}(d) can be reduced into 1-time interaction. The improvement details of CHGS are discussed in the following results section.

\begin{figure}[ht!]
\centering
\vspace{-0.1in}
\includegraphics[width=3.4in]{figures/module-Pack.pdf}
\caption{The comparison of features-based packing and our tokens-first packing. %The CHGS processes multiple stacked operations using a single calculation, and most HE-based operations are moved to the offline phase. 
}
\vspace{-0.25in}
\label{f:PACK-CHGS}
\end{figure}

\subsection{Tokens-first Packing, i.e., Primer-FPC Construction}
Embedding is used to compress a large and sparse one-hot vector into a small and dense vector. How to efficiently support high-dimension ($>30K$) matrix multiplication is not studied. Each word (token) will be a vector of size $30522$ which is larger than the ciphertext slot numbers. For multiple words in a sentence, how to pack these words into ciphertext is a new challenge. We propose tokens-first packing to tackle this challenge, instead of prior feature-based ciphertext packing used in~\cite{Brutzkus:ICML2019,GAZELLE:USENIX18, Delphi:usenix2020, Roshan:PLDI20}. In feature-based ciphertext batching,  multiple features (e.g. pixels in an input image) are batched into the same ciphertext. However, we found that directly applying the feature-based packing method on Transformer-based NLP models introduces massive FHE rotations. 

We propose a tokens-first packing method instead of the prior features-based packing method to reduce the homomorphic rotations in Primer-FC. Figure~\ref{f:PACK-CHGS}(a) depicts the pseudo-code of encrypted matrix multiplication based on feature-base packing between $X[i]$, i.e., $X[i][0:n][0:d_{oh}]$ and $W[i]$, i.e., $W[i][0:d_{oh}][0:d_{emb}]$, where $n$, $d_{oh}$, and $d_{emb}$ are input tokens number, one-hot dimension of a token, and embedding dimension, respectively. 
Here $X[i][0:n][0:d_{oh}]$ represents the shape of matrix $X[i]$. The result of matrix multiplication is $X[i+1]$ with a size of $X[i+1][n][d_{emb}]$. Lines 2 through 8 are used to pack the input matrix $X[i][0:n][0:d_{oh}]$ into $c$ plaintexts $p[0:c]$ and encrypt them into $c$ ciphertexts via the encryption function $Enc()$. Each plaintext $p$ has $M$ slots so it can hold $M$ entries. In the features-based packing, the one-hot features $X[i][h][0:j]$ of a token $h$ are first placed into plaintexts, then the features of the next token $h+1$ are packed until all tokens' features are packed and encrypted. The features-based packing requires $c\times M$ $Rotate$ operations since each ciphertext with $M$ features needs $M$ rotations shown in line 9 $\sim$ line 14. One key observation is that features from different tokens are independent and they are not required to accumulate in the matrix multiplication, which motivates us to propose tokens-first packing to batch tokens as much as possible into the same ciphertext. As Figure~\ref{f:PACK-CHGS}(b) shows, the lines of 2, 3, 6, and 10 of features-based packing are replaced so that the  $j$-th feature $X[i][0:n][j]$ of all $n$ tokens are packed into a ciphertext, then the $j+1$-th feature will be packed. Using this tokens-first packing, one ciphertext only has $\sim\frac{M}{n}$ features so that one ciphertext only requires $\sim\frac{M}{n}$ $Rotate$ operations. Considering both features-based packing and our tokens-first packing have similar ciphertext numbers $c$, our tokens-first packing reduces $c\times (M-\frac{M}{n})$ rotations. 







\vspace{-0.1in}
\begin{table}[htbp!]
\centering
\small
\setlength{\tabcolsep}{4pt}
\caption{Comparison of our method and other works on private BERT-base inference. "/" means non-applicable. }
\begin{tabular}{l|cccc}
\toprule
\multirow{1}{*}{Scheme}   &   Offline(s)    & Online(s)      &  Total(s) & Acc.(\%) \\
\midrule 
THE-X      & $/$  & $4.7K$ & $4.7K$ & $77.3\%$ \\
%MPC-Former & 7.5K  & 9.8K & & 85.1\%\\
GCFormer    &$7.5K$  & $9.8K$ & $17.3K$ & $85.1\%$\\
Primer-F     &$6.5K$ & $0.04K$& $6.54K$ & $84.6\%$ \\
Primer-FPC (Primer)       &$0.4K$ & $0.04K$ & $0.44K$ & $84.6\%$\\

%%%%%%% 3.25X

%Primer-base     & 6622  & 2.7       & 408        & 12.7         & 30             & 32.3          & 0.03      & 53.2  & 3.2  & 7.5    & 6902      & 25.8     & 13965.2 & 134.2 & 85.1\\

%Primer-pack     & 286.5  & 2.7       & 83.5        & 12.7         & 22.4             & 25.1         & 0.03      & 53.2        & 3.2  & 7.5    & 466.1      & 25.8     & 861.7  & 127    & 85.1\\

%Primer-CHGS     & 0  & 0       & 0       & 0       & 385.4     & 37.7 & 0.03      & 48.1 & 3.2  & 7.5    & 466.1      & 21.8     & 854.7 & 115.1 & 85.1\\


%########## old data

%Primer-topk     & 6622  & 2.7       & 80.1        & 2.7         & 5.9             & 6.7          & 0.01      & 21.0  & 0.6  & 4.1    & 1355      & 5.2     & 8063 & 42 & 84.3\\
%Primer-pack     & 286.5  & 2.7       & 16.4        & 2.7         & 4.4             & 5.2          & 0.01      & 21.0  & 0.6  & 4.1    & 91.5      & 5.2     & 399 & 41 & 84.3\\
%Primer-CHGS     & 286.5  & 2.3       & 16.4        & 0.3         & 4.4             & 5.2          & 0.01      & 19.0  & 0.6  & 4.1    & 91.5      & 4.4     & 399 & 35.4 & 84.3\\
%Primer-CHGS     & 0  & 0       & 0       & 0       & 307.3     & 7.8 & 0.01      & 19.0 & 0.6  & 4.1    & 91.5      & 4.4     & 399 & 35.4 & 84.3\\
\bottomrule
\end{tabular}
%\vspace{-0.2in}
\label{t:overall_comparisons}
%\vspace{-0.2in}
\end{table}





\begin{table*}[htbp!]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\caption{A performance ablation study of proposed techniques. Primer-base is constructed by a hybrid cryptographic protocol, where FHE is used for polynomial operations and MPC is for non-polynomial operations. Primer-F, Primer-FP, and Primer-FPC apply FHGS protocols, tokens-first packing, and CHGS techniques, in a cascade manner. The offline and online latency (seconds) of each step in BERT-base are listed. Acc. means accuracy on the MNLI-m dataset. "/" means non-applicable.}
\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|c}
\toprule
Scheme   & \multicolumn{2}{c}{\ding{182}\ding{183}  $Embed$}  & \multicolumn{2}{c}{\ding{184}  $QKV$} & \multicolumn{2}{c}{\ding{185} $Q\times K$} & \multicolumn{2}{c}{\ding{186} $SoftMax$} & \multicolumn{2}{c}{\ding{187} $Atten. Value$} & \multicolumn{2}{c}{ Others}  & \multicolumn{2}{|c|}{Total} & Acc.(\%)\\\hline
& offline  & online & offline  & online  & offline  & online & offline  & online & offline  & online & offline  & online & offline  &online
& \\ \midrule

Primer-base     & $/$ & $3094.4$     &   $/$     & $190.6$         &   $/$          & $25$         & $0.01$      & $16.4$  & $0.8$  & $2.3$    & $/$      &  $3224.5$  & $0.81$  & $6553.2$ & $84.6$\\
+FHGS (Primer-F)     & $3094.4$ & $0.8$      & $190.6$        & $3.9$         & $14$             & $9.9$         & $0.01$      & $16.4$  & $0.8$  & $2.3$    & $3224.5$      & $7.9$    &$6524.3$  & $41.2$ & $84.6$\\

+Pack (Primer-FP)    & $134.5$  & $0.8$       & $39$       & $3.9$         & $10.5$            & $7.7$         & $0.01$      & $16.4$        & $0.8$  & $2.3$    & $220.4$      & $7.9$     & $405.2$  & $39$    & $84.6$\\

+CHGS (Primer-FPC)     & $0$  & $0$       & $0$       & $0$       &  $178.2$    & $11.6$ & $0.01$      & $14.8$  & $0.8$  & $2.3$    &  $220.4$     & $6.7$     & $399.4$ & $35.4$ & $84.6$\\

%%%%%%% 3.25X

%Primer-base     & 6622  & 2.7       & 408        & 12.7         & 30             & 32.3          & 0.03      & 53.2  & 3.2  & 7.5    & 6902      & 25.8     & 13965.2 & 134.2 & 85.1\\

%Primer-pack     & 286.5  & 2.7       & 83.5        & 12.7         & 22.4             & 25.1         & 0.03      & 53.2        & 3.2  & 7.5    & 466.1      & 25.8     & 861.7  & 127    & 85.1\\

%Primer-CHGS     & 0  & 0       & 0       & 0       & 385.4     & 37.7 & 0.03      & 48.1 & 3.2  & 7.5    & 466.1      & 21.8     & 854.7 & 115.1 & 85.1\\


%########## old data

%Primer-topk     & 6622  & 2.7       & 80.1        & 2.7         & 5.9             & 6.7          & 0.01      & 21.0  & 0.6  & 4.1    & 1355      & 5.2     & 8063 & 42 & 84.3\\
%Primer-pack     & 286.5  & 2.7       & 16.4        & 2.7         & 4.4             & 5.2          & 0.01      & 21.0  & 0.6  & 4.1    & 91.5      & 5.2     & 399 & 41 & 84.3\\
%Primer-CHGS     & 286.5  & 2.3       & 16.4        & 0.3         & 4.4             & 5.2          & 0.01      & 19.0  & 0.6  & 4.1    & 91.5      & 4.4     & 399 & 35.4 & 84.3\\
%Primer-CHGS     & 0  & 0       & 0       & 0       & 307.3     & 7.8 & 0.01      & 19.0 & 0.6  & 4.1    & 91.5      & 4.4     & 399 & 35.4 & 84.3\\
\bottomrule
\end{tabular}
%\vspace{-0.2in}
\label{t:overall_ablation_study}
\vspace{-0.1in}
\end{table*}

%\subsection{BERT}
\vspace{-0.2in}
\begin{table*}[htbp!]
\centering
\scriptsize
\setlength{\tabcolsep}{5pt}
\caption{The performance of Primer over various BERT models on multiple datasets.}
\vspace{-0.1in}
\begin{tabular}{l|cccc|ccccc|cc|c|c}
\toprule
Model   & \multicolumn{4}{c|}{Hyper-parameters}  & \multicolumn{5}{c|}{GLUE and SQuAD Accuracy (\%)} & \multicolumn{2}{c|}{Latency(s)}& Throughput & \multicolumn{1}{c}{Message}\\\hline
& $N$  & $d_{emb}$ & $H$ &  $n$  & MNLI-m  & MRPC  & SST-2  & SQuAD1 & SQuAD2 & offline  &online & tokens/s & GB\\ \midrule

BERT-tiny  & $3$  & $768$ & $12$  & $30$ & $77.6$ & $79.3$   & $88.2$          & $86.2$  & $76.6$  & $318.5$    & $10.6$ & $2.83$  & $0.9$ \\
BERT-small & $6$  & $768$ & $12$  & $30$ & $81.6$ & $84.5$  & $91.1$       & $88.5$ & $78.2$  &  $345.2$    & $18.9$     & $1.59$ & $1.8$ \\
BERT-base  & $12$ & $768$ & $12$  & $30$ & $84.6$ & $86.3$ &$92.5$             & $90.7$  & $80.3$  & $399.4$   & $35.4$    & $0.85$ & $3.6$\\
BERT-medium& $12$  & $1024$ & $16$ & $30$ & $85.4$ & $86.3$ & $93.1$     & $92.2$  & $81.6$  & $452.8$    & $45.1$    &$0.67$& $3.9$\\
BERT-large  & $24$  & $1024$ & $16$ & $30$ & $86.6$ & $87.6$ & $93.5$     & $93.1$ & $82.9$     & $586.4$  &$91.6$     &$0.33$ & $7.9$\\
%BERT-base-10  & 12 & 768 & 12  & 10 & 84.3 & 86.3 &92.5            & 90.7  & 80.3  & 449.1   & 12.8    & 0.78 & 1.7\\
%BERT-base-90  & 12 & 768 & 12  & 90 & 84.3 & 86.3 &92.5            & 90.7  & 80.3  & 487.4   & 164.6    & 0.55 & 14.4\\
\bottomrule
\end{tabular}
\label{t:overall_comp_bert_all1}
\vspace{-0.15in}
\end{table*}




\section{Experimental Methodology}
\label{s:exp}
\textbf{System setup and security analysis.} We run the privacy-preserving Transformer experiments on two instances that are equipped with an Intel Xeon E7-4850 CPU and 128 GB DRAM, and each instance was provided with 4 threads. In our current system setup, the average network delay between these two instances is 2.3 ms and the bandwidth is about 100 MB/s. %This setting is almost 3x faster than the WAN setting of AWS between us-east-1 to ap-southeast-2. 
The layer-wise PAHE used in Primer is implemented by SEAL~\cite{sealcrypto} libraries where only additive HE operations and rotations are used and ciphertext-ciphertext multiplications are not required. We adopt an extension version of JustGarble tool~\cite{Mihir:Justgarble} used in~\cite{GAZELLE:USENIX18} to implement GC-based operations, including additions of secret sharings and activation functions. The HE parameters and GC settings are selected to provide 128-bit security level. The inputs and weights use 15-bit fix-point representation and the intermediate results are truncated into 15 bits to avoid overflow. The training, fine-tuning, and testing of the Transformer on plaintext was implemented in Python on 4 NVIDIA Tesla V100 GPUs. 

%Transformer training inference methods(Spatten, hardwares, softwares). HE, GC, (Delphi, Gazelle, Safenet, hardwares and softwares) SECRUTY LEVEL : 128 BIT. 



\textbf{Transformer architecture and NLP datasets.} We evaluated Primer on five discriminative NLP models shown in Table~\ref{t:overall_comp_bert_all1}: BERT-Tiny, BERT-small, BERT-base, BERT-medium, BERT-large. The hyper-parameters of these models are listed in Table~\ref{t:overall_comp_bert_all1}. For example, the BERT-tiny model has $N=3$ blocks, $d_{emb}=768$ embedding dimensions, $H=12$ attention heads, and $n=30$ input tokens.  Datasets for five BERT tasks are SQuAD1~\cite{Pranav:SQuAD}, SQuAD2~\cite{lee:etal-2020:squad2}, and MNLI-m, MRPC, SST-2 from GLUE benchmarks~\cite{wang:emnlp2018:glue}. 





%\vspace{-0.1in}
\section{Results and Analysis}
\label{s:res}
%\vspace{-0.1in}
\textbf{Comparison with Prior Works.} We compare our primer with prior works on private BERT-base inference for MNLI-m dataset in table \ref{t:overall_comparisons}. Prior work, e.g., THE-X~\cite{chen:ACL2022} that only uses FHE for private inference only achieves $77.3\%$ accuracy with $4.7k$ seconds latency due to polynomial approximation and expensive FHE operations. We use prior GC-based work~\cite{Deepsecure:dac18} to implement a GCFormer. It achieves an accurate performance, i.e., $85.1\%$ accuracy, but it takes a larger latency than THE-X. Our Primer-F significantly improves
the accuracy of THE-X, e.g., $7.3\%$ accuracy increase, and
    reduces the latency of GCFormer. To reduce the large offline latency of Primer-F, we further propose Primer-FPC, i.e., Primer, with tokens-first packing and combined FHGS. Our primer only takes $\sim 0.4k$-second latency, thus achieving a $\sim 16\times$ latency reduction.


\textbf{Ablation Study.}
Table~\ref{t:overall_ablation_study} describes the performance breakdown and the ablation effects of proposed techniques using BERT-base model with $n=30$ on MNLI-m dataset. The Primer-base implemented by FHE and MPC protocols requires $\sim$ 6553 seconds latency to perform one inference on a sentence in MNLI-m dataset and achieves $84.6\%$ accuracy. We further propose FHGS, denoted by Primer-F to  Offload computations into the offline phase from the online phase, which significantly shrinks the offline latency from $\sim 6553$ seconds to $\sim 41$ seconds, introducing almost $160\times$ latency reduction. Primer-FP is proposed to reduce the latency of embedding layers and the following layers that include HE operations. It further decreases $5.3\%$ online latency over Primer-F and has $16\times$ offline latency reduction. Primer-FPC has the similar offline latency and accuracy with Primer-topk, but further reduces the online latency by $9.2\%$.  Table~\ref{t:overall_ablation_study} shows Primer (Primer-FPC) achieves competitive NLP accuracy and reduces the online and offline inference latency by $90.6\% \sim 97.5\%$ over Primer-base.              




\vspace{0.1in}
\textbf{Results on Different Models.} Table~\ref{t:overall_comp_bert_all1} studies the effects of different BERT models and datasets on Primer. Privacy-preserving BERT-tiny with only 3 Transformer blocks achieves average $81.7\%$ accuracy on three GLUE datasets including MNLI-m , MRPC, SST-2, and average $81.4\%$ F1 test accuracy on SQuAD1 and SQuAD2. Primer with BERT-tiny requires 10.6 seconds to perform an inference or classification on a sequence with $30$ tokens, thereby attaining a throughput of 2.83 tokens per second. Also, the communicational message size between clients and a server is $\sim 0.9$ GB. Primer with BERT-small or BERT-base achieves $2.4\% \sim 7.5\%$ higher accuracy by adding more Transformer blocks, but increases latency by $78.3\% \sim 230\%$. Also, Primer with BERT-small and BERT-base require more than 2$\times$ and 3 $\times$ message size, respectively, than BERT-tiny based Primer.  BERT-medium with larger embedding dimension and BERT-large with 24 block numbers are also supported by Primer, and they take 45.1 seconds and 91.6 seconds to perform an inference on a sentence with $30$ tokens.  BERT-large achieves state-of-the-art accuracy on GLUE and SQuAD benchmarks. 


%\subsection{GPT-2}




%\vspace{-0.1in}
\section{Conclusion}
%\vspace{-0.1in}
In this paper, we present Primer to enable a fast and accurate privacy-preserving Transformer for NLP tasks. First, a na\"ive version of our Primer, called Primer-base, is constructed by a hybrid interactive cryptographic protocol. Secondly, we propose tokens-first packing instead of prior features-first packing to reduce the offline and online overhead brought by HE. Finally, we demonstrate that multiple secret sharing layers in the Transformer can be combined to reduce the latency. Primer establishes a solid baseline and shed the light on private Transformer inference over encrypted data. 

\section*{Acknowledgment}
This work was supported in part by NSF awards CCF-1908992, CCF-1909509, and CCF-2105972. %Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any funding agencies.

%\section{Appendix}

%\textbf{Distinct points of Transformers from CNNs.} Attention in Transformer is different from convolution in DNN. In the private DNN like CryptoNets and Delphi, weights are not encrypted and only inputs are encrypted so that private convolution is a type of ciphertext-plaintext operation. In Attention, however, all query, key, value matrices are encrypted. The Attention operations require ciphertext-ciphertext operations. Ciphertext-ciphertext operations are not only more expensive than ciphertext-plaintext operations but also cannot directly use existing ciphertext packing operations that are proposed for ciphertext-plaintext operations. Prior HGS method works well for convolution, but we identify HGS does not work well in Attention, thus we propose FHGS protocol to solve this problem.

%DNN does not have Embedding, but Transformer requires it. Embedding is used to compress a large and sparse one-hot vector into a small and dense vector. How to efficiently support high-dimension (>30K) matrix multiplication is not studied. Each word (token) will be a vector of size 30522 which is larger than the ciphertext slot numbers. For multiple words in a sentence, how to pack these words into ciphertext is a new challenge. We propose tokens-first packing to tackle this challenge.

%Online pruning and top-k are required in Transformer but DNN can do them offline. DNN weights do not rely on inputs so that weights can be pruned before the deployment. In the privacy-preserving DNN inference, privacy-preserving pruning is not required since the DNN weights are pruned offline. However, Attention weights in Transformer are dependent on the inputs and cannot be pruned in advance. During privacy-preserving inference of Transformer, privacy-preserving pruning on encrypted data is required and it is a new challenge identified by our work.

%\textbf{Top-$k$ tokens pruning for efficient Transformer. }
%Linear transformers~\cite{Reformer,performer} work well in the unencrypted-domain tasks, but they suffer from a huge accuracy loss in the encrypted-domain tasks since float-point values should be quantized into fix-point values to support homomorphic encryption and secret sharing. Instead, linearithmic transformer ~\cite{Wang:hpca2021,zhou:aaai2021:informer} is used in this paper. Since $X_Q$, $X_K$, $X_V$ have $n$ tokens, $\mathcal{O}{(n^2)}$ dot-product and $SoftMax$ operation are required in a MHSA block. A large amount of unessential tokens can be pruned away in human languages to improve efficiency. Recent works~\cite{Wang:hpca2021,zhou:aaai2021:informer} propose tokens pruning to evaluate token importance according to attention probabilities (weight), keep top-$k$ important tokens, and remove trivial ones. $k$ is dependent on tokens number $n$. To maintain a competitive accuracy, $k$ can even be $log(n)$ shown in ~\cite{Wang:hpca2021,zhou:aaai2021:informer}.  $k$ tokens to be selected are determined by an array of accumulative attention probabilities for each token. Here, each of $n$ tokens has $n$ probabilities which are summed to indicate the importance of this token to the sentence. The $k$ tokens with larger accumulative attention probabilities are left and the others are pruned. This token pruning is cascaded which means that once a token is pruned, it is removed in all the later layers. 

%\textbf{Cryptographic Primitives.} 
%HE is an encryption method that enables anyone to perform computations on encrypted data without decryption.  Recent works~\cite{GAZELLE:USENIX18,Delphi:usenix2020} proved that HE has superior performance over MPC methods such as Garbled Circuits (GC) on linear operations, e.g., matrix-vector multiplication in CNN. This is because HE with a ciphertext packing technique supports efficient operations in a SIMD (single instruction multiple data) manner.   Therefore, HE is usually used to support private linear operations where a client encrypts input and sends it to the server, and the server returns encrypted output to the client that decrypts the received output. Since ciphertext-ciphertext multiplication is not required in a private CNN~\cite{GAZELLE:USENIX18,Delphi:usenix2020}, a simple but efficient Packed additive HE (PAHE) is used in prior works. In this paper, we also adapt the same PAHE method to support a private transformer.  GC~\cite{yao1982protocols} and SS~\cite{GMW:1987:SS} are two paramount methods of two-party secure computations. In the state-of-the-art mixed protocols~\cite{Delphi:usenix2020}, GC shows superior performance over HE in non-linear operations such as activation functions. And SS is used to combine GC and HE in the mixed protocol~\cite{Delphi:usenix2020}. Inspired by Beaver's Triple~\cite{beaver1995}, HE can be used to efficiently perform multiplications on two additive secret shares~\cite{Liu:CCS2017:MiniONN,Delphi:usenix2020}.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\textbf{GPT2 Results.} Different from BERT models that are used to perform language inference or classification tasks, GPT2 models are utilized in generative NLP tasks, e.g., machine translation tasks. Table~\ref{t:overall_comp_gpt_all} shows a performance comparison of CryptoNLP with various GPT2 models. GPT2-small is constructed by 12 Transformer decoder blocks, 768 embedding dimension, and 12-head self-attention, achieving 66.56, 29.52, 38.21 perplexity on PBT, Wiki2, Wiki103, LAMB datasets and 18.9 BLEU score  on WMT-14 dataset. Meanwhile, CryptoNLP using GPT2-small generates $\sim$ 0.27 tokens in one second. The latency of CryptoNLP on generative tasks is skipped since each translation task involves multiple-round inferences. Lower perplexity and BLEU score represent better generative performance. For example, lower BLEU shows better machine translation performance. To make CryptoNLP obtain better generative performance, one can enlarge Transformer block numbers $N$ or embedding dimension $d_{emb}$. For instance, GPT2-base and GPT2-large reduce $20.1\%\sim 36.6\%$ perplexity or BLEU score over GPT2-small, but decreases throughput by $55.6\% \sim 77.8\%$ and increases message overhead by $1\times \sim 1.5\times$. 

\bibliographystyle{short}
\bibliography{hebib}
\end{document}