
@article{adler_out_2019,
	title = {Out of distribution detection for intra-operative functional imaging},
	volume = {11840},
	url = {http://arxiv.org/abs/1911.01877},
	doi = {10.1007/978-3-030-32689-0_8},
	abstract = {Multispectral optical imaging is becoming a key tool in the operating room. Recent research has shown that machine learning algorithms can be used to convert pixel-wise reﬂectance measurements to tissue parameters, such as oxygenation. However, the accuracy of these algorithms can only be guaranteed if the spectra acquired during surgery match the ones seen during training. It is therefore of great interest to detect so-called out of distribution (OoD) spectra to prevent the algorithm from presenting spurious results. In this paper we present an information theory based approach to OoD detection based on the widely applicable information criterion (WAIC). Our work builds upon recent methodology related to invertible neural networks (INN). Speciﬁcally, we make use of an ensemble of INNs as we need their tractable Jacobians in order to compute the WAIC. Comprehensive experiments with in silico, and in vivo multispectral imaging data indicate that our approach is well-suited for OoD detection. Our method could thus be an important step towards reliable functional imaging in the operating room.},
	language = {en},
	urldate = {2020-03-06},
	journal = {arXiv:1911.01877 [physics, stat]},
	author = {Adler, Tim J. and Ayala, Leonardo and Ardizzone, Lynton and Kenngott, Hannes G. and Vemuri, Anant and Müller-Stich, Beat P. and Rother, Carsten and Köthe, Ullrich and Maier-Hein, Lena},
	year = {2019},
	note = {arXiv: 1911.01877},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Physics - Medical Physics, Statistics - Machine Learning},
	pages = {75--82},
	file = {Adler et al. - 2019 - Out of distribution detection for intra-operative .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EW4K9MW6\\Adler et al. - 2019 - Out of distribution detection for intra-operative .pdf:application/pdf},
}

@article{adler_out_2019-1,
	title = {Out of distribution detection for intra-operative functional imaging},
	volume = {11840},
	url = {http://arxiv.org/abs/1911.01877},
	doi = {10.1007/978-3-030-32689-0_8},
	abstract = {Multispectral optical imaging is becoming a key tool in the operating room. Recent research has shown that machine learning algorithms can be used to convert pixel-wise reﬂectance measurements to tissue parameters, such as oxygenation. However, the accuracy of these algorithms can only be guaranteed if the spectra acquired during surgery match the ones seen during training. It is therefore of great interest to detect so-called out of distribution (OoD) spectra to prevent the algorithm from presenting spurious results. In this paper we present an information theory based approach to OoD detection based on the widely applicable information criterion (WAIC). Our work builds upon recent methodology related to invertible neural networks (INN). Speciﬁcally, we make use of an ensemble of INNs as we need their tractable Jacobians in order to compute the WAIC. Comprehensive experiments with in silico, and in vivo multispectral imaging data indicate that our approach is well-suited for OoD detection. Our method could thus be an important step towards reliable functional imaging in the operating room.},
	language = {en},
	urldate = {2020-03-06},
	journal = {arXiv:1911.01877 [physics, stat]},
	author = {Adler, Tim J. and Ayala, Leonardo and Ardizzone, Lynton and Kenngott, Hannes G. and Vemuri, Anant and Müller-Stich, Beat P. and Rother, Carsten and Köthe, Ullrich and Maier-Hein, Lena},
	year = {2019},
	note = {arXiv: 1911.01877},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Physics - Medical Physics, Statistics - Machine Learning},
	pages = {75--82},
	file = {Adler et al. - 2019 - Out of distribution detection for intra-operative .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\V5SIKNPR\\Adler et al. - 2019 - Out of distribution detection for intra-operative .pdf:application/pdf},
}

@article{locatello_challenging_2019,
	title = {Challenging {Common} {Assumptions} in the {Unsupervised} {Learning} of {Disentangled} {Representations}},
	url = {http://arxiv.org/abs/1811.12359},
	abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
	urldate = {2020-03-04},
	journal = {arXiv:1811.12359 [cs, stat]},
	author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Rätsch, Gunnar and Gelly, Sylvain and Schölkopf, Bernhard and Bachem, Olivier},
	month = jun,
	year = {2019},
	note = {arXiv: 1811.12359},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VF9TB2FB\\Locatello et al. - 2019 - Challenging Common Assumptions in the Unsupervised.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8EMQSCVV\\1811.html:text/html},
}

@article{grigoroiu_deep_2020,
	title = {Deep learning applied to hyperspectral endoscopy for online spectral classification},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-60574-6},
	doi = {10.1038/s41598-020-60574-6},
	abstract = {Hyperspectral imaging (HSI) is being explored in endoscopy as a tool to extract biochemical information that may improve contrast for early cancer detection in the gastrointestinal tract. Motion artefacts during medical endoscopy have traditionally limited HSI application, however, recent developments in the field have led to real-time HSI deployments. Unfortunately, traditional HSI analysis methods remain unable to rapidly process the volume of hyperspectral data in order to provide real-time feedback to the operator. Here, a convolutional neural network (CNN) is proposed to enable online classification of data obtained during HSI endoscopy. A five-layered CNN was trained and fine-tuned on a dataset of 300 hyperspectral endoscopy images acquired from a planar Macbeth ColorChecker chart and was able to distinguish between its 18 constituent colors with an average accuracy of 94.3\% achieved at 8.8 fps. Performance was then tested on a set of images simulating an endoscopy environment, consisting of color charts warped inside a rigid tube mimicking a lumen. The algorithm proved robust to such variations, with classification accuracies over 90\% being obtained despite the variations, with an average drop in accuracy of 2.4\% being registered at the points of longest working distance and most inclination. For further validation of the color-based classification system, ex vivo videos of a methylene blue dyed pig esophagus and images of different disease stages in the human esophagus were analyzed, showing spatially distinct color classifications. These results suggest that the CNN has potential to provide color-based classification during real-time HSI in endoscopy.},
	language = {en},
	number = {1},
	urldate = {2020-03-04},
	journal = {Scientific Reports},
	author = {Grigoroiu, Alexandru and Yoon, Jonghee and Bohndiek, Sarah E.},
	month = mar,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1--10},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\W4CUNQJB\\Grigoroiu et al. - 2020 - Deep learning applied to hyperspectral endoscopy f.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\LWIHMBQ9\\s41598-020-60574-6.html:text/html},
}

@article{sorrenson_disentanglement_2020,
	title = {Disentanglement by {Nonlinear} {ICA} with {General} {Incompressible}-flow {Networks} ({GIN})},
	url = {http://arxiv.org/abs/2001.04872},
	abstract = {A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modiﬁcation of the RealNVP invertible neural network architecture (Dinh et al., 2016) which is particularly suitable for this type of problem: the General Incompressibleﬂow Network (GIN). Experiments on artiﬁcial data and EMNIST demonstrate that theoretical predictions are indeed veriﬁed in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.},
	language = {en},
	urldate = {2020-03-02},
	journal = {arXiv:2001.04872 [cs, stat]},
	author = {Sorrenson, Peter and Rother, Carsten and Köthe, Ullrich},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.04872},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Sorrenson et al. - 2020 - Disentanglement by Nonlinear ICA with General Inco.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XNNP23NP\\Sorrenson et al. - 2020 - Disentanglement by Nonlinear ICA with General Inco.pdf:application/pdf},
}

@article{dinh_density_2017,
	title = {Density estimation using {Real} {NVP}},
	url = {http://arxiv.org/abs/1605.08803},
	abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Speciﬁcally, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful, stably invertible, and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact and efﬁcient sampling, exact and efﬁcient inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation, and latent variable manipulations.},
	language = {en},
	urldate = {2020-03-02},
	journal = {arXiv:1605.08803 [cs, stat]},
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	month = feb,
	year = {2017},
	note = {arXiv: 1605.08803},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {1605.08803.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KGD669RR\\1605.08803.pdf:application/pdf},
}

@misc{noauthor_7-point_nodate,
	title = {7-point criteria evaluation {Database}},
	url = {http://derm.cs.sfu.ca/Welcome.html},
	urldate = {2020-02-26},
	file = {7-point criteria evaluation Database:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\N4FTTVKJ\\Welcome.html:text/html},
}

@article{ma_effective_2017,
	title = {Effective features to classify skin lesions in dermoscopic images},
	volume = {84},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417417303184},
	doi = {10.1016/j.eswa.2017.05.003},
	abstract = {Features such as shape and color are indispensable to determine whether a skin lesion is a melanoma or not. However, there are no ﬁxed guidelines to deﬁne which features are effective and how to combine them for classiﬁcation. This lack of deﬁnition impedes the development of the automatic analyses of dermoscopic images. In this work, a search for effective features was carried out using a support vector machine. Three image databases were used to verify the feasibility and sensitivity of the automatic classiﬁcation used. The results showed which features had a major inﬂuence on the classiﬁcation performance, and conﬁrmed the need to use various types of features in this process.},
	language = {en},
	urldate = {2020-02-26},
	journal = {Expert Systems with Applications},
	author = {Ma, Zhen and Tavares, João Manuel R.S.},
	month = oct,
	year = {2017},
	pages = {92--101},
	file = {Ma und Tavares - 2017 - Effective features to classify skin lesions in der.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\REE2Z644\\Ma und Tavares - 2017 - Effective features to classify skin lesions in der.pdf:application/pdf},
}

@article{abbas_melanoma_2013,
	title = {Melanoma recognition framework based on expert definition of {ABCD} for dermoscopic images},
	volume = {19},
	issn = {1600-0846},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1600-0846.2012.00614.x},
	doi = {10.1111/j.1600-0846.2012.00614.x},
	abstract = {Background/purpose Melanoma Recognition based on clinical ABCD rule is widely used for clinical diagnosis of pigmented skin lesions in dermoscopy images. However, the current computer-aided diagnostic (CAD) systems for classification between malignant and nevus lesions using the ABCD criteria are imperfect due to use of ineffective computerized techniques. Methods In this study, a novel melanoma recognition system (MRS) is presented by focusing more on extracting features from the lesions using ABCD criteria. The complete MRS system consists of the following six major steps: transformation to the CIEL*a*b* color space, preprocessing to enhance the tumor region, black-frame and hair artifacts removal, tumor-area segmentation, quantification of feature using ABCD criteria and normalization, and finally feature selection and classification. Results The MRS system for melanoma-nevus lesions is tested on a total of 120 dermoscopic images. To test the performance of the MRS diagnostic classifier, the area under the receiver operating characteristics curve (AUC) is utilized. The proposed classifier achieved a sensitivity of 88.2\%, specificity of 91.3\%, and AUC of 0.880. Conclusions The experimental results show that the proposed MRS system can accurately distinguish between malignant and benign lesions. The MRS technique is fully automatic and can easily integrate to an existing CAD system. To increase the classification accuracy of MRS, the CASH pattern recognition technique, visual inspection of dermatologist, contextual information from the patients, and the histopathological tests can be included to investigate the impact with this system.},
	language = {en},
	number = {1},
	urldate = {2020-02-26},
	journal = {Skin Research and Technology},
	author = {Abbas, Qaisar and Celebi, M. Emre and Garcia, Irene Fondón and Ahmad, Waqar},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1600-0846.2012.00614.x},
	keywords = {ABCD criteria, computer-aided diagnostic, dermoscopy, melanoma, pattern recognition},
	pages = {e93--e102},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IXPUFE5J\\Abbas et al. - 2013 - Melanoma recognition framework based on expert def.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4L5RK89Z\\j.1600-0846.2012.00614.html:text/html},
}

@misc{noauthor_covalic_nodate,
	title = {Covalic},
	url = {https://challenge.kitware.com/#challenge/5aab46f156357d5e82b00fe5},
	urldate = {2020-02-26},
	file = {Covalic:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\M3T3V9GY\\challenge.kitware.com.html:text/html},
}

@inproceedings{mendonca_ph2_2013,
	address = {Osaka},
	title = {{PH}$^{\textrm{2}}$ - {A} dermoscopic image database for research and benchmarking},
	isbn = {978-1-4577-0216-7},
	url = {http://ieeexplore.ieee.org/document/6610779/},
	doi = {10.1109/EMBC.2013.6610779},
	abstract = {The increasing incidence of melanoma has recently promoted the development of computer-aided diagnosis systems for the classiﬁcation of dermoscopic images. Unfortunately, the performance of such systems cannot be compared since they are evaluated in different sets of images by their authors and there are no public databases available to perform a fair evaluation of multiple systems. In this paper, a dermoscopic image database, called PH2, is presented. The PH2 database includes the manual segmentation, the clinical diagnosis, and the identiﬁcation of several dermoscopic structures, performed by expert dermatologists, in a set of 200 dermoscopic images. The PH2 database will be made freely available for research and benchmarking purposes.},
	language = {en},
	urldate = {2020-02-26},
	booktitle = {2013 35th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	publisher = {IEEE},
	author = {Mendonca, Teresa and Ferreira, Pedro M. and Marques, Jorge S. and Marcal, Andre R. S. and Rozeira, Jorge},
	month = jul,
	year = {2013},
	pages = {5437--5440},
	file = {Mendonca et al. - 2013 - PHsup2sup - A dermoscopic image database for .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ED2D4ZUG\\Mendonca et al. - 2013 - PHsup2sup - A dermoscopic image database for .pdf:application/pdf},
}

@article{kawahara_7-point_2018,
	title = {7-{Point} {Checklist} and {Skin} {Lesion} {Classiﬁcation} using {Multi}-{Task} {Multi}-{Modal} {Neural} {Nets}},
	abstract = {We propose a multi-task deep convolutional neural network, trained on multi-modal data (clinical and dermoscopic images, and patient meta-data), to classify the 7-point melanoma checklist criteria and perform skin lesion diagnosis. Our neural network is trained using several multi-task loss functions, where each loss considers different combinations of the input modalities, which allows our model to be robust to missing data at inference time. Our ﬁnal model classiﬁes the 7-point checklist and skin condition diagnosis, produces multi-modal feature vectors suitable for image retrieval, and localizes clinically discriminant regions. We benchmark our approach using 1011 lesion cases, and report comprehensive results over all 7-point criteria and diagnosis. We also make our dataset (images and metadata) publicly available online at http://derm.cs.sfu.ca.},
	language = {en},
	author = {Kawahara, Jeremy and Daneshvar, Sara and Argenziano, Giuseppe and Hamarneh, Ghassan},
	year = {2018},
	pages = {8},
	file = {Kawahara et al. - 2018 - 7-Point Checklist and Skin Lesion Classiﬁcation us.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VLVTDJ6L\\Kawahara et al. - 2018 - 7-Point Checklist and Skin Lesion Classiﬁcation us.pdf:application/pdf},
}

@article{giotis_med-node_2015,
	title = {{MED}-{NODE}: {A} computer-assisted melanoma diagnosis system using non-dermoscopic images},
	volume = {42},
	issn = {0957-4174},
	shorttitle = {{MED}-{NODE}},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417415002705},
	doi = {10.1016/j.eswa.2015.04.034},
	abstract = {Melanoma is one of the most aggressive types of skin cancer and in many cases it is difficult to differentiate from benign naevi. In this contribution we present a decision support (expert) system, which we call MED-NODE, able to assist physicians with this challenging task. The proposed system makes use of non-dermoscopic digital images of lesions from which it automatically extracts the lesion regions and then computes descriptors regarding the color and texture. In addition, a set of visual attributes is provided by the examining physician. The automatically extracted descriptors and the attributes provided by the physician are separately used for automatic prediction. Final classification is achieved by a majority vote of all predictions. The proposed system achieves high diagnostic accuracy results (81\%) and performs comparably to state-of-the-art methods that are using dermoscopic images, though such images contain more detailed information and are subject to less noise and illumination effects. The simple input requirements and the robustness of its descriptors allow MED-NODE to be an effective tool within the diagnostic process for melanoma. In addition, the modular nature of the system allows for it to be easily extended.},
	language = {en},
	number = {19},
	urldate = {2020-02-26},
	journal = {Expert Systems with Applications},
	author = {Giotis, Ioannis and Molders, Nynke and Land, Sander and Biehl, Michael and Jonkman, Marcel F. and Petkov, Nicolai},
	month = nov,
	year = {2015},
	keywords = {Classification, Computer-assisted diagnosis, Dermatology, Medical imaging, Melanoma},
	pages = {6578--6585},
	file = {ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\B45DFYJI\\S0957417415002705.html:text/html},
}

@misc{noauthor_dermatology_nodate,
	title = {Dermatology database used in {MED}-{NODE}},
	url = {http://www.cs.rug.nl/~imaging/databases/melanoma_naevi/},
	urldate = {2020-02-26},
	file = {Dermatology database used in MED-NODE:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Y9SVJV9A\\melanoma_naevi.html:text/html},
}

@article{brinker_convolutional_2019,
	title = {A convolutional neural network trained with dermoscopic images performed on par with 145 dermatologists in a clinical melanoma image classification task},
	volume = {111},
	issn = {0959-8049},
	url = {http://www.sciencedirect.com/science/article/pii/S0959804919301443},
	doi = {10.1016/j.ejca.2019.02.005},
	abstract = {Background
Recent studies have demonstrated the use of convolutional neural networks (CNNs) to classify images of melanoma with accuracies comparable to those achieved by board-certified dermatologists. However, the performance of a CNN exclusively trained with dermoscopic images in a clinical image classification task in direct competition with a large number of dermatologists has not been measured to date. This study compares the performance of a convolutional neuronal network trained with dermoscopic images exclusively for identifying melanoma in clinical photographs with the manual grading of the same images by dermatologists.
Methods
We compared automatic digital melanoma classification with the performance of 145 dermatologists of 12 German university hospitals. We used methods from enhanced deep learning to train a CNN with 12,378 open-source dermoscopic images. We used 100 clinical images to compare the performance of the CNN to that of the dermatologists. Dermatologists were compared with the deep neural network in terms of sensitivity, specificity and receiver operating characteristics.
Findings
The mean sensitivity and specificity achieved by the dermatologists with clinical images was 89.4\% (range: 55.0\%–100\%) and 64.4\% (range: 22.5\%–92.5\%). At the same sensitivity, the CNN exhibited a mean specificity of 68.2\% (range 47.5\%–86.25\%). Among the dermatologists, the attendings showed the highest mean sensitivity of 92.8\% at a mean specificity of 57.7\%. With the same high sensitivity of 92.8\%, the CNN had a mean specificity of 61.1\%.
Interpretation
For the first time, dermatologist-level image classification was achieved on a clinical image classification task without training on clinical images. The CNN had a smaller variance of results indicating a higher robustness of computer vision compared with human assessment for dermatologic image classification tasks.},
	language = {en},
	urldate = {2020-02-26},
	journal = {European Journal of Cancer},
	author = {Brinker, Titus J. and Hekler, Achim and Enk, Alexander H. and Klode, Joachim and Hauschild, Axel and Berking, Carola and Schilling, Bastian and Haferkamp, Sebastian and Schadendorf, Dirk and Fröhling, Stefan and Utikal, Jochen S. and von Kalle, Christof and Ludwig-Peitsch, Wiebke and Sirokay, Judith and Heinzerling, Lucie and Albrecht, Magarete and Baratella, Katharina and Bischof, Lena and Chorti, Eleftheria and Dith, Anna and Drusio, Christina and Giese, Nina and Gratsias, Emmanouil and Griewank, Klaus and Hallasch, Sandra and Hanhart, Zdenka and Herz, Saskia and Hohaus, Katja and Jansen, Philipp and Jockenhöfer, Finja and Kanaki, Theodora and Knispel, Sarah and Leonhard, Katja and Martaki, Anna and Matei, Liliana and Matull, Johanna and Olischewski, Alexandra and Petri, Maximilian and Placke, Jan-Malte and Raub, Simon and Salva, Katrin and Schlott, Swantje and Sody, Elsa and Steingrube, Nadine and Stoffels, Ingo and Ugurel, Selma and Sondermann, Wiebke and Zaremba, Anne and Gebhardt, Christoffer and Booken, Nina and Christolouka, Maria and Buder-Bakhaya, Kristina and Bokor-Billmann, Therezia and Enk, Alexander and Gholam, Patrick and Hänßle, Holger and Salzmann, Martin and Schäfer, Sarah and Schäkel, Knut and Schank, Timo and Bohne, Ann-Sophie and Deffaa, Sophia and Drerup, Katharina and Egberts, Friederike and Erkens, Anna-Sophie and Ewald, Benjamin and Falkvoll, Sandra and Gerdes, Sascha and Harde, Viola and Hauschild, Axel and Jost, Marion and Kosova, Katja and Messinger, Laetitia and Metzner, Malte and Morrison, Kirsten and Motamedi, Rogina and Pinczker, Anja and Rosenthal, Anne and Scheller, Natalie and Schwarz, Thomas and Stölzl, Dora and Thielking, Federieke and Tomaschewski, Elena and Wehkamp, Ulrike and Weichenthal, Michael and Wiedow, Oliver and Bär, Claudia Maria and Bender-Säbelkampf, Sophia and Horbrügger, Marc and Karoglan, Ante and Kraas, Luise and Faulhaber, Jörg and Geraud, Cyrill and Guo, Ze and Koch, Philipp and Linke, Miriam and Maurier, Nolwenn and Müller, Verena and Thomas, Benjamin and Utikal, Jochen Sven and Alamri, Ali Saeed M. and Baczako, Andrea and Berking, Carola and Betke, Matthias and Haas, Carolin and Hartmann, Daniela and Heppt, Markus V. and Kilian, Katharina and Krammer, Sebastian and Lapczynski, Natalie Lidia and Mastnik, Sebastian and Nasifoglu, Suzan and Ruini, Cristel and Sattler, Elke and Schlaak, Max and Wolff, Hans and Achatz, Birgit and Bergbreiter, Astrid and Drexler, Konstantin and Ettinger, Monika and Haferkamp, Sebastian and Halupczok, Anna and Hegemann, Marie and Dinauer, Verena and Maagk, Maria and Mickler, Marion and Philipp, Biance and Wilm, Anna and Wittmann, Constanze and Gesierich, Anja and Glutsch, Valerie and Kahlert, Katrin and Kerstan, Andreas and Schilling, Bastian and Schrüfer, Philipp},
	month = apr,
	year = {2019},
	keywords = {Melanoma, Artificial intelligence, Diagnostics, Skin cancer},
	pages = {148--154},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Z5YIU6LI\\Brinker et al. - 2019 - A convolutional neural network trained with dermos.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\F4283ZXR\\S0959804919301443.html:text/html},
}

@article{ali_systematic_2012,
	title = {A {Systematic} {Review} of {Automated} {Melanoma} {Detection} in {Dermatoscopic} {Images} and its {Ground} {Truth} {Data}},
	volume = {1},
	doi = {10.1117/12.912389},
	abstract = {Malignant melanoma is the third most frequent type of skin cancer and one of the most malignant tumors, accounting for 79\% of skin cancer deaths. Melanoma is highly curable if diagnosed early and treated properly as survival rate varies between 15\% and 65\% from early to terminal stages, respectively. So far, melanoma diagnosis is depending subjectively on the dermatologist's expertise. Computer-aided diagnosis (CAD) systems based on epiluminescense light microscopy can provide an objective second opinion on pigmented skin lesions (PSL). This work systematically analyzes the evidence of the effectiveness of automated melanoma detection in images from a dermatoscopic device. Automated CAD applications were analyzed to estimate their diagnostic outcome.
Searching online databases for publication dates between 1985 and 2011, a total of 182 studies on dermatoscopic CAD were found. With respect to the systematic selection criterions, 9 studies were included, published between 2002 and 2011. Those studies formed databases of 14,421
dermatoscopic images including both malignant "melanoma" and benign "nevus", with 8,110 images being available ranging in resolution from 150 x 150 to 1568 x 1045 pixels. Maximum and minimum of sensitivity and specificity are 100.0\% and 80.0\% as well as 98.14\% and 61.6\%,
respectively. Area under the receiver operator characteristics (AUC) and pooled sensitivity, specificity and diagnostics odds ratio are
respectively 0.87, 0.90, 0.81, and 15.89. So, although that automated melanoma detection showed good accuracy in terms of sensitivity,
specificity, and AUC, but diagnostic performance in terms of DOR was found to be poor. This might be due to the lack of dermatoscopic image resources (ground truth) that are needed for comprehensive assessment of diagnostic performance. In future work, we aim at testing this
hypothesis by joining dermatoscopic images into a unified database that serves as a standard reference for dermatology related research in PSL classification.},
	author = {Ali, Abder-Rahman and Deserno, Thomas},
	month = feb,
	year = {2012},
	pages = {50},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MTXEIMRJ\\Ali und Deserno - 2012 - A Systematic Review of Automated Melanoma Detectio.pdf:application/pdf},
}

@article{tschandl_ham10000_2018,
	title = {The {HAM10000} dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions},
	volume = {5},
	copyright = {2018 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata2018161},
	doi = {10.1038/sdata.2018.161},
	abstract = {Design Type(s)   database creation objective • data integration objective • image format conversion objective    Measurement Type(s)   skin lesions    Technology Type(s)   digital curation    Factor Type(s)   diagnosis • Diagnostic Procedure • age • biological sex • animal body part    Sample Characteristic(s)   Homo sapiens • skin of body            Machine-accessible metadata file describing the reported data (ISA-Tab format)},
	language = {en},
	number = {1},
	urldate = {2020-02-26},
	journal = {Scientific Data},
	author = {Tschandl, Philipp and Rosendahl, Cliff and Kittler, Harald},
	month = aug,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1--9},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\URSYE4VP\\Tschandl et al. - 2018 - The HAM10000 dataset, a large collection of multi-.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MLI2PVIJ\\sdata2018161.html:text/html},
}

@article{esteva_dermatologist-level_2017,
	title = {Dermatologist-level classification of skin cancer with deep neural networks},
	volume = {542},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature21056},
	doi = {10.1038/nature21056},
	abstract = {An artificial intelligence trained to classify images of skin lesions as benign lesions or malignant skin cancers achieves the accuracy of board-certified dermatologists.},
	language = {en},
	number = {7639},
	urldate = {2020-02-26},
	journal = {Nature},
	author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
	month = feb,
	year = {2017},
	pages = {115--118},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ML8Z4QXJ\\Esteva et al. - 2017 - Dermatologist-level classification of skin cancer .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2J7F8BZB\\nature21056.html:text/html},
}

@misc{wheeler_is_2019,
	title = {Is the media’s reluctance to admit {AI}’s weaknesses putting us at risk?},
	url = {https://towardsdatascience.com/is-the-medias-reluctance-to-admit-ai-s-weaknesses-putting-us-at-risk-c355728e9028},
	abstract = {Publication bias is shaping our perceptions of AI},
	language = {en},
	urldate = {2020-02-26},
	journal = {Medium},
	author = {Wheeler, Nicole},
	month = aug,
	year = {2019},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3L8GFZ7Z\\is-the-medias-reluctance-to-admit-ai-s-weaknesses-putting-us-at-risk-c355728e9028.html:text/html},
}

@article{codella_skin_2018,
	title = {Skin {Lesion} {Analysis} {Toward} {Melanoma} {Detection}: {A} {Challenge} at the 2017 {International} {Symposium} on {Biomedical} {Imaging} ({ISBI}), {Hosted} by the {International} {Skin} {Imaging} {Collaboration} ({ISIC})},
	shorttitle = {Skin {Lesion} {Analysis} {Toward} {Melanoma} {Detection}},
	url = {http://arxiv.org/abs/1710.05006},
	abstract = {This article describes the design, implementation, and results of the latest installment of the dermoscopic image analysis benchmark challenge. The goal is to support research and development of algorithms for automated diagnosis of melanoma, the most lethal skin cancer. The challenge was divided into 3 tasks: lesion segmentation, feature detection, and disease classiﬁcation. Participation involved 593 registrations, 81 pre-submissions, 46 ﬁnalized submissions (including a 4-page manuscript), and approximately 50 attendees, making this the largest standardized and comparative study in this ﬁeld to date. While the ofﬁcial challenge duration and ranking of participants has concluded, the dataset snapshots remain available for further research and development.},
	language = {en},
	urldate = {2020-02-24},
	journal = {arXiv:1710.05006 [cs]},
	author = {Codella, Noel C. F. and Gutman, David and Celebi, M. Emre and Helba, Brian and Marchetti, Michael A. and Dusza, Stephen W. and Kalloo, Aadi and Liopyris, Konstantinos and Mishra, Nabin and Kittler, Harald and Halpern, Allan},
	month = jan,
	year = {2018},
	note = {arXiv: 1710.05006},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Codella et al. - 2018 - Skin Lesion Analysis Toward Melanoma Detection A .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4FFEXXGQ\\Codella et al. - 2018 - Skin Lesion Analysis Toward Melanoma Detection A .pdf:application/pdf},
}

@article{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2020-02-20},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.05709},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IMJI72JH\\Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\F8ANJ4KZ\\2002.html:text/html},
}

@article{ardizzone_exact_2020,
	title = {Exact {Information} {Bottleneck} with {Invertible} {Neural} {Networks}: {Getting} the {Best} of {Discriminative} and {Generative} {Modeling}},
	shorttitle = {Exact {Information} {Bottleneck} with {Invertible} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2001.06448},
	abstract = {The Information Bottleneck (IB) principle offers a uniﬁed approach to many learning and prediction problems. Although optimal in an information-theoretic sense, practical applications of IB are hampered by a lack of accurate high-dimensional estimators of mutual information, its main constituent. We propose to combine IB with invertible neural networks (INNs), which for the ﬁrst time allows exact calculation of the required mutual information. Applied to classiﬁcation, our proposed method results in a generative classiﬁer we call IB-INN. It accurately models the class conditional likelihoods, generalizes well to unseen data and reliably recognizes out-of-distribution examples. In contrast to existing generative classiﬁers, these advantages incur only minor reductions in classiﬁcation accuracy in comparison to corresponding discriminative methods such as feed-forward networks. Furthermore, we provide insight into why IB-INNs are superior to other generative architectures and training procedures and show experimentally that our method outperforms alternative models of comparable complexity.},
	language = {en},
	urldate = {2020-02-20},
	journal = {arXiv:2001.06448 [cs, stat]},
	author = {Ardizzone, Lynton and Mackowiak, Radek and Köthe, Ullrich and Rother, Carsten},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.06448},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 68T01},
	file = {Ardizzone et al. - 2020 - Exact Information Bottleneck with Invertible Neura.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\S5UFQHRU\\Ardizzone et al. - 2020 - Exact Information Bottleneck with Invertible Neura.pdf:application/pdf},
}

@article{minaee_image_2020,
	title = {Image {Segmentation} {Using} {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Image {Segmentation} {Using} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2001.05566},
	abstract = {Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.},
	language = {en},
	urldate = {2020-02-20},
	journal = {arXiv:2001.05566 [cs]},
	author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.05566},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Minaee et al. - 2020 - Image Segmentation Using Deep Learning A Survey.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\98KGSD76\\Minaee et al. - 2020 - Image Segmentation Using Deep Learning A Survey.pdf:application/pdf},
}

@article{maier-hein_surgical_2018,
	title = {Surgical {Data} {Science}: {A} {Consensus} {Perspective}},
	shorttitle = {Surgical {Data} {Science}},
	url = {http://arxiv.org/abs/1806.03184},
	abstract = {Surgical data science is a scientiﬁc discipline with the objective of improving the quality of interventional healthcare and its value through capturing, organization, analysis, and modeling of data. The goal of the 1st workshop on Surgical Data Science was to bring together researchers working on diverse topics in surgical data science in order to discuss existing challenges, potential standards and new research directions in the ﬁeld. Inspired by current open space and think tank formats, it was organized in June 2016 in Heidelberg. While the ﬁrst day of the workshop, which was dominated by interactive sessions, was open to the public, the second day was reserved for a board meeting on which the information gathered on the public day was processed by (1) discussing remaining open issues, (2) deriving a joint deﬁnition for surgical data science and (3) proposing potential strategies for advancing the ﬁeld. This document summarizes the key ﬁndings.},
	language = {en},
	urldate = {2020-02-16},
	journal = {arXiv:1806.03184 [cs]},
	author = {Maier-Hein, Lena and Eisenmann, Matthias and Feldmann, Carolin and Feussner, Hubertus and Forestier, Germain and Giannarou, Stamatia and Gibaud, Bernard and Hager, Gregory D. and Hashizume, Makoto and Katic, Darko and Kenngott, Hannes and Kikinis, Ron and Kranzfelder, Michael and Malpani, Anand and März, Keno and Müuller-Stich, Beat and Navab, Nassir and Neumuth, Thomas and Padoy, Nicolas and Park, Adrian and Pugh, Carla and Schoch, Nicolai and Stoyanov, Danail and Taylor, Russell and Wagner, Martin and Vedula, S. Swaroop and Jannin*, Pierre and Speidel*, Stefanie},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.03184},
	keywords = {Computer Science - Computers and Society},
	file = {Maier-Hein et al. - 2018 - Surgical Data Science A Consensus Perspective.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\39D7ZAFH\\Maier-Hein et al. - 2018 - Surgical Data Science A Consensus Perspective.pdf:application/pdf},
}

@article{fabelo_spatio-spectral_2018,
	title = {Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0193721},
	doi = {10.1371/journal.pone.0193721},
	abstract = {Surgery for brain cancer is a major problem in neurosurgery. The diffuse infiltration into the surrounding normal brain by these tumors makes their accurate identification by the naked eye difficult. Since surgery is the common treatment for brain cancer, an accurate radical resection of the tumor leads to improved survival rates for patients. However, the identification of the tumor boundaries during surgery is challenging. Hyperspectral imaging is a non-contact, non-ionizing and non-invasive technique suitable for medical diagnosis. This study presents the development of a novel classification method taking into account the spatial and spectral characteristics of the hyperspectral images to help neurosurgeons to accurately determine the tumor boundaries in surgical-time during the resection, avoiding excessive excision of normal tissue or unintentionally leaving residual tumor. The algorithm proposed in this study to approach an efficient solution consists of a hybrid framework that combines both supervised and unsupervised machine learning methods. Firstly, a supervised pixel-wise classification using a Support Vector Machine classifier is performed. The generated classification map is spatially homogenized using a one-band representation of the HS cube, employing the Fixed Reference t-Stochastic Neighbors Embedding dimensional reduction algorithm, and performing a K-Nearest Neighbors filtering. The information generated by the supervised stage is combined with a segmentation map obtained via unsupervised clustering employing a Hierarchical K-Means algorithm. The fusion is performed using a majority voting approach that associates each cluster with a certain class. To evaluate the proposed approach, five hyperspectral images of surface of the brain affected by glioblastoma tumor in vivo from five different patients have been used. The final classification maps obtained have been analyzed and validated by specialists. These preliminary results are promising, obtaining an accurate delineation of the tumor area.},
	language = {en},
	number = {3},
	urldate = {2020-02-16},
	journal = {PLOS ONE},
	author = {Fabelo, Himar and Ortega, Samuel and Ravi, Daniele and Kiran, B. Ravi and Sosa, Coralia and Bulters, Diederik and Callicó, Gustavo M. and Bulstrode, Harry and Szolna, Adam and Piñeiro, Juan F. and Kabwama, Silvester and Madroñal, Daniel and Lazcano, Raquel and J-O’Shanahan, Aruma and Bisshopp, Sara and Hernández, María and Báez, Abelardo and Yang, Guang-Zhong and Stanciulescu, Bogdan and Salvador, Rubén and Juárez, Eduardo and Sarmiento, Roberto},
	month = mar,
	year = {2018},
	keywords = {Algorithms, Blood vessels, Cancer detection and diagnosis, In vivo imaging, Support vector machines, Surgical and invasive medical procedures, Surgical oncology, Tumor resection},
	pages = {e0193721},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Z9EQTDSQ\\Fabelo et al. - 2018 - Spatio-spectral classification of hyperspectral im.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MTDAXJDW\\article.html:text/html},
}

@incollection{ourselin_probe-based_2016,
	address = {Cham},
	title = {Probe-{Based} {Rapid} {Hybrid} {Hyperspectral} and {Tissue} {Surface} {Imaging} {Aided} by {Fully} {Convolutional} {Networks}},
	volume = {9902},
	isbn = {978-3-319-46725-2 978-3-319-46726-9},
	url = {http://link.springer.com/10.1007/978-3-319-46726-9_48},
	abstract = {Tissue surface shape and reflectance spectra provide rich intraoperative information useful in surgical guidance. We propose a hybrid system which displays an endoscopic image with a fast joint inspection of tissue surface shape using structured light (SL) and hyperspectral imaging (HSI). For SL a miniature fibre probe is used to project a coloured spot pattern onto the tissue surface. In HSI mode standard endoscopic illumination is used, with the fibre probe collecting reflected light and encoding the spatial information into a linear format that can be imaged onto the slit of a spectrograph. Correspondence between the arrangement of fibres at the distal and proximal ends of the bundle was found using spectral encoding. Then during pattern decoding, a fully convolutional network (FCN) was used for spot detection, followed by a matching propagation algorithm for spot identification. This method enabled fast reconstruction (12 frames per second) using a GPU. The hyperspectral image was combined with the white light image and the reconstructed surface, showing the spectral information of different areas. Validation of this system using phantom and ex vivo experiments has been demonstrated.},
	language = {en},
	urldate = {2020-02-16},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} - {MICCAI} 2016},
	publisher = {Springer International Publishing},
	author = {Lin, Jianyu and Clancy, Neil T. and Sun, Xueqing and Qi, Ji and Janatka, Mirek and Stoyanov, Danail and Elson, Daniel S.},
	editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
	year = {2016},
	doi = {10.1007/978-3-319-46726-9_48},
	pages = {414--422},
	file = {Lin et al. - 2016 - Probe-Based Rapid Hybrid Hyperspectral and Tissue .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SIAFPWRH\\Lin et al. - 2016 - Probe-Based Rapid Hybrid Hyperspectral and Tissue .pdf:application/pdf},
}

@article{dang_deep-tissue_2019,
	title = {Deep-tissue optical imaging of near cellular-sized features},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-39502-w},
	doi = {10.1038/s41598-019-39502-w},
	abstract = {Detection of biological features at the cellular level with sufficient sensitivity in complex tissue remains a major challenge. To appreciate this challenge, this would require finding tens to hundreds of cells (a 0.1 mm tumor has {\textasciitilde}125 cells), out of {\textasciitilde}37 trillion cells in the human body. Near-infrared optical imaging holds promise for high-resolution, deep-tissue imaging, but is limited by autofluorescence and scattering. To date, the maximum reported depth using second-window near-infrared (NIR-II: 1000–1700 nm) fluorophores is 3.2 cm through tissue. Here, we design an NIR-II imaging system, “Detection of Optically Luminescent Probes using Hyperspectral and diffuse Imaging in Near-infrared” (DOLPHIN), that resolves these challenges. DOLPHIN achieves the following: (i) resolution of probes through up to 8 cm of tissue phantom; (ii) identification of spectral and scattering signatures of tissues without a priori knowledge of background or autofluorescence; and (iii) 3D reconstruction of live whole animals. Notably, we demonstrate noninvasive real-time tracking of a 0.1 mm-sized fluorophore through the gastrointestinal tract of a living mouse, which is beyond the detection limit of current imaging modalities.},
	language = {en},
	number = {1},
	urldate = {2020-02-16},
	journal = {Scientific Reports},
	author = {Dang, Xiangnan and Bardhan, Neelkanth M. and Qi, Jifa and Gu, Li and Eze, Ngozi A. and Lin, Ching-Wei and Kataria, Swati and Hammond, Paula T. and Belcher, Angela M.},
	month = mar,
	year = {2019},
	pages = {1--12},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YL9A6X7I\\Dang et al. - 2019 - Deep-tissue optical imaging of near cellular-sized.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4WMNHCMH\\s41598-019-39502-w.html:text/html},
}

@article{dahlstrand_extended-wavelength_2019,
	title = {Extended-wavelength diffuse reflectance spectroscopy with a machine-learning method for in vivo tissue classification},
	volume = {14},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0223682},
	doi = {10.1371/journal.pone.0223682},
	abstract = {Objectives OPEN ACCESS Citation: Dahlstrand U, Sheikh R, Dybelius Ansson C, Memarzadeh K, Reistad N, Malmsjo¨ M (2019) Extended-wavelength diffuse reflectance spectroscopy with a machine-learning method for in vivo tissue classification. PLoS ONE 14(10): e0223682. https://doi.org/10.1371/journal. pone.0223682 Editor: Alberto Dalla Mora, Politecnico di Milano, ITALY Received: May 6, 2019 Accepted: September 25, 2019 Published: October 10, 2019 Copyright: © 2019 Dahlstrand et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Data Availability Statement: All relevant data are within the manuscript and its Supporting Information files. An extended-wavelength diffuse reflectance spectroscopy (EWDRS) technique was evaluated for its ability to differentiate between and classify different skin and tissue types in an in vivo pig model. Materials and methods EWDRS recordings (450–1550 nm) were made on skin with different degrees of pigmentation as well as on the pig snout and tongue. The recordings were used to train a support vector machine to identify and classify the different skin and tissue types.
Results The resulting EWDRS curves for each skin and tissue type had a unique profile. The support vector machine was able to classify each skin and tissue type with an overall accuracy of 98.2\%. The sensitivity and specificity were between 96.4 and 100.0\% for all skin and tissue types.
Conclusion EWDRS can be used in vivo to differentiate between different skin and tissue types with good accuracy. Further development of the technique may potentially lead to a novel diagnostic tool for e.g. non-invasive tumor margin delineation.},
	language = {en},
	number = {10},
	urldate = {2020-02-15},
	journal = {PLOS ONE},
	author = {Dahlstrand, Ulf and Sheikh, Rafi and Dybelius Ansson, Cu and Memarzadeh, Khashayar and Reistad, Nina and Malmsjö, Malin},
	editor = {Dalla Mora, Alberto},
	month = oct,
	year = {2019},
	pages = {e0223682},
	file = {Dahlstrand et al. - 2019 - Extended-wavelength diffuse reflectance spectrosco.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PCLKZY8G\\Dahlstrand et al. - 2019 - Extended-wavelength diffuse reflectance spectrosco.pdf:application/pdf},
}

@article{jonasson_vivo_2018,
	title = {In vivo characterization of light scattering properties of human skin in the 475- to 850-nm wavelength range in a {Swedish} cohort},
	volume = {23},
	issn = {1083-3668, 1560-2281},
	url = {https://www.spiedigitallibrary.org/journals/Journal-of-Biomedical-Optics/volume-23/issue-12/121608/In-vivo-characterization-of-light-scattering-properties-of-human-skin/10.1117/1.JBO.23.12.121608.short},
	doi = {10.1117/1.JBO.23.12.121608},
	abstract = {We have determined in vivo optical scattering properties of normal human skin in 1734 subjects, mostly with fair skin type, within the Swedish CArdioPulmonary bioImage Study. The measurements were performed with a noninvasive system, integrating spatially resolved diffuse reflectance spectroscopy and laser Doppler flowmetry. Data were analyzed with an inverse Monte Carlo algorithm, accounting for both scattering, geometrical, and absorbing properties of the tissue. The reduced scattering coefficient was found to decrease from 3.16 ± 0.72 to 1.13 ± 0.27 mm−1 (mean ± SD) in the 475- to 850-nm wavelength range. There was a negative correlation between the reduced scattering coefficient and age, and a significant difference between men and women in the reduced scattering coefficient as well as in the fraction of small scattering particles. This large study on tissue scattering with mean values and normal variation can serve as a reference when designing diagnostic techniques or when evaluating the effect of therapeutic optical systems.},
	number = {12},
	urldate = {2020-02-15},
	journal = {Journal of Biomedical Optics},
	author = {Jonasson, Hanna and Fredriksson, Ingemar and Bergstrand, Sara and Östgren, Carl Johan and Larsson, Marcus and Strömberg, Tomas},
	month = sep,
	year = {2018},
	pages = {121608},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\HJ9PD5WF\\Jonasson et al. - 2018 - In vivo characterization of light scattering prope.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\I3NNAK26\\1.JBO.23.12.121608.html:text/html},
}

@article{yoon_clinically_2019,
	title = {A clinically translatable hyperspectral endoscopy ({HySE}) system for imaging the gastrointestinal tract},
	volume = {10},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-019-09484-4},
	doi = {10.1038/s41467-019-09484-4},
	language = {en},
	number = {1},
	urldate = {2020-02-11},
	journal = {Nature Communications},
	author = {Yoon, Jonghee and Joseph, James and Waterhouse, Dale J. and Luthman, A. Siri and Gordon, George S. D. and di Pietro, Massimiliano and Januszewicz, Wladyslaw and Fitzgerald, Rebecca C. and Bohndiek, Sarah E.},
	month = dec,
	year = {2019},
	pages = {1902},
}

@article{nepogodiev_global_2019,
	title = {Global burden of postoperative death},
	volume = {393},
	issn = {01406736},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673618331398},
	doi = {10.1016/S0140-6736(18)33139-8},
	language = {en},
	number = {10170},
	urldate = {2020-02-10},
	journal = {The Lancet},
	author = {Nepogodiev, Dmitri and Martin, Janet and Biccard, Bruce and Makupe, Alex and Bhangu, Aneel and Nepogodiev, Dmitri and Martin, Janet and Biccard, Bruce and Makupe, Alex and Ademuyiwa, Adesoji and Adisa, Adewale Oluseye and Aguilera, Maria-Lorena and Chakrabortee, Sohini and Fitzgerald, J. Edward and Ghosh, Dhruva and Glasbey, James C. and Harrison, Ewen M. and Ingabire, J.C. Allen and Salem, Hosni and Lapitan, Marie Carmela and Lawani, Ismail and Lissauer, David and Magill, Laura and Moore, Rachel and Osei-Bordom, Daniel C. and Pinkney, Thomas D. and Qureshi, Ahmad Uzair and Ramos-De la Medina, Antonio and Rayne, Sarah and Sundar, Sudha and Tabiri, Stephen and Verjee, Azmina and Yepez, Raul and Garden, O. James and Lilford, Richard and Brocklehurst, Peter and Morton, Dion G. and Bhangu, Aneel},
	month = feb,
	year = {2019},
	pages = {401},
	file = {Nepogodiev et al. - 2019 - Global burden of postoperative death.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SIDBI2C6\\Nepogodiev et al. - 2019 - Global burden of postoperative death.pdf:application/pdf},
}

@article{tummers_intraoperative_2018,
	title = {Intraoperative {Pancreatic} {Cancer} {Detection} using {Tumor}-{Specific} {Multimodality} {Molecular} {Imaging}},
	volume = {25},
	issn = {1068-9265},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5977989/},
	doi = {10.1245/s10434-018-6453-2},
	abstract = {Background
Operative management of pancreatic ductal adenocarcinoma (PDAC) is complicated by several key decisions during the procedure. Identification of metastatic disease at the outset and, when none is found, complete (R0) resection of primary tumor are key to optimizing clinical outcomes. The use of tumor-targeted molecular imaging, based on photoacoustic and fluorescence optical imaging, can provide crucial information to the surgeon. The first-in-human use of multimodality molecular imaging for intraoperative detection of pancreatic cancer is reported using cetuximab-IRDye800, a near-infrared fluorescent agent that binds to epidermal growth factor receptor.

Methods
A dose-escalation study was performed to assess safety and feasibility of targeting and identifying PDAC in a tumor-specific manner using cetuximab-IRDye800 in patients undergoing surgical resection for pancreatic cancer. Patients received a loading dose of 100 mg of unlabeled cetuximab before infusion of cetuximab-IRDye800 (50 mg or 100 mg). Multi-instrument fluorescence imaging was performed throughout the surgery in addition to fluorescence and photoacoustic imaging ex vivo.

Results
Seven patients with resectable pancreatic masses suspected to be PDAC were enrolled in this study. Fluorescence imaging successfully identified tumor with a significantly higher mean fluorescence intensity in the tumor (0.09 ± 0.06) versus surrounding normal pancreatic tissue (0.02 ± 0.01), and pancreatitis (0.04 ± 0.01; p {\textless} 0.001), with a sensitivity of 96.1\% and specificity of 67.0\%. The mean photoacoustic signal in the tumor site was 3.7-fold higher than surrounding tissue.

Conclusions
The safety and feasibilty of intraoperative, tumor-specific detection of PDAC using cetuximab-IRDye800 with multimodal molecular imaging of the primary tumor and metastases was demonstrated.},
	number = {7},
	urldate = {2020-02-10},
	journal = {Annals of surgical oncology},
	author = {Tummers, Willemieke S. and Miller, Sarah E. and Teraphongphom, Nutte T. and Gomez, Adam and Steinberg, Idan and Huland, David M. and Hong, Steve and Kothapalli, Sri-Rajasekhar and Hasan, Alifia and Ertsey, Robert and Bonsing, Bert A. and Vahrmeijer, Alexander L. and Swijnenburg, Rutger-Jan and Longacre, Teri A. and Fisher, George A. and Gambhir, Sanjiv S. and Poultsides, George A. and Rosenthal, Eben L.},
	month = jul,
	year = {2018},
	pmid = {29667116},
	pmcid = {PMC5977989},
	pages = {1880--1888},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DEERSG8K\\Tummers et al. - 2018 - Intraoperative Pancreatic Cancer Detection using T.pdf:application/pdf},
}

@article{hanahan_hallmarks_2011,
	title = {Hallmarks of cancer: the next generation},
	volume = {144},
	issn = {1097-4172},
	shorttitle = {Hallmarks of cancer},
	doi = {10.1016/j.cell.2011.02.013},
	abstract = {The hallmarks of cancer comprise six biological capabilities acquired during the multistep development of human tumors. The hallmarks constitute an organizing principle for rationalizing the complexities of neoplastic disease. They include sustaining proliferative signaling, evading growth suppressors, resisting cell death, enabling replicative immortality, inducing angiogenesis, and activating invasion and metastasis. Underlying these hallmarks are genome instability, which generates the genetic diversity that expedites their acquisition, and inflammation, which fosters multiple hallmark functions. Conceptual progress in the last decade has added two emerging hallmarks of potential generality to this list-reprogramming of energy metabolism and evading immune destruction. In addition to cancer cells, tumors exhibit another dimension of complexity: they contain a repertoire of recruited, ostensibly normal cells that contribute to the acquisition of hallmark traits by creating the "tumor microenvironment." Recognition of the widespread applicability of these concepts will increasingly affect the development of new means to treat human cancer.},
	language = {eng},
	number = {5},
	journal = {Cell},
	author = {Hanahan, Douglas and Weinberg, Robert A.},
	month = mar,
	year = {2011},
	pmid = {21376230},
	keywords = {Animals, Genomic Instability, Humans, Neoplasm Invasiveness, Neoplasms, Signal Transduction, Stromal Cells},
	pages = {646--674},
	file = {Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MSBWZPRX\\Hanahan und Weinberg - 2011 - Hallmarks of cancer the next generation.pdf:application/pdf},
}

@article{li_benign_2019,
	title = {Benign and malignant classification of mammogram images based on deep learning},
	volume = {51},
	issn = {17468094},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1746809419300618},
	doi = {10.1016/j.bspc.2019.02.017},
	abstract = {Breast cancer is one of the most common malignant tumors in women, which seriously affect women’s physical and mental health and even threat to life. At present, mammography is an important criterion for doctors to diagnose breast cancer. However, due to the complex structure of mammogram images, it is relatively difﬁcult for doctors to identify breast cancer features. At present, deep learning is the most mainstream image classiﬁcation algorithm. Therefore, this study proposes an improved DenseNet neural network model, also known as the DenseNet-II neural network model, for the effective and accurate classiﬁcation of benign and malignant mammography images. Firstly, the mammogram images are preprocessed. Image normalization avoids interference from light, while the adoption of data enhancement prevents over-ﬁtting cause by small data set. Secondly, the DenseNet neural network model is improved, and a new DenseNet-II neural network model is invented, which is to replace the ﬁrst convolutional layer of the DenseNet neural network model with the Inception structure. Finally, the pre-processed mammogram datasets are input into AlexNet, VGGNet, GoogLeNet, DenseNet network model and DenseNet-II neural network model, and the experimental results are analyzed and compared. According to the 10-fold cross validation method, the results show that the DenseNet-II neural network model has better classiﬁcation performance than other network models. The average accuracy of the model reaches 94.55\%, which improves the accuracy of the benign and malignant classiﬁcation of mammogram images. At the same time, it also proves that the model has good generalization and robustness.},
	language = {en},
	urldate = {2020-02-10},
	journal = {Biomedical Signal Processing and Control},
	author = {Li, Hua and Zhuang, Shasha and Li, Deng-ao and Zhao, Jumin and Ma, Yanyun},
	month = may,
	year = {2019},
	pages = {347--354},
	file = {Li et al. - 2019 - Benign and malignant classification of mammogram i.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PYUGWTHZ\\Li et al. - 2019 - Benign and malignant classification of mammogram i.pdf:application/pdf},
}

@inproceedings{trussell_dominance_2012,
	title = {The dominance of {Poisson} noise in color digital cameras},
	doi = {10.1109/ICIP.2012.6466862},
	abstract = {The various components that contribute to the noise in digital cameras are primarily Poisson noise from the discrete nature of the photons, Gaussian and Poisson noise from various electronic and thermal processes and uniform noise from quantization. Our experiments show that the photon Poisson noise is the dominant contributor to uncertainty in the raw data captured by high-performance sensors using in color digital cameras. This finding is consistent with previous monochrome work and indicates that research on denoising and demosaicking of color digital images should consider Poisson noise models as more important than the usual Gaussian models.},
	booktitle = {2012 19th {IEEE} {International} {Conference} on {Image} {Processing}},
	author = {Trussell, H. J. and Zhang, R.},
	month = sep,
	year = {2012},
	note = {ISSN: 2381-8549},
	keywords = {cameras, Cameras, color digital cameras, color digital image demosaicking, color digital image denoising, Colored noise, demosaicking, denoising, digital cameras, digital imaging, discrete nature, Gaussian noise, image coding, Image color analysis, image colour analysis, image denoising, image segmentation, Lighting, noise models, Photonics, photons, Poisson noise, quantization, Quantization},
	pages = {329--332},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\96YCG78J\\6466862.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6S3PGH69\\Trussell und Zhang - 2012 - The dominance of Poisson noise in color digital ca.pdf:application/pdf},
}

@article{van_de_weijer_edge-based_2007,
	title = {Edge-{Based} {Color} {Constancy}},
	volume = {16},
	issn = {1057-7149},
	url = {http://ieeexplore.ieee.org/document/4287009/},
	doi = {10.1109/TIP.2007.901808},
	abstract = {Color constancy is the ability to measure colors of objects independent of the color of the light source. A well-known color constancy method is based on the Grey-World assumption which assumes that the average reﬂectance of surfaces in the world is achromatic. In this article, we propose a new hypothesis for color constancy namely the Grey-Edge hypothesis, which assumes that the average edge difference in a scene is achromatic. Based on this hypothesis, we propose an algorithm for color constancy. Contrary to existing color constancy algorithms, which are computed from the zero-order structure of images, our method is based on the derivative structure of images. Furthermore, we propose a framework which uniﬁes a variety of known (Grey-World, max-RGB, Minkowski norm) and the newly proposed Grey-Edge and higher-order Grey-Edge algorithms. The quality of the various instantiations of the framework is tested and compared to the state-of-the-art color constancy methods on two large data sets of images recording objects under a large number of different light sources. The experiments show that the proposed color constancy algorithms obtain comparable results as the state-of-the-art color constancy methods with the merit of being computationally more efﬁcient.},
	language = {en},
	number = {9},
	urldate = {2020-02-05},
	journal = {IEEE Transactions on Image Processing},
	author = {van de Weijer, J. and Gevers, T. and Gijsenij, A.},
	month = sep,
	year = {2007},
	pages = {2207--2214},
	file = {van de Weijer et al. - 2007 - Edge-Based Color Constancy.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\LDNMU3SL\\van de Weijer et al. - 2007 - Edge-Based Color Constancy.pdf:application/pdf},
}

@article{finlayson_solving_nodate,
	title = {Solving for {Colour} {Constancy} using a {Constrained} {Dichromatic} {Reflection} {Model}},
	abstract = {Statistics-based colour constancy algorithms work well as long as there are many colours in a scene, they fail however when the encountering scenes comprise few surfaces. In contrast, physics-based algorithms, based on an understanding of physical processes such as highlights and interreﬂections, are theoretically able to solve for colour constancy even when there are as few as two surfaces in a scene. Unfortunately, physics-based theories rarely work outside the lab. In this paper we show that a combination of physical and statistical knowledge leads to a surprisingly simple and powerful colour constancy algorithm, one that also works well for images of natural scenes.},
	language = {en},
	author = {Finlayson, Graham D and Schaefer, Gerald},
	pages = {18},
	file = {Finlayson und Schaefer - Solving for Colour Constancy using a Constrained D.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KNMQFITH\\Finlayson und Schaefer - Solving for Colour Constancy using a Constrained D.pdf:application/pdf},
}

@misc{finlayson_shades_2004,
	type = {Text},
	title = {Shades of {Gray} and {Colour} {Constancy}},
	url = {https://www.ingentaconnect.com/content/ist/cic/2004/00002004/00000001/art00008},
	language = {en},
	urldate = {2020-02-05},
	author = {Finlayson, Graham D. and Trezzi, Elisabetta},
	year = {2004},
	file = {6cdadfec869f136602ea41cad8b07e3f8ddb.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MXA87X7U\\6cdadfec869f136602ea41cad8b07e3f8ddb.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QSRB6QH9\\art00008.html:text/html},
}

@article{khan_illuminant_2017,
	title = {Illuminant estimation in multispectral imaging},
	volume = {34},
	copyright = {\&\#169; 2017 Optical Society of America},
	issn = {1520-8532},
	url = {https://www.osapublishing.org/josaa/abstract.cfm?uri=josaa-34-7-1085},
	doi = {10.1364/JOSAA.34.001085},
	abstract = {With the advancement in sensor technology, the use of multispectral imaging is gaining wide popularity for computer vision applications. Multispectral imaging is used to achieve better discrimination between the radiance spectra, as compared to the color images. However, it is still sensitive to illumination changes. This study evaluates the potential evolution of illuminant estimation models from color to multispectral imaging. We first present a state of the art on computational color constancy and then extend a set of algorithms to use them in multispectral imaging. We investigate the influence of camera spectral sensitivities and the number of channels. Experiments are performed on simulations over hyperspectral data. The outcomes indicate that extension of computational color constancy algorithms from color to spectral gives promising results and may have the potential to lead towards efficient and stable representation across illuminants. However, this is highly dependent on spectral sensitivities and noise. We believe that the development of illuminant invariant multispectral imaging systems will be a key enabler for further use of this technology.},
	language = {EN},
	number = {7},
	urldate = {2020-02-05},
	journal = {JOSA A},
	author = {Khan, Haris Ahmad and Thomas, Jean-Baptiste and Hardeberg, Jon Yngve and Laligant, Olivier},
	month = jul,
	year = {2017},
	keywords = {Hyperspectral imaging, Imaging noise, Imaging systems, Imaging techniques, Multispectral imaging, Spectral imaging},
	pages = {1085--1098},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XM6Y5US7\\Khan et al. - 2017 - Illuminant estimation in multispectral imaging.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3GWEKHCZ\\abstract.html:text/html},
}

@article{holmer_hyperspectral_2018,
	title = {Hyperspectral imaging in perfusion and wound diagnostics – methods and algorithms for the determination of tissue parameters},
	volume = {63},
	issn = {0013-5585},
	url = {https://www.degruyter.com/view/j/bmte.2018.63.issue-5/bmt-2017-0155/bmt-2017-0155.xml},
	doi = {10.1515/bmt-2017-0155},
	abstract = {Blood perfusion is the supply of tissue with blood, and oxygen is a key factor in the field of minor and major wound healing. Reduced perfusion of a wound bed or transplant often causes various complications. Reliable methods for an objective evaluation of perfusion status are still lacking, and insufficient perfusion may remain undiscovered, resulting in chronic processes and failing transplants. Hyperspectral imaging (HSI) represents a novel method with increasing importance for clinical practice. Therefore, methods, software and algorithms for a new HSI system are presented which can be used to observe tissue oxygenation and other parameters that are of importance in supervising healing processes. This could offer an improved insight into wound perfusion allowing timely intervention.},
	number = {5},
	urldate = {2020-02-04},
	journal = {Biomedical Engineering / Biomedizinische Technik},
	author = {Holmer, Amadeus and Marotz, Jörg and Wahl, Philip and Dau, Michael and Kämmerer, Peer W.},
	year = {2018},
	keywords = {hyperspectral imaging, perfusion monitoring, tissue oximetry imaging},
	pages = {547--556},
	file = {Holmer et al. - 2018 - Hyperspectral imaging in perfusion and wound diagn.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UYBQ9HJS\\Holmer et al. - 2018 - Hyperspectral imaging in perfusion and wound diagn.pdf:application/pdf},
}

@article{healey_radiometric_1994,
	title = {Radiometric {CCD} camera calibration and noise estimation},
	volume = {16},
	issn = {1939-3539},
	doi = {10.1109/34.276126},
	abstract = {Changes in measured image irradiance have many physical causes and are the primary cue for several visual processes, such as edge detection and shape from shading. Using physical models for charged-coupled device (CCD) video cameras and material reflectance, we quantify the variation in digitized pixel values that is due to sensor noise and scene variation. This analysis forms the basis of algorithms for camera characterization and calibration and for scene description. Specifically, algorithms are developed for estimating the parameters of camera noise and for calibrating a camera to remove the effects of fixed pattern nonuniformity and spatial variation in dark current. While these techniques have many potential uses, we describe in particular how they can be used to estimate a measure of scene variation. This measure is independent of image irradiance and can be used to identify a surface from a single sensor band over a range of situations. Experimental results confirm that the models presented in this paper are useful for modeling the different sources of variation in real images obtained from video cameras.{\textless}{\textgreater}},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Healey, G.E. and Kondepudy, R.},
	month = mar,
	year = {1994},
	keywords = {Cameras, calibration, Calibration, camera characterization, CCD image sensors, Charge coupled devices, Charge-coupled image sensors, computer vision, dark current, digitized pixel values, edge detection, fixed pattern nonuniformity, Image edge detection, Layout, material reflectance, noise estimation, Noise shaping, parameter estimation, primary cue, radiometric CCD camera calibration, radiometry, Radiometry, reflectivity, Reflectivity, scene description, scene variation, semiconductor device models, semiconductor device noise, sensor noise, shape from shading, Shape measurement, spatial variation, video cameras, visual processes},
	pages = {267--276},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\E6KVLCA6\\276126.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VCWWCGDC\\Healey und Kondepudy - 1994 - Radiometric CCD camera calibration and noise estim.pdf:application/pdf},
}

@article{katrasnik_radiometric_2013,
	title = {Radiometric calibration and noise estimation of acousto-optic tunable filter hyperspectral imaging systems},
	volume = {52},
	copyright = {\&\#169; 2013 Optical Society of America},
	issn = {2155-3165},
	url = {https://www.osapublishing.org/ao/abstract.cfm?uri=ao-52-15-3526},
	doi = {10.1364/AO.52.003526},
	abstract = {The accuracy of the radiometric response of acousto-optic tunable filter (AOTF) hyperspectral imaging systems is crucial for obtaining reliable measurements. It is therefore important to know the radiometric response and noise characteristics of the hyperspectral imaging system used. A radiometric model of an AOTF hyperspectral imaging system composed of an imaging sensor radiometric model (CCD, CMOS, and sCMOS) and an AOTF light transmission model is proposed. Using the radiometric model, a method for obtaining the fixed pattern noise (FPN) of the imaging system by displacing and imaging an illuminated reference target is developed. Methods for estimating the temporal noise of the imaging system, using the photon transfer method, and for correcting FPN are also presented. Noise estimation and image restoration methods were tested on an AOTF hyperspectral imaging system. The results indicate that the developed methods can accurately calculate temporal and FPN, and can effectively correct the acquired images. After correction, the signal-to-noise ratio of the acquired images was shown to increase by 26\%.},
	language = {EN},
	number = {15},
	urldate = {2020-02-03},
	journal = {Applied Optics},
	author = {Katrašnik, Jaka and Pernuš, Franjo and Likar, Boštjan},
	month = may,
	year = {2013},
	keywords = {Hyperspectral imaging, Imaging systems, Image metrics, Integrating spheres, Photon counting, Tunable filters},
	pages = {3526--3537},
	file = {Katrašnik et al. - 2013 - Radiometric calibration and noise estimation of ac.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YHEZRKD9\\Katrašnik et al. - 2013 - Radiometric calibration and noise estimation of ac.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2TV3W4QJ\\abstract.html:text/html},
}

@inproceedings{beck_hyperspektral-imaging_2019,
	address = {Estrel Congress Center Berlin},
	title = {Hyperspektral-{Imaging} ({HSI}) zur intraoperativen {Tumorzellklassifikation}},
	url = {http://www.thieme-connect.de/DOI/DOI?10.1055/s-0039-1685821},
	doi = {10.1055/s-0039-1685821},
	language = {de},
	urldate = {2020-01-31},
	author = {Beck, R and Köhler, H and Goltz, F and Dietz, A and Chalopin, C and Gockel, I},
	month = apr,
	year = {2019},
	pages = {s--0039--1685821},
	file = {Beck et al. - 2019 - Hyperspektral-Imaging (HSI) zur intraoperativen Tu.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\T67GGD72\\Beck et al. - 2019 - Hyperspektral-Imaging (HSI) zur intraoperativen Tu.pdf:application/pdf},
}

@misc{noauthor_karl_2019,
	title = {{KARL} {STORZ} {Announces} a {Product} {Expansion} to include {Hyperspectral} {Imaging}},
	url = {https://idataresearch.com/karl-storz-announces-a-product-expansion-to-include-hyperspectral-imaging/},
	abstract = {Register to receive a free Operating Room Equipment Market Report Suite for US 2018-2024 synopsis and brochure Karl Storz, a global endoscopy leader, has announced the expansion of its portfolio to include hyperspectral imaging. To execute this goal, Karl Storz is partnering with Diaspective Vision GmbH from Pepelow in Germany. Diaspective Vision has been specializing in the...},
	language = {en-US},
	urldate = {2020-01-31},
	journal = {iData Research},
	month = aug,
	year = {2019},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\H54NIQ7H\\karl-storz-announces-a-product-expansion-to-include-hyperspectral-imaging.html:text/html},
}

@article{gockel_moglichkeiten_2019,
	title = {Möglichkeiten und {Perspektiven} der {Hyperspektralbildgebung} in der {Viszeralchirurgie}},
	issn = {1433-0385},
	url = {https://doi.org/10.1007/s00104-019-01016-6},
	doi = {10.1007/s00104-019-01016-6},
	abstract = {Die Hyperspektralbildgebung („HyperSpectral Imaging“ [HSI]) erlaubt quantitative Gewebeanalysen über die Limitationen des menschlichen Auges hinaus. Somit dient es als neues Diagnostikinstrument der optischen Eigenschaften verschiedener Gewebe. Im Gegensatz zu anderen intraoperativen bildgebenden Methoden ist HSI kontaktlos, nichtinvasiv und bedarf keiner Kontrastmittelapplikation. Die Messungen nehmen nur wenige Sekunden in Anspruch und stören somit die Operationsabläufe unwesentlich. Erste HSI-Anwendungen in der Viszeralchirurgie sind vielversprechend mit dem Potenzial optimierter Ergebnisse. Aktuelle Konzepte, Möglichkeiten und neue Perspektiven der HSI-Technologie sowie deren Limitationen werden in dieser Arbeit diskutiert.},
	language = {de},
	urldate = {2020-01-31},
	journal = {Der Chirurg},
	author = {Gockel, I. and Jansen-Winkeln, B. and Holfert, N. and Rayes, N. and Thieme, R. and Maktabi, M. and Sucher, R. and Seehofer, D. and Barberio, M. and Diana, M. and Rabe, S. M. and Mehdorn, M. and Moulla, Y. and Niebisch, S. and Branzan, D. and Rehmet, K. and Takoh, J. P. and Petersen, T.-O. and Neumuth, T. and Melzer, A. and Chalopin, C. and Köhler, H.},
	month = aug,
	year = {2019},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3VM5ECQI\\Gockel et al. - 2019 - Möglichkeiten und Perspektiven der Hyperspektralbi.pdf:application/pdf},
}

@article{kulcke_compact_2018,
	title = {A compact hyperspectral camera for measurement of perfusion parameters in medicine},
	volume = {63},
	issn = {1862-278X, 0013-5585},
	url = {http://www.degruyter.com/view/j/bmte.2018.63.issue-5/bmt-2017-0145/bmt-2017-0145.xml},
	doi = {10.1515/bmt-2017-0145},
	abstract = {Abstract
            
              Worldwide, chronic wounds are still a major and increasing problem area in medicine with protracted suffering of patients and enormous costs. Beside conventional wound treatment, for instance kinds of oxygen therapy and cold plasma technology have been tested, providing an improvement in the perfusion of wounds and their healing potential, but these methods are unfortunately not sufficiently validated and accepted for clinical practice to date. Using hyperspectral imaging technology in the visible (VIS) and near infrared (NIR) region with high spectral and spatial resolution, perfusion parameters of tissue and wounds can be determined. We present a new compact hyperspectral camera which can be used in clinical practice. From hyperspectral data the hemoglobin oxygenation (StO
              2
              ), the relative concentration of hemoglobin [tissue hemoglobin index (THI)] and the so-called NIR-perfusion index can be determined. The first two parameters are calculated from the VIS-part of the spectrum and represent the perfusion of superficial tissue layers, whereas the NIR-perfusion index is calculated from the NIR-part representing the perfusion in deeper layers. First clinical measurements of transplanted flaps and chronic ulcer wounds show, that the perfusion level can be determined quantitatively allowing sensitive evaluation and monitoring for an optimization of the wound treatment planning and for validation of new treatment methods.},
	language = {en},
	number = {5},
	urldate = {2020-01-31},
	journal = {Biomedical Engineering / Biomedizinische Technik},
	author = {Kulcke, Axel and Holmer, Amadeus and Wahl, Philip and Siemers, Frank and Wild, Thomas and Daeschlein, Georg},
	month = oct,
	year = {2018},
	pages = {519--527},
	file = {Kulcke et al. - 2018 - A compact hyperspectral camera for measurement of .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Y6J463MB\\Kulcke et al. - 2018 - A compact hyperspectral camera for measurement of .pdf:application/pdf},
}

@misc{noauthor_hyperspektral-imaging_nodate,
	title = {Hyperspektral-{Imaging} bei gastrointestinalen {Anastomosen}},
	url = {https://www.springermedizin.de/hyperspektral-imaging-bei-gastrointestinalen-anastomosen/15603986},
	abstract = {Anastomoseninsuffizienzen (AIs) sind die schwerwiegendsten Komplikationen in der gastrointestinalen (GI-)Chirurgie mit assoziierter Verlängerung des stationären Aufenthalts und signifikanter Mortalität. Zudem haben Anastomosenkomplikationen …},
	language = {de},
	urldate = {2020-01-31},
	journal = {springermedizin.de},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YZEFJEGT\\15603986.html:text/html},
}

@article{jansen-winkeln_determination_2019,
	title = {Determination of the transection margin during colorectal resection with hyperspectral imaging ({HSI})},
	volume = {34},
	issn = {1432-1262},
	url = {https://doi.org/10.1007/s00384-019-03250-0},
	doi = {10.1007/s00384-019-03250-0},
	abstract = {This study evaluated the use of hyperspectral imaging for the determination of the resection margin during colorectal resections instead of clinical macroscopic assessment.},
	language = {en},
	number = {4},
	urldate = {2020-01-31},
	journal = {International Journal of Colorectal Disease},
	author = {Jansen-Winkeln, Boris and Holfert, N. and Köhler, H. and Moulla, Y. and Takoh, J. P. and Rabe, S. M. and Mehdorn, M. and Barberio, M. and Chalopin, C. and Neumuth, T. and Gockel, I.},
	month = apr,
	year = {2019},
	pages = {731--739},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\D6T7ULWR\\Jansen-Winkeln et al. - 2019 - Determination of the transection margin during col.pdf:application/pdf},
}

@article{kohler_evaluation_2019,
	title = {Evaluation of hyperspectral imaging ({HSI}) for the measurement of ischemic conditioning effects of the gastric conduit during esophagectomy},
	volume = {33},
	issn = {1432-2218},
	url = {https://doi.org/10.1007/s00464-019-06675-4},
	doi = {10.1007/s00464-019-06675-4},
	abstract = {Hyperspectral imaging (HSI) is a relatively new method used in image-guided and precision surgery, which has shown promising results for characterization of tissues and assessment of physiologic tissue parameters. Previous methods used for analysis of preconditioning concepts in patients and animal models have shown several limitations of application. The aim of this study was to evaluate HSI for the measurement of ischemic conditioning effects during esophagectomy.},
	language = {en},
	number = {11},
	urldate = {2020-01-31},
	journal = {Surgical Endoscopy},
	author = {Köhler, Hannes and Jansen-Winkeln, Boris and Maktabi, Marianne and Barberio, Manuel and Takoh, Jonathan and Holfert, Nico and Moulla, Yusef and Niebisch, Stefan and Diana, Michele and Neumuth, Thomas and Rabe, Sebastian M. and Chalopin, Claire and Melzer, Andreas and Gockel, Ines},
	month = nov,
	year = {2019},
	pages = {3775--3782},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TU8TQF7F\\Köhler et al. - 2019 - Evaluation of hyperspectral imaging (HSI) for the .pdf:application/pdf},
}

@article{barberio_hyperspectral_2019,
	title = {{HYPerspectral} {Enhanced} {Reality} ({HYPER}): a physiology-based surgical guidance tool},
	issn = {1432-2218},
	shorttitle = {{HYPerspectral} {Enhanced} {Reality} ({HYPER})},
	url = {https://doi.org/10.1007/s00464-019-06959-9},
	doi = {10.1007/s00464-019-06959-9},
	abstract = {HSI is an optical technology allowing for a real-time, contrast-free snapshot of physiological tissue properties, including oxygenation. Hyperspectral imaging (HSI) has the potential to quantify the gastrointestinal perfusion intraoperatively. This experimental study evaluates the accuracy of HSI, in order to quantify bowel perfusion, and to obtain a superposition of the hyperspectral information onto real-time images.},
	language = {en},
	urldate = {2020-01-31},
	journal = {Surgical Endoscopy},
	author = {Barberio, Manuel and Longo, Fabio and Fiorillo, Claudio and Seeliger, Barbara and Mascagni, Pietro and Agnus, Vincent and Lindner, Veronique and Geny, Bernard and Charles, Anne-Laure and Gockel, Ines and Worreth, Marc and Saadi, Alend and Marescaux, Jacques and Diana, Michele},
	month = jul,
	year = {2019},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5D4YH5SG\\Barberio et al. - 2019 - HYPerspectral Enhanced Reality (HYPER) a physiolo.pdf:application/pdf},
}

@article{barberio_hyperspectral_2018,
	title = {Hyperspectral based discrimination of thyroid and parathyroid during surgery},
	volume = {4},
	issn = {2364-5504},
	url = {http://www.degruyter.com/view/j/cdbme.2018.4.issue-1/cdbme-2018-0095/cdbme-2018-0095.xml},
	doi = {10.1515/cdbme-2018-0095},
	abstract = {Unintended injuring of anatomical structures during endocrine neck operations can have severe consequences for patient. Especially the nerves and the parathyroid gland can be hard to identify visually. Therefore, intraoperative methods are needed to support the surgeon in this task. Hyperspectral imaging (HSI) is a new approach in the medical area which combines a camera with a spectrometer. It showed promising results for the discrimination of tissue. In this work, HSI-data of seven patients were acquired during thyroid and parathyroid operations. The mean absorbance spectra of both glands showed differences in the range between 600 and 700 nm and at 760 and 960 nm. This means that thyroid and parathyroid have different oxygenation states and different contents of deoxygenated hemoglobin and water. From these observations, it is possible to define spectral signatures to characterize both glands. We showed on one patient how spectral signatures can be used in classification algorithms to automatically identify the thyroid and parathyroid from other structures.},
	language = {en},
	number = {1},
	urldate = {2020-01-31},
	journal = {Current Directions in Biomedical Engineering},
	author = {Barberio, Manuel and Maktabi, Marianne and Gockel, Ines and Rayes, Nada and Jansen-Winkeln, Boris and Köhler, Hannes and Rabe, Sebastian M. and Seidemann, Lena and Takoh, Jonathan P. and Diana, Michele and Neumuth, Thomas and Chalopin, Claire},
	month = sep,
	year = {2018},
	pages = {399--402},
	file = {Barberio et al. - 2018 - Hyperspectral based discrimination of thyroid and .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JZTAZ59E\\Barberio et al. - 2018 - Hyperspectral based discrimination of thyroid and .pdf:application/pdf},
}

@article{katrasnik_radiometric_2013-1,
	title = {Radiometric calibration and noise estimation of acousto-optic tunable filter hyperspectral imaging systems},
	volume = {52},
	copyright = {\&\#169; 2013 Optical Society of America},
	issn = {2155-3165},
	url = {https://www.osapublishing.org/ao/abstract.cfm?uri=ao-52-15-3526},
	doi = {10.1364/AO.52.003526},
	abstract = {The accuracy of the radiometric response of acousto-optic tunable filter (AOTF) hyperspectral imaging systems is crucial for obtaining reliable measurements. It is therefore important to know the radiometric response and noise characteristics of the hyperspectral imaging system used. A radiometric model of an AOTF hyperspectral imaging system composed of an imaging sensor radiometric model (CCD, CMOS, and sCMOS) and an AOTF light transmission model is proposed. Using the radiometric model, a method for obtaining the fixed pattern noise (FPN) of the imaging system by displacing and imaging an illuminated reference target is developed. Methods for estimating the temporal noise of the imaging system, using the photon transfer method, and for correcting FPN are also presented. Noise estimation and image restoration methods were tested on an AOTF hyperspectral imaging system. The results indicate that the developed methods can accurately calculate temporal and FPN, and can effectively correct the acquired images. After correction, the signal-to-noise ratio of the acquired images was shown to increase by 26\%.},
	language = {EN},
	number = {15},
	urldate = {2020-01-27},
	journal = {Applied Optics},
	author = {Katrašnik, Jaka and Pernuš, Franjo and Likar, Boštjan},
	month = may,
	year = {2013},
	pages = {3526--3537},
	file = {Katrašnik et al. - 2013 - Radiometric calibration and noise estimation of ac.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\T9Q5VGP2\\Katrašnik et al. - 2013 - Radiometric calibration and noise estimation of ac.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DIHYQ2VB\\ViewMedia.html:text/html},
}

@article{healey_radiometric_1994-1,
	title = {Radiometric {CCD} camera calibration and noise estimation},
	volume = {16},
	issn = {1939-3539},
	doi = {10.1109/34.276126},
	abstract = {Changes in measured image irradiance have many physical causes and are the primary cue for several visual processes, such as edge detection and shape from shading. Using physical models for charged-coupled device (CCD) video cameras and material reflectance, we quantify the variation in digitized pixel values that is due to sensor noise and scene variation. This analysis forms the basis of algorithms for camera characterization and calibration and for scene description. Specifically, algorithms are developed for estimating the parameters of camera noise and for calibrating a camera to remove the effects of fixed pattern nonuniformity and spatial variation in dark current. While these techniques have many potential uses, we describe in particular how they can be used to estimate a measure of scene variation. This measure is independent of image irradiance and can be used to identify a surface from a single sensor band over a range of situations. Experimental results confirm that the models presented in this paper are useful for modeling the different sources of variation in real images obtained from video cameras.{\textless}{\textgreater}},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Healey, G.E. and Kondepudy, R.},
	month = mar,
	year = {1994},
	keywords = {Cameras, calibration, Calibration, camera characterization, CCD image sensors, Charge coupled devices, Charge-coupled image sensors, computer vision, dark current, digitized pixel values, edge detection, fixed pattern nonuniformity, Image edge detection, Layout, material reflectance, noise estimation, Noise shaping, parameter estimation, primary cue, radiometric CCD camera calibration, radiometry, Radiometry, reflectivity, Reflectivity, scene description, scene variation, semiconductor device models, semiconductor device noise, sensor noise, shape from shading, Shape measurement, spatial variation, video cameras, visual processes},
	pages = {267--276},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7V3CPRVW\\276126.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DRDEE6IQ\\Healey und Kondepudy - 1994 - Radiometric CCD camera calibration and noise estim.pdf:application/pdf},
}

@article{jones_bayesian_2017,
	title = {Bayesian {Estimation} of {Intrinsic} {Tissue} {Oxygenation} and {Perfusion} {From} {RGB} {Images}},
	volume = {36},
	issn = {1558-254X},
	doi = {10.1109/TMI.2017.2665627},
	abstract = {Multispectral imaging (MSI) can potentially assist the intra-operative assessment of tissue structure, function and viability, by providing information about oxygenation. In this paper, we present a novel technique for recovering intrinsic MSI measurements from endoscopic RGB images without custom hardware adaptations. The advantage of this approach is that it requires no modification to existing surgical and diagnostic endoscopic imaging systems. Our method uses a radiometric color calibration of the endoscopic camera's sensor in conjunction with a Bayesian framework to recover a per-pixel measurement of the total blood volume (THb) and oxygen saturation (SO2) in the observed tissue. The sensor's pixel measurements are modeled as weighted sums over a mixture of Poisson distributions and we optimize the variables SO2 and THb to maximize the likelihood of the observations. To validate our technique, we use synthetic images generated from Monte Carlo physics simulation of light transport through soft tissue containing sub-surface blood vessels. We also validate our method on in vivo data by comparing it to a MSI dataset acquired with a hardware system that sequentially images multiple spectral bands without overlap. Our results are promising and show that we are able to provide surgeons with additional relevant information by processing endoscopic images with our modeling and inference framework.},
	number = {7},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Jones, Geoffrey and Clancy, Neil T. and Helo, Yusuf and Arridge, Simon and Elson, Daniel S. and Stoyanov, Danail},
	month = jul,
	year = {2017},
	keywords = {Cameras, image colour analysis, Lighting, Multispectral imaging, calibration, Attenuation, Bayes methods, Bayes Theorem, Bayesian estimation, Bayesian inference, biomedical optical imaging, biophotonics, blood vessels, Color, data acquisition, diagnostic endoscopic imaging, endoscopes, endoscopic camera sensor, endoscopic image processing, endoscopic RGB images, Estimation, hardware system, image sensors, image sequences, inference framework, intraoperative assessment, intrinsic MSI measurements, intrinsic tissue oxygenation, intrinsic tissue perfusion, Light, light transport, likelihood maximisation, medical image processing, Minimally invasive surgery, Monte Carlo Method, Monte Carlo methods, Monte Carlo physics simulation, MSI dataset acquisition, multispectral imaging, optimisation, oximetry, Oxygen, oxygen saturation, per-pixel measurement, Photography, Poisson distribution, Poisson distributions, radiometric color calibration, Scattering, sensor pixel measurements, sequentially images multiple spectral bands, soft tissue, subsurface blood vessels, surgery, Surgery, surgical endoscopic imaging, surgical vision, synthetic image generation, THb, tissue function, tissue structure, tissue viability, total blood volume, variable SO2 optimization},
	pages = {1491--1501},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\67SBI6WB\\7859372.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Q9PEGWDF\\Jones et al. - 2017 - Bayesian Estimation of Intrinsic Tissue Oxygenatio.pdf:application/pdf},
}

@article{kester_real-time_2011,
	title = {Real-time snapshot hyperspectral imaging endoscope},
	volume = {16},
	issn = {1083-3668, 1560-2281},
	url = {https://www.spiedigitallibrary.org/journals/Journal-of-Biomedical-Optics/volume-16/issue-5/056005/Real-time-snapshot-hyperspectral-imaging-endoscope/10.1117/1.3574756.short},
	doi = {10.1117/1.3574756},
	abstract = {Hyperspectral imaging has tremendous potential to detect important molecular biomarkers of early cancer based on their unique spectral signatures. Several drawbacks have limited its use for in vivo screening applications: most notably the poor temporal and spatial resolution, high expense, and low optical throughput of existing hyperspectral imagers. We present the development of a new real-time hyperspectral endoscope (called the image mapping spectroscopy endoscope) based on an image mapping technique capable of addressing these challenges. The parallel high throughput nature of this technique enables the device to operate at frame rates of 5.2 frames per second while collecting a (x, y, λ) datacube of 350 × 350 × 48. We have successfully imaged tissue in vivo, resolving a vasculature pattern of the lower lip while simultaneously detecting oxy-hemoglobin.},
	number = {5},
	urldate = {2020-01-22},
	journal = {Journal of Biomedical Optics},
	author = {Kester, Robert T. and Bedard, Noah and Gao, Liang S. and Tkaczyk, Tomasz S.},
	month = may,
	year = {2011},
	pages = {056005},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IQTFYHTA\\Kester et al. - 2011 - Real-time snapshot hyperspectral imaging endoscope.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\E8X5LHEE\\1.3574756.html:text/html},
}

@article{thies_deferred_2019,
	title = {Deferred neural rendering: image synthesis using neural textures},
	volume = {38},
	issn = {07300301},
	shorttitle = {Deferred neural rendering},
	url = {http://dl.acm.org/citation.cfm?doid=3306346.3323035},
	doi = {10.1145/3306346.3323035},
	abstract = {The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, highquality 3D content as input. In this work, we explore the use of imperfect 3D content, for instance, obtained from photo-metric reconstructions with noisy and incomplete surface geometry, while still aiming to produce photo-realistic (re-)renderings. To address this challenging problem, we introduce Deferred Neural Rendering, a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable components. Specifically, we propose Neural Textures, which are learned feature maps that are trained as part of the scene capture process. Similar to traditional textures, neural textures are stored as maps on top of 3D mesh proxies; however, the high-dimensional feature maps contain significantly more information, which can be interpreted by our new deferred neural rendering pipeline. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect. In contrast to traditional, black-box 2D generative neural networks, our 3D representation gives us explicit control over the generated output, and allows for a wide range of application domains. For instance, we can synthesize temporally-consistent video re-renderings of recorded 3D scenes as our representation is inherently embedded in 3D space. This way, neural textures can be utilized to coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates. We show the effectiveness of our approach in several experiments on novel view synthesis, scene editing, and facial reenactment, and compare to state-of-the-art approaches that leverage the standard graphics pipeline as well as conventional generative neural networks.},
	language = {en},
	number = {4},
	urldate = {2020-01-21},
	journal = {ACM Transactions on Graphics},
	author = {Thies, Justus and Zollhöfer, Michael and Nießner, Matthias},
	month = jul,
	year = {2019},
	pages = {1--12},
	file = {Thies et al. - 2019 - Deferred neural rendering image synthesis using n.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XZTH6YPV\\Thies et al. - 2019 - Deferred neural rendering image synthesis using n.pdf:application/pdf},
}

@article{fabelo_-vivo_2019,
	title = {\textit{{In}-{Vivo}} {Hyperspectral} {Human} {Brain} {Image} {Database} for {Brain} {Cancer} {Detection}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8667294/},
	doi = {10.1109/ACCESS.2019.2904788},
	abstract = {The use of hyperspectral imaging for medical applications is becoming more common in recent years. One of the main obstacles that researchers ﬁnd when developing hyperspectral algorithms for medical applications is the lack of speciﬁc, publicly available, and hyperspectral medical data. The work described in this paper was developed within the framework of the European project HELICoiD (HypErspectraL Imaging Cancer Detection), which had as a main goal the application of hyperspectral imaging to the delineation of brain tumors in real-time during neurosurgical operations. In this paper, the methodology followed to generate the ﬁrst hyperspectral database of in-vivo human brain tissues is presented. Data was acquired employing a customized hyperspectral acquisition system capable of capturing information in the Visual and Near InfraRed (VNIR) range from 400 to 1000 nm. Repeatability was assessed for the cases where two images of the same scene were captured consecutively. The analysis reveals that the system works more efﬁciently in the spectral range between 450 and 900 nm. A total of 36 hyperspectral images from 22 different patients were obtained. From these data, more than 300 000 spectral signatures were labeled employing a semi-automatic methodology based on the spectral angle mapper algorithm. Four different classes were deﬁned: normal tissue, tumor tissue, blood vessel, and background elements. All the hyperspectral data has been made available in a public repository.},
	language = {en},
	urldate = {2020-01-21},
	journal = {IEEE Access},
	author = {Fabelo, Himar and Ortega, Samuel and Szolna, Adam and Bulters, Diederik and Pineiro, Juan F. and Kabwama, Silvester and J-O'Shanahan, Aruma and Bulstrode, Harry and Bisshopp, Sara and Kiran, B. Ravi and Ravi, Daniele and Lazcano, Raquel and Madronal, Daniel and Sosa, Coralia and Espino, Carlos and Marquez, Mariano and De La Luz Plaza, Maria and Camacho, Rafael and Carrera, David and Hernandez, Maria and Callico, Gustavo M. and Morera Molina, Jesus and Stanciulescu, Bogdan and Yang, Guang-Zhong and Salvador Perea, Ruben and Juarez, Eduardo and Sanz, Cesar and Sarmiento, Roberto},
	year = {2019},
	pages = {39098--39116},
	file = {Fabelo et al. - 2019 - iIn-Vivoi Hyperspectral Human Brain Image Dat.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VDEMVJ6R\\Fabelo et al. - 2019 - iIn-Vivoi Hyperspectral Human Brain Image Dat.pdf:application/pdf;Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\75YE4UR5\\Fabelo et al. - 2019 - In-Vivo Hyperspectral Human Brain Image Database f.pdf:application/pdf},
}

@article{hong_invariant_2020,
	title = {Invariant {Attribute} {Profiles}: {A} {Spatial}-{Frequency} {Joint} {Feature} {Extractor} for {Hyperspectral} {Image} {Classification}},
	issn = {0196-2892, 1558-0644},
	shorttitle = {Invariant {Attribute} {Profiles}},
	url = {http://arxiv.org/abs/1912.08847},
	doi = {10.1109/TGRS.2019.2957251},
	abstract = {Up to the present, an enormous number of advanced techniques have been developed to enhance and extract the spatially semantic information in hyperspectral image processing and analysis. However, locally semantic change, such as scene composition, relative position between objects, spectral variability caused by illumination, atmospheric effects, and material mixture, has been less frequently investigated in modeling spatial information. As a consequence, identifying the same materials from spatially different scenes or positions can be difficult. In this paper, we propose a solution to address this issue by locally extracting invariant features from hyperspectral imagery (HSI) in both spatial and frequency domains, using a method called invariant attribute profiles (IAPs). IAPs extract the spatial invariant features by exploiting isotropic filter banks or convolutional kernels on HSI and spatial aggregation techniques (e.g., superpixel segmentation) in the Cartesian coordinate system. Furthermore, they model invariant behaviors (e.g., shift, rotation) by the means of a continuous histogram of oriented gradients constructed in a Fourier polar coordinate. This yields a combinatorial representation of spatial-frequency invariant features with application to HSI classification. Extensive experiments conducted on three promising hyperspectral datasets (Houston2013 and Houston2018) demonstrate the superiority and effectiveness of the proposed IAP method in comparison with several state-of-the-art profile-related techniques. The codes will be available from the website: https://sites.google.com/view/danfeng-hong/data-code.},
	language = {en},
	urldate = {2020-01-21},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Hong, Danfeng and Wu, Xin and Ghamisi, Pedram and Chanussot, Jocelyn and Yokoya, Naoto and Zhu, Xiao Xiang},
	year = {2020},
	note = {arXiv: 1912.08847},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--18},
	file = {Hong et al. - 2020 - Invariant Attribute Profiles A Spatial-Frequency .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EWSW8VLS\\Hong et al. - 2020 - Invariant Attribute Profiles A Spatial-Frequency .pdf:application/pdf},
}

@inproceedings{xie_multispectral_2019,
	title = {Multispectral and {Hyperspectral} {Image} {Fusion} by {MS}/{HS} {Fusion} {Net}},
	url = {http://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Multispectral_and_Hyperspectral_Image_Fusion_by_MSHS_Fusion_Net_CVPR_2019_paper.html},
	urldate = {2020-01-21},
	author = {Xie, Qi and Zhou, Minghao and Zhao, Qian and Meng, Deyu and Zuo, Wangmeng and Xu, Zongben},
	year = {2019},
	pages = {1585--1594},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\G7XDMC7G\\Xie et al. - 2019 - Multispectral and Hyperspectral Image Fusion by MS.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PZLBK9PR\\Xie_Multispectral_and_Hyperspectral_Image_Fusion_by_MSHS_Fusion_Net_CVPR_2019_paper.html:text/html},
}

@incollection{shen_generating_2019,
	address = {Cham},
	title = {Generating {Large} {Labeled} {Data} {Sets} for {Laparoscopic} {Image} {Processing} {Tasks} {Using} {Unpaired} {Image}-to-{Image} {Translation}},
	volume = {11768},
	isbn = {978-3-030-32253-3 978-3-030-32254-0},
	url = {http://link.springer.com/10.1007/978-3-030-32254-0_14},
	abstract = {In the medical domain, the lack of large training data sets and benchmarks is often a limiting factor for training deep neural networks. In contrast to expensive manual labeling, computer simulations can generate large and fully labeled data sets with a minimum of manual eﬀort. However, models that are trained on simulated data usually do not translate well to real scenarios. To bridge the domain gap between simulated and real laparoscopic images, we exploit recent advances in unpaired image-to-image translation. We extent an image-to-image translation method to generate a diverse multitude of realistically looking synthetic images based on images from a simple laparoscopy simulation. By incorporating means to ensure that the image content is preserved during the translation process, we ensure that the labels given for the simulated images remain valid for their realistically looking translations. This lets us generate a large, fully labeled synthetic data set. We show that this data set can be used to train models for the task of liver segmentation in laparoscopic images. We achieve median dice scores of up to 0.89 in some patients without manually labeling a single laparoscopic image and show that using our synthetic data to pre-train models can greatly improve their performance. The synthetic data set is made publicly available, fully labeled with segmentation maps, depth maps, normal maps, and positions of tools and camera (http://opencas.dkfz.de/image2image).},
	language = {en},
	urldate = {2020-01-21},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Pfeiffer, Micha and Funke, Isabel and Robu, Maria R. and Bodenstedt, Sebastian and Strenger, Leon and Engelhardt, Sandy and Roß, Tobias and Clarkson, Matthew J. and Gurusamy, Kurinchi and Davidson, Brian R. and Maier-Hein, Lena and Riediger, Carina and Welsch, Thilo and Weitz, Jürgen and Speidel, Stefanie},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	doi = {10.1007/978-3-030-32254-0_14},
	pages = {119--127},
	file = {Pfeiffer et al. - 2019 - Generating Large Labeled Data Sets for Laparoscopi.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\897BEMJL\\Pfeiffer et al. - 2019 - Generating Large Labeled Data Sets for Laparoscopi.pdf:application/pdf},
}

@misc{noauthor_biophysics-inspired_2019,
	title = {Biophysics-{Inspired} {AI} {Uses} {Photons} to {Help} {Surgeons} {Identify} {Cancer}},
	copyright = {© Copyright IBM Corp. 2020},
	url = {https://www.ibm.com/blogs/research/2019/02/biophysics-inspired-ai/},
	abstract = {Biophysics-inspired AI tools would provide richer information to support intraoperative decisions of surgeons during removal of cancerous tissue.},
	language = {en-US},
	urldate = {2020-01-21},
	journal = {IBM Research Blog},
	month = feb,
	year = {2019},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KWIQM9WA\\biophysics-inspired-ai.html:text/html},
}

@article{parr_matrix_2018,
	title = {The {Matrix} {Calculus} {You} {Need} {For} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1802.01528},
	abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here. See related articles at http://explained.ai},
	urldate = {2020-01-21},
	journal = {arXiv:1802.01528 [cs, stat]},
	author = {Parr, Terence and Howard, Jeremy},
	month = jul,
	year = {2018},
	note = {arXiv: 1802.01528},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\H2YFAGG9\\Parr und Howard - 2018 - The Matrix Calculus You Need For Deep Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6UGL6JZQ\\1802.html:text/html},
}

@incollection{shen_generating_2019-1,
	address = {Cham},
	title = {Generating {Large} {Labeled} {Data} {Sets} for {Laparoscopic} {Image} {Processing} {Tasks} {Using} {Unpaired} {Image}-to-{Image} {Translation}},
	volume = {11768},
	isbn = {978-3-030-32253-3 978-3-030-32254-0},
	url = {http://link.springer.com/10.1007/978-3-030-32254-0_14},
	abstract = {In the medical domain, the lack of large training data sets and benchmarks is often a limiting factor for training deep neural networks. In contrast to expensive manual labeling, computer simulations can generate large and fully labeled data sets with a minimum of manual eﬀort. However, models that are trained on simulated data usually do not translate well to real scenarios. To bridge the domain gap between simulated and real laparoscopic images, we exploit recent advances in unpaired image-to-image translation. We extent an image-to-image translation method to generate a diverse multitude of realistically looking synthetic images based on images from a simple laparoscopy simulation. By incorporating means to ensure that the image content is preserved during the translation process, we ensure that the labels given for the simulated images remain valid for their realistically looking translations. This lets us generate a large, fully labeled synthetic data set. We show that this data set can be used to train models for the task of liver segmentation in laparoscopic images. We achieve median dice scores of up to 0.89 in some patients without manually labeling a single laparoscopic image and show that using our synthetic data to pre-train models can greatly improve their performance. The synthetic data set is made publicly available, fully labeled with segmentation maps, depth maps, normal maps, and positions of tools and camera (http://opencas.dkfz.de/image2image).},
	language = {en},
	urldate = {2020-01-21},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Pfeiffer, Micha and Funke, Isabel and Robu, Maria R. and Bodenstedt, Sebastian and Strenger, Leon and Engelhardt, Sandy and Roß, Tobias and Clarkson, Matthew J. and Gurusamy, Kurinchi and Davidson, Brian R. and Maier-Hein, Lena and Riediger, Carina and Welsch, Thilo and Weitz, Jürgen and Speidel, Stefanie},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	doi = {10.1007/978-3-030-32254-0_14},
	pages = {119--127},
	file = {Pfeiffer et al. - 2019 - Generating Large Labeled Data Sets for Laparoscopi.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KCSQQ5VK\\Pfeiffer et al. - 2019 - Generating Large Labeled Data Sets for Laparoscopi.pdf:application/pdf},
}

@article{choi_waic_2019,
	title = {{WAIC}, but {Why}? {Generative} {Ensembles} for {Robust} {Anomaly} {Detection}},
	shorttitle = {{WAIC}, but {Why}?},
	url = {http://arxiv.org/abs/1810.01392},
	abstract = {Machine learning models encounter Out-ofDistribution (OoD) errors when the data seen at test time are generated from a different stochastic generator than the one used to generate the training data. One proposal to scale OoD detection to high-dimensional data is to learn a tractable likelihood approximation of the training distribution, and use it to reject unlikely inputs. However, likelihood models on natural data are themselves susceptible to OoD errors, and even assign large likelihoods to samples from other datasets. To mitigate this problem, we propose Generative Ensembles, which robustify density-based OoD detection by way of estimating epistemic uncertainty of the likelihood model. We present a puzzling observation in need of an explanation – although likelihood measures cannot account for the typical set of a distribution, and therefore should not be suitable on their own for OoD detection, WAIC performs surprisingly well in practice.},
	language = {en},
	urldate = {2020-01-21},
	journal = {arXiv:1810.01392 [cs, stat]},
	author = {Choi, Hyunsun and Jang, Eric and Alemi, Alexander A.},
	month = may,
	year = {2019},
	note = {arXiv: 1810.01392},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Choi et al. - 2019 - WAIC, but Why Generative Ensembles for Robust Ano.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TAB7XFCJ\\Choi et al. - 2019 - WAIC, but Why Generative Ensembles for Robust Ano.pdf:application/pdf},
}

@article{codella_collaborative_2018,
	title = {Collaborative {Human}-{AI} ({CHAI}): {Evidence}-{Based} {Interpretable} {Melanoma} {Classification} in {Dermoscopic} {Images}},
	shorttitle = {Collaborative {Human}-{AI} ({CHAI})},
	url = {http://arxiv.org/abs/1805.12234},
	abstract = {Automated dermoscopic image analysis has witnessed rapid growth in diagnostic performance. Yet adoption faces resistance, in part, because no evidence is provided to support decisions. In this work, an approach for evidence-based classiﬁcation is presented. A feature embedding is learned with CNNs, triplet-loss, and global average pooling, and used to classify via kNN search. Evidence is provided as both the discovered neighbors, as well as localized image regions most relevant to measuring distance between query and neighbors. To ensure that results are relevant in terms of both label accuracy and human visual similarity for any skill level, a novel hierarchical triplet logic is implemented to jointly learn an embedding according to disease labels and non-expert similarity. Results are improved over baselines trained on disease labels alone, as well as standard multiclass loss. Quantitative relevance of results, according to non-expert similarity, as well as localized image regions, are also signiﬁcantly improved.},
	language = {en},
	urldate = {2020-01-21},
	journal = {arXiv:1805.12234 [cs]},
	author = {Codella, Noel C. F. and Lin, Chung-Ching and Halpern, Allan and Hind, Michael and Feris, Rogerio and Smith, John R.},
	month = aug,
	year = {2018},
	note = {arXiv: 1805.12234},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Codella et al. - 2018 - Collaborative Human-AI (CHAI) Evidence-Based Inte.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\692WTLUG\\Codella et al. - 2018 - Collaborative Human-AI (CHAI) Evidence-Based Inte.pdf:application/pdf},
}

@article{kuwajima_improving_2019,
	title = {Improving {Transparency} of {Deep} {Neural} {Inference} {Process}},
	url = {http://arxiv.org/abs/1903.05501},
	abstract = {Deep learning techniques are rapidly advanced recently, and becoming a necessity component for widespread systems. However, the inference process of deep learning is black-box, and not very suitable to safety-critical systems which must exhibit high transparency. In this paper, to address this black-box limitation, we develop a simple analysis method which consists of 1) structural feature analysis: lists of the features contributing to inference process, 2) linguistic feature analysis: lists of the natural language labels describing the visual attributes for each feature contributing to inference process, and 3) consistency analysis: measuring consistency among input data, inference (label), and the result of our structural and linguistic feature analysis. Our analysis is simplified to reflect the actual inference process for high transparency, whereas it does not include any additional black-box mechanisms such as LSTM for highly human readable results. We conduct experiments and discuss the results of our analysis qualitatively and quantitatively, and come to believe that our work improves the transparency of neural networks. Evaluated through 12,800 human tasks, 75\% workers answer that input data and result of our feature analysis are consistent, and 70\% workers answer that inference (label) and result of our feature analysis are consistent. In addition to the evaluation of the proposed analysis, we find that our analysis also provide suggestions, or possible next actions such as expanding neural network complexity or collecting training data to improve a neural network.},
	language = {en},
	urldate = {2020-01-21},
	journal = {arXiv:1903.05501 [cs, stat]},
	author = {Kuwajima, Hiroshi and Tanaka, Masayuki and Okutomi, Masatoshi},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.05501},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Kuwajima et al. - 2019 - Improving Transparency of Deep Neural Inference Pr.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\R3PS72VG\\Kuwajima et al. - 2019 - Improving Transparency of Deep Neural Inference Pr.pdf:application/pdf},
}

@inproceedings{alafandy_artificial_2019,
	address = {Rabat, Morocco},
	series = {{BDIoT}'19},
	title = {Artificial {Neural} {Networks} {Optimization} and {Convolution} {Neural} {Networks} to {Classifying} {Images} in {Remote} {Sensing}: {A} {Review}},
	isbn = {978-1-4503-7240-4},
	shorttitle = {Artificial {Neural} {Networks} {Optimization} and {Convolution} {Neural} {Networks} to {Classifying} {Images} in {Remote} {Sensing}},
	url = {https://doi.org/10.1145/3372938.3372945},
	doi = {10.1145/3372938.3372945},
	abstract = {One of important functions of remote sensing data is producing the land-use/land-cover maps. Image classification is one of important applications for remote sensing imaginary. Machine learning (ML) techniques are the most widely used for this purpose in recent years. With the advent of computer vision thus, the need to deal with a large amount of data and avoiding any data redundancy, the deep learning techniques were appeared. Deep learning (DL) is a branch of machine learning that imitates the human brain structure and depends on the artificial neural networks (ANNs). Optimization of the neural networks is necessary for reduce the loss functions and avoiding any redundancy data in the training set, thus raise the accuracy. Genetic algorithms (GA) are the most widely used in the neural networks optimization, which considered as fully connected neural networks. Convolution neural networks (CNNs) are a branch of the artificial neural networks that are saving the computing cost and processing time. Thus, this paper presents a review of the deep learning algorithms specially the artificial neural networks, the genetic algorithm and the convolution neural networks. This paper also introduces a comparative study between the genetic algorithm and the convolution neural networks method. This comparison based on the overall accuracy (OA) and the kappa coefficient. This comparison shows that there are many conditions can affect the classifier accuracy. The results demonstrate that the CNNs algorithms are more accurate than the GA and in the other hand, the CNNs algorithms have lower computing cost.},
	urldate = {2020-01-13},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Big} {Data} and {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {AlAfandy, Khalid A. and Omara, Hicham and Lazaar, Mohamed and Al Achhab, Mohammed},
	month = oct,
	year = {2019},
	keywords = {ANNs, CNNs, Deep Learning, GA, Image Classification, Machine Learning, optimization, Remote Sensing},
	pages = {1--8},
}

@article{bischof_multispectral_1992,
	title = {Multispectral classification of {Landsat}-images using neural networks},
	volume = {30},
	issn = {1558-0644},
	doi = {10.1109/36.142926},
	abstract = {The authors report the application of three-layer back-propagation networks for classification of Landsat TM data on a pixel-by-pixel basis. The results are compared to Gaussian maximum likelihood classification. First, it is shown that the neural network is able to perform better than the maximum likelihood classifier. Secondly, in an extension of the basic network architecture it is shown that textural information can be integrated into the neural network classifier without the explicit definition of a texture measure. Finally, the use of neural networks for postclassification smoothing is examined.{\textless}{\textgreater}},
	number = {3},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Bischof, H. and Schneider, W. and Pinz, A.J.},
	month = may,
	year = {1992},
	keywords = {Multispectral imaging, Artificial neural networks, Bayesian methods, classifier, computerised pattern recognition, geophysical techniques, geophysics computing, land surface, Landsat, Maximum likelihood estimation, MSS method, Multi-layer neural network, multispectral method, neural nets, neural network, Neural networks, optical image classification, postclassification smoothing, remote sensing, Remote sensing, Satellites, Smoothing methods, textural information, three-layer back-propagation networks, Very large scale integration},
	pages = {482--490},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5W86B7TF\\142926.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\NMFUX787\\Bischof et al. - 1992 - Multispectral classification of Landsat-images usi.pdf:application/pdf},
}

@inproceedings{zhang_tissue_2016,
	title = {Tissue classification for laparoscopic image understanding based on multispectral texture analysis},
	volume = {9786},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9786/978619/Tissue-classification-for-laparoscopic-image-understanding-based-on-multispectral-texture/10.1117/12.2216090.short},
	doi = {10.1117/12.2216090},
	abstract = {Intra-operative tissue classification is one of the prerequisites for providing context-aware visualization in computer-assisted minimally invasive surgeries. As many anatomical structures are difficult to differentiate in conventional RGB medical images, we propose a classification method based on multispectral image patches. In a comprehensive \textit{ex vivo} study we show (1) that multispectral imaging data is superior to RGB data for organ tissue classification when used in conjunction with widely applied feature descriptors and (2) that combining the tissue texture with the reflectance spectrum improves the classification performance. Multispectral tissue analysis could thus evolve as a key enabling technique in computer-assisted laparoscopy.},
	urldate = {2020-01-13},
	booktitle = {Medical {Imaging} 2016: {Image}-{Guided} {Procedures}, {Robotic} {Interventions}, and {Modeling}},
	publisher = {International Society for Optics and Photonics},
	author = {Zhang, Yan and Wirkert, Sebastian J. and Iszatt, Justin and Kenngott, Hannes and Wagner, Martin and Mayer, Benjamin and Stock, Christian and Clancy, Neil T. and Elson, Daniel S. and Maier-Hein, Lena},
	month = mar,
	year = {2016},
	pages = {978619},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7NDI47GJ\\Zhang et al. - 2016 - Tissue classification for laparoscopic image under.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SAXTEI4N\\12.2216090.html:text/html},
}

@inproceedings{yu_hybrid_2015,
	title = {A hybrid convolutional neural networks with extreme learning machine for {WCE} image classification},
	doi = {10.1109/ROBIO.2015.7419037},
	abstract = {Wireless Capsule Endoscopy (WCE) is considered as a promising technology for non-invasive gastrointestinal disease examination. This paper studies the classification problem of the digestive organs for wireless capsule endoscopy (WCE) images aiming at saving the review time of doctors. Our previous study has proved the Convolutional Neural Networks (CNN)-based WCE classification system is able to achieve 95\% classification accuracy in average, but it is difficult to further improve the classification accuracy owing to the variations of individuals and the complex digestive tract circumstance. Research shows that there are two possible approaches to improve classification accuracy: to extract more discriminative image features and to employ a more powerful classifier. In this paper, we propose to design a WCE classification system by a hybrid CNN with Extreme Learning Machine (ELM). In our approach, we construct the CNN as a data-driven feature extractor and the cascaded ELM as a strong classifier instead of the conventional used full-connection classifier in deep CNN classification system. Moreover, to improve the convergence and classification capability of ELM under supervision manner, a new initialization is employed. Our developed WCE image classification system is named as HCNN-NELM. With about 1 million real WCE images (25 examinations), intensive experiments are conducted to evaluate its performance. Results illustrate its superior performance compared to traditional classification methods and conventional CNN-based method, where about 97.25\% classification accuracy can be achieved in average.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	author = {Yu, Jia-sheng and Chen, Jin and Xiang, Z.Q. and Zou, Yue-Xian},
	month = dec,
	year = {2015},
	note = {ISSN: null},
	keywords = {endoscopes, medical image processing, neural nets, cascaded ELM, CNN-based WCE classification system, complex digestive tract, data-driven feature extractor, deep CNN classification system, digestive organs, discriminative image features extraction, diseases, Endoscopes, Esophagus, extreme learning machine, feature extraction, Feature extraction, full-connection classifier, HCNN-NELM, hybrid CNN, hybrid convolutional neural networks, image classification, Image classification, Iron, learning (artificial intelligence), noninvasive gastrointestinal disease examination, Testing, Training, WCE image classification system, wireless capsule endoscopy},
	pages = {1822--1827},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Q6UHSC46\\7419037.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WUP45ZGS\\Yu et al. - 2015 - A hybrid convolutional neural networks with extrem.pdf:application/pdf},
}

@inproceedings{zou_classifying_2015,
	title = {Classifying digestive organs in wireless capsule endoscopy images based on deep convolutional neural network},
	doi = {10.1109/ICDSP.2015.7252086},
	abstract = {This paper studies the classification problem of the digestive organs in wireless capsule endoscopy (WCE) images based on deep convolutional neural network (DCNN) framework. Essentially, DCNN proves having powerful ability to learn layer-wise hierarchy models with huge training data, which works similar to human biological visual systems. Classifying digestive organs in WCE images intuitively means to recognize higher semantic image features. To achieve this, an effective deep CNN-based WCE classification system has been constructed (DCNN-WCE-CS). With about 1 million real WCE images, intensive experiments are conducted to evaluate its performance by setting different network parameters. Results illustrate its superior performance compared to traditional classification methods, where about 95\% classification accuracy can be achieved in average. Moreover, it is observed that the DCNN-WCE-CS is robust to the large variations of the WCE images due to the individuals and complex digestive tract circumstance, including the rotation, the luminance change of the WCE images.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Digital} {Signal} {Processing} ({DSP})},
	author = {Zou, Yuexian and Li, Lei and Wang, Yi and Yu, Jiasheng and Li, Yi and Deng, W. J.},
	month = jul,
	year = {2015},
	note = {ISSN: 1546-1874},
	keywords = {biomedical optical imaging, endoscopes, medical image processing, Endoscopes, feature extraction, Feature extraction, image classification, learning (artificial intelligence), Training, wireless capsule endoscopy, Accuracy, biological organs, brightness, complex digestive tract circumstance, Convolution, DCNN-WCE-CS, deep CNN-based WCE classification system, deep convolutional neural network, digestive organ classification problem, digestive organs classification, feedforward neural nets, human biological visual systems, Intestines, layer-wise hierarchy models, luminance change, object recognition, parameter selection, semantic image feature recognition, training data, wireless capsule endoscopy images, Wireless communication},
	pages = {1274--1278},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VGDV79PK\\citations.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\437CK882\\Zou et al. - 2015 - Classifying digestive organs in wireless capsule e.pdf:application/pdf},
}

@article{yoon_clinically_2019-1,
	title = {A clinically translatable hyperspectral endoscopy ({HySE}) system for imaging the gastrointestinal tract},
	volume = {10},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-019-09484-4},
	doi = {10.1038/s41467-019-09484-4},
	language = {en},
	number = {1},
	urldate = {2019-12-18},
	journal = {Nature Communications},
	author = {Yoon, Jonghee and Joseph, James and Waterhouse, Dale J. and Luthman, A. Siri and Gordon, George S. D. and di Pietro, Massimiliano and Januszewicz, Wladyslaw and Fitzgerald, Rebecca C. and Bohndiek, Sarah E.},
	month = dec,
	year = {2019},
	pages = {1902},
	file = {41467_2019_9484_MOESM4_ESM.avi:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EFSNNBWL\\41467_2019_9484_MOESM4_ESM.avi:video/x-msvideo;41467_2019_9484_MOESM5_ESM.avi:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UIKGQ47L\\41467_2019_9484_MOESM5_ESM.avi:video/x-msvideo;41467_2019_9484_MOESM6_ESM.avi:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\BG9ZVIPP\\41467_2019_9484_MOESM6_ESM.avi:video/x-msvideo;suppl_movie_1.avi:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TCBVWAEZ\\suppl_movie_1.avi:video/x-msvideo;suppl_movie_2.avi:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UEIX3FIY\\suppl_movie_2.avi:video/x-msvideo;suppl_movie_3.avi:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3HXJW38I\\suppl_movie_3.avi:video/x-msvideo;Yoon et al. - 2019 - A clinically translatable hyperspectral endoscopy .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\68LF4EUH\\Yoon et al. - 2019 - A clinically translatable hyperspectral endoscopy .pdf:application/pdf;Yoon et al.-2019-HySE_suppl_movie_description.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9BTIRVPP\\Yoon et al.-2019-HySE_suppl_movie_description.pdf:application/pdf;Yoon et al.-2019-HySE_supplementary information.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WYEEU8A4\\Yoon et al.-2019-HySE_supplementary information.pdf:application/pdf},
}

@article{autor_exact_2020,
	title = {Exact {Information} {Bottleneck} with {Invertible} {Neural} {Networks}: {Getting} the {Best} of {Discriminative} and {Generative} {Modeling}},
	abstract = {The Information Bottleneck (IB) principle offers a uniﬁed approach to many learning and prediction problems. Although optimal in an information-theoretic sense, practical applications of IB are hampered by a lack of accurate high-dimensional estimators of mutual information, its main constituent. We propose to combine IB with invertible neural networks (INNs), which for the ﬁrst time allows exact calculation of the required mutual information. Applied to classiﬁcation, our proposed method results in a generative classiﬁer we call IB-INN. It accurately models the class conditional likelihoods, generalizes well to unseen data and reliably recognizes out-ofdistribution examples. In contrast to existing generative classiﬁers, these advantages incur only minor reductions in classiﬁcation accuracy in comparison to corresponding discriminative methods such as feed-forward networks. We provide insight into why IB-INNs are superior to traditional generative architectures and training procedures and show experimentally that our method outperforms alternative models of comparable complexity.},
	language = {en},
	author = {Autor, Anonymous},
	year = {2020},
	pages = {10},
	file = {Autor - 2020 - Exact Information Bottleneck with Invertible Neura.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\45JWMCFR\\Autor - 2020 - Exact Information Bottleneck with Invertible Neura.pdf:application/pdf},
}

@article{hendrycks_benchmark_2019,
	title = {A {Benchmark} for {Anomaly} {Segmentation}},
	url = {http://arxiv.org/abs/1911.11132},
	abstract = {Detecting out-of-distribution examples is important for safety-critical machine learning applications such as selfdriving vehicles. However, existing research mainly focuses on small-scale images where the whole image is considered anomalous. We propose to segment only the anomalous regions within an image, and hence we introduce the Combined Anomalous Object Segmentation benchmark for the more realistic task of large-scale anomaly segmentation. Our benchmark combines two novel datasets for anomaly segmentation that incorporate both realism and anomaly diversity. Using both real images and those from a simulated driving environment, we ensure the background context and a wide variety of anomalous objects are naturally integrated, unlike before. Additionally, we improve out-of-distribution detectors on large-scale multi-class datasets and introduce detectors for the previously unexplored setting of multi-label out-of-distribution detection. These novel baselines along with our anomaly segmentation benchmark open the door to further research in large-scale out-of-distribution detection and segmentation.},
	language = {en},
	urldate = {2019-12-02},
	journal = {arXiv:1911.11132 [cs]},
	author = {Hendrycks, Dan and Basart, Steven and Mazeika, Mantas and Mostajabi, Mohammadreza and Steinhardt, Jacob and Song, Dawn},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.11132},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Hendrycks et al. - 2019 - A Benchmark for Anomaly Segmentation.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CMY332DK\\Hendrycks et al. - 2019 - A Benchmark for Anomaly Segmentation.pdf:application/pdf},
}

@article{hassan_learning_nodate,
	title = {Learning {Joint} {Neural} {Rendering} and {Intrinsic} {Image} {Decomposition}},
	abstract = {Learning Joint Neural Rendering and Intrinsic Image Decomposition},
	number = {2020},
	journal = {CVPR},
	author = {Hassan},
	file = {hassan-intrinsic-images.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FAZ3QK2Z\\hassan-intrinsic-images.pdf:application/pdf},
}

@article{greenfeld_learning_2019,
	title = {Learning to {Optimize} {Multigrid} {PDE} {Solvers}},
	url = {http://arxiv.org/abs/1902.10248},
	abstract = {Constructing fast numerical solvers for partial differential equations (PDEs) is crucial for many scientiﬁc disciplines. A leading technique for solving large-scale PDEs is using multigrid methods. At the core of a multigrid solver is the prolongation matrix, which relates between different scales of the problem. This matrix is strongly problem-dependent, and its optimal construction is critical to the efﬁciency of the solver. In practice, however, devising multigrid algorithms for new problems often poses formidable challenges. In this paper we propose a framework for learning multigrid solvers. Our method learns a (single) mapping from a family of parameterized PDEs to prolongation operators. We train a neural network once for the entire class of PDEs, using an efﬁcient and unsupervised loss function. Experiments on a broad class of 2D diffusion problems demonstrate improved convergence rates compared to the widely used Black-Box multigrid scheme, suggesting that our method successfully learned rules for constructing prolongation matrices.},
	language = {en},
	urldate = {2019-11-29},
	journal = {arXiv:1902.10248 [cs, math]},
	author = {Greenfeld, Daniel and Galun, Meirav and Kimmel, Ron and Yavneh, Irad and Basri, Ronen},
	month = aug,
	year = {2019},
	note = {arXiv: 1902.10248},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {Greenfeld et al. - 2019 - Learning to Optimize Multigrid PDE Solvers.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6Z92TPYD\\Greenfeld et al. - 2019 - Learning to Optimize Multigrid PDE Solvers.pdf:application/pdf},
}

@article{zoph_neural_2017,
	title = {Neural {Architecture} {Search} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1611.01578},
	abstract = {Neural networks are powerful and ﬂexible models that work well for many difﬁcult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
	language = {en},
	urldate = {2019-11-29},
	journal = {arXiv:1611.01578 [cs]},
	author = {Zoph, Barret and Le, Quoc V.},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.01578},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {Zoph und Le - 2017 - Neural Architecture Search with Reinforcement Lear.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FJMH7LZS\\Zoph und Le - 2017 - Neural Architecture Search with Reinforcement Lear.pdf:application/pdf},
}

@inproceedings{paternina-arboleda_simulation-optimization_2008,
	address = {Miami, FL, USA},
	title = {Simulation-optimization using a reinforcement learning approach},
	isbn = {978-1-4244-2707-9},
	url = {https://ieeexplore.ieee.org/document/4736213/},
	doi = {10.1109/WSC.2008.4736213},
	abstract = {The global optimization of complex systems such as industrial systems often necessitates the use of computer simulation. In this paper, we suggest the use of reinforcement learning (RL) algorithms and artificial neural networks for the optimization of simulation models. Several types of variables are taken into account in order to find global optimum values. After a first evaluation through mathematical functions with known optima, the benefits of our approach are illustrated through the example of an inventory control problem frequently found in manufacturing systems. Single-item and multi-item inventory cases are considered. The efficiency of the proposed procedure is compared against a commercial tool.},
	language = {en},
	urldate = {2019-11-29},
	booktitle = {2008 {Winter} {Simulation} {Conference}},
	publisher = {IEEE},
	author = {Paternina-Arboleda, Carlos D. and Montoya-Torres, Jairo R. and Fabregas-Ariza, Aldo},
	month = dec,
	year = {2008},
	pages = {1376--1383},
	file = {Paternina-Arboleda et al. - 2008 - Simulation-optimization using a reinforcement lear.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CTXKSZRK\\Paternina-Arboleda et al. - 2008 - Simulation-optimization using a reinforcement lear.pdf:application/pdf},
}

@article{fujimoto_addressing_2018,
	title = {Addressing {Function} {Approximation} {Error} in {Actor}-{Critic} {Methods}},
	url = {http://arxiv.org/abs/1802.09477},
	abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
	language = {en},
	urldate = {2019-11-29},
	journal = {arXiv:1802.09477 [cs, stat]},
	author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.09477},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6QAZ442L\\Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf:application/pdf},
}

@article{lillicrap_continuous_2019,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to ﬁnd policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
	language = {en},
	urldate = {2019-11-29},
	journal = {arXiv:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = jul,
	year = {2019},
	note = {arXiv: 1509.02971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\68KTUSRT\\Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf:application/pdf},
}

@article{ruiz_learning_2019,
	title = {{LEARNING} {TO} {SIMULATE}},
	abstract = {Simulation is a useful tool in situations where training data for machine learning models is costly to annotate or even hard to acquire. In this work, we propose a reinforcement learning-based method for automatically adjusting the parameters of any (non-differentiable) simulator, thereby controlling the distribution of synthesized data in order to maximize the accuracy of a model trained on that data. In contrast to prior art that hand-crafts these simulation parameters or adjusts only parts of the available parameters, our approach fully controls the simulator with the actual underlying goal of maximizing accuracy, rather than mimicking the real data distribution or randomly generating a large volume of data. We ﬁnd that our approach (i) quickly converges to the optimal simulation parameters in controlled experiments and (ii) can indeed discover good sets of parameters for an image rendering simulator in actual computer vision applications.},
	language = {en},
	author = {Ruiz, Nataniel and Schulter, Samuel and Chandraker, Manmohan},
	year = {2019},
	pages = {12},
	file = {Ruiz et al. - 2019 - LEARNING TO SIMULATE.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\R6ZDHGS8\\Ruiz et al. - 2019 - LEARNING TO SIMULATE.pdf:application/pdf},
}

@article{amaran_simulation_2016,
	title = {Simulation optimization: a review of algorithms and applications},
	volume = {240},
	issn = {0254-5330, 1572-9338},
	shorttitle = {Simulation optimization},
	url = {http://link.springer.com/10.1007/s10479-015-2019-x},
	doi = {10.1007/s10479-015-2019-x},
	abstract = {Simulation optimization (SO) refers to the optimization of an objective function subject to constraints, both of which can be evaluated through a stochastic simulation. To address speciﬁc features of a particular simulation—discrete or continuous decisions, expensive or cheap simulations, single or multiple outputs, homogeneous or heterogeneous noise—various algorithms have been proposed in the literature. As one can imagine, there exist several competing algorithms for each of these classes of problems. This document emphasizes the difﬁculties in SO as compared to algebraic model-based mathematical programming, makes reference to state-of-the-art algorithms in the ﬁeld, examines and contrasts the different approaches used, reviews some of the diverse applications that have been tackled by these methods, and speculates on future directions in the ﬁeld.},
	language = {en},
	number = {1},
	urldate = {2019-11-29},
	journal = {Annals of Operations Research},
	author = {Amaran, Satyajith and Sahinidis, Nikolaos V. and Sharda, Bikram and Bury, Scott J.},
	month = may,
	year = {2016},
	pages = {351--380},
	file = {Amaran et al. - 2016 - Simulation optimization a review of algorithms an.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6GPBLC7Z\\Amaran et al. - 2016 - Simulation optimization a review of algorithms an.pdf:application/pdf},
}

@article{parr_matrix_2018-1,
	title = {The {Matrix} {Calculus} {You} {Need} {For} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1802.01528},
	abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don’t worry if you get stuck at some point along the way—just go back and reread the previous section, and try writing down and working through some examples. And if you’re still stuck, we’re happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here.},
	language = {en},
	urldate = {2019-11-26},
	journal = {arXiv:1802.01528 [cs, stat]},
	author = {Parr, Terence and Howard, Jeremy},
	month = jul,
	year = {2018},
	note = {arXiv: 1802.01528},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Parr und Howard - 2018 - The Matrix Calculus You Need For Deep Learning.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9YJI8YUN\\Parr und Howard - 2018 - The Matrix Calculus You Need For Deep Learning.pdf:application/pdf},
}

@article{jones_bayesian_2017-1,
	title = {Bayesian {Estimation} of {Intrinsic} {Tissue} {Oxygenation} and {Perfusion} {From} {RGB} {Images}},
	volume = {36},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/7859372/},
	doi = {10.1109/TMI.2017.2665627},
	abstract = {Multispectral imaging (MSI) can potentially assist the intra-operative assessment of tissue structure, function and viability, by providing information about oxygenation. In this paper, we present a novel technique for recovering intrinsic MSI measurements from endoscopic RGB images without custom hardware adaptations. The advantage of this approach is that it requires no modiﬁcation to existing surgical and diagnostic endoscopic imaging systems. Our method uses a radiometric color calibration of the endoscopic camera’s sensor in conjunction with a Bayesian framework to recover a per-pixel measurement of the total blood volume (THb) and oxygen saturation (SO2) in the observed tissue. The sensor’s pixel measurements are modeled as weighted sums over a mixture of Poisson distributions and we optimize the variables SO2 and THb to maximize the likelihood of the observations. To validate our technique, we use synthetic images generated from Monte Carlo physics simulation of light transport through soft tissue containing sub-surface blood vessels. We also validate our method on in vivo data by comparing it to a MSI dataset acquired with a hardware system that sequentially images multiple spectral bands without overlap. Our results are promising and show that we are able to provide surgeons with additional relevant information by processing endoscopic images with our modeling and inference framework. Index Terms— Multispectral imaging, Minimally invasive surgery, Bayesian inference, biophotonics, surgical vision.},
	language = {en},
	number = {7},
	urldate = {2019-11-26},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Jones, Geoffrey and Clancy, Neil T. and Helo, Yusuf and Arridge, Simon and Elson, Daniel S. and Stoyanov, Danail},
	month = jul,
	year = {2017},
	pages = {1491--1501},
	file = {Jones et al. - 2017 - Bayesian Estimation of Intrinsic Tissue Oxygenatio.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3R4UML6K\\Jones et al. - 2017 - Bayesian Estimation of Intrinsic Tissue Oxygenatio.pdf:application/pdf},
}

@article{ross_exploiting_2018,
	title = {Exploiting the potential of unlabeled endoscopic video data with self-supervised learning},
	url = {http://arxiv.org/abs/1711.09726},
	abstract = {Purpose Surgical data science is a new research ﬁeld that aims to observe all aspects of the patient treatment process in order to provide the right assistance at the right time. Due to the breakthrough successes of deep learning-based solutions for automatic image annotation, the availability of reference annotations for algorithm training is becoming a major bottleneck in the ﬁeld. The purpose of this paper was to investigate the concept of self-supervised learning to address this issue.},
	language = {en},
	urldate = {2019-11-26},
	journal = {arXiv:1711.09726 [cs]},
	author = {Ross, Tobias and Zimmerer, David and Vemuri, Anant and Isensee, Fabian and Wiesenfarth, Manuel and Bodenstedt, Sebastian and Both, Fabian and Kessler, Philip and Wagner, Martin and Müller, Beat and Kenngott, Hannes and Speidel, Stefanie and Kopp-Schneider, Annette and Maier-Hein, Klaus and Maier-Hein, Lena},
	month = jan,
	year = {2018},
	note = {arXiv: 1711.09726},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ross et al. - 2018 - Exploiting the potential of unlabeled endoscopic v.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Z7TUKAN4\\Ross et al. - 2018 - Exploiting the potential of unlabeled endoscopic v.pdf:application/pdf},
}

@article{botina_estimation_2019,
	title = {Estimation of {Biological} {Parameters} of {Cutaneous} {Ulcers} {Caused} by {Leishmaniasis} in an {Animal} {Model} {Using} {Diffuse} {Reflectance} {Spectroscopy}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/21/4674},
	doi = {10.3390/s19214674},
	abstract = {Cutaneous leishmaniasis (CL) is a neglected tropical disease that requires novel tools for its understanding, diagnosis, and treatment follow-up. In the cases of other cutaneous pathologies, such as cancer or cutaneous ulcers due to diabetes, optical diﬀuse reﬂectance-based tools and methods are widely used for the investigation of those illnesses. These types of tools and methods oﬀer the possibility to develop portable diagnosis and treatment follow-up systems. In this article, we propose the use of a three-layer diﬀuse reﬂectance model for the study of the formation of cutaneous ulcers caused by CL. The proposed model together with an inverse-modeling procedure were used in the evaluation of diﬀuse-reﬂectance spectral signatures acquired from cutaneous ulcers formed in the dorsal area of 21 golden hamsters inoculated with Leishmanisis braziliensis. As result, the quantiﬁcation of the model’s variables related to the main biological parameters of skin were obtained, such as: diameter and volumetric fraction of keratinocytes, collagen; volumetric fraction of hemoglobin, and oxygen saturation. Those parameters show statistically signiﬁcant diﬀerences among the diﬀerent stages of the CL ulcer formation. We found that these diﬀerences are coherent with histopathological manifestations reported in the literature for the main phases of CL formation.},
	language = {en},
	number = {21},
	urldate = {2019-11-26},
	journal = {Sensors},
	author = {Botina, Deivid and Franco, Ricardo and Murillo, Javier and Galeano, July and Zarzycki, Artur and Torres-Madronero, Maria C. and Bermúdez, Camilo and Montaño, Jaime and Garzón, Johnson and Marzani, Franck and Robledo, Sara M.},
	month = oct,
	year = {2019},
	pages = {4674},
	file = {Botina et al. - 2019 - Estimation of Biological Parameters of Cutaneous U.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JGXUBCSC\\Botina et al. - 2019 - Estimation of Biological Parameters of Cutaneous U.pdf:application/pdf},
}

@incollection{shen_generating_2019-2,
	address = {Cham},
	title = {Generating {Large} {Labeled} {Data} {Sets} for {Laparoscopic} {Image} {Processing} {Tasks} {Using} {Unpaired} {Image}-to-{Image} {Translation}},
	volume = {11768},
	isbn = {978-3-030-32253-3 978-3-030-32254-0},
	url = {http://link.springer.com/10.1007/978-3-030-32254-0_14},
	abstract = {In the medical domain, the lack of large training data sets and benchmarks is often a limiting factor for training deep neural networks. In contrast to expensive manual labeling, computer simulations can generate large and fully labeled data sets with a minimum of manual eﬀort. However, models that are trained on simulated data usually do not translate well to real scenarios. To bridge the domain gap between simulated and real laparoscopic images, we exploit recent advances in unpaired image-to-image translation. We extent an image-to-image translation method to generate a diverse multitude of realistically looking synthetic images based on images from a simple laparoscopy simulation. By incorporating means to ensure that the image content is preserved during the translation process, we ensure that the labels given for the simulated images remain valid for their realistically looking translations. This lets us generate a large, fully labeled synthetic data set. We show that this data set can be used to train models for the task of liver segmentation in laparoscopic images. We achieve median dice scores of up to 0.89 in some patients without manually labeling a single laparoscopic image and show that using our synthetic data to pre-train models can greatly improve their performance. The synthetic data set is made publicly available, fully labeled with segmentation maps, depth maps, normal maps, and positions of tools and camera (http://opencas.dkfz.de/image2image).},
	language = {en},
	urldate = {2019-11-26},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Pfeiffer, Micha and Funke, Isabel and Robu, Maria R. and Bodenstedt, Sebastian and Strenger, Leon and Engelhardt, Sandy and Roß, Tobias and Clarkson, Matthew J. and Gurusamy, Kurinchi and Davidson, Brian R. and Maier-Hein, Lena and Riediger, Carina and Welsch, Thilo and Weitz, Jürgen and Speidel, Stefanie},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	doi = {10.1007/978-3-030-32254-0_14},
	pages = {119--127},
	file = {Pfeiffer et al. - 2019 - Generating Large Labeled Data Sets for Laparoscopi.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EEGS66IG\\Pfeiffer et al. - 2019 - Generating Large Labeled Data Sets for Laparoscopi.pdf:application/pdf},
}

@article{xie_multispectral_nodate,
	title = {Multispectral and {Hyperspectral} {Image} {Fusion} by {MS}/{HS} {Fusion} {Net}},
	abstract = {Hyperspectral imaging can help better understand the characteristics of different materials, compared with traditional image systems. However, only high-resolution multispectral (HrMS) and low-resolution hyperspectral (LrHS) images can generally be captured at video rate in practice. In this paper, we propose a model-based deep learning approach for merging an HrMS and LrHS images to generate a high-resolution hyperspectral (HrHS) image. In speciﬁc, we construct a novel MS/HS fusion model which takes the observation models of low-resolution images and the lowrankness knowledge along the spectral mode of HrHS image into consideration. Then we design an iterative algorithm to solve the model by exploiting the proximal gradient method. And then, by unfolding the designed algorithm, we construct a deep network, called MS/HS Fusion Net, with learning the proximal operators and model parameters by convolutional neural networks. Experimental results on simulated and real data substantiate the superiority of our method both visually and quantitatively as compared with state-of-the-art methods along this line of research.},
	language = {en},
	author = {Xie, Qi and Zhou, Minghao and Zhao, Qian and Meng, Deyu and Zuo, Wangmeng and Xu, Zongben},
	pages = {10},
	file = {Xie et al. - Multispectral and Hyperspectral Image Fusion by MS.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\HIYF5VLK\\Xie et al. - Multispectral and Hyperspectral Image Fusion by MS.pdf:application/pdf},
}

@article{clancy_surgical_2020,
	title = {Surgical spectral imaging},
	volume = {63},
	issn = {1361-8415},
	doi = {10.1016/j.media.2020.101699},
	abstract = {Recent technological developments have resulted in the availability of miniaturised spectral imaging sensors capable of operating in the multi- (MSI) and hyperspectral imaging (HSI) regimes. Simultaneous advances in image-processing techniques and artificial intelligence (AI), especially in machine learning and deep learning have made these data-rich modalities highly attractive as a means of extracting biological information non-destructively. Surgery in particular is poised to benefit from this, as spectrally-resolved tissue optical properties can offer enhanced contrast as well as diagnostic and guidance information during interventions. This is particularly relevant for procedures where inherent contrast is low under standard white light visualisation. This review summarises recent work in surgical spectral imaging techniques, taken predominantly from Pubmed and Google Scholar searches spanning the period 2013-2019. New hardware, optimised for clinical use, is described, as well as computational approaches to extract spectral information. Model-based and machine-learning methods of data analysis are discussed in addition to simulation, phantom and clinical validation experiments.},
	language = {en},
	number = {101699},
	journal = {Medical Image Analysis},
	author = {Clancy, Neil T and Jones, Geoffrey and Maier-Hein, Lena and Elson, Daniel S and Stoyanov, Danail},
	month = jul,
	year = {2020},
	file = {Clancy et al. - 2019 - Surgical spectral imaging.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\49BFZ2EZ\\Clancy et al. - 2019 - Surgical spectral imaging.pdf:application/pdf},
}

@article{barberio_hyperspectral_2019-1,
	title = {{HYPerspectral} {Enhanced} {Reality} ({HYPER}): a physiology-based surgical guidance tool},
	issn = {0930-2794, 1432-2218},
	shorttitle = {{HYPerspectral} {Enhanced} {Reality} ({HYPER})},
	url = {http://link.springer.com/10.1007/s00464-019-06959-9},
	doi = {10.1007/s00464-019-06959-9},
	abstract = {Background  HSI is an optical technology allowing for a real-time, contrast-free snapshot of physiological tissue properties, including oxygenation. Hyperspectral imaging (HSI) has the potential to quantify the gastrointestinal perfusion intraoperatively. This experimental study evaluates the accuracy of HSI, in order to quantify bowel perfusion, and to obtain a superposition of the hyperspectral information onto real-time images.
Methods  In 6 pigs, 4 ischemic bowel loops were created (A, B, C, D) and imaged at set time points (from 5 to 360 min). A commercially available HSI system provided pseudo-color maps of the perfusion status (StO2, Near-InfraRed perfusion) and the tissue water index. An ad hoc software was developed to superimpose HSI information onto the live video, creating the HYPerspectral-based Enhanced Reality (HYPER). Seven regions of interest (ROIs) were identified in each bowel loop according to StO2 ranges, i.e., vascular (VASC proximal and distal), marginal vascular (MV proximal and distal), marginal ischemic (MI proximal and distal), and ischemic (ISCH). Local capillary lactates (LCL), reactive oxygen species (ROS), and histopathology were measured at the ROIs. A machine-learning-based prediction algorithm of LCL, based on the HSIStO2\%, was trained in the 6 pigs and tested on 5 additional animals.
Results  HSI parameters (StO2 and NIR) were congruent with LCL levels, ROS production, and histopathology damage scores at the ROIs discriminated by HYPER. The global mean error of LCL prediction was 1.18 ± 1.35 mmol/L. For StO2 values {\textgreater} 30\%, the mean error was 0.3 ± 0.33.
Conclusions  HYPER imaging could precisely quantify the overtime perfusion changes in this bowel ischemia model.},
	language = {en},
	urldate = {2019-11-26},
	journal = {Surgical Endoscopy},
	author = {Barberio, Manuel and Longo, Fabio and Fiorillo, Claudio and Seeliger, Barbara and Mascagni, Pietro and Agnus, Vincent and Lindner, Veronique and Geny, Bernard and Charles, Anne-Laure and Gockel, Ines and Worreth, Marc and Saadi, Alend and Marescaux, Jacques and Diana, Michele},
	month = jul,
	year = {2019},
	file = {Barberio et al. - 2019 - HYPerspectral Enhanced Reality (HYPER) a physiolo.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\L4MDBMSQ\\Barberio et al. - 2019 - HYPerspectral Enhanced Reality (HYPER) a physiolo.pdf:application/pdf},
}

@article{jones_bayesian_2017-2,
	title = {Bayesian {Estimation} of {Intrinsic} {Tissue} {Oxygenation} and {Perfusion} {From} {RGB} {Images}},
	volume = {36},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/7859372/},
	doi = {10.1109/TMI.2017.2665627},
	abstract = {Multispectral imaging (MSI) can potentially assist the intra-operative assessment of tissue structure, function and viability, by providing information about oxygenation. In this paper, we present a novel technique for recovering intrinsic MSI measurements from endoscopic RGB images without custom hardware adaptations. The advantage of this approach is that it requires no modiﬁcation to existing surgical and diagnostic endoscopic imaging systems. Our method uses a radiometric color calibration of the endoscopic camera’s sensor in conjunction with a Bayesian framework to recover a per-pixel measurement of the total blood volume (THb) and oxygen saturation (SO2) in the observed tissue. The sensor’s pixel measurements are modeled as weighted sums over a mixture of Poisson distributions and we optimize the variables SO2 and THb to maximize the likelihood of the observations. To validate our technique, we use synthetic images generated from Monte Carlo physics simulation of light transport through soft tissue containing sub-surface blood vessels. We also validate our method on in vivo data by comparing it to a MSI dataset acquired with a hardware system that sequentially images multiple spectral bands without overlap. Our results are promising and show that we are able to provide surgeons with additional relevant information by processing endoscopic images with our modeling and inference framework. Index Terms— Multispectral imaging, Minimally invasive surgery, Bayesian inference, biophotonics, surgical vision.},
	language = {en},
	number = {7},
	urldate = {2019-11-26},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Jones, Geoffrey and Clancy, Neil T. and Helo, Yusuf and Arridge, Simon and Elson, Daniel S. and Stoyanov, Danail},
	month = jul,
	year = {2017},
	pages = {1491--1501},
	file = {Jones et al. - 2017 - Bayesian Estimation of Intrinsic Tissue Oxygenatio.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KZMLKLE5\\Jones et al. - 2017 - Bayesian Estimation of Intrinsic Tissue Oxygenatio.pdf:application/pdf},
}

@article{luthman_bimodal_2018,
	title = {Bimodal reflectance and fluorescence multispectral endoscopy based on spectrally resolving detector arrays},
	volume = {24},
	issn = {1083-3668},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-biomedical-optics/volume-24/issue-03/031009/Bimodal-reflectance-and-fluorescence-multispectral-endoscopy-based-on-spectrally-resolving/10.1117/1.JBO.24.3.031009.full},
	doi = {10.1117/1.JBO.24.3.031009},
	language = {en},
	number = {03},
	urldate = {2019-11-26},
	journal = {Journal of Biomedical Optics},
	author = {Luthman, A. Siri and Waterhouse, Dale J. and Ansel-Bollepalli, Laura and Yoon, Jonghee and Gordon, George S. D. and Joseph, James and di Pietro, Massimiliano and Januszewicz, Wladyslaw and Bohndiek, Sarah E.},
	month = oct,
	year = {2018},
	pages = {1},
	file = {Luthman et al. - 2018 - Bimodal reflectance and fluorescence multispectral.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IBNXDW3M\\Luthman et al. - 2018 - Bimodal reflectance and fluorescence multispectral.pdf:application/pdf},
}

@article{yoon_clinically_2019-2,
	title = {A clinically translatable hyperspectral endoscopy ({HySE}) system for imaging the gastrointestinal tract},
	volume = {10},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-019-09484-4},
	doi = {10.1038/s41467-019-09484-4},
	language = {en},
	number = {1},
	urldate = {2019-11-26},
	journal = {Nature Communications},
	author = {Yoon, Jonghee and Joseph, James and Waterhouse, Dale J. and Luthman, A. Siri and Gordon, George S. D. and di Pietro, Massimiliano and Januszewicz, Wladyslaw and Fitzgerald, Rebecca C. and Bohndiek, Sarah E.},
	month = dec,
	year = {2019},
	pages = {1902},
	file = {Yoon et al. - 2019 - A clinically translatable hyperspectral endoscopy .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\LGKTQYAY\\Yoon et al. - 2019 - A clinically translatable hyperspectral endoscopy .pdf:application/pdf},
}

@article{chao_application_2019,
	title = {Application of {Artificial} {Intelligence} in the {Detection} and {Differentiation} of {Colon} {Polyps}: {A} {Technical} {Review} for {Physicians}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Application of {Artificial} {Intelligence} in the {Detection} and {Differentiation} of {Colon} {Polyps}},
	url = {https://www.mdpi.com/2075-4418/9/3/99},
	doi = {10.3390/diagnostics9030099},
	abstract = {Research in computer-aided diagnosis (CAD) and the application of artificial intelligence (AI) in the endoscopic evaluation of the gastrointestinal tract is novel. Since colonoscopy and detection of polyps can decrease the risk of colon cancer, it is recommended by multiple national and international societies. However, the procedure of colonoscopy is performed by humans where there are significant interoperator and interpatient variations, and hence, the risk of missing detection of adenomatous polyps. Early studies involving CAD and AI for the detection and differentiation of polyps show great promise. In this appraisal, we review existing scientific aspects of AI in CAD of colon polyps and discuss the pitfalls and future directions for advancing the science. This review addresses the technical intricacies in a manner that physicians can comprehend to promote a better understanding of this novel application.},
	language = {en},
	number = {3},
	urldate = {2019-11-04},
	journal = {Diagnostics},
	author = {Chao, Wei-Lun and Manickavasagan, Hanisha and Krishna, Somashekar G.},
	month = sep,
	year = {2019},
	keywords = {artificial intelligence, colon polyp, colonoscopy, computer-aided diagnosis, machine learning},
	pages = {99},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9KI7IMJ9\\Chao et al. - 2019 - Application of Artificial Intelligence in the Dete.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ZA4IG5TX\\99.html:text/html},
}

@article{halicek_-vivo_2019,
	title = {In-{Vivo} and {Ex}-{Vivo} {Tissue} {Analysis} through {Hyperspectral} {Imaging} {Techniques}: {Revealing} the {Invisible} {Features} of {Cancer}},
	volume = {11},
	issn = {2072-6694},
	shorttitle = {In-{Vivo} and {Ex}-{Vivo} {Tissue} {Analysis} through {Hyperspectral} {Imaging} {Techniques}},
	url = {https://www.mdpi.com/2072-6694/11/6/756},
	doi = {10.3390/cancers11060756},
	abstract = {In contrast to conventional optical imaging modalities, hyperspectral imaging (HSI) is able to capture much more information from a certain scene, both within and beyond the visual spectral range (from 400 to 700 nm). This imaging modality is based on the principle that each material provides different responses to light reflection, absorption, and scattering across the electromagnetic spectrum. Due to these properties, it is possible to differentiate and identify the different materials/substances presented in a certain scene by their spectral signature. Over the last two decades, HSI has demonstrated potential to become a powerful tool to study and identify several diseases in the medical field, being a non-contact, non-ionizing, and a label-free imaging modality. In this review, the use of HSI as an imaging tool for the analysis and detection of cancer is presented. The basic concepts related to this technology are detailed. The most relevant, state-of-the-art studies that can be found in the literature using HSI for cancer analysis are presented and summarized, both in-vivo and ex-vivo. Lastly, we discuss the current limitations of this technology in the field of cancer detection, together with some insights into possible future steps in the improvement of this technology.},
	language = {en},
	number = {6},
	urldate = {2019-06-04},
	journal = {Cancers},
	author = {Halicek, Martin and Fabelo, Himar and Ortega, Samuel and Callico, Gustavo M. and Fei, Baowei},
	month = may,
	year = {2019},
	pages = {756},
	file = {Halicek et al. - 2019 - In-Vivo and Ex-Vivo Tissue Analysis through Hypers.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KHYHLW3Y\\Halicek et al. - 2019 - In-Vivo and Ex-Vivo Tissue Analysis through Hypers.pdf:application/pdf},
}

@article{cabitza_proof_2019,
	title = {The proof of the pudding: in praise of a culture of real-world validation for medical artificial intelligence},
	volume = {7},
	issn = {2305-5847},
	shorttitle = {The proof of the pudding},
	url = {http://atm.amegroups.com/article/view/25300},
	doi = {10.21037/25300},
	abstract = {Artificial Intelligence (AI) in healthcare has become a quasi-normal subject (1). In the last few years, there has been an impressive increase in the number of publications concerning the application of machine learning (ML), a set of techniques and models for building data-driven AI systems, to medical tasks, such as the diagnosis, prognosis and anticipation of treatment effects and complications (2).},
	language = {en},
	number = {8},
	urldate = {2019-05-29},
	journal = {Annals of Translational Medicine},
	author = {Cabitza, Federico and Zeitoun, Jean-David},
	month = apr,
	year = {2019},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5C5H2PWV\\html.html:text/html},
}

@article{teredesai_explainable_nodate,
	title = {Explainable {Models} for {Healthcare} {AI}},
	language = {en},
	author = {Teredesai, Ankur and Ahmad, Muhammad Aurangzeb},
	pages = {42},
	file = {Teredesai and Ahmad - Explainable Models for Healthcare AI.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\U6HX4VV5\\Teredesai and Ahmad - Explainable Models for Healthcare AI.pdf:application/pdf},
}

@misc{kensci_kensci:_nodate,
	title = {{KenSci}: {Explainable} {ML} {Tutorial}},
	shorttitle = {{KenSci}},
	url = {http://kensci-5059707.hs-sites.com/kensci-explainable-ml-tutorial},
	language = {en},
	urldate = {2019-05-29},
	author = {KenSci},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UXVE88XC\\kensci-explainable-ml-tutorial.html:text/html},
}

@article{holzinger_what_2017,
	title = {What do we need to build explainable {AI} systems for the medical domain?},
	url = {http://arxiv.org/abs/1712.09923},
	abstract = {Artiﬁcial intelligence (AI) generally and machine learning (ML) speciﬁcally demonstrate impressive practical success in many diﬀerent application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. However, the central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles of such models they lack an explicit declarative knowledge representation, hence have diﬃculty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation (GDPR and ISO/IEC 27001) entering into force on May 25th 2018, will make black-box approaches diﬃcult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. This is beneﬁcial, e.g. for general understanding, for teaching, for learning, for research, and it can be helpful in court. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and speciﬁcally help to facilitate transparency and trust.},
	language = {en},
	urldate = {2019-05-29},
	journal = {arXiv:1712.09923 [cs, stat]},
	author = {Holzinger, Andreas and Biemann, Chris and Pattichis, Constantinos S. and Kell, Douglas B.},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.09923},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Holzinger et al. - 2017 - What do we need to build explainable AI systems fo.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\P94AK44A\\Holzinger et al. - 2017 - What do we need to build explainable AI systems fo.pdf:application/pdf},
}

@article{roscher_explainable_2019,
	title = {Explainable {Machine} {Learning} for {Scientific} {Insights} and {Discoveries}},
	url = {http://arxiv.org/abs/1905.08883},
	abstract = {Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientiﬁc insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientiﬁc outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientiﬁc consistency. In this article we review explainable machine learning in view of applications in the natural sciences and discuss three core elements which we identiﬁed as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientiﬁc works incorporating machine learning, and in particular to the way that explainable machine learning is used in their respective application areas.},
	language = {en},
	urldate = {2019-05-29},
	journal = {arXiv:1905.08883 [cs, stat]},
	author = {Roscher, Ribana and Bohn, Bastian and Duarte, Marco F. and Garcke, Jochen},
	month = may,
	year = {2019},
	note = {arXiv: 1905.08883},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Roscher et al. - 2019 - Explainable Machine Learning for Scientific Insigh.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7QH5UM34\\Roscher et al. - 2019 - Explainable Machine Learning for Scientific Insigh.pdf:application/pdf},
}

@article{samek_explainable_2017,
	title = {Explainable {Artificial} {Intelligence}: {Understanding}, {Visualizing} and {Interpreting} {Deep} {Learning} {Models}},
	shorttitle = {Explainable {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/1708.08296},
	abstract = {With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classiﬁcation, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artiﬁcial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this ﬁeld and makes a plea for more interpretability in artiﬁcial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classiﬁcation tasks.},
	language = {en},
	urldate = {2019-05-29},
	journal = {arXiv:1708.08296 [cs, stat]},
	author = {Samek, Wojciech and Wiegand, Thomas and Müller, Klaus-Robert},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.08296},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Computers and Society},
	file = {Samek et al. - 2017 - Explainable Artificial Intelligence Understanding.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ZPY72R5B\\Samek et al. - 2017 - Explainable Artificial Intelligence Understanding.pdf:application/pdf},
}

@incollection{holzinger_explainable_2018,
	address = {Cham},
	title = {Explainable {AI}: {The} {New} 42?},
	volume = {11015},
	isbn = {978-3-319-99739-1 978-3-319-99740-7},
	shorttitle = {Explainable {AI}},
	url = {http://link.springer.com/10.1007/978-3-319-99740-7_21},
	abstract = {Explainable AI is not a new ﬁeld. Since at least the early exploitation of C.S. Pierce’s abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-deﬁned domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.},
	language = {en},
	urldate = {2019-05-29},
	booktitle = {Machine {Learning} and {Knowledge} {Extraction}},
	publisher = {Springer International Publishing},
	author = {Goebel, Randy and Chander, Ajay and Holzinger, Katharina and Lecue, Freddy and Akata, Zeynep and Stumpf, Simone and Kieseberg, Peter and Holzinger, Andreas},
	editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
	year = {2018},
	doi = {10.1007/978-3-319-99740-7_21},
	pages = {295--303},
	file = {Goebel et al. - 2018 - Explainable AI The New 42.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\GBXBU3HE\\Goebel et al. - 2018 - Explainable AI The New 42.pdf:application/pdf},
}

@techreport{herent_brain_2018,
	type = {preprint},
	title = {Brain age prediction of healthy subjects on anatomic {MRI} with deep learning: going beyond with an "explainable {AI}" mindset},
	shorttitle = {Brain age prediction of healthy subjects on anatomic {MRI} with deep learning},
	url = {http://biorxiv.org/lookup/doi/10.1101/413302},
	abstract = {Objectives Define a clinically usable preprocessing pipeline for MRI data Predict brain age using various machine learning and deep learning algorithms Define Caveat against common machine learning traps Data and Methods We used 1597 open-access T1 weighted MRI from 24 hospitals. Preprocessing consisted in applying : N4 bias field correction, registration to MNI152 space, white and grey stripe intensity normalization, skull stripping and brain tissue segmentation Prediction of brain age was done with growing complexity of data input (histograms, grey matter from segmented MRI, raw data) and models for training (linear models, non linear model such as gradient boosting over decision trees, and 2D and 3D convolutional neural networks). Work on interpretability consisted in (i) proceeding on basic data visualization like correlations maps between age and voxels value, and generating (ii) weights maps of simpler models, (iii) heatmaps from CNNs model with occlusion method.
Results Processing time seemed feasible in a radiological workflow : 5 min for one 3D T1 MRI. We found a significant correlation between age and gray matter volume with a correlation r = -0.74. Our best model obtained a mean absolute error of 3.60 years, with fine tuned convolution neural network (CNN) pretrained on ImageNet. We carefully analyzed and interpreted the center effect. Our work on interpretability on simpler models permitted to observe heterogeneity of prediction depending on brain regions known for being involved in ageing (grey matter, ventricles). Occlusion method of CNN showed the importance of Insula and deep grey matter (thalami, caudate nuclei) in predictions.
Conclusions Predicting the brain age using deep learning could be a standardized metric usable in daily neuroradiological reports. An explainable algorithm gives more confidence and acceptability for its use in practice. More clinical studies using this new quantitative biomarker in neurological diseases will show how to use it at its best.},
	language = {en},
	urldate = {2019-05-29},
	institution = {Neuroscience},
	author = {Herent, Paul and Jegou, Simon and Wainrib, Gilles and Clozel, Thomas},
	month = sep,
	year = {2018},
	doi = {10.1101/413302},
	file = {Herent et al. - 2018 - Brain age prediction of healthy subjects on anatom.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PBQFHYLT\\Herent et al. - 2018 - Brain age prediction of healthy subjects on anatom.pdf:application/pdf},
}

@article{lee_explainable_2019,
	title = {An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets},
	volume = {3},
	issn = {2157-846X},
	url = {http://www.nature.com/articles/s41551-018-0324-9},
	doi = {10.1038/s41551-018-0324-9},
	language = {en},
	number = {3},
	urldate = {2019-05-29},
	journal = {Nature Biomedical Engineering},
	author = {Lee, Hyunkwang and Yune, Sehyo and Mansouri, Mohammad and Kim, Myeongchan and Tajmir, Shahein H. and Guerrier, Claude E. and Ebert, Sarah A. and Pomerantz, Stuart R. and Romero, Javier M. and Kamalian, Shahmir and Gonzalez, Ramon G. and Lev, Michael H. and Do, Synho},
	month = mar,
	year = {2019},
	pages = {173--182},
	file = {Lee et al. - 2019 - An explainable deep-learning algorithm for the det.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YD4QP5I8\\Lee et al. - 2019 - An explainable deep-learning algorithm for the det.pdf:application/pdf},
}

@inproceedings{ahmad_interpretable_2018,
	address = {New York, NY},
	title = {Interpretable {Machine} {Learning} in {Healthcare}},
	isbn = {978-1-5386-5377-7},
	url = {https://ieeexplore.ieee.org/document/8419428/},
	doi = {10.1109/ICHI.2018.00095},
	abstract = {This tutorial extensively covers the deﬁnitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.},
	language = {en},
	urldate = {2019-05-29},
	booktitle = {2018 {IEEE} {International} {Conference} on {Healthcare} {Informatics} ({ICHI})},
	publisher = {IEEE},
	author = {Ahmad, Muhammad Aurangzeb and Teredesai, Ankur and Eckert, Carly},
	month = jun,
	year = {2018},
	pages = {447--447},
	file = {Ahmad et al. - 2018 - Interpretable Machine Learning in Healthcare.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5ZZ97JS2\\Ahmad et al. - 2018 - Interpretable Machine Learning in Healthcare.pdf:application/pdf},
}

@article{litjens_survey_2017,
	title = {A survey on deep learning in medical image analysis},
	volume = {42},
	issn = {13618415},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841517301135},
	doi = {10.1016/j.media.2017.07.005},
	abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the ﬁeld, most of which appeared in the last year. We survey the use of deep learning for image classiﬁcation, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research. © 2017 Elsevier B.V. All rights reserved.},
	language = {en},
	urldate = {2019-05-29},
	journal = {Medical Image Analysis},
	author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A.W.M. and van Ginneken, Bram and Sánchez, Clara I.},
	month = dec,
	year = {2017},
	pages = {60--88},
	file = {Litjens et al. - 2017 - A survey on deep learning in medical image analysi.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XEGW5M2U\\Litjens et al. - 2017 - A survey on deep learning in medical image analysi.pdf:application/pdf},
}

@article{vellido_importance_2019,
	title = {The importance of interpretability and visualization in machine learning for applications in medicine and health care},
	issn = {0941-0643, 1433-3058},
	url = {http://link.springer.com/10.1007/s00521-019-04051-w},
	doi = {10.1007/s00521-019-04051-w},
	abstract = {In a short period of time, many areas of science have made a sharp transition towards data-dependent methods. In some cases, this process has been enabled by simultaneous advances in data acquisition and the development of networked system technologies. This new situation is particularly clear in the life sciences, where data overabundance has sparked a ﬂurry of new methodologies for data management and analysis. This can be seen as a perfect scenario for the use of machine learning and computational intelligence techniques to address problems in which more traditional data analysis approaches might struggle. But, this scenario also poses some serious challenges. One of them is model interpretability and explainability, especially for complex nonlinear models. In some areas such as medicine and health care, not addressing such challenge might seriously limit the chances of adoption, in real practice, of computer-based systems that rely on machine learning and computational intelligence methods for data analysis. In this paper, we reﬂect on recent investigations about the interpretability and explainability of machine learning methods and discuss their impact on medicine and health care. We pay speciﬁc attention to one of the ways in which interpretability and explainability in this context can be addressed, which is through data and model visualization. We argue that, beyond improving model interpretability as a goal in itself, we need to integrate the medical experts in the design of data analysis interpretation strategies. Otherwise, machine learning is unlikely to become a part of routine clinical and health care practice.},
	language = {en},
	urldate = {2019-05-29},
	journal = {Neural Computing and Applications},
	author = {Vellido, Alfredo},
	month = feb,
	year = {2019},
	file = {Vellido - 2019 - The importance of interpretability and visualizati.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WNTKN4I4\\Vellido - 2019 - The importance of interpretability and visualizati.pdf:application/pdf},
}

@inproceedings{holzinger_machine_2018,
	title = {From {Machine} {Learning} to {Explainable} {AI}},
	doi = {10.1109/DISA.2018.8490530},
	abstract = {The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible “glass-box” approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.},
	booktitle = {2018 {World} {Symposium} on {Digital} {Intelligence} for {Systems} and {Machines} ({DISA})},
	author = {Holzinger, A.},
	month = aug,
	year = {2018},
	keywords = {learning (artificial intelligence), artificial intelligence, AI winter, big quality data, Cognitive science, computational power, context-adaptive procedures, contextual explanatory models, Data mining, Data visualization, deep learning approaches, engineering effort, Games, glass-box approaches, high-dimensional vectors, knowledge representations, logical approaches, Machine learning, mathematical principles, ontologies, ontologies (artificial intelligence), probabilistic learning methods, probability, rule-based approaches, statistical machine learning methods, Uncertainty},
	pages = {55--66},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\HYC2V4Q2\\8490530.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5ZY5DAZH\\Holzinger - 2018 - From Machine Learning to Explainable AI.pdf:application/pdf},
}

@inproceedings{hirasawa_effectiveness_2018,
	title = {{EFFECTIVENESS} {OF} {ARTIFICIAL} {INTELLIGENCE} {USING} {DEEP} {LEARNING} {FOR} {DETECTING} {GASTRIC} {CANCER} {IN} {ENDOSCOPIC} {IMAGES}},
	volume = {50},
	copyright = {Georg Thieme Verlag KG Stuttgart · New York},
	url = {http://www.thieme-connect.de/DOI/DOI?10.1055/s-0038-1637183},
	doi = {10.1055/s-0038-1637183},
	abstract = {Thieme E-Books \& E-Journals},
	language = {en},
	urldate = {2019-05-29},
	booktitle = {Endoscopy},
	publisher = {Georg Thieme Verlag KG},
	author = {Hirasawa, T. and Aoyama, K. and Fujisaki, J. and Tada, T.},
	month = apr,
	year = {2018},
	keywords = {{\textless}conference{\textgreater}{\textless}conf-date{\textgreater}{\textless}day{\textgreater}19{\textless}/day{\textgreater}{\textless}txt{\textgreater}–{\textless}/txt{\textgreater}{\textless}day{\textgreater}21{\textless}/day{\textgreater}{\textless}month{\textgreater}04{\textless}/month{\textgreater}{\textless}year{\textgreater}2018{\textless}/year{\textgreater}{\textless}/conf-date{\textgreater}{\textless}conf-name{\textgreater}ESGE Days 2018 accepted abstracts{\textless}/conf-name{\textgreater}{\textless}conf-num{\textgreater}01{\textless}/conf-num{\textgreater}{\textless}conf-loc{\textgreater}Budapest, Hungary{\textless}/conf-loc{\textgreater}{\textless}conf-sponsor{\textgreater}European Society of Gastrointestinal Endoscopy (ESGE){\textless}/conf-sponsor{\textgreater}{\textless}conf-president/{\textgreater}{\textless}/conference{\textgreater}},
	pages = {OP146},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MUYU7SN8\\login.html:text/html},
}

@article{hwang_application_2018,
	title = {Application of {Artificial} {Intelligence} in {Capsule} {Endoscopy}: {Where} {Are} {We} {Now}?},
	volume = {51},
	issn = {2234-2400, 2234-2443},
	shorttitle = {Application of {Artificial} {Intelligence} in {Capsule} {Endoscopy}},
	url = {http://e-ce.org/journal/view.php?doi=10.5946/ce.2018.173},
	doi = {10.5946/ce.2018.173},
	language = {en},
	number = {6},
	urldate = {2019-05-29},
	journal = {Clinical Endoscopy},
	author = {Hwang, Youngbae and Park, Junseok and Lim, Yun Jeong and Chun, Hoon Jai},
	month = nov,
	year = {2018},
	pages = {547--551},
	file = {Hwang et al. - 2018 - Application of Artificial Intelligence in Capsule .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6JZDPRW8\\Hwang et al. - 2018 - Application of Artificial Intelligence in Capsule .pdf:application/pdf},
}

@article{iakovidis_detecting_2018,
	title = {Detecting and {Locating} {Gastrointestinal} {Anomalies} {Using} {Deep} {Learning} and {Iterative} {Cluster} {Unification}},
	volume = {37},
	issn = {0278-0062},
	doi = {10.1109/TMI.2018.2837002},
	abstract = {This paper proposes a novel methodology for automatic detection and localization of gastrointestinal (GI) anomalies in endoscopic video frame sequences. Training is performed with weakly annotated images, using only image-level, semantic labels instead of detailed, and pixel-level annotations. This makes it a cost-effective approach for the analysis of large videoendoscopy repositories. Other advantages of the proposed methodology include its capability to suggest possible locations of GI anomalies within the video frames, and its generality, in the sense that abnormal frame detection is based on automatically derived image features. It is implemented in three phases: 1) it classifies the video frames into abnormal or normal using a weakly supervised convolutional neural network (WCNN) architecture; 2) detects salient points from deeper WCNN layers, using a deep saliency detection algorithm; and 3) localizes GI anomalies using an iterative cluster unification (ICU) algorithm. ICU is based on a pointwise cross-feature-map (PCFM) descriptor extracted locally from the detected salient points using information derived from the WCNN. Results, from extensive experimentation using publicly available collections of gastrointestinal endoscopy video frames, are presented. The data sets used include a variety of GI anomalies. Both anomaly detection and localization performance achieved, in terms of the area under receiver operating characteristic (AUC), were {\textgreater}80\%. The highest AUC for anomaly detection was obtained on conventional gastroscopy images, reaching 96\%, and the highest AUC for anomaly localization was obtained on wireless capsule endoscopy images, reaching 88\%.},
	number = {10},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Iakovidis, D. K. and Georgakopoulos, S. V. and Vasilakakis, M. and Koulaouzidis, A. and Plagianakos, V. P.},
	month = oct,
	year = {2018},
	keywords = {Image color analysis, biomedical optical imaging, endoscopes, image sequences, medical image processing, neural nets, Endoscopes, feature extraction, Feature extraction, image classification, learning (artificial intelligence), Training, biological organs, object recognition, wireless capsule endoscopy images, machine learning, abnormal frame detection, anomaly detection, anomaly localization, automatic detection, automatically derived image features, computer-aided detection and diagnosis, cost-effective approach, deep learning, deep saliency detection algorithm, endoscopic video frame sequences, Endoscopy, gastrointestinal anomalies, gastrointestinal endoscopy video frames, gastrointestinal tract, Gastrointestinal tract, gastroscopy images, GI anomalies, Image segmentation, image-level, iterative cluster unification algorithm, iterative methods, Lesions, localization performance, object detection, pixel-level annotations, pointwise cross-feature-map descriptor, salient points, video signal processing, WCNN layers, weakly annotated images, weakly supervised convolutional neural network},
	pages = {2196--2210},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KPSQ7JY3\\Iakovidis et al. - 2018 - Detecting and Locating Gastrointestinal Anomalies .pdf:application/pdf},
}

@article{yuan_deep_2017,
	title = {Deep learning for polyp recognition in wireless capsule endoscopy images},
	volume = {44},
	issn = {00942405},
	url = {http://doi.wiley.com/10.1002/mp.12147},
	doi = {10.1002/mp.12147},
	abstract = {Purpose: Wireless capsule endoscopy (WCE) enables physicians to examine the digestive tract without any surgical operations, at the cost of a large volume of images to be analyzed. In the computer-aided diagnosis of WCE images, the main challenge arises from the difficulty of robust characterization of images. This study aims to provide discriminative description of WCE images and assist physicians to recognize polyp images automatically.
Methods: We propose a novel deep feature learning method, named stacked sparse autoencoder with image manifold constraint (SSAEIM), to recognize polyps in the WCE images. Our SSAEIM differs from the traditional sparse autoencoder (SAE) by introducing an image manifold constraint, which is constructed by a nearest neighbor graph and represents intrinsic structures of images. The image manifold constraint enforces that images within the same category share similar learned features and images in different categories should be kept far away. Thus, the learned features preserve large intervariances and small intravariances among images.
Results: The average overall recognition accuracy (ORA) of our method for WCE images is 98.00\%. The accuracies for polyps, bubbles, turbid images, and clear images are 98.00\%, 99.50\%, 99.00\%, and 95.50\%, respectively. Moreover, the comparison results show that our SSAEIM outperforms existing polyp recognition methods with relative higher ORA.
Conclusion: The comprehensive results have demonstrated that the proposed SSAEIM can provide descriptive characterization for WCE images and recognize polyps in a WCE video accurately. This method could be further utilized in the clinical trials to help physicians from the tedious image reading work. © 2017 American Association of Physicists in Medicine [https://doi.org/10.1002/mp.12147]},
	language = {en},
	number = {4},
	urldate = {2019-05-29},
	journal = {Medical Physics},
	author = {Yuan, Yixuan and Meng, Max Q.-H.},
	month = apr,
	year = {2017},
	pages = {1379--1389},
	file = {Yuan and Meng - 2017 - Deep learning for polyp recognition in wireless ca.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2MKGM9BT\\Yuan and Meng - 2017 - Deep learning for polyp recognition in wireless ca.pdf:application/pdf},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359121},
	urldate = {2019-05-29},
	file = {IEEE Xplore Full-Text PDF\::C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6XHG84TF\\stamp.html:text/html},
}

@inproceedings{zou_classifying_2015-1,
	title = {Classifying digestive organs in wireless capsule endoscopy images based on deep convolutional neural network},
	doi = {10.1109/ICDSP.2015.7252086},
	abstract = {This paper studies the classification problem of the digestive organs in wireless capsule endoscopy (WCE) images based on deep convolutional neural network (DCNN) framework. Essentially, DCNN proves having powerful ability to learn layer-wise hierarchy models with huge training data, which works similar to human biological visual systems. Classifying digestive organs in WCE images intuitively means to recognize higher semantic image features. To achieve this, an effective deep CNN-based WCE classification system has been constructed (DCNN-WCE-CS). With about 1 million real WCE images, intensive experiments are conducted to evaluate its performance by setting different network parameters. Results illustrate its superior performance compared to traditional classification methods, where about 95\% classification accuracy can be achieved in average. Moreover, it is observed that the DCNN-WCE-CS is robust to the large variations of the WCE images due to the individuals and complex digestive tract circumstance, including the rotation, the luminance change of the WCE images.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Digital} {Signal} {Processing} ({DSP})},
	author = {Zou, Y. and Li, L. and Wang, Y. and Yu, J. and Li, Y. and Deng, W. J.},
	month = jul,
	year = {2015},
	keywords = {biomedical optical imaging, endoscopes, medical image processing, Endoscopes, feature extraction, Feature extraction, image classification, learning (artificial intelligence), Training, wireless capsule endoscopy, Accuracy, biological organs, brightness, complex digestive tract circumstance, Convolution, DCNN-WCE-CS, deep CNN-based WCE classification system, deep convolutional neural network, digestive organ classification problem, digestive organs classification, feedforward neural nets, human biological visual systems, Intestines, layer-wise hierarchy models, luminance change, object recognition, parameter selection, semantic image feature recognition, training data, wireless capsule endoscopy images, Wireless communication},
	pages = {1274--1278},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\J3H8CLJS\\7252086.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\V2NGCT39\\Zou et al. - 2015 - Classifying digestive organs in wireless capsule e.pdf:application/pdf},
}

@article{abusrewil_pth-002_nodate,
	title = {{PTH}-002 {COLONOSCOPY} {AFTER} {BOWELSCOPE} ({BS}) {FLEXIBLE} {SIGMOIDOSCOPY} ({FS}) – {URGENT} {OR} {ROUTINE}?},
	abstract = {Introduction and aims The implementation of BS (1 off FS for 55 year olds) has led to significant pressures on endoscopy units with large numbers of procedures. In addition patients with certain findings (which include number, size, dysplasia and villous component degree of adenomatous polyps) will require colonoscopy (FC). This has to be done within 2 weeks. The North of Tyne screening centre serves a population {\textgreater}8 60 000. ‘Roll out’ of BS started in 2014 and now covers patients enrolled in 50\% of our regional GP practices. At this ‘halfway stage’ we aimed to assess: . Attendance and findings in those invited . Proportion of patients who require FC after FS and significance of proximal pathology . Incidence and sites of any malignancy . The proportion with neoplasia after colonoscopy requiring future surveillance
Method Data was collected on all patients who had FC after FS in the BS program for the 12 months from 1/1/2017 (obtained from the central database and crosschecked with local records). We reviewed all endoscopy and histology reports to obtain patient demographics, FC indication, findings and all histology. The extent of each FS was accurately recorded with aid of Olympus imager (scope guide).
Results 2698 of the 3629 who responded to the written invitation attended for FS. 130 (4.8\% of attenders) met criteria for FC – Main reasons:!10 mm polyp (34\%);!3 polyps (21.5\%); villous histology (21.5\%); anticoagulant/antiplatelet use (4.6\%) After colonoscopy, 54 have neoplasia requiring for future surveillance – 33 high risk category (1 year); 21, intermediate risk (3 years). 4 patients had malignancies: 1x rectal polyp cancer; 1x sigmoid cancer (T2N0); 1x descending colon cancer (T3N1M1); 1 splenic flexure cancer (T4N1); At colonoscopy, 37 patients had adenomas proximal to the splenic flexure but all were {\textless}10 mm with low-grade dysplasia
Conclusions . 74\% of patients who initially showed interest attended for FS . Almost 5\% of patients attending for BS require FC; of these 41.5\% will have intermediate or high risk neoplasia requiring future surveillance . A small proportion (1.5/1000 screened) of attenders were found to have a cancer . 28.5\% had neoplastic lesions beyond the splenic flexure, none with high grade dysplasia/cancer},
	language = {en},
	author = {Abusrewil, Anwar Suleiman and Hayat, Mumtaz and Dixon, Heather and Rowell, Karen and Nylander, David and Ahmad, Ahmir and Bottle, Juliet and Laverty, Anthony and Murray, Sam and Ewing, Iain and Ahmad, Omer F and Luengo, Imanol and Herrera, Luis Garcia Peraza and Brandao, Patrick and Li, Wenqi and Everson, Martin and Haidry, Rehan and Vega, Roser and Seward, Ed and Vercauteren, Tom and Lovat, Laurence B},
	pages = {2},
	file = {Abusrewil et al. - PTH-002 COLONOSCOPY AFTER BOWELSCOPE (BS) FLEXIBLE.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\AYH5RL2F\\Abusrewil et al. - PTH-002 COLONOSCOPY AFTER BOWELSCOPE (BS) FLEXIBLE.pdf:application/pdf},
}

@article{blanes-vidal_addressing_2019,
	title = {Addressing priority challenges in the detection and assessment of colorectal polyps from capsule endoscopy and colonoscopy in colorectal cancer screening using machine learning},
	volume = {58},
	issn = {0284-186X, 1651-226X},
	url = {https://www.tandfonline.com/doi/full/10.1080/0284186X.2019.1584404},
	doi = {10.1080/0284186X.2019.1584404},
	abstract = {Background: Colorectal capsule endoscopy (CCE) is a potentially valuable patient-friendly technique for colorectal cancer screening in large populations. Before it can be widely applied, significant research priorities need to be addressed. We present two innovative data science algorithms which can considerably improve acquisition and analysis of relevant data on colorectal polyps obtained from capsule endoscopy.},
	language = {en},
	number = {sup1},
	urldate = {2019-05-29},
	journal = {Acta Oncologica},
	author = {Blanes-Vidal, Victoria and Baatrup, Gunnar and Nadimi, Esmaeil S.},
	month = apr,
	year = {2019},
	pages = {S29--S36},
	file = {Blanes-Vidal et al. - 2019 - Addressing priority challenges in the detection an.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PBGLT2UN\\Blanes-Vidal et al. - 2019 - Addressing priority challenges in the detection an.pdf:application/pdf},
}

@article{mori_real-time_2018,
	title = {Real-{Time} {Use} of {Artificial} {Intelligence} in {Identification} of {Diminutive} {Polyps} {During} {Colonoscopy}: {A} {Prospective} {Study}},
	volume = {169},
	issn = {0003-4819},
	shorttitle = {Real-{Time} {Use} of {Artificial} {Intelligence} in {Identification} of {Diminutive} {Polyps} {During} {Colonoscopy}},
	url = {http://annals.org/article.aspx?doi=10.7326/M18-0249},
	doi = {10.7326/M18-0249},
	language = {en},
	number = {6},
	urldate = {2019-05-29},
	journal = {Annals of Internal Medicine},
	author = {Mori, Yuichi and Kudo, Shin-ei and Misawa, Masashi and Saito, Yutaka and Ikematsu, Hiroaki and Hotta, Kinichi and Ohtsuka, Kazuo and Urushibara, Fumihiko and Kataoka, Shinichi and Ogawa, Yushi and Maeda, Yasuharu and Takeda, Kenichi and Nakamura, Hiroki and Ichimasa, Katsuro and Kudo, Toyoki and Hayashi, Takemasa and Wakamura, Kunihiko and Ishida, Fumio and Inoue, Haruhiro and Itoh, Hayato and Oda, Masahiro and Mori, Kensaku},
	month = sep,
	year = {2018},
	pages = {357},
	file = {Mori et al. - 2018 - Real-Time Use of Artificial Intelligence in Identi.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DPVA6PWF\\Mori et al. - 2018 - Real-Time Use of Artificial Intelligence in Identi.pdf:application/pdf},
}

@article{byrne_real-time_2019,
	title = {Real-time differentiation of adenomatous and hyperplastic diminutive colorectal polyps during analysis of unaltered videos of standard colonoscopy using a deep learning model},
	volume = {68},
	issn = {0017-5749, 1468-3288},
	url = {http://gut.bmj.com/lookup/doi/10.1136/gutjnl-2017-314547},
	doi = {10.1136/gutjnl-2017-314547},
	abstract = {Background In general, academic but not community endoscopists have demonstrated adequate endoscopic differentiation accuracy to make the ’resect and discard’ paradigm for diminutive colorectal polyps workable. Computer analysis of video could potentially eliminate the obstacle of interobserver variability in endoscopic polyp interpretation and enable widespread acceptance of ’resect and discard’. Study design and methods  We developed an artificial intelligence (AI) model for real-time assessment of endoscopic video images of colorectal polyps. A deep convolutional neural network model was used. Only narrow band imaging video frames were used, split equally between relevant multiclasses. Unaltered videos from routine exams not specifically designed or adapted for AI classification were used to train and validate the model. The model was tested on a separate series of 125 videos of consecutively encountered diminutive polyps that were proven to be adenomas or hyperplastic polyps.
Results The AI model works with a confidence mechanism and did not generate sufficient confidence to predict the histology of 19 polyps in the test set, representing 15\% of the polyps. For the remaining 106 diminutive polyps, the accuracy of the model was 94\% (95\% CI 86\% to 97\%), the sensitivity for identification of adenomas was 98\% (95\% CI 92\% to 100\%), specificity was 83\% (95\% CI 67\% to 93\%), negative predictive value 97\% and positive predictive value 90\%.
Conclusions An AI model trained on endoscopic video can differentiate diminutive adenomas from hyperplastic polyps with high accuracy. Additional study of this programme in a live patient clinical trial setting to address resect and discard is planned.},
	language = {en},
	number = {1},
	urldate = {2019-05-29},
	journal = {Gut},
	author = {Byrne, Michael F and Chapados, Nicolas and Soudan, Florian and Oertel, Clemens and Linares Pérez, Milagros and Kelly, Raymond and Iqbal, Nadeem and Chandelier, Florent and Rex, Douglas K},
	month = jan,
	year = {2019},
	pages = {94--100},
	file = {Byrne et al. - 2019 - Real-time differentiation of adenomatous and hyper.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ELGZB3XN\\Byrne et al. - 2019 - Real-time differentiation of adenomatous and hyper.pdf:application/pdf},
}

@article{chen_accurate_2018,
	title = {Accurate {Classification} of {Diminutive} {Colorectal} {Polyps} {Using} {Computer}-{Aided} {Analysis}},
	volume = {154},
	issn = {00165085},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0016508517362510},
	doi = {10.1053/j.gastro.2017.10.010},
	language = {en},
	number = {3},
	urldate = {2019-05-29},
	journal = {Gastroenterology},
	author = {Chen, Peng-Jen and Lin, Meng-Chiung and Lai, Mei-Ju and Lin, Jung-Chun and Lu, Henry Horng-Shing and Tseng, Vincent S.},
	month = feb,
	year = {2018},
	pages = {568--575},
	file = {Chen et al. - 2018 - Accurate Classification of Diminutive Colorectal P.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\GEEW9YGI\\Chen et al. - 2018 - Accurate Classification of Diminutive Colorectal P.pdf:application/pdf},
}

@article{urban_deep_2018,
	title = {Deep {Learning} {Localizes} and {Identifies} {Polyps} in {Real} {Time} {With} 96\% {Accuracy} in {Screening} {Colonoscopy}},
	volume = {155},
	issn = {00165085},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0016508518346596},
	doi = {10.1053/j.gastro.2018.06.037},
	language = {en},
	number = {4},
	urldate = {2019-05-29},
	journal = {Gastroenterology},
	author = {Urban, Gregor and Tripathi, Priyam and Alkayali, Talal and Mittal, Mohit and Jalali, Farid and Karnes, William and Baldi, Pierre},
	month = oct,
	year = {2018},
	pages = {1069--1078.e8},
	file = {Urban et al. - 2018 - Deep Learning Localizes and Identifies Polyps in R.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\NG2YE2T4\\Urban et al. - 2018 - Deep Learning Localizes and Identifies Polyps in R.pdf:application/pdf},
}

@article{ahmad_human-machine_2019,
	title = {Human-machine collaboration: bringing artificial intelligence into colonoscopy},
	volume = {10},
	issn = {2041-4137, 2041-4145},
	shorttitle = {Human-machine collaboration},
	url = {http://fg.bmj.com/lookup/doi/10.1136/flgastro-2018-101047},
	doi = {10.1136/flgastro-2018-101047},
	language = {en},
	number = {2},
	urldate = {2019-05-29},
	journal = {Frontline Gastroenterology},
	author = {Ahmad, Omer F and Stoyanov, Danail and Lovat, Laurence B},
	month = apr,
	year = {2019},
	pages = {198--199},
	file = {Ahmad et al. - 2019 - Human-machine collaboration bringing artificial i.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XTEGG6JV\\Ahmad et al. - 2019 - Human-machine collaboration bringing artificial i.pdf:application/pdf},
}

@article{chen_making_2018,
	title = {Making deep learning models transparent},
	volume = {1},
	issn = {26172496},
	url = {http://jmai.amegroups.com/article/view/4428/5445},
	doi = {10.21037/jmai.2018.07.01},
	language = {en},
	urldate = {2019-05-29},
	journal = {Journal of Medical Artificial Intelligence},
	author = {Chen, Lujia and Lu, Xinghua},
	year = {2018},
	pages = {1--1},
	file = {Chen and Lu - 2018 - Making deep learning models transparent.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FZR6IU4Q\\Chen and Lu - 2018 - Making deep learning models transparent.pdf:application/pdf},
}

@article{abd_ghani_decision-level_2018,
	title = {Decision-level fusion scheme for nasopharyngeal carcinoma identification using machine learning techniques},
	issn = {0941-0643, 1433-3058},
	url = {http://link.springer.com/10.1007/s00521-018-3882-6},
	doi = {10.1007/s00521-018-3882-6},
	abstract = {Making an accurate diagnosis of nasopharyngeal carcinoma (NPC) disease is a challenging task that involves many parties such as radiology specialists often times need to delineate NPC boundaries on various tumor-bearing endoscopic images. It is a tedious and time-consuming operation exceedingly based on doctors and experience of radiologist. NPC has complex and irregular structures which makes it difﬁcult to diagnose even by an expert physician. However, the diagnosis accuracy results of such methods are still insigniﬁcant and need improvement in order to manifest robust solution. The study aim is to develop and propose a new automatic classiﬁcation of NPC tumor using machine learning techniques and feature-based decision-level fusion scheme from endoscopic images. We have implemented the fusion of the three image texture-based schemes (local binary patterns, the ﬁrst-order statistics histogram properties, and histogram of gray scale) at the decision level and tested the performance of this scheme using the same experimental setup in the previous section for simple scorelevel fusion, but for comparison, We used the classiﬁers methods which are support vector machines (SVM), k-nearest neighbors’ algorithm, and artiﬁcial neural network (ANN). The results demonstrate that the majority rule for decisionbased fusion is outperformed considerably by the single best performing feature scheme (FFGF) for the SVM classiﬁer, but for the ANN and KNN classiﬁer it is signiﬁcantly outperformed by each of the components features. The classiﬁers approaches were listed a high accuracy of 94.07\%, the sensitivity of 92.05\%, and speciﬁcity of 93.07\%.},
	language = {en},
	urldate = {2019-05-29},
	journal = {Neural Computing and Applications},
	author = {Abd Ghani, Mohd Khanapi and Mohammed, Mazin Abed and Arunkumar, N. and Mostafa, Salama A. and Ibrahim, Dheyaa Ahmed and Abdullah, Mohamad Khir and Jaber, Mustafa Musa and Abdulhay, Enas and Ramirez-Gonzalez, Gustavo and Burhanuddin, M. A.},
	month = nov,
	year = {2018},
	file = {Abd Ghani et al. - 2018 - Decision-level fusion scheme for nasopharyngeal ca.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UBANSJUE\\Abd Ghani et al. - 2018 - Decision-level fusion scheme for nasopharyngeal ca.pdf:application/pdf},
}

@article{hirasawa_application_2018,
	title = {Application of artificial intelligence using a convolutional neural network for detecting gastric cancer in endoscopic images},
	volume = {21},
	issn = {1436-3291, 1436-3305},
	url = {http://link.springer.com/10.1007/s10120-018-0793-2},
	doi = {10.1007/s10120-018-0793-2},
	abstract = {Background  Image recognition using artificial intelligence with deep learning through convolutional neural networks (CNNs) has dramatically improved and been increasingly applied to medical fields for diagnostic imaging. We developed a CNN that can automatically detect gastric cancer in endoscopic images.
Methods  A CNN-based diagnostic system was constructed based on Single Shot MultiBox Detector architecture and trained using 13,584 endoscopic images of gastric cancer. To evaluate the diagnostic accuracy, an independent test set of 2296 stomach images collected from 69 consecutive patients with 77 gastric cancer lesions was applied to the constructed CNN.
Results  The CNN required 47 s to analyze 2296 test images. The CNN correctly diagnosed 71 of 77 gastric cancer lesions with an overall sensitivity of 92.2\%, and 161 non-cancerous lesions were detected as gastric cancer, resulting in a positive predictive value of 30.6\%. Seventy of the 71 lesions (98.6\%) with a diameter of 6 mm or more as well as all invasive cancers were correctly detected. All missed lesions were superficially depressed and differentiated-type intramucosal cancers that were difficult to distinguish from gastritis even for experienced endoscopists. Nearly half of the false-positive lesions were gastritis with changes in color tone or an irregular mucosal surface.
Conclusion  The constructed CNN system for detecting gastric cancer could process numerous stored endoscopic images in a very short time with a clinically relevant diagnostic ability. It may be well applicable to daily clinical practice to reduce the burden of endoscopists.},
	language = {en},
	number = {4},
	urldate = {2019-05-29},
	journal = {Gastric Cancer},
	author = {Hirasawa, Toshiaki and Aoyama, Kazuharu and Tanimoto, Tetsuya and Ishihara, Soichiro and Shichijo, Satoki and Ozawa, Tsuyoshi and Ohnishi, Tatsuya and Fujishiro, Mitsuhiro and Matsuo, Keigo and Fujisaki, Junko and Tada, Tomohiro},
	month = jul,
	year = {2018},
	pages = {653--660},
	file = {Hirasawa et al. - 2018 - Application of artificial intelligence using a con.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ML4INE7M\\Hirasawa et al. - 2018 - Application of artificial intelligence using a con.pdf:application/pdf},
}

@inproceedings{hwang_self-transfer_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Self-{Transfer} {Learning} for {Weakly} {Supervised} {Lesion} {Localization}},
	isbn = {978-3-319-46723-8},
	abstract = {Recent advances of deep learning have achieved remarkable performances in various computer vision tasks including weakly supervised object localization. Weakly supervised object localization is practically useful since it does not require fine-grained annotations. Current approaches overcome the difficulties of weak supervision via transfer learning from pre-trained models on large-scale general images such as ImageNet. However, they cannot be utilized for medical image domain in which do not exist such priors. In this work, we present a novel weakly supervised learning framework for lesion localization named as self-transfer learning (STL). STL jointly optimizes both classification and localization networks to help the localization network focus on correct lesions without any types of priors. We evaluate STL framework over chest X-rays and mammograms, and achieve significantly better localization performance compared to previous weakly supervised localization approaches.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2016},
	publisher = {Springer International Publishing},
	author = {Hwang, Sangheum and Kim, Hyo-Eun},
	editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
	year = {2016},
	keywords = {Convolutional neural networks, Lesion localization, Weakly supervised learning},
	pages = {239--246},
}

@article{yu_artificial_2018,
	title = {Artificial intelligence in healthcare},
	volume = {2},
	issn = {2157-846X},
	url = {http://www.nature.com/articles/s41551-018-0305-z},
	doi = {10.1038/s41551-018-0305-z},
	language = {en},
	number = {10},
	urldate = {2019-05-29},
	journal = {Nature Biomedical Engineering},
	author = {Yu, Kun-Hsing and Beam, Andrew L. and Kohane, Isaac S.},
	month = oct,
	year = {2018},
	pages = {719--731},
	file = {Yu et al. - 2018 - Artificial intelligence in healthcare.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\947XIES7\\Yu et al. - 2018 - Artificial intelligence in healthcare.pdf:application/pdf},
}

@article{ribeiro_exploring_2016,
	title = {Exploring {Deep} {Learning} and {Transfer} {Learning} for {Colonic} {Polyp} {Classification}},
	volume = {2016},
	issn = {1748-670X, 1748-6718},
	url = {https://www.hindawi.com/journals/cmmm/2016/6584725/},
	doi = {10.1155/2016/6584725},
	abstract = {Recently, Deep Learning, especially through Convolutional Neural Networks (CNNs) has been widely used to enable the extraction of highly representative features. This is done among the network layers by filtering, selecting, and using these features in the last fully connected layers for pattern classification. However, CNN training for automated endoscopic image classification still provides a challenge due to the lack of large and publicly available annotated databases. In this work we explore Deep Learning for the automated classification of colonic polyps using different configurations for training CNNs from scratch (or full training) and distinct architectures of pretrained CNNs tested on 8-HD-endoscopic image databases acquired using different modalities. We compare our results with some commonly used features for colonic polyp classification and the good results suggest that features learned by CNNs trained from scratch and the “off-the-shelf” CNNs features can be highly relevant for automated classification of colonic polyps. Moreover, we also show that the combination of classical features and “off-the-shelf” CNNs features can be a good approach to further improve the results.},
	language = {en},
	urldate = {2019-05-29},
	journal = {Computational and Mathematical Methods in Medicine},
	author = {Ribeiro, Eduardo and Uhl, Andreas and Wimmer, Georg and Häfner, Michael},
	year = {2016},
	pages = {1--16},
	file = {Ribeiro et al. - 2016 - Exploring Deep Learning and Transfer Learning for .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KJPIZKG5\\Ribeiro et al. - 2016 - Exploring Deep Learning and Transfer Learning for .pdf:application/pdf},
}

@article{sahiner_deep_2019,
	title = {Deep learning in medical imaging and radiation therapy},
	volume = {46},
	issn = {2473-4209},
	url = {https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.13264},
	doi = {10.1002/mp.13264},
	abstract = {The goals of this review paper on deep learning (DL) in medical imaging and radiation therapy are to (a) summarize what has been achieved to date; (b) identify common and unique challenges, and strategies that researchers have taken to address these challenges; and (c) identify some of the promising avenues for the future both in terms of applications as well as technical innovations. We introduce the general principles of DL and convolutional neural networks, survey five major areas of application of DL in medical imaging and radiation therapy, identify common themes, discuss methods for dataset expansion, and conclude by summarizing lessons learned, remaining challenges, and future directions.},
	language = {en},
	number = {1},
	urldate = {2019-05-29},
	journal = {Medical Physics},
	author = {Sahiner, Berkman and Pezeshk, Aria and Hadjiiski, Lubomir M. and Wang, Xiaosong and Drukker, Karen and Cha, Kenny H. and Summers, Ronald M. and Giger, Maryellen L.},
	year = {2019},
	keywords = {machine learning, deep learning, computer-aided detection/characterization, reconstruction, segmentation, treatment},
	pages = {e1--e36},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\X9C7QLLC\\Sahiner et al. - 2019 - Deep learning in medical imaging and radiation the.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YJBUYNV2\\mp.html:text/html},
}

@article{topol_high-performance_2019,
	title = {High-performance medicine: the convergence of human and artificial intelligence},
	volume = {25},
	issn = {1078-8956, 1546-170X},
	shorttitle = {High-performance medicine},
	url = {http://www.nature.com/articles/s41591-018-0300-7},
	doi = {10.1038/s41591-018-0300-7},
	language = {en},
	number = {1},
	urldate = {2019-05-29},
	journal = {Nature Medicine},
	author = {Topol, Eric J.},
	month = jan,
	year = {2019},
	pages = {44--56},
	file = {Topol - 2019 - High-performance medicine the convergence of huma.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\957Q56B2\\Topol - 2019 - High-performance medicine the convergence of huma.pdf:application/pdf},
}

@article{adadi_peeking_2018,
	title = {Peeking {Inside} the {Black}-{Box}: {A} {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Peeking {Inside} the {Black}-{Box}},
	doi = {10.1109/ACCESS.2018.2870052},
	abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
	journal = {IEEE Access},
	author = {Adadi, A. and Berrada, M.},
	year = {2018},
	keywords = {artificial intelligence, Machine learning, AI-based systems, Biological system modeling, black-box models, black-box nature, Conferences, explainable AI, explainable artificial intelligence, Explainable artificial intelligence, fourth industrial revolution, interpretable machine learning, Machine learning algorithms, Market research, Prediction algorithms, XAI},
	pages = {52138--52160},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6BSCGFUK\\8466590.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\NQGUN5XM\\Adadi and Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf:application/pdf},
}

@article{adadi_gastroenterology_2019,
	title = {Gastroenterology {Meets} {Machine} {Learning}: {Status} {Quo} and {Quo} {Vadis}},
	volume = {2019},
	issn = {1687-8027, 1687-8035},
	shorttitle = {Gastroenterology {Meets} {Machine} {Learning}},
	url = {https://www.hindawi.com/journals/abi/2019/1870975/},
	doi = {10.1155/2019/1870975},
	abstract = {Machine learning has undergone a transition phase from being a pure statistical tool to being one of the main drivers of modern medicine. In gastroenterology, this technology is motivating a growing number of studies that rely on these innovative methods to deal with critical issues related to this practice. Hence, in the light of the burgeoning research on the use of machine learning in gastroenterology, a systematic review of the literature is timely. In this work, we present the results gleaned through a systematic review of prominent gastroenterology literature using machine learning techniques. Based on the analysis of 88 journal articles, we delimit the scope of application, we discuss current limitations including bias, lack of transparency, accountability, and data availability, and we put forward future avenues.},
	language = {en},
	urldate = {2019-05-29},
	journal = {Advances in Bioinformatics},
	author = {Adadi, Amina and Adadi, Safae and Berrada, Mohammed},
	month = apr,
	year = {2019},
	pages = {1--24},
	file = {Adadi et al. - 2019 - Gastroenterology Meets Machine Learning Status Qu.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\I6UQF9J5\\Adadi et al. - 2019 - Gastroenterology Meets Machine Learning Status Qu.pdf:application/pdf},
}

@article{bernal_comparative_2017,
	title = {Comparative {Validation} of {Polyp} {Detection} {Methods} in {Video} {Colonoscopy}: {Results} {From} the {MICCAI} 2015 {Endoscopic} {Vision} {Challenge}},
	volume = {36},
	issn = {0278-0062},
	shorttitle = {Comparative {Validation} of {Polyp} {Detection} {Methods} in {Video} {Colonoscopy}},
	doi = {10.1109/TMI.2017.2664042},
	abstract = {Colonoscopy is the gold standard for colon cancer screening though some polyps are still missed, thus preventing early disease detection and treatment. Several computational systems have been proposed to assist polyp detection during colonoscopy but so far without consistent evaluation. The lack of publicly available annotated databases has made it difficult to compare methods and to assess if they achieve performance levels acceptable for clinical use. The Automatic Polyp Detection sub-challenge, conducted as part of the Endoscopic Vision Challenge (http://endovis.grand-challenge.org) at the international conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) in 2015, was an effort to address this need. In this paper, we report the results of this comparative evaluation of polyp detection methods, as well as describe additional experiments to further explore differences between methods. We define performance metrics and provide evaluation databases that allow comparison of multiple methodologies. Results show that convolutional neural networks are the state of the art. Nevertheless, it is also demonstrated that combining different methodologies can lead to an improved overall performance.},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Bernal, J. and Tajkbaksh, N. and Sánchez, F. J. and Matuszewski, B. J. and Chen, H. and Yu, L. and Angermann, Q. and Romain, O. and Rustad, B. and Balasingham, I. and Pogorelov, K. and Choi, S. and Debard, Q. and Maier-Hein, L. and Speidel, S. and Stoyanov, D. and Brandao, P. and Córdova, H. and Sánchez-Montes, C. and Gurudu, S. R. and Fernández-Esparrach, G. and Dray, X. and Liang, J. and Histace, A.},
	month = jun,
	year = {2017},
	keywords = {Humans, endoscopes, neural nets, Endoscopes, machine learning, Lesions, video signal processing, AD 2015, Biomedical imaging, cancer, Cancer, Colon, colon cancer screening, Colonic Neoplasms, Colonic Polyps, Colonoscopy, convolutional neural networks, Databases, disease treatment, Early Detection of Cancer, early disease detection, Endoscopic vision, handcrafted features, important, Medical Image Computing and Computer Assisted Intervention, medical signal processing, MICCAI 2015 Endoscopic Vision Challenge, Neural Networks (Computer), polyp detection, polyp detection methods, validation framework, video colonoscopy},
	pages = {1231--1249},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\863FGTH4\\7840040.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SXG85I29\\Bernal et al. - 2017 - Comparative Validation of Polyp Detection Methods .pdf:application/pdf},
}

@techreport{fawcett_roc_2003,
	address = {Palo Alto, CA},
	type = {Technical {Report}},
	title = {{ROC} {Graphs}:  {Notes} and {Practical} {Considerations} for {Data} {Mining} {Researchers}},
	abstract = {Receiver Operating Characteristics (ROC) graphs are a useful technique for organizing classiﬁers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been increasingly adopted in the machine learning and data mining research communities. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. This article serves both as a tutorial introduction to ROC graphs and as a practical guide for using them in research.},
	language = {en},
	number = {HPL-2003–4},
	institution = {HP Laboratories},
	author = {Fawcett, Tom},
	year = {2003},
	keywords = {background, ROC},
	pages = {28},
	file = {Fawcett - ROC Graphs  Notes and Practical Considerations fo.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\HADQ994R\\Fawcett - ROC Graphs  Notes and Practical Considerations fo.pdf:application/pdf},
}

@article{rogart_narrow-band_2008,
	title = {Narrow-band imaging without high magnification to differentiate polyps during real-time colonoscopy: improvement with experience},
	volume = {68},
	issn = {00165107},
	shorttitle = {Narrow-band imaging without high magnification to differentiate polyps during real-time colonoscopy},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S001651070801780X},
	doi = {10.1016/j.gie.2008.04.035},
	abstract = {Background: There is no widely adopted, easily applied method for distinguishing between adenomatous and nonadenomatous polyps during real-time colonoscopy.
Objective: To compare white light (WL) with narrow-band imaging (NBI) for the differentiation of colorectal polyps in vivo and to assess for a learning curve. Design: A prospective polyp series. Patients and Setting: A total of 302 patients referred for colonoscopy, between August 2006 and July 2007, to a single tertiary-referral center in the United States. Intervention: Standard WL colonoscopy was performed with Olympus 180-series colonoscopes. Each detected polyp was ﬁrst characterized by WL and then by NBI. Modiﬁed Kudo pit pattern and vascular color intensity (VCI) were recorded, and the histology was predicted. Endoscopists were given feedback every 2 weeks. Main Outcome Measurements: Overall accuracy and sensitivity and speciﬁcity of endoscopic diagnosis by using WL alone and with NBI, as well as improvement in endoscopists’ performance.
Results: A total of 265 polyps were found in 131 patients. Diagnostic accuracy was 80\% with NBI and 77\% with WL (P Z .35). NBI performed better than WL in diagnosing adenomas (sensitivity 80\% vs 69\%, P ! .05). Nonadenomatous polyps were more likely to have a ‘‘light’’ VCI compared with adenomas (71\% vs 29\%, P ! .001). During the second half of the study, NBI accuracy improved, from 74\% to 87\%, and outperformed an unchanged WL accuracy of 79\% (P ! .05).
Conclusions: Overall, NBI was not more accurate than WL in differentiating colorectal polyps in vivo; however, once a learning curve was achieved, NBI performed signiﬁcantly better. Further reﬁnements of an NBI pitpattern classiﬁcation and VCI scale are needed before broad application to clinical decisions regarding the necessity of polypectomy. (Gastrointest Endosc 2008;68:1136-45.)},
	language = {en},
	number = {6},
	urldate = {2019-05-27},
	journal = {Gastrointestinal Endoscopy},
	author = {Rogart, Jason N. and Jain, Dhanpat and Siddiqui, Uzma D. and Oren, Tal and Lim, Joseph and Jamidar, Priya and Aslanian, Harry},
	month = dec,
	year = {2008},
	pages = {1136--1145},
	file = {Rogart et al. - 2008 - Narrow-band imaging without high magnification to .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DWNW2LXR\\Rogart et al. - 2008 - Narrow-band imaging without high magnification to .pdf:application/pdf},
}

@article{li_kudos_2014,
	title = {Kudo’s pit pattern classification for colorectal neoplasms: {A} meta-analysis},
	volume = {20},
	issn = {1007-9327},
	shorttitle = {Kudo’s pit pattern classification for colorectal neoplasms},
	url = {http://www.wjgnet.com/1007-9327/full/v20/i35/12649.htm},
	doi = {10.3748/wjg.v20.i35.12649},
	abstract = {AIM: To analyze the current available evidence of Kudo’ s pit pattern classification for diagnosing colorectal neoplasms.},
	language = {en},
	number = {35},
	urldate = {2019-05-27},
	journal = {World Journal of Gastroenterology},
	author = {Li, Ming},
	year = {2014},
	pages = {12649},
	file = {Li - 2014 - Kudo’s pit pattern classification for colorectal n.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\S8GUJ235\\Li - 2014 - Kudo’s pit pattern classification for colorectal n.pdf:application/pdf},
}

@article{bisschops_basic_2018,
	title = {{BASIC} ({BLI} {Adenoma} {Serrated} {International} {Classification}) classification for colorectal polyp characterization with blue light imaging},
	volume = {50},
	issn = {0013-726X, 1438-8812},
	url = {http://www.thieme-connect.de/DOI/DOI?10.1055/s-0043-121570},
	doi = {10.1055/s-0043-121570},
	abstract = {Backgound and study aim Advanced endoscopic imaging has revolutionized the characterization of lesions during colonoscopy. The aim of this study was to create a new classification for differentiating subcentimetric hyperplastic and adenomatous polyps, and deeply invasive malignant lesions using blue-light imaging (BLI) with high definition, with and without optical magnification, as well as to assess its interobserver concordance.},
	language = {en},
	number = {03},
	urldate = {2019-05-27},
	journal = {Endoscopy},
	author = {Bisschops, Raf and Hassan, Cesare and Bhandari, Pradeep and Coron, Emmanuel and Neumann, Helmut and Pech, Oliver and Correale, Loredana and Repici, Alessandro},
	month = mar,
	year = {2018},
	pages = {211--220},
	file = {Bisschops et al. - 2018 - BASIC (BLI Adenoma Serrated International Classifi.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UMHR2YFC\\Bisschops et al. - 2018 - BASIC (BLI Adenoma Serrated International Classifi.pdf:application/pdf},
}

@misc{noauthor_endoscopy_nodate,
	title = {Endoscopy {Campus} - {WASP} classification - optical diagnosis of polyps},
	url = {https://www.endoscopy-campus.com/klassifikationen/wasp-classification-optical-diagnosis-of-polyps/},
	abstract = {Recently sessile serrated lesions (SSLs) have been recognized as another important precursor lesion to CRC. SSLs are thought be responsible for 15–30\% of colorectal cancer. SSL is also often referred to as sessile serrated polyp (SSP), sessile serrated adenoma (SSA) or SSA/P.},
	language = {de-DE},
	urldate = {2019-05-27},
	journal = {Endoscopy Campus},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\54Z4YETM\\wasp-classification-optical-diagnosis-of-polyps.html:text/html},
}

@article{ijspeert_development_2016,
	title = {Development and validation of the {WASP} classification system for optical diagnosis of adenomas, hyperplastic polyps and sessile serrated adenomas/polyps},
	volume = {65},
	issn = {0017-5749, 1468-3288},
	url = {http://gut.bmj.com/lookup/doi/10.1136/gutjnl-2014-308411},
	doi = {10.1136/gutjnl-2014-308411},
	abstract = {Objective Accurate endoscopic differentiation would enable to resect and discard small and diminutive colonic lesions, thereby increasing cost-efﬁciency. Current classiﬁcation systems based on narrow band imaging (NBI), however, do not include neoplastic sessile serrated adenomas/polyps (SSA/Ps). We aimed to develop and validate a new classiﬁcation system for endoscopic differentiation of adenomas, hyperplastic polyps and SSA/Ps {\textless}10 mm. Design We developed the Workgroup serrAted polypS and Polyposis (WASP) classiﬁcation, combining the NBI International Colorectal Endoscopic classiﬁcation and criteria for differentiation of SSA/Ps in a stepwise approach. Ten consultant gastroenterologists predicted polyp histology, including levels of conﬁdence, based on the endoscopic aspect of 45 polyps, before and after participation in training in the WASP classiﬁcation. After 6 months, the same endoscopists predicted polyp histology of a new set of 50 polyps, with a ratio of lesions comparable to daily practice.
Results The accuracy of optical diagnosis was 0.63 (95\% CI 0.54 to 0.71) at baseline, which improved to 0.79 (95\% CI 0.72 to 0.86, p{\textless}0.001) after training. For polyps diagnosed with high conﬁdence the accuracy was 0.73 (95\% CI 0.64 to 0.82), which improved to 0.87 (95\% CI 0.80 to 0.95, p{\textless}0.01). The accuracy of optical diagnosis after 6 months was 0.76 (95\% CI 0.72 to 0.80), increasing to 0.84 (95\% CI 0.81 to 0.88) considering high conﬁdence diagnosis. The combined negative predictive value with high conﬁdence of diminutive neoplastic lesions (adenomas and SSA/Ps together) was 0.91 (95\% CI 0.83 to 0.96).
Conclusions We developed and validated the ﬁrst integrative classiﬁcation method for endoscopic differentiation of small and diminutive adenomas, hyperplastic polyps and SSA/Ps. In a still image evaluation setting, introduction of the WASP classiﬁcation signiﬁcantly improved the accuracy of optical diagnosis overall as well as SSA/P in particular, which proved to be sustainable after 6 months.},
	language = {en},
	number = {6},
	urldate = {2019-05-27},
	journal = {Gut},
	author = {IJspeert, Joep E G and Bastiaansen, Barbara A J and van Leerdam, Monique E and Meijer, Gerrit A and van Eeden, Susanne and Sanduleanu, Silvia and Schoon, Erik J and Bisseling, Tanya M and Spaander, Manon CW and van Lelyveld, Niels and Bargeman, Marloes and Wang, Junfeng and Dekker, Evelien and {Dutch Workgroup serrAted polypS \& Polyposis (WASP)}},
	month = jun,
	year = {2016},
	pages = {963--970},
	file = {IJspeert et al. - 2016 - Development and validation of the WASP classificat.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\B8DN7HC7\\IJspeert et al. - 2016 - Development and validation of the WASP classificat.pdf:application/pdf},
}

@article{hayashi_endoscopic_2013,
	title = {Endoscopic prediction of deep submucosal invasive carcinoma: validation of the {Narrow}-{Band} {Imaging} {International} {Colorectal} {Endoscopic} ({NICE}) classification},
	volume = {78},
	issn = {00165107},
	shorttitle = {Endoscopic prediction of deep submucosal invasive carcinoma},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0016510713018531},
	doi = {10.1016/j.gie.2013.04.185},
	abstract = {Background: A simple endoscopic classiﬁcation to accurately predict deep submucosal invasive (SM-d) carcinoma would be clinically useful.
Objective: To develop and assess the validity of the NBI international colorectal endoscopic (NICE) classiﬁcation for the characterization of SM-d carcinoma. Design: The study was conducted in 4 phases: (1) evaluation of endoscopic differentiation by NBI-experienced colonoscopists; (2) extension of the NICE classiﬁcation to incorporate SM-d (type 3) by using a modiﬁed Delphi method; (3) prospective validation of the individual criteria by inexperienced participants, by using highdeﬁnition still images without magniﬁcation of known histology; and (4) prospective validation of the individual criteria and overall classiﬁcation by inexperienced participants after training. Setting: Japanese academic unit. Main Outcome Measurements: Performance characteristics of the NICE criteria (phase 3) and overall classiﬁcation (phase 4) for SM-d carcinoma; sensitivity, speciﬁcity, predictive values, and accuracy.
Results: We expanded the NICE classiﬁcation for the endoscopic diagnosis of SM-d carcinoma (type 3) and established the predictive validity of its individual components. The negative predictive values of the individual criteria for diagnosis of SM-d carcinoma were 76.2\% (color), 88.5\% (vessels), and 79.1\% (surface pattern). When any 1 of the 3 SM-d criteria was present, the sensitivity was 94.9\%, and the negative predictive value was 95.9\%. The overall sensitivity and negative predictive value of a global, high-conﬁdence prediction of SM-d carcinoma was 92\%. Interobserver agreement for an overall SM-d carcinoma prediction was substantial (kappa 0.70). Limitations: Single Japanese center, use of still images without prospective clinical evaluation.
Conclusion: The NICE classiﬁcation is a valid tool for predicting SM-d carcinomas in colorectal tumors. (Gastrointest Endosc 2013;78:625-32.)},
	language = {en},
	number = {4},
	urldate = {2019-05-27},
	journal = {Gastrointestinal Endoscopy},
	author = {Hayashi, Nana and Tanaka, Shinji and Hewett, David G. and Kaltenbach, Tonya R. and Sano, Yasushi and Ponchon, Thierry and Saunders, Brian P. and Rex, Douglas K. and Soetikno, Roy M.},
	month = oct,
	year = {2013},
	pages = {625--632},
	file = {Hayashi et al. - 2013 - Endoscopic prediction of deep submucosal invasive .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\D2QVREGL\\Hayashi et al. - 2013 - Endoscopic prediction of deep submucosal invasive .pdf:application/pdf},
}

@article{sumimoto_diagnostic_2017,
	title = {Diagnostic performance of {Japan} {NBI} {Expert} {Team} classification for differentiation among noninvasive, superficially invasive, and deeply invasive colorectal neoplasia},
	volume = {86},
	issn = {00165107},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0016510717301207},
	doi = {10.1016/j.gie.2017.02.018},
	abstract = {Backgrounds and Aims: The Japan NBI Expert Team (JNET) classiﬁcation is the ﬁrst universal narrow-band imaging magnifying endoscopic classiﬁcation of colorectal tumors. Considering each type in this classiﬁcation, the diagnostic ability of Type 2B is the weakest. Generally, clinical behavior is believed to be different in each gross type of colorectal tumor. We evaluated the differences in the diagnostic performance of JNET classiﬁcation for each gross type (polypoid and superﬁcial) and examined whether the diagnostic performance of Type 2B could be improved by subtyping.
Methods: We analyzed 2933 consecutive cases of colorectal lesions, including 136 hyperplastic polyps/sessile serrated polyps, 1926 low-grade dysplasias (LGDs), 571 high-grade dysplasias (HGDs), and 300 submucosal (SM) carcinomas. We classiﬁed lesions as polypoid and superﬁcial type and compared the diagnostic performance of the classiﬁcation system in each type. Additionally, we subtyped Type 2B into 2B-low and 2B-high based on the level of irregularity in surface and vessel patterns, and we evaluated the relationship between the subtypes and histology, as analyzed separately for polypoid and superﬁcial types. We also estimated interobserver and intraobserver variability.
Results: The diagnostic performance of JNET classiﬁcation did not differ signiﬁcantly between polypoid and superﬁcial lesions. Ninety-nine percent of Type 2B-low lesions were LGDs, HGDs, or superﬁcial submucosal invasive (SM-s) carcinomas. In contrast, 60\% of Type 2B-high lesions were deep submucosal invasive (SM-d) carcinomas. The results were not different between each gross type. Interobserver and intraobserver agreements for Type 2B subtyping were good, with kappa values of .743 and .786, respectively.
Conclusions: Type 2B subtyping may be useful for identifying lesions that are appropriate for endoscopic resection. JNET classiﬁcation and Type 2B sub classiﬁcation are useful criteria, regardless of gross type. (Gastrointest Endosc 2017;86:700-9.)},
	language = {en},
	number = {4},
	urldate = {2019-05-27},
	journal = {Gastrointestinal Endoscopy},
	author = {Sumimoto, Kyoku and Tanaka, Shinji and Shigita, Kenjiro and Hayashi, Nana and Hirano, Daiki and Tamaru, Yuzuru and Ninomiya, Yuki and Oka, Shiro and Arihiro, Koji and Shimamoto, Fumio and Yoshihara, Masaharu and Chayama, Kazuaki},
	month = oct,
	year = {2017},
	pages = {700--709},
	file = {Sumimoto et al. - 2017 - Diagnostic performance of Japan NBI Expert Team cl.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UABG94VX\\Sumimoto et al. - 2017 - Diagnostic performance of Japan NBI Expert Team cl.pdf:application/pdf},
}

@article{rex_serrated_2012,
	title = {Serrated {Lesions} of the {Colorectum}: {Review} and {Recommendations} {From} an {Expert} {Panel}:},
	volume = {107},
	issn = {0002-9270},
	shorttitle = {Serrated {Lesions} of the {Colorectum}},
	url = {http://Insights.ovid.com/crossref?an=00000434-201209000-00011},
	doi = {10.1038/ajg.2012.161},
	abstract = {Serrated lesions of the colorectum are the precursors of perhaps one-third of colorectal cancers. Cancers arising in serrated lesions are usually in the proximal colon, and account for a disproportionate fraction of cancer identified after colonoscopy.},
	language = {en},
	number = {9},
	urldate = {2019-05-27},
	journal = {American Journal of Gastroenterology},
	author = {Rex, Douglas K and Ahnen, Dennis J and Baron, John A and Batts, Kenneth P and Burke, Carol A and Burt, Randall W and Goldblum, John R and Guillem, José G and Kahi, Charles J and Kalady, Matthew F and O′Brien, Michael J and Odze, Robert D and Ogino, Shuji and Parry, Susan and Snover, Dale C and Torlakovic, Emina Emilia and Wise, Paul E and Young, Joanne and Church, James},
	month = sep,
	year = {2012},
	pages = {1315--1329},
	file = {Rex et al. - 2012 - Serrated Lesions of the Colorectum Review and Rec.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PFWL3FRT\\Rex et al. - 2012 - Serrated Lesions of the Colorectum Review and Rec.pdf:application/pdf},
}

@article{kudo_diagnosis_1996,
	title = {Diagnosis of colorectal tumorous lesions by magnifying endoscopy},
	volume = {44},
	issn = {00165107},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0016510796702225},
	doi = {10.1016/S0016-5107(96)70222-5},
	abstract = {Background: The magnifying colonoscope allows 100-fold magnified viewing of the colonic surface.
Methods: We examined 2050 colorectal tumorous lesions by magnifying endoscopy, stereomicroscopy, and histopathology and classified these lesions according to pit pattern. Based on stereomicroscopy, lesions with a type 1 or 2 pit pattern were nontumors, whereas lesions with types 3s, 3L, 4, and/or 5 pit patterns were neoplastic tumors.
Results: The pit patterns observed by magnifying endoscopy were fundamentally similar to those demonstrated in stereomicroscopic images. When the diagnosis by magnifying endoscopy was compared with the stereomicroscopic diagnosis, there was agreement in 1130 of 1387 lesions (81.5\%). True neoplasms could be differentiated from non-neoplastic lesions. Of lesions with a type 5 pit pattern with a bounded surface, 11 of 22 (50\%) were found to be invasive cancers with involvement of the submucosal layer. If this pit pattern is found to involve a relatively broad area of the mucosal surface, extensive malignant invasion (sm-massive) should be strongly suspected.
Conclusions:The magnifying colonoscope provides an accurate instantaneous assessment of the histology of colorectal tumorous lesions. This may help in decision making during colonoscopy. (Gastrointest Endosc 1996;44:8-14.)},
	language = {en},
	number = {1},
	urldate = {2019-05-27},
	journal = {Gastrointestinal Endoscopy},
	author = {Kudo, Shin-ei and Tamura, Satoru and Nakajima, Takashi and Yamano, Hiro-o and Kusaka, Hisashi and Watanabe, Hidenobu},
	month = jul,
	year = {1996},
	pages = {8--14},
	file = {Kudo et al. - 1996 - Diagnosis of colorectal tumorous lesions by magnif.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\R6XZ8GLI\\Kudo et al. - 1996 - Diagnosis of colorectal tumorous lesions by magnif.pdf:application/pdf},
}

@incollection{kok_polyp_2007,
	address = {Berlin, Heidelberg},
	title = {Polyp {Detection} in {Endoscopic} {Video} {Using} {SVMs}},
	volume = {4702},
	isbn = {978-3-540-74975-2 978-3-540-74976-9},
	url = {http://link.springer.com/10.1007/978-3-540-74976-9_34},
	abstract = {Colon cancer is one of the most common cancers in developed countries. Most of these cancers start with a polyp. Polyps are easily detected by physicians. Our goal is to mimic this detection ability so that endoscopic videos can be pre-scanned with our algorithm before the physician analyses them. The method will indicate which part of the video needs attention (polyps were detected there) and hence can speedup the procedures. In this paper we present a method for polyp detection in endoscopic images that uses SVM for classiﬁcation. Our experiments yielded a result of 93.16 ± 0.09\% of area under the Receiver Operating Characteristic (ROC) curve on a database of 4620 images indicating that the approach proposed is well suited to the detection of polyps in endoscopic video.},
	language = {en},
	urldate = {2019-05-27},
	booktitle = {Knowledge {Discovery} in {Databases}: {PKDD} 2007},
	publisher = {Springer Berlin Heidelberg},
	author = {Alexandre, Luís A. and Casteleiro, João and Nobreinst, Nuno},
	editor = {Kok, Joost N. and Koronacki, Jacek and Lopez de Mantaras, Ramon and Matwin, Stan and Mladenič, Dunja and Skowron, Andrzej},
	year = {2007},
	doi = {10.1007/978-3-540-74976-9_34},
	pages = {358--365},
	file = {Alexandre et al. - 2007 - Polyp Detection in Endoscopic Video Using SVMs.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\E748UEP7\\Alexandre et al. - 2007 - Polyp Detection in Endoscopic Video Using SVMs.pdf:application/pdf},
}

@article{sanchez-montes_computer-aided_2019,
	title = {Computer-aided prediction of polyp histology on white light colonoscopy using surface pattern analysis},
	volume = {51},
	issn = {0013-726X, 1438-8812},
	url = {http://www.thieme-connect.de/DOI/DOI?10.1055/a-0732-5250},
	doi = {10.1055/a-0732-5250},
	abstract = {Background This study aimed to evaluate a new computational histology prediction system based on colorectal polyp textural surface patterns using high definition white light images.
Methods Textural elements (textons) were characterized according to their contrast with respect to the surface, shape, and number of bifurcations, assuming that dysplastic polyps are associated with highly contrasted, large tubular patterns with some degree of bifurcation. Computer-aided diagnosis (CAD) was compared with pathological diagnosis and the diagnosis made by endoscopists using Kudo and Narrow-Band Imaging International Colorectal Endoscopic classifications.
Results Images of 225 polyps were evaluated (142 dysplastic and 83 nondysplastic). The CAD system correctly classified 205 polyps (91.1 \%): 131/142 dysplastic (92.3 \%) and 74/83 (89.2 \%) nondysplastic. For the subgroup of 100 diminutive polyps (≤ 5 mm), CAD correctly classified 87 polyps (87.0 \%): 43/50 (86.0 \%) dysplastic and 44/50 (88.0 \%) nondysplastic. There were no statistically significant differences in polyp histology prediction between the CAD system and endoscopist assessment.
Conclusion A computer vision system based on the characterization of the polyp surface in white light accurately predicted colorectal polyp histology.},
	language = {en},
	number = {03},
	urldate = {2019-05-27},
	journal = {Endoscopy},
	author = {Sánchez-Montes, Cristina and Sánchez, Francisco and Bernal, Jorge and Córdova, Henry and López-Cerón, María and Cuatrecasas, Miriam and Rodríguez de Miguel, Cristina and García-Rodríguez, Ana and Garcés-Durán, Rodrigo and Pellisé, María and Llach, Josep and Fernández-Esparrach, Glòria},
	month = mar,
	year = {2019},
	pages = {261--265},
	file = {Sánchez-Montes et al. - 2019 - Computer-aided prediction of polyp histology on wh.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\P5BPN5YV\\Sánchez-Montes et al. - 2019 - Computer-aided prediction of polyp histology on wh.pdf:application/pdf},
}

@article{sanchez-montes_computer-aided_2019-1,
	title = {Computer-aided prediction of polyp histology on white light colonoscopy using surface pattern analysis},
	volume = {51},
	issn = {0013-726X, 1438-8812},
	url = {http://www.thieme-connect.de/DOI/DOI?10.1055/a-0732-5250},
	doi = {10.1055/a-0732-5250},
	abstract = {Abstract
            Background This study aimed to evaluate a new computational histology prediction system based on colorectal polyp textural surface patterns using high definition white light images.
            Methods Textural elements (textons) were characterized according to their contrast with respect to the surface, shape, and number of bifurcations, assuming that dysplastic polyps are associated with highly contrasted, large tubular patterns with some degree of bifurcation. Computer-aided diagnosis (CAD) was compared with pathological diagnosis and the diagnosis made by endoscopists using Kudo and Narrow-Band Imaging International Colorectal Endoscopic classifications.
            Results Images of 225 polyps were evaluated (142 dysplastic and 83 nondysplastic). The CAD system correctly classified 205 polyps (91.1 \%): 131/142 dysplastic (92.3 \%) and 74/83 (89.2 \%) nondysplastic. For the subgroup of 100 diminutive polyps (≤ 5 mm), CAD correctly classified 87 polyps (87.0 \%): 43/50 (86.0 \%) dysplastic and 44/50 (88.0 \%) nondysplastic. There were no statistically significant differences in polyp histology prediction between the CAD system and endoscopist assessment.
            Conclusion A computer vision system based on the characterization of the polyp surface in white light accurately predicted colorectal polyp histology.},
	language = {en},
	number = {03},
	urldate = {2019-05-27},
	journal = {Endoscopy},
	author = {Sánchez-Montes, Cristina and Sánchez, Francisco and Bernal, Jorge and Córdova, Henry and López-Cerón, María and Cuatrecasas, Miriam and Rodríguez de Miguel, Cristina and García-Rodríguez, Ana and Garcés-Durán, Rodrigo and Pellisé, María and Llach, Josep and Fernández-Esparrach, Glòria},
	month = mar,
	year = {2019},
	pages = {261--265},
}

@inproceedings{iakovidis_comparative_2005,
	title = {A comparative study of texture features for the discrimination of gastric polyps in endoscopic video},
	doi = {10.1109/CBMS.2005.6},
	abstract = {In this paper, we extend the application of four texture feature extraction methods proposed for the detection of colorectal lesions, into the discrimination of gastric polyps in endoscopic video. Support Vector Machines have been utilized for the texture classification task. The polyp discrimination performance of the surveyed schemes is compared by means of Receiver Operating Characteristics (ROC). The results advocate the feasibility of a computer-based system for polyp detection in video gastroscopy that exploits the textural characteristics of the gastric mucosa in conjunction with its color appearance.},
	booktitle = {18th {IEEE} {Symposium} on {Computer}-{Based} {Medical} {Systems} ({CBMS}'05)},
	author = {Iakovidis, D. K. and Maroulis, D. E. and Karkanis, S. A. and Brokos, A.},
	month = jun,
	year = {2005},
	keywords = {Image edge detection, feature extraction, important, ROC, colorectal lesion, endoscopic video, gastric polyps, good introduction, good summary feature extraction methods, SVM, texture},
	pages = {575--580},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TEQL8V4U\\1467755.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\F94CJE8V\\Iakovidis et al. - 2005 - A comparative study of texture features for the di.pdf:application/pdf},
}

@article{moccia_computer-assisted_2018,
	title = {Computer-assisted liver graft steatosis assessment via learning-based texture analysis},
	volume = {13},
	issn = {1861-6410, 1861-6429},
	url = {http://link.springer.com/10.1007/s11548-018-1787-6},
	doi = {10.1007/s11548-018-1787-6},
	abstract = {Purpose Fast and accurate graft hepatic steatosis (HS) assessment is of primary importance for lowering liver dysfunction risks after transplantation. Histopathological analysis of biopsied liver is the gold standard for assessing HS, despite being invasive and time consuming. Due to the short time availability between liver procurement and transplantation, surgeons perform HS assessment through clinical evaluation (medical history, blood tests) and liver texture visual analysis. Despite visual analysis being recognized as challenging in the clinical literature, few efforts have been invested to develop computerassisted solutions for HS assessment. The objective of this paper is to investigate the automatic analysis of liver texture with machine learning algorithms to automate the HS assessment process and offer support for the surgeon decision process.
Methods Forty RGB images of forty different donors were analyzed. The images were captured with an RGB smartphone camera in the operating room (OR). Twenty images refer to livers that were accepted and 20 to discarded livers. Fifteen randomly selected liver patches were extracted from each image. Patch size was 100 × 100. This way, a balanced dataset of 600 patches was obtained. Intensity-based features (INT), histogram of local binary pattern (HLBPriu2 ), and gray-level cooccurrence matrix (FGLCM) were investigated. Blood-sample features (Blo) were included in the analysis, too. Supervised and semisupervised learning approaches were investigated for feature classiﬁcation. The leave-one-patient-out cross-validation was performed to estimate the classiﬁcation performance.
Results With the best-performing feature set (HLBPriu2 +INT+Blo) and semisupervised learning, the achieved classiﬁcation sensitivity, speciﬁcity, and accuracy were 95, 81, and 88\%, respectively.
Conclusions This research represents the ﬁrst attempt to use machine learning and automatic texture analysis of RGB images from ubiquitous smartphone cameras for the task of graft HS assessment. The results suggest that is a promising strategy to develop a fully automatic solution to assist surgeons in HS assessment inside the OR.},
	language = {en},
	number = {9},
	urldate = {2019-05-16},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Moccia, Sara and Mattos, Leonardo S. and Patrini, Ilaria and Ruperti, Michela and Poté, Nicolas and Dondero, Federica and Cauchy, François and Sepulveda, Ailton and Soubrane, Olivier and De Momi, Elena and Diaspro, Alberto and Cesaretti, Manuela},
	month = sep,
	year = {2018},
	keywords = {ROC, SVM, texture, GLCM, hepatic steatosis, INT, LBP, liver, MIL, ML, RF, RGB, semisupervised learning, SIL, supervised learning, transplantation},
	pages = {1357--1367},
	file = {Moccia et al. - 2018 - Computer-assisted liver graft steatosis assessment.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4RWY9B5L\\Moccia et al. - 2018 - Computer-assisted liver graft steatosis assessment.pdf:application/pdf},
}

@inproceedings{alexandre_color_2008,
	address = {Sanya, China},
	title = {Color and {Position} versus {Texture} {Features} for {Endoscopic} {Polyp} {Detection}},
	isbn = {978-0-7695-3118-2},
	url = {http://ieeexplore.ieee.org/document/4549131/},
	doi = {10.1109/BMEI.2008.246},
	abstract = {This paper presents a comparison of texture based and color and position based methods for polyp detection in endoscopic video images. Two methods for texture feature extraction that presented good results in previous studies were implemented and their performance is compared against a simple combination of color and position features. Although this more simple approach produces a much higher number of features than the other approaches, a SVM with a RBF kernel is able to deal with this high dimensional input space and it turns out that it outperforms the previous approaches on the experiments performed in a database of 4620 images from endoscopic video.},
	language = {en},
	urldate = {2019-05-23},
	booktitle = {2008 {International} {Conference} on {BioMedical} {Engineering} and {Informatics}},
	publisher = {IEEE},
	author = {Alexandre, Lu and Nobre, Nuno and Casteleiro, Jo},
	month = may,
	year = {2008},
	pages = {38--42},
	file = {Alexandre et al. - 2008 - Color and Position versus Texture Features for End.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RUMYV5WQ\\Alexandre et al. - 2008 - Color and Position versus Texture Features for End.pdf:application/pdf},
}

@incollection{kok_polyp_2007-1,
	address = {Berlin, Heidelberg},
	title = {Polyp {Detection} in {Endoscopic} {Video} {Using} {SVMs}},
	volume = {4702},
	isbn = {978-3-540-74975-2 978-3-540-74976-9},
	url = {http://link.springer.com/10.1007/978-3-540-74976-9_34},
	abstract = {Colon cancer is one of the most common cancers in developed countries. Most of these cancers start with a polyp. Polyps are easily detected by physicians. Our goal is to mimic this detection ability so that endoscopic videos can be pre-scanned with our algorithm before the physician analyses them. The method will indicate which part of the video needs attention (polyps were detected there) and hence can speedup the procedures. In this paper we present a method for polyp detection in endoscopic images that uses SVM for classiﬁcation. Our experiments yielded a result of 93.16 ± 0.09\% of area under the Receiver Operating Characteristic (ROC) curve on a database of 4620 images indicating that the approach proposed is well suited to the detection of polyps in endoscopic video.},
	language = {en},
	urldate = {2019-05-23},
	booktitle = {Knowledge {Discovery} in {Databases}: {PKDD} 2007},
	publisher = {Springer Berlin Heidelberg},
	author = {Alexandre, Luís A. and Casteleiro, João and Nobreinst, Nuno},
	editor = {Kok, Joost N. and Koronacki, Jacek and Lopez de Mantaras, Ramon and Matwin, Stan and Mladenič, Dunja and Skowron, Andrzej},
	year = {2007},
	doi = {10.1007/978-3-540-74976-9_34},
	pages = {358--365},
	file = {Alexandre et al. - 2007 - Polyp Detection in Endoscopic Video Using SVMs.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2QIG9RYV\\Alexandre et al. - 2007 - Polyp Detection in Endoscopic Video Using SVMs.pdf:application/pdf},
}

@article{inoue_paris_2003,
	title = {The {Paris} endoscopic classification of superficial neoplastic lesions: esophagus, stomach, and colon},
	volume = {58},
	issn = {00165107},
	shorttitle = {The {Paris} endoscopic classification of superficial neoplastic lesions},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S001651070302159X},
	doi = {10.1016/S0016-5107(03)02159-X},
	language = {en},
	number = {6},
	urldate = {2019-05-23},
	journal = {Gastrointestinal Endoscopy},
	author = {Inoue, H. and Kashida, H. and Kudo, S. and Sasako, M. and Shimoda, T. and Watanabe, H. and Yoshida, S. and Guelrud, M. and Lightdale, C. and Wang, K.},
	month = dec,
	year = {2003},
	pages = {S3--S43},
	file = {Participants in the Paris Workshop - 2003 - The Paris endoscopic classification of superficial.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MWASBZLI\\Participants in the Paris Workshop - 2003 - The Paris endoscopic classification of superficial.pdf:application/pdf},
}

@article{endoscopic_classification_review_group_update_2005,
	title = {Update on the {Paris} {Classification} of {Superficial} {Neoplastic} {Lesions} in the {Digestive} {Tract}},
	volume = {37},
	issn = {0013-726X, 1438-8812},
	url = {http://www.thieme-connect.de/DOI/DOI?10.1055/s-2005-861352},
	doi = {10.1055/s-2005-861352},
	language = {en},
	number = {6},
	urldate = {2019-05-23},
	journal = {Endoscopy},
	author = {{Endoscopic Classification Review Group}},
	month = jun,
	year = {2005},
	pages = {570--578},
	file = {Endoscopic Classification Review Group - 2005 - Update on the Paris Classification of Superficial .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ZXRUG29V\\Endoscopic Classification Review Group - 2005 - Update on the Paris Classification of Superficial .pdf:application/pdf},
}

@article{laine_scenic_2015,
	title = {{SCENIC} international consensus statement on surveillance and management of dysplasia in inflammatory bowel disease},
	volume = {81},
	issn = {00165107},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0016510714025784},
	doi = {10.1016/j.gie.2014.12.009},
	language = {en},
	number = {3},
	urldate = {2019-05-23},
	journal = {Gastrointestinal Endoscopy},
	author = {Laine, Loren and Kaltenbach, Tonya and Barkun, Alan and McQuaid, Kenneth R. and Subramanian, Venkataraman and Soetikno, Roy and East, James E. and Farraye, Francis A. and Feagan, Brian and Ioannidis, John and Kiesslich, Ralf and Krier, Michael and Matsumoto, Takayuki and McCabe, Robert P. and Mönkemüller, Klaus and Odze, Robert and Picco, Michael and Rubin, David T. and Rubin, Michele and Rubio, Carlos A. and Rutter, Matthew D. and Sanchez-Yague, Andres and Sanduleanu, Silvia and Shergill, Amandeep and Ullman, Thomas and Velayos, Fernando and Yakich, Douglas and Yang, Yu-Xiao},
	month = mar,
	year = {2015},
	keywords = {dysplasia classification, IBD},
	pages = {489--501.e26},
	file = {Laine et al. - 2015 - SCENIC international consensus statement on survei.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QHD7Z37K\\Laine et al. - 2015 - SCENIC international consensus statement on survei.pdf:application/pdf},
}

@article{vemuri_survey_2019,
	title = {Survey of {Computer} {Vision} and {Machine} {Learning} in {Gastrointestinal} {Endoscopy}},
	url = {http://arxiv.org/abs/1904.13307},
	abstract = {This paper attempts to provide the reader a place to begin studying the application of computer vision and machine learning to gastrointestinal (GI) endoscopy. They have been classified into 18 categories. It should be be noted by the reader that this is a review from pre-deep learning era. A lot of deep learning based applications have not been covered in this thesis.},
	language = {en},
	urldate = {2019-05-22},
	journal = {arXiv:1904.13307 [physics]},
	author = {Vemuri, Anant S.},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.13307},
	keywords = {polyp detection, GI endoscopy, oesophagus, polyp classification},
	file = {Vemuri - 2019 - Survey of Computer Vision and Machine Learning in .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PHW2WSTR\\Vemuri - 2019 - Survey of Computer Vision and Machine Learning in .pdf:application/pdf},
}

@phdthesis{vemuri_inter-operative_nodate,
	title = {Inter-operative biopsy site relocalization in gastroscopy: application to oesophagus},
	language = {en},
	author = {Vemuri, Anant Suraj},
	file = {Vemuri - Inter-operative biopsy site relocalization in gast.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\F85F7PIW\\Vemuri - Inter-operative biopsy site relocalization in gast.pdf:application/pdf},
}

@inproceedings{shen_lesion_2012,
	title = {Lesion detection of electronic gastroscope images based on multiscale texture feature},
	doi = {10.1109/ICSPCC.2012.6335638},
	abstract = {Electronic gastroscope has been playing an important role in the examination of gastrointestinal tract. However, due to its great dependence on the doctor's experience and skills, the rate of misdiagnosis is still high. Therefore, an automatic lesion detection system is a huge help for doctors. In this paper, we design a new scheme for gastroscopic image lesion detection. Two new multiscale texture features are utilized and compared which combine contourlet transform with gray level co-occurrence matrix (GLCM) and local binary pattern (LBP) respectively. Combined with color feature and with AdaBoost as a classifier, experiments show that it is promising to utilize the proposed scheme to detect lesions in gastroscopic images. The best performance comes from the combination of color feature and contourlet based local binary pattern feature with false negative rate of 11.94\%, false positive rate of 16.10\%, and error rate of 13.99\%.},
	booktitle = {2012 {IEEE} {International} {Conference} on {Signal} {Processing}, {Communication} and {Computing} ({ICSPCC} 2012)},
	author = {Shen, X. and Sun, K. and Zhang, S. and Cheng, S.},
	month = aug,
	year = {2012},
	keywords = {Image color analysis, image colour analysis, medical image processing, Endoscopes, feature extraction, Feature extraction, image classification, Lesions, Cancer, texture, GLCM, LBP, AdaBoost, AdaBoost classifier, automatic lesion detection system, color feature, contourlet, contourlet based local binary pattern feature, contourlet transform, electronic gastroscope images, gastrointestinal tract examination, gastroscopic image lesion detection, gray level cooccurrence matrix, image texture, lesion detection, local binary pattern, multiscale feature, multiscale texture feature, Wavelet transforms},
	pages = {756--759},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\D8N3B75V\\6335638.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TF5UDEMC\\Shen et al. - 2012 - Lesion detection of electronic gastroscope images .pdf:application/pdf},
}

@article{li_texture_2009,
	title = {Texture analysis for ulcer detection in capsule endoscopy images},
	volume = {27},
	issn = {02628856},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0262885609000080},
	doi = {10.1016/j.imavis.2008.12.003},
	abstract = {Capsule endoscopy (CE) has gradually seen its wide application in hospitals in the last few years because it can view the entire small bowel without invasiveness. However, CE produces too many images each time, thus causing a huge burden to physicians, so it is meaningful to help clinicians if we can employ computerized methods to diagnose. This paper presents a new texture extraction scheme for ulcer region discrimination in CE images. A new idea of curvelet based local binary pattern is proposed as textural features to distinguish ulcer regions from normal regions, which makes full use of curvelet transformation and local binary pattern. The proposed new textural features can capture multi-directional features and show robustness to illumination changes. Extensive classiﬁcation experiments using multilayer perceptron neural network and support vector machines on our image data validate that it is promising to employ the proposed texture features to recognize ulcer regions in CE images.},
	language = {en},
	number = {9},
	urldate = {2019-05-22},
	journal = {Image and Vision Computing},
	author = {Li, Baopu and Meng, Max Q.-H.},
	month = aug,
	year = {2009},
	pages = {1336--1342},
	file = {Li and Meng - 2009 - Texture analysis for ulcer detection in capsule en.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CQZE29I5\\Li and Meng - 2009 - Texture analysis for ulcer detection in capsule en.pdf:application/pdf},
}

@incollection{magjarevic_ulcer_2010,
	address = {Berlin, Heidelberg},
	title = {Ulcer {Detection} in {Wireless} {Capsule} {Endoscopy} {Images} {Using} {Bidimensional} {Nonlinear} {Analysis}},
	volume = {29},
	isbn = {978-3-642-13038-0 978-3-642-13039-7},
	url = {http://link.springer.com/10.1007/978-3-642-13039-7_59},
	abstract = {Wireless Capsule Endoscopy (WCE) constitutes a recent technological breakthrough that enables the observation of the gastrointestinal tract (GT) and especially the entire small bowel in a non-invasive way compared to the traditional imaging techniques. WCE allows a detailed inspection of the intestine and identification of its clinical lesions. However, the main drawback of this method is the time consuming task of reviewing the vast amount of images produced. To address this, a novel technique for discriminating abnormal endoscopic images related to ulcer, the most common disease of GT, is presented here. Towards this direction, the Bidimensional Ensemble Empirical Mode Decomposition (BEEMD) was applied to images of the small bowel acquired by a WCE system in order to extract their Intrinsic Mode Functions (IMFs). The IMFs reveal differences in structure from their finest to their coarsest scale providing a new analysis domain. Additionally, lacunarity analysis was employed as a method to quantify and extract the texture patterns of the images and differentiate the ulcer from the healthy regions. Experimental results demonstrated promising classification accuracy ({\textgreater}90\%), exhibiting a high potential towards WCE analysis.},
	language = {en},
	urldate = {2019-05-22},
	booktitle = {{XII} {Mediterranean} {Conference} on {Medical} and {Biological} {Engineering} and {Computing} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Charisis, Vasileios and Tsiligiri, Alexandra and Hadjileontiadis, Leontios J. and Liatsos, Christos N. and Mavrogiannis, Christos C. and Sergiadis, George D.},
	editor = {Magjarevic, Ratko and Bamidis, Panagiotis D. and Pallikarakis, Nicolas},
	year = {2010},
	doi = {10.1007/978-3-642-13039-7_59},
	pages = {236--239},
	file = {Charisis et al. - 2010 - Ulcer Detection in Wireless Capsule Endoscopy Imag.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RNZCPGJS\\Charisis et al. - 2010 - Ulcer Detection in Wireless Capsule Endoscopy Imag.pdf:application/pdf},
}

@article{esteva_dermatologist-level_2017-1,
	title = {Dermatologist-level classification of skin cancer with deep neural networks},
	volume = {542},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature21056},
	doi = {10.1038/nature21056},
	language = {en},
	number = {7639},
	urldate = {2019-05-22},
	journal = {Nature},
	author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
	month = feb,
	year = {2017},
	pages = {115--118},
	file = {Esteva et al. - 2017 - Dermatologist-level classification of skin cancer .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\P5X3SAYC\\Esteva et al. - 2017 - Dermatologist-level classification of skin cancer .pdf:application/pdf},
}

@article{maier-hein_surgical_2017,
	title = {Surgical data science for next-generation interventions},
	volume = {1},
	issn = {2157-846X},
	url = {http://www.nature.com/articles/s41551-017-0132-7},
	doi = {10.1038/s41551-017-0132-7},
	language = {en},
	number = {9},
	urldate = {2019-05-20},
	journal = {Nature Biomedical Engineering},
	author = {Maier-Hein, Lena and Vedula, Swaroop S. and Speidel, Stefanie and Navab, Nassir and Kikinis, Ron and Park, Adrian and Eisenmann, Matthias and Feussner, Hubertus and Forestier, Germain and Giannarou, Stamatia and Hashizume, Makoto and Katic, Darko and Kenngott, Hannes and Kranzfelder, Michael and Malpani, Anand and März, Keno and Neumuth, Thomas and Padoy, Nicolas and Pugh, Carla and Schoch, Nicolai and Stoyanov, Danail and Taylor, Russell and Wagner, Martin and Hager, Gregory D. and Jannin, Pierre},
	month = sep,
	year = {2017},
	pages = {691--696},
	file = {Maier-Hein et al. - 2017 - Surgical data science for next-generation interven.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\H53JYJTV\\Maier-Hein et al. - 2017 - Surgical data science for next-generation interven.pdf:application/pdf},
}

@article{maier-hein_surgical_2017-1,
	title = {Surgical {Data} {Science}: {Enabling} next-generation surgery},
	volume = {1},
	language = {en},
	number = {9},
	author = {Maier-Hein, Lena and Vedula, Swaroop and Speidel, Stefanie and Navab, Nassir and Kikinis, Ron and Park, Adrian and Eisenmann, Matthias and Feussner, Hubertus and Forestier, Germain and Hashizume, Makoto and Katic, Darko and Kenngott, Hannes and Kranzfelder, Michael and Malpani, Anand and März, Keno and Neumuth, Thomas and Padoy, Nicolas and Pugh, Carla and Stoyanov, Danail and Taylor, Russell and Wagner, Martin and Hager, Gregory D and Jannin, Pierre},
	year = {2017},
	pages = {691},
	file = {Maier-Hein et al. - Surgical Data Science Enabling next-generation su.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TGWA3KWF\\Maier-Hein et al. - Surgical Data Science Enabling next-generation su.pdf:application/pdf},
}

@article{moccia_confident_2017,
	title = {Confident texture-based laryngeal tissue classification for early stage diagnosis support},
	volume = {4},
	issn = {2329-4302},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-4/issue-03/034502/Confident-texture-based-laryngeal-tissue-classification-for-early-stage-diagnosis/10.1117/1.JMI.4.3.034502.full},
	doi = {10.1117/1.JMI.4.3.034502},
	language = {en},
	number = {03},
	urldate = {2019-05-20},
	journal = {Journal of Medical Imaging},
	author = {Moccia, Sara and De Momi, Elena and Guarnaschelli, Marco and Savazzi, Matteo and Laborai, Andrea},
	month = sep,
	year = {2017},
	pages = {1},
	file = {Moccia et al. - 2017 - Confident texture-based laryngeal tissue classific.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TQUFYEEA\\Moccia et al. - 2017 - Confident texture-based laryngeal tissue classific.pdf:application/pdf},
}

@article{misawa_accuracy_2017,
	title = {Accuracy of computer-aided diagnosis based on narrow-band imaging endocytoscopy for diagnosing colorectal lesions: comparison with experts},
	volume = {12},
	issn = {1861-6410, 1861-6429},
	shorttitle = {Accuracy of computer-aided diagnosis based on narrow-band imaging endocytoscopy for diagnosing colorectal lesions},
	url = {http://link.springer.com/10.1007/s11548-017-1542-4},
	doi = {10.1007/s11548-017-1542-4},
	abstract = {Purpose Real-time characterization of colorectal lesions during colonoscopy is important for reducing medical costs, given that the need for a pathological diagnosis can be omitted if the accuracy of the diagnostic modality is sufﬁciently high. However, it is sometimes difﬁcult for community-based gastroenterologists to achieve the required level of diagnostic accuracy. In this regard, we developed a computer-aided diagnosis (CAD) system based on endocytoscopy (EC) to evaluate cellular, glandular, and vessel structure atypia in vivo. The purpose of this study was to compare the diagnostic ability and efﬁcacy of this CAD system with the performances of human expert and trainee endoscopists.},
	language = {en},
	number = {5},
	urldate = {2019-05-20},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Misawa, Masashi and Kudo, Shin-ei and Mori, Yuichi and Takeda, Kenichi and Maeda, Yasuharu and Kataoka, Shinichi and Nakamura, Hiroki and Kudo, Toyoki and Wakamura, Kunihiko and Hayashi, Takemasa and Katagiri, Atsushi and Baba, Toshiyuki and Ishida, Fumio and Inoue, Haruhiro and Nimura, Yukitaka and Oda, Msahiro and Mori, Kensaku},
	month = may,
	year = {2017},
	pages = {757--766},
	file = {Misawa et al. - 2017 - Accuracy of computer-aided diagnosis based on narr.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\M3XFKTQH\\Misawa et al. - 2017 - Accuracy of computer-aided diagnosis based on narr.pdf:application/pdf},
}

@inproceedings{liang_computer-aided_2012,
	title = {A computer-aided lesion diagnose method based on gastroscopeimage},
	doi = {10.1109/ICInfA.2012.6246904},
	abstract = {It is time-consuming and exhausting for clinicians to make gastrointestinal disease diagnosis by gastroscope images (GI) with naked eye. However, method use GI as data source to make computer aided diagnosis does not exist at present. In this paper, we discussed a computer aided method for GI, which can be divided into two parts: First, take samples in images which were labeled by clinicians, and extract RG, OPPO, HUE color histogram, and LBP texture in these samples, then use SVM classifier to make classification, the classification accuracy of the above characteristics are given respectively. Second, the detecting images should be divided into little patch, for each patch use features mentioned above and SVM classifier to make classification, then, get the position of the diseased areas. Finally, through experiments shows the effectiveness of the method, and give its diagnostic accuracy rate.},
	booktitle = {2012 {IEEE} {International} {Conference} on {Information} and {Automation}},
	author = {Liang, P. and Cong, Y. and Guan, M.},
	month = jun,
	year = {2012},
	keywords = {Support vector machines, Image color analysis, image colour analysis, endoscopes, medical image processing, diseases, feature extraction, Feature extraction, image classification, Accuracy, Lesions, cancer, SVM, LBP, image texture, classification accuracy, clinicians, color histogram, computer-aided lesion diagnose method, Diseases, gastrointestinal disease diagnosis, gastroscope images, GI, Histograms, HUE color histogram extraction, image detection, image labelling, LBP texture extraction, Lesion diagnose, OPPO color histogram extraction, patches, RG color histogram extraction, support vector machine, support vector machines, SVM classifier, tumours},
	pages = {871--875},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9988NXD7\\6246904.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5SJRE29V\\Liang et al. - 2012 - A computer-aided lesion diagnose method based on g.pdf:application/pdf},
}

@article{haralick_textural_1973,
	title = {Textural {Features} for {Image} {Classification}},
	volume = {SMC-3},
	issn = {0018-9472},
	doi = {10.1109/TSMC.1973.4309314},
	abstract = {Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on gray-tone spatial dependancies, and illustrates their application in category-identification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications.},
	number = {6},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Haralick, R. M. and Shanmugam, K. and Dinstein, I.},
	month = nov,
	year = {1973},
	keywords = {Humans, Satellites, Image classification, Testing, Application software, Crops, Earth, Image resolution, Piecewise linear techniques, Spatial resolution},
	pages = {610--621},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\D9MHX2PE\\4309314.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YYAZJINQ\\Haralick et al. - 1973 - Textural Features for Image Classification.pdf:application/pdf},
}

@article{lu_spectral-spatial_2014,
	title = {Spectral-spatial classification for noninvasive cancer detection using hyperspectral imaging},
	volume = {19},
	issn = {1083-3668},
	url = {http://biomedicaloptics.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JBO.19.10.106004},
	doi = {10.1117/1.JBO.19.10.106004},
	language = {en},
	number = {10},
	urldate = {2019-05-16},
	journal = {Journal of Biomedical Optics},
	author = {Lu, Guolan and Halig, Luma and Wang, Dongsheng and Qin, Xulei and Chen, Zhuo Georgia and Fei, Baowei},
	month = oct,
	year = {2014},
	pages = {106004},
	file = {Lu et al. - 2014 - Spectral-spatial classification for noninvasive ca.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6F52D856\\Lu et al. - 2014 - Spectral-spatial classification for noninvasive ca.pdf:application/pdf},
}

@article{lu_medical_2014,
	title = {Medical hyperspectral imaging: a review},
	volume = {19},
	issn = {1083-3668},
	shorttitle = {Medical hyperspectral imaging},
	url = {http://biomedicaloptics.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JBO.19.1.010901},
	doi = {10.1117/1.JBO.19.1.010901},
	language = {en},
	number = {1},
	urldate = {2019-05-16},
	journal = {Journal of Biomedical Optics},
	author = {Lu, Guolan and Fei, Baowei},
	month = jan,
	year = {2014},
	pages = {010901},
	file = {Lu and Fei - 2014 - Medical hyperspectral imaging a review.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\K22NWNV3\\Lu and Fei - 2014 - Medical hyperspectral imaging a review.pdf:application/pdf},
}

@article{pike_minimum_2016,
	title = {A {Minimum} {Spanning} {Forest} {Based} {Method} for {Noninvasive} {Cancer} {Detection} with {Hyperspectral} {Imaging}},
	volume = {63},
	issn = {0018-9294},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4791052/},
	doi = {10.1109/TBME.2015.2468578},
	abstract = {Goal
The purpose of this paper is to develop a classification method that combines both spectral and spatial information for distinguishing cancer from healthy tissue on hyperspectral images in an animal model.

Methods
An automated algorithm based on a minimum spanning forest (MSF) and optimal band selection has been proposed to classify healthy and cancerous tissue on hyperspectral images. A support vector machine (SVM) classifier is trained to create a pixel-wise classification probability map of cancerous and healthy tissue. This map is then used to identify markers that are used to compute mutual information for a range of bands in the hyperspectral image and thus select the optimal bands. An MSF is finally grown to segment the image using spatial and spectral information.

Conclusion
The MSF based method with automatically selected bands proved to be accurate in determining the tumor boundary on hyperspectral images.

Significance
Hyperspectral imaging combined with the proposed classification technique has the potential to provide a noninvasive tool for cancer detection.},
	number = {3},
	urldate = {2019-05-16},
	journal = {IEEE transactions on bio-medical engineering},
	author = {Pike, Robert and Lu, Guolan and Wang, Dongsheng and Chen, Zhuo Georgia and Fei, Baowei},
	month = mar,
	year = {2016},
	pmid = {26285052},
	pmcid = {PMC4791052},
	pages = {653--663},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UYVW7LPI\\Pike et al. - 2016 - A Minimum Spanning Forest Based Method for Noninva.pdf:application/pdf},
}

@article{lu_framework_2015,
	title = {Framework for hyperspectral image processing and quantification for cancer detection during animal tumor surgery},
	volume = {20},
	issn = {1083-3668},
	url = {http://biomedicaloptics.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JBO.20.12.126012},
	doi = {10.1117/1.JBO.20.12.126012},
	language = {en},
	number = {12},
	urldate = {2019-05-16},
	journal = {Journal of Biomedical Optics},
	author = {Lu, Guolan and Wang, Dongsheng and Qin, Xulei and Halig, Luma and Muller, Susan and Zhang, Hongzheng and Chen, Amy and Pogue, Brian W. and Chen, Zhuo Georgia and Fei, Baowei},
	month = dec,
	year = {2015},
	pages = {126012},
	file = {Lu et al. - 2015 - Framework for hyperspectral image processing and q.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3NGLGP6B\\Lu et al. - 2015 - Framework for hyperspectral image processing and q.pdf:application/pdf},
}

@article{lu_detection_2017,
	title = {Detection of {Head} and {Neck} {Cancer} in {Surgical} {Specimens} {Using} {Quantitative} {Hyperspectral} {Imaging}},
	volume = {23},
	issn = {1078-0432},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5649622/},
	doi = {10.1158/1078-0432.CCR-17-0906},
	abstract = {Purpose
This study intends to investigate the feasibility of using hyperspectral imaging (HSI) to detect and delineate cancers in fresh, surgical specimens of patients with head and neck cancers.

Experimental Design
A clinical study was conducted in order to collect and image fresh, surgical specimens from patients (N = 36) with head and neck cancers undergoing surgical resection. A set of machine-learning tools were developed to quantify hyperspectral images of the resected tissue in order to detect and delineate cancerous regions which were validated by histopathologic diagnosis. More than two million reflectance spectral signatures were obtained by HSI and analyzed using machine-learning methods. The detection results of HSI were compared with autofluorescence imaging and fluorescence imaging of two vital-dyes of the same specimens.

Results
Quantitative HSI differentiated cancerous tissue from normal tissue in ex vivo surgical specimens with a sensitivity and specificity of 91\% and 91\%, respectively, and which was more accurate than autofluorescence imaging (P {\textless} 0.05) or fluorescence imaging of 2-NBDG (P {\textless} 0.05) and proflavine (P {\textless} 0.05). The proposed quantification tools also generated cancer probability maps with the tumor border demarcated and which could provide real-time guidance for surgeons regarding optimal tumor resection.

Conclusions
This study highlights the feasibility of using quantitative HSI as a diagnostic tool to delineate the cancer boundaries in surgical specimens, and which could be translated into the clinic application with the hope of improving clinical outcomes in the future.},
	number = {18},
	urldate = {2019-05-16},
	journal = {Clinical cancer research : an official journal of the American Association for Cancer Research},
	author = {Lu, Guolan and Little, James V. and Wang, Xu and Zhang, Hongzheng and Patel, Mihir R. and Griffith, Christopher C. and El-Deiry, Mark W. and Chen, Amy Y. and Fei, Baowei},
	month = sep,
	year = {2017},
	pmid = {28611203},
	pmcid = {PMC5649622},
	pages = {5426--5436},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5FJ6JZEP\\Lu et al. - 2017 - Detection of Head and Neck Cancer in Surgical Spec.pdf:application/pdf},
}

@article{halicek_deep_2017,
	title = {Deep convolutional neural networks for classifying head and neck cancer using hyperspectral imaging},
	volume = {22},
	issn = {1083-3668},
	url = {http://biomedicaloptics.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JBO.22.6.060503},
	doi = {10.1117/1.JBO.22.6.060503},
	language = {en},
	number = {6},
	urldate = {2019-05-16},
	journal = {Journal of Biomedical Optics},
	author = {Halicek, Martin and Lu, Guolan and Little, James V. and Wang, Xu and Patel, Mihir and Griffith, Christopher C. and El-Deiry, Mark W. and Chen, Amy Y. and Fei, Baowei},
	month = jun,
	year = {2017},
	pages = {060503},
	file = {Halicek et al. - 2017 - Deep convolutional neural networks for classifying.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EQC7K793\\Halicek et al. - 2017 - Deep convolutional neural networks for classifying.pdf:application/pdf},
}

@article{maier-hein_surgical_2018-1,
	title = {Surgical {Data} {Science}: {A} {Consensus} {Perspective}},
	shorttitle = {Surgical {Data} {Science}},
	url = {http://arxiv.org/abs/1806.03184},
	abstract = {Surgical data science is a scientiﬁc discipline with the objective of improving the quality of interventional healthcare and its value through capturing, organization, analysis, and modeling of data. The goal of the 1st workshop on Surgical Data Science was to bring together researchers working on diverse topics in surgical data science in order to discuss existing challenges, potential standards and new research directions in the ﬁeld. Inspired by current open space and think tank formats, it was organized in June 2016 in Heidelberg. While the ﬁrst day of the workshop, which was dominated by interactive sessions, was open to the public, the second day was reserved for a board meeting on which the information gathered on the public day was processed by (1) discussing remaining open issues, (2) deriving a joint deﬁnition for surgical data science and (3) proposing potential strategies for advancing the ﬁeld. This document summarizes the key ﬁndings.},
	language = {en},
	urldate = {2019-05-16},
	journal = {arXiv:1806.03184 [cs]},
	author = {Maier-Hein, Lena and Eisenmann, Matthias and Feldmann, Carolin and Feussner, Hubertus and Forestier, Germain and Giannarou, Stamatia and Gibaud, Bernard and Hager, Gregory D. and Hashizume, Makoto and Katic, Darko and Kenngott, Hannes and Kikinis, Ron and Kranzfelder, Michael and Malpani, Anand and März, Keno and Müuller-Stich, Beat and Navab, Nassir and Neumuth, Thomas and Padoy, Nicolas and Park, Adrian and Pugh, Carla and Schoch, Nicolai and Stoyanov, Danail and Taylor, Russell and Wagner, Martin and Vedula, S. Swaroop and Jannin*, Pierre and Speidel*, Stefanie},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.03184},
	keywords = {Computer Science - Computers and Society},
	file = {Maier-Hein et al. - 2018 - Surgical Data Science A Consensus Perspective.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5JATTMV4\\Maier-Hein et al. - 2018 - Surgical Data Science A Consensus Perspective.pdf:application/pdf},
}

@article{grohl_confidence_2018,
	title = {Confidence {Estimation} for {Machine} {Learning}-{Based} {Quantitative} {Photoacoustics}},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2313-433X/4/12/147},
	doi = {10.3390/jimaging4120147},
	abstract = {In medical applications, the accuracy and robustness of imaging methods are of crucial importance to ensure optimal patient care. While photoacoustic imaging (PAI) is an emerging modality with promising clinical applicability, state-of-the-art approaches to quantitative photoacoustic imaging (qPAI), which aim to solve the ill-posed inverse problem of recovering optical absorption from the measurements obtained, currently cannot comply with these high standards. This can be attributed to the fact that existing methods often rely on several simplifying a priori assumptions of the underlying physical tissue properties or cannot deal with realistic noise levels. In this manuscript, we address this issue with a new method for estimating an indicator of the uncertainty of an estimated optical property. Specifically, our method uses a deep learning model to compute error estimates for optical parameter estimations of a qPAI algorithm. Functional tissue parameters, such as blood oxygen saturation, are usually derived by averaging over entire signal intensity-based regions of interest (ROIs). Therefore, we propose to reduce the systematic error of the ROI samples by additionally discarding those pixels for which our method estimates a high error and thus a low confidence. In silico experiments show an improvement in the accuracy of optical absorption quantification when applying our method to refine the ROI, and it might thus become a valuable tool for increasing the robustness of qPAI methods.},
	language = {en},
	number = {12},
	urldate = {2019-05-16},
	journal = {Journal of Imaging},
	author = {Gröhl, Janek and Kirchner, Thomas and Adler, Tim and Maier-Hein, Lena},
	month = dec,
	year = {2018},
	keywords = {deep learning, confidence learning, error analysis, quantitative photoacoustic imaging, uncertainty estimation},
	pages = {147},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\F64UN2IN\\Gröhl et al. - 2018 - Confidence Estimation for Machine Learning-Based Q.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EQWBM9LU\\htm.html:text/html},
}

@article{periyasamy_advances_2017,
	title = {Advances in {Monte} {Carlo} {Simulation} for {Light} {Propagation} in {Tissue}},
	volume = {10},
	issn = {1937-3333},
	doi = {10.1109/RBME.2017.2739801},
	abstract = {Monte Carlo (MC) simulation for light propagation in tissue is the gold standard for studying the light propagation in biological tissue and has been used for years. Interaction of photons with a medium is simulated based on its optical properties. New simulation geometries, tissue-light interaction methods, and recording techniques recently have been designed. Applications, such as whole mouse body simulations for fluorescence imaging, eye modeling for blood vessel imaging, skin modeling for terahertz imaging, and human head modeling for sinus imaging, have emerged. Here, we review the technical advances and recent applications of MC simulation.},
	journal = {IEEE Reviews in Biomedical Engineering},
	author = {Periyasamy, V. and Pramanik, M.},
	year = {2017},
	keywords = {Photonics, biomedical optical imaging, Monte Carlo methods, Scattering, Biological system modeling, bio-optics, biological tissue, biological tissues, Biological tissues, Biomedical optical imaging, blood vessel imaging, computational modeling, eye modeling, fluorescence imaging, human head modeling, light propagation, light tissue interaction, Mathematical model, MC simulation, Monte Carlo (MC) simulation, Monte Carlo simulation, Optical imaging, optical properties, photon interaction, photon propagation, Refractive index, review, reviews, sinus imaging, skin modeling, terahertz imaging, tissue optics, tissue-light interaction methods, whole mouse body simulations},
	pages = {122--135},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ET6DUWCY\\8010394.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MY9ITSIW\\Periyasamy and Pramanik - 2017 - Advances in Monte Carlo Simulation for Light Propa.pdf:application/pdf},
}

@article{adler_uncertainty-aware_2019,
	title = {Uncertainty-aware performance assessment of optical imaging modalities with invertible neural networks},
	issn = {1861-6410, 1861-6429},
	url = {http://link.springer.com/10.1007/s11548-019-01939-9},
	doi = {10.1007/s11548-019-01939-9},
	abstract = {Purpose Optical imaging is evolving as a key technique for advanced sensing in the operating room. Recent research has shown that machine learning algorithms can be used to address the inverse problem of converting pixel-wise multispectral reﬂectance measurements to underlying tissue parameters, such as oxygenation. Assessment of the speciﬁc hardware used in conjunction with such algorithms, however, has not properly addressed the possibility that the problem may be ill-posed.
Methods We present a novel approach to the assessment of optical imaging modalities, which is sensitive to the different types of uncertainties that may occur when inferring tissue parameters. Based on the concept of invertible neural networks, our framework goes beyond point estimates and maps each multispectral measurement to a full posterior probability distribution which is capable of representing ambiguity in the solution via multiple modes. Performance metrics for a hardware setup can then be computed from the characteristics of the posteriors.
Results Application of the assessment framework to the speciﬁc use case of camera selection for physiological parameter estimation yields the following insights: (1) estimation of tissue oxygenation from multispectral images is a well-posed problem, while (2) blood volume fraction may not be recovered without ambiguity. (3) In general, ambiguity may be reduced by increasing the number of spectral bands in the camera.
Conclusion Our method could help to optimize optical camera design in an application-speciﬁc manner.},
	language = {en},
	urldate = {2019-05-16},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Adler, Tim J. and Ardizzone, Lynton and Vemuri, Anant and Ayala, Leonardo and Gröhl, Janek and Kirchner, Thomas and Wirkert, Sebastian and Kruse, Jakob and Rother, Carsten and Köthe, Ullrich and Maier-Hein, Lena},
	month = mar,
	year = {2019},
	file = {Adler et al. - 2019 - Uncertainty-aware performance assessment of optica.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\U5FF4ZCB\\Adler et al. - 2019 - Uncertainty-aware performance assessment of optica.pdf:application/pdf},
}

@article{vemuri_hyperspectral_2019,
	title = {Hyperspectral {Camera} {Selection} for {Interventional} {Health}-care},
	url = {http://arxiv.org/abs/1904.02709},
	abstract = {Hyperspectral imaging (HSI) is an emerging modality in health-care applications for disease diagnosis, tissue assessment and image-guided surgery. Tissue reﬂectances captured by a HSI camera encode physiological properties including oxygenation and blood volume fraction. Optimal camera properties such as ﬁlter responses depend crucially on the application, and choosing a suitable HSI camera for a research project and/or a clinical problem is not straightforward. We propose a generic framework for quantitative and application-speciﬁc performance assessment of HSI cameras and optical subsystem without the need for any physical setup. Based on user input about the camera characteristics and properties of the target domain, our framework quantiﬁes the performance of the given camera conﬁguration using large amounts of simulated data and a user-deﬁned metric. The application of the framework to commercial camera selection and band selection in the context of oxygenation monitoring in interventional health-care demonstrates its integration into the design work-ﬂow of an engineer. The advantage of being able to test the desired conﬁguration without the need for purchasing expensive components may save system engineers valuable resources.},
	language = {en},
	urldate = {2019-05-16},
	journal = {arXiv:1904.02709 [physics]},
	author = {Vemuri, Anant S. and Wirkert, Sebastian and Maier-Hein, Lena},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.02709},
	keywords = {Physics - Medical Physics},
	file = {Vemuri et al. - 2019 - Hyperspectral Camera Selection for Interventional .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2MY7EFFW\\Vemuri et al. - 2019 - Hyperspectral Camera Selection for Interventional .pdf:application/pdf},
}

@inproceedings{ayala_abstract:_2019,
	series = {Informatik aktuell},
	title = {Abstract: {Multispectral} {Imaging} {Enables} {Visualization} of {Spreading} {Depolarizations} in {Gyrencephalic} {Brain}},
	isbn = {978-3-658-25326-4},
	shorttitle = {Abstract},
	abstract = {Spreading Depolarization (SD) is a phenomenon in the brain related to the abrupt depolarization of neurons in gray matter which results from a break-down of ion gradients across the neuron membrane and propagates like a wave of ischemia. While modulating the hemodynamic response of the SDs is a therapeutic target, the lack of imaging methods that allow for monitoring SDs with high spatiotemporal resolution hinder progress in the field. In this work, we address this bottleneck with a new method for brain imaging based on multispectral imaging (MSI).},
	language = {de},
	booktitle = {Bildverarbeitung für die {Medizin} 2019},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Ayala, Leonardo and Wirkert, SJ and Herrera, MA and Hernández-Aguilera, Adrián and Vermuri, AS and Santos, E. and Maier-Hein, L.},
	editor = {Handels, Heinz and Deserno, Thomas M. and Maier, Andreas and Maier-Hein, Klaus Hermann and Palm, Christoph and Tolxdorff, Thomas},
	year = {2019},
	pages = {244--244},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ANICP732\\Ayala et al. - 2019 - Abstract Multispectral Imaging Enables Visualizat.pdf:application/pdf},
}

@article{adler_uncertainty_2019,
	title = {Uncertainty handling in intra-operative multispectral imaging with invertible neural networks},
	language = {en},
	journal = {Proceeding of Machine Learning Research},
	author = {Adler, Tim J. and Ardizzone, Lynton and Ayala, Leonardo and Gröhl, Janek and Vemuri, Anant and Wirkert, Sebastian J. and Müller-Stich, Beat P. and Rother, Carsten and Köthe, Ullrich and Maier-Hein, Lena},
	year = {2019},
	keywords = {MSI, uncertainty},
	file = {MICCAI - 2017 - Medical image computing and computer assisted inte.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YYPMQ9XJ\\MICCAI - 2017 - Medical image computing and computer assisted inte.pdf:application/pdf},
}

@article{ardizzone_analyzing_2018,
	title = {Analyzing {Inverse} {Problems} with {Invertible} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1808.04730},
	abstract = {For many applications, in particular in natural science, the task is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is well-deﬁned, whereas the inverse problem is ambiguous: multiple parameter sets can result in the same measurement. To fully characterize this ambiguity, the full posterior parameter distribution, conditioned on an observed measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task – so-called Invertible Neural Networks (INNs). Unlike classical neural networks, which attempt to solve the ambiguous inverse problem directly, INNs focus on learning the forward process, using additional latent output variables to capture the information otherwise lost. Due to invertibility, a model of the corresponding inverse process is learned implicitly. Given a speciﬁc measurement and the distribution of the latent variables, the inverse pass of the INN provides the full posterior over parameter space. We prove theoretically and verify experimentally, on artiﬁcial data and real-world problems from medicine and astrophysics, that INNs are a powerful analysis tool to ﬁnd multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters.},
	language = {en},
	urldate = {2019-05-16},
	journal = {arXiv:1808.04730 [cs, stat]},
	author = {Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W. and Klessen, Ralf S. and Maier-Hein, Lena and Rother, Carsten and Köthe, Ullrich},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.04730},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 68T01},
	file = {Ardizzone et al. - 2018 - Analyzing Inverse Problems with Invertible Neural .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\U9PAFV3N\\Ardizzone et al. - 2018 - Analyzing Inverse Problems with Invertible Neural .pdf:application/pdf},
}

@incollection{luo_endoscopic_2014,
	address = {Cham},
	title = {Endoscopic {Sheffield} {Index} for {Unsupervised} {In} {Vivo} {Spectral} {Band} {Selection}},
	volume = {8899},
	isbn = {978-3-319-13409-3 978-3-319-13410-9},
	url = {http://link.springer.com/10.1007/978-3-319-13410-9_11},
	abstract = {Endoscopic procedures provide important information about the internal patient anatomy but are currently restricted to a 2D texture analysis of the visible organ surfaces. Spectral imaging has high potential in generating valuable complementary information about the molecular tissue composition but su ers from long image acquisition times. As the technique requires an aligned stack of images, its bene t in endoscopic procedures is still very limited due to continuous motion of the camera and the tissue. In this paper, we present an information theory based approach to band selection for endoscopic spectral imaging. In contrast to previous approaches, our concept does not require labelled training data or an elaborate light-tissue interaction model. According to a validation study using phantom data as well as in vivo spectral data obtained from  ve surgeries in a porcine model, only few bands selected by our method are su cient to reconstruct the tissue composition with a similar accuracy as obtainable with the full spectrum.},
	language = {en},
	urldate = {2019-05-16},
	booktitle = {Computer-{Assisted} and {Robotic} {Endoscopy}},
	publisher = {Springer International Publishing},
	author = {Wirkert, Sebastian J. and Clancy, Neil T. and Stoyanov, Danail and Arya, Shobhit and Hanna, George B. and Schlemmer, Heinz-Peter and Sauer, Peter and Elson, Daniel S. and Maier-Hein, Lena},
	editor = {Luo, Xiongbiao and Reichl, Tobias and Mirota, Daniel and Soper, Timothy},
	year = {2014},
	doi = {10.1007/978-3-319-13410-9_11},
	pages = {110--120},
	file = {Wirkert et al. - 2014 - Endoscopic Sheffield Index for Unsupervised In Viv.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RXNDY9DQ\\Wirkert et al. - 2014 - Endoscopic Sheffield Index for Unsupervised In Viv.pdf:application/pdf},
}

@article{clancy_intraoperative_2015,
	title = {Intraoperative measurement of bowel oxygen saturation using a multispectral imaging laparoscope},
	volume = {6},
	issn = {2156-7085, 2156-7085},
	url = {https://www.osapublishing.org/abstract.cfm?URI=boe-6-10-4179},
	doi = {10.1364/BOE.6.004179},
	abstract = {Intraoperative monitoring of tissue oxygen saturation (StO2) has potentially important applications in procedures such as organ transplantation or colorectal surgery, where successful reperfusion affects the viability and integrity of repaired tissues. In this paper a liquid crystal tuneable filter-based multispectral imaging (MSI) laparoscope is described. Motion-induced image misalignments are reduced, using feature-based registration, before regression of the tissue reflectance spectra to calculate relative quantities of oxy- and deoxyhaemoglobin. The laparoscope was validated in vivo, during porcine abdominal surgery, by making parallel MSI and blood gas measurements of the small bowel vasculature. Ischaemic conditions were induced by local occlusion of the mesenteric arcade and monitored using the system. The MSI laparoscope was capable of measuring StO2 over a wide range (30-100\%) with a temporal error of ± 7.5\%. The imager showed sensitivity to spatial changes in StO2 during dynamic local occlusions, as well as tracking the recovery of tissues postocclusion.},
	language = {en},
	number = {10},
	urldate = {2019-05-16},
	journal = {Biomedical Optics Express},
	author = {Clancy, Neil T. and Arya, Shobhit and Stoyanov, Danail and Singh, Mohan and Hanna, George B. and Elson, Daniel S.},
	month = oct,
	year = {2015},
	pages = {4179},
	file = {Clancy et al. - 2015 - Intraoperative measurement of bowel oxygen saturat.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\28VEAQ7L\\Clancy et al. - 2015 - Intraoperative measurement of bowel oxygen saturat.pdf:application/pdf},
}

@incollection{wirkert_physiological_2017,
	address = {Singapore},
	title = {Physiological {Parameter} {Estimation} from {Multispectral} {Images} {Unleashed}},
	volume = {217},
	isbn = {9789811359439 9789811359446},
	url = {http://link.springer.com/10.1007/978-3-319-66179-7_16},
	abstract = {Multispectral imaging in laparoscopy can provide tissue reﬂectance measurements for each point in the image at multiple wavelengths of light. These reﬂectances encode information on important physiological parameters not visible to the naked eye. Fast decoding of the data during surgery, however, remains challenging. While modelbased methods suﬀer from inaccurate base assumptions, a major bottleneck related to competing machine learning-based solutions is the lack of labelled training data. In this paper, we address this issue with the ﬁrst transfer learning-based method to physiological parameter estimation from multispectral images. It relies on a highly generic tissue model that aims to capture the full range of optical tissue parameters that can potentially be observed in vivo. Adaptation of the model to a speciﬁc clinical application based on unlabelled in vivo data is achieved using a new concept of domain adaptation that explicitly addresses the high variance often introduced by conventional covariance-shift correction methods. According to comprehensive in silico and in vivo experiments our approach enables accurate parameter estimation for various tissue types without the need for incorporating speciﬁc prior knowledge on optical properties and could thus pave the way for many exciting applications in multispectral laparoscopy.},
	language = {en},
	urldate = {2019-05-16},
	booktitle = {Physics and {Engineering} of {Metallic} {Materials}},
	publisher = {Springer Singapore},
	author = {Wirkert, Sebastian J. and Vemuri, Anant S. and Kenngott, Hannes G. and Moccia, Sara and Götz, Michael and Mayer, Benjamin F. B. and Maier-Hein, Klaus H. and Elson, Daniel S. and Maier-Hein, Lena},
	year = {2017},
	doi = {10.1007/978-3-319-66179-7_16},
	pages = {134--141},
	file = {Wirkert et al. - 2017 - Physiological Parameter Estimation from Multispect.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PPULA7A7\\Wirkert et al. - 2017 - Physiological Parameter Estimation from Multispect.pdf:application/pdf},
}

@article{wirkert_robust_2016,
	title = {Robust near real-time estimation of physiological parameters from megapixel multispectral images with inverse {Monte} {Carlo} and random forest regression},
	volume = {11},
	issn = {1861-6410, 1861-6429},
	url = {http://link.springer.com/10.1007/s11548-016-1376-5},
	doi = {10.1007/s11548-016-1376-5},
	abstract = {Purpose Multispectral imaging can provide reﬂectance measurements at multiple spectral bands for each image pixel. These measurements can be used for estimation of important physiological parameters, such as oxygenation, which can provide indicators for the success of surgical treatment or the presence of abnormal tissue. The goal of this work was to develop a method to estimate physiological parameters in an accurate and rapid manner suited for modern high-resolution laparoscopic images.},
	language = {en},
	number = {6},
	urldate = {2019-05-16},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Wirkert, Sebastian J. and Kenngott, Hannes and Mayer, Benjamin and Mietkowski, Patrick and Wagner, Martin and Sauer, Peter and Clancy, Neil T. and Elson, Daniel S. and Maier-Hein, Lena},
	month = jun,
	year = {2016},
	pages = {909--917},
	file = {Wirkert et al. - 2016 - Robust near real-time estimation of physiological .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7CRJRP7I\\Wirkert et al. - 2016 - Robust near real-time estimation of physiological .pdf:application/pdf},
}

@inproceedings{webster_tissue_2016,
	address = {San Diego, California, United States},
	title = {Tissue classification for laparoscopic image understanding based on multispectral texture analysis},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2216090},
	doi = {10.1117/12.2216090},
	abstract = {Intra-operative tissue classiﬁcation is one of the prerequisites for providing context-aware visualization in computerassisted minimally invasive surgeries. As many anatomical structures are diﬃcult to diﬀerentiate in conventional RGB medical images, we propose a classiﬁcation method based on multispectral image patches. In a comprehensive ex vivo study we show (1) that multispectral imaging data is superior to RGB data for organ tissue classiﬁcation when used in conjunction with widely applied feature descriptors and (2) that combining the tissue texture with the reﬂectance spectrum improves the classiﬁcation performance. Multispectral tissue analysis could thus evolve as a key enabling technique in computer-assisted laparoscopy.},
	language = {en},
	urldate = {2019-05-16},
	author = {Zhang, Yan and Wirkert, Sebastian J. and Iszatt, Justin and Kenngott, Hannes and Wagner, Martin and Mayer, Benjamin and Stock, Christian and Clancy, Neil T. and Elson, Daniel S. and Maier-Hein, Lena},
	editor = {Webster, Robert J. and Yaniv, Ziv R.},
	month = mar,
	year = {2016},
	pages = {978619},
	file = {Zhang et al. - 2016 - Tissue classification for laparoscopic image under.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QHTNQE46\\Zhang et al. - 2016 - Tissue classification for laparoscopic image under.pdf:application/pdf},
}

@article{calin_hyperspectral_2014,
	title = {Hyperspectral {Imaging} in the {Medical} {Field}: {Present} and {Future}},
	volume = {49},
	issn = {0570-4928, 1520-569X},
	shorttitle = {Hyperspectral {Imaging} in the {Medical} {Field}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/05704928.2013.838678},
	doi = {10.1080/05704928.2013.838678},
	abstract = {Hyperspectral imaging is an optical method that provides a large amount of information about the investigated object. Its medical applications are reviewed in this article, including tumor delimitation and identiﬁcation, assessing tissue perfusion and its pathological conditions (including some complications like diabetic foot ulceration), making accurate surgical decisions, evaluating the health of dental structures, etc. Many of the articles show very promising results that required brief comments by the authors. It is clear that choosing the appropriate hyperspectral imaging system for each medical ﬁeld, together with the most reliable hyperspectral image processing methods, are the main goals of future studies, before hyperspectral imaging becomes a widely applicable evaluation method in medicine. The authors try to answer some questions on this topic and set up some directions for future research.},
	language = {en},
	number = {6},
	urldate = {2019-05-16},
	journal = {Applied Spectroscopy Reviews},
	author = {Calin, Mihaela Antonina and Parasca, Sorin Viorel and Savastru, Dan and Manea, Dragos},
	month = aug,
	year = {2014},
	pages = {435--447},
	file = {Calin et al. - 2014 - Hyperspectral Imaging in the Medical Field Presen.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FFBGPVM7\\Calin et al. - 2014 - Hyperspectral Imaging in the Medical Field Presen.pdf:application/pdf},
}

@article{moccia_uncertainty-aware_2018,
	title = {Uncertainty-{Aware} {Organ} {Classification} for {Surgical} {Data} {Science} {Applications} in {Laparoscopy}},
	volume = {65},
	issn = {0018-9294, 1558-2531},
	url = {https://ieeexplore.ieee.org/document/8310960/},
	doi = {10.1109/TBME.2018.2813015},
	abstract = {Objective: Surgical data science is evolving into a research ﬁeld that aims to observe everything occurring within and around the treatment process to provide situation-aware data-driven assistance. In the context of endoscopic video analysis, the accurate classiﬁcation of organs in the ﬁeld of view of the camera proffers a technical challenge. Herein, we propose a new approach to anatomical structure classiﬁcation and image tagging that features an intrinsic measure of conﬁdence to estimate its own performance with high reliability and which can be applied to both RGB and multispectral imaging (MI) data. Methods: Organ recognition is performed using a superpixel classiﬁcation strategy based on textural and reﬂectance information. Classiﬁcation conﬁdence is estimated by analyzing the dispersion of class probabilities. Assessment of the proposed technology is performed through a comprehensive in vivo study with seven pigs. Results: When applied to image tagging, mean accuracy in our experiments increased from 65\% (RGB) and 80\% (MI) to 90\% (RGB) and 96\% (MI) with the conﬁdence measure. Conclusion: Results showed that the conﬁdence measure had a signiﬁcant inﬂuence on the classiﬁcation accuracy, and MI data are better suited for anatomical structure labeling than RGB data. Signiﬁcance: This paper signiﬁcantly enhances the state of art in automatic labeling of endoscopic videos by introducing the use of the conﬁdence metric, and by being the ﬁrst study to use MI data for in vivo laparoscopic tissue classiﬁcation. The data of our experiments will be released as the ﬁrst in vivo MI dataset upon publication of this paper.},
	language = {en},
	number = {11},
	urldate = {2019-05-16},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Moccia, Sara and Wirkert, Sebastian J. and Kenngott, Hannes and Vemuri, Anant S. and Apitz, Martin and Mayer, Benjamin and De Momi, Elena and Mattos, Leonardo S. and Maier-Hein, Lena},
	month = nov,
	year = {2018},
	pages = {2649--2659},
	file = {Moccia et al. - 2018 - Uncertainty-Aware Organ Classification for Surgica.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\N2RZHJGK\\Moccia et al. - 2018 - Uncertainty-Aware Organ Classification for Surgica.pdf:application/pdf},
}

@article{momi_supervised_nodate,
	title = {{SUPERVISED} {TISSUE} {CLASSIFICATION} {IN} {OPTICAL} {IMAGES}: {TOWARDS} {NEW} {APPLICATIONS} {OF} {SURGICAL} {DATA} {SCIENCE}},
	language = {en},
	author = {Momi, Elena De and Mattos, Leonardo S and Dellacá, Raffaele and Aliverti, Andrea and Moccia, Sara},
	pages = {155},
	file = {Momi et al. - SUPERVISED TISSUE CLASSIFICATION IN OPTICAL IMAGES.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\G5H6C39W\\Momi et al. - SUPERVISED TISSUE CLASSIFICATION IN OPTICAL IMAGES.pdf:application/pdf},
}

@misc{noauthor_explainability_2018,
	title = {Explainability in {Neural} {Networks}: {Path} {Methods} for {Attribution}},
	shorttitle = {Explainability in {Neural} {Networks}},
	url = {https://deep.ghost.io/path-methods/},
	abstract = {This post will delve deeper into Path Integrated Gradient Methods for Feature Attribution in Neural Networks.},
	language = {en},
	urldate = {2020-03-19},
	journal = {Alta Cognita},
	month = nov,
	year = {2018},
	note = {Library Catalog: deep.ghost.io},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XJDT4C36\\path-methods.html:text/html},
}

@article{sturmfels_visualizing_2020,
	title = {Visualizing the {Impact} of {Feature} {Attribution} {Baselines}},
	volume = {5},
	issn = {2476-0757},
	url = {https://distill.pub/2020/attribution-baselines},
	doi = {10.23915/distill.00022},
	abstract = {Exploring the baseline input hyperparameter, and how it impacts interpretations of neural network behavior.},
	language = {en},
	number = {1},
	urldate = {2020-03-19},
	journal = {Distill},
	author = {Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
	month = jan,
	year = {2020},
	pages = {e22},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FG5VQFXH\\attribution-baselines.html:text/html},
}

@article{sundararajan_axiomatic_2017,
	title = {Axiomatic {Attribution} for {Deep} {Networks}},
	url = {http://arxiv.org/abs/1703.01365},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisﬁed by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modiﬁcation to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	language = {en},
	urldate = {2020-03-19},
	journal = {arXiv:1703.01365 [cs]},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	month = jun,
	year = {2017},
	note = {arXiv: 1703.01365},
	keywords = {Computer Science - Machine Learning},
	file = {Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ZMGJ3PJJ\\Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf:application/pdf},
}

@techreport{song_covid-19_2020,
	type = {preprint},
	title = {{COVID}-19 early warning score: a multi-parameter screening tool to identify highly suspected patients},
	shorttitle = {{COVID}-19 early warning score},
	url = {http://medrxiv.org/lookup/doi/10.1101/2020.03.05.20031906},
	abstract = {BACKGROUND Corona Virus Disease 2019 (COVID-19) is spreading worldwide. Effective screening for patients is important to limit the epidemic. However, some defects make the currently applied diagnosis methods are still not very ideal for early warning of patients. We aimed to develop a diagnostic model that allows for the quick screening of highly suspected patients using easy-to-get variables.
METHODS A total of 1,311 patients receiving severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) nucleicacid detection were included, whom with a positive result were classified into COVID-19 group. Multivariate logistic regression analyses were performed to construct the diagnostic model. Receiver operating characteristic (ROC) curve analysis were used for model validation.
RESULTS After analysis, signs of pneumonia on CT, history of close contact, fever, neutrophil-to-lymphocyte ratio (NLR), Tmax and sex were included in the diagnostic model. Age and meaningful respiratory symptoms were enrolled into COVID-19 early warning score (COVID-19 EWS). The areas under the ROC curve (AUROC) indicated that both of the diagnostic model (training dataset 0.956 [95\%CI 0.935-0.977, P {\textless} 0.001]; validation dataset 0.960 [95\%CI 0.919-1.0, P {\textless} 0.001] ) and COVID-19 EWS (training dataset 0.956 [95\%CI 0.934-0.978, P {\textless} 0.001] ; validate dataset 0.966 [95\%CI 0.929-1, P {\textless} 0.001]) had good discrimination capacity. In addition, we also obtained the cut-off values of disease severity predictors, such as CT score, CD8+ T cell count, CD4+ T cell count, and so on.
CONCLUSIONS The new developed COVID-19 EWS was a considerable tool for early and relatively accurately warning of SARS-CoV-2 infected patients.},
	language = {en},
	urldate = {2020-03-27},
	institution = {Infectious Diseases (except HIV/AIDS)},
	author = {Song, Cong-Ying and Xu, Jia and He, Jian-Qin and Lu, Yuan-Qiang},
	month = mar,
	year = {2020},
	doi = {10.1101/2020.03.05.20031906},
	file = {Song et al. - 2020 - COVID-19 early warning score a multi-parameter sc.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5HVU7T4N\\Song et al. - 2020 - COVID-19 early warning score a multi-parameter sc.pdf:application/pdf},
}

@misc{noauthor_clinical_nodate,
	title = {Clinical course and risk factors for mortality of adult inpatients with {COVID}-19 in {Wuhan}, {China}: a retrospective cohort study {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {Clinical course and risk factors for mortality of adult inpatients with {COVID}-19 in {Wuhan}, {China}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0140673620305663?token=C0446607A7538EF4876FCD40E3E92C95672599E4D9BAAB25AB21EF31910B03A1B20AB02396996D76CA698808E752CDB0},
	language = {en},
	urldate = {2020-03-27},
	doi = {10.1016/S0140-6736(20)30566-3},
	note = {Library Catalog: reader.elsevier.com},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5B44IUCU\\S0140673620305663.html:text/html;Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XBGQ2M9L\\Clinical course and risk factors for mortality of .pdf:application/pdf},
}

@article{ilse_diva_nodate,
	title = {{DIVA}: {Domain} {Invariant} {Variational} {Autoencoders}},
	abstract = {We consider the problem of domain generalization, namely, how to learn representations given data from a set of domains that generalize to data from a previously unseen domain. We propose the Domain Invariant Variational Autoencoder (DIVA), a generative model that tackles this problem by learning three independent latent subspaces, one for the domain, one for the class, and one for any residual variations. We highlight that due to the generative nature of our model we can also incorporate unlabeled data from known or previously unseen domains. To the best of our knowledge this has not been done before in a domain generalization setting. This property is highly desirable in ﬁelds like medical imaging where labeled data is scarce. We experimentally evaluate our model on the rotated MNIST benchmark and a malaria cell images dataset where we show that (i) the learned subspaces are indeed complementary to each other, (ii) we improve upon recent works on this task and (iii) incorporating unlabelled data can boost the performance even further.},
	language = {en},
	author = {Ilse, Maximilian and Tomczak, Jakub M and Louizos, Christos and Welling, Max},
	keywords = {important},
	pages = {27},
	file = {Ilse et al. - DIVA Domain Invariant Variational Autoencoders.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\HZ6DT4SL\\Ilse et al. - DIVA Domain Invariant Variational Autoencoders.pdf:application/pdf},
}

@article{hong_invariant_2020-1,
	title = {Invariant {Attribute} {Profiles}: {A} {Spatial}-{Frequency} {Joint} {Feature} {Extractor} for {Hyperspectral} {Image} {Classification}},
	issn = {0196-2892, 1558-0644},
	shorttitle = {Invariant {Attribute} {Profiles}},
	url = {http://arxiv.org/abs/1912.08847},
	doi = {10.1109/TGRS.2019.2957251},
	abstract = {Up to the present, an enormous number of advanced techniques have been developed to enhance and extract the spatially semantic information in hyperspectral image processing and analysis. However, locally semantic change, such as scene composition, relative position between objects, spectral variability caused by illumination, atmospheric effects, and material mixture, has been less frequently investigated in modeling spatial information. As a consequence, identifying the same materials from spatially different scenes or positions can be difficult. In this paper, we propose a solution to address this issue by locally extracting invariant features from hyperspectral imagery (HSI) in both spatial and frequency domains, using a method called invariant attribute profiles (IAPs). IAPs extract the spatial invariant features by exploiting isotropic filter banks or convolutional kernels on HSI and spatial aggregation techniques (e.g., superpixel segmentation) in the Cartesian coordinate system. Furthermore, they model invariant behaviors (e.g., shift, rotation) by the means of a continuous histogram of oriented gradients constructed in a Fourier polar coordinate. This yields a combinatorial representation of spatial-frequency invariant features with application to HSI classification. Extensive experiments conducted on three promising hyperspectral datasets (Houston2013 and Houston2018) demonstrate the superiority and effectiveness of the proposed IAP method in comparison with several state-of-the-art profile-related techniques. The codes will be available from the website: https://sites.google.com/view/danfeng-hong/data-code.},
	language = {en},
	urldate = {2020-01-21},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Hong, Danfeng and Wu, Xin and Ghamisi, Pedram and Chanussot, Jocelyn and Yokoya, Naoto and Zhu, Xiao Xiang},
	year = {2020},
	note = {arXiv: 1912.08847},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--18},
}

@article{sorrenson_disentanglement_2020-1,
	title = {Disentanglement by {Nonlinear} {ICA} with {General} {Incompressible}-flow {Networks} ({GIN})},
	url = {http://arxiv.org/abs/2001.04872},
	abstract = {A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modiﬁcation of the RealNVP invertible neural network architecture (Dinh et al., 2016) which is particularly suitable for this type of problem: the General Incompressibleﬂow Network (GIN). Experiments on artiﬁcial data and EMNIST demonstrate that theoretical predictions are indeed veriﬁed in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.},
	language = {en},
	urldate = {2020-07-21},
	journal = {arXiv:2001.04872 [cs, stat]},
	author = {Sorrenson, Peter and Rother, Carsten and Köthe, Ullrich},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.04872},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Sorrenson et al. - 2020 - Disentanglement by Nonlinear ICA with General Inco.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Z73CSD2D\\Sorrenson et al. - 2020 - Disentanglement by Nonlinear ICA with General Inco.pdf:application/pdf},
}

@article{ganin_domain-adversarial_2016,
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.07818},
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but diﬀerent distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for eﬀective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.},
	language = {en},
	urldate = {2020-08-27},
	journal = {arXiv:1505.07818 [cs, stat]},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and Marchand, Mario and Lempitsky, Victor},
	month = may,
	year = {2016},
	note = {arXiv: 1505.07818},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Ganin et al. - 2016 - Domain-Adversarial Training of Neural Networks.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ZD3VNF87\\Ganin et al. - 2016 - Domain-Adversarial Training of Neural Networks.pdf:application/pdf},
}

@article{shankar_generalizing_2018,
	title = {Generalizing {Across} {Domains} via {Cross}-{Gradient} {Training}},
	url = {http://arxiv.org/abs/1804.10745},
	abstract = {We present CROSSGRAD, a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD parallelly trains a label and a domain classifier on examples perturbed by loss gradients of each other's objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and that (2) data augmentation is a more stable and accurate method than domain adversarial training.},
	language = {en},
	urldate = {2020-08-27},
	journal = {arXiv:1804.10745 [cs, stat]},
	author = {Shankar, Shiv and Piratla, Vihari and Chakrabarti, Soumen and Chaudhuri, Siddhartha and Jyothi, Preethi and Sarawagi, Sunita},
	month = may,
	year = {2018},
	note = {arXiv: 1804.10745},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Shankar et al. - 2018 - Generalizing Across Domains via Cross-Gradient Tra.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YZIL7X7Y\\Shankar et al. - 2018 - Generalizing Across Domains via Cross-Gradient Tra.pdf:application/pdf},
}

@article{wang_learning_2019,
	title = {Learning {Robust} {Representations} by {Projecting} {Superficial} {Statistics} {Out}},
	url = {http://arxiv.org/abs/1903.06256},
	abstract = {Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superﬁcial statistics of the training data and are liable to break under distribution shift. For example, subtle changes to the background or texture of an image can break a seemingly powerful classiﬁer. Building on previous work on domain generalization, we hope to produce a classiﬁer that will generalize to previously unseen domains, even when domain identiﬁers are not available during training. This setting is challenging because the model may extract many distribution-speciﬁc (superﬁcial) signals together with distribution-agnostic (semantic) signals. To overcome this challenge, we incorporate the gray-level cooccurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superﬁcial: they are sensitive to texture but unable to capture the gestalt of an image. Then we introduce two techniques for improving our networks’ outof-sample performance. The ﬁrst method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable. The second method is built on the independence introduced by projecting the model’s representation onto the subspace orthogonal to GLCM representation’s. We test our method on battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.},
	language = {en},
	urldate = {2020-08-27},
	journal = {arXiv:1903.06256 [cs]},
	author = {Wang, Haohan and He, Zexue and Lipton, Zachary C. and Xing, Eric P.},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.06256},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Wang et al. - 2019 - Learning Robust Representations by Projecting Supe.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2I8I55NX\\Wang et al. - 2019 - Learning Robust Representations by Projecting Supe.pdf:application/pdf},
}

@article{khemakhem_variational_nodate,
	title = {Variational {Autoencoders} and {Nonlinear} {ICA}: {A} {Unifying} {Framework}},
	abstract = {The framework of variational autoencoders allows us to eﬃciently learn deep latent-variable models, such that the model’s marginal distribution over observed variables ﬁts the data. Often, we’re interested in going a step further, and want to approximate the true joint distribution over observed and latent variables, including the true prior and posterior distributions over latent variables. This is known to be generally impossible due to unidentiﬁability of the model. We address this issue by showing that for a broad family of deep latentvariable models, identiﬁcation of the true joint distribution over observed and latent variables is actually possible up to very simple transformations, thus achieving a principled and powerful form of disentanglement. Our result requires a factorized prior distribution over the latent variables that is conditioned on an additionally observed variable, such as a class label or almost any other observation. We build on recent developments in nonlinear ICA, which we extend to the case with noisy, undercomplete or discrete observations, integrated in a maximum likelihood framework. The result also trivially contains identiﬁable ﬂow-based generative models as a special case.},
	language = {en},
	author = {Khemakhem, Ilyes and Kingma, Diederik P and Monti, Ricardo Pio and Hyvärinen, Aapo},
	pages = {10},
	file = {Khemakhem et al. - Variational Autoencoders and Nonlinear ICA A Unif.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YNCK9F8V\\Khemakhem et al. - Variational Autoencoders and Nonlinear ICA A Unif.pdf:application/pdf},
}

@article{tzeng_simultaneous_nodate,
	title = {Simultaneous {Deep} {Transfer} {Across} {Domains} and {Tasks}},
	abstract = {Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning deep models in a new domain can require a signiﬁcant amount of labeled data, which for many applications is simply not available. We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data. Our approach simultaneously optimizes for domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks. Our proposed adaptation method offers empirical performance which exceeds previously published results on two standard benchmark visual domain adaptation tasks, evaluated across supervised and semi-supervised adaptation settings.},
	language = {en},
	author = {Tzeng, Eric and Hoffman, Judy and Darrell, Trevor and Saenko, Kate},
	pages = {9},
	file = {Tzeng et al. - Simultaneous Deep Transfer Across Domains and Task.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6NUGKNQR\\Tzeng et al. - Simultaneous Deep Transfer Across Domains and Task.pdf:application/pdf},
}

@misc{hollander_deep_2019,
	title = {Deep {Domain} {Adaptation} {In} {Computer} {Vision}},
	url = {https://towardsdatascience.com/deep-domain-adaptation-in-computer-vision-8da398d3167f},
	abstract = {Adapting generic source models in computer vision to domain-specific target tasks},
	language = {en},
	urldate = {2020-08-27},
	journal = {Medium},
	author = {Holländer, Branislav},
	month = jul,
	year = {2019},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\F89D2IVL\\deep-domain-adaptation-in-computer-vision-8da398d3167f.html:text/html},
}

@inproceedings{esser_disentangling_2020,
	address = {Seattle, WA, USA},
	title = {A {Disentangling} {Invertible} {Interpretation} {Network} for {Explaining} {Latent} {Representations}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157170/},
	doi = {10.1109/CVPR42600.2020.00924},
	abstract = {Neural networks have greatly boosted performance in computer vision by learning powerful representations of input data. The drawback of end-to-end training for maximal overall performance are black-box models whose hidden representations are lacking interpretability: Since distributed coding is optimal for latent layers to improve their robustness, attributing meaning to parts of a hidden feature vector or to individual neurons is hindered. We formulate interpretation as a translation of hidden representations onto semantic concepts that are comprehensible to the user. The mapping between both domains has to be bijective so that semantic modiﬁcations in the target domain correctly alter the original representation. The proposed invertible interpretation network can be transparently applied on top of existing architectures with no need to modify or retrain them. Consequently, we translate an original representation to an equivalent yet interpretable one and backwards without affecting the expressiveness and performance of the original. The invertible interpretation network disentangles the hidden representation into separate, semantically meaningful concepts. Moreover, we present an efﬁcient approach to deﬁne semantic concepts by only sketching two images and also an unsupervised strategy. Experimental evaluation demonstrates the wide applicability to interpretation of existing classiﬁcation and image generation networks as well as to semantically guided image manipulation.},
	language = {en},
	urldate = {2020-09-01},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
	month = jun,
	year = {2020},
	pages = {9220--9229},
	file = {Esser et al. - 2020 - A Disentangling Invertible Interpretation Network .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7X8XEESH\\Esser et al. - 2020 - A Disentangling Invertible Interpretation Network .pdf:application/pdf},
}

@article{reyes_interpretability_2020,
	title = {On the {Interpretability} of {Artificial} {Intelligence} in {Radiology}: {Challenges} and {Opportunities}},
	volume = {2},
	issn = {2638-6100},
	shorttitle = {On the {Interpretability} of {Artificial} {Intelligence} in {Radiology}},
	url = {http://pubs.rsna.org/doi/10.1148/ryai.2020190043},
	doi = {10.1148/ryai.2020190043},
	abstract = {Interpretability methods hold the potential to improve understanding, trust, and verification of radiology artificial intelligence systems; active involvement of the radiology community is necessary for their development and evaluation.},
	language = {en},
	number = {3},
	urldate = {2020-09-01},
	journal = {Radiology: Artificial Intelligence},
	author = {Reyes, Mauricio and Meier, Raphael and Pereira, Sérgio and Silva, Carlos A. and Dahlweid, Fried-Michael and Tengg-Kobligk, Hendrik von and Summers, Ronald M. and Wiest, Roland},
	month = may,
	year = {2020},
	pages = {e190043},
	file = {Reyes et al. - 2020 - On the Interpretability of Artificial Intelligence.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3938ST6B\\Reyes et al. - 2020 - On the Interpretability of Artificial Intelligence.pdf:application/pdf},
}

@article{leeb_structural_2020,
	title = {Structural {Autoencoders} {Improve} {Representations} for {Generation} and {Transfer}},
	url = {http://arxiv.org/abs/2006.07796},
	abstract = {We study the problem of structuring a learned representation to signiﬁcantly improve performance without supervision. Unlike most methods which focus on using side information like weak supervision or deﬁning new regularization objectives, we focus on improving the learned representation by structuring the architecture of the model. We propose a self-attention based architecture to make the encoder explicitly associate parts of the representation with parts of the input observation. Meanwhile, our structural decoder architecture encourages a hierarchical structure in the latent space, akin to structural causal models, and learns a natural ordering of the latent mechanisms. We demonstrate how these models learn a representation which improves results in a variety of downstream tasks including generation, disentanglement, and transfer using several challenging and natural image datasets.},
	language = {en},
	urldate = {2020-09-02},
	journal = {arXiv:2006.07796 [cs, stat]},
	author = {Leeb, Felix and Annadani, Yashas and Bauer, Stefan and Schölkopf, Bernhard},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.07796},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Leeb et al. - 2020 - Structural Autoencoders Improve Representations fo.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WKUZ8MN3\\Leeb et al. - 2020 - Structural Autoencoders Improve Representations fo.pdf:application/pdf},
}

@article{chen_isolating_2019,
	title = {Isolating {Sources} of {Disentanglement} in {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1802.04942},
	abstract = {We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate the β-TCVAE (Total Correlation Variational Autoencoder) algorithm, a reﬁnement and plug-in replacement of the β-VAE for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classiﬁer-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the model is trained using our framework.},
	language = {en},
	urldate = {2020-09-02},
	journal = {arXiv:1802.04942 [cs, stat]},
	author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
	month = apr,
	year = {2019},
	note = {arXiv: 1802.04942},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Chen et al. - 2019 - Isolating Sources of Disentanglement in Variationa.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4M33HI8P\\Chen et al. - 2019 - Isolating Sources of Disentanglement in Variationa.pdf:application/pdf},
}

@article{chen_infogan_2016,
	title = {{InfoGAN}: {Interpretable} {Representation} {Learning} by {Information} {Maximizing} {Generative} {Adversarial} {Nets}},
	shorttitle = {{InfoGAN}},
	url = {http://arxiv.org/abs/1606.03657},
	abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efﬁciently. Speciﬁcally, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods.},
	language = {en},
	urldate = {2020-09-02},
	journal = {arXiv:1606.03657 [cs, stat]},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03657},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8HQA9IRW\\Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by .pdf:application/pdf},
}

@article{kohler_laparoscopic_2020,
	title = {Laparoscopic system for simultaneous high-resolution video and rapid hyperspectral imaging in the visible and near-infrared spectral range},
	volume = {25},
	issn = {1083-3668, 1560-2281},
	url = {https://www.spiedigitallibrary.org/journals/Journal-of-Biomedical-Optics/volume-25/issue-8/086004/Laparoscopic-system-for-simultaneous-high-resolution-video-and-rapid-hyperspectral/10.1117/1.JBO.25.8.086004.short},
	doi = {10.1117/1.JBO.25.8.086004},
	abstract = {Significance: Hyperspectral imaging (HSI) can support intraoperative perfusion assessment, the identification of tissue structures, and the detection of cancerous lesions. The practical use of HSI for minimal-invasive surgery is currently limited, for example, due to long acquisition times, missing video, or large set-ups. Aim: An HSI laparoscope is described and evaluated to address the requirements for clinical use and high-resolution spectral imaging. Approach: Reflectance measurements with reference objects and resected human tissue from 500 to 1000 nm are performed to show the consistency with an approved medical HSI device for open surgery. Varying object distances are investigated, and the signal-to-noise ratio (SNR) is determined for different light sources. Results: The handheld design enables real-time processing and visualization of HSI data during acquisition within 4.6 s. A color video is provided simultaneously and can be augmented with spectral information from push-broom imaging. The reflectance data from the HSI system for open surgery at 50 cm and the HSI laparoscope are consistent for object distances up to 10 cm. A standard rigid laparoscope in combination with a customized LED light source resulted in a mean SNR of 30 to 43 dB (500 to 950 nm). Conclusions: Compact and rapid HSI with a high spatial- and spectral-resolution is feasible in clinical practice. Our work may support future studies on minimally invasive HSI to reduce intra- and postoperative complications.},
	number = {8},
	urldate = {2020-09-08},
	journal = {Journal of Biomedical Optics},
	author = {Köhler, Hannes and Kulcke, Axel and Maktabi, Marianne and Moulla, Yusef and Jansen-Winkeln, Boris and Barberio, Manuel and Diana, Michele and Gockel, Ines and Neumuth, Thomas and Chalopin, Claire},
	month = aug,
	year = {2020},
	note = {Publisher: International Society for Optics and Photonics},
	pages = {086004},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KZBGKVAN\\Köhler et al. - 2020 - Laparoscopic system for simultaneous high-resoluti.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7VXKJ2MG\\1.JBO.25.8.086004.html:text/html},
}

@article{cooney_comparison_2020,
	title = {Comparison of spectral characteristics in human and pig biliary system with hyperspectral imaging ({HSI})},
	volume = {6},
	issn = {2364-5504},
	url = {https://www.degruyter.com/view/journals/cdbme/6/1/article-20200012.xml},
	doi = {10.1515/cdbme-2020-0012},
	abstract = {Injuries to the biliary tree during surgical, endoscopic or invasive radiological diagnostic or therapeutic procedures involving the pancreas, liver or organs of the upper gastrointestinal tract give rise to the need to develop a method for clear discrimination of biliary anatomy from surrounding tissue. Hyperspectral imaging (HSI) is an emerging optical technique in disease diagnosis and image-guided surgery with inherent advantages of being a non-contact, non-invasive, and non-ionizing technique. HSI can produce quantitative diagnostic information about tissue pathology, morphology, and chemical composition. HSI was applied in human liver transplantation and compared to porcine model operations to assess the capability of discriminating biliary anatomy from surrounding biological tissue. Absorbance spectra measured from bile ducts, gall bladder, and liver show a dependence on tissue composition and bile concentration, with agreement between human and porcine datasets. The bile pigment biliverdin and structural proteins collagen and elastin were identified as contributors to the bile duct and gall bladder absorbance spectra.},
	language = {en},
	number = {1},
	urldate = {2020-09-30},
	journal = {Current Directions in Biomedical Engineering},
	author = {Cooney, Gary Sean and Barberio, Manuel and Diana, Michele and Sucher, Robert and Chalopin, Claire and Köhler, Hannes},
	month = sep,
	year = {2020},
	file = {Cooney et al. - 2020 - Comparison of spectral characteristics in human an.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RLTJI49A\\Cooney et al. - 2020 - Comparison of spectral characteristics in human an.pdf:application/pdf},
}

@article{guo_calibration_2017,
	title = {On {Calibration} of {Modern} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.04599},
	abstract = {Conﬁdence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classiﬁcation models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors inﬂuencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classiﬁcation datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a singleparameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
	language = {en},
	urldate = {2020-10-07},
	journal = {arXiv:1706.04599 [cs]},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
	month = aug,
	year = {2017},
	note = {arXiv: 1706.04599},
	keywords = {Computer Science - Machine Learning},
	file = {Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8MVUFB58\\Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf:application/pdf},
}

@article{lacis_hybrid_nodate,
	title = {Hybrid optical prototype for sepsis bedside diagnostics},
	abstract = {Severe sepsis accounts for a third of patient deaths in the intensive care units worldwide therefore early sepsis detection and therapy onset is crucial for reduction of patient mortality and expenditures. In the present study the device prototype for sepsis bedside diagnostics is proposed, by combining hyperspectral and thermal imaging. The diagnostic principle is based on evaluation of skin oxygenation and micro perfusion inhomogeneity. Preliminary results confirm early heterogeneity detection of skin oxygenation (hemoglobin saturation with oxygen) and temperature caused by oxygen utilization and blood perfusion impairments. The present findings indicate on high sensitivity of developed prototype compared to routine subjective visual assessment of patient’s skin by clinicians.},
	language = {en},
	author = {Lacis, M and Kazune, S and Marcinkevics, Z and Rubins, U and Grabovskis, A},
	pages = {8},
	file = {111050P.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\V4KUZ7K9\\111050P.pdf:application/pdf},
}

@book{saknite_novel_2017,
	title = {Novel hybrid technology for early diagnostics of sepsis},
	volume = {10057},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10057/100570F/Novel-hybrid-technology-for-early-diagnostics-of-sepsis/10.1117/12.2253597.short},
	abstract = {Sepsis is a potentially fatal disease with mortality rate as high as 50\% in patients with septic shock; mortality rate can increase by 7.6\% per hour if appropriate treatment is not started. Internationally accepted guidelines for diagnosis of sepsis rely on vital sign monitoring and laboratory tests in order to recognize organ failure. This pilot study aims to explore the potential of hyperspectral and thermal imaging techniques to identify and quantify early alterations in skin oxygenation and perfusion induced by sepsis. The study comprises both physiological model experiments on healthy volunteers in a laboratory environment, as well as screening case series of patients with septic shock in the intensive care department. Hyperspectral imaging is used to determine one of the main characteristic visual signs of skin oxygenation abnormalities - skin mottling, whereas changes in peripheral perfusion have been visualized by thermal imaging as heterogeneous skin temperature areas. In order to mimic septic skin mottling in a reproducible way in laboratory environment, arterial occlusion provocation test was utilized on healthy volunteers. Visualization of oxygen saturation by hyperspectral imaging allows diagnosing microcirculatory alterations induced by sepsis earlier than visual assessment of mottling. Thermal images of sepsis patients in the clinic clearly reveal hotspots produced by perforating arteries, as well as cold regions of low blood supply. The results of this pilot study show that thermal imaging in combination with hyperspectral imaging allows the determination of oxygen supply and utilization in critically ill septic patients.},
	urldate = {2020-12-10},
	publisher = {International Society for Optics and Photonics},
	author = {Saknite, Inga and Grabovskis, Andris and Kazune, Sigita and Rubins, Uldis and Marcinkevics, Zbignevs and Volceka, Karina and Kviesis-Kipge, Edgars and Spigulis, Janis},
	month = feb,
	year = {2017},
	file = {citation-10057_76.bib:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4PE2724F\\citation-10057_76.bib:application/x-bibtex;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\GKFY5B2M\\12.2253597.html:text/html},
}

@article{dietrich_bedside_2020,
	title = {Bedside hyperspectral imaging for the evaluation of microcirculatory alterations in perioperative intensive care medicine: a study protocol for an observational clinical pilot study ({HySpI}-{ICU})},
	volume = {10},
	copyright = {© Author(s) (or their employer(s)) 2020. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {2044-6055, 2044-6055},
	shorttitle = {Bedside hyperspectral imaging for the evaluation of microcirculatory alterations in perioperative intensive care medicine},
	url = {https://bmjopen.bmj.com/content/10/9/e035742},
	doi = {10.1136/bmjopen-2019-035742},
	abstract = {Introduction Normalisation of macrocirculatory parameters during resuscitation therapy does not guarantee the restoration of microcirculatory perfusion in critical illness due to haemodynamic incoherence. Persistent microcirculatory abnormalities are associated with severity of organ dysfunction and mandate the development of bedside microcirculatory monitoring. A novel hyperspectral imaging (HSI) system can visualise changes in skin perfusion, oxygenation and water content at the bedside. We aim to evaluate the effectiveness of HSI for bedside monitoring of skin microcirculation and the association of HSI parameters with organ dysfunction in patients with sepsis and major abdominal surgery.
Methods and analysis Three independent groups will be assessed and separately analysed within a clinical prospective observational study: (1) 25 patients with sepsis or septic shock (according to sepsis-3 criteria), (2) 25 patients undergoing pancreatic surgery and (3) 25 healthy controls. Patients with sepsis and patients undergoing pancreatic surgery will receive standard therapy according to local protocols derived from international guidelines. In addition, cardiac output of perioperative patients and patients with sepsis will be measured. Healthy controls undergo one standardised evaluation. The TIVITA Tissue System is a novel HSI system that uses the visible and near-infrared spectral light region to determine tissue microcirculatory parameters. HSI analysis (hand/knee) will be done in parallel to haemodynamic monitoring within defined intervals during a 72-hour observation period. HSI data will be correlated with the Sequential Organ Failure Assessment score, global haemodynamics, inflammation and glycocalyx markers, surgical complications and 30-day outcome.
Ethics and dissemination The protocol has been approved by the local ethics committee of the University of Heidelberg (S-148/2019). Study results will be submitted to peer-reviewed journals and medical conferences.
Trial registration number DRKS00017313; Pre-results.},
	language = {en},
	number = {9},
	urldate = {2020-12-10},
	journal = {BMJ Open},
	author = {Dietrich, Maximilian and Marx, Sebastian and Bruckner, Thomas and Nickel, Felix and Müller-Stich, Beat Peter and Hackert, Thilo and Weigand, Markus A. and Uhle, Florian and Brenner, Thorsten and Schmidt, Karsten},
	month = sep,
	year = {2020},
	keywords = {adult anaesthesia, adult intensive \& critical care, anaesthetics, intensive \& critical care},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\HHU7D8K8\\Dietrich et al. - 2020 - Bedside hyperspectral imaging for the evaluation o.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\M29ALEE7\\e035742.html:text/html},
}

@article{kazune_impact_2019,
	title = {Impact of increased mean arterial pressure on skin microcirculatory oxygenation in vasopressor-requiring septic patients: an interventional study},
	volume = {9},
	issn = {2110-5820},
	shorttitle = {Impact of increased mean arterial pressure on skin microcirculatory oxygenation in vasopressor-requiring septic patients},
	url = {https://doi.org/10.1186/s13613-019-0572-1},
	doi = {10.1186/s13613-019-0572-1},
	abstract = {Heterogeneity of microvascular blood flow leading to tissue hypoxia is a common finding in patients with septic shock. It may be related to suboptimal systemic perfusion pressure and lead to organ failure. Mapping of skin microcirculatory oxygen saturation and relative hemoglobin concentration using hyperspectral imaging allows to identify heterogeneity of perfusion and perform targeted measurement of oxygenation. We hypothesized that increasing mean arterial pressure would result in improved oxygenation in areas of the skin with most microvascular blood pooling.},
	number = {97},
	urldate = {2020-12-10},
	journal = {Annals of Intensive Care},
	author = {Kazune, Sigita and Caica, Anastasija and Luksevics, Einars and Volceka, Karina and Grabovskis, Andris},
	month = aug,
	year = {2019},
	keywords = {Mean arterial pressure, Noradrenaline, Septic shock, Tissue oxygenation},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2NXQ4PEN\\Kazune et al. - 2019 - Impact of increased mean arterial pressure on skin.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\REEVZIHF\\s13613-019-0572-1.html:text/html},
}

@article{gillies_systemic_2003,
	title = {Systemic {Effects} of {Shock} and {Resuscitation} {Monitored} by {Visible} {Hyperspectral} {Imaging}},
	volume = {5},
	issn = {1520-9156},
	url = {https://www.liebertpub.com/doi/abs/10.1089/152091503322527058},
	doi = {10.1089/152091503322527058},
	abstract = {Hyperspectral imaging (HSI) has been useful in monitoring several medical conditions, which to date have generally involved local changes in skin oxygenation of isolated regions of interest such as skin          flaps or small burns. Here, by contrast, we present a study in which HSI was used to assess the local cutaneous manifestations of significant systemic events. HSI of the ventral surface of the lower jaw          was used to monitor changes in skin oxygenation during hypovolemic shock induced by hemorrhage with additional pulmonary contusion injury in a porcine model, and to monitor the subsequent recovery of oxygenation          with resuscitation. Quantitative and qualitative changes were observed in the level of skin oxygenation during shock and recovery. Quantitative values were obtained by fitting reference spectra of oxyhemoglobin          and deoxyhemoglobin to sample spectra. Qualitative changes included changes in the observed spatial distribution or pattern of skin oxygenation. A mottled pattern of oxygen saturation was observed during          hemorrhagic shock, but not observed during hypovolemic shock or following resuscitation. Historically, the assessment of skin color and mottling has been an important, albeit inexact, component of resuscitation          algorithms. Now, it is possible to analyze these variables during shock and resuscitation in an objective manner. The clinical utility of these advances needs to be determined.},
	number = {5},
	urldate = {2020-12-10},
	journal = {Diabetes Technology \& Therapeutics},
	author = {Gillies, Robert and Freeman, Jenny E. and Cancio, Leopoldo C. and Brand, Derek and Hopmeier, Michael and Mansfield, James R.},
	month = oct,
	year = {2003},
	note = {Publisher: Mary Ann Liebert, Inc., publishers},
	pages = {847--855},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UJYFKPMC\\152091503322527058.html:text/html},
}

@phdthesis{wirkert_multispectral_2018,
	address = {Karlsruhe},
	title = {Multispectral image analysis in laparoscopy – {A} machine learning approach to live perfusion monitoring},
	school = {Karlsruher Institut für Technologie (KIT)},
	author = {Wirkert, Sebastian},
	month = jan,
	year = {2018},
	file = {Thesis_Wirkert.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\P5ZCBHCT\\Thesis_Wirkert.pdf:application/pdf},
}

@article{bhojanapalli_reproducibility_2021,
	title = {On the {Reproducibility} of {Neural} {Network} {Predictions}},
	url = {http://arxiv.org/abs/2102.03349},
	abstract = {Standard training techniques for neural networks involve multiple sources of randomness, e.g., initialization, mini-batch ordering and in some cases data augmentation. Given that neural networks are heavily over-parameterized in practice, such randomness can cause churn – for the same input, disagreements between predictions of the two models independently trained by the same algorithm, contributing to the ‘reproducibility challenges’ in modern machine learning. In this paper, we study this problem of churn, identify factors that cause it, and propose two simple means of mitigating it. We ﬁrst demonstrate that churn is indeed an issue, even for standard image classiﬁcation tasks (CIFAR and ImageNet), and study the role of the different sources of training randomness that cause churn. By analyzing the relationship between churn and prediction conﬁdences, we pursue an approach with two components for churn reduction. First, we propose using minimum entropy regularizers to increase prediction conﬁdences. Second, we present a novel variant of co-distillation approach [Anil et al., 2018] to increase model agreement and reduce churn. We present empirical results showing the effectiveness of both techniques in reducing churn while improving the accuracy of the underlying model.},
	language = {en},
	urldate = {2021-02-15},
	journal = {arXiv:2102.03349 [cs]},
	author = {Bhojanapalli, Srinadh and Wilber, Kimberly and Veit, Andreas and Rawat, Ankit Singh and Kim, Seungyeon and Menon, Aditya and Kumar, Sanjiv},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.03349},
	keywords = {Computer Science - Machine Learning},
	file = {Bhojanapalli et al. - 2021 - On the Reproducibility of Neural Network Predictio.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\65IC9NPX\\Bhojanapalli et al. - 2021 - On the Reproducibility of Neural Network Predictio.pdf:application/pdf},
}

@article{aix-covnet_common_2021,
	title = {Common pitfalls and recommendations for using machine learning to detect and prognosticate for {COVID}-19 using chest radiographs and {CT} scans},
	volume = {3},
	issn = {2522-5839},
	url = {http://www.nature.com/articles/s42256-021-00307-0},
	doi = {10.1038/s42256-021-00307-0},
	abstract = {Abstract
            Machine learning methods offer great promise for fast and accurate detection and prognostication of coronavirus disease 2019 (COVID-19) from standard-of-care chest radiographs (CXR) and chest computed tomography (CT) images. Many articles have been published in 2020 describing new machine learning-based models for both of these tasks, but it is unclear which are of potential clinical utility. In this systematic review, we consider all published papers and preprints, for the period from 1 January 2020 to 3 October 2020, which describe new machine learning models for the diagnosis or prognosis of COVID-19 from CXR or CT images. All manuscripts uploaded to bioRxiv, medRxiv and arXiv along with all entries in EMBASE and MEDLINE in this timeframe are considered. Our search identified 2,212 studies, of which 415 were included after initial screening and, after quality screening, 62 studies were included in this systematic review. Our review finds that none of the models identified are of potential clinical use due to methodological flaws and/or underlying biases. This is a major weakness, given the urgency with which validated COVID-19 models are needed. To address this, we give many recommendations which, if followed, will solve these issues and lead to higher-quality model development and well-documented manuscripts.},
	language = {en},
	number = {3},
	urldate = {2021-04-26},
	journal = {Nature Machine Intelligence},
	author = {{AIX-COVNET} and Roberts, Michael and Driggs, Derek and Thorpe, Matthew and Gilbey, Julian and Yeung, Michael and Ursprung, Stephan and Aviles-Rivero, Angelica I. and Etmann, Christian and McCague, Cathal and Beer, Lucian and Weir-McCall, Jonathan R. and Teng, Zhongzhao and Gkrania-Klotsas, Effrossyni and Rudd, James H. F. and Sala, Evis and Schönlieb, Carola-Bibiane},
	month = mar,
	year = {2021},
	pages = {199--217},
	file = {AIX-COVNET et al. - 2021 - Common pitfalls and recommendations for using mach.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\62KYE58W\\AIX-COVNET et al. - 2021 - Common pitfalls and recommendations for using mach.pdf:application/pdf},
}

@misc{gmbh_sepsis_nodate,
	title = {Sepsis: {Gefährliches} {Gefecht} im {Körper}},
	shorttitle = {Sepsis},
	url = {https://www.pharmazeutische-zeitung.de/ausgabe-042012/gefaehrliches-gefecht-im-koerper/},
	abstract = {Von Claudia Borchard-Tuch / Die Blutvergiftung oder Sepsis ist eine ernste Erkrankung, die tödlich enden kann. Nur wenn sie rechtzeitig erkannt wird, kann ...},
	language = {de},
	urldate = {2021-04-28},
	journal = {Pharmazeutische Zeitung online},
	author = {GmbH, Avoxa-Mediengruppe Deutscher Apotheker},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\R2RGPEZG\\gefaehrliches-gefecht-im-koerper.html:text/html},
}

@article{gillies_systemic_2003-1,
	title = {Systemic {Effects} of {Shock} and {Resuscitation} {Monitored} by {Visible} {Hyperspectral} {Imaging}},
	volume = {5},
	issn = {1520-9156, 1557-8593},
	url = {http://www.liebertpub.com/doi/10.1089/152091503322527058},
	doi = {10.1089/152091503322527058},
	abstract = {Hyperspectral imaging (HSI) has been useful in monitoring several medical conditions, which to date have generally involved local changes in skin oxygenation of isolated regions of interest such as skin flaps or small burns. Here, by contrast, we present a study in which HSI was used to assess the local cutaneous manifestations of significant systemic events. HSI of the ventral surface of the lower jaw was used to monitor changes in skin oxygenation during hypovolemic shock induced by hemorrhage with additional pulmonary contusion injury in a porcine model, and to monitor the subsequent recovery of oxygenation with resuscitation. Quantitative and qualitative changes were observed in the level of skin oxygenation during shock and recovery. Quantitative values were obtained by fitting reference spectra of oxyhemoglobin and deoxyhemoglobin to sample spectra. Qualitative changes included changes in the observed spatial distribution or pattern of skin oxygenation. A mottled pattern of oxygen saturation was observed during hemorrhagic shock, but not observed during hypovolemic shock or following resuscitation. Historically, the assessment of skin color and mottling has been an important, albeit inexact, component of resuscitation algorithms. Now, it is possible to analyze these variables during shock and resuscitation in an objective manner. The clinical utility of these advances needs to be determined.},
	language = {en},
	number = {5},
	urldate = {2021-05-01},
	journal = {Diabetes Technology \& Therapeutics},
	author = {Gillies, Robert and Freeman, Jenny E. and Cancio, Leopoldo C. and Brand, Derek and Hopmeier, Michael and Mansfield, James R.},
	month = oct,
	year = {2003},
	pages = {847--855},
	file = {Gillies et al. - 2003 - Systemic Effects of Shock and Resuscitation Monito.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\56TFWG2X\\Gillies et al. - 2003 - Systemic Effects of Shock and Resuscitation Monito.pdf:application/pdf},
}

@article{dietrich_bedside_2021,
	title = {Bedside hyperspectral imaging indicates a microcirculatory sepsis pattern - an observational study},
	volume = {136},
	issn = {0026-2862},
	url = {https://www.sciencedirect.com/science/article/pii/S0026286221000340},
	doi = {10.1016/j.mvr.2021.104164},
	abstract = {Introduction
Microcirculatory alterations are key mechanisms in sepsis pathophysiology leading to tissue hypoxia, edema formation, and organ dysfunction. Hyperspectral imaging (HSI) is an emerging imaging technology that uses tissue-light interactions to evaluate biochemical tissue characteristics including tissue oxygenation, hemoglobin content and water content. Currently, clinical data for HSI technologies in critical ill patients are still limited.
Methods and analysis
TIVITA® Tissue System was used to measure Tissue oxygenation (StO2), Tissue Hemoglobin Index (THI), Near Infrared Perfusion Index (NPI) and Tissue Water Index (TWI) in 25 healthy volunteers and 25 septic patients. HSI measurement sites were the palm, the fingertip, and a suprapatellar knee area. Septic patients were evaluated on admission to the ICU (E), 6 h afterwards (E+6) and three times a day (t3-t9) within a total observation period of 72 h. Primary outcome was the correlation of HSI results with daily SOFA-scores.
Results
Serial HSI at the three measurement sites in healthy volunteers showed a low mean variance expressing high retest reliability. HSI at E demonstrated significantly lower StO2 and NPI as well as higher TWI at the palm and fingertip in septic patients compared to healthy volunteers. StO2 and TWI showed corresponding results at the suprapatellar knee area. In septic patients, palm and fingertip THI identified survivors (E-t4) and revealed predictivity for 28-day mortality (E). Fingertip StO2 and THI correlated to SOFA-score on day 2. TWI was consistently increased in relation to the TWI range of healthy controls during the observation time. Palm TWI correlated positively with SOFA scores on day 3.
Discussion
HSI results in septic patients point to a distinctive microcirculatory pattern indicative of reduced skin oxygenation and perfusion quality combined with increased blood pooling and tissue water content. THI might possess risk-stratification properties and TWI could allow tissue edema evaluation in critically ill patients.
Conclusion
HSI technologies could open new perspectives in microcirculatory monitoring by visualizing oxygenation and perfusion quality combined with tissue water content in critically ill patients – a prerequisite for future tissue perfusion guided therapy concepts in intensive care medicine.},
	language = {en},
	number = {104164},
	urldate = {2021-05-01},
	journal = {Microvascular Research},
	author = {Dietrich, M. and Marx, S. and von der Forst, M. and Bruckner, T. and Schmitt, F. C. F. and Fiedler, M. O. and Nickel, F. and Studier-Fischer, A. and Müller-Stich, B. P. and Hackert, T. and Brenner, T. and Weigand, M. A. and Uhle, F. and Schmidt, K.},
	month = jul,
	year = {2021},
	keywords = {Hyperspectral imaging, Tissue oxygenation, Critical care, Microcirculatory monitoring, Sepsis, Tissue water content},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\M2YVHPPC\\Dietrich et al. - 2021 - Bedside hyperspectral imaging indicates a microcir.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\R6GB59PP\\S0026286221000340.html:text/html},
}

@article{kazune_relationship_2019,
	title = {Relationship of mottling score, skin microcirculatory perfusion indices and biomarkers of endothelial dysfunction in patients with septic shock: an observational study},
	volume = {23},
	issn = {1364-8535},
	shorttitle = {Relationship of mottling score, skin microcirculatory perfusion indices and biomarkers of endothelial dysfunction in patients with septic shock},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6739999/},
	doi = {10.1186/s13054-019-2589-0},
	abstract = {Background
In patients with septic shock, the skin is often chosen for the evaluation of peripheral perfusion and oxygenation. Changes in skin microcirculatory vessel oxygen saturation and relative hemoglobin concentration can be described using a mottling score or captured with hyperspectral imaging. However, the effectiveness of the mottling score in assessing microcirculation remains to be shown. We hypothesize that the mottling score in patients with septic shock is related to skin microcirculatory perfusion indices quantified by hyperspectral imaging, biomarkers that reflect endothelium activation and damage, and clinical outcome.

Methods
Hyperspectral imaging of the knee area was performed in 95 intensive care patients with septic shock enrolled in a single-center observational study to obtain relative oxy/deoxyhemoglobin concentration values and construct anatomical maps of skin microcirculatory saturation. The blood was sampled to obtain concentrations of thrombomodulin, plasminogen activator inhibitor-1 (PAI-1), soluble intercellular adhesion molecule-1 (ICAM-1), soluble vascular cell adhesion molecule-1 (VCAM-1), angiopoietin-2, and syndecan-1. The spectrophotometrically obtained skin microvascular perfusion indices were compared to the mottling score and biomarker concentration. The association between mottling score, skin microcirculatory perfusion indices, and 28-day mortality was also analyzed.

Results
Microcirculatory oxygen saturation was significantly lower and total hemoglobin concentration was significantly higher in patients with a mottling score of 2 compared to those with a score of 0 (p = 0.02), with no difference between other scores. We found an association between microcirculatory oxygen saturation and PAI-1 levels (rho = − 0.3; p = 0.007). Increased mottling score and decreased microcirculatory oxygen saturation were predictive of 28-day mortality (mottling score 2 vs 0: OR 15.31, 95\% CI 4.12–68.11; microcirculatory oxygen saturation: OR 0.90, 95\% CI 0.85–0.95). Endothelial biomarkers did not increase the predictive value of skin microcirculatory perfusion indices.

Conclusions
Higher mottling scores are associated with lower microcirculatory oxygen saturation but with significant overlap between scores. Microcirculatory oxygen saturation is a quantitative measure of peripheral oxygenation and is more specific than the mottling score in predicting 28-day mortality.

Electronic supplementary material
The online version of this article (10.1186/s13054-019-2589-0) contains supplementary material, which is available to authorized users.},
	urldate = {2021-05-01},
	journal = {Critical Care},
	author = {Kazune, Sigita and Caica, Anastasija and Volceka, Karina and Suba, Olegs and Rubins, Uldis and Grabovskis, Andris},
	month = sep,
	year = {2019},
	pmid = {31511042},
	pmcid = {PMC6739999},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\K2GPSWR9\\Kazune et al. - 2019 - Relationship of mottling score, skin microcirculat.pdf:application/pdf},
}

@article{grambow_evaluation_2019,
	title = {Evaluation of peripheral artery disease with the {TIVITA}® {Tissue} hyperspectral imaging camera system},
	volume = {73},
	issn = {13860291, 18758622},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/CH-199215},
	doi = {10.3233/CH-199215},
	abstract = {BACKGROUND: Objective, reliable and easy screening for peripheral artery disease (PAD) is essential to conﬁrm the diagnosis and initiate the respective treatment. Therefore, a new non-invasive hyperspectral camera (TIVITA® Tissue) was tested in patients with and without PAD.
OBJECTIVE: It was hypothesized that the oxygenation parameters of the TIVITA® Tissue correlate to established modalities for detection of PAD and allow differentiation between individuals with and without PAD.
METHODS: Evaluation of tissue oxygenation was performed in the angiosome of the medial plantar artery in 25 healthy young people and in 24 patients with and 25 patients without PAD in comparable age. Thereby, superﬁcial oxygenation (StO2) and near-infrared (NIR) perfusion index were measured with the TIVITA® Tissue. Additionally, the ankle-brachial-index (ABI), the complaint free walking distance and the vascular quality of life were assessed and demographic data were obtained from all participants.
RESULTS: TIVITA® Tissue analysis revealed signiﬁcantly reduced StO2 and NIR perfusion index in PAD compared to healthy young participants and patients without PAD. StO2 and NIR perfusion index positively correlated with ABI, the complaint free walking distance and the vascular quality of life score.
CONCLUSIONS: In summary, this new hyperspectral imaging camera bears great potential for PAD screening as well as for follow up.},
	language = {en},
	number = {1},
	urldate = {2021-05-01},
	journal = {Clinical Hemorheology and Microcirculation},
	author = {Grambow, Eberhard and Dau, Michael and Sandkühler, Niels Arne and Leuchter, Matthias and Holmer, Amadeus and Klar, Ernst and Weinrich, Malte},
	editor = {Wiggermann, P. and Krüger-Genge, A. and Jung, F.},
	month = nov,
	year = {2019},
	pages = {3--17},
	file = {Grambow et al. - 2019 - Evaluation of peripheral artery disease with the T.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MKQ52VSS\\Grambow et al. - 2019 - Evaluation of peripheral artery disease with the T.pdf:application/pdf},
}

@article{cecconi_sepsis_2018,
	title = {Sepsis and septic shock},
	volume = {392},
	issn = {0140-6736, 1474-547X},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(18)30696-2/abstract},
	doi = {10.1016/S0140-6736(18)30696-2},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}p{\textgreater}Sepsis is a common condition that is associated with unacceptably high mortality and, for many of those who survive, long-term morbidity. Increased awareness of the condition resulting from ongoing campaigns and the evidence arising from research in the past 10 years have increased understanding of this problem among clinicians and lay people, and have led to improved outcomes. The World Health Assembly and WHO made sepsis a global health priority in 2017 and have adopted a resolution to improve the prevention, diagnosis, and management of sepsis. In 2016, a new definition of sepsis (Sepsis-3) was developed. Sepsis is now defined as infection with organ dysfunction. This definition codifies organ dysfunction using the Sequential Organ Failure Assessment score. Ongoing research aims to improve definition of patient populations to allow for individualised management strategies matched to a patient's molecular and biochemical profile. The search continues for improved diagnostic techniques that can facilitate this aim, and for a pharmacological agent that can improve outcomes by modifying the disease process. While waiting for this goal to be achieved, improved basic care driven by education and quality-improvement programmes offers the best hope of increasing favourable outcomes.{\textless}/p{\textgreater}},
	language = {English},
	number = {10141},
	urldate = {2021-05-01},
	journal = {The Lancet},
	author = {Cecconi, Maurizio and Evans, Laura and Levy, Mitchell and Rhodes, Andrew},
	month = jul,
	year = {2018},
	pmid = {29937192},
	note = {Publisher: Elsevier},
	pages = {75--87},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ZAYA58S3\\Cecconi et al. - 2018 - Sepsis and septic shock.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CMZE8JJN\\fulltext.html:text/html},
}

@incollection{butler_sepsis_2019,
	address = {Cham},
	title = {Sepsis and {Septic} {Shock}: {A} {Review} of {Definitions}, {Pathogenesis}, and {Treatment}},
	isbn = {978-3-030-17148-3},
	shorttitle = {Sepsis and {Septic} {Shock}},
	url = {https://doi.org/10.1007/978-3-030-17148-3_22},
	abstract = {An improved definition(sepsis-3) of sepsis and septic shock in 2016 shifted focus from inflammatory criteria to organ dysfunction, hypotension, and blood lactate concentrations. Early Goal-Directed Therapy and Surviving Sepsis Campaign guidelines have improved outcomes, but mortality remained high in a range of 20-30\% due in part to advancing age and underlying diseases of patients. Additionally, recommended fluid volumes for resuscitation and hemodynamic interventions did not show survival benefit. Clinical trials in the past 25 years to identify new anti-sepsis therapies failed to bring into our armamentarium several candidate drugs that targeted single molecules, including anti-endotoxin antibodies, synthetic lipid A to block Toll-like receptor 4, anti-cytokine therapies, an inhibitor of nitric oxide synthase, and inhibitors of blood coagulation because promising benefits in animal models did not translate well into human application. Life-saving therapies are initiating antimicrobial drugs promptly in the first hour after diagnosis and employing norepinephrine as the vasopressor of choice.},
	language = {en},
	urldate = {2021-05-01},
	booktitle = {Endotoxin {Detection} and {Control} in {Pharma}, {Limulus}, and {Mammalian} {Systems}},
	publisher = {Springer International Publishing},
	author = {Butler, Thomas and Levin, Jack},
	editor = {Williams, Kevin L.},
	year = {2019},
	doi = {10.1007/978-3-030-17148-3_22},
	keywords = {Septic shock, Sepsis, Early goal-directed therapy, Multiple organ dysfunction syndrome, Surviving sepsis campaign},
	pages = {807--835},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\963XVSUY\\Butler und Levin - 2019 - Sepsis and Septic Shock A Review of Definitions, .pdf:application/pdf},
}

@article{gruartmoner_microcirculatory_2017,
	title = {Microcirculatory monitoring in septic patients: {Where} do we stand?},
	volume = {41},
	issn = {2173-5727},
	shorttitle = {Microcirculatory monitoring in septic patients},
	url = {http://www.medintensiva.org/en-microcirculatory-monitoring-in-septic-patients-articulo-resumen-S0210569116302650},
	doi = {10.1016/j.medin.2016.11.011},
	abstract = {Microcirculatory alterations play a pivotal role in sepsis-related morbidity and mortality.},
	language = {en},
	number = {1},
	urldate = {2021-05-01},
	journal = {Medicina Intensiva (English Edition)},
	author = {Gruartmoner, G. and Mesquida, J. and Ince, C.},
	month = jan,
	year = {2017},
	note = {Publisher: Elsevier},
	pages = {44--52},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ANS46TEE\\en-microcirculatory-monitoring-in-septic-patients-articulo-resumen-S0210569116302650.html:text/html},
}

@misc{noauthor_septic_2015,
	title = {Septic {Shock}},
	url = {https://clinicalgate.com/septic-shock/},
	abstract = {Visit the post for more.},
	language = {en-US},
	urldate = {2021-05-01},
	journal = {Clinical Gate},
	month = mar,
	year = {2015},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EP9IYTTX\\septic-shock.html:text/html},
}

@article{vincent_sepsis_2016,
	title = {Sepsis: older and newer concepts},
	volume = {4},
	issn = {22132600},
	shorttitle = {Sepsis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2213260015005226},
	doi = {10.1016/S2213-2600(15)00522-6},
	language = {en},
	number = {3},
	urldate = {2021-05-01},
	journal = {The Lancet Respiratory Medicine},
	author = {Vincent, Jean-Louis and Mira, Jean-Paul and Antonelli, Massimo},
	month = mar,
	year = {2016},
	pages = {237--240},
	file = {Vincent et al. - 2016 - Sepsis older and newer concepts.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\N9TAIMLU\\Vincent et al. - 2016 - Sepsis older and newer concepts.pdf:application/pdf},
}

@article{schramowski_making_2020,
	title = {Making deep neural networks right for the right scientific reasons by interacting with their explanations},
	url = {http://arxiv.org/abs/2001.05371},
	abstract = {Deep neural networks have shown excellent performances in many real-world applications. Unfortunately, they may show “Clever Hans”-like behavior—making use of confounding factors within datasets—to achieve high performance. In this work, we introduce the novel learning setting of “explanatory interactive learning” (XIL) and illustrate its beneﬁts on a plant phenotyping research task. XIL adds the scientist into the training loop such that she interactively revises the original model via providing feedback on its explanations. Our experimental results demonstrate that XIL can help avoiding Clever Hans moments in machine learning and encourages (or discourages, if appropriate) trust into the underlying model.},
	language = {en},
	urldate = {2021-05-01},
	journal = {arXiv:2001.05371 [cs, stat]},
	author = {Schramowski, Patrick and Stammer, Wolfgang and Teso, Stefano and Brugger, Anna and Shao, Xiaoting and Luigs, Hans-Georg and Mahlein, Anne-Katrin and Kersting, Kristian},
	month = jun,
	year = {2020},
	note = {arXiv: 2001.05371},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Schramowski et al. - 2020 - Making deep neural networks right for the right sc.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4C9V9XNH\\Schramowski et al. - 2020 - Making deep neural networks right for the right sc.pdf:application/pdf},
}

@article{vanbrabant_multitemporal_2019,
	title = {Multitemporal {Chlorophyll} {Mapping} in {Pome} {Fruit} {Orchards} from {Remotely} {Piloted} {Aircraft} {Systems}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/11/12/1468},
	doi = {10.3390/rs11121468},
	abstract = {Early and precise spatio-temporal monitoring of tree vitality is key for steering management decisions in pome fruit orchards. Spaceborne remote sensing instruments face a tradeoff between spatial and spectral resolution, while manned aircraft sensor-platform systems are very expensive. In order to address the shortcomings of these platforms, this study investigates the potential of Remotely Piloted Aircraft Systems (RPAS) to facilitate rapid, low cost, and flexible chlorophyll monitoring. Due to the complexity of orchard scenery a robust chlorophyll retrieval model on RPAS level has not yet been developed. In this study, specific focus therefore lies on evaluating the sensitivity of retrieval models to confounding factors. For this study, multispectral and hyperspectral imagery was collected over pome fruit orchards. Sensitivities of both univariate and multivariate retrieval models were demonstrated under different species, phenology, shade, and illumination scenes. Results illustrate that multivariate models have a significantly higher accuracy than univariate models as the former provide accuracies for the canopy chlorophyll content retrieval of R2 = 0.80 and Relative Root Mean Square Error (RRMSE) = 12\% for the hyperspectral sensor. Random forest regression on multispectral imagery (R2 \&gt; 0.9 for May, June, July, and August, and R2 = 0.5 for October) and hyperspectral imagery (0.6 \&lt; R2 \&lt; 0.9) led to satisfactory high and consistent accuracies for all months.},
	language = {en},
	number = {12},
	urldate = {2021-05-02},
	journal = {Remote Sensing},
	author = {Vanbrabant, Yasmin and Tits, Laurent and Delalieux, Stephanie and Pauly, Klaas and Verjans, Wim and Somers, Ben},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {chlorophyll, fruit orchards, hyperspectral remote sensing, multispectral remote sensing, multivariate, random forest, RPAS},
	pages = {1468},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\GUPY2QSG\\Vanbrabant et al. - 2019 - Multitemporal Chlorophyll Mapping in Pome Fruit Or.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Z3VEZ8QV\\htm.html:text/html},
}

@article{shao_right_nodate,
	title = {Right for {Better} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Influence} {Function}},
	abstract = {Explaining black-box models such as deep neural networks is becoming increasingly important as it helps to boost trust and debugging. Popular forms of explanations map the features to a vector indicating their individual importance to a decision on the instance-level. They can then be used to prevent the model from learning the wrong bias in data possibly due to ambiguity. For instance, Ross et al.’s “right for the right reasons” propagates user explanations backwards to the network by formulating differentiable constraints based on input gradients. Unfortunately, input gradients as well as many other widely used explanation methods form an approximation of the decision boundary and assume the underlying model to be ﬁxed. Here, we demonstrate how to make use of inﬂuence functions—a well known robust statistic—in the constraints to correct the models behaviour more effectively. Our empirical evidence demonstrates that this “right for better reasons”(RBR) considerably reduces the time to correct the classiﬁer at training time and boosts the quality of explanations at inference time compared to input gradients.},
	language = {en},
	author = {Shao, Xiaoting and Skryagin, Arseny and Stammer, Wolfgang and Schramowski, Patrick and Kersting, Kristian},
	pages = {8},
	file = {Shao et al. - Right for Better Reasons Training Differentiable .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CCX4Q8PM\\Shao et al. - Right for Better Reasons Training Differentiable .pdf:application/pdf},
}

@article{stammer_right_2021,
	title = {Right for the {Right} {Concept}: {Revising} {Neuro}-{Symbolic} {Concepts} by {Interacting} with their {Explanations}},
	shorttitle = {Right for the {Right} {Concept}},
	url = {http://arxiv.org/abs/2011.12854},
	abstract = {Most explanation methods in deep learning map importance estimates for a model’s prediction back to the original input space. These “visual” explanations are often insufﬁcient, as the model’s actual concept remains elusive. Moreover, without insights into the model’s semantic concept, it is difﬁcult —if not impossible— to intervene on the model’s behavior via its explanations, called Explanatory Interactive Learning. Consequently, we propose to intervene on a Neuro-Symbolic scene representation, which allows one to revise the model on the semantic level, e.g. “never focus on the color to make your decision”. We compiled a novel confounded visual scene data set, the CLEVR-Hans data set, capturing complex compositions of different objects. The results of our experiments on CLEVR-Hans demonstrate that our semantic explanations, i.e. compositional explanations at a per-object level, can identify confounders that are not identiﬁable using “visual” explanations only. More importantly, feedback on this semantic level makes it possible to revise the model from focusing on these factors.},
	language = {en},
	urldate = {2021-05-02},
	journal = {arXiv:2011.12854 [cs]},
	author = {Stammer, Wolfgang and Schramowski, Patrick and Kersting, Kristian},
	month = mar,
	year = {2021},
	note = {arXiv: 2011.12854},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Stammer et al. - 2021 - Right for the Right Concept Revising Neuro-Symbol.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\P3HKQ4X9\\Stammer et al. - 2021 - Right for the Right Concept Revising Neuro-Symbol.pdf:application/pdf},
}

@article{paoletti_capsule_2019,
	title = {Capsule {Networks} for {Hyperspectral} {Image} {Classification}},
	volume = {57},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2018.2871782},
	abstract = {Convolutional neural networks (CNNs) have recently exhibited an excellent performance in hyperspectral image classification tasks. However, the straightforward CNN-based network architecture still finds obstacles when effectively exploiting the relationships between hyperspectral imaging (HSI) features in the spectral-spatial domain, which is a key factor to deal with the high level of complexity present in remotely sensed HSI data. Despite the fact that deeper architectures try to mitigate these limitations, they also find challenges with the convergence of the network parameters, which eventually limit the classification performance under highly demanding scenarios. In this paper, we propose a new CNN architecture based on spectral-spatial capsule networks in order to achieve a highly accurate classification of HSIs while significantly reducing the network design complexity. Specifically, based on Hinton's capsule networks, we develop a CNN model extension that redefines the concept of capsule units to become spectral-spatial units specialized in classifying remotely sensed HSI data. The proposed model is composed by several building blocks, called spectral-spatial capsules, which are able to learn HSI spectral-spatial features considering their corresponding spatial positions in the scene, their associated spectral signatures, and also their possible transformations. Our experiments, conducted using five well-known HSI data sets and several state-of-the-art classification methods, reveal that our HSI classification approach based on spectral-spatial capsules is able to provide competitive advantages in terms of both classification accuracy and computational time.},
	number = {4},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Paoletti, Mercedes E. and Haut, Juan Mario and Fernandez-Beltran, Ruben and Plaza, Javier and Plaza, Antonio and Li, Jun and Pla, Filiberto},
	month = apr,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Hyperspectral imaging, Feature extraction, Training, Capsule networks (CapsNets), Complexity theory, convolutional neural networks (CNNs), Data models, hyperspectral imaging (HSI)},
	pages = {2145--2160},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ATSEZS7A\\8509610.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3LF9EG87\\Paoletti et al. - 2019 - Capsule Networks for Hyperspectral Image Classific.pdf:application/pdf},
}

@article{paoletti_deep_2019,
	title = {Deep learning classifiers for hyperspectral imaging: {A} review},
	volume = {158},
	issn = {0924-2716},
	shorttitle = {Deep learning classifiers for hyperspectral imaging},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271619302187},
	doi = {10.1016/j.isprsjprs.2019.09.006},
	abstract = {Advances in computing technology have fostered the development of new and powerful deep learning (DL) techniques, which have demonstrated promising results in a wide range of applications. Particularly, DL methods have been successfully used to classify remotely sensed data collected by Earth Observation (EO) instruments. Hyperspectral imaging (HSI) is a hot topic in remote sensing data analysis due to the vast amount of information comprised by this kind of images, which allows for a better characterization and exploitation of the Earth surface by combining rich spectral and spatial information. However, HSI poses major challenges for supervised classification methods due to the high dimensionality of the data and the limited availability of training samples. These issues, together with the high intraclass variability (and interclass similarity) –often present in HSI data– may hamper the effectiveness of classifiers. In order to solve these limitations, several DL-based architectures have been recently developed, exhibiting great potential in HSI data interpretation. This paper provides a comprehensive review of the current-state-of-the-art in DL for HSI classification, analyzing the strengths and weaknesses of the most widely used classifiers in the literature. For each discussed method, we provide quantitative results using several well-known and widely used HSI scenes, thus providing an exhaustive comparison of the discussed techniques. The paper concludes with some remarks and hints about future challenges in the application of DL techniques to HSI classification. The source codes of the methods discussed in this paper are available from: https://github.com/mhaut/hyperspectral\_deeplearning\_review.},
	language = {en},
	urldate = {2021-05-02},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Paoletti, M. E. and Haut, J. M. and Plaza, J. and Plaza, A.},
	month = dec,
	year = {2019},
	keywords = {Classification, Deep learning (DL), Earth observation (EO), Hyperspectral imaging (HSI)},
	pages = {279--317},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VCKA5NTU\\Paoletti et al. - 2019 - Deep learning classifiers for hyperspectral imagin.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2STJM2FD\\S0924271619302187.html:text/html},
}

@article{gewali_machine_2019,
	title = {Machine learning based hyperspectral image analysis: {A} survey},
	shorttitle = {Machine learning based hyperspectral image analysis},
	url = {http://arxiv.org/abs/1802.08701},
	abstract = {Hyperspectral sensors enable the study of the chemical and physical properties of scene materials remotely for the purpose of identiﬁcation, detection, chemical composition analysis, and physical parameter estimation of objects in the environment. Hence, hyperspectral images captured from earth observing satellites and aircraft have been increasingly important in agriculture, environmental monitoring, urban planning, mining, and defense. Machine learning algorithms due to their outstanding predictive power have become a key tool for modern hyperspectral image analysis. Therefore, a solid understanding of machine learning techniques have become essential for remote sensing researchers and practitioners. This paper surveys and compares recent machine learning-based hyperspectral image analysis methods published in literature. We organize the methods by the image analysis task and by the type of machine learning algorithm, and present a two-way mapping between the image analysis tasks and the types of machine learning algorithms that can be applied to them. The paper is comprehensive in coverage of both hyperspectral image analysis tasks and machine learning algorithms. The image analysis tasks considered are land cover classiﬁcation, target/anomaly detection, unmixing, and physical/chemical parameter estimation. The machine learning algorithms covered are Gaussian models, linear regression, logistic regression, support vector machines, Gaussian mixture model, latent linear models, sparse linear models, Gaussian mixture models, ensemble learning, directed graphical models, undirected graphical models, clustering, Gaussian processes, Dirichlet processes, and deep learning. We also discuss the open challenges in the ﬁeld of hyperspectral image analysis and explore possible future directions.},
	language = {en},
	urldate = {2021-05-02},
	journal = {arXiv:1802.08701 [cs, eess]},
	author = {Gewali, Utsav B. and Monteiro, Sildomar T. and Saber, Eli},
	month = feb,
	year = {2019},
	note = {arXiv: 1802.08701},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Computer Vision and Pattern Recognition},
	file = {Gewali et al. - 2019 - Machine learning based hyperspectral image analysi.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PC8VL8EY\\Gewali et al. - 2019 - Machine learning based hyperspectral image analysi.pdf:application/pdf},
}

@article{nalepa_hyperspectral_2019,
	title = {Hyperspectral {Data} {Augmentation}},
	url = {http://arxiv.org/abs/1903.05580},
	abstract = {Data augmentation is a popular technique which helps improve generalization capabilities of deep neural networks. It plays a pivotal role in remote-sensing scenarios in which the amount of high-quality ground truth data is limited, and acquiring new examples is costly or impossible. This is a common problem in hyperspectral imaging, where manual annotation of image data is difficult, expensive, and prone to human bias. In this letter, we propose online data augmentation of hyperspectral data which is executed during the inference rather than before the training of deep networks. This is in contrast to all other state-of-the-art hyperspectral augmentation algorithms which increase the size (and representativeness) of training sets. Additionally, we introduce a new principal component analysis based augmentation. The experiments revealed that our data augmentation algorithms improve generalization of deep networks, work in real-time, and the online approach can be effectively combined with offline techniques to enhance the classification accuracy.},
	urldate = {2021-05-02},
	journal = {arXiv:1903.05580 [cs]},
	author = {Nalepa, Jakub and Myller, Michal and Kawulok, Michal},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.05580},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\M2JXJ6NG\\Nalepa et al. - 2019 - Hyperspectral Data Augmentation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\LV6JML2J\\1903.html:text/html},
}

@article{zhu_generative_2018,
	title = {Generative {Adversarial} {Networks} for {Hyperspectral} {Image} {Classification}},
	volume = {56},
	issn = {0196-2892, 1558-0644},
	url = {https://ieeexplore.ieee.org/document/8307247/},
	doi = {10.1109/TGRS.2018.2805286},
	abstract = {A Generative Adversarial Network (GAN) usually contains a generative network and a discriminative network in competition with each other. The GAN has shown their capability in a variety of applications. In this paper, the usefulness and effectiveness of GAN for classification of hyperspectral images (HSIs) is explored for the first time. In the proposed GAN, a convolutional neural network (CNN) is designed to discriminate the inputs and another CNN is used to generate so-called fake inputs. The aforementioned CNNs are trained together: The generative CNN tries to generate fake inputs that are as real as possible, and the discriminative CNN tries to classify the real and fake inputs. This kind of adversarial training improves the generalization capability of the discriminative CNN, which is really important when the training samples are limited. Specifically, we propose two schemes: (1) a well-designed 1D-GAN as a spectral classifier, and (2) a robust 3D-GAN is as a spectral-spatial classifier. Furthermore, the generated adversarial samples are used with real training samples to fine-tune the discriminative CNN, which improves the final classification performance. The proposed classifiers are carried out on three widely-used hyperspectral datasets: Salinas, Indiana Pines, and Kennedy Space Center. The obtained results reveal that the proposed models provide competitive results compared to the state-of-the-art methods. In addition, the proposed GANs open new opportunities in the remote sensing community for the challenging task of HSIs classification, and also, reveal the huge potential of GAN-based methods for the analysis of such complex and inherently nonlinear data.},
	language = {en},
	number = {9},
	urldate = {2021-05-02},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Zhu, Lin and Chen, Yushi and Ghamisi, Pedram and Benediktsson, Jon Atli},
	month = sep,
	year = {2018},
	pages = {5046--5063},
	file = {Zhu et al. - 2018 - Generative Adversarial Networks for Hyperspectral .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\L6X45JP7\\Zhu et al. - 2018 - Generative Adversarial Networks for Hyperspectral .pdf:application/pdf},
}

@article{pan_hyperspectral_2021,
	title = {Hyperspectral {Image} {Classification} across {Different} {Datasets}: {A} {Generalization} to {Unseen} {Categories}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Hyperspectral {Image} {Classification} across {Different} {Datasets}},
	url = {https://www.mdpi.com/2072-4292/13/9/1672},
	doi = {10.3390/rs13091672},
	abstract = {With the rapid developments of hyperspectral imaging, the cost of collecting hyperspectral data has been lower, while the demand for reliable and detailed hyperspectral annotations has been much more substantial. However, limited by the difficulties of labelling annotations, most existing hyperspectral image (HSI) classification methods are trained and evaluated on a single hyperspectral data cube. It brings two significant challenges. On the one hand, many algorithms have reached a nearly perfect classification accuracy, but their trained models are hard to generalize to other datasets. On the other hand, since different hyperspectral datasets are usually not collected in the same scene, different datasets will contain different classes. To address these issues, in this paper, we propose a new paradigm for HSI classification, which is training and evaluating separately across different hyperspectral datasets. It is of great help to labelling hyperspectral data. However, it has rarely been studied in the hyperspectral community. In this work, we utilize a three-phase scheme, including feature embedding, feature mapping, and label reasoning. More specifically, we select a pair of datasets acquired by the same hyperspectral sensor, and the classifier learns from one dataset and then evaluated it on the other. Inspired by the latest advances in zero-shot learning, we introduce label semantic representation to establish associations between seen categories in the training set and unseen categories in the testing set. Extensive experiments on two pairs of datasets with different comparative methods have shown the effectiveness and potential of zero-shot learning in HSI classification.},
	language = {en},
	number = {9},
	urldate = {2021-05-02},
	journal = {Remote Sensing},
	author = {Pan, Erting and Ma, Yong and Fan, Fan and Mei, Xiaoguang and Huang, Jun},
	month = jan,
	year = {2021},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, classification across datasets, hyperspectral image, semantic representation, zero-shot learning},
	pages = {1672},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WG2VEEYD\\Pan et al. - 2021 - Hyperspectral Image Classification across Differen.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TXZRNBP5\\htm.html:text/html},
}

@article{schreck_empirical_2010,
	title = {Empirical decomposition of the explained variation in the vari- ance components form of the linear mixed model},
	language = {en},
	journal = {Biometrical Journal},
	author = {Schreck, Nicholas},
	year = {2010},
	pages = {22},
	file = {Schreck - 2010 - Empirical decomposition of the explained variation.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\GR962VLV\\Schreck - 2010 - Empirical decomposition of the explained variation.pdf:application/pdf},
}

@article{schreck_best_nodate,
	title = {Best {Prediction} of the {Additive} {Genomic} {Variance} in {Random}-{Effects} {Models}},
	abstract = {The additive genomic variance in linear models with random marker effects can be deﬁned as a random variable that is in accordance with classical quantitative genetics theory. Common approaches to estimate the genomic variance in random-effects linear models based on genomic marker data can be regarded as estimating the unconditional (or prior) expectation of this random additive genomic variance, and result in a negligence of the contribution of linkage disequilibrium (LD). We introduce a novel best prediction (BP) approach for the additive genomic variance in both the current and the base population in the framework of genomic prediction using the genomic best linear unbiased prediction (gBLUP) method. The resulting best predictor is the conditional (or posterior) expectation of the additive genomic variance when using the additional information given by the phenotypic data, and is structurally in accordance with the genomic equivalent of the classical additive genetic variance in random-effects models. In particular, the best predictor includes the contribution of (marker) LD to the additive genomic variance and possibly fully eliminates the missing contribution of LD that is caused by the assumptions of statistical frameworks such as the random-effects model. We derive an empirical best predictor (eBP) and compare its performance with common approaches to estimate the additive genomic variance in random-effects models on commonly used genomic datasets.},
	language = {en},
	author = {Schreck, Nicholas and Piepho, Hans-Peter and Schlather, Martin},
	pages = {16},
	file = {Schreck et al. - Best Prediction of the Additive Genomic Variance i.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\F5HY9VWQ\\Schreck et al. - Best Prediction of the Additive Genomic Variance i.pdf:application/pdf},
}

@article{schreck_advanced_nodate,
	title = {Advanced {Topics} in {Biostatistics} 2020/2021  {Linear} {Mixed} {Models} - {Dr}.{Nicholas} {Schreck}  {Biostatistics} - {C060}  nicholas.schreck@dkfz-heidelberg.de},
	language = {en},
	author = {Schreck, Nicholas},
	pages = {45},
	file = {Schreck - Advanced Topics in Biostatistics 20202021  Linear.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Q4EVG7ZW\\Schreck - Advanced Topics in Biostatistics 20202021  Linear.pdf:application/pdf},
}

@article{schreck_variance_nodate,
	title = {Variance {Decomposition}  in the {Linear} {Mixed} {Model}  - {Nicholas} {Schreck}  {Biostatistics} - {C060}  nicholas.schreck@dkfz-heidelberg.de},
	language = {en},
	author = {Schreck, Nicholas},
	pages = {19},
	file = {Schreck - Variance Decomposition  in the Linear Mixed Model .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4R5EIE4T\\Schreck - Variance Decomposition  in the Linear Mixed Model .pdf:application/pdf},
}

@article{khan_trends_2021,
	title = {Trends in {Deep} {Learning} for {Medical} {Hyperspectral} {Image} {Analysis}},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9385082/},
	doi = {10.1109/ACCESS.2021.3068392},
	abstract = {Deep learning algorithms have seen acute growth of interest in their applications throughout several fields of interest in the last decade, with medical hyperspectral imaging being a particularly promising domain. So far, to the best of our knowledge, there is no review paper that discusses the implementation of deep learning for medical hyperspectral imaging, which is what this work aims to accomplish by examining publications that currently utilize deep learning to perform effective analysis of medical hyperspectral imagery. This paper discusses deep learning concepts that are relevant and applicable to medical hyperspectral imaging analysis, several of which have been implemented since the boom in deep learning. This will comprise of reviewing the use of deep learning for classification, segmentation, and detection in order to investigate the analysis of medical hyperspectral imaging. Lastly, we discuss the current and future challenges pertaining to this discipline and the possible efforts to overcome such trials.},
	language = {en},
	urldate = {2021-05-03},
	journal = {IEEE Access},
	author = {Khan, Uzair and Paheding, Sidike and Elkin, Colin and Devabhaktuni, Vijay},
	year = {2021},
	pages = {1--1},
	file = {Khan et al. - 2021 - Trends in Deep Learning for Medical Hyperspectral .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ZG9ATNPT\\Khan et al. - 2021 - Trends in Deep Learning for Medical Hyperspectral .pdf:application/pdf},
}

@article{bengs_spectral-spatial_2020,
	title = {Spectral-{Spatial} {Recurrent}-{Convolutional} {Networks} for {In}-{Vivo} {Hyperspectral} {Tumor} {Type} {Classification}},
	url = {http://arxiv.org/abs/2007.01042},
	abstract = {Early detection of cancerous tissue is crucial for long-term patient survival. In the head and neck region, a typical diagnostic procedure is an endoscopic intervention where a medical expert manually assesses tissue using RGB camera images. While healthy and tumor regions are generally easier to distinguish, differentiating benign and malignant tumors is very challenging. This requires an invasive biopsy, followed by histological evaluation for diagnosis. Also, during tumor resection, tumor margins need to be verified by histological analysis. To avoid unnecessary tissue resection, a non-invasive, image-based diagnostic tool would be very valuable. Recently, hyperspectral imaging paired with deep learning has been proposed for this task, demonstrating promising results on ex-vivo specimens. In this work, we demonstrate the feasibility of in-vivo tumor type classification using hyperspectral imaging and deep learning. We analyze the value of using multiple hyperspectral bands compared to conventional RGB images and we study several machine learning models' ability to make use of the additional spectral information. Based on our insights, we address spectral and spatial processing using recurrent-convolutional models for effective spectral aggregating and spatial feature learning. Our best model achieves an AUC of 76.3\%, significantly outperforming previous conventional and deep learning methods.},
	urldate = {2021-05-03},
	journal = {arXiv:2007.01042 [cs, eess]},
	author = {Bengs, Marcel and Gessert, Nils and Laffers, Wiebke and Eggert, Dennis and Westermann, Stephan and Mueller, Nina A. and Gerstner, Andreas O. H. and Betz, Christian and Schlaefer, Alexander},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.01042},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7MWYZTKY\\Bengs et al. - 2020 - Spectral-Spatial Recurrent-Convolutional Networks .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VL4RWS5Z\\2007.html:text/html},
}

@article{bengs_spectral-spatial_2020-1,
	title = {Spectral-{Spatial} {Recurrent}-{Convolutional} {Networks} for {In}-{Vivo} {Hyperspectral} {Tumor} {Type} {Classification}},
	url = {http://arxiv.org/abs/2007.01042},
	abstract = {Early detection of cancerous tissue is crucial for long-term patient survival. In the head and neck region, a typical diagnostic procedure is an endoscopic intervention where a medical expert manually assesses tissue using RGB camera images. While healthy and tumor regions are generally easier to distinguish, diﬀerentiating benign and malignant tumors is very challenging. This requires an invasive biopsy, followed by histological evaluation for diagnosis. Also, during tumor resection, tumor margins need to be veriﬁed by histological analysis. To avoid unnecessary tissue resection, a non-invasive, image-based diagnostic tool would be very valuable. Recently, hyperspectral imaging paired with deep learning has been proposed for this task, demonstrating promising results on ex-vivo specimens. In this work, we demonstrate the feasibility of in-vivo tumor type classiﬁcation using hyperspectral imaging and deep learning. We analyze the value of using multiple hyperspectral bands compared to conventional RGB images and we study several machine learning models’ ability to make use of the additional spectral information. Based on our insights, we address spectral and spatial processing using recurrentconvolutional models for eﬀective spectral aggregating and spatial feature learning. Our best model achieves an AUC of 76.3 \%, signiﬁcantly outperforming previous conventional and deep learning methods.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:2007.01042 [cs, eess]},
	author = {Bengs, Marcel and Gessert, Nils and Laffers, Wiebke and Eggert, Dennis and Westermann, Stephan and Mueller, Nina A. and Gerstner, Andreas O. H. and Betz, Christian and Schlaefer, Alexander},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.01042},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Computer Vision and Pattern Recognition},
	file = {2007.01042.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CSMVQB9A\\2007.01042.pdf:application/pdf},
}

@article{zhao_training_2020,
	title = {Training confounder-free deep learning models for medical applications},
	volume = {11},
	issn = {2041-1723},
	url = {https://doi.org/10.1038/s41467-020-19784-9},
	doi = {10.1038/s41467-020-19784-9},
	abstract = {The presence of confounding effects (or biases) is one of the most critical challenges in using deep learning to advance discovery in medical imaging studies. Confounders affect the relationship between input data (e.g., brain MRIs) and output variables (e.g., diagnosis). Improper modeling of those relationships often results in spurious and biased associations. Traditional machine learning and statistical models minimize the impact of confounders by, for example, matching data sets, stratifying data, or residualizing imaging measurements. Alternative strategies are needed for state-of-the-art deep learning models that use end-to-end training to automatically extract informative features from large set of images. In this article, we introduce an end-to-end approach for deriving features invariant to confounding factors while accounting for intrinsic correlations between the confounder(s) and prediction outcome. The method does so by exploiting concepts from traditional statistical methods and recent fair machine learning schemes. We evaluate the method on predicting the diagnosis of HIV solely from Magnetic Resonance Images (MRIs), identifying morphological sex differences in adolescence from those of the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA), and determining the bone age from X-ray images of children. The results show that our method can accurately predict while reducing biases associated with confounders. The code is available at https://github.com/qingyuzhao/br-net.},
	number = {1},
	journal = {Nature Communications},
	author = {Zhao, Qingyu and Adeli, Ehsan and Pohl, Kilian M.},
	month = nov,
	year = {2020},
	pages = {1--9},
}

@misc{noauthor_7_2020,
	title = {7 {Types} of {Data} {Bias} in {Machine} {Learning}},
	url = {https://lionbridge.ai/articles/7-types-of-data-bias-in-machine-learning/},
	abstract = {We’ve listed common types of data bias in machine learning to help you analyze and understand where it happens, and what you can do about it.},
	language = {en},
	urldate = {2021-05-19},
	journal = {Lionbridge AI},
	month = jul,
	year = {2020},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Z3ZAPR66\\7-types-of-data-bias-in-machine-learning.html:text/html},
}

@article{mitchell_bias_nodate,
	title = {Bias in the {Vision} and {Language} of {Artificial} {Intelligence}},
	language = {en},
	author = {Mitchell, Margaret and Ai, Google},
	pages = {143},
	file = {Mitchell und Ai - Bias in the Vision and Language of Artificial Inte.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EYQN2LTC\\Mitchell und Ai - Bias in the Vision and Language of Artificial Inte.pdf:application/pdf},
}

@article{cirillo_sex_2020,
	title = {Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare},
	volume = {3},
	copyright = {2020 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-020-0288-5},
	doi = {10.1038/s41746-020-0288-5},
	abstract = {Precision Medicine implies a deep understanding of inter-individual differences in health and disease that are due to genetic and environmental factors. To acquire such understanding there is a need for the implementation of different types of technologies based on artificial intelligence (AI) that enable the identification of biomedically relevant patterns, facilitating progress towards individually tailored preventative and therapeutic interventions. Despite the significant scientific advances achieved so far, most of the currently used biomedical AI technologies do not account for bias detection. Furthermore, the design of the majority of algorithms ignore the sex and gender dimension and its contribution to health and disease differences among individuals. Failure in accounting for these differences will generate sub-optimal results and produce mistakes as well as discriminatory outcomes. In this review we examine the current sex and gender gaps in a subset of biomedical technologies used in relation to Precision Medicine. In addition, we provide recommendations to optimize their utilization to improve the global health and disease landscape and decrease inequalities.},
	language = {en},
	number = {1},
	urldate = {2021-05-19},
	journal = {npj Digital Medicine},
	author = {Cirillo, Davide and Catuara-Solarz, Silvina and Morey, Czuee and Guney, Emre and Subirats, Laia and Mellino, Simona and Gigante, Annalisa and Valencia, Alfonso and Rementeria, María José and Chadha, Antonella Santuccione and Mavridis, Nikolaos},
	month = jun,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1--11},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8D6TW9KI\\Cirillo et al. - 2020 - Sex and gender differences and biases in artificia.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UVI6W4TB\\s41746-020-0288-5.html:text/html},
}

@article{suresh_framework_2020,
	title = {A {Framework} for {Understanding} {Unintended} {Consequences} of {Machine} {Learning}},
	url = {http://arxiv.org/abs/1901.10002},
	abstract = {As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of potential sources of unwanted consequences. For instance, downstream harms to particular groups are often blamed on "biased data," but this concept encompass too many issues to be useful in developing solutions. In this paper, we provide a framework that partitions sources of downstream harm in machine learning into six distinct categories spanning the data generation and machine learning pipeline. We describe how these issues arise, how they are relevant to particular applications, and how they motivate different solutions. In doing so, we aim to facilitate the development of solutions that stem from an understanding of application-specific populations and data generation processes, rather than relying on general statements about what may or may not be "fair."},
	language = {en},
	urldate = {2021-05-19},
	journal = {arXiv:1901.10002 [cs, stat]},
	author = {Suresh, Harini and Guttag, John V.},
	month = feb,
	year = {2020},
	note = {arXiv: 1901.10002},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Suresh und Guttag - 2020 - A Framework for Understanding Unintended Consequen.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4ZJVEJML\\Suresh und Guttag - 2020 - A Framework for Understanding Unintended Consequen.pdf:application/pdf},
}

@techreport{baker_algorithmic_2021,
	type = {preprint},
	title = {Algorithmic {Bias} in {Education}},
	url = {https://osf.io/pbmvz},
	abstract = {In this paper, we review algorithmic bias in education, discussing the causes of that bias and reviewing the empirical literature on the specific ways that algorithmic bias is known to have manifested in education. While other recent work has reviewed mathematical definitions of fairness and expanded algorithmic approaches to reducing bias, our review focuses instead on solidifying the current understanding of the concrete impacts of algorithmic bias in education—which groups are known to be impacted and which stages and agents in the development and deployment of educational algorithms are implicated. We discuss theoretical and formal perspectives on algorithmic bias, connect those perspectives to the machine learning pipeline, and review metrics for assessing bias. Next, we review the evidence around algorithmic bias in education, beginning with the most heavily-studied categories of race/ethnicity, gender, and nationality, and moving to the available evidence of bias for less-studied categories, such as socioeconomic status, disability, and military-connected status. Acknowledging the gaps in what has been studied, we propose a framework for moving from unknown bias to known bias and from fairness to equity. We discuss obstacles to addressing these challenges and propose four areas of effort for mitigating and resolving the problems of algorithmic bias in AIED systems and other educational technology.},
	language = {en},
	urldate = {2021-05-19},
	institution = {EdArXiv},
	author = {Baker, Ryan Shaun and Hawn, Aaron},
	month = mar,
	year = {2021},
	doi = {10.35542/osf.io/pbmvz},
	file = {Baker und Hawn - 2021 - Algorithmic Bias in Education.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4N6UXBH6\\Baker und Hawn - 2021 - Algorithmic Bias in Education.pdf:application/pdf},
}

@misc{noauthor_algorithmic_nodate,
	title = {Algorithmic bias: from discrimination discovery to fairness-aware data mining},
	url = {http://www.francescobonchi.com/algorithmic_bias_tutorial.html},
	urldate = {2021-05-19},
	file = {Algorithmic bias\: from discrimination discovery to fairness-aware data mining:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FT4X69GZ\\algorithmic_bias_tutorial.html:text/html},
}

@article{delgado-rodriguez_bias_2004,
	title = {Bias},
	volume = {58},
	issn = {0143-005X},
	url = {https://jech.bmj.com/lookup/doi/10.1136/jech.2003.008466},
	doi = {10.1136/jech.2003.008466},
	language = {en},
	number = {8},
	urldate = {2021-05-19},
	journal = {Journal of Epidemiology \& Community Health},
	author = {Delgado-Rodriguez, M},
	month = aug,
	year = {2004},
	pages = {635--641},
	file = {Delgado-Rodriguez - 2004 - Bias.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5F8Y8DDB\\Delgado-Rodriguez - 2004 - Bias.pdf:application/pdf},
}

@article{badgeley_deep_2019,
	title = {Deep learning predicts hip fracture using confounding patient and healthcare variables},
	volume = {2},
	copyright = {2019 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-019-0105-1},
	doi = {10.1038/s41746-019-0105-1},
	abstract = {Hip fractures are a leading cause of death and disability among older adults. Hip fractures are also the most commonly missed diagnosis on pelvic radiographs, and delayed diagnosis leads to higher cost and worse outcomes. Computer-aided diagnosis (CAD) algorithms have shown promise for helping radiologists detect fractures, but the image features underpinning their predictions are notoriously difficult to understand. In this study, we trained deep-learning models on 17,587 radiographs to classify fracture, 5 patient traits, and 14 hospital process variables. All 20 variables could be individually predicted from a radiograph, with the best performances on scanner model (AUC = 1.00), scanner brand (AUC = 0.98), and whether the order was marked “priority” (AUC = 0.79). Fracture was predicted moderately well from the image (AUC = 0.78) and better when combining image features with patient data (AUC = 0.86, DeLong paired AUC comparison, p = 2e-9) or patient data plus hospital process features (AUC = 0.91, p = 1e-21). Fracture prediction on a test set that balanced fracture risk across patient variables was significantly lower than a random test set (AUC = 0.67, DeLong unpaired AUC comparison, p = 0.003); and on a test set with fracture risk balanced across patient and hospital process variables, the model performed randomly (AUC = 0.52, 95\% CI 0.46–0.58), indicating that these variables were the main source of the model’s fracture predictions. A single model that directly combines image features, patient, and hospital process data outperforms a Naive Bayes ensemble of an image-only model prediction, patient, and hospital process data. If CAD algorithms are inexplicably leveraging patient and process variables in their predictions, it is unclear how radiologists should interpret their predictions in the context of other known patient data. Further research is needed to illuminate deep-learning decision processes so that computers and clinicians can effectively cooperate.},
	language = {en},
	number = {1},
	urldate = {2021-05-19},
	journal = {npj Digital Medicine},
	author = {Badgeley, Marcus A. and Zech, John R. and Oakden-Rayner, Luke and Glicksberg, Benjamin S. and Liu, Manway and Gale, William and McConnell, Michael V. and Percha, Bethany and Snyder, Thomas M. and Dudley, Joel T.},
	month = apr,
	year = {2019},
	pages = {1--10},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RJHUF883\\Badgeley et al. - 2019 - Deep learning predicts hip fracture using confound.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\K332DIAT\\s41746-019-0105-1.html:text/html},
}

@article{rudd_global_2020,
	title = {Global, regional, and national sepsis incidence and mortality, 1990–2017: analysis for the {Global} {Burden} of {Disease} {Study}},
	volume = {395},
	issn = {0140-6736, 1474-547X},
	shorttitle = {Global, regional, and national sepsis incidence and mortality, 1990–2017},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(19)32989-7/abstract},
	doi = {10.1016/S0140-6736(19)32989-7},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Background{\textless}/h3{\textgreater}{\textless}p{\textgreater}Sepsis is life-threatening organ dysfunction due to a dysregulated host response to infection. It is considered a major cause of health loss, but data for the global burden of sepsis are limited. As a syndrome caused by underlying infection, sepsis is not part of standard Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) estimates. Accurate estimates are important to inform and monitor health policy interventions, allocation of resources, and clinical treatment initiatives. We estimated the global, regional, and national incidence of sepsis and mortality from this disorder using data from GBD 2017.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater}{\textless}p{\textgreater}We used multiple cause-of-death data from 109 million individual death records to calculate mortality related to sepsis among each of the 282 underlying causes of death in GBD 2017. The percentage of sepsis-related deaths by underlying GBD cause in each location worldwide was modelled using mixed-effects linear regression. Sepsis-related mortality for each age group, sex, location, GBD cause, and year (1990–2017) was estimated by applying modelled cause-specific fractions to GBD 2017 cause-of-death estimates. We used data for 8·7 million individual hospital records to calculate in-hospital sepsis-associated case-fatality, stratified by underlying GBD cause. In-hospital sepsis-associated case-fatality was modelled for each location using linear regression, and sepsis incidence was estimated by applying modelled case-fatality to sepsis-related mortality estimates.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Findings{\textless}/h3{\textgreater}{\textless}p{\textgreater}In 2017, an estimated 48·9 million (95\% uncertainty interval [UI] 38·9–62·9) incident cases of sepsis were recorded worldwide and 11·0 million (10·1–12·0) sepsis-related deaths were reported, representing 19·7\% (18·2–21·4) of all global deaths. Age-standardised sepsis incidence fell by 37·0\% (95\% UI 11·8–54·5) and mortality decreased by 52·8\% (47·7–57·5) from 1990 to 2017. Sepsis incidence and mortality varied substantially across regions, with the highest burden in sub-Saharan Africa, Oceania, south Asia, east Asia, and southeast Asia.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Interpretation{\textless}/h3{\textgreater}{\textless}p{\textgreater}Despite declining age-standardised incidence and mortality, sepsis remains a major cause of health loss worldwide and has an especially high health-related burden in sub-Saharan Africa.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Funding{\textless}/h3{\textgreater}{\textless}p{\textgreater}The Bill \& Melinda Gates Foundation, the National Institutes of Health, the University of Pittsburgh, the British Columbia Children's Hospital Foundation, the Wellcome Trust, and the Fleming Fund.{\textless}/p{\textgreater}},
	language = {English},
	number = {10219},
	urldate = {2021-05-20},
	journal = {The Lancet},
	author = {Rudd, Kristina E. and Johnson, Sarah Charlotte and Agesa, Kareha M. and Shackelford, Katya Anne and Tsoi, Derrick and Kievlan, Daniel Rhodes and Colombara, Danny V. and Ikuta, Kevin S. and Kissoon, Niranjan and Finfer, Simon and Fleischmann-Struzek, Carolin and Machado, Flavia R. and Reinhart, Konrad K. and Rowan, Kathryn and Seymour, Christopher W. and Watson, R. Scott and West, T. Eoin and Marinho, Fatima and Hay, Simon I. and Lozano, Rafael and Lopez, Alan D. and Angus, Derek C. and Murray, Christopher J. L. and Naghavi, Mohsen},
	month = jan,
	year = {2020},
	pages = {200--211},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\E4F3IDY2\\Rudd et al. - 2020 - Global, regional, and national sepsis incidence an.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\USTUAEMZ\\fulltext.html:text/html},
}

@article{ferrer_empiric_2014,
	title = {Empiric {Antibiotic} {Treatment} {Reduces} {Mortality} in {Severe} {Sepsis} and {Septic} {Shock} {From} the {First} {Hour}: {Results} {From} a {Guideline}-{Based} {Performance} {Improvement} {Program}},
	volume = {42},
	abstract = {Objectives: Compelling evidence has shown that aggressive resuscitation bundles, adequate source control, appropriate antibiotic therapy, and organ support are cornerstone for the success in the treatment of patients with sepsis. Delay in the initiation of appropriate antibiotic therapy has been recognized as a risk factor for mortality. To perform a retrospective analysis on the Surviving Sepsis Campaign database to evaluate the relationship between timing of antibiotic administration and mortality. Design: Retrospective analysis of a large dataset collected prospectively for the Surviving Sepsis Campaign. Setting: One hundred sixty-five ICUs in Europe, the United States, and South America. Patients: A total of 28,150 patients with severe sepsis and septic shock, from January 2005 through February 2010, were evaluated. Interventions: Antibiotic administration and hospital mortality.
Measurements and Main Results: A total of 17,990 patients received antibiotics after sepsis identification and were included in the analysis. ­In-hospital mortality was 29.7\% for the cohort as a whole. There was a statically significant increase in the probability of death associated with the number of hours of delay for first antibiotic administration. Hospital mortality adjusted for severity (sepsis severity score), ICU admission source (emergency department, ward, vs ICU), and geographic region increased steadily after 1 hour of time to antibiotic administration. Results were similar in patients with severe sepsis and septic shock, regardless of the number of organ failure.
Conclusions: The results of the analysis of this large population of patients with severe sepsis and septic shock demonstrate that delay in first antibiotic administration was associated with increased in-hospital mortality. In addition, there was a linear},
	language = {en},
	number = {8},
	journal = {Critical Care Medicine},
	author = {Ferrer, Ricard and Martin-Loeches, Ignacio and Phillips, Gary and Osborn, Tiffany M and Townsend, Sean and Dellinger, R Phillip and Artigas, Antonio and Schorr, Christa and Levy, Mitchell M},
	year = {2014},
	pages = {1749--1755},
	file = {Ferrer et al. - 2014 - Empiric Antibiotic Treatment Reduces Mortality in .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\735V86FE\\Ferrer et al. - 2014 - Empiric Antibiotic Treatment Reduces Mortality in .pdf:application/pdf},
}

@article{singer_third_2016,
	title = {The {Third} {International} {Consensus} {Definitions} for {Sepsis} and {Septic} {Shock} ({Sepsis}-3)},
	volume = {315},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2016.0287},
	doi = {10.1001/jama.2016.0287},
	abstract = {Definitions of sepsis and septic shock were last revised in 2001. Considerable advances have since been made into the pathobiology (changes in organ function, morphology, cell biology, biochemistry, immunology, and circulation), management, and epidemiology of sepsis, suggesting the need for reexamination.To evaluate and, as needed, update definitions for sepsis and septic shock.A task force (n = 19) with expertise in sepsis pathobiology, clinical trials, and epidemiology was convened by the Society of Critical Care Medicine and the European Society of Intensive Care Medicine. Definitions and clinical criteria were generated through meetings, Delphi processes, analysis of electronic health record databases, and voting, followed by circulation to international professional societies, requesting peer review and endorsement (by 31 societies listed in the Acknowledgment).Limitations of previous definitions included an excessive focus on inflammation, the misleading model that sepsis follows a continuum through severe sepsis to shock, and inadequate specificity and sensitivity of the systemic inflammatory response syndrome (SIRS) criteria. Multiple definitions and terminologies are currently in use for sepsis, septic shock, and organ dysfunction, leading to discrepancies in reported incidence and observed mortality. The task force concluded the term severe sepsis was redundant.Sepsis should be defined as life-threatening organ dysfunction caused by a dysregulated host response to infection. For clinical operationalization, organ dysfunction can be represented by an increase in the Sequential [Sepsis-related] Organ Failure Assessment (SOFA) score of 2 points or more, which is associated with an in-hospital mortality greater than 10\%. Septic shock should be defined as a subset of sepsis in which particularly profound circulatory, cellular, and metabolic abnormalities are associated with a greater risk of mortality than with sepsis alone. Patients with septic shock can be clinically identified by a vasopressor requirement to maintain a mean arterial pressure of 65 mm Hg or greater and serum lactate level greater than 2 mmol/L (\&gt;18 mg/dL) in the absence of hypovolemia. This combination is associated with hospital mortality rates greater than 40\%. In out-of-hospital, emergency department, or general hospital ward settings, adult patients with suspected infection can be rapidly identified as being more likely to have poor outcomes typical of sepsis if they have at least 2 of the following clinical criteria that together constitute a new bedside clinical score termed quickSOFA (qSOFA): respiratory rate of 22/min or greater, altered mentation, or systolic blood pressure of 100 mm Hg or less.These updated definitions and clinical criteria should replace previous definitions, offer greater consistency for epidemiologic studies and clinical trials, and facilitate earlier recognition and more timely management of patients with sepsis or at risk of developing sepsis.},
	number = {8},
	urldate = {2021-05-20},
	journal = {JAMA},
	author = {Singer, Mervyn and Deutschman, Clifford S. and Seymour, Christopher Warren and Shankar-Hari, Manu and Annane, Djillali and Bauer, Michael and Bellomo, Rinaldo and Bernard, Gordon R. and Chiche, Jean-Daniel and Coopersmith, Craig M. and Hotchkiss, Richard S. and Levy, Mitchell M. and Marshall, John C. and Martin, Greg S. and Opal, Steven M. and Rubenfeld, Gordon D. and van der Poll, Tom and Vincent, Jean-Louis and Angus, Derek C.},
	month = feb,
	year = {2016},
	pages = {801--810},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\46NEDFTZ\\2492881.html:text/html;Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DM5I7796\\Singer et al. - 2016 - The Third International Consensus Definitions for .pdf:application/pdf},
}

@article{moor_early_2021,
	title = {Early {Prediction} of {Sepsis} in the {ICU} using {Machine} {Learning}: {A} {Systematic} {Review}},
	volume = {8},
	shorttitle = {Early {Prediction} of {Sepsis} in the {ICU} using {Machine} {Learning}},
	url = {https://www.medrxiv.org/content/10.1101/2020.08.31.20185207v1},
	doi = {10.3389/fmed.2021.607952},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}h3{\textgreater}Background{\textless}/h3{\textgreater} {\textless}p{\textgreater}Sepsis is among the leading causes of death in intensive care units (ICU) world-wide and its recognition, particularly in the early stages of the disease, remains a medical challenge. The advent of an affluence of available digital health data has created a setting in which machine learning can be used for digital biomarker discovery, with the ultimate goal to advance the early recognition of sepsis.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Objective{\textless}/h3{\textgreater} {\textless}p{\textgreater}To systematically review and evaluate studies employing machine learning for the prediction of sepsis in the ICU.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Data sources{\textless}/h3{\textgreater} {\textless}p{\textgreater}Using Embase, Google Scholar, PubMed/Medline, Scopus, and Web of Science, we systematically searched the existing literature for machine learning-driven sepsis onset prediction for patients in the ICU.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Study eligibility criteria{\textless}/h3{\textgreater} {\textless}p{\textgreater}All peer-reviewed articles using machine learning for the prediction of sepsis onset in adult ICU patients were included. Studies focusing on patient populations outside the ICU were excluded.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Study appraisal and synthesis methods{\textless}/h3{\textgreater} {\textless}p{\textgreater}A systematic review was performed according to the PRISMA guidelines. Moreover, a quality assessment of all eligible studies was performed.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater} {\textless}p{\textgreater}Out of 974 identified articles, 22 and 21 met the criteria to be included in the systematic review and quality assessment, respectively. A multitude of machine learning algorithms were applied to refine the early prediction of sepsis. The quality of the studies ranged from “poor” (satisfying ≤ 40\% of the quality criteria) to “very good” (satisfying ≥ 90\% of the quality criteria). The majority of the studies (\textit{n} = 19, 86.4\%) employed an offline training scenario combined with a horizon evaluation, while two studies implemented an online scenario (\textit{n} = 2, 9.1\%). The massive inter-study heterogeneity in terms of model development, sepsis definition, prediction time windows, and outcomes precluded a meta-analysis. Last, only 2 studies provided publicly-accessible source code and data sources fostering reproducibility.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Limitations{\textless}/h3{\textgreater} {\textless}p{\textgreater}Articles were only eligible for inclusion when employing machine learning algorithms for the prediction of sepsis onset in the ICU. This restriction led to the exclusion of studies focusing on the prediction of septic shock, sepsis-related mortality, and patient populations outside the ICU.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusions and key findings{\textless}/h3{\textgreater} {\textless}p{\textgreater}A growing number of studies employs machine learning to optimise the early prediction of sepsis through digital biomarker discovery. This review, however, highlights several shortcomings of the current approaches, including low comparability and reproducibility. Finally, we gather recommendations how these challenges can be addressed before deploying these models in prospective analyses.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Systematic review registration number{\textless}/h3{\textgreater} {\textless}p{\textgreater}CRD42020200133{\textless}/p{\textgreater}},
	language = {en},
	number = {607952},
	urldate = {2021-05-20},
	journal = {Frontiers in Medicine},
	author = {Moor, Michael and Rieck, Bastian and Horn, Max and Jutzeler, Catherine R. and Borgwardt, Karsten},
	month = may,
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9TRL34WA\\Moor et al. - 2020 - Early Prediction of Sepsis in the ICU using Machin.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IYBLAUJN\\2020.08.31.20185207v1.html:text/html},
}

@incollection{li_lda_2009,
	address = {Boston, MA},
	title = {{LDA} ({Linear} {Discriminant} {Analysis})},
	isbn = {978-0-387-73003-5},
	url = {https://doi.org/10.1007/978-0-387-73003-5_349},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {Encyclopedia of {Biometrics}},
	publisher = {Springer US},
	editor = {Li, Stan Z. and Jain, Anil},
	year = {2009},
	doi = {10.1007/978-0-387-73003-5_349},
	pages = {899--899},
}

@article{jager_confounding_2008,
	title = {Confounding: {What} it is and how to deal with it},
	volume = {73},
	issn = {0085-2538},
	shorttitle = {Confounding},
	url = {https://www.sciencedirect.com/science/article/pii/S0085253815529748},
	doi = {10.1038/sj.ki.5002650},
	abstract = {As confounding obscures the ‘real’ effect of an exposure on outcome, investigators performing etiological studies do their utmost best to prevent or control confounding. Unfortunately, in this process, errors are frequently made. This paper explains that to be a potential confounder, a variable needs to satisfy all three of the following criteria: (1) it must have an association with the disease, that is, it should be a risk factor for the disease; (2) it must be associated with the exposure, that is, it must be unequally distributed between exposure groups; and (3) it must not be an effect of the exposure; this also means that it may not be part of the causal pathway. In addition, a number of different techniques are described that may be applied to prevent or control for confounding: randomization, restriction, matching, and stratification. Finally, a number of examples outline commonly made errors, most of which result from ‘overadjustment’ for variables that do not satisfy the criteria for potential confounders. Such an example of an error frequently occurring in the literature is the incorrect adjustment for blood pressure while studying the relationship between body mass index and the development of end-stage renal disease. Such errors will introduce new bias instead of preventing it.},
	language = {en},
	number = {3},
	urldate = {2021-05-21},
	journal = {Kidney International},
	author = {Jager, K. J. and Zoccali, C. and MacLeod, A. and Dekker, F. W.},
	month = feb,
	year = {2008},
	keywords = {confounding, epidemiology, matching, randomization, stratification},
	pages = {256--260},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MHPGJDXS\\Jager et al. - 2008 - Confounding What it is and how to deal with it.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CRUX4B9U\\S0085253815529748.html:text/html},
}

@article{roberts_common_2021,
	title = {Common pitfalls and recommendations for using machine learning to detect and prognosticate for {COVID}-19 using chest radiographs and {CT} scans},
	volume = {3},
	copyright = {2021 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-021-00307-0},
	doi = {10.1038/s42256-021-00307-0},
	abstract = {Machine learning methods offer great promise for fast and accurate detection and prognostication of coronavirus disease 2019 (COVID-19) from standard-of-care chest radiographs (CXR) and chest computed tomography (CT) images. Many articles have been published in 2020 describing new machine learning-based models for both of these tasks, but it is unclear which are of potential clinical utility. In this systematic review, we consider all published papers and preprints, for the period from 1 January 2020 to 3 October 2020, which describe new machine learning models for the diagnosis or prognosis of COVID-19 from CXR or CT images. All manuscripts uploaded to bioRxiv, medRxiv and arXiv along with all entries in EMBASE and MEDLINE in this timeframe are considered. Our search identified 2,212 studies, of which 415 were included after initial screening and, after quality screening, 62 studies were included in this systematic review. Our review finds that none of the models identified are of potential clinical use due to methodological flaws and/or underlying biases. This is a major weakness, given the urgency with which validated COVID-19 models are needed. To address this, we give many recommendations which, if followed, will solve these issues and lead to higher-quality model development and well-documented manuscripts.},
	language = {en},
	number = {3},
	urldate = {2021-05-21},
	journal = {Nature Machine Intelligence},
	author = {Roberts, Michael and Driggs, Derek and Thorpe, Matthew and Gilbey, Julian and Yeung, Michael and Ursprung, Stephan and Aviles-Rivero, Angelica I. and Etmann, Christian and McCague, Cathal and Beer, Lucian and Weir-McCall, Jonathan R. and Teng, Zhongzhao and Gkrania-Klotsas, Effrossyni and Rudd, James H. F. and Sala, Evis and Schönlieb, Carola-Bibiane},
	month = mar,
	year = {2021},
	pages = {199--217},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5WZ8NR6K\\Roberts et al. - 2021 - Common pitfalls and recommendations for using mach.pdf:application/pdf;MEDIA-D-21-00170_R1_reviewer.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SESDGYJA\\MEDIA-D-21-00170_R1_reviewer.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\HRLS8394\\s42256-021-00307-0.html:text/html},
}

@article{zhao_training_2020-1,
	title = {Training confounder-free deep learning models for medical applications},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-19784-9},
	doi = {10.1038/s41467-020-19784-9},
	abstract = {The presence of confounding effects (or biases) is one of the most critical challenges in using deep learning to advance discovery in medical imaging studies. Confounders affect the relationship between input data (e.g., brain MRIs) and output variables (e.g., diagnosis). Improper modeling of those relationships often results in spurious and biased associations. Traditional machine learning and statistical models minimize the impact of confounders by, for example, matching data sets, stratifying data, or residualizing imaging measurements. Alternative strategies are needed for state-of-the-art deep learning models that use end-to-end training to automatically extract informative features from large set of images. In this article, we introduce an end-to-end approach for deriving features invariant to confounding factors while accounting for intrinsic correlations between the confounder(s) and prediction outcome. The method does so by exploiting concepts from traditional statistical methods and recent fair machine learning schemes. We evaluate the method on predicting the diagnosis of HIV solely from Magnetic Resonance Images (MRIs), identifying morphological sex differences in adolescence from those of the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA), and determining the bone age from X-ray images of children. The results show that our method can accurately predict while reducing biases associated with confounders. The code is available at https://github.com/qingyuzhao/br-net.},
	language = {en},
	number = {1},
	urldate = {2021-05-21},
	journal = {Nature Communications},
	author = {Zhao, Qingyu and Adeli, Ehsan and Pohl, Kilian M.},
	month = nov,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {6010},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\BGZ2AQCV\\Zhao et al. - 2020 - Training confounder-free deep learning models for .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8MH4XD2A\\s41467-020-19784-9.html:text/html},
}

@article{obermeyer_dissecting_2019,
	title = {Dissecting racial bias in an algorithm used to manage the health of populations},
	volume = {366},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aax2342},
	doi = {10.1126/science.aax2342},
	abstract = {Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.},
	language = {en},
	number = {6464},
	urldate = {2021-05-21},
	journal = {Science},
	author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	month = oct,
	year = {2019},
	pages = {447--453},
	file = {Obermeyer et al. - 2019 - Dissecting racial bias in an algorithm used to man.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\U44X77MA\\Obermeyer et al. - 2019 - Dissecting racial bias in an algorithm used to man.pdf:application/pdf},
}

@article{tat_addressing_2020,
	title = {Addressing bias: artificial intelligence in cardiovascular medicine},
	volume = {2},
	issn = {25897500},
	shorttitle = {Addressing bias},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2589750020302491},
	doi = {10.1016/S2589-7500(20)30249-1},
	language = {en},
	number = {12},
	urldate = {2021-05-21},
	journal = {The Lancet Digital Health},
	author = {Tat, Emily and Bhatt, Deepak L and Rabbat, Mark G},
	month = dec,
	year = {2020},
	pages = {e635--e636},
	file = {Tat et al. - 2020 - Addressing bias artificial intelligence in cardio.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4KE4Q5CF\\Tat et al. - 2020 - Addressing bias artificial intelligence in cardio.pdf:application/pdf},
}

@article{grimes_bias_2002,
	title = {Bias and causal associations in observational research},
	volume = {359},
	issn = {0140-6736},
	doi = {10.1016/S0140-6736(02)07451-2},
	abstract = {Readers of medical literature need to consider two types of validity, internal and external. Internal validity means that the study measured what it set out to; external validity is the ability to generalise from the study to the reader's patients. With respect to internal validity, selection bias, information bias, and confounding are present to some degree in all observational research. Selection bias stems from an absence of comparability between groups being studied. Information bias results from incorrect determination of exposure, outcome, or both. The effect of information bias depends on its type. If information is gathered differently for one group than for another, bias results. By contrast, non-differential misclassification tends to obscure real differences. Confounding is a mixing or blurring of effects: a researcher attempts to relate an exposure to an outcome but actually measures the effect of a third factor (the confounding variable). Confounding can be controlled in several ways: restriction, matching, stratification, and more sophisticated multivariate techniques. If a reader cannot explain away study results on the basis of selection, information, or confounding bias, then chance might be another explanation. Chance should be examined last, however, since these biases can account for highly significant, though bogus results. Differentiation between spurious, indirect, and causal associations can be difficult. Criteria such as temporal sequence, strength and consistency of an association, and evidence of a dose-response effect lend support to a causal link.},
	language = {eng},
	number = {9302},
	journal = {Lancet (London, England)},
	author = {Grimes, David A. and Schulz, Kenneth F.},
	month = jan,
	year = {2002},
	pmid = {11812579},
	keywords = {Humans, Bias, Causality, Confounding Factors, Epidemiologic, Data Collection, Multivariate Analysis, Reproducibility of Results, Research Design, Selection Bias},
	pages = {248--252},
}

@article{willemink_preparing_2020,
	title = {Preparing {Medical} {Imaging} {Data} for {Machine} {Learning}},
	volume = {295},
	issn = {0033-8419, 1527-1315},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.2020192224},
	doi = {10.1148/radiol.2020192224},
	abstract = {Supervised artificial intelligence (AI) methods for evaluation of medical images require a curation process for data to optimally train, validate, and test algorithms. The chief obstacles to development and clinical implementation of AI algorithms include availability of sufficiently large, curated, and representative training data that includes expert labeling (eg, annotations).},
	language = {en},
	number = {1},
	urldate = {2021-05-26},
	journal = {Radiology},
	author = {Willemink, Martin J. and Koszek, Wojciech A. and Hardell, Cailin and Wu, Jie and Fleischmann, Dominik and Harvey, Hugh and Folio, Les R. and Summers, Ronald M. and Rubin, Daniel L. and Lungren, Matthew P.},
	month = apr,
	year = {2020},
	pages = {4--15},
	file = {Willemink et al. - 2020 - Preparing Medical Imaging Data for Machine Learnin.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WHXIN7WF\\Willemink et al. - 2020 - Preparing Medical Imaging Data for Machine Learnin.pdf:application/pdf},
}

@article{larrazabal_gender_2020,
	title = {Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1919012117},
	doi = {10.1073/pnas.1919012117},
	abstract = {Artificial intelligence (AI) systems for computer-aided diagnosis and image-based screening are being adopted worldwide by medical institutions. In such a context, generating fair and unbiased classifiers becomes of paramount importance. The research community of medical image computing is making great efforts in developing more accurate algorithms to assist medical doctors in the difficult task of disease diagnosis. However, little attention is paid to the way databases are collected and how this may influence the performance of AI systems. Our study sheds light on the importance of gender balance in medical imaging datasets used to train AI systems for computer-assisted diagnosis. We provide empirical evidence supported by a large-scale study, based on three deep neural network architectures and two well-known publicly available X-ray image datasets used to diagnose various thoracic diseases under different gender imbalance conditions. We found a consistent decrease in performance for underrepresented genders when a minimum balance is not fulfilled. This raises the alarm for national agencies in charge of regulating and approving computer-assisted diagnosis systems, which should include explicit gender balance and diversity recommendations. We also establish an open problem for the academic medical image computing community which needs to be addressed by novel algorithms endowed with robustness to gender imbalance.},
	language = {en},
	number = {23},
	urldate = {2021-05-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Larrazabal, Agostina J. and Nieto, Nicolás and Peterson, Victoria and Milone, Diego H. and Ferrante, Enzo},
	month = jun,
	year = {2020},
	pages = {12592--12594},
	file = {Larrazabal et al. - 2020 - Gender imbalance in medical imaging datasets produ.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\28NU766W\\Larrazabal et al. - 2020 - Gender imbalance in medical imaging datasets produ.pdf:application/pdf},
}

@article{kochling_discriminated_2020,
	title = {Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making in the context of {HR} recruitment and {HR} development},
	volume = {13},
	issn = {2198-2627},
	shorttitle = {Discriminated by an algorithm},
	url = {https://doi.org/10.1007/s40685-020-00134-w},
	doi = {10.1007/s40685-020-00134-w},
	abstract = {Algorithmic decision-making is becoming increasingly common as a new source of advice in HR recruitment and HR development. While firms implement algorithmic decision-making to save costs as well as increase efficiency and objectivity, algorithmic decision-making might also lead to the unfair treatment of certain groups of people, implicit discrimination, and perceived unfairness. Current knowledge about the threats of unfairness and (implicit) discrimination by algorithmic decision-making is mostly unexplored in the human resource management context. Our goal is to clarify the current state of research related to HR recruitment and HR development, identify research gaps, and provide crucial future research directions. Based on a systematic review of 36 journal articles from 2014 to 2020, we present some applications of algorithmic decision-making and evaluate the possible pitfalls in these two essential HR functions. In doing this, we inform researchers and practitioners, offer important theoretical and practical implications, and suggest fruitful avenues for future research.},
	language = {en},
	number = {3},
	urldate = {2021-05-26},
	journal = {Business Research},
	author = {Köchling, Alina and Wehner, Marius Claus},
	month = nov,
	year = {2020},
	pages = {795--848},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\N3SLA5XR\\Köchling und Wehner - 2020 - Discriminated by an algorithm a systematic review.pdf:application/pdf},
}

@article{panch_artificial_nodate,
	title = {Artificial intelligence and algorithmic bias: implications for health systems},
	volume = {9},
	issn = {2047-2978},
	shorttitle = {Artificial intelligence and algorithmic bias},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6875681/},
	doi = {10.7189/jogh.09.020318},
	number = {2},
	urldate = {2021-05-26},
	journal = {Journal of Global Health},
	author = {Panch, Trishan and Mattie, Heather and Atun, Rifat},
	pmid = {31788229},
	pmcid = {PMC6875681},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VRXIG4M4\\Panch et al. - Artificial intelligence and algorithmic bias impl.pdf:application/pdf},
}

@article{obermeyer_dissecting_2019-1,
	title = {Dissecting racial bias in an algorithm used to manage the health of populations},
	volume = {366},
	copyright = {Copyright © 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/366/6464/447},
	doi = {10.1126/science.aax2342},
	abstract = {Racial bias in health algorithms
The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care.
Science, this issue p. 447; see also p. 421
Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.
A health algorithm that uses health costs as a proxy for health needs leads to racial bias against Black patients.
A health algorithm that uses health costs as a proxy for health needs leads to racial bias against Black patients.},
	language = {en},
	number = {6464},
	urldate = {2021-05-26},
	journal = {Science},
	author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	month = oct,
	year = {2019},
	pmid = {31649194},
	note = {Publisher: American Association for the Advancement of Science
Section: Research Article},
	pages = {447--453},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\N8PF7AJ5\\Obermeyer et al. - 2019 - Dissecting racial bias in an algorithm used to man.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FLAYLUYD\\tab-pdf.html:text/html},
}

@article{vetter_bias_2017,
	title = {Bias, {Confounding}, and {Interaction}: {Lions} and {Tigers}, and {Bears}, {Oh} {My}!},
	volume = {125},
	issn = {1526-7598},
	shorttitle = {Bias, {Confounding}, and {Interaction}},
	doi = {10.1213/ANE.0000000000002332},
	abstract = {Epidemiologists seek to make a valid inference about the causal effect between an exposure and a disease in a specific population, using representative sample data from a specific population. Clinical researchers likewise seek to make a valid inference about the association between an intervention and outcome(s) in a specific population, based upon their randomly collected, representative sample data. Both do so by using the available data about the sample variable to make a valid estimate about its corresponding or underlying, but unknown population parameter. Random error in an experiment can be due to the natural, periodic fluctuation or variation in the accuracy or precision of virtually any data sampling technique or health measurement tool or scale. In a clinical research study, random error can be due to not only innate human variability but also purely chance. Systematic error in an experiment arises from an innate flaw in the data sampling technique or measurement instrument. In the clinical research setting, systematic error is more commonly referred to as systematic bias. The most commonly encountered types of bias in anesthesia, perioperative, critical care, and pain medicine research include recall bias, observational bias (Hawthorne effect), attrition bias, misclassification or informational bias, and selection bias. A confounding variable is a factor associated with both the exposure of interest and the outcome of interest. A confounding variable (confounding factor or confounder) is a variable that correlates (positively or negatively) with both the exposure and outcome. Confounding is typically not an issue in a randomized trial because the randomized groups are sufficiently balanced on all potential confounding variables, both observed and nonobserved. However, confounding can be a major problem with any observational (nonrandomized) study. Ignoring confounding in an observational study will often result in a "distorted" or incorrect estimate of the association or treatment effect. Interaction among variables, also known as effect modification, exists when the effect of 1 explanatory variable on the outcome depends on the particular level or value of another explanatory variable. Bias and confounding are common potential explanations for statistically significant associations between exposure and outcome when the true relationship is noncausal. Understanding interactions is vital to proper interpretation of treatment effects. These complex concepts should be consistently and appropriately considered whenever one is not only designing but also analyzing and interpreting data from a randomized trial or observational study.},
	language = {eng},
	number = {3},
	journal = {Anesthesia and Analgesia},
	author = {Vetter, Thomas R. and Mascha, Edward J.},
	month = sep,
	year = {2017},
	pmid = {28817531},
	keywords = {Animals, Humans, Bias, Confounding Factors, Epidemiologic, Biomedical Research, Mental Recall, Random Allocation},
	pages = {1042--1048},
}

@article{kelly_key_2019,
	title = {Key challenges for delivering clinical impact with artificial intelligence},
	volume = {17},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-019-1426-2},
	doi = {10.1186/s12916-019-1426-2},
	abstract = {Artificial intelligence (AI) research in healthcare is accelerating rapidly, with potential applications being demonstrated across various domains of medicine. However, there are currently limited examples of such techniques being successfully deployed into clinical practice. This article explores the main challenges and limitations of AI in healthcare, and considers the steps required to translate these potentially transformative technologies from research to clinical practice.},
	number = {1},
	urldate = {2021-05-26},
	journal = {BMC Medicine},
	author = {Kelly, Christopher J. and Karthikesalingam, Alan and Suleyman, Mustafa and Corrado, Greg and King, Dominic},
	month = oct,
	year = {2019},
	keywords = {Artificial intelligence, Algorithms, Machine learning, Evaluation, Regulation, Translation},
	pages = {195},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JW7CXPMS\\Kelly et al. - 2019 - Key challenges for delivering clinical impact with.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WW3I3AG4\\s12916-019-1426-2.html:text/html},
}

@article{suresh_framework_2020-1,
	title = {A {Framework} for {Understanding} {Unintended} {Consequences} of {Machine} {Learning}},
	url = {http://arxiv.org/abs/1901.10002},
	abstract = {As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of potential sources of unwanted consequences. For instance, downstream harms to particular groups are often blamed on "biased data," but this concept encompass too many issues to be useful in developing solutions. In this paper, we provide a framework that partitions sources of downstream harm in machine learning into six distinct categories spanning the data generation and machine learning pipeline. We describe how these issues arise, how they are relevant to particular applications, and how they motivate different solutions. In doing so, we aim to facilitate the development of solutions that stem from an understanding of application-specific populations and data generation processes, rather than relying on general statements about what may or may not be "fair."},
	language = {en},
	urldate = {2021-05-26},
	journal = {arXiv:1901.10002 [cs, stat]},
	author = {Suresh, Harini and Guttag, John V.},
	month = feb,
	year = {2020},
	note = {arXiv: 1901.10002},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Suresh und Guttag - 2020 - A Framework for Understanding Unintended Consequen.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\T5FCDBK2\\Suresh und Guttag - 2020 - A Framework for Understanding Unintended Consequen.pdf:application/pdf},
}

@article{noauthor_table_nodate,
	title = {Table 2 {Source} of undesirable bias in {Artificial} {Intelligence} with examples in health research and practice.},
	copyright = {©2021 Macmillan Publishers Limited. All Rights Reserved.},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-020-0288-5/tables/2},
	language = {en},
	urldate = {2021-05-26},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\LCQZEBJC\\2.html:text/html},
}

@article{johnson_sampling_2000,
	title = {Sampling {Bias} and {Other} {Methodological} {Threats} to the {Validity} of {Health} {Survey} {Research}},
	volume = {7},
	issn = {1573-3424},
	url = {https://doi.org/10.1023/A:1009589812697},
	doi = {10.1023/A:1009589812697},
	abstract = {Data from a longitudinal occupational health survey of professional fire fighters were used to explore the potential impact of two types of methodological bias: sample selection and reactivity. No significant differences on demographic variables were observed between the group who first responded after a within-study change in survey administration format (Delayed) and respondents who had completed surveys since the study's inception (Initial). However, statistically significant differences in the study's 26 outcome measures provided some evidence that between-group differences did exist and that an “administration format” type of response bias was also potentially present. The effect sizes associated with the 37 observed significant differences ranged from small to medium. These results provide a context for a reexamination of standard techniques for the identification and interpretation of survey research biases. Methods are suggested to strengthen tests for selection bias and to minimize the impact of response biases.},
	language = {en},
	number = {4},
	urldate = {2021-05-26},
	journal = {International Journal of Stress Management},
	author = {Johnson, L. Clark and Beaton, Randal and Murphy, Shirley and Pike, Kenneth},
	month = oct,
	year = {2000},
	pages = {247--267},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YDLGV3UU\\Johnson et al. - 2000 - Sampling Bias and Other Methodological Threats to .pdf:application/pdf},
}

@article{delgado-rodriguez_bias_2004-1,
	title = {Bias},
	volume = {58},
	issn = {0143-005X},
	url = {https://jech.bmj.com/lookup/doi/10.1136/jech.2003.008466},
	doi = {10.1136/jech.2003.008466},
	language = {en},
	number = {8},
	urldate = {2021-05-26},
	journal = {Journal of Epidemiology \& Community Health},
	author = {Delgado-Rodriguez, M},
	month = aug,
	year = {2004},
	pages = {635--641},
	file = {Delgado-Rodriguez - 2004 - Bias.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XGPUVZEJ\\Delgado-Rodriguez - 2004 - Bias.pdf:application/pdf},
}

@article{sterne_funnel_2001,
	title = {Funnel plots for detecting bias in meta-analysis},
	volume = {54},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435601003778},
	doi = {10.1016/S0895-4356(01)00377-8},
	language = {en},
	number = {10},
	urldate = {2021-05-26},
	journal = {Journal of Clinical Epidemiology},
	author = {Sterne, Jonathan A.C and Egger, Matthias},
	month = oct,
	year = {2001},
	pages = {1046--1055},
}

@article{greenland_confounding_2001,
	title = {Confounding in {Health} {Research}},
	volume = {22},
	issn = {0163-7525, 1545-2093},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.publhealth.22.1.189},
	doi = {10.1146/annurev.publhealth.22.1.189},
	language = {en},
	number = {1},
	urldate = {2021-05-26},
	journal = {Annual Review of Public Health},
	author = {Greenland, Sander and Morgenstern, Hal},
	month = may,
	year = {2001},
	pages = {189--212},
	file = {Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\R3GI4ILX\\Greenland und Morgenstern - 2001 - Confounding in Health Research.pdf:application/pdf},
}

@article{potter_epidemiology_2003,
	title = {Epidemiology, cancer genetics and microarrays: making correct inferences, using appropriate designs},
	volume = {19},
	issn = {01689525},
	shorttitle = {Epidemiology, cancer genetics and microarrays},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016895250300297X},
	doi = {10.1016/j.tig.2003.10.005},
	language = {en},
	number = {12},
	urldate = {2021-05-26},
	journal = {Trends in Genetics},
	author = {Potter, John D},
	month = dec,
	year = {2003},
	pages = {690--695},
}

@article{grimes_bias_2002-1,
	title = {Bias and causal associations in observational research},
	volume = {359},
	issn = {01406736},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673602074512},
	doi = {10.1016/S0140-6736(02)07451-2},
	language = {en},
	number = {9302},
	urldate = {2021-05-26},
	journal = {The Lancet},
	author = {Grimes, David A and Schulz, Kenneth F},
	month = jan,
	year = {2002},
	pages = {248--252},
}

@misc{dorak_tevfik_bias_nodate,
	title = {Bias \& {Confounding}},
	url = {http://www.dorak.info/epi/bc.html},
	author = {Dorak, Tevfik},
}

@inproceedings{madras_fairness_2019,
	address = {New York, NY, USA},
	series = {{FAT}* '19},
	title = {Fairness through {Causal} {Awareness}: {Learning} {Causal} {Latent}-{Variable} {Models} for {Biased} {Data}},
	isbn = {978-1-4503-6125-5},
	shorttitle = {Fairness through {Causal} {Awareness}},
	url = {https://doi.org/10.1145/3287560.3287564},
	doi = {10.1145/3287560.3287564},
	abstract = {How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.},
	urldate = {2021-05-26},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
	month = jan,
	year = {2019},
	keywords = {causal inference, fairness in machine learning, variational inference},
	pages = {349--358},
}

@inproceedings{madras_fairness_2019-1,
	address = {New York, NY, USA},
	series = {{FAT}* '19},
	title = {Fairness through {Causal} {Awareness}: {Learning} {Causal} {Latent}-{Variable} {Models} for {Biased} {Data}},
	isbn = {978-1-4503-6125-5},
	shorttitle = {Fairness through {Causal} {Awareness}},
	url = {https://doi.org/10.1145/3287560.3287564},
	doi = {10.1145/3287560.3287564},
	abstract = {How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.},
	urldate = {2021-05-26},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
	month = jan,
	year = {2019},
	keywords = {causal inference, fairness in machine learning, variational inference},
	pages = {349--358},
}

@misc{noauthor_cause_2021,
	title = {Cause and {Effect}: {Concept}-based {Explanation} of {Neural} {Networks}},
	shorttitle = {Cause and {Effect}},
	url = {https://deepai.org/publication/cause-and-effect-concept-based-explanation-of-neural-networks},
	abstract = {05/14/21 - In many scenarios, human decisions are explained based on some high-level
concepts. In this work, we take a step in the interpreta...},
	urldate = {2021-05-26},
	journal = {DeepAI},
	month = may,
	year = {2021},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\25EUXITF\\cause-and-effect-concept-based-explanation-of-neural-networks.html:text/html},
}

@article{zaeem_cause_2021,
	title = {Cause and {Effect}: {Concept}-based {Explanation} of {Neural} {Networks}},
	shorttitle = {Cause and {Effect}},
	url = {http://arxiv.org/abs/2105.07033},
	abstract = {In many scenarios, human decisions are explained based on some high-level concepts. In this work, we take a step in the interpretability of neural networks by examining their internal representation or neuron’s activations against concepts. A concept is characterized by a set of samples that have speciﬁc features in common. We propose a framework to check the existence of a causal relationship between a concept (or its negation) and task classes. While the previous methods focus on the importance of a concept to a task class, we go further and introduce four measures to quantitatively determine the order of causality. Through experiments, we demonstrate the effectiveness of the proposed method in explaining the relationship between a concept and the predictive behaviour of a neural network.},
	language = {en},
	urldate = {2021-05-26},
	journal = {arXiv:2105.07033 [cs]},
	author = {Zaeem, Mohammad Nokhbeh and Komeili, Majid},
	month = may,
	year = {2021},
	note = {arXiv: 2105.07033},
	keywords = {Computer Science - Machine Learning},
	file = {Zaeem und Komeili - 2021 - Cause and Effect Concept-based Explanation of Neu.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SN4YKZ38\\Zaeem und Komeili - 2021 - Cause and Effect Concept-based Explanation of Neu.pdf:application/pdf},
}

@article{beschastnov_current_2018,
	title = {Current {Methods} for the {Assessment} of {Oxygen} {Status} and {Biotissue} {Microcirculation} {Condition}: {Diffuse} {Optical} {Spectroscopy} ({Review})},
	volume = {10},
	shorttitle = {Current {Methods} for the {Assessment} of {Oxygen} {Status} and {Biotissue} {Microcirculation} {Condition}},
	doi = {10.17691/stm2018.10.4.22},
	abstract = {The problem of studying the oxygen status and biotissue microcirculation is of special interest for many directions in medical science since one of the causes of hypoxia development as a typical pathophysiological process is a microcirculatory “failure” associated with the impairment of normal anatomy of the capillary wall, changes in the rheological blood properties, acceleration or slowdown of the blood flow. Current imaging techniques enable the researchers to study the processes of biosystem vital activity at various levels: from organs and tissues to the substance molecular composition. Methods of functional bioimaging implemented into clinical practice provide the opportunity of watching online the processes of substance movement in the body, monitoring blood flow parameters, assessing hypoxia level, characterizing metabolism in greater detail, and, at the same time, correcting timely pathological conditions. The main advantages and disadvantages of bioimaging examination methods such as BOLD functional magnetic resonance tomography, positron emission tomography, optical imaging, laser Doppler flowmetry, and transcutaneous oximetry are considered in the present review. Special attention is paid to diffuse optical spectroscopy as a noninvasive method of lifetime study of substance content in biotissues. The principle of diffuse optical spectroscopy is based on the ability of tissue chromophores (oxyhemoglobin, deoxyhemoglobin, fatty acids, collagen) to absorb diffusely scattered light of a definite wavelength. Their concentrations are calculated with the allowance for the absorption coefficients of chromophores. Diffuse optical spectroscopy is being introduced in clinical practice to diagnose the degree of tumor malignization, evaluate vascularization in reconstructive operations, diagnose hypoxic tissue conditions, monitor intraoperatively blood flow parameters, measure hypoxia levels in diabetes mellitus. It provides the possibility to define and make clear indications to skin plastic surgery and, conceivably, to develop new methods of skin plasty. © 2019, Privolzhsky Research Medical University. All rights reserved.},
	number = {4},
	journal = {Modern Technologies in Medicine},
	author = {Beschastnov, V.V. and Ryabkov, M.G. and Pavlenko, I.V. and Bagryantsev, M.V. and Dezortsev, I.L. and Kichin, V.V. and Baleyev, M.S. and Maslennikova, Anna and Orlova, Anna and Kleshnin, Mikhail and Turchin, Ilya},
	month = dec,
	year = {2018},
	pages = {183--192},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\95AZPGVH\\Beschastnov et al. - 2018 - Current Methods for the Assessment of Oxygen Statu.pdf:application/pdf},
}

@inproceedings{moor_early_2019,
	title = {Early {Recognition} of {Sepsis} with {Gaussian} {Process} {Temporal} {Convolutional} {Networks} and {Dynamic} {Time} {Warping}},
	url = {http://proceedings.mlr.press/v106/moor19a.html},
	abstract = {Sepsis is a life-threatening host response to infection that is associated with high mortality, morbidity, and health costs. Its management is highly time-sensitive because each hour of delayed tre...},
	language = {en},
	urldate = {2021-05-31},
	booktitle = {Machine {Learning} for {Healthcare} {Conference}},
	publisher = {PMLR},
	author = {Moor, Michael and Horn, Max and Rieck, Bastian and Roqueiro, Damian and Borgwardt, Karsten},
	month = oct,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2--26},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Y7CE3DNL\\Moor et al. - 2019 - Early Recognition of Sepsis with Gaussian Process .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UW427AFE\\moor19a.html:text/html},
}

@article{fleuren_machine_2020,
	title = {Machine learning for the prediction of sepsis: a systematic review and meta-analysis of diagnostic test accuracy},
	volume = {46},
	issn = {1432-1238},
	shorttitle = {Machine learning for the prediction of sepsis},
	url = {https://doi.org/10.1007/s00134-019-05872-y},
	doi = {10.1007/s00134-019-05872-y},
	abstract = {Early clinical recognition of sepsis can be challenging. With the advancement of machine learning, promising real-time models to predict sepsis have emerged. We assessed their performance by carrying out a systematic review and meta-analysis.},
	language = {en},
	number = {3},
	urldate = {2021-05-31},
	journal = {Intensive Care Medicine},
	author = {Fleuren, Lucas M. and Klausch, Thomas L. T. and Zwager, Charlotte L. and Schoonmade, Linda J. and Guo, Tingjie and Roggeveen, Luca F. and Swart, Eleonora L. and Girbes, Armand R. J. and Thoral, Patrick and Ercole, Ari and Hoogendoorn, Mark and Elbers, Paul W. G.},
	month = mar,
	year = {2020},
	pages = {383--400},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8TE23BJE\\Fleuren et al. - 2020 - Machine learning for the prediction of sepsis a s.pdf:application/pdf},
}

@article{barton_evaluation_2019,
	title = {Evaluation of a machine learning algorithm for up to 48-hour advance prediction of sepsis using six vital signs},
	volume = {109},
	issn = {1879-0534},
	doi = {10.1016/j.compbiomed.2019.04.027},
	abstract = {OBJECTIVE: Sepsis remains a costly and prevalent syndrome in hospitals; however, machine learning systems can increase timely sepsis detection using electronic health records. This study validates a gradient boosted ensemble machine learning tool for sepsis detection and prediction, and compares its performance to existing methods.
MATERIALS AND METHODS: Retrospective data was drawn from databases at the University of California, San Francisco (UCSF) Medical Center and the Beth Israel Deaconess Medical Center (BIDMC). Adult patient encounters without sepsis on admission, and with at least one recording of each of six vital signs (SpO2, heart rate, respiratory rate, temperature, systolic and diastolic blood pressure) were included. We compared the performance of the machine learning algorithm (MLA) to that of commonly used scoring systems. Area under the receiver operating characteristic (AUROC) curve was our primary measure of accuracy. MLA performance was measured at sepsis onset, and at 24 and 48 h prior to sepsis onset.
RESULTS: The MLA achieved an AUROC of 0.88, 0.84, and 0.83 for sepsis onset and 24 and 48 h prior to onset, respectively. These values were superior to those of SIRS (0.66), MEWS (0.61), SOFA (0.72), and qSOFA (0.60) at time of onset. When trained on UCSF data and tested on BIDMC data, sepsis onset AUROC was 0.89.
DISCUSSION AND CONCLUSION: The MLA predicts sepsis up to 48 h in advance and identifies sepsis onset more accurately than commonly used tools, maintaining high performance for sepsis detection when trained and tested on separate datasets.},
	language = {eng},
	journal = {Computers in Biology and Medicine},
	author = {Barton, Christopher and Chettipally, Uli and Zhou, Yifan and Jiang, Zirui and Lynn-Palevsky, Anna and Le, Sidney and Calvert, Jacob and Das, Ritankar},
	month = jun,
	year = {2019},
	pmid = {31035074},
	pmcid = {PMC6556419},
	keywords = {Humans, Machine Learning, Machine learning, Sepsis, Adolescent, Adult, Aged, Databases, Factual, Diagnosis, Computer-Assisted, Electronic health records, Electronic Health Records, Female, Intensive Care Units, Male, Middle Aged, Prediction, Severity of Illness Index, Vital Signs},
	pages = {79--84},
	file = {Akzeptierte Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\LE2HSSVR\\Barton et al. - 2019 - Evaluation of a machine learning algorithm for up .pdf:application/pdf},
}

@article{kaji_attention_2019,
	title = {An attention based deep learning model of clinical events in the intensive care unit},
	volume = {14},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0211057},
	doi = {10.1371/journal.pone.0211057},
	abstract = {This study trained long short-term memory (LSTM) recurrent neural networks (RNNs) incorporating an attention mechanism to predict daily sepsis, myocardial infarction (MI), and vancomycin antibiotic administration over two week patient ICU courses in the MIMIC-III dataset. These models achieved next-day predictive AUC of 0.876 for sepsis, 0.823 for MI, and 0.833 for vancomycin administration. Attention maps built from these models highlighted those times when input variables most influenced predictions and could provide a degree of interpretability to clinicians. These models appeared to attend to variables that were proxies for clinician decision-making, demonstrating a challenge of using flexible deep learning approaches trained with EHR data to build clinical decision support. While continued development and refinement is needed, we believe that such models could one day prove useful in reducing information overload for ICU physicians by providing needed clinical decision support for a variety of clinically important tasks.},
	language = {en},
	number = {2},
	urldate = {2021-05-31},
	journal = {PLOS ONE},
	author = {Kaji, Deepak A. and Zech, John R. and Kim, Jun S. and Cho, Samuel K. and Dangayach, Neha S. and Costa, Anthony B. and Oermann, Eric K.},
	month = feb,
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Deep learning, Sepsis, Antibiotics, Intensive care units, Ischemia, Myocardial infarction, Troponin, Vancomycin},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\GDM4WEUH\\Kaji et al. - 2019 - An attention based deep learning model of clinical.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SYVHYV5Y\\article.html:text/html},
}

@article{thorsen-meyer_dynamic_2020,
	title = {Dynamic and explainable machine learning prediction of mortality in patients in the intensive care unit: a retrospective study of high-frequency data in electronic patient records},
	volume = {2},
	issn = {2589-7500},
	shorttitle = {Dynamic and explainable machine learning prediction of mortality in patients in the intensive care unit},
	url = {https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20)30018-2/abstract},
	doi = {10.1016/S2589-7500(20)30018-2},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Background{\textless}/h3{\textgreater}{\textless}p{\textgreater}Many mortality prediction models have been developed for patients in intensive care units (ICUs); most are based on data available at ICU admission. We investigated whether machine learning methods using analyses of time-series data improved mortality prognostication for patients in the ICU by providing real-time predictions of 90-day mortality. In addition, we examined to what extent such a dynamic model could be made interpretable by quantifying and visualising the features that drive the predictions at different timepoints.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater}{\textless}p{\textgreater}Based on the Simplified Acute Physiology Score (SAPS) III variables, we trained a machine learning model on longitudinal data from patients admitted to four ICUs in the Capital Region, Denmark, between 2011 and 2016. We included all patients older than 16 years of age, with an ICU stay lasting more than 1 h, and who had a Danish civil registration number to enable 90-day follow-up. We leveraged static data and physiological time-series data from electronic health records and the Danish National Patient Registry. A recurrent neural network was trained with a temporal resolution of 1 h. The model was internally validated using the holdout method with 20\% of the training dataset and externally validated using previously unseen data from a fifth hospital in Denmark. Its performance was assessed with the Matthews correlation coefficient (MCC) and area under the receiver operating characteristic curve (AUROC) as metrics, using bootstrapping with 1000 samples with replacement to construct 95\% CIs. A Shapley additive explanations algorithm was applied to the prediction model to obtain explanations of the features that drive patient-specific predictions, and the contributions of each of the 44 features in the model were analysed and compared with the variables in the original SAPS III model.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Findings{\textless}/h3{\textgreater}{\textless}p{\textgreater}From a dataset containing 15 615 ICU admissions of 12 616 patients, we included 14 190 admissions of 11 492 patients in our analysis. Overall, 90-day mortality was 33·1\% (3802 patients). The deep learning model showed a predictive performance on the holdout testing dataset that improved over the timecourse of an ICU stay: MCC 0·29 (95\% CI 0·25–0·33) and AUROC 0·73 (0·71–0·74) at admission, 0·43 (0·40–0·47) and 0·82 (0·80–0·84) after 24 h, 0·50 (0·46–0·53) and 0·85 (0·84–0·87) after 72 h, and 0·57 (0·54–0·60) and 0·88 (0·87–0·89) at the time of discharge. The model exhibited good calibration properties. These results were validated in an external validation cohort of 5827 patients with 6748 admissions: MCC 0·29 (95\% CI 0·27–0·32) and AUROC 0·75 (0·73–0·76) at admission, 0·41 (0·39–0·44) and 0·80 (0·79–0·81) after 24 h, 0·46 (0·43–0·48) and 0·82 (0·81–0·83) after 72 h, and 0·47 (0·44–0·49) and 0·83 (0·82–0·84) at the time of discharge.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Interpretation{\textless}/h3{\textgreater}{\textless}p{\textgreater}The prediction of 90-day mortality improved with 1-h sampling intervals during the ICU stay. The dynamic risk prediction can also be explained for an individual patient, visualising the features contributing to the prediction at any point in time. This explanation allows the clinician to determine whether there are elements in the current patient state and care that are potentially actionable, thus making the model suitable for further validation as a clinical tool.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Funding{\textless}/h3{\textgreater}{\textless}p{\textgreater}Novo Nordisk Foundation and the Innovation Fund Denmark.{\textless}/p{\textgreater}},
	language = {English},
	number = {4},
	urldate = {2021-05-31},
	journal = {The Lancet Digital Health},
	author = {Thorsen-Meyer, Hans-Christian and Nielsen, Annelaura B. and Nielsen, Anna P. and Kaas-Hansen, Benjamin Skov and Toft, Palle and Schierbeck, Jens and Strøm, Thomas and Chmura, Piotr J. and Heimann, Marc and Dybdahl, Lars and Spangsege, Lasse and Hulsen, Patrick and Belling, Kirstine and Brunak, Søren and Perner, Anders},
	month = apr,
	year = {2020},
	pmid = {33328078},
	note = {Publisher: Elsevier},
	pages = {e179--e191},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\P3IBIYUU\\Thorsen-Meyer et al. - 2020 - Dynamic and explainable machine learning predictio.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EM7N6WU9\\fulltext.html:text/html},
}

@article{bloch_machine_2019,
	title = {Machine {Learning} {Models} for {Analysis} of {Vital} {Signs} {Dynamics}: {A} {Case} for {Sepsis} {Onset} {Prediction}},
	volume = {2019},
	issn = {2040-2295},
	shorttitle = {Machine {Learning} {Models} for {Analysis} of {Vital} {Signs} {Dynamics}},
	url = {https://www.hindawi.com/journals/jhe/2019/5930379/},
	doi = {10.1155/2019/5930379},
	abstract = {Objective. Achieving accurate prediction of sepsis detection moment based on bedside monitor data in the intensive care unit (ICU). A good clinical outcome is more probable when onset is suspected and treated on time, thus early insight of sepsis onset may save lives and reduce costs. Methodology. We present a novel approach for feature extraction, which focuses on the hypothesis that unstable patients are more prone to develop sepsis during ICU stay. These features are used in machine learning algorithms to provide a prediction of a patient’s likelihood to develop sepsis during ICU stay, hours before it is diagnosed. Results. Five machine learning algorithms were implemented using R software packages. The algorithms were trained and tested with a set of 4 features which represent the variability in vital signs. These algorithms aimed to calculate a patient’s probability to become septic within the next 4 hours, based on recordings from the last 8 hours. The best area under the curve (AUC) was achieved with Support Vector Machine (SVM) with radial basis function, which was 88.38\%. Conclusions. The high level of predictive accuracy along with the simplicity and availability of input variables present great potential if applied in ICUs. Variability of a patient’s vital signs proves to be a good indicator of one’s chance to become septic during ICU stay.},
	language = {en},
	urldate = {2021-05-31},
	journal = {Journal of Healthcare Engineering},
	author = {Bloch, Eli and Rotem, Tammy and Cohen, Jonathan and Singer, Pierre and Aperstein, Yehudit},
	month = nov,
	year = {2019},
	note = {Publisher: Hindawi},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\72X5EVI8\\Bloch et al. - 2019 - Machine Learning Models for Analysis of Vital Sign.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\V4PDFE4E\\5930379.html:text/html},
}

@article{winkler_association_2019,
	title = {Association {Between} {Surgical} {Skin} {Markings} in {Dermoscopic} {Images} and {Diagnostic} {Performance} of a {Deep} {Learning} {Convolutional} {Neural} {Network} for {Melanoma} {Recognition}},
	volume = {155},
	issn = {2168-6084},
	doi = {10.1001/jamadermatol.2019.1735},
	abstract = {Importance: Deep learning convolutional neural networks (CNNs) have shown a performance at the level of dermatologists in the diagnosis of melanoma. Accordingly, further exploring the potential limitations of CNN technology before broadly applying it is of special interest.
Objective: To investigate the association between gentian violet surgical skin markings in dermoscopic images and the diagnostic performance of a CNN approved for use as a medical device in the European market.
Design and Setting: A cross-sectional analysis was conducted from August 1, 2018, to November 30, 2018, using a CNN architecture trained with more than 120 000 dermoscopic images of skin neoplasms and corresponding diagnoses. The association of gentian violet skin markings in dermoscopic images with the performance of the CNN was investigated in 3 image sets of 130 melanocytic lesions each (107 benign nevi, 23 melanomas).
Exposures: The same lesions were sequentially imaged with and without the application of a gentian violet surgical skin marker and then evaluated by the CNN for their probability of being a melanoma. In addition, the markings were removed by manually cropping the dermoscopic images to focus on the melanocytic lesion.
Main Outcomes and Measures: Sensitivity, specificity, and area under the curve (AUC) of the receiver operating characteristic (ROC) curve for the CNN's diagnostic classification in unmarked, marked, and cropped images.
Results: In all, 130 melanocytic lesions (107 benign nevi and 23 melanomas) were imaged. In unmarked lesions, the CNN achieved a sensitivity of 95.7\% (95\% CI, 79\%-99.2\%) and a specificity of 84.1\% (95\% CI, 76.0\%-89.8\%). The ROC AUC was 0.969. In marked lesions, an increase in melanoma probability scores was observed that resulted in a sensitivity of 100\% (95\% CI, 85.7\%-100\%) and a significantly reduced specificity of 45.8\% (95\% CI, 36.7\%-55.2\%, P {\textless} .001). The ROC AUC was 0.922. Cropping images led to the highest sensitivity of 100\% (95\% CI, 85.7\%-100\%), specificity of 97.2\% (95\% CI, 92.1\%-99.0\%), and ROC AUC of 0.993. Heat maps created by vanilla gradient descent backpropagation indicated that the blue markings were associated with the increased false-positive rate.
Conclusions and Relevance: This study's findings suggest that skin markings significantly interfered with the CNN's correct diagnosis of nevi by increasing the melanoma probability scores and consequently the false-positive rate. A predominance of skin markings in melanoma training images may have induced the CNN's association of markings with a melanoma diagnosis. Accordingly, these findings suggest that skin markings should be avoided in dermoscopic images intended for analysis by a CNN.
Trial Registration: German Clinical Trial Register (DRKS) Identifier: DRKS00013570.},
	language = {eng},
	number = {10},
	journal = {JAMA dermatology},
	author = {Winkler, Julia K. and Fink, Christine and Toberer, Ferdinand and Enk, Alexander and Deinlein, Teresa and Hofmann-Wellenhof, Rainer and Thomas, Luc and Lallas, Aimilios and Blum, Andreas and Stolz, Wilhelm and Haenssle, Holger A.},
	month = oct,
	year = {2019},
	pmid = {31411641},
	pmcid = {PMC6694463},
	pages = {1135--1141},
	file = {Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2LMIA5GY\\Winkler et al. - 2019 - Association Between Surgical Skin Markings in Derm.pdf:application/pdf},
}

@article{chan_machine_2020,
	title = {Machine {Learning} in {Dermatology}: {Current} {Applications}, {Opportunities}, and {Limitations}},
	volume = {10},
	issn = {2190-9172},
	shorttitle = {Machine {Learning} in {Dermatology}},
	url = {https://doi.org/10.1007/s13555-020-00372-0},
	doi = {10.1007/s13555-020-00372-0},
	abstract = {Machine learning (ML) has the potential to improve the dermatologist’s practice from diagnosis to personalized treatment. Recent advancements in access to large datasets (e.g., electronic medical records, image databases, omics), faster computing, and cheaper data storage have encouraged the development of ML algorithms with human-like intelligence in dermatology. This article is an overview of the basics of ML, current applications of ML, and potential limitations and considerations for further development of ML. We have identified five current areas of applications for ML in dermatology: (1) disease classification using clinical images; (2) disease classification using dermatopathology images; (3) assessment of skin diseases using mobile applications and personal monitoring devices; (4) facilitating large-scale epidemiology research; and (5) precision medicine. The purpose of this review is to provide a guide for dermatologists to help demystify the fundamentals of ML and its wide range of applications in order to better evaluate its potential opportunities and challenges.},
	language = {en},
	number = {3},
	urldate = {2021-05-31},
	journal = {Dermatology and Therapy},
	author = {Chan, Stephanie and Reddy, Vidhatha and Myers, Bridget and Thibodeaux, Quinn and Brownstone, Nicholas and Liao, Wilson},
	month = jun,
	year = {2020},
	pages = {365--386},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2W7NFZ8Q\\Chan et al. - 2020 - Machine Learning in Dermatology Current Applicati.pdf:application/pdf},
}

@article{esteva_dermatologist-level_2017-2,
	title = {Dermatologist-level classification of skin cancer with deep neural networks},
	volume = {542},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature21056},
	doi = {10.1038/nature21056},
	abstract = {An artificial intelligence trained to classify images of skin lesions as benign lesions or malignant skin cancers achieves the accuracy of board-certified dermatologists.},
	language = {en},
	number = {7639},
	urldate = {2021-05-31},
	journal = {Nature},
	author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
	month = feb,
	year = {2017},
	note = {Number: 7639
Publisher: Nature Publishing Group},
	pages = {115--118},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\N363342E\\Esteva et al. - 2017 - Dermatologist-level classification of skin cancer .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IEVSBCRP\\nature21056.html:text/html},
}

@article{liu_deep_2020,
	title = {A deep learning system for differential diagnosis of skin diseases},
	volume = {26},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-020-0842-3},
	doi = {10.1038/s41591-020-0842-3},
	abstract = {Skin conditions affect 1.9 billion people. Because of a shortage of dermatologists, most cases are seen instead by general practitioners with lower diagnostic accuracy. We present a deep learning system (DLS) to provide a differential diagnosis of skin conditions using 16,114 de-identified cases (photographs and clinical data) from a teledermatology practice serving 17 sites. The DLS distinguishes between 26 common skin conditions, representing 80\% of cases seen in primary care, while also providing a secondary prediction covering 419 skin conditions. On 963 validation cases, where a rotating panel of three board-certified dermatologists defined the reference standard, the DLS was non-inferior to six other dermatologists and superior to six primary care physicians (PCPs) and six nurse practitioners (NPs) (top-1 accuracy: 0.66 DLS, 0.63 dermatologists, 0.44 PCPs and 0.40 NPs). These results highlight the potential of the DLS to assist general practitioners in diagnosing skin conditions.},
	language = {en},
	number = {6},
	urldate = {2021-05-31},
	journal = {Nature Medicine},
	author = {Liu, Yuan and Jain, Ayush and Eng, Clara and Way, David H. and Lee, Kang and Bui, Peggy and Kanada, Kimberly and de Oliveira Marinho, Guilherme and Gallegos, Jessica and Gabriele, Sara and Gupta, Vishakha and Singh, Nalini and Natarajan, Vivek and Hofmann-Wellenhof, Rainer and Corrado, Greg S. and Peng, Lily H. and Webster, Dale R. and Ai, Dennis and Huang, Susan J. and Liu, Yun and Dunn, R. Carter and Coz, David},
	month = jun,
	year = {2020},
	pages = {900--908},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8CRI49D4\\Liu et al. - 2020 - A deep learning system for differential diagnosis .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7ZUCYQAN\\s41591-020-0842-3.html:text/html},
}

@article{choy_current_2018,
	title = {Current {Applications} and {Future} {Impact} of {Machine} {Learning} in {Radiology}},
	volume = {288},
	issn = {0033-8419},
	url = {https://pubs.rsna.org/doi/full/10.1148/radiol.2018171820},
	doi = {10.1148/radiol.2018171820},
	abstract = {Recent advances and future perspectives of machine learning techniques offer promising applications in medical imaging. Machine learning has the potential to improve different steps of the radiology workflow including order scheduling and triage, clinical decision support systems, detection and interpretation of findings, postprocessing and dose estimation, examination quality control, and radiology reporting. In this article, the authors review examples of current applications of machine learning and artificial intelligence techniques in diagnostic radiology. In addition, the future impact and natural extension of these techniques in radiology practice are discussed.© RSNA, 2018},
	number = {2},
	urldate = {2021-05-31},
	journal = {Radiology},
	author = {Choy, Garry and Khalilzadeh, Omid and Michalski, Mark and Do, Synho and Samir, Anthony E. and Pianykh, Oleg S. and Geis, J. Raymond and Pandharipande, Pari V. and Brink, James A. and Dreyer, Keith J.},
	month = jun,
	year = {2018},
	pages = {318--328},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\49EKZKR7\\Choy et al. - 2018 - Current Applications and Future Impact of Machine .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XZBEYW6C\\radiol.html:text/html},
}

@article{hosny_artificial_2018,
	title = {Artificial intelligence in radiology},
	volume = {18},
	copyright = {2018 Macmillan Publishers Ltd., part of Springer Nature},
	issn = {1474-1768},
	url = {https://www.nature.com/articles/s41568-018-0016-5},
	doi = {10.1038/s41568-018-0016-5},
	abstract = {Artificial intelligence (AI) algorithms, particularly deep learning, have demonstrated remarkable progress in image-recognition tasks. Methods ranging from convolutional neural networks to variational autoencoders have found myriad applications in the medical image analysis field, propelling it forward at a rapid pace. Historically, in radiology practice, trained physicians visually assessed medical images for the detection, characterization and monitoring of diseases. AI methods excel at automatically recognizing complex patterns in imaging data and providing quantitative, rather than qualitative, assessments of radiographic characteristics. In this Opinion article, we establish a general understanding of AI methods, particularly those pertaining to image-based tasks. We explore how these methods could impact multiple facets of radiology, with a general focus on applications in oncology, and demonstrate ways in which these methods are advancing the field. Finally, we discuss the challenges facing clinical implementation and provide our perspective on how the domain could be advanced.},
	language = {en},
	number = {8},
	urldate = {2021-05-31},
	journal = {Nature Reviews Cancer},
	author = {Hosny, Ahmed and Parmar, Chintan and Quackenbush, John and Schwartz, Lawrence H. and Aerts, Hugo J. W. L.},
	month = aug,
	year = {2018},
	pages = {500--510},
	file = {Akzeptierte Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TJFYYFRR\\Hosny et al. - 2018 - Artificial intelligence in radiology.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XEBE5PJ7\\s41568-018-0016-5.html:text/html},
}

@article{sung_artificial_2020,
	title = {Artificial intelligence in gastroenterology: where are we heading?},
	volume = {14},
	issn = {2095-0225},
	shorttitle = {Artificial intelligence in gastroenterology},
	url = {https://doi.org/10.1007/s11684-020-0742-4},
	doi = {10.1007/s11684-020-0742-4},
	abstract = {Artificial intelligence (AI) is coming to medicine in a big wave. From making diagnosis in various medical conditions, following the latest advancements in scientific literature, suggesting appropriate therapies, to predicting prognosis and outcome of diseases and conditions, AI is offering unprecedented possibilities to improve care for patients. Gastroenterology is a field that AI can make a significant impact. This is partly because the diagnosis of gastrointestinal conditions relies a lot on image-based investigations and procedures (endoscopy and radiology). AI-assisted image analysis can make accurate assessment and provide more information than conventional analysis. AI integration of genomic, epigenetic, and metagenomic data may offer new classifications of gastrointestinal cancers and suggest optimal personalized treatments. In managing relapsing and remitting diseases such as inflammatory bowel disease, irritable bowel syndrome, and peptic ulcer bleeding, convoluted neural network may formulate models to predict disease outcome, enhancing treatment efficacy. AI and surgical robots can also assist surgeons in conducting gastrointestinal operations. While the advancement and new opportunities are exciting, the responsibility and liability issues of AI-assisted diagnosis and management need much deliberations.},
	language = {en},
	number = {4},
	urldate = {2021-05-31},
	journal = {Frontiers of Medicine},
	author = {Sung, Joseph JY and Poon, Nicholas CH},
	month = aug,
	year = {2020},
	pages = {511--517},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\I89K6AHP\\Sung und Poon - 2020 - Artificial intelligence in gastroenterology where.pdf:application/pdf},
}

@misc{stidham_ryan_w_artificial_nodate,
	title = {Artificial {Intelligence} for {Understanding} {Imaging}, {Text}, and {Data} in {Gastroenterology} – {Gastroenterology} \& {Hepatology}},
	url = {https://www.gastroenterologyandhepatology.net/archives/july-2020/artificial-intelligence-for-understanding-imaging-text-and-data-in-gastroenterology/},
	language = {en-US},
	urldate = {2021-05-31},
	author = {Stidham, Ryan W.},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FYPSLMZH\\artificial-intelligence-for-understanding-imaging-text-and-data-in-gastroenterology.html:text/html},
}

@article{straw_automation_2020,
	title = {The automation of bias in medical {Artificial} {Intelligence} ({AI}): {Decoding} the past to create a better future},
	volume = {110},
	issn = {0933-3657},
	shorttitle = {The automation of bias in medical {Artificial} {Intelligence} ({AI})},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365720312306},
	doi = {10.1016/j.artmed.2020.101965},
	abstract = {Medicine is at a disciplinary crossroads. With the rapid integration of Artificial Intelligence (AI) into the healthcare field the future care of our patients will depend on the decisions we make now. Demographic healthcare inequalities continue to persist worldwide and the impact of medical biases on different patient groups is still being uncovered by the research community. At a time when clinical AI systems are scaled up in response to the Covid19 pandemic, the role of AI in exacerbating health disparities must be critically reviewed. For AI to account for the past and build a better future, we must first unpack the present and create a new baseline on which to develop these tools. The means by which we move forwards will determine whether we project existing inequity into the future, or whether we reflect on what we hold to be true and challenge ourselves to be better. AI is an opportunity and a mirror for all disciplines to improve their impact on society and for medicine the stakes could not be higher.},
	language = {en},
	urldate = {2021-05-31},
	journal = {Artificial Intelligence in Medicine},
	author = {Straw, Isabel},
	month = nov,
	year = {2020},
	keywords = {Artificial intelligence, Bias, Data science, Digital health, Disparities, Health, Healthcare, Inequality, Medicine},
	pages = {101965},
	file = {ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QV39GLDJ\\S0933365720312306.html:text/html},
}

@article{yu_framing_2019,
	title = {Framing the challenges of artificial intelligence in medicine},
	volume = {28},
	copyright = {© Author(s) (or their employer(s)) 2019. No commercial re-use. See rights and permissions. Published by BMJ.},
	issn = {2044-5415, 2044-5423},
	url = {https://qualitysafety.bmj.com/content/28/3/238},
	doi = {10.1136/bmjqs-2018-008551},
	abstract = {On a clear January morning in Florida, a Tesla enthusiast and network entrepreneur was driving his new Tesla Model S on US Highway 27A, returning from a family trip. He had posted dozens of widely circulated YouTube tutorial videos on his vehicle and clearly understood many of the technical details of his car. That day, he let the vehicle run autonomously on Autopilot mode for 37 min, before it crashed into the trailer of a truck turning left. The Autopilot did not identify the white side of the trailer as a potential hazard, and the driver was killed, leaving his family and his high-tech business behind.1 This tragedy is not a metaphor for artificial intelligence (AI) applications but an example of a long-recognised challenge in AI: the Frame Problem.2 Although rarely appreciated in the scholarly and lay descriptions of the stunning recent successes of AI in medical applications, the Frame Problem and related AI challenges will have unintended harmful effects to the care of patients if not directly addressed.

With the recent advancement in machine learning algorithms, many medical tasks previously thought to require human expertise have been replicated by AI systems at or above the level of accuracy in human experts. These important demonstrations range from evaluating fundus retinography3 and histopathology4 to reading chest radiographs5 and assessment of skin lesions.6 These studies have encompassed very large numbers of patient cases and have been extensively benchmarked against clinicians. However, all these studies are retrospective in that they involve a collection of labelled cases against which the AI systems are trained and another collection against which they are tested or validated. So far, they have not entered into routine prospective use in the clinic where the Frame Problem will manifest itself most pathologically.

The Frame …},
	language = {en},
	number = {3},
	urldate = {2021-05-31},
	journal = {BMJ Quality \& Safety},
	author = {Yu, Kun-Hsing and Kohane, Isaac S.},
	month = mar,
	year = {2019},
	pmid = {30291179},
	note = {Publisher: BMJ Publishing Group Ltd
Section: Viewpoint},
	keywords = {decision support, computerized, general practice, implementation science, information technology, patient safety},
	pages = {238--241},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CS2KBGWB\\Yu und Kohane - 2019 - Framing the challenges of artificial intelligence .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JW43QYE8\\238.html:text/html},
}

@article{mac_namee_problem_2002,
	title = {The problem of bias in training data in regression problems in medical decision support},
	volume = {24},
	issn = {0933-3657},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365701000926},
	doi = {10.1016/S0933-3657(01)00092-6},
	abstract = {This paper describes a bias problem encountered in a machine learning approach to outcome prediction in anticoagulant drug therapy. The outcome to be predicted is a measure of the clotting time for the patient; this measure is continuous and so the prediction task is a regression problem. Artificial neural networks (ANNs) are a powerful mechanism for learning to predict such outcomes from training data. However, experiments have shown that an ANN is biased towards values more commonly occurring in the training data and is thus, less likely to be correct in predicting extreme values. This issue of bias in training data in regression problems is similar to the associated problem with minority classes in classification. However, this bias issue in classification is well documented and is an on-going area of research. In this paper, we consider stratified sampling and boosting as solutions to this bias problem and evaluate them on this outcome prediction problem and on two other datasets. Both approaches produce some improvements with boosting showing the most promise.},
	language = {en},
	number = {1},
	urldate = {2021-05-31},
	journal = {Artificial Intelligence in Medicine},
	author = {Mac Namee, B. and Cunningham, P. and Byrne, S. and Corrigan, O. I.},
	month = jan,
	year = {2002},
	keywords = {Artificial neural networks, Anticoagulant drug therapy, Medical decision support, Regression},
	pages = {51--70},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CAZTSK3H\\Mac Namee et al. - 2002 - The problem of bias in training data in regression.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\USV9BHGE\\S0933365701000926.html:text/html},
}

@article{panch_inconvenient_2019,
	title = {The “inconvenient truth” about {AI} in healthcare},
	volume = {2},
	copyright = {2019 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-019-0155-4},
	doi = {10.1038/s41746-019-0155-4},
	language = {en},
	number = {1},
	urldate = {2021-05-31},
	journal = {npj Digital Medicine},
	author = {Panch, Trishan and Mattie, Heather and Celi, Leo Anthony},
	month = aug,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1--3},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8VP6ZF7K\\Panch et al. - 2019 - The “inconvenient truth” about AI in healthcare.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EDKAKG2L\\s41746-019-0155-4.html:text/html},
}

@article{rajkomar_machine_2019,
	title = {Machine {Learning} in {Medicine}},
	copyright = {Copyright © 2019 Massachusetts Medical Society. All rights reserved.},
	url = {https://www.nejm.org/doi/10.1056/NEJMra1814259},
	abstract = {Review Article from The New England Journal of Medicine — Machine Learning in Medicine},
	language = {en},
	urldate = {2021-05-31},
	journal = {New England Journal of Medicine},
	author = {Rajkomar, Alvin and Dean, Jeffrey and Kohane, Isaac},
	month = apr,
	year = {2019},
	note = {Publisher: Massachusetts Medical Society},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7R3CC4DD\\NEJMra1814259.html:text/html},
}

@misc{noauthor_artificial_nodate,
	title = {Artificial {Intelligence} for {Understanding} {Imaging}, {Text}, and {Data} in {Gastroenterology} – {Gastroenterology} \& {Hepatology}},
	url = {https://www.gastroenterologyandhepatology.net/archives/july-2020/artificial-intelligence-for-understanding-imaging-text-and-data-in-gastroenterology/},
	language = {en-US},
	urldate = {2021-05-31},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SMRZWTHY\\artificial-intelligence-for-understanding-imaging-text-and-data-in-gastroenterology.html:text/html},
}

@misc{noauthor_artificial_nodate-1,
	title = {Artificial {Intelligence} for {Understanding} {Imaging}, {Text}, and {Data} in {Gastroenterology} – {Gastroenterology} \& {Hepatology}},
	url = {https://www.gastroenterologyandhepatology.net/archives/july-2020/artificial-intelligence-for-understanding-imaging-text-and-data-in-gastroenterology/},
	language = {en-US},
	urldate = {2021-05-31},
}

@article{stidham_artificial_2020,
	title = {Artificial {Intelligence} for {Understanding} {Imaging}, {Text}, and {Data} in {Gastroenterology}},
	volume = {16},
	abstract = {Artificial intelligence (AI) could change the practice of gastroenterology through its ability to both acquire and analyze information with speed, reproducibility, and, potentially, insight that may exceed that of human medical specialists. AI is powered by computational methods that allow machines to replicate clinical pattern recognition used by gastroenterology specialists to interpret endoscopic or cross-sectional images; understand the meaning and intent of medical documents; and merge different types of data to infer a diagnosis, prognosis, or expected outcome. Ongoing research is studying the use of AI for automated interpretation of text from colonoscopy and clinical documents for improved quality and patient phenotyping as well as enhanced detection and descriptions of polyps and other endoscopic lesions, and for predicting the probability of future therapeutic response early in a treatment course. This article introduces emerging technologies of natural language processing, machine vision, and machine learning for data analytics, and describes current and future applications in gastroenterology.},
	language = {en},
	number = {7},
	journal = {Gastroenterology \& Hepatology},
	author = {Stidham, Ryan W},
	month = jul,
	year = {2020},
	pages = {9},
	file = {Stidham - Artificial Intelligence for Understanding Imaging,.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FPJ33GM5\\Stidham - Artificial Intelligence for Understanding Imaging,.pdf:application/pdf},
}

@article{challen_artificial_2019,
	title = {Artificial intelligence, bias and clinical safety},
	volume = {28},
	issn = {2044-5415, 2044-5423},
	url = {https://qualitysafety.bmj.com/lookup/doi/10.1136/bmjqs-2018-008370},
	doi = {10.1136/bmjqs-2018-008370},
	language = {en},
	number = {3},
	urldate = {2021-05-31},
	journal = {BMJ Quality \& Safety},
	author = {Challen, Robert and Denny, Joshua and Pitt, Martin and Gompels, Luke and Edwards, Tom and Tsaneva-Atanasova, Krasimira},
	month = mar,
	year = {2019},
	pages = {231--237},
	file = {Challen et al. - 2019 - Artificial intelligence, bias and clinical safety.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IKWQHXC6\\Challen et al. - 2019 - Artificial intelligence, bias and clinical safety.pdf:application/pdf},
}

@article{davis_calibration_2017,
	title = {Calibration drift in regression and machine learning models for acute kidney injury},
	volume = {24},
	issn = {1527-974X},
	doi = {10.1093/jamia/ocx030},
	abstract = {Objective: Predictive analytics create opportunities to incorporate personalized risk estimates into clinical decision support. Models must be well calibrated to support decision-making, yet calibration deteriorates over time. This study explored the influence of modeling methods on performance drift and connected observed drift with data shifts in the patient population.
Materials and Methods: Using 2003 admissions to Department of Veterans Affairs hospitals nationwide, we developed 7 parallel models for hospital-acquired acute kidney injury using common regression and machine learning methods, validating each over 9 subsequent years.
Results: Discrimination was maintained for all models. Calibration declined as all models increasingly overpredicted risk. However, the random forest and neural network models maintained calibration across ranges of probability, capturing more admissions than did the regression models. The magnitude of overprediction increased over time for the regression models while remaining stable and small for the machine learning models. Changes in the rate of acute kidney injury were strongly linked to increasing overprediction, while changes in predictor-outcome associations corresponded with diverging patterns of calibration drift across methods.
Conclusions: Efficient and effective updating protocols will be essential for maintaining accuracy of, user confidence in, and safety of personalized risk predictions to support decision-making. Model updating protocols should be tailored to account for variations in calibration drift across methods and respond to periods of rapid performance drift rather than be limited to regularly scheduled annual or biannual intervals.},
	language = {eng},
	number = {6},
	journal = {Journal of the American Medical Informatics Association: JAMIA},
	author = {Davis, Sharon E. and Lasko, Thomas A. and Chen, Guanhua and Siew, Edward D. and Matheny, Michael E.},
	month = nov,
	year = {2017},
	pmid = {28379439},
	pmcid = {PMC6080675},
	keywords = {Humans, calibration, Bayes Theorem, Machine Learning, machine learning, Aged, Female, Male, Middle Aged, acute kidney injury, Acute Kidney Injury, clinical decision support, Clinical Decision-Making, clinical prediction, Decision Support Techniques, discrimination, Hospitals, Veterans, Logistic Models, United States},
	pages = {1052--1061},
	file = {Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ILGUGEID\\Davis et al. - 2017 - Calibration drift in regression and machine learni.pdf:application/pdf},
}

@article{amodei_concrete_2016,
	title = {Concrete {Problems} in {AI} {Safety}},
	url = {http://arxiv.org/abs/1606.06565},
	abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	urldate = {2021-05-31},
	journal = {arXiv:1606.06565 [cs]},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	month = jul,
	year = {2016},
	note = {arXiv: 1606.06565},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EXET27CH\\Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\BD57KWSS\\1606.html:text/html},
}

@article{chin_evaluation_2011,
	title = {Evaluation of hyperspectral technology for assessing the presence and severity of peripheral artery disease},
	volume = {54},
	issn = {07415214},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S074152141101384X},
	doi = {10.1016/j.jvs.2011.06.022},
	abstract = {Background: Hyperspectral imaging is a novel technology that can noninvasively measure oxyhemoglobin and deoxyhemoglobin concentrations to create an anatomic oxygenation map. It has predicted healing of diabetic foot ulcers; however, its ability to assess peripheral arterial disease (PAD) has not been studied. The aims of this study were to determine if hyperspectral imaging could accurately assess the presence or absence of PAD and accurately predict PAD severity.
Methods: This prospective study included consecutive consenting patients presenting to the vascular laboratory at the Jesse Brown VA Medical Center during a 10-week period for a lower extremity arterial study, including ankle-brachial index (ABI) and Doppler waveforms. Patients with lower extremity edema were excluded. Patients underwent hyperspectral imaging at nine angiosomes on each extremity. Additional sites were imaged when tissue loss was present. Medical records of enrolled patients were reviewed for demographic data, active medications, surgical history, and other information pertinent to PAD. Patients were separated into no-PAD and PAD groups. Differences in hyperspectral values between the groups were evaluated using the two-tailed t test. Analysis for differences in values over varying severities of PAD, as deﬁned by triphasic, biphasic, or monophasic Doppler waveforms, was conducted using one-way analysis of variance. Hyperspectral values were correlated with the ABI using a Pearson bivariate linear correlation test.
Results: The study enrolled 126 patients (252 limbs). After exclusion of 15 patients, 111 patients were left for analysis, including 46 (92 limbs) no-PAD patients and 65 (130 limbs) PAD patients. Groups differed in age, diabetes, coronary artery disease, congestive heart failure, tobacco use, and insulin use. Deoxyhemoglobin values for the plantar metatarsal, arch, and heel angiosomes were signiﬁcantly different between patients with and without PAD (P {\textless} .005). Mean deoxyhemoglobin values for the same three angiosomes showed signiﬁcant differences between patients with monophasic, biphasic, and triphasic waveforms (P {\textless} .05). In patients with PAD, there was also signiﬁcant correlation between deoxyhemoglobin values and ABI for the same three angiosomes (P ؍ .001). Oxyhemoglobin values did not predict the presence or absence of PAD, did not correlate with PAD severity, and did not correlate with the ABI.
Conclusions: These results suggest the ability of hyperspectral imaging to detect the presence of PAD. Hyperspectral measurements can also evaluate different severities of PAD. ( J Vasc Surg 2011;54:1679-88.)},
	language = {en},
	number = {6},
	urldate = {2021-06-08},
	journal = {Journal of Vascular Surgery},
	author = {Chin, Jason A. and Wang, Edward C. and Kibbe, Melina R.},
	month = dec,
	year = {2011},
	pages = {1679--1688},
	file = {Chin et al. - 2011 - Evaluation of hyperspectral technology for assessi.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YWUNKQHA\\Chin et al. - 2011 - Evaluation of hyperspectral technology for assessi.pdf:application/pdf},
}

@article{chin_hyperspectral_2012,
	title = {Hyperspectral imaging for early detection of oxygenation and perfusion changes in irradiated skin},
	volume = {17},
	issn = {1083-3668},
	url = {http://biomedicaloptics.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JBO.17.2.026010},
	doi = {10.1117/1.JBO.17.2.026010},
	language = {en},
	number = {2},
	urldate = {2021-06-08},
	journal = {Journal of Biomedical Optics},
	author = {Chin, Michael S.},
	month = mar,
	year = {2012},
	file = {Chin - 2012 - Hyperspectral imaging for early detection of oxyge.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\AWR6EAD7\\Chin - 2012 - Hyperspectral imaging for early detection of oxyge.pdf:application/pdf},
}

@article{rahrovan_male_2018,
	title = {Male versus female skin: {What} dermatologists and cosmeticians should know},
	volume = {4},
	issn = {2352-6475},
	shorttitle = {Male versus female skin},
	url = {https://www.sciencedirect.com/science/article/pii/S2352647518300133},
	doi = {10.1016/j.ijwd.2018.03.002},
	abstract = {Introduction
The skin is important for the perception of health and beauty. Knowledge of the physiological, chemical, and biophysical differences between the skin of male and female patients helps dermatologists develop a proper approach not only for the management of skin diseases but also to properly take care of cosmetic issues. The influence of genetic and environmental factors on skin characteristics is also critical to consider.
Methods
A literature search of PubMed and Google was conducted to compare the biophysical and biomechanical properties of the skin of male and female patients using the keywords "skin", "hydration", "water loss", "sebum", "circulation", "color", "thickness", "elasticity", "pH", "friction", "wrinkle", "sex", "male", and "female".
Results
A total of 1070 titles were found. After removing duplications and non-English papers, the number was reduced to 632. Of the 632 titles, 57 were deemed suitable for inclusion in this review. The studies show that the skin parameters of hydration, transepidermal water loss, sebum, microcirculation, pigmentation, and thickness are generally higher in men but skin pH is higher in women.
Conclusions
These parameters can be considered as age markers in some cases and are susceptible to change according to environment and life style. Biometrological studies of the skin provide useful information in the selection of active principles and other ingredients of formulations to develop a specific approach for cosmetic treatments.},
	language = {en},
	number = {3},
	urldate = {2021-06-09},
	journal = {International Journal of Women's Dermatology},
	author = {Rahrovan, S. and Fanian, F. and Mehryan, P. and Humbert, P. and Firooz, A.},
	month = sep,
	year = {2018},
	pages = {122--130},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2GTB7AYC\\Rahrovan et al. - 2018 - Male versus female skin What dermatologists and c.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5EULWQBD\\S2352647518300133.html:text/html},
}

@article{friedman_fluid_2010,
	title = {Fluid and electrolyte therapy: a primer},
	volume = {25},
	issn = {0931-041X},
	shorttitle = {Fluid and electrolyte therapy},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2839473/},
	doi = {10.1007/s00467-009-1189-7},
	abstract = {The prescription of fluid therapy in pediatrics is a common clinical event. The foundations that underpin such therapy should be understood by all clinicians involved in the short-term care of children. This article describes some important basic principles of fluid management.},
	number = {5},
	urldate = {2021-06-09},
	journal = {Pediatric Nephrology (Berlin, Germany)},
	author = {Friedman, Aaron},
	month = may,
	year = {2010},
	pmid = {19444484},
	pmcid = {PMC2839473},
	pages = {843--846},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\R6G4NBJH\\Friedman - 2010 - Fluid and electrolyte therapy a primer.pdf:application/pdf},
}

@article{marx_fluid_2003,
	title = {Fluid therapy in sepsis with capillary leakage},
	volume = {20},
	issn = {0265-0215},
	url = {https://journals.lww.com/ejanaesthesiology/Fulltext/2003/06000/Fluid_therapy_in_sepsis_with_capillary_leakage.2.aspx},
	abstract = {Sepsis is associated with a profound intravascular fluid deficit due to vasodilatation, venous pooling and capillary leakage. Fluid therapy is aimed at restoration of intravascular volume status, haemodynamic stability and organ perfusion. Circulatory stability following fluid resuscitation is usually achieved in the septic patient at the expense of tissue oedema formation that may significantly influence vital organ function. The type of fluid therapy, crystalloid or colloid, in sepsis with capillary leakage remains an area of intensive and controversial discussion. The current understanding of the physiology of increased microvascular permeability in health and sepsis is incomplete. Furthermore, there is a lack of appropriate clinical study end-points for fluid resuscitation. This review considers critically the clinical and experimental data analysing the assessment of capillary leakage in sepsis and investigating the effects of different fluid types on increased microvascular permeability in sepsis.},
	language = {en-US},
	number = {6},
	urldate = {2021-06-09},
	journal = {European Journal of Anaesthesiology {\textbar} EJA},
	author = {Marx, G.},
	month = jun,
	year = {2003},
	pages = {429--442},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WCTB8KEV\\Fluid_therapy_in_sepsis_with_capillary_leakage.2.html:text/html},
}

@incollection{vanvalkinburgh_inotropes_2021,
	address = {Treasure Island (FL)},
	title = {Inotropes {And} {Vasopressors}},
	copyright = {Copyright © 2021, StatPearls Publishing LLC.},
	url = {http://www.ncbi.nlm.nih.gov/books/NBK482411/},
	abstract = {Vasopressors and inotropes are medications used to create vasoconstriction or increase cardiac contractility, respectively, in patients with shock. The hallmark of shock is decreased perfusion to vital organs, resulting in multiorgan dysfunction and eventually death. Vasopressors increase vasoconstriction, which leads to increased systemic vascular resistance (SVR). Increasing the SVR leads to increased mean arterial pressure (MAP) and increased perfusion to organs. Inotropes increase cardiac contractility which improves cardiac output (CO), aiding in maintaining MAP and perfusion to the body. This activity describes the mode of action of inotropes and vasopressors, including mechanism of action, pharmacology, adverse event profiles, eligible patient populations, monitoring, and highlights the role of the interprofessional team in the management of conditions where vasopressors and inotropes.},
	language = {eng},
	urldate = {2021-06-09},
	booktitle = {{StatPearls}},
	publisher = {StatPearls Publishing},
	author = {VanValkinburgh, Danny and Kerndt, Connor C. and Hashmi, Muhammad F.},
	year = {2021},
	pmid = {29494018},
	file = {Printable HTML:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\K84FHZPE\\NBK482411.html:text/html},
}

@article{zuzak_visible_2002,
	title = {Visible {Reflectance} {Hyperspectral} {Imaging}:  {Characterization} of a {Noninvasive}, in {Vivo} {System} for {Determining} {Tissue} {Perfusion}},
	volume = {74},
	issn = {0003-2700},
	shorttitle = {Visible {Reflectance} {Hyperspectral} {Imaging}},
	url = {https://doi.org/10.1021/ac011275f},
	doi = {10.1021/ac011275f},
	abstract = {We characterize a visible reflectance hyperspectral imaging system for noninvasive, in vivo, quantitative analysis of human tissue in a clinical environment. The subject area is illuminated with a quartz−tungsten−halogen light source, and the reflected light is spectrally discriminated by a liquid crystal tunable filter (LCTF) and imaged onto a silicon charge-coupled device detector. The LCTF is continuously tunable within its useful visible spectral range (525−725 nm) with an average spectral full width at half-height bandwidth of 0.38 nm and an average transmittance of 10.0\%. A standard resolution target placed 5.5 ft from the system results in a field of view with a 17-cm diameter and an optimal spatial resolution of 0.45 mm. The measured reflectance spectra are quantified in terms of apparent absorbance and formatted as a hyperspectral image cube. As a clinical example, we examine a model of vascular dysfunction involving both ischemia and reactive hyperemia during tissue reperfusion. In this model, spectral images, based upon oxyhemoglobin and deoxyhemoblobin signals in the 525−645-nm region, are deconvoluted using a multivariate least-squares regression analysis to visualize the spatial distribution of the percentages of oxyhemoglobin and deoxyhemoglobin in specific skin tissue areas.},
	number = {9},
	urldate = {2021-06-09},
	journal = {Analytical Chemistry},
	author = {Zuzak, Karel J. and Schaeberle, Michael D. and Lewis, E. Neil and Levin, Ira W.},
	month = may,
	year = {2002},
	pages = {2021--2028},
	file = {ACS Full Text Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UAEC5KEB\\ac011275f.html:text/html;Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6Z3G2PCL\\Zuzak et al. - 2002 - Visible Reflectance Hyperspectral Imaging  Charac.pdf:application/pdf},
}

@article{chin_hyperspectral_2012-1,
	title = {Hyperspectral imaging for early detection of oxygenation and perfusion changes in irradiated skin},
	volume = {17},
	issn = {1083-3668, 1560-2281},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-biomedical-optics/volume-17/issue-2/026010/Hyperspectral-imaging-for-early-detection-of-oxygenation-and-perfusion-changes/10.1117/1.JBO.17.2.026010.short},
	doi = {10.1117/1.JBO.17.2.026010},
	abstract = {Studies examining acute oxygenation and perfusion changes in irradiated skin are limited. Hyperspectral imaging (HSI), a method of wide-field, diffuse reflectance spectroscopy, provides noninvasive, quantified measurements of cutaneous oxygenation and perfusion. This study examines whether HSI can assess acute changes in oxygenation and perfusion following irradiation. Skin on both flanks of nude mice (n = 20) was exposed to 50 Gy of beta radiation from a strontium-90 source. Hyperspectral images were obtained before irradiation and on selected days for three weeks. Skin reaction assessment was performed concurrently with HSI. Desquamative injury formed in all irradiated areas. Skin reactions were first seen on day 7, with peak formation on day 14, and resolution beginning by day 21. HSI demonstrated increased tissue oxygenation on day 1 before cutaneous changes were observed (p{\textless}0.001). Further increases over baseline were seen on day 14, but returned to baseline levels by day 21. For perfusion, similar increases were seen on days 1 and 14. Unlike tissue oxygenation, perfusion was decreased below baseline on day 21 (p{\textless}0.002). HSI allows for complete visualization and quantification of tissue oxygenation and perfusion changes in irradiated skin, and may also allow prediction of acute skin reactions based on early changes seen after irradiation.},
	number = {2},
	urldate = {2021-06-09},
	journal = {Journal of Biomedical Optics},
	author = {Chin, Michael S. and Freniere, Brian B. and Lo, Yuan-Chyuan and Saleeby, Jonathon H. and Baker, Stephen P. and Strom, Heather M. and Ignotz, Ronald A. and Lalikos, Janice F. and Fitzgerald, Thomas J.},
	month = mar,
	year = {2012},
	note = {Publisher: International Society for Optics and Photonics},
	pages = {026010},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Q84B5FDX\\Chin et al. - 2012 - Hyperspectral imaging for early detection of oxyge.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\J9RSVLQ8\\1.JBO.17.2.026010.html:text/html},
}

@inproceedings{holmer_ability_2017,
	title = {The ability of hyperspectral imaging to detect perfusion disorders},
	copyright = {\&\#169; 2017 SPIE},
	url = {https://www.osapublishing.org/abstract.cfm?uri=ECBO-2017-1041213},
	doi = {10.1117/12.2286207},
	abstract = {Blood perfusion as the supply of tissue with blood and therefore oxygen is a key factor in clinical practice. Especially in the field of flap surgery, a reduced perfusion of transplanted skin or operated areas is often cause of various complications. The success of microvascular reconstructions is directly related to the flap perfusion. The intraoperative and postoperative assessment of the anastomoses is of great importance in order to recognize possible complications at an early stage and to revise them in good time. Is the affected tissue located on the face, successful treatment and rapid healing is even more important since aesthetic aspects play a not insignificant role. A poor perfusion is often concealed, since methods are missing for an objective assessment of the perfusion status. A method with increasing importance for clinical practice is given by hyperspectral imaging. We developed a new hyperspectral imaging system that can be used to observe tissue oxygenation and other tissue parameters and present the technical background and the parameter validation.},
	language = {EN},
	urldate = {2021-06-09},
	booktitle = {Diffuse {Optical} {Spectroscopy} and {Imaging} {VI}},
	publisher = {Optical Society of America},
	author = {Holmer, Amadeus and Kämmerer, Peer W. and Dau, Michael and Grambow, Eberhard and Wahl, Philip},
	month = jun,
	year = {2017},
	keywords = {Hyperspectral imaging, Multispectral imaging, Image resolution, Extinction coefficients, Tissue, Visible light},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6EWQ6WRE\\abstract.html:text/html},
}

@article{vaughan-shaw_oedema_2013,
	title = {Oedema is associated with clinical outcome following emergency abdominal surgery},
	volume = {95},
	issn = {0035-8843},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4188284/},
	doi = {10.1308/003588413X13629960046552},
	abstract = {Introduction
Oedema is observed frequently following surgery and may be associated with worse outcomes. To date, no study has investigated the role of oedema in the emergency surgical patient. This study assesses the incidence of oedema following emergency abdominal surgery and the value of early postoperative oedema measurement in predicting clinical outcome.

Methods
A prospective cohort study of patients undergoing emergency abdominal surgery at a university unit over a two-month period was undertaken. Nutritional and clinical outcome data were collected and oedema was measured in the early postoperative period. Predictors of oedema and outcomes associated with postoperative oedema were identified through univariate and multivariate analysis.

Results
Overall, 55 patients (median age: 66 years) were included in the study. Postoperative morbidity included ileus (n=22) and sepsis (n=6) with 12 deaths at follow-up. Postoperative oedema was present in 19 patients and was associated with prolonged perioperative fasting (107 vs 30 hours, p=0.009) but not with body mass index (24kg/m2 vs 27kg/m2, p=0.169) or preadmission weight loss (5\% vs 3\%, p=0.923). On multivariate analysis, oedema was independently associated with gastrointestinal recovery (B=6.91, p=0.038), artificial nutritional support requirement (odds ratio: 6.91, p=0.037) and overall survival (χ2=13.1, df=1, p=0.001).

Conclusions
Generalised oedema is common after emergency abdominal surgery and appears to independently predict gastrointestinal recovery, the need for artificial nutritional support and survival. Oedema is not associated with commonly applied markers of nutritional status such as body mass index or recent weight loss. Measurement of oedema offers utility in identifying those at risk of poor clinical outcome or those requiring artificial nutritional support following emergency abdominal surgery.},
	number = {6},
	urldate = {2021-06-09},
	journal = {Annals of The Royal College of Surgeons of England},
	author = {Vaughan-Shaw, PG and Saunders, J and Smith, T and King, AT and Stroud, MA},
	month = sep,
	year = {2013},
	pmid = {24025285},
	pmcid = {PMC4188284},
	pages = {390--396},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\HL542DQ3\\Vaughan-Shaw et al. - 2013 - Oedema is associated with clinical outcome followi.pdf:application/pdf},
}

@article{chincarini_major_2018,
	title = {Major pancreatic resections: normal postoperative findings and complications},
	volume = {9},
	issn = {1869-4101},
	shorttitle = {Major pancreatic resections},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5893491/},
	doi = {10.1007/s13244-018-0595-4},
	abstract = {Objectives
(1) To illustrate and describe the main types of pancreatic surgery; (2) to discuss the normal findings after pancreatic surgery; (3) to review the main complications and their radiological findings.

Background
Despite the decreased postoperative mortality, morbidity still remains high resulting in longer hospitalisations and greater costs. Imaging findings following major pancreatic resections can be broadly divided into “normal postoperative alterations” and real complications. The former should regress within a few months whereas complications may be life-threatening and should be promptly identified and treated.

Imaging findings
CT is the most effective postoperative imaging technique. MRI and fluoroscopy are used less often and only in specific cases such as assessing the gastro-intestinal function or the biliary tree. The most common normal postoperative findings are pneumobilia, perivascular cuffing, fluid collections, lymphadenopathy, acute anastomotic oedema and stranding of the peri-pancreatic/mesenteric fat. Imaging depicts the anastomoses and the new postoperative anatomy. It can also demonstrate early and late complications: pancreatic fistula, haemorrhage, delayed gastric emptying, hepatic infarction, acute pancreatitis of the remnant, porto-mesenteric thrombosis, abscess, biliary anastomotic leaks, anastomotic stenosis and local recurrence.

Conclusions
Radiologists should be aware of surgical procedures, postoperative anatomy and normal postoperative imaging findings to better detect complications and recurrent disease.

Teaching Points
• Morbidity after pancreatic resections is high., • CT is the most effective postoperative imaging technique., • Imaging depicts the anastomoses and the new postoperative anatomy., • Pancreatic fistula is the most common complication after partial pancreatic resection.},
	number = {2},
	urldate = {2021-06-09},
	journal = {Insights into Imaging},
	author = {Chincarini, Marco and Zamboni, Giulia A. and Pozzi Mucelli, Roberto},
	month = feb,
	year = {2018},
	pmid = {29450852},
	pmcid = {PMC5893491},
	pages = {173--187},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KB6H8QWK\\Chincarini et al. - 2018 - Major pancreatic resections normal postoperative .pdf:application/pdf},
}

@article{cao_multispectral_2013,
	title = {Multispectral imaging in the extended near-infrared window based on endogenous chromophores},
	volume = {18},
	issn = {1083-3668},
	url = {http://biomedicaloptics.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JBO.18.10.101318},
	doi = {10.1117/1.JBO.18.10.101318},
	language = {en},
	number = {10},
	urldate = {2021-06-09},
	journal = {Journal of Biomedical Optics},
	author = {Cao, Qian and Zhegalova, Natalia G. and Wang, Steven T. and Akers, Walter J. and Berezin, Mikhail Y.},
	month = aug,
	year = {2013},
	file = {Cao et al. - 2013 - Multispectral imaging in the extended near-infrare.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9QYJ5B28\\Cao et al. - 2013 - Multispectral imaging in the extended near-infrare.pdf:application/pdf},
}

@article{manea_hyperspectral_2015,
	title = {Hyperspectral imaging in different light conditions},
	volume = {63},
	doi = {10.1179/1743131X15Y.0000000001},
	abstract = {The aim of this paper is to investigate the influence of the illumination conditions on the hyperspectral image quality, especially saturation, shadow and dark current noise effects. The study was performed on a salt sample using one and two light sources. The results emphasised that at high light intensity an increased image brightness was noticed leading to a loss of information. On the other hand at low light intensity the dark current noise introduces peaks on the entire spectral range that disturb the data. The shadow effect seen in each image can be avoided only by a uniform illumination. In conclusion, the hyperspectral image quality depends strongly on the lighting conditions, a good image quality being achieved only if the light intensity is perfectly balanced.},
	journal = {The Imaging Science Journal},
	author = {Manea, Dragos and Calin, Mihaela},
	month = feb,
	year = {2015},
	pages = {1743131X15Y.000},
}

@article{yoon_background_2020,
	title = {A background correction method to compensate illumination variation in hyperspectral imaging},
	volume = {15},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7069652/},
	doi = {10.1371/journal.pone.0229502},
	abstract = {Hyperspectral imaging (HSI) can measure both spatial (morphological) and spectral (biochemical) information from biological tissues. While HSI appears promising for biomedical applications, interpretation of hyperspectral images can be challenging when data is acquired in complex biological environments. Variations in surface topology or optical power distribution at the sample, encountered for example during endoscopy, can lead to errors in post-processing of the HSI data, compromising disease diagnostic capabilities. Here, we propose a background correction method to compensate for such variations, which estimates the optical properties of illumination at the target based on the normalised spectral profile of the light source and the measured HSI intensity values at a fixed wavelength where the absorption characteristics of the sample are relatively low (in this case, 800 nm). We demonstrate the feasibility of the proposed method by imaging blood samples, tissue-mimicking phantoms, and ex vivo chicken tissue. Moreover, using synthetic HSI data composed from experimentally measured spectra, we show the proposed method would improve statistical analysis of HSI data. The proposed method could help the implementation of HSI techniques in practical clinical applications, where controlling the illumination pattern and power is difficult.},
	number = {3},
	urldate = {2021-06-09},
	journal = {PLoS ONE},
	author = {Yoon, Jonghee and Grigoroiu, Alexandru and Bohndiek, Sarah E.},
	month = mar,
	year = {2020},
	pmid = {32168335},
	pmcid = {PMC7069652},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MBF33Z8F\\Yoon et al. - 2020 - A background correction method to compensate illum.pdf:application/pdf},
}

@article{ashton_novel_2008,
	title = {A novel method for illumination suppression in hyperspectral images - art. no. {69660C}},
	doi = {10.1117/12.777153},
	abstract = {We have proposed a new method for illumination suppression in hyperspectral image data. This involves transforming the data into a hyperspherical coordinate system, segmenting the data cloud into a large number of classes according to the radius dimension, and then demeaning each class, thereby eliminating the distortion introduced by differential absorption in shaded regions. This method was evaluated against two other illumination-suppression methods using two metrics: visual assessment and spectral similarity of similar materials in shaded and fully illuminated regions. The proposed method shows markedly superior performance by each of these metrics.},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	author = {Ashton, Edward and Wemett, Brian and Leathers, Robert and Downes, Trijntje},
	month = apr,
	year = {2008},
}

@inproceedings{kudavelly_simple_2011,
	title = {A simple and accurate method for estimating bilirubin from blood},
	doi = {10.1109/IMTC.2011.5944035},
	abstract = {The presence of large number of modern methods (devices and papers published discussing issues and challenges with existing methodology) for the determination of bilirubin in the blood (plasma) indicates the difficulties of estimation and suggests the need for simpler yet reliable method. Also hospital turn-around time between obtaining a blood sample and receiving results from a central clinical laboratory often slows the pediatrician's management of healthy and hyperbilirubinemic neonates and results in delaying the discharge of mother and child from the hospital thereby increasing the healthcare cost. Furthermore, the most frequent cause of hospital readmissions for the neonates is due to rebound jaundice caused due to inefficient assessment of the hyperbilirubinemia. In view of the importance of bilirubin levels in the diagnosis and treatment of neonatal jaundice, a portable bilirubinometer with accuracy similar to clinical method of evaluation is needed. This paper provides the outcome of the research groundwork (literature \& product survey) and technical output (system design, experimental setup, results) in realizing a point-of-care bilirubinometer based on the serum analysis.},
	booktitle = {2011 {IEEE} {International} {Instrumentation} and {Measurement} {Technology} {Conference}},
	author = {Kudavelly, Srinivas and Keswarpu, Payal and Balakrishnan, S},
	month = may,
	year = {2011},
	note = {ISSN: 1091-5281},
	keywords = {Accuracy, absorbance, bilirubin, Blood, Data acquisition, Laboratories, Optical filters, Pediatrics, Prototypes, spectrophotometry},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\I5CP4GBP\\5944035.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\K5DGKL7T\\Kudavelly et al. - 2011 - A simple and accurate method for estimating biliru.pdf:application/pdf},
}

@article{ansari_hemorrhage_2017,
	title = {Hemorrhage after {Major} {Pancreatic} {Resection}: {Incidence}, {Risk} {Factors}, {Management}, and {Outcome}},
	volume = {106},
	doi = {https://doi.​org/10.1177/1457496916631854},
	abstract = {Background and Aims: Hemorrhage is a rare but dreaded complication after pancreatic surgery. The aim of this study was to examine the incidence, risk factors, management, and outcome of postpancreatectomy hemorrhage in a tertiary care center.},
	language = {en},
	number = {1},
	journal = {Scandinavian Journal of Surgery},
	author = {Ansari, D and Tingstedt, B and Lindell, G and Keussen, I and Ansari, D and Andersson, R},
	year = {2017},
	pages = {47--53},
	file = {Ansari et al. - Hemorrhage after Major Pancreatic Resection Incid.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KZ5L5QS8\\Ansari et al. - Hemorrhage after Major Pancreatic Resection Incid.pdf:application/pdf},
}

@article{jung_relationship_2019,
	title = {Relationship between low hemoglobin levels and mortality in patients with septic shock},
	volume = {34},
	issn = {2586-6052},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6786672/},
	doi = {10.4266/acc.2019.00465},
	abstract = {Background
Hemoglobin levels are a critical parameter for oxygen delivery in patients with shock. On comparing target hemoglobin levels upon transfusion initiation, the correlation between the severity of decrease in hemoglobin levels and patient outcomes remains unclear. We evaluated the association between initial hemoglobin levels and mortality in patients with septic shock treated with protocol-driven resuscitation bundle therapy at an emergency department.

Methods
Data of adult patients diagnosed with septic shock between June 2012 and December 2016 were extracted from a prospectively compiled septic shock registry at a single academic medical center. Patients were classified into four groups according to initial hemoglobin levels: ≥9.0 g/dl, 8.0−8.9 g/dl, 7.0−7.9 g/dl, and {\textless}7.0 g/dl. The primary endpoint was 90-day mortality.

Results
In total, 2,265 patients (male, 58.3\%; median age, 70.0 years [interquartile range, 60 to 78 years]) with septic shock were included. For the four groups, 90-day mortality rates were as follows: 29.1\%, 43.0\%, 46.5\%, and 46.9\% for ≥9.0 g/dl (n=1,808), 8.0−8.9 g/dl (n=217), 7.0−7.9 g/dl (n=135), and {\textless}7.0 g/dl (n=105), respectively (P{\textless}0.001). Multivariate logistic regression showed that initial hemoglobin levels were an independent factor associated with 90-day mortality and mortality proportionally increased with decreasing hemoglobin levels (odds ratio [OR], 1.88; 95\% confidence interval [CI], 1.36 to 2.61 for 8.0−8.9 g/dl; OR, 1.97; 95\% CI, 1.31 to 2.95 for 7.0–7.9 g/dl; and OR, 2.35; 95\% CI, 1.52 to 3.63 for {\textless}7.0 g/dl).

Conclusions
Low hemoglobin levels ({\textless}9.0 g/dl) were observed in approximately 20\% of patients with septic shock, and the severity of decrease in these levels correlated with mortality.},
	number = {2},
	urldate = {2021-06-11},
	journal = {Acute and Critical Care},
	author = {Jung, Sung Min and Kim, Youn-Jung and Ryoo, Seung Mok and Kim, Won Young},
	month = may,
	year = {2019},
	pmid = {31723919},
	pmcid = {PMC6786672},
	pages = {141--147},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\T5KX6LUX\\Jung et al. - 2019 - Relationship between low hemoglobin levels and mor.pdf:application/pdf},
}

@article{malbrain_principles_2018,
	title = {Principles of fluid management and stewardship in septic shock: it is time to consider the four {D}’s and the four phases of fluid therapy},
	volume = {8},
	issn = {2110-5820},
	shorttitle = {Principles of fluid management and stewardship in septic shock},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5964054/},
	doi = {10.1186/s13613-018-0402-x},
	abstract = {In patients with septic shock, the administration of fluids during initial hemodynamic resuscitation remains a major therapeutic challenge. We are faced with many open questions regarding the type, dose and timing of intravenous fluid administration. There are only four major indications for intravenous fluid administration: aside from resuscitation, intravenous fluids have many other uses including maintenance and replacement of total body water and electrolytes, as carriers for medications and for parenteral nutrition. In this paradigm-shifting review, we discuss different fluid management strategies including early adequate goal-directed fluid management, late conservative fluid management and late goal-directed fluid removal. In addition, we expand on the concept of the “four D’s” of fluid therapy, namely drug, dosing, duration and de-escalation. During the treatment of patients with septic shock, four phases of fluid therapy should be considered in order to provide answers to four basic questions. These four phases are the resuscitation phase, the optimization phase, the stabilization phase and the evacuation phase. The four questions are “When to start intravenous fluids?”, “When to stop intravenous fluids?”, “When to start de-resuscitation or active fluid removal?” and finally “When to stop de-resuscitation?” In analogy to the way we handle antibiotics in critically ill patients, it is time for fluid stewardship.},
	urldate = {2021-06-11},
	journal = {Annals of Intensive Care},
	author = {Malbrain, Manu L. N. G. and Van Regenmortel, Niels and Saugel, Bernd and De Tavernier, Brecht and Van Gaal, Pieter-Jan and Joannes-Boyau, Olivier and Teboul, Jean-Louis and Rice, Todd W. and Mythen, Monty and Monnet, Xavier},
	month = may,
	year = {2018},
	pmid = {29789983},
	pmcid = {PMC5964054},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MSSLE8F7\\Malbrain et al. - 2018 - Principles of fluid management and stewardship in .pdf:application/pdf},
}

@article{hexsel_variation_2013,
	title = {Variation of melanin levels in the skin in areas exposed and not exposed to the sun following winter and summer},
	volume = {5},
	abstract = {Introduction The pigment mainly responsible for the color of the skin, melanin is directly influenced by exposure to sunlight.
Objective: The present study assessed the effects of solar radiation on the levels of melanin in areas exposed and not exposed to the sun, taking into consideration the seasonality of exposure.
Methods: Melanin levels were evaluated on the forehead, sacral region, and forearm, in the post-summer and post-winter periods, using spectrophotometry.
Results: The levels of melanin after winter were lower than those after summer in the forehead (168.1 vs. 177.0), sacral region (132.0 vs. 140.4), and forearm (218.7 vs. 260. 4), with a statistically significant reduction only in the forearm (p{\textless}0.0001).Additionally, erythema was significantly less intense in the forearm and forehead (p{\textless}0.0001 and p=0.002) after winter than after summer.
Conclusion: The significant reduction of melanin levels in the forearm after winter reinforces the influence of seasonality on skin pigmentation changes to body areas exposed to the sun without protection.The small variation in the levels of melanin found in the unexposed area (sacrum) confirms that the effect of exposure to the sun on the levels of melanin is predominantly local. Increased production of melanin is directly related to local exposure to UV rays.},
	language = {en},
	journal = {Surgical and Cosmetic Dermatology},
	author = {Hexsel, Doris and Caspary, Patrícia and Dini, Taciana Dal Forno and Schilling-Souza, Juliana and Siega, Carolina},
	month = jan,
	year = {2013},
	pages = {298--301},
	file = {Hexsel et al. - Variação dos níveis de melanina da pele em áreas e.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\GHB9XA74\\Hexsel et al. - Variação dos níveis de melanina da pele em áreas e.pdf:application/pdf},
}

@article{manea_hyperspectral_2015-1,
	title = {Hyperspectral imaging in different light conditions},
	volume = {63},
	doi = {10.1179/1743131X15Y.0000000001},
	abstract = {The aim of this paper is to investigate the influence of the illumination conditions on the hyperspectral image quality, especially saturation, shadow and dark current noise effects. The study was performed on a salt sample using one and two light sources. The results emphasised that at high light intensity an increased image brightness was noticed leading to a loss of information. On the other hand at low light intensity the dark current noise introduces peaks on the entire spectral range that disturb the data. The shadow effect seen in each image can be avoided only by a uniform illumination. In conclusion, the hyperspectral image quality depends strongly on the lighting conditions, a good image quality being achieved only if the light intensity is perfectly balanced.},
	journal = {The Imaging Science Journal},
	author = {Manea, Dragos and Calin, Mihaela},
	month = feb,
	year = {2015},
	pages = {1743131X15Y.000},
}

@article{dietrich_hyperspectral_2020,
	title = {Hyperspectral {Imaging} for {Perioperative} {Monitoring} of {Microcirculatory} {Tissue} {Oxygenation} and {Tissue} {Water} {Content} in {Pancreatic} {Surgery} – {An} {Observational} {Clinical} {Pilot} {Study}},
	language = {en},
	journal = {mansucript under review in Perioperative Medicine (PERI-D-20-00132)},
	author = {Dietrich, Maximilian and Marx, Sebastian and von der Forst, Maik and Bruckner, Thomas and Schmitt, Felix C F and Fiedler, Mascha O. and Nickel, Felix and Studier-Fischer, Alexander and Mueller-Stich, Beat P. and Hackert, Tilo and Brenner, Thorsten and Weigand, Markus A. and Uhle, Florian and Schmidt, Karsten},
	month = dec,
	year = {2020},
	file = {Dietrich et al. - 1 Hyperspectral Imaging for Perioperative Monitori.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\V9C23HMT\\Dietrich et al. - 1 Hyperspectral Imaging for Perioperative Monitori.pdf:application/pdf},
}

@article{grohl_learned_2021,
	title = {Learned spectral decoloring enables photoacoustic oximetry},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-83405-8},
	doi = {10.1038/s41598-021-83405-8},
	abstract = {The ability of photoacoustic imaging to measure functional tissue properties, such as blood oxygenation sO\$\$\_2\$\$, enables a wide variety of possible applications. sO\$\$\_2\$\$can be computed from the ratio of oxyhemoglobin HbO\$\$\_2\$\$and deoxyhemoglobin Hb, which can be distuinguished by multispectral photoacoustic imaging due to their distinct wavelength-dependent absorption. However, current methods for estimating sO\$\$\_2\$\$yield inaccurate results in realistic settings, due to the unknown and wavelength-dependent influence of the light fluence on the signal. In this work, we propose learned spectral decoloring to enable blood oxygenation measurements to be inferred from multispectral photoacoustic imaging. The method computes sO\$\$\_2\$\$pixel-wise, directly from initial pressure spectra \$\$S\_\{{\textbackslash}text \{p\}\_0\}({\textbackslash}lambda , {\textbackslash}mathbf \{x\})\$\$, which represent initial pressure values at a fixed spatial location \$\${\textbackslash}mathbf \{x\}\$\$over all recorded wavelengths \$\${\textbackslash}lambda\$\$. The method is compared to linear unmixing approaches, as well as pO\$\$\_2\$\$and blood gas analysis reference measurements. Experimental results suggest that the proposed method is able to obtain sO\$\$\_2\$\$estimates from multispectral photoacoustic measurements in silico, in vitro, and in vivo.},
	language = {en},
	number = {1},
	urldate = {2021-06-15},
	journal = {Scientific Reports},
	author = {Gröhl, Janek and Kirchner, Thomas and Adler, Tim J. and Hacker, Lina and Holzwarth, Niklas and Hernández-Aguilera, Adrián and Herrera, Mildred A. and Santos, Edgar and Bohndiek, Sarah E. and Maier-Hein, Lena},
	month = mar,
	year = {2021},
	pages = {1--12},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\G6UZLAVB\\Gröhl et al. - 2021 - Learned spectral decoloring enables photoacoustic .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Y5M2LKBA\\s41598-021-83405-8.html:text/html},
}

@article{grohl_semantic_2021,
	title = {Semantic segmentation of multispectral photoacoustic images using deep learning},
	url = {http://arxiv.org/abs/2105.09624},
	abstract = {Photoacoustic imaging has the potential to revolutionise healthcare due to the valuable information on tissue physiology that is contained in multispectral photoacoustic measurements. Clinical translation of the technology requires conversion of the high-dimensional acquired data into clinically relevant and interpretable information. In this work, we present a deep learning-based approach to semantic segmentation of multispectral photoacoustic images to facilitate the interpretability of recorded images. Manually annotated multispectral photoacoustic imaging data are used as gold standard reference annotations and enable the training of a deep learning-based segmentation algorithm in a supervised manner. Based on a validation study with experimentally acquired data of healthy human volunteers, we show that automatic tissue segmentation can be used to create powerful analyses and visualisations of multispectral photoacoustic images. Due to the intuitive representation of high-dimensional information, such a processing algorithm could be a valuable means to facilitate the clinical translation of photoacoustic imaging.},
	language = {en},
	urldate = {2021-06-15},
	journal = {arXiv preprint arXiv:2105.09624},
	author = {Gröhl, Janek and Schellenberg, Melanie and Dreher, Kris and Holzwarth, Niklas and Tizabi, Minu D. and Seitel, Alexander and Maier-Hein, Lena},
	month = may,
	year = {2021},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Physics - Medical Physics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Gröhl et al. - 2021 - Semantic segmentation of multispectral photoacoust.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\79MXKZUG\\Gröhl et al. - 2021 - Semantic segmentation of multispectral photoacoust.pdf:application/pdf},
}

@inproceedings{inga_saknite_novel_2017,
	title = {Novel hybrid technology for early diagnostics of sepsis},
	volume = {10057},
	url = {https://doi.org/10.1117/12.2253597},
	author = {{Inga Saknite} and {Andris Grabovskis} and {Sigita Kazune} and {Uldis Rubins} and {Zbignevs Marcinkevics} and {Karina Volceka} and {Edgars Kviesis-Kipge} and {Janis Spigulis}},
	month = feb,
	year = {2017},
}

@inproceedings{saknite_novel_2017-1,
	title = {Novel hybrid technology for early diagnostics of sepsis},
	volume = {10057},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10057/100570F/Novel-hybrid-technology-for-early-diagnostics-of-sepsis/10.1117/12.2253597.short},
	doi = {10.1117/12.2253597},
	abstract = {Sepsis is a potentially fatal disease with mortality rate as high as 50\% in patients with septic shock; mortality rate can increase by 7.6\% per hour if appropriate treatment is not started. Internationally accepted guidelines for diagnosis of sepsis rely on vital sign monitoring and laboratory tests in order to recognize organ failure. This pilot study aims to explore the potential of hyperspectral and thermal imaging techniques to identify and quantify early alterations in skin oxygenation and perfusion induced by sepsis. The study comprises both physiological model experiments on healthy volunteers in a laboratory environment, as well as screening case series of patients with septic shock in the intensive care department. Hyperspectral imaging is used to determine one of the main characteristic visual signs of skin oxygenation abnormalities - skin mottling, whereas changes in peripheral perfusion have been visualized by thermal imaging as heterogeneous skin temperature areas. In order to mimic septic skin mottling in a reproducible way in laboratory environment, arterial occlusion provocation test was utilized on healthy volunteers. Visualization of oxygen saturation by hyperspectral imaging allows diagnosing microcirculatory alterations induced by sepsis earlier than visual assessment of mottling. Thermal images of sepsis patients in the clinic clearly reveal hotspots produced by perforating arteries, as well as cold regions of low blood supply. The results of this pilot study show that thermal imaging in combination with hyperspectral imaging allows the determination of oxygen supply and utilization in critically ill septic patients.},
	urldate = {2021-06-15},
	booktitle = {Multimodal {Biomedical} {Imaging} {XII}},
	publisher = {International Society for Optics and Photonics},
	author = {Saknite, Inga and Grabovskis, Andris and Kazune, Sigita and Rubins, Uldis and Marcinkevics, Zbignevs and Volceka, Karina and Kviesis-Kipge, Edgars and Spigulis, Janis},
	month = feb,
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KAD6FID4\\Saknite et al. - 2017 - Novel hybrid technology for early diagnostics of s.pdf:application/pdf;Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VRYXB53X\\Saknite et al. - 2017 - Novel hybrid technology for early diagnostics of s.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MNIGJ2IE\\12.2253597.html:text/html},
}

@article{annane_global_2018,
	title = {A global perspective on vasoactive agents in shock},
	volume = {44},
	issn = {0342-4642, 1432-1238},
	url = {http://link.springer.com/10.1007/s00134-018-5242-5},
	doi = {10.1007/s00134-018-5242-5},
	abstract = {Purpose:  We set out to summarize the current knowledge on vasoactive drugs and their use in the management of shock to inform physicians’ practices. Methods:  This is a narrative review by a multidisciplinary, multinational—from six continents—panel of experts including physicians, a pharmacist, trialists, and scientists. Results and conclusions:  Vasoactive drugs are an essential part of shock management. Catecholamines are the most commonly used vasoactive agents in the intensive care unit, and among them norepinephrine is the first-line therapy in most clinical conditions. Inotropes are indicated when myocardial function is depressed and dobutamine remains the first-line therapy. Vasoactive drugs have a narrow therapeutic spectrum and expose the patients to potentially lethal complications. Thus, these agents require precise therapeutic targets, close monitoring with titration to the minimal efficacious dose and should be weaned as promptly as possible. Moreover, the use of vasoactive drugs in shock requires an individualized approach. Vasopressin and possibly angiotensin II may be useful owing to their norepinephrine-sparing effects.},
	language = {en},
	number = {6},
	urldate = {2021-06-15},
	journal = {Intensive Care Medicine},
	author = {Annane, Djillali and Ouanes-Besbes, Lamia and de Backer, Daniel and Du, Bin and Gordon, Anthony C. and Hernández, Glenn and Olsen, Keith M. and Osborn, Tiffany M. and Peake, Sandra and Russell, James A. and Cavazzoni, Sergio Zanotti},
	month = jun,
	year = {2018},
	pages = {833--846},
	file = {Annane et al. - 2018 - A global perspective on vasoactive agents in shock.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XN5FCYBM\\Annane et al. - 2018 - A global perspective on vasoactive agents in shock.pdf:application/pdf},
}

@article{dietrich_machine_2021,
	title = {Machine learning-based analysis of hyperspectral images for automated sepsis diagnosis},
	url = {http://arxiv.org/abs/2106.08445},
	abstract = {Sepsis is a leading cause of mortality and critical illness worldwide. While robust biomarkers for early diagnosis are still missing, recent work indicates that hyperspectral imaging (HSI) has the potential to overcome this bottleneck by monitoring microcirculatory alterations. Automated machine learning-based diagnosis of sepsis based on HSI data, however, has not been explored to date. Given this gap in the literature, we leveraged an existing data set to ( ) investigate whether HSI-based automated diagnosis of sepsis is possible and ( ) put forth a list of possible confounders relevant for HSI-based tissue classi cation. While we were able to classify sepsis with an accuracy of over 98 \% using the existing data, our research also revealed several subject-, therapy- and imaging-related confounders that may lead to an overestimation of algorithm performance when not balanced across the patient groups. We conclude that further prospective studies, carefully designed with respect to these confounders, are necessary to con rm the preliminary results obtained in this study.},
	language = {en},
	urldate = {2021-06-17},
	journal = {arXiv:2106.08445 [cs]},
	author = {Dietrich, Maximilian and Seidlitz, Silvia and Schreck, Nicholas and Wiesenfarth, Manuel and Godau, Patrick and Tizabi, Minu and Sellner, Jan and Marx, Sebastian and Knödler, Samuel and Allers, Michael M. and Ayala, Leonardo and Schmidt, Karsten and Brenner, Thorsten and Studier-Fischer, Alexander and Nickel, Felix and Müller-Stich, Beat P. and Kopp-Schneider, Annette and Weigand, Markus A. and Maier-Hein, Lena},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.08445},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Computer Vision and Pattern Recognition, I.2.10, I.4, I.5, J.3},
	file = {2106.08445.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5TULT2XG\\2106.08445.pdf:application/pdf},
}

@techreport{cruz_public_2021,
	type = {preprint},
	title = {Public {Covid}-19 {X}-ray datasets and their impact on model bias - a systematic review of a significant problem},
	url = {http://medrxiv.org/lookup/doi/10.1101/2021.02.15.21251775},
	abstract = {ABSTRACT
          Computer-aided-diagnosis for COVID-19 based on chest X-ray suffers from weak bias assessment and limited quality-control. Undetected bias induced by inappropriate use of datasets, and improper consideration of confounders prevents the translation of prediction models into clinical practice. This study provides a systematic evaluation of publicly available COVID-19 chest X-ray datasets, determining their potential use and evaluating potential sources of bias.
          Only 5 out of 256 identified datasets met at least the criteria for proper assessment of risk of bias and could be analysed in detail. Remarkably almost all of the datasets utilised in 78 papers published in peer-reviewed journals, are not among these 5 datasets, thus leading to models with high risk of bias. This raises concerns about the suitability of such models for clinical use.
          This systematic review highlights the limited description of datasets employed for modelling and aids researchers to select the most suitable datasets for their task.},
	language = {en},
	urldate = {2021-07-19},
	institution = {Radiology and Imaging},
	author = {Cruz, Beatriz Garcia Santa and Bossa, Matías Nicolás and Sölter, Jan and Husch, Andreas Dominik},
	month = feb,
	year = {2021},
	doi = {10.1101/2021.02.15.21251775},
	file = {Cruz et al. - 2021 - Public Covid-19 X-ray datasets and their impact on.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\W9NUEXTA\\Cruz et al. - 2021 - Public Covid-19 X-ray datasets and their impact on.pdf:application/pdf},
}

@misc{ros_comparative_nodate,
	title = {Comparative validation of multi-instance instrument segmentation in endoscopy: {Results} of the {ROBUST}-{MIS} 2019 challenge},
	shorttitle = {Comparative validation of multi-instance instrument segmentation in endoscopy},
	url = {https://reader.elsevier.com/reader/sd/pii/S136184152030284X?token=284EA8CD877CB4160C6B905B9C69C8961BF68A54E42B68A7E21B2E7BC541AEC2000BD1C235C73947CE9128A9E4C70A97&originRegion=eu-west-1&originCreation=20210823151938},
	language = {en},
	urldate = {2021-08-23},
	author = {Roß, Tobias and Reinke, Annika and Full, Peter M. and Wagner, Martin and Kenngott, Hannes and Apitz, Martin and Hempe, Hellena and Mindroc-Filimon, Diana and Scholz, Patrick and Tran, Thuy Nuong and Bruno, Pierangela and Arbeláez, Pablo and Bian, Gui-Bin and Bodenstedt, Sebastian and Bolmgren, Jon Lindström and Bravo-Sánchez, Laura},
	doi = {10.1016/j.media.2020.101920},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\R376WMPW\\S136184152030284X.html:text/html;Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WXRH4U4I\\Comparative validation of multi-instance instrumen.pdf:application/pdf},
}

@article{ros_comparative_2021,
	title = {Comparative validation of multi-instance instrument segmentation in endoscopy: {Results} of the {ROBUST}-{MIS} 2019 challenge},
	volume = {70},
	issn = {1361-8415},
	shorttitle = {Comparative validation of multi-instance instrument segmentation in endoscopy},
	url = {https://www.sciencedirect.com/science/article/pii/S136184152030284X},
	doi = {10.1016/j.media.2020.101920},
	abstract = {Intraoperative tracking of laparoscopic instruments is often a prerequisite for computer and robotic-assisted interventions. While numerous methods for detecting, segmenting and tracking of medical instruments based on endoscopic video images have been proposed in the literature, key limitations remain to be addressed: Firstly, robustness, that is, the reliable performance of state-of-the-art methods when run on challenging images (e.g. in the presence of blood, smoke or motion artifacts). Secondly, generalization; algorithms trained for a specific intervention in a specific hospital should generalize to other interventions or institutions. In an effort to promote solutions for these limitations, we organized the Robust Medical Instrument Segmentation (ROBUST-MIS) challenge as an international benchmarking competition with a specific focus on the robustness and generalization capabilities of algorithms. For the first time in the field of endoscopic image processing, our challenge included a task on binary segmentation and also addressed multi-instance detection and segmentation. The challenge was based on a surgical data set comprising 10,040 annotated images acquired from a total of 30 surgical procedures from three different types of surgery. The validation of the competing methods for the three tasks (binary segmentation, multi-instance detection and multi-instance segmentation) was performed in three different stages with an increasing domain gap between the training and the test data. The results confirm the initial hypothesis, namely that algorithm performance degrades with an increasing domain gap. While the average detection and segmentation quality of the best-performing algorithms is high, future research should concentrate on detection and segmentation of small, crossing, moving and transparent instrument(s) (parts).},
	language = {en},
	urldate = {2021-08-23},
	journal = {Medical Image Analysis},
	author = {Roß, Tobias and Reinke, Annika and Full, Peter M. and Wagner, Martin and Kenngott, Hannes and Apitz, Martin and Hempe, Hellena and Mindroc-Filimon, Diana and Scholz, Patrick and Tran, Thuy Nuong and Bruno, Pierangela and Arbeláez, Pablo and Bian, Gui-Bin and Bodenstedt, Sebastian and Bolmgren, Jon Lindström and Bravo-Sánchez, Laura and Chen, Hua-Bin and González, Cristina and Guo, Dong and Halvorsen, Pål and Heng, Pheng-Ann and Hosgor, Enes and Hou, Zeng-Guang and Isensee, Fabian and Jha, Debesh and Jiang, Tingting and Jin, Yueming and Kirtac, Kadir and Kletz, Sabrina and Leger, Stefan and Li, Zhixuan and Maier-Hein, Klaus H. and Ni, Zhen-Liang and Riegler, Michael A. and Schoeffmann, Klaus and Shi, Ruohua and Speidel, Stefanie and Stenzel, Michael and Twick, Isabell and Wang, Gutai and Wang, Jiacheng and Wang, Liansheng and Wang, Lu and Zhang, Yujie and Zhou, Yan-Jie and Zhu, Lei and Wiesenfarth, Manuel and Kopp-Schneider, Annette and Müller-Stich, Beat P. and Maier-Hein, Lena},
	month = may,
	year = {2021},
	keywords = {Minimally invasive surgery, Surgical data science, Multi-instance instrument, Robustness and generalization},
	pages = {101920},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5GUE9746\\Roß et al. - 2021 - Comparative validation of multi-instance instrumen.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\23RZFG48\\S136184152030284X.html:text/html},
}

@article{maier-hein_surgical_2017-2,
	title = {Surgical data science for next-generation interventions},
	volume = {1},
	copyright = {2017 Publisher},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-017-0132-7},
	doi = {10.1038/s41551-017-0132-7},
	abstract = {Interventional healthcare will evolve from an artisanal craft based on the individual experiences, preferences and traditions of physicians into a discipline that relies on objective decision-making on the basis of large-scale data from heterogeneous sources.},
	language = {en},
	number = {9},
	urldate = {2021-08-23},
	journal = {Nature Biomedical Engineering},
	author = {Maier-Hein, Lena and Vedula, Swaroop S. and Speidel, Stefanie and Navab, Nassir and Kikinis, Ron and Park, Adrian and Eisenmann, Matthias and Feussner, Hubertus and Forestier, Germain and Giannarou, Stamatia and Hashizume, Makoto and Katic, Darko and Kenngott, Hannes and Kranzfelder, Michael and Malpani, Anand and März, Keno and Neumuth, Thomas and Padoy, Nicolas and Pugh, Carla and Schoch, Nicolai and Stoyanov, Danail and Taylor, Russell and Wagner, Martin and Hager, Gregory D. and Jannin, Pierre},
	month = sep,
	year = {2017},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 9
Primary\_atype: Comments \& Opinion
Publisher: Nature Publishing Group
Subject\_term: Health care;Surgery
Subject\_term\_id: health-care;surgery},
	pages = {691--696},
	file = {Eingereichte Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\D9KVAPVT\\Maier-Hein et al. - 2017 - Surgical data science for next-generation interven.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PJ8K42HI\\s41551-017-0132-7.html:text/html},
}

@article{scheikl_deep_2020,
	title = {Deep learning for semantic segmentation of organs and tissues in laparoscopic surgery},
	volume = {6},
	doi = {10.1515/cdbme-2020-0016},
	abstract = {Semantic segmentation of organs and tissue types is an important sub-problem in image based scene understanding for laparoscopic surgery and is a prerequisite for context-aware assistance and cognitive robotics. Deep Learning (DL) approaches are prominently applied to segmentation and tracking of laparoscopic instruments. This work compares different combinations of neural networks, loss functions, and training strategies in their application to semantic segmentation of different organs and tissue types in human laparoscopic images in order to investigate their applicability as components in cognitive systems. TernausNet-11 trained on Soft-Jaccard loss with a pretrained, trainable encoder performs best in regard to segmentation quality (78.31\% mean Intersection over Union [IoU]) and inference time (28.07 ms) on a single GTX 1070 GPU.},
	journal = {Current Directions in Biomedical Engineering},
	author = {Scheikl, Paul and Laschewski, Stefan and Kisilenko, Anna and Davitashvili, Tornike and Müller, Benjamin and Capek, Manuela and Müller, Beat and Wagner, Martin and Ullrich, Franziska},
	month = sep,
	year = {2020},
	pages = {20200016},
	file = {Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DB7GKNNY\\Scheikl et al. - 2020 - Deep learning for semantic segmentation of organs .pdf:application/pdf},
}

@article{grammatikopoulou_cadis_2021,
	title = {{CaDIS}: {Cataract} dataset for surgical {RGB}-image segmentation},
	volume = {71},
	issn = {1361-8415},
	shorttitle = {{CaDIS}},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521000992},
	doi = {10.1016/j.media.2021.102053},
	abstract = {Video feedback provides a wealth of information about surgical procedures and is the main sensory cue for surgeons. Scene understanding is crucial to computer assisted interventions (CAI) and to post-operative analysis of the surgical procedure. A fundamental building block of such capabilities is the identification and localization of surgical instruments and anatomical structures through semantic segmentation. Deep learning has advanced semantic segmentation techniques in the recent years but is inherently reliant on the availability of labelled datasets for model training. This paper introduces a dataset for semantic segmentation of cataract surgery videos complementing the publicly available CATARACTS challenge dataset. In addition, we benchmark the performance of several state-of-the-art deep learning models for semantic segmentation on the presented dataset. The dataset is publicly available at https://cataracts-semantic-segmentation2020.grand-challenge.org/.},
	language = {en},
	urldate = {2021-08-23},
	journal = {Medical Image Analysis},
	author = {Grammatikopoulou, Maria and Flouty, Evangello and Kadkhodamohammadi, Abdolrahim and Quellec, Gwenolé and Chow, Andre and Nehme, Jean and Luengo, Imanol and Stoyanov, Danail},
	month = jul,
	year = {2021},
	keywords = {Cataract surgery, Dataset, Semantic segmentation},
	pages = {102053},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FGAGJGQ5\\Grammatikopoulou et al. - 2021 - CaDIS Cataract dataset for surgical RGB-image seg.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MHTP58ZE\\S1361841521000992.html:text/html},
}

@inproceedings{garifullin_hyperspectral_2018,
	title = {Hyperspectral {Image} {Segmentation} of {Retinal} {Vasculature}, {Optic} {Disc} and {Macula}},
	doi = {10.1109/DICTA.2018.8615761},
	abstract = {The most common approach for retinal imaging is the eye fundus photography which usually results in RGB images. Recent studies show that the additional spectral information provides useful features for automatic retinal image analysis. The current work extends recent research on the joint segmentation of retinal vasculature, optic disc and macula which often appears in different retinal image analysis tasks. Fully convolutional neural networks are utilized to solve the segmentation problem. It is shown that the network architectures can be effectively modified for the spectral data and the utilization of spectral information provides moderate improvements in retinal image segmentation.},
	booktitle = {2018 {Digital} {Image} {Computing}: {Techniques} and {Applications} ({DICTA})},
	author = {Garifullin, Azat and Kööbi, Peeter and Ylitepsa, Pasi and Ådjers, Kati and Hauta-Kasari, Markku and Uusitalo, Hannu and Lensu, Lasse},
	month = dec,
	year = {2018},
	keywords = {Hyperspectral imaging, Image segmentation, Optical imaging, Dimensionality reduction, Object segmentation, Retina, Task analysis},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\S6XNZWNK\\8615761.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EEGW4CHD\\Garifullin et al. - 2018 - Hyperspectral Image Segmentation of Retinal Vascul.pdf:application/pdf},
}

@article{clevert_fast_2016,
	title = {Fast and {Accurate} {Deep} {Network} {Learning} by {Exponential} {Linear} {Units} ({ELUs})},
	url = {http://arxiv.org/abs/1511.07289},
	abstract = {We introduce the “exponential linear unit” (ELU) which speeds up learning in deep neural networks and leads to higher classiﬁcation accuracies. Like rectiﬁed linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence.},
	language = {en},
	urldate = {2021-08-26},
	journal = {arXiv:1511.07289 [cs]},
	author = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.07289},
	keywords = {Computer Science - Machine Learning},
	file = {Clevert et al. - 2016 - Fast and Accurate Deep Network Learning by Exponen.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CMF2ZRC2\\Clevert et al. - 2016 - Fast and Accurate Deep Network Learning by Exponen.pdf:application/pdf},
}

@inproceedings{szegedy_rethinking_2016,
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	doi = {10.1109/CVPR.2016.308},
	abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2\% top-1 and 5:6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5\% top-5 error and 17:3\% top-1 error on the validation set and 3:6\% top-5 error on the official test set.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Training, Convolution, Benchmark testing, Computational efficiency, Computational modeling, Computer architecture, Computer vision},
	pages = {2818--2826},
	file = {Eingereichte Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6DZW27Z6\\Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\I8EMTJZK\\7780677.html:text/html},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2021-08-26},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {Kingma und Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UV8KGTPX\\Kingma und Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf},
}

@article{izmailov_averaging_2018,
	title = {Averaging {Weights} {Leads} to {Wider} {Optima} and {Better} {Generalization}},
	urldate = {2021-09-03},
	journal = {Proceedings of the International Conference on Uncertainty in Artificial Intelligence},
	author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VNSSYAX4\\Izmailov et al. - 2019 - Averaging Weights Leads to Wider Optima and Better.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7W6K35T6\\1803.html:text/html},
}

@article{achanta_slic_2012,
	title = {{SLIC} {Superpixels} {Compared} to {State}-of-the-{Art} {Superpixel} {Methods}},
	volume = {34},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2012.120},
	abstract = {Computer vision applications have come to rely increasingly on superpixels in recent years, but it is not always clear what constitutes a good superpixel algorithm. In an effort to understand the benefits and drawbacks of existing methods, we empirically compare five state-of-the-art superpixel algorithms for their ability to adhere to image boundaries, speed, memory efficiency, and their impact on segmentation performance. We then introduce a new superpixel algorithm, simple linear iterative clustering (SLIC), which adapts a k-means clustering approach to efficiently generate superpixels. Despite its simplicity, SLIC adheres to boundaries as well as or better than previous methods. At the same time, it is faster and more memory efficient, improves segmentation performance, and is straightforward to extend to supervoxel generation.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and Süsstrunk, Sabine},
	month = nov,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Image color analysis, Image edge detection, Image segmentation, segmentation, Complexity theory, Approximation algorithms, clustering, Clustering algorithms, k-means, Measurement uncertainty, Superpixels},
	pages = {2274--2282},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\U6P4RMG6\\6205760.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Z76JNWPV\\Achanta et al. - 2012 - SLIC Superpixels Compared to State-of-the-Art Supe.pdf:application/pdf},
}

@article{tan_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a ﬁxed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefﬁcient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.},
	language = {en},
	journal = {International Conference on Machine Learning},
	author = {Tan, Mingxing and Le, Quoc V},
	year = {2019},
	pages = {6105--6114},
	file = {Tan und Le - EfficientNet Rethinking Model Scaling for Convolu.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7AMVVN2T\\Tan und Le - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IWYUVDXC\\5206848.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Q44L8SQL\\Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:application/pdf},
}

@article{wiesenfarth_methods_2021,
	title = {Methods and open-source toolkit for analyzing and visualizing challenge results},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-82017-6},
	doi = {10.1038/s41598-021-82017-6},
	abstract = {Grand challenges have become the de facto standard for benchmarking image analysis algorithms. While the number of these international competitions is steadily increasing, surprisingly little effort has been invested in ensuring high quality design, execution and reporting for these international competitions. Specifically, results analysis and visualization in the event of uncertainties have been given almost no attention in the literature. Given these shortcomings, the contribution of this paper is two-fold: (1) we present a set of methods to comprehensively analyze and visualize the results of single-task and multi-task challenges and apply them to a number of simulated and real-life challenges to demonstrate their specific strengths and weaknesses; (2) we release the open-source framework challengeR as part of this work to enable fast and wide adoption of the methodology proposed in this paper. Our approach offers an intuitive way to gain important insights into the relative and absolute performance of algorithms, which cannot be revealed by commonly applied visualization techniques. This is demonstrated by the experiments performed in the specific context of biomedical image analysis challenges. Our framework could thus become an important tool for analyzing and visualizing challenge results in the field of biomedical image analysis and beyond.},
	language = {en},
	number = {1},
	urldate = {2021-09-13},
	journal = {Scientific Reports},
	author = {Wiesenfarth, Manuel and Reinke, Annika and Landman, Bennett A. and Eisenmann, Matthias and Saiz, Laura Aguilera and Cardoso, M. Jorge and Maier-Hein, Lena and Kopp-Schneider, Annette},
	month = jan,
	year = {2021},
	pages = {2369},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8CDXKG5R\\Wiesenfarth et al. - 2021 - Methods and open-source toolkit for analyzing and .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QGYWA7P4\\s41598-021-82017-6.html:text/html},
}

@misc{wiesenfarth_methods_2021-1,
	title = {Methods and open-source toolkit for analyzing and visualizing challenge results},
	copyright = {GPL-2.0},
	url = {https://github.com/wiesenfa/challengeR},
	urldate = {2021-09-13},
	author = {Wiesenfarth, Manuel},
	month = sep,
	year = {2021},
	note = {original-date: 2019-09-06T12:48:20Z},
}

@article{reinke_common_2021,
	title = {Common {Limitations} of {Image} {Processing} {Metrics}: {A} {Picture} {Story}},
	shorttitle = {Common {Limitations} of {Image} {Processing} {Metrics}},
	url = {http://arxiv.org/abs/2104.05642},
	abstract = {While the importance of automatic image analysis is increasing at an enormous pace, recent meta-research revealed major flaws with respect to algorithm validation. Specifically, performance metrics are key for objective, transparent and comparative performance assessment, but relatively little attention has been given to the practical pitfalls when using specific metrics for a given image analysis task. A common mission of several international initiatives is therefore to provide researchers with guidelines and tools to choose the performance metrics in a problem-aware manner. This dynamically updated document has the purpose to illustrate important limitations of performance metrics commonly applied in the field of image analysis. The current version is based on a Delphi process on metrics conducted by an international consortium of image analysis experts.},
	urldate = {2021-09-14},
	journal = {arXiv:2104.05642 [cs, eess]},
	author = {Reinke, Annika and Eisenmann, Matthias and Tizabi, Minu D. and Sudre, Carole H. and Rädsch, Tim and Antonelli, Michela and Arbel, Tal and Bakas, Spyridon and Cardoso, M. Jorge and Cheplygina, Veronika and Farahani, Keyvan and Glocker, Ben and Heckmann-Nötzel, Doreen and Isensee, Fabian and Jannin, Pierre and Kahn, Charles E. and Kleesiek, Jens and Kurc, Tahsin and Kozubek, Michal and Landman, Bennett A. and Litjens, Geert and Maier-Hein, Klaus and Menze, Bjoern and Müller, Henning and Petersen, Jens and Reyes, Mauricio and Rieke, Nicola and Stieltjes, Bram and Summers, Ronald M. and Tsaftaris, Sotirios A. and van Ginneken, Bram and Kopp-Schneider, Annette and Jäger, Paul and Maier-Hein, Lena},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.05642},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\B5UY8YMD\\Reinke et al. - 2021 - Common Limitations of Image Processing Metrics A .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6FBCSQSJ\\2104.html:text/html},
}

@article{dice_measures_1945,
	title = {Measures of the {Amount} of {Ecologic} {Association} {Between} {Species}},
	volume = {26},
	issn = {0012-9658},
	url = {https://www.jstor.org/stable/1932409},
	doi = {10.2307/1932409},
	number = {3},
	urldate = {2021-09-14},
	journal = {Ecology},
	author = {Dice, Lee R.},
	year = {1945},
	note = {Publisher: Ecological Society of America},
	pages = {297--302},
}

@article{yeghiazaryan_family_2018,
	title = {Family of boundary overlap metrics for the evaluation of medical image segmentation},
	volume = {5},
	issn = {2329-4302},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5817231/},
	doi = {10.1117/1.JMI.5.1.015006},
	abstract = {All medical image segmentation algorithms need to be validated and compared, yet no evaluation framework is widely accepted within the imaging community. None of the evaluation metrics that are popular in the literature are consistent in the way they rank segmentation results: they tend to be sensitive to one or another type of segmentation error (size, location, and shape) but no single metric covers all error types. We introduce a family of metrics, with hybrid characteristics. These metrics quantify the similarity or difference of segmented regions by considering their average overlap in fixed-size neighborhoods of points on the boundaries of those regions. Our metrics are more sensitive to combinations of segmentation error types than other metrics in the existing literature. We compare the metric performance on collections of segmentation results sourced from carefully compiled two-dimensional synthetic data and three-dimensional medical images. We show that our metrics: (1) penalize errors successfully, especially those around region boundaries; (2) give a low similarity score when existing metrics disagree, thus avoiding overly inflated scores; and (3) score segmentation results over a wider range of values. We analyze a representative metric from this family and the effect of its free parameter on error sensitivity and running time.},
	number = {1},
	urldate = {2021-09-14},
	journal = {Journal of Medical Imaging},
	author = {Yeghiazaryan, Varduhi and Voiculescu, Irina},
	month = jan,
	year = {2018},
	pmid = {29487883},
	pmcid = {PMC5817231},
	pages = {015006},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\64MRQ3YJ\\Yeghiazaryan und Voiculescu - 2018 - Family of boundary overlap metrics for the evaluat.pdf:application/pdf},
}

@article{arya_optimal_nodate,
	title = {An {Optimal} {Algorithm} for {Approximate} {Nearest} {Neighbor} {Searching} in {Fixed} {Dimensions}},
	language = {en},
	author = {Arya, Sunil},
	pages = {33},
	file = {Arya - An Optimal Algorithm for Approximate Nearest Neigh.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JI3M8MGH\\Arya - An Optimal Algorithm for Approximate Nearest Neigh.pdf:application/pdf},
}

@article{nikolov_deep_2021,
	title = {Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy},
	url = {http://arxiv.org/abs/1809.04430},
	abstract = {Over half a million individuals are diagnosed with head and neck cancer each year worldwide. Radiotherapy is an important curative treatment for this disease, but it requires manual time consuming delineation of radio-sensitive organs at risk (OARs). This planning process can delay treatment, while also introducing inter-operator variability with resulting downstream radiation dose differences. While auto-segmentation algorithms offer a potentially time-saving solution, the challenges in defining, quantifying and achieving expert performance remain. Adopting a deep learning approach, we demonstrate a 3D U-Net architecture that achieves expert-level performance in delineating 21 distinct head and neck OARs commonly segmented in clinical practice. The model was trained on a dataset of 663 deidentified computed tomography (CT) scans acquired in routine clinical practice and with both segmentations taken from clinical practice and segmentations created by experienced radiographers as part of this research, all in accordance with consensus OAR definitions. We demonstrate the model's clinical applicability by assessing its performance on a test set of 21 CT scans from clinical practice, each with the 21 OARs segmented by two independent experts. We also introduce surface Dice similarity coefficient (surface DSC), a new metric for the comparison of organ delineation, to quantify deviation between OAR surface contours rather than volumes, better reflecting the clinical task of correcting errors in the automated organ segmentations. The model's generalisability is then demonstrated on two distinct open source datasets, reflecting different centres and countries to model training. With appropriate validation studies and regulatory approvals, this system could improve the efficiency, consistency, and safety of radiotherapy pathways.},
	urldate = {2021-09-14},
	journal = {arXiv:1809.04430 [physics, stat]},
	author = {Nikolov, Stanislav and Blackwell, Sam and Zverovitch, Alexei and Mendes, Ruheena and Livne, Michelle and De Fauw, Jeffrey and Patel, Yojan and Meyer, Clemens and Askham, Harry and Romera-Paredes, Bernardino and Kelly, Christopher and Karthikesalingam, Alan and Chu, Carlton and Carnell, Dawn and Boon, Cheng and D'Souza, Derek and Moinuddin, Syed Ali and Garie, Bethany and McQuinlan, Yasmin and Ireland, Sarah and Hampton, Kiarna and Fuller, Krystle and Montgomery, Hugh and Rees, Geraint and Suleyman, Mustafa and Back, Trevor and Hughes, Cían and Ledsam, Joseph R. and Ronneberger, Olaf},
	month = jan,
	year = {2021},
	note = {arXiv: 1809.04430},
	keywords = {Computer Science - Machine Learning, Physics - Medical Physics, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MSRSQ35X\\Nikolov et al. - 2021 - Deep learning to achieve clinically applicable seg.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PBH96854\\1809.html:text/html},
}

@article{maier-hein_why_2018,
	title = {Why rankings of biomedical image analysis competitions should be interpreted with care},
	volume = {9},
	copyright = {2018 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-07619-7},
	doi = {10.1038/s41467-018-07619-7},
	abstract = {International challenges have become the standard for validation of biomedical image analysis methods. Given their scientific impact, it is surprising that a critical analysis of common practices related to the organization of challenges has not yet been performed. In this paper, we present a comprehensive analysis of biomedical image analysis challenges conducted up to now. We demonstrate the importance of challenges and show that the lack of quality control has critical consequences. First, reproducibility and interpretation of the results is often hampered as only a fraction of relevant information is typically provided. Second, the rank of an algorithm is generally not robust to a number of variables such as the test data used for validation, the ranking scheme applied and the observers that make the reference annotations. To overcome these problems, we recommend best practice guidelines and define open research questions to be addressed in the future.},
	language = {en},
	number = {1},
	urldate = {2021-09-14},
	journal = {Nature Communications},
	author = {Maier-Hein, Lena and Eisenmann, Matthias and Reinke, Annika and Onogur, Sinan and Stankovic, Marko and Scholz, Patrick and Arbel, Tal and Bogunovic, Hrvoje and Bradley, Andrew P. and Carass, Aaron and Feldmann, Carolin and Frangi, Alejandro F. and Full, Peter M. and van Ginneken, Bram and Hanbury, Allan and Honauer, Katrin and Kozubek, Michal and Landman, Bennett A. and März, Keno and Maier, Oskar and Maier-Hein, Klaus and Menze, Bjoern H. and Müller, Henning and Neher, Peter F. and Niessen, Wiro and Rajpoot, Nasir and Sharp, Gregory C. and Sirinukunwattana, Korsuk and Speidel, Stefanie and Stock, Christian and Stoyanov, Danail and Taha, Abdel Aziz and van der Sommen, Fons and Wang, Ching-Wei and Weber, Marc-André and Zheng, Guoyan and Jannin, Pierre and Kopp-Schneider, Annette},
	month = dec,
	year = {2018},
	pages = {5217},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\AUHFMXF2\\Maier-Hein et al. - 2018 - Why rankings of biomedical image analysis competit.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WAIQ3QK9\\s41467-018-07619-7.html:text/html},
}

@article{heimann_comparison_2009,
	title = {Comparison and {Evaluation} of {Methods} for {Liver} {Segmentation} {From} {CT} {Datasets}},
	volume = {28},
	issn = {1558-254X},
	doi = {10.1109/TMI.2009.2013851},
	abstract = {This paper presents a comparison study between 10 automatic and six interactive methods for liver segmentation from contrast-enhanced CT images. It is based on results from the "MICCAI 2007 Grand Challenge" workshop, where 16 teams evaluated their algorithms on a common database. A collection of 20 clinical images with reference segmentations was provided to train and tune algorithms in advance. Participants were also allowed to use additional proprietary training data for that purpose. All teams then had to apply their methods to 10 test datasets and submit the obtained results. Employed algorithms include statistical shape models, atlas registration, level-sets, graph-cuts and rule-based systems. All results were compared to reference segmentations five error measures that highlight different aspects of segmentation accuracy. All measures were combined according to a specific scoring system relating the obtained values to human expert variability. In general, interactive methods reached higher average scores than automatic approaches and featured a better consistency of segmentation quality. However, the best automatic methods (mainly based on statistical shape models with some additional free deformation) could compete well on the majority of test images. The study provides an insight in performance of different segmentation approaches under real-world conditions and highlights achievements and limitations of current image analysis techniques.},
	number = {8},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Heimann, Tobias and van Ginneken, Bram and Styner, Martin A. and Arzhaeva, Yulia and Aurich, Volker and Bauer, Christian and Beck, Andreas and Becker, Christoph and Beichel, Reinhard and Bekes, GyÖrgy and Bello, Fernando and Binnig, Gerd and Bischof, Horst and Bornik, Alexander and Cashman, Peter M. M. and Chi, Ying and Cordova, AndrÉs and Dawant, Benoit M. and Fidrich, MÁrta and Furst, Jacob D. and Furukawa, Daisuke and Grenacher, Lars and Hornegger, Joachim and KainmÜller, Dagmar and Kitney, Richard I. and Kobatake, Hidefumi and Lamecker, Hans and Lange, Thomas and Lee, Jeongjin and Lennon, Brian and Li, Rui and Li, Senhu and Meinzer, Hans-Peter and Nemeth, GÁbor and Raicu, Daniela S. and Rau, Anne-Mareike and van Rikxoort, Eva M. and Rousson, MikaËl and Rusko, LÁszlÓ and Saddi, Kinda A. and Schmidt, GÜnter and Seghers, Dieter and Shimizu, Akinobu and Slagmolen, Pieter and Sorantin, Erich and Soza, Grzegorz and Susomboon, Ruchaneewan and Waite, Jonathan M. and Wimmer, Andreas and Wolf, Ivo},
	month = aug,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Humans, Testing, Image segmentation, segmentation, liver, Evaluation, Image databases, Computed tomography, Deformable models, Knowledge based systems, Liver, Shape, Training data},
	pages = {1251--1265},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UEDRU8ZB\\4781564.html:text/html;Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EUPJ3LH2\\Heimann et al. - 2009 - Comparison and Evaluation of Methods for Liver Seg.pdf:application/pdf},
}

@article{ros_comparative_2021-1,
	title = {Comparative validation of multi-instance instrument segmentation in endoscopy: {Results} of the {ROBUST}-{MIS} 2019 challenge},
	volume = {70},
	issn = {1361-8415},
	shorttitle = {Comparative validation of multi-instance instrument segmentation in endoscopy},
	url = {https://www.sciencedirect.com/science/article/pii/S136184152030284X},
	doi = {10.1016/j.media.2020.101920},
	abstract = {Intraoperative tracking of laparoscopic instruments is often a prerequisite for computer and robotic-assisted interventions. While numerous methods for detecting, segmenting and tracking of medical instruments based on endoscopic video images have been proposed in the literature, key limitations remain to be addressed: Firstly, robustness, that is, the reliable performance of state-of-the-art methods when run on challenging images (e.g. in the presence of blood, smoke or motion artifacts). Secondly, generalization; algorithms trained for a specific intervention in a specific hospital should generalize to other interventions or institutions. In an effort to promote solutions for these limitations, we organized the Robust Medical Instrument Segmentation (ROBUST-MIS) challenge as an international benchmarking competition with a specific focus on the robustness and generalization capabilities of algorithms. For the first time in the field of endoscopic image processing, our challenge included a task on binary segmentation and also addressed multi-instance detection and segmentation. The challenge was based on a surgical data set comprising 10,040 annotated images acquired from a total of 30 surgical procedures from three different types of surgery. The validation of the competing methods for the three tasks (binary segmentation, multi-instance detection and multi-instance segmentation) was performed in three different stages with an increasing domain gap between the training and the test data. The results confirm the initial hypothesis, namely that algorithm performance degrades with an increasing domain gap. While the average detection and segmentation quality of the best-performing algorithms is high, future research should concentrate on detection and segmentation of small, crossing, moving and transparent instrument(s) (parts).},
	language = {en},
	urldate = {2021-09-15},
	journal = {Medical Image Analysis},
	author = {Roß, Tobias and Reinke, Annika and Full, Peter M. and Wagner, Martin and Kenngott, Hannes and Apitz, Martin and Hempe, Hellena and Mindroc-Filimon, Diana and Scholz, Patrick and Tran, Thuy Nuong and Bruno, Pierangela and Arbeláez, Pablo and Bian, Gui-Bin and Bodenstedt, Sebastian and Bolmgren, Jon Lindström and Bravo-Sánchez, Laura and Chen, Hua-Bin and González, Cristina and Guo, Dong and Halvorsen, Pål and Heng, Pheng-Ann and Hosgor, Enes and Hou, Zeng-Guang and Isensee, Fabian and Jha, Debesh and Jiang, Tingting and Jin, Yueming and Kirtac, Kadir and Kletz, Sabrina and Leger, Stefan and Li, Zhixuan and Maier-Hein, Klaus H. and Ni, Zhen-Liang and Riegler, Michael A. and Schoeffmann, Klaus and Shi, Ruohua and Speidel, Stefanie and Stenzel, Michael and Twick, Isabell and Wang, Gutai and Wang, Jiacheng and Wang, Liansheng and Wang, Lu and Zhang, Yujie and Zhou, Yan-Jie and Zhu, Lei and Wiesenfarth, Manuel and Kopp-Schneider, Annette and Müller-Stich, Beat P. and Maier-Hein, Lena},
	month = may,
	year = {2021},
	keywords = {Minimally invasive surgery, Surgical data science, Multi-instance instrument, Robustness and generalization},
	pages = {101920},
	file = {ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Z4EEXP9J\\S136184152030284X.html:text/html},
}

@book{kullback_information_1968,
	address = {Mineola, New York},
	title = {Information {Theory} and {Statistics}},
	isbn = {978-0-486-69684-3},
	abstract = {Highly useful text studies logarithmic measures of information and their application to testing statistical hypotheses. Includes numerous worked examples and problems. References. Glossary. Appendix. 1968 2nd, revised edition.},
	language = {en},
	publisher = {Dover Publications Inc.},
	author = {Kullback, Solomon},
	year = {1968},
	note = {Google-Books-ID: luHcCgAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General},
}

@article{kullback_information_1951,
	title = {On {Information} and {Sufficiency}},
	volume = {22},
	issn = {0003-4851},
	url = {https://www.jstor.org/stable/2236703},
	number = {1},
	urldate = {2021-09-20},
	journal = {The Annals of Mathematical Statistics},
	author = {Kullback, S. and Leibler, R. A.},
	year = {1951},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {79--86},
}

@inproceedings{pham_problems_2020,
	title = {Problems and {Opportunities} in {Training} {Deep} {Learning} {Software} {Systems}: {An} {Analysis} of {Variance}},
	shorttitle = {Problems and {Opportunities} in {Training} {Deep} {Learning} {Software} {Systems}},
	abstract = {Deep learning (DL) training algorithms utilize nondeterminism to improve models' accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation. This work is the first to study the variance of DL systems and the awareness of this variance among researchers and practitioners. Our experiments on three datasets with six popular networks show large overall accuracy differences among identical training runs. Even after excluding weak models, the accuracy difference is 10.8\%. In addition, implementation-level factors alone cause the accuracy difference across identical training runs to be up to 2.9\%, the per-class accuracy difference to be up to 52.4\%, and the training time difference to be up to 145.3\%. All core libraries (TensorFlow, CNTK, and Theano) and low-level libraries (e.g., cuDNN) exhibit implementation-level variance across all evaluated versions. Our researcher and practitioner survey shows that 83.8\% of the 901 participants are unaware of or unsure about any implementation-level variance. In addition, our literature survey shows that only 19.5±3\% of papers in recent top software engineering (SE), artificial intelligence (AI), and systems conferences use multiple identical training runs to quantify the variance of their DL approaches. This paper raises awareness of DL variance and directs SE researchers to challenging tasks such as creating deterministic DL implementations to facilitate debugging and improving the reproducibility of DL software and results.},
	booktitle = {2020 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
	month = sep,
	year = {2020},
	note = {ISSN: 2643-1572},
	keywords = {Training, deep learning, Deep learning, Computational modeling, Debugging, Libraries, nondeterminism, Software, Software engineering, variance},
	pages = {771--783},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MXQMM69R\\9286042.html:text/html},
}

@article{trajanovski_tongue_2021,
	title = {Tongue {Tumor} {Detection} in {Hyperspectral} {Images} {Using} {Deep} {Learning} {Semantic} {Segmentation}},
	volume = {68},
	issn = {1558-2531},
	doi = {10.1109/TBME.2020.3026683},
	abstract = {OBJECTIVE: The utilization of hyperspectral imaging (HSI) in real-time tumor segmentation during a surgery have recently received much attention, but it remains a very challenging task.
METHODS: In this work, we propose semantic segmentation methods, and compare them with other relevant deep learning algorithms for tongue tumor segmentation. To the best of our knowledge, this is the first work using deep learning semantic segmentation for tumor detection in HSI data using channel selection, and accounting for more spatial tissue context, and global comparison between the prediction map, and the annotation per sample. Results, and Conclusion: On a clinical data set with tongue squamous cell carcinoma, our best method obtains very strong results of average dice coefficient, and area under the ROC-curve of [Formula: see text], and [Formula: see text], respectively on the original spatial image size. The results show that a very good performance can be achieved even with a limited amount of data. We demonstrate that important information regarding tumor decision is encoded in various channels, but some channel selection, and filtering is beneficial over the full spectra. Moreover, we use both visual (VIS), and near-infrared (NIR) spectrum, rather than commonly used only VIS spectrum; although VIS spectrum is generally of higher significance, we demonstrate NIR spectrum is crucial for tumor capturing in some cases.
SIGNIFICANCE: The HSI technology augmented with accurate deep learning algorithms has a huge potential to be a promising alternative to digital pathology or a doctors' supportive tool in real-time surgeries.},
	language = {eng},
	number = {4},
	journal = {IEEE transactions on bio-medical engineering},
	author = {Trajanovski, Stojan and Shan, Caifeng and Weijtmans, Pim J. C. and de Koning, Susan G. Brouwer and Ruers, Theo J. M.},
	month = apr,
	year = {2021},
	pmid = {32976092},
	keywords = {Humans, Deep Learning, Carcinoma, Squamous Cell, Semantics, Tongue, Tongue Neoplasms},
	pages = {1330--1340},
	file = {Trajanovski et al. - 2021 - Tongue Tumor Detection in Hyperspectral Images Usi.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MUTG8CRW\\Trajanovski et al. - 2021 - Tongue Tumor Detection in Hyperspectral Images Usi.pdf:application/pdf},
}

@inproceedings{garifullin_hyperspectral_2018-1,
	title = {Hyperspectral {Image} {Segmentation} of {Retinal} {Vasculature}, {Optic} {Disc} and {Macula}},
	doi = {10.1109/DICTA.2018.8615761},
	abstract = {The most common approach for retinal imaging is the eye fundus photography which usually results in RGB images. Recent studies show that the additional spectral information provides useful features for automatic retinal image analysis. The current work extends recent research on the joint segmentation of retinal vasculature, optic disc and macula which often appears in different retinal image analysis tasks. Fully convolutional neural networks are utilized to solve the segmentation problem. It is shown that the network architectures can be effectively modified for the spectral data and the utilization of spectral information provides moderate improvements in retinal image segmentation.},
	booktitle = {2018 {Digital} {Image} {Computing}: {Techniques} and {Applications} ({DICTA})},
	author = {Garifullin, Azat and Kööbi, Peeter and Ylitepsa, Pasi and Ådjers, Kati and Hauta-Kasari, Markku and Uusitalo, Hannu and Lensu, Lasse},
	month = dec,
	year = {2018},
	keywords = {Hyperspectral imaging, Image segmentation, Optical imaging, Dimensionality reduction, Object segmentation, Retina, Task analysis},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\NHDP7IBP\\8615761.html:text/html;Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\M2PW2KBK\\Garifullin et al. - 2018 - Hyperspectral Image Segmentation of Retinal Vascul.pdf:application/pdf},
}

@article{badrinarayanan_segnet_2016,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	shorttitle = {{SegNet}},
	url = {http://arxiv.org/abs/1511.00561},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	urldate = {2021-09-29},
	journal = {arXiv:1511.00561 [cs]},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = oct,
	year = {2016},
	note = {arXiv: 1511.00561},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\E686CLWF\\Badrinarayanan et al. - 2016 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YVWNJZ7J\\1511.html:text/html},
}

@article{jegou_one_2017,
	title = {The {One} {Hundred} {Layers} {Tiramisu}: {Fully} {Convolutional} {DenseNets} for {Semantic} {Segmentation}},
	shorttitle = {The {One} {Hundred} {Layers} {Tiramisu}},
	url = {http://arxiv.org/abs/1611.09326},
	abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py},
	urldate = {2021-09-29},
	journal = {arXiv:1611.09326 [cs]},
	author = {Jégou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua},
	month = oct,
	year = {2017},
	note = {arXiv: 1611.09326},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\57A8XY77\\Jégou et al. - 2017 - The One Hundred Layers Tiramisu Fully Convolution.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3G4VACB4\\1611.html:text/html},
}

@article{boser_training_1992,
	title = {A {Training} {Algorithm} for {Optimal} {Margin} {Classifiers}},
	abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classi action functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
	language = {en},
	journal = {Proceedings of the fifth annual workshop on Computational learning theory},
	author = {Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
	year = {1992},
	pages = {144--152},
	file = {Boser et al. - OAptTimraainl iMngarAglignoCritlahsmsi feorrs.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\GB5MHJR7\\Boser et al. - OAptTimraainl iMngarAglignoCritlahsmsi feorrs.pdf:application/pdf},
}

@article{vali_deep_2020,
	title = {Deep {Learning} for {Land} {Use} and {Land} {Cover} {Classification} {Based} on {Hyperspectral} and {Multispectral} {Earth} {Observation} {Data}: {A} {Review}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Deep {Learning} for {Land} {Use} and {Land} {Cover} {Classification} {Based} on {Hyperspectral} and {Multispectral} {Earth} {Observation} {Data}},
	url = {https://www.mdpi.com/2072-4292/12/15/2495},
	doi = {10.3390/rs12152495},
	abstract = {Lately, with deep learning outpacing the other machine learning techniques in classifying images, we have witnessed a growing interest of the remote sensing community in employing these techniques for the land use and land cover classification based on multispectral and hyperspectral images; the number of related publications almost doubling each year since 2015 is an attest to that. The advances in remote sensing technologies, hence the fast-growing volume of timely data available at the global scale, offer new opportunities for a variety of applications. Deep learning being significantly successful in dealing with Big Data, seems to be a great candidate for exploiting the potentials of such complex massive data. However, there are some challenges related to the ground-truth, resolution, and the nature of data that strongly impact the performance of classification. In this paper, we review the use of deep learning in land use and land cover classification based on multispectral and hyperspectral images and we introduce the available data sources and datasets used by literature studies; we provide the readers with a framework to interpret the-state-of-the-art of deep learning in this context and offer a platform to approach methodologies, data, and challenges of the field.},
	language = {en},
	number = {15},
	urldate = {2021-09-29},
	journal = {Remote Sensing},
	author = {Vali, Ava and Comai, Sara and Matteucci, Matteo},
	month = jan,
	year = {2020},
	note = {Number: 15
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {machine learning, convolutional neural networks, data fusion, deep Learning, end-to-end learning, feature engineering, ground-truth scarcity, hyperspectral data, LULC classification, multispectral data, remote sensing data},
	pages = {2495},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RGM4DZLK\\Vali et al. - 2020 - Deep Learning for Land Use and Land Cover Classifi.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\S58TFHUG\\htm.html:text/html},
}

@article{signoroni_deep_2019,
	title = {Deep {Learning} {Meets} {Hyperspectral} {Image} {Analysis}: {A} {Multidisciplinary} {Review}},
	volume = {5},
	issn = {2313-433X},
	shorttitle = {Deep {Learning} {Meets} {Hyperspectral} {Image} {Analysis}},
	url = {https://www.mdpi.com/2313-433X/5/5/52},
	doi = {10.3390/jimaging5050052},
	abstract = {Modern hyperspectral imaging systems produce huge datasets potentially conveying a great abundance of information; such a resource, however, poses many challenges in the analysis and interpretation of these data. Deep learning approaches certainly offer a great variety of opportunities for solving classical imaging tasks and also for approaching new stimulating problems in the spatial–spectral domain. This is fundamental in the driving sector of Remote Sensing where hyperspectral technology was born and has mostly developed, but it is perhaps even more true in the multitude of current and evolving application sectors that involve these imaging technologies. The present review develops on two fronts: on the one hand, it is aimed at domain professionals who want to have an updated overview on how hyperspectral acquisition techniques can combine with deep learning architectures to solve speciﬁc tasks in different application ﬁelds. On the other hand, we want to target the machine learning and computer vision experts by giving them a picture of how deep learning technologies are applied to hyperspectral data from a multidisciplinary perspective. The presence of these two viewpoints and the inclusion of application ﬁelds other than Remote Sensing are the original contributions of this review, which also highlights some potentialities and critical issues related to the observed development trends.},
	language = {en},
	number = {5},
	urldate = {2021-09-29},
	journal = {Journal of Imaging},
	author = {Signoroni, Alberto and Savardi, Mattia and Baronio, Annalisa and Benini, Sergio},
	month = may,
	year = {2019},
	pages = {52},
	file = {Signoroni et al. - 2019 - Deep Learning Meets Hyperspectral Image Analysis .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PVQ7AS4X\\Signoroni et al. - 2019 - Deep Learning Meets Hyperspectral Image Analysis .pdf:application/pdf},
}

@article{khan_modern_2018,
	title = {Modern {Trends} in {Hyperspectral} {Image} {Analysis}: {A} {Review}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Modern {Trends} in {Hyperspectral} {Image} {Analysis}},
	url = {https://ieeexplore.ieee.org/document/8314827/},
	doi = {10.1109/ACCESS.2018.2812999},
	abstract = {Over the past three decades, signiﬁcant developments have been made in hyperspectral imaging due to which it has emerged as an effective tool in numerous civil, environmental, and military applications. Modern sensor technologies are capable of covering large surfaces of earth with exceptional spatial, spectral, and temporal resolutions. Due to these features, hyperspectral imaging has been effectively used in numerous remote sensing applications requiring estimation of physical parameters of many complex surfaces and identiﬁcation of visually similar materials having ﬁne spectral signatures. In the recent years, ground based hyperspectral imaging has gained immense interest in the research on electronic imaging for food inspection, forensic science, medical surgery and diagnosis, and military applications. This review focuses on the fundamentals of hyperspectral image analysis and its modern applications such as food quality and safety assessment, medical diagnosis and image guided surgery, forensic document examination, defense and homeland security, remote sensing applications such as precision agriculture and water resource management and material identiﬁcation and mapping of artworks. Moreover, recent research on the use of hyperspectral imaging for examination of forgery detection in questioned documents, aided by deep learning, is also presented. This review can be a useful baseline for future research in hyperspectral image analysis.},
	language = {en},
	urldate = {2021-09-29},
	journal = {IEEE Access},
	author = {Khan, Muhammad Jaleed and Khan, Hamid Saeed and Yousaf, Adeel and Khurshid, Khurram and Abbas, Asad},
	year = {2018},
	pages = {14118--14129},
	file = {08314827.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7LDBXXJE\\08314827.pdf:application/pdf},
}

@article{ghamisi_new_2018,
	title = {New {Frontiers} in {Spectral}-{Spatial} {Hyperspectral} {Image} {Classification}: {The} {Latest} {Advances} {Based} on {Mathematical} {Morphology}, {Markov} {Random} {Fields}, {Segmentation}, {Sparse} {Representation}, and {Deep} {Learning}},
	volume = {6},
	issn = {2168-6831},
	shorttitle = {New {Frontiers} in {Spectral}-{Spatial} {Hyperspectral} {Image} {Classification}},
	doi = {10.1109/MGRS.2018.2854840},
	abstract = {In recent years, airborne and spaceborne hyperspectral imaging systems have advanced in terms of spectral and spatial resolution, which makes the data sets they produce a valuable source for land cover classification. The availability of hyperspectral data with fine spatial resolution has revolutionized hyperspectral image (HSI) classification techniques by taking advantage of both spectral and spatial information in a single classification framework.},
	number = {3},
	journal = {IEEE Geoscience and Remote Sensing Magazine},
	author = {Ghamisi, Pedram and Maggiori, Emmanuel and Li, Shutao and Souza, Roberto and Tarablaka, Yuliya and Moser, Gabriele and De Giorgi, Andrea and Fang, Leyuan and Chen, Yushi and Chi, Mingmin and Serpico, Sebastiano B. and Benediktsson, Jón Atli},
	month = sep,
	year = {2018},
	note = {Conference Name: IEEE Geoscience and Remote Sensing Magazine},
	keywords = {Hyperspectral imaging, Spatial resolution, Aerospace electronics, Space vehicles},
	pages = {10--43},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\STX9YB2J\\8474403.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2XIMYV24\\Ghamisi et al. - 2018 - New Frontiers in Spectral-Spatial Hyperspectral Im.pdf:application/pdf},
}

@inproceedings{alam_crf_2016,
	address = {Beijing, China},
	title = {{CRF} learning with {CNN} features for hyperspectral image segmentation},
	isbn = {978-1-5090-3332-4},
	url = {http://ieeexplore.ieee.org/document/7730798/},
	doi = {10.1109/IGARSS.2016.7730798},
	abstract = {This paper proposes a method that uses both spectral and spatial information to segment remote sensing hyperspectral images. After a hyperspectral image is over-segmented into superpixels, a deep Convolutional Neural Network (CNN) is used to perform superpixel-level labelling. To further delineate objects from a hyperspectral scene, this paper attempts to combine the properties of CNN and Conditional Random Field (CRF). A mean-ﬁeld approximation algorithm for CRF inference is used and formulated with Gaussian pairwise potentials as Recurrent Neural Network. This combined network is then plugged into the CNN which leads to a deep network that has robust characteristics of both CNN and CRF. Preliminary results suggest the usefulness of this framework to a promising extent.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {2016 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium} ({IGARSS})},
	publisher = {IEEE},
	author = {Alam, Fahim Irfan and Zhou, Jun and Liew, Alan Wee-Chung and Jia, Xiuping},
	month = jul,
	year = {2016},
	pages = {6890--6893},
	file = {Alam et al. - 2016 - CRF learning with CNN features for hyperspectral i.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FWX6T2T8\\Alam et al. - 2016 - CRF learning with CNN features for hyperspectral i.pdf:application/pdf},
}

@article{paul_classification_2021,
	title = {Classification of hyperspectral imagery using spectrally partitioned {HyperUnet}},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-021-06532-3},
	doi = {10.1007/s00521-021-06532-3},
	abstract = {Classification is one of the forefront research areas in hyperspectral image processing. The large intra-class and small inter-class variance in the pixel values of objects of interest still poses challenges in classification task. The huge dimension and a minimal number of labeled information further add challenges in the case of hyperspectral image classification. Therefore, in the present research, a novel architecture is conceived which is inspired by the U-net architecture along with spectral partitioning. The proposed architecture (HyperUnet) mainly addresses the broader issue of classification of hyperspectral images by classifying each pixel. The performance of the proposed model is evaluated on two benchmark datasets and compared with existing U-net-based models. The overall classification accuracy obtained in experiments is more than 93\% which is better than the other compared methods in the same field.},
	language = {en},
	urldate = {2021-09-30},
	journal = {Neural Computing and Applications},
	author = {Paul, Arati and Bhoumik, Sanghamita},
	month = sep,
	year = {2021},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2IWRUBEW\\Paul und Bhoumik - 2021 - Classification of hyperspectral imagery using spec.pdf:application/pdf},
}

@article{zhu_spectral-spatial-dependent_2021,
	title = {A {Spectral}-{Spatial}-{Dependent} {Global} {Learning} {Framework} for {Insufficient} and {Imbalanced} {Hyperspectral} {Image} {Classification}},
	issn = {2168-2275},
	doi = {10.1109/TCYB.2021.3070577},
	abstract = {Deep learning techniques have been widely applied to hyperspectral image (HSI) classification and have achieved great success. However, the deep neural network model has a large parameter space and requires a large number of labeled data. Deep learning methods for HSI classification usually follow a patchwise learning framework. Recently, a fast patch-free global learning (FPGA) architecture was proposed for HSI classification according to global spatial context information. However, FPGA has difficulty in extracting the most discriminative features when the sample data are imbalanced. In this article, a spectral-spatial-dependent global learning (SSDGL) framework based on the global convolutional long short-term memory (GCL) and global joint attention mechanism (GJAM) is proposed for insufficient and imbalanced HSI classification. In SSDGL, the hierarchically balanced (H-B) sampling strategy and the weighted softmax loss are proposed to address the imbalanced sample problem. To effectively distinguish similar spectral characteristics of land cover types, the GCL module is introduced to extract the long short-term dependency of spectral features. To learn the most discriminative feature representations, the GJAM module is proposed to extract attention areas. The experimental results obtained with three public HSI datasets show that the SSDGL has powerful performance in insufficient and imbalanced sample problems and is superior to other state-of-the-art methods.},
	journal = {IEEE Transactions on Cybernetics},
	author = {Zhu, Qiqi and Deng, Weihuan and Zheng, Zhuo and Zhong, Yanfei and Guan, Qingfeng and Lin, Weihua and Zhang, Liangpei and Li, Deren},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Cybernetics},
	keywords = {Hyperspectral imaging, Feature extraction, Training, Convolution, Data mining, Deep learning, feature representations, Field programmable gate arrays, hyperspectral image (HSI) classification, imbalanced sample, patchwise},
	pages = {1--15},
	file = {Eingereichte Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JY46MULV\\Zhu et al. - 2021 - A Spectral-Spatial-Dependent Global Learning Frame.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XEC9DTFE\\9440852.html:text/html},
}

@article{nalepa_towards_2020,
	title = {Towards resource-frugal deep convolutional neural networks for hyperspectral image segmentation},
	volume = {73},
	issn = {0141-9331},
	url = {https://www.sciencedirect.com/science/article/pii/S0141933119302844},
	doi = {10.1016/j.micpro.2020.102994},
	abstract = {Hyperspectral image analysis has been gaining research attention thanks to the current advances in sensor design which have made acquiring such imagery much more affordable. Although there exist various approaches for segmenting hyperspectral images, deep learning has become the mainstream. However, such large-capacity learners are characterized by significant memory footprints. This is a serious obstacle in employing deep neural networks on board a satellite for Earth observation. In this paper, we introduce resource-frugal quantized convolutional neural networks, and greatly reduce their size without adversely affecting the classification capability. Our experiments performed over two hyperspectral benchmarks showed that the quantization process can be seamlessly applied during the training, and it leads to much smaller and still well-generalizing deep models.},
	language = {en},
	urldate = {2021-09-30},
	journal = {Microprocessors and Microsystems},
	author = {Nalepa, Jakub and Antoniak, Marek and Myller, Michal and Ribalta Lorenzo, Pablo and Marcinkiewicz, Michal},
	month = mar,
	year = {2020},
	keywords = {Classification, Quantization, Hyperspectral imaging, Convolutional neural network, Deep neural network, Segmentation},
	pages = {102994},
	file = {Nalepa et al. - 2020 - Towards resource-frugal deep convolutional neural .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9NFAJZVR\\Nalepa et al. - 2020 - Towards resource-frugal deep convolutional neural .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RGMJ53EI\\S0141933119302844.html:text/html},
}

@article{nalepa_validating_2019,
	title = {Validating {Hyperspectral} {Image} {Segmentation}},
	volume = {16},
	issn = {1558-0571},
	doi = {10.1109/LGRS.2019.2895697},
	abstract = {Hyperspectral satellite imaging attracts enormous research attention in the remote sensing community, and hence, automated approaches for precise segmentation of such imagery are being rapidly developed. In this letter, we share our observations on the strategy for validating hyperspectral image segmentation algorithms currently followed in the literature, and show that it can lead to overoptimistic experimental insights. We introduce a new routine for generating segmentation benchmarks and use it to elaborate ready-to-use hyperspectral training-test data partitions. They can be utilized for fair validation of new and existing algorithms without any training-test data leakage.},
	number = {8},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Nalepa, Jakub and Myller, Michal and Kawulok, Michal},
	month = aug,
	year = {2019},
	note = {Conference Name: IEEE Geoscience and Remote Sensing Letters},
	keywords = {Classification, Hyperspectral imaging, hyperspectral imaging, Training, Data mining, Image segmentation, segmentation, Benchmark testing, deep learning (DL), Imaging, validation},
	pages = {1264--1268},
	file = {Eingereichte Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\R5WXZGJX\\Nalepa et al. - 2019 - Validating Hyperspectral Image Segmentation.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\X8FH7MIE\\8642388.html:text/html},
}

@article{cervantes-sanchez_automatic_2021,
	title = {Automatic tissue segmentation of hyperspectral images in liver and head neck surgeries using machine learning},
	volume = {1},
	url = {https://aisjournal.net/article/view/4291},
	doi = {10.20517/ais.2021.05},
	abstract = {Automatic tissue segmentation of hyperspectral images in liver and head neck surgeries using machine learning},
	language = {en-us},
	urldate = {2021-09-30},
	journal = {Artificial Intelligence Surgery},
	author = {Cervantes-Sanchez, Fernando and Maktabi, Marianne and Köhler, Hannes and Sucher, Robert and Rayes, Nada and Avina-Cervantes, Juan Gabriel and Cruz-Aceves, Ivan and Chalopin, Claire},
	month = aug,
	year = {2021},
	pages = {22--37},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CDSGDVVM\\Cervantes-Sanchez et al. - 2021 - Automatic tissue segmentation of hyperspectral ima.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2UPG53XC\\4291.html:text/html},
}

@inproceedings{akbari_wavelet-based_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Wavelet-{Based} {Compression} and {Segmentation} of {Hyperspectral} {Images} in {Surgery}},
	isbn = {978-3-540-79982-5},
	doi = {10.1007/978-3-540-79982-5_16},
	abstract = {Considering the anatomical variations and unpredictable nature of surgeries, visibility during surgery is very important especially to correctly diagnose problems. Hyperspectral imaging has developed as a compact imaging and spectroscopic tool that can be used for different applications including medical diagnostics. This paper presents the application of hyperspectral imaging as a visual supporting tool to detect different organs and tissues during surgeries. It will be useful for finding ectopic tissues and diagnosis of tissue abnormalities. The high-dimensional data were compressed using wavelet transform and classified using artificial neural networks. The performance of this method is evaluated for the detection of the spleen, colon, small intestine, urinary bladder, and peritoneum in a surgery on a pig.},
	language = {en},
	booktitle = {Medical {Imaging} and {Augmented} {Reality}},
	publisher = {Springer},
	author = {Akbari, Hamed and Kosugi, Yukio and Kojima, Kazuyuki and Tanaka, Naofumi},
	editor = {Dohi, Takeyoshi and Sakuma, Ichiro and Liao, Hongen},
	year = {2008},
	keywords = {Segmentation, Hyperspectral Image, Medical Imaging, Multi-Dimensional Image},
	pages = {142--149},
	file = {Akbari et al. - 2008 - Wavelet-Based Compression and Segmentation of Hype.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\URHBSK4C\\Akbari et al. - 2008 - Wavelet-Based Compression and Segmentation of Hype.pdf:application/pdf},
}

@incollection{kohonen_learning_1995,
	address = {Berlin, Heidelberg},
	series = {Springer {Series} in {Information} {Sciences}},
	title = {Learning {Vector} {Quantization}},
	isbn = {978-3-642-97610-0},
	url = {https://doi.org/10.1007/978-3-642-97610-0_6},
	abstract = {Closely related to VQ and SOM is Learning Vector Quantization (LVQ). This name signifies a class of related algorithms, such as LVQ1, LVQ2, LVQ3, and OLVQ1. While VQ and the basic SOM are unsupervised clustering and learning methods, LVQ describes supervised learning. On the other hand, unlike in SOM, no neighborhoods around the “winner” are defined during learning in the basic LVQ, whereby also no spatial order of the codebook vectors is expected to ensue.},
	language = {en},
	urldate = {2021-10-01},
	booktitle = {Self-{Organizing} {Maps}},
	publisher = {Springer},
	author = {Kohonen, Teuvo},
	editor = {Kohonen, Teuvo},
	year = {1995},
	doi = {10.1007/978-3-642-97610-0_6},
	pages = {175--189},
}

@article{grammatikopoulou_cadis_2021-1,
	title = {{CaDIS}: {Cataract} dataset for surgical {RGB}-image segmentation},
	volume = {71},
	issn = {1361-8415},
	shorttitle = {{CaDIS}},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521000992},
	doi = {10.1016/j.media.2021.102053},
	abstract = {Video feedback provides a wealth of information about surgical procedures and is the main sensory cue for surgeons. Scene understanding is crucial to computer assisted interventions (CAI) and to post-operative analysis of the surgical procedure. A fundamental building block of such capabilities is the identification and localization of surgical instruments and anatomical structures through semantic segmentation. Deep learning has advanced semantic segmentation techniques in the recent years but is inherently reliant on the availability of labelled datasets for model training. This paper introduces a dataset for semantic segmentation of cataract surgery videos complementing the publicly available CATARACTS challenge dataset. In addition, we benchmark the performance of several state-of-the-art deep learning models for semantic segmentation on the presented dataset. The dataset is publicly available at https://cataracts-semantic-segmentation2020.grand-challenge.org/.},
	language = {en},
	urldate = {2021-10-01},
	journal = {Medical Image Analysis},
	author = {Grammatikopoulou, Maria and Flouty, Evangello and Kadkhodamohammadi, Abdolrahim and Quellec, Gwenolé and Chow, Andre and Nehme, Jean and Luengo, Imanol and Stoyanov, Danail},
	month = jul,
	year = {2021},
	keywords = {Cataract surgery, Dataset, Semantic segmentation},
	pages = {102053},
	file = {ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PGLMMVP7\\S1361841521000992.html:text/html;Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UVGWZ5F7\\Grammatikopoulou et al. - 2021 - CaDIS Cataract dataset for surgical RGB-image seg.pdf:application/pdf},
}

@article{rivas-blanco_review_2021,
	title = {A {Review} on {Deep} {Learning} in {Minimally} {Invasive} {Surgery}},
	volume = {PP},
	doi = {10.1109/ACCESS.2021.3068852},
	abstract = {In the last five years, deep learning has attracted great interest in computer-assisted systems for Minimally Invasive Surgery. The straightforward accessibility to images in surgical interventions makes deep neural networks enormously powerful for solving classification problems in complex surgical scenarios. The objective of this work is to provide readers a survey on deep learning models applied to minimally invasive surgery, identifying the different architectures used depending on the application, the results achieved until now, and the publicly available surgical datasets that can be used for validating new studies. A total of 85 publications have been extracted from manual research from four databases (IEEE Xplorer, Springer Link, Science Direct, and ACM Digital Library). After analyzing all these studies, they have been classified into four applications: surgical image analysis, surgical task analysis, surgical skill assessment, and automation of surgical tasks. This work provides a technical description of these works and a comparison among them. Finally, promising research directions to advance in this field are identified.},
	journal = {IEEE Access},
	author = {Rivas-Blanco, Irene and Perez-del-Pulgar, Carlos and Garcia-Morales, Isabel and Munoz, Victor},
	month = mar,
	year = {2021},
	pages = {1--1},
	file = {Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ITQYHQGN\\Rivas-Blanco et al. - 2021 - A Review on Deep Learning in Minimally Invasive Su.pdf:application/pdf},
}

@incollection{zhou_feature_2019,
	address = {Cham},
	title = {Feature {Aggregation} {Decoder} for {Segmenting} {Laparoscopic} {Scenes}},
	volume = {11796},
	isbn = {978-3-030-32694-4 978-3-030-32695-1},
	url = {http://link.springer.com/10.1007/978-3-030-32695-1_1},
	abstract = {Laparoscopic scene segmentation is one of the key building blocks required for developing advanced computer assisted interventions and robotic automation. Scene segmentation approaches often rely on encoder-decoder architectures that encode a representation of the input to be decoded to semantic pixel labels. In this paper, we propose to use the deep Xception model for the encoder and a simple yet effective decoder that relies on a feature aggregation module. Our feature aggregation module constructs a mapping function that reuses and transfers encoder features and combines information across all feature scales to build a richer representation that keeps both high-level context and low-level boundary information. We argue that this aggregation module enables us to simplify the decoder and reduce the number of parameters in the decoder. We have evaluated our approach on two datasets and our experimental results show that our model outperforms state-of-the-art models on the same experimental setup and signiﬁcantly improves the previous results, 98.44\% vs 89.00\%, on the EndoVis'15 dataset.},
	language = {en},
	urldate = {2021-10-02},
	booktitle = {{OR} 2.0 {Context}-{Aware} {Operating} {Theaters} and {Machine} {Learning} in {Clinical} {Neuroimaging}},
	publisher = {Springer International Publishing},
	author = {Kadkhodamohammadi, Abdolrahim and Luengo, Imanol and Barbarisi, Santiago and Taleb, Hinde and Flouty, Evangello and Stoyanov, Danail},
	editor = {Zhou, Luping and Sarikaya, Duygu and Kia, Seyed Mostafa and Speidel, Stefanie and Malpani, Anand and Hashimoto, Daniel and Habes, Mohamad and Löfstedt, Tommy and Ritter, Kerstin and Wang, Hongzhi},
	year = {2019},
	doi = {10.1007/978-3-030-32695-1_1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {3--11},
	file = {Kadkhodamohammadi et al. - 2019 - Feature Aggregation Decoder for Segmenting Laparos.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RFEJFSPN\\Kadkhodamohammadi et al. - 2019 - Feature Aggregation Decoder for Segmenting Laparos.pdf:application/pdf},
}

@inproceedings{leibetseder_lapgyn4_2018,
	title = {Lapgyn4: a dataset for 4 automatic content analysis problems in the domain of laparoscopic gynecology},
	shorttitle = {Lapgyn4},
	doi = {10.1145/3204949.3208127},
	abstract = {Modern imaging technology enables medical practitioners to perform minimally invasive surgery (MIS), i.e. a variety of medical interventions inflicting minimal trauma upon patients, hence, greatly improving their recoveries. Not only patients but also surgeons can benefit from this technology, as recorded media can be utilized for speeding-up tedious and time-consuming tasks such as treatment planning or case documentation. In order to improve the predominantly manually conducted process of analyzing said media, with this work we publish four datasets extracted from gynecologic, laparoscopic interventions with the intend on encouraging research in the field of post-surgical automatic media analysis. These datasets are designed with the following use cases in mind: medical image retrieval based on a query image, detection of instrument counts, surgical actions and anatomical structures, as well as distinguishing on which anatomical structure a certain action is performed. Furthermore, we provide suggestions for evaluation metrics and first baseline experiments.},
	author = {Leibetseder, Andreas and Petscharnig, Stefan and Primus, Manfred and Kietz, Sabrina and Münzer, Bernd and Schoeffmann, Klaus and Keckstein, Jörg},
	month = jun,
	year = {2018},
	pages = {357--362},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QF9SA9FP\\Leibetseder et al. - 2018 - Lapgyn4 a dataset for 4 automatic content analysi.pdf:application/pdf},
}

@inproceedings{webster_deep_2017,
	address = {Orlando, Florida, United States},
	title = {Deep residual networks for automatic segmentation of laparoscopic videos of the liver},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2255975},
	doi = {10.1117/12.2255975},
	abstract = {Method: We defined a CNN architecture comprising fully-convolutional deep residual networks with multi-resolution loss functions. The CNN was trained in a leave-one-patient-out cross-validation on 2050 video frames from 6 liver resections and 7 laparoscopic staging procedures, and evaluated using the Dice score.
Results: The CNN yielded segmentations with Dice scores ≥0.95 for the majority of images; however, the inter-patient variability in median Dice score was substantial. Four failure modes were identified from low scoring segmentations: minimal visible liver tissue, inter-patient variability in liver appearance, automatic exposure correction, and pathological liver tissue that mimics non-liver tissue appearance.
Conclusion: CNNs offer a feasible approach for accurately segmenting liver from other anatomy on laparoscopic video, but additional data or computational advances are necessary to address challenges due to the high inter-patient variability in liver appearance.},
	language = {en},
	urldate = {2021-10-02},
	author = {Gibson, Eli and Robu, Maria R. and Thompson, Stephen and Edwards, P. Eddie and Schneider, Crispin and Gurusamy, Kurinchi and Davidson, Brian and Hawkes, David J. and Barratt, Dean C. and Clarkson, Matthew J.},
	editor = {Webster, Robert J. and Fei, Baowei},
	month = mar,
	year = {2017},
	pages = {101351M},
	file = {Gibson et al. - 2017 - Deep residual networks for automatic segmentation .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\X3A6D92G\\Gibson et al. - 2017 - Deep residual networks for automatic segmentation .pdf:application/pdf},
}

@article{madad_zadeh_surgai_2020,
	title = {{SurgAI}: deep learning for computerized laparoscopic image understanding in gynaecology},
	volume = {34},
	issn = {1432-2218},
	shorttitle = {{SurgAI}},
	url = {https://doi.org/10.1007/s00464-019-07330-8},
	doi = {10.1007/s00464-019-07330-8},
	abstract = {In laparoscopy, the digital camera offers surgeons the opportunity to receive support from image-guided surgery systems. Such systems require image understanding, the ability for a computer to understand what the laparoscope sees. Image understanding has recently progressed owing to the emergence of artificial intelligence and especially deep learning techniques. However, the state of the art of deep learning in gynaecology only offers image-based detection, reporting the presence or absence of an anatomical structure, without finding its location. A solution to the localisation problem is given by the concept of semantic segmentation, giving the detection and pixel-level location of a structure in an image. The state-of-the-art results in semantic segmentation are achieved by deep learning, whose usage requires a massive amount of annotated data. We propose the first dataset dedicated to this task and the first evaluation of deep learning-based semantic segmentation in gynaecology.},
	language = {en},
	number = {12},
	urldate = {2021-10-02},
	journal = {Surgical Endoscopy},
	author = {Madad Zadeh, Sabrina and Francois, Tom and Calvet, Lilian and Chauvet, Pauline and Canis, Michel and Bartoli, Adrien and Bourdel, Nicolas},
	month = dec,
	year = {2020},
	pages = {5377--5383},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4TLYURR4\\Madad Zadeh et al. - 2020 - SurgAI deep learning for computerized laparoscopi.pdf:application/pdf},
}

@article{gong_using_2021,
	title = {Using deep learning to identify the recurrent laryngeal nerve during thyroidectomy},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-93202-y},
	doi = {10.1038/s41598-021-93202-y},
	abstract = {Surgeons must visually distinguish soft-tissues, such as nerves, from surrounding anatomy to prevent complications and optimize patient outcomes. An accurate nerve segmentation and analysis tool could provide useful insight for surgical decision-making. Here, we present an end-to-end, automatic deep learning computer vision algorithm to segment and measure nerves. Unlike traditional medical imaging, our unconstrained setup with accessible handheld digital cameras, along with the unstructured open surgery scene, makes this task uniquely challenging. We investigate one common procedure, thyroidectomy, during which surgeons must avoid damaging the recurrent laryngeal nerve (RLN), which is responsible for human speech. We evaluate our segmentation algorithm on a diverse dataset across varied and challenging settings of operating room image capture, and show strong segmentation performance in the optimal image capture condition. This work lays the foundation for future research in real-time tissue discrimination and integration of accessible, intelligent tools into open surgery to provide actionable insights.},
	language = {en},
	number = {1},
	urldate = {2021-10-02},
	journal = {Scientific Reports},
	author = {Gong, Julia and Holsinger, F. Christopher and Noel, Julia E. and Mitani, Sohei and Jopling, Jeff and Bedi, Nikita and Koh, Yoon Woo and Orloff, Lisa A. and Cernea, Claudio R. and Yeung, Serena},
	month = jul,
	year = {2021},
	pages = {14306},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DHVLMICM\\Gong et al. - 2021 - Using deep learning to identify the recurrent lary.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TWNEPN3X\\s41598-021-93202-y.html:text/html},
}

@article{al-surmi_new_2014,
	title = {A new human heart vessel identification, segmentation and {3D} reconstruction mechanism},
	volume = {9},
	issn = {1749-8090},
	url = {https://doi.org/10.1186/s13019-014-0161-1},
	doi = {10.1186/s13019-014-0161-1},
	abstract = {The identification and segmentation of inhomogeneous image regions is one of the most challenging issues nowadays. The surface vessels of the human heart are important for the surgeons to locate the region where to perform the surgery and to avoid surgical injuries. In addition, such identification, segmentation, and visualisation helps novice surgeons in the training phase of cardiac surgery.},
	number = {1},
	urldate = {2021-10-02},
	journal = {Journal of Cardiothoracic Surgery},
	author = {Al-Surmi, Aqeel and Wirza, Rahmita and Mahmod, Ramlan and Khalid, Fatimah and Dimon, Mohd Zamrin},
	month = oct,
	year = {2014},
	keywords = {3D Model, Heart surgery, Image enhancement, Vessel segmentations},
	pages = {161},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6M95PRDH\\Al-Surmi et al. - 2014 - A new human heart vessel identification, segmentat.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9WPC3ABA\\s13019-014-0161-1.html:text/html},
}

@article{al_hajj_cataracts_2019,
	title = {{CATARACTS}: {Challenge} on automatic tool annotation for {cataRACT} surgery},
	volume = {52},
	issn = {1361-8415},
	shorttitle = {{CATARACTS}},
	url = {https://www.sciencedirect.com/science/article/pii/S136184151830865X},
	doi = {10.1016/j.media.2018.11.008},
	abstract = {Surgical tool detection is attracting increasing attention from the medical image analysis community. The goal generally is not to precisely locate tools in images, but rather to indicate which tools are being used by the surgeon at each instant. The main motivation for annotating tool usage is to design efficient solutions for surgical workflow analysis, with potential applications in report generation, surgical training and even real-time decision support. Most existing tool annotation algorithms focus on laparoscopic surgeries. However, with 19 million interventions per year, the most common surgical procedure in the world is cataract surgery. The CATARACTS challenge was organized in 2017 to evaluate tool annotation algorithms in the specific context of cataract surgery. It relies on more than nine hours of videos, from 50 cataract surgeries, in which the presence of 21 surgical tools was manually annotated by two experts. With 14 participating teams, this challenge can be considered a success. As might be expected, the submitted solutions are based on deep learning. This paper thoroughly evaluates these solutions: in particular, the quality of their annotations are compared to that of human interpretations. Next, lessons learnt from the differential analysis of these solutions are discussed. We expect that they will guide the design of efficient surgery monitoring tools in the near future.},
	language = {en},
	urldate = {2021-10-02},
	journal = {Medical Image Analysis},
	author = {Al Hajj, Hassan and Lamard, Mathieu and Conze, Pierre-Henri and Roychowdhury, Soumali and Hu, Xiaowei and Maršalkaitė, Gabija and Zisimopoulos, Odysseas and Dedmari, Muneer Ahmad and Zhao, Fenqiang and Prellberg, Jonas and Sahu, Manish and Galdran, Adrian and Araújo, Teresa and Vo, Duc My and Panda, Chandan and Dahiya, Navdeep and Kondo, Satoshi and Bian, Zhengbing and Vahdat, Arash and Bialopetravičius, Jonas and Flouty, Evangello and Qiu, Chenhui and Dill, Sabrina and Mukhopadhyay, Anirban and Costa, Pedro and Aresta, Guilherme and Ramamurthy, Senthil and Lee, Sang-Woong and Campilho, Aurélio and Zachow, Stefan and Xia, Shunren and Conjeti, Sailesh and Stoyanov, Danail and Armaitis, Jogundas and Heng, Pheng-Ann and Macready, William G. and Cochener, Béatrice and Quellec, Gwenolé},
	month = feb,
	year = {2019},
	keywords = {Deep learning, Cataract surgery, Challenge, Video analysis},
	pages = {24--41},
	file = {Eingereichte Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MNUZ3GC8\\Al Hajj et al. - 2019 - CATARACTS Challenge on automatic tool annotation .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2ELK88VH\\S136184151830865X.html:text/html},
}

@article{maier-hein_heidelberg_2021,
	title = {Heidelberg colorectal data set for surgical data science in the sensor operating room},
	volume = {8},
	copyright = {2021 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-021-00882-2},
	doi = {10.1038/s41597-021-00882-2},
	abstract = {Image-based tracking of medical instruments is an integral part of surgical data science applications. Previous research has addressed the tasks of detecting, segmenting and tracking medical instruments based on laparoscopic video data. However, the proposed methods still tend to fail when applied to challenging images and do not generalize well to data they have not been trained on. This paper introduces the Heidelberg Colorectal (HeiCo) data set - the first publicly available data set enabling comprehensive benchmarking of medical instrument detection and segmentation algorithms with a specific emphasis on method robustness and generalization capabilities. Our data set comprises 30 laparoscopic videos and corresponding sensor data from medical devices in the operating room for three different types of laparoscopic surgery. Annotations include surgical phase labels for all video frames as well as information on instrument presence and corresponding instance-wise segmentation masks for surgical instruments (if any) in more than 10,000 individual frames. The data has successfully been used to organize international competitions within the Endoscopic Vision Challenges 2017 and 2019.},
	language = {en},
	number = {1},
	urldate = {2021-10-02},
	journal = {Scientific Data},
	author = {Maier-Hein, Lena and Wagner, Martin and Ross, Tobias and Reinke, Annika and Bodenstedt, Sebastian and Full, Peter M. and Hempe, Hellena and Mindroc-Filimon, Diana and Scholz, Patrick and Tran, Thuy Nuong and Bruno, Pierangela and Kisilenko, Anna and Müller, Benjamin and Davitashvili, Tornike and Capek, Manuela and Tizabi, Minu D. and Eisenmann, Matthias and Adler, Tim J. and Gröhl, Janek and Schellenberg, Melanie and Seidlitz, Silvia and Lai, T. Y. Emmy and Pekdemir, Bünyamin and Roethlingshoefer, Veith and Both, Fabian and Bittel, Sebastian and Mengler, Marc and Mündermann, Lars and Apitz, Martin and Kopp-Schneider, Annette and Speidel, Stefanie and Nickel, Felix and Probst, Pascal and Kenngott, Hannes G. and Müller-Stich, Beat P.},
	month = apr,
	year = {2021},
	pages = {101},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5U7ICTWC\\Maier-Hein et al. - 2021 - Heidelberg colorectal data set for surgical data s.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\LGCFRUXV\\s41597-021-00882-2.html:text/html},
}

@article{fu_more_2019,
	title = {More unlabelled data or label more data? {A} study on semi-supervised laparoscopic image segmentation},
	shorttitle = {More unlabelled data or label more data?},
	url = {http://arxiv.org/abs/1908.08035},
	abstract = {Improving a semi-supervised image segmentation task has the option of adding more unlabelled images, labelling the unlabelled images or combining both, as neither image acquisition nor expert labelling can be considered trivial in most clinical applications. With a laparoscopic liver image segmentation application, we investigate the performance impact by altering the quantities of labelled and unlabelled training data, using a semi-supervised segmentation algorithm based on the mean teacher learning paradigm. We ﬁrst report a signiﬁcantly higher segmentation accuracy, compared with supervised learning. Interestingly, this comparison reveals that the training strategy adopted in the semisupervised algorithm is also responsible for this observed improvement, in addition to the added unlabelled data. We then compare diﬀerent combinations of labelled and unlabelled data set sizes for training semisupervised segmentation networks, to provide a quantitative example of the practically useful trade-oﬀ between the two data planning strategies in this surgical guidance application.},
	language = {en},
	urldate = {2021-10-02},
	journal = {arXiv:1908.08035 [cs, eess, stat]},
	author = {Fu, Yunguan and Robu, Maria R. and Koo, Bongjin and Schneider, Crispin and van Laarhoven, Stijn and Stoyanov, Danail and Davidson, Brian and Clarkson, Matthew J. and Hu, Yipeng},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.08035},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Fu et al. - 2019 - More unlabelled data or label more data A study o.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\HQVEKZS4\\Fu et al. - 2019 - More unlabelled data or label more data A study o.pdf:application/pdf},
}

@article{allan_2018_2020,
	title = {2018 {Robotic} {Scene} {Segmentation} {Challenge}},
	url = {http://arxiv.org/abs/2001.11190},
	abstract = {In 2015 we began a sub-challenge at the EndoVis workshop at MICCAI in Munich using endoscope images of exvivo tissue with automatically generated annotations from robot forward kinematics and instrument CAD models. However, the limited background variation and simple motion rendered the dataset uninformative in learning about which techniques would be suitable for segmentation in real surgery. In 2017, at the same workshop in Quebec we introduced the robotic instrument segmentation dataset with 10 teams participating in the challenge to perform binary, articulating parts and type segmentation of da Vinci R instruments. This challenge included realistic instrument motion and more complex porcine tissue as background and was widely addressed with modiﬁcations on U-Nets and other popular CNN architectures [1].},
	language = {en},
	urldate = {2021-10-02},
	journal = {arXiv:2001.11190 [cs]},
	author = {Allan, Max and Kondo, Satoshi and Bodenstedt, Sebastian and Leger, Stefan and Kadkhodamohammadi, Rahim and Luengo, Imanol and Fuentes, Felix and Flouty, Evangello and Mohammed, Ahmed and Pedersen, Marius and Kori, Avinash and Alex, Varghese and Krishnamurthi, Ganapathy and Rauber, David and Mendel, Robert and Palm, Christoph and Bano, Sophia and Saibro, Guinther and Shih, Chi-Sheng and Chiang, Hsun-An and Zhuang, Juntang and Yang, Junlin and Iglovikov, Vladimir and Dobrenkii, Anton and Reddiboina, Madhu and Reddy, Anubhav and Liu, Xingtong and Gao, Cong and Unberath, Mathias and Kim, Myeonghyeon and Kim, Chanho and Kim, Chaewon and Kim, Hyejin and Lee, Gyeongmin and Ullah, Ihsan and Luna, Miguel and Park, Sang Hyun and Azizian, Mahdi and Stoyanov, Danail and Maier-Hein, Lena and Speidel, Stefanie},
	month = aug,
	year = {2020},
	note = {arXiv: 2001.11190},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {Allan et al. - 2020 - 2018 Robotic Scene Segmentation Challenge.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3IY9L8BR\\Allan et al. - 2020 - 2018 Robotic Scene Segmentation Challenge.pdf:application/pdf},
}

@article{he_mask_2018,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	urldate = {2021-10-02},
	journal = {arXiv:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = jan,
	year = {2018},
	note = {arXiv: 1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VALEMN8C\\He et al. - 2018 - Mask R-CNN.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\E6EWJ5GT\\1703.html:text/html},
}

@misc{noauthor_deep_nodate,
	title = {Deep learning classifiers for hyperspectral imaging\_ {A} review {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0924271619302187?token=612AAF71920A49F41FD8BBD91E948B5C15872E624BC9DB4E6B6C32D6F4E74A3D0044472AAAD16153C80FCF4F04771D26&originRegion=eu-west-1&originCreation=20211003132404},
	language = {en},
	urldate = {2021-10-03},
	doi = {10.1016/j.isprsjprs.2019.09.006},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\D2V5FJCR\\S0924271619302187.html:text/html},
}

@incollection{liu_multiple_2020,
	address = {Cham},
	series = {Advances in {Computer} {Vision} and {Pattern} {Recognition}},
	title = {Multiple {Kernel} {Learning} for {Hyperspectral} {Image} {Classification}},
	isbn = {978-3-030-38617-7},
	url = {https://doi.org/10.1007/978-3-030-38617-7_9},
	abstract = {With the rapid development of spectral imaging techniques, classification of hyperspectral images (HSIs) has attracted great attention in various applications such as land survey and resource monitoring in the field of remote sensing. A key challenge in HSI classification is how to explore effective approaches to fully use the spatial–spectral information provided by the data cube. Multiple Kernel Learning (MKL) has been successfully applied to HSI classification due to its capacity to handle heterogeneous fusion of both spectral and spatial features. This approach can generate an adaptive kernel as an optimally weighted sum of a few fixed kernels to model a nonlinear data structure. In this way, the difficulty of kernel selection and the limitation of a fixed kernel can be alleviated. Various MKL algorithms have been developed in recent years, such as the general MKL, the subspace MKL, the nonlinear MKL, the sparse MKL, and the ensemble MKL. The goal of this chapter is to provide a systematic review of MKL methods, which have been applied to HSI classification. We also analyze and evaluate different MKL algorithms and their respective characteristics in different cases of HSI classification cases. Finally, we discuss the future direction and trends of research in this area.},
	language = {en},
	urldate = {2021-10-03},
	booktitle = {Hyperspectral {Image} {Analysis}: {Advances} in {Machine} {Learning} and {Signal} {Processing}},
	publisher = {Springer International Publishing},
	author = {Liu, Tianzhu and Gu, Yanfeng},
	editor = {Prasad, Saurabh and Chanussot, Jocelyn},
	year = {2020},
	doi = {10.1007/978-3-030-38617-7_9},
	keywords = {Classification, Remote sensing, Heterogeneous features, Hyperspectral images, Multiple kernel learning (MKL)},
	pages = {259--293},
}

@incollection{moreno-martinez_machine_2020,
	address = {Cham},
	series = {Advances in {Computer} {Vision} and {Pattern} {Recognition}},
	title = {Machine {Learning} {Methods} for {Spatial} and {Temporal} {Parameter} {Estimation}},
	isbn = {978-3-030-38617-7},
	url = {https://doi.org/10.1007/978-3-030-38617-7_2},
	abstract = {Monitoring vegetation with satellite remote sensing is of paramount relevance to understand the status and health of our planet. Accurate and constant monitoring of the biosphere has large societal, economical, and environmental implications, given the increasing demand of biofuels and food by the world population. The current democratization of machine learning, big data, and high processing capabilities allow us to take such endeavor in a decisive manner. This chapter proposes three novel machine learning approaches to exploit spatial, temporal, multi-sensor, and large-scale data characteristics. We show (1) the application of multi-output Gaussian processes for gap-filling time series of soil moisture retrievals from three spaceborne sensors; (2) a new kernel distribution regression model that exploits multiple observations and higher order relations to estimate county-level crop yield from time series of vegetation optical depth; and finally (3) we show the combination of radiative transfer models with random forests to estimate leaf area index, fraction of absorbed photosynthetically active radiation, fraction vegetation cover, and canopy water content at global scale from long-term time series of multispectral data exploiting the Google Earth Engine cloud processing capabilities. The approaches demonstrate that machine learning algorithms can ingest and process multi-sensor data and provide accurate estimates of key parameters for vegetation monitoring.},
	language = {en},
	urldate = {2021-10-03},
	booktitle = {Hyperspectral {Image} {Analysis}: {Advances} in {Machine} {Learning} and {Signal} {Processing}},
	publisher = {Springer International Publishing},
	author = {Moreno-Martínez, Álvaro and Piles, María and Muñoz-Marí, Jordi and Campos-Taberner, Manuel and Adsuara, Jose E. and Mateo, Anna and Perez-Suay, Adrián and Javier García-Haro, Francisco and Camps-Valls, Gustau},
	editor = {Prasad, Saurabh and Chanussot, Jocelyn},
	year = {2020},
	doi = {10.1007/978-3-030-38617-7_2},
	pages = {5--35},
}

@book{prasad_hyperspectral_2020,
	address = {Cham},
	series = {Advances in {Computer} {Vision} and {Pattern} {Recognition}},
	title = {Hyperspectral {Image} {Analysis}: {Advances} in {Machine} {Learning} and {Signal} {Processing}},
	isbn = {978-3-030-38616-0 978-3-030-38617-7},
	shorttitle = {Hyperspectral {Image} {Analysis}},
	url = {http://link.springer.com/10.1007/978-3-030-38617-7},
	language = {en},
	urldate = {2021-10-03},
	publisher = {Springer International Publishing},
	editor = {Prasad, Saurabh and Chanussot, Jocelyn},
	year = {2020},
	doi = {10.1007/978-3-030-38617-7},
	file = {Prasad und Chanussot - 2020 - Hyperspectral Image Analysis Advances in Machine .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VDBTP4DD\\Prasad und Chanussot - 2020 - Hyperspectral Image Analysis Advances in Machine .pdf:application/pdf},
}

@article{ivantsits_dl-based_2020,
	title = {{DL}-based segmentation of endoscopic scenes for mitral valve repair},
	volume = {6},
	issn = {2364-5504},
	url = {https://www.degruyter.com/document/doi/10.1515/cdbme-2020-0017/html},
	doi = {10.1515/cdbme-2020-0017},
	abstract = {Minimally invasive surgery is increasingly utilized for mitral valve repair and replacement. The intervention is performed with an endoscopic field of view on the arrested heart. Extracting the necessary information from the live endoscopic video stream is challenging due to the moving camera position, the high variability of defects, and occlusion of structures by instruments. During such minimally invasive interventions there is no time to segment regions of interest manually. We propose a real-time-capable deep-learning-based approach to detect and segment the relevant anatomical structures and instruments. For the universal deployment of the proposed solution, we evaluate them on pixel accuracy as well as distance measurements of the detected contours. The U-Net, Google’s DeepLab v3, and the Obelisk-Net models are cross-validated, with DeepLab showing superior results in pixel accuracy and distance measurements.},
	language = {en},
	number = {1},
	urldate = {2021-10-03},
	journal = {Current Directions in Biomedical Engineering},
	author = {Ivantsits, Matthias and Tautz, Lennart and Sündermann, Simon and Wamala, Isaac and Kempfert, Jörg and Kuehne, Titus and Falk, Volkmar and Hennemuth, Anja},
	month = may,
	year = {2020},
	note = {Publisher: De Gruyter},
	keywords = {surgery, machine learning, deep learning, segmentation, detection, endoscopic, mitral valve, mitral valve leaflet},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MICTUZPA\\Ivantsits et al. - 2020 - DL-based segmentation of endoscopic scenes for mit.pdf:application/pdf},
}

@article{holland-letz_drawing_2020,
	title = {Drawing statistical conclusions from experiments with multiple quantitative measurements per subject},
	volume = {152},
	issn = {1879-0887},
	doi = {10.1016/j.radonc.2020.08.009},
	abstract = {In experiments with multiple quantitative measurements per subject, for example measurements on multiple lesions per patient, the additional measurements on the same patient provide limited additional information. Treating these measurements as independent observations will produce biased estimators for standard deviations and confidence intervals, and increases the risk of false positives in statistical tests. The problem can be remedied in a simple way by first taking the average of all observations of each specific patient, and then doing all further calculations only on the list of these patient means. A more sophisticated statistical modeling of the experiment, for example in a linear mixed model, is only required if (i) there is a large imbalance in the number of observations per patient or (ii) there is a specific interest in actually identifying the various sources of variation in the experiment.},
	language = {eng},
	journal = {Radiotherapy and Oncology: Journal of the European Society for Therapeutic Radiology and Oncology},
	author = {Holland-Letz, Tim and Kopp-Schneider, Annette},
	month = nov,
	year = {2020},
	pmid = {32828840},
	keywords = {Humans, Biological replicates, Biostatistics, Correlated observations, Linear mixed models, Linear Models, Models, Statistical, Technical replicates},
	pages = {30--33},
}

@article{joskowicz_inter-observer_2019,
	title = {Inter-observer variability of manual contour delineation of structures in {CT}},
	volume = {29},
	issn = {1432-1084},
	url = {https://doi.org/10.1007/s00330-018-5695-5},
	doi = {10.1007/s00330-018-5695-5},
	abstract = {To quantify the inter-observer variability of manual delineation of lesions and organ contours in CT to establish a reference standard for volumetric measurements for clinical decision making and for the evaluation of automatic segmentation algorithms.},
	language = {en},
	number = {3},
	urldate = {2021-10-03},
	journal = {European Radiology},
	author = {Joskowicz, Leo and Cohen, D. and Caplan, N. and Sosna, J.},
	month = mar,
	year = {2019},
	pages = {1391--1399},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8IWVQQZR\\Joskowicz et al. - 2019 - Inter-observer variability of manual contour delin.pdf:application/pdf},
}

@article{trajanovski_tongue_2021-1,
	title = {Tongue {Tumor} {Detection} in {Hyperspectral} {Images} {Using} {Deep} {Learning} {Semantic} {Segmentation}},
	volume = {68},
	issn = {0018-9294, 1558-2531},
	url = {https://ieeexplore.ieee.org/document/9206125/},
	doi = {10.1109/TBME.2020.3026683},
	abstract = {Objective: The utilization of hyperspectral imaging (HSI) in real-time tumor segmentation during a surgery have recently received much attention, but it remains a very challenging task. Methods: In this work, we propose semantic segmentation methods, and compare them with other relevant deep learning algorithms for tongue tumor segmentation. To the best of our knowledge, this is the ﬁrst work using deep learning semantic segmentation for tumor detection in HSI data using channel selection, and accounting for more spatial tissue context, and global comparison between the prediction map, and the annotation per sample. Results, and Conclusion: On a clinical data set with tongue squamous cell carcinoma, our best method obtains very strong results of average dice coefﬁcient, and area under the ROC-curve of 0.891 ± 0.053, and 0.924 ± 0.036, respectively on the original spatial image size. The results show that a very good performance can be achieved even with a limited amount of data. We demonstrate that important information regarding tumor decision is encoded in various channels, but some channel selection, and ﬁltering is beneﬁcial over the full spectra. Moreover, we use both visual (VIS), and near-infrared (NIR) spectrum, rather than commonly used only VIS spectrum; although VIS spectrum is generally of higher signiﬁcance, we demonstrate NIR spectrum is crucial for tumor capturing in some cases. Signiﬁcance: The HSI technology augmented with accurate deep learning algorithms has a huge potential to be a promising alternative to digital pathology or a doctors’ supportive tool in real-time surgeries.},
	language = {en},
	number = {4},
	urldate = {2021-10-04},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Trajanovski, Stojan and Shan, Caifeng and Weijtmans, Pim J. C. and de Koning, Susan G. Brouwer and Ruers, Theo J. M.},
	month = apr,
	year = {2021},
	pages = {1330--1340},
}

@article{antonelli_medical_2021,
	title = {The {Medical} {Segmentation} {Decathlon}},
	url = {http://arxiv.org/abs/2106.05735},
	abstract = {International challenges have become the de facto standard for comparative assessment of image analysis algorithms given a speciﬁc task. Segmentation is so far the most widely investigated medical image processing task, but the various segmentation challenges have typically been organized in isolation, such that algorithm development was driven by the need to tackle a single speciﬁc clinical problem. We hypothesized that a method capable of performing well on multiple tasks will generalize well to a previously unseen task and potentially outperform a custom-designed solution. To investigate the hypothesis, we organized the Medical Segmentation Decathlon (MSD)—a biomedical image analysis challenge, in which algorithms compete in a multitude of both tasks and modalities. The underlying data set was designed to explore the axis of diﬃculties typically encountered when dealing with medical images, such as small data sets, unbalanced labels, multi-site data and small objects. The MSD challenge conﬁrmed that algorithms with a consistent good performance on a set of tasks preserved their good average performance on a diﬀerent set of previously unseen tasks. Moreover, by monitoring the MSD winner for two years, we found that this algorithm continued generalizing well to a wide range of other clinical problems, further conﬁrming our hypothesis. Three main conclusions can be drawn from this study: (1) state-of-the-art image segmentation algorithms are mature, accurate, and generalize well when retrained on unseen tasks; (2) consistent algorithmic performance across multiple tasks is a strong surrogate of algorithmic generalizability; (3) the training of accurate AI segmentation models is now commoditized to non AI experts.},
	language = {en},
	urldate = {2021-10-05},
	journal = {arXiv:2106.05735 [cs, eess]},
	author = {Antonelli, Michela and Reinke, Annika and Bakas, Spyridon and Farahani, Keyvan and AnnetteKopp-Schneider and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and van Ginneken, Bram and Bilello, Michel and Bilic, Patrick and Christ, Patrick F. and Do, Richard K. G. and Gollub, Marc J. and Heckers, Stephan H. and Huisman, Henkjan and Jarnagin, William R. and McHugo, Maureen K. and Napel, Sandy and Pernicka, Jennifer S. Goli and Rhode, Kawal and Tobon-Gomez, Catalina and Vorontsov, Eugene and Huisman, Henkjan and Meakin, James A. and Ourselin, Sebastien and Wiesenfarth, Manuel and Arbelaez, Pablo and Bae, Byeonguk and Chen, Sihong and Daza, Laura and Feng, Jianjiang and He, Baochun and Isensee, Fabian and Ji, Yuanfeng and Jia, Fucang and Kim, Namkug and Kim, Ildoo and Merhof, Dorit and Pai, Akshay and Park, Beomhee and Perslev, Mathias and Rezaiifar, Ramin and Rippel, Oliver and Sarasua, Ignacio and Shen, Wei and Son, Jaemin and Wachinger, Christian and Wang, Liansheng and Wang, Yan and Xia, Yingda and Xu, Daguang and Xu, Zhanwei and Zheng, Yefeng and Simpson, Amber L. and Maier-Hein, Lena and Cardoso, M. Jorge},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.05735},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Computer Vision and Pattern Recognition, 68T07},
	file = {Antonelli et al. - 2021 - The Medical Segmentation Decathlon.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\GT6Q3QBM\\Antonelli et al. - 2021 - The Medical Segmentation Decathlon.pdf:application/pdf},
}

@article{laves_dataset_2019,
	title = {A dataset of laryngeal endoscopic images with comparative study on convolution neural network-based semantic segmentation},
	volume = {14},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-018-01910-0},
	doi = {10.1007/s11548-018-01910-0},
	abstract = {Automated segmentation of anatomical structures in medical image analysis is a prerequisite for autonomous diagnosis as well as various computer- and robot-aided interventions. Recent methods based on deep convolutional neural networks (CNN) have outperformed former heuristic methods. However, those methods were primarily evaluated on rigid, real-world environments. In this study, existing segmentation methods were evaluated for their use on a new dataset of transoral endoscopic exploration.},
	language = {en},
	number = {3},
	urldate = {2021-10-07},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Laves, Max-Heinrich and Bicker, Jens and Kahrs, Lüder A. and Ortmaier, Tobias},
	month = mar,
	year = {2019},
	pages = {483--492},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\73H9RHUV\\Laves et al. - 2019 - A dataset of laryngeal endoscopic images with comp.pdf:application/pdf},
}

@article{buslaev_albumentations_2020,
	title = {Albumentations: {Fast} and {Flexible} {Image} {Augmentations}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Albumentations},
	url = {https://www.mdpi.com/2078-2489/11/2/125},
	doi = {10.3390/info11020125},
	abstract = {Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve corresponding output labels. In computer vision, image augmentations have become a common implicit regularization technique to combat overfitting in deep learning models and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations of flipping, rotating, scaling, and cropping. Moreover, image processing speed varies in existing image augmentation libraries. We present Albumentations, a fast and flexible open source library for image augmentation with many various image transform operations available that is also an easy-to-use wrapper around other augmentation libraries. We discuss the design principles that drove the implementation of Albumentations and give an overview of the key features and distinct capabilities. Finally, we provide examples of image augmentations for different computer vision tasks and demonstrate that Albumentations is faster than other commonly used image augmentation tools on most image transform operations.},
	language = {en},
	number = {2},
	urldate = {2021-10-28},
	journal = {Information},
	author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {computer vision, deep learning, data augmentation},
	pages = {125},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5WVFIMGK\\Buslaev et al. - 2020 - Albumentations Fast and Flexible Image Augmentati.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SKC2P4KY\\125.html:text/html},
}

@article{zhang_tissue_2017,
	title = {Tissue classification for laparoscopic image understanding based on multispectral texture analysis},
	volume = {4},
	issn = {2329-4302},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5265243/},
	doi = {10.1117/1.JMI.4.1.015001},
	abstract = {Intraoperative tissue classification is one of the prerequisites for providing context-aware visualization in computer-assisted minimally invasive surgeries. As many anatomical structures are difficult to differentiate in conventional RGB medical images, we propose a classification method based on multispectral image patches. In a comprehensive ex vivo study through statistical analysis, we show that (1) multispectral imaging data are superior to RGB data for organ tissue classification when used in conjunction with widely applied feature descriptors and (2) combining the tissue texture with the reflectance spectrum improves the classification performance. The classifier reaches an accuracy of 98.4\% on our dataset. Multispectral tissue analysis could thus evolve as a key enabling technique in computer-assisted laparoscopy.},
	number = {1},
	urldate = {2021-10-28},
	journal = {Journal of Medical Imaging},
	author = {Zhang, Yan and Wirkert, Sebastian J. and Iszatt, Justin and Kenngott, Hannes and Wagner, Martin and Mayer, Benjamin and Stock, Christian and Clancy, Neil T. and Elson, Daniel S. and Maier-Hein, Lena},
	month = jan,
	year = {2017},
	pmid = {28149926},
	pmcid = {PMC5265243},
	pages = {015001},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VM8NZI4C\\Zhang et al. - 2017 - Tissue classification for laparoscopic image under.pdf:application/pdf},
}

@article{maier-hein_surgical_2021,
	title = {Surgical {Data} {Science} -- from {Concepts} toward {Clinical} {Translation}},
	url = {http://arxiv.org/abs/2011.02284},
	abstract = {Recent developments in data science in general and machine learning in particular have transformed the way experts envision the future of surgery. Surgical Data Science (SDS) is a new research ﬁeld that aims to improve the quality of interventional healthcare through the capture, organization, analysis and modeling of data. While an increasing number of data-driven approaches and clinical applications have been studied in the ﬁelds of radiological and clinical data science, translational success stories are still lacking in surgery. In this publication, we shed light on the underlying reasons and provide a roadmap for future advances in the ﬁeld. Based on an international workshop involving leading researchers in the ﬁeld of SDS, we review current practice, key achievements and initiatives as well as available standards and tools for a number of topics relevant to the ﬁeld, namely (1) infrastructure for data acquisition, storage and access in the presence of regulatory constraints, (2) data annotation and sharing and (3) data analytics. We further complement this technical perspective with (4) a review of currently available SDS products and the translational progress from academia and (5) a roadmap for faster clinical translation and exploitation of the full potential of SDS, based on an international multi-round Delphi process.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:2011.02284 [cs, eess]},
	author = {Maier-Hein, Lena and Eisenmann, Matthias and Sarikaya, Duygu and März, Keno and Collins, Toby and Malpani, Anand and Fallert, Johannes and Feussner, Hubertus and Giannarou, Stamatia and Mascagni, Pietro and Nakawala, Hirenkumar and Park, Adrian and Pugh, Carla and Stoyanov, Danail and Vedula, Swaroop S. and Cleary, Kevin and Fichtinger, Gabor and Forestier, Germain and Gibaud, Bernard and Grantcharov, Teodor and Hashizume, Makoto and Heckmann-Nötzel, Doreen and Kenngott, Hannes G. and Kikinis, Ron and Mündermann, Lars and Navab, Nassir and Onogur, Sinan and Sznitman, Raphael and Taylor, Russell H. and Tizabi, Minu D. and Wagner, Martin and Hager, Gregory D. and Neumuth, Thomas and Padoy, Nicolas and Collins, Justin and Gockel, Ines and Goedeke, Jan and Hashimoto, Daniel A. and Joyeux, Luc and Lam, Kyle and Leff, Daniel R. and Madani, Amin and Marcus, Hani J. and Meireles, Ozanan and Seitel, Alexander and Teber, Dogu and Ückert, Frank and Müller-Stich, Beat P. and Jannin, Pierre and Speidel, Stefanie},
	month = jul,
	year = {2021},
	note = {arXiv: 2011.02284},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society},
	file = {2011.02284.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MFQPMB5M\\2011.02284.pdf:application/pdf},
}

@misc{noauthor_reproducibility_nodate,
	title = {Reproducibility — {PyTorch} 1.10.0 documentation},
	url = {https://pytorch.org/docs/stable/notes/randomness.html},
	urldate = {2021-10-28},
	file = {Reproducibility — PyTorch 1.10.0 documentation:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FHIV4DXR\\randomness.html:text/html},
}

@book{haykin_neural_1994,
	title = {Neural {Networks}: {A} {Comprehensive} {Foundation}},
	publisher = {Prentice Hall PTR},
	author = {Haykin, Simon},
	year = {1994},
	file = {71836187-with-cover-page-v2.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JY2Z9NJ7\\71836187-with-cover-page-v2.pdf:application/pdf},
}

@article{cox_regression_1958,
	title = {The {Regression} {Analysis} of {Binary} {Sequences}},
	volume = {20},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2983890},
	abstract = {A sequence of 0's and 1's is observed and it is suspected that the chance that a particular trial is a 1 depends on the value of one or more independent variables. Tests and estimates for such situations are considered, dealing first with problems in which the independent variable is preassigned and then with independent variables that are functions of the sequence. There is a considerable amount of earlier work, which is reviewed.},
	number = {2},
	urldate = {2021-10-28},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Cox, D. R.},
	year = {1958},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {215--242},
}

@misc{yakubovskiy_segmentation_2020,
	title = {Segmentation {Models} {Pytorch}},
	url = {https://github.com/qubvel/segmentation_models.pytorch},
	publisher = {GitHub},
	author = {Yakubovskiy, Pavel},
	year = {2020},
	note = {Publication Title: GitHub repository},
}

@misc{consortium_monai_2020,
	title = {{MONAI}: {Medical} {Open} {Network} for {AI}},
	shorttitle = {{MONAI}},
	url = {https://zenodo.org/record/5525502},
	abstract = {AI Toolkit for Healthcare Imaging},
	urldate = {2021-11-03},
	publisher = {Zenodo},
	author = {Consortium, MONAI},
	month = mar,
	year = {2020},
	doi = {10.5281/zenodo.5525502},
	file = {Zenodo Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QRIV9VGG\\5525502.html:text/html},
}

@inproceedings{webster_deep_2017-1,
	address = {Orlando, Florida, United States},
	title = {Deep residual networks for automatic segmentation of laparoscopic videos of the liver},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2255975},
	doi = {10.1117/12.2255975},
	abstract = {Method: We defined a CNN architecture comprising fully-convolutional deep residual networks with multi-resolution loss functions. The CNN was trained in a leave-one-patient-out cross-validation on 2050 video frames from 6 liver resections and 7 laparoscopic staging procedures, and evaluated using the Dice score.
Results: The CNN yielded segmentations with Dice scores ≥0.95 for the majority of images; however, the inter-patient variability in median Dice score was substantial. Four failure modes were identified from low scoring segmentations: minimal visible liver tissue, inter-patient variability in liver appearance, automatic exposure correction, and pathological liver tissue that mimics non-liver tissue appearance.
Conclusion: CNNs offer a feasible approach for accurately segmenting liver from other anatomy on laparoscopic video, but additional data or computational advances are necessary to address challenges due to the high inter-patient variability in liver appearance.},
	language = {en},
	urldate = {2021-11-04},
	author = {Gibson, Eli and Robu, Maria R. and Thompson, Stephen and Edwards, P. Eddie and Schneider, Crispin and Gurusamy, Kurinchi and Davidson, Brian and Hawkes, David J. and Barratt, Dean C. and Clarkson, Matthew J.},
	editor = {Webster, Robert J. and Fei, Baowei},
	month = mar,
	year = {2017},
	pages = {101351M},
	file = {Gibson et al. - 2017 - Deep residual networks for automatic segmentation .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CREQN5F4\\Gibson et al. - 2017 - Deep residual networks for automatic segmentation .pdf:application/pdf},
}

@article{rivas-blanco_review_2021-1,
	title = {A {Review} on {Deep} {Learning} in {Minimally} {Invasive} {Surgery}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3068852},
	abstract = {In the last five years, deep learning has attracted great interest in computer-assisted systems for Minimally Invasive Surgery. The straightforward accessibility to images in surgical interventions makes deep neural networks enormously powerful for solving classification problems in complex surgical scenarios. The objective of this work is to provide readers a survey on deep learning models applied to minimally invasive surgery, identifying the different architectures used depending on the application, the results achieved until now, and the publicly available surgical datasets that can be used for validating new studies. A total of 85 publications have been extracted from manual research from four databases (IEEE Xplorer, Springer Link, Science Direct, and ACM Digital Library). After analyzing all these studies, they have been classified into four applications: surgical image analysis, surgical task analysis, surgical skill assessment, and automation of surgical tasks. This work provides a technical description of these works and a comparison among them. Finally, promising research directions to advance in this field are identified.},
	journal = {IEEE Access},
	author = {Rivas-Blanco, Irene and Pérez-Del-Pulgar, Carlos J. and García-Morales, Isabel and Muñoz, Víctor F.},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Minimally invasive surgery, Feature extraction, Deep learning, minimally invasive surgery, Data models, Task analysis, Libraries, convolutional neural network, deep neural network, Hidden Markov models, laparoscopic surgery, robot-assisted surgery},
	pages = {48658--48678},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FCRKK6YR\\9386091.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YBFR5NBH\\Rivas-Blanco et al. - 2021 - A Review on Deep Learning in Minimally Invasive Su.pdf:application/pdf},
}

@misc{noauthor_segmenting_nodate,
	title = {Segmenting the {Uterus} in {Monocular} {Laparoscopic} {Images} without {Manual} {Input}},
	url = {https://www.springerprofessional.de/en/segmenting-the-uterus-in-monocular-laparoscopic-images-without-m/2544760},
	abstract = {Automatically segmenting organs in monocular laparoscopic images is an important and challenging research objective in computer-assisted intervention. For the uterus this is difficult because of high inter-patient variability in tissue appearance and},
	language = {en},
	urldate = {2021-11-04},
	journal = {springerprofessional.de},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WRQVVZWR\\2544760.html:text/html},
}

@inproceedings{collins_segmenting_2015,
	address = {Cham},
	title = {Segmenting the {Uterus} in {Monocular} {Laparoscopic} {Images} without {Manual} {Input}},
	url = {https://doi.org/10.1007/978-3-319-24574-4_22},
	abstract = {Automatically segmenting organs in monocular laparoscopic images is an important and challenging research objective in computer-assisted intervention. For the uterus this is difficult because of high inter-patient variability in tissue appearance and},
	language = {en},
	urldate = {2021-11-04},
	publisher = {Springer},
	author = {Collins, Toby and Bartoli, Adrien and Bourdel, Nicolas and Canis, Michel},
	year = {2015},
	file = {Collins et al. - 2015 - Segmenting the Uterus in Monocular Laparoscopic Im.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\R3ZNWW3N\\Collins et al. - 2015 - Segmenting the Uterus in Monocular Laparoscopic Im.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\C9YE7AY3\\2544760.html:text/html},
}

@incollection{fei_chapter_2020,
	series = {Hyperspectral {Imaging}},
	title = {Chapter 3.6 - {Hyperspectral} imaging in medical applications},
	volume = {32},
	url = {https://www.sciencedirect.com/science/article/pii/B9780444639776000213},
	abstract = {Hyperspectral imaging (HSI) is an emerging imaging modality for medical applications, especially in disease diagnosis and image-guided surgery. HSI acquires three-dimensional data set called hypercube with two spatial dimensions and one spectral dimension. Spatially resolved spectral obtained by HSI provides diagnostic information about the tissue physiology, morphology, and composition. This chapter presents an overview of literature on medical hyperspectral imaging (MHSI) technology and its applications. The aim of this chapter is threefold: an introduction for those new to the field of MHSI, an overview for those working in the field, and a reference for those searching for literature on a specific application. This chapter starts to review the basic mechanisms of MHSI and then classify MHSI technology based on acquisition mode, spectral range and spatial resolution, measurement mode, dispersive devices, detector arrays, and combination with other techniques. Image analysis methods for MHSI are summarized with preprocessing, feature extraction and selection, image classification methods. The part on applications is a reference of the literature available on disease diagnosis and surgical guidance. MHSI applications on cancer detection, cardiology, pathology, retinal imaging, diabetic foot, shock, tissue pathology, mastectomy, gallbladder surgery, renal surgery, and abdominal surgery are reviewed. This chapter closes with a discussion on future challenges and perspectives.},
	language = {en},
	urldate = {2021-11-07},
	booktitle = {Data {Handling} in {Science} and {Technology}},
	publisher = {Elsevier},
	author = {Fei, Baowei},
	editor = {Amigo, José Manuel},
	month = jan,
	year = {2020},
	doi = {10.1016/B978-0-444-63977-6.00021-3},
	keywords = {Cancer detection and diagnosis, Image analysis and classification, Image-guided surgery, Medical hyperspectral imaging, Quantitative imaging, Tissue optics},
	pages = {523--565},
	file = {ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\E6LDWJCG\\B9780444639776000213.html:text/html},
}

@article{aix-covnet_common_2021-1,
	title = {Common pitfalls and recommendations for using machine learning to detect and prognosticate for {COVID}-19 using chest radiographs and {CT} scans},
	volume = {3},
	issn = {2522-5839},
	url = {http://www.nature.com/articles/s42256-021-00307-0},
	doi = {10.1038/s42256-021-00307-0},
	abstract = {Abstract
            Machine learning methods offer great promise for fast and accurate detection and prognostication of coronavirus disease 2019 (COVID-19) from standard-of-care chest radiographs (CXR) and chest computed tomography (CT) images. Many articles have been published in 2020 describing new machine learning-based models for both of these tasks, but it is unclear which are of potential clinical utility. In this systematic review, we consider all published papers and preprints, for the period from 1 January 2020 to 3 October 2020, which describe new machine learning models for the diagnosis or prognosis of COVID-19 from CXR or CT images. All manuscripts uploaded to bioRxiv, medRxiv and arXiv along with all entries in EMBASE and MEDLINE in this timeframe are considered. Our search identified 2,212 studies, of which 415 were included after initial screening and, after quality screening, 62 studies were included in this systematic review. Our review finds that none of the models identified are of potential clinical use due to methodological flaws and/or underlying biases. This is a major weakness, given the urgency with which validated COVID-19 models are needed. To address this, we give many recommendations which, if followed, will solve these issues and lead to higher-quality model development and well-documented manuscripts.},
	language = {en},
	number = {3},
	urldate = {2022-01-09},
	journal = {Nature Machine Intelligence},
	author = {{AIX-COVNET} and Roberts, Michael and Driggs, Derek and Thorpe, Matthew and Gilbey, Julian and Yeung, Michael and Ursprung, Stephan and Aviles-Rivero, Angelica I. and Etmann, Christian and McCague, Cathal and Beer, Lucian and Weir-McCall, Jonathan R. and Teng, Zhongzhao and Gkrania-Klotsas, Effrossyni and Rudd, James H. F. and Sala, Evis and Schönlieb, Carola-Bibiane},
	month = mar,
	year = {2021},
	pages = {199--217},
}

@article{sutton_meeting_2019,
	title = {Meeting the review family: exploring review types and associated information retrieval requirements},
	volume = {36},
	issn = {1471-1834, 1471-1842},
	shorttitle = {Meeting the review family},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/hir.12276},
	doi = {10.1111/hir.12276},
	language = {en},
	number = {3},
	urldate = {2022-01-11},
	journal = {Health Information \& Libraries Journal},
	author = {Sutton, Anthea and Clowes, Mark and Preston, Louise and Booth, Andrew},
	month = sep,
	year = {2019},
	keywords = {review paper typology},
	pages = {202--222},
	file = {Accepted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ZIFK3XVS\\Sutton et al. - 2019 - Meeting the review family exploring review types .pdf:application/pdf;Sutton et al. - 2019 - Meeting the review family exploring review types .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\G5ESX4IH\\Sutton et al. - 2019 - Meeting the review family exploring review types .pdf:application/pdf},
}

@article{pare_synthesizing_2015,
	title = {Synthesizing information systems knowledge: {A} typology of literature reviews},
	volume = {52},
	issn = {03787206},
	shorttitle = {Synthesizing information systems knowledge},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378720614001116},
	doi = {10.1016/j.im.2014.08.008},
	language = {en},
	number = {2},
	urldate = {2022-01-12},
	journal = {Information \& Management},
	author = {Paré, Guy and Trudel, Marie-Claude and Jaana, Mirou and Kitsiou, Spyros},
	month = mar,
	year = {2015},
	keywords = {review paper typology},
	pages = {183--199},
	file = {Paré et al. - 2015 - Synthesizing information systems knowledge A typo.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ESEZE4RV\\Paré et al. - 2015 - Synthesizing information systems knowledge A typo.pdf:application/pdf},
}

@article{ortega_hyperspectral_2020,
	title = {Hyperspectral and multispectral imaging in digital and computational pathology: a systematic review [{Invited}]},
	volume = {11},
	copyright = {\&\#169; 2020 Optical Society of America},
	issn = {2156-7085},
	shorttitle = {Hyperspectral and multispectral imaging in digital and computational pathology},
	url = {https://www.osapublishing.org/boe/abstract.cfm?uri=boe-11-6-3195},
	doi = {10.1364/BOE.386338},
	abstract = {Hyperspectral imaging (HSI) and multispectral imaging (MSI) technologies have the potential to transform the fields of digital and computational pathology. Traditional digitized histopathological slides are imaged with RGB imaging. Utilizing HSI/MSI, spectral information across wavelengths within and beyond the visual range can complement spatial information for the creation of computer-aided diagnostic tools for both stained and unstained histological specimens. In this systematic review, we summarize the methods and uses of HSI/MSI for staining and color correction, immunohistochemistry, autofluorescence, and histopathological diagnostic research. Studies include hematology, breast cancer, head and neck cancer, skin cancer, and diseases of central nervous, gastrointestinal, and genitourinary systems. The use of HSI/MSI suggest an improvement in the detection of diseases and clinical practice compared with traditional RGB analysis, and brings new opportunities in histological analysis of samples, such as digital staining or alleviating the inter-laboratory variability of digitized samples. Nevertheless, the number of studies in this field is currently limited, and more research is needed to confirm the advantages of this technology compared to conventional imagery.},
	language = {EN},
	number = {6},
	urldate = {2022-01-13},
	journal = {Biomedical Optics Express},
	author = {Ortega, Samuel and Ortega, Samuel and Ortega, Samuel and Ortega, Samuel and Halicek, Martin and Halicek, Martin and Halicek, Martin and Fabelo, Himar and Callico, Gustavo M. and Callico, Gustavo M. and Fei, Baowei and Fei, Baowei and Fei, Baowei and Fei, Baowei},
	month = jun,
	year = {2020},
	note = {Publisher: Optical Society of America},
	keywords = {pathology},
	pages = {3195--3233},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\75254Q5C\\Ortega et al. - 2020 - Hyperspectral and multispectral imaging in digital.pdf:application/pdf},
}

@article{ortega_use_2019,
	title = {Use of {Hyperspectral}/{Multispectral} {Imaging} in {Gastroenterology}. {Shedding} {Some}–{Different}–{Light} into the {Dark}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2077-0383/8/1/36},
	doi = {10.3390/jcm8010036},
	abstract = {Hyperspectral/Multispectral imaging (HSI/MSI) technologies are able to sample from tens to hundreds of spectral channels within the electromagnetic spectrum, exceeding the capabilities of human vision. These spectral techniques are based on the principle that every material has a different response (reflection and absorption) to different wavelengths. Thereby, this technology facilitates the discrimination between different materials. HSI has demonstrated good discrimination capabilities for materials in fields, for instance, remote sensing, pollution monitoring, field surveillance, food quality, agriculture, astronomy, geological mapping, and currently, also in medicine. HSI technology allows tissue observation beyond the limitations of the human eye. Moreover, many researchers are using HSI as a new diagnosis tool to analyze optical properties of tissue. Recently, HSI has shown good performance in identifying human diseases in a non-invasive manner. In this paper, we show the potential use of these technologies in the medical domain, with emphasis in the current advances in gastroenterology. The main aim of this review is to provide an overview of contemporary concepts regarding HSI technology together with state-of-art systems and applications in gastroenterology. Finally, we discuss the current limitations and upcoming trends of HSI in gastroenterology.},
	language = {en},
	number = {1},
	urldate = {2022-01-13},
	journal = {Journal of Clinical Medicine},
	author = {Ortega, Samuel and Fabelo, Himar and Iakovidis, Dimitris K. and Koulaouzidis, Anastasios and Callico, Gustavo M.},
	month = jan,
	year = {2019},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {gastroenterology},
	pages = {36},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\BN64BZC7\\Ortega et al. - 2019 - Use of HyperspectralMultispectral Imaging in Gast.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\V4Y3XRGC\\36.html:text/html},
}

@article{levenson_multispectral_2008,
	title = {Multispectral imaging and pathology: seeing and doing more},
	volume = {2},
	issn = {1753-0059},
	shorttitle = {Multispectral imaging and pathology},
	url = {https://doi.org/10.1517/17530059.2.9.1067},
	doi = {10.1517/17530059.2.9.1067},
	abstract = {Background: The current appreciation of the biological complexity of disease has led to increasing demands on pathologists to provide clinically relevant, quantitative morphological and molecular information while preserving cellular and tissue context. This can be technically challenging, especially when signals of interest are colocalized. With fluorescence-based methods, sensitivity and quantitative reliability may be compromised by spectral cross-talk between labels and by autofluorescence. In brightfield microscopy, overlapping chromogenic signals pose similar imaging difficulties. Approach: These challenges can be addressed using commercially available multispectral imaging technologies attached to standard microscope platforms, or alternatively, integrated into whole-slide scanning instruments. Assessment: Multispectral techniques, along with other developments in digital analysis, will allow pathologists to deliver appropriate quantitative and multiplexed analyses in a reproducible and timely manner. Caveats apply – as the complexity of the sample preparation and analysis components increases, commensurate attention must be paid to the use of appropriate controls for all stages of the process.},
	number = {9},
	urldate = {2022-01-13},
	journal = {Expert Opinion on Medical Diagnostics},
	author = {Levenson, Richard M and Fornari, Alessandro and Loda, Massimo},
	month = sep,
	year = {2008},
	pmid = {23495926},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1517/17530059.2.9.1067},
	keywords = {pathology, oncology},
	pages = {1067--1081},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QAMQM4DY\\Levenson et al. - 2008 - Multispectral imaging and pathology seeing and do.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\L73EQU6W\\17530059.2.9.html:text/html},
}

@article{thatcher_imaging_2016,
	title = {Imaging {Techniques} for {Clinical} {Burn} {Assessment} with a {Focus} on {Multispectral} {Imaging}},
	volume = {5},
	issn = {2162-1918},
	url = {https://www.liebertpub.com/doi/abs/10.1089/wound.2015.0684},
	doi = {10.1089/wound.2015.0684},
	abstract = {Significance: Burn assessments, including extent and severity, are some of the most critical diagnoses in burn care, and many recently developed imaging techniques may have the potential to improve the accuracy of these evaluations.

Recent Advances: Optical devices, telemedicine, and high-frequency ultrasound are among the highlights in recent burn imaging advancements. We present another promising technology, multispectral imaging (MSI), which also has the potential to impact current medical practice in burn care, among a variety of other specialties.

Critical Issues: At this time, it is still a matter of debate as to why there is no consensus on the use of technology to assist burn assessments in the United States. Fortunately, the availability of techniques does not appear to be a limitation. However, the selection of appropriate imaging technology to augment the provision of burn care can be difficult for clinicians to navigate. There are many technologies available, but a comprehensive review summarizing the tissue characteristics measured by each technology in light of aiding clinicians in selecting the proper device is missing. This would be especially valuable for the nonburn specialists who encounter burn injuries.

Future Directions: The questions of when burn assessment devices are useful to the burn team, how the various imaging devices work, and where the various burn imaging technologies fit into the spectrum of burn care will continue to be addressed. Technologies that can image a large surface area quickly, such as thermography or laser speckle imaging, may be suitable for initial burn assessment and triage. In the setting of presurgical planning, ultrasound or optical microscopy techniques, including optical coherence tomography, may prove useful. MSI, which actually has origins in burn care, may ultimately meet a high number of requirements for burn assessment in routine clinical use.},
	number = {8},
	urldate = {2022-01-13},
	journal = {Advances in Wound Care},
	author = {Thatcher, Jeffrey E. and Squiers, John J. and Kanick, Stephen C. and King, Darlene R. and Lu, Yang and Wang, Yulin and Mohan, Rachit and Sellke, Eric W. and DiMaio, J. Michael},
	month = aug,
	year = {2016},
	note = {Publisher: Mary Ann Liebert, Inc., publishers},
	keywords = {burn assessment},
	pages = {360--378},
	file = {Full Text:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8Y7YZPD5\\Thatcher et al. - 2016 - Imaging Techniques for Clinical Burn Assessment wi.pdf:application/pdf},
}

@article{mansfield_multispectral_2014,
	title = {Multispectral {Imaging}: {A} {Review} of {Its} {Technical} {Aspects} and {Applications} in {Anatomic} {Pathology}},
	volume = {51},
	issn = {0300-9858},
	shorttitle = {Multispectral {Imaging}},
	url = {https://doi.org/10.1177/0300985813506918},
	doi = {10.1177/0300985813506918},
	abstract = {The field of anatomic pathology has changed significantly over the last decades and, as a result of the technological developments in molecular pathology and genetics, has had increasing pressures put on it to become quantitative and to provide more information about protein expression on a cellular level in tissue sections. Multispectral imaging (MSI) has a long history as an advanced imaging modality and has been used for over a decade now in pathology to improve quantitative accuracy, enable the analysis of multicolor immunohistochemistry, and drastically reduce the impact of contrast-robbing tissue autofluorescence common in formalin-fixed, paraffin-embedded tissues. When combined with advanced software for the automated segmentation of different tissue morphologies (eg, tumor vs stroma) and cellular and subcellular segmentation, MSI can enable the per-cell quantitation of many markers simultaneously. This article covers the role that MSI has played in anatomic pathology in the analysis of formalin-fixed, paraffin-embedded tissue sections, discusses the technological aspects of why MSI has been adopted, and provides a review of the literature of the application of MSI in anatomic pathology.},
	language = {en},
	number = {1},
	urldate = {2022-01-13},
	journal = {Veterinary Pathology},
	author = {Mansfield, J. R.},
	month = jan,
	year = {2014},
	note = {Publisher: SAGE Publications Inc},
	keywords = {veterinary pathology},
	pages = {185--210},
	file = {SAGE PDF Full Text:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7VADX8DU\\Mansfield - 2014 - Multispectral Imaging A Review of Its Technical A.pdf:application/pdf},
}

@incollection{vasefi_chapter_2016,
	address = {Boston},
	title = {Chapter 16 - {Hyperspectral} and {Multispectral} {Imaging} in {Dermatology}},
	isbn = {978-0-12-802838-4},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128028384000169},
	abstract = {The capacity of optical imaging methods to characterize biological tissue in general, and skin in particular, with high resolution and discrimination ability provides a very valuable tool in the reliable, noninvasive assessment of differences between normal and abnormal states, thus allowing meaningful clinical applications, including early diagnosis of disease. For such approaches to fully realize their potential, one must start from a good understanding of tissue properties, and an equally advanced understanding of the tools deployed. Certain difficult problems may require the combined use of several imaging methods, and molecular-level understanding, with hyperspectral imaging emerging as one of the more powerful technologies in this regard, as reviewed here.},
	language = {en},
	urldate = {2022-01-13},
	booktitle = {Imaging in {Dermatology}},
	publisher = {Academic Press},
	author = {Vasefi, F. and MacKinnon, N. and Farkas, D. L.},
	editor = {Hamblin, Michael R. and Avci, Pinar and Gupta, Gaurav K.},
	month = jan,
	year = {2016},
	doi = {10.1016/B978-0-12-802838-4.00016-9},
	keywords = {dermatology},
	pages = {187--201},
	file = {ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TJRUXRC3\\B9780128028384000169.html:text/html},
}

@article{saiko_hyperspectral_2020,
	title = {Hyperspectral imaging in wound care: {A} systematic review},
	volume = {17},
	issn = {1742-481X},
	shorttitle = {Hyperspectral imaging in wound care},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/iwj.13474},
	doi = {10.1111/iwj.13474},
	abstract = {Multispectral and hyperspectral imaging (HSI) are emerging imaging techniques with the potential to transform the way patients with wounds are cared for, but it is not clear whether current systems are capable of delivering real-time tissue characterisation and treatment guidance. We conducted a systematic review of HSI systems that have been assessed in patients, published over the past 32 years. We analysed 140 studies, including 10 different HSI systems. Current in vivo HSI systems generate a tissue oxygenation map. Tissue oxygenation measurements may help to predict those patients at risk of wound formation or delayed healing. No safety concerns were reported in any studies. A small number of studies have demonstrated the capabilities of in vivo label-free HSI, but further work is needed to fully integrate it into the current clinical workflow for different wound aetiologies. As an emerging imaging modality for medical applications, HSI offers great potential for non-invasive disease diagnosis and guidance when treating patients with both acute and chronic wounds.},
	language = {en},
	number = {6},
	urldate = {2022-01-13},
	journal = {International Wound Journal},
	author = {Saiko, Gennadi and Lombardi, Phoebe and Au, Yunghan and Queen, Douglas and Armstrong, David and Harding, Keith},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/iwj.13474},
	keywords = {wound care},
	pages = {1840--1856},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\INZ4MCJQ\\Saiko et al. - 2020 - Hyperspectral imaging in wound care A systematic .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\LA2YDYH9\\iwj.html:text/html},
}

@inproceedings{al_maadeed_multispectral_2017,
	title = {Multispectral imaging and machine learning for automated cancer diagnosis},
	doi = {10.1109/IWCMC.2017.7986547},
	abstract = {Advancing technologies in the current era paved a lot to break the hurdles in medical diagnostic field. When cancer turned out to be the most common and dangerous disease of the age, novel diagnostic methodologies were introduced to enable early detection and hence save numerous lives. Accomplishment of various automatic and semi-automatic approaches in the diagnosis has proved its sufficient impetus to improve diagnostic speed and accuracy. A wide range of image processing based tools are currently available as a part of automatic cancer detection systems. Different imaging modalities have been utilized for extracting the suspected patient information, where the multispectral imaging has emerged as an efficient means for capturing the entire range of spectral and spatial data. In this paper, we review the current multispectral imaging based methods for automatic diagnosis of major types of cancer and discuss the limitations which are yet to be overcome, so as to improve the existing systems.},
	booktitle = {2017 13th {International} {Wireless} {Communications} and {Mobile} {Computing} {Conference} ({IWCMC})},
	author = {Al Maadeed, Somaya and Kunhoth, Suchithra and Bouridane, Ahmed and Peyret, Remy},
	month = jun,
	year = {2017},
	note = {ISSN: 2376-6506},
	keywords = {Cancer detection},
	pages = {1740--1744},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\BAADCRVY\\7986547.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JZBW95TR\\Al Maadeed et al. - 2017 - Multispectral imaging and machine learning for aut.pdf:application/pdf},
}

@article{lu_medical_2014-1,
	title = {Medical hyperspectral imaging: a review},
	volume = {19},
	issn = {1083-3668, 1560-2281},
	shorttitle = {Medical hyperspectral imaging},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-biomedical-optics/volume-19/issue-1/010901/Medical-hyperspectral-imaging-a-review/10.1117/1.JBO.19.1.010901.full},
	doi = {10.1117/1.JBO.19.1.010901},
	abstract = {Hyperspectral imaging (HSI) is an emerging imaging modality for medical applications, especially in disease diagnosis and image-guided surgery. HSI acquires a three-dimensional dataset called hypercube, with two spatial dimensions and one spectral dimension. Spatially resolved spectral imaging obtained by HSI provides diagnostic information about the tissue physiology, morphology, and composition. This review paper presents an overview of the literature on medical hyperspectral imaging technology and its applications. The aim of the survey is threefold: an introduction for those new to the field, an overview for those working in the field, and a reference for those searching for literature on a specific application.},
	number = {1},
	urldate = {2022-01-13},
	journal = {Journal of Biomedical Optics},
	author = {Lu, Guolan and Fei, Baowei},
	month = jan,
	year = {2014},
	note = {Publisher: SPIE},
	pages = {010901},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IWA228K2\\Lu and Fei - 2014 - Medical hyperspectral imaging a review.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\T3JYRLQQ\\1.JBO.19.1.010901.html:text/html},
}

@article{zaffino_review_2020,
	title = {A {Review} on {Advances} in {Intra}-operative {Imaging} for {Surgery} and {Therapy}: {Imagining} the {Operating} {Room} of the {Future}},
	volume = {48},
	issn = {1573-9686},
	shorttitle = {A {Review} on {Advances} in {Intra}-operative {Imaging} for {Surgery} and {Therapy}},
	url = {https://doi.org/10.1007/s10439-020-02553-6},
	doi = {10.1007/s10439-020-02553-6},
	abstract = {With the advent of Minimally Invasive Surgery (MIS), intra-operative imaging has become crucial for surgery and therapy guidance, allowing to partially compensate for the lack of information typical of MIS. This paper reviews the advancements in both classical (i.e. ultrasounds, X-ray, optical coherence tomography and magnetic resonance imaging) and more recent (i.e. multispectral, photoacoustic and Raman imaging) intra-operative imaging modalities. Each imaging modality was analyzed, focusing on benefits and disadvantages in terms of compatibility with the operating room, costs, acquisition time and image characteristics. Tables are included to summarize this information. New generation of hybrid surgical room and algorithms for real time/in room image processing were also investigated. Each imaging modality has its own (site- and procedure-specific) peculiarities in terms of spatial and temporal resolution, field of view and contrasted tissues. Besides the benefits that each technique offers for guidance, considerations about operators and patient risk, costs, and extra time required for surgical procedures have to be considered. The current trend is to equip surgical rooms with multimodal imaging systems, so as to integrate multiple information for real-time data extraction and computer-assisted processing. The future of surgery is to enhance surgeons eye to minimize intra- and after-surgery adverse events and provide surgeons with all possible support to objectify and optimize the care-delivery process.},
	language = {en},
	number = {8},
	urldate = {2022-01-13},
	journal = {Annals of Biomedical Engineering},
	author = {Zaffino, Paolo and Moccia, Sara and De Momi, Elena and Spadea, Maria Francesca},
	month = aug,
	year = {2020},
	pages = {2171--2191},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RJN3M5VU\\Zaffino et al. - 2020 - A Review on Advances in Intra-operative Imaging fo.pdf:application/pdf},
}

@article{shapey_intraoperative_2019,
	title = {Intraoperative multispectral and hyperspectral label-free imaging: {A} systematic review of in vivo clinical studies},
	volume = {12},
	issn = {1864-0648},
	shorttitle = {Intraoperative multispectral and hyperspectral label-free imaging},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jbio.201800455},
	doi = {10.1002/jbio.201800455},
	abstract = {Multispectral and hyperspectral imaging (HSI) are emerging optical imaging techniques with the potential to transform the way surgery is performed but it is not clear whether current systems are capable of delivering real-time tissue characterization and surgical guidance. We conducted a systematic review of surgical in vivo label-free multispectral and HSI systems that have been assessed intraoperatively in adult patients, published over a 10-year period to May 2018. We analysed 14 studies including 8 different HSI systems. Current in-vivo HSI systems generate an intraoperative tissue oxygenation map or enable tumour detection. Intraoperative tissue oxygenation measurements may help to predict those patients at risk of postoperative complications and in-vivo intraoperative tissue characterization may be performed with high specificity and sensitivity. All systems utilized a line-scanning or wavelength-scanning method but the spectral range and number of spectral bands employed varied significantly between studies and according to the system's clinical aim. The time to acquire a hyperspectral cube dataset ranged between 5 and 30 seconds. No safety concerns were reported in any studies. A small number of studies have demonstrated the capabilities of intraoperative in-vivo label-free HSI but further work is needed to fully integrate it into the current surgical workflow.},
	language = {en},
	number = {9},
	urldate = {2022-01-13},
	journal = {Journal of Biophotonics},
	author = {Shapey, Jonathan and Xie, Yijing and Nabavi, Eli and Bradford, Robert and Saeed, Shakeel R and Ourselin, Sebastien and Vercauteren, Tom},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jbio.201800455},
	keywords = {hyperspectral imaging, multispectral imaging, surgery},
	pages = {e201800455},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3MRKYJA3\\Shapey et al. - 2019 - Intraoperative multispectral and hyperspectral lab.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KPIHCFD3\\jbio.html:text/html},
}

@inproceedings{zeng_target_2021,
	address = {Zhuhai China},
	title = {Target {Classification} {Algorithms} {Based} on {Multispectral} {Imaging}: {A} {Review}},
	isbn = {978-1-4503-8916-7},
	shorttitle = {Target {Classification} {Algorithms} {Based} on {Multispectral} {Imaging}},
	url = {https://dl.acm.org/doi/10.1145/3449388.3449393},
	doi = {10.1145/3449388.3449393},
	abstract = {Multispectral imaging extracts rich spectral information from targets, which greatly expands the function of traditional imaging technology. Multispectral imaging is widely used in agriculture, military, medicine, industry, and meteorology. Because of the information redundancy in multispectral images, it is necessary to reduce the dimension by pre-processing. In recent years, most of the researchers have adopted the methods of pre-processing before classification. Based on the principles of feature selection, feature transformation, and feature extraction, common dimensionality reduction methods are introduced, and the advantages and disadvantages of them are discussed. Afterwards, classification methods are divided into traditional methods and deep learning methods, and their characteristics and application prospect are discussed. Through comparison, the former are cost-effective and have the mature theories, while the latter have strong adaptability and high classification accuracy. At present, methods could be optimized from the perspective of saving computing resources and using spectral information efficiently. In the future, traditional methods will be improved and comprehensively used, while new methods with stronger adaptability and precision will be developed.},
	language = {en},
	urldate = {2022-01-13},
	booktitle = {2021 6th {International} {Conference} on {Multimedia} and {Image} {Processing}},
	publisher = {ACM},
	author = {Zeng, Zimu and Wang, Weifeng and Zhang, Wenbo},
	month = jan,
	year = {2021},
	pages = {12--21},
	file = {Zeng et al. - 2021 - Target Classification Algorithms Based on Multispe.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EU4KYPJF\\Zeng et al. - 2021 - Target Classification Algorithms Based on Multispe.pdf:application/pdf},
}

@article{page_prisma_2021,
	title = {The {PRISMA} 2020 statement: an updated guideline for reporting systematic reviews},
	volume = {372},
	copyright = {© Author(s) (or their employer(s)) 2019. Re-use permitted under CC                 BY. No commercial re-use. See rights and permissions. Published by                 BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
	issn = {1756-1833},
	shorttitle = {The {PRISMA} 2020 statement},
	url = {https://www.bmj.com/content/372/bmj.n71},
	doi = {10.1136/bmj.n71},
	abstract = {{\textless}p{\textgreater}The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement, published in 2009, was designed to help systematic reviewers transparently report why the review was done, what the authors did, and what they found. Over the past decade, advances in systematic review methodology and terminology have necessitated an update to the guideline. The PRISMA 2020 statement replaces the 2009 statement and includes new reporting guidance that reflects advances in methods to identify, select, appraise, and synthesise studies. The structure and presentation of the items have been modified to facilitate implementation. In this article, we present the PRISMA 2020 27-item checklist, an expanded checklist that details reporting recommendations for each item, the PRISMA 2020 abstract checklist, and the revised flow diagrams for original and updated reviews.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2022-01-15},
	journal = {BMJ},
	author = {Page, Matthew J. and McKenzie, Joanne E. and Bossuyt, Patrick M. and Boutron, Isabelle and Hoffmann, Tammy C. and Mulrow, Cynthia D. and Shamseer, Larissa and Tetzlaff, Jennifer M. and Akl, Elie A. and Brennan, Sue E. and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M. and Hróbjartsson, Asbjørn and Lalu, Manoj M. and Li, Tianjing and Loder, Elizabeth W. and Mayo-Wilson, Evan and McDonald, Steve and McGuinness, Luke A. and Stewart, Lesley A. and Thomas, James and Tricco, Andrea C. and Welch, Vivian A. and Whiting, Penny and Moher, David},
	month = mar,
	year = {2021},
	pmid = {33782057},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	pages = {n71},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KWW8JP58\\Page et al. - 2021 - The PRISMA 2020 statement an updated guideline fo.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\A4X5P38Q\\bmj.n71.html:text/html},
}

@article{page_prisma_2021-1,
	title = {{PRISMA} 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews},
	volume = {372},
	copyright = {© Author(s) (or their employer(s)) 2019. Re-use permitted under CC                 BY. No commercial re-use. See rights and permissions. Published by                 BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
	issn = {1756-1833},
	shorttitle = {{PRISMA} 2020 explanation and elaboration},
	url = {https://www.bmj.com/content/372/bmj.n160},
	doi = {10.1136/bmj.n160},
	abstract = {{\textless}p{\textgreater}The methods and results of systematic reviews should be reported in sufficient detail to allow users to assess the trustworthiness and applicability of the review findings. The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement was developed to facilitate transparent and complete reporting of systematic reviews and has been updated (to PRISMA 2020) to reflect recent advances in systematic review methodology and terminology. Here, we present the explanation and elaboration paper for PRISMA 2020, where we explain why reporting of each item is recommended, present bullet points that detail the reporting recommendations, and present examples from published reviews. We hope that changes to the content and structure of PRISMA 2020 will facilitate uptake of the guideline and lead to more transparent, complete, and accurate reporting of systematic reviews.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2022-01-15},
	journal = {BMJ},
	author = {Page, Matthew J. and Moher, David and Bossuyt, Patrick M. and Boutron, Isabelle and Hoffmann, Tammy C. and Mulrow, Cynthia D. and Shamseer, Larissa and Tetzlaff, Jennifer M. and Akl, Elie A. and Brennan, Sue E. and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M. and Hróbjartsson, Asbjørn and Lalu, Manoj M. and Li, Tianjing and Loder, Elizabeth W. and Mayo-Wilson, Evan and McDonald, Steve and McGuinness, Luke A. and Stewart, Lesley A. and Thomas, James and Tricco, Andrea C. and Welch, Vivian A. and Whiting, Penny and McKenzie, Joanne E.},
	month = mar,
	year = {2021},
	pmid = {33781993},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	pages = {n160},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\NBILWQSW\\Page et al. - 2021 - PRISMA 2020 explanation and elaboration updated g.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VI56EMWY\\bmj.n160.html:text/html},
}

@article{page_prisma_2021-2,
	title = {The {PRISMA} 2020 statement: an updated guideline for reporting systematic reviews},
	issn = {1756-1833},
	shorttitle = {The {PRISMA} 2020 statement},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.n71},
	doi = {10.1136/bmj.n71},
	language = {en},
	urldate = {2022-01-15},
	journal = {BMJ},
	author = {Page, Matthew J and McKenzie, Joanne E and Bossuyt, Patrick M and Boutron, Isabelle and Hoffmann, Tammy C and Mulrow, Cynthia D and Shamseer, Larissa and Tetzlaff, Jennifer M and Akl, Elie A and Brennan, Sue E and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M and Hróbjartsson, Asbjørn and Lalu, Manoj M and Li, Tianjing and Loder, Elizabeth W and Mayo-Wilson, Evan and McDonald, Steve and McGuinness, Luke A and Stewart, Lesley A and Thomas, James and Tricco, Andrea C and Welch, Vivian A and Whiting, Penny and Moher, David},
	month = mar,
	year = {2021},
	pages = {n71},
	file = {Page et al. - 2021 - The PRISMA 2020 statement an updated guideline fo.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\39Z5UKPD\\Page et al. - 2021 - The PRISMA 2020 statement an updated guideline fo.pdf:application/pdf},
}

@article{page_prisma_2021-3,
	title = {The {PRISMA} 2020 statement: an updated guideline for reporting systematic reviews},
	issn = {1756-1833},
	shorttitle = {The {PRISMA} 2020 statement},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.n71},
	doi = {10.1136/bmj.n71},
	language = {en},
	urldate = {2022-01-15},
	journal = {BMJ},
	author = {Page, Matthew J and McKenzie, Joanne E and Bossuyt, Patrick M and Boutron, Isabelle and Hoffmann, Tammy C and Mulrow, Cynthia D and Shamseer, Larissa and Tetzlaff, Jennifer M and Akl, Elie A and Brennan, Sue E and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M and Hróbjartsson, Asbjørn and Lalu, Manoj M and Li, Tianjing and Loder, Elizabeth W and Mayo-Wilson, Evan and McDonald, Steve and McGuinness, Luke A and Stewart, Lesley A and Thomas, James and Tricco, Andrea C and Welch, Vivian A and Whiting, Penny and Moher, David},
	month = mar,
	year = {2021},
	pages = {n71},
	file = {Page et al. - 2021 - The PRISMA 2020 statement an updated guideline fo.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\LUP6IQY9\\Page et al. - 2021 - The PRISMA 2020 statement an updated guideline fo.pdf:application/pdf},
}

@article{page_prisma_2021-4,
	title = {The {PRISMA} 2020 statement: an updated guideline for reporting systematic reviews},
	issn = {1756-1833},
	shorttitle = {The {PRISMA} 2020 statement},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.n71},
	doi = {10.1136/bmj.n71},
	language = {en},
	urldate = {2022-01-15},
	journal = {BMJ},
	author = {Page, Matthew J and McKenzie, Joanne E and Bossuyt, Patrick M and Boutron, Isabelle and Hoffmann, Tammy C and Mulrow, Cynthia D and Shamseer, Larissa and Tetzlaff, Jennifer M and Akl, Elie A and Brennan, Sue E and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M and Hróbjartsson, Asbjørn and Lalu, Manoj M and Li, Tianjing and Loder, Elizabeth W and Mayo-Wilson, Evan and McDonald, Steve and McGuinness, Luke A and Stewart, Lesley A and Thomas, James and Tricco, Andrea C and Welch, Vivian A and Whiting, Penny and Moher, David},
	month = mar,
	year = {2021},
	pages = {n71},
	file = {Page et al. - 2021 - The PRISMA 2020 statement an updated guideline fo.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\HENFP4BI\\Page et al. - 2021 - The PRISMA 2020 statement an updated guideline fo.pdf:application/pdf},
}

@article{khan_trends_2021-1,
	title = {Trends in deep learning for medical hyperspectral image analysis},
	volume = {9},
	issn = {2169-3536},
	url = {http://arxiv.org/abs/2011.13974},
	doi = {10.1109/ACCESS.2021.3068392},
	abstract = {Deep learning algorithms have seen acute growth of interest in their applications throughout several fields of interest in the last decade, with medical hyperspectral imaging being a particularly promising domain. So far, to the best of our knowledge, there is no review paper that discusses the implementation of deep learning for medical hyperspectral imaging, which is what this review paper aims to accomplish by examining publications that currently utilize deep learning to perform effective analysis of medical hyperspectral imagery. This paper discusses deep learning concepts that are relevant and applicable to medical hyperspectral imaging analysis, several of which have been implemented since the boom in deep learning. This will comprise of reviewing the use of deep learning for classification, segmentation, and detection in order to investigate the analysis of medical hyperspectral imaging. Lastly, we discuss the current and future challenges pertaining to this discipline and the possible efforts to overcome such trials.},
	urldate = {2022-01-15},
	journal = {IEEE Access},
	author = {Khan, Uzair and Sidike, Paheding and Elkin, Colin and Devabhaktuni, Vijay},
	year = {2021},
	note = {arXiv: 2011.13974},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Computer Vision and Pattern Recognition},
	pages = {79534--79548},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9ZQY6CNR\\Khan et al. - 2021 - Trends in deep learning for medical hyperspectral .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\N82YTR3J\\2011.html:text/html},
}

@article{pallua_new_2021,
	title = {New perspectives of hyperspectral imaging for clinical research},
	volume = {32},
	issn = {0960-3360},
	url = {https://doi.org/10.1177/09603360211024971},
	doi = {10.1177/09603360211024971},
	abstract = {New developments in instrumentation and data analysis have further improved the perspectives of hyperspectral imaging in clinical use. Thus, hyperspectral imaging can be considered as “Next Generation Imaging” for future clinical research. As a contactless, non-invasive method with short process times of just a few seconds, it quantifies predefined substance classes. Results of hyperspectral imaging may support the detection of carcinomas and the classification of different tissue structures as well as the assessment of tissue blood flow. Taken together, this method combines the principle of spectroscopy with imaging using conventional visual cameras. Compared to other optical imaging methods, hyperspectral imaging also analyses deeper layers of tissue.},
	language = {en},
	number = {3-4},
	urldate = {2022-01-15},
	journal = {NIR news},
	author = {Pallua, Johannes D and Brunner, Andrea and Zelger, Bernhard and Huck, Christian W and Schirmer, Michael and Laimer, Johannes and Putzer, David and Thaler, Martin and Zelger, Bettina},
	month = jun,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Hyperspectral imaging, pathology, diagnostic use},
	pages = {5--13},
	file = {SAGE PDF Full Text:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FZUUZ844\\Pallua et al. - 2021 - New perspectives of hyperspectral imaging for clin.pdf:application/pdf},
}

@article{ozdemirsup1sup_deep_2020,
	title = {Deep {Learning} {Applications} for {Hyperspectral} {Imaging}: {A} {Systematic} {Review}},
	volume = {2},
	issn = {2643-8240},
	shorttitle = {Deep {Learning} {Applications} for {Hyperspectral} {Imaging}},
	url = {https://iecscience.org/jpapers/49#abstract},
	doi = {10.33969/JIEC.2020.21004},
	abstract = {{\textless}p{\textgreater}Since the acquisition of digital images, scientific studies on these images have been making significant progress. The sizes and quality of the images obtained have increased greatly from past to present. However, when the information contained in these images remains on the visible band (RGB band), the results that can be obtained are limited. For this reason, the need to acquire images with more broadband information has emerged. Hyperspectral Imaging (HSI) method has been developed to meet this need. A hyperspectral image consists of reflections in hundreds of different bands of the electromagnetic spectrum. Each object exhibits a unique reflection characteristic. Due to this characteristic, objects can be separated from each other using hyperspectral imaging.{\textless}/p{\textgreater}{\textless}p{\textgreater}Hyperspectral cameras are used to obtain this image. The information it contains is much more than an RGB image, so deeper results can be achieved than the human eye can see.{\textless}/p{\textgreater}{\textless}p{\textgreater}In this respect, it has great importance. Artificial intelligence technologies are used extensively in image processing as well as in many other fields. As a result, classification studies are carried out on hyperspectral images with machine learning methods. Machine learning methods can be considered as the most general terms of supervised machine learning, unsupervised machine learning, and reinforced machine learning. Supervised machine learning methods mainly: Support Vector Machines(SVM), k-Nearest Neighborhood(k-NN), Decision Trees, Random Forest, Linear Regression and Neural Networks(NN). Neural networks are also used as an unsupervised learning method. However, Deep Learning, a specialized method of artificial neural networks, is highly preferred due to its unique structure. Artificial intelligence methods have been widely used in recent years, especially for the classification of hyperspectral images containing complex information. Considering the studies, it is seen that especially deep learning is used intensively. At this point, studies have revealed different types of models. The number of models and their successes are increasing day by day.{\textless}/p{\textgreater}},
	language = {en},
	number = {1},
	urldate = {2022-01-15},
	journal = {Journal of the Institute of Electronics and Computer},
	author = {Ozdemir$^{\textrm{1}}$, {\textless}p{\textgreater}Akin and Polat$^{\textrm{2}}${\textless}/p{\textgreater}, Kemal},
	month = feb,
	year = {2020},
	note = {Publisher: Institute of Electronics and Computer},
	pages = {39--56},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CJ2JTQN9\\Ozdemir1 and Polat2  - 2020 - Deep Learning Applications for Hyperspectral Imagi.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\USTETN33\\49.html:text/html},
}

@article{yoon_hyperspectral_2022,
	title = {Hyperspectral {Imaging} for {Clinical} {Applications}},
	issn = {2092-7843},
	url = {https://doi.org/10.1007/s13206-021-00041-0},
	doi = {10.1007/s13206-021-00041-0},
	abstract = {Measuring morphological and biochemical features of tissue is crucial for disease diagnosis and surgical guidance, providing clinically significant information related to pathophysiology. Hyperspectral imaging (HSI) techniques obtain both spatial and spectral features of tissue without labeling molecules such as fluorescent dyes, which provides rich information for improved disease diagnosis and treatment. Recent advances in HSI systems have demonstrated its potential for clinical applications, especially in disease diagnosis and image-guided surgery. This review summarizes the basic principle of HSI and optical systems, deep-learning-based image analysis, and clinical applications of HSI to provide insight into this rapidly growing field of research. In addition, the challenges facing the clinical implementation of HSI techniques are discussed.},
	language = {en},
	urldate = {2022-01-15},
	journal = {BioChip Journal},
	author = {Yoon, Jonghee},
	month = jan,
	year = {2022},
	file = {Springer Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4CT6JESL\\Yoon - 2022 - Hyperspectral Imaging for Clinical Applications.pdf:application/pdf},
}

@article{halicek_-vivo_2019-1,
	title = {In-{Vivo} and {Ex}-{Vivo} {Tissue} {Analysis} through {Hyperspectral} {Imaging} {Techniques}: {Revealing} the {Invisible} {Features} of {Cancer}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {In-{Vivo} and {Ex}-{Vivo} {Tissue} {Analysis} through {Hyperspectral} {Imaging} {Techniques}},
	url = {https://www.mdpi.com/2072-6694/11/6/756},
	doi = {10.3390/cancers11060756},
	abstract = {In contrast to conventional optical imaging modalities, hyperspectral imaging (HSI) is able to capture much more information from a certain scene, both within and beyond the visual spectral range (from 400 to 700 nm). This imaging modality is based on the principle that each material provides different responses to light reflection, absorption, and scattering across the electromagnetic spectrum. Due to these properties, it is possible to differentiate and identify the different materials/substances presented in a certain scene by their spectral signature. Over the last two decades, HSI has demonstrated potential to become a powerful tool to study and identify several diseases in the medical field, being a non-contact, non-ionizing, and a label-free imaging modality. In this review, the use of HSI as an imaging tool for the analysis and detection of cancer is presented. The basic concepts related to this technology are detailed. The most relevant, state-of-the-art studies that can be found in the literature using HSI for cancer analysis are presented and summarized, both in-vivo and ex-vivo. Lastly, we discuss the current limitations of this technology in the field of cancer detection, together with some insights into possible future steps in the improvement of this technology.},
	language = {en},
	number = {6},
	urldate = {2022-01-15},
	journal = {Cancers},
	author = {Halicek, Martin and Fabelo, Himar and Ortega, Samuel and Callico, Gustavo M. and Fei, Baowei},
	month = jun,
	year = {2019},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {hyperspectral imaging, biomedical optical imaging, artificial intelligence, machine learning, cancer, clinical diagnosis, medical diagnostic imaging},
	pages = {756},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JUZJ5JJT\\Halicek et al. - 2019 - In-Vivo and Ex-Vivo Tissue Analysis through Hypers.pdf:application/pdf},
}

@article{barberio_intraoperative_2021,
	title = {Intraoperative {Guidance} {Using} {Hyperspectral} {Imaging}: {A} {Review} for {Surgeons}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Intraoperative {Guidance} {Using} {Hyperspectral} {Imaging}},
	url = {https://www.mdpi.com/2075-4418/11/11/2066},
	doi = {10.3390/diagnostics11112066},
	abstract = {Hyperspectral imaging (HSI) is a novel optical imaging modality, which has recently found diverse applications in the medical field. HSI is a hybrid imaging modality, combining a digital photographic camera with a spectrographic unit, and it allows for a contactless and non-destructive biochemical analysis of living tissue. HSI provides quantitative and qualitative information of the tissue composition at molecular level in a contrast-free manner, hence making it possible to objectively discriminate between different tissue types and between healthy and pathological tissue. Over the last two decades, HSI has been increasingly used in the medical field, and only recently it has found an application in the operating room. In the last few years, several research groups have used this imaging modality as an intraoperative guidance tool within different surgical disciplines. Despite its great potential, HSI still remains far from being routinely used in the daily surgical practice, since it is still largely unknown to most of the surgical community. The aim of this study is to provide clinical surgeons with an overview of the capabilities, current limitations, and future directions of HSI for intraoperative guidance.},
	language = {en},
	number = {11},
	urldate = {2022-01-15},
	journal = {Diagnostics},
	author = {Barberio, Manuel and Benedicenti, Sara and Pizzicannella, Margherita and Felli, Eric and Collins, Toby and Jansen-Winkeln, Boris and Marescaux, Jacques and Viola, Massimo Giuseppe and Diana, Michele},
	month = nov,
	year = {2021},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {hyperspectral imaging, image-guided surgery, intraoperative guidance, intraoperative imaging, optical imaging, precision surgery},
	pages = {2066},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9SZB6HIE\\Barberio et al. - 2021 - Intraoperative Guidance Using Hyperspectral Imagin.pdf:application/pdf},
}

@article{johansen_recent_2020,
	title = {Recent advances in hyperspectral imaging for melanoma detection},
	volume = {12},
	issn = {1939-0068},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1465},
	doi = {10.1002/wics.1465},
	abstract = {Skin cancer is one of the most common types of cancer. Skin cancers are classified as nonmelanoma and melanoma, with the first type being the most frequent and the second type being the most deadly. The key to effective treatment of skin cancer is early detection. With the recent increase of computational power, the number of algorithms to detect and classify skin lesions has increased. The overall verdict on systems based on clinical and dermoscopic images captured with conventional RGB (red, green, and blue) cameras is that they do not outperform dermatologists. Computer-based systems based on conventional RGB images seem to have reached an upper limit in their performance, while emerging technologies such as hyperspectral and multispectral imaging might possibly improve the results. These types of images can explore spectral regions beyond the human eye capabilities. Feature selection and dimensionality reduction are crucial parts of extracting salient information from this type of data. It is necessary to extend current classification methodologies to use all of the spatiospectral information, and deep learning models should be explored since they are capable of learning robust feature detectors from data. There is a lack of large, high-quality datasets of hyperspectral skin lesion images, and there is a need for tools that can aid with monitoring the evolution of skin lesions over time. To understand the rich information contained in hyperspectral images, further research using data science and statistical methodologies, such as functional data analysis, scale-space theory, machine learning, and so on, are essential. This article is categorized under: Applications of Computational Statistics {\textgreater} Health and Medical Data/Informatics},
	language = {en},
	number = {1},
	urldate = {2022-01-15},
	journal = {WIREs Computational Statistics},
	author = {Johansen, Thomas Haugland and Møllersen, Kajsa and Ortega, Samuel and Fabelo, Himar and Garcia, Aday and Callico, Gustavo M. and Godtliebsen, Fred},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1465},
	keywords = {melanoma, machine learning, hyperspectral, skin cancer},
	pages = {e1465},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FZ6EUCDY\\Johansen et al. - 2020 - Recent advances in hyperspectral imaging for melan.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VEWV7H7P\\wics.html:text/html},
}

@article{huang_systematic_2021,
	title = {A systematic review of machine learning and automation in burn wound evaluation: {A} promising but developing frontier},
	volume = {47},
	issn = {0305-4179},
	shorttitle = {A systematic review of machine learning and automation in burn wound evaluation},
	url = {https://www.sciencedirect.com/science/article/pii/S0305417921001777},
	doi = {10.1016/j.burns.2021.07.007},
	abstract = {Background
Visual evaluation is the most common method of evaluating burn wounds. Its subjective nature can lead to inaccurate diagnoses and inappropriate burn center referrals. Machine learning may provide an objective solution. The objective of this study is to summarize the literature on ML in burn wound evaluation.
Methods
A systematic review of articles published between January 2000 and January 2021 was performed using PubMed and MEDLINE (OVID). Articles reporting on ML or automation to evaluate burn wounds were included. Keywords included burns, machine/deep learning, artificial intelligence, burn classification technology, and mobile applications. Data were extracted on study design, method of data acquisition, machine learning techniques, and machine learning accuracy.
Results
Thirty articles were included. Nine studies used machine learning and automation to estimate percent total body surface area (\%TBSA) burned, 4 calculated fluid estimations, 19 estimated burn depth, 5 estimated need for surgery, and 2 evaluated scarring. Models calculating \%TBSA burned demonstrated accuracies comparable to or better than paper methods. Burn depth classification models achieved accuracies of {\textgreater}83\%.
Conclusion
Machine learning provides an objective adjunct that may improve diagnostic accuracy in evaluating burn wound severity. Existing models remain in the early stages with future studies needed to assess their clinical feasibility.},
	language = {en},
	number = {8},
	urldate = {2022-01-15},
	journal = {Burns},
	author = {Huang, Samantha and Dang, Justin and Sheckter, Clifford C. and Yenikomshian, Haig A. and Gillenwater, Justin},
	year = {2021},
	keywords = {Surgery, Machine learning, Deep learning, Burn},
	pages = {1691--1704},
	file = {ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KQJ87JNS\\S0305417921001777.html:text/html},
}

@article{huang_systematic_2021-1,
	title = {A systematic review of machine learning and automation in burn wound evaluation: {A} promising but developing frontier},
	volume = {47},
	issn = {03054179},
	shorttitle = {A systematic review of machine learning and automation in burn wound evaluation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0305417921001777},
	doi = {10.1016/j.burns.2021.07.007},
	abstract = {Background: Visual evaluation is the most common method of evaluating burn wounds. Its subjective nature can lead to inaccurate diagnoses and inappropriate burn center referrals. Machine learning may provide an objective solution. The objective of this study is to summarize the literature on ML in burn wound evaluation.
Methods: A systematic review of articles published between January 2000 and January 2021 was performed using PubMed and MEDLINE (OVID). Articles reporting on ML or automation to evaluate burn wounds were included. Keywords included burns, machine/deep learning, artificial intelligence, burn classification technology, and mobile applications. Data were extracted on study design, method of data acquisition, machine learning techniques, and machine learning accuracy.
Results: Thirty articles were included. Nine studies used machine learning and automation to estimate percent total body surface area (\%TBSA) burned, 4 calculated fluid estimations, 19 estimated burn depth, 5 estimated need for surgery, and 2 evaluated scarring. Models calculating \%TBSA burned demonstrated accuracies comparable to or better than paper methods. Burn depth classification models achieved accuracies of {\textgreater}83\%.
Conclusion: Machine learning provides an objective adjunct that may improve diagnostic accuracy in evaluating burn wound severity. Existing models remain in the early stages with future studies needed to assess their clinical feasibility.},
	language = {en},
	number = {8},
	urldate = {2022-01-15},
	journal = {Burns},
	author = {Huang, Samantha and Dang, Justin and Sheckter, Clifford C. and Yenikomshian, Haig A. and Gillenwater, Justin},
	month = dec,
	year = {2021},
	pages = {1691--1704},
	file = {Huang et al. - 2021 - A systematic review of machine learning and automa.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Y99DDAL2\\Huang et al. - 2021 - A systematic review of machine learning and automa.pdf:application/pdf},
}

@article{gusenbauer_which_2020,
	title = {Which academic search systems are suitable for systematic reviews or meta-analyses? {Evaluating} retrieval qualities of {Google} {Scholar}, {PubMed}, and 26 other resources},
	volume = {11},
	issn = {1759-2887},
	shorttitle = {Which academic search systems are suitable for systematic reviews or meta-analyses?},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1378},
	doi = {10.1002/jrsm.1378},
	abstract = {Rigorous evidence identification is essential for systematic reviews and meta-analyses (evidence syntheses) because the sample selection of relevant studies determines a review's outcome, validity, and explanatory power. Yet, the search systems allowing access to this evidence provide varying levels of precision, recall, and reproducibility and also demand different levels of effort. To date, it remains unclear which search systems are most appropriate for evidence synthesis and why. Advice on which search engines and bibliographic databases to choose for systematic searches is limited and lacking systematic, empirical performance assessments. This study investigates and compares the systematic search qualities of 28 widely used academic search systems, including Google Scholar, PubMed, and Web of Science. A novel, query-based method tests how well users are able to interact and retrieve records with each system. The study is the first to show the extent to which search systems can effectively and efficiently perform (Boolean) searches with regards to precision, recall, and reproducibility. We found substantial differences in the performance of search systems, meaning that their usability in systematic searches varies. Indeed, only half of the search systems analyzed and only a few Open Access databases can be recommended for evidence syntheses without adding substantial caveats. Particularly, our findings demonstrate why Google Scholar is inappropriate as principal search system. We call for database owners to recognize the requirements of evidence synthesis and for academic journals to reassess quality requirements for systematic reviews. Our findings aim to support researchers in conducting better searches for better evidence synthesis.},
	language = {en},
	number = {2},
	urldate = {2022-01-16},
	journal = {Research Synthesis Methods},
	author = {Gusenbauer, Michael and Haddaway, Neal R.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1378},
	keywords = {academic search systems, discovery, evaluation, information retrieval, systematic review, systematic search},
	pages = {181--217},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VTQHY65V\\Gusenbauer and Haddaway - 2020 - Which academic search systems are suitable for sys.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6LCAA2ID\\jrsm.html:text/html},
}

@article{singh_journal_2021,
	title = {The journal coverage of {Web} of {Science}, {Scopus} and {Dimensions}: {A} comparative analysis},
	volume = {126},
	issn = {0138-9130, 1588-2861},
	shorttitle = {The journal coverage of {Web} of {Science}, {Scopus} and {Dimensions}},
	url = {https://link.springer.com/10.1007/s11192-021-03948-5},
	doi = {10.1007/s11192-021-03948-5},
	abstract = {Traditionally, Web of Science and Scopus have been the two most widely used databases for bibliometric analyses. However, during the last few years some new scholarly databases, such as Dimensions, have come up. Several previous studies have compared different databases, either through a direct comparison of article coverage or by comparing the citations across the databases. This article aims to present a comparative analysis of the journal coverage of the three databases (Web of Science, Scopus and Dimensions), with the objective to describe, understand and visualize the differences in them. The most recent master journal lists of the three databases is used for analysis. The results indicate that the databases have significantly different journal coverage, with the Web of Science being most selective and Dimensions being the most exhaustive. About 99.11\% and 96.61\% of the journals indexed in Web of Science are also indexed in Scopus and Dimensions, respectively. Scopus has 96.42\% of its indexed journals also covered by Dimensions. Dimensions database has the most exhaustive journal coverage, with 82.22\% more journals than Web of Science and 48.17\% more journals than Scopus. This article also analysed the research outputs for 20 selected countries for the 2010-2018 period, as indexed in the three databases, and identified databaseinduced variations in research output volume, rank, global share and subject area composition for different countries. It is found that there are clearly visible variations in the research output from different countries in the three databases, along with differential coverage of different subject areas by the three databases. The analytical study provides an informative and practically useful picture of the journal coverage of Web of Science, Scopus and Dimensions databases.},
	language = {en},
	number = {6},
	urldate = {2022-01-16},
	journal = {Scientometrics},
	author = {Singh, Vivek Kumar and Singh, Prashasti and Karmakar, Mousumi and Leta, Jacqueline and Mayr, Philipp},
	month = jun,
	year = {2021},
	pages = {5113--5142},
	file = {Singh et al. - 2021 - The journal coverage of Web of Science, Scopus and.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8T5DEMQ8\\Singh et al. - 2021 - The journal coverage of Web of Science, Scopus and.pdf:application/pdf},
}

@article{mcguinness_riskbias_2021,
	title = {Risk‐of‐bias {VISualization} (robvis): {An} {R} package and {Shiny} web app for visualizing risk‐of‐bias assessments},
	volume = {12},
	issn = {1759-2879, 1759-2887},
	shorttitle = {Risk‐of‐bias {VISualization} (robvis)},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1411},
	doi = {10.1002/jrsm.1411},
	language = {en},
	number = {1},
	urldate = {2022-01-16},
	journal = {Research Synthesis Methods},
	author = {McGuinness, Luke A. and Higgins, Julian P. T.},
	month = jan,
	year = {2021},
	pages = {55--61},
	file = {McGuinness and Higgins - 2021 - Risk‐of‐bias VISualization (robvis) An R package .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SZ2IV5NI\\McGuinness and Higgins - 2021 - Risk‐of‐bias VISualization (robvis) An R package .pdf:application/pdf},
}

@article{guertin_systematic_2020,
	title = {A {Systematic} {Review} of {Methods} {Used} for {Confounding} {Adjustment} in {Observational} {Economic} {Evaluations} in {Cardiology} {Conducted} between 2013 and 2017},
	volume = {40},
	issn = {0272-989X},
	url = {https://doi.org/10.1177/0272989X20937257},
	doi = {10.1177/0272989X20937257},
	abstract = {Background. Observational economic evaluations (i.e., economic evaluations in which treatment allocation is not randomized) are prone to confounding bias. Prior reviews published in 2013 have shown that adjusting for confounding is poorly done, if done at all. Although these reviews raised awareness on the issues, it is unclear if their results improved the methodological quality of future work. We therefore aimed to investigate whether and how confounding was accounted for in recently published observational economic evaluations in the field of cardiology. Methods. We performed a systematic review of PubMed, Embase, Cochrane Library, Web of Science, and PsycInfo databases using a set of Medical Subject Headings and keywords covering topics in “observational economic evaluations in health within humans” and “cardiovascular diseases.” Any study published in either English or French between January 1, 2013, and December 31, 2017, addressing our search criteria was eligible for inclusion in our review. Our protocol was registered with PROSPERO (CRD42018112391). Results. Forty-two (0.6\%) out of 7523 unique citations met our inclusion criteria. Fewer than half of the selected studies adjusted for confounding (n = 19 [45.2\%]). Of those that adjusted for confounding, propensity score matching (n = 8 [42.1\%]) and other matching-based approaches were favored (n = 8 [42.1\%]). Our results also highlighted that most authors who adjusted for confounding rarely justified their methodological choices. Conclusion. Our results indicate that adjustment for confounding is often ignored when conducting an observational economic evaluation. Continued knowledge translation efforts aimed at improving researchers’ knowledge regarding confounding bias and methods aimed at addressing this issue are required and should be supported by journal editors.},
	language = {en},
	number = {5},
	urldate = {2022-01-16},
	journal = {Medical Decision Making},
	author = {Guertin, Jason R. and Conombo, Blanchard and Langevin, Raphaël and Bergeron, Frédéric and Holbrook, Anne and Humphries, Brittany and Matteau, Alexis and Potter, Brian J. and Renoux, Christel and Tarride, Jean-Éric and Durand, Madeleine},
	month = jul,
	year = {2020},
	note = {Publisher: SAGE Publications Inc STM},
	keywords = {systematic review, cardiology, confounding adjustment, economic evaluations, observational studies},
	pages = {582--595},
	file = {SAGE PDF Full Text:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9L62MNGA\\Guertin et al. - 2020 - A Systematic Review of Methods Used for Confoundin.pdf:application/pdf},
}

@article{dekkers_cosmos-e_2019,
	title = {{COSMOS}-{E}: {Guidance} on conducting systematic reviews and meta-analyses of observational studies of etiology},
	volume = {16},
	issn = {1549-1676},
	shorttitle = {{COSMOS}-{E}},
	url = {https://dx.plos.org/10.1371/journal.pmed.1002742},
	doi = {10.1371/journal.pmed.1002742},
	abstract = {Background To our knowledge, no publication providing overarching guidance on the conduct of systematic reviews of observational studies of etiology exists.},
	language = {en},
	number = {2},
	urldate = {2022-01-16},
	journal = {PLOS Medicine},
	author = {Dekkers, Olaf M. and Vandenbroucke, Jan P. and Cevallos, Myriam and Renehan, Andrew G. and Altman, Douglas G. and Egger, Matthias},
	month = feb,
	year = {2019},
	pages = {e1002742},
	file = {Dekkers et al. - 2019 - COSMOS-E Guidance on conducting systematic review.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9H5IX9CT\\Dekkers et al. - 2019 - COSMOS-E Guidance on conducting systematic review.pdf:application/pdf},
}

@article{moons_critical_2014,
	title = {Critical {Appraisal} and {Data} {Extraction} for {Systematic} {Reviews} of {Prediction} {Modelling} {Studies}: {The} {CHARMS} {Checklist}},
	volume = {11},
	issn = {1549-1676},
	shorttitle = {Critical {Appraisal} and {Data} {Extraction} for {Systematic} {Reviews} of {Prediction} {Modelling} {Studies}},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001744},
	doi = {10.1371/journal.pmed.1001744},
	abstract = {Carl Moons and colleagues provide a checklist and background explanation for critically appraising and extracting data from systematic reviews of prognostic and diagnostic prediction modelling studies. Please see later in the article for the Editors' Summary},
	language = {en},
	number = {10},
	urldate = {2022-01-17},
	journal = {PLOS Medicine},
	author = {Moons, Karel G. M. and Groot, Joris A. H. de and Bouwmeester, Walter and Vergouwe, Yvonne and Mallett, Susan and Altman, Douglas G. and Reitsma, Johannes B. and Collins, Gary S.},
	month = oct,
	year = {2014},
	note = {Publisher: Public Library of Science},
	keywords = {Cancer detection and diagnosis, Measurement, Cancer risk factors, Diagnostic medicine, Forecasting, Medical risk factors, Systematic reviews, Type 2 diabetes risk},
	pages = {e1001744},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FDHGRXDW\\Moons et al. - 2014 - Critical Appraisal and Data Extraction for Systema.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3GY8GLLQ\\article.html:text/html},
}

@article{maier-hein_bias_2020,
	title = {{BIAS}: {Transparent} reporting of biomedical image analysis challenges},
	volume = {66},
	issn = {1361-8415},
	shorttitle = {{BIAS}},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841520301602},
	doi = {10.1016/j.media.2020.101796},
	abstract = {The number of biomedical image analysis challenges organized per year is steadily increasing. These international competitions have the purpose of benchmarking algorithms on common data sets, typically to identify the best method for a given problem. Recent research, however, revealed that common practice related to challenge reporting does not allow for adequate interpretation and reproducibility of results. To address the discrepancy between the impact of challenges and the quality (control), the Biomedical Image Analysis ChallengeS (BIAS) initiative developed a set of recommendations for the reporting of challenges. The BIAS statement aims to improve the transparency of the reporting of a biomedical image analysis challenge regardless of field of application, image modality or task category assessed. This article describes how the BIAS statement was developed and presents a checklist which authors of biomedical image analysis challenges are encouraged to include in their submission when giving a paper on a challenge into review. The purpose of the checklist is to standardize and facilitate the review process and raise interpretability and reproducibility of challenge results by making relevant information explicit.},
	language = {en},
	urldate = {2022-01-17},
	journal = {Medical Image Analysis},
	author = {Maier-Hein, Lena and Reinke, Annika and Kozubek, Michal and Martel, Anne L. and Arbel, Tal and Eisenmann, Matthias and Hanbury, Allan and Jannin, Pierre and Müller, Henning and Onogur, Sinan and Saez-Rodriguez, Julio and van Ginneken, Bram and Kopp-Schneider, Annette and Landman, Bennett A.},
	year = {2020},
	keywords = {Biomedical challenges, Biomedical image analysis, Good scientific practice, Guideline},
	pages = {101796},
	file = {Full Text:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\LBYKR7RD\\Maier-Hein et al. - 2020 - BIAS Transparent reporting of biomedical image an.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DX7724FG\\S1361841520301602.html:text/html},
}

@article{mongan_checklist_2020,
	title = {Checklist for {Artificial} {Intelligence} in {Medical} {Imaging} ({CLAIM}): {A}                     {Guide} for {Authors} and {Reviewers}},
	volume = {2},
	shorttitle = {Checklist for {Artificial} {Intelligence} in {Medical} {Imaging} ({CLAIM})},
	url = {https://pubs.rsna.org/doi/full/10.1148/ryai.2020200029},
	doi = {10.1148/ryai.2020200029},
	number = {2},
	urldate = {2022-01-17},
	journal = {Radiology: Artificial Intelligence},
	author = {Mongan, John and Moy, Linda and Kahn, Charles                             E.},
	year = {2020},
	note = {Publisher: Radiological Society of North America},
	pages = {e200029},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7DTE44Z7\\Mongan et al. - 2020 - Checklist for Artificial Intelligence in Medical I.pdf:application/pdf},
}

@article{norgeot_minimum_2020,
	title = {Minimum information about clinical artificial intelligence modeling: the {MI}-{CLAIM} checklist},
	volume = {26},
	copyright = {2020 Springer Nature America, Inc.},
	issn = {1546-170X},
	shorttitle = {Minimum information about clinical artificial intelligence modeling},
	url = {https://www.nature.com/articles/s41591-020-1041-y},
	doi = {10.1038/s41591-020-1041-y},
	abstract = {Here we present the MI-CLAIM checklist, a tool intended to improve transparent reporting of AI algorithms in medicine.},
	language = {en},
	number = {9},
	urldate = {2022-01-17},
	journal = {Nature Medicine},
	author = {Norgeot, Beau and Quer, Giorgio and Beaulieu-Jones, Brett K. and Torkamani, Ali and Dias, Raquel and Gianfrancesco, Milena and Arnaout, Rima and Kohane, Isaac S. and Saria, Suchi and Topol, Eric and Obermeyer, Ziad and Yu, Bin and Butte, Atul J.},
	month = sep,
	year = {2020},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 9
Primary\_atype: Comments \& Opinion
Publisher: Nature Publishing Group
Subject\_term: Communication and replication;Machine learning
Subject\_term\_id: communication-and-replication;machine-learning},
	keywords = {Machine learning, Communication and replication},
	pages = {1320--1324},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9P6BT27N\\Norgeot et al. - 2020 - Minimum information about clinical artificial inte.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SZFUG2C6\\s41591-020-1041-y.html:text/html},
}

@article{shelmerdine_review_2021,
	title = {Review of study reporting guidelines for clinical studies using artificial intelligence in healthcare},
	volume = {28},
	issn = {2632-1009},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8383863/},
	doi = {10.1136/bmjhci-2021-100385},
	abstract = {High-quality research is essential in guiding evidence-based care, and should be reported in a way that is reproducible, transparent and where appropriate, provide sufficient detail for inclusion in future meta-analyses. Reporting guidelines for various study designs have been widely used for clinical (and preclinical) studies, consisting of checklists with a minimum set of points for inclusion. With the recent rise in volume of research using artificial intelligence (AI), additional factors need to be evaluated, which do not neatly conform to traditional reporting guidelines (eg, details relating to technical algorithm development). In this review, reporting guidelines are highlighted to promote awareness of essential content required for studies evaluating AI interventions in healthcare. These include published and in progress extensions to well-known reporting guidelines such as Standard Protocol Items: Recommendations for Interventional Trials-AI (study protocols), Consolidated Standards of Reporting Trials-AI (randomised controlled trials), Standards for Reporting of Diagnostic Accuracy Studies-AI (diagnostic accuracy studies) and Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis-AI (prediction model studies). Additionally there are a number of guidelines that consider AI for health interventions more generally (eg, Checklist for Artificial Intelligence in Medical Imaging (CLAIM), minimum information (MI)-CLAIM, MI for Medical AI Reporting) or address a specific element such as the ‘learning curve’ (Developmental and Exploratory Clinical Investigation of Decision-AI). Economic evaluation of AI health interventions is not currently addressed, and may benefit from extension to an existing guideline. In the face of a rapid influx of studies of AI health interventions, reporting guidelines help ensure that investigators and those appraising studies consider both the well-recognised elements of good study design and reporting, while also adequately addressing new challenges posed by AI-specific elements.},
	number = {1},
	urldate = {2022-01-17},
	journal = {BMJ Health \& Care Informatics},
	author = {Shelmerdine, Susan Cheng and Arthurs, Owen J and Denniston, Alastair and Sebire, Neil J},
	month = aug,
	year = {2021},
	pmid = {34426417},
	pmcid = {PMC8383863},
	pages = {e100385},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ZJ9YW9T4\\Shelmerdine et al. - 2021 - Review of study reporting guidelines for clinical .pdf:application/pdf},
}

@book{information_what_2016,
	title = {What types of studies are there?},
	url = {https://www.ncbi.nlm.nih.gov/books/NBK390304/},
	abstract = {There are various types of scientific studies such as experiments and comparative analyses, observational studies, surveys, or interviews. The choice of study type will mainly depend on the research question being asked.},
	language = {en},
	urldate = {2022-01-17},
	publisher = {Institute for Quality and Efficiency in Health Care (IQWiG)},
	author = {Information, National Center for Biotechnology and Pike, U. S. National Library of Medicine 8600 Rockville and MD, Bethesda and Usa, 20894},
	month = sep,
	year = {2016},
	note = {Publication Title: InformedHealth.org [Internet]},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7HDB7XUM\\NBK390304.html:text/html},
}

@article{moons_probast_2019,
	title = {{PROBAST}: {A} {Tool} to {Assess} {Risk} of {Bias} and {Applicability} of {Prediction} {Model} {Studies}: {Explanation} and {Elaboration}},
	volume = {170},
	issn = {0003-4819},
	shorttitle = {{PROBAST}},
	url = {http://annals.org/article.aspx?doi=10.7326/M18-1377},
	doi = {10.7326/M18-1377},
	language = {en},
	number = {1},
	urldate = {2022-01-17},
	journal = {Annals of Internal Medicine},
	author = {Moons, Karel G.M. and Wolff, Robert F. and Riley, Richard D. and Whiting, Penny F. and Westwood, Marie and Collins, Gary S. and Reitsma, Johannes B. and Kleijnen, Jos and Mallett, Sue},
	month = jan,
	year = {2019},
	pages = {W1},
	file = {Moons et al. - 2019 - PROBAST A Tool to Assess Risk of Bias and Applica.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WCRVJNYG\\Moons et al. - 2019 - PROBAST A Tool to Assess Risk of Bias and Applica.pdf:application/pdf},
}

@article{wolff_probast_2019,
	title = {{PROBAST}: {A} {Tool} to {Assess} the {Risk} of {Bias} and {Applicability} of {Prediction} {Model} {Studies}},
	volume = {170},
	issn = {1539-3704},
	shorttitle = {{PROBAST}},
	doi = {10.7326/M18-1376},
	abstract = {Clinical prediction models combine multiple predictors to estimate risk for the presence of a particular condition (diagnostic models) or the occurrence of a certain event in the future (prognostic models). PROBAST (Prediction model Risk Of Bias ASsessment Tool), a tool for assessing the risk of bias (ROB) and applicability of diagnostic and prognostic prediction model studies, was developed by a steering group that considered existing ROB tools and reporting guidelines. The tool was informed by a Delphi procedure involving 38 experts and was refined through piloting. PROBAST is organized into the following 4 domains: participants, predictors, outcome, and analysis. These domains contain a total of 20 signaling questions to facilitate structured judgment of ROB, which was defined to occur when shortcomings in study design, conduct, or analysis lead to systematically distorted estimates of model predictive performance. PROBAST enables a focused and transparent approach to assessing the ROB and applicability of studies that develop, validate, or update prediction models for individualized predictions. Although PROBAST was designed for systematic reviews, it can be used more generally in critical appraisal of prediction model studies. Potential users include organizations supporting decision making, researchers and clinicians who are interested in evidence-based medicine or involved in guideline development, journal editors, and manuscript reviewers.},
	language = {eng},
	number = {1},
	journal = {Annals of Internal Medicine},
	author = {Wolff, Robert F. and Moons, Karel G. M. and Riley, Richard D. and Whiting, Penny F. and Westwood, Marie and Collins, Gary S. and Reitsma, Johannes B. and Kleijnen, Jos and Mallett, Sue and {PROBAST Group†}},
	month = jan,
	year = {2019},
	pmid = {30596875},
	keywords = {Humans, Bias, Research Design, Decision Support Techniques, Models, Statistical, Delphi Technique, Diagnosis, Prognosis, Systematic Reviews as Topic},
	pages = {51--58},
	file = {Accepted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9A83DJEA\\Wolff et al. - 2019 - PROBAST A Tool to Assess the Risk of Bias and App.pdf:application/pdf},
}

@article{heus_uniformity_2019,
	title = {Uniformity in measuring adherence to reporting guidelines: the example of {TRIPOD} for assessing completeness of reporting of prediction model studies},
	volume = {9},
	issn = {2044-6055, 2044-6055},
	shorttitle = {Uniformity in measuring adherence to reporting guidelines},
	url = {https://bmjopen.bmj.com/lookup/doi/10.1136/bmjopen-2018-025611},
	doi = {10.1136/bmjopen-2018-025611},
	abstract = {To promote uniformity in measuring adherence to the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) statement, a reporting guideline for diagnostic and prognostic prediction model studies, and thereby facilitate comparability of future studies assessing its impact, we transformed the original 22 TRIPOD items into an adherence assessment form and defined adherence scoring rules. TRIPOD specific challenges encountered were the existence of different types of prediction model studies and possible combinations of these within publications. More general issues included dealing with multiple reporting elements, reference to information in another publication, and nonapplicability of items. We recommend our adherence assessment form to be used by anyone (eg, researchers, reviewers, editors) evaluating adherence to TRIPOD, to make these assessments comparable. In general, when developing a form to assess adherence to a reporting guideline, we recommend formulating specific adherence elements (if needed multiple per reporting guideline item) using unambiguous wording and the consideration of issues of applicability in advance.},
	language = {en},
	number = {4},
	urldate = {2022-01-17},
	journal = {BMJ Open},
	author = {Heus, Pauline and Damen, Johanna A A G and Pajouheshnia, Romin and Scholten, Rob J P M and Reitsma, Johannes B and Collins, Gary S and Altman, Douglas G and Moons, Karel G M and Hooft, Lotty},
	month = apr,
	year = {2019},
	pages = {e025611},
	file = {Heus et al. - 2019 - Uniformity in measuring adherence to reporting gui.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UWTUDKNG\\Heus et al. - 2019 - Uniformity in measuring adherence to reporting gui.pdf:application/pdf},
}

@article{collins_protocol_2021,
	title = {Protocol for development of a reporting guideline ({TRIPOD}-{AI}) and risk of bias tool ({PROBAST}-{AI}) for diagnostic and prognostic prediction model studies based on artificial intelligence},
	volume = {11},
	copyright = {© Author(s) (or their employer(s)) 2021. Re-use permitted under CC BY. Published by BMJ.. https://creativecommons.org/licenses/by/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported (CC BY 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See: https://creativecommons.org/licenses/by/4.0/.},
	issn = {2044-6055, 2044-6055},
	url = {https://bmjopen.bmj.com/content/11/7/e048008},
	doi = {10.1136/bmjopen-2020-048008},
	abstract = {Introduction The Transparent Reporting of a multivariable prediction model of Individual Prognosis Or Diagnosis (TRIPOD) statement and the Prediction model Risk Of Bias ASsessment Tool (PROBAST) were both published to improve the reporting and critical appraisal of prediction model studies for diagnosis and prognosis. This paper describes the processes and methods that will be used to develop an extension to the TRIPOD statement (TRIPOD-artificial intelligence, AI) and the PROBAST (PROBAST-AI) tool for prediction model studies that applied machine learning techniques.
Methods and analysis TRIPOD-AI and PROBAST-AI will be developed following published guidance from the EQUATOR Network, and will comprise five stages. Stage 1 will comprise two systematic reviews (across all medical fields and specifically in oncology) to examine the quality of reporting in published machine-learning-based prediction model studies. In stage 2, we will consult a diverse group of key stakeholders using a Delphi process to identify items to be considered for inclusion in TRIPOD-AI and PROBAST-AI. Stage 3 will be virtual consensus meetings to consolidate and prioritise key items to be included in TRIPOD-AI and PROBAST-AI. Stage 4 will involve developing the TRIPOD-AI checklist and the PROBAST-AI tool, and writing the accompanying explanation and elaboration papers. In the final stage, stage 5, we will disseminate TRIPOD-AI and PROBAST-AI via journals, conferences, blogs, websites (including TRIPOD, PROBAST and EQUATOR Network) and social media. TRIPOD-AI will provide researchers working on prediction model studies based on machine learning with a reporting guideline that can help them report key details that readers need to evaluate the study quality and interpret its findings, potentially reducing research waste. We anticipate PROBAST-AI will help researchers, clinicians, systematic reviewers and policymakers critically appraise the design, conduct and analysis of machine learning based prediction model studies, with a robust standardised tool for bias evaluation.
Ethics and dissemination Ethical approval has been granted by the Central University Research Ethics Committee, University of Oxford on 10-December-2020 (R73034/RE001). Findings from this study will be disseminated through peer-review publications.
PROSPERO registration number CRD42019140361 and CRD42019161764.},
	language = {en},
	number = {7},
	urldate = {2022-01-17},
	journal = {BMJ Open},
	author = {Collins, Gary S. and Dhiman, Paula and Navarro, Constanza L. Andaur and Ma, Jie and Hooft, Lotty and Reitsma, Johannes B. and Logullo, Patricia and Beam, Andrew L. and Peng, Lily and Calster, Ben Van and Smeden, Maarten van and Riley, Richard D. and Moons, Karel GM},
	month = jul,
	year = {2021},
	pmid = {34244270},
	note = {Publisher: British Medical Journal Publishing Group
Section: Medical publishing and peer review},
	keywords = {epidemiology, general medicine (see internal medicine), statistics \& research methods},
	pages = {e048008},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\NVGZG2B9\\Collins et al. - 2021 - Protocol for development of a reporting guideline .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CCQXPMGP\\e048008.html:text/html},
}

@article{vollmer_machine_2020,
	title = {Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness},
	volume = {368},
	copyright = {© Author(s) (or their employer(s)) 2019. Re-use permitted under CC             BY-NC. No commercial re-use. See rights and permissions. Published by             BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
	issn = {1756-1833},
	shorttitle = {Machine learning and artificial intelligence research for patient benefit},
	url = {https://www.bmj.com/content/368/bmj.l6927},
	doi = {10.1136/bmj.l6927},
	abstract = {{\textless}p{\textgreater}Machine learning, artificial intelligence, and other modern statistical methods are providing new opportunities to operationalise previously untapped and rapidly growing sources of data for patient benefit. Despite much promising research currently being undertaken, particularly in imaging, the literature as a whole lacks transparency, clear reporting to facilitate replicability, exploration for potential ethical concerns, and clear demonstrations of effectiveness. Among the many reasons why these problems exist, one of the most important (for which we provide a preliminary solution here) is the current lack of best practice guidance specific to machine learning and artificial intelligence. However, we believe that interdisciplinary groups pursuing research and impact projects involving machine learning and artificial intelligence for health would benefit from explicitly addressing a series of questions concerning transparency, reproducibility, ethics, and effectiveness (TREE). The 20 critical questions proposed here provide a framework for research groups to inform the design, conduct, and reporting; for editors and peer reviewers to evaluate contributions to the literature; and for patients, clinicians and policy makers to critically appraise where new findings may deliver patient benefit. {\textless}/p{\textgreater}},
	language = {en},
	urldate = {2022-01-17},
	journal = {BMJ},
	author = {Vollmer, Sebastian and Mateen, Bilal A. and Bohner, Gergo and Király, Franz J. and Ghani, Rayid and Jonsson, Pall and Cumbers, Sarah and Jonas, Adrian and McAllister, Katherine S. L. and Myles, Puja and Grainger, David and Birse, Mark and Branson, Richard and Moons, Karel G. M. and Collins, Gary S. and Ioannidis, John P. A. and Holmes, Chris and Hemingway, Harry},
	month = mar,
	year = {2020},
	pmid = {32198138},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	pages = {l6927},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DQGLAT9S\\Vollmer et al. - 2020 - Machine learning and artificial intelligence resea.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\LITRBDNN\\bmj.l6927.html:text/html},
}

@article{schramowski_making_2020-1,
	title = {Making deep neural networks right for the right scientific reasons by interacting with their explanations},
	volume = {2},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-0212-3},
	doi = {10.1038/s42256-020-0212-3},
	abstract = {Deep neural networks have demonstrated excellent performances in many real-world applications. Unfortunately, they may show Clever Hans-like behaviour (making use of confounding factors within datasets) to achieve high performance. In this work we introduce the novel learning setting of explanatory interactive learning and illustrate its benefits on a plant phenotyping research task. Explanatory interactive learning adds the scientist into the training loop, who interactively revises the original model by providing feedback on its explanations. Our experimental results demonstrate that explanatory interactive learning can help to avoid Clever Hans moments in machine learning and encourages (or discourages, if appropriate) trust in the underlying model.},
	language = {en},
	number = {8},
	urldate = {2022-01-18},
	journal = {Nature Machine Intelligence},
	author = {Schramowski, Patrick and Stammer, Wolfgang and Teso, Stefano and Brugger, Anna and Herbert, Franziska and Shao, Xiaoting and Luigs, Hans-Georg and Mahlein, Anne-Katrin and Kersting, Kristian},
	month = aug,
	year = {2020},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 8
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Machine learning;Plant sciences
Subject\_term\_id: machine-learning;plant-sciences},
	keywords = {Machine learning, Plant sciences},
	pages = {476--486},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\I8U4WQ6V\\Schramowski et al. - 2020 - Making deep neural networks right for the right sc.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\26E9CH6S\\s42256-020-0212-3.html:text/html},
}

@article{neto_using_2018,
	title = {Using permutations to detect, quantify and correct for confounding in machine learning predictions},
	url = {http://arxiv.org/abs/1805.07465},
	abstract = {Clinical machine learning applications are often plagued with confounders that are clinically irrelevant, but can still artiﬁcially boost the predictive performance of the algorithms. Confounding is especially problematic in mobile health studies run “in the wild”, where it is challenging to balance the demographic characteristics of participants that self select to enter the study. An eﬀective approach to remove the inﬂuence of confounders is to match samples in order to improve the balance in the data. The caveat is that we end-up with a smaller number of participants to train and evaluate the machine learning algorithm. Alternative confounding adjustment methods that make more eﬃcient use of the data (e.g., inverse probability weighting) usually rely on modeling assumptions, and it is unclear how robust these methods are to violations of these assumptions. Here, rather than proposing a new approach to prevent/reduce the learning of confounding signals by a machine learning algorithm, we develop novel statistical tools to detect, quantify and correct for the inﬂuence of observed confounders. Our tools are based on restricted and standard permutation approaches and can be used to evaluate how well a confounding adjustment method is actually working. We use restricted permutations to test if an algorithm has learned disease signal in the presence of confounding signal, and to develop a novel statistical test to detect confounding learning per se. Furthermore, we prove that restricted permutations provide an alternative method to compute partial correlations, and use this result as a motivation to develop a novel approach to estimate the corrected predictive performance of a learner. We evaluate the statistical properties of our methods in simulation studies.},
	language = {en},
	urldate = {2022-01-18},
	journal = {arXiv:1805.07465 [stat]},
	author = {Neto, Elias Chaibub},
	month = nov,
	year = {2018},
	note = {arXiv: 1805.07465},
	keywords = {Statistics - Applications},
	file = {Neto - 2018 - Using permutations to detect, quantify and correct.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RNMRN539\\Neto - 2018 - Using permutations to detect, quantify and correct.pdf:application/pdf},
}

@article{lu_metadata_2021,
	title = {Metadata {Normalization}},
	url = {http://arxiv.org/abs/2104.09052},
	abstract = {Batch Normalization (BN) and its variants have delivered tremendous success in combating the covariate shift induced by the training step of deep learning methods. While these techniques normalize feature distributions by standardizing with batch statistics, they do not correct the inﬂuence on features from extraneous variables or multiple distributions. Such extra variables, referred to as metadata here, may create bias or confounding effects (e.g., race when classifying gender from face images). We introduce the Metadata Normalization (MDN) layer, a new batch-level operation which can be used end-to-end within the training framework, to correct the inﬂuence of metadata on feature distributions. MDN adopts a regression analysis technique traditionally used for preprocessing to remove (regress out) the metadata effects on model features during training. We utilize a metric basemdluo3n5d5i@stastnacnefocrodr.erdeluation to quantify the distribution bias from the metadata and demonstrate that our method successfully removes metadata effects on four diverse settings: one synthetic, one 2D image, one video, and one 3D medical image dataset.},
	language = {en},
	urldate = {2022-01-18},
	journal = {arXiv:2104.09052 [cs]},
	author = {Lu, Mandy and Zhao, Qingyu and Zhang, Jiequan and Pohl, Kilian M. and Fei-Fei, Li and Niebles, Juan Carlos and Adeli, Ehsan},
	month = may,
	year = {2021},
	note = {arXiv: 2104.09052},
	keywords = {Computer Science - Machine Learning},
	file = {Lu et al. - 2021 - Metadata Normalization.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\G4FBRZMC\\Lu et al. - 2021 - Metadata Normalization.pdf:application/pdf},
}

@article{zhao_training_2020-2,
	title = {Training confounder-free deep learning models for medical applications},
	volume = {11},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-020-19784-9},
	doi = {10.1038/s41467-020-19784-9},
	abstract = {Abstract
            
              The presence of confounding effects (or biases) is one of the most critical challenges in using deep learning to advance discovery in medical imaging studies. Confounders affect the relationship between input data (e.g., brain MRIs) and output variables (e.g., diagnosis). Improper modeling of those relationships often results in spurious and biased associations. Traditional machine learning and statistical models minimize the impact of confounders by, for example, matching data sets, stratifying data, or residualizing imaging measurements. Alternative strategies are needed for state-of-the-art deep learning models that use end-to-end training to automatically extract informative features from large set of images. In this article, we introduce an end-to-end approach for deriving features invariant to confounding factors while accounting for intrinsic correlations between the confounder(s) and prediction outcome. The method does so by exploiting concepts from traditional statistical methods and recent fair machine learning schemes. We evaluate the method on predicting the diagnosis of HIV solely from Magnetic Resonance Images (MRIs), identifying morphological sex differences in adolescence from those of the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA), and determining the bone age from X-ray images of children. The results show that our method can accurately predict while reducing biases associated with confounders. The code is available at
              https://github.com/qingyuzhao/br-net
              .},
	language = {en},
	number = {1},
	urldate = {2022-01-18},
	journal = {Nature Communications},
	author = {Zhao, Qingyu and Adeli, Ehsan and Pohl, Kilian M.},
	month = dec,
	year = {2020},
	pages = {6010},
	file = {Zhao et al. - 2020 - Training confounder-free deep learning models for .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DZHFGXCP\\Zhao et al. - 2020 - Training confounder-free deep learning models for .pdf:application/pdf},
}

@article{sert_arrive_2020,
	title = {The {ARRIVE} guidelines 2.0: {Updated} guidelines for reporting animal research},
	volume = {18},
	issn = {1545-7885},
	shorttitle = {The {ARRIVE} guidelines 2.0},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000410},
	doi = {10.1371/journal.pbio.3000410},
	abstract = {Reproducible science requires transparent reporting. The ARRIVE guidelines (Animal Research: Reporting of In Vivo Experiments) were originally developed in 2010 to improve the reporting of animal research. They consist of a checklist of information to include in publications describing in vivo experiments to enable others to scrutinise the work adequately, evaluate its methodological rigour, and reproduce the methods and results. Despite considerable levels of endorsement by funders and journals over the years, adherence to the guidelines has been inconsistent, and the anticipated improvements in the quality of reporting in animal research publications have not been achieved. Here, we introduce ARRIVE 2.0. The guidelines have been updated and information reorganised to facilitate their use in practice. We used a Delphi exercise to prioritise and divide the items of the guidelines into 2 sets, the “ARRIVE Essential 10,” which constitutes the minimum requirement, and the “Recommended Set,” which describes the research context. This division facilitates improved reporting of animal research by supporting a stepwise approach to implementation. This helps journal editors and reviewers verify that the most important items are being reported in manuscripts. We have also developed the accompanying Explanation and Elaboration (E\&E) document, which serves (1) to explain the rationale behind each item in the guidelines, (2) to clarify key concepts, and (3) to provide illustrative examples. We aim, through these changes, to help ensure that researchers, reviewers, and journal editors are better equipped to improve the rigour and transparency of the scientific process and thus reproducibility.},
	language = {en},
	number = {7},
	urldate = {2022-01-18},
	journal = {PLOS Biology},
	author = {Sert, Nathalie Percie du and Hurst, Viki and Ahluwalia, Amrita and Alam, Sabina and Avey, Marc T. and Baker, Monya and Browne, William J. and Clark, Alejandra and Cuthill, Innes C. and Dirnagl, Ulrich and Emerson, Michael and Garner, Paul and Holgate, Stephen T. and Howells, David W. and Karp, Natasha A. and Lazic, Stanley E. and Lidster, Katie and MacCallum, Catriona J. and Macleod, Malcolm and Pearl, Esther J. and Petersen, Ole H. and Rawle, Frances and Reynolds, Penny and Rooney, Kieron and Sena, Emily S. and Silberberg, Shai D. and Steckler, Thomas and Würbel, Hanno},
	month = jul,
	year = {2020},
	note = {Publisher: Public Library of Science},
	keywords = {Experimental design, Reproducibility, Research design, Research reporting guidelines, Research validity, Science policy, Statistical data, Statistical methods},
	pages = {e3000410},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\CR2T66IA\\Sert et al. - 2020 - The ARRIVE guidelines 2.0 Updated guidelines for .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RWY2H8SR\\article.html:text/html},
}

@article{zeng_methodological_2015,
	title = {The methodological quality assessment tools for preclinical and clinical studies, systematic review and meta-analysis, and clinical practice guideline: a systematic review},
	volume = {8},
	issn = {1756-5391},
	shorttitle = {The methodological quality assessment tools for preclinical and clinical studies, systematic review and meta-analysis, and clinical practice guideline},
	doi = {10.1111/jebm.12141},
	abstract = {OBJECTIVE: To systematically review the methodological assessment tools for pre-clinical and clinical studies, systematic review and meta-analysis, and clinical practice guideline.
METHODS: We searched PubMed, the Cochrane Handbook for Systematic Reviews of Interventions, Joanna Briggs Institute (JBI) Reviewers Manual, Centre for Reviews and Dissemination, Critical Appraisal Skills Programme (CASP), Scottish Intercollegiate Guidelines Network (SIGN), and the National Institute for Clinical Excellence (NICE) up to May 20th, 2014. Two authors selected studies and extracted data; quantitative analysis was performed to summarize the characteristics of included tools.
RESULTS: We included a total of 21 assessment tools for analysis. A number of tools were developed by academic organizations, and some were developed by only a small group of researchers. The JBI developed the highest number of methodological assessment tools, with CASP coming second. Tools for assessing the methodological quality of randomized controlled studies were most abundant. The Cochrane Collaboration's tool for assessing risk of bias is the best available tool for assessing RCTs. For cohort and case-control studies, we recommend the use of the Newcastle-Ottawa Scale. The Methodological Index for Non-Randomized Studies (MINORS) is an excellent tool for assessing non-randomized interventional studies, and the Agency for Healthcare Research and Quality (ARHQ) methodology checklist is applicable for cross-sectional studies. For diagnostic accuracy test studies, the Quality Assessment of Diagnostic Accuracy Studies-2 (QUADAS-2) tool is recommended; the SYstematic Review Centre for Laboratory animal Experimentation (SYRCLE) risk of bias tool is available for assessing animal studies; Assessment of Multiple Systematic Reviews (AMSTAR) is a measurement tool for systematic reviews/meta-analyses; an 18-item tool has been developed for appraising case series studies, and the Appraisal of Guidelines, Research and Evaluation (AGREE)-II instrument is widely used to evaluate clinical practice guidelines.
CONCLUSIONS: We have successfully identified a variety of methodological assessment tools for different types of study design. However, further efforts in the development of critical appraisal tools are warranted since there is currently a lack of such tools for other fields, e.g. genetic studies, and some existing tools (nested case-control studies and case reports, for example) are in need of updating to be in line with current research practice and rigor. In addition, it is very important that all critical appraisal tools remain subjective and performance bias is effectively avoided.},
	language = {eng},
	number = {1},
	journal = {Journal of Evidence-Based Medicine},
	author = {Zeng, Xiantao and Zhang, Yonggang and Kwong, Joey S. W. and Zhang, Chao and Li, Sheng and Sun, Feng and Niu, Yuming and Du, Liang},
	month = feb,
	year = {2015},
	pmid = {25594108},
	keywords = {Animals, Humans, Research Design, Biomedical Research, systematic review, Clinical practice guideline, Clinical Studies as Topic, Data Accuracy, Evidence-Based Medicine, meta-analysis, Meta-Analysis as Topic, methodological quality, Practice Guidelines as Topic, primary study, Review Literature as Topic, risk of bias},
	pages = {2--10},
}

@inproceedings{mughees_efficient_2016,
	title = {Efficient {Deep} {Auto}-{Encoder} {Learning} for the {Classification} of {Hyperspectral} {Images}},
	doi = {10.1109/ICVRV.2016.16},
	abstract = {Hyperspectral Image (HSI) classification is one of the most pervasive issue in hyperspectral remote sensing field. Deep learning is an efficient learning algorithm that has been recently applied to HSI classification. This paper proposes a new spectralspatial HSI classification method based on the deep features extraction using stacked-auto-encoders (SAE) and unsupervised HIS segmentation. Specifically, first the SAE model is exploited as a classical spectral information-based classifier to extract the deep features. Second, spatial dominated information is extracted by using effective boundary adjustment based segmentation technique. Finally, maximum voting criteria is used to merge the extracted spectral and spatial features, which results into the accurate spectral-spatial HSI classification. Experimental results with widely-used hyperspectral data confirms that the new spectral and spatial classification approach is able to improve results significantly in terms of classification accuracies.},
	booktitle = {2016 {International} {Conference} on {Virtual} {Reality} and {Visualization} ({ICVRV})},
	author = {Mughees, Atif and Tao, Linmi},
	month = sep,
	year = {2016},
	keywords = {Support vector machines, Hyperspectral imaging, Feature extraction, Training, deep learning, Image segmentation, Segmentation, hyperspectral Image Classification, Image reconstruction, Stacked auto-encoder (SAE), support vector machine (SVM)},
	pages = {44--51},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VKLINW3C\\7938171.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EDTB6ZY9\\Mughees and Tao - 2016 - Efficient Deep Auto-Encoder Learning for the Class.pdf:application/pdf},
}

@article{ravi_manifold_2017,
	title = {Manifold {Embedding} and {Semantic} {Segmentation} for {Intraoperative} {Guidance} {With} {Hyperspectral} {Brain} {Imaging}},
	volume = {36},
	issn = {1558-254X},
	doi = {10.1109/TMI.2017.2695523},
	abstract = {Recent advances in hyperspectral imaging have made it a promising solution for intra-operative tissue characterization, with the advantages of being non-contact, non-ionizing, and non-invasive. Working with hyperspectral images in vivo, however, is not straightforward as the high dimensionality of the data makes real-time processing challenging. In this paper, a novel dimensionality reduction scheme and a new processing pipeline are introduced to obtain a detailed tumor classification map for intra-operative margin definition during brain surgery. However, existing approaches to dimensionality reduction based on manifold embedding can be time consuming and may not guarantee a consistent result, thus hindering final tissue classification. The proposed framework aims to overcome these problems through a process divided into two steps: dimensionality reduction based on an extension of the T-distributed stochastic neighbor approach is first performed and then a semantic segmentation technique is applied to the embedded results by using a Semantic Texton Forest for tissue classification. Detailed in vivo validation of the proposed method has been performed to demonstrate the potential clinical value of the system.},
	number = {9},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Ravì, Daniele and Fabelo, Himar and Callic, Gustavo Marrero and Yang, Guang-Zhong},
	month = sep,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Hyperspectral imaging, hyperspectral imaging, Image segmentation, Cancer, Semantics, Brain, brain cancer detection, Manifold embedding, Manifolds, semantic segmentation, Tumors},
	pages = {1845--1857},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4EGEDQGB\\7907323.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\S72S46HS\\Ravì et al. - 2017 - Manifold Embedding and Semantic Segmentation for I.pdf:application/pdf},
}

@article{fabelo_spatio-spectral_2018-1,
	title = {Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0193721},
	doi = {10.1371/journal.pone.0193721},
	abstract = {Surgery for brain cancer is a major problem in neurosurgery. The diffuse infiltration into the surrounding normal brain by these tumors makes their accurate identification by the naked eye difficult. Since surgery is the common treatment for brain cancer, an accurate radical resection of the tumor leads to improved survival rates for patients. However, the identification of the tumor boundaries during surgery is challenging. Hyperspectral imaging is a non-contact, non-ionizing and non-invasive technique suitable for medical diagnosis. This study presents the development of a novel classification method taking into account the spatial and spectral characteristics of the hyperspectral images to help neurosurgeons to accurately determine the tumor boundaries in surgical-time during the resection, avoiding excessive excision of normal tissue or unintentionally leaving residual tumor. The algorithm proposed in this study to approach an efficient solution consists of a hybrid framework that combines both supervised and unsupervised machine learning methods. Firstly, a supervised pixel-wise classification using a Support Vector Machine classifier is performed. The generated classification map is spatially homogenized using a one-band representation of the HS cube, employing the Fixed Reference t-Stochastic Neighbors Embedding dimensional reduction algorithm, and performing a K-Nearest Neighbors filtering. The information generated by the supervised stage is combined with a segmentation map obtained via unsupervised clustering employing a Hierarchical K-Means algorithm. The fusion is performed using a majority voting approach that associates each cluster with a certain class. To evaluate the proposed approach, five hyperspectral images of surface of the brain affected by glioblastoma tumor in vivo from five different patients have been used. The final classification maps obtained have been analyzed and validated by specialists. These preliminary results are promising, obtaining an accurate delineation of the tumor area.},
	language = {en},
	number = {3},
	urldate = {2022-02-02},
	journal = {PLOS ONE},
	author = {Fabelo, Himar and Ortega, Samuel and Ravi, Daniele and Kiran, B. Ravi and Sosa, Coralia and Bulters, Diederik and Callicó, Gustavo M. and Bulstrode, Harry and Szolna, Adam and Piñeiro, Juan F. and Kabwama, Silvester and Madroñal, Daniel and Lazcano, Raquel and J-O’Shanahan, Aruma and Bisshopp, Sara and Hernández, María and Báez, Abelardo and Yang, Guang-Zhong and Stanciulescu, Bogdan and Salvador, Rubén and Juárez, Eduardo and Sarmiento, Roberto},
	month = mar,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Blood vessels, Cancer detection and diagnosis, Surgical and invasive medical procedures, Surgical oncology, Tumor resection, Cancers and neoplasms, Malignant tumors},
	pages = {e0193721},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PU26Y6A7\\Fabelo et al. - 2018 - Spatio-spectral classification of hyperspectral im.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FV4SCBWV\\article.html:text/html},
}

@inproceedings{trajanovski_tumor_2019,
	title = {Tumor {Semantic} {Segmentation} in {Hyperspectral} {Images} using {Deep} {Learning}},
	abstract = {Real-time feedback based on hyperspectral images (HSI) to a surgeon can lead to a higher precision and additional insights compared to the standard techniques. To the best of our knowledge, deep learning with semantic segmentation utilizing both visual (VIS) and infrared channels (NIR) has never been exploited with the HSI data with human tumors. We propose using channels selection with U-Net deep neural network for tumor segmentation in hyperspectral images. The proposed method, based on bigger patches, accounts for bigger spatial context and achieves better results (average dice coeﬃcient 0.89 ± 0.07 and area under the ROC-curve AUC 0.93 ± 0.04) than pixel-level spectral and structural approaches in a clinical data set with tongue squamous cell carcinoma. The importance of VIS channel for the performance is higher, but NIR contribution is non-negligible.},
	language = {en},
	booktitle = {Extended {Abstract} {Track}},
	author = {Trajanovski, Stojan and Shan, Caifeng and Weijtmans, Pim J C},
	year = {2019},
	pages = {4},
	file = {Trajanovski et al. - Tumor Semantic Segmentation in Hyperspectral Image.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2TWWY8ML\\Trajanovski et al. - Tumor Semantic Segmentation in Hyperspectral Image.pdf:application/pdf},
}

@article{fabelo_surgical_2019,
	title = {Surgical {Aid} {Visualization} {System} for {Glioblastoma} {Tumor} {Identification} based on {Deep} {Learning} and {In}-{Vivo} {Hyperspectral} {Images} of {Human} {Patients}},
	volume = {10951},
	issn = {0277-786X},
	doi = {10.1117/12.2512569},
	abstract = {Brain cancer surgery has the goal of performing an accurate resection of the tumor and preserving as much as possible the quality of life of the patient. There is a clinical need to develop non-invasive techniques that can provide reliable assistance for tumor resection in real-time during surgical procedures. Hyperspectral imaging (HSI) arises as a new, noninvasive and non-ionizing technique that can assist neurosurgeons during this difficult task. In this paper, we explore the use of deep learning (DL) techniques for processing hyperspectral (HS) images of in-vivo human brain tissue. We developed a surgical aid visualization system capable of offering guidance to the operating surgeon to achieve a successful and accurate tumor resection. The employed HS database is composed of 26 in-vivo hypercubes from 16 different human patients, among which 258,810 labelled pixels were used for evaluation. The proposed DL methods achieve an overall accuracy of 95\% and 85\% for binary and multiclass classifications, respectively. The proposed visualization system is able to generate a classification map that is formed by the combination of the DL map and an unsupervised clustering via a majority voting algorithm. This map can be adjusted by the operating surgeon to find the suitable configuration for the current situation during the surgical procedure.},
	language = {eng},
	journal = {Proceedings of SPIE--the International Society for Optical Engineering},
	author = {Fabelo, Himar and Halicek, Martin and Ortega, Samuel and Szolna, Adam and Morera, Jesus and Sarmiento, Roberto and Callico, Gustavo M. and Fei, Baowei},
	month = feb,
	year = {2019},
	pmid = {31447494},
	pmcid = {PMC6708415},
	keywords = {hyperspectral imaging, classifier, deep learning, intraoperative imaging, Brain tumor, cancer surgery, convolutional neural network (CNN), supervised classification},
	pages = {1095110},
	file = {Accepted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JN5FRVCU\\Fabelo et al. - 2019 - Surgical Aid Visualization System for Glioblastoma.pdf:application/pdf},
}

@article{shotton_semantic_2008,
	title = {Semantic {Texton} {Forests} for {Image} {Categorization} and {Segmentation}},
	abstract = {We propose semantic texton forests, efﬁcient and powerful new low-level features. These are ensembles of decision trees that act directly on image pixels, and therefore do not need the expensive computation of ﬁlter-bank responses or local descriptors. They are extremely fast to both train and test, especially compared with k-means clustering and nearest-neighbor assignment of feature descriptors. The nodes in the trees provide (i) an implicit hierarchical clustering into semantic textons, and (ii) an explicit local classiﬁcation estimate. Our second contribution, the bag of semantic textons, combines a histogram of semantic textons over an image region with a region prior category distribution. The bag of semantic textons is computed over the whole image for categorization, and over local rectangular regions for segmentation. Including both histogram and region prior allows our segmentation algorithm to exploit both textural and semantic context. Our third contribution is an image-level prior for segmentation that emphasizes those categories that the automatic categorization believes to be present. We evaluate on two datasets including the very challenging VOC 2007 segmentation dataset. Our results signiﬁcantly advance the state-of-the-art in segmentation accuracy, and furthermore, our use of efﬁcient decision forests gives at least a ﬁve-fold increase in execution speed.},
	language = {en},
	author = {Shotton, Jamie and Johnson, Matthew and Cipolla, Roberto},
	year = {2008},
	pages = {8},
	file = {Shotton et al. - Semantic Texton Forests for Image Categorization a.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\82LYZB5U\\Shotton et al. - Semantic Texton Forests for Image Categorization a.pdf:application/pdf},
}

@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	number = {86},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605},
	file = {vandermaaten08a.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PKMH4UAR\\vandermaaten08a.pdf:application/pdf},
}

@misc{noauthor_applied_nodate,
	title = {Applied {Sciences} {\textbar} {Free} {Full}-{Text} {\textbar} {Oral} and {Dental} {Spectral} {Image} {Database}—{ODSI}-{DB}},
	url = {https://www.mdpi.com/2076-3417/10/20/7246},
	urldate = {2022-02-07},
	file = {Applied Sciences | Free Full-Text | Oral and Dental Spectral Image Database—ODSI-DB:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\L87KKMUX\\7246.html:text/html},
}

@article{hyttinen_oral_2020,
	title = {Oral and {Dental} {Spectral} {Image} {Database}—{ODSI}-{DB}},
	volume = {10},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/20/7246},
	doi = {10.3390/app10207246},
	abstract = {The most common imaging methods used in dentistry are X-ray imaging and RGB color photography. However, both imaging methods provide only a limited amount of information on the wavelength-dependent optical properties of the hard and soft tissues in the mouth. Spectral imaging, on the other hand, provides signiﬁcantly more information on the medically relevant dental and oral features (e.g. caries, calculus, and gingivitis). Due to this, we constructed a spectral imaging setup and acquired 316 oral and dental reﬂectance spectral images, 215 of which are annotated by medical experts, of 30 human test subjects. Spectral images of the subjects’ faces and other areas of interest were captured, along with other medically relevant information (e.g., pulse and blood pressure). We collected these oral, dental, and face spectral images, their annotations and metadata into a publicly available database that we describe in this paper. This oral and dental spectral image database (ODSI-DB) provides a vast amount of data that can be used for developing, e.g., pattern recognition and machine vision applications for dentistry.},
	language = {en},
	number = {20},
	urldate = {2022-02-07},
	journal = {Applied Sciences},
	author = {Hyttinen, Joni and Fält, Pauli and Jäsberg, Heli and Kullaa, Arja and Hauta-Kasari, Markku},
	month = oct,
	year = {2020},
	pages = {7246},
	file = {Hyttinen et al. - 2020 - Oral and Dental Spectral Image Database—ODSI-DB.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TKEEJRI2\\Hyttinen et al. - 2020 - Oral and Dental Spectral Image Database—ODSI-DB.pdf:application/pdf},
}

@article{fabelo_-vivo_2019-1,
	title = {In-{Vivo} {Hyperspectral} {Human} {Brain} {Image} {Database} for {Brain} {Cancer} {Detection}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2904788},
	abstract = {The use of hyperspectral imaging for medical applications is becoming more common in recent years. One of the main obstacles that researchers find when developing hyperspectral algorithms for medical applications is the lack of specific, publicly available, and hyperspectral medical data. The work described in this paper was developed within the framework of the European project HELICoiD (HypErspectraL Imaging Cancer Detection), which had as a main goal the application of hyperspectral imaging to the delineation of brain tumors in real-time during neurosurgical operations. In this paper, the methodology followed to generate the first hyperspectral database of in-vivo human brain tissues is presented. Data was acquired employing a customized hyperspectral acquisition system capable of capturing information in the Visual and Near InfraRed (VNIR) range from 400 to 1000 nm. Repeatability was assessed for the cases where two images of the same scene were captured consecutively. The analysis reveals that the system works more efficiently in the spectral range between 450 and 900 nm. A total of 36 hyperspectral images from 22 different patients were obtained. From these data, more than 300 000 spectral signatures were labeled employing a semi-automatic methodology based on the spectral angle mapper algorithm. Four different classes were defined: normal tissue, tumor tissue, blood vessel, and background elements. All the hyperspectral data has been made available in a public repository.},
	journal = {IEEE Access},
	author = {Fabelo, Himar and Ortega, Samuel and Szolna, Adam and Bulters, Diederik and Piñeiro, Juan F. and Kabwama, Silvester and J-O'Shanahan, Aruma and Bulstrode, Harry and Bisshopp, Sara and Kiran, B. Ravi and Ravi, Daniele and Lazcano, Raquel and Madroñal, Daniel and Sosa, Coralia and Espino, Carlos and Marquez, Mariano and De La Luz Plaza, María and Camacho, Rafael and Carrera, David and Hernández, María and Callicó, Gustavo M. and Morera Molina, Jesús and Stanciulescu, Bogdan and Yang, Guang-Zhong and Salvador, Rubén and Juárez, Eduardo and Sanz, César and Sarmiento, Roberto},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Hyperspectral imaging, Surgery, medical diagnostic imaging, Brain, Tumors, biomedical imaging, cancer detection, Hospitals, image databases, Magnetic resonance imaging},
	pages = {39098--39116},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\X967PR5D\\8667294.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DC7N6Z2V\\Fabelo et al. - 2019 - In-Vivo Hyperspectral Human Brain Image Database f.pdf:application/pdf},
}

@inproceedings{hossam_automated_2021,
	title = {Automated {Dental} {Diagnosis} using {Deep} {Learning}},
	doi = {10.1109/ICCES54031.2021.9686185},
	abstract = {Dental Diseases affect billions of individuals world-wide and constitute to countries facing a significant health and economic burden. However, due to the noticeable decline in dental hygiene awareness and the absence of dental care resources, many oral conditions are left undiagnosed and untreated. Thus, emerged the need for a cheap and reliable tool that substitutes clinical visits and facilitates the dental diagnosis process. Despite the fact that deep learning methods have posed as a major asset in the medical diagnosis field, the utilization of its features in the field of dentistry is still a vaguely researched area. Therefore, the purpose of this paper is to make use of deep learning approaches and create a reliable self-examination tool that automatically diagnoses 17 different oral conditions. The study also pioneers in exploring another yet undiscovered research area as it tests the efficiency of the fairly recent ODSI-DB dataset; a public dataset of dental images manually annotated by experts in the field. A variety of deep learning models; CenterNet ResNet, EfficientNet and MobileNet, were studied and tested, and their performance was compared to identify the best method for diagnosis. Results revealed that out of the three tested models, MobileNet performed best as it achieved the most superior results, with classification, localization and regularization reaching an accuracy of 95.2 \%, 90.7\% and 91 \% respectively, making it the optimal choice for further developments.},
	booktitle = {2021 16th {International} {Conference} on {Computer} {Engineering} and {Systems} ({ICCES})},
	author = {Hossam, Abdelaziz and Mohamed, Kareem and Tarek, Reem and Elsayed, Asmaa and Mostafa, Hanya and Selim, Sahar},
	month = dec,
	year = {2021},
	keywords = {Deep Learning, Training, Biological system modeling, Deep learning, Dental Diagnosis, Dentistry, Diagnostic Tool, Location awareness, Medical diagnosis, Oral Health, Reliability, self-examination},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\K3A22EMG\\9686185.html:text/html},
}

@inproceedings{hossam_automated_2021-1,
	address = {Cairo, Egypt, Egypt},
	title = {Automated {Dental} {Diagnosis} using {Deep} {Learning}},
	isbn = {978-1-66540-867-7},
	url = {https://ieeexplore.ieee.org/document/9686185/},
	doi = {10.1109/ICCES54031.2021.9686185},
	abstract = {Dental Diseases affect billions of individuals worldwide and constitute to countries facing a signiﬁcant health and economic burden. However, due to the noticeable decline in dental hygiene awareness and the absence of dental care resources, many oral conditions are left undiagnosed and untreated. Thus, emerged the need for a cheap and reliable tool that substitutes clinical visits and facilitates the dental diagnosis process. Despite the fact that deep learning methods have posed as a major asset in the medical diagnosis ﬁeld, the utilization of its features in the ﬁeld of dentistry is still a vaguely researched area. Therefore, the purpose of this paper is to make use of deep learning approaches and create a reliable self-examination tool that automatically diagnoses 17 different oral conditions. The study also pioneers in exploring another yet undiscovered research area as it tests the efﬁciency of the fairly recent ODSI-DB dataset; a public dataset of dental images manually annotated by experts in the ﬁeld. A variety of deep learning models; CenterNet ResNet, EfﬁcientNet and MobileNet, were studied and tested, and their performance was compared to identify the best method for diagnosis. Results revealed that out of the three tested models, MobileNet performed best as it achieved the most superior results, with classiﬁcation, localization and regularization reaching an accuracy of 95.2\%, 90.7\% and 91\% respectively, making it the optimal choice for further developments.},
	language = {en},
	urldate = {2022-02-07},
	booktitle = {2021 16th {International} {Conference} on {Computer} {Engineering} and {Systems} ({ICCES})},
	publisher = {IEEE},
	author = {Hossam, Abdelaziz and Mohamed, Kareem and Tarek, Reem and Elsayed, Asmaa and Mostafa, Hanya and Selim, Sahar},
	month = dec,
	year = {2021},
	pages = {1--5},
	file = {Hossam et al. - 2021 - Automated Dental Diagnosis using Deep Learning.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\GXZ5UN75\\Hossam et al. - 2021 - Automated Dental Diagnosis using Deep Learning.pdf:application/pdf},
}

@inproceedings{fabelo_helicoid_2016,
	title = {{HELICoiD} project: a new use of hyperspectral imaging for brain cancer detection in real-time during neurosurgical operations},
	volume = {9860},
	shorttitle = {{HELICoiD} project},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9860/986002/HELICoiD-project--a-new-use-of-hyperspectral-imaging-for/10.1117/12.2223075.full},
	doi = {10.1117/12.2223075},
	abstract = {Hyperspectral images allow obtaining large amounts of information about the surface of the scene that is captured by the sensor. Using this information and a set of complex classification algorithms is possible to determine which material or substance is located in each pixel. The HELICoiD (HypErspectraL Imaging Cancer Detection) project is a European FET project that has the goal to develop a demonstrator capable to discriminate, with high precision, between normal and tumour tissues, operating in real-time, during neurosurgical operations. This demonstrator could help the neurosurgeons in the process of brain tumour resection, avoiding the excessive extraction of normal tissue and unintentionally leaving small remnants of tumour. Such precise delimitation of the tumour boundaries will improve the results of the surgery. The HELICoiD demonstrator is composed of two hyperspectral cameras obtained from Headwall. The first one in the spectral range from 400 to 1000 nm (visible and near infrared) and the second one in the spectral range from 900 to 1700 nm (near infrared). The demonstrator also includes an illumination system that covers the spectral range from 400 nm to 2200 nm. A data processing unit is in charge of managing all the parts of the demonstrator, and a high performance platform aims to accelerate the hyperspectral image classification process. Each one of these elements is installed in a customized structure specially designed for surgical environments. Preliminary results of the classification algorithms offer high accuracy (over 95\%) in the discrimination between normal and tumour tissues.},
	urldate = {2022-02-07},
	booktitle = {Hyperspectral {Imaging} {Sensors}: {Innovative} {Applications} and {Sensor} {Standards} 2016},
	publisher = {SPIE},
	author = {Fabelo, Himar and Ortega, Samuel and Kabwama, Silvester and Callico, Gustavo M. and Bulters, Diederik and Szolna, Adam and Pineiro, Juan F. and Sarmiento, Roberto},
	month = may,
	year = {2016},
	pages = {986002},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XM9LSWNS\\12.2223075.html:text/html},
}

@inproceedings{bannon_helicoid_2016,
	address = {Baltimore, Maryland, United States},
	title = {{HELICoiD} project: a new use of hyperspectral imaging for brain cancer detection in real-time during neurosurgical operations},
	shorttitle = {{HELICoiD} project},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2223075},
	doi = {10.1117/12.2223075},
	abstract = {Hyperspectral images allow obtaining large amounts of information about the surface of the scene that is captured by the sensor. Using this information and a set of complex classification algorithms is possible to determine which material or substance is located in each pixel. The HELICoiD (HypErspectraL Imaging Cancer Detection) project is a European FET project that has the goal to develop a demonstrator capable to discriminate, with high precision, between normal and tumour tissues, operating in real-time, during neurosurgical operations. This demonstrator could help the neurosurgeons in the process of brain tumour resection, avoiding the excessive extraction of normal tissue and unintentionally leaving small remnants of tumour. Such precise delimitation of the tumour boundaries will improve the results of the surgery. The HELICoiD demonstrator is composed of two hyperspectral cameras obtained from Headwall. The first one in the spectral range from 400 to 1000 nm (visible and near infrared) and the second one in the spectral range from 900 to 1700 nm (near infrared). The demonstrator also includes an illumination system that covers the spectral range from 400 nm to 2200 nm. A data processing unit is in charge of managing all the parts of the demonstrator, and a high performance platform aims to accelerate the hyperspectral image classification process. Each one of these elements is installed in a customized structure specially designed for surgical environments. Preliminary results of the classification algorithms offer high accuracy (over 95\%) in the discrimination between normal and tumour tissues.},
	language = {en},
	urldate = {2022-02-07},
	author = {Fabelo, Himar and Ortega, Samuel and Kabwama, Silvester and Callico, Gustavo M. and Bulters, Diederik and Szolna, Adam and Pineiro, Juan F. and Sarmiento, Roberto},
	editor = {Bannon, David P.},
	month = may,
	year = {2016},
	pages = {986002},
	file = {Fabelo et al. - 2016 - HELICoiD project a new use of hyperspectral imagi.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WULAWYF4\\Fabelo et al. - 2016 - HELICoiD project a new use of hyperspectral imagi.pdf:application/pdf},
}

@article{smith_dont_2018,
	title = {Don't {Decay} the {Learning} {Rate}, {Increase} the {Batch} {Size}},
	url = {http://arxiv.org/abs/1711.00489},
	abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate \${\textbackslash}epsilon\$ and scaling the batch size \$B {\textbackslash}propto {\textbackslash}epsilon\$. Finally, one can increase the momentum coefficient \$m\$ and scale \$B {\textbackslash}propto 1/(1-m)\$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to \$76.1{\textbackslash}\%\$ validation accuracy in under 30 minutes.},
	urldate = {2022-02-09},
	journal = {arXiv:1711.00489 [cs, stat]},
	author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
	month = feb,
	year = {2018},
	note = {arXiv: 1711.00489},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4E6ZNJAA\\Smith et al. - 2018 - Don't Decay the Learning Rate, Increase the Batch .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VNRHJDKQ\\1711.html:text/html},
}

@article{kandel_effect_2020,
	title = {The effect of batch size on the generalizability of the convolutional neural networks on a histopathology dataset},
	volume = {6},
	issn = {2405-9595},
	url = {https://www.sciencedirect.com/science/article/pii/S2405959519303455},
	doi = {10.1016/j.icte.2020.04.010},
	abstract = {Many hyperparameters have to be tuned to have a robust convolutional neural network that will be able to accurately classify images. One of the most important hyperparameters is the batch size, which is the number of images used to train a single forward and backward pass. In this study, the effect of batch size on the performance of convolutional neural networks and the impact of learning rates will be studied for image classification, specifically for medical images. To train the network faster, a VGG16 network with ImageNet weights was used in this experiment. Our results concluded that a higher batch size does not usually achieve high accuracy, and the learning rate and the optimizer used will have a significant impact as well. Lowering the learning rate and decreasing the batch size will allow the network to train better, especially in the case of fine-tuning.},
	language = {en},
	number = {4},
	urldate = {2022-02-09},
	journal = {ICT Express},
	author = {Kandel, Ibrahem and Castelli, Mauro},
	year = {2020},
	keywords = {Image classification, Convolutional neural networks, Deep learning, Batch size, Medical images},
	pages = {312--315},
	file = {Full Text:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\BXFLAT7P\\Kandel and Castelli - 2020 - The effect of batch size on the generalizability o.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EU8J2TYW\\S2405959519303455.html:text/html},
}

@inproceedings{wilson_need_2001,
	title = {The need for small learning rates on large problems},
	volume = {1},
	doi = {10.1109/IJCNN.2001.939002},
	abstract = {In gradient descent learning algorithms such as error backpropagation, the learning rate parameter can have a significant effect on generalization accuracy. In particular, decreasing the learning rate below that which yields the fastest convergence can significantly improve generalization accuracy, especially on large, complex problems. The learning rate also directly affects training speed, but not necessarily in the way that many people expect. Many neural network practitioners currently attempt to use the largest learning rate that still allows for convergence, in order to improve training speed. However, a learning rate that is too large can be as slow as a learning rate that is too small, and a learning rate that is too large or too small can require orders of magnitude more training time than one that is in an appropriate range. The paper illustrates how the learning rate affects training speed and generalization accuracy, and thus gives guidelines on how to efficiently select a learning rate that maximizes generalization accuracy.},
	booktitle = {{IJCNN}'01. {International} {Joint} {Conference} on {Neural} {Networks}. {Proceedings} ({Cat}. {No}.{01CH37222})},
	author = {Wilson, D.R. and Martinez, T.R.},
	month = jul,
	year = {2001},
	note = {ISSN: 1098-7576},
	keywords = {Neural networks, Approximation algorithms, Computer errors, Computer science, Convergence, Guidelines, Nominations and elections},
	pages = {115--119 vol.1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VNDXE434\\939002.html:text/html;Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IZ8D4R8P\\Wilson and Martinez - 2001 - The need for small learning rates on large problem.pdf:application/pdf},
}

@inproceedings{smith_dont_2018-1,
	title = {Don't {Decay} the {Learning} {Rate}, {Increase} the {Batch} {Size}},
	url = {https://openreview.net/forum?id=B1Yy1BxCZ},
	abstract = {Decaying the learning rate and increasing the batch size during training are equivalent.},
	language = {en},
	urldate = {2022-02-10},
	author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
	month = feb,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SBV2P93Z\\Smith et al. - 2018 - Don't Decay the Learning Rate, Increase the Batch .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\N2ZL3WUV\\forum.html:text/html},
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2022-02-10},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3L8ZH7SU\\Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\D9N84KZR\\1502.html:text/html},
}

@book{noauthor_notitle_nodate,
}

@book{montavon_neural_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Neural {Networks}: {Tricks} of the {Trade}: {Second} {Edition}},
	volume = {7700},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	shorttitle = {Neural {Networks}},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8},
	language = {en},
	urldate = {2022-02-14},
	publisher = {Springer Berlin Heidelberg},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8},
}

@article{zhang_applications_2020,
	title = {Applications of hyperspectral imaging in the detection and diagnosis of solid tumors},
	volume = {9},
	issn = {2219-6803, 2218-676X},
	url = {https://tcr.amegroups.com/article/view/34678},
	doi = {10.21037/tcr.2019.12.53},
	abstract = {Applications of hyperspectral imaging in the detection and diagnosis of solid tumors},
	language = {en},
	number = {2},
	urldate = {2022-02-18},
	journal = {Translational Cancer Research},
	author = {Zhang, Yating and Wu, Xiaoqian and He, Li and Meng, Chan and Du, Shunda and Bao, Jie and Zheng, Yongchang},
	month = feb,
	year = {2020},
	note = {Publisher: AME Publishing Company},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8II9ZED9\\Zhang et al. - 2020 - Applications of hyperspectral imaging in the detec.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9PPQISA2\\html.html:text/html},
}

@article{ayala_video-rate_2021,
	title = {Video-rate multispectral imaging in laparoscopic surgery: {First}-in-human application},
	shorttitle = {Video-rate multispectral imaging in laparoscopic surgery},
	url = {http://arxiv.org/abs/2105.13901},
	abstract = {Multispectral and hyperspectral imaging (MSI/HSI) can provide clinically relevant information on morphological and functional tissue properties. Application in the operating room (OR), however, has so far been limited by complex hardware setups and slow acquisition times. To overcome these limitations, we propose a novel imaging system for video-rate spectral imaging in the clinical workflow. The system integrates a small snapshot multispectral camera with a standard laparoscope and a clinically commonly used light source, enabling the recording of multispectral images with a spectral dimension of 16 at a frame rate of 25 Hz. An ongoing in patient study shows that multispectral recordings from this system can help detect perfusion changes in partial nephrectomy surgery, thus opening the doors to a wide range of clinical applications.},
	language = {en},
	urldate = {2022-02-18},
	journal = {arXiv:2105.13901 [cs, eess]},
	author = {Ayala, Leonardo and Wirkert, Sebastian and Vemuri, Anant and Adler, Tim and Seidlitz, Silvia and Pirmann, Sebastian and Engels, Christina and Teber, Dogu and Maier-Hein, Lena},
	month = may,
	year = {2021},
	note = {arXiv: 2105.13901},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Ayala et al. - 2021 - Video-rate multispectral imaging in laparoscopic s.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FKARAVYA\\Ayala et al. - 2021 - Video-rate multispectral imaging in laparoscopic s.pdf:application/pdf},
}

@article{lacoste_quantifying_2019,
	title = {Quantifying the {Carbon} {Emissions} of {Machine} {Learning}},
	url = {https://arxiv.org/abs/1910.09700v2},
	abstract = {From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.},
	language = {en},
	urldate = {2022-02-22},
	author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
	month = oct,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\54XSE4IF\\Lacoste et al. - 2019 - Quantifying the Carbon Emissions of Machine Learni.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\WMB625PY\\1910.html:text/html},
}

@article{lacoste_quantifying_2019-1,
	title = {Quantifying the {Carbon} {Emissions} of {Machine} {Learning}},
	url = {http://arxiv.org/abs/1910.09700},
	abstract = {From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.},
	urldate = {2022-02-22},
	journal = {arXiv:1910.09700 [cs]},
	author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
	month = nov,
	year = {2019},
	note = {arXiv: 1910.09700},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RLA79DCM\\Lacoste et al. - 2019 - Quantifying the Carbon Emissions of Machine Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\276ILS4Q\\1910.html:text/html},
}

@article{lacoste_quantifying_2019-2,
	title = {Quantifying the {Carbon} {Emissions} of {Machine} {Learning}},
	url = {http://arxiv.org/abs/1910.09700},
	abstract = {From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.},
	urldate = {2022-02-22},
	journal = {arXiv:1910.09700 [cs]},
	author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
	month = nov,
	year = {2019},
	note = {arXiv: 1910.09700},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\GPYPGGQ7\\Lacoste et al. - 2019 - Quantifying the Carbon Emissions of Machine Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\669I8M5J\\1910.html:text/html},
}

@article{lacoste_quantifying_2019-3,
	title = {Quantifying the {Carbon} {Emissions} of {Machine} {Learning}},
	url = {http://arxiv.org/abs/1910.09700},
	abstract = {From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.},
	urldate = {2022-02-22},
	journal = {arXiv:1910.09700 [cs]},
	author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
	month = nov,
	year = {2019},
	note = {arXiv: 1910.09700},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JFQLL6J5\\Lacoste et al. - 2019 - Quantifying the Carbon Emissions of Machine Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IY4928WY\\1910.html:text/html},
}

@article{meglinski_quantitative_2002,
	title = {Quantitative assessment of skin layers absorption and skin reflectance spectra simulation in the visible and near-infrared spectral regions},
	volume = {23},
	issn = {0967-3334},
	url = {https://iopscience.iop.org/article/10.1088/0967-3334/23/4/312},
	doi = {10.1088/0967-3334/23/4/312},
	abstract = {We have simulated diffuse reﬂectance spectra of skin by assuming a wavelength-independent scattering coefﬁcient for the different skin tissues and using the known wavelength dependence of the absorption coefﬁcient of oxyand deoxyhaemoglobin and water. A stochastic Monte Carlo method is used to convert the wavelength-dependent absorption coefﬁcient and wavelengthindependent scattering coefﬁcient into reﬂected intensity. The absorption properties of skin tissues in the visible and near-infrared spectral regions are estimated by taking into account the spatial distribution of blood vessels, water and melanin content within distinct anatomical layers. The geometrical peculiarities of skin histological structure, degree of blood oxygenation and the haematocrit index are also taken into account. We demonstrate that when the model is supplied with reasonable physical and structural parameters of skin, the results of the simulation agree reasonably well with the results of in vivo measurements of skin spectra.},
	language = {en},
	number = {4},
	urldate = {2022-02-23},
	journal = {Physiological Measurement},
	author = {Meglinski, Igor V and Matcher, Stephen J},
	month = nov,
	year = {2002},
	pages = {741--753},
	file = {Meglinski and Matcher - 2002 - Quantitative assessment of skin layers absorption .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6F7J4QHP\\Meglinski and Matcher - 2002 - Quantitative assessment of skin layers absorption .pdf:application/pdf},
}

@article{lister_optical_2012,
	title = {Optical properties of human skin},
	volume = {17},
	issn = {1083-3668, 1560-2281},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-biomedical-optics/volume-17/issue-9/090901/Optical-properties-of-human-skin/10.1117/1.JBO.17.9.090901.full},
	doi = {10.1117/1.JBO.17.9.090901},
	abstract = {A survey of the literature is presented that provides an analysis of the optical properties of human skin, with particular regard to their applications in medicine. Included is a description of the primary interactions of light with skin and how these are commonly estimated using radiative transfer theory (RTT). This is followed by analysis of measured RTT coefficients available in the literature. Orders of magnitude differences are found within published absorption and reduced-scattering coefficients. Causes for these discrepancies are discussed in detail, including contrasts between data acquired in vitro and in vivo. An analysis of the phase functions applied in skin optics, along with the remaining optical coefficients (anisotropy factors and refractive indices) is also included. The survey concludes that further work in the field is necessary to establish a definitive range of realistic coefficients for clinically normal skin.},
	number = {9},
	urldate = {2022-02-23},
	journal = {Journal of Biomedical Optics},
	author = {Lister, Tom and Wright, Philip A. and Chappell, Paul H.},
	month = sep,
	year = {2012},
	note = {Publisher: SPIE},
	pages = {090901},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SWUPMBCS\\Lister et al. - 2012 - Optical properties of human skin.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\EJ44Z99N\\1.JBO.17.9.090901.html:text/html},
}

@article{ruiz_breast_2017,
	title = {Breast density quantification using structured-light-based diffuse optical tomography simulations},
	volume = {56},
	doi = {10.1364/AO.56.007146},
	abstract = {We present the feasibility of structured-light-based diffuse optical tomography (DOT) to quantify the breast density with an extensive simulation study. This study is performed on multiple numerical breast phantoms built from magnetic resonance imaging (MRI) images. These phantoms represent realistic tissue morphologies and are given typical breast optical properties. First, synthetic data are simulated at five wavelengths using our structured-light-based DOT forward problem. Afterwards, the inverse problem is solved to obtain the absorption images and subsequently the chromophore concentration maps. Parameters, such as segmented volumes and mean concentrations, are extracted from these maps and used in a regression model to estimate the percent breast densities. These estimations are correlated with the true values from MRI, 𝑟=0.97r=0.97, showing that our new technique is promising in measuring breast density.},
	journal = {Applied Optics},
	author = {Ruiz, Jessica and Nouizi, Farouk and Cho, Jaedu and Zheng, Jie and Li, Yifan and Chen, Jeon-Hor and Su, Min-Ying and Gulsen, Gultekin},
	month = aug,
	year = {2017},
	pages = {7146--7157},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TNBFK2JG\\Ruiz et al. - 2017 - Breast density quantification using structured-lig.pdf:application/pdf},
}

@article{manifold_versatile_2021,
	title = {A versatile deep learning architecture for classification and label-free prediction of hyperspectral images},
	volume = {3},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-021-00309-y},
	doi = {10.1038/s42256-021-00309-y},
	abstract = {Hyperspectral imaging is a technique that provides rich chemical or compositional information not regularly available to traditional imaging modalities such as intensity imaging or colour imaging based on the reflection, transmission or emission of light. Analysis of hyperspectral imaging often relies on machine learning methods to extract information. Here we present a new flexible architecture—the U-within-U-Net—that can perform classification, segmentation and prediction of orthogonal imaging modalities on a variety of hyperspectral imaging techniques. Specifically, we demonstrate feature segmentation and classification on the Indian Pines hyperspectral dataset and simultaneous location prediction of multiple drugs in mass spectrometry imaging of rat liver tissue. We further demonstrate label-free fluorescence image prediction from hyperspectral stimulated Raman scattering microscopy images. The applicability of the U-within-U-Net architecture on diverse datasets with widely varying input and output dimensions and data sources suggest that it has great potential in advancing the use of hyperspectral imaging across many different application areas ranging from remote sensing, to medical imaging, to microscopy.},
	language = {en},
	number = {4},
	urldate = {2022-02-24},
	journal = {Nature Machine Intelligence},
	author = {Manifold, Bryce and Men, Shuaiqian and Hu, Ruoqian and Fu, Dan},
	month = apr,
	year = {2021},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Optical imaging, Imaging and sensing, Imaging studies, Mathematics and computing},
	pages = {306--315},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6C85RX7U\\Manifold et al. - 2021 - A versatile deep learning architecture for classif.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YL4J9TVG\\s42256-021-00309-y.html:text/html},
}

@article{zhang_high-speed_2020,
	title = {High-{Speed} {Chemical} {Imaging} by {Dense}-{Net} {Learning} of {Femtosecond} {Stimulated} {Raman} {Scattering}},
	volume = {11},
	url = {https://doi.org/10.1021/acs.jpclett.0c01598},
	doi = {10.1021/acs.jpclett.0c01598},
	abstract = {Hyperspectral stimulated Raman scattering (SRS) by spectral focusing can generate label-free chemical images through temporal scanning of chirped femtosecond pulses. Yet, pulse chirping decreases the pulse peak power and temporal scanning increases the acquisition time, resulting in a much slower imaging speed compared to single-frame SRS using femtosecond pulses. In this paper, we present a deep learning algorithm to solve the inverse problem of getting a chemically labeled image from a single-frame femtosecond SRS image. Our DenseNet-based learning method, termed as DeepChem, achieves high-speed chemical imaging with a large signal level. Speed is improved by 2 orders of magnitude with four subcellular components (lipid droplet, endoplasmic reticulum, nuclei, cytoplasm) classified in MIA PaCa-2 cells and other cell types which were not used for training. Lipid droplet dynamics and cellular response to dithiothreitol in live MIA PaCa-2 cells are demonstrated using this computationally multiplex method.},
	number = {20},
	urldate = {2022-02-24},
	journal = {The Journal of Physical Chemistry Letters},
	author = {Zhang, Jing and Zhao, Jian and Lin, Haonan and Tan, Yuying and Cheng, Ji-Xin},
	month = oct,
	year = {2020},
	note = {Publisher: American Chemical Society},
	pages = {8573--8578},
	file = {Zhang et al. - 2020 - High-Speed Chemical Imaging by Dense-Net Learning .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7AJUHVFJ\\Zhang et al. - 2020 - High-Speed Chemical Imaging by Dense-Net Learning .pdf:application/pdf},
}

@techreport{liu_instant_2022,
	title = {Instant diagnosis of gastroscopic biopsy via deep-learned single-shot femtosecond stimulated {Raman} histology},
	url = {https://www.researchsquare.com/article/rs-1129577/v1},
	abstract = {Gastroscopic biopsy provides the only effective way for gastric cancer diagnosis, but the gold standard histopathology is time-consuming and incompatible with gastroscopy. Conventional stimulated Raman scattering (SRS) microscopy has shown promise in label-free diagnosis on human tissues, yet it requires the tuning of picosecond lasers to achieve chemical specificity at the cost of time and complexity. Here, we demonstrated single-shot femtosecond SRS (femto-SRS) could reach the maximum speed and sensitivity with preserved chemical resolution by integrating with U-Net. Fresh gastroscopic biopsy was imaged in \&amp;lt; 60 seconds, revealing essential histoarchitectural hallmarks perfectly agreed with standard histopathology. Moreover, a diagnostic neural network (CNN) was constructed based on images from 279 patients that predicts gastric cancer with accuracy \&amp;gt; 96\%. We further demonstrated semantic segmentation of intratumor heterogeneity and evaluation of resection margins of endoscopic submucosal dissection (ESD) tissues to simulate rapid and automated intraoperative diagnosis. Our method holds potential for synchronizing gastroscopy and histopathological diagnosis.},
	urldate = {2022-03-04},
	author = {Liu, Zhijie and Su, Wei and Ao, Jianpeng and Wang, Min and Jiang, Qiuli and He, Jie and Gao, Hua and Lei, Shu and Nie, Jinshan and Yan, Xuefeng and Guo, Xiaojing and Zhou, Pinghong and Hu, Hao and Ji, Minbiao},
	year = {2022},
	doi = {10.21203/rs.3.rs-1129577/v1},
	note = {ISSN: 2693-5015
Type: article},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VI758VV6\\Liu et al. - 2022 - Instant diagnosis of gastroscopic biopsy via deep-.pdf:application/pdf},
}

@article{ayala_band_2022,
	title = {Band selection for oxygenation estimation with multispectral/hyperspectral imaging},
	volume = {13},
	issn = {2156-7085, 2156-7085},
	url = {https://opg.optica.org/abstract.cfm?URI=boe-13-3-1224},
	doi = {10.1364/BOE.441214},
	language = {en},
	number = {3},
	urldate = {2022-03-07},
	journal = {Biomedical Optics Express},
	author = {Ayala, Leonardo and Isensee, Fabian and Wirkert, Sebastian J. and Vemuri, Anant S. and Maier-Hein, Klaus H. and Fei, Baowei and Maier-Hein, Lena},
	month = mar,
	year = {2022},
	pages = {1224},
	file = {boe-13-3-1224.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UZ7RSVGF\\boe-13-3-1224.pdf:application/pdf},
}

@article{torres_spatial_2020,
	title = {Spatial {Spectral} {Band} {Selection} for {Enhanced} {Hyperspectral} {Remote} {Sensing} {Classification} {Applications}},
	volume = {6},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/6/9/87},
	doi = {10.3390/jimaging6090087},
	abstract = {Despite the numerous band selection (BS) algorithms reported in the ﬁeld, most if not all have exhibited maximal accuracy when more spectral bands are utilized for classiﬁcation. This apparently disagrees with the theoretical model of the ‘curse of dimensionality’ phenomenon, without apparent explanations. If it were true, then BS would be deemed as an academic piece of research without real beneﬁts to practical applications. This paper presents a spatial spectral mutual information (SSMI) BS scheme that utilizes a spatial feature extraction technique as a preprocessing step, followed by the clustering of the mutual information (MI) of spectral bands for enhancing the eﬃciency of the BS. Through the SSMI BS scheme, a sharp ’bell’-shaped accuracy-dimensionality characteristic that peaks at about 20 bands has been observed for the very ﬁrst time. The performance of the proposed SSMI BS scheme has been validated through 6 hyperspectral imaging (HSI) datasets (Indian Pines, Botswana, Barrax, Pavia University, Salinas, and Kennedy Space Center (KSC)), and its classiﬁcation accuracy is shown to be approximately 10\% better than seven state-of-the-art BS schemes (Saliency, HyperBS, SLN, OCF, FDPC, ISSC, and Convolution Neural Network (CNN)). The present result conﬁrms that the high eﬃciency of the BS scheme is essentially important to observe and validate the Hughes’ phenomenon in the analysis of HSI data. Experiments also show that the classiﬁcation accuracy can be aﬀected by as much as approximately 10\% when a single ‘crucial’ band is included or missed out for classiﬁcation.},
	language = {en},
	number = {9},
	urldate = {2022-03-07},
	journal = {Journal of Imaging},
	author = {Torres, Ruben Moya and Yuen, Peter W.T. and Yuan, Changfeng and Piper, Johathan and McCullough, Chris and Godfree, Peter},
	month = aug,
	year = {2020},
	pages = {87},
	file = {Torres et al. - 2020 - Spatial Spectral Band Selection for Enhanced Hyper.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ETHQ4S63\\Torres et al. - 2020 - Spatial Spectral Band Selection for Enhanced Hyper.pdf:application/pdf},
}

@article{chen_hyperspectral_2021,
	title = {Hyperspectral {Imaging} {Assessment} of {Systemic} {Sclerosis} {Using} the {Soft} {Abundance} {Score} and {Band} {Selection}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3082918},
	abstract = {Hyperspectral imaging (HSI) is an optical remote sensing technology that has the advantages of high spatial and spectral resolution. Aside from its use in geographical research, HSI has been widely used in medical diagnosis. Systemic sclerosis (SSc) is a multiorgan autoimmune disease that leads to skin tightness, thickness, and fibrosis. Internal organ involvement and mortality in this progressive disease are strongly correlated with the extent and severity of abnormal skin thickness. Our prior study demonstrated that HSI can outperform conventional assessment tools as a diagnostic modality to evaluate the severity of skin sclerosis in SSc patients. However, the analysis algorithm has its limitations. This study aimed to investigate a novel soft abundance score for HSI. We also explored the influence of band selection on the HSI analysis of SSc patients. In total, we enrolled 30 SSc patients (male: 10; female: 20, median age±range, 49.9±17.0 years) and 24 healthy controls (male: 12; female: 12, median age±range, 37.0±11.0 years). We found that most of the spectral bands generated by different band selection methods were similar. Moreover, in the task of distinguishing SSc patients from healthy controls, the soft abundance scores calculated from these bands exhibited greater discriminative power than a spectral angle mapper (SAM), skin scores determined by clinical assessments, or skin thickness determined through ultrasonography. Our results suggest that the spectral bands selected in this study should be taken into consideration to guide future hardware improvement. The analytic algorithm can also be applied as a new clinical method.},
	journal = {IEEE Access},
	author = {Chen, Hsian-Min and Lai, Kuo-Lung and Chen, Hsin-Hua and Chen, Jun-Peng and Sung, Chiu-Chin and Chen, Yi-Ming},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Hyperspectral imaging, Hospitals, band selection (BS), Correlation, Covariance matrices, Eigenvalues and eigenfunctions, Hybrid fiber coaxial cables, Hyperspectral imaging soft abundance score (HSISAS), Skin, skin thickness, systemic sclerosis (SSc)},
	pages = {80275--80287},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9JCJ6Q6R\\9439474.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\M5LK2IR8\\Chen et al. - 2021 - Hyperspectral Imaging Assessment of Systemic Scler.pdf:application/pdf},
}

@inproceedings{rodriguez_-vivo_2022,
	address = {San Francisco, United States},
	title = {In-vivo human study of skin optical properties in individuals with obesity},
	isbn = {978-1-5106-4807-4 978-1-5106-4808-1},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11968/2610464/In-vivo-human-study-of-skin-optical-properties-in-individuals/10.1117/12.2610464.full},
	doi = {10.1117/12.2610464},
	abstract = {Obesity is a widespread chronic illness which affects over 40\% of the US adult population and its world-wide prevalence has increased over the years impacting both low and high-income countries. Obesity has been linked to higher risk of non-communicable diseases such as cardiovascular disease, type-2 diabetes, dyslipidemia, hypertension, among others. Currently the mostly prescribed regimes to combat chronic illness associated with obesity are efforts to change diet, behavior, and physical activity. Wearable devices have the potential of helping users reduce their obesity levels as these devices can easily acquire and communicate biometric data with users and clinicians. However, these technologies depend on optical sensors that are sensitive to molecular skin composition. We hypothesize that individuals with high BMI levels will present changes in skin optical properties when compared to their non-obese counterparts. Our objective is to capture skin optical properties at the wrist among a diverse cohort using a commercial optical system for research use. To meet an appropriate power, the human study, composed of males and females, is conducted with 100 adult participants. Statistical methods, including linear regression and t-tests, are used to determine interactions between measured data and participant demographics. We believe these results can improve design of optical wearables for the obese.},
	language = {en},
	urldate = {2022-03-09},
	booktitle = {Optical {Diagnostics} and {Sensing} {XXII}: {Toward} {Point}-of-{Care} {Diagnostics}},
	publisher = {SPIE},
	author = {Rodriguez, Andres J. and Alzamora, Michael and Ajmal, Ajmal and Boonya-Ananta, Tananant and Du Le, Vinh Nguyen and Fredriksson, Ingemar and Strömberg, Tomas and Ramella-Roman, Jessica C.},
	editor = {Coté, Gerard L.},
	month = mar,
	year = {2022},
	pages = {19},
	file = {Rodriguez et al. - 2022 - In-vivo human study of skin optical properties in .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MGYFR9IS\\Rodriguez et al. - 2022 - In-vivo human study of skin optical properties in .pdf:application/pdf},
}

@inproceedings{gresele_incomplete_2020,
	title = {The {Incomplete} {Rosetta} {Stone} problem: {Identifiability} results for {Multi}-view {Nonlinear} {ICA}},
	shorttitle = {The {Incomplete} {Rosetta} {Stone} problem},
	url = {https://proceedings.mlr.press/v115/gresele20a.html},
	abstract = {We consider the problem of recovering a common latent source with independent components from multiple views. This applies to settings in which a variable is measured with multiple experimental modalities, and where the goal is to synthesize the disparate measurements into a single unified representation. We consider the case that the observed views are a nonlinear mixing of component-wise corruptions of the sources. When the views are considered separately, this reduces to nonlinear Independent Component Analysis (ICA) for which it is provably impossible to undo the mixing. We present novel identifiability proofs that this is possible when the multiple views are considered jointly, showing that the mixing can theoretically be undone using function approximators such as deep neural networks. In contrast to known identifiability results for nonlinear ICA, we prove that independent latent sources with arbitrary mixing can be recovered as long as multiple, sufficiently different noisy views are available.},
	language = {en},
	urldate = {2022-03-24},
	booktitle = {Proceedings of {The} 35th {Uncertainty} in {Artificial} {Intelligence} {Conference}},
	publisher = {PMLR},
	author = {Gresele, Luigi and Rubenstein, Paul K. and Mehrjou, Arash and Locatello, Francesco and Schölkopf, Bernhard},
	month = aug,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {217--227},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\G3KDTIIT\\Gresele et al. - 2020 - The Incomplete Rosetta Stone problem Identifiabil.pdf:application/pdf;Supplementary PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DYWGNMEK\\Gresele et al. - 2020 - The Incomplete Rosetta Stone problem Identifiabil.pdf:application/pdf},
}

@article{muller_learning_2021,
	title = {Learning {Robust} {Models} {Using} {The} {Principle} of {Independent} {Causal} {Mechanisms}},
	url = {http://arxiv.org/abs/2010.07167},
	abstract = {Standard supervised learning breaks down under data distribution shift. However, the principle of independent causal mechanisms (ICM, Peters et al. (2017)) can turn this weakness into an opportunity: one can take advantage of distribution shift between different environments during training in order to obtain more robust models. We propose a new gradient-based learning framework whose objective function is derived from the ICM principle. We show theoretically and experimentally that neural networks trained in this framework focus on relations remaining invariant across environments and ignore unstable ones. Moreover, we prove that the recovered stable relations correspond to the true causal mechanisms under certain conditions. In both regression and classification, the resulting models generalize well to unseen scenarios where traditionally trained models fail.},
	urldate = {2022-03-24},
	journal = {arXiv:2010.07167 [cs, stat]},
	author = {Müller, Jens and Schmier, Robert and Ardizzone, Lynton and Rother, Carsten and Köthe, Ullrich},
	month = feb,
	year = {2021},
	note = {arXiv: 2010.07167},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\HCWVU6HP\\Müller et al. - 2021 - Learning Robust Models Using The Principle of Inde.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TTB4MWY3\\2010.html:text/html},
}

@inproceedings{mackowiak_generative_2021,
	title = {Generative {Classifiers} as a {Basis} for {Trustworthy} {Image} {Classification}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Mackowiak_Generative_Classifiers_as_a_Basis_for_Trustworthy_Image_Classification_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-03-24},
	author = {Mackowiak, Radek and Ardizzone, Lynton and Kothe, Ullrich and Rother, Carsten},
	year = {2021},
	pages = {2971--2981},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XS8PZY94\\Mackowiak et al. - 2021 - Generative Classifiers as a Basis for Trustworthy .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IF2SM4EL\\Mackowiak_Generative_Classifiers_as_a_Basis_for_Trustworthy_Image_Classification_CVPR_2021_pape.html:text/html},
}

@techreport{ayala_spectral_2022,
	title = {Spectral imaging enables contrast agent-free real-time ischemia monitoring in laparoscopic surgery},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2022.03.08.22271465v1},
	abstract = {Laparoscopic surgery has evolved as a key technique for cancer diagnosis and therapy. While characterization of the tissue perfusion is crucial in various procedures, such as partial nephrectomy, doing so by means of visual inspection remains highly challenging. Spectral imaging takes advantage of the fact that different tissue components have unique optical properties to recover relevant information on tissue function such as ischemia. However, clinical success stories for advancing laparoscopic surgery with spectral imaging are lacking to date. To address this bottleneck, we developed the first laparoscopic real-time multispectral imaging (MSI) system featuring a compact and lightweight multispectral camera and the possibility to complement the conventional RGB (Red, Green, and Blue) surgical view of the patient with functional information at a video rate of 25 Hz. To account for the high inter-patient variability of human tissue, we phrase the problem of ischemia detection as an out-of-distribution (OoD) detection problem that does not rely on data from any other patient. Using an ensemble of invertible neural networks (INNs) as a core component, our algorithm computes the likelihood of ischemia based on a short (several seconds) video sequence acquired at the beginning of each surgery. A first-in-human trial performed on 10 patients undergoing partial nephrectomy demonstrates the feasibility of our approach for fully-automatic live ischemia monitoring during laparoscopic surgery. Compared to the clinical state-of-the-art approach based on indocyanine green (ICG) fluorescence, the proposed MSI-based method does not require the injection of a contrast agent and is repeatable if the wrong segment has been clamped. Spectral imaging combined with advanced deep learning-based analysis tools could thus evolve as an important tool for fast, efficient, reliable and safe functional imaging in minimally invasive surgery.},
	language = {en},
	urldate = {2022-03-24},
	institution = {medRxiv},
	author = {Ayala, Leonardo and Adler, Tim J. and Seidlitz, Silvia and Wirkert, Sebastian and Engels, Christina and Seitel, Alexander and Sellner, Jan and Aksenov, Alexey and Bodenbach, Matthias and Bader, Pia and Baron, Sebastian and Vemuri, Anant and Wiesenfarth, Manuel and Schreck, Nicholas and Mindroc, Diana and Tizabi, Minu and Pirmann, Sebastian and Everitt, Brittaney and Kopp-Schneider, Annette and Teber, Dogu and Maier-Hein, Lena},
	month = mar,
	year = {2022},
	doi = {10.1101/2022.03.08.22271465},
	note = {ISSN: 2227-1465
Type: article},
	pages = {2022.03.08.22271465},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RWYPZRGV\\Ayala et al. - 2022 - Spectral imaging enables contrast agent-free real-.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\H9QSKGDV\\2022.03.08.html:text/html},
}

@article{dietrich_machine_2021-1,
	title = {Machine learning-based analysis of hyperspectral images for automated sepsis diagnosis},
	url = {http://arxiv.org/abs/2106.08445},
	abstract = {Sepsis is a leading cause of mortality and critical illness worldwide. While robust biomarkers for early diagnosis are still missing, recent work indicates that hyperspectral imaging (HSI) has the potential to overcome this bottleneck by monitoring microcirculatory alterations. Automated machine learning-based diagnosis of sepsis based on HSI data, however, has not been explored to date. Given this gap in the literature, we leveraged an existing data set to (1) investigate whether HSI-based automated diagnosis of sepsis is possible and (2) put forth a list of possible confounders relevant for HSI-based tissue classification. While we were able to classify sepsis with an accuracy of over \$98{\textbackslash},{\textbackslash}\%\$ using the existing data, our research also revealed several subject-, therapy- and imaging-related confounders that may lead to an overestimation of algorithm performance when not balanced across the patient groups. We conclude that further prospective studies, carefully designed with respect to these confounders, are necessary to confirm the preliminary results obtained in this study.},
	urldate = {2022-04-06},
	journal = {arXiv:2106.08445 [cs, eess]},
	author = {Dietrich, Maximilian and Seidlitz, Silvia and Schreck, Nicholas and Wiesenfarth, Manuel and Godau, Patrick and Tizabi, Minu and Sellner, Jan and Marx, Sebastian and Knödler, Samuel and Allers, Michael M. and Ayala, Leonardo and Schmidt, Karsten and Brenner, Thorsten and Studier-Fischer, Alexander and Nickel, Felix and Müller-Stich, Beat P. and Kopp-Schneider, Annette and Weigand, Markus A. and Maier-Hein, Lena},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.08445},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Computer Vision and Pattern Recognition, I.2.10, I.4, I.5, J.3},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\B8AGKXC8\\Dietrich et al. - 2021 - Machine learning-based analysis of hyperspectral i.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\V7FNBME6\\2106.html:text/html},
}

@techreport{dinga_controlling_2020,
	title = {Controlling for effects of confounding variables on machine learning predictions},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.08.17.255034v1},
	abstract = {Machine learning predictive models are being used in neuroimaging to predict information about the task or stimuli or to identify potentially clinically useful biomarkers. However, the predictions can be driven by confounding variables unrelated to the signal of interest, such as scanner effect or head motion, limiting the clinical usefulness and interpretation of machine learning models. The most common method to control for confounding effects is regressing out the confounding variables separately from each input variable before machine learning modeling. However, we show that this method is insufficient because machine learning models can learn information from the data that cannot be regressed out. Instead of regressing out confounding effects from each input variable, we propose controlling for confounds post-hoc on the level of machine learning predictions. This allows partitioning of the predictive performance into the performance that can be explained by confounds and performance independent of confounds. This approach is flexible and allows for parametric and non-parametric confound adjustment. We show in real and simulated data that this method correctly controls for confounding effects even when traditional input variable adjustment produces false-positive findings.},
	language = {en},
	urldate = {2022-04-06},
	institution = {bioRxiv},
	author = {Dinga, Richard and Schmaal, Lianne and Penninx, Brenda W. J. H. and Veltman, Dick J. and Marquand, Andre F.},
	month = aug,
	year = {2020},
	doi = {10.1101/2020.08.17.255034},
	note = {Section: New Results
Type: article},
	pages = {2020.08.17.255034},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8LNJLMGR\\Dinga et al. - 2020 - Controlling for effects of confounding variables o.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FRLY5ETU\\2020.08.17.html:text/html},
}

@article{adeli_bias-resilient_2019,
	title = {Bias-{Resilient} {Neural} {Network}},
	abstract = {A method based on the adversarial training strategy to learn discriminative features unbiased and invariant to the confounder(s) by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and learned features. Presence of bias and confounding effects is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in the recent years. Such challenges range from spurious associations of confounding variables in medical studies to the bias of race in gender or face recognition systems. One solution is to enhance datasets and organize them such that they do not reflect biases, which is a cumbersome and intensive task. The alternative is to make use of available data and build models considering these biases. Traditional statistical methods apply straightforward techniques such as residualization or stratification to precomputed features to account for confounding variables. However, these techniques are not in general applicable to end-to-end deep learning methods. In this paper, we propose a method based on the adversarial training strategy to learn discriminative features unbiased and invariant to the confounder(s). This is enabled by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and learned features. We apply our method to a synthetic, a medical diagnosis, and a gender classification (Gender Shades) dataset. Our results show that the learned features by our method not only result in superior prediction performance but also are uncorrelated with the bias or confounder variables. The code is available at http://blinded\_for\_review/.},
	journal = {ArXiv},
	author = {Adeli, E. and Zhao, Qingyu and Pfefferbaum, A. and Sullivan, E. and Fei-Fei, Li and Niebles, Juan Carlos and Pohl, K.},
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6K4JW6TN\\Adeli et al. - 2019 - Bias-Resilient Neural Network.pdf:application/pdf},
}

@inproceedings{zhong_enlightening_2017,
	title = {Enlightening {Deep} {Neural} {Networks} with {Knowledge} of {Confounding} {Factors}},
	doi = {10.1109/ICCVW.2017.131},
	abstract = {Deep learning techniques have demonstrated significant capacity in modeling some of the most challenging real world problems of high complexity. Despite the popularity of deep models, we still strive to better understand the underlying mechanism that drives their success. Motivated by observations that neurons in trained deep nets predict variation explaining factors indirectly related to the training tasks, we recognize that a deep network learns representations more general than the task at hand in order to disentangle impacts of multiple confounding factors governing the data, isolate the effects of the concerning factors, and optimize the given objective. Consequently, we propose to augment training of deep models with auxiliary information on explanatory factors of the data, in an effort to boost this disentanglement. Such deep networks, trained to comprehend data interactions and distributions more accurately, possess improved generalizability and compute better feature representations. Since pose is one of the most dominant confounding factors for object recognition, we adopt this principle to train a pose-aware deep convolutional neural network to learn both the class and pose of an object, so that it can make more informed classification decisions taking into account image variations induced by the object pose. We demonstrate that auxiliary pose information improves the classification accuracy in our experiments on Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) tasks. This general principle is readily applicable to improve the recognition and classification performance in various deep-learning applications.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	author = {Zhong, Yu and Ettinger, Gil},
	month = oct,
	year = {2017},
	note = {ISSN: 2473-9944},
	keywords = {Training, Data models, Biological neural networks, Neurons, Object recognition},
	pages = {1077--1086},
}

@article{rogozhnikov_hierarchical_2022,
	title = {Hierarchical confounder discovery in the experiment-machine learning cycle},
	issn = {2666-3899},
	url = {https://www.sciencedirect.com/science/article/pii/S2666389922000241},
	doi = {10.1016/j.patter.2022.100451},
	abstract = {The promise of machine learning (ML) to extract insights from high-dimensional datasets is tempered by confounding variables. It behooves scientists to determine if a model has extracted the desired information or instead fallen prey to bias. Due to features of natural phenomena and experimental design constraints, bioscience datasets are often organized in nested hierarchies that obfuscate the origins of confounding effects and render confounder amelioration methods ineffective. We propose a non-parametric statistical method called the rank-to-group (RTG) score that identifies hierarchical confounder effects in raw data and ML-derived embeddings. We show that RTG scores correctly assign the effects of hierarchical confounders when linear methods fail. In a public biomedical image dataset, we discover unreported effects of experimental design. We then use RTG scores to discover crossmodal correlated variability in a multi-phenotypic biological dataset. This approach should be generally useful in experiment-analysis cycles and to ensure confounder robustness in ML models.},
	language = {en},
	urldate = {2022-04-06},
	journal = {Patterns},
	author = {Rogozhnikov, Alex and Ramkumar, Pavan and Bedi, Rishi and Kato, Saul and Escola, G. Sean},
	month = feb,
	year = {2022},
	keywords = {machine learning, bias, confounders, debiasing, experimental design, hierarchical confounders, Mann-Whitney U test, robustness, stem cell biology},
	pages = {100451},
	file = {ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4TMCE4XQ\\S2666389922000241.html:text/html},
}

@techreport{rogozhnikov_hierarchical_2021,
	type = {preprint},
	title = {Hierarchical confounder discovery in the experiment–machine learning cycle},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.05.11.443616},
	abstract = {The promise of machine learning (ML) to extract insights from high-dimensional datasets is tempered by confounding variables. It behooves scientists to determine if a model has extracted the desired information or instead fallen prey to bias. Due to features of natural phenomena and experimental design constraints, bioscience datasets are often organized in nested hierarchies that obfuscate the origins of confounding effects and render confounder amelioration methods ineffective. We propose a non-parametric statistical method called the rank-to-group (RTG) score that identiﬁes hierarchical confounder effects in raw data and ML-derived embeddings. We show that RTG scores correctly assign the effects of hierarchical confounders when linear methods fail. In a public biomedical image dataset, we discover unreported effects of experimental design. We then use RTG scores to discover crossmodal correlated variability in a multiphenotypic biological dataset. This approach should be generally useful in experiment-analysis cycles and to ensure confounder robustness in ML models.},
	language = {en},
	urldate = {2022-04-06},
	institution = {Bioinformatics},
	author = {Rogozhnikov, Alex and Ramkumar, Pavan and Bedi, Rishi and Kato, Saul and Escola, G. Sean},
	month = may,
	year = {2021},
	doi = {10.1101/2021.05.11.443616},
	file = {Rogozhnikov et al. - 2021 - Hierarchical confounder discovery in the experimen.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\B3XRDJBJ\\Rogozhnikov et al. - 2021 - Hierarchical confounder discovery in the experimen.pdf:application/pdf},
}

@article{neto_causality-aware_2020,
	title = {Causality-aware counterfactual confounding adjustment for feature representations learned by deep models},
	url = {http://arxiv.org/abs/2004.09466},
	abstract = {Causal modeling has been recognized as a potential solution to many challenging problems in machine learning (ML). Here, we describe how a recently proposed counterfactual approach developed to deconfound linear structural causal models can still be used to deconfound the feature representations learned by deep neural network (DNN) models. The key insight is that by training an accurate DNN using softmax activation at the classification layer, and then adopting the representation learned by the last layer prior to the output layer as our features, we have that, by construction, the learned features will fit well a (multi-class) logistic regression model, and will be linearly associated with the labels. As a consequence, deconfounding approaches based on simple linear models can be used to deconfound the feature representations learned by DNNs. We validate the proposed methodology using colored versions of the MNIST dataset. Our results illustrate how the approach can effectively combat confounding and improve model stability in the context of dataset shifts generated by selection biases.},
	urldate = {2022-04-06},
	journal = {arXiv:2004.09466 [cs, stat]},
	author = {Neto, Elias Chaibub},
	month = nov,
	year = {2020},
	note = {arXiv: 2004.09466},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QD2K64J4\\Neto - 2020 - Causality-aware counterfactual confounding adjustm.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\C6DM3HXH\\2004.html:text/html},
}

@article{geirhos_shortcut_2020,
	title = {Shortcut {Learning} in {Deep} {Neural} {Networks}},
	volume = {2},
	issn = {2522-5839},
	url = {http://arxiv.org/abs/2004.07780},
	doi = {10.1038/s42256-020-00257-z},
	abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning's problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
	number = {11},
	urldate = {2022-04-06},
	journal = {Nature Machine Intelligence},
	author = {Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
	month = nov,
	year = {2020},
	note = {arXiv: 2004.07780},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
	pages = {665--673},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FYBW8MHB\\Geirhos et al. - 2020 - Shortcut Learning in Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KTDG6TEI\\2004.html:text/html},
}

@article{grohl_deep_2021,
	title = {Deep learning for biomedical photoacoustic imaging: {A} review},
	volume = {22},
	issn = {2213-5979},
	shorttitle = {Deep learning for biomedical photoacoustic imaging},
	url = {https://www.sciencedirect.com/science/article/pii/S2213597921000033},
	doi = {10.1016/j.pacs.2021.100241},
	abstract = {Photoacoustic imaging (PAI) is a promising emerging imaging modality that enables spatially resolved imaging of optical tissue properties up to several centimeters deep in tissue, creating the potential for numerous exciting clinical applications. However, extraction of relevant tissue parameters from the raw data requires the solving of inverse image reconstruction problems, which have proven extremely difficult to solve. The application of deep learning methods has recently exploded in popularity, leading to impressive successes in the context of medical imaging and also finding first use in the field of PAI. Deep learning methods possess unique advantages that can facilitate the clinical translation of PAI, such as extremely fast computation times and the fact that they can be adapted to any given problem. In this review, we examine the current state of the art regarding deep learning in PAI and identify potential directions of research that will help to reach the goal of clinical applicability.},
	language = {en},
	urldate = {2022-04-06},
	journal = {Photoacoustics},
	author = {Gröhl, Janek and Schellenberg, Melanie and Dreher, Kris and Maier-Hein, Lena},
	month = jun,
	year = {2021},
	keywords = {Deep learning, Image reconstruction, Optoacoustic imaging, Photoacoustic imaging, Photoacoustic tomography, Signal quantification},
	pages = {100241},
	file = {ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3IAWNQ92\\S2213597921000033.html:text/html;Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\28M6JC9D\\Gröhl et al. - 2021 - Deep learning for biomedical photoacoustic imaging.pdf:application/pdf},
}

@article{rissanen_critical_nodate,
	title = {A {Critical} {Look} at the {Consistency} of {Causal} {Estimation} with {Deep} {Latent} {Variable} {Models}},
	abstract = {Using deep latent variable models in causal inference has attracted considerable interest recently, but an essential open question is their ability to yield consistent causal estimates. While they have demonstrated promising results and theory exists on some simple model formulations, we also know that causal effects are not even identiﬁable in general with latent variables. We investigate this gap between theory and empirical results with analytical considerations and extensive experiments under multiple synthetic and real-world data sets, using the causal effect variational autoencoder (CEVAE) as a case study. While CEVAE seems to work reliably under some simple scenarios, it does not estimate the causal effect correctly with a misspeciﬁed latent variable or a complex data distribution, as opposed to its original motivation. Hence, our results show that more attention should be paid to ensuring the correctness of causal estimates with deep latent variable models.},
	language = {en},
	author = {Rissanen, Severi and Marttinen, Pekka},
	pages = {11},
	file = {Rissanen and Marttinen - A Critical Look at the Consistency of Causal Estim.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UQJE7WGU\\Rissanen and Marttinen - A Critical Look at the Consistency of Causal Estim.pdf:application/pdf},
}

@misc{brownlee_role_2018,
	title = {The {Role} of {Randomization} to {Address} {Confounding} {Variables} in {Machine} {Learning}},
	url = {https://machinelearningmastery.com/confounding-variables-in-machine-learning/},
	abstract = {A large part of applied machine learning is about running controlled experiments to discover what algorithm or algorithm configuration to […]},
	language = {en-US},
	urldate = {2022-04-07},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jul,
	year = {2018},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KZJBBP2M\\confounding-variables-in-machine-learning.html:text/html},
}

@misc{sellner_hyperspectral_2022,
	title = {Hyperspectral {Tissue} {Classification}},
	url = {https://zenodo.org/record/6577615},
	abstract = {This is the announcement release for our code and pre-trained models which will be available soon.},
	urldate = {2022-05-24},
	publisher = {Zenodo},
	author = {Sellner, Jan and Seidlitz, Silvia},
	month = may,
	year = {2022},
	doi = {10.5281/zenodo.6577615},
	note = {Language: eng},
	keywords = {hyperspectral imaging, deep learning, surgical data science, open surgery, organ segmentation, semantic scene segmentation},
	file = {Zenodo Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\E2FMXPWJ\\6577615.html:text/html},
}

@inproceedings{ilse_diva_2020,
	title = {{DIVA}: {Domain} {Invariant} {Variational} {Autoencoders}},
	shorttitle = {{DIVA}},
	url = {https://proceedings.mlr.press/v121/ilse20a.html},
	abstract = {We consider the problem of domain generalization, namely, how to learn representations given data from a set of domains that generalize to data from a previously unseen domain. We propose the Domain Invariant Variational Autoencoder (DIVA), a generative model that tackles this problem by learning three independent latent subspaces, one for the domain, one for the class, and one for any residual variations. We highlight that due to the generative nature of our model we can also incorporate unlabeled data from known or previously unseen domains. To the best of our knowledge this has not been done before in a domain generalization setting. This property is highly desirable in fields like medical imaging where labeled data is scarce. We experimentally evaluate our model on the rotated MNIST benchmark and a malaria cell images dataset where we show that (i) the learned subspaces are indeed complementary to each other, (ii) we improve upon recent works on this task and (iii) incorporating unlabelled data can boost the performance even further.},
	language = {en},
	urldate = {2022-09-25},
	booktitle = {Proceedings of the {Third} {Conference} on {Medical} {Imaging} with {Deep} {Learning}},
	publisher = {PMLR},
	author = {Ilse, Maximilian and Tomczak, Jakub M. and Louizos, Christos and Welling, Max},
	month = sep,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {322--348},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SEAQ9BF9\\Ilse et al. - 2020 - DIVA Domain Invariant Variational Autoencoders.pdf:application/pdf},
}

@article{studier-fischer_spectral_2022,
	title = {Spectral organ fingerprints for machine learning-based intraoperative tissue classification with hyperspectral imaging in a porcine model},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-15040-w},
	doi = {10.1038/s41598-022-15040-w},
	abstract = {Visual discrimination of tissue during surgery can be challenging since different tissues appear similar to the human eye. Hyperspectral imaging (HSI) removes this limitation by associating each pixel with high-dimensional spectral information. While previous work has shown its general potential to discriminate tissue, clinical translation has been limited due to the method’s current lack of robustness and generalizability. Specifically, the scientific community is lacking a comprehensive spectral tissue atlas, and it is unknown whether variability in spectral reflectance is primarily explained by tissue type rather than the recorded individual or specific acquisition conditions. The contribution of this work is threefold: (1) Based on an annotated medical HSI data set (9059 images from 46 pigs), we present a tissue atlas featuring spectral fingerprints of 20 different porcine organs and tissue types. (2) Using the principle of mixed model analysis, we show that the greatest source of variability related to HSI images is the organ under observation. (3) We show that HSI-based fully-automatic tissue differentiation of 20 organ classes with deep neural networks is possible with high accuracy ({\textgreater} 95\%). We conclude from our study that automatic tissue discrimination based on HSI data is feasible and could thus aid in intraoperative decisionmaking and pave the way for context-aware computer-assisted surgery systems and autonomous robotics.},
	language = {en},
	number = {1},
	urldate = {2022-09-25},
	journal = {Scientific Reports},
	author = {Studier-Fischer, Alexander and Seidlitz, Silvia and Sellner, Jan and Özdemir, Berkin and Wiesenfarth, Manuel and Ayala, Leonardo and Odenthal, Jan and Knödler, Samuel and Kowalewski, Karl Friedrich and Haney, Caelan Max and Camplisson, Isabella and Dietrich, Maximilian and Schmidt, Karsten and Salg, Gabriel Alexander and Kenngott, Hannes Götz and Adler, Tim Julian and Schreck, Nicholas and Kopp-Schneider, Annette and Maier-Hein, Klaus and Maier-Hein, Lena and Müller-Stich, Beat Peter and Nickel, Felix},
	month = jun,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Medical research},
	pages = {11028},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3JNEAVZG\\Studier-Fischer et al. - 2022 - Spectral organ fingerprints for machine learning-b.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5ZX62CFF\\s41598-022-15040-w.html:text/html},
}

@article{seidlitz_robust_2022,
	title = {Robust deep learning-based semantic organ segmentation in hyperspectral images},
	volume = {80},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841522001359},
	doi = {10.1016/j.media.2022.102488},
	abstract = {Semantic image segmentation is an important prerequisite for context-awareness and autonomous robotics in surgery. The state of the art has focused on conventional RGB video data acquired during minimally invasive surgery, but full-scene semantic segmentation based on spectral imaging data and obtained during open surgery has received almost no attention to date. To address this gap in the literature, we are investigating the following research questions based on hyperspectral imaging (HSI) data of pigs acquired in an open surgery setting: (1) What is an adequate representation of HSI data for neural network-based fully automated organ segmentation, especially with respect to the spatial granularity of the data (pixels vs. superpixels vs. patches vs. full images)? (2) Is there a benefit of using HSI data compared to other modalities, namely RGB data and processed HSI data (e.g. tissue parameters like oxygenation), when performing semantic organ segmentation? According to a comprehensive validation study based on 506 HSI images from 20 pigs, annotated with a total of 19 classes, deep learning-based segmentation performance increases — consistently across modalities — with the spatial context of the input data. Unprocessed HSI data offers an advantage over RGB data or processed data from the camera provider, with the advantage increasing with decreasing size of the input to the neural network. Maximum performance (HSI applied to whole images) yielded a mean DSC of 0.90 ((standard deviation (SD)) 0.04), which is in the range of the inter-rater variability (DSC of 0.89 ((standard deviation (SD)) 0.07)). We conclude that HSI could become a powerful image modality for fully-automatic surgical scene understanding with many advantages over traditional imaging, including the ability to recover additional functional tissue information. Our code and pre-trained models are available at https://github.com/IMSY-DKFZ/htc.},
	language = {en},
	urldate = {2022-09-25},
	journal = {Medical Image Analysis},
	author = {Seidlitz, Silvia and Sellner, Jan and Odenthal, Jan and Özdemir, Berkin and Studier-Fischer, Alexander and Knödler, Samuel and Ayala, Leonardo and Adler, Tim J. and Kenngott, Hannes G. and Tizabi, Minu and Wagner, Martin and Nickel, Felix and Müller-Stich, Beat P. and Maier-Hein, Lena},
	month = aug,
	year = {2022},
	keywords = {Hyperspectral imaging, Surgical data science, Deep learning, Open surgery, Organ segmentation, Semantic scene segmentation},
	pages = {102488},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SB3FH6NJ\\Seidlitz et al. - 2022 - Robust deep learning-based semantic organ segmenta.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JY4KM8MN\\S1361841522001359.html:text/html},
}

@article{chalopin_kunstliche_2022,
	title = {Künstliche {Intelligenz} und hyperspektrale {Bildgebung} zur bildgestützten {Assistenz} in der minimal-invasiven {Chirurgie}},
	volume = {93},
	issn = {2731-6971, 2731-698X},
	url = {https://link.springer.com/10.1007/s00104-022-01677-w},
	doi = {10.1007/s00104-022-01677-w},
	language = {de},
	number = {10},
	urldate = {2023-01-25},
	journal = {Die Chirurgie},
	author = {Chalopin, Claire and Nickel, Felix and Pfahl, Annekatrin and Köhler, Hannes and Maktabi, Marianne and Thieme, René and Sucher, Robert and Jansen-Winkeln, Boris and Studier-Fischer, Alexander and Seidlitz, Silvia and Maier-Hein, Lena and Neumuth, Thomas and Melzer, Andreas and Müller-Stich, Beat Peter and Gockel, Ines},
	month = oct,
	year = {2022},
	pages = {940--947},
	file = {Chalopin et al. - 2022 - Künstliche Intelligenz und hyperspektrale Bildgebu.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ZBA5S547\\Chalopin et al. - 2022 - Künstliche Intelligenz und hyperspektrale Bildgebu.pdf:application/pdf},
}

@article{carstens_dresden_2023,
	title = {The {Dresden} {Surgical} {Anatomy} {Dataset} for {Abdominal} {Organ} {Segmentation} in {Surgical} {Data} {Science}},
	volume = {10},
	copyright = {2023 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-022-01719-2},
	doi = {10.1038/s41597-022-01719-2},
	abstract = {Laparoscopy is an imaging technique that enables minimally-invasive procedures in various medical disciplines including abdominal surgery, gynaecology and urology. To date, publicly available laparoscopic image datasets are mostly limited to general classifications of data, semantic segmentations of surgical instruments and low-volume weak annotations of specific abdominal organs. The Dresden Surgical Anatomy Dataset provides semantic segmentations of eight abdominal organs (colon, liver, pancreas, small intestine, spleen, stomach, ureter, vesicular glands), the abdominal wall and two vessel structures (inferior mesenteric artery, intestinal veins) in laparoscopic view. In total, this dataset comprises 13195 laparoscopic images. For each anatomical structure, we provide over a thousand images with pixel-wise segmentations. Annotations comprise semantic segmentations of single organs and one multi-organ-segmentation dataset including segments for all eleven anatomical structures. Moreover, we provide weak annotations of organ presence for every single image. This dataset markedly expands the horizon for surgical data science applications of computer vision in laparoscopic surgery and could thereby contribute to a reduction of risks and faster translation of Artificial Intelligence into surgical practice.},
	language = {en},
	number = {1},
	urldate = {2023-02-03},
	journal = {Scientific Data},
	author = {Carstens, Matthias and Rinner, Franziska M. and Bodenstedt, Sebastian and Jenke, Alexander C. and Weitz, Jürgen and Distler, Marius and Speidel, Stefanie and Kolbinger, Fiona R.},
	month = jan,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Endoscopy, Gastrointestinal system, Translational research},
	pages = {3},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\66B2KZXK\\Carstens et al. - 2023 - The Dresden Surgical Anatomy Dataset for Abdominal.pdf:application/pdf},
}

@article{kang_bridging_2022,
	title = {Bridging {Feature} {Gaps} to {Improve} {Multi}-{Organ} {Segmentation} on {Abdominal} {Magnetic} {Resonance} {Image}},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2022.3229315},
	abstract = {Accurate segmentation of abdominal organs on MRI is crucial for computer-aided surgery and computer-aided diagnosis. Most state-of-the-art methods for MRI segmentation employ an encoder-decoder structure, with skip connections concatenating shallow features from the encoder and deep features from the decoder. In this work, we noticed that simply concatenating shallow and deep features was insufficient for segmentation due to the feature gap between shallow features and deep features. To mitigate this problem, we quantified the feature gap from spatial and semantic aspects and proposed a spatial loss and a semantic loss to bridge the feature gap. The spatial loss enhanced spatial details in deep features, and the semantic loss introduced semantic information into shallow features. The proposed method successfully aggregated the complementary information between shallow and deep features by formulating and bridging the feature gap. Experiments on two abdominal MRI datasets demonstrated the effectiveness of the proposed method, which improved the segmentation performance over a baseline with nearly zero additional parameters. Particularly, the proposed method has advantages for segmenting organs with blurred boundaries or in a small scale, achieving superior performance than state-of-the-art methods.},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Kang, Susu and Yang, Muyuan and Qi, X. Sharon and Jiang, Jun and Tan, Shan},
	year = {2022},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	keywords = {Feature extraction, deep learning, Image segmentation, convolutional neural networks, Semantics, Bioinformatics, Decoding, feature aggregation, Fuses, Logic gates, MRI segmentation},
	pages = {1--11},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9JM8IXVW\\9987670.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\B6FLJJRD\\Kang et al. - 2022 - Bridging Feature Gaps to Improve Multi-Organ Segme.pdf:application/pdf},
}

@inproceedings{laina_concurrent_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Concurrent {Segmentation} and {Localization} for {Tracking} of {Surgical} {Instruments}},
	isbn = {978-3-319-66185-8},
	doi = {10.1007/978-3-319-66185-8_75},
	abstract = {Real-time instrument tracking is a crucial requirement for various computer-assisted interventions. To overcome problems such as specular reflection and motion blur, we propose a novel method that takes advantage of the interdependency between localization and segmentation of the surgical tool. In particular, we reformulate the 2D pose estimation as a heatmap regression and thereby enable a robust, concurrent regression of both tasks via deep learning. Throughout experimental results, we demonstrate that this modeling leads to a significantly better performance than directly regressing the tool position and that our method outperforms the state-of-the-art on a Retinal Microsurgery benchmark and the MICCAI EndoVis Challenge 2015.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} − {MICCAI} 2017},
	publisher = {Springer International Publishing},
	author = {Laina, Iro and Rieke, Nicola and Rupprecht, Christian and Vizcaíno, Josué Page and Eslami, Abouzar and Tombari, Federico and Navab, Nassir},
	editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D. Louis and Duchesne, Simon},
	year = {2017},
	keywords = {Concurrent Regression, Heatmap Regression, Real-time Instrument Tracking, Retinal Microsurgery (RM), Strong Illumination Changes},
	pages = {664--672},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\J2W75AJH\\Laina et al. - 2017 - Concurrent Segmentation and Localization for Track.pdf:application/pdf},
}

@inproceedings{garcia-peraza-herrera_toolnet_2017,
	title = {{ToolNet}: {Holistically}-nested real-time segmentation of robotic surgical tools},
	shorttitle = {{ToolNet}},
	doi = {10.1109/IROS.2017.8206462},
	abstract = {Real-time tool segmentation from endoscopic videos is an essential part of many computer-assisted robotic surgical systems and of critical importance in robotic surgical data science. We propose two novel deep learning architectures for automatic segmentation of non-rigid surgical instruments. Both methods take advantage of automated deep-learning-based multi-scale feature extraction while trying to maintain an accurate segmentation quality at all resolutions. The two proposed methods encode the multi-scale constraint inside the network architecture. The first proposed architecture enforces it by cascaded aggregation of predictions and the second proposed network does it by means of a holistically-nested architecture where the loss at each scale is taken into account for the optimization process. As the proposed methods are for real-time semantic labeling, both present a reduced number of parameters. We propose the use of parametric rectified linear units for semantic labeling in these small architectures to increase the regularization of the network while maintaining the segmentation accuracy. We compare the proposed architectures against state-of-the-art fully convolutional networks. We validate our methods using existing benchmark datasets, including ex vivo cases with phantom tissue and different robotic surgical instruments present in the scene. Our results show a statistically significant improved Dice Similarity Coefficient over previous instrument segmentation methods. We analyze our design choices and discuss the key drivers for improving accuracy.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {García-Peraza-Herrera, Luis C. and Li, Wenqi and Fidon, Lucas and Gruijthuijsen, Caspar and Devreker, Alain and Attilakos, George and Deprest, Jan and Poorten, Emmanuel Vander and Stoyanov, Danail and Vercauteren, Tom and Ourselin, Sébastien},
	month = sep,
	year = {2017},
	note = {ISSN: 2153-0866},
	keywords = {Surgery, Image segmentation, Computer architecture, Instruments, Real-time systems, Robots, Tools},
	pages = {5717--5722},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3XXG67VH\\8206462.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5JIIM86T\\García-Peraza-Herrera et al. - 2017 - ToolNet Holistically-nested real-time segmentatio.pdf:application/pdf},
}

@inproceedings{milletari_cfcm_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{CFCM}: {Segmentation} via {Coarse} to {Fine} {Context} {Memory}},
	isbn = {978-3-030-00937-3},
	shorttitle = {{CFCM}},
	doi = {10.1007/978-3-030-00937-3_76},
	abstract = {Recent neural-network-based architectures for image segmentation make extensive usage of feature forwarding mechanisms to integrate information from multiple scales. Although yielding good results, even deeper architectures and alternative methods for feature fusion at different resolutions have been scarcely investigated for medical applications. In this work we propose to implement segmentation via an encoder-decoder architecture which differs from any other previously published method since (i) it employs a very deep architecture based on residual learning and (ii) combines features via a convolutional Long Short Term Memory (LSTM), instead of concatenation or summation. The intuition is that the memory mechanism implemented by LSTMs can better integrate features from different scales through a coarse-to-fine strategy; hence the name Coarse-to-Fine Context Memory (CFCM). We demonstrate the remarkable advantages of this approach on two datasets: the Montgomery county lung segmentation dataset, and the EndoVis 2015 challenge dataset for surgical instrument segmentation.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2018},
	publisher = {Springer International Publishing},
	author = {Milletari, Fausto and Rieke, Nicola and Baust, Maximilian and Esposito, Marco and Navab, Nassir},
	editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-López, Carlos and Fichtinger, Gabor},
	year = {2018},
	pages = {667--674},
	file = {Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QPVSX39Q\\Milletari et al. - 2018 - CFCM Segmentation via Coarse to Fine Context Memo.pdf:application/pdf},
}

@inproceedings{pakhomov_searching_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Searching for {Efficient} {Architecture} for {Instrument} {Segmentation} in {Robotic} {Surgery}},
	isbn = {978-3-030-59716-0},
	doi = {10.1007/978-3-030-59716-0_62},
	abstract = {Segmentation of surgical instruments is an important problem in robot-assisted surgery: it is a crucial step towards full instrument pose estimation and is directly used for masking of augmented reality overlays during surgical procedures. Most applications rely on accurate real-time segmentation of high-resolution surgical images. While previous research focused primarily on methods that deliver high accuracy segmentation masks, majority of them can not be used for real-time applications due to their computational cost. In this work, we design a light-weight and highly-efficient deep residual architecture which is tuned to perform real-time inference of high-resolution images. To account for reduced accuracy of the discovered light-weight deep residual network and avoid adding any additional computational burden, we perform a differentiable search over dilation rates for residual units of our network. We test our discovered architecture on the EndoVis 2017 Robotic Instruments dataset and verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff with a speed of up to 125 FPS on high resolution images.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Pakhomov, Daniil and Navab, Nassir},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	pages = {648--656},
	file = {Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QHJPV2MJ\\Pakhomov and Navab - 2020 - Searching for Efficient Architecture for Instrumen.pdf:application/pdf},
}

@inproceedings{shvets_automatic_2018,
	title = {Automatic {Instrument} {Segmentation} in {Robot}-{Assisted} {Surgery} using {Deep} {Learning}},
	doi = {10.1109/ICMLA.2018.00100},
	abstract = {Semantic segmentation of robotic instruments is an important problem for the robot-assisted surgery. One of the main challenges is to correctly detect an instrument's position for the tracking and pose estimation in the vicinity of surgical scenes. Accurate pixel-wise instrument segmentation is needed to address this challenge. In this paper we describe our deep learning-based approach for robotic instrument segmentation. Our approach demonstrates an improvement over the state-of-the-art results using several deep neural network architectures. It addressed the binary segmentation problem, where every pixel in an image is labeled as an instrument or background from the surgery video feed. In addition, we address a multi-class segmentation problem, in which we distinguish between different instruments or different parts of an instrument from the background. In this setting, our approach outperforms other methods for automatic instrument segmentation thereby providing state-of-the-art results for these problems. The source code for our solution is made publicly available at https://github.com/ternaus/robot-surgery-segmentation.},
	booktitle = {2018 17th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Shvets, Alexey A. and Rakhlin, Alexander and Kalinin, Alexandr A. and Iglovikov, Vladimir I.},
	month = dec,
	year = {2018},
	keywords = {image segmentation, computer vision, Surgery, Training, Convolution, deep learning, Image segmentation, Task analysis, robot-assisted surgery, Instruments, Robots, medical imaging},
	pages = {624--628},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ALRTY9UK\\8614125.html:text/html;Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\G64IJKPW\\Shvets et al. - 2018 - Automatic Instrument Segmentation in Robot-Assiste.pdf:application/pdf},
}

@article{islam_real-time_2019,
	title = {Real-{Time} {Instrument} {Segmentation} in {Robotic} {Surgery} {Using} {Auxiliary} {Supervised} {Deep} {Adversarial} {Learning}},
	volume = {4},
	issn = {2377-3766},
	doi = {10.1109/LRA.2019.2900854},
	abstract = {Robot-assisted surgery is an emerging technology that has undergone rapid growth with the development of robotics and imaging systems. Innovations in vision, haptics, and accurate movements of robot arms have enabled surgeons to perform precise minimally invasive surgeries. Real-time semantic segmentation of the robotic instruments and tissues is a crucial step in robot-assisted surgery. Accurate and efficient segmentation of the surgical scene not only aids in the identification and tracking of instruments but also provides contextual information about the different tissues and instruments being operated with. For this purpose, we have developed a light-weight cascaded convolutional neural network to segment the surgical instruments from high-resolution videos obtained from a commercial robotic system. We propose a multi-resolution feature fusion module to fuse the feature maps of different dimensions and channels from the auxiliary and main branch. We also introduce a novel way of combining auxiliary loss and adversarial loss to regularize the segmentation model. Auxiliary loss helps the model to learn low-resolution features, and adversarial loss improves the segmentation prediction by learning higher order structural information. The model also consists of a light-weight spatial pyramid pooling unit to aggregate rich contextual information in the intermediate stage. We show that our model surpasses existing algorithms for pixelwise segmentation of surgical instruments in both prediction accuracy and segmentation time of high-resolution videos.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Islam, Mobarakol and Atputharuban, Daniel Anojan and Ramesh, Ravikiran and Ren, Hongliang},
	month = apr,
	year = {2019},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Surgery, Image segmentation, object detection, Instruments, Real-time systems, Robots, Tools, Deep learning in robotics and automation, segmentation and categorization, Tracking, visual tracking},
	pages = {2188--2195},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JHW7Q5IQ\\8648150.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\HVCT4XYK\\Islam et al. - 2019 - Real-Time Instrument Segmentation in Robotic Surge.pdf:application/pdf},
}

@inproceedings{kamrul_hasan_u-netplus_2019,
	title = {U-{NetPlus}: {A} {Modified} {Encoder}-{Decoder} {U}-{Net} {Architecture} for {Semantic} and {Instance} {Segmentation} of {Surgical} {Instruments} from {Laparoscopic} {Images}},
	shorttitle = {U-{NetPlus}},
	doi = {10.1109/EMBC.2019.8856791},
	abstract = {With the advent of robot-assisted surgery, there has been a paradigm shift in medical technology for minimally invasive surgery. However, it is very challenging to track the position of the surgical instruments in a surgical scene, and accurate detection \& identification of surgical tools is paramount. Deep learning-based semantic segmentation in frames of surgery videos has the potential to facilitate this task. In this work, we modify the U-Net architecture by introducing a pre-trained encoder and re-design the decoder part, by replacing the transposed convolution operation with an upsampling operation based on nearest-neighbor (NN) interpolation. To further improve performance, we also employ a very fast and flexible data augmentation technique. We trained the framework on 8 × 225 frame sequences of robotic surgical videos available through the MICCAI 2017 EndoVis Challenge dataset and tested it on 8 × 75 frame and 2 × 300 frame videos. Using our U-NetPlus architecture, we report a 90.20\% DICE for binary segmentation, 76.26\% DICE for instrument part segmentation, and 46.07\% for instrument type (i.e., all instruments) segmentation, outperforming the results of previous techniques implemented and tested on these data.},
	booktitle = {2019 41st {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Kamrul Hasan, S. M. and Linte, Cristian A.},
	month = jul,
	year = {2019},
	note = {ISSN: 1558-4615},
	keywords = {Surgery, Training, Convolution, Image segmentation, Computer architecture, Instruments, Interpolation},
	pages = {7205--7211},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\B4VLXHJR\\8856791.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ZHK64UXP\\Kamrul Hasan and Linte - 2019 - U-NetPlus A Modified Encoder-Decoder U-Net Archit.pdf:application/pdf},
}

@inproceedings{attia_surgical_2017,
	title = {Surgical tool segmentation using a hybrid deep {CNN}-{RNN} auto encoder-decoder},
	doi = {10.1109/SMC.2017.8123151},
	abstract = {Surgical tool segmentation is used for detection, tracking and pose estimation of the tools in the vicinity of surgical scenes. It is considered as an essential task in surgical phase recognition and flow identification. Surgical flow identification is an unresolved task in the domain of context-aware surgical systems, which is used extensively on computer assisted intervention (CAI). CAI is used for staff assignment, automated guidance during intervention, surgical alert systems, automatic indexing of surgical video databases and optimisation of the real-time scheduling of operating room. Semantic segmentation is used for accurate delineation of surgical tools from the background. In semantic segmentation, each label is assigned to a class as a tool or a background. In this presented work, we applied a hybrid method utilising both recurrent and convolutional networks to achieve higher accuracy of surgical tools segmentation. The proposed method is trained and tested using a public dataset MICCAI 2016 Endoscopic Vision Challenge Robotic Instruments dataset "EndoVis". We achieved better performance using the proposed method compared to state-of-the-art methods on the same dataset for benchmarking. We achieved a balanced accuracy of 93.3\% and Jaccard index of 82.7\%.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Attia, Mohamed and Hossny, Mohammed and Nahavandi, Saeid and Asadi, Hamed},
	month = oct,
	year = {2017},
	keywords = {Surgery, Feature extraction, Image segmentation, Semantics, Tools, Recurrent neural networks},
	pages = {3373--3378},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XE69VZNI\\8123151.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4CHT98X7\\Attia et al. - 2017 - Surgical tool segmentation using a hybrid deep CNN.pdf:application/pdf},
}

@inproceedings{sahu_endo-sim2real_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Endo-{Sim2Real}: {Consistency} {Learning}-{Based} {Domain} {Adaptation} for {Instrument} {Segmentation}},
	isbn = {978-3-030-59716-0},
	shorttitle = {Endo-{Sim2Real}},
	doi = {10.1007/978-3-030-59716-0_75},
	abstract = {Surgical tool segmentation in endoscopic videos is an important component of computer assisted interventions systems. Recent success of image-based solutions using fully-supervised deep learning approaches can be attributed to the collection of big labeled datasets. However, the annotation of a big dataset of real videos can be prohibitively expensive and time consuming. Computer simulations could alleviate the manual labeling problem, however, models trained on simulated data do not generalize to real data. This work proposes a consistency-based framework for joint learning of simulated and real (unlabeled) endoscopic data to bridge this performance generalization issue. Empirical results on two data sets (15 videos of the Cholec80 and EndoVis’15 dataset) highlight the effectiveness of the proposed Endo-Sim2Real method for instrument segmentation. We compare the segmentation of the proposed approach with state-of-the-art solutions and show that our method improves segmentation both in terms of quality and quantity.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Sahu, Manish and Strömsdörfer, Ronja and Mukhopadhyay, Anirban and Zachow, Stefan},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {Consistency learning, Endoscopic instrument segmentation, Self-supervised learning, Unsupervised domain adaptation},
	pages = {784--794},
	file = {Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Q3IRERSM\\Sahu et al. - 2020 - Endo-Sim2Real Consistency Learning-Based Domain A.pdf:application/pdf},
}

@article{lee_weakly_2019,
	title = {Weakly supervised segmentation for real-time surgical tool tracking},
	volume = {6},
	issn = {2053-3713},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/htl.2019.0083},
	doi = {10.1049/htl.2019.0083},
	abstract = {Surgical tool tracking has a variety of applications in different surgical scenarios. Electromagnetic (EM) tracking can be utilised for tool tracking, but the accuracy is often limited by magnetic interference. Vision-based methods have also been suggested; however, tracking robustness is limited by specular reflection, occlusions, and blurriness observed in the endoscopic image. Recently, deep learning-based methods have shown competitive performance on segmentation and tracking of surgical tools. The main bottleneck of these methods lies in acquiring a sufficient amount of pixel-wise, annotated training data, which demands substantial labour costs. To tackle this issue, the authors propose a weakly supervised method for surgical tool segmentation and tracking based on hybrid sensor systems. They first generate semantic labellings using EM tracking and laparoscopic image processing concurrently. They then train a light-weight deep segmentation network to obtain a binary segmentation mask that enables tool tracking. To the authors’ knowledge, the proposed method is the first to integrate EM tracking and laparoscopic image processing for generation of training labels. They demonstrate that their framework achieves accurate, automatic tool segmentation (i.e. without any manual labelling of the surgical tool to be tracked) and robust tool tracking in laparoscopic image sequences.},
	language = {en},
	number = {6},
	urldate = {2023-02-03},
	journal = {Healthcare Technology Letters},
	author = {Lee, Eung-Joo and Plishker, William and Liu, Xinyang and Bhattacharyya, Shuvra S. and Shekhar, Raj},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/htl.2019.0083},
	keywords = {image segmentation, computer vision, endoscopes, image sequences, medical image processing, surgery, neural nets, learning (artificial intelligence), annotated training data, automatic tool segmentation, binary segmentation mask, deep learning-based methods, electromagnetic tracking, laparoscopic image processing, light-weight deep segmentation network, medical robotics, pixel-wise training data, real-time surgical tool tracking, robust tool tracking, supervised segmentation, surgical scenarios, surgical tool segmentation, tracking, tracking robustness, vision-based methods},
	pages = {231--236},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FK5BJC9Y\\Lee et al. - 2019 - Weakly supervised segmentation for real-time surgi.pdf:application/pdf},
}

@inproceedings{kletz_identifying_2019,
	title = {Identifying {Surgical} {Instruments} in {Laparoscopy} {Using} {Deep} {Learning} {Instance} {Segmentation}},
	doi = {10.1109/CBMI.2019.8877379},
	abstract = {Recorded videos from surgeries have become an increasingly important information source for the field of medical endoscopy, since the recorded footage shows every single detail of the surgery. However, while video recording is straightforward these days, automatic content indexing - the basis for content-based search in a medical video archive - is still a great challenge due to the very special video content. In this work, we investigate segmentation and recognition of surgical instruments in videos recorded from laparoscopic gynecology. More precisely, we evaluate the achievable performance of segmenting surgical instruments from their background by using a region-based fully convolutional network for instance-aware (1) instrument segmentation as well as (2) instrument recognition. While the first part addresses only binary segmentation of instances (i.e., distinguishing between instrument or background) we also investigate multi-class instrument recognition (i.e., identifying the type of instrument). Our evaluation results show that even with a moderately low number of training examples, we are able to localize and segment instrument regions with a pretty high accuracy. However, the results also reveal that determining the particular instrument is still very challenging, due to the inherently high similarity of surgical instruments.},
	booktitle = {2019 {International} {Conference} on {Content}-{Based} {Multimedia} {Indexing} ({CBMI})},
	author = {Kletz, Sabrina and Schoeffmann, Klaus and Benois-Pineau, Jenny and Husslein, Heinrich},
	month = sep,
	year = {2019},
	note = {ISSN: 1949-3991},
	keywords = {Surgery, deep learning, Image segmentation, Laparoscopes, Task analysis, data augmentation, Instruments, Robots, instance segmentation, laparoscopic videos, region-based convolutional neural network, surgical instruments, Videos},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Y5NFRS6Q\\8877379.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KXA3SI6U\\Kletz et al. - 2019 - Identifying Surgical Instruments in Laparoscopy Us.pdf:application/pdf},
}

@inproceedings{liu_unsupervised_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Unsupervised {Surgical} {Instrument} {Segmentation} via {Anchor} {Generation} and {Semantic} {Diffusion}},
	isbn = {978-3-030-59716-0},
	doi = {10.1007/978-3-030-59716-0_63},
	abstract = {Surgical instrument segmentation is a key component in developing context-aware operating rooms. Existing works on this task heavily rely on the supervision of a large amount of labeled data, which involve laborious and expensive human efforts. In contrast, a more affordable unsupervised approach is developed in this paper. To train our model, we first generate anchors as pseudo labels for instruments and background tissues respectively by fusing coarse handcrafted cues. Then a semantic diffusion loss is proposed to resolve the ambiguity in the generated anchors via the feature correlation between adjacent video frames. In the experiments on the binary instrument segmentation task of the 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge dataset, the proposed method achieves 0.71 IoU and 0.81 Dice score without using a single manual annotation, which is promising to show the potential of unsupervised learning for surgical tool segmentation.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Liu, Daochang and Wei, Yuhui and Jiang, Tingting and Wang, Yizhou and Miao, Rulin and Shan, Fei and Li, Ziyu},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {Semantic diffusion, Surgical instrument segmentation, Unsupervised learning},
	pages = {657--667},
	file = {Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\K8BRR5DN\\Liu et al. - 2020 - Unsupervised Surgical Instrument Segmentation via .pdf:application/pdf},
}

@inproceedings{ni_attention-guided_2020,
	title = {Attention-{Guided} {Lightweight} {Network} for {Real}-{Time} {Segmentation} of {Robotic} {Surgical} {Instruments}},
	doi = {10.1109/ICRA40945.2020.9197425},
	abstract = {The real-time segmentation of surgical instruments plays a crucial role in robot-assisted surgery. However, it is still a challenging task to implement deep learning models to do real-time segmentation for surgical instruments due to their high computational costs and slow inference speed. In this paper, we propose an attention-guided lightweight network (LWANet), which can segment surgical instruments in real-time. LWANet adopts encoder-decoder architecture, where the encoder is the lightweight network MobileNetV2, and the decoder consists of depthwise separable convolution, attention fusion block, and transposed convolution. Depthwise separable convolution is used as the basic unit to construct the decoder, which can reduce the model size and computational costs. Attention fusion block captures global contexts and encodes semantic dependencies between channels to emphasize target regions, contributing to locating the surgical instrument. Transposed convolution is performed to upsample feature maps for acquiring refined edges. LWANet can segment surgical instruments in real-time while takes little computational costs. Based on 960x544 inputs, its inference speed can reach 39 fps with only 3.39 GFLOPs. Also, it has a small model size and the number of parameters is only 2.06 M. The proposed network is evaluated on two datasets. It achieves state-of-the- art performance 94.10\% mean IOU on Cata7 and obtains a new record on EndoVis 2017 with a 4.10\% increase on mean IOU.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Ni, Zhen-Liang and Bian, Gui-Bin and Hou, Zeng-Guang and Zhou, Xiao-Hu and Xie, Xiao-Liang and Li, Zhen},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Surgery, Convolution, Computational efficiency, Semantics, Decoding, Instruments, Real-time systems},
	pages = {9939--9945},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KZ7HN6R9\\9197425.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\ELR9DEGQ\\Ni et al. - 2020 - Attention-Guided Lightweight Network for Real-Time.pdf:application/pdf},
}

@inproceedings{colleoni_synthetic_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Synthetic and {Real} {Inputs} for {Tool} {Segmentation} in {Robotic} {Surgery}},
	isbn = {978-3-030-59716-0},
	doi = {10.1007/978-3-030-59716-0_67},
	abstract = {Semantic tool segmentation in surgical videos is important for surgical scene understanding and computer-assisted interventions as well as for the development of robotic automation. The problem is challenging because different illumination conditions, bleeding, smoke and occlusions can reduce algorithm robustness. At present labelled data for training deep learning models is still lacking for semantic surgical instrument segmentation and in this paper we show that it may be possible to use robot kinematic data coupled with laparoscopic images to alleviate the labelling problem. We propose a new deep learning based model for parallel processing of both laparoscopic and simulation images for robust segmentation of surgical tools. Due to the lack of laparoscopic frames annotated with both segmentation ground truth and kinematic information a new custom dataset was generated using the da Vinci Research Kit (dVRK) and is made available.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Colleoni, Emanuele and Edwards, Philip and Stoyanov, Danail},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {Computer assisted interventions, Instrument detection and segmentation, Surgical vision},
	pages = {700--710},
	file = {Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9ZUSGLS9\\Colleoni et al. - 2020 - Synthetic and Real Inputs for Tool Segmentation in.pdf:application/pdf},
}

@article{lu_learning_2020,
	title = {A {Learning} {Approach} for {Suture} {Thread} {Detection} {With} {Feature} {Enhancement} and {Segmentation} for 3-{D} {Shape} {Reconstruction}},
	volume = {17},
	issn = {1558-3783},
	doi = {10.1109/TASE.2019.2950005},
	abstract = {A vision-based system presents one of the most reliable methods for achieving an automated robot-assisted manipulation associated with surgical knot tying. However, some challenges in suture thread detection and automated suture thread grasping significantly hinder the realization of a fully automated surgical knot tying. In this article, we propose a novel algorithm that can be used for computing the 3-D coordinates of a suture thread in knot tying. After proper training with our data set, we built a deep-learning model for accurately locating the suture's tip. By applying a Hessian-based filter with multiscale parameters, the environmental noises can be eliminated while preserving the suture thread information. A multistencils fast marching method was then employed to segment the suture thread, and a precise stereomatching algorithm was implemented to compute the 3-D coordinates of this thread. Experiments associated with the precision of the deep-learning model, the robustness of the 2-D segmentation approach, and the overall accuracy of 3-D coordinate computation of the suture thread were conducted in various scenarios, and the results quantitatively validate the feasibility and reliability of the entire scheme for automated 3-D shape reconstruction.},
	number = {2},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Lu, Bo and Yu, X. B. and Lai, J. W. and Huang, K. C. and Chan, Keith C. C. and Chu, Henry K.},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Automation Science and Engineering},
	keywords = {Feature extraction, Image segmentation, Computational modeling, 3-D coordinates computation, Instruction sets, Robot kinematics, stereovision, surgical robot, suture thread detection, Yarn},
	pages = {858--870},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7R7C8QPJ\\8913674.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\GGKFKL4T\\Lu et al. - 2020 - A Learning Approach for Suture Thread Detection Wi.pdf:application/pdf},
}

@inproceedings{fu_more_2019-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {More {Unlabelled} {Data} or {Label} {More} {Data}? {A} {Study} on {Semi}-supervised {Laparoscopic} {Image} {Segmentation}},
	isbn = {978-3-030-33391-1},
	shorttitle = {More {Unlabelled} {Data} or {Label} {More} {Data}?},
	doi = {10.1007/978-3-030-33391-1_20},
	abstract = {Improving a semi-supervised image segmentation task has the option of adding more unlabelled images, labelling the unlabelled images or combining both, as neither image acquisition nor expert labelling can be considered trivial in most clinical applications. With a laparoscopic liver image segmentation application, we investigate the performance impact by altering the quantities of labelled and unlabelled training data, using a semi-supervised segmentation algorithm based on the mean teacher learning paradigm. We first report a significantly higher segmentation accuracy, compared with supervised learning. Interestingly, this comparison reveals that the training strategy adopted in the semi-supervised algorithm is also responsible for this observed improvement, in addition to the added unlabelled data. We then compare different combinations of labelled and unlabelled data set sizes for training semi-supervised segmentation networks, to provide a quantitative example of the practically useful trade-off between the two data planning strategies in this surgical guidance application.},
	language = {en},
	booktitle = {Domain {Adaptation} and {Representation} {Transfer} and {Medical} {Image} {Learning} with {Less} {Labels} and {Imperfect} {Data}},
	publisher = {Springer International Publishing},
	author = {Fu, Yunguan and Robu, Maria R. and Koo, Bongjin and Schneider, Crispin and van Laarhoven, Stijn and Stoyanov, Danail and Davidson, Brian and Clarkson, Matthew J. and Hu, Yipeng},
	editor = {Wang, Qian and Milletari, Fausto and Nguyen, Hien V. and Albarqouni, Shadi and Cardoso, M. Jorge and Rieke, Nicola and Xu, Ziyue and Kamnitsas, Konstantinos and Patel, Vishal and Roysam, Badri and Jiang, Steve and Zhou, Kevin and Luu, Khoa and Le, Ngan},
	year = {2019},
	keywords = {Image segmentation, Laparoscopic video, Semi-supervised},
	pages = {173--180},
	file = {Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2W56Z4KK\\Fu et al. - 2019 - More Unlabelled Data or Label More Data A Study o.pdf:application/pdf},
}

@inproceedings{kadkhodamohammadi_feature_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Feature {Aggregation} {Decoder} for {Segmenting} {Laparoscopic} {Scenes}},
	isbn = {978-3-030-32695-1},
	doi = {10.1007/978-3-030-32695-1_1},
	abstract = {Laparoscopic scene segmentation is one of the key building blocks required for developing advanced computer assisted interventions and robotic automation. Scene segmentation approaches often rely on encoder-decoder architectures that encode a representation of the input to be decoded to semantic pixel labels. In this paper, we propose to use the deep Xception model for the encoder and a simple yet effective decoder that relies on a feature aggregation module. Our feature aggregation module constructs a mapping function that reuses and transfers encoder features and combines information across all feature scales to build a richer representation that keeps both high-level context and low-level boundary information. We argue that this aggregation module enables us to simplify the decoder and reduce the number of parameters in the decoder. We have evaluated our approach on two datasets and our experimental results show that our model outperforms state-of-the-art models on the same experimental setup and significantly improves the previous results, \$\$98.44{\textbackslash}\%\$\$vs \$\$89.00{\textbackslash}\%\$\$, on the EndoVis’15 dataset.},
	language = {en},
	booktitle = {{OR} 2.0 {Context}-{Aware} {Operating} {Theaters} and {Machine} {Learning} in {Clinical} {Neuroimaging}},
	publisher = {Springer International Publishing},
	author = {Kadkhodamohammadi, Abdolrahim and Luengo, Imanol and Barbarisi, Santiago and Taleb, Hinde and Flouty, Evangello and Stoyanov, Danail},
	editor = {Zhou, Luping and Sarikaya, Duygu and Kia, Seyed Mostafa and Speidel, Stefanie and Malpani, Anand and Hashimoto, Daniel and Habes, Mohamad and Löfstedt, Tommy and Ritter, Kerstin and Wang, Hongzhi},
	year = {2019},
	keywords = {Minimally invasive surgery, Semantic segmentation, Surgical vision},
	pages = {3--11},
	file = {Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QHRXLVUS\\Kadkhodamohammadi et al. - 2019 - Feature Aggregation Decoder for Segmenting Laparos.pdf:application/pdf},
}

@article{nazir_spst-cnn_2020,
	title = {{SPST}-{CNN}: {Spatial} pyramid based searching and tagging of liver’s intraoperative live views via {CNN} for minimal invasive surgery},
	volume = {106},
	issn = {1532-0464},
	shorttitle = {{SPST}-{CNN}},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046420300587},
	doi = {10.1016/j.jbi.2020.103430},
	abstract = {Laparoscopic liver surgery is challenging to perform because of compromised ability of the surgeon to localize subsurface anatomy due to minimal invasive visibility. While image guidance has the potential to address this barrier, intraoperative factors, such as insufflations and variable degrees of organ mobilization from supporting ligaments, may generate substantial deformation. The navigation ability in terms of searching and tagging within liver views has not been characterized, and current object detection methods do not account for the mechanics of how these features could be applied to the liver images. In this research, we have proposed spatial pyramid based searching and tagging of liver’s intraoperative views using convolution neural network (SPST-CNN). By exploiting a hybrid combination of an image pyramid at input and spatial pyramid pooling layer at deeper stages of SPST-CNN, we reveal the gains of full-image representations for searching and tagging variable scaled liver live views. SPST-CNN provides pinpoint searching and tagging of intraoperative liver views to obtain up-to-date information about the location and shape of the area of interest. Downsampling input using image pyramid enables SPST-CNN framework to deploy input images with a diversity of resolutions for achieving scale-invariance feature. We have compared the proposed approach to the four recent state-of-the-art approaches and our method achieved better mAP up to 85.9\%.},
	language = {en},
	urldate = {2023-02-03},
	journal = {Journal of Biomedical Informatics},
	author = {Nazir, Anam and Cheema, Muhammad Nadeem and Sheng, Bin and Li, Ping and Li, Huating and Yang, Po and Jung, Younhyun and Qin, Jing and Feng, David Dagan},
	month = jun,
	year = {2020},
	keywords = {Convolution neural network, Hybrid combination, Laparoscopy, Liver’s intraoperative views, Minimal invasive surgery, Navigation systems},
	pages = {103430},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QIJCUR8J\\Nazir et al. - 2020 - SPST-CNN Spatial pyramid based searching and tagg.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IASJA842\\S1532046420300587.html:text/html},
}

@article{hwang_adaptive_2019,
	title = {An {Adaptive} {Regularization} {Approach} to {Colonoscopic} {Polyp} {Detection} {Using} a {Cascaded} {Structure} of {Encoder}–{Decoders}},
	volume = {21},
	issn = {2199-3211},
	url = {https://doi.org/10.1007/s40815-019-00694-y},
	doi = {10.1007/s40815-019-00694-y},
	abstract = {This research aims to segment colonoscopic images by automatically extracting polyp features by exploiting the strengths of convolution neural networks (CNN). The proposed model employs deep learning and adaptive regularization techniques. The model is structurally composed of two cascaded encoder–decoder networks, each of which is constructed by four CNN layers and two full connection layers. The front model is built on backpropagation learning for segmenting a colonoscopic polyp image. The output images from the precedent hetero-encoder are regarded as corrupted labeled images, especially during the time period close to the end of learning, and are selectively fed into the successive auto-encoder for denoising learning to enhance its discriminative power and relieve the problem of a lack of labeled data for medical image tasks. The performance of the proposed model can be further improved by a simple fuzzy logic approach setting the regularization parameter in the loss function. The proposed method utilizes features learned from some open medical datasets and our own collected dataset. The performance of the proposed architecture is compared with a state-of-the-art network. The evaluation shows the performances of the proposed method are consistent across all the datasets and often outperform the state-of-art model.},
	language = {en},
	number = {7},
	urldate = {2023-02-03},
	journal = {International Journal of Fuzzy Systems},
	author = {Hwang, Maxwell and Wang, Da and Jiang, Wei-Cheng and Pan, Xiang and Fu, Dongliang and Hwang, Kao-Shing and Ding, Kefeng},
	month = oct,
	year = {2019},
	keywords = {Convolution neural networks, Encoder–decoder networks, Fuzzy logic},
	pages = {2091--2101},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XLR754ER\\Hwang et al. - 2019 - An Adaptive Regularization Approach to Colonoscopi.pdf:application/pdf},
}

@article{kitaguchi_limited_2022,
	title = {Limited generalizability of single deep neural network for surgical instrument segmentation in different surgical environments},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-16923-8},
	doi = {10.1038/s41598-022-16923-8},
	abstract = {Clarifying the generalizability of deep-learning-based surgical-instrument segmentation networks in diverse surgical environments is important in recognizing the challenges of overfitting in surgical-device development. This study comprehensively evaluated deep neural network generalizability for surgical instrument segmentation using 5238 images randomly extracted from 128 intraoperative videos. The video dataset contained 112 laparoscopic colorectal resection, 5 laparoscopic distal gastrectomy, 5 laparoscopic cholecystectomy, and 6 laparoscopic partial hepatectomy cases. Deep-learning-based surgical-instrument segmentation was performed for test sets with (1) the same conditions as the training set; (2) the same recognition target surgical instrument and surgery type but different laparoscopic recording systems; (3) the same laparoscopic recording system and surgery type but slightly different recognition target laparoscopic surgical forceps; (4) the same laparoscopic recording system and recognition target surgical instrument but different surgery types. The mean average precision and mean intersection over union for test sets 1, 2, 3, and 4 were 0.941 and 0.887, 0.866 and 0.671, 0.772 and 0.676, and 0.588 and 0.395, respectively. Therefore, the recognition accuracy decreased even under slightly different conditions. The results of this study reveal the limited generalizability of deep neural networks in the field of surgical artificial intelligence and caution against deep-learning-based biased datasets and models.},
	language = {en},
	number = {1},
	urldate = {2023-02-03},
	journal = {Scientific Reports},
	author = {Kitaguchi, Daichi and Fujino, Toru and Takeshita, Nobuyoshi and Hasegawa, Hiro and Mori, Kensaku and Ito, Masaaki},
	month = jul,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Colorectal cancer, Information storage},
	pages = {12575},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XLX9VVHV\\Kitaguchi et al. - 2022 - Limited generalizability of single deep neural net.pdf:application/pdf},
}

@inproceedings{wang_rethinking_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Rethinking {Surgical} {Instrument} {Segmentation}: {A} {Background} {Image} {Can} {Be} {All} {You} {Need}},
	isbn = {978-3-031-16449-1},
	shorttitle = {Rethinking {Surgical} {Instrument} {Segmentation}},
	doi = {10.1007/978-3-031-16449-1_34},
	abstract = {Data diversity and volume are crucial to the success of training deep learning models, while in the medical imaging field, the difficulty and cost of data collection and annotation are especially huge. Specifically in robotic surgery, data scarcity and imbalance have heavily affected the model accuracy and limited the design and deployment of deep learning-based surgical applications such as surgical instrument segmentation. Considering this, we rethink the surgical instrument segmentation task and propose a one-to-many data generation solution that gets rid of the complicated and expensive process of data collection and annotation from robotic surgery. In our method, we only utilize a single surgical background tissue image and a few open-source instrument images as the seed images and apply multiple augmentations and blending techniques to synthesize amounts of image variations. In addition, we also introduce the chained augmentation mixing during training to further enhance the data diversities. The proposed approach is evaluated on the real datasets of the EndoVis-2018 and EndoVis-2017 surgical scene segmentation. Our empirical analysis suggests that without the high cost of data collection and annotation, we can achieve decent surgical instrument segmentation performance. Moreover, we also observe that our method can deal with novel instrument prediction in the deployment domain. We hope our inspiring results will encourage researchers to emphasize data-centric methods to overcome demanding deep learning limitations besides data shortage, such as class imbalance, domain adaptation, and incremental learning. Our code is available at https://github.com/lofrienger/Single\_SurgicalScene\_For\_Segmentation.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Wang, An and Islam, Mobarakol and Xu, Mengya and Ren, Hongliang},
	editor = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo},
	year = {2022},
	pages = {355--364},
	file = {Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\3QCNHHZF\\Wang et al. - 2022 - Rethinking Surgical Instrument Segmentation A Bac.pdf:application/pdf},
}

@article{kong_accurate_2021,
	title = {Accurate instance segmentation of surgical instruments in robotic surgery: model refinement and cross-dataset evaluation},
	volume = {16},
	issn = {1861-6429},
	shorttitle = {Accurate instance segmentation of surgical instruments in robotic surgery},
	url = {https://doi.org/10.1007/s11548-021-02438-6},
	doi = {10.1007/s11548-021-02438-6},
	abstract = {Automatic segmentation of surgical instruments in robot-assisted minimally invasive surgery plays a fundamental role in improving context awareness. In this work, we present an instance segmentation model based on refined Mask R-CNN for accurately segmenting the instruments as well as identifying their types.},
	language = {en},
	number = {9},
	urldate = {2023-02-03},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Kong, Xiaowen and Jin, Yueming and Dou, Qi and Wang, Ziyi and Wang, Zerui and Lu, Bo and Dong, Erbao and Liu, Yun-Hui and Sun, Dong},
	month = sep,
	year = {2021},
	keywords = {Surgical instrument segmentation, Cross-dataset evaluation, Instance segmentation, Robot-assisted surgery},
	pages = {1607--1614},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\KP84PCE5\\Kong et al. - 2021 - Accurate instance segmentation of surgical instrum.pdf:application/pdf},
}

@misc{ali_comprehensive_2023,
	title = {A comprehensive survey on recent deep learning-based methods applied to surgical data},
	url = {http://arxiv.org/abs/2209.01435},
	doi = {10.48550/arXiv.2209.01435},
	abstract = {Minimally invasive surgery is highly operator dependant with a lengthy procedural time causing fatigue to surgeon and risks to patients such as injury to organs, infection, bleeding, and complications of anesthesia. To mitigate such risks, real-time systems are desired to be developed that can provide intra-operative guidance to surgeons. For example, an automated system for tool localization, tool (or tissue) tracking, and depth estimation can enable a clear understanding of surgical scenes preventing miscalculations during surgical procedures. In this work, we present a systematic review of recent machine learning-based approaches including surgical tool localization, segmentation, tracking, and 3D scene perception. Furthermore, we provide a detailed overview of publicly available benchmark datasets widely used for surgical navigation tasks. While recent deep learning architectures have shown promising results, there are still several open research problems such as a lack of annotated datasets, the presence of artifacts in surgical scenes, and non-textured surfaces that hinder 3D reconstruction of the anatomical structures. Based on our comprehensive review, we present a discussion on current gaps and needed steps to improve the adaptation of technology in surgery.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Ali, Mansoor and Pena, Rafael Martinez Garcia and Ruiz, Gilberto Ochoa and Ali, Sharib},
	month = jan,
	year = {2023},
	note = {arXiv:2209.01435 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\S42TJU37\\Ali et al. - 2023 - A comprehensive survey on recent deep learning-bas.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2RTAPVVE\\2209.html:text/html},
}

@article{luo_fast_2023,
	title = {Fast instruments and tissues segmentation of micro-neurosurgical scene using high correlative non-local network},
	volume = {153},
	issn = {0010-4825},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482522012392},
	doi = {10.1016/j.compbiomed.2022.106531},
	abstract = {Surgical scene segmentation provides critical information for guidance in micro-neurosurgery. Segmentation of instruments and critical tissues contributes further to robot assisted surgery and surgical evaluation. However, due to the lack of relevant scene segmentation dataset, scale variation and local similarity, micro-neurosurgical segmentation faces many challenges. To address these issues, a high correlative non-local network (HCNNet), is proposed to aggregate multi-scale feature by optimized non-local mechanism. HCNNet adopts two-branch design to generate features of different scale efficiently, while the two branches share common weights in shallow layers. Several short-term dense concatenate (STDC) modules are combined as the backbone to capture both semantic and spatial information. Besides, a high correlative non-local module (HCNM) is designed to guide the upsampling process of the high-level feature by modeling global context generated from the low-level feature. It filters out confused pixels of different classes in the non-local correlation map. Meanwhile, a large segmentation dataset named NeuroSeg is constructed, which contains 15 types of instruments and 3 types of tissues that appear in meningioma resection surgery. The proposed HCNNet achieves the state-of-the-art performance on NeuroSeg, it reaches an inference speed of 54.85 FPS with the highest accuracy of 59.62\% mIoU, 74.7\% Dice, 70.55\% mAcc and 87.12\% aAcc.},
	language = {en},
	urldate = {2023-02-03},
	journal = {Computers in Biology and Medicine},
	author = {Luo, Yu-Wen and Chen, Hai-Yong and Li, Zhen and Liu, Wei-Peng and Wang, Ke and Zhang, Li and Fu, Pan and Yue, Wen-Qian and Bian, Gui-Bin},
	month = feb,
	year = {2023},
	keywords = {Micro-neurosurgery, Multi-scale feature fusion, Non-local attention mechanism, Real-time segmentation},
	pages = {106531},
	file = {ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MPD4ZFYK\\S0010482522012392.html:text/html},
}

@article{wu_scs-net_2021,
	title = {{SCS}-{Net}: {A} {Scale} and {Context} {Sensitive} {Network} for {Retinal} {Vessel} {Segmentation}},
	volume = {70},
	issn = {1361-8415},
	shorttitle = {{SCS}-{Net}},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521000712},
	doi = {10.1016/j.media.2021.102025},
	abstract = {Accurately segmenting retinal vessel from retinal images is essential for the detection and diagnosis of many eye diseases. However, it remains a challenging task due to (1) the large variations of scale in the retinal vessels and (2) the complicated anatomical context of retinal vessels, including complex vasculature and morphology, the low contrast between some vessels and the background, and the existence of exudates and hemorrhage. It is difficult for a model to capture representative and distinguishing features for retinal vessels under such large scale and semantics variations. Limited training data also make this task even harder. In order to comprehensively tackle these challenges, we propose a novel scale and context sensitive network (a.k.a., SCS−Net) for retinal vessel segmentation. We first propose a scale-aware feature aggregation (SFA) module, aiming at dynamically adjusting the receptive fields to effectively extract multi-scale features. Then, an adaptive feature fusion (AFF) module is designed to guide efficient fusion between adjacent hierarchical features to capture more semantic information. Finally, a multi-level semantic supervision (MSS) module is employed to learn more distinctive semantic representation for refining the vessel maps. We conduct extensive experiments on the six mainstream retinal image databases (DRIVE, CHASEDB1, STARE, IOSTAR, HRF, and LES-AV). The experimental results demonstrate the effectiveness of the proposed SCS-Net, which is capable of achieving better segmentation performance than other state-of-the-art approaches, especially for the challenging cases with large scale variations and complex context environments.},
	language = {en},
	urldate = {2023-02-03},
	journal = {Medical Image Analysis},
	author = {Wu, Huisi and Wang, Wei and Zhong, Jiafu and Lei, Baiying and Wen, Zhenkun and Qin, Jing},
	month = may,
	year = {2021},
	keywords = {Adaptive feature fusion, Multi-level semantic supervision, Retinal vessel segmentation, Scale-aware feature aggregation},
	pages = {102025},
	file = {ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\AGRB9SDD\\S1361841521000712.html:text/html;Wu et al. - 2021 - SCS-Net A Scale and Context Sensitive Network for.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Z3XZFJVQ\\Wu et al. - 2021 - SCS-Net A Scale and Context Sensitive Network for.pdf:application/pdf},
}

@article{mahmood_dsrd-net_2022,
	title = {{DSRD}-{Net}: {Dual}-stream residual dense network for semantic segmentation of instruments in robot-assisted surgery},
	volume = {202},
	issn = {0957-4174},
	shorttitle = {{DSRD}-{Net}},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422007606},
	doi = {10.1016/j.eswa.2022.117420},
	abstract = {In conventional robot-assisted minimally invasive procedures (RMIS), surgeons have narrow visual and complex working spaces, along with specular reflection, blood, camera-lens fogging, and complex backgrounds, which increase the risk of human error and tissue damage. The use of deep learning-based techniques can decrease these risks by providing segmented instruments, real-time tracking, pose estimation, and surgeons’ skill assessment. Recently, several deep learning-based methods have been proposed for surgical instrument segmentation. These methods have shown significant performance for the RMIS. However, we found that most of these methods still have scope for improvement in terms of accuracy, robustness, and computational cost. In addition, gastrointestinal pathologies have not been explored in previous studies. Therefore, we propose a dual-stream residual dense network (DSRD-Net), an accurate and robust deep learning-based surgical instrument segmentation method that mainly utilizes the strength of residual, dense, and atrous spatial pyramid pooling architectures. Our proposed method was tested on publicly available gastrointestinal endoscopy (the Kvasir-Instrument Dataset) and abdominal porcine procedures datasets (The 2017 Robotic Instrument Segmentation Challenge Dataset). The experimental results show that the proposed method outperforms the state-of-the-art methods.},
	language = {en},
	urldate = {2023-02-03},
	journal = {Expert Systems with Applications},
	author = {Mahmood, Tahir and Cho, Se Woon and Park, Kang Ryoung},
	month = sep,
	year = {2022},
	keywords = {Minimally invasive surgery, DSRD-Net, Gastrointestinal endoscopy and abdominal porcine procedures, Surgical instruments segmentation},
	pages = {117420},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\49NUJCYW\\Mahmood et al. - 2022 - DSRD-Net Dual-stream residual dense network for s.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RN7SP5X7\\S0957417422007606.html:text/html},
}

@article{shorten_survey_2019,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	number = {1},
	urldate = {2023-02-04},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	month = jul,
	year = {2019},
	keywords = {Deep Learning, Big data, Data Augmentation, GANs, Image data},
	pages = {60},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\F2ZWTFTA\\Shorten and Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8C7IT7KG\\s40537-019-0197-0.html:text/html},
}

@misc{xu_comprehensive_2022,
	title = {A {Comprehensive} {Survey} of {Image} {Augmentation} {Techniques} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2205.01491},
	abstract = {Deep learning has been achieving decent performance in computer vision requiring a large volume of images, however, collecting images is expensive and difficult in many scenarios. To alleviate this issue, many image augmentation algorithms have been proposed as effective and efficient strategies. Understanding current algorithms is essential to find suitable methods or develop novel techniques for given tasks. In this paper, we perform a comprehensive survey on image augmentation for deep learning with a novel informative taxonomy. To get the basic idea why we need image augmentation, we introduce the challenges in computer vision tasks and vicinity distribution. Then, the algorithms are split into three categories; model-free, model-based, and optimizing policy-based. The model-free category employs image processing methods while the model-based method leverages trainable image generation models. In contrast, the optimizing policy-based approach aims to find the optimal operations or their combinations. Furthermore, we discuss the current trend of common applications with two more active topics, leveraging different ways to understand image augmentation, such as group and kernel theory, and deploying image augmentation for unsupervised learning. Based on the analysis, we believe that our survey gives a better understanding helpful to choose suitable methods or design novel algorithms for practical applications.},
	language = {en},
	urldate = {2023-02-04},
	publisher = {arXiv},
	author = {Xu, Mingle and Yoon, Sook and Fuentes, Alvaro and Park, Dong Sun},
	month = nov,
	year = {2022},
	note = {arXiv:2205.01491 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Xu et al. - 2022 - A Comprehensive Survey of Image Augmentation Techn.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\APFTVB6F\\Xu et al. - 2022 - A Comprehensive Survey of Image Augmentation Techn.pdf:application/pdf},
}

@misc{yang_image_2022,
	title = {Image {Data} {Augmentation} for {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Image {Data} {Augmentation} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2204.08610},
	abstract = {Deep learning has achieved remarkable results in many computer vision tasks. Deep neural networks typically rely on large amounts of training data to avoid overﬁtting. However, labeled data for realworld applications may be limited. By improving the quantity and diversity of training data, data augmentation has become an inevitable part of deep learning model training with image data.},
	language = {en},
	urldate = {2023-02-04},
	publisher = {arXiv},
	author = {Yang, Suorong and Xiao, Weikang and Zhang, Mengcheng and Guo, Suhan and Zhao, Jian and Shen, Furao},
	month = apr,
	year = {2022},
	note = {arXiv:2204.08610 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Yang et al. - 2022 - Image Data Augmentation for Deep Learning A Surve.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\NRERBK2X\\Yang et al. - 2022 - Image Data Augmentation for Deep Learning A Surve.pdf:application/pdf},
}

@article{khalifa_comprehensive_2022,
	title = {A comprehensive survey of recent trends in deep learning for digital images augmentation},
	volume = {55},
	issn = {0269-2821, 1573-7462},
	url = {https://link.springer.com/10.1007/s10462-021-10066-4},
	doi = {10.1007/s10462-021-10066-4},
	abstract = {Deep learning proved its efficiency in many fields of computer science such as computer vision, image classifications, object detection, image segmentation, and more. Deep learning models primarily depend on the availability of huge datasets. Without the existence of many images in datasets, different deep learning models will not be able to learn and produce accurate models. Unfortunately, several fields don’t have access to large amounts of evidence, such as medical image processing. For example. The world is suffering from the lack of COVID-19 virus datasets, and there is no benchmark dataset from the beginning of 2020. This pandemic was the main motivation of this survey to deliver and discuss the current image data augmentation techniques which can be used to increase the number of images. In this paper, a survey of data augmentation for digital images in deep learning will be presented. The study begins and with the introduction section, which reflects the importance of data augmentation in general. The classical image data augmentation taxonomy and photometric transformation will be presented in the second section. The third section will illustrate the deep learning image data augmentation. Finally, the fourth section will survey the state of the art of using image data augmentation techniques in the different deep learning research and application.},
	language = {en},
	number = {3},
	urldate = {2023-02-04},
	journal = {Artificial Intelligence Review},
	author = {Khalifa, Nour Eldeen and Loey, Mohamed and Mirjalili, Seyedali},
	month = mar,
	year = {2022},
	pages = {2351--2377},
	file = {Khalifa et al. - 2022 - A comprehensive survey of recent trends in deep le.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IEVEEK6B\\Khalifa et al. - 2022 - A comprehensive survey of recent trends in deep le.pdf:application/pdf},
}

@article{chlap_review_2021,
	title = {A review of medical image data augmentation techniques for deep learning applications},
	volume = {65},
	issn = {1754-9477},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1111/1754-9485.13261},
	doi = {10.1111/1754-9485.13261},
	abstract = {Summary Research in artificial intelligence for radiology and radiotherapy has recently become increasingly reliant on the use of deep learning-based algorithms. While the performance of the models which these algorithms produce can significantly outperform more traditional machine learning methods, they do rely on larger datasets being available for training. To address this issue, data augmentation has become a popular method for increasing the size of a training dataset, particularly in fields where large datasets aren?t typically available, which is often the case when working with medical images. Data augmentation aims to generate additional data which is used to train the model and has been shown to improve performance when validated on a separate unseen dataset. This approach has become commonplace so to help understand the types of data augmentation techniques used in state-of-the-art deep learning models, we conducted a systematic review of the literature where data augmentation was utilised on medical images (limited to CT and MRI) to train a deep learning model. Articles were categorised into basic, deformable, deep learning or other data augmentation techniques. As artificial intelligence models trained using augmented data make their way into the clinic, this review aims to give an insight to these techniques and confidence in the validity of the models produced.},
	number = {5},
	urldate = {2023-02-04},
	journal = {Journal of Medical Imaging and Radiation Oncology},
	author = {Chlap, Phillip and Min, Hang and Vandenberg, Nym and Dowling, Jason and Holloway, Lois and Haworth, Annette},
	month = aug,
	year = {2021},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {deep learning, data augmentation, medical imaging, CT, MRI},
	pages = {545--563},
	file = {Chlap et al. - 2021 - A review of medical image data augmentation techni.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8TCFJIIG\\Chlap et al. - 2021 - A review of medical image data augmentation techni.pdf:application/pdf},
}

@article{grammatikopoulou_cadis_2021-2,
	title = {{CaDIS}: {Cataract} dataset for surgical {RGB}-image segmentation},
	volume = {71},
	issn = {13618415},
	shorttitle = {{CaDIS}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841521000992},
	doi = {10.1016/j.media.2021.102053},
	language = {en},
	urldate = {2023-02-04},
	journal = {Medical Image Analysis},
	author = {Grammatikopoulou, Maria and Flouty, Evangello and Kadkhodamohammadi, Abdolrahim and Quellec, Gwenolé and Chow, Andre and Nehme, Jean and Luengo, Imanol and Stoyanov, Danail},
	month = jul,
	year = {2021},
	pages = {102053},
	file = {Grammatikopoulou et al. - 2021 - CaDIS Cataract dataset for surgical RGB-image seg.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\L4CTK7K8\\Grammatikopoulou et al. - 2021 - CaDIS Cataract dataset for surgical RGB-image seg.pdf:application/pdf},
}

@misc{noauthor_review_nodate,
	title = {Review: {Augmentations} in semantic surgical scene segmentation},
	shorttitle = {Review},
	url = {https://docs.google.com/spreadsheets/d/1CPG6-7Zc0_SlgZhOqV-lsI0qo5o1Ljw0JGI1zg7W-NM/edit?usp=drive_web&ouid=105997189715722108733&usp=embed_facebook},
	abstract = {Tabellenblatt1

origin,first\_author,year,doi/link,subject,modality,focus on generalizability,geometrical OOD addressed?,other domain shifts addressed?,horizontal flip,vertical flip,random crop
surgical instrument segmentation" +"deep learning",Kitaguchi,2022,{\textless}a href="https://doi.org/10.1038/s4159...},
	language = {de},
	urldate = {2023-02-06},
	journal = {Google Docs},
	file = {Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YS44Z5WW\\edit.html:text/html},
}

@misc{allan_2018_2020-1,
	title = {2018 {Robotic} {Scene} {Segmentation} {Challenge}},
	url = {http://arxiv.org/abs/2001.11190},
	abstract = {In 2015 we began a sub-challenge at the EndoVis workshop at MICCAI in Munich using endoscope images of exvivo tissue with automatically generated annotations from robot forward kinematics and instrument CAD models. However, the limited background variation and simple motion rendered the dataset uninformative in learning about which techniques would be suitable for segmentation in real surgery. In 2017, at the same workshop in Quebec we introduced the robotic instrument segmentation dataset with 10 teams participating in the challenge to perform binary, articulating parts and type segmentation of da Vinci R instruments. This challenge included realistic instrument motion and more complex porcine tissue as background and was widely addressed with modiﬁcations on U-Nets and other popular CNN architectures [1].},
	language = {en},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Allan, Max and Kondo, Satoshi and Bodenstedt, Sebastian and Leger, Stefan and Kadkhodamohammadi, Rahim and Luengo, Imanol and Fuentes, Felix and Flouty, Evangello and Mohammed, Ahmed and Pedersen, Marius and Kori, Avinash and Alex, Varghese and Krishnamurthi, Ganapathy and Rauber, David and Mendel, Robert and Palm, Christoph and Bano, Sophia and Saibro, Guinther and Shih, Chi-Sheng and Chiang, Hsun-An and Zhuang, Juntang and Yang, Junlin and Iglovikov, Vladimir and Dobrenkii, Anton and Reddiboina, Madhu and Reddy, Anubhav and Liu, Xingtong and Gao, Cong and Unberath, Mathias and Kim, Myeonghyeon and Kim, Chanho and Kim, Chaewon and Kim, Hyejin and Lee, Gyeongmin and Ullah, Ihsan and Luna, Miguel and Park, Sang Hyun and Azizian, Mahdi and Stoyanov, Danail and Maier-Hein, Lena and Speidel, Stefanie},
	month = aug,
	year = {2020},
	note = {arXiv:2001.11190 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {Allan et al. - 2020 - 2018 Robotic Scene Segmentation Challenge.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9PLQQAVI\\Allan et al. - 2020 - 2018 Robotic Scene Segmentation Challenge.pdf:application/pdf},
}

@misc{allan_2017_2019,
	title = {2017 {Robotic} {Instrument} {Segmentation} {Challenge}},
	url = {http://arxiv.org/abs/1902.06426},
	abstract = {In mainstream computer vision and machine learning, public datasets such as ImageNet [1], COCO [2] and KITTI [3] have helped drive enormous improvements by enabling researchers to understand the strengths and limitations of different algorithms via performance comparison. However, this type of approach has had limited translation to problems in robotic assisted surgery as this ﬁeld has never established the same level of common datasets and benchmarking methods.},
	language = {en},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Allan, Max and Shvets, Alex and Kurmann, Thomas and Zhang, Zichen and Duggal, Rahul and Su, Yun-Hsuan and Rieke, Nicola and Laina, Iro and Kalavakonda, Niveditha and Bodenstedt, Sebastian and Herrera, Luis and Li, Wenqi and Iglovikov, Vladimir and Luo, Huoling and Yang, Jian and Stoyanov, Danail and Maier-Hein, Lena and Speidel, Stefanie and Azizian, Mahdi},
	month = feb,
	year = {2019},
	note = {arXiv:1902.06426 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Allan et al. - 2019 - 2017 Robotic Instrument Segmentation Challenge.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\NGN7KJ5W\\Allan et al. - 2019 - 2017 Robotic Instrument Segmentation Challenge.pdf:application/pdf},
}

@techreport{kolbinger_better_2022,
	type = {preprint},
	title = {Better than humans? {Machine} learning-based anatomy recognition in minimally-invasive abdominal surgery},
	shorttitle = {Better than humans?},
	url = {http://medrxiv.org/lookup/doi/10.1101/2022.11.11.22282215},
	abstract = {Abstract
          
            Background
            Lack of anatomy recognition represents a clinically relevant risk factor in abdominal surgery. While machine learning methods have the potential to aid in recognition of visible patterns and structures, limited availability and diversity of (annotated) laparoscopic image data restrict the clinical potential of such applications in practice. This study explores the potential of machine learning algorithms to identify and delineate abdominal organs and anatomical structures using a robust and comprehensive dataset, and compares algorithm performance to that of humans.
          
          
            Methods
            Based on the Dresden Surgical Anatomy Dataset providing 13195 laparoscopic images with pixel-wise segmentations of eleven anatomical structures, two machine learning algorithms were developed: individual segmentation algorithms for each structure, and a combined algorithm with a common encoder and structure-specific decoders. Performance was assessed using F1 score, Intersection-over-Union (IoU), precision, recall, and specificity. Using the example of pancreas segmentation on a sample dataset of 35 images, algorithm performance was compared to that of a cohort of 28 physicians, medical students, and medical laypersons.
          
          
            Results
            Mean IoU for segmentation of intraabdominal structures ranged from 0.28 to 0.83 and from 0.22 to 0.78 for the structure-specific and the combined semantic segmentation model, respectively. Average inference for the structure-specific (one anatomical structure) and the combined model (eleven anatomical structures) took 28 ms and 71 ms, respectively. Both models outperformed 26 out of 28 human participants in pancreas segmentation.
          
          
            Conclusions
            Machine learning methods have the potential to provide relevant assistance in anatomy recognition in minimally-invasive surgery in near-real-time. Future research should investigate the educational value and subsequent clinical impact of respective assistance systems.},
	language = {en},
	urldate = {2023-02-06},
	institution = {Surgery},
	author = {Kolbinger, Fiona R. and Rinner, Franziska M. and Jenke, Alexander C. and Carstens, Matthias and Leger, Stefan and Distler, Marius and Weitz, Jürgen and Speidel, Stefanie and Bodenstedt, Sebastian},
	month = nov,
	year = {2022},
	doi = {10.1101/2022.11.11.22282215},
	file = {Kolbinger et al. - 2022 - Better than humans Machine learning-based anatomy.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\YPALKC8G\\Kolbinger et al. - 2022 - Better than humans Machine learning-based anatomy.pdf:application/pdf},
}

@inproceedings{deng_automated_2021,
	title = {Automated detection of surgical wounds in videos of open neck procedures using a mask {R}-{CNN}},
	volume = {11598},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11598/1159817/Automated-detection-of-surgical-wounds-in-videos-of-open-neck/10.1117/12.2580908.full},
	doi = {10.1117/12.2580908},
	abstract = {Open surgery represents a dominant proportion of procedures performed, but has lagged behind endoscopic surgery in video-based insights due to the difficulty obtaining high-quality open surgical video. Automated detection of the open surgical wound would enhance tracking and stabilization of body-worn cameras to optimize video capture for these procedures. We present results using a mask R-CNN to identify the surgical wound (the “area of interest”, AOI) in image sets derived from 27 open neck procedures (a 2310-image training/validation set and a 1163-image testing set). Bounding box application to the surgical wound was reliable (F-1 \&gt; 0.905) in the testing sets with a \&lt;5\% false positive rate (recognizing non-wound areas as the AOI). Mask application to greater than 50\% of the wound area also had good success (F-1 = 0.831) under parameters set for high specificity. When applied to short video clips as proof-of-principle, the model performed well both with emerging AOI (i.e., identifying the wound as incisions were developed) and with recapture of the AOI following obstruction). Overall, we identified image lighting quality and the presence of distractors (e.g., bloody sponges) as the primary sources of model errors on visual review. These data serve as a first demonstration of open surgical wound detection using first-person video footage, and sets the stage for further work in this area.},
	urldate = {2023-02-07},
	booktitle = {Medical {Imaging} 2021: {Image}-{Guided} {Procedures}, {Robotic} {Interventions}, and {Modeling}},
	publisher = {SPIE},
	author = {Deng, TingYan and Gulati, Shubham and Kumar, Ashwin and Rodriguez, William and Dawant, Benoit M. and Langerman, Alexander},
	month = feb,
	year = {2021},
	pages = {323--328},
	file = {Deng et al. - 2021 - Automated detection of surgical wounds in videos o.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DH5KH7H8\\Deng et al. - 2021 - Automated detection of surgical wounds in videos o.pdf:application/pdf},
}

@article{al-surmi_new_2014-1,
	title = {A new human heart vessel identification, segmentation and {3D} reconstruction mechanism},
	volume = {9},
	issn = {1749-8090},
	url = {https://doi.org/10.1186/s13019-014-0161-1},
	doi = {10.1186/s13019-014-0161-1},
	abstract = {The identification and segmentation of inhomogeneous image regions is one of the most challenging issues nowadays. The surface vessels of the human heart are important for the surgeons to locate the region where to perform the surgery and to avoid surgical injuries. In addition, such identification, segmentation, and visualisation helps novice surgeons in the training phase of cardiac surgery.},
	number = {1},
	urldate = {2023-02-07},
	journal = {Journal of Cardiothoracic Surgery},
	author = {Al-Surmi, Aqeel and Wirza, Rahmita and Mahmod, Ramlan and Khalid, Fatimah and Dimon, Mohd Zamrin},
	month = oct,
	year = {2014},
	keywords = {3D Model, Heart surgery, Image enhancement, Vessel segmentations},
	pages = {161},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\FB4DSBS9\\Al-Surmi et al. - 2014 - A new human heart vessel identification, segmentat.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\2X4KJ29Y\\s13019-014-0161-1.html:text/html},
}

@misc{maqbool_m2caiseg_2020,
	title = {{m2caiSeg}: {Semantic} {Segmentation} of {Laparoscopic} {Images} using {Convolutional} {Neural} {Networks}},
	shorttitle = {{m2caiSeg}},
	url = {http://arxiv.org/abs/2008.10134},
	abstract = {Purpose Autonomous surgical procedures, in particular minimal invasive surgeries, are the next frontier for Artiﬁcial Intelligence research. However, the existing challenges include precise identiﬁcation of the human anatomy and the surgical settings, and modeling the environment for training of an autonomous agent. To address the identiﬁcation of human anatomy and the surgical settings, we propose a deep learning based semantic segmentation algorithm to identify and label the tissues and organs in the endoscopic video feed of the human torso region.
Methods We present an annotated dataset, m2caiSeg, created from endoscopic video feeds of real-world surgical procedures. Overall, the data consists of 307 images, each of which is annotated for the organs and different surgical instruments present in the scene. We propose and train a deep convolutional neural network for the semantic segmentation task. To cater for the low quantity of annotated data, we use unsupervised pre-training and data augmentation.
Results The trained model is evaluated on an independent test set of the proposed dataset. We obtained a F1 score of 0.33 while using all the labeled categories for the semantic segmentation task. Secondly, we labeled all instruments into an ’Instruments’ superclass to evaluate the model’s performance on discerning the various organs and obtained a F1 score of 0.57.
Conclusion We propose a new dataset and a deep learning method for pixel level identiﬁcation of various organs and instruments in a endoscopic surgical scene. Surgical scene understanding is one of the ﬁrst steps towards automating surgical procedures.},
	language = {en},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Maqbool, Salman and Riaz, Aqsa and Sajid, Hasan and Hasan, Osman},
	month = dec,
	year = {2020},
	note = {arXiv:2008.10134 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Maqbool et al. - 2020 - m2caiSeg Semantic Segmentation of Laparoscopic Im.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\9KAAUUQT\\Maqbool et al. - 2020 - m2caiSeg Semantic Segmentation of Laparoscopic Im.pdf:application/pdf},
}

@inproceedings{pakhomov_towards_2020,
	address = {Las Vegas, NV, USA},
	title = {Towards {Unsupervised} {Learning} for {Instrument} {Segmentation} in {Robotic} {Surgery} with {Cycle}-{Consistent} {Adversarial} {Networks}},
	isbn = {978-1-72816-212-6},
	url = {https://ieeexplore.ieee.org/document/9340816/},
	doi = {10.1109/IROS45743.2020.9340816},
	abstract = {Surgical tool segmentation in endoscopic images is an important problem: it is a crucial step towards full instrument pose estimation and it is used for integration of pre- and intra-operative images into the endoscopic view. While many recent approaches based on convolutional neural networks have shown great results, a key barrier to progress lies in the acquisition of a large number of manually-annotated images which is necessary for an algorithm to generalize and work well in diverse surgical scenarios. Unlike the surgical image data itself, annotations are difﬁcult to acquire and may be of variable quality. On the other hand, synthetic annotations can be automatically generated by using forward kinematic model of the robot and CAD models of tools by projecting them onto an image plane. Unfortunately, this model is very inaccurate and cannot be used for supervised learning of image segmentation models. Since generated annotations will not directly correspond to endoscopic images due to errors, we formulate the problem as an unpaired image-to-image translation where the goal is to learn the mapping between an input endoscopic image and a corresponding annotation using an adversarial model. Our approach allows to train image segmentation models without the need to acquire expensive annotations and can potentially exploit large unlabeled endoscopic image collection outside the annotated distributions of image/annotation data. We test our proposed method on Endovis 2017 challenge dataset and show that it is competitive with supervised segmentation methods.},
	language = {en},
	urldate = {2023-02-07},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Pakhomov, Daniil and Shen, Wei and Navab, Nassir},
	month = oct,
	year = {2020},
	pages = {8499--8504},
	file = {Pakhomov et al. - 2020 - Towards Unsupervised Learning for Instrument Segme.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\BC84BFPB\\Pakhomov et al. - 2020 - Towards Unsupervised Learning for Instrument Segme.pdf:application/pdf},
}

@article{madani_artificial_2022,
	title = {Artificial {Intelligence} for {Intraoperative} {Guidance}: {Using} {Semantic} {Segmentation} to {Identify} {Surgical} {Anatomy} {During} {Laparoscopic} {Cholecystectomy}},
	volume = {276},
	issn = {1528-1140},
	shorttitle = {Artificial {Intelligence} for {Intraoperative} {Guidance}},
	doi = {10.1097/SLA.0000000000004594},
	abstract = {OBJECTIVE: The aim of this study was to develop and evaluate the performance of artificial intelligence (AI) models that can identify safe and dangerous zones of dissection, and anatomical landmarks during laparoscopic cholecystectomy (LC).
SUMMARY BACKGROUND DATA: Many adverse events during surgery occur due to errors in visual perception and judgment leading to misinterpretation of anatomy. Deep learning, a subfield of AI, can potentially be used to provide real-time guidance intraoperatively.
METHODS: Deep learning models were developed and trained to identify safe (Go) and dangerous (No-Go) zones of dissection, liver, gallbladder, and hepatocystic triangle during LC. Annotations were performed by 4 high-volume surgeons. AI predictions were evaluated using 10-fold cross-validation against annotations by expert surgeons. Primary outcomes were intersection- over-union (IOU) and F1 score (validated spatial correlation indices), and secondary outcomes were pixel-wise accuracy, sensitivity, specificity, ± standard deviation.
RESULTS: AI models were trained on 2627 random frames from 290 LC videos, procured from 37 countries, 136 institutions, and 153 surgeons. Mean IOU, F1 score, accuracy, sensitivity, and specificity for the AI to identify Go zones were 0.53 (±0.24), 0.70 (±0.28), 0.94 (±0.05), 0.69 (±0.20). and 0.94 (±0.03), respectively. For No-Go zones, these metrics were 0.71 (±0.29), 0.83 (±0.31), 0.95 (±0.06), 0.80 (±0.21), and 0.98 (±0.05), respectively. Mean IOU for identification of the liver, gallbladder, and hepatocystic triangle were: 0.86 (±0.12), 0.72 (±0.19), and 0.65 (±0.22), respectively.
CONCLUSIONS: AI can be used to identify anatomy within the surgical field. This technology may eventually be used to provide real-time guidance and minimize the risk of adverse events.},
	language = {eng},
	number = {2},
	journal = {Annals of Surgery},
	author = {Madani, Amin and Namazi, Babak and Altieri, Maria S. and Hashimoto, Daniel A. and Rivera, Angela Maria and Pucher, Philip H. and Navarrete-Welton, Allison and Sankaranarayanan, Ganesh and Brunt, L. Michael and Okrainec, Allan and Alseidi, Adnan},
	month = aug,
	year = {2022},
	pmid = {33196488},
	pmcid = {PMC8186165},
	keywords = {Humans, Semantics, Artificial Intelligence, Cholecystectomy, Laparoscopic, Gallbladder, Surgeons},
	pages = {363--369},
}

@misc{hong_cholecseg8k_2020,
	title = {{CholecSeg8k}: {A} {Semantic} {Segmentation} {Dataset} for {Laparoscopic} {Cholecystectomy} {Based} on {Cholec80}},
	shorttitle = {{CholecSeg8k}},
	url = {http://arxiv.org/abs/2012.12453},
	doi = {10.48550/arXiv.2012.12453},
	abstract = {Computer-assisted surgery has been developed to enhance surgery correctness and safety. However, researchers and engineers suffer from limited annotated data to develop and train better algorithms. Consequently, the development of fundamental algorithms such as Simultaneous Localization and Mapping (SLAM) is limited. This article elaborates on the efforts of preparing the dataset for semantic segmentation, which is the foundation of many computer-assisted surgery mechanisms. Based on the Cholec80 dataset [3], we extracted 8,080 laparoscopic cholecystectomy image frames from 17 video clips in Cholec80 and annotated the images. The dataset is named CholecSeg8K and its total size is 3GB. Each of these images is annotated at pixel-level for thirteen classes, which are commonly founded in laparoscopic cholecystectomy surgery. CholecSeg8k is released under the license CC BY- NC-SA 4.0.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Hong, W.-Y. and Kao, C.-L. and Kuo, Y.-H. and Wang, J.-R. and Chang, W.-L. and Shih, C.-S.},
	month = dec,
	year = {2020},
	note = {arXiv:2012.12453 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\LN9N5XUE\\Hong et al. - 2020 - CholecSeg8k A Semantic Segmentation Dataset for L.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XNXEPILW\\2012.html:text/html},
}

@inproceedings{fox_impact_2022,
	address = {Newark NJ USA},
	title = {The {Impact} of {Dataset} {Splits} on {Classification} {Performance} in {Medical} {Videos}},
	isbn = {978-1-4503-9238-9},
	url = {https://dl.acm.org/doi/10.1145/3512527.3531424},
	doi = {10.1145/3512527.3531424},
	language = {en},
	urldate = {2023-02-07},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {ACM},
	author = {Fox, Markus and Schoeffmann, Klaus},
	month = jun,
	year = {2022},
	pages = {6--10},
	file = {Fox and Schoeffmann - 2022 - The Impact of Dataset Splits on Classification Per.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\BKHDRTZV\\Fox and Schoeffmann - 2022 - The Impact of Dataset Splits on Classification Per.pdf:application/pdf},
}

@article{martinez-vega_evaluation_2022,
	title = {Evaluation of {Preprocessing} {Methods} on {Independent} {Medical} {Hyperspectral} {Databases} to {Improve} {Analysis}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/22/8917},
	doi = {10.3390/s22228917},
	abstract = {Currently, one of the most common causes of death worldwide is cancer. The development of innovative methods to support the early and accurate detection of cancers is required to increase the recovery rate of patients. Several studies have shown that medical Hyperspectral Imaging (HSI) combined with artificial intelligence algorithms is a powerful tool for cancer detection. Various preprocessing methods are commonly applied to hyperspectral data to improve the performance of the algorithms. However, there is currently no standard for these methods, and no studies have compared them so far in the medical field. In this work, we evaluated different combinations of preprocessing steps, including spatial and spectral smoothing, Min-Max scaling, Standard Normal Variate normalization, and a median spatial smoothing technique, with the goal of improving tumor detection in three different HSI databases concerning colorectal, esophagogastric, and brain cancers. Two machine learning and deep learning models were used to perform the pixel-wise classification. The results showed that the choice of preprocessing method affects the performance of tumor identification. The method that showed slightly better results with respect to identifing colorectal tumors was Median Filter preprocessing (0.94 of area under the curve). On the other hand, esophagogastric and brain tumors were more accurately identified using Min-Max scaling preprocessing (0.93 and 0.92 of area under the curve, respectively). However, it is observed that the Median Filter method smooths sharp spectral features, resulting in high variability in the classification performance. Therefore, based on these results, obtained with different databases acquired by different HSI instrumentation, the most relevant preprocessing technique identified in this work is Min-Max scaling.},
	language = {en},
	number = {22},
	urldate = {2023-02-16},
	journal = {Sensors},
	author = {Martinez-Vega, Beatriz and Tkachenko, Mariia and Matkabi, Marianne and Ortega, Samuel and Fabelo, Himar and Balea-Fernandez, Francisco and La Salvia, Marco and Torti, Emanuele and Leporati, Francesco and Callico, Gustavo M. and Chalopin, Claire},
	month = jan,
	year = {2022},
	note = {Number: 22
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {hyperspectral imaging, machine learning, deep learning, brain cancer, colon cancer, esophagogastric cancer, median filter, min-max scaling, standard normal variate normalization},
	pages = {8917},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XKUTA9K2\\Martinez-Vega et al. - 2022 - Evaluation of Preprocessing Methods on Independent.pdf:application/pdf},
}

@inproceedings{singh_hide-and-seek_2017,
	title = {Hide-and-{Seek}: {Forcing} a {Network} to be {Meticulous} for {Weakly}-{Supervised} {Object} and {Action} {Localization}},
	shorttitle = {Hide-and-{Seek}},
	doi = {10.1109/ICCV.2017.381},
	abstract = {We propose `Hide-and-Seek', a weakly-supervised framework that aims to improve object localization in images and action localization in videos. Most existing weakly-supervised methods localize only the most discriminative parts of an object rather than all relevant parts, which leads to suboptimal performance. Our key idea is to hide patches in a training image randomly, forcing the network to seek other relevant parts when the most discriminative part is hidden. Our approach only needs to modify the input image and can work with any network designed for object localization. During testing, we do not need to hide any patches. Our Hide-and-Seek approach obtains superior performance compared to previous methods for weakly-supervised object localization on the ILSVRC dataset. We also demonstrate that our framework can be easily extended to weakly-supervised action localization.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Singh, Krishna Kumar and Lee, Yong Jae},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {Testing, Training, Videos, Dogs, Face, Visualization},
	pages = {3544--3553},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6KBTFUXM\\8237643.html:text/html;Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4G6DMKPN\\Singh and Lee - 2017 - Hide-and-Seek Forcing a Network to be Meticulous .pdf:application/pdf},
}

@article{zhong_random_2020,
	title = {Random {Erasing} {Data} {Augmentation}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7000},
	doi = {10.1609/aaai.v34i07.7000},
	abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
	language = {en},
	number = {07},
	urldate = {2023-02-18},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {13001--13008},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PIAVCEZG\\Zhong et al. - 2020 - Random Erasing Data Augmentation.pdf:application/pdf},
}

@misc{yang_image_2022-1,
	title = {Image {Data} {Augmentation} for {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Image {Data} {Augmentation} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2204.08610},
	abstract = {Deep learning has achieved remarkable results in many computer vision tasks. Deep neural networks typically rely on large amounts of training data to avoid overﬁtting. However, labeled data for realworld applications may be limited. By improving the quantity and diversity of training data, data augmentation has become an inevitable part of deep learning model training with image data.},
	language = {en},
	urldate = {2023-02-20},
	publisher = {arXiv},
	author = {Yang, Suorong and Xiao, Weikang and Zhang, Mengcheng and Guo, Suhan and Zhao, Jian and Shen, Furao},
	month = apr,
	year = {2022},
	note = {arXiv:2204.08610 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Yang et al. - 2022 - Image Data Augmentation for Deep Learning A Surve.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\L37V7DU9\\Yang et al. - 2022 - Image Data Augmentation for Deep Learning A Surve.pdf:application/pdf},
}

@article{kar_review_2021,
	title = {A {Review} on {Progress} in {Semantic} {Image} {Segmentation} and {Its} {Application} to {Medical} {Images}},
	volume = {2},
	issn = {2661-8907},
	url = {https://doi.org/10.1007/s42979-021-00784-5},
	doi = {10.1007/s42979-021-00784-5},
	abstract = {Semantic image segmentation is a popular image segmentation technique where each pixel in an image is labeled with an object class. This technique has become a vital part of image analysis nowadays as it facilitates the description, categorization, and visualization of the regions of interest in an image. The recent developments in computer vision algorithms and the increasing availability of large datasets have made semantic image segmentation very popular in the field of computer vision. Motivated by the human visual system which can identify objects in a complex scene very efficiently, researchers are interested in building a model that can semantically segment an image into meaningful object classes. This paper reviews deep learning-based semantic segmentation techniques that use deep neural network architectures for image segmentation of biomedical images. We have provided a discussion on the fundamental concepts related to deep learning methods used in semantic segmentation for the benefit of readers. The standard datasets and existing deep network architectures used in both medical and non-medical fields are discussed with their significance. Finally, this paper concludes by discussing the challenges and future research directions in the field of deep learning-based semantic segmentation for applications in the medical field.},
	language = {en},
	number = {5},
	urldate = {2023-02-20},
	journal = {SN Computer Science},
	author = {Kar, Mithun Kumar and Nath, Malaya Kumar and Neog, Debanga Raj},
	month = jul,
	year = {2021},
	keywords = {Deep learning, Semantic segmentation, Deep neural network, Convolution neural network, Automated medical image analysis, Recurrent neural network},
	pages = {397},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JDGK8A7B\\Kar et al. - 2021 - A Review on Progress in Semantic Image Segmentatio.pdf:application/pdf},
}

@article{alomar_data_2023,
	title = {Data {Augmentation} in {Classification} and {Segmentation}: {A} {Survey} and {New} {Strategies}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-433X},
	shorttitle = {Data {Augmentation} in {Classification} and {Segmentation}},
	url = {https://www.mdpi.com/2313-433X/9/2/46},
	doi = {10.3390/jimaging9020046},
	abstract = {In the past decade, deep neural networks, particularly convolutional neural networks, have revolutionised computer vision. However, all deep learning models may require a large amount of data so as to achieve satisfying results. Unfortunately, the availability of sufficient amounts of data for real-world problems is not always possible, and it is well recognised that a paucity of data easily results in overfitting. This issue may be addressed through several approaches, one of which is data augmentation. In this paper, we survey the existing data augmentation techniques in computer vision tasks, including segmentation and classification, and suggest new strategies. In particular, we introduce a way of implementing data augmentation by using local information in images. We propose a parameter-free and easy to implement strategy, the random local rotation strategy, which involves randomly selecting the location and size of circular regions in the image and rotating them with random angles. It can be used as an alternative to the traditional rotation strategy, which generally suffers from irregular image boundaries. It can also complement other techniques in data augmentation. Extensive experimental results and comparisons demonstrated that the new strategy consistently outperformed its traditional counterparts in, for example, image classification.},
	language = {en},
	number = {2},
	urldate = {2023-02-20},
	journal = {Journal of Imaging},
	author = {Alomar, Khaled and Aysel, Halil Ibrahim and Cai, Xiaohao},
	month = feb,
	year = {2023},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, segmentation, convolutional neural networks, data augmentation, classification, image processing},
	pages = {46},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\BLGWUL7P\\Alomar et al. - 2023 - Data Augmentation in Classification and Segmentati.pdf:application/pdf},
}

@inproceedings{yun_cutmix_2019,
	address = {Seoul, Korea (South)},
	title = {{CutMix}: {Regularization} {Strategy} to {Train} {Strong} {Classifiers} {With} {Localizable} {Features}},
	isbn = {978-1-72814-803-8},
	shorttitle = {{CutMix}},
	url = {https://ieeexplore.ieee.org/document/9008296/},
	doi = {10.1109/ICCV.2019.00612},
	abstract = {Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classiﬁers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefﬁciency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efﬁcient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classiﬁcation tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classiﬁer, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch.},
	language = {en},
	urldate = {2023-02-20},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Yun, Sangdoo and Han, Dongyoon and Chun, Sanghyuk and Oh, Seong Joon and Yoo, Youngjoon and Choe, Junsuk},
	month = oct,
	year = {2019},
	pages = {6022--6031},
	file = {Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RKSQMQ6F\\Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf:application/pdf},
}

@inproceedings{dwibedi_cut_2017,
	title = {Cut, {Paste} and {Learn}: {Surprisingly} {Easy} {Synthesis} for {Instance} {Detection}},
	shorttitle = {Cut, {Paste} and {Learn}},
	doi = {10.1109/ICCV.2017.146},
	abstract = {A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets. For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires expensive data collection and annotation. In this paper, we propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that ensuring only patch-level realism provides enough training signal for current object detector models. We automatically `cut' object instances and `paste' them on random backgrounds. A naive way to do this results in pixel artifacts which result in poor performance for trained models. We show how to make detectors ignore these artifacts during training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21\% on benchmark datasets. In a cross-domain setting, our synthetic data combined with just 10\% real data outperforms models trained on all real data.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Dwibedi, Debidatta and Misra, Ishan and Hebert, Martial},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {Feature extraction, Training, Visualization, Bars, Detectors, Object detection},
	pages = {1310--1319},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\5UJ2I5GJ\\8237408.html:text/html;Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\M2FKZWW8\\Dwibedi et al. - 2017 - Cut, Paste and Learn Surprisingly Easy Synthesis .pdf:application/pdf},
}

@article{chen_image_2019,
	title = {Image {Block} {Augmentation} for {One}-{Shot} {Learning}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4212},
	doi = {10.1609/aaai.v33i01.33013379},
	abstract = {Given one or a few training instances of novel classes, oneshot learning task requires that the classiﬁer generalizes to these novel classes. Directly training one-shot classiﬁer may suffer from insufﬁcient training instances in one-shot learning. Previous one-shot learning works investigate the metalearning or metric-based algorithms; in contrast, this paper proposes a Self-Training Jigsaw Augmentation (Self-Jig) method for one-shot learning. Particularly, we solve one-shot learning by directly augmenting the training images through leveraging the vast unlabeled instances. Precisely our proposed Self-Jig algorithm can synthesize new images from the labeled probe and unlabeled gallery images. The labels of gallery images are predicted to help the augmentation process, which can be taken as a self-training scheme. Intrinsically, we argue that we provide a very useful way of directly generating massive amounts of training images for novel classes. Extensive experiments and ablation study not only evaluate the efﬁcacy but also reveal the insights, of the proposed Self-Jig method.},
	language = {en},
	number = {01},
	urldate = {2023-02-20},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Zitian and Fu, Yanwei and Chen, Kaiyu and Jiang, Yu-Gang},
	month = jul,
	year = {2019},
	pages = {3379--3386},
	file = {Chen et al. - 2019 - Image Block Augmentation for One-Shot Learning.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TAJ7KP2W\\Chen et al. - 2019 - Image Block Augmentation for One-Shot Learning.pdf:application/pdf},
}

@misc{dwibedi_cut_2017-1,
	title = {Cut, {Paste} and {Learn}: {Surprisingly} {Easy} {Synthesis} for {Instance} {Detection}},
	shorttitle = {Cut, {Paste} and {Learn}},
	url = {http://arxiv.org/abs/1708.01642},
	abstract = {A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets. For example, ﬁnding a large labeled dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires expensive data collection and annotation. In this paper, we propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that ensuring only patch-level realism provides enough training signal for current object detector models. We automatically ‘cut’ object instances and ‘paste’ them on random backgrounds. A naive way to do this results in pixel artifacts which result in poor performance for trained models. We show how to make detectors ignore these artifacts during training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21\% on benchmark datasets. In a cross-domain setting, our synthetic data combined with just 10\% real data outperforms models trained on all real data.},
	language = {en},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Dwibedi, Debidatta and Misra, Ishan and Hebert, Martial},
	month = aug,
	year = {2017},
	note = {arXiv:1708.01642 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Dwibedi et al. - 2017 - Cut, Paste and Learn Surprisingly Easy Synthesis .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MTSTR7I6\\Dwibedi et al. - 2017 - Cut, Paste and Learn Surprisingly Easy Synthesis .pdf:application/pdf},
}

@inproceedings{ghiasi_simple_2021,
	address = {Nashville, TN, USA},
	title = {Simple {Copy}-{Paste} is a {Strong} {Data} {Augmentation} {Method} for {Instance} {Segmentation}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578639/},
	doi = {10.1109/CVPR46437.2021.00294},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D. and Le, Quoc V. and Zoph, Barret},
	month = jun,
	year = {2021},
	pages = {2917--2927},
	file = {Ghiasi et al. - 2021 - Simple Copy-Paste is a Strong Data Augmentation Me.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Q7D3YSS2\\Ghiasi et al. - 2021 - Simple Copy-Paste is a Strong Data Augmentation Me.pdf:application/pdf},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	shorttitle = {U-{Net}},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Data Augmentation, Convolutional Layer, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IUAJK3V3\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf},
}

@misc{maier-hein_metrics_2023,
	title = {Metrics reloaded: {Pitfalls} and recommendations for image analysis validation},
	shorttitle = {Metrics reloaded},
	url = {http://arxiv.org/abs/2206.01653},
	doi = {10.48550/arXiv.2206.01653},
	abstract = {Increasing evidence shows that flaws in machine learning (ML) algorithm validation are an underestimated global problem. Particularly in automatic biomedical image analysis, chosen performance metrics often do not reflect the domain interest, thus failing to adequately measure scientific progress and hindering translation of ML techniques into practice. To overcome this, our large international expert consortium created Metrics Reloaded, a comprehensive framework guiding researchers in the problem-aware selection of metrics. Following the convergence of ML methodology across application domains, Metrics Reloaded fosters the convergence of validation methodology. The framework was developed in a multi-stage Delphi process and is based on the novel concept of a problem fingerprint - a structured representation of the given problem that captures all aspects that are relevant for metric selection, from the domain interest to the properties of the target structure(s), data set and algorithm output. Based on the problem fingerprint, users are guided through the process of choosing and applying appropriate validation metrics while being made aware of potential pitfalls. Metrics Reloaded targets image analysis problems that can be interpreted as a classification task at image, object or pixel level, namely image-level classification, object detection, semantic segmentation, and instance segmentation tasks. To improve the user experience, we implemented the framework in the Metrics Reloaded online tool, which also provides a point of access to explore weaknesses, strengths and specific recommendations for the most common validation metrics. The broad applicability of our framework across domains is demonstrated by an instantiation for various biological and medical image analysis use cases.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Maier-Hein, Lena and Reinke, Annika and Godau, Patrick and Tizabi, Minu D. and Büttner, Florian and Christodoulou, Evangelia and Glocker, Ben and Isensee, Fabian and Kleesiek, Jens and Kozubek, Michal and Reyes, Mauricio and Riegler, Michael A. and Wiesenfarth, Manuel and Kavur, Emre and Sudre, Carole H. and Baumgartner, Michael and Eisenmann, Matthias and Heckmann-Nötzel, Doreen and Rädsch, A. Tim and Acion, Laura and Antonelli, Michela and Arbel, Tal and Bakas, Spyridon and Benis, Arriel and Blaschko, Matthew and Cardoso, M. Jorge and Cheplygina, Veronika and Cimini, Beth A. and Collins, Gary S. and Farahani, Keyvan and Ferrer, Luciana and Galdran, Adrian and van Ginneken, Bram and Haase, Robert and Hashimoto, Daniel A. and Hoffman, Michael M. and Huisman, Merel and Jannin, Pierre and Kahn, Charles E. and Kainmueller, Dagmar and Kainz, Bernhard and Karargyris, Alexandros and Karthikesalingam, Alan and Kenngott, Hannes and Kofler, Florian and Kopp-Schneider, Annette and Kreshuk, Anna and Kurc, Tahsin and Landman, Bennett A. and Litjens, Geert and Madani, Amin and Maier-Hein, Klaus and Martel, Anne L. and Mattson, Peter and Meijering, Erik and Menze, Bjoern and Moons, Karel G. M. and Müller, Henning and Nichyporuk, Brennan and Nickel, Felix and Petersen, Jens and Rajpoot, Nasir and Rieke, Nicola and Saez-Rodriguez, Julio and Sánchez, Clara I. and Shetty, Shravya and van Smeden, Maarten and Summers, Ronald M. and Taha, Abdel A. and Tiulpin, Aleksei and Tsaftaris, Sotirios A. and Van Calster, Ben and Varoquaux, Gaël and Jäger, Paul F.},
	month = feb,
	year = {2023},
	note = {arXiv:2206.01653 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
