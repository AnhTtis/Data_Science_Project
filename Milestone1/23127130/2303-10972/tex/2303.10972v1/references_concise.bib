
@article{alomar_data_2023,
	title = {Data {Augmentation} in {Classification} and {Segmentation}: {A} {Survey} and {New} {Strategies}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-433X},
	shorttitle = {Data {Augmentation} in {Classification} and {Segmentation}},
	abstract = {In the past decade, deep neural networks, particularly convolutional neural networks, have revolutionised computer vision. However, all deep learning models may require a large amount of data so as to achieve satisfying results. Unfortunately, the availability of sufficient amounts of data for real-world problems is not always possible, and it is well recognised that a paucity of data easily results in overfitting. This issue may be addressed through several approaches, one of which is data augmentation. In this paper, we survey the existing data augmentation techniques in computer vision tasks, including segmentation and classification, and suggest new strategies. In particular, we introduce a way of implementing data augmentation by using local information in images. We propose a parameter-free and easy to implement strategy, the random local rotation strategy, which involves randomly selecting the location and size of circular regions in the image and rotating them with random angles. It can be used as an alternative to the traditional rotation strategy, which generally suffers from irregular image boundaries. It can also complement other techniques in data augmentation. Extensive experimental results and comparisons demonstrated that the new strategy consistently outperformed its traditional counterparts in, for example, image classification.},
	language = {en},
	number = {2},
	urldate = {2023-02-20},
	journal = {Journal of Imaging},
	author = {Alomar, Khaled and Aysel, Halil Ibrahim and Cai, Xiaohao},
	month = feb,
	year = {2023},
	keywords = {deep learning, segmentation, convolutional neural networks, data augmentation, classification, image processing},
	pages = {46},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\BLGWUL7P\\Alomar et al. - 2023 - Data Augmentation in Classification and Segmentati.pdf:application/pdf},
}

@article{chen_image_2019,
	title = {Image {Block} {Augmentation} for {One}-{Shot} {Learning}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	abstract = {Given one or a few training instances of novel classes, oneshot learning task requires that the classiﬁer generalizes to these novel classes. Directly training one-shot classiﬁer may suffer from insufﬁcient training instances in one-shot learning. Previous one-shot learning works investigate the metalearning or metric-based algorithms; in contrast, this paper proposes a Self-Training Jigsaw Augmentation (Self-Jig) method for one-shot learning. Particularly, we solve one-shot learning by directly augmenting the training images through leveraging the vast unlabeled instances. Precisely our proposed Self-Jig algorithm can synthesize new images from the labeled probe and unlabeled gallery images. The labels of gallery images are predicted to help the augmentation process, which can be taken as a self-training scheme. Intrinsically, we argue that we provide a very useful way of directly generating massive amounts of training images for novel classes. Extensive experiments and ablation study not only evaluate the efﬁcacy but also reveal the insights, of the proposed Self-Jig method.},
	language = {en},
	number = {01},
	urldate = {2023-02-20},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Zitian and Fu, Yanwei and Chen, Kaiyu and Jiang, Yu-Gang},
	month = jul,
	year = {2019},
	pages = {3379--3386},
	file = {Chen et al. - 2019 - Image Block Augmentation for One-Shot Learning.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\TAJ7KP2W\\Chen et al. - 2019 - Image Block Augmentation for One-Shot Learning.pdf:application/pdf},
}

@article{dice_measures_1945,
	title = {Measures of the {Amount} of {Ecologic} {Association} {Between} {Species}},
	volume = {26},
	issn = {0012-9658},
	number = {3},
	journal = {Ecology},
	author = {Dice, Lee R.},
	year = {1945},
	pages = {297--302},
}

@misc{dwibedi_cut_2017,
	title = {Cut, {Paste} and {Learn}: {Surprisingly} {Easy} {Synthesis} for {Instance} {Detection}},
	shorttitle = {Cut, {Paste} and {Learn}},
	abstract = {A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets. For example, ﬁnding a large labeled dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires expensive data collection and annotation. In this paper, we propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that ensuring only patch-level realism provides enough training signal for current object detector models. We automatically ‘cut’ object instances and ‘paste’ them on random backgrounds. A naive way to do this results in pixel artifacts which result in poor performance for trained models. We show how to make detectors ignore these artifacts during training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21\% on benchmark datasets. In a cross-domain setting, our synthetic data combined with just 10\% real data outperforms models trained on all real data.},
	language = {en},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Dwibedi, Debidatta and Misra, Ishan and Hebert, Martial},
	month = aug,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Dwibedi et al. - 2017 - Cut, Paste and Learn Surprisingly Easy Synthesis .pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\MTSTR7I6\\Dwibedi et al. - 2017 - Cut, Paste and Learn Surprisingly Easy Synthesis .pdf:application/pdf},
}

@inproceedings{ghiasi_simple_2021,
	address = {Nashville, TN, USA},
	title = {Simple {Copy}-{Paste} is a {Strong} {Data} {Augmentation} {Method} for {Instance} {Segmentation}},
	isbn = {978-1-66544-509-2},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D. and Le, Quoc V. and Zoph, Barret},
	month = jun,
	year = {2021},
	pages = {2917--2927},
	file = {Ghiasi et al. - 2021 - Simple Copy-Paste is a Strong Data Augmentation Me.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\Q7D3YSS2\\Ghiasi et al. - 2021 - Simple Copy-Paste is a Strong Data Augmentation Me.pdf:application/pdf},
}

@article{izmailov_averaging_2018,
	title = {Averaging {Weights} {Leads} to {Wider} {Optima} and {Better} {Generalization}},
	journal = {Proceedings of the International Conference on Uncertainty in Artificial Intelligence},
	author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\VNSSYAX4\\Izmailov et al. - 2019 - Averaging Weights Leads to Wider Optima and Better.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7W6K35T6\\1803.html:text/html},
}

@inproceedings{kadkhodamohammadi_feature_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Feature {Aggregation} {Decoder} for {Segmenting} {Laparoscopic} {Scenes}},
	isbn = {978-3-030-32695-1},
	abstract = {Laparoscopic scene segmentation is one of the key building blocks required for developing advanced computer assisted interventions and robotic automation. Scene segmentation approaches often rely on encoder-decoder architectures that encode a representation of the input to be decoded to semantic pixel labels. In this paper, we propose to use the deep Xception model for the encoder and a simple yet effective decoder that relies on a feature aggregation module. Our feature aggregation module constructs a mapping function that reuses and transfers encoder features and combines information across all feature scales to build a richer representation that keeps both high-level context and low-level boundary information. We argue that this aggregation module enables us to simplify the decoder and reduce the number of parameters in the decoder. We have evaluated our approach on two datasets and our experimental results show that our model outperforms state-of-the-art models on the same experimental setup and significantly improves the previous results, \$\$98.44{\textbackslash}\%\$\$vs \$\$89.00{\textbackslash}\%\$\$, on the EndoVis’15 dataset.},
	language = {en},
	booktitle = {{OR} 2.0 {Context}-{Aware} {Operating} {Theaters} and {Machine} {Learning} in {Clinical} {Neuroimaging}},
	publisher = {Springer International Publishing},
	author = {Kadkhodamohammadi, Abdolrahim and Luengo, Imanol and Barbarisi, Santiago and Taleb, Hinde and Flouty, Evangello and Stoyanov, Danail},
	editor = {Zhou, Luping and Sarikaya, Duygu and Kia, Seyed Mostafa and Speidel, Stefanie and Malpani, Anand and Hashimoto, Daniel and Habes, Mohamad and Löfstedt, Tommy and Ritter, Kerstin and Wang, Hongzhi},
	year = {2019},
	keywords = {Minimally invasive surgery, Semantic segmentation, Surgical vision},
	pages = {3--11},
	file = {Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QHRXLVUS\\Kadkhodamohammadi et al. - 2019 - Feature Aggregation Decoder for Segmenting Laparos.pdf:application/pdf},
}

@article{kar_review_2021,
	title = {A {Review} on {Progress} in {Semantic} {Image} {Segmentation} and {Its} {Application} to {Medical} {Images}},
	volume = {2},
	issn = {2661-8907},
	abstract = {Semantic image segmentation is a popular image segmentation technique where each pixel in an image is labeled with an object class. This technique has become a vital part of image analysis nowadays as it facilitates the description, categorization, and visualization of the regions of interest in an image. The recent developments in computer vision algorithms and the increasing availability of large datasets have made semantic image segmentation very popular in the field of computer vision. Motivated by the human visual system which can identify objects in a complex scene very efficiently, researchers are interested in building a model that can semantically segment an image into meaningful object classes. This paper reviews deep learning-based semantic segmentation techniques that use deep neural network architectures for image segmentation of biomedical images. We have provided a discussion on the fundamental concepts related to deep learning methods used in semantic segmentation for the benefit of readers. The standard datasets and existing deep network architectures used in both medical and non-medical fields are discussed with their significance. Finally, this paper concludes by discussing the challenges and future research directions in the field of deep learning-based semantic segmentation for applications in the medical field.},
	language = {en},
	number = {5},
	urldate = {2023-02-20},
	journal = {SN Computer Science},
	author = {Kar, Mithun Kumar and Nath, Malaya Kumar and Neog, Debanga Raj},
	month = jul,
	year = {2021},
	keywords = {Deep learning, Semantic segmentation, Deep neural network, Convolution neural network, Automated medical image analysis, Recurrent neural network},
	pages = {397},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JDGK8A7B\\Kar et al. - 2021 - A Review on Progress in Semantic Image Segmentatio.pdf:application/pdf},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2021-08-26},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	keywords = {Computer Science - Machine Learning},
	file = {Kingma und Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\UV8KGTPX\\Kingma und Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf},
}

@article{kitaguchi_limited_2022,
	title = {Limited generalizability of single deep neural network for surgical instrument segmentation in different surgical environments},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	abstract = {Clarifying the generalizability of deep-learning-based surgical-instrument segmentation networks in diverse surgical environments is important in recognizing the challenges of overfitting in surgical-device development. This study comprehensively evaluated deep neural network generalizability for surgical instrument segmentation using 5238 images randomly extracted from 128 intraoperative videos. The video dataset contained 112 laparoscopic colorectal resection, 5 laparoscopic distal gastrectomy, 5 laparoscopic cholecystectomy, and 6 laparoscopic partial hepatectomy cases. Deep-learning-based surgical-instrument segmentation was performed for test sets with (1) the same conditions as the training set; (2) the same recognition target surgical instrument and surgery type but different laparoscopic recording systems; (3) the same laparoscopic recording system and surgery type but slightly different recognition target laparoscopic surgical forceps; (4) the same laparoscopic recording system and recognition target surgical instrument but different surgery types. The mean average precision and mean intersection over union for test sets 1, 2, 3, and 4 were 0.941 and 0.887, 0.866 and 0.671, 0.772 and 0.676, and 0.588 and 0.395, respectively. Therefore, the recognition accuracy decreased even under slightly different conditions. The results of this study reveal the limited generalizability of deep neural networks in the field of surgical artificial intelligence and caution against deep-learning-based biased datasets and models.},
	language = {en},
	number = {1},
	urldate = {2023-02-03},
	journal = {Scientific Reports},
	author = {Kitaguchi, Daichi and Fujino, Toru and Takeshita, Nobuyoshi and Hasegawa, Hiro and Mori, Kensaku and Ito, Masaaki},
	month = jul,
	year = {2022},
	keywords = {Colorectal cancer, Information storage},
	pages = {12575},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\XLX9VVHV\\Kitaguchi et al. - 2022 - Limited generalizability of single deep neural net.pdf:application/pdf},
}

@misc{maier-hein_metrics_2023,
	title = {Metrics reloaded: {Pitfalls} and recommendations for image analysis validation},
	shorttitle = {Metrics reloaded},
	abstract = {Increasing evidence shows that flaws in machine learning (ML) algorithm validation are an underestimated global problem. Particularly in automatic biomedical image analysis, chosen performance metrics often do not reflect the domain interest, thus failing to adequately measure scientific progress and hindering translation of ML techniques into practice. To overcome this, our large international expert consortium created Metrics Reloaded, a comprehensive framework guiding researchers in the problem-aware selection of metrics. Following the convergence of ML methodology across application domains, Metrics Reloaded fosters the convergence of validation methodology. The framework was developed in a multi-stage Delphi process and is based on the novel concept of a problem fingerprint - a structured representation of the given problem that captures all aspects that are relevant for metric selection, from the domain interest to the properties of the target structure(s), data set and algorithm output. Based on the problem fingerprint, users are guided through the process of choosing and applying appropriate validation metrics while being made aware of potential pitfalls. Metrics Reloaded targets image analysis problems that can be interpreted as a classification task at image, object or pixel level, namely image-level classification, object detection, semantic segmentation, and instance segmentation tasks. To improve the user experience, we implemented the framework in the Metrics Reloaded online tool, which also provides a point of access to explore weaknesses, strengths and specific recommendations for the most common validation metrics. The broad applicability of our framework across domains is demonstrated by an instantiation for various biological and medical image analysis use cases.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Maier-Hein, Lena and Reinke, Annika and Godau, Patrick and Tizabi, Minu D. and Büttner, Florian and Christodoulou, Evangelia and Glocker, Ben and Isensee, Fabian and Kleesiek, Jens and Kozubek, Michal and Reyes, Mauricio and Riegler, Michael A. and Wiesenfarth, Manuel and Kavur, Emre and Sudre, Carole H. and Baumgartner, Michael and Eisenmann, Matthias and Heckmann-Nötzel, Doreen and Rädsch, A. Tim and Acion, Laura and Antonelli, Michela and Arbel, Tal and Bakas, Spyridon and Benis, Arriel and Blaschko, Matthew and Cardoso, M. Jorge and Cheplygina, Veronika and Cimini, Beth A. and Collins, Gary S. and Farahani, Keyvan and Ferrer, Luciana and Galdran, Adrian and van Ginneken, Bram and Haase, Robert and Hashimoto, Daniel A. and Hoffman, Michael M. and Huisman, Merel and Jannin, Pierre and Kahn, Charles E. and Kainmueller, Dagmar and Kainz, Bernhard and Karargyris, Alexandros and Karthikesalingam, Alan and Kenngott, Hannes and Kofler, Florian and Kopp-Schneider, Annette and Kreshuk, Anna and Kurc, Tahsin and Landman, Bennett A. and Litjens, Geert and Madani, Amin and Maier-Hein, Klaus and Martel, Anne L. and Mattson, Peter and Meijering, Erik and Menze, Bjoern and Moons, Karel G. M. and Müller, Henning and Nichyporuk, Brennan and Nickel, Felix and Petersen, Jens and Rajpoot, Nasir and Rieke, Nicola and Saez-Rodriguez, Julio and Sánchez, Clara I. and Shetty, Shravya and van Smeden, Maarten and Summers, Ronald M. and Taha, Abdel A. and Tiulpin, Aleksei and Tsaftaris, Sotirios A. and Van Calster, Ben and Varoquaux, Gaël and Jäger, Paul F.},
	month = feb,
	year = {2023},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{nikolov_deep_2021,
	title = {Clinically Applicable Segmentation of Head and Neck Anatomy for Radiotherapy: Deep Learning Algorithm Development and Validation Study},
	volume = {23},
	issn = {1438-8871},
	number = {7},
	journal = {J Med Internet Res},
	author = {Nikolov, Stanislav
	and Blackwell, Sam
	and Zverovitch, Alexei
	and Mendes, Ruheena
	and Livne, Michelle
	and De Fauw, Jeffrey
	and Patel, Yojan
	and Meyer, Clemens
	and Askham, Harry
	and Romera-Paredes, Bernadino
	and Kelly, Christopher
	and Karthikesalingam, Alan
	and Chu, Carlton
	and Carnell, Dawn
	and Boon, Cheng
	and D'Souza, Derek
	and Moinuddin, Syed Ali
	and Garie, Bethany
	and McQuinlan, Yasmin
	and Ireland, Sarah
	and Hampton, Kiarna
	and Fuller, Krystle
	and Montgomery, Hugh
	and Rees, Geraint
	and Suleyman, Mustafa
	and Back, Trevor
	and Hughes, C{\'i}an Owen
	and Ledsam, Joseph R
	and Ronneberger, Olaf},
	month = jul,
	year = {2021},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	shorttitle = {U-{Net}},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Data Augmentation, Convolutional Layer, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\IUAJK3V3\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf},
}

@article{scheikl_deep_2020,
	title = {Deep learning for semantic segmentation of organs and tissues in laparoscopic surgery},
	volume = {6},
	abstract = {Semantic segmentation of organs and tissue types is an important sub-problem in image based scene understanding for laparoscopic surgery and is a prerequisite for context-aware assistance and cognitive robotics. Deep Learning (DL) approaches are prominently applied to segmentation and tracking of laparoscopic instruments. This work compares different combinations of neural networks, loss functions, and training strategies in their application to semantic segmentation of different organs and tissue types in human laparoscopic images in order to investigate their applicability as components in cognitive systems. TernausNet-11 trained on Soft-Jaccard loss with a pretrained, trainable encoder performs best in regard to segmentation quality (78.31\% mean Intersection over Union [IoU]) and inference time (28.07 ms) on a single GTX 1070 GPU.},
	journal = {Current Directions in Biomedical Engineering},
	author = {Scheikl, Paul and Laschewski, Stefan and Kisilenko, Anna and Davitashvili, Tornike and Müller, Benjamin and Capek, Manuela and Müller, Beat and Wagner, Martin and Ullrich, Franziska},
	month = sep,
	year = {2020},
	pages = {20200016},
	file = {Volltext:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\DB7GKNNY\\Scheikl et al. - 2020 - Deep learning for semantic segmentation of organs .pdf:application/pdf},
}

@article{seidlitz_robust_2022,
	title = {Robust deep learning-based semantic organ segmentation in hyperspectral images},
	volume = {80},
	issn = {1361-8415},
	abstract = {Semantic image segmentation is an important prerequisite for context-awareness and autonomous robotics in surgery. The state of the art has focused on conventional RGB video data acquired during minimally invasive surgery, but full-scene semantic segmentation based on spectral imaging data and obtained during open surgery has received almost no attention to date. To address this gap in the literature, we are investigating the following research questions based on hyperspectral imaging (HSI) data of pigs acquired in an open surgery setting: (1) What is an adequate representation of HSI data for neural network-based fully automated organ segmentation, especially with respect to the spatial granularity of the data (pixels vs. superpixels vs. patches vs. full images)? (2) Is there a benefit of using HSI data compared to other modalities, namely RGB data and processed HSI data (e.g. tissue parameters like oxygenation), when performing semantic organ segmentation? According to a comprehensive validation study based on 506 HSI images from 20 pigs, annotated with a total of 19 classes, deep learning-based segmentation performance increases — consistently across modalities — with the spatial context of the input data. Unprocessed HSI data offers an advantage over RGB data or processed data from the camera provider, with the advantage increasing with decreasing size of the input to the neural network. Maximum performance (HSI applied to whole images) yielded a mean DSC of 0.90 ((standard deviation (SD)) 0.04), which is in the range of the inter-rater variability (DSC of 0.89 ((standard deviation (SD)) 0.07)). We conclude that HSI could become a powerful image modality for fully-automatic surgical scene understanding with many advantages over traditional imaging, including the ability to recover additional functional tissue information. Our code and pre-trained models are available at https://github.com/IMSY-DKFZ/htc.},
	language = {en},
	urldate = {2022-09-25},
	journal = {Medical Image Analysis},
	author = {Seidlitz, Silvia and Sellner, Jan and Odenthal, Jan and Özdemir, Berkin and Studier-Fischer, Alexander and Knödler, Samuel and Ayala, Leonardo and Adler, Tim J. and Kenngott, Hannes G. and Tizabi, Minu and Wagner, Martin and Nickel, Felix and Müller-Stich, Beat P. and Maier-Hein, Lena},
	month = aug,
	year = {2022},
	keywords = {Hyperspectral imaging, Surgical data science, Deep learning, Open surgery, Organ segmentation, Semantic scene segmentation},
	pages = {102488},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\SB3FH6NJ\\Seidlitz et al. - 2022 - Robust deep learning-based semantic organ segmenta.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\JY4KM8MN\\S1361841522001359.html:text/html},
}

@article{shorten_survey_2019,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	issn = {2196-1115},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	number = {1},
	urldate = {2023-02-04},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	month = jul,
	year = {2019},
	keywords = {Deep Learning, Big data, Data Augmentation, GANs, Image data},
	pages = {60},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\F2ZWTFTA\\Shorten and Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8C7IT7KG\\s40537-019-0197-0.html:text/html},
}

@inproceedings{singh_hide-and-seek_2017,
	title = {Hide-and-{Seek}: {Forcing} a {Network} to be {Meticulous} for {Weakly}-{Supervised} {Object} and {Action} {Localization}},
	shorttitle = {Hide-and-{Seek}},
	abstract = {We propose `Hide-and-Seek', a weakly-supervised framework that aims to improve object localization in images and action localization in videos. Most existing weakly-supervised methods localize only the most discriminative parts of an object rather than all relevant parts, which leads to suboptimal performance. Our key idea is to hide patches in a training image randomly, forcing the network to seek other relevant parts when the most discriminative part is hidden. Our approach only needs to modify the input image and can work with any network designed for object localization. During testing, we do not need to hide any patches. Our Hide-and-Seek approach obtains superior performance compared to previous methods for weakly-supervised object localization on the ILSVRC dataset. We also demonstrate that our framework can be easily extended to weakly-supervised action localization.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Singh, Krishna Kumar and Lee, Yong Jae},
	month = oct,
	year = {2017},
	keywords = {Testing, Training, Videos, Dogs, Face, Visualization},
	pages = {3544--3553},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\6KBTFUXM\\8237643.html:text/html;Submitted Version:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\4G6DMKPN\\Singh and Lee - 2017 - Hide-and-Seek Forcing a Network to be Meticulous .pdf:application/pdf},
}

@article{tan_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a ﬁxed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefﬁcient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.},
	language = {en},
	journal = {International Conference on Machine Learning},
	author = {Tan, Mingxing and Le, Quoc V},
	year = {2019},
	pages = {6105--6114},
	file = {Tan und Le - EfficientNet Rethinking Model Scaling for Convolu.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\7AMVVN2T\\Tan und Le - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf},
}

@article{wiesenfarth_methods_2021,
	title = {Methods and open-source toolkit for analyzing and visualizing challenge results},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	abstract = {Grand challenges have become the de facto standard for benchmarking image analysis algorithms. While the number of these international competitions is steadily increasing, surprisingly little effort has been invested in ensuring high quality design, execution and reporting for these international competitions. Specifically, results analysis and visualization in the event of uncertainties have been given almost no attention in the literature. Given these shortcomings, the contribution of this paper is two-fold: (1) we present a set of methods to comprehensively analyze and visualize the results of single-task and multi-task challenges and apply them to a number of simulated and real-life challenges to demonstrate their specific strengths and weaknesses; (2) we release the open-source framework challengeR as part of this work to enable fast and wide adoption of the methodology proposed in this paper. Our approach offers an intuitive way to gain important insights into the relative and absolute performance of algorithms, which cannot be revealed by commonly applied visualization techniques. This is demonstrated by the experiments performed in the specific context of biomedical image analysis challenges. Our framework could thus become an important tool for analyzing and visualizing challenge results in the field of biomedical image analysis and beyond.},
	language = {en},
	number = {1},
	urldate = {2021-09-13},
	journal = {Scientific Reports},
	author = {Wiesenfarth, Manuel and Reinke, Annika and Landman, Bennett A. and Eisenmann, Matthias and Saiz, Laura Aguilera and Cardoso, M. Jorge and Maier-Hein, Lena and Kopp-Schneider, Annette},
	month = jan,
	year = {2021},
	pages = {2369},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\8CDXKG5R\\Wiesenfarth et al. - 2021 - Methods and open-source toolkit for analyzing and .pdf:application/pdf;Snapshot:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\QGYWA7P4\\s41598-021-82017-6.html:text/html},
}

@inproceedings{yun_cutmix_2019,
	address = {Seoul, Korea (South)},
	title = {{CutMix}: {Regularization} {Strategy} to {Train} {Strong} {Classifiers} {With} {Localizable} {Features}},
	isbn = {978-1-72814-803-8},
	shorttitle = {{CutMix}},
	abstract = {Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classiﬁers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefﬁciency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efﬁcient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classiﬁcation tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classiﬁer, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch.},
	language = {en},
	urldate = {2023-02-20},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Yun, Sangdoo and Han, Dongyoon and Chun, Sanghyuk and Oh, Seong Joon and Yoo, Youngjoon and Choe, Junsuk},
	month = oct,
	year = {2019},
	pages = {6022--6031},
	file = {Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\RKSQMQ6F\\Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf:application/pdf},
}

@article{zhong_random_2020,
	title = {Random {Erasing} {Data} {Augmentation}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
	language = {en},
	number = {07},
	urldate = {2023-02-18},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
	month = apr,
	year = {2020},
	pages = {13001--13008},
	file = {Full Text PDF:C\:\\Users\\Silvia\\Programme\\PortableApps\\ZoteroPortable\\Data\\profile\\storage\\PIAVCEZG\\Zhong et al. - 2020 - Random Erasing Data Augmentation.pdf:application/pdf},
}