% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
 reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}
\let\vaccent=\v % rename builtin command \v{} to \vaccent{}
\renewcommand{\v}[1]{\ensuremath{\mathbf{#1}}} % for vectors
\newcommand{\gv}[1]{\ensuremath{\mbox{\boldmath$ #1 $}}} 
% for vectors of Greek letters, original had del operator as g-vector
\newcommand{\uv}[1]{\ensuremath{\mathbf{\hat{#1}}}} % for unit vector
\newcommand{\abs}[1]{\left| #1 \right|} % for absolute value
\let\underdot=\d % rename builtin command \d{} to \underdot{}
\renewcommand{\d}[2]{\frac{d #1}{d #2}} % for derivatives
\newcommand{\dd}[2]{\frac{d^2 #1}{d #2^2}} % for double derivatives
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} 
                                            % for partial derivatives
\newcommand{\pdd}[2]{\frac{\partial^2 #1}{\partial #2^2}} 
   				       % for double partial derivatives
\let\oldeqref=\eqref % rename builtin command \eqref to \oldeqref
\renewcommand\eqref[1]{Eq.\;\ref{#1}} % new version of eqref
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %number a single equation in an align* environment

% fix outer quotes
\usepackage [english]{babel}
\usepackage [english = american]{csquotes}
\MakeOuterQuote{"}

% Code for commenting:
\usepackage{color}					% Allows picking of custom RGB colors
\newcommand{\tr}[1]{{\leavevmode\color{red}#1}}
\newcommand{\tb}[1]{{\leavevmode\color{blue}#1}}

\begin{document}


\title{Large statistical learning models effectively forecast diverse chaotic systems}
%\title{Large statistical learning models effectively forecast diverse chaotic systems}
%\title{Large forecasting models effectively forecast diverse chaotic systems}
%\title{Comparing forecasting models across a space of chaotic systems}
% NComms, Science Advances


\author{William Gilpin}
% \altaffiliation[Also at ]{NSF-Simons Quantitative Biology Initiative, Harvard University.}
%Lines break automatically or can be forced with \\
%\author{Second Author}%
 \email{wgilpin@utexas.edu}
\affiliation{%
Department of Physics, The University of Texas at Austin, Austin, Texas 78712, USA
}%
\affiliation{%
Oden Institute for Computational Engineering and Sciences, The University of Texas at Austin, Austin, Texas 78712, USA
}%



\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
Chaos and unpredictability are traditionally synonymous, yet recent advances in statistical forecasting suggest that large machine learning models can derive unexpected insight from extended observation of complex systems. Here, we study the forecasting of chaos at scale, by performing a large-scale comparison of $24$ representative state-of-the-art multivariate forecasting methods on a crowdsourced database of $135$ distinct low-dimensional chaotic systems. We find that large, domain-agnostic time series forecasting methods based on artificial neural networks consistently exhibit strong forecasting performance, in some cases producing accurate predictions lasting for dozens of Lyapunov times. Best-in-class results for forecasting chaos are achieved by recently-introduced hierarchical neural basis function models, though even generic transformers and recurrent neural networks perform strongly. However, physics-inspired hybrid methods like neural ordinary equations and reservoir computers contain inductive biases conferring greater data efficiency and lower training times in data-limited settings. We observe consistent correlation across all methods despite their widely-varying architectures, as well as universal structure in how predictions decay over long time intervals. Our results suggest that a key advantage of modern forecasting methods stems not from their architectural details, but rather from their capacity to learn the large-scale structure of chaotic attractors.
\end{abstract}
%We find that large, domain-agnostic time series forecasting models based on neural networks challenge the performance of physically-motivated models like neural ordinary equations and reservoir computers. 

%\keywords{Suggested keywords}%Use showkeys class option if keyword
%display desired
\maketitle

%%% 3750 words, not including abstract, authors list and affiliations, and references.
%%% Introduction: 1-1.2 columns at most

\section{Introduction}

Chaos traditionally implies the butterfly effect: that a small change to a system's inputs grows exponentially over time, complicating efforts to reliably predict the system's long-term evolution. Yet recent efforts to generate long-term forecasts of real-world complex systems using statistical learning undermine this intuition, providing compelling examples of successful data-driven predictions of diverse systems such as cellular signaling pathways \cite{yazdani2020systems}, hourly precipitation forecasts \cite{espeholt2022deep}, and tokamak plasma disruptions \cite{zheng2018hybrid}. These successes complement recent works showing that large, overparameterized statistical learning methods can learn latent representations of complex systems that implicitly linearize dynamics, raising the possibility of forecasting even strongly-chaotic systems using models with sufficient capacity \cite{brunton2017chaos,wang2023koopman,otto2021koopman}. Such results potentially elucidate the emergence of reservoir computers as best-in-class forecasting methods for dynamical systems \cite{gauthier2021next,bompas2020accuracy,platt2021robust,pathak2018model,jiang2019model,chattopadhyay2020data,vlachas2020backpropagation}; these models use emergent properties of random networks to lift complex time series into a random feature space, substantially reducing training requirements by reducing learning to linear regression \cite{maass2002real,jaeger2004harnessing}. However, other recently-introduced hybrid models successfully forecast chaotic systems by encoding dynamical constraints within model formulation \cite{karniadakis2021physics}; among these are neural ordinary differential equations \cite{chen2018neural}, physics-informed neural networks \cite{raissi2019physics}, and recurrent neural networks with domain-specific architectures \cite{sangiorgio2020robustness,bhat2022recurrent,yu2017learning}. 
% ecosystems XXX \cite{heinmunch}, other good examples from wang2023
%kernel based methods \cite{burov2021kernel,yang2023learning} and operator methods \cite{berry2015nonparametri}
% reservoir computing and Koopman \cite{bollt2021explaining}

However, there is little consensus regarding the best choice of forecasting model for a given dynamical system, and whether dynamical constraints should be encoded, or learned from data via large domain-agnostic models---the former provides an inductive bias that economizes training, while the latter allows greater flexibility and reduces model selection bias. Complicating comparisons among methods are the widely-varying contexts in which prior methods have been evaluated, which vary from domain-specific weather or medical time series to individual well-known chaotic systems like the Lorenz attractor \cite{makridakis2022m5,costa2019adaptive,godahewa2021monash,yu2017learning,bai2018empirical}. A larger set of representative systems, and a controlled comparison among methods, is necessary to disentangle the relationship between chaoticity, predictability, and forecasting model architecture, as well as to understand how the properties of different black-box machine learning methods interact with the systems they predict.
%However, while prior studies have considered individual, well-known chaotic systems like the Lorenz attractor or XXX, and have performed XXX, prior works have not generalized XXXX. A larger scale comparative benchmark across many chaotic systems and diverse forecasting models is needed to disentangle the relationship among experiment design, method architecture, and mathematical properties of the underlying chaotic system being forecast-
%

Here, we seek to systematically quantify the relationship between chaos and empirical predictability in large-scale, controlled experiments. We recently introduced a benchmark dataset containing $135$ low-dimensional differential equations describing known chaotic attractors \cite{gilpin2021chaos}. Initially curated from published works to include well-known systems such as the Lorenz, R\"ossler, and Chua attractors, the dataset has grown through crowdsourcing to include examples spanning diverse domains such as climatology, neuroscience, and astrophysics. Each dynamical system is aligned with respect to its dominant timescale and integration timestep using surrogate significance testing \cite{kantz2004nonlinear}, and is annotated with calculations of its invariant properties such as the Lyapunov exponent spectrum, fractal dimension, and metric entropy.
%and has since been applied to benchmarking symbolic regression methods \cite{alan}, kernel-based data assimilation \cite{hamza}, 

While each system has a different largest Lyapunov exponent ($\lambda_\text{max}$), and thus putative chaoticity, some systems are more closely related than others. For example, the Sprott attractor subfamily ($\lambda_\text{max} \in [0.01, 1.1]$) exhibit similar qualitative structure such as paired lobes, owing to the presence of predominantly quadratic nonlinearities in the governing differential equations \cite{sprott1994some,kaptanoglu2023benchmarking}. To identify such relationships across our dataset, we convert each dynamical system into a high-dimensional vector by first generating a long trajectory, and then computing $747$ characteristic mathematical signal properties such as the metric entropy, power spectral coefficients, Hurst exponents, et al. that are invariant to the initial conditions and sampling rate \cite{christ2018time,makowski2021neurokit2}. We then use uniform manifold approximation and projection (UMAP) to visualize these high-dimensional vectors within a two-dimensional plane (Fig. \ref{orbit}) \cite{mcinnes2018umap,fulcher2013highly}. The resulting space of chaotic systems shows clear structure, with the Sprott and other scroll-like subfamilies clustering together, while other systems separate. These results suggest that our dataset contains high dynamical diversity beyond absolute chaoticity $\lambda_\text{max}$, which correlates only weakly with the embedding ($\rho = 0.15 \pm 0.03$, bootstrapped Spearman rank-order coefficient).


\begin{figure}
{
\centering
\includegraphics[width=\linewidth]{fig_systems.pdf}
\caption{
{\bf A space of chaotic dynamical systems.} (A) A nonlinear embedding of $135$ distinct low-dimensional chaotic systems, colored by largest Lyapunov exponent ($\lambda_\text{max}$). Contours denote $50\%$ confidence intervals in the embedding of each system over $500$ random initial conditions and feature subsets; points denote centroids for each system. (B) Example systems from the dataset, colored by $\lambda_\text{max}$.
}
\label{orbit}
}
\end{figure}

%\section{Methods}
%\subsection{Forecasting benchmark design}

Our dataset allows us systematically compare the forecasting ability of different statistical forecasting methods across diverse dynamical systems. Forecasting dynamical systems from observations is a well-established field \cite{kantz2004nonlinear}, and we structure our experiments as a standard long-term autoregressive forecasting task \cite{vlachas2023learning}. For each dynamical system, we define two time series $Y_\text{train}, Y_\text{test} \in \mathbb{R}^{T \times D}$ corresponding to multivariate trajectories emanating from different initial conditions, and we define a splitting time such that $T = T_\text{past} + T_\text{fut}$, $Y_\text{train} = Y_\text{train}^\text{past} \cup Y_\text{train}^\text{fut}$, etc. The model parameters are first fit using $Y_\text{train}^\text{past}$, and the accuracy of the resulting predictions on $Y_\text{train}^\text{fut}$ are used for model selection and hyperparameter tuning. After completing model selection, the final model from each model class is then fit on $Y_\text{test}^\text{past}$, and the resulting predictions on $Y_\text{test}^\text{fut}$ produce the error score $\epsilon_{ik}(t)$ representing the performance of the $i^{th}$ forecasting model on the $k^{th}$ dynamical system at future time $t \in [0, T_\text{fut}]$ after the end of training data availability. We compute $17$ different accuracy metrics, including root mean-squared error, pointwise correlation, mutual information, and Granger causality. We report our results in the main text in terms of the symmetric mean absolute percent error (sMAPE) $\epsilon(t) \equiv (2/N) \sum_{t'=1}^t (| \v{y}(t') - \hat{\v{y}}(t')|) / (|\v{y}(t')| + |\hat{\v{y}}(t')|)$ due to its common use, conceptual simplicity, and correlation with other metrics \cite{godahewa2021monash,makridakis2022m5,gilpin2023recurrences,gilpin2021chaos,wang2023koopman}.

Each forecasting method represents a class of possible models parametrized by choices made regarding architecture (model size, number of layers, activation function) or training (optimization epochs, batch sizes). Such choices can strongly affect the performance of different methods \cite{platt2021robust,kantz2004nonlinear,godahewa2021monash}, yet different methods do not necessarily have equivalent adjustable hyperparameters. In order to fairly compare different methods, we restrict hyperparameter tuning to the equivalent of the input time window for each model---such as the input size for deep neural networks, the order of autoregressive models, or the number of time lags for state space models. We tune hyperparameters separately on each system and forecasting model pair. Our overall experiment design is characterized by several timescales: the \textit{lookback window} hyperparameter corresponds to the number of past timepoints seen simultaneously by a model at a given time during training or prediction; the \textit{history length} represents the total number of timepoints available to learn the model's parameters before prediction; the \textit{forecast horizon} represents the number of unseen timepoints into the future that are predicted autoregressively; and the \textit{Lyapunov time} $\lambda_\text{max}^{-1}$ is an invariant property of each distinct dynamical system, representing the characteristic timescale over which forecasts are expected to lose accuracy due to the butterfly effect.


We evaluate $24$ statistical forecasting models across all $135$ dynamical systems. We choose forecasting methods representative of the broad diversity of methods available in the recent literature \cite{lim2021time,herzen2022darts}. Traditional methods include standard linear regression, autoregressive moving averages (ARIMA), exponential smoothing, Kalman filtering, Fourier mode extrapolation, boosted random forest models \cite{kantz2004nonlinear}, and newly-introduced linear models that account for trends and distribution shift \cite{zeng2022transformers}. Current state-of-the-art models for general time series forecasting are based on deep neural networks: the transformer model \cite{zhou2021informer}, long-short-term-memory networks (LSTM), block recurrent neural networks (RNN), temporal convolutional neural networks \cite{bai2018empirical}, and neural basis expansion/neural hierarchical interpolation (NBEATS/NHiTS) \cite{oreshkin2020nbeats,challu2023nhits}. The latter methods generates forecasts hierarchically by aggregating separate forecasts at distinct timescales (NBEATS), and can explicitly coarse-grain the time series to further reduce computational costs (NHiTS). We also consider hybrid physics-motivated methods such as neural ordinary differential equations \cite{chen2018neural}, which approximate the continuous-time differential equation underlying time series, as well as echo-state networks and their generalization, nonlinear vector autoregressive models (nVAR), which use a trainable linear recurrent structure with fixed "reservoir" of random nonlinearities \cite{maass2002real,jaeger2004harnessing,gauthier2021next}.
% reservoir computing as nVAR \cite{bollt2021explaining}


%For a given dynamical system $\v{f}(\v{y})$, the continuous-time forecasting problem corresponds to learning a flow map functional $\hat{\v{y}}(t') = \Phi_{\theta,\tau}[\v{y}(t)](t')$ parametrized by $\theta$ that, given values of $\v{y}(t)$ over a finite lookback window into the past $\tau \in (0, \infty)$, computes an estimate of the values of the input function $\hat{\v{y}}(t')$ at forecast horizon times $t' \in (0, \infty)$. This calculation may be simplified by first learning a differential operator $\Phi^{(0)}$ that propagates the flow at $\v{y}(t)$ to $\v{y}(t + dt)$, and then calculating the future forecast times autoregressively, $xx$. The forecasting problem therefore becomes the problem of learning $\Phi_{\theta,\tau}$ given past sufficient observations of $\v{y}(t)$, such that the difference between predicted values $\hat{\v{y}}(t')$ and true future values $\hat{\v{y}}(t')$ remains small for $t'$ as large as possible. 
%Deep learning models with general regression structure
%As well as variants of transformers with purely linear (NLinear)




%\section{Results}
%\subsection{Large, domain-agnostic time series models successfully forecast diverse chaotic systems}. 

Our main forecasting results are summarized in Figure \ref{forecast}. Our long-term forecasting experiments require $\sim\!10^{18}$ floating point operations for training, model selection and hyperparameter tuning, a figure comparable to the scale of other recent large-scale machine learning benchmarks \cite{canziani2016analysis}. For brevity, we report here only results for the top $14$ forecasting models in terms of the sMAPE error, and defer the full tabular results and accuracy metrics to the supplementary material and our open-source repository. 

We observe that large, domain-agnostic forecasting methods successfully forecast diverse chaotic systems, with the strongest methods succeeding consistently across diverse systems and forecasting horizons. The strong relative performance of machine learning models is highlighted in Fig \ref{forecast}C, where NBEATS successfully forecasts the Mackey-Glass equation for $\sim\!\! 22$ Lyapunov times without losing track of the global phase. This and other strongly-performing methods, such as the transformer and LSTM, comprise large models originally designed for generic sequential datasets, and which do not make any assumptions regarding whether observed time series data arises from a dynamical system. This observation suggests that more flexible, generic architectures may prove preferable for problems where some physical structure is present (e.g. analytic generating functions in the form of ordinary differential equations) but stronger domain knowledge (e.g. symmetries or symplecity) is unavailable to further constrain learning.
%, while the rule-based random forest does not assume any underlying metric relates neighboring timepoints, which previous studies argue confers flexibility for unstructured time series \cite{xxx}

The strong performance of NBEATS and NHiTS suggest that these models may have structural features favoring the chaotic systems dataset. Because these methods compute forecasts hierarchically, they can flexibly integrate information across multiple timescales in a manner inaccessible to classical statistical models \cite{challu2023nhits}. While chaotic systems exhibit continuous spectra and thus contain information relevant to forecasting at a variety of timescales, many systems exhibit topologically-preferred timescales such as unstable periodic orbits---like the "loops" on either side of the Lorenz attractor---that dominate the system's underlying measure \cite{cvitanovic1988invariant}, and which therefore may represent higher priority motifs for learning. We thus argue that performant model architectures contain implicit inductive biases that advantage them on chaotic systems relative to other time series. Consistent with this finding, we note that the next strongest-performing model, nVAR, comprises a reservoir computing architecture \cite{maass2002real,jaeger2004harnessing}, which prior works have shown to perform particularly well on time series from dynamical systems \cite{gauthier2021next,bompas2020accuracy,platt2021robust,pathak2018model,jiang2019model,chattopadhyay2020data,vlachas2020backpropagation}. Interestingly, the neural ordinary differential equation model performs poorly, despite putatively learning the underlying generator of the time series. We find that over long forecast horizons, neural ODE tend to settle into prolonged oscillations, favoring complex non-chaotic limit cycles over chaotic attractors given finite training data.

While the utility of deep learning methods for forecasting general time series has been questioned \cite{makridakis2022m5,godahewa2021monash}, our results agree with recent benchmarks suggesting that large models strongly outperform classical forecasting methods on long-horizon forecasting tasks \cite{zhou2021informer}. We find that classical methods like exponential smoothing or ARIMA do not appear among the top 14 models, implying that the size and diversity of our chaotic systems dataset, as well as the long duration of the forecasting task, require larger models with greater intrinsic capacity to represent complex nonlinear systems. Relative performance among models remains stable across two orders of magnitude in Lyapunov time, indicating that strong models better approximate the underlying propagator for the flow even at small forecasting horizons. Given the autoregressive nature of forecasting, an initial accuracy advantage compounds over time due to the exponential sensitivity of chaotic systems to early errors. 



\begin{figure}
{
\centering
\includegraphics[width=\linewidth]{fig_results.pdf}
\caption{
{\bf Comparison of statistical forecasting methods across all chaotic systems.}  (A) Distributions of the errors of the $14$ best-performing method of the $24$ evaluated, measured at $t = 1 / \lambda_\text{max}$. (B) The average error of each model $\langle \epsilon_{ik}(t) \rangle_k$ as a function of Lyapunov time. (C) The predictions of best-performing forecast model (red), relative to a held-out true trajectory from the Mackey-Glass model (gray) at short and long forecasting horizons.
}
\label{forecast}
}
\end{figure}


%\subsection{Domain-specific models successfully balance accuracy with computational expense}. 

While the best forecasts are generated by domain-agnostic time series methods, we note that the different forecasting methods have different intrinsic model complexities and thus capacities. Fig. \ref{relation}A shows the forecasting error at $\lambda_\text{max}^{-1}$ versus the computational walltime required to train each model on one central processing unit. Training walltime measures model efficiency, and we interpret it as a loose proxy for model complexity because model size and parameter count are not directly quantifiable across highly-distinct and regularized architectures \cite{canziani2016analysis}. We find that error and training time exhibit negative correlation ($\rho = -0.21 \pm 0.04$, bootstrapped Spearman coefficient), which persists within most method groups.  The best-performing machine learning models require considerable training times; in contrast, nVAR exhibits competitive performance with two orders of magnitude less training time due to its linear structure. The strong performance of reservoir computers on our chaotic systems task may indicate an inductive bias for learning complex dynamical systems, due to their disordered reservoir allowing them to more readily represent systems with continuous spectra \cite{jaeger2004harnessing,kim2021teaching,smith2022learning,sussillo2009generating}. 


%\subsection{Correlation between embedding and forecast}. 
\begin{figure*}
{
\centering
\includegraphics[width=0.85\linewidth]{fig_resources.pdf}
\caption{
{\bf Relationships among forecasting methods.} (B) Error versus training time at fixed horizon $t = 1 / \lambda_\text{max}$ for all models. Bar lengths denote standard deviations along principle axes, with angle indicating Spearman correlation within each model group in order to detect cases of Simpson's paradox. A linear fit is underlaid. (B) Median relative correlation of each forecasting model with the average prediction, across different forecast horizons. (C) Median model errors at $t = 1 / \lambda_\text{max}$ as the amount of history data increases. All error bars correspond to 95\% confidence intervals, and colors match methods from previous figures.
}
\label{relation}
}
\end{figure*}



This general tradeoff between performance and training time motivates us to search for universal similarities across different forecasting methods. In Fig. \ref{relation}B we compute the time-resolved Spearman correlation between each method's forecast error and the average error of that method, $\tilde\rho_i(t) = \rho(\epsilon_{ik}(t), \langle \epsilon_{ik} \rangle_t))_k$. We find universal non-monotonic behavior, in which nearly all methods exhibit peak correlation at one Lyapunov time $\lambda_\text{max}^{-1}$ (Fig \ref{relation}B). This observation underscores that the largest Lyapunov exponent represents an appropriate timescale for comparing different dynamical systems, and that diverse forecasting models interact with this property in a shared manner. Like other disordered complex systems, the trained models exhibit peak correlation at peak variance \cite{sethna2021statistical}, coinciding with when average forecast errors rapidly increase in Fig \ref{forecast}B. The critical forecast horizon $\lambda_\text{max}^{-1}$ is sufficiently long to distinguish different dynamical systems based on their invariant properties, but short enough that forecast methods do not accrue instabilities, large phase offsets, and other artifacts that saturate forecast error and mask intrinsic differences among systems.

To further investigate model complexity and performance, we next perform a series of experiments in which we titrate history length, which determines the the total amount of training data available to each method before generating a forecast (Fig. \ref{relation}C). Unlike measuring training time or parameter count, these experiments seek to determine how effectively different models utilize additional observations. As expected, all models asymptotically improve given more training data. However, the three best-performing models improve more quickly and reach lower asymptotes as the available history increases, suggesting that they better leverage additional data than less performant models. These results match the intuition that these models combine high intrinsic capacity with inductive biases for chaotic dynamical systems in order to outperform other methods on our dataset.


%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section{Discussion}
%
%%%%%%%%%%%%%%%%%%%%%%%%

Our results show that recently-developed large, overparameterized statistical forecasting models efficiently leverage long-term observations of chaotic attractors, producing best-in-class forecasts that can remain accurate for dozens of Lyapunov times. Commonalities in predictions across highly distinct model classes suggest that performance arises primarily from model capacity and generalization ability, rather than specific architectural choices, and that performance at long prediction times is ultimately limited by a model's ability to learn long-term properties of a dynamical system's underlying attractor.  Our observations echo recent findings from other domains and represent an intuitive consequence of the "no free lunch" theorem for model selection \cite{brown2020language,wolpert1997no}. Nonetheless, our results are practically informative for forecasting real-world time series driven by underlying dynamical systems. In the absence of restrictions on data availability or training resources, large domain-agnostic models are likely to produce high-quality forecasts without the need for system-specific knowledge. However, in restricted settings, models encoding strong domain knowledge, particularly recent reservoir computing architectures \cite{gauthier2021next}, exhibit the strongest performance relative to their computational requirements.
%cite sutton
%We have performed a large-scale empirical test of ability of XX to forecast XXX distinct low-dimensional chaotic systems. Our results suggest that statistical forecasting models effectively model long-term properties of diverse chaotic systems, allowing forecasts lasting dozens of Lyapunov times in some cases. Moreover, model capacity plays a greater role than model architecture in determining average performance

While NBEATS, NHiTS, and nVAR perform particularly well in our experiments, we refrain from endorsing these specific models to the detriment of other methods. Our results may be specific to our chaotic systems dataset and, more importantly, the recent literature contains a broad variety of new forecasting models, as well as infinite variations of each method due to hyperparameter and architectural choices, which could potentially exhibit strong performance. Rather, we have chosen representative set of forecasting models bridging different foci of the literature \cite{makridakis2022m5,lim2021time}, and aim to highlight general trends and the emerging strength of new models on the classical problem of forecasting chaos.

In future work, we plan to relate our empirical forecasting results to invariant properties of the different dynamical systems in our dataset, such as various measures of fractality and entropy \cite{tang2020introduction}. Such characterization could improve the interpretability of machine learning-based forecasting models, which ostensibly provide less insight into a time series's structure than classical methods \cite{oreshkin2020nbeats,godahewa2021monash}. However, the strong empirical performance of machine learning suggests the potential for these methods to reveal new properties of nonlinear dynamics and, ultimately, new bounds on the intrinsic predictability and thus reducibility of chaotic systems.

%kolmogorov complexity
%We also note that $\lambda_\text{max}$ is insufficient to fully characterize whether a system remains empirically predictable over extended periods, a known limitation that motivates recent effects to introduce alternative ad hoc invariant properties that more directly determine predictability \cite{expansionentropy}. In future work, we plan to investigate in further detail the relationship between our empirical forecasting results and various invariant properties of the different dynamical systems in our dataset. However, we note that among well-studied properties of different attractors, such as the fractal dimension and metric entropy, none showed a stronger prediction with long-term predictability than $\lambda_\text{max}$.

%While the intuitive notion that larger models more effectively learn patterns in from large training datasets echoes similar findings in other fields,\cite{bitterpill} we emphasize that model capacity appears to have a stronger role than model architecture in our experiments. The best-performing models strongly correlate with one another, particularly at forecasting horizons where differences among time series most strongly manifest.
%Lyap not necessarily the determinant of chaos, but practically the most common continuous measure of chaoticity \cite{expansionentropy}.


\section{Acknowledgments}

W. G. was supported by the University of Texas at Austin. Computational resources for this study were provided by the Texas Advanced Computing Center (TACC) at The University of Texas at Austin.

\section{Code availability}

All code used in this study is available online at \url{https://github.com/williamgilpin/dysts}


%\clearpage
\bibliography{forecast_cites} 
\bibliographystyle{naturemag}



\end{document}