\documentclass[11pt]{article}

\usepackage{natbib}
\usepackage[margin=2cm]{geometry}%top=,bottom=,left=,right=
% \input{macros.tex}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

\title{\textbf{Response to Reviewers' Comments}}
\date{February, 2023}

\begin{document}

\maketitle

\section*{Comments from the editors and reviewers}
Ref:  IJDL-D-22-00084 \\
Title: Retrievability in an Integrated Retrieval System: An Extended Study \\
Journal:  International Journal on Digital Libraries \\

% Dear authors,

% thank you for your submission. Please find the reviewers' comments below. We would like you to read them carefully and to pick-up as much of the suggestions and requests as possible. We especially would like to ask you to put a bit more effort to contextualize your work and to pickup the questions regarding the superset from reviewer #2.

% We expect you to resubmit an updated version of your paper in the next 30 days. Please contact us, if you struggle with this timeline.

% All the best,

% Philipp Schaer and Annika Hinze


Dear reviewers,

First of all, we would like to thank you for the constructive and helpful feedback on our submission. We have revised the manuscript and addressed all of your comments and remarks.

Please find our responses to your valuable feedback and suggestions below.


\section{Reviewer: \#1}

\begin{bfseries}

The paper has had sufficient additional work to merit publication in the journal.  There are some grammatical issues that should be addressed. Some are really necessary; others are not as important.  I list them below, but do not guarantee that I caught everything.
\end{bfseries}

\begin{quote}
We would like to thank the reviewer for shortlisting a set of copy-editing issues. In the revised version, we have addressed all the following points as well as, we have performed a thorough copy-editing to fix any further similar issues.
\end{quote}

\begin{bfseries}
Citations.  Many of the citations appear in a sentence without any kind of enclosing marks.  This makes it difficult to read because the citation text appears to be part of the sentence.  Please enclose all citations in brackets or parentheses.  (Some of the citations are enclosed in parentheses, most are not.  Of course, there are places where the citation really is part of the sentence and these should not be bracketed.
\end{bfseries}

\begin{quote}
We would like to thank the reviewer for pointing this out. Indeed the citation was a mix-up due to the use of different bibliography styles.
This is now fixed in the revised draft.
\end{quote}

%%%%%%
\begin{bfseries}
The word "utilize" appears frequently.  In almost all cases, "use" is more appropriate.

\end{bfseries}

\begin{quote}
The frequent use of the term `utilized' is fixed.
\end{quote}


%%%%%%
\begin{bfseries}
There are a number of places where the verb and nouns do not agree with regard to singular/plural form. 
Examples: first paragraph "that search and assimilate results from assorted sources" should be searches and assimilates\\
has become increasingly important in the (interactive) information retrieval and digital library community.  $\rightarrow$ should be communities\\
Page 10 a number of places that need attention.  Column 1: higher for publication than dataset at the lower rank cut-offs $\rightarrow$  higher for publications than datasets at the lower rank cut-offs\\
same on next sentence and on the top of the next column. \\
Page 11: top 10 position $\rightarrow$ positions\\
100 ranked document $\rightarrow$ documents\\
top 100 position $\rightarrow$ positions\\
Page 14 first column: in publication and datasets $\rightarrow$ publications and datasets\\
last line: dataset as compared to publications $\rightarrow$ datasets
\end{bfseries}

\begin{quote}
All the points are addressed as suggested.
\end{quote}


%%%%%%
\begin{bfseries}

First paragraph "the massive amount of research datasets available" should be the massive number of research datasets available
\end{bfseries}

\begin{quote}
Thanks for the suggestion. The sentence is changed to "...the enormous number of research datasets available."
\end{quote}


%%%%%%
\begin{bfseries}
Page 2 Section Research questions
questions raised and responded in Roy et all $\rightarrow$ should be responded to
\end{bfseries}

\begin{quote}
Thanks for the suggestion. The sentence is changed to ``We verify the research questions put forward and discussed by~Roy et al."
\end{quote}

\begin{bfseries}
Page 3, section 2 Background and related work, first paragraph  In context of document $\rightarrow$ should be in the context of related work\\
paragraph after the bullets describing the equation indicator function that respectively returns  $\rightarrow$ delete respectively\\
end of that same column:  Considering a fairly sized collection $\rightarrow$  what is a "fairly sized" collection.  Replace with large sized\\
Top of next column: all queries Q is to use a query log however acquiring -- all queries Q is to use a query log; however, acquiring ...\\
Another "fairly sized collection in that column should be changed\\
Thanks to the reviewer for pointing this missing semicolon. It has been fixed in the revised draft.
Also, the later sentence is modified to:
"This approach may result in an enormous number of queries if a large collection of documents is considered.".
\end{bfseries}

\begin{quote}
    All the above points are fixed in the revised draft.
\end{quote}

%%%%%%
\begin{bfseries}
Page 4: second column, Wilkie and Azzopardi -- has there been a previous use of that citation?  I did not see it but might have missed it.  The first time it is used, of course, it needs to have a full citation with date.
\end{bfseries}

\begin{quote}
The concerning sentence is changed to ``A topic-centric query generation technique, tested on the Associated Press (AP) document collection, is proposed in Wilkie and Azzopardi (2016)" in the revised draft.
\end{quote}


%%%%%%
\begin{bfseries}
Page 5, second column near the end of the paragraph before section 4.  direct feedback from the them $\rightarrow$  delete the\\
same paragraph: In this article, we are going present $\rightarrow$  delete "we are" or change to "we are going to present"
\end{bfseries}

\begin{quote}
The revised draft fixed these points accordingly.
\end{quote}


%%%%%%
\begin{bfseries}

Page 7 first line: system returns six search result pages $\rightarrow$ delete system returns.   The end of the previous page has "containing a total of"
\end{bfseries}

\begin{quote}
We believe the reviewer has skipped a page in between. 
Page 5 was ending with ``...containing a total of" which was carried on to page 6 with: ``860K indexed records...".
The start of page 7 is a continuation of the sentence: ``Given a query, the".
\end{quote}


%%%%%%
\begin{bfseries}


Same paragraph: having the most number of entries $\rightarrow$ having the largest number of entries\\
Page 8, Section 4.2 First paragraph.  "queries formed using a free-text format\\
are possible to be answered by the collection.  $\rightarrow$ delete everything after possible\\
same page, second column: The value of Gini coefficient $\rightarrow$ the Gini coefficient\\
same page, second column  where as should be one word: whereas\\
(This appears elsewhere in the paper also)\\
Page 9, second column: value of c will restrict less number of documents $\rightarrow$ reduce the number of documents\\
Section 4.4, first paragraph: datasets are far diverse than other ... $\rightarrow$ far more diverse\\
Page 10, line 4: we can see that this change in the ... $\rightarrow$ these changes\\
line 6: Note that, we have $\rightarrow$ no comma -- Note that we have ...\\
line 11: quantity of repeated queries $\rightarrow$ number of repeated queries\\
line 13: resulting into a vast diversity $\rightarrow$ resulting in a large diversity\\
second column, line 3 smaller than the publication (should be publications) $\rightarrow$ smaller than for publications and datasets\\
line 5: variable category than the other types $\rightarrow$ variable category compared to the other types\\
Next paragraph: From the Figure, it is evidently comprehensible $\rightarrow$ include the figure number, and change "evidently comprehensible" to "seen"\\
Last paragraph on the page: score of documents escalate with higher values $\rightarrow$ score of documents escalates with higher values\\
Page 11 first paragraph: more number of variables $\rightarrow$ more variables\\
Next paragraph, second sentence: Note that, $\rightarrow$ remove the ,\\
near bottom of collumn: in this section, we report -- remove the ,\\
next column, near end: observed for the variable among $\rightarrow$ variable category\\
Page 12, first column indicates a imperceptible $\rightarrow$ an imperceptible\\
the most diverse results are observed in case of the publication $\rightarrow$ 2 changes: the most diverse results are observed in the case of the publication type (or category)\\
3rd line from end of column: such as the lists are needed to be conjoint $\rightarrow$ such as the lists needing to be conjoint\\
second column: particularly for the publication and dataset $\rightarrow$ publication and dataset types.\\
retrievability of irrespective of the type $\rightarrow$ delete first "of"\\
Page 14: column 1 datasets is also eminent $\rightarrow$ evident, not eminent\\
earlier work is, we have used a deduplicated $\rightarrow$ earlier work is that we have used ...\\
Page 15 near end of first paragraph: we examined that $\rightarrow$ we showed that\\
end of next paragraph: category is close to equality and items $\rightarrow$ closer\\

\end{bfseries}

\begin{quote}
All the above typos and grammatical issues are now addressed properly in the revised draft.
\end{quote}


\section{Reviewer \#2}


%%%%%%
\begin{bfseries}

This is an interesting paper using statistical methods to try to conjecture on the retrievability of documents in a corpus given an arbitrary superset of "all possible queries".
Unfortunately it is not really clear what the boundaries of the query superset are nor how they really contribute to adjudicating the arbitrary retrievability of any random document that is a member of the corpus. 

Selectors for queries are language based; as such I was expecting a little more discussion of language-based elements. For instance it is not clear if the superset of queries (described as a set of unigrams and bigrams through references to other literature) is restricted just to the vocabulary being employed to provide technical terms for topics or also encompasses a full-text index of either all of the other metadata fields (e.g., title, author, etc.) or the complete documents themselves (or both). 

\end{bfseries}

\begin{quote} 
We would like to thank the reviewer for the comments. We try to clarify. Similar to our earlier study (Roy et al. JCDL 2022), we have used an updated and more recent version of the query log containing raw keyword queries submitted by real users to the GESIS search system. GESIS search searches via an ElasticSearch 
 index over bibliographic descriptions of items. No full text of whole documents is involved. Hence, there are no AI-generated queries involved.
As there is no control in the language in which the queries are formulated, we do not have any language-specific components.
\end{quote}

%%%%%%
\begin{bfseries}

[I should note that rather late in the paper you mention that queries
consisting of 5- or larger n-grams are discounted as "known item searches"; I find I have a question of whether or not Azzopardi \& Vinay's research is extensible to 3- and 4-grams or if you shouldn't have constrained Q to only unigrams and bigrams--and it isn't clear because you never explicitly state what n-grams are in Q.]  

\end{bfseries}

\begin{quote}
The whole idea of retrievability is to quantify how findable the documents are in a collection.
Considering each document has the ability to answer certain questions (information needs), the findability of a document can be approximated by the number of queries for which the document is retrieved within the top-rank position, as defined by Azzopardi \& Vinay.
The set of queries submitted to a search engine is the ideal candidate to compute retrievability of a collection of documents.
In absence of such a query log, Azzopardi \& Vinay had constructed a set of cosmetic queries from the original corpus of documents in their original work.
They formed the queries consisting of the most frequent unigrams and bigrams with some threshold based on the count to make the query set size tractable. 
This is an extension of the Query Based Sampling technique proposed in Callan and Connell, TOIS 2001 (Callan J, Connell M (2001) in the draft).

However, when the actual user queries are available, they may serve the purpose better as compared to the synthetic queries, at least in terms of better reflecting the information needs.
Hence the real-user queries beyond bigrams are considered while computing the retrievability.

\end{quote}

%%%%%%
\begin{bfseries}


If the superset of queries is narrowly scoped to just the subject vocabulary used for the topical then I would expect some comparison of search terms being employed by users to those provided by the subject vocabulary and a mathematical weight given to the gap between the two. For example consider a corpus of documents regarding the management of floodplains that have been cataloged using the traditional library approach and employing the Library of Congress Subject Headings (LCSH). The technical term "basin" commonly occurs both in the documents (i.e., in titles, document text, etc.) and is employed in the common parlance of the users (e.g., flood
plain management for the Ohio River basin), However, the word "basin" is not a word used as a topical term describing floodplains by the LCSH; instead the term "watershed" is employed (e.g., Ohio River watershed). If the scope of our superset Q is such that it only contains the topical terms used in the subject vocabulary then any possible queries using the word "basin" are not going to retrieve documents relevant to the work of users like floodplain managers (and this is why we include the full text of the metadata records--in order to broaden the scope of Q to include technical terms that appear in titles but which are not matched in subject vocabularies). 

So, the precise scope of Q is critically important to answering the research questions the authors are investigating and more time needs to be taken to fully contextualize what things are in Q and what things are not. It might also be useful to consider the bias created by the employment of chosen subject vocabularies. A
vocabulary like LCSH is broad and general while something like the Synthetic Biology Open Language (SBOL) ontology has a very narrow focus with a fit of purpose that is limited to a very small Q. So also, fit for purpose of the subject vocabulary likely also has a measurable impact on phenomena being studied here.

\end{bfseries}

\begin{quote}
We are not sure if we understand this comment correctly. So indeed there can occur some type of vocabulary mismatch between the indexed documents and the queries issued by the users. However, our approach here uses the queries submitted by real users. Hence, the concern here by reviewer 2 that ``the superset of queries does not only contains the topical terms used in the subject vocabulary" is not critical for our setup. Nevertheless, we absolutely agree that  ``the precise scope of Q is critically important to answering the research questions''. But since we use real user queries we work with a realistic representation of their search behaviour. 

To clarify, the objective of our work is to measure the retrievability of different document types in a real-life system, not to overcome or handle vocabulary mismatch via query expansion or search term recommendation.

\end{quote}




%%%%%%
\begin{bfseries}

Finally, I took a look at GESIS-Suche and had some questions about the metadata that is available for datasets and more specifically--for variables (i.e., data features). Thinking back to studies that examine reuse of data within research domains (e.g., Palmer's 2014 "Putting research data into context: scholarly, professional, \& educational approaches to curating data for reuse."), it doesn't appear that either datasets or variables have sufficient metadata that contextualizes them. In particular the topics used to describe them seem more appropriate for describing the research articles they relate to. E.g., do topics like "Political behaviour and attitudes", "Family life and marriage", etc. adequately describe a variable like "3rd Hobby"? What measures have been taken to assess whether or not the subject vocabulary being used in the topical metadata is fit for purpose? If it isn't fit for purpose, how does that impact the study results showcased in this paper?

\end{bfseries}

\begin{quote}
Thank you for this very important point. We agree, that the level and granularity of the metadata description of individual items will have consequences for their retrievability. 
Whether the metadata in GESIS Search is actually sufficient to contextualise the datasets is a valid point and in particular for variables very challenging. However, our study presented here is a representation of a real search system that includes indexed documents used in production. Since our aim was to get a realistic measure of retrievability of the documents we decided not to take any measures to assess or modify the underlying metadata used for indexing. 
Our results and also your comment motivated us to go back to the maintainers/indexers of the variables, and inform them to consider more and/or other ways of indexing the survey variables in the future. 

\end{quote}

We would like to thank the reviewers once again for their significant comments. We believe we have addressed them all which has improved the quality of the paper.

\vspace{1cm}
Best regards,\\
Dwaipayan Roy, Zeljko Carevic, Philipp Mayr.

\end{document}
