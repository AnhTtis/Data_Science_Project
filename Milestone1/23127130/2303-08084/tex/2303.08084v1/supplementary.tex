\section{Additional Results}
\label{app:extra_results}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/extra_results_2.pdf}
    \caption{Additional results using TIME. After applying the requested edit (in black) to the text-to-image model, related prompts (green) change their behavior accordingly, whereas unrelated ones (gray) remain unaffected.}
    \label{fig:extra_res}
\end{figure*}

\section{Closed-Form Solution Proof}
\label{app:proof}
We aim to minimize the loss function presented in \autoref{eq:loss}, which is
\begin{align*}
    L({\W'}_K, {\W'}_V) =
    \sum_{i=1}^{l}
    \left\lVert {\W'}_K \cc_i - \kk^*_i \right\rVert_2^2
    + \lambda \left\lVert {\W'}_K - {\W}_K \right\rVert_F^2 + \sum_{i=1}^{l}
    \left\lVert {\W'}_V \cc_i - \vv^*_i \right\rVert_2^2
    + \lambda \left\lVert {\W'}_V - {\W}_V \right\rVert_F^2.
\end{align*}
To find the optimal ${\W'}_K$, we differentiate w.r.t.\ it and set to zero:
\begin{align*}
\frac{\partial L({\W'}_K, {\W'}_V)}{\partial {\W'}_K} & = &
\sum_{i=1}^{l}
2 \left( {\W'}_K \cc_i - \kk^*_i \right) \cc_i^\top
+ 2 \lambda \left( {\W'}_K - {\W}_K \right) & = 0 \\
& \Rightarrow &
\sum_{i=1}^{l}
\left( {\W'}_K \cc_i - \kk^*_i \right) \cc_i^\top
+ \lambda \left( {\W'}_K - {\W}_K \right) & = 0 \\
& \Rightarrow &
\sum_{i=1}^{l} {\W'}_K \cc_i \cc_i^\top - \sum_{i=1}^{l} \kk^*_i \cc_i^\top
+ \lambda {\W'}_K - \lambda {\W}_K & = 0 \\
& \Rightarrow &
\sum_{i=1}^{l} {\W'}_K \cc_i \cc_i^\top 
+ \lambda {\W'}_K  & = \sum_{i=1}^{l} \kk^*_i \cc_i^\top + \lambda {\W}_K \\
& \Rightarrow &
\lambda {\W'}_K + \sum_{i=1}^{l} {\W'}_K \cc_i \cc_i^\top  & = \lambda {\W}_K + \sum_{i=1}^{l} \kk^*_i \cc_i^\top \\
& \Rightarrow &
{\W'}_K \left( \lambda \I + \sum_{i=1}^{l} \cc_i \cc_i^\top \right)  & = \lambda {\W}_K + \sum_{i=1}^{l} \kk^*_i \cc_i^\top \\
& \Rightarrow &
{\W'}_K  & = \left( \lambda {\W}_K + \sum_{i=1}^{l} \kk^*_i \cc_i^\top \right) \left( \lambda \I + \sum_{i=1}^{l} \cc_i \cc_i^\top \right)^{-1}.
\end{align*}
The last implication holds because $\cc_i \cc_i^\top$ are symmetric rank-one matrices with a positive eigenvalue and therefore positive semi-definite, 
and $\lambda \I$ is positive definite ($\lambda>0$), which makes their total sum positive definite and therefore invertible.
This makes the obtained solution unique and well-defined.
Similarly, we find the optimal ${\W'}_V$ using the same method and obtain
\begin{align*}
{\W'}_V  & = \left( \lambda {\W}_V + \sum_{i=1}^{l} \vv^*_i \cc_i^\top \right) \left( \lambda \I + \sum_{i=1}^{l} \cc_i \cc_i^\top \right)^{-1},
\end{align*}
thus completing the proof. \textit{Q.E.D.}

\section{Implementation Details}
\label{app:implementation}
We use Stable Diffusion~\cite{rombach2022high} version 1.4 with its default hyperparameters: $50$ diffusion timesteps, a classifier-free guidance~\cite{ho2021classifier} scale of $7.5$, and a maximum number of tokens of $77$. The model generates images of size $512\times512$ pixels.
Unless specified otherwise, we use $\lambda=0.1$ for TIME.
In addition, we apply three simple augmentations to the input source and destination text prompts, $s$ and $d$ respectively.
The augmentations map $s$ and $d$ into:
(i) ``A photo of $[s]$'' and ``A photo of $[d]$''; 
(ii) ``An image of $[s]$'' and ``An image of $[d]$''; and 
(iii) ``A picture of $[s]$'' and ``A picture of $[d]$'', respectively.
The original $s$ and $d$ and their augmentations constitute four lists of corresponding token embeddings $\{\cc_i\}_{i=1}^{l}$, $\{\cc^*_i\}_{i=1}^{l}$ (as denoted in \autoref{sec:method}). We concatenate these lists into a unified corresponding embedding list $\{\cc_i\}_{i=1}^{L}$, $\{\cc^*_i\}_{i=1}^{L}$ and use it for the loss function in \autoref{eq:loss} and its solution in \autoref{eq:closed-form}.

To quantify efficacy, generality, and specificity, we use the CLIP~\cite{clip} ViT-B/32 model as a zero-shot text-based classifier.
When calculating metrics over the MS-COCO~\cite{lin2014microsoft} dataset, we follow standard practice~\cite{rombach2022high, saharia2022photorealistic, ramesh2022hierarchical, balaji2022ediffi}:
We randomly sample $30000$ captions from MS-COCO and generate images based on them.
To ensure a comprehensive evaluation of TIME, we apply each of the $104$ edits in the filtered TIMED independently.
Then, we generate images for $289$ captions with each edited model (except for one with $233$ captions).
Finally, we compute CLIP Score~\cite{hessel2021clipscore} against the $30000$ captions, and FID~\cite{fid} against the entire MS-COCO validation set (center cropped and resized to $512\times512$ pixels).


Our source code and datasets are available at \url{https://github.com/bahjat-kawar/time-diffusion}.

\section{Filtering TIMED for Quantitative Evaluation}
\label{app:dataset_filter}

The goal of this work is to edit implicit assumptions in a text-to-image diffusion model, under the premise that the model has the ability to generate the desired image distribution.
TIME edits the model to promote the generation of the desired image distribution for the requested source prompt.
Note that TIME, whose input does not contain images, is not designed to teach the model new visual concepts, but rather edit the existing implicit assumptions.

Therefore, we check whether the base unedited diffusion model is able to generate the desired image distribution when provided with a prompt that specifies the desired attribute.
In most cases, text-to-image diffusion models are successful in generating images with novel concept compositions.
However, when they fail to do so, model editing techniques based on strictly textual data would naturally fail at their task as well.
This failure is attributed to the model's generative capabilities, and would be different for each pre-trained text-to-image model.

We use the pre-trained unedited Stable Diffusion~\cite{rombach2022high} v1.4 model, and generate $24$ images for each positive destination prompt in TIMED (making this setting an \textit{oracle}).
We then use CLIP~\cite{clip} to classify these images as either the source or destination prompt.
Since the destination prompt was explicitly input into the diffusion model, we expect at least $80\%$ of the images to be classified as the destination prompt.
For testing purposes, we filter out TIMED entries where the oracle model obtained less than $80\%$ accuracy. Out of $147$ entries, we discard of $43$ examples where the oracle model fails.
Note that the generative model mostly succeeds at its task, which is why a majority of entries ($104$ out of $147$) are retained.
We then evaluate our method, the unedited model, and the oracle one on these $104$ entries, and the results are summarized in \autoref{tbl:results}.

We provide the TIMED dataset ($147$ test set and $8$ validation set entries) in \url{https://github.com/bahjat-kawar/time-diffusion}.
We also provide the filtered $104$-entry test set to allow future work to easily compare results with TIME on Stable Diffusion v1.4.

\begin{table*}
    \centering
    \begin{tabular}{c c | ccc | ccc}
         \toprule
         & & \multicolumn{3}{c|}{\textbf{Optimizing $\W_V$ only}} & \multicolumn{3}{c}{\textbf{Optimizing $\W_V$ and $\W_K$}} \\
         \vspace{-0.7em}&&&&&&& \\
         \textbf{Augmentations} & $\boldsymbol{\lambda}$
         & \textbf{Generality} ($\uparrow$) & \textbf{Specificity} ($\uparrow$) & \textbf{Mean}  ($\uparrow$) & \textbf{Generality} ($\uparrow$) & \textbf{Specificity} ($\uparrow$) & \textbf{Mean} ($\uparrow$) \\
         \midrule
         \multirow{8}{*}{\textbf{No}}
         & $0.01$ & $55.50\%$ & $73.30\%$ & $\underline{63.17\%}$ & $64.60\%$ & $68.00\%$ & $\underline{66.26\%}$ \\
         & $0.1$ & $51.70\%$ & $71.80\%$ & $60.11\%$ & $60.20\%$ & $67.80\%$ & $63.77\%$ \\
         & $1$ & $51.80\%$ & $69.50\%$ & $59.36\%$ & $61.10\%$ & $68.00\%$ & $64.37\%$ \\
         & $10$ & $51.20\%$ & $68.50\%$ & $58.60\%$ & $61.00\%$ & $67.30\%$ & $64.00\%$ \\
         & $100$ & $48.80\%$ & $69.60\%$ & $57.37\%$ & $57.30\%$ & $68.00\%$ & $62.19\%$ \\
         & $1000$ & $44.30\%$ & $68.00\%$ & $53.65\%$ & $46.60\%$ & $67.50\%$ & $55.14\%$ \\
         & $10000$ & $37.10\%$ & $70.60\%$ & $48.64\%$ & $37.00\%$ & $71.60\%$ & $48.79\%$ \\
         & $100000$ & $21.40\%$ & $80.80\%$ & $33.84\%$ & $21.60\%$ & $81.60\%$ & $34.16\%$ \\
         \midrule
         \multirow{8}{*}{\textbf{Yes}}
         & $0.01$ & $55.50\%$ & $64.90\%$ & $59.83\%$ & $65.10\%$ & $62.30\%$ & $63.67\%$ \\
         & $0.1$ & $59.80\%$ & $69.40\%$ & $\underline{64.24\%}$ & $67.80\%$ & $65.40\%$ & $\mathbf{\underline{66.58\%}}$ \\
         & $1$ & {$57.80\%$} & {$68.90\%$} & {$62.86\%$} & $66.70\%$ & $64.50\%$ & $65.58\%$ \\
         & $10$ & $56.30\%$ & $69.20\%$ & $62.09\%$ & $65.90\%$ & $65.10\%$ & $65.50\%$ \\
         & $100$ & $54.80\%$ & $69.80\%$ & $61.40\%$ & $62.50\%$ & $67.20\%$ & $64.76\%$ \\
         & $1000$ & $51.00\%$ & $67.70\%$ & $58.18\%$ & $57.00\%$ & $68.00\%$ & $62.02\%$ \\
         & $10000$ & $46.50\%$ & $67.90\%$ & $55.20\%$ & $49.30\%$ & $66.50\%$ & $56.62\%$ \\
         & $100000$ & $31.60\%$ & $74.90\%$ & $44.45\%$ & $33.10\%$ & $74.50\%$ & $45.84\%$ \\
         \bottomrule
    \end{tabular}
    \caption{Ablation study results. ``Mean'' is the harmonic mean of generality and specificity. The highest mean in each category is \underline{underlined}, and the highest one overall is also \textbf{\underline{in bold}}.}
    \label{tbl:ablation}
\end{table*}

\section{Ablation Study}
\label{app:ablation_study}
To quantify the effect of each element of our method, we conduct an ablation study using the $8$-entry TIMED validation set.
We measure the effect of optimizing only the value projection matrices $\W_V$ versus optimizing both $\W_V$ and $\W_K$.
We also measure the effect of utilizing the textual augmentations detailed in \autoref{app:implementation}.
Finally, we experiment with different $\lambda$ values to traverse the generality--specificity tradeoff.

We evaluate generality and specificity as described in \autoref{sec:results}, and present the ablation study results in \autoref{tbl:ablation}.
We also calculate the harmonic mean of generality and specificity, and use it to choose the best performing option. Thus, the main TIME algorithm discussed in the paper uses text augmentations, optimizes both $\W_V$ and $\W_K$, and uses $\lambda=0.1$.
Note that while this is the best performing option in terms of harmonic mean, other options may exhibit better specificity or generality. Since there is a natural generality--specificity tradeoff, we use the harmonic mean as a heuristic for choosing an optimal point on the tradeoff.
Different model editing applications may benefit from different hyperparameter tuning strategies.
Our closed-form solution becomes numerically unstable for $\lambda<0.01$.
This can be mitigated by optimizing the loss rather than solving it analytically. 
Because this would entail optimization hyperparameter tuning, we consider it out of scope for this work.


\section{Gender Bias Mitigation}
\label{app:gender}
\subsection{Dataset}
\label{app:gender_dataset}
\input{tables/gender_dataset}

In \autoref{tbl:gender_dataset}, we present a sample of the data used to perform and evaluate TIME for gender debiasing. The professions are taken from the list of stereotypical professions by \cite{zhao-etal-2018-gender}. Some of the stereotypes listed in the original list did not align with the stereotypes observed on the tested text-to-image model (\textit{e.g.}, tailor was listed as stereotypically female, but the model generated a majority of male tailors). Thus we aligned the stereotypes with what is observed in the model.
Moreover, we dropped professions for which the model did not generate pictures of humans (\textit{e.g.}, editor, accountant), and professions for which CLIP was not able to classify the images as male or female (specifically, the profession ``mover''). The dataset is provided in \url{https://github.com/bahjat-kawar/time-diffusion}.

\subsection{Implementation Details and Results}
\label{app:gender_implementation}

TIME edits according to the \textit{editing} prompt (from \autoref{tbl:gender_dataset}), without utilizing textual augmentations.
We search for an ideal  $\lambda_p$ per profession, for which $\Delta_p < 0.1$ on the \textit{validation} prompt.

In \autoref{tbl:gender_full_results} we present the full results for every profession we operated on using the \textit{testing} prompts, including the $\lambda_p$ we used to get these results.
Our results are computed across 24 seeds. For computing $\Delta_p$ (as well as $\Delta$ in \autoref{tbl:gender_results}), a distribution of images is required, thus we compute $\Delta_p$ on 8 seeds, and repeat the experiment 3 times to get an average $\Delta_p$.
Note that this is different from computing $\Delta_p$ on each seed and averaging, since the metric $\Delta_p$ is not defined over a single generated image.
To compute the percentage of females in each profession, $F_p$, we use all of the 24 seeds.

\input{tables/gender_full_results}