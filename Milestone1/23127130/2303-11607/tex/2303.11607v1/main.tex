% !TEX spellcheck = en_US
\documentclass[journal]{IEEEtran}
\usepackage{graphicx}
\usepackage{multirow}
%\usepackage[dvipsnames]{xcolor}
\usepackage[table,xcdraw,dvipsnames]{xcolor}
\usepackage{longtable}
\usepackage{color}
\usepackage{xcolor,colortbl}
\usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
\usepackage{pifont}
\usepackage{comment}
%BS: to make more compact:
\usepackage[activate]{microtype}
%\usepackage[colorlinks=true,linkcolor=green]{hyperref}
\usepackage[colorlinks=false]{hyperref}

\sloppy
%end 
\newcommand{\cmark}{\text{\ding{51}}}

\newcommand{\xmark}{\text{\ding{55}}}
\usepackage{authblk}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
%\usepackage{dblfloatfix} 
\usepackage{url}
\usepackage[dvipsnames]{xcolor}
\usepackage{soul}
\newcommand*{\SL}[1]{\textcolor{blue}{#1}}
\newcommand*{\MS}[1]{\textcolor{red}{#1}}
\newcommand*{\SK}[1]{\textcolor{green}{#1}}
\newcommand*{\EC}[1]{\textcolor{purple}{#1}}
\newcommand*{\jq}[1]{\textcolor{purple}{#1}}
\newcommand*{\fm}[1]{\textcolor{purple}{#1}}
\usepackage[separate-uncertainty = true,multi-part-units = repeat]{siunitx}
\usepackage{array,amsfonts,amsmath}
\usepackage{caption}
\usepackage{verbatim}
%\usepackage{subcaption}
\ifCLASSOPTIONcompsoc
 \usepackage[nocompress]{cite}
\else
 % normal IEEE
 \usepackage{cite}
\fi
\ifCLASSINFOpdf
\else
\fi

\begin{document}

%\title{A Survey on Deep Reinforcement Learning for Audio Processing}
\title{Transformers in Speech Processing: A Survey}

\author[1]{Siddique Latif\thanks{Email: siddique.latif@usq.edu.au}}
\author[2]{Aun Zaidi}
\author[3]{Heriberto Cuay\'ahuitl}
\author[4]{Fahad Shamshad}
\author[5]{Moazzam Shoukat}
\author[6]{Junaid Qadir}


\affil[1]{Queensland University of Technology (QUT), Australia}
\affil[2]{Information Technology University, Pakistan}
\affil[3]{University of Lincoln, UK}
\affil[4]{Mohamed bin Zayed University of Artificial Intelligence}
\affil[5]{EmulationAI}
\affil[6]{Qatar University, Doha}




% \maketitle

\maketitle

\begin{abstract}

The remarkable success of transformers in the field of natural language processing has sparked the interest of the speech-processing community, leading to an exploration of their potential for modeling long-range dependencies within speech sequences. Recently, transformers have gained prominence across various speech-related domains, including automatic speech recognition, speech synthesis, speech translation, speech para-linguistics, speech enhancement, spoken dialogue systems, and numerous multimodal applications. In this paper, we present a comprehensive survey that aims to bridge research studies from diverse subfields within speech technology. By consolidating findings from across the speech technology landscape, we provide a valuable resource for researchers interested in harnessing the power of transformers to advance the field. We identify the challenges encountered by transformers in speech processing while also offering insights into potential solutions to address these issues.


%Our survey serves as a foundation for future studies, fostering a deeper understanding of the unique strengths and limitations of transformers in the context of speech processing and encouraging continued innovation in this rapidly evolving domain.

%Transformers have achieved great success in natural language processing and it also triggered the speech community to exploit them for modeling long dependencies between speech sequences. Currently, transformers are widely being utilized in different speech-related domains including automatic speech recognition, speech synthesis, speech translation, speech para-linguistics, and speech enhancement. spoken dialogue systems,  and various multimodal applications. In this paper, we perform a comprehensive survey by bringing together research studies across different related areas in speech technology. We highlighted the challenges faced by transformers in speech processing and highlight the direction to solve these issues. The findings of this paper will help researchers interested in utilizing transformers in speech technology.


\end{abstract}

% \begin{IEEEkeywords}
% Transformers, speech processing, self-attention
% \end{IEEEkeywords}

% \IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:introduction}

Transformers have garnered significant attention in the speech processing and natural language processing (NLP) communities~\cite{karita2019comparative,lin2022survey,tay2022efficient,zhang2020transformer,wolf2019huggingface,wolf2020transformers} owing to their remarkable performance across a spectrum of applications, including  machine translation~\cite{wang2019learning},  automatic speech recognition (ASR)~\cite{dong2018speech,song2022multimodal}, question answering~\cite{yang2021just}, speech enhancement~\cite{dang2022dpt}, speech emotion recognition~\cite{wagner2022dawn}, and speech separation~\cite{subakan2021attention}, to name a few. These models have even surpassed traditional recurrent neural networks (RNNs), 
%Due to their exceptional performance, transformers have become the preferred method for sequence-to-sequence (S2S) modeling, outperforming traditional recurrent neural networks (RNNs) 
that struggle with long sequences and the vanishing gradient problem on sequence-to-sequence tasks~\cite{karita2019comparative}. The rapid development and popularity of transformer-based models in speech processing have generated a wealth of literature investigating the unique features that underlie their superior performance.


Transformers have an advantage in comprehending speech, as they analyze the entire sentence at once, whereas RNNs can only process smaller sections at a time. This is made possible by the unique self-attention-based architecture of transformers~\cite{vaswani2017attention}, which enables them to learn long-term dependencies, which is critical for speech processing tasks.
%The unique self-attention-based architecture of transformers allows them to learn long-term relationships, which is essential for tasks like speech recognition and natural language understanding. 
Moreover, the multi-head attention mechanism~\cite{voita2019analyzing}---a specialized feature in transformers---allows for more efficient parallelization during training, making them ideal for handling large datasets, which is a common challenge in speech processing tasks. This unique combination of self-attention and multi-head attention empowers transformers to achieve exceptional performance in sequence-to-sequence modeling, making them an indispensable tool for researchers and practitioners in the field of speech processing.

%Transformers have gained significant attention due to their exceptional performance on various natural language processing (NLP) tasks such as machine translation, question answering, and text classification and speech processing (SP) tasks such as automatic speech recognition (ASR), speech enhancement, speech emotion recognition, and speech data retrieval. Transformers are now increasingly becoming the method of choice for sequence-to-sequence (S2S) modeling, surpassing recurrent neural networks (RNNs), which do not work as well with long sequences being prone to the vanishing gradient problem. Transformers can better understand speech because they look at the whole sentence at once, while RNNs can only understand small parts of the sentence at a time. Transformers have a unique architecture based on a self-attention mechanism that allows them to attend to the entire sequence, enabling them to learn long-range relationships which is crucial for tasks like speech recognition and natural language understanding. Additionally, the multi-head attention mechanism, a specialized implementation in transformers, allows for better parallelization during training, making them more efficient to train on large datasets, a common issue in SP tasks.


%Sequence-to-sequence (S2S) models such as recurrent neural networks (RNNs) have significantly impacted the field of SP due to their ability to optimize the individual components directly in an end-to-end manner. These sequential models have demonstrated remarkable improvements in numerous SP applications, such as automatic speech recognition (ASR), speech enhancement, emotion recognition, and speech data retrieval, to name a few. Although effective, the sequential training of RNNs limits the computation parallelization required to train end-to-end SP systems efficiently. Furthermore, these architectures have also been known to suffer from the vanishing gradient problem due to their recurrent nature.

%Sequence-to-sequence (S2S) models, such as recurrent neural networks (RNNs), have had a significant impact on the field of SP due to their ability to directly optimize individual components in an end-to-end manner. These models have demonstrated improvements in various SP applications, such as automatic speech recognition (ASR), speech enhancement, emotion recognition, and speech data retrieval. However, RNNs have limitations in terms of computation parallelization during training and are prone to the vanishing gradient problem due to their recurrent nature.

%Meanwhile, transformer models have shown outstanding performance on various natural language tasks, such as machine translation, question answering, and text classification, among others. These models are based on a self-attention mechanism that learns relationships between elements of a sequence. Unlike recurrent networks, which process sequence elements recursively and can only attend to short-term context, transformers can attend to entire sequences, allowing them to learn long-range relationships. While attention mechanisms have been used in recurrent networks, transformers are solely based on the attention mechanism and have a specialized implementation (i.e., multi-head attention) that allows for better parallelization. These advantages of transformers have generated significant interest in the SP community in adapting them for tasks related to speech.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Meanwhile, Transformer models have demonstrated exemplary performance on numerous natural language tasks including machine translation, question answering, and text classification to name a few. These Transformer models are based on a self-attention mechanism that learns the relationships between elements of a sequence. As opposed to recurrent networks that process sequence elements recursively and can only attend to short-term context, Transformers can attend to complete sequences thereby learning long-range relationships. Although attention models have been used in the recurrent networks, Transformers are based solely on the attention mechanism and have a unique implementation (i.e., multi-head attention) that is optimized for parallelization. These desirable properties of Transformers have sparked great interest in the speech community to adapt them for the SP tasks. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As the use of transformers for speech processing research community is gaining popularity, it is timely to review the existing literature and present a comprehensive overview of the field. In this regard, we provide a comprehensive overview of transformer model applications in the speech processing domain. Our aim is to assist researchers and practitioners in grasping the major trends and recent advancements in the field. Specifically, the main contributions of this survey are as follows:

\begin{itemize}
\item We present the first comprehensive survey of the application of transformer models in the speech processing field. Our survey covers more than 100 papers to cover the recent progress.

\item We provide detailed coverage of this rapidly evolving field by categorizing the papers based on their applications
 % as depicted in Fig~\ref{fig:taxonomy}. 
Specifically, these applications include automatic speech recognition, neural speech synthesis, speech translation, speech enhancement, multi-modal applications, and spoken dialogue systems.

\item Finally, based on our thorough analysis, 
%we develop a comprehensive taxonomy, 
we identify various challenges and propose future research directions. Moreover, we provide valuable insights into potential solutions based on the literature reviewed.
\end{itemize}
% Despite growing interest in speech processing using transformers, existing studies are scattered across different speech related research areas and a comprehensive survey is missing. To address this, we present a comprehensive survey on speech processing using transformers by covering major speech related areas. 
We compare our paper with recent surveys on transformers and speech processing in Table \ref{table:lit-review}. It can be found that most of the transformer-related survey papers are focused on computer vision and natural language processing (NLP). The articles focused on speech processing do not cover transformers. Here, we focus on recent development in speech technology using transformers. Although other recent surveys have focused on deep learning 
techniques for  SR \cite{latif2023survey}, ASR \cite{zhang2018deep,nassif2019speech}, and SER \cite{khalil2019speech,latif2021survey}, none of
these has focused on transformers for speech processing. This
study bridges this gap by presenting an up-to-date survey of
research that focused on speech processing using transformers. The paper is organised as follows. Section \ref{sec:background} provides an overview of the applications of seq2seq models in SP and introduces the salient concepts underlying transformers. Section \ref{sec:applications} presents a comprehensive review of the applications of transformer models in SP. Section \ref{sec:challenges} discusses open problems and future research directions. Finally, in Section \ref{sec:conclusion}, we summarize and conclude the paper.

\begin{table*}[!ht]
\centering
\scriptsize
\caption{Comparison of this paper with other recent survey papers. Where Speech Processing = SP, Speech Emotion Recognition (SER), Natural Language Processing = NLP, Computer Vision = CV, Action Recognition = AR, and Reinforcement Learning = RL}
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Author (year)} & \textbf{\begin{tabular}[c]{@{}l@{}}Speech\\ Focused\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Transformer\\ Focused\end{tabular}} & \textbf{Domain}  & \textbf{Details}   \\ \hline
\begin{tabular}[c]{@{}l@{}}Deng et al.\\ 2016 \cite{deng2016deep}\end{tabular}    & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}   & \begin{tabular}[c]{@{}l@{}}SP\end{tabular} & \begin{tabular}[c]{@{}l@{}}The paper reviews self-supervised representation learning methods in\\ speech processing, discussing generative, contrastive, and predictive\\ methods and multi-modal data approaches.\end{tabular} 
  \\ \hline
\begin{tabular}[c]{@{}l@{}}Khalil et al. \\2019 \cite{khalil2019speech}\end{tabular}    & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}   & \begin{tabular}[c]{@{}l@{}}SER\end{tabular}   & \begin{tabular}[c]{@{}l@{}}This paper provides an overview of deep learning techniques for \\speech  based emotion recognition, covering databases used, emotions \\extracted, contributions made, and limitations related to it.\end{tabular} 



    \\ \hline
\begin{tabular}[c]{@{}l@{}}Nassif et al. \\2019 \cite{nassif2019speech}\end{tabular}    & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}   & \begin{tabular}[c]{@{}l@{}}SP\end{tabular} & \begin{tabular}[c]{@{}l@{}}This survey paper reviews the use of deep learning for speech-related\\ applications, providing a statistical analysis of 174 papers published\\ between 2006 and 2018.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Alam et. al.\\ 2020 \cite{alam2020survey}\end{tabular} & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}   & Multimodal & \begin{tabular}[c]{@{}l@{}}The survey covers DNN architectures, algorithms, and systems for\\ speech and vision applications, not limited to transformers or speech.\end{tabular}  

   \\ \hline
\begin{tabular}[c]{@{}l@{}}Bracsoveanu et al.\\ 2020 \cite{bracsoveanu2020visualizing}\end{tabular} & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark}  & \begin{tabular}[c]{@{}l@{}}NLP\end{tabular}  & \begin{tabular}[c]{@{}l@{}}The survey paper focuses on explaining Transformer architectures\\ through visualizations to provide better understanding and proposes\\ a set of requirements for future Transformer visualization frameworks.\end{tabular}   


   \\ \hline
\begin{tabular}[c]{@{}l@{}}Tan et al.\\ 2021 \cite{tan2021survey}\end{tabular} & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}   & \begin{tabular}[c]{@{}l@{}}SP\end{tabular} & \begin{tabular}[c]{@{}l@{}}This paper provides a comprehensive survey on neural text-to-speech,\\ covering key components such as text analysis, acoustic models, \\ as well as advanced topics like fast, low-resource, robust, expressive, \\and adaptive TTS, and it also discusses future research directions.\end{tabular}  \\ \hline
\begin{tabular}[c]{@{}l@{}}Alharbi et al. \\2021 \cite{alharbi2021automatic}\end{tabular} & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}   & \begin{tabular}[c]{@{}l@{}}ASR\end{tabular} & \begin{tabular}[c]{@{}l@{}}This survey paper provides a systematic review of automatic speech\\ recognition (ASR) technology, covering significant topics and recent\\ challenges published in the last six years.\end{tabular}  \\ \hline
\begin{tabular}[c]{@{}l@{}}Malik et al.\\ 2021 \cite{malik2021automatic}\end{tabular} & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}   & \begin{tabular}[c]{@{}l@{}}ASR\end{tabular} & \begin{tabular}[c]{@{}l@{}}The survey paper compares various deep learning techniques and \\feature extraction methods for ASR and discusses the impact of \\different speech datasets on ASR performance, providing online \\resources and language models for ASR formulation.\end{tabular} 


  \\ \hline
\begin{tabular}[c]{@{}l@{}}Liu et al.\\ 2021 \cite{liu2021survey}\end{tabular}    & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark}  & \begin{tabular}[c]{@{}l@{}}CV\end{tabular}   & \begin{tabular}[c]{@{}l@{}}The survey explores Transformer-based architectures in CV tasks, \\proposing a taxonomy and evaluating and comparing existing \\methods. It suggests three research directions for future investment.\end{tabular} 


\\ \hline
\begin{tabular}[c]{@{}l@{}}Xu et al.\\ 2022 \cite{xu2022multimodal}\end{tabular}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark}  & Multimodal & \begin{tabular}[c]{@{}l@{}}The paper surveys Transformer techniques in multimodal learning, \\including theoretical reviews and applications. It aims to provide \\insights for researchers and practitioners.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Lin et al. \\2022 \cite{lin2022survey}\end{tabular}    & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark}  & Multimodal & \begin{tabular}[c]{@{}l@{}}The survey reviews various Transformer variants in AI fields and \\proposes a new taxonomy. It covers architectural modifications, pre-\\training, applications, and potential directions for future research.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Shamshad et al.\\ 2022 \cite{shamshad2022transformers}\end{tabular}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark}  & Medical Imaging  & \begin{tabular}[c]{@{}l@{}}The paper reviews Transformer models in medical imaging, discussing \\their applications and identifying open problems and future directions.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Acheampong et al.\\ 2022 \cite{acheampong2021transformer}\end{tabular}   & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark}  & \begin{tabular}[c]{@{}l@{}}NLP\end{tabular}  & \begin{tabular}[c]{@{}l@{}}The paper reviews Transformer-based models used for NLP tasks in\\ emotion recognition, highlighting their strengths and limitations, and\\ providing future research directions.\end{tabular}    \\ \hline
\begin{tabular}[c]{@{}l@{}}Khan et al. \\2022 \cite{khan2022transformers}\end{tabular}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark}  & \begin{tabular}[c]{@{}l@{}}CV\end{tabular}   & \begin{tabular}[c]{@{}l@{}}The paper reviews Transformer models in CV tasks, covering a wide \\range of tasks and comparing the advantages and limitations of popular \\techniques. It also discusses research directions and future works.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Tay et al.\\ 2022 \cite{tay2022efficient}\end{tabular} & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark}  & Multimodal & \begin{tabular}[c]{@{}l@{}}The article provides an overview of "X-former" models in multiple \\domains, aimed at improving efficiency and helping researchers \\navigate the evolving field.\end{tabular}    \\ \hline
\begin{tabular}[c]{@{}l@{}}Aleissaee et al. \\2022 \cite{aleissaee2022transformers}\end{tabular}    & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark}  & Remote Sensing   & \begin{tabular}[c]{@{}l@{}}The paper reviews transformers-based methods for remote sensing\\ problems. It also discusses different challenges and open issues.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Ulhaq et al. \\2022 \cite{ulhaq2022vision}\end{tabular}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark}  & AR & \begin{tabular}[c]{@{}l@{}}It reviews literature on vision transformer techniques for action \\recognition, providing taxonomies, network learning strategies, \\and evaluation metrics, while discussing challenges and future \\research directions.\end{tabular}  \\ \hline
\begin{tabular}[c]{@{}l@{}}Bhangale et al. \\2022 \cite{bhangale2022survey}\end{tabular}  & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}   & \begin{tabular}[c]{@{}l@{}}SP\end{tabular} & \begin{tabular}[c]{@{}l@{}}The paper presents a survey of deep learning techniques for various\\ speech processing applications. It covers various deep learning models\\ such as Auto-Encoder, GAN, RBN, DBN, DNN, CNN, RNN, and \\DRL, along with speech databases and evaluation metrics.\end{tabular}  
  \\ \hline
\begin{tabular}[c]{@{}l@{}}Lahoud et al. \\2023 \cite{lahoud20223d}\end{tabular}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark}  & 3D Vision  & \begin{tabular}[c]{@{}l@{}}It reviews over 100 transformer methods on various 3D CV tasks,\\ comparing their performance to common non-transformer methods\\ on 3D benchmarks. It also discusses open directions and challenges.\end{tabular}

\\ \hline
\begin{tabular}[c]{@{}l@{}}Li et al. \\2023 \cite{li2023survey}\end{tabular}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark}  & \begin{tabular}[c]{@{}l@{}}RL\end{tabular}  & \begin{tabular}[c]{@{}l@{}}The paper reviews recent advances and applications of transformers in\\ reinforcement learning, providing a taxonomy of existing works in the\\ field and summarizing future prospects.\end{tabular}  \\ \hline
This paper & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{\checkmark}  & \begin{tabular}[c]{@{}l@{}} SP\end{tabular} & \begin{tabular}[c]{@{}l@{}}
The paper reviews applications of transformers and challenges faced \\in various speech processing applications, like speech recognition, \\synthesis, translation, and enhancement, and suggests future research\\ directions for improving speech technology with transformers.

\end{tabular} \\ \hline
%The survey paper covers the use of transformers in various speech\\ processing applications, including automatic speech recognition, speech\\ synthesis, speech translation, speech paralinguistics, speech enhancement,\\ spoken dialogue systems, and multimodal applications, while also\\ discussing the challenges faced by transformers in speech processing and\\ proposing future research directions, serving as a valuable resource for\\ researchers interested in utilizing transformers in speech technology.
\end{tabular}
% \bigskip
% \vspace{-2mm}
% \small\textsubscript{Speech Processing = SP, Speech Emotion Recognition (SER),Natural Language Processing = NLP, Computer Vision = CV}, \vspace{-0.1mm}\small\textsubscript{Action Recognition = AR, Reinforcement Learning = RL}
\label{table:lit-review}
\end{table*}

\section{Background} \label{sec:background}

In this section, we will provide a comprehensive overview of transformer architecture, starting with a brief overview of sequential models and their limitations in handling sequential data. We will then delve into the key concepts behind the transformer's operation, highlighting the unique features that enable it to outperform traditional recurrent neural networks. Lastly, we will discuss popular transformers for speech processing. 



% will provide a summary of the use of transformers in the SP domain, highlighting some of their most successful applications.

\subsection{Sequential Models for Speech Processing} 

%Early deep learning based approaches in the SP domain were generally variants of convolutional neural networks (CNNs)~\cite{abdel2014convolutional,zhang2017towards}. One of the inherent drawbacks of these CNN-based approaches is the failure to capture the sequential nature of the speech data. These properties of the speech data gave rise to the seq2seq architectures that are specifically designed for sequential data.
%These architectures include RNNs and long short-term memory networks (LSTMs). RNNs are well-suited to deal with sequential data since long sequences can be processed step-by-step with limited memory of previous sequence elements. Recently, the complementary strengths of the RNN and CNN are also combined by exploiting the convolutional layers for extracting the audio features and using these features as input to train RNN. However, RNNs have been shown to suffer from vanishing/exploding gradient issues. To handle this issue, LSTM utilizes a gating mechanism and memory cells to mitigate the information flow and alleviate gradient problems. A variety of LSTM networks, such as Frequency-LSTM, Time-Frequency LSTMs, Bi-directional LSTMs, ConvLSTMs, and Stacked LSTMs, have been proposed for SP tasks. Although effective, these S2S models have difficulties in modeling long-term contexts. Furthermore, as S2S models process the input sequentially, i.e., frame by frame, the utilization of parallel computer hardware is limited.

Early deep learning approaches in the SP domain typically employed variants of convolutional neural networks (CNNs)~\cite{abdel2014convolutional,zhang2017towards}. However, a drawback of these CNN-based approaches is their inability to capture the sequential nature of speech data. This limitation of CNNs led to the development of sequence-to-sequence (seq2seq) architectures, such as RNNs and long short-term memory networks (LSTMs), which are specifically designed for sequential data. RNNs are well-suited for sequential data because they can process long sequences step-by-step with limited memory of previous sequence elements. More recently, researchers have also combined the strengths of CNNs and RNNs by using CNNs to extract audio features and using these features as input to train RNNs. However, RNNs have been shown to have issues with the vanishing or exploding gradient problem. To address this issue, LSTMs use a gating mechanism and memory cells to control the flow of information and alleviate gradient problems. Many LSTM variations---such as Frequency-LSTM, Time-Frequency LSTMs, Bi-directional LSTMs, ConvLSTMs, and Stacked LSTMs---have been proposed for SP tasks. Despite their effectiveness, seq2seq models are limited in important ways: they cannot take advantage of parallel computing hardware and have difficulties modeling long-term context. 


%RNNs and LSTMs have been generally proposed for the sequence of fixed length. {\color{red}Sequence-to-sequence (Seq2Seq) models were motivated due to problems requiring sequences of unknown lengths [41]. In a Seq2Seq model, while one RNN reads the inputs in order to generate a vector representation (the encoder), another RNN inherits those learned features to generate the outputs (the decoder). The neural architectures can be single or multilayer, unidirectional or bidirectional, and they can combine multiple architectures using end-to-end learning by optimizing a joint objective instead of independent ones. Seq2Seq models have been gaining much popularity in the speech community due to their capability of transducing input to output sequences. DL frameworks are particularly suitable for this direct translation task due to their large model capacity and their capability to train in an end-to-end mannerâ€”to directly map the input signals to the target sequences. Various Seq2Seq models have been explored in the speech, audio, and language processing literature including Recurrent Neural Network Transducer (RNNT), Monotonic Alignments, Listen, Attend and Spell (LAS), Neural Transducer, Recurrent Neural Aligner (RNA), and transformer Networks, among others.}

\subsection{Overview of Transformers} 

Transformers were first introduced in the seminal work of Vaswani et al. titled ``Attention Is All You Need'' \cite{vaswani2017attention} for machine translation tasks in Natural Language Processing (NLP) and have recently shown impressive performance in other domains, including computer vision, medical imaging, and remote sensing. Transformer models use self-attention layers to effectively capture long-range dependencies among the input sequence, which is in contrast to traditional recurrent neural networks that struggle to capture such interaction. Furthermore, self-attention allows for more parallelization compared to recurrent neural networks, as these can process the speech sequence as a whole without relying on past states to capture dependencies. More specifically, two types of attention were introduced by Vaswani et al. including (1) scaled dot-product attention, and (2) multi-head attention. In addition, positional encoding is also used to inject information about the relative or absolute position of the tokens in the sequence. Due to these desirable properties, transformers have garnered immense interest in the speech community, and several approaches have been proposed that build upon transformers. We provide next a brief overview of the core components of transformers, which are multi-head self-attention layers and a position-wise feed-forward network (positional encoding). 
%Like the models invented before it, the Transformer is an encoder-decoder architecture. The encoder consists of a set of encoding layers that processes the input iteratively one layer after another and the decoder consists of a set of decoding layers that does the same thing to the output of the encoder. The function of each encoder layer is to process its input to generate encodings, containing information about which parts of the inputs are relevant to each other. It passes its set of encodings to the next encoder layer as inputs. Each decoder layer does the opposite, taking all the encodings and processes them, using their incorporated contextual information to generate an output sequence. To achieve this, each encoder and decoder layer makes use of an attention mechanism, which for each input, weighs the relevance of every other input and draws information from them accordingly to produce the output. Each decoder layer also has an additional attention mechanism that draws information from the outputs of previous decoders, before the decoder layer draws information from the encodings. Both the encoder and decoder layers have a feed-forward neural network for additional processing of the outputs and contain residual connections and layer normalization steps.


% \begin{figure*}[!t]
% \centering
% \includegraphics[width=\textwidth]{figures/Sections-in-Paper.pdf}
% \caption{Illustration of this paper's organization.}
% \label{fig:taxonomy}
% \end{figure*}


\vspace{2mm}
\subsubsection{Self-Attention Layer}

Self-attention (SA) layer aims to capture the internal correlation of sequence or features by aggregating global information from the entire input sequence, a task that is challenging for conventional recurrent models. 
Specifically, the input $\boldsymbol{X} \in \mathbb{R}^{N \times D}$ consisting of $N$ entities each having dimension $D$ is transformed into query ($\boldsymbol{Q} \in \mathbb{R}^{N \times D_k}$), key ($\boldsymbol{K} \in \mathbb{R}^{N \times D_k}$) and value ($\boldsymbol{V} \in \mathbb{R}^{N \times D_k}$) matrices via learnable weight matrices $\boldsymbol{W}^{Q} \in \mathbb{R}^{D \times D_k}$, $\boldsymbol{W}^{K} \in \mathbb{R}^{D \times D_k}$, and $\boldsymbol{W}^{V} \in \mathbb{R}^{D \times D_k}$ respectively. Mathematically,
\begin{align}
 \boldsymbol{Q} = \boldsymbol{X}\boldsymbol{W}^{Q}, \quad
 \boldsymbol{K} = \boldsymbol{X}\boldsymbol{W}^{K}, \quad
 \boldsymbol{V} = \boldsymbol{X}\boldsymbol{W}^{V}.
\end{align}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{figures/transformer_arch.pdf}
\caption{Architecture of the standard transformer (Adapted from Vaswani et al., 2017 \cite{vaswani2017attention} and Tay et al., 2020 \cite{tay2022efficient}). \textit{The model comprises encoder and decoder layers, each with stacked self-attention and feed-forward sub-layers. The encoder produces hidden states from an input token sequence, while the decoder generates predictions from an output token sequence and attends to the encoder's states for input information.}}
\label{fig:lc}
\end{figure*}

Then the dot-product of the query matrix $\boldsymbol{Q}$ with all the keys $\boldsymbol{Q}$ in a given sequence is computed and the resulting matrix is normalized using the softmax operator to get the attention matrix $\boldsymbol{A} \in \mathbb{R}^{N \times N}$ as
\begin{equation}
 \boldsymbol{A} = \textit{softmax} \left( \frac{\boldsymbol{QK}^{T}}{\sqrt{D_k}}\right)\boldsymbol{V}.
\end{equation}

The output of the SA layer $\boldsymbol{Z}$ is the attention matrix $\boldsymbol{A}$ multiplied by the value matrix $\boldsymbol{V}$
\begin{equation}
 \boldsymbol{Z} = \boldsymbol{AV}.
\end{equation}




\vspace{2mm}
\subsubsection{Masked Self-Attention} In the original transformers paper \cite{vaswani2017attention}, the SA blocks used in the decoder are masked to prevent attending to the subsequent future entities by element-wise multiplication of the mask $\boldsymbol{M} \in \mathbb{R}^{N \times N}$ as: 
\begin{equation}
 \boldsymbol{Z} = \textit{softmax} \left( \frac{\boldsymbol{QK}^{T}}{\sqrt{d_k}} \circ \boldsymbol{M}\right)\boldsymbol{V},
\end{equation}
where $\boldsymbol{M}$ is the upper triangular matrix and $\circ$ denotes the Hadamard product. This is called masked self-attention.


\vspace{2mm}
\subsubsection{Multi-Head Attention}
Rather than only computing the attention once, Multi-Head Self-Attention (MHSA) consists of multiple SA blocks. These SA blocks are concatenated together channel-wise to model dependencies among different elements in the input sequence. Each head in MHSA has its own learnable weight matrices denoted by $\{\mathbf{W}^{Q_i},\mathbf{W}^{K_i},\mathbf{W}^{V_i} \}$, where $i=0 \cdots (h{-}1)$ and $h$ denotes total number of heads in MHSA block. Specifically,
$$\text{MHSA}(\mathbf{Q},\mathbf{K},\mathbf{V}) = [\mathbf{Z}_0,\mathbf{Z}_1,...,\mathbf{Z}_{h-1}]\mathbf{W}^{O},$$
whereas $\mathbf{W}^{O} \in \mathbb{R}^{h.D_k \times N}$ computes linear transformation of heads and $\mathbf{Z}_i$ can be written as,
$$ \mathbf{Z}_i = \textit{softmax}\left (\frac{\mathbf{QW}^{Q_i}(\mathbf{KW}^{K_i})^{T}}{\sqrt{D_k/h}}\right ) \mathbf{VW}^{V_i}.$$

\iffalse
\begin{figure}[!t]
\centering
\includegraphics[width=0.3\textwidth]{figures/multi_head.png}
\caption{The architecture of the multi-head self-attention block of the transformers where the self-attention layer has been replicated \textit{h} times.}
\label{fig:multihead}
\end{figure}
\fi


\vspace{2mm}
\subsubsection{Positional Encoding}

The self-attention mechanism in transformer models allows for input speech frames to be processed in no particular order or position. To account for this, positional encoding is used to provide the transformer model with information about the order of the input sequence. This is done by associating each position in the input sequence with a vector that helps the transformer learn positional relationships. Positional encoding can be learned during training or pre-defined and can be encoded in relative or absolute ways for SP tasks.



\subsection{Popular Transformers for Speech} 


\begin{comment}
\begin{table}[]
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Model Name} & \textbf{Year} & \textbf{Parameters} & \textbf{Tasks} \\ \hline
Tacotron 2 \cite{shen2018natural} & 2017  & 28.2 million  & Text-to-speech (TTS) \\ \hline
vq-wav2vec \cite{baevski2020vq}   & 2019  & 6.5 million & \begin{tabular}[c]{@{}l@{}}Speech representation \\ learning (SRL)\end{tabular}  \\ \hline
Mockingjay \cite{liu2019mockingjay} & 2019  & 85 million  & \begin{tabular}[c]{@{}l@{}}Speech representation\\ learning (SRL)\end{tabular}   \\ \hline
Conformer \cite{gulati2020conformer} & 2020  & \begin{tabular}[c]{@{}l@{}}43 million\end{tabular}   & \begin{tabular}[c]{@{}l@{}}Automatic speech\\ recognition (ASR)\end{tabular} \\ \hline
w2v Conformer  & 2020  & 98 million  & \begin{tabular}[c]{@{}l@{}}Automatic speech\\ recognition (ASR)\end{tabular} \\ \hline
FastSpeech 2 \cite{ren2020fastspeech} & 2020  & 29 million  & Text-to-speech (TTS) \\ \hline
wav2vec 2.0 \cite{baevski2020wav2vec} & 2020  & 95 million  & \begin{tabular}[c]{@{}l@{}}Automatic speech\\ recognition (ASR)\end{tabular} \\ \hline
DeCoAR 2.0 \cite{chang2020decoar} & 2020  & 95 million  & \begin{tabular}[c]{@{}l@{}}Speech representation\\ learning (SRL)\end{tabular}   \\ \hline
FastPitch \cite{lachowicz2020fastpitch} & 2020  & 9.4 million & Text-to-speech (TTS) \\ \hline
w2v BERT \cite{wang2019bridging}  & 2020  & 110 million & \begin{tabular}[c]{@{}l@{}}Automatic speech\\ recognition (ASR)\end{tabular} \\ \hline
UniSpeech \cite{chen2021unspeech} & 2021  & 110 million & \begin{tabular}[c]{@{}l@{}}Speech representation\\ learning (SRL)\end{tabular}   \\ \hline
HuBERT \cite{hsu2021hubert}  & 2021  & \begin{tabular}[c]{@{}l@{}}95 million\end{tabular}   & \begin{tabular}[c]{@{}l@{}}Speech representation\\ learning (SRL)\end{tabular}   \\ \hline
BigSSL \cite{hu2021bigssl} & 2021  & 95 million  & \begin{tabular}[c]{@{}l@{}}Automatic speech\\ recognition (ASR)\end{tabular} \\ \hline
XLS-R \cite{conneau2020unsupervised}  & 2021  & \begin{tabular}[c]{@{}l@{}}3.4 billion\end{tabular} & \begin{tabular}[c]{@{}l@{}}Automatic speech\\ recognition (ASR)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}UniSpeech-SAT\\  \cite{liu2021unispeechsat}\end{tabular} & 2021  & 110 million & \begin{tabular}[c]{@{}l@{}}ASR / \\ Speaker verification / \\ Speech translation /\\ Text-to-speech (TTS) \end{tabular} \\ \hline
WavLM \cite{liu2021wavlm} & 2021  & 285 million & \begin{tabular}[c]{@{}l@{}}Automatic speech\\ recognition (ASR)\end{tabular} \\ \hline
DeltaLM \cite{liu2021deltalm} & 2021  & \begin{tabular}[c]{@{}l@{}}560 million\end{tabular} & Speech Generation  \\ \hline
data2vec \cite{baevski2022data2vec} & 2022  & 95 million  & \begin{tabular}[c]{@{}l@{}}Predict latent\\ representations\end{tabular}  \\ \hline
data2vec 2.0 \cite{baevski2022efficient}  & 2022  & 85 million  & \begin{tabular}[c]{@{}l@{}}Image Classification /\\ Speech Recognition\end{tabular}  \\ \hline
Whisper AI \cite{radford2022robust}   & 2022  & 1.6 billion & \begin{tabular}[c]{@{}l@{}}Automatic speech\\ recognition (ASR)\end{tabular} \\ \hline
XDoc \cite{chen-etal-2022-xdoc} & 2022  & 500 million & \begin{tabular}[c]{@{}l@{}}Sentiment Analysis / \\ Document classification /\\ Form understanding\end{tabular}  \\ \hline
VALL-E \cite{chen2022valle} & 2023  & 30 million  & Text-to-speech (TTS) \\ \hline
\end{tabular}
\caption{Overview of Speech Transformer Models}
\label{table:sds-transformers}
\end{table}
\end{comment}

% \subsection{Transformers} 
Transformers are a novel neural network architecture that relies solely on attention mechanisms to handle sequential data, such as natural language and speech, for various tasks. Since the seminal work of Vaswani et al. (2017) \cite{vaswani2017attention}, many extensions and applications of transformers have been developed for natural language processing (NLP) tasks, such as language modeling, question answering, sentiment analysis, text generation, etc. Transformers are also becoming increasingly popular in the speech community due to their suitability for various tasks including speech recognition, enhancement, text-to-speech synthesis, speaker recognition, and multi-microphone processing. Various open-source libraries including Hugging Face, SpeechBrain, and torch audio are accelerating the research in the speech domain. Big tech companies like Google, Meta, Amazon, etc., are building large speech domain-related transformer models. 
% We discuss some popular speech-related transformer models next. % are following the trend in NLP and are building large speech transformer models. % . This trend has been further accelerated by the availability of specialized libraries developed specifically for transformer-based SP tasks, such as  These libraries include Hugging Face, SpeechBrain, and Touch Audio. Big tech companies like Google, Meta, and Amazon are following the trend in NLP and are building large speech transformer models. These models use a two-stage approach to train transformers on large-scale unlabeled text data first and then fine-tune them on specific downstream tasks later. This allows them to leverage the rich semantic and syntactic information encoded in the pre-trained representations and achieve state-of-the-art results on various NLP benchmarks \cite{kumar2020data},\cite{lan2019albert},\cite{radford2019language}.
While transformers were initially developed for NLP tasks, they have since been adapted for other data types, including speech. BERT \cite{devlin2018bert} is a language model that uses masked-language modeling (MLM) as its pre-training objective. BERT consists of two modules: an embedding layer that maps tokens to vectors, and an encoder layer that applies self-attention and feed-forward networks to learn contextualized representations. While BERT and similar text-based large language models (e.g., GPT \cite{radford2018improving}), XLNet \cite{yang2019xlnet}, T5 \cite{raffel2020exploring}), etc.) have been successful in various NLP tasks, their application to speech processing is limited due to several shortcomings. For instance, they require discrete tokens as input, which means it needs a tokenizer or a speech recognition system to convert raw audio signals into text, introducing errors and noise in the process that can negatively impact the performance \cite{wu2020sentence}. Additionally, these models are pre-trained on large-scale text corpora, which may not match the domain or style of speech data, leading to domain mismatch issues. 

% BERT's reliance on self-attention mechanisms also imposes high computational and memory costs, limiting its scalability and efficiency for speech processing tasks that require real-time or low-latency processing.

\subsubsection{wav2vec}
To overcome these limitations, dedicated frameworks for learning speech representations, such as wav2vec, have been developed. wav2vec uses a self-supervised training approach that leverages the contrastive predictive coding (CPC) loss function to learn speech representations without the need for transcription or segmentation \cite{baevski2020vq,baevski2020wav2vec}. This approach allows wav2vec to achieve state-of-the-art performance on several speech processing tasks, including speech recognition, speaker recognition, and spoken language understanding, among others \cite{baevski2020vq,baevski2020wav2vec}. % Notably, wav2vec has been shown to outperform BERT on several speech processing tasks \cite{chung2021w2v}. In fact, the success of BERT in natural language processing tasks directly inspired the development of wav2vec via a framework called w2v-BERT \cite{chung2021w2v}. wav2vec is a framework that learns speech representations by maximizing agreement between differently augmented views of the same speech segment. wav2vec consists of two modules: a feature encoder that maps raw audio to latent representations, and a contrastive module that predicts a representation from another view using a contrastive loss.
w2v-BERT is a framework that combines contrastive learning and MLM for self-supervised speech pre-training and builds on the success of wav2vec. w2v-BERT consists of three modules: a feature encoder, a quantization module, and a masked prediction module. The feature encoder is similar to wav2vec, but the quantization module discretizes the continuous speech representations into a finite set of speech units using Gumbel-softmax. 
% The masked prediction module randomly masks some speech units and asks the model to predict them based on the context using cross-entropy loss. 
The main differences between wav2vec and w2v-BERT lie in their pre-training objectives and the data types they operate on. wav2vec focuses on raw audio signals and uses contrastive learning to learn speech representations. In contrast, w2v-BERT operates on discrete tokens and it uses both MLM and contrastive learning as pre-training objectives. 
% Additionally, w2v-BERT has a quantization module that discretizes continuous speech representations into a finite set of speech units using Gumbel-softmax, as well as a masked prediction module for predicting masked speech units based on context. BERT only has a masked prediction module, while wav2vec does not have either a quantization or masked prediction module.
% \subsubsection{XLSR}
 After the success of the wav2vec models, Baevski et al. demonstrated that it could be fine-tuned with a small amount of labeled data to achieve state-of-the-art results on speech recognition tasks \cite{pepino2021emotion,novoselov2022robust}. This breakthrough led to the development of a series of models aimed at building cross-lingual or multilingual speech recognition systems using pre-trained transformer models. 
 % One such model is XLSR \cite{conneau-etal-2020-unsupervised}, a transformer-based masked language model that is pre-trained on one hundred languages using over two terabytes of filtered CommonCrawl data. XLSR outperforms multilingual BERT (mBERT) on various cross-lingual transfer tasks, such as XNLI, MLQA, and NER. 
Another model is XLS-R \cite{babu2021xls}, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. XLS-R is trained on nearly half a million hours of publicly available speech audio in 128 languages and achieves state-of-the-art results on a wide range of tasks, domains, data regimes, and languages. These models leverage large-scale multilingual data augmentation and contrastive learning techniques to learn universal speech representations that can be transferred across languages and domains.

\subsubsection{data2vec}

While the success of wav2vec was inspired by the achievements of BERT, it paved the way for the development of other dedicated frameworks that use transformers to learn representations from multi-modal data. One such example is data2vec, which aims to learn multi-modal representations of data, including speech, images, and text, using a contrastive learning objective \cite{baevski2022data2vec}. Similar to wav2vec, data2vec uses a self-supervised training approach that does not require labels or annotations and learns representations by maximizing agreement between differently augmented views of the same data sample. However, unlike wav2vec, which focuses solely on speech signals, data2vec can operate on various types of data and can learn joint representations that capture cross-modal correlations and transfer knowledge across modalities. 
% data2vec is one such framework that leverages self-supervised learning to learn representations for multimodal data. data2vec learns representations by training a neural network to predict the masked words in a sentence, similar to BERT's MLM objective. However, instead of using discrete tokens, data2vec uses continuous vectors as input, which allows it to operate on multiple modalities, such as audio, images, and text, simultaneously \cite{baevski2022data2vec}. 
Data2vec's self-supervised training approach allows it to learn representations without the need for labeled data, making it a scalable and cost-effective solution for many applications. Data2vec has been shown to outperform other unsupervised approaches for learning multimodal representations, such as Skip-thought \cite{kiros2015skip} and Paragraph Vector \cite{le2014distributed}, on several benchmark datasets \cite{baevski2022data2vec}. However, it should be noted that while data2vec is suitable for learning representations that generalize across domains and modalities, it may not perform as well as domain-specific models for certain tasks, such as speech recognition or speaker identification, where the data has a specific domain or language.

\subsubsection{Whisper}

Whisper \cite{radford2022robust} is a general-purpose model designed for speech recognition in noisy or low-resource settings, and is capable of performing multiple speech-related tasks.  Whisper uses weak supervision and a minimalist approach to data pre-processing. It achieves state-of-the-art results, showcasing the potential of using advanced machine-learning techniques in speech processing.
% Whisper is a speech recognition model developed by OpenAI that is trained using large-scale weak supervision from unlabeled data. The model is designed to solve the problem of robust speech recognition in noisy or low-resource settings by leveraging the power of web-scale weak supervision to train models with improved performance. 
Whisper is capable of performing multilingual speech recognition, speech translation, and language identification. It is trained on a large dataset of diverse audio and is a multitasking model that can handle various speech-related tasks, such as transcription, voice assistants, education, entertainment, and accessibility. 
% The model is designed to be flexible and can be applied to various domains and applications, making it a versatile tool for speech processing. 
The Whisper model is unique in that it uses a minimalist approach to data pre-processing, allowing models to predict the raw text of transcripts without significant standardization. This eliminates the need for a separate inverse text normalization step to produce naturalistic transcriptions, simplifying the speech recognition pipeline. The resulting models can generalize well to standard benchmarks and are competitive with prior fully supervised results without fine-tuning. 

% One of the advantages of the Whisper model is that it can achieve high-quality results on specific distributions without the need for dataset-specific fine-tuning. It approaches human accuracy and robustness, but there are still challenges to be addressed, such as the problem of formatting differences in transcript style that can affect the word error rate metric.

\subsubsection{Tacotron}

Transformer-based models have also gained popularity in speech synthesis tasks. A prime example of such a model is Tacotron \cite{wang2017tacotron}, which uses a sequence-to-sequence architecture with attention mechanisms to generate high-quality speech from text input. The limitations of the Griffin-Lim algorithm used for audio signal generation led to the development of Tacotron 2 \cite{shen2018natural} by Google AI in 2018, which used WaveNet to generate raw audio waveforms directly from mel-spectrograms, resulting in more natural-sounding speech. Furthermore, Microsoft introduced Transformer TTS \cite{li2019neural} in 2019, which employs a transformer network instead of the convolutional and recurrent networks used in Tacotron 2, along with a duration predictor and a new training method that uses teacher forcing for faster convergence and better performance. Despite these advancements, current systems still have limitations in generating natural-sounding speech for non-English languages, handling complex intonations and accents, and real-time speech synthesis for applications such as voice assistants and automated phone systems.

%The success of transformer-based models in Natural Language Processing (NLP) has paved the way for their application in speech-processing tasks. There has been a surge of interest in developing transformer-based models for speech recognition and synthesis. Tacotron \cite{wang2017tacotron} is a popular model for speech synthesis. It was introduced in 2017 by Google AI as a sequence-to-sequence architecture with attention mechanisms that generate high-quality speech from text input. This model used a combination of a convolutional network and a recurrent network to generate mel-spectrograms, which were then converted into audio signals using a Griffin-Lim algorithm. However, the Griffin-Lim algorithm had limitations in generating high-quality audio, leading to unnatural-sounding speech. To address this limitation, Tacotron 2 \cite{shen2018natural} was developed by Google AI in 2018, which improved upon the original model by replacing the Griffin-Lim algorithm with a neural vocoder known as WaveNet. WaveNet uses a deep neural network to generate raw audio waveforms directly from the mel-spectrograms, resulting in more natural-sounding speech. Tacotron 2 also introduced a new feature called ``guided attention,'' which aligns the attention mechanism to the audio waveform during training to improve the accuracy of the alignment between the audio and text inputs.

%Following the success of Tacotron 2, Microsoft introduced Transformer TTS \cite{li2019neural} in 2019, which uses a transformer network instead of the convolutional and recurrent networks used in Tacotron 2. Transformer TTS also introduced a novel duration predictor to predict the duration of each phoneme in the text input, which helps improve the alignment between the audio and text inputs. Additionally, the authors proposed a new training method that uses teacher forcing during training, resulting in faster convergence and better overall performance. Despite these improvements in transformers-based TTS, there are still limitations in the current state-of-the-art speech synthesis systems. For instance, current systems struggle with producing natural-sounding speech for languages other than English, as well as in handling complex intonations and accents. Additionally, current systems are limited in their ability to generate speech in real time, which is crucial for applications such as voice assistants and automated phone systems. 


\begin{table*}[!ht]
\begin{center}
\scriptsize
\caption{Transformer Speech Models: Release Year and Parameter Count with Task Compatibility. * means that the parameters for this model cannot be found or are proprietary.}
% \small\textsubscript{} 
\begin{tabular}{|l|l|l|lll|l|}
\hline
\multirow{2}{*}{\textbf{Model Name}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Release\\ Year\end{tabular}}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Number of\\ Parameters\end{tabular}}}} & \multicolumn{3}{c|}{\textbf{Tasks}}                                                                                                                                                                                                                                                                           & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Multimodal}}} \\ \cline{4-6}
                                     & \multicolumn{1}{c|}{}                                                                                 & \multicolumn{1}{c|}{}                                                                                         & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Speech\\ Synthesis (TTS)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Speech\\ Translation (ST)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Automatic Speech\\ Recognition (ASR)\end{tabular}}} & \multicolumn{1}{c|}{}                                     \\ \hline
Tacotron \cite{wang2017tacotron}    & 2017  &  13 million \cite{wang2019deep} & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}   \\ \hline
Tacotron 2 \cite{shen2018natural}   & 2017  & 28.2 million \cite{beliaev2021talknet}  & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}    \\ \hline
Transformer-TTS \cite{li2019neural}    & 2018  & 30.7 million \cite{ren2019fastspeech}  & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$} \\ \hline
vq-wav2vec \cite{baevski2020vq} & 2019  &  34 million\cite{bgn2021timeline} & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{$\times$} \\ \hline
Mockingjay \cite{liu2019mockingjay} & 2019  &  85 million\cite{bgn2021timeline} & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{$\times$} \\ \hline
FastSpeech  \cite{ren2019fastspeech} & 2019  &  23 million \cite{ren2020fastspeech} & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}    \\ \hline
wav2vec \cite{schneider2019wav2vec}  & 2019  & 16 million \cite{chen2022speechformer}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{$\times$}    \\ \hline
wav2vec 2.0 \cite{baevski2020wav2vec} & 2020  & 317 million\cite{bgn2021timeline}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{$\times$}    \\ \hline
FastSpeech 2 \cite{ren2020fastspeech} & 2020  & 27 million \cite{ren2020fastspeech}  & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}    \\ \hline
FastPitch \cite{lachowicz2020fastpitch}   & 2020  &  26.8 million & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$} \\ \hline
Conformer \cite{gulati2020conformer}  & 2020  & 1 billion\cite{bgn2021timeline}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\checkmark$} & \multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{$\times$}    \\ \hline
DeCoAR 2.0 \cite{chang2020decoar}   & 2020  & 317 million\cite{bgn2021timeline}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{$\times$} \\ \hline
w2v-Conformer  & 2021  &  1 billion\cite{bgn2021timeline} & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{$\times$}    \\ \hline
w2v-BERT \cite{wang2019bridging}    & 2021  & 1 billion\cite{bgn2021timeline}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{$\times$} \\ \hline
HuBERT \cite{hsu2021hubert}& 2021  & 317 million\cite{bgn2021timeline}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{$\times$}   \\ \hline
XLS-R \cite{babu2021xls}& 2021  & 2 billion\cite{bgn2021timeline}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{$\times$}    \\ \hline
UniSpeech \cite{chen2021unspeech}   & 2021  & 317 million\cite{bgn2021timeline}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{$\times$} \\ \hline
UniSpeech-SAT  \cite{liu2021unispeechsat} & 2021  &  317 million\cite{bgn2021timeline}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{$\times$} \\ \hline
BigSSL \cite{hu2021bigssl} & 2021  & 8 billion\cite{bgn2021timeline}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{$\times$} \\ \hline
WavLM \cite{liu2021wavlm}  & 2021  & 317 million\cite{bgn2021timeline}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark}  &\multicolumn{1}{c|}{$\times$} \\ \hline
DeltaLM \cite{liu2021deltalm} & 2021  &  360 million \cite{ma2021deltalm} & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{$\times$} & \multicolumn{1}{c|}{$\checkmark$} \\ \hline
SpeechT5 \cite{ao2021speecht5}& 2021  & \multicolumn{1}{c|}{11 billion \cite{ao2021speecht5}}  & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{$\checkmark$}   \\ \hline
data2vec \cite{baevski2022data2vec} & 2022  & \multicolumn{1}{c|}{*}  & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{$\checkmark$}    \\ \hline
data2vec 2.0 \cite{baevski2022efficient}  & 2022  &  \multicolumn{1}{c|}{*} & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{$\checkmark$}    \\ \hline
SpeechFormer \cite{chen2022speechformer}    & 2022  &  \multicolumn{1}{c|}{3.5 million \cite{chen2022speechformer}} & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{$\times$} & \multicolumn{1}{c|}{$\checkmark$}  & \multicolumn{1}{c|}{\checkmark}   \\ \hline
Whisper \cite{radford2022robust}    & 2022  &  \multicolumn{1}{c|}{1.6 billion \cite{openai2022whisper}} & \multicolumn{1}{c|}{$\times$}    & \multicolumn{1}{c|}{\checkmark} & \multicolumn{1}{c|}{\checkmark}  & \multicolumn{1}{c|}{$\times$}   \\ \hline
VALL-E \cite{chen2022valle}& 2023  & \multicolumn{1}{c|}{*}  & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}    \\ \hline
VALL-E X \cite{zhang2023speak}& 2023  &  \multicolumn{1}{c|}{*} & \multicolumn{1}{c|}{\checkmark}   & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}  & \multicolumn{1}{c|}{$\times$}    \\ \hline
\end{tabular}
\label{table:transformer-models}
\end{center}
\end{table*}


\subsubsection{VALL-E}
VALL-E \cite{chen2022valle} is another model that has gained attention, a zero-shot text-to-speech synthesis system that uses a language modeling approach, treating TTS as a conditional language modeling task rather than continuous signal regression. It is trained using discrete codes from an off-the-shelf neural audio codec model and pre-trained on 60,000 hours of English speech data, providing strong in-context learning capabilities. Unlike previous TTS systems, VALL-E does not require additional structure engineering, pre-designed acoustic features, or fine-tuning. It can synthesize high-quality personalized speech with only a 3-second acoustic prompt from an unseen speaker. The model also provides diverse outputs with the same input text and can preserve the acoustic environment and the speaker's emotion of the acoustic prompt. VALL-E's speaker dimension is built on a generalized TTS system, leveraging a large amount of semi-supervised data. This approach is significant, as scaling up semi-supervised data has been underestimated for TTS. Evaluation results show that VALL-E significantly outperforms the state-of-the-art zero-shot TTS system on LibriSpeech and VCTK datasets in terms of speech naturalness and speaker similarity. VALL-E X \cite{zhang2023speak} was developed as a natural extension of VALL-E to address the challenge of cross-lingual speech synthesis. Cross-lingual speech synthesis involves generating speech in a target language, using a source language speech prompt and a target language text prompt. While VALL-E was designed for zero-shot TTS in English, there was a need for a model that could handle cross-lingual speech synthesis in multiple languages. VALL-E X, therefore, extends VALL-E to support cross-lingual synthesis by training a multi-lingual model to predict acoustic token sequences in the target language using prompts in both source and target languages. This enables the model to generate high-quality speech in the target language while preserving the voice, emotion, and acoustic environment of the unseen speaker and effectively alleviates the foreign accent problem, which can be controlled by a language ID.


% However, ongoing research in the Tacotron family and other transformer-based architectures continues to push the boundaries of speech synthesis and recognition, with the potential to revolutionize the field in the coming years.

% In addition to transformer-based architectures such as Tacotron and Transformer TTS, 

\subsubsection{Conformer}
Recent advances in transformers for speech processing have also seen the emergence of the Conformer models \cite{gulati2020conformer}. The Conformer architecture combines convolutional and transformer layers, enabling it to capture both local and global context information. This makes Conformer models well-suited for speech-processing tasks such as speech recognition and speaker identification, where capturing long-range dependencies is crucial. Conformer achieved state-of-the-art performance on benchmarks such as LibriSpeech and AISHELL-1. However, previous limitations in speech synthesis and recognition, such as the struggle to produce natural-sounding speech in languages other than English and generate speech in real-time, remained. In response, Wang et al. \cite{zhang2020pushing} presented an ASR model that uses a combination of noisy student training with SpecAugment and giant Conformer models pre-trained using the wav2vec 2.0 pre-training method on the Libri-Light dataset. They achieved state-of-the-art word error rates on the LibriSpeech dataset. In 2021, Wang et al. \cite{wang2022conformer} extended Conformer and developed Conformer-LHUC, which utilized learning hidden unit contribution (LHUC) for speaker adaptation. Conformer-LHUC showed superior performance in elderly speech recognition and has potential implications for clinical diagnosis and treatment of Alzheimer's disease. 

\subsubsection{UniSpeech}
There are other emerging models that are gaining traction in speech processing, such as the UniSpeech model, which focuses on developing models that can handle low-resource and cross-lingual speech tasks. Microsoft's UniSpeech approach \cite{wang2021unispeech} proposes a unified pre-training method that combines supervised and unsupervised learning for speech representation learning. The approach uses phonetic CTC learning and phonetically-aware contrastive self-supervised learning to capture more phonetic information and generalize better across languages and domains. The authors evaluate UniSpeech on cross-lingual representation learning and achieve state-of-the-art results on low-resource speech recognition tasks. 
% To scale, they propose a two-stage method consisting of contrastive learning and transducer learning. The authors also introduce UniSpeech-Large, a large-scale audio dataset, and show that by fine-tuning a Conformer model pre-trained on it, they can achieve state-of-the-art results on low-resource speech recognition tasks. 
In a related paper \cite{chen2022unispeech}, the authors propose UniSpeech-SAT, a universal speech representation learning method with speaker-aware pre-training. The method improves existing self-supervised learning for speaker representation learning by using utterance-wise contrastive learning and utterance mixing augmentation. The method achieves state-of-the-art performance in universal representation learning, especially for speaker identification tasks, and can be easily adapted to downstream tasks with minimal fine-tuning. 

\subsubsection{Speechformer}
After the success of UniSpeech models, the field of end-to-end speech recognition has continued to advance. In June 2021, Speechformer \cite{papi2021speechformer}, a self-supervised pre-trained model for end-to-end speech recognition that leverages masked acoustic modeling and contrastive predictive coding was proposed. Unlike previous models that used either convolutional or recurrent neural networks, Speechformer uses a transformer-based encoder-decoder architecture with relative position encoding and layer normalization. It is pre-trained on 53k hours of unlabeled speech data and achieves competitive results on several ASR benchmarks. 

\subsubsection{WavLM}
Microsoft Research Asia released WavLM \cite{chen2022wavlm}, a large-scale self-supervised pre-trained model that can solve full-stack downstream speech tasks such as ASR, TTS, and speaker verification. WavLM jointly learns masked speech prediction and denoising in pre-training and employs gated relative position bias for the Transformer structure to better capture the sequence ordering of the input speech. It is trained on a massive dataset of 94k hours of speech and achieves state-of-the-art results on several downstream speech tasks for 10 languages. 

There are various other transformers for speech-related tasks that we present in Table \ref{table:transformer-models}.




% To scale weakly supervised speech recognition to 10,000 and 30,000 hours of noisier training data, Whisper AI uses two pipelines: one based on an existing end-to-end model (wav2vec 2.0) and one based on an existing hybrid model (Kaldi). The model achieves state-of-the-art results on several benchmarks for speech recognition and speech translation, such as LibriSpeech, Common Voice, TED-LIUM 3, CoVoST 2, and MuST-C.




% \subsection{Speech vs. Text/Image Transformers}
% Transformers are a type of neural network architecture that has gained tremendous popularity in recent years due to their ability to process sequential data, such as text, images, and speech, using self-attention mechanisms \cite{vaswani2017attention}. Transformers for text and images typically take a sequence of tokens as input and output a sequence of embeddings that can be used for various tasks such as classification, translation, or generation \cite{GoogleAI2020Transformers,wang2022git}. Conversely, transformers for speech take a sequence of acoustic features as input and output a sequence of phonemes or words that represent the speech content \cite{subakan2022using,kim2022squeezeformer}.

% When comparing transformers for speech to those for text and images, some unique differences emerge. For instance, transformers for speech often use convolutional layers or squeeze operations to reduce the input sequence length and memory consumption \cite{subakan2022using,kim2022squeezeformer}. In contrast, transformers for text and images usually split the input into fixed-size tokens and embed them into a vector space. Transformers for speech use variable-length segments and apply convolutional layers or feature extraction methods to obtain embeddings \cite{subakan2022using,kim2022squeezeformer}.

% Another difference between transformers for speech and those for text and images is their ability to fuse information from multiple modalities. Transformers for text and images can unify both modalities by treating them as unified sequences of tokens and learning cross-modal interactions \cite{huang2021unifying,kiela2019supervised}. However, transformers for speech usually focus on a single modality, such as speech separation or recognition, and do not leverage other modalities such as text or images.

% In terms of model design, transformers for speech also differ from those for text and images. For example, transformers for speech often use a causal self-attention mechanism that only attends to past tokens to avoid leaking future information \cite{xue2021bayesian}. In contrast, transformers for text and images use a full self-attention mechanism that can process both past and future tokens simultaneously. Additionally, transformers for speech use a hybrid architecture that integrates attention and convolution equally to capture both local and global features, whereas transformers for text and images rely mainly on pure attention, with convolutional layers added as an auxiliary component \cite{kim2022squeezeformer}.

% Finally, transformers for speech may require additional modules such as beam search, language model fusion, or speaker adaptation to improve recognition accuracy \cite{kim2022squeezeformer}. Pre-training on large-scale unlabeled data such as LibriSpeech or Common Voice may also benefit transformers for speech.

% In summary, although transformers share some commonalities across different modalities, there are also significant differences in their design and use depending on the data modality. Transformers for speech have unique challenges and require specific architectural modifications to handle variable-length inputs and exploit the temporal dependencies of speech data.

\begin{comment}
\begin{table}[]
\begin{tabular}{|l|c|c|c|c|}
\hline
 & \begin{tabular}[c]{@{}c@{}}Speech\\ Synthesis\end{tabular} & \begin{tabular}[c]{@{}c@{}}Speech\\ Translation\end{tabular} & \begin{tabular}[c]{@{}c@{}}Multi-\\ Modal\end{tabular} & \begin{tabular}[c]{@{}c@{}}Automatic\\ Speech\\ Recognition\end{tabular} \\ \hline
Whisper AI \cite{radford2022robust}&    & \checkmark  && \checkmark  \\ \hline
\begin{tabular}[c]{@{}l@{}}wav2vec \cite{schneider2019wav2vec}\\ wav2vec 2.0 \cite{baevski2020wav2vec}\end{tabular}  &    &  && \checkmark  \\ \hline
XLS-R \cite{babu2021xls}  &    & \checkmark  && \checkmark  \\ \hline
\begin{tabular}[c]{@{}l@{}}Tacotron \cite{wang2017tacotron}\\ Tacotron 2 \cite{shen2018natural}\end{tabular}  & \checkmark&  &&\\ \hline
DeCoAR 2.0 \cite{chang2020decoar}&    &  && \checkmark  \\ \hline
\begin{tabular}[c]{@{}l@{}}FastSpeech \\ FastSpeech 2 \cite{ren2020fastspeech}\end{tabular} & \checkmark&  &&\\ \hline
UniSpeech \cite{chen2021unspeech}  &    &  && \checkmark  \\ \hline
WavLM \cite{liu2021wavlm} &    &  & \checkmark& \checkmark  \\ \hline
DeltaLM \cite{liu2021deltalm}    &    & \checkmark  &&\\ \hline
FastPitch \cite{lachowicz2020fastpitch}  & \checkmark&  &&\\ \hline
\begin{tabular}[c]{@{}l@{}}data2vec \cite{baevski2022data2vec}\\ data2vec 2.0 \cite{baevski2022efficient}\end{tabular}    &    &  & \checkmark& \checkmark  \\ \hline
SpeechT5 \cite{ao2021speecht5}   &    & \checkmark  & \checkmark& \checkmark  \\ \hline
VALL-E \cite{chen2022valle}  & \checkmark&  &&\\ \hline
\end{tabular}
\caption{Tasks of Speech Transformer Models}
\label{table:taxonomy}
\end{table}
\end{comment}

% \begin{figure*}[!t]
% \centering
% \includegraphics[width=\textwidth]{figures/Transformers-in-Speech-Graph-2.png}
% \caption{Transformer models for speech processing (SP). More to be added. Figure adapted from --.}
% \label{fig:trend}
% \end{figure*}

% \begin{figure*}[!t]
% \centering
% \includegraphics[width=\textwidth]{figures/SectionsinPaper.png}
% \caption{Transformer models for speech processing (SP). More to be added. Figure adapted from --.}
% \label{fig:trend}
% \end{figure*}



\section{Literature Review} \label{sec:applications}



\subsection{Automatic Speech Recognition (ASR)}

ASR enables machines to recognize uttered speech and transform it into the corresponding sequence of text (words or sub-words). State-of-the-art ASR systems achieved improved performance by using RNNs with long short-term memory (LSTM) \cite{schmidhuber1997long} units as their backbone networks. Recently, there has been an increasing interest in the exploitation of transformers \cite{vaswani2017attention} for ASR, inspired by their success in different NLP tasks such as language modeling \cite{dai2019transformer} and machine translation \cite{vaswani2018tensor2tensor}. RNNs process the input signal in a sequential manner by utilizing expensive back-propagation through time (BPTT) \cite{werbos1990backpropagation} algorithm to learn temporal dependencies. Transformers circumvent this with a self-attention mechanism to capture the temporal correlations among the sequential data. This enables transformers to capture longer temporal correlations with less computation complexity. Another advantage of using a transformer is the ability to parallelize the computations in transformers, which can reduce the time for training deeper models on larger datasets. 

In ASR, transformers achieved a competitive recognition rate compared to RNN-based baseline models. For instance, Karita et al. \cite{karita2019comparative} experimentally compared transformers with conventional RNNs. Based on the results, they showed various training and performance benefits achieved with transformers in comparison to RNNs. In \cite{zeyer2019comparison}, Zeyer et al. performed a comparison of the transformer encoder decoder-attention model with RNNs and found that transformers are more stable compared to LSTM, however, they face the problem of overfitting and generalization. They also found that the pretraining leads to faster convergence and performance boost. Li et al. \cite{li2020comparison} performed a comparison between RNN and transformer-based end-to-end models using the 65 thousand hours of Microsoft
anonymized training data. They found that transformer-based attention encoder-decoder architecture achieved the best accuracy. Similarly, studies \cite{wang2020transformer,zhou2018comparison} also performed a comparison of transformers with different ASR systems and highlights the benefits of transformers and pointers for future research.


In hybrid ASR, an acoustic encoder is used to encode an input sequence to high-level embedding vectors that are exploited to generate the posterior distribution of tied states of the hidden Markov model (HMM). Combined with other knowledge sources, these posterior distributions are used to construct a search graph. A decoder network is then used to determine the best hypothesis. Different deep models can be used as acoustic encoders in hybrid ASR. Recently, studies started using transformers for improving hybrid acoustic modeling. Wang et al. \cite{wang2020transformer} evaluated a transformer-based acoustic model for hybrid speech recognition. They explored multiple modeling choices and losses for training deep transformers. Based on the results, they showed that the proposed hybrid ASR can achieve significantly improved WER compared to the very strong bi-directional LSTM (BLSTM) baselines. 

For streaming applications of ASR, Wu et al. \cite{wu2020streaming} presented an acoustic model based on an augmented memory self-attention transformer for hybrid ASR. The proposed model attends a short segment of the input sequence and accumulates information into memory banks. This makes the segment information equally accessible. Evaluations were performed on Librispeech data, which showed that the proposed model achieves a 15\% error reduction in contrast to the widely used LC-BLSTM baseline. 


Recurrent sequence-to-sequence models have achieved great progress in ASR. These models are based on the encoder-decoder architecture, where the encoder transforms the speech feature sequence into hidden representations and generates an output sequence. Conventional RNNs-based sequence-to-sequence models suffer from slow training and training parallelization issues. In a transformer-based sequence-to-sequence model, the encoder and decoder network are composed of multi-head attention and position-wise feed-forward networks rather than RNNs. Also, the encoder outputs are attended by each decoder block respectively. This makes training transformer-based sequence-to-sequence models faster and allows for parallel training. Dong et al. \cite{dong2018speech} presented a Speech-Transformer with no recurrence to learn positional dependencies in speech signals entirely relying on attention mechanisms. Evaluations were performed on Wall Street Journal (WSJ) dataset, which showed that transformers can achieve a competitive word error rate (WER), significantly faster than the published results using RNN-based sequence-to-sequence models. Zhou et al. \cite{zhou2018comparison} explored the modeling units in ASR using transformer-based sequence-to-sequence models on Mandarin Chinese speech. They performed a comparison among five modeling units including context-independent phonemes, syllables, words, sub-words, and characters. Based on the results, they found that the character-based model performs best and achieves state-of-the-art (SoTA) CER on the HKUST dataset. 

%Similarly, the authors in \cite{zhou2018syllable} considered syllables as the modeling unit and performed comparison a comparison between the context-independent (CI)-phoneme-based model and syllable-based model using the transformer. They used HKUST datasets and empirically showed that the syllable-based model performs better in contrast to the CI-phoneme-based counterpart. In \cite{hrinchuk2020correction}, the authors proposed a transformer-based sequence-to-sequence model to improve ASR performance by correcting the ASR system output. The proposed model was able to correct erroneous outputs into semantically and grammatically correct text, which help improve the performance. In order to solve the asynchronous problem between the encoding and decoding of the sequence-to-sequence model, Tain et al. \cite{tian2020synchronous} presented a synchronous transformer that can predict the output chunk by chunk. Based on the experiments, they show that the proposed model was able to encode and decode synchronously, which help achieve an improved CER rate.

In a study by Zhou et al. \cite{zhou2018syllable}, the authors compared the performance of a context-independent (CI)-phoneme-based model and a syllable-based model using a transformer on the HKUST dataset. The results showed that the syllable-based model performed better than the CI-phoneme-based model. In Hrinchuk et al. \cite{hrinchuk2020correction}, a transformer-based sequence-to-sequence model was proposed to improve the performance of automatic speech recognition (ASR) by correcting the ASR system output. The proposed model was able to correct erroneous outputs into semantically and grammatically correct text, which helped improve the performance. To address the issue of asynchronous encoding and decoding in sequence-to-sequence models, Tain et al. \cite{tian2020synchronous} presented a synchronous transformer that can predict the output in chunks. The experiments showed that the proposed model was able to encode and decode synchronously, which led to an improved character error rate (CER) rate.

Transformers have also shown promising results in large-scale ASR. Lu et al. \cite{lu2020exploring} explored transformers for large-scale ASR with 65,000 hours of training data. They investigated different aspects such as warm-up training, model initialisation, and layer normalization techniques on scaling up transformers for ASR. Chen et al. \cite{chen2020developing} evaluated the potential of transformer Transducer models for the first pass decoding with low latency and fast speed on a large-scale ASR dataset. Based on the experiments, they showed that the Transformer Transducer model outperforms RNN Transducer (RNN-T) \cite{graves2012sequence}, streamable transformer, and hybrid model in the streaming scenario. In \cite{li2019speechtransformer}, Li et al. focused on a large-scale Mandarin ASR and propose three optimization strategies to improve the efficiency and performance of SpeechTransformer \cite{dong2018speech}. Wang et al. \cite{wang2020transformer} performed a comparative study on the transformer-based acoustic model on large-scale ASR. They found that the transformer-based ASR model achieves better performance compared to LSTM for voice assistant
tasks. The aforementioned studies show the effectiveness of transformers for ASR. We summarise recent studies on transformers for ASR in Table \ref{tab:ASR-Studies}. 

\subsection{Neural Speech Synthesis}

Neural speech synthesis, or Neural text-to-speech (TTS), is an important field of research that aims to synthesize speech from text input. Traditional TTS systems are composed of complex components including acoustic frontends, duration model, acoustic prediction model, and vocoder models \cite{taylor2009text}. The complexity of the TTS has been recently overcome with deep end-to-end TTS architectures \cite{wang2017tacotron,arik2017deep}. These systems can synthesize realistic-sounding speech by training on $<$text,audio$>$ pairs, and eliminate the need for complex sub-components and their separate training. Prominent models includes Tacotron \cite{wang2017tacotron}, Tacotron 2 \cite{shen2018natural}, Deep Voice 3 \cite{ping2018deep}, and Clarinet \cite{ping2018clarinet}. These models generate Mel-spectrogram from text input, which is then used to synthesize speech by vocoder such as Griffin-Lim \cite{griffin1984signal}, WaveNet \cite{vanwavenet}, and Waveglow \cite{prenger2019waveglow}. 

Recently, transformers are becoming popular to generate Mel-spectrogram in TTS systems. Particularly, they replace RNN structures in end-to-end TTS to improve training and inference efficiency. In \cite{li2019neural}, Li et al. attempted to utilize the multi-head attention mechanism to replace RNN structures as well as the vanilla attention mechanism in Tacotron 2. This helps in improving pluralization by solving the long-distance dependency problem. They generated the Mel-spectrogram using the phoneme sequences as input and exploited WaveNet as a vocoder to synthesize speech samples. Based on the results, they showed that transformer TTS was able to speed up training 4.25 times compared to Tacotron 2 and achieve a similar MOS performance. 



\begin{table*}[!ht]
\centering
\caption{ Recent studies on transformers for \textbf{Automatic Speech Recognition (ASR)}.}
\scriptsize
\begin{tabular}{|l|l|l|l|}
\hline
Author (year) & Dataset   & Performance   & Architecture  \\ \hline
Zeyer et al. \cite{zeyer2019comparison} &\begin{tabular}[c]{@{}l@{}}LibriSpeech (1000 hr), \\TED-LIUM 2 (200 hr) \\ and Switchboard (300 hr)\end{tabular} & \begin{tabular}[c]{@{}l@{}}WER\\LibriSpeech: 2.81\% \\TED LIUM: 12.0\% \\Switchboard: 10.6 \% \end{tabular} & \begin{tabular}[c]{@{}l@{}}Transformer encoder-decoder-attention model and\\ LSTM encoder-decoder-attention model.\end{tabular} 
 \\ \hline
Li et al. \cite{li2020comparison} & \begin{tabular}[c]{@{}l@{}} Microsoft transcribed\\data (65000 hr)\end{tabular} & WER: 9.16\% & \begin{tabular}[c]{@{}l@{}} Recurrent neural network transducer (RNN-T),\\ RNN attention-based encoder-decoder (AED), and \\Transformer-AED \end{tabular} \\ \hline
 
 Wang et al. \cite{wang2020transformer} & \begin{tabular}[c]{@{}l@{}} Personal Assistant Dataset\\ \end{tabular} &\begin{tabular}[c]{@{}l@{}}
 WER: 3.94 \end{tabular} & \begin{tabular}[c]{@{}l@{}} Transformer, \\Emformer (streamable variant of Transformer), \\latency-controlled BLSTM (LCBLSTM), and\\ LSTM \end{tabular} \\ \hline

Wu et al \cite{wu2020streaming} &
\begin{tabular}[c]{@{}l@{}}LibriSpeech and \\ German and Russian \\ video dataset \end{tabular} 
 & \begin{tabular}[c]{@{}l@{}}WER\\LibriSpeech: 2.8\% 
 \\Russian: 18.0\% \\German: 17.4\% \end{tabular} & \begin{tabular}[c]{@{}l@{}}Streaming Transformer with self-attention with \\augmented memory (SAAM) module.\end{tabular} \\\hline

Dong et al. \cite{dong2018speech} & \begin{tabular}[c]{@{}l@{}}WSJ dataset\end{tabular} & WER: 10.9\% & \begin{tabular}[c]{@{}l@{}}Transformer encoder-decoder-attention model with\\ 2D-Attention mechanism.\end{tabular} \\ \hline

Zhou et al. \cite{zhou2018syllable} & HKUST Dataset & CER: 28.77\% & \begin{tabular}[c]{@{}l@{}}Transformer encoder-decoder-attention model with \\syllable-based input and output.\end{tabular} 
\\ \hline
Hrinchuk et al. \cite{hrinchuk2020correction} &
\begin{tabular}[c]{@{}l@{}} Prepared own dataset \\ 101K sample for first name and\\ 580K sample for last name \end{tabular} & \begin{tabular}[c]{@{}l@{}}WER: 9.2\% on first \\ and 6.6\% on last name\end{tabular} & \begin{tabular}[c]{@{}l@{}}Transformer encoder-decoder-attention model for \\ASR post-processing.\end{tabular} \\\hline
Tian et al. \cite{tian2020synchronous} & AIShell& \begin{tabular}[c]{@{}l@{}}CER: 8.91\% \end{tabular} & \begin{tabular}[c]{@{}l@{}} Synchronous Transformer (a variant of Transformer \\with chunk-by-chunk prediction)
\end{tabular} \\ \hline

Lu et al. \cite{lu2020exploring} & Microsoft data (65000 hr) & \begin{tabular}[c]{@{}l@{}}WER: 12.2\% \end{tabular} & \begin{tabular}[c]{@{}l@{}}Transformer encoder-decoder\end{tabular} \\ \hline

Chen et al. \cite{chen2020developing} & Microsoft data (65000 hr) & \begin{tabular}[c]{@{}l@{}}WER
: 8.19\% \\\end{tabular} & \begin{tabular}[c]{@{}l@{}}Transformer transducer\end{tabular} \\ \hline

Li et al. \cite{li2019speechtransformer} & \begin{tabular}[c]{@{}l@{}}AiShell 1(165 hr) \\ HKUST(156 hr)\end{tabular} & \begin{tabular}[c]{@{}l@{}}CER\\AiShell-1: 13.09\% \\ HKUST: 28.95\%\end{tabular} &
\begin{tabular}[c]{@{}l@{}} 
SpeechTransformer
\end{tabular} \\ \hline

Lancucki et al. \cite{lancucki2020fastpitch} & \begin{tabular}[c]{@{}l@{}} LJSpeech-1.1 Dataset \end{tabular} & \begin{tabular}[c]{@{}l@{}}
MOS Values\\
4.071Â±0.164
\end{tabular} & \begin{tabular}[c]{@{}l@{}}	FastPitch, a variant of FastSpeech \end{tabular} \\ \hline

Mohammed et al. \cite{mohamed2019transformers} &
\begin{tabular}[c]{@{}l@{}} LJSpeech-1.1 Dataset \end{tabular}& \begin{tabular}[c]{@{}l@{}}WER: 4.7\% \end{tabular}& \begin{tabular}[c]{@{}l@{}}Transformer encoder-decoder with convolutional\\ context modules \end{tabular} \\ \hline

Zhang et al. \cite{zhang2021transmask} &
\begin{tabular}[c]{@{}l@{}} SWBD\\AMI\\AISHELL\\ \end{tabular}& \begin{tabular}[c]{@{}l@{}}WER: \\SWBD: 7.1\%\\AMI: 24.1\%\\AISHELL: 4.7\% \end{tabular}& \begin{tabular}[c]{@{}l@{}}TransMask, a transformer encoder-decoder with\\ mask prediction heads\end{tabular} \\ \hline

Moriya et al.\cite{moriya2020self} &
\begin{tabular}[c]{@{}l@{}} WER: WSJ, Switchboard, \\Librispeech, CSJ and \\NTT Japanese dataset. \end{tabular}& \begin{tabular}[c]{@{}l@{}}WER: \\SWBD: 8.9\%\\LibriSpeech: 4.4\%\\CER:\\CSJ: 3.9\% \\NTT: 4.2\% \end{tabular}& \begin{tabular}[c]{@{}l@{}}CTC-Transformer, a transformer encoder-decoder \\with connectionist temporal classification (CTC) loss\end{tabular} \\ \hline

Cao et al.\cite{cao2021improving} &
\begin{tabular}[c]{@{}l@{}} LibriSpeech \end{tabular}& \begin{tabular}[c]{@{}l@{}}WER: 3.5\%\end{tabular}& \begin{tabular}[c]{@{}l@{}}Streaming Transformer, a transformer encoder-decoder\\ with block processing and latency control mechanisms\end{tabular} \\ \hline

Tsunoo et al.\cite{tsunoo2019transformer} &
\begin{tabular}[c]{@{}l@{}}
WSJ, Librispeech, \\VoxForge Italian, and\\ AISHELL-1
 \end{tabular}& \begin{tabular}[c]{@{}l@{}}
WER:\\
Librispeech: 4.6\%\\
WSJ: 5.7\%\\
AISHELL-1: 7.6\%\\
VoxForge: 10.3\%\end{tabular}& \begin{tabular}[c]{@{}l@{}}Transformer encoder-decoder with contextual block\\ processing (CBP), a technique to improve streaming ASR \\performance by using past and future context information\end{tabular} \\ \hline

Jain et al.\cite{jain2020finnish} &
\begin{tabular}[c]{@{}l@{}}
YLE news dataset
 \end{tabular}& \begin{tabular}[c]{@{}l@{}}
WER:\\
17.71 \% \end{tabular}& \begin{tabular}[c]{@{}l@{}}Transformer encoder-decoder with deep self-\\attention layers\end{tabular} \\ \hline

Yu et al.\cite{yu2020dual} &
\begin{tabular}[c]{@{}l@{}}
LibriSpeech and MultiDomain

 \end{tabular}& \begin{tabular}[c]{@{}l@{}}
WER:\\LibriSpeech: 2.5\%\\
MultiDomain: 6.0\% 

\end{tabular}& \begin{tabular}[c]{@{}l@{}}Dual-mode ASR (DM-ASR), a hybrid model that \\combines streaming ASR (S-ASR) and full-context ASR \\(F-ASR) using two parallel transformer encoders and one\\ shared decoder.\end{tabular} \\ \hline


  
\end{tabular}
\label{tab:ASR-Studies}
\end{table*}


In order to improve the inference speed, FastSpeech \cite{ren2019fastspeech} used a feed-forward network based on 1D convolution \cite{gehring2017convolutional,jin2018fftnet} and the self-attention mechanism in transformers to generate Mel-spectrogram in parallel. It utilizes the length regulator based on duration predictor to solve the issue of sequence length mismatch between the Mel-spectrogram sequence and its corresponding phoneme sequence. Fastspeech was evaluated on the LJSpeech dataset and results showed that it can significantly speed up the generation of Mel-spectrogram while achieving comparable performance to the autoregressive transformer model. FastPitch \cite{lancucki2020fastpitch} improves FastSpeech by conditioning the TTS model on fundamental frequency or pitch contour. Pitch conditioning improved the convergence and removed the requirement for knowledge distillation of Mel-spectrogram targets in FastSpeech. Fastspeech 2 \cite{ren2020fastspeech} is another transformer-based TTS system that resolved the issues in Fastspeech and better addressed the one-to-many mapping problem in TTS. It uses more diverse information of speech (e.g., energy, pitch, and more accurate duration) as conditional inputs and directly train the system on a ground-truth target. Fastspeech 2s is another variant proposed in \cite{ren2020fastspeech}, which further simplifies the speech synthesis pipeline by directly generating speech from the text in inference without using Mel-spectrograms as intermediate output. Experiments on the LJSpeech data showed that FastSpeech 2 and FastSpeech 2s present a simplified training pipeline with fast, robust, and controllable speech synthesis compared to FastSpeech. 

%End-to-end TTS systems (e.g., \cite{ren2019fastspeech,yu2019durian}) use the duration model to infer the alignments between the output acoustic features and the input text. They involve a multi-stage training pipeline, which may slow down the training procedure. To solve this issue, Lim et al. \cite{lim2020jdi} presented a jointly trained duration-informed transformer (JDI-T) that can jointly train a feed-forward transformer with a duration predictor to generate acoustic feature sequences from an input text without explicit alignments. They evaluated the proposed model on the Korean Single speaker Speech (KSS) dataset and show that the proposed model achieves state-of-the-art performance by synthesising high-quality speech in contrast to popular TTS models including FastSpeech and Tacotron 2. The neural TTS model suffers from robustness issues and sometimes generates bad audio samples, especially for unseen context or unusual text. 

%In \cite{li2020robutrans}, the authors proposed a robust transformer (RobuTrans) to solve such issues. They performed multiple modifications in transformers. First, they convert input texts to linguistic features before feeding them to the encoder. They replace the encoder-decoder attention with duration-based hard attention and the causal self-attention with pseudo-non-causal attention to learn the holistic information from the input. They also replace position embedding with 1-D CNN. Based on these modifications, they solved the robustness issues and achieved improved MOS scores compared to popular TTS models. Segment-Transformer (s-Transformer) \cite{wang2020s} is another system that models speech at the segment level to achieve robustness. It captures long-term dependence from segment recurrence and utilizes segment-level encoder-decoder attention to solve the difficult handling of long sequence pairs. s-Transformer achieve similar performance compared to the standard transformer and showed robustness on extra-long sentences ( e.g., a couple of minutes). Zheng et al. \cite{zheng2020improving} introduce a local recurrent neural network into the transformer to capture both sequential and local information in the sequences. Evaluations on a 20-hour Mandarin speech corpus show that the proposed model can achieve better performance compared to a transformer.

End-to-end TTS systems, such as FastSpeech \cite{ren2019fastspeech} and Durian \cite{yu2019durian}, utilize a duration model to align output acoustic features with the input text. However, the multi-stage training pipeline used in these systems can be slow. To address this issue, Lim et al. proposed a jointly trained duration-informed transformer (JDI-T) that uses a feed-forward transformer with a duration predictor to generate acoustic feature sequences without explicit alignments. JDI-T achieved state-of-the-art performance on the Korean Single Speaker Speech (KSS) dataset and synthesized high-quality speech compared to other popular TTS models. 

However, neural TTS models can suffer from robustness issues and generate poor audio samples for unseen or unusual text. To overcome these issues, Li et al. proposed RobuTrans, a robust transformer that converts input texts to linguistic features before feeding them to the encoder \cite{li2020robutrans}. They also modified the attention mechanism and position embedding to improve the learning of holistic information from the input, resulting in improved MOS scores compared to other popular TTS models. Another approach to achieving robustness in TTS systems is the segment-transformer (s-Transformer) \cite{wang2020s} proposed by Wang et al. The s-Transformer is capable of modeling speech at the segment level, allowing it to capture long-term dependencies and use segment-level encoder-decoder attention to handle long sequence pairs. This approach enables the s-Transformer to achieve similar performance to the standard transformer while also exhibiting robustness on extra-long sentences. Lastly, Zheng et al. \cite{zheng2020improving} proposed an approach that incorporates a local recurrent neural network into the transformer to capture both sequential and local information in sequences. Evaluation on a 20-hour Mandarin speech corpus demonstrated that this model outperforms the transformer alone in terms of performance.

Speech synthesis using multi-speaker voices is another interesting field of research. Chen et al. presented a MultiSpeech model, based on transformer TTS, that can synthesize high-quality speech in multi-speaker voices with fast inference speed. To achieve this, they designed a special component in the transformer to preserve positional information and prevent copy between consecutive speech frames. The MultiSpeech model was evaluated on VCTK and LibriTTS datasets, and the results demonstrated superior performance compared to existing models. Voice conversion, on the other hand, focuses on altering the source speaker's voice to match the target voice without changing the linguistic content. While various studies have explored RNN-based sequence-to-sequence models for voice conversion, these models require extensive training data and often suffer from mispronunciation issues. To address these challenges, Huang et al. presented a voice transformer network that utilized pre-training to improve data-efficient training and achieve better results compared to RNN-based models. Recent studies have continued to push the boundaries of speech synthesis systems, exploring various approaches to improve performance. The summary of recent studies on speech synthesis is presented in Table \ref{tab:SpeechSynthesis-Studies}.

\begin{table*}[!ht]
\scriptsize
\caption{Recent studies on transformers for \textbf{Speech Synthesis}.}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{Author (year)} & \multicolumn{1}{c|}{Datasets} & Performance& \multicolumn{1}{c|}{Architecture}\\ \hline
\hline
%
Ren et al. \cite{ren2019fastspeech} & \begin{tabular}[c]{@{}l@{}} LJSpeech dataset \end{tabular} & \begin{tabular}[c]{@{}l@{}}
MOS: 3.84 Â± 0.08
\end{tabular} & \begin{tabular}[c]{@{}l@{}}FastSpeech, a feed-forward Transformer TTS \end{tabular}\\ \hline

Ren et al. \cite{ren2020fastspeech} & \begin{tabular}[c]{@{}l@{}} LJSpeech dataset \end{tabular} & \begin{tabular}[c]{@{}l@{}}
MAE: \\FastSpeech2:0.131 \\FastSpeech2s:0.133
\end{tabular} &\begin{tabular}[c]{@{}l@{}} FastSpeech 2, an improved FastSpeech with\\ more variance information\end{tabular} \\ \hline

Chen et al. \cite{chen2020multispeech} & \begin{tabular}[c]{@{}l@{}} VCTK and LibriTTS \\datasets \end{tabular} & \begin{tabular}[c]{@{}l@{}}
MOS: \\VCTK: 3.65 Â± 0.14 \\ LibriTTS: 2.95 Â± 0.14 
\end{tabular} &\begin{tabular}[c]{@{}l@{}} MultiSpeech, a multi-speaker Transformer TTS\\ with speaker embeddings and classifier loss\end{tabular} \\ \hline


{\L}a{\'n}cucki et al. \cite{lancucki2020fastpitch} & \begin{tabular}[c]{@{}l@{}} LJSpeech Dataset \end{tabular} & \begin{tabular}[c]{@{}l@{}}
 MOS: 3.707 Â± 0.218 
\end{tabular} &\begin{tabular}[c]{@{}l@{}} FastPitch, a FastSpeech variant with pitch\\ prediction and control\end{tabular} \\ \hline

Gehringi et al. \cite{gehring2017convolutional} & \begin{tabular}[c]{@{}l@{}} WMT'14 English-French \\
WMT'14 English-German.\\WMT'14 English-Romanian. \end{tabular} & \begin{tabular}[c]{@{}l@{}}
WMT'14 English-French: 20.51\\
WMT'14 English-German: 26.43\\WMT'14 English-Romanian: 41.62
\end{tabular} &\begin{tabular}[c]{@{}l@{}} ConvS2S, a CNN-based \\sequence-to-sequence model. \end{tabular} 
\\ \hline
Yu et al. \cite{yu2019durian} & \begin{tabular}[c]{@{}l@{}} Hours of Speech: \\
Male speaker: 18 hours\\
Female speaker: 7 hours\\
 \end{tabular} & \begin{tabular}[c]{@{}l@{}}
Male: 4.11 \\Female: 4.26
\end{tabular} &\begin{tabular}[c]{@{}l@{}} DurIAN, a multimodal TTS model with \\duration-informed attention network (DIAN) \end{tabular} \\ \hline

Lim et al. \cite{lim2020jdi} & \begin{tabular}[c]{@{}l@{}} Internal Speaker Dataset\\ Korean Speaker Dataset \end{tabular} & \begin{tabular}[c]{@{}l@{}}
MOS Values on a scale of 5\\
Internal: 3.77\\
KSS: 3.52
\end{tabular} & \begin{tabular}[c]{@{}l@{}}JDI-T, a feed-forward Transformer TTS with\\ duration predictor \end{tabular} \\ \hline

Wang et al. \cite{wang2020s} & \begin{tabular}[c]{@{}l@{}} Professional enUS\\ A speaker dataset (46 hour) \end{tabular} & \begin{tabular}[c]{@{}l@{}}
MOS Values on a scale of 5\\
Short: 4.29\\
Long: 4.2\\
Extra Long: 3.99
\end{tabular} & \begin{tabular}[c]{@{}l@{}}s-Transformer, a segment-wise Transformer \\TTS\end{tabular} \\ \hline

Zheng et al. \cite{zheng2020improving} & \begin{tabular}[c]{@{}l@{}} Mandarin speech corpus \end{tabular} & \begin{tabular}[c]{@{}l@{}}
MOS Values \\
4.34Â±0.05
\end{tabular} & \begin{tabular}[c]{@{}l@{}}LRN-Transformer, a Transformer TTS with \\LRNs\end{tabular} \\ \hline

Huang et al. \cite{huang2020voice} & \begin{tabular}[c]{@{}l@{}} CMU Arctic dataset \end{tabular} & \begin{tabular}[c]{@{}l@{}}
WER: 7.8\% \\
CER: 4.8\%

\end{tabular} & \begin{tabular}[c]{@{}l@{}}VTN, a seq2seq voice conversion model with\\ TTS pretraining \end{tabular} \\ \hline

Hu et al. \cite{hu2020unsupervised} & 
\begin{tabular}[c]{@{}l@{}} LibriTTS\\ VCTK \end{tabular} & \begin{tabular}[c]{@{}l@{}}
WER:
LibriTTS: 33.3 Â± 1.2
\\ VCTK: 20.3 Â± 1.2
\end{tabular} & \begin{tabular}[c]{@{}l@{}}MI-TTS, an unsupervised TTS model with \\VAEs and mutual information minimization \end{tabular} \\ \hline
Chen et al. \cite{chen2022fine} & 
\begin{tabular}[c]{@{}l@{}} LJSpeech\\ VCTK \end{tabular} & \begin{tabular}[c]{@{}l@{}}
WER:
LJSpeech: 9.5\%
\\ VCTK: 12.5 \%
\end{tabular} & \begin{tabular}[c]{@{}l@{}}TransformerTTS with local style tokens (LST) \\and cross-attention blocks \end{tabular} \\ \hline

Liu et al. \cite{liu2021graphspeech} & 
\begin{tabular}[c]{@{}l@{}} LJSpeech database\end{tabular} & \begin{tabular}[c]{@{}l@{}}
RMSE: 1.625\%

\end{tabular} & \begin{tabular}[c]{@{}l@{}}GraphSpeech, a graph neural network (GNN)\\ based TTS that encodes syntactic information \\as a dependency graph \end{tabular} \\ \hline

Wang et al. \cite{wang2021patnet} & 
\begin{tabular}[c]{@{}l@{}} LJSpeech database\end{tabular} & \begin{tabular}[c]{@{}l@{}}
CER: 1.7\%

\end{tabular} & \begin{tabular}[c]{@{}l@{}}PatNet, a phoneme-level autoregressive \\Transformer (TTS) that predicts\\ mel-spectrograms from phoneme sequences\end{tabular} \\ \hline

\end{tabular}
\label{tab:SpeechSynthesis-Studies}
\end{table*}


\begin{table*}[!ht]
\scriptsize
\caption{Recent studies on transformers for \textbf{Speech Translation}.}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Author (year)} & \textbf{Datasets} & \textbf{Performance} & \textbf{Architecture(s)} \\ \hline
\begin{tabular}[c]{@{}l@{}}Kano et al.\\ 2021 \cite{kano2021transformer}\end{tabular} & \begin{tabular}[c]{@{}l@{}}Fisher Spanish-English,\\ LibriSpeech English-French,\\ LibriSpeech English-German\end{tabular}  & \begin{tabular}[c]{@{}l@{}}BLEU:\\ 16.9 (es-en), 15.8 (en-fr), 10.1 (en-de);\\ MOS:\\ 3.87 Â± 0.08 (es-en), 3.81 Â± 0.09 (en-fr), \\3.77 Â± 0.09 (en-de);\\ MAE:\\ 0.131 (es-en), 0.132 (en-fr), 0.133 (en-de)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Encoder-Decoder with attention and transcoder \\ module\end{tabular}  \\ \hline
\begin{tabular}[c]{@{}l@{}}Zhang et al.\\ 2019 \cite{zhang2019lattice}\end{tabular} & \begin{tabular}[c]{@{}l@{}}IWSLT 2017 en-de ST task,\\ MuST-C en-de ST task\end{tabular}  & \begin{tabular}[c]{@{}l@{}}BLEU: 17.9 on IWSLT 2017 en-de;\\ BLEU: 20.8 on MuST-C en-de\end{tabular} & \begin{tabular}[c]{@{}l@{}}A novel controllable lattice attention mechanism \\ that leverages the extra information from the\\ lattice structure of ASR output\end{tabular}  \\ \hline
\begin{tabular}[c]{@{}l@{}}Huawei et al.\\ 2022 \cite{huawei2022transformer}\end{tabular} & \begin{tabular}[c]{@{}l@{}}WMT 2014 English-German5,\\ WMT 2014 English-French5,\\ WMT 2016 Romanian-English5,\\ WMT 2016 English-Czech5,\\ WMT 2017 English-Turkish\end{tabular} & \begin{tabular}[c]{@{}l@{}}BLEU score:\\ 28.4 (en-de),\\ 41.0 (en-fr),\\ 32.8 (ro-en),\\ 26.7 (en-cs),\\ 24.9 (en-tr)\end{tabular} & Encoder-decoder with self-attention layers \\ \hline
\begin{tabular}[c]{@{}l@{}}Ao et al.\\ 2021 \cite{ao2021speecht5}\end{tabular}  & \begin{tabular}[c]{@{}l@{}}LibriSpeech,\\ LibriTTS,\\ Common Voice,\\ LJSpeech,\\ AISHELL-1/2/3,\\ VCTK-Corpus,\\ VoxCeleb1/2\end{tabular}    & \begin{tabular}[c]{@{}l@{}}WER: \\ 2.9\% (LibriSpeech test-clean), \\7.0\% (LibriSpeech test-other),\\ 6.8\% (AISHELL-1), \\6.0\% (AISHELL-2), \\7.9\% (AISHELL-3);\\ MOS:\\ 4.06 (LibriTTS), 4.01 (LJSpeech);\\ BLEU:\\ 17.8 (English-to-Chinese ST);\\ MCD:\\ 6.38 dB (voice conversion);\\ PESQ: 2.97 (speech enhancement);\\ EER: 0.83\% (speaker identification)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Shared encoder-decoder network with six\\ modal-specific pre/post-nets, pre-trained with\\ contrastive loss, masked speech/text modeling \\loss and cross-modal alignment loss\end{tabular} \\ \hline
\end{tabular}
\label{tab:SpeechTranslation-Studies}
\end{table*}

\subsection{Speech Translation (ST)}

Speech Translation (ST) is the process of translating the spoken speech in the source language into the target language. ST systems are typically divided into two categories: cascaded systems and end-to-end systems. Cascaded ST systems consist of an automatic speech recognition (ASR) system and a machine translation (MT) system. ASR system generates text from the spoken sentence, which is then used by a machine translation system to translate it into the target language. Cascaded ST systems face the problem of errors compounding between components, e.g., recognition errors leading to larger translation errors. In contrast, end-to-end ST systems optimize a single model that directly translates the spoken utterance into the target language. Various studies explored methods and techniques to improve the performance of both cascaded ST systems \cite{ney1999speech,matusov2005integration,do2017toward} as well as end-to-end ST systems \cite{berard2016listen,berard2018end,weiss2017sequence}. 

Presently, ST research explores transformers for solving different issues. Vila et al. \cite{vila2018end} used a transformer for end-to-end speech translation. Evaluations were performed on the Spanish-to-English translation task and a bilingual evaluation understudy score was computed, which showed that the end-to-end architecture was able to outperform the concatenated systems. Zhang et al. \cite{zhang2019lattice} presented a lattice transformer for speech translation, which also uses lattice representation in addition to the traditional sequential input. They evaluated the proposed model on Spanish-English Speech Translation Corpus and achieved improvements over strong baseline results. In \cite{di2019adapting}, the authors present an adaptation of the transformer to end-to-end ST. They performed down-sampling of input with convolutional neural networks to i) make the training process feasible on GPUs, ii) model the bi-dimensional nature of a spectrogram, and iii) add a distance penalty to the attention, so as to bias it towards the local context. Furthermore, several distinct studies (Jia et al., 2021 \cite{jia2021translatotron}; Zhang et al., 2023 \cite{zhang2023speak}; Huang et al., 2022 \cite{huang2022transpeech}; Li et al., 2020 \cite{li2020multilingual}; Wang et al., 2020 \cite{wang2020fairseq}; Zeng et al., 2021 \cite{zeng2021realtrans}) explore various Speech Translation model implementations or models designed for Speech Translation tasks.






%In \cite{di2019enhancing}, the authors attempted to enhance the performance of transformer for ST task. 

\begin{table*}
\scriptsize
\caption{Recent studies on transformers for \textbf{Speech Paralinguistics}.}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Author (year)}  & \textbf{Datasets} & \textbf{Performance}  & \textbf{Architecture(s)}    \\ \hline
\begin{tabular}[c]{@{}l@{}}Chen et. al.\\ 2022 \cite{chen2022wavlm}\end{tabular}   & \begin{tabular}[c]{@{}l@{}}SUPERB benchmark,\\ LibriSpeech (ASR),\\ VoxCeleb1/2 (SV),\\ AMI (SD),\\ CommonVoice (LID),\\ CREMA-D (SER)\end{tabular} & \begin{tabular}[c]{@{}l@{}}SUPERB: 0.9231234,\\ LibriSpeech WER: 1.9/4.81,\\ VoxCeleb1/2 EER: 0.69/0.831,\\ AMI DER: 7.51,\\ CommonVoice LID ACC: 0.9781,\\ CREMA-D SER UAR: 0.7221\end{tabular}    & \begin{tabular}[c]{@{}l@{}}Transformer with gated relative position bias\\ and utterance mixing.\end{tabular}   \\ \hline
\begin{tabular}[c]{@{}l@{}}Shor et al.\\ 2022 \cite{shor2022trillsson}\end{tabular}  & \begin{tabular}[c]{@{}l@{}}SUPERB benchmark,\\ LibriSpeech (ASR),\\ VoxCeleb1/2 (SV),\\ AMI (SD),\\ CommonVoice (LID),\\ CREMA-D (SER)\end{tabular} & \begin{tabular}[c]{@{}l@{}}SUPERB: 0.906123,\\ LibriSpeech WER: 2.4/5.81,\\ VoxCeleb1/2 EER: 0.75/0.911,\\ AMI DER: 8.71,\\ CommonVoice LID ACC: 0.9761,\\ CREMA-D SER UAR: 0.713\end{tabular}  & \begin{tabular}[c]{@{}l@{}}EfficientNet-B0 with Audio Spectrogram Transformer \\encoder and ResNet-50 encoder for fixed-length and \\arbitrary-length inputs respectively.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Shor et al.\\ 2022 \cite{shor2022universal}\end{tabular}  & \begin{tabular}[c]{@{}l@{}}SUPERB benchmark,\\ LibriSpeech (ASR),\\ VoxCeleb1/2 (SV),\\ AMI (SD),\\ CommonVoice (LID),\\ CREMA-D (SER)\end{tabular} & \begin{tabular}[c]{@{}l@{}}SUPERB: 0.9511234,\\ LibriSpeech WER: 2.3/5.61,\\ VoxCeleb1/2 EER: 0.67/0.831,\\ AMI DER: 8.31,\\ CommonVoice LID ACC: 0.9781,\\ CREMA-D SER UAR: 0.726\end{tabular} & \begin{tabular}[c]{@{}l@{}}A stack of convolution-augmented transformer blocks\\ known as Conformers with a total of 600M parameters\\ trained on YT-U dataset using self-supervision.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Xu et al.\\ 2021 \cite{xu2021transformer}\end{tabular}  & \begin{tabular}[c]{@{}l@{}}IEMOCAP,\\ MSP-IMPROV,\\ RAVDESS\end{tabular}  & \begin{tabular}[c]{@{}l@{}}IEMOCAP: WA: 0.661, UA: 0.633;\\ MSP-IMPROV: WA: 0.651, UA: 0.644;\\ RAVDESS: WA: 0.713, UA: 0.712\end{tabular}  & \begin{tabular}[c]{@{}l@{}}A transformer-based end-to-end model that \\consists of a CNN encoder, a transformer encoder, \\a temporal attention layer, and a softmax classifier.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Gao et al.\\ 2022 \cite{gao2022paraformer}\end{tabular} & \begin{tabular}[c]{@{}l@{}}ComParE 2019-2021,\\ IEMOCAP,\\ MSP-IMPROV,\\ RAVDESS,\\ SAVEE,\\ EmoDB,\\ CREMA-D,\\ EMOVO-Corpus,\\ ShEMO-Corpus\end{tabular}  & \begin{tabular}[c]{@{}l@{}}ComParE 2019: UAR: 0.72\\ ComParE 2020: UAR: 0.67\\ ComParE 2021: UAR: 0.65\\ IEMOCAP: F1: 0.71\\ MSP-IMPROV: F1: 0.69\\ RAVDESS: UAR: 0.76\\ SAVEE:  UAR: 0.77\\ EmoDB: UAR: 0.80\\ CREMA-D: UAR: 0.81\\ EMOVO-Corpus: UAR: 0.79\\ ShEMO-Corpus: UAR: 0.78\end{tabular} & \begin{tabular}[c]{@{}l@{}}A hierarchical framework that consists of a \\transformer-based encoder-decoder model \\and a multi-task learning module.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Chen et al.\\ 2023 \cite{chen2023speechformer++}\end{tabular} & \begin{tabular}[c]{@{}l@{}}LibriSpeech \\test-clean/test-other.\end{tabular} & WER: 2.3\%/5.4\%  & \begin{tabular}[c]{@{}l@{}}A parallel transformer that utilizes a continuous \\integrate-and-fire based predictor to predict \\the number of tokens and generate hidden variables.\end{tabular} \\ \hline
\end{tabular}
\label{tab:SpeechParalinguistics-Studies}
\end{table*}


\subsection{Speech Paralinguistics}

Speech paralinguistics is a term that refers to the non-verbal aspects of speech communication, such as tone, pitch, volume, speed, emotion, and accent \cite{schotz2002paralinguistic}. In terms of NLP, speech paralinguistics is an area that aims to analyse and synthesize speech signals with paralinguistic features. These features can convey important information about the speakerâ€™s identity, intention, attitude, and mood, and can enhance the performance and naturalness of various speech applications. In this section, we discuss how transformers can be used for speech paralinguistic tasks, which involve analyzing and synthesizing speech signals with non-verbal features such as emotion, speaker identity, and accent. We focus on a recent paper by Chen et al. (2021) \cite{chen2022wavlm}, which proposes a new pre-trained model called WavLM for full-stack speech processing. WavLM uses a masked speech prediction and a speech-denoising objective to learn universal speech representations from large-scale unlabeled data. WavLM also employs a gated relative position bias mechanism to capture the sequence order of speech signals. The paper shows that WavLM achieves state-of-the-art results on the SUPERB benchmark \cite{yang2021superb} and improves performance on several other speech benchmarks for tasks such as speech emotion recognition (SER), speaker verification (SV), speaker diarization (SD), and speech separation (SS). We review the main contributions of WavLM and compare it with other transformer-based models for speech paralinguistic tasks.

In speech paralinguistics, Xu et al. (2021) proposed a new attention mechanism called local dense synthesizer attention (LDSA) \cite{xu2021transformer}. The mechanism restricts attention scope to a local range around the current frame and eliminates dot products and pairwise interactions to improve the performance of end-to-end speech recognition models while reducing computational complexity. The study also combines LDSA with self-attention to extract both local and global information from speech signals. Shor et al. (2022) proposed a new pre-trained model, Conformer-HuBERT, that combines conformers and HuBERT to learn universal speech representations for paralinguistic tasks such as SER, SV, SD, and SS \cite{shor2022universal}. Conformers are hybrid architectures that integrate CNNs and transformers, while HuBERT is a self-supervised learning framework that learns from large-scale unlabeled data. The study demonstrates that Conformer-HuBERT outperforms existing models on several benchmarks for paralinguistic tasks, achieving state-of-the-art results.

Shor and Venugopalan (2022) \cite{shor2022trillsson} propose a collection of small and performant models called TRILLsson, which are distilled from a large self-supervised model called CAP12 \cite{shor2022universal}. TRILLsson uses knowledge distillation on public data to reduce the size of CAP12 by up to 100x while retaining 90-96 percent of its performance. The paper demonstrates that TRILLsson outperforms previous models on the Non-Semantic Speech (NOSS) benchmark \cite{shor2020towards}. Additionally, the paper releases the TRILLsson models publicly. Another attempt by Chen et al. (2022) \cite{chen2022speechformer} propose a novel framework, SpeechFormer, that incorporates the unique characteristics of speech signals into transformer models. The framework comprises three components: a hierarchical encoder that reduces the input sequence length using convolutional and pooling layers, a local self-attention module that captures dependencies within a fixed window size, and a global self-attention module that captures dependencies across different windows. The paper demonstrates that SpeechFormer achieves competitive results on several speech benchmarks for tasks such as automatic speech recognition (ASR), speaker verification (SV), speaker diarization (SD), and emotion recognition. Another recent paper, SpeechFormer++: by Chen et al. (2023) \cite{chen2023speechformer++} builds on the previous work of SpeechFormer \cite{chen2022speechformer} and incorporates the unique characteristics of speech signals into transformer models. The framework includes a unit encoder that models the intra- and inter-unit information, a merging block that generates features at different granularities based on the hierarchical relationship in speech signals, and a word encoder that integrates word-grained features into each unit encoder. The paper demonstrates that SpeechFormer++ outperforms the standard transformer on various paralinguistic tasks, such as speech emotion recognition (SER), depression classification (DC), and Alzheimer's disease detection (ADD).

Gao et al. (2022) introduced Paraformer, a new model for non-autoregressive end-to-end speech recognition that uses parallel attention and parallel decoder techniques \cite{gao2022paraformer}. Paraformer's encoder-decoder architecture allows each decoder layer to attend to all encoder outputs simultaneously without waiting for previous decoder outputs, and each output token to be predicted independently without depending on previous output tokens. The paper shows that Paraformer outperforms existing non-autoregressive models on several ASR datasets, achieving faster inference speed and higher accuracy. These recent innovations in transformer-based models such as WavLM, Conformer-HuBERT, TRILLsson, SpeechFormer, and Paraformer have shown promising results for speech paralinguistic tasks, paving the way for more natural and efficient speech applications.







%The evolution of models in the field of SP has been marked by attempts to employ various techniques for producing results. Initial work in this direction was presented in the paper by Radford et al. \cite{radford2018improving}, where a two-step approach was proposed for the training process. The authors suggested a combination of Supervised and Unsupervised Learning models to make use of the vast pool of unlabeled data, by pre-training a model on large amounts of unlabeled data to produce a generalized model, followed by fine-tuning it for specific tasks. Subsequently, numerous language models have been developed and existing models have been refined to achieve state-of-the-art results. One such model is the wav2vec \cite{schneider2019wav2vec} which utilized the abundance of unlabeled speech data in pre-training to enhance supervised speech recognition. The wav2vec model is a CNN that transforms raw audio input into a general representation, which is then used as input for SP tasks. The model was later improved upon by Baevski et al. \cite{baevski2020wav2vec} who masked the speech input in the latent space and solved a contrastive task defined over a quantization of the latent representations learned jointly. This approach required far less labeled data and outperformed the previous state-of-the-art, using 100x less labeled data (1 hour versus 100 hours). With just 10 minutes of labeled data and pre-training on 53,000 hours of unlabeled data, a Word Error Rate of 4.8/8.2 was achieved.

%The utilization of the previously mentioned techniques helps mitigate the reliance on labeled data through the implementation of pre-training processes, where language representations can be learned from unlabeled data. The challenge of unavailability of low-resource language speech data remains, however, such as for Urdu. In the paper by Baevski et al. \cite{baevski2020wav2vec}, the XLSR model was proposed, which had the capability to learn cross-language speech representations by pre-training a single model using multiple languages and the raw waveform of unlabeled speech data. The XLSR model builds upon the wav2vec 2.0 approach, which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latent shared across languages. In 2019, Liu et al. \cite{liu2019roberta} introduced XLM-R, a multilingual language model pre-trained at scale, which showed significant performance gains for various cross-lingual transfer tasks, outperforming the mBERT model on multiple cross-lingual benchmarks. For the language of Urdu, the XLM-R model achieved a SoTA of 11.4 \% compared to previous XLM models. In Das et al.'s study \cite{das2021emotion}, the authors explored emotion recognition using the low-resource language Bengali, comparing various models and finding XLS-R to perform the best. They also established a Bengali emotion corpus.

%The investigation of speech recognition models and techniques has proven to be beneficial for emotion classification tasks as well. In recent studies, the implementation of transformers in combination with other models has been prevalent in emotion recognition \cite{sordoni2015hierarchical}. An early approach, the Hierarchical Recurrent Long-Short Term Memory Network with Contextual Encoders (HRLCE), utilized a combination of Hierarchical Recurrent Neural Network for context (HRED) and BERT for improved results \cite{sordoni2015hierarchical}. The HRED network consisted of two types of RNN networks, an encoder RNN and context RNN, with BERT applied to the second layer. Using this method, the authors achieved a Harmonic Mean score of 0.7706 and 0.7666 for Dev and Test, respectively, for emotions including happy, angry, and sad.


% The issue of comprehending compositional sentiment semantics presents a challenge in the field of emotion recognition. Simple sentences can contain complex context that is difficult to encode in models, leading to the possibility of overlooking the underlying emotions. For instance, the sentence ``Frenetic but not really funny'' contains two negations and one conjunction, making it challenging for models to understand the sentiment. Several approaches have been made to tackle this challenge, including the use of tree structures in the transfer of data.

% In \cite{yin2020sentibert}, the authors present a novel model, SentiBERT, which incorporates sentiment semantics into BERT (Devlin et al., 2019). The model pre-trains deep bidirectional transformers using unlabeled data to produce representations. When used in conjunction with RoBERTa, SentiBERT produced results of 67.2 for Emotion Intensity and 74.67 for EmoContext. RoBERTa, built on top of BERT, addresses the issue of undertraining by evaluating the impact of several key hyperparameters and training data size, resulting in improved performance.

% In a separate study, the authors of \cite{ke2019sentilare} introduced an innovative approach that trains downstream tasks separately. They proposed a context-aware sentiment attention mechanism, utilizing SentiWordNet to acquire the sentiment polarity of each word along with its part-of-speech tag. This was followed by the development of a new pre-training task known as the label-aware masked language model, aimed at constructing knowledge-aware language representation. The resulting model, named SentiLARE, was compared to other state-of-the-art models such as SentiBERT and BERT and demonstrated superior results.

% In \cite{xia2019rthn}, the authors proposed an RNN-based emotion extraction framework named RTHN, which effectively addresses the challenge of comprehending the relationship and connection between sentences. The model employs two-level encoders, comprising a low word-level encoder and a clause-level encoder, to learn the correlation between multiple clauses within a sentence. Upon comparison with 12 other systems, RTHN achieved the best performance, elevating the F1 score from 72.69\% to 76.77\%, surpassing the existing state-of-the-art.


\begin{table*}[!ht]
\caption{Recent studies on transformers for \textbf{Speech Enhancement}.}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Author (year)} & \textbf{Datasets}   & \textbf{Performance}  & \textbf{Architecture(s)}   \\ \hline
\begin{tabular}[c]{@{}l@{}}Yu et al.\\ 2022 \cite{yu2022setransformer}\end{tabular} & \begin{tabular}[c]{@{}l@{}}VCTK,\\ CHiME-3\end{tabular} & \begin{tabular}[c]{@{}l@{}}PESQ: 2.97 Â± 0.01,\\ STOI: 0.94 Â± 0.00,\\ SI-SNRi: 16.9 Â± 0.1 dB\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Encoder-LSTM-Multi-head attention-\\ Decoder\end{tabular}   \\ \hline
\begin{tabular}[c]{@{}l@{}}Kim et al.\\ 2020 \cite{kim2020t}\end{tabular} & VCTK    & \begin{tabular}[c]{@{}l@{}}PESQ: 3.06 Â± 0.01,\\ STOI: 0.95 Â± 0.00,\\ SI-SNRi: 17.8 Â± 0.1 dB\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Encoder-Self-attention with Gaussian \\ weights-Decoder\end{tabular}    \\ \hline
\begin{tabular}[c]{@{}l@{}}Wang et al.\\ 2021 \cite{wang2021tstnn}\end{tabular} & \begin{tabular}[c]{@{}l@{}}VCTK,\\ DNSCL,\\ DNSCL-R\end{tabular}    & \begin{tabular}[c]{@{}l@{}}PESQ:\\ Voice Bank + DEMAND: 3.08,\\ DNSCL: 3.02,\\ DNSCL-R: 2.93;\\ STOI:\\ Voice Bank + DEMAND: 0.95,\\ DNSCL: 0.94,\\ DNSCL-R: 0.92\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Encoder-Two-stage transformer module-\\ Masking module-Decoder\end{tabular}   \\ \hline
\begin{tabular}[c]{@{}l@{}}Subakan et al.\\ 2021 \cite{subakan2021attention}\end{tabular} & \begin{tabular}[c]{@{}l@{}}WSJ0-2mix,\\ WSJ0-3mix\end{tabular}  & \begin{tabular}[c]{@{}l@{}}WSJ0-2mix:\\ SDR: 20.8 dB, SI-SNR: 21.9 dB;\\ WSJ0-3mix:\\ SDR: 17.6 dB, SI-SNR: 18.7 dB\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Multi-scale transformer with multi-head \\ attention and feed-forward layers\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Zhang et al.\\ 2022 \cite{zhang2022cross}\end{tabular}   &\begin{tabular}[c]{@{}l@{}}LibriSpeech,\\ LibriMix,\\ WHAMR,\\ WHAM\end{tabular} &  \begin{tabular}[c]{@{}l@{}}LibriMix:\\ PESQ: 3.25, STOI: 0.95, ESTOI: 0.93;\\ WHAMR:\\ PESQ: 2.98, STOI: 0.94, ESTOI: 0.91;\\ WHAM\\ PESQ: 3.04, STOI: 0.95, ESTOI: 0.92;\end{tabular} & \begin{tabular}[c]{@{}l@{}}Streaming transformer with cross-attention \\ between encoder and decoder layers\end{tabular}   \\ \hline
\begin{tabular}[c]{@{}l@{}}Zhao et al.\\ 2021 \cite{zhao2021multi}\end{tabular} &  \begin{tabular}[c]{@{}l@{}}WSJ0-2mix,\\ WSJ0-3mix\end{tabular} & \begin{tabular}[c]{@{}l@{}}WSJ0-2mix:\\ SDR: 20.6 dB, SI-SNR: 21.7 dB;\\ WSJ0-3mix9:\\ SDR: 17.5 dB, SI-SNR: 18.6 dB\end{tabular} & \begin{tabular}[c]{@{}l@{}}Multi-scale group transformer with dense-\\ fusion or light-fusion and time-domain\\ audio separation network (TasNet)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Jiang et al.\\ 2023 \cite{jiang2023low}\end{tabular} & LibriSpeech   & \begin{tabular}[c]{@{}l@{}}PESQ: 3.25,\\ STOI: 0.95,\\ ESTOI: 0.93,\\ SI-SNRi: 19.1 dB\end{tabular} & \begin{tabular}[c]{@{}l@{}}Hierarchical frame-level Swin\\ Transformer with adaptive windowing and\\ convolutional layers\end{tabular} \\ \hline
\end{tabular}
\label{tab:SpeechEnhancement-Studies}
\end{table*}


\subsection{Speech Enhancement and Separation}
The area of speech enhancement involves the application of various algorithms for enhancing the quality of speech. % The tasks associated with speech enhancement encompass noise reduction, personalized speech enhancement, speech separation, and others. In the following, these tasks will be discussed in detail. %All these are discussed in subsections next. % \subsubsection{Personalized Speech Enhancement}
% personalized
Speech enhancement (SE) aims to isolate the speech of a targeted user from a group of others. Previously, neural networks have been employed in an attempt to achieve this goal. One such implementation utilized an audio embedding network to extract the audio embedding of different speakers, which was then utilized in a spectrogram masking network to produce an output with masks \cite{wang2018voicefilter}. This approach yielded a more efficient and faster model as the embedding for each speaker was computed in advance. The features extracted were subsequently employed in the PSE network for enhancing the speech signals of specific users, enabling the segregation of separate networks and allowing for further individual improvements \cite{wang2018voicefilter,wang2020voicefilter}. At inference, the speaker embedding is concatenated with the intermediate features of the PSE network for conditionality purposes. In another approach, audio signal embedding vectors representing the desired speaker were utilized to improve noise and echo cancellation and speech enhancement \cite{o2021conformer}. Another model, known as the Sound-Filter model, employed unlabeled data for speech enhancement. This wave-to-wave convolutional neural network was trained using mixtures generated from a collection of unlabeled audio recordings. It was assumed that speech was from a single source for the entire duration of a clip, based on the use of short intervals of audio signals for the same type of sound. This problem was approached as a one-shot learning challenge, resulting in models with conditioning encoder clusters that mapped acoustically similar sounds together in the embedding space \cite{gfeller2021one}. Additionally, activation functions were learned to personalise the output \cite{ramos2022conditioning}.

% \subsubsection{Speech Separation}
The application of speech separation is a crucial aspect of speech processing, and Recurrent Neural Networks (RNNs) were predominantly utilized for this purpose since their introduction. However, the trend has shifted towards the usage of Transformers as they enable parallel computation through the attention mechanism. The sequence-modeling capabilities of Transformers have the potential to enhance speech separation. An example of this approach is the proposed SepFormer model in \cite{subakan2021attention}. The authors leveraged the parallel computation benefits of Transformers in the implementation of their model. Furthermore, the utilization of Transformers in speech separation has been observed in other studies as well, such as the work of Zhang et al. \cite{zhang2021transmask}. The issue of increased computational complexity with longer sequences of sentences in the field of speech separation has been addressed through the utilization of the multi-scale group transformer (MSTG) approach. As reported by Zhao et al. \cite{zhao2021multi}, this approach leverages the self-attention capabilities of transformers and incorporates multi-scale fusion to capture long-term dependencies, thereby reducing computation complexity. The size of state-of-the-art models for speech separation tasks, however, often reaches hundreds of gigabytes, presenting a common challenge in the field. 

To mitigate this challenge, various approaches have been employed including Knowledge Distillation \cite{hinton2015distilling}. The Teacher-Student model \cite{chen2022ultra} has been utilized in an approach aimed at reducing the size and complexity of models for speech separation. Another approach reported in \cite{subakan2022resource} achieved this through the utilization of non-overlapping blocks in latent space and compact latent summaries calculated for each chunk. The authors developed the RE-SepFormer and achieved performance on par with existing state-of-the-art models. Another innovative approach, Tiny-Sepformer \cite{luo2022tiny}, uses a time-domain transformer neural network and achieved this by splitting Convolution Attention (CA) blocks and implementing parameter sharing within CA blocks.


\subsection{Spoken Dialogue Systems}
Table~\ref{table:sds-transformers} shows a list of related works on spoken dialogue systems using Transformer networks. But it should be noted that most of those neural architectures have been originally applied to text-based language processing tasks, not to speech data with some exceptions \cite{GulatiQCPZYHWZW20,DIET}. From the popularity chart in Fig.~\ref{fig:sds-transformers}, it can be noted that the most popular neural architecture is BERT (Bidirectional Encoder Transformer) \cite{DevlinCLT19} and the second most popular choice of architecture is either the original/vanilla transformer architecture \cite{vaswani2017attention} or GPT-2 (Generative Pre-Trained Transformer) \cite{radford2018improving}. Whilst BERT and GPT-2 are generalizations of the vanilla transformer networks, BERT uses encoder blocks (no decoder blocks) and bidirectional representations whilst GPT-2 uses decoder blocks (no encoder blocks) and left-to-right representations. Other generalizations of Transformers include the following: XLM (Cross-lingual Language Model) \cite{ConneauL19} to benefit from data in different languages; DistillBERT (Distilled version of BERT) \cite{DistilBERT} to train more compact models via knowledge distillation; ConvERT \cite{HendersonCMSWV20} to perform faster training via pre-trained response selection; BART (Bidirectional Auto-Regressive Transformers) \cite{LewisLGGMLSZ20} to pre-train sequence-to-sequence models via a denoising autoencoder (by predicting outputs without noise from noisy inputs); T5 (Text-to-Text Transfer Transformer) \cite{RaffelSRLNMZLL20} to learn from data in multiple language tasks by converting it to text-to-text format and then carrying out transfer learning to a specific language task; Conformer (Convolution-augmented Transformer) \cite{GulatiQCPZYHWZW20} to bring the advantages of Transformer and Convolutional neural nets into a single architecture suitable for audio sequences; DIET (Dual Intent and Entity Transformer) \cite{DIET} to perform language understanding of intents and entities in utterances without pretraining; GPT-3 (large Generative Pre-Trained Transformer) \cite{BrownMRSKDNSSAA20} to learn large language models without the need of fine-tuning; and RoBERTa (Robustly optimised BERT approach) \cite{LiuLSZ21} to learn language models with an improved methodology over BERT. Some other related architectures have been proposed and trained with text instead of speech data \cite{TransferTranfo,BudzianowskiV19,LiuWLXF20,ZhangSGCBGGLD20,HODChatbot,MadottoCW0LLF20,LinLHNGHFG21,RohmatillahC21,EppsEtAl,Sun0BRRCR21,RollerDGJWLXOSB21,WuJ22,AbroARUMQ22,GODEL,Jang0K22,MHC}.






\begin{figure}[!t]
\centering
\includegraphics[width=0.49\textwidth]{figures/Image-1.png}
%Popularity chart of 
\caption{Transformer-based architectures in Spoken Dialogue Systems.}
\label{fig:sds-transformers}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=0.49\textwidth]{figures/Image-2.png}
\caption{Tasks of Transformer-based Spoken Dialogue Systems.}
\label{fig:sds-tasks}
\end{figure}

From Fig.~\ref{fig:sds-tasks}, it can be noted that Transformer networks have been mostly applied to language understanding tasks (52\% of related works) including intent recognition and semantic tagging (a.k.a. slot filling). They are followed by turn-taking prediction and dialogue generation\footnote{Dialogue generation in this paper refers to generating a system response either word by word or by selecting a sentence from a pool of candidates). } (25\% of related works). Other less popular application areas (23\% of related works) include emotion recognition, performance prediction (of language understanding or user satisfaction), dialogue state tracking, punctuation prediction, disfluency detection, text normalization, and speech recognition for conversational data. The full set of tasks gives us an idea of the range of skills involved in a spoken dialogue system. 

\begin{table*}[!ht]
\scriptsize
\caption{Representative list of publications in Transformer-based \textbf{Spoken Dialogue Systems} in the period 2019-2023.}
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Author (year) & Architecture(s) & Task(s) & Dataset(s)\\
\hline
\hline
Takatsu et al. 2019 \cite{TakatsuYMHFK19} & BERT & Intent recognition & Majority Vote\\
\hline
Chen et al. 2019 \cite{SlotCarryOver}  & Vanilla Transformer & Dialog state tracking & DSTC2\\
\hline
Liu et al. 2019 \cite{BERT_BiLSTM} & BERT, BiLSTM & Intent recognition & CamRest\\
\hline
Korpusik et al. 2019 \cite{KorpusikLG19} & BERT & Semantic tagging & ATIS, Restaurants\\
\hline
Zhao H. Wang. 2019 \cite{ZhangW19} & Vanilla Transformer, CRF & Intent recognition, semantic tagging & ATIS, SNIPS\\
\hline
Quian et al. 2020 \cite{QianSZ20} & GPT-2 & Semantic tagging & DSTC2\\
\hline
Hong et al. 2020 \cite{HongKK20} & BERT & Semantic tagging & CamRest\\
\hline
Ekstedt et al. 2020 \cite{EkstedtS20} & GPT-2 & Turn-taking prediction & Maptask, Switchboard\\
\hline
Gopalakrishnan et al. 2020 \cite{0001HWLH20} & GPT-2 & Dialogue generation: writen vs. spoken & Topical-chat\\
\hline
Chen et al. 2020 \cite{ChenCLW20} & Vanilla/CT Transformer & Punctuation/disfluency prediction & IWSLT2011, in-house Chinese data\\
\hline
Hori et al. 2020 \cite{HoriMHR20} & Vanilla Transformer & ASR specialized for conversational data & Switchboard, HKUST\\
\hline
Lian et al. 2021 \cite{Lian0T21} & Vanilla Transformer & Emotion recognition in dialogue & IEMOCAP, MELD\\
\hline
Andreas et al. 2021 \cite{ComparativeStudy} & BART, GPT, BERT & Dialogue generation: model comparison & CamRest\\
\hline
Chapuis et al. 2021 \cite{ChapuisCMLC20} & BERT, hierarchical arch. & Emotion recognition, dialogue act recog. & SILICONE\\
\hline
Lai et al. 2021 \cite{TextNorm21} & BERT, RoBERTa & (Inverse) Text normalization & Google TN dataset\\
\hline
Kim et al. 2021 \cite{KimKSL21} & BERT & Intent recognition/classification & FSC, SNIPS, SmartLights\\
\hline
Lopez-Zorrilla et al. 2021 \cite{Lopez-ZorrillaT21} & GPT-2, RL & Dialogue generation: policy learning & DSTC2\\
\hline
Bechet et al. 2021 \cite{BechetRHAMD21} & BERT & SLU performance prediction & ATIS, MEDIA, SNIPS, M2M\\
\hline
Okur et al. 2022 \cite{OkurEtAl} & BERT, ConveRT, DIET & Intent recognition & Math-Game\\
\hline
Sakuma et al. 2022 \cite{SakumaFK22} & Conformer & End-of-utterance prediction & Japanese Restaurant Data\\
\hline
Lin et al. 2022 \cite{LinWHSSL22} & Vanilla Transformer & User-state/barge-in/backchannel detection & Chinese dialogues\\
\hline
Ahmed 2022 \cite{AhmedPhDThesis} & BERT & Intent recognition, semantic tagging & ATIS, SNIPS\\
\hline
Bekal et al. 2022 \cite{BekalEtAl} & BERT, HuBERT & Turn-taking prediction: barge-in detection & 12k chat-bot prompts\\
\hline
Dong et al. 2022 \cite{DongFZLW22} & BERT & Intent recognition/classification & FSC, SmartLights\\
\hline
Svec et al. 2022 \cite{SvecFBL22} & T5 & Intent recognition/classification & HHTT, TIA (in Czech)\\
\hline
Shen et al. 2022 \cite{ShenHZZX22} & Vanilla Transformer, TBM & User satisfaction prediction & Industry data: 18M/32M/5M samples\\
\hline
Yang et al. 2022 \cite{YangWZFCH22} & Vanilla Transformer & Turn-taking prediction & Commercial phone-dialogues\\
\hline
Sunder et al. 2022 \cite{SunderTKGKF22} & BERT & Dialogue act recognition & HarperValleyBank\\
\hline
Lopez-Zorrilla et al. 2022 \cite{Lopez-ZorrillaT23} & GPT-2, RL & Dialogue generation: policy learning & DSTC2\\
\hline
Firdaus et al. 2023 \cite{FirdausEC23} & mBERT, RoBERTa, XML & Intent recognition, semantic tagging & ATIS, TRAINS, SNIPs, FRAMES\\
\hline
Mei et al. 2023 \cite{MeiWTDH23} & BERT & Intent recognition, semantic tagging & ATIS, SNIPS\\
\hline
\end{tabular}

\label{table:sds-transformers}
\end{center}
\end{table*}

Regarding the performance of Transformers, BERT models have been shown to outperform their predecessor recurrent/convolutional neural nets in language understanding tasks (semantic tagging and intent recognition) \cite{BERT_BiLSTM,KorpusikLG19,HongKK20,FirdausEC23,MeiWTDH23}, text normalization \cite{TextNorm21}, performance prediction of language understanding \cite{BechetRHAMD21}, and turn-taking prediction \cite{BekalEtAl} â€“ though not always clearly and sometimes with only small differences. Using the popular stages of pre-training and fine-tuning has been reported to achieve better performance over using only one stage avoiding pre-training \cite{TakatsuYMHFK19,KimKSL21,DongFZLW22}. When extended in a hierarchical way to model word sequences at the low level, utterances at the mid-level, and dialogues at the high level, a hierarchical BERT is able to outperform its single model counterpart \cite{ChapuisCMLC20}. Hierarchical models are also trained with fewer parameters than their counterpart non-hierarchical models \cite{SunderTKGKF22}. Similarly, when combining specialised architectures such as DIET and ConVERT has shown improved results over using standard Transformers with pre-trained BERT embeddings \cite{OkurEtAl}. Even when combining more standard architectures such as BERT with biLSTMs (bidirectional Long-Short Term Memory Networks) has shown improved results over BERT-like baselines \cite{AhmedPhDThesis}. Moreover, BERT models have also shown to benefit from using data augmentation to obtain further gains over only pre-training and fine-tuning \cite{KimKSL21}. Furthermore, BERT models can also benefit from combining multiple modalities (such as audio and text) and using a cross-modal contrastive loss over using a single modality \cite{DongFZLW22}. 

Similarly, GPT-2 models have shown to outperform their predecessor recurrent neural nets in turn-taking prediction \cite{EkstedtS20}. GPT-2 models have also shown promising results by training a semantic tagger jointly with a speech recognizer and outperforming independent models \cite{QianSZ20}. When training GPT-2-based models on text data and testing on noisy data derived from speech recognition, the selected responses may be out of contextâ€”suggesting the need for training Transformer-based spoken dialogue systems using noisy data \cite{0001HWLH20}. The latter is supported by the work of \cite{Lopez-ZorrillaT21,Lopez-ZorrillaT23}, who extended GPT-2 pre-trained language models with audio embeddings in order to train models with improved performance over using only text-based representations. When extending GPT-2 models (among other Transformer-based architectures) with knowledge embeddings (KE) to port a knowledge base into the model parameters \cite{MadottoCW0LLF20}, the KE-enhanced models outperform the KE-unaware models \cite{ComparativeStudy}. In a similar vein, vanilla transformers have shown to outperform their predecessor recurrent neural nets (RNN) in tasks such as dialogue state tracking \cite{SlotCarryOver}, intent recognition and semantic tagging \cite{ZhangW19}, punctuation prediction and disfluency detection \cite{ChenCLW20}, speech recognition for conversational data \cite{HoriMHR20}, emotion recognition \cite{Lian0T21}, user satisfaction prediction \cite{ShenHZZX22}, and turn-taking prediction \cite{YangWZFCH22}. Whilst transformers have mostly reported positive improvements over RNN baselines across different spoken language processing tasks, \cite{LinWHSSL22} found no significant differences between them in the task of turn-taking prediction using a dataset of 10K labeled instances. 

Some recent models have shown further improvements over vanilla, BERT, or GPT-2 Transformers. The following are some examples. \cite{ComparativeStudy} have reported BART and T5 models to outperform vanilla and GPT-2 Transformers in the task of dialogue generation. \cite{OkurEtAl} have shown that models combining DIET and ConVERT are able to outperform vanilla transformers with BERT embeddings in the task of intent recognition. \cite{FirdausEC23} have found that multilingual multi-task BERT models are able to outperform strong baselines such as RoBERTa and XLM in the tasks of intent recognition and semantic tagging.

Other recent works in Transformer-based dialogue systems but using text data are also mostly based on BERT-based architectures \cite{LiuWLXF20,RohmatillahC21,EppsEtAl,Sun0BRRCR21,RollerDGJWLXOSB21,WuJ22,AbroARUMQ22} or GPT-based architectures \cite{TransferTranfo,BudzianowskiV19,ZhangSGCBGGLD20,MadottoCW0LLF20,GODEL,Jang0K22,MHC}. Notable large-scale efforts include GODEL \cite{GODEL} and InstructGPT \cite{InstructGPT}. GODEL (Grounded Open Dialogue Language Model) is framed as suitable for open-ended goal-directed dialogue, in its base and large versions containing 220M and 770M parameters, has been shown to outperform BART, T5, DialoGPT and GPT-3 across different benchmarks. Those results are based on automatic and human evaluations. InstructGPT, a fine-tuned GPT-3 model using reinforcement learning with human feedback, outperforms GPT-3 baselines that do not learn from human feedback. The latter results in a more compact model of 1.3 billion parameters being preferred over a larger model of 175 billion parameters. InstructGPT is the neural architecture of the widely known dialogue system ChatGPT\footnote{\url{https://openai.com/blog/chatgpt}}. 

Although previous works have shown remarkable progress in spoken and text-based language processing using Transformer networks, however, it is unclear which is the best neural architecture for a particular task or across tasks. Some works include a few baselines and some others have different baselines, and there is a wide variety of datasets being used---due to the number of tasks involved in dialogue systems. While performing such comparisons requires significant resources in terms of data and computing power, they would be highly valuable for establishing a clearer understanding of the current state-of-the-art in the field. Efforts such as GLUE in NLP \cite{WangSMHLB19} are needed in spoken dialogue systems. Nonetheless, the battle so far seems to be between BERT and GPT---and novel architectures performing even better remain to be discovered. While there have been some notable successes in conversational AI, the average citizen has yet to fully benefit from them. Many services, whether accessed over the phone or through the web, still rely on rudimentary methods, such as requiring people to fill out long web-based forms or endure long wait times to speak with customer representatives. However, this is likely to change as technologies such as Transformer-based text and spoken dialogue systems become more accessible and easier to deploy. Additionally, given that experimental results in this field continue to show room for improvement in the correctness and safety of generated responses, there is a need for more effective methods to be developed.


% the following limitations remain among others:
% \begin{itemize}



% \item \textit{Thorough comparisons across neural architectures}. From the works in Table~\ref{table:sds-transformers}, 

% \item \textit{Real-world and useful applications}. 
% \end{itemize}






\begin{table*}[!ht]
\centering
\caption{Recent studies on transformers for \textbf{Multi-Modal Applications}.}
\scriptsize
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Author (year)}  & \textbf{Datasets}    & \textbf{Performance}  & \textbf{Architecture(s)}    \\ \hline
\begin{tabular}[c]{@{}l@{}}Chuang et al.\\ 2019 \cite{chuang2019speechbert}\end{tabular} & \begin{tabular}[c]{@{}l@{}}LibriSpeech,\\ TIMIT,\\ SQuAD v2.01,\\ SWBD-Fisher\end{tabular}   & \begin{tabular}[c]{@{}l@{}}LibriSpeech: \\ WER 9.8\% (dev-clean),\\ 23.4\% (dev-other);\\ TIMIT: PER 14.7\% (test);\\ SQuAD v2.0:\\ F1 score 76.6\%,\\ EM score 69.8\%;\\ SWBD-Fisher: WER 10.9\%\end{tabular}  & \begin{tabular}[c]{@{}l@{}}BERT-base model with a speech encoder consisting of two \\convolutional layers and four self-attention layers  \end{tabular}   \\ \hline
\begin{tabular}[c]{@{}l@{}}Song et al.\\ 2019 \cite{song2019speech}\end{tabular}   & \begin{tabular}[c]{@{}l@{}}TIMIT; \\ WSJ\end{tabular}  & \begin{tabular}[c]{@{}l@{}}TIMIT: PER 12.5\% (test);\\ WSJ: \\ WER 11.4\% (dev93), 10.4\% (eval92)\end{tabular} & \begin{tabular}[c]{@{}l@{}}XLNet-base model with a speech encoder consisting of two \\convolutional layers and six self-attention layers.\end{tabular}  \\ \hline
\begin{tabular}[c]{@{}l@{}}Ao et al.\\ 2021 \cite{ao2021speecht5}\end{tabular} & \begin{tabular}[c]{@{}l@{}}LibriSpeech;\\ LibriTTS;\\ Common Voice;\\ TIMIT;\\ WSJ;\\ LJSpeech;\\ VCTK\end{tabular}  & \begin{tabular}[c]{@{}l@{}}LibriSpeech: WER 4.0\% (test-clean),\\ 10.9\% (test-other)1;\\ LibriTTS: MOS 4.011;\\ Common Voice: WER 6.8\% (en);\\ TIMIT: PER 9.7\% (test);\\ WSJ: WER 6.0\% (dev93),\\ 5.7\% (eval92)1;\\ LJSpeech: MOS 4.021; VCTK: MOS 3.97\end{tabular} & \begin{tabular}[c]{@{}l@{}}T5-base model with a speech encoder consisting of \\two convolutional layers and six self-attention layers.\end{tabular}   \\ \hline
\begin{tabular}[c]{@{}l@{}}Arjmand et al.\\ 2021 \cite{arjmand2021teasel}\end{tabular}   & CMU-MOSI & \begin{tabular}[c]{@{}l@{}}Accuracy score of 76\%,\\ F-score of 0.75,\\ MAE score of 0.94 on test set\end{tabular}    & \begin{tabular}[c]{@{}l@{}}A pre-trained language model such as BERT or RoBERTa\\ with a speech prefix consisting of two \\convolutional layers and one self-attention layer.\end{tabular}  \\ \hline
\begin{tabular}[c]{@{}l@{}}Sant et al.\\ 2022 \cite{sant2022multiformer}\end{tabular}    & \begin{tabular}[c]{@{}l@{}}MuST-C,\\ CoVoST v2\end{tabular}    & \begin{tabular}[c]{@{}l@{}}MuST-C: BLEU 25.9 (en-de),\\ 28.8 (en-es), 29.1 (en-fr),\\ 20.3 (en-it), 21.0 (en-nl),\\ 22.8 (en-pt), 18.9 (en-ro);\\ CoVoST v2:\\ BLEU 24.1 (es-en), 23.3 (fr-en)\end{tabular}   & \begin{tabular}[c]{@{}l@{}}A Transformer model with a head-configurable self-\\attention module that allows the use of different attention \\mechanisms in each head.  \end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Lin et al.\\ 2022 \cite{lin2022compressing}\end{tabular}  & \begin{tabular}[c]{@{}l@{}}LibriSpeech,\\VoxCeleb,\\VoxCeleb\end{tabular}
& \begin{tabular}[c]{@{}l@{}}LibriSpeech:\\ WER 3.0\% (test-clean),\\ 7.6\% (test-other);\\ VoxCeleb1: EER 1.67\%;\\ VoxCeleb2: EER 2.12\%\end{tabular}   & \begin{tabular}[c]{@{}l@{}}A simplified version of HuBERT with a \\convolutional encoder and a Transformer decoder \end{tabular}   \\ \hline
\begin{tabular}[c]{@{}l@{}}Chung et al.\\ 2018 \cite{chung2018speech2vec}\end{tabular}   & \begin{tabular}[c]{@{}l@{}}TIMIT,\\ Buckeye Corpus\end{tabular}  & \begin{tabular}[c]{@{}l@{}}TIMIT:\\ Accuracy 0.81 (skip-gram),\\ 0.80 (cbow);\\ Buckeye Corpus:\\ Accuracy 0.83 (skip-gram),\\ 0.82 (cbow)\end{tabular} & \begin{tabular}[c]{@{}l@{}}A sequence-to-sequence model with an RNN encoder\\ and decoder that learns fixed-length vector representations \\of speech segments.\end{tabular}   \\ \hline
\begin{tabular}[c]{@{}l@{}}Li et al.\\ 2019 \cite{li2019neural}\end{tabular} & \begin{tabular}[c]{@{}l@{}}LJSpeech,\\ Blizzard2012,\\ Blizzard2011\end{tabular} & \begin{tabular}[c]{@{}l@{}}LJSpeech:\\ MOS 4.13 Â± 0.08, MAE:0.13110;\\ Blizzard2012:\\ MOS 4.03 Â± 0.07, MAE:0.13610;\\ Blizzard2011:\\ MOS 4.01 Â± 0.07, MAE:0.138\end{tabular}  & \begin{tabular}[c]{@{}l@{}}A non-autoregressive Transformer model with a multi-\\head self-attention network and a feed-forward network for \\both encoder and decoder.\end{tabular} \\ \hline
\end{tabular}
\label{tab:Multimodal-Studies}
\end{table*}








\subsection{Multi-Modal Applications}
In human-computer interaction, a modality refers to the representation of a human sense using an individual channel of sensory input/output. The modalities in computing encompass vision, audition, reaction, gustation, and olfaction, among others. Multi-modal learning encompasses the use of multiple modalities in conjunction to solve various real-world applications, in much the same way as humans use their multiple senses to complete tasks. For instance, an image of a road sign board can provide an understanding of the type of sign being displayed, while the text on the board adds further context. To mimic this process in computers, data must be solved as separate problems, such as NLP and computer vision. Multi-Modal Learning (MML) provides a general approach to constructing robust models utilizing information from multi-modal data \cite{baltrusaitis2019multimodal}. In order to achieve generalized models in the real world, it is necessary to have excellent models of certain modalities and to use them in conjunction with one another, as some modalities are interdependent in context and deeper understanding, especially in speech processing and NLP-related problems and tasks \cite{zadeh2017tensor}. Transformers have demonstrated promise in achieving good generalization for multi-modal applications, as evidenced by the utilization of multi-model transformers in building AI models for classification \cite{lee2020parameter,nagrani2021attention}, segmentation \cite{strudel2021segmenter}, and cross-modal retrieval \cite{kim2021vilt}.

% In the field of machine learning, modality-specific models optimized for uni-modal benchmarks are commonly used. To address multi-modal problems, the prevalent approach has been to train models on modality-specific data and then concatenate the predictions in a ``late-fusion'' approach \cite{perez2017automatic, poria2016deeper}.

Recently, there has been a shift towards developing innovative fusion models and architectures. In 2019, the Multimodal Transformer (MulT) was proposed \cite{tsai2019multimodal} to address the issues of long-range dependencies across modalities and non-alignment of data with different sampling rates through the utilization of the attention mechanism in transformers. Another approach was presented in \cite{sun2019videobert} where the authors aimed to learn low-level representations by utilizing both visual and linguistic modalities to generate high-level features without explicit supervision. This model, built on top of BERT, utilized bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and speech recognition outputs respectively, producing state-of-the-art results. A similar approach was presented in \cite{huang2021unifying} with the proposal of a unified image-and-text generative framework based on a single multi-modal model to jointly study bi-directional tasks. The use of an Encoder architecture for learning generalized representations has gained significant popularity in recent times. A notable effort was made by Nagrani et al. \cite{nagrani2021attention} to implement this architecture for multi-modal applications, utilizing fusion bottlenecks for the integration of data from various modalities at multiple layers. This fusion process enabled the bottleneck layers to learn more comprehensive representations of the data, leading to improved performance and reduced computational expenses.

Self-supervised learning has also been utilized as a solution to multi-modal problems, with a majority of such work being focused on video and image applications. Zellers et al. \cite{zellers2021merlot} introduced a model, MERLOT, which leveraged self-supervised learning to acquire multi-modal script knowledge through observation of videos transcribed with speech. The model was pre-trained with both frame-level and video-level targets, allowing it to contextualize the data globally. This approach has gained widespread popularity in the video domain due to the abundance of video data available on the internet. Gabeur et al. \cite{gabeur2020multi} proposed a multi-modal transformer, which jointly encodes different modalities in video and facilitates their mutual attention, enabling the encoding and mapping of temporal information. A novel modification to the Encoder architecture was presented in the work of Chen et al. \cite{chen2020uniter}. The authors proposed a joint random masking technique applied to two modalities and utilized conditional masking for pre-training tasks, resulting in the creation of the UNITER model. Another self-supervised approach was introduced in the study by Akbari et al. \cite{akbari2021vatt}, where their model, VATT, took raw input and extracted multi-modal representations through a tokenizer layer, embedding layer, and transformer. This approach leveraged the attention mechanism of transformers to learn the representations of data, producing a model that was more robust for visual and language tasks than prior modality-specific models. The popularity of adversarial learning in creating robust models was also applied, as demonstrated in the work of Li et al. \cite{li2020closer}, where an adversarial learning model was implemented on noise input to produce the improved MANGO model on top of UNITER. This approach achieved state-of-the-art results in terms of robustness benchmarks.

% The design of multi-modal systems presents two significant challenges, inter-modality dynamics and intra-modality dynamics \cite{zadeh2017tensor}. Inter-modality dynamics refer to the ambiguity among decisions made by individual modalities and disparities among them when comparing unimodal, bimodal, trimodal, etc. The intra-modality dynamics pose difficulties when comparing different types, such as formal/informal. An effort was made to address this issue through the introduction of the Tensor Fusion Network (TFN) model, which learned both the intra-modality and inter-modality dynamics end-to-end, rather than through concatenation or fusion at the end. The authors employed an LSTM before a deep neural network to learn across different modalities. The use of Transformers for sequential modeling in multi-modal systems is well-established, due in part to their non-recurrent architecture. The attention mechanism in Transformers allows for information to be learned across a sequence, making them a suitable candidate for multi-modal applications. In this regard, a new model, referred to as the Factorised Multi-modal Transformer (FMT) \cite{zadeh2019factorized}, was proposed, which incorporates the use of Transformers for multi-modal systems. FMT enables asynchronous modeling of both intra-modal and inter-modal dynamics, and its architecture consists of an embedding layer, followed by Multi-modal Transformer Layers (MTL), each of which contains multiple Factorised Multimodal Self-attentions (FMS) that account for intra-modal and inter-modal factors within the multi-modal input. This new model, FMT, has been shown to improve upon existing state-of-the-art models.

The application of Transformers in multi-modal systems is made feasible by their non-recurrent architecture, which enables sequential modeling. The attention mechanism of Transformers allows for learning across a sequence. The Factorised Multi-modal Transformer (FMT) \cite{zadeh2019factorized} is a new model that makes use of Transformers in multi-modal applications and offers an improvement over existing state-of-the-art models through asynchronous modeling of both intra-modal and inter-modal dynamics. The input in the architecture is first passed through an embedding layer, followed by Multi-modal Transformer Layers (MTL), where each MTL comprises multiple Factorised Multimodal Self-attentions (FMS) that factor in inter-modal and intra-modal aspects of the multi-modal input. The authors of FMT conducted evaluations of its zero-shot task performance and examined if the model learns general representations from pre-trained models. Moreover, they demonstrated that a reduction in the model's losses does not always translate to expected performance gains in multi-modal Transformers. Research has shown that multi-modal Transformer models outperform deeper models with modality-specific attention mechanisms when compared with modality-specific models \cite{hendricks2021decoupling}.


% \section{Challenges and Future Directions} \label{sec:challenges}

% \begin{enumerate}

 
%  \item \SL{However, the parallel computation increases the difficulty while learning the alignment between text and speech in transformers, which is further magnified in the multi-speaker scenario with noisy data and diverse speakers, and hinders the applicability of transformer for multi-speaker TTS \cite{chen2020multispeech}}
 
 

% \end{enumerate}




 

% \section{Summary and Future Pointers}
% \label{summary}
\section{Challenges and Future Work}
\label{sec:challenges}

\subsection{Training Challenges}
Transformers have been proven very effective in speech-related tasks as presented in Section \ref{sec:applications}. However, transformers' training is complex and requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers \cite{liu2020understanding}. The challenge in terms of applying self-attention to speech recognition is that individual speech frames are not like lexical units such as words. Speech frames do not convey distinct meanings or perform unique functions, which makes it hard for the self-attention mechanism to compute proper attentive weights on speech frames. Considering that adjacent speech frames could form a chunk to represent more meaningful units like phonemes, some sort of pre-processing mechanisms such as convolutions to capture an embedding for a group of nearby speech frames would be helpful for self-attention. Transformers were originally proposed for machine translation, where sequence lengths are short in contrast to speech technology. For instance, sequence lengths in SER are larger and contain a few thousand frames. Self-attention encoders in transformers have quadratic computational complexity and computation of self-attention between all the pairs of frames is expensive to compute. In addition, speech sequences are less informationally dense compared to the word sequences in textual data. Therefore, researchers exploit tricks including time-restricted self-attention \cite{povey2018time}, truncated self-attention \cite{yeh2019transformer}, down-sampling \cite{sperber2018self}, sub-sampling \cite{salazar2019self} and pooling \cite{dong2019self} are being used as transformers in speech technology to tackle sequence length problems.   Positional encoding is another main component in transformers to include a piece of positional information about each word about its position in the sentence. This helps the transformers to capture longer dependencies in sequential data. The original paper utilized sinusoidal position encoding, which can hurt performance in speech-based systems due to longer sequences \cite{bie2019simplified} and generalize poorly in certain conditions \cite{likhomanenko2021cape}. Different approaches \cite{mohamed2019transformers,likhomanenko2021cape} have been explored to address this issue. However, these approaches are exploited in ASR and further research is required in other speech-related domains. 




\subsection{Computational Cost and Efficiency}
Recently, transformer-based end-to-end models have achieved great success in many speech-related areas. However, compared to LSTM models, the heavy computational cost of the transformer during inference is a key issue to prevent their applications \cite{lu2020exploring}. The computational cost of the Transformer Transducer grows significantly with respect to the input sequence length, which obstacles the practical use of T-T. Recently conformer Transducer (C-T) \cite{gulati2020conformer} was proposed to further improve T-T, but it is not streamable because its encoder has attention on full sequence \cite{chen2020developing}. However, it requires access to the full sequence, and the computational cost grows quadratically with respect to the input sequence length. These factors limit its adoption for streaming applications \cite{wu2020streaming}. 
% One of the main challenges is the high computational cost of the transformer model when applied to speech signals, which have long sequence lengths compared to text. This makes it difficult to train and even at inference time. % with large models on large datasets, and the transformer model requires a fixed-length input representation, which is not suitable for speech signals that have variable lengths depending on the speaker and utterance. 
% Moreover, the transformer model suffers from data sparsity issues, particularly for low-resource languages or domains, limiting its generalization ability and performance on unseen data. 
Additionally, transformers' high memory consumption and inference time pose practical difficulties for deploying and updating large-scale models. 

% Several solutions have been proposed to address the above challenges.
Self-attention mechanism in transformers has quadratic complexity with respect to sequence length, limiting scalability for long sequences. To address this issue, several solutions, such as sparse attention patterns \cite{zhao2022adaptive,woo2021speech,avinava2021constructing}, low-rank factorization \cite{winata2020lightweight}, random feature maps \cite{krzysztof2020rethinking}, and locality-sensitive hashing \cite{kitaev2020reformer,woo2021speech,roy2021efficient}, have been proposed \cite{tay2022efficient}. The memory consumption of transformers grows linearly with sequence length and quadratically with hidden dimension size, creating challenges for large-scale data training and inference. Some solutions to this problem include reversible residual connections \cite{yu2022auxiliary,duan2022dual}, gradient checkpointing \cite{walmart2021sparse}, weight sharing \cite{xiao2019sharing,han2021connection}, or parameter pruning \cite{li2021differentiable,li2019improving} to save memory. Chen et al. \cite{chen2021developing} also show the use of streaming processing and early stopping to reduce latency and run-time cost in speech models. Lin et al. \cite{lin2022compressing} used weight pruning, head pruning, low-rank approximation, and knowledge distillation to reduce parameters. Parallelizing and accelerating transformer models on different hardware platforms may encounter challenges such as load imbalance, communication overhead, or memory fragmentation \cite{shi2021emformer}. Several solutions have been proposed to improve hardware utilization, including tensor decomposition \cite{ma2019tensorized,gu2022heat,pham2022tt,li2022hypoformer}, kernel fusion \cite{denis2022accelerated}, mixed precision arithmetic \cite{xu2021mixed}, or hardware-aware optimization \cite{wang2020hat,kuchaiev2018mixed}.


% Pre-trained transformer models may not be efficient for all downstream tasks due to different data distributions. Knowledge distillation and few shot learning are explored in the literature as potential solutions to address this issue. 

Efficiency is another major concern for Transformers due to their large and complex architectures. When these models are pre-trained, they may not be efficient for all downstream tasks due to different data distributions. To improve efficiency, recent efforts have attempted to find solutions to use fewer training data and/or parameters. These solutions include knowledge distillation, simplifying and compressing the model, using asymmetrical network structures, improving utilization of training samples, compressing and pruning the model, optimizing the complexity of self-attention, optimizing the complexity of self-attention-based multimodal interaction/fusion, and optimizing other strategies. Several specific methods have been proposed to address these issues, including knowledge distillation by Miech et al. \cite{miech2021thinking} and Touvron et al. \cite{touvron2021training}, model simplification by Xu et al. \cite{xu2021e2e}, Kim et al. \cite{kim2021vilt}, and Akbari et al. \cite{akbari2021vatt}, weight-sharing by Wen et al. \cite{wen2021cookie} and Lee et al. \cite{lee2020parameter}, training with fewer samples by Li et al. \cite{li2021supervision}, compressing and pruning the model by Gan et al. \cite{gan2022playing}, optimizing the complexity of self-attention by Child et al. \cite{child2019generating} and Transformer-LS \cite{gan2021transformerls}, optimizing the complexity of self-attention based multimodal interaction/fusion by Nagrani et al. \cite{nagrani2021attention} and optimizing other strategies by Yan et al. \cite{yan2022multiview}. These efforts demonstrate the importance of addressing efficiency in the development of Transformers.

% model parameter capacity makes Second, their time and memory complexities grow quadratically with input sequence length, which is caused by self-attention. In multimodal contexts, this calculation explosion becomes worse due to high-dimensional joint representations. 
% Some techniques have been proposed to improve the efficiency and reduce the computational cost of transformers. For instance, a linear attention mechanism that replaces softmax attention with kernel feature maps reduces quadratic complexity to linear complexity. A dynamic convolutional layer that adapts its kernel size according to the input length reduces redundant computations for short inputs, as claimed by Alastruey et al. \cite{alastruey2021efficient}. Furthermore, the complexity of the Transformer model, with a large number of parameters and a large size, makes training and inference on limited computational resources difficult. To overcome this, the batch size can be reduced, and the gradient steps can be increased.  A sliding window approach and gradient checkpointing can also be employed to reduce computational costs. 

% In addition, data scarcity is a challenge for prosodic segmentation tasks due to limited and noisy datasets, negatively affecting data-hungry models such as Transformers. To address this, data augmentation techniques such as pitch shifting, time stretching, and noise injection can be employed to increase the diversity and robustness of the training data. Another solution is to use pre-trained models such as Whisper \cite{whisper2021}, which have learned general speech features from large-scale unlabeled data \cite{ma2020data} \cite{park2019specaugment}.



% In summary, several challenges exist in the efficient deployment of Transformers in speech applications, including high computational cost, efficiency, data scarcity, and complexity issues. However, various solutions, including data augmentation, pre-training, hardware optimization, and network compression, have been proposed to resolve these. 

% Moreover, some techniques have been proposed to improve the efficiency and reduce the computational cost of transformers. For instance, a linear attention mechanism that replaces softmax attention with kernel feature maps reduces quadratic complexity to linear complexity. A dynamic convolutional layer that adapts its kernel size according to the input length reduces redundant computations for short inputs, as claimed by Alastruey et al. \cite{alastruey2021efficient}.In summary, several challenges exist in the efficient deployment of Transformers in speech applications, including high computational cost, efficiency, data scarcity, and complexity issues. However, various solutions, including data augmentation, pre-training, hardware optimization, and network compression, have been proposed to resolve these.


\subsection{Large Data Requirements}


%Speech recognition models based on transformers face a significant challenge related to data requirements, which is the limited availability of labeled data for training speech models. The highly diverse nature of speech data, with variations in languages, accents, noises, and speakers, makes it expensive and time-consuming to collect and annotate large amounts of high-quality labeled data \cite{radford2022robust}. The scarcity of labeled data can cause speech models to struggle with generalization across different domains, languages, and tasks. This issue can be particularly severe in low-resource settings, where there may be a shortage of labeled data for a specific language or dialect.

One significant challenge faced by transformers-based speech models is the requirement for a large amount of data for effective training. 
% Speech recognition tasks involve long sequences of audio signals with complex patterns and dependencies across time, frequency, and domains. These sequences require a lot of data to capture, making it challenging to train speech models efficiently \cite{chen2022wavlm}. 
While recent text-based conversational AI models have been trained on large amounts of data (e.g., GODEL, which used around 800GB of data, and GPT-3, which was pretrained on 570GB of data), the amount of speech data available for training models for spoken technology is more limited. For instance, the works in Table~\ref{table:sds-transformers} (with some exceptions) use datasets consisting of several thousand spoken dialogues for various tasks, which is far less compared to the hundreds of gigabytes of text data used by text-based models. This highlights the need to either develop more efficient ways of creating datasets to train speech-related systems more efficiently.

One approach to enhancing the performance of transformer-based models for speech recognition is to collect a large-scale dataset of multilingual and multitask audio data from various sources \cite{radford2022robust}. This dataset can then be used to train a transformer-based model with self-attention layers using weak supervision. Evaluating the model on various downstream tasks and benchmarks can further improve its performance. Additionally, data augmentation and transfer learning can also be used to improve model performance \cite{chen2022wavlm}. Data augmentation techniques such as pitch shifting, time stretching, noise injection, SpecAugment, etc.,  can be employed to increase the diversity and robustness of the training data. Another solution is to use pre-trained models by training them on learning to learn generalised representation from large-scale unlabeled data \cite{ma2020data} \cite{park2019specaugment}. These models can be fine-tuned using few-shot learning to get better performance on downstream tasks. 
% The computational requirements are particularly significant for transformer architectures, which are designed to process large amounts of data. This means that speech recognition models based on transformers require a significant amount of computational resources to achieve good performance, making them difficult to train and deploy in low-resource settings.
Multi-task learning approaches can also be utilized  to enhance the performance of transformer-based models for speech and language processing with smaller datasets \cite{chen2022wavlm}. This includes masked acoustic modeling, contrastive predictive coding, speaker classification, emotion recognition, and sentiment analysis. 
% Applying data augmentation techniques, such as SpecAugment, speed perturbation, volume perturbation, and reverberation, can further improve the model's performance. Evaluating the model on various downstream tasks, such as automatic speech recognition, speaker verification, emotion recognition, sentiment analysis, and natural language understanding, can also help improve its performance.

% To reduce the computational complexity and memory consumption of transformer-based models, several techniques can be employed. One approach is to use a hybrid architecture \cite{lu2020exploring} that combines convolutional neural networks with transformers. Applying layer-wise adaptive learning rate scaling to optimize transformer parameters and incorporating relative positional encoding into transformers can also help reduce computational complexity. Finally, employing SpecAugment data augmentation technique can also help improve model performance \cite{lu2020exploring}.


\subsection{Generalization and Transferability}
Transformers face challenges with generalization and transferability that can affect their ability to handle a broader range of tasks and scenarios. One of the main issues with generalization is the absence of inductive biases in pure transformers, which makes them heavily reliant on large-scale training data for optimal performance \cite{dosovitskiy2020image}. This can lead to poor performance on downstream tasks if the training data quality is poor. Additionally, transformers lack built-in biases, unlike convolutional neural networks, which makes it more difficult for them to generalize to new tasks or scenarios \cite{dosovitskiy2020image}. 
% The fixed-length positional embedding and self-attention mechanism of transformers can also make them struggle with long sequences \cite{zhou2019improving}. 
To address the challenge of poor generalization on new domains, Xue et al. \cite{xue2021bayesian} proposed the Bayesian Transformer Language Model (BTLM), which integrates a Bayesian framework to enhance the model's ability to handle out-of-domain data.  Bayesian Transformer \cite{xue2021bayesian} uses variational inference to estimate the latent parameter posterior distributions and account for model uncertainty while another model \cite{lin2022compressing} resolves generalization issue by applying several compression techniques, such as weight pruning, head pruning, low-rank approximation, and knowledge distillation, to a 12-layer Transformer model trained with contrastive predictive coding (CPC). This approach delivers improved performance on out-of-domain data compared to traditional language models. Other proposed solutions include Parallel Scheduled Sampling (PSS) and Relative Positional Embedding (RPE) \cite{zhou2019improving}. PSS improves robustness and reduces exposure bias by randomly sampling tokens from either ground truth or predicted sequences during training. RPE encodes relative distances between tokens rather than absolute positions, which enhances the modeling capability for long sequences. Various other papers also discuss the issues and solutions of generalisation in transformer-based speech models along with various solutions. Zhou et al. \cite{zhou2019improving} propose a text-to-speech model, GenerSpeech that transfer unseen style features from an acoustic reference to a target text. It improves generalization by decomposing the speech variation into style-agnostic and style-specific parts, and by using a content adaptor with Mix-Style Layer Normalization to eliminate style information in the linguistic content representation.

Transferability is also a significant challenge for transformers due to domain gaps.  Several methods have been proposed to enhance the transferability of  transformers, including data augmentation, adversarial perturbation strategies, learning a shared embedding space, and knowledge distillation \cite{gan2020large,radford2021learning,kervadec2021supervising}. While these methods have shown promising results, there are still some obstacles to transferability for multimodal applications. One of the challenges is the distribution gap between training data and practical data, as shown by Zhan et al. \cite{zhan2021product1m}. They demonstrate that transferring supervised multimodal transformers pre-trained on well-aligned cross-modal pairs/tuples to weakly aligned test data is challenging. Rahman et al. \cite{rahman2020integrating} and Xia et al. \cite{xia2021xgpt} show that transferring multimodal transformers across different tasks requires careful adaptation and fine-tuning. In multi-language data settings, transformers also face transferability challenges as demonstrated by Zhou et al. \cite{zhou2021uc2} and Ni et al. \cite{ni2021m3p}. %They show that transferring multimodal transformers across different languages requires additional cross-lingual alignment and pretraining.



% Cross-lingual gap due to different languages and cultures is also a major issue, 

% Overall, addressing the generalization and transferability challenges of transformers is crucial to enhance their effectiveness in handling a broader range of tasks and scenarios. The proposed solutions, such as the BTLM, PSS, RPE, data augmentation, adversarial perturbation strategies, learning a shared multimodal embedding space, and knowledge distillation, provide promising ways to enhance the generalization of transformers to new domains and improve their performance on challenging tasks. However, more research is needed to overcome the remaining challenges and achieve more robust and effective models.


\subsection{Multimodal Training}
In Multimodal Learning (MML) Transformers, a fusion of information across multiple modalities is typically achieved at three conventional levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion) \cite{chen1998audio}. Middle fusion can be achieved by directly feeding the representations of two modalities into the standard attention module, which is followed by latent adaptation and ends up with a late fusion of final bimodality representations \cite{tsai2019multimodal,sahu2020low}. This idea can be extended by alternating or compounding with unimodal attention, or token exchange across modalities \cite{gao2019dynamic,li2021causal,chen2021crossvit}. On the other hand, inspired by the success of BERT, different modalities can integrate as early as at the input stage \cite{sun2019videobert,li2019visualbert,chen2020uniter,li2020unicoder,li2020oscar,huang2020pixel,zhu2020actbert,qi2020imagebert,nagrani2021attention,zhuge2021kaleidobert}. These models are known as one-stream architecture, which allows the adoption of the merits of BERT with minimal architectural modification. However, a major difference with these one-stream models is the usage of problem-specific modalities with variant masking techniques. A noticeable fusion scheme is introduced based on a notion of bottleneck tokens, which applies for both early and middle fusion by simply choosing to-be-fused layers \cite{nagrani2021attention}. Late fusion based on prediction is less adopted in MML Transformers as the focus is on learning stronger multimodal contextual representations \cite{chen1998audio,owens2018audio}. The interaction between modalities can be explored further for enhancing and interpreting the fusion of MML \cite{liu2021probing}.

Another issue with multimodal transformer models in real-world scenarios is that the data often exist in multiple modalities with intrinsic synchronization, such as audio-visual correspondence \cite{morgado2020learning}, which forms the basis for cross-modal alignment. Recently, there has been a surge of interest in leveraging large quantities of web data (e.g., image-text pairs) for vision and language tasks using Transformers-based alignment \cite{radford2021learning,jia2021scaling,xu2021videoclip,lei2021less}. The approach involves mapping two modalities into a common representation space with contrastive learning over paired samples and is typically implemented using massive multimodal models (MML) that are expensive to train. As a result, subsequent works have focused on utilizing pre-trained models for various downstream tasks \cite{wang2022clip,li2022align,luo2022clip4clip,fang2021clip2video,narasimhan2021clip}. These alignment models are capable of zero-shot transfer, particularly for image classification via prompt engineering, which is remarkable given that image classification is traditionally considered a uni-modal learning problem, and zero-shot classification remains a difficult challenge despite extensive research \cite{xian2018zero}. The approach has also been extended to more challenging and fine-grained tasks, such as object detection \cite{gu2021open}, visual question answering \cite{tan2019lxmert,chen2020uniter,li2020oscar,liu2021align}, and instance retrieval \cite{liu2021gilbert,liu2021align}, by introducing region-level alignment, which incurs additional computational costs from explicit region detection. TEASEL \cite{arjmand2021teasel} uses a speech-prefixed language model that takes speech features as input and predicts masked tokens in text and resolves issues with multimodal training. 

% Recent ideas for mitigating these costs include random sampling \cite{huang2020pixel}, learning concept dictionary \cite{kim2021seeing}, uniform masking \cite{liu2021e2e}, patch projection \cite{kim2021vilt}, joint learning of a region detector \cite{likhosherstov2021polyvit}, and representation aligning before mask prediction \cite{liu2021align}. Cross-modal alignment is a critical capability for numerous real-world applications. For example, Transformer-based MML models have been designed for speaker localisation in multi-speaker videos \cite{liu2021right}, speech translation \cite{zheng2021fused}, text-to-speech \cite{chen2020multispeech}, text-to-video retrieval \cite{wang2021t2vlad}, and visual grounding of natural language \cite{zhang2021explainable}. Furthermore, when several modalities are distributed across tasks and trained in a multi-task manner, task-level representation alignment can be achieved with high parameter efficiency \cite{lu2020multi, likhosherstov2021polyvit}.



% Another significant issue with the multi-modal transformer models is the potential oversight of specific modalities, which can impact the performance of the model in real-world scenarios where complete modality data may not always be available. To address this challenge, multi-task optimization has been proposed as a solution to enhance the robustness of transformer models. Additionally, studies evaluating the integration of cross-modal information by multi-modal models have revealed the models' asymmetric nature in processing different modalities. Specifically, these models had much greater difficulty in predicting text when visual information was removed, compared to predicting visual object categories when the text was removed, highlighting the need for further research in this area. \cite{ma2022multimodal, frank2021vision}.

\subsection{Robustness}

Despite their widespread adoption in speech processing applications, transformers exhibit sensitivity to domain shifts and noise in speech data leading to a sub-optimal performance in downstream tasks. Additionally, these models may not generalize well to other languages when trained solely on monolingual data~\cite{kawakami2020learning}.
This issue has been identified as one of the causes of performance degradation in transformer-based ASR system~\cite{burchi2023audio}, speech-to-animation models~\cite{novoselov2022robust}, speaker recognition and speech-to-speech translation models~\cite{jia2021translatotron} as raw speech features used as input render these models sensitive to noise and speaker variations. Moreover, the performance of these models is negatively impacted by the lack of prosody information, as they do not explicitly model it.


%Speech-to-animation (S2A) models based on transformers are also susceptible to lack of robustness and efficiency. Raw speech features as input make these models sensitive to noise and speaker variations, and they do not model prosody information explicitly. Speaker recognition, which identifies or verifies a speaker's identity from speech signals, faces the same sensitivity to noise and variability caused by channel distortion, background noise, or speaker emotion, and requires a large amount of labeled data for supervised training \cite{novoselov2022robust}. Similarly, speech-to-speech translation (S2ST) models based on transformers encounter challenges in terms of robustness and quality\cite{jia2021translatotron}. 


%Transformers have seen widespread adoption in natural language processing tasks, but their application to speech representation learning poses several challenges that impact their robustness. One of the most significant difficulties is their sensitivity to domain shifts and noise in speech data, leading to poor performance in downstream tasks. These models also demand large amounts of labeled data for supervised fine-tuning, which may not be feasible for many languages or domains. It is also worth noting that transformers may not generalize well to other languages or accents when trained on monolingual data \cite{kawakami2020learning}. Automatic speech recognition (ASR) systems based on neural networks face similar challenges and still struggle with performance degradation caused by noisy or distorted speech signals, and the reliance on a substantial amount of hand-labeled data and computational resources to train deep neural networks \cite{burchi2023audio}.

%Speech-to-animation (S2A) models based on transformers are also susceptible to lack of robustness and efficiency. Raw speech features as input make these models sensitive to noise and speaker variations, and they do not model prosody information explicitly. Additionally, these models require large-scale datasets with high-quality annotations of facial landmarks or expressions, which are costly and time-consuming to obtain \cite{chen2022transformer}. Speaker recognition, which identifies or verifies a speaker's identity from speech signals, faces the same sensitivity to noise and variability caused by channel distortion, background noise, or speaker emotion, and requires a large amount of labeled data for supervised training \cite{novoselov2022robust}. Similarly, speech-to-speech translation (S2ST) models based on transformers encounter challenges in terms of robustness and quality\cite{jia2021translatotron}. 
%These models usually involve a cascade of separate models for speech recognition, machine translation, and speech synthesis, which introduces errors and latency at each stage. Furthermore, they do not model the speaker identity and prosody information of speech signals explicitly, resulting in unnatural or inconsistent output speech. Additionally, they require vast amounts of parallel data for supervised training, which might not be feasible for many language pairs or domains \cite{jia2021translatotron}.

%Machine learning models are inherently brittle, and small changes in the training data can result in different predictions. Data poisoning, where an attacker can manipulate the training data to evade detection or cause misclassification, is a significant concern for many applications. The current techniques to verify the robustness of machine learning models focus on adversarial examples, where an attacker can perturb a single input at test time to change its prediction. However, these techniques are not sufficient to capture data poisoning attacks, where an attacker can modify multiple inputs at training time to affect many predictions \cite{drews2020proving}.

Several solutions have been proposed to overcome these challenges. One approach is to learn robust and multilingual speech representations using contrastive learning, which is a self-supervised technique that encourages the model to distinguish between similar and dissimilar speech segments. A multilingual phonetic vocabulary is used to capture cross-lingual similarities and enable transfer learning across languages. Additionally, self-training and semi-supervised learning are applied to leverage unlabeled data and enhance the quality of the representations. These approaches have been shown to outperform previous methods on various benchmarks and achieve state-of-the-art results on low-resource speech recognition \cite{kawakami2020learning}. To address the challenges associated with noisy or distorted speech signals, a novel audio-visual ASR model has been proposed, leveraging both speech and lip movement information to improve recognition accuracy and robustness. The model is based on the Efficient Conformer architecture, which combines convolutional neural networks. To overcome the challenges faced by speaker recognition models, an unsupervised approach that uses a contrastive loss to learn speaker embeddings directly from raw speech signals has also been proposed. The model uses a multi-task learning approach, where both speaker recognition and ASR tasks are learned jointly. This approach can help to overcome the variability in speech signals caused by different speakers and speaking styles, thereby improving the robustness of the ASR system.

%In addition to these advances, recent research has also explored the use of transfer learning to improve ASR performance on low-resource languages. By leveraging pre-trained models on high-resource languages and fine-tuning them on low-resource languages, it is possible to achieve significant improvements in ASR accuracy, even with limited training data. This is particularly important for applications such as speech-to-text translation, which require accurate transcription of speech in multiple languages. Overall, these recent developments in ASR hold great promise for improving the accuracy and robustness of speech recognition systems, enabling a wide range of new applications and use cases.




\section{Summary and Conclusions} \label{sec:conclusion}

The transformer architecture has emerged as a highly effective neural network architecture in the field of speech processing due to its ability to handle sequential data for various speech-related tasks. The popularity of transformers has been further accelerated by the availability of specialized libraries for transformer-based speech-processing tasks. The key innovation of transformers lies in their ability to capture long-range dependencies among input sequences using self-attention layers, and its effectiveness has been demonstrated in various speech processing tasks such as automatic speech recognition, text-to-speech synthesis, speaker recognition, multi-microphone processing, and speech translation. 

This pioneering paper presents the first detailed and comprehensive survey of the applications of transformers in the audio domain. Our review shows that transformers are increasingly becoming popular in the speech-processing community. The main findings of this paper are summarized below.

\begin{itemize}
\item Transformers provide a competitive alternative to Recurrent Neural Network (RNN)-based models in Speech Processing tasks and have shown promising results in Automatic Speech Recognition (ASR) and Text-to-Speech (TTS). Transformers use self-attention layers to capture long-range dependencies among input sequences, allowing more parallelization than RNNs.
\item Pre-training with self-supervised learning techniques like wav2vec and data2vec can lead to faster convergence and performance boost in transformers.
\item Hybrid models that combine transformers with conventional acoustic modeling techniques can improve word error rates and reduce computational complexity.
\item Attention should be paid to overfitting and generalization problems in transformer models in ASR and TTS.
\item Different modeling units such as syllables and phonemes should be compared in ASR using transformers.
\item Multi-head attention mechanisms in transformers can improve parallelization by solving the long-distance dependency problem in end-to-end TTS architectures.
\item Length regulator based on duration predictor can be used to solve the issue of sequence length mismatch in TTS.
\item Data augmentation and contrastive learning techniques can be used to build cross-lingual or multilingual speech recognition systems using pre-trained transformer models.
\item Ethical concerns around privacy, bias, and fairness should be considered when developing and deploying speech processing systems using transformers.
\item Robustness of transformer models to adversarial attacks and out-of-distribution inputs should be carefully evaluated and addressed in ASR and TTS applications.
%\item Further research is needed to explore the benefits of transformers and pointers for future ASR and TTS systems.

\end{itemize}

Future research on cross-lingual/multilingual systems is needed to address the performance issues highlighted in this review. These recommendations are intended for researchers and developers in the field of speech processing. 

% Transformers have shown promise in ASR and TTS, and pre-training with self-supervised learning can boost performance. Comparing different modeling units and using multi-head attention and length regulators can improve ASR and TTS. Overfitting, generalization, and 
% With ongoing research and development in the field, Transformers are expected to continue playing a critical role in speech processing in the years to come.
%In this work, we have focused on presenting a comprehensive review of the applications of transformers in the audio domain. This literature review shows that transformers are increasingly becoming popular in the speech-processing community and are slowly replacing recurrent and LSTM networks. 
% Following is a brief summary of suggestions and recommendations we discussed in our paper

% By considering these recommendations, future ASR and TTS systems can improve their performance and efficiency.


\section{Acknowledgements}
We would like to thank Fatima Seemab (NUST) for initially working on this paper.




% \bibliographystyle{IEEEtran}
% \bibliography{Reference}


% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{100}

\bibitem{karita2019comparative}
S.~Karita, N.~Chen, T.~Hayashi, T.~Hori, H.~Inaguma, Z.~Jiang, M.~Someki,
  N.~E.~Y. Soplin, R.~Yamamoto, X.~Wang \emph{et~al.}, ``A comparative study on
  transformer vs {RNN} in speech applications,'' in \emph{2019 IEEE Automatic
  Speech Recognition and Understanding Workshop (ASRU)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2019, pp. 449--456.

\bibitem{lin2022survey}
T.~Lin, Y.~Wang, X.~Liu, and X.~Qiu, ``A survey of transformers,'' \emph{AI
  Open}, 2022.

\bibitem{tay2022efficient}
Y.~Tay, M.~Dehghani, D.~Bahri, and D.~Metzler, ``Efficient transformers: A
  survey,'' \emph{ACM Computing Surveys}, vol.~55, no.~6, pp. 1--28, 2022.

\bibitem{zhang2020transformer}
Q.~Zhang, H.~Lu, H.~Sak, A.~Tripathi, E.~McDermott, S.~Koo, and S.~Kumar,
  ``Transformer transducer: A streamable speech recognition model with
  transformer encoders and {RNN-T} loss,'' in \emph{ICASSP 2020-2020 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 7829--7833.

\bibitem{wolf2019huggingface}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz \emph{et~al.}, ``Huggingface's transformers:
  State-of-the-art natural language processing,'' \emph{arXiv preprint
  arXiv:1910.03771}, 2019.

\bibitem{wolf2020transformers}
------, ``Transformers: State-of-the-art natural language processing,'' in
  \emph{Proceedings of the 2020 conference on empirical methods in natural
  language processing: system demonstrations}, 2020, pp. 38--45.

\bibitem{wang2019learning}
Q.~Wang, B.~Li, T.~Xiao, J.~Zhu, C.~Li, D.~F. Wong, and L.~S. Chao, ``Learning
  deep transformer models for machine translation,'' \emph{arXiv preprint
  arXiv:1906.01787}, 2019.

\bibitem{dong2018speech}
L.~Dong, S.~Xu, and B.~Xu, ``Speech-transformer: a no-recurrence
  sequence-to-sequence model for speech recognition,'' in \emph{2018 IEEE
  international conference on acoustics, speech and signal processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 5884--5888.

\bibitem{song2022multimodal}
Q.~Song, B.~Sun, and S.~Li, ``Multimodal sparse transformer network for
  audio-visual speech recognition,'' \emph{IEEE Transactions on Neural Networks
  and Learning Systems}, 2022.

\bibitem{yang2021just}
A.~Yang, A.~Miech, J.~Sivic, I.~Laptev, and C.~Schmid, ``Just ask: Learning to
  answer questions from millions of narrated videos,'' in \emph{Proceedings of
  the IEEE/CVF International Conference on Computer Vision}, 2021, pp.
  1686--1697.

\bibitem{dang2022dpt}
F.~Dang, H.~Chen, and P.~Zhang, ``Dpt-fsnet: Dual-path transformer based
  full-band and sub-band fusion network for speech enhancement,'' in
  \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2022, pp. 6857--6861.

\bibitem{wagner2022dawn}
J.~Wagner, A.~Triantafyllopoulos, H.~Wierstorf, M.~Schmitt, F.~Eyben, and B.~W.
  Schuller, ``Dawn of the transformer era in speech emotion recognition:
  closing the valence gap,'' \emph{arXiv preprint arXiv:2203.07378}, 2022.

\bibitem{subakan2021attention}
C.~Subakan, M.~Ravanelli, S.~Cornell, M.~Bronzi, and J.~Zhong, ``Attention is
  all you need in speech separation,'' in \emph{ICASSP 2021-2021 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 21--25.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in
  \emph{Proceedings of the 31st International Conference on Neural Information
  Processing Systems}, 2017, pp. 6000--6010.

\bibitem{voita2019analyzing}
E.~Voita, D.~Talbot, F.~Moiseev, R.~Sennrich, and I.~Titov, ``Analyzing
  multi-head self-attention: Specialized heads do the heavy lifting, the rest
  can be pruned,'' \emph{arXiv preprint arXiv:1905.09418}, 2019.

\bibitem{latif2023survey}
S.~Latif, H.~Cuay{\'a}huitl, F.~Pervez, F.~Shamshad, H.~S. Ali, and E.~Cambria,
  ``A survey on deep reinforcement learning for audio-based applications,''
  \emph{Artificial Intelligence Review}, vol.~56, no.~3, pp. 2193--2240, 2023.

\bibitem{zhang2018deep}
Z.~Zhang, J.~Geiger, J.~Pohjalainen, A.~E.-D. Mousa, W.~Jin, and B.~Schuller,
  ``Deep learning for environmentally robust speech recognition: An overview of
  recent developments,'' \emph{ACM Transactions on Intelligent Systems and
  Technology (TIST)}, vol.~9, no.~5, pp. 1--28, 2018.

\bibitem{nassif2019speech}
A.~B. Nassif, I.~Shahin, I.~Attili, M.~Azzeh, and K.~Shaalan, ``Speech
  recognition using deep neural networks: A systematic review,'' \emph{IEEE
  access}, vol.~7, pp. 19\,143--19\,165, 2019.

\bibitem{khalil2019speech}
R.~A. Khalil, E.~Jones, M.~I. Babar, T.~Jan, M.~H. Zafar, and T.~Alhussain,
  ``Speech emotion recognition using deep learning techniques: A review,''
  \emph{IEEE Access}, vol.~7, pp. 117\,327--117\,345, 2019.

\bibitem{latif2021survey}
S.~Latif, R.~Rana, S.~Khalifa, R.~Jurdak, J.~Qadir, and B.~W. Schuller,
  ``Survey of deep representation learning for speech emotion recognition,''
  \emph{IEEE Transactions on Affective Computing}, 2021.

\bibitem{deng2016deep}
L.~Deng, ``Deep learning: from speech recognition to language and multimodal
  processing,'' \emph{APSIPA Transactions on Signal and Information
  Processing}, vol.~5, p.~e1, 2016.

\bibitem{alam2020survey}
M.~Alam, M.~D. Samad, L.~Vidyaratne, A.~Glandon, and K.~M. Iftekharuddin,
  ``Survey on deep neural networks in speech and vision systems,''
  \emph{Neurocomputing}, vol. 417, pp. 302--321, 2020.

\bibitem{bracsoveanu2020visualizing}
A.~M. Bra{\c{s}}oveanu and R.~Andonie, ``Visualizing transformers for nlp: a
  brief survey,'' in \emph{2020 24th International Conference Information
  Visualisation (IV)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp.
  270--279.

\bibitem{tan2021survey}
X.~Tan, T.~Qin, F.~Soong, and T.-Y. Liu, ``A survey on neural speech
  synthesis,'' \emph{arXiv preprint arXiv:2106.15561}, 2021.

\bibitem{alharbi2021automatic}
S.~Alharbi, M.~Alrazgan, A.~Alrashed, T.~Alnomasi, R.~Almojel, R.~Alharbi,
  S.~Alharbi, S.~Alturki, F.~Alshehri, and M.~Almojil, ``Automatic speech
  recognition: Systematic literature review,'' \emph{IEEE Access}, vol.~9, pp.
  131\,858--131\,876, 2021.

\bibitem{malik2021automatic}
M.~Malik, M.~K. Malik, K.~Mehmood, and I.~Makhdoom, ``Automatic speech
  recognition: a survey,'' \emph{Multimedia Tools and Applications}, vol.~80,
  pp. 9411--9457, 2021.

\bibitem{liu2021survey}
Y.~Liu, Y.~Zhang, Y.~Wang, F.~Hou, J.~Yuan, J.~Tian, Y.~Zhang, Z.~Shi, J.~Fan,
  and Z.~He, ``A survey of visual transformers,'' \emph{arXiv preprint
  arXiv:2111.06091}, 2021.

\bibitem{xu2022multimodal}
P.~Xu, X.~Zhu, and D.~A. Clifton, ``Multimodal learning with transformers: A
  survey,'' \emph{arXiv preprint arXiv:2206.06488}, 2022.

\bibitem{shamshad2022transformers}
F.~Shamshad, S.~Khan, S.~W. Zamir, M.~H. Khan, M.~Hayat, F.~S. Khan, and H.~Fu,
  ``Transformers in medical imaging: A survey,'' \emph{arXiv preprint
  arXiv:2201.09873}, 2022.

\bibitem{acheampong2021transformer}
F.~A. Acheampong, H.~Nunoo-Mensah, and W.~Chen, ``Transformer models for
  text-based emotion detection: a review of {BERT-based} approaches,''
  \emph{Artificial Intelligence Review}, vol.~54, no.~8, pp. 5789--5829, 2021.

\bibitem{khan2022transformers}
S.~Khan, M.~Naseer, M.~Hayat, S.~W. Zamir, F.~S. Khan, and M.~Shah,
  ``Transformers in vision: A survey,'' \emph{ACM computing surveys (CSUR)},
  vol.~54, no. 10s, pp. 1--41, 2022.

\bibitem{aleissaee2022transformers}
A.~A. Aleissaee, A.~Kumar, R.~M. Anwer, S.~Khan, H.~Cholakkal, G.-S. Xia
  \emph{et~al.}, ``Transformers in remote sensing: A survey,'' \emph{arXiv
  preprint arXiv:2209.01206}, 2022.

\bibitem{ulhaq2022vision}
A.~Ulhaq, N.~Akhtar, G.~Pogrebna, and A.~Mian, ``Vision transformers for action
  recognition: A survey,'' \emph{arXiv preprint arXiv:2209.05700}, 2022.

\bibitem{bhangale2022survey}
K.~B. Bhangale and M.~Kothandaraman, ``Survey of deep learning paradigms for
  speech processing,'' \emph{Wireless Personal Communications}, vol. 125,
  no.~2, pp. 1913--1949, 2022.

\bibitem{lahoud20223d}
J.~Lahoud, J.~Cao, F.~S. Khan, H.~Cholakkal, R.~M. Anwer, S.~Khan, and M.-H.
  Yang, ``{3D} vision with transformers: A survey,'' \emph{arXiv preprint
  arXiv:2208.04309}, 2022.

\bibitem{li2023survey}
W.~Li, H.~Luo, Z.~Lin, C.~Zhang, Z.~Lu, and D.~Ye, ``A survey on transformers
  in reinforcement learning,'' \emph{arXiv preprint arXiv:2301.03044}, 2023.

\bibitem{abdel2014convolutional}
O.~Abdel-Hamid, A.-r. Mohamed, H.~Jiang, L.~Deng, G.~Penn, and D.~Yu,
  ``Convolutional neural networks for speech recognition,'' \emph{{IEEE}/{ACM}
  Transactions on audio, speech, and language processing}, vol.~22, no.~10,
  2014.

\bibitem{zhang2017towards}
Y.~Zhang, M.~Pezeshki, P.~Brakel, S.~Zhang, C.~L.~Y. Bengio, and A.~Courville,
  ``Towards end-to-end speech recognition with deep convolutional neural
  networks,'' \emph{arXiv preprint arXiv:1701.02720}, 2017.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``{BERT}: Pre-training of
  deep bidirectional transformers for language understanding,'' \emph{arXiv
  preprint arXiv:1810.04805}, 2018.

\bibitem{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever \emph{et~al.}, ``Improving
  language understanding by generative pre-training,'' 2018.

\bibitem{yang2019xlnet}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~R. Salakhutdinov, and Q.~V. Le,
  ``{XLN}et: Generalized autoregressive pretraining for language
  understanding,'' \emph{Advances in neural information processing systems},
  vol.~32, 2019.

\bibitem{raffel2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a
  unified text-to-text transformer,'' \emph{The Journal of Machine Learning
  Research}, vol.~21, no.~1, pp. 5485--5551, 2020.

\bibitem{wu2020sentence}
N.~R. Wu and I.~Gurevych, ``Sentence-{BERT: Sentence Embeddings using Siamese
  BERT}-networks,'' in \emph{Proceedings of the 2020 Conference on Empirical
  Methods in Natural Language Processing (EMNLP)}, 2020, p. 3982â€“3992.

\bibitem{baevski2020vq}

A.~Baevski, M.~A. Schneider, and M.~Auli, ``vq-wav2vec: Self-supervised
  learning of discrete speech representations,'' in \emph{International
  Conference on Learning Representations}, 2020. [Online]. Available:
  \url{https://openreview.net/forum?id=r1gEjCNYwS}


\bibitem{baevski2020wav2vec}
A.~Baevski, Y.~Zhou, A.~Mohamed, and M.~Auli, ``wav2vec 2.0: A framework for
  self-supervised learning of speech representations,'' \emph{Advances in
  Neural Information Processing Systems}, vol.~33, pp. 12\,449--12\,460, 2020.

\bibitem{pepino2021emotion}
L.~Pepino, P.~Riera, and L.~Ferrer, ``Emotion recognition from speech using
  wav2vec 2.0 embeddings,'' \emph{arXiv preprint arXiv:2104.03502}, 2021.

\bibitem{novoselov2022robust}
S.~Novoselov, G.~Lavrentyeva, A.~Avdeeva, V.~Volokhov, and A.~Gusev, ``Robust
  speaker recognition with transformers using wav2vec 2.0,'' \emph{arXiv
  preprint arXiv:2203.15095}, 2022.

\bibitem{babu2021xls}
A.~Babu, C.~Wang, A.~Tjandra, K.~Lakhotia, Q.~Xu, N.~Goyal, K.~Singh, P.~von
  Platen, Y.~Saraf, J.~Pino \emph{et~al.}, ``{XLS-R}: Self-supervised
  cross-lingual speech representation learning at scale,'' \emph{arXiv preprint
  arXiv:2111.09296}, 2021.

\bibitem{baevski2022data2vec}
A.~Baevski, W.-N. Hsu, Q.~Xu, A.~Babu, J.~Gu, and M.~Auli, ``Data2vec: A
  general framework for self-supervised learning in speech, vision and
  language,'' in \emph{International Conference on Machine Learning}.\hskip 1em
  plus 0.5em minus 0.4em\relax PMLR, 2022, pp. 1298--1312.

\bibitem{kiros2015skip}
R.~Kiros, Y.~Zhu, R.~Salakhutdinov, R.~S. Zemel, A.~Torralba, R.~Urtasun, and
  S.~Fidler, ``Skip-thought vectors,'' \url{https://arxiv.org/abs/1506.06726},
  2015.

\bibitem{le2014distributed}
Q.~Le and T.~Mikolov, ``Distributed representations of sentences and
  documents,'' in \emph{Proceedings of The 31st International Conference on
  Machine Learning}, 2014, p. 1188â€“1196.

\bibitem{radford2022robust}
A.~Radford, J.~W. Kim, T.~Xu, G.~Brockman, C.~McLeavey, and I.~Sutskever,
  ``Robust speech recognition via large-scale weak supervision,'' \emph{arXiv
  preprint arXiv:2212.04356}, 2022.

\bibitem{wang2017tacotron}
Y.~Wang, R.~Skerry-Ryan, D.~Stanton, Y.~Wu, R.~J. Weiss, N.~Jaitly, Z.~Yang,
  Y.~Xiao, Z.~Chen, S.~Bengio \emph{et~al.}, ``Tacotron: Towards end-to-end
  speech synthesis,'' \emph{Proc. Interspeech 2017}, pp. 4006--4010, 2017.

\bibitem{shen2018natural}
J.~Shen, R.~Pang, R.~J. Weiss, M.~Schuster, N.~Jaitly, Z.~Yang, Z.~Chen,
  Y.~Zhang, Y.~Wang, R.~Skerrv-Ryan \emph{et~al.}, ``{Natural TTS Synthesis by
  Conditioning Wavenet on MEL Spectrogram Predictions},'' in \emph{2018 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 4779--4783.

\bibitem{li2019neural}
N.~Li, S.~Liu, Y.~Liu, S.~Zhao, and M.~Liu, ``Neural speech synthesis with
  transformer network,'' in \emph{Proceedings of the AAAI Conference on
  Artificial Intelligence}, vol.~33, no.~01, 2019, pp. 6706--6713.

\bibitem{wang2019deep}
G.~Wang, ``Deep text-to-speech system with seq2seq model,'' \emph{arXiv
  preprint arXiv:1903.07398}, 2019.

\bibitem{beliaev2021talknet}
S.~Beliaev and B.~Ginsburg, ``Talknet 2: Non-autoregressive depth-wise
  separable convolutional model for speech synthesis with explicit pitch and
  duration prediction,'' \emph{arXiv preprint arXiv:2104.08189}, 2021.

\bibitem{ren2019fastspeech}
Y.~Ren, Y.~Ruan, X.~Tan, T.~Qin, S.~Zhao, Z.~Zhao, and T.-Y. Liu, ``Fastspeech:
  Fast, robust and controllable text to speech,'' \emph{arXiv preprint
  arXiv:1905.09263}, 2019.

\bibitem{bgn2021timeline}
J.~Bgn, ``Timeline of transformers for speech,''
  \url{https://jonathanbgn.com/2021/12/31/timeline-transformers-speech.html},
  2021, accessed: 2023-03-07.

\bibitem{liu2019mockingjay}
A.~T.-Y. Liu, S.-w. Yang, and C.-S. F. J. C.-H. H.~H. yi~Lee Lin-shan Lee,
  ``Mockingjay: Unsupervised speech representation learning with deep
  bidirectional transformer encoders,'' \emph{arXiv preprint arXiv:1910.12638},
  2019.

\bibitem{ren2020fastspeech}
Y.~Ren, C.~Hu, T.~Qin, S.~Zhao, Z.~Zhao, and T.-Y. Liu, ``Fastspeech 2: Fast
  and high-quality end-to-end text-to-speech,'' \emph{arXiv preprint
  arXiv:2006.04558}, 2020.

\bibitem{schneider2019wav2vec}
S.~Schneider, A.~Baevski, R.~Collobert, and M.~Auli, ``wav2vec: Unsupervised
  pre-training for speech recognition,'' \emph{arXiv preprint
  arXiv:1904.05862}, 2019.

\bibitem{chen2022speechformer}
W.~Chen, X.~Xing, X.~Xu, J.~Pang, and L.~Du, ``Speechformer: A hierarchical
  efficient framework incorporating the characteristics of speech,''
  \emph{arXiv preprint arXiv:2203.03812}, 2022.

\bibitem{lachowicz2020fastpitch}
A.~{\L}a{\'n}cucki, ``Fastpitch: Parallel text-to-speech with pitch
  prediction,'' in \emph{ICASSP 2021-2021 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2021, pp. 6588--6592.

\bibitem{gulati2020conformer}
A.~Gulati, J.~Qin, C.-C. Chiu, N.~Parmar, Y.~Zhang, J.~Yu, W.~Han, S.~Wang,
  Z.~Zhang, Y.~Wu \emph{et~al.}, ``Conformer: Convolution-augmented transformer
  for speech recognition,'' \emph{arXiv preprint arXiv:2005.08100}, 2020.

\bibitem{chang2020decoar}
S.-w. Chang, P.-H. Hsu, Y.-A. Tsai, S.-L. Chen, and H.-y. Lee, ``Decoar 2.0:
  Deep contextualized acoustic representations with vector quantization,''
  \emph{arXiv preprint arXiv:2012.06659}, 2020.

\bibitem{wang2019bridging}
Z.~Wang and S.~Zhang, ``Bridging commonsense reasoning and probabilistic
  planning via a differentiable neural logic framework,'' in \emph{Proceedings
  of the Twenty-Eighth International Joint Conference on Artificial
  Intelligence (IJCAI-19)}, 2019, p. 1091â€“1097.

\bibitem{hsu2021hubert}
W.-N. Hsu, B.~Bolte, Y.-H.~H. Tsai, K.~Lakhotia, R.~Salakhutdinov, and
  A.~Mohamed, ``Hubert: Self-supervised speech representation learning by
  masked prediction of hidden units,'' \emph{IEEE/ACM Transactions on Audio,
  Speech, and Language Processing}, vol.~29, pp. 3451--3460, 2021.

\bibitem{chen2021unspeech}
C.~Wang, Y.~Wu, Y.~Qian, K.~Kumatani, S.~Liu, F.~Wei, M.~Zeng, and X.~Huang,
  ``Unispeech: Unified speech representation learning with labeled and
  unlabeled data,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp.
  10\,937--10\,947.

\bibitem{liu2021unispeechsat}
Z.~Liu, Y.~Chen, X.~Liang, and J.~Zhang, ``Unispeech-sat: Universal speech
  representation learning with speaker aware pre-training,'' \emph{arXiv
  preprint arXiv:2110.05752}, 2021.

\bibitem{hu2021bigssl}
Y.~Zhang, D.~S. Park, W.~Han, J.~Qin, A.~Gulati, J.~Shor, A.~Jansen, Y.~Xu,
  Y.~Huang, S.~Wang \emph{et~al.}, ``{BigSSL}: Exploring the frontier of
  large-scale semi-supervised learning for automatic speech recognition,''
  \emph{IEEE Journal of Selected Topics in Signal Processing}, vol.~16, no.~6,
  pp. 1519--1532, 2022.

\bibitem{liu2021wavlm}
Z.~Liu, Y.~Chen, X.~Liang, and J.~Zhang, ``Wavlm: Large-scale self-supervised
  pre-training for full stack speech processing,'' \emph{arXiv preprint
  arXiv:2110.13900}, 2021.

\bibitem{liu2021deltalm}
J.~Liu, Y.~Hou, and W.~Che, ``Deltalm: Encoder-decoder pre-training for
  language generation and understanding,'' 2021.

\bibitem{ma2021deltalm}
S.~Ma, L.~Dong, S.~Huang, D.~Zhang, A.~Muzio, S.~Singhal, H.~H. Awadalla,
  X.~Song, and F.~Wei, ``Deltalm: Encoder-decoder pre-training for language
  generation and translation by augmenting pretrained multilingual encoders,''
  \emph{arXiv preprint arXiv:2106.13736}, 2021.

\bibitem{ao2021speecht5}
J.~Ao, R.~Wang, L.~Zhou, C.~Wang, S.~Ren, Y.~Wu, S.~Liu, T.~Ko, Q.~Li, Y.~Zhang
  \emph{et~al.}, ``Speecht5: Unified-modal encoder-decoder pre-training for
  spoken language processing,'' \emph{arXiv preprint arXiv:2110.07205}, 2021.

\bibitem{baevski2022efficient}
A.~Baevski, A.~Babu, W.-N. Hsu, and M.~Auli, ``Efficient self-supervised
  learning with contextualized target representations for vision, speech and
  language,'' \emph{arXiv preprint arXiv:2212.07525}, 2022.

\bibitem{openai2022whisper}
OpenAI, ``Whisper: Robust speech recognition via large-scale weak
  supervision,'' \url{https://github.com/openai/whisper}.

\bibitem{chen2022valle}
Z.~Chen, J.~Zhang, Z.~Liu, X.~Chen, and X.~Liang, ``Neural codec language
  models are zero-shot text to speech synthesizers,'' \emph{arXiv preprint
  arXiv:2301.02111}, 2022.

\bibitem{zhang2023speak}

Z.~Zhang, L.~Zhou, C.~Wang, S.~Chen, Y.~Wu, S.~Liu, Z.~Chen, Y.~Liu, H.~Wang,
  J.~Li, L.~He, S.~Zhao, and F.~Wei, ``Speak foreign languages with your own
  voice: Cross-lingual neural codec language modeling,'' 2023. [Online].
  Available: \url{https://arxiv.org/abs/2303.03926}


\bibitem{zhang2020pushing}
Y.~Zhang, J.~Qin, D.~S. Park, W.~Han, C.-C. Chiu, R.~Pang, Q.~V. Le, and Y.~Wu,
  ``Pushing the limits of semi-supervised learning for automatic speech
  recognition,'' \emph{arXiv preprint arXiv:2010.10504}, 2020.

\bibitem{wang2022conformer}
T.~Wang, J.~Deng, M.~Geng, Z.~Ye, S.~Hu, Y.~Wang, M.~Cui, Z.~Jin, X.~Liu, and
  H.~Meng, ``Conformer based elderly speech recognition system for alzheimer's
  disease detection,'' \emph{arXiv preprint arXiv:2206.13232}, 2022.

\bibitem{wang2021unispeech}
C.~Wang, Y.~Wu, Y.~Qian, K.~Kumatani, S.~Liu, F.~Wei, M.~Zeng, and X.~Huang,
  ``{UniSpeech}: Unified speech representation learning with labeled and
  unlabeled data,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp.
  10\,937--10\,947.

\bibitem{chen2022unispeech}
S.~Chen, Y.~Wu, C.~Wang, Z.~Chen, Z.~Chen, S.~Liu, J.~Wu, Y.~Qian, F.~Wei,
  J.~Li \emph{et~al.}, ``{UniSpeech-SAT}: Universal speech representation
  learning with speaker aware pre-training,'' in \emph{ICASSP 2022-2022 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 6152--6156.

\bibitem{papi2021speechformer}
S.~Papi, M.~Gaido, M.~Negri, and M.~Turchi, ``Speechformer: Reducing
  information loss in direct speech translation,'' in \emph{Proceedings of the
  2021 Conference on Empirical Methods in Natural Language Processing}, 2021,
  pp. 1698--1706.

\bibitem{chen2022wavlm}
S.~Chen, C.~Wang, Z.~Chen, Y.~Wu, S.~Liu, Z.~Chen, J.~Li, N.~Kanda,
  T.~Yoshioka, X.~Xiao \emph{et~al.}, ``Wavlm: Large-scale self-supervised
  pre-training for full stack speech processing,'' \emph{IEEE Journal of
  Selected Topics in Signal Processing}, vol.~16, no.~6, pp. 1505--1518, 2022.

\bibitem{schmidhuber1997long}
J.~Schmidhuber and S.~Hochreiter, ``Long short-term memory,'' \emph{Neural
  Comput}, vol.~9, no.~8, pp. 1735--1780, 1997.

\bibitem{dai2019transformer}
Z.~Dai, Z.~Yang, Y.~Yang, J.~G. Carbonell, Q.~Le, and R.~Salakhutdinov,
  ``Transformer-xl: Attentive language models beyond a fixed-length context,''
  in \emph{Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, 2019, pp. 2978--2988.

\bibitem{vaswani2018tensor2tensor}
A.~Vaswani, S.~Bengio, E.~Brevdo, F.~Chollet, A.~Gomez, S.~Gouws, L.~Jones,
  {\L}.~Kaiser, N.~Kalchbrenner, N.~Parmar \emph{et~al.}, ``Tensor2tensor for
  neural machine translation,'' in \emph{Proceedings of the 13th Conference of
  the Association for Machine Translation in the Americas (Volume 1: Research
  Track)}, 2018, pp. 193--199.

\bibitem{werbos1990backpropagation}
P.~J. Werbos, ``Backpropagation through time: what it does and how to do it,''
  \emph{Proceedings of the IEEE}, vol.~78, no.~10, pp. 1550--1560, 1990.

\bibitem{zeyer2019comparison}
A.~Zeyer, P.~Bahar, K.~Irie, R.~Schl{\"u}ter, and H.~Ney, ``A comparison of
  transformer and {LSTM encoder decoder models for ASR},'' in \emph{2019 IEEE
  Automatic Speech Recognition and Understanding Workshop (ASRU)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2019, pp. 8--15.

\bibitem{li2020comparison}
J.~Li, Y.~Wu, Y.~Gaur, C.~Wang, R.~Zhao, and S.~Liu, ``On the comparison of
  popular end-to-end models for large scale speech recognition,'' \emph{Proc.
  Interspeech 2020}, pp. 1--5, 2020.

\bibitem{wang2020transformer}
Y.~Wang, Y.~Shi, F.~Zhang, C.~Wu, J.~Chan, C.-F. Yeh, and A.~Xiao,
  ``Transformer in action: a comparative study of transformer-based acoustic
  models for large scale speech recognition applications,'' \emph{arXiv
  preprint arXiv:2010.14665}, 2020.

\bibitem{zhou2018comparison}
S.~Zhou, L.~Dong, S.~Xu, and B.~Xu, ``A comparison of modeling units in
  sequence-to-sequence speech recognition with the transformer on mandarin
  chinese,'' in \emph{International Conference on Neural Information
  Processing}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2018, pp.
  210--220.

\bibitem{wu2020streaming}
C.~Wu, Y.~Wang, Y.~Shi, C.-F. Yeh, and F.~Zhang, ``Streaming transformer-based
  acoustic models using self-attention with augmented memory,'' \emph{Proc.
  Interspeech 2020}, pp. 2132--2136, 2020.

\bibitem{zhou2018syllable}
S.~Zhou, L.~Dong, S.~Xu, and B.~Xu, ``Syllable-based sequence-to-sequence
  speech recognition with the transformer in mandarin chinese,'' \emph{Proc.
  Interspeech 2018}, pp. 791--795, 2018.

\bibitem{hrinchuk2020correction}
O.~Hrinchuk, M.~Popova, and B.~Ginsburg, ``Correction of automatic speech
  recognition with transformer sequence-to-sequence model,'' in \emph{ICASSP
  2020-2020 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp.
  7074--7078.

\bibitem{tian2020synchronous}
Z.~Tian, J.~Yi, Y.~Bai, J.~Tao, S.~Zhang, and Z.~Wen, ``Synchronous
  transformers for end-to-end speech recognition,'' in \emph{ICASSP 2020-2020
  IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 7884--7888.

\bibitem{lu2020exploring}
L.~Lu, C.~Liu, J.~Li, and Y.~Gong, ``Exploring transformers for large-scale
  speech recognition,'' \emph{arXiv preprint arXiv:2005.09684}, 2020.

\bibitem{chen2020developing}
X.~Chen, Y.~Wu, Z.~Wang, S.~Liu, and J.~Li, ``Developing real-time streaming
  transformer transducer for speech recognition on large-scale dataset,''
  \emph{arXiv preprint arXiv:2010.11395}, 2020.

\bibitem{graves2012sequence}
A.~Graves, ``Sequence transduction with recurrent neural networks,''
  \emph{arXiv preprint arXiv:1211.3711}, 2012.

\bibitem{li2019speechtransformer}
J.~Li, X.~Wang, Y.~Li \emph{et~al.}, ``The speechtransformer for large-scale
  {Mandarin Chinese} speech recognition,'' in \emph{ICASSP 2019-2019 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019, pp. 7095--7099.

\bibitem{taylor2009text}
P.~Taylor, \emph{Text-to-speech synthesis}.\hskip 1em plus 0.5em minus
  0.4em\relax Cambridge university press, 2009.

\bibitem{arik2017deep}
S.~{\"O}. Ar{\i}k, M.~Chrzanowski, A.~Coates, G.~Diamos, A.~Gibiansky, Y.~Kang,
  X.~Li, J.~Miller, A.~Ng, J.~Raiman \emph{et~al.}, ``Deep voice: Real-time
  neural text-to-speech,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2017, pp. 195--204.

\bibitem{ping2018deep}
W.~Ping, K.~Peng, A.~Gibiansky, S.~O. Arik, A.~Kannan, S.~Narang, J.~Raiman,
  and J.~Miller, ``Deep voice 3: 2000-speaker neural text-to-speech,''
  \emph{Proc. ICLR}, pp. 214--217, 2018.

\bibitem{ping2018clarinet}
W.~Ping, K.~Peng, and J.~Chen, ``Clarinet: Parallel wave generation in
  end-to-end text-to-speech,'' in \emph{International Conference on Learning
  Representations}, 2018.

\bibitem{griffin1984signal}
D.~Griffin and J.~Lim, ``Signal estimation from modified short-time {Fourier}
  transform,'' \emph{IEEE Transactions on acoustics, speech, and signal
  processing}, vol.~32, no.~2, pp. 236--243, 1984.

\bibitem{vanwavenet}
A.~van~den Oord, S.~Dieleman, H.~Zen, K.~Simonyan, O.~Vinyals, A.~Graves,
  N.~Kalchbrenner, A.~Senior, and K.~Kavukcuoglu, ``Wavenet: A generative model
  for raw audio,'' \url{https://arxiv.org/abs/1609.03499}, 2016.

\bibitem{prenger2019waveglow}
R.~Prenger, R.~Valle, and B.~Catanzaro, ``Waveglow: A flow-based generative
  network for speech synthesis,'' in \emph{ICASSP 2019-2019 IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2019, pp. 3617--3621.

\bibitem{lancucki2020fastpitch}
A.~{\L}a{\'n}cucki, ``Fastpitch: Parallel text-to-speech with pitch
  prediction,'' \emph{arXiv preprint arXiv:2006.06873}, 2020.

\bibitem{mohamed2019transformers}
A.~Mohamed, D.~Okhonko, and L.~Zettlemoyer, ``Transformers with convolutional
  context for asr,'' \emph{arXiv preprint arXiv:1904.11660}, 2019.

\bibitem{zhang2021transmask}
Z.~Zhang, B.~He, and Z.~Zhang, ``Transmask: A compact and fast speech
  separation model based on transformer,'' in \emph{ICASSP 2021-2021 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 5764--5768.

\bibitem{moriya2020self}
T.~Moriya, T.~Ochiai, S.~Karita, H.~Sato, T.~Tanaka, T.~Ashihara, R.~Masumura,
  Y.~Shinohara, and M.~Delcroix, ``Self-distillation for improving
  {CTC-Transformer-Based {ASR} Systems},'' in \emph{INTERSPEECH}, 2020, pp.
  546--550.

\bibitem{cao2021improving}
S.~Cao, Y.~Kang, Y.~Fu, X.~Xu, S.~Sun, Y.~Zhang, and L.~Ma, ``Improving
  streaming transformer based {ASR} under a framework of self-supervised
  learning,'' \emph{arXiv preprint arXiv:2109.07327}, 2021.

\bibitem{tsunoo2019transformer}
E.~Tsunoo, Y.~Kashiwagi, T.~Kumakura, and S.~Watanabe, ``Transformer {ASR} with
  contextual block processing,'' in \emph{2019 IEEE Automatic Speech
  Recognition and Understanding Workshop (ASRU)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2019, pp. 427--433.

\bibitem{jain2020finnish}
A.~Jain, A.~Rouhe, S.-A. Gr{\"o}nroos, M.~Kurimo \emph{et~al.}, ``Finnish {ASR}
  with deep transformer models.'' in \emph{Interspeech}, 2020, pp. 3630--3634.

\bibitem{yu2020dual}
J.~Yu, W.~Han, A.~Gulati, C.-C. Chiu, B.~Li, T.~N. Sainath, Y.~Wu, and R.~Pang,
  ``{Dual-mode ASR: Unify and improve streaming ASR with full-context
  modeling},'' \emph{arXiv preprint arXiv:2010.06030}, 2020.

\bibitem{gehring2017convolutional}
J.~Gehring, M.~Auli, D.~Grangier, D.~Yarats, and Y.~N. Dauphin, ``Convolutional
  sequence to sequence learning,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2017, pp. 1243--1252.

\bibitem{jin2018fftnet}
Z.~Jin, A.~Finkelstein, G.~J. Mysore, and J.~Lu, ``Fftnet: A real-time
  speaker-dependent neural vocoder,'' in \emph{2018 IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 2251--2255.

\bibitem{yu2019durian}
C.~Yu, H.~Lu, N.~Hu, M.~Yu, C.~Weng, K.~Xu, P.~Liu, D.~Tuo, S.~Kang, G.~Lei
  \emph{et~al.}, ``Durian: Duration informed attention network for multimodal
  synthesis,'' \emph{arXiv preprint arXiv:1909.01700}, 2019.

\bibitem{li2020robutrans}
N.~Li, Y.~Liu, Y.~Wu, S.~Liu, S.~Zhao, and M.~Liu, ``Robutrans: A robust
  transformer-based text-to-speech model,'' in \emph{Proceedings of the AAAI
  Conference on Artificial Intelligence}, vol.~34, no.~05, 2020, pp.
  8228--8235.

\bibitem{wang2020s}
X.~Wang, H.~Ming, L.~He, and F.~K. Soong, ``s-transformer: Segment-transformer
  for robust neural speech synthesis,'' \emph{arXiv preprint arXiv:2011.08480},
  2020.

\bibitem{zheng2020improving}
Y.~Zheng, X.~Li, F.~Xie, and L.~Lu, ``Improving end-to-end speech synthesis
  with local recurrent neural network enhanced transformer,'' in \emph{ICASSP
  2020-2020 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp.
  6734--6738.

\bibitem{chen2020multispeech}
M.~Chen, X.~Tan, Y.~Ren, J.~Xu, H.~Sun, S.~Zhao, and T.~Qin, ``Multispeech:
  Multi-speaker text to speech with transformer,'' \emph{Proc. Interspeech
  2020}, pp. 4024--4028, 2020.

\bibitem{lim2020jdi}
D.~Lim, W.~Jang, O.~Gyeonghwan, H.~Park, B.~Kim, and J.~Yoon, ``{JDI-T: Jointly
  Trained Duration Informed Transformer for Text-To-Speech without Explicit
  Alignment},'' \emph{Proc. Interspeech 2020}, pp. 4004--4008, 2020.

\bibitem{huang2020voice}
W.-C. Huang, T.~Hayashi, Y.-C. Wu, H.~Kameoka, and T.~Toda, ``Voice transformer
  network: Sequence-to-sequence voice conversion using transformer with
  text-to-speech pretraining,'' \emph{Proc. Interspeech 2020}, pp. 4676--4680,
  2020.

\bibitem{hu2020unsupervised}
T.-Y. Hu, A.~Shrivastava, O.~Tuzel, and C.~Dhir, ``Unsupervised style and
  content separation by minimizing mutual information for speech synthesis,''
  in \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2020, pp. 3267--3271.

\bibitem{chen2022fine}
L.-W. Chen and A.~Rudnicky, ``Fine-grained style control in transformer-based
  text-to-speech synthesis,'' in \emph{ICASSP 2022-2022 IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 7907--7911.

\bibitem{liu2021graphspeech}
R.~Liu, B.~Sisman, and H.~Li, ``Graphspeech: Syntax-aware graph attention
  network for neural speech synthesis,'' in \emph{ICASSP 2021-2021 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 6059--6063.

\bibitem{wang2021patnet}
S.~Wang, Z.~Ling, R.~Fu, J.~Yi, and J.~Tao, ``Patnet: A phoneme-level
  autoregressive transformer network for speech synthesis,'' in \emph{ICASSP
  2021-2021 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp.
  5684--5688.

\bibitem{kano2021transformer}
T.~Kano, S.~Sakti, and S.~Nakamura, ``Transformer-based direct speech-to-speech
  translation with transcoder,'' in \emph{2021 IEEE Spoken Language Technology
  Workshop (SLT)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp.
  958--965.

\bibitem{zhang2019lattice}
P.~Zhang, N.~Ge, B.~Chen, and K.~Fan, ``Lattice transformer for speech
  translation,'' in \emph{Proceedings of the 57th Annual Meeting of the
  Association for Computational Linguistics}, 2019, pp. 6475--6484.

\bibitem{huawei2022transformer}

Huawei. (2022) Speaking your language: The transformer in machine translation.
  [Online]. Available:
  \url{https://blog.huawei.com/2022/02/01/speaking-your-language-transformer-machine-translation/}


\bibitem{ney1999speech}
H.~Ney, ``Speech translation: Coupling of recognition and translation,'' in
  \emph{1999 IEEE International Conference on Acoustics, Speech, and Signal
  Processing. Proceedings. ICASSP99 (Cat. No. 99CH36258)}, vol.~1.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 1999, pp. 517--520.

\bibitem{matusov2005integration}
E.~Matusov, S.~Kanthak, and H.~Ney, ``On the integration of speech recognition
  and statistical machine translation,'' in \emph{Ninth European Conference on
  Speech Communication and Technology}, 2005.

\bibitem{do2017toward}
Q.~T. Do, S.~Sakti, and S.~Nakamura, ``Toward expressive speech translation: A
  unified sequence-to-sequence lstms approach for translating words and
  emphasis.'' in \emph{INTERSPEECH}, 2017, pp. 2640--2644.

\bibitem{berard2016listen}
A.~B{\'e}rard, O.~Pietquin, L.~Besacier, and C.~Servan, ``Listen and translate:
  A proof of concept for end-to-end speech-to-text translation,'' in \emph{NIPS
  Workshop on end-to-end learning for speech and audio processing}, 2016.

\bibitem{berard2018end}
A.~B{\'e}rard, L.~Besacier, A.~C. Kocabiyikoglu, and O.~Pietquin, ``End-to-end
  automatic speech translation of audiobooks,'' in \emph{2018 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 6224--6228.

\bibitem{weiss2017sequence}
R.~J. Weiss, J.~Chorowski, N.~Jaitly, Y.~Wu, and Z.~Chen,
  ``Sequence-to-sequence models can directly translate foreign speech,''
  \emph{Proc. Interspeech 2017}, pp. 2625--2629, 2017.

\bibitem{vila2018end}
L.-C. Vila, C.~Escolano, J.~A. Fonollosa, and M.-R. Costa-Juss{\`a},
  ``End-to-end speech translation with the transformer,'' \emph{Proc.
  IberSPEECH 2018}, pp. 60--63, 2018.

\bibitem{di2019adapting}
M.~A. Di~Gangi, M.~Negri, and M.~Turchi, ``Adapting transformer to end-to-end
  spoken language translation,'' in \emph{INTERSPEECH 2019}.\hskip 1em plus
  0.5em minus 0.4em\relax International Speech Communication Association
  (ISCA), 2019, pp. 1133--1137.

\bibitem{jia2021translatotron}
Y.~Jia, M.~T. Ramanovich, T.~Remez, and R.~Pomerantz, ``Translatotron 2:
  High-quality direct speech-to-speech translation with voice preservation,''
  \emph{arXiv preprint arXiv:2107.08661}, 2021.

\bibitem{huang2022transpeech}
R.~Huang, Z.~Zhao, J.~Liu, H.~Liu, Y.~Ren, L.~Zhang, and J.~He, ``Transpeech:
  Speech-to-speech translation with bilateral perturbation,'' \emph{arXiv
  preprint arXiv:2205.12523}, 2022.

\bibitem{li2020multilingual}
X.~Li, C.~Wang, Y.~Tang, C.~Tran, Y.~Tang, J.~Pino, A.~Baevski, A.~Conneau, and
  M.~Auli, ``Multilingual speech translation with efficient finetuning of
  pretrained models,'' \emph{arXiv preprint arXiv:2010.12829}, 2020.

\bibitem{wang2020fairseq}
C.~Wang, Y.~Tang, X.~Ma, A.~Wu, S.~Popuri, D.~Okhonko, and J.~Pino, ``fairseq
  s2t: Fast speech-to-text modeling with fairseq,'' \emph{arXiv preprint
  arXiv:2010.05171}, 2020.

\bibitem{zeng2021realtrans}
X.~Zeng, L.~Li, and Q.~Liu, ``Realtrans: End-to-end simultaneous speech
  translation with convolutional weighted-shrinking transformer,'' \emph{arXiv
  preprint arXiv:2106.04833}, 2021.

\bibitem{shor2022trillsson}
J.~Shor and S.~Venugopalan, ``{TRILLsson: Distilling universal paralinguistic
  speech representations},'' 2022.

\bibitem{shor2022universal}
J.~Shor, A.~Jansen, W.~Han, D.~Park, and Y.~Zhang, ``Universal paralinguistic
  speech representations using self-supervised conformers,'' in \emph{ICASSP
  2022-2022 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp.
  3169--3173.

\bibitem{xu2021transformer}
M.~Xu, S.~Li, and X.-L. Zhang, ``Transformer-based end-to-end speech
  recognition with local dense synthesizer attention,'' in \emph{ICASSP
  2021-2021 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp.
  5899--5903.

\bibitem{gao2022paraformer}
Z.~Gao, S.~Zhang, I.~McLoughlin, and Z.~Yan, ``Paraformer: Fast and accurate
  parallel transformer for non-autoregressive end-to-end speech recognition,''
  \emph{arXiv preprint arXiv:2206.08317}, 2022.

\bibitem{chen2023speechformer++}
W.~Chen, X.~Xing, X.~Xu, J.~Pang, and L.~Du, ``Speechformer++: A hierarchical
  efficient framework for paralinguistic speech processing,'' \emph{IEEE/ACM
  Transactions on Audio, Speech, and Language Processing}, 2023.

\bibitem{schotz2002paralinguistic}
S.~Sch{\"o}tz, ``Paralinguistic phonetics in nlp models \& methods,'' \emph{NLP
  term paper}, 2002.

\bibitem{yang2021superb}
S.-w. Yang, P.-H. Chi, Y.-S. Chuang, C.-I.~J. Lai, K.~Lakhotia, Y.~Y. Lin,
  A.~T. Liu, J.~Shi, X.~Chang, G.-T. Lin \emph{et~al.}, ``Superb: Speech
  processing universal performance benchmark,'' \emph{arXiv preprint
  arXiv:2105.01051}, 2021.

\bibitem{shor2020towards}
J.~Shor, A.~Jansen, R.~Maor, O.~Lang, O.~Tuval, F.~d.~C. Quitry,
  M.~Tagliasacchi, I.~Shavitt, D.~Emanuel, and Y.~Haviv, ``Towards learning a
  universal non-semantic representation of speech,'' \emph{arXiv preprint
  arXiv:2002.12764}, 2020.

\bibitem{yu2022setransformer}
W.~Yu, J.~Zhou, H.~Wang, and L.~Tao, ``{SETransformer}: speech enhancement
  transformer,'' \emph{Cognitive Computation}, pp. 1--7, 2022.

\bibitem{kim2020t}
J.~Kim, M.~El-Khamy, and J.~Lee, ``{T-GSA:} transformer with
  {G}aussian-weighted self-attention for speech enhancement,'' in \emph{ICASSP
  2020-2020 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp.
  6649--6653.

\bibitem{wang2021tstnn}
K.~Wang, B.~He, and W.-P. Zhu, ``{TSTNN}: Two-stage transformer based neural
  network for speech enhancement in the time domain,'' in \emph{ICASSP
  2021-2021 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp.
  7098--7102.

\bibitem{zhang2022cross}
S.~Zhang, M.~Chadwick, A.~G.~C. Ramos, and S.~Bhattacharya, ``Cross-attention
  is all you need: Real-time streaming transformers for personalised speech
  enhancement,'' \emph{arXiv preprint arXiv:2211.04346}, 2022.

\bibitem{zhao2021multi}
Y.~Zhao, C.~Luo, Z.-J. Zha, and W.~Zeng, ``Multi-scale group transformer for
  long sequence modeling in speech separation,'' in \emph{Proceedings of the
  Twenty-Ninth International Conference on International Joint Conferences on
  Artificial Intelligence}, 2021, pp. 3251--3257.

\bibitem{jiang2023low}

W.~Jiang, C.~Sun, F.~Chen, Y.~Leng, Q.~Guo, J.~Sun, and J.~Peng, ``Low
  complexity speech enhancement network based on frame-level {Swin}
  transformer,'' \emph{Electronics}, 2023. [Online]. Available:
  \url{https://www.mdpi.com/2079-9292/12/6/1330}


\bibitem{wang2018voicefilter}
Q.~Wang, H.~Muckenhirn, K.~Wilson, P.~Sridhar, Z.~Wu, J.~Hershey, R.~A.
  Saurous, R.~J. Weiss, Y.~Jia, and I.~L. Moreno, ``Voicefilter: Targeted voice
  separation by speaker-conditioned spectrogram masking,'' \emph{arXiv preprint
  arXiv:1810.04826}, 2018.

\bibitem{wang2020voicefilter}
Q.~Wang, I.~L. Moreno, M.~Saglam, K.~Wilson, A.~Chiao, R.~Liu, Y.~He, W.~Li,
  J.~Pelecanos, M.~Nika \emph{et~al.}, ``Voicefilter-lite: Streaming targeted
  voice separation for on-device speech recognition,'' \emph{arXiv preprint
  arXiv:2009.04323}, 2020.

\bibitem{o2021conformer}
T.~O'Malley, A.~Narayanan, Q.~Wang, A.~Park, J.~Walker, and N.~Howard, ``A
  conformer-based {ASR} frontend for joint acoustic echo cancellation, speech
  enhancement and speech separation,'' in \emph{2021 IEEE Automatic Speech
  Recognition and Understanding Workshop (ASRU)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2021, pp. 304--311.

\bibitem{gfeller2021one}
B.~Gfeller, D.~Roblek, and M.~Tagliasacchi, ``One-shot conditional audio
  filtering of arbitrary sounds,'' in \emph{ICASSP 2021-2021 IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 501--505.

\bibitem{ramos2022conditioning}
A.~G. C.~P. Ramos, A.~Mehrotra, N.~D. Lane, and S.~Bhattacharya, ``Conditioning
  sequence-to-sequence networks with learned activations,'' in
  \emph{International Conference on Learning Representations}, 2022.

\bibitem{hinton2015distilling}
G.~Hinton, O.~Vinyals, J.~Dean \emph{et~al.}, ``Distilling the knowledge in a
  neural network,'' \emph{arXiv preprint arXiv:1503.02531}, vol.~2, no.~7,
  2015.

\bibitem{chen2022ultra}
S.~Chen, Y.~Wu, Z.~Chen, J.~Wu, T.~Yoshioka, S.~Liu, J.~Li, and X.~Yu, ``Ultra
  fast speech separation model with teacher student learning,'' \emph{arXiv
  preprint arXiv:2204.12777}, 2022.

\bibitem{subakan2022resource}
C.~Subakan, M.~Ravanelli, S.~Cornell, F.~Lepoutre, and F.~Grondin,
  ``Resource-efficient separation transformer,'' \emph{arXiv preprint
  arXiv:2206.09507}, 2022.

\bibitem{luo2022tiny}
J.~Luo, J.~Wang, N.~Cheng, E.~Xiao, X.~Zhang, and J.~Xiao, ``Tiny-sepformer: A
  tiny time-domain transformer network for speech separation,'' \emph{arXiv
  preprint arXiv:2206.13689}, 2022.

\bibitem{GulatiQCPZYHWZW20}
A.~Gulati, J.~Qin, C.~Chiu, N.~Parmar, Y.~Zhang, J.~Yu, W.~Han, S.~Wang,
  Z.~Zhang, and Y.~Wu, ``Conformer: Convolution-augmented transformer for
  speech recognition,'' in \emph{{INTERSPEECH}}.\hskip 1em plus 0.5em minus
  0.4em\relax {ISCA}, 2020, p. 5036â€“5040.

\bibitem{DIET}
T.~Bunk, D.~Varshneya, V.~Vlasov, and A.~Nichol, ``{DIET:} lightweight language
  understanding for dialogue systems,'' \emph{CoRR}, vol. abs/2004.09936, 2020.

\bibitem{DevlinCLT19}
J.~Devlin, M.~Chang, K.~Lee, and K.~Toutanova, ``{BERT:} pre-training of deep
  bidirectional transformers for language understanding,'' in
  \emph{{NAACL-HLT}}, J.~Burstein, C.~Doran, and T.~Solorio, Eds.\hskip 1em
  plus 0.5em minus 0.4em\relax Association for Computational Linguistics, 2019.

\bibitem{ConneauL19}
A.~Conneau and G.~Lample, ``Cross-lingual language model pretraining,'' in
  \emph{{NeurIPS}}, 2019.

\bibitem{DistilBERT}
V.~Sanh, L.~Debut, J.~Chaumond, and T.~Wolf, ``Distilbert, a distilled version
  of {BERT:} smaller, faster, cheaper and lighter,'' \emph{CoRR}, vol.
  abs/1910.01108, 2019.

\bibitem{HendersonCMSWV20}
M.~Henderson, I.~Casanueva, N.~Mrksic, P.~Su, T.~Wen, and I.~Vulic, ``Convert:
  Efficient and accurate conversational representations from transformers,'' in
  \emph{Findings of the ACL: {EMNLP}}, vol. {EMNLP} 2020.\hskip 1em plus 0.5em
  minus 0.4em\relax Association for Computational Linguistics, 2020.

\bibitem{LewisLGGMLSZ20}
M.~Lewis, Y.~Liu, N.~Goyal, M.~Ghazvininejad, A.~Mohamed, O.~Levy, V.~Stoyanov,
  and L.~Zettlemoyer, ``{BART:} denoising sequence-to-sequence pre-training for
  natural language generation, translation, and comprehension,'' in
  \emph{{ACL}}.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2020.

\bibitem{RaffelSRLNMZLL20}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a
  unified text-to-text transformer,'' \emph{J. Mach. Learn. Res.}, vol.~21,
  2020.

\bibitem{BrownMRSKDNSSAA20}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal,
  A.~Herbert{-}Voss, G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~M.
  Ziegler, J.~Wu, C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray,
  B.~Chess, J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and
  D.~Amodei, ``Language models are few-shot learners,'' in \emph{{NeurIPS}},
  2020.

\bibitem{LiuLSZ21}
Z.~Liu, W.~Lin, Y.~Shi, and J.~Zhao, ``A robustly optimized {BERT} pre-training
  approach with post-training,'' in \emph{Chinese Computational Linguistics
  {CCL}}, ser. Lecture Notes in Computer Science, vol. 12869.\hskip 1em plus
  0.5em minus 0.4em\relax Springer, 2021.

\bibitem{TransferTranfo}
T.~Wolf, V.~Sanh, J.~Chaumond, and C.~Delangue, ``Transfertransfo: {A} transfer
  learning approach for neural network based conversational agents,''
  \emph{CoRR}, vol. abs/1901.08149, 2019.

\bibitem{BudzianowskiV19}
P.~Budzianowski and I.~Vulic, ``Hello, it's {GPT-2} - how can {I} help you?
  towards the use of pretrained language models for task-oriented dialogue
  systems,'' in \emph{3rd Workshop on Neural Generation and
  Translation@{EMNLP-IJCNLP}}.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2019.

\bibitem{LiuWLXF20}
Z.~Liu, G.~I. Winata, Z.~Lin, P.~Xu, and P.~Fung, ``Attention-informed
  mixed-language training for zero-shot cross-lingual task-oriented dialogue
  systems,'' in \emph{{AAAI}}.\hskip 1em plus 0.5em minus 0.4em\relax {AAAI}
  Press, 2020.

\bibitem{ZhangSGCBGGLD20}
Y.~Zhang, S.~Sun, M.~Galley, Y.~Chen, C.~Brockett, X.~Gao, J.~Gao, J.~Liu, and
  B.~Dolan, ``{DIALOGPT} : Large-scale generative pre-training for
  conversational response generation,'' in \emph{{ACL} System
  Demonstrations}.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2020.

\bibitem{HODChatbot}
D.~Adiwardana, M.~Luong, D.~R. So, J.~Hall, N.~Fiedel, R.~Thoppilan, Z.~Yang,
  A.~Kulshreshtha, G.~Nemade, Y.~Lu, and Q.~V. Le, ``Towards a human-like
  open-domain chatbot,'' \emph{CoRR}, vol. abs/2001.09977, 2020.

\bibitem{MadottoCW0LLF20}
A.~Madotto, S.~Cahyawijaya, G.~I. Winata, Y.~Xu, Z.~Liu, Z.~Lin, and P.~Fung,
  ``Learning knowledge bases with parameters for task-oriented dialogue
  systems,'' in \emph{Findings of the {ACL}: {EMNLP}}, vol. {EMNLP}.\hskip 1em
  plus 0.5em minus 0.4em\relax Association for Computational Linguistics, 2020.

\bibitem{LinLHNGHFG21}
H.~Lin, N.~Lubis, S.~Hu, C.~van Niekerk, C.~Geishauser, M.~Heck, S.~Feng, and
  M.~Gasic, ``Domain-independent user simulation with transformers for
  task-oriented dialogue systems,'' in \emph{{SIGdial}}.\hskip 1em plus 0.5em
  minus 0.4em\relax Association for Computational Linguistics, 2021.

\bibitem{RohmatillahC21}
M.~Rohmatillah and J.~Chien, ``Causal confusion reduction for robust
  multi-domain dialogue policy,'' in \emph{{Interspeech}}.\hskip 1em plus 0.5em
  minus 0.4em\relax {ISCA}, 2021.

\bibitem{EppsEtAl}
M.~Epps, J.~Uribe, and M.~Korpusik, ``A new dataset for natural language
  understanding of exercise logs in a food and fitness spoken dialogue
  system,'' in \emph{2021 IEEE Spoken Language Technology Workshop {SLT}},
  2021.

\bibitem{Sun0BRRCR21}
W.~Sun, S.~Zhang, K.~Balog, Z.~Ren, P.~Ren, Z.~Chen, and M.~de~Rijke,
  ``Simulating user satisfaction for the evaluation of task-oriented dialogue
  systems,'' in \emph{{SIGIR}}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM},
  2021.

\bibitem{RollerDGJWLXOSB21}
S.~Roller, E.~Dinan, N.~Goyal, D.~Ju, M.~Williamson, Y.~Liu, J.~Xu, M.~Ott,
  E.~M. Smith, Y.~Boureau, and J.~Weston, ``Recipes for building an open-domain
  chatbot,'' in \emph{{EACL}}.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2021.

\bibitem{WuJ22}
T.~Wu and B.~Juang, ``Induce spoken dialog intents via deep unsupervised
  context contrastive clustering,'' in \emph{{Interspeech}}.\hskip 1em plus
  0.5em minus 0.4em\relax {ISCA}, 2022.

\bibitem{AbroARUMQ22}
W.~A. Abro, A.~Aicher, N.~Rach, S.~Ultes, W.~Minker, and G.~Qi, ``Natural
  language understanding for argumentative dialogue systems in the opinion
  building domain,'' \emph{Knowl. Based Syst.}, vol. 242, 2022.

\bibitem{GODEL}
B.~Peng, M.~Galley, P.~He, C.~Brockett, L.~Liden, E.~Nouri, Z.~Yu, B.~Dolan,
  and J.~Gao, ``{GODEL:} large-scale pre-training for goal-directed dialog,''
  \emph{CoRR}, vol. abs/2206.11309, 2022.

\bibitem{Jang0K22}
Y.~Jang, J.~Lee, and K.~Kim, ``{GPT-Critic}: Offline reinforcement learning for
  end-to-end task-oriented dialogue systems,'' in \emph{{ICLR}}.\hskip 1em plus
  0.5em minus 0.4em\relax OpenReview.net, 2022.

\bibitem{MHC}
A.~Srivastava, I.~Pandey, M.~S. Akhtar, and T.~Chakraborty, ``Response-act
  guided reinforced dialogue generation for mental health counseling,''
  \emph{CoRR}, vol. abs/2301.12729, 2023.

\bibitem{TakatsuYMHFK19}
H.~Takatsu, K.~Yokoyama, Y.~Matsuyama, H.~Honda, S.~Fujie, and T.~Kobayashi,
  ``Recognition of intentions of users' short responses for conversational news
  delivery system,'' in \emph{{Interspeech}}.\hskip 1em plus 0.5em minus
  0.4em\relax {ISCA}, 2019.

\bibitem{SlotCarryOver}
T.~Chen, C.~Naik, H.~He, P.~Rastogi, and L.~Mathias, ``Improving long distance
  slot carryover in spoken dialogue systems,'' \emph{CoRR}, vol.
  abs/1906.01149, 2019.

\bibitem{BERT_BiLSTM}
D.~Liu, Z.~Zhao, and L.-D. Gan, ``Intention detection based on bert-bilstm in
  taskoriented dialogue system,'' in \emph{International Computer Conference on
  Wavelet Active Media Technology and Information Processing}, 2019.

\bibitem{KorpusikLG19}
M.~Korpusik, Z.~Liu, and J.~R. Glass, ``A comparison of deep learning methods
  for language understanding,'' in \emph{{Interspeech}}.\hskip 1em plus 0.5em
  minus 0.4em\relax {ISCA}, 2019.

\bibitem{ZhangW19}
L.~Zhang and H.~Wang, ``Using bidirectional transformer-crf for spoken language
  understanding,'' in \emph{International Conference on Natural Language
  Processing and Chinese Computing {NLPCC}}, ser. Lecture Notes in Computer
  Science, vol. 11838.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2019.

\bibitem{QianSZ20}
Y.~Qian, Y.~Shi, and M.~Zeng, ``Discriminative transfer learning for optimizing
  {ASR} and semantic labeling in task-oriented spoken dialog,'' in
  \emph{{Interspeech}}.\hskip 1em plus 0.5em minus 0.4em\relax {ISCA}, 2020.

\bibitem{HongKK20}
T.~Hong, O.~Kwon, and Y.~Kim, ``End-to-end task-oriented dialog system through
  template slot value generation,'' in \emph{{Interspeech}}.\hskip 1em plus
  0.5em minus 0.4em\relax {ISCA}, 2020.

\bibitem{EkstedtS20}
E.~Ekstedt and G.~Skantze, ``{TurnGPT}: a transformer-based language model for
  predicting turn-taking in spoken dialog,'' in \emph{Findings of the {ACL}:
  {EMNLP}}, vol. {EMNLP} 2020.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2020.

\bibitem{0001HWLH20}
K.~Gopalakrishnan, B.~Hedayatnia, L.~Wang, Y.~Liu, and D.~Hakkani{-}T{\"{u}}r,
  ``Are neural open-domain dialog systems robust to speech recognition errors
  in the dialog history? an empirical study,'' in \emph{{Interspeech}}.\hskip
  1em plus 0.5em minus 0.4em\relax {ISCA}, 2020.

\bibitem{ChenCLW20}
Q.~Chen, M.~Chen, B.~Li, and W.~Wang, ``Controllable time-delay transformer for
  real-time punctuation prediction and disfluency detection,'' in
  \emph{{ICASSP}}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE}, 2020.

\bibitem{HoriMHR20}
T.~Hori, N.~Moritz, C.~Hori, and J.~L. Roux, ``Transformer-based long-context
  end-to-end speech recognition,'' in \emph{{Interspeech}}.\hskip 1em plus
  0.5em minus 0.4em\relax {ISCA}, 2020.

\bibitem{Lian0T21}
Z.~Lian, B.~Liu, and J.~Tao, ``Ctnet: Conversational transformer network for
  emotion recognition,'' \emph{{IEEE} {ACM} Trans. Audio Speech Lang.
  Process.}, vol.~29, 2021.

\bibitem{ComparativeStudy}
V.~M. Andreas, G.~I. Winata, and A.~Purwarianti, ``A comparative study on
  language models for task-oriented dialogue systems,'' \emph{CoRR}, vol.
  abs/2201.08687, 2022.

\bibitem{ChapuisCMLC20}
E.~Chapuis, P.~Colombo, M.~Manica, M.~Labeau, and C.~Clavel, ``Hierarchical
  pre-training for sequence labelling in spoken dialog,'' in \emph{Findings of
  the {ACL}: {EMNLP}}, ser. Findings of {ACL}, vol. {EMNLP} 2020.\hskip 1em
  plus 0.5em minus 0.4em\relax Association for Computational Linguistics, 2020.

\bibitem{TextNorm21}
T.~M. Lai, Y.~Zhang, E.~Bakhturina, B.~Ginsburg, and H.~Ji, ``A unified
  transformer-based framework for duplex text normalization,'' \emph{CoRR},
  vol. abs/2108.09889, 2021.

\bibitem{KimKSL21}
S.~Kim, G.~Kim, S.~Shin, and S.~Lee, ``Two-stage textual knowledge distillation
  for end-to-end spoken language understanding,'' in \emph{{ICASSP}}.\hskip 1em
  plus 0.5em minus 0.4em\relax {IEEE}, 2021.

\bibitem{Lopez-ZorrillaT21}
A.~L{\'{o}}pez{-}Zorrilla, M.~I. Torres, and H.~Cuay{\'{a}}huitl, ``Audio
  embeddings help to learn better dialogue policies,'' in \emph{{ASRU}}.\hskip
  1em plus 0.5em minus 0.4em\relax {IEEE}, 2021.

\bibitem{BechetRHAMD21}
F.~B{\'{e}}chet, C.~Raymond, A.~Hamane, R.~Abrougui, G.~Marzinotto, and
  G.~Damnati, ``Can we predict how challenging spoken language understanding
  corpora are across sources, languages, and domains?'' in \emph{Conversational
  {AI} for Natural Human-Centric Interaction - International Workshop on Spoken
  Dialogue System Technology {IWSDS}}, ser. Lecture Notes in Electrical
  Engineering, vol. 943.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2021.

\bibitem{OkurEtAl}
E.~Okur, S.~Sahay, R.~Fuentes{-}Alba, and L.~Nachman, ``End-to-end evaluation
  of a spoken dialogue system for learning basic mathematics,'' \emph{CoRR},
  vol. abs/2211.03511, 2022.

\bibitem{SakumaFK22}
J.~Sakuma, S.~Fujie, and T.~Kobayashi, ``Response timing estimation for spoken
  dialog systems based on syntactic completeness prediction,'' in
  \emph{{SLT}}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE}, 2022.

\bibitem{LinWHSSL22}
T.~Lin, Y.~Wu, F.~Huang, L.~Si, J.~Sun, and Y.~Li, ``Duplex conversation:
  Towards human-like interaction in spoken dialogue systems,'' in
  \emph{{KDD}}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM}, 2022.

\bibitem{AhmedPhDThesis}
A.~Waheed, ``Combining neural networks with knowledge for spoken dialogue
  systems,'' Ph.D. dissertation, Ulm University, 2022.

\bibitem{BekalEtAl}
D.~Bekal, S.~Srinivasan, S.~Bodapati, S.~Ronanki, and K.~Kirchhoff, ``Device
  directedness with contextual cues for spoken dialog systems,'' \emph{CoRR},
  vol. abs/2211.13280, 2022.

\bibitem{DongFZLW22}
J.~Dong, J.~Fu, P.~Zhou, H.~Li, and X.~Wang, ``Improving spoken language
  understanding with cross-modal contrastive learning,'' in
  \emph{{Interspeech}}.\hskip 1em plus 0.5em minus 0.4em\relax {ISCA}, 2022.

\bibitem{SvecFBL22}
J.~Svec, A.~Fr{\'{e}}mund, M.~Bul{\'{\i}}n, and J.~Lehecka, ``Transfer learning
  of transformers for spoken language understanding,'' in \emph{International
  Conference on Text, Speech, and Dialogue {(TSD)}}, ser. Lecture Notes in
  Computer Science, vol. 13502.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2022.

\bibitem{ShenHZZX22}
W.~Shen, X.~He, C.~Zhang, X.~Zhang, and J.~Xie, ``A transformer-based user
  satisfaction prediction for proactive interaction mechanism in dueros,'' in
  \emph{International Conference on Information {\&} Knowledge Management
  {(CIKM)}}, M.~A. Hasan and L.~Xiong, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax {ACM}, 2022.

\bibitem{YangWZFCH22}
J.~Yang, P.~Wang, Y.~Zhu, M.~Feng, M.~Chen, and X.~He, ``Gated multimodal
  fusion with contrastive learning for turn-taking prediction in human-robot
  dialogue,'' in \emph{{ICASSP}}.\hskip 1em plus 0.5em minus 0.4em\relax
  {IEEE}, 2022.

\bibitem{SunderTKGKF22}
V.~Sunder, S.~Thomas, H.~J. Kuo, J.~Ganhotra, B.~Kingsbury, and
  E.~Fosler{-}Lussier, ``Towards end-to-end integration of dialog history for
  improved spoken language understanding,'' in \emph{{ICASSP}}.\hskip 1em plus
  0.5em minus 0.4em\relax {IEEE}, 2022.

\bibitem{Lopez-ZorrillaT23}
A.~L{\'{o}}pez{-}Zorrilla, M.~I. Torres, and H.~Cuay{\'{a}}huitl, ``Audio
  embedding-aware dialogue policy learning,'' \emph{{IEEE} {ACM} Trans. Audio
  Speech Lang. Process.}, vol.~31, 2023.

\bibitem{FirdausEC23}
M.~Firdaus, A.~Ekbal, and E.~Cambria, ``Multitask learning for multilingual
  intent detection and slot filling in dialogue systems,'' \emph{Inf. Fusion},
  vol.~91, 2023.

\bibitem{MeiWTDH23}
J.~Mei, Y.~Wang, X.~Tu, M.~Dong, and T.~He, ``Incorporating {BERT} with
  probability-aware gate for spoken language understanding,'' \emph{{IEEE}
  {ACM} Trans. Audio Speech Lang. Process.}, vol.~31, 2023.

\bibitem{InstructGPT}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~L. Wainwright, P.~Mishkin, C.~Zhang,
  S.~Agarwal, K.~Slama, A.~Ray, J.~Schulman, J.~Hilton, F.~Kelton, L.~Miller,
  M.~Simens, A.~Askell, P.~Welinder, P.~F. Christiano, J.~Leike, and R.~Lowe,
  ``Training language models to follow instructions with human feedback,''
  \emph{CoRR}, vol. abs/2203.02155, 2022.

\bibitem{WangSMHLB19}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman, ``{GLUE:}
  {A} multi-task benchmark and analysis platform for natural language
  understanding,'' in \emph{International Conference on Learning
  Representations {ICLR}}.\hskip 1em plus 0.5em minus 0.4em\relax
  OpenReview.net, 2019.

\bibitem{chuang2019speechbert}
Y.-S. Chuang, C.-L. Liu, H.-Y. Lee, and L.-s. Lee, ``Speechbert: An
  audio-and-text jointly learned language model for end-to-end spoken question
  answering,'' \emph{arXiv preprint arXiv:1910.11559}, 2019.

\bibitem{song2019speech}
X.~Song, G.~Wang, Z.~Wu, Y.~Huang, D.~Su, D.~Yu, and H.~Meng, ``Speech-xlnet:
  Unsupervised acoustic model pretraining for self-attention networks,''
  \emph{arXiv preprint arXiv:1910.10387}, 2019.

\bibitem{arjmand2021teasel}
M.~Arjmand, M.~J. Dousti, and H.~Moradi, ``Teasel: a transformer-based
  speech-prefixed language model,'' \emph{arXiv preprint arXiv:2109.05522},
  2021.

\bibitem{sant2022multiformer}
G.~Sant, G.~I. G{\'a}llego, B.~Alastruey, and M.~R. Costa-Juss{\`a},
  ``Multiformer: A head-configurable transformer-based model for direct speech
  translation,'' \emph{arXiv preprint arXiv:2205.07100}, 2022.

\bibitem{lin2022compressing}
T.-Q. Lin, T.-H. Yang, C.-Y. Chang, K.-M. Chen, T.-h. Feng, H.-y. Lee, and
  H.~Tang, ``Compressing transformer-based self-supervised models for speech
  processing,'' \emph{arXiv preprint arXiv:2211.09949}, 2022.

\bibitem{chung2018speech2vec}
Y.-A. Chung and J.~Glass, ``Speech2vec: A sequence-to-sequence framework for
  learning word embeddings from speech,'' \emph{arXiv preprint
  arXiv:1803.08976}, 2018.

\bibitem{baltrusaitis2019multimodal}
T.~Baltru{\v{s}}aitis, C.~Ahuja, and L.-P. Morency, ``Multimodal machine
  learning: A survey and taxonomy,'' \emph{IEEE transactions on pattern
  analysis and machine intelligence}, vol.~41, no.~2, pp. 423--443, 2019.

\bibitem{zadeh2017tensor}
A.~Zadeh, M.~Chen, S.~Poria, E.~Cambria, and L.-P. Morency, ``Tensor fusion
  network for multimodal sentiment analysis,'' \emph{arXiv preprint
  arXiv:1707.07250}, 2017.

\bibitem{lee2020parameter}
S.~Lee, Y.~Yu, G.~Kim, T.~Breuel, J.~Kautz, and Y.~Song, ``Parameter efficient
  multimodal transformers for video representation learning,'' \emph{arXiv
  preprint arXiv:2012.04124}, 2020.

\bibitem{nagrani2021attention}
A.~Nagrani, S.~Yang, A.~Arnab, A.~Jansen, C.~Schmid, and C.~Sun, ``Attention
  bottlenecks for multimodal fusion,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~34, pp. 14\,200--14\,213, 2021.

\bibitem{strudel2021segmenter}
R.~Strudel, R.~Garcia, I.~Laptev, and C.~Schmid, ``Segmenter: Transformer for
  semantic segmentation,'' in \emph{Proceedings of the IEEE/CVF International
  Conference on Computer Vision}, 2021, pp. 7262--7272.

\bibitem{kim2021vilt}
W.~Kim, B.~Son, and I.~Kim, ``Vilt: Vision-and-language transformer without
  convolution or region supervision,'' in \emph{International Conference on
  Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp.
  5583--5594.

\bibitem{tsai2019multimodal}
Y.-H.~H. Tsai, S.~Bai, P.~P. Liang, J.~Z. Kolter, L.-P. Morency, and
  R.~Salakhutdinov, ``Multimodal transformer for unaligned multimodal language
  sequences,'' in \emph{Proceedings of the conference. Association for
  Computational Linguistics. Meeting}, vol. 2019.\hskip 1em plus 0.5em minus
  0.4em\relax NIH Public Access, 2019, p. 6558.

\bibitem{sun2019videobert}
C.~Sun, A.~Myers, C.~Vondrick, K.~Murphy, and C.~Schmid, ``Videobert: A joint
  model for video and language representation learning,'' in \emph{Proceedings
  of the IEEE/CVF International Conference on Computer Vision}, 2019, pp.
  7464--7473.

\bibitem{huang2021unifying}
Y.~Huang, H.~Xue, B.~Liu, and Y.~Lu, ``Unifying multimodal transformer for
  bi-directional image and text generation,'' in \emph{Proceedings of the 29th
  ACM International Conference on Multimedia}, 2021, pp. 1138--1147.

\bibitem{zellers2021merlot}
R.~Zellers, X.~Lu, J.~Hessel, Y.~Yu, J.~S. Park, J.~Cao, A.~Farhadi, and
  Y.~Choi, ``Merlot: Multimodal neural script knowledge models,''
  \emph{Advances in Neural Information Processing Systems}, vol.~34, pp.
  23\,634--23\,651, 2021.

\bibitem{gabeur2020multi}
V.~Gabeur, C.~Sun, K.~Alahari, and C.~Schmid, ``Multi-modal transformer for
  video retrieval,'' in \emph{European Conference on Computer Vision}.\hskip
  1em plus 0.5em minus 0.4em\relax Springer, 2020, pp. 214--229.

\bibitem{chen2020uniter}
Y.-C. Chen, L.~Li, L.~Yu, A.~El~Kholy, F.~Ahmed, Z.~Gan, Y.~Cheng, and J.~Liu,
  ``Uniter: Universal image-text representation learning,'' in \emph{European
  conference on computer vision}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2020, pp. 104--120.

\bibitem{akbari2021vatt}
H.~Akbari, L.~Yuan, R.~Qian, W.-H. Chuang, S.-F. Chang, Y.~Cui, and B.~Gong,
  ``Vatt: Transformers for multimodal self-supervised learning from raw video,
  audio and text,'' \emph{Advances in Neural Information Processing Systems},
  vol.~34, pp. 24\,206--24\,221, 2021.

\bibitem{li2020closer}
L.~Li, Z.~Gan, and J.~Liu, ``A closer look at the robustness of
  vision-and-language pre-trained models,'' \emph{arXiv preprint
  arXiv:2012.08673}, 2020.

\bibitem{zadeh2019factorized}
A.~Zadeh, C.~Mao, K.~Shi, Y.~Zhang, P.~P. Liang, S.~Poria, and L.-P. Morency,
  ``Factorized multimodal transformer for multimodal sequential learning,''
  \emph{arXiv preprint arXiv:1911.09826}, 2019.

\bibitem{hendricks2021decoupling}
L.~A. Hendricks, J.~Mellor, R.~Schneider, J.-B. Alayrac, and A.~Nematzadeh,
  ``Decoupling the role of data, attention, and losses in multimodal
  transformers,'' \emph{Transactions of the Association for Computational
  Linguistics}, vol.~9, pp. 570--585, 2021.

\bibitem{liu2020understanding}
L.~Liu, X.~Liu, J.~Gao, W.~Chen, and J.~Han, ``Understanding the difficulty of
  training transformers,'' in \emph{Conference on Empirical Methods in Natural
  Language Processing, EMNLP 2020}.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics (ACL), 2020.

\bibitem{povey2018time}
D.~Povey, H.~Hadian, P.~Ghahremani, K.~Li, and S.~Khudanpur, ``A
  time-restricted self-attention layer for asr,'' in \emph{2018 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 5874--5878.

\bibitem{yeh2019transformer}
C.-F. Yeh, J.~Mahadeokar, K.~Kalgaonkar, Y.~Wang, D.~Le, M.~Jain, K.~Schubert,
  C.~Fuegen, and M.~L. Seltzer, ``Transformer-transducer: End-to-end speech
  recognition with self-attention,'' \emph{arXiv preprint arXiv:1910.12977},
  2019.

\bibitem{sperber2018self}
M.~Sperber, J.~Niehues, G.~Neubig, S.~St{\"u}ker, and A.~Waibel,
  ``Self-attentional acoustic models,'' \emph{Proc. Interspeech 2018}, pp.
  3723--3727, 2018.

\bibitem{salazar2019self}
J.~Salazar, K.~Kirchhoff, and Z.~Huang, ``Self-attention networks for
  connectionist temporal classification in speech recognition,'' in
  \emph{Icassp 2019-2019 ieee international conference on acoustics, speech and
  signal processing (icassp)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2019, pp. 7115--7119.

\bibitem{dong2019self}
L.~Dong, F.~Wang, and B.~Xu, ``Self-attention aligner: A latency-control
  end-to-end model for {ASR} using self-attention network and chunk-hopping,''
  in \emph{ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2019, pp. 5656--5660.

\bibitem{bie2019simplified}
A.~Bie, B.~Venkitesh, J.~Monteiro, M.~Haidar, M.~Rezagholizadeh \emph{et~al.},
  ``A simplified fully quantized transformer for end-to-end speech
  recognition,'' \emph{arXiv preprint arXiv:1911.03604}, 2019.

\bibitem{likhomanenko2021cape}
T.~Likhomanenko, Q.~Xu, G.~Synnaeve, R.~Collobert, and A.~Rogozhnikov,
  ``{CAPE}: Encoding relative positions with continuous augmented positional
  embeddings,'' \emph{Advances in Neural Information Processing Systems},
  vol.~34, pp. 16\,079--16\,092, 2021.

\bibitem{zhao2022adaptive}
C.~Zhao, J.~Wang, X.~Qu, H.~Wang, J.~Xiao \emph{et~al.}, ``Adaptive sparse and
  monotonic attention for transformer-based automatic speech recognition,''
  \emph{arXiv preprint arXiv:2209.15176}, 2022.

\bibitem{woo2021speech}
B.~J. Woo, H.~Y. Kim, J.~Kim, and N.~S. Kim, ``Speech separation based on
  dptnet with sparse attention,'' in \emph{2021 7th IEEE International
  Conference on Network Intelligence and Digital Content (IC-NIDC)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 339--343.

\bibitem{avinava2021constructing}
M.~Zaheer, G.~Guruganesh, A.~Dubey, J.~Ainslie, C.~Alberti, S.~Ontanon,
  P.~Pham, A.~Ravula, Q.~Wang, L.~Yang \emph{et~al.}, ``Constructing
  transformers for longer sequences with sparse attention methods,''
  \emph{Google AI Blog}, 2021.

\bibitem{winata2020lightweight}
G.~I. Winata, S.~Cahyawijaya, Z.~Lin, Z.~Liu, and P.~Fung, ``Lightweight and
  efficient end-to-end speech recognition using low-rank transformer,'' in
  \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2020, pp. 6144--6148.

\bibitem{krzysztof2020rethinking}
K.~Choromanski and L.~Colwell, ``Rethinking attention with performers,''
  \emph{Google AI Blog}, 2020.

\bibitem{kitaev2020reformer}
N.~Kitaev, {\L}.~Kaiser, and A.~Levskaya, ``Reformer: The efficient
  transformer,'' \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem{roy2021efficient}
A.~Roy, M.~Saffar, A.~Vaswani, and D.~Grangier, ``Efficient content-based
  sparse attention with routing transformers,'' \emph{Transactions of the
  Association for Computational Linguistics}, vol.~9, pp. 53--68, 2021.

\bibitem{yu2022auxiliary}
Y.~Yu, D.~Park, and H.~K. Kim, ``Auxiliary loss of transformer with residual
  connection for end-to-end speaker diarization,'' in \emph{ICASSP 2022-2022
  IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 8377--8381.

\bibitem{duan2022dual}
Z.~Duan, G.~Gao, J.~Chen, S.~Li, J.~Ruan, G.~Yang, and X.~Yu, ``Dual-residual
  transformer network for speech recognition,'' \emph{Journal of the Audio
  Engineering Society}, vol.~70, no.~10, pp. 871--881, 2022.

\bibitem{walmart2021sparse}
W.~G. Tech, ``Sparse transformers and longformers: A comprehensive summary of
  space and time optimizations on transformer architectures,''
  \url{https://medium.com/walmartglobaltech/sparse-transformers-and-longformers-a-comprehensive-summary-of-space-and-time-optimizations-on-4caa5c388693},
  2021, accessed: 2022-01-28.

\bibitem{xiao2019sharing}
T.~Xiao, Y.~Li, J.~Zhu, Z.~Yu, and T.~Liu, ``Sharing attention weights for fast
  transformer,'' \emph{arXiv preprint arXiv:1906.11024}, 2019.

\bibitem{han2021connection}
Q.~Han, Z.~Fan, Q.~Dai, L.~Sun, M.-M. Cheng, J.~Liu, and J.~Wang, ``On the
  connection between local attention and dynamic depth-wise convolution,''
  \emph{arXiv preprint arXiv:2106.04263}, 2021.

\bibitem{li2021differentiable}
J.~Li, R.~Cotterell, and M.~Sachan, ``Differentiable subset pruning of
  transformer heads,'' \emph{Transactions of the Association for Computational
  Linguistics}, vol.~9, pp. 1442--1459, 2021.

\bibitem{li2019improving}
S.~Li, R.~Dabre, X.~Lu, P.~Shen, T.~Kawahara, and H.~Kawai, ``Improving
  transformer-based speech recognition systems with compressed structure and
  speech attributes augmentation.'' in \emph{Interspeech}, 2019, pp.
  4400--4404.

\bibitem{chen2021developing}
X.~Chen, Y.~Wu, Z.~Wang, S.~Liu, and J.~Li, ``Developing real-time streaming
  transformer transducer for speech recognition on large-scale dataset,'' in
  \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2021, pp. 5904--5908.

\bibitem{shi2021emformer}
Y.~Shi, Y.~Wang, C.~Wu, C.-F. Yeh, J.~Chan, F.~Zhang, D.~Le, and M.~Seltzer,
  ``Emformer: Efficient memory transformer based acoustic model for low latency
  streaming speech recognition,'' in \emph{ICASSP 2021-2021 IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 6783--6787.

\bibitem{ma2019tensorized}
X.~Ma, P.~Zhang, S.~Zhang, N.~Duan, Y.~Hou, M.~Zhou, and D.~Song, ``A
  tensorized transformer for language modeling,'' \emph{Advances in neural
  information processing systems}, vol.~32, 2019.

\bibitem{gu2022heat}
J.~Gu, B.~Keller, J.~Kossaifi, A.~Anandkumar, B.~Khailany, and D.~Z. Pan,
  ``Heat: Hardware-efficient automatic tensor decomposition for transformer
  compression,'' \emph{arXiv preprint arXiv:2211.16749}, 2022.

\bibitem{pham2022tt}
H.~Pham~Minh, N.~Nguyen~Xuan, and S.~Tran~Thai, ``Tt-vit: Vision transformer
  compression using tensor-train decomposition,'' in \emph{Computational
  Collective Intelligence: 14th International Conference, ICCCI 2022, Hammamet,
  Tunisia, September 28--30, 2022, Proceedings}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2022, pp. 755--767.

\bibitem{li2022hypoformer}
S.~Li, P.~Zhang, G.~Gan, X.~Lv, B.~Wang, J.~Wei, and X.~Jiang, ``Hypoformer:
  Hybrid decomposition transformer for edge-friendly neural machine
  translation,'' in \emph{Proceedings of the 2022 Conference on Empirical
  Methods in Natural Language Processing}, 2022, pp. 7056--7068.

\bibitem{denis2022accelerated}
D.~Timonin, B.~Y. Hsueh, and V.~Nguyen, ``Accelerated inference for large
  transformer models using nvidia triton inference server,''
  \url{https://developer.nvidia.com/blog/accelerated-inference-for-large-transformer-models-using-nvidia-fastertransformer-and-nvidia-triton-inference-server/},
  Aug 2022.

\bibitem{xu2021mixed}
J.~Xu, S.~Hu, J.~Yu, X.~Liu, and H.~Meng, ``Mixed precision quantization of
  transformer language models for speech recognition,'' in \emph{ICASSP
  2021-2021 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp.
  7383--7387.

\bibitem{wang2020hat}
H.~Wang, Z.~Wu, Z.~Liu, H.~Cai, L.~Zhu, C.~Gan, and S.~Han, ``Hat:
  Hardware-aware transformers for efficient natural language processing,''
  \emph{arXiv preprint arXiv:2005.14187}, 2020.

\bibitem{kuchaiev2018mixed}
O.~Kuchaiev, B.~Ginsburg, I.~Gitman, V.~Lavrukhin, J.~Li, H.~Nguyen, C.~Case,
  and P.~Micikevicius, ``Mixed-precision training for nlp and speech
  recognition with openseq2seq,'' \emph{arXiv preprint arXiv:1805.10387}, 2018.

\bibitem{miech2021thinking}
A.~Miech, J.-B. Alayrac, I.~Laptev, J.~Sivic, and A.~Zisserman, ``Thinking fast
  and slow: Efficient text-to-visual retrieval with transformers,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2021, pp. 9826--9836.

\bibitem{touvron2021training}
H.~Touvron, M.~Cord, M.~Douze, F.~Massa, A.~Sablayrolles, and H.~J{\'e}gou,
  ``Training data-efficient image transformers \& distillation through
  attention,'' in \emph{International conference on machine learning}.\hskip
  1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp. 10\,347--10\,357.

\bibitem{xu2021e2e}
H.~Xu, M.~Yan, C.~Li, B.~Bi, S.~Huang, W.~Xiao, and F.~Huang, ``E2e-vlp:
  end-to-end vision-language pre-training enhanced by visual learning,''
  \emph{arXiv preprint arXiv:2106.01804}, 2021.

\bibitem{wen2021cookie}
K.~Wen, J.~Xia, Y.~Huang, L.~Li, J.~Xu, and J.~Shao, ``Cookie: Contrastive
  cross-modal knowledge sharing pre-training for vision-language
  representation,'' in \emph{Proceedings of the IEEE/CVF International
  Conference on Computer Vision}, 2021, pp. 2208--2217.

\bibitem{li2021supervision}
Y.~Li, F.~Liang, L.~Zhao, Y.~Cui, W.~Ouyang, J.~Shao, F.~Yu, and J.~Yan,
  ``Supervision exists everywhere: A data efficient contrastive language-image
  pre-training paradigm,'' \emph{arXiv preprint arXiv:2110.05208}, 2021.

\bibitem{gan2022playing}
Z.~Gan, Y.-C. Chen, L.~Li, T.~Chen, Y.~Cheng, S.~Wang, J.~Liu, L.~Wang, and
  Z.~Liu, ``Playing lottery tickets with vision and language,'' in
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~36, no.~1, 2022, pp. 652--660.

\bibitem{child2019generating}
R.~Child, S.~Gray, A.~Radford, and I.~Sutskever, ``Generating long sequences
  with sparse transformers,'' \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem{gan2021transformerls}
Z.~Gan, Y.-C. Li, Y.~Cheng, J.~Liu, and J.~Gao, ``Long-short transformer:
  Efficient transformers for language and vision,'' \emph{arXiv preprint
  arXiv:2107.02192}, 2021.

\bibitem{yan2022multiview}
S.~Yan, X.~Xiong, A.~Arnab, Z.~Lu, M.~Zhang, C.~Sun, and C.~Schmid, ``Multiview
  transformers for video recognition,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2022, pp. 3333--3343.

\bibitem{ma2020data}
X.~Ma, W.-N. Zhang, and C.~Wang, ``Data augmentation for end-to-end speech
  recognition,'' \emph{arXiv preprint arXiv:2301.02111}, 2020.

\bibitem{park2019specaugment}
D.~S. Park, W.~Chan, Y.~Zhang, C.-C. Chiu, B.~Zoph, E.~D. Cubuk, and Q.~V. Le,
  ``Specaugment: A simple data augmentation method for automatic speech
  recognition,'' \emph{arXiv preprint arXiv:1904.08779}, 2019.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly \emph{et~al.},
  ``An image is worth 16x16 words: Transformers for image recognition at
  scale,'' \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{xue2021bayesian}
B.~Xue, J.~Yu, J.~Xu, S.~Liu, S.~Hu, Z.~Ye, M.~Geng, X.~Liu, and H.~Meng,
  ``Bayesian transformer language models for speech recognition,'' in
  \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2021, pp. 7378--7382.

\bibitem{zhou2019improving}
P.~Zhou, R.~Fan, W.~Chen, and J.~Jia, ``Improving generalization of transformer
  for speech recognition with parallel schedule sampling and relative
  positional embedding,'' \emph{arXiv preprint arXiv:1911.00203}, 2019.

\bibitem{gan2020large}
Z.~Gan, Y.-C. Chen, L.~Li, C.~Zhu, Y.~Cheng, and J.~Liu, ``Large-scale
  adversarial training for vision-and-language representation learning,''
  \emph{Advances in Neural Information Processing Systems}, vol.~33, pp.
  6616--6628, 2020.

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark \emph{et~al.}, ``Learning transferable visual
  models from natural language supervision,'' in \emph{International conference
  on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp.
  8748--8763.

\bibitem{kervadec2021supervising}
C.~Kervadec, C.~Wolf, G.~Antipov, M.~Baccouche, and M.~Nadri, ``Supervising the
  transfer of reasoning patterns in vqa,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~34, pp. 18\,256--18\,267, 2021.

\bibitem{zhan2021product1m}
X.~Zhan, Y.~Wu, X.~Dong, Y.~Wei, M.~Lu, Y.~Zhang, H.~Xu, and X.~Liang,
  ``Product1{M}: Towards weakly supervised instance-level product retrieval via
  cross-modal pretraining,'' in \emph{Proceedings of the IEEE/CVF International
  Conference on Computer Vision}, 2021, pp. 11\,782--11\,791.

\bibitem{rahman2020integrating}
W.~Rahman, M.~K. Hasan, S.~Lee, A.~Zadeh, C.~Mao, L.-P. Morency, and E.~Hoque,
  ``Integrating multimodal information in large pretrained transformers,'' in
  \emph{Proceedings of the conference. Association for Computational
  Linguistics. Meeting}, vol. 2020.\hskip 1em plus 0.5em minus 0.4em\relax NIH
  Public Access, 2020, p. 2359.

\bibitem{xia2021xgpt}
Q.~Xia, H.~Huang, N.~Duan, D.~Zhang, L.~Ji, Z.~Sui, E.~Cui, T.~Bharti, and
  M.~Zhou, ``{XGPT}: Cross-modal generative pre-training for image
  captioning,'' in \emph{Natural Language Processing and Chinese Computing:
  10th CCF International Conference, NLPCC 2021, Qingdao, China, October
  13--17, 2021, Proceedings, Part I 10}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2021, pp. 786--797.

\bibitem{zhou2021uc2}
M.~Zhou, L.~Zhou, S.~Wang, Y.~Cheng, L.~Li, Z.~Yu, and J.~Liu, ``Uc2: Universal
  cross-lingual cross-modal vision-and-language pre-training,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2021, pp. 4155--4165.

\bibitem{ni2021m3p}
M.~Ni, H.~Huang, L.~Su, E.~Cui, T.~Bharti, L.~Wang, D.~Zhang, and N.~Duan,
  ``M3p: Learning universal representations via multitask multilingual
  multimodal pre-training,'' in \emph{Proceedings of the IEEE/CVF conference on
  computer vision and pattern recognition}, 2021, pp. 3977--3986.

\bibitem{chen1998audio}
T.~Chen and R.~Rao, ``Audio-visual integration in multimodal communication,''
  \emph{Proceedings of the IEEE}, vol.~86, no.~5, pp. 837--852, 1998.

\bibitem{sahu2020low}
A.~K. Sahu, L.-P. Morency, and T.~Baltru{\v{s}}aitis, ``Low rank fusion based
  transformers for multimodal sequences,'' in \emph{Proceedings of The Third
  Works}.

\bibitem{gao2019dynamic}
P.~Gao, Z.~Jiang, H.~You, Z.~Lu, S.~C. Hoi, and X.~Wang, ``Dynamic fusion with
  intra- and inter-modality attention flow for visual question answering,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2019, p. 6639â€“6648.

\bibitem{li2021causal}
Z.~Li, Z.~Liu, and X.~Zhang, ``Causal attention for vision-language tasks,'' in
  \emph{Proceedings of the IEEE/CVF International Conference on Computer
  Vision}, 2021, p. 10533â€“10542.

\bibitem{chen2021crossvit}
Y.~Chen, Y.~Wang, X.~Liu, C.~Qian, L.~Lin, and C.~C. Loy, ``Crossvit:
  Cross-attention multi-scale vision transformer for image classification,'' in
  \emph{Proceedings of the IEEE/CVF International Conference on Computer
  Vision}, 2021, p. 10442â€“10451.

\bibitem{li2019visualbert}
L.~H. Li, M.~Yatskar, D.~Yin, C.-J. Hsieh, and K.-W. Chang, ``Visualbert: A
  simple and performant baseline for vision and language,'' \emph{arXiv
  preprint arXiv:1908.03557}, 2019.

\bibitem{li2020unicoder}
G.~Li, N.~Duan, Y.~Fang, D.~Jiang, and M.~Zhou, ``Unicoder-vl: A universal
  encoder for vision and language by cross-modal pre-training,'' \emph{arXiv
  preprint arXiv:1908.06066}, 2020.

\bibitem{li2020oscar}
X.~Li, X.~Yin, C.~Li, P.~Zhang, X.~Hu, L.~Zhang, L.~Wang, H.~Hu, L.~Dong,
  F.~Wei \emph{et~al.}, ``Oscar: Object-semantics aligned pre-training for
  vision-language tasks,'' in \emph{Computer Vision--ECCV 2020: 16th European
  Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXX
  16}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2020, pp. 121--137.

\bibitem{huang2020pixel}
Z.~Huang, Z.~Zeng, B.~Liu, D.~Fu, and J.~Fu, ``Pixel-bert: Aligning image
  pixels with text by deep multi-modal transformers,'' \emph{arXiv preprint
  arXiv:2004.00849}, 2020.

\bibitem{zhu2020actbert}
L.~Zhu, Z.~Xu, and Y.~Yang, ``Actbert: Learning global-local video-text
  representations,'' in \emph{Proceedings of the IEEE/CVF Conference on
  Computer Vision and Pattern Recognition}, 2020, pp. 9697--9706.

\bibitem{qi2020imagebert}
W.~Qi, Y.~Su, Y.~Zhu, Q.~Huang, L.~Li, and G.~Wang, ``Imagebert: Cross-modal
  pre-training with large-scale weak-supervised image-text data,'' \emph{arXiv
  preprint arXiv:2001.07966}, 2020.

\bibitem{zhuge2021kaleidobert}
C.~Zhuge, H.~Zhang, J.~Liang, X.~Zhang, and Z.~Luo, ``Kaleido-bert:
  Vision-language pre-training on fashion domain,'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021, p.
  14529â€“14538.

\bibitem{owens2018audio}
A.~Owens, A.~A. Wu, J.~H. McDermott, W.~T. Freeman, and A.~Torralba,
  ``Audio-visual scene analysis with self-supervised multisensory features,''
  \emph{arXiv preprint arXiv:1804.03641}, 2018.

\bibitem{liu2021probing}
Y.~Liu, H.~Zhang, X.~Liang, P.~Liang, and J.~Sun, ``Probing inter-modality:
  Visual parsing with self-attention for vision-language pre-training,''
  \emph{arXiv preprint arXiv:2106.13488}, 2021.

\bibitem{morgado2020learning}
P.~Morgado, Y.~Li, and N.~Nvasconcelos, ``Learning representations from
  audio-visual spatial alignment,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~33, pp. 4733--4744, 2020.

\bibitem{jia2021scaling}
C.~Jia, Y.~Yang, Y.~Xia, Y.-T. Chen, Z.~Parekh, H.~Pham, Q.~Le, Y.-H. Sung,
  Z.~Li, and T.~Duerig, ``Scaling up visual and vision-language representation
  learning with noisy text supervision,'' in \emph{International Conference on
  Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp.
  4904--4916.

\bibitem{xu2021videoclip}
H.~Xu, G.~Ghosh, P.-Y. Huang, D.~Okhonko, A.~Aghajanyan, F.~Metze,
  L.~Zettlemoyer, and C.~Feichtenhofer, ``Videoclip: Contrastive pre-training
  for zero-shot video-text understanding,'' \emph{arXiv preprint
  arXiv:2109.14084}, 2021.

\bibitem{lei2021less}
J.~Lei, L.~Li, L.~Zhou, Z.~Gan, T.~L. Berg, M.~Bansal, and J.~Liu, ``Less is
  more: Clipbert for video-and-language learning via sparse sampling,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2021, pp. 7331--7341.

\bibitem{wang2022clip}
Z.~Wang, N.~Codella, Y.-C. Chen, L.~Zhou, J.~Yang, X.~Dai, B.~Xiao, H.~You,
  S.-F. Chang, and L.~Yuan, ``Clip-td: Clip targeted distillation for
  vision-language tasks,'' \emph{arXiv preprint arXiv:2201.05729}, 2022.

\bibitem{li2022align}
D.~Li, J.~Li, H.~Li, J.~C. Niebles, and S.~C. Hoi, ``Align and prompt:
  Video-and-language pre-training with entity prompts,'' in \emph{Proceedings
  of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2022,
  pp. 4953--4963.

\bibitem{luo2022clip4clip}
H.~Luo, L.~Ji, M.~Zhong, Y.~Chen, W.~Lei, N.~Duan, and T.~Li, ``Clip4clip: An
  empirical study of clip for end to end video clip retrieval and captioning,''
  \emph{Neurocomputing}, vol. 508, pp. 293--304, 2022.

\bibitem{fang2021clip2video}
H.~Fang, P.~Xiong, L.~Xu, and Y.~Chen, ``Clip2video: Mastering video-text
  retrieval via image clip,'' \emph{arXiv preprint arXiv:2106.11097}, 2021.

\bibitem{narasimhan2021clip}
M.~Narasimhan, A.~Rohrbach, and T.~Darrell, ``Clip-it! language-guided video
  summarization,'' \emph{Advances in Neural Information Processing Systems},
  vol.~34, pp. 13\,988--14\,000, 2021.

\bibitem{xian2018zero}
Y.~Xian, C.~H. Lampert, B.~Schiele, and Z.~Akata, ``Zero-shot learningâ€”a
  comprehensive evaluation of the good, the bad and the ugly,'' \emph{IEEE
  transactions on pattern analysis and machine intelligence}, vol.~41, no.~9,
  pp. 2251--2265, 2018.

\bibitem{gu2021open}
X.~Gu, T.-Y. Lin, W.~Kuo, and Y.~Cui, ``Open-vocabulary object detection via
  vision and language knowledge distillation,'' \emph{arXiv preprint
  arXiv:2104.13921}, 2021.

\bibitem{tan2019lxmert}
H.~Tan and M.~Bansal, ``Lxmert: Learning cross-modality encoder representations
  from transformers,'' in \emph{Proceedings of the 2019 Conference on Empirical
  Methods in Natural Language Processing and the 9th International Joint
  Conference on Natural Language Processing (EMNLP-IJCNLP)}, 2019, p.
  5103â€“5114.

\bibitem{liu2021align}
L.~H. Liu, H.~Shen, and A.~L. Yuille, ``Align and prompt: Video-and-language
  pre-training with entity prompts,'' \emph{arXiv preprint arXiv:2112.09583},
  2021.

\bibitem{liu2021gilbert}
------, ``Gilbert: Generative vision-language pre-training for image-text
  retrieval,'' in \emph{Proceedings of the Web Conference 2021}, 2021, p.
  1143â€“1154.

\bibitem{kawakami2020learning}
K.~Kawakami, L.~Wang, C.~Dyer, P.~Blunsom, and A.~v.~d. Oord, ``Learning robust
  and multilingual speech representations,'' \emph{arXiv preprint
  arXiv:2001.11128}, 2020.

\bibitem{burchi2023audio}
M.~Burchi and R.~Timofte, ``Audio-visual efficient conformer for robust speech
  recognition,'' in \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, 2023, pp. 2258--2267.

\end{thebibliography}







\end{document}
