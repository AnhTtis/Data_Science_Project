\section{Experiments}\label{sec:exp}
\begin{figure*}[t]
	\centering
	\begin{subfigure}{.5\textwidth}
		\includegraphics[width=1.0\linewidth]{plots/comparison_deblur_v2_compressed.png}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\includegraphics[width=1.0\linewidth]{plots/comparison_inpaint_v2_compressed.png}
	\end{subfigure}
	\caption{Visual comparison of reconstructions: Gaussian blur (left) and inpainting (right).	More samples in Appendix \ref{apx:visual}.}
	\label{fig:comparison}
\end{figure*}
%\begin{figure}
%	\centering
%		\includegraphics[width=0.97\linewidth]{plots/comparison_deblur.png}
%	\caption{Visual comparison of reconstructions on images from FFHQ (top 2 rows) and ImageNet (bottom 2 rows) on the Gaussian deblurring task.}
%	\label{fig:comparison_blur}
%\end{figure}
%\begin{figure}
%	\centering
%		\includegraphics[width=0.97\linewidth]{plots/comparison_inpaiting_v2.png}
%	\caption{Visual comparison of reconstructions on images from FFHQ (top 2 rows) and ImageNet (bottom 2 rows) on the inpainting task with Gaussian masks.}
%	\label{fig:comparison_inpaint}
%\end{figure}
\textbf{Experimental setup --} We evaluate our method on CelebA-HQ ($256 \times 256$) \citep{karras_progressive_2018} and ImageNet ($256 \times 256$) \citep{deng2009imagenet}. For competing methods that require a score model, we use pre-trained SDE-VP models. For \methodname{}, we train models from scratch using the NCSN++\citep{song2020score}  architecture.  As the pre-trained score-models for competing methods have been trained on the full CelebA-HQ dataset, we test all methods for fair comparison on the first $1k$ images of the FFHQ \citep{karras2019style} dataset. For ImageNet experiments, we sample $1$ image from each class from the official validation split to create disjoint validation and test sets of $1k$ images each. We only train our model on the train split of ImageNet.

We investigate two degradation processes of very different properties: Gaussian blur and inpainting. In all cases, Gaussian noise with $\sigma_1 = 0.05$ is added to the measurements in the $[0, 1]$ range. We use standard geometric noise scheduling with $\sigma_{max} = 0.05$ and $\sigma_{min} = 0.01$ in the SDP. For Gaussian blur, we use a kernel size of $61$, with standard deviation of $w_{max} =3$. We vary the standard deviation of the kernel between $w_{max}$(strongest) and $0.3$ (weakest) to parameterize the severity of Gaussian blur in the degradation process, and use the scheduling method described in Appendix \ref{apx:scheduling} to specify $\mathcal{A}_t$. For inpainting, we generate a smooth mask in the form $\left(1 - \frac{f(\vct{x}; w_t)}{\max_{\vct{x}} f(\vct{x}; w_t)}\right)^k$, where $f(\vct{x}; w_t)$ denotes the density of a zero-mean isotropic Gaussian with standard deviation $w_t$ that controls the size of the mask and $k=4$ for sharper transition. We set $w_1 = 50$ for CelebA-HQ/FFHQ inpainting and $30$ for ImageNet inpainting. More details on the experimental setting and operators can be found in Appendix \ref{apx:exp_details}.

We compare our method against DDRM \citep{kawar2022denoising}, a well-established diffusion-based linear inverse problem solver; DPS \cite{chung2022diffusion}, a recent, state-of-the-art diffusion technique for noisy inverse problems; SwinIR \citep{liang_swinir_2021}, a state-of-the-art transformer-based supervised image restoration model; PnP-ADMM \citep{chan2016plug}, a reliable traditional solver with learned denoiser; and ADMM-TV, a classical optimization technique. For more details see Appendix \ref{apx:comparison_methods}. To evaluate performance, we use PSNR and SSIM as distortion metrics and LPIPS and FID as perceptual quality metrics.

\textbf{Deblurring --} We train our model on $	\mathcal{L}_{IR}(\Delta t=0, {\vct{\theta}}) $, as we observed no significant difference in using other incremental reconstruction losses, due to the smoothness of the degradation (see ablation in Appendix \ref{apx:irn_ablations}). We show results on our perception-optimized (PO) reconstructions, tuned for best LPIPS and our distortion-optimized (DO) reconstructions, tuned for best PSNR on a separate validation set via early-stopping (see Fig. \ref{fig:perception_distortion}). Our results, summarized in Table \ref{tab:table_results} (left side), demonstrate superior performance compared with other diffusion methods in terms of both distortion and perceptual metrics. Our DO model closely matches the distortion quality of SwinIR,  a strong non-diffusion baseline known to outperform other diffusion solvers in terms of distortion metrics \citep{chung2022diffusion}. Visual comparison in Figure \ref{fig:comparison} reveals that DDRM produces reliable reconstructions, similar to our DO images, but they often lack detail. In contrast, DPS produces detailed images, similar to our PO reconstructions, but often with hallucinated details inconsistent with the measurement. Finally, we demonstrate the robustness of \methodname{} to test-time perturbations in the forward operator and noise level in Appendix \ref{apx:robustness}.

\textbf{Inpainting --} We train our model on $	\mathcal{L}_{IR}(\Delta t=1, {\vct{\theta}}) $, as we see improvement in reconstruction quality as $\Delta t$ is increased. We hypothesize that this is due to sharp changes in the inpainting operator with respect to $t$, which can be mitigated by the incremental reconstruction loss according to Theorem \ref{thm:basic_error}. Ablations on the effect of $\Delta t$ in the incremental reconstruction loss can be found in Appendix \ref{apx:irn_ablations}. We tuned models to optimize FID, as it is more suitable than pairwise image metrics to evaluate generated image content. Our results in Table \ref{tab:table_results} shows best performance in most metrics, followed by DDRM. Fig. \ref{fig:comparison} (right) shows, that our method generates high quality images even when limited context is available. 

\begin{table*}[t]
	\centering
	\resizebox{15.5cm}{!}{
		\begin{tabular}{ lccccccccc }
			\toprule
			&\multicolumn{4}{c}{\textbf{Deblurring}} & &\multicolumn{4}{c}{\textbf{Inpainting}} \\
			\cmidrule{2-5}\cmidrule{7-10}
			\textbf{Method} &PSNR$(\uparrow)$&SSIM$(\uparrow)$&LPIPS$(\downarrow)$&FID$(\downarrow)$& &PSNR$(\uparrow)$&SSIM$(\uparrow)$&LPIPS$(\downarrow)$&FID$(\downarrow)$ \\
			\midrule
			\methodname-PO (ours) &26.67&0.7418&\textbf{0.2716}&\textbf{53.36}& &25.41&0.7595&0.2611&\textbf{39.43} \\
			\methodname-DO (ours) &\underline{28.47}&\underline{0.8054}&0.2972&69.15& &\textbf{26.98}&\textbf{0.8435}&\textbf{0.2234}&\underline{51.87} \\
			\hline
			DPS \citep{chung2022diffusion} &25.56&0.6878&0.3008&\underline{65.68}& &21.06&0.7238&0.2899&57.92 \\
			DDRM \citep{kawar2022denoising} &27.21&0.7671&\underline{0.2849}&65.84& &\underline{25.62}&0.8132&\underline{0.2313}&54.37 \\
			SwinIR \citep{liang_swinir_2021}  &\textbf{28.53}&\textbf{0.8070}&0.3048&72.93& &24.46&\underline{0.8134}&0.2660&59.94\\
			PnP-ADMM \citep{chan2016plug}&27.02
			&0.7596&0.3973&74.17& &12.27&0.6205&0.4471&192.36 \\
			ADMM-TV &26.03&0.7323&0.4126&89.93& &11.73&0.5618&0.5042&264.62 \\
			\bottomrule
		\end{tabular}
	}
	\resizebox{15.5cm}{!}{
		\begin{tabular}{ lccccccccc }
			\toprule
			&\multicolumn{4}{c}{\textbf{Deblurring}} & &\multicolumn{4}{c}{\textbf{Inpainting}} \\
			\cmidrule{2-5}\cmidrule{7-10}
			\textbf{Method} &PSNR$(\uparrow)$&SSIM$(\uparrow)$&LPIPS$(\downarrow)$&FID$(\downarrow)$& &PSNR$(\uparrow)$&SSIM$(\uparrow)$&LPIPS$(\downarrow)$&FID$(\downarrow)$ \\
			\midrule
			\methodname-PO (ours) &24.68&0.6582&\textbf{0.3302}&\underline{53.91}& &26.36&0.8087&0.2079&\underline{34.33} \\
			\methodname-DO (ours) &\textbf{25.76}&\textbf{0.7085}&0.3705&83.23& &\textbf{28.92}&\textbf{0.8958}&\textbf{0.1676}&38.25 \\
			\hline
			DPS \citep{chung2022diffusion} &21.51&0.5163&0.4235&\textbf{52.60}& &22.71&0.8026&\underline{0.1986}&34.55 \\
			DDRM \citep{kawar2022denoising} &24.53&0.6676&\underline{0.3917}&61.06& &25.92&0.8347&0.2138&\textbf{33.71} \\
			SwinIR \citep{liang_swinir_2021}  &\underline{25.07}&\underline{0.6801}&0.4159&84.80& &\underline{26.87}&\underline{0.8490}&0.2161&45.69\\
			PnP-ADMM \citep{chan2016plug}&25.02&0.6722&0.4565&98.72& &18.14&0.7901&0.2709&101.25 \\
			ADMM-TV &24.31&0.6441&0.4578&88.26& &17.60&0.7229&0.3157&120.22 \\
			\bottomrule
		\end{tabular} 
	}
	\caption{\label{tab:table_results} Experimental results on the FFHQ (top) and ImageNet (bottom) test splits.}
\end{table*}

\textbf{Data consistency --} Consistency between reconstructions and the measurement is crucial in inverse problem solving. Our proposed method has the additional benefit of maintaining data consistency throughout the reverse process, as shown in Theorem \ref{thm:dc} in the ideal case, however we empirically validate this claim. Figure \ref{fig:curves} (left) shows the evolution of $\epsilon_{dc} := \|\vct{\tilde{y}} - \fwd{1}{\hat{\vct{x}}_0(\vct{y}_t)}\|^2$, where $\hat{\vct{x}}_0(\vct{y}_t)$ is the clean image estimate at time $t$ ($\Phi_{\vct{\theta}}(\vct{y}_t, t)$ for our method). Since $\vct{\tilde{y}} = \fwd{1}{\vct{x}_0} + \sigma_1^2$, we expect $\epsilon_{dc}$ to approach $\sigma_1^2$ in case of perfect data consistency. We observe that our method, without applying guidance,  stays close to the noise floor throughout the reverse process, while other techniques approach data consistency only close to $t=1$.  In case of DPS, we observe that data consistency is not always satisfied (see Figure \ref{fig:curves}, right), as DPS only guides the iterates towards data consistency, but does not directly enforce it. As our technique reverses an SDP, our intermediate reconstructions are always interpretable as degradations of varying severity of the same underlying image. This property allows us to early-stop the reconstruction and still obtain consistent reconstructions. 
 
\textbf{Sampling speed --} \methodname{} requires low number of reverse diffusion steps for high quality reconstructions leading to fast sampling. Figure \ref{fig:nfe_compare} compares the perceptual quality at different number of reverse diffusion steps for diffusion-based solvers. Our method typically requires $20-100$ steps for optimal perceptual quality, and shows the most favorable scaling in the low-NFE (Neural Function Evaluations) regime. Due to early-stopping we can trade-off perceptual quality for better distortion metrics and even further sampling speed-up. We obtain acceptable results even with as low as a single step of reconstruction. 

%\textbf{Comparison with blending --} Our proposed method interpolates between degraded and clean distributions via a SDP. A parallel line of work \cite{delbracio2023inversion, heitz2023iterative} considers an alternative formulation in which the intermediate distributions are convex combinations of degraded-clean image pairs, that is $ \vct{y_t} = t \vct{\tilde{y}} + (1-t) \vct{x_0}$. We compare the InDI \cite{delbracio2023inversion} formulation to \methodname{} on the FFHQ dataset (Table \ref{tab:indi_compare}). We observe comparable results on the deblurring task, however the blending parametrization is not suitable for inpainting as reflected by the large gap in FID. To see this, we point out that in \methodname{} $t$ directly parametrizes the severity of the degradation, that is our model learns a continuum of reconstruction problems with smoothly changing difficulty. On the other hand, blending missing pixels with the clean image does not offer a smooth transition in terms of reconstruction difficulty: for any $0 \leq t < 1$ the reconstruction of $\vct{x_0}$  from $ \vct{y_t}$ becomes trivial. Furthermore, as our model is trained on a wide range of noise levels due to the SDP formulation, we observe improved robustness to test-time perturbations in measurement noise compared to the blending formulation (Fig. \ref{fig:robustness}).
%
%%\begin{table}
%%	\centering
%%	\resizebox{6.5cm}{!}{
%%		\begin{tabular}{ lccccc }
%%			\toprule
%%			&\multicolumn{2}{c}{\textbf{Deblurring}} & &\multicolumn{2}{c}{\textbf{Inpainting}} \\
%%			\cmidrule{2-3}\cmidrule{5-6}
%%			\textbf{Method} &LPIPS$(\downarrow)$&FID$(\downarrow)$& &LPIPS$(\downarrow)$&FID$(\downarrow)$ \\
%%			\midrule
%%			Blending& \textbf{0.2604}&56.27& &\textbf{0.2424}& 54.08 \\
%%			Dirac-PO (ours) &0.2716&\textbf{53.36}& &0.2626&\textbf{39.43} \\
%%			\bottomrule \vspace{0.025cm}
%%		\end{tabular}
%%	}
%%	\caption{\label{tab:indi_compare} Comparison with blending schedule on the FFHQ test split. }
%%\end{table}
%
%\begin{figure}
%		\begin{minipage}[b]{.49\linewidth}
%		\centering
%		\resizebox{7.5cm}{!}{
%			\begin{tabular}{ lccccc }
%				\toprule
%				&\multicolumn{2}{c}{\textbf{Deblurring}} & &\multicolumn{2}{c}{\textbf{Inpainting}} \\
%				\cmidrule{2-3}\cmidrule{5-6}
%				\textbf{Method} &LPIPS$(\downarrow)$&FID$(\downarrow)$& &LPIPS$(\downarrow)$&FID$(\downarrow)$ \\
%				\midrule
%				Blending& \textbf{0.2604}&56.27& &\textbf{0.2424}& 54.08 \\
%				Dirac-PO (ours) &0.2716&\textbf{53.36}& &0.2626&\textbf{39.43} \\
%				\bottomrule \vspace{0.025cm}
%			\end{tabular}
%		}
%		\captionof{table}{\label{tab:indi_compare} Comparison with blending schedule on the FFHQ test split. }
%	\end{minipage}\hfill
%	\begin{minipage}[t]{.49\linewidth}
%		\centering
%		\includegraphics[width=0.7\linewidth]{plots/robustness_compare.pdf}%
%		\caption
%		{%
%			Robustness experiment: we simulate a mismatch between train and test noise levels (FFHQ test split, deblurring). Dirac is more robust to perturbations in measurement noise variance.
%			\label{fig:robustness}%
%		}%
%	\end{minipage}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.5\linewidth]{plots/increcon_nfe_lpips_curves.pdf}
%	\caption{ Number of reverse diffusion steps vs. perceptual reconstruction quality.}
%	\label{fig:nfe}
%\end{figure}