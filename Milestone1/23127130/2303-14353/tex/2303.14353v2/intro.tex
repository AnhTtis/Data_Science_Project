\section{Introduction}
Diffusion models (DMs) are powerful generative models capable of synthesizing samples of exceptional quality by reversing a diffusion process that gradually corrupts a clean image by adding Gaussian noise. DMs have been explored from two perspectives: Denoising Diffusion Probabilistic Models (DDPM) \citep{sohl2015deep, ho_denoising_2020} and Score-Based Models \citep{song_generative_2020, song_improved_2020}, which have been unified under a general framework of Stochastic Differential Equations (SDEs) \citep{song2020score}.  DMs have established new state of the art in image generation \citep{dhariwal_diffusion_2021, saharia2022photorealistic, ramesh2022hierarchical, rombach2022high}, audio \citep{kong2020diffwave} and video synthesis \citep{ho2022video}. Recently, there has been a push to broaden the notion of Gaussian diffusion, such as extension to other noise distributions \citep{deasy2021heavy, nachmani2021denoising, okhotin2023star}. In the context of image generation, there has been work to generalize the corruption process, such as blur diffusion \citep{lee2022progressive, hoogeboom2022blurring}, inverse heat dissipation \citep{rissanen2022generative} and arbitrary linear corruptions \citep{daras2022soft} with \citet{bansal2022cold} questioning the necessity of stochasticity in the generative process all together. However, these are general frameworks for unconditional image generation and are not readily applicable for image reconstruction. The key challenge introduced by the inverse problem setting is the strong requirement for producing final images that are consistent with the observation. This adds a significant layer of complexity that requires novel solutions both in theory and algorithm design.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.75\linewidth]{plots/front_page_compressed.pdf}
	\captionof{figure}{ \textbf{Overview of our method:} measurement acquisition is modeled as a gradual degradation and noising of an underlying clean ground truth signal via a Stochastic Degradation Process. We reconstruct the clean image from noisy measurements by learning to reverse the degradation process.  Our technique allows for obtaining a variety of reconstructions with different perceptual quality-distortion trade-offs, all in a single sampling trajectory. }
	\label{fig:front_page}
\end{figure*}

In inverse problems, one wishes to recover a signal $\vct{x}$ from a noisy observation $\vct{y} = \fwd{}{\vct{x}} + \vct{z}$ where $\mathcal{A}$ is typically non-invertible. 
%Prior knowledge on $x$ has been incorporated in a variety of ways including sparsity-inducing regularizers \cite{candes_stable_2006}, plug-and-play priors \cite{venkatakrishnan2013plug} and learned deep learning priors \cite{ongie_deep_2020}. DMs learn a prior over the data distribution by matching the gradient of the log density. 
The unconditional score function learned by DMs has been successfully leveraged to solve inverse problems without any task-specific training \citep{kadkhodaie2021stochastic, jalal2021robust, saharia_image_2021} resulting in reconstructions with exceptional perceptual quality. However, these methods underperform in distortion metrics, such as PSNR and SSIM \citep{chung2022diffusion} due to the so called perception-distortion trade-off \citep{blau2018perception}.  Authors in \citet{delbracio2023inversion} observe that in their framework, the total number of restoration steps controls the perception-distortion trade-off, with less steps yielding results closer to the minimum distortion estimate. Similar observation is made in \citet{whang2022deblurring} in the context of blind image deblurring, where authors additionally propose to average multiple reconstructions for improved distortion metrics.  Authors in \citet{kawar2022denoising} report that, the amount of noise injected at each timestep controls the trade-off between reconstruction error and image quality.%Nonetheless, if data consistency is not explicitly enforced during reconstruction, we may obtain visually appealing samples, that are in fact not faithful to our original observations. 

Beyond image quality, a key requirement imposed on reconstructions is data consistency, that is faithfulness to the original observation. In the context of diffusion-based solvers, different methods have been proposed to enforce consistency between the generated image and the corresponding observations. These methods include alternating between a step of unconditional update and a step of projection \citep{song2021solving, chung2022score, chung2022come} or other correction techniques \citep{chung2022diffusion, chung2022improving, song2023solving} to guide the diffusion process towards data consistency. Another line of work proposes diffusion in the spectral space of the forward operator, achieving high quality reconstructions, however requires costly singular value decomposition \citep{kawar2021snips, kawar2022denoising, kawar2022jpeg}. \citet{songpseudoinverse} uses pseudo-inverse guidance to incorporate the model into the reconstruction process. All of these methods utilize a pre-trained score function learned for a standard diffusion process that simply adds Gaussian noise to clean images. %, and therefore the specific corruption model of the inverse problem is not incorporated into model training directly. 
Recently, there has been some work on extending Gaussian diffusion by incorporating the image degradation into the score-model training procedure. A recent example is \citet{welker2022driftrec} proposing adding an additional drift term to the forward SDE that pulls the iterates towards the corrupted measurement and demonstrates high quality reconstructions for JPEG compression artifact removal. A blending parametrization \citep{heitz2023iterative,delbracio2023inversion} has been proposed that defines the forward process as convex combinations between the clean image and corrupted observation. \citet{liu20232} leverages Schr√∂dinger bridges for image restoration, a nonlinear extension of score-based models defined between degraded and clean distributions. \citet{yue2024resshift} defines a Markov chain between the distributions of high and low-resolution images in the forward process by shifting their residual for image super-resolution. Even though these methods utilize degraded-clean image pairs for training, they do not explicitly leverage the forward operator for score-model training.% and thus data consistency, a crucial requirement in inverse problems, cannot be guaranteed.
%Despite all of the successes of diffusion models in high-quality image generation, the requirements imposed on inverse problems are markedly different from image synthesis. 

In this paper, we propose a novel framework for solving inverse problems using a generalized notion of diffusion that mimics the corruption process that produced the observation. We call our method \methodname{}: \underline{D}enoising and \underline{I}ncremental \underline{R}econstruction with \underline{A}ssured data-\underline{C}onsistency.  As the forward model and noising process are directly incorporated into the framework, our method maintains data consistency throughout the reverse diffusion process, without any additional steps such as projections. Furthermore, we make the key observation that details are gradually added to the posterior mean estimates during the sampling process. This property imbues \methodname{} with great flexibility: by leveraging early-stopping we can freely trade off perceptual quality for better distortion metrics and sampling speedup or vice versa. We provide theoretical analysis on the accuracy and limitations of our method that are well-supported by empirical results. Our experiments demonstrate state-of-the-art results in terms of both perceptual and distortion metrics with fast sampling. 