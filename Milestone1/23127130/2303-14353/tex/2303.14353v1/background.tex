\section{Background}
\textbf{Diffusion models --} Diffusion models are generative models based on a corruption process that gradually transforms a clean image distribution $q_0$ into  a known prior distribution which is tractable, but contains no information of data. The corruption level, or \textit{severity} as we refer to it in this paper, is indexed by time $t$ and increases from $t=0$ (clean images) to $t=1$ (pure noise). The typical corruption process consists of adding Gaussian noise of increasing magnitude to clean images, that is $q_t(\vct{x}_t| \vct{x}_0) \sim \mathcal{N}(\vct{x}_0, \sigma_t^2 \mathbf{I})$, where $\vct{x}_0\sim q_0$ is a clean image, and $\vct{x}_t$ is the corrupted image at time $t$. By learning to reverse the corruption process, one can generate samples from $q_0$ by sampling from a simple noise distribution and running the learned reverse diffusion process from $t=1$ to $t=0$. 

Diffusion models have been explored along two seemingly different trajectories. Score-Based Models \cite{song_generative_2020, song_improved_2020} attempt to learn the gradient of the log likelihood and use Langevin dynamics for sampling, whereas DDPM \cite{sohl2015deep, ho_denoising_2020} adopts a variational inference interpretation. More recently, a unified framework based on SDEs \cite{song2020score} has been proposed. Namely, both Score-Based Models and DDPM can be expressed via a Forward SDE in the form $\diff\vct{x} = f(\vct{x} , t) \diff t + g(t) \diff \vct{w}$
%\begin{equation} \label{eq:forward_sde}
%	\diff\vct{x} = f(\vct{x} , t) \diff t + g(t) \diff \vct{w} ,	
%\end{equation}
with different choices of $f$ and $g$. Here $\vct{w} $ denotes the standard Wiener process. This SDE is reversible \cite{anderson1982reverse}, and the Reverse SDE can be written as
\begin{equation}\label{eq:reverse_sde}
	\diff \vct{x}  = [f(\vct{x} , t) - g^2(t) \nabla_{\vct{x} } \log q_t(\vct{x} )] \diff t + g(t) \diff \bar{\vct{w} },
\end{equation}
where $ \bar{\vct{w}}$ is the standard Wiener process, where time flows in the reverse direction. The true score $ \nabla_{\vct{x} } \log q_t(\vct{x} )$ is approximated by a neural network $s_{\vct{\theta}}(\vct{x}_t, t)$ from the tractable conditional distribution $q_t(\vct{x}_t| \vct{x}_0)$ by minimizing 
\begin{equation}\label{eq:score_matching}
\mathbb{E}_ {t\sim U[0, 1], (\vct{x}_0, \vct{x}_t)}\left[w(t) \left\|  s_{\vct{\theta}}(\vct{x}_t, t)  - \nabla_{\vct{x}_t} q_t(\vct{x}_t| \vct{x}_0) \right\|^2\right],
\end{equation}
where $(\vct{x}_0, \vct{x}_t) \sim q_0(\vct{x}_0) q_t(\vct{x}_t| \vct{x}_0)$ and $w(t)$ is a weighting function. By applying different discretization schemes to \eqref{eq:reverse_sde}, one can derive various algorithms to simulate the reverse diffusion process for sample generation.

\textbf{Diffusion Models for Inverse problems --}  Our goal is to solve a noisy, possibly nonlinear inverse problem 
\begin{equation} \label{eq:inv_prob}
	\vct{\tilde{y}} = \mathcal{A}(\vct{x}_0) + \vct{z}, ~ \vct{z} \sim \gaussian{\sigma^2},
\end{equation}
with $	\vct{\tilde{y}}, \vct{x}_0~\in \mathbb{R}^n$ and $\mathcal{A}: \mathbb{R}^n \rightarrow  \mathbb{R}^n$. 
%We will also denote the set of possible ground truth signals (all clean images) as $\mathcal{X}_0$, and sometimes we assume that the ground truth images come from an unknown prior distribution $q_0(\vct{x}_0)$.
That is, we are interested in solving a reconstruction problem, where we observe a measurement $	\vct{\tilde{y}}$ that is known to be produced by applying a non-invertible mapping $\mathcal{A}$ to a ground truth signal $\vct{x}_0$ and is corrupted by additive noise $\vct{z}$. We refer to $\mathcal{A}$ as the degradation, and $\mathcal{A}(\vct{x}_0)$ as a degraded signal. Our goal is to recover $\vct{x}_0$ as faithfully as possible, which can be thought of as generating samples from the posterior distribution $q(\vct{x}_0| 	\vct{\tilde{y}})$. However, as information is fundamentally lost in the measurement process in \eqref{eq:inv_prob}, prior information on clean signals needs to be incorporated to make recovery possible. In the classical field of compressed sensing \cite{candes_stable_2006}, a sparsity-inducing regularizer is directly added to the reconstruction objective. An alternative is to leverage diffusion models as the prior to obtain a reverse diffusion sampler for sampling from the posterior based on \eqref{eq:reverse_sde}. Using Bayes rule, the score of the posterior can be written as 
\begin{equation}
	\nabla_{\vct{x}} \log q_t(\vct{x}|	\vct{\tilde{y}}) = \nabla_{\vct{x}} \log q_t(\vct{x}) + \nabla_{\vct{x}} \log q_t(	\vct{\tilde{y}} | \vct{x}),
\end{equation}
where the first term can be approximated using score-matching as in \eqref{eq:score_matching}. On the other hand, the second term cannot be expressed in closed-form in general, and therefore a flurry of activity emerged recently to circumvent computing the likelihood directly.  The most prominent approach is to alternate between unconditional update from \eqref{eq:reverse_sde} and some form of projection to enforce consistency with the measurement \cite{song2021solving, chung2022score, chung2022come}. In recent work, it has been shown that the projection step may throw the sampling path off the data manifold and therefore additional correction steps are proposed to keep the solver close to the manifold \cite{chung2022improving, chung2022diffusion}.