\section{Experiments}\label{sec:exp}
\begin{table}
	\centering
	\resizebox{15.5cm}{!}{
		\begin{tabular}{ lccccccccc }
			\toprule
			&\multicolumn{4}{c}{\textbf{Deblurring}} & &\multicolumn{4}{c}{\textbf{Inpainting}} \\
			\cmidrule{2-5}\cmidrule{7-10}
			\textbf{Method} &PSNR$(\uparrow)$&SSIM$(\uparrow)$&LPIPS$(\downarrow)$&FID$(\downarrow)$& &PSNR$(\uparrow)$&SSIM$(\uparrow)$&LPIPS$(\downarrow)$&FID$(\downarrow)$ \\
			\midrule
			\methodname-PO (ours) &26.67&0.7418&\textbf{0.2716}&\textbf{53.36}& &25.41&0.7595&0.2611&\textbf{39.43} \\
			\methodname-DO (ours) &\textbf{28.47}&\textbf{0.8054}&0.2972&69.15& &\textbf{26.98}&\textbf{0.8435}&\textbf{0.2234}&\underline{51.87} \\
			\hline
			DPS \cite{chung2022diffusion} &25.56&0.6878&0.3008&\underline{65.68}& &21.06&0.7238&0.2899&57.92 \\
			DDRM \cite{kawar2022denoising} &\underline{27.21}&\underline{0.7671}&\underline{0.2849}&65.84& &\underline{25.62}&\underline{0.8132}&\underline{0.2313}&54.37 \\
			PnP-ADMM \cite{chan2016plug}&27.02
			&0.7596&0.3973&74.17& &12.27&0.6205&0.4471&192.36 \\
			ADMM-TV &26.03&0.7323&0.4126&89.93& &11.73&0.5618&0.5042&264.62 \\
			\bottomrule \vspace{0.025cm}
		\end{tabular}
	}
	\resizebox{15.5cm}{!}{
		\begin{tabular}{ lccccccccc }
			\toprule
			&\multicolumn{4}{c}{\textbf{Deblurring}} & &\multicolumn{4}{c}{\textbf{Inpainting}} \\
			\cmidrule{2-5}\cmidrule{7-10}
			\textbf{Method} &PSNR$(\uparrow)$&SSIM$(\uparrow)$&LPIPS$(\downarrow)$&FID$(\downarrow)$& &PSNR$(\uparrow)$&SSIM$(\uparrow)$&LPIPS$(\downarrow)$&FID$(\downarrow)$ \\
			\midrule
			\methodname-PO (ours) &24.68&0.6582&\textbf{0.3302}&\underline{53.91}& &25.48&0.8077&0.2185&40.46 \\
			\methodname-DO (ours) &\textbf{25.76}&\textbf{0.7085}&0.3705&83.23& &\textbf{28.42}&\textbf{0.8906}&\textbf{0.1760}&40.73 \\
			\hline
			DPS \cite{chung2022diffusion} &21.51&0.5163&0.4235&\textbf{52.60}& &22.71&0.8026&\underline{0.1986}&\underline{34.55} \\
			DDRM \cite{kawar2022denoising} &24.53&0.6676&\underline{0.3917}&61.06& &\underline{25.92}&\underline{0.8347}&0.2138&\textbf{33.71} \\
			PnP-ADMM \cite{chan2016plug}&\underline{25.02}&\underline{0.6722}&0.4565&98.72& &18.14&0.7901&0.2709&101.25 \\
			ADMM-TV &24.31&0.6441&0.4578&88.26& &17.60&0.7229&0.3157&120.22 \\
			\bottomrule
			\vspace{0.1cm}
		\end{tabular} 
	}
	\caption{\label{tab:table_results} Experimental results on the FFHQ (top) and ImageNet (bottom) test splits. }
\end{table}
\begin{figure}
	\centering
		\includegraphics[width=0.97\linewidth]{plots/comparison_deblur.png}
	\caption{Visual comparison of reconstructions on images from FFHQ (top 2 rows) and ImageNet (bottom 2 rows) on the Gaussian deblurring task.}
	\label{fig:comparison_blur}
\end{figure}
\begin{figure}
	\centering
		\includegraphics[width=0.97\linewidth]{plots/comparison_inpaiting_v2.png}
	\caption{Visual comparison of reconstructions on images from FFHQ (top 2 rows) and ImageNet (bottom 2 rows) on the inpainting task with Gaussian masks.}
	\label{fig:comparison_inpaint}
\end{figure}
\textbf{Experimental setup --} We evaluate our method on CelebA-HQ ($256 \times 256$) \cite{karras_progressive_2018} and ImageNet ($256 \times 256$) \cite{deng2009imagenet}. For competing methods that require a score model, we use pre-trained SDE-VP models. For \methodname{}, we train models from scratch using the NCSN++\cite{song2020score}  architecture.  As the pre-trained score-models for competing methods have been trained on the full CelebA-HQ dataset, we test all methods for fair comparison on the first $1k$ images of the FFHQ \cite{karras2019style} dataset. For ImageNet experiments, we sample $1$ image from each class from the official validation split to create disjoint validation and test sets of $1k$ images each. We only train our model on the train split of ImageNet.

We investigate two degradation processes of very different properties: Gaussian blur and inpainting, both with additive Gaussian noise. In all cases, noise with $\sigma_1 = 0.05$ is added to the measurements in the $[0, 1]$ range. We use standard geometric noise scheduling with $\sigma_{max} = 0.05$ and $\sigma_{min} = 0.01$ in the SDP. For Gaussian blur, we use a kernel size of $61$, with standard deviation of $w_{max} =3$. We change the standard deviation of the kernel between $w_{max}$(strongest) and $0.3$ (weakest) to parameterize the severity of Gaussian blur in the degradation process, and use the scheduling method described in the supplementary to specify $\mathcal{A}_t$. For inpainting, we generate a smooth mask in the form $\left(1 - \frac{f(\vct{x}; w_t)}{\max_{\vct{x}} f(\vct{x}; w_t)}\right)^k$, where $f(\vct{x}; w_t)$ denotes the density of a zero-mean isotropic Gaussian with standard deviation $w_t$ that controls the size of the mask and $k=4$ for sharper transition. We set $w_1 = 50$ for CelebA-HQ/FFHQ inpainting and $30$ for ImageNet inpainting.

We compare our method against DDRM \cite{kawar2022denoising}, a well-established diffusion-based linear inverse problem solver; DPS \cite{chung2022diffusion}, a recent, state-of-the-art diffusion technique for noisy inverse problems; PnP-ADMM \cite{chan2016plug}, a reliable traditional solver with learned denoiser; and ADMM-TV, a classical optimization technique.  To evaluate performance, we use PSNR and SSIM as distortion metrics and LPIPS and FID as perceptual quality metrics.

\textbf{Deblurring --} We train our model on $	\mathcal{L}_{IRN}(\Delta t=0, {\vct{\theta}}) $, as we observed no significant difference in using other incremental reconstruction losses, due to the smoothness of the degradation. We show results on our perception-optimized (PO) reconstructions, tuned for best LPIPS and our distortion-optimized (DO) reconstructions, tuned for best PSNR on a separate validation set via early-stopping at the PSNR-peak (see Fig. \ref{fig:perception_distortion}). Our results, summarized in Table \ref{tab:table_results} (left side), demonstrate superior performance compared with other benchmark methods in terms of both distortion and perceptual metrics. Visual comparison in Figure \ref{fig:comparison_blur} reveals that DDRM produces reliable reconstructions, similar to our DO images, but these reconstructions tend to lack detail. On the other hand, DPS produces detailed images, similar to our PO reconstructions, but often with hallucinated details inconsistent with the measurement. 

\textbf{Inpainting --} We train our model on $	\mathcal{L}_{IRN}(\Delta t=1, {\vct{\theta}}) $, as we see improvement in reconstruction quality as $\Delta t$ is increased. We hypothesize that this is due to sharp changes in the inpainting operator with respect to $t$, which can be mitigated by the incremental reconstruction loss according to Theorem \ref{thm:basic_error}. We tuned models to optimize FID, as it is more suitable than pairwise image metrics to evaluate generated image content. Our results in Table \ref{tab:table_results} (right side) shows best performance in most metrics, followed by DDRM. Fig. \ref{fig:comparison_inpaint} shows, that our method generates high quality images even when limited context is available. Ablations on the effect of $\Delta t$ in the incremental reconstruction loss can be found in the supplementary.

\textbf{Data consistency --} Consistency between reconstructions and the original measurement is a crucial requirement in inverse problem solving. Our proposed method has the additional benefit of maintaining data consistency throughout the reverse process, as shown in Theorem \ref{thm:dc} in the ideal case, however we empirically validate this claim. Figure \ref{fig:curves} (left) shows the evolution of $\epsilon_{dc} := \|\vct{\tilde{y}} - \fwd{1}{\hat{\vct{x}}_0(\vct{y}_t)}\|^2$, where $\hat{\vct{x}}_0(\vct{y}_t)$ is the clean image estimate at time $t$ ($\Phi_{\vct{\theta}}(\vct{y}_t, t)$ for our method). Since $\vct{\tilde{y}} = \fwd{1}{\vct{x}_0} + \sigma_1^2$, we expect $\epsilon_{dc}$ to approach $\sigma_1^2$ in case of perfect data consistency. We observe that our method, without applying guidance,  stays close to the noise floor throughout the reverse process, while other techniques approach data consistency only close to $t=1$.  In case of DPS, we observe that data consistency is not always satisfied (see Figure \ref{fig:curves}, center), as DPS only guides the iterates towards data consistency, but does not directly enforce it. As our technique reverses an SDP, our intermediate reconstructions are always interpretable as degradations of varying severity of the same underlying image. This property allows us to early-stop the reconstruction and still obtain consistent reconstructions. 
 
\textbf{Sampling speed --} \methodname{} requires low number of reverse diffusion steps for high quality reconstructions leading to fast sampling. Figure \ref{fig:curves} (right) compares the perceptual reconstruction quality at different number of reverse diffusion steps for diffusion-based inverse problem solvers. Our method typically requires $20-100$ steps for optimal perceptual quality, and shows the most favorable scaling in the low-NFE regime. Due to early-stopping we can trade-off perceptual quality for better distortion metrics and even further sampling speed-up. We obtain acceptable results even with one-shot reconstruction. 
%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.5\linewidth]{plots/increcon_nfe_lpips_curves.pdf}
%	\caption{ Number of reverse diffusion steps vs. perceptual reconstruction quality.}
%	\label{fig:nfe}
%\end{figure}