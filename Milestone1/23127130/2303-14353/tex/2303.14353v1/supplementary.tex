\section*{Supplementary material}
\section{Proofs}
\subsection{Denoising score-matching guarantee}
Just as in standard diffusion, we approximate the score of the noisy, degraded data distribution $\nabla_{\yt} q_t(\yt)$  by matching the score of the tractable conditional distribution $\nabla_{\yt} q_t(\yt|\xo)$ via minimizing the loss in \eqref{eq:loss_final}. For standard Score-Based Models with $\mathcal{A}_t = \mathbf{I}$, the seminal work of \cite{vincent2011connection} guarantees that the true score is learned by denoising score-matching. More recently, \cite{daras2022soft} points out that this result holds for a wide range of corruption processes, with the technical condition that the SDP assigns non-zero probability to all $\yt$ for any given clean image $\xo$. This condition is satisfied by adding Gaussian noise. For the sake of completeness, we include the theorem from \cite{daras2022soft} updated with the notation from this paper. 
\begin{theorem}\label{thm:sm}
	Let $q_0$ and $q_t$ be two distributions in $\mathbb{R}^n$. Assume that all conditional distributions, $q_t(\yt | \xo)$, are supported and differentiable in $\mathbb{R}^n$. Let:
	\begin{equation}\label{eq:sm2}
		J_1(\theta) = \frac{1}{2}\mathbb{E}_{\yt \sim q_t}\left[\norm{\vct{s}_\theta(\yt, t) - \nabla_{\yt} \log q_t(\yt)}^2\right],
	\end{equation} 
	\begin{equation}\label{eq:sm1}
	J_2(\theta) = \frac{1}{2}\mathbb{E}_{(\xo, \yt) \sim q_0(\xo) q_t(\yt | \xo)}\left[\norm{\vct{s}_\theta(\yt, t) - \nabla_{\yt} \log q_t(\yt| \xo)}^2\right].
\end{equation} 
Then, there is a universal constant $C$ (that does not depend on $\theta$) such that: $J_1(\theta) = J_2(\theta) + C$.
\end{theorem}
The proof, that follows the calculations of \cite{vincent2011connection},  can be found in Appendix A.1. of \cite{daras2022soft}. This result implies that by minimizing the denoising score-matching objective in \eqref{eq:sm1}, the objective in \eqref{eq:sm2} is also minimized, thus the true score is learned via matching the tractable conditional distribution $q_t(\yt| \xo)$ governing SDPs.

\subsection{Theorem 3.4.}
\begin{assumption}[Lipschitzness of degradation]\label{as:lipschitz} Assume that $\|\fwd{t}{\vct{x}} - \fwd{t}{\vct{y}}\| \leq L_x^{(t)} \|\vct{x} - \vct{y}\|, ~ \forall \vct{x}, \vct{y} \in \mathbb{R}^n,~ \forall t \in [0, 1]$ and $\|\fwd{t'}{\vct{x}} - \fwd{t''}{\vct{x}}\| \leq L_t |t'- t''|, ~ \forall \vct{x} \in \mathbb{R}^n,~ \forall t', t'' \in [0, 1]$.
\end{assumption}
\begin{assumption}[Bounded signals]\label{as:energy} Assume that each entry of clean signals $\vct{x}_0$ are bounded as $\vct{x}_0[i] \leq B, ~\forall i \in (1, 2, ..., n)$.
\end{assumption}
\begin{lemma}\label{lem:posterior_mean_error}
	Assume $\vct{y}_t = \fwd{t}{\vct{x}_0} + \vct{z}_t$ with $\vct{x}_0\sim q_0(\vct{x}_0)$ and $\vct{z}_t \sim \mathcal{N}(0, \sigma_t^2 \mathbf{I})$ and that Assumption \ref{as:lipschitz} holds. Then, the Jensen gap is upper bounded as $\norm{\mathbb{E} [\fwd{t'}{\vct{x}_0} | \vct{y}_t] - \fwd{t'}{\mathbb{E} [\vct{x}_0| \vct{y}_t]}} \leq L_x^{(t')} \sqrt{n} B, ~\forall t, t' \in [0, 1]$. 
\end{lemma}
\begin{proof}
\begin{align*}
	\norm{\mathbb{E} [\fwd{t'}{\vct{x}_0} | \vct{y}_t] - \fwd{t'}{\mathbb{E} [\vct{x}_0| \vct{y}_t]}} &\myleq{1} 
	\int \norm{\fwd{t'}{\vct{x}_0} - \fwd{t'}{\mathbb{E} [\vct{x}_0| \vct{y}_t]}} p(\vct{x}_0|\vct{y}_t) d\vct{x}_0  \\
	&\myleq{2}\sqrt{\int \norm{\fwd{t'}{\vct{x}_0} - \fwd{t'}{\mathbb{E} [\vct{x}_0| \vct{y}_t]}}^2 p(\vct{x}_0|\vct{y}_t) d\vct{x}_0}  \\
	&\leq L_x^{(t')}\sqrt{\int \norm{\vct{x}_0- \mathbb{E} [\vct{x}_0| \vct{y}_t]}^2 p(\vct{x}_0|\vct{y}_t) d\vct{x}_0} \myleq{3}   L_x^{(t')}\sqrt{\int \norm{\vct{x}_0}^2 p(\vct{x}_0|\vct{y}_t) d\vct{x}_0} \\
	&\leq   L_x^{(t')}\sqrt{\int n B^2 p(\vct{x}_0|\vct{y}_t) d\vct{x}_0}  = L_x^{(t')} \sqrt{n} B
\end{align*}	
Here $(1)$ and $(2)$ hold due to Jensen's inequality, and in $(3)$ we use the fact that $ \mathbb{E} [\vct{x}_0| \vct{y}_t]$ is the minimum mean-squared error (MMSE) estimator of $\vct{x}_0$, thus we can replace it with $0$ to get an upper bound.
\end{proof}
\begin{thmn}{\textbf{3.4}}
	Let  $\hat{\mathcal{R}}(t, \Delta t; \vct{y}_t)  = \fwd{t-\Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} -  \fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}$ denote our estimate of the incremental reconstruction, where $\Phi_{\vct{\theta}}(\vct{y}_t, t)$ is trained on the loss in $(13)$. Let  $\mathcal{R}^*(t, \Delta t; \vct{y}_t) = \mathbb{E}[	\mathcal{R}(t, \Delta t; \vct{x}_0) | \vct{y}_t]$ denote the MMSE estimator of $\mathcal{R}(t, \Delta t; \vct{x}_0)$. If Assumptions \ref{as:energy} and \ref{as:lipschitz} hold and the error in our score network is bounded by $\| s_{{\vct{\theta}}}(\vct{y}_t, t) -  \nabla_{\vct{y}_t}\log q_t(\vct{y}_t) \| \leq \frac{\epsilon_t}{\sigma_t^2}, ~\forall t \in [0,1]$, then 
	\begin{equation*}
		\|\hat{\mathcal{R}}(t, \Delta t; \vct{y}_t) - \mathcal{R}^*(t, \Delta t; \vct{y}_t)\| \leq \\ (L_x^{(t)} + L_x^{(t - \Delta t)}) \sqrt{n} B  + 2 L_t \Delta t +  2\epsilon_t.
	\end{equation*}
\end{thmn} 
\begin{proof}
	First, we note that due to Tweedie's formula, 
	\begin{equation*}
		\mathbb{E}[\fwd{t}{\vct{x}_0}| \vct{y}_t] = \vct{y}_t + \sigma_t^2 \nabla_{\vct{y}_t} \log q_t(\vct{y}_t).
	\end{equation*}
Since we parameterized our score model as 
\begin{equation*}
	s_{\vct{\theta}}(\vct{y}_t, t) =  \frac{ \fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} - \vct{y}_t}{\sigma_t^2},
	\end{equation*}
the assumption that $\| s_{{\vct{\theta}}}(\vct{y}_t, t) -  \nabla_{\vct{y}_t}\log q_t(\vct{y}_t) \| \leq \frac{\epsilon_t}{\sigma_t^2}$, is equivalent to 
\begin{equation}\label{eq:score_error}
	\norm{\fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}  - 	\mathbb{E}[\fwd{t}{\vct{x}_0}| \vct{y}_t] } \leq \epsilon_t.
		\end{equation}
	By applying the triangle inequality repeatedly, and applying Lemma \ref{lem:posterior_mean_error} and \eqref{eq:score_error}
	\begin{align*}
			\norm{\hat{\mathcal{R}}(t, \Delta t; \vct{y}_t) - \mathcal{R}^*(t, \Delta t; \vct{y}_t)} &= \norm{\left(\fwd{t-\Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} -  \fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}\right) - \left(\mathbb{E}[\fwd{t-\Delta t}{\vct{x}_0}|\vct{y}_t] -  \mathbb{E}[\fwd{t}{\vct{x}_0} | \vct{y}_t]\right)} \\
			&\leq \norm{\fwd{t-\Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} -  \mathbb{E}[\fwd{t-\Delta t}{\vct{x}_0}|\vct{y}_t]} + \norm{ \fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}-  \mathbb{E}[\fwd{t}{\vct{x}_0} | \vct{y}_t]} \\
			&\leq  \norm{\fwd{t-\Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} -\fwd{t-\Delta t}{\mathbb{E} [\vct{x}_0| \vct{y}_t]} + \fwd{t-\Delta t}{\mathbb{E} [\vct{x}_0| \vct{y}_t]}-  \mathbb{E}[\fwd{t-\Delta t}{\vct{x}_0}|\vct{y}_t]} + \epsilon_t \\ 
			&\leq   \norm{\fwd{t-\Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} -\fwd{t-\Delta t}{\mathbb{E} [\vct{x}_0| \vct{y}_t]}} + L_x^{(t-\Delta t)} \sqrt{n} B + \epsilon_t \\
			& \leq  \norm{\fwd{t-\Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} - \fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}} + \norm{\fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} - \fwd{t}{\mathbb{E}[\vct{x}_0| \vct{y}_t]}} \\
			&\quad+  \norm{\fwd{t}{\mathbb{E}[\vct{x}_0| \vct{y}_t]} - \fwd{t-\Delta t}{\mathbb{E} [\vct{x}_0| \vct{y}_t]}} + L_x^{(t-\Delta t)} \sqrt{n} B + \epsilon_t \\
			& \leq \norm{\fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} - \fwd{t}{\mathbb{E}[\vct{x}_0| \vct{y}_t]}}  + 2  L_t \Delta t + L_x^{(t-\Delta t)} \sqrt{n} B + \epsilon_t  \\
			&\leq \norm{\fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} - \mathbb{E}[\fwd{t}{\vct{x}_0}|\vct{y}_t]} + \norm{ \mathbb{E}[\fwd{t}{\vct{x}_0}|\vct{y}_t] - \fwd{t}{\mathbb{E}[\vct{x}_0| \vct{y}_t]}}   \\
			&\quad+ 2  L_t \Delta t + L_x^{(t-\Delta t)} \sqrt{n} B + \epsilon_t  \\
			&\leq 2  L_t \Delta t + (L_x^{(t-\Delta t)} + L_x^{(t)}) \sqrt{n} B + 2 \epsilon_t .
	\end{align*}

\end{proof}
%\subsection{Incremental reconstruction loss guarantee}
\subsection{Incremental reconstruction loss guarantee}
\begin{assumption} \label{as:G_lipschitz}
	The forward degradation transition function  $\mathcal{G}_{t' \rightarrow t''}$ for any $t', t'' \in [0, 1],~ t' < t''$ is Lipschitz continuous: $\| \mathcal{G}_{t' \rightarrow t''}(\vct{x}) - \mathcal{G}_{t' \rightarrow t''}(\vct{y})\| \leq L_G(t', t'') \|\vct{x} - \vct{y}\|, ~\forall t', t'' \in [0,1],~ t' < t'',~ \forall \vct{x},\vct{y} \in \mathbb{R}^n$.
\end{assumption}
This is a very natural assumption, as we don't expect the distance between two images after applying a degradation to grow arbitrarily large.
\begin{proposition} \label{thm:irn_loss}
	If the model $\Phi_{\vct{\theta}}(\vct{y}_t, t)$ has large enough capacity, such that $	\mathcal{L}_{IR}(\Delta t, {\vct{\theta}}) = 0$ is achieved, then $s_{\vct{\theta}}(\vct{y}_t, t) =  \nabla_{\vct{y}_t} \log q_t(\vct{y}_t), ~\forall t \in [0, 1]$. Otherwise, if Assumption \ref{as:G_lipschitz} holds, then we have 
	\begin{equation}
		\mathcal{L}({\vct{\theta}}) \leq \max_{t\in[0, 1]}( L_G(\tau, t)) \mathcal{L}_{IR}(\Delta t, {\vct{\theta}}) .
	\end{equation}
\end{proposition}
\begin{proof}
	We denote $\tau=\max(0, t - \Delta t)$.  First, if $	\mathcal{L}_{IR}(\Delta t, {\vct{\theta}}) = 0$, then $$ \fwd{\tau}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}  =  \fwd{\tau}{\vct{x}_0}$$  for all $(\vct{x}_0, \vct{y}_t)$ such that $q_t(\vct{x}_0, \vct{y}_t) > 0$.  Applying the forward degradation transition function to both sides yields $$\mathcal{G}_{\tau\rightarrow t}( \fwd{\tau}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}) = \mathcal{G}_{\tau\rightarrow t}(  \fwd{\tau}{\vct{x}_0}),$$ which is equivalent to $$ \fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}  =  \fwd{t}{\vct{x}_0}.$$ This in turn means that $\mathcal{L}({\vct{\theta}}) = 0$ and thus due to Theorem \ref{thm:sm} the score is learned.
	
	In the more general case, 
	\begin{align*}
			\mathcal{L}({\vct{\theta}}) &= \mathbb{E}_ {t, (\vct{x}_0, \vct{y}_t)}\left[w_t \left\|  \fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}  -  \fwd{t}{\vct{x}_0} \right\|^2\right] \\
			&=  \mathbb{E}_ {t, (\vct{x}_0, \vct{y}_t)}\left[w_t \left\|  \mathcal{G}_{\tau \rightarrow t}(\fwd{\tau}{\Phi_{\vct{\theta}}(\vct{y}_t, t)})  - \mathcal{G}_{\tau \rightarrow t}(\fwd{\tau}{\vct{x}_0}) \right\|^2\right]  \\
			&\leq \mathbb{E}_ {t, (\vct{x}_0, \vct{y}_t)}\left[w_t L_G(\tau, t) \left\|  \fwd{\tau}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}  -\fwd{\tau}{\vct{x}_0} \right\|^2\right]  \\
			&\leq \max_{t\in[0, 1]}( L_G(\tau, t))\mathbb{E}_ {t, (\vct{x}_0, \vct{y}_t)}\left[w_t \left\|  \fwd{\tau}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}  -\fwd{\tau}{\vct{x}_0} \right\|^2\right]  \\
			&=  \max_{t\in[0, 1]}( L_G(\tau, t)) \mathcal{L}_{IR}(\Delta t, {\vct{\theta}}) 
	\end{align*}
\end{proof}
This means that if the model has large enough capacity, minimizing the incremental reconstruction loss in \eqref{eq:loss_irn} also implies minimizing \eqref{eq:loss_final}, and thus the true score is learned (denoising is achieved). Otherwise, the incremental reconstruction loss is an upper bound on the loss in \eqref{eq:loss_final}. Training a model on \eqref{eq:loss_irn}, the model learns not only to denoise, but also to perform small, incremental reconstructions of the degraded image such that $\fwd{t-\Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} \approx  \fwd{t-\Delta t}{\vct{x}_0}$. There is however a trade-off between incremental reconstruction performance and learning the score: as Proposition \ref{thm:irn_loss} indicates, we are optimizing an upper bound to \eqref{eq:loss_final} and thus it is possible that the score estimation is less accurate. We expect our proposed incremental reconstruction loss to work best in scenarios where the degradation may change rapidly with respect to $t$ and hence a network trained to accurately estimate $\fwd{t}{\vct{x}_0}$ from $\vct{y}_t$ may become inaccurate when predicting $\fwd{t-\Delta t}{\vct{x}_0}$ from $\vct{y}_t$. This hypothesis is further supported by our experiments in Section \ref{sec:exp}. Finally, we mention that in the extreme case where we choose $\Delta t = 1$, we obtain a loss function purely in clean image domain.

\subsection{Theorem 3.7}
\begin{lemma}[Transitivity of data consistency]\label{lem:dc}
	If $\vct{y}_{t^+}\dc \vct{y}_t$ and $\vct{y}_{t^{++}}\dc \vct{y}_{t+}$ with $t < t^+ < t^{++}$, then $\vct{y}_{t^{++}} \dc \vct{y}_t$.
\end{lemma}
\begin{proof}
	By the definition of data consistency $\vct{y}_{t^{++}}\dc \vct{y}_{t+} \Rightarrow \exists \vct{x}_0: \fwd{t^{++}}{\vct{x}_0} = \vct{y}_{t^{++}}$ and $\fwd{t^{+}}{\vct{x}_0} = y_{t^{+}}$. On the other hand,  $y_{t^{+}}\dc \vct{y}_{t} \Rightarrow \exists \vct{x}_0': \fwd{t^{+}}{\vct{x}_0'} = y_{t^{+}}$ and $\fwd{t}{\vct{x}_0'} = \vct{y}_{t}$. Therefore,
	$$\vct{y}_{t^{++}} = \fwd{t^{++}}{\vct{x}_0} = \mathcal{G}_{t^+ \rightarrow t^{++}}(\fwd{t^+}{\vct{x}_0}) = \mathcal{G}_{t^+ \rightarrow t^{++}}(\vct{y}_{t^+}) =   \mathcal{G}_{t^+ \rightarrow t^{++}}(\fwd{t^+}{\vct{x}_0'}) =  \fwd{t^{++}}{\vct{x}_0'}.$$ By the definition of data consistency, this implies $\vct{y}_{t^{++}} \dc \vct{y}_t$.
\end{proof}

\begin{thmn}{\textbf{3.7.}}
	Assume that we run the updates in \eqref{eq:update} with $s_{\vct{\theta}}(\vct{y}_t, t) =  \nabla_{\vct{y}_t} \log q_t(\vct{y}_t | \vct{x}_0), ~\forall t \in [0, 1]$ and $	\hat{\mathcal{R}}(t, \Delta t; \vct{y}_t) = \mathcal{R}(t, \Delta t; \vct{x}_0), ~ \vct{x}_0\in \mathcal{X}_0$. If we start from a noisy degraded observation $\vct{\tilde{y}} = \fwd{1}{\vct{x}_0} + \vct{z}_{1}, ~ \vct{x}_0\in \mathcal{X}_0, ~\vct{z}_{1} \sim \gaussian{\sigma_{1}^2} $ and run the updates in \eqref{eq:update} for $\tau = 1,1-\Delta t, ..., \Delta t, 0$, then we have
	\begin{equation}
		\mathbb{E}[ \vct{\tilde{y}} ] \dc  \mathbb{E}[ \vct{y}_\tau] , ~	\forall \tau \in [1,1-\Delta t, ..., \Delta t, 0].
	\end{equation}
\end{thmn}
\begin{proof}
	Assume that we start from a known measurement $\vct{\tilde{y}} := \vct{y}_t = \fwd{t}{\vct{x}_0} + \vct{z}_t$ at arbitrary time $t$ and run reverse diffusion from $t$ with time step $\Delta t$. Starting from $t=1$ that we have looked at in the paper is a subset of this problem. Starting from arbitrary $\vct{y}_t$, the first update takes the form
	\begin{align*}
		\vct{y}_{t-\Delta t} &= \vct{y}_t + \fwd{t - \Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} - \fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} - (\sigma_{t - \Delta t}^2 - \sigma_t^2) \frac{\fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} - \vct{y}_t}{\sigma_t^2} + \sqrt{\sigma_t^2 - \sigma_{t - \Delta t}^2} \vct{z} = \\
		&=  \fwd{t}{\vct{x}_0} + \vct{z}_t + \fwd{t - \Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} - \fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} - (\sigma_{t - \Delta t}^2 - \sigma_t^2) \frac{\fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} - \fwd{t}{\vct{x}_0} - \vct{z}_t }{\sigma_t^2} \\
		&+\sqrt{\sigma_t^2 - \sigma_{t - \Delta t}^2} \vct{z} 
	\end{align*}
Due to our assumption on learning the score function, we have $\fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} = \fwd{t}{\vct{x}_0}$ and due to the perfect incremental reconstruction assumption $\fwd{t - \Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} = \fwd{t - \Delta t}{\vct{x}_0}$. Thus, we have 
$$
\vct{y}_{t-\Delta t} = \fwd{t - \Delta t}{\vct{x}_0} + \frac{\sigma_{t - \Delta t}^2}{\sigma_t^2} \vct{z}_t + \sqrt{\sigma_t^2 - \sigma_{t - \Delta t}^2} \vct{z}. 
$$
Since $z$ and $\vct{z}_t$ are independent Gaussian, we can combine the noise terms to yield
\begin{equation}\label{eq:update_form}
\vct{y}_{t-\Delta t} = \fwd{t - \Delta t}{\vct{x}_0} + \vct{z}_{t-\Delta t}, 
\end{equation}
with $\vct{z}_{t - \Delta_t} \sim \mathcal{N}(\vct{0}, \left[\left(\frac{\sigma_{t-\Delta t}^2}{\sigma_t}\right)^2 + \sigma^2_{t} - \sigma^2_{t-\Delta t}\right] \mathbf{I})$. This form is identical to the expression on our original measurement $\vct{\tilde{y}} = \vct{y}_t = \fwd{t}{\vct{x}_0} + \vct{z}_t$, but with slightly lower degradation severity and noise variance. It is also important to point out that $\mathbb{E}[\vct{y}_t] \dc \mathbb{E}[\vct{y}_{t-\Delta t}]$. If we repeat the update to find $\vct{y}_{t - 2\Delta t}$, we will have the same form as in \eqref{eq:update_form} and $\mathbb{E}[\vct{y}_{t-\Delta t}] \dc \mathbb{E}[\vct{y}_{t-2\Delta t}]$. Due to the transitive property of data consistency (Lemma \ref{lem:dc}), we also have $\mathbb{E}[\vct{y}_{t}] \dc \mathbb{E}[\vct{y}_{t-2\Delta t}]$, that is data consistency is preserved with the original measurement. This reasoning can be then extended for every further update using the transitivity property, therefore we have data consistency in each iteration.
\end{proof}
%\subsection{Guidance guarantee}
%\begin{proof}
%	\begin{equation*}
%		q_t(\vct{\tilde{y}} | \vct{y}_t) = \int q_t(\vct{\tilde{y}} | \vct{y}_t, \vct{x}_0) q_t(\vct{x}_0| \vct{y}_t) d\vct{x}_0= \int q_t(\vct{\tilde{y}}| \vct{x}_0) q_t(\vct{x}_0| \vct{y}_t) d\vct{x}_0= \mathbb{E}[f(\vct{x}_0)|\vct{y}_t],
%	\end{equation*}
%where $f = h(\fwd{1}{\cdot})$ and $h$ is the multivariate Gaussian distribution with mean $\vct{\tilde{y}}$ and covariance $\sigma_1^2\mathbf{I}$. 
%Then, the Jensen gap
%\begin{equation*}
%	\norm{f(\mathbb{E}[\vct{x}_0|\vct{y}_t]) -\mathbb{E}[f(\vct{x}_0)|\vct{y}_t]} \leq \int \norm{f(\mathbb{E}[\vct{x}_0|\vct{y}_t])) - f(\vct{x}_0)} q_t(\vct{x}_0| \vct{y}_t) d\vct{x}_0,
%\end{equation*}
%where we applied Jensen's inequality. We apply Lemma 3. from \cite{chung2022diffusion} on the Lipschitzness of multivariate normal distribution to obtain
%\begin{equation*}
%	 \int \norm{f(\mathbb{E}[\vct{x}_0|\vct{y}_t])) - f(\vct{x}_0)} q_t(\vct{x}_0| \vct{y}_t) d\vct{x}_0\leq \frac{n}{\sqrt{2\pi \sigma_1^2}}e^{-\frac{1}{2\sigma_1^2}} \int \norm{\fwd{1}{\mathbb{E}[\vct{x}_0|\vct{y}_t]} - \fwd{1}{\vct{x}_0}} q_t(\vct{x}_0| \vct{y}_t) d\vct{x}_0.
%\end{equation*}	
%Applying Assumption \ref{as:lipschitz}, followed by Jensen's inequality
%\begin{align*}
%&\frac{n}{\sqrt{2\pi \sigma_1^2}}e^{-\frac{1}{2\sigma_1^2}} \int \norm{\fwd{1}{\mathbb{E}[\vct{x}_0|\vct{y}_t]} - \fwd{1}{\vct{x}_0}} q_t(\vct{x}_0| \vct{y}_t) d\vct{x}_0\leq \\
%&\leq L_x^{(1)} \frac{n}{\sqrt{2\pi \sigma_1^2}}e^{-\frac{1}{2\sigma_1^2}} \int \norm{\mathbb{E}[\vct{x}_0|\vct{y}_t] - \vct{x}_0} q_t(\vct{x}_0| \vct{y}_t) d\vct{x}_0\leq \\
%&\leq  L_x^{(1)} \frac{n}{\sqrt{2\pi \sigma_1^2}}e^{-\frac{1}{2\sigma_1^2}} \int \norm{\mathbb{E}[\vct{x}_0|\vct{y}_t] - \vct{x}_0} q_t(\vct{x}_0| \vct{y}_t) d\vct{x}_0
%\end{align*}	
%
%	
%\end{proof}
\section{Degradation scheduling}\label{sec:scheduling}
\begin{figure}[t]
	\centering
	\begin{subfigure}{.29\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{plots/scheduling_blur.pdf}
	\end{subfigure}%
	\begin{subfigure}{.29\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{plots/scheduling_inpaint_celebA.pdf}
	\end{subfigure}
	\begin{subfigure}{.29\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{plots/scheduling_inpaint_imagenet.pdf}
	\end{subfigure}
	\caption{ Results of degradation scheduling from Algorithm \ref{alg:scheduling_simple}. \underline{Left}: Gaussian blur with kernel std $w_t$ on CelebA-HQ. \underline{Center}: inpainting with Gaussian mask with kernel width $w_t$ on CelebA-HQ. \underline{Right}:  inpainting with Gaussian mask on ImageNet.}
	\label{fig:supp_sched}
\end{figure}
When solving inverse problems, we have access to a noisy measurement $\vct{\tilde{y}} = \fwd{}{\vct{x}_0} + \vct{z}$ and we would like to find the corresponding clean image $\vct{x}_0$. In order to deploy our method, we need to define how the degradation changes with respect to severity $t$ following the properties specified in Definition \ref{def:sdp}. That is, we have to determine how to interpolate between the identity mapping $\fwd{0}{\vct{x}} = \vct{x}$ for $t=0$ and the most severe degradation $\fwd{1}{\cdot} = \fwd{}{\cdot}$ for $t=1$.  Theorem \ref{thm:basic_error} suggests that sharp changes in the degradation function with respect to $t$ should be avoided, however a more principled method of scheduling is needed.

 In the context of image generation, \cite{daras2022soft} proposes a scheduling framework that splits the path between the distribution of clean images $\mathcal{D}_0$ and the distribution of pure noise $\mathcal{D}_1$ into $T$ candidate distributions $\mathcal{D}_i, ~ i \in[1/T, 2/T, ..., \frac{T-1}{T}]$. Then, they find a path through the candidate distributions that minimizes the total path length, where the distance between $\mathcal{D}_i$ and $\mathcal{D}_j$ is measured by the Wasserstein-distance. However, for image reconstruction, instead of distance between image distributions, we are more interested in how much a given image degrades in terms of image quality metrics such as PSNR or LPIPS. Therefore, we replace the Wasserstein-distance by a notion of distance between two degradation severities $d(t_i, t_j) := \mathbb{E}_{\vct{x}_0\sim \mathcal{D}_0}[\mathcal{M}(\fwd{t_i}{\vct{x}_0}, \fwd{t_j}{\vct{x}_0})]$, where $\mathcal{M}$ is some distortion-based or perceptual image quality metric that acts on a corresponding pair of images. 

We propose a greedy algorithm to select a set of degradations from the set of candidates based on the above notion of dataset-dependent distance, such that the maximum distance is minimized. That is, our scheduler is not only a function of the degradation $\mathcal{A}_t$, but also the data. The intuitive reasoning to minimize the maximum distance is that our model has to be imbued with enough capacity to bridge the gap between any two consecutive distributions during the reverse process, and thus the most challenging transition dictates the required network capacity.  In particular, given a budget of $m$ intermediate distributions on $[0, 1]$, we would like to pick a set of $m$ interpolating severities $\mathcal{S}$ such that
\begin{equation}
	\mathcal{S} = arg \min_{\mathcal{T}} \max_{i} d(t_i, t_{i+1}),
\end{equation}
where $\mathcal{T} = \{t_1, t_2, ..., t_m | t_i \in [0,1], ~ t_i < t_{i+1}~ \forall i\in (1,2,...,m)\}$ is the set of possible interpolating severities with budget $m$. To this end, we start with $\mathcal{S} =\{0, 1\}$ and add new interpolating severities one-by-one, such that the new point splits the interval in $\mathcal{S}$ with the maximum distance. Thus, over iterations the maximum distance is non-increasing. We also have local optimality, as moving a single interpolating severity must increase the maximum distance by the construction of the algorithm. Finally, we use linear interpolation in between the selected interpolating severities. The technique is summarized in Algorithm \ref{alg:scheduling_simple}, and we refer the reader to the source code for implementation details. 

The results of our proposed greedy scheduling algorithm are shown in Figure \ref{fig:supp_sched}, where the distance is defined based on the LPIPS metric. In case of blurring, we see a sharp decrease in degradation severity close to $t=1$. This indicates, that LPIPS difference between heavily blurred images is small, therefore most of the diffusion takes place at lower blur levels. On the other hand, we find that inpainting mask size is scaled almost linearly by our algorithm on both datasets we investigated.

\begin{algorithm}
	\caption{Greedy Degradation Scheduling}\label{alg:scheduling_simple}
	\begin{algorithmic}
		\Require $\mathcal{M}$: pairwise image dissimilarity metric, $\mathcal{X}_0$: clean samples, $\mathcal{A}_t$: unscheduled degradation function, $N$: number of candidate points, $m$: number of interpolation points
		\State $ts \gets (0, \frac{1}{N-1}, \frac{2}{N-1}, ..., \frac{N-2}{N-1}, 1)$ \Comment{$N$ candidate severities uniformly distributed over $[0, 1]$}
		\State{$\mathcal{S} \gets (1,  N)$} \Comment{Array of indices of output severities in $ts$}
		\State{$d_{max} \gets Distance(ts[1], ts[N])$} \Comment{Maximum distance between two severities in the output array}
		\State{$e_{start} \gets 1$} \Comment{Start index of edge with maximum distance}
		\State{$e_{end} \gets N$} \Comment{End index of edge with maximum distance}
		\For{$i = 1 \text{ to }m$}
			\State{$s \gets FindBestSplit(e_{start}, e_{end}, d_{max} )$}
			\State{$Append(\mathcal{S}, s )$}
			\State{$d_{max}, e_{start}, e_{end} \gets UpdateMax(\mathcal{S})$}
		\EndFor
		\Ensure{$\mathcal{S}$} \\
		\Procedure{Distance}{$t_i, t_j$} \Comment{Distance between degradation severities $t_i$ and $t_j$}
		\State $d \gets \frac{1}{|\mathcal{X}_0|}\sum_{x \in \mathcal{X}_0} \mathcal{M}(\mathcal{A}_{t_i}(x), \mathcal{A}_{t_j}(x))$
		\Ensure $d$
		\EndProcedure \\
		\Procedure{FindBestSplit}{$e_{start}, e_{end}, d_{max}$} \Comment{Split edge into two new edges with minimal maximum distance}
		\State{$MaxDistance \gets  d_{max}$}
		\For{$j = e_{start} + 1 \text{ to } e_{end} - 1$}
			\State{$d_1 \gets Distance(ts[e_{start}],  ts[j])$}
			\State{$d_2 \gets Distance( ts[j], ts[e_{end}])$}
			\If{$ \max(d_1, d_2) < MaxDistance$}
				\State{$MaxDistance \gets \max(d_1, d_2)$}
				\State{$Split \gets j$}
			\EndIf
		\EndFor
		\Ensure $Split$
		\EndProcedure \\
		\Procedure{UpdateMax}{$\mathcal{S}$}
		\State{$MaxDistance \gets 0$}
		\For{$i = 1 \text{ to } |S| - 1$}
			\State{$e_{start} \gets \mathcal{S}[i]$}
			\State{$e_{end} \gets \mathcal{S}[i+1]$}
			\State{$d \gets Distance(ts[e_{start}], ts[e_{end}] )$}
			\If{$d > MaxDistance$}
				\State{$MaxDistance \gets d $}
				\State{$NewStart \gets e_{start} $}
				\State{$NewEnd \gets e_{end}  $}
			\EndIf
		\EndFor
		\Ensure $MaxDistance, NewStart, NewEnd$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\section{Guidance details}\label{sec:supp_guidance}
Even though \methodname{} does not need to rely on $\vct{\tilde{y}}$ after the initial update for maintaining data-consistency, we observe small improvements in reconstructions when adding a guidance scheme to our algorithm. As described in Section \ref{sec:guidance}, our approximation of the posterior score takes the form
\begin{equation}\label{eq:supp_guidance}
	\small{
		s'_{\vct{\theta}}(\vct{y}_t, t) = s_{\vct{\theta}}(\vct{y}_t, t)  - \eta_t \nabla_{\vct{y}_t}\frac{\|\vct{\tilde{y}} - \fwd{1}{ \Phi_{\vct{\theta}}(\vct{y}_t, t)} \|^2}{2 \sigma_1^2},}
\end{equation}
where $\eta_t$ is a hyperparameter that tunes how much we rely on the original noisy measurement. For the sake of simplicity, in this discussion we merge the scaling of the gradient into the step size parameter as follows: 
\begin{equation}
	\small{
		s'_{\vct{\theta}}(\vct{y}_t, t) = s_{\vct{\theta}}(\vct{y}_t, t)  - \eta_t' \nabla_{\vct{y}_t}\|\vct{\tilde{y}} - \fwd{1}{ \Phi_{\vct{\theta}}(\vct{y}_t, t)} \|^2}
\end{equation}

We experiment with two choices of step size scheduling for the guidance term $\eta_t'$:
\begin{itemize}
	\item \textit{Standard deviation scaled (constant)}: $\eta_t = \eta \frac{1}{2\sigma_1^2}$, where $\eta$ is a constant hyperparameter and $\sigma_1^2$ is the noise level on the measurements. This scaling is justified by our derivation of the posterior score approximation, and matches \eqref{eq:supp_guidance}.
	\item \textit{Error scaled}: $\eta_t = \eta \frac{1}{\|\vct{\tilde{y}} - \fwd{1}{ \Phi_{\vct{\theta}}(\vct{y}_t, t)} \|}$, which has been proposed in \cite{chung2022diffusion}. This method attempts to normalize the gradient of the data consistency term. 
\end{itemize}
In general, we find that constant step size works better for deblurring, whereas error scaling performed slightly better for inpainting experiments, however the difference is minor. Figure \ref{fig:supp_abl_guidance} shows the results of our ablation study on the effect of  $\eta_t$. We perform deblurring experiments on the CelebA-HQ validation set and plot the mean LPIPS (lower the better) with different step size scheduling methods and varying step size. We see some improvement in LPIPS when adding guidance to our method, however it is not a crucial component in obtaining high quality reconstructions, or for maintaining data-consistency.
\begin{figure}[t]
	\centering
	\begin{subfigure}{.39\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{plots/ablation_guidance_std_scaled_celebaHQ.pdf}
	\end{subfigure}%
	\begin{subfigure}{.39\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{plots/ablation_guidance_error_scaled_celebaHQ.pdf}
	\end{subfigure}
	\caption{ Effect of guidance step size on best reconstruction in terms of LPIPS. We perform experiments on the CelebA-HQ validation set on the deblurring task.}
	\label{fig:supp_abl_guidance}
\end{figure}
\section{Note on the output of the algorithm}
 In the ideal case, $\sigma_0 = 0$ and $\mathcal{A}_{0} = \mathbf{I}$. However, in practice due to geometric noise scheduling (e.g. $\sigma_0 = 0.01$), there is small magnitude additive noise expected on the final iterate. Moreover, in order to keep the scheduling of the degradation smooth, and due to numerical stability in practice  $\mathcal{A}_{0}$ may slightly deviate from the identity mapping close to $t=0$ (for example very small amount of blur). Thus, even close to $t=0$, there may be a gap between the iterates $\yt$ and the posterior mean estimates $\hat{\vct{x}}_0=\Phi_\theta(\yt, t)$. Due to these reasons, we observe that in some experiments taking $\Phi_\theta(\yt, t)$ as the final output yields better reconstructions. In case of early stopping, taking $\hat{\vct{x}}_0$ as the output is instrumental, as an intermediate iterate $\yt$ represents a sample from the reverse SDP, thus it is expected to be noisy and degraded. However, as $\Phi_\theta(\yt, t)$ always predicts the clean image, it can be used at any time step $t$ to obtain an early-stopped prediction of $\xo$.

\section{Experimental details}
\textbf{Datasets -- }We evaluate our method on CelebA-HQ ($256 \times 256$) \cite{karras_progressive_2018} and ImageNet ($256 \times 256$) \cite{deng2009imagenet}. For CelebA-HQ training, we use $80\%$ of the dataset for training, and the rest for validation and testing. For ImageNet experiments, we sample $1$ image from each class from the official validation split to create disjoint validation and test sets of $1k$ images each. We only train our model on the official train split of ImageNet. We center-crop and resize ImageNet images to $256 \times 256$ resolution. For both datasets, we scale images to $[0, 1]$ range.

\textbf{Comparison methods -- } We compare our method against DDRM \cite{kawar2022denoising}, the most well-established diffusion-based linear inverse problem solver; DPS \cite{chung2022diffusion}, a very recent, state-of-the-art diffusion technique for noisy and possibly nonlinear inverse problems; PnP-ADMM \cite{chan2016plug}, a reliable traditional solver with learned denoiser; and ADMM-TV, a classical optimization technique.  More details can be found in Section \ref{sec:comparison_methods}.

\textbf{Models --  } For \methodname{}, we train new models from scratch using the NCSN++\cite{song2020score} architecture with \texttt{67M} parameters for all tasks except for ImageNet inpainting, for which we scale the model to \texttt{126M} parameters. For competing methods that require a score model, we use pre-trained SDE-VP models\footnote{CelebA-HQ: \url{https://github.com/ermongroup/SDEdit} \\ ImageNet: \url{https://github.com/openai/guided-diffusion}} (\texttt{126M} parameters for CelebA-HQ, \texttt{553M} parameters for ImageNet). The architectural hyper-parameters for the various score-models can be seen in Table \ref{tab:arch_hparams}.

\textbf{Training details -- }We train all models with Adam optimizer, with learning rate $0.0001$ and batch size $32$ on $8 \times$ Titan RTX GPUs. We do not use exponential moving averaging or learning rate scheduling schemes. We train for approximately $10M$ examples seen by the network. For the weighting factor $w(t)$ in the loss, we set $w(t) = \frac{1}{\sigma_t^2}$ in all experiments.

\textbf{Degradations -- } We investigate two degradation processes of very different properties: Gaussian blur and inpainting, both with additive Gaussian noise. In all cases, noise with $\sigma_1 = 0.05$ is added to the measurements in the $[0, 1]$ range. We use standard geometric noise scheduling with $\sigma_{max} = 0.05$ and $\sigma_{min} = 0.01$ in the SDP. For Gaussian blur, we use a kernel size of $61$, with standard deviation of $w_{max} =3$ to create the measurements. We change the standard deviation of the kernel between $w_{max}$ (strongest) and $0.3$ (weakest) to parameterize the severity of Gaussian blur in the degradation process, and use the scheduling method described in Section \ref{sec:scheduling} to specify $\mathcal{A}_t$. We keep an imperceptible amount of blur for $t=0$ to avoid numerical instability with very small kernel widths. For inpainting, we generate a smooth mask in the form $\left(1 - \frac{f(\vct{x}; w_t)}{\max_{\vct{x}} f(\vct{x}; w_t)}\right)^k$, where $f(\vct{x}; w_t)$ denotes the density of a zero-mean isotropic Gaussian with standard deviation $w_t$ that controls the size of the mask and $k=4$ for sharper transition. We set $w_1 = 50$ for CelebA-HQ/FFHQ inpainting and $30$ for ImageNet inpainting.

\textbf{Evaluation method -- }To evaluate performance, we use PSNR and SSIM as distortion metrics and LPIPS and FID as perceptual quality metrics. For the final reported results, we scale and clip all outputs to the $[0, 1]$ range before computing the metrics. We use validation splits to tune the hyper-parameters for all methods, where we optimize for best LPIPS in the deblurring task and for best FID for inpainting. As the pre-trained score-models for competing methods have been trained on the full CelebA-HQ dataset, we test all methods for fair comparison on the first $1k$ images of the FFHQ \cite{karras2019style} dataset. The list of test images for ImageNet can be found in the source code.

\textbf{Sampling hyperparameters -- } The settings are summarized in Table \ref{tab:sampling}. We tune the reverse process hyper-parameters on validation data. For the interpretation of 'guidance scaling' we refer the reader to the explanation of guidance step size methods in Section \ref{sec:supp_guidance}. In Table \ref{tab:sampling}, 'output' refers to whether the final reconstruction is the last model output (posterior mean estimate, $\hat{\vct{x}}_0=\Phi_\theta(\yt, t)$) or the final iterate $\yt$.

\begin{table}
	\centering
		\begin{tabular}{ lccccc }
			\toprule
			&\multicolumn{4}{c}{\textbf{\methodname{}(Ours)}} & \\
			\cmidrule{2-5}
			\textbf{Hparam} &Deblur/CelebA-HQ&Deblur/ImageNet&Inpainting/CelebA-HQ&Inpainting/ImageNet&\\
			\midrule
		    \code{model\_channels}       &$128$&$128$&$128$&$128$&  \\
			\code{channel\_mult} &$[1, 1, 2, 2, 2, 2, 2]$&$[1, 1, 2, 2, 2, 2, 2]$&$[1, 1, 2, 2, 2, 2, 2]$&$[1, 1, 2, 2, 4, 4]$&  \\
			\code{num\_res\_blocks}       &$2$&$2$&$2$&$2$&  \\
			\code{attn\_resolutions} &$[16]$&$[16]$&$[16]$&$[16]$&  \\
			\code{dropout}       &$0.1$&$0.1$&$0.1$&$0.0$&  \\
			\hline
			Total \# of parameters       &\code{67M}&\code{67M}&\code{67M}&\code{126M}&  \\
			\bottomrule
		\end{tabular}
		\centering
	\begin{tabular}{ lccccc }
		\toprule
		&\multicolumn{4}{c}{\textbf{DDRM/DPS}} & \\
		\cmidrule{2-5}
		\textbf{Hparam} &Deblur/CelebA-HQ&Deblur/ImageNet&Inpainting/CelebA-HQ&Inpainting/ImageNet&\\
		\midrule
		\code{model\_channels}       &$128$&$256$&$128$&$256$&  \\
		\code{channel\_mult} &$[1, 1, 2, 2, 4, 4]$&$[1, 1, 2, 2, 4, 4]$&$[1, 1, 2, 2, 4, 4]$&$[1, 1, 2, 2, 4, 4]$&  \\
		\code{num\_res\_blocks}       &$2$&$2$&$2$&$2$&  \\
		\code{attn\_resolutions} &$[16]$&$[32, 16, 8]$&$[16]$&$[32, 16, 8]$&  \\
		\code{dropout}       &$0.0$&$0.0$&$0.0$&$0.0$&  \\
		\hline
		Total \# of parameters       &\code{126M}&\code{553M}&\code{126M}&\code{553M}&  \\
		\bottomrule
	\end{tabular}
	\caption{Architectural hyper-parameters for the score-models for \methodname{} (top) and other diffusion-based methods (bottom) in our experiments.\label{tab:arch_hparams}}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{ lccccc }
		\toprule
		&\multicolumn{4}{c}{\textbf{PO Sampling hyper-parameters}} & \\
		\cmidrule{2-5}
		\textbf{Hparam} &Deblur/CelebA-HQ&Deblur/ImageNet&Inpainting/CelebA-HQ&Inpainting/ImageNet&\\
		\midrule
		$\Delta t$     &$0.02$&$0.02$&$0.005$&$0.01$&  \\
		$t_{stop}$ &$0.25$&$0.0$&$0.0$&$0.0$&  \\
		$\eta_t$       &$0.5$&$0.2$&$1.0$&$0.0$&  \\
		Guidance scaling &std&std&error&-&  \\
		Output       &$\hat{\vct{x}}_0$&$\hat{\vct{x}}_0$&$\yt$&$\yt$&  \\
		\bottomrule
	\end{tabular}
	\begin{tabular}{ lccccc }
	\toprule
	&\multicolumn{4}{c}{\textbf{DO Sampling hyper-parameters}} & \\
	\cmidrule{2-5}
	\textbf{Hparam} &Deblur/CelebA-HQ&Deblur/ImageNet&Inpainting/CelebA-HQ&Inpainting/ImageNet&\\
	\midrule
	$\Delta t$     &$0.02$&$0.02$&$0.005$&$0.01$&  \\
	$t_{stop}$ &$0.98$&$0.7$&$0.995$&$0.99$&  \\
	$\eta_t$       &$0.5$&$1.5$&$1.0$&$0.0$&  \\
	Guidance scaling &std&std&error&-&  \\
	Output       &$\hat{\vct{x}}_0$&$\hat{\vct{x}}_0$&$\hat{\vct{x}}_0$&$\hat{\vct{x}}_0$&  \\
	\bottomrule
\end{tabular}
\caption{Settings for perception optimized (PO) and distortion optimized (DO) sampling for all experiments on test data. \label{tab:sampling}}
\end{table}

\section{Incremental reconstruction loss ablations}
We propose the incremental reconstruction loss, that combines learning to denoise and reconstruct simultaneously in the form 
\begin{equation} \label{eq:supp_loss_irn}
	\mathcal{L}_{IR}(\Delta t, {\vct{\theta}}) = \\ \mathbb{E}_ {t, (\vct{x}_0, \vct{y}_t)}\left[w(t) \left\|  \fwd{\tau}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}  -  \fwd{\tau}{\vct{x}_0} \right\|^2\right],
\end{equation}
where $\tau = \text{max}(t-\Delta t, 0), ~ t\sim U[0,1],$ $(\vct{x}_0, \vct{y}_t)\sim q_0(\vct{x}_0) q_t(\vct{y}_t | \vct{x}_0)$. This loss directly improves incremental reconstruction by encouraging $\fwd{t-\Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} \approx \fwd{t-\Delta t}{\xo}$. We show in Proposition \ref{thm:irn_loss} that  $\mathcal{L}_{IR}(\Delta t, {\vct{\theta}})$ is an upper bound to the denoising score-matching objective $\mathcal{L}(\vct{\theta})$. Furthermore, we show that given enough model capacity,  minimizing  $\mathcal{L}_{IR}(\Delta t, {\vct{\theta}})$ also minimizes $\mathcal{L}(\vct{\theta})$. However, if the model capacity is limited compared to the difficulty of the task, we expect a trade-off between incremental reconstruction accuracy and score accuracy. This trade-off might not be favorable in tasks where incremental reconstruction is accurate enough due to the smoothness properties of the degradation (see Theorem \ref{thm:basic_error}). Here, we perform further ablation studies to investigate the effect of the \textit{look-ahead} parameter $\Delta t$ in the incremental reconstruction loss. 

\textbf{Deblurring -- }In case of deblurring, we did not find a significant difference in perceptual quality with different $\Delta t$ settings. Our results on the CelebA-HQ validation set can be seen in Figure \ref{fig:supp_inc_recon_loss} (left). We observe that using $\Delta t =0$ (that is optimizing $\mathcal{L}(\vct{\theta})$) yields slightly better reconstructions (difference in the third digit of LPIPS) than optimizing with $\Delta t = 1$, that is minimizing 
\begin{equation}
	\mathcal{L}_{IR}(\Delta t = 1, {\vct{\theta}}) := 	\mathcal{L}_{IR}^{\mathcal{X}_0}(\vct{\theta}) =\\ \mathbb{E}_ {t, (\vct{x}_0, \vct{y}_t)}\left[w(t) \left\|  \Phi_{\vct{\theta}}(\vct{y}_t, t)  -  \vct{x}_0 \right\|^2\right].
\end{equation}
This loss encourages one-shot reconstruction and denoising from any degradation severity, intuitively the most challenging task to learn. We hypothesize, that the blur degradation used in our experiments is smooth enough, and thus the incremental reconstruction as per Theorem \ref{thm:basic_error} is accurate. Therefore, we do not need to trade off score approximation accuracy for better incremental reconstruction.

\textbf{Inpainting -- } We observe very different characteristics in case of inpainting. In fact, using the vanilla score-matching loss $\mathcal{L}(\vct{\theta})$, which is equivalent to $\mathcal{L}_{IR}(\Delta t, {\vct{\theta}})$ with $\Delta t =0$, we are unable to learn a meaningful inpainting model. As we increase the look-ahead $\Delta t$, reconstructions consistently improve. We obtain the best results in terms of FID when minimizing $\mathcal{L}_{IR}^{\mathcal{X}_0}(\vct{\theta})$. Our results are summarized in Figure \ref{fig:supp_inc_recon_loss} (middle). We hypothesize that due to rapid changes in the inpainting operator, our incremental reconstruction estimator produces very high errors when trained on $\mathcal{L}(\vct{\theta})$ (see Theorem \ref{thm:basic_error}). Therefore, in this scenario improving incremental reconstruction at the expense of score accuracy is beneficial. Figure \ref{fig:supp_inc_recon_loss} (right) demonstrates how reconstructions visually change as we increase the look-ahead $\Delta t$. With $\Delta t =0$, the reverse process misses the clean image manifold completely. As we increase $\Delta t$, reconstruction quality visually improves, but the generated images often have features inconsistent with natural images in the training set. We obtain high quality, detailed reconstructions for $\Delta t = 1$ when minimizing $\mathcal{L}_{IR}^{\mathcal{X}_0}(\vct{\theta})$. 
\begin{figure}[t]
	\centering
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[width=0.95\linewidth]{plots/increcon_loss_ablation_deblur.pdf}
	\end{subfigure}%
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[width=0.95\linewidth]{plots/increcon_loss_ablation_inpaint.pdf}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[width=1.0\linewidth]{plots/increcon_loss_ablation_vis.png}
\end{subfigure}
	\caption{ Effect of incremental reconstruction loss step size on the CelebA-HQ validation set for deblurring (left) and inpainting (middle). Visual comparison of inpainted samples is shown on the right.}
	\label{fig:supp_inc_recon_loss}
\end{figure}
\section{Further incremental reconstruction approximations}
In this work, we focused on estimating the incremental reconstruction
\begin{equation}\label{eq:supp_R}
		\mathcal{R}(t, \Delta t; \vct{x}_0) := \fwd{t-\Delta t}{\vct{x}_0} -  \fwd{t}{\vct{x}_0}
\end{equation}
in the form 
\begin{equation}\label{eq:supp_R_hat}
		\hat{\mathcal{R}}(t, \Delta t; \vct{y}_t) = \fwd{t-\Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} -  \fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)},
\end{equation} 
which we call the \textit{look-ahead method}. The challenge with this formulation is that we use $\yt$ with degradation severity $t$ to predict $\fwd{t-\Delta t}{\xo}$ with less severe degradation $t - \Delta t$. That is, as we discussed in the paper $\Phi_\theta(\yt, t)$ does not only need to denoise images with arbitrary degradation severity, but also has to be able to perform incremental reconstruction, which we address with the incremental reconstruction loss. However, other methods of approximating \eqref{eq:supp_R} are also possible, with different trade-offs. The key idea is to use different methods to estimate the gradient of $\mathcal{A}_t(\xo)$ with respect to the degradation severity, followed by first-order Taylor expansion to estimate $\mathcal{A}_{t-\Delta t}(\xo)$. 

\textbf{Small look-ahead (SLA) -- }We use the approximation
\begin{equation}
	\fwd{t-\Delta t}{\vct{x}_0} -  \fwd{t}{\vct{x}_0} \approx  \Delta t \cdot \frac{\fwd{t-\delta t}{\vct{x}_0} -  \fwd{t}{\vct{x}_0}}{\delta t},
\end{equation}
where $0 < \delta t < \Delta t$ to obtain
 \begin{equation}\label{eq:supp_R_hat_sla}
	\hat{\mathcal{R}}^{SLA} (t, \Delta t; \vct{y}_t)=  \Delta t \cdot \frac{\fwd{t-\delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} -  \fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}}{\delta t}.
\end{equation}
The potential benefit of this method is that $\fwd{t-\delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}$ may approximate $\fwd{t-\delta t}{\xo}$ much more accurately than $\fwd{t-\Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}$ can approximate $\fwd{t-\Delta t}{\xo}$, since $t-\delta t$ is closer in severity to $t$ than $t - \Delta t$. However, depending on the sharpness of $\mathcal{A}_t$, the first-order Taylor approximation may accumulate large error.

\textbf{Look-back (LB) -- } We use the approximation
\begin{equation}
	\fwd{t-\Delta t}{\vct{x}_0} -  \fwd{t}{\vct{x}_0} \approx \fwd{t}{\vct{x}_0} -  \fwd{t + \Delta t}{\vct{x}_0},
\end{equation}
that is we predict the incremental reconstruction based on the most recent change in image degradation.  Plugging in our model yields
\begin{equation}
	\hat{\mathcal{R}}^{LB}(t, \Delta t; \vct{y}_t) = \fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} -  \fwd{t+\Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}.
\end{equation} 
The clear advantage of this formulation over \eqref{eq:supp_R_hat} is that if the loss in \eqref{eq:loss_final} is minimized such that $\fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} = \fwd{t}{\xo}$, then we also have $$\fwd{t + \Delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} = \mathcal{G}_{t \rightarrow t + \Delta t}(\fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}) = \mathcal{G}_{t \rightarrow t + \Delta t}(\fwd{t}{\xo}) = \fwd{t + \Delta t}{\xo}. $$
However, this method may also accumulate large error if $\mathcal{A}_{t}$ changes rapidly close to $t$.

\textbf{Small look-back (SLB)-- }Combining the idea in SLA with LB yields the approximation 
\begin{equation}
	\fwd{t-\Delta t}{\vct{x}_0} -  \fwd{t}{\vct{x}_0} \approx  \Delta t \cdot \frac{\fwd{t}{\vct{x}_0} -  \fwd{t +\delta t}{\vct{x}_0}}{\delta t},
\end{equation}
where $0 < \delta t < \Delta t$. Using our model, the estimator of the incremental reconstruction takes the form  
 \begin{equation}\label{eq:supp_R_hat_slb}
	\hat{\mathcal{R}}^{SLB} (t, \Delta t; \vct{y}_t)=  \Delta t \cdot \frac{\fwd{t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} -  \fwd{t+ \delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)}}{\delta t}.
\end{equation}
Compared with LB, we still have $\fwd{t+ \delta t}{\Phi_{\vct{\theta}}(\vct{y}_t, t)} = \fwd{t+ \delta t}{\xo}$ and the error due to first-order Taylor-approximation is reduced, however potentially higher than in case of SLA.

\textbf{Incremental Reconstruction Network -- }Finally, an additional model $\phi_{\vct{\theta}'}$ can be trained to directly approximate the incremental reconstruction, that is  $\phi_{\vct{\theta}'}(\yt, t) \approx 	\mathcal{R}(t, \Delta t; \vct{x}_0)$. All these approaches are interesting directions for future work.

\subsection{Comparison methods}\label{sec:comparison_methods}
For all methods, hyperparameters are tuned based on first $100$ images of the folder \texttt{"00001"} for FFHQ and tested on the folder \texttt{"00000"}. For ImageNet experiments, we use the first samples of the first $100$ classes of ImageNet validation split to tune, last samples of each class as the test set.
\subsubsection{DPS}
We use the default value of 1000 NFEs for all tasks. We make no changes to the Gaussian blurring operator in the official source code. For inpainting, we copy our operator and apply it in the image input range $[0,1]$. The step size $\zeta'$ is tuned via grid search for each task separately based on LPIPS metric. The optimal values are as follows:
\begin{enumerate}
	\item FFHQ Deblurring: $\zeta' = 3.0$
	\item FFHQ Inpainting: $\zeta' = 2.0$
	\item ImageNet Deblurring: $\zeta' = 0.3$
	\item ImageNet Inpainting: $\zeta' = 3.0$
\end{enumerate}
As a side note, at the time of writing this paper, the official implementation of DPS\footnote{\url{https://github.com/DPS2022/diffusion-posterior-sampling}} adds the noise to the measurement after scaling it to the range $[-1,1]$. For the same noise standard deviation, the effect of the noise is halved as compared to applying in $[0,1]$ range. To compensate for this discrepancy, we set the noise std in the official code to $\sigma=0.1$ for all DPS experiments which is the same effective noise level as $\sigma=0.05$ for our experiments.
\subsubsection{DDRM}
We keep the default settings $\eta_B=1.0$, $\eta=0.85$ for all of the experiments and sample for $20$ NFEs with DDIM \cite{song2021denoising}. For the Gaussian deblurring task, the linear operator has been implemented via separable 1D convolutions as described in D.5 of DDRM \cite{kawar2022denoising}. We note that for blurring task, the operator is applied to the reflection padded input. For Gaussian inpainting task, we set the left and right singular vectors of the operator to be identity ($\textbf{U}=\textbf{V}=\textbf{I}$) and store the mask values as the singular values of the operator. For both tasks, operators are applied to the image in the $[-1,1]$ range.
\subsubsection{PnP-ADMM}
We take the implementation from the \texttt{scico} library. Specifically the code is modified from the sample notebook\footnote{\url{https://github.com/lanl/scico-data/blob/main/notebooks/superres_ppp_dncnn_admm.ipynb}}. We set the number of ADMM iterations to be \texttt{maxiter}=$12$ and tune the ADMM penalty parameter $\rho$ via grid search for each task based on LPIPS metric.  The values for each task are as follows: 
\begin{enumerate}
	\item FFHQ Deblurring: $\rho = 0.1$
	\item FFHQ Inpainting: $\rho = 0.4$
	\item ImageNet Deblurring: $\rho = 0.1$
	\item ImageNet Inpainting: $\rho = 0.4$
\end{enumerate}
The proximal mappings are done via pre-trained DnCNN denoiser with \texttt{17M} parameters.
\subsubsection{ADMM-TV}
We want to solve the following objective:
\begin{equation*}
	\argmin_{\vct{x}} \frac{1}{2} \| \vct{y} - \mathcal{A}_1(\vct{x})\|_2^2 + \lambda \|\textbf{D}\vct{x}\|_{2,1}
\end{equation*}
where $\vct{y}$ is the noisy degraded measurement, $\mathcal{A}_1(\cdot)$ refers to blurring/masking operator and $\textbf{D}$ is a finite difference operator. $\|\textbf{D}\vct{x}\|_{2,1}$ TV regularizes the prediction $\vct{x}$ and $\lambda$ controls the regularization strength. For a matrix $\textbf{A} \in \mathbb{R}^{m\times n}$, the matrix norm $\|.\|_{2,1}$ is defined as:
\begin{equation*}
	\|\textbf{A}\|_{2,1} = \sum_{i=1}^m \sqrt{\sum_{j=1}^n \textbf{A}_{ij}^2}
\end{equation*}
The implementation is taken from \texttt{scico} library where the code is based on the sample notebook\footnote{\url{https://github.com/lanl/scico-data/blob/main/notebooks/deconv_tv_padmm.ipynb}}. We note that for consistency, the blurring operator is applied to the reflection padded input. In addition to the penalty parameter $\rho$, we need to tune the regularization strength $\lambda$ in this problem. We tune the pairs $(\lambda,\rho)$ for each task via grid search based on LPIPS metric. Optimal values are as follows:
\begin{enumerate}
	\item FFHQ Deblurring: $(\lambda,\rho) = (0.007,0.8)$
	\item FFHQ Inpainting: $(\lambda,\rho) = (0.02,0.2)$
	\item ImageNet Deblurring: $(\lambda,\rho) = (0.007,0.5)$
	\item ImageNet Inpainting: $(\lambda,\rho) = (0.02,0.2)$
\end{enumerate}
\section{Further reconstruction samples}
Here, we provide more samples from \methodname{} reconstructions on the test split of CelebA-HQ and ImageNet datasets. We visualize the uncertainty of samples via pixelwise standard deviation across $n=10$ generated samples. In experiments where the distortion peak is achieved via one-shot reconstruction, we omit the uncertainty map.
\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{plots/increcon_celeba_blur_figure.pdf}
	\caption{Distortion and Perception optimized deblurring results for the CelebA-HQ dataset (test split). Uncertainty is calculated over $n=10$ reconstructions from the same measurement.}
	\label{fig:celeba_blur_figure}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.85\linewidth]{plots/increcon_celeba_inpainting_figure.pdf}
	\caption{Distortion and Perception optimized inpainting results for the CelebA-HQ dataset (test split). Uncertainty is calculated over $n=10$ reconstructions from the same measurement. For distortion optimized runs, images are generated in one-shot, hence we don't provide uncertainty maps.}
	\label{fig:celeba_inpainting_figure}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{plots/increcon_imagenet_blur_figure.pdf}
	\caption{Distortion and Perception optimized deblurring results for the ImageNet dataset (test split). Uncertainty is calculated over $n=10$ reconstructions from the same measurement.}
	\label{fig:imagenet_blur_figure}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.85\linewidth]{plots/increcon_imagenet_inpainting_figure.pdf}
	\caption{Distortion and Perception optimized inpainting results for the ImageNet dataset (test split). Uncertainty is calculated over $n=10$ reconstructions from the same measurement. For distortion optimized runs, images are generated in one-shot, hence we don't provide uncertainty maps.}
	\label{fig:imagenet_inpainting_figure}
\end{figure}