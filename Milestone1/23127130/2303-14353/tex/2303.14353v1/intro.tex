\section{Introduction}
Diffusion models are powerful generative models capable of synthesizing samples of exceptional quality by reversing a diffusion process that gradually corrupts a clean image by adding Gaussian noise. Diffusion models have been explored from two perspectives: Denoising Diffusion Probabilistic Models (DDPM) \cite{sohl2015deep, ho_denoising_2020} and Score-Based Models \cite{song_generative_2020, song_improved_2020}, which have been recently unified under a general framework of Stochastic Differential Equations (SDEs) \cite{song2020score}.  Diffusion models have established new state of the art in image generation \cite{dhariwal_diffusion_2021, nichol2021glide, saharia2022photorealistic, ramesh2022hierarchical, rombach2022high,  ho2022cascaded, saharia2022palette}, audio \cite{kong2020diffwave} and video synthesis \cite{ho2022video}, and recently have been deployed for solving inverse problems with great success. 

In inverse problems, one wishes to recover a signal $\vct{x}$ from a noisy observation $\vct{y}$ connected via the forward model $\mathcal{A}$ and measurement noise $\vct{z}$ in the form $\vct{y} = \fwd{}{\vct{x}} + \vct{z}$. As $\mathcal{A}$ is typically non-invertible, prior knowledge on $x$ has been incorporated in a variety of ways including sparsity-inducing regularizers \cite{candes_stable_2006, donoho_compressed_2006}, plug-and-play priors \cite{venkatakrishnan2013plug} and learned deep learning priors \cite{ongie_deep_2020}. Diffusion models learn a prior over the data distribution by matching the gradient of the log density (Stein-score). The unconditional score function learned by diffusion models has been successfully leveraged to solve inverse problems without any task-specific training \cite{kadkhodaie2021stochastic, jalal2021robust, saharia_image_2021}. However, as the score of the posterior distribution is intractable in general, different methods have been proposed to enforce consistency between the generated image and the corresponding observations. These methods include alternating between a step of unconditional update and a step of projection\cite{song2021solving, chung2022score, chung2022come} or other correction techniques \cite{chung2022diffusion, chung2022improving} to guide the diffusion process towards data consistency. Another line of work proposes diffusion in the spectral space of the forward operator, achieving high quality reconstructions, however requires costly singular value decomposition \cite{kawar2021snips, kawar2022denoising, kawar2022jpeg}. Concurrent work uses pseudo-inverse guidance \cite{songpseudoinverse} to incorporate the model into the reconstruction process. All of these methods utilize a score function learned from denoising score-matching on a standard diffusion process that simply adds Gaussian noise to clean images, and therefore the specific corruption model of the inverse problem is not incorporated into model training directly.

There has been a recent push to broaden the notion of Gaussian diffusion, such as extension to other noise distributions \cite{deasy2021heavy, nachmani2021denoising, okhotin2023star}. In the context of image generation, there has been work to generalize the corruption process, such as blur diffusion \cite{lee2022progressive, hoogeboom2022blurring}, inverse heat dissipation \cite{rissanen2022generative} and  arbitrary linear corruptions \cite{daras2022soft}. \cite{bansal2022cold} questions the necessity of stochasticity in the generative process all together and demonstrates empirical results on noiseless corruptions with arbitrary deterministic degradations. However, designing the diffusion process specifically for inverse problem solving has not been explored extensively yet.  A recent example is \cite{welker2022driftrec} proposing adding an additional drift term to the forward SDE that pulls the iterates towards the corrupted measurement and demonstrates high quality reconstructions for JPEG compression artifact removal. Concurrent work \cite{delbracio2023inversion} defines the forward process as convex combinations between the clean image and the corrupted observation, obtaining promising results in motion deblurring and super-resolution.

Despite all of these successes of diffusion models in high-quality image generation, the requirements imposed on inverse problems are very different from synthetic image generation. First, due to the perception-distortion trade-off \cite{blau2018perception}, as diffusion models generate images of exceptional detail, typically reconstruction methods relying on diffusion underperform in distortion metrics, such as PSNR and SSIM \cite{chung2022diffusion}, that are traditionally used to evaluate image reconstructions. Moreover, as data consistency is not always explicitly enforced during reverse diffusion, we may obtain visually appealing reconstructions, that are in fact not faithful to our original observations.  

In this paper, we propose a novel framework for solving inverse problems using a generalized notion of diffusion that mimics the corruption process that produced the observation. We call our method \methodname{}: \underline{D}enoising and \underline{I}ncremental \underline{R}econstruction with \underline{A}ssured data-\underline{C}onsistency.  As the forward model and noising process are directly incorporated into the framework, our method maintains data consistency throughout the reverse diffusion process, without any additional steps such as projections. Furthermore, we make the key observation that details are gradually added to the posterior mean estimates during the sampling process. This property imbues our method with great flexibility: by leveraging early-stopping we can freely trade off perceptual quality for better distortion metrics and sampling speedup or vice versa. We provide theoretical analysis on the accuracy, performance and limitations of our method that are well-supported by empirical results. Our numerical experiments demonstrate state-of-the-art results in terms of both perceptual and distortion metrics with fast sampling. 