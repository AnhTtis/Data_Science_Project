@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})
% DL survey
@article{dong2021survey,
  title={A survey on deep learning and its applications},
  author={Dong, Shi and Wang, Ping and Abbas, Khushnood},
  journal={Computer Science Review},
  volume={40},
  pages={100379},
  year={2021},
  publisher={Elsevier}
}
% KD survey
@article{wang2021knowledge,
  title={Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks},
  author={Wang, Lin and Yoon, Kuk-Jin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  publisher={IEEE}
}
% KD is regularization
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}
% dml
@inproceedings{zhang2018deep,
  title={Deep mutual learning},
  author={Zhang, Ying and Xiang, Tao and Hospedales, Timothy M and Lu, Huchuan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4320--4328},
  year={2018}
}
% kdcl
@inproceedings{guo2020online,
  title={Online knowledge distillation via collaborative learning},
  author={Guo, Qiushan and Wang, Xinjiang and Wu, Yichao and Yu, Zhipeng and Liang, Ding and Hu, Xiaolin and Luo, Ping},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11020--11029},
  year={2020}
}
% BAN
@inproceedings{furlanello2018born,
  title={Born again neural networks},
  author={Furlanello, Tommaso and Lipton, Zachary and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  booktitle={International Conference on Machine Learning},
  pages={1607--1616},
  year={2018},
  organization={PMLR}
}
% ONE
@article{zhu2018knowledge,
  title={Knowledge distillation by on-the-fly native ensemble},
  author={Zhu, Xiatian and Gong, Shaogang and others},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
% PCL
@inproceedings{wu2021peer,
  title={Peer collaborative learning for online knowledge distillation},
  author={Wu, Guile and Gong, Shaogang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={12},
  pages={10302--10310},
  year={2021}
}
% OKDDip
@inproceedings{chen2020online,
  title={Online knowledge distillation with diverse peers},
  author={Chen, Defang and Mei, Jian-Ping and Wang, Can and Feng, Yan and Chen, Chun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={3430--3437},
  year={2020}
}
% FFL
@inproceedings{kim2021feature,
  title={Feature fusion for online mutual knowledge distillation},
  author={Kim, Jangho and Hyun, Minsung and Chung, Inseop and Kwak, Nojun},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={4619--4625},
  year={2021},
  organization={IEEE}
}
% ODCC
@article{xie2019training,
  title={Training convolutional neural networks with cheap convolutions and online distillation},
  author={Xie, Jiao and Lin, Shaohui and Zhang, Yichen and Luo, Linkai},
  journal={arXiv preprint arXiv:1909.13063},
  year={2019}
}
% AFID aggregate predictions
@inproceedings{su2021attention,
  title={Attention-based Feature Interaction for Efficient Online Knowledge Distillation},
  author={Su, Tongtong and Liang, Qiyu and Zhang, Jinsong and Yu, Zhaoyang and Wang, Gang and Liu, Xiaoguang},
  booktitle={2021 IEEE International Conference on Data Mining (ICDM)},
  pages={579--588},
  year={2021},
  organization={IEEE}
}
@inproceedings{wang2021adaptable,
  title={Adaptable ensemble distillation},
  author={Wang, Yankai and Yang, Dawei and Zhang, Wei and Jiang, Zhe and Zhang, Wenqiang},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1675--1679},
  year={2021},
  organization={IEEE}
}
% combine features
@article{li2022embedded,
  title={Embedded mutual learning: A novel online distillation method integrating diverse knowledge sources},
  author={Li, Chuanxiu and Li, Guangli and Zhang, Hongbin and Ji, Donghong},
  journal={Applied Intelligence},
  pages={1--14},
  year={2022},
  publisher={Springer}
}
@inproceedings{phuong2019distillation,
  title={Distillation-based training for multi-exit architectures},
  author={Phuong, Mary and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1355--1364},
  year={2019}
}
% work with peers
@inproceedings{zhu2022online,
  title={Online Knowledge Distillation with Multi-Architecture Peers},
  author={Zhu, Xuan and Yao, Wangshu and Song, Kang},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2022},
  organization={IEEE}
}

% Model Soup
@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International Conference on Machine Learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}
% Git re-basin
@article{ainsworth2022git,
  title={Git re-basin: Merging models modulo permutation symmetries},
  author={Ainsworth, Samuel K and Hayase, Jonathan and Srinivasa, Siddhartha},
  journal={arXiv preprint arXiv:2209.04836},
  year={2022}
}
% Model Fusion
@article{singh2020model,
  title={Model fusion via optimal transport},
  author={Singh, Sidak Pal and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22045--22055},
  year={2020}
}
% LMC
@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}
% Loss surfaces, ensembling of dnns
@article{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
% Loss landscape
@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
% Essentially no barriers in neural network energy landscape
@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}
% The role of permutation invariance in linear mode connectivity of neural networks
@article{entezari2021role,
  title={The role of permutation invariance in linear mode connectivity of neural networks},
  author={Entezari, Rahim and Sedghi, Hanie and Saukh, Olga and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2110.06296},
  year={2021}
}
% What is being transferred in transfer learning
@article{neyshabur2020being,
  title={What is being transferred in transfer learning?},
  author={Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={512--523},
  year={2020}
}
% Flattness of basin
@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}
@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
% PAC-Bayes
@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={arXiv preprint arXiv:1703.11008},
  year={2017}
}
% 泛化与noisy有关
@article{langford2001not,
  title={(Not) bounding the true error},
  author={Langford, John and Caruana, Rich},
  journal={Advances in Neural Information Processing Systems},
  volume={14},
  year={2001}
}
% 泛化性 最小描述长度
@inproceedings{hinton1993keeping,
  title={Keeping the neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey E and Van Camp, Drew},
  booktitle={Proceedings of the sixth annual conference on Computational learning theory},
  pages={5--13},
  year={1993}
}
% PAC-BAYES
@inproceedings{mcallester1999pac,
  title={PAC-Bayesian model averaging},
  author={McAllester, David A},
  booktitle={Proceedings of the twelfth annual conference on Computational learning theory},
  pages={164--170},
  year={1999}
}
% PAC-Bayesian 
@article{catoni2007pac,
  title={PAC-Bayesian supervised classification: the thermodynamics of statistical learning},
  author={Catoni, Olivier},
  journal={arXiv preprint arXiv:0712.0248},
  year={2007}
}
% Spectrally margin
@article{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
% norm capacity
@inproceedings{neyshabur2015norm,
  title={Norm-based capacity control in neural networks},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={1376--1401},
  year={2015},
  organization={PMLR}
}
% PAC-Bayesian
@inproceedings{mcallester1998some,
  title={Some pac-bayesian theorems},
  author={McAllester, David A},
  booktitle={Proceedings of the eleventh annual conference on Computational learning theory},
  pages={230--234},
  year={1998}
}
% over parameter
@article{li2018over,
  title={Over-parameterized deep neural networks have no strict local minima for any continuous activations},
  author={Li, Dawei and Ding, Tian and Sun, Ruoyu},
  journal={arXiv preprint arXiv:1812.11039},
  year={2018}
}
@article{liu2022loss,
  title={Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={Applied and Computational Harmonic Analysis},
  volume={59},
  pages={85--116},
  year={2022},
  publisher={Elsevier}
}

% cifar 10 cifar 100
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}
% imagenet
@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}
% Resnet
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
% WRN
@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}
% VGG
@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
% DenseNet
@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}
% PCA
@article{mackiewicz1993principal,
  title={Principal components analysis (PCA)},
  author={Ma{\'c}kiewicz, Andrzej and Ratajczak, Waldemar},
  journal={Computers \& Geosciences},
  volume={19},
  number={3},
  pages={303--342},
  year={1993},
  publisher={Elsevier}
}
% self distillation is regularization
@article{mobahi2020self,
  title={Self-distillation amplifies regularization in hilbert space},
  author={Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3351--3361},
  year={2020}
}
@article{zhang2020self,
  title={Self-distillation as instance-specific label smoothing},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2184--2195},
  year={2020}
}

% pytorch
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
% cutout
@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}
% RandAugment
@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={702--703},
  year={2020}
}
% auto augment
@inproceedings{cubuk2019autoaugment,
  title={Autoaugment: Learning augmentation strategies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={113--123},
  year={2019}
}
% SGD
@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013},
  organization={PMLR}
}
% Dir
@article{lda,
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},  doi = {http://dx.doi.org/10.1162/jmlr.2003.3.4-5.993},
  interhash = {9d1b808272b9e511425cbf557571e59a},
  intrahash = {bc34cc810fa7dfa12b949b60c23d9f5c},
  issn = {1532-4435},
  journal = {J. Mach. Learn. Res.},
  keywords = {LDA allocation dirichlet latent},
  pages = {993--1022},
  publisher = {JMLR.org},
  timestamp = {2010-06-16T11:05:42.000+0200},
  title = {Latent dirichlet allocation},
  url = {http://portal.acm.org/citation.cfm?id=944937},
  volume = 3,
  year = 2003
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

%% Parametr fusion
% swa
@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}
% EMA
@article{tarvainen2017mean,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
% SAM
@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}
% KR
@inproceedings{ding2021knowledge,
  title={Knowledge refinery: Learning from decoupled label},
  author={Ding, Qianggang and Wu, Sifan and Dai, Tao and Sun, Hao and Guo, Jiadong and Fu, Zhang-Hua and Xia, Shutao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={8},
  pages={7228--7235},
  year={2021}
}

%% Jing

@inproceedings{jing2021amalgamating,
  title={Amalgamating knowledge from heterogeneous graph neural networks},
  author={Jing, Yongcheng and Yang, Yiding and Wang, Xinchao and Song, Mingli and Tao, Dacheng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={15709--15718},
  year={2021}
}

@article{yang2022deep,
  title={Deep model reassembly},
  author={Yang, Xingyi and Daquan, Zhou and Liu, Songhua and Ye, Jingwen and Wang, Xinchao},
  journal={arXiv preprint arXiv:2210.17409},
  year={2022}
}

@inproceedings{yang2022factorizing,
  title={Factorizing knowledge in neural networks},
  author={Yang, Xingyi and Ye, Jingwen and Wang, Xinchao},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXIV},
  pages={73--91},
  year={2022},
  organization={Springer}
}

@article{xue2021kdexplainer,
  title={Kdexplainer: A task-oriented attention model for explaining knowledge distillation},
  author={Xue, Mengqi and Song, Jie and Wang, Xinchao and Chen, Ying and Wang, Xingen and Song, Mingli},
  journal={arXiv preprint arXiv:2105.04181},
  year={2021}
}

@inproceedings{shen2019customizing,
  title={Customizing student networks from heterogeneous teachers via adaptive knowledge amalgamation},
  author={Shen, Chengchao and Xue, Mengqi and Wang, Xinchao and Song, Jie and Sun, Li and Song, Mingli},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3504--3513},
  year={2019}
}

@inproceedings{song2021tree,
  title={Tree-like decision distillation},
  author={Song, Jie and Zhang, Haofei and Wang, Xinchao and Xue, Mengqi and Chen, Ying and Sun, Li and Tao, Dacheng and Song, Mingli},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13488--13497},
  year={2021}
}