\section{Deep Neural Level Set Algorithms}
\label{sec:algo}

As depicted in \Cref{fig:develset_flow}, the proposed DevelSet framework consists of two parts, DevelSet-Optimizer (DSO) and DevelSet-Net (DSN).
In this section, we will first introduce the improved level set-based ILT algorithm of DSO in \Cref{subsec:dso} applying curvature term to improve the mask manufacturability,
along with the full implementation in CUDA platform by combining the mechanism of GPU parallelism with the numerical setting of level set.
% Then the novel deep neural network architecture carefully designed for level set methods to achieve instant mask optimization in \Cref{subsec:dsn}.
% \move{DSN predicts LSF $\phi_\theta$ as a better initialization for DSO.}
Then a novel multi-branch neural network \ie DSN is proposed in \Cref{subsec:dsn} to: \textit{1)} provide a better initial LSF for DSO to reduce the total iterations,
\textit{2)} predict a weighted matrix to selectively regularize the mask boundary and compensate for mask printability loss caused by the curvature term.
Finally, we perform the end-to-end joint optimization for DevelSet in \Cref{subsec:develset} to accomplish instant mask optimization with higher mask printability and lower mask complexity.

\ifshowfig
\begin{figure*}[tb!]
  \centering
  \includegraphics[width=.99\linewidth]{network}
  \caption{Overview of DevelSet framework with the end-to-end joint optimization flow of DSN and DSO.}
  \label{fig:develset_flow}
\end{figure*}
\fi


\subsection{DevelSet-Optimizer~(DSO)}
\label{subsec:dso}
The objective of the conventional ILT-based OPC method is to find an optimized mask $\mathbf{M}_\mathrm{opt} = \mathcal{L}^{-1}(\mathbf{Z}_\mathrm{t};\mathbf{P}_\mathrm{nom})$,
where $\mathbf{Z}_\mathrm{t}$ is the design target and the $\mathcal{L}\left(\cdot ; \mathbf{P}_{\mathrm {nom }}\right)$ denotes the forward lithography process under the nominal condition.
The pixel-based ILT methods represent the intensity wafer image as the pixel-wise parameters and then update the mask pixel-by-pixel with the guidance of the inverse gradient from the lithography model.
While the level set-based ILT methods regard the optimization process as the evolution of the level set continuum, the mask is formulated by the cross-section of zero height plane and the level set continuum.
Mathematically, the level set continuum is represented with LSF $\phi$, the evolution procedure can be expressed by \Cref{eq:levelset_evolution}.
The SOTA pixel-based method Neural-ILT~\cite{NEURAL-ILT-ICCAD2020-Jiang} brings the ILT to the on-neural-network training solution with a CUDA accelerated lithography simulator, achieving a breakthrough in runtime boosting.
It applies a mask complexity refinement layer and domain knowledge re-training to eliminate the complex shape in masks thus reducing mask complexity.
Nonetheless, Neural-ILT sacrifices mask printability that the sum of $L_2$ and PVBand is worse than the previous learning-based work PGAN-OPC~\cite{OPC-TCAD2020-Yang}.
The SOTA level set-based method GLS-ILT~\cite{OPC-TR2020-Yu} leveraged GPU accelerated FFT to speed up the momentum-based algorithm.
However, there is still room for acceleration since GLS-ILT only accelerates the FFT module and much runtime is wasted on CPU and GPU transfers.


DSO is a CUDA accelerated iterative mask optimizer applying our improved level set algorithm.
By controlling the curvature term and transferring all the computations to the GPU, DSO obtains mask printability superiority and breakthrough in runtime performance simultaneously.
As revealed in \Cref{eq:levelset_evolution}, the key to leverage level set methods in mask optimization tasks is the definition of the LSF $\phi$ and the velocity term $v$.
In DSO, we carefully design the LSF $\phi$, and then integrate the ILT-based gradient as the velocity term $v$.
Moreover, we introduce the curvature term to improve mask printability and reduce mask complexity.


% However, since different masks may yeild same wafer result, the ILT-based OPC is an ill-posed problem.

% \subsubsection{Algorithm behind the DSO}
\subsubsection{The Improved Level Set-Based ILT}

\textbf{Truncated signed distance function}.
The theoretical framework of the level set method is independent of which particular LSF $\phi$ is used.
A popular type of LSF is the signed distance function (SDF) in~\Cref{eq:sdf}.
SDF is Lipschitz continuous and from Rademacher's theorem, it is almost everywhere differentiable.
However, instead of using SDF, we adopt truncated signed distance function (TSDF)
as our LSF with an upper bound $D_u$ and a lower bound $D_l$,
\begin{equation}
    \label{eq:tsdf}
    \phi_{\mathrm{TSDF}} =\left\{\begin{array}{ll}
        D_{u}, & \mathrm{if} \ \phi_{\mathrm{SDF}} > D_u, \\
        \phi_{\mathrm{SDF}}, & \mathrm{if} \  D_{l} \leq \phi_{\mathrm{SDF}} \leq D_u, \\
        D_l , & \mathrm {if} \ \phi_{\mathrm{SDF}} < D_l.
    \end{array}\right.
\end{equation}
In our work, we set $D_u$ to $900$ and $D_l$ to $-100$ according to the design rules of the benchmark.
The TSDF improves the stability of optimization procedure by reducing the variance of the dataset.
More importantly, the TSDF ensures the fast convergence of the DevelSet-Net to enable the end-to-end joint optimization of the whole mask optimization framework.

\textbf{Motion term}.
Motion term  $\frac{\partial\phi}{\partial t}$ is the core component in level set methods determining the evolution process.
According to \Cref{eq:phi_v}, it is dominated by the velocity term $v$.
In our work, the velocity is the gradient back-propagated from the forward lithography simulation process of the partial coherent imaging system.
All the techniques used in the previous pixel-based framework can be seamlessly migrated to the DevelSet framework,
for DSO provides a proxy that allows us to perfectly control the gradient near the mask boundary.
The objective function of DSO consists of the commonly used ILT loss and the PVBand loss,
\begin{equation}
  L_{\mathrm{DSO}} =  \alpha L_{\mathrm{ilt}} + \beta L_{\mathrm{pvb}}.
\end{equation}
\par
To minimize the image difference of target image and nominal image, the ILT loss is given by
\begin{equation}
  L_{\mathrm{ilt}}=\sum_{x=1}^{N} \sum_{y=1}^{N}\left(\mathbf{Z}(x, y)-\mathbf{Z}_{\mathrm{t}}(x, y)\right)^{2},
\end{equation}
where the $\mathbf{Z}_t$ is the target image; $\mathbf{Z}$ is the wafer image after the lithography under the nominal condition; N is the width of the target image.
To enable the evolution process differentiable, the step function in \Cref{eq:binary_intensity} is approximated as
\begin{equation}
  \mathbf{Z} = \frac{1}{1+\exp \left(-\sigma_{z} \times\left(\mathbf{I}-I_{t h}\right)\right)},
\end{equation}
where $\sigma_z$ is the steepness of the sigmoid function. Then the gradient of ILT loss can be expressed as
% \begin{small}
\begin{equation}
  \label{eq:ilt_gradient}
  \begin{aligned}
  &\frac{\partial L_{\mathrm{ilt}}}{\partial \mathbf{M}}= 2 \times\left(\mathbf{Z}-\mathbf{Z}_{\mathrm{t}}\right) \odot \frac{\partial \mathbf{Z}}{\partial \mathbf{M}} \\
  &=2\sigma_{z} \times\left\{\mathbf{H}^\mathrm{\prime} \otimes\left[\left(\mathbf{Z}-\mathbf{Z}_{\mathrm{t}}\right) \odot \mathbf{Z} \odot(1-\mathbf{Z}) \odot\left(\mathbf{M} \otimes \mathbf{H}^{*}\right)\right]\right.\\
  &\left.+(\mathbf{H}^{\mathrm{\prime}})^{*} \otimes\left[\left(\mathbf{Z}-\mathbf{Z}_{\mathrm{t}}\right) \odot \mathbf{Z} \odot(1-\mathbf{Z}) \odot(\mathbf{M} \otimes \mathbf{H})\right]\right\} ,
  \end{aligned}
\end{equation}
where the $\mathbf{H}^{\prime}$ is the $180^{\circ}$ rotation of the optical kernel set $\mathbf{H}$, and the $\mathbf{H}^{*}$ is the conjugate of $\mathbf{H}$.
% \end{small}


% To improve mask printability,
To minimize the area of PVBand, we expect the innermost/outermost wafer under min/max process conditions as close to the target image as possible.
The PVBand loss is given by,
% \begin{equation}
%   \begin{aligned}
%   L_{\mathrm{pvb}}  =& \sum_{x=1}^{N} \sum_{y=1}^{N}\left(\mathbf{Z}_\mathrm{in}(x, y)-\mathbf{Z}_{\mathrm{t}}(x, y)\right)^{2} \\
%                     +&  \sum_{x=1}^{N} \sum_{y=1}^{N}\left(\mathbf{Z}_\mathrm{out}(x, y)-\mathbf{Z}_{\mathrm{t}}(x, y)\right)^{2}
%   \end{aligned}
% \end{equation}
\begin{equation}
  L_{\mathrm{pvb}}  = \left(\mathbf{Z}_\mathrm{in}-\mathbf{Z}_{\mathrm{t}}\right)^{2} + \left(\mathbf{Z}_\mathrm{out}-\mathbf{Z}_{\mathrm{t}}\right)^{2} .\\
\end{equation}
The gradient of PVBand loss can be represented as
% \begin{small}
\begin{equation}
  \label{eq:pvb_gradient}
  \begin{aligned}
    \frac{\partial L_{\mathrm{pvb}}}{\partial \mathbf{M}} =& 2 \times\left(\mathbf{Z}_{\mathrm{in}}-\mathbf{Z}_{\mathrm{t}}\right)\odot\frac{\partial \mathbf{Z}_\mathrm{in}}{\partial \mathbf{M}} \\
                                                          +& 2 \times\left(\mathbf{Z}_{\mathrm{out}}-\mathbf{Z}_{\mathrm{t}}\right)\odot\frac{\partial \mathbf{Z}_\mathrm{out}}{\partial \mathbf{M}}.
  \end{aligned}
\end{equation}
The detailed derivation of \Cref{eq:pvb_gradient} is similar to \Cref{eq:ilt_gradient}.
% \begin{equation}
%   \frac{\partial L_{\mathrm{pvb}}}{\partial \mathbf{M}}=2 \times\left(\mathbf{Z}_{\mathrm{in}}-\mathbf{Z}_{\mathrm{out}}\right) \odot\left(\mathbf{Z}_{\mathrm{in}}^{\prime}-\mathbf{Z}_{\mathrm{out}}^{\prime}\right)
% \end{equation}
% \end{small}
% Where $\mathbf{Z}^{\prime}_\mathrm{in}$ is given by:
% % \begin{small}
% \begin{equation*}
%   \begin{aligned}
%   \mathrm{Z}_{\mathrm{in}}^{\prime} =& \theta_{Z} \times\left\{\mathbf{H}_{\mathrm{def}}^{\mathrm{flip}} \otimes\left[\mathbf{Z}_{\mathrm{in }} \odot\left(1-\mathbf{Z}_{\mathrm{in}}\right) \odot\left(\mathbf{M} \otimes \mathbf{H}_{\mathrm{def}}^{*}\right)\right]\right. \\
%   &+\left. (\mathbf{H}_{\mathrm{def}}^{\mathrm{flip}})^{*} \otimes\left[\mathbf{Z}_{\mathrm{in}} \odot\left(1-\mathbf{Z}_{\mathrm{in}}\right) \odot\left(\mathbf{M} \otimes \mathbf{H}_{\mathrm{def}}\right)\right]\right\}
%   \end{aligned}
% \end{equation*}
% % \end{small}
Now the velocity $v$ is
\begin{equation}
  \label{eq:vel}
  v = \alpha \frac{\partial L_{\mathrm{ilt}}}{\partial \mathbf{M}} + \beta \frac{\partial L_{\mathrm{pvb}}}{\partial \mathbf{M}} .
\end{equation}
And the motion equation is finally derived as
\begin{equation}
  \label{eq:phi_v_grad}
  \frac{\partial \phi_{i}}{\partial t}= -(\alpha \frac{\partial L_{\mathrm{ilt}}}{\partial \mathbf{M}} + \beta \frac{\partial L_{\mathrm{pvb}}}{\partial \mathbf{M}})|\nabla\phi_i|.
\end{equation}


\textbf{Curvature term}.
As revealed in \Cref{eq:levelset_evolution}, the evolution manner of level set is defined by several updating terms,
which can be roughly divided into two categories :
(1) external terms that attract the curve to the desired location-based on the data evidence, such as the inverse lithography gradient or the optimization methods,
and (2) internal regularization terms on the curve shape, \textit{e.g.} curvature and the length of the curvature.
Previous level set-based methods focus on the improvements of the external terms since the internal term such as curvature requires extensive calculations to get the second-order derivatives.
However, with GPU acceleration, DSO takes the maximum advantage of the effective feature in the implicit representation to obtain the curvature of the boundaries,
which is practical to control the smoothness of the front and eliminate the noise points on the mask pattern.
The curvature term is formally defined as
\begin{equation}
  \label{eq:curvature_term}
  \kappa = \lambda \mathbf{m}_\theta \left|\nabla \phi_i\right| \operatorname{div}\left(\frac{\nabla \phi_i}{\left| \nabla \phi_i \right|}\right),
\end{equation}
where $\lambda$ is the curvature weight. However, in OPC tasks,
there may exist sharp corners in some parts of the masks.
Directly apply curvature term on the level set evolution process may harm the lithography results.
Thus, we add a weighted matrix $\mathbf{m}_\theta$ to control the curvature term,
and the subscript $\theta$ denotes the $\mathbf{m}_\theta$ is predicted by the modulation branch parameters of DSN.
% The $\mathbf{m}_\theta$ has the same dimensions as the mask image $\mathbf{M}$ which will be predicted by the modulation branch of DSN.
We will introduce the modulation branch in \Cref{subsubsec:dsn_arch} detailedly.
% In most cases, high curvature should be penalized to reduce mask complexity.
Then the level set evolution of DSO can be described as the sum of the motion term and curvature term,
% \begin{small}
\begin{equation}
  \label{eq:dso_evolution}
  \begin{aligned}
  \frac{\partial \phi_i}{\partial t} =& -(\alpha \frac{\partial L_{\mathrm{ilt}}}{\partial \mathbf{M}} + \beta \frac{\partial L_{\mathrm{pvb}}}{\partial \mathbf{M}})|\nabla\phi_i| \\
  &+  \lambda \mathbf{m}_\theta \left|\nabla \phi_i\right| \operatorname{div}\left(\frac{\nabla \phi_i}{\left| \nabla \phi_i \right|}\right) .
  \end{aligned}
\end{equation}

\subsubsection{The CUDA Implementation of DSO}

Conventional ILT-based mask optimization methods suffer from severe computational overhead,
and the situation grows worse in level set-based methods.
When the new terms are leveraged to improve mask printability,
the new computational cost is also introduced to the already burdensome computation system.
Hence, the major challenge for DSO is to overcome the drawback of high computational effort.
% With the rapid development of GPU and parallel computing
% The rapid development of general-purpose graphic process unit (GPGPU) computing technology
% has resulted in continuous improvement in GPU parallel computing performance.
By implementing the entire DSO framework on the CUDA platform,
we find a way to balance efficiency and performance.
Next, we will detail the CUDA implementation of each term in level set algorithm,
as well as engineering tricks to make our DSO framework significantly faster.

% \noindent\textbf{Target-specific truncated signed distance function.}
% \noindent\textbf{CUDA-based TSDF.}
\textbf{Numerical settings}.
The level set-based mask optimization methods focus on 2D situation with an image as the input.
The space is discretized by a Cartesian grid with steps $\Delta x, \Delta y$, where the coordinates $(x, y)$ represent the $x^{th}, y^{th}$ pixel in the image.
% \Cref{eq:dso_evolution} is a PDE
The first-order derivatives in space and time of \Cref{eq:dso_evolution} can be approximated using finite difference techniques.
We apply weighted essential nonoscillatory (WENO)~\cite{HARTEN1987231} numerical polynomial interpolation method that uses the smoothest possible polynomial interpolation to find $\phi$.
And the first-order and second-order spatial derivatives of $\phi$ can be represented with central differences as
% \begin{small}
\begin{equation}
  \label{eq:numerical-settings}
  \begin{aligned}
  \nabla \phi_{x} =& \frac{1}{2}(\phi(x+1,\ y)-\phi(x-1,\ y)), \\
  \nabla \phi_{y} =& \frac{1}{2}(\phi(x,\ y+1)-\phi(x,\ y-1)), \\
  % \nabla \phi_{xx} =& (\phi(x+1, y)-\phi(x, y))-(\phi(x, y)-\phi(x-1, y)) \\
  \nabla \phi_{xx} =& \phi(x+1,\ y) + \phi(x-1,\ y) - 2 \times \phi(x,\ y), \\
  % \nabla \phi_{yy} =& (\phi(x, y+1)-\phi(x, y))-(\phi(x, y)-\phi(x, y-1)) \\
  \nabla \phi_{yy} =& \phi(x,\ y+1) + \phi(x,\ y-1) - 2 \times \phi(x,\ y), \\
  \nabla \phi_{xy} =& \frac{1}{4}\ [(\phi(x+1,\ y+1)-\phi(x-1,\ y+1)) \\
  &-(\phi(x+1,\ y-1)-\phi(x-1,\ y-1))],
  \end{aligned}
\end{equation}
% \end{small}
and the curvature term is then computed numerically with
% \begin{small}
\begin{equation}
  \label{eq:curva_numer}
  \begin{aligned}
    \kappa &= \lambda \mathbf{m}_\theta \left|\nabla \phi_i\right| \operatorname{div}\left(\frac{\nabla \phi_i}{\left| \nabla \phi_i \right|}\right) \\
          &= \lambda \mathbf{m}_\theta \frac{\nabla\phi_{x x} {\nabla\phi_{y}}^{2}-2 \nabla\phi_{y} \nabla\phi_{x} \nabla\phi_{x y}+ \nabla\phi_{y y} {\nabla\phi_{x}}^{2}}{{\nabla\phi_{x}}^{2}+ {\nabla\phi_{y}}^{2}} .
  \end{aligned}
\end{equation}


\textbf{CUDA-based TSDF}.
The first tremendous challenge is to calculate the truncated signed distance function (TSDF) on a given target image (2048 $\times$ 2048), in an extremely short period.
The most celebrated method to calculate signed distance function is the Fast Marching Method introduced by \cite{sethian1996fast}.
Instead of using Fast Marching Method, we have specially designed the TSDF algorithm based on the characteristics of CUDA parallelism.
In DSO, we use the target pattern as the initial mask.
The first step focuses on extracting the boundary segments and calculating the distance towards the boundary using the \texttt{CUDA\_TSDF} function in \Cref{alg:parallel-ls}.
We apply pixel-wise \texttt{Shift} and \texttt{XOR} operation to obtain the mask boundary lines $b_h$, $b_v$ (line 2-5).
% In lines 2 and 3 target $\mathbf{Z}_\mathrm{t}$ is translated upwards, downwards, leftwards, rightwards by 1 pixel to get $\mathbf{Z}_{tu}$, $\mathbf{Z}_{td}$, $\mathbf{Z}_{tl}$, $\mathbf{Z}_{tr}$;
% In lines 4 and 5 we calculate the \texttt{XOR} region between $\mathbf{Z}_\mathrm{t}$ and $\mathbf{Z}_{tu}$, $\mathbf{Z}_{td}$, $\mathbf{Z}_{tl}$, $\mathbf{Z}_{tr}$ parallel to get horizontal and vertical boundary line $b_h$, $b_v$;
Then for all pixels $p$ on mask plates we calculate the distance towards all boundary lines and select the minimum distance for each point in parallel.
Finally, we apply the \Cref{eq:tsdf} to generate the truncated signed distance function (line 6-10).
For a complicated mask generated from the neural network, experimental result shows the \texttt{CUDA\_TSDF} can achieve more than 98\% reduction in TSDF calculation time.
% reduces TSDF calculation time from 20 seconds to 0.2 seconds.
% The bottleneck of the \texttt{CUDA\_TSDF} is line 7, here we need to calculate the distance from each pixel to each boundary

% All the operations in function \texttt{CUDA\_TSDF} are
% which not only reduces the total runtime it possible to integrate level set evolution into neural network.

% if the distance is larger than the truncated threshold, it is set to the threshold value.
% \noindent\textbf{CUDA-based geometry gradient and curvature term.}
% \pComparison of pixel-based aragraph{}


\textbf{CUDA-based geometry gradient and curvature term}.
As demonstrated in \Cref{eq:numerical-settings}, the numerical settings are well compatible with CUDA parallelism.
The spatial derivatives of $\phi$ are calculated in function \texttt{CUDA\_geometry\_gradient} of \Cref{alg:parallel-ls}.
And the curvature term can be calculated with the function \texttt{CUDA\_curvature} with GPU acceleration.
All the operations in \Cref{alg:parallel-ls} such as \texttt{shift} and \texttt{XOR} are pixel-wise independent and can be parallelly performed per pixel per thread,
which not only reduces the total runtime of the DSO but also makes it possible to integrate the level set evolution into neural network.
% We apply pixel-wise 
% we calculate the geometric gradient of the TSDF $\phi$ as is shown in \Cref{alg:parallel-ls}.
% First we translate the $\phi_\mathrm{TSDF}$ upwards, downwards, leftwards, rightwards by 1 pixel to get $\phi_{u}$, $\phi_{d}$, $\phi_{l}$, $\phi_{r}$,
% as is described in line 13, 14.
% In line 15 and 16, we use the central difference method to calculate the gradient in $x$ and $y$ direction respectively.

% In \texttt{CUDA\_curvature} function, we use the TSDF $\phi$ and geometric gradient $\nabla \phi_x$, $\nabla \phi_y$ as the input.
% In line 18, we calculate the second order derivatives using graient\_geometry function and calculated first-order gradient term $\nabla \phi_x$, $\nabla \phi_y$.
% In line 19, we shift the $D$ towards 4 diagonal directions by 1 pixel.
% The second order gradient term $\nabla \phi_{xy}$ is calculated using central difference method in line 20.
% The curvature term $k$ is finally given in line 21.


\textbf{CUDA accelerated lithography simulation}.
% In ILT-based mask optimization process, the lithography simulation will be conducted iteratively to guide the mask contour evolves.
According to the previous experimental analysis,
lithography simulation is the most time-consuming part of the mask optimization flow,
since it involves plenty of convolution operations between different kernels and the mask images.
% To maximize the hardware resource utilization and improving the computational efficiency,
% Previous work Neural-ILT~\cite{NEURAL-ILT-ICCAD2020-Jiang} transplant the lithography toolset of ICCAD 2013 contest onto GPU and develop a high-performance CUDA-based lithography simulation tool\cite{OPC-ICCAD2013-Banerjee}.
Inspired by Neural-ILT~\cite{NEURAL-ILT-ICCAD2020-Jiang}, we implement our CUDA accelerated lithography simulator and integrate the forward and backward functionalities into the popular machine learning framework \texttt{PyTorch}, with some engineering improvements.
First, the optical kernels and corresponding weights are loaded and pinned in GPU memory throughout the optimization process, all the computations are performed on GPU to reduce the data transfer time from CPU to GPU.
Second, the runtime bottleneck of the CUDA lithography simulator lies on the \texttt{CUDA\_FFT} and \texttt{CUDA\_IFFT} operators.
Our improved \texttt{CUDA\_FFT} operator runs faster than the commonly used \texttt{cuFFT} and the \texttt{torch.fft} libraries.


% So far, the improved level set algorithm with fully CUDA implementation of DSO has been thoroughly introduced.
% Experimental results show that DSO achieves 3$\times$ faster than SOTA~\cite{NEURAL-ILT-ICCAD2020-Jiang}.





\begin{algorithm}[h]
  \caption{CUDA Level Set Algorithms}
  \label{alg:parallel-ls}
  % \small
  \begin{algorithmic}[1]
      \Require Target image $\mathbf{Z}_\mathrm{t}$
      \Function{CUDA\_TSDF}{$\mathbf{Z}_\mathrm{t}$}
      \State $\mathbf{Z}_{tu}, \mathbf{Z}_{td} \gets$ Shift $\mathbf{Z}_\mathrm{t}$ upwards, downwards by 1 pixel;
      \State $\mathbf{Z}_{tl}, \ \mathbf{Z}_{tr} \gets$ Shift $\mathbf{Z}_\mathrm{t}$ leftwards, rightwards by 1 pixel;
      \State $b_{h} \gets (\mathbf{Z}_\mathrm{t}$ \texttt{XOR} $\mathbf{Z}_{tu}) + (\mathbf{Z}_\mathrm{t}$ \texttt{XOR} $\mathbf{Z}_{td})$;
      \State $b_{v} \gets (\mathbf{Z}_\mathrm{t}$ \texttt{XOR} $\mathbf{Z}_{tl}\ ) + (\mathbf{Z}_\mathrm{t}$ \texttt{XOR} $\mathbf{Z}_{tr})$;
      \ForAll{pixels on target image $\mathbf{Z}_\mathrm{t}$}
      \State $d_{ij}$ $\gets$ Distance from pixel ${p}_i$ to boundary ${b}_j$;
      \State $d_i$ \ $\gets$ Minimum distance of point ${p}_i$ in all $d_{ij}$;
      \State $\phi_\mathrm{SDF}\ \ $ $\gets$ SDF matrix from all $d_i$;
      \State $\phi_\mathrm{TSDF}$ $\gets$ TSDF matrix using \Cref{eq:tsdf};
      \EndFor
      \State \Return $\phi_\mathrm{TSDF}$;
      \EndFunction
      \Ensure Truncated Signed Distance Function $\phi_\mathrm{TSDF}$;
      \Statex
      \Require TSDF matrix $\phi_\mathrm{TSDF}$;
      \Function{CUDA\_geometry\_gradient}{$\phi$}
      \State $\phi_{u}, \phi_{d} \gets$ Shift $\phi$ upwards, downwards by 1 pixel;
      \State $\phi_{l}, \ \phi_{r} \gets$ Shift $\phi$ leftwards, rightwards by 1 pixel;
      \State $\nabla \phi_x \gets (\phi_{r} - \phi_{l}) / 2$; $\ \nabla \phi_y \gets (\phi_{u} - \phi_{d}) / 2$;
      \State \Return $\nabla \phi_x, \nabla \phi_y$;
      \EndFunction
      \Ensure Geometry gradient $\nabla \phi_x$, $\nabla \phi_y$;
      \Statex
      \Require TSDF $\phi_\mathrm{TSDF}$, geometry gradient $\nabla \phi_x, \nabla \phi_y$;
      \Function{CUDA\_curvature}{$\phi, \nabla \phi_x, \nabla \phi_y$}
      \State $\nabla \phi_{xx} \gets $ \texttt{CUDA\_geometry\_gradient}$(\nabla \phi_x)$;
      \State $\nabla \phi_{yy} \gets $ \texttt{CUDA\_geometry\_gradient}$(\nabla \phi_y)$;
      \State $\phi_{ul}, \phi_{ur}, \phi_{dl}, \phi_{dr} \gets$ Shift $\phi$ to 4 diagonal directions;
      \State $\nabla \phi_{xy} \gets ((\phi_{ur}-\phi_{ul}) - (\phi_{dr} - \phi_{dl}))/4$;
      \State $\kappa \gets $ Curvature term using \Cref{eq:curva_numer};
      \State \Return $\kappa$;
      \EndFunction
      \Ensure Curvature term $\kappa$;
  \end{algorithmic}
\end{algorithm}


\subsection{DevelSet-Net~(DSN)}
\label{subsec:dsn}

Although the CUDA accelerated DSO framework has achieved a remarkable speedup, there is still much room for improvement.
Given the recent advance of deep learning on OPC,
we propose a novel neural network with level set embeddings to improve efficiency and mask printability.




\subsubsection{Network Architecture and Training}
\label{subsubsec:dsn_arch}
As illustrated in \Cref{fig:develset_flow}, the DSN is a mulit-branch neural network adopting the simple UNet~\cite{U-Net} as the backbone.
Our key contribution is the integration of level set embeddings with the conventional OPC networks.

% DSN achieves an end-to-end trainable deep level set neural network framework for mask optimization.

\textbf{Multi-branch pre-training}.
To utilize the advance of the mulit-branch neural networks, two types of losses are optimized simultaneously,

\begin{equation}
  L_{\mathrm{DSN}}(\theta)=L_{0}(\theta)+ L_{m}(\theta) .
\end{equation}

\textbf{Level set branch supervision}.
As illustrated in \Cref{fig:develset_flow}, different from the tipical OPC networks, the level set branch predicts the initial LSF $\phi_{0,\theta}$ for DSO, instead of the pixel-wise mask.
The mean square error is employed as the objective function,
\begin{equation}
  L_{0}(\theta)=\sum_{(x, y)}\left(\phi_{0, \theta}(x, y)-\phi_{\mathrm{gt}}(x, y)\right)^{2},
\end{equation}
where $\phi_{0,\theta}$ is the predicted LSF with network parameters $\theta$.
The $\phi_{gt}$ is the ground truth LSF generate by DSO.

\input{doc/res_table1.tex}

\textbf{Modulation branch supervision}.
During the training process, the modulation branch aims to find the best $\mathbf{m}_{\theta}$ in \Cref{eq:dso_evolution} for curvature term evolution in DSO,
which is a boundary-aware model for detecting the curvature-sensitive areas.
The idea is carried out by shifting the ground-truth TSDF $\phi_{\mathrm{gt}}$ with a set of distance $\Delta h$,
\begin{equation}
  \begin{aligned}
    \tilde{\phi}_{m}(x, y) &= \phi_{\mathrm{gt}}(x, y)+\Delta h, \\
    \tilde{m} &= H(\tilde{\phi}_{m}(x, y)) ,
  \end{aligned}
\end{equation}
where $\Delta h$ is uniformly sampled from $[-20, 20]$, $\tilde{m}$ is a set of $\mathbf{m}_\theta$. $H(\phi)$ is the Heaviside function,
\begin{equation}
  \label{eq:heaviside}
  H(z)=\left\{\begin{array}{l}
  1, z \geq 0, \\
  0, z<0.
  \end{array}\right.
\end{equation}
For every target image $Z_\mathrm{t}$, the ground-truth of modulation branch is
\begin{equation}
  m_\mathrm{gt}=\underset{\tilde{m}}{\operatorname{argmin}}\ L_\mathrm{DSO}.
\end{equation}
During the training, the modulation branch learns to simulate an optimized $\mathbf{m}_\theta$.
% We employ a weighted binary cross entropy loss to supervise the output $H(\phi_{0, m})$.
As suggested in \cite{chan2001atc}, the simple Heaviside function in \Cref{eq:heaviside} acts on zero level set, which may get stuck in the local minima.
To tackle this, we replace it with the Approximated Heaviside Function~(AHF) with a parameter $\varepsilon$,
\begin{equation}
  \begin{aligned}
  H_{\varepsilon}(\phi) &=\frac{1}{2}\left(1+\frac{2}{\pi} \arctan \left(\frac{\phi}{\varepsilon}\right)\right).
  % \delta_{\varepsilon} &=\frac{\partial H_{\varepsilon}(\phi)}{\partial \phi}=\frac{1}{\pi} \cdot \frac{\varepsilon}{\varepsilon^{2}+\phi^{2}} .
  \end{aligned}
\end{equation}
Thus, the objective function is
\begin{equation}
  L_{m}(\theta)=\sum_{(x, y)}\left(H_{\varepsilon}(\phi_{m, \theta}(x, y))-m_{\mathrm{gt}}(x, y)\right)^{2},
\end{equation}
where $H_{\varepsilon}(\phi_{m, \theta})$ is the output of modulation branch.
% \begin{equation}
%   \begin{aligned}
%   L_{m}(\theta) =& \sum_{(x, y)}-w_{p} m_{\mathrm{gt}}(x, y) \log H\left({\phi}_{m}(x, y)\right) \\
%   - w_{n}&\left(1-m_{\mathrm{gt}}(x, y)\right) \log \left(1-H\left(\phi_{m}(x, y)\right)\right)
%   \end{aligned}
% \end{equation}

\subsection{DevelSet~(DSN+DSO) End to End Joint Optimization}
\label{subsec:develset}
As illustrated in \Cref{fig:develset_flow}, we apply the \texttt{CUDA\_TSDF} function to facilitate the fast transform from pixel-wise target image to LSF $\phi_0$.
After pre-training of the two branches of DSN, we fix all the parameters of DSN
then directly feed the output of level set branch $\phi_{0, \theta}$
and modulation branch $\mathbf{m}_{\theta}$ into the evolution process of DSO to generate the final mask.
We choose the conjugate gradient (CG) method~\cite{OPC-JVSTB2013-Lv} for optimization in DSO,
and follow CFL condition~\cite{OPC-JVSTB2013-Lv} to set the time step $\Delta t  = \eta / \mathrm{max}(\lvert v \rvert)$, where $v$ is evolution velocity in \Cref{eq:vel}, and $\eta$ is CFL condition number.

% \todo{The early stop function}
% \todo{Should add more details for end to end optimization.}
% \todo{Add CG method and the time step delta t.}
% \todo{Dominating advantage, add more for end to end.}
% \todo{Mark the difference with the pixel-based end-to-end flow.}
% \move{By controlling the curvature term, DSO can directly increase the mask manufacturability without computational explosion.
% Furthermore, with the rapid development of deep learning technology,
% DSN is designed to further accelerate the DSO by predicting the LSF,
% which will provide the DSO a better initial solution parameters.
% Consequently, the total iteration of DSO will decrease significantly.}
% \move{Although Neural-ILT achieves SOTA turn around time speed up with lower mask complexity and higher mask printability,
% the proposed DevelSet framework outperforms Neural-ILT with instant turn around time.}
% \move{In OPC tasks, after the mask optimization process,
% the printed mask will become extremely irregular and therefore difficult to manufacture.}
% \move{During the evolution process, we further regularize the to reduce.
% We will show the relationship between the curvature term and mask complexity.
% Previous work Neural-ILT introduce the shot to measure mask complexity,
% we design experiments to show that curvature can reduce mask complexity.}



% \begin{algorithm}[h]
%     \caption{DevelSet-Optimizer}
%     \small
%     \begin{algorithmic}[1]
%         \Require Target image $\mathbf{Z}^*$, optical kernels $h$;
%         \Ensure Optimized mask $\mathbf{M}^*$;
%         \State $\mathbf{M}_0 \leftarrow \mathbf{Z}^*$;
%         \State $\phi_0 \leftarrow \mathbf{M}_0$;
%         \State $\mathbf{G}_0 \leftarrow \frac{\partial \mathbf{C}_0(M)}{\partial \mathbf{M}_0}$;
%         \State $\mathbf{v}_0 \leftarrow -\mathbf{G}_0|\nabla\phi|$;
%         \Repeat
%         \For{all sites in 2-D pattern, parallel computing:}
%         \State \textbf{forward:}
%         \State Curvature: $\kappa_i \leftarrow \nabla\frac{\nabla \phi_i}{| \nabla \phi_i |}$;
%         \State Change: $\Delta \phi_i \leftarrow (\mathbf{v}_i + \kappa_i)\delta t$;
%         \State Level set function: $\phi_{i+1} \leftarrow \phi_i + \Delta \phi_i$;
%         \State Mask pattern: $\mathbf{M}_{i+1} \leftarrow \phi_{i+1}$;
%         \State \textbf{backward:}
%         \State Simulate wafer pattern $\mathbf{R}$ parallel;
%         \State Gradient of cost function: $\mathbf{G}_I{i+1} \leftarrow \frac{\partial \mathbf{C}_{i+1}(\mathbf{M})}{\partial\mathbf{M}}$;
%         \State Update velocity: $\mathbf{v}_{i+1} = -\mathbf{{G}_{i+1}}(\mathbf{M})|\nabla\phi_{i+1}| + \lambda\cdot\mathbf{v}_i$;
%         \EndFor
%         \Until{$|\mathbf{v}|_{max} < \epsilon$}
%     \end{algorithmic}
%     \label{alg:parallel-ls}
% \end{algorithm}
