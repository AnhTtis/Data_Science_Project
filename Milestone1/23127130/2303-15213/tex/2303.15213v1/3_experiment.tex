\section{Experiment}

\subsection{Experiment Setup}

\subsubsection{Train Data-Set Preparation}
%The data set used for training the PV-RNN model was time series data of Torobo joint angles while performing four different cyclic movements \emph{A, B, C} and \emph{D}, where each movement pattern consists of 20 time-step long.
The proposed PV-RNN model was trained in a supervised manner by preparing 4-dimensional joint angle teaching trajectories.
In preparation for teaching trajectories, we considered four types of cyclic movement patterns (20 time-steps for each cycle) using Torobo's shoulder and elbow joint angles in both arms.
% Each cyclic movement is a simple movement that uses Torobo's shoulder and elbow joints, such as raising both hands and lowering them back.
Then, it was assumed that cycling movement patterns transit from one to another following a probabilistic finite state machine (Fig.\ref{fig:PFSM}).
For example, after a movement pattern \emph{A} is generated for one cycle with the state at \emph{S1}, \emph{A} can be generated for one more cycle with $90\%$ probability staying at the same state, or \emph{B} or \emph{C} can transit to \emph{S2} or \emph{S3} with a probability of $3\%$ and $7\%$, respectively. 
% When each cyclic movement ends, the transition occurs with a probability of around $10$ \%.
%For instance, while Torobo is performing \emph{A}, after each \emph{A}, it keeps generating \emph{A} with a probability of $90$ \%.
%On the other hand, with $3$ \% and $7$ \%, Torobo switches its movement into \emph{B} and \emph{C}, respectively.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/Probabilistic_FSM.pdf}
  \caption{Schematic of the probabilistic finite state machine from which training data was generated.}
  \label{fig:PFSM}
\end{figure}
%The data set was four-dimensional, where each dimension corresponds to the angles of Torobo's shoulders and elbows.
We prepared 10 sequences that each consisted of 200 cycles of movement patterns, extending 4000 time-steps.

\subsubsection{The Network Configuration and Training}
To conduct human-robot dyadic interaction experiments, PV-RNN was trained 3 times with identical parameters (Table.\ref{tab:parameters}).
$\#\mathbf{d}, \#\mathbf{z}, \tau, \mathbf{w}^t, \mathbf{w}^i$ indicates the number of d neurons and z neurons, time constant, and meta-prior during the training phase and interaction phase, respectively.
%Among each PV-RNN model, parameters remained the same except for the random seed used for generating random numbers to structure a different model from the same given data set.
The network was trained for 50,000 epochs to minimize the evidence-free energy shown in Eq.\ref{eq:evidenceF}, starting with random weights generated with different seeds for each training.
%
\begin{table}[htbp]\centering
\caption{PV-RNN Parameters}
\label{tab:parameters}
\begin{tabular}{cccccc}
\toprule\footnotesize
     & $\#\mathbf{d}$ & $\#\mathbf{z}$ & $\tau$ & $\mathbf{w}^{t}$ & $\mathbf{w}^{i}$\\
\cmidrule(lr){2-6}
\textbf{Layer 1 (Top)} & 60 & 6 & $3$  & 0.01 & $[0.01, 0.05, 0.1]$\\
\textbf{Layer 2 (Bottom)} & 30 & 3 & $9$ & 0.01 & $[0.01, 0.05, 0.1]$ \\
\bottomrule
\end{tabular}
\end{table}
%
% Fig. 4
Fig.\ref{fig:train_result} shows one of the resultant training processes.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/train_result.png}
  \caption{
    Resultant time-development from the training phase of one of the PV-RNN models.
  Development of KLD with respect to the number of training epochs is shown for (a) the Top layer (layer 2) and (b) the Bottom layer (layer 1).
  Development of the average prediction error over time steps in each teaching sequence is shown in (c).}
  %All three values clearly decrease with respect to epoch, which indicates that the PV-RNN is successfully learning the train data set.}
  \label{fig:train_result}
\end{figure}
It can be seen that prediction error and the KL divergence in both the top and the bottom layers decreased throughout training.
All three training processes converged in a similar way, achieving prediction errors and KL-divergences shown in Table.\ref{tab:TrainingResult}.


\begin{table}[htbp]
\centering
\caption{Training processes of all three PV-RNN model}
\label{tab:TrainingResult}
\begin{tabular}{ ccc  c c }
\cmidrule(lr){1-5}
 && \multicolumn{3}{c}{Epoch} \\
\cmidrule(lr){3-5}
\cmidrule(lr){3-5}
     & Model &    2,000     &       5,000       &    50,000\\
\cmidrule(lr){2-5}
\multirow{3}{*}{KL divergence} & 1 & $2.0 \times 10^{-3}$ & $5.4 \times 10^{-4}$ & $4.4 \times 10^{-5}$\\
& 2     &  $2.4 \times 10^{-3}$ &  $9.1 \times 10^{-4}$  & $5.6 \times 10^{-5}$\\
& 3    & $2.5 \times 10^{-3}$ &  $7.2 \times 10^{-4}$  &  $6.9 \times 10^{-5}$\\
\cmidrule(lr){1-5}
\multirow{3}{*}{Pred. Err.} &  1       &$1.8 \times 10^{-2}$ &  $7.2 \times 10^{-4}$  & $1.4 \times 10^{-4}$\\
& 2     & $1.4 \times 10^{-2}$ &  $6.6 \times 10^{-3}$  & $4.6 \times 10^{-5}$\\
& 3    & $1.3 \times 10^{-2}$  &  $1.6 \times 10^{-3}$  &  $1.4 \times 10^{-4}$\\
 \hline
\end{tabular}
\end{table}
%
% One of the training results over the number of epochs is shown in Fig.\ref{fig:train_result}. top and the middle row of Fig.\ref{fig:train_result} are the KL-divergence calculated in layer 1 (top layer) and layer 2 (bottom layer), respectively. The bottom row is the mean squared error calculated following Fig.\ref{eq:MSE} from the $target$ value, which in this case is the data set and the output of the PV-RNN model.

\subsubsection{Evaluation of the trained networks}
Trained PV-RNNs were evaluated on the basis of how closely the state transition probability wasreconstructed compared with the target (Fig.\ref{fig:PFSM}). 
For this purpose, we conducted \emph{Prior Generation} in which the forward computation by following Eq.\ref{eq:hiddenlayer}-\ref{eq:output} was performed without any external observation for 40,000 time-steps for each trained network.
Then, probabilities for all possible movement pattern transitions were measured during prior generation and the resulting state transition probability was inferred (Table.\ref{tab:TrainEvaluation}).
The state transition probability computed for all trained networks is quite similar to the target one. 
Output and network dynamics of movement pattern transitions during prior generation are shown in Appendix, Fig.\ref{fig:PriorGeneration}.
%
\begin{table}[htbp]
    \centering
    \caption{Transition probabilities of the \emph{Prior Generation} and data set}
    \begin{tabular}{cccccc}
    \cmidrule(lr){1-6}
    Model & \emph{S1} $\rightarrow$ \emph{S2} & \emph{S1} $\rightarrow$  \emph{S3} & \emph{S2} $\rightarrow$  \emph{S4} & \emph{S3} $\rightarrow$  \emph{S4} & \emph{S4} $\rightarrow$  \emph{S1} \\
    \cmidrule(lr){2-6}
    1 & 2.8 \% & 3.5 \% & 15.6 \% & 12.5 \% & 3.1 \% \\
    2 & 2.7 \% & 9.3 \% & 14.1 \% & 11.5 \% & 7.2 \% \\
    3 & 3.1 \% & 4.1 \% & 9.8 \%  & 12.3 \% & 5.1 \% \\
    Target & 3 \% & 7 \% & 10 \% & 15 \% & 5 \% \\
    \cmidrule(lr){1-6}
    \end{tabular}
    \label{tab:TrainEvaluation}
\end{table}

\subsubsection{Human-Robot Interaction}
After the training phase, we conducted two types of human-robot interaction experiments using trained PV-RNNs.
In these experiments, while Torobo was generating movement pattern transitions successively based on the training, the human experimenter attempted to induce various movement pattern transitions by grasping both arms of Torobo and exerting force on them. 
These movement pattern transitions included trained transitions (\emph{AB, AC, BD, CD, DA}) and untrained transitions (\emph{AD, DB, DC, BA, CA}) in which \emph{AB}, for example, dictates that \emph{A} pattern is forced to transit to \emph{B} pattern.
Each transition from one pattern to another requires some guiding force by a human experimenter, even for trained transitions, since ongoing patterns tend to repeat another cycle with high probability, more than $85\%$ for all patterns.
This means that there exist some conflicts between movement trajectories intended by the robot and those by the human experimenter during both trained and untrained transitions.

Experiment-1 examined the effect of $w^i$ settings on the interactions.
While Torobo was generating movement pattern transitions successively for 2,200 time-steps, the human experimenter attempted to induce one of the trained transition every 200 time-steps starting from $t = 200$ which resulted in 10 trained transitions.
The duration of each attempt lasted 100 time-steps at the most.
This experiment was conducted three times for all three trained PV-RNNs by changing $w^i$ with $0.01$, $0.05$, and $0.1$.

Experiment-2 examined the difference between trained and untrained transitions by setting $w^i$ with a fixed value.
The experimental procedure was the same as in Experiment-1, although this time, $w^i$ was fixed at $0.01$ for both layers.
This experiment, using all three trained PV-RNNs, resulted in 30 untrained transition attempts.
In these experiments, we recorded the time development of essential values including latent variables, predicted and observed joint angles, prediction error, and the KL-divergence in both layers for later analysis of experiment results.

\subsection{Experiment Results}
In this section, we show the results of the aforementioned experiments.

\subsubsection{Experiment-1}
First, we examined time-development of essential values during movement pattern transitions induced by the experimenter for each case with a different meta-prior setting.
Fig.\ref{fig:TimeDevelopmentDifferentW} shows an example snapshot of future prediction and past reflection, which shifted every 7 time-steps of the current time during the transition \emph{AB} performed under different settings of meta-prior, $w^i = 0.01, 0.05, 0.1$.
% Fig.5
\begin{figure*}[h] % 
\begin{center}
\includegraphics[width=0.95\linewidth]{figures/TimeDevelopment.png}
\caption{Time-development of excess torque, prediction error, and KL-divergence between the approximate posterior and the prior in trained movement transitions
in cases with three different meta-prior $w^i$ settings with $0.01$, $0.05$, and $0.1$. The grey area represents the past window where the head of the window is the current time. The window is shifted 5 times during the movement transition \emph{AB}.
}
\label{fig:TimeDevelopmentDifferentW}
\end{center}
\end{figure*}
%
Each snapshot shows one of the observed joint angles $\theta$ (dotted blue) and its prediction $\Bar{\theta}$ (blue) in the top row, the KL-divergences between the approximate posterior and the prior in the layer 1 (orange) and in the layer 2(dark orange) in the second row, the prediction error (green) in the third row, and the excess torque (black) in the bottom row.
The grey area represents the past window where the approximate posterior in terms of adaptive variables $\mathbf{A}_t^{\mu}, \mathbf{A}_t^{\sigma}$ is updated.
We provide two supplementary videos for experiment-1 showing the interaction between the experimenter and Torobo, as well as network dynamics in the case with the meta-prior $w^i$ set to 0.01 (\href{https://youtu.be/jQGnfPMAWes}{video-link1}) and 0.1 (\href{https://youtu.be/th1-5Ay603Y}{video-link2}).

The sequences of time-shifted snapshots in Fig.\ref{fig:TimeDevelopmentDifferentW}, show that excess torque appears first followed by rises in the prediction error (negative log-likelihood) and the KL-divergence. 
Later, the predicted joint angle pattern shifts from pattern \emph{A} to \emph{B} while the gap between the observed joint angle and the reconstructed joint angle remains in the past window.
This is the same for all three cases with different $w^i$ settings.

However, we can see some qualitative differences in the transition process depending on the $w^i$ setting.
The prediction error and the excess torque in the case with a small $w^i$ ($w^i=0.01$) are smaller than those in the case with large $w^i$ ($w^i=0.1$).
Also, the error and torque with a small $w^i$ are less persistent than those with a larger $w^i$.
However, the KL-divergence in the case with a small $w^i$ is larger and persists longer than that in the case with large $w^i$.
In order to confirm these observations, we conducted statistical analysis on the excess torque, KL-divergence, and prediction error time-averaged for each transition period (100 steps). 
This computation was repeated 10 times for each of three different trained networks set with three different $w^i$ values.
%In order to confirm these observations, we conducted statistical analysis on the time-averaged excess torque, KL-divergence, and prediction error in each transition repeated 10 times for each of three different trained networks set with three different $w^i$ values.

The results are shown in Fig.\ref{fig:TransitionForDifferentW}.
%
% Fig. 6
\begin{figure}[h] % 
\begin{center}
\includegraphics[width=0.95\linewidth]{figures/Exc_KL_MSE_per_attempt.png}
\caption{Time-averaged excess torque, prediction error, and the KL-divergence between the approximate posterior and the prior in the trained movement transition in cases with three different meta-prior $w^i$ settings with $0.01$, $0.05$, and $0.1$.
}
\label{fig:TransitionForDifferentW}
\end{center}
\end{figure}
%
Both the time-averaged prediction error and the excess torque measured in the cases with small $w^i$ are significantly smaller than those with large $w^i$.
On the other hand, the time-averaged KL-divergence with small $w^i$ is significantly larger than that with large $w^i$.

By considering these statistical results, movement pattern transitions exerted by the experimenter require greater force when $w^i$ is set larger, since the approximate posterior distribution strongly follows the prior distribution representing the current movement intention of PV-RNN, minimizing the KL-divergence between them while the error $\Delta \theta_t$ between the predicted joint angle $\bar{\theta_t}$ and the observed joint angle $\theta_t$ becomes larger.
If the experimenter attempts to move the trajectory of the robot's joint angles in a direction different from that predicted by the PV-RNN, this requires a large excess torque $\tilde{e}_t$ to counteract the large error $\Delta \theta_t$, as derived from Eq. \ref{eq:target joint angle}.

On the other hand, when $w^i$ is set smaller, the approximate posterior follows the prior only weakly, allowing larger KL-divergence between them while the prediction error becomes smaller.
In this case, only a small amount of excess torque is necessary to counteract the small error.
The top-down actional intention of the robot became stronger in the case of larger $w^i$ settings; therefore, the human experimenter was required to exert more force on the robot arms to induce a transition, whereas less force was required with smaller $w^i$ settings, since the top-down actional intention of the robot became weaker.

In the current experiment, the value of $w^i$ was set between $0.01$ and $0.1$.
This is because our preliminary experiments showed that the robot's behaviour became noisy when $w^i$ was set smaller than $0.01$, since the approximate posterior could easily deviate from the prior by noise sampling.
On the other hand, it became difficult for the experimenter to initiate a transition when $w^i$ was set larger than $0.1$, because of substantially increased resistance. 

\subsubsection{Experiment-2}
Next, we looked at the difference in the excess torque, prediction error, and KL-divergence between trained and untrained transitions while $w^i$ was fixed at a given value.
Both trained and untrained transitions were attempted 10 times for each of the three trained networks with $w^i$ set to $0.01$ for both trained and untrained cases.
However, for the untrained case, among 30 attempts, only 24 succeeded in performing a transition.
Our preliminary experiment showed that the untrained transition became more difficult when $w^i$ was set higher than $0.01$ because of the strong resistance when the robot attempted to lead trained movement transitions.
Fig.\ref{fig:TrainedVSUntrained} shows the time-average of the excess torque, prediction error, and KL-divergence for both trained and untrained transitions attempted by the experimenter.
% Fig. 7
\begin{figure}[h] % 
\begin{center}
\includegraphics[width=0.95\linewidth]{figures/TrainedVSUntrained_box.png}
\caption{The amount of excess torque, prediction error exerted and the KL-divergence between the approximate posterior and the priorc during each attempt for trained and untrained transitions. The meta-prior $w^i$ was set to $0.01$.
}
\label{fig:TrainedVSUntrained}
\end{center}
\end{figure}
%
The time-average of the prediction error, the excess torque, and the KL-divergence are larger in untrained than trained transitions.
This means that untrained transitions require the experimenter to exert more force than for trained transitions because of the free energy, which is the sum of the prediction error and the KL-divergence, and which increases more in untrained transitions.
We provide two supplementary videos for experiment-2 showing the interaction between the experimenter and Torobo as well as the network dynamics in the case with meta-prior $w^i$ set to 0.01, where trained transitions (\href{https://youtu.be/jQGnfPMAWes}{video-link1}) and untrained transitions (\href{https://youtu.be/lZEn5Qvun90}{video-link3}) are performed.

%%%%%%%%%%% END
%%%%%%%%%%%




