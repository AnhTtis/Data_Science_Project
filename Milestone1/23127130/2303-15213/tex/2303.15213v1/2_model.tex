\section{Model} % The original is save in 2_model.2-27old.tex

\subsection{Free Energy Principle}
As mentioned in the Introduction, the current study used PV-RNN, based on the free energy principle, as the basic model.
The free energy principle assumes that brains learn as well as inferred for given observations, by minimizing free energy, defined in Eq. \ref{eq:F}.
% Eq. 1
\begin{equation}
\begin{aligned}\label{eq:F}
    \mathcal{F} = \underbrace{D_{\rm KL}[q_\phi({\boldsymbol{z}}|{\boldsymbol{X}})\Vert p_\theta(\boldsymbol{z})]}_{\rm complexity} - \underbrace{ \mathbb{E}_{q_\phi({\boldsymbol{z}}|{\boldsymbol{X}})}[\log p_\theta({\boldsymbol{X}}|{\boldsymbol{z}})]}_{\rm accuracy},
\end{aligned}
\end{equation}
%
$p_\theta({\boldsymbol{X}}|{\boldsymbol{z}})$ is the likelihood of the sensory observation $\boldsymbol{X}$, given the probabilistic latent variables $z$ which is parameterized by $\theta$.
$q_\phi({\boldsymbol{z}}|{\boldsymbol{X}})$ is the inference model parameterised by $\phi$.
As shown in Eq. \ref{eq:F}, free energy consists of two terms.
The first term indicates the $complexity$, which is the divergence between the approximate posterior probability distribution and prior probability distribution for the latent variable $z$, and the second term is the $accuracy$ in predicting the sensory observation.

\subsection{PV-RNN implementation}
PV-RNN is operated in two distinct phases, a training phase and an interaction phase.
In the training phase, we prepared a dataset (joint angle sequences of the robot) and trained the PV-RNN model using it. 
In the interaction phase, the robot and the human experimenter interact physically, such that the PV-RNN attempts to drive the robot's arms by predicting next-time-step target joint angles while it infers latent variables using actual joint angle readings.
Each operation phase is explained in detail below.

\subsubsection{Training Phase}
%
First, we start with computation during the training phase.
The evidence-free energy ${\mathcal{F}}^{train}$ as a loss function for training the PV-RNN with the joint angle observation from time step $1$ to $T$ is shown in Eq.\ref{eq:evidenceF} (the exact derivation should refer to \cite{ahmadi2019novel}).
% Eq. 2
\begin{equation}
    \label{eq:evidenceF}
\begin{split}
    {\mathcal F}^{train} &= \beta \times \underbrace{D_{\text{KL}} [ q_\phi(\mathbf{z}_1|\mathbf{d}_0,\mathbf{X}_1) \Vert p_\theta(\mathbf{z}_1)]}_{\rm complexity} \\
    +&  \underbrace{w \times \sum_{t=1}^{T-1} \mathbb{E}_{q_\phi(\mathbf{z}_{1:t}|\mathbf{d}_{t},\mathbf{X}_{t:T+1})}\big [D_{\text{KL}}
    [q_\phi(\mathbf{z}_{t+1}|\mathbf{d}_t,\mathbf{X}_{t+1:T+1})\Vert p_\theta(\mathbf{z}_{t+1}|\mathbf{d}_t)]\big]}_{\rm complexity}  \\
    -& \underbrace{\sum_{t=1}^T\mathbb{E}_{q_\phi(\mathbf{z}_{1:t-1}|\mathbf{d}_{t-1},\mathbf{X}_{t:T})}[\ln p_\theta(\mathbf{X}_t|\mathbf{d}_t)]}_{\rm accuracy},
\end{split}
\end{equation}
%
where $q(\phi)$ and $p(\theta)$ are the inference model parameterised by $\phi$ and the generative model parameterised by $\theta$, respectively.

As shown in Eq.\ref{eq:evidenceF}, the meta-prior $w$ weights the complexity term, except for the initial step. 
As described previously, the setting of meta-prior in the training phase strongly affects behavioural characteristics of the trained network.
Prior studies \cite{ahmadi2019novel, wirkuttis2021leading} showed that a network trained with a larger $w$ develops higher precision, i.e., smaller standard deviation, in the prior distribution, whereas a smaller $w$ develops lower precision in the prior.
More or less precision in the prior means stronger or weaker top-down belief, respecyively.
PV-RNN is composed of two variables $d, z$; in which the former is a deterministic latent variable, and the latter is a random latent variable sampled from a Gaussian distribution. 
During forward computation of PV-RNN, $d$ in layer $l$ ($l = 1$ is the bottom layer, which is closest to the output layer) at time step $t$ is computed as,
% Eq.3
\begin{equation} \label{eq:hiddenlayer} 
\begin{aligned}
    \mathbf{h}^l_t = &\left(1 - \frac{1}{\tau^l}\right)\mathbf{h}^l_{t-1}
    + \frac{1}{\tau^l} \biggl( \mathbf{W}^{ll}_{dd}\mathbf{d}^l_{t-1} + \mathbf{W}^{ll}_{zd}\mathbf{z}^l_t
    + \mathbf{W}^{ll+1}_{dd}\mathbf{d}^{l+1}_{t} + \mathbf{b}_h \biggr), \\
    \mathbf{d}^l_t = &\tanh(\mathbf{h}^l_t).
\end{aligned}
\end{equation}
%
Here, $\mathbf{h}_t^l$ denotes the internal state of $\mathbf{d}_t^l$ before the activation function $\tanh$ is applied, and $\mathbf{b}_h$ denotes the bias term of $\mathbf{h}$.
$\tau^l$ is a time constant unique to each layer, which encourages the network to process information by following an intrinsic time scale of the layer \cite{yamashita2008, Pio-lopez-hierarchy-AIF2016, Schillaci2020, hwang2020dealing}.
Matrices $\mathbf{W}_{dd}$ and $\mathbf{W}_{zd}$ express intra- and inter-layer connectivity weights in the network, respectively.
At $t=1$, the input to $\mathbf{h}$ in the top layer is calculated only from $\mathbf{z}_1^l$ and $\mathbf{b}_h$. 

Each dimension of $\mathbf{z}_t^p$ is sampled over the prior distribution, which is a Gaussian distribution of mean $\pmb{\mu}_t^p$ and standard deviation $\pmb{\sigma}_t^p$ individually.
At $t=1$, $\mathbf{z}_1^p$ is sampled from a standard normal distribution, and $z_t^p$ for the following time steps is computed by,
%
% Eq. 4
\begin{equation} \label{eq:p} 
\begin{aligned}
    \pmb{\mu}^p_t &= \tanh(\mathbf{W}^{ll}_{d\mu}\mathbf{d}_{t-1} + \mathbf{b}_\mu^p),\\ 
    \pmb{\sigma}^p_t &= \exp(\mathbf{W}^{ll}_{d\sigma}\mathbf{d}_{t-1} + \mathbf{b}_\sigma^p),     
\end{aligned}
% Eq. 5
\end{equation}
\begin{equation}
    \mathbf{z}^p_t = {\pmb{\mu}^p_t} + {\pmb{\sigma}^p_t} * \pmb{\epsilon}_t, \quad \textrm{with }\pmb{\epsilon}_t\sim\mathcal{N}(\mathbf{0},\mathbf{I}),
\end{equation}
%
where $*$ indicates the element-wise product of the two vectors. 
This equation follows the idea of the conditional prior \cite{chung2015recurrent}.
Here, $\mathbf{W}^{ll}_{d\mu}, \mathbf{W}^{ll}_{d\sigma}, \mathbf{b}_\mu^p$ and $\mathbf{b}_\sigma^p$ are the weight matrices and the bias terms for $\pmb{\mu}_t^p$ and $\pmb{\sigma}_t^p$, respectively.
$\pmb{\epsilon}$ is a value sampled from the standard normal distribution. 
In order to make model parameters differentiable through the random latent variable, we use the reparametrization trick \cite{kingma2013auto} to sample $\mathbf{z}_t^p$.

On the other hand,  $\mathbf{z}_t^q$ in inference model $q_\phi$ is sampled from the approximate posterior probability distribution $q(\mathbf{z}_{t})$ which is a Gaussian distribution with mean $\mu_t^q$ and standard deviation $\sigma_t^q$ which are computed as,
%
% Eq. 6
\begin{equation} 
\begin{aligned}
    \pmb{\mu}^q_t &= \tanh( \mathbf{A}^\mu_t),\\
    \pmb{\sigma}^q_t &= \exp(\mathbf{A}^\sigma_t),\\
    \mathbf{z}^q_t &= \pmb{\mu}^q_t + \pmb{\sigma}^q_t * \pmb{\epsilon}_t, \quad \textrm{with }\pmb{\epsilon}_t\sim\mathcal{N}(\mathbf{0},\mathbf{I})
\end{aligned}
\end{equation}
%
$\mathbf{A}^\mu_t, \mathbf{A}^\sigma_t$ are adaptive variables which are optimised during training. 
During the training phase, $\mathbf{z}_t$ in Eq.\ref{eq:hiddenlayer} is sampled from the approximate posterior.
On the other hand, $\mathbf{z}_t^p$  is sampled from the prior distribution to compute the Kullback-Leibler divergence (KLD) between the approximate posterior and the prior, which is the complexity term of the loss function Eq.\ref{eq:evidenceF}.

The obtained $\mathbf{d}_t^1$ is used for computing the output layer, of which function is,
% Eq. 7
\begin{equation}\label{eq:output}
\mathbf{o}_t = \mathbf{W}_o \mathbf{d}_t^1 + \mathbf{b}_0,
\end{equation}
%
where $\mathbf{W}_o, \mathbf{b}_o$ is the weight matrix and the bias of the output layer, respectively. 

Finally, we compute the predicted output $\mathbf{\bar{x}}_t$.
Each dimension of the predicted output is represented in a probabilistic distribution using the Softmax of $N_{soft}$ elements.
Therefore, the $j$-th softmax element of the $i$-th dimension of the predicted output $\bar{x}_t^{i, j}$ is computed by,
% Eq. 8
\begin{equation}
    \bar{x}_{t, i, j} = \frac{\exp(o_{t, i, j})}{\sum_{j=1}^{N_{soft}} \exp(o_{t, i, j})}.
\end{equation}
%
As shown in Eq.\ref{eq:evidenceF}, the evidence-free energy $\mathcal{F}$, which is the loss function of this model, is the sum of the KLD between the approximate posterior $q(\mathbf{z}_t| \mathbf{x}_t)$ and prior $p(\mathbf{z}_t | \mathbf{d}_{t-1})$, and the negative log-likelihood calculated from the output $\mathbf{\bar{x}}_t$ and the perception $\mathbf{x}_t$.

At time step $t$, the KLD between the approximate posterior and prior is defined as,
% Eq. 9
\begin{equation}\label{eq:KLD_definition}
    D_{KL} [q(\mathbf{z}_t| \mathbf{x}_{t:T})||p(\mathbf{z}_t | \mathbf{d}_{t-1})]
    = \sum_i^{N_z} q(z_t^i| \mathbf{x}_{t:T}) \ln \frac{p(z_t^i | \mathbf{d}_{t-1})}{q(z_t^i| \mathbf{x}_{t:T})},
\end{equation}
%
where $z_t^i$ is the $i$-th dimension of $\mathbf{z}_t$, $T$ is the length of the sequence and $N_z$ is the dimension of $\mathbf{z}_t$ of each layer.
Here, $q(\mathbf{z}_t| \mathbf{x}_t)$ and $p(\mathbf{z}_t | \mathbf{d}_{t-1})$ are both assumed to follow multivariate Gaussian distributions with diagonal covariant matrices. Hence, they are expressed with the mean $\pmb{\mu}_t$ ($=[\mu_{t, 1}, \mu_{t, 2}, ..., \mu_{t, N_z}]$) and the standard deviation $\pmb{\sigma}_t$ ($=[\sigma_{t,1}, \sigma_{t,2}, ..., \sigma_{t, N_z]}$) as below,
% Eq. 10
\begin{equation}
    q(z_t^i| \mathbf{x}_{t:T}) = \frac{1}{\sqrt{2\pi(\sigma_{t, i}^q)^2}}\exp{\left[-\frac{1}{2} \left( \frac{z_t^i - \mu^q_{t, i}}{\sigma_{t, i}^q} \right) ^2\right]},
\end{equation}
% Eq. 11
\begin{equation}
    p(z_t^i| \mathbf{d}_{t-1}) = \frac{1}{\sqrt{2\pi(\sigma_{t, i}^p)^2}}\exp{\left[-\frac{1}{2} \left( \frac{z_t^i - \mu_{t, i}^p}{\sigma_{t, i}^p} \right) ^2\right]}.
\end{equation}
%
Therefore, the KLD loss $r_t$ between the approximate posterior and the prior is computed as,
% Eq. 12
\begin{equation}\label{eq:KLD}
r_t = \begin{cases}
{\sum_i D_{KL} [q(z_t^i| \mathbf{x}_t)||p(z_t^i | \mathcal{N}(0,1))]}, \quad \textrm{if } t=1,\\
{\sum_i D_{KL} [q(z_t^i| \mathbf{x}_t)||p(z_t^i | \mathbf{d}_{t-1})]}, \quad \textrm{otherwise}.
\end{cases}
\end{equation}
%
On the other hand, as an equivalent of the negative log-likelihood term of Eq.\ref{eq:evidenceF}, the KLD between the target and the predicted output is computed.
As mentioned above, the predicted output is represented as a probabilistic distribution $P(x_t^{i} | \mathbf{\theta})$ using Softmax.
The target sequence is also transformed into a probabilistic distribution $Q(x_{t}^{i})$ along the same line \cite{ahmadi2017can}.
Therefore, the KLD is computed as,
% Eq. 13
\begin{equation}
    \label{eq:negativeLog}
        e_t = \sum_i^{N_x} D_{KL} [Q(\mathbf{x}_{t, i}) || P(\mathbf{x}_{t, i})]
        = \sum_{i}^{N_x} \sum_j^{N_{soft}} Q(x_{t, i, j}) \ln \frac{Q(x_{t, i, j})}{P(x_{t, i} | \mathbf{\theta})}.
\end{equation}
%
The loss function of our model which is equivalent to the evidence-free energy $\mathcal{F}^{train}$ is computed as the weighted sum of Eq.\ref{eq:KLD} and Eq.\ref{eq:negativeLog}.
As shown in Eq.\ref{eq:evidenceF}, the KLD between the approximate posterior and prior is regularised with a meta-prior $\beta$ for $t = 1$ and $w$ for $t \neq 1$. Here, $\beta$ is the weighting parameter specifically for $t = 1$, which regulates the initial sensitivity of the PV-RNN model.
Therefore, the loss function of our model during the training phase is computed as,
% Eq. 14
\begin{equation} \label{eq:LossFunction}
    \mathcal{F} = \begin{cases}
    \sum_{t=1}^T \left( e_t + \frac{N_x}{N_z} \beta r_t \right), \quad \textrm{if } t=1, \\
    \sum_{t=1}^T \left( e_t + \frac{N_x}{N_z} w^t r_t \right), \quad \textrm{otherwise}
    \end{cases}
\end{equation}
%
where $N_x$ and $N_z$ are the $\Bar{\mathbf{x}}$ dimension and $\mathbf{z}$ dimension in each layer, respectively.
$T$ is the length of the dataset sequence.
The negative log-likelihood, which we denote as \emph{prediction error}, $e_t$ is proportional to $N_x$ and the KLD between the approximate posterior and prior is proportional to $N_z$, under the assumption that each $\Bar{\mathbf{x}}$ dimension and $\mathbf{z}$ dimension is independent.
Therefore, the meta-prior during the training phase $w^t$ is normalised by $N_x$ and $N_z$ so that it can be compared among PV-RNN models with different $\Bar{\mathbf{x}}$ dimension and $\mathbf{z}$ dimension.
Through training, weight matrices, biases and adaptive variables $\mathbf{A}_t^\mu, \mathbf{A}_t^\sigma$ are updated based on back-propagation through time (BPTT) \cite{werbos1990backpropagation, lillicrap2019backpropagation, rumelhart1986learning}.
%
\subsubsection{Interaction phase}
We performed a human-robot real-time interaction in the interaction phase where the pre-trained PV-RNN drives the robotâ€™s movement by predicting joint angles of the next time step.
The forward computation part remains the same as that in the training phase.
However, there are some differences in the loss function used and in ways of updating variables.
Unlike the training phase, weight matrices and biases are not updated.
Only the adaptive variables $\mathbf{A}_t^\mu$ and $ \mathbf{A}_t^\sigma$ are updated so that the approximate posterior can adapt to the ongoing observation of the joint angle sequence.

PV-RNN predicts the future sensation and infers past latent variables using the past window spanning from time-step $t_{c}-t_{w}$ to the current time-step $t_c$ with a window size $t_w$ as shown in Fig. \ref{fig:PVRNN}.
% Fig.1
\begin{figure}[htbp] % 
\begin{center}
\includegraphics[width=0.60\columnwidth]{figures/PVRNN_english.png}
\caption{Graphical representation of the interaction phase of PV-RNN. $d_{l, t}, z^p_{l, t}, z^q_{l, t}, x_{t}$ and $\Bar{x}_{t}$ indicates the deterministic latent variable, prior distribution, approximate posterior, output and target of layer $l$ and time-step $t$, respectively.
Blue and red arrows indicate forward propagation and backward propagation, respectively. The future, the past within the window and the past outside the window are coloured red, blue, and green respectively. The past window size is 2 in this graphical model. a) shows the network at time-step $t$. b) shows the network at time-step $t+1$.
}
\label{fig:PVRNN}
\end{center}
\end{figure}
%
The evidence-free energy in the interaction phase $\mathcal{F}^{int}$, normalized in the same way as Eq.\ref{eq:LossFunction}, is computed inside the past window as:
% Eq. 15
\begin{equation}
    \mathcal{F}^{int} = \begin{cases} 
    \sum_{t = t_c - t_w}^{t_c} \left( e_t + \frac{N_x}{N_z} \beta r_t \right), \quad \textrm{if } t = 1, \\
    \sum_{t = t_c - t_w}^{t_c} \left( e_t + \frac{N_x}{N_z} w^i r_t \right),
    \quad \textrm{otherwise},
    \end{cases}
    \label{eq:LossFunction_interaction}
\end{equation}
%
The adaptive variables $\mathbf{A}_t^\mu$ and $ \mathbf{A}_t^\sigma$ at each time-step $t$ from $t_{c}-t_{w}$ to $t_c$ are modified so as to minimize $\mathcal{F}^{int}$ by iterating the forward computation and the error back-propagation through time for a fixed number of times, called \emph{epochs}.
After modifying the approximate posterior by going through \emph{epochs} of iteration, next-step joint angles are predicted and fed into the PID controller of the robot in order to generate its movement.
Then, the past window is shifted one step ahead.
$w^i$, which is the meta-prior used in the interaction phase, could be set with different values from $w^t$ used in the training phase.
In \cite{ohata2020investigation}, a simulation experiment using a PV-RNN model shows that when a PV-RNN trained with a particular $w^t$ was reset with a smaller $w^i$ in the later interaction phase, the approximate posterior shifted away from the prior and the PV-RNN tended to adapt to the observed sensory sequence. 
This means that top-down intention in the PV-RNN became weaker.
On the other hand, when reset with a larger $w^i$, the PV-RNN tended to ignore the observed sensory sequence by generating its own intended patterns, which means that top-down intention becomes stronger.
However, the precision structure in the prior does not change even as $w^i$ is changed, since the prior distribution was developed in the learning phase.


% The original version of this sub-section is saved in controller.2-21.tex
\subsection{The employed robot controller}
The current study uses a humanoid robot, Torobo \footnote[1]{Humanoid-robot Torobo from Tokyo Robotics: \url{https://robotics.tokyo/products/torobo/}} to conduct human-robot kinesthetic interaction experiments.
Torobo is equipped with a built-in force-feedback controller that enables humans to back-drive joint angles with subtle force.
In the Torobo control system, when a human exerts certain torque on joints by pushing or pulling the limbs of Torobo, the exerted torque can be estimated by subtracting the torque inferred as necessary to account for the current static state as well as a dynamic state of the robot from the actual torque measured in the joints.
By computing the next time-step joint target positions by adding the current positions with the estimated exerted torque multiplied by a constant gain and feeding them in the PID controller, Torobo's limbs move by following the force exerted on them by the human.
This force-feedback controller is integrated with the PV-RNN, which generates the next time-step target joint angles (Fig.\ref{fig:controller}).
% Fig.2
\begin{figure*}[tbph]
  \centering
  \includegraphics[width=0.99\textwidth]{figures/schematics_of_experiment.png}
  \caption{the human interacting with Torobo, the PV-RNN target joint angle generator, the inverse model, and the PID joint controller.}
  \label{fig:controller} 
\end{figure*}

The overall control diagram is shown in Fig.\ref{fig:controller}, which includes a human experimenter interacting with Torobo, the PV-RNN target joint angle generator, the inverse model, and the PID joint controller.
The inverse model and PID controller were developed by the manufacturer of Torobo.
Details are as follows.
Excess torque, $\tau_t^{exc}$ which is torque exerted on joints by the human, can be estimated by extracting $\tau_t^{dynm}$ as the torque inferred for the current position, velocity, and acceleration of the joints by using the inverse model of Torobo from the current measured torque, $\tau_t$.
Next, excess torque, $\tau_t^{exc}$ is applied with a threshold control and $e_t$ as excess torque after threshold control is obtained.
Then, $e_t$ is time-filtered with the decay parameter $0< \alpha <1$, which generates $\tilde{e_t}$ as the time-filtered excess torque.
These operations are necessary to prevent unnecessary overreaction of the torque estimation against noise. 
These lines of processes are described in the following mathematical form:
% Eq. 16
\begin{equation} 
\begin{aligned}
\label{eq:ExcessTorque}
    \tau_t^{exc} &= \tau_t - \tau_t^{dynm}, \\
    e_t &= \begin{cases}
\max(0, \tau_t^{exc} - \tau^{th}), \quad \textrm{if } \tau_t^{ext} > 0,\\
\min(0, \tau_t^{exc} + \tau^{th}), \quad \textrm{otherwise},
\end{cases} \\
    \tilde{e_t} &=
    \begin{cases}
    e_t, \quad \textrm{for } t = 1,\\
    \max \Bigl( \alpha \times \tilde{e}_{t-1} + e_t, e_{max} \Bigl), \quad \textrm{otherwise},
    \end{cases}
\end{aligned}
\end{equation}
%
where $\tilde{e_t}$ has a fixed upper limit $e_{max}$.

The PV-RNN predicts $\bar{\theta_t}$ next time-step joint angles while performing the online inference, which is subtracted by $\theta_t$ as the current joint angle to generate $\Delta \theta_t$, the target joint angle difference shown as:
% Eq. 16
\begin{equation}
    \Delta \theta_t = \bar{\theta_t} - \theta_t
\end{equation}
Finally,  $\Delta \theta_t$ representing the next move intended by the PV-RNN and $\tilde{e}_t$ representing the estimate of torque exerted by the human are multiplied by each gain $k_r$ and $k_p$, respectively and they are added to the current joint angles to generate the final target joint positions, $\bar{\theta_t}$, which are fed into the PID controller as shown in the following:
% Eq. 16
\begin{equation}
\label{eq:target joint angle}
    \accentset{\ast}{\mathbf{\theta}}_{t+1} = \Delta \theta_t \times k_r + \tilde{e}_t \times k_p + \theta_t,
\end{equation}



