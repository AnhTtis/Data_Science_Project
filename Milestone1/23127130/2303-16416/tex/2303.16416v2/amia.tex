\documentclass{amia}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{booktabs}
\setlength{\bibsep}{0pt} %Comment out if you don't want to condense the bibliography

\begin{document}


\title{Zero-shot Clinical Entity Recognition using ChatGPT}

\author{Yan Hu$^1$, Iqra Ameer$^1$, Xu Zuo,$^1$, Xueqing Peng$^2$, Yujia Zhou$^1$, Zehan Li$^1$, Yiming Li$^1$, Jianfu Li$^1$, Xiaoqian Jiang$^1$, Hua Xu$^2$ }

\institutes{
    $^1$ School of Biomedical Informatics, University of Texas Health Science at Houston, Houston, USA \\
    $^2$ Section of Biomedical Informatics and Data Science, School of Medicine, Yale University, New Haven, USA
}

\maketitle

\section*{Abstract}

\textit{In this study, we investigated the potential of ChatGPT, a large language model developed by OpenAI, for the clinical named entity recognition task defined in the 2010 i2b2 challenge, in a zero-shot setting with two different prompt strategies. We compared its performance with GPT-3 in a similar zero-shot setting, as well as a fine-tuned BioClinicalBERT model using a set of synthetic clinical notes from MTSamples. Our findings revealed that ChatGPT outperformed GPT-3 in the zero-shot setting, with F1 scores of 0.418 (vs.0.250) and 0.620 (vs. 0.480) for exact- and relaxed-matching, respectively. Moreover, prompts affected ChatGPT’s performance greatly, with relaxed-matching F1 scores of 0.628 vs.0.541 for two different prompt strategies. Although ChatGPT's performance was still lower than that of the supervised BioClinicalBERT model (i.e., relaxed-matching F1 scores of 0.620 vs. 0.888), our study demonstrates the great potential of ChatGPT for clinical NER tasks in a zero-shot setting, which is much more appealing as it does not require any annotation. }

\section*{1 Introduction}

Electronic health records (EHRs) contain a vast quantity of unstructured data, including clinical notes, which can offer valuable insights for patient care and clinical research \cite{jensen2012mining}. However, manually extracting pertinent information from clinical notes presents a challenge, as it is labor-intensive and time-consuming. To address these challenges, researchers have developed various natural language processing (NLP) techniques for automating the clinical information extraction process. Clinical named entity recognition (NER) is a critical clinical NLP task focusing on recognizing boundaries of clinical entities (i.e., words/phrases) and determining their semantic categories, such as medical problems, treatment, and tests \cite{nadkarni2011natural}. With the help of advancements in clinical NER, the time and effort required for manual chart review and coding by health professionals can be significantly reduced, thus improving patient care efficiency and accelerating clinical research \cite{neveol2018clinical}.

Early clinical NER systems are often dependent on predefined lexical resources and syntactic/semantic rules derived from extensive manual analysis of text \cite{wang2018clinical}. Over the past decade, machine learning-based approaches have gained popularity in clinical NER research \cite{huang2015bidirectional}. Current popular clinical information extraction systems, such as cTAKES and CLAMP, are hybrid systems that integrate rule-based and machine learning-based techniques \cite{savova2010mayo}. Nevertheless, a bottleneck in building machine learning-based clinical NER models is to develop large, annotated corpora, which often require domain experts and take a long time to build. More recently, transformer-based large language models have emerged as the leading method for developing clinical NLP applications. Bidirectional Encoder Representations from Transformers (BERT) is a widely used pre-trained language model that learns contextual representations of free text \cite{devlin2018bert}. Utilizing BERT as the foundation, domain-specific language models like BioBERT, PubMedBERT (trained on biomedical literature), and ClinicalBERT (trained on the MIMIC-III dataset) have been further developed \cite{lee2020biobert,gu2021domain,huang2019clinicalbert}. These models have been applied to clinical NER tasks via transfer learning (i.e., fine-tuning the models on clinical NER corpora), and have shown improved performance with fewer annotated samples \cite{lee2020biobert,gu2021domain,huang2019clinicalbert}.

Generative Pre-trained Transformers (GPT) represent another type of large language model capable of generating human-like responses based on textual input. In November 2022, OpenAI unveiled ChatGPT \cite{openai}, a groundbreaking language model that quickly garnered interest from researchers and technology enthusiasts. As an extension of GPT-3, one of the most advanced NLP models at the time, ChatGPT (or GPT-3.5) serves as a conversational agent adept at following complex instructions and generating high-quality responses across various scenarios. Besides its conversational skills, ChatGPT has exhibited remarkable performance in many other NLP tasks, such as machine translation and question-answering \cite{bang2023multitask}, even in the zero-shot or few-shot learning scenarios \cite{brown2020language}, where the model can be applied to new tasks without any fine-tuning or with fine-tuning using a very small amount of data.

As interest in ChatGPT continues to surge, numerous studies are currently exploring the vast array of possibilities offered by this innovative language model, including those in the medical domain. One prominent example of ChatGPT for medicine is that it passed the US medical license exam with about 60 accuracy \cite{gilson2023does}. More applications of ChatGPT and GPT-4 in healthcare have also been discussed, such as clinical decision support and clinical documentation assistance \cite{kung2023performance,rao2023evaluating,antaki2023evaluating,jeblick2022chatgpt,peter_goldbert_kohane_2023}. With those motivations, this study aims to investigate ChatGPT's potential for clinical NER tasks in a zero-shot setting (i.e., without using annotated samples for model fine-tuning). 

The primary contributions of this work include: (1) establishing a benchmark to evaluate ChatGPT's performance in zero-shot clinical NER tasks, consisting of HPI (History and Present Illness) sections from 100 synthetic discharge summaries collected from the MTSamples dataset \cite{MTSamples}; (2) comparing the performance of ChatGPT and GPT-3 in the similar zero-shot setting, as well as a supervised model based on the domain-specific large language model, BioClinicalBERT, on this benchmark dataset; (3) testing two prompt strategies and analyzing the effects of these prompts on ChatGPT's performance; and (4) conducting error analysis to provide insights for future work.



\section*{2 Methods}
\subsection*{2.1 Task Overview}
This study aims to assess the zero-shot capability of ChatGPT in the clinical NER task, as defined in the 2010 i2b2 challenge \cite{uzuner20112010}. We compared the performance of ChatGPT and GPT-3 in a similar zero-shot setting and included a baseline model, BioClinicalBERT, which was trained on the 2010 i2b2 dataset detailed below. The primary workflow of our investigation is depicted in Figure~\ref{fig:workflow}. Two different prompts were crafted to identify three types of clinical entities: Medical Problem, Treatment, and Test from clinical text using both ChatGPT and GPT-3. Additionally, we trained a supervised  BioClinicalBERT model using an annotated corpus from the 2010 i2b2 challenge, as a baseline. All three models were then evaluated using an annotated corpus consisting of HPI sections from 100 discharge summaries in the MTSamples collection (see next section).

\begin{figure}[H]
  \centering
    \includegraphics[width=1.0\textwidth]{workflow}
 \caption{The overall workflow of this study. The highlighted part in yellow shows the zero-shot NER workflow using ChatGPT and GPT-3. The grey part illustrates the BioClinlcalBERT-based fine-tuning workflow.}
 \label{fig:workflow}
\end{figure}

\subsection*{2.2 Dataset}
The study utilized two datasets: (1) a collection of 292 annotated, de-identified discharge summaries from the 2010 i2b2 challenge \cite{li2021synthetic}, and (2) a set of 100 annotated discharge summaries from MTSamples, which is a collection of synthetic clinical notes, following the same 2010 i2b2 shared task annotation guidelines. These guidelines aim to identify three types of entities: Medical Problem, Treatment, and Test. To simplify the task and to make sure the text length is within the scope of ChatGPT, we used text in the HPI section only. HPI sections from the 292 notes from the 2010 i2b2 challenge were used for supervised NER model training (see below). HPI sections from the 100 MTSamples notes were used for performance evaluation for all models. Table~\ref{tab:Statistics} presents the statistics for these datasets. 

\begin{table}[H]
\begin{center}
\begin{tabular}{lll}
\toprule
Entity	            &i2b2 (HPI sections from 292 notes)	&MTSamples (HPI sections from 100 notes) \\
\midrule
Medical Problem                       &3579	    &983                 \\
Treatment	            &1383	    &256                 \\
Test                      &1159        &247                 \\
Total	                    &6121      &1486               \\
\bottomrule
\end{tabular}
\end{center}
\caption{Statistics of the datasets used in this study.}
\label{tab:Statistics}
\end{table}

\subsection*{2.3 Models}
\subsubsection*{2.3.1 Supervised Learning}
To compare ChatGPT with conventional supervised learning, we trained a NER model on the i2b2 dataset using BioClinicalBERT \cite{alsentzer2019publicly}. We present two sets of results for supervised learning: (1) results of 10-fold cross-validation on the i2b2 dataset itself and (2) results on the 100 MTSamples test set. We initialized the model weights using the transformers package available at https://huggingface.co/emilyalsentzer/Bio\_ClinicalBERT \cite{wolf2020transformers}. The hyperparameters employed during model training included a learning rate of 5e-5, a batch size of 8, 20 epochs, and a weight decay of 0.01 using the AdamW optimizer \cite{loshchilov2017decoupled}.

\subsubsection*{2.3.2 Zero-shot Learning}
In zero-shot learning, we directly input the HPI text from the MTSamples dataset into ChatGPT and GPT-3 and instructed both models to extract entities.

\paragraph{Prompt design: }Based on our preliminary experiments, we noticed that ChatGPT tends to rephrase extracted terms. To prevent this, we instructed ChatGPT not to rephrase any extracted terms and to present them in a list format, which simplifies the process of collecting answers. During our experiments, we tested two versions of prompts for each entity type. The first version included only the entity types (e.g., medical problems, treatments, and tests), while the second version added additional information about three types of entities (e.g., synonyms and specific subtypes). The final prompts used for NER are displayed in Table~\ref{tab:Prompts}. We also evaluated GPT-3's zero-shot learning capabilities by inputting the same notes, but with prompts tailored specifically for GPT-3.

\begin{table}[h]
\begin{tabularx}{\textwidth}{l|X|X}
\toprule
Entity	            &Prompt-1	&Prompt-2 \\
\hline
Medical Problem                       &Extract without rephrasing all medical problem entities from the following note in a list format: 	    &Extract without rephrasing all medical condition, diagnosis, medical problem, medical symptom entities from the following note in a list format:             \\
\hline
Treatment	            &Extract without rephrasing all treatment entities from the following note in a list format:	    &Extract without rephrasing all medical treatment, medical procedure, medical intervention, medication, drug entities from the following note in a list format:              \\
\hline
Test                     &Extract without rephrasing all test entities from the following note in a list format:       &Extract without rephrasing all vital signs, laboratory test, medical test, imaging study, diagnostic test entities from the following note in a list format:                 \\
\bottomrule
\end{tabularx}
\caption{Prompts for ChatGPT used in this study.}
\label{tab:Prompts}
\end{table}


\paragraph{Answer Collection: }The results generated by ChatGPT were manually gathered through the online interface by our team members on February 22nd, 2023, as the ChatGPT API had not been released at that point. To obtain NER results for each note, the prompts listed in Table~\ref{tab:Prompts} were employed to guide ChatGPT's extraction process. We initiated a new thread for every prompt, preventing the dialogic history from potentially affecting ChatGPT's replies. Furthermore, each entity was input on a separate line within a single cell of an Excel spreadsheet. In cases where the output produced no entity, the corresponding cell was left empty without copying the output, allowing for subsequent performance evaluation.

\subsection*{2.4 Evaluation}
We used Precision (P), Recall (R), and micro-averaged F1 scores to assess the performance of the models. We calculate these scores using both exact- and relaxer-match criteria. For an exact match, an extracted entity must possess the same tokens and entity type as the gold standard to be deemed a True Positive. As ChatGPT is a generative model, the produced entities may vary in terms of entity boundaries or phrasing. To avoid penalizing the model for such differences, we also evaluate the results using relaxed match criteria. Under this approach, an extracted entity that overlaps text and shares the same entity type as the gold standard is considered a True Positive.\\
To ensure that stop words do not influence the evaluation, we remove all stop words from both the gold standard and ChatGPT outputs. Moreover, we exclude all duplicate entities from the assessment, as the same entity may appear multiple times within a document; but ChatGPT outputs it only once.

\section*{3 Results}
\subsection*{3.1 Supervised learning}
The performance of NER models utilizing BioClinicalBERT, assessed on 10-fold cross-validation on the i2b2 dataset and the MTSamples test set, is presented in Table~\ref{tab:performance1}. The i2b2 dataset produced overall F1 scores of 0.884 and 0.939 for exact- and relaxed-match, respectively. However, when applying the model directly to the MTSamples test set, there was a significant drop in performance, resulting in F1 scores of 0.806 and 0.888 for exact- and relaxed-match, respectively.


\begin{table}[H]
\begin{center}
\begin{tabular}{llllllll}
\toprule
& &\multicolumn{3}{c}{Exact Match} &\multicolumn{3}{c}{Relaxed match} \\
\cmidrule(lr){3-5}\cmidrule(lr){6-8}
Dataset &Entity &Precision &Recall &F1 &Precision &Recall &F1 \\
\midrule
\multirow{4}{7em}{i2b2 10-fold CV} &Problem &0.892 &0.891 &0.891 &0.944 &0.940 &0.942 \\
 &Treatment &0.864 &0.865 &0.865 &0.938 &0.932 &0.935 \\
 &Test &0.879 &0.897 &0.888 &0.924 &0.944 &0.934 \\
 &\textbf{Overall} &\textbf{0.882} &\textbf{0.885} &\textbf{0.884} &\textbf{0.939} &\textbf{0.939} &\textbf{0.939} \\
\midrule
\multirow{4}{8em}{MTSamples} &Problem &0.829 &0.851 &0.840 &0.912 &0.931 &0.922 \\
 &Treatment &0.734 &0.732 &0.733 &0.837 &0.827 &0.832 \\
 &Test &0.826 &0.696 &0.755 &0.894 &0.753 &0.818 \\
 &\textbf{Overall} &\textbf{0.810} &\textbf{0.802} &\textbf{0.806} &\textbf{0.895} &\textbf{0.881} &\textbf{0.888} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Performance of BioClinicalBERT-based supervised learning model on two datasets.}
\label{tab:performance1}
\end{table}

\subsection*{3.2 Zero-shot learning}
The NER results of ChatGPT (with two versions of prompts) and GPT-3 are summarized in Table~\ref{tab:performance2}. The results reveal that both ChatGPT and GPT-3 exhibited considerably lower performance under exact match evaluation, when compared with that of BioClinicalBERT. GPT-3 achieved an overall F1 score of 0.250; while ChatGPT outperformed GPT-3 with overall F1 scores of 0.311 and 0.418 for the two prompts, respectively.\\
Both of the models demonstrated significant improvement under relaxed-match evaluation. GPT-3 achieved an overall F1 score of 0.48; while ChatGPT achieved an overall F1 score of 0.529 and 0.639 for two different prompts. Prompt-2 yields a higher overall F1 score, with an increase of over 0.107 in exact-match criteria, compared to prompt-1. This underlines the importance of prompt design and the effectiveness of incorporating more appropriate explanations of entity types into the prompt.

\begin{table}[H]
\begin{center}
\begin{tabular}{llllllll}
\toprule
& &\multicolumn{3}{c}{Exact Match} &\multicolumn{3}{c}{Relaxed match} \\
\cmidrule(lr){3-5}\cmidrule(lr){6-8}
Model &Entity &Precision &Recall &F1 &Precision &Recall &F1 \\
\midrule
\multirow{4}{8em}{GPT-3} &Problem &0.447 &0.299 &0.358 &0.692 &0.475 &0.563 \\
 &Treatment &0.174 &0.335 &0.229 &0.407 &0.599 &0.485 \\
 &Test &0.057 &0.152 &0.083 &0.225 &0.443 &0.298 \\
 &\textbf{Overall} &\textbf{0.225} &\textbf{0.281} &\textbf{0.250} &\textbf{0.467} &\textbf{0.493} &\textbf{0.480} \\
\midrule
\multirow{4}{8em}{ChatGPT (Prompt-1)} &Problem &0.441 &0.459 &0.450 &0.640 &0.689 &0.664 \\
 &Treatment &0.261 &0.479 &0.337 &0.472 &0.747 &0.578 \\
 &Test &0.065 &0.296 &0.106 &0.169 &0.578 &0.261 \\
 &\textbf{Overall} &\textbf{0.242} &\textbf{0.435} &\textbf{0.311} &\textbf{0.433} &\textbf{0.681} &\textbf{0.529} \\
\midrule
\multirow{4}{8em}{ChatGPT (Prompt-2)} &Problem &0.480 &0.558 &0.516 &0.627 &0.735 &0.677 \\
 &Treatment &0.211 &0.514 &0.299 &0.382 &0.798 &0.516 \\
 &Test &0.260 &0.222 &0.239 &0.636 &0.509 &0.565 \\
 &\textbf{Overall} &\textbf{0.364} &\textbf{0.492} &\textbf{0.418} &\textbf{0.552} &\textbf{0.708} &\textbf{0.620} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Zero-shot performance of ChatGPT and GPT-3 on the MTSamples test set.}
\label{tab:performance2}
\end{table}

\subsection*{4 Discussion}
Our results highlight the promising potential of ChatGPT in a zero-shot clinical NER task, with a relaxed-match F1 of 0.626. Although it still achieved lower performance than that of the supervised learning model, it is very appealing as no additional annotation is needed. To further understand situations where ChatGPT's performance is suboptimal, we conducted an error analysis by manually reviewing and categorizing errors from ten test files (see Table~\ref{tab:error}).\\
The analysis revealed that false positive (FP) errors were the most prevalent issue, constituting 80.30\% of total errors, while false negatives (FN) accounted for 19.70\%. Most errors occurred when ChatGPT extracted unreasonable entities such as “Dr. X” or “Mr. ABC” as test entities or occasionally overlooked entities like “fever” as a problem entity. Among all FPs, 42.45\% errors stemmed from ChatGPT's inability to accurately classify entity types. For instance, “CT scan”, was occasionally misclassified as a medical problem instead of a test.\\
Additionally, we observed that ChatGPT sometimes attempts to infer or summarize information in the notes. In the example of “The patient is a 17-year-old female who presents to the emergency room with foreign body and airway compromise and was taken to the operating room. She was intubated and fishbone.”, ChatGPT outputted an entity of "difficulty breathing", even though it was not mentioned in the input text at all. Another noteworthy FP error occurred when abnormal laboratory test results, like "a blood sugar level of 40" and "white blood cell count of 23,500", were classified as a medical problem. Although these abnormal lab tests could indicate medical problems, they were not annotated following the i2b2 shared task guidelines. In such cases, it is debatable if we should penalize ChatGPT for such output.\\
Despite instructing ChatGPT not to rephrase extracted entities, it occasionally fails to adhere to this instruction. In the example of "the patient eventually became unresponsive", ChatGPT outputted "unresponsiveness" as a medical problem, instead of "unresponsive", which is annotated in the gold standard.  These “errors” accounted for 9.43\% of total FPs and 19.23\% of total FNs, indicating the actual performance of ChatGPT could be better if we count them as correct outputs.\\

\begin{table}[H]
\begin{center}
\begin{tabular}{llllll}
\toprule
Error types	&Number of occurrences	&Percentage	&Reason	&Number of occurrences	&Percentage \\
\midrule
\multirow{3}{8em}{Wrong extraction (FP)}	&\multirow{3}{0em}{106}	&\multirow{3}{0em}{80.30\%}	&Wrong Type	&45	&42.45\% \\
 & & &Rephrase	&10	&9.43\% \\
 & & &Other	&51	&48.11\% \\
 \midrule
\multirow{2}{8em}{Missed entity (FN)}	&\multirow{2}{0em}{26}	&\multirow{2}{0em}{19.70\%}	&Rephrase	&5	&19.23\% \\
 & & &Other	&21	&80.77\% \\
\bottomrule

\end{tabular}
\end{center}
\caption{Summary of different error types.}
\label{tab:error}
\end{table}

We noticed that ChatGPT struggled to extract co-reference entities like "her medications" or "her symptoms", which should be annotated in accordance with the 2010 i2b2 annotation guidelines, for coreference identification purposes. After we removed those co-reference entities in the gold standard and re-evaluated the performance of both ChatGPT and GPT-3, we observed modest increases in performance, with ChatGPT achieving an F1 score of 0.628 using Prompt-2 and GPT-3 attaining an F1 score of 0.500 in the relaxed-match criteria.\\
Moreover, we observed a significant degree of randomness in ChatGPT's output. Even when presented with the same prompt and the same input text, it sometimes generated responses with considerable differences in format and content. This phenomenon was particularly prevalent when the input note was lengthy, despite our efforts to minimize input sequence length by limiting it to the HPI section. We anticipate this issue will be addressed when GPT-4 allows much longer text. \\
Although it is not clear whether clinical corpora (and what types of clinical corpora) are used in training ChatGPT, ChatGPT has demonstrated its understanding of the medical text to a certain degree. We believe fine-tuning ChatGPT with domain-specific corpora, assuming OpenAI will provide such an API, will further improve its performance on clinical NLP tasks such as NER in the zero-shot fashion.

\subsection*{5 Conclusion}
In this study, we evaluated the capabilities of ChatGPT in executing a zero-shot clinical NER task defined in the 2010 i2b2 challenge, by comparing its performance with GPT-3 and BioClinicalBERT-based models. Our findings show that while ChatGPT's zero-shot performance is not as good as that of fine-tuned BioClinicalBERT-based clinical NER model, it achieves reasonable performance (an F1 of 0.628 in relaxed matching) and demonstrates a remarkable improvement over GPT-3. Furthermore, we identified and categorized various error types and provided insights for future improvement.

\subsection*{Acknowledgments}
This study is partially supported by the following grants: NIA 1RF1AG072799 and NIA 1R01AG080429.

\subsection*{Conflict of Interest}
Dr. Hua Xu and The University of Texas Health Science Center at Houston have research-related financial interests in Melax Technologies, Inc.

% References as numbers
\makeatletter
\renewcommand{\@biblabel}[1]{\hfill #1.}
\makeatother

% unstr is used to keep citation order
\bibliographystyle{vancouver}
\bibliography{amia}  

\end{document}
