\section{Coaching \& Match Strategy}
\label{sec:coaching}

We first discuss multiagent challenges related to sports analytics in the context of coaching.
Coaches rely on information about player, group, and opponent performances to make decisions about who plays what positions within each match.
Related multiagent problems include optimizing team arrangement, player and group valuation, and opponent prediction and modeling.
The ability to provide quantitative support to make line-up or team strategy decisions is an open problem in sports analytics.
We propose the following high-level research questions:
% \noindent
\textbf{RQ1}: How can multiagent research help coaches identify good combinations of players in the context of opponents and development?
% \noindent
% \textbf{RQ2}: How can game theory and strategy prediction help inform coaching behaviors both before and during games?
\textbf{RQ2}: How can the temporal and inter-agent complexity of in-match problems within invasion games support future MAS research?


% where MAS can have a direct impact.

% \begin{enumerate}
%   \item ``Coaching" - Formation and Strategy
%   \begin{itemize}
%      \item Formation prediction for other team
%      \item Formation generation for own team (i.e., who plays best together), special teams.
%      \item Evaluating/predicting best match-ups of players/lines
%      \item Allocation of training/game playing time (keep players performing best, motivation, COMSOC problem?, etc...)
%      \item Injury prediction/load management \todo{(can this relate to multiagent?)}
%      \item Player Development (Learn from the common actions of the best players) \todo{Could this be its own section? Maybe not enough variety.}
%   \end{itemize}
% \end{enumerate}


% \subsection{Contextual State and Player Valuation}
\subsection{Team Arrangement}
% \label{sec:state_value}

% \todo{coalition structure generation??}


% In team sports analytics, one popular area of research focuses on evaluating the contributions of different players.
Understanding how to best utilize a group of players requires an understanding of multiagent interaction.
A team must operate as a cohesive group of individual players, often divided into sub-teams with various roles or positions (i.e., defense or forwards).
In sports such as ice hockey where groups of players are replaced at high frequency (i.e., players change about once per-minute), the goals of disparate groups that are used in the same position may be heterogeneous (i.e., scoring forwards versus defensive forwards).
% For example, one group of forwards may be highly skilled and relied upon to score goals, while another group may be better at defending against an opponent's scorers.
Coaches are tasked with assessing the value of their players in continuously dynamic contexts to decide which players fit into particular roles and which sub-groups perform best together.
This is an online coalition formation problem: constructing sub-groups within their team to satisfy various types of goals conditioned on how players respond to their peers and game context~\cite{rahwan2015coalition}.

Existing coalition formation or team forming algorithms usually compose agent groups with the goal of maximizing reward, fairness, or robustness to failure~\cite{schwind2021partial}.
Agent policies in these environments are typically assumed to be static~\cite{andrejczuk2018composition}.
%  in environments typically assumed to be static
% The goals and rewards in existing CSG implementations are typically fully observable and well defined, unlike the goals for sub-groups of players that coaches may define.
% \newtext{Recent work in Multiagent Reinforcement Learning (MARL) has showed the importance of sub-team structure on the success of an overall population~\cite{radke2022importance,Radke2022Exploring}.}
These algorithms could be adapted to team sports to measure some notion of expected performance; however, fully capturing the complexity of team sports for coalition formation requires considering dynamically changing environment states, opponent policies, and under-specified goals.
% that are difficult to directly measure.
% Adapting coalition formation to the context of team sports will require models that can value groups despite potentially under-specified goals that are more difficult to measure than pure reward feedback.
For example, groups that are desired to maximize defensive play (instead of offense) must consider opponent strength and properly allocate credit to defensive actions.
% players responsible for defense.
% (i.e., groups with better defensive play).
% Furthermore, the environments that players operate in may not be comparable (i.e., opponent difficulties are different), meaning generalizability and performance prediction are emphasized in the context of this CSG problem.
% Recent breakthroughs with RL have shown that AI has the ability to learn complex behaviors despite under-specified constraints \todo{(CITE GT SOPHY)}.
Future algorithms will need to understand the value of different actions and be generalizable to different rosters, opponents, and goals.
% while also being interpretable.

\noindent
\ding{229} Coalition Formation \\
\ding{229} Teamwork and Team Forming


\subsection{Player and Group Valuation}
\label{sec:state_value}


% Another problem space is providing more contextual player performance metrics to provide more context for coaches to form groups of their own.
Traditional components of player valuation fail to consider full contributions players make to team success (i.e., mostly offensive metrics, even for defensive players).
Newer approaches have focused on learning the value of passes, carries, shots, or full trajectories, providing more context to the decision making of players or groups~\cite{beal2020learning,Radke2021Passing,fernandez2021framework,ritchie2022pass}.
However, agents' abilities are often a direct consequence of their teammates' impacts on the game state~\cite{goes2019not}.
% However, the value of a player's decisions are not solely dependent on themselves, as their teammate's actions impact how an opponent team moves in relation to the current game state~\cite{goes2019not}.
% Existing methods to measure a player's marginal value include ``above replacement'' metrics to calculate how an individual compares to the \emph{average} player.
% Some examples include ``goals above replacement'' (GAR) and ``wins above replacement'' (WAR), which compare the number of goals/offense and wins a player generates compared to if they were replaced by an average player.
% While being easy to understand, the definition of ``average'' across different sports can lead to increasing uncertainty and inaccuracies.
% For example, ``above replacement'' models assume every player in a particular position acts in identical state spaces even though players of the same position may have different goals or play against more difficult opponents.
While all player movement and locations are easily captured with tracking data, learning the value of inter-agent and off-ball/puck movement is an active area of research~\cite{spearman2018beyond,raabe2022graph}.
We emphasize that players must be evaluated not only by their individual actions, but in the context of their team, opponents, and game situations.

% The value of teamwork has been measured through the outcomes of various trajectories~\cite{beal2020learning}.
Learning the value of a player within a larger context requires a rich understanding of high-dimensional inter-agent interactions and causal reasoning (i.e., off-ball/puck movements).
% Assessing the value of individual actions that impact future states are important challenges when determining the marginal value of agents within a group, an important area in multiagent research.
% As a result, situational context is typically lost when evaluating players at an individual level; however, an important area of multiagent research relies on estimating marginal value in different scenarios.
% problem is similar to the existing MARL research problems to identify marginal contribution to a group.
Estimating the marginal value of agents in AI is important for credit assignment, group cohesion, and developing effective joint policies.
Some existing models use a supervised component to learn marginal contribution~\cite{rashid2018qmix}, while others rely on methods from cooperative game theory~\cite{derks1993shapley,Yan2020EvaluatingAR}; however, they mainly function in online settings.

For multiagent methods that estimate marginal contribution to be effective in sports analytics, they must perform offline policy evaluation from data.
Models must also consider the potential for different players to develop better joint policies if they were to play together.
% For a learned marginal contribution model to be effective, it must consider the contributions of all agents on the dynamics of the underlying environment.
This means understanding player types and contributions from data separate from easily identifiable signals such as scoring goals; for instance, identifying skilled defensive players.
% is difficult to extract from data but is significant to team success.
Identifying alternative goals and multiple tasks, such as beneficial state-action pairs which may not produce reward themselves, is already an important research problem in sparse reward and offline environments~\cite{arjona2019rudder}, although mostly explored in single-agent scenarios.
Developing new AI solutions in this direction requires modeling poorly specified rewards, types of goals, and action causal/counterfactual reasoning with multiple agents.
% This will also lead to new models to predict emergent behavior between players that can evolve a more effective joint policy together.
% group agency.
% in this direction will include developing AI algorithms to identify alternative contributions to group success beyond just direct reward-generating actions.
Through this development of new AI algorithms, credit assignment and value decomposition models will be directly improved to better estimate marginal value.
Relaying this information to coaches would have a direct impact on team strategy, groupings, and group agency or joint policies.


\noindent
% \ding{229} Cooperative Game Theory \\
\ding{229} Learning Agent Capabilities \\
\ding{229} Emergent Behavior %Multi-task Learning



% In reinforcement learning (RL), model-free agents construct an internal representation of the value for taking an action in a particular state (i.e., the $Q$-value), and the general value of a state (i.e., the value function).
% Estimating the value of a state becomes increasingly difficult in multiagent RL (MARL) as more agents are added to the environment due to environmental nonstationarity.
% Developing methods for agents to value their individually observable state, and the global state of a system, are ongoing topics of research within the multiagent and broader AI communities.
% Some models use a supervised component to learn the marginal contribution of component agents in a cooperative environment (CITE QMIX), while others rely on methods from cooperative game theory to assess marginal contributions (CITE SHAPLEY).

% Given access to a global environmental state (i.e., fully observable), trained models for reward decomposition are able to identify how much of a global reward agent's should receive for their actions.
% In team sports, even though individual players score goals, the benefits (i.e., rewards) are shared among the team.
% Developing more intelligent methods to identify how much individual players contribute to this shared team-based reward is a challenging research problem in which existing MARL approaches could make some significant gains.
% Furthermore, learning models mixed with game theoretical solution concepts such as the Shapley Value could offer some further transparency behind the reasoning of certain outputs.
% state value
% qmix/reward decomposition
% game theory/shapley







\subsection{Opponent Prediction and Strategy}
\label{sec:opponent_modeling}

% Free-flowing team sports create numerous multiagent problems within the actual gameplay.
Sports teams execute team-based strategies and systems in an attempt to outperform opposing teams.
% For example, teams may execute different defense strategies in ice hockey or different player formations in soccer.
% Predicting another team's formation or strategy and composing a best response strategy is inherently a multiagent prediction problem that is typically delegated to coaching staffs.
% This becomes more difficult considering that an opponent's formation could change at any moment in response to the current state of the game (i.e., time or score) or formation of one's own team.
Predicting opponent team formations and strategies is a challenging problem typically delegated to coaching staffs.
Providing quantitative support for this challenge requires a rich understanding of agent-based modeling, prediction, coordination, and identifying tendencies~\cite{visser2000recognizing,lucey2012characterizing}.
These problems are reminiscent of opponent modeling and planning, such as work done within the popular multiagent RoboSoccer domain~\cite{pourmehr2011overview,stone2000defining,ledezma2009ombo}.
Further AI development in this direction must consider multiple layers of complexity such as game state, individual player match-ups, player availability, and team-wide risk.
% that may all be dynamic.
% ; however, strategy prediction models must simultaneously consider single opponent prediction and team-wide strategy prediction where multiple agents coordinate their behavior.


There may also be situations where an opponent's individual incentives may diverge from their team-based strategy, causing their behavior to stray from their team's strategy based on team alignment and types of goals~\cite{radke2022importance}.
% single agent's best behavioral response may not coincide with the team-wide best response, creating a within-team social dilemma.
% In such situations, an opposing player's behavior may stray some amount from their team's strategy based on their degree of team alignment and personal behavioral incentives for various goals~\cite{radke2022importance}.
% For example, individual players in professional sports have various incentives which may impact their in-game personal policies.
% While team sports must be played by teams of players, each player has their own contract which assigns value to their contributions via salary.
% Thus, different players may follow a team strategy with varying degrees of detail.
% and identifying these scenarios may be of interest to communities studying game theory incentives~\cite{Radke2022Exploring}, group alignment~\cite{schroder2016modeling} and self-organization~\cite{gorodetskii2012self}.
Learning when this may happen requires a rich understanding of agent and team-level modeling around game theory~\cite{Radke2022Exploring}, bounded rationality~\cite{simon1990bounded}, and group alignment~\cite{schroder2016modeling} beyond the current state of those fields.
Utilities and reward-based incentives may not be enough to describe the behavior of players depending on game or group context.
Progress on individual and team-level opponent modeling and strategy prediction will push multiagent research to develop better models of human behavior and strategic decisions within domains with high stochasticity.
These steps forward will directly benefit other multiagent domains such as autonomous vehicles~\cite{fisac2019hierarchical} and strategic reasoning under uncertainty~\cite{van2005logic}.

% make advances in inverse reinforcement learning for predicting value functions, opponent modeling of behavior and credo (degrees of group alignment), and modeling group cohesion or best response strategies.

\noindent
\ding{229} Agent-based Modelling and Simulation: Applications/Analysis
\ding{229} Non-Cooperative Game Theory

% % \subsection{Formation Generation and Modeling}
% \subsection{Team Formations}

% Not only is it important to predict and evaluate opponent team behavior and formations, but understanding how best to utilize one's own team requires an understanding of multiagent interaction.
% Sports teams are typically composed of several sub-teams for position, which can sometimes be divided even further (i.e., individual lines of forwards in hockey).
% We propose two main domains in which approaching team formation as a multiagent problem is ripe for exploration: evaluating sub-teams of players that perform well together and ---.


% While sub-teams significantly influence how players interact and perform, players are often evaluated individually.
% Recent work in Multiagent Reinforcement Learning (MARL) has showed that sub-teams of individual agents perform better than a fully cooperative population~\cite{radke2022importance}, and are even robust to some selfishness~\cite{Radke2022Exploring}.
% Collected in sequential social dilemma environments, these results were shown in situations similar to the individual incentive dilemma players face presented in Section~\ref{sec:opponent_modeling}.
% In team sports, sub-teams may be composed of groups of forwards, midfielders, or defense that all must learn to coordinate within their sub-team to achieve high performance.
% Predicting which groups, or features of groups, lead to better performance is a challenging multiagent problem that must also consider variables such as opponent match-ups (who the sub-team plays against), training time, and how each sub-team is utilized within a game.

% Modeling the groups of forwards or defense that match-up well against the groups of forwards or defense on opposing teams can help optimize in-game strategy (includes Section~\ref{sec:opponent_modeling}).
% Furthermore, some groups of forwards or defense on the same team may perform better together than if other sub-teams were paired (i.e., between-sub-team coordination).
% Considering the training time required for groups of players to reach high performance is necessary when deciding which sub-teams should be modified or kept constant \todo{hind to comsoc here}.
% Beyond training time, the problem also must consider the amount of time and impact a sub-team is expected to have in a game and adjust models accordingly.





