\section{Opportunity Analysis}
\label{sec:ai}

%\TK{THis is the motivation section right? You identify opportunity for speedup if you had AIbest? Maybe rename this to Motivation: Arithmetic Intensity of Ops within CG} 

In this section, we quantitatively analyze the best case\footnote{Meaning the application's best case arithmetic intensity without hardware and compiler constraints} arithmetic intensity ($AI_{best}$), assuming full reuse as the number of floating point multiplications divided by the number of memory accesses in terms of number of elements\footnote{We measure AI\textsubscript{best} in ops/word agnostic of the bit precision. Using ops/byte will only further increase the intensity of deep learning because of the higher bit-precision requirements of HPC applications (4-8x).}. We show that the arithmetic intensity within a single tensor operation is limited in the HPC applications which implies limited reuse inside a \GEMM from an application side. We then demonstrate that the arithmetic intensity of chained Einsums with inter-operation reuse is significantly higher. Finally, we then provide an overview of reuse opportunities in CG.

\subsection{Dense GEMMs in Isolation}

For an Einsum $Z_{m,n}=\sum_{k}A_{m,k}*B_{k,n}$, the ideal case arithmetic intensity (assuming infinite on-chip buffer) is as follows-
\vspace{-2.5mm}
\begin{equation}
    AI_{best} = \frac{Floating\,point\,multiplications}{Compulsory\,memory\,accesses}
\end{equation}
\vspace{-2.5mm}

Total number of matrix multiplications is equal to $M\times K\times N$. Assuming perfect reuse, the $A$ and $B$ matrices need to be read once and the $Z$ matrix needs to be written once which makes the number of memory accesses equal to $M\times K+K\times N+M\times N$.
\vspace{-2.5mm}

\begin{equation}
    AI_{best} = \frac{M\times K\times N}{M\times K+K\times N+M\times N}
\end{equation}

For matrices with high $M$, $N$ and $K$--for example, in DNNs--the arithmetic intensity (multiplications/elements) is high due to a higher order numerator:
\vspace{-2.5mm}

\begin{equation}
    \lim_{M,K,N\to\infty} \frac{M\times K\times N}{M\times K+K\times N+M\times N} = \infty
\end{equation}

\begin{comment}
\begin{equation}
    \lim_{M,K,N\to\infty} \frac{M1\times K1\times N1\times M0\times K0\times N0}{M1K1\times M0K0+M1K1N1\times K0N0+M1K1N1\times M0N0} = \infty
\end{equation}
\end{comment}

This is in marked contrast to operations like Matrix-Vector multiply, whose intensity is invariant of tensor shape:

\begin{equation}
    \lim_{M,K\to\infty} \frac{M\times K}{M\times K+K+M} = \frac{1}{2}
\end{equation}

This low intensity means that the matrix-vector multiply is memory-bound on state-of-the-art accelerators. This is demonstrated by the importance of batch size in deep learning on GPUs and TPUs.

This does not mean that GEMMs are fundamentally high intensity. Notably, in Conjugate Gradient applications, within \GEMM we observe that one dimension is too large and other dimensions are too small which limits the arithmetic intensity. For example, if $M$ is the large dimension and $K=N$:
\vspace{-3.5mm}

\begin{equation}
    \lim_{M\to\infty} \frac{M\times K\times N}{M\times K+K\times N+M\times N} = \reviewme{\frac{N}{2}}
\end{equation}

\reviewme{Basically, in the limit a skewed GEMM approaches an intensity bounded by its small dimensions. For workloads where N <= 16 (as in Conjugate Gradient as we demonstrate), this does not give sufficient reuse to reach the compute bound of any reasonable accelerator microarchitecture, and therefore degrades to memory-bound performance in isolation.} This motivates the need to exploit inter-operation reuse to increase intensity in scientific applications.

%In ~\autoref{alg:cg_einsum}, in line 3, the large dimension is uncontracted ($P.\alpha$) while in line 5, the large dimension is contracted. However, in each \GEMM operation there only one large dimension which limits the intra-operation reuse.

\subsection{SpMM in Isolation}

For an SpMM operation, where A matrix is sparse and compressed in a format such as CSR, the number of multiplications is equal to $(nz_{M=1}+nz_{M=2}+nz_{M=3})\times N$. This is equal to $nnz\times N$, where $nnz$ is the total number of non-zeros in the MK matrix. The minimum number of accesses assuming complete reuse is $2\times nnz+M$ for MK in CSR format. Here we consider MN as uncompressed, thus accesses for that equals $M\times N$. For KN matrix, in CG application, every index of K has atleast one non zero. Thus the number of accesses for KN matrix are $K\times N$. Thus the arithmetic intensity is:
\vspace{-2.5mm}

\begin{equation}
    AI_{best} = \frac{nnz\times N}{2\times nnz+M
+N\times K+M\times N}
\end{equation}

Here, arithemtic intensity is low due to high sparsity ratio which makes $nnz$ of the same order as M and K. Considering M and K to be large and equal as in CG SpMM. Also, the average number of non zeros per row is $nz_{av}$
\vspace{-2.5mm}

\begin{equation}
   \lim_{M,K\to\infty} \frac{nnz\times N}{2\times nnz+M
+2\times M \times N}=\frac{nz_{av}\times N}{2nz_{av}+1+2N}
\end{equation}

This depends on $nz_{av}$ and $N$ but is strictly less than $\frac{N}{2}$.

\insertFigure{aidl}{AI\textsubscript{best} for example GEMMs used popular DNN applications.}


\insertFigure{ai}{AI\textsubscript{best} for (a) Conjugate Gradient skewed Dense GEMM and (b) Conjugate Gradient skewed SpMM operations for CG workloads. N=16, M,K and nnz vary by application.}

\insertFigure{aicg}{AI\textsubscript{best} for full reuse across 'iter' iterations of the CG loop.}




~\autoref{fig:aidl} shows $AI_{best}$ for DNN workloads while ~\autoref{fig:ai} shows the $AI_{best}$ of skewed SpMMs and Dense skewed GEMMs for CG workloads. The $A$ matrix for these workloads is obtained from suitesparse matrix collection~\cite{suitesparse}. %Assuming 32-bit floating point representation, op per byte ratio for DenseGEMMs is $8/4=2$ which is orders of magnitude lower than DNNs thus limiting intra-operation reuse.
Again, this data motivates the need to exploit inter-operation reuse to escape the memory bound.

%\subsection{Spatial Accelerators}

\input{tables/intensity.tex}


\subsection{Inter-Operation Reuse}

%In the ideal case, the arithmetic intensity increases as we reuse the operations on-chip and we reduce the memory bandwidth requirement. The difference is that we avoid counting the back and forth of tensors in and out of memory.

In a multi-Einsum scenario, we can increase intensity if we can avoid writing back the output of the first operation, and similarly avoid reading it in for the second. To show how much impact this can have, in ~\autoref{table:ai} we demonstrate the ideal inter-operation reuse potential for two chains of Einsums. The first is like a GNN and DNN where a new tensor is injected in every operation by the application (i.e., the layer's weights). The second is like a solver where there are only three inputs and one output introduced by the application and intermediate tensors are reused locally between the Einsums.

Without inter-operation reuse and with execution at the operation granularity, the denominator which is the number of words, always consists of three tensors for every operation. Thus in the above example, with 5 Einsums, the denominator has 15 terms without inter-operation reuse. With inter-operation reuse, we only count each input once and the final output of the Einsum sequence. In NN-like case where a new weight is injected in every operation, there are six inputs $A,B,C,D,E,F$ and one final output $V$, thus 7 terms in the denominator. For an iterative HPC-like example, there are only 3 inputs from the application side $A,B,C$ and there is one final output $V$ leading to four terms in the denominator thus less traffic is injected by HPC application the application compared to NN. Thus, for a sequence of Einsums, the arithmetic intensity increases substantially if all the intermediate outputs are reused on-chip. For similar sized tensors, the arithmetic intensity improves by 2.14x and 3.75x respectively. 

For low intensity individual operations with one large rank, if we assume that $N$ is the only large rank for all the sequence, $N\rightarrow\infty$, and consider all other ranks are equal, the arithmetic intensity without inter-operation reuse is $\frac{M}{2}$ for each case. With inter-operation reuse, the arithmetic intensity would be $\frac{5M}{2}$, increasing the intensity by 5x. Thus we observe through these examples, that an application with low intensity individual operation sequences can significantly benefit from inter-operation reuse. 
%Next we discuss the reuse opportunities in CG application.%~\autoref{fig:aicg} shows the arithmetic intensity of 1,5 and 10 iterations of CG loop rather than 1 operation and it shows considerable improvement over multiple iterations.

As a practical example of CG, the $AI_{best}$ plots in ~\autoref{fig:aicg} uses loop unrolling to demonstrate that the complete Conjugate Gradient kernel can achieve a high AI\textsubscript{best}, even though the, AI\textsubscript{best} for each individual GEMM is limited. %With this, we can further improve the 'iter=1' intensity from 13.8 to 21.8 in lshp3466, from 11 to 13.5 in raefsky2 and from 13.3 to 19.9 in aft02.
Unrolling of the loop (iter>1) and reusing the data amongst the unrolled iterations in~\autoref{alg:cg_einsum} improves the intensity by huge amounts.
However, loop unrolling is not a practical solution as it requires buffering extremely large amounts of data on-chip. Instead, we propose a mapping strategy that achieves substantial benefits even with realistic buffer sizes. 


\begin{comment}~~~~~~~~~~~~~~~\textbf{\underline{NN}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\textbf{\underline{HPC}}~~~~~~~~~~~~~~~
\vspace{-2mm}
\begin{small}
\begin{verbatim}

Z(m,n)+=A(m,k)*B(k,n)       Z(m,n)+=A(m,k)*B(k,n)
Y(o,n)+=C(o,j)*Z(j,n)       Y(o,n)+=C(o,j)*Z(j,n)
X(p,n)+=D(p,i)*Y(i,n)       X(m,n)+=Z(m,i)*Y(i,n)
W(q,n)+=E(q,h)*X(h,n)       W(m,n)+=Z(m,h)*X(h,n)
V(r,n)+=F(r,g)*W(g,n)       V(o,n)+=Y(o,g)*W(g,n)


\end{verbatim}
\end{small}
\vspace{-2mm}

\input{tables/intensity.tex}

For the NN example, case assuming there is no inter-operation reuse, the ideal arithmetic intensity is
\vspace{-2mm}
\begin{small}
\begin{equation}
    \frac{MNK+ONJ+PIN+QHN+RGN}{MN+NK+KN+OJ+JN+ON+PI+IN+PN+QH+HN+QN+RG+GN+RN}
\end{equation}
\end{small}
\vspace{-2mm}
\end{comment}






\begin{comment}
\subsection{Reuse opportunities in CG}
\subsubsection{Arithmetic Intensity of CG}

As~\autoref{sec:ai} shows, reuse opportunities within a operation are less compared to traditional DNNs due to the low arithmetic intensity. Similar to the traditional DNNs, some reuse in the GEMMs can be achieved by using temporally local register files in the PEs or spatially, through multicasting or peer-peer forwarding. Prior works like Alrescha~\cite{asgari2020alrescha} have proposed techniques to extract locality from dense regions of the sparse matrices. %\RG{word operation, tensor operator}

\subsubsection{Inter-operation Reuse Opportunities Within the CG Loop}
Inter-operation reuse can be achieved by reusing the portion of the output of the first operation immediately into the next operation. The design-space of even two of these \textit{pipelined} operations is non-trivial owing to the inter-dependence of the intra-operation dataflows~\cite{garg2021understanding}. However, Conjugate Gradient offers immense pipelining opportunities within one loop iteration of CG Algorithm in ~\autoref{alg:cg_einsum}. Matrix $A$ is the input for every iteration and has to be read once. Every CG loop iteration has dynamically generated matrices $R_{prev}$, $P_{prev}$, $(R_{prev})^T$, $(P_{prev})^T$ and $X_{prev}$ as the inputs and $R_{cur}$, $P_{cur}$, $(R_{cur})^T$, $(P_{cur})^T$ and $X_{cur}$ as the outputs for the loop. Rest of the intermediate variables (or variable product expressions) are consumed within the single loop iteration. Since, the memory footprint of inputs and outputs to the loop is low, theoretically, the reuse benefit of inter-operation pipelining is high. ~\autoref{fig:aicg} 'iter=1' bar shows the $AI_{best}$ for one iteration of the full Conjugate Gradient Loop.

\subsubsection{Reuse opportunities between Multiple Iterations of the CG loop}
At an even further granularity, the theoretical reuse across iterations scales. On unrolling the CG for loop $iter$ times, the inputs to a big loop iteration are $A$, $R_{cur-iter}$, $P_{cur-iter}$, $(R_{cur-iter})^T$, $(P_{cur-iter})^T$ and $ X_{cur-iter}$ while the outputs are $R_{cur}$, $P_{cur}$, $(R_{cur})^T$, $(P_{cur})^T$ and $X_{cur}$. Since all of the variables in the CG loop are generated from previous operations and since the only input from the application side $A$ is same across all iterations, the memory accesses in an ideal case are independent of the value of $iter$. However, the number of multiplications scale linearly with $iter$. Thus maximum achievable arithmetic intensity scales linearly with the unrolling factor $iter$.

On unrolling the CG for loop $iter-1$ times, the inputs to a big unrolled loop iteration are $A$, $R^{cur-iter}$ (and transpose), $P^{cur-iter}$ (and transpose) and $ X_{cur-iter}$ while the outputs are $R_{cur}$ (and transpose), $P_{cur}$ (and transpose) and $X_{cur}$. Since all of the variables in the CG loop are generated from previous operations and since the only input from the application side $A$ is same across all iterations, the memory accesses in an ideal case are independent of the value of $iter$. However, the number of multiplications scale linearly with $iter$. Thus maximum achievable arithmetic intensity scales linearly with the unrolling factor $iter$.
The $AI_{best}$ plots in ~\autoref{fig:aicg} show that the complete Conjugate Gradient kernel can achieve a high AI\textsubscript{best}, however, AI\textsubscript{best} for an individual GEMM is limited. This measurement counts $R^T$ and $P^T$ separately from $R$ and $P$. However, with right pairs of dataflows, one can eliminate the need to read and hold a tensor and its transpose separately as we discuss in~\autoref{sec:gogeta}. With this, we can further improve the 'iter=1' intensity from 13.8 to 21.8 in lshp3466, from 11 to 13.5 in raefsky2 and from 13.3 to 19.9 in aft02.
\end{comment}