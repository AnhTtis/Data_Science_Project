
\subsection{Results}
\label{sec:eval}

\subsubsection{DRAM accesses in Conjugate Gradient}

We evaluate the DRAM accesses for Conjugate Gradient
%, SRAM accesses and memory footprint of operations
for the workloads and datasets in~\autoref{table:dataset} as ~\autoref{fig:DRAM} shows. We achieve a geomean of 6.7x reduction in DRAM traffic over the Seq-Flex baseline, ranging from 1.18x to 23.7x.

For a dataset with tensors smaller than the SRAM size for example, aft02 and nasa4704 at 4/16 MB SRAM, \DataflowName hits the ideal (perfect reuse) DRAM accesses. For such cases SEQ-Overflow which uses a FIFO order write policy as demonstrated in~\autoref{sec:tornado}, also achieves a close performance but \DataflowName performs better due to the ability to use the same tensor for transpose as well.

For extremely large datasets like ecology1, for small memory size, SEQ-Flex and SEQ-Overflow incur similar DRAM accesses and \DataflowName performs 18 to 30\% better than Sequential baselines due to pipelining\_with\_writeback which avoids reading the operand from the DRAM for the consecutive operation. In such large workloads, pipelining plays a major role in reducing the off-chip traffic.

For all the intermediate datapoints, where some but not all tensors fit the SRAM, there is a clear downward trend between SEQ-Flex, SEQ-Overflow and \DataflowName in terms of DRAM accesses. This is due to a combination of multiple reasons, including pipelining\_with\_writeback, ability to organize data in the SRAM based on reuse distance of downstream tensor operations and the ability to achieve zero swizzle penalty.

Thus \DataflowName achieves better performance than the Sequential baselines due to pipelining, swizzle aware loop ordering and due to data organization strategy.
We also evaluate DRAM accesses for Graph Neural Networks  in~\autoref{sec:GNN} to demonstrate the generality of \DataflowName.
%For Conjugate Gradient, we evaluate 10 iterations, for GCNs, 1 layer and for ResNet, we evaluate one residual block.% We also compare inter-cluster communication between GOGETA-df and GOGETA-map to demonstrate the benefit of pipelining with fine-grained spatial multiplexing. %We also evaluate a set of loop orders with loop orders that support pipelining but incur swizzling. 

%\TK{@Raveesh -- need to highlight following points: geomean speedup, impact of sram size, and how far you are from ideal}


%\subsubsection{DRAM accesses in Graph Neural Networks}:


\subsubsection{DRAM accesses in GNNs}
\label{sec:GNN}
\label{sec:archvision}
\insertFigure{GNN}{Data movement for Graph Neural Networks.}

To demonstrate the generality across applications, we also show the DRAM accesses for a GCN layer for graphs with chemistry applications. GCN layer consists of two operations with a pipelienable reuse pattern between them. We evaluate them for SRAM size of 1MB as~\autoref{fig:GNN} shows.

Cora has a large feature matrix $M\times N$ but still has low arithmetic intensity due to low number of non zeros per row. There is a marginal improvement in SEQ with overflow but considerable improvement with pipelining and hits the ideal. Protein dataset is relatively small and the tensors fit in the SRAM, thus both SEQ-Overflow and GOGETA-map hit the ideal. Graph Neural Network layers do not have downstream tensors using the same data thus pipelining makes it easier to achieve the ideal DRAM accesses.



\insertFigurePartnn{comm}{Inter-cluster NoC link traversals in KBs for N=8. We cap the x-axis due to high range.}% Considers best case for GOGETA-df and worst for GOGETA-map.}

\subsubsection{Communication analysis of Tiling}:

%Pipelining between two operations involves, communicating the data between the parts of the compute where the operations are mapped.
If we consider operations 4 and 5 in CG~(\autoref{alg:cg_einsum}), the total traversals on the inter-cluster NoC for GOGETA-df is $sizeof(R)$ which is $M\times N$ even if these operations are allocated such that they are 1 hop apart which leads to considerable delay and energy since inter-cluster bandwidth ($\equiv$ inter-chiplet) is low compared to in-cluster bandwidth. Instead, if we use GOGETA-map where pipelining happens inside the cluster and $M$ rank is sliced across the clusters, maximum inter-cluster link traversals are $sizeof(\Lambda)\times Broadcast~traversals$ + $sizeof(\Gamma)\times Reduce~traversals$ which is $2\times N\times N\times 15$ ($N\times N$ data moved through at most 15 links for broadcast and reduction) for 16 clusters.~\autoref{fig:comm} shows inter-cluster link traversal for GOGETA-df and GOGETA-map in KB's. Workload can scale up considerably and the dimension that will scale is $M$. By changing these to intra-cluster traversals, GOGETA-map makes better use of high bandwidth NoC.