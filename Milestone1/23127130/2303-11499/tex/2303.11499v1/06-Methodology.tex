\section{Evaluations}

\subsection{Experimental Methodology}
\label{sec:expt}

We describe our experimental methodology for demonstrating the efficiency of \DataflowName across baseline dataflows. We focus on the inter-cluster communication and the DRAM accesses which is same regardless of microarchitectural parameters.

\subsubsection{Analytical Framework}
\label{sec:simulation}
%We modify the OMEGA~\cite{garg2021understanding} framework to model the DAG of tensor operations for applications like Conjugate Gradient, Graph Convolution Networks, ResNet. OMEGA is a cost model for modelling performance and energy for various mappings with two-operation pipelining.
%Our framework builds upon STONNE~\cite{STONNE21} which is a cycle accurate simulator that models one DNN layer at a time and models flexible interconnects~\cite{kwon2018maeri} with ability to support multiple intra-operation dataflows. STONNE has been extensively validated against MAERI~\cite{kwon2018maeri} and SIGMA~\cite{sigma} RTLs. STONNE can model SpMM, DenseGEMMs/CONVs on the accelerator with reconfigurable interconnects. Prior work OMEGA~\cite{garg2021understanding} builds a cost model which uses the STONNE output and timestamps to model pipelining between two operations. We modify the cost model that OMEGA uses to model a DAG of tensors in applications like Conjugate Gradient. We use that to simulate the baselines and \DataflowName which are described in~\autoref{sec:baseline}.

We build an analytical model to capture the memory accesses and communication across clusters for the baselines and \DataflowName to compare the achieved arithmetic intensity of the dataflows independent of the accelerator implementation. We validate the data movement statistics for a single layer by using STONNE~\cite{STONNE21}, which is a cycle accurate simulator that models one DNN layer at a time and models flexible interconnects~\cite{kwon2018maeri} with ability to support multiple intra-operation dataflows. STONNE has been validated against MAERI~\cite{kwon2018maeri} and SIGMA~\cite{sigma} RTLs. We validated the data movement for a pair of two pipelined tensor operations using OMEGA~\cite{garg2021understanding,omega} which is a wrapper around STONNE to model pipelining between two operations. 

\subsubsection{Workloads and Datasets}

\input{tables/workload.tex}

\label{sec:dataset}
We evaluate our methodology and the \DataflowName mapping strategy for Conjugate Gradient and GNN (GNN results in~\autoref{sec:GNN}). We obtain the sparse matrices of the Conjugate Gradient Datasets from Suitesparse~\cite{suitesparse} for scientific problems like structural problems, stencils, acoustics etc. We obtain GNN graphs from OMEGA~\cite{omega}.

\subsubsection{Dataflow Configurations}
\label{sec:baseline}

\input{tables/dataflow_config.tex}

We compare multiple dataflow configurations described in~\autoref{table:dataflow_config}. SEQ-Flex dataflow has the lowest possible DRAM accesses for individual layers but it considers the output tensor of each operation ending up in the DRAM and the input coming from the DRAM. SEQ-Overflow dataflow considers that the data is written in the SRAM and the portion that does not fit is sent to the DRAM. To make the baseline strong and fair, we writeback the tensor in FIFO order (but do not consider reuse distance based replacement) in SEQ-Overflow. GOGETA-df uses inter-op patterns to reduce pressure on memory and finds the individual loop order with pipelining and also avoids swizzle\_penalty. GOGETA-map also uses the scalable tiling strategy that reduces inter-cluster communication overhead in addition to GOGETA-df. GOGETA-df and GOGETA-map also include the tensor allocation strategy of FIFO order write and reuse distance based replacement in~\autoref{sec:tornado}. Ideal, represents the DRAM accesses with perfect reuse.


\subsubsection{Workload and Architecture Parameters}
\label{sec:params}

\input{tables/config.tex}
\insertWideFigure{DRAM}{DRAM data movement in Megabytes for N (x-axis) = 1, 8, 16 and SRAM sizes, 1MB, 4MB and 16MB. We cut the Y-axis at a lesser value for certain plots due to large disparity between SEQ-Flex and GOGETA-map.} %\RG{Only SEQ-Flex is real, rest are placeholders for now.}}


~\autoref{table:config} describes the architecture configuration we use for the evaluation and the parameters we sweep. We run workloads of different sizes and nnz's and we also sweep the $N$ rank that corresponds to number of simultaneous initial guesses and the SRAM size to capture different scenarios with varying ratios of tensor and SRAM sizes.
We consider a clustered architecture, each cluster of 1024 PEs backed by its own SRAM slice.
