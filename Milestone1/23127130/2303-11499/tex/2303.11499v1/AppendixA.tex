\begin{appendices}
\subsubsection{DRAM accesses in GNNs}
\label{sec:GNN}
\label{sec:archvision}
\insertFigure{GNN}{Data movement for Graph Neural Networks.}

To demonstrate the generality across applications, we also show the DRAM accesses for a GCN layer for graphs with chemistry applications. GCN layer consists of two operations with a pipelienable reuse pattern between them. We evaluate them for SRAM size of 1MB as~\autoref{fig:GNN} shows.

Cora has a large feature matrix $M\times N$ but still has low arithmetic intensity due to low number of non zeros per row. There is a marginal improvement in SEQ with overflow but considerable improvement with pipelining and hits the ideal. Protein dataset is relatively small and the tensors fit in the SRAM, thus both SEQ-Overflow and GOGETA-map hit the ideal. Graph Neural Network layers do not have downstream tensors using the same data thus pipelining makes it easier to achieve the ideal DRAM accesses.

\end{appendices}