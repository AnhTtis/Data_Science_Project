\begin{abstract}

HPC applications are critical in various scientific domains ranging from molecular dynamics to chemistry to fluid dynamics. Conjugate Gradient (CG) is a popular application kernel used in iterative linear HPC solvers and has applications in numerous scientific domains. However, the HPCG benchmark shows that the peformance achieved by Top500 HPC systems on CG is a small fraction of the performance achieved on Linpack. While CG applications have significant portions of computations that are dense and sparse matrix multiplications, skewed SpMMs/GEMMs in the HPC solvers have poor arithmetic intensities which makes their execution highly memory bound unlike GEMMs in DNNs which have high arithmetic intensity. The problem of low intensity individual \GEMM also exists in various emerging workloads from other domains like Graph Neural Networks, Transformers etc. In this work we identify various reuse opportunities between the tensors in these solver applications to extract reuse in the entire Directed Acyclic Graph of the tensor operations rather than individual tensor operations. These opportunities essentially depend on the dimensions of the tensors and the structure of the tensor dependency graph. We propose a systematic methodology to determine various kinds of reuse opportunities in the graph of operations and determine the loop order and tiling in the interdependent operations. As a result, we propose a novel mapping strategy \DataflowName that improves reuse of HPC applications on spatial accelerators. We also propose a data organization strategy in the buffer. Our mapping achieves geomean 6.7x reduction in memory accesses. 



%The TOP500 supercomputers achieve only a fraction of performance on HPC solvers compared to Linpack benchmark on which they are ranked. Majority of the computations in this algorithm are GEMMs. Spatial accelerators have gained huge success over acceleration of DNN applications where the data movement is minimized by exploiting reuse within a single matrix multiplication. However, matrix multiplications in many HPC applications like iterative linear solvers have arithmetic intensities. However, in this work we show various reuse and acceleration opportunities within these solver algorithms to minimize the data movement in the entire dataflow graph of the operation. We propose a novel dataflow \DataflowName to maximize reuse in the complete DAG of tensor operations for HPC and other applications from other domains like Machine Learning and Graph that use GEMMs. We observe that \DataflowName reduces the memory accesses by \TODO{xx\%} and inter-node communication by \TODO{xx\%} 

\end{abstract}