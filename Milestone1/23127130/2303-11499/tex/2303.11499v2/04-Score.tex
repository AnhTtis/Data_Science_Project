

% \subsection{\AccelName Overview}

% \autoref{fig:accelerator-figure} shows an overview of the proposed ~\AccelName accelerator. 
% We leverage a configurable PE array that enables both temporal and spatial scheduling of multiple operators (as we show later in \autoref{alg:cg_einsum}) inspired by prior works~\cite{planaria,sara_dac2022,plasticine}.
% Our key novelty is in the on-chip memory architecture to extract inter-operation reuse in arbitrary DAG of operations. The on-chip memory architecture consists of different buffers.  
% A scratchpad (explicit) called \textit{input buffer} fetches input tensors from DRAM. An explicit \textit{pipeline buffer} is used to stage producers and consumer for operations where pipelining is possible (without DRAM being involved).
% \SpadName is our proposed hybrid explicit-implicit buffer idiom to reuse tensor operands that have to be written back where pipelining them is not possible.
% %\SpadName is the largest SRAM buffer within~\AccelName memory hierarchy. It uses a combination of tensor-level replacement policies and high-level workload information from the scheduler to do the hybrid data orchestration to determine whether the data is written or sent to DRAM.
% In the rest of this paper, we demonstrate how co-designing the ~\DataflowName scheduler (\autoref{sec:score}) and the~\SpadName buffering idiom (\autoref{sec:chord}) in  \AccelName allows us to identify delayed downstream dependencies and reuse opportunities among them in complex DAGs, without exploding the scheduling search space.

%~\autoref{sec:score} discusses the~\DataflowName scheduler in detail along with how operands are bound to different buffer types.~\autoref{sec:chord} discusses~\SpadNamenospace, the novel buffer structure with tensor-level replacement policies and hybrid data orchestration.

\input{tables/related_score}


\section{\AccelName Accelerator Overview}
\label{sec:interface}

%\TK{I felt the accel overview made sense here}

\autoref{fig:accelerator-figure} shows an overview of the overview of \AccelNamenospace. 
\AccelName leverages a configurable PE array that enables both temporal and spatial scheduling of multiple operators (as shown in \autoref{alg:cg_einsum}) inspired by prior works~\cite{planaria,sara_dac2022,plasticine,kwon2018maeri}.
Our key novelty is in the on-chip memory hierarchy to extract inter-operation reuse in arbitrary DAG of operations. The on-chip memory hierarchy consists of different buffers.  
A scratchpad (explicit) called \textit{input buffer} fetches input tensors from DRAM. An explicit \textit{pipeline buffer} is used to stage producers and consumer for operations where pipelining is possible (without DRAM being involved).
\SpadName is our proposed hybrid explicit-implicit buffer mechanism to reuse tensor operands that have to be written back where pipelining them is not possible.

We demonstrate how accelerator~\AccelName uses co-design of the \DataflowName scheduler (\autoref{sec:score}) and the~\SpadName buffering mechanism (\autoref{sec:chord}) to identify delayed downstream dependencies and exploit reuse opportunities among them in complex DAGs, without exploding the scheduling search space.~\autoref{sec:arguments} also discusses the shortcomings of caches and scratchpads which~\SpadName resolves.
Table \ref{tables:related_score} shows a detailed breakdown of existing state-of-the art scheduling approaches versus \DataflowName and how they respond to these challenges. %Given this complexity it is a natural area to consider hardware assistance. 
Table \ref{tables:related_chord} shows a similar breakdown of existing and the \SpadName hardware buffering mechanisms, and how their approach is exposed to the scheduler or programmer.

\insertFigureScaled{accelerator-figure}{
Overview of \AccelNamenospace. Within the memory hierarchy, input buffer, pipeline buffer and register files are explicitly managed while~\SpadName is hybrid implict-explicit. We expand on~\SpadName in~\autoref{sec:chord} (\autoref{fig:chord-fig}).}{1}




\section{Software Scheduler: \DataflowNameTitle}
\label{sec:score}

The~\DataflowName scheduler takes in the application represented as a DAG of tensor operations, as the input as~\autoref{fig:system} shows. It identifies types of dependencies at the tensor level and then determines the schedule of the operands that maximizes exploiting pipelining wherever it can and steers the operands with downstream consumers requiring writeback to~\SpadNamenospace. \autoref{tables:related_score} shows the scope of dependencies~\DataflowName can cover compared to previous schedulers.

\input{algorithm/interop.tex}


\subsection{Tensor-level Dependencies in the DAG}
\label{sec:patterns}


We taxonomize different tensor-level dependences as follows.
%\TK{``There are five .. " or ``We identify five .." ? Also - point to the figure - esp as the color coding matches that, and in each pattern pls point to the figure as example}

\textbf{Sequential Dependency} - The sequential dependency implies that the nodes connecting that edge are executed one by one, although it may be possible to reuse the data on-chip depending on the space in the buffer. There are generally no constraints with this type of dependency and this is used when the dependency is not pipelineable. Operands are written to and later read from off-chip or on-chip memory. We call the dependency Sequential when the source operation does not pipeline with the adjacent destination operation. %Also, this pattern does not give any benefit, and the output of the operation is both written and re-read from on-chip or off-chip depending on the capacity.

%\insertFigure{buffer}{Inter-operation reuse patterns in steady state and their data orchestration. %Please note that, this figure abstracts the finer details of~\SpadName and shows the whole buffer space.
%\vspace{-3mm}
%}

%\insertFigure{inter-op-reuse}{Output of~\autoref{alg:inter-op} using colored edges. Letters in the node denote dominance, 'U' means uncontracted, 'C' means contracted and 'bal' means all ranks are moderately big. Note that first tensor is uncontracted dominant since we compress the K rank (CSR format).\vspace{-1mm}}

\textbf{{Pipelineable Dependency}} - A piplineable dependency refers to the edge of the DAG where pipelining used between the two operations. Prior works~\cite{flat,garg2021understanding,tangram,yan2020hygcn} make use of this. ~\autoref{fig:system} shows how pipelining reuses the data within SRAM in blue arrows. It discards the tile once it's consumed.
Pipelining does not provide benefit when the contracted rank is dominant (ie lines 2 and 5 in~\autoref{alg:cg_einsum}) because the bulk of compute is just used in producing data and that stage duration would become a rate limiting step.
%However, the {pipelineable} pattern does not guarantee pipelining unless additional conditions related to intra-operation dataflow are satisfied which we discuss in~\autoref{sec:gogeta}. 
 Pipelining completely removes the need to write and read the intermediate tensor. Most of the prior works including TileFlow~\cite{tileflow}, SET~\cite{isca-pip} exploit this reuse opportunity to pipeline the tensors wherever possible. However, there are additional delayed writeback or hold dependencies where there is a downstream consumer. 


\insertFigureScaled{system}{Overview of~\DataflowName showing dependency classification, scheduling and binding attributes to~\SpadNamenospace. We show how the example schedule maps to hardware. 'U' and 'C' represent uncontracted and contracted ranks.}{0.8}


\insertFigure{hold}{Delayed\_hold buffer example.}

\insertFigure{inter-op-reuse}{Output of~\autoref{alg:inter-op} using colored edges. The letters in the node denote dominance, 'U' means uncontracted, 'C' means contracted, and 'bal' means that all ranks are big. The first operation is 'U' because the contracted rank is compressed.}

\textbf{{Delayed\_Writeback dependency}} -  In some cases, the intermediate tensor might be needed in a future computation, a dependency that has not been observed in previously popular applications like DNNs, GNNs etc. This is represented by the transitive edge on the graph. Therefore, as~\autoref{fig:system} shows in brick red arrow, the produced tensor needs to be stored in an on-chip SRAM buffer or DRAM depending on the capacity and tiles cannot be overwritten unlike simple pipelineable dependency. %We can still avoid reading the tensor for the adjacent operation.
{Delayed\_writeback} dependency can be seen in CG as~\autoref{fig:inter-op-reuse} shows in brick red color.

\textbf{{Delayed\_Hold Dependency}} - In some cases with downstream consumers, there is a possibility that the complete chain until the future destination of this tensor is pipelineable. In that case, we hold the tile in the SRAM until the destination consumes that tile.~\autoref{fig:hold} shows an example of this. The number of tiles held essentially depends on the reuse distance of the downstream dependency (in terms of the number of operations). This dependency does not preclude us from reaping the full benefit of pipelining but requires slightly more occupancy in the on-chip SRAM. This dependency can be found in the ResNet~\cite{resnet} block with skip connections, as~\autoref{fig:inter-op-reuse} shows in cyan color. This is exploited by TANGRAM~\cite{tangram} and SET~\cite{isca-pip}.

%\textbf{Parallel multicast}
%This work expands the \textit{pipelineable} pattern to mean mean adjacent pipeline, pipeling with hold and pipeling with writeback.

\textbf{{Parallel Multicast}} - It is also possible to reuse the tensor in multiple parallel tensor operations, which we call {Parallel\_multicast}. These parallel operations are non-transitive edges from the multicasting nodes to the directly connected nodes.






%\subsubsection{Applying Inter-operation patterns to DAG}


\autoref{alg:inter-op} shows the methodology to mark the classify the dependencies for arbitrarily connected DAG of tensor operations. %A node which has more than one non-transitive edges can multicast its output into more than one parallel operations and thus all the non-transitive edges from that node have the~{parllel\_multicast} pattern.
A non-transitive edge\footnote{A transitive edge is the edge not on the longest path between the source and the destination} where the source node is uncontracted dominant and the destination is unshared dominant has a~{pipelineable} dependency. Note that~{pipelineable} dependency does not guarantee pipelining since it also depends on the loop order of the individual operations as we discuss in \autoref{sec:loop}. Any edge where the source node is contracted dominant or the destination is unshared dominant, has a sequential dependency. A transitive edge with uncontracted dominant source node could have~{Delayed\_hold} dependency, if all the edges on the corresponding critical path are pipelienable, otherwise it has the~{Delayed\_writeback} dependency. ~\autoref{fig:system} (first box within~\DataflowNamenospace) and
~\autoref{fig:inter-op-reuse} show example outputs of~\autoref{alg:inter-op}.

\insertFigureScaled{spacetime}{CG iteration schedule. On top, the overall compute is divided into multiple parts each executing one tensor. At bottom, the tensors are split across the node by the dominant rank and the sub-tensors are pipelined within a cluster. Refer to~\autoref{alg:cg_einsum} for rank names and line numbers.}{0.8}

\subsection{Scheduling Operations}

\label{sec:loop}

\DataflowName schedules the operands, based on the dependencies and the DAG, such that pipelining is possible for maximum pipelineable edges. ~\autoref{fig:system} (second box within~\DataflowNamenospace) and~\autoref{fig:spacetime} show examples of the scheduler output.

\textbf {Loop orders}: 
\begin{comment}
For an example pair of tensor operations $Z_{m,n}=A_{m,k}*B_{k,n}$ and ${W_{o,n}=C_{o,j}*Z_{j,n}}$, the conditions for pipelining a loop order pair are as follows-
%\vspace{-2mm}
\squishlist
\item The edge connecting the nodes has a {pipelineable} dependency.
\item The source node has an uncontracted rank as the outermost loop. Having contraction ($K$) as the outermost loop implies that the complete sum that is usable by the next operation is generated at the very end.
\item The destination node has a shared rank as the outermost loop. Having unshared rank ($O$) as the outermost loop implies that the shared tensor $Z$ is needed all over again when $o$ changes. But the idea of pipelining is to slice the intermediate tensor and consume it so that its not needed.
\item The shared tensor $Z$ is not swizzled since the same portion of data that is produced, should be consumed. %Thus no swizzling at that edge is allowed.
\squishend
\end{comment}
If the edge has a pipelineable or Delayed\_hold dependency, the schedule tries to satisfy the codependence conditions of producer and the consumers in pipelining. 

For an example pair of Einsums $Z_{m,n}=A_{m,k}*B_{k,n}$ and ${W_{o,n}=C_{o,j}*Z_{j,n}}$, the conditions for pipelining a loop order pair are as follows-
%\vspace{-2mm}
\squishlist
\item The edge has a {pipelineable} inter-operation pattern.
\item Source node has uncontracted rank as the outermost loop.
\item Destination node has shared rank as the outermost loop. 
\item The shared tensor $Z$ is not swizzled since the same portion of data that is produced, should be consumed. %Thus no swizzling at that edge is allowed.
\squishend


If the edge has a Delayed\_writeback or sequential dependency, the schedule tries to minimize layout transformation (swizzle) of a tensor, among various consumers.

We also keep the dominant rank in the outermost loop. Hence, large tensor is stationary and the small tensor is streamed from the register file.

\input{tables/related_chord}


\textbf{Tiling:} 
Skewed GEMMs consist of two small dimensions, and as a result, have one small tensor. The small tensors are stored inside the registered file (explicit), and streamed from there. On the other hand, the large tensors are stationary. As a result, we keep the large dimension in the outermost loop. Even though, the register files are explicit, they do not require scheduling search, since we fix the mapping to stream the small tensor from the register file (where it fits entirely). This schedule already achieves the best-case intra-operation reuse and the best possible inter-operation pipelining.

However, for delayed downstream consumers, \DataflowName by itself does not do a fine-grained buffer allocation, as there are multiple downstream tensors contending for the space in~\SpadNamenospace. \DataflowName just provides~\SpadNamenospace's policies coarse-grained metadata about each operand, and the~\SpadName policies make fine-grained buffer allocation decisions.~\autoref{sec:arguments} explains the cost of search for the delayed downstream consumers.

%\subsection{Design Discussion}


%There are limited options for tiling individual low intensity \GEMM because the non-dominant dimensions are small. In fact, such GEMMs are often able to achieve the best possible data reuse with memory accesses $MK\text{+}KN\text{+}MN$ since the tensor made of the non-dominant ranks can practically fit inside a portion of the Register File (RF) or parallelized across few PEs which means that the smaller tensor can just be accessed continuously from the RF while a tile of the large tensor is stationary. We keep the dominant rank in the outermost loop since the register file can store a bulk of the small tensor. However, as discussed in~\autoref{sec:background}, even the best intensity is low for \GEMM. 

\textbf{Handling sparsity} We store the sparse tensor in compressed (CSR/CSC) format, and we tile based on occupancy. Our tiling achieves the best possible arithmetic intensity for the individual SpMM operation.~\SpadName stores the data and the metadata in CSR/CSC format.

%\TK{Add an explicit question about sparsity}

\textbf{Scalable Dataflow:} To ensure scalability for multiple nodes, the schedule also makes sure that pipelining is done within a node and not split across nodes in a multi-node architecture. In case of multiple node, we parallelize the dominant rank across the nodes. ~\autoref{fig:spacetime} bottom row shows the scalably tiled schedule. This makes sure that we are moving the small tensors across nodes instead of the large ones, as multiple tiles of large dimension will share the same small tensor. For example, if pipelining between operations 4 and 5, the top strategy in~\autoref{fig:spacetime} will require moving $SIZE_R$ data ie $M\times N$ words through the NoC. The bottom stragegy will require moving ($SIZE_{\Lambda}\times HOPS_{broadcast}$)+($SIZE_{\Gamma}\times HOPS_{Reduce}$) ie $N\times N\times $($Hops_{Broadcast} $+ $Hops_{Reduce}$). 

$M>>N$ in CG, and $M>>hops$ since one core can hold large M size, so the number of cores on which the execution is distributed is obviously way lower than $M$. 





%\insertFigureScaled{accelerator-figure}{Overview of the~\AccName accelerator.~\autoref{fig:system} shows the scheduler and the accelerator interface and~\autoref{fig:idiom} zooms into~\SpadNamenospace, the main accelerator contribution.}{0.9}





\subsection{\DataflowNamenospace-\SpadName Interface}

\DataflowName determines the type of buffer used in the memory hierarchy based on the dependency.
 We use the \textit{pipeline buffers} for delayed\_hold and pipelineable dependencies (\autoref{fig:system} and~\autoref{fig:hold}), by allocating slightly more space. %Such tensors can consumed tile-wise and can be discarded after use by the consumer or just held for a bit longer. Therefore, these tensors occupy a very small space in the overall global buffer ecosystem which is allocated for producer, consumer and held tiles (referred to as "pipeline buffer") as~\autoref{fig:system} shows.~\autoref{fig:accelerator-figure} shows pipeline buffer in the context of the the~\AccelName accelerator.

The rest of the operands (sequential and writeback dependencies) are completely written back and required entirely in a downstream consumer therefore use~\SpadNamenospace (\autoref{fig:system}).~\autoref{sec:chord} describes the architecture of~\SpadName for maximally reusing such operands. Exploiting reuse in cases of pipelineable and Delayed\_hold dependencies also enables us to reduce the number of contending tensors for~\SpadNamenospace.

%\TK{The following on line para seems a bit out of place .. do we only use RIFF ? then why do we even propose prelude?}
%We also use replacement policy~\PolicyB (\autoref{sec:chord}) which uses information from the input DAG, and~\DataflowName relays that information from input to the~\PolicyB controller.





