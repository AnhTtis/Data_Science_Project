
\insertWideFigure{AI}{Performance of accelerators (\autoref{table:dataflow_config}) for different problems that use CG algorithm (higher is better). First plot is shown on roofline. %\revision{"BestIntra" represents the best-case outcome of intra-operation mapping works such as GAMMA~\cite{kao2020gamma}, MAESTRO~\cite{kwon2019understanding},\cite{timeloop}, as mentioned in~\autoref{sec:baseline}. Likewise, Pipe represents the best case outcomes of works that rely on pipelining without delayed downstream consumers, for example, TileFlow\cite{tileflow} and FusedCNN\cite{fused}.}
%\TK{the three arrows are a bit confusing .. for the top and bottom one I think just use a dotted line to show you are expanding it to roofline. The red arrow in the middle seems to be a typo}
}

\insertFigure{AI2}{Performance of accelerators (\autoref{table:dataflow_config}) for GNN and BiCGStab (higher is better).%\TK{too much white space. Cut the y-axis off a little bit beyond the highest data point in each chart}.%\TK{The ResNet should be one chart .. and just like you have in CG one, you should have blue and orange bars for it .. and for consistency same for GNNs too.} \vspace{-3mm}%\textbf{Note that~\SpadName does not lose compared to Exp+Pipe when there is no downstream consumer. The memory bandwidth of the chosen configuration is high enough to make layers of ResNet compute bound, however, see~\autoref{fig:energy} to see the benefits in terms of DRAM accesses.}
%\TK{x axis will sweep Mem BW. Workloads: CG, BiCGstub, GNNs, BERT, ResNet}
}

\insertFigureScaled{energy}{Off-chip energy of schedules and buffer structures for different workloads (lower is better). Energy is geomeaned across different parameters or datasets in a workload.%\TK{for consistency i would expect this would also have BiCGstab?}
%\TK{Maybe make this in horizontal format like Fig 17 to save space}
}{1}

\insertFigure{areapower2}{a) Area and b) per access energy comparison (Lower is better).
}

\insertWideFigureScaled{sens}{All additional studies: (a) Performance and Energy of ResNet, with an additional baseline SET; (b)Variation of performance of \AccelName with \SpadName sizes on CG.; (c) \PolicyAnospace-only comparison against Flexagon, FLAT and \AccelName on CG.}{1}




\section{Evaluation}

\label{sec:eval}

\subsection{Methodology}

\subsubsection{Schedule and Buffer Baselines}
\label{sec:baseline}
%\TK{@Raveesh - pls a pass and make sure this is consistent with Table IV}
We compare different combinations of schedules and buffer configurations described in detail in~\autoref{table:dataflow_config}. The Best intra-layer baseline dataflow corresponds to the oracle operation-by-operation dataflow, which only incurs the cold accesses, which corresponds to accessing each tensor once. In fact, such GEMMs are often able to achieve the best possible data reuse with memory accesses $MK\text{+}KN\text{+}MN$ since the tensor made of the non-dominant ranks can practically fit inside a portion of the Register File (RF) or parallelized across few PEs which means that the smaller tensor can just be accessed continuously from the RF while a tile of the large tensor is stationary. We keep the dominant rank in the outermost loop since the register file can store all of the small tensor. However, as discussed in~\autoref{sec:apps}, even the best individual op arithmetic intensity is low for skewed GEMMs.
%
We have chosen Flexagon's~\cite{flexagon} microarchitecture\footnote{\revision{Flexagon's microarchitecture is also capable of running dense GEMMs and SpMMs in addition to SpGEMMs}} to model the best intra-layer baseline because of its flexibility with the loop orders, which is required to achieve the best intra-layer baseline performance for SpMMs and GEMMs with contracted and uncontracted dominant ranks.
Note that the efficiency within the compute array does not matter significantly in this work since 
%Moreover, this does not impact the strength of the baseline 
since stalls due to memory bandwidth dominate the delay. Therefore, we model the oracle op-by-op dataflow, which is the upper-bound for prior works (\ref{tables:related_score}) that use op-by-op execution. Similarly, FLAT configuration further considers adjacent pipelining capabilities in addition to to oracle op-by-op dataflow and is the oracle pipelined dataflow.


\subsubsection{Evaluation Framework}
%We extend Timeloop-Orojenesis
%~\cite{orojenesis_web}
%the an in-house simulator 
%for evaluating the different accelerators.
%DRAM accesses. 
%The input is in the form of memory access traces of the different workloads.

%We add different backends for evaluating different buffer structures.
We use LRU and BRRIP policy simulators to model set associative caches. We use OMEGA~\cite{garg2021understanding} and STONNE~\cite{STONNE21} to evaluate accesses for explicit baseline that emulates FLAT-like and Flexagon-like architectures.
%On obtaining the DRAM accesses, we obtain the arithmetic intensity and hence throughput from using memory bandwidth. 
To implement~\AccelNamenospace, we additionally, model writeback/hold and buffer occupancy behavior due to~\PolicyA and~\PolicyBnospace.  We model the area and access energy for caches and~\SpadName using CACTI~\cite{cacti}. We model off-chip energy based on the DRAM accesses.
\autoref{table:config} presents the hardware parameters.


\subsubsection{Workloads and Datasets}

\autoref{table:dataset} shows the HPC workloads and datasets.
We use the CG algorithm to model different problems - 2D/3D problem, computational fluid dynamics and circuit simulation.  
We obtain the sparse matrices for these
%of the 2D/3D problem, fluid dynamics and circuit simulation datasets 
from Suitesparse~\cite{suitesparse}.
%for scientific problems like structural problems, circuit simulation, and so on.
%The sparsity patterns in these datasets is mostly diagonal atmost 6\% non-zero defections. 
%For CG, we also sweep the $N$ rank that corresponds to number of simultaneous initial guesses, as shown in \autoref{table:config}. 
We also obtain GNN graphs from OMEGA~\cite{omega}.
%We run workloads of different sizes and nnz's. %We also evaluate ResNet and SSD ResNet.
We also evaluate \AccelName on ResNet in \autoref{sec:sens}.





%\subsubsection{Hardware and Workload Parameters}

%\autoref{table:config} provides details about the hardware and workload parameters.

 %Together with diverse tensor sizes across workloads and datasets, this allows us to study the efficacy of \DataflowName across diverse tensor shapes and sizes. %We perform sensitivity studies with~\SpadName size and associativity as~\autoref{fig:capacity} and~\autoref{fig:policy} show.

\subsection{Main Results}

%\TK{TODO - make a pass and update ``SCORE + CHORD" everywhere to CELLO}

\subsubsection{Performance of Accelerators} - We compare the schedule and buffer-hierarchy configurations listed in~\autoref{table:dataflow_config} for workloads and datasets in~\autoref{table:dataset}. \autoref{fig:AI} and~\autoref{fig:AI2} show the performance across these workloads. \AccelName achieves 4x geomean speedup.


%\insertFigure{expoverhead}{Accounting for design-time overhead of explicit policies.\RG{In progress. Placeholder for now}}




\textbf{Conjugate Gradient}: As~\autoref{fig:AI} shows, we observe that~\AccelName(\DataflowNamenospace+\SpadNamenospace) provides the highest throughput compared to the baselines evaluated. This is due to~\DataflowNamenospace's ability to identify pipelining and pipelining\_with\_writeback scenarios and~\SpadNamenospace's abililty to reuse intermediate tensors within the cache. Conjugate Gradient has operations where intermediate tensor always has a delayed downstream consumer. Therefore, works that only consider pipelining and parallel\_multicast are not beneficial here. LRU and BRRIP perform worse than best case schedule with explicit management.% Moreover, with N=16, the GEMM blocks are not dead on arrival, leading to better overall performance of LRU.
Across the datasets, \AccelName varies in the amount of benefit. For smaller datasets, \DataflowName easily uses space inside the buffer. For larger working sets, \SpadName has better overall conflict miss management despite capacity limitations, thus allowing \AccelName to perform better.

\textbf{BiCGStab}: BiCGStab~\cite{bicgstab} is a different PDE solver algorithm, which can be used to solve the same PDE solver problems in~\autoref{table:dataset}. Like CG,~\AccelName outperforms the baseline for BiCGStab as well.%For the same problems, \AccelName achieves upto twice the gain it achieved in CG, which increases the gain over Explicit+BestIntra, to upto 30x for N=1 as~\autoref{fig:AI2} shows. 

\textbf{Graph Neural Networks}: Within graph neural networks, \AccelName achieves the same performance as FLAT(\textit{Pipe+Exp}). The reason for the is that only tensor to be reused across operations in a GNN layer, is pipelineable without additional dependency. These configurations outperform Flexagon-like and its variants. For cora, due to the large feature map size, cache policies perform worse than Flexagon-like with explicit management.

%\textbf{ResNet}: We also evaluate convolution operations in ResNet. At 1TB/sec, ResNet is compute bound because the memory bandwidth is sufficient to saturate the compute. As a result, all configurations, including LRU cache hit the peak performance. However, with memory bandwidth of 250 GB/s, minimum arithmetic intensity for compute bound changes from 16.384 ops/byte to 65.536 ops/byte. %With 128 GB/s, the required arithmetic intensity changes to 131.072 ops/byte.
%We clearly see that \AccelName maintain the roofline performance even with lower memory bandwidth for ResNet, where the baselines cannot. We also show the energy advantage of~\AccelName in~\autoref{fig:energy}.



%\subsubsection{Map-space exploration overhead}
%~\autoref{fig:map-overhead}(a) shows the design-space of buffer allocation with scratchpads. For complex reuse, the search space used by scratchpads is search space of operation-by-operation reuse raised to the power of five affirming~\autoref{sec:arguments} \textbf{Q3}.~\autoref{fig:map-overhead}(b) shows the runtime for GAMMA~\cite{gamma} that maps individual operations. The runtime of G2\_circuit SkewedGEMM is 5us. Even for 100-1000s of CG iterations (well conditioned matrices have even fewer iterations), the map-space time is orders of magnitude higher than the actual runtime, thus leading to extra overhead, if that's the only matrix corresponding to that problem, affirming~\autoref{sec:arguments} \textbf{Q4}.

%We show additional map-space overhead of explicit orchestration in~\autoref{fig:map-overhead} by incorporating the offline mapping search time.\RG{WIP}. We observe that BestIntra+Explicit as xx slower than BestIntra+LRU when we take mapping search overhead into account.~\SpadName avoids this overhead by doing buffer allocation online through its policies and~\DataflowName scheduler can make scheduling desicions online.

%\insertFigure{map-overhead}{(a) Map space overhead of individual operation vs inter-operation reuse (b) Mapping search time for one opertion compared to actual runtime of the operation for different number of iterations.\vspace{-2mm}%\TK{try to make figure more self sufficient - mention which workload this is for etc}
%}

\subsubsection{Off-chip Energy}



~\autoref{fig:energy} shows the workload-wise geomeaned off-chip energy, relative to \textit{BestIntra+Exp}. Our proposed \AccelName has the lowest energy for each workload, and the its reduced by 64\% to 83\%. Cache replacement policies have relatively higher energy compared to the explicit due to conflict misses. FLAT misses out on reuse along delayed dependencies. \AccelName achieves 4x geomean reduction in energy.


\subsubsection{Area and Energy of Buffer Structures}





\autoref{fig:areapower2}a and b show the area and energy comparisons of 4MB buffet, caches and~\SpadNamenospace. The area of buffets is the 6.72 mm\textsuperscript{2} as their controller adds only 2\% area overhead over data array~\cite{buffets}. Cache on the other hand has the total area of 9.87mm\textsuperscript{2}, with data array being 6.59mm\textsuperscript{2} and tag array being 1.85mm\textsuperscript{2}.~\SpadName has the area of 6.74mm\textsuperscript{2}.~\PolicyBnospace-index table requires 0.01x area compared to tag area in cache, to store information about 64 tensors across many operations. Energy per access is far greater for cache compared to buffets and~\SpadName. Infact, sizeable chunk of cache's energy comes from tag comparisons which ~\SpadName avoids.

\subsection{Additional Studies}
\label{sec:sens}

\subsubsection{Efficacy on DNN's with Complex DAGs: ResNet}

\autoref{fig:sens}(a) shows the performance and off-chip energy for all the configurations with ResNet. At 1TB/sec, ResNet is compute bound because the memory bandwidth is sufficient to saturate the compute. However, with memory bandwidth of 250 GB/s, minimum arithmetic intensity for compute bound changes from 16.384 ops/byte to 65.536 ops/byte. We also show the energy advantage of~\AccelName for ResNet. SET~\cite{isca-pip} takes care of delayed hold dependency (but not delayed writeback) and performs same as \AccelName on ResNet. However, it performs the same as FLAT and Flexagon on CG workload and is worse than \AccelName, since CG relies on delayed writeback dependency, which is not supported in SET.

\subsubsection{Sensitivity Study: SRAM Sizes}


\autoref{fig:sens}(b) shows the effect of reducing the~\SpadName capacity on \AccelNamenospace. In particular, \PolicyA is also affected by memory capacity. For larger N, the occupancy of the buffer for a tensor increases, which leads to higher occupancy causing the memory traffic to increase. With increasing size,~\SpadName is able to make replacement decisions more efficiently, even though its not too large for the workload at N=16. For N=1, the 4MB and 16MB SRAM are sufficiently large for the workload.

\subsubsection{\PolicyAnospace-only+Flexagon Configuration}

\autoref{fig:sens}(c) shows the CG results with additional \PolicyAnospace-only configuration to study the effect of \PolicyA alone. For CG, \PolicyA provides an advantage over "Flexagon" baseline and the "FLAT" baseline, since writeback support is more helpful than traditional pipelining support in case of CG. However, \PolicyB policy in \AccelName is effective in considering reuse frequency of a tensor, allows more frequent tensors to stay resident in \SpadNamenospace, which \PolicyAnospace-only does not consider, and hence \AccelName perfroms better. \PolicyAnospace-only configuration is closer to the baselines for N=16, while its closer to \AccelName with N=1. This is because \PolicyA benefits from tensor size relative to the on-chip SRAM.

%\subsubsection{Sensitivity Study - Size}

%\insertFigureScaled{capacity}{Effect of capacity.}{1}

%\vspace{-2mm}



%\textbf{Increasing Cache Associativity}: ~\autoref{fig:assoc} shows the geomean (all datasets in a workload are geomeaned) performance of LRU and BRRIP cache for associativity 16 and 64 and compares it to~\SpadName for all workloads.

%\textbf{\DataflowName+variable backends}