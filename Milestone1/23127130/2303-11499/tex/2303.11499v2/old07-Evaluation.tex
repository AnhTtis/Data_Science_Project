\vspace{-2mm}
\subsection{Results}
\vspace{-1mm}
\label{sec:eval}

\indent \textbf{Throughput for Conjugate Gradient:~}
We model the substrate and sweep the parameters as discussued in~\autoref{sec:expt} for the CG workloads and datasets in~\autoref{table:dataset}, and plot the throughput
%, SRAM accesses and memory footprint of operations
in ~\autoref{fig:AI}.
We expand the last plot to a roofline showing both AI and throughput.
\DataflowNamenospace -\textsc{all} achieves a \reviewme{geomean of 5.97x improvement in AI over the Flexagon-like baseline, ranging from 1.34x to 23x}. This translates to geomean 5.25x improvement in throughput.

We discuss key observations and insights next.
In CG, all the tensors that can be pipelined also have a delayed consumer and as a result, the adjacent pipelining baseline cannot overwrite the fan out tensor as that will violate the later dependency, thus showing same performance as op-by-op (i.e., Flexagon-like).

\reviewme{For a dataset with tensors smaller than the SRAM size for example, fv1 and nasa4704 at 4 and 16 MB SRAM}, \DataflowNamenospace -\textsc{all} and \DataflowNamenospace -\textsc{Qpad} achieve the peak throughput, which is orders of magnitude more than the Flexagon and adjacent pipeline baselines.

\reviewme{For large datasets like G2\textunderscore Circuit, for small memory size, Flexagon-like and \DataflowNamenospace -\textsc{Qpad} have the similar AI and \DataflowNamenospace -\textsc{all} achieves roughly 30 to 50\% better throughput} than them due to pipelining\_with\_writeback which avoids reading the operand from the DRAM for the consecutive operation. However, given 16MB SRAM size, G2\textunderscore Circuit, it can show better improvement with N=1. Though large datasets are not executed on a single compute node and are often distributed on machines. We also show the communication reduction, \DataflowNamenospace -\textsc{all} can achieve due to tiling in~\autoref{fig:comm}. 

For all the intermediate data points, where SRAM is neither too large and nor too small, there is a clear upward trend between Flexagon-like(=Adjacent pipelining), \DataflowNamenospace -\textsc{Qpad} and \DataflowNamenospace -\textsc{all} in terms of throughput. This is due to a combination of multiple reasons, including pipelining\_with\_writeback, scratchpad management based on reuse distances of downstream tensor operations and the ability to achieve zero swizzle penalty with~\DataflowName.



%\reviewme{To summarize, we show geomean improvement in \DataflowNamenospace -\textsc{all} over Flexagon-like dataflow for each dataset in~\autoref{fig:AI}}

%\input{tables/geo}

Thus \DataflowNamenospace -\textsc{all} achieves the best performance due to pipelining, swizzle aware loop ordering and due to scratchpad management strategies.
%We also evaluate DRAM accesses for Graph Neural Networks  in~\autoref{sec:GNN} to demonstrate the generality of \DataflowName.
%For Conjugate Gradient, we evaluate 10 iterations, for GCNs, 1 layer and for ResNet, we evaluate one residual block.% We also compare inter-cluster communication between GOGETA-df and GOGETA-map to demonstrate the benefit of pipelining with fine-grained spatial multiplexing. %We also evaluate a set of loop orders with loop orders that support pipelining but incur swizzling. 

%\TK{@Raveesh -- need to highlight following points: geomean speedup, impact of sram size, and how far you are from ideal}
%\vspace{-.5mm}
%\subsubsection{Roofline Analysis for Graph Neural Networks}
\label{sec:GNN}
\label{sec:archvision}




\indent \textbf{Throughput plots for Graph Neural Networks:~} We evaluate GCN for SRAM size of 1MB as~\autoref{fig:GNN}a shows. Cora has a large feature matrix $M\times N$ but still has low arithmetic intensity due to low number of non zeros per row. There is a marginal improvement in \DataflowNamenospace -\textsc{Qpad} but considerable improvement with pipelining and hits the ideal. Protein dataset is relatively small and the tensors fit in the SRAM, thus both \DataflowNamenospace -\textsc{Qpad} and \DataflowNamenospace -\textsc{all} hit the ideal. Graph Neural Network layers do not have downstream tensors using the same data thus pipelining makes it easier to achieve the ideal DRAM accesses. Adjacent pipeline also achieves equal throughput as \DataflowNamenospace -\textsc{all} due to adjacent pipeline opportunities without delayed dependencies.

\textbf{Throughput plots for ResNet and SSD ResNet:} ~\autoref{fig:GNN}b and c show throughput plots for ResNets and SSD-ResNets. They always hit the peak being compute bound (\autoref{fig:roofline}). Thus pipelining is not helpful for performance, but can potentially help reduce energy, as we discuss next.

\textbf{DRAM access reduction}:~\autoref{fig:comm}a shows the DRAM access reduction across various applications. The adjacent pipeline baseline shows major reduction in DRAM accesses in all applications except CG due to delayed dependencies.
For ResNet and SSD-ResNet,
\DataflowNamenospace-\textsc{All} leads to additional reduction in DRAM accesses because of the ability to pipeline\textunderscore with\textunderscore hold as~\autoref{fig:buffer} shows. %\TK{@Raveesh - point back to Figure with ResNet pipeline with hold and remind that adj pipeline wont pipeline that}. 
Also, between ~\DataflowNamenospace-\textsc{Qpad} and adjacent pipelining, adjacent pipelining is better in case of SSD-ResNets and GNNs as~\DataflowNamenospace-\textsc{Qpad}'s benefit diminishes with larger working sets. However, in CG, adjacent pipelining has no benefit, and in ResNet, the working set is small enough for ~\DataflowNamenospace-\textsc{Qpad} to fully reuse the activation on chip.
%This is because, between adjacent pipeline and \DataflowNamenospace-\textsc{Qpad}, the better dataflow depends on the amount of adjacent pipelining that can be done and on the working set size.
%\TK{Raveesh - pls elaborate on the above explanation .. didnt understand. can u explan why a large or small working set affects Qpad performance?}
 ~\DataflowNamenospace-\textsc{All}, combining both of these and more optimizations, has the best DRAM access reduction for all the workloads.


\indent %\reviewme{\textbf{Roofline plots for Attention Layers:~}
%We evaluate attention layers for SRAM size of 4MB as~\autoref{fig:bert} shows. For sequence length of 64, \DataflowNamenospace -\textsc{Qpad} achieves near ideal and \DataflowNamenospace -\textsc{all} achieve the ideal AI. Note that even though, the baseline intensity relatively high, \DataflowName would allow the substrate to have a lower memory bandwidth. With increase in B and L, the gap between Flexgon-like and ideal widens. For sequence length of 256, baseline achieves significantly lower portion of the ideal, while \DataflowNamenospace -\textsc{Qpad} near ideal AI and \DataflowNamenospace -\textsc{all} achieves the ideal in case of B=1 At B=4, L=256, the working set becomes too big for the SRAM, \DataflowNamenospace \textsc{all} still achieves 60\% of the ideal AI due to fusion of operations.}
%\subsubsection{DRAM accesses in Graph Neural Networks}:

% Considers best case for GOGETA-df and worst for GOGETA-map.}
%\vspace{-2mm}
\label{sec:comm}
\indent \textbf{Impact of tiling on scalability:~}
%Pipelining between two operations involves, communicating the data between the parts of the compute where the operations are mapped.
~\autoref{fig:comm}b shows inter-cluster link traversal for with and without the tiling consideration in KB's when considering pipelining between operations defined in line 4 and 5 of~\autoref{alg:cg_einsum}. As~\autoref{sec:tiling} shows, splitting the dominant rank makes the inter-cluster traversal independent of M. Workload can scale up considerably and the dimension that will scale is $M$. By changing these to intra-cluster traversals, \DataflowName and even adjacent pipelining can make better use of the high bandwidth NoC. This analysis can be extrapolated to distributed systems where the cross compute node bandwidth is even worse than inter-cluster bandwidth.

\textbf{Impact of Memory BW}: Limiting the memory bandwidth further affects the throughput of the memory bound region and it increases the AI required to hit the peak.~\autoref{fig:comm}c shows the throughput of FV1 with memory BW of 1000 and 250 GB/s. With memory BW=250 GB/s, the performance in the memory bound region is lower and \DataflowNamenospace-\textsc{All} performs better than \DataflowNamenospace-\textsc{Qpad} due to higher arithmetic intensity.

