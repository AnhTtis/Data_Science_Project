%\vspace{-3mm}
\section{Introduction}
\label{sec:introduction}


\begin{comment}
    Points to be made in the intro
        - High value GEMM workloads have complex dependencies []
        - Traditional pipelining does not respond well to these dependencies
        -Keeping operands on-chip is key to get a good performance.
                - Caches - Implicit and Workload agnostic + hardware overheads
                - Spads - Explicitly managed (large map-space and expensive search and buffer allocation)
                - CHORD: A hybrid implicit and explicit management, which is cycle-level implicit and coarse-grained explicit, so the cost lowers enough to manage the SRAM online..
        - SCORE: Fast scheduler for low intensity ops that deals with more complex dependencies, and also provides metadata to CHORD..
        - Skewed GEMMs can have surprisingly low arithmetic intensity??
        
    


    
\end{comment}





As Deep Learning (DL) emerged as a high-value workload, the computer architecture community responded by proposing custom accelerators for DNNs~\cite{eyeriss2016isca,kwon2018maeri,tpu-isca,nvdla}. 
The most common fundamental operation for these DNNs was matrix multiplication, often expanded from convolutional layers that were common in DNNs at that time. These GEMMs offered good reuse opportunities because of their large dimensions and relatively cubic aspect ratios, allowing early DNN accelerators to successfully schedule each layer independently and achieve maximum utilization for the networks they targeted at design-time. Simultaneously, these schedules could be efficiently implemented using \emph{scratchpad} buffers that were explicitly controlled by the schedule to the stage intermediate \emph{tiles} of data according to the traversal order within each GEMM, without concern for inter-layer efficiency.

%Furthermore, prior works have also looked at the accelerators for sparse tensor workloads~\cite{sigma,eie,extensor,eyeriss2} which eliminate irrelevant multiplications and memory accesses.}% Unfortunately, this approach lowers intensity even further, and straightforwardly applying these to CG on a GEMM-by-GEMM operation basis results in compute under-utilization due to limited memory bandwidth.


Recently, work like the Tensor Algebra COmplier (TACO) \cite{XXXTaco} has spurred research interest in generalized tensor applications, including sparse tensor algebra. %The computer architecture community has responded with recent proposals that generalize custom accelerators to efficiently target a superset of DNNs \cite{extensor, tensaurus, dave2020hardware, tensorlib, flextensor} namely, applications represented as arbitrary DAGs of \emph{tensor operations} and non-linearities for DL activation functions.
However, this generalization brings new challenges: tensor-algebra applications have diverse shapes, sparsity ratios, and dependency graphs. %Worse, this is coupled with a trend in DNNs away from large, monolithic GEMMs and towards numerous small independent matrix multiplications\footnote{Often called \emph{batched} GEMMs, which should not be confused with DNN batch size.} with less cubic aspect ratios.
This is significant because of an underappreciated fundamental property: namely, \textit{not all dense GEMMs with large number of multiplications are compute-bound even in the best case}. As shown in \autoref{fig:ai1}, a \emph{skewed} aspect ratio inherently decreases \emph{arithmetic intensity} (AI), thus making the individual GEMM memory-bound and leaving datapath resources idle. For example, to saturate its datapaths the TPU v3 and v4 architectures require an AI of approximately 150 and 250 respectively~\cite{tpuv4}.  %Sparse tensor algebra introduces similar challenges, as removing multiplication-by-zero decreases the AI numerator, while transferring \emph{metadata} for compression formats increases the denominator relatively.}

\insertFigureScaled{ai1}{Degradation of arithmetic intensity on two GEMMs with the same number of multiplications due to aspect ratio skew.\vspace{-3mm}}{.75}



%\insertFigure{no-fusion}{Pipelining cannot simply be applied to complex DAGs due to - 1) Delayed downstream dependency, 2) varying shapes, 3) consumers at multiple reuse distances 4) need to preserve layout across conusmers.\vspace{-3mm}}


%The reason for this is low arithmetic intensity for a skewed GEMM and it can be as low below 1 op/byte. Currently, most scheduling strategies optimize matrix multiplications independently and compose them to execute in an op-by-op manner, which leads to low performance in applications with skewed GEMMs.}

\begin{comment}
A real-world quantification of this phenomenon is captured by HPCG benchmark~\cite{dongarra2015hpcg,hpcg2021}, which runs Conjugate Gradient (CG)---a widely used HPC solver application represented as a DAG of tensor operations.

\input{tables/hpcg}

%\footnote{HPCG is a benchmark for supercomputers that runs CG. \MP{Footnote 1 adds no real information. Cut. }}
As \autoref{tables:hpcg} shows, CG achieves only 1-3\% of peak performance on top 7 supercomputers. 
Therefore, intra-operation reuse alone is not sufficient.
The overall throughput can be increased by improving the arithmetic intensity (i.e., on-chip data reuse), which can be done by seeking inter-operation reuse.
\end{comment}

Thus to achieve full utilization we must consider inter-GEMM operation scheduling. Prior works have also shown that simple pipelining of adjacent operation~\cite{tileflow,isca-pip,yan2020hygcn} can improve things, but these solutions exclude significant potential sources of reuse, including delayed downstream consumers, and multiple consumers with varying reuse distances or traversal orders as~\autoref{fig:no-fusion} shows. To make matters worse, these extra options explode the scheduling space for finding good scratchpad configurations, as the total combinations and proportions of allocations explodes with operation DAG depth and the number of tensors involved. This means that exhaustive schedule-space exploration techniques become unacceptably slow, and also that heuristic solutions are more likely to miss the true optimal.

%It is also challenging to simply apply traditional inter-operation pipelining in cases of these dependencies, because of the delayed downstream dependencies, varying shapes of skewed GEMMs and multiple of these downstream consumers as~\autoref{fig:no-fusion} shows. %\TK{Fig 4 getting referenced before Fig 3, i suggest bring it before Fig 4 then} 
%~\autoref{fig:dfg} shows an actual complex cascade of tensor operations found in CG, a high-value HPC workload rich in these complexities.
%Thus, we need mechanisms to cache these intermediate tensors within SRAMs, since there are situations where simple pipelining cannot be applied.

%Prior works like buffets~\cite{buffets} make use of \emph{explicit decoupled data orchectration} to supplement scratchpad RAM with simple pointer and credit management scoreboarding. This is called explicit because data placement and RAM replacement are schedule-controlled, and decoupled because it operates on bulk-synchronous fills. %These work well for executing one layer at a time, when all dimensions of matrices have sufficient reuse.
%While explicit orchestration works well for scheduling single matrix multiplication at a time, considering data placement, for inter-operation reuse statically in general tensor algebra is a diabolically hard problem. 
% In order to reuse such operands, specially where intra-operation reuse is simply not enough, storing these operands in the on-chip buffer is essential, and multiple of these operands contend for space inside the buffer. We show in~\autoref{sec:arguments} that the design-space to accommodate multiple tensors inside a buffer space explodes in complexity. ~\autoref{sec:arguments} also discusses why running statically known DAGs does not necessarily imply that scratchpads are not burdensome and don't have design-time overheads.
%Overall, the co-dependence on schedule, available scratchpad capacity and different downstream consumers make the explicit scheduling problem too challenging.

\insertFigure{no-fusion}{Pipelining cannot simply be applied to complex DAGs due to - 1) Delayed downstream dependency, 2) varying shapes, 3) consumers at multiple reuse distances 4) need to preserve layout across conusmers. We discuss this in detail in~\autoref{sec:nofusion}\vspace{-3mm}}

Of course, scratchpads can be supplemented with pointer and credit management logic to become queues or buffets~\cite{buffets}, but these do not solve the schedule explosion problem as their staging decisions are still \emph{explicitly} controlled. Alternatively, caches are widespread on-chip storage structures that use \emph{implicit} data orchestration. Ideally, the presence of the cache is not architecturally exposed and the hardware itself does the best job possible of reusing the data. (In practice, best optimization is often made by cache-aware scheduling policies, blurring the line between implicit and explicit.) However, the area and energy overhead for tag matching make it less appealing for custom accelerators, as well as the possibility of increased misses due to conflicts. Overheads aside, cache policies typically operate at line granularity of unified address streams, rather than having higher-level knowledge of tensors, blocks, or intended reuse distance. This results in cache policies often rejecting the data that may have high reuse frequency (by virtue of reuse of the whole block) but might not be the immediate vicinity (i.e., high reuse distance).

In order to effectively exploit all available sources of reuse, we propose a unique approach: co-designing the buffer storage idiom with the scheduling algorithm. Our goal is that it becomes tractable to find schedules that obtain a sufficient level of inter-operation reuse to reach the compute bound, even in the face of complex DAGs of dependencies. To achieve this, we propose ~\SpadNamenospace\footnote{\SpadNameexp}, a novel \emph{explicit+implicit hybrid} buffering scheme that aims to combine the ``best of both worlds'' by placing coarse-grained decisions under the explicit control of the scheduler for efficiency, while making low-level fine-grained decisions implicitly like a cache, thus significantly reducing the size of the schedule space.
%~\SpadName is workload-aware, as it uses high-level metadata like starting and ending global address of a tensor, tensor-level reuse frequency and distance from the workload (the explicit component), but implicitly controls placement of elements of tensors without being burdensome to the programmer. ~%\TK{it might be worth adding a footnote saying you use scratchpad and buffer interchangeably in this paper} 
%\TK{based on what the footnote says its unclear whether chord is a scratchpad or cache }
Notably, this approach also significantly reduces the area and energy overheads of traditional line-level caches. %Specifically,~\SpadName uses per-tensor replacement policies that can be configured by the schedule at coarse granularity.
%for operands with downstream consumers that uses implicit replacement at a tensor granularity rather than a line granularity. 
%In this work we propose two such policies- \\(1) \PolicyA - \SpadName is filled in the queue order and once the buffer is full, the spilling data is sent straight to the DRAM.\\
%(2) \PolicyBnospace\footnote{\PolicyBexp} - If the current tensor has a higher reuse frequency and lower reuse distance (in case of same frequency) than the previously written tensor, the current tensor starts replacing the previous tensor by the tail.
%Because of its extremely coarsened granularity, ~\SpadName retains the benefit of a scratchpad over a cache (i.e., with minimal (<1\% of that of cache) tag matching overhead) 
%Hence it gets rid of caches' tag matching overhead and 
%while
%our proposed implicit tensor replacement policies~\PolicyA and \PolicyB remove the challenge of statically coming up with the data placement and replacement strategies statically for operands in multiple operations.

To schedule accelerators that use this structure, we propose ~\DataflowNamenospace\footnote{\DataflowNameexp} a downstream dependency aware novel scheduling strategy. 
\DataflowName identifies the delayed downstream dependencies that require writeback from those where pipelining would work, and steers the tensors with delayed writeback dependencies to~\SpadNamenospace. In order to exploit the reuse on delayed downstream consumers, it is important to make sure that the order in which the elements are produced as same as the order in which they are consumed, otherwise, layout transformation is required. Unlike prior mappers which search for tile-sizes for fine-grained buffer allocation, \DataflowNamenospace's involvement in buffer allocation is coarse-grained at an operand granularity rather than element-wise granularity, and low-level fine-grained decisions are made implicitly by~\SpadNamenospace's policies. 

All in all, we show that \SpadName and \DataflowName are applicable to diverse tensor applications and achieve 2.51x geomean speedup and 4x improvement in energy efficiency across a broad range of workloads (\autoref{sec:eval}).

%\TK{minimizing layout transformation is an important feature. Lets highlight it more explcitly. As in mention that the dataflow of the current op and downstream op may be different requiring expensive layout transformation, and SCORE tries to minimize that.}%It consists of the following steps - \\ 
%\TK{there seems to be some missing text here?}
%(1) Marking all the edges in the DAG of tensor operations with pipelining opportunities. This is useful in situations where tensors have delayed downstream consumers which can be visualized as a long edge in the DAG.\\
%(2) Assigning loop orders and tiling strategies to ensure that pipelining is actually exploited and layout transformation overhead of a tensor is minimized.\\
%\DataflowName reduces the contending tensors and also ensures minimum layout transformation (aka swizzling) which is also the motivation behind~\PolicyA policy.
%The main reason is the low arithmetic intensity achieved by the execution of Conjugate Gradient on CPUs/GPUs. 
%The main reason for this that the tensor multiplications---which we term \emph{operation} for this paper---%\MP{This definition is also too important to be in a footnote
 %used in CG have extremely unbalanced aspect ratios, which significantly lowers arithmetic intensity and data reuse.} %\MP{I feel like this point should come earlier, perhaps before the previous paragraph.}
%We use the term \emph{skewed} GEMMs to refer to such SpMMs/GEMMs---though notably, in the limit they can devolve to matrix-vector multiplication, i.e. if the shape of the matrix is 100,000:8.}%The main reason for this is costly data movement and communication between compute clusters/pods\MP{This explanation doesn't make sense either. Needs a reuse/intensity component. Even if this made sense, does GOGETA actually fix this problem?}.




%\TK{@Raveesh - I think we can move this para to para 3. Basically start with current para 2 on GEMMs, reuse and spatial accelerators. Then introduce CG. So intersperse this para with current para 3}


%making inter-operation reuse essential.

%This work explores inter-operation reuse opportunities for kernels like CG to enhance its arithmetic intensity.

%However, the~\GEMM in CG have orders of magnitude lower arithmetic intensity compared to SpMMs/GEMMs in DNNs and offer less reuse opportunities within one operation as~\autoref{fig:ai}~(\autoref{sec:ai}) discusses later in the paper. 
%Thus, executing CG at the granularity of a GEMM, reading tensors from DRAM and writing the output of each GEMM to DRAM, makes it highly memory bound leading to low compute utilization as~\autoref{\:hpcg} shows. 
\begin{comment}
Fusing \textit{adjacent} low AI matrix multiplication operators has recently been leveraged for applications like Graph Convolutional Networks (GCN)~\cite{garg2021understanding,yan2020hygcn,liang2020engn} and Transformers~\cite{flat,flashattention} 
wherein tiles of the intermediate tensor are manifested and consumed within the 
on-chip memory hierarchy in a \textit{pipelined} manner, reducing  intermediate output data movement to and from DRAM. We call this \emph{adjacent-op pipelining} in this work.
Other recent works have looked at enumerating the design-space for such adjacent-op pipelining~\cite{garg2021understanding} and 
%\TK{is this what the isca paper does?}\RG{Yep}
identifying optimal pipelined dataflow choices~\cite{isca-pip}, thereby enhancing better compute utilization when running memory-bound tensors.

While traditional adjacent-op pipelining is promising for DAGs with linear chains of operators (i.e., most DNNs today and GCNs), we show in this work that it is insufficient to capture nuances in more complex DAGs, such as those used in HPC kernels like CG.
Specifically, delayed downstream consumption of fanout of tensors, implies that traditional pipelining which overwrites the previously consumed tile cannot be applied directly due to a later dependency.%~\autoref{fig:no-fusion} shows an example, where tensor S has a delayed consumer.
 Moreover nuances such as data layouts of one tensor consumed in various operations is not captured by adjacent pipelining.
%that prior works cannot capture, as we discuss later. \autoref{table:related} enumerates this.

In this work, we coin the term \emph{generalized inter-operation reuse} to widen the scope of inter-operation reuse beyond adjacent operators to include additional reuse opportunities (\autoref{table:related}).
%as a wider generalization of traditional \emph{adjacent-operator pipelining}, introducing three additional reuse opportunities (\autoref{table:related}). 
We also propose \DataflowName (\TitleExpansion), which is a systematic strategy for mapping DAG of tensor operations exploiting the generalized inter-operation reuse opportunities, with the goal of enhancing the AI of the overall application.
%\DataflowName is applicable to any DAG of operations. 
We demonstrate that while \DataflowName is essential for extracting reuse in complex DAGs like CG, it can also be applied to simpler DAGs like GCNs and DNNs, thus preserving generalizability.

%\insertFigurePartnnnn{no-fusion}{A part of the CG tensor dependency graph where a node represents the equation in~\autoref{alg:cg_einsum} and edge represents the output of the source node equation. Please refer to ~\autoref{fig:dfg} for complete graph of CG.\vspace{-3mm}}%\RG{contraction heavy}}
We summarize our contributions below:
%\TK{I think the contribitions can be listed more succinctly. I would suggest each contribution bullet pointing to a specific section of paper. }

\squishlist
%\item We characterize the challenge of skewed GEMMs/SpMMs in tensor-algebra applications and the challenges of generalizing traditional inter-operation pipelining~\footnote{Not to be confused with inter-operation "reuse" since pipelining is a narrow aspect of inter-operation reuse}. (\autoref{sec:ai}).
%We also observe that these patterns are frequent across other HPC workloads which we discuss in ~\autoref{sec:background}.
%\item We propose a systematic methodology to formulate the data reuse opportunities in an arbitrary DAG of tensor operations and based on the reuse opportunities, we derive the loop orders and tile sizes.(\autoref{sec:dataflows}).
%\item We co-optimize the loop orders and tiling strategies for the GEMM operations in order to leverage reuse from both traditional inter-operation pipelining and distance based inter-operation reuse.

\item We propose~\DataflowName (\autoref{sec:score}), a scheduler which identifies the operands with delayed dependencies that require writeback (and hence \SpadNamenospace), and proposes a schedule that maximizes inter-operation reuse and minimizes the layout transformation of the operands across different consumers.
\item We propose~\SpadName(\autoref{sec:chorus}), a buffer structure for operands with downstream consumers that uses tensor-operand level replacement, that reduces tag match overhead and considers a more wholistic view of the object rather than a cache line. Cycle-level implicit tensor level replacement also eases the burden of solving tensor allocation involving multiple tensors statically which is a hard problem. 
%\item %Prior works on pipelining~\cite{flat,tangram,garg2021understanding,yan2020hygcn} divide the whole compute region into contiguous chunks and an operation is mapped on to a chunk as~\autoref{fig:spacetime} (top sub-figure) shows.\TK{seems odd to cite Fig 12 in intro}
%However, there is not much reuse within a single operation and the whole tensor needs to be communicated between the chunks. 
%\TK{this seems like a very specific optimization - and confusing here about what is lower BW vs higher BW NoC .. in fact i dont recall our arch section / Table IV talking about two NoCs with diff BWs? cant you state this contribution more generally about a communication BW optimized scalable inter-operation tiling strategy?}
%We propose a scalable inter-operation tiling strategy that reduces inter-cluster communication(~\autoref{sec:tiling}).  %a mapping that reduces memory accesses and communication at the tensor dependency graph level.
%\end{itemize}
%\item We propose buffer management schemes that reuse initial tiles of the output and can also replace the tensor based on future reuse distance and frequency~(\autoref{sec:tornado}). %the notion of Tensor-operand level reuse distance for such patterns and a tensor organization strategy inside the buffer hierarchy (\autoref{sec:tornado}).

\item We demonstrate the limitations of caches, namely, area overhead and line-level policies and the limitations of scratchpad, namely complexities in static buffer allocation of multiple delayed downstream consumers (\autoref{sec:arguments}).

\item We show that \SpadName and \DataflowName are applicable to diverse tensor applications and achieve 2.51x geomean speedup and 4x improvement in energy efficiency across a broad range of workloads (\autoref{sec:eval}).

%\item \reviewme{ } 
%\item We propose a novel data orchestration technique \DataflowName which allows for efficient replacement, placement and prefetching of data for applications with variable reuse distance patterns. Since reuse distance is a generalization, \DataflowName can be used for applications with only intra-operation reuse and inter-operation pipelining opportunities as well.
%\item We propose a novel buffer idiom \TODO{name} which improves performance and energy over caches and scratchpads by \TODO{xx} and \TODO{xx}
\squishend
\end{comment}


\begin{comment}
\emph{Inter-operation pipelining (aka fusion)} has been shown to be beneficial for accelerators for applications like Graph Neural Networks~\cite{garg2021understanding,yan2020hygcn,liang2020engn} where the intermediate tensor is reused between an SpMM and a GEMM,  reducing  intermediate output data movement to and from DRAM.
It has been shown that this approach is more challenging~\cite{dnnfusion,flat} and has a larger design-space~\cite{garg2021understanding} than straightforward element-wise fusion done by ML compilers today, for example, matrix-multiplication and ReLU fusion.
%Unfortunately, \textit{traditional inter-operation pipelining} often consumes the intermediate data and does not keep it in the memory. 
%Traditionally, \textit{inter-operation pipelining} has been exploited in prior works~\cite{tangram,garg2021understanding,yan2020hygcn,flat} %across various application domains 
%to reduce the data movement to DRAM, by consuming the portion of the tensor as it is produced 
\TK{I think we need to transition to saying that inter-operation pipelining does not capture all inter-operation reuse opportunities as this work identifies.}
Unfortunately, the additional complexity of inter-operation dependency graphs in certain applications introduces additional challenges which confounds the attempts at \emph{traditional inter-operation pipelining.} Furthermore, just capturing reuse in adjacent operations misses the overall opportunity to consider future instances of reuse of tensors in the entire program. \reviewme{We use CG as an example to discuss these challenges and missed opportunities:}

(1) Operations can have a delayed downstream consumer. Therefore, the data must remain resident in the memory hierarchy. However, traditional pipelining overwrites tiles that are consumed by the adjacent operation.
%~\autoref{fig:dfg} shows CG's dependency graph. Output of operation 1 is required in a delayed downstream consumer (op4) in addition to the adjacent consumer (op2). %The intermediate matrix cannot be overwritten or discarded, since its required in a future computation. 


%This complex dependency graph can often complicate the optimization of loop orders and tiling strategies to determine efficient intra-operation and inter-operation dataflows.


(2) CG has some tensor operations with contracted rank being much larger than the other ranks. This diminishes the benefit of pipelining because significant computation is required to produce any given final sum; therefore, making it a rate limiting step. 
%This prevents pipelining the entire graph.
%\TK{the following line seems out of place - since its one of many techniques we propose. Should maybe remove it}
%We propose \textit{pipelining with writeback}, which involves traditional pipelining and writing back the intermediate data to the buffer, hence incurring only write accesses and avoiding the read accesses for the intermediate matrix.

\insertFigure{no-fusion}{A part of the CG tensor dependency graph where a node represents the equation in~\autoref{alg:cg_einsum} and edge represents the output of the source node equation. The data cannot be consumed and be shielded from memory hierarchy since its reused again in another tensor. Please refer to ~\autoref{fig:dfg} for complete graph of CG.\vspace{-3mm}}%\RG{contraction heavy}}

(3) As the DAG becomes more complex, it is important to make sure that the loop order choice minimizes the data layout transformation (we use the term swizzling for it) of these tensors across various consumer operations.

(4) The downstream consumers also result in multiple \textit{tensor operand reuse distances}.

In this work, we identify various \textit{inter-operation} reuse opportunities to enhance the arithmetic intensity of such applications.
Our proposed \emph{generalized inter-operation reuse} is a wider generalization of \emph{traditional inter-operation pipelining}. We propose \DataflowName (\TitleExpansion), a strategy for mapping DAG of tensor operations which exploits reuse opportunities between operations.

\DataflowName is applicable to any DAG of operations. So while it helps extract reuse in the complex DAGs like CG, these patterns can also apply to simpler DAGs like GCNs, thus preserving generalizability.

%\reviewme{\DataflowName consists of the following contributions.

 %First, we identify the inter-operation reuse patterns in an arbitrary DAG of tensor operations. We propose a methodology to mark the edge of the DAG with the appropriate reuse pattern. This addresses the DAG complexity problems.

% Second, we propose loop orders that try to take the maximum advantage of the reuse patterns, given that the ability to extract reuse also depends upon the order of loops in these operations. The loop order also minimize swizzling

 %Third, prior works on pipelining~\cite{flat,tangram,garg2021understanding,yan2020hygcn}, divide the whole compute region into contiguous chunks and an operation is mapped on to a chunk as in~\autoref{fig:spacetime} (top sub-figure) shows. However, there is not much reuse within a single operation and the whole tensor needs to be communicated between the chunks. Instead, we pipeline within the cluster and split dominant ranks across cluster (\autoref{fig:spacetime} bottom).


% Fourth, given that we minimize swizzling, we propose a buffer management scheme that writes the data into the SRAM in the same order and skips the SRAM once its full. Then it reads the portion that's already in the SRAM first. We show that this can improve the op-by-op baseline considerably. We also propose buffer management strategies to prioritize operations with low reuse distances and high reuse frequencies.

% }

%In this work, we identify various \textit{inter-operation} reuse opportunities which are not limited to \textit{inter-operation pipelining}.
%Please note that \textit{inter-operation reuse} is a wider generalization of \textit{traditional inter-operation pipelining} here, since inter-operation pipelining only exploits reuse between consecutive operations. \autoref{fig:venn} shows the scope of our work on inter-operation dataflows compared to prior works on acceleration.
%One example is \textit{pipelining with writeback}

%Moreover, it is important to focus on leveraging the knowledge of future occurrence of the tensor beyond the immediate tensor. In CG, these tensor operands have variable \textit{tensor operand reuse distances}. This pattern is frequent in various scientific applications as~\autoref{sec:background} shows. This complex dependency graph can often complicate the determination of loop orders and tiling strategies to determine efficient intra-operation and inter-operation dataflows.

%Also, given that the graph of tensors becomes more complex, it is important to make sure that the loop order choice minimzes the transformation of layouts of these tensors across various operations where that tensor is used. We use the term \textit{swizzling} for the layout transformation. Thus we need to minimize swizzling.

%\RG{Can we cut this para and make contributions slightly longer ?}
%Based on the insights from the tensor dependency graphs of various applications, we propose a systematic methodology to identify, classify and exploit reuse opportunities in an arbitrary graph of tensor operations targeting spatial accelerators. This also involves deriving the loop orders and tile sizes for individual operations since the ability to exploit the inter-operation reuse can also depend on the individual operation's dataflow.
%Some other applications which have individual \GEMM of low arithmetic intensity include Graph Neural Network, Transformers etc. where our methodology is applicable, however, we often refer to Conjugate Gradient for demonstration purposes in this paper since its tensor dependency graph already has the characteristics of low intensity Graph and ML workloads but also has additional unique characteristics providing opportunity to demonstrate different kinds of inter-operation reuse.
%We also propose a scalable tiling strategy that involves splitting a memory bound GEMM by the dominating rank into sub-tensors and reusing the data in a fine-grained manner between the sub-tensors from different operations within a compute node as the bottom half of~\autoref{fig:spacetime} shows.
% (shown later in~\autoref{fig:spacetime})\TK{not sure if we need to cite a figure that'll come this late. Might be better to point to the section that will discuss this}. 
%
% We propose \DataflowName (\TitleExpansion), a strategy for mapping DAG of tensor operations which exploits reuse opportunities between operations. It comprises of identifying inter-operation reuse patterns from the DAG structure, loop-reordering and tiling based on those patterns and custom scratchpad management strategies. 
Our key contributions are as follows:
%\vspace{-5mm}

%\begin{itemize}
\squishlist
\item We characterize the challenge of skewed GEMMs/SpMMs in tensor-algebra applications and systematically formulate \emph{generalized inter-operation reuse opportunities} (beyond pipelining) in a DAG of operations (\autoref{sec:dataflows}).
%We also observe that these patterns are frequent across other HPC workloads which we discuss in ~\autoref{sec:background}.
%\item We propose a systematic methodology to formulate the data reuse opportunities in an arbitrary DAG of tensor operations and based on the reuse opportunities, we derive the loop orders and tile sizes.(\autoref{sec:dataflows}).
%\item We co-optimize the loop orders and tiling strategies for the GEMM operations in order to leverage reuse from both traditional inter-operation pipelining and distance based inter-operation reuse.
\item Based on the reuse opportunities, we derive loop orders for the operations that allow consumers to maximally reuse the data, and require minimum changes to memory layout~(\autoref{sec:loop}).
\item Prior works on pipelining~\cite{flat,tangram,garg2021understanding,yan2020hygcn} divide the whole compute region into contiguous chunks and an operation is mapped on to a chunk as~\autoref{fig:spacetime} (top sub-figure) shows. However, there is not much reuse within a single operation and the whole tensor needs to be communicated between the chunks. We propose a scalable inter-operation tiling strategy which splits the dominant rank across compute clusters and pipelines the operations within a cluster as~\autoref{fig:spacetime} (bottom) shows~(\autoref{sec:tiling}).  %a mapping that reduces memory accesses and communication at the tensor dependency graph level.
%\end{itemize}
\item We propose a buffer management scheme that writes the data into the SRAM in the same order and skips the SRAM once its full. Then it reads the portion that's already in the SRAM taking advantage of swizzle minimzation. We also propose buffer management strategies that prioritize operations with low reuse distances and high reuse frequencies~(\autoref{sec:tornado}). %the notion of Tensor-operand level reuse distance for such patterns and a tensor organization strategy inside the buffer hierarchy (\autoref{sec:tornado}).
\item \DataflowName achieves \reviewme {geomean 5.97x (ranging from 1.34x to 23x)} improvement in the arithmetic intensity over operation by operation execution~(\autoref{sec:eval}).

\item \reviewme{\DataflowName is generally applicable to diverse tensor applications as it applies to all DAG structures. We also evaluate it on GCNs and ResNets and obtain 2.71x and xx geomean improvement in arithemetic intensity over op-by-op baseline~(\autoref{sec:eval}).} 
%\item We propose a novel data orchestration technique \DataflowName which allows for efficient replacement, placement and prefetching of data for applications with variable reuse distance patterns. Since reuse distance is a generalization, \DataflowName can be used for applications with only intra-operation reuse and inter-operation pipelining opportunities as well.
%\item We propose a novel buffer idiom \TODO{name} which improves performance and energy over caches and scratchpads by \TODO{xx} and \TODO{xx}
\squishend
\end{comment}

\begin{comment}

\begin{enumerate}
    \item Identifying inter-operation reuse patterns in an arbitrary DAG of operations
    \item Deriving loop orders to make sure that they are amenable to inter-operation reuse and minimize swizzling
    \item Proposing tiling strategy that does pipelining within the compute node and splits the large rank across nodes (bottom half of~\autoref{fig:spacetime}).
    \item Proposing scratchpad management strategies like reuse distance and frequency based tensor replacement and proposing strategies to extract reuse from the portion of the tensor that does fit in SRAM.
    
\end{enumerate}
\vspace{-1mm}
Our methodology is also applicable to %(and evaluated on) 
other applications with low intensity GEMMs like GNNs and transformers. %where our methodology is applicable,
However, we often refer to Conjugate Gradient as the application for demonstration purposes since its tensor dependency graph has the characteristics of low intensity Graph and ML workloads and also has additional unique characteristics providing opportunity to demonstrate different kinds of inter-operation reuse.
\autoref{fig:venn} shows the scope of our work on inter-operation dataflows compared to prior works on accelerator dataflow.
%Our proposed \textit{inter-operation reuse} is a wider generalization of \textit{traditional inter-operation pipelining} here, since inter-operation pipelining only exploits reuse between consecutive operations.
%\autoref{fig:venn} shows the scope of our work on inter-operation dataflows compared to prior works on acceleration.

%Therefore data orchestration which accounts for such reuse is critical for HPC workloads. Cache replacement policies often have a global view of an individual line rather than a tensor as a whole, which limits the applicability for such algorithms. Moreover, LRU replacement does not see the future reuse of the tensor and replaces it if it has not been used recently. Belady's optimal replacement policy requires knowledge of future accesses for each line and often requires additional structures and meta data accesses to replace one line which is costly to implement~\cite{popt-hpca21}. Scratchpads, on the other hand provide ability to the programmer to have control over data orchestration. However, these scratchpads are often over-provisioned for the worst case applications.

%To this end, we propose \DataflowName data orchestration that analyzes the tensor operand-level reuse patterns in the algorithm and the dataflows of the individual GEMMs to manage the data in the memory hierarchy. \DataflowName also enables efficient prefetching for these HPC algorithms due to the knowledge of the future tensor reuse pattern and dataflows.

 %Some other applications with low arithmetic intensity include Graph Neural Network, Transformers etc., however, we often refer to Conjugate Gradient for demonstration purposes in this paper since its DAG already has the characteristics of Graph and ML workloads but also has additional unique characteristics providing opportunity to demonstrate different kinds of inter-operation reuse.


\noindent  \textbf{\textit{The key contributions of this paper are:}}
\end{comment}

%Our methodology is also applicable to %(and evaluated on) 
%other applications with low intensity GEMMs like GNNs and transformers. %where our methodology is applicable,
%However, we often refer to Conjugate Gradient as the application for demonstration purposes since its tensor dependency graph has the characteristics of low intensity Graph and ML workloads and also has additional unique characteristics providing opportunity to demonstrate different kinds of inter-operation reuse.
%\autoref{fig:venn} shows the scope of our work on inter-operation dataflows compared to prior works on accelerator dataflow.
%Our proposed \textit{inter-operation reuse} is a wider generalization of \textit{traditional inter-operation pipelining} here, since inter-operation pipelining only exploits reuse between consecutive operations.
%\autoref{fig:venn} shows the scope of our work on inter-operation dataflows compared to prior works on acceleration.


































\begin{comment}
\section{Problem and Motivation}
\label{sec:introduction}

Sparse and Dense matrix multiplications are prime operations for a variety of applications spanning Graph Analytics~\cite{kipf2017semisupervised,hamilton2017inductive}, High-Performance Computing~\cite{cools2017communication,cerebras} and Artificial Intelligence~\cite{resnet,nlp}. The GEMM operations used in DNNs offer vast reuse opportunities~\cite{kwon2019understanding,interstellar,timeloop,eyeriss2016isca} owing to their high arithmetic intensity. This has led to a plethora of spatial accelerators for DNN applications~\cite{eyeriss2016isca,kwon2018maeri,tpu-isca,nvdla,shi}. Prior works have also looked at the acceleration for Sparse workloads~\cite{sigma,eie,extensor,eyeriss2} by eliminating redundant matrix multiplications and memory accesses.

However, certain PDE solver algorithms used scientific applications like the Conjugate Gradient~\cite{hestenes1952methods} have SpMM/SpMV and GEMM/GEMV operations with low arithmetic intensity~\cite{cerebras}. The best supercomputers ranked on High-Performance Linpack benchmark are able to achieve only upto 3\% of the performance on the HPCG benchmark~\cite{cerebras,osti_1089988}.
~\autoref{alg:cg_einsum} shows the Block Conjugate Gradient Algorithm. ~\autoref{fig:results}a) and b) show maximum achievable arithmetic intensity of individual operations. Please note that instead of counting multiplications and additions separately, we count MAC as a unit of computation. We also consider matrix addition of, for example, $X^{k-1}$ with $P^{k-1}.\alpha^{k-1}$ in the equation $X^{k}=X^{k-1}+P^{k-1}\alpha^{k-1}$ to be done immediately after the MAC and we absorb the addition in same operation. Therefore, effectively we count number of multiplications per memory access of an element (independent of the bit precision). Please note, here number of RHS is a parameter with values 1,8,16,32 and 64. 

\insertFigure{cg}{Block CG Algorithm.}


~\autoref{fig:results}a) shows the maximum achievable arithmetic intensity for individual SpMM operations for the suitesparse CG matrices. We note that the SpMMs, especially the highly sparse ones like barth4 have extremely low arithmetic intensity even for RHS=64.
~\autoref{fig:results}b) shows the maximum achievable arithmetic intensity of dense GEMMs in an individual operation in terms of number of multiplications per memory access. We notice that the arithmetic intensity of the DenseGEMMs is severely limited by the number of RHS (ie. the number of problems solved simultaneously.) Therefore traditional DNN accelerators~\cite{eyeriss2016isca,kwon2018maeri,tpu-isca} do not help accelerate these workloads due to fundamentally low reuse.

~\autoref{fig:results}c) shows the arithmetic intensity for one iteration of the CG loop. Please note that for matrix inverse, we consider LU factorization followed by Triangular Solve. Also, we conservatively count accesses to $P$ and $P^T$ separately owing to different layout in the memory. We note that the amount of reuse that can be extracted between the operations is high and this motivates us to design a new dataflow for HPCG workloads which tries to maximize inter-operation rather than optimizing for inter-operation reuse.

\end{comment}