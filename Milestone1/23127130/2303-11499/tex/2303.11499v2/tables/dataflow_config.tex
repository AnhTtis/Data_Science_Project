\begin{scriptsize}
\begin{table*}[h]
    
\begin{scriptsize}
\begin{center}

    %\vspace{-3mm}
  \center
  \caption{Different schedules and buffers configurations evaluated with the corresponding SOTA accelerator works.}%\TK{please use the accelerator names in Fig 14 and Fig 15. In fact I would suggest naming the accelerators as A, B, C, in those figures so that the x axis label reduces and this will save significant spacre as we are short on space. And A, B, C would have been defined in this table}}%\TK{We can change this to accelerators evaluated, and include another column at the left with the accelerators. First one will be Flexagon-like, second and third you can give your own names Flex-LRU, Flex-BRRIP say, and then fourth should maybe list name of SOTA pipleined acceleratr and fifth can list our name. I think having the accel names will also help reviewers who feel we didnt compare against SOTA. Now hopefully the SOTA pipelined accel will show up by name}} 
  \label{table:dataflow_config}
    
\begin{tabular}{|p{0.11\linewidth}|p{0.10\linewidth}|p{.15\linewidth}|p{0.53\linewidth}|}
    \hline
    \textbf{Schedule} & \textbf{Buffer-hierarchy} & \textbf{Combined Configuration} & \textbf{Description of the Combination of Schedule and the Buffer-hierarchy}  \\
    \hline
    %Seq & $V\times F$ & $t_{AGG}+t_{CMB}$\\
    %\hline
%    SEQ-Inner & Op-by-op Dataflow with Inner Product\\ \hline
    Best Intra-layer   
 & Explicit  & Flexagon-like (Flexagon)~\cite{flexagon} & Flexagon's flexible architecture can optimally map a single (Op-by-op) operation with any shape and sparsity. This combination achieves  the best possible single operation reuse. All ops begin and end in DRAM. Its the oracle operation-by-operation dataflow.\\
    \hline
{Best Intra-layer} 
 & LRU Cache  & Flexagon with LRU (Flex+LRU) & All accesses go through the LRU cache without any explicit management.\\\hline

{Best Intra-layer}  
 & BRRIP Cache  &  Flexagon with BRRIP (Flex+BRRIP) & All accesses go through the BRRIP cache without any explicit management.\\\hline

    Pipelining  & {Explicit} & FLAT-like (FLAT)~\cite{flat} & We model the FLAT R-gran dataflow for pipelining between two operations when it is possible to apply (instances with delayed downstream consumers are not considered as pipeline just consumes the tensor without writeback). Also note, that this work assumes Parallel Pipeline (PP) throughout, and this baseline captures the dependency of intra-operation dataflows according to FLAT R-Gran, even though, the actual hardware implementation of FLAT uses Sequenatial Pipeline (SP) dataflow~\cite{garg2021understanding}. However, this does not impact the DRAM accesses. \\\hline
  %  GOGETA-df & GOGETA dataflow with inter-op pattern~\autoref{alg:inter-op} \\\hline
  %  & and loop orders~\autoref{alg:looporder}.\\\hline

%\DataflowNamenospace & Explicit RF, Explicit pipeline buffer and LRU for downstream tensors (\textit{LRU-hybrid}) & We use the schedule generated by~\DataflowNamenospace. Here, we use the LRU for large tensors that are reused within multiple operations. We still use register files for small operands and p-sum reduction, pipeline buffer for inter-operation pipelining. LRU is only used for downstream tensors (we just the~\SpadName buffer by cache).\\\hline
    
    %\DataflowNamenospace & \textit{BRRIP-hybrid} & Similar to~\DataflowName with \textit{LRU-hybrid}, except we use the BRRIP policy instead of the LRU.
   % \\\hline

    \DataflowNamenospace & \SpadNamenospace & \AccelNamenospace\ &(\textbf{This work}) We use~\SpadName buffer for reuse of large tensors that have downstream consumers along with the explicit pipeline buffers,  and  RFs.  It uses both~\PolicyA and~\PolicyB policies.
    \\\hline
    \hline

    Best Intra-layer & \PolicyAnospace-only & \PolicyAnospace-only & \textbf{(Additional study in \autoref{sec:sens})} We turn off all other optimizations, and model the effect of an SRAM with \PolicyA as the only policy.\\\hline

     Pipelining + Delayed Hold & Explicit & SET-like (SET)~\cite{isca-pip} & \textbf{(Additional study in \autoref{sec:sens})} We add another baseline SET which supports the delayed hold, for the ResNet evaluation since that is the only place the dependency shows up. \\\hline
%    GOGETA+org & GOGETA-map with optimal tensor allocation ~\autoref{sec:tornado}\\\hline
    
    %Total L2 capacity & 32MB\\ \hline
    %Total L3 capacity & 256MB\\ \hline
    
  \end{tabular}\end{center}

  %*{While Flexagon proposes an accelerator for SpMSpM, we model Flexagonâ€™s flexible NoC and microarchitecture which can run any single operation dataflow and model SpMM and \GEMM on it.}

%\vspace{-3mm}





 % \vspace{1mm}
%\vspace{-4mm}
\end{scriptsize}

\end{table*}

\end{scriptsize}