%\vspace{-3mm}
\section{Introduction}
\label{sec:introduction}


\begin{comment}
    Points to be made in the intro
        - High value GEMM workloads have complex dependencies []
        - Traditional pipelining does not respond well to these dependencies
        -Keeping operands on-chip is key to get a good performance.
                - Caches - Implicit and Workload agnostic + hardware overheads
                - Spads - Explicitly managed (large map-space and expensive search and buffer allocation)
                - CHORD: A hybrid implicit and explicit management, which is cycle-level implicit and coarse-grained explicit, so the cost lowers enough to manage the SRAM online..
        - SCORE: Fast scheduler for low intensity ops that deals with more complex dependencies, and also provides metadata to CHORD..
        - Skewed GEMMs can have surprisingly low arithmetic intensity??
        
    


    
\end{comment}


%One of the primary mechanisms for efficiency in Deep Learning accelerators comes from their ability to extract data reuse. 
%On the algorithm end, reuse opportunities come from the \emph{arithmetic intensity (AI)}, i.e., number of operations performed on each byte of data fetched from memory. The most common operation in Deep Neural Networks (DNNs) is matrix multiplication, which inherently offers reuse opportunities since a weight value gets multiplied by several inputs, and vice versa.
%On the hardware end, 
%these reuse opportunities are leveraged by buffering the operands to be reused within the accelerator's scratchpads which are programmed in an \textbf{\textit{explicit}} manner. 


%Since the amount of on-chip scratchpad is finite, and cannot hold the entire model and all activations, the weights and activations are tiled, and staged through the on-chip memory hierarchy. This mechanism is called mapping (\textit{aka} scheduling). The mapping determines the order of tiles to fetch from memory, which tiles to keep \textit{stationary} and for how long. The number of legal mappings for any individual layer can be in the order of $10^{15}$~\cite{timeloop,gamma}, and different mappings trade-off the amount of data reuse they can extract and thereby performance and energy-efficiency. To this end, there has been a lot of work in searching for and determining optimal schedules for each layer of a DNN~\cite{timeloop,chatarasi2020marvel,gamma}. For many DNN models, there is ample reuse within each layer. This has allowed DNN accelerators to successfully schedule each layer independently, one after the other, and achieve high utilization via an efficient mapping strategy~\cite{timeloop,gamma}. In fact, prior works have shown that several mapping strategies (including heuristics) can achieve high-utilization given the high AI within each layer~\cite{flexion}. 






% For many of the early DNN models, such as convolutional neural networks, there was ample reuse within each layer.
% This allowed DNN accelerators to successfully schedule each layer independently and achieve high utilization via an efficient mapping strategy. Moreover, several mapping strategies (including heuristics) were good enough given the high AI within each layer~\cite{flexion}. Unfortunately, emerging DNNs such as Transformers and generalized tensor applications (used in high-performance computing) exhibit much lower AI. 

%For many DNN models, there is ample reuse within each layer. This has allowed DNN accelerators to successfully schedule each layer independently, one after the other, and achieve high utilization via an efficient mapping strategy~\cite{timeloop,gamma}. In fact, prior works have shown that several mapping strategies (including heuristics) can achieve high-utilization given the high AI within each layer~\cite{flexion}. 



%\TK{The intro flow can be improved .. I recall our earlier  flow was to introducing ML accelerators and reuse. And then opportunity to run general tensor algebra workloads on ML accelerators - but how the inherent mechanisms for data reuse extraction Fail. And this is coz of two reasons. Skewed shapes and complex DAG. Skewed shaoes addressed by oast work. Complex DAG focus of this work ..  I know you want to highlight complex DAG first and hence got that into first para .. but if you want to do that then you still need a way to introduce the concept of reuse - right now it kinda jumps out of nowehere .. and then we have a tangent towars skewed GEMMs. One suggestion the following: para 1 introduce accelerators and concept of reuse, para 2 mention reuse can be extracted today either via intra-op loop transformations (aka intra-op dataflow) or fusion/pipeling. para 3 current technique  insufficient for workloads like CG with complex DAGs -- it exploded the space etc etc - that is current para 3 + 4.. and then current para 5.  }

Recently, works like the Tensor Algebra COmplier (TACO) \cite{XXXTaco} have spurred research interest in generalized tensor applications (used in high-performance computing (HPC)), and their acceleration using Deep Neural Network (DNN) Accelerators~\cite{extensor,asgari2020alrescha,fdmax,cerebras,plasticine,eyeriss2016isca,kwon2018maeri,tpu-isca}. However, this generalization presents new challenges. Tensor-algebra applications often have much more complex dependency graphs between operations.
For example, \autoref{fig:dfg} shows the complex cascade (aka DAG) of tensor operations found in the conjugate gradient (CG) over two iterations of CG loop, which is a high value HPC solver rich in these complexities and a key workload of interest in this work.
Many such HPC applications have memory-bounded \textit{skewed GEMMs}.

Tensor operations (e.g., GEMMs) used within DNNs offered good \emph{reuse} opportunities because of their relatively cubic aspect ratios and large dimensions. Earlier DNN accelerators exploited this \emph{reuse} by searching the optimal schedule for each tensor operation independently to achieve maximum reuse for the networks they targeted at design time. 
Simultaneously, these schedules could be efficiently implemented using \emph{scratchpad} buffers that were explicitly controlled by the schedule to stage the intermediate \emph{tiles} of the data according to the traversal order within each GEMM\footnote{For explicitly managed buffers (e.g. scratchpads), buffer allocation aka tiling is an additional knob in scheduling search space.}.
%Traditionally, in spatial accelerators, the reuse has been exploited through \textit{explicit} on-chip scratchpads that are explicitly exposed to the programmer. This has led to better on-chip data reuse and lower on-chip energy consumption, specifically for DNN applications, which however, have simpler DAGs.
There are two forms of data reuse in such a DAG that any scheduler could leverage: (i) within individual tensor operators and (ii) over the edges of the DAG. Earlier DNN accelerators exploited (i) by optimizing the schedule for tensor independently

Unfortunately, reuse within individual tensor operators is fairly limited in many HPC tensor operators, including CG, due to highly skewed shapes of the operators (\autoref{sec:apps}). This is due to an underappreciated yet fundamental property, namely, \textit{not all dense GEMMs with a large number of multiplications are compute bound even in the best case}. SkewedGEMMs are memory bound and leave datapath resources idle. %\autoref{fig:class} shows that the only GEMMs in CG are skewed.

\insertFigureScaled{dfg}{Tensor dependency graph of intermediates in Conjigate Gradient across first two iterations of the CG loop.}{0.9}


Therefore, this work focuses on extracting reuse over the DAG edges (aka inter-operation reuse). 
Some recent works have demonstrated that \textit{pipeline reuse} between adjacent operators~\cite{tileflow,isca-pip,yan2020hygcn,garg2021understanding} can be leveraged by using on-chip \emph{scratchpads} to additionally stage the intermediate tensor, thereby reducing traffic to main memory.
Unfortunately, it is challenging to generalize inter-operation pipelining to complex DAGs since the transitive edges in complex DAGs introduce additional data dependencies, multicasts, diverse reuse distances across operators, and possible data layout transformations (\textit{aka} swizzle) depending on the consumer. 
Thus, extending this to arbitrary DAGs remains an open problem to the best of our knowledge. 
Scheduling linear DNN DAGs on the accelerator scratchpads is already known to be a highly challenging compiler problem due to the large scheduling space and layout choices~\cite{dnnfusion,TelaMalloc}.
%, with most DNN compilers today only performing operator-by-operator scheduling. 
%Dealing with memory-boundedness due to skewed GEMMs is currently an open-challenge for tensor algebra accelerators. 
At the DAG level, these compilers 
only perform fusion of tensor operators with adjacent element-wise operations~\cite{dnnfusion}, or perform tensor-tensor fusion (e.g., FlashAttention~\cite{FlashAttention}, FLAT~\cite{flat} and OMEGA~\cite{garg2021understanding}). 

The aforementioned DAG complexity explodes the scheduling space for finding good scratchpad configurations further, as the total combinations and proportions of allocations burgeon with operation DAG depth and the number of tensors involved. For example, for a 7-operator DAG in ~\autoref{fig:dfg} (one iteration), the number of buffer allocation choices alone goes from 7$\times 10^{15}$ in a baseline scenario with reuse only in individual tensor operations to $10^{80}$ when additionally considering reuse along edges of the DAG (\autoref{sec:arguments}). To make matters worse, scientific applications like CG have different tensor shapes for each problem, unlike DNNs, which means that costly buffer allocation needs to be done for every problem.
Hence, state-of-the-art accelerators that use explicit data orchestration cannot reuse along delayed downstream dependencies.


%\insertFigureScaled{ai1}{Degradation of arithmetic intensity (AI) on GEMMs with the same number of multiplications due to aspect ratio skew. The GEMM on the left would be compute-bounded while the one on the right would get memory-bounded on a TPUv4 (which needs an AI of 250 to saturate its datapath~\cite{tpuv4}).\vspace{-2mm}}{.75}

%Furthermore, ignoring the DAG complexity and scheduling the operators sequentially (similar to DNNs) can get prohibitve. 
%This is due to an underappreciated yet fundamental property, namely, \textbf{\textit{not all dense GEMMs with large number of multiplications are compute-bound even in the best case}}. As shown in \autoref{fig:ai1}, a \emph{skewed} aspect ratio inherently decreases AI, thus making the individual GEMM memory-bound and leaving datapath resources idle. For example, to saturate its datapaths the TPU v3 and v4 architectures require an AI of approximately 150 and 250 respectively~\cite{tpuv4}. 

\begin{comment}

Recently, works like the Tensor Algebra COmplier (TACO) \cite{XXXTaco} have spurred research interest in generalized tensor applications (used in high-performance computing (HPC)), and their acceleration using DNN Accelerators~\cite{extensor,asgari2020alrescha,fdmax,cerebras,plasticine}.
%\TK{if we can cite works on generalized tensore acceleration here, thatll be nice..  eg extensor, maybe the HPC accelerator from ISCA last year - . fwiw we should cite alrescha too somewhere?}
However, this generalization brings new challenges: tensor-algebra applications have diverse operator shapes and dependency graphs.
Moreover, many of these 
applications 
operators 
exhibit much lower AI than DNNs.
This is due to an underappreciated yet fundamental property, namely, \textbf{\textit{not all dense GEMMs with large number of multiplications are compute-bound even in the best case}}. As shown in \autoref{fig:ai1}, a \emph{skewed} aspect ratio inherently decreases AI, thus making the individual GEMM memory-bound and leaving datapath resources idle. For example, to saturate its datapaths the TPU v3 and v4 architectures require an AI of approximately 150 and 250 respectively~\cite{tpuv4}. Dealing with memory-boundedness due to skewed GEMMs is currently an open-challenge for tensor algebra accelerators. 


Fortunately, one way of mitigating memory-boundedness is to identify additional reuse opportunities by \textit{fusing} 
%the entire workload DAG and not just individual operators. 
multiple operators and scheduling them together.
For many tensor workloads, specially DNNs, the output produced by a layer is consumed by its adjacent layer, offering an opportunity to leverage \textbf{pipeline reuse}.
%, (\textit{aka} layer fusion).
%that open up if we consider the entire workload DAG and not just individual operators.
%~\autoref{fig:dfg} shows the complex cascade of tensor operations found in the DAG for Conjugate Gradient, a high-value HPC workload rich in these complexities, across two iterations. 
%For example, the output produced by a layer and consumed by its adjacent layer offers another dimension of reuse, called \emph{pipeline reuse}.
Many recent works have demonstrated that pipeline reuse between adjacent operations~\cite{tileflow,isca-pip,yan2020hygcn} can be leveraged by staging the intermediate tensor through on-chip scratchpads, thereby reducing  traffic to main memory.




Unfortunately, as we identify in this work (\autoref{sec:nofusion}), simple pipelining is challenging to implement in complex tensor algebra DAGs that include 
%additional potential sources of reuse such as 
delayed downstream consumers and multiple consumers with varying reuse distances or traversal orders. 
For instance, \autoref{fig:dfg} shows the complex cascade of tensor operations found in the DAG for Conjugate Gradient (CG), a high-value HPC workload rich in these complexities, across two iterations, and a key workload of interest in this work.
The transitive edges in this DAG introduce additional data dependencies, multiple reuse instances with diverse reuse distances, and possible data layout transformations (\textit{aka} swizzle) depending on the consumer. 
Unfortunately, scheduling even linear DNN DAGs on to accelerator scratchpads is already known to be a highly challenging compiler problem~\cite{dnnfusion,TelaMalloc}, with most compilers today only supporting fusion of tensors with element-wise operation, or performing very specific hand-optimized adjacent tensor-operation fusion (e.g., FlashAttention~\cite{FlashAttention} and FLAT~\cite{flat}). 
The aforementioned complexity explodes the mapping space for finding good scratchpad configurations further, as the total combinations and proportions of allocations burgeon with operation DAG depth and the number of tensors involved\footnote{For example, for a 7-operator DAG in ~\autoref{fig:dfg} (one iteration), the size of the scratchpad allocation search space goes from 7$\times 10^{15}$ in a baseline op-by-op scenario to $10^{80}$ when considering entire DAG-level %scratchpad allocation 
reuse (\autoref{sec:arguments})}.%\TK{shouldnt we change this to when considering ``DAG-level scratchpad allocation"? 
%Coz even 7$\times 10^{15}$ is considering scratchpad alloc, just that its op by op}
%, as we discuss in~\autoref{sec:arguments}.}.
For this reason, state-of-the-art accelerators using explicit data orchestration cannot reuse along delayed downstream dependencies.
\end{comment}


%\insertFigure{ai2}{Data reuse in chain of skewed GEMMs and arithmetic intensity with and without inter-operation reuse. We also represent the operations visually.}


To this end, this work proposes an accelerator~\AccelNamenospace\footnote{\AccelNameexp}. \AccelName makes the scheduling problem tractable by introducing a novel explicit-implicit on-chip buffering mechanism called \SpadNamenospace\footnote{\SpadNameexp}, which we classify as a hybrid between explicitly-programmed scratchpads and implicitly managed caches, leveraging the strengths of both.
Specifically, \SpadName leverages high-level information (such as reuse distance and frequency, start and end addresses, indices) about the tensor operands from a software scheduler, but manages tensor placement and replacement via implicit (i.e., hardware-managed) policies. %\SpadName policies are implicit at cycle-level and leverage only high-level tensor DAG information from software, thus avoiding the costly search for optimal buffer allocation of explicitly managed scratchpads. \SpadName policies operate at a tensor granularity as opposed to line-level policies used by caches. As a result they avoid significant area overhead and myopic view of cacheline level policies. 
\AccelName uses our proposed scheduler, called \DataflowNamenospace\footnote{\DataflowNameexp}, which ``identifies" diverse reuse opportunities of the various operands and edges in an arbitrary DAG, which can then be leveraged by \SpadNamenospace. 
\DataflowName and \SpadName together enable~\AccelName to manage coarse-grained buffer management decisions at the tensor granularity (not cacheline) while enabling the hardware to manage track and leverage data reuse (removing this complexity from the compiler). 


%\TK{I suggest breaking this such that 1-2 lines are on score, followed by on chord. for score after you say its more comprehensive, maybe mention how it acheives that.. similar to how you've done it for chord}
\DataflowName is more comprehensive than prior work in scheduling along edges of an arbitrary DAG as it also identifies reuse opportunities for all kinds of delayed downstream consumers, and minimizes the need to transform data layout (aka swizzle). The buffer allocation aspect of scheduling for complex dependencies is done by a hybrid implicit/explicit manner by~\SpadNamenospace, as opposed to buffer mechanisms in accelerators that do explicit buffer allocation~\cite{buffets}. \SpadName makes the problem tractable by reducing the complexity of the buffer allocation step from $10^{80}$ to $10^2$ (\autoref{sec:arguments}), since it only leverages high level DAG connectivity information and makes cycle-level decisions implicitly. On the other hand, \SpadName also significantly reduces the metadata area overhead of cache as it needs one entry per tensor, and replacement policies consider the tensor as a whole as opposed to individual lines leading to better policies than caches.
\AccelName achieves 4x geomean speedup and 4x improvement in energy efficiency across a broad range of scientific workloads %(\autoref{sec:eval}) 
over the state-of-the-art.

%Co-designing the scheduling algorithm (\DataflowNamenospace) and buffer storage idiom (\SpadNamenospace) enables us to effectively exploit all available sources of reuse within the DAG.  By placing coarse-grained buffer allocation decisions under the explicit control of the scheduler for efficiency, while making low-level fine-grained decisions implicitly, we can significantly reduce the size of the schedule space.
%This makes it tractable to find schedules with sufficient level of inter-operation reuse to reach the compute bound regime, even in the face of complex DAGs of dependencies.
%All in all, we show that \SpadName and \DataflowName are applicable to diverse tensor applications and achieve 2.51x geomean speedup and 4x improvement in energy efficiency across a broad range of workloads (\autoref{sec:eval}).

%To leverage additional reuse than prior work, \textbf{\textit{the first contribution of this work is 
%~\DataflowNamenospace,}}\footnote{\DataflowNameexp} \textbf{\textit{a downstream dependency aware DAG-level scheduler (\autoref{sec:score}).}}

%operand-aware implicit (i.e., hardware-managed) policies to guide the cycle-level placement of data elements in the buffer.
%We classify \SpadNamenospace to be a hybrid between completely explicitly-programmed scratchpads and hardware-managed caches, leveraging the best of both.


%Scratchpads are pervasive in accelerators due to which suffer from the data placement and scheduling problem described above)
%(which add area overheads and complexity that is not needed for domain-specific accelerators where the workload is statically known).








%~\autoref{sec:arguments} also discusses why running statically known DAGs does not necessarily imply that scratchpads are not burdensome and don't have compile-time overheads.

%This makes static buffer allocation for \textit{explicit} scratchpads unacceptably slow, and also that heuristic solutions are more likely to miss the true optimal. Minimizing the architectural exposure with \textit{implicit} buffering mechanism is desirable, but caches have high area and energy overheads, and also propose line-level policies agnostic to the the operand structure.


%For this reason, state-of-the-art accelerators supporting pipeline reuse would deem such scenarios as non-pipelinable, leaving performance on the table.

%Moreover, current pipelining implementations do not take the data dependency caused by delayed downstream consumers into account, as we discuss in~\autoref{sec:nofusion}. Moreover, these downstream consumers have multiple reuse distances and instances, and making scheduling decisions to avoid layout transformations~\cite{feather_isca2024} (\textit{aka} swizzle) is important.
%as~\autoref{fig:no-fusion} shows. 

%To leverage additional reuse than prior work, \textbf{\textit{the first contribution of this work is 
%~\DataflowNamenospace,}}\footnote{\DataflowNameexp} \textbf{\textit{a downstream dependency aware DAG-level scheduler (\autoref{sec:score}).}}

% DAG-level reuse options unfortunately explode the mapping space for finding good scratchpad configurations, as the total combinations and proportions of allocations explode with operation DAG depth and the number of tensors involved. For example, for a 7-operator DAG in ~\autoref{fig:dfg} (one iteration), the size of the scratchpad allocation search space goes from 7$\times 10^{15}$ in a baseline op-by-op scenario to $10^{80}$ when considering entire DAG-level scratchpad allocation reuse%\TK{shouldnt we change this to when considering ``DAG-level scratchpad allocation"? 
% %Coz even 7$\times 10^{15}$ is considering scratchpad alloc, just that its op by op}
% , as we show in~\autoref{sec:arguments}. ~\autoref{sec:arguments} also discusses why running statically known DAGs does not necessarily imply that scratchpads are not burdensome and don't have compile-time overheads.
% This makes static buffer allocation for \textit{explicit} scratchpads unacceptably slow, and also that heuristic solutions are more likely to miss the true optimal. Minimizing the architectural exposure with \textit{implicit} buffering mechanism is desirable, but caches have high area and energy overheads, and also propose line-level policies agnostic to the the operand structure.
% To address this, \textbf{\textit{the second contribution of this work is \SpadNamenospace}}\footnote{\SpadNameexp},  \textbf{\textit{a novel hybrid implicit+explicit buffering idiom that aims to combine the ``best of both worlds'', by using high-level information about a tensor operand from the scheduler, but using operand-aware implicit policies to guide the cycle-level placement of data elements in the buffer. (\autoref{sec:chord})}}.
%\TK{the prev lines is identical as the one in next para .. can we be a little precise on what exactly chord does in prev line}
%a buffer structure that obtains DAG-level information from \DataflowNamenospace (similar to explictly software-programmed scratchpads) 
%but places elements of tensor within the buffer with hardware-managed replacement policies (analogous to implictly-managed caches).
%\TK{@Raveesh can you rewrite above line which i kinda borrwed from your earlier text? Its not clear what we mean by getting DAG-level information and waht it means to place elements of tensor implicitly .. }
%\footnote{We use "buffer" as an overarching term for on-chip SRAMs like cache, scratchpad, FIFOs etc.}. 
%~\SpadName is workload-aware, as it uses high-level metadata like starting and ending global address of a tensor, tensor-level reuse frequency and distance from the workload (the explicit component), but implicitly controls placement of elements of tensors without being burdensome to the programmer. ~%\TK{it might be worth adding a footnote saying you use scratchpad and buffer interchangeably in this paper} 
%\TK{based on what the footnote says its unclear whether chord is a scratchpad or cache }


% Co-designing the scheduling algorithm (\DataflowNamenospace) and buffer storage idiom (\SpadNamenospace) enables us to effectively exploit all available sources of reuse within the DAG.  By placing coarse-grained buffer allocation decisions under the explicit control of the scheduler for efficiency, while making low-level fine-grained decisions implicitly, we can significantly reduce the size of the schedule space.
% This makes it tractable to find schedules with sufficient level of inter-operation reuse to reach the compute bound regime, even in the face of complex DAGs of dependencies.
% All in all, we show that \SpadName and \DataflowName are applicable to diverse tensor applications and achieve 2.51x geomean speedup and 4x improvement in energy efficiency across a broad range of workloads (\autoref{sec:eval}).


\begin{comment}
As Deep Learning (DL) emerged as a high-value workload, the computer architecture community responded by proposing custom accelerators for DNNs~\cite{eyeriss2016isca,kwon2018maeri,tpu-isca,nvdla}. 
The most common fundamental operation for these DNNs was matrix multiplication, often expanded from convolutional layers that were common in DNNs at that time. These GEMMs offered good reuse opportunities because of their large dimensions and relatively cubic aspect ratios, allowing early DNN accelerators to successfully schedule each layer independently and achieve maximum utilization for the DNNs they targeted. 
This also led to work in developing efficient mappers to identify optimal schedules for each layer~\cite{timeloop,chatarasi2020marvel,gamma} to push each GEMM towards its natural compute-bound regime.
%at design-time. 
Diverse schedules could be efficiently implemented using \emph{scratchpad} buffers that were explicitly controlled by the compiler to stage intermediate \emph{tiles} of data according to the traversal order within each GEMM.
%, without concern for inter-layer efficiency.

%Furthermore, prior works have also looked at the accelerators for sparse tensor workloads~\cite{sigma,eie,extensor,eyeriss2} which eliminate irrelevant multiplications and memory accesses.}% Unfortunately, this approach lowers intensity even further, and straightforwardly applying these to CG on a GEMM-by-GEMM operation basis results in compute under-utilization due to limited memory bandwidth.


Recent works have shown that simple pipelining of adjacent operations~\cite{tileflow,isca-pip,yan2020hygcn} can reduce memory traffic (by staging the intermediate tensor through on-chip scratchpads).
Unfortunately, these solutions exclude additional sources of reuse such as  delayed downstream consumers, and multiple consumers with varying reuse distances or traversal orders as \autoref{fig:no-fusion} shows.


It is also challenging to simply apply traditional inter-operation pipelining in cases of these dependencies, because of the delayed downstream dependencies, varying shapes of skewed GEMMs and multiple of these downstream consumers as~\autoref{fig:no-fusion} shows. %\TK{Fig 4 getting referenced before Fig 3, i suggest bring it before Fig 4 then} 
~\autoref{fig:dfg} shows an actual complex cascade of tensor operations found in CG, a high-value HPC workload rich in these complexities.
Thus, we need mechanisms to cache these intermediate tensors within SRAMs, since there are situations where simple pipelining cannot be applied.


To make matters worse, these extra options explode the scheduling space for finding good scratchpad configurations, as the total combinations and proportions of allocations explodes with operation DAG depth and the number of tensors involved. This means that exhaustive schedule-space exploration techniques become unacceptably slow, and also that heuristic solutions are more likely to miss the true optimal.





\insertFigureScaled{ai1}{Degradation of arithmetic intensity on two GEMMs with the same number of multiplications due to aspect ratio skew.\vspace{-3mm}}{.75}



%\insertFigure{no-fusion}{Pipelining cannot simply be applied to complex DAGs due to - 1) Delayed downstream dependency, 2) varying shapes, 3) consumers at multiple reuse distances 4) need to preserve layout across conusmers.\vspace{-3mm}}


%The reason for this is low arithmetic intensity for a skewed GEMM and it can be as low below 1 op/byte. Currently, most scheduling strategies optimize matrix multiplications independently and compose them to execute in an op-by-op manner, which leads to low performance in applications with skewed GEMMs.}
\end{comment}

\begin{comment}
A real-world quantification of this phenomenon is captured by HPCG benchmark~\cite{dongarra2015hpcg,hpcg2021}, which runs Conjugate Gradient (CG)---a widely used HPC solver application represented as a DAG of tensor operations.

\input{tables/hpcg}

%\footnote{HPCG is a benchmark for supercomputers that runs CG. \MP{Footnote 1 adds no real information. Cut. }}
As \autoref{tables:hpcg} shows, CG achieves only 1-3\% of peak performance on top 7 supercomputers. 
Therefore, intra-operation reuse alone is not sufficient.
The overall throughput can be increased by improving the arithmetic intensity (i.e., on-chip data reuse), which can be done by seeking inter-operation reuse.
\end{comment}


\begin{comment}
Thus to achieve full utilization we must consider inter-GEMM operation scheduling. Prior works have also shown that simple pipelining of adjacent operation~\cite{tileflow,isca-pip,yan2020hygcn} can improve things, but these solutions exclude significant potential sources of reuse, including delayed downstream consumers, and multiple consumers with varying reuse distances or traversal orders as~\autoref{fig:no-fusion} shows. To make matters worse, these extra options explode the scheduling space for finding good scratchpad configurations, as the total combinations and proportions of allocations explodes with operation DAG depth and the number of tensors involved. This means that exhaustive schedule-space exploration techniques become unacceptably slow, and also that heuristic solutions are more likely to miss the true optimal.

%It is also challenging to simply apply traditional inter-operation pipelining in cases of these dependencies, because of the delayed downstream dependencies, varying shapes of skewed GEMMs and multiple of these downstream consumers as~\autoref{fig:no-fusion} shows. %\TK{Fig 4 getting referenced before Fig 3, i suggest bring it before Fig 4 then} 
%~\autoref{fig:dfg} shows an actual complex cascade of tensor operations found in CG, a high-value HPC workload rich in these complexities.
%Thus, we need mechanisms to cache these intermediate tensors within SRAMs, since there are situations where simple pipelining cannot be applied.

%Prior works like buffets~\cite{buffets} make use of \emph{explicit decoupled data orchectration} to supplement scratchpad RAM with simple pointer and credit management scoreboarding. This is called explicit because data placement and RAM replacement are schedule-controlled, and decoupled because it operates on bulk-synchronous fills. %These work well for executing one layer at a time, when all dimensions of matrices have sufficient reuse.
%While explicit orchestration works well for scheduling single matrix multiplication at a time, considering data placement, for inter-operation reuse statically in general tensor algebra is a diabolically hard problem. 
% In order to reuse such operands, specially where intra-operation reuse is simply not enough, storing these operands in the on-chip buffer is essential, and multiple of these operands contend for space inside the buffer. We show in~\autoref{sec:arguments} that the design-space to accommodate multiple tensors inside a buffer space explodes in complexity. ~\autoref{sec:arguments} also discusses why running statically known DAGs does not necessarily imply that scratchpads are not burdensome and don't have design-time overheads.
%Overall, the co-dependence on schedule, available scratchpad capacity and different downstream consumers make the explicit scheduling problem too challenging.

\insertFigure{no-fusion}{Pipelining cannot simply be applied to complex DAGs due to - 1) Delayed downstream dependency, 2) varying shapes, 3) consumers at multiple reuse distances 4) need to preserve layout across conusmers. We discuss this in detail in~\autoref{sec:nofusion}\vspace{-3mm}}

Of course, scratchpads can be supplemented with pointer and credit management logic to become queues or buffets~\cite{buffets}, but these do not solve the schedule explosion problem as their staging decisions are still \emph{explicitly} controlled. Alternatively, caches are widespread on-chip storage structures that use \emph{implicit} data orchestration. Ideally, the presence of the cache is not architecturally exposed and the hardware itself does the best job possible of reusing the data. (In practice, best optimization is often made by cache-aware scheduling policies, blurring the line between implicit and explicit.) However, the area and energy overhead for tag matching make it less appealing for custom accelerators, as well as the possibility of increased misses due to conflicts. Overheads aside, cache policies typically operate at line granularity of unified address streams, rather than having higher-level knowledge of tensors, blocks, or intended reuse distance. This results in cache policies often rejecting the data that may have high reuse frequency (by virtue of reuse of the whole block) but might not be the immediate vicinity (i.e., high reuse distance).

In order to effectively exploit all available sources of reuse, we propose a unique approach: co-designing the buffer storage idiom with the scheduling algorithm. Our goal is that it becomes tractable to find schedules that obtain a sufficient level of inter-operation reuse to reach the compute bound, even in the face of complex DAGs of dependencies. To achieve this, we propose ~\SpadNamenospace\footnote{\SpadNameexp}, a novel \emph{explicit+implicit hybrid} buffering scheme that aims to combine the ``best of both worlds'' by placing coarse-grained decisions under the explicit control of the scheduler for efficiency, while making low-level fine-grained decisions implicitly like a cache, thus significantly reducing the size of the schedule space.
%~\SpadName is workload-aware, as it uses high-level metadata like starting and ending global address of a tensor, tensor-level reuse frequency and distance from the workload (the explicit component), but implicitly controls placement of elements of tensors without being burdensome to the programmer. ~%\TK{it might be worth adding a footnote saying you use scratchpad and buffer interchangeably in this paper} 
%\TK{based on what the footnote says its unclear whether chord is a scratchpad or cache }
Notably, this approach also significantly reduces the area and energy overheads of traditional line-level caches. %Specifically,~\SpadName uses per-tensor replacement policies that can be configured by the schedule at coarse granularity.
%for operands with downstream consumers that uses implicit replacement at a tensor granularity rather than a line granularity. 
%In this work we propose two such policies- \\(1) \PolicyA - \SpadName is filled in the queue order and once the buffer is full, the spilling data is sent straight to the DRAM.\\
%(2) \PolicyBnospace\footnote{\PolicyBexp} - If the current tensor has a higher reuse frequency and lower reuse distance (in case of same frequency) than the previously written tensor, the current tensor starts replacing the previous tensor by the tail.
%Because of its extremely coarsened granularity, ~\SpadName retains the benefit of a scratchpad over a cache (i.e., with minimal (<1\% of that of cache) tag matching overhead) 
%Hence it gets rid of caches' tag matching overhead and 
%while
%our proposed implicit tensor replacement policies~\PolicyA and \PolicyB remove the challenge of statically coming up with the data placement and replacement strategies statically for operands in multiple operations.

To schedule accelerators that use this structure, we propose ~\DataflowNamenospace\footnote{\DataflowNameexp} a downstream dependency aware novel scheduling strategy. 
\DataflowName identifies the delayed downstream dependencies that require writeback from those where pipelining would work, and steers the tensors with delayed writeback dependencies to~\SpadNamenospace. In order to exploit the reuse on delayed downstream consumers, it is important to make sure that the order in which the elements are produced as same as the order in which they are consumed, otherwise, layout transformation is required. Unlike prior mappers which search for tile-sizes for fine-grained buffer allocation, \DataflowNamenospace's involvement in buffer allocation is coarse-grained at an operand granularity rather than element-wise granularity, and low-level fine-grained decisions are made implicitly by~\SpadNamenospace's policies. 

All in all, we show that \SpadName and \DataflowName are applicable to diverse tensor applications and achieve 2.51x geomean speedup and 4x improvement in energy efficiency across a broad range of workloads (\autoref{sec:eval}).
\end{comment}

%\TK{minimizing layout transformation is an important feature. Lets highlight it more explcitly. As in mention that the dataflow of the current op and downstream op may be different requiring expensive layout transformation, and SCORE tries to minimize that.}%It consists of the following steps - \\ 
%\TK{there seems to be some missing text here?}
%(1) Marking all the edges in the DAG of tensor operations with pipelining opportunities. This is useful in situations where tensors have delayed downstream consumers which can be visualized as a long edge in the DAG.\\
%(2) Assigning loop orders and tiling strategies to ensure that pipelining is actually exploited and layout transformation overhead of a tensor is minimized.\\
%\DataflowName reduces the contending tensors and also ensures minimum layout transformation (aka swizzling) which is also the motivation behind~\PolicyA policy.
%The main reason is the low arithmetic intensity achieved by the execution of Conjugate Gradient on CPUs/GPUs. 
%The main reason for this that the tensor multiplications---which we term \emph{operation} for this paper---%\MP{This definition is also too important to be in a footnote
 %used in CG have extremely unbalanced aspect ratios, which significantly lowers arithmetic intensity and data reuse.} %\MP{I feel like this point should come earlier, perhaps before the previous paragraph.}
%We use the term \emph{skewed} GEMMs to refer to such SpMMs/GEMMs---though notably, in the limit they can devolve to matrix-vector multiplication, i.e. if the shape of the matrix is 100,000:8.}%The main reason for this is costly data movement and communication between compute clusters/pods\MP{This explanation doesn't make sense either. Needs a reuse/intensity component. Even if this made sense, does GOGETA actually fix this problem?}.




%\TK{@Raveesh - I think we can move this para to para 3. Basically start with current para 2 on GEMMs, reuse and spatial accelerators. Then introduce CG. So intersperse this para with current para 3}


%making inter-operation reuse essential.

%This work explores inter-operation reuse opportunities for kernels like CG to enhance its arithmetic intensity.

%However, the~\GEMM in CG have orders of magnitude lower arithmetic intensity compared to SpMMs/GEMMs in DNNs and offer less reuse opportunities within one operation as~\autoref{fig:ai}~(\autoref{sec:ai}) discusses later in the paper. 
%Thus, executing CG at the granularity of a GEMM, reading tensors from DRAM and writing the output of each GEMM to DRAM, makes it highly memory bound leading to low compute utilization as~\autoref{\:hpcg} shows. 
\begin{comment}
Fusing \textit{adjacent} low AI matrix multiplication operators has recently been leveraged for applications like Graph Convolutional Networks (GCN)~\cite{garg2021understanding,yan2020hygcn,liang2020engn} and Transformers~\cite{flat,flashattention} 
wherein tiles of the intermediate tensor are manifested and consumed within the 
on-chip memory hierarchy in a \textit{pipelined} manner, reducing  intermediate output data movement to and from DRAM. We call this \emph{adjacent-op pipelining} in this work.
Other recent works have looked at enumerating the design-space for such adjacent-op pipelining~\cite{garg2021understanding} and 
%\TK{is this what the isca paper does?}\RG{Yep}
identifying optimal pipelined dataflow choices~\cite{isca-pip}, thereby enhancing better compute utilization when running memory-bound tensors.

While traditional adjacent-op pipelining is promising for DAGs with linear chains of operators (i.e., most DNNs today and GCNs), we show in this work that it is insufficient to capture nuances in more complex DAGs, such as those used in HPC kernels like CG.
Specifically, delayed downstream consumption of fanout of tensors, implies that traditional pipelining which overwrites the previously consumed tile cannot be applied directly due to a later dependency.%~\autoref{fig:no-fusion} shows an example, where tensor S has a delayed consumer.
 Moreover nuances such as data layouts of one tensor consumed in various operations is not captured by adjacent pipelining.
%that prior works cannot capture, as we discuss later. \autoref{table:related} enumerates this.

In this work, we coin the term \emph{generalized inter-operation reuse} to widen the scope of inter-operation reuse beyond adjacent operators to include additional reuse opportunities (\autoref{table:related}).
%as a wider generalization of traditional \emph{adjacent-operator pipelining}, introducing three additional reuse opportunities (\autoref{table:related}). 
We also propose \DataflowName (\TitleExpansion), which is a systematic strategy for mapping DAG of tensor operations exploiting the generalized inter-operation reuse opportunities, with the goal of enhancing the AI of the overall application.
%\DataflowName is applicable to any DAG of operations. 
We demonstrate that while \DataflowName is essential for extracting reuse in complex DAGs like CG, it can also be applied to simpler DAGs like GCNs and DNNs, thus preserving generalizability.

%\insertFigurePartnnnn{no-fusion}{A part of the CG tensor dependency graph where a node represents the equation in~\autoref{alg:cg_einsum} and edge represents the output of the source node equation. Please refer to ~\autoref{fig:dfg} for complete graph of CG.\vspace{-3mm}}%\RG{contraction heavy}}
We summarize our contributions below:
%\TK{I think the contribitions can be listed more succinctly. I would suggest each contribution bullet pointing to a specific section of paper. }

\squishlist
%\item We characterize the challenge of skewed GEMMs/SpMMs in tensor-algebra applications and the challenges of generalizing traditional inter-operation pipelining~\footnote{Not to be confused with inter-operation "reuse" since pipelining is a narrow aspect of inter-operation reuse}. (\autoref{sec:ai}).
%We also observe that these patterns are frequent across other HPC workloads which we discuss in ~\autoref{sec:background}.
%\item We propose a systematic methodology to formulate the data reuse opportunities in an arbitrary DAG of tensor operations and based on the reuse opportunities, we derive the loop orders and tile sizes.(\autoref{sec:dataflows}).
%\item We co-optimize the loop orders and tiling strategies for the GEMM operations in order to leverage reuse from both traditional inter-operation pipelining and distance based inter-operation reuse.

\item We propose~\DataflowName (\autoref{sec:score}), a scheduler which identifies the operands with delayed dependencies that require writeback (and hence \SpadNamenospace), and proposes a schedule that maximizes inter-operation reuse and minimizes the layout transformation of the operands across different consumers.
\item We propose~\SpadName(\autoref{sec:chorus}), a buffer structure for operands with downstream consumers that uses tensor-operand level replacement, that reduces tag match overhead and considers a more wholistic view of the object rather than a cache line. Cycle-level implicit tensor level replacement also eases the burden of solving tensor allocation involving multiple tensors statically which is a hard problem. 
%\item %Prior works on pipelining~\cite{flat,tangram,garg2021understanding,yan2020hygcn} divide the whole compute region into contiguous chunks and an operation is mapped on to a chunk as~\autoref{fig:spacetime} (top sub-figure) shows.\TK{seems odd to cite Fig 12 in intro}
%However, there is not much reuse within a single operation and the whole tensor needs to be communicated between the chunks. 
%\TK{this seems like a very specific optimization - and confusing here about what is lower BW vs higher BW NoC .. in fact i dont recall our arch section / Table IV talking about two NoCs with diff BWs? cant you state this contribution more generally about a communication BW optimized scalable inter-operation tiling strategy?}
%We propose a scalable inter-operation tiling strategy that reduces inter-cluster communication(~\autoref{sec:tiling}).  %a mapping that reduces memory accesses and communication at the tensor dependency graph level.
%\end{itemize}
%\item We propose buffer management schemes that reuse initial tiles of the output and can also replace the tensor based on future reuse distance and frequency~(\autoref{sec:tornado}). %the notion of Tensor-operand level reuse distance for such patterns and a tensor organization strategy inside the buffer hierarchy (\autoref{sec:tornado}).

\item We demonstrate the limitations of caches, namely, area overhead and line-level policies and the limitations of scratchpad, namely complexities in static buffer allocation of multiple delayed downstream consumers (\autoref{sec:arguments}).

\item We show that \SpadName and \DataflowName are applicable to diverse tensor applications and achieve 2.51x geomean speedup and 4x improvement in energy efficiency across a broad range of workloads (\autoref{sec:eval}).

%\item \reviewme{ } 
%\item We propose a novel data orchestration technique \DataflowName which allows for efficient replacement, placement and prefetching of data for applications with variable reuse distance patterns. Since reuse distance is a generalization, \DataflowName can be used for applications with only intra-operation reuse and inter-operation pipelining opportunities as well.
%\item We propose a novel buffering mechanism \TODO{name} which improves performance and energy over caches and scratchpads by \TODO{xx} and \TODO{xx}
\squishend
\end{comment}


\begin{comment}
\emph{Inter-operation pipelining (aka fusion)} has been shown to be beneficial for accelerators for applications like Graph Neural Networks~\cite{garg2021understanding,yan2020hygcn,liang2020engn} where the intermediate tensor is reused between an SpMM and a GEMM,  reducing  intermediate output data movement to and from DRAM.
It has been shown that this approach is more challenging~\cite{dnnfusion,flat} and has a larger design-space~\cite{garg2021understanding} than straightforward element-wise fusion done by ML compilers today, for example, matrix-multiplication and ReLU fusion.
%Unfortunately, \textit{traditional inter-operation pipelining} often consumes the intermediate data and does not keep it in the memory. 
%Traditionally, \textit{inter-operation pipelining} has been exploited in prior works~\cite{tangram,garg2021understanding,yan2020hygcn,flat} %across various application domains 
%to reduce the data movement to DRAM, by consuming the portion of the tensor as it is produced 
\TK{I think we need to transition to saying that inter-operation pipelining does not capture all inter-operation reuse opportunities as this work identifies.}
Unfortunately, the additional complexity of inter-operation dependency graphs in certain applications introduces additional challenges which confounds the attempts at \emph{traditional inter-operation pipelining.} Furthermore, just capturing reuse in adjacent operations misses the overall opportunity to consider future instances of reuse of tensors in the entire program. \reviewme{We use CG as an example to discuss these challenges and missed opportunities:}

(1) Operations can have a delayed downstream consumer. Therefore, the data must remain resident in the memory hierarchy. However, traditional pipelining overwrites tiles that are consumed by the adjacent operation.
%~\autoref{fig:dfg} shows CG's dependency graph. Output of operation 1 is required in a delayed downstream consumer (op4) in addition to the adjacent consumer (op2). %The intermediate matrix cannot be overwritten or discarded, since its required in a future computation. 


%This complex dependency graph can often complicate the optimization of loop orders and tiling strategies to determine efficient intra-operation and inter-operation dataflows.


(2) CG has some tensor operations with contracted rank being much larger than the other ranks. This diminishes the benefit of pipelining because significant computation is required to produce any given final sum; therefore, making it a rate limiting step. 
%This prevents pipelining the entire graph.
%\TK{the following line seems out of place - since its one of many techniques we propose. Should maybe remove it}
%We propose \textit{pipelining with writeback}, which involves traditional pipelining and writing back the intermediate data to the buffer, hence incurring only write accesses and avoiding the read accesses for the intermediate matrix.

\insertFigure{no-fusion}{A part of the CG tensor dependency graph where a node represents the equation in~\autoref{alg:cg_einsum} and edge represents the output of the source node equation. The data cannot be consumed and be shielded from memory hierarchy since its reused again in another tensor. Please refer to ~\autoref{fig:dfg} for complete graph of CG.\vspace{-3mm}}%\RG{contraction heavy}}

(3) As the DAG becomes more complex, it is important to make sure that the loop order choice minimizes the data layout transformation (we use the term swizzling for it) of these tensors across various consumer operations.

(4) The downstream consumers also result in multiple \textit{tensor operand reuse distances}.

In this work, we identify various \textit{inter-operation} reuse opportunities to enhance the arithmetic intensity of such applications.
Our proposed \emph{generalized inter-operation reuse} is a wider generalization of \emph{traditional inter-operation pipelining}. We propose \DataflowName (\TitleExpansion), a strategy for mapping DAG of tensor operations which exploits reuse opportunities between operations.

\DataflowName is applicable to any DAG of operations. So while it helps extract reuse in the complex DAGs like CG, these patterns can also apply to simpler DAGs like GCNs, thus preserving generalizability.

%\reviewme{\DataflowName consists of the following contributions.

 %First, we identify the inter-operation reuse patterns in an arbitrary DAG of tensor operations. We propose a methodology to mark the edge of the DAG with the appropriate reuse pattern. This addresses the DAG complexity problems.

% Second, we propose loop orders that try to take the maximum advantage of the reuse patterns, given that the ability to extract reuse also depends upon the order of loops in these operations. The loop order also minimize swizzling

 %Third, prior works on pipelining~\cite{flat,tangram,garg2021understanding,yan2020hygcn}, divide the whole compute region into contiguous chunks and an operation is mapped on to a chunk as in~\autoref{fig:spacetime} (top sub-figure) shows. However, there is not much reuse within a single operation and the whole tensor needs to be communicated between the chunks. Instead, we pipeline within the cluster and split dominant ranks across cluster (\autoref{fig:spacetime} bottom).


% Fourth, given that we minimize swizzling, we propose a buffer management scheme that writes the data into the SRAM in the same order and skips the SRAM once its full. Then it reads the portion that's already in the SRAM first. We show that this can improve the op-by-op baseline considerably. We also propose buffer management strategies to prioritize operations with low reuse distances and high reuse frequencies.

% }

%In this work, we identify various \textit{inter-operation} reuse opportunities which are not limited to \textit{inter-operation pipelining}.
%Please note that \textit{inter-operation reuse} is a wider generalization of \textit{traditional inter-operation pipelining} here, since inter-operation pipelining only exploits reuse between consecutive operations. \autoref{fig:venn} shows the scope of our work on inter-operation dataflows compared to prior works on acceleration.
%One example is \textit{pipelining with writeback}

%Moreover, it is important to focus on leveraging the knowledge of future occurrence of the tensor beyond the immediate tensor. In CG, these tensor operands have variable \textit{tensor operand reuse distances}. This pattern is frequent in various scientific applications as~\autoref{sec:background} shows. This complex dependency graph can often complicate the determination of loop orders and tiling strategies to determine efficient intra-operation and inter-operation dataflows.

%Also, given that the graph of tensors becomes more complex, it is important to make sure that the loop order choice minimzes the transformation of layouts of these tensors across various operations where that tensor is used. We use the term \textit{swizzling} for the layout transformation. Thus we need to minimize swizzling.

%\RG{Can we cut this para and make contributions slightly longer ?}
%Based on the insights from the tensor dependency graphs of various applications, we propose a systematic methodology to identify, classify and exploit reuse opportunities in an arbitrary graph of tensor operations targeting spatial accelerators. This also involves deriving the loop orders and tile sizes for individual operations since the ability to exploit the inter-operation reuse can also depend on the individual operation's dataflow.
%Some other applications which have individual \GEMM of low arithmetic intensity include Graph Neural Network, Transformers etc. where our methodology is applicable, however, we often refer to Conjugate Gradient for demonstration purposes in this paper since its tensor dependency graph already has the characteristics of low intensity Graph and ML workloads but also has additional unique characteristics providing opportunity to demonstrate different kinds of inter-operation reuse.
%We also propose a scalable tiling strategy that involves splitting a memory bound GEMM by the dominating rank into sub-tensors and reusing the data in a fine-grained manner between the sub-tensors from different operations within a compute node as the bottom half of~\autoref{fig:spacetime} shows.
% (shown later in~\autoref{fig:spacetime})\TK{not sure if we need to cite a figure that'll come this late. Might be better to point to the section that will discuss this}. 
%
% We propose \DataflowName (\TitleExpansion), a strategy for mapping DAG of tensor operations which exploits reuse opportunities between operations. It comprises of identifying inter-operation reuse patterns from the DAG structure, loop-reordering and tiling based on those patterns and custom scratchpad management strategies. 
Our key contributions are as follows:
%\vspace{-5mm}

%\begin{itemize}
\squishlist
\item We characterize the challenge of skewed GEMMs/SpMMs in tensor-algebra applications and systematically formulate \emph{generalized inter-operation reuse opportunities} (beyond pipelining) in a DAG of operations (\autoref{sec:dataflows}).
%We also observe that these patterns are frequent across other HPC workloads which we discuss in ~\autoref{sec:background}.
%\item We propose a systematic methodology to formulate the data reuse opportunities in an arbitrary DAG of tensor operations and based on the reuse opportunities, we derive the loop orders and tile sizes.(\autoref{sec:dataflows}).
%\item We co-optimize the loop orders and tiling strategies for the GEMM operations in order to leverage reuse from both traditional inter-operation pipelining and distance based inter-operation reuse.
\item Based on the reuse opportunities, we derive loop orders for the operations that allow consumers to maximally reuse the data, and require minimum changes to memory layout~(\autoref{sec:loop}).
\item Prior works on pipelining~\cite{flat,tangram,garg2021understanding,yan2020hygcn} divide the whole compute region into contiguous chunks and an operation is mapped on to a chunk as~\autoref{fig:spacetime} (top sub-figure) shows. However, there is not much reuse within a single operation and the whole tensor needs to be communicated between the chunks. We propose a scalable inter-operation tiling strategy which splits the dominant rank across compute clusters and pipelines the operations within a cluster as~\autoref{fig:spacetime} (bottom) shows~(\autoref{sec:tiling}).  %a mapping that reduces memory accesses and communication at the tensor dependency graph level.
%\end{itemize}
\item We propose a buffer management scheme that writes the data into the SRAM in the same order and skips the SRAM once its full. Then it reads the portion that's already in the SRAM taking advantage of swizzle minimzation. We also propose buffer management strategies that prioritize operations with low reuse distances and high reuse frequencies~(\autoref{sec:tornado}). %the notion of Tensor-operand level reuse distance for such patterns and a tensor organization strategy inside the buffer hierarchy (\autoref{sec:tornado}).
\item \DataflowName achieves \reviewme {geomean 5.97x (ranging from 1.34x to 23x)} improvement in the arithmetic intensity over operation by operation execution~(\autoref{sec:eval}).

\item \reviewme{\DataflowName is generally applicable to diverse tensor applications as it applies to all DAG structures. We also evaluate it on GCNs and ResNets and obtain 2.71x and xx geomean improvement in arithemetic intensity over op-by-op baseline~(\autoref{sec:eval}).} 
%\item We propose a novel data orchestration technique \DataflowName which allows for efficient replacement, placement and prefetching of data for applications with variable reuse distance patterns. Since reuse distance is a generalization, \DataflowName can be used for applications with only intra-operation reuse and inter-operation pipelining opportunities as well.
%\item We propose a novel buffering mechanism \TODO{name} which improves performance and energy over caches and scratchpads by \TODO{xx} and \TODO{xx}
\squishend
\end{comment}

\begin{comment}

\begin{enumerate}
    \item Identifying inter-operation reuse patterns in an arbitrary DAG of operations
    \item Deriving loop orders to make sure that they are amenable to inter-operation reuse and minimize swizzling
    \item Proposing tiling strategy that does pipelining within the compute node and splits the large rank across nodes (bottom half of~\autoref{fig:spacetime}).
    \item Proposing scratchpad management strategies like reuse distance and frequency based tensor replacement and proposing strategies to extract reuse from the portion of the tensor that does fit in SRAM.
    
\end{enumerate}
\vspace{-1mm}
Our methodology is also applicable to %(and evaluated on) 
other applications with low intensity GEMMs like GNNs and transformers. %where our methodology is applicable,
However, we often refer to Conjugate Gradient as the application for demonstration purposes since its tensor dependency graph has the characteristics of low intensity Graph and ML workloads and also has additional unique characteristics providing opportunity to demonstrate different kinds of inter-operation reuse.
\autoref{fig:venn} shows the scope of our work on inter-operation dataflows compared to prior works on accelerator dataflow.
%Our proposed \textit{inter-operation reuse} is a wider generalization of \textit{traditional inter-operation pipelining} here, since inter-operation pipelining only exploits reuse between consecutive operations.
%\autoref{fig:venn} shows the scope of our work on inter-operation dataflows compared to prior works on acceleration.

%Therefore data orchestration which accounts for such reuse is critical for HPC workloads. Cache replacement policies often have a global view of an individual line rather than a tensor as a whole, which limits the applicability for such algorithms. Moreover, LRU replacement does not see the future reuse of the tensor and replaces it if it has not been used recently. Belady's optimal replacement policy requires knowledge of future accesses for each line and often requires additional structures and meta data accesses to replace one line which is costly to implement~\cite{popt-hpca21}. Scratchpads, on the other hand provide ability to the programmer to have control over data orchestration. However, these scratchpads are often over-provisioned for the worst case applications.

%To this end, we propose \DataflowName data orchestration that analyzes the tensor operand-level reuse patterns in the algorithm and the dataflows of the individual GEMMs to manage the data in the memory hierarchy. \DataflowName also enables efficient prefetching for these HPC algorithms due to the knowledge of the future tensor reuse pattern and dataflows.

 %Some other applications with low arithmetic intensity include Graph Neural Network, Transformers etc., however, we often refer to Conjugate Gradient for demonstration purposes in this paper since its DAG already has the characteristics of Graph and ML workloads but also has additional unique characteristics providing opportunity to demonstrate different kinds of inter-operation reuse.


\noindent  \textbf{\textit{The key contributions of this paper are:}}
\end{comment}

%Our methodology is also applicable to %(and evaluated on) 
%other applications with low intensity GEMMs like GNNs and transformers. %where our methodology is applicable,
%However, we often refer to Conjugate Gradient as the application for demonstration purposes since its tensor dependency graph has the characteristics of low intensity Graph and ML workloads and also has additional unique characteristics providing opportunity to demonstrate different kinds of inter-operation reuse.
%\autoref{fig:venn} shows the scope of our work on inter-operation dataflows compared to prior works on accelerator dataflow.
%Our proposed \textit{inter-operation reuse} is a wider generalization of \textit{traditional inter-operation pipelining} here, since inter-operation pipelining only exploits reuse between consecutive operations.
%\autoref{fig:venn} shows the scope of our work on inter-operation dataflows compared to prior works on acceleration.


































\begin{comment}
\section{Problem and Motivation}
\label{sec:introduction}

Sparse and Dense matrix multiplications are prime operations for a variety of applications spanning Graph Analytics~\cite{kipf2017semisupervised,hamilton2017inductive}, High-Performance Computing~\cite{cools2017communication,cerebras} and Artificial Intelligence~\cite{resnet,nlp}. The GEMM operations used in DNNs offer vast reuse opportunities~\cite{kwon2019understanding,interstellar,timeloop,eyeriss2016isca} owing to their high arithmetic intensity. This has led to a plethora of spatial accelerators for DNN applications~\cite{eyeriss2016isca,kwon2018maeri,tpu-isca,nvdla,shi}. Prior works have also looked at the acceleration for Sparse workloads~\cite{sigma,eie,extensor,eyeriss2} by eliminating redundant matrix multiplications and memory accesses.

However, certain PDE solver algorithms used scientific applications like the Conjugate Gradient~\cite{hestenes1952methods} have SpMM/SpMV and GEMM/GEMV operations with low arithmetic intensity~\cite{cerebras}. The best supercomputers ranked on High-Performance Linpack benchmark are able to achieve only upto 3\% of the performance on the HPCG benchmark~\cite{cerebras,osti_1089988}.
~\autoref{alg:cg_einsum} shows the Block Conjugate Gradient Algorithm. ~\autoref{fig:results}a) and b) show maximum achievable arithmetic intensity of individual operations. Please note that instead of counting multiplications and additions separately, we count MAC as a unit of computation. We also consider matrix addition of, for example, $X^{k-1}$ with $P^{k-1}.\alpha^{k-1}$ in the equation $X^{k}=X^{k-1}+P^{k-1}\alpha^{k-1}$ to be done immediately after the MAC and we absorb the addition in same operation. Therefore, effectively we count number of multiplications per memory access of an element (independent of the bit precision). Please note, here number of RHS is a parameter with values 1,8,16,32 and 64. 

\insertFigure{cg}{Block CG Algorithm.}


~\autoref{fig:results}a) shows the maximum achievable arithmetic intensity for individual SpMM operations for the suitesparse CG matrices. We note that the SpMMs, especially the highly sparse ones like barth4 have extremely low arithmetic intensity even for RHS=64.
~\autoref{fig:results}b) shows the maximum achievable arithmetic intensity of dense GEMMs in an individual operation in terms of number of multiplications per memory access. We notice that the arithmetic intensity of the DenseGEMMs is severely limited by the number of RHS (ie. the number of problems solved simultaneously.) Therefore traditional DNN accelerators~\cite{eyeriss2016isca,kwon2018maeri,tpu-isca} do not help accelerate these workloads due to fundamentally low reuse.

~\autoref{fig:results}c) shows the arithmetic intensity for one iteration of the CG loop. Please note that for matrix inverse, we consider LU factorization followed by Triangular Solve. Also, we conservatively count accesses to $P$ and $P^T$ separately owing to different layout in the memory. We note that the amount of reuse that can be extracted between the operations is high and this motivates us to design a new dataflow for HPCG workloads which tries to maximize inter-operation rather than optimizing for inter-operation reuse.

\end{comment}