%   \reviewme{\subsection{Architectural Implications}


\section{Evaluations}

\subsection{Experimental Methodology}
\label{sec:expt}

We describe our experimental methodology for demonstrating the efficiency of \DataflowName across baseline dataflows.
%\vspace{-2mm}


%\indent \textbf{Analytical Framework:~}
%\label{sec:simulation}
%We modify the OMEGA~\cite{garg2021understanding} framework to model the DAG of tensor operations for applications like Conjugate Gradient, Graph Convolution Networks, ResNet. OMEGA is a cost model for modelling performance and energy for various mappings with two-operation pipelining.
%Our framework builds upon STONNE~\cite{STONNE21} which is a cycle accurate simulator that models one DNN layer at a time and models flexible interconnects~\cite{kwon2018maeri} with ability to support multiple intra-operation dataflows. STONNE has been extensively validated against MAERI~\cite{kwon2018maeri} and SIGMA~\cite{sigma} RTLs. STONNE can model SpMM, DenseGEMMs/CONVs on the accelerator with reconfigurable interconnects. Prior work OMEGA~\cite{garg2021understanding} builds a cost model which uses the STONNE output and timestamps to model pipelining between two operations. We modify the cost model that OMEGA uses to model a DAG of tensors in applications like Conjugate Gradient. We use that to simulate the baselines and \DataflowName which are described in~\autoref{sec:baseline}.

\reviewme{\textbf{Modeling Framework.}
%\label{sec:simulation}
We developed an analytical framework to model the DAG of tensor operations for applications like Conjugate Gradient, GNNs, DNNs (e.g., ResNets) and schedule the operators in an op-by-op manner or via all the pipelined schemes introduced in this work. For each operator, we determine the best intra-operator dataflows and tile sizes (using individual operation tiling strategies in~\autoref{sec:tiling}) to maximize compute utilization.
%This dataflow search is similar to the one employed by mappers in MAESTRO~\cite{chatarasi2020marvel} and Timeloop~\cite{timeloop}.
%, assuming a flexible dataflow accelerator. 
Next, we leverage \DataflowName to determine the inter-operator mapping.
%We then schedule the operators over an accelerator simulated using STONNE~\cite{STONNE21}, which is a cycle accurate simulator for flexible dense and sparse dataflow accelerators, including our baseline Flexagon~\cite{flexagon}\footnote{\reviewme{The reason to pick STONNE, rather than an analytical cost model~\cite{timeloop} is due to its support for sparse inputs.}}.
Our framework captures the memory acceseses through the hierarchy and communication over the NoC across all the mapping strategies. DRAM accesses for individual operations are validated using the STONNE cycle-accurate simulator~\cite{STONNE21}. Our framework also computes the arithmetic intensities from the DRAM accesses and the throughput using the equations in~\autoref{sec:roofline}, which we use to plot gaps from ideal in our results. 
%We plot the throughput of all the data points and expand the last one into a roofline showing both throughput and arithmetic intensity. We also plot reduction in DRAM accesses and sensitivity studies on inter-cluster NoC communication and change in memory bandwidth (\autoref{fig:comm}).
}


% We developed an in-house analytical model to capture the memory accesses and communication across an accelerator's memory hierarchy. We validated per-layer statistics against STONNE~\cite{STONNE21}, which is a cycle accurate simulator that models one operation (it can model GEMM, SpMM, SpMSpM, CONV) at a time. 
% STONNE also models our baseline flexible accelerator Flexagon~\cite{flexagon}.

% We build an analytical model to capture the memory accesses and communication across clusters for the baselines and \DataflowName to compare the achieved arithmetic intensity of the dataflows independent of the accelerator implementation. We validate the DRAM statistics for a single layer by using STONNE~\cite{STONNE21}, which is a cycle accurate simulator that models one operation (it can model GEMM, SpMM, SpMSpM, CONV) at a time. STONNE can be supplied with various dataflows and tile sizes as inputs. STONNE also models the flexible accelerator Flexagon~\cite{flexagon}. We model SpMM (the sparse matrix is in CSR format) and GEMM the microarchitecture of Flexagon since it is Flexible enough to support any mapping to get a strong op-by-op baseline. %We validated the data movement for a pair of two pipelined tensor operations using OMEGA~\cite{garg2021understanding,omega} which is a wrapper around STONNE to model pipelining between two operations. 

\input{tables/config.tex}
\input{tables/workload.tex}

\input{tables/dataflow_config.tex}
\label{sec:dataset}

\reviewme{\textbf{Architecture Parameters}
\label{sec:arch} 
We consider a clustered architecture, each cluster of 1024 PEs connected using a NoC, backed by its own SRAM slice, as shown in \autoref{fig:spacc}. We sweep different SRAM capacities and memory bandwidths as shown in \autoref{table:config} . Together with diverse tensor sizes across workloads and datasets, this allows us to study the efficacy of \DataflowName across diverse tensor shapes and sizes.} 

\indent \textbf{Workloads and Datasets:~} 
\autoref{table:dataset} shows the workloads and the datasets.
We obtain the sparse matrices of the CG datasets from Suitesparse~\cite{suitesparse} for scientific problems like structural problems, circuit simulation, and so on. For CG, we also sweep the $N$ rank that corresponds to number of simultaneous initial guesses, as shown in \autoref{table:config}. 
We obtain GNN graphs from OMEGA~\cite{omega}.
We run workloads of different sizes and nnz's. We also evaluate ResNet and SSD ResNet.%{\color{red} @Raveesh -- the table also shows params for Attention layer now. Might be good to simply point to the table sayin we vary different params across workloads to save space}




% \indent \textbf{Workload and Architecture Parameters:~}
% \autoref{table:config} describes the architecture configuration we use for the evaluation and the parameters we sweep. We run workloads of different sizes and nnz's and we also sweep the $N$ rank that corresponds to number of simultaneous initial guesses and the SRAM size to capture different scenarios with varying ratios of tensor and SRAM sizes. In some cases, tensors are either too small or too large for the SRAM and in some cases, a varying proportions of tensors can fit.
% We consider a clustered architecture, each cluster of 1024 PEs backed by its own SRAM slice.

\label{sec:baseline}



\indent \textbf{Dataflow Configurations:~}
%\TK{Note to self -- we should be able to shorten this .. is repeating what table IV says}
We compare multiple dataflow configurations described in~\autoref{table:dataflow_config}. 
We evaluate all schemes over the same HW substrate (\autoref{sec:arch_implications}) to isolate the effects due to the mapping instead of microarchitectural effects.

\begin{comment}
Flexagon-like dataflow models an oracle operation-by-operation dataflow, running SpMM and~\GEMM on the flexible microarchitecture.
%It is able to achieve the lowest possible DRAM accesses for individual operations, but it requires the output tensor of each operation ending up in the DRAM and the input coming from the DRAM.}
%
%Flexagon-like dataflow is modelled based on the microarchitecture of Flexagon~\cite{flexagon}. We model SpMM and~\GEMM on the flexible microarchitecture. It is able to achieve the lowest possible DRAM accesses for individual operations but it considers the output tensor of each operation ending up in the DRAM and the input coming from the DRAM. Its the oracle operation-by-operation dataflow. 
%
Adjacent pipeline baseline consists of situations where only adjacent pipeline is possible and any kind of dependency that results in writeback or hold results in sequential execution. We demonstrate that in CG, adjacent pipeline cannot be applied while it can be applied in GNN layer and only part of ResNet layer.
\DataflowNamenospace-\textsc{Qpad} does not consider pipelining, however, it writes the portion of the tensor that fits in the scratchpad in FIFO order proposed in~\autoref{sec:tornado} but does not consider distance/frequency based replacement. This ensures that parts of intermediate tensors that could fit, are still reused inside the SRAM. \DataflowNamenospace-\textsc{Qpad} by itself does not minimize swizzle penalty, it still writes the data in FIFO manner for tensors that are not swizzled later.
%
\DataflowNamenospace-\textsc{all} uses all the contributions proposed.
%It uses the inter-op patterns proposed in~\autoref{sec:dataflows} and finds the individual loop orders which enable pipelining and avoid swizzle\_penalty proposed in~\autoref{sec:loop} and also uses the scratchpad management in~\autoref{sec:tornado}. It also uses scalable tiling strategy that reduces inter-cluster communication overhead proposed in~\autoref{sec:tiling}.

Ideal, represents the arithmetic intensity with perfect reuse and infinite SRAM capacity.
\end{comment}

\label{sec:params}

\insertFigure{AI}{Throughput for CG workload for first 10 iterations of the CG loop. Memory BW = 1000 GB/s. We sweep the SRAM capacity - \{1MB, 4MB, 16MB\}. N=\{1,16\} and is mentioned in parenthesis as (1)/(16) along with the dataflow configuration. \reviewme{At the bottom, we show the dataset-wise geomean throughput gain of \DataflowName \textsc{all} over Flexagon-like. We also show the last chart on a roofline.}\vspace{-3mm}}

\insertFigure{GNN}{Throughput for GNNs and ResNets\vspace{-1mm}}

\insertWideFigure{comm}{(a) Normalized DRAM access reduction for each dataflow configuration geomeaned across workload and architecture configurations.
(b) Inter-cluster link traversals in KBs for N=8. (c) Effect of Memory BW on throughput}
%\insertFigure{Energy}{Normalized DRAM accesses for each dataflow configuration geomeaned across all workload and architecture configurations. This directly impacts the energy.\vspace{-1mm}}
%\insertFigureRevision{bert}{Roofline plots for attention layers.}

% \indent \textbf{Workload and Architecture Parameters:~}
%\autoref{table:config} describes the architecture configuration we use for the evaluation and the parameters we sweep. We run workloads of different sizes and nnz's and we also sweep the $N$ rank that corresponds to number of simultaneous initial guesses and the SRAM size to capture different scenarios with varying ratios of tensor and SRAM sizes. In some cases, tensors are either too small or too large for the SRAM and in some cases, a varying proportions of tensors can fit.
%We consider a clustered architecture, each cluster of 1024 PEs backed by its own SRAM slice.
