\begin{comment}

\section{Inter-operation Reuse Opportunities}
\label{sec:ai}

In this section, we discuss the arithmetic intensity of GEMMs, and how inter-operation reuse can improve the arithmetic intensity. We also discuss why traditional inter-operation pipelining is a narrow aspect of inter-operation reuse, in terms of scope of the reuse and in terms of the applicability to all DAGs, and why a more generalized view of inter-operation reuse is important.

\subsection{Roofline Plots and Arithmetic Intensities of GEMMs}
\label{sec:roofline}
Roofline plot~\cite{roofline} is an intuitive way to find whether a given program is memory bound or compute bound on a given system. Given that memory accesses can be overlapped with floating point operations,

\vspace{-2.5mm}
\begin{equation}
    Throughput \left(\theta\right) = min\left(Compute_{\theta}, Memory_{\theta}\right)
\end{equation}
\vspace{-4.5mm}

Thus, either compute or memory defines the upper bound.

\vspace{-3.5mm}
\begin{equation}
\label{eqn:throughput}
    Throughput = min\left(Max Ops \big/ sec, Memory\,BW\times AI\right)
\end{equation}
\vspace{-4.5mm}

where AI is the total $Operations\big/bytes$.


\insertFigure{roofline}{Roofline plot comparing GEMMs in Conjugate Gradient (CG) and DNNs. Memory BW=250GB/s and 2000 GB/s and number of PEs=16384. This assumes the best case with only cold misses. We label the exact AIs. %\TK{i think Fig 5, 6 and this chart can all come together in one figure to save space - they are all conveying same point. Or the charts in Fig 6 can also be converted to a table sitting next to this. In fact - cant you try and plot AI of full iteration CG (i.e., Fig 7) on this roofline as well (to show what you are trying to achieve? So I suggest putting all the AIs in a table and put it next to this roofline and plotting them all on the roofline}
\vspace{-2mm}}

%Roofline model plots Throughput vs Arithmetic intensity on a 2D plot as~\autoref{fig:roofline} shows. Lower AI, for example, in case of CG, implies that the program is likely to be memory bound and the throughput will be limited by the memory bandwidth. Note that the prior accelerators designed for DNN that focus on compute utilization and the datapath organization do not help the throughput for such applications. Improving AI (aka reuse) or the memory bandwidth will improve the throughput in such cases (as also seen with BW=250GB/s and 2000GB/s). In this work, our focus is on improving the AI by improving the on-chip reuse and minimizing the off-chip accesses.
%
%We further show AI can be improved by inter-operation reuse.


%\TK{THis is the motivation section right? You identify opportunity for speedup if you had AIbest? Maybe rename this to Motivation: Arithmetic Intensity of Ops within CG} 

%In this section, we show that memory bound applications are limited by arithmetic intensity (AI).
%We, then, quantitatively analyze the best case\footnote{Meaning the application's best case AI without hardware and compiler constraints.} AI ($AI_{best}$), assuming full reuse as the number of floating point multiplications divided by the number of memory accesses in terms of number of elements\footnote{In~\autoref{sec:ai}, we measure AI\textsubscript{best} in ops/word agnostic of the bit precision. Using ops/byte will only further reduce the intensity of CG because of the higher bit-precision requirements of HPC applications (4-8x). We measure the actual op/byte intensity in~\autoref{sec:eval}.}. We show that the arithmetic intensity within a single tensor operation is limited in the HPC applications which implies limited reuse inside a skewed GEMM. We then demonstrate that the arithmetic intensity of chained Operations with inter-operation reuse is significantly higher. %Finally, we then provide an overview of reuse opportunities in CG.

%\subsection{What's Different about Memory-bound Operations?}

%\subsection{Operations in Isolation}
\label{sec:isolation}
For an operation $Z_{m,n}=A_{m,k}*B_{k,n}$, the AI is as follows-

    
\vspace{-2.5mm}
\begin{equation}
    AI = \frac{Floating\,point\,multiplications}{Compulsory\,accesses\text{ + }Replacement\,accesses}
\end{equation}
\vspace{-2.5mm}

%Hypothetically, if we are able to achieve perfect reuse or have infinite capacity, such that the only accesses are compulsory accesses, the AI is as follows-

\vspace{-2.5mm}

\vspace{-2.5mm}

For GEMMs, this translates to

%For GEMMs number of matrix multiplications is equal to $M\times K\times N$. Assuming perfect reuse, the $A$ and $B$ matrices need to be read once and the $Z$ matrix needs to be written once which makes the number of memory accesses equal to $M\times K\text{ + }K\times N\text{ + }M\times N$.
\vspace{-2.5mm}

\begin{gather}
    AI_{best} = \frac{M\times K\times N}{M\times K\text{\text{ + }}K\times N\text{ + }M\times N}
\end{gather}

For matrices with high $M$, $N$ and $K$--for example, in DNNs--the AI is high due to a higher order numerator:
\vspace{-2.5mm}

\begin{equation}
    \lim_{M,K,N\to\infty} \frac{M\times K\times N}{M\times K\text{ + }K\times N\text{ + }M\times N} = \infty
\end{equation}

In the limit, this approaches infinite due to higher order numerator.

\begin{equation}
    \lim_{M,K,N\to\infty} \frac{M1\times K1\times N1\times M0\times K0\times N0}{M1K1\times M0K0\text{ + }M1K1N1\times K0N0\text{ + }M1K1N1\times M0N0} = \infty
\end{equation}

%This is in marked contrast to operations like Matrix-Vector multiply, whose intensity is invariant of tensor shape:
%\vspace{-0.5mm}
%\begin{equation}
    %\lim_{M,K\to\infty} \frac{M\times K}{M\times K\text{ + }K\text{ + }M} = 1
%\end{equation}

%This low AI means that the matrix-vector multiply is memory-bound on state-of-the-art accelerators. This is demonstrated by the importance of batch size in DNNs on GPUs and TPUs.

This does not mean that GEMMs are fundamentally high intensity. Notably, in CG applications, within \GEMM we observe that one dimension is too large and other dimensions are too small which limits the arithmetic intensity, which~\autoref{fig:class} also shows. 
For example, if $M$ is the large dimension and $K=N$:
%\vspace{-5mm}

\begin{equation}
    \lim_{M\to\infty} \frac{M\times K\times N}{M\times K\text{ + }K\times N\text{ + }M\times N} = \frac{N}{2}
\end{equation}

Basically, in the limit a skewed GEMM approaches an intensity bounded by its small dimensions. For workloads where N <= 16 (as in CG as we demonstrate), this does not give sufficient reuse to reach the compute bound of any reasonable accelerator microarchitecture, and therefore degrades to memory-bound performance in isolation (see \autoref{fig:ai1}). As practical examples, ~\autoref{fig:roofline} shows $AI_{best}$ across individual tensor operations.
This motivates the need to exploit inter-operation reuse to increase intensity in scientific applications.

%\reviewme{~\autoref{fig:ai1} demonstrates this visually. We consider regular GEMMs (512x512x512) and skewed GEMMs (524288x16x16), which have almost same matrix multiplications. When GEMM is skewed, the overall intra-operation reuse is reduced.}

%In ~\autoref{alg:cg_operation}, in line 3, the large dimension is uncontracted ($P.\alpha$) while in line 5, the large dimension is contracted. However, in each \GEMM operation there only one large dimension which limits the intra-operation reuse.

%\subsection{SpMM in Isolation}

%For an SpMM operation, where A matrix is sparse and compressed in a format such as CSR, the number of multiplications is equal to $nnz\times N$, where $nnz$ is the total number of non-zeros in the MK matrix. The minimum number of accesses assuming complete reuse is $2\times nnz\text{ + }M$ for MK in CSR format. Here we consider MN as uncompressed, thus accesses for that equals $M\times N$. For KN matrix, in CG application, every index of K has atleast one non zero. Thus the number of accesses for KN matrix are $K\times N$. Thus the AI is:

    \vspace{-2.5mm}

\begin{equation}
    AI_{best} = \frac{nnz\times N}{2\times nnz\text{ + }M
\text{ + }N\times K\text{ + }M\times N}
\end{equation}



%Here, arithemtic intensity is low due to high sparsity ratio which makes $nnz$ of the same order as M and K. Considering M and K to be large and equal as in CG SpMM. Also, the average number of non zeros per row \reviewmehpca{(ie $nnz/M$)} is $nz_{av}$
\vspace{-2.5mm}

\begin{equation}
   \lim_{M,K\to\infty} \frac{nnz\times N}{2\times nnz\text{ + }M
\text{ + }2\times M \times N}=\frac{nz_{av}\times N}{2nz_{av}\text{ + }1\text{ + }2N}
\end{equation}

This depends on $nz_{av}$ and $N$ but is strictly less than $\frac{N}{2}$.

%\insertFigure{aidl}{AI\textsubscript{best} of GEMMs in popular DNN applications.}


%\insertFigure{ai}{AI\textsubscript{best} for (a) CG skewed GEMM and (b) CG skewed SpMM operations for CG workloads. N=16, M,K and nnz vary by application.}
%The $A$ matrix for SpMM is obtained from suitesparse matrix collection~\cite{suitesparse}. %Assuming 32-bit floating point representation, op per byte ratio for DenseGEMMs is $8/4=2$ which is orders of magnitude lower than DNNs thus limiting intra-operation reuse.
%Again, this data motivates the need to exploit inter-operation reuse to escape the memory bound.

%\subsection{Spatial Accelerators}

%\input{tables/intensity.tex}


\subsection{Promise of Inter-operation Reuse}

%In the ideal case, the arithmetic intensity increases as we reuse the operations on-chip and we reduce the memory bandwidth requirement. The difference is that we avoid counting the back and forth of tensors in and out of memory.

In a multi-operation scenario, we can increase intensity if we can avoid writing back the output of the first operation, and similarly avoid reading it in for the second. %To show how much impact this can have, in \reviewme{~\autoref{fig:ai2}} we demonstrate the ideal inter-operation reuse potential for two chains of Operations. %The first is like ML applications where a new tensor is injected in every operation by the application (i.e., the layer's weights). The second is like a solver where there are only three inputs and one output introduced by the application and intermediate tensors are reused between the Operations.

%\insertFigureScaled{aicg}{AI\textsubscript{best} for N=1, N=16 for 10 iterations of the CG loop. We compare of op-by-op and ideal reuse (perfect reuse/infinite on-chip capacity) and label op-by-op arithmetic intensity as a percentage of ideal.\vspace{0mm}}{0.9}


Without inter-operation reuse and with execution at the operation granularity, the denominator which is the number of words, always consists of three tensors for every operation. As also shown in the corresponding equation in ~\autoref{fig:ai2}, with 5 operations, the denominator includes all intermediate tensors. In the above example, the arithmetic intensity is $\approx$8 ops/word, same as that of an individual matrix multiplication in~\autoref{fig:ai1}b.

With inter-operation reuse, as~\autoref{fig:ai2} shows, the tile of the A matrix can be reused by tiles of C,D,E,F matrices as well in addition to just the B matrix, and the corresponding reuse trajectory of that tile is highlighted across different operations. Matrices $Z$,$Y$,$X$,$W$ are intermediate tensors, which need not be written to and read from the main memory and are deducted from the denominator as shown in the corresponding equation in~\autoref{fig:ai2}. We observe, an arithmetic improvement of 5x, and the resulting arithmetic intensity is $\approx$40 ops/word. This can also be seen visually since the same tile effectively is reused by 5 tensors, leading to 5x gain.
%In NN-like case where a new weight is injected in every operation, there are six inputs $A,B,C,D,E,F$ and one final output $V$, thus 7 terms in the denominator. For an iterative HPC-like example, there are only 3 inputs from the application side $A,B,C$ and there is one final output $V$ leading to four terms in the denominator thus less traffic is injected by HPC application the application compared to NN.

Thus, for a sequence of operations, the AI increases substantially if all the intermediate outputs are reused on-chip. Our next section focusses on the inter-operation reuse opportunities and dependencies, in a cascade of operations.

\end{comment}



\subsection{Limitations and Challenges of Inter-operation Pipelining}
\label{sec:nofusion}

%\TK{given this is a section on inter op reuse, i think Fig 4 should be mentioned and discussed here}

%\TK{Lets add a line saying, ``Inter-operation pipelining is a form of Inter-operation reuse".. they define what it is, and cite works that do it, \textit{before} you start bashing it .. right now we start by assuming folks know what Inter-operation pipelining is and bashing it.}

%\insertWideFigure{ai2}{Data reuse in chain of skewed GEMMs and arithmetic intensity with and without inter-operation reuse. We also represent the operations visually\vspace{-1mm}}


%Prior works have proposed inter-operation pipelining~\cite{fused,tangram,tileflow}, which is a form of inter-operation reuse. %As~\autoref{fig:ai2} shows, the part of the intermediate tensor that is produced, is consumed immediately. %\TK{explain what inter operation pipelining foes}
%However, it is a very specific form of inter-operation reuse which is much broader.
Effectively exploiting inter-operation pipelining (\autoref{sec:inter-op-pipelining}) %essentially 
has several challenges that we detail here.~\autoref{fig:no-fusion}(b) visually shows these challenges. %(shown in~\autoref{fig:no-fusion}).
%Furthermore, there are challenges to apply adjacent inter-operation pipelining for every layer, specially when we have a operands with multiple downstream consumers. \autoref{fig:no-fusion} in~\autoref{sec:introduction} visually shows these challenges.

\textbf{Challenge 1:} A delayed downstream consumer needs to use the intermediate operand. Traditional pipelining involves overwriting the previously produced tile when its no longer needed by the immediate consumer, which only works when there is no downstream consumer. Therefore these operands need to be stored in the on-chip or off-chip memory.

 \textbf{Challenge 2:} Not all layers pipeline with the next layer. For example, if a layer has large contraction (ie. lines 2 and 5 in~\autoref{alg:cg_einsum}), most of the compute is just spent on generating a usable output.

 \textbf{Challenge 3:} Once we have multiple downstream consumers, their on-chip reuse is a challenging problem given multiple reuse distances.

 \textbf{Challenge 4:} Given that the same operand has multiple consumers in a complex cascade, preserving the layout of the data in the on-chip memory is also crucial to exploit the reuse opportunity, and the scheduler needs to take that into account.




%Thus inter-operation pipelining is not simply applicable everywhere and is a small aspect of vast space of "inter-operation reuse". 
%\TK{now explicitly re-iterate our contributions -- here maybe we should mention SCORE first as identifying such inter-reuse opportunities and CHORD leveraging them}

%In the rest of this paper, we introduce the accelerator~\AccelName and demonstrate how co-designing the ~\DataflowName scheduler and the~\SpadName buffering idiom allows us to identify delayed downstream dependencies and reuse opportunities among them, without exploding the scheduling search space.


%We further discuss how our proposed buffer idiom~\SpadName and proposed scheduler~\DataflowName enable generalized operation reuse.

%For \GEMM with one large rank, if we assume that $N$ is the only large rank for all the sequence, $N\rightarrow\infty$, and consider all other ranks are equal, the arithmetic intensity without inter-operation reuse is $\frac{M}{2}$ for each case. With inter-operation reuse, the arithmetic intensity would be $\frac{5M}{2}$, increasing the intensity by 5x.% Thus we observe through these examples, that an application with low intensity individual operation sequences can significantly benefit from inter-operation reuse. 
%Next we discuss the reuse opportunities in CG application.%~\autoref{fig:aicg} shows the arithmetic intensity of 1,5 and 10 iterations of CG loop rather than 1 operation and it shows considerable improvement over multiple iterations.

%As a practical example of CG, the $AI_{best}$ plot in ~\autoref{fig:aicg} shows that by reusing the data between iterations, the complete CG kernel can 
%potentially achieve a high AI\textsubscript{best}, even if the AI\textsubscript{best} for each individual GEMM is limited. %With this, we can further improve the 'iter=1' intensity from 13.8 to 21.8 in lshp3466, from 11 to 13.5 in raefsky2 and from 13.3 to 19.9 in aft02.
%Reusing the data amongst the multiple CG iterations in~\autoref{alg:cg_operation} improves the intensity by huge amounts.
%However, this is idealistic since it does not consider buffer sizes. Instead, we propose a mapping strategy that achieves substantial benefits even with realistic buffer sizes. 

%Moreover, in the overall execution of Conjugate Gradient, sparsity does not affect the arithmetic intensity as much as the N dimension as~\autoref{fig:heatmap} shows, whether its the ideal case or an op-by-op execution. Therefore, regardless of the sparsity, achieved arithmetic intensity can be improved via inter-operation reuse.

\begin{comment}~~~~~~~~~~~~~~~\textbf{\underline{NN}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\textbf{\underline{HPC}}~~~~~~~~~~~~~~~
\vspace{-2mm}
\begin{small}
\begin{verbatim}

Z(m,n)\text{ + }=A(m,k)*B(k,n)       Z(m,n)\text{ + }=A(m,k)*B(k,n)
Y(o,n)\text{ + }=C(o,j)*Z(j,n)       Y(o,n)\text{ + }=C(o,j)*Z(j,n)
X(p,n)\text{ + }=D(p,i)*Y(i,n)       X(m,n)\text{ + }=Z(m,i)*Y(i,n)
W(q,n)\text{ + }=E(q,h)*X(h,n)       W(m,n)\text{ + }=Z(m,h)*X(h,n)
V(r,n)\text{ + }=F(r,g)*W(g,n)       V(o,n)\text{ + }=Y(o,g)*W(g,n)


\end{verbatim}
\end{small}
\vspace{-2mm}

\input{tables/intensity.tex}

For the NN example, case assuming there is no inter-operation reuse, the ideal arithmetic intensity is
\vspace{-2mm}
\begin{small}
\begin{equation}
    \frac{MNK\text{ + }ONJ\text{ + }PIN\text{ + }QHN\text{ + }RGN}{MN\text{ + }NK\text{ + }KN\text{ + }OJ\text{ + }JN\text{ + }ON\text{ + }PI\text{ + }IN\text{ + }PN\text{ + }QH\text{ + }HN\text{ + }QN\text{ + }RG\text{ + }GN\text{ + }RN}
\end{equation}
\end{small}
\vspace{-2mm}
\end{comment}






\begin{comment}
\subsection{Reuse opportunities in CG}
\subsubsection{Arithmetic Intensity of CG}

As~\autoref{sec:ai} shows, reuse opportunities within a operation are less compared to traditional DNNs due to the low arithmetic intensity. Similar to the traditional DNNs, some reuse in the GEMMs can be achieved by using temporally local register files in the PEs or spatially, through multicasting or peer-peer forwarding. Prior works like Alrescha~\cite{asgari2020alrescha} have proposed techniques to extract locality from dense regions of the sparse matrices. %\RG{word operation, tensor operator}

\subsubsection{Inter-operation Reuse Opportunities Within the CG Loop}
Inter-operation reuse can be achieved by reusing the portion of the output of the first operation immediately into the next operation. The design-space of even two of these \textit{pipelined} operations is non-trivial owing to the inter-dependence of the intra-operation dataflows~\cite{garg2021understanding}. However, Conjugate Gradient offers immense pipelining opportunities within one loop iteration of CG Algorithm in ~\autoref{alg:cg_operation}. Matrix $A$ is the input for every iteration and has to be read once. Every CG loop iteration has dynamically generated matrices $R_{prev}$, $P_{prev}$, $(R_{prev})^T$, $(P_{prev})^T$ and $X_{prev}$ as the inputs and $R_{cur}$, $P_{cur}$, $(R_{cur})^T$, $(P_{cur})^T$ and $X_{cur}$ as the outputs for the loop. Rest of the intermediate variables (or variable product expressions) are consumed within the single loop iteration. Since, the memory footprint of inputs and outputs to the loop is low, theoretically, the reuse benefit of inter-operation pipelining is high. ~\autoref{fig:aicg} 'iter=1' bar shows the $AI_{best}$ for one iteration of the full Conjugate Gradient Loop.

\subsubsection{Reuse opportunities between Multiple Iterations of the CG loop}
At an even further granularity, the theoretical reuse across iterations scales. On unrolling the CG for loop $iter$ times, the inputs to a big loop iteration are $A$, $R_{cur-iter}$, $P_{cur-iter}$, $(R_{cur-iter})^T$, $(P_{cur-iter})^T$ and $ X_{cur-iter}$ while the outputs are $R_{cur}$, $P_{cur}$, $(R_{cur})^T$, $(P_{cur})^T$ and $X_{cur}$. Since all of the variables in the CG loop are generated from previous operations and since the only input from the application side $A$ is same across all iterations, the memory accesses in an ideal case are independent of the value of $iter$. However, the number of multiplications scale linearly with $iter$. Thus maximum achievable arithmetic intensity scales linearly with the unrolling factor $iter$.

On unrolling the CG for loop $iter-1$ times, the inputs to a big unrolled loop iteration are $A$, $R^{cur-iter}$ (and transpose), $P^{cur-iter}$ (and transpose) and $ X_{cur-iter}$ while the outputs are $R_{cur}$ (and transpose), $P_{cur}$ (and transpose) and $X_{cur}$. Since all of the variables in the CG loop are generated from previous operations and since the only input from the application side $A$ is same across all iterations, the memory accesses in an ideal case are independent of the value of $iter$. However, the number of multiplications scale linearly with $iter$. Thus maximum achievable arithmetic intensity scales linearly with the unrolling factor $iter$.
The $AI_{best}$ plots in ~\autoref{fig:aicg} show that the complete Conjugate Gradient kernel can achieve a high AI\textsubscript{best}, however, AI\textsubscript{best} for an individual GEMM is limited. This measurement counts $R^T$ and $P^T$ separately from $R$ and $P$. However, with right pairs of dataflows, one can eliminate the need to read and hold a tensor and its transpose separately as we discuss in~\autoref{sec:gogeta}. With this, we can further improve the 'iter=1' intensity from 13.8 to 21.8 in lshp3466, from 11 to 13.5 in raefsky2 and from 13.3 to 19.9 in aft02.
\end{comment}

%\insertFigure{heatmap}{Variation of arithmetic intensity with N and nz\textsubscript{av} for M=1000000 for SpMM, op-by-op execution of 10 iterations and ideal case for 10 iterations. Green line represents the maximum realistic value of N in CG.\vspace{-2mm}}