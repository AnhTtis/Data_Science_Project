
%\input{algorithm/looporder.tex}
\section{GOGETA Mapping}
\label{sec:gogeta}
\vspace{-1mm}
In this section, we determine the dataflows and tile sizes of individual operations, given that the dependency graph is marked with inter-operation reuse patterns. Through this, we determine the entire mapping which we call~\DataflowName.

%\input{tables/inter_equations.tex}
%\insertWideFigure{inter-op}{(a) Algorithm for dynamic determination Inter-operation reuse opportunities for every edge in the DAG of tensor operations and the output of the algorithm shows colored edges. Letters inside the node denote dominance and 'bal' means all ranks are moderately big. Note that first tensor is M dominant since we compress the K rank (CSR format). (b) Algorithm to determine the loop orders of the tensor operations to exploit reuse.}




%\insertWideFigure{spacetime-dist}{Space-time representation of an iteration of Conjugate Gradient with loop orders. The tensors are split across the node by the dominant rank and the sub-tensors are pipelined within a cluster. This is the set of loop orders with zero swizzle penalty.}
\insertFigureScaled{spacetime}{Example space-time representation of an iteration of CG loop orders with zero swizzle penalty. In the top chart, the overall compute is divided into multiple parts each executing one tensor. In the bottom chart, the tensors are split across the node by the dominant rank and the sub-tensors are pipelined within a cluster. See~\autoref{alg:cg_einsum} for rank names. 
%$M$ represents the large rank, $N$ ($\equiv N'$) represents the small ranks. $K$ represents contractions of size $M$ and $J$ represents contractions of size $N$.
\vspace{-3mm}}{0.85}
%\vspace{-2mm}

\insertFigure{tornado}{Data organization in the scratchpad. Top shows scratchpad filling up without replacement. The part that could not fit is sent to DRAM. Bottom part shows that after queue is filled, the tail of $X$ is evicted and next element of $R$ is enqueued at the head to fill the space.\vspace{-2mm}}

\subsection{Loop order and Layouts}
\label{sec:loop}

In this section, we systematically determine the loop orders of the individual tensors after the inter-operation reuse patterns have been marked as a result of~\autoref{alg:inter-op}.  We determine the loop orders to make sure that we are able to pipeline wherever the inter-operation reuse pattern is pipelineable, which is important for low AI individual operations, and we retry with a new set of loop orders if a pipelineable edge cannot be pipelined due to the loop order pair of the source and the destination nodes.
This constraint could be relaxed into a penalty rather than having to retry for workloads which have higher intensity GEMMs.

For an example pair of Einsums $Z_{m,n}=\sum_{k}A_{m,k}*B_{k,n}$ and ${W_{o,n}=\sum_{j}C_{o,j}*Z_{j,n}}$. The conditions for pipelining a loop order pair are as follows-
%\vspace{-2mm}
\squishlist
\item The edge connecting the nodes has a {pipelineable} inter-operation pattern.
\item The source node has an uncontracted rank as the outermost loop. Having contraction ($K$) as the outermost loop implies that the complete sum that is usable by the next operation is generated at the very end.
\item The destination node has a shared rank as the outermost loop. Having unshared rank ($O$) as the outermost loop implies that the shared tensor $Z$ is needed all over again when $o$ changes. But the idea of pipelining is to slice the intermediate tensor and consume it so that its not needed.
\item The shared tensor $Z$ is not swizzled since the same portion of data that is produced, should be consumed. %Thus no swizzling at that edge is allowed.
\squishend


The algorithm ensures that each of the edges with {adjacent pipeline} and {pipeline\_with\_hold} patterns have compatible loop orders. For {parallel multicasting}, the loop orders of all consumer nodes should be the same.
%TODO RE-ADD FOR CAMERA READY
%The algorithm assigns the dataflow to the node with no assignment yet, based on pipelining and multicasting conditions.
%It also checks whether those conditions are met in case the dataflow was already assigned by some other incoming edge. 
%It returns a retry if a pipelineable edge cannot be pipelined due to a certain loop order pair.
In the {Pipeline\_with\_writeback} and Sequential case, it increases the penalty in case a tensor is swizzled. If the swizzle penalty exceeds a threshold $\epsilon$, then the algorithm retries. In the end, a set of loop orders with all or maximum possible pipeline friendly edges and minimum swizzle penalty is selected. %This threshold, could start from zero and gradually increase if we don't find a set of loop orders.

For CG, we determine a set of loop orders (shown in~\autoref{fig:spacetime}) 
%These 
%orders 
%have 
with zero swizzle penalty with pipeline conditions satisfied. %and parallel\_multicasted operations having same order wrt the multicasted tensor $\Lambda$ which is $MKN$ ie $P-$stationary and $S$-stationary respectively.

\subsection{Tiling Strategy}
\label{sec:tiling}

\textbf{Individual GEMMs}: There are limited options for tiling individual low intensity \GEMM because the non-dominant dimensions are small. In fact such GEMMs are often able to achieve the best possible data reuse with memory accesses $MK+KN+MN$ since the tensor made of the non-dominant ranks (for e.g., $\Lambda$ in line 3 and 4 of~\autoref{alg:cg_einsum} and $\Gamma$ in line 5) can practically fit inside a portion of the Register File (RF) or parallelized across few PEs which means that the smaller tensor can just be accessed continuously from the RF while a tile of the large tensor is stationary. However, as discussed in~\autoref{sec:ai}, even the best intensity is low for \GEMM.

\textbf{Considering multiple GEMMs}: With pipelining and parallel\_multicast, there are different ways in which data can be communicated spatially. Assuming multiple clusters of sub-accelerators, one could pipeline by having the whole tensor be local and occupy the two halves of compute entirely. Prior works like~\cite{simba,tangram} have divided the substrate into large chunks for each tensor just like~\autoref{fig:spacetime} top shows. The major disadvantage of that would be a significant sized tensor being communicated through the inter-cluster NoC which usually has lower bandwidth than the intra-cluster NoC. On the other hand, for skewed GEMMs, which do not have significant intra-operation reuse anyway, we propose to slice the dominant rank on multiple clusters instead and have the tensors be pipelined within a cluster as~\autoref{fig:spacetime} bottom shows. When the dominant rank is split ($M$ for example in line 4 of~\autoref{alg:cg_einsum}) the chunks would share the small tensor formed by non-dominating ranks($\Lambda$) and $\Gamma$. Broadcasting $\Lambda$ and reducing $\Gamma$ on the inter-cluster NoC has orders of magnitude less communication on the inter-cluster NoC compared to communicating $R$ between clusters.

\reviewmehpca{As an example, if we consider operations 4 and 5 in CG~(\autoref{alg:cg_einsum}), the total traversals on the inter-cluster NoC, obtained by dividing the total compute into contiguous chunks and mapping operations on them (\autoref{fig:spacetime} up) is $sizeof(R)$ which is $M\times N$ even if these operations are allocated such that they are 1 hop apart which leads to considerable delay and energy since inter-cluster bandwidth ($\equiv$ inter-chiplet) is low compared to in-cluster bandwidth. Instead, if we use considered the tiling strategy proposed in~\autoref{sec:tiling} and where pipelining happens inside the cluster and $M$ rank is sliced across the clusters (\autoref{fig:spacetime} down), maximum inter-cluster link traversals are $sizeof(\Lambda)\times Broadcast~traversals$ + $sizeof(\Gamma)\times Reduce~traversals$ which is $2\times N\times N\times 15$ ($N\times N$ data moved through at most 15 links for broadcast and reduction) for 16 clusters. Given that M is the dimensions that scale with larger workloads, this tiling strategy can prevent proportional increase of communication traffic in potentially large systems with even off-chip networks.}



\subsection{Custom Scratchpad Management}
\label{sec:tornado}
\vspace{-1mm}
There are various scenarios regarding the amount of tensors the scratchpad can hold ranging from a fraction to multiple tensors depending on the workload size and the architecture. In \DataflowName, we choose the loop orders to minimize the swizzling of ranks in a tensor and hence the tensor will be read in the same order as it was written. Due to this, we write the tensor in the queue order and if the scratchpad is full, we send the remaining portion of that tensor directly to DRAM as~\autoref{fig:tornado} shows. We call this \textsc{Qpad} for the purpose of evaluation. This makes sure that we access the portion of the tensor in the scratchpad first rather than replacing it by the later part of the tensor and again reading the earlier part. This is in contrast to the various buffer management schemes that keep the recently accessed data. If the tensor fits, we repeat this for multiple operations. We read the tensor at head.

While this makes sure that we minimize the off-chip traffic for that particular tensor, there is still a chance that a particular tensor which is not reused until later (for example, $X$ in line 3 of~\autoref{alg:cg_einsum} which is used in line 3 of the next iteration) is written to scratchpad at the expense of a future tensor that is used before the reuse of the previous tensor (for example $R$ in line 4 which is reused in lines 5 and 7 of the same iteration). In that case, based on the reuse distance, we prioritize $R$ and evict out a portion of $X$ to make room for $R$ by evicting the later elements of $X$ first. Then $R$ is read at its head. This is the first work to the best of our knowledge that considers reuse distance and frequency of a tensor in the dependency graph and proposes data organization strategies at a tensor granularity. We call this \textsc{FDpad} (Frequency and Distance).

If 3 and 4 were executed in parallel as~\autoref{fig:spacetime} shows, $R$ and $X$ would occupy equal halves in the second step and the later part of $R$ will eventually replace the tail of earlier part of $X$ and the later part of $X$ would be sent straight to DRAM, resulting in the same proportions of $R$ and $X$.


























\begin{comment}
\subsection{Loop Optimizations for operation pipelining}

For operation pipelining to work, the order of production and the order of consumption of data should be the same and the contracted rank should not be the outermost loop of the first operation and the rank exclusive to second operation should not be the outermost loop of the second operation~\cite{garg2021understanding}.
For example, between $S=AP$ and $var=P^TS$, matrix $S$ is $M\times N$ for the first operation and $K\times N$ for the second operation. $MNK\xbottomarrow{}KNM$ is valid loop order pair since the dimensions corresponding to S are in the outer loop and are in the same row major order. $KNM\xbottomarrow{}MNK$ is an invalid loop order pair for pipelining since contracted rank on the outermost loop of the first operation implies that the actual output of the first operation is produced in the end and the outermost M on second operation implies that the all outputs of the first operation need to be available every time M changes in the second operation, negating the benefit of pipelining.

Moreover, splitting the higher rank enables a finer grained pipelining, thus this work considers the biggest rank to always be the outermost loop as shown in ~\autoref{fig:spacetime} loop orders.

Moreover, balancing the production rate and consumption rate is necessary for operation pipelining. For example, between $S=AP$ and $var=P^TS$, matrix $S$ is $M\times N$ for the first operation and $K\times N$ for the second operation, tile sizes $M0_{operation 1}=K0_{operation 2}$ and $N0_{operation 1}=N0_{operation 2}$ will ensure fine grained operation pipelining without the need to take LCM~\cite{garg2021understanding}.

For a continuous pipelining between the tensor operations, the biggest ranks should ideally be the uncontracted ranks since the high contracted rank requires significant portion of compute to just produce the output in the first place.
However, in the Conjugate Gradient Algorithm, operations 2 and 5 have contracted rank (K) as the dominant rank due to which they do not provide benefit of fusing further. 
Therefore, for the loop of  we fuse lines (1,2a($var={P_{prev}}^TS_{prev}$)); (4,5) and parallelize 3 and 4 since the output of operation 2, $\alpha$, branches into operations 3 and 4 and can be multicasted. The space time representation of kernels is shown in ~\autoref{fig:spacetime}. Please note that operations 2b ($\alpha_{prev}=var^{-1}\gamma_{prev}$) and 6 have all small ranks thus have negligible compute and memory footprint compared to other operations. We discuss the individual phase dataflow in more detail along with the accelerator architecture in ~\autoref{sec:arch}.

\section{Generalizing Inter-Operation Reuse}

\subsection{Limitations of treaditional operation pipelining}
%\RG{move in the intro, give a hint of next}

Operation pipelining can significantly reduce the data movement between multiple operations in the following situations:
\squishlist
\item For applications with tensor operations where the all the matrix ranks are large enough specially the non-contracted rank. For example, in Deep Learning, batch size is a non-contracted rank and the operation pipelining can be done by splitting batches into stages. Also, deep learning has relatively large number of input and output features.

\item For applications where the arithmetic intensity of tensors is low and the dominant rank is \textit{always} the non-contracted rank.

However, operations with contracted rank as the dominant rank do not result in significant data movement reduction due to operation pipelining since a significant number of operations and data movement is needed to just generate the output matrix. Examples of this include operations 2a and 5 of the Conjugate Gradient algorithm.

%\insertFigure{dfg-analysis}{Tensor dependency graphs of some applications with fusion opportunities. Letters inside the node represent the dominant rank in case of a low intensity operation. 'Bal' means that all ranks are somewhat large. Please note that in a) annd b), we assume that the first operation follows the CSR format where MK is the sparse matrix and K rank is compressed, which makes M the dominant rank.}

~\autoref{fig:inter-op} shows examples of applications where \textit{traditional operation pipelining} works and applications where it does not work. Conjugate Gradient does not offer benefit of pipelining the entire graph because of operations with dominating contracted ranks. A single layer of GNNs supports pipelining since M is the dominant rank in both the operations assuming sparse phase is executed first. Similarly, ResNet allows pipelining of all the operations inside the residual block, this is typically done by batch since batches are non-contracted ranks in all the operations. Prior works like TANGRAM~\cite{tangram} and SIMBA~\cite{simba} pipeline operations in the block. In certain cases, the arithmetic intensity of an operation might be high such that it offers better reuse opportunities within the operation.

Therefore, based on (1) tensor dependency graph (2) Shape and Arithmetic Intensity of the operations, we determine whether pipelining is required and whether traditional pipelining is possible or if writeback is required.

\squishend
\subsection{Beyond Pipelining: Tensor Reuse Distance}



\end{comment}


\reviewme{\subsection{Architectural Support}
\label{sec:arch_implications}

\autoref{fig:spacc} shows the architecture of a flexible spatial accelerator~\cite{eyeriss2016isca,plasticine,kwon2019understanding,flexagon,kwon2018maeri,sigma} that we use in our evaluations.
We describe the key features needed in an accelerator to be able to  run GOGETA mappings. 

\textbf{Flexible Dataflow.} 
GOGETA relies on an accelerator being able to run individual GEMM operators, both dense and sparse, with any dataflow (i.e., loop orders and parallelism) and diverse tile sizes, as discussed earlier in \autoref{sec:loop} and \autoref{sec:tiling} respectively. For instance, CG contains a contracted dominant GEMM which requires tiling both inputs and large parallel reduction, and an uncontracted dominant GEMM which requires tiling one input and one output and large parallel input broadcast.
%
Several prior accelerators~\cite{eyeriss2,kwon2018maeri,sigma,flexagon} have demonstrated the required support within the interconnect and scratchpads to run diverse dataflows. Specifically, support for broadcast and reductions within the interconnect, and programmable address generators for the scratchpad can allow any dataflow to be orchestrated~\cite{flexion}. 
Further, leveraging fully addressable buffers allows for picking diverse tile sizes. 
%
In this work, we select Flexagon~\cite{flexagon} as the baseline accelerator, as it is the most recently published work in this line of research, and can in principal, support flexible dataflows for both dense and sparse GEMM operators. However, recall that, Flexagon, and almost all prior works, run the individual operators in op-by-op manner, as discussed before in \autoref{sec:background}.

\textbf{Addressable Scratchpad.} 
Beyond the dataflow (\autoref{sec:loop}) and tile sizes (\autoref{sec:tiling}), GOGETA's proposed scratchpad management techniques can either be implemented in HW via custom buffers, or implemented in SW over a fully programmable scratchpad. While the former is certainly expected to be more area efficient, it limits the applicability of the accelerator to only support a certain set of workloads that may use \DataflowNamenospace. In this work, we model the latter. 
Several commercial accelerators today~\cite{tpuv4,cerebras,sambanova}
%like TPUv4~\cite{tpuv4}, Cerebras\cite{cerebras}, SambaNova~\cite{sambanova} 
provide such programmable scratchpads already. \textsc{Qpad} can be implemented by writing to consecutive addresses until the SRAM is full, and implement the \textsc{FDpad} strategy by overwriting on the addresses where the previous tensor needs to be replaced.
}
%{\color{red} @Raveesh -- what does ``end" mean in the prev sentence. Might be good to elaborate a bit more}

\insertFigure{spacc}{Cannonical spatial accelerator. We focus on one compute node. A cluster could be a sub-accelerator~\cite{kwon2021heterogeneous} or chiplet~\cite{simba}.\vspace{-2mm}}