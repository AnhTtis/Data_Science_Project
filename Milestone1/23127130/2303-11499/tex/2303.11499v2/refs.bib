@article{naumov2020zion,
  title={Deep Learning Training in Facebook Data Centers: Design of Scale-up and Scale-out Systems},
  author={Naumov, Maxim and Kim, John and Mudigere, Dheevatsa and Sridharan, Srinivas and Wang, Xiaodong and Zhao, Whitney and Yilmaz, Serhat and Kim, Changkyu and Yuen, Hector and Ozdal, Mustafa and others},
  journal={arXiv preprint arXiv:2003.09518},
  year={2020}
}

@INPROCEEDINGS{isosceles,
  author={Yang, Yifan and Emer, Joel S. and Sanchez, Daniel},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={ISOSceles: Accelerating Sparse CNNs through Inter-Layer Pipelining}, 
  year={2023},
  volume={},
  number={},
  pages={598-610},
  keywords={Costs;Computer architecture;Dynamic scheduling;Pipeline processing},
  doi={10.1109/HPCA56546.2023.10071080}}


@INPROCEEDINGS{xlayer,
  author={Vedula, Naveen and Hojabr, Reza and Khonsari, Ahmad and Shriraman, Arrvindh},
  booktitle={2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)}, 
  title={X-Layer: Building Composable Pipelined Dataflows for Low-Rank Convolutions}, 
  year={2021},
  volume={},
  number={},
  pages={103-115},
  keywords={Buildings;Random access memory;Tools;System-on-chip;Space exploration;Parallel architectures;Hardware acceleration;Neural Networks;Low rank convolutions;dataflow;cross layer;pipelining},
  doi={10.1109/PACT52795.2021.00015}}



@INPROCEEDINGS{fused,
  author={Alwani, Manoj and Chen, Han and Ferdman, Michael and Milder, Peter},
  booktitle={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Fused-layer CNN accelerators}, 
  year={2016},
  volume={},
  number={},
  pages={1-12},
  keywords={System-on-chip;Bandwidth;Random access memory;Neural networks;Convolution;Field programmable gate arrays;Data transfer},
  doi={10.1109/MICRO.2016.7783725}}


@article{roofline,
author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
title = {Roofline: An Insightful Visual Performance Model for Multicore Architectures},
year = {2009},
issue_date = {April 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/1498765.1498785},
doi = {10.1145/1498765.1498785},
abstract = {The Roofline model offers insight on how to improve the performance of software and hardware.},
journal = {Commun. ACM},
month = {apr},
pages = {65–76},
numpages = {12}
}

@inproceedings{10.1145/3489048.3530974,
author = {Kao, Sheng-Chun and Kwon, Hyoukjun and Pellauer, Michael and Parashar, Angshuman and Krishna, Tushar},
title = {A Formalism of DNN Accelerator Flexibility},
year = {2022},
isbn = {9781450391412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489048.3530974},
doi = {10.1145/3489048.3530974},
booktitle = {Abstract Proceedings of the 2022 ACM SIGMETRICS/IFIP PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {53–54},
numpages = {2},
keywords = {accelerator, dnn workloads, hardware flexibility},
location = {Mumbai, India},
series = {SIGMETRICS/PERFORMANCE '22}
}

@inproceedings{isca-pip,
author = {Cai, Jingwei and Wei, Yuchen and Wu, Zuotong and Peng, Sen and Ma, Kaisheng},
title = {Inter-Layer Scheduling Space Definition and Exploration for Tiled Accelerators},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589048},
doi = {10.1145/3579371.3589048},
abstract = {With the continuous expansion of the DNN accelerator scale, inter-layer scheduling, which studies the allocation of computing resources to each layer and the computing order of all layers in a DNN, plays an increasingly important role in maintaining a high utilization rate and energy efficiency of DNN inference accelerators. However, current inter-layer scheduling is mainly conducted based on some heuristic patterns. The space of inter-layer scheduling has not been clearly defined, resulting in significantly limited optimization opportunities and a lack of understanding on different inter-layer scheduling choices and their consequences.To bridge the gaps, we first propose a uniform and systematic notation, the Resource Allocation Tree (RA Tree), to represent different inter-layer scheduling schemes and depict the overall space of inter-layer scheduling. Based on the notation, we then thoroughly analyze how different inter-layer scheduling choices influence the performance and energy efficiency of an accelerator step by step. Moreover, we show how to represent existing patterns in our notation and analyze their features. To thoroughly explore the space of the inter-layer scheduling for diverse tiled accelerators and workloads, we develop an end-to-end and highly-portable scheduling framework, SET. Compared with the state-of-the-art (SOTA) open-source Tangram framework, SET can, on average, achieves 1.78\texttimes{} performance improvement and 13.2\% energy cost reduction simultaneously. Moreover, the SET framework will be open-sourced.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {13},
numpages = {17},
keywords = {scheduling, inter-layer scheduling, neural networks, tiled accelerators},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{tileflow,
    author = {Size Zheng and Siyuan Chen and Siyuan Gao and Liancheng Jia and Guangyu Sun and Runsheng Wang and Yun Liang},
    title = {TileFlow: A Framework for Modeling Fusion Dataflow via Tree-based Analysis},
    booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
    year = {2023}
}

@inproceedings{tailors,
    author = {Zi Yu Xue and Yannan Nellie Wu and Joel Emer and Vivienne Sze},
    title = {Tailors: Accelerating Sparse Tensor Algebra by Overbooking Buffer Capacity
},
   booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
    year = {2023}
}

@INPROCEEDINGS{patch,
  author={Clemons, Jason and Cheng, Chih-Chi and Frosio, Iuri and Johnson, Daniel and Keckler, Stephen W.},
  booktitle={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={A patch memory system for image processing and computer vision}, 
  year={2016},
  volume={},
  number={},
  pages={1-13},
  doi={10.1109/MICRO.2016.7783754}}


@inproceedings{10.1145/1815961.1815971,
author = {Jaleel, Aamer and Theobald, Kevin B. and Steely, Simon C. and Emer, Joel},
title = {High Performance Cache Replacement Using Re-Reference Interval Prediction (RRIP)},
year = {2010},
isbn = {9781450300537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1815961.1815971},
doi = {10.1145/1815961.1815971},
abstract = {Practical cache replacement policies attempt to emulate optimal replacement by predicting the re-reference interval of a cache block. The commonly used LRU replacement policy always predicts a near-immediate re-reference interval on cache hits and misses. Applications that exhibit a distant re-reference interval perform badly under LRU. Such applications usually have a working-set larger than the cache or have frequent bursts of references to non-temporal data (called scans). To improve the performance of such workloads, this paper proposes cache replacement using Re-reference Interval Prediction (RRIP). We propose Static RRIP (SRRIP) that is scan-resistant and Dynamic RRIP (DRRIP) that is both scan-resistant and thrash-resistant. Both RRIP policies require only 2-bits per cache block and easily integrate into existing LRU approximations found in modern processors. Our evaluations using PC games, multimedia, server and SPEC CPU2006 workloads on a single-core processor with a 2MB last-level cache (LLC) show that both SRRIP and DRRIP outperform LRU replacement on the throughput metric by an average of 4\% and 10\% respectively. Our evaluations with over 1000 multi-programmed workloads on a 4-core CMP with an 8MB shared LLC show that SRRIP and DRRIP outperform LRU replacement on the throughput metric by an average of 7\% and 9\% respectively. We also show that RRIP outperforms LFU, the state-of the art scan-resistant replacement algorithm to-date. For the cache configurations under study, RRIP requires 2X less hardware than LRU and 2.5X less hardware than LFU.},
booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
pages = {60–71},
numpages = {12},
keywords = {replacement, scan resistance, shared cache, thrashing},
location = {Saint-Malo, France},
series = {ISCA '10}
}

@inproceedings{feldmann2024azul,
  title={Azul: An Accelerator for Sparse Iterative Solvers Leveraging Distributed On-Chip Memory},
  author={Feldmann, Axel and Golden, Courtney and Yang, Yifan and Emer, Joel S and Sanchez, Daniel},
  booktitle={2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={643--656},
  year={2024},
  organization={IEEE}
}

@inproceedings{rocki,
author = {Rocki, Kamil and Van Essendelft, Dirk and Sharapov, Ilya and Schreiber, Robert and Morrison, Michael and Kibardin, Vladimir and Portnoy, Andrey and Dietiker, Jean Francois and Syamlal, Madhava and James, Michael},
title = {Fast stencil-code computation on a wafer-scale processor},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {The performance of CPU-based and GPU-based systems is often low for PDE codes, where large, sparse, and often structured systems of linear equations must be solved. Iterative solvers are limited by data movement, both between caches and memory and between nodes. Here we describe the solution of such systems of equations on the Cerebras Systems CS-1, a wafer-scale processor that has the memory bandwidth and communication latency to perform well. We achieve 0.86 PFLOPS on a single wafer-scale system for the solution by BiCGStab of a linear system arising from a 7-point finite difference stencil on a 600 \texttimes{} 595 \texttimes{} 1536 mesh, achieving about one third of the machine's peak performance. We explain the system, its architecture and programming, and its performance on this problem and related problems. We discuss issues of memory capacity and floating point precision. We outline plans to extend this work towards full applications.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {58},
numpages = {14},
keywords = {algorithms for numerical methods and algebraic systems, computational fluid dynamics and mechanics, multi-processor architecture and microarchitecture},
location = {Atlanta, Georgia},
series = {SC '20}
}

@article{rrip,
author = {Jaleel, Aamer and Theobald, Kevin B. and Steely, Simon C. and Emer, Joel},
title = {High Performance Cache Replacement Using Re-Reference Interval Prediction (RRIP)},
year = {2010},
issue_date = {June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/1816038.1815971},
doi = {10.1145/1816038.1815971},
abstract = {Practical cache replacement policies attempt to emulate optimal replacement by predicting the re-reference interval of a cache block. The commonly used LRU replacement policy always predicts a near-immediate re-reference interval on cache hits and misses. Applications that exhibit a distant re-reference interval perform badly under LRU. Such applications usually have a working-set larger than the cache or have frequent bursts of references to non-temporal data (called scans). To improve the performance of such workloads, this paper proposes cache replacement using Re-reference Interval Prediction (RRIP). We propose Static RRIP (SRRIP) that is scan-resistant and Dynamic RRIP (DRRIP) that is both scan-resistant and thrash-resistant. Both RRIP policies require only 2-bits per cache block and easily integrate into existing LRU approximations found in modern processors. Our evaluations using PC games, multimedia, server and SPEC CPU2006 workloads on a single-core processor with a 2MB last-level cache (LLC) show that both SRRIP and DRRIP outperform LRU replacement on the throughput metric by an average of 4\% and 10\% respectively. Our evaluations with over 1000 multi-programmed workloads on a 4-core CMP with an 8MB shared LLC show that SRRIP and DRRIP outperform LRU replacement on the throughput metric by an average of 7\% and 9\% respectively. We also show that RRIP outperforms LFU, the state-of the art scan-resistant replacement algorithm to-date. For the cache configurations under study, RRIP requires 2X less hardware than LRU and 2.5X less hardware than LFU.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {60–71},
numpages = {12},
keywords = {replacement, scan resistance, thrashing, shared cache}
}



@inproceedings{gamma-sparse,
author = {Zhang, Guowei and Attaluri, Nithya and Emer, Joel S. and Sanchez, Daniel},
title = {Gamma: Leveraging Gustavson’s Algorithm to Accelerate Sparse Matrix Multiplication},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446702},
doi = {10.1145/3445814.3446702},
abstract = {Sparse matrix-sparse matrix multiplication (spMspM) is at the heart of a wide range of scientific and machine learning applications. spMspM is inefficient on general-purpose architectures, making accelerators attractive. However, prior spMspM accelerators use inner- or outer-product dataflows that suffer poor input or output reuse, leading to high traffic and poor performance. These prior accelerators have not explored Gustavson's algorithm, an alternative spMspM dataflow that does not suffer from these problems but features irregular memory access patterns that prior accelerators do not support.  We present GAMMA, an spMspM accelerator that uses Gustavson's algorithm to address the challenges of prior work. GAMMA performs spMspM's computation using specialized processing elements with simple high-radix mergers, and performs many merges in parallel to achieve high throughput. GAMMA uses a novel on-chip storage structure that combines features of both caches and explicitly managed buffers. This structure captures Gustavson's irregular reuse patterns and streams thousands of concurrent sparse fibers (i.e., lists of coordinates and values for rows or columns) with explicitly decoupled data movement. GAMMA features a new dynamic scheduling algorithm to achieve high utilization despite irregularity. We also present new preprocessing algorithms that boost GAMMA's efficiency and versatility. As a result, GAMMA outperforms prior accelerators by gmean 2.1x, and reduces memory traffic by gmean 2.2x and by up to 13x.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {687–701},
numpages = {15},
keywords = {high-radix merge, sparse linear algebra, Gustavson's algorithm, accelerator, sparse matrix multiplication, explicit data orchestration, data movement reduction},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@INPROCEEDINGS{stash,
  author={Komuravelli, Rakesh and Sinclair, Matthew D. and Alsop, Johnathan and Huzaifa, Muhammad and Kotsifakou, Maria and Srivastava, Prakalp and Adve, Sarita V. and Adve, Vikram S.},
  booktitle={2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)}, 
  title={Stash: Have your scratchpad and cache it too}, 
  year={2015},
  volume={},
  number={},
  pages={707-719},
  doi={10.1145/2749469.2750374}}


@INPROCEEDINGS{genetic-pipeline,
  author={Karl, Sebastian and Symons, Arne and Fasfous, Nael and Verhelst, Marian},
  booktitle={2023 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)}, 
  title={Genetic Algorithm-based Framework for Layer-Fused Scheduling of Multiple DNNs on Multi-core Systems}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  doi={10.23919/DATE56975.2023.10137070}}

@article{cacti,
author = {Balasubramonian, Rajeev and Kahng, Andrew B. and Muralimanohar, Naveen and Shafiee, Ali and Srinivas, Vaishnav},
title = {CACTI 7: New Tools for Interconnect Exploration in Innovative Off-Chip Memories},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3085572},
doi = {10.1145/3085572},
abstract = {Historically, server designers have opted for simple memory systems by picking one of a few commoditized DDR memory products. We are already witnessing a major upheaval in the off-chip memory hierarchy, with the introduction of many new memory products—buffer-on-board, LRDIMM, HMC, HBM, and NVMs, to name a few. Given the plethora of choices, it is expected that different vendors will adopt different strategies for their high-capacity memory systems, often deviating from DDR standards and/or integrating new functionality within memory systems. These strategies will likely differ in their choice of interconnect and topology, with a significant fraction of memory energy being dissipated in I/O and data movement. To make the case for memory interconnect specialization, this paper makes three contributions.First, we design a tool that carefully models I/O power in the memory system, explores the design space, and gives the user the ability to define new types of memory interconnects/topologies. The tool is validated against SPICE models, and is integrated into version 7 of the popular CACTI package. Our analysis with the tool shows that several design parameters have a significant impact on I/O power.We then use the tool to help craft novel specialized memory system channels. We introduce a new relay-on-board chip that partitions a DDR channel into multiple cascaded channels. We show that this simple change to the channel topology can improve performance by 22\% for DDR DRAM and lower cost by up to 65\% for DDR DRAM. This new architecture does not require any changes to DIMMs, and it efficiently supports hybrid DRAM/NVM systems.Finally, as an example of a more disruptive architecture, we design a custom DIMM and parallel bus that moves away from the DDR3/DDR4 standards. To reduce energy and improve performance, the baseline data channel is split into three narrow parallel channels and the on-DIMM interconnects are operated at a lower frequency. In addition, this allows us to design a two-tier error protection strategy that reduces data transfers on the interconnect. This architecture yields a performance improvement of 18\% and a memory power reduction of 23\%.The cascaded channel and narrow channel architectures serve as case studies for the new tool and show the potential for benefit from re-organizing basic memory interconnects.},
journal = {ACM Trans. Archit. Code Optim.},
month = {jun},
articleno = {14},
numpages = {25},
keywords = {DRAM, Memory, NVM, interconnects, tools}
}

@inproceedings{tpuv4,
author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589350},
doi = {10.1145/3579371.3589350},
abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are &lt;5\% of system cost and &lt;3\% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x--7x yet use only 5\% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of ~60\% of peak FLOPS/second. For similar sized systems, it is ~4.3x--4.5x faster than the Graphcore IPU Bow and is 1.2x--1.7x faster and uses 1.3x--1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~2--6x less energy and produce ~20x less CO2e than contemporary DSAs in typical on-premise data centers.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {82},
numpages = {14},
keywords = {energy, reconfigurable, domain specific architecture, carbon emissions, optical interconnect, machine learning, large language model, GPU, TPU, CO2 equivalent emissions, warehouse scale computer, embeddings, supercomputer, power usage effectiveness, IPU},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@article{flexion,
author = {Kao, Sheng-Chun and Kwon, Hyoukjun and Pellauer, Michael and Parashar, Angshuman and Krishna, Tushar},
title = {A Formalism of DNN Accelerator Flexibility},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/3547353.3530974},
doi = {10.1145/3547353.3530974},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jul},
pages = {53–54},
numpages = {2},
keywords = {accelerator, hardware flexibility, dnn workloads}
}



@inproceedings{flexagon,
author = {Mu\~{n}oz-Mart\'{\i}nez, Francisco and Garg, Raveesh and Pellauer, Michael and Abell\'{a}n, Jos\'{e} L. and Acacio, Manuel E. and Krishna, Tushar},
title = {Flexagon: A Multi-Dataflow Sparse-Sparse Matrix Multiplication Accelerator for Efficient DNN Processing},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582069},
doi = {10.1145/3582016.3582069},
abstract = {Sparsity is a growing trend in modern DNN models. Existing Sparse-Sparse Matrix Multiplication (SpMSpM) accelerators are tailored to a particular SpMSpM dataflow (i.e., Inner Product, Outer Product or Gustavson's), which determines their overall efficiency. We demonstrate that this static decision inherently results in a suboptimal dynamic solution. This is because different SpMSpM kernels show varying features (i.e., dimensions, sparsity pattern, sparsity degree), which makes each dataflow better suited to different data sets. In this work we present Flexagon, the first SpMSpM reconfigurable accelerator that is capable of performing SpMSpM computation by using the particular dataflow that best matches each case. Flexagon accelerator is based on a novel Merger-Reduction Network (MRN) that unifies the concept of reducing and merging in the same substrate, increasing efficiency. Additionally, Flexagon also includes a new L1 on-chip memory organization, specifically tailored to the different access characteristics of the input and output compressed matrices. Using detailed cycle-level simulation of contemporary DNN models from a variety of application domains, we show that Flexagon achieves average performance benefits of 4.59x, 1.71x, and 1.35x with respect to the state-of-the-art SIGMA-like, SpArch-like and GAMMA-like accelerators (265%, 67%, and 18%, respectively, in terms of average performance/area efficiency).},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {252–265},
numpages = {14},
keywords = {Memory Hierarchy, Merger-Reduction Network, Dataflow, Deep Neural Network Accelerators, Sparse-Sparse Matrix Multiplication},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@article{wang2020gnnadvisor,
  title={GNNAdvisor: An Efficient Runtime System for GNN Acceleration on GPUs},
  author={Wang, Yuke and Feng, Boyuan and Li, Gushu and Li, Shuangchen and Deng, Lei and Xie, Yuan and Ding, Yufei},
  journal={arXiv preprint arXiv:2006.06608},
  year={2020}
}

@INPROCEEDINGS{timeloop,  author={Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A. and Mukkara, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W. and Emer, Joel},  booktitle={2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},   title={Timeloop: A Systematic Approach to DNN Accelerator Evaluation},   year={2019},  volume={},  number={},  pages={304-315},  doi={10.1109/ISPASS.2019.00042}}

@article{ProfDallyTalk,
author = {Dally, William J. and Turakhia, Yatish and Han, Song},
title = {Domain-Specific Hardware Accelerators},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3361682},
doi = {10.1145/3361682},
abstract = {DSAs gain efficiency from specialization and performance from parallelism.},
journal = {Commun. ACM},
month = jun,
pages = {48–57},
numpages = {10}
}

@inproceedings{eyeriss2016isca,
author = {Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
title = {Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks},
year = {2016},
isbn = {9781467389471},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2016.40},
doi = {10.1109/ISCA.2016.40},
abstract = {Deep convolutional neural networks (CNNs) are widely used in modern AI systems for their superior accuracy but at the cost of high computational complexity. The complexity comes from the need to simultaneously process hundreds of filters and channels in the high-dimensional convolutions, which involve a significant amount of data movement. Although highly-parallel compute paradigms, such as SIMD/SIMT, effectively address the computation requirement to achieve high throughput, energy consumption still remains high as data movement can be more expensive than computation. Accordingly, finding a dataflow that supports parallel processing with minimal data movement cost is crucial to achieving energy-efficient CNN processing without compromising accuracy.In this paper, we present a novel dataflow, called row-stationary (RS), that minimizes data movement energy consumption on a spatial architecture. This is realized by exploiting local data reuse of filter weights and feature map pixels, i.e., activations, in the high-dimensional convolutions, and minimizing data movement of partial sum accumulations. Unlike dataflows used in existing designs, which only reduce certain types of data movement, the proposed RS dataflow can adapt to different CNN shape configurations and reduces all types of data movement through maximally utilizing the processing engine (PE) local storage, direct inter-PE communication and spatial parallelism. To evaluate the energy efficiency of the different dataflows, we propose an analysis framework that compares energy cost under the same hardware area and processing parallelism constraints. Experiments using the CNN configurations of AlexNet show that the proposed RS dataflow is more energy efficient than existing dataflows in both convolutional (1.4\texttimes{} to 2.5\texttimes{}) and fully-connected layers (at least 1.3\texttimes{} for batch size larger than 16). The RS dataflow has also been demonstrated on a fabricated chip, which verifies our energy analysis.},
booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
pages = {367–379},
numpages = {13},
location = {Seoul, Republic of Korea},
series = {ISCA '16}
}

@article{geng2019awb,
  title={{AWB-GCN:} A Graph Convolutional Network Accelerator with Runtime Workload Rebalancing},
  author={Geng, Tong and Li, Ang and Wang, Tianqi and Wu, Chunshu and Li, Yanfei and Shi, Runbin and Tumeo, Antonino and Che, Shuai and Reinhardt, Steve and Herbordt, Martin},
  journal={MICRO},
  year={2020}
}

@INPROCEEDINGS{yan2020hygcn,
  author={Yan, Mingyu and Deng, Lei and Hu, Xing and Liang, Ling and Feng, Yujing and Ye, Xiaochun and Zhang, Zhimin and Fan, Dongrui and Xie, Yuan},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={HyGCN: A GCN Accelerator with Hybrid Architecture}, 
  year={2020},
  volume={},
  number={},
  pages={15-29},
  doi={10.1109/HPCA47549.2020.00012}}

@article{hwang2020centaur,
  title={Centaur: A Chiplet-based, Hybrid Sparse-Dense Accelerator for Personalized Recommendations},
  author={Hwang, Ranggi and Kim, Taehun and Kwon, Youngeun and Rhu, Minsoo},
  journal={ISCA},
  year={2020}
}

@article{jia2020improving,
  title={Improving the accuracy, scalability, and performance of graph neural networks with roc},
  author={Jia, Zhihao and Lin, Sina and Gao, Mingyu and Zaharia, Matei and Aiken, Alex},
  journal={MLSys},
  year={2020}
}

@article{liang2020engn,
  title={EnGN: A High-Throughput and Energy-Efficient Accelerator for Large Graph Neural Networks},
  author={Liang, Shengwen and Wang, Ying and Liu, Cheng and He, Lei and Huawei, LI and Xu, Dawen and Li, Xiaowei},
  journal={IEEE Transactions on Computers},
  year={2020},
  publisher={IEEE}
}

@inproceedings{hamilton2017inductive,
  title={Inductive representation learning on large graphs},
  author={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  booktitle={Advances in neural information processing systems},
  pages={1024--1034},
  year={2017}
}

@INPROCEEDINGS{graphabcd,
  author={Y. {Yang} and Z. {Li} and Y. {Deng} and Z. {Liu} and S. {Yin} and S. {Wei} and L. {Liu}},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={GraphABCD: Scaling Out Graph Analytics with Asynchronous Block Coordinate Descent}, 
  year={2020},
  volume={},
  number={},
  pages={419-432},}
  
  @article{sundaram2015graphmat,
  title={{GraphMat:} High performance graph analytics made productive},
  author={Sundaram, Narayanan and Satish, Nadathur and Patwary, Md Mostofa Ali and Dulloor, Subramanya R and Anderson, Michael J and Vadlamudi, Satya Gautam and Das, Dipankar and Dubey, Pradeep},
  journal={Proceedings of the VLDB Endowment},
  volume={8},
  number={11},
  pages={1214--1225},
  year={2015},
  publisher={VLDB Endowment}
}

@article{kersting2016benchmark,
  title={Benchmark data sets for graph kernels, 2016},
  author={Kersting, Kristian and Kriege, Nils M and Morris, Christopher and Mutzel, Petra and Neumann, Marion},
  journal={URL http://graphkernels.cs.tu-dortmund.de},
  volume={795},
  year={2016}
}

@inproceedings{yanardag2015deep,
  title={Deep graph kernels},
  author={Yanardag, Pinar and Vishwanathan, SVN},
  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1365--1374},
  year={2015}
}

@inproceedings{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  booktitle={Advances in neural information processing systems},
  pages={103--112},
  year={2019}
}

@article{chen2018efficient,
  title={Efficient and robust parallel dnn training through model parallelism on multi-gpu platform},
  author={Chen, Chi-Chung and Yang, Chia-Lin and Cheng, Hsiang-Yun},
  journal={arXiv preprint arXiv:1809.02839},
  year={2018}
}

@inproceedings{yan2020tinygnn,
author = {Yan, Bencheng and Wang, Chaokun and Guo, Gaoyang and Lou, Yunkai},
title = {TinyGNN: Learning Efficient Graph Neural Networks},
year = {2020},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
series = {KDD '20}
}

@article{jia2019redundancy,
  title={Redundancy-free computation graphs for graph neural networks},
  author={Jia, Zhihao and Lin, Sina and Ying, Rex and You, Jiaxuan and Leskovec, Jure and Aiken, Alex},
  journal={KDD},
  year={2020}
}

@article{kipf2017semisupervised,
      title={Semi-Supervised Classification with Graph Convolutional Networks}, 
      author={Thomas N. Kipf and Max Welling},
    journal={ICLR},
    year={2017}
}

@inproceedings{kwon2019understanding,
  title={Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach},
  author={Kwon, Hyoukjun and Chatarasi, Prasanth and Pellauer, Michael and Parashar, Angshuman and Sarkar, Vivek and Krishna, Tushar},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={754--768},
  year={2019},
  organization={ACM}
}

@inproceedings{kwon2018maeri,
  title={{MAERI:} Enabling Flexible Dataflow Mapping over DNN Accelerators via Programmable Interconnects},
  author={Kwon, Hyoukjun and Samajdar, Ananda and Krishna, Tushar},
  booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages = {461–475},
  year={2018},
  organization={ACM}
}




%%%%%% Papers from GNN survey
@article{li2015gated,
  title={Gated graph sequence neural networks},
  author={Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
  journal={arXiv preprint arXiv:1511.05493},
  year={2015}
}
@article{Rahimi2018SemisupervisedUG,
  title={Semi-supervised User Geolocation via Graph Convolutional Networks},
  author={Afshin Rahimi and Trevor Cohn and Timothy Baldwin},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.08049}
}
@article{Gui2019,
  title={A survey on graph processing accelerators: Challenges and opportunities},
  author={Gui, Chuang-Yi and Zheng, Long and He, Bingsheng and Liu, Cheng and Chen, Xin-Yu and Liao, Xiao-Fei and Jin, Hai},
  journal={Journal of Computer Science and Technology},
  volume={34},
  number={2},
  pages={339--371},
  year={2019},
  publisher={Springer}
}
@article{Tang,
archivePrefix = {arXiv},
journal={arXiv preprint arXiv:2003.06307v1},
author = {Tang, Zhenheng and Shi, Shaohuai and Chu, Xiaowen and Wang, Wei and Li, Bo},
eprint = {arXiv:2003.06307v1},
file = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/2003.06307.pdf:pdf},
number = {1},
pages = {1--23},
year = {2020},
title = {{Communication-Efficient Distributed Deep Learning : A Comprehensive Survey}}
}
@article{Mittal2020,
author = {Mittal, Sparsh},
journal = {Neural Comput. \& Appl.},
doi = {10.1007/s00521-018-3761-1},
file = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/Mittal2020{\_}Article{\_}ASurveyOfFPGA-basedAccelerator.pdf:pdf},
isbn = {0123456789},
issn = {1433-3058},
keywords = {Deep learning,Neural network (NN),Convolutional NN (CNN),Binarized NN,Hardware architecture for machine learning,FPGA,Reconfigurable computing,Parallelization,Low power,cnn,deep learning {\'{a}} neural,hardware architecture for,machine learning {\'{a}} fpga,network,nn,parallelization {\'{a}} low power,{\'{a}} binarized nn {\'{a}},{\'{a}} convolutional nn,{\'{a}} reconfigurable computing {\'{a}}},
pages = {1109--1139},
publisher = {Springer London},
title = {{A survey of FPGA-based accelerators for convolutional neural networks}},
volume = {32},
year = {2020}
}
@article{Chen2017,
author={Y. {Chen} and T. {Krishna} and J. S. {Emer} and V. {Sze}},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks}, 
  year={2017},
  volume={52},
  number={1},
  pages={127-138}}

@InProceedings{YongSccar,
author="Yong, S. L.
and Hagenbuchner, M.
and Tsoi, A. C.
and Scarselli, F.
and Gori, M.",
editor="Fuhr, Norbert
and Lalmas, Mounia
and Trotman, Andrew",
title="Document Mining Using Graph Neural Network",
booktitle="Comparative Evaluation of XML Information Retrieval Systems",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="458--472",
abstract="The Graph Neural Network is a relatively new machine learning method capable of encoding data as well as relationships between data elements. This paper applies the Graph Neural Network for the first time to a given learning task at an international competition on the classification of semi-structured documents. Within this setting, the Graph Neural Network is trained to encode and process a relatively large set of XML formatted documents. It will be shown that the performance using the Graph Neural Network approach significantly outperforms the results submitted by the best competitor.",
isbn="978-3-540-73888-6"
}
@article{Xie2020,
author = {Xie, Zhipu and Lv, Weifeng and Huang, Shangfo and Lu, Zhilong and Du, Bowen},
doi = {10.1109/ACCESS.2019.2915364},
file = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/08708297.pdf:pdf},
isbn = {1711000051},
journal = {IEEE Access},
pages = {63349--63358},
publisher = {IEEE},
title = {{Sequential Graph Neural Network for Urban Road Traffic Speed Prediction}},
volume = {8},
year = {2020}
}
@phdthesis{Vashishth2020,
archivePrefix = {arXiv},
arxivId = {arXiv:1911.03042v3},
author = {Vashishth, Shikhar},
eprint = {arXiv:1911.03042v3},
file = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/1911.03042.pdf:pdf},
school = {IISc Bangalore},
title = {{Neural Graph Embedding Methods for Natural Language Processing}},
volume = {012},
year = {2020}
}
@ARTICLE{nlp,  author={T. {Young} and D. {Hazarika} and S. {Poria} and E. {Cambria}},  journal={IEEE Computational Intelligence Magazine},   title={Recent Trends in Deep Learning Based Natural Language Processing [Review Article]},   year={2018},  volume={13},  number={3},  pages={55-75},}

@article{Nettleton2013,
author = {Nettleton, David F},
doi = {10.1016/j.cosrev.2012.12.001},
file = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/1-s2.0-S1574013712000445-main.pdf:pdf},
issn = {1574-0137},
journal = {Comput. Sci. Rev.},
pages = {1--34},
publisher = {Elsevier Inc.},
title = {{Data mining of social networks represented as graphs}},
url = {http://dx.doi.org/10.1016/j.cosrev.2012.12.001},
volume = {7},
year = {2013}
}
@article{Rajpura2007,
title={Object Detection Using Deep CNNs Trained on Synthetic Images},
    author={Param S. Rajpura and Hristo Bojinov and Ravi S. Hegde},
    year={2017},
    journal={arXiv preprint arXiv:1706.06782},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@article{Gkioxari,
archivePrefix = {arXiv},
journal={arXiv preprint arXiv:1406.5212v1},
author = {Gkioxari, Georgia and Girshick, Ross},
eprint = {arXiv:1406.5212v1},
file = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/1406.5212.pdf:pdf},
pages = {1--8},
year = {2014},
title = {{R-CNNs for Pose Estimation and Action Detection}}
}
@article{Rawat,
author = {Rawat, Waseem and Wang, Zenghui},
title = {Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review},
journal = {Neural Computation},
volume = {29},
number = {9},
pages = {2352-2449},
year = {2017},
doi = {10.1162/neco\_a\_00990},
    note ={PMID: 28599112},

URL = { 
        https://doi.org/10.1162/neco_a_00990
    
},
eprint = { 
        https://doi.org/10.1162/neco_a_00990
    
}
,
    abstract = { Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art deep learning systems. Along the way, we analyze (1) their early successes, (2) their role in the deep learning renaissance, (3) selected symbolic works that have contributed to their recent popularity, and (4) several improvement attempts by reviewing contributions and challenges of over 300 publications. We also introduce some of their current trends and remaining challenges. }
}

@article{XXXtaco,
  author = {Kjolstad, Fredrik and Kamil, Shoaib and Chou, Stephen and Lugato, David and Amarasinghe, Saman},
  title = {The Tensor Algebra Compiler},
  journal = {Proc. ACM Program. Lang.},
  issue_date = {October 2017},
  volume = {1},
  number = {OOPSLA},
  month = oct,
  year = {2017},
  issn = {2475-1421},
  pages = {77:1--77:29},
  articleno = {77},
  numpages = {29},
  url = {http://doi.acm.org/10.1145/3133901},
  doi = {10.1145/3133901},
  acmid = {3133901},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {code generation, iteration graphs, linear algebra, merge lattices, parallelism, performance, sparse data structures, tensor algebra, tensors}
}

@misc{omega,
title = {{OMEGA}},
howpublished = {\url{https://github.com/stonne-simulator/omega}}}

@misc{ampere,
title = {{Ampere Tensor Core}},
howpublished = {\url{https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf}}
}

@inproceedings{dream,
  title={DREAM: A Dynamic Scheduler for Dynamic Real-time Multi-model ML Workloads},
  author={Kim, Seah and Kwon, Hyoukjun and Song, Jinook and Jo, Jihyuck and Chen, Yu-Hsin and Lai, Liangzhen and Chandra, Vikas},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
  pages={73--86},
  year={2023}
}

@inproceedings{kim2023aurora,
  title={AuRORA: Virtualized Accelerator Orchestration for Multi-Tenant Workloads},
  author={Kim, Seah and Zhao, Jerry and Asanovic, Krste and Nikolic, Borivoje and Shao, Yakun Sophia},
  booktitle={Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={62--76},
  year={2023}
}

@article{Gu2018,
author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
doi = {10.1016/j.patcog.2017.10.013},
file = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/1-s2.0-S0031320317304120-main.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognit.},
keywords = {Convolutional neural network,Deep learning,convolutional neural network},
pages = {354--377},
publisher = {Elsevier Ltd},
title = {{Recent advances in convolutional neural networks}},
url = {https://doi.org/10.1016/j.patcog.2017.10.013},
volume = {77},
year = {2018}
}
@article{Yan2020a,
title={Characterizing and understanding {GCNs} on {GPU}},
  author={Yan, Mingyu and Chen, Zhaodong and Deng, Lei and Ye, Xiaochun and Zhang, Zhimin and Fan, Dongrui and Xie, Yuan},
  journal={IEEE Computer Architecture Letters},
  volume={19},
  number={1},
  pages={22--25},
  year={2020},
  publisher={IEEE}
}
@article{Tripathy2020,
title={Reducing Communication in Graph Neural Network Training},
    author={Alok Tripathy and Katherine Yelick and Aydin Buluc},
    year={2020},
    journal={arXiv preprint arXiv:2005.03300},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@Article{AGT1957,
  volume        = {6},
  number        = {2},
  journal       = {Applied General Topology},
  pages         = {229--240},
  keywords      = {Dualistic partial metric; Partial metric; Complete; Quasi-metric; Fixed point},
  issn          = {1989-4147},
  year          = {2005},
  author        = {Oscar Valero},
  abstract      = {In this paper we prove several generalizations of the Banach fixed point theorem for partial metric spaces (in the sense of O’Neill) given in, obtaining as a particular case of our results the Banach fixed point theorem of Matthews ([12]), and some well-known classical fixed point theorems when the partial metric is, in fact, a metric.},
  title         = {On Banach fixed point theorems for partial metric spaces}
}
@inproceedings{Almeida1987ALR,
   title={A learning rule for asynchronous perceptrons with feedback in a combinatorial environment},
  author={Almeida, Luis B},
  booktitle={Artificial neural networks: concept learning},
  pages={102--111},
  year={1990}
}
@inproceedings{Atwood2016,
  title={Diffusion-convolutional neural networks},
  author={Atwood, James and Towsley, Don},
  booktitle={Advances in neural information processing systems},
  pages={1993--2001},
  year={2016}
}
@article{Auten2020,
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/dac20.pdf:pdf},
  journal       = {DAC},
  year          = {2020},
  author        = {Auten, Adam and Tomei, Matthew and Kumar, Rakesh},
  isbn          = {9781538655412},
  title         = {{Hardware Acceleration of Graph Neural Networks}}
}
@Article{Balog2019,
  title={Fast training of sparse graph neural networks on dense hardware},
  author={Balog, Matej and others},
  journal={arXiv preprint arXiv:1906.11786},
  year={2019}
}
@article{Bandinelli2010,
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Learning long-term dependencies using layered graph neural networks.pdf:pdf},
  journal       = {Proceedings of the International Joint Conference on Neural Networks},
  keywords      = {GNN types},
  year          = {2010},
  author        = {Bandinelli, Niccol{\`{o}} and Bianchini, Monica and Scarselli, Franco},
  isbn          = {9781424469178},
  abstract      = {Graph Neural Networks (GNNs) are a powerful tool for processing graphs, that represent a natural way to collect information coming from several areas of science and engineering - e.g. data mining, computer vision, molecular chemistry, molecular biology, pattern recognition -, where data are intrinsically organized in entities and relationships among entities. Nevertheless, GNNs suffer, so as recurrent/recursive models, from the long-term dependency problem that makes the learning difficult in deep structures. In this paper, we present a new architecture, called Layered GNN (LGNN), realized by a cascade of GNNs: each layer is fed with the original data and with the state information calculated by the previous layer in the cascade. Intuitively, this allows each GNN to solve a subproblem, related only to those patterns that were misclassified by the previous GNNs. Some experimental results are reported, based on synthetic and real-world datasets, which assess a significant improvement in performances w.r.t. the standard GNN approach. {\textcopyright} 2010 IEEE.},
  title         = {{Learning long-term dependencies using layered graph neural networks}},
  mendeley-tags = {GNN types},
  doi           = {10.1109/IJCNN.2010.5596634}
}
@Article{Battaglia2016,
  title={Interaction networks for learning about objects, relations and physics},
  author={Battaglia, Peter and others},
  journal={Advances in neural information processing systems},
  pages={4502--4510},
  year={2016}
}

@article{Battaglia2018,
 title={Relational inductive biases, deep learning, and graph networks},
    author={Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
    year={2018},
   journal={arXiv preprint arXiv:1806.01261},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{Bianchini2005,
  keywords      = {GNN complexity,GNN types},
  year          = {2005},
  author        = {Bianchini, M. and Maggini, M. and Sarti, L. and Scarselli, F.},
  abstract      = {In this paper, we introduce a new recursive neural network model able to process directed acyclic graphs with labelled edges. The model uses a state transition function which considers the edge labels and is independent both from the number and the order of the children of each node. The computational capabilities of the new recursive architecture are assessed. Moreover, in order to test the proposed architecture on a practical challenging application, the problem of object detection in images is also addressed. In fact, the localization of target objects is a preliminary step in any recognition system. The proposed technique is general and can be applied in different detection systems, since it does not exploit any a priori knowledge on the particular problem. Some experiments on face detection, carried out on scenes acquired by an indoor camera, are reported, showing very promising results. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
  title         = {{Recursive neural networks for processing graphs with labelled edges: Theory and applications}},
  mendeley-tags = {GNN complexity,GNN types},
  volume        = {18},
  number        = {8},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Recursive neural networks for processing graphs with labelled edges Theory and applications.pdf:pdf},
  journal       = {Neural Networks},
  pages         = {1040--1050},
  issn          = {08936080},
  doi           = {10.1016/j.neunet.2005.07.003}
}
@article{Blank2009,
  volume        = {1},
  number        = {3},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/05005647.pdf:pdf},
  journal       = {IEEE Des. Test Comput.},
  pages         = {21--39},
  issn          = {0740-7475},
  year          = {2009},
  author        = {Blank, Tom},
  abstract      = {Hardware accelerators, or special-purpose engines, have been used in computer-aided design applications for nearly 20 years. In this time, roughly 20 machines have been built and tested specifically for such purposes as simulation, design rule checking, placement, and routing. Their uses are increasing, and the machines are becoming commercially available. This survey describes not only the machines but also their problems and limitations. It also gives comparative data on speed-up techniques and performance. Examples include a simulation machine that achieves roughly a million-times speed-up over a conventional 1-MIP mainframe and a very low cost machine for design rule checking that provides a 100-times improvement. These and other examples clearly demonstrate the viability of special-purpose engines.},
  title         = {{A Survey of Hardware Accelerators Used in Computer-Aided Design}},
  doi           = {10.1109/mdt.1984.5005647}
}
@article{Borgwardt2005,
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/Protein{\_}function{\_}prediction{\_}via{\_}graph{\_}kernels.pdf:pdf},
  journal       = {Bioinformatics},
  year          = {2005},
  author        = {Borgwardt, Karsten M. and Ong, Cheng Soon and Schonauer, Stefan and Vishwanathan, S.V.N and Smola, Alex J. and Kriegel, Hans Peter},
  title         = {{Protein function prediction via graph kernels}},
  doi           = {10.1093/bioinformatics/bti1007}
}

@misc{tran2018filter,
    title={On Filter Size in Graph Convolutional Networks},
    author={Dinh Van Tran and Nicolò Navarin and Alessandro Sperduti},
    year={2018},
    eprint={1811.10435},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@inproceedings{Bresson2016,
  title={Convolutional neural networks on graphs with fast localized spectral filtering},
  author={Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
  booktitle={Advances in neural information processing systems},
  pages={3844--3852},
  year={2016}
}

@inproceedings{dualgraph,
author = {Zhuang, Chenyi and Ma, Qiang},
year = {2018},
month = {04},
pages = {499-508},
title = {Dual Graph Convolutional Networks for Graph-Based Semi-Supervised Classification},
doi = {10.1145/3178876.3186116}
}

@article{Bresson2018,
  title={Residual gated graph convnets},
  author={Bresson, Xavier and Laurent, Thomas},
  journal={arXiv preprint arXiv:1711.07553},
  year={2017}
}
@article{Bronstein2017,
  year          = {2017},
  author        = {Bronstein, Michael M. and Bruna, Joan and Lecun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  abstract      = {Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.},
  title         = {{Geometric Deep Learning: Going beyond Euclidean data}},
  volume        = {34},
  eprint        = {1611.08097},
  number        = {4},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/07974879.pdf:pdf},
  journal       = {IEEE Signal Process. Mag.},
  pages         = {18--42},
  issn          = {10535888},
  publisher     = {IEEE},
  archiveprefix = {arXiv},
  arxivid       = {1611.08097},
  doi           = {10.1109/MSP.2017.2693418}
}
@article{Brugere2018,
  year          = {2018},
  author        = {Brugere, Ivan and Gallagher, Brian and Berger-Wolf, Tanya Y.},
  abstract      = {Networks represent relationships between entities in many complex systems, spanning from online social interactions to biological cell development and brain connectivity. In many cases, relationships between entities are unambiguously known: are two users "friends" in a social network? Do two researchers collaborate on a published article? Do two road segments in a transportation system intersect? These are directly observable in the system in question. In most cases, relationships between nodes are not directly observable and must be inferred: Does one gene regulate the expression of another? Do two animals who physically co-locate have a social bond? Who infected whom in a disease outbreak in a population? Existing approaches for inferring networks from data are found across many application domains and use specialized knowledge to infer and measure the quality of inferred network for a specific task or hypothesis. However, current research lacks a rigorous methodology that employs standard statistical validation on inferred models. In this survey, we examine (1) how network representations are constructed from underlying data, (2) the variety of questions and tasks on these representations over several domains, and (3) validation strategies for measuring the inferred network's capability of answering questions on the system of interest. c 2018 ACM 0360-0300/2018/04-ART24 {\$}15.00.},
  title         = {{Network structure inference, a survey: Motivations, methods, and applications}},
  volume        = {51},
  eprint        = {1610.00782},
  number        = {2},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/3154524.pdf:pdf},
  journal       = {ACM Comput. Surv.},
  issn          = {15577341},
  archiveprefix = {arXiv},
  arxivid       = {1610.00782},
  doi           = {10.1145/3154524}
}
@article{Bruna2014,
  eprint        = {1312.6203},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spectral networks and deep locally connected networks on graphs.pdf:pdf},
  journal       = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
  pages         = {1--14},
  keywords      = {GNN complexity},
  year          = {2014},
  author        = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
  archiveprefix = {arXiv},
  abstract      = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
  title         = {{Spectral networks and deep locally connected networks on graphs}},
  arxivid       = {1312.6203},
  mendeley-tags = {GNN complexity}
}
@article{Chen,
  title={Bridging the Gap between Spatial and Spectral Domains: A Survey on Graph Neural Networks},
  author={Chen, Zhiqian and others},
  journal={arXiv preprint arXiv:2002.11867},
  year={2020}
}
@article{Chen2018,
  keywords      = {GNN complexity},
  year          = {2018},
  author        = {Chen, Jianfei and Zhu, Jun and Song, Le},
  isbn          = {9781510867963},
  abstract      = {Graph convolutional networks (GCNs) arc powerful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive field size grow exponentially wilh the number of layers. Previous attempts on reducing the receptive field size by subsampling neighbors do not have convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, wc develop control vari- Ate based algorithms with new theoretical guarantee to converge to a local optimum of GCN regardless of the neighbor sampling size. Empirical results show that our algorithms enjoy similar convergence rate and model quality with the exact algorithm using only two neighbors per node. The running time of our algorithms on a large Keddit dataset is only one seventh of previous neighbor sampling algorithms.},
  title         = {{Stochastic training of graph convolutional networks with variance reduction}},
  mendeley-tags = {GNN complexity},
  url           = {http://arxiv.org/abs/1710.10568},
  volume        = {3},
  eprint        = {1710.10568},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded//Stochastic training of graph convolutional networks with variance reduction.pdf:pdf},
  journal       = {35th International Conference on Machine Learning, ICML 2018},
  pages         = {1503--1532},
  month         = {oct},
  archiveprefix = {arXiv},
  arxivid       = {1710.10568}
}
@article{Chen2018a,
  keywords      = {GNN Types},
  year          = {2018},
  author        = {Chen, Jie and Ma, Tengfei and Xiao, Cao},
  abstract      = {The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work—FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.},
  title         = {{FastGCN: Fast learning with graph convolutional networks via importance sampling}},
  mendeley-tags = {GNN Types},
  url           = {http://arxiv.org/abs/1801.10247},
  eprint        = {1801.10247},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/FastGCN Fast learning with graph convolu-tional networks via importance sampling.pdf:pdf},
  journal       = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
  month         = {jan},
  archiveprefix = {arXiv},
  arxivid       = {1801.10247}
}
@article{Chen2019,
  eprint        = {1705.08415},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Supervised community detection with line graph neural networks.pdf:pdf},
  journal       = {7th International Conference on Learning Representations, ICLR 2019},
  pages         = {1--23},
  keywords      = {GNN complexity},
  year          = {2019},
  author        = {Chen, Zhengdao and Bruna, Joan and Li, Lisha},
  archiveprefix = {arXiv},
  abstract      = {Community detection in graphs can be solved via spectral methods or posterior inference under certain probabilistic graphical models. Focusing on random graph families such as the stochastic block model, recent research has unified both approaches and identified both statistical and computational detection thresholds in terms of the signal-to-noise ratio. By recasting community detection as a node-wise classification problem on graphs, we can also study it from a learning perspective. We present a novel family of Graph Neural Networks (GNNs) for solving community detection problems in a supervised learning setting. We show that, in a data-driven manner and without access to the underlying generative models, they can match or even surpass the performance of the belief propagation algorithm on binary and multiclass stochastic block models, which is believed to reach the computational threshold in these cases. In particular, we propose to augment GNNs with the non-backtracking operator defined on the line graph of edge adjacencies. The GNNs are achieved good performance on real-world datasets. In addition, we perform the first analysis of the optimization landscape of using (linear) GNNs to solve community detection problems, demonstrating that under certain simplifications and assumptions, the loss value at any local minimum is close to the loss value at the global minimum/minima.},
  title         = {{Supervised community detection with line graph neural networks}},
  arxivid       = {1705.08415},
  mendeley-tags = {GNN complexity}
}
@misc{wu2019simplifying,
    title={Simplifying Graph Convolutional Networks},
    author={Felix Wu and Tianyi Zhang and Amauri Holanda de Souza Jr. and Christopher Fifty and Tao Yu and Kilian Q. Weinberger},
    year={2019},
    eprint={1902.07153},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}



@inproceedings{Chiang,
  title={Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks},
  author={Chiang, Wei-Lin and Liu, Xuanqing and Si, Si and Li, Yang and Bengio, Samy and Hsieh, Cho-Jui},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={257--266},
  year={2019}
}
@article{Cho2014,
 title={On the properties of neural machine translation: Encoder-decoder approaches},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.1259},
  year={2014}
}

@article{Yu_2018,
   title={Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting},
   ISBN={9780999241127},
   url={http://dx.doi.org/10.24963/ijcai.2018/505},
   DOI={10.24963/ijcai.2018/505},
   journal={Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence},
   publisher={International Joint Conferences on Artificial Intelligence Organization},
   author={Yu, Bing and Yin, Haoteng and Zhu, Zhanxing},
   year={2018},
   month={Jul}
}

@article{Cui2019,
  keywords      = {GNN applications},
  year          = {2019},
  author        = {Cui, Zhiyong and Henrickson, Kristian and Ke, Ruimin and Wang, Yinhai},
  abstract      = {Traffic forecasting is a particularly challenging application of spatiotemporal forecasting, due to the complicated spatial dependencies on roadway networks and the time-varying traffic patterns. To address this challenge, we learn the traffic network as a graph and propose a novel deep learning framework, Traffic Graph Convolutional Long Short-Term Memory Neural Network (TGC-LSTM), to learn the interactions between roadways in the traffic network and forecast the network-wide traffic state. We define the traffic graph convolution based on the physical network topology. The relationship between traffic graph convolution and the spectral graph convolution is also discussed. The proposed model employs L1-norms on the graph convolution weights and L2-norms on the extracted features to identify the most influential roadways in the traffic network. Experiments show that our TGC-LSTM network is able to capture the complex spatial-temporal dependencies efficiently present in a vehicle traffic network and consistently outperforms state-of-the-art baseline methods on two heterogeneous real-world traffic datasets. The visualization of graph convolution weights shows that the proposed framework can accurately recognize the most influential roadway segments in real-world traffic networks.},
  title         = {{Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting}},
  mendeley-tags = {GNN applications},
  eprint        = {1802.07007},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Traffic Graph Convolutional Recurrent Neural Network A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting.pdf:pdf},
  journal       = {IEEE Transactions on Intelligent Transportation Systems},
  pages         = {1--12},
  issn          = {1524-9050},
  archiveprefix = {arXiv},
  arxivid       = {1802.07007},
  doi           = {10.1109/tits.2019.2950416}
}
@article{Cui2019b,
  keywords      = {Network embedding,data science,graph embedding,network analysis},
  year          = {2019},
  author        = {Cui, Peng and Wang, Xiao and Pei, Jian and Zhu, Wenwu},
  abstract      = {Network embedding assigns nodes in a network to low-dimensional representations and effectively preserves the network structure. Recently, a significant amount of progresses have been made toward this emerging network analysis paradigm. In this survey, we focus on categorizing and then reviewing the current development on network embedding methods, and point out its future research directions. We first summarize the motivation of network embedding. We discuss the classical graph embedding algorithms and their relationship with network embedding. Afterwards and primarily, we provide a comprehensive overview of a large number of network embedding methods in a systematic manner, covering the structure- and property-preserving network embedding methods, the network embedding methods with side information, and the advanced information preserving network embedding methods. Moreover, several evaluation approaches for network embedding and some useful online resources, including the network data sets and softwares, are reviewed, too. Finally, we discuss the framework of exploiting these network embedding methods to build an effective system and point out some potential future directions.},
  title         = {{A Survey on Network Embedding}},
  volume        = {31},
  eprint        = {1711.08752},
  number        = {5},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/08392745.pdf:pdf},
  journal       = {IEEE Trans. Knowl. Data Eng.},
  pages         = {833--852},
  issn          = {15582191},
  publisher     = {IEEE},
  archiveprefix = {arXiv},
  arxivid       = {1711.08752},
  doi           = {10.1109/TKDE.2018.2849727}
}
@article{DeCao2018,
   title={MolGAN: An implicit generative model for small molecular graphs},
  author={De Cao, Nicola and Kipf, Thomas},
  journal={arXiv preprint arXiv:1805.11973},
  year={2018}
}
@article{Dehmamy2019,
  eprint        = {1907.05008},
  number        = {NeurIPS},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/9675-understanding-the-representation-power-of-graph-neural-networks-in-learning-graph-topology.pdf:pdf},
  year          = {2019},
  author        = {Dehmamy, Nima and Barab{\'{a}}si, Albert-L{\'{a}}szl{\'{o}} and Yu, Rose},
  archiveprefix = {arXiv},
  abstract      = {To deepen our understanding of graph neural networks, we investigate the representation power of Graph Convolutional Networks (GCN) through the looking glass of graph moments, a key property of graph topology encoding path of various lengths. We find that GCNs are rather restrictive in learning graph moments. Without careful design, GCNs can fail miserably even with multiple layers and nonlinear activation functions. We analyze theoretically the expressiveness of GCNs, concluding a modular GCN design, using different propagation rules with residual connections could significantly improve the performance of GCN. We demonstrate that such modular designs are capable of distinguishing graphs from different graph generation models for surprisingly small graphs, a notoriously difficult problem in network science. Our investigation suggests that, depth is much more influential than width, with deeper GCNs being more capable of learning higher order graph moments. Additionally, combining GCN modules with different propagation rules is critical to the representation power of GCNs.},
  title         = {{Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology}},
  arxivid       = {1907.05008},
  url           = {http://arxiv.org/abs/1907.05008}
}
@article{DiMassa2006,
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A comparison between recursive neural networks and graph neural networks.pdf:pdf},
  journal       = {IEEE International Conference on Neural Networks - Conference Proceedings},
  pages         = {778--785},
  issn          = {10987576},
  keywords      = {GNN complexity,Generic},
  year          = {2006},
  author        = {{Di Massa}, Vincenzo and Monfardini, Gabriele and Sarti, Lorenzo and Scarselli, Franco and Maggini, Marco and Gori, Marco},
  isbn          = {0780394909},
  abstract      = {Recursive Neural Networks (RNNs) and Graph Neural Networks (GNNs) are two connectionist models that can directly process graphs. RNNs and GNNs exploit a similar processing framework, but they can be applied to different input domains. RNNs require the input graphs to be directed and acyclic, whereas GNNs can process any kind of graphs. The aim of this paper consists in understanding whether such a difference affects the behaviour of the models on a real application. An experimental comparison on an image classification problem is presented, showing that GNNs outperforms RNNs. Moreover the main differences between the models are also discussed w.r.t. their input domains, their approximation capabilities and their learning algorithms. {\textcopyright}2006 IEEE.},
  title         = {{A comparison between recursive neural networks and graph neural networks}},
  mendeley-tags = {GNN complexity,Generic},
  doi           = {10.1109/ijcnn.2006.246763}
}
@article{du2019graph,
  journal={arXiv preprint arXiv:1905.13192},
  year          = {2019},
  author        = {Simon S. Du and Kangcheng Hou and Barnabás Póczos and Ruslan Salakhutdinov and Ruosong Wang and Keyulu Xu},
  archiveprefix = {arXiv},
  title         = {Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels},
  primaryclass  = {cs.LG}
}
@Article{Duvenaud2015,
  mendeley-groups= {SOA Paper},
  keywords      = {GNN applications},
  year          = {2015},
  author        = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and G{\'{o}}mez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al{\'{a}}n and Adams, Ryan P},
  abstract      = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
  title         = {{Convolutional networks on graphs for learning molecular fingerprints}},
  mendeley-tags = {GNN applications},
  eprint        = {1509.09292},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf:pdf},
  journal       = {Adv. Neural Inf. Process. Syst.},
  pages         = {2224--2232},
  issn          = {10495258},
  archiveprefix = {arXiv},
  arxivid       = {1509.09292}
}
@article{Dwivedi2017,
  title={Benchmarking graph neural networks},
  author={Dwivedi, Vijay Prakash and Joshi, Chaitanya K and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  journal={arXiv preprint arXiv:2003.00982},
  year={2020}
}





@article{Fan2019,
  keywords      = {GNN application,GNN types,Graph Neural Networks,Neural Networks,Recommender Systems,Social Network,Social Recommendation},
  year          = {2019},
  author        = {Fan, Wenqi and Ma, Yao and Li, Qing and He, Yuan and Zhao, Eric and Tang, Jiliang and Yin, Dawei},
  isbn          = {9781450366748},
  abstract      = {In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key. However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the user-user social graph and the user-item graph). To address the three aforementioned challenges simultaneously, in this paper, we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec.},
  title         = {{Graph neural networks for social recommendation}},
  mendeley-tags = {GNN application,GNN types},
  eprint        = {1902.07243},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graph neural networks for social recommendation.pdf:pdf},
  journal       = {The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019},
  pages         = {417--426},
  archiveprefix = {arXiv},
  arxivid       = {1902.07243},
  doi           = {10.1145/3308558.3313488}
}


@article{Fout2017,
  volume        = {2017-Decem},
  number        = {Nips},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Protein interface prediction using graph convolutional networks.pdf:pdf},
  journal       = {Advances in Neural Information Processing Systems},
  pages         = {6531--6540},
  issn          = {10495258},
  keywords      = {GNN applications},
  year          = {2017},
  author        = {Fout, Alex and Byrd, Jonathon and Shariat, Basir and Ben-Hur, Asa},
  abstract      = {We consider the prediction of interfaces between proteins, a challenging problem with important applications in drug discovery and design, and examine the performance of existing and newly proposed spatial graph convolution operators for this task. By performing convolution over a local neighborhood of a node of interest, we are able to stack multiple layers of convolution and learn effective latent representations that integrate information across the graph that represent the three dimensional structure of a protein of interest. An architecture that combines the learned features across pairs of proteins is then used to classify pairs of amino acid residues as part of an interface or not. In our experiments, several graph convolution operators yielded accuracy that is better than the state-of-the-art SVM method in this task.},
  title         = {{Protein interface prediction using graph convolutional networks}},
  mendeley-tags = {GNN applications}
}
@inproceedings{Garcia-Duran2017,
  title={Learning graph representations with embedding propagation},
  author={Duran, Alberto Garcia and Niepert, Mathias},
  booktitle={Advances in neural information processing systems},
  pages={5119--5130},
  year={2017}
}
@article{Garcia2018,
  eprint        = {1711.04043},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Few-shot learning with graph neural networks.pdf:pdf},
  journal       = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
  pages         = {1--13},
  keywords      = {GNN types},
  year          = {2018},
  author        = {Garcia, Victor and Bruna, Joan},
  archiveprefix = {arXiv},
  abstract      = {We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on ‘relational' tasks.},
  title         = {{Few-shot learning with graph neural networks}},
  arxivid       = {1711.04043},
  mendeley-tags = {GNN types}
}

@article{Ghosh2018,
  volume        = {27},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/1-s2.0-S1574013717301429-main.pdf:pdf},
  journal       = {Comput. Sci. Rev.},
  pages         = {88--111},
  issn          = {1574-0137},
  year          = {2018},
  author        = {Ghosh, Swarnendu and Das, Nibaran and Gon{\c{c}}alves, Teresa and Quaresma, Paulo},
  publisher     = {Elsevier Inc.},
  title         = {{The journey of graph kernels through two decades}},
  url           = {https://doi.org/10.1016/j.cosrev.2017.11.002},
  doi           = {10.1016/j.cosrev.2017.11.002}
}

@article{stock,
title = "Using artificial neural network models in stock market index prediction",
journal = "Expert Systems with Applications",
volume = "38",
number = "8",
pages = "10389 - 10397",
year = "2011",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2011.02.068",
url = "http://www.sciencedirect.com/science/article/pii/S0957417411002740",
author = "Erkam Guresen and Gulgun Kayakutlu and Tugrul U. Daim",
keywords = "Financial time series (FTS) prediction, Recurrent neural networks (RNN), Dynamic artificial neural networks (DAN2), Hybrid forecasting models",
abstract = "Forecasting stock exchange rates is an important financial problem that is receiving increasing attention. During the last few years, a number of neural network models and hybrid models have been proposed for obtaining accurate prediction results, in an attempt to outperform the traditional linear and nonlinear approaches. This paper evaluates the effectiveness of neural network models which are known to be dynamic and effective in stock-market predictions. The models analysed are multi-layer perceptron (MLP), dynamic artificial neural network (DAN2) and the hybrid neural networks which use generalized autoregressive conditional heteroscedasticity (GARCH) to extract new input variables. The comparison for each model is done in two view points: Mean Square Error (MSE) and Mean Absolute Deviate (MAD) using real exchange daily rate values of NASDAQ Stock Exchange index."
}

@Article{Gilmer2017,
  mendeley-groups= {SOA Paper},
  keywords      = {GNN modelling,GNN types,Generic},
  year          = {2017},
  author        = {Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  isbn          = {9781510855144},
  abstract      = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  title         = {{Neural message passing for quantum chemistry}},
  mendeley-tags = {GNN modelling,GNN types,Generic},
  url           = {http://arxiv.org/abs/1704.01212},
  volume        = {3},
  eprint        = {1704.01212},
  journal       = {34th Int. Conf. Mach. Learn. ICML 2017},
  pages         = {2053--2070},
  month         = {apr},
  archiveprefix = {arXiv},
  arxivid       = {1704.01212}
}

@inproceedings{FlashAttention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}


@article{Glen2006,
  title={Circular fingerprints: flexible molecular descriptors with applications from physical chemistry to ADME},
  author={Glen, Robert C and Bender, Andreas and Arnby, Catrin H and Carlsson, Lars and Boyer, Scott and Smith, James},
  journal={IDrugs},
  volume={9},
  number={3},
  pages={199},
  year={2006},
  publisher={Thomson Scientific}
}
@article{Gori2005,
  volume        = {2},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A new model for learning in Graph domains.pdf:pdf},
  journal       = {Proceedings of the International Joint Conference on Neural Networks},
  pages         = {729--734},
  keywords      = {GNN modelling,Pioneer},
  year          = {2005},
  author        = {Gori, Marco and Monfardini, Gabriele and Scarselli, Franco},
  isbn          = {0780390482},
  abstract      = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in the way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network (GNN), capable of directly processing graphs. GNNs extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for GNNs is proposed and some experiments are discussed which assess the properties of the model. {\textcopyright} 2005 IEEE.},
  title         = {{A new model for learning in Graph domains}},
  mendeley-tags = {GNN modelling,Pioneer},
  doi           = {10.1109/IJCNN.2005.1555942}
}


@article{graphs,
  journal       = {Wolfram MathWorld: The web's most extensive mathematics resource},
  issn          = {10987576},
  author        = {{Eric W. Weisstein}},
  isbn          = {0780394909},
  title         = {{Graph}}
}
@inproceedings{Guo2019,
    title={Attention based spatial-temporal graph convolutional networks for traffic flow forecasting},
  author={Guo, Shengnan and Lin, Youfang and Feng, Ning and Song, Chao and Wan, Huaiyu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={922--929},
  year={2019}
}
@article{Guo2019b,
  author = {Guo, Kaiyuan and Zeng, Shulin and Yu, Jincheng and Wang, Yu and Yang, Huazhong},
title = {[DL] A Survey of FPGA-Based Neural Network Inference Accelerators},
year = {2019},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3289185},
doi = {10.1145/3289185},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = mar,
articleno = {2},
numpages = {26},
keywords = {neural network, parallel processing, FPGA architecture}
}
@article{Hamaguchi2017,
  title={Knowledge transfer for out-of-knowledge-base entities: A graph neural network approach},
  author={Hamaguchi, Takuo and Oiwa, Hidekazu and Shimbo, Masashi and Matsumoto, Yuji},
  journal={arXiv preprint arXiv:1706.05674},
  year={2017}
}
@Article{Hamilton,
  eprint        = {arXiv:1709.05584v3},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/Representation Learning on Graphs Methods and Applications.pdf:pdf},
  pages         = {1--24},
  author        = {Hamilton, William L and Ying, Rex},
  archiveprefix = {arXiv},
  title         = {{Representation Learning on Graphs : Methods and Applications}},
  arxivid       = {arXiv:1709.05584v3}
}
@article{Hamilton2017,
  title={Representation Learning on Graphs: Methods and Applications},
    author={William L. Hamilton and Rex Ying and Jure Leskovec},
    year={2017},
    journal={arXiv preprint arXiv:1709.05584},
    archivePrefix={arXiv},
    primaryClass={cs.SI}
}
@article{Hamilton2017a,
  keywords      = {GNN accelerator,GNN modelling},
  year          = {2017},
  author        = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  abstract      = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  title         = {{Inductive representation learning on large graphs}},
  mendeley-tags = {GNN accelerator,GNN modelling},
  volume        = {2017-Decem},
  eprint        = {1706.02216},
  number        = {Nips},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Inductive representation learning on large graphs.pdf:pdf},
  journal       = {Advances in Neural Information Processing Systems},
  pages         = {1025--1035},
  issn          = {10495258},
  archiveprefix = {arXiv},
  arxivid       = {1706.02216}
}

@Article{Henaff2015,
  title={Deep convolutional networks on graph-structured data},
  author={Henaff, Mikael and Bruna, Joan and LeCun, Yann},
  journal={arXiv preprint arXiv:1506.05163},
  year={2015}
}

@misc{hu2019hierarchical,
    title={Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification},
    author={Fenyu Hu and Yanqiao Zhu and Shu Wu and Liang Wang and Tieniu Tan},
    year={2019},
    eprint={1902.06667},
    archivePrefix={arXiv},
    primaryClass={cs.SI}
}
@article{Jiang2019,
  keywords      = {GNN modelling,Machine Learning Applications: Other Applications,Machine Learning: Deep Learning,Machine Learning: Relational Learning,Machine Learning: Semi-Supervised Learning},
  year          = {2019},
  author        = {Jiang, Xiaodong and Ji, Pengsheng and Li, Sheng},
  isbn          = {9780999241141},
  abstract      = {In this paper, we present CensNet, Convolution with Edge-Node Switching graph neural network, for semi-supervised classification and regression in graph-structured data with both node and edge features. CensNet is a general graph embedding framework, which embeds both nodes and edges to a latent feature space. By using line graph of the original undirected graph, the role of nodes and edges are switched, and two novel graph convolution operations are proposed for feature propagation. Experimental results on real-world academic citation networks and quantum chemistry graphs show that our approach has achieved or matched the state-of-the-art performance.},
  title         = {{CensNet: Convolution with edge-node switching in graph neural networks}},
  mendeley-tags = {GNN modelling},
  volume        = {2019-Augus},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/CensNet Convolution with edge-node switching in graph neural networks.pdf:pdf},
  journal       = {IJCAI International Joint Conference on Artificial Intelligence},
  pages         = {2656--2662},
  issn          = {10450823},
  doi           = {10.24963/ijcai.2019/369}
}
@misc{Joshi,
  author        = {Joshi, Chaitanya},
  title         = {{Transformers are Graph Neural Networks}},
  url           = {https://towardsdatascience.com/transformers-are-graph-neural-networks-bca9f75412aa}
}
@article{Kawamoto2018,
  keywords      = {GNN complexity},
  year          = {2018},
  author        = {Kawamoto, Tatsuro and Tsubaki, Masashi and Obuchi, Tomoyuki},
  abstract      = {A theoretical performance analysis of the graph neural network (GNN) is presented. For classification tasks, the neural network approach has the advantage in terms of flexibility that it can be employed in a data-driven manner, whereas Bayesian inference requires the assumption of a specific model. A fundamental question is then whether GNN has a high accuracy in addition to this flexibility. Moreover, whether the achieved performance is predominately a result of the backpropagation or the architecture itself is a matter of considerable interest. To gain a better insight into these questions, a mean-field theory of a minimal GNN architecture is developed for the graph partitioning problem. This demonstrates a good agreement with numerical experiments.},
  title         = {{Mean-field theory of graph neural networks in graph partitioning}},
  mendeley-tags = {GNN complexity},
  volume        = {2018-Decem},
  eprint        = {1810.11908},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mean-field theory of graph neural networks in graph partitioning.pdf:pdf},
  journal       = {Advances in Neural Information Processing Systems},
  pages         = {4361--4371},
  issn          = {10495258},
  publisher     = {IOP Publishing},
  archiveprefix = {arXiv},
  arxivid       = {1810.11908},
  doi           = {10.1088/1742-5468/ab3456}
}
@article{Kim2020,
  title={Understanding graph isomorphism network for brain MR functional connectivity analysis},
  author={Kim, Byung-Hoon and Ye, Jong Chul},
  journal={arXiv preprint arXiv:2001.03690},
  year={2020}
}
@article{Kipf2016,
  title={Variational graph auto-encoders},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1611.07308},
  year={2016}
}

@misc{node2vec,
    title={node2vec: Scalable Feature Learning for Networks},
    author={Aditya Grover and Jure Leskovec},
    year={2016},
    eprint={1607.00653},
    archivePrefix={arXiv},
    primaryClass={cs.SI}
}

@article{Perozzi_2014,
   title={DeepWalk},
   ISBN={9781450329569},
   url={http://dx.doi.org/10.1145/2623330.2623732},
   DOI={10.1145/2623330.2623732},
   journal={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD  ’14},
   publisher={ACM Press},
   author={Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
   year={2014}
}

@misc{graph2vec,
    title={graph2vec: Learning Distributed Representations of Graphs},
    author={Annamalai Narayanan and Mahinthan Chandramohan and Rajasekar Venkatesan and Lihui Chen and Yang Liu and Shantanu Jaiswal},
    year={2017},
    eprint={1707.05005},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@Article{Kipf2019,
  mendeley-groups= {SOA Paper},
  keywords      = {GNN modelling},
  year          = {2019},
  author        = {Kipf, Thomas N and Welling, Max},
  abstract      = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  title         = {{Semi-supervised classification with graph convolutional networks}},
  mendeley-tags = {GNN modelling},
  url           = {http://arxiv.org/abs/1609.02907},
  eprint        = {1609.02907},
  journal       = {5th Int. Conf. Learn. Represent. ICLR 2017 - Conf. Track Proc.},
  month         = {sep},
  archiveprefix = {arXiv},
  arxivid       = {1609.02907}
}
@article{Kriege_2020,
  volume        = {5},
  number        = {1},
  journal       = {Applied Network Science},
  issn          = {2364-8228},
  month         = {Jan},
  year          = {2020},
  author        = {Kriege, Nils M. and Johansson, Fredrik D. and Morris, Christopher},
  publisher     = {Springer Science and Business Media LLC},
  title         = {A survey on graph kernels},
  url           = {http://dx.doi.org/10.1007/s41109-019-0195-3},
  doi           = {10.1007/s41109-019-0195-3}
}
@article{Lamb2020,
  title={Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective},
    author={Luis C. Lamb and Artur Garcez and Marco Gori and Marcelo Prates and Pedro Avelar and Moshe Vardi},
    year={2020},
    journal={arXiv preprint arXiv:2003.00330},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
@article{Lee2019,
  keywords      = {Attention mechanism,Deep learning,Graph attention,Graph attention survey},
  year          = {2019},
  author        = {Lee, John Boaz and Rossi, Ryan A. and Kim, Sungchul and Ahmed, Nesreen K. and Koh, Eunyee},
  abstract      = {Graph-structured data arise naturally in many different application domains. By representing data as graphs, we can capture entities (i.e., nodes) as well as their relationships (i.e., edges) with each other. Many useful insights can be derived from graph-structured data as demonstrated by an ever-growing body of work focused on graph mining. However, in the real-world, graphs can be both large-with many complex patterns-and noisy, which can pose a problem for effective graph mining. An effective way to deal with this issue is to incorporate "attention" into graph mining solutions. An attention mechanism allows a method to focus on task-relevant parts of the graph, helping it to make better decisions. In this work, we conduct a comprehensive and focused survey of the literature on the emerging field of graph attention models. We introduce three intuitive taxonomies to group existing work. These are based on problem setting (type of input and output), the type of attention mechanism used, and the task (e.g., graph classification, link prediction). We motivate our taxonomies through detailed examples and use each to survey competing approaches from a unique standpoint. Finally, we highlight several challenges in the area and discuss promising directions for future work.},
  title         = {{Attention models in graphs: A survey}},
  volume        = {13},
  eprint        = {1807.07984},
  number        = {6},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/3363574.pdf:pdf},
  journal       = {ACM Trans. Knowl. Discov. Data},
  issn          = {1556472X},
  archiveprefix = {arXiv},
  arxivid       = {1807.07984},
  doi           = {10.1145/3363574}
}
@Article{Li2016,
  mendeley-groups= {SOA Paper},
  keywords      = {GNN types},
  year          = {2016},
  author        = {Li, Yujia and Zemel, Richard and Brockschmidt, Marc and Tarlow, Daniel},
  abstract      = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be described as abstract data structures.},
  title         = {{Gated graph sequence neural networks}},
  mendeley-tags = {GNN types},
  url           = {http://arxiv.org/abs/1511.05493},
  eprint        = {1511.05493},
  journal       = {4th Int. Conf. Learn. Represent. ICLR 2016 - Conf. Track Proc.},
  month         = {nov},
  archiveprefix = {arXiv},
  arxivid       = {1511.05493}
}
@article{Li2017,
  keywords      = {GNN applications},
  year          = {2017},
  author        = {Li, Ruiyu and Tapaswi, Makarand and Liao, Renjie and Jia, Jiaya and Urtasun, Raquel and Fidler, Sanja},
  isbn          = {9781538610329},
  abstract      = {We address the problem of recognizing situations in images. Given an image, the task is to predict the most salient verb (action), and fill its semantic roles such as who is performing the action, what is the source and target of the action, etc. Different verbs have different roles (e.g. attacking has weapon), and each role can take on many possible values (nouns). We propose a model based on Graph Neural Networks that allows us to efficiently capture joint dependencies between roles using neural networks defined on a graph. Experiments with different graph connectivities show that our approach that propagates information between roles significantly outperforms existing work, as well as multiple baselines. We obtain roughly 3-5{\%} improvement over previous work in predicting the full situation. We also provide a thorough qualitative analysis of our model and influence of different roles in the verbs.},
  title         = {{Situation Recognition with Graph Neural Networks}},
  mendeley-tags = {GNN applications},
  volume        = {2017-Octob},
  eprint        = {1708.04320},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Situation Recognition with Graph Neural Networks.pdf:pdf},
  journal       = {Proceedings of the IEEE International Conference on Computer Vision},
  pages         = {4183--4192},
  issn          = {15505499},
  archiveprefix = {arXiv},
  arxivid       = {1708.04320},
  doi           = {10.1109/ICCV.2017.448}
}
@article{Li2018,
  title={Learning deep generative models of graphs},
  author={Li, Yujia and Vinyals, Oriol and Dyer, Chris and Pascanu, Razvan and Battaglia, Peter},
  journal={arXiv preprint arXiv:1803.03324},
  year={2018}
}

@article{kroneker,
author = {Leskovec, Jure and Chakrabarti, Deepayan and Kleinberg, Jon and Faloutsos, Christos and Ghahramani, Zoubin},
title = {Kronecker Graphs: An Approach to Modeling Networks},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {How can we generate realistic networks? In addition, how can we do so with a mathematically tractable model that allows for rigorous analysis of network properties? Real networks exhibit a long list of surprising properties: Heavy tails for the in- and out-degree distribution, heavy tails for the eigenvalues and eigenvectors, small diameters, and densification and shrinking diameters over time. Current network models and generators either fail to match several of the above properties, are complicated to analyze mathematically, or both. Here we propose a generative model for networks that is both mathematically tractable and can generate networks that have all the above mentioned structural properties. Our main idea here is to use a non-standard matrix operation, the Kronecker product, to generate graphs which we refer to as "Kronecker graphs".First, we show that Kronecker graphs naturally obey common network properties. In fact, we rigorously prove that they do so. We also provide empirical evidence showing that Kronecker graphs can effectively model the structure of real networks.We then present KRONFIT, a fast and scalable algorithm for fitting the Kronecker graph generation model to large real networks. A naive approach to fitting would take super-exponential time. In contrast, KRONFIT takes linear time, by exploiting the structure of Kronecker matrix multiplication and by using statistical simulation techniques.Experiments on a wide range of large real and synthetic networks show that KRONFIT finds accurate parameters that very well mimic the properties of target networks. In fact, using just four parameters we can accurately model several aspects of global network structure. Once fitted, the model parameters can be used to gain insights about the network structure, and the resulting synthetic graphs can be used for null-models, anonymization, extrapolations, and graph summarization.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {985–1042},
numpages = {58}
}
@article{miscgan,
author = {Zhou, Dawei and Zheng, Lecheng and Xu, Jiejun and He, Jingrui},
year = {2019},
month = {04},
pages = {3},
title = {Misc-GAN: A Multi-scale Generative Model for Graphs},
volume = {2},
journal = {Frontiers in Big Data},
doi = {10.3389/fdata.2019.00003}
}

@article{FORTUNATO201075,
title = "Community detection in graphs",
journal = "Physics Reports",
volume = "486",
number = "3",
pages = "75 - 174",
year = "2010",
issn = "0370-1573",
doi = "https://doi.org/10.1016/j.physrep.2009.11.002",
url = "http://www.sciencedirect.com/science/article/pii/S0370157309002841",
author = "Santo Fortunato",
keywords = "Graphs, Clusters, Statistical physics",
abstract = "The modern science of networks has brought significant advances to our understanding of complex systems. One of the most relevant features of graphs representing real systems is community structure, or clustering, i.e. the organization of vertices in clusters, with many edges joining vertices of the same cluster and comparatively few edges joining vertices of different clusters. Such clusters, or communities, can be considered as fairly independent compartments of a graph, playing a similar role like, e.g., the tissues or the organs in the human body. Detecting communities is of great importance in sociology, biology and computer science, disciplines where systems are often represented as graphs. This problem is very hard and not yet satisfactorily solved, despite the huge effort of a large interdisciplinary community of scientists working on it over the past few years. We will attempt a thorough exposition of the topic, from the definition of the main elements of the problem, to the presentation of most methods developed, with a special focus on techniques designed by statistical physicists, from the discussion of crucial issues like the significance of clustering and how methods should be tested and compared against each other, to the description of applications to real networks."
}

@misc{liu2017learning,
    title={Learning Graph Topological Features via GAN},
    author={Weiyi Liu and Hal Cooper and Min Hwan Oh and Sailung Yeung and Pin-Yu Chen and Toyotaro Suzumura and Lingli Chen},
    year={2017},
    eprint={1709.03545},
    archivePrefix={arXiv},
    primaryClass={cs.SI}
}

@article{MALLIAROS201395,
title = "Clustering and community detection in directed networks: A survey",
journal = "Physics Reports",
volume = "533",
number = "4",
pages = "95 - 142",
year = "2013",
note = "Clustering and Community Detection in Directed Networks: A Survey",
issn = "0370-1573",
doi = "https://doi.org/10.1016/j.physrep.2013.08.002",
url = "http://www.sciencedirect.com/science/article/pii/S0370157313002822",
author = "Fragkiskos D. Malliaros and Michalis Vazirgiannis",
keywords = "Community detection, Graph clustering, Directed networks, Complex networks, Graph mining",
abstract = "Networks (or graphs) appear as dominant structures in diverse domains, including sociology, biology, neuroscience and computer science. In most of the aforementioned cases graphs are directed — in the sense that there is directionality on the edges, making the semantics of the edges nonsymmetric as the source node transmits some property to the target one but not vice versa. An interesting feature that real networks present is the clustering or community structure property, under which the graph topology is organized into modules commonly called communities or clusters. The essence here is that nodes of the same community are highly similar while on the contrary, nodes across communities present low similarity. Revealing the underlying community structure of directed complex networks has become a crucial and interdisciplinary topic with a plethora of relevant application domains. Therefore, naturally there is a recent wealth of research production in the area of mining directed graphs — with clustering being the primary method sought and the primary tool for community detection and evaluation. The goal of this paper is to offer an in-depth comparative review of the methods presented so far for clustering directed networks along with the relevant necessary methodological background and also related applications. The survey commences by offering a concise review of the fundamental concepts and methodological base on which graph clustering algorithms capitalize on. Then we present the relevant work along two orthogonal classifications. The first one is mostly concerned with the methodological principles of the clustering algorithms, while the second one approaches the methods from the viewpoint regarding the properties of a good cluster in a directed network. Further, we present methods and metrics for evaluating graph clustering results, demonstrate interesting application domains and provide promising future research directions."
}


@article{Li2018b,
  eprint        = {1801.07606},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/1801.07606.pdf:pdf},
  journal       = {32nd AAAI Conf. Artif. Intell. AAAI 2018},
  pages         = {3538--3545},
  year          = {2018},
  author        = {Li, Qimai and Han, Zhichao and Wu, Xiao Ming},
  isbn          = {9781577358008},
  archiveprefix = {arXiv},
  abstract      = {Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semi-supervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.},
  title         = {{Deeper insights into graph convolutional networks for semi-supervised learning}},
  arxivid       = {1801.07606}
}
@article{Liao2018,
  keywords      = {GNN Dataflow},
  year          = {2018},
  author        = {Liao, Renjie and Brockschmidt, Marc and Tarlow, Daniel and Gaunt, Alexander L. and Urtasun, Raquel and Zemel, Richard S.},
  abstract      = {We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with spectral partitioning and also propose a modified multi-seed flood fill for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.},
  title         = {{Graph partition neural networks for semi-supervised classification}},
  mendeley-tags = {GNN Dataflow},
  url           = {http://arxiv.org/abs/1803.06272},
  eprint        = {1803.06272},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded//Graph partition neural networks for semi-supervised classification.pdf:pdf},
  journal       = {6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings},
  month         = {mar},
  archiveprefix = {arXiv},
  arxivid       = {1803.06272}
}
@inproceedings{Ma2018a,
  title={AffinityNet: semi-supervised few-shot learning for disease type prediction},
  author={Ma, Tianle and Zhang, Aidong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={1069--1076},
  year={2019}
}
@inproceedings{Ma2019,
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded//Neugraph Parallel deep neural network computation on large graphs.pdf:pdf},
  pages         = {443--458},
  keywords      = {GNN Dataflow,GNN HW/SW requirements},
  year          = {2019},
  author        = {Ma, Lingxiao and Yang, Zhi and Miao, Youshan and Xue, Jilong and Wu, Ming and Zhou, Lidong and Dai, Yafei},
  isbn          = {9781939133038},
  abstract      = {Recent deep learning models have moved beyond low dimensional regular grids such as image, video, and speech, to high-dimensional graph-structured data, such as social networks, e-commerce user-item graphs, and knowledge graphs. This evolution has led to large graph-based neural network models that go beyond what existing deep learning frameworks or graph computing systems are designed for. We present NeuGraph, a new framework that bridges the graph and dataflow models to support efficient and scalable parallel neural network computation on graphs. NeuGraph introduces graph computation optimizations into the management of data partitioning, scheduling, and parallelism in dataflow-based deep learning frameworks. Our evaluation shows that, on small graphs that can fit in a single GPU, NeuGraph outperforms state-of-the-art implementations by a significant margin, while scaling to large real-world graphs that none of the existing frameworks can handle directly with GPUs.},
  booktitle     = {2019 {USENIX} Annual Technical Conference ({USENIX} {ATC} 19)},
  title         = {{Neugraph: Parallel deep neural network computation on large graphs}},
  mendeley-tags = {GNN Dataflow,GNN HW/SW requirements},
  
}
@article{Ma2019a,
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/High performance graph convolutional networks with applications in testability analysis.pdf:pdf},
  journal       = {Proceedings - Design Automation Conference},
  issn          = {0738100X},
  keywords      = {GNN applications},
  year          = {2019},
  author        = {Ma, Yuzhe and Ren, Haoxing and Khailany, Brucek and Sikka, Harbinder and Luo, Lijuan and Natarajan, Karthikeyan and Yu, Bei},
  isbn          = {9781450367257},
  abstract      = {Applications of deep learning to electronic design automation (EDA) have recently begun to emerge, although they have mainly been limited to processing of regular structured data such as images. However, many EDA problems require processing irregular structures, and it can be non-trivial to manually extract important features in such cases. In this paper, a high performance graph convolutional network (GCN) model is proposed for the purpose of processing irregular graph representations of logic circuits. A GCN classifier is firstly trained to predict observation point candidates in a netlist. The GCN classifier is then used as part of an iterative process to propose observation point insertion based on the classification results. Experimental results show the proposed GCN model has superior accuracy to classical machine learning models on difficult-to-observation nodes prediction. Compared with commercial testability analysis tools, the proposed observation point insertion flow achieves similar fault coverage with an 11{\%} reduction in observation points and a 6{\%} reduction in test pattern count.},
  title         = {{High performance graph convolutional networks with applications in testability analysis}},
  mendeley-tags = {GNN applications},
  doi           = {10.1145/3316781.3317838}
}
@article{Mahe2006,
  title={Graph kernels based on tree patterns for molecules},
  author={Mah{\'e}, Pierre and Vert, Jean-Philippe},
  journal={Machine learning},
  volume={75},
  number={1},
  pages={3--35},
  year={2009},
  publisher={Springer}
}
@article{Monfardini2006,
  keywords      = {GNN applications},
  year          = {2006},
  author        = {Monfardini, Gabriele and {Di Massa}, Vincenzo and Scarselli, Franco and Gori, Marco},
  isbn          = {9781586036423},
  abstract      = {Graph Neural Networks (GNNs) are a recently proposed connectionist model that extends previous neural methods to structured domains. GNNs can be applied on datasets that contain very general types of graphs and, under mild hypotheses, they have been proven to be universal approximators on graphical domains. Whereas most of the common approaches to graphs processing are based on a preliminary phase that maps each graph onto a simpler data type, like a vector or a sequence of reals, GNNs have the ability to directly process input graphs, thus embedding their connectivity into the processing scheme. In this paper, the main theoretical properties of GNNs are briefly reviewed and they are proposed as a tool for object localization. An experimentation has been carried out on the task of locating the face of a popular Walt Disney character in comic covers. In the dataset the character is shown in a number of different poses, often in cluttered backgrounds, and in high variety of colors. The proposed learning framework provides a way to deal with complex data arising from image segmentation process, without exploiting any prior knowledge on the dataset. The results are very encouraging, prove the viability of the method and the effectiveness of the structural representation of images. {\textcopyright} 2006 The authors.},
  title         = {{Graph neural networks for object localization}},
  mendeley-tags = {GNN applications},
  volume        = {141},
  number        = {August 2019},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graph neural networks for object localization.pdf:pdf},
  journal       = {Frontiers in Artificial Intelligence and Applications},
  pages         = {665--669},
  issn          = {09226389}
}
@article{Monti2017,
  keywords      = {GNN applications,GNN types},
  year          = {2017},
  author        = {Monti, Federico and Bronstein, Michael M. and Bresson, Xavier},
  abstract      = {Matrix completion models are among the most common formulations of recommender systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationary structures on user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines a novel multi-graph convolutional neural network that can learn meaningful statistical graph-structured patterns from users and items, and a recurrent neural network that applies a learnable diffusion on the score matrix. Our neural network system is computationally attractive as it requires a constant number of parameters independent of the matrix size. We apply our method on several standard datasets, showing that it outperforms state-of-the-art matrix completion techniques.},
  title         = {{Geometric matrix completion with recurrent multi-graph neural networks}},
  mendeley-tags = {GNN applications,GNN types},
  volume        = {2017-Decem},
  eprint        = {1704.06803},
  number        = {Nips},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Geometric matrix completion with recurrent multi-graph neural networks.pdf:pdf},
  journal       = {Advances in Neural Information Processing Systems},
  pages         = {3698--3708},
  issn          = {10495258},
  archiveprefix = {arXiv},
  arxivid       = {1704.06803}
}
@article{Myska2019,
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded//Graph neural network for website element detection.pdf:pdf},
  journal       = {2019 42nd International Conference on Telecommunications and Signal Processing, TSP 2019},
  pages         = {216--219},
  keywords      = {Deep learning,GNN applications,Graph neural network,Machine learning,Mark-up languages,Node classification},
  year          = {2019},
  author        = {Myska, Vojtech and Burget, Radim and Peter, Brezany},
  isbn          = {9781728118642},
  publisher     = {IEEE},
  abstract      = {Websites are a mixture of structured HTML tags, unstructured natural language and styling, which gives a wide range of possibilities how a website can look like. The paper introduces a website node detector based on the so-called graph neural networks - A new kind of neural networks, which are not working just with tensors like traditional neural networks do, but operates with graphs (or tree structures-special variations of graphs). To assess the accuracy of the proposed methodology, a privately collected and labeled data set was created. Although the data set used for the experiment is relatively limited, results on this limited data set suggest, that this methodology may be a promising path for automatic content generation.},
  title         = {{Graph neural network for website element detection}},
  mendeley-tags = {GNN applications},
  doi           = {10.1109/TSP.2019.8769036}
}
@Article{Nickel2015,
  author={M. {Nickel} and K. {Murphy} and V. {Tresp} and E. {Gabrilovich}},
  journal={Proceedings of the IEEE}, 
  title={A Review of Relational Machine Learning for Knowledge Graphs}, 
  year={2016},
  volume={104},
  number={1},
  pages={11-33}}

@Article{Niepert2016,
  mendeley-groups= {SOA Paper},
  keywords      = {GNN modelling},
  year          = {2016},
  author        = {Niepert, Mathias and Ahmad, Mohamed and Kutzkov, Konstantin},
  isbn          = {9781510829008},
  abstract      = {Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.},
  title         = {{Learning convolutional neural networks for graphs}},
  mendeley-tags = {GNN modelling},
  volume        = {4},
  eprint        = {1605.05273},
  journal       = {33rd Int. Conf. Mach. Learn. ICML 2016},
  pages         = {2958--2967},
  archiveprefix = {arXiv},
  arxivid       = {1605.05273}
}
@article{nikolentzos2017kernel,
 journal={arXiv preprint arXiv:1710.10689},
  year          = {2017},
  author        = {Giannis Nikolentzos and Polykarpos Meladianos and Antoine Jean-Pierre Tixier and Konstantinos Skianis and Michalis Vazirgiannis},
  archiveprefix = {arXiv},
  title         = {Kernel Graph Convolutional Neural Networks},
  primaryclass  = {cs.LG}
}
@article{Onoro-Rubio2017,
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Representation Learning for Visual-Relational Knowledge Graphs.pdf:pdf},
  journal       = {ArXiv e-prints},
  keywords      = {Computer Science - Artificial,Computer Science - Machine Learning,GNN modelling,Intelligence},
  year          = {2017},
  author        = {O{\~{n}}oro-Rubio, Daniel and Niepert, Mathias and Garc{\'{i}}a-Dur{\'{a}}n, Alberto and Gonz{\'{a}}lez, Roberto and L{\'{o}}pez-Sastre, Roberto J},
  abstract      = {A visual-relational knowledge graph (KG) is a multi-relational graph whose entities are associated with images. We introduce ImageGraph, a KG with 1,330 relation types, 14,870 entities, and 829,931 images. Visual- relational KGs lead to novel probabilistic query types where images are treated as first-class citizens. Both the prediction of relations between unseen images and multi-relational image retrieval can be formulated as query types in a visual-relational KG. We approach the problem of answering such queries with a novel combination of deep convolutional networks and models for learning knowledge graph embeddings. The resulting models can answer queries such as "How are these two unseen images related to each other?" We also explore a zero- shot learning scenario where an image of an entirely new entity is linked with multiple relations to entities of an existing KG. The multi- relational grounding of unseen entity images into a knowledge graph serves as the description of such an entity. We conduct experiments to demonstrate that the proposed deep architectures in combination with KG embedding objectives can answer the visual-relational queries efficiently and accurately.},
  title         = {{Representation Learning for Visual-Relational Knowledge Graphs}},
  mendeley-tags = {GNN modelling},
  url           = {https://ui.adsabs.harvard.edu/{\%}5C{\#}abs/2017arXiv170902314O}
}
@article{Pan2017,
  title={Adversarially regularized graph autoencoder for graph embedding},
  author={Pan, Shirui and Hu, Ruiqi and Long, Guodong and Jiang, Jing and Yao, Lina and Zhang, Chengqi},
  journal={arXiv preprint arXiv:1802.04407},
  year={2018}
}
@inproceedings{Park2019,
 title={Exploiting Interaction Links for Node Classification with Deep Graph Neural Networks.},
  author={Park, Hogun and Neville, Jennifer},
  booktitle={IJCAI},
  pages={3223--3230},
  year={2019}
}
@article{Paulheim2017,
  year          = {2017},
  author        = {Paulheim, Heiko},
  title         = {{Knowledge graph refinement: A survey of approaches and evaluation methods}},
  journal = {Semantic Web},
  volume = {8},
  number = {3},
  pages = {489--508}
 }
@article{Pineda,
  year          = {1987},
  author        = {Pineda, Fernando J},
  isbn          = {1079-7114},
  abstract      = {An adaptive neural network with asymmetric connections is introduced. This network is related to the Hopfield network with graded neurons and uses a recurrent generalization of the 6 rule of Rumelhart, Hinton, and Williams to modify adaptively the synaptic weights. The new network bears a resemblance to the master/slave network of Lapedes and Farber, but it is architecturally simpler.},
  pmid          = {10035458},
  title         = {{Generalization of Back-Propagation to Recurrent Neural Networks}},
  url           = {http://www.worldcat.org/title/kitab-i-aqdas/oclc/635987099},
  volume        = {59},
  number        = {19},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/PhysRevLett.59.2229.pdf:pdf},
  journal       = {Phys. Rev. Lett.},
  pages         = {2229--2232},
  issn          = {00319007},
  doi           = {https://doi.org/10.1103/PhysRevLett.59.2229}
}
@article{Pineda1987,
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Generalization of Backpropagation To Recurrent and High-Order Networks.pdf:pdf},
  pages         = {15},
  keywords      = {GNN complexity},
  year          = {1987},
  author        = {Pineda, Fernando J.},
  abstract      = {Summary form only given. A general method for deriving back-propagation algorithms for networks with feedback and higher order connectivity is introduced. As an example, this method is used to derive an algorithm for adaptively modifying the weights of a recurrent generalization of the network introduced by Rumelhart et al. The back-propagation involves the integration of an associated differential equation. The weight matrix is updated only after the forward-propagation, back-propagation, and differential equations have reached steady state.},
  title         = {{Generalization of Backpropagation To Recurrent and High-Order Networks.}},
  mendeley-tags = {GNN complexity}
}

@inproceedings{Adaptive,
	author = {Ruoyu Li and Sheng Wang and Feiyun Zhu and Junzhou Huang},
	title = {Adaptive Graph Convolutional Neural Networks},
	booktitle = {AAAI Conference on Artificial Intelligence},
	year = {2018},
	keywords = {graph CNN;spectral filter;metric learning},
	abstract = {Graph Convolutional Neural Networks (Graph CNNs) are generalizations of classical CNNs to handle graph data such as molecular data, point could and social networks. Current filters in graph CNNs are built for fixed and shared graph structure. However, for most real data, the graph structures varies in both size and connectivity. The paper proposes a generalized and flexible graph CNN taking data of arbitrary graph structure as input. In that way a task-driven adaptive graph is learned for each graph data while training. To efficiently learn the graph, a distance metric learning is proposed. Extensive experiments on nine graph-structured datasets have demonstrated the superior performance improvement on both convergence speed and predictive accuracy.},

	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16642}
}

@Article{Qiu2018,
  journal       = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  month         = {Jul},
  year          = {2018},
  author        = {Qiu, Jiezhong and Tang, Jian and Ma, Hao and Dong, Yuxiao and Wang, Kuansan and Tang, Jie},
  isbn          = {9781450355520},
  publisher     = {ACM},
  title         = {{DeepInf: Social Influence Prediction with Deep Learning}},
  url           = {http://dx.doi.org/10.1145/3219819.3220077},
  doi           = {10.1145/3219819.3220077}
}
@Article{recom,
  journal       = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  month         = {Jul},
  year          = {2018},
  author        = {Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
  isbn          = {9781450355520},
  publisher     = {ACM},
  title         = {{Graph Convolutional Neural Networks for Web-Scale Recommender Systems}},
  url           = {http://dx.doi.org/10.1145/3219819.3219890},
  doi           = {10.1145/3219819.3219890}
}
@Misc{representationLearning,
  journal       = {IEEE Data Engineering Bulletin, 40(3):52–74, 2017b},
  year          = {2017},
  author        = {William L. Hamilton and Rex Ying and Jure Leskovec},
  title         = {{Representation Learning on Graphs: Methods and Applications}}
}
@inproceedings{resnet,
  year          = {2016},
  author        = {K. {He \em et al.}},
  booktitle     = {CVPR},
  title         = {{Deep Residual Learning for Image Recognition}}
}
@Article{Rusek2019,
  journal       = {Proceedings of the 2019 ACM Symposium on SDN Research  - SOSR  ’19},
  year          = {2019},
  author        = {Rusek, Krzysztof and Suárez-Varela, José and Mestres, Albert and Barlet-Ros, Pere and Cabellos-Aparicio, Albert},
  isbn          = {9781450367103},
  publisher     = {ACM Press},
  title         = {{Unveiling the potential of Graph Neural Networks for network modeling and optimization in SDN}},
  url           = {http://dx.doi.org/10.1145/3314148.3314357},
  doi           = {10.1145/3314148.3314357}
}
@article{Rusek2019a,
  keywords      = {GNN applications,GNN types,Knowledge plane,machine learning,message-passing neural networks (MPNN),queuing networks,random graphs},
  year          = {2019},
  author        = {Rusek, Krzysztof and Cholda, Piotr},
  abstract      = {This letter presents a solution to the problem of universal representation of graphs exemplifying communication network topologies with the help of neural networks. The proposed approach is based on message-passing neural networks. The approach enables us to represent topologies and operational aspects of networks. The usefulness of the solution is illustrated with a case study of delay prediction in queuing networks. This shows that performance evaluation can be provided without having to apply complex modeling. In consequence, the proposed solution makes it possible to effectively apply the methods elaborated in the field of machine learning in communications.},
  title         = {{Message-Passing Neural Networks Learn Little's Law}},
  mendeley-tags = {GNN applications,GNN types},
  volume        = {23},
  eprint        = {1901.05748},
  number        = {2},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Message-Passing Neural Networks Learn Little's Law.pdf:pdf},
  journal       = {IEEE Communications Letters},
  pages         = {274--277},
  issn          = {15582558},
  month         = {feb},
  publisher     = {Institute of Electrical and Electronics Engineers Inc.},
  archiveprefix = {arXiv},
  arxivid       = {1901.05748},
  doi           = {10.1109/LCOMM.2018.2886259}
}
@article{Salha2019,
  title={Keep it simple: Graph autoencoders without graph convolutional networks},
  author={Salha, Guillaume and Hennequin, Romain and Vazirgiannis, Michalis},
  journal={arXiv preprint arXiv:1910.00942},
  year={2019}
}
@article{Sanchez-Gonzalez2018,
  keywords      = {GNN applications,GNN types},
  year          = {2018},
  author        = {Sanchez-Gonzalez, Alvaro and Heess, Nicolas and Springenberg, Jost Tobias and Merel, Josh and Riedmiller, Martin and Hadsell, Raia and Battaglia, Peter},
  isbn          = {9781510867963},
  abstract      = {Understanding and interacting with everyday physical scenes requires rich knowledge about the structure of the world, represented either implicitly in a value or policy function, or explicitly in a transition model. Here we introduce a new class of learnable models-based on graph networks-which implement an inductive bias for object- and relation-centric representations of complex, dynamical systems. Our results show that as a forward model, our approach supports accurate predictions from real and simulated data, and surprisingly strong and efficient generalization, across eight distinct physical systems which we varied parametrically and structurally. We also found that our inference model can perform system identification. Our models are also differentiate, and support online planning via gradientbased trajectory optimization, as well as offline policy optimization. Our framework offers new opportunities for harnessing and exploiting rich knowledge about the world, and takes a key step toward building machines with more human-like representations of the world.},
  title         = {{Graph networks as learnable physics engines for inference and control}},
  mendeley-tags = {GNN applications,GNN types},
  volume        = {10},
  eprint        = {1806.01242},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graph networks as learnable physics engines for inference and control.pdf:pdf},
  journal       = {35th International Conference on Machine Learning, ICML 2018},
  pages         = {7097--7117},
  archiveprefix = {arXiv},
  arxivid       = {1806.01242}
}
@article{Sanchez-Lengeling2019,
  title={Machine learning for scent: Learning generalizable perceptual representations of small molecules},
  author={Sanchez-Lengeling, Benjamin and others},
  journal={arXiv preprint arXiv:1910.10685},
  year={2019}
}
@article{Scarselli2005,
  volume        = {2005},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graph neural networks for ranking web pages.pdf:pdf},
  journal       = {Proceedings - 2005 IEEE/WIC/ACM InternationalConference on Web Intelligence, WI 2005},
  pages         = {666--672},
  keywords      = {GNN applications,GNN modelling},
  year          = {2005},
  author        = {Scarselli, Franco and Hagenbuchner, Markus and Yong, Sweah Liang and Tsoi, Ah Chung and Gori, Marco and Maggini, Marco},
  isbn          = {076952415X},
  abstract      = {An artificial neural network model, capable of processing general types of graph structured data, has recently been proposed. This paper applies the new model to the computation of customised page ranks problem in the World Wide Web. The class of customised page ranks that can be implemented in this way is very general and easy because the neural network model is learned by examples. Some preliminary experimental findings show that the model generalizes well over unseen web pages, and hence, may be suitable for the task of page rank computation on a large web graph. {\textcopyright} 2005 IEEE.},
  title         = {{Graph neural networks for ranking web pages}},
  mendeley-tags = {GNN applications,GNN modelling},
  doi           = {10.1109/WI.2005.67}
}
@article{Scarselli2009a,
  keywords      = {Approximation theory,GNN modelling,Graph neural networks (GNNs),Graphical domains,Universal approximators},
  year          = {2009},
  author        = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  abstract      = {In this paper, we will consider the approximation properties of a recently introduced neural network model called graph neural network (GNN), which can be used to process-structured data inputs, e.g., acyclic graphs, cyclic graphs, and directed or undirected graphs. This class of neural networks implements a function $\tau$(G,n) ∈ Rm that maps a graph G and one of its nodes n onto an m-dimensional Euclidean space. We characterize the functions that can be approximated by GNNs, in probability, up to any prescribed degree of precision. This set contains the maps that satisfy a property called preservation of the unfolding equivalence, and includes most of the practically useful functions on graphs; the only known exception is when the input graph contains particular patterns of symmetries when unfolding equivalence may not be preserved. The result can be considered an extension of the universal approximation property established for the classic feedforward neural networks (FNNs). Some experimental examples are used to show the computational capabilities of the proposed model. {\textcopyright} 2008 IEEE.},
  title         = {{Computational capabilities of graph neural networks}},
  mendeley-tags = {GNN modelling},
  volume        = {20},
  number        = {1},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Computational capabilities of graph neural networks.pdf:pdf},
  journal       = {IEEE Transactions on Neural Networks},
  pages         = {81--102},
  issn          = {10459227},
  doi           = {10.1109/TNN.2008.2005141}
}
@Article{Scarselli2009,
  volume        = {20},
  number        = {1},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/04700287.pdf:pdf},
  journal       = {IEEE Trans. Neural Networks},
  pages         = {61--80},
  issn          = {1045-9227},
  month         = {jan},
  year          = {2009},
  author        = {Scarselli, Franco and Gori, Marco and {Ah Chung Tsoi} and Hagenbuchner, Markus and Monfardini, G.},
  title         = {{The Graph Neural Network Model}},
  doi           = {10.1109/TNN.2008.2005605}
}
@article{Schlichtkrull2018,
  keywords      = {GNN Types,GNN applications,GNN modelling},
  year          = {2018},
  author        = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and van den Berg, Rianne and Titov, Ivan and Welling, Max},
  isbn          = {9783319934167},
  abstract      = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8{\%} on FB15k-237 over a decoder-only baseline.},
  title         = {{Modeling Relational Data with Graph Convolutional Networks}},
  mendeley-tags = {GNN Types,GNN applications,GNN modelling},
  url           = {http://arxiv.org/abs/1703.06103},
  volume        = {10843 LNCS},
  eprint        = {1703.06103},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Modeling Relational Data with Graph Convolutional Networks.pdf:pdf},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  pages         = {593--607},
  issn          = {16113349},
  month         = {mar},
  archiveprefix = {arXiv},
  arxivid       = {1703.06103},
  doi           = {10.1007/978-3-319-93417-4_38}
}
@book{Scholkopf,
 title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Schlkopf, Bernhard and Smola, Alexander J and Bach, Francis},
  year={2018},
  publisher={the MIT Press}
}
@misc{Son,
  title={Graph Neural Networks With Efficient Tensor Operations in CUDA/GPU and Graphflow Deep Learning Framework in C++ for Quantum Chemistry},
  author={Son, Hy Truong and Jones, Chris},
  year={2019},
  publisher={Accessed: Nov}
}
@article{StJohn2019,
  keywords      = {GNN applications,GNN complexity},
  year          = {2019},
  author        = {{St John}, Peter C. and Phillips, Caleb and Kemper, Travis W. and Wilson, A. Nolan and Guan, Yanfei and Crowley, Michael F. and Nimlos, Mark R. and Larsen, Ross E.},
  abstract      = {Machine learning methods have shown promise in predicting molecular properties, and given sufficient training data, machine learning approaches can enable rapid high-throughput virtual screening of large libraries of compounds. Graph-based neural network architectures have emerged in recent years as the most successful approach for predictions based on molecular structure and have consistently achieved the best performance on benchmark quantum chemical datasets. However, these models have typically required optimized 3D structural information for the molecule to achieve the highest accuracy. These 3D geometries are costly to compute for high levels of theory, limiting the applicability and practicality of machine learning methods in high-throughput screening applications. In this study, we present a new database of candidate molecules for organic photovoltaic applications, comprising approximately 91 000 unique chemical structures. Compared to existing datasets, this dataset contains substantially larger molecules (up to 200 atoms) as well as extrapolated properties for long polymer chains. We show that message-passing neural networks trained with and without 3D structural information for these molecules achieve similar accuracy, comparable to state-of-the-art methods on existing benchmark datasets. These results therefore emphasize that for larger molecules with practical applications, near-optimal prediction results can be obtained without using optimized 3D geometry as an input. We further show that learned molecular representations can be leveraged to reduce the training data required to transfer predictions to a new density functional theory functional.},
  title         = {{Message-passing neural networks for high-throughput polymer screening}},
  mendeley-tags = {GNN applications,GNN complexity},
  volume        = {150},
  eprint        = {1807.10363},
  number        = {23},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Message-passing neural networks for high-throughput polymer screening.pdf:pdf},
  journal       = {Journal of Chemical Physics},
  issn          = {00219606},
  month         = {jun},
  publisher     = {American Institute of Physics Inc.},
  archiveprefix = {arXiv},
  arxivid       = {1807.10363},
  doi           = {10.1063/1.5099132}
}
@article{Thekumparampil2018,
  title={Attention-based graph neural network for semi-supervised learning},
  author={Thekumparampil, Kiran K and others},
  journal={arXiv preprint arXiv:1803.03735},
  year={2018}
}
@inproceedings{Thomas2004,
  title={Cyclic pattern kernels for predictive graph mining},
  author={Horv{\'a}th, Tam{\'a}s and G{\"a}rtner, Thomas and Wrobel, Stefan},
  booktitle={Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={158--167},
  year={2004}
}
@inproceedings{Tms,
  editor        = {Sch{\"o}lkopf, Bernhard and Warmuth, Manfred K.},
  address       = {Berlin, Heidelberg},
  pages         = {129--143},
  year          = {2003},
  author        = {G{\"a}rtner, Thomas and Flach, Peter and Wrobel, Stefan},
  isbn          = {978-3-540-45167-9},
  publisher     = {Springer Berlin Heidelberg},
  abstract      = {As most `real-world' data is structured, research in kernel methods has begun investigating kernels for various kinds of structured data. One of the most widely used tools for modeling structured data are graphs. An interesting and important challenge is thus to investigate kernels on instances that are represented by graphs. So far, only very specific graphs such as trees and strings have been considered.},
  title         = {On Graph Kernels: Hardness Results and Efficient Alternatives},
  booktitle     = {Learning Theory and Kernel Machines}
}
@article{Vaswani2017,
  eprint        = {arXiv:1706.03762v5},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/1706.03762.pdf:pdf},
  journal       = {Nips},
  year          = {2017},
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  archiveprefix = {arXiv},
  title         = {{Attention Is All You Need}},
  arxivid       = {arXiv:1706.03762v5}
}

@inproceedings{skeleton,
	author = {Sijie Yan and Yuanjun Xiong and Dahua Lin},
	booktitle = {Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2018},
	keywords = {graph convolutional network; skeleton; action recognition},
	abstract = {Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.},

	url = {https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17135}
}

@article{Velickovic2018,
  eprint        = {1710.10903},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graph attention networks.pdf:pdf},
  journal       = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
  pages         = {1--12},
  keywords      = {GNN types},
  year          = {2018},
  author        = {Veli{\v{c}}kovi{\'{c}}, Petar and Casanova, Arantxa and Li{\`{o}}, Pietro and Cucurull, Guillem and Romero, Adriana and Bengio, Yoshua},
  archiveprefix = {arXiv},
  abstract      = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  title         = {{Graph attention networks}},
  arxivid       = {1710.10903},
  mendeley-tags = {GNN types}
}

@article{BrainGNNIB,
  title={BrainGNN: Interpretable Brain Graph Neural Network for fMRI Analysis},
  author={X. Li and Yuan Zhou and Siyuan Gao and N. Dvornek and M. Zhang and Juntang Zhuang and S. Gu and D. Scheinost and L. Staib and Pamela Ventola and J. Duncan},
  journal={bioRxiv},
  year={2020}
}

@article{Verma2019,
  keywords      = {GNN applications,Interpretability,Point-wise explanations,Ranking},
  year          = {2019},
  author        = {Verma, Manisha and Ganguly, Debasis},
  isbn          = {9781450361729},
  abstract      = {Information retrieval (IR) models often employ complex variations in term weights to compute an aggregated similarity score of a query-document pair. Treating IR models as black-boxes makes it difficult to understand or explain why certain documents are retrieved at top-ranks for a given query. Local explanation models have emerged as a popular means to understand individual predictions of classification models. However, there is no systematic investigation that learns to interpret IR models, which is in fact the core contribution of our work in this paper. We explore three sampling methods to train an explanation model and propose two metrics to evaluate explanations generated for an IR model. Our experiments reveal some interesting observations, namely that a) diversity in samples is important for training local explanation models, and b) the stability of a model is inversely proportional to the number of parameters used to explain the model.},
  title         = {{Graph Edit Distance Computation via Graph Neural Networks Yunsheng}},
  mendeley-tags = {GNN applications},
  eprint        = {arXiv:1808.05689v3},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graph Edit Distance Computation via Graph Neural Networks Yunsheng.pdf:pdf},
  journal       = {SIGIR 2019 - Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages         = {1281--1284},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1808.05689v3},
  doi           = {10.1145/nnnnnnn.nnnnnnn}
}
@article{Verma2019a,
  keywords      = {Deep learning,GNN complexity,Generalization guarantees,Graph convolutional neural networks,Graph mining,Stability},
  year          = {2019},
  author        = {Verma, Saurabh and Zhang, Zhi Li},
  isbn          = {9781450362016},
  abstract      = {Inspired by convolutional neural networks on 1D and 2D data, graph convolutional neural networks (GCNNs) have been developed for various learning tasks on graph data, and have shown superior performance on real-world datasets. Despite their success, there is a dearth of theoretical explorations of GCNN models such as their generalization properties. In this paper, we take a first step towards developing a deeper theoretical understanding of GCNN models by analyzing the stability of single-layer GCNN models and deriving their generalization guarantees in a semi-supervised graph learning setting. In particular, we show that the algorithmic stability of a GCNN model depends upon the largest absolute eigenvalue of its graph convolution filter. Moreover, to ensure the uniform stability needed to provide strong generalization guarantees, the largest absolute eigenvalue must be independent of the graph size. Our results shed new insights on the design of new {\&} improved graph convolution filters with guaranteed algorithmic stability. We evaluate the generalization gap and stability on various real-world graph datasets and show that the empirical results indeed support our theoretical findings. To the best of our knowledge, we are the first to study stability bounds on graph learning in a semi-supervised setting and derive generalization bounds for GCNN models.},
  title         = {{Stability and generalization of graph convolutional neural networks}},
  mendeley-tags = {GNN complexity},
  eprint        = {1905.01004},
  number        = {May},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stability and generalization of graph convolutional neural networks.pdf:pdf},
  journal       = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages         = {1539--1548},
  archiveprefix = {arXiv},
  arxivid       = {1905.01004},
  doi           = {10.1145/3292500.3330956}
}
@article{Wang2018,
  keywords      = {GNN types},
  year          = {2018},
  author        = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  isbn          = {9781538664209},
  abstract      = {Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our nonlocal models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.},
  title         = {{Non-local Neural Networks}},
  mendeley-tags = {GNN types},
  eprint        = {1711.07971},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Non-local Neural Networks.pdf:pdf},
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages         = {7794--7803},
  issn          = {10636919},
  archiveprefix = {arXiv},
  arxivid       = {1711.07971},
  doi           = {10.1109/CVPR.2018.00813}
}
@article{Weisfeiler1968,
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/wl{\_}paper{\_}translation.pdf:pdf},
  journal       = {Nauchno-Technicheskaya Informatsia , 2(9)},
  pages         = {2--16},
  year          = {1968},
  author        = {Weisfeiler, B Yu and Leman, A A},
  abstract      = {We consider an algorithm for the reduction of a given finite multigraph $\Gamma$ to canonical form. Therein the new invariant of a graph appears-the algebra A($\Gamma$). The study of properties of the algebra A($\Gamma$) turns out to be helpful in solving a number of graph-theoretic problems. We pose and discuss some conjectures on the relation between properties of the algebra A($\Gamma$) and the automorphism group Aut($\Gamma$) of a graph $\Gamma$. We give an example of undirected graph $\Gamma$ whose algebra A($\Gamma$) coincides with the group algebra of some noncommutative group. English abstract from the original article. An algorithm is considered, reducing the specified finite multigraph $\Gamma$ to canonical form. In the course of this reduction, a new invariant of the graph is generated-algebra A($\Gamma$). Study of the properties of the algebra A($\Gamma$) proves helpful in solving a number of graph-theoretic problems. Some propositions concerning the relationships between the properties of the algebra A($\Gamma$) and the graph's automorphism group Aut($\Gamma$) are discussed. An example of non-oriented graph $\Gamma$ is constructed whose algebra A($\Gamma$) coincides with the group algebra of a non-commutative group. English title from the original article. A reduction of a graph to canonical form and an algebra arising during this reduction. 1. Consider a finite graph $\Gamma$ and its adjacency matrix A($\Gamma$) = {\{}a ij {\}}, where a ij is the number of edges from ith vertex to jth one; i, j = 1, 2,. .. , n. If $\Gamma$ is an undirected graph then set a ij = a ji. A canonical form of a graph is defined to be its adjacency matrix with respect to a canonical labeling of its vertices, that is a partial ordering of the vertex set such that if vertices a and b are incomparable then there is an automorphism of a graph moving a to b and preserving the adjacency relation. In Sections 6 and 7, we describe the reduction of a graph to canonical form which consists of a step-by-step reordering of rows and columns of the matrix A($\Gamma$) and, roughly speaking, adds up to the following. Consider for simplicity an undirected graph without multiple edges. Associate with every vertex of the graph the characteristic vector which has one component equal to the number of neighbors of this vertex. Then divide vertices into classes such that vertices with equal characteristic vectors belong to the same class and order classes according to the natural order on the set of characteristic vectors. Further, associate with every vertex the characteristic vector v i = (l, v i1 , v i2 ,. . .), where v ik is the number of neighbors of vertex i from class k and l is the number of the class which contains vertex i. Now again divide vertices into classes according to new characteristic vectors ordered lexicographically, etc. Note that if vertices a and b belong to different classes and the condition a {\textless} b holds at some step then this condition also holds at the next steps. This implies that the described process stops after at most n steps and after the stop either all vertices belong to different classes (i.e. a canonical labeling was constructed) or further division does not proceed.},
  title         = {{A reduction of a graph to a canonical form and an algebra arising during this reduction.}}
}
@inproceedings{Wu2019a,
  keywords      = {Degree-specific Convolution,GNN complexity,Graph Isomorphism Test,Graph Neural Network,Multi-task Learning},
  year          = {2019},
  author        = {Wu, Jun and He, Jingrui and Xu, Jiejun},
  isbn          = {9781450362016},
  abstract      = {Graph data widely exist in many high-impact applications. Inspired by the success of deep learning in grid-structured data, graph neural network models have been proposed to learn powerful node-level or graph-level representation. However, most of the existing graph neural networks suffer from the following limitations: (1) there is limited analysis regarding the graph convolution properties, such as seed-oriented, degree-aware and order-free; (2) the node's degree-specific graph structure is not explicitly expressed in graph convolution for distinguishing structure-aware node neighborhoods; (3) the theoretical explanation regarding the graph-level pooling schemes is unclear. To address these problems, we propose a generic degree-specific graph neural network named DEMO-Net motivated by Weisfeiler-Lehman graph isomorphism test that recursively identifies 1-hop neighborhood structures. In order to explicitly capture the graph topology integrated with node attributes, we argue that graph convolution should have three properties: seed-oriented, degree-aware, order-free. To this end, we propose multi-task graph convolution where each task represents node representation learning for nodes with a specific degree value, thus leading to preserving the degree-specific graph structure. In particular, we design two multi-task learning methods: degree-specific weight and hashing functions for graph convolution. In addition, we propose a novel graph-level pooling/readout scheme for learning graph representation provably lying in a degree-specific Hilbert kernel space. The experimental results on several node and graph classification benchmark data sets demonstrate the effectiveness and efficiency of our proposed DEMO-Net over state-of-the-art graph neural network models.},
  title         = {{Demo-Net: Degree-specific graph neural networks for node and graph classification}},
  mendeley-tags = {GNN complexity},
  eprint        = {1906.02319},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Demo-Net Degree-specific graph neural networks for node and graph classification.pdf:pdf},
  pages         = {406--415},
  month         = {jul},
  publisher     = {Association for Computing Machinery},
  archiveprefix = {arXiv},
  booktitle     = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  arxivid       = {1906.02319},
  doi           = {10.1145/3292500.3330950}
}
@article{wu2019comprehensive,
  journal={arXiv preprint arXiv:1901.00596},
  year          = {2019},
  author        = {Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},
  archiveprefix = {arXiv},
  title         = {A Comprehensive Survey on Graph Neural Networks},
  primaryclass  = {cs.LG}
}
@Article{Wu2020,
  year          = {2020},
  author        = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  abstract      = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
  title         = {{A Comprehensive Survey on Graph Neural Networks}},
  eprint        = {1901.00596},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/09046288.pdf:pdf},
  journal       = {IEEE Trans. Neural Networks Learn. Syst.},
  pages         = {1--21},
  issn          = {2162-237X},
  publisher     = {IEEE},
  archiveprefix = {arXiv},
  arxivid       = {1901.00596},
  doi           = {10.1109/tnnls.2020.2978386}
}


@Article{Xu2019,
  mendeley-groups= {SOA Paper},
  eprint        = {1810.00826},
  journal       = {7th Int. Conf. Learn. Represent. ICLR 2019},
  pages         = {1--17},
  keywords      = {GNN complexity,GNN types},
  year          = {2019},
  author        = {Xu, Keyulu and Jegelka, Stefanie and Hu, Weihua and Leskovec, Jure},
  archiveprefix = {arXiv},
  abstract      = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  title         = {{How powerful are graph neural networks?}},
  arxivid       = {1810.00826},
  mendeley-tags = {GNN complexity,GNN types}
}


@article{Yan2018,
  eprint        = {arXiv:2001.10160v1},
  number        = {2},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/Yan et al. - 2020 - Characterizing and Understanding GCNs on GPU.pdf:pdf},
  year          = {2018},
  author        = {Yan, Mingyu and Chen, Zhaodong and Deng, Lei and Ye, Xiaochun and Zhang, Zhimin and Fan, Dongrui and Xie, Yuan},
  archiveprefix = {arXiv},
  title         = {{Characterizing and Understanding GCNs on GPU}},
  arxivid       = {arXiv:2001.10160v1}
}

@inproceedings{Yang2019,
  title={SPAGAN: Shortest Path Graph Attention Network.},
  author={Yang, Yiding and Wang, Xinchao and Song, Mingli and Yuan, Junsong and Tao, Dacheng},
  booktitle={IJCAI},
  pages={4099--4105},
  year={2019}
}
@Article{Ying2018,
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/3219819.3219890.pdf:pdf},
  journal       = {Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.},
  pages         = {974--983},
  year          = {2018},
  author        = {Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
  isbn          = {9781450355520},
  abstract      = {Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.},
  title         = {{Graph convolutional neural networks for web-scale recommender systems}},
  doi           = {10.1145/3219819.3219890}
}
@inproceedings{Ying2019,
  title={Gnnexplainer: Generating explanations for graph neural networks},
  author={Ying, Zhitao and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
  booktitle={Advances in neural information processing systems},
  pages={9244--9255},
  year={2019}
}
@inproceedings{Yun2019,
  title={Graph transformer networks},
  author={Yun, Seongjun and Jeong, Minbyul and Kim, Raehyun and Kang, Jaewoo and Kim, Hyunwoo J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11983--11993},
  year={2019}
}
@article{Zayats2018,
  title={Conversation modeling on Reddit using a graph-structured LSTM},
  author={Zayats, Victoria and Ostendorf, Mari},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={121--132},
  year={2018},
  publisher={MIT Press}
}
@article{Zhang2018,
  keywords      = {GNN applications,GNN modelling},
  year          = {2018},
  author        = {Zhang, Muhan and Chen, Yixin},
  abstract      = {Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a “heuristic” that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel -decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the -decaying theory, we propose a new method to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.},
  title         = {{Link prediction based on graph neural networks}},
  mendeley-tags = {GNN applications,GNN modelling},
  volume        = {2018-Decem},
  eprint        = {1802.09691},
  number        = {NeurIPS},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Link prediction based on graph neural networks.pdf:pdf},
  journal       = {Advances in Neural Information Processing Systems},
  pages         = {5165--5175},
  issn          = {10495258},
  archiveprefix = {arXiv},
  arxivid       = {1802.09691}
}
@inproceedings{Zhang2019,
 title={Dynamic graph message passing networks},
  author={Zhang, Li and Xu, Dan and Arnab, Anurag and Torr, Philip HS},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3726--3735},
  year={2020}
}

@misc{zhang2020simple,
    title={A Simple and General Graph Neural Network with Stochastic Message Passing},
    author={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},
    year={2020},
    eprint={2009.02562},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Zhang2020a,
  year          = {2020},
  author        = {Zhang, Ziwei and Cui, Peng and Zhu, Wenwu},
  abstract      = {Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.},
  title         = {{Deep Learning on Graphs: A Survey}},
  volume        = {14},
  eprint        = {1812.04202},
  number        = {8},
  file          = {:C$\backslash$:/Users/Akshay/Desktop/N3CAT{\_}Documents/GNNPapers/09039675.pdf:pdf},
  journal       = {IEEE Trans. Knowl. Data Eng.},
  pages         = {1--1},
  issn          = {1041-4347},
  publisher     = {IEEE},
  archiveprefix = {arXiv},
  arxivid       = {1812.04202},
  doi           = {10.1109/tkde.2020.2981333}
}
@article{Zhou2018a,
  title={Graph Neural Networks: A Review of Methods and Applications},
    author={Jie Zhou and Ganqu Cui and Zhengyan Zhang and Cheng Yang and Zhiyuan Liu and Lifeng Wang and Changcheng Li and Maosong Sun},
    year={2018},
    journal={arXiv preprint arXiv:1812.08434},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@Article{Zhu2018,
  mendeley-groups= {SOA Paper},
  keywords      = {Big geo-data,Deep neural networks,GNN applications,Graph convolution,Spatial pattern,Urban configuration},
  year          = {2018},
  author        = {Zhu, Di and Liu, Yu},
  isbn          = {9783959770835},
  abstract      = {The understanding of geographical reality is a process of data representation and pattern discovery. Former studies mainly adopted continuous-field models to represent spatial variables and to investigate the underlying spatial continuity/heterogeneity in a regular spatial domain. In this article, we introduce a more generalized model based on graph convolutional neural networks that can capture the complex parameters of spatial patterns underlying graph-structured spatial data, which generally contain both Euclidean spatial information and non-Euclidean feature information. A trainable site-selection framework is proposed to demonstrate the feasibility of our model in geographic decision problems.},
  title         = {{Modelling spatial patterns using graph convolutional networks}},
  mendeley-tags = {GNN applications},
  volume        = {114},
  eprint        = {1808.09802},
  journal       = {Leibniz Int. Proc. Informatics, LIPIcs},
  pages         = {1--11},
  issn          = {18688969},
  archiveprefix = {arXiv},
  arxivid       = {1808.09802},
  doi           = {10.4230/LIPIcs.GIScience.2018.73}
}
@article{Zugner2019,
  eprint        = {1902.08412},
  file          = {:home/jorge/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Adversarial attacks on graph neural networks via meta learning.pdf:pdf},
  journal       = {7th International Conference on Learning Representations, ICLR 2019},
  pages         = {1--15},
  keywords      = {GNN complexity},
  year          = {2019},
  author        = {Z{\"{u}}gner, Daniel and G{\"{u}}nnemann, Stephan},
  archiveprefix = {arXiv},
  abstract      = {Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.},
  title         = {{Adversarial attacks on graph neural networks via meta learning}},
  arxivid       = {1902.08412},
  mendeley-tags = {GNN complexity}
}

@inproceedings{asgari2020alrescha,
  title={Alrescha: A lightweight reconfigurable sparse-computation accelerator},
  author={Asgari, Bahar and Hadidi, Ramyad and Krishna, Tushar and Kim, Hyesoon and Yalamanchili, Sudhakar},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={249--260},
  year={2020},
  organization={IEEE}
}

@inproceedings{buffets,
author = {Pellauer, Michael and Shao, Yakun Sophia and Clemons, Jason and Crago, Neal and Hegde, Kartik and Venkatesan, Rangharajan and Keckler, Stephen W. and Fletcher, Christopher W. and Emer, Joel},
title = {Buffets: An Efficient and Composable Storage Idiom for Explicit Decoupled Data Orchestration},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304025},
doi = {10.1145/3297858.3304025},
abstract = {Accelerators spend significant area and effort on custom on-chip buffering. Unfortunately, these solutions are strongly tied to particular designs, hampering re-usability across other accelerators or domains. We present buffets, an efficient and composable storage idiom for the needs of accelerators that is independent of any particular design. Buffets have several distinguishing characteristics, including efficient decoupled fills and accesses with fine-grained synchronization, hierarchical composition, and efficient multi-casting. We implement buffets in RTL and show that they only add 2% control overhead over an 8KB RAM. When compared with DMA-managed double-buffered scratchpads and caches across a range of workloads, buffets improve energy-delay-product by 1.53x and 5.39x, respectively.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {137–151},
numpages = {15},
keywords = {synchronization, data orchestration, staging buffers, accelerators},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@article{10.1145/3296957.3173176,
author = {Kwon, Hyoukjun and Samajdar, Ananda and Krishna, Tushar},
title = {{MAERI}: Enabling Flexible Dataflow Mapping over {DNN} Accelerators via Reconfigurable Interconnects},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296957.3173176},
doi = {10.1145/3296957.3173176},
journal = {SIGPLAN Not.},
month = mar,
pages = {461–475},
numpages = {15},
keywords = {convolutional neural network, spatial architecture, network-on-chip, deep learning accelerator, recurrent neural network, machine learning}
}

  @article{osti_1089988,
title = {Toward a new metric for ranking high performance computing systems.},
author = {Heroux, Michael Allen and Dongarra, Jack.},
abstractNote = {The High Performance Linpack (HPL), or Top 500, benchmark [1] is the most widely recognized and discussed metric for ranking high performance computing systems. However, HPL is increasingly unreliable as a true measure of system performance for a growing collection of important science and engineering applications. In this paper we describe a new high performance conjugate gradient (HPCG) benchmark. HPCG is composed of computations and data access patterns more commonly found in applications. Using HPCG we strive for a better correlation to real scientific application performance and expect to drive computer system design and implementation in directions that will better impact performance improvement.},
doi = {10.2172/1089988},
url = {https://www.osti.gov/biblio/1089988}, journal = {Sandia National Labs Technical Report},
place = {United States},
year = {2013},
month = {6}
}

@INPROCEEDINGS{sigma,  author={Qin, Eric and Samajdar, Ananda and Kwon, Hyoukjun and Nadella, Vineet and Srinivasan, Sudarshan and Das, Dipankar and Kaul, Bharat and Krishna, Tushar},  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},   title={SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training},   year={2020},  volume={},  number={},  pages={58-70},  doi={10.1109/HPCA47549.2020.00015}}

@ARTICLE{eyeriss2,  author={Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel and Sze, Vivienne},  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},   title={Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices},   year={2019},  volume={9},  number={2},  pages={292-308},  doi={10.1109/JETCAS.2019.2910232}}

@inproceedings{10.1145/3079856.3080246,
author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080246},
doi = {10.1145/3079856.3080246},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {1–12},
numpages = {12},
keywords = {TensorFlow, RNN, TPU, neural network, GPU, deep learning, CNN, MLP, DNN, domain-specific architecture, LSTM, accelerator},
location = {Toronto, ON, Canada},
series = {ISCA ’17}
}

@article{o1980block,
  title={The block conjugate gradient algorithm and related methods},
  author={O'Leary, Dianne P},
  journal={Linear algebra and its applications},
  volume={29},
  pages={293--322},
  year={1980},
  publisher={Elsevier}
}
  
@INPROCEEDINGS{ucnn,  author={K. {Hegde} and J. {Yu} and R. {Agrawal} and M. {Yan} and M. {Pellauer} and C. {Fletcher}},  booktitle={2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)},   title={UCNN: Exploiting Computational Reuse in Deep Neural Networks via Weight Repetition},   year={2018},  volume={},  number={},  pages={674-687},}

@misc{nvdla,
    title = {{NVDLA deep learning accelerator}},
    howpublished = {\url{http://nvdla.org}},
    year = {2017}
}

@misc{hpcg2021, title = {{HPCG Results November 2023}}, howpublished={\url{https://www.hpcg-benchmark.org/custom/sc23.html}}
}

@inproceedings{martinmars,
author = {Maas, Martin and Beaugnon, Ulysse and Chauhan, Arun and Ilbeyi, Berkin},
title = {TelaMalloc: Efficient On-Chip Memory Allocation for Production Machine Learning Accelerators},
year = {2022},
isbn = {9781450399159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567955.3567961},
doi = {10.1145/3567955.3567961},
abstract = {Memory buffer allocation for on-chip memories is a major challenge in modern machine learning systems that target ML accelerators. In interactive systems such as mobile phones, it is on the critical path of launching ML-enabled applications. In data centers, it is part of complex optimization loops that run many times and are the limiting factor for the quality of compilation results.  

In contrast to the traditional memory allocation problem in languages such as C++, where allocation requests dynamically arrive as the application is executing, ML systems typically execute a static control flow graph that is known in advance. The task of the memory allocator is to choose buffer locations in device memory such that the total amount of used memory never exceeds the total memory available on-device. This is a high dimensional, NP-hard optimization problem that is challenging to solve.  

Today, ML frameworks approach this problem either using ad-hoc heuristics or solver-based methods. Heuristic solutions work for simple cases but fail for more complex instances of this problem. Solver-based solutions can handle these more complex instances, but are expensive and impractical in scenarios where memory allocation is on the critical path, such as on mobile devices that compile models on-the-fly. We encountered this problem in the development of Google's Pixel 6 phone, where some important models took prohibitively long to compile.  

We introduce an approach that solves this challenge by combining constraint optimization with domain-specific knowledge to achieve the best properties of both. We combine a heuristic-based search with a solver to guide its decision making. Our approach matches heuristics for simple inputs while being significantly faster than the best Integer Linear Program (ILP) solver-based approach for complex inputs. We also show how ML can be used to continuously improve the search for the long tail of workloads. Our approach is shipping in two production systems: Google's Pixel 6 phone and TPUv4. It achieves up to two orders of magnitude allocation time speed-up on real ML workloads compared to a highly-tuned production ILP approach that it replaces and enables important real-world models that could not otherwise be supported.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
pages = {123–137},
numpages = {15},
keywords = {Memory Allocation, Machine Learning, ML for Systems, ILP, CP},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@article{dongarra2015hpcg,
  title={Hpcg benchmark: a new metric for ranking high performance computing systems},
  author={Dongarra, Jack and Heroux, Michael A and Luszczek, Piotr},
  journal={Knoxville, Tennessee},
  pages={42},
  year={2015}
}

@inproceedings{fdmax,
author = {Li, Jiajun and Zhang, Yuxuan and Zheng, Hao and Wang, Ke},
title = {FDMAX: An Elastic Accelerator Architecture for Solving Partial Differential Equations},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589083},
doi = {10.1145/3579371.3589083},
abstract = {Partial Differential Equations (PDEs) are widely employed to describe natural phenomena in many science and engineering fields. Many PDEs do not have analytical solutions, hence, numerical methods have become prevalent for approximating PDE solutions. The most widely used numerical method is the Finite Difference Method (FDM), which requires fine grids and high-precision numerical iterations that are both compute- and memory-intensive. PDE-solving accelerators have been proposed in the literature, however, they usually focus on specific types of PDEs with rigid grid sizes which limits their broader applicability. Besides, they rarely provided insight into the optimizations of parallel computing and data accesses for solving PDEs, which hinders further improvements in performance and energy efficiency.This paper presents FDMAX, an elastic accelerator to efficiently support FDM for different types of PDEs with any grid size. FDMAX employs a customized Processing Element (PE) array architecture that maximizes data reuse with minimized interconnection overhead. The PE array can be reconfigured to break into a set of subarrays to adapt to different grid sizes for optimal efficiency. Moreover, the PE array exploits computation and data reuse for increased performance and energy efficiency, and is reconfigurable to support a wide range of PDEs such as elliptic, parabolic, and hyperbolic equations. Evaluated on four well-known PDEs, our simulation results show that FDMAX achieves on average 1189\texttimes{} speedup with 1123\texttimes{} energy reduction over Intel Xeon CPU, and 4.9\texttimes{} speedup with 6.3\texttimes{} energy reduction over NVIDIA RTX3090 GPU, and 2.9\texttimes{} speedup over Alrescha, the state-of-the-art PDE-solving accelerator.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {48},
numpages = {12},
keywords = {dataflow architecture, accelerator, partial differential equations},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@article{bicgstab,
author = {van der Vorst, H. A.},
title = {Bi-CGSTAB: A Fast and Smoothly Converging Variant of Bi-CG for the Solution of Nonsymmetric Linear Systems},
journal = {SIAM Journal on Scientific and Statistical Computing},
volume = {13},
number = {2},
pages = {631-644},
year = {1992},
doi = {10.1137/0913035},

URL = { 
    
        https://doi.org/10.1137/0913035
    
    

},
eprint = { 
    
        https://doi.org/10.1137/0913035
         }}
    

@article{cools2017communication,
  title={The communication-hiding pipelined BiCGStab method for the parallel solution of large unsymmetric linear systems},
  author={Cools, Siegfried and Vanroose, Wim},
  journal={Parallel Computing},
  volume={65},
  pages={1--20},
  year={2017},
  publisher={Elsevier}
}

@misc{graphcore,
    title = {{Graphcore IPU}},
    note = {https://www.graphcore.ai/products/ipu}
}



@inproceedings{greta,
  author    = "Kevin Kiningham and Philip Levis and Christopher Re",
  title     = "{GReTA: Hardware Optimized Graph Processing for GNNs}",
  booktitle = "{Proceedings of the Workshop on Resource-Constrained Machine Learning (ReCoML 2020)}",
  year      = {2020},
  month     = {March}
}

@article{kearnes,
author = {Kearnes, Steven and McCloskey, Kevin and Berndl, Marc and Pande, Vijay and Riley, Patrick},
year = {2016},
month = {03},
pages = {},
title = {Molecular Graph Convolutions: Moving Beyond Fingerprints},
volume = {30},
journal = {Journal of Computer-Aided Molecular Design},
doi = {10.1007/s10822-016-9938-8}
}


@inproceedings{shi,
author = {Du, Zidong and Fasthuber, Robert and Chen, Tianshi and Ienne, Paolo and Li, Ling and Luo, Tao and Feng, Xiaobing and Chen, Yunji and Temam, Olivier},
title = {ShiDianNao: Shifting Vision Processing Closer to the Sensor},
year = {2015},
isbn = {9781450334020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749469.2750389},
doi = {10.1145/2749469.2750389},
abstract = {In recent years, neural network accelerators have been shown to achieve both high
energy efficiency and high performance for a broad application scope within the important
category of recognition and mining applications.Still, both the energy efficiency
and performance of such accelerators remain limited by memory accesses. In this paper,
we focus on image applications, arguably the most important category among recognition
and mining applications. The neural networks which are state-of-the-art for these
applications are Convolutional Neural Networks (CNN), and they have an important property:
weights are shared among many neurons, considerably reducing the neural network memory
footprint. This property allows to entirely map a CNN within an SRAM, eliminating
all DRAM accesses for weights. By further hoisting this accelerator next to the image
sensor, it is possible to eliminate all remaining DRAM accesses, i.e., for inputs
and outputs.In this paper, we propose such a CNN accelerator, placed next to a CMOS
or CCD sensor. The absence of DRAM accesses combined with a careful exploitation of
the specific data access patterns within CNNs allows us to design an accelerator which
is 60× more energy efficient than the previous state-of-the-art neural network
accelerator. We present a full design down to the layout at 65 nm, with a modest footprint
of 4.86mm2 and consuming only 320mW, but still about 30\texttimes{} faster than high-end GPUs.},
booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
pages = {92–104},
numpages = {13},
location = {Portland, Oregon},
series = {ISCA '15}
}

@article{gnnadvisor,
    title={{GNNAdvisor:} An Efficient Runtime System for GNN Acceleration on GPUs},
    author={Yuke Wang and Boyuan Feng and Gushu Li and Shuangchen Li and Lei Deng and Yuan Xie and Yufei Ding},
    year={2020},
    journal={arXiv preprint arXiv:2006.06608},
    archivePrefix={arXiv},
    primaryClass={cs.DC}
}

@article{Botvinick2019,
abstract = {Deep reinforcement learning (RL)methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient – that is, it may simply be too slow – to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning.},
author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X. and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
doi = {10.1016/j.tics.2019.02.006},
file = {:C$\backslash$:/Users/sergi/Documents/Mendeley Desktop/Botvinick et al. - 2019 - Reinforcement Learning, Fast and Slow.pdf:pdf},
issn = {1879307X},
journal = {Trends in Cognitive Sciences},
mendeley-groups = {POSTDOC/NEURAL NETWORKS / ML/DRL},
number = {5},
pages = {408--422},
pmid = {31003893},
publisher = {Elsevier Ltd},
title = {{Reinforcement Learning, Fast and Slow}},
url = {https://doi.org/10.1016/j.tics.2019.02.006},
volume = {23},
year = {2019}
}

@article{Lecun2015,
abstract = {Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.},
archivePrefix = {arXiv},
arxivId = {1807.07987},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {1807.07987},
file = {:C$\backslash$:/Users/sergi/Documents/Mendeley Desktop/Lecun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
isbn = {9781450358095},
issn = {14764687},
journal = {Nature},
mendeley-groups = {POSTDOC/NEURAL NETWORKS / ML/BASICS},
number = {7553},
pages = {436--444},
pmid = {10463930},
title = {{Deep learning}},
volume = {521},
year = {2015}
}

@article{Esteva2017,
abstract = {Skin cancer, the most common human malignancy, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs) show potential for general and highly variable tasks across many fine-grained object categories. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images-two orders of magnitude larger than previous datasets-consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care.},
author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
doi = {10.1038/nature21056},
file = {:C$\backslash$:/Users/sergi/Documents/Mendeley Desktop/Esteva et al. - 2017 - Dermatologist-level classification of skin cancer with deep neural networks.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
mendeley-groups = {POSTDOC/NEURAL NETWORKS / ML/BASICS},
number = {7639},
pages = {115--118},
pmid = {28117445},
publisher = {Nature Publishing Group},
title = {{Dermatologist-level classification of skin cancer with deep neural networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28117445},
volume = {542},
year = {2017}
}

@article{Zhang2020b,
abstract = {Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This letter tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.},
author = {Zhang, Zhihui and Leng, Jingwen and Ma, Lingxiao and Miao, Youshan and Li, Chao and Guo, Minyi},
doi = {10.1109/LCA.2020.2988991},
issn = {15566064},
journal = {IEEE Computer Architecture Letters},
keywords = {Characterization,Computation analysis,Deep learning,Graph neural networks},
number = {1},
pages = {59--62},
title = {{Architectural Implications of Graph Neural Networks}},
volume = {19},
year = {2020}
}

    @article{Nagasaka2019,
abstract = {Graph Convolutional Networks (GCNs) are recently getting much attention in bioinformatics and chemoinformatics as a state-of-the-art machine learning approach with high accuracy. GCNs process convolutional operations along with graph structures, and GPUs are used to process enormous operations including sparse-dense matrix multiplication (SpMM) when the graph structure is expressed as an adjacency matrix with sparse matrix format. However, the SpMM operation on small graph, where the number of nodes is tens or hundreds, hardly exploits high parallelism or compute power of GPU. Therefore, SpMM becomes a bottleneck of training and inference in GCNs applications. In order to improve the performance of GCNs applications, we propose new SpMM algorithm especially for small sparse matrix and Batched SpMM, which exploits high parallelism of GPU by processing multiple SpMM operations with single CUDA kernel. To the best of our knowledge, this is the first work of batched approach for SpMM. We evaluated the performance of the GCNs application on TSUBAME3.0 implementing NVIDIA Tesla P100 GPU, and our batched approach shows significant speedups of up to 1.59x and 1.37x in training and inference, respectively.},
annote = {Software to improve sparse matrix multiplication efficacy in GPUs, only if we have multiple small of such matrices (only in chemistry use cases)},
archivePrefix = {arXiv},
arxivId = {1903.11409},
author = {Nagasaka, Yusuke and Nukada, Akira and Kojima, Ryosuke and Matsuoka, Satoshi},
doi = {10.1109/CCGRID.2019.00037},
eprint = {1903.11409},
file = {:C$\backslash$:/Users/sergi/Documents/Mendeley Desktop/Nagasaka et al. - 2019 - Batched sparse matrix multiplication for accelerating graph convolutional networks(2).pdf:pdf},
isbn = {9781728109121},
journal = {Proceedings - 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGrid 2019},
keywords = {Batched,GCN,Graph Convolution,SpMM},
mendeley-groups = {POSTDOC/WiPLASH/GNN},
pages = {231--240},
title = {{Batched sparse matrix multiplication for accelerating graph convolutional networks}},
year = {2019}
}

@article{Gao_2018,
   title={Large-Scale Learnable Graph Convolutional Networks},
   ISBN={9781450355520},
   url={http://dx.doi.org/10.1145/3219819.3219947},
   DOI={10.1145/3219819.3219947},
   journal={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   publisher={ACM},
   author={Gao, Hongyang and Wang, Zhengyang and Ji, Shuiwang},
   year={2018},
   month={Jul}
}


@inproceedings{Jia2019,
author = {Jia, Zhihao and Lin, Sina and Gao, Mingyu and Zaharia, Matei and Aiken, Alex},
booktitle = {Proceedings of MLSys '20},
file = {:C$\backslash$:/Users/sergi/Documents/Mendeley Desktop/Jia et al. - 2020 - Improving the accuracy, scalability, and performance of Graph Neural Networks with ROC.pdf:pdf},
mendeley-groups = {POSTDOC/WiPLASH/GNN},
title = {{Improving the accuracy, scalability, and performance of Graph Neural Networks with ROC}},
year = {2020}
}

@article{Tian2020,
abstract = {Inspired by the successes of convolutional neural networks (CNN) in computer vision, the convolutional operation has been moved beyond low-dimension grids (e.g., images) to high-dimensional graph-structured data (e.g., web graphs, social networks), leading to graph convolutional network (GCN). And GCN has been gaining popularity due to its success in real-world applications such as recommendation, natural language processing, etc. Because neural network and graph propagation have high computation complexity, GPUs have been introduced to both neural network training and graph processing. However, it is notoriously difficult to perform efficient GCN computing on data parallel hardware like GPU due to the sparsity and irregularity in graphs. In this paper, we present PCGCN, a novel and general method to accelerate GCN computing by taking advantage of the locality in graphs. We experimentally demonstrate that real-world graphs usually have the clustering property that can be used to enhance the data locality in GCN computing. Then, PCGCN proposes to partition the whole graph into chunks according to locality and process subgraphs with a dual-mode computing strategy which includes a selective and a full processing methods for sparse and dense subgraphs, respectively. Compared to existing state-of-the-art implementations of GCN on real-world and synthetic datasets, our implementation on top of TensorFlow achieves up to 8.8× speedup over the fastest one of the baselines.},
annote = {Software acceleration via pre-processing of graph.},
author = {Tian, Chao and Ma, Lingxiao and Yang, Zhi and Dai, Yafei},
doi = {10.1109/IPDPS47924.2020.00100},
file = {:C$\backslash$:/Users/sergi/Documents/Mendeley Desktop/Tian et al. - 2020 - PCGCN Partition-Centric Processing for Accelerating Graph Convolutional Network.pdf:pdf},
isbn = {9781728168760},
journal = {Proceedings of IPDPS 2020},
keywords = {GPU,deep learning,graph,graph convolutional network (GCN)},
mendeley-groups = {POSTDOC/WiPLASH/GNN},
pages = {936--945},
title = {{PCGCN: Partition-Centric Processing for Accelerating Graph Convolutional Network}},
year = {2020}
}


@inproceedings{Jia2020,
author = {Jia, Zhihao and Lin, Sina and Ying, Rex and Aiken, Alex},
booktitle = {Proceedings of the KDD '20},
file = {:C$\backslash$:/Users/sergi/Documents/Mendeley Desktop/Jia et al. - 2020 - Redundancy-Free Computation for Graph Neural Networks.pdf:pdf},
isbn = {9781450379984},
keywords = {Graph neural network,acm reference format,and alex aiken,graph neural network,jiaxuan you,jure leskovec,redundancy-free computation,rex ying,sina lin,zhihao jia},
mendeley-groups = {POSTDOC/WiPLASH/GNN},
title = {{Redundancy-Free Computation for Graph Neural Networks}},
year = {2020}
}

@article{tkaczyk2015cermine,
  title={CERMINE: automatic extraction of structured metadata from scientific literature},
  author={Tkaczyk, Dominika and Szostek, Pawe{\l} and Fedoryszak, Mateusz and Dendek, Piotr Jan and Bolikowski, {\L}ukasz},
  journal={International Journal on Document Analysis and Recognition (IJDAR)},
  volume={18},
  number={4},
  pages={317--335},
  year={2015},
  publisher={Springer}
}

@inproceedings{webber2012programmatic,
  title={A programmatic introduction to neo4j},
  author={Webber, Jim},
  booktitle={Proceedings of the 3rd annual conference on Systems, programming, and applications: software for humanity},
  pages={217--218},
  year={2012}
}



@article{wang2019deep,
  title={Deep graph library: Towards efficient and scalable deep learning on graphs},
  author={Wang, Minjie and Yu, Lingfan and Zheng, Da and Gan, Quan and Gai, Yu and Ye, Zihao and Li, Mufei and Zhou, Jinjing and Huang, Qi and Ma, Chao and Huang, Ziyue and Guo, Qipeng and Zhang, Hao and Lin, Haibin and Zhao, Junbo and Li, Jinyang and Smola, {Alexander J.} and Zhang, Zheng},
  journal={arXiv preprint arXiv:1909.01315},
  year={2019}
}
@article{fey2019fast,
  title={Fast graph representation learning with PyTorch Geometric},
  author={Fey, Matthias and Lenssen, Jan Eric},
  journal={arXiv preprint arXiv:1903.02428},
  year={2019}
}

@article{grattarola2020graph,
  title={Graph Neural Networks in TensorFlow and Keras with Spektral},
  author={Grattarola, Daniele and Alippi, Cesare},
  journal={arXiv preprint arXiv:2006.12138},
  year={2020}
}

@article{Zhu2018ali,
abstract = {An increasing number of machine learning tasks require dealing with large graph datasets, which capture rich and complex relationship among potentially billions of elements. Graph Neural Network (GNN) becomes an effective way to address the graph learning problem by converting the graph data into a low dimensional space while keeping both the structural and property information to the maximum extent and constructing a neural network for training and referencing. However, it is challenging to provide an efficient graph storage and computation capabilities to facilitate GNN training and enable development of new GNN algorithms. In this paper, we present a comprehensive graph neural network system, namely AliGraph, which consists of distributed graph storage, optimized sampling operators and runtime to efficiently support not only existing popular GNNs but also a series of in-house developed ones for different scenarios. The system is currently deployed at Alibaba to support a variety of business scenarios, including product recommendation and personalized search at Alibaba's E-Commerce platform. By conducting extensive experiments on a real-world dataset with 492.90 million vertices, 6.82 billion edges and rich attributes, Ali- Graph performs an order of magnitude faster in terms of graph building (5 minutes vs hours reported from the state-of-the-art PowerGraph platform). At training, AliGraph runs 40{\%}-50{\%} faster with the novel caching strategy and demonstrates around 12 times speed up with the improved runtime. In addition, our in-house developed GNN models all showcase their statistically significant superiorities in terms of both effectiveness and efficiency (e.g., 4.12{\%}-17.19{\%} lift by F1 scores).},
archivePrefix = {arXiv},
arxivId = {1902.08730},
author = {Zhu, Rong and Zhao, Kun and Yang, Hongxia and Lin, Wei and Zhou, Chang and Ai, Baole and Li, Yong and Zhou, Jingren},
doi = {10.14778/3352063.3352127},
eprint = {1902.08730},
file = {:C$\backslash$:/Users/sergi/Documents/Mendeley Desktop/Zhu et al. - 2018 - AliGraph A comprehensive graph neural network platform.pdf:pdf},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
mendeley-groups = {POSTDOC/WiPLASH,POSTDOC/WiPLASH/GNN},
number = {12},
pages = {2094--2105},
title = {{AliGraph: A comprehensive graph neural network platform}},
volume = {12},
year = {2018}
}

@article{Kiningham2020,
  title={GRIP: a graph neural network accelerator architecture},
  author={Kiningham, Kevin and Re, Christopher and Levis, Philip},
  journal={arXiv preprint arXiv:2007.13828},
  year={2020}
}



@ARTICLE{sambanova,  author={Emani, Murali and Vishwanath, Venkatram and Adams, Corey and Papka, Michael E. and Stevens, Rick and Florescu, Laura and Jairath, Sumti and Liu, William and Nama, Tejas and Sujeeth, Arvind},  journal={Comput Sci Eng},   title={Accelerating Scientific Applications With SambaNova Reconfigurable Dataflow architecture},   year={2021},  volume={23},  number={2},  pages={114-119},  doi={10.1109/MCSE.2021.3057203}}



@article{Zeng2020,
abstract = {Graph Convolutional Networks (GCNs) have emerged as the stateof- the-art deep learning model for representation learning on graphs. It is challenging to accelerate training of GCNs, due to (1) substantial and irregular data communication to propagate information within the graph, and (2) intensive computation to propagate information along the neural network layers. To address these challenges, we design a novel accelerator for training GCNs on CPU-FPGA heterogeneous systems, by incorporating multiple algorithm-architecture co-optimizations. We first analyze the computation and communication characteristics of various GCN training algorithms, and select a subgraph-based algorithm that is well suited for hardware execution. To optimize the feature propagation within subgraphs, we propose a light-weight pre-processing step based on a graph theoretic approach. Such pre-processing performed on the CPU significantly reduces the memory access requirements and the computation to be performed on the FPGA. To accelerate the weight update in GCN layers, we propose a systolic array based design for efficient parallelization. We integrate the above optimizations into a complete hardware pipeline, and analyze its load-balance and resource utilization by accurate performance modeling. We evaluate our design on a Xilinx Alveo U200 board hosted by a 40-core Xeon server. On three large graphs, we achieve an order of magnitude training speedup with negligible accuracy loss, compared with state-of-the-art implementation on a multi-core platform.},
archivePrefix = {arXiv},
arxivId = {2001.02498},
author = {Zeng, Hanqing and Prasanna, Viktor},
doi = {10.1145/3373087.3375312},
eprint = {2001.02498},
file = {:C$\backslash$:/Users/sergi/Documents/Mendeley Desktop/Zeng, Prasanna - 2020 - GraphACT Accelerating GCN training on CPU-FPGA heterogeneous platforms.pdf:pdf},
isbn = {9781450370998},
journal = {FPGA 2020 - 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
mendeley-groups = {POSTDOC/WiPLASH/GNN},
pages = {255--265},
title = {{GraphACT: Accelerating GCN training on CPU-FPGA heterogeneous platforms}},
year = {2020}
}


@inproceedings{GEOM,
file = {:C$\backslash$:/Users/sergi/Documents/Mendeley Desktop/Unknown - 2020 - Geom A Hierarchical Spatial Architecture with Hybrid Dataflows for Graph Learning.pdf:pdf},
mendeley-groups = {POSTDOC/WiPLASH/GNN},
pages = {1--13},
title = {{Geom: A Hierarchical Spatial Architecture with Hybrid Dataflows for Graph Learning}},
year = {2020}
}



@inproceedings{Zhang2020acc,
author = {Zhang, Bingyi and Zeng, Hanqing and Prasanna, Viktor},
booktitle = {Proceedings of the ASAP '20},
doi = {10.1109/asap49362.2020.00019},
file = {:C$\backslash$:/Users/sergi/Documents/Mendeley Desktop/Zhang, Zeng, Prasanna - 2020 - Hardware Acceleration of Large Scale GCN Inference.pdf:pdf},
mendeley-groups = {POSTDOC/WiPLASH/GNN},
pages = {61--68},
title = {{Hardware Acceleration of Large Scale GCN Inference}},
year = {2020}
}

@misc{GNets,
title = {{Build Graph Nets in Tensorflow}},
url = {https://github.com/deepmind/graph_nets},
year = {2019}
}

@article{Sze2017,
author = {Sze, V and Chen, YH and Yang, TJ and Emer, Joel S},
file = {:C$\backslash$:/Users/sergi/Documents/Mendeley Desktop/Sze et al. - 2017 - Efficient processing of deep neural networks A tutorial and survey.pdf:pdf},
journal = {Proceedings of the IEEE},
keywords = {asic,computer architecture,convolutional,dataflow processing,deep,deep learning,energy-efficient accelerators,low power,machine learning,neural networks,spatial architectures,vlsi},
mendeley-groups = {POSTDOC/NEURAL NETWORKS / ML/BASICS},
number = {12},
pages = {2295--2329},
title = {{Efficient processing of deep neural networks: A tutorial and survey}},
volume = {105},
year = {2017}
}

@article{METIS,
  title={A fast and high quality multilevel scheme for partitioning irregular graphs},
  author={Karypis, George and Kumar, Vipin},
  journal={SIAM Journal on scientific Computing},
  volume={20},
  number={1},
  pages={359--392},
  year={1998},
  publisher={SIAM}
}

@inproceedings{sukhbaatar2016learning,
  title={Learning multiagent communication with backpropagation},
  author={Sukhbaatar, Sainbayar and Fergus, Rob and others},
  booktitle={Advances in neural information processing systems},
  pages={2244--2252},
  year={2016}
}


@article{jia2019beyond,
  title={Beyond Data and Model Parallelism for Deep Neural Networks},
  author={Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  journal={SysML 2019},
  year={2019}
}

@inproceedings{wang2016gunrock,
  title={Gunrock: A high-performance graph processing library on the GPU},
  author={Wang, Yangzihao and Davidson, Andrew and Pan, Yuechao and Wu, Yuduo and Riffel, Andy and Owens, John D},
  booktitle={Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={1--12},
  year={2016}
}

@article{hu2020open,
  title={Open graph benchmark: Datasets for machine learning on graphs},
  author={Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  journal={arXiv preprint arXiv:2005.00687},
  year={2020}
}

@article{huang2020ge,
  title={GE-SpMM: General-purpose Sparse Matrix-Matrix Multiplication on GPUs for Graph Neural Networks},
  author={Huang, Guyue and Dai, Guohao and Wang, Yu and Yang, Huazhong},
  journal={arXiv preprint arXiv:2007.03179},
  year={2020}
}

@article{hu2020featgraph,
  title={FeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems},
  author={Hu, Yuwei and Ye, Zihao and Wang, Minjie and Yu, Jiali and Zheng, Da and Li, Mu and Zhang, Zheng and Zhang, Zhiru and Wang, Yida},
  journal={arXiv preprint arXiv:2008.11359},
  year={2020}
}


@inproceedings{wang2020gnn,
  title={GNN-PIM: A Processing-in-Memory Architecture for Graph Neural Networks},
  author={Wang, Zhao and Guan, Yijin and Sun, Guangyu and Niu, Dimin and Wang, Yuhao and Zheng, Hongzhong and Han, Yinhe},
  booktitle={Conference on Advanced Computer Architecture},
  pages={73--86},
  year={2020},
  organization={Springer}
}

@article{zhang2020agl,
  title={AGL: a Scalable System for Industrial-purpose Graph Machine Learning},
  author={Zhang, Dalong and Huang, Xin and Liu, Ziqi and Hu, Zhiyang and Song, Xianzheng and Ge, Zhibang and Zhang, Zhiqiang and Wang, Lin and Zhou, Jun and Qi, Yuan},
  journal={arXiv preprint arXiv:2003.02454},
  year={2020}
}

@article{you2018graphrnn,
  title={Graphrnn: Generating realistic graphs with deep auto-regressive models},
  author={You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William L and Leskovec, Jure},
  journal={arXiv preprint arXiv:1802.08773},
  year={2018}
}

@inproceedings{ying2018hierarchical,
  title={Hierarchical graph representation learning with differentiable pooling},
  author={Ying, Zhitao and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, Will and Leskovec, Jure},
  booktitle={Advances in neural information processing systems},
  pages={4800--4810},
  year={2018}
}

@article{dave2020hardware, 
  author={Dave, Shail and Baghdadi, Riyadh and Nowatzki, Tony and Avancha, Sasikanth and Shrivastava, Aviral and Li, Baoxin},
  journal={Proceedings of the IEEE}, 
  title={Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights}, 
  year={2021},
  volume={109},
  number={10},
  pages={1706-1752},
  doi={10.1109/JPROC.2021.3098483}
}

@article{Kwon2018c,
author = {Kwon, Hyoukjun and Samajdar, Ananda and Krishna, Tushar},
doi = {10.1109/MM.2018.2877289},
issn = {19374143},
journal = {IEEE Micro},
number = {6},
pages = {25--35},
title = {{A Communication-centric Approach for Designing Flexible DNN Accelerators}},
volume = {38},
year = {2018}
}

@inproceedings{Kwon2017,
author = {Kwon, Hyoukjun and Krishna, Tushar},
booktitle = {Proceedings of the NoCS '17},
doi = {10.1145/3130218.3130230},
isbn = {9781450349840},
pages = {Art. 19},
title = {{Rethinking NoCs for Spatial Neural Network Accelerators}},
year = {2017}
}

@inproceedings{wang2016cnn,
  title={{CNN-RNN}: A unified framework for multi-label image classification},
  author={Wang, Jiang and Yang, Yi and Mao, Junhua and Huang, Zhiheng and Huang, Chang and Xu, Wei},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2285--2294},
  year={2016}
}

@inproceedings{Lerer2019,
archivePrefix = {arXiv},
arxivId = {1903.12287},
author = {Lerer, Adam and Wu, Ledell and Shen, Jiajun and Lacroix, Timothee and Wehrstedt, Luca and Bose, Abhijit and Peysakhovich, Alex},
booktitle = {Proceedings of MLSys '19},
eprint = {1903.12287},
title = {{PyTorch-BigGraph: A Large-scale Graph Embedding System}},
url = {http://arxiv.org/abs/1903.12287},
year = {2019}
}

@inproceedings{Zhang2020c,
archivePrefix = {arXiv},
arxivId = {2003.02454},
author = {Zhang, Dalong and Huang, Xin and Liu, Ziqi and Hu, Zhiyang and Song, Xianzheng and Ge, Zhibang and Zhang, Zhiqiang and Wang, Lin and Zhou, Jun and Shuang, Yang and Qi, Yuan},
booktitle = {Proceedings of the VLDB Endowment},
eprint = {2003.02454},
title = {{AGL: a Scalable System for Industrial-purpose Graph Machine Learning}},
url = {http://arxiv.org/abs/2003.02454},
year = {2020}
}

@article{dean2008mapreduce,
  title={MapReduce: simplified data processing on large clusters},
  author={Dean, Jeffrey and Ghemawat, Sanjay},
  journal={Communications of the ACM},
  volume={51},
  number={1},
  pages={107--113},
  year={2008},
  publisher={ACM New York, NY, USA}
}

@inproceedings{james2020ispd,
  title={Physical Mapping of Neural Networks on a Wafer-Scale Deep Learning Accelerator},
  author={James, Michael and Tom, Marvin and Groeneveld, Patrick and Kibardin, Vladimir},
  booktitle={Proceedings of the 2020 International Symposium on Physical Design},
  pages={145--149},
  year={2020}
}

@inproceedings{abts2020think,
  title={Think fast: a tensor streaming processor ({TSP}) for accelerating deep learning workloads},
  author={Abts, Dennis and Ross, Jonathan and Sparling, Jonathan and Wong-VanHaren, Mark and Baker, Max and Hawkins, Tom and Bell, Andrew and Thompson, John and Kahsai, Temesghen and Kimmell, Garrin and others},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={145--158},
  year={2020},
  organization={IEEE}
}

@inproceedings{nsight,
 author = {},
 title = {Nvidia Nsight},
 booktitle = {https://developer.nvidia.com/nsight-visual-studio-edition},
 year = {2018},
 pages = {},
}

@inproceedings{fan2019graph,
  title={Graph neural networks for social recommendation},
  author={Fan, Wenqi and Ma, Yao and Li, Qing and He, Yuan and Zhao, Eric and Tang, Jiliang and Yin, Dawei},
  booktitle={The World Wide Web Conference},
  pages={417--426},
  year={2019}
}

@inproceedings{duvenaud2015convolutional,
  title={Convolutional networks on graphs for learning molecular fingerprints},
  author={Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al{\'a}n and Adams, Ryan P},
  booktitle={Advances in neural information processing systems},
  pages={2224--2232},
  year={2015}
}

@article{miwa2016end,
  title={End-to-end relation extraction using lstms on sequences and tree structures},
  author={Miwa, Makoto and Bansal, Mohit},
  journal={arXiv preprint arXiv:1601.00770},
  year={2016}
}

@inproceedings{qi20173d,
  title={{3d} graph neural networks for {RGBD} semantic segmentation},
  author={Qi, Xiaojuan and Liao, Renjie and Jia, Jiaya and Fidler, Sanja and Urtasun, Raquel},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={5199--5208},
  year={2017}
}

@inproceedings{jiang2019anomaly,
  title={Anomaly Detection with Graph Convolutional Networks for Insider Threat and Fraud Detection},
  author={Jiang, Jianguo and Chen, Jiuming and Gu, Tianbo and Choo, Kim-Kwang Raymond and Liu, Chao and Yu, Min and Huang, Weiqing and Mohapatra, Prasant},
  booktitle={MILCOM 2019-2019 IEEE Military Communications Conference (MILCOM)},
  pages={109--114},
  year={2019},
  organization={IEEE}
}

@misc{snapnets,
  author       = {Jure Leskovec and Andrej Krevl},
  title        = {{SNAP Datasets}: {Stanford} Large Network Dataset Collection},
  howpublished = {\url{http://snap.stanford.edu/data}},
  month        = jun,
  year         = 2014
}



%%%% New extra
@article{yan2020fpgan,
  title={{FPGAN:} An {FPGA} Accelerator for Graph Attention Networks With Software and Hardware Co-Optimization},
  author={Yan, Weian and Tong, Weiqin and Zhi, Xiaoli},
  journal={IEEE Access},
  volume={8},
  pages={171608--171620},
  year={2020},
  publisher={IEEE}
}


@article{chen2020rubik,
  author={Chen, Xiaobing and Wang, Yuke and Xie, Xinfeng and Hu, Xing and Basak, Abanti and Liang, Ling and Yan, Mingyu and Deng, Lei and Ding, Yufei and Du, Zidong and Xie, Yuan},
  journal={IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst.}, 
  title={Rubik: A Hierarchical Architecture for Efficient Graph Neural Network Training}, 
  year={2021}
}


@inproceedings{tpu-isca,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi,Norman P. and  and Young,Cliff and Patil,Nishant and Patterson,David and Agrawal,Gaurav and Bajwa,Raminder and Bates,Sarah and Bhatia,Suresh and Boden,Nan and Borchers,Al and Boyle,Rick and Cantin,Pierre luc and Chao,Clifford and Clark,Chris and Coriell,Jeremy and Daley,Mike and Dau,Matt and Dean,Jeffrey and Gelb,Ben and Ghaemmaghami,Tara Vazir and Gottipati,Rajendra and Gulland,William and Hagmann,Robert and Ho,C. Richard and Hogberg,Doug and Hu,John and Hundt,Robert and Hurt,Dan and Ibarz,Julian and Jaffey,Aaron and Jaworski,Alek and Kaplan,Alexander and Khaitan,Harshit and Killebrew,Daniel and Koch,Andy and Kumar,Naveen and Lacy,Steve and Laudon,James and Law,James and Le,Diemthu and Leary,Chris and Liu,Zhuyuan and Lucke,Kyle and Lundin,Alan and MacKean,Gordon and Maggiore,Adriana and Mahony,Maire and Miller,Kieran and Nagarajan,Rahul and Narayanaswami,Ravi and Ni,Ray and Nix,Kathy and Norrie,Thomas and Omernick,Mark and Penukonda,Narayana and Phelps,Andy and Ross,Jonathan and Ross,Matt and Salek,Amir and Samadiani,Emad and Severn,Chris and Sizikov,Gregory and Snelham,Matthew and Souter,Jed and Steinberg,Dan and Swing,Andy and Tan,Mercedes and Thorson,Gregory and Tian,Bo and Toma,Horia and Tuttle,Erick and Vasudevan,Vijay and Walter,Richard and Wang,Walter and Wilcox,Eric and Yoon,Doe Hyun},
  booktitle={Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)},
  year={2017},
}

@Article{STONNEarxiv,
  author =       {Francisco Mu{\~n}oz-Matr{\'i}nez and Jos{\'e} L. Abell{\'a}n and Manuel E. Acacio and Tushar Krishna},
  title =        {STONNE: A Detailed Architectural Simulator for Flexible Neural Network Accelerators},
  journal =      {arXiv preprint arXiv:2006.07137v1},
  year =         {2020},
  volume =       {},
  number =       {},
  pages =        {},
  month =        Jun,
}

@INPROCEEDINGS{iscas,
  author={Robert Guirado and Akshay Jain and Sergi Abadal and Eduard Alarc{\'o}n},
  booktitle={2021 IEEE International Symposium on Circuits and Systems (ISCAS)}, 
  title={Characterizing the Communication Requirements of GNN Accelerators: A Model-Based Approach}, 
  year={2021},
  volume={},
  number={}}
  
  @article{rupp2016pipelined,
  title={Pipelined iterative solvers with kernel fusion for graphics processing units},
  author={Rupp, Karl and Weinbub, Josef and J{\"u}ngel, Ansgar and Grasser, Tibor},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={43},
  number={2},
  pages={1--27},
  year={2016},
  publisher={ACM New York, NY, USA}
}


@inproceedings{gcnax,
  title={GCNAX: A Flexible and Energy-efficient Accelerator
for Graph Convolutional Neural Networks},
  author={Jiajun Li and Ahmed Louri and Avinash Karanth and Razvan Bunescu},
  booktitle={2021 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={775--788},
  year={2021}
}

@inbook{capstan, author = {Rucker, Alexander and Vilim, Matthew and Zhao, Tian and Zhang, Yaqi and Prabhakar, Raghu and Olukotun, Kunle}, title = {Capstan: A Vector RDA for Sparsity}, year = {2021}, isbn = {9781450385572}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3466752.3480047}, abstract = { This paper proposes Capstan: a scalable, parallel-patterns-based, reconfigurable dataflow accelerator (RDA) for sparse and dense tensor applications. Instead of designing for one application, we start with common sparse data formats, each of which supports multiple applications. For a variety of sparse applications, Capstan with DDR4 memory is 18\texttimes{} faster than a multi-core CPU baseline, while Capstan with HBM2 memory is 16\texttimes{} faster than an Nvidia V100 GPU. For sparse applications that can be mapped to Plasticine, a recent dense RDA, Capstan is 7.6\texttimes{} to 365\texttimes{} faster and only 16 larger. }, booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture}, pages = {1022–1035}, numpages = {14} }


@article{chatarasi2020marvel,
  title={Marvel: A Data-centric Compiler for DNN Operators on Spatial Accelerators},
  author={Chatarasi, Prasanth and Kwon, Hyoukjun and Raina, Natesh and Malik, Saurabh and Haridas, Vaisakh and Parashar, Angshuman and Pellauer, Michael and Krishna, Tushar and Sarkar, Vivek},
  journal={arXiv preprint arXiv:2002.07752},
  year={2020}
}

@inproceedings{kao2020gamma,
  title={GAMMA: automating the HW mapping of DNN models on accelerators via genetic algorithm},
  author={Kao, Sheng-Chun and Krishna, Tushar},
  booktitle={2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)},
  pages={1--9},
  year={2020},
  organization={IEEE}
}

@article{naumov2019deep,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and Dzhulgakov, Dmytro and Mallevich, Andrey and Cherniavskii, Ilia and Lu, Yinghai and Krishnamoorthi, Raghuraman and Yu, Ansha and Kondratenko, Volodymyr and Pereira, Stephanie and Chen, Xianjie and Chen, Wenlin and Rao, Vijay and Jia, Bill and Xiong, Liang and Smelyanskiy, Misha},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}

@article{fox2021concentric,
  title={Concentric Spherical GNN for 3D Representation Learning},
  author={Fox, James and Zhao, Bo and Rajamanickam, Sivasankaran and Ramprasad, Rampi and Song, Le},
  journal={arXiv preprint arXiv:2103.10484},
  year={2021}
}

@inproceedings{eie,
author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
title = {EIE: Efficient Inference Engine on Compressed Deep Neural Network},
year = {2016},
isbn = {9781467389471},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2016.30},
doi = {10.1109/ISCA.2016.30},
abstract = {State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power.Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120\texttimes{} energy saving; Exploiting sparsity saves 10\texttimes{}; Weight sharing gives 8\texttimes{}; Skipping zero activations from ReLU saves another 3\texttimes{}. Evaluated on nine DNN benchmarks, EIE is 189\texttimes{} and 13\texttimes{} faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102 GOPS working directly on a compressed network, corresponding to 3 TOPS on an uncompressed network, and processes FC layers of AlexNet at 1.88\texttimes{}104 frames/sec with a power dissipation of only 600mW. It is 24,000\texttimes{} and 3,400\texttimes{} more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9\texttimes{}, 19\texttimes{} and 3\texttimes{} better throughput, energy efficiency and area efficiency.},
booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
pages = {243–254},
numpages = {12},
keywords = {hardware acceleration, algorithm-hardware co-design, ASIC, model compression, deep learning},
location = {Seoul, Republic of Korea},
series = {ISCA '16}
}

@inproceedings{extensor,
author = {Hegde, Kartik and Asghari-Moghaddam, Hadi and Pellauer, Michael and Crago, Neal and Jaleel, Aamer and Solomonik, Edgar and Emer, Joel and Fletcher, Christopher W.},
title = {ExTensor: An Accelerator for Sparse Tensor Algebra},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358275},
doi = {10.1145/3352460.3358275},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {319–333},
numpages = {15},
keywords = {Sparse Computation, Tensor Algebra, Hardware Acceleration},
location = {Columbus, OH, USA},
series = {MICRO '52}
}


@inproceedings{interstellar,
author = {Yang, Xuan and Gao, Mingyu and Liu, Qiaoyi and Setter, Jeff and Pu, Jing and Nayak, Ankita and Bell, Steven and Cao, Kaidi and Ha, Heonjae and Raina, Priyanka and Kozyrakis, Christos and Horowitz, Mark},
title = {Interstellar: Using Halide's Scheduling Language to Analyze DNN Accelerators},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378514},
doi = {10.1145/3373376.3378514},
abstract = {We show that DNN accelerator micro-architectures and their program mappings represent specific choices of loop order and hardware parallelism for computing the seven nested loops of DNNs, which enables us to create a formal taxonomy of all existing dense DNN accelerators. Surprisingly, the loop transformations needed to create these hardware variants can be precisely and concisely represented by Halide's scheduling language. By modifying the Halide compiler to generate hardware, we create a system that can fairly compare these prior accelerators. As long as proper loop blocking schemes are used, and the hardware can support mapping replicated loops, many different hardware dataflows yield similar energy efficiency with good performance. This is because the loop blocking can ensure that most data references stay on-chip with good locality and the processing units have high resource utilization. How resources are allocated, especially in the memory system, has a large impact on energy and performance. By optimizing hardware resource allocation while keeping throughput constant, we achieve up to 4.2X energy improvement for Convolutional Neural Networks (CNNs), 1.6X and 1.8X improvement for Long Short-Term Memories (LSTMs) and multi-layer perceptrons (MLPs), respectively.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {369–383},
numpages = {15},
keywords = {neural networks, domain specific language, dataflow},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@inproceedings{
xu2018how,
title={How Powerful are Graph Neural Networks?},
author={Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ryGs6iA5Km},
}

@inproceedings{cosa,
author = {Huang, Qijing and Kang, Minwoo and Dinh, Grace and Norell, Thomas and Kalaiah, Aravind and Demmel, James and Wawrzynek, John and Shao, Yakun Sophia},
title = {CoSA: Scheduling by constrained optimization for spatial accelerators},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00050},
doi = {10.1109/ISCA52012.2021.00050},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {554–566},
numpages = {13},
keywords = {accelerator, compiler optimizations, neural networks, scheduling},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@ARTICLE{stonnecal,  author={Munoz-Martinez, Francisco and Abellan, Jose and Acacio, Manuel E. and Krishna, Tushar},  journal={IEEE Computer Architecture Letters},   title={STONNE: Enabling Cycle-Level Microarchitectural Simulation for DNN Inference Accelerators},   year={2021},  volume={},  number={},  pages={1-1},  doi={10.1109/LCA.2021.3097253}}

@article{moon2021evaluating,
  title={Evaluating Spatial Accelerator Architectures with Tiled Matrix-Matrix Multiplication},
  author={Moon, Gordon E and Kwon, Hyoukjun and Jeong, Geonhwa and Chatarasi, Prasanth and Rajamanickam, Sivasankaran and Krishna, Tushar},
  journal={arXiv preprint arXiv:2106.10499},
  year={2021}
}

@inproceedings{sara_isca2021,
author = {Yaqi Zhang and Nathan Zhang and Tian Zhao and Matt Vilim and Muhammad Shahbaz and Kunle Olukotun},
title = {SARA: Scaling a Reconfigurable Dataflow Accelerator},
year = {2021},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
series = {ISCA '21}
}

@article{10.1145/3140659.3080256,
author = {Prabhakar, Raghu and Zhang, Yaqi and Koeplinger, David and Feldman, Matt and Zhao, Tian and Hadjis, Stefan and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Plasticine: A Reconfigurable Architecture For Parallel Paterns},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080256},
doi = {10.1145/3140659.3080256},
abstract = {Reconfigurable architectures have gained popularity in recent years as they allow
the design of energy-efficient accelerators. Fine-grain fabrics (e.g. FPGAs) have
traditionally suffered from performance and power inefficiencies due to bit-level
reconfigurable abstractions. Both fine-grain and coarse-grain architectures (e.g.
CGRAs) traditionally require low level programming and suffer from long compilation
times. We address both challenges with Plasticine, a new spatially reconfigurable
architecture designed to efficiently execute applications composed of parallel patterns.
Parallel patterns have emerged from recent research on parallel programming as powerful,
high-level abstractions that can elegantly capture data locality, memory access patterns,
and parallelism across a wide range of dense and sparse applications.We motivate Plasticine
by first observing key application characteristics captured by parallel patterns that
are amenable to hardware acceleration, such as hierarchical parallelism, data locality,
memory access patterns, and control flow. Based on these observations, we architect
Plasticine as a collection of Pattern Compute Units and Pattern Memory Units. Pattern
Compute Units are multi-stage pipelines of reconfigurable SIMD functional units that
can efficiently execute nested patterns. Data locality is exploited in Pattern Memory
Units using banked scratchpad memories and configurable address decoders. Multiple
on-chip address generators and scatter-gather engines make efficient use of DRAM bandwidth
by supporting a large number of outstanding memory requests, memory coalescing, and
burst mode for dense accesses. Plasticine has an area footprint of 113 mm2 in a 28nm
process, and consumes a maximum power of 49 W at a 1 GHz clock. Using a cycle-accurate
simulator, we demonstrate that Plasticine provides an improvement of up to 76.9x in
performance-per-Watt over a conventional FPGA over a wide range of dense and sparse
applications.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {389–402},
numpages = {14},
keywords = {hardware accelerators, reconfigurable architectures, CGRAs, parallel patterns}
}

@inproceedings{kwon2021heterogeneous,
  title={Heterogeneous Dataflow Accelerators for Multi-DNN Workloads},
  author={Kwon, Hyoukjun and Lai, Liangzhen and Pellauer, Michael and Krishna, Tushar and Chen, Yu-Hsin and Chandra, Vikas},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={71--83},
  organization={IEEE}
}

@book{hestenes1952methods,
  title={Methods of conjugate gradients for solving linear systems},
  author={Hestenes, Magnus Rudolph and Stiefel, Eduard and others},
  year={1952},
  publisher={NBS Washington, DC}
}

@article{van1992bi,
  title={Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems},
  author={Van der Vorst, Henk A},
  journal={SIAM Journal on scientific and Statistical Computing},
  volume={13},
  number={2},
  pages={631--644},
  year={1992},
  publisher={SIAM}
}

@article{bramble1988preconditioning,
  title={A preconditioning technique for indefinite systems resulting from mixed approximations of elliptic problems},
  author={Bramble, James H and Pasciak, Joseph E},
  journal={Mathematics of Computation},
  volume={50},
  number={181},
  pages={1--17},
  year={1988}
}

@inproceedings{plasticine,
author = {Prabhakar, Raghu and Zhang, Yaqi and Koeplinger, David and Feldman, Matt and Zhao, Tian and Hadjis, Stefan and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Plasticine: A Reconfigurable Architecture For Parallel Paterns},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080256},
doi = {10.1145/3079856.3080256},
abstract = {Reconfigurable architectures have gained popularity in recent years as they allow
the design of energy-efficient accelerators. Fine-grain fabrics (e.g. FPGAs) have
traditionally suffered from performance and power inefficiencies due to bit-level
reconfigurable abstractions. Both fine-grain and coarse-grain architectures (e.g.
CGRAs) traditionally require low level programming and suffer from long compilation
times. We address both challenges with Plasticine, a new spatially reconfigurable
architecture designed to efficiently execute applications composed of parallel patterns.
Parallel patterns have emerged from recent research on parallel programming as powerful,
high-level abstractions that can elegantly capture data locality, memory access patterns,
and parallelism across a wide range of dense and sparse applications.We motivate Plasticine
by first observing key application characteristics captured by parallel patterns that
are amenable to hardware acceleration, such as hierarchical parallelism, data locality,
memory access patterns, and control flow. Based on these observations, we architect
Plasticine as a collection of Pattern Compute Units and Pattern Memory Units. Pattern
Compute Units are multi-stage pipelines of reconfigurable SIMD functional units that
can efficiently execute nested patterns. Data locality is exploited in Pattern Memory
Units using banked scratchpad memories and configurable address decoders. Multiple
on-chip address generators and scatter-gather engines make efficient use of DRAM bandwidth
by supporting a large number of outstanding memory requests, memory coalescing, and
burst mode for dense accesses. Plasticine has an area footprint of 113 mm2 in a 28nm
process, and consumes a maximum power of 49 W at a 1 GHz clock. Using a cycle-accurate
simulator, we demonstrate that Plasticine provides an improvement of up to 76.9x in
performance-per-Watt over a conventional FPGA over a wide range of dense and sparse
applications.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {389–402},
numpages = {14},
keywords = {parallel patterns, CGRAs, reconfigurable architectures, hardware accelerators},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@inproceedings{cerebras,
  title={Fast stencil-code computation on a wafer-scale processor},
  author={Rocki, Kamil and Van Essendelft, Dirk and Sharapov, Ilya and Schreiber, Robert and Morrison, Michael and Kibardin, Vladimir and Portnoy, Andrey and Dietiker, Jean Francois and Syamlal, Madhava and James, Michael},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--14},
  year={2020},
  organization={IEEE}
}

@article{abadal2020computing,
  title={Computing graph neural networks: A survey from algorithms to accelerators},
  author={Abadal, Sergi and Jain, Akshay and Guirado, Robert and L{\'o}pez-Alonso, Jorge and Alarc{\'o}n, Eduard},
  journal={ACM Computing Surveys},
  year={2021}
}

@INPROCEEDINGS{STONNE21,
  author =       {Francisco Mu{\~n}oz-Matr{\'i}nez and Jos{\'e} L. Abell{\'a}n and Manuel E. Acacio and Tushar Krishna},
  title =        {STONNE: Enabling Cycle-Level Microarchitectural Simulation for DNN Inference Accelerators},
  booktitle =    {2021 IEEE International Symposium on Workload Characterization (IISWC)}, 
  year =         {2021},
  volume =       {},
  number =       {},
  pages =        {},
}

@inproceedings{popt-hpca21,
  title={P-OPT: Practical Optimal Cache Replacement for Graph Analytics},
  author={Balaji, Vignesh and Crago, Neal and Jaleel, Aamer and Lucia, Brandon},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={668--681},
  year={2021},
  organization={IEEE}
}

@article{gmres,
author = {Saad, Youcef and Schultz, Martin H.},
title = {GMRES: A Generalized Minimal Residual Algorithm for Solving Nonsymmetric Linear Systems},
journal = {SIAM Journal on Scientific and Statistical Computing},
volume = {7},
number = {3},
pages = {856-869},
year = {1986},
doi = {10.1137/0907058},

URL = { 
        https://doi.org/10.1137/0907058
    
},
eprint = { 
        https://doi.org/10.1137/0907058
    
}
,
    abstract = { We present an iterative method for solving linear systems, which has the property of minimizing at every step the norm of the residual vector over a Krylov subspace. The algorithm is derived from the Arnoldi process for constructing an \$l\_2 \$-orthogonal basis of Krylov subspaces. It can be considered as a generalization of Paige and Saunders’ MINRES algorithm and is theoretically equivalent to the Generalized Conjugate Residual (GCR) method and to ORTHODIR. The new algorithm presents several advantages over GCR and ORTHODIR. }
}

@inproceedings{simba,
author = {Shao, Yakun Sophia and Clemons, Jason and Venkatesan, Rangharajan and Zimmer, Brian and Fojtik, Matthew and Jiang, Nan and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and Tell, Stephen G. and Zhang, Yanqing and Dally, William J. and Emer, Joel and Gray, C. Thomas and Khailany, Brucek and Keckler, Stephen W.},
title = {Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358302},
doi = {10.1145/3352460.3358302},
abstract = {Package-level integration using multi-chip-modules (MCMs) is a promising approach for building large-scale systems. Compared to a large monolithic die, an MCM combines many smaller chiplets into a larger system, substantially reducing fabrication and design costs. Current MCMs typically only contain a handful of coarse-grained large chiplets due to the high area, performance, and energy overheads associated with inter-chiplet communication. This work investigates and quantifies the costs and benefits of using MCMs with fine-grained chiplets for deep learning inference, an application area with large compute and on-chip storage requirements. To evaluate the approach, we architected, implemented, fabricated, and tested Simba, a 36-chiplet prototype MCM system for deep-learning inference. Each chiplet achieves 4 TOPS peak performance, and the 36-chiplet MCM package achieves up to 128 TOPS and up to 6.1 TOPS/W. The MCM is configurable to support a flexible mapping of DNN layers to the distributed compute and storage units. To mitigate inter-chiplet communication overheads, we introduce three tiling optimizations that improve data locality. These optimizations achieve up to 16% speedup compared to the baseline layer mapping. Our evaluation shows that Simba can process 1988 images/s running ResNet-50 with batch size of one, delivering inference latency of 0.50 ms.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {14–27},
numpages = {14},
keywords = {neural networks, Multi-chip module, accelerator architecture},
location = {Columbus, OH, USA},
series = {MICRO '52}
}



@inproceedings{tangram,
author = {Gao, Mingyu and Yang, Xuan and Pu, Jing and Horowitz, Mark and Kozyrakis, Christos},
title = {TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304014},
doi = {10.1145/3297858.3304014},
abstract = {The use of increasingly larger and more complex neural networks (NNs) makes it critical to scale the capabilities and efficiency of NN accelerators. Tiled architectures provide an intuitive scaling solution that supports both coarse-grained parallelism in NNs: intra-layer parallelism, where all tiles process a single layer, and inter-layer pipelining, where multiple layers execute across tiles in a pipelined manner. This work proposes dataflow optimizations to address the shortcomings of existing parallel dataflow techniques for tiled NN accelerators. For intra-layer parallelism, we develop buffer sharing dataflow that turns the distributed buffers into an idealized shared buffer, eliminating excessive data duplication and the memory access overheads. For inter-layer pipelining, we develop alternate layer loop ordering that forwards the intermediate data in a more fine-grained and timely manner, reducing the buffer requirements and pipeline delays. We also make inter-layer pipelining applicable to NNs with complex DAG structures. These optimizations improve the performance of tiled NN accelerators by 2x and reduce their energy consumption by 45% across a wide range of NNs. The effectiveness of our optimizations also increases with the NN size and complexity.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {807–820},
numpages = {14},
keywords = {neural networks, dataflow, parallelism},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@article{flat,
  title={An Optimized Dataflow for Mitigating Attention Performance Bottlenecks},
  author={Kao, Sheng-Chun and Subramanian, Suvinay and Agrawal, Gaurav and Krishna, Tushar},
  journal={arXiv preprint arXiv:2107.06419},
  year={2021}
}

@inproceedings{garg2021understanding,
  title={Understanding the Design-Space of Sparse/Dense Multiphase GNN dataflows on Spatial Accelerators},
  author={Garg, Raveesh and Qin, Eric and Mu{\~n}oz-Mart{\'\i}nez, Francisco and Guirado, Robert and Jain, Akshay and Abadal, Sergi and Abell{\'a}n, Jos{\'e} L and Acacio, Manuel E and Alarc{\'o}n, Eduard and Rajamanickam, Sivasankaran and Krishna, Tushar},
  booktitle={2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  year={2022}
  }

@article{suitesparse,
author = {Davis, Timothy A. and Hu, Yifan},
title = {The University of Florida Sparse Matrix Collection},
year = {2011},
issue_date = {November 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/2049662.2049663},
doi = {10.1145/2049662.2049663},
abstract = {We describe the University of Florida Sparse Matrix Collection, a large and actively growing set of sparse matrices that arise in real applications. The Collection is widely used by the numerical linear algebra community for the development and performance evaluation of sparse matrix algorithms. It allows for robust and repeatable experiments: robust because performance results with artificially generated matrices can be misleading, and repeatable because matrices are curated and made publicly available in many formats. Its matrices cover a wide spectrum of domains, include those arising from problems with underlying 2D or 3D geometry (as structural engineering, computational fluid dynamics, model reduction, electromagnetics, semiconductor devices, thermodynamics, materials, acoustics, computer graphics/vision, robotics/kinematics, and other discretizations) and those that typically do not have such geometry (optimization, circuit simulation, economic and financial modeling, theoretical and quantum chemistry, chemical process simulation, mathematics and statistics, power networks, and other networks and graphs). We provide software for accessing and managing the Collection, from MATLAB™, Mathematica™, Fortran, and C, as well as an online search capability. Graph visualization of the matrices is provided, and a new multilevel coarsening scheme is proposed to facilitate this task.},
journal = {ACM Trans. Math. Softw.},
month = {dec},
articleno = {1},
numpages = {25},
keywords = {Graph drawing, sparse matrices, performance evaluation, multilevel algorithms}
}



@inproceedings{dnnfusion,
  title={{DNNFusion: Accelerating Deep Neural Networks Execution with Advanced Operator Fusion}},
  author={Niu, Wei and Guan, Jiexiong and Wang, Yanzhi and Agrawal, Gagan and Ren, Bin},
  booktitle={PLDI},
  year={2021}
}

@INPROCEEDINGS{tensaurus,
  author={Srivastava, Nitish and Jin, Hanchen and Smith, Shaden and Rong, Hongbo and Albonesi, David and Zhang, Zhiru},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={Tensaurus: A Versatile Accelerator for Mixed Sparse-Dense Tensor Computations}, 
  year={2020},
  volume={},
  number={},
  pages={689-702},
  doi={10.1109/HPCA47549.2020.00062}
}

@INPROCEEDINGS{tensorlib,
  author={Jia, Liancheng and Luo, Zizhang and Lu, Liqiang and Liang, Yun},
  booktitle={2021 58th ACM/IEEE Design Automation Conference (DAC)}, 
  title={TensorLib: A Spatial Accelerator Generation Framework for Tensor Algebra}, 
  year={2021},
  volume={},
  number={},
  pages={865-870},
  doi={10.1109/DAC18074.2021.9586329}
}

@inproceedings{flextensor,
author = {Zheng, Size and Liang, Yun and Wang, Shuo and Chen, Renze and Sheng, Kaiwen},
title = {FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378508},
doi = {10.1145/3373376.3378508},
abstract = {Tensor computation plays a paramount role in a broad range of domains, including machine learning, data analytics, and scientific computing. The wide adoption of tensor computation and its huge computation cost has led to high demand for flexible, portable, and high-performance library implementation on heterogeneous hardware accelerators such as GPUs and FPGAs. However, the current tensor library implementation mainly requires programmers to manually design low-level implementation and optimize from the algorithm, architecture, and compilation perspectives. Such a manual development process often takes months or even years, which falls far behind the rapid evolution of the application algorithms.In this paper, we introduce FlexTensor, which is a schedule exploration and optimization framework for tensor computation on heterogeneous systems. FlexTensor can optimize tensor computation programs without human interference, allowing programmers to only work on high-level programming abstraction without considering the hardware platform details. FlexTensor systematically explores the optimization design spaces that are composed of many different schedules for different hardware. Then, FlexTensor combines different exploration techniques, including heuristic method and machine learning method to find the optimized schedule configuration. Finally, based on the results of exploration, customized schedules are automatically generated for different hardware. In the experiments, we test 12 different kinds of tensor computations with totally hundreds of test cases and FlexTensor achieves average 1.83x performance speedup on NVIDIA V100 GPU compared to cuDNN; 1.72x performance speedup on Intel Xeon CPU compared to MKL-DNN for 2D convolution; 1.5x performance speedup on Xilinx VU9P FPGA compared to OpenCL baselines; 2.21x speedup on NVIDIA V100 GPU compared to the state-of-the-art.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {859–873},
numpages = {15},
keywords = {code generation, heterogeneous systems, machine learning, compiler optimization},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}



@inproceedings{telamalloc,
  title={Telamalloc: Efficient on-chip memory allocation for production machine learning accelerators},
  author={Maas, Martin and Beaugnon, Ulysse and Chauhan, Arun and Ilbeyi, Berkin},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
  pages={123--137},
  year={2022}
}

@INPROCEEDINGS{planaria,
  author={Ghodrati, Soroush and Ahn, Byung Hoon and Kyung Kim, Joon and Kinzer, Sean and Yatham, Brahmendra Reddy and Alla, Navateja and Sharma, Hardik and Alian, Mohammad and Ebrahimi, Eiman and Kim, Nam Sung and Young, Cliff and Esmaeilzadeh, Hadi},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Planaria: Dynamic Architecture Fission for Spatial Multi-Tenant Acceleration of Deep Neural Networks}, 
  year={2020},
  volume={},
  number={},
  pages={681-697},
  keywords={Industries;Microarchitecture;Computer architecture;Quality of service;Throughput;Acceleration;Task analysis;Accelerators;Deep Neural Networks;DNN;DNN Acceleration;Multi-Tenancy;Spatial DNN Task Co-Location;Multi-Tenant DNN Acceleration;Dynamic Architecture Fission;Omni-Directional Systolic Arrays},
  doi={10.1109/MICRO50266.2020.00062}}

@inproceedings{aimt,
  title={A Multi-Neural Network Acceleration Architecture},
  author={Baek, Eunjin and Kwon, Dongup and Kim, Jangwoo},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={940--953},
  year={2020},
  organization={IEEE}
}


@inproceedings{sara_dac2022,
  title={Self adaptive reconfigurable arrays (SARA) learning flexible GEMM accelerator configuration and mapping-space using ML},
  author={Samajdar, Ananda and Qin, Eric and Pellauer, Michael and Krishna, Tushar},
  booktitle={Proceedings of the 59th ACM/IEEE Design Automation Conference},
  pages={583--588},
  year={2022}
}


orojenesis_isca_web

@misc{orojenesis_web,
    title = {{Orojenesis
}},
    howpublished = {\url{https://timeloop.csail.mit.edu/orojenesis}},
    year = {2024}
}
