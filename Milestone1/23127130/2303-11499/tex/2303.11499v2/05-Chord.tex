
\insertFigure{tornado}{~\SpadName policies. Left half shows~\PolicyA which starts by filling up without replacement. The part that could not fit is sent to DRAM. Right half shows~\PolicyB. After queue is filled, the tail of $X$ is evicted and next element of $R$ is enqueued at the head to fill the space.}

\insertWideFigureScaled{chord-fig}{Hardware mechanism \SpadNamenospace.}{0.95}


\section{Hardware Buffer Mechanism: \SpadNameTitle}
\label{sec:chord}

%\TK{@Raveesh - see if you are OK with this para}
We propose~\SpadName for getting reuse out of tensors within the SRAM in a cycle-level implicit manner with coarse-grained explicit information from~\DataflowNamenospace.
%\TK{maybe we should give a little bit more detail Raveesh about the diff buffers in that figure}
In~\SpadNamenospace, we propose two replacement policies, \PolicyA and~\PolicyBnospace, which replace at an operand level rather than a cache line level. This eliminates the overhead of maintaining counters %for each line 
and tag matching for each line.  We also highlight the challenges with existing cache replacement policies and explicit scratchpad management. \autoref{tables:related_chord} summarizes \SpadName properties compared to prior buffering mechanisms.%\footnote{Note that \AccelName  can also be operated in a purely explicit manner in case \DataflowName identifies no delayed writeback or hold dependences.}.

\subsection{\SpadNameTitle~ Policies and Hardware Mechanism}

%\subsection{Custom Scratchpad Management}



\label{sec:tornado}
%There are various scenarios regarding the amount of tensors the scratchpad can hold ranging from a fraction to multiple tensors depending on the workload size and the architecture.
We discuss ~\SpadName's two policies here.

\textbf{\PolicyA}- %In \DataflowName, we choose the loop orders to minimize the swizzling of ranks in a tensor and hence the tensor will be read in the same order as it was written.
We write the tensor in the queue order and if the scratchpad is full, we send the remaining portion of that tensor directly to DRAM as~\autoref{fig:tornado} shows. This makes sure that we access the portion of the tensor in the scratchpad first rather than replacing it by the later part of the tensor and again reading the earlier part. This is in stark contrast to the cache replacement schemes that keep the recently accessed data. If the tensor fits, we repeat this for multiple operations. We read the tensor at head.

~\textbf{\PolicyB}- While~\PolicyA makes sure that we minimize the off-chip traffic for that particular tensor, there is still a chance that a particular tensor which is not reused until later (for example, $X$ in line 3 of~\autoref{alg:cg_einsum} which is used in line 3 of the next iteration) is written to the buffer at the expense of a future tensor that is re-used more frequently and before the reuse of the previous tensor (for example $R$ in line 4 which is reused in lines 5 and 7 of the same iteration). Based on the reuse frequency, we prioritize $R$ and evict out a portion of $X$ to make room for $R$ by evicting the later elements of $X$ first. We call this policy~\PolicyB. Then $R$ is read at its head. This is the first work to the best of our knowledge that considers reuse distance and frequency of a tensor in the dependency graph and proposes replacement at an operand granularity rather than line granularity. %We call this \textsc{FDpad} (Frequency and Distance).

%If 3 and 4 were executed in parallel as~\autoref{fig:spacetime} shows, $R$ and $X$ would occupy equal halves in the second step and the later part of $R$ will eventually replace the tail of earlier part of $X$ and the later part of $X$ would be sent straight to DRAM, resulting in the same proportions of $R$ and $X$.


%\subsection{\SpadNameTitle~ Hardware Organization}


%\autoref{fig:idiom} 

%\TK{@Raveesh - i uncommented this para:}
\autoref{fig:chord-fig}
shows the HW organization of~\SpadNamenospace. The buffer mechanism consists of a~\PolicyA controller which is responsible for detecting spills and send the spilling elements from the same operand to DRAM. While considering a different operand, \PolicyB index table determines whether the tensor must be replaced or not. Note that this index table will have far fewer entries than the total number of lines with a counter for each line. This table also keeps track of the starting and ending addresses of the slice of tensor residing in the SRAM and the ending address of the complete tensor.
%\TK{this question belongs in Section VI according to me -- unclear why we are suddenly talking about caches. Better to out it in Section VI}
%\textbf{Q. How does~\SpadName use the metadata to reduce the overhead of caches?.}
The tensors in~\SpadName are contiguous and ordered. We also keep track of starting and ending index of the tensor as a part of per-tensor metadata. Thus, on hit, we can use this order and continuity to calculate the index in the buffer based on the request address without searching for it. \textit{This eliminates the need for tag matching per line.}
The information about the tensor's future reuse is obtained from~\DataflowName and is also part of the meta data used by~\PolicyBnospace. %\SpadName also consists of a controller that steers the tiles to the pipeline buffer, which is determined by~\DataflowName.
On a miss, we first check whether there is empty space by checking all end indices of existing tensors. If not, we check whether the tensor can replace any of the existing tensors, using the~\PolicyBnospace. If there is no replacement possible, we just send the data to DRAM. If there is a tensor element that can be replaced (which does not belong to the same tensor as the written tensor), we replace one entry from its tail. This is done by enqueuing the end index of the written tensor and letting the other elements of the written tensor push out the tail of the victim tensor.

%Access and replacement within~\SpadName works as follows:

\insertFigure{replacement}{Benefit of~\PolicyA and~\PolicyB over LRU and BRRIP.}


\subsection{Why \SpadName over Cache and Scratchpad?}
\label{sec:arguments}

\textbf{Better replacement than line-level cache replacement policies:}
Policy-wise, the main drawback of caches is the myopic view of lines which misses the tensor-level reuse opportunities.
\autoref{fig:replacement} shows a toy example to demonstrate the effectiveness of~\SpadName for tensor programs. In the first step, when a tensor is written to~\SpadName, it only keeps the head of the tensor (\PolicyAnospace) as that will be re-referenced first, while LRU kicks that out and then has to bring that in again. BRRIP~\cite{rrip} also keeps the first three data to prevent scanning behavior. In the next step,~\SpadName sees the reuse of the tensor T3 and overwrites it (\PolicyBnospace), while LRU and BRRIP have lingering elements from T1 as well, and the elements from T3 are the elements that will be re-referenced at the end. In the final step, LRU and BRRIP need to bring g1,g2... back, while~\SpadName already has them. Thus operand-level replacement is beneficial for such tensor programs.




\textbf{Hardware overhead reduction over caches:}
In terms of area, cache controller and tag bits are significant overheads and can account for almost a third of the cache area. Scratchpads on the other hand, do not incur control and tag area overhead, and the overall overhead is about 2\% compared to the data array size~\cite{buffets}. ~\SpadName needs only one entry of 512 bits per tensor (which is sufficient to include the metadata fields \autoref{fig:chord-fig} shows -- tensor ID, three addresses, two indices, tensor-level reuse history and reuse frequency and distance) to hold information about 64 tensors and their future operations in the DAG. This amounts to 0.01x of the area of the tag array inside the cache (4MB, 8-way associative). 

In terms of on-chip energy, tag access energy is comparable to data access energy, because of the size of the tag array and also due to set associativity. Whereas \SpadName completely minimizes this energy by taking advantage of contiguity in tensors and calculating the local address by just knowing the 512 bit metadata entry corresponding to the tensor in question, so that major energy comes from data accesses.

\textbf{Lower complexity of \SpadName compared to caches:}

\SpadName is less complex compared to the cache, \textit{in terms of both determining hit or miss, and changes in metadata.}

\textit{(1) Determining a hit or miss}: In a cache, determining hit or miss requires tag matching per line for every access. On the other hand, \SpadName uses bulk tracking at the tensor level where the tensors are contiguous, thus avoiding the need for tag matching per line. Hit or miss is determined by comparing the requested address with the end\_\SpadName address of requested tensor and that requires accessing a table that is 100x smaller than cache metadata.


\textit{(2) Changes to metadata:} In a cache, every hit or miss leads to a change in the recently used line, which requires updating the counters.
On the other hand,  \PolicyA and \PolicyB minimize the tracking required in \SpadName. On a hit, no change to the metadata is required, and the index calculation is performed based on start index and the start address of the tensor, which is obtained by reading from the \PolicyBnospace-index table, which is 100x smaller than cache metadata.
On a miss, the priority of the requested tensor is determined based on \PolicyBnospace, and the tail of the tensor tensor with the lowest priority is replaced. While this is happening, the start and end indices and end-\SpadName address associated with specific tensor IDs change, but that only corresponds to one entry per tensor in the \PolicyBnospace-index table. However, if the requested tensor has lower priority than all the other tensors in the \SpadNamenospace, the tensor is sent straight to DRAM, and there is no change within \SpadNamenospace.



Therefore, \SpadName avoids complexity by (1) avoiding tag matching per line, and instead only accessing "end-\SpadNamenospace" for the requested tensor from 100x smaller \PolicyBnospace-index table, and (2) changing start and end indices and end-\SpadName address in \PolicyBnospace-index table, only when there is a miss and the tensor is higher priority than atleast one tensor present inside \SpadNamenospace. 


\textbf{High cost of scratchpad allocation solved by~\SpadNamenospace.}
%Programming the scratchpad is a complex process, which seems easy on paper but is often taken for granted. %by architects. 
%But the programmers are better off with workload awareness and limited architecture exposure, but not at the level of every line of the buffer and at the level of every cycle. This is one of the major obstacles in adoption of flexible accelerators with ability to support multiple dataflows, because of the overhead in the number of configurations required.
%If the workload has sufficient reuse within all the dimensions, then it is true that scratchpad can be programmed with a few distinct configurations. However, the space explodes when one operation does not have sufficient reuse, and multiple operands are competing for the space in the buffer, to support inter-operation reuse.

%Therefore accelerators like TPU~\cite{tpuv4} is adopted because its simple enough to be barely programmable at the cycle level. 



The cost of programming a scratchpad for delayed operand reuse (with five tensors as an example) can be modeled in terms of number of buffer allocation choices as follows-

%\RG{Mathematical modelling of the space is still in progress. It involves constrained allocation of buffer space to multiple tensors with varying space allocated to each tensor operand - t1+t2+..tn<=cap. Which portions t1,t2 of tensors T1,T2 reside in the buffer is also variable. This is different over time, and changes with a different problem.}

(1) The cost of dynamically allocating buffer to different slices of tensors.

$T1_{slice}+T2_{slice}+T3_{slice}+T4_{slice}+T5_{slice}<size$

$Num_{choices} = \frac{(size+4)!}{size!4!}\approx size^4$

For a 4MB buffer (assuming 32-bit data), this is approximately $\mathit{10^{80}}$ and it scales $\mathit{size^{T-1}}$ for $T$ tensors. Compared to this, the size of the map space is $10^{15}$ for op-by-op buffer allocation.

(2) Cost of arranging all the lines is $size!$ which must be multiplied. However, assuming that a tensor block is contiguous can bring this to $T!$, which is 120 for five tensors.

(3) Choosing the elements of the slice itself is expensive.

For each $Ti_{slice}$, there are $\frac{Ti!}{Ti_{slice}!(Ti-Ti_{slice})!}$ choices. This has a \textit{factorial space complexity} which is bigger than exponential complexity. However, assuming a contiguous block of elements chosen in a slice, this can be brought down to $Ti-Ti_{slice}$. For T tensors, this is raised to power T.

(4) Considering, the varying aspect of scratchpad allocation, a realistic constraint equation is 

${T1_{slice}(t)+T2_{slice}(t)+...+T5_{slice}(t)<size}$

In a reasonably modeled scratchpad, allocation of tensors changes with each access as we move through the program. Therefore the choices in (1),(2),(3) need to be raised to the power of number of time steps. Moreover, these costs are just for buffer allocation, excluding scheduling knobs, for example, loop order, parallelism etc.

Hybrid explicit-implicit data orchestration helps minimize the cost of static buffer allocation.
\PolicyB solves step (1) by dynamically controlling buffer allocation of the tensors.
\PolicyA co-designed with~\DataflowNamenospace's swizzle minimization solves step (3).~\SpadName as a whole being cycle-level implicit and coarse-grained explicit avoids the cost of (4). \revision{The design-space of~\SpadName corresponding to the buffer allocation step only corresponds to the~\PolicyB policy decisions based on tensor-level reuse distance and frequency is $O(nodes+edges)$ since it only needs high level DAG information, e.g. reuse distance. Normally, the number of nodes looked ahead by~\SpadName are of the order of $10^2$, which is drastically lower than $10^{80}$.}

\input{tables/dataflow_config.tex}

\input{tables/config.tex}

\input{tables/workload.tex}

\textbf{Scratchpads are not panacea for statically known DAGs:} {Statically known DAG does not automatically imply that the costly buffer allocation only needs to be done once.} 
One of the preconceptions related to the use of scratchpads is that, the programmer just needs to go through the costly buffer allocation only once, and it keeps serving the purpose, given that the DAG of tensor operations is known. This is true only if the workload is sufficiently compute bound, and the matrices are relatively cubic to allow the same mapping to be applied for all operations and across all problem sizes.

For memory bound tensors in CG that are competing for space in the buffer, the ideal proportion of tensor allocation does in fact change with the problem size. This means that for every new CG problem, this buffer allocation step needs to be done, which is different from DNN models, and each new problem has a different shape and size.

The cost of using a scratchpad to do such buffer allocation amounts to exploring $>10^{80}$ choices, and here that needs to be performed for \textit{every new problem}. The use of scratchpads for this is impractical. \SpadName reduces this space to $10^2$, making it practical for every problem.
%Therefore static buffer allocation for complex DAGs is expensive. 
%For n-level tile, GAMMA's~\cite{kao2020gamma} design-space is $10^{12n}$.


%\insertFigure{buff-alloc}{Inefficiency of explicit buffer allocation with problems of different sizes requiring different allocation strategies for each dataset (Q4.).
%\vspace{-3mm}
%}

%~\autoref{fig:expcost} shows the overhead automating programming of scratchpads. 
%The search space for programming scratchpads at line level is xx times that of~\SpadNamenospace. \RG{To quantify}. Aside from this, supporting inter-operation reuse across downstream consumers makes it challenging to configure scratchpads despite statically known DAG structure.

%\textbf{Scratchpads are not a panacea for all statically known DAGs:}
%In an idealistic case, scratchpads can do the exact thing that~\SpadName does. 
%\TK{``However, programming scratchpads is an extremely challenging problem~\cite{martin-maas-paper, other papers?}}
 %Another one of the preconceptions related to the use of scratchpads is that, the programmer just needs to go through the costly buffer allocation only once, and it keeps serving the purpose, given that the DAG of tensor operations is known. This is true only if the workload is sufficiently compute bound, and the matrices are relatively cubic to allow the same mapping to be applied for all operations and across all problem sizes.
 
 %However CG has memory-bound GEMMs. Moreover, when multiple portions of tensors contend for the space in the buffer, the problem of allocating portions of tensors to the lines in the buffer explodes in complexity ($>10^{80}$). 
%We find that proportions of allocation of buffer lines to downstream tensors, do infact change with the problem size and sparsity. In case of CG, the size and sparsity of the $A$ matrix is different for each problem, and one would not run the same PDE again once its solved. So, it is safe to say that for every instance of CG kernel being called, there needs to be costly buffer allocation step for each operation, and each instance of CG.%~\autoref{fig:buff-alloc} shows this visually.



%Dynamicity introduced by MoE, activation sparsity or in recent DNN workloads~\cite{dream} makes scratchpads useless for even one-time use despite statically known DAGs.

%\SpadName resolves this by reducing map-space complexity drastically (to $10^2$). On the other hand, dedicated scratchpad for downstream consumer or a unified scratchpad with space allocated for downstream consumer would require costly reprogramming (complexity of $10^{80}$) for every new problem.

%Additionally in case of dynamicity, scratchpads provision resources and perform optimizations for the worst case.~\SpadName avoids this problem by using the address range of the input tensor and making dynamic space allocation decisions using~\PolicyA and~\PolicyB. The only thing that is decided statically is the reuse distance and frequency metadata for~\PolicyBnospace, which is used to determine which tensor has more priority.

%~\autoref{fig:buff-alloc} visually shows the overhead.

%\item Note that the above examples are still static, ie the DAG and sparsity are known at the design-time but the buffer allocation for the scratchpad is only usable once. However, for dynamicity, for example, MoE, activation sparsity etc., scratchpads are not a good choice for even one time use.


%Another reason for not considering scrathpads is that in the real world, even if the DAG is known, no one fixes the problem size. While input size in resnet is 224$\times$224, actual applications have different problem sizes, and no one would run the application again to solve the same problem, thus destroying the applicability of static buffer allocation.




%\subsection{Hardness of Static Explicit Management}

%\subsubsection{Belady's}

%\textbf{Q5. Are all tensor algebra applications static anyway?} Static determination of buffering strategies is done assuming the worst case or a fixed case. For example, BERT uses the maximum possible sequence length of 512. However, the actual sequence length of an input sequence might be as low as 32 or the actual workload might have a lower batch size, which is possible for all domains including scientific computing. As a result the static allocation done by the scratchpad can end up over-provisioning the space for the tensor in the buffer.~\SpadName avoids this problem by using the address range of the input tensor and making dynamic space allocation decisions using~\PolicyA and~\PolicyB. The only thing that is decided statically is the reuse distance and frequency metadata for~\PolicyBnospace, which is used to determine which tensor has more priority.
