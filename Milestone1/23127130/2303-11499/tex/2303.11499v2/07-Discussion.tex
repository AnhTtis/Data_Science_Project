% \reviewme{\section{Architectural Implications}
% The characteristics of accelerators required to support \DataflowName are as follows:
\vspace{-1mm}
% \textbf{Compute:} In order to support various loop orders, and tile sizes, the accelerator should have high 'flexibility'~\cite{flexion} with respect to all dimensions - order, shape, tiling and parallelism. Flexibility in order ensures support for various loop orders. Flexibility in tiling, parallelism and shape makes sure that GEMMs of different sizes can be mapped. For example, CG contains a contracted dominant GEMM which requires tiling both inputs and large parallel reduction, and an uncontracted dominant GEMM which requires tiling one input and one output and large parallel input broadcast. Accelerators like MAERI~\cite{kwon2018maeri} and Flexagon~\cite{flexagon} support fully flexible compute.

% \textbf{Off-chip Memory:} With individual skewed GEMMs being memory bound, ~\DataflowName tries to improve the arithmetic intensity by exploiting cross-operation reuse. To add to this, higher off-chip memory bandwidth would improves the performance in the memory bound region.

% \textbf{On-chip memory:} In principle, a fully programmable scratchpads used in Cerebras\cite{cerebras}, SambaNova~\cite{sambanova}, have the ability to partition the tensors. These scratchpads typically do support arbitrary loop orders and tiling strategies, given the reconfigurability of these accelerators. \textsc{Qpad} strategy can be applied by just writing to consecutive addresses until the end is reached, while \textsc{FDpad} strategy can be applied by overwriting on the addresses where the previous tensor needs to be replaced.
% }

\section{Related Work}




\textbf{Scheduling tensor-algebra operations:} \autoref{tables:related_score} shows the related works on schedulers and their capabilities. Initial works~\cite{kwon2019understanding,timeloop,interstellar,kao2020gamma,cosa} propose schedule optimization search space, cost model or a search algorithm for a single tensor operation at a time. Prior works \cite{flat,garg2021understanding,genetic-pipeline,tileflow,FlashAttention,xlayer} propose a scheduling search space, cost model or novel schedules for pipelining between two operations. Some works~\cite{tangram,isca-pip} do map ResNet~\cite{resnet} demonstrating delayed hold capability which can be handled by pipeline buffer. However, pipeline buffer cannot be used in situations when there is a downstream consumer with a delayed\_writeback dependency. Thus, \DataflowName is more comprehensive than prior work.

\textbf{On-chip data orchestration}: 
%\TK{We should point to Table III here?}
~\autoref{tables:related_chord} shows the common buffer mechanisms. Buffets~\cite{buffets} is a recent work that uses explicit-decoupled data orchestration. Buffets are ideal for scheduling one operation at a time, however, explicitly allocating multiple intermediate tensors statically in an arbitrary DAG of einsums is a hard problem. Recent work Tailors~\cite{tailors} is another work on data orchestration targeting the problem of irregular tile sizes that spill over the buffer space. It employs a hybrid solution by automatically shrinking the element from the tail and replacing the new element by it in a coupled fashion. ~\SpadName is also a hybrid design-point. The policies implicitly orchestrate data elements in~\SpadNamenospace, while they obtain highly coarse-grained information (for example, tensor's global address range and tensor-level reuse distance and frequency) from the workload. 

\textbf{Conjugate Gradient accelerators:} Prior accelerators like Cerebras~\cite{rocki} and Azul~\cite{feldmann2024azul} use all-SRAM architectures with Giga-bytes large SRAMs to avoid DRAM accesses alltogether. In contrast \AccelName demonstrates speedup on traditional accelerators with few Mega-bytes of SRAM. Alrescha~\cite{asgari2020alrescha} is another accelerator, but it focuses on sparse operations alone, which have lower arithmetic intensity to begin with, while \AccelName lifts the upper limitation of arithmetic intensity by exctracting inter-operation reuse.

%\vspace{-2mm}
\section{Conclusion}
\label{sec:discussion}
\vspace{-1mm}
%HPC/scientific applications are interesting because of the extreme demands they put on our hardware systems.
Ideally, tensor algebra applications would be made up of kernels that can be independently analyzed and optimized, then composed together into dependency graphs. In this paper we demonstrate that this individual operation view is too limited in scope: there is major opportunity to improve optimization by using an inter-operation viewpoint to increase arithmetic intensity, and that inter-operation pipelining fails to exploit reuse in the face of complex dependencies. Our proposed scheduler~\DataflowName identifies the complex reuse opportunities and the downstream tensors are reused by our proposed buffer structure~\SpadName at a tensor granularity. Our accelerator~\AccelName achieves 64\% to 83\% reduction in DRAM accesses compared to the state-of-the-art.

\section*{Acknowledgment}
Support for this work was in part provided through the ARIAA co-design center funded by the U.S. Department of Energy (DOE) Office of Science, Advanced Scientific Computing Research program. Sandia National Laboratories is a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia, LLC., a  wholly owned subsidiary of Honeywell International, Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA-0003525.

Part of this work was also supported by ACE, one of the seven centers in JUMP 2.0, a
Semiconductor Research Corporation (SRC) program sponsored by DARPA.

We also thank Axel Feldmann, Daniel Sanchez, Angshuman Parashar, and Nandeeka Nayak for their insightful feedback.

%and implicitly extend the data reuse to DAG of tensor operations.% For future work, we envision extending~\SpadName to larger systems with support for coherence and consistency between multiple~\SpadName instances. We also envision composing~\SpadName to deeper hierarchies. 

%Infact such reuse patterns are a generalization of popular emerging algorithms. Conjugate Gradient has interesting inter-operation reuse patterns which also captures reuse patterns in other algorithms like Graph Convolution Networks which pipeline two operations. Infact operations 1 and 2 in Conjugate Gradient~\autoref{alg:cg_einsum} are similar to SpMM and Dense phases in a layer of GCN making it a wider generalization.
%
%We have demonstrated that \textsc{Gogeta} can apply in scenarios where traditional loop fusion and unrolling are not sufficient. We demonstrate a \reviewme {geomean 5.97x} improvement in AI on CG.  Today, existing mapping tools such as Timeloop \cite{timeloop} focus on single Einsum optimization. We hope that integrating the \textsc{Gogeta} algorithm directly into their toolflow will allow a broad audience to exploit inter-operation reuse.

