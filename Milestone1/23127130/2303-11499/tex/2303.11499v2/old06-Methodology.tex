\section{Evaluations}

\subsection{Experimental Methodology}
\label{sec:expt}

We describe our experimental methodology for demonstrating the efficiency of \DataflowName across baseline dataflows.
%\vspace{-2mm}
\indent \textbf{Analytical Framework:~}
\label{sec:simulation}
%We modify the OMEGA~\cite{garg2021understanding} framework to model the DAG of tensor operations for applications like Conjugate Gradient, Graph Convolution Networks, ResNet. OMEGA is a cost model for modelling performance and energy for various mappings with two-operation pipelining.
%Our framework builds upon STONNE~\cite{STONNE21} which is a cycle accurate simulator that models one DNN layer at a time and models flexible interconnects~\cite{kwon2018maeri} with ability to support multiple intra-operation dataflows. STONNE has been extensively validated against MAERI~\cite{kwon2018maeri} and SIGMA~\cite{sigma} RTLs. STONNE can model SpMM, DenseGEMMs/CONVs on the accelerator with reconfigurable interconnects. Prior work OMEGA~\cite{garg2021understanding} builds a cost model which uses the STONNE output and timestamps to model pipelining between two operations. We modify the cost model that OMEGA uses to model a DAG of tensors in applications like Conjugate Gradient. We use that to simulate the baselines and \DataflowName which are described in~\autoref{sec:baseline}.
We build an analytical model to capture the memory accesses and communication across clusters for the baselines and \DataflowName to compare the achieved arithmetic intensity of the dataflows independent of the accelerator implementation. We validate the DRAM statistics for a single layer by using STONNE~\cite{STONNE21}, which is a cycle accurate simulator that models one operation (it can model GEMM, SpMM, SpMSpM, CONV) at a time. STONNE can be supplied with various dataflows and tile sizes as inputs. STONNE also models the flexible accelerator Flexagon~\cite{flexagon}. We model SpMM (the sparse matrix is in CSR format) and GEMM the microarchitecture of Flexagon since it is Flexible enough to support any mapping to get a strong op-by-op baseline. %We validated the data movement for a pair of two pipelined tensor operations using OMEGA~\cite{garg2021understanding,omega} which is a wrapper around STONNE to model pipelining between two operations. 

\input{tables/workload.tex}
\label{sec:dataset}

\indent \textbf{Workloads and Datasets:~}
We evaluate our methodology and the \DataflowName mapping strategy for Conjugate Gradient and GNN. We obtain the sparse matrices of the Conjugate Gradient Datasets from Suitesparse~\cite{suitesparse} for scientific problems like structural problems, circuit simulation, etc. We obtain GNN graphs from OMEGA~\cite{omega}.~\autoref{table:dataset} shows the workloads and the datasets.

\label{sec:baseline}
\input{tables/dataflow_config.tex}

\indent \textbf{Dataflow Configurations:~}
We compare multiple dataflow configurations described in~\autoref{table:dataflow_config}. Flexagon-like dataflow is modelled based on the microarchitecture of Flexagon~\cite{flexagon}. We model SpMM and~\GEMM on the flexible microarchitecure. It is able to achieve the lowest possible DRAM accesses for individual operations but it considers the output tensor of each operation ending up in the DRAM and the input coming from the DRAM. Its the oracle operation-by-operation dataflow. 

\DataflowNamenospace-\textsc{Qpad} does not consider pipelining, however, it writes the portion of the tensor that fits in the scratchpad in FIFO order proposed in~\autoref{sec:tornado} but does not consider distance/frequency based replacement. This ensures that parts of intermediate tensors that could fit, are still reused inside the SRAM. \DataflowNamenospace-\textsc{Qpad} by itself does not minimize swizzle penalty, it still writes the data in FIFO manner for tensors that are not swizzled later.

\DataflowNamenospace-\textsc{all} uses all the contributions. It uses the inter-op patterns proposed in~\autoref{sec:dataflows} and finds the individual loop orders which enable pipelining and avoid swizzle\_penalty proposed in~\autoref{sec:loop} and also uses the scratchpad management in~\autoref{sec:tornado}. It also uses scalable tiling strategy that reduces inter-cluster communication overhead proposed in~\autoref{sec:tiling}. Ideal, represents the arithmetic intensity with perfect reuse and infinite SRAM capacity.

\label{sec:params}

\input{tables/config.tex}


\indent \textbf{Workload and Architecture Parameters:~}
\autoref{table:config} describes the architecture configuration we use for the evaluation and the parameters we sweep. We run workloads of different sizes and nnz's and we also sweep the $N$ rank that corresponds to number of simultaneous initial guesses and the SRAM size to capture different scenarios with varying ratios of tensor and SRAM sizes. In some cases, tensors are either too small or too large for the SRAM and in some cases, a varying proportions of tensors can fit.
We consider a clustered architecture, each cluster of 1024 PEs backed by its own SRAM slice.
