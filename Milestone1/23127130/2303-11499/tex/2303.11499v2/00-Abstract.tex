\begin{comment}

\end{comment}

\vspace{-3mm}

\begin{abstract}

Tensor algebra accelerators have been gaining popularity for running high-performance computing (HPC) workloads. 
Identifying optimal schedules for individual tensor operations and designing hardware to run these schedules is an active area of research. 
Unfortunately, operators in HPC workloads such as Conjugate Gradient often have operators with skewed shapes, fundamentally limiting the reuse any schedule can leverage. 
Moreover, the operators form a complex DAG of dependencies, making it challenging to apply simple fusion/pipelining techniques to extract inter-operation reuse.
To address these challenges, 
this work proposes an accelerator \AccelNamenospace. \AccelName uses a novel on-chip buffer mechanism called \SpadName co-designed with a novel scheduler called \DataflowNamenospace, which together enables identifying and exploiting reuse over complex DAGs of tensor operations. \AccelName provides 4x geomean speedup and 4x energy efficiency over state-of-the-art accelerators
across HPC workloads.
\end{abstract}

\begin{IEEEkeywords}
hardware accelerator, Conjugate Gradient, inter-operation reuse, hybrid implicit/explicit data orchestration, data movement reduction, scheduling
\end{IEEEkeywords}

\begin{comment}
\begin{abstract}

Tensor-algebra applications are made up of operations that are independently optimized for reuse within the on-chip scratchpad.
While this approach works well for GEMMs with good reuse, several
tensor-algebra applications have operations with skewed aspect ratios which severely limits the reuse, making them memory bound. Worse, inter-operation pipelining cannot simply be applied in workloads with complex DAGs of tensor operations because of the additional complexities created by a combination of varying operation shapes and delayed downstream dependencies. This also leads to the explosion of the design-space, which makes finding optimized scratchpad configurations unacceptably complex and slow.
%On the other hand, conventional implicit structures like caches have a high tag matching area and energy overhead and a limited line-level view of reuse which could lead to missing tile level reuse opportunities.
%
In order to exploit DAG-level reuse opportunities in the face of skewed GEMMs and complex dependencies, we propose an accelerator \AccelNamenospace, that includes a novel buffer mechansim called \SpadName for hybrid (explicit + implicit) management coupled with  a software scheduler called \DataflowName that provides  4x geomean speedup and 4x energy efficiency over conventional accelerators across a range of workloads.
\end{abstract}
\end{comment}

%~\SpadNamenospace, an \textit{explicit+implicit hybrid} scheme that is coarse-grained explicit (at operand granularity) and fine-grained implicit (at element granularity). We also propose~\DataflowNamenospace, a downstream dependency aware scheduling strategy. \SpadName and \DataflowName together achieve 4x geomean speedup and 4x energy efficiency over conventional accelerators across a range of workloads.

%Existing implicit mechansims like cache incur high area and energy overheads, which are not suitable for domain-specific accelerators. 

%Scratchpads use explicit data orchestration, and are unacceptably slow with the explosion in sched



%Thus, exploiting inter-operation reuse of these intermediate tensors within the on-chip buffers is essential to exploit reuse.


\begin{comment}
\begin{abstract}
%\vspace{-1mm}
%HPC applications are critical in various scientific domains ranging from molecular dynamics to chemistry to fluid dynamics.
%Conjugate Gradient (CG) is a popular kernel used in iterative linear HPC solvers which have applications in numerous scientific domains like chemistry and fluid dynamics. However, CG is known for having poor arithmetic intensities, and the HPCG benchmark shows that the performance achieved by top 5 supercomputers on CG is 1-3\% of peak. Furthermore, CG has a complex DAG where intermediate tensors have multiple delayed downstream consumers which makes traditional fusion impossible. 

Tensor-algebra applications are made up of operations that are independently analyzed and optimized when mapping them onto custom accelerators.
While this approach works well for GEMMs with good reuse, we show that several
tensor-algebra applications have operations with skewed aspect ratios which severely limits the reuse, making them memory bound. Worse, inter-operation pipelining cannot simply be applied in workloads with complex DAGs of tensor operations because of the additional complexities created by a combination of varying operation shapes and delayed downstream dependencies.
Thus, exploiting inter-operation reuse of these intermediate tensors within the on-chip buffers is essential to exploit reuse.

However, scratchpads are explicitly programmed, which means that entire data placement in the scratchpad is decided statically. We show that this is challenging, specially when considering inter-operation reuse in complex cascades of operations. On the other hand, conventional implicit structures like caches have a high tag matching area and energy overhead and a limited line-level view of reuse which could lead to missing tile level reuse opportunities.


To this end, we propose \SpadNamenospace, a novel buffer structure that uses reuse-distance and frequency aware policies with whole tensor-level view as opposed to line-level policies used by caches, and requires coarse-grained architectural exposure, as opposed to full exposure required in scratchpads. Thus, \SpadName has a hybrid design, which is coarse-grained explicit and cycle-level implicit. We also propose~\DataflowNamenospace, a scheduler for complex cascade of eisnums, that also provides~\SpadName the necessary operand-level metadata. ~\SpadName and~\DataflowName together reduce the number of DRAM accesses by 64\% to 83\% across workloads evaluated. 
%improvement in arithmetic intensity and xx 
%improvement in throughput
% \reviewme{and 1.75x on Attention Layers.} 
%which is a generalization of inter-operation pipelining. 



%The TOP500 supercomputers achieve only a fraction of performance on HPC solvers compared to Linpack benchmark on which they are ranked. Majority of the computations in this algorithm are GEMMs. Spatial accelerators have gained huge success over acceleration of DNN applications where the data movement is minimized by exploiting reuse within a single matrix multiplication. However, matrix multiplications in many HPC applications like iterative linear solvers have arithmetic intensities. However, in this work we show various reuse and acceleration opportunities within these solver algorithms to minimize the data movement in the entire dataflow graph of the operation. We propose a novel dataflow \DataflowName to maximize reuse in the complete DAG of tensor operations for HPC and other applications from other domains like Machine Learning and Graph that use GEMMs. We observe that \DataflowName reduces the memory accesses by \TODO{xx\%} and inter-node communication by \TODO{xx\%} 

%HPC solvers have critical applications in various application domains ranging from computational fluid dynamics to chemistry applications to quantitative finance. Conjugate Gradient (CG) is a popular algorithm, used in iterative linear solvers. 

%TODO
\end{abstract}
\end{comment}