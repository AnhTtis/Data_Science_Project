% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}
% \documentclass{llncs}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage{epsfig}
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{amsthm}
\usepackage{multirow}
% \usepackage{hyperref}
% \usepackage{authblk}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% \cvprfinalcopy % *** Uncomment this line for the final submission

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5645} % *** Enter the CVPR Paper ID here
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\def\confName{CVPR}
\def\confYear{2023}

% \makeatletter
% \newcommand{\printfnsymbol}[1]{%
%   \textsuperscript{\@fnsymbol{#1}}%
% }
% \makeatother

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Adaptive Spot-Guided Transformer for Consistent Local Feature Matching}

\author{Jiahuan Yu\footnotemark[1], Jiahao Chang\footnotemark[1], Jianfeng He, Tianzhu Zhang\footnotemark[2], Feng Wu\\
{University of Science and Technology of China}
% {\tt\small yjh.cs.1998@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Jiahao Chang\\
% University of Science and Technology of China\\
% {\tt\small secondauthor@i2.org}
% {\small \textsuperscript{1}University of Science and Technology of China, \textsuperscript{2}Deep Space Exploration Lab, \textsuperscript{3}China Academy of Space Technology}
}


% \renewcommand{\thefootnote}{\fnsymbol{footnote}}
% \footnotetext[1]{test}
% \footnotetext[2]{test}

% University of Science and Technology of China\\

% \maketitle
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=1.0\linewidth]{graph/ASTR_motivation.pdf}
%     \caption{
%     The architecture of our ASTR consists of two major components, including the spot-guided aggregation module and the adaptive scaling modules.
%     }\label{fig:motivation}
% \end{figure*}
% \maketitle

\twocolumn[{%
    \renewcommand\twocolumn[1][]{#1}%
	\maketitle
	\begin{center}
	\includegraphics[width=1.0\linewidth]{graph/ASTR_motivation.pdf}
	\captionof{figure}{The visualization of the cross attention heatmaps and matching results.
    We sample two similar adjacent points in the reference image (a), marked with green and red.
    (b) are two heatmaps of the linear cross attention in LoFTR~\cite{sun2021loftr} when green and red pixels are queries.
    (c) are two heatmaps obtained from the vanilla cross attention.
    (d) are two heatmaps generated by our spot-guided attention.
    (e) are the comparison of the final matching results produced by LoFTR~\cite{sun2021loftr} (top) and our method (down).}
	\label{fig:motivation}
	\end{center}
}]

% \maketitle
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=1.0\linewidth]{graph/ASTR_motivation.pdf}
% 	\captionof{figure}{The visualization of the cross attention heatmaps and matching results.
%     We sample two similar adjacent points in the reference image (a), marked with green and red.
%     (b) are two heatmaps of the linear cross attention in LoFTR~\cite{sun2021loftr} when green and red pixels are queries.
%     (c) are two heatmaps obtained from the vanilla cross attention.
%     (d) are two heatmaps generated by our spot-guided attention.
%     (e) are the comparison of the final matching results produced by LoFTR~\cite{sun2021loftr} (top) and our method (down).}
% 	\label{fig:motivation}
% \end{figure*}

\renewcommand{\thefootnote}{\fnsymbol{footnote}} %将脚注符号设置为fnsymbol类型，即特殊符号表示
\footnotetext[1]{Equal Contribution} %对应脚注[1]
\footnotetext[2]{Corresponding Author} %对应脚注[2]

%%%%%%%%% ABSTRACT
\begin{abstract}
\input{abstract}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
\input{intro}

%------------------------------------------------------------------------
\section{Related Work}
\label{sec:rw}
\input{rw}

%-------------------------------------------------------------------------
\section{Our Approach}
\label{sec:method}
\input{method}

%-------------------------------------------------------------------------
\section{Experiments}
\label{sec:expr}
\input{expr}

%------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}
\input{conclusion}

\clearpage

%%%%%%%%% TITLE - PLEASE UPDATE
\twocolumn[{
\begin{center}
\textbf{
\Large Adaptive Spot-Guided Transformer for Consistent Local Feature Matching\\
\rule[3pt]{1.0cm}{0.1em}Supplementary Material\rule[3pt]{1.0cm}{0.1em}
}
\end{center}
}]
% \title{
% Adaptive Spot-Guided Transformer for Consistent Local Feature Matching\\
% \rule[3pt]{1.0cm}{0.1em}Supplementary Material\rule[3pt]{1.0cm}{0.1em}
% }

% \maketitle
% \maketitle
In this supplementary material, we first introduce the general sparse attention operator in Section~\ref{sec:1}.
In Section~\ref{sec:2}, we provide some details about our experiment.
In Section~\ref{sec:3}, we show additional visualizations about the spot-guided attention and adaptive scaling modules.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\linewidth]{graph/sparse_attention.pdf}
  \vspace{-6mm}
\caption{
  The illustration of our general sparse attention operator.
  }\label{fig:sparse}
\end{figure}

%%%%%%%%% BODY TEXT
\section{General Sparse Attention Operator}
\label{sec:1}

% Please follow the steps outlined below when submitting your manuscript to the IEEE Computer Society Press.
% This style guide now has several important modifications (for example, you are no longer warned against the use of sticky tape to attach your artwork to the paper), so all authors should read this new version.

% Due to irregular key/value token number for each query in Sopt Attention, naive implementation by PyTorch is not efficient for memory and computation, which use a mask to set unwanted values in the attention map to $0$.
% Instead, inspired by PointNet and Stratified Transformer, we implement a memory and computationally efficient Spot Attention operator using CUDA, which only compute the necessary attention between much less query/key tokens.

% We can divide an vanilla attention operator into 3 steps.
% First, compute attention map $A=QK^T$.
% Then, compute softmax $S=\mathrm{softmax}(A/\sqrt{d_k})$,
% Finally, compute result $O=SV$.

% In step 1, because only few results in $A$ are needed for Spot Attention, we do not directly compute the full $A$.
% Instead, suppose we actually need to compute $L_m$ dot products between all query and key tokens.
% Suppose two arrays $M_q$ and $M_k$, whose length are both $L_m$, and $\left( M_q[i], M_k[i] \right)$ indicate a dot production between $Q[M_q[i]]$ and $K[M_k[i]]$ is needed.
% So we only compute $L_m$ times dot productions and the result is $attn$, which is also an array of $L_m$ length and $attn[i]=Q[M_q[i]]K[M_k[i]]^T$.

% In step 2, we apply scatter $\mathrm{softmax}$ on the elements in $attn$ with the same $M_q[i]$, denote the result as $\mathrm{attn\_softmax}$.

% In step 3, we compute the result
% \begin{equation}\label{eq:ep1}
% \noindent O[q]=\sum_{M_q[i]=q}{\mathrm{attn\_softmax}[i] \cdot V[M_k[i]]}.
% \end{equation}
% All of three steps is implemented in CUDA, see our github repository for more details.

% Comparing to the naive implementation using PyTorch, our highly optimized implementation using CUDA reduce the memory and time complexity from $\mathcal{O}(N_q \cdot N_k \cdot N_h \cdot N_d)$ to $\mathcal{O}(L_m \cdot N_h \cdot N_d)$, where $N_q$ and $N_k$ is the number of query and key tokens, $N_h$ is the number of attention heads, and $N_d$ is the dimension of each head, $N_s$ is the number of token in the spot area.
% Considering $L_m \ll N_q \cdot N_k$, our implementation is much more efficient than naive implementation.
% Furthermore, this implementation can not only be used for Spot Attention, but also to accelerate the case where where the numbers of key/value corresponding to queries is not the same.

Due to irregular key/value token number for each query in Spot Attention, the naive implementation by PyTorch~\cite{paszke2019pytorch} is not efficient for memory and computation, which uses a mask to set unwanted values in the attention map to $0$.
More generally, the same problem also exists when the numbers of key corresponding to queries are not the same.
Inspired by PointNet~\cite{qi2017pointnet} and Stratified Transformer~\cite{lai2022stratified},
%{\color{red}we implement a memory and computationally efficient general sparse attention operator using CUDA,
%%
%which only compute the necessary attention between much less query/key tokens.}
%
%
%{\color{red}
%We can divide an vanilla attention operator into 3 steps.
%First, compute attention map $A=QK^T$.
%Then, compute softmax $S=\mathrm{softmax}(A/\sqrt{d_k})$,
%Finally, compute result $O=SV$.}
%
%
%{\color{red}
%In step 1, because only few results in $A$ are needed for Sparse Attention, we do not directly compute the full $A$.
%Instead, suppose we actually need to compute $L_m$ dot products between all query and key tokens.
%Suppose two arrays $M_q$ and $M_k$, whose length are both $L_m$, and $\left( M_q[i], M_k[i] \right)$ indicate a dot production between $Q[M_q[i]]$ and $K[M_k[i]]$ is needed.
%So we only compute $L_m$ times dot productions and the result is $attn$, which is also an array of $L_m$ length and $attn[i]=Q[M_q[i]]K[M_k[i]]^T$.
%
%
%
%
%In step 2, we apply scatter $\mathrm{softmax}$ on the elements in $attn$ with the same $M_q[i]$, denote the result as $\mathrm{{attn}_s}$.
%
%In step 3, we compute the result
%\begin{equation}\label{eq:ep1}
%\noindent O[q]=\sum_{M_q[i]=q}{{attn}_s[i] \cdot V[M_k[i]]}.
%\end{equation}
%All of three steps is implemented in CUDA.
%
%Comparing to the naive implementation using PyTorch~\cite{paszke2019pytorch}, our highly optimized implementation using CUDA reduce the memory and time complexity from $\mathcal{O}(N_q \cdot N_k \cdot N_h \cdot N_d)$ to $\mathcal{O}(L_m \cdot N_h \cdot N_d)$, where $N_q$ and $N_k$ is the number of query and key tokens, $N_h$ is the number of attention heads, and $N_d$ is the dimension of each head, $N_s$ is the number of token in the spot area or other general sparse attention.
%Considering $L_m \ll N_q \cdot N_k$, our implementation is much more efficient than naive implementation.
%
%In particular, we also calulate the matching matrix in spot-guided attention in this way and set the probability of unrelated pixels to 0, which can greatly reduce the memory and computation costs.
%}
we implement a general sparse attention operator 
using CUDA that is efficient in terms of memory and computation.
We attempt to only compute the necessary attention between much less query/key tokens.


We can divide a vanilla attention operator into 3 steps.
Inputs are grouped as query $Q$, key $K$ and value $V$.
First, the attention map $A$ is computed by dot production as $A=QK^T$.
Then, a softmax operator is performed on the attention map: $A_s=\mathrm{softmax}(A/\sqrt{d_k})$.
Finally, the updated query $O$ can be obtained by $O=A_s V$.
We optimize  these three steps separately.

In the step 1, because only a few results in $A$ are useful for sparse attention, we do not need to compute the full $A$.
Instead, we compute the dot productions between $L_m$ pairs of query and key.
% Given two index arrays $M_q$ and $M_k$, whose length are both $L_m$, and $\left( M_q[i], M_k[i] \right)$ indicate that a dot production between $Q[M_q[i]]$ and $K[M_k[i]]$ is needed.
$M_q$ and $M_k$ record the indexes of query and key tokens whose dot productions are needed.
The length of $M_q$ and $M_k$ are both $L_m$.
% , so we only compute $L_m$ dot productions.
%  and the result is $attn$, which is also an array of $L_m$ length and $attn[i]=Q[M_q[i]]K[M_k[i]]^T$.
Here, we denote the sparse attention map as $attn$, which is calculated by
\begin{equation}\label{eq:ep1}
\noindent attn[i]=Q[M_q[i]]K[M_k[i]]^T, \; i=0,1,\cdots, L_m-1.
\end{equation}

% In step 2, we apply grouped $\mathrm{softmax}$ on the elements in $attn$ with the same $M_q[i]$, denote the result as $\mathrm{{attn}_s}$.
In the step 2, we group the elements in $attn$ with the same query index and apply $\mathrm{softmax}$ on each group.
The result is denoted as $attn_s$.


In the step 3, we compute the updated query
\begin{equation}\label{eq:ep2}
O[q]=\sum_{M_q[i]=q}{{attn}_s[i] \cdot V[M_k[i]]}.
\end{equation}

All of three steps are implemented in CUDA.
% see our github repository for more details.

Compared with the naive implementation using PyTorch~\cite{paszke2019pytorch}, our highly optimized implementation reduces the memory and time complexity from $\mathcal{O}(N_q \cdot N_k \cdot N_h \cdot N_d^2)$ to $\mathcal{O}(L_m \cdot N_h \cdot N_d^2)$, where $N_q$, $N_k$ and $N_h$ are separately the numbers of query tokens, key tokens and attention heads, and $N_d$ is the dimension of each head.
Considering $L_m \ll N_q \cdot N_k$, our implementation is much more efficient than the naive implementation.

In particular, we also calculate the matching matrix in spot-guided attention in this way and set the probability of unrelated pixels to 0, which can greatly reduce the memory and computation cost.


\begin{figure}[t]
  \centering
  \includegraphics[width=0.75\linewidth]{graph/learning_rate.pdf}
  \vspace{-3mm}
\caption{
Learning rate curve while training on MegaDepth~\cite{li2018megadepth}.
  }\label{fig:learning_rate}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{graph/ASTR_spot_supp.pdf}
  \vspace{-6mm}
\caption{
Visualizations of vanilla and spot-guided attention maps on MegaDepth~\cite{li2018megadepth} (outdoor) and ScanNet~\cite{dai2017scannet} (indoor).
  }\label{fig:spot}
\end{figure}

\section{Experimental Details}
\label{sec:2}

\subsection{Training Details}\label{sec:2.1}
To reduce the GPU memory, we randomly sample $50\%$ of ground truth matches to supervise the matching matrix at the coarse stage.
And we sample $20\%$ of the maximum number of coarse-level possible matches at the fine stage.
We train ASTR on MegaDepth~\cite{li2018megadepth} for 15 epochs.
The initial learning rate is $1 \times 10^{-3}$, with a linear learning rate warm-up for 15000 iterations.
The learning rate curve is shown in Figure~\ref{fig:learning_rate}.

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{graph/ASTR_scale_supp.pdf}
  \vspace{-6mm}
\caption{
Visualizations of grids from adaptive scaling module and corresponding depth maps on MegaDepth~\cite{li2018megadepth}.
Note that we use depth values with scale uncertainty to compose the depth maps.
  }\label{fig:scale}
\end{figure}


\subsection{Differences between Baseline and LoFTR}\label{sec:2.2}
%
%
There are two main differences between our baseline and LoFTR~\cite{sun2021loftr}.

\textbf{(1) Normalized Positional Encoding.}
LoFTR~\cite{sun2021loftr} adopts the absolute sinusoidal positional encoding by following~\cite{carion2020end}:
\begin{equation}\label{eq:ep2}
  % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
\mathrm{PE}_i(x,y) = \left\{
  \begin{array}{ll}
  \mathrm{sin}(w_k \cdot x), & i = 4k \\
  \mathrm{cos}(w_k \cdot x), & i = 4k + 1 \\
  \mathrm{sin}(w_k \cdot y), & i = 4k + 2 \\
  \mathrm{cos}(w_k \cdot y), & i = 4k + 3 \\
  \end{array},
\right.
\end{equation}
where $w_k = \frac{1}{10000^{2k/d}}$, $d$ denotes the number of feature channels and $i$ is the index for feature channels.
Considering the gap in image resolution between training and testing, we utilize the normalized positional encoding as~\cite{chen2022aspanformer}, which is proven to mitigate the impact of image resolution changes in~\cite{chen2022aspanformer}.
The normalized positional encoding $\mathrm{NPE}_i(\cdot,\cdot)$ can be expressed as
\begin{equation}\label{eq:ep2}
\mathrm{NPE}_i(x,y) = \mathrm{PE}_i(x * \frac{W_{train}}{W_{test}}, y * \frac{H_{train}}{H_{test}}),
\end{equation}
where $W_{train/test}$ and $H_{train/test}$ are width and height of training/testing images.

\textbf{(2) Convolution in Attention.}
Chen et al.~\cite{chen2022aspanformer} find that replacing the self attention with convolution can improve the performance.
Hence, we deprecate self attention and MLP,  and utilize a $3 \times 3$ convolution in our ASTR.

\subsection{CNN Backbone}\label{sec:2.3}
Here we leverage a deepened version of Feature Pyramid Network (FPN)~\cite{lin2017feature}, which achieves a minimum resolution of 1/32.
% The number of feature channels at each stage is [128,128,196,256,256,256].
The initial dimension for the stem is still 128 as LoFTR~\cite{sun2021loftr}, and the number of feature channels for subsequent stages is [128, 196, 256, 256, 256].

\section{Visualization Results}\label{sec:3}

In Figure~\ref{fig:spot}, we pick up two similar adjacent pixels as queries and visualize the corresponding attention maps of vanilla and our spot-guided attention for comparison.
%
The vanilla attention mechanism is vulnerable to repetitive textures, while our spot-guided attention can focus on the correct areas in these repeated texture regions.
%
Because large scale variation occurs frequently on outdoor datasets,
%
we mainly visualize the grids from the adaptive scaling module and corresponding depth maps on MegaDepth~\cite{li2018megadepth}.
% With adaptive scaling module, our ASTR can handle large scale changes well.
% Our adaptive scaling module can produce aligned grids according to depth information.
As shown in Figure~\ref{fig:scale}, our adaptive scaling module can adjust the size of grids according to depth information.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{CVPR2023bib}
}

\end{document}
