In this section, we evaluate our ASTR with extensive experiments.
First of all, we introduce implementation details, followed by experiments on five benchmarks and some visualizations.
Finally, we conduct a series of ablation studies to verify the effectiveness of each component.

\subsection{Implementation Details}\label{4.1}
We implement the proposed model in Pytorch~\cite{paszke2019pytorch}.
Our ASTR is trained on the MegaDepth dataset~\cite{li2018megadepth}.
In the training phase, we input images with the size of $832 \times 832$ for training.
The CNN extractor is a deepened ResNet-18~\cite{he2016deep} with features at $1/32$ resolution.
In spot-guided attention, we set the kernel size of local region $l$ to 5 and $k$ to 4 in $\mathrm{topk}$.
Threshold $\theta_c$ in coarse matching is chosen to 0.2.
% Temperature coefficient $\tau$ is 10.
At the fine stage, window size $s_i$ in the reference image is fixed to 5, and window size $s_j$ in the source image will be adaptively calculated according to the depth information.
In particular, $s_j/s_i$ is clamped into $[1, 3]$.
Our network is trained for 15 epochs with a batch size of 8 by Adam~\cite{kingma2014adam} optimizer.
The initial learning rate is $1 \times 10^{-3}$.
In order to establish spot-guided attention efficiently, we implement a highly optimized general sparse attention operator based on CUDA.
Please refer to the Supplementary Material for more details about the operator.
\begin{table}[ht]
	\centering
	\small
	\caption{Evaluation on HPatches~\cite{balntas2017hpatches} for homography estimation.}
	\label{tab:HPatches_result}
	\vspace{-3mm}
	\scalebox{0.75}{
		\begin{tabular}{c l c c c c}
			\hline
            \multirow{2}{*}{Category} &\multicolumn{1}{c}{\multirow{2}{*}{Method}} &\multicolumn{3}{c}{Homography est. AUC} &\multirow{2}{*}{matches} \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{3-5}
                    & &{@3px}  &{@5px}  &{@10px} \\
			\hline
            \multirow{5}{*}{Detector-based} &D2Net~\cite{dusmanu2019d2}+NN &23.2 &35.9 &53.6 &0.2K \\

			                                &R2D2~\cite{r2d2}+NN &50.6 &63.9 &76.8 &0.5K \\

                                            &DISK~\cite{tyszkiewicz2020disk}+NN &52.3 &64.9 &78.9 &1.1K \\

                                            &SP~\cite{detone2018superpoint}+SuperGlue~\cite{sarlin2020superglue} &53.9 &68.3 &81.7 &0.6K \\

                                            &Patch2Pix~\cite{zhou2021patch2pix} &46.4 &59.2 &73.1 & 1.0k \\
            \hline
            \multirow{6}{*}{Detector-free} &Sparse-NCNet~\cite{rocco2020efficient} &48.9 &54.2 &67.1 &1.0K \\ 

                                            & COTR~\cite{jiang2021cotr} &41.9 &57.7 &74.0 &1.0K \\
                                            
                                            &DRC-Net~\cite{li20dualrc} &50.6 &56.2 &68.3 &1.0K \\
                                            
                                            &LoFTR~\cite{sun2021loftr} &65.9 &75.6 &84.6 &1.0K \\           

                                            &PDC-Net+~\cite{truong2023pdc} &66.7 & 76.8 & 85.8 & 1.0k \\

			                                &\textbf{ASTR(ours)}       &\bf 71.7 &\bf 80.3 &\bf 88.0 &1.0K   \\
			\hline
		\end{tabular}
	}
\end{table}

\subsection{Homography Estimation}\label{4.2}
\textbf{Dataset and Metric.}
HPatches~\cite{balntas2017hpatches} is a popular benchmark for image matching.
Following~\cite{dusmanu2019d2} , we choose 56 sequences under significant viewpoint changes and 52 sequences with large illumination variation to evaluate the performance of our ASTR trained on MegaDepth~\cite{li2018megadepth}.
We use the same evaluation protocol as LoFTR~\cite{sun2021loftr}.
% \textbf{Metric.} 
We report the area under the cumulative curve (AUC) of the corner error distance up to 3, 5, and 10 pixels, respectively.
We limit the maximum number of output matches to 1k.

\textbf{Results.}
In Table~\ref{tab:HPatches_result}, we can see that our ASTR achieves new state-of-the-art performance on HPatches~\cite{balntas2017hpatches} under all error thresholds, which strongly proves the effectiveness of our method.
ASTR outperforms the best method before ({PDC-net+}~\cite{truong2023pdc}), achieving a large margin of $\textbf{4.4\%}$ under 3 pixels, $\textbf{3.5\%}$ under 5 pixels, and $\textbf{2.5\%}$ under 10 pixels.
Thanks to the proposed spot-guided aggregation module and adaptive scaling module, our method can yield more accurate matches under extreme viewpoint and illumination variations.
% limiting the number of matches.  
% Table~\ref{tab:HPatches_result} summarizes the performance comparison between our ASTR and state-of-the-art image matching methods on HPatches dataset.

\subsection{Relative Pose Estimation}\label{4.3}
\textbf{Dataset and Metric.}
We use MegaDepth~\cite{li2018megadepth} and ScanNet~\cite{dai2017scannet} to demonstrate the performance of our ASTR in relative pose estimation.
MegaDepth~\cite{li2018megadepth} is a large-scale outdoor dataset that contains 1 million internet images of 196 different outdoor scenes.
Each scene is reconstructed by COLMAP~\cite{schonberger2016structure}. 
Depth maps as intermediate results can be converted to ground truth matches.
We sample the same 1500 pairs as ~\cite{sun2021loftr} for testing.
All test images are resized such that their longer dimensions are 1216.
ScanNet~\cite{dai2017scannet} is usually used to validate the performance of indoor pose estimation.
It is composed of monocular sequences with ground truth poses and depth maps.
Wide baselines and extensive textureless regions in image pairs make ScanNet~\cite{dai2017scannet} challenging.
For a fair comparison, we follow the same testing pairs and evaluation protocol as ~\cite{sun2021loftr}.
And all test images are resized to $640 \times 480$.
Note that we use our ASTR trained on MegaDepth~\cite{li2018megadepth} to evaluate its performance on ScanNet~\cite{dai2017scannet}.   
% \textbf{Metric.}
We report the AUC of the pose error at thresholds $(5^{\circ}, 10^{\circ}, 20^{\circ})$, where pose error is the maximum angular error in rotation and translation.
The angular error is computed between the ground truth pose and the predicted pose.

\begin{table}[t]
	\centering
	\small
	\caption{Evaluation on MegaDepth~\cite{li2018megadepth} for outdoor relative position estimation.}
	\label{tab:MegaDepth_result}
	\vspace{-3mm}
	\scalebox{0.75}{
		\begin{tabular}{c l c c c c}
			\hline
            \multirow{2}{*}{Category} &\multicolumn{1}{c}{\multirow{2}{*}{Method}} &\multicolumn{3}{c}{Pose estimation AUC} \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{3-5}
                    & &{@$5^\circ$}  &{@$10^\circ$}  &{@$20^\circ$} \\
			\hline
            \multirow{2}{*}{Detector-based} &SP~\cite{detone2018superpoint}+SuperGlue~\cite{sarlin2020superglue} &42.2 &59.0 &73.6 \\
            
                                            &SP~\cite{detone2018superpoint}+SGMNet~\cite{chen2021learning} &40.5 &59.0 &73.6 \\
            \hline
            \multirow{7}{*}{Detector-free} &DRC-Net~\cite{li20dualrc} &27.0 &42.9 &58.3 \\
                                            
                                            &PDC-Net+(H)~\cite{truong2023pdc}  &43.1  &61.9  &76.1 \\
                                            
                                            &LoFTR~\cite{sun2021loftr} &52.8 &69.2 &81.2 \\                          
                                            
                                            &MatchFormer~\cite{wang2022matchformer} &53.3 &69.7 &81.8 \\
                                            
                                            &QuadTree~\cite{tang2022quadtree} & 54.6 & 70.5 & 82.2 \\
                                            
                                            &ASpanFormer~\cite{chen2022aspanformer} &55.3 &71.5 &83.1 \\

			                                &\textbf{ASTR(ours)}       &\textbf{58.4} & \textbf{73.1} &\ \textbf{83.8}  \\
			\hline
		\end{tabular}
	}
\end{table}

\begin{table}[t]
	\centering
	\small
	\caption{Evaluation on ScanNet~\cite{dai2017scannet} for indoor relative position estimation. * indicates models trained on MegaDepth~\cite{li2018megadepth}.}
	\label{tab:ScanNet_result}
	\vspace{-3mm}
	\scalebox{0.75}{
		\begin{tabular}{c l c c c c}
			\hline
            \multirow{2}{*}{Category} &\multicolumn{1}{c}{\multirow{2}{*}{Method}} &\multicolumn{3}{c}{Pose estimation AUC} \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{3-5}
                    & &{@$5^\circ$}  &{@$10^\circ$}  &{@$20^\circ$} \\
			\hline
            \multirow{3}{*}{Detector-based} &D2-Net~\cite{dusmanu2019d2}+NN &5.3 &14.5 &28.0 \\

											&SP~\cite{detone2018superpoint}+OANet~\cite{zhang2019learning} &11.8 &26.9 &43.9 \\
											
											&SP~\cite{detone2018superpoint}+SuperGlue~\cite{sarlin2020superglue} &16.2 &33.8 &51.8 \\
            \hline
            \multirow{5}{*}{Detector-free}  &DRC-Net~\cite{li20dualrc}* &7.7 &17.9 &30.5 \\                         
            
                                            &MatchFormer~\cite{wang2022matchformer}* &15.8  &32.0   &48.0 \\

											&LoFTR-OT~\cite{sun2021loftr}* &16.9 &33.6 &50.6 \\
                                            
                                            &Quadtree~\cite{tang2022quadtree}* &19.0 &37.3  &53.5 \\

			                                &\textbf{ASTR(ours)*}       & \textbf{19.4} & \textbf{37.6} & \textbf{54.4}  \\
			\hline
		\end{tabular}
	}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{graph/ASTR_comparision_exp.pdf}
    \vspace{-6mm}
	\caption{
	Qualitative results of dense matching on MegaDepth~\cite{li2018megadepth} and ScanNet~\cite{dai2017scannet}.
	% The green color indicates epipolar error within $5 \times 10^{-4}$ for indoor scenes and $1 \times 10^{-4}$ for outdoor scenes (in the normalized image coordinates).
    }\label{fig:qualitative}
    \vspace{-3mm}
\end{figure}

\textbf{Results.}
As shown in Table~\ref{tab:MegaDepth_result}, our ASTR outperforms other state-of-the-art methods on MegaDepth~\cite{li2018megadepth}.
In particular, our ASTR improves by $\textbf{3.1\%}$ in AUC$@5^{\circ}$ and $\textbf{1.6\%}$ in AUC$@10^{\circ}$.
Table~\ref{tab:ScanNet_result} summarizes the performance comparison between the proposed ASTR and state-of-the-art methods on ScanNet~\cite{dai2017scannet}.
Our ASTR ranks first when only considering models not trained on ScanNet~\cite{dai2017scannet}, indicating the impressive generalization of our method.
Thanks to the proposed spot-guided aggregation module and adaptive scaling module, our method can yield more correct matches, resulting in more accurate pose estimation.
In order to further demonstrate the effectiveness of our ASTR, in Figure~\ref{fig:qualitative}, we visually demonstrate the comparison with other methods on the matching result.
Notably, our methods can better handle the challenges such as textureless areas, repetitive patterns, and scale variations.

% Comparison results in Fig.~\ref{fig:comparison} verify that our ASTR follows the consistency principle to obtain accurate matching results.

% our spot-guided aggreagtion module has the ability to focus the interaction area of most pixels to the correct area.
% In contrast, the regions of linear attention are very scattered.
\subsection{Visual Localization}\label{4.4}
\textbf{Dataset and Metric.}
In this experiment, InLoc~\cite{taira2018inloc} and Aachen Day-Night v1.1~\cite{zhang2021reference} are used to verify the ability of our ASTR in visual localization.
InLoc~\cite{taira2018inloc} is an indoor dataset with 9972 RGBD images, of which 329 RGB images are employed as queries for visual localization.
The challenge of InLoc~\cite{taira2018inloc} mainly comes from textureless regions and repetitive patterns under large viewpoint changes.
In Aachen Day-Night v1.1~\cite{zhang2021reference}, 824 day-time images and 191 night-time images are chosen as queries for outdoor visual localization.
Large illumination and viewpoint changes pose challenges for Aachen~\cite{zhang2021reference}.
For both benchmarks, we evaluate the performance of our ASTR trained on MegaDepth~\cite{li2018megadepth} in the same way as ~\cite{sun2021loftr}.
% \textbf{Metric.}
The metrics of Inloc~\cite{taira2018inloc} and Aachen~\cite{zhang2021reference} are the same, which measure the percentage of images registered within given error thresholds.

\begin{table}[t]
	\centering
	\small
	\caption{Visual localization evaluation on the InLoc~\cite{taira2018inloc} benchmark.}
	\label{tab:Inloc_result}
	\vspace{-3mm}
	\scalebox{0.75}{
		\begin{tabular}{l c c}
			\hline
            \multicolumn{1}{c}{\multirow{2}{*}{Method}} & DUC1 & DUC2 \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{2-3}
                     & \multicolumn{2}{c}{$\left(0.25m, 10^\circ\right)$ / $\left(0.5m, 10^\circ\right)$ / $\left(1m, 10^\circ\right)$} \\
			\hline
            % \multirow{1}{*}{Detector-based} & SP~\cite{detone2018superpoint}+SuperGlue~\cite{sarlin2020superglue} &  49.0 / 68.7 / 80.8 & 53.4 / 77.1 / 82.4  \\
            Patch2Pix~\cite{zhou2021patch2pix}(w.SP~\cite{sarlin2020superglue}+CAPS~\cite{wang2020learning}) & 42.4 / 62.6 / 76.3 & 43.5 / 61.1 / 71.0 \\
            LoFTR~\cite{sun2021loftr} & 47.5 / 72.2 / 84.8 & 54.2 / 74.8 / \textbf{85.5} \\
            MatchFormer~\cite{wang2022matchformer} & 46.5 / 73.2 / 85.9 & \textbf{55.7} / 71.8 / 81.7 \\
            ASpanFormer~\cite{chen2022aspanformer} & 51.5 / \textbf{73.7} / 86.4 & 55.0 / 74.0 / 81.7 \\
            \textbf{ASTR(ours)} & \textbf{53.0} / \textbf{73.7} / \textbf{87.4} & 52.7 / \textbf{76.3} / 84.0 \\
			\hline
		\end{tabular}
	}
\end{table}

\begin{table}[t]
	\centering
	\small
	\caption{Visual localization evaluation on the Aachen Day-Night benchmark v1.1~\cite{zhang2021reference}.}
	\label{tab:Aachen_result}
	\vspace{-3mm}
	\scalebox{0.75}{
		\begin{tabular}{ l c c}
			\hline
            \multicolumn{1}{c}{\multirow{2}{*}{Method}} & Day & Night \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{2-3}
                     & \multicolumn{2}{c}{$\left(0.25m, 2^\circ\right)$ / $\left(0.5m, 5^\circ\right)$ / $\left(1m, 10^\circ\right)$} \\
			\hline
			\multicolumn{3}{l}{\textbf{Localization with matching pairs provided in dataset}} \\
			\hline
            R2D2~\cite{r2d2}+NN & - & 71.2 / 86.9 / 98.9 \\
            ASLFeat~\cite{luo2020aslfeat}+NN & - & 72.3 / 86.4 / 97.9 \\
            SP~\cite{detone2018superpoint}+SuperGlue~\cite{sarlin2020superglue} & - & 73.3 / 88.0 / 98.4 \\
            SP~\cite{detone2018superpoint}+SGMNet~\cite{chen2021learning} & - & 72.3 / 85.3 / 97.9 \\
			\hline
			\multicolumn{3}{l}{\textbf{Localization with matching pairs generated by HLoc}} \\
			\hline
            % SP~\cite{detone2018superpoint}+SuperGlue~\cite{sarlin2020superglue} & 89.8 / 96.1 / 99.4 & 77.0 / 90.6 / 100.0 \\
            LoFTR~\cite{sun2021loftr} & 88.7 / 95.6 / 99.0 & 78.5 / 90.6 / 99.0 \\
            ASpanFormer~\cite{chen2022aspanformer} & 89.4 / 95.6 / 99.0 & 77.5 / 91.6 / 99.0 \\
            AdaMatcher~\cite{huang2022adaptive} & 89.2 / \textbf{95.9}  / \textbf{99.2} & \textbf{79.1} / \textbf{92.1} / \textbf{99.5} \\
			\textbf{ASTR(ours)} & \textbf{89.9} / 95.6 / \textbf{99.2} & 76.4 / \textbf{92.1} / \textbf{99.5}    \\
			\hline
		\end{tabular}
	}
        \vspace{-3mm}
\end{table}

\textbf{Results.}
For InLoc~\cite{taira2018inloc} benchmark, our method achieves the best performance on DUC1 and is on par with state-of-the-art methods on DUC2 (in Tabel~\ref{tab:Inloc_result}).
% For Aachen~\cite{zhang2021reference} benchmark, our ASTR outperforms other methods on Day scenes and performs comparative with others on Night scenes (in Tabel~\ref{tab:Aachen_result}).
For Aachen~\cite{zhang2021reference} benchmark, our ASTR performs comparative with others on Day and Night scenes (in Tabel~\ref{tab:Aachen_result}).
Overall, our method exhibits strong generalization ability in visual localization.

\subsection{Ablation Study}\label{4.5}

\begin{table}[t]
	\centering
	\small
	\caption{Ablation Study of each component on MegaDepth~\cite{li2018megadepth}.}
	\label{tab:ablation_study}
	\vspace{-3mm}
	\scalebox{0.75}{
		\begin{tabular}{c c c c c c c}
    		\hline
    		% \multirow{2}{*}{Method} & 
			\multirow{2}{*}{Index} & \multirow{2}{*}{Multi-Level} & \multirow{1}{*}{Spot-Guided} & \multirow{2}{*}{Scaling}
		    &\multicolumn{3}{c}{Pose estimation AUC} \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{5-7}
                & &($l=5,k=4$) & & {@$5^\circ$}  &{@$10^\circ$}  &{@$20^\circ$} \\
            % \hline
		    % LoFTR~\cite{sun2021loftr} & & & & & 45.9 & 63.3 & 76.8 \\
		    \hline
		    % \multirow{4}{*}{ASTR(ours)}
		      1 & & & & 45.6 & 62.2 & 75.3 \\
		      2 & \checkmark & & & 46.7 & 63.1 & 76.3 \\
		      3 & \checkmark & \checkmark & & 47.7 & 64.5 & 77.4\\
		      4 & \checkmark & \checkmark & \checkmark & \bf 48.3 & \bf 65.0 & \bf 77.7\\
		     \hline
		\end{tabular}
	}
\end{table}

To deeply analyze the proposed method, we perform detailed ablation studies on MegaDepth~\cite{li2018megadepth} to evaluate the effectiveness of each component in ASTR.
Here, we use images with a size of 544 for training and evaluation.
As shown in Table~\ref{tab:ablation_study}, we intend to gradually add these components to the baseline.
The baseline (Index-1) we used is slightly different from LoFTR~\cite{sun2021loftr}.
More details can be found in Supplementary Material.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{graph/ASTR_heatmaps.pdf}
    \vspace{-6mm}
	\caption{
	Visualization of vanilla and spot-guided cross attention maps on MegaDepth~\cite{li2018megadepth} (outdoor) and ScanNet~\cite{dai2017scannet} (indoor).
    }\label{fig:spot_vis}
\end{figure}

\textbf{Effectiveness of Spot-Guided Aggregation Module.}
We divide the spot-guided aggregation module into multi-level cross attention and spot-guided attention for ablation studies.
We first add vanilla cross attention layers at 1/32 resolution to the baseline (Index-2 in Table~\ref{tab:ablation_study}).
Comparing the results of Index-2 and Index-1, we conclude that 1/32 resolution global interaction across images is beneficial for image matching.
Then, in Index-3, linear attention layers at 1/8 resolution are substituted for the spot-guided attention layers.
The performance of Index-3 is improved compared with Index-2, which verifies the effectiveness of our spot-guided attention.
In Figure~\ref{fig:spot_vis}, we visualize vanilla and our spot-guided cross attention maps for contrast, showing that spot-guided attention can indeed avoid interference from unrelated areas.
% Please refer to Supplementary Material for ablation studies on values of $l$ and $k$.

\begin{table}[t]
	\centering
	\small
	\caption{Ablation Study with different $k$ and $l$ in spot-guided attention on MegaDepth~\cite{li2018megadepth}.}
	\label{tab:k_l_result}
	\vspace{-3mm}
	\scalebox{0.75}{
		\begin{tabular}{c c c c}
    		\hline
    		% \multirow{2}{*}{Method} & 
			\multirow{2}{*}{$k$($l=5$)} &\multicolumn{3}{c}{Pose estimation AUC} \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{2-4}
                & {@$5^\circ$}  &{@$10^\circ$}  &{@$20^\circ$} \\
            % \hline
		    % LoFTR~\cite{sun2021loftr} & & & & & 45.9 & 63.3 & 76.8 \\
		    \hline
		    % \multirow{4}{*}{ASTR(ours)}
		      1  & 46.0 & 62.7 & 76.2 \\
			  2  & 47.5 & 64.0 & 77.1\\
		      3  & 47.3 & 63.8 & 76.7 \\
		    %   3  & 47.5 & 64.0 & 77.1\\
		      4  & \bf 47.7 & \bf 64.5 & \bf 77.4\\
			  5  & 47.1 & 63.7 & 77.0\\
			  6  & 46.9 & 63.6 & 76.6\\
		     \hline
		\end{tabular}
	}
	\scalebox{0.85}{
		\begin{tabular}{c c c c}
    		\hline
    		% \multirow{2}{*}{Method} & 
			\multirow{2}{*}{$l$($k=4$)} &\multicolumn{3}{c}{Pose estimation AUC} \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{2-4}
                & {@$5^\circ$}  &{@$10^\circ$}  &{@$20^\circ$} \\
            % \hline
		    % LoFTR~\cite{sun2021loftr} & & & & & 45.9 & 63.3 & 76.8 \\
		    \hline
		    % \multirow{4}{*}{ASTR(ours)}
		      3  & 46.7 & 63.2 & 76.1 \\
			  5  & \bf 47.7 & \bf 64.5 & \bf 77.4\\
		      7  & 47.2 & 63.4 & 76.8 \\
		      9  & 43.0 & 60.5 & 74.8\\
		     \hline
		\end{tabular}
	}
\end{table}

% \begin{table}[t]
% 	\centering
% 	\small
% 	\caption{\textbf{Ablation Study with different $l$ in spot-guided attention.} Higher is better.}
% 	\label{tab:MegaDepth_result}
% 	\scalebox{1.0}{
% 		\begin{tabular}{c c c c}
%     		\hline
%     		% \multirow{2}{*}{Method} & 
% 			\multirow{2}{*}{$l$($k=4$)} &\multicolumn{3}{c}{Pos estimation AUC} \\
% 			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
% 			\cline{2-4}
%                 & {@$5^\circ$}  &{@$10^\circ$}  &{@$20^\circ$} \\
%             % \hline
% 		    % LoFTR~\cite{sun2021loftr} & & & & & 45.9 & 63.3 & 76.8 \\
% 		    \hline
% 		    % \multirow{4}{*}{ASTR(ours)}
% 		      3  & 46.7 & 63.2 & 76.1 \\
% 			  5  & \bf 47.7 & \bf 64.5 & \bf 77.4\\
% 		      7  & 47.2 & 63.4 & 76.8 \\
% 		      9  & 43.0 & 60.5 & 74.8\\
% 		     \hline
% 		\end{tabular}
% 	}
% \end{table}

To maximize the effectiveness of our spot-guided attention, we explore how to set suitable parameters $l$ and $k$.
First, in the setting of Index-3, we fix $l=5$ and vary $k$ from 1 to 6.
After observing the results in Table~\ref{tab:k_l_result}, the performance drops when $k$ is smaller than 4 or larger than 4.
Then, we fix $k=4$ and vary $l$ from 3 to 9.
As shown in Table~\ref{tab:k_l_result}, we find that the model achieves the best performance at $l=5$.
The reason may be that the spot area is too small to provide sufficient information from another image when using small $k$ or $l$.
With large $k$ or $l$, for a certain pixel, some matching areas of low confidence or dissimilar points will damage its feature aggregation.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{graph/ASTR_grids.pdf}
    \vspace{-6mm}
	\caption{
	Visualization of grids from adaptive scaling module on MegaDepth~\cite{li2018megadepth} (outdoor) and ScanNet~\cite{dai2017scannet} (indoor).
    }\label{fig:grids_vis}
\end{figure}


\textbf{Effectiveness of Adaptive Scaling Module.}
As shown in Table~\ref{tab:ablation_study}, comparing the results of Index-4 and Index-3, we can see that the performance is improved, which indicates that coarse-level matching results are better refined with adaptive scaling module.
In Figure~\ref{fig:grids_vis}, we visualize the cropped grids from adaptive scaling module, indicating that our adaptive scaling module can adaptively crop grids of different sizes according to scale variations.

