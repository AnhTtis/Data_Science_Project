\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{graph/ASTR_pipeline.pdf}
    \caption{
    The architecture of ASTR.
    Our ASTR consists of two major components: spot-guided aggregation module and the adaptive scaling module.
    ``Cross Attention'' means vanilla cross attention, unless otherwise stated.
    Please refer to the text for detailed architecture.
    }\label{fig:method}
\end{figure*}
In this section, we present our proposed Adaptive Spot-guided Transformer (ASTR) for Consistent Local Feature Matching.
The overall architecture is illustrated in Figure~\ref{fig:method}.
\subsection{Overview}\label{3.1}
As shown in Figure~\ref{fig:method}, the proposed ASTR mainly consists of two modules, including a spot-guided aggregation module and an adaptive scaling module.
Here we give a brief introduction to the entire process.
Given an image pair $I_{Ref}$ and $I_{Src}$, to start with, we extract multi-scale feature maps of each image through a shared Feature Pyramid Network (FPN)~\cite{lin2017feature}.
We denote feature maps with the size of $1/i$ as $F^{1/i} = \{F^{1/i}_{ref}, F^{1/i}_{src}\}$.
Then, $F^{1/32}$ and $F^{1/8}$ are fed into the spot-guided aggregation module for coarse-matching and depth maps.
% The acquisition of coarse matching is divided into two stages
Here, the coarse matching result is acquired in three phases.
First, we need to compute the similarity matrix, which can be given by $S(i,j) = \tau \langle F^{1/8}_{ref}(i), F^{1/8}_{src}(j) \rangle$ with flattened features, where $\tau$ is the temperature coefficient.
Then we perform dural-softmax operator on $S$ to calculate matching matrix $\mathrm{P_c}$:
\begin{small}
\begin{equation}\label{eq:epc7}
    \noindent \mathrm{P_c}(i, j) = \mathrm{softmax}(S(i, :))(i, j) \cdot \mathrm{softmax}(S(:, j))(i, j).
\end{equation}
\end{small}
Finally, we use the mutual nearest neighbor strategy and the threshold $\theta_c$ to filter out the coarse-matching result $M_c$.
% After that, we calculate depth maps with size of ${1/8}$ from coarse-matching results.
According to depth information and coarse-matching result, we can crop different size grids on the high-resolution feature map $F^{1/2}$.
% We can crop different size grids on high resolution feature map $F^{1/2}$ according to coarse-matching results and depth information.
After linear self and cross attention layers, features of the cropped grids are used to produce the final fine-level matching result.
% In sopt-guided aggregation module, we first interacte globally across images to update $F^{1/8}$ and $F^{1/32}$.
% After that, we conduct vanilla cross attention on $F^{1/32}$ and
\subsection{Spot-Guided Aggregation Module}\label{3.2}
Correct matching always satisfies the local matching consistency, i.e., the matching points of two similar adjacent pixels are also close to each other in the other image.
When humans establish dense matches between two images, they will first scan through the two images quickly and keep in mind some landmarks that are easier to match correctly.
For those trouble points similar to surrounding landmarks, it is not easy to obtain correct matches in the beginning.
But now, they can focus attention around the matching points of landmarks to revisit trouble points' matches.
In this way, more correctly matched landmarks are obtained.
After several iterations of the above process, eventually, they will get the matching result for the whole image.
Inspired by this idea, we design  a spot-guided aggregation module.
% Section~\ref{3.2.1} demonstrates the design of the entire spot-guided aggregation module.
Section~\ref{3.2.1} introduces the preliminaries of vanilla attention and linear attention.
Section~\ref{3.2.2} describes our spot-guided attention mechanism.
Section~\ref{3.2.3} demonstrates the design of the entire spot-guided aggregation module.


\subsubsection{Preliminaries}\label{3.2.1}
% As an important part in transformer,
The calculation of vanilla attention requires three inputs: query $Q$, key $K$, and value $V$.
% we can obtain the affinity matrix with Scaled Dot-Product Attention.
% Furthermore, the weighted sum of the value is computed through the following equation:
The output of vanilla attention is a weighted sum of the value, where the weight matrix is determined by the query and its corresponding key.
The process can be described as
\begin{small}
\begin{equation}\label{eq:epc1}
    \mathrm{Attention}(Q, K, V) = \mathrm{softmax}(QK^T)V.
\end{equation}
\end{small}
However, in vision tasks, the size of the weight matrix $\mathrm{softmax}(QK^T)$ increases quadratically as the image resolution grows.
When the image resolution is large, the memory and computational cost of vanilla attention is unacceptable.
To solve this problem, 
Linear attention ~\cite{katharopoulos2020transformers} is proposed to replace the softmax operator with the product of two kernel functions:
\begin{small}
\begin{equation}\label{eq:epc2}
    \mathrm{Linear\_attention}(Q, K, V) = \phi (Q) (\phi (K^T)V),
\end{equation}
\end{small}
where $\phi(\cdot) = \mathrm{elu}(\cdot) + 1$.
Since the number of feature channels is much smaller than the number of pixels, the computational complexity is reduced from quadratic to linear.
% Although linear attention makes it possible to process high-resolution images, some empirical studies show that linear attention does not work well.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{graph/ASTR_spot.pdf}
    \caption{
    The illustration of our spot-guided attention.
    % Here, Sel. Score is the result of element-wise multiplication of Sim. Score and Conf. Score.
    % $p$ only interacts with spot areas on the source image.
    }\label{fig:spot}
\end{figure}

\subsubsection{Spot-Guided Attention}\label{3.2.2}
It is known from the local matching consistency that the matching points of similar adjacent pixels are also close to each other.
In Figure~\ref{fig:spot}, we illustrate the case that the reference image as query aggregates features from the source image.
% As shown in Fig 3, for any pixel A on the reference feature map,
Given reference and source feature maps $F^{1/8} = \{F^{1/8}_{ref}, F^{1/8}_{src}\}$, 
we compute a matching matrix $P_s$ across images.
% {\color{red}We illustrate the case that the reference image as query retrieves information from the source image.}
% {\color{blue}We illustrate the case in Fig.~\ref{fig:spot} that the reference image as query aggregates features from the source image.}
% % As shown in Fig 3, for any pixel A on the reference feature map,
% It is known from the local matching consistency that the matching points of similar adjacent pixels are also close to each other.
% Thus, for any pixel on the reference feature map, we should focus the search range on matching regions of local high-quality similar points.
For any pixel $p$ in Figure~\ref{fig:spot}, we first compute the similarity score $\mathrm{S_{sim}}(p) \in R^{l^2 - 1}$ between $p$ and other pixels in the $l \times l$ area around $p$.
Specifically, the similarity score can be obtained as
% \begin{equation}\label{eq:epc3}
% \noindent S(p_i) = \langle F^{1/8}_{Ref.}(p), F^{1/8}_{Ref.}(p_i) \rangle,
%     % Score_S(p) = \mathop{softmax}\limits_{i}(\{S(p_i)\}_{p_i \in N(p)}),
%     % Sim(p) = \langle F^{1/8}_{Ref.}(p), F^{1/8}_{Ref.}(p_i) \rangle,
% \end{equation}
\begin{small}
\begin{equation}\label{eq:ep3}
    % S(p_i) = \langle F^{1/8}_{Ref.}(p), F^{1/8}_{Ref.}(p_i) \rangle,
\noindent \mathrm{S_{sim}}\left(p\right) = \mathop{\mathrm{softmax}}\limits_{i}\left(\left\{\langle F^{1/8}_{ref}(p), F^{1/8}_{src}(p_i) \rangle\right\}_{p_i \in N(p)}\right),
\end{equation}
\end{small}
% Sim(p) = \langle F^{1/8}_{Ref.}(p), F^{1/8}_{Ref.}(p_i) \rangle,
% $Sim(p)$ denotes the similarity score of pixel p in reference image.
where $\langle \cdot, \cdot \rangle$ is the inner product, and $N(p)$ is the set of pixels in the $l \times l$ field around pixel $p$. 
% (except pixel $p$).
In addition, we should also consider the reliability of points in $N(p)$.
% The confidence score can be viewed as the similarity score between each pixel on the reference image
For each $p_i \in N(p)$, confidence can be viewed as the highest similarity to all pixels on the source images.
% within the last search range of $p_i$.
Meanwhile, we can also get the matching point position of $p_i$, denoted as $\mathrm{Loc}(p_i)$.
Hence, $\mathrm{Loc}(p_i)$ and confidence score $\mathrm{S_{conf}}(p) \in R^{l^2 - 1}$ can be computed in the following way:
% \begin{equation}\label{eq:epc5}
% \noindent C(p_i) = \mathop{max}\limits_{j}\mathop{softmax}\limits_{j}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
%     % Score_C(p) = {Conf(p_i)}_{p_i \in N(p)},
% \end{equation}
% \begin{equation}\label{eq:epc6}
% \noindent P(p_i) = \mathop{argmax}\limits_{j}\mathop{softmax}\limits_{j}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
%     % Score_C(p) = {Conf(p_i)}_{p_i \in N(p)},
% \end{equation}
\begin{small}
\begin{equation}\label{eq:epc4}
    \begin{aligned}
    % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
    % &\mathrm{P_s}\left(p_i,:\right) =\mathop{\mathrm{softmax}}\limits_{j}\left(\left\{\langle F^{1/8}_{Ref}(p_i), F^{1/8}_{Src}(p_j) \rangle\right\}_{p_j \in I_{src}}\right),
    % \\
    % &\mathrm{C}\left(p_i\right) = \mathop{\mathrm{max}}\limits_{j}\left(\mathrm{P_s}\left(p_i,:\right)\right),
    % \\
    &\mathrm{S_{conf}}\left(p\right) = \left\{\mathop{\mathrm{max}}\left(\mathrm{P_s}\left(p_i,:\right)\right)\right\}_{p_i \in N(p)}.
    \\
    &\mathrm{Loc}\left(p_i\right) = \mathop{\mathrm{argmax}}\left(\mathrm{P_s}\left(p_i,:\right)\right),
    % \\
    % &\mathrm{S_{conf}}\left(p\right) = \left\{\mathop{\mathrm{max}}\left(\mathrm{P_s}\left(p_i,:\right)\right)\right\}_{p_i \in N(p)}.
    \end{aligned}
\end{equation}
\end{small}
% where $I_{src}$ represents the pixels on .
% where $\mathrm{P_s}(p_i,:)$ denotes the matching probability in $I_{src}$.
% In particular, the initial search range of each point is the entire image.
Combining two scores, we select $p$ and top-k points $\mathrm{Topk}(p)$ whose matching points are used as seed points $\mathrm{Seed}(p)$:
% \begin{equation}\label{eq:epc8}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% \noindent topk(p) = topk\{Score_S(p) \cdot Score_C(p)\},
% \end{equation}
\begin{small}
\begin{equation}\label{eq:epc5}
    \begin{aligned}
    % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
    &\mathrm{Topk}(p) = \{p\} \cup \mathrm{topk}\{\mathrm{S_{sim}}(p) \cdot \mathrm{S_{conf}}(p)\},
    \\
    &\mathrm{Seed}(p) = \{\mathrm{Loc}(q)\}_{q \in \mathrm{Topk}(p)},%\mathrm{topk}\{\mathrm{Score_S}(p) \cdot \mathrm{Score_C}(p)\}},
    \end{aligned}
\end{equation}
\end{small}
Following that, we extend $l \times l$ regions centered on these seed points $\mathrm{Seed}(p)$ on $I_{src}$, which are the spot areas of $p$.
Finally, cross attention is performed between $p$ and corresponding spot areas.
After exchanging the source image and the reference image, the source feature map is updated in the same way. 

%  and expand $k \times k$ areas around the matching points of these points
\subsubsection{Spot-Guided Feature Aggregation}\label{3.2.3}
For the input features $F^{1/32}$ and $F^{1/8}$, $F^{1/32}$ is updated by vanilla cross attention, and $F^{1/8}$ is updated by linear cross attention for initialization.
Then, two features of different resolutions are fed into the spot-guided aggregation blocks.
In each block, $F^{1/32}$ and $F^{1/8}$ are first fused into each other in the following way:
% \begin{equation}\label{eq:epc10}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% \noindent \hat{F}^{1/32} = F^{1/32} + Conv_{1 \times 1}(Down(F^{1/8})),
% \end{equation}
\begin{small}
\begin{equation}\label{eq:epc6}
    \begin{aligned}
    % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
    &\hat{F}^{1/32} = F^{1/32} + \mathrm{Conv_{1 \times 1}}(\mathrm{Down}(F^{1/8})),
    \\
    &\hat{F}^{1/8} = F^{1/8} + \mathrm{Conv_{1 \times 1}}(\mathrm{Up}(F^{1/32})),
    \end{aligned}
\end{equation}
\end{small}
% \begin{equation}\label{eq:epc10}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% \noindent \hat{F}^{1/32} = F^{1/32} + Conv_{1 \times 1}(Down(F^{1/8})),
% \end{equation}
where $\hat{F}^{1/32}$ and $\hat{F}^{1/8}$ are features after fusion. $\mathrm{Down}(\cdot)$ and $\mathrm{Up}(\cdot)$ are downsampling and upsampling.
And then, $\hat{F}^{1/32}$ aggregate features across images by vanilla attention.
In the meantime, $\hat{F}^{1/8}$ aggregate features across images by spot-guided attention.
After four spot-guided aggregation blocks, $1/32$-resolution features are fused into $1/8$-resolution features, which are used to obtain the coarse-matching result $M_c$.

% , which are flattened to $F^{1/8}_{Ref} \in R^{m \times c}$ and $F^{1/8}_{Src} \in R^{n \times c}$.

% Similarity matrix can be given by $S(i,j) = \tau \langle F^{1/8}_{Ref}(i), F^{1/8}_{Ref}(j) \rangle$, where $\tau$ is the temperature coefficient.
% Then we perform dural-softmax operator on $S$ to calculate probability matrix $\mathrm{P_c}$:
% \begin{equation}\label{eq:epc7}
%     \noindent \mathrm{P_c}(i, j) = \mathrm{softmax}(S(i, :)) \cdot \mathrm{softmax}(S(:, j)).
% \end{equation}
% Finally, we use the mutual nearest neighbor strategy and the threshold $\theta_c$ to filter out the coarse-matching result $M_c$.
% to produce the final coarse-matching matrix with size of $1/8$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{graph/ASTR_adaptive_scaling.pdf}
    \caption{
    The illustration of our adaptive scaling module.
    On the left is the reference image, whose optical center is $C_{Ref}$.
    On the right is the source image, whose optical center is $C_{Src}$.
    $x_i$ and $x_j$ are the projections of the real-world point $X$.
    }\label{fig:adaptive_scaling}
\end{figure}

\subsection{Adaptive Scaling Module}\label{3.3}
At the fine stage, previous methods usually crop fixed-size grids based on the coarse matching result.
When there is a large scale variation, fine matching may fail since the ground-truth matching points are out of grids.
Thus, we refer to depth information to adaptively crop grids of different sizes between images.
Section~\ref{3.3.1} describes the way to obtain depth information from the coarse-matching result.
Section~\ref{3.3.2} demonstrates the process of adaptively cropping grids.

\subsubsection{Depth Information}\label{3.3.1}

% Since we have already obtained coarse-level matching result, the relative pose of the two pictures $\{R, T\}$
With the coarse-level matching result, we can obtain the relative pose of two images $\{R, T\}$ through RANSAC~\cite{fischler1981random}.
It should be noted that the $T$ calculated here has a scale uncertainty, i.e., $T_{real} = \alpha T$, where $\alpha$ is the scale factor.
Given the image coordinates of any pair of matching points $\{x_i, x_j\}$ from coarse-level matching result, they satisfy the following equation:
\begin{small}
\begin{equation}\label{eq:epc8}
    % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% \noindent d_j K^{-1}_j \binom{x_j^T}{1} = d_i R K^{-1}_i \binom{x_i^T}{1} + \alpha T ,
\noindent d_j K^{-1}_j (x_j, 1)^T = d_i R K^{-1}_i (x_i, 1)^T + \alpha T ,
\end{equation}
\end{small}
where $d_i$ and $d_j$ are the depth values of $x_i$ and $x_j$.
$K_i$ and $K_j$ are corresponding camera intrinsics.
We let $p_i = R K^{-1}_i (x_i, 1)^T$ and $p_j = K^{-1}_j (x_j, 1)^T$.
From Equation~\eqref{eq:epc8} it can be deduced that:
\begin{small}
\begin{equation}\label{eq:epc9}
\begin{aligned}
    % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
&d_j p_j = d_i p_i + \alpha T, \\
\Rightarrow &\left\{
    \begin{array}{lr}
    (d_j / \alpha) p_j \wedge p_i = 0 + T \wedge p_i, \\
    0 = (d_i / \alpha) p_i \wedge p_j + T \wedge p_j,
    \end{array}
\right.
\\
\Rightarrow &\left\{
    \begin{array}{lr}
    d_j / \alpha = \mathrm{mean}(\mathrm{div}(T \wedge p_i, p_j \wedge p_i)), \\
    d_i / \alpha = \mathrm{mean}(\mathrm{div}(- T \wedge p_j, p_i \wedge p_j)),
    \end{array}
\right.
% \noindent \frac{d_j}{\alpha} = \frac{div(T \wedge p_i, p_j \wedge p_i)}{3},
% \\
% \noindent \frac{d_i}{\alpha} = \frac{div(- T \wedge p_j, p_i \wedge p_j)}{3},
\end{aligned}
\end{equation}
\end{small}
where $\wedge$ indicates outer product.
$\mathrm{div}(\cdot, \cdot)$ denotes element-wise division between two vectors.
$\mathrm{mean}(\cdot)$ is the scalar mean of each component of a vector.
In this way, we have obtained depth information of $x_i$ and $x_j$ with scale uncertainty.

\subsubsection{Adaptive Scaling Strategy}\label{3.3.2}
As shown in Figure~\ref{fig:adaptive_scaling}, $x_i$ and $x_j$ are a pair of matching points at the coarse stage.
$d_i$ and $d_j$ are depth values of $x_i$ and $x_j$.
To begin with, we crop a $s_i \times s_i$ region centered on $x_i$.
When the scale changes too much, the correct matching point $\widetilde{x_j}$ may be beyond the $s_i \times s_i$ region around $x_j$.
Because everything looks small in the distance and big on the contrary, the size of cropped grid $s_j$ should satisfy:
\begin{small}
\begin{equation}\label{eq:epc10}
    % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
\noindent \frac{s_j}{s_i} = \frac{d_i}{d_j} = (\frac{d_i}{\alpha})(\frac{d_j}{\alpha})^{-1},
\end{equation}
\end{small}
Following the above approach, we can crop different sizes of grids adaptively according to the scale variation.
% Features in the grids centered on $x_i$ and $x_j$ are further fed into linear self and cross layers.
After the same refinement as LoFTR~\cite{sun2021loftr}, we get the final matching position $\widetilde{x_j}$ of $x_i$.
% Please refer to Supplementary Material for more details.

\subsection{Loss Function}\label{3.3}
Our loss function mainly consists of three parts, spot matching loss, coarse matching loss, and fine matching loss.
Spot matching loss is the cross entropy loss to supervise the matching matrix during spot-guided attention:
\begin{equation}\label{eq:epc11}
    % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
\noindent L_s = - \frac{1}{\left\lvert M^{gt}_c \right\rvert} \sum_{(i,j) \in M^{gt}_c} \log \mathrm{P_s}(i,j),
\end{equation}
where $M^{gt}_c$ is the ground truth matches at coarse resolution.
%  over the spot-guided attention areas.
Coarse matching loss is also the cross entropy loss to supervise the coarse matching matrix:
\begin{equation}\label{eq:epc12}
    % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
\noindent L_c = - \frac{1}{\left\lvert M^{gt}_c \right\rvert} \sum_{(i,j) \in M^{gt}_c} \log \mathrm{P_c}(i,j).
\end{equation}
% where $M^{gt}_c$ is the ground truth matches at coarse resolution.
Fine matching loss $L_f$ is a weighted $L_2$ loss same as LoFTR~\cite{sun2021loftr}.
% \begin{equation}\label{eq:epc13}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% \noindent L_f = \frac{1}{\left\lvert M^{gt}_f \right\rvert} \sum_{(i,j) \in M^{gt}_f} \frac{1}{\sigma ^2(i)}{\left\lVert j - j_{gt}\right\rVert}_2 ,
% \end{equation}
% where $M^{gt}_f$ is the ground truth matches at fine resolution and $\sigma^2(i)$ means the total variance of the heatmap.
Therefore, our total loss is:
\begin{equation}\label{eq:epc14}
    % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{Ref.}(p_i), F^{1/8}_{Src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
\noindent L_{total} = L_s + L_c + L_f.
\end{equation}
