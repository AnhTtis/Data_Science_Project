%Establishing correspondences between two images is a key step for many applications in computer vision including 3D reconstruction,
%image retrieval, visual localization, etc.
In this section, we briefly review several research lines that are related to sparse matching methods, dense matching methods, and vision Transformer.

% \noindent\textbf{Sparse Matching Methods.}
% The sparse matching methods~\cite{tian2017l2,ono2018lf,dusmanu2019d2,luo2020aslfeat,r2d2,sarlin2020superglue}.

\noindent\textbf{Local Feature Matching}.
Local feature matching can categorized into detector-based and detector-free methods.
Detector-based methods can be divided into three stages: feature detection, feature description, and feature matching.
SIFT~\cite{lowe2004distinctive} and ORB~\cite{rublee2011orb} are the most popular hand-crafted local features, while learning-based methods~\cite{r2d2,dusmanu2019d2,detone2018superpoint,rublee2011orb,barroso2019key,zhou2017progressive,he2021consistency} also obtain good performance improvement compared to classical methods.
There are also some works focusing on improving the feature matching stage.
D2Net~\cite{dusmanu2019d2} fuses the detection and description stages.
R2D2~\cite{r2d2} attempts to train a network to find reliable and repeatable local features.
SuperGlue~\cite{sarlin2020superglue} proposes an attention-based GNN network to update extracted local features in alternating self and cross attentions.
However, detector-based methods rely on local feature extractors, which limits the performance in challenging scenarios such as repetitive textures, weak textures, and illumination changes.
Unlike detector-based approaches, detector-free approaches do not require a local feature detector, but find dense feature matching between pixels directly.
The classical methods~\cite{lucas1981iterative,horn1981determining} exists, but few of them outperform detector-based methods.
Learning-based methods change the game, which can be divided into cost-volume-based methods~\cite{rocco2018neighbourhood,li20dualrc,truong2020glu,truong2023pdc} and Transformer-based methods~\cite{jiang2021cotr,sun2021loftr,wang2022matchformer,chen2022aspanformer,huang2022adaptive,chen2022guide}.
Good performance have been achieved by cost-volume-based methods, but most of them are limited by the small receptive field of CNN, which is overcome by Transformer-based methods~\cite{sun2021loftr}.
Detector-free methods attain better performance in local feature matching, so we adopt this paradigm as the baseline.

% \noindent\textbf{Detector-Based Local Feature Matching}.
% Detector-based methods, also known as \textit{extract-to-match} or \textit{sparse} methods, can be divided into three stages: feature detection, feature description, and feature matching.
% SIFT~\cite{lowe2004distinctive} and ORB~\cite{rublee2011orb} are the most popular hand-crafted local features, while learning-based methods, such as LIFT~\cite{yi2016lift} and SuperPoint~\cite{detone2018superpoint}, also obtain good performance improvement compared to classical methods.
% There are also some works focusing on improving the feature matching stage.
% D2Net~\cite{dusmanu2019d2} fuses the detection and description stages. R2D2~\cite{r2d2} attempts to train a network to find reliable and repeatable local features.
% SuperGlue~\cite{sarlin2020superglue} proposes an attention-based GNN network to update extracted local features in alternating self and cross attentions, which archives a significant performance improvement.
% However, detector-based methods rely on local feature extractors, which limits the performance in challenging scenarios such as repetitive textures, weak textures, and illumination changes.

% \noindent\textbf{Detector-Free Local Feature Matching.}
% Unlike detector-based approaches, detector-free approaches do not require a local feature detector, but find dense feature matching between pixels directly.
% Therefore, the detector-free method is also called the \textit{dense} method.
% The classical methods contain Lucas-Kanade~\cite{lucas1981iterative} and Horn-Schunck~\cite{horn1981determining}, but few of them outperform detector-based methods.
% Learning-based methods change the game, which can be divided into cost-volume-based methods~\cite{rocco2018neighbourhood,li20dualrc,truong2020glu,truong2020gocor} and Transformer-based methods~\cite{jiang2021cotr,sun2021loftr,wang2022matchformer,chen2022aspanformer}.
% % The precursor of cost volume is NCNet~\cite{rocco2018neighbourhood}, which constructs a 4D cost volume that is regularized by 4D convolutions.
% % Although effective, the large computational and memory overhead limits the input image size and performance.
% % DRC-Net~\cite{li20dualrc} follows NCNet~\cite{rocco2018neighbourhood}, builds cost volumes at two different scales, and then fuses them to obtain dense matches.
% % GLU-Net~\cite{truong2020glu} also uses a multi-level structure that combines global and local correlations in a feature pyramid.
% % GOCor~\cite{truong2020gocor} improves the feature correlation layers to disambiguate repeated patterns.
% % But these methods are limited by the small receptive field of CNN.
% % At the same time, Transformer-based local feature matching has also been proposed.
% % COTR~\cite{jiang2021cotr} uses a Transformer decoder to functionally obtain the matching point for any query point.
% % LoFTR~\cite{sun2021loftr} follows SuperGlue~\cite{sarlin2020superglue}, interleaves self and cross attention to update dense features in the coarse stage, and refines the matches in the fine stage.
% % MatchFormer~\cite{wang2022matchformer} proposes a multi-scale fusion, and Aspanformer~\cite{chen2022aspanformer} uses flow maps and probabilistic models to select an adaptive attention span.
% Detector-free methods attain better performance in local feature matching, so we adopt this paradigm as the baseline.


\noindent\textbf{Vision Transformer.}
Transformer~\cite{vaswani2017attention} has been proven to be better at capturing long-range correlations than CNN in vision tasks~\cite{meng2022adversarial,carion2020end,meng2022task}.
% While becoming a popular method in NLP, Transformer has also been widely used in vision tasks~\cite{dosovitskiy2020image,carion2020end,sun2021loftr}.
Despite the great success, the computational cost of vanilla attention at high resolution is unacceptable, so some approximations~\cite{katharopoulos2020transformers,liu2021swin,tang2022quadtree,wang2020linformer} have been proposed, which inevitably leads to performance degradation.
Linear Attention~\cite{katharopoulos2020transformers} approximates softmax with ELU~\cite{clevert2015fast} to reduce the computational complexity to linear but degrades the focusing ability of attention. Swin-Transformer~\cite{liu2021swin} limits attention in local windows, which harms the ability to establish long-range associations.
At the same time, QuadTree~\cite{tang2022quadtree} calculates attention in a coarse-to-fine manner, and ASpanFormer~\cite{chen2022aspanformer} proposes an adaptive method for selecting attention spans, but few of them consider local consistency.
% but they both lack local consistency.
Different from the existing attention mechanism, we explicitly model local consistency in our spot-guided attention without introducing excessive computation and memory costs.
% {\color{blue}Our method outperform vanilla attention in feature matching while greatly reduce computational overhead, and explicitly models local consistency.}

\noindent\textbf{Local Feature Matching with Scale Invariance.}
Scale variation is one of the main challenges faced by local feature matching.
Many works have explored solutions.
Hand-crafted local features~\cite{rublee2011orb,liu2010sift,bay2008speeded,rosten2006machine} use Gaussian pyramid model to alleviate the problem.
Following the hand-crafted methods, Some learning-based descriptors~\cite{r2d2,barroso2019key,barroso2020hdd,luo2020aslfeat,liu2021densernet,zhou2017progressive} also use the multi-scale representation.
ScaleNet~\cite{barroso2022scalenet} and Scale-Net~\cite{fu2021scale}, instead, try to directly estimate the scale ratio.
Another popular paradigm is to perform a wrap or scaling operation to eliminate the distortion caused by the scale variance.
GeoWrap~\cite{berton2021viewpoint} introduces a homography regression and warps images to increase overlap area.
OETR~\cite{chen2022guide} limits the keypoint detection in estimated overlap areas.
COTR~\cite{jiang2021cotr} estimates scale by finding co-visible regions, and then finds correspondence by recursively zooming.
However, most of above methods require significant modifications to the network architecture, and introduce additional computation overhead.
Therefore, we design a fully pluggable, lightweight and training-free module for coarse-to-fine architecture.