%Establish consistent and accurate matches for local features across images
%is a long-standing problem in computer vision
Local feature matching (LFM) is a fundamental task in computer vision, which aims to establish correspondence for local features across image pairs.
As a basis for many 3D vision tasks, local feature matching can be applied in Structure-from-Motion (SfM)~\cite{schonberger2016structure}, 3D reconstruction~\cite{dai2017bundlefusion}, visual localization~\cite{sattler2018benchmarking, taira2018inloc}, and pose estimation~\cite{grabner20183d, persson2018lambda}.
%
%Tremendous works in either sparse matching or dense matching have been proposed to find accurate correspondences, which have made
%great progress in the image matching field~\cite{detone2018superpoint,sarlin2020superglue,luo2020aslfeat,rocco2018neighbourhood,li20dualrc}.
%
Because of its broad applications, local feature matching has attracted substantial attention and facilitated the development of many researches~\cite{detone2018superpoint, li20dualrc, r2d2, rocco2018neighbourhood, sun2021loftr}.
%And thanks to the rapid growth of deep learning, these methods can learn discriminative local features in a data-driven manner, which have made great success in the local feature matching field.
%With the rapid growth of deep learning, tremendous learning-based matching methods[xxx] attempt to learn discriminative local features in a data-driven manner, which have achieved state-of-the-art performance.
%
However, finding consistent and accurate matches is
still difficult due to various challenging factors such as illumination variations, scale changes, poor textures, and repetitive patterns.
%

To deal with the above challenges, numerous matching methods have been proposed, which can be generally categorized into two major groups, including detector-based matching methods~\cite{barroso2019key, detone2018superpoint, dusmanu2019d2, ono2018lf, r2d2, sarlin2020superglue} and detector-free matching methods~\cite{huang2019dynamic, li20dualrc, rocco2020efficient, rocco2018neighbourhood, sun2021loftr, chen2022aspanformer}.
%The current predominant approaches for feature matching are sparse paradigm~\cite{barroso2019key,pautrat2020online,zhang2019learning,sarlin2020superglue,detone2018superpoint,ono2018lf,dusmanu2019d2,r2d2,luo2020aslfeat}. %
Detector-based matching methods require to first design a keypoint detector to extract the keypoints between two images, and then establish matches between these extracted keypoints.
%
%These methods usually involve the following steps: keypoint detection, feature description, matching, and outlier rejection.
The quality of detected keypoints will significantly  affect  the performance of detector-based matching methods.
% {\color{blue} have a significant impact on}
Therefore, many works aim to improve keypoint detection through multi-scale detection~\cite{luo2020aslfeat}, repeatable and reliable verification~\cite{r2d2}.
%For example, R2D2 try to predict two scores including the repeatable score and the reliable score, which are combined to produce both repeatable and reliable keypoints.
%ASLfeat implicitly applying multi-scale detection
%KeyNet [17], handcrafted and learned filters are combined to detect repeatable keypoints.
%Methods directly performing convolution upon multi-scale images
%
Thanks to the high-quality keypoints detected, these methods can achieve satisfactory performance while maintaining high computational and memory efficiency.
%The detector-based matching methods usually have high computational and memory efficiency because they only establish matches between the relatively small keypoint sets.
%
%In general, the performance of sparse matching methods depends heavily on whether the repeatable and reliable keypoints can be detected.
%
However, these detector-based matching methods may have difficulty in finding reliable matches in textureless areas, where keypoints are challenging to detect.
%
Differently, detector-free matching methods do not need to detect keypoints and try to establish pixel-level matches between local features.
In this way, it is possible to establish matches in the texture-less areas.
Due to the power of attention in capturing long-distance dependencies, many Transformer-based methods~\cite{sun2021loftr, tang2022quadtree, wang2022matchformer, chen2022aspanformer} have emerged in recent years.
% in the detector-free branch.
As a representative work,  considering the computation and memory costs, 
LoFTR~\cite{sun2021loftr} applies Linear Transformer~\cite{katharopoulos2020transformers} to aggregate global features at the coarse stage and then crops fixed-size grids for further refinement.
To alleviate the problem caused by scale changes, COTR~\cite{jiang2021cotr} calculate the co-visible area iteratively through attention mechanism.
% These Transformer-based methods achieve promising performance.
The promising performance of Transformer-based methods proves that attention mechanism is effective on local feature matching.
% , achieving promising performance.
%, which get a great performance.
% To address the potential impact of scale variation, adaptive-loftr allows many-to-one patch assignment in the fine-level.
% But [Quadtree] shows that linear transformer attends on large decentralized areas, to quickly focus on local regions, [Quadtree] proposes to compute attention in a coarse-to-fine manner.
% {\color{blue}achieving promising performance.
Nevertheless, some recent works~\cite{li2022depthformer, yang2021transformer} indicate Transformer lacks spatial inductive bias for continuous dense prediction tasks, which may cause inconsistent local matching results. 
%{\color{red} Although some works~\cite{li2022depthformer, yang2021transformer} indicate transformer lacks spatial inductive bias for continuous dense prediction tasks, which may cause inconsistent local matching results.
%The great performance of transformer-based methods proves that attention mechanism is very effective on local feature matching.}
% However, some works[depthformer] indicate transformer lacks spatial inductive bias in modeling the local information, which may cause inconsistent local matching results.

% For example, LoFTR applies linear transformer to aggregate global features across images, and then crops fixed-size grids for refinement.
% The great performance of LoFTR proves that modeling long-range dependency with attention mechanism is very effective on local feature matching.
% However, global interaction always introduces many irrelavent regions that affect feature updates, leading to some false matching results.
% However, visualization in [Quadtree] shows that linear transformer attends on large decentralized areas, so [Quadtree] attempts to focus on
% For example, LoFTR applies linear transformer to capture long-range dependencies at coarse stage,

% Based on the above discussions, in order to obtain the dense correspondence between images, the following two important issues should be taken into account:
By studying the previous matching methods, we sum up two
% {\color{blue}aspects that are ignored}
issues that are imperative for obtaining the dense correspondence between images.
% (1) {\color{red}\textbf{How to maintain local consistency.}}
(1) \textbf{How to maintain local consistency.}
The correct matching result usually satisfies the local matching consistency, i.e., for two similar adjacent pixels, their matching points are also extremely close to each other.
% As shown in Figure~\ref{fig:motivation} (b) and (c), 
Existing methods~\cite{sun2021loftr, wang2022matchformer,jiang2021cotr} utilize global attention in feature aggregation, introducing many irrelevant regions that affect feature updates.
% As an approximations of vanilla attention, linear attention will introduce many irrelavent regions that affect feature updates (see Fig.~\ref{fig:motivation} (b)), leading to false matching results (see Fig.~\ref{fig:motivation} (e)).
% So in Fig.~\ref{fig:motivation} (c), we adopt vanilla attention with the same features as the linear attention.
% However, vanilla attention is still disturbed by noisy or repetitive areas, and highlighted regions of the two pixels are inconsistent with each other.
% aggregate information from wrong regions, leading to false matching results.
Some pixels are disturbed by noisy or similar areas and aggregate information from wrong regions, leading to false matching results.
As shown in Figure~\ref{fig:motivation} (b), for two adjacent similar pixels, highlighted regions of global linear attention are decentralized and inconsistent with each other.
The inconsistency is also present in vanilla attention (see Figure~\ref{fig:motivation} (c)).
% Therefore, it is necessary to design an attention mechanism to maintain local consistency.
Therefore, it is necessary to utilize local consistency to focus the attention area on the correct place. 
% (2) {\color{red}\textbf{How to handle scale variation.}}
(2) \textbf{How to handle scale variation.}
In a coarse-to-fine architecture, since the attention mechanism at the coarse stage is not sensitive to scale variations, we should focus on the fine stage.
Previous methods~\cite{li20dualrc, sun2021loftr, wang2022matchformer, chen2022aspanformer} select fixed-size grids for matching at the fine stage.
However, when the scale varies too much across images, the correct match point may be out of the range of the grid, resulting in matching failure.
% Adaptive-loftr notices this problem, while their many-to-one strategy imposes additional computation and memory costs on the fine stage.
% Hence, an adaptive size grid extraction scheme needs to accommodate the variation in scale between images.
Hence, the scheme of cropping grids should be adaptively adjusted according to scale variation across views.

To deal with the above issues, we propose a novel Adaptive Spot-guided Transformer (ASTR) for consistent local feature matching, including a spot-guided aggregation module and an adaptive scaling module.
In the \textbf{spot-guided aggregation module}, towards the goal of maintaining local consistency, we design a novel attention mechanism called spot-guided attention: each point is guided by similar high-confidence points around it, focusing on a local candidate region at each layer.
Here, we also adopt global features to enhance the matching ability of the network in the candidate regions.
% help the network get correct matches in the candidate regions.
Specifically, for any point $p$, we pick the points with high feature similarity and matching confidence in the local area.
Their corresponding matching regions are used for the next attention of point $p$.
In addition, global features are applied to help the network to make judgments.
The coarse feature maps are iteratively updated in the above way.
With our spot-guided aggregation module, the red and green pixels are guided to the correct area, avoiding the interference of repetitive patterns (see Figure~\ref{fig:motivation} (d)).
In Figure~\ref{fig:motivation} (e), our ASTR produces more accurate matching results, which maintains local matching consistency. 
In the \textbf{adaptive scaling module}, to fully account of possible scale variations, we attempt to adaptively crop different sizes of grids for alignment.
In detail, we compute the corresponding depth map using the coarse matching result and leverage the depth information to crop adaptive size grids from the high-resolution feature maps for fine matching.

The contributions of our method could be summarized into three-fold:
(1) We propose a novel Adaptive Spot-guided Transformer (ASTR) for  local feature matching, including a spot-guided aggregation module and an adaptive scaling module.
(2) 
%{\color{red} We design a spot-guided aggregation module can focus on the candidate matching region and maintain local consistency, which avoids the interference of irrelavent regions.}
%
We design a spot-guided aggregation module that can maintain local consistency and be unaffected by irrelevant regions while aggregating features.
%
Our  adaptive scaling module is able to leverage depth information to adaptively crop different size grids for refinement.
(3) Extensive experimental results on five challenging benchmarks show that our proposed method performs favorably against state-of-the-art image matching methods.
