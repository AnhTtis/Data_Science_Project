% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{amsmath, bm}
\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage{wrapfig}
\usepackage{booktabs} % mid/top/bottom rules for table

\usepackage{times}
% space after command
\usepackage{xspace}
\newcommand{\sine}{SInE\xspace}

\usepackage[nocompress]{cite}


%KK
\usepackage[textsize=scriptsize
%           ,disable
           ]{todonotes}

\newcommand{\KK}[2][]{\todo[color=purple!20,#1]{KK: #2}}

\begin{document}
%
\title{Graph Sequence Learning for Premise Selection}
%
%\titlerunning{Captioning Conjectures}
%\titlerunning{Captioning Conjectures}

% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%

\author{Edvard K. Holden
\and
Konstantin Korovin
}

\author{Edvard K. Holden
\and
Konstantin Korovin
}
%
\authorrunning{Holden \& Korovin}

\institute{The University of Manchester}

% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%

\maketitle              % typeset the header of the contribution
%
\begin{abstract}

Premise selection is crucial for large theory reasoning as the sheer size of the problems quickly leads to resource starvation.
This paper proposes a premise selection approach inspired by the domain of image captioning, where language models automatically generate a suitable caption for a given image.
Likewise, we attempt to generate the sequence of axioms required to construct the proof of a given problem.
This is achieved by combining a pre-trained graph neural network with a language model.
We evaluated different configurations of our method and experience a 17.7\% improvement gain over the baseline. 



\keywords{Automated Theorem Proving  \and Machine Learning \and Premise Selection \and Sequence Learning \and Graph Neural Network}
\end{abstract}

\section{Introduction}
\input{introduction}


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Related Works}\label{sec:related_works}
\input{related_works}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 




% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% METHOD section
\section{Axiom Captioning}\label{sec:captioning}
\input{method_caption}


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 


%\section{Graph Neural Network Embeddings}
\section{Problem Embeddings}\label{sec:problem_embedding}\label{sec:embedding}

\input{method_graph}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\section{Experimental Evaluation}\label{sec:evaluation}

% Whether to use the 10K or the 6K results (automatic for tables, nothing else. Should it be?)
%\newcommand{\vocabsize}{vocab_10k}
\newcommand{\vocabsize}{vocab_6k}


\input{experiments}


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 



% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Conclusion}\label{sec:conclusion}

In this paper, we presented a novel approach for performing premise selection.
It parallels image captioning, combining transfer learning on graph neural networks with sequence learning.
The graph representation provides a holistic view of the problem structure, while the sequence model uses this embedding to predict the sequence of axioms necessary for the proof.
Our evaluation found that the model performs better when the GNN is pre-trained on a related and supervised task with embeddings containing information of all the nodes in the graph.
Further, we observed that effective axiom captioning requires a fixed axiom order and a greedy decoder sampler.
Lastly, the proposed approach dramatically increases the number of solved problems when complemented with \sine and significantly outperforms related machine learning methods.


\subsection*{Acknowledgements}
%We thank Michael Rawson for providing the formula to graph translator utilised in this work.

We thank Michael Rawson for providing a translator from formulas to graphs, utilised in this work.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
%\begin{thebibliography}{8}
%\end{thebibliography}

\bibliographystyle{plain}
\bibliography{ref}



\end{document}
