


The image captioning problem can be stated as follows: given an image $I$ and a dictionary of words $\Omega$, generate an accurate and grammatical caption $S$, consisting of words from $\Omega$.
This challenging problem goes beyond the already non-trivial task of identifying the image objects.
Rather, it requires identifying and comprehending: the objects, their attributes and their relation.
Moreover, this information must be decoded and represented as a grammatically correct sentence in the target language.


State-of-the-art image captioning approaches join the machine learning fields of image classification and language modelling.
An example of a captioning model based on the inject architecture is shown in Figure  \ref{fig:captioning_model}.
It consists of three components: an image encoder, a language model, and a dense output layer.
The image encoder extracts and embeds the image semantics as a feature vector.
The language model combines these salient features with an input word to produce an encoding of the current sequence.
Finally, the dense layer maps the encoding to a probability distribution over the vocabulary.


\begin{figure}
% https://docs.google.com/drawings/d/1emuPMr3dvgGAUhy2OM9iaIBYKOiIRq9SfijR7u7hyiw/edit
\centering
\includegraphics[scale=0.38]{diagrams/captioning_model.png} 
\caption{The inject architecture for image captioning.}
\label{fig:captioning_model}
\end{figure}


Despite the challenges of image captioning, the models produce appropriate and detailed image descriptions.
Due to their expressiveness, we believe these methods can be utilised for premise selection.
In the remaining parts of this section, we describe the sequence model.
%The embedding model is detailed in Section \ref{sec:problem_embedding}.





% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 


\subsection{Sequence Learning}

In the original task of image captioning, the model operates on pairs of images and captions in a target language.
In the context of premise selection, the images are replaced by problems and the captions are replaced with the axioms that appear in the proof of the problems.
Assume we have a problem $I$ with a corresponding proof $S^*$ and an axiom resource bank $\Omega$.
Next, we extract and impose an order on the $m$ axioms in $S^*$, resulting in $S =\langle s_1, \ldots , s_m \rangle$, $s_i \in \Omega$ for $1\leq i \leq m$. 
 %resulting in $S = \{s_1, \ldots , s_t \in \Omega^t \} $.
We describe the task of premise selection in the context of sequence learning as maximising the probability of producing the sequence of axioms used in the proof of a given problem.
Given the problem-axioms pair $(I, S)$ we can compute its log probability as:


$$
\log p (S | I) = \sum_{t=1}^m \log p(s_t | s_{t-1}, \ldots, s_1, I).
$$



We estimate $\log p(s_t|s_{t-1},\cdots, s_1, I)$ with the recurrent neural network (RNN) $\sigma$ with learnable parameters $\theta$.
RNNs exhibit a dynamic behaviour over a sequence of inputs due to their internal memory state $h_t$, which captures the previous inputs sequentially.
In particular, the output at step $t$ depends on the previous memory state $h_{t-1}$ and the input $s_{t-1}$.
The hidden state is defined as:

\[
% Should we start from t=1 and include s_0 as it is the start token?
h_t = 
\left\{
\begin{array}{r@{\quad}l}
     \sigma(I;\theta) & \text{if } t = 1,\\
    \sigma(h_{t-1}, s_{t-1};\theta)  & \text{otherwise}.\\
\end{array} \right.
\]

The RNN is trained to predict the next token in a sequence based on the previous token and the current memory state.
Over a training set of problem-axiom pairs  $\{(I^i, S^i)\}^N_{i=1}$, the model is trained to maximise the log probability of producing the correct sequence of axioms:
% https://arxiv.org/pdf/1609.06647v1.pdf 
$$
\theta^* = \argmax_\theta \sum_{I,S} \log p(S|I;\theta).
$$

Thus, the model predicts axioms based on the problem and the previously predicted axioms.
In our implementation, we use Long-Short Term Memory (LSTM)~\cite{lstm} cells as the underlying RNN.
LSTM is among the most popular RNN models due to its robustness towards vanishing and exploding gradients.





% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Axiom Captioning}



% https://arxiv.org/pdf/1703.09137.pdf
The generative axiom prediction model is constructed using the par-inject architecture \cite{DBLP:journals/corr/TantiGC17}, as illustrated in Figure \ref{fig:rnn_sequence}.
This architecture takes a token embedding $\textbf{s}$ and a problem embedding $\textbf{I}$ at each time step.
The model is given the special start token $s_{start}$ to initialise the axiom generation process.
Likewise, a special end token, $s_{end}$, represents the end of a sequence. 
Consequently, start and end tokens are added to each axiom sequence such that the model is trained on the target sequence  $\langle s_{start} , s_1, \ldots , s_m, s_{end} \rangle$.
Axioms with few occurrences in the dataset are replaced by the Out-Of-Vocabulary token $s_{unkown}$.
These three special tokens are included in the dictionary $\Omega$.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5


\begin{figure}[!htpb]
% TODO: make dense fo the other way?
% https://docs.google.com/drawings/d/1wKfGzKwfxPLpEPpUQPa3VjB_9eKDOIEU6TEq8eZafhU/edit - General Overview
% https://docs.google.com/drawings/d/1QWBVa-MxMCS2uvMNYijSFhlXOkZsaNIPSS--Af1x1js/edit
\centering
%\includegraphics[scale=0.38]{diagrams/rnn_sequence.png}
\includegraphics[width=0.95\linewidth]{diagrams/rnn_sequence_hidden.png} 
\caption{Recurrent Neural Network predicting the next token in a sequence.}
\label{fig:rnn_sequence}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


At training time, we apply teacher forcing., which feeds the next token of the training sequence to the model instead of its previous prediction.
This prevents the model from being unable to recover from poor predictions.
Hence, the prediction at each training step is expressed as:


\[
\hat{y_t} = 
\left\{
\begin{array}{r@{\quad}l}
      \sigma( \textbf{s}_{start}, \textbf{h}_{0}, \textbf{I} ; \theta ) & \text{if } t = 1,\\
    \sigma(\textbf{s}_{t-1}, \textbf{h}_{t-1}, \textbf{I} ; \theta)  & \text{otherwise.}\\
\end{array} \right.
\]

Where $\hat{y_t}$ is a probability distribution at time $t$ over the axioms in $\Omega$.






% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Neural Attention Captioning} \label{sec:attention}




The captioning decoder is fed a static input entity at each time step, but it can be advantageous to emphasise different parts of the embedding based on the current model state~\cite{https://doi.org/10.48550/arxiv.1502.03044}.
This is achieved through a separate attention network that dynamically weighs some input according to a model state.
In other settings, the attention mechanism can emphasise particular words in a sequence or regions of an image with respect to the model state.
In this scenario, the incentive of attention is to emphasise particular sections and elements of the averaged graph representation to enhance the embedding.




The attention mechanism computes a context vector which is used as input to the next stage of the model.
It is a weighted sum of the $n$ embedding elements where each weight is the quantity of attention applied to the corresponding element:



\[
\bm{c}_t = \sum_{i=1}^n \alpha_{t,i} \bm{I}_i.
\]


The weights $\alpha_{t,i}$ are computed based on an alignment score function which measures how well each element matches the current state.
The scores are scaled by softmax into weights in the range of $[0, 1]$ where the sum of the weights equals to 1:
\begin{wrapfigure}[14]{}{0.35\textwidth}
    % https://docs.google.com/drawings/d/1eh82u7T8aDP7xJfElDQtM6h3W1nNxy-wxhJ-VI9aWnw/edit
\includegraphics[width=0.98\linewidth]{diagrams/attention.png} 
%\caption{Attention Neural Network}
\caption{Language model with Bahdanau attention.}
\label{fig:attention}
\end{wrapfigure}


\[
\alpha_{t,i} = 
\frac{
\exp(score(\bm{I}_i, \bm{h}_t))
}
{
\sum_{j=1}^n \exp(score(\bm{I}_j, \bm{h}_t))
}.
\]


In Section~\ref{exp:attention}, we experimented with both Loung attention~\cite{DBLP:journals/corr/LuongPM15} and Bahdanau attention~\cite{https://doi.org/10.48550/arxiv.1409.0473}.
The alignment function of all three attention types is shown in Table~\ref{tab:attention_alignment_functions}, where 
$W, W_1, W_2$ and $V$ are learnable attention parameters.
In the Bahdanau style, the context vector is concatenated with the token embedding and fed to the RNN decoder, as illustrated by Figure~\ref{fig:attention}.
This is a key difference from the Loung style, where the alignment scores are computed on the output of the RNN prior to the dense layer.




\begin{table}[!htpb]
% https://lilianweng.github.io/posts/2018-06-24-attention/
    \centering
    \begin{tabular}{@{}l c l l @{}} 
         Attention Style & &  & Alignment  \\ \toprule
         Bahdanau       & &  score($\bm{I_i}, \bm{h}_{t-1}$)  & = $V^\top \cdot tanh( W_1 \cdot \textbf{I}_i + W_2 \cdot \textbf{h}_{t-1}) $    \\ 
         Loung Dot      & &  score($\bm{I_i}, \bm{h}_{t}$)    & = $\bm{h}_{t}^\top \cdot \bm{I}_i  $ \\
         Loung Concat   & &  score($\bm{I_i}, \bm{h}_{t}$)    & = $V^\top \cdot tanh( W[\textbf{I}_i;\textbf{h}_{t}]) $   \\ \bottomrule
    \end{tabular}
    \caption{Overview of attention alignment functions.}
    \label{tab:attention_alignment_functions}
\end{table}




% Loung attention follows similar principles but the key difference is here it is implemented on the output of the RNN prior to the dense output layer.
% The alignment function of all three attention types is shown in Table~\ref{tab:attention_alignment_functions}.


