



\subsection{Drafty}
Should I have a separate part explaining image captioning??

Need to start with the RNN.
How this predicts the next word. Maybe also the tokenizer?

The main parts:
- Image captioning, how it works in more detail
- Need to also include tokenizer and word-level embeddings.
- How this translated to training?
- The inner workings of the RNN + how the model is trained?
- The overall architecture (init-inject?)
- The attention mechanism


TODO need some notation for this!

Explain the parallel to image captioning.
E.g. model gets an image as input and tries to generate the sequence

Implemented with LSTM

%  Given a sentence prefix, a neural language model will predict which words are likely to follow. With a small modification, this simple model can be extended into an image caption generator, that is, a language model whose predictions are conditioned on image features
% The RNN is trained to encode the image-language mixture into a single vector in such a way that this vector can be used to predict the next word in the prefix


Should start with the basics around the RNN. Axiom is abtracted into token, tokens are enumerated.
Create a mapping between an enumeration and a vector in the embedding space (embedding layer).
Then how the RNN operates on this in sequence.
(start and end token?)
We get a sequence architecture by feeding the previous hidden state into the RNN along with the token.
In our experiments we have used the LSTM network.

Teacher forcing

%consist of vectors that have been randomly initialised.
% Instead, the embeddings are trained as part of the neural network in order to learn the best representations of words for the task.

Next we can condition on the image by incld


Give an overview of the system
How a tokenizer is utilised.

Picture showing the parallels between image captioning and axiom generation.


DECODING STEPS

Attention mechanism?
Explain how attention works.
How it works on the "image".
Might alter the original embedding too much to be useful.
It is zero in the beginning














%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
GNN stuff


% TODO maybe restructure?

% Although the graph representation is able to capture many aspects of the formulae, they have no direct mapping real-valued vectors.
% TODO: connect with par below
Graphs are a natural representation of problems and generalise better to new and unseen instances than features based on symbol names or problem signature.
However, neural networks traditionally operate on vector-shaped structured data.
Instead, graphs consists of nodes, edges and their attributes, which can be represented in a structured array format by the use of graph neural networks.


% updates the representation of a node by aggregating the embedding of itself and the embeddings of its neighbors












