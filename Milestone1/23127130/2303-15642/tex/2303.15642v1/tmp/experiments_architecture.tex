
% TODO change to attention experiment?
Next, we experiment with different architectures to investigate whether the model encapsulates the sequences.
This includes an LSTM language model with and without attention and a dense model feed-forward neural network.
The models are trained with the frequency ordering.

The results are presented in Table~\ref{table:order_results}.
We observe that the dense model can memorise the embedding-token combination quite well but cannot perform equally well on the validation set.
We can also see that the attention network reduces the performance considerably.
This is entirely unexpected and might be due to the challenges of training larger networks without sufficiently large datasets.



\begin{table}[]
\centering

\input{results/\vocabsize/experiment_architecture}

\caption{The train and validation set performance of three different model architectures.}
\label{table:architecture_results}
\end{table}

