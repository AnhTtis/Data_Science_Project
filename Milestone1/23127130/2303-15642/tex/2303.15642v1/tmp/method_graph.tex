


An embedding is a fixed-size, real-valued vector representation of an entity, where semantically similar entities ideally are close in the embedding space.
%The embedding function maps entries from an initial feature space to the more favoured embedding space.
In the original task of image captioning, image embeddings consist of low-level image features obtained from pre-trained convolutional neural networks over extensive image classification datasets.
%These salient features can be extracted as powerful embeddings for other downstream image-based learning tasks.


Computing problem embeddings in a similar fashion pose multiple challenges in the context of first-order problems.
Firstly, the problems have no natural fixed-size vector representation as they consist of unordered sets of tree-structured formulae.
Thus, encoding syntactic, structural and semantic properties as a vector is non-trivial.
Secondly, there is no immediate classification task for learning the semantics of first-order problems.
%This is in stark contrast to digital images, which have a natural vector representation and where identifying objects in images is a well-researched problem.
This paper attempts to overcome these challenges by producing embeddings via graph neural networks.






% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\subsection{Problem Graph}


A first-order logic formula has an intrinsic tree-shaped structure and is naturally represented as a directed acyclic graph $G$ with vertices $V$ and edges $E$.
The vertices, also known as nodes, correspond to the types of elements occurring in the formula, such as function symbols and constants.
The edges denote a relationship between the vertices, e.g., an argument supplied to a function.
Figure~\ref{fig:graph_conjecture} illustrates the graph representation of a conjecture, spanning four different node types as visually represented by the colouring. 

This representation extends to sets of formulas by computing a global graph over the node elements in the formulae, as shown in Figure \ref{fig:graph_problem}.
The graph representation captures many aspects of the formulae while invariant to symbol renaming and encoding problems with previously unseen symbols.
This paper uses a graph encoding of 17 node types as described in~\cite{DBLP:conf/cade/RawsonR20}.


% I can only find 15 node types in the code??
% argument order in function and predicate application must be preserved in order to
% maintain a lossless representation. This is achieved by use of an auxiliary “argument node”
% for each argument in an application, connected by edges indicating the order of arguments, shown in Figure 3


\begin{figure}[!htpb]
    \centering
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{diagrams/graph_conjecture.png}
        %\caption{Graph representation of the conjecture {\tt  fof(t6\_numbers, conjecture, r2\_xboole\_0(k4\_numbers, k2\_numbers))}}
        \caption{A conjecture graph.}

        \label{fig:graph_conjecture}
    \end{minipage}\hfill
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{diagrams/graph_problem.png} 
        %\caption{Graph representation of problem {\tt t6\_numbers}, which consists of one conjecture and six axioms.}
        \caption{A problem graph of one conjecture and six axioms.}
        \label{fig:graph_problem}
    \end{minipage}
\end{figure}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Graph Neural Networks}


The problem graph is embedded into an $n$-dimensional embedding space via a graph neural network.
A graph neural network is an optimisable transformation that operates on the attributes of a graph.
It utilises a ``graph-in, graph-out" methodology where it embeds the graph while preserving the structure and connectivity of the original graph.

A randomly initialised vector represents each node type $\Phi$ across all graphs in an $n$-dimensional embedding space.
Next, each node in a graph is assigned to its corresponding embedding vector $\bm{x}_\Phi$, resulting in the node feature matrix $X$.
The GNN embeds the type features of each node $\bm{x}_\Phi$ into the node feature embedding $\bm{x}_\Phi'$ through a node update function.
This effectively transforms the graph features $X$ into a more favourable embedding  $X'$.
Adjacent nodes are incorporated into the update of a node to encode the structure through message passing~\cite{DBLP:journals/corr/GilmerSRVD17}.


Message passing is accomplished through graph convolutional layers, and we utilise the operation described in~\cite{https://doi.org/10.48550/arxiv.1609.02907}.
The node-wise convolutional operation for the attributes $\bm{x}_i^{(k)}$ of node $i$ at step $k$ is described as:

% https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv
\[ 
\bm{x}_i^{(k)} = W
\sum_{j \in \mathcal{N}(i) \bigcup \{i\}}
\frac{e_{j,i}}
{\sqrt{ \hat{d_j} \hat{d_i} } }
\bm{x}_j^{k-1}
%\cdot (\bm{W}^\top  \bm{x}_j^{k-1})
%+ \bm{b}
\]	

% https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv
where $W$ is a learnable weight matrix, $\mathcal{N}(i)$ is the set of neighbouring nodes of $i$ and $\hat{d_i} = 1 + \sum_{j \in \mathcal{N}(i)}e_{j,i}$. 
The variable $e_{j,i}$ denotes the edge weight from $j$ to $i$.
 In this setting, all edge weights are 1. % and the layers scale linearly in the number of graph edges.
The convolutional operations are applied synchronously to all nodes in the graph and learn hidden layer representations that encode both local graph structure and nodes features.
% As a result, we obtain more semantically meaningful node features while retaining the graph structure.


%The convolutional operation for the attributes $x_i^{(k)}$ of node $i$ at step $k$ is described as;
% https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html - just general message passing
% update with the formula we are using.
%\[ 
%\bm{x}_i^{(k)} = 
%\sum_{j \in \mathcal{N}(i) \bigcup \{i\}}
%\frac{1}{\sqrt{deg(i)} \cdot \sqrt{deg(j)} }
%\cdot (\bm{W}^\top  \bm{x}_j^{k-1})
%+ \bm{b}
%\]	




%The operation extracts the node $i$ and all its neighbours and performs a linear transformation on their attributes.
%Over a node $i$ and its adjacent node $j$, the transformation multiples the node attributes $\bm{x}_j^{k-1}$ with the weights $\bm{W}$ and scales the result according to the degree of the node pair.
%Further, the scaled attributes are aggregated with a sum function.
%Lastly, it applies the bias vector $\bm{b}$ before updating the attributes of node $i$.


%and is achieved through graph convolutional operators on the adjacency matrix $A$.


% https://docs.google.com/drawings/d/1O4xTRboNRjzMkgcBnc_9eddkvlg5l9FaApT0eO1DnlA/edit
%\begin{figure}[!htpb]
\begin{figure}
\centering
\includegraphics[scale=0.38]{diagrams/gnn_pipeline.png} 
\caption{Graph Neural Network for classification of graph or node properties.}
\label{fig:graph_workflow}
\end{figure}


After computing the graph embeddings, they are pooled and passed through the prediction layer, which produces the final model output.
We experiment with three different mean pooling approaches all nodes in the graph, only axiom nodes, and only the conjecture node.
An overview of the GNN pipeline is shown in Figure~\ref{fig:graph_workflow}.

In this approach, the GNN is pre-trained on auxiliary tasks, computing the embeddings before training the captioning model.
We experiment with supervised and unsupervised pre-training GNN approaches, as described below.




\subsection{Supervised Problem Embedding} 


In the supervised approach, the GNN is trained on the node level by performing binary premise selection over the axiom nodes, as described in~\cite{DBLP:conf/cade/RawsonR20}.
Based on their node embedding, the model learns to predict whether an axiom occurs in the proof of a problem.
%The model operates on a directional graph but performs message passing with bi-directional convolutional layers.
%This means that the node embeddings are a concatenation of one convolutional layer operating on the original adjacency matrix $A$ and another convolutional layer operating on the transposed adjacency matrix $A^T$.
%As a result, message passing occurs in both edge directions. 
During training, the resulting axiom node embeddings become increasingly valuable for modelling their contribution towards the proof.
Therefore, the node embeddings are expected to contain information crucial to premise selection.
Our experiments show that this information prevails through average pooling.

% However, whether this information will prevail when the nodes are pooled and injected into the captioning model is unclear.\KK{elaborate, do results form this paper clarify }



% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %



\subsection{Unsupervised Problem Embedding}

The supervised learning task emphasises the axiom nodes, but it might be advantageous with a learning task encapsulating all the nodes in a graph.
Alas, no sensible labels are directly derivable from the problems to train a prediction model.
Thus, we employ unsupervised training through a synthetic dataset which utilises a relation property encapsulating all graph nodes.


The unsupervised training approach consists of training a matching model which learns the difference between two graphs according to some relational property, as described in~\cite{unsupervised_graph_classification}.
The model takes two graphs, $g_i$, $g_j$, as input and passes them through the Siamese GNN model, as illustrated in Figure \ref{fig:embedding_unsupervised}.
Next, the nodes of the embedded graphs are pooled into two graph embedding vectors.
The similarity of the two input graphs is approximated as the vector norm between the two graph embeddings:  $ || GNN(g_i) - GNN(g_j) || $. 
Training the GNN in this fashion enables it to produce embeddings encompassing structural similarities and dissimilarities.


% https://docs.google.com/drawings/d/1SN44rps-Ainpq6kzthcVkL0Xje2B75rge_1bWOk761k/edit
% https://docs.google.com/drawings/d/1DRFLgQJ-wf6uWvnD3OTRqU7tfMdhti7OQxk3KS7uKGk/edit
\begin{figure}[!htpb]
\centering
% \includegraphics[scale=0.45]{diagrams/GNN_unsupervised.png} 
\includegraphics[scale=0.37]{diagrams/gnn_unsupervised_detailed.png} 
\caption{Unsupervised GNN training based on pairwise graph similarity.}
\label{fig:embedding_unsupervised}
\end{figure}



The synthetic dataset consists of pairs of undirected graphs and a numeric property describing their relation.
The relational property utilised is the Laplacian spectrum distance~\cite{Wills_2020}, which can be defined as follows.
Given a  graph $G$, the adjacency matrix $A$ represents the node connections in the graph.
The diagonal degree matrix $D$ of $G$ represents the degree of each node, e.g. the number of neighbours.
Further, the Laplacian of the graph is defined as the degree matrix subtracted from the adjacency matrix:


\[
L = D - A
\]

\noindent
The eigenvalues $\lambda_1 \leq \ldots \lambda_i \ldots \leq \lambda_k$ of the Laplacian are given as $L \bm{x} = \lambda_i \bm{x}$.
Accordingly, the Laplacian spectrum distance $\pi$ of two graphs $G$ and $G'$, is defined as:

%\[
%\pi (G, G') = 
%\underset{k \in min(n, m)}{}
%\lVert \lambda_0 - \lambda_0' , \ldots \lambda_k - \lambda_k' \rVert
%\]

% Frobenius norm - np.linalg.norm
\[
\pi (G, G') = 
\sqrt{
\sum_{i=1}^{k} (\lambda_i - \lambda_i')^2
},
\]
%
where $k \in min(n, m)$, and $n$ and $m$ are the numbers of nodes in $G$ and $G'$.
 
The Laplacian spectrum distance is a computationally cheap metric, even for graphs of the magnitude required to represent first-order problems.
Although the metric encapsulates graph structure, it neither considers node types nor edge directions. % which could lead to the loss of valuable problem semantics.
Still, the distance provides an overall description of the structural similarity of the graphs and considers all graph nodes. 
%\KK{there are distances for weighted graphs which probably can represent labels}






