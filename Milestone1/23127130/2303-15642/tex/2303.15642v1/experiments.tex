


This section describes the experimental results and evaluation of our premise selection approach\footnote{Experiments available at \url{https://github.com/EdvardHolden/axiom\_caption}}.
First, we describe the dataset, followed by five experiments.
The first experiment investigates the performance of the graph embeddings\footnote{Embedding computation available at \url{https://github.com/EdvardHolden/gnn-entailment-caption}}.
The second experiment explores input orders, and the third investigates the effect of attention.
The fourth experiment explores decoder sampling methods, and the fifth is an online evaluation of our approach and related methods.



%  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %

\subsection{Proof Dataset}


We used the synthetic DeepMath~\cite{DBLP:journals/corr/AlemiCISU16} dataset for the model training and evaluation.
It consists of 32524 FOF problems based on proofs of the Mizar40 benchmark.
DeepMath was created to evaluate binary premise selection methods.
Hence, the number of positive and ``superfluous'' axioms in a problem is balanced.
The main advantages of the dataset are a large number of problems combined with consistent formula naming and the reuse of axioms across problems.


We impose a maximum sequence limit of 20 axioms resulting in 30805 problems, where 20\% are used for testing, 8\% for validation, and the rest for training.
The vocabulary consists of the 6K most common axioms occurring in the proofs of the training set.
The other axioms are mapped to the Out-Of-Vocabulary (OOV) and removed.
Table~\ref{table:dataset} displays the key statistics of each problem set.
While many proofs contain rarely occurring axioms, few problems are represented solely by OOV tokens. 


% % % % % % % % % % 


% TODO show the plot of axiom distributions(?)

\begin{table}
\centering

\input{results/\vocabsize/problem_statistics}

\caption{Key statistics of each dataset partition.}
\label{table:dataset}
\end{table}



%  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %

\subsection{Experiment 1: Supervised vs Unsupervised Graph Embeddings}

The goal of this experiment is to examine the pre-trained embedding methods.
The supervised approach was trained according to the methods in the original paper~\cite{DBLP:conf/cade/RawsonR20}.
The unsupervised approach was trained on 12000 graph pairs.
The output of each GNN is combined with one of the pooling mechanisms to produce a total of six embedding variations.
The captioning training parameters are shown in Table~\ref{tab:training_setup}.
The evaluation metrics are defined as follows, where $A$ is the set of predicted tokens, and $B$ is the ground truth:


\begin{align*}
Jaccard(A,B) = \frac{|A \cap B |}{| A \cup B |}  && Coverage(A, B) = \frac{|A \cap B |}{|B|} 
\end{align*}



\begin{table}
\centering
\begin{tabular}{@{}lllll@{}}
\multicolumn{2}{c}{\bf Training Prameters} &  & \multicolumn{2}{c}{\bf Model Parameters}     \\ \toprule
Optimiser               & Adam  &  & RNN Type              & LSTM  \\
Learning Rate           & 0.001 &  & RNN Units             & 32    \\
Max Epochs              & 80    &  & Stateful RNN          & True  \\
Early Stopping          & 5     &  & Batch Normalisation   & False \\ \midrule
Dropout Rate            & 0.1   &  & Feature Normalisation & True  \\
Teacher Forcing Rate    & 1.0   &  & No Dense Units        & 512   \\
Batch Size              & 64    &  & Embedding Size        & 50    \\
Maximum Sequence Length & 22    &  & Target Vocab Size     & 6000  \\ \bottomrule
\end{tabular}
\caption{The captioning model and training parameters.}\label{tab:training_setup}
\end{table}




%We trained an instance of the par-inject model with 32 LSTM units and a dense layer of 512 units.
%The models are trained with the Adam optimiser with a learning rate of 0.001 and a dropout of 0.1 for a maximum of 100 epochs with early stopping if there is no improvement in the loss of the validation set after five epochs.
%These parameters where determined experimentally.



The results are presented in Table \ref{table:graph_embedding_results} and show that supervised embeddings perform better than unsupervised embeddings.
This indicates that a contextual learning task is required to produce good embeddings.
It also shows that the embeddings created by pooling problem and premise nodes perform better, indicating that essential structures persist through averaging.
The results show issues with overfitting, but an increase in the dropout rate led to a decrease in validation performance.
On the other hand, unsupervised learning is relatively less prone to overfitting.
The following experiments utilise the supervised problem embeddings.


% In the results:
%if the unsupervised performs well, we can use it for more general approaches.
% If the supervised performs well, it already has the node information. Shows that this is carred over.
%hat makes this interesting is that the supervised approach lets the model know about which axioms are useful for the given conjecture. 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55




\begin{table}[!htpb]
\centering

\input{results/\vocabsize/experiment_features}


\caption{The training and validation performance of the embedding approaches.}
\label{table:graph_embedding_results}
\end{table}





%  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %
\subsection{Experiment 2: Axiom Order}\label{exp:axiom_order}

This experiment examines various input ordering schemes.
While the ATP treats the axioms as a set, RNNs operate over input sequences.
Therefore, some input orders may be more advantageous.
We explore the following ordering schemes:

\begin{itemize}
    \item \textbf{Original}: Ordered as in the original problem.
    \item \textbf{Length}: Ordered the smallest string representation to the longest.
    %\item \textbf{Lexicographical}: Lexicographical ordering of the axiom strings. \KK{seems strange}
    \item \textbf{Frequency}: Ordered from the most frequently occurring axioms to the least. %, as appearing in the training set.
    \item \textbf{Random}: Random order of axioms for each sequence.
    \item \textbf{Global Random}: Static random order.

\end{itemize}


The results are displayed in Table~\ref{table:order_results}.
Although most configurations perform similarly, the length and frequency order have the best validation performance.
However, random ordering has a surprisingly low performance.
As this is not reflected in the other ordering schemes, including the global random order, it indicates that a consistent relative position across sequences is essential.


\begin{table}
\centering

\input{results/\vocabsize/experiment_order}


\caption{The train and validation set performance on the different axiom orders.}
\label{table:order_results}
\end{table}



\subsection{Experiment 3: Attention Mechanisms}\label{exp:attention}

This experiment evaluates the impact of utilising an attention mechanism for the axiom captioning method,
The models were trained with the length order, and the results are presented in Table~\ref{table:attention_results}.
The results show that Loung dot attention can provide a minor performance improvement.
%This suggests that the attention is best applied to the hidden state containing sequence information instead weighing the raw problem embedding.



\begin{table}
\centering
\begin{tabular}{lllllll}
\multirow{2}{*}{Attention} &  & \multicolumn{2}{c}{Train}     &  & \multicolumn{2}{c}{Validation} \\ \cline{3-4} \cline{6-7} 
                           &  & Jaccard       & Coverage      &  & Jaccard        & Coverage      \\ \cline{1-1} \cline{3-4} \cline{6-7} 
% None                       &  & 0.37          & 0.47          &  & 0.23           & 0.32          \\ % - inconsistent from run
None                       &  & \textbf{0.43} & \textbf{0.53} &  & \textbf{0.24}  & 0.33 \\ % From exp above
Bahdanau                   &  & 0.37          & 0.50          &  & 0.22           & 0.32          \\
Loung concat               &  & 0.38          & 0.49          &  & \textbf{0.24}  & 0.33          \\
Loung dot                  &  & 0.39          & 0.51          &  & \textbf{0.24}  & \textbf{0.34} \\ \hline
\end{tabular}

\caption{The performance of attention mechanisms and no attention.}
\label{table:attention_results}
\end{table}



% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 




%  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %
\subsection{Experiment 4: Model Decoding}

This experiment shifts the focus from optimising the training parameters toward the premise selection task by examining sampling methods.
Although the model is given a single input token at each step, multiple tokens can be sampled from the output distribution.
We explore three different sampling methods over the test set for selecting which axioms to include in the final problem:
\begin{itemize}
    \item \textbf{Greedy}: Select $n$ axioms with the maximum probability.
    \item \textbf{Top-K}: Redistribute the probability distribution over the top $K$ axioms, and sample $n$ axioms from the new probability distribution.
    \item \textbf{Temperature}: Scale the logits by the temperature prior to applying softmax and sample $n$ axioms from the new probability distribution.
\end{itemize}

At each generative step, the sampling method selects $n$ axioms and adds them to the set of the selected axiom.
Only the top axiom is fed back into the model.
The results are displayed in Table \ref{table:decoding_results}.
They show that the greedy sampling method is superior, and the coverage score is monotonic on $n$. We found that an increase in coverage gives a substantial improvement for online performance compared to a slight decrease in Jaccard. Moreover, greedy sampling selects a small total number of axioms despite producing the highest coverage scores.
Through experimentation, we discovered that achieving a high coverage score is crucial for solving the problems in this dataset.




\begin{table}
\centering

\input{results/\vocabsize/experiment_decoding}


\caption{The performance of each sampling method.}
\label{table:decoding_results}
\end{table}



%  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %  %
\subsection{Experiment 5: Online System Evaluation}

The final experiment performs an online evaluation of our premise selection method and other methods with the state-of-the-art ATP, iProver\footnote{iProver is available at:\url{https://gitlab.com/korovin/iprover}}.
During initial experimentation, it was discovered that the Deepmath problems were too small for a meaningful assessment as iProver solved 86\% of the problems without premise selection. % 27915 / 32524
Therefore, the problems were extended by merging the Deepmath problems with the larger Mizar40\footnote{Problems are available at \url{http://grid01.ciirc.cvut.cz/~mptp/7.13.01_4.181.1147/mptp/problems_small_consist.tar.gz}} problems, reducing this ratio to 52\%. %  16988 / 32524


% OLD \url{http://grid01.ciirc.cvut.cz/\~{}mptp/7.13.01\_4.181.1147/mptp/problems\_small\_consist.tar.gz}

Nine different premise selection configurations are utilised to generate problem versions used in the online evaluation.
This consists of axiom captioning, SInE, no premise selection, and three related axiom captioning methods.
SInE is evaluated with tolerance-depth parameters of $(1, 1)$ and $(3, 0)$, note that $0$ corresponds to unbounded depth.


In the case of axiom captioning, the model is trained on the 6K most common positive axioms in the benchmark.
The positive axioms outside the vocabulary are essential for the proofs but appear too rarely to be learnt by the model.
Thus, these axioms are not reachable for the captioning model.
Consequently, our axiom captioning method implements a ``rare-axiom'' procedure for selecting rare but valuable axioms.
The procedure scans a given problem and selects any axioms that have previously appeared positively but rarely.
Furthermore, it is unlikely that a single method can encapsulate all aspects of the conjecture-axiom relationship.
Hence, we also evaluate the combination of axiom captioning and SInE as two complementing approaches.


Further, we evaluate three related ML-based methods for premise selection: Binary Graph ~\cite{DBLP:conf/cade/RawsonR20}, Conjecture RNN~\cite{DBLP:conf/lpar/PiotrowskiU20} and the Conjecture Transformer~\cite{DBLP:conf/mkm/ProrokovicWS21}.
The Binary Graph method is used to compute the supervised embeddings, and the comparison will therefore reveal whether axiom captioning improves the utilisation of the graph embeddings.
Still, the method expects a balanced dataset and will likely introduce much positive bias for larger problems.
Conjecture RNN is a sequence-to-sequence approach where the input is a tokenised embedding of the conjecture.
A caveat with this approach is that it trains on a specific axiom order derived from the proofs unavailable for this dataset.
Hence, it is trained on the consistent ``length" order (see, Section~\ref{exp:axiom_order}).
The approach utilises an LSTM architecture but puts no restriction on the input and output vocabulary, resulting in poor training performance on this dataset.
The Conjecture Transformer approach is an extension where the RNNs are replaced with the Transformer~\cite{https://doi.org/10.48550/arxiv.1706.03762} architecture.
This approach was modified slightly in the input and output layers to enforce input length restrictions and OOV tokens.
The Conjecture* approaches rely solely on the token vocabulary and do not inspect the given problem.
Hence, their encoders are unaware of the increased problem size.


iProver is run with a default heuristic over the clausified problem versions generated by each configuration with a time limit of 10 seconds.
The results are displayed in Figure~\ref{fig:cactus}.
Conjecture RNN only solves one problem selecting only the most frequently occurring axiom.
The ``Conjecture Transformer'' slightly improves the performance but does not come close to axiom captioning.
This suggests that graph embeddings are superior to conjecture token embeddings for premise selection.
Binary Graph performs best of the related methods and is close to the performance of SInE $(1,1)$.
However, it suffers from generating large problems with many superfluous axioms and a low coverage score due to being a binary classifier trained on a balanced dataset.
Nevertheless, half of the problems solved by Binary Graph complement the solution of axiom captioning.
Hence, graph embeddings facilitate method diversification.


Axiom caption outperforms the related ML-based methods and is slightly enhanced by its combination with SInE $(1, 1)$.
However, it is behind SInE $(3, 0)$, which performs nearly identically to no premise selection (Original).
The real power of axiom captioning comes through when paired with SInE $(3, 0)$.
Axiom captioning considers axioms carefully and consequently predicts many of the essential axioms.
On the other hand, SInE $(3,0)$ selects many axioms, resulting in reasonable coverage scores.
Hence, their combination results in axiom selection with sufficiently large coverage scores while maintaining a computationally feasible size.
The result is a method which solves 17.7\% more problems than the baseline, which does not employ premise selection. 


\begin{figure}

% PREVIOUS
% python3 mkplot.py --timeout 10 --db_data_ltb --data_type db '{"115507": "Original", "115508": "SInE 1 1", "115511": "Caption", "115509": "SInE 3 0", "115510": "Caption + SInE 3 0", "115512": "Caption + SInE 1 1"}'
 % Update captioning experiment
% python3 mkplot.py --timeout 10 --db_data_ltb --data_type db '{"115507": "Original", "115508": "SInE 1 1", "117251": "Caption", "115509": "SInE 3 0", "115510": "Caption + SInE 3 0", "115512": "Caption + SInE 1 1"}'
 % Add related works 
%  python3 mkplot.py --timeout 10 --db_data_ltb --data_type db '{"115507": "Original", "115508": "SInE 1 1", "117251": "Caption", "115509": "SInE 3 0", "115510": "Caption + SInE 3 0", "115512": "Caption + SInE 1 1", "117026": "Binary Graph", "117028": "Conjecture Transformer", "117029": "Conjecture RNN"}'
 
%  python3 mkplot.py --timeout 10 --db_data_ltb --data_type db '{"115507": "Original iProver", "115508": "SInE 1 1", "117251": "Caption", "115509": "SInE 3 0", "115510": "Caption + SInE 3 0", "115512": "Caption + SInE 1 1", "117026": "Binary Graph", "117028": "Conjecture Transformer", "117029": "Conjecture RNN"}'

\centering
%\includegraphics[width=0.8\textwidth]{results/\vocabsize/cactus.png} 
%\includegraphics[width=0.8\textwidth]{diagrams/cade_plot.png} 
\includegraphics[width=0.8\textwidth]{diagrams/cade_cactus_original_iprover.png} 
\caption{Online evaluation of SInE, Captioning, their combination, and related methods.}
\label{fig:cactus}
\end{figure}


