

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


Automated Theorem Provers (ATPs) construct formal proofs without human interaction and have seen great success in hardware and software verification, as it automatically verifies system properties. 
They have also played a significant role in formalisation projects such as Mizar~\cite{DBLP:journals/jar/Urban06} and as hammers in interactive theorem proving~\cite{DBLP:conf/lpar/PaulsonB10,DBLP:conf/cade/SteenB18}.
%by proving lemmas as a part of the interactive theorem prover Isabelle \cite{DBLP:conf/lpar/PaulsonB10}.


State-of-the-art ATPs such as iProver~\cite{DBLP:conf/cade/Korovin08,DBLP:conf/cade/DuarteK20}, E~\cite{DBLP:conf/lpar/Schulz13}, Vampire~\cite{DBLP:conf/cav/KovacsV13} and SPASS~\cite{DBLP:conf/cade/WeidenbachDFKSW09} attempts to solve problems consisting of a conjecture and a set of axioms through saturation.
This process consists of clausifying the input problem and computing all possible inferences between the clauses until it derives a contradiction or the set of clauses is saturated.
However, computing inferences quickly lead to a combinatorial explosion in the number of clauses which exhausts the computational resources.
Therefore, the proof search is guided by heuristics.
This is essential because the chance of deriving a contradiction reduces considerably as the number of clauses grows.
Machine learning is currently being used to discover efficient heuristics~\cite{DBLP:conf/gcai/SchaferS15, DBLP:journals/corr/JakubuvU16a, DBLP:conf/mkm/HoldenK21}, and to intelligently operate internal heuristic components~\cite{DBLP:conf/cade/Bartek021, DBLP:conf/frocos/GoertzelCJOU21, DBLP:journals/jar/FarberKU21, DBLP:conf/cade/RawsonR18}.
 


However, strong heuristics are insufficient when reasoning over large theories.
Problems concerning verification and mathematical statements often contain a vast amount of axioms,  quickly resulting in resource starvation due to the sheer size of the initial space.
A key observation is that, despite the large set of axioms, only a fraction is typically required to construct the proof.
Consequently, by removing all ``superfluous'' axioms, the problems become computationally feasible, and the chance of finding proof increases dramatically.
This task is known as \textit{premise selection}.
%Estimating the relevance of an axiom according to a given conjecture is challenging as it depends on both the inter-axiom and the axiom-conjecture relationships.
%Furthermore, there is no fixed threshold for how many superfluous axioms are selected before it starts jeopardising the proof search.
%This leads to a natural trade-off; selecting fewer clauses improves the conditions for effective reasoning while decreasing the chance of selecting all clauses necessary to produce a proof.




This paper explores the adaptation of image captioning to the premise selection task.
Image captioning models aim to produce a sentence in natural language that describes a given image.
The captions are generated by embedding images using a pre-trained image model and combining the embedding with a language model.
%This task draws parallels to premise selection, where we want to predict the set of axioms required for producing the proof of a given conjecture.
Such methods are novel for premise selection and hold multiple compelling properties.
First, %and foremost, 
the axioms are represented by tokens, and their embeddings are learnt during training.
With an abstract token representation of the axioms, we can leverage both the conjecture-axiom and the inter-axiom relationship.
This %challenges 
 is in contrast to
approaches that accentuate the structural and symbol similarity of conjecture-axiom pairs.
A vital property of the language model is encapsulating sequences of axioms as opposed to treating them separately.
By representing the axioms occurring in a proof as a sorted set, the model can learn the conditional relationship between the axioms occurring in a proof.
This is crucial for treating the axioms as a collective set.
  %Nonetheless, sentences in natural language adhere to grammatical rules and naturally form a sequence, while axioms act as a set with no inherent order.



Another challenging aspect of a captioning approach is computing problem embedding entailing problem semantics.
First-order problems consist of a set of tree-structured formulas which are not easily represented through a feature vector, as required for machine learning.
This paper investigates pre-trained graph neural networks (GNNs) to embed problems via transfer learning.
GNNs operate on graphs and can incorporate structural properties into the embedding.
Nevertheless, there is no apparent pre-training task for FOF problems as there is for, e.g. images.
Therefore, we investigate using a supervised pre-training step where the GNN learns the embedding by training on the premise selection problem in the binary setting.
Additionally, we experiment using an unsupervised approach that learns to embed the problems based on graph similarity.
The graph embeddings are further enhanced by emphasising different sections of the embedding at given steps of axiom generation by exploring attention mechanisms.


Due to the challenges of premise selection, a single approach is unlikely to encapsulate all productive aspects of the axiom-conjecture relationship.
Hence, we also explore the combination of our method and SInE~\cite{DBLP:conf/cade/HoderV11}.



Our main contributions are:
\begin{itemize}

    \item Adapt approaches from image captioning to the task of premise selection.

    \item Novel method for combining graphs with sequence learning in the context of premise selection, outperforming previous tokenised conjecture approaches.
    
    \item Novel method for unsupervised training of GNNs embeddings of FOF problems.
    
    \item Usage of GNNs on problem graphs for transfer learning.
    %Novel use of GNNs on problem graphs for transfer learning in combination with sequence learning.
    
    \item 'Rare Axiom' inclusion technique for training with a reduced vocabulary while maintaining rare positive axioms.

    %\item Evaluate the effect of different attention mechanisms for premise selection.
    %\item Discover the performance trade-off between different sampling methods.
    %\item Combine SInE with token-based premise selection to further increase the number of solved problems.
    %\item Investigate word-level sequence representations of axioms in relation to a problem graph representation.


    %\item Evaluate the performance trade-off between different sampling methods.
    %\item Evaluate \sine and combinations of \sine and the captioning method on the DeepMath dataset.
    %\item Evaluate the combination of premise selection methods.
\end{itemize}


We evaluated our approach over an extended version of the DeepMath dataset.
The results show that the specificity of our approach, in combination with the breadth of \sine, significantly outperforms related methods and results in a 17.7\% increase in the number of solved problems over the baseline.


This paper is structured as follows: in Section~\ref{sec:related_works} we present the related works.
In Section~\ref{sec:captioning} we present the sequence model and in Section~\ref{sec:problem_embedding} we describe our approach for obtaining problem embeddings.
In Section~\ref{sec:evaluation}, we evaluate our approach experimentally both offline and online, before concluding the paper in Section~\ref{sec:conclusion}.
