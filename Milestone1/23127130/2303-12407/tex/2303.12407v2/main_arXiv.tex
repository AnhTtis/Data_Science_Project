\documentclass[12pt,reqno]{amsart}
\usepackage[margin=1in]{geometry}

% order of citations
% https://tex.stackexchange.com/questions/190842/same-author-same-year-bibliography-order
\newcommand{\DateOfPub}[1]{}

% maketitle
\usepackage[UKenglish]{babel}

% math fonts and symbols
\usepackage{amsfonts,amsmath,amsthm,amssymb} % \text and \mathbb command
%\usepackage{amsaddr}
%\usepackage[foot]{amsaddr}
\usepackage{mathrsfs} % script type font
\usepackage{stmaryrd} % new bracket
\usepackage{mathtools}

\usepackage{lscape} %to make the page landscape

\usepackage{natbib}
\usepackage{url}

\usepackage{tikz-cd}
\usepackage{ascmac}
\usepackage{soul}

% others
\usepackage{comment} % commenting out multiple lines
\usepackage{graphicx} % graphics
% \usepackage{setspace}
\usepackage{color}

\usepackage{multirow}

\allowdisplaybreaks[1]
%\setlength\parindent{15pt}

% removing dots at the ends of items in the bibliography
% https://tex.stackexchange.com/questions/152892/how-to-delete-a-full-stop-on-reference-ending
\newcommand\EatDot[1]{}

% \usepackage[justification=centering]{caption}


\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\newcommand{\C}{\mathbf{C}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\loc}{\mathrm{loc}}
\newcommand{\Hilbert}{\mathbf{H}}
\newcommand{\Epsilon}{\mathcal{E}}
\newcommand{\Wasserstein}{\mathcal{W}}
\newcommand{\Beta}{\mathrm{B}}
\newcommand{\FMC}{\mathbb{M}}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\indicator}{\mathbb{I}}
\newcommand{\Borel}{\mathcal{B}}
\newcommand{\error}{\mathbf{e}}
\newcommand{\bias}{\mathbf{b}}
\newcommand{\variance}{\mathbf{v}}
\newcommand{\data}{\mathbf{z}}
\newcommand{\dataSpace}{\mathsf{Z}^{n}}
\DeclareMathOperator{\argmin}{argmin}


\title[Langevin-type Monte Carlo]{Non-asymptotic analysis of Langevin-type Monte Carlo algorithms}
\author{Shogo Nakakita}
\address{Komaba Institute for Science, the University of Tokyo, 3-8-1 Komaba, Meguro, Tokyo 153-8902, Japan}

\begin{document}
\maketitle

\begin{abstract}
    We study the Langevin-type algorithms for Gibbs distributions such that the potentials are dissipative and their weak gradients have the finite moduli of continuity.
    Our main result is a non-asymptotic upper bound of the 2-Wasserstein distance between the Gibbs distribution and the law of general Langevin-type algorithms based on the Liptser--Shiryaev theory and functional inequalities.
    We apply this bound to show that the dissipativity of the potential and the $\alpha$-H\"{o}lder continuity of the gradient with $\alpha>1/3$ are sufficient for the convergence of the Langevin Monte Carlo algorithm with appropriate control of the parameters.
    We also propose Langevin-type algorithms with spherical smoothing for potentials without convexity or continuous differentiability.
\end{abstract}

\section{Introduction}

Consider the sampling from the tempered Gibbs distribution $\pi(\diff x)\propto\exp(-\beta U(x))\diff x$ on $(\R^{d},\Borel(\R^{d}))$, 
where $U:\R^{d}\to[0,\infty)$ is a non-negative potential function and $\beta>0$ is the inverse temperature.
One of the extensively used types of algorithms for the sampling is the Langevin type motivated by the Langevin dynamics, the solution of the following $d$-dimensional stochastic differential equation (SDE):
\begin{align}\label{eq:LD}
    \diff X_{t}=-\nabla U\left(X_{t}\right)\diff t+\sqrt{2\beta^{-1}}\diff B_{t},\ X_{0}=\xi,
\end{align}
where $\{B_{t}\}_{t\ge0}$ is a $d$-dimensional Brownian motion and $\xi$ is a $d$-dimensional random vector.
Since the 2-Wasserstein or total variation distance between $\pi$ and the law of $X_{t}$ is convergent under mild conditions, we expect that the Langevin-type algorithms inspired by $X_{t}$ should converge to $\pi$.
However, most of the theoretical guarantees for the algorithms are based on the convexity of $U$, twice continuous differentiability, or Lipschitz continuity of the gradient $\nabla U(x)$, whilst some well-known models in statistics and machine learning do not satisfy those assumptions.
The main interest of this study is a unified approach for the analysis and proposal of Langevin-type algorithms under minimal assumptions.

The stochastic gradient Langevin Monte Carlo (SG-LMC) algorithm or stochastic gradient Langevin dynamics with a constant step size $\eta>0$ is the discrete observation $\{Y_{i\eta}\}_{i=0,\ldots,k}$ of the solution of the following $d$-dimensional SDE:
\begin{align}\label{eq:SGLMC}
    \diff Y_{t}=-{G}\left(Y_{\lfloor t/\eta\rfloor\eta},a_{\lfloor t/\eta\rfloor\eta}\right)\diff t+\sqrt{2\beta^{-1}}\diff B_{t},\ Y_{0}=\xi,
\end{align}
where $\{a_{i\eta}\}_{i=0,\ldots,k}$ is a sequence of independent and identically distributed (i.i.d.) random variables on a measurable space $(A,\mathcal{A})$ and ${G}$ is a $\R^{d}$-valued measurable function.
We assume that $\{a_{i\eta}\}$, $\{B_{t}\}$, and $\xi$ are independent.
Note that the Langevin Monte Carlo (LMC) algorithm is a special case of SG-LMC; it has the representation as the discrete observation $\{Y_{i\eta}\}_{i=0,\ldots,k}$ of the solution of the following diffusion-type SDE:
\begin{align}\label{eq:LMC}
    \diff Y_{t}=-\nabla U\left(Y_{\lfloor t/\eta\rfloor\eta}\right)\diff t+\sqrt{2\beta^{-1}}\diff B_{t},\ Y_{0}=\xi,
\end{align}
which corresponds to the case ${G}=\nabla U$.

To see what difficulties we need to deal with, we review a typical analysis \citep{raginsky2017non} based on the Lipschitz continuity of $\nabla U$. % \citep{Dal17,durmus2017nonasymptotic,raginsky2017non}.
This property has the following two virtues.
The first one is that the Lipschitz continuity combined with the dissipativity yields a bound for the constant of a logarithmic Sobolev inequality.
If $\nabla U$ is Lipschitz and $U$ is of class $\mathcal{C}^{2}$, the Lipschitz constant $K>0$ gives the bound for the Hessian $\nabla ^{2}U\succeq -KI_{d}$, where $\succeq$ is the Loewner order of matrices.
This bound leads to a concise bound for the constant of a logarithmic Sobolev inequality by the result of \citet{cattiaux2010note}.
In the second place, the Lipschitz continuity combined with weak conditions ensures the representation of the likelihood ratio between $\{X_{t}\}$ and $\{Y_{t}\}$, which is critical when we bound the Kullback--Leibler divergence.
\citet{liptser2001statistics} exhibit much weaker conditions than Novikov's or Kazamaki's condition for the explicit representation if \eqref{eq:LD} has the unique strong solution.
Since the Lipschitz continuity of $\nabla U$ is sufficient for the existence and the uniqueness of the strong solution of \eqref{eq:LD}, the framework of \citet{liptser2001statistics} is applicable.

The main idea to overcome the difficulties of the lack of the Lipschitz continuity of the gradients is mollification, a classical approach to dealing with non-smoothness in differential equations.
We consider the convolution $\nabla \Bar{U}_{r}:=\nabla (U\ast \rho_{r})$ on $\nabla U$, a weak gradient of $U$, and some sufficiently smooth non-negative function $\rho_{r}$ with compact support in a ball of centre $\zero$ and radius $r\in(0,1]$.
Since the convolution $\nabla \Bar{U}_{r}$ is Lipschitz continuous if the modulus of continuity of $\nabla U$ is finite, we obtain a bound for the constant of a logarithmic Sobolev inequality under the Gibbs distribution $\Bar{\pi}^{r}(\diff x)\propto \exp(-\beta (U\ast \rho_{r})(x))\diff x$ by assuming the dissipativity of $U$.
In addition, a concise representation of the likelihood ratios between the mollified dynamics $\{\Bar{X}_{t}\}$ defined by the SDE
\begin{align*}
    \diff \Bar{X}_{t}^{r}=-\nabla \Bar{U}_{r}\left(\Bar{X}_{t}^{r}\right)\diff t +\sqrt{2\beta^{-1}}\diff B_{t},\ \Bar{X}_{0}^{r}=\xi
\end{align*}
and $\{Y_{t}\}$ is available, and we can evaluate the Kullback--Leibler divergence under weak assumptions.

As our analysis relies on mollification, the bias--variance decomposition of $G$ with respect to $\nabla \Bar{U}_{r}$ rather than $\nabla U$ is crucial.
This decomposition gives us a unified approach to analyse well-known Langevin-type algorithms and propose new algorithms for $U$ without continuous differentiability.
Concretely speaking, we show that the sampling error of LMC under the dissipativity of $U$ of class $\mathcal{C}^{1}$ and $\alpha$-H\"{o}lder continuous $\nabla U$ with $\alpha>1/3$ can be arbitrarily small by controlling $k$, $\eta$, and $r$ carefully and letting the bias converge.
We also propose a new algorithm named spherically smoothed stochastic gradient Langevin Monte Carlo (SS-SG-LMC) algorithm, whose error can be arbitrarily small under the dissipativity of $U$ and the boundedness of the modulus of continuity of a weak gradient $\nabla U$.


\subsection{Related works}


The non-asymptotic analysis of Langevin-based algorithms under convex potentials has been one of the subjects of much attention and intense research \citep{Dal17,durmus2017nonasymptotic,durmus2019high}, and that without convexity has also gathered keen interest \citep{raginsky2017non,XCZ+18,erdogdu2018global}.
Whilst most previous studies are based on the Lipschitz continuity of the gradients of the potentials, several studies extend the settings to those without global Lipschitz continuity.
We can classify the settings of potentials in those studies into three types: (1) potentials with H\"{o}lder continuous or bounded gradients and convexity \citep{chatterji2020langevin,lehec2021langevin}; (2) potentials with H\"{o}lder continuous gradients and degenerate convexity at infinity or outside a ball \citep{erdogdu2021convergence,nguyen2022unadjusted,chewi2022analysis}; and (3) potentials with local Lipschitz gradients \citep{brosse2019tamed,zhang2023nonasymptotic}.
We review the results (1) and (2) as our study gives the error estimate of LMC with H\"{o}lder continuous gradients and Langevin-type algorithms with discontinuous gradients.

\citet{chatterji2020langevin} and \citet{lehec2021langevin} study Langevin-type algorithms under the convexity of potentials and non-smoothness or weak smoothness.
\citet{chatterji2020langevin} propose the perturbed Langevin Monte Carlo algorithm for non-smooth potential functions and show its convergence.
The difference between perturbed LMC and ordinary LMC is the input of the gradients; we need to add Gaussian noises not only to the gradients but also to the input of the gradient.
The main idea of the algorithm is to use the Gaussian smoothing of the potential functions studied in \citet{nesterov2017random}; the expectation of non-smooth convex potentials with inputs perturbed by Gaussian random vectors is smoother than the potentials themselves.
\citet{lehec2021langevin} investigates the projected LMC for potentials with convexity, global Lipschitz continuity and discontinuous bounded gradients.
The analysis is based on convexity and the estimate for local times of diffusion processes with reflecting boundaries.
The study also generalizes the result to the potentials with local Lipschitz by considering a ball as the support of the algorithm and letting the radius diverge.

\citet{erdogdu2021convergence}, \citet{chewi2022analysis}, and \citet{nguyen2022unadjusted} estimate the error of LMC under non-convex $\mathcal{C}^{2}$ potentials with degenerate convexity, weak smoothness, and weak-dissipativity.
\citet{erdogdu2021convergence} show the convergence of LMC under degenerate convexity at infinity and weak-dissipativity of the potentials with H\"{o}lder continuous gradients, which are the assumptions for modified logarithmic Sobolev inequalities.
\citet{nguyen2022unadjusted} relaxes the condition of \citet{erdogdu2021convergence} by considering the degenerate convexity outside a large ball and the mixture weak smoothness of potential functions.
\citet{chewi2022analysis} analyse the convergence with respect to the R\'{e}nyi divergence under either Lata\l{}a--Oleszkiewics inequalities or modified logarithmic Sobolev inequalities.

Note that our proof of the results uses approaches similar to the smoothing of \citet{chatterji2020langevin} and the control of the radius of \citet{lehec2021langevin}, whilst our motivations and settings are close to those of the studies under non-convexity.

\subsection{Contributions}
Theorem \ref{thm:sglmc}, the main theoretical result of this paper, gives an upper bound for the 2-Wasserstein distance between general SG-LMC given by Eq.~\eqref{eq:SGLMC} and the target distribution $\pi$ under weak conditions.
We assume the weak differentiability of $U$ combined with the boundedness of the modulus of continuity of a weak gradient $\nabla U$ rather than the twice continuous differentiability of $U$ and the Lipschitz continuity of $\nabla U$.
The generality of the assumptions results in a concise and general framework for the analysis of Langevin-type algorithms.
We demonstrate the strength of this framework through the analysis of LMC under weak smoothness and the proposal of new Langevin-type algorithms without continuous differentiability or convexity of $U$.

Our contribution to the analysis of LMC is to show a theoretical guarantee of LMC under non-convexity and weak smoothness in a direction different to the previous studies.
The main difference between our assumptions and those of the other studies under non-convex potentials is whether to assume (a) the strong dissipativity of the potentials and $\alpha$-H\"{o}lder continuity of the gradients with $\alpha>1/3$ or (b) the degenerate convexity and twice continuous differentiability of the potentials.
Our analysis needs neither the degenerate convexity nor twice continuous differentiability of the potentials, whilst we need the dissipativity and the decay of the modulus of continuity of $\nabla U$ stronger than those assumed in the previous studies.
Since we can easily consider examples such that only the assumptions of our analysis are satisfied (for instance, $U(x):=L(x)+R(x)$ with the strongly dissipative non-convex smooth $L(x)$ and $R(x)=\sum_{i=1}^{d} |x^{(i)}|^{1+\alpha}$ with $\alpha>1/3$) and vice versa, one of our main contributions is not to strengthen the previous studies but to broaden the theoretical guarantees of LMC under weak smoothness in a different direction.

Moreover, our proposal on Langevin-type algorithms with non-asymptotic error estimates for the potentials without convexity, continuous differentiability, or bounded gradients is also a significant contribution.
The proposed algorithms are useful for sampling from the posterior distributions with some models in statistics and machine learning, whose corresponding potential functions are dissipative and weakly differentiable but neither convex nor continuously differentiable (e.g., weakly differentiable non-convex losses with elastic net regularization).


\subsection{Notations}
Let $|\cdot|$ denote the Euclidean norm of $\R^{\ell}$ for all $\ell\in\N$.
$\langle \cdot,\cdot\rangle $ is the Eucllidean inner product of $\R^{\ell}$.
$\|\cdot\|_{2}$ is the spectral norm of matrices, which equals the largest singular value.
For arbitrary matrix $A$, $A^{\top}$ denotes the transpose of $A$.
For all $x\in\R^{\ell}$ and $R>0$, let $B_{R}(x)$ and $\Bar{B}_{R}(x)$ be an open ball and a closed one of centre $x$ and radius $R$ with respect to the Euclidean metric respectively.

For arbitrary two probability measures $\mu$ and $\nu$ on $(\R^{d},\Borel(\R^{d}))$, we define the $p$-Wasserstein distance between $\mu$ and $\nu$ such that
\begin{align*}
    \Wasserstein_{p}\left(\mu,\nu\right):=\left(\inf_{\pi\in\Pi(\mu,\nu)}\int_{\R^{d}\times\R^{d}}\left|x-y\right|^{p}\diff \pi\left(x,y\right)\right)^{\frac{1}{p}},
\end{align*}
where $\Pi(\mu,\nu)$ is the set of all couplings for $\mu$ and $\nu$.
We also define the Kullback--Leibler divergence of $\mu$ from $\nu$ with $\mu \ll \nu$ such that
\begin{align*}
    D\left(\mu\|\nu\right)=\int_{\R^{d}} \log\left(\frac{\diff \mu}{\diff \nu}\right)\diff \mu.
\end{align*}

\section{Main results}\label{sec:main}
This section gives the main theorem for non-asymptotic estimates of the error of general SG-LMC algorithms in 2-Wasserstein distance.

\subsection{Estimate of the errors of general SG-LMC}
We consider a compact polynomial mollifier \citep{anderson2014compact} $\rho:\R^{d}\to[0,\infty)$ as follows:
\begin{align}\label{eq:mollifier}
    \rho(x)=\begin{cases}
        \left(\frac{\pi^{d/2}\Beta(d/2,4)}{\Gamma(d/2)}\right)^{-1}\left(1-|x|^{2}\right)^{3}&\text{if }|x|\le 1,\\
        0 & \text{otherwise,}
    \end{cases}
\end{align}
where $\Beta(\cdot,\cdot)$ is the beta function and $\Gamma(\cdot)$ is the gamma function.
Note that $\nabla \rho$ has the explicit $L^{1}$-bound, which is the reason to adopt $\rho$ as the mollifier in our analysis; we give more detailed discussions on $\rho$ in Section \ref{sec:mollifier}.
Let $\rho_{r}(x)=r^{-d}\rho(x/r)$ with $r>0$.

We define $\widetilde{G}(x)$ such that for fixed $x$,
\begin{align*}
    \widetilde{G}(x):=\E\left[{G}\left(x,a_{0}\right)\right],
\end{align*}
whose measurability is given by the assumptions below and Tonelli's theorem.


We set the following assumptions on $U$ and $G$.
\begin{enumerate}
    \item[(A1)] $U\in W_{\loc}^{1,1}(\R^{d})$.
    \item[(A2)] For each $a\in A$ and $x\in\R^{d}$, $|G(x,a)|<\infty$.
\end{enumerate}
Under (A1), we fix a representative $\nabla U$ and consider the assumptions on $\nabla U$ and $\widetilde{G}$.
\begin{enumerate}
    \item[(A3)] $|\nabla U(\zero)|<\infty$ and $|\widetilde{G}(\zero)|<\infty$, and the moduli of continuity of $\nabla U$ and $\widetilde{G}$ are bounded, that is,
    \begin{align*}
        \omega_{\nabla U}(r)&:=\sup_{x,y\in\R^{d}:|x-y|\le r}\left|\nabla U(x)-\nabla U(y)\right|<\infty,\\
        \omega_{\widetilde{G}}(r)&:=\sup_{x,y\in\R^{d}:|x-y|\le r}\left|\widetilde{G}(x)-\widetilde{G}(y)\right|<\infty
    \end{align*}
    for some $r\in(0,1]$.
    \item[(A4)] For some $m,\Tilde{m},b,\Tilde{b}>0$, for all $x\in\R^{d}$,
    \begin{align*}
        \left\langle x,\nabla U\left(x\right)\right\rangle \ge m\left|x\right|^{2}-b,\
        \left\langle x,\widetilde{G}\left(x\right)\right\rangle \ge \Tilde{m}\left|x\right|^{2}-\Tilde{b}.
    \end{align*}
\end{enumerate}

\begin{remark}
    The boundedness of the moduli of continuity in Assumption (A4) is equivalent to the boundedness for all $r>0$ or for some $r>0$; see Lemma \ref{lem:FMC:representation}.
\end{remark}
Under (A1) and (A3), we can define the mollification 
\begin{align*}
    \nabla \Bar{U}_{r}(x):=\nabla \left(U\ast \rho_{r}\right)(x)=\nabla \int_{\R^{d}} U\left(y\right)\rho_{r}\left(x-y\right)\diff y.
\end{align*}
Using this mollification, we assume the quadratic growths of the bias of $\widetilde{G}$ with respect to $\nabla \Bar{U}_{r}$ and the variance as well.
\begin{enumerate}
    \item[(A5)] For some $\delta_{r}:=(\delta_{\bias,r,0},\delta_{\bias,r,2},\delta_{\variance,0},\delta_{\variance,2})\in[0,\infty)^{4}$, for all $x\in\R^{d}$, 
    \begin{align*}
        \left|\widetilde{G}(x)-\nabla \Bar{U}_{r}(x)\right|^{2}&\le 2\delta_{\bias,r,2}\left|x\right|^{2}+2\delta_{\bias,r,0},\\
        \E\left[\left|{G}\left(x,a_{0}\right)-\widetilde{G}(x)\right|^{2}\right]&\le 2\delta_{\variance,2}\left|x\right|^{2}+2\delta_{\variance,0}.
    \end{align*}
\end{enumerate}
For brevity, we use the notation $\delta_{r,i}=\delta_{\bias,r,i}+\delta_{\variance,i}$ for both $i=0,2$.
We also give the condition on the initial value $\xi$.
\begin{enumerate}
    \item[(A6)] The law $\mu_{0}$ of the initial value $\xi$ has the density $p_{0}$ with respect to the Lebesgue measure on $(\R^{d},\Borel(\R^{d}))$, and
    \begin{align*}
        \kappa_{0}:=\log\int_{\R^{d}}e^{|x|}p_{0}(x)\diff x<\infty.
    \end{align*}
\end{enumerate}

Let $\mu_{t}$ with $t\ge0$ denote the probability measure of $Y_{t}$.
The following theorem gives an upper bound for the 2-Wasserstein distance between $\mu_{k\eta}$ and $\pi$, whose proof is given in Section \ref{sec:proof}.

\begin{theorem}[error estimate of general SG-LMC]\label{thm:sglmc}
    Assume (A1)--(A6) and $\eta\in(0,1\wedge(\Tilde{m}/2((\omega_{\widetilde{G}}(1))^{2}+\delta_{\variance,2})))$.
    It holds that for any $r\in(0,1]$ and $k\in\N$,
    \begin{align*}
        \Wasserstein_{2}\left(\mu_{k\eta},\pi\right)\le &2C_{1}\left.\left(x^{\frac{1}{2}}+ x^{\frac{1}{4}}\right)\right|_{x=f(\delta_{r},r,k,\eta)}+C_{1}^{\prime}\sqrt{C_{2}+\sqrt{C_{2}}}e^{-k\eta/2\beta c_{\rm LS}\left(\Bar{\pi}_{r}\right)},
    \end{align*}
    where $f$ is the following function such that
    \begin{align*}
        f(\delta_{r},r,k,\eta):=\left(C_{0}\frac{\omega_{\nabla U}(r)}{r}\eta+\beta\left(\delta_{r,2}\kappa_{\infty}+\delta_{r,0}\right)\right)k\eta+\frac{\beta r\left\|\nabla U\right\|_{\FMC}}{2}\left(3+\sqrt{\frac{b+d/\beta}{m}}\right),
    \end{align*}
    $C_{0},C_{1},C_{1}^{\prime},C_{2},\kappa_{\infty}$ are the positive constants such that
    \begin{align*}
        C_{0}&:=\left(d+4\right)\left(\frac{\beta}{3}\left(\delta_{\variance,0}+\left\|\widetilde{G}\right\|_{\FMC}^{2}+\left((\omega_{\widetilde{G}}(1))^{2}+\delta_{\variance,2}\right)\kappa_{\infty}\right)+\frac{d}{2}\right)\\
        C_{1}&:=2\sqrt{4\kappa_{0}+\frac{32(b+m+d/\beta)}{m}+\frac{10}{1\wedge (\beta m/4)}}\\
        C_{1}^{\prime}&:=2\sqrt{\frac{32(b+m+d/\beta)}{m}+\frac{10}{1\wedge (\beta m/4)}}\\
        C_{2}&:=\sqrt{\log \|p_{0}\|_{\infty}+\frac{d}{2}\log\frac{3\pi}{m\beta}+\beta\left(\frac{\omega_{\nabla U}(1)}{2}\kappa_{0}+\frac{5}{2}\left\|\nabla U\right\|_{\FMC}\kappa_{0}^{\frac{1}{2}}+U_{0}+\frac{b}{2}\log3\right)}\\
        \kappa_{\infty}&:=\kappa_{0}+2\left(1\vee \frac{1}{\Tilde{m}}\right)\left(\Tilde{b}+\eta\left\|\widetilde{G}\right\|_{\FMC}^{2}+\delta_{\variance,0}+\frac{d}{\beta}\right),
    \end{align*}
    $U_{0}$, $\|\nabla U\|_{\FMC}$, and $\|\widetilde{G}\|_{\FMC}$ are positive constants such that
    \begin{align*}
        U_{0}:=\left\|U\right\|_{L^{\infty}(B_{1}(\zero))},\ \left\|\nabla U\right\|_{\FMC}:=\left|\nabla U(\zero)\right|+\omega_{\nabla U}(1),\     \left\|\widetilde{G}\right\|_{\FMC}:=\left|\widetilde{G}(\zero)\right|+\omega_{\widetilde{G}}(1),
    \end{align*}
    $c_{\rm P}(\Bar{\pi}^{r})$ is a positive constant with the inequality such that
    \begin{align*}
        c_{\rm P}(\Bar{\pi}^{r})
        &\le \frac{4}{m\beta\left(d+\left(b+m\right)\beta\right)}\\
        &\quad+\frac{8a\left(d+\left(b+m\right)\beta\right)}{m\beta}\exp\left(\beta\left(\frac{25}{16}\left\|\nabla U\right\|_{\FMC}\left(1+\frac{8\left(d+\left(b+m\right)\beta\right)}{m\beta}\right)+U_{0}\right)\right),
    \end{align*}
    $c_{\rm LS}(\Bar{\pi}^{r})$ is a positive constant with the inequality such that
    \begin{align*}
        c_{\rm LS}(\Bar{\pi}^{r})&\le \frac{\left(d+4\right)\omega_{\nabla U}\left(r\right)}{r}\left(\frac{32}{m^{2}\beta^{2}}+\frac{12\left(d+\left(b+m\right)\beta\right)c_{\rm P}\left(\Bar{\pi}^{r}\right)}{m\beta}\right)\\
        &\quad+\frac{2r}{\left(d+4\right)\omega_{\nabla U}\left(r\right)}+2c_{\rm P}\left(\Bar{\pi}^{r}\right),
    \end{align*}
    and $a>0$ is a positive absolute constant.
\end{theorem}

\subsection{A concise representation of Theorem \ref{thm:sglmc}}
Since the constants and the upper bounds for some of them in Theorem \ref{thm:sglmc} depend on a number of parameters, we give a concise representation of the result for the error analyses below.
Assuming that $f(\delta_{r},r,k,\eta)\le 1$ and $\eta\in(0,1\wedge \Tilde{m}/2((\omega_{\widetilde{G}}(1))^{2}+\delta_{\variance,2}))$, we obtain that for some $C\ge 1$ independent of $\delta,r,k,\eta,d$,
\begin{align}\label{eq:asymp:sglmc}
        \Wasserstein_{2}\left(\mu_{k\eta},\pi\right)&\le C\sqrt{d}\sqrt[4]{\left(d\left(d+\delta_{\variance,0}\right)\frac{\omega_{\nabla U}(r)}{r}\eta+\delta_{r,2}+\delta_{r,0}\right)k\eta+r\sqrt{d}}\notag\\
        &\quad+Cd\exp\left(-\frac{k\eta}{C\frac{\omega_{\nabla U}(r)}{r}d^{3}e^{Cd}}\right).
\end{align}

\section{The sampling complexities of Langevin-type algorithms}
We give the analyses of LMC and the algorithm named spherically smoothed SG-LMC (SS-SG-LMC) as the corollaries of Theorem \ref{thm:sglmc} based on the concise representation \eqref{eq:asymp:sglmc}.
We also propose the spherically smoothed LMC (SS-LMC) as a special case of SS-SG-LMC.

\subsection{Analysis of the LMC algorithm for $U$ of class $\mathcal{C}^{1,\alpha}$ with $\alpha>1/3$}
We examine the LMC algorithm for $U$ with the H\"{o}lder continuous gradient.
More precisely, we assume a weaker assumption than both $\alpha$-H\"{o}lder continuity and Lipschitz continuity: there exist $M>0$ and $\alpha\in(1/3,1]$ such that for all $x,y\in\R^{d}$,
\begin{align*}
    \left|\nabla U(x)-\nabla U(y)\right|\le M\left(\left|x-y\right|^{\alpha}\vee\left|x-y\right|\right).
\end{align*}
This assumption allows the gradient $\nabla U(x)$ to be at most linear growth, whilst $\alpha$-H\"{o}lder continuity with $\alpha\in(0,1)$ lets the gradient be at most sublinear growth.
This setting is seen in \citet{chatterji2020langevin} and a special case of the $\alpha$-mixture weak smoothness by \citet{nguyen2022unadjusted}.
Under this setting, we yield the representation of $\omega_{\nabla U}$ as
\begin{align*}
    \omega_{\nabla U}(r)=Mr^{\alpha}
\end{align*}
for all $r\in(0,1]$.

Under the LMC algorithm, we obtain $G=\nabla U$ and $\widetilde{G}=\nabla U$.
Therefore, the bias--variance decomposition in (A5) is given as
\begin{align*}
    \delta_{\bias,r,0}=\frac{(\omega_{\nabla U}(r))^{2}}{2},\  \delta_{\bias,r,2}= \delta_{\variance,0}= \delta_{\variance,2}=0
\end{align*}
by Lemma \ref{lem:FMC:lipMap} below.

We present the following assumptions.
\begin{enumerate}
    \item[(B1)] $U(x)$ is of class $\mathcal{C}^{1}$.
    \item[(B2)] There exist $\alpha\in(1/3,1]$ and $M>0$ such that for all $x,y\in\R^{d}$
    \begin{align*}
        \left|\nabla U(x)-\nabla U(y)\right|\le M\left(\left|x-y\right|^{\alpha}\vee\left|x-y\right|\right).
    \end{align*}
    \item[(B3)] There exist $m,b>0$ such that for all $x\in\R^{d}$,
    \begin{align*}
        \left\langle x,\nabla U(x)\right\rangle \ge m\left|x\right|^{2}-b.
    \end{align*}
\end{enumerate}

(A1)--(A4) hold immediately by (B1)--(B3); therefore, we yield the following corollary.
\begin{corollary}[error estimate of LMC]
Under (B1)--(B3) and (A6), there exists a constant $C\ge1$ independent of $r,\alpha,k,\eta,d$ such that for all $k\in\N$, $\eta\in(0,1\wedge m/2(\omega_{\nabla U}(1))^{2})$, and $r\in(0,1]$ with $\left(d^{2}r^{\alpha-1}\eta+r^{2\alpha}\right)k\eta+r\sqrt{d}\le 1$, 
\begin{align}\label{eq:asymp:lmc}
    \Wasserstein_{2}\left(\mu_{k\eta},\pi\right)&\le C\sqrt{d}\sqrt[4]{\left(d^{2}r^{\alpha-1}\eta+r^{2\alpha}\right)k\eta+r\sqrt{d}}+Cd\exp\left(-\frac{k\eta}{Cr^{\alpha-1}d^{3}e^{Cd}}\right).
\end{align}
\end{corollary}

%\subsubsection{LMC with $\alpha\in(1/3,2/3]$}
\subsubsection{The sampling complexity of the LMC algorithm}
We present the propositions regarding the sampling complexity in the cases (1) $\alpha\in(1/3,2/3]$, (2) $\alpha\in(2/3,1)$, and (3) $\alpha=1$.


%The next proposition shows the complexity of LMC under $\alpha\in(1/3,2/3]$.
\begin{proposition}\label{prop:LMC:1}
    Assume that (B1)--(B3) and (A6) hold and fix $\epsilon\in(0,1]$.
    If $\alpha\in(1/3,2/3]$ and $k$ satisfies
    \begin{align*}
        k&\ge d^{2}\left(Cd^{3}e^{Cd}\log(2Cd/\epsilon)\right)^{\frac{3\alpha+1}{3\alpha-1}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{2}{3\alpha-1}},
    \end{align*}
    then $\Wasserstein_{2}\left(\mu_{k\eta},\pi\right)\le \epsilon$ for $\eta$ and $r$ such that
    \begin{align*}
        \eta=\left[\left(\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k}\right)^{\frac{1+\alpha}{1+3\alpha}}\right)\wedge \frac{m}{2(\omega_{\nabla U}(1))^{2}}\right],\ r=\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1}{2\alpha}}.
    \end{align*}
\end{proposition}

\begin{proof}
    We only need to check 
    \begin{align*}
        \max\left\{d^{2}r^{\alpha-1}k\eta^{2},r^{2\alpha}k\eta,r\sqrt{d}\right\}\le \frac{\epsilon^{4}}{48C^{4}d^{2}},\ Cd\exp\left(-\frac{k\eta}{Cr^{\alpha-1}d^{3}e^{Cd}}\right)\le \frac{\epsilon}{2}.
    \end{align*}
    Note that
    \begin{align*}
        k\eta %&=\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{1+3\alpha}}k^{\frac{2\alpha}{1+3\alpha}}\\
        &\ge \left(Cd^{3}e^{Cd}\log(2Cd/\epsilon)\right)^{\frac{2\alpha}{3\alpha-1}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{3\alpha-1}}.
    \end{align*}
    % (1) We see that
    % \begin{align*}
    %     d^{2}r^{\alpha-1}k\eta^{2}%&=d^{2}\left(\frac{48C^{4}d^{2}k\eta}{\epsilon^{4}}\right)^{\frac{1-\alpha}{2\alpha}}k\eta^{2}\\
    %     % &= d^{2}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{2\alpha}}(k\eta)^{\frac{1+\alpha}{2\alpha}}\eta\\
    %     % &=d^{2}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{2\alpha}}\left(\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{1+3\alpha}}k^{\frac{2\alpha}{1+3\alpha}}\right)^{\frac{1+\alpha}{2\alpha}}\\
    %     % &\quad\times\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k}\right)^{\frac{1+\alpha}{1+3\alpha}}\\
    %     % &=\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{2\alpha}}\left(\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{1+3\alpha}}\right)^{\frac{1+\alpha}{2\alpha}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{1+3\alpha}}\\
    %     % &=\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{2\alpha}}\left(\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{1+3\alpha}}\right)^{\frac{1+3\alpha}{2\alpha}}\\
    %     % &=\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{2\alpha}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{2\alpha}}\\
    %     % &=\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{2\alpha}-\frac{1-\alpha}{2\alpha}}\\
    %     &=\frac{\epsilon^{4}}{48C^{4}d^{2}}.
    % \end{align*}
    % (2) We have
    We have $d^{2}r^{\alpha-1}k\eta^{2}=\epsilon^{4}/48C^{4}d^{2}$.
    % \begin{align*}
    %     d^{2}r^{\alpha-1}k\eta^{2}=r^{2\alpha}k\eta =\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}k\eta=\frac{\epsilon^{4}}{48C^{4}d^{2}}.
    % \end{align*}
    Since $\alpha\le 2/3$ and
    \begin{align*}
        r\le \left(\frac{\epsilon^{4}}{48C^{5}d^{5}e^{Cd}\log(2Cd/\epsilon)}\right)^{\frac{1}{3\alpha-1}},
    \end{align*}
    it holds that $r\sqrt{d}\le \epsilon^{4}/48C^{4}d^{2}$.
    % \begin{align*}
    %     r\sqrt{d}\le \frac{\epsilon^{4}}{48C^{4}d^{2}}.
    % \end{align*}
    Finally, we easily obtain that $Cd\exp(-k\eta/Cr^{\alpha-1}d^{3}e^{Cd})\le \epsilon/2$.
    % \begin{align*}
    %     Cd\exp\left(-\frac{k\eta}{Cr^{\alpha-1}d^{3}e^{Cd}}\right)%&= Cd\exp\left(-\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1-\alpha}{2\alpha}}\frac{(k\eta)^{\frac{3\alpha-1}{2\alpha}}}{Cd^{3}e^{Cd}}\right)\\
    %     %&\le Cd\exp\left(-\frac{Cd^{3}e^{Cd}\log(2Cd/\epsilon)}{Cd^{3}e^{Cd}}\right)\\
    %     &\le \frac{\epsilon}{2}.
    % \end{align*}
    Therefore, the statement holds true.
\end{proof}

% We control $k\eta$, $\eta$ and $r$ to let the following inequalities hold true:
% \begin{align*}
%     \max\left\{d^{2}r^{\alpha-1}k\eta^{2},r^{2\alpha}k\eta,r\sqrt{d}\right\}\le \frac{\epsilon^{4}}{48C^{4}d^{2}},\ Cd\exp\left(-\frac{k\eta}{Cr^{\alpha-1}d^{3}e^{Cd}}\right)\le \frac{\epsilon}{2}.
% \end{align*}
% $\eta$ can be arbitrarily small, and thus we firstly consider to control $r$, which should not be too large or small.

% For given $k\eta$, $r\in(0,1]$ must satisfy
% % \begin{align*}
% %     k\eta \in\left[Cr^{\alpha-1}d^{3}e^{Cd}\log(2Cd/\epsilon), r^{-2\alpha}\frac{\epsilon^{4}}{48C^{4}d^{2}}\right].
% % \end{align*}
% \begin{align*}
%     r \in\left[\left(\frac{Cd^{3}e^{Cd}\log(2Cd/\epsilon)}{k\eta}\right)^{\frac{1}{1-\alpha}}, \left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1}{2\alpha}}\right]=:R_{1}.
% \end{align*}
% The set $R_{1}$ is nonempty if it holds that
% \begin{align*}
%     k\eta \ge \left(Cd^{3}e^{Cd}\log(2Cd/\epsilon)\right)^{\frac{2\alpha}{3\alpha-1}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{3\alpha-1}}
%     % r\le \left(\frac{\epsilon^{4}}{48C^{5}d^{5}e^{Cd}\log(2Cd/\epsilon)}\right)^{\frac{1}{3\alpha-1}}.
% \end{align*}
% and thus we consider the conditions on $k$ and $\eta$ such that $k\eta$ satisfy this inequality.
% This yields that for arbitrary $r\in R_{1}$,
% \begin{align*}
%     r
%     \le \left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1}{2\alpha}}
%     \le \left(\frac{\epsilon^{4}}{48C^{5}d^{5}e^{Cd}\log(2Cd/\epsilon)}\right)^{\frac{1}{3\alpha-1}}.
% \end{align*}
% $r$ with this property immediately leads to 
% \begin{align*}
%     r\sqrt{d}\le \frac{\epsilon^{4}}{48C^{4}d^{2}}
% \end{align*}
% since $\alpha\le 2/3$.
% We consider 
% \begin{align*}
%     \eta \le \frac{r^{1-\alpha}}{k\eta}\frac{\epsilon^{4}}{48C^{4}d^{4}}.
% \end{align*}
% Since $\eta$ must be small if $r$ is small, the optimal choice of $r$ is 
% \begin{align*}
%     r=\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1}{2\alpha}},
% \end{align*}
% and thus we choose $\eta$ for each $k$ such that
% \begin{align*}
%     \eta =\frac{1}{d^{2}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1-\alpha}{2\alpha}}\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}=\frac{1}{d^{2}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1+\alpha}{2\alpha}}
% \end{align*}
% and thus
% \begin{align*}
%     \eta &=\left(\frac{1}{d^{2}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k}\right)^{\frac{1+\alpha}{2\alpha}}\right)^{\frac{2\alpha}{1+3\alpha}}\\
%     &=\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k}\right)^{\frac{1+\alpha}{1+3\alpha}}.
% \end{align*}
% % \begin{align*}
% %     \eta &=\left(\frac{1}{k}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1-\alpha}{2\alpha}}\frac{\epsilon^{4}}{48C^{4}d^{4}}\right)^{\frac{1}{2}}\\
% %     \eta &=\left(\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k}\right)^{\frac{1-\alpha}{2\alpha}}\frac{\epsilon^{4}}{48C^{4}d^{4}k}\right)^{\frac{2\alpha}{1+3\alpha}}\\
% %     &=\left(\frac{1}{d^{2}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k}\right)^{\frac{1-\alpha}{2\alpha}+1}\right)^{\frac{2\alpha}{1+3\alpha}}\\
% %     &=\left(\frac{1}{d^{2}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k}\right)^{\frac{1+\alpha}{2\alpha}}\right)^{\frac{2\alpha}{1+3\alpha}}\\
% % \end{align*}
% Finally we obtain the following sampling complexity:
% because
% \begin{align*}
%     k\eta=\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{1+3\alpha}}k^{1-\frac{1+\alpha}{1+3\alpha}}=\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{1+3\alpha}}k^{\frac{2\alpha}{1+3\alpha}},
% \end{align*}
% we obtain
% \begin{align*}
%     k&\ge d^{2}\left(Cd^{3}e^{Cd}\log(2Cd/\epsilon)\right)^{\frac{3\alpha+1}{3\alpha-1}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\left(\frac{1-\alpha}{3\alpha-1}+\frac{1+\alpha}{3\alpha+1}\right)\frac{3\alpha+1}{2\alpha}}\\
%     &\ge d^{2}\left(Cd^{3}e^{Cd}\log(2Cd/\epsilon)\right)^{\frac{3\alpha+1}{3\alpha-1}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{4\alpha}{(3\alpha-1)(3\alpha+1)}\frac{3\alpha+1}{2\alpha}}\\
%     &=\Omega\left(e^{\mathcal{O}(d)}\left(\frac{1}{\epsilon^{4}}\right)^{1+\frac{1-1\alpha}{3\alpha-1}}\left(\log\left(\frac{1}{\epsilon}\right)\right)^{\frac{4\alpha}{3\alpha-1}}\right).
% \end{align*}

% % \begin{align*}
% %     k\eta&=\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{3\alpha-1}}\left(Cd^{3}e^{Cd}\log\left(2Cd/\epsilon\right)\right)^{\frac{2\alpha}{3\alpha-1}},\\
% %     \eta&= \left(\frac{1}{\log\left(2Cd/\epsilon\right)}\right)^{\frac{2\alpha}{3\alpha-1}}\left(\frac{1}{Cd^{3}e^{Cd}}\right)^{\frac{1+\alpha}{3\alpha-1}}\frac{\epsilon^{4}}{48 C^{4}d^{4}}
% % \end{align*}
% % and thus
% % \begin{align*}
% %     % k&\ge \frac{48 C^{4}d^{4}}{\epsilon^{4}}\left(\frac{48C^{5}d^{5}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{3\alpha-1}}\left(Cd^{3}e^{Cd}\log\left(2Cd/\epsilon\right)\right)^{\frac{4\alpha}{3\alpha-1}}
% %     k=\Tilde{\Omega}\left(e^{\mathcal{O}(d)}\left(1/\epsilon^{4}\right)^{1+\frac{1-\alpha}{3\alpha-1}}\right).
% % \end{align*}

% % A sufficient condition for $\Wasserstein_{2}\left(\mu_{k\eta},\pi\right)\le \epsilon$ for arbitrary $\epsilon\in(0,1)$ is
% % \begin{align*}
% %     k\eta&\ge \left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{3\alpha-1}}\left(Cd^{3}e^{Cd}\log\left(2Cd/\epsilon\right)\right)^{\frac{2\alpha}{3\alpha-1}}\\
% %     \eta&= \left(\frac{1}{\log\left(2Cd/\epsilon\right)}\right)^{\frac{2\alpha}{3\alpha-1}}\left(\frac{1}{Cd^{3}e^{Cd}}\right)^{\frac{1+\alpha}{3\alpha-1}}\frac{\epsilon^{4}}{48 C^{4}d^{4}}\\
% %     k&\ge \frac{48 C^{4}d^{4}}{\epsilon^{4}}\left(\frac{48C^{5}d^{5}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{3\alpha-1}}\left(Cd^{3}e^{Cd}\log\left(2Cd/\epsilon\right)\right)^{\frac{4\alpha}{3\alpha-1}}.
% % \end{align*}
% % Therefore, the sampling complexity for $\epsilon$-neighbourhood is
% % \begin{align*}
% %     %\Omega\left(\frac{d^{4}}{\epsilon^{4}}\left(\frac{d^{5}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{3\alpha-1}}\left(d^{3}e^{\mathcal{O}(d)}\log\left(\frac{d}{\epsilon}\right)\right)^{\frac{4\alpha}{3\alpha-1}}\right)
% %     %\Omega\left(e^{\mathcal{O}(d)}\left(\frac{1}{\epsilon}\right)^{4+\frac{4-4\alpha}{3\alpha-1}}\left(\log\left(\frac{1}{\epsilon}\right)\right)^{\frac{4\alpha}{3\alpha-1}}\right).
% %     \Tilde{\Omega}\left(e^{\mathcal{O}(d)}\left(1/\epsilon^{4}\right)^{1+\frac{1-\alpha}{3\alpha-1}}\right).
% % \end{align*}

% % (1) If $r\ge (\epsilon^{4}/(48C^{5}d^{5}e^{Cd}\log (2Cd/\epsilon)))^{1/(3\alpha-1)}$, we obtain that
% % \begin{align*}
% %     &Cd\exp\left(-r^{1-\alpha}\left(\frac{k\eta}{Cd^{3}e^{Cd}}\right)\right)\\
% %     &\le Cd\exp\left(-r^{1-\alpha}\left(\frac{48C^{5}d^{5}e^{Cd}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{3\alpha-1}}\left(\log\left(\frac{2Cd}{\epsilon}\right)\right)^{\frac{2\alpha}{3\alpha-1}}\right)\le \frac{\epsilon}{2}.
% % \end{align*}

% % (2) Since
% % \begin{align*}
% %     \left(\frac{\epsilon^{4}}{48C^{5}d^{5}e^{Cd}}\right)^{\frac{1}{3\alpha-1}}
% %     =\left(\frac{\epsilon^{4}}{48C^{4}d^{2}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{3\alpha-1}}\left(Cd^{3}e^{Cd}\right)^{\frac{2\alpha}{3\alpha-1}}}\right)^{\frac{1}{2\alpha}}
% %     \le \left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta }\right)^{\frac{1}{2\alpha}}
% % \end{align*}
% % if it holds that
% % \begin{align*}
% %     r&\le \left(\frac{\epsilon^{4}}{48C^{5}d^{5}e^{Cd}}\right)^{\frac{1}{3\alpha-1}},
% % \end{align*}
% % then
% % \begin{align*}
% %     r^{2\alpha}k\eta\le \frac{\epsilon^{4}}{48C^{4}d^{2}}.
% % \end{align*}

% % We can take such a $r$ because
% % \begin{align*}
% %     (\epsilon^{4}/(48C^{5}d^{5}e^{Cd}\log (2Cd/\epsilon)))^{1/(3\alpha-1)}\le \left(\frac{\epsilon^{4}}{48C^{5}d^{5}e^{Cd}}\right)^{\frac{1}{3\alpha-1}}.
% % \end{align*}

% % In the next place, we consider $\eta$ such that
% % \begin{align*}
% %     d^{2}r^{\alpha-1}k\eta^{2}\le \frac{\epsilon^{4}}{48 C^{4}d^{2}}.
% % \end{align*}
% % This is satisfied if
% % \begin{align*}
% %     \eta &\le 
% %     \left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{-\frac{1-\alpha}{3\alpha-1}}\left(Cd^{3}e^{Cd}\log\left(2Cd/\epsilon\right)\right)^{-\frac{2\alpha}{3\alpha-1}}\left(\frac{\epsilon^{4}}{48C^{5}d^{5}e^{Cd}}\right)^{\frac{1-\alpha}{3\alpha-1}}\frac{\epsilon^{4}}{48 C^{4}d^{2}}d^{-2}\\
% %     &=\left(\frac{1}{Cd^{3}e^{Cd}\log\left(2Cd/\epsilon\right)}\right)^{\frac{2\alpha}{3\alpha-1}}\left(\frac{1}{Cd^{3}e^{Cd}}\right)^{\frac{1-\alpha}{3\alpha-1}}\frac{\epsilon^{4}}{48 C^{4}d^{4}}\\
% %     &=\left(\frac{1}{\log\left(2Cd/\epsilon\right)}\right)^{\frac{2\alpha}{3\alpha-1}}\left(\frac{1}{Cd^{3}e^{Cd}}\right)^{\frac{1+\alpha}{3\alpha-1}}\frac{\epsilon^{4}}{48 C^{4}d^{4}}.
% % \end{align*}
% % We should check that
% % \begin{align*}
% %     r\sqrt{d}\le \frac{\epsilon^{4}}{48C^{4}d^{2}}
% % \end{align*}
% % and this is equivalent to 
% % \begin{align*}
% %     r\le \frac{\epsilon^{4}}{48C^{4}d^{3/2}}
% % \end{align*}
% % which holds if $\alpha\le 2/3$.

% \subsubsection{LMC with $\alpha\in(2/3,1)$}

%In the second place, we examine the LMC with $\alpha\in(2/3,1)$.
\begin{proposition}\label{prop:LMC:2}
    Assume that (B1)--(B3) and (A6) hold and fix $\epsilon\in(0,1]$.
    If $\alpha\in(2/3,1)$ and $k$ satisfies
    \begin{align*}
        k&\ge \left[\left(d^{\frac{5+3\alpha}{2}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{3\alpha}\right)\vee \left(d^{2}\left(Cd^{3}e^{Cd}\log(2Cd/\epsilon)\right)^{\frac{3\alpha+1}{3\alpha-1}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{2}{3\alpha-1}}\right)\right],
    \end{align*}
    then $\Wasserstein_{2}\left(\mu_{k\eta},\pi\right)\le \epsilon$ for $\eta$ and $r$ such that
    \begin{align*}
        \eta=\left[\left(\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k}\right)^{\frac{1+\alpha}{1+3\alpha}}\right)\wedge \frac{m}{2(\omega_{\nabla U}(1))^{2}}\right],\ r=\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1}{2\alpha}}.
    \end{align*}
\end{proposition}

\begin{proof}
    The difference to Proposition \ref{prop:LMC:1} is the bound for $r\sqrt{d}$.
    Since
    \begin{align*}
        k\eta %&= k^{2\alpha/(1+3\alpha)}\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{1+3\alpha}}\\
        %&\ge \left(d^{\frac{5+3\alpha}{2}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{3\alpha}\right)^{2\alpha/(1+3\alpha)}\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{1+3\alpha}}\\
        &\ge d^{\alpha} \left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{2\alpha-1},
    \end{align*}
    it holds that 
    \begin{align*}
        r\sqrt{d}\le\sqrt{d}\left(d^{-\alpha}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{1+2\alpha-1}\right)^{\frac{1}{2\alpha}}= \frac{\epsilon^{4}}{48C^{4}d^{2}}.
    \end{align*}
    The rest of the proof is identical to that of Proposition \ref{prop:LMC:1}.
\end{proof}

% We control $k\eta$, $\eta$ and $r$ to let the following inequalities hold true:
% \begin{align*}
%     \max\left\{d^{2}r^{\alpha-1}k\eta^{2},r^{2\alpha}k\eta,r\sqrt{d}\right\}\le \frac{\epsilon^{4}}{48C^{4}d^{2}},\ Cd\exp\left(-\frac{k\eta}{Cr^{\alpha-1}d^{3}e^{Cd}}\right)\le \frac{\epsilon}{2}.
% \end{align*}
% $\eta$ can be arbitrarily small, and thus we firstly consider to control $r$, which should not be too large or small.

% For given $k\eta$, $r\in(0,1]$ must satisfy
% % \begin{align*}
% %     k\eta \in\left[Cr^{\alpha-1}d^{3}e^{Cd}\log(2Cd/\epsilon), r^{-2\alpha}\frac{\epsilon^{4}}{48C^{4}d^{2}}\right].
% % \end{align*}
% \begin{align*}
%     r \in\left[\left(\frac{Cd^{3}e^{Cd}\log(2Cd/\epsilon)}{k\eta}\right)^{\frac{1}{1-\alpha}}, \left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1}{2\alpha}}\wedge \frac{\epsilon^{4}}{48C^{4}d^{5/2}}\right]=:R_{1}.
% \end{align*}
% We assume that
% \begin{align*}
%     k\eta\ge d^{\alpha}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{2\alpha-1}.
% \end{align*}
% Then we obtain that
% \begin{align*}
%     \left[\left(\frac{Cd^{3}e^{Cd}\log(2Cd/\epsilon)}{k\eta}\right)^{\frac{1}{1-\alpha}}, \left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1}{2\alpha}}\right]=R_{1}
% \end{align*}
% The set $R_{1}$ is nonempty if it holds that
% \begin{align*}
%     k\eta \ge \left(Cd^{3}e^{Cd}\log(2Cd/\epsilon)\right)^{\frac{2\alpha}{3\alpha-1}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{3\alpha-1}}
%     % \left(Cd^{3}e^{Cd}\log(2Cd/\epsilon)\right)^{\frac{2\alpha}{3\alpha-1}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{3\alpha-1}}\vee\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{1-\alpha}\left(Cd^{3}e^{Cd}\log(2Cd/\epsilon)\right).
% \end{align*}
% and thus we consider the conditions on $k$ and $\eta$ such that $k\eta$ satisfy the inequality
% \begin{align*}
%     k\eta\ge d^{\alpha}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{2\alpha-1}\vee\left(Cd^{3}e^{Cd}\log(2Cd/\epsilon)\right)^{\frac{2\alpha}{3\alpha-1}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{1-\alpha}{3\alpha-1}}
% \end{align*}.
% This yields that for arbitrary $r\in R_{1}$,
% \begin{align*}
%     r
%     \le \left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1}{2\alpha}}
%     \le \left(\frac{\epsilon^{4}}{48C^{5}d^{5}e^{Cd}\log(2Cd/\epsilon)}\right)^{\frac{1}{3\alpha-1}}.
% \end{align*}
% We consider 
% \begin{align*}
%     \eta \le \frac{r^{1-\alpha}}{k\eta}\frac{\epsilon^{4}}{48C^{4}d^{4}}.
% \end{align*}
% Since $\eta$ must be small if $r$ is small, the optimal choice of $r$ is 
% \begin{align*}
%     r=\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1}{2\alpha}},
% \end{align*}
% and thus we choose $\eta$ for each $k$ such that
% \begin{align*}
%     \eta =\frac{1}{d^{2}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1-\alpha}{2\alpha}}\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}=\frac{1}{d^{2}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k\eta}\right)^{\frac{1+\alpha}{2\alpha}}
% \end{align*}
% and thus
% \begin{align*}
%     \eta &=\left(\frac{1}{d^{2}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k}\right)^{\frac{1+\alpha}{2\alpha}}\right)^{\frac{2\alpha}{1+3\alpha}}\\
%     &=\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}k}\right)^{\frac{1+\alpha}{1+3\alpha}}.
% \end{align*}
% Finally we obtain the following sampling complexity:
% because
% \begin{align*}
%     k\eta=\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{1+3\alpha}}k^{1-\frac{1+\alpha}{1+3\alpha}}=\frac{1}{d^{4\alpha/(1+3\alpha)}}\left(\frac{\epsilon^{4}}{48C^{4}d^{2}}\right)^{\frac{1+\alpha}{1+3\alpha}}k^{\frac{2\alpha}{1+3\alpha}},
% \end{align*}
% we obtain
% \begin{align*}
%     k&\ge \left(d^{4\alpha/(1+3\alpha)+\alpha}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{2\alpha-1+\frac{1+\alpha}{1+3\alpha}}\right)^{\frac{1+3\alpha}{2\alpha}}\vee d^{2}\left(Cd^{3}e^{Cd}\log(2Cd/\epsilon)\right)^{\frac{3\alpha+1}{3\alpha-1}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\left(\frac{1-\alpha}{3\alpha-1}+\frac{1+\alpha}{3\alpha+1}\right)\frac{3\alpha+1}{2\alpha}}\\
%     &\ge \left(d^{\frac{5+3\alpha}{2}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{3\alpha}\right)\vee d^{2}\left(Cd^{3}e^{Cd}\log(2Cd/\epsilon)\right)^{\frac{3\alpha+1}{3\alpha-1}}\left(\frac{48C^{4}d^{2}}{\epsilon^{4}}\right)^{\frac{4\alpha}{(3\alpha-1)(3\alpha+1)}\frac{3\alpha+1}{2\alpha}}\\
%     &=\Omega\left(e^{\mathcal{O}(d)}\left(\frac{1}{\epsilon^{4}}\right)^{1+\frac{1-1\alpha}{3\alpha-1}}\left(\log\left(\frac{1}{\epsilon}\right)\right)^{\frac{4\alpha}{3\alpha-1}}\right).
% \end{align*}

% \subsubsection{LMC with $\alpha=1$}

%Finally, we consider the complexity of the LMC with $\alpha=1$.

\begin{proposition}
    Assume that (B1)--(B3) and (A6) hold and fix $\epsilon\in(0,1]$.
    If $\alpha=1$ and $k$ satisfies
    \begin{align*}
        k&\ge \frac{16C^{6}d^{10}C^{2}d^{6}e^{2Cd}(\log(2Cd/\epsilon))^{2}}{\epsilon^{4}},
    \end{align*}
    then $\Wasserstein_{2}\left(\mu_{k\eta},\pi\right)\le \epsilon$ for $\eta$ such that
    \begin{align*}
        \eta=\sqrt{\frac{\epsilon^{4}}{16C^{4}d^{4}k}}.
    \end{align*}
\end{proposition}

\begin{proof}
    If $\alpha=1$, then by taking the infimum with respect to $r$,
    \begin{align*}
        \Wasserstein_{2}\left(\mu_{k\eta},\pi\right)&\le Cd\sqrt[4]{k\eta^{2}}+Cd\exp\left(-\frac{k\eta}{Cd^{3}e^{Cd}}\right).
    \end{align*}
    Since $k\eta \ge Cd^{3}e^{Cd}\log(2Cd/\epsilon)$ and $k\eta^{2}\le \epsilon^{4}/16 C^{4}d^{4}$, it clearly holds.
\end{proof}

% \begin{align*}
%     k\eta \ge Cd^{3}e^{Cd}\log(2Cd/\epsilon),\ \eta\le \frac{\epsilon^{4}}{16C^{5}d^{7}e^{Cd}\log(2Cd/\epsilon)},\ k\ge \frac{16C^{6}d^{10}e^{2Cd}(\log(2Cd/\epsilon))^{2}}{\epsilon^{4}}
% \end{align*}
% gives $\epsilon$-neighbourhood.
This recovers the complexity by \citet{raginsky2017non} under $U$ of class $\mathcal{C}^{2}$ and with the Lipschitz continuous gradient $\nabla U$.
Note that we only need to assume that $U$ is of class $\mathcal{C}^{1}$ and its gradient is Lipschitz continuous.

% \begin{lemma}
%     \begin{align*}
%         r^{2\alpha}T&\le \frac{\epsilon^{4}}{48}\\
%         r&\le \frac{\epsilon^{4}}{48}\\
%         \exp\left(-T/r^{\alpha-1}\right)&\le \frac{\epsilon}{2}
%     \end{align*}
% \end{lemma}

% \begin{proof}
%     $T$ must satisfy that $T\in [r^{\alpha-1}\log(2/\epsilon), r^{-2\alpha}\epsilon^{4}/48]$ and thus we need the condition that the set is nonempty.
%     This can hold if $r^{3\alpha-1}\le \epsilon^{4}/48\log(2/\epsilon)$.
%     This gives that $r\le (\epsilon^{4}/48\log(2/\epsilon))^{1/(3\alpha-1)}$.
%     Thus $r$ should satisfy that
%     \begin{align*}
%         r\le \left(\frac{\epsilon^{4}}{48\log(2/\epsilon)}\right)^{1/(3\alpha-1)}\wedge \frac{\epsilon^{4}}{48}
%     \end{align*}
% \end{proof}

% In the application of Monte Carlo algorithms, we confront with a number of loss functions without twice continuous differentiability.
% A trivial and motivating example is the bridge regression with linear statistical models and square loss: 
% \begin{align}\label{eq:example:bridge}
%     U(x)=\frac{1}{2}\sum_{i=1}^{n}\left(y_{i}-\langle f_{i},x\rangle\right)^{2}+\lambda\sum_{j=1}^{d}|x_{j}|^{\gamma}
% \end{align}
% where $\gamma\in(4/3,2]$, $\{y_{i}\}$ is a sequence of real-valued responses, $\{f_{i}\}$ is a sequence of $d$-dimensional features such that $\sum_{i}f_{i}f_{i}^{\top}$ is positive definite.
% Note that this is a convex potential, but the gradient $\nabla U(x)$ is neither Lipschitz nor H\"{o}lder unless $\gamma=2$.
% Hence the previous studies do not give the convergence guarantee of the Langevin Monte Carlo algorithm \citep{erdogdu2021convergence}.
% The perturbed LMC by \citet{chatterji2020langevin} has the explicit rate of convergence for this potential, whilst our concern is about the convergence of LMC.
% The most closest result is the mixture weak smoothness by \citet{nguyen2022unadjusted}, but the potential \eqref{eq:example:bridge} is not in $\mathcal{C}^{2}$.
% The projected LMC \citep{lehec2021langevin} also gives the convergence if the $U(x)$ satisfies a logarithmic Sobolev inequality; however, it is not straightforward to check it as the potential \eqref{eq:example:bridge} is not of class $\mathcal{C}^{2}$ if $\gamma<2$. % ; therefore, the well-known criteria by the spectral gap or the Hessian are not applicable.

% Our mollification approach works well even for the models with Lipschitz gradients but without Hessian: for example, the Huber loss with ridge regularization:
% \begin{align*}
%     U(x)=\frac{1}{2}\sum_{i=1}^{n}\psi_{\delta}\left(y-f_{i}^{\top}x\right)+\lambda\left|x\right|^{2},
% \end{align*}
% where $\psi_{\delta}:\R\to\R_{+}$ is defined as 
% \begin{align*}
%     \psi_{\delta}\left(z\right)=\begin{cases}
%         \frac{1}{2}z^{2} & \text{ if }|z|\le \delta,\\
%         \delta\left(|z|-\delta/2\right) & \text{ otherwise},
%     \end{cases}
% \end{align*}
% and $\delta>0$ is a fixed constant.
% By deteriorating the Lipschitz constants by $d+4$, we obtain the same results in the previous results with convexity of $U$ and twice differentiability.

% We can consider the following potential in statistical inference:
% \begin{align*}
%     U(x)=\sum_{i=1}^{n}L\left(y_{i},f_{i}(x)\right)+\lambda\left|x\right|^{2}
% \end{align*}
% where $L$ is the loss function and $f_{i}(x)$ is the statistical model such that $\nabla L(y_{i},f_{i}(x))=\partial_{f}L(y_{i},f_{i}(x))\nabla f_{i}(x)$ are $\alpha$-H\"{o}lder continuous in $x$ with $\alpha\in(1/3,1]$.
% One of the trivial examples is the square loss and the linear regression models such that $L(y_{i},f_{i}(x))=(y_{i}-f_{i}(x))^{2}$ and $f_{i}(x)=\langle f_{i},x\rangle $ with $f_{i}\in\R^{d}$; the assumption is clearly satisfied with $\alpha=1$.
% We set the $\ell_{2}$ regularization with sufficiently large $\lambda>0$ to give the dissipativity of $U$.

% It is also parallel to extend the other studies on the convergence of ULA or SGLD under smoothness \citep[e.g.,][]{erdogdu2018global,XCZ+18}.

% \begin{remark}
%     We do not extend the bound by \citet{XCZ+18}, which gives concise representations and better convergence because the representation of $C_{\psi}>0$, an upper bound for the solution and its derivatives for a Poisson equation, is complex even if we use the result of \citet{erdogdu2018global}.
%     %$\E[|\nabla^{j}\psi(X_{k})|^{2}]$ with $j=0,1,2$ and $\psi$ is the solution of the Poisson equation $\mathcal{L}\psi=F-\Bar{F}$
% \end{remark}

% Let us define the following measurement for the flatness of each minimum
% \begin{align*}
%     \Delta_{F}(x,R):=\sup_{y\in\R^{d}:|y-x|\le R}U\left(y\right)-F(x),
% \end{align*}
% which is similar to the metric for the flatness of minima defined by \citet{KMN+17}.

% Under this flat minima hypothesis, we yield that for any $x^{\ast}\in\argmin\{F(x)\}$,
% \begin{align*}
%     \min_{x\in\R^{d}}F_{R}\left(x\right)-F^{\ast}&\le F_{R}\left(x^{\ast}\right)-F\left(x^{\ast}\right)\\
%     &=\int_{\R^{d}}\left(F\left(x^{\ast}-y\right)-F\left(x^{\ast}\right)\right)\rho_{r}\left(y\right)\diff y\\
%     &\le \Delta\left(x^{\ast};R\right),
% \end{align*}
% and thus
% \begin{align*}
%     \min_{x\in\R^{d}}F_{R}\left(x\right)-F^{\ast}&\le \min_{x^{\ast}\in\argmin\{F(x)\}}\Delta\left(x^{\ast};R\right).
% \end{align*}

% As \citet{raginsky2017non}, we also assume as follows: 
% \begin{itemize}
%     \item[(A1)] The function $U$ takes nonnegative real values, and there exist constants $A,B\ge 0$ such that
%     \begin{align*}
%         \sup_{x:|x|\le 1}\left|U\left(x\right)\right|\le A.
%     \end{align*}
%     \item[(A3)] The gradient $\nabla U(x)$ of $U(x)$ is in $\FMC$.
%     \item[(A4)] $U(\cdot)$ is $(m,b)$-dissipative with $m,b>0$; that is, 
%     \begin{align*}
%         \langle x,\nabla U (x)\rangle \ge m|x|^{2}-b.
%     \end{align*}
%     \item[(A6)] The law $\mu_{0}$ of the initial value $\xi$ has the bounded and strictly positive density $p_{0}$ with respect to the Lebesgue measure on $\R^{d}$, and
%     \begin{align*}
%         \kappa_{0}:=\log\int_{\R^{d}}e^{|x|}p_{0}(x)\diff x<\infty.
%     \end{align*}
% \end{itemize}


% We define the uniform spectral gap
% \begin{align*}
%     \Bar{\lambda}_{\ast}:=\inf_{\data\in\mathsf{Z}^{n}}\inf\left\{\frac{\int_{\R^{d}}|\nabla g|^{2}\diff \Bar{\pi}^{r}}{\int g^{2}\diff \Bar{\pi}^{r}}:g\in\mathcal{C}^{1}\left(\R^{d}\right)\cap L^{2}(\Bar{\pi}^{r}),g\neq0,\int_{R^{d}}g\diff \Bar{\pi}^{r}=0\right\},
% \end{align*}
% where $\Bar{\pi}^{r}$ is the invariant probability measure of the following Langevin dynamics

% Note that the bound for $\Bar{\lambda}_{\ast}$ via the discussion of \citet{raginsky2017non} varies under different $R$.

% \begin{remark}
%     Without assuming Lipschitz or flat minima, We cannot bound $\omega_{\nabla U(\cdot)}/r$ with a constant and make $\omega_{\nabla U}$ to be convergent: let us consider $\alpha$-H\"{o}lder case.
%     $\omega_{\nabla U}(r)=MR^{\alpha}$, and then $\omega_{\nabla U}(r)/r=MR^{\alpha-1}$.
%     If for some $\epsilon>0$, we consider $R^{\alpha-1}<\epsilon^{-1}$, then $R>\epsilon^{1-\alpha}$.
%     This yields that $L >M\epsilon^{\alpha-\alpha^{2}}$.
% \end{remark}

% The next result is immediately obtained as a corollary of Theorem 3.6 of \citealp{XCZ+18}.
% \begin{theorem}
%     It holds true that
%     \begin{align*}
%         \E\left[F(Y_{K})\right]-F^{\ast}&\le \min_{x^{\ast}\in\argmin\{F(x)\}}\max_{x\in\R^{d}:|x-x^{\ast}|\le R}\left(F(x)-F(x^{\ast})\right) \\
%         &\quad+C_{1}\Gamma K\eta \left(\frac{\beta(n-B)\left(M\sqrt{\Gamma}+G\right)^{2}}{B(n-1)}+L\right)^{1/2}\\
%         &\quad+\Theta e^{-\lambda K\eta}+\frac{C_{\psi}\eta}{\beta}+\frac{d}{2\beta}\log\left(\frac{eM(b\beta/d+1)}{m}\right),
%     \end{align*}
%     where $G$, $M$, $b$, $m$, $\beta$, $\Gamma$, $\Theta$, and $\lambda$ are the problem-dependent parameters such that
%     \begin{align*} 
%         G&:=\max_{i=1,\ldots,n}|\nabla f_{i}(x^{\ast})|+\frac{bM}{m}\\
%         M&:=\frac{(d+4)L}{R}\\
%         m&:=\frac{\Tilde{m}}{2}\\
%         b&:=\Tilde{b}+R^{2}\\
%         \Gamma&:=2\left(1+\frac{1}{m}\right)\left(b+2G^{2}+\frac{d}{\beta}\right)\\
%         \Theta&:=\frac{C_{0}M\left(b\beta+m\beta+d\right)\left(m+e^{m\eta}M(b\beta+m\beta+d)\right)}{m^{2}\rho^{d/2}},\\
%         \lambda&:=\frac{2m\rho^{d}}{\log(2M(b\beta+m\beta+d)/m)},
%     \end{align*}
%     $C_{1}>0$ is an absolute constant, $C_{\psi}>0$ is a constant dependent on the $F_{n}$ and $\beta$, and $\rho\in(0,1),C_{0}>0$ are constants.
% \end{theorem}

% \begin{proof}
%     We just extend Lemma 4.4 of \citet{XCZ+18} and give the first term of the upper bound; other terms are just descendants of \citet{XCZ+18} and the evaluations for the smoothed loss functions above.
% \end{proof}

% \begin{theorem}[error estimate of LMC]\label{thm:ergodic}
%     It holds that
%     \begin{align*}
%         \Wasserstein_{2}\left(\mu_{k\eta},\pi\right)\le \inf_{r\in(0,1]}&\left(4C_{1}\max\left\{f(r,k,\eta)^{\frac{1}{2}},f(r,k,\eta)^{\frac{1}{4}}\right\}+C_{1}^{\prime}(R)e^{-k\eta/c_{\rm LS}\left(\Bar{\pi}^{r}\right)}\right)
%     \end{align*}
%     where
%     \begin{align*}
%         f(r,k,\eta):=\left(C_{0}\frac{\omega_{\nabla U}(r)}{r}\eta+\frac{\beta}{2}(\omega_{\nabla U}(r))^{2}\right)k\eta+\beta \left(2r\left\|\nabla U\right\|_{\FMC}+\omega_{\nabla U}(r)\sqrt{\frac{b+d/\beta}{m}}\right)
%     \end{align*}
% \end{theorem}

% \begin{theorem}[a risk bound with the minimax flatness]\label{thm:risk}
%     Assume that $\beta\ge 2/m$.
%     It holds that
%     \begin{align*}
%         \E\left[F(X_{k}^{\rm SGLD})\right]-F^{\ast}&\le \min_{x^{\ast}\in\R^{d}}\max_{x\in\R^{d}:|x-x^{\ast}|\le R}F\left(x\right) -F^{\ast}\\
%         &\quad+Cd^{2}\left(\frac{\omega_{\nabla U}(r)}{r}\right)^{2}\left(\sqrt{\beta}\sqrt{\beta+d}+\frac{d\omega_{\nabla U}(r)}{R}\right)\left(\beta+d\right)\\
%         &\qquad\times\left(\frac{d\omega_{\nabla U}(r)}{\beta R}+\frac{\left(\beta+d\right)}{\lambda_{\ast}}\right)\left(\left(\delta^{1/4}+\beta^{1/8}(\omega_{\nabla U}(r))^{1/4}\right)\log\frac{1}{\epsilon}+\epsilon\right)\\
%         &\quad+C\left(R+\omega_{\nabla U}(r)\sqrt{1+d/\beta}\right)+C\frac{\left(d\omega_{\nabla U}(r)\right)^{2}\left(\beta+d\right)^{2}}{R^{2}\Bar{\lambda}_{\ast}n}\\
%         &\quad+C\frac{d\log( \frac{d\omega_{\nabla U}(r)}{R}\left(\beta+1\right))}{\beta}.
%     \end{align*}
%     % \begin{align*}
%     %     \E\left[F(X_{k})\right]-F^{\ast}&\le \min_{x^{\ast}\in\argmin\{F(x)\}}\max_{x\in\R^{d}:|x-x^{\ast}|\le R}\left(F(x)-F(x^{\ast})\right) \\
%     %     &\quad+CM^{2}\left(\sqrt{\beta}\sqrt{\beta+d}+M\right)\left(\beta+d\right)\left(\frac{M}{\beta}+\frac{\left(\beta+d\right)}{\lambda_{\ast}}\right)\\
%     %     &\qquad\times\left(\left(\delta^{1/4}+\beta^{1/8}(\omega_{\nabla U}(r))^{1/4}\right)\log\frac{1}{\epsilon}+\epsilon\right)\\
%     %     &\quad+C\left(R+\omega_{\nabla U}(r)\sqrt{1+d/\beta}\right)+C\frac{M^{2}\left(\beta+d\right)^{2}}{\lambda_{\ast}n}+C\frac{d\log M\left(\beta+1\right)}{\beta}.
%     % \end{align*}
%     where $C$ is independent of $\beta,d,R,\Bar{\lambda}_{\ast},\delta,\epsilon$.
% \end{theorem}
% \subsection{Discussion with continuous gradients}

% \subsubsection{Lipschitz-continuous gradients}

% \subsubsection{H\"{o}lder-continuous gradients}
% We need to bound $M=L/R$ from the above and this leads that $R\le \epsilon^{1-\alpha}$.

% We set partition functions $\Bar{\mathcal{Z}}^{r}(\beta):=\int\exp(-\beta F_{R}(x))\diff x$ and $\mathcal{Z}(\beta):=\exp(-\beta F(x))\diff x$.
% Let $\Pi_{\beta,R}(\diff x)=\Bar{\mathcal{Z}}^{r}(\beta)^{-1}\exp(-\beta F_{R}(x))\diff x$ and $\Pi_{\beta}\left(\diff x\right)=\mathcal{Z}(\beta)^{-1}\exp(-\beta F(x))\diff x$, which are the invariant probability measures of the $R$-smoothed Langevin and the original one respectively.

% Note that $\nabla\left(F\ast \rho_{r}\right)=\left(\nabla U\right)\ast \rho_{r}$ owing to differentiability of $F$ and local boundedness of $F$.
% Thus, the corresponding Gibbs distribution is $\exp\left(-\beta F\ast\rho_{r}\right)$.

% Thus if the log-Sobolev inequality holds with some good constant,
% \begin{align*}
%     \Wasserstein_{2}\left(\Pi_{\beta},\Pi_{\beta,R}\right)\le \sqrt{2c\alpha D\left(\Pi_{\beta}\|\Pi_{\beta,R}\right)}
% \end{align*}
% by the Otto--Villani theorem.

% \begin{remark}
% If we assume the convexity of $F$, then the bounds are more trivial.
% For example, by letting $\rho(x)=\mu(|x|)$ for some $\mu:[0,\infty)\to[0,\infty)$ with $\mu(t)=0$, $t\ge1$, we obtain
% \begin{align*}
%     \mathcal{Z}\left(\beta,R\right)&=\int_{\R^{d}}\exp\left(-\beta \int_{\R^{d}}F\left(y\right)\rho_{r}\left(x-y\right)\diff y\right)\diff x\\
%     &\le \int_{\R^{d}}\exp\left(-\beta F\left(\int_{\R^{d}}y\rho_{r}\left(x-y\right)\diff y\right)\right)\diff x \\
%     &= \int_{\R^{d}}\exp\left(-\beta F\left(x\right)\right)\diff x \\
%     &\le \mathcal{Z}\left(\beta\right)
% \end{align*}
% because $F\left(\int_{\R^{d}}y\rho_{r}\left(x-y\right)\diff y\right)\le \int_{\R^{d}}F\left(y\right)\rho_{r}\left(x-y\right)\diff y$
% and $\int_{\R^{d}}y\rho_{r}\left(x-y\right)\diff y=x$.
% % as
% % \begin{align*}
% %     \int_{\R^{d}}y\rho_{r}\left(x-y\right)\diff y&=x-\int_{\R^{d}}\left(x-y\right)\rho_{r}\left(x-y\right)\diff y\\
% %     &=x-\int_{\R^{d}}y\mu_{R}\left(|y|\right)\diff y\\
% %     &=x.
% % \end{align*}
% This leads to
% \begin{align*}
%     D\left(\Pi_{\beta}|\Pi_{\beta,R}\right)&\le \beta \left(R\left|\nabla U\left(\zero\right)\right|+2LR+L\Pi_{\beta}\left(|\cdot|\right)\right).
% \end{align*}
% \end{remark}

% \subsubsection{A corollary for $\alpha$-weakly smooth potentials with $\alpha>1/3$}

% We assume that $\omega_{\nabla U}(r)=R^{\alpha}$ for some $\alpha\in(0,1)$.
% Then the $f(r,k,\eta)$ has the form
% \begin{align*}
%     f(r,k,\eta)\le C(R^{\alpha-1}\eta+R^{2\alpha})k\eta+CR^{\alpha}
% \end{align*}
% and $C_{1}^{\prime}\le C(R^{(\alpha-1)/2})$ and $c_{\rm LS}\le C(R^{\alpha-1})$.
% Then we have the upper bound
% \begin{align*}
%     \Wasserstein_{2}\left(\mu_{k\eta},\pi\right)&\lesssim_{C}\sqrt[4]{R^{\alpha-1}k\eta^{2}+R^{2\alpha}k\eta}+R^{(\alpha-1)/2}e^{-k\eta/CR^{\alpha-1}}
% \end{align*}
% if we let $k\eta=2\log(3\epsilon)^{-1}R^{\alpha-1}$, the last term is smaller than $\epsilon/3$.
% Thus
% \begin{align*}
%     \Wasserstein_{2}\left(\mu_{k\eta},\pi\right)&\lesssim_{C}\sqrt[4]{R^{\alpha-1}2\log(3\epsilon)^{-1}R^{\alpha-1}\eta+R^{2\alpha}2\log(3\epsilon)^{-1}R^{\alpha-1}}+\epsilon/3\\
%     &\le \sqrt[4]{2\log(3\epsilon)^{-1}R^{2\alpha-2}\eta}+\sqrt[4]{2\log(3\epsilon)^{-1}R^{3\alpha-1}}+\epsilon/3
% \end{align*}
% then we fix $R^{3\alpha-1}=\epsilon^{4}/(3^{4}2\log(3\epsilon)^{-1})$.
% We obtain
% \begin{align*}
%     \Wasserstein_{2}\left(\mu_{k\eta},\pi\right)&\lesssim_{C} \sqrt[4]{2\log(3\epsilon)^{-1}R^{2\alpha-2}\eta}+2\epsilon/3
% \end{align*}
% finally, we fix $\eta=\epsilon^{4}/(3^{4}2\log(3\epsilon)^{-1}R^{2\alpha-2})$ and then
% \begin{align*}
%     \Wasserstein_{2}\left(\mu_{k\eta},\pi\right)&\lesssim_{C}\epsilon.
% \end{align*}

% Let us compute the complexity of sampling.
% \begin{align*}
%     R&=\left(\frac{\epsilon^{4}}{162\log(3\epsilon)^{-1}}\right)^{\frac{1}{3\alpha-1}}\\
%     \eta&=\frac{\epsilon^{4}}{162\log(3\epsilon)^{-1}}\left(\frac{\epsilon^{4}}{162\log(3\epsilon)^{-1}}\right)^{\frac{2-2\alpha}{3\alpha-1}}=\left(\frac{\epsilon^{4}}{162\log(3\epsilon)^{-1}}\right)^{\frac{1+\alpha}{3\alpha-1}}\\
%     k&=2\log(3\epsilon)^{-1}\left(\frac{\epsilon^{4}}{162\log(3\epsilon)^{-1}}\right)^{\frac{-\alpha-1}{3\alpha-1}+\frac{\alpha-1}{3\alpha-1}}=2\log(3\epsilon)^{-1}\left(\frac{\epsilon^{4}}{162\log(3\epsilon)^{-1}}\right)^{\frac{-2}{3\alpha-1}}\\
%     &=2\left(\log(3\epsilon)^{-1}\right)^{\frac{3\alpha+1}{3\alpha-1}}\epsilon^{\frac{-8}{3\alpha-1}}\left(162\right)^{\frac{2}{3\alpha-1}}
% \end{align*}

\subsection{The spherically smoothed stochastic gradient Langevin Monte Carlo (SS-SG-LMC) method}
We consider a stochastic gradient $G$ unbiased for $\nabla \Bar{U}_{r}$ with fixed $r\in(0,1]$ such that the sampling error can be sufficiently small.

Note that $\rho$ is the density of random variables which we can generate as a product of random variables following the uniform distribution on $S^{d-1}=\{x\in\R^{d}:|x|=1\}$ and the beta distribution $\textrm{Beta}(d/2,4)$.
Therefore, we can consider spherical smoothing with the random variables whose density is $\rho_{r}$ as an analogue to Gaussian smoothing of \citet{chatterji2020langevin}.

Within this section, we assume that the potential is the sum of non-negative measurable functions $U_{i}:\R^{d}\to[0,\infty)$ with $i=1,\ldots,N$ and $N\in\N$ such that
\begin{align*}
    U(x)=\sum_{i=1}^{N}U_{i}\left(x\right).
\end{align*}
We set the stochastic gradient 
\begin{align*}
    {G}\left(x,a_{i\eta}\right)=\frac{N}{N_{B}}\sum_{j=1}^{N_{B}}\nabla U_{\lambda_{i,j}}\left(x+r'\zeta_{i,j}\right),
\end{align*}
where $(N_{B},r')\in\N\times(0,1]$, $a_{i\eta}=[\lambda_{i,1},\ldots,\lambda_{i,N_{B}},\zeta_{i,1},\ldots,\zeta_{i,N_{B}}]$, $\{\lambda_{i,j}\}$ is a sequence of i.i.d.~random variables with the discrete uniform distribution on the integers $1,\ldots,N$, and $\{\zeta_{i,j}\}$ is a sequence of i.i.d.~random variables with the density $\rho$ and independence of $\{\lambda_{i,j}\}$.
Then we clearly see that for any $x\in\R^{d}$,
\begin{align*}
    \E\left[{G}(x,a_{i\eta})\right]=\nabla \Bar{U}_{r'}(x),\ \E\left[\left|{G}\left(x,a_{i\eta}\right)-\nabla \Bar{U}_{r'}(x)\right|^{2}\right]\le \frac{(\omega_{\nabla U}(r'))^{2}}{N_{B}},
\end{align*}
and (A5) holds if $\nabla \Bar{U}_{r'}(x)$ is well-defined and $\omega_{\nabla U}(r')<\infty$ exists.

The main idea is to let $r'=r$, where $r$ is the radius of the implicit mollification and $r'$ is the radius of the support of the random noises which we control.
Hence, the stochastic gradient $G$ with $r'=r$ is an unbiased estimator of the mollified gradient $\nabla \Bar{U}_{r}(x)$.
We call the algorithm with this $G$ the spherically smoothed stochastic gradient Langevin Monte Carlo (SS-SG-LMC) algorithm.
We can control the sampling error of SS-SG-LMC to be close to zero by taking a sufficiently small $r$.
Note that SS-SG-LMC is also useful to reduce computational complexity if $N\gg N_{B}$.

Let us set the following assumptions.
\begin{enumerate}
    \item[(C1)] For all $i=1,\ldots,N$, $U_{i}\in W_{\loc}^{1,1}(\R^{d})$ and $|U_{i}(x)|<\infty$ for each $x\in\R^{d}$.
    \item[(C2)] $|\nabla U_{i}(\zero)|<\infty$ for all $i=1,\ldots,N$ and there exists a function $\Hat{\omega}:[0,\infty)\to[0,\infty)$ such that for all $r>0$ and $i=1,\ldots,N$,
    \begin{align*}
        \sup_{x,y\in\R^{d}:|x-y|\le r}\left|U_{i}(x)-U_{i}(y)\right|\le \frac{1}{N}\Hat{\omega}\left(r\right)<\infty.
    \end{align*}
    \item[(C3)] There exist $m,b>0$ such that for all $x\in\R^{d}$,
    \begin{align*}
        \left\langle x,\sum_{i=1}^{N}\nabla U_{i}\left(x\right)\right\rangle &\ge m\left|x\right|^{2}-b.
    \end{align*}
\end{enumerate}

We confirm that (C1)--(C3) yield (A1)--(A5).
(A1) and (A2) immediately hold by (C1).
(C2) yields $|\nabla \Bar{U}_{r}(\zero)|\le |\nabla U(\zero)|+\omega_{\nabla U}(1)$, $\omega_{\nabla U}(1)<\infty$, and $\omega_{\nabla \Bar{U}_{r}}(1)\le 3\omega_{\nabla U}(1)<\infty$; therefore, (A3) holds true.
(A4) also holds since 
\begin{align*}
    \left\langle x,\nabla \Bar{U}_{r}\left(x\right)\right\rangle&\ge \frac{1}{2}m|x|^{2}-m-b;
\end{align*}
Section \ref{sec:proof} gives the detailed derivation of this inequality.
(A5) is given by (C2) and the discussion above.

\begin{corollary}[error estimate of SS-SG-LMC]
    Under (C1)--(C3) and (A6), there exists a constant $C\ge 1$ independent of $N_{B},r,k,\eta,d$ such that for all $k\in\N$, $\eta\in(0,1\wedge m/(2(\Hat{\omega}(1))^{2}))$, $r\in(0,1]$, and $N_{B}\in\N$ with $((\Hat{\omega}(r)/r)\eta+(\Hat{\omega}(r))^{2}/N_{B})k\eta+r\sqrt{d}\le 1$, 
    \begin{align*}
        \Wasserstein_{2}\left(\mu_{k\eta},\pi\right)\le C\sqrt{d}\sqrt[4]{\left(d^{2}\frac{\Hat{\omega}(r)}{r}\eta+\frac{(\Hat{\omega}(r))^{2}}{N_{B}}\right)k\eta+r\sqrt{d}}+Cde^{-k\eta/(C(\Hat{\omega}(r)/r)d^{3}e^{Cd})}.
    \end{align*}
\end{corollary}

Letting $N=1$, we also obtain a non-asymptotic bound for errors of the spherically smoothed Langevin Monte Carlo (SS-LMC) algorithm with general $U=U_{1}$ satisfying (C1)--(C3) and $G$ such that
\begin{align*}
    G\left(x,a_{i\eta}\right)=\frac{1}{N_{B}}\sum_{j=1}^{N_{B}}\nabla U\left(x+r'\zeta_{i,j}\right)
\end{align*}
with $(N_{B},r')\in\N\times(0,1]$.


\subsubsection{The sampling complexity of SS-SG-LMC}
We analyse the behaviour of SS-SG-LMC; to see that we do not need the convergence $\Hat{\omega}(r)\downarrow 0$, we consider a rough version of the upper bound by replacing $\Hat{\omega}(r)$ with the constant $\Hat{\omega}(1)$.
\begin{corollary}
Under (C1)--(C3) and (A6), there exists a constant $C\ge1$ independent of $N_{B},r,k,\eta,d$ such that for all $N_{B}\in\N$, $k\in\N$, $\eta\in(0,1\wedge m/(2(\Hat{\omega}(1))^{2})$, and $r\in(0,1]$ with $\left(d^{2}r^{-1}\eta+N_{B}^{-1}\right)k\eta+r\sqrt{d}\le 1$,
\begin{align}\label{eq:asymp:s3glmc}
    \Wasserstein_{2}\left(\mu_{k\eta},\pi\right)&\le C\sqrt{d}\sqrt[4]{\left(d^{2}r^{-1}\eta+N_{B}^{-1}\right)k\eta+r\sqrt{d}}+Cd\exp\left(-\frac{k\eta}{Cr^{-1}d^{3}e^{Cd}}\right).
\end{align}
\end{corollary}

We yield the following estimate of the sampling complexity.
\begin{proposition}
    Assume (C1)--(C3) and (A6) and fix $\epsilon\in(0,1]$.
    If $k\in\N$ satisfies
    \begin{align*}
        k&\ge \frac{(48)^{4}C^{18}d^{35/2}e^{2Cd}(\log(2Cd/\epsilon))^{2}}{\epsilon^{16}},
    \end{align*}
    then $\Wasserstein_{2}\left(\mu_{k\eta},\pi\right)\le \epsilon$ for $\eta\in(0,1],N_{B}\in\N,r\in(0,1]$ such that
    \begin{align*}
        \eta=\sqrt{\frac{\epsilon^{4}}{16C^{4}d^{4}k}}\wedge \frac{m}{2(\Hat{\omega}(1))^{2}},\ N_{B}=\frac{48C^{4}d^{2}}{\epsilon^{4}k\eta},\ r=\frac{\epsilon^{4}}{48C^{4}d^{5/2}}.
    \end{align*}
\end{proposition}

\begin{proof}
    By noting the inequalities
\begin{align*}
%     r&=\frac{\epsilon^{4}}{48C^{4}d^{5/2}},\\
%     \eta&=\sqrt{\frac{r\epsilon^{4}}{48C^{4}d^{4}k}}=\frac{1}{\sqrt{k}}\frac{\epsilon^{4}}{48C^{4}d^{13/4}},\\
    k\eta&\ge \left(\frac{48C^{4}d^{5/2}}{\epsilon^{4}}\right)Cd^{3}e^{Cd}\log(2Cd/\epsilon),\\
    k&\ge \left(\frac{48C^{4}d^{13/4}}{\epsilon^{4}}\right)^{2}\left(\frac{48C^{4}d^{5/2}}{\epsilon^{4}}\right)^{2}C^{2}d^{6}e^{2Cd}(\log(2Cd/\epsilon))^{2},
% B&=\frac{48C^{4}d^{2}}{\epsilon^{4}k\eta}
\end{align*}
the statement immediately holds.
\end{proof}


\section{Preliminary results}
We give the preliminary results on the compact polynomial mollifier, the mollification of the functions with the finite moduli of continuity, and the representation of the likelihood ratio between the solutions of SDEs via the Liptser--Shiryaev theory.
We also introduce the fundamental theorem of calculus for weakly differentiable functions, the well-known sufficient conditions of functional inequalities, and upper bounds for Wasserstein distances.

\subsection{The fundamental theorem of calculus for weakly differentiable functions}

We use the following known result on the fundamental theorem of calculus for functions in $W_{\loc}^{1,1}(\R^{d})$.

\begin{proposition}[\citealp{lieb2001analysis,anastassiou2009distributional}]\label{prop:fundamental}
    For each $f\in W_{\loc}^{1,1}(\R^{d})$, for almost all $x,y\in\R^{d}$,
    \begin{align}\label{eq:FTC}
        f(y)-f(x)=\int_{0}^{1}\left\langle \nabla f\left(x+t\left(y-x\right)\right),y-x\right\rangle \diff t.
    \end{align}
\end{proposition}

\subsection{Properties of the compact polynomial mollifier}\label{sec:mollifier}
We analyse the mollifier $\rho$ proposed in Eq.~\eqref{eq:mollifier}.
Note that our non-asymptotic analysis needs the mollifiers of class $\mathcal{C}^{2}(\R^{d})$ whose gradients have explicit $L^{1}$-bounds and whose supports are in the unit ball of $\R^{d}$, and well-known mollifiers of class $\mathcal{C}^{\infty}$ do not have explicit $L^{1}$-bounds for the gradients in general.
% The mollifier $\rho:\R^{d}\to[0,\infty)$ with the hopeful properties as follows:
% \begin{align*}
%     \rho(x)=\begin{cases}
%         \left(\frac{\pi^{d/2}\Beta(d/2,4)}{\Gamma(d/2)}\right)^{-1}\left(1-|x|^{2}\right)^{3}&\text{if }|x|\le 1,\\
%         0 & \text{otherwise,}
%     \end{cases}
% \end{align*}
% where $\Beta(\cdot,\cdot)$ is the beta function.
% This is a special case of compact polynomial mollifiers introduced by \citet{anderson2014compact}.

\begin{remark}
    We need mollifiers of class $\mathcal{C}^{2}$ to give the bound for the constant of a logarithmic Sobolev inequality by \citet{cattiaux2010note}; see Proposition \ref{prop:CGW10T12}.
\end{remark}

The following lemma gives some properties of $\rho$.

\begin{lemma}\label{lem:kernel:2}
    (1) $\rho\in\mathcal{C}^{2}(\R^{d})$, (2) $\int \rho(x)\diff x=1$, and (3) $\int |\nabla \rho(x)|\diff x\le d+4$.
\end{lemma}

\begin{proof}
    (1) We check the behaviours of $\nabla \rho$ and $\nabla^{2}\rho$ on a neighbourhood of $\{x\in\R^{d}:|x|=1\}$.
    For all $x$ with $|x|<1$,
    \begin{align*}
        \nabla \rho(x)&=\left(\frac{\pi^{d/2}\Beta(d/2,4)}{\Gamma(d/2)}\right)^{-1}\left(-6\right)\left(1-|x|^{2}\right)^{2}x,\\
        \nabla^{2} \rho(x)&=\left(\frac{\pi^{d/2}\Beta(d/2,4)}{\Gamma(d/2)}\right)^{-1}\left(-6\left(1-|x|^{2}\right)^{2}I_{d}+24\left(1-|x|^{2}\right)xx^{\top}\right),
    \end{align*}
    and thus $\nabla \rho(x)$ and $\nabla^{2} \rho(x)$ are continuous for any $x\in\R^{d}$ by $\nabla \rho(x)=\zero$ and $\nabla^{2} \rho(x)=O$ for all $x$ with $|x|=1$.
    
    (2) We have
    \begin{align*}
        \int \rho(x)\diff x&=\frac{2}{\Beta(d/2,4)}\int_{0}^{1} r^{d-1}\left(1-r^{2}\right)^{3}\diff r=\frac{1}{\Beta(d/2,4)}\int_{0}^{1} s^{d/2-1}\left(1-s\right)^{3}\diff s=1
    \end{align*}
    with the change of the coordinate from the Euclid one to the hyperspherical one, and the change of variable such that $\sqrt{s}=r$ and $(1/2\sqrt{s})\diff s=\diff r$.

    (3) With respect to the $L^{1}$-norm of the gradient, it holds
    \begin{align*}
        \int |\nabla \rho(x)|\diff x&=\int_{|x|\le 1}\left(\frac{\pi^{d/2}\Beta(d/2,4)}{\Gamma(d/2)}\right)^{-1}6\left(1-|x|^{2}\right)^{2}|x|\diff x\\
        &=\frac{12}{\Beta(d/2,4)}\int_{0}^{1}r^{d}\left(1-r^{2}\right)^{2}\diff r\\
        &=\frac{6}{\Beta(d/2,4)}\int_{0}^{1}s^{d/2-1/2}\left(1-s\right)^{2}\diff s\\
        &=\frac{6\Beta(d/2+1/2,3)}{\Beta(d/2,4)}\\
        &=\frac{6\Gamma(d/2+1/2)\Gamma(3)\Gamma(d/2+4)}{\Gamma(d/2+7/2)\Gamma(d/2)\Gamma(4)}\\
        % &=2\frac{\Gamma(d/2+4)}{\Gamma(d/2)}\frac{\Gamma(d/2+1/2)}{\Gamma(d/2+7/2)}\\
        &=\frac{(d+6)(d+4)(d+2)d}{(d+5)(d+3)(d+1)}\\
        &\le d+4
    \end{align*}
    because $(d+6)(d+2)d\le(d+5)(d+3)(d+1)$.
    Therefore, the statement holds true.
\end{proof}


We show the optimality of the compact polynomial mollifier; there is no continuously differentiable probability density function with support in $B_{1}(\zero)$ such that the $L^{1}$-norm of the score function is smaller than $d$.

\begin{lemma}
Assume that $p(x)$ is a probability density function whose support is in the unit ball of $\R^{d}$ and continuously differentiable in $\R^{d}$.
It holds that
\begin{align*}
    \int_{\R^{d}}\left|\nabla p(x)\right|\diff x\ge d.
\end{align*}
\end{lemma}

\begin{proof}
    Since $p(x)\in \mathcal{C}^{1}$, the $L^{1}$-norm of the gradient equals the total variation: for arbitrary $R> 1$,
    \begin{align*}
        \int_{\R^{d}}\left|\nabla p(x)\right|\diff x&=\int_{B_{R}\left(\zero\right)}\left|\nabla p(x)\right|\diff x\\
        &=\sup\left\{\int_{B_{R}\left(\zero\right)}p(x)\mathrm{div}\varphi(x)\diff x\left|\varphi\in\mathcal{C}_{0}^{1}\left(B_{R}\left(\zero\right);\R^{d}\right),\left\|\varphi\right\|_{\infty}\le 1\right.\right\},
    \end{align*}
    where $\mathcal{C}_{0}^{1}(B_{R}(\zero),\R^{d})$ is a class of continuously differentiable functions $\varphi:B_{R}(\zero)\to\R^{d}$ with compact supports in $B_{R}(\zero)$, and $\|\varphi\|_{\infty}=\sup_{x}|\varphi(x)|$.
    For all $\delta\in(0,1]\cap \Q$, by fixing $\varphi_{\delta}(x)\in\mathcal{C}_{0}^{1}(B_{R}(\zero),\R^{d})$ such that $\varphi_{\delta}(x)=x$ for all $x\in B_{1-\delta}(\zero)$ and $\|\varphi_{\delta}\|_{\infty}\le 1$, we have
    \begin{align*}
        \int_{\R^{d}}\left|\nabla p(x)\right|\diff x&\ge \int_{B_{R}\left(\zero\right)}p(x)\mathrm{div}\varphi_{\delta}(x)\diff x=\int_{B_{1}\left(\zero\right)}p(x)\mathrm{div}\varphi_{\delta}(x)\diff x\ge d\int_{B_{1-\delta}(\zero)}p(x)\diff x.
    \end{align*}
    The monotone convergence theorem gives the conclusion.
\end{proof}


\subsection{Functions with the finite moduli of continuity and their properties}
We consider a class of possibly discontinuous functions and show lemmas useful for the analysis of SG-LMC such that $\nabla U$ and $\widetilde{G}$ are in this class.

Let $\FMC=\FMC(\R^{d};\R^{\ell})$ with fixed $d,\ell\in\N$ denote a class of measurable functions $\phi:(\R^{d},\Borel(\R^{d}))\to(\R^{\ell},\Borel(\R^{\ell}))$ with (1) $|\phi(\zero)|<\infty$ and (2) $\omega_{\phi}(1)<\infty$, where $\omega_{\phi}(\cdot)$ is the well-known modulus of continuity defined as
\begin{align*}
    \omega_{\phi}(r):=\sup_{x,y\in\R^{d}:|x-y|\le r}\left|\phi(x)-\phi(y)\right|,
\end{align*}
where $r>0$.
Note that we use the modulus of continuity not to measure the continuity of $\phi$, but to measure the fluctuation of $\phi$ within $\Bar{B}_{r}(x)$ for all $x\in\R^{d}$.
An intuitive element of $\FMC$ is $\indicator_{A}$ for an arbitrary measurable set $A\in\Borel(\R^{d})$ because $\omega_{\indicator_{A}}(r)\le 1$ for any $A$ and $r>0$.
In the rest of the paper, we sometimes use the notation $\|\phi\|_{\FMC}:=|\phi(\zero)|+\omega_{\phi}(1)$ with $\phi\in\FMC$ just for brevity; it is easy to see that $\FMC$ equipped with $\|\cdot\|_{\FMC}$ is a Banach space.

We introduce the following lemma: this ensures that we can change $r>0$ arbitrarily if $\omega_{\phi}(r)<\infty$ with some $r>0$, and reveal that considering $r=1$ is sufficient to capture the large-scale behaviour.
\begin{lemma}\label{lem:FMC:representation}
    For any $r>0$ and $\phi\in\FMC$, $\omega_{\phi}(r)=\sup_{t>0}\lceil t\rceil^{-1}\omega_{\phi}(tr)$.
\end{lemma}

\begin{proof}
    $\omega_{\phi}(r)\le \sup_{t>0}\lceil t\rceil^{-1}\omega_{\phi}(rt)$ and $\omega_{\phi}(r)\ge \lceil t\rceil^{-1}\omega_{\phi}(rt)$ with $t\in(0,1]$ hold immediately.
    Thus we only examine $\omega_{\phi}(r)\ge \lceil t\rceil^{-1}\omega_{\phi}(rt)$ for all $t>1$. 

    We fix $t>1$.
    For any $x,y\in \R^{d}$ with $|x-y|\le rt$,
    \begin{align*}
        \left|\phi\left(x\right)-\phi\left(y\right)\right|&\le \sum_{i=1}^{\lceil t\rceil}\left|\phi\left(\frac{(\lceil t\rceil-i+1)x+(i-1)y}{\lceil t\rceil}\right)-\phi\left(\frac{(\lceil t\rceil-i)x+iy}{\lceil t\rceil}\right)\right|\\
        &\le \lceil t\rceil \omega_{\phi}(r)
    \end{align*}
    because $|((\lceil t\rceil-i+1)x+(i-1)y)/\lceil t\rceil-((\lceil t\rceil-i)x+iy)/\lceil t\rceil|=|x-y|/\lceil t\rceil\le r$.
\end{proof}


\begin{remark}
    Note that continuity and boundedness of the modulus of continuity do not imply each other.
    For example, $f(x)=x\sin x$ with $x\in\R$ is a continuous function without the finite modulus of continuity.
    On the other hand, $f(x)=\indicator_{\Q}\left(x\right)$ with $x\in\R$ is a trivial example of a function with the finite modulus of continuity and without continuity.

    Moreover, continuity along with the boundedness of the modulus of continuity does not imply uniform continuity, which we can easily observe by $f(x)=\sin(x^{2})$ with $x\in\R$.
\end{remark}


\begin{itembox}[l]{The chains of implications}
\centering
\begin{tikzcd}
        (\textrm{bounded }f) \arrow[dr,Rightarrow] 
        & (\textrm{Lipschitz }f\ast\rho_{r}) 
        & \\
        (\textrm{uniformly continuious }f) \arrow[d,Rightarrow] \arrow[r,Rightarrow] 
        & (\textrm{bounded } \omega_{f}(r)) \arrow[r,Rightarrow] \arrow[u,Rightarrow] \arrow[d,Rightarrow] 
        & (f \textrm{ of linear growth}) \\
        (\textrm{continuous } f)  
        & (\textrm{local Lipschitz }\int f) 
        & 
    \end{tikzcd}
\end{itembox}
% For convenience, we call $f\in\FMC_{L,R}$ as a function with $(L,R)$-stable linear growth.

% Clearly uniform continuity is a sufficient condition for steadiness, and boundedness is another one.

%All $\phi\in\FMC$ have the following properties.

\begin{lemma}[linear growth of functions with the finite moduli of continuity]\label{lem:FMC:linearGrowth}
    For any $\phi\in\FMC(\R^{d};\R^{\ell})$, it holds that for all $x\in\R^{d}$,
    \begin{align*}
        \left|\phi\left(x\right)\right| \le \left|\phi(\zero)\right|+\omega_{\phi}(1)+\omega_{\phi}(1)|x|.
    \end{align*}
\end{lemma}

\begin{proof}%[Proof of Lemma \ref{lem:FMC:linearGrowth}]
    Fix $x\in\R^{d}$.
    Lemma \ref{lem:FMC:representation} gives
    \begin{align*}
        \left|\phi\left(x\right)\right|-\left|\phi(\zero)\right|\le \left|\phi\left(x\right)-\phi(\zero)\right|\le \omega_{\phi}(|x|)\le \lceil |x|\rceil\omega_{\phi}(1)\le (1+|x|)\omega_{\phi}(1).
    \end{align*}
    Therefore, the statement holds.
\end{proof}


\begin{lemma}[local Lipschitz continuity by gradients with the finite moduli of continuity]\label{lem:FMC:localLipschitz}
    Assume that $\Phi\in W_{\loc}^{1,1}(\R^{d})$ and a representative weak gradient $\nabla \Phi$ is in $\FMC(\R^{d};\R^{d})$.
    It holds that for almost all $x,y\in\R^{d}$,
    \begin{align*}
        \left|\Phi\left(x\right)-\Phi\left(y\right)\right|\le \left(\left|\nabla\Phi(\zero)\right|+\omega_{\nabla\Phi}(1)\left(1+\frac{\left|x\right|+\left|y\right|}{2}\right)\right)\left|y-x\right|.
    \end{align*}
    In addition, for almost all $x\in\R^{d}$ and $r>0$,
    \begin{align*}
        \left|\Phi(x)-\Bar{\Phi}_{r}(x)\right|\le r\left(\left|\nabla \Phi\left(\zero\right)\right|+\omega_{\nabla\Phi}(1)\left(1+\frac{r}{2}+\frac{\left|x\right|}{2}\right)\right)
    \end{align*}
    with $\Bar{\Phi}_{r}(x)=(\Phi\ast\rho_{r})(x)$.
\end{lemma}

\begin{proof}%[Proof of Lemma \ref{lem:FMC:localLipschitz}]
    Proposition \ref{prop:fundamental} and Lemma \ref{lem:FMC:linearGrowth} yield that for almost all $x,y\in\R^{d}$,
    \begin{align*}
        |\Phi(x)-\Phi(y)|&= \left|\int_{0}^{1}\left\langle\nabla \Phi\left(x+t(y-x)\right),y-x\right\rangle\diff t\right|\\
        &\le \int_{0}^{1}\left|\nabla \Phi\left(x+t(y-x)\right)\right|\diff t\left|y-x\right|\\
        &\le \int_{0}^{1}\left(\left|\nabla \Phi\left(\zero\right)\right|+\omega_{\nabla \Phi}\left(1\right)\left(1+\left|(1-t)x+ty\right|\right)\right)\diff t\left|y-x\right|\\
        &\le \left(\left|\nabla \Phi\left(\zero\right)\right|+\omega_{\nabla\Phi}(1)\left(1+\frac{|x|+|y|}{2}\right)\right)\left|y-x\right|.
    \end{align*}
    Hence we obtain the first statement.
    The second statement also holds because for almost all $x\in\R^{d}$ and all $r>0$,
    \begin{align*}
        \left|\Phi(x)-\Bar{\Phi}_{r}(x)\right|&=\left|\Phi(x)-\int_{\R^{d}}\Phi(y)\rho_{r}(x-y)\diff y\right|\\
        &=\left|\int_{\R^{d}}\left(\Phi(x)-\Phi(y)\right)\rho_{r}(x-y)\diff y\right|\\
        &\le \left(\left|\nabla \Phi\left(\zero\right)\right|+\omega_{\nabla\Phi}(1)\left(1+\frac{|x|+r}{2}\right)\right)\int_{\R^{d}}\left|x-y\right|\rho_{r}(x-y)\diff y\\
        &\le r\left(\left|\nabla \Phi\left(\zero\right)\right|+\omega_{\nabla\Phi}(1)\left(1+\frac{|x|+r}{2}\right)\right).
        %&\le r\left(\left|\nabla \Phi\left(\zero\right)\right|+\left(1+r\right)\omega_{\nabla\Phi}(1)\right)+r\omega_{\nabla\Phi}(1)|x|.
    \end{align*}
    We obtain the conclusion.
\end{proof}

\begin{lemma}[quadratic growth by gradients with the finite moduli of continuity]\label{lem:FMC:quadGrowth}
    Assume that $\Phi\in W_{\loc}^{1,1}(\R^{d})$ and a representative weak gradient $\nabla \Phi$ is in $\FMC(\R^{d};\R^{d})$.
    It holds that for almost all $x\in\R^{d}$ 
    %Under the same assumptions as Lemma \ref{lem:FMC:localLipschitz}, for almost all $x\in\R^{d}$,
    \begin{align*}
        \Phi\left(x\right)&\le \frac{\omega_{\nabla \Phi}(1)}{2}\left|x\right|^{2}+\left(\left|\nabla \Phi\left(\zero\right)\right|+\frac{3}{2}\omega_{\nabla \Phi}(1)\right)\left|x\right|+\left\|\Phi\right\|_{L^{\infty}\left(B_{1}\left(\zero\right)\right)}.
    \end{align*}
    and for all $x\in\R^{d}$ and $r\in(0,1]$,
    \begin{align*}
        \Bar{\Phi}_{r}(x)&\le \frac{\omega_{\nabla \Phi}(1)}{2}\left|x\right|^{2}+\left(\left|\nabla \Phi\left(\zero\right)\right|+\frac{5}{2}\omega_{\nabla \Phi}(1)\right)\left|x\right|+\left\|\Phi\right\|_{L^{\infty}\left(B_{1}\left(\zero\right)\right)}
    \end{align*}
    with $\Bar{\Phi}_{r}(x)=(\Phi\ast\rho_{r})(x)$.
\end{lemma}

\begin{proof}
    Lemma \ref{lem:FMC:localLipschitz} gives that for almost all $x\in\R^{d}$ and $y \in B_{1}(\zero)\cap B_{|x|}(x)$,
    \begin{align*}
        \Phi\left(x\right)&\le \frac{\omega_{\nabla \Phi}(1)}{2}\left|x\right|\left|x-y\right|+\left(\left|\nabla \Phi\left(\zero\right)\right|+\frac{3}{2}\omega_{\nabla \Phi}(1)\right)\left|x-y\right|+\Phi\left(y\right)\\
        &\le \frac{\omega_{\nabla \Phi}(1)}{2}\left|x\right|^{2}+\left(\left|\nabla \Phi\left(\zero\right)\right|+\frac{3}{2}\omega_{\nabla \Phi}(1)\right)\left|x\right|+\left\|\Phi\right\|_{L^{\infty}\left(B_{1}\left(\zero\right)\right)}.
    \end{align*}
    % Proposition \ref{prop:fundamental} and Lemma \ref{lem:FMC:linearGrowth} yield that for almost all $x\in\R^{d}$ and $y \in B_{1}(\zero)\cap B_{|x|}(x)$,
    % \begin{align*}
    %     \Phi(x)&%=\Phi(y)+\int_{0}^{1}\left\langle \nabla \Phi(y+t(x-y)),x-y\right\rangle\diff t\\
    %     %&\le 
    %     \le\Phi(y)+\int_{0}^{1}\left|\nabla \Phi(y+t(x-y))\right|\left|x-y\right|\diff t\\
    %     &\le \left\|\Phi\right\|_{L^{\infty}(B_{1}(\zero))}+\int_{0}^{1}\left(\left|\nabla \Phi\left(\zero\right)\right|+\omega_{\nabla \Phi}(1)\left(1+(1-t)|y|+t\left|x\right|\right)\right)\left|x-y\right|\diff t\\
    %     &\le \left\|\Phi\right\|_{L^{\infty}(B_{1}(\zero))}+\int_{0}^{1}\left(\left|\nabla \Phi\left(\zero\right)\right|+\omega_{\nabla \Phi}(1)\left(\frac{3}{2}+t\left|x\right|\right)\right)\left|x\right|\diff t\\
    %     &\le  \frac{\omega_{\nabla \Phi}(1)}{2}\left|x\right|^{2}+\left(\left|\nabla \Phi\left(\zero\right)\right|+\frac{3}{2}\omega_{\nabla \Phi}(1)\right)\left|x\right|+\left\|\Phi\right\|_{L^{\infty}(B_{1}(\zero))}.
    % \end{align*}
    Regarding the second statement, it holds that 
    \begin{align*}
        \Bar{\Phi}_{r}(x)&=\int_{\R^{d}} \Phi\left(x-y\right)\rho_{r}\left(y\right)\diff y\\
        &=\int_{\R^{d}} \left(\Phi\left(-y\right)+\int_{0}^{1}\left\langle\nabla \Phi\left(-y+t\left(x-y\right)\right),x\right\rangle \diff t\right)\rho_{r}\left(y\right)\diff y\\
        &\le \int_{\R^{d}} \left(\Phi\left(-y\right)+\int_{0}^{1}\left(\left|\nabla \Phi(\zero)\right|+\omega_{\nabla \Phi}\left(1\right)\left(1+\left|-(1+t)y+tx\right|\right)\right)\left|x\right|\diff t\right)\rho_{r}\left(y\right)\diff y\\
        &\le \left\|\Phi\right\|_{L^{\infty}(B_{1}(\zero))}+\int_{0}^{1}\left(\left|\nabla \Phi(\zero)\right|+\omega_{\nabla \Phi}\left(1\right)\left(1+(1+t)+t\left|x\right|\right)\right)\left|x\right|\diff t\\
        &\le \frac{\omega_{\nabla \Phi}(1)}{2}\left|x\right|^{2}+\left(\left|\nabla \Phi\left(\zero\right)\right|+\frac{5}{2}\omega_{\nabla \Phi}(1)\right)\left|x\right|+\left\|\Phi\right\|_{L^{\infty}(B_{1}(\zero))}.
    \end{align*}
    We obtain the conclusion.
\end{proof}


\begin{lemma}[bounded gradients of convolution]\label{lem:FMC:boundedConvolutedGradient}
For all $\phi\in\FMC(\R^{d};\R^{\ell})$, $r>0$, and $x\in\R^{d}$,
it holds that 
\begin{align*}
    \left\|\nabla \Bar{\phi}_{r}(x)\right\|_{2}\le \left(d+4\right)\frac{\omega_{\phi}(r)}{r},
\end{align*}
where $\Bar{\phi}_{r}(x)=(\phi\ast\rho_{r})(x)$.
\end{lemma}

\begin{proof}%[Proof of Lemma \ref{lem:FMC:boundedConvolutedGradient}]
We obtain
\begin{align*}
    \nabla \left(\Bar{\phi}_{r}\right)(x) %&= \nabla \int_{\R^{d}}f\left(y\right)\rho_{r}\left(x-y\right)\diff y\\
    % &=\lim_{h\to0} \frac{1}{h}\left[\int_{\R^{d}}\left(\nabla U\right)\left(y\right)\left(\rho_{r}\left(x-y-he_{1}\right)-\rho_{r}\left(x-y\right)\right)\diff y,\ldots\right]\\
    % &=\lim_{h\to0} \frac{1}{h}\left[\int_{\R^{d}}\left(\nabla U\right)\left(y\right)\left(\langle \nabla \rho_{r}\left(x-y-hc(h)e_{1}\right),he_{1}\rangle\right)\diff y,\ldots\right]\\
    &=\int_{\R^{d}}\phi\left(y\right)\left(\nabla \rho_{r}\right)\left(x-y\right)\diff y
    =\int_{\R^{d}}\left(\phi\left(y\right)-\phi\left(x\right)\right)\left(\nabla \rho_{r}\right)\left(x-y\right)\diff y
\end{align*}
by using $\int\nabla \rho_{r}(x)\diff x=0$, % mean value theorem and mollifier must have bounded derivatives
and thus
\begin{align*}
    \left\|\nabla \left(\Bar{\phi}_{r}\right)(x)\right\|_{2}&\le \int_{\R^{d}}\left\|\left(\phi\left(y\right)-\phi\left(x\right)\right)\left(\nabla \rho_{r}\right)\left(x-y\right)\right\|_{2}\diff y\\
    &= \int_{\R^{d}}\left|\phi\left(y\right)-\phi\left(x\right)\right|\left|\nabla \rho_{r}\left(x-y\right)\right|\diff y\\
    &\le \omega_{\phi}(r) \int_{\R^{d}}\left|\nabla \rho_{r}\left(y\right)\right|\diff y\\
    &= \omega_{\phi}(r)\int_{\R^{d}}\left| \frac{1}{r^{d+1}}\nabla\rho\left(\frac{y}{r}\right)\right|\diff y\\
    &= \omega_{\phi}(r)\int_{\R^{d}}\left|\frac{1}{r^{d+1}}\nabla \rho\left(z\right)\right|r^{d}\diff z\\
    &\le \frac{(d+4)\omega_{\phi}(r)}{r} 
\end{align*}
by change of the variables $z=y/r$ with $r^{d}\diff z=\diff y$ and the fact $\int\left|\nabla \rho\left(z\right)\right|\diff z\le d+4$ by Lemma \ref{lem:kernel:2}.
\end{proof}

% $\ell^{\infty}$ from van der Vaart and Wellner (1996)
\begin{lemma}[$1$-Lipschitz mapping to $\ell^{\infty}$]\label{lem:FMC:lipMap}
    For all $\phi\in\FMC(\R^{d};\R^{\ell})$ and $r>0$,
    \begin{align*}
        \left\|\phi\ast \rho_{r}-\phi\right\|_{\infty}\le \omega_{\phi}(r), %\le \left\|\phi\right\|_{\FMC}.
    \end{align*}
    where $\|f\|_{\infty}:=\sup_{x}|f(x)|$ for all $f:\R^{d}\to\R^{\ell}$.
\end{lemma}

\begin{proof}%[Proof of Lemma \ref{lem:FMC:lipMap}]
    Since $\int\rho_{r}(x)\diff x=1$, for all $x\in\R^{d}$, 
    \begin{align*}
        \left|\phi\ast\rho_{r}(x)-\phi(x)\right|&=\left|\int_{\R^{d}}\phi(y)\rho_{r}(x-y)\diff y-\phi(x)\right|\\
        &=\left|\int_{\R^{d}}\phi(y)\rho_{r}(x-y)\diff y-\int_{\R^{d}}\phi(x)\rho_{r}(x-y)\diff y\right|\\
        &=\left|\int_{\R^{d}}\left(\phi(y)-\phi(x)\right)\rho_{r}(x-y)\diff y\right|\\
        &\le \int_{\R^{d}}\left|\phi(y)-\phi(x)\right|\rho_{r}(x-y)\diff y\\
        &\le \omega_{\phi}(r).
    \end{align*}
    This is the desired conclusion.
\end{proof}

\subsection{Liptser--Shiryaev theory}

We show the existence of explicit likelihood ratios of diffusion-type processes based on Theorem 7.19 and Lemma 7.6 of \citet{liptser2001statistics}.
We fix $T>0$ throughout this section.
Let $(W_{T},\mathcal{W}_{T})$ be a measurable space of $\R^{d}$-valued continuous functions $w_{t}$ with $t\in[0,T]$ and $\mathcal{W}_{T}=\sigma(w_{s}:w\in W_{T},s\le T)$.
We also use the notation $\mathcal{W}_{t}=\sigma(w_{s}:w\in W_{T}, s\le t)$ for $t\in[0,T]$.
Let $(\Omega, \mathcal{F}, \mu)$ be a complete probability space and $(\Tilde{\Omega},\Tilde{\mathcal{F}},\Tilde{\mu})$ be its identical copy.
We assume that the filtration $\{\mathcal{F}_{t}\}_{t\in[0,T]}$ satisfies the usual conditions.
Let $(B_{t},\mathcal{F}_{t})$ with $t\in[0,T]$ be a $d$-dimensional Brownian motion and $\xi$ be a $\mathcal{F}_{0}$-measurable $d$-dimensional random vector.
We set $\{a_{t}\}_{t\in[0,T]}$, an $\mathcal{F}_{t}$-adapted random process whose trajectory is in some measurable space $(A_{T},\mathcal{A}_{T})$ and let $\mathcal{A}_{t}=\sigma(a_{s}:a\in A_{T},s\le t)$.
Assume that $a=\{a_{t}\}_{t\in[0,T]}$, $B=\{B_{t}\}_{t\in[0,T]}$, and $\xi$ are independent of each other.
$\mu_{a},\mu_{B}$, and $\mu_{\xi}$ denote the probability measures induced by $a,B,\xi$ on $(A_{T},\mathcal{A}_{T})$, $(W_{T},\mathcal{W}_{T})$, and $(\R^{d},\Borel(\R^{d}))$ respectively.

Consider the solutions $X^{P}=\{X_{t}^{P}\}_{t\in[0,T]}$ and $X^{Q}=\{X_{t}^{Q}\}_{t\in[0,T]}$ of the following SDEs:
\begin{align}
    \diff X_{t}^{P}&=b^{P}\left(t,a,X^{P}\right)\diff t+\sqrt{2\beta^{-1}}\diff B_{t},\ X_{0}^{P}=\xi,\label{eq:LS:P}\\
    \diff X_{t}^{Q}&=b^{Q}\left(X_{t}^{Q}\right)\diff t+\sqrt{2\beta^{-1}}\diff B_{t},\ X_{0}^{Q}=\xi.\label{eq:LS:Q}
\end{align}

We set the following assumptions, partially adapted from \citet{liptser2001statistics} but containing some differences in $\xi$ and the structure of $X^{Q}$.
\begin{itemize}
    \item[(LS1)] $X_{t}^{P}$ is a strong solution of the equation \eqref{eq:LS:P}, that is, there exists a measurable functional $F_{t}$ for each $t$ such that 
    \begin{align*}
        X_{t}^{P}(\omega)=F_{t}(a(\omega),B(\omega),\xi(\omega))
    \end{align*}
    $\mu$-almost surely.
    \item[(LS2)] The functional $b^{P}$ is non-anticipative, that is, $\mathcal{A}_{t}\times\mathcal{W}_{t}$-measurable for each $t\in[0,T]$, and for fixed $a\in A_{T}$ and $w\in W_{T}$,
    \begin{align*}
        \int_{0}^{T}\left|b^{P}(t,a,w)\right|\diff t<\infty.
    \end{align*}
    \item[(LS3)] $b^{Q}:\R^{d}\to\R^{d}$ is Lipschitz continuous, so that $X^{Q}$ is the unique strong solution of the equation \eqref{eq:LS:Q}.
    \item[(LS4)] It holds that
    \begin{align*}
        &\mu\left(\int_{0}^{T}\left(\left|b^{P}\left(t,a,X^{P}\right)\right|^{2}+\left|b^{Q}\left(X_{t}^{P}\right)\right|^{2}\right)\diff t<\infty\right)\\
        &=\mu\left(\int_{0}^{T}\left(\left|b^{P}\left(t,a,X^{Q}\right)\right|^{2}+\left|b^{Q}\left(X_{t}^{Q}\right)\right|^{2}\right)\diff t<\infty\right)=1.
    \end{align*}
\end{itemize}

We consider a variant of \eqref{eq:LS:P} with fixed $a\in A_{T}$:
\begin{align*}
    \diff X_{t}^{P|a}=b^{P}\left(t,a,X^{P|a}\right)\diff t+\sqrt{2\beta^{-1}}\diff B_{t},\ X_{0}^{P|a}=\xi.
\end{align*}
Then the assumption (LS1) yields that 
\begin{align}\label{eq:LS:cond}
    X_{t}^{P|a}(\omega)=F_{t}(a,B(\omega),\xi(\omega))
\end{align}
$\mu_{a}\times \mu$-almost surely.
We assume that $\Omega=A_{T}\times W_{T}\times \R^{d}$, $\mathcal{F}=\mathcal{A}_{T}\times \mathcal{W}_{T}\times\Borel(\R^{d})$, and $\mu=\mu_{a}\times\mu_{B}\times\mu_{\xi}$ without loss of generality.
Then each $\omega\in\Omega$ has the form $\omega=(a,B,\xi)$ and we can assume that $a,B$, and $\xi$ are projections such as $a(\omega)=a$, $B(\omega)=B$, and $\xi(\omega)=\xi$; therefore, the equality \eqref{eq:LS:cond} holds $\mu_{a}\times\mu_{B}\times\mu_{\xi}$-almost surely.

We consider a process on the product space $(\Omega\times\Tilde{\Omega},\mathcal{F}\times\Tilde{\mathcal{F}},\mu\times \Tilde{\mu})$:
\begin{align*}
    \diff X_{t}^{P|a(\omega)}(\Tilde{\omega})=b^{P}\left(t,a(\omega),X^{P|a(\omega)}(\Tilde{\omega})\right)\diff t+\sqrt{2\beta^{-1}}\diff B_{t}(\Tilde{\omega}),\ X_{0}^{P|a(\omega)}=\xi(\Tilde{\omega}).
\end{align*}
(LS1) gives that 
\begin{align*}
    X_{t}^{P|a(\omega)}(\Tilde{\omega})=F_{t}\left(a(\omega),B(\Tilde{\omega}),\xi(\Tilde{\omega})\right),
\end{align*}
$\mu\times\Tilde{\mu}$-almost surely.

\begin{lemma}[\citealp{liptser2001statistics}]\label{lem:LS:L0705}
    Under (LS1), for any $C\in\mathcal{W}_{T}$,
    \begin{align*}
        \mu\left(X^{P}(a,B,\xi)\in C|\sigma(a)\right)=\Tilde{\mu}\left(X^{P}\left(a,\Tilde{B},\Tilde{\xi}\right)\left(\Tilde{\omega}\right)\in C\right)
    \end{align*}
    $\mu$-almost surely.
\end{lemma}

\begin{proof}
    The proof is essentially identical to that of Lemma 7.5 of \citet{liptser2001statistics} except for the randomness of $\xi$.
    We first show that for fixed $t\in[0,T]$ and $C_{t}\in\Borel(\R^{d})$,
    \begin{align*}
        \mu\left(F_{t}(a,B,\xi)\in C_{t}|\sigma(a)\right)=\Tilde{\mu}\left(F_{t}\left(a,\Tilde{B},\Tilde{\xi}\right)\in C_{t}\right)
    \end{align*}
    $\mu$-almost surely.
    Note that the following probability for fixed $a$ is $\mathcal{A}_{T}$-measurable owing to (LS1) and Fubini's theorem:
    \begin{align*}
        \Tilde{\mu}\left(F_{t}\left(a,\Tilde{B},\Tilde{\xi}\right)\in C_{t}\right)=\left(\mu_{B}\times\mu_{\xi}\right)\left(F_{t}(a,B,\xi)\in C_{t}\right).
    \end{align*}
    Let $f(a(\omega))$ be a $\sigma(a)$-measurable bounded random variable.
    Again Fubini's theorem gives that
    \begin{align*}
        \E\left[f(a(\omega))\indicator_{F_{t}(a,B,\xi)\in C_{t}}\right]&=\int_{A_{T}}\int_{W_{T}}\int_{\R^{d}}f(a)\indicator_{F_{t}(a,w,x)\in C_{t}}\mu_{a}(\diff a)\mu_{B}(\diff w)\mu_{\xi}(\diff x)\\
        &=\int_{A_{T}}f(a)\left(\int_{W_{T}}\int_{\R^{d}}\indicator_{F_{t}(a,w,x)\in C_{t}}\mu_{B}(\diff w)\mu_{\xi}(\diff x)\right)\mu_{a}(\diff a)\\
        &=\int_{A_{T}}f(a)\left(\mu_{B}\times \mu_{\xi}\right)\left(F_{t}(a,B,\xi)\in C_{t}\right)\mu_{a}(\diff a)\\
        &=\int_{A_{T}}f(a)\Tilde{\mu}\left(F_{t}\left(a,\Tilde{B},\Tilde{\xi}\right)\in C_{t}\right)\mu_{a}(\diff a)\\
        &=\E\left[f(a)\Tilde{\mu}\left(F_{t}\left(a,\Tilde{B},\Tilde{\xi}\right)\in C_{t}\right)\right]
    \end{align*}
    and thus the definition of conditional expectation yields the result.
    Similarly, we obtain that for all $n\in\N$, $0\le t_{1}<\cdots<t_{n}\le T$, and $C_{t_{i}}\in \Borel(\R^{d})$, $i=1,\ldots,n$,
    \begin{align*}
        &\mu\left(F_{t_{1}}(a,B,\xi)\in C_{t_{1}}\cdots F_{t_{n}}(a,B,\xi)\in C_{t_{n}}|\sigma(a)\right)\\
        &=\Tilde{\mu}\left(F_{t_{1}}\left(a,\Tilde{B},\Tilde{\xi}\right)\in C_{t_{1}},\cdots,F_{t_{n}}\left(a,\Tilde{B},\Tilde{\xi}\right)\in C_{t_{n}}\right).
    \end{align*}
    Therefore, the statement holds true.
\end{proof}

Let $P_{T}$ and $Q_{T}$ denote the laws of $\{(a_{t},X_{t}^{P});t\in[0,T]\}$ and $\{(a_{t},X_{t}^{Q});t\in[0,T]\}$.
Note that $a_{t}$ and $X_{t}^{Q}$ are independent of each other by the assumptions.
The following proposition gives the equivalence and the representation of the likelihood ratio.
\begin{proposition}[\citealp{liptser2001statistics}]\label{prop:LS01L0706}
    Under (LS1)--(LS4), it holds that
    \begin{align*}
        &\frac{\diff Q_{T}}{\diff P_{T}}\left(a,X^{P}\right)\\
        &=\exp\left(-\frac{\beta}{2}\int_{0}^{T}\left\langle \left(b^{P}-b^{Q}\right)\left(t,a,X^{P}\right),\diff B_{t}\right\rangle +\frac{\beta^{2}}{4}\int_{0}^{T}\left|\left(b^{P}-b^{Q}\right)\left(t,a,X^{P}\right)\right|^{2}\diff t\right).
    \end{align*}
\end{proposition}

\begin{proof}
It is quite parallel to the proof of Lemma 7.6 of \citet{liptser2001statistics}.
For arbitrary set $\Gamma=\Gamma_{1}\times \Gamma_{2}$, $\Gamma_{1}\in\mathcal{A}_{T}$ and $\Gamma_{2}\in\mathcal{W}_{T}$, by Lemma \ref{lem:LS:L0705},
\begin{align*}
    \mu\left(\left(a,X^{P}\right)\in\Gamma\right)&=\int_{A_{T}\times W_{T}\times \R^{d}}\indicator_{a\in \Gamma_{1}}\indicator_{X^{P}(a,w,x)\in\Gamma_{2}}\mu_{a}(\diff a)\mu_{B}\left(\diff w\right)\mu_{\xi}\left(\diff x\right)\\
    &=\int_{a\in \Gamma_{1}}\mu\left(X^{P}(a,B,\xi)\in \Gamma_{2}|\sigma(a)\right)\mu_{a}(\diff a)\\
    &=\int_{a\in \Gamma_{1}}\Tilde{\mu}\left(X^{P}\left(a,\Tilde{B},\Tilde{\xi}\right)\in \Gamma_{2}\right)\mu_{a}(\diff a)\\
    &=\int_{a\in \Gamma_{1}}\left(P|a\right)_{T}(\Gamma_{2})\mu_{a}(\diff a),
\end{align*}
where $(P|a)_{T}$ is the law of \eqref{eq:LS:cond}.
Let $(Q|a)_{T}$ denote the law of $X^{Q}$.
For $\mu_{a}$-almost all $a$, under (LS1)--(LS4) and Theorem 7.19 of \citet{liptser2001statistics}, $(P|a)_{T}\sim (Q|a)_{T}$ and the likelihood ratio is given as
\begin{align*}
    &\frac{\diff (P|a)_{T}}{\diff (Q|a)_{T}}\left(X^{Q}\right)\\
    &=\exp\left(\frac{\beta}{2}\int_{0}^{T}\left\langle \left(b^{P}-b^{Q}\right)\left(t,a,X^{Q}\right),\diff B_{t}\right\rangle -\frac{\beta^{2}}{4}\int_{0}^{T}\left|\left(b^{P}-b^{Q}\right)\left(t,a,X^{Q}\right)\right|^{2}\diff t\right).
\end{align*}
Therefore, we have
\begin{align*}
    \mu\left(\left(a,X^{P}\right)\in\Gamma\right)&=\int_{\Gamma_{1}}\int_{\Gamma_{2}}\left(\frac{\diff (P|a)_{T}}{\diff (Q|a)_{T}}(\diff w)(Q|a)_{T}(\diff w)\right)\mu_{a}(\diff a)\\
    &=\int_{\Gamma_{1}}\int_{\Gamma_{2}}\frac{\diff (P|a)_{T}}{\diff (Q|a)_{T}}(\diff w)\left(\mu_{a}\times (Q|a)_{T}\right)(\diff a \diff w)\\
    &=\int_{\Gamma}\frac{\diff (P|a)_{T}}{\diff (Q|a)_{T}}(\diff w)Q_{T}(\diff a \diff w).
\end{align*}
Since $Q_{T}(a,w:(\diff (P|a)_{T})/(\diff (Q|a)_{T})(w)=0)=0$, Lemma 6.8 of \citet{liptser2001statistics} yields the desired conclusion.
\end{proof}

We revisit the following result.
\begin{proposition}[Kullback--Leibler divergence]\label{prop:KL}
    Under (LS1)--(LS4), we obtain
    \begin{align*}
        D\left(P_{T}\left\|Q_{T}\right.\right)\le  \frac{\beta^{2}}{4}\E\left[\int_{0}^{T}\left|\left(b^{P}-b^{Q}\right)\left(s,a,X^{P}\right)\right|^{2}\diff s\right],
    \end{align*}
    where the expectation in the right-hand side is possibly infinite.
\end{proposition}

\begin{proof}
    Let $\{\sigma_{n}\}$ be a sequence of stopping times such that
    \begin{align*}
        \sigma_{n}=\inf\left\{t\ge0;\int_{0}^{t}\left|\left(b^{P}-b^{Q}\right)\left(s,a,X^{P}\right)\right|^{2}\diff s\ge  n\right\}.
    \end{align*}
    The non-negative property of the Kullback--Leibler divergence, Fatou's lemma, Proposition \ref{prop:LS01L0706}, Corollary 2.21 of \citet{karatzas1998brownian}, and the monotone convergence theorem give 
    \begin{align*}
        D\left(P_{T}\left\|Q_{T}\right.\right)&=\E\left[\log\frac{\diff P_{T}}{\diff Q_{T}}\left(a,X^{P}\right)\right]\\
        &=\E\left[\lim_{n\to\infty}\log\frac{\diff P_{T\wedge\sigma_{n}}}{\diff Q_{T\wedge\sigma_{n}}}\left(a,X^{P}\right)\right]\\
        &\le \liminf_{n\to\infty}\E\left[\log\frac{\diff P_{T\wedge\sigma_{n}}}{\diff Q_{T\wedge\sigma_{n}}}\left(a,X^{P}\right)\right]\\
        &=\liminf_{n\to\infty}\E\left[-\log\frac{\diff Q_{T\wedge\sigma_{n}}}{\diff P_{T\wedge\sigma_{n}}}\left(a,X^{P}\right)\right]\\
        &\le \liminf_{n\to\infty}\E\left[\frac{\beta^{2}}{4}\int_{0}^{T\wedge\sigma_{n}}\left|\left(b^{P}-b^{Q}\right)\left(s,a,X^{Q}\right)\right|^{2}\diff s\right]\\
        &\quad-\limsup_{n\to\infty}\E\left[\frac{\beta}{2}\int_{0}^{T\wedge\sigma_{n}}\left\langle \left(b^{P}-b^{Q}\right)\left(s,a,X^{P}\right),\diff B_{s}\right\rangle\right]\\
        &=\liminf_{n\to\infty}\frac{\beta^{2}}{4}\E\left[\int_{0}^{T\wedge\sigma_{n}}\left|\left(b^{P}-b^{Q}\right)\left(s,a,X^{P}\right)\right|^{2}\diff s\right]\\
        &=\frac{\beta^{2}}{4}\E\left[\int_{0}^{T}\left|\left(b^{P}-b^{Q}\right)\left(s,a,X^{P}\right)\right|^{2}\diff s\right].
    \end{align*}
    Hence we obtain the conclusion.
\end{proof}

\subsection{Functional inequalities}


Let us consider the Poincar\'{e} and the log-Sobolev inequalities of the probability measure $P_{\Phi}$ whose density is proportional to $e^{-\Phi}$ with $\Phi\in\mathcal{C}^{2}(\R^{d})$.
Let $L:=\Delta -\langle \nabla \Phi,\nabla \rangle$, which is $P_{\Phi}$-symmetric, $P_{t}$ be the Markov semigroup with the infinitesimal generator $L$, and $\mathcal{E}$ denote the Dirichlet form
\begin{align*}
    \mathcal{E}(g):=\lim_{t\to0}\frac{1}{t}\int_{\R^{d}}g \left(g-P_{t}g\right) \diff P_{\Phi},
\end{align*}
where $g\in L^{2}(P_{\Phi})$ such that the limit exists.
Here, we say that a probability measure $P_{\Phi}$ satisfies a Poincar\'{e} inequality with constant $c_{\rm P}(P_{\Phi})$ (the Poincar\'{e} constant) if for any $Q\ll P_{\Phi}$,
\begin{align*}
    \chi^{2}\left(Q\|P_{\Phi}\right)\le c_{\rm P}(P_{\Phi})\mathcal{E}\left(\sqrt{\frac{\diff Q}{\diff P_{\Phi}}}\right)
\end{align*}
where $\chi^{2}(Q\|P_{\Phi})=\int_{\R^{d}} |(\diff Q/\diff P_{\Phi})-1|^{2}\diff P_{\Phi}$,
and $P_{\Phi}$ satisfies a logarithmic Sobolev inequality with constant $c_{\rm LS}(P_{\Phi})$ (the log-Sobolev constant) if for any $Q\ll P_{\Phi}$,
\begin{align*}
    D\left(Q\|P_{\Phi}\right)\le 2c_{\rm LS}(P_{\Phi})\mathcal{E}\left(\sqrt{\frac{\diff Q}{\diff P_{\Phi}}}\right).
\end{align*}



We adopt the following statement from \citet{raginsky2017non}; although it is different to the original discussion of \citet{bakry2008simple}, the difference is negligible because Eq.~(2.3) of \citet{bakry2008simple} yields the same upper bound.
\begin{proposition}[\citealp{bakry2008simple}]\label{prop:BBCG08T14}
    Assume that there exists a Lyapunov function $V\in\mathcal{C}^{2}$ with $V:\R^{d}\to[1,\infty)$ such that
    \begin{align*}
        \frac{LV\left(x\right)}{V\left(x\right)}\le -\lambda_{0}+\kappa_{0}\indicator_{B_{\Tilde{R}}(\zero)}\left(x\right)
    \end{align*}
    for some $\lambda_{0}>0$, $\kappa_{0}\ge 0$ and $\Tilde{R}>0$, where $LV(x)=\Delta V-\langle \nabla \Phi,\nabla V\rangle$.
    Then $P_{\Phi}$ satisfies a Poincar\'{e} inequality with constant $c_{\rm P}(P_{\Phi})$ such that
    \begin{align*}
        c_{\rm P}(P_{\Phi})\le \frac{1}{\lambda_{0}}\left(1+a\kappa_{0}\Tilde{R}^{2}e^{\mathrm{Osc}_{\Tilde{R}}}\right),
    \end{align*}
    where $a>0$ is an absolute constant and $\mathrm{Osc}_{\Tilde{R}}:=\max_{x:|x|\le \Tilde{R}}\Phi(x)-\min_{x:|x|\le \Tilde{R}}\Phi(x)$.
\end{proposition}

The following proposition gives the estimate of the constant of logarithmic Sobolev inequalities.
\begin{proposition}[\citealp{cattiaux2010note}]\label{prop:CGW10T12}
    Assume the following conditions.
    \begin{enumerate}
        \item There exist positive constants $\kappa,\gamma>0$ and a Lyapunov function $V\in\mathcal{C}^{2}$ with $V:\R^{d}\to[1,\infty)$ such that for all $x\in\R^{d}$,
        \begin{align*}
            \frac{LV(x)}{V(x)}\le \kappa -\gamma \left|x\right|^{2}.
        \end{align*}
        \item $P_{\Phi}$ satisfies a Poincar\'{e} inequality with constant $c_{\rm P}(P_{\Phi})$.
        \item For some $K\ge0$, $\nabla^{2} \Phi\succeq -KI_{d}$.
    \end{enumerate}
    For arbitrary $\epsilon>0$, let $C_{1},C_{1}^{\prime}$ be constants such that
    \begin{align*}
        C_{1}:=\frac{2}{\gamma}\left(\frac{1}{\epsilon}+\frac{K}{2}\right)+\epsilon,\ C_{1}^{\prime}:=\frac{2}{\gamma}\left(\frac{1}{\epsilon}+\frac{K}{2}\right)\left(\kappa+\gamma\int_{\R^{d}}\left|x\right|P_{\Phi}\left(\diff x\right)\right).
    \end{align*}
    Then $P_{\Phi}$ satisfies a logarithmic Sobolev inequality with constant $$c_{\rm LS}\left(P_{\Phi}\right)=C_{1}+\left(C_{1}^{\prime}+2\right)c_{\rm P}(P_{\Phi}).$$
\end{proposition}
As \citet{raginsky2017non}, we can choose $\epsilon=2/K$ if $K\neq0$ and thus
\begin{align*}
    c_{\rm LS}\left(P_{\Phi}\right)=\frac{2K}{\gamma}+\frac{2}{K}+\left(\frac{2K}{\gamma}\left(\kappa+\gamma\int_{\R^{d}}\left|x\right|P_{\Phi}\left(\diff x\right)\right)+2\right)c_{\rm P}(P_{\Phi}).
\end{align*}

\subsection{A bound for the 2-Wasserstein distance by the Kullback--Leibler divergence}

The next proposition is an immediate result by \citet{bolley2005weighted}.
\begin{proposition}[\citealp{bolley2005weighted}]\label{prop:BV05C23}
    Let $\mu,\nu$ be probability measures on $(\R^{d},\Borel(\R^{d}))$.
    Assume that there exists a constant $\lambda>0$ such that $\int \exp(\lambda |x|^{2})\nu(\diff x)<\infty$.
    Then for any $\mu$, it holds that
    \begin{align*}
        \Wasserstein_{2}\left(\mu,\nu\right)\le C_{\nu}\left(D(\mu\|\nu)^{\frac{1}{2}}+\left(\frac{D(\mu\|\nu)}{2}\right)^{\frac{1}{4}}\right),
    \end{align*}
    where
    \begin{align*}
        C_{\nu}:=2\inf_{\lambda>0}\left(\frac{1}{\lambda}\left(\frac{3}{2}+\log\int_{\R^{d}}e^{\lambda\left|x\right|^{2}}\nu\left(\diff x\right)\right)\right)^{\frac{1}{2}}.
    \end{align*}
\end{proposition}

\section{Proof of the main theorem}\label{sec:proof}
In this section, we use the notations $\|\nabla U\|_{\FMC}:=|\nabla U(\zero)|+\omega_{\nabla U}(1)$ and $\|\widetilde{G}\|_{\FMC}:=|\widetilde{G}(\zero)|+\omega_{\widetilde{G}}(1)$.
Let $\Bar{\nu}_{t}^{r}$ denote the probability measure of $\Bar{X}_{t}^{r}$, the unique strong solution of the SDE under (A3) owing to Lemma \ref{lem:FMC:boundedConvolutedGradient}:
\begin{align}\label{eq:sLD}
    \diff \Bar{X}_{t}^{r}=-\nabla \Bar{U}_{r}\left(\Bar{X}_{t}^{r}\right)+\sqrt{2\beta^{-1}}\diff w_{t},\ \Bar{X}_{0}^{r}=\xi,
\end{align}
and define $\Bar{\pi}^{r}$, a probability measure on $(\R^{d},\Borel(\R^{d}))$, as
\begin{align*}
    \Bar{\pi}^{r}\left(\diff x\right)=\frac{1}{\Bar{\mathcal{Z}}^{r}\left(\beta\right)}\exp\left(-\beta \Bar{U}_{r}\left(x\right)\right)\diff x,
\end{align*}
where $\Bar{\mathcal{Z}}^{r}(\beta)=\int\exp(-\beta \Bar{U}_{r}(x))\diff x$.
We also let $\mathcal{Z}(\beta)=\int\exp(-\beta U(x))\diff x$ and then $\pi$ has the representation
\begin{align*}
    \pi\left(\diff x\right)=\frac{1}{\mathcal{Z}\left(\beta\right)}\exp\left(-\beta U\left(x\right)\right)\diff x.
\end{align*}
Note that $\Bar{U}_{r}$ is $(\Bar{m},\Bar{b})$-dissipative with $\Bar{m}:=m/2,\Bar{b}:=b+m$ by $r\le 1$ as 
\begin{align*}
    \left\langle x,\nabla \Bar{U}_{r}\left(x\right)\right\rangle&=\int\left\langle x,\nabla U\left(x-y\right)\right\rangle \rho_{r}(y)\diff y\\
    &\ge \int\left(m|x-y|^{2}-b\right) \rho_{r}(y)\diff y\\
    &\ge \int\left(\frac{1}{2}m|x|^{2}-m|y|^{2}-b\right) \rho_{r}(y)\diff y\\
    &\ge \frac{1}{2}m|x|^{2}-mr^{2}-b.
\end{align*}


\subsection{Moments of SG-LMC algorithms}

\begin{lemma}[square integrability]\label{lem:RRT17L03}
    Assume that (A1)--(A6) hold.
    (1) For all $0<\eta<1\wedge (\Tilde{m}/2((\omega_{\widetilde{G}}(1))^{2}+\delta_{\variance,2}))$,
    \begin{align*}
        \sup_{k\ge 0}\E\left[\left|Y_{k\eta}\right|^{2}\right]&\le \kappa_{0}+2\left(1\vee \frac{1}{\Tilde{m}}\right)\left(\Tilde{b}+\eta\left\|\widetilde{G}\right\|_{\FMC}^{2}+\delta_{\variance,0}+\frac{d}{\beta}\right)=:\kappa_{\infty}. %\left(b+\left\|\nabla U\right\|_{\FMC}^{2}+\frac{d}{\beta}\right).
    \end{align*}
    (2) It holds that for any $t\ge0$, 
    \begin{align*}
        \E\left[\left|X_{t}\right|^{2}\right]&\le \kappa_{0}e^{-2mt}+\frac{b+d/\beta}{m}\left(1-e^{-2mt}\right),
    \end{align*}
    and for any $r\in(0,1]$,
    \begin{align*}
        \E\left[\left|\Bar{X}_{t}^{r}\right|^{2}\right]&\le \kappa_{0}e^{-mt}+\frac{2\left(b+m+d/\beta\right)}{m}\left(1-e^{-mt}\right).
    \end{align*}
\end{lemma}

\begin{proof}%[Proof of Lemma \ref{lem:RRT17L03}]
    The proof is partially adapted from Lemma 3 of \citet{raginsky2017non}.
    
    (1) The independence among $Y_{k\eta}$, $a_{k\eta}$, and $B_{(k+1)\eta}-B_{k\eta}$ leads to
    \begin{align*}
        \E\left[\left|Y_{(k+1)\eta}\right|^{2}\right]&=\E\left[\left|Y_{k\eta}-\eta{G}\left(Y_{k\eta},a_{k\eta}\right)+\sqrt{2\beta^{-1}}\left(B_{(k+1)\eta}-B_{k\eta}\right)\right|^{2}\right]\\
        &=\E\left[\left|Y_{k\eta}-\eta{G}\left(Y_{k\eta},a_{k\eta}\right)\right|^{2}\right]+\frac{2}{\beta}\E\left[\left|B_{(k+1)\eta}-B_{k\eta}\right|^{2}\right]\\
        &=\E\left[\left|Y_{k\eta}-\eta\widetilde{G}\left(Y_{k\eta}\right)+\eta\widetilde{G}\left(Y_{k\eta}\right)-\eta{G}\left(Y_{k\eta},a_{k\eta}\right)\right|^{2}\right]+\frac{2\eta d}{\beta}\\
        &=\E\left[\left|Y_{k\eta}-\eta \widetilde{G}(Y_{k\eta})\right|^{2}\right]+\eta^{2}\E\left[\left|\widetilde{G}(Y_{k\eta})-{G}\left(Y_{k\eta},a_{k\eta}\right)\right|^{2}\right]+\frac{2\eta d}{\beta}.
    \end{align*}
    Lemma \ref{lem:FMC:linearGrowth} gives
    \begin{align*}
        &\E\left[\left|Y_{k\eta}-\eta \widetilde{G}(Y_{k\eta})\right|^{2}\right]\\
        &=\E\left[\left|Y_{k\eta}\right|^{2}\right]-2\eta\E\left[\left\langle Y_{k\eta}, \widetilde{G}(Y_{k\eta})\right\rangle \right]+\eta^{2}\E\left[\left|\widetilde{G}(Y_{k\eta})\right|^{2}\right]\\
        &\le \E\left[\left|Y_{k\eta}\right|^{2}\right]+2\eta\left(\Tilde{b}-\Tilde{m}\E\left[\left|Y_{k\eta}\right|^{2}\right]\right)+2\eta^{2}\left(\left\|\widetilde{G}\right\|_{\FMC}^{2}+\left(\omega_{\widetilde{G}}(1)\right)^{2}\E\left[\left|Y_{k\eta}\right|^{2}\right]\right)\\
        &=\left(1-2\eta \Tilde{m}+2\eta^{2}\left(\omega_{\widetilde{G}}(1)\right)^{2}\right)\E\left[\left|Y_{k\eta}\right|^{2}\right]+2\eta \Tilde{b}+2\eta^{2}\left\|\widetilde{G}\right\|_{\FMC}^{2}.
    \end{align*}
    By Assumption (A5) and the independence between $a_{k\eta}$ and $Y_{k\eta}$, we also have
    \begin{align*}
        \E\left[\left|\widetilde{G}(Y_{k\eta})-{G}\left(Y_{k\eta},a_{k\eta}\right)\right|^{2}\right]\le 2\delta_{\variance,2}\E\left[\left|Y_{k\eta}\right|^{2}\right]+2\delta_{\variance,0}.
    \end{align*}
    Hence it holds that
    \begin{align*}
        &\E\left[\left|Y_{(k+1)\eta}\right|^{2}\right]\\
        &\le \left(1-2\eta \Tilde{m}+2\eta^{2}\left(\left(\omega_{\widetilde{G}}(1)\right)^{2}+\delta_{\variance,2}\right)\right)\E\left[\left|Y_{k\eta}\right|^{2}\right]+2\eta \Tilde{b}+2\eta^{2}\left\|\widetilde{G}\right\|_{\FMC}^{2}+2\eta^{2}\delta_{\variance,0}+\frac{2\eta d}{\beta}.
    \end{align*}
    If $1-2\eta \Tilde{m}+2\eta^{2}((\omega_{\widetilde{G}}(1))^{2}+\delta_{\variance,2})\le 0$, then it is obvious that
    \begin{align*}
        \E\left[\left|Y_{(k+1)\eta}\right|^{2}\right]\le 2\Tilde{b}+2\left\|\widetilde{G}\right\|_{\FMC}^{2}+2\delta_{\variance,0}+\frac{2d}{\beta},
    \end{align*}
    and if $1-2\eta \Tilde{m}+2\eta^{2}((\omega_{\widetilde{G}}(1))^{2}+\delta_{\variance,2})\in(0,1)$,
    \begin{align*}
        \E\left[\left|Y_{k\eta}\right|^{2}\right]&\le \left(1-2\eta \Tilde{m}+2\eta^{2}\left(\left(\omega_{\widetilde{G}}(1)\right)^{2}+\delta_{\variance,2}\right)\right)^{k}\E\left[\left|Y_{0}\right|^{2}\right]\\
        &\quad+\frac{2\eta \Tilde{b}+2\eta^{2}\left\|\widetilde{G}\right\|_{\FMC}^{2}+2\eta^{2}\delta_{\variance,0}+\frac{2\eta d}{\beta}}{2\eta \Tilde{m}-2\eta^{2}\left(\left(\omega_{\widetilde{G}}(1)\right)^{2}+\delta_{\variance,2}\right)^{2}}\\
        &\le \E\left[\left|Y_{0}\right|^{2}\right]+\frac{2\Tilde{b}+2\eta\left\|\widetilde{G}\right\|_{\FMC}^{2}+2\eta \delta_{\variance,0}+\frac{2d}{\beta}}{2\Tilde{m}-2\eta\left(\left(\omega_{\widetilde{G}}(1)\right)^{2}+\delta_{\variance,2}\right)}\\
        &\le \E\left[\left|Y_{0}\right|^{2}\right]+\frac{2}{\Tilde{m}}\left(\Tilde{b}+\eta\left\|\widetilde{G}\right\|_{\FMC}^{2}+\delta_{\variance,0}+\frac{d}{\beta}\right).
    \end{align*}
    Jensen's inequality yields $\E\left[\left|\xi\right|^{2}\right]\le \log\E\left[\exp\left|\xi\right|^{2}\right]\le \kappa_{0}$.

    (2) We just give the proof for the $X_{t}$ because that for $\Bar{X}_{t}^{r}$ is parallel.
    Let $\{B'_{t}\}$ be a $d$-dimensional Brownian motion such that $(X_{t},B_{t}')$ with some stochastic basis is the unique weak solution of Eq.~\eqref{eq:LD}. 
    It\^{o}'s formula yields
    \begin{align*}
        e^{2mt}\left|X_{t}\right|^{2}&=\left|X_{0}\right|^{2}+\int_{0}^{t}\left(e^{2ms}\left\langle -\nabla U\left(X_{s}\right),2X_{s}\right\rangle+e^{2ms}\frac{2d}{\beta}+2me^{2ms}\left|X_{s}\right|^{2}\right)\diff s\\
        &\quad+\sqrt{2\beta^{-1}}\int_{0}^{t}e^{2ms}\left\langle 2X_{s},\diff B_{s}'\right\rangle.
    \end{align*}
    The dissipativity and the martingale property of the last term lead to
    \begin{align*}
        \E\left[\left|X_{t}\right|^{2}\right]&=e^{-2mt}\E\left[\left|X_{0}\right|^{2}\right]\\
        &\quad+2\int_{0}^{t}e^{2m(s-t)}\left(\E\left[\left\langle -\nabla U\left(X_{s}\right),X_{s}\right\rangle+m\left|X_{s}\right|^{2}\right]+\frac{d}{\beta}\right)\diff s\\
        &\le e^{-2mt}\E\left[\left|X_{0}\right|^{2}\right]+2\int_{0}^{t}e^{2m(s-t)}\left(\E\left[-m\left|X_{s}\right|^{2}+b+m\left|X_{s}\right|^{2}\right]+\frac{d}{\beta}\right)\diff s\\
        &= e^{-2mt}\E\left[\left|X_{0}\right|^{2}\right]+\frac{b+d/\beta}{m}\left(1-e^{-2mt}\right).
    \end{align*}
    Jensen's inequality yields $\E\left[\left|\xi\right|^{2}\right]\le \log\E\left[\exp\left|\xi\right|^{2}\right]\le \kappa_{0}$ and thus we obtain the conclusion.
\end{proof}

\begin{lemma}[exponential integrability of mollified Langevin dynamics]\label{lem:LD:exponential}
    Assume (A1)--(A6).
    For all $r\in(0,1]$ and $\alpha\in(0,\beta \Bar{m})$ such that $\E[\exp(\alpha|\xi|^{2})]<\infty$, 
    \begin{align*}
        \E\left[\exp\left(\alpha |\Bar{X}_{t}^{r}|^{2}\right)\right]&\le  \E\left[e^{\alpha |\xi|^{2}}\right]e^{-2\alpha \left(\Bar{b}+d/\beta\right)t}+2 \exp\left(\frac{2\alpha(\Bar{b}+d/\beta)}{\Bar{m}-\alpha/\beta}\right)\left(1- e^{-2\alpha \left(\Bar{b}+d/\beta\right)t}\right).
    \end{align*}
    In particular, for $\alpha=1\wedge (\beta \Bar{m}/2)$,
    \begin{align*}
        \sup_{t\ge0}\log\E\left[e^{\alpha|\Bar{X}_{t}^{r}|^{2}}\right]&\le \log\left(e^{\alpha\kappa_{0}}+ 2e^{4\alpha(\Bar{b}+d/\beta)/\Bar{m}}\right)\le \alpha\kappa_{0}+\frac{4\alpha(\Bar{b}+d/\beta)}{\Bar{m}}+1.
    \end{align*}
\end{lemma}

\begin{remark}
    We obtain the same result for $X_{t}$ by replacing $\Bar{m},\Bar{b}$ with $m,b$.
\end{remark}

\begin{proof}%[Proof of Lemma \ref{lem:LD:exponential}]
    Let $V_{2}(x):=\exp(\alpha|x|^{2})$.
    Note that
    \begin{align*}
        \nabla V_{2}(x)= 2\alpha V_{2}(x)x,\ 
        \nabla^{2} V_{2}(x)=2\alpha^{2} V_{2}(x)xx^{\top}+2\alpha V_{2}(x)I_{d}.
    \end{align*}
    Let $\Bar{\mathcal{L}}^{r}$ denote the extended generator of $\Bar{X}_{t}^{r}$ such that $\Bar{\mathcal{L}}^{r}f:=\beta^{-1}\Delta f-\langle \nabla \Bar{U}_{r}, \nabla f\rangle$ for $f\in\mathcal{C}^{2}$.
    It holds that
    \begin{align*}
        \Bar{\mathcal{L}}^{r}V_{2}(x)&\le 2\alpha V_{2}(x)\left\langle \nabla \Bar{U}_{r}(x),x\right\rangle +\alpha/\beta V_{2}(x)\left(\alpha\left|x\right|^{2}+d\right)\\
        &\le 2\alpha V_{2}(x)\left(\left(-\Bar{m}\left|x\right|^{2}+\Bar{b}\right) +\left(\alpha\left|x\right|^{2}+d\right)/\beta\right)\\
        &=2\alpha V_{2}(x)\left(\left(\alpha/\beta-\Bar{m}\right)\left|x\right|^{2}+\Bar{b}+ d/\beta\right).
    \end{align*}
    Let $R^{2}= 2(\Bar{b}+d/\beta)/(\Bar{m}-\alpha/\beta)$ be a fixed constant and then we obtain for all $x$ with $|x|\ge R$,
    \begin{align*}
        \Bar{\mathcal{L}}^{r}V_{2}(x)&\le -2\alpha \left(\Bar{m}+ d/\beta\right)V_{2}(x)
    \end{align*}
    and trivially for all $x$ with $|x|<R$,
    \begin{align*}
        \Bar{\mathcal{L}}^{r}V_{2}(x)&\le 2\alpha e^{2\alpha(\Bar{b}+d/\beta)/(\Bar{m}-\alpha/\beta)}\left(\Bar{b}+d/\beta\right)\\
        &\le 4\alpha e^{2\alpha(\Bar{b}+d/\beta)/(\Bar{m}-\alpha/\beta)}\left(\Bar{b}+d/\beta\right)-2\alpha \left(\Bar{b}+ d/\beta\right)V_{2}(x).
    \end{align*}
    Thus we have for all $x\in\R^{d}$,
    \begin{align*}
        \Bar{\mathcal{L}}^{r}V_{2}(x)\le -2\alpha \left(\Bar{b}+ d/\beta\right)V_{2}(x)+4\alpha e^{2\alpha(\Bar{b}+d/\beta)/(\Bar{m}-\alpha/\beta)}\left(\Bar{b}+d/\beta\right).
    \end{align*}
    By It\^{o}'s formula, there exists a sequence of stopping times $\{\sigma_{n}\in[0,\infty)\}_{n\in\N}$ with $\sigma_{n}<\sigma_{n+1}$ for all $n\in\N$ and $\sigma_{n}\uparrow \infty$ as $n\to\infty$ almost surely such that for all $n\in\N$ and $t\ge0$,
    \begin{align*}
        &\E\left[e^{2\alpha \left(\Bar{b}+ d/\beta\right)(t\wedge\sigma_{n})}V_{2}(X_{t\wedge\sigma_{n}})\right]\\
        &= \E\left[V_{2}(X_{0})\right]\\
        &\quad+\E\left[\int_{0}^{t\wedge\sigma_{n}}\left(e^{2\alpha \left(\Bar{b}+ d/\beta\right)s}\Bar{\mathcal{L}}^{r}V_{2}(X_{s})+2\alpha \left(\Bar{b}+ d/\beta\right)e^{2\alpha \left(\Bar{b}+ d/\beta\right)s}V_{2}(X_{s})\right)\diff s\right].
    \end{align*}
    It holds that
    \begin{align*}
        &\E\left[\int_{0}^{t\wedge\sigma_{n}}\left(e^{2\alpha \left(\Bar{b}+ d/\beta\right)s}\Bar{\mathcal{L}}^{r}V_{2}(X_{s})+2\alpha \left(\Bar{b}+ d/\beta\right)e^{2\alpha \left(\Bar{b}+ d/\beta\right)s}V_{2}(X_{s})\right)\diff s\right]\\
        &\le 4\alpha e^{2\alpha(\Bar{b}+d/\beta)/(\Bar{m}-\alpha/\beta)}\left(\Bar{b}+d/\beta\right)\E\left[\int_{0}^{t\wedge\sigma_{n}}e^{2\alpha \left(\Bar{b}+ d/\beta\right)s}\diff s\right]\\
        &\le 4\alpha e^{2\alpha(\Bar{b}+d/\beta)/(\Bar{m}-\alpha/\beta)}\left(\Bar{b}+d/\beta\right)\int_{0}^{t}e^{2\alpha \left(\Bar{b}+ d/\beta\right)s}\diff s
    \end{align*}
    and thus Fatou's lemma gives
    \begin{align*}
        &\E\left[e^{2\alpha \left(\Bar{b}+ d/\beta\right)t}V_{2}(X_{t})\right]\\
        &=\E\left[\lim_{n\to\infty} e^{2\alpha \left(\Bar{b}+ d/\beta\right)(t\wedge\sigma_{n})}V_{2}(X_{t\wedge\sigma_{n}})\right]\\
        &\le \liminf_{n\to\infty}\E\left[e^{2\alpha \left(\Bar{b}+ d/\beta\right)(t\wedge\sigma_{n})}V_{2}(X_{t\wedge\sigma_{n}})\right]\\
        &\le \E\left[V_{2}(X_{0})\right]+4\alpha e^{2\alpha(\Bar{b}+d/\beta)/(\Bar{m}-\alpha/\beta)}\left(\Bar{b}+d/\beta\right)\int_{0}^{t}e^{2\alpha \left(\Bar{b}+ d/\beta\right)s}\diff s.
    \end{align*}
    Therefore,
    \begin{align*}
        \E\left[V_{2}(X_{t})\right]&\le \E\left[e^{\alpha |X_{0}|^{2}}\right]e^{-2\alpha \left(\Bar{b}+ d/\beta\right)t}+2 e^{2\alpha(\Bar{b}+d/\beta)/(\Bar{m}-\alpha/\beta)}\left(1- e^{-2\alpha \left(\Bar{b}+ d/\beta\right)t}\right)
    \end{align*}
    and we obtain the desired conclusion.
\end{proof}


\subsection{Functional inequalities for mollified potentials}

\begin{lemma}[Lemma 2 of \citealp{raginsky2017non}]\label{lem:RRT17L02}
    Under (A1) and (A4), for almost all $x\in\R^{d}$,
    \begin{align*}
        U(x)\ge \frac{m}{3}|x|^{2}-\frac{b}{2}\log3. %\le \frac{\omega_{\nabla U}(1)}{2}\left|x\right|^{2}+\frac{3}{2}\left\|\nabla U\right\|_{\FMC}\left|x\right|+\left\|U\right\|_{L^{\infty}(B_{1}(\zero))}.
    \end{align*}
\end{lemma}

\begin{proof}%[Proof of Lemma \ref{lem:RRT17L02}]
    The proof is adapted from Lemma 2 of \citet{raginsky2017non}.
    We first fix $c\in(0,1]$.
    Since $\{x\in\R^{d}:x\text{ or }cx\text{ is in the set such that Eq.~\eqref{eq:FTC} does not hold}\}$ is null, for almost all $x\in\R^{d}$,
    \begin{align*}
        U(x)&=U(cx)+\int_{0}^{1}\left\langle \nabla U(cx+t(x-cx)),x-cx\right\rangle \diff t\\\
        &\ge \int_{0}^{1}\left\langle \nabla U((c+t(1-c))x),(1-c)x\right\rangle \diff t\\
        &=\int_{0}^{1}\frac{1-c}{c+t(1-c)}\left\langle \nabla U((c+t(1-c))x),(c+t(1-c))x\right\rangle \diff t\\
        &\ge \int_{0}^{1}\frac{1-c}{c+t(1-c)}\left(m(c+t(1-c))^{2}|x|^{2}-b\right) \diff t\\
        &=\int_{c}^{1}\frac{1}{s}\left(ms^{2}|x|^{2}-b\right) \diff s\\
        &=\frac{1-c^{2}}{2}m|x|^{2}-b\log c.
    \end{align*}
    Here, $s=c+t(1-c)$ and thus $\diff t=(1-c)^{-1}\diff s$.
    $c=1/\sqrt{3}$ yields the conclusion.
\end{proof}

Let $\Bar{L}^{r}$ be an operator such that $\Bar{L}^{r}f:=\Delta f-\beta\langle \nabla \Bar{U}_{r},\nabla f\rangle $ for all $f\in\mathcal{C}^{2}$.

\begin{lemma}[A bound for the constant of a Poincar\'{e} inequality of $\Bar{\pi}^{r}$]\label{lem:FI:PI:smooth}
    Under (A1)--(A4), for some absolute constant $a>0$, for all $r\in(0,1]$,
    \begin{align*}
        c_{\rm P}(\Bar{\pi}^{r})&\le \frac{4}{m\beta\left(d+\left(b+m\right)\beta\right)}\\
        &\quad+\frac{8a\left(d+\left(b+m\right)\beta\right)}{m\beta}\exp\left(\beta\left(\frac{25}{16}\left\|\nabla U\right\|_{\FMC}\left(1+\frac{8\left(d+\left(b+m\right)\beta\right)}{m\beta}\right)+U_{0}\right)\right),
    \end{align*}
    where $U_{0}:=\left\|U\right\|_{L^{\infty}(B_{1}(\zero))}$.
\end{lemma}
\begin{remark}
    Note that this upper bound is independent of $r$.
\end{remark}

\begin{proof}
    We adapt the discussion of \citet{raginsky2017non}.
    For the Lyapunov function $V(x)=e^{m\beta|x|^{2}/8}$, we obtain
    \begin{align*}
        \Bar{L}^{r}V(x)&=\left(\frac{dm\beta}{4}+\frac{\left(m\beta\right)^{2}}{16}\left|x\right|^{2}-\frac{m\beta^{2}}{4}\left\langle \nabla \Bar{U}_{r}\left(x\right),x\right\rangle \right)V(x)\\
        &=\left(\frac{dm\beta}{4}+\frac{\left(m\beta\right)^{2}}{16}\left|x\right|^{2}-\frac{m^{2}\beta^{2}}{8}\left|x\right|^{2}+\frac{m\beta^{2}}{4}\left(b+m\right)\right)V(x)\\
        &\le \left(\frac{m\beta\left(d+\left(b+m\right)\beta\right)}{4}-\frac{m^{2}\beta^{2}}{16}|x|^{2}\right)V(x).
    \end{align*}
    We fix the constants
    \begin{align*}
        \kappa=\frac{m\beta\left(d+\left(b+m\right)\beta\right)}{4},\ \gamma=\frac{m^{2}\beta^{2}}{16},\ \Tilde{R}^{2}=\frac{2\kappa}{\gamma}=\frac{8\left(d+\left(b+m\right)\beta\right)}{m\beta}.
    \end{align*}
    Lemmas \ref{lem:FMC:quadGrowth} and \ref{lem:RRT17L02}, $5a/2\le a^{2}+(5/4)^{2}$ for $a>0$, and $U(x)\ge 0$ give that
    \begin{align*}
        \text{Osc}_{\Tilde{R}}\left(\beta \Bar{U}_{r}\right)&\le \beta\left(\frac{\omega_{\nabla U}(1)}{2}\Tilde{R}^{2}+\frac{5}{2}\left\|\nabla U\right\|_{\FMC}\Tilde{R}+\left\|U\right\|_{L^{\infty}(B_{1}(\zero))}\right)\\
        &\le \beta\left(\left(\left\|\nabla U\right\|_{\FMC}+\frac{\omega_{\nabla U}(1)}{2}\right)\Tilde{R}^{2}+\frac{25}{16}\left\|\nabla U\right\|_{\FMC}+\left\|U\right\|_{L^{\infty}(B_{1}(\zero))}\right)\\
        &\le \beta\left(\frac{3}{2}\left\|\nabla U\right\|_{\FMC}\Tilde{R}^{2}+\frac{25}{16}\left\|\nabla U\right\|_{\FMC}+\left\|U\right\|_{L^{\infty}(B_{1}(\zero))}\right)\\
        &\le \beta\left(\frac{25}{16}\left\|\nabla U\right\|_{\FMC}\left(1+\Tilde{R}^{2}\right)+\left\|U\right\|_{L^{\infty}(B_{1}(\zero))}\right).
    \end{align*}
    Proposition \ref{prop:BBCG08T14} with $\lambda_{0}=\kappa_{0}=\kappa$ yields that for some absolute constant $a>0$,
    \begin{align*}
        c_{\rm P}\left(\Bar{\pi}^{r}\right)&\le \frac{4}{m\beta\left(d+\left(b+m\right)\beta\right)}\\
        &\quad\times\left(1+a\frac{m\beta\left(d+\left(b+m\right)\beta\right)}{4}\frac{8\left(d+\left(b+m\right)\beta\right)}{m\beta}e^{\text{Osc}_{\Tilde{R}}\left(\beta \Bar{U}_{r}\right)}\right)\\
        &=\frac{4}{m\beta\left(d+\left(b+m\right)\beta\right)}\\
        &\quad+\frac{8a\left(d+\left(b+m\right)\beta\right)}{m\beta}\exp\left(\beta\left(\frac{25}{16}\left\|\nabla U\right\|_{\FMC}\left(1+\frac{8\left(d+\left(b+m\right)\beta\right)}{m\beta}\right)+U_{0}\right)\right).
    \end{align*}
    Hence the statement holds true.
\end{proof}

\begin{lemma}[A bound for the constant in a logarithmic Sobolev inequality of $\Bar{\pi}^{r}$]\label{lem:FI:LSI:smooth}
    Under (A1)--(A4), for all $r\in(0,1]$,
    \begin{align*}
        c_{\rm LS}\left(\Bar{\pi}^{r}\right)% &\le\frac{32\left(d+4\right)\omega_{\nabla U}\left(R\right)}{m^{2}\beta^{2}R}+\frac{2r}{\left(d+4\right)\omega_{\nabla U}\left(r\right)}\\
        %&\quad+\left(\frac{\left(d+4\right)\omega_{\nabla U}\left(r\right)}{r}\left(\frac{8\left(d+\left(b+m\right)\beta\right)}{m\beta}+\frac{4\left(b+m+d/\beta\right)}{m}\right)+2\right)c_{\rm P}\left(\Bar{\pi}^{r}\right)\\
        &\le \frac{\left(d+4\right)\omega_{\nabla U}\left(r\right)}{r}\left(\frac{32}{m^{2}\beta^{2}}+\frac{12\left(d+\left(b+m\right)\beta\right)c_{\rm P}\left(\Bar{\pi}^{r}\right)}{m\beta}\right)\\
        &\quad+\frac{2r}{\left(d+4\right)\omega_{\nabla U}\left(r\right)}+2c_{\rm P}\left(\Bar{\pi}^{r}\right).
    \end{align*}
\end{lemma}

\begin{proof}
    We only need to check the conditions of Proposition \ref{prop:CGW10T12}.
    We see that the first condition is satisfied with the $\kappa$ and $\gamma$ in the proof of Lemma \ref{lem:FI:PI:smooth}.
    The second condition is obvious by Lemma \ref{lem:FI:PI:smooth}.
    The third condition follows from the twice differentiability and Lemma \ref{lem:FMC:boundedConvolutedGradient} by $K=(d+4)\omega_{\nabla U}(r)/r$.
    Therefore, Lemma \ref{lem:RRT17L03} and Fatou's lemma yield
    \begin{align*}
        &c_{\rm LS}\left(\Bar{\pi}^{r}\right)\\
        &\le \frac{32\left(d+4\right)\omega_{\nabla U}\left(r\right)}{m^{2}\beta^{2}r}+\frac{2r}{\left(d+4\right)\omega_{\nabla U}\left(r\right)}\\
        &\quad+\left(\frac{32\left(d+4\right)\omega_{\nabla U}\left(r\right)}{m^{2}\beta^{2}r}\left(\frac{m\beta\left(d+\left(b+m\right)\beta\right)}{4}+\frac{m^{2}\beta^{2}}{16}\int_{\R^{d}}|x|^{2}\Bar{\pi}^{r}(\diff x)\right)+2\right)c_{\rm P}\left(\Bar{\pi}^{r}\right)\\
        &=\frac{32\left(d+4\right)\omega_{\nabla U}\left(r\right)}{m^{2}\beta^{2}r}+\frac{2r}{\left(d+4\right)\omega_{\nabla U}\left(r\right)}\\
        &\quad+\left(\frac{\left(d+4\right)\omega_{\nabla U}\left(r\right)}{r}\left(\frac{8\left(d+\left(b+m\right)\beta\right)}{m\beta}+2\left(\frac{2\left(b+m+d/\beta\right)}{m}\right)\right)+2\right)c_{\rm P}\left(\Bar{\pi}^{r}\right)
    \end{align*}
    and thus the desired conclusion holds true.
\end{proof}

\subsection{Kullback--Leibler divergences}

\begin{lemma}\label{lem:RRT17L07}
    Under (A1)--(A5), for any $k\in\N$ and $\eta\in(0,1\wedge (\Tilde{m}/2((\omega_{\widetilde{G}}(1))^{2}+\delta_{\variance,2})))$, it holds true that
    \begin{align*}
        D(\mu_{k\eta}\|\Bar{\nu}_{k\eta}^{r})\le \left(C_{0}\frac{\omega_{\nabla U}(r)}{r}\eta+\beta\left(\delta_{r,2}\kappa_{\infty}+\delta_{r,0}\right)\right)k\eta,
    \end{align*}
    where $C_{0}$ is a positive constant such that
    \begin{align*}
        C_{0}&=\left(d+4\right)\left(\frac{\beta}{3}\left(\delta_{\variance,0}+\left\|\widetilde{G}\right\|_{\FMC}^{2}+\left((\omega_{\widetilde{G}}(1))^{2}+\delta_{\variance,2}\right)\kappa_{\infty}\right)+\frac{d}{2}\right).
    \end{align*}
\end{lemma}

\begin{proof}%[Proof of Lemma \ref{lem:RRT17L07}]
    We set $a_{k\eta}:=\{\mathbf{a}_{t}=a_{\lfloor t/\eta\rfloor \eta}';a_{i\eta}'\in A,i=0,\ldots,k\}$ and $\mathcal{A}_{k\eta}:=\sigma(\{\mathbf{a}\in a_{k\eta};\mathbf{a}_{t_{j}}\in C_{j},j=1,\ldots,n\};t_{j}\in[0,k\eta],C_{j}\in\mathcal{A},n\in\N)$.
    Let $P_{k\eta}$ and $Q_{k\eta}$ denote the probability measures on $(a_{k\eta}\times W_{k\eta},\mathcal{A}_{k\eta}\times \mathcal{W}_{k\eta})$ of $\{(a_{\lfloor t/\eta\rfloor \eta},Y_{t});0\le t\le T\}$ and $\{(a_{\lfloor t/\eta\rfloor \eta},\Bar{X}_{t}^{r});0\le t\le T\}$ respectively.
    Note that $\Bar{X}_{t}^{r}$ is the unique strong solution to Eq.~\eqref{eq:sLD}
    and such a unique strong solution of this equation exists for any $r>0$ since $\nabla \Bar{U}_{r}$ is Lipschitz continuous by Lemma \ref{lem:FMC:boundedConvolutedGradient}.
    We obtain
    \begin{align*}
        &\frac{\beta}{4}\E\left[\int_{0}^{k\eta}\left|\nabla \Bar{U}_{r}\left(Y_{t}\right)-G\left(Y_{\lfloor t/\eta\rfloor \eta},a_{\lfloor t/\eta\rfloor\eta}\right)\right|^{2}\diff t\right]\\
        &\le \frac{\beta}{2}\sum_{j=0}^{k-1}\E\left[\int_{j\eta}^{(j+1)\eta}\left|\nabla \Bar{U}_{r}\left(Y_{t}\right)-\nabla \Bar{U}_{r}\left(Y_{\lfloor t/\eta\rfloor \eta}\right)\right|^{2}\diff t\right]\\
        &\quad+\frac{\beta}{2}\sum_{j=0}^{k-1}\E\left[\int_{j\eta}^{(j+1)\eta}\left|\nabla \Bar{U}_{r}\left(Y_{\lfloor t/\eta\rfloor \eta}\right)-G\left(Y_{\lfloor t/\eta\rfloor \eta},a_{\lfloor t/\eta\rfloor\eta}\right)\right|^{2}\diff t\right]\\
        &\le \frac{\beta}{2}\frac{\left(d+4\right)\omega_{\nabla U}(r)}{r}\sum_{j=0}^{k-1}\E\left[\int_{j\eta}^{(j+1)\eta}\left|Y_{t}-Y_{\lfloor t/\eta\rfloor \eta}\right|^{2}\diff t\right]\\
        &\quad+\frac{\beta}{2}\sum_{j=0}^{k-1}\E\left[\eta\left|\nabla \Bar{U}_{r}\left(Y_{j \eta}\right)-G\left(Y_{j\eta},a_{j\eta}\right)\right|^{2}\right].
    \end{align*}
    We have for all $t\in[j\eta,(j+1)\eta)$, by Lemmas \ref{lem:FMC:linearGrowth} and \ref{lem:RRT17L03},
    \begin{align*}
        &\E\left[\left|Y_{t}-Y_{\lfloor t/\eta\rfloor \eta}\right|^{2}\right]\\
        &=\E\left[\left|-\left(t-j\eta\right)G\left(Y_{j\eta},a_{j\eta}\right)+\sqrt{2\beta^{-1}}\left(B_{t}-B_{j\eta}\right)\right|^{2}\right]\\
        &=\left(t-j\eta\right)^{2}\E\left[\left|G\left(Y_{j\eta},a_{j\eta}\right)\right|^{2}\right]+2\beta^{-1}\E\left[\left|B_{t}-B_{j\eta}\right|^{2}\right]\\
        &\le \left(t-j\eta\right)^{2}\left(2\delta_{\variance,2}\E\left[\left|Y_{j\eta}\right|^{2}\right]+2\delta_{\variance,0}+\E\left[\left|\widetilde{G}\left(Y_{j\eta}\right)\right|^{2}\right]\right)+2\beta^{-1}\E\left[\left|B_{t}-B_{j\eta}\right|^{2}\right]\\
        &\le 2\left(t-j\eta\right)^{2}\left(\delta_{\variance,0}+\left\|\widetilde{G}\right\|_{\FMC}^{2}+\left((\omega_{\widetilde{G}}(1))^{2}+\delta_{\variance,2}\right)\E\left[\left|Y_{j\eta}\right|^{2}\right]\right)+2\beta^{-1}d\left(t-j\eta\right)\\
        &\le 2\left(t-j\eta\right)^{2}\left(\delta_{\variance,0}+\left\|\widetilde{G}\right\|_{\FMC}^{2}+\left((\omega_{\widetilde{G}}(1))^{2}+\delta_{\variance,2}\right)\kappa_{\infty}\right)+2\beta^{-1}d\left(t-j\eta\right)\\
        &=:2\left(t-j\eta\right)^{2}C'+2\beta^{-1}d\left(t-j\eta\right)
    \end{align*}
    and thus
    \begin{align*}
        \sum_{j=0}^{k-1}\E\left[\int_{j\eta}^{(j+1)\eta}\left|Y_{t}-Y_{\lfloor t/\eta\rfloor \eta}\right|^{2}\diff t\right]\le \left(\frac{2C'\eta^{3}}{3}+\frac{d\eta^{2}}{\beta}\right)k\le \left(\frac{2C'}{3}+\frac{d}{\beta}\right)k\eta^{2}.
    \end{align*}
    Since $Y_{j\eta}$ is $\sigma(Y_{(j-1)\eta},a_{(j-1)\eta},B_{j\eta}-B_{(j-1)\eta})$-measurable and $a_{j\eta}$ is independent of this $\sigma$-algebra,
    \begin{align*}
        &\E\left[\left|\nabla \Bar{U}_{r}\left(Y_{j\eta}\right)-G\left(Y_{j\eta},a_{j\eta}\right)\right|^{2}\right]\\
        &=\E\left[\E\left[\left.\left|\nabla \Bar{U}_{r}\left(Y_{j\eta}\right)-G\left(Y_{j\eta},a_{j\eta}\right)\right|^{2}\right|Y_{(j-1) \eta},a_{(j-1)\eta},B_{j\eta}-B_{(j-1)\eta}\right]\right]\\
        &\le \E\left[2\left(\delta_{\bias,r,2}+\delta_{\variance,2}\right)\left|Y_{j\eta}\right|^{2}+2\left(\delta_{\bias,r,0}+\delta_{\variance,0}\right)\right]\\
        &\le 2\delta_{r,2}\kappa_{\infty}+2\delta_{r,0}.
    \end{align*}
    Now we see that the assumptions (LS1)--(LS4) of Propositions \ref{prop:LS01L0706} and \ref{prop:KL} are satisfied owing to (A1)--(A5), Lemma \ref{lem:RRT17L03}, and linear growths of $\widetilde{G}(w_{\lfloor \cdot /\eta\rfloor \eta})$ with respect to $\max_{i=0,\ldots,k}|w_{i\eta}|$ and $\nabla \Bar{U}_{r}(w_{t})$ with respect to $|w_{t}|$.
    Therefore, Jensen's inequality for conditional expectation and Proposition \ref{prop:KL} give
    \begin{align*}
        D(\mu_{k\eta}\|\Bar{\nu}_{k\eta}^{r})&=\int\log\left(\left.\frac{\diff P_{k\eta}}{\diff Q_{k\eta}}\right|_{\sigma(x_{k\eta};x\in W_{k\eta})}\right)\diff P_{k\eta}\\
        &\le \int\log\left(\frac{\diff P_{k\eta}}{\diff  Q_{k\eta}}\right)\diff P_{k\eta}\\
        &\le \frac{\beta}{4}\E\left[\int_{0}^{k\eta}\left|\nabla \Bar{U}_{r}\left(Y_{t}\right)-G\left(Y_{\lfloor t/\eta\rfloor \eta},a_{\lfloor t/\eta\rfloor\eta}\right)\right|^{2}\diff t\right]\\
        &\le \frac{(d+4)\omega_{\nabla U}(r)}{r}\left(\frac{C'\beta}{3}+\frac{d}{2}\right)k\eta^{2}+\beta\left(\delta_{r,2}\kappa_{\infty}+\delta_{r,0}\right)k\eta\\
        &=\left(\frac{(d+4)\omega_{\nabla U}(r)}{r}\left(\frac{C'\beta}{3}+\frac{d}{2}\right)\eta+\beta\left(\delta_{r,2}\kappa_{\infty}+\delta_{r,0}\right)\right)k\eta\\
        &=\left(C_{0}\frac{\omega_{\nabla U}(r)}{r}\eta+\beta\left(\delta_{r,2}\kappa_{\infty}+\delta_{r,0}\right)\right)k\eta.
    \end{align*}
    This is the desired conclusion.
\end{proof}

\begin{lemma}[Lemma 5 of \citealp{raginsky2017non}]\label{lem:RRT17L05}
    Under (A1)--(A6), it holds that for all $r\in(0,1]$,
    \begin{align*}
        D(\mu_{0}\|\Bar{\pi}^{r})\le \log \|p_{0}\|_{\infty}+\frac{d}{2}\log\frac{3\pi}{m\beta}+\beta\left(\frac{\omega_{\nabla U}(1)}{2}\kappa_{0}+\frac{5}{2}\left\|\nabla U\right\|_{\FMC}\kappa_{0}^{\frac{1}{2}}+U_{0}+\frac{b}{2}\log3\right).
    \end{align*}
\end{lemma}

\begin{proof}
    The density of $\Bar{\pi}^{r}$ is given as $(\diff\Bar{\pi}^{r}/\diff x)(x)=\Bar{\mathcal{Z}}^{r}(\beta)^{-1}e^{-\beta \Bar{U}_{r}\left(x\right)}$ and 
    \begin{align*}
        D(\mu_{0}\|\Bar{\pi}^{r})&=\int_{\R^{d}}p_{0}\left(x\right)\log\left(\frac{p_{0}\left(x\right)}{\Bar{\mathcal{Z}}^{r}(\beta)^{-1}e^{-\beta \Bar{U}_{r}\left(x\right)}}\right)\diff x\\
        &\le \log \|p_{0}\|_{\infty}+\log\Bar{\mathcal{Z}}^{r}(\beta)+\beta\int_{\R^{d}}p_{0}(x)\Bar{U}_{r}(x)\diff x.
    \end{align*}
    Lemma \ref{lem:RRT17L02}, Jensen's inequality, and convexity of $e^{-x}$ yield
    \begin{align*}
        \Bar{\mathcal{Z}}^{r}(\beta)&=\int_{\R^{d}}e^{-\beta \Bar{U}_{r}\left(x\right)}\diff x\\
        &=\int_{\R^{d}}e^{-\beta \int _{\R^{d}}U(x-y)\rho_{r}(y)\diff y}\diff x\\
        &\le \int_{\R^{d}}\int_{\R^{d}}e^{-\beta U(x-y)}\diff x\rho_{r}(y)\diff y\\
        &\le e^{\frac{1}{2}\beta b\log 3}\int_{\R^{d}}\int_{\R^{d}}e^{-m\beta|x-y|^{2}/3}\diff x\rho_{r}(y)\diff y\\
        &=3^{\beta b/2}\left(\frac{3\pi}{m\beta}\right)^{d/2}.
    \end{align*}
    Lemma \ref{lem:FMC:quadGrowth} gives
    \begin{align*}
        \int_{\R^{d}}p_{0}(x)\Bar{U}_{r}(x)\diff x
        &\le \int_{\R^{d}}p_{0}(x)\left(\frac{\omega_{\nabla U}(1)}{2}|x|^{2}+\frac{5}{2}\left\|\nabla U\right\|_{\FMC}|x|+\left\|U\right\|_{L^{\infty}(B_{1}(\zero))}\right)\diff x\\
        &\le \frac{\omega_{\nabla U}(1)}{2}\kappa_{0}+\frac{5}{2}\left\|\nabla U\right\|_{\FMC}\kappa_{0}^{\frac{1}{2}}+\left\|U\right\|_{L^{\infty}(B_{1}(\zero))}.
    \end{align*}
    Therefore,
    \begin{align*}
        D(\mu_{0}\|\Bar{\pi}^{r})\le \log \|p_{0}\|_{\infty}+\frac{d}{2}\log\frac{3\pi}{m\beta}+\beta\left(\frac{\omega_{\nabla U}(1)}{2}\kappa_{0}+\frac{5}{2}\left\|\nabla U\right\|_{\FMC}\kappa_{0}^{\frac{1}{2}}+U_{0}+\frac{b}{2}\log3\right).
    \end{align*}
    Hence we obtain the conclusion.
\end{proof}

\begin{lemma}[Kullback--Leibler divergence of Gibbs distributions]\label{lem:KLGibbs}
    Under (A1)--(A4), it holds that 
    \begin{align*}
        D\left(\pi\|\Bar{\pi}^{r}\right)\le \beta r\left(\left|\nabla U\left(\zero\right)\right|+\frac{3\omega_{\nabla U}(1)}{2}+\frac{\omega_{\nabla U}(1)}{2}\int_{\R^{d}}|x|\pi(\diff x)\right).
    \end{align*}
\end{lemma}
\begin{proof}%[Proof of Lemma \ref{lem:KLGibbs}]
    
The divergence of $\pi$ from $\Bar{\pi}^{r}$ is
\begin{align*}
    D\left(\pi\|\Bar{\pi}^{r}\right)&=\frac{1}{\mathcal{Z}(\beta)}\int \exp\left(-\beta U\left(x\right)\right)\log\left[\frac{\Bar{\mathcal{Z}}^{r}(\beta)\exp\left(-\beta U\left(x\right)\right)}{\mathcal{Z}(\beta)\exp\left(-\beta \Bar{U}_{r}\left(x\right)\right)}\right]\diff x\\
    &=\frac{\beta}{\mathcal{Z}(\beta)}\int \exp\left(-\beta U\left(x\right)\right)\left(\Bar{U}_{r}(x)-U\left(x\right)\right)\diff x\\
    &\quad+\left(\log\Bar{\mathcal{Z}}^{r}(\beta)-\log\mathcal{Z}(\beta)\right).
    % &= \frac{\beta}{\mathcal{Z}(\beta)}\int \exp\left(-\beta U\left(x\right)\right)\int\left(U\left(y\right)-  U\left(x\right)\right)\rho_{r}(x-y)\diff y\diff x\\
    % &\quad+\left(\log\Bar{\mathcal{Z}}^{r}(\beta)-\log\mathcal{Z}(\beta)\right)
\end{align*}
% where $\Bar{\mathcal{Z}}^{r}(\beta):=\int \exp(-\beta \Bar{U}_{r}(x))\diff x$.
Lemma \ref{lem:FMC:localLipschitz} yields
\begin{align*}
    &\frac{\beta}{\mathcal{Z}(\beta)}\int \exp\left(-\beta U\left(x\right)\right)\left(\Bar{U}_{r}(x)-U\left(x\right)\right)\diff x\\
    &\le \frac{\beta}{\mathcal{Z}(\beta)}\int \exp\left(-\beta U\left(x\right)\right)\left|\Bar{U}_{r}(x)-U\left(x\right)\right|\diff x\\
    &\le \frac{\beta r}{\mathcal{Z}(\beta)}\int \left(\left|\nabla U\left(\zero\right)\right|+\left(1+\frac{r}{2}\right)\omega_{\nabla U}(1)+\omega_{\nabla U}(1)\frac{|x|}{2}\right)\exp\left(-\beta U\left(x\right)\right)\diff x\\
    &\le \beta r\left(\left|\nabla U\left(\zero\right)\right|+\frac{3}{2}\omega_{\nabla U}(1)+\frac{\omega_{\nabla U}(1)}{2}\int|x|\pi(\diff x)\right).
\end{align*}
Jensen's inequality and Fubini's theorem give
\begin{align*}
    \Bar{\mathcal{Z}}^{r}\left(\beta\right)&=\int_{\R^{d}}\exp\left(-\beta \int_{\R^{d}}U\left(x-y\right)\rho_{r}\left(y\right)\diff y\right)\diff x\\
    &\le \int_{\R^{d}}\int_{\R^{d}}\exp\left(-\beta U\left(x-y\right)\right)\rho_{r}\left(y\right)\diff y\diff x\\
    &=\int_{\R^{d}}\int_{\R^{d}}\exp\left(-\beta U\left(x-y\right)\right)\diff x\rho_{r}\left(y\right)\diff y\\
    &= \mathcal{Z}\left(\beta\right)
\end{align*}
and thus
\begin{align*}
    D\left(\pi\|\Bar{\pi}^{r}\right)
    &\le \beta r\left(\left|\nabla U\left(\zero\right)\right|+\frac{3\omega_{\nabla U}(1)}{2}+\frac{\omega_{\nabla U}(1)}{2}\int_{\R^{d}}|x|\pi(\diff x)\right).
\end{align*}
This is the desired conclusion.
\end{proof}


\subsection{Proof of Theorem \ref{thm:sglmc}}
We complete the proof of Theorem \ref{thm:sglmc}.

\begin{proof}[Proof of Theorem \ref{thm:sglmc}]
We decompose the 2-Wasserstein distance as follows:
    \begin{align*}
    \Wasserstein_{2}(\mu_{k\eta},\pi)\le \underbrace{\Wasserstein_{2}(\mu_{k\eta},\Bar{\nu}_{k\eta}^{r})}_\textrm{(1)}+\underbrace{\Wasserstein_{2}(\Bar{\nu}_{k\eta}^{r},\Bar{\pi}^{r})}_\textrm{(2)}+\underbrace{\Wasserstein_{2}(\Bar{\pi}^{r},\pi)}_\textrm{(3)}.
\end{align*}

(1) We first consider the upper bound for $\Wasserstein_{2}(\mu_{k\eta},\Bar{\nu}_{k\eta}^{r})$.
Proposition \ref{prop:BV05C23} gives
\begin{align*}
    \Wasserstein_{2}(\mu_{k\eta},\Bar{\nu}_{k\eta}^{r})\le C_{\Bar{\nu}_{k\eta}^{r}}\left(D\left(\mu_{k\eta}\|\Bar{\nu}_{k\eta}^{r}\right)^{\frac{1}{2}}+\left(\frac{D\left(\mu_{k\eta}\|\Bar{\nu}_{k\eta}^{r}\right)}{2}\right)^{\frac{1}{4}}\right),
\end{align*}
where
\begin{align*}
    C_{\Bar{\nu}_{k\eta}^{r}}:=2\inf_{\lambda>0}\left(\frac{1}{\lambda}\left(\frac{3}{2}+\log\int_{\R^{d}}e^{\lambda\left|x\right|^{2}}\Bar{\nu}_{k\eta}^{r}\left(\diff x\right)\right)\right)^{\frac{1}{2}}.
\end{align*}
We fix $\lambda=1\wedge (\beta m/4)$ and then Lemma \ref{lem:LD:exponential} leads to
\begin{align*}
    C_{\Bar{\nu}_{k\eta}^{r}}&\le \frac{1}{\lambda^{1/2}}\left(6+4\log\int_{\R^{d}}e^{\lambda\left|x\right|^{2}}\Bar{\nu}_{k\eta}^{r}\left(\diff x\right)\right)^{\frac{1}{2}}\\
    &\le \frac{1}{\lambda^{1/2}}\left(6+4\left(\lambda\kappa_{0}+\frac{8\lambda(b+m+d/\beta)}{m}+1\right)\right)^{\frac{1}{2}}\\
    &\le \left(4\kappa_{0}+\frac{32(b+m+d/\beta)}{m}+\frac{10}{1\wedge (\beta m/4)}\right)^{\frac{1}{2}}.
\end{align*}
Hence Lemma \ref{lem:RRT17L07} gives the following bound:
\begin{align*}
    \Wasserstein_{2}(\mu_{k\eta},\Bar{\nu}_{k\eta}^{r})
    &\le C_{1}\max\left.\left\{x^{\frac{1}{2}},x^{\frac{1}{4}}\right\}\right|_{x=\left(C_{0}\frac{\omega_{\nabla U}(r)}{r}\eta+\beta\left(\delta_{r,2}\kappa_{\infty}+\delta_{r,0}\right)\right)k\eta}.
\end{align*}

(2)
In the second place, let us give the bound for $\Wasserstein_{2}(\Bar{\nu}_{k\eta}^{r},\Bar{\pi}^{r})$.
Proposition \ref{prop:BV05C23} yields
\begin{align*}
    \Wasserstein_{2}(\Bar{\nu}_{k\eta}^{r},\Bar{\pi}^{r})\le C_{\Bar{\pi}^{r}}\left(D\left(\Bar{\nu}_{k\eta}^{r}\|\Bar{\pi}^{r}\right)^{\frac{1}{2}}+\left(\frac{D\left(\Bar{\nu}_{k\eta}^{r}\|\Bar{\pi}^{r}\right)}{2}\right)^{\frac{1}{4}}\right),
\end{align*}
where
\begin{align*}
    C_{\Bar{\pi}^{r}}:=2\inf_{\lambda>0}\left(\frac{1}{\lambda}\left(\frac{3}{2}+\log\int_{\R^{d}}e^{\lambda\left|x\right|^{2}}\Bar{\pi}^{r}\left(\diff x\right)\right)\right)^{\frac{1}{2}}.
\end{align*}
We fix $\lambda=1\wedge (\beta m/4)$ and then Lemma \ref{lem:LD:exponential} along with Fatou's lemma leads to
\begin{align*}
    C_{\Bar{\pi}^{r}}&\le \frac{1}{\lambda^{1/2}}\left(6+4\log\int_{\R^{d}}e^{\lambda\left|x\right|^{2}}\Bar{\pi}^{r}\left(\diff x\right)\right)^{\frac{1}{2}}\\
    &\le \frac{1}{\lambda^{1/2}}\left(6+4\left(\frac{8\lambda(b+m+d/\beta)}{m}+1\right)\right)^{\frac{1}{2}}\\
    &\le \left(\frac{32(b+m+d/\beta)}{m}+\frac{10}{1\wedge (\beta m/4)}\right)^{\frac{1}{2}}.
\end{align*}
Hence Lemma \ref{lem:RRT17L05} and the exponential decay of entropy \citep{bakry2014analysis} give the upper bound
\begin{align*}
        \Wasserstein_{2}\left(\mu_{k\eta},\Bar{\pi}^{r}\right)&\le C_{1}^{\prime}\sqrt{C_{2}+\sqrt{C_{2}}}\exp(-k\eta/2\beta c_{\rm LS}(\Bar{\pi}^{r})).
\end{align*}

(3)
As (2), Proposition \ref{prop:BV05C23} along with Lemmas \ref{lem:RRT17L03} and \ref{lem:KLGibbs} yields the bound
\begin{align*}
    \Wasserstein_{2}(\Bar{\pi}^{r},\pi)\le C_{1}'\max\left.\left\{y^{\frac{1}{2}},y^{\frac{1}{4}}\right\}\right|_{y=\beta r\left(\frac{3}{2}\left\|\nabla U\right\|_{\FMC}+\frac{\omega_{\nabla U}(1)}{2}\sqrt{\frac{b+d/\beta}{m}}\right)}.
\end{align*}

(4)
By (1) and (3), since $C_{1}'\le C_{1}$,
\begin{align*}
    \Wasserstein_{2}(\mu_{k\eta},\Bar{\nu}_{k\eta}^{r})+\Wasserstein_{2}(\Bar{\pi}^{r},\pi)&\le C_{1}\left(\max\left\{x^{\frac{1}{2}},x^{\frac{1}{4}}\right\}+\max\left\{y^{\frac{1}{2}},y^{\frac{1}{4}}\right\}\right)\\
    &\le C_{1}\left(x^{\frac{1}{2}}+x^{\frac{1}{4}}+y^{\frac{1}{2}}+y^{\frac{1}{4}}\right)\\
    &\le C_{1}\left(2\left(x+y\right)^{\frac{1}{2}}+2\left(x+y\right)^{\frac{1}{4}}\right),
\end{align*}
where
\begin{align*}
    x&=\left(C_{0}\frac{\omega_{\nabla U}(r)}{r}\eta+\beta\left(\delta_{r,2}\kappa_{\infty}+\delta_{r,0}\right)\right)k\eta,\\ 
    y&=\beta r\left(\frac{3}{2}\left\|\nabla U\right\|_{\FMC}+\frac{\omega_{\nabla U}(1)}{2}\sqrt{\frac{b+d/\beta}{m}}\right).
\end{align*}
Hence, we obtain the desired conclusion.
\end{proof}

\section*{Acknowledgement}
The author was supported by JSPS KAKENHI Grant Number JP21K20318 and JST CREST Grant Numbers JPMJCR21D2 and JPMJCR2115.

\bibliographystyle{apalike}
\bibliography{bibSGLMC}

\end{document}