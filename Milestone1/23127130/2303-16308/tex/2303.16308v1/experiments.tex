\section{Experiments}

\begin{figure}[t]
     \centering
    \subfigure[Speech keyword detection]{\includegraphics[width=0.9\linewidth]{images/speech.png}\label{fig:speech1}}\\
    \subfigure[Human activity recognition]{\includegraphics[width=0.9\linewidth]{images/har.png}\label{fig:har1}}
        \caption{Certificates against online adversarial attacks for varying smoothing noises. Here we can perturb each input only once. The average size of perturbation is computed as per equation \ref{eq:adv_constraint}.}
        \label{fig:certificates1}
        \vspace{-3mm}
\end{figure}

% \subsection{Setup}
We test our certificates for two streaming tasks -- speech keyword detection and human activity recognition. We use a subset of the Speech commands dataset \citep{speechcommandsv2} for our speech keyword detection task. The subset we use contains ten keyword classes, corresponding to utterances of numbers from zero to nine recorded at a sample rate of 16 kHz. This dataset also contains noise clips such as audio of running tap water and exercise bike. We add these noise clips to the speech audio to simulate real-world scenarios and stitch them together to generate longer audio clips. We use the UCI HAR dataset \citep{reyes2012uci} for human activity recognition. This contains a 6-D triaxial accelerometer and gyroscope readings measured with human subjects. The objective in HAR is to recognize various human activities based on sensor readings. The UCI HAR dataset contains signals recorded at 50 Hz that correspond to six human activities such as standing, sitting, laying, walking, walking up, and walking down. 


We use the M5 network described in \cite{m5} with an SGD optimizer and an initial learning rate of 0.1, which we anneal using a cosine scheduler. For the speech detection task, we train a M5 network with 128 channels for 30 epochs with a batch size of 128. For the human activity recognition task, we use a M5 network with 32 channels for 30 epochs with a batch size of 256. We apply isotropic Gaussian noise for smoothing and use the $\ell_2$-norm to define the average distance measure $d$. For the speech keyword detection task, we use smoothing noises with standard deviations of 0.1, 0.2, 0.4, 0.6, and 0.8. For the human activity recognition task, we use smoothing noises with standard deviations of 4, 6, 8, and 10. See Appendix \ref{app:exps} for more details on the experiments. We compute certificates for both scenarios, where the input is attacked only once and where each window can be attacked with the ability to re-attack inputs. These experiments show that our certificates provide 
 meaningful guarantees against adversarial perturbations.


\begin{algorithm}[tb]
   \caption{Our streaming attack}
   \label{alg:onlineattack}
\begin{algorithmic}
   \STATE {\bfseries Input:} time-step $j$, clean inputs $x_j, x_{j-1}, ..., x_{j-w+1}$, perturbed inputs $x'_{j-1}, ..., x'_{j-w+1}$, attack budget $\epsilon$, search parameter $\alpha \in \mathbb{N}$.
   % \REPEAT
   \STATE $d_{j-1} = \sum_{i=1}^{j-1} d(x_i, x_i')$
   \STATE $budget_j = j \epsilon - d_{j-1}$
   % \STATE Initialize $noChange = true$.
   \FOR{$i=0$ {\bfseries to} $\alpha$}
   \STATE $\epsilon' = \frac{i}{\alpha} \cdot budget_j$
   \STATE $x = \arg \min_{x} f_j(x,...,x_{j-w+1}')$ s.t. $d(x, x_j) \leq \epsilon'$
   \IF{$f_j(x_j', ...,x_{j-w+1}') = 0$}
   \STATE $x_j' = x$
   \STATE break
   \ELSE 
   \STATE $x_j' = x_j$
   \ENDIF
   \ENDFOR
   % \UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}

\subsection{Attacking an input only once} \label{sec:attackonce}

\begin{figure}[t]
     \centering
    \subfigure[Speech keyword detection]{\includegraphics[width=0.9\linewidth]{images/best_windows_speech.png}\label{fig:bestspeech}}\\
    \subfigure[Human activity recognition]{\includegraphics[width=0.9\linewidth]{images/best_windows_har.png}\label{fig:besthar}}
    %\vspace{-4mm}
        \caption{Best certificates across varying smoothing noises for different window sizes. Streaming models with smaller window sizes are more robust to adversarial perturbations.}
        \label{fig:bestcertificates1}
        \vspace{-6mm}
\end{figure}

\begin{figure}[t]
     \centering
    \subfigure[Speech keyword detection]{\includegraphics[width=0.9\linewidth]{images/speech_w2_2.png}\label{fig:speech2}}\\
    \subfigure[Human activity recognition]{\includegraphics[width=0.9\linewidth]{images/har_w2_2.png}\label{fig:har2}}
    %\vspace{-4mm}
        \caption{Certificates against online adversarial attacks for varying smoothing noises. Here we attack each window with the ability to re-attack inputs. The average size of perturbation is computed as per equation \ref{eq:adv_constraint_window}.}
        \label{fig:certificates2}
        \vspace{-6mm}
\end{figure}


We evaluate the robustness of undefended models using a custom-made attack that is constrained by the $\ell_2$-norm budget, as described in equation \ref{eq:adv_constraint}. To adhere to this constraint at each time-step $j$, the attacker must only perturb the input $x_j$, since the previous inputs $(x_{j-w+1},...,x_{j-1})$ have already been perturbed. This creates a significant challenge in creating a strong adversary. We design an adversary that only perturbs the last input $x_j$ at every time-step $j$ using projected gradient descent to minimize $f_j$. In our experiments, we set $f_j = 1$ if the model outputs the correct class and $f_j = 0$ when the model misclassifies. We linearly search using grid search parameter $\alpha$ for the smallest distance $d(x_j, x_j')$ such that the input $(x_{j-w+1}',...,x_j')$ leads to a misclassification at time-step $j$. We perturb $x_j$ if $(x_{j-w+1}',...,x_j')$ leads to misclassification and the average distance budget at time-step $j$ is less than $\epsilon$. Else, we do not perturb $x_j$. In this manner, our attack perturbs the streaming input in a greedy fashion. See Algorithm \ref{alg:onlineattack} for details.


We conduct our streaming attack on the keyword recognition task with a window size of $w=2$, where each input $x_j$ is a 4000-dimensional vector in the range [0,1]. We also perform the attack on the human activity recognition task with $w=2$, where each input $x_j$ is a 250x6-dimensional matrix. We use search parameter $\alpha = 15$. We plot the results of our certificates for various smoothing noises (see Figure \ref{fig:certificates1}). Note that the attack budget $\epsilon$ is calculated as per the definition in equation \ref{eq:adv_constraint}. In Figure \ref{fig:bestcertificates1}, we also plot our best certificates across various smoothing noises for different window sizes $w$. This plot supports our theory that streaming models with smaller window sizes are more robust to adversarial perturbations. Figures \ref{fig:attacksmoothhar} and \ref{fig:attacksmoothspeech} in Appendix \ref{app:attacksmooth} show that the empirical performance of smooth models after the online adversarial attack is lower bound by our certificates. These plots validate our certificates.

\subsection{Attacking each window}


Now, we perform experiments for the attack setting described in Section \ref{sec:attackingeahcwindow}. Note that here we need to calculate the attack budget $\epsilon$ based on equation \ref{eq:adv_constraint_window}. In this setting, we can re-attack an input for every window, making it a stronger attack. To attack the undefended models, we search for window perturbations that lead to misclassification using a minimum distance budget. Similar to our previous attack in Section \ref{sec:attackonce}, we only perturb a window at time-step $j$ if the average window distance at time-step $j$ is less than $\epsilon$. Also, we do not perturb a window if the window can not be perturbed to reduce the performance $f_j$. In Figure \ref{fig:certificates2}, we plot our certificates for this attack setting along with the accuracy of the undefended model for different attack budgets. These experiments show that our certificates produce meaningful performance guarantees against adversarial perturbations even if an attacker has the ability to re-attack the inputs. Figure \ref{fig:attacksmoothhar_2} in Appendix \ref{app:attacksmooth} shows that the empirical performance of smooth models after the online adversarial attack is lower bound by our certificates. These plots validate our certificates.