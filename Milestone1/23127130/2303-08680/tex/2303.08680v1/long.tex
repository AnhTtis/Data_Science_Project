\documentclass[letterpaper,12pt]{article}
%\documentclass[10pt,journal,compsoc]{IEEEtran}
%\documentclass[12pt,draftclsnofoot,onecolumn]{IEEEtran}
\usepackage{geometry}
 \geometry{
left=15mm,
right=15mm,
 top=18mm,
 }
 \usepackage{authblk}
\usepackage{tikz}  
\usetikzlibrary{calc,intersections}
 \usepackage{verbatim}
\usepackage{textcomp}
\usepackage[T1]{fontenc}
\usepackage{bbm}
\usepackage{multirow}
%\usepackage[utf8]{inputenc} 
%\usepackage{authblk}
%\usepackage{epstopdf}
\usepackage{dsfont}
\usepackage{cite}
\usepackage{epsfig}
\usepackage{float}
\usepackage{enumerate}
\usepackage{balance}
\usepackage{color}
\usepackage{subcaption}
\usepackage{caption}
\captionsetup{font=scriptsize,labelfont=scriptsize}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algrenewcommand\Return{\State \algorithmicreturn{} } 
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
		citecolor=blue,
}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{array}
\usepackage{graphicx}
\usepackage{lettrine}
\usepackage{epsf} 
\usepackage{psfrag}
\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes
\usepackage[space]{grffile} % For spaces in paths
\usepackage{etoolbox} % For spaces in paths
\usepackage{url} 

\usepackage{multirow}
\newtheorem*{remark}{Remark}
\newtheorem{theorem1}{Theorem}
\newtheorem{proposition}[theorem1]{Proposition}
\newtheorem{theorem2}{Theorem}
\newtheorem{definition}[theorem2]{Definition}
\newtheorem{theorem3}{Theorem}
\newtheorem{corollary}[theorem3]{Corollary}
\newtheorem{theorem4}{Theorem}
\newtheorem{lemma}[theorem4]{Lemma}
\newtheorem{mydef}{Definition}
\usepackage{optidef}
%\usepackage[table]{xcolor}
%\renewcommand{\qedsymbol}{$\blacksquare$}
\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}
\newcommand{\houcine}[1]{\todo[inline]{{\textbf{El houcine:} \emph{#1}}}}

\begin{document}

\title{A Meta-MARL approach for data collection and trajectory optimizations for UAVs assisted networks}


 %\author{\authorblockN{Mouhamed Naby Ndiaye\authorrefmark{1}, Hajar El Hammouti\authorrefmark{1}, ElHoucine Bergou\authorrefmark{1} }
 %\authorblockA{\authorrefmark{1}School of Computer Science, Mohammed VI Polytechnic University (UM6P), Benguerir, Morocco}}
 
\author[1]{Mouhamed Naby Ndiaye}
\author[1]{El Houcine Bergou}
% \author[2]{Mounir Ghogho}
\author[1]{Hajar El Hammouti}
\affil[1]{School of Computer Science, Mohammed VI Polytechnic University (UM6P), Benguerir, Morocco, \authorcr
emails: {\{naby.ndiaye,hajar.elhammouti,elhoucine.bergou\}@um6p.ma}}
% \affil[2]{TICLab, International University of Rabat (UIR), Rabat, Morocco,\authorcr
% email: {mounir.ghogho@uir.ac.ma}}


%  El Houcine Bergou\authorrefmark{1}, Mounir Ghogho \authorrefmark{2}, Hajar El Hammouti\authorrefmark{1} }
%  \authorblockA{\authorrefmark{1}School of Computer Science, Mohammed VI Polytechnic University (UM6P), Benguerir, Morocco,\\\authorrefmark{2}TICLab, International University of Rabat (UIR), Rabat, Morocco,}
% emails: \{naby.ndiaye,hajar.elhammouti,elhoucine.bergou\}@um6p.ma, mounir.ghogho@uir.ac.ma}


\maketitle

% ===================
% # I. Abstract #
% ===================
\begin{abstract}
Unmanned aerial vehicle (UAV) is seen as a promising technology to accomplish a plethora of tasks in many fields. In telecommunications in particular, UAVs are often used as base stations or as data collectors. In this work we consider a deployment of a group of UAVs to collect data as fresh as possible generated by IoT devices on the ground. We formulate our problem as a multi-objective optimization problem where the main objectives are to maximize the total data collected and minimize the Age-of-Update of the system. In order to efficiently solve our optimization problem, we propose a cooperative Multi Agent Reinforcement Learning (MARL) algorithm based on the popular on-policy RL algorithm Policy Proximal Optimization (PPO). We also propose a meta-training procedure that allows UAVs to generalize their knowledge and quickly adapt to a new environment. Simulation results show that our proposed algorithm performs very well compared to the traditional off-policy MARL algorithm and, with our meta-procedure, UAVs are able to adapt quickly to unknown environments.


\end{abstract}
% \IEEEoverridecommandlockouts
% \begin{IEEEkeywords}
% 3D placements of UAVs, Age-of-Updates, Convex optimization, UE' association.
% \end{IEEEkeywords}


% \IEEEpeerreviewmaketitle







% ===================
% # I. Introduction #
% ===================

\section{Introduction}
Nowadays, Unmanned Aerials Vehicles (UAVs), due to their flexibility and manoeuvrability, offer a plethora of possibilities in the field of telecommunication~\cite{bajracharya20226g}. They can be used as base stations or as data collectors. Their agility allows them to fly close to Users Equipment (UEs), collect the data they generate and relay it to the Base Station (BS). However, despite their agility, UAVs need their trajectory to be efficiently designed to carry out their mission. Moreover, drones, like all machines, have no intelligence and are therefore not autonomous. Furthermore, as the data generated by UEs are time sensitive and may lose their value after a certain period of time, it is crucial to design the trajectory of UAVs in such a way as to collect the freshest possible information. Therefore, in this work, we present a practical framework for the deployment of autonomous UAVs capable of adapting to previously unknown environments working cooperatively to achieve several goals, which are maximising the total data collected and minimising the age of the collected data information captured by a metric called Age-of-Updtae (AoU).


\subsection{Related Work}
Many problems related to UAV-assisted networks have been studied in the existing literature ~\cite{Wang2020EnergyEfficientDC,Yuan2021NovelOT,Wang2022JointOO,Jiang2021PeakAO,Cao2021PeakAO}. On the one hand, work has focused on the design of trajectories for efficient data collection and on the other hand on the freshness of the information collected, often quantified by a metric called Age-of-Information (AoI). In the paper In\cite{Wang2020EnergyEfficientDC}, The authors propose that the UAV trajectory and the device transmission schedule be jointly optimized, reducing the maximum energy consumption of all devices while ensuring the reliability of the collected data. The authors of this paper defined their problem as a non-convex mixed integer optimisation problem and solved it effectively using a differential evolution approach. 

However, many of these works are limited to a single UAV and none of them design UAV trajectories in cooperative mode. In this perspective, a handful of works have been done on the cooperation of UAVs to accomplish their mission, usually based on multi-agent reinforcement learning (MARML)~\cite{Cui2020MultiAgentRL,Seid2021MultiAgentDF}. The authors in\cite{Cui2020MultiAgentRL} formulate their problem as a stochastic game to maximise expected rewards and propose a MARL-based solution to solve it. Similarly in\cite{Seid2021MultiAgentDF} formulate their problem as a natural extension of the Markov decision process (MDP) concerning stochastic game, to minimize the long-term computation cost in terms of energy and delay, and propose a multi-agent deep reinforcement learning (MADRL)-based approach to ensure the quality of service (QoS) requirements of IoT devices while minimizing the overall network computation cost.

Nevertheless, most existing MARL-based solutions as those proposed, agents can search for strategies independently based on their own actions and states, as shown in the MARL-based solutions proposed in\cite{Hu2020CooperativeIO,Lin2021DecentralizedPD}. However, as agents act in a decentralised manner and only optimise their individual goals, this type of solution will not allow drones to maximise the total data collected nor minimise the overall AoU of the system. In contrast, the solutions proposed in~\cite{Hu2021DistributedMM,Hu2022DistributedAD} allow agents to share neither their states, nor their rewards, nor their strategies, while achieving the global goals. However in the real life, particularly in tasks which need collaboration, agents need to communicate for achieving their mission. These types of solutions that allow agent to share knowledge when they learn and act independently, are generally based on the Centralised Learning and Decentralised Execution (CTDE)\cite{Wong2021MultiagentDR} paradigm , where agents learn centrally and act independently. 

Furthermore, in general, within the MARL paradigm, Policy Gradient (PG) methods are generally considered less efficient in terms of sampling than Value Decomposition (VD) methods~\cite{Hu2021DistributedMM}, which are not policy-based, due to their nature. However, some recent empirical studies~\cite{Witt2020IsIL,Papoudakis2021BenchmarkingMD} show that with appropriate hyper-parametric parameters and input representation, PG method can achieve surprisingly high performance compared to non-policy based VD methods. These can learn a blending function that blends local Q nets into a global Q function. To satisfy the Individual-Global-Max (IGM) principle, which ensures that greedily choosing the optimal action locally for each agent, the optimal joint action can be computed, this mixture function is usually applied. In contrast, PG methods directly apply the policy gradient to learn a centralized value function for each agent and an individual policy. For an accurate estimate of the global value, the value function can take as input the concatenation of all local observations, or the global state.

Moreover, the traditional solutions proposed, based on RL, do not allow agents to adapt quickly and efficiently to unknown environments, as they are often too adapted to the tasks they have been trained for. Indeed, in order for these algorithms to adapt to learning tasks, hyper-parameters and initialization are adapted manually. Thus, when the agents are confronted with a new environment, in other words an unknown task, these algorithms converge very slowly or may even fail to converge.  In contrast, in the work ~\cite{Hu2021DistributedMM,Zhou2022MultiagentFM}, the proposed solutions allow agents to easily adapt to unknown environments through model-agnostic meta-learning. By meta-training agents on multiple tasks and not on a single task as in traditional RL solutions, the solutions proposed in ~\cite{Hu2021DistributedMM,Zhou2022MultiagentFM} allow rapid adaptation of drones to unknown environments.

Finally, none of these works consider a multi-objective optimization problem. In most of these works, agents learn to achieve a single goal. In this work, we propose a framework in which agents learn to achieve several goals at the same time in a collaborative way. Our proposed MARL algorithm use the actor critic architecture, where the critic is central (centralized value function) and the actor is decentralized (individual policy for each UAVs).  Furthermore, we propose a meta-training procedure that allows UAVs to adapt quickly to unknown environments.


\subsection{Contribution}
The major contribution of this paper is a new distributed framework for designing the trajectories of a swarm of cooperative UAVs in highly dynamic and especially unpredictable environments. Our major contributions can be listed in the points below:
\begin{itemize}

    \item We consider a practical UAV-assisted network system in which a team of UAVs is deployed to act as a relay between low-capacity user equipment and remote BS in order to collect as much fresh data generated by these user equipment on the ground as possible, despite strict mission time constraints and limited information on their environments. The UAVs can collect data generated by UEs while adapting their trajectory to minimize the system's AoU. The problem is formulated as a multi-objective optimization problem, which turns out to be non-convex. To solve this formulated non-convex multi-objective optimization problem, we propose a distributed reinforcement learning algorithm based on the popular single-agent reinforcement learning algorithm called Proximal Policy Optimization (PPO). The proposed algorithm, MAPPO-TO-AoU, is built on the Central Training and Decentralized Execution (CTDE) paradigm based on the actor-critic architecture, where a critic learns a central value function and an actor finds the local optimal policy. This proposed MAPPO-TO-AoU algorithm allows UAVs to find a local optimal solution to the studied multi-objective problem using only their own local states information. 
    
    \item In order to find an optimal policy and value function initialization with appropriate estimation over a multitude of environments, we propose to meta-train our proposed MAPPO-TO-AoU algorithm using model-agnostic meta-learning (MAML). This meta-training procedure improves the convergence speed of the MAPPO-TO-AoU algorithm for unseen environments. The advantage of this meta-training procedure is that it is less complex than other meta-training procedures~\cite{ritter2018been} because it does not require additional neural networks. Thus, using this meta-training approach, drones are able to cope with various environments (handle various tasks) instead of just one, due to the use of meta-training initialization. The simulation results show that the proposed MAPPO-TO-AoU algorithm requires fewer iterations to converge and obtain higher rewards compared to the existing traditional off-policy MARL solutions used as benchmark. In addition with MAPPO-TO-AoU, UAVs collect more data, quickly minimize the AoU of the system and serve more UEs. The results also show that our meta-trained MAPPO-TO-AoU improves the MAPPO-TO-AoU algorithm by converging faster and achieving even higher rewards and data collected. 
    
\end{itemize}




\subsection{Organization}
The remainder of the paper is organized as follows. First, the studied system model is described in Section~\ref{Sys}. The mathematical formulation of the problem is given in Section~\ref{Prob}. In Section~\ref{Algo}, we describe the joint association probability and 3D locations to solve the underlying optimization efficiently. Next, in Section~\ref{Simu}, we show the performance of our algorithm for various scenarios. Finally, concluding remarks are provided in Section~\ref{Conc}.

% =============================================
% # II. System model #
% =============================================

\section{System Model}\label{Sys}

We consider a system concerning of set $\mathcal{I}$ of $I$ UEs and a set $\mathcal{U}$ of $U$  UAVs. Since the BS is far from UEs, a team of UAVs which acted as data collector are deployed. Each UAVs must collect data and reach it's final position which are located at $(x_{u,F},y_{u,F})$.  For avoiding collision each UAV fly as the same altitude given by $H_{u}$. Thus, at the time step $t$, the location of the UAV $u$ is given by $(x_{u}[t], y_{u}[t], z_{u}[t])$. The location of UE $i$ is given by, $(x_{i}, y_{i}, 0)$. Therefore the distance between UAV $u$ and user $i$, at time step $t$ is given by 
\begin{equation*}
    d_{i,u}[t]=\sqrt{(x_{u}[t]-x_{i})^2+(y_{u}[t]-y_{i})^2+(H_{u}[t])^2}.
\end{equation*}
We assume that the initial position of UAVs is the Center and Docker Station (CDS) located at $(x_{u,CDS},y_{u,CDS})$. We also assume that the speed of UAVs are constant and denoted by $V$. UAVs must find a global and local strategy to collect the maximum amount of data possible while ensuring the freshness of the data collected. Let $S_{u}$ is the set of visited location by the $u$-th UAV, the time needed to fly from one location to another is given by: 
\begin{equation*}
    T^{C}_{u}=\sum_{t=1}^{\left| S_{u} \right|}\frac{\sqrt{(x_{u}[t]-x_{u}[t-1])^2+(y_{u}[t]-y_{u}[t-1])^2}}{V}
\end{equation*}

\subsection{Communication Model}
Each UAV communicates with user via the air-to-ground channel. To model the air-to-ground channel, we assume a Rician fading distribution $\widehat{\Xi}_{i,u}[t]$
\begin{equation*}
    \widehat{\Xi}_{i,u}[t]=\left(\sqrt{\frac{\Phi}{\Phi+1}} \bar{\Xi}_{i,u}[t]+\sqrt{\frac{1}{\Phi+1}} \widetilde{\Xi}_{i,u}[t]\right),
\end{equation*}
where $\Phi$ is the Rician factor, $\bar{\Xi}_{i,u}[t]$ is the line-of-sight (LoS) component with $\left|\bar{\Xi}_{i,u}[t]\right|=1$, and $\widetilde{\Xi}_{i,u}[t]$ the random non-line-of-sight (NLoS) component.

Each user $i$ transmits its updates with a power $P_i[t]$ during time interval $t$. Thus, the received power at UAV $u$ is $\left| \widehat{\Xi}_{i,u}[t]\right|^2\beta_{0}\left(d_{i,u}[t]\right)^{-2}P_i[t]$, where  $\beta_{0}$ is the average channel power gain at a reference distance $d_{0}=1 \mathrm{~m}$.

The signal-to-noise ratio (SNR) of IoT device $i$ with respect to UAV $u$ is given by
 \begin{equation*}
     \Theta_{i,u}[t]= P_{i}[t]\left|\widehat{\Xi}_{i,u}[t]\right|^{2} \beta_{0}\left(d_{i,u}[t]\right)^{-2} / \sigma^{2}, 
 \end{equation*}
 where $\sigma^{2}$ is the thermal noise power. Therefore, the rate of IoT device $i$ when it is associated with UAV $u$ during time slot $k$ can be written
 
 \begin{equation*}
     \overline{R}_{i, u}[t]=B_{i,u}[t]\log _{2}\left(1+\Theta_{i,u}[t]\right),
 \end{equation*}
where $B_{i,u}[t]$ is the allocated bandwidth between device $i$ and UAV $u$ during time slot $t$. 

\subsection{Age-of-Updates Metric}
The objective of this work is to maximize the total data collected, while maximizing the freshness of the data and ensure that all UAVs complete their mission. In particular, our aim is to minimize the AoU.

Let $a_{i,u}[t]$ be a binary variable equal to $1$ where the UAV $u$ is associate with UE $i$ at time slot $t$.

To model the updates' freshness, we use the same definition as in \cite{yang2020age}. Particularly, if device $i$ is associated with UAV $u$ during time interval $t$, its AoU evolves as follows,

\begin{equation*}
    T_{i,u}[t]=\left(T_{i,u}[t-1]+1\right)\left(1-a_{i,u}[t-1]\right),
    \label{Tiu}
\end{equation*}

where $T_{i,u}[0]=0$. $T_{i,u}[t]$ is the age of the updates received by drone $u$ and collected from device $i$ during time interval $t$. Specifically, when the updates of device $i$ are not collected during time interval $t$ (i.e., $a_{i,u}[k]=0$), the AoU is increased by one unit of time. Inversely, when the updates are transmitted, the AoU is reinitialized to zero. 
% =============================================
% # III. Problem formulation #
% =============================================
\section{Problem Formulation }\label{Prob}
In this work, our aim is to optimize the joint trajectory optimization, data collection and AoU. In this kind of problem, the objective must be designed to be dependent on both the total sum-rate of ground UE associated with UAVs and the AoU of collected data. 

Thus the problem can be seen as a multi objective optimization problem. The objective of each UAVs at each time slot is to maximize the following:
\begin{equation*}
    \mathfrak{r}_{u}[t]=\sum_{i}^{I}\lambda a_{i,u}[t] \overline{R}_{i, u}[t]- (1-\lambda)  T_{i,u}[t]
\end{equation*}


Thus the objective of the UAVs team is given by:
\begin{equation}
   \sum_{u}^{U} \sum_{t}^{T} r_{u}[t]
\end{equation}
The optimization problem formulated as game is as follow:
\begin{subequations}\label{prob:main}
\begin{align}
\begin{split}
\begin{aligned}
\max_{\mathbf{a},\mathbf{x},\mathbf{y}} \sum_{t}^{T}\sum_{u}^{U}\sum_{i}^{I} \Bigg(\lambda a_{i,u}[t] \overline{R}_{i, u}[t]+ (1-\lambda)  T_{i,u}[t] \Bigg)
\end{aligned}
\label{third:a}
\end{split}\\
\begin{split}
 \text{s . t.} \quad R_{i, u}[t] >= R_{min}, \forall i \in \mathcal{I}, u \in U, \forall t \in T
 \label{third:b}
\end{split}\\
\begin{split}
 \sum_{u \in U}a_{i,u}[t]\le 1, \; \forall i \in \mathcal{I}, \forall k \in \mathcal{K}
  \label{third:c}
\end{split}\\
\begin{split}
a_{i,u}[t] \in {0,1}, \forall i \in \mathcal{I}, u \in U, \forall t \in T
  \label{third:d}
\end{split}\\
\begin{split}
T_{u}^{c} \le T, \forall  u \in U
  \label{third:e}
\end{split}\\
\begin{split}
    (x_{u}[T], y_{u}[T]) = (x_{u,F}, y_{u,F}),\forall u \in U
  \label{third:f}
\end{split}\\
\begin{split}
    x_{u\text{min}}\le x_{u}[t] \le x_{u\text{max}},\forall u \in U, \forall t \in T
  \label{third:g}
\end{split}\\
\begin{split}
    y_{u\text{min}}\le y_{u}[t] \le y_{u\text{max}},\forall u \in U, \forall t \in T
  \label{third:h}
\end{split}
\end{align}
\label{PF}
\end{subequations}
where $\mathbf{a}$ are the UAVs-UE association vectors. In this formulation, the constraint (\ref{third:b}) ensures that the uplink rate is above a predefined threshold $R_{min}$, so that data is transmitted on time. The constraint (\ref{third:c}) ensures that no more than one drone serves the same IoT device at the same time. The constraint (\ref{third:d}) ensures that each UAV complete their mission within a predefined time. Moreover each UAV need to reach their final position at the end of the predefined time.  Finally, the constraints (\ref{third:g}) and (\ref{third:h}) ensure to limit the movement of the UAVs. 


% ==================
% # IV. Multi Agent Reinforcement Learning
% ==================
\section{Proposed Approach}
\subsection{Muti-Agent Proximal Policy Optimization Algorithm for trajectory and AoU optimization (MAPPO-TO-AoU)}
In this section we describe the proposed algorithm to solve the multi-objective optimisation and the meta procedure used two generalize the knowledge of UAVs for quickly adapt in unseen environment.

The reward gleaned by each agent in a multi-agent system is not only related to its own actions but also to those of other agents.  The selection of the optimal policy of the other agents will be affected by the change of the policy of an agent and the estimation of the value function will be imprecise, so it will be difficult to ensure the convergence of the algorithm. In order to solve this problem, MAPPO\cite{yu2021surprising} uses the idea of CTDE as shown in figure \ref{fig:ctde} and extend the Proximal Policy Optimization Algorithm (PPO)\cite{schulman2017proximal} for the multiagent system. The Critic network learns a central value function. It can observe global information, including information from other agents and environmental information. In the case of the Actor network, each agent calculates the policy only from its own local observations.  

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{images/CTDE.png}
    \caption{Example of CTDE}
    \label{fig:ctde}
\end{figure}
The proposed MAPPO-TO-AoU algorithms consist of:
\begin{itemize}
    \item \textbf{Agents}: $\mathcal{U}$ is the set of agent represented by the UAVs swarm. Each UAV in the swarm is an agent.
    
    \item \textbf{States}: All UAVs have a state constituted by their location. Thus the set of state of all UAVs is $\mathcal{S}=\{\boldsymbol{S}[t], t=1\ldots T\}$. The state of the UAV $u$ at the time step $t$ is $s_{u}[t]=\{x_{u}[t],y_{u}[t]\}$ and the global state of UAVs at timestep $t$ is $\boldsymbol{S}[t]=[{s}_{u}[t], u=1, \ldots ,U]$.
    
    \item \textbf{Actions}: The action of each UAV is the UE that it seeks to collect data or return to CDS after completing their mission. Each UAVs can move along $x$ or $y$ axis. At time step $t$ the action taken by UAV $u$ is given by $a_u[t] \in \left\{ 0,1,2,3,4 \right\}$, where action $0$ is to stay as its position and action $1,2,3,4$ design respectively going to UP, DOWN, RIGHT, and LEFT. The joint action take by UAVs at timestep $t$ is, $\boldsymbol{a}[t]=[a_{u}[t], u=1, \ldots, U]$

    \item  \textbf{Rewards}: The team global reward $r(\boldsymbol{a}, \mathcal{S})$ represent the total AoU of all UAVs as defined in . Hence the reward of each 
    
    \item \textbf{Policy function}: We define a policy function $\boldsymbol{\pi}_{\boldsymbol{\theta}}(a_{u},s_{u})$ represented by the actor network and parameterised by $\boldsymbol{\theta}$, it is used to generate the UAV strategy. The agent share the same network but each UAV $u$ takes action based on its own local state $s_{u}$, and optimize its accumulated discounted accumulated reward $J(\theta)=\mathbb{E}_{a[t], s[t]}\left[\sum_t \gamma^t R\left(s[t], a[t]\right)\right]$.

    \item \textbf{Value function}: The centralized value function $V_{\boldsymbol{\phi}}(\boldsymbol{S})$ is represented by the critic network and parameterised by $\boldsymbol{\phi}$. These network is used to estimate the achievable future reward by UAVs at every state $\boldsymbol{S}$. The goal of the critic network is to find the global optimal strategy that maximizes the expected reward. 
\end{itemize}

UAVs interact with their environment by selecting actions based on the selected strategy generated by $\boldsymbol{\pi}_{\boldsymbol{\theta}}$. In each step the UAVs select actions and receive the team global reward and store its experiences.

The implementation of MAPPO-TO-AoU closely resembles the structure of PPO in single-agent settings by learning a policy $\boldsymbol{\pi}_{\boldsymbol{\theta}}$ and a value function $V_{\boldsymbol{\phi}}(\boldsymbol{S})$. These two functions are represented as two separate networks, respectively an actor network and a critic network.

$V_{\boldsymbol{\phi}}(\boldsymbol{S})$ is used for variance reduction and is only utilized during training; hence, it can take as input extra global information not present in the agentâ€™s local state, allowing PPO in multi-agent domains to follow the CTDE structure. If these networks are homogenous, they can be shared by all agents, but each agent can also have a unique collection of actor and critic networks. For the purpose of notational simplicity, we assume that all UAVs share the critic and actor networks.

In, MAPPO-TO-AoU algorithm, The actor network is trained to maximize
\begin{equation*}
    \mathcal{L}(\boldsymbol{\theta})=\frac{1}{UB} \sum_{b=1}^B \sum_{u=1}^U \left[\min \left(r_{\boldsymbol{\theta}, u}^{b} A_{u}^{b}, \operatorname{clip}\left(r_{\boldsymbol{\theta},u}^{b}, 1-\epsilon, 1+\epsilon\right) A_{u}^{b}\right)+\sigma E\left[\pi_\theta\left(s_{u}^{b}\right)\right)\right]
\end{equation*}
where $r_{\boldsymbol{\theta},u}^{b}=\frac{\pi_\boldsymbol{\theta}\left(a_{u}^{b} \mid s_{u}^{b}\right)}{\pi_{\boldsymbol{\theta}_{old}}\left(a_{u}^{b} \mid s_{u}^{b}\right)}$. The advantage $A_{u}^{b}$  is computed using the GAE method, $E$ is the policy entropy, and $\sigma$ is the entropy coefficient hyperparameter.

The critic network is trained to minimize the loss function 
\begin{equation*}
    \mathcal{L}(\boldsymbol{\phi})=\frac{1}{B U} \sum_{b=1}^B \sum_{u=1}^U\max \left[\left(V_{\boldsymbol{\phi}}\left(s_{u}^{b}\right)-\hat{r}^{b}\right)^2,\left(\operatorname{clip}\left(V_{\boldsymbol{\phi}}\left(s_{u}^{b}\right), V_{\boldsymbol{\phi}_{\text {old }}}\left(s_{u}^{b}\right)-\varepsilon, V_{\boldsymbol{\phi}_{\text {old }}}\left(s_{u}^{b}\right)+\varepsilon\right)-\hat{r}^{b}\right)^2\right]
\end{equation*}
where $\hat{r}^{b}$ is the discounted reward-to-go. 

The proposed MAPPO-TO-AoU solution is summarized in algorithm \ref{alg:MAPPO}.

\begin{algorithm}[h]
\caption{MAPPO-TO-AoU}\label{alg:MAPPO}
\begin{algorithmic}[1]
\State{\textbf{Input: } User positions, $T$ and $R_{\text{min}}$.}
\State{\textbf{Init: }  Initialize actor network parameter $\boldsymbol{\phi}$, and initial critic network parameter $\boldsymbol{\theta}$.}
\For{training epoch e=1:E}
    \State{Initialize data Buffer D}
    \For{b=1:B}
        \For{t=1:T}
             \State{UAVs Carry experience by executing joint action $\boldsymbol{a}[t]$ and observe reward $r[t]$ and the next state $\boldsymbol{s}[t+1]$} using initial policy $\boldsymbol{\pi}_{\boldsymbol{\theta}}$
            \State{Store experience $\boldsymbol{e}[t]$ in $\tau$}
         \EndFor       
        \State{Compute Advantage using GAE on experience in $\tau$}
        \State{Compute Discounted reward on experience in $\tau$}
        \State{Store $\tau$ in D}
    \EndFor
    \State{Update $\boldsymbol{\theta}$ with data from random mini-batch sample on D}
    \State{Update $\boldsymbol{\phi} $ with data from random mini-batch sample on D}
\EndFor
\end{algorithmic}
\end{algorithm}


\subsection{Meta training}
In this section we present our proposed meta training approach that meta train our MAPPO-TO-AoU solution using model agnostic meta-learning (MAML)\cite{finn2017model}. The meta training procedure will allow the agent to reach quickly the optimal team strategy in various environment. During the meta training an environment is created using sample $\boldsymbol{p}_m$ drawn from $\mathcal{P}=[\boldsymbol{T},\boldsymbol{R_{min}}]$. Then UAVs collect experiences $\boldsymbol{e}_{m}$ using their initial policy $\boldsymbol{\pi}_{\boldsymbol{\theta},m}$ and update their policy and value function parameterized respectively by $\boldsymbol{\theta}^{\prime}_{m}$ and $\boldsymbol{\phi}^{\prime}_{m}$ using algorithm \ref{alg:MAPPO}. Then in a new experiences $\boldsymbol{e}^{\prime}_{m}$ collected using $\boldsymbol{\pi}_{\boldsymbol{\theta}^{\prime},m}$ MAPPO-TO-AoU test the updated policy and value function at current environment sampling $\boldsymbol{p}_c$. The values of the loss functions that measure the distance between the policy and the updated value functions that produce optimal team strategies, relative to the policy and the optimal value functions, constitute the feedback from these tests to the UAVs. These updates are done as follows:
\begin{equation}
\mathcal{L}(\boldsymbol{\theta}^{\prime}_{m})=\frac{1}{UB} \sum_{b=1}^B \sum_{u=1}^U \left[\min \left(r_{\boldsymbol{\theta}^{\prime}_{m}, u}^{b} A_{u}^{b}, \operatorname{clip}\left(r_{\boldsymbol{\theta}^{\prime}_{m},u}^{b}, 1-\epsilon, 1+\epsilon\right) A_{u}^{b}\right)+\sigma E\left[\pi_{\boldsymbol{\theta}^{\prime}_{m}}\left(s_{u}^{b}\right)\right)\right]
\label{policy_update_meta}
\end{equation}

\begin{equation}
    \mathcal{L}(\boldsymbol{\phi}^{\prime}_{m})=\frac{1}{B U} \sum_{b=1}^B \sum_{u=1}^U\max \left[\left(V_{\boldsymbol{\phi}^{\prime}_{m}}\left(s_{u}^{b}\right)-\hat{r}^{b}\right)^2,\left(\operatorname{clip}\left(V_{\boldsymbol{\phi}^{\prime}_{m}}\left(s_{u}^{b}\right), V_{\boldsymbol{\phi}^{\prime}_{m,\text {old }}}\left(s_{u}^{b}\right)-\varepsilon, V_{\boldsymbol{\phi}^{\prime}_{m,\text {old }}}\left(s_{u}^{b}\right)+\varepsilon\right)-\hat{r}^{b}\right)^2\right]
\label{value_update_meta}
\end{equation}

The initial critic and actor parameter are updated as follow:
\begin{equation}
    \mathcal{L}(\boldsymbol{\phi})=\sum_{\boldsymbol{p}_m \sim \mathcal{P}}\mathcal{L}({\boldsymbol{\phi}^{\prime}_{m}})
     \label{update_meta_phi}
\end{equation}

\begin{equation}
    \mathcal{L}(\boldsymbol{\theta})=\sum_{\boldsymbol{p}_m \sim\mathcal{P}}\mathcal{L}^{\mathrm{CLIP}}(\boldsymbol{\theta}^{\prime}_{m})
\label{update_meta_theta}
\end{equation}

The proposed Meta-MAPPO-TO-AoU solution is summarized in algorithm 


\begin{algorithm}[h]
\caption{META-MAPPO-TO-AoU}\label{alg:MAPPO}
\begin{algorithmic}[1]
\State{\textbf{Input: } User positions, $T$ and $R_{\text{min}}$.}
\State{\textbf{Init: }  Initialize actor network parameter $\boldsymbol{\phi}$, and initial critic network parameter $\boldsymbol{\theta}$.}
\For{training epoch e=1:E}
    \For {environment sample m=1:M}
        \State{sample $\boldsymbol{p}_m \sim \mathcal{P}$.}
        \State{Initialize data Buffer D.}
        \For{b=1:B}
            \For{t=1:T}
                 \State{UAVs carry experience using initial policy $\boldsymbol{\pi}_{\boldsymbol{\theta}}$.}
                \State{Store experience $\boldsymbol{e}_{m}[t]$ in $\tau$.}
             \EndFor       
            \State{Compute Advantage using GAE on experience in $\tau$.}
            \State{Compute reward-to-go on experience in $\tau$.}
            \State{Store $\tau$ in D.}
            \State{Update value function using (\ref{value_update_meta}) with data from random mini-batch sample on D.}
        \State{Update policy function using (\ref{policy_update_meta}) with data from random mini-batch sample on D.}
            \For{t=1:T}
                  \State{UAVs carry experience using initial policy $\boldsymbol{\pi}^{\prime}_{\boldsymbol{\theta}_{m}}$.}
                \State{Store experience $\boldsymbol{e}^{\prime}_{m}[t]$ in $\tau$.}
             \EndFor  
             \State{Store $\tau$ in D.}
        \EndFor 
             \State{Compute loss using $\mathcal{L}({\boldsymbol{\phi}^{\prime}_{m}})$ and $\mathcal{L}^{\mathrm{CLIP}}(\boldsymbol{\theta}^{\prime}_{m})$  using (\ref{policy_update_meta}) and (\ref{value_update_meta}) with data from random mini-batch sample on D.}
   \EndFor 
   \State{Update $\boldsymbol{\theta}$ with data from random mini-batch sample on D using (\ref{update_meta_theta}).}
    \State{Update $\boldsymbol{\phi} $ with data from random mini-batch sample on D using (\ref{update_meta_phi}).}
\EndFor
\end{algorithmic}
\end{algorithm}

% \subsubsection{Monotonic Value Function Factorisation (QMIX)}

% The VDN algorithm is improved by the QMIX algorithm by giving a more general form to the constraint by defining it as 
% $$
% \frac{\partial Q_{t o t}}{\partial Q_a} \geq 0, \forall a
% $$
% where $Q_{text {tot }}$ is the joint value function and $Q_a$ is the value function for each agent. 
% Intuitively we can notice that the QMIX algorithm wants the weights of any individual value function $Q_a$ to be positive. Since the higher $Q_a$ is, the lower the joint value $Q_{t o t}$ is, if the weights of the individual value function $Q_a$ are negative, then this will discourage the agent from cooperating. The VDN constraint is trivially just a special case that $\frac{\partial Q_{\text {tot }}}{\partial Q_a}=1$.


% ==================
% # V. Simulation result #
% ==================
\begin{figure*}[t]
\centering
\minipage{0.33\textwidth}
  \includegraphics[width=1\linewidth]{images/reward010405.png}
   \caption{Mean reward over iterations}
    \label{fig:reward010405}
\endminipage\hfill
\minipage{0.33\textwidth}
  \includegraphics[width=1\linewidth]{images/data_collected010405.png}
   \caption{Mean data collected over iterations}
    \label{fig:data_collected010405}
\endminipage \hfill
\minipage{0.33\textwidth}
  \includegraphics[width=1\linewidth]{images/AoU010405.png}
   \caption{Mean AoU over iterations}
    \label{fig:AoU010405}
    \endminipage
\caption{$\lambda_{1}=0.1$, $\lambda_{2}=0.4$ and $\lambda_{3}=0.5$}
\label{010405}
\end{figure*}

\begin{figure*}[t]
\centering
\minipage{0.33\textwidth}
  \includegraphics[width=1\linewidth]{images/reward010603.png}
    \caption{Mean reward over iterations}
    \label{fig:reward010603}
\endminipage\hfill
\minipage{0.33\textwidth}
  \includegraphics[width=1\linewidth]{images/data_collected010603.png}
    \caption{Mean data collected over iterations}
    \label{fig:data_collected010603}
\endminipage \hfill
\minipage{0.33\textwidth}
  \includegraphics[width=1\linewidth]{images/AoU010603.png}
    \caption{Mean AoU over iterations}
    \label{fig:AoU010603}
    \endminipage
    \caption{$\lambda_{1}=0.1$, $\lambda_{2}=0.6$ and $\lambda_{3}=0.3$}
\label{010603}
\end{figure*}
\section{Simulation Results}
\begin{table}[t]
\begin{tabular}{|l|l|l|l|}
\hline
$I$           & $20$       & $U$                   & $3$           \\ \hline
$R_{min}$     & $100000$   & $\lambda_{3}$         & $0.4$         \\ \hline
$x$           & $5$        & $\beta_{0}$           & $1$           \\ \hline
$y$           & $5$        & $B_{i,u}$             & $[1500,1700]$ \\ \hline
$H$           & $[80,100]$ & $\alpha$              & $2$           \\ \hline
$\lambda_{1}$ & $0.3$      & $\sigma^{2}$          & $1e^{-15}$    \\ \hline
$\lambda_{2}$ & $0.3$      & $T$                   & $100$         \\ \hline
$P_{i}$       & $[0,1]$    & $\widehat{\Xi}_{i,u}$ & $[0,1]$       \\ \hline
\end{tabular}
\caption{Experiment setup}
\label{tab:my-table-parameter}
\end{table}
In order to evaluate the performance of our algorithm, we perform a simulation with the parameters described in the table \ref{tab:my-table-parameter}. We performed simulations on two sets of lambda as shown in figure \ref{010405} and \ref{010603}, where we plot the reward gleaned by the UAVs over iterations, as well as the total data collected and the AoU of the system. The results show that, in both situations, our algorithm converges faster and achieves higher rewards (figure \ref{fig:reward010405} and \ref{010603}), also the results show that the UAVs can find quickly a global strategy that minimises the AoU (figure \ref{fig:AoU010405} and \ref{fig:AoU010603}) and maximises the total data collected by the UAVs (figure \ref{fig:AoU010405} and \ref{fig:AoU010603}), compared to the algorithms used as a benchmark. This confirms the fact that on-policy methods can perform very well when solving tasks where cooperation is required.  Furthermore, our meta-training procedure shows that the agents adapt very quickly to a new environment and manage to collect even more data while minimising the AoU of the system.  With these two sets of lambda, we notice that the AoU and the total data collected sometimes follow the same evolution. The AoU formulation allows us to say that the AoU is minimal when the number of users served is maximal and furthermore it can be said that serving more users is synonymous with collecting more data. But as in our case the IoT devices generate different data over time, the total data collected can increase or decrease without the AoU increasing or decreasing, we can see this in figures \ref{fig:AoU010405} and \ref{fig:data_collected010405} where the AoU remains constant over time, but the total data collected fluctuates. This shows that drones can find an overall strategy to serve all users of the system but are not always sure to collect the same amount of data.

For meta learning, we sample $100$ tasks for $\boldsymbol{R_{min}}=[100000, 120000]$ and $\boldsymbol{T}=[100, 200]$. The results presented in figures \ref{fig:reward_meta}, \ref{fig:data_collected_meta} and \ref{fig:AoU_meta} show that our meta-training procedure converges quickly and achieves the best rewards among all algorithms.


% ==================
% V. Conclusion #
% ==================

\section{Conclusion}\label{Conc}
In this work, we develop a practical framework in which drones can act cooperatively to achieve multiple objectives. We formulate our problem into a multi-objective optimization problem that is shown to be non-convex. We then propose a solution based on MARML to efficiently solve our formulated problem. In order to enable our UAVs to perform well in unknown environments, we propose a meta-procedure that allows meta-training of UAVs. Simulation results show that our proposed solution converges quickly and solves the task better than the reference solution. As we used a solution based on the actor-critic framework, the extension of this work is to consider the optimisation of the 3D position of the UAVs rather than the 2D position.



% ==============
% # REFERENCES #
% ==============
 \balance
\newpage
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,biblio_traps_dynamics}

\end{document}
