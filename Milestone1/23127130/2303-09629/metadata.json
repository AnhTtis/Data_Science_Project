{
    "arxiv_id": "2303.09629",
    "paper_title": "Online Reinforcement Learning in Periodic MDP",
    "authors": [
        "Ayush Aniket",
        "Arpan Chattopadhyay"
    ],
    "submission_date": "2023-03-16",
    "revised_dates": [
        "2023-03-20"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "abstract": "We study learning in periodic Markov Decision Process (MDP), a special type of non-stationary MDP where both the state transition probabilities and reward functions vary periodically, under the average reward maximization setting. We formulate the problem as a stationary MDP by augmenting the state space with the period index, and propose a periodic upper confidence bound reinforcement learning-2 (PUCRL2) algorithm. We show that the regret of PUCRL2 varies linearly with the period $N$ and as $\\mathcal{O}(\\sqrt{Tlog T})$ with the horizon length $T$. Utilizing the information about the sparsity of transition matrix of augmented MDP, we propose another algorithm PUCRLB which enhances upon PUCRL2, both in terms of regret ($O(\\sqrt{N})$ dependency on period) and empirical performance. Finally, we propose two other algorithms U-PUCRL2 and U-PUCRLB for extended uncertainty in the environment in which the period is unknown but a set of candidate periods are known. Numerical results demonstrate the efficacy of all the algorithms.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.09629v1"
    ],
    "publication_venue": "arXiv admin note: substantial text overlap with arXiv:2207.12045"
}