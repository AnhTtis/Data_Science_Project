@article{riis1965discounted,
  title={Discounted Markov programming in a periodic process},
  author={Riis, Jens Ove},
  journal={Operations Research},
  volume={13},
  number={6},
  pages={920--929},
  year={1965},
  publisher={INFORMS}
}

@article{su1972generalization,
  title={Generalization of White's method of successive approximations to periodic Markovian decision processes},
  author={Su, Shiaw Y and Deininger, Rolf A},
  journal={Operations Research},
  volume={20},
  number={2},
  pages={318--326},
  year={1972},
  publisher={INFORMS}
}

@article{veugen1983numerical,
  title={The numerical exploitation of periodicity in Markov decision processes},
  author={Veugen, LMM and van der Wal, J and Wessels, J},
  journal={Operations-Research-Spektrum},
  volume={5},
  number={2},
  pages={97--103},
  year={1983},
  publisher={Springer}
}

@inproceedings{hu2014near,
  title={Near-optimality bounds for greedy periodic policies with application to grid-level storage},
  author={Hu, Yuhai and Defourny, Boris},
  booktitle={2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},
  pages={1--8},
  year={2014},
  organization={IEEE}
}

@article{auer2008near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@article{gajane2018sliding,
  title={A sliding-window algorithm for markov decision processes with arbitrarily changing rewards and transitions},
  author={Gajane, Pratik and Ortner, Ronald and Auer, Peter},
  journal={arXiv preprint arXiv:1805.10066},
  year={2018}
}

@inproceedings{li2019online,
  title={Online learning for markov decision processes in nonstationary environments: A dynamic regret analysis},
  author={Li, Yingying and Li, Na},
  booktitle={2019 American Control Conference (ACC)},
  pages={1232--1237},
  year={2019},
  organization={IEEE}
}

@inproceedings{ortner2020variational,
  title={Variational regret bounds for reinforcement learning},
  author={Ortner, Ronald and Gajane, Pratik and Auer, Peter},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={81--90},
  year={2020},
  organization={PMLR}
}

@article{fei2020dynamic,
  title={Dynamic regret of policy optimization in non-stationary environments},
  author={Fei, Yingjie and Yang, Zhuoran and Wang, Zhaoran and Xie, Qiaomin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6743--6754},
  year={2020}
}

@inproceedings{domingues2021kernel,
  title={A kernel-based approach to non-stationary reinforcement learning in metric spaces},
  author={Domingues, Omar Darwiche and M{\'e}nard, Pierre and Pirotta, Matteo and Kaufmann, Emilie and Valko, Michal},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3538--3546},
  year={2021},
  organization={PMLR}
}
@inproceedings{mao2021near,
  title={Near-Optimal Model-Free Reinforcement Learning in Non-Stationary Episodic MDPs},
  author={Mao, Weichao and Zhang, Kaiqing and Zhu, Ruihao and Simchi-Levi, David and Basar, Tamer},
  booktitle={International Conference on Machine Learning},
  pages={7447--7458},
  year={2021},
  organization={PMLR}
}
@article{zhou2020nonstationary,
  title={Nonstationary reinforcement learning with linear function approximation},
  author={Zhou, Huozhi and Chen, Jinglin and Varshney, Lav R and Jagmohan, Ashish},
  journal={arXiv preprint arXiv:2010.04244},
  year={2020}
}

@inproceedings{cheung2020reinforcement,
  title={Reinforcement learning for non-stationary markov decision processes: The blessing of (more) optimism},
  author={Cheung, Wang Chi and Simchi-Levi, David and Zhu, Ruihao},
  booktitle={International Conference on Machine Learning},
  pages={1843--1854},
  year={2020},
  organization={PMLR}
}

@article{touati2020efficient,
  title={Efficient learning in non-stationary linear markov decision processes},
  author={Touati, Ahmed and Vincent, Pascal},
  journal={arXiv preprint arXiv:2010.12870},
  year={2020}
}
@inproceedings{wei2021non,
  title={Non-stationary reinforcement learning without prior knowledge: An optimal black-box approach},
  author={Wei, Chen-Yu and Luo, Haipeng},
  booktitle={Conference on Learning Theory},
  pages={4300--4354},
  year={2021},
  organization={PMLR}
}
@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}
@inproceedings{bourel2020tightening,
  title={Tightening exploration in upper confidence reinforcement learning},
  author={Bourel, Hippolyte and Maillard, Odalric and Talebi, Mohammad Sadegh},
  booktitle={International Conference on Machine Learning},
  pages={1056--1066},
  year={2020},
  organization={PMLR}
}
@article{weissman2003inequalities,
  title={Inequalities for the L1 deviation of the empirical distribution},
  author={Weissman, Tsachy and Ordentlich, Erik and Seroussi, Gadiel and Verdu, Sergio and Weinberger, Marcelo J},
  journal={Hewlett-Packard Labs, Tech. Rep},
  year={2003}
}
@phdthesis{fruit2019exploration,
  title={Exploration-exploitation dilemma in Reinforcement Learning under various form of prior knowledge},
  author={Fruit, Ronan},
  year={2019},
  school={Universit{\'e} de Lille 1, Sciences et Technologies; CRIStAL UMR 9189}
}

@INPROCEEDINGS{5137416,
  author={Yu, Jia Yuan and Mannor, Shie},
  booktitle={2009 International Conference on Game Theory for Networks}, 
  title={Online learning in Markov decision processes with arbitrarily changing rewards and transitions}, 
  year={2009},
  volume={},
  number={},
  pages={314-322},
  doi={10.1109/GAMENETS.2009.5137416}}
  
  @article{zadorojniy2009strongly,
  title={A strongly polynomial algorithm for controlled queues},
  author={Zadorojniy, Alexander and Even, Guy and Shwartz, Adam},
  journal={Mathematics of Operations Research},
  volume={34},
  number={4},
  pages={992--1007},
  year={2009},
  publisher={INFORMS}
}

@article{bacoyannis2018idiosyncrasies,
  title={Idiosyncrasies and challenges of data driven learning in electronic trading},
  author={Bacoyannis, Vangelis and Glukhov, Vacslav and Jin, Tom and Kochems, Jonathan and Song, Doo Re},
  journal={arXiv preprint arXiv:1811.09549},
  year={2018}
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{gottesman2019guidelines,
  title={Guidelines for reinforcement learning in healthcare},
  author={Gottesman, Omer and Johansson, Fredrik and Komorowski, Matthieu and Faisal, Aldo and Sontag, David and Doshi-Velez, Finale and Celi, Leo Anthony},
  journal={Nature medicine},
  volume={25},
  number={1},
  pages={16--18},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{audibert2009exploration,
  title={Exploration--exploitation tradeoff using variance estimates in multi-armed bandits},
  author={Audibert, Jean-Yves and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Theoretical Computer Science},
  volume={410},
  number={19},
  pages={1876--1902},
  year={2009},
  publisher={Elsevier}
}

@article{osband2013more,
  title={(More) efficient reinforcement learning via posterior sampling},
  author={Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}