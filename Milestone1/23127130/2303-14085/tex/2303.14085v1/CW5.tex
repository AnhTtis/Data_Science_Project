\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{verbatim}
\usepackage{setspace}
\usepackage{color}
\usepackage{pdfsync}
%\usepackage{refcheck}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{geometry}
\usetikzlibrary{patterns}
\usepackage{mathtools}
\usepackage[numbers]{natbib}
\geometry{left=30mm, right=30mm, top=30mm, bottom=35mm}
\usepackage{url}

\newcommand{\crl}[1]{\ensuremath{ \left\{ #1 \right\} }}
\newcommand{\edg}[1]{\ensuremath{\! \left[ #1 \right] }}
\newcommand{\brak}[1]{\ensuremath{\left( #1 \right)}}
\newcommand{\angl}[1]{\ensuremath{ \left< #1 \right> }}
\newcommand{\n}[1]{\ensuremath{ \| #1 \| }}
\newcommand{\no}[1]{\ensuremath{ \left\| #1 \right\| }}
\newcommand{\abs}[1]{\ensuremath{ \left| #1 \right| }}

\usepackage{chngcntr}
\counterwithin*{equation}{section}
 \def\theequation{\thesection.\arabic{equation}}
 \newcommand{\be}{\begin{equation}}
 \newcommand{\ee}{\end{equation}}
 \newcommand{\bea}{\begin{eqnarray}}
 \newcommand{\eea}{\end{eqnarray}}
 \newcommand{\beas}{\begin{eqnarray*}}
\newcommand{\eeas}{\end{eqnarray*}}
\newcommand{\ds}{\displaystyle}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{question}{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem*{counterexample}{Counterexample}
\newtheorem*{examples}{Examples}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{notation}[theorem]{Notation}
\theoremstyle{remark}
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem*{acknowledgements}{Acknowledgements}

%\renewcommand{\labelenumi}{(\roman{enumi})}
%\renewcommand{\labelenumii}{(enumii)}
\newcommand{\ind}{{\perp\!\!\!\perp}} 
\newcommand{\id}{\text{\rm id}}
\newcommand{\pr}{\text{\rm pr}}
%\newcommand{\do}{\text{\rm do}}
\newcommand{\Do}{\text{\rm do}}

%\def\theequation{\arabic{equation}}
\newcommand{\eins}{\text{\ensuremath{1\hspace*{-0.9ex}1}}}
\newcommand{\ebp}{\text{EB}^{(p)}}
\newcommand{\pa}{\text{\rm pa}}
\newcommand{\anc}{\text{\rm anc}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\bQ}{\mathbb{Q}}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\cVar}{\mathrm{Var}}

\newcommand{\R}{\mathbb{R}}
\usepackage{tikz} 

\newcommand{\SE}[1]{{\color{blue}{#1}}}

% \title{Optimal transport problems with an information structure and Wasserstein distances for causal models}
\begin{document}
\title{\vspace{-3em}
Optimal transport and Wasserstein distances for causal models\footnote{
We are grateful to Beatrice Acciaio, Julio Backhoff-Veraguas, Daniel Bartl, Mathias Beiglböck, Songyan Hou, Nicolai Meinshausen, Alexander Neitz, Gudmund Pammer and Drago Ple\v{c}ko for interesting discussions and helpful comments.}
}
\date{March 2023}

\author{
	Patrick Cheridito%
	\thanks{
	Department of Mathematics, ETH Zurich, patrick.cheridito@math.ethz.ch.}
	\and
	Stephan Eckstein%
	\thanks{Department of Mathematics, ETH Zurich, stephan.eckstein@math.ethz.ch.
	}
}
\maketitle \vspace{-1.2em}

\begin{abstract}
In this paper we introduce a variant of optimal transport adapted to the causal 
structure given by an underlying directed graph. Different graph structures lead to 
different specifications of the optimal transport problem. For instance, a fully connected 
graph yields standard optimal transport, a linear graph structure corresponds to 
adapted optimal transport, and an empty graph leads to a notion of optimal transport 
related to CO-OT, Gromov--Wasserstein distances and factored OT. We derive 
different characterizations of causal transport plans and introduce 
Wasserstein distances between causal models that respect the underlying graph structure.
We show that average treatment effects are continuous with respect 
to causal Wasserstein distances and small perturbations of structural causal models 
lead to small deviations in causal Wasserstein distance. We also
introduce an interpolation between causal models based on causal Wasserstein distance
and compare it to standard Wasserstein interpolation.
	\end{abstract}
{\small
{\bf Keywords} Optimal transport, causality, directed graphs, causal Wasserstein distance, average treatment effect\\[1mm]
	{\bf MSC 2020 Subject Classification}
	62H22; % Probabilistic graphical models
	60D99; % Geometric probability and stochastic geometry (In my opinion, this fits due to interpolation and the special structure of the probability measures we consider)
	90C15; % Stochastic programming (Often optimal transport papers use this category)
	62H22 % Graphical methods in statistics (In my opinion, this fits in relation to the part on average treatment effect)
%	62D20; % Causal inference from observational studies
%	28A50 % Integration and disintegration of measures
}
\vspace{.6em}
\section{Introduction}

Originally, optimal transport problems were introduced by Monge \cite{monge1781memoire} and, in a 
more general form, by Kantorovich \cite{kantorovich1942translocation} to study the most efficient
way to transport goods from a set of production sites to different destinations.
But in addition to this immediate application, optimal transport theory has also 
lead to the the notion of Wasserstein distance \cite{kantorovich1960mathematical, vallender1974calculation, vaserstein1969markov}, which defines a metric
between different probability distributions. Over the years, optimal transport has 
found applications in different areas of economics 
\cite{chiappori2010hedonic, galichon2018optimal, pflug20121}, 
probability theory \cite{Rachev1985, RachRuesch1998}
statistics \cite{fournier2015rate, ghosal2022multivariate, niles2022minimax}, 
differential geometry \cite{de2014monge,figalli2010mass,Sud76}, 
robust optimization \cite{blanchet2019quantifying,mohajerin2018data, ConOpt2017}, 
machine learning and data science \cite{arjovsky2017wasserstein,cuturi2014fast, peyre2019computational,IPM2012}, just to name a few.
At the same time, various variants and extensions of optimal transport have emerged,
like multi-marginal versions \cite{agueh2011barycenters, embrechts2013model, pass2015multi}, optimal transport with additional constraints \cite{beiglbock2016problem, CKT, CKPS, de2020minmax,korman2015optimal,nutz2022directional}, optimal transport between measures with different masses \cite{chizat2018unbalanced, unbalancedThibault}, relaxations \cite{bonneel2019spot, LiLin2021} 
and regularizations \cite{cuturi2013sinkhorn,lorenz2021quadratically}.
Causal research examines causal relationships between different events; see e.g.
\cite{holland1986statistics, pearl2009causality, peters2016causal, scholkopf2022causality}.
In probabilistic theories of causality, the underlying causal structure is typically 
described by a directed graph, leading to graphical causal models \cite{bongers2021foundations}.

In this paper we introduce a version of optimal transport that is adapted to the 
causal structure given by an underlying directed graph and investigate 
corresponding Wasserstein distances between different causal models.
The structure of the graph determines the exact specification of the optimal transport problem.
A fully connected graph corresponds to standard optimal transport, which consists
in finding an optimal transport plan transforming a given distribution 
into another one. Missing edges translate into additional constraints, 
which increase the causal Wasserstein distance and therefore,
lead to a finer topology on the corresponding set of causal models. For instance,
a linear graph requires transport plans to respect the temporal structure. This results in 
so-called adapted transport problems, which have been studied in \cite{backhoff2019stability,backhoff2017causal,bartl2021wasserstein,lassalle2018causal}.
An empty graph restricts transport plans the most and is related to CO-OT problems \cite{titouan2020co},
the Gromov--Wasserstein distance \cite{memoli2011gromov} and factored OT \cite{tran2021factored}.

The remainder of this paper is organized as follows: In Section \ref{sec:notation}
we introduce the setup and provide definitions of causal transport plans together 
with the corresponding optimal transport problems in Definitions \ref{def:causal} and \ref{def:causalot}. In Section \ref{subsec:specialcases} we showcase the relation to existing concepts of optimal transport from the literature. In Section \ref{sec:DAG} we 
focus on causal transport maps for directed acyclic graphs. We derive alternative 
characterizations of causal transport plans in Theorem \ref{thm:main} and Corollary \ref{cor:main} and study the structure of sets of 
causal transport plans in Proposition \ref{prop:basic_prop}. In Section \ref{sec:Wasserstein} we introduce 
Wasserstein distances between causal models that respect the structure of the 
underlying graph. We show that they satisfy all properties of a metric except for 
the triangle inequality in Proposition \ref{prop:wassersteinprop}. In Section \ref{subsec:ate}, we prove that
average treatment effects, which estimate the
causal effect of a treatment or intervention, are continuous with respect to 
causal Wasserstein distance under a change of the underlying probability model
while this is not the case for standard Wasserstein distances.
On the other hand, in Section \ref{subsec:closeclose}, we show that small perturbations of a 
structural causal model correspond to small deviations in causal Wasserstein 
distance. Finally, in Section \ref{subsec:geodesic} we study a Wasserstein interpolation between causal 
models that respects the causal structure and compare it to standard 
Wasserstein interpolation.

\section{Notation and definitions}
\label{sec:notation}

\subsection{Causal structures described by directed graphs}

We consider a finite set $V = \crl{1, \dots, n}$ 
for some $n \in \bN$, endowed with a causal structure
given by a set of {\sl directed edges} $E \subseteq \{(i,j) \in V \times V : i \neq j\}$. 
This turns $G = (V,E)$ into a {\sl directed graph}. For $j \in V$, we denote by 
$\pa_j$ the parents of $j$, given by 
\[
\pa_j := \{i \in V : (i,j) \in E\},
\]
and for a subset $A \subseteq V$, we define
\[
\pa_A := \bigcup_{j \in A} \pa_j.
\]
We order both sets $\pa_j$ and $\pa_A$ according to the natural ranking of 
$V = \{1, \dots, n\}$. 

If $G$ has no {\sl cycles}, it is called a {\sl directed acyclic graph} (DAG). 
By reordering the vertices, a DAG can always assumed to be {\sl sorted}, which 
means that it only contains edges $(i,j)$ such that $i < j$; see e.g.
\cite{bongers2021foundations, lauritzen1996graphical, pearl2009causality}.

\subsection{Spaces and measures}

We consider two product spaces of the form $\cX = \cX_1\times \dots \times \cX_n$ and 
$\cY = \cY_1 \times \dots \times \cY_n$ for non-empty 
Polish spaces ${\cal X}_i, {\cal Y}_i$, $i = 1, \dots, n$.
We endow $\cX$, $\cY$ and $\cX \times \cY$ 
with the product topologies, which makes them again Polish; see e.g. \cite[Chapter IX, §6.1, Proposition I]{bourbakitopology}. By
$\cP(\cX)$, $\cP(\cY)$ and $\cP(\cX \times \cY)$ we denote the set of all Borel probability 
measures on $\cX$, $\cY$ and $\cX \times \cY$, respectively. 
If we write $X \sim \mu$ for a measure $\mu \in \mathcal{P}(\cX)$, we mean 
that $X$ is an $\cX$-valued random variable with distribution $\mu$ defined on 
some underlying probability space.
Moreover, we use the notation $X_{1:i}$ to denote the tuple $(X_1, \dots, X_i)$ for 
$i \in V = \{1, \dots, n\}$, and similarly, $X_A$ for any ordered subset $A$ of $V = \{1, \dots, n\}$.
$X_{1:0}$ will be understood as $\emptyset$.

\subsection{Transport plans and causal couplings}

A transport map between two probability measures $\mu \in \cP(\cX)$ and $\nu \in \cP(\cY)$
in the sense of Monge \cite{monge1781memoire} is a measurable mapping 
$T \colon \cX \to \cY$ such that $T_{\#} \mu = \nu$, where $T_{\#} \mu$ 
denotes the push-forward $\mu \circ T^{-1}$ of $\mu$ along $T$.
A Monge transport map does not always exist, e.g., if $\mu$ is a Dirac measure and $\nu$ is not.
Therefore, Kantorovich \cite{kantorovich1942translocation} extended the set of transport plans by considering couplings 
between $\mu$ and $\nu$, which are probability measures $\pi \in \cP(\cX \times \cY)$ 
with marginals $\mu$ and $\nu$. The set $\Pi(\mu, \nu)$ of all couplings 
between $\mu$ and $\nu$ always contains the product measure $\mu \otimes \nu$, 
and therefore, is non-empty. Moreover, for every Monge transport map $T \colon \cX \to \cY$ 
between $\mu$ and $\nu$, the distribution of the mapping 
$(\mbox{id}, T) \colon \cX \to \cX \times \cY$ under $\mu$ is a coupling in $\Pi(\mu, \nu)$.
On the other hand, every coupling $\pi \in \Pi(\mu, \nu)$ can be realized as a {\sl randomized
transport map}. Indeed, since $\cY$ is Polish, $\pi$ admits a disintegration of the form
$\pi(dx, dy) = \mu(dx) \pi(dy \mid x)$, where 
$\pi(dy \mid x)$ is a regular version of the conditional 
distribution of $y \in \cY$ under $\pi$ given $x \in \cX$; see e.g. 
\cite[Lemma 3.4]{kal3rd}. Therefore, one obtains from the kernel 
representation \cite[Lemma 4.22]{kal3rd} that
$\pi$ can be realized as the distribution of a pair of 
random variables $(X,Y)$ such that $X \sim \mu$ and $Y$ is of the form 
$Y = g(X,U)$ for a measurable map $g \colon \cX \times \R \to \cY$ and 
an $\R$-valued random variable $U$ that is independent of $X$.

To motivate our definition of causal couplings, we first consider the case of 
Monge transport maps $T \colon \cX \to \cY$. Intuitively, one might want to
define $G$-causality of $T$ by saying that $T$ is
$G$-causal if it is of the form 
\be \label{T1}
T_i(x) = g_i(x_i, x_{\pa_i}) \quad \mbox{for all } x \in \cX 
\mbox{ and } i = 1, \dots, n,
\ee
for measurable mappings $g_i \colon \cX_i \times \cX_{\pa_i} \to \cY_i$,
$i = 1, \dots, n$. More generally, one could call $T$ $G$-causal if 
it is of the form \be \label{T2}
T_i(x) = g_i(x_i, x_{\pa_i}, T_{\pa_i}(x)) \quad \mbox{for all } x \in \cX
\mbox{ and } i = 1, \dots, n,
\ee
for measurable mappings $g_i \colon \cX_i \times \cX_{\pa_i} \times \cY_{\pa_i} \to \cY_i$,
$i = 1, \dots, n$. It is easy to see that in the following three cases, conditions 
\eqref{T1} and \eqref{T2} are equivalent:

\begin{itemize}
\item if the graph $G= (V,E)$ is fully connected, that is, 
$E = \{(i,j) \in V^2 : i \neq j\}$,
\item 
if the graph $G= (V,E)$ is empty, that is, $E = \emptyset$, or
\item
if the graph $G= (V,E)$ has the linear structure $E = \{(i,j) \in V^2 : i < j\}$.
\end{itemize}
On the other hand, 
\begin{itemize}
\item if the graph $G= (V,E)$ has the Markovian structure 
$E = \{(i-1, i) : 2 \le i \le n\}$,
\end{itemize}
\eqref{T2} is slightly more flexible than \eqref{T1}.
In general, \eqref{T1} is too restrictive for our purposes. Therefore, we use \eqref{T2} to 
define $G$-causal Monge transport maps, which leads to the following general definition.

\begin{definition}[$G$-causal couplings] \label{def:causal} 
Let $G = (V,E)$ be a directed graph. We call a coupling $\pi \in \Pi(\mu, \nu)$
between two measures $\mu \in \mathcal{P}(\cX)$ and $\nu \in \mathcal{P}(\cY)$
$G$-causal, if there exists a pair of random variables $(X, Y) \sim \pi$ together with 
measurable mappings
\[
g_i \colon \cX_{i} \times \cX_{\pa_i} \times \cY_{\pa_i} \times \mathbb{R} \rightarrow \cY_i,
\quad i = 1, \dots, n,
\]
and $\R$-valued random variables $U_1, \dots, U_n$ 
such that $X,U_1, \dots, U_n$ are independent and
\begin{equation}
\label{condcausal} 
Y_i = g_i(X_{i}, X_{\pa_i}, Y_{\pa_i}, U_i) \quad \mbox{for all } i = 1, \dots, n.
\end{equation}
We call $\pi$ $G$-bicausal, if $\pi$ and $\tilde\pi \sim (Y, X)$ are $G$-causal. 
We denote the set of $G$-causal and $G$-bicausal couplings between $\mu$ and $\nu$ by $\Pi_{G}(\mu, \nu)$ and $\Pi^{\rm bc}_{G}(\mu, \nu)$, respectively.
\end{definition}

$\Pi^{\rm bc}_G(\mu, \nu)$ is a symmetrized version of $\Pi_G(\mu, \nu)$
consisting of transport plans that are $G$-causal in both directions. 
In Section \ref{sec:DAG} below we give alternative characterizations of 
$G$-causal and $G$-bicausal transport plans in the case where 
$G$ is a DAG. But the following transport problems can be 
defined for arbitrary directed graphs $G$.

\begin{definition}[$G$-causal optimal transport]
\label{def:causalot}
Let $G = (V,E)$ be a directed graph. For $\mu \in \cP(\cX)$, $\nu \in \cP(\cY)$ 
and a measurable cost function $c : \cX \times \cY \rightarrow \mathbb{R}_+$,
we define the $G$-causal and $G$-bicausal optimal transport problems as
\be \label{ot}
\inf_{\pi \in \Pi_{G}(\mu, \nu)} \int_{\cX \times \cY} c \; d\pi
\qquad \mbox{and} \qquad
\inf_{\pi \in \Pi^{\rm bc}_{G}(\mu, \nu)} \int_{\cX \times \cY} c \; d\pi,
\ee
respectively.
\end{definition}

\subsection{Special cases and related concepts}\label{subsec:specialcases}

\subsubsection{Fully connected graphs and standard optimal transport}
\label{ss:full}

If $E$ equals the set $\{(i,j) \in V^2 : i \neq j\}$ of all possible edges, 
a coupling $\pi \in \Pi(\mu, \nu)$ between two measures 
$\mu \in \cP(\cX)$ and $\nu \in \cP(\cY)$ satisfies Definition \ref{def:causal}
if and only if there exists a pair of random variables 
$(X, Y) \sim \pi$ together with measurable mappings
\[
g_i \colon \cX \times \cY_{V \setminus \{i\}} \times \mathbb{R} \rightarrow \cY_i, \quad i = 1, \dots, n,
\]
and $\R$-valued random variables $U_1, \dots, U_n$ 
such that $X,U_1, \dots, U_n$ are independent and
\begin{equation}
\label{condfull}
Y_i = g_i(X, Y_{V \setminus \{i\}}, U_i) \quad \mbox{for all } i = 1, \dots, n.
\end{equation}
Since $\cX$ and $\cY_i$, $i = 1, \dots, n$, are Polish spaces, 
any coupling $\pi \in \Pi(\mu, \nu)$ admits a disintegration of the form 
\[
\pi(dx, dy) = \mu(dx) \bigotimes_{i=1}^n \pi(dy_i \mid x, y_{1:i-1});
\]
see e.g., \cite[Theorem 3.4]{kal3rd}, and it follows from the kernel representation 
\cite[Lemma 4.22]{kal3rd} that $\pi$ has a realization of the form \eqref{condfull}. 
This shows that 
\[
\Pi_G(\mu, \nu) = \Pi^{\rm bc}_G(\mu, \nu) = \Pi(\mu, \nu),
\]
and the two optimal transport problems in \eqref{ot} reduce to standard optimal transport.

\subsubsection{Empty graphs, CO-OT and Gromov--Wasserstein distances}
\label{ss:empty}

If the graph has no edges, that is, $E = \emptyset$, then a coupling 
$\pi \in \Pi(\mu, \nu)$ satisfies Definition 
\ref{def:causal} if and only if there exists a pair of random variables $(X, Y) \sim \pi$ together with 
measurable mappings
\[
g_i \colon \cX_i \times \mathbb{R} \rightarrow \cY_i, \quad i = 1, \dots, n,
\]
and $\R$-valued random variables $U_1, \dots, U_n$ 
such that $X,U_1, \dots, U_n$ are independent and
\be \label{condempty}
Y_i = g_i(X_i, U_i) \quad 
\mbox{for all } i = 1, \dots, n.
\ee
Since $\cX_i, \cY_i$, $i=1, \dots, n$, are Polish spaces, every 
measure $\pi_i \in \Pi(\mu_i, \nu_i)$ can be disintegrated as
\[
\pi_i(dx_i, dy_i) = \mu_i(dx_i) \pi_i(dy_i \mid x_i);
\]
see e.g. \cite[Theorem 3.4]{kal3rd}. Therefore, we obtain from the 
kernel representation \cite[Lemma 2.2]{kal3rd} that $\pi_i$ has a 
representation of the form \eqref{condempty}. In particular, if 
$\mu = \mu_1 \otimes \cdots \otimes \mu_n$ and
$\nu = \nu_1 \otimes \cdots \otimes \nu_n$ are product measures, one has
\[
\Pi_G(\mu, \nu) = \Pi^{\rm bc}_G(\mu, \nu) = 
\crl{\pi_1 \otimes \cdots \otimes \pi_n : 
\pi_i \in \Pi(\mu_i, \nu_i) \mbox{ for all } i = 1, \dots, n}.
\]
This set of couplings is related to CO-OT problems \cite{titouan2020co},
the Gromov--Wasserstein distance \cite{memoli2011gromov}
and factored OT \cite{tran2021factored}.
CO-OT problems also optimize over couplings of marginals of product form.
They aim to simultaneously optimize two transport plans between 
features and labels of two datasets. Therefore,
$n=2$ is used. The Gromov--Wasserstein distance 
measures the discrepancy between two spaces. It considers $n = 2$ and marginal 
measures of the special form $\mu = \mu_1 \otimes \mu_1$, $\nu = \nu_1 \otimes \nu_1$.
Factored OT is a generalization of CO-OT to a multi-marginal setting based on couplings of the form
\[
\{\pi_1 \otimes \dots \otimes \pi_n : 
\pi_i \in \Pi \big(\mu_{i,1}, \dots, \mu_{i,n_i} \big) \text{ for } i=1, \dots, n\}.
\]

\subsubsection{Linear graph structure and adapted optimal transport}
\label{ss:linear}

If the graph has the linear structure $E = \{(i,j) \in V^2 : i < j\}$, 
then a coupling $\pi \in \Pi(\mu, \nu)$ satisfies Definition \ref{def:causal}
if and only if there exists a pair of random variables 
$(X, Y) \sim \pi$ together with measurable mappings
\[
g_i \colon \cX_{1:i} \times \cY_{1:i-1} \times \mathbb{R} \rightarrow \cY_i, \quad i = 1, \dots, n,
\]
and $\R$-valued random variables $U_1, \dots, U_n$ 
such that $X,U_1, \dots, U_n$ are independent and
\[
Y_i = g_i(X_{1:i}, Y_{1:i-1}, U_i) \quad 
\mbox{for all } i = 1, \dots, n.
\]
If $\mu$ and $\nu$ are the distributions of two discrete-time stochastic processes with 
indices $i = 1, \dots, n$, the coupling $\pi$ can 
be viewed as a transport plan that respects the temporal structure. Such couplings 
lead to adapted optimal transport, which has been studied in for instance by \cite{bartl2021wasserstein,lassalle2018causal,pflug2012distance}
and has applications in robust finance
\cite{acciaio2021cournot,backhoff2017causal,backhoff2019stability,pflug2015dynamic}
as well as machine learning \cite{xu2021quantized,xu2020cot}. For relations to 
different topologies on spaces of stochastic processes, see e.g.\cite{aldous1981weak,backhoff2020all,hellwig1996sequential,pammer2022note}.


\subsubsection{Markovian graph structure}
\label{ss:Markov}

If the graph $G = (V,E)$ has the Markovian structure $E = \{(i-1,i) : 2 \le i \le n\}$, 
a coupling $\pi \in \Pi(\mu, \nu)$ satisfies Definition \ref{def:causal}
if and only if if there exists a pair of random variables $(X, Y) \sim \pi$ together with 
measurable mappings
\[
g_1 \colon \cX_1 \times \R \to \cY_1, \quad
g_i \colon \cX_{i-1:i} \times \cY_{i-1} \times \mathbb{R} \rightarrow \cY_i, \quad i = 2, \dots, n,
\]
and $\R$-valued random variables $U_1, \dots, U_n$ 
such that $X,U_1, \dots, U_n$ are independent and
\[
Y_1 = g_1(X_1, U_1), \quad Y_i = g_i(X_{i-1:i}, Y_{i-1}, U_i), \quad 
i = 2, \dots, n.
\]

\subsubsection{Related concepts}
\label{ss:related}

Different variants of optimal transport have been proposed in the literature
that do not exactly fit into the framework of this paper but are related.
\cite{plecko2020fair} proposes a coupling between two populations, 
such as male and female job applicants, along a given graph with the goal of
achieving a fair embedding of one group into the other. 
\cite{kocaoglu2017causalgan} proposes a GAN framework based on 
structural assumptions similar to \eqref{condcausal}.
\cite{de2021transport} studies collections of couplings between conditional 
distributions. 

\section{Causal couplings for DAGs} \label{sec:char_dag}
\label{sec:DAG}

In this section we focus on $G$-causal and $G$-bicausal couplings for DAGs.
We provide alternative characterizations of $G$-causal and $G$-bicausal couplings,
which will allow us to derive simple properties of the sets of all such couplings. 

We start by recalling the notions of conditional independence and 
compatibility of a probability measure with a given DAG. 
For more details, we refer to \cite{bongers2021foundations, lauritzen1996graphical, pearl2009causality}.
For random variables $S,W,Z$ defined on the same probability space 
$(\Omega, \cF, \bP)$ with values in measurable spaces $\cS, \cW, \cZ$, we say 
$S$ is independent of $W$ given $Z$ and write $S \ind_Z W$ if 
\be \label{condindep1}
\bP[S \in A, W \in B \mid Z] = \bP[S \in A \mid Z] \, \bP[W \in B \mid Z] \quad \mbox{$\bP$-a.s.}
\ee
for all measurable subsets $A \subseteq \cS$ and $B \subseteq \cW$, which is equivalent to 
\be \label{condindep2}
\bP[S \in A \mid W, Z] = \bP[S \in A \mid Z] \quad \mbox{$\bP$-a.s.}
\ee
for all measurable subsets $A \subseteq \cS$. Since the conditional 
probabilities in \eqref{condindep1}--\eqref{condindep2} can all be 
written as $g(Z)$ or $h(W,Z)$ for measurable functions 
$g \colon \cZ \to [0,1]$ or $h \colon \cW \times \cZ \to [0,1]$ 
(see e.g. \cite[Lemma 1.14]{kal3rd}), 
\eqref{condindep1}--\eqref{condindep2} can equivalently be formulated as 
\[
\pi[A \times B \times \cZ \mid \cF_{\cZ}] 
= \pi[A \times \cW \times \cZ \mid \cF_{\cZ}] \, 
\pi[\cV \times B \times \cZ \mid \cF_{\cZ}] \quad \mbox{$\pi$-a.s.}
\]
and 
\[
\pi[A \times \cW \times \cZ \mid \cF_{\cW, \cZ}] 
= \pi[A \times \cW \times \cZ \mid \cF_{\cZ}] 
\quad \mbox{$\pi$-a.s.}, 
\]
where $\pi$ is the distribution of $(S,W,Z)$ and $\cF_{\cZ}$,
$\cF_{\cW, \cZ}$ are the $\sigma$-algebras generated by the
projections from $\cS \times \cW \times \cZ$ to 
$\cZ$ and $\cW \times \cZ$, respectively. This shows 
that conditions \eqref{condindep1}--\eqref{condindep2}
only depend on the distribution of $(S,W,Z)$.

If $\cS$ and $\cW$ are Polish spaces equipped with their Borel $\sigma$-algebras,
the distribution $\rho$ of $(S,W)$ admits a regular conditional version
$\rho( \,\cdot \mid z)$, $z \in {\cal Z}$ (see e.g. \cite[Theorem 3.4]{kal3rd}),
and conditions \eqref{condindep1}--\eqref{condindep2} 
can equivalently be written as
\[
\rho(ds, dw \mid z) = \rho(ds \mid z) 
\rho(dw \mid z) \quad \mbox{for $Z_{\#} \bP$-almost all $z \in \cZ$.}
\]

\begin{definition}[$G$-compatible measures]
\label{def:Gcomp}
A measure $\mu \in \mathcal{P}(\cX)$ is said to be compatible with a sorted DAG 
$G = (V,E)$ if any of the following three equivalent conditions hold. We denote the set of $G$-compatible measures in $\cP(\cX)$ by $\mathcal{P}_G(\cX)$.
\begin{itemize}
\item[(i)] There exists a random variable $X \sim \mu$ together with measurable functions
$f_i : \cX_{\pa_i} \times \mathbb{R} \rightarrow  \cX_i$, $i = 1, \dots, n$, 
and independent $\mathbb{R}$-valued random variables 
	$U_1, \dots, U_n$ such that
	\be \label{XfU}
		X_i = f_i(X_{\pa_i}, U_i) \quad \text{for all $i =1 , \dots, n$}.
	\ee
\item[(ii)] 
		For every random variable $X \sim \mu$, one has
		\begin{equation} \label{Xindep}
		X_i \ind_{X_{\pa_i}} X_{1:i-1} \quad \mbox{for all $i = 2, \dots, n.$}
		\end{equation}
				\item[(iii)] 
			
		The measure $\mu$ can be disintegrated as
		\be \label{mudis}
		\mu(dx_1, \dots, dx_n) = \bigotimes_{i=1}^n \mu \brak{dx_i \mid x_{\pa_i}},
		\ee
		where $\mu(\cdot \mid x_{\pa_i})$ are often referred to as the causal mechanisms of $\mu$.
	\end{itemize}
\end{definition}

\begin{remark}
The equivalence of the conditions (i)--(iii) in Definition \ref{def:Gcomp} 
follows from the definitions \eqref{condindep1}--\eqref{condindep2}
of conditional independence together with some well-known properties
of probability measures on Polish spaces. For the sake of completeness, 
we give a short proof:

(i) $\Leftrightarrow$ (ii): If $X$ is of the form \eqref{XfU} for 
independent $\mathbb{R}$-valued random variables $U_1, \dots, U_n$,
every $U_i$ is independent of $X_{1:i-1}$, and it can easily be verified that \eqref{Xindep} holds by checking that \eqref{condindep2} is satisfied. 
On the other hand, if \eqref{Xindep} holds, 
one obtains from \cite[Proposition 8.20]{kal3rd} that, possibly after extending 
the underlying probability space, all $X_i$ have a 
representation of the form \eqref{XfU} for measurable functions 
$f_i \colon \cX_{\pa_i} \times \bR \to \cX_i$ and $U(0,1)$-distributed 
random variables $U_i$ satisfying $U_i \ind X_{1:i-1}$ for all $i = 2, \dots, n$.
So $X$ almost satisfies condition (i), except that the random variables $U_1, \dots, U_n$ 
are not necessarily independent. But this can easily be remedied by considering 
independent $U(0,1)$-distributed random variables $\tilde{U}_1, \dots, \tilde{U}_n$
on a new probability space and iteratively 
defining $\tilde{X}_i = f_i(\tilde{X}_{\pa_i}, \tilde{U}_i)$, $i = 1, \dots, n$.
Then the sequence $\tilde{X}_1, \dots, \tilde{X}_n$ has the same distribution as 
$X_1, \dots, X_n$ and satisfies condition (i).

(ii) $\Leftrightarrow$ (iii): Since $\cX_1, \dots, \cX_n$ are Polish spaces, one obtains 
from \cite[Theorem 3.4]{kal3rd}  that $\mu$ admits 
a disintegration of the form
\[
\mu(dx_1, \dots, dx_n) = \bigotimes_{i=1}^n \mu \brak{dx_i \mid x_1, \dots, x_{i-1}}, 
\]
from which it can be seen that \eqref{Xindep} is equivalent to \eqref{mudis}.
\end{remark}

The following result provides some elementary properties of the set $\cP_G(\cX)$.

\begin{proposition}
Let $G = (V,E)$ be a sorted DAG. Then
\begin{itemize}
\item[{\rm (i)}] $\cP_G(\cX)$ is non-empty,
\item[{\rm (ii)}] $\cP_G(\cX)$ is closed in total variation,
\item[{\rm (iii)}] if ${\cal X}$ is a discrete\footnote{i.e. $\cX$ is a countable 
discrete topological space} Polish space, 
$\cP_G(\cX)$ is weakly closed,
\item[{\rm (iv)}] but in general, $\cP_G(\cX)$ is not weakly closed.
\end{itemize}
\end{proposition}

\begin{proof}
According to Definition \ref{def:Gcomp}.(iii), the product measure 
$\mu_1 \otimes \dots \otimes \mu_n$ belongs to $\cP_G(\cX)$
for arbitrary marginal measures $\mu_i \in \cP(\cX_i)$, $i = 1, \dots, n$.
This shows (i).

Moreover, it follows from Lemma \ref{lemma:stable} in Appendix \ref{app:stability} below 
that Definition \ref{def:Gcomp}.(ii) is stable under convergence in total variation, which proves (ii).

(iii) follows from (ii) since in case $\cX$ is a discrete Polish space, weak convergence is equivalent to convergence in total variation.

To show (iv), we consider the Markovian graph $G = (V,E)$ given by $V = \{1, 2, 3\}$
and $G = \{(1,2), (2,3)\}$. Let $\cX_1 = \cX_2 = \cX_3 = [-1,1]$ and consider the
product space $\cX = [-1,1]^3$ with the Borel $\sigma$-algebra. The measures
$\mu^k = \frac{1}{2} \delta_{(1, 1/k, 1)} + \frac{1}{2} \delta_{(-1, -1/k, -1)}$, $k \in \bN$,
belong to $\cP_G(\cX)$ for all $k \in \bN$ and weakly converge to
the measure $\frac{1}{2} \delta_{(1, 0, 1)} + \frac{1}{2} \delta_{(-1, 0, -1)}$,
which does not belong to $\cP_G(\cX)$. This proves (iv).
\end{proof}

The following is the main structural result of this paper. It gives different characterizations of 
$G$-causal couplings between two measures 
$\mu \in \cP_G(\cX)$ and $\nu \in \cP(\cY)$.

\begin{theorem} \label{thm:main}
Let $G = (V,E)$ be a sorted DAG, and consider measures 
$\mu \in \mathcal{P}_G(\cX)$, $\nu \in \mathcal{P}(\cY)$, $\pi \in \Pi(\mu, \nu)$. Then, 
the following are equivalent:
\begin{itemize}
\item[{\rm (i)}] $\pi \in \Pi_{G}(\mu, \nu)$,
\item[{\rm (ii)}] for $(X, Y) \sim \pi$, one has
\[ Y_i \ind_{X_{i}, X_{\pa_i}, Y_{\pa_i}} (X, Y_{1:i-1}) 
	\quad \mbox{for all } i =1,\dots, n,
	\]
\item[{\rm (iii)}] for $(X, Y) \sim \pi$, one has
 \[
	X_i \ind_{X_{\pa_i}} (X_{1:i-1}, Y_{1:i-1}) \quad \mbox{and} \quad
	Y_i \ind_{X_i, X_{\pa_i}, Y_{\pa_i}} (X_{1:i}, Y_{1:i-1}) 
	\quad \mbox{for all } i =1,\dots, n,
		\]
\item[{\rm (iv)}] $\pi$ is jointly $G$-compatible, that is,
\[
\pi(dx_1, dy_1, \dots, dx_n, dy_n)
= \bigotimes_{i=1}^n \pi(dx_i, dy_i \mid x_{\pa_i}, y_{\pa_i}),
\] and
\[
		\pi(d x_i \mid x_{\pa_i}, y_{\pa_i}) = \mu(d x_i \mid x_{\pa_i})
		\quad  \mbox{for all $i = 1, \dots, n$ and 
		$\pi$-almost all $(x,y) \in (\cX,\cY)$.}\]
	\end{itemize}
\end{theorem}

Before turning to the proof of Theorem \ref{thm:main}, we note that 
it implies the following alternative characterizations of $G$-bicausal couplings
between two measures $\mu \in \cP_G(\cX)$ and $\nu \in \cP_G(\cY)$.

\begin{corollary} \label{cor:main}
Let $G = (V,E)$ be a sorted DAG, $\mu \in \mathcal{P}_G(\cX)$, 
$\nu \in \mathcal{P}_G(\cY)$ and $\pi \in \Pi(\mu, \nu)$. 
Then, the following are equivalent:
\begin{itemize}
\item[{\rm (i)}] 
$\pi \in \Pi^{\rm bc}_G(\mu, \nu)$,
\item[{\rm (ii)}] 
for $(X, Y) \sim \pi$, one has
\[ 
(X_i, Y_i) \ind_{X_{\pa_i}, Y_{\pa_i}} (X_{1:i-1}, Y_{1:i-1}),
\quad
X_i \ind_{X_{\pa_i}} Y_{\pa_i} \quad \mbox{and} 
\quad  Y_i \ind_{Y_{\pa_i}} X_{\pa_i}
\quad \mbox{for all } i =2,\dots, n,
\]
\item[{\rm (iii)}] $\pi$ is jointly $G$-compatible, that is,
\[
\pi(dx_1, dy_1, \dots, dx_n, dy_n) = \bigotimes_{i=1}^n \pi(dx_i, dy_i \mid 
x_{\pa_i}, y_{\pa_i}),
\]
and
\[
\pi(dx_i, dy_i \mid x_{\pa_i}, y_{\pa_i}) \in \Pi(\mu(dx_i \mid x_{\pa_i}), \nu(dy_i \mid y_{\pa_i}))
\]
for all $i = 1, \dots, n$ and $\pi$-almost all $(x, y) \in \cX \times \cY$.
\end{itemize}
\end{corollary}

\noindent
{\sl Proof of Theorem \ref{thm:main}}\\
(i) $\Rightarrow$ (ii): If (i) holds, there exists $(X,Y) \sim \pi$ such that 
the $Y_i = g_i(X_i, X_{\pa_i}, Y_{\pa_i}, U_i)$ for measurable functions 
$g_i \colon \cX_i \times \cX_{\pa_i} \times \cY_i \times \bR \to \cY_i$ 
and $\R$-valued random variables $U_1, \dots, U_n$ 
such that $X,U_1, \dots, U_n$ are independent. It follows that
$U_i \ind (X, Y_{1:i-1})$ for all $i = 1, \dots,  n$, from which, together with \cite[Proposition 8.20]{kal3rd}, it can be seen that 
(ii) holds.

(ii) $\Rightarrow$ (i):
If $(X,Y)$ satisfies (ii), one obtains 
from \cite[Proposition 8.20]{kal3rd} that, possibly after extending the underlying 
probability space, the $Y_i$ can be represented as
$Y_i = g_i(X_i, X_{\pa_i}, Y_{\pa_i}, U_i)$ 
for measurable functions $g_i \colon \cX_i \times \cX_{\pa_i} \times \cY_{\pa_i} \times \R \to \cY_i$
and $U(0,1)$-distributed random variables $U_i$ satisfying
$U_i \ind (X, Y_{1:i-1})$ for all $i = 1, \dots, n$. So $(X,Y)$ is of the form 
\eqref{condcausal}, except that the random variables $U_1, \dots, U_n$ are not 
necessarily independent. However, by extending the probability space further
if necessary, one can assume that there exist $U(0,1)$-distributed random 
variables $\tilde{U}_1, \dots, \tilde{U}_n$ 
such that $X, \tilde{U}_1, \dots, \tilde{U}_n$ are independent. If one iteratively
defines $\tilde{Y}_i = g_i(X_i, X_{\pa_i}, \tilde{Y}_{\pa_i}, \tilde{U}_i)$, $i = 1, \dots, n$,
$(X,\tilde{Y})$ has the same distribution as $(X,Y)$ and satisfies all requirements of
Definition \ref{def:causal}.

(ii) $\Rightarrow$ (iii): The second condition of (iii) is immediate from (ii).
In addition, one obtains from (ii) that $X_i \ind_{X_{1:i-1}, Y_{1:j-1}} Y_j$ 
for all $j =1, \dots, i-1$, which, by the chain rule \cite[Theorem 8.12]{kal3rd} yields 
$X_i \ind_{X_{1:i-1}} Y_{1:i-1}$. This together with 
$X_i \ind_{X_{\pa_i}} X_{1:i-1}$, which holds since $\mu \in \cP_G(\mu)$,
gives $X_i \ind_{X_{\pa_i}} (X_{1:i-1}, Y_{1:i-1})$, which shows that also the
first condition of (iii) is fulfilled. 

(iii) $\Rightarrow$ (ii):
The first condition of (iii) implies
$X_j \ind_{X_{1:j-1}} Y_{1:i}$, and therefore,
$Y_i \ind_{X_{1:j-1}, Y_{1:i-1}} X_j$ for all $j = i+1, \dots, n$. 
This, together with the second condition of (iii) and the 
chain rule \cite[Theorem 8.12]{kal3rd}, yield (ii). 

(iii) $\Leftrightarrow$ (iv): Since $\cX_i$, $\cY_i$, $i = 1, \dots, n$, are Polish 
spaces, it follows from \cite[Theorem 3.4]{kal3rd} that $\pi$ can be disintegrated as
\[
\pi(dx_1, dy_1, \dots, dx_n, dy_n) 
= \bigotimes_{i=1}^n \pi \brak{dx_i \mid x_{1:i-1}, y_{1:i-1}}
\pi \brak{dy_i \mid x_{1:i} \, , y_{1:i-1}}.
\]
Since $\pi \in \Pi(\mu, \nu)$, one has
\[
X_i \ind_{X_{\pa_i}} (X_{1:i-1}, Y_{1:i-1}) \quad \Leftrightarrow \quad
\pi(d x_i \mid x_{1:i-1}, y_{1:i-1}) = \mu(d x_i \mid x_{\pa_i})
	\quad  \mbox{$\pi$-almost surely}
\]
and
\[
Y_i \ind_{X_i, X_{\pa_i}, Y_{\pa_i}} (X_{1:i}, Y_{1:i-1}) \quad \Leftrightarrow \quad
\pi \brak{dy_i \mid x_{1:i} \, , y_{1:i-1}} =
\pi \brak{dy_i \mid x_i, x_{\pa_i} , y_{\pa_i}}
\quad  \mbox{$\pi$-almost surely,}
\]
which shows that (iii) is equivalent to 
\[
\pi(dx_1, dy_1, \dots, dx_n, dy_n) 
= \bigotimes_{i=1}^n \mu \brak{dx_i \mid x_{\pa_i}}
\pi \brak{dy_i \mid x_i, x_{\pa_i} , y_{\pa_i}},
\]
which, in turn, is equivalent to (iv). \qed

\bigskip \noindent
{\sl Proof of Corollary \ref{cor:main}}\\
(i) $\Leftrightarrow$ (iii) follows directly from the equivalence of (i) and (iv) 
of Theorem \ref{thm:main}.

(ii) $\Leftrightarrow$ (iii): Similarly to the proof of Theorem \ref{thm:main}, 
we obtain from \cite[Theorem 3.4]{kal3rd} that $\pi$ can be disintegrated as
\[
\pi(dx_1, dy_1, \dots, dx_n, dy_n) 
= \bigotimes_{i=1}^n \pi \brak{dx_i, dy_i \mid x_{1:i-1}, y_{1:i-1}},
\]
from which it can be seen that (ii) is equivalent to (iii). \quad

\bigskip
In the following result we derive some elementary properties of the sets
$\Pi_{G}(\mu, \nu)$ and $\Pi^{\rm bc}_{G}(\mu, \nu)$.

\begin{proposition} \label{prop:basic_prop}
Let $G = (V,E)$ be a sorted DAG and
$\mu \in \mathcal{P}_G(\cX)$, $\nu \in \mathcal{P}_G(\cY)$. Then
	\begin{itemize}
\item[{\rm (i)}] $\Pi_{G}(\mu, \nu)$ and $\Pi^{\rm bc}_{G}(\mu, \nu)$ are both non-empty,
\item[{\rm (ii)}] $\Pi_{G}(\mu, \nu)$ and $\Pi^{\rm bc}_{G}(\mu, \nu)$ are closed in
total variation,
\item[{\rm (iii)}] 
if $\mu$ and $\nu$ are finitely supported,
$\Pi_{G}(\mu, \nu)$ and $\Pi^{\rm bc}_{G}(\mu, \nu)$ are weakly closed,
\item[{\rm (iv)}] but in general, $\Pi_{G}(\mu, \nu)$ and $\Pi^{\rm bc}_{G}(\mu, \nu)$ are not weakly closed.
\end{itemize}
\end{proposition}

\begin{proof}
Since $\mu$ and $\nu$ are both compatible with $G$, they 
admit a decomposition of the form \eqref{mudis}. So, it can be seen 
from Corollary \ref{cor:main} that the product measure $\mu \otimes \nu$ 
belongs to $\Pi^{\rm bc}_{G}(\mu, \nu)$, which is contained in $\Pi_{G}(\mu, \nu)$.
This shows (i).

By Lemma \ref{lemma:stable} in Appendix \ref{app:stability}, conditions (ii) and (iii) 
of Theorem \ref{thm:main} are stable under convergence in total variation, which
implies (ii).
		
Regarding (iii), if $\mu$ and $\nu$ have finite supports $S_{\mu} \subseteq \cX$ and 
$S_{\nu} \subseteq \cY$, respectively, every measure $\pi \in \Pi(\mu, \nu)$ is 
supported by the finite subset $S_{\mu} \times S_{\nu} \subseteq \cX \times \cY$. Since
weak convergence among such measures is equivalent to convergence in 
total variation, (iii) follows from (ii).

To show (iv), we consider a Markovian graph $G = (V,E)$ with $V = \{1, 2, 3\}$ and 
$E = \{(1,2), (2,3) \}$. Consider the measures
$\mu = \nu = \theta \otimes \eta \otimes \theta$ on $[0,1]^3$, where 
$\theta = \frac{1}{2}(\delta_{0} + \delta_1)$ and $\eta$ is the uniform 
distribution on $[0,1]$. We show that $\Pi^{\rm bc}_G(\mu, \nu)$ is not weakly 
closed. Then $\Pi_G(\mu, \nu)$ cannot be weakly closed either. Otherwise, 
$\Pi_G(\nu, \mu)$ and, as a consequence, $\Pi^{\rm bc}_G(\mu, \nu)$ 
would also have to be weakly closed. To prove that $\Pi^{\rm bc}_G(\mu, \nu)$ 
is not weakly closed, we denote by $\gamma \in \Pi(\eta, \eta)$ the uniform 
distribution on the diagonal $D = \{(x,x) \in [0,1]^2 : x \in [0,1] \}$ and by 
$\gamma^{1,k}, \gamma^{2,k} \in \Pi(\eta, \eta)$, $k \ge 3$, the uniform distributions 
on the shifted diagonals $D^{1, k}, D^{2, k}$ given by
\begin{align*}
D^{1, k} &= \left\{(x, x+ 1/k): x \in [0, 1- 1/k]\right\} 
\cup \left\{(x, x+ 1/k -1) : x \in (1- 1/k, 1]\right\}, \\
D^{2, k} &= \left\{(x, x + 1- 1/k) : x \in [0, 1/k]\right\} \cup 
\left\{[x, x- 1/k) : x \in [1/k, 1]\right\}.
\end{align*}
Clearly, $\gamma^{1,k}$ and $\gamma^{2,k}$ both weakly converge to $\gamma$
for $k \to \infty$. Now, let us define the measures $\pi^k$, $k \ge 3$, on $([0,1]^2)^3$
through 
\[
\pi^k(dz_1, dz_2, dz_3) = \pi_1^k(dz_1) \pi_{1, 2}^k(z_1, dz_2) \pi_{2, 3}^k(z_2, dz_3)
\] 
for
\[
\pi_1^k = \frac{1}{2}(\delta_{(0, 0)} + \delta_{(1, 1)}), \quad
\pi_{1, 2}^k(0, 0) = \gamma^{1, k}, \quad
\pi_{1, 2}^k(1, 1) = \gamma^{2, k}
\]
and
\[
\pi_{2, 3}^k(z_2) = 
\begin{cases}
\frac{1}{2} (\delta_{(0, 0)} + \delta_{(1, 1)}) \text{ if } z_2 \in D^{1, k}\\
\frac{1}{2} (\delta_{(0, 1)} + \delta_{(1, 0)}) \text{ if } z_2 \in D^{2, k}.
\end{cases}
\]
Then, $\pi^k \in \Pi^{\rm bc}_G(\mu, \nu)$ for all $k \ge 3$, and for $k \to \infty$, $\pi^k$ weakly 
converges to $\pi$ given by
\[
\pi(dz_1, dz_2, dz_3) = \pi_1(dz_1) \pi_{1, 2}(dz_2) \pi_{2, 3}(z_1, dz_3)
\] 
for
\[
\pi_1 = \frac{1}{2}(\delta_{(0, 0)} + \delta_{(1, 1)}), \quad
\pi_{1, 2} = \gamma
\]
and
\[
\pi_{2, 3}(z_1) = 
\begin{cases}
\frac{1}{2} (\delta_{(0, 0)} + \delta_{(1, 1)}) \text{ if } z_1 = (0,0)\\
\frac{1}{2} (\delta_{(0, 1)} + \delta_{(1, 0)}) \text{ if } z_1 = (1,1).
\end{cases}
\]
So $\pi$ is no longer Markovian, and therefore, $\pi \not\in \Pi^{\rm bc}_G(\mu, \nu)$, 
which proves (iv).
\end{proof}

\section{Causal Wasserstein distances}
\label{sec:Wasserstein}

In this section, we study Wasserstein distances arising from $G$-bicausal couplings.
We show that they define semimetrics on appropriate spaces of $G$-compatible 
measures but in general do not satisfy the triangle inequality. In Section \ref{subsec:ate} 
we prove that average treatment effects are continuous with respect to $G$-Wasserstein 
distance, and in Section \ref{subsec:closeclose} we show that small perturbations of structural 
causal models lead to small deviations in $G$-Wasserstein distance. 
In Subsection \ref{subsec:geodesic} we investigate the interpolation of two $G$-compatible 
measures such that the causal structure given by $G$ is respected.
In the whole section, we let $\cX = \cY$ and denote by $d_{\cX}$ a metric 
$d_{\cX} \colon \cX \times \cX \to \bR_+$ generating\footnote{we assume
$d_{\cX}$ generates the topology, but $\cX$ is not necessarily complete with respect to 
$d_{\cX}$} the given Polish topology on $\cX$.

\begin{remark}[Standard Wasserstein distances]
For a $p \in [1, \infty)$, we denote by $\cP_p(\cX)$ the set of all 
measures $\mu \in \cP(\cX)$ satisfying 
\be \label{x0}
\int_{\cX} d_{\cX}(x, x_0)^p \mu(dx) < \infty \quad \mbox{for some } x_0 \in \cX.
\ee
Note that \eqref{x0} implies 
\[
\int_{\cX} d_{\cX}(x, x'_0)^p \mu(dx)
\le 2^{p-1} \brak{ \int_{\cX} d_{\cX}(x,x_0)^p \mu(dx) + 
d_{\cX}(x_0, x'_0)^p} < \infty \quad \mbox{for every } x'_0 \in \cX.
\]
So, if \eqref{x0} holds for one, it holds for all $x_0 \in \cX$. 
The standard $p$-Wasserstein distance $W_p(\mu, \nu)$ between two 
measures $\mu$ and $\nu$ in $\cP_p(\cX)$ is given by 
\[
W_p^p(\mu, \nu) := \inf_{\pi \in \Pi(\mu, \nu)} 
 \int_{\cX \times \cX} d_{\cX}(x, y)^p \,\pi(dx, dy).
\]
It is well known (see e.g. \cite{villani2009optimal}) that the infimum is attained and
$W_p$ defines a metric on $\cP_p(\cX)$. Moreover, 
$\Pi(\mu, \nu)$ is a weakly compact subset of $\cP(\cX \times \cX)$.

\end{remark}

\begin{definition}[$G$-Wasserstein distance]\label{def:gwasserstein}
Let $G = (V,E)$ be a sorted DAG and $p \in [1, \infty)$. 
We denote $\cP_{G,p}(\cX) = \cP_G(\cX) \cap \cP_p(\cX)$
and define the $p$-th order $G$-Wasserstein distance 
$W_{G, p}(\mu, \nu)$ between two measures $\mu$ and 
$\nu$ in $\mathcal{P}_{G,p}(\cX)$ by
\be \label{WGp}
W^p_{G, p}(\mu, \nu) 
:= \inf_{\pi \in \Pi^{\rm bc}_G(\mu, \nu)} \int_{\cX \times \cX} d_{\cX}(x, y)^p \,\pi(dx, dy).
\ee
\end{definition}

The following result provides some elementary properties of the 
$G$-Wasserstein distance $W_{G,p}$.

\begin{proposition} \label{prop:wassersteinprop}
Let $G = (V,E)$ be a sorted DAG and $p \in [1, \infty)$. Then
	\begin{itemize}
\item[{\rm (i)}] for another directed graph $G' = (V, E')$ with $E' \subseteq E$, one has 
$W_{G, p} \leq W_{G', p}$, and therefore,
	\[
\{\nu \in \mathcal{P}(\cX) : W_{G', p}(\mu, \nu) \leq r \} 
\subseteq \{\nu \in \mathcal{P}(\cX) : W_{G, p}(\mu, \nu) \leq r \}
		\]
for all $\mu \in \mathcal{P}(\cX)$ and $r > 0$, 
\item[{\rm (ii)}] $W_{G,p}$ is a semimetric\footnote{that is,
it has all properties of a metric except the triangle inequality} on $\mathcal{P}_{G,p}(\cX)$
and
\item[{\rm (iii)}] if $\mu, \nu \in \cP_{G,p}(\cX)$ are finitely supported, the infimum 
in \eqref{WGp} is attained.
	\end{itemize} 
	\end{proposition}
	\begin{proof}
(i): If $E' \subseteq E$, one has $\pa'_i \subseteq \pa_i$, $i = 1, \dots, n$, for the 
corresponding parent sets. Therefore, it can be seen from Definition \ref{def:causal} 
that $\Pi^{\rm bc}_{G'}(\mu, \nu) \subseteq \Pi^{\rm bc}_G(\mu, \nu)$ for all $\mu, \nu \in \cP(\cX)$,
which implies (i).

(ii) It is clear that $W_{G,p}$ is symmetric and non-negative. 
Moreover, it follows from
Proposition \ref{prop:basic_prop}.(i) that for 
all $\mu, \nu \in \cP_{G,p}$, there exists a coupling $\pi \in \Pi^{\rm bc}_G(\mu, \nu)$.
Therefore, one has for any $x_0 \in \cX$, 
\beas
W^p_G(\mu, \nu) &\le& \int_{\cX \times \cX} 
(d_{\cX}(x,y))^p \pi(dx, dy) \le \int_{\cX \times \cX} 
(d_{\cX}(x,x_0) + d_{\cX}(y,x_0) )^p \pi(dx, dy) \\
&\le& 2^{p-1} \brak{ \int_{\cX} 
(d_{\cX}(x,x_0))^p \mu(dx) + \int_{\cX} 
(d_{\cX}(x, x_0))^p \nu(dx)} < \infty.
\eeas
Next, note that for $X \sim \mu \in \cP_{G,p}(\cX)$, the distribution of $(X,X)$ belongs to 
$\Pi^{\rm bc}_{G}(\mu, \mu)$, from which one obtains $W_{G, p}(\mu, \mu) = 0$.
Finally, since $\Pi^{\rm bc}_G(\mu, \nu) \subseteq \Pi(\mu, \nu)$, one has 
\[
W_{G, p}(\mu, \nu) \geq W_p(\mu, \nu) > 0 \quad \mbox{if } \mu \neq \nu,
\]
where the last inequality holds since the standard $p$-Wasserstein distance 
$W_p$ is a metric.

(iii) If $\mu, \nu \in \cP_{G,p}(\cX)$ are finitely supported, choose
a sequence $(\pi^k)_{k \ge 1}$ in $\Pi^{\rm bc}_G(\mu, \nu)$ such that 
\be \label{limk}
\int_{\cX} d_{\cX}(x,y)^p d\pi^k(dx, dy) \to W_{G,p}^p(\mu, \nu) \quad
\mbox{for } k \to \infty.
\ee
Since $\Pi(\mu, \nu)$ is a weakly compact subset of
$\cP(\cX \times \cX)$, there exists a subsequence, again denoted $(\pi^k)_{k \ge 1}$, 
which weakly converges to a measure $\pi \in \Pi(\mu, \nu)$.
By Proposition \ref{prop:basic_prop}.(iii), $\pi$ belongs to $\Pi^{\rm bc}_G(\mu, \nu)$, 
and, by \eqref{limk}, it minimizes \eqref{WGp}.
\end{proof}

We show in Appendix \ref{sec:counter} below that in general, 
$W_{G,p}$ does not satisfy the triangle inequality.

\subsection{Continuity of average treatment effects}
\label{subsec:ate}

The average treatment effect estimates the causal effect of a treatment or intervention. 
In the following we show that it is Lipschitz continuous with respect to $W_{G,1}$ 
under a change of the underlying probability model while the same is not
true for the standard Wasserstein distance. An important implication of this result is that 
one can obtain bounds on the error resulting from estimating average treatment
effects from data by controlling the $G$-Wasserstein estimation error. For recent 
studies on how average treatment effects can be estimated from data; see e.g.
\cite{savje2021average, huang2022robust}.

Let us assume there exist two indices $j,k \in V = \{1, \dots, n\}$ such that $j < k$, 
$\cX_j = \{0, 1\}$ and $\cX_k$ is a compact subset of $\bR$. 
$X_j$ indicates whether a treatment is applied 
or not, and $X_k$ describes a resulting outcome. Suppose the metric $d_{\cX}$ on 
$\cX = \cX_1 \times \dots \times \cX_n$ is 
given by $d_{\cX}(x,y) := \sum_{i=1}^n d_{\cX_i}(x_i, y_i)$, where 
$d_{\cX_i}(x_i, y_i) = |x_i - y_i|$ for $i = j, k$.
Using Pearl's do-notation (cf. the back-door adjustment 
\cite[Theorem 3.3.2]{pearl2009causality} applied with $\pa_j$),
the average treatment effect under a model $\mu \in \cP_G(\cX)$ is given by
\be \label{ate}
\begin{aligned}
\psi^\mu &:= \int_{\cX_k} x_k \,\mu(dx_k \mid \Do(x_j = 1)) - \int_{\cX_k}
x_k \, \mu(dx_k \mid \Do(x_j = 0)) \\
&= \int_{\cX_{\pa_i}} \int_{\cX_k} x_k \,\mu(dx_k \mid x_j = 1, x_{\pa_j}) \, \mu(dx_{\pa_j}) 
-\int_{\cX_{\pa_i}} \int_{\cX_k} x_k \,\mu(dx_k \mid x_j = 0, x_{\pa_j}) \, \mu(dx_{\pa_j}),
\end{aligned}
\ee
where for the purposes of this paper, the second line 
serves as definition of the do-notation in the first line.
In the following, we fix a constant $\delta > 0$ and consider the set 
$\mathcal{P}_{G}^{\delta, j}(\cX)$ of models $\mu \in \mathcal{P}_G(\cX)$ 
such that the propensity score $\mu(x_j = 1 \mid x_{\pa_j})$ satisfies 
\[
\delta \le \mu(x_j = 1 \mid x_{\pa_j}) \le 1 - \delta 
\quad \mbox{for $\mu$-almost all $x_{\pa_i}$}.
\]
Then the following holds.

\begin{proposition}\label{prop:ate}
Let $G= (V,E)$ be a sorted DAG, $\delta > 0$ and $j < k$, $\cX_j, \cX_k$, $d_{\cX}$ as above.
Then there exists a constant $C \ge 0$ such that 
\[
\left| \psi^\mu - \psi^\nu \right| \leq C \, W_{G, 1}(\mu, \nu)
\]
for all $\mu, \nu \in \mathcal{P}_G^{\delta, j}(\cX)$.	
\end{proposition}

\begin{proof}
Note that 
	\begin{align*}
&\int_{\cX_k} x_k \,\mu(dx_k \mid \Do(x_j = 1)) \\
&= \int_{\cX_{\pa_j}} \int_{\cX_k}  x_k  \, \mu(dx_k \mid x_j = 1, 
x_{\pa_j}) \frac{\mu(x_j = 1 \mid x_{\pa_j})}{\mu(x_j = 1 \mid x_{\pa_j})} \, 
\mu(dx_{\pa_j}) \\
&= \int_{\cX_{\pa_j}} \int_{\cX_j} \int_{\cX_k} 
\frac{x_k 1_{\crl{x_j = 1}} }{\mu(x_j = 1 \mid x_{\pa_j})}   \,
\mu(dx_k \mid x_j, x_{\pa_j}) \mu(dx_j \mid x_{\pa_j}) \, \mu(dx_{\pa_j}) \\
&= \int_{\cX_{\pa_j}} \int_{\cX_j} \int_{\cX_k} \frac{x_k 1_{\crl{x_j = 1}}}{\mu(x_j = 1 
\mid x_{\pa_j})} \,\mu(dx_k, dx_j, dx_{\pa_j}) \\
&= \int_{\cX}  \frac{x_k 1_{\crl{x_j = 1}}}{\mu(x_j = 1 \mid x_{\pa_j})} \,\mu(dx)\\
&= \int_{\cX} f g h \, \mu(dx),
\end{align*}
where $f(x) = 1_{\{x_j = 1\}} = x_j$, $g(x) = x_k$ and  
$h(x) = \mu(x_j = 1 \mid x_{\pa_j})^{-1}$. 
Moreover, one has
\beas
&& f(x)g(x)h(x) - f(y)g(y)h(y)\\
&=& [f(x) - f(y)]\, g(x)h(x) + f(y) [g(x) - g(y)] h(x)) + f(y) g(y) [h(x) - h(y)].
\eeas
Since by assumption, $\cX_k$ is compact and $\mu(x_j \mid x_{\pa_j})$
bounded from below, there exists a constant $K \ge 0$ such that 
\[
|g(x) h(x)| \le K, \quad |f(y)h(x)| \le K \quad \mbox{and} \quad 
|f(y) g(y)| \le K \quad \mbox{for all} \quad x,y \in \cX.
\]
Let $\varepsilon > 0$ and choose a coupling $\pi \in \Pi^{\rm bc}_G(\mu, \nu)$ such that
\[
\int_{\cX^2} d_{\cX}(x,y) \pi(dx, dy) \le W_{G,1}(\mu, \nu) + \varepsilon.
\]
Then, 
\beas
	&&\left| \int_{\cX_k} x_k \,\mu(dx_k \mid \Do(x_j = 1)) 
	- \int_{\cX_k} x_k \,\nu(dx_k \mid \Do(x_j = 1)) \right| 
	= \left| \int_{\cX} fgh \,d\mu - \int_{\cX} fgh \,d\nu \right| \\
	&=& \left| \int \edg{f(x)g(x)h(x) - f(y)g(y) h(y)} \,\pi(dx, dy)\right| \\
	&\leq & K \int_{\cX} \brak{|f(x) - f(y)| + |g(x) - g(y)| 
	+ |h(x) - h(y)|} \,\pi(dx, dy)
	\eeas
	It is clear that $\int_{\cX} \brak{|f(x) - f(y)| + |g(x) - g(y)|} \, \pi(dx, dy) \leq W_{G, 1}(\mu, \nu) 
	+ \varepsilon$. Moreover, by the lower bound on the propensity score and since
	\[
	\left| \frac{1}{a} - \frac{1}{b}\right| \leq \frac{1}{\delta^2} \, |a-b| \text{ for all } a, b \ge \delta,
	\]
we obtain from Corollary \ref{cor:main} that
\beas
&& \delta^2 \int_{\cX^2} |h(x) - h(y)| \,\pi(dx, dy)
\leq \int_{\cX^2} \left| \mu(x_j = 1 \mid x_{\pa_j}) - 
\nu(y_j = 1 \mid y_{\pa_j}) \right| \,\pi(dx, dy) \\
&=& \int_{\cX^2} \left| \pi(x_j = 1 \mid x_{\pa_j}, y_{\pa_j}) - \pi(y_j = 1 \mid x_{\pa_j}, 
y_{\pa_j}) \right| \pi(dx, dy) \\
&=& \int_{\cX^2_{\pa_j}} \left| \int_{\cX_j^2} (x_j - y_j) \pi(dx_j, dy_j \mid y_{\pa_j}, y_{\pa_j}) \right| 
\pi(dx_{\pa_j}, dy_{\pa_j})\\
&\leq& \int_{\cX^2} |x_j - y_j| \, \pi(dx, dy) \leq W_{G, 1}(\mu, \nu) + \varepsilon.
\eeas
Since $\varepsilon > 0$ was arbitrary, this shows that
\be \label{Do1}
\left| \int_{\cX_k} x_k \,\mu(dx_k \mid \Do(x_j = 1)) 
- \int_{\cX_k} x_k \,\nu(dx_k \mid \Do(x_j = 1)) \right| 
\le K \brak{1 + \frac{1}{\delta^2}} W_{G,1}(\mu, \nu).
\ee
Analogously, one deduces from the upper bound on the propensity score that
\be \label{Do0}
\left| \int_{\cX_k} x_k \,\mu(dx_k \mid \Do(x_j = 0)) 
- \int_{\cX_k} x_k \,\nu(dx_k \mid \Do(x_j = 0)) \right| 
\le K \brak{1 + \frac{1}{\delta^2}} W_{G,1}(\mu, \nu).
\ee
Now, the proposition follows from a combination of \eqref{Do1} and \eqref{Do0}.
\end{proof}

\begin{remark}
It is easy to see that Proposition \ref{prop:ate} does not hold with the 
standard $1$-Wasserstein distance $W_1$ instead of $W_{G,1}$ since 
the topology generated by $W_1$ is not fine enough. Indeed, measures that are 
close with respect to $W_1$ may have completely different 
transition probabilities $\mu(dx_k \mid x_j = 1, x_{\pa_j})$, which play 
an important role in the definition of the average treatment effect $\psi^\mu$; 
see \eqref{ate}.
\end{remark}

\subsection{Perturbation of structural causal models}
\label{subsec:closeclose}

We know from Proposition \ref{prop:wassersteinprop}.(i) that the topology 
generated by $W_{G,p}$ is the finer the fewer edges the graph $G$ has.
In Section \ref{subsec:ate} we saw that average treatment effects are Lipschitz continuous 
with respect to $W_{G,1}$ but not with respect to the standard $1$-Wasserstein distance $W_1$, 
which corresponds to a fully connected graph. In this section we show that 
the topology generated by $W_{G,1}$ is not too fine for practical purposes by 
proving that small perturbations of structural causal modes lead to distributions that 
are close with respect to $W_{G,1}$.

\begin{proposition}
Let $G = (V,E)$ be a sorted DAG and consider for each $i=1, \dots, n$,
a metric $d_{\cX_i}$ on $\cX_i$, which generates the topology on $\cX_i$.
Define the metric $d_{\cX}$ on 
$\cX = \cX_1 \times \dots \times \cX_n$ by 
$d_{\cX}(x, y) = \sum_{i=1}^n d_{\cX_i}(x_i, y_i)$ and consider
$\cX$-valued random variables 
$X \sim \mu \in \mathcal{P}_G(\cX)$ and $Y \sim \nu \in \mathcal{P}_G(\cX)$
such that for all $i =1, \dots, n$,
\[
X_i = f_i(X_{\pa_i}, U_i) \quad \mbox{and} \quad
Y_i = g_i(Y_{\pa_i}, V_i)
\]
for measurable functions $f_i, g_i  \colon \cX_{\pa_i} \times \bR \to \cX_i$
and $\bR$-valued random variables $U_i, V_i$ satisfying
\begin{itemize}
\item[{\rm (i)}] $f_i$ is $L_i$-Lipschitz for a constant $L_i \ge 0$,
\item[{\rm (ii)}] $U_1, \dots, U_n$ are independent and
\item[{\rm (iii)}] $V_1, \dots, V_n$ are independent.
\end{itemize}
Then there exists a constant $C \ge 0$ depending on $G$ and
$L_1, \dots, L_n$ such that
\be \label{WGeps}
W_{G, 1}(\mu, \nu) \leq C \sum_{i=1}^n \big\{\|f_i - g_i \|_{\infty} 
+ W_1(\cL(U_i), \cL(V_i)) \big\},
\ee
where $\cL(U_i)$ and $\cL(V_i)$ are the distributions of $U_i$ and $V_i$, respectively.
\end{proposition}

\begin{proof}
It is well-known that there exist independent pairs of random variables
$(\tilde{U}_i, \tilde{V}_i)$ such that
$\cL(\tilde{U}_i) = \cL(U_i)$, $\cL(\tilde{V}_i) = \cL(V_i)$ and
\[
\bE \, |\tilde{U}_i - \tilde{V}_i| = W_1(\cL(U_i), \cL(V_i))
\quad \mbox{for all } i = 1, \dots, n;
\]
see e.g. \cite{villani2009optimal}. 
Let us define iteratively
		\be \label{tildeXY}
		(\tilde{X}_i, \tilde{Y}_i) = (f_i(\tilde{X}_{\pa_i}, \tilde{U}_i), 
		g_i(\tilde{Y}_{\pa_i}, \tilde{V}_i)), \quad 
		i = 1, \dots, n.
		\ee
Then, $\tilde{X} \sim X$ and $\tilde{Y} \sim Y$. Moreover, it can be seen from 
\eqref{tildeXY} that 		\[
		(\tilde{X}_i, \tilde{Y}_i) \ind_{\tilde{X}_{\pa_i}, \tilde{Y}_{\pa_i}} 
		(\tilde{X}_{1:i-1}, \tilde{Y}_{1:i-1}), \quad
		\tilde{X}_i \ind_{\tilde{X}_{\pa_i}} \tilde{Y}_{\pa_i} \quad \mbox{and} \quad
		\tilde{Y}_i \ind_{\tilde{Y}_{\pa_i}} \tilde{X}_{\pa_i}
		\quad \mbox{for all } i =2, \dots, n.
		\]
So it follows from Corollary \ref{cor:main} that the distribution $\pi$ of
		$(\tilde{X}, \tilde{Y})$ belongs to $\Pi^{\rm bc}_G(\mu, \nu)$.
		
If we can show that 
\be \label{Ci}
\bE \, d_{\cX_i}(\tilde{X}_i, \tilde{Y}_i)
\leq C_i \sum_{j=1}^i \big\{ \|f_j - g_j \|_{\infty} 
+ W_1(\cL(U_j), \cL(V_j)) \big\}
\ee
for constants $C_i \ge 0$ depending on $G$ and $L_1, \dots, L_i$,
we obtain \eqref{WGeps} with $C = \sum_{i=1}^n C_i$. 	
We prove \eqref{Ci} by induction over $i = 1, \dots, n$. First, note that 
\beas
&& \bE \, d_{\cX_1}(\tilde{X}_1, \tilde{Y}_1) \leq 
 \bE \edg{d_{\cX_1}(f_1(\tilde{U}_1), f_1(\tilde{V}_1)) 
+ d_{\cX_1} (f_1(\tilde{V}_1), g_1(\tilde{V}_1)}\\
&\le& L_1 \, \bE |\tilde{U}_1 - \tilde{V}_1| 
+ \|f_1 - g_1\|_{\infty} 
= L_1 \, W_1(\cL(U_1), \cL(V_1))
+ \|f_1 - g_1\|_{\infty},
\eeas
showing that for $i = 1$, \eqref{Ci} holds with $C_1 = \max \crl{L_1,1}$.
For $i \ge 2$, assuming that \eqref{Ci} holds for all $j < i$, one obtains
	\beas
&& \bE \, d_{\cX_i}(\tilde{X}_i, \tilde{Y}_i) \leq 
\bE \edg{d_{\cX_i}(f_i(\tilde{X}_{\pa_i}, \tilde{U}_i), f_i(\tilde{Y}_{\pa_i}, \tilde{V}_i)) 
+ d_{\cX_i} (f_i(\tilde{Y}_{\pa_i}, \tilde{V}_i), g_i(\tilde{Y}_{\pa_i}, \tilde{V}_i)} \\
&\leq& L_i\, \bE \edg{\sum_{j \in \pa_i} d_{\cX_j}(\tilde{X}_j, \tilde{Y}_j) + |\tilde{U}_i - \tilde{V}_i| }
+ \|f_i - g_i\|_\infty\\
&\le& L_i \sum_{j \in \pa_i} C_j \sum_{k=1}^{j} \big\{ \|f_k - g_k \|_{\infty} 
+ W_1(\cL(U_k), \cL(V_k)) \big\}
+ L_i  W_1(\cL(U_i), \cL(V_i)) + \|f_i - g_i\|_{\infty}.
\eeas
So \eqref{Ci} holds with 
\[
C_i = \max \crl{L_i \sum_{j \in \pa_i} C_j \, ,\, L_i \, , \, 1},
\quad i = 1, \dots, n,
\] 
and the proposition follows.
\end{proof}

\subsection{Causal Wasserstein interpolation}\label{subsec:geodesic} 

In this section, we assume that in addition to a metric $d_{\cX}$ compatible 
with the Polish topology, $\cX$ is endowed with a vector space structure 
so that the vector space operations are continuous.

Note that for a sorted DAG $G = (V,E)$, $p \in [1, \infty)$ and given measures 
$\mu, \nu \in \cP_{G,p}(\cX)$,
there exists a sequence $(\pi^k)_{k \ge 1}$ of 
couplings in $\Pi^{\rm bc}_{G}(\mu, \nu)$ such that
\[
\int_{\cX \times \cX} d_{\cX}(x,y)^p d\pi^k(dx, dy) \to W_{G,p}^p(\mu, \nu) \quad
\mbox{for } k \to \infty.
\]
Since $\Pi^{\rm bc}_{G}(\mu, \nu) \subseteq \Pi(\mu, \nu)$, and the latter
is a weakly compact subset of
$\cP(\cX \times \cX)$, there exists a subsequence, again denoted $(\pi^k)_{k \ge 1}$, 
which weakly converges to a measure $\pi \in \Pi(\mu, \nu)$. This leads us to the 
following 

\begin{definition}\label{def:interpol}
Let $G = (V,E)$ be a sorted DAG and $p \in [1, \infty)$.
For given $\mu, \nu \in \cP_{G,p}(\cX)$, let $(\pi^k)_{k \ge 1}$ 
be a sequence of couplings in $\Pi^{\rm bc}_{G}(\mu, \nu)$ such that 
\[
\int_{\cX \times \cX} d_{\cX}(x,y)^p d\pi^k(dx, dy) \to W_{G,p}^p(\mu, \nu) \quad 
\mbox{for } k \to \infty,
\]
and $\pi^k$ weakly converges to some $\pi \in \Pi(\mu, \nu)$. For $\lambda \in [0,1]$, 
denote by $\kappa_{\lambda}$ the distribution of 
$(1-\lambda) X + \lambda Y$, where $(X, Y) \sim \pi$. Then we call
$(\kappa_\lambda)_{\lambda \in [0, 1]}$ a $W_{G, p}$-interpolation between $\mu$ and $\nu$.
\end{definition}

The measure $\pi$ in Definition \ref{def:interpol} 
is a limit of measures $\pi^k \in \Pi^{\rm bc}_{G}(\mu, \nu)$.
But in general, it might not belong to $\Pi^{\rm bc}_{G}(\mu, \nu)$ 
(see Proposition \ref{prop:basic_prop}.(iv)), and even if 
it does, the measures $\kappa_{\lambda}$ are not necessarily $G$-compatible
(see Example \ref{ex:Markov} below). Nevertheless, there are relevant situations where the interpolations $\kappa_\lambda$ are $G$-compatible, which we study below.

The following result gives conditions under which the distribution of a convex 
combination of the form $(1-\lambda)X + \lambda Y$ is $G$-compatible. 

\begin{proposition} \label{prop:Gcomp}
Let $G = (V,E)$ be a sorted DAG and $\mu, \nu \in \cP_{G}(\cX)$. 
Assume $(X,Y) \sim \pi$ for a distribution $\pi \in \Pi^{\rm bc}_G(\mu, \nu)$. 
Then, for any $\lambda \in [0,1]$, the distribution of $(X, Y, (1-\lambda)X + \lambda Y)$ is 
$G$-compatible. Moreover, if the maps 
\[
h_{\lambda, i} : \cX_{\pa_i} \times \cX_{\pa_i} \rightarrow \cX_{\pa_i}, 
~(x_{\pa_i}, y_{\pa_i}) \mapsto (1-\lambda) x_{\pa_i} + \lambda y_{\pa_i}
\] 
are $\pi_{\pa_i}$-almost surely injective for all $i = 2, \dots, n$, 
then the distribution of $\lambda X + (1-\lambda)Y$ is $G$-compatible.
\end{proposition}

\begin{proof}
Since $\pi$ is in $\Pi^{\rm bc}_G(\mu, \nu)$, we obtain from Corollary \ref{cor:main} that
	\[
(X_i, Y_i) \ind_{X_{\pa_i}, Y_{\pa_i}} (X_{1:i-1}, Y_{1:i-1}), 
	\]
which implies
\be \label{XYZ}
(X_i, Y_i, Z_{\lambda, i}) 
\ind_{X_{\pa_i}, Y_{\pa_i}, Z_{\lambda, \pa_i}} (X_{1:i-1}, Y_{1:i-1}, Z_{\lambda, 1:i-1})
\ee
for all $\lambda \in [0,1]$ and $Z_{\lambda} = (1-\lambda) X + \lambda Y$. By
Definition \ref{def:Gcomp}, this shows that the distribution of $(X,Y, Z_{\lambda})$ 
is $G$-compatible.

Now, let us assume that for a given $\lambda \in [0,1]$, the
maps $h_{\lambda,i}$ are $\pi_{\pa_i}$-almost surely injective for all 
$i = 2, \dots, n$. Then, there exists a measurable subset 
$A \subseteq \cX \times \cX$ with $\pi(A) = 1$ such that 
$h_{\lambda, i}$ is injective on the projection of $A$ on $\cX_{\pa_i} \times \cX_{\pa_i}$
for all $i = 2, \dots, n$. If $(\Omega, {\cal F}, \bP)$ is the probability space on which
$(X,Y)$ is defined, we can, without loss of generality, assume 
that $\Omega = (X,Y)^{-1}(A)$ since restricting $(X,Y)$ to
$(X,Y)^{-1}(A)$ does not change its distribution.
But then, $(X_{\pa_i}(\omega), Y_{\pa_i}(\omega)) \mapsto 
(1-\lambda) X_{\pa_i}(\omega) + \lambda Y_{\pa_i}(\omega)$ 
is injective for all $\omega \in \Omega$ and $i=2, \dots, n$. It follows that 
$\sigma(X_{\pa_i}, Y_{\pa_i}) = \sigma(Z_{\lambda, \pa_i})$, which together with 
\eqref{XYZ} gives
\[
Z_{\lambda, i} \ind_{Z_{\lambda, \pa_i}} Z_{\lambda, 1:i-1} 
\quad \mbox{for all } i = 2, \dots, n,
\]
showing that the distribution of $Z_{\lambda}$ is $G$-compatible.
\end{proof}

\begin{corollary} \label{cor:finmarg}
Let $G = (V,E)$ be a sorted DAG and $\mu, \nu \in \cP_{G}(\cX)$. 
Assume $(X,Y) \sim \pi$ for a distribution $\pi \in \Pi^{\rm bc}_G(\mu, \nu)$. 
\begin{itemize}
\item[{\rm (i)}] If $\mu$ and $\nu$ are finitely supported, the distribution 
of $(1-\lambda) X + \lambda Y$ is $G$-compatible for all but finitely 
many $\lambda \in [0,1]$.
\item[{\rm (ii)}] If $\mu$ and $\nu$ are countably supported, the distribution 
of $(1-\lambda) X + \lambda Y$ is $G$-compatible for all but countably 
many $\lambda \in [0,1]$.
\end{itemize}
\end{corollary}

\begin{proof}
We show (i). The proof of (ii) is analogous. Assume 
$\mu$ and $\nu$ are of the form $\mu = \sum_{j=1}^J p_j \delta_{x_i}$ and 
$\nu = \sum_{j=1}^J q_i \delta_{y_j}$ for $p_j, q_j \ge 0$ and 
$x_j, y_j \in \cX$, $j = 1, \dots, J$. Then, for all
$i \ge 2$, $x,x' \in \{x_1, \dots, x_J\}$ and $y, y' \in \{y_1, \dots, y_J\}$ such that 
 $(x_{\pa_i}, y_{\pa_i}) \neq (x'_{\pa_i}, y'_{\pa_i})$, there exists at most one
 $\lambda \in [0,1]$ satisfying
 \[
\begin{aligned}
&(1-\lambda) x_{\pa_i} + \lambda y_{\pa_i} 
= (1-\lambda) x'_{\pa_i} + \lambda y'_{\pa_i} \\
\Leftrightarrow & \quad 
(1-\lambda) (x_{\pa_i} - x'_{\pa_i}) =
\lambda (y'_{\pa_i} - y_{\pa_i} ).
\end{aligned}
\]
Therefore, for all but finitely many $\lambda \in [0,1]$, the maps
\[
h_{\lambda, i} : \cX_{\pa_i} \times \cX_{\pa_i} \rightarrow \cX_{\pa_i}, 
~(x_{\pa_i}, y_{\pa_i}) \mapsto (1-\lambda) x_{\pa_i} + \lambda y_{\pa_i}
\] 
are $\pi_{\pa_i}$-almost surely injective for all $i = 2, \dots, n$, and it follows 
from Proposition \ref{prop:Gcomp} that the distribution of 
$\lambda X + (1-\lambda)Y$ is $G$-compatible.
\end{proof}

\begin{corollary}\label{cor:interpolation}
Let $G = (V,E)$ be a sorted DAG, $p \in [1,\infty)$ and $\mu, \nu$ two finitely supported 
measures in $\cP_{G}(\cX)$ (and therefore also in $\cP_{G,p}(\cX)$). Then 
there exists a measure $\pi \in \Pi^{\rm bc}_G(\mu, \nu)$ such that 
\be \label{minpi}
\int_{\cX \times \cX} d_{\cX}(x,y)^p d\pi(dx, dy) = W_{G,p}^p(\mu, \nu).
\ee
Moreover, for $(X,Y) \sim \pi$, the distribution of $(1-\lambda)X + \lambda Y$
is $G$-compatible for all but finitely many $\lambda \in [0,1]$.
\end{corollary}

\begin{proof}
That there is a measure $\pi \in \Pi^{\rm bc}_G(\mu, \nu)$ satisfying \eqref{minpi} 
follows directly from Proposition \ref{prop:wassersteinprop}.(iii). Moreover, for $(X,Y) \sim \pi$, 
we obtain from Corollary \ref{cor:finmarg}.(i) that the distribution of $(1-\lambda)X + \lambda Y$
is $G$-compatible for all but finitely many $\lambda \in [0,1]$.
\end{proof}

\begin{example} \label{ex:Markov}
Consider the Markovian graph $G = (V,E)$ with $V = \{1, 2, 3\}$ and $E = \{(1,2), (2,3)\}$,
and take $\cX = \bR^3$ endowed with the Euclidean distance. The 
distributions $\mu = \frac{1}{2}(\delta_{(0, 0, 0)} + \delta_{(1, 1, 1)})$ and 
$\nu = \frac{1}{2}(\delta_{(0, 1, 0)} + \delta_{(1, 0, 1)})$ are both Markovian, and 
clearly, the optimal $W_2$-transport plan transports $(0, 0, 0)$ to $(0, 1, 0)$ 
and $(1, 1, 1)$ to $(1, 0, 1)$. It can directly be seen from Definition \ref{def:causal}
that this transport plan belongs to $\Pi^{\rm bc}_G(\mu, \nu)$. Therefore,
\[
\kappa_{\lambda} = \frac{1}{2} \brak{\delta_{(0, \lambda, 0)} + \delta_{(1, 1- \lambda, 1)}}, 
\quad \lambda \in [0,1],
\]
is the unique $W_{G,2}$-interpolation between $\mu$ and $\nu$. Note that
$\kappa_{\lambda}$ is Markovian for all $\lambda \in [0, 1]$ expect for 
$\lambda = 1/2$. But for $\lambda = 1/2$, it is still a weak limit of 
Markovian distributions.
\end{example}

\begin{figure}
	\includegraphics[width=0.32\textwidth]{binom.png}
	\includegraphics[width=0.32\textwidth]{trinom.png}
	\includegraphics[width=0.32\textwidth]{interpol033.png}
	\caption{The binomial model $\mu$ on the left, the trinomial model $\nu$ in the middle, and the interpolating measure $\kappa_{1/3}$ under $W_{G, 2}$ on the right as given by Corollary \ref{cor:interpolation} using the euclidean distance on $\mathbb{R}^3$ as the underlying metric.}
	\label{fig:binomtrinom}
\end{figure}

Next, we study an example where standard Wasserstein interpolation between 
two $G$-compatible measures leads to distributions that are far from $G$-compatible.

\begin{example} \label{ex:interpol}
Consider the same setup as in Example \ref{ex:Markov}. Denote by $\mu$ the distribution 
of the random walk $(X_1, X_2, X_3)$ given by $X_i = \sum_{j=1}^i \xi_j$ for i.i.d.\ innovations
$\xi_j$ with distribution $\bP[\xi_j = \pm 1] = 1/2$ and $\nu$ the distribution of 
$(Y_1, Y_2, Y_3)$ given by $Y_i = \sum_{j=1}^i \eta_j$ for i.i.d.\ innovations
$\eta_j$ with distribution $\bP[\eta_j = -1,0,1] = 1/3$. The first two pictures in
Figure \ref{fig:binomtrinom} show all possible trajectories of $X = (X_1, X_2, X_3)$
and $Y = (Y_1, Y_2, Y_3)$, respectively.

For the standard Wasserstein distance $W_2(\mu, \nu)$, a numerically obtained optimal coupling $\pi$ can be seen to satisfy 
	\begin{align*}
	\pi\big((X_3, Y_3) = (1, 0) \mid (X_1, Y_1) = (-1, 0), (X_2, Y_2) = (0, 0)\big) \\ < \pi\big((X_3, Y_3) = (1, 0) \mid (X_1, Y_1) = (1, 0), (X_2, Y_2) = (0, 0) \big),
	\end{align*}
showing that $(X_i, Y_i)_{i=1, 2, 3}$ is not Markovian under $\pi$. It follows that, for all 
but finitely many $\lambda \in [0,1]$, the resulting interpolations $\kappa_{\lambda}$ 
under this coupling $\pi$ are also not Markovian.\footnote{This follows from the 
same arguments as in the proofs of Proposition \ref{prop:Gcomp} and 
Corollary \ref{cor:finmarg}, where the main argument is that for $Z^\lambda_k = (1-\lambda) X_k + \lambda Y_k$, one has $\sigma(Z^\lambda_k) = \sigma(X_k, Y_k)$ for all but finitely many $\lambda \in [0, 1]$.}
	
On the other hand, using a $W_{G, p}$ interpolation as given by Corollary \ref{cor:interpolation}, the resulting interpolating measures $\kappa_\lambda$ are Markovian for all but finitely 
many $\lambda \in [0, 1]$. The last picture in Figure \ref{fig:binomtrinom} shows 
an illustration of the numerically computed causal interpolation
$\kappa_{1/3}$. 
\end{example}

\appendix
\section{Stability of conditional independence}
\label{app:stability}

\begin{lemma} \label{lemma:stable}
	Let $\cS, \cW, \cZ$ be measurable spaces and
	$S, W, Z$ the projections from $\cS \times \cW \times \cZ$
	to $\cS$, $\cW$, $\cZ$, respectively. Denote by $\cP(\cS \times \cW \times \cZ)$ the 
	collection of all probability measures on $\cS \times \cW \times \cZ$ equipped with 
	the product $\sigma$-algebra. Then the set
	\be \label{set}
	\{\pi \in \cP(\cS \times \cW \times \cZ) : S \ind_Z W \mbox{ with respect to } \pi \}
	\ee
	is closed under convergence in total variation.
\end{lemma}

\begin{proof}
	Let $(\pi^k)_{k \in \bN}$ be a sequence in \eqref{set} converging to a measure 
	$\pi \in \cP(\cS \times \cW \times \cZ)$ in total variation. By \eqref{condindep2}, one has
	\be \label{k=}
	\pi^k[A \times \cW \times \cZ \mid W, Z] = \pi^k[A \times \cW \times \cZ \mid Z] \quad \mbox{$\pi^k$-a.s.}
	\ee
	for every measurable subset $A \subseteq \cS$ and all $k \in \bN$.
	Let us denote 
	\[
	H_k = \pi^k[A \times \cW \times \cZ \mid Z] \quad \mbox{and} \quad
	H = \pi[A \times \cW \times \cZ \mid Z].
	\]
	If $\bE^k$ denotes expectation with respect to $\pi^k$ and $\bE$ 
	expectation with respect to $\pi$, one has
	\[
	\abs{\bE^k U - \bE \, U} =
	\abs{\int_0^1 \brak{\pi^k[U > u] - \pi[U > u]} du }
	\le \int_0^1 \abs{\pi^k[U > u] - \pi[U > u]} du
	\]
	for all random variables $U \colon \cS \times \cW \times \cZ \to [0,1]$,
	which shows that for every $\varepsilon > 0$, there exists a $k_0 \in \bN$ such that 
	\[
	\abs{\bE^k U - \bE \, U} \le \varepsilon \quad 
	\mbox{for every } k \ge k_0 \mbox{ and all such $U$.}
	\]
	In particular, 
	\[
	\abs{\bE^k (1_{A \times \cW \times \cZ} - H)^2 
		- \bE \, (1_{A \times \cW \times \cZ} - H)^2} \le \varepsilon
	\]
	and
	\[
	\abs{\bE^k (1_{A \times \cW \times \cZ} - H_k)^2 
		- \bE \, (1_{A \times \cW \times \cZ} - H_k)^2} \le \varepsilon
	\]
	for all $k \ge k_0$.
	Moreover, since the $H_k$ can be viewed as $L^2$-projections of 
	$1_{A \times \cW \times \cZ}$ on the space of square-integrable $Z$-measurable 
	random variables, one has
	\[
	\bE^k (1_{A \times \cW \times \cZ}  - H_k)^2 \le \bE^k(1_{A \times \cW \times \cZ}  - H)^2.
	\]
	Together with Pythagoras's theorem this gives
	\beas
	\bE (H_k - H)^2 &=& \bE (1_{A \times \cW \times \cZ}  - H_k)^2 
	- \bE (1_{A \times \cW \times \cZ}  - H)^2\\
	&\le& \bE^k (1_{A \times \cW \times \cZ}  - H_k)^2 + \varepsilon
	- \bE (1_{A \times \cW \times \cZ}  - H)^2\\
	&\le& \bE^k (1_{A \times \cW \times \cZ}  - H)^2 + \varepsilon
	- \bE (1_{A \times \cW \times \cZ}  - H)^2\\
	&\le& \bE (1_{A \times \cW \times \cZ}  - H)^2 + 2 \varepsilon
	- \bE (1_{A \times \cW \times \cZ}  - H)^2
	= 2 \varepsilon
	\quad \mbox{for all } k \ge k_0, 
	\eeas
	showing that $H_k \to H$ in $L^2(\pi)$. Analogously, one obtains that 
	$\pi^k[A \times \cW \times \cZ \mid W, Z]$ converges to 
	$\pi[A \times \cW \times \cZ \mid W, Z]$ in $L^2(\pi)$.
	So, equation \eqref{k=} is stable under convergence in total variation, 
	which proves the lemma.
\end{proof}

\section{Counterexample to the triangle inequality}
\label{sec:counter}

In this section, we provide an example showing that in general, 
$W_{G, 1}$ does not satisfy the triangle inequality. 
We consider the Markovian graph $G = (V,E)$ with vertices $V = \{1,2,3\}$ and edges 
$\{(1,2), (2,3) \}$ and choose $\cX = \cX_1 \times \cX_2 \times \cX_3$, where 
$\cX_1$, $\cX_2$, $\cX_3$ are all equal to the same abstract discrete 
Polish space $\crl{x_1, x_2, x_3, x_4}$. Note that the following three measures are all
Markovian:
\begin{align*}
\mu &= \frac{1}{4} \left( \delta_{(x_1, x_1, x_1)} + \delta_{(x_1, x_1, x_2)} 
+ \delta_{(x_2, x_1, x_1)} + \delta_{(x_2, x_1, x_2)} \right) \\
\nu &= \frac{1}{4} \left( \delta_{(x_1, x_2, x_1)} + \delta_{(x_1, x_2, x_2)} 
+ \delta_{(x_2, x_3, x_1)} + \delta_{(x_2, x_3, x_2)} \right)\\
\eta &= \frac{1}{4} \left( \delta_{(x_3, x_4, x_3)} + \delta_{(x_3, x_4, x_4)} 
+ \delta_{(x_4, x_4, x_3)} + \delta_{(x_4, x_4, x_4)} \right).
\end{align*}
We specify the distances between the 12 sequences $(x_1,x_1, x_1), (x_1, x_1, x_2), \dots$
used to define the measures $\mu, \nu, \eta$ with the following $12 \times 12$ matrix $M$:
\setcounter{MaxMatrixCols}{12}
\[\begin{bmatrix}
0  & 0.53 & 1.08 & 1.33 & 1.29 & 0.78 & 0.64 & 0.44 & 1.15 & 1.3  & 1.92 & 1.38 \\
0.53&  0  & 0.7&  1.05 &1.18& 0.91 &0.11 &0.97 &1.1  &1.83 &1.9  &1.91 \\
1.08& 0.7  &0   &0.98 &1.11 &0.49 &0.59 &0.64 &0.86 &1.88 &1.82 &1.58 \\
1.33& 1.05 &0.98 &0   &0.13 &1.15 &0.94 &0.98 &1.52 &1.25 &1.87 &1.92 \\
1.29& 1.18 &1.11 &0.13 &0   &1.02 &1.07 &0.85 &1.39 &1.38 &2  &1.79 \\
0.78& 0.91 &0.49 &1.15 &1.02 &0   &1.02 &1.13 &0.37 &1.74 &1.77 &1.22 \\
0.64& 0.11 &0.59 &0.94 &1.07 &1.02 &0   &1.08 &1.21 &1.94 &1.79 &2.02 \\
0.44& 0.97 &0.64 &0.98 &0.85 &1.13 &1.08 &0   &1.5  &1.24 &1.86 &0.94 \\
1.15& 1.1  &0.86 &1.52 &1.39 &0.37 &1.21 &1.5  &0   &1.37 &1.4  &0.85 \\
1.3 & 1.83 &1.88 &1.25 &1.38 &1.74 &1.94 &1.24 &1.37 &0   &0.62 &1.2  \\
1.92& 1.9  &1.82 &1.87 &2   &1.77 &1.79 &1.86 &1.4  &0.62 &0   &1.69 \\
1.38& 1.91 &1.58 &1.92 &1.79 &1.22 &2.02 &0.94 &0.85 &1.2  &1.69 &0  
\end{bmatrix} .
\]
That is, $M_{1,2}$ is the distance between $(x_1,x_1, x_1)$ and $(x_1, x_1, x_2)$, 
$M_{1,3}$ is the distance between $(x_1, x_1, x_1)$ and $(x_2, x_1, x_1)$ and so on. 
We obtained $M$ by simulating symmetric matrices with zeros on the diagonal and 
positive entries off the diagonal and iteratively updating the entries as long as the 
triangle inequality was violated. More precisely, we started with a symmetric random matrix
$M^0$ with zeros on the diagonal and positive off-diagonal entries. Then we iteratively set
\[
M^k_{i,j} = 
\min\{M^{k-1}_{i, j}, M^{k-1}_{i,1} + M^{k-1}_{1,j}, M^{k-1}_{i,2} + M^{k-1}_{2,j}, \dots, 
M^{k-1}_{i,12} + M^{k-1}_{12,j}\}
\]
until $M^{k} = M^{k-1}$. This guarantees that $M$ defines a metric on 
$(x_1,x_1, x_1), (x_1, x_1, x_2) \dots$ From there it can be extended to a metric 
on $\cX$ by Fr\'echet embedding. Finally, we used Gurobi \cite{gurobi} to compute 
$W_{G, 1}(\mu, \nu) =  0.585$, $W_{G, 1}(\nu, \eta) = 2.24$ and
$W_{G, 1}(\mu, \eta) = 2.925$, showing that 
\[
W_{G, 1}(\mu, \eta) >  W_{G, 1}(\mu, \nu) + W_{G, 1}(\nu, \eta).
\]

\bibliographystyle{plainnat} 
\bibliography{ideabib.bib}

\end{document}