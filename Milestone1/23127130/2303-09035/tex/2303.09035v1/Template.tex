% Template for ICIP-2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}} 
\renewcommand{\algorithmicensure}{ \textbf{Output:}}
%\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{mathrsfs}
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Extracting the Brain-like Representation by an Improved Self-Organizing Map for Image Classification}
%
% Single address.
% ---------------




\name{
    %Authors
    % All authors must be in the same font size and format.
    Jiahong Zhang,\textsuperscript{\rm 1,2}
    Lihong Cao, \textsuperscript{\rm 1,2 } 
    Moning Zhang, \textsuperscript{\rm 1,2}
    Wenlong Fu\textsuperscript{\rm 1,2}
    \thanks{The corresponding author is Lihong Cao (lihong.cao@cuc.edu.cn).}
}
\address{
    %Afiliations
    \textsuperscript{\rm 1} State Key Laboratory of Media Convergence and
Communication, Communication University of China,\\
    \textsuperscript{\rm 2} Neuroscience and Intelligent Media Institute, Communication University of China,\\Beijing,China
%
% See more examples next
}

%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document} \normalsize
%\ninept
%
\maketitle
%
\begin{abstract}
Backpropagation-based supervised learning has achieved great success in computer vision tasks. However, its biological plausibility is always controversial. Recently, the bio-inspired Hebbian learning rule (HLR) has received extensive attention. Self-Organizing Map (SOM) uses the competitive HLR to establish connections between neurons, obtaining visual features in an unsupervised way. Although the representation of SOM neurons shows some brain-like characteristics, it is still quite different from the neuron representation in the human visual cortex. This paper proposes an improved SOM with multi-winner, multi-code, and local receptive field, named mlSOM. We observe that the neuron representation of mlSOM is similar to the human visual cortex. Furthermore, mlSOM shows a sparse distributed representation of objects, which has also been found in the human inferior temporal area. In addition, experiments show that mlSOM achieves better classification accuracy than the original SOM and other state-of-the-art HLR-based methods. The code is accessible at https://github.com/JiaHongZ/mlSOM.
\end{abstract}
%
\begin{keywords}
Self Organizing Maps, Unsupervised Learning, Image classification
\end{keywords}
%

\section{Introduction}
Backpropagation-based supervised learning has been extensively studied in recent years. For image classification, the adoption of backpropagation enables convolutional neural networks (CNNs) to extract features effectively \cite{AlexNet,VGG,ResNet}, thus improving classification performance. However, the biological plausibility of the backpropagation is always controversial \cite{2000Computational}.

\begin{figure}[t]
	\centering	
    \centering\includegraphics[width=7cm,height=7cm]{map.jpg}

	\caption{Visualization of the neuron representations:  (a) HebbNet, (b) Backpropagation, (c) SOM for handwritten digits. (d) TE for objects (Image Source: \cite{ITPart}). Colored bars in the top (red), middle (green) and bottom graphs in (d) are penetration sites inside the active spots of the stimulus.}
	\label{fig1}
\end{figure}

\begin{figure*}[!t]
	\centerline{\includegraphics[width=15cm]{architecture1.jpg}}
	\caption{ The architecture of mlSOM. For a given input image, mlSOM first divide it into patches through a sliding window. Then for each patch, its WNs in the hidden layer are calculated in the SOM phase. In the Coding phase, the neuron states of the hidden layer will be coded to a feature map, in which $1$ denotes the WN. The feature map is then send to the linear classifier to obtain the classification result.
	}
	\label{fig2}
\end{figure*}

The Hebbian learning rule (HLR) is a biologically plausible unsupervised learning mechanism and has been proposed for a long time \cite{allport1985distributed,klopf1972brain}, which suggests that: "Neurons that fire together wire together." In a broad sense, the HLR refers to a family of methods based on the idea of Hebbian. Unfortunately, vanilla HLR does not guarantee high performance for image classification. Recently, several methods have been proposed to improve the classification accuracy for HLR  \cite{hopfieldhebb,HebbNet,amato2019hebbian}. Self-Organizing Map (SOM) uses the Winner-Takes-All competition HLR  to establish connections between neurons \cite{rumelhart1985feature, 1992Neural}, which achieves high classification performance. However, they failed to obtain brain-like neuron representation. In human visual cortex, the representation of an object presents a topological structure \cite{ITPart}. For example, Fig.~\ref{fig1} (d) shows the representation of a complex object and its parts in the anterior part of the IT cortex (architectonically defined as area TE). By comparison, it can be found that the neuron representations of existing HLR methods (Fig.~\ref{fig1} (b) and (c)) and the backpropagation method (Fig.~\ref{fig1} (a)) lack the object parts. 


This paper proposes an improved SOM, mlSOM. Compared with the original SOM, three modifications are made in mlSOM: from global receptive field (GRF) to local receptive field (LRF), from single-winner to multi-winner, and from single-code to multi-code. 
The main contributions of this work contain at least three key advantages. Firstly, we propose to improve the representation of SOM from a brain-like perspective, which may contribute to a promising future research direction for SOM. Secondly, we improve the original SOM in three ways inspired by the human brain and demonstrate their effectiveness for classification. Thirdly, the proposed mlSOM shows brain-like representation and gets high classification performance compared with other state-of-the-art Hebbian learning-based methods.

\begin{table}[t]\footnotesize 
\renewcommand\arraystretch{1}
	\caption{The hyper-parameters of mlSOM.}
	\begin{center}
		\begin{tabular}{c|c|c|c|c|c|c|c}
			\hline
			\textbf{Datasets} & \multicolumn{4}{c}\textbf{Hyper-parameters}\\ 	\hline	
			
			& \textit{hidden neurons}
			& \textit{w}
			& \textit{s}
			& \textit{$n$}
			& \textit{$\sigma$} & \textbf{$k$} & \textbf{$lr$}\\
			\hline
			\multirow{1}{*}{MNIST}& $44\times44$ & $14\times14$ & 7 & 5 & 2 & 20 & 0.3\\
			\hline
			\multirow{1}{*}{CIFAR-10} 
			& $44\times44$ & $16\times16$ & 4 & 5 & 2 & 100 & 0.3\\
			\hline
		\end{tabular}
		\label{tab1}
	\end{center}
\end{table}


\section{Related Work}
Self-Organizing Map (SOM) is a kind of HLR-based neural network. For image classification, current studies for SOM mainly focus on improving classification accuracy. Supervised SOM was proposed in \cite{supervisedSOM3} and got good classification results. Some work presented that deep SOM would get higher classification performance than the single-layer SOM \cite{dsom1, dsom2}. Combining SOM and CNN to obtain both accuracy and biological plausibility has also attracted widespread interest \cite{somcnn1,somcnn2,somcnn3,somcnn4}. This paper presents a new idea to improve SOM from the neuron representation perspective.


\begin{algorithm}[!h] \small
    \caption{Learning algorithm for mlSOM}
    \begin{algorithmic}[1]
    \REQUIRE ~~\\ %算法的输入参数：Input
    Training set of images and labels ($\mathscr{X}, Y$) \\
    \ENSURE 
    Trained mlSOM 
    \STATE initialize the model parameter $W$ with the standard normal distribution and hyper-parameters in Table \ref{tab1}\\
	{lr, epochs $\leftarrow$ initialize the learning rate, 
	epochs} \\
    
    \STATE \% SOM Phase (unsupervised)
    \FOR{epo $\in$ epochs}
       \FOR{$x \in \mathscr{X}$}
        \STATE $ x_p \leftarrow Sliding(x)$ \% image patches obtained by the sliding window;
        \FOR{$x_{p_i} \in x_p$}
        \STATE \% distance of $x_{p_i}$ and neurons in mlSOM
        \FOR {$W_j \in W$}
        \STATE $d_{ij} \leftarrow  \Vert x_{p_i} - W_j \Vert$
        \ENDFOR
        \STATE sort $d_{ij}$ in ascending order;
        \STATE WNs $\leftarrow$ the top $n$ $W$;
        
        \STATE $[\left(X_{WN},Y_{WN} \right)] \leftarrow$ the coordinate of the first $n$ WNs;
        
        \STATE \% update learning rate
        \STATE $lr_{epo} \leftarrow lr \times \left( 1- \frac{epo}{epochs}\right)$;

        \FOR {$WN_i \in WNs$}
        \FOR {$W_j \in W$}
        \STATE \% compute updating neuron vector decay value;
        \STATE $decay = e^{\frac{{\Vert \left( X_{WN_i},Y_{WN_i} \right) - \left( X_{W_j},Y_{W_j} \right) \Vert}_2 }{2\sigma^2}}$ ;
		\STATE $\Delta W = lr_{epo} \times decay \times ( W_i - WN$);
		\STATE $W_i \leftarrow W_i + \Delta W$ ;
        \ENDFOR
        \ENDFOR
        \ENDFOR
        \ENDFOR
        \ENDFOR    
        \STATE \% Coding phase (supervised)
        \FOR{$epo \in epochs$}
            \FOR{$x \in \mathscr{X}$}     
            \STATE $ x_p \leftarrow Sliding(x)$;
            \FOR{$x_{p_i} \in x_p$}
            \STATE $G_p \leftarrow$ binary 2D grid with $k$ WNs of the hidden layer for $x_p$;
            \STATE $G_{sum} = Binary(\sum{G_p})$;
    		\STATE feature map $\leftarrow$ $G_{sum}$;
    		\STATE \% prediction of the classifier
    		\STATE $pre = f(feature map)$;
    		\STATE minimize $L(pre, Y)$;
            \ENDFOR
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
    \label{al2}
\end{algorithm}


\section{Proposed method}
The proposed mlSOM is based on the unsupervised SOM algorithm mentioned in \cite{dsom1}. We first revisit it.
\subsection{The original SOM}
SOM is a classic unsupervised learning algorithm using the "winner-take-all" learning rule, which gets a non-linear projection of high-dimensional data over a small space. Each neuron in SOM consists of a trainable vector. SOM computes Euclidean distances between the input pattern and each neuron. The neuron that has the least distance is the winner neuron (WN). WN and its neighbors will be updated to be closer to the input pattern during training. The update value decreases as the distance between neurons and WN increases.

\subsection{mlSOM}
Fig.~\ref{fig2} shows the architecture of mlSOM whose hyper-parameters are illustrated in Table~\ref{tab1}. mlSOM is based on SOM, and the three modifications in mlSOM are as follows.

%\textbf{1) From GRF to LRF.} SOM use the whole input image to compute the distance with hidden layer neurans which leads to a large weight vector. We propose to use LRF inspired by human eye movement. This function can be realized by sliding window. As shown in Figure~\ref{fig2}, the input image is $28\times28$ and the window size is $14\times14$ with stride $(7,7)$. The weight vector in mlSOM is $14\times14$ which is four times smaller than that of original SOM $28\times28$. Furthermore, 
\textbf{1) From GRF to LRF.} SOM computes the distance between the whole input image and hidden layer neurons, leading to huge neuron vectors. Inspired by the human eye movement, we propose to use LRF, which can be realized by a sliding window. As shown in Fig.~\ref{fig2}, the size of the input image is $(H,W)$ and the window size is set to $(w, w)$ with stride $(s,s)$. The size of the neuron vector in mlSOM is $w^2$, which is $(\frac{w^2}{H \times W})$ of the original SOM.

%Furthermore, Section \ref{population} shows that LRF makes mlSOM have more brain-like popularity activity.

\textbf{2) From single-winner to multi-winner.} The original SOM uses the "winner-take-all" learning rule. It computes Euclidean distances between the input image and neurons in the hidden layer and chooses one WN. However, population coding widely exists in the human visual cortex \cite{golledge2003correlations}. It motivates us to change the single-winner to multi-winner. As shown in Fig.~\ref{fig2} SOM Phase, for every image patch, the hidden layer of mlSOM obtains one 2D grid with $n$ winners. For every winner, the vector-updating algorithm is similar to the original SOM. 

\textbf{3) From single-code to multi-code.} The original SOM and some deep SOMs use the 2D grid with a single WN as the classification feature map. Also inspired by the population coding, mlSOM generates the feature map with multiple WNs, as shown in Fig.~\ref{fig2} Coding Phase.
Specifically, mlSOM uses neurons with the first $k$ WNs to achieve the multi-code. Here, $k$ can be different from the multi-winner $n$. In Fig.~\ref{fig2} Coding phase, the 2D grids of input image patches are transformed to the corresponding binary matrixs, in which $1$ denotes the WN. These matrices are summed together and binarized as the feature map of the input image.

mlSOM is an unsupervised learning algorithm. We trained a linear classifier to classify images using their mlSOM feature maps.
%  The classifier consists of two layers, which have $w^2$ and $c$ neurons, respectively. Here, $c$ is equal to the number of the category. The classifier will output the classification result given the feature map in the Coding Phase. 

\begin{figure*}[!t]
	\centering	
	\subfigure[representations]{
		\centering\includegraphics[width=6.5cm]{mnist.png}
	}
	\subfigure[coding]{
		\centering\includegraphics[width=9cm]{pa.png}
	}
% 	\subfigure[CIFAR-10]{
% 		\centering\includegraphics[width=8cm]{cifar.png}
% 	}
	\caption{(a) Neuron representations learnt by the mlSOM after training on MNIST. (b) Visualization of the feature maps and neuron representations of the specific input images. The top row shows the input images and their feature maps. Yellow dots in feature maps represent the WNs. The bottom row shows the neuron vectors of the WNs.}
	\label{pa}
	\vspace{-15pt}
\end{figure*}

\subsection{Training method}
The training method of mlSOM is shown in Algorithm \ref{al2}. 
This training process can be divided into two phases. In the SOM phase, an input image is firstly divided into patches by the sliding window. Then, Euclidean distances of the image patches and the hidden layer neurons are computed. The top $n$ neurons with minimum distance will be selected as WNs. For each neuron of the WNs, the algorithm updating the neuron vectors in mlSOM is similar to the original SOM. 

When the SOM phase is finished, we get a trained hidden layer. During the coding phase, for each input image patch, a corresponding 2D grid is obtained from the hidden layer of mlSOM. We convert this 2D grid into a binary coding matrix, in which $1$ denotes the WN. Here, the number of $1$ in the coding matrix is set to $k$. Then, we obtain the sum of all the patch grids and binarize it to get the feature map representing a specific object. 
To verify the classification ability of the feature map, we train a linear classifier with the help of error backpropagation. The training object is minimizing the cross-entropy loss:
\begin{equation}
	L(x) = \sum_{c=1}^{N}ylog(f(x)),
\end{equation}
where x denotes the input image, $f(\cdot)$ denotes the classifier, and y denotes its label.



\section{Experiments}
% \subsection{Dataset}
%  MNIST is a hand-written characters dataset (digits from 0-9) including 60000 train images and 10000 test images with $28\times28$. Images in MNIST are gray-scale. In addition, CIFAR-10 is used to test the performance of mlSON on color-scale images. CIFAR-10 contains RGB color images of 10 categories: aircraft, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The size of each image is $32\times32$. There are 6000 images in each category, 5000 for training and 1000 for testing.

\subsection{Experiment results} \label{results}
%This section we compare mlSOM with some popular Hebbian-learning-based methods with single hidden layers. In Table\ref{mnist}, mlSOM has significant high performance than the vanilla Hebbian rule and originial SOM. The there modifies, LRF, multi-winner and multi-code all contribute this results, according to the ablation experiments in Table \ref{mnist}.
We use two datasets MNIST\cite{mnist}, CIFAR-10\cite{cifar10} to evaluate the proposed mlSOM. This section compares mlSOM to some popular Hebbian learning-based methods. We choose the methods with a single hidden layer for a fair comparison. As shown in Table \ref{mnist}, mlSOM performs significantly better classification results than that of HebbNet and SOM, achieving 96.79\% test accuracy on MNIST. According to the ablation experiments results in Table \ref{mnist}, the three modifications, LRF, multi-winner, and multi-code, effectively contribute to better performance. Table \ref{cifar} shows the results on CIFAR-10. It can be observed that mlSOM can achieve competitive results with the state-of-the-art methods. Furthermore, mlSOM trades off the classification accuracy and the convergence speed.

\begin{table} \small 
	\renewcommand\arraystretch{1} 
	\caption{Classification accuracy results and ablation experiments results of mlSOM on the MNIST dataset.}
	\begin{center}  
	\begin{tabular}{ll} % l is this column to aligh left  
		\hline                      
		\textbf{Method} & Test Accuracy  \\  
		\hline  
		Vanilla Hebbian  &  10.28\\ 
		HebbNet\cite{HebbNet}  &  93.25\\ 
		SOM  & 93.07\\
		DSOM \cite{dsom1} & 96.17 \\
		\hline 
		SOM+mult-winner  & 95.40\\
		SOM+mult-winner+LRF  & 96.72\\
		\textbf{mlSOM}  & \textbf{96.79}\\      
		\hline  
	\end{tabular}  
\end{center}
\vspace{-25pt}
\label{mnist}
\end{table}



% Figure~\ref{pa} 展示了mlSOm在MNIST和CIFAR-10训练集上的隐藏层权重。对于MNIST，我们观察到mlSOM的neuron representations包含物体和物体的patch，这与人类视觉皮层的激活是相似的。对于CIFAR-10，可以清晰地看到mlSOM的神经元存在颜色偏好，形状偏好，这些特点也与人脑中视觉皮层的发现一致。因此提出的mlSOM方法表现出了很高的类脑特性。
% The neuron representation of mlSOM on the training set of MNIST is demonstrated in Figure~\ref{pa} (a). It can be seen that mlSOM's representation contains objects and their parts, which is similar to TE (see Fig.~\ref{fig1}). These results demonstrate that the proposed mlSOM can exhibit high brain-like properties.

% \begin{table}[t]\footnotesize 
% 	\caption{The impacts of hyper-parameter on classification accuracy of mlSOM on MNIST.}
% 	\begin{center}
% 		\begin{tabular}{c|c|c|c}
% 			\hline
% 			\textbf{Fixed parameters} & \multicolumn{2}{l}\textbf{Changed parameter}\\
% 			\hline
% 			\multirow{2}{*}{Fixed $\sigma=2$, $k=20$} 
% 			 & $n=1$ & $n=5$ & $n=10$ \\
% 			 & 72.53 & 96.79 & 96.74 \\
% 			\hline
% 			\multirow{2}{*}{Fixed $n=5$, $k=20$} 
% 			& $\sigma=1$ & $\sigma=2$ & $\sigma=4$ \\
% 			& 95.92 & 96.79 & 96.48 \\
% 			\hline
% 			\multirow{2}{*}{Fixed $n=5$, $\sigma=2$} 
% 			& $k=5$ & $k=10$ & $k=20$ \\
% 			& 95.86 & 96.35 & 96.79 \\
% 			\hline
% 		\end{tabular}
% 		\label{hy}
% 	\end{center}
% \end{table}

% \begin{figure*}[!t]
% 	\centerline{\includegraphics[width=18cm]{pa.png}}
% 	\caption{Visualization of the feature maps and neuron representations of the specific input images. The top row shows the input images and their feature maps. Yellow dots in feature maps represent the WNs. The bottom row shows the weight vectors of the WNs. It can be found that these WNs consist of the representations of the object and corresponding parts.
% 	}
% 	\label{pa2}
% 	\vspace{-0.5em}
% \end{figure*}

% \section{与其他Hebbian方法的比较}
%提出的mlSOM在图像分类任务上，获得了与其他基于Hebbian规则方法有竞争力的结果。现在Hebbian规则主要改进的点在于为其增加限制。我们认为mlSOM具备这些限制条件。比如mlSOM使用竞争学习规则，这使特征具备像HebbNet\cite{HebbNet}中提出的稀疏性。mlSOM中按距离衰减的权重更新规则，实现了侧抑制功能in bio learning in \cite{hopfieldhebb}. 这些特点使mlSOM获得了好的特征提取能力。 
\subsection{Neuron representations of mlSOM}
According to Fig.~\ref{fig1} and Fig.~\ref{pa}, the neuron representation obtained by mlSOM exhibits brain-like characteristics which are similar to the neuron activity detected in the human TE \cite{ITPart}. 
Specifically, we found that neurons in mlSOM respond to the whole and part of the object. Furthermore, mlSOM presents a brain-like distributed coding method. Fig.~\ref{pa} (b) shows the feature map and neuron representations in mlSOM for digits two and eight. Taking the digit two as an example, its representations in mlSOM consist of the arcs, slashes, object features, and corresponding parts. These representations are observed in the human visual cortex \cite{kandel2000principles}. It is worth noting that neurons representing slashes are also detected in digit eight. This evidence suggests that neurons in mlSOM can respond to feature combinations of different objects, contributing to a larger encoding capacity. It is very similar to the sparse distributed representation in human IT \cite{rolls1995sparseness,rolls1997information,franco2007neuronal}.

\begin{table}  \small 
	\renewcommand\arraystretch{1} 
	\caption{Classification accuracy results on the CIFAR-10 dataset.}
	\begin{center}  
		\begin{tabular}{llll} % l is this column to aligh left  
			\hline                      
			\textbf{Method} & Train Acc & Test Acc & Epochs \\  
			\hline  
			Vanilla Hebbian  &   11.56 &  15.23 &  200\\ 
			BackProp  &  39.89 &  41.28 & 200\\ 
			Krotov et al. \cite{hopfieldhebb} & \textbf{55.05} & \textbf{50.75} & 1500\\
			Amato et al. \cite{amato2019hebbian} & - & 41.78 & \textbf{20}\\
			HebbNet \cite{HebbNet} & 43.13 & 45.69 & 200\\
			\textbf{mlSOM}  & 51.82 & 43.65 & 200 \\      
			\hline  
		\end{tabular}  
	\end{center}
	\vspace{-20pt}
	\label{cifar}
\end{table}

\section{Conclusion}
\vspace{-5pt}
This paper proposes an improved SOM method, mlSOM, which achieves better classification accuracy than other Hebbian learning-based methods on MNIST and competitive accuracy on CIFAR-10. mlSOM makes improvements based on the brain-inspired designs, including LRF, multi-winner, and multi-code. Ablation experiments show the effectiveness of these three modifications. The most significant contribution of mlSOM is that it exhibits brain-like neuronal representations and coding. Our research may inspire the design of visual cortex computing model and provide a novel direction for SOM research.
\vspace{-10pt}
\section{Acknowledgment}
\vspace{-5pt}
This paper is supported by supported by the STI 2030—Major Projects (grant No. 2021ZD0200300) and the National Natural Science Foundation of China (grant No. 62176241).

\label{sec:refs} \small

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}
