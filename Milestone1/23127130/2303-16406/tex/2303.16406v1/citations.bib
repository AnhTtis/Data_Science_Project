@inproceedings{miech19howto100m,
   title={How{T}o100{M}: {L}earning a {T}ext-{V}ideo {E}mbedding by {W}atching {H}undred {M}illion {N}arrated {V}ideo {C}lips},
   author={Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
   booktitle={ICCV},
   year={2019},
}

@inproceedings{lin2021SwinBERTend-to-end,
title={SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning},
author={Lin, Kevin and Li, Linjie and Lin, Chung-Ching and Ahmed, Faisal and Gan, Zhe and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
booktitle = {CVPR},
year = {2022},
}

@article{wang2022git,
  title={GIT: A Generative Image-to-text Transformer for Vision and Language},
  author={Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  journal={arXiv preprint arXiv:2205.14100},
  year={2022}
}

@Article{Luo2020UniVL,
  author  = {Huaishao Luo and Lei Ji and Botian Shi and Haoyang Huang and Nan Duan and Tianrui Li and Jason Li and Taroon Bharti and Ming Zhou},
  title   = {UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation},
  journal = {arXiv preprint arXiv:2002.06353},
  year    = {2020},
}

@article{Aafaq2019SpatioTemporalDA,
  title={Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning},
  author={Nayyer Aafaq and Naveed Akhtar and W. Liu and Syed Zulqarnain Gilani and Ajmal S. Mian},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={12479-12488}
}

@article{Pei2019MemoryAttendedRN,
  title={Memory-Attended Recurrent Network for Video Captioning},
  author={Wenjie Pei and Jiyuan Zhang and Xiangrong Wang and Lei Ke and Xiaoyong Shen and Yu-Wing Tai},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={8339-8348}
}

@article{Pan2020SpatioTemporalGF,
  title={Spatio-Temporal Graph for Video Captioning With Knowledge Distillation},
  author={Boxiao Pan and Haoye Cai and De-An Huang and Kuan-Hui Lee and Adrien Gaidon and Ehsan Adeli and Juan Carlos Niebles},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={10867-10876}
}

@article{Zhou2018EndtoEndDV,
  title={End-to-End Dense Video Captioning with Masked Transformer},
  author={Luowei Zhou and Yingbo Zhou and Jason J. Corso and Richard Socher and Caiming Xiong},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={8739-8748}
}

@article{Wang2021EndtoEndDV,
  title={End-to-End Dense Video Captioning with Parallel Decoding},
  author={Teng Wang and Ruimao Zhang and Zhichao Lu and Feng Zheng and Ran Cheng and Ping Luo},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={6827-6837}
}

@article{Zhu2022EndtoendDV,
  title={End-to-end Dense Video Captioning as Sequence Generation},
  author={Wanrong Zhu and Bo Pang and Ashish V. Thapliyal and William Yang Wang and Radu Soricut},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.08121}
}

@inproceedings{BMT_Iashin_2020,
  title={A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer},
  author={Iashin, Vladimir and Rahtu, Esa},
  booktitle={British Machine Vision Conference (BMVC)},
  year={2020}
}

@inproceedings{Zhou2018TowardsALYouCook2,
  title={Towards Automatic Learning of Procedures From Web Instructional Videos},
  author={Luowei Zhou and Chenliang Xu and Jason J. Corso},
  booktitle={AAAI},
  year={2018}
}

@inproceedings{Tang2019COIN,
    title={COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis},
    author={Yansong Tang and Dajun Ding and Yongming Rao and Yu Zheng and Danyang Zhang and Lili Zhao and Jiwen Lu and Jie Zhou},
    booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2019}
}

@inproceedings{Zhukov2019CrossTask,  author={Zhukov, Dimitri and Alayrac, Jean-Baptiste and Cinbis, Ramazan Gokberk and Fouhey, David and Laptev, Ivan and Sivic, Josef},  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Cross-Task Weakly Supervised Learning From Instructional Videos},   year={2019},  volume={},  number={},  pages={3532-3540},  doi={10.1109/CVPR.2019.00365}}

@inproceedings{lei2020tvr,
  title={TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval},
  author={Lei, Jie and Yu, Licheng and Berg, Tamara L and Bansal, Mohit},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{yaya2022EmScore,
author = {Yaya, Shi and Yang, Xu and Xu, Haiyang and Yuan, Chunfeng and Li, Bing and Hu, Weiming and Zha, Zheng-Jun},
year = {2022},
month = {06},
pages = {17908-17917},
title = {EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching},
doi = {10.1109/CVPR52688.2022.01740}
}

@inproceedings{Parikh2016ADAEntailment,
  title={A Decomposable Attention Model for Natural Language Inference},
  author={Ankur P. Parikh and Oscar T{\"a}ckstr{\"o}m and Dipanjan Das and Jakob Uszkoreit},
  booktitle={EMNLP},
  year={2016}
}

@ARTICLE{Wang2004SSIM,  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},  journal={IEEE Transactions on Image Processing},   title={Image quality assessment: from error visibility to structural similarity},   year={2004},  volume={13},  number={4},  pages={600-612},  doi={10.1109/TIP.2003.819861}}


@article{Radford2021CLIP,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.00020}
}

@inproceedings{wang-etal-2019-youmakeup,
    title = "{Y}ou{M}akeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension",
    author = "Wang, Weiying  and
      Wang, Yongcheng  and
      Chen, Shizhe  and
      Jin, Qin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1517",
    doi = "10.18653/v1/D19-1517",
    pages = "5133--5143",
}

@INPROCEEDINGS{Rohrbach2012MP2,  
author={Rohrbach, Marcus and Amin, Sikandar and Andriluka, Mykhaylo and Schiele, Bernt},  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},   title={A database for fine grained activity detection of cooking activities},   year={2012},  volume={},  number={},  pages={1194-1201},  doi={10.1109/CVPR.2012.6247801}}

@article{Kuehne2014TheLOBreakfast,
  title={The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities},
  author={Hilde Kuehne and Ali Bilgin Arslan and Thomas Serre},
  journal={2014 IEEE Conference on Computer Vision and Pattern Recognition},
  year={2014},
  pages={780-787}
}

@misc{Radford2019LanguageMAGPT2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@article{mokady2021clipcap,
  title={ClipCap: CLIP Prefix for Image Captioning},
  author={Mokady, Ron and Hertz, Amir and Bermano, Amit H},
  journal={arXiv preprint arXiv:2111.09734},
  year={2021}
}

@inproceedings{Lei2021QVHighlightsDM,
  title={QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries},
  author={Jie Lei and Tamara L. Berg and Mohit Bansal},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{Sharghi2017QueryFocusedVS,
  title={Query-Focused Video Summarization: Dataset, Evaluation, and a Memory Network Based Approach},
  author={Aidean Sharghi and Jacob S. Laurel and Boqing Gong},
  booktitle={CVPR},
  year={2017},
  pages={2127-2136}
}

@inproceedings{Narasimhan2021CLIPItLV,
  title={CLIP-It! Language-Guided Video Summarization},
  author={Medhini Narasimhan and Anna Rohrbach and Trevor Darrell},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{Song2015TVSumSW,
  title={TVSum: Summarizing web videos using titles},
  author={Yale Song and Jordi Vallmitjana and Amanda Stent and Alejandro Jaimes},
  booktitle={CVPR},
  year={2015},
  pages={5179-5187}
}

@inproceedings{Miech2020EndtoEndLO,
  title={End-to-End Learning of Visual Representations From Uncurated Instructional Videos},
  author={Antoine Miech and Jean-Baptiste Alayrac and Lucas Smaira and Ivan Laptev and Josef Sivic and Andrew Zisserman},
  booktitle={CVPR},
  year={2020},
  pages={9876-9886}
}

@inproceedings{Narasimhan2022TLDWSI,
  title={TL;DW? Summarizing Instructional Videos with Task Relevance \& Cross-Modal Saliency},
  author={Medhini Narasimhan and Arsha Nagrani and Chen Sun and Michael Rubinstein and Trevor Darrell and Anna Rohrbach and Cordelia Schmid},
  booktitle={ECCV},
  year={2022},
  volume={abs/2208.06773}
}

@inproceedings{Hendricks2017LocalizingMI,
  title={Localizing Moments in Video with Natural Language},
  author={Lisa Anne Hendricks and Oliver Wang and Eli Shechtman and Josef Sivic and Trevor Darrell and Bryan C. Russell},
  booktitle={ICCV},
  year={2017},
  pages={5804-5813}
}

@inproceedings{Gygli2014CreatingSF,
  title={Creating Summaries from User Videos},
  author={Michael Gygli and Helmut Grabner and Hayko Riemenschneider and Luc Van Gool},
  booktitle={ECCV},
  year={2014}
}

@inproceedings{Bain21FrozenInTime,
  author       = "Max Bain and Arsha Nagrani and G{\"u}l Varol and Andrew Zisserman",
  title        = "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
  booktitle    = "IEEE International Conference on Computer Vision",
  year         = "2021",
}

@article{Lee2021ViSeRetAS,
  title={ViSeRet: A simple yet effective approach to moment retrieval via fine-grained video segmentation},
  author={Aiden Seung Joon Lee and Hanseok Oh and Minjoon Seo},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.05146}
}

@inproceedings{Papineni2002BleuAM,
  title={Bleu: a Method for Automatic Evaluation of Machine Translation},
  author={Kishore Papineni and Salim Roukos and Todd Ward and Wei-Jing Zhu},
  booktitle={ACL},
  year={2002}
}

@article{Vedantam2015CIDErCI,
  title={CIDEr: Consensus-based image description evaluation},
  author={Ramakrishna Vedantam and C. Lawrence Zitnick and Devi Parikh},
  journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={4566-4575}
}

@inproceedings{Banerjee2005METEORAA,
  title={METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments},
  author={Satanjeev Banerjee and Alon Lavie},
  booktitle={IEEvaluation@ACL},
  year={2005}
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@inproceedings{Anderson2016SPICESP,
  title={SPICE: Semantic Propositional Image Caption Evaluation},
  author={Peter Anderson and Basura Fernando and Mark Johnson and Stephen Gould},
  booktitle={ECCV},
  year={2016}
}

@article{RaffelT5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{Cho2021,
abstract = {Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https://github.com/j-min/VL-T5},
archivePrefix = {arXiv},
arxivId = {2102.02779},
author = {Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
booktitle = {ICML},
eprint = {2102.02779},
file = {:Users/jaeminc/Paper{\_}Download/2102.02779.pdf:pdf},
mendeley-groups = {DALLE-Analysis},
month = {feb},
title = {{Unifying Vision-and-Language Tasks via Text Generation}},
url = {http://arxiv.org/abs/2102.02779},
year = {2021}
}

@inproceedings{SungVLAdapter2022,
abstract = {Recently, fine-tuning language models pre-trained on large text corpora have provided huge improvements on vision-and-language (V{\&}L) tasks as well as on pure language tasks. However, fine-tuning the entire parameter set of pre-trained models becomes impractical since the model size is growing rapidly. Hence, in this paper, we introduce adapter-based parameter-efficient transfer learning techniques to V{\&}L models such as VL-BART and VLT5. We evaluate our methods in a unified multi-task setup on both image-text and video-text benchmarks. For the image-text tasks, we use four diverse V{\&}L datasets: VQAv2, GQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA, How2QA, TVC, and YC2C. With careful training and thorough experiments, we benchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter) against the standard full fine-tuning and the recently proposed prompt-tuning approach. We also enhance the efficiency and performance of adapters by sharing their weights to attain knowledge across tasks. Our results demonstrate that training the adapter with the weight-sharing technique (4.18{\%} of total parameters for image-text tasks and 3.39{\%} for video-text tasks) can match the performance of fine-tuning the entire model. Lastly, we present a comprehensive analysis including the combination of adapter and task-specific prompts and the impact of V{\&}L pre-training on adapters. Our code is available at: https://github.com/ylsung/VL{\_}adapter.},
archivePrefix = {arXiv},
arxivId = {2112.06825},
author = {Sung, Yi-Lin and Cho, Jaemin and Bansal, Mohit},
booktitle = {CVPR},
doi = {10.1109/cvpr52688.2022.00516},
eprint = {2112.06825},
file = {:Users/jaeminc/Paper{\_}Download/VL-Adapter{\_} Parameter-Efficient Transfer Learning for Vision-and-Language Tasks, Yi-Lin Sung et al., 2021.pdf:pdf},
title = {{VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks}},
year = {2022}
}

@inproceedings{Xu2016MSRVTT,
author = {Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
booktitle = {CVPR},
file = {:Users/jaeminc/Paper{\_}Download/cvpr16.msr-vtt.tmei{\_}-1.pdf:pdf},
title = {{MSR-VTT: A Large Video Description Dataset for Bridging Video and Language}},
url = {http://research.microsoft.com/apps/pubs/default.aspx?id=264836},
year = {2016}
}

@inproceedings{Yu2017,
abstract = {We propose a high-level concept word detector that can be integrated with any video-to-language models. It takes a video as input and generates a list of concept words as useful semantic priors for language generation models. The proposed word detector has two important properties. First, it does not require any external knowledge sources for training. Second, the proposed word detector is trainable in an end-to-end manner jointly with any video-to-language models. To effectively exploit the detected words, we also develop a semantic attention mechanism that selectively focuses on the detected concept words and fuse them with the word encoding and decoding in the language model. In order to demonstrate that the proposed approach indeed improves the performance of multiple video-to-language tasks, we participate in all the four tasks of LSMDC 2016 [18]. Our approach has won three of them, including fill-in-theblank, multiple-choice test, and movie retrieval.},
archivePrefix = {arXiv},
arxivId = {1610.02947},
author = {Yu, Youngjae and Ko, Hyungjin and Choi, Jongwook and Kim, Gunhee},
booktitle = {CVPR},
doi = {10.1109/CVPR.2017.347},
eprint = {1610.02947},
file = {:Users/jaeminc/Paper{\_}Download/End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering, Youngjae Yu et al., 2016.pdf:pdf},
isbn = {9781538604571},
pages = {3261--3269},
title = {{End-to-end concept word detection for video captioning, retrieval, and question answering}},
volume = {2017-Janua},
year = {2017}
}

@inproceedings{Li2020Hero,
archivePrefix = {arXiv},
arxivId = {arXiv:2005.00200v2},
author = {Li, Linjie and Chen, Yen-Chun and {Yu Cheng}, Zhe Gan and Yu, Licheng and Liu, Jingjing},
booktitle = {EMNLP},
eprint = {arXiv:2005.00200v2},
file = {:Users/jaeminc/Paper{\_}Download/2005.00200.pdf:pdf},
mendeley-groups = {VL-T5-ICML2021},
title = {{HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training}},
year = {2020}
}

@inproceedings{Xu2015ShowAttendAndTell,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03044v3},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
booktitle = {ICML},
eprint = {arXiv:1502.03044v3},
file = {:Users/jaeminc/Library/Application Support/Mendeley Desktop/Downloaded/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:pdf},
mendeley-groups = {ML/Vision/Image / Video Captioning,CLIP-Captioning},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
year = {2015}
}

@inproceedings{Vinyals2015bShowAndTell,
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4555v2},
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
booktitle = {CVPR},
eprint = {arXiv:1411.4555v2},
file = {:Users/jaeminc/Library/Application Support/Mendeley Desktop/Downloaded/Vinyals et al. - 2015 - Show and Tell A Neural Image Caption Generator.pdf:pdf},
mendeley-groups = {ML/Vision/Image / Video Captioning,CLIP-Captioning},
title = {{Show and Tell: A Neural Image Caption Generator}},
year = {2015}
}


@inproceedings{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
booktitle = {NAACL},
eprint = {1802.05365},
file = {:Users/jaeminc/Library/Application Support/Mendeley Desktop/Downloaded/Peters et al. - 2018 - Deep contextualized word representations.pdf:pdf},
mendeley-groups = {Focus-EMNLP2019,VL-T5-ICML2021},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}


@article{Carreira2017QuoVAI3D,
  title={Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset},
  author={Jo{\~a}o Carreira and Andrew Zisserman},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={4724-4733}
}

@article{Hershey2017CNNAFVGGish,
  title={CNN architectures for large-scale audio classification},
  author={Shawn Hershey and Sourish Chaudhuri and Daniel P. W. Ellis and Jort F. Gemmeke and Aren Jansen and R. Channing Moore and Manoj Plakal and Devin Platt and Rif A. Saurous and Bryan Seybold and Malcolm Slaney and Ron J. Weiss and Kevin W. Wilson},
  journal={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2017},
  pages={131-135}
}

@inproceedings{Bowman2015,
abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.},
archivePrefix = {arXiv},
arxivId = {1508.05326},
author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
booktitle = {EMNLP},
doi = {10.18653/v1/d15-1075},
eprint = {1508.05326},
file = {:Users/jaeminc/Paper{\_}Download/A large annotated corpus for learning natural language inference, Samuel R. Bowman et al., 2015.pdf:pdf},
isbn = {9781941643327},
title = {{A large annotated corpus for learning natural language inference}},
year = {2015}
}

@inproceedings{Li2021VALUE,
abstract = {Most existing video-and-language (VidL) research focuses on a single dataset, or multiple datasets of a single task. In reality, a truly useful VidL system is expected to be easily generalizable to diverse tasks, domains, and datasets. To facilitate the evaluation of such systems, we introduce Video-And-Language Understanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets over 3 popular tasks: (i) text-to-video retrieval; (ii) video question answering; and (iii) video captioning. VALUE benchmark aims to cover a broad range of video genres, video lengths, data volumes, and task difficulty levels. Rather than focusing on single-channel videos with visual information only, VALUE promotes models that leverage information from both video frames and their associated subtitles, as well as models that share knowledge across multiple tasks. We evaluate various baseline methods with and without large-scale VidL pre-training, and systematically investigate the impact of video input channels, fusion methods, and different video representations. We also study the transferability between tasks, and conduct multi-task learning under different settings. The significant gap between our best model and human performance calls for future study for advanced VidL models. VALUE is available at https://value-benchmark.github.io/.},
archivePrefix = {arXiv},
arxivId = {2106.04632},
author = {Li, Linjie and Lei, Jie and Gan, Zhe and Yu, Licheng and Chen, Yen-Chun and Pillai, Rohit and Cheng, Yu and Zhou, Luowei and Wang, Xin Eric and Wang, William Yang and Berg, Tamara Lee and Bansal, Mohit and Liu, Jingjing and Wang, Lijuan and Liu, Zicheng},
booktitle = {NeurIPS},
eprint = {2106.04632},
file = {:Users/jaeminc/Paper{\_}Download/2106.04632.pdf:pdf},
pages = {1--21},
title = {{VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation}},
url = {http://arxiv.org/abs/2106.04632},
year = {2021}
}


@inproceedings{Krishna2017,
abstract = {Most natural videos contain numerous events. For example, in a video of a 'man playing a piano', the video might also contain 'another man dancing' or 'a crowd clapping'. We introduce the task of dense-captioning events, which involves both detecting and describing events in a video. We propose a new model that is able to identify all events in a single pass of the video while simultaneously describing the detected events with natural language. Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes. To capture the dependencies between the events in a video, our model introduces a new captioning module that uses contextual information from past and future events to jointly describe all events. We also introduce ActivityNet Captions, a large-scale benchmark for dense-captioning events. ActivityNet Captions contains 20k videos amounting to 849 video hours with 100k total descriptions, each with its unique start and end time. Finally, we report performances of our model for dense-captioning events, video retrieval and localization.},
archivePrefix = {arXiv},
arxivId = {1705.00754},
author = {Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Niebles, Juan Carlos},
booktitle = {ICCV},
doi = {10.1109/ICCV.2017.83},
eprint = {1705.00754},
file = {:Users/jaeminc/Paper{\_}Download/Dense-Captioning Events in Videos, Ranjay Krishna et al., 2017.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
title = {{Dense-Captioning Events in Videos}},
year = {2017}
}


@misc{Radford2022whisper,
  doi = {10.48550/ARXIV.2212.04356},
  
  url = {https://arxiv.org/abs/2212.04356},
  
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  
  keywords = {Audio and Speech Processing (eess.AS), Computation and Language (cs.CL), Machine Learning (cs.LG), Sound (cs.SD), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{miech19endtoend,
   title={End-to-End Learning of Visual Representations from Uncurated Instructional Videos},
   author={Miech, Antoine and Alayrac, Jean-Baptiste and Smaira, Lucas and Laptev, Ivan and Sivic, Josef and Zisserman, Andrew},
   booktitle={CVPR},
   year={2020},
}

@INPROCEEDINGS{Perazzi16DAVIS,
  author={Perazzi, F. and Pont-Tuset, J. and McWilliams, B. and Van Gool, L. and Gross, M. and Sorkine-Hornung, A.},
  booktitle={CVPR}, 
  title={A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation}, 
  year={2016},
  volume={},
  number={},
  doi={10.1109/CVPR.2016.85}}


@INPROCEEDINGS{Jiang2019SVD,
  author={Jiang, Qing-Yuan and He, Yi and Li, Gen and Lin, Jian and Li, Lei and Li, Wu-Jun},
  booktitle={ICCV}, 
  title={SVD: A Large-Scale Short Video Dataset for Near-Duplicate Video Retrieval}, 
  year={2019},
  volume={},
  number={},
  doi={10.1109/ICCV.2019.00538}}


@INPROCEEDINGS{Revaud2013CVPR,
  author={Revaud, Jérôme and Douze, Matthijs and Schmid, Cordelia and Jégou, Hervé},
  booktitle={CVPR}, 
  title={Event Retrieval in Large Video Collections with Circulant Temporal Encoding}, 
  year={2013},
  volume={},
  number={},
  doi={10.1109/CVPR.2013.318}}


@misc{Wei2022COT,
  doi = {10.48550/ARXIV.2201.11903},
  
  url = {https://arxiv.org/abs/2201.11903},
  
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{Zhang2023multimodalCOT,
  doi = {10.48550/ARXIV.2302.00923},
  
  url = {https://arxiv.org/abs/2302.00923},
  
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multimodal Chain-of-Thought Reasoning in Language Models},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}

@inproceedings{Devlin2019,
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
booktitle = {NAACL},
eprint = {1810.04805},
file = {:Users/jmin/Library/Application Support/Mendeley Desktop/Downloaded/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
mendeley-groups = {X-LXMERT-EMNLP2020,Focus-EMNLP2019,VL-T5-ICML2021},
month = {oct},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2019}
}

@inproceedings{Seo2017,
abstract = {Machine Comprehension (MC), answering a query about a given context, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to summarize the context (or query) into a single vector, couple attentions temporally, and often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail Cloze Test.},
archivePrefix = {arXiv},
arxivId = {1611.01603},
author = {Seo, Minjoon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hananneh},
booktitle = {ICLR},
eprint = {1611.01603},
file = {:Users/jmin/Library/Application Support/Mendeley Desktop/Downloaded/Seo et al. - 2017 - Bi-Directional Attention Flow for Machine Comprehension.pdf:pdf},
keywords = {SQuAD,attentional,question answering},
mendeley-groups = {ML/NLP/MC / QA / Dialogue / Chatbot},
pages = {1--12},
title = {{Bi-Directional Attention Flow for Machine Comprehension}},
year = {2017}
}



@inproceedings{Vaswani2017,
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
booktitle = {NIPS},
eprint = {1706.03762},
file = {:Users/jmin/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention Is All You Need.pdf:pdf},
mendeley-groups = {X-LXMERT-EMNLP2020,Focus-EMNLP2019,DALLE-Analysis,CLIP-Captioning,ML/Architecture/Attention / Pointer,VL-T5-ICML2021},
title = {{Attention Is All You Need}},
url = {https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
year = {2017}
}

@inproceedings{Sharma2018ConceptualCap,
abstract = {We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.},
author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
booktitle = {ACL},
file = {:Users/jmin/Library/Application Support/Mendeley Desktop/Downloaded/Sharma et al. - 2018 - Conceptual captions A cleaned, hypernymed, image alt-text dataset for automatic image captioning.pdf:pdf},
isbn = {9781948087322},
mendeley-groups = {DALLE-Analysis,CLIP-Captioning,VL-T5-ICML2021},
title = {{Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning}},
url = {https://www.aclweb.org/anthology/P18-1238/},
year = {2018}
}


@inproceedings{Fang2023EVA,
  title={EVA: Exploring the Limits of Masked Visual Representation Learning at Scale},
  author={Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  booktitle = {CVPR},
  journal={arXiv preprint arXiv:2211.07636},
  year={2023}
}

@inproceedings{Tang2021CLIP4Caption,
abstract = {Video captioning is a challenging task since it requires generating sentences describing various diverse and complex videos. Existing video captioning models lack adequate visual representation due to the neglect of the existence of gaps between videos and texts. To bridge this gap, in this paper, we propose a CLIP4Caption framework that improves video captioning based on a CLIP-enhanced video-text matching network (VTM). This framework is taking full advantage of the information from both vision and language and enforcing the model to learn strongly text-correlated video features for text generation. Besides, unlike most existing models using LSTM or GRU as the sentence decoder, we adopt a Transformer structured decoder network to effectively learn the long-range visual and language dependency. Additionally, we introduce a novel ensemble strategy for captioning tasks. Experimental results demonstrate the effectiveness of our method on two datasets: 1) on MSR-VTT dataset, our method achieved a new state-of-the-art result with a significant gain of up to 10% in CIDEr; 2) on the private test data, our method ranking 2nd place in the ACM MM multimedia grand challenge 2021: Pre-training for Video Understanding Challenge. It is noted that our model is only trained on the MSR-VTT dataset.},
archivePrefix = {arXiv},
arxivId = {2110.06615},
author = {Tang, Mingkang and Wang, Zhanyu and Liu, Zhenhua and Rao, Fengyun and Li, DIan and Li, Xiu},
booktitle = {ACM MM},
doi = {10.1145/3474085.3479207},
eprint = {2110.06615},
file = {:Users/jmin/Paper_download/3474085.3479207.pdf:pdf},
isbn = {9781450386517},
keywords = {pre-train,transformer,video caption,video-text matching},
title = {{CLIP4Caption: CLIP for Video Caption}},
year = {2021}
}



@inproceedings{Zhang2019BertScore,
archivePrefix = {arXiv},
arxivId = {arXiv:1904.09675v2},
author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
booktitle = {ICLR},
eprint = {arXiv:1904.09675v2},
file = {:Users/jmin/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2019 - BERTScore Evaluating Text Generation with BERT.pdf:pdf},
mendeley-groups = {CLIP-Captioning},
title = {{BERTScore: Evaluating Text Generation with BERT}},
year = {2019}
}


@inproceedings{Hessel2021,
abstract = {Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in stark contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker vs reference-based metrics, e.g., news captions that require richer contextual knowledge.},
archivePrefix = {arXiv},
arxivId = {2104.08718},
author = {Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Bras, Ronan Le and Choi, Yejin},
booktitle = {EMNLP},
eprint = {2104.08718},
file = {:Users/jmin/Library/Application Support/Mendeley Desktop/Downloaded/Hessel et al. - 2021 - CLIPScore A Reference-free Evaluation Metric for Image Captioning.pdf:pdf},
mendeley-groups = {DALLE-Analysis,CLIP-Captioning},
title = {{CLIPScore: A Reference-free Evaluation Metric for Image Captioning}},
url = {http://arxiv.org/abs/2104.08718},
year = {2021}
}


@misc{Liu2019c,
abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
archivePrefix = {arXiv},
arxivId = {1907.11692},
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
eprint = {1907.11692},
title = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
url = {http://arxiv.org/abs/1907.11692},
year = {2019}
}
