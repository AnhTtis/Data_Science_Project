% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, \eg for an arXiv version

% Include other packages here, before hyperref.
% \usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{multirow,tabularx}
\usepackage{makecell}

\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, \eg with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%% MACROS
\newcommand{\dataname}{\textsc{HiREST}}


\newcommand{\videoretrievaltask}{video retrieval}
\newcommand{\momentretrievaltask}{moment retrieval}
\newcommand{\stepretrievalsubtask}{moment segmentation}
\newcommand{\momentcaptioningtask}{step captioning}

\newcommand{\Videoretrievaltask}{Video retrieval}
\newcommand{\Momentretrievaltask}{Moment retrieval}
\newcommand{\Stepretrievalsubtask}{Moment segmentation}
\newcommand{\Momentcaptioningtask}{Step captioning}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{12156} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{
Hierarchical Video-Moment Retrieval and Step-Captioning
}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\author{
  Abhay Zala\thanks{equal contribution}\ \ $^1$ \qquad
  Jaemin Cho\samethanks{}\ \ $^1$ \qquad
  Satwik Kottur$^2$ \qquad
  Xilun Chen$^2$  \\
  Barlas Oguz$^2$ \qquad 
  Yashar Mehdad$^2$ \qquad
  Mohit Bansal$^1$ \\
  UNC Chapel Hill$^1$ \quad \quad Meta AI$^2$\\
  {\tt\small \{jmincho, aszala, mbansal\}@cs.unc.edu} \quad \quad 
  {\tt\small \{skottur, xilun, barlaso, mehdad\}@fb.com} 
  \\  
  {\tt \normalsize \href{https://hirest-cvpr2023.github.io}{https://hirest-cvpr2023.github.io}}
}
\maketitle

\begin{abstract}
There is growing interest in searching for information from large video corpora.
Prior works have studied relevant tasks, such as text-based video retrieval, moment retrieval, video summarization, and video captioning in isolation,
without an end-to-end setup that can jointly search from video corpora and generate summaries.
Such an end-to-end setup would allow for many interesting applications,
\eg, a text-based search that finds a relevant video from a video corpus, extracts the most relevant moment from that video, and segments the moment into important steps with captions.
To address this,
we present the \dataname{}
(\textbf{HI}erarchical \textbf{RE}trieval and \textbf{ST}ep-captioning) dataset
and propose a new benchmark that covers hierarchical information retrieval and visual/textual stepwise summarization from an instructional video corpus.
\dataname{} consists of 3.4K
text-video pairs from an instructional video dataset, 
where 1.1K videos have annotations of moment spans relevant to text query and breakdown of each moment into key instruction steps with caption and timestamps (totaling 8.6K step captions).
Our hierarchical benchmark consists of
video retrieval, moment retrieval, and two novel
moment segmentation and step captioning tasks.
In moment segmentation, models break down a video moment
into instruction steps and identify start-end boundaries.
In step captioning, models generate a textual summary
for each step.
We also present starting point task-specific and end-to-end joint baseline models for our new benchmark.
While the baseline models show some promising results, there still exists large room for future improvement by the community.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\input{fig_main_figure.tex}

Encouraged by the easy access to smartphones, recording software, and video hosting platforms, people are increasingly accumulating videos of all kinds.
To fuel the subsequent growing interest in using machine learning systems to extract and summarize important information 
from these large video corpora based on text queries,
progress has been made in
video retrieval~\cite{Xu2016MSRVTT,Yu2017,lei2020tvr,Li2020Hero,Bain21FrozenInTime},
moment retrieval~\cite{Hendricks2017LocalizingMI,lei2020tvr,Lei2021QVHighlightsDM},
video summarization~\cite{Gygli2014CreatingSF,Song2015TVSumSW,Sharghi2017QueryFocusedVS,Narasimhan2022TLDWSI}, and
video captioning~\cite{Xu2016MSRVTT,Yu2017,BMT_Iashin_2020,lin2021SwinBERTend-to-end}.
Previous works have generally focused on solving these tasks independently; however, all these tasks share the common goal of retrieving information from a video corpus, at different levels of scales and via different modalities.
Hence, in this work, we introduce a new hierarchical benchmark that combines all four tasks to enable novel and useful real-world applications.
For example,
a text-based search service that finds a relevant video from a large video corpus, extracts the most relevant moment from that video, segments the moment into important steps, and captions them for easy indexing and retrieval.
To support this, we introduce \dataname{}, a hierarchical instructional video dataset for a holistic benchmark of information retrieval from a video corpus 
(see \cref{sec:dataset}).
\dataname{} consists of four annotations:
1) $3.4K$ pairs of text query about open-domain instructions (\eg, \textit{`how to make glow in the dark slime'}) and videos,
2) relevant moment timestamps inside the $1.1K$ videos, where only a part of the video ($<75\%$) is relevant to the text query,
3) moment breakdown in several instructional steps with timestamps ($7.6$ steps per video, total $8.6K$ steps),
and,
4) an manually curated English caption for each step (\eg \textit{`pour shampoo in container'}).
We collect fine-grained step-wise annotations of \dataname{} in a two-step annotation process with online crowdworkers on instructional text-video pairs from the HowTo100M~\cite{miech19howto100m} dataset (see \cref{sec:dataset_collection}).
The instructional videos often come with clear step-by-step instructions, allowing fine-grained segmentation of the videos into short steps.
While there are existing video datasets with step annotations, they are based on a small number of predefined task names~\cite{Zhukov2019CrossTask,Tang2019COIN} (thus step captions are not diverse), or are limited to a single topic (\eg cooking~\cite{Zhou2018TowardsALYouCook2}).
\dataname{} covers various domains and provides diverse step captions with timestamps written by human annotators (see \Cref{tbl:datasetcompare}), presenting new challenging and realistic benchmarks for hierarchical video information retrieval.

Using the \dataname{} dataset, we benchmark four tasks:
1) \videoretrievaltask{},
2) \momentretrievaltask{},
3) \stepretrievalsubtask{},
and
4) \momentcaptioningtask{} (see \cref{fig:mainfigure} and \cref{sec:tasks}).
In the \videoretrievaltask{} task, models have to identify a video that is most relevant to a given text query.
In the \momentretrievaltask{} task, models have to select the relevant span of the video, by trimming the parts irrelevant to the text query (blue boundary in \cref{fig:mainfigure}).
In the \stepretrievalsubtask{} task, models have to break down the relevant portion into several instructional steps and identify the start-end boundaries of each step
(green boundaries in \cref{fig:mainfigure}).
Finally, in the \momentcaptioningtask{} task, models have to generate step captions (\eg \textit{`spray the warm water on carpet'}) of the instructional steps.
To provide good starting points to the community for our new task hierarchy, we show the performance of recent baseline models on \dataname{}. For baselines, we use strong models including
CLIP~\cite{Radford2021CLIP},
EVA-CLIP~\cite{Fang2023EVA},
Frozen-in-Time\cite{Bain21FrozenInTime},
BMT~\cite{BMT_Iashin_2020}, and SwinBERT~\cite{lin2021SwinBERTend-to-end}.
On all four tasks, we find that finetuning models on \dataname{} improve performance; however, there exists a large room to improve performance.
\par
We summarize our contributions in this paper:
1) We present \dataname{} dataset and propose a new benchmark that covers hierarchy in information retrieval and visual/textual summarization from an instructional video corpus.
2) Unlike existing video datasets with step captions based on predefined task names or limited to a single topic, our \dataname{} provides diverse, high-quality step captions with timestamps written by human annotators.
3) We provide a joint baseline model that can perform \momentretrievaltask{},
\stepretrievalsubtask{},
and
\momentcaptioningtask{} with a single architecture.
4) We provide comprehensive dataset analyses and show experiments with baseline models for each task, where there is a large room to improve model performance.
We hope that \dataname{} can foster future work on end-to-end systems for holistic information retrieval and summarization on large video corpus.
In addition,
our manually annotated step captions can also be a good source for training and testing the step-by-step reasoning of large multimodal language models~\cite{Wei2022COT,Zhang2023multimodalCOT}.





\section{Related Work}
\label{sec:related_work}

\subsection{Text-based Information Retrieval from Video}
With growing interest in building machine learning systems to search for useful information from large video corpora via text searches, several lines of work have been proposed.
In text-to-video retrieval, a system finds the most relevant videos from a list of videos with a given text query~\cite{Xu2016MSRVTT,Yu2017,lei2020tvr,Li2020Hero,Bain21FrozenInTime}.
In moment retrieval, a system finds the most relevant moments (usually a few seconds of frame spans), from a single video~\cite{Hendricks2017LocalizingMI,lei2020tvr,Lei2021QVHighlightsDM}.
In query-focused video summarization, which is a text-conditional version of generic video summarization~\cite{Gygli2014CreatingSF,Song2015TVSumSW}, a system finds the most relevant frames from a video with text query~\cite{Sharghi2017QueryFocusedVS,Narasimhan2022TLDWSI}.
In video captioning, a system generates a short textual description of a given video~\cite{Xu2016MSRVTT,Yu2017,BMT_Iashin_2020,lin2021SwinBERTend-to-end}.
While all these tasks share common goals, information retrieval and summarization from a video corpus, previous works have focused on systems that are specialized in a single task.
In this work, we introduce a holistic setup that combines video retrieval, moment retrieval, query-focused video summarization (called moment segmentation), and generating a stepwise textual summary of short clip (called step captioning), so that users can search for the most relevant video, the most relevant moment inside the video, and get the stepwise text summarization of the moment.


\input{tab_dataset_comparisons.tex}




\subsection{Instructional Video Datasets}
Recently, there have also been several efforts towards creating instructional video datasets~\cite{Zhou2018TowardsALYouCook2,wang-etal-2019-youmakeup,Rohrbach2012MP2,Kuehne2014TheLOBreakfast,miech19howto100m,Tang2019COIN,Zhukov2019CrossTask}. While many of these datasets do a good job of providing high-quality instructional videos, they primarily only target a single domain~\cite{Zhou2018TowardsALYouCook2,wang-etal-2019-youmakeup,Rohrbach2012MP2,Kuehne2014TheLOBreakfast}. There have been recent strong efforts towards developing more diverse instructional datasets~\cite{miech19howto100m,Tang2019COIN,Zhukov2019CrossTask}. Datasets like HowTo100M~\cite{miech19howto100m} provide diverse instructional videos but lack specific step-by-step annotations. 
Some previous works such as \cite{Tang2019COIN,Zhukov2019CrossTask} provide step-level annotations for open domain videos, however, are restricted to a set of predefined steps that are reapplied across several videos.
Our \dataname{} dataset provides step annotations on diverse instructional videos, where
all step captions are manually written to answer the input text query by human annotators (see \Cref{tbl:datasetcompare}).












\section{\dataname{}: Hierarchical Retrieval and Step-Captioning Dataset}
\label{sec:dataset}

We present \dataname{}, a video dataset consisting of 3.4K text-video pairs, 1.8K moments, and 8.6K step caption annotations. It covers the hierarchy of video/moment retrieval and stepwise captioning from a diverse instructional video corpus.
Previous step annotations in video datasets used predefined task descriptions with small vocabulary~\cite{Zhukov2019CrossTask,Tang2019COIN} or limited to a single domain (\eg cooking~\cite{Zhou2018TowardsALYouCook2}).
In contrast, the step captions of \dataname{} are manually written by human annotators and cover diverse domains with a large vocabulary (see \Cref{tbl:datasetcompare}).
We describe the data collection process (\cref{sec:dataset_collection}),
dataset analysis (\cref{sec:dataset_analysis}),
and four hierarchical tasks that stem from our dataset (\cref{sec:tasks}).



\subsection{Dataset Collection}
\label{sec:dataset_collection}

In the following, we describe the two-stage data collection process.
In the appendix, we provide screenshots of the data collection interface for each stage and worker qualification process.

\vspace{3pt} \par \noindent\textbf{Stage 1: Video and Moment Retrieval.}
We collect the pairs of text queries and relevant videos from the HowTo100M~\cite{miech19howto100m} dataset.
Since videos were originally automatically collected from YouTube, we ensure that all videos are actually relevant to the query through human annotation.
We employ crowdworkers from Amazon Mechanical Turk\footnote{\url{https://www.mturk.com}} and ask them to label whether or not the video correctly answers/solves the associated text query.

If the video is labeled as relevant to the text query, then we collect relevant `moment' annotation from the video, by asking the crowdworkers to trim the video to the parts that are directly associated with the text (\ie remove video parts unrelated to the text query, such as intro or other topics).
We define a video as \textit{clippable} to a moment, if the moment relevant to the query is less than 75\% of the original video length.
A system that can retrieve moments from videos would help people directly watch the video portion they are interested in and save time.
For the retrieved moments, we collect more fine-grained annotations by dividing the moment into steps and captioning each step. We explain the moment annotation below.



\vspace{3pt} \par \noindent\textbf{Stage 2: Moment Segmentation and Step Captions.}
In this stage, we collect fine-grained, stepwise annotations of the retrieved moments.
We ask crowdworkers to watch retrieved moments, divide them into several steps and mark the start timestamp of each step.
Then, for each of the marked moment segments, they are asked to write a \textit{step caption} that describes the specific step to complete (\eg ``add crayons to the candle'', ``melt it in bowl with hot water'', ``stir it well until dry'').
Our text queries from HowTo100M~\cite{miech19howto100m} are instructional questions starting with ``how to'', and we want the step captions to serve as short textual summaries of moments/steps.
we ask crowdworkers to start each caption with an action verb (\eg ``add", ``apply") and limit the length of the captions to seven words.


\input{fig_dataset_cat_distro.tex}



\subsection{Dataset Analysis}
\label{sec:dataset_analysis}

\vspace{3pt} \par \noindent\textbf{Task Category Distribution.}
Our videos and text queries are collected from the HowTo100M~\cite{miech19howto100m} dataset, and hence our category labels match theirs. As shown in \cref{fig:datasetcategorydistribution}, the most frequently occurring categories (for all text-video pairs and just videos with step captions) are ``Hobbies and Crafts", ``Food and Entertaining", and ``Home and Garden". While these are the most common categories (similar to HowTo100M's most common categories), other categories still have a presence in our dataset. 

\vspace{3pt} \par \noindent\textbf{Dataset Statistics.}
We collected a total of 3.4K text-video pairs, which are 287 seconds long on average, with a total duration of 270 hours.
Out of 3.4K videos, 1.8K videos are \textit{clippable} to a moment; \ie, only a short clip ($<$75\% of the original video) is relevant to the text query.
The average moment length is 148 seconds, which is 55\% of the original videos.
Out of the 1.8K moments, we provide moment segmentation and step caption annotations for the randomly chosen 1.1K moments.
The 1.1K moments are broken down to 7.6 steps on average, totaling 8.6K steps.
Each step is annotated with a start-end timestamp and a step caption.
The step captions are on average 4.42 words long and have 633 unique starting verbs with 3382 unique words.
\cref{fig:top10words} shows the most frequent starting verbs and the most frequent words in the step captions (not counting the starting word and stop words).
\cref{fig:step_caption_sunburst} shows the first three words of 50 random step caption samples (ignoring stop words).
As shown in the visualizations, the manually written step captions of \dataname{} cover open domain instruction steps and have a diverse vocabulary.


\vspace{3pt} \par \noindent\textbf{Comparisons to Other Datasets with Step Captions.}
\Cref{tbl:datasetcompare} compares our \dataname{} dataset to other video datasets with step annotations.
\dataname{} covers various open-domain videos with many step annotations per video and high-quality step captions written by human annotators.
While COIN~\cite{Tang2019COIN} and CrossTask \cite{Zhukov2019CrossTask} also provide step-level annotations for open-domain videos, however, they are restricted to a set of predefined steps.
In contrast, all the step captions of \dataname{} are manually written to answer the input text query.

\input{fig_sunburst.tex}

\vspace{3pt} \par \noindent\textbf{Data Splits.}
Since there are cases where multiple videos are retrieved from the same query, we split our dataset into train/val/test splits by query instead of video.
We split our queries into 546/292/546 (1507/477/1391 videos) for train/val/test splits, respectively.


\input{fig_top_word_distributions.tex}



\subsection{Hierarchical Tasks Enabled by \dataname{}}
\label{sec:tasks}

In the following, we introduce four tasks connected in a hierarchy based on our \dataname{} dataset.
See \cref{fig:mainfigure} for an overview and visual examples of the tasks.

\vspace{3pt} \par \noindent\textbf{Video Retrieval.}
This task gives models an instructional text query (\eg ``How to make a memory jar''), and the models need to determine which videos are relevant and retrieve the top results. The models must retrieve videos among 
4.2K test split videos (1.4K videos paired with text queries + 2.8K distractor videos from HowTo100M~\cite{miech19howto100m}).
Distractor videos serve as negative examples (hence `distractors'), similar to Revaud \etal~\cite{Revaud2013CVPR}. We include these distractors to help increase the difficulty of our video retrieval task.


\vspace{3pt} \par \noindent\textbf{Moment Retrieval.}
In this task, the goal is to extract the portion of the video that is directly relevant to the given text query (i.e. to remove any unnecessary information from the start/end of the video).

\vspace{3pt} \par \noindent\textbf{Moment Segmentation.}
In this task, models should identify all relevant key `steps' from the retrieved relevant moment of the video. Models should generate a list of start and end times for every key step in a given video.

\vspace{3pt} \par \noindent\textbf{Step Captioning.}
This task requires models to generate short textual step captions for each retrieved step in a video. Models are provided with the source video and start/end times of each step. They should then generate a short instructional step caption for every step.








\section{Experiments}
\label{sec:Experiments}

For all four \dataname{} tasks,
we conduct experiments with
task-specific baseline models (\cref{sec:models}),
a joint baseline model  (\cref{sec:joint_model}),
and evaluate them with different standard metrics (\cref{sec:metrics}).
We represent each video as 32 frames with uniform intervals, if not specified.


\begin{figure*}[t]
    \centering
    \includegraphics[width=.90\linewidth]{images/joint_model.pdf}
    \caption{Illustration of our joint model  that handles moment retrieval, moment segmentation, and step captioning tasks (\cref{sec:joint_model}).
    We learn a shallow multimodal transformer encoder layer that adapts the four pretrained models:
    EVA-CLIP (frozen),
    Whisper (frozen),
    MiniLM (frozen),
    and CLIP4Caption (finetuned).
    }
    \label{fig:joint_model}
\end{figure*}

\subsection{Task-specific Models}
\label{sec:models}

\vspace{3pt} \par \noindent\textbf{Video Retrieval.}
We experiment with
CLIP (ViT-B/32)~\cite{Radford2021CLIP},
EVA-CLIP (ViT-G/14)~\cite{Fang2023EVA},
Frozen-in-Time~\cite{Bain21FrozenInTime},
and
MIL-NCE (S3D)~\cite{miech19endtoend},
which are pretrained text-to-image (CLIP/EVA-CLIP) and text-to-video (Frozen-in-Time/MIL-NCE) retrieval models, respectively.
For CLIP and EVA-CLIP,
we obtain a video embedding by averaging frame embeddings.
We compute the matching score by taking the cosine similarity between video and text query embedding.
Following the original setup, we use 4 frames for Frozen-in-Time and 32 frames for MIL-NCE.


\vspace{3pt} \par \noindent\textbf{Moment Retrieval.}
We experiment with two CLIP-based heuristics methods and the event proposal module of BMT~\cite{BMT_Iashin_2020}, a dense video captioning model pretrained on ActivityNet Captions~\cite{Krishna2017}.
With CLIP, we compute the cosine similarity between all frames and the text query and find the frame with the highest score.
Then we determine the start/end boundary of a moment with two different heuristics:
1) picking the frames where the similarity score drops from the highest scoring frame by a certain threshold (\eg, 0.10);
2) picking the 8 frames to the left and right, totaling up to 17 (= 8+1+8) frames
(see appendix for details).
Furthermore, we experiment with the BMT~\cite{BMT_Iashin_2020} event proposal module, which predicts video event proposals with center/length/confidence values.
We allow BMT to generate various events and then take the minimum start time and maximum end time across the events as the retrieved moment.
For BMT, we give the model the I3D~\cite{Carreira2017QuoVAI3D} RGB+Flow features and VGGish~\cite{Hershey2017CNNAFVGGish} audio features of the entire video, extracted at 1fps.

\vspace{3pt} \par \noindent\textbf{Moment Segmentation.}
We experiment with 1) framewise difference with the Structural Similarity Index Measure (SSIM)~\cite{Wang2004SSIM}, and
2) the event proposal module of BMT~\cite{BMT_Iashin_2020}.
For SSIM, if two adjacent frames have an SSIM below a certain threshold (\eg, 0.85), we mark that as a step boundary.
For BMT, we feed the model I3D and VGGish features (extracted at 1fps) of the entire video and directly use the video event proposal prediction.

\vspace{3pt} \par \noindent\textbf{Step Captioning.}
We experiment with BMT and SwinBERT~\cite{lin2021SwinBERTend-to-end}, a pretrained video captioning model.
For BMT, we use I3D and VGGish features of each step, extracted at 1fps.
We do not use its event proposal module for this task, as we give the features within the ground-truth step boundaries.
For SwinBERT, we use YouCook2~\cite{Zhou2018TowardsALYouCook2} checkpoint and 32 video frames from each step as input to the model.




\subsection{Joint Model}
\label{sec:joint_model}

We also experiment with an end-to-end joint baseline model that handles moment retrieval, moment segmentation, and step captioning tasks with a single architecture.
As shown in \cref{fig:joint_model}, our model 
is built on four existing pretrained models:
EVA-CLIP~\cite{Fang2023EVA},
Whisper~\cite{Radford2022whisper},
MiniLM~\cite{reimers-2019-sentence-bert},
and
CLIP4Caption~\cite{Tang2021CLIP4Caption}.
EVA-CLIP visual encoder maps a video frame into a visual embedding,
EVA-CLIP text encoder maps a text query into a text embedding,
Whisper extracts speech transcription from audio,
MiniLM text encoder maps the speech transcription into a text embedding.
To adapt the video, text, and audio embeddings,
we finetune a two-layer multimodal encoder and a two-layer text decoder, which are initialized from CLIP4Caption (MSRVTT~\cite{Xu2016MSRVTT} checkpoint).
% The encoder and decoder are shallow two-layer transformer~\cite{Vaswani2017} architectures.
We train the joint model in a multi-task setup in a round-robin fashion, by sampling a batch from one of the data loaders at each step~\cite{Cho2021}.

\vspace{3pt} \par \noindent\textbf{Input Embedding.}
We construct the multimodal input embedding to the transformer by combining
1) EVA-CLIP video frame embedding,
2) EVA-CLIP text query embedding (tiled to the number of video frames),
3) and MiniLM speech transcription embedding (temporally warped into each frame),
and
4) task-specific mask embeddings.
For moment retrieval and moment segmentation tasks, we feed the same multimodal embeddings while masking out the frames that are outside of interest.

\vspace{3pt} \par \noindent\textbf{Moment Retrieval \& Moment Segmentation.}
Following the span-based text question answering models~\cite{Seo2017, Devlin2019},
we learn linear layers that predict the boundaries of moments and steps.
Concretely, we use three linear layers predicting moment start, moment end, and step boundaries.
For the moment retrieval, our joint start and end predictor predicts the moment boundary in parallel, and we do not mask out the video inputs.
For the moment segmentation, our joint model autoregressively predicts each step's boundaries with masking; \ie, we mask out
1) frames that are outside of the moment
and 2) frames that are included in the previous steps.
For both tasks, we feed the video in 1fps.

\vspace{3pt} \par \noindent\textbf{Step Captioning.}
Following CLIP4Caption~\cite{Tang2021CLIP4Caption},
we sample 20 frames from each step.
The autoregressive text decoder attends to the multimodal encoder output via cross-attention and generates each step caption independently.

\input{tab_task1_video_retrieval_results.tex}
\input{tab_task2_moment_retrieval_results.tex}

\subsection{Metrics}
\label{sec:metrics}

\vspace{3pt} \par \noindent\textbf{Video Retrieval.}
Following previous work~\cite{lei2020tvr,Li2020Hero,Yu2017,Bain21FrozenInTime,Li2021VALUE},
We evaluate models on Recall@k metrics: R@1, R@5, and R@10.

\vspace{3pt} \par \noindent\textbf{Moment Retrieval.}
Following previous work~\cite{lei2020tvr,Lei2021QVHighlightsDM},
we evaluate model outputs against the ground-truth (GT) moment spans with Recall@1 with Intersection over Union (IoU) thresholds (0.5 and 0.7).

\vspace{3pt} \par \noindent\textbf{Moment Segmentation.}
Following previous work~\cite{lei2020tvr,Lei2021QVHighlightsDM},
we evaluate models on how similar the generated step spans are to the GT spans using IoU.
We then compute the recall and precision with IoU thresholds (0.5 and 0.7).

\vspace{3pt} \par \noindent\textbf{Step Captioning.}
Following previous work~\cite{Xu2016MSRVTT,lin2021SwinBERTend-to-end,BMT_Iashin_2020,Li2021VALUE},
we evaluate with the N-gram metrics:
CIDEr~\cite{Vedantam2015CIDErCI}, METEOR~\cite{Banerjee2005METEORAA}, and SPICE~\cite{Anderson2016SPICESP} with the language\_evaluation package.\footnote{\url{https://github.com/bckim92/language-evaluation}}
We also report two sentence-level embedding-based metrics  BERTScore~\cite{Zhang2019BertScore} and CLIPScore~\cite{Hessel2021}.

For BERTScore, we use the RoBERTa-Large~\cite{Liu2019c}.
For CLIPScore, we CLIP ViT-B/32~\cite{Radford2021CLIP} and report the average of frame-caption cosine similarities using 4 frames uniformly sampled from each step.
In addition, we compute the entailment of generated sentences to the GT sentences using the ELMo~\cite{Peters2018}-based Decomposable Attention model~\cite{Parikh2016ADAEntailment} pretrained on SNLI~\cite{Bowman2015} with 3 labels: \{\texttt{entailment}, \texttt{contradict}, \texttt{neutral}\}.\footnote{\url{https://docs.allennlp.org/models/main/models/pair_classification/models/decomposable_attention/}}
We use the ratio of \texttt{entailment} prediction as the entailment score.






\input{tab_task3_moment_segmentation_results.tex}
\input{tab_task4_step_captioning_results.tex}

\input{fig_gt_model_example.tex}

\section{Results and Discussions}
\label{sec:results}

In the following, we present the experiment results on the four tasks and the visualization of the pipelined model predictions.
Our baseline models show promising initial results, but there exists some gap between the current model performance and the upper bound accuracies, leaving large room for future improvements.
% In our examples, models finetuned on \dataname{} 



\vspace{3pt} \par \noindent\textbf{Video Retrieval.}
\Cref{tab:video_retrieval} shows the video retrieval results.
Increasing input frames increases the recall until 20 frames.
Although CLIP was not trained on a video dataset,
CLIP outperforms Frozen-in-Time (4 frames)
shows comparable performance with MIL-NCE (32 frames).
This is likely due to the fact that CLIP was trained on a much larger dataset than Frozen-in-Time.
Finetuning CLIP on \dataname{} does not show a big difference.
EVA-CLIP, a larger CLIP architecture with 1B parameters, outperforms all the other models with a big margin.
Thus, we use EVA-CLIP as our video retrieval model and use its features for the three downstream tasks for our joint model.


\vspace{3pt} \par \noindent\textbf{Moment Retrieval.}
\Cref{tab:moment_retrieval} shows the results for moment retrieval. 
Among the cosine similarity-based zero-shot methods, 
the 8-frame left/right method outperforms the similarity score drop difference method for both CLIP and EVA-CLIP.
BMT achieves better R@0.5 than the zero-shot methods,
and the finetuning improves both recall metrics.
Our joint model outperforms finetuned BMT on the R@0.5, while finetuned BMT achieves a higher score on R@0.7.

\vspace{3pt} \par \noindent\textbf{Moment Segmentation.}
Table~\ref{tab:moment_segmentation} shows the results for the moment segmentation task.
In the zero-shot setting, BMT fails to adapt the span distribution of \dataname{}, and simple SSIM methods could outperform the BMT model on both recall and precision.
But after finetuning, BMT shows significant improvement over its zero-shot version and SSIM methods 
on recall metrics.
Our joint model achieves a better performance than BMT on both recall and precision.

\vspace{3pt} \par \noindent\textbf{Step Captioning.}
Table~\ref{tab:step_captioning} shows the results of the step captioning task.
For both BMT and SwinBERT, zero-shot inference did not result in a good result in N-gram (\eg, CIDEr) and entailment metrics,
indicating the domain gap between their pretraining datasets (ActivityNet caption and YouCook2) and \dataname{} is not negligible. For example, their captions are longer than step captions of \dataname{}.
Finetuning brings a performance boost to BMT and SwinBERT in N-gram and entailment metrics but not in sentence-level embedding-based metrics (BERTScore and CLIPScore).
Compared to SwinBERT, our joint model achieves similar CIDEr and sentence-level embedding-based metrics. Notably, our joint model outperforms SwinBERT significantly on the entailment metric.
Future work on our dataset can also hopefully explore the complementary strengths of SwinBERT and our joint model.


\input{tab_audio_ablation.tex}

\vspace{3pt} \par \noindent\textbf{Audio Ablation.}
\Cref{tab:audio_ablation} shows the ablation study about using (top rows) and not using (bottom rows) audio input with BMT and our joint model.
Overall, both models show a performance drop without audio input.
For \momentretrievaltask{} and \stepretrievalsubtask{}, removing audio input significantly drops the scores for both models,
indicating that audio is very helpful for the tasks that require models to detect the boundaries of events.
For the \momentcaptioningtask{} task, removing audio input significantly drops the score for our joint model, while BMT does not show a big difference.


\vspace{3pt} \par \noindent\textbf{Visualization of Hierarchical Model Pipelining.}
In \cref{fig:modelvsgt_generationexample},
we visualize the model prediction results and ground-truth annotation for moment retrieval, moment segmentation, and step captioning tasks on a video associated with a query `How to make butter biscuits'.
The retrieved moment matches with the video moment about making the batter (36-159s) with the ground truth (GT) annotations.
The predicted step boundaries and step captions also show semantic correspondence with GT annotations and the video.
For example, the predicted caption `mix it'  matches the GT captions `add the mixture' (84-87s) and `mix it' (106-117s).
The model also captions `take one cup sugar' during that part where ingredients are added (47-55s).
The model makes mistakes by missing the end of the dough cutting and the final cooking process (160-213s) during moment retrieval. In this period, we find that a human instructor stands up and describes the process, making the frames visually very different from the previous batter-making process.


\section{Conclusion}

In this work, we present the \dataname{} dataset and propose a new benchmark that covers hierarchy in information retrieval and summarization from an instructional video corpus.
Our benchmark consists of four tasks: \videoretrievaltask{}, \momentretrievaltask{}, and our new \stepretrievalsubtask{} and \momentcaptioningtask{} tasks.
Different from existing video datasets with step captions, our \dataname{} provides unique, diverse, high-quality instruction steps with timestamps written by human annotators.
We provide comprehensive dataset analysis and present experiments with several task-specific and end-to-end joint baseline models for each task as starting points.
We hope that \dataname{} can foster future work on multimodal systems for holistic video information retrieval, summarization, and step-by-step reasoning.


\section*{Acknowledgments}
We thank the reviewers for their helpful comments. This work was supported by Meta AI, ARO Award W911NF2110220, DARPA KAIROS Grant FA8750-19-2-1004, and NSF-AI Engage Institute DRL-211263. The views, opinions, and/or findings contained in this article are those of the authors and not of the funding agency. For this work, the collection of data and the subsequent experiments were performed by the University of North Carolina at Chapel Hill, and not by Meta AI. As a result, both the data and code will be released by UNC Chapel Hill. 


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{citations}
}

% \newpage

\input{appendix}

\end{document}

