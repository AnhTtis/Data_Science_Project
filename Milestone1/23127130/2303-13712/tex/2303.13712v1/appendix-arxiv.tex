
\section{General Identification Result}\label{app:identification}

\begin{theorem}\label{thm:identify_gen}
    Consider known nonlinear and differentiable functions $h_i:\R^{1+L}\to [0,1]$.
    Suppose that for $i\in\{1,\dots, B\}$ observations are generated according to
    \[y_i = h_i(\beta^\top z_i, \theta)\]
    for known variables $z_i$ and unknown parameters $\beta\in\mathcal B$ and $\theta\in\mathcal T$, where $\mathcal B$ and $\mathcal T$ are simply connected and compact subsets of $\mathbb{R}^D$ and $\mathbb{R}^L$ respectively.

    % Suppose that $h_i$ is proper on the restricted domain
    Denote by $h_i'$ the partial derivative of $h_i$ with respect to the first argument and $\nabla_\theta h_i$ the gradient with respect to the latter $L$ arguments.
    Define the matrices 
    \begin{align}
        \label{eq:identification-mx-appendix}
        Z = \begin{bmatrix}
         z_1^\top \\ \vdots \\   z_N^\top \end{bmatrix},
        \quad
        B_{\beta,\theta} = \begin{bmatrix}
        h_1'(\beta^\top z_1, \theta) && \\ &\ddots &\\  &&h_N'(\beta^\top z_N, \theta) \end{bmatrix},
        \quad
        \Theta_{\beta,\theta} = \begin{bmatrix}
            \nabla_\theta h_1(\beta^\top z_1, \theta)\\
            \vdots \\ 
            \nabla_\theta h_N( \beta^\top z_N, \theta)
        \end{bmatrix}
    \end{align}
    Then the parameters $\beta$ and $\theta$ can be uniquely identified from a dataset of $\{z_i, y_i\}_{i=1}^N$ if the following conditions hold for all $\beta\in\mathcal B$ and $\theta\in\mathcal T$:
    \begin{itemize}
        % \item TODO: condition on $g,\mathcal B,\mathcal G$ for proper $F_N$; maybe condition for simply connected M2
        \item Rank condition: $B_{\beta,\theta}Z$ and $\Theta_{\beta,\theta}$ are full rank, i.e. $\rank(B_{\beta,\theta}Z) = D$ and $\rank(\Theta_{\beta,\theta}) = L$ 
        \item Independence condition: the column spaces of $B_{\beta,\theta}X$ and $\Theta_{\beta,\theta}$ are perpendicular, i.e. $\colspace(B_{\beta,\theta}X) \perp \colspace(\Theta_{\beta,\theta})$.
    \end{itemize}
\end{theorem}

\begin{lemma}\label{lem:localinv}
Define the observation map $F_N:\mathcal B\times \mathcal T \to \mathbb R^N$ as
\[F_N(\beta, \theta) = \begin{bmatrix}
h_1( \beta^\top z_1, \theta)\\
\vdots \\ 
h_N( \beta^\top z_N, \theta)
\end{bmatrix}\]
The Jacobian of $F_N$ is invertible at $\beta,\theta$ if and only if the rank and independence conditions hold.
\end{lemma}
\begin{proof}
Denote by $J$ the Jacobian of $F_N$. Then
\[J = \begin{bmatrix}\nabla h_1(\beta^\top z_1,\theta) \\ \vdots \\\nabla g_N(\beta^\top z_N,\theta)  \end{bmatrix}= \begin{bmatrix}B_{\beta,\theta}X & \Theta_{\beta,\theta}\end{bmatrix}\:.\]
% By the rank-nullity theorem, 
The Jacobian $J$ is invertible if and only if the nullspace of $J$ contains only zero.

We first argue that the rank and independence conditions are sufficient. Suppose that $Jv=0$ for some $v$.
Letting $v=[v_1,v_2]$, this is equivalent to $BXv_1 + \Gamma v_2=0$.
Notice these terms are elements of $\colspace(BX)$ and $\colspace(\Gamma)$ respectively. 
By the independence condition, it must be that $BXv_1=0$ and $\Gamma v_2=0$.
By the rank condition and the rank-nullity theorem, it must be that $v_1=0$ and $v_2=0$.
Thus the rank and independence conditions imply that $J$ is invertible.

We now show that the rank and independence conditions are necessary.
If the independence condition does not hold, there is some nonzero $u$ 
% contained in both $\colspace(BX)$ and $\colspace(\Gamma)$, which implies that we can write 
such that $u=BXv_1=\Gamma v_2$.
Then $v=[v_1,-v_2]\neq 0$ is in the nullspace of $J$ so $J$ is not invertible.
If either $DX$ or $\Gamma$ is not full rank, then a nonzero element of their nullspace can be used to construct a nonzero element of the nullspace of then $J$.
This concludes the proof.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{thm:identify_gen}]
Define $\mathcal F\subseteq \mathbb R^N$ as the image of the map $F_N$
defined in Lemma~\ref{lem:localinv}.
With some abuse of notation, we will now consider the function $F_N:\mathcal B \times \mathcal G\to \mathcal F$.
Identifiability of the parameters $\beta$ and $\gamma$ is equivalent to global invertibility of the function $F_N$.
We will use a Theorem due to Hadamard~\cite[Theorem 6.2.8]{krantz2002implicit} 
% [TODO perhaps type out].
% This theorem applies because $\mathcal B\times \mathcal G$ is a smooth and connected manifold by assumption. Since $F$ is continuous, the image is also smooth and connected and has the same dimension.
which states that
$F_N$ is globally invertible if it is proper, if the Jacobian never vanishes, and if $\mathcal F$ is simply connected.

% F proper if X is compact and Y is Hausdorf
$F_N$ is proper because each $h_i$ is proper and $Z$ is full rank.
% , so sequences ${\beta_i,\gamma_i}$ limiting to the boundary of $\mathcal B\times\mathcal G$ also limit to the boundary of $\mathcal F$.
Since $\mathcal F$ is the image of a simply connected space under a continuous mapping, it is also simply connected.
Finally, by Lemma~\ref{lem:localinv}, the Jacobian of $F_N$ is everywhere invertible under the rank and independence conditions.\end{proof}



% \section{Proof for Lemma~\ref{lemma1}}\label{app:pflemma1}
% \begin{proof}
%     Let $u < v$ be two actions induced in equilibrium. Since the algorithm anticipates what actions will be induced with its recommendations, there must exists $m_1, m_2 \in [\delta,1-\delta]$ such that 
%     \begin{align*}
%         U^A(u,m_1) > U_A(v,m_1) \\
%         U^A(v,m_2) > U_A(u,m_2)
%     \end{align*}
%     By continuity of $U^A(u,\cdot)-U^A(v,\cdot)$, there exists an $\overline{m}$ between $m_1$ and $m_2$ such that $ U^A(u,\overline{m}) = U_A(v,\overline{m})$. Since $U^A_{11}(\cdot)<0$, $U^A$ has a unique maximum in $y$ for any given $m$. Therefore, 
%     \begin{align}\label{eq:lemma1}
%         u < y^A(\overline{m}) < v 
%     \end{align}
%     Also, $U^A_{12}(\cdot)>0$, so the algorithm prefers a higher action when the true state of the world is higher. This implies 
%     \begin{align*}
%         \text{$u$ is not induced by any $m > \overline{m}$} \\
%         \text{$v$ is not induced by any $m < \overline{m}$}
%     \end{align*}
%     The above statements and our assumption that $U^J_{12}(\cdot)>0$ imply that 
%     \begin{align}\label{eq:lemma2}
%         u \leq y^J(\overline{m},b) \leq v 
%     \end{align}
%     However, if $y^A(m) \neq y^J(m,b)$ for all $m\in [\delta,1-\delta]$, there exists an $\epsilon>0$ such that $|y^A(m)-y^J(m,b)|\geq \epsilon$ for all $m\in [\delta,1-\delta]$. It follows from \eqref{eq:lemma1} and \eqref{eq:lemma2} that $|u-v|\geq \epsilon$. Since the set of actions is bounded and $U^J_{12}>0$, the set of actions must be finite in equilibrium.
% \end{proof}

% \section{Proof for Proposition \ref{prop:partitioneq}} \label{app:pfeq}
