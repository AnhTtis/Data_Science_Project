\documentclass[11pt,reqno]{amsart}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
%\usepackage[notcite,notref]{showkeys}
\usepackage[usenames]{color}
\usepackage{eepic,epic}
\usepackage{url} 
\usepackage{verbatim}
\usepackage{epstopdf}

\usepackage{multirow}
\usepackage{array}

\usepackage{makecell}
\usepackage[normalem]{ulem}
\usepackage[dvipsnames]{xcolor}
\usepackage{algorithm,algpseudocode}
\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{tikz, xcolor}

%\usepackage{epsfig}
%\usepackage{latexcad}

% PAGE SETTING ---------------------------------------
\textheight 22.5  true cm
\textwidth 15 true cm
%\textwidth 18 true cm

\voffset -1.0 true cm
%\hoffset -2.5 true cm
\hoffset -1.5 true cm

\marginparwidth= 2 true cm
\renewcommand{\baselinestretch}{1.2}






% THEOREMS -------------------------------------------

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}{Lemma}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{slem}[thm]{Sublemma}
\newtheorem{ass}{Assumption}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{step}{Step}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\numberwithin{equation}{section}


\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
{\renewcommand\theinnercustomthm{#1}\innercustomthm}
{\endinnercustomthm}


\newcommand{\ep}{\epsilon}
\newcommand{\lx}{{\lambda, \xi}}
\newcommand{\pa}{\partial}

\newcommand{\chg}[1]{{\color{red}#1}}
\newcommand{\rpl}[1]{{\color{blue}#1}}
\newcommand{\mc}{\mathcal{C}}
\newcommand{\mh}{\mathcal{H}}
\newcommand{\mt}{\mathcal{T}}
\newcommand{\mv}{\mathcal{V}}
\newcommand{\mn}{\mathbb{N}}
\newcommand{\mr}{\mathbb{R}}

\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}

% MATH ------------------------------------------------

\newcommand{\supp}{\text{supp }}
\newcommand{\inp}[2]{\langle #1,#2 \rangle}
\newcommand{\dist}{\text{dist }}
\newcommand{\ty}{{\widetilde y}}
\newcommand{\R}{\mathbb{R}}


%-----------------------------------------------------------------

\newcommand {\com}[1]{\texttt{($\leftarrow$ #1)}}

% ----------------------------------------------------------------
\begin{document}
\title[]
{On the convergence analysis of the decentralized projected gradient descent} 
\author{Woocheol Choi, Jimyeong Kim \\
(Sungkyunkwan University, Mathematics)}
\address{}
\email{choiwc@skku.edu}
\email{jimkim@skku.edu}


\maketitle

\begin{abstract}
In this work, we are concerned with the decentralized optimization problem:
\begin{equation*}
\min_{x \in \Omega}~f(x) = \frac{1}{n} \sum_{i=1}^n f_i (x),
\end{equation*}
where $\Omega \subset \mathbb{R}^d$ is a convex domain and each $f_i : \Omega \rightarrow \mathbb{R}$ is a local cost only known to agent $i$.   A fundamental algorithm is the decentralized projected gradient method (DPG) given by
\begin{equation*}
            x_i(t+1)=\mathcal{P}_\Omega\Big[\sum^n_{j=1}w_{ij} x_j(t) -\alpha(t)\nabla f_i(x_i(t))\Big]
\end{equation*} 
where $\mathcal{P}_{\Omega}$ is the projection operator to $\Omega$ and $ \{w_{ij}\}_{1\leq i,j \leq n}$ are communication weight among the agents. While this method has been widely used in the literatures, its sharp convergence property has not been established well so far, except for the special case $\Omega  = \mathbb{R}^n$. This work establishes new convergence estimates of DPG when the aggregate cost $f$ is strongly convex and each function $f_i$ is smooth. If the stepsize is given by constant $\alpha (t) \equiv\alpha >0$ and suitably small, we prove that each $x_i (t)$ converges to an $O(\sqrt{\alpha})$-neighborhood of the optimal point. In addition, we take a one-dimensional example $f_i$ and prove that the point $x_i (t)$ converges to an $O(\alpha)$-neighborhood of the optimal point. Also, we obtain convergence estimates for decreasing stepsizes. Numerical experiments are also provided to support the convergence results.
\end{abstract}

\section{Introduction}\label{sec1}
Let us consider a multi-agent system with $n$ agents that form a connected network and cooperatively solve the following constrained  optimization problem:
\begin{equation}\label{problem}
\min_{x\in \Omega} f(x) =\frac{1}{n} \sum^n_{i=1} f_i(x),
\end{equation} 
where $f_i : \Omega \rightarrow \mathbb{R}$ is a local cost function only known to agent $i\in \mathcal{V}=\{1,2,\cdots,m\}$, and $\Omega\subset\mathbb{R}^d$ denotes a common convex closed set.  %One way to solve \eqref{problem} is the centralized gradient descent method that is
%\begin{equation}\label{centralized}
%x(t+1) = \mathcal{P}_{\Omega}\Big[ x(t) - \alpha(t) \nabla f(x(t))\Big],
%\end{equation}
%where
%Let $x^*$ denote the optimal solution of the optimization problem defined in \eqref{problem}, that is, $x^*$ is the value of $x$ that minimizes the objective function over the feasible set $\Omega$. We assume that such an optimal solution exists.
 This problem arises in many applications like engineering problems \cite{BCM, CYRC}, signal processing \cite{Boyd Gossip, QT} and machine learning problems \cite{BCN, FCG, RB}. We consider the decentralized projected gradient (DPG)  algorithm \cite{NO1, RNV} given by
\begin{equation}\label{scheme}
x_i(t+1)=\mathcal{P}_{\Omega}\bigg[\sum^n_{j=1}w_{ij}x_j(t) -\alpha(t)\nabla f_i({x}_i(t))\bigg],
\end{equation}
where $P_{\Omega}$ denotes the projection of a vector $y\in \mathbb{R}^d$ onto the domain $\Omega$:
\begin{equation*}
\mathcal{P}_{\Omega}[y] = \arg\min_{x\in \Omega}\|x-y\|.
\end{equation*}
The nonnegative weight scalars $w_{ij}$ are related to the communication pattern among agents in \eqref{problem} (see section \ref{sec2} for the detail). This algorithm is studied for various settings containing stochastic distributed optimization \cite{ALBR, KLBJ, NO3, RNV}, event-triggered communication \cite{KHT, LLSX}, and online problems \cite{AGL, CB, HCM}. %Moreover, there are various distributed algorithms to solve \eqref{problem} containing the distributed dual averaging method \cite{DAW}, consensus-based dual decomposition \cite{FMGP, SJ}, and the alternating direction method of multipliers (ADMM) based algorithms \cite{MJ, SLYW}. 
In addition, the algorithm has been widely used as a backbone for developing various algorithms such as the decentralized TD learning for the multi-agent reinforcement learning \cite{DMR} and distributed model predictive control \cite{JLYC}. Despite of its wide applications, the convergence property of the algorithm \eqref{scheme} has not been established well due to the difficulty of handling the projection operator $P_{\Omega}$ in the convergence analysis.

When $\Omega = \mathbb{R}^n$, the algorithm \eqref{scheme} becomes the unconstrained decentralized gradient descent as given by 
\begin{equation}\label{scheme2}
x_i(t+1)= \sum^n_{j=1}w_{ij}x_j(t) -\alpha(t)\nabla f_i({x}_i(t)).
\end{equation}
The convergence estimates of \eqref{scheme2} have been established well in the previous works \cite{NO1, NO2, YLY, CK}. For a constant stepsize, Nedi\'{c}-Ozdaglar \cite{NO1} showed that the sequence converges to an $O(\alpha)$-neighborhood of the optimal set.  In the earlier works \cite{NO1, NO2}, the convergence results were established under the assumption that the gradient $\nabla f_i$ has a uniform bound for $1\leq i \leq n$. Also, the sequence converges to the optimal set for suitable decreasing stepsize. In the recent work, \cite{YLY}, the convergence result was established for local cost functions with Lipschitz continuous gradient instead of the uniform boundedness assumption on the gradient. It was shown that the sequence with constant stepsize $\alpha (t) \equiv \alpha >0$ (smaller than a specific value) converges to an $O(\alpha)$-neighborhood of the optimal point exponentially fast if the global cost function is strongly convex. This result was extended \cite{CK} to the case with decreasing stepsize of the form $\alpha (t) = c/(t+w)^{\alpha}$ for $0 <\alpha \leq 1$. 

We mention that, if $\Omega \neq \mathbb{R}^n$, the convergence analysis for \eqref{scheme} becomes more challenging due to the projection operator. In the original works \cite{RNV,I, LQX}, the convergence results of \eqref{scheme} were established under the assumption that the gradient $\nabla f_i$ has a uniform bound for $1\leq i \leq n$. The work \cite{LQX} obtained the convergence rate $O(1/\sqrt{t})$ when the stepsize is chosen as $\alpha (t) = c/t$ for a suitable range of $c>0$.

The uniform boundedness of the gradient assumption was replaced by the $L$-smoothness property in the work \cite{LLSX}. For the algorithm \eqref{scheme} with constant stepsize $\alpha >0$, the authors showed that there exists a uniform bound  $D>0$ of the gradients $\|\nabla f_i (x_t)\|$ for all $t \geq 0$ and $1 \leq i \leq n$. In addition, the following convergence estimate was obtained:
\begin{equation}\label{eq-1-5}
\begin{split}
\|x_{i}(t+1) -x_*\| & \leq (1-c\alpha)^{t+1}\|\bar{x}_0 - x_*\| + \beta^{t+1} \|x_0\| + \frac{2\alpha D}{1-\beta} 
\\
&\quad + \sum_{i=0}^{t} (1-c\alpha)^{t-i}\Big( \alpha L \gamma_i + \frac{\alpha D}{\sqrt{n}}\Big),
\end{split}
\end{equation}
where $x_*$ denotes an optimizer of \eqref{problem} and $\gamma_i = \beta^{i} \|x_0\| + \frac{2\alpha D}{1-\beta}$.  The above mentioned results are summarized in Table \ref{known results}.

We remark that the right hand side of \eqref{eq-1-5} involves the following term
\begin{equation*}
\sum_{i=0}^{t} (1-c\alpha)^{t-i} \frac{\alpha D}{\sqrt{n}} =\frac{D}{c\sqrt{n}} \Big[ 1- (1-c\alpha)^{t+1}\Big],
\end{equation*}
which converges to $\frac{D}{c\sqrt{n}}$ as the number of iterations $t$ goes to infinity. This limit is independent of the stepsize $\alpha >0$. Therefore, the right hand side of above estimate \eqref{eq-1-5} in the limit $t\rightarrow \infty$ involves the term $\frac{D}{c\sqrt{n}}$. However, this convergence estimate is not as strong as the estimate for \eqref{scheme2} by the work in \cite{YLY} which showed that the sequence of \eqref{scheme2} with constant stepsize $\alpha (t) \equiv \alpha >0$ (below a certain threshold) converges exponentially fast to an $O(\alpha)$-neighborhood of the optimal point. Having these results, it is natural to pose the following question:

\medskip 

\noindent \emph{\textbf{Question}: Does the algorithm \eqref{scheme} with constant stepsize $\alpha >0$ converges to the optimizer $x_*$ up to an $O(\alpha^c)$ error for some $c>0$?}

\medskip 

\noindent It is worth mentioning that this fundamental question is still open and has not been resolved as of now. In this work, we show that the convergence property of this question holds with $c=1/2$ if the total cost function $f$ is $\mu$ strongly convex and each local cost function $f_i$ is $L_i$ smooth. In addition, we exhibit a concrete example where the property holds with $c=1$. 

To explain the difficulty in the convergence analysis of \eqref{scheme} compared to the case $\Omega = \mathbb{R}^n$, we note that averaging \eqref{scheme2} gives 
\begin{equation}\label{centralized}
\bar{x}(t+1) = \bar{x}(t) - \frac{\alpha (t)}{n} \sum_{i=1}^n \nabla f_i (x_i (t)),
\end{equation}
where $\bar{x}(t) = \frac{1}{n} \sum_{i=1}^n x_i (t)$. Then, if the stepsize is set to $\alpha \leq \frac{2}{\mu +L}$, one can obtain the following inequality:
\begin{equation*}
\begin{split}
\|\bar{x}(t+1)-x_*\|&\leq \Big(1-\frac{\mu L}{\mu+L}\alpha\Big) \|\bar{x}(t) -x_*\| + \frac{L\alpha}{n} \sum_{i=1}^n \|x_i (t) -\bar{x}(t)\|,
\end{split}
\end{equation*}
 when $f$ is $\mu$-strongly convex and each $f_i$ is $L$-smooth. This inequality is a major ingredient in the convergence estimate of \eqref{scheme2} in the work \cite{YLY, CK}, but the identity \eqref{centralized} no longer holds for \eqref{scheme} due to the projection. Instead,  we proceed to obtain a sequential estimate of the quantity $$\sum_{i=1}^N \|x_i (t) -x_*\|^2,$$
which enables us to offset the projection operator efficiently using the contraction property of the projection operator  (see Section \ref{sec4} for the detail).  As a result, we obtain a convergence result up to an error $O(\sqrt{\alpha})$. We point out that our result is obtained for \eqref{scheme} with the projection operator to an arbitrary convex set $\Omega \subset \mathbb{R}^d$ which is possibly unbounded. %Given this result, one may ask if the error $O(\sqrt{\alpha})$ can be improved to an $O(\alpha)$ error as in the case \eqref{scheme2}. Interestingly, we show that this is actually true for a specific example in dimension one. % and show that the projected algorithm \eqref{scheme} converges to up to an $O(\alpha)$ error by direct computation. 
%We believe that this computation will be helpful for establishing the convergence up to $O(\alpha)$ error for general convex domain in any dimension.




 \begin{table}[ht]
\centering
\label{known results}
\begin{tabular}{|c|c|c|c|c|c| }\cline{1-6}
& Cost& Smooth & Learning rate &  Regret  &Rate  
%\\
%\hline
%\makecell{Saeed Ghadimi \\ Guanghui Lan}&NC & No & &&  &
\\
\hline
&&&&&\\[-1em]
\cite{RNV} & C  & $\|\nabla f_i \|_{{\infty}}< \infty$ &\makecell{ $\sum_{t=1}^{\infty}\alpha (t) = \infty$\\ $\sum_{t=1}^{\infty}\alpha(t)^2 < \infty$} & $\|x_i (t) -x_*\|$ & $o(1)$  
\\
&&&&&\\[-1em]
\hline
&&&&&\\[-1em]
\cite{I} & C  &$\|\nabla f_i \|_{{\infty}}< \infty$ & $\alpha (t) = \frac{c}{t^{\alpha}}$ & \makecell{$\min_{1 \leq k \leq n}$ \\ $f(x_k (t)) - f_* $}& \makecell{$O(\frac{1}{n^{\alpha}})$ if $0 < \alpha < \frac{1}{2}$ \\  $O( \frac{\log n}{\sqrt{n}})$ if $\alpha = \frac{1}{2}$ \\ $O(\frac{1}{n^{1-\alpha}})$ if $\frac{1}{2}<\alpha <1$}  
\\
&&&&&\\[-1em]
\hline
&&&&&\\[-1em]
\cite{LQX} & SC  & $\|\nabla f_i \|_{{\infty}}< \infty$ &$\alpha (t) = c/t$ & $\|x_i (t) -x_*\|$ & $O(1/\sqrt{t})$  
\\
&&&&&\\[-1em]
\hline
&&&&&\\[-1em]
\cite{LLSX} & SC  & L-smooth &$\alpha(t)\equiv \alpha$  & $\|x_i (t) -x_*\|$ & $O(e^{-ct}) + O(\sqrt{\alpha}) + O(\frac{D}{\sqrt{n}})$  
\\
&&&&&\\[-1em]
\hline
&&&&&\\[-1em]
\makecell{This \\ work} & SC & L-smooth & $\alpha (t) \equiv \alpha $&$ \|\bar{x}(t) - x_*\|$ & $O(e^{-ct}) + O(\sqrt{\alpha})$  
\\ 
&&&&&\\[-1.2em]
\hline
&&&&&\\[-1em]
\makecell{This \\ work} &\makecell{Specific\\ example} & L-smooth & $\alpha (t) \equiv \alpha $&$ \|\bar{x}(t) - x_*\|$ & $O(e^{-ct}) + O({\alpha})$  
\\ 
&&&&&\\[-1.2em]
\hline
\makecell{This \\ work} & SC & L-smooth & $\alpha (t) = \frac{c}{t^{\alpha}}$&$ \|\bar{x}(t) - x_*\|$ & \makecell{ $O( \frac{1}{t^{\alpha/2}})$ if $0<\alpha <1$\\ $O(\frac{1}{t^{\mu_0}})$ if $\alpha =1$}  
\\ 
\hline
\end{tabular}
\vspace{0.1cm}
\caption{Convergene results for DPG. Here C and SC mean that convex and $\mu$-strongly convex, respectively.}
\end{table}
 


%We remark that the results \cite{YLY, CK} are established for decentralized gradient descent on whole space $\mathbb{R}^d$ not involving the projection operator. 

The rest of this paper is organized as follows. In Section \ref{sec2} we introduce some assumptions and state our main results. In Section \ref{sec3}, we recall some preliminary results and give some useful estimatest that we will use throughout the paper. Section \ref{sec4} is devoted to obtaining sequential estimates for the algorithm. Based on these sequential estimates, we establish the uniform boundedness of the sequence in Section \ref{sec5}. Then we obtain consensus estimates in Section \ref{sec6} and prove the main convergence results in  Section \ref{sec7}. In Section \ref{sec9}, we derive an optimal convergence result for a specific example in dimension one. Finally, we perform numerical experiments to support the main theorems in Section \ref{sec8}.
  
%Before ending this section, we state several notations used in this paper. The notation $[x]$ for $x\in\mathbb{R}$ means the largest integer less or equal to $x$. We use the bar notationto represent the average; e.g., for a vector $x = (x_1,x_2,\cdots,x_n)\in\mathbb{R}^n$, $\bar{x} = \frac{1}{n}\sum^n_{i=1}$. For a matrix $W \in \mathbb{R}^{n \times n}$, $w_{ij}$ denotes the $(i,j)$th entry of A. For a vector $x\in\mathbb{R}^d$, $\|x\|=\sqrt{x^Tx}$ denotes the standard Euclidean norm. In addition, for $X \in \mathbb{R}^{n\times d}$ given by $X=[x_1; x_2; \cdots ; x_n]$ with row vector $x_k \in \mathbb{R}^{d}$, we define the mixed norm $\|X\|$ by $\|X\| = (\sum_{i=1}^n \|x_i\|^2)^{\frac{1}{2}}$ and the maximum norm $\|X\|_{\infty} = \max_{1\leq i\leq n} \|x_i\|$. 

 
\section{Assumptions and main results}\label{sec2}

In this section, we state the assumptions on the total and local cost functions in \eqref{problem} and communication patterns among agents. Then we give the main results of this paper.  
We are interested in  \eqref{problem} when the local cost functions and the total cost functions satisfy the following strong convexity and smoothness assumption.
\begin{ass}\label{LS}
For each $i\in\{1,\cdots n\}$, the local cost function $f_i$ is $L_i$-smooth for some $L_i>0$, i.e., for any $x, y \in \Omega$ we have
\begin{equation*}\label{L-smooth}
\| \nabla f_i(x) - \nabla f_i(y)\| \leq L_i\|x-y\|\quad \forall~x,y \in \Omega.
\end{equation*}
We set $L = \max_{1\leq i \leq n} L_i$. Then   the total cost function $f(\cdot)=\frac{1}{n}\sum^{n}_{i=1}f_i(\cdot)$ is $L$-smooth.
\end{ass} 
Throughout the paper, we use $\|\cdot\|$ to denote the euclidean norm.
\begin{ass}\label{sc}
 The total cost function $f(\cdot)=\frac{1}{n}\sum^{n}_{i=1}f_i(\cdot)$ is $\mu$-strongly convex for some $\mu>0$, i.e., for any $x,y\in\Omega$, we have
\begin{equation*}\label{strong}
f(y) \geq f(x) + (y-x)\nabla f(x) + \frac{\mu}{2}\|y-x\|^2.
\end{equation*}
\end{ass}
Under this assumption, the function $f$ has a unique optimizier $x_* \in \Omega$. 
In decentralized optimization, a local agent informs its own information to other agents relying on shared communication networks which are characterized by an undirected graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$, where each node in $\mathcal{V}$ represents an agent, and each edge $\{i,j\} \in \mathcal{E}$ means that $i$ can send messages to $j$. The graph $\mathcal{G}$ is assumed to satisfy the following assumption. 
\begin{ass}\label{graph}
The communication graph $\mathcal{G}$ is fixed and connected.
\end{ass}
We define the mixing matrix $W=[w_{ij}]_{1\leq i,j\leq n}$ as follows. The nonnegative weight $w_{ij}$ is given for each communication link $\{i, j\}\in \mathcal{E},$ where $w_{ij}\neq0$ if $\{i,j\}\in\mathcal{E}$ and $w_{ij} = 0$ if $\{i,j\}\notin\mathcal{E}$. In this paper, we make the following assumption on the mixing matrix $W$. 
\begin{ass}\label{ass-1-1}
The mixing matrix $W = \{w_{ij}\}_{1 \leq i,j \leq n}$ is symmetric and doubly stochastic. The network is strongly connected and the weight matrix $W$ satisfies $\textrm{null}(I-W) = \textrm{span}\{1_n\}$. 
\end{ass}
Without loss of generality, we arrange the eigenvalues of $W$ to satisfy
\begin{equation*}
1=|\lambda_1 (W)| > |\lambda_2 (W)| \geq \cdots \geq |\lambda_n (W)| \geq 0.
\end{equation*}
It is well-known that we have $\beta:= |\lambda_2 (W) | <1$ under Assumption \ref{ass-1-1}. Furthermore, we have the following lemma.
\begin{lem}\label{lem-1-1}
Suppose that the mixing matrix $W$ satisfies the Assumption \ref{ass-1-1}. Then, for any $x = (x_1, \cdots, x_n) \in \mathbb{R}^{d\cdot n}$ we have
\begin{equation*}
\sum_{i=1}^n \Big\| \sum_{j=1}^n w_{ij} (x_j - \bar{x}) \Big\|^2 \leq \beta^2 \sum_{i=1}^n \|x_i - \bar{x}\|^2,
\end{equation*}
where $\bar{x} = \frac{1}{n}\sum^{n}_{i=1} x_i$.
\end{lem}
\begin{proof}
We refer to Lemma 1 in \cite{PN}.
\end{proof} 

\subsection{Main Results}
In centralized optimization, it is enough to show that the sequence generated by \eqref{centralized} converges to the optimal solution of \eqref{problem} since the central coordinate control all agents simultaneously. On the other hand, in decentralized optimization, each agent makes its own sequence and only informs its own information to its neighbor agents. Therefore, we also need to show that each sequence generated by \eqref{scheme} converges to the same point, in which case we say the consensus is achieved. Then we reveal this point converges to the optimal point. Before stating the results, we introduce some constants used to state and prove the results. Let $D :=\max_{1\leq i \leq n}\| \nabla f_i(x_*)\|$. We fix a variable $\delta>0$ such that $(1+\delta)\beta^2<1$ and let $\tilde{\beta}:= (1+\delta)\beta^2$. Also, we set the following constants
\begin{equation}\label{eq-2-51}
c_1 := 3L^2\bigg(1+\frac{1}{\delta}\bigg),\ c_2 := 3nD^2\bigg(1+\frac{1}{\delta}\bigg),\ c_3 := c_1 + L^2 , \ c_4 := \frac{4L^2}{\mu}
\end{equation}
and denote $\mathbf{x}(t)$, $\bar{\mathbf{x}}(t)$ and $\mathbf{x}_*\in\mathbb{R}^{d\cdot n}$ by
\begin{equation*}
\begin{split}
\mathbf{x}(t) =[x_1(t), x_2(t)\cdots,x_n(t)]^T,\
 \bar{\mathbf{x}}(t) =[\bar{x}(t),\cdots \bar{x}(t)]^T,\
 \mathbf{x}_* = [ x_*, \cdots, x_*]^T.
 \end{split}
\end{equation*}
where $\bar{x}(t) = \frac{1}{n}\sum^{n}_{i=1} x_i(t)$. We note that
\begin{equation*}
\|\mathbf{x}(t)-\mathbf{\bar{x}}(t)\|^2=\sum^n_{i=1}\|x_i(t)-\bar{x}(t)\|^2 \ \text{and} \ \|\mathbf{x}(t)-\mathbf{x}_*\|^2=n \|\bar{x}(t)-x_*\|^2.
\end{equation*}
In addition, since $\frac{1}{n}\sum^{n}_{i=1} x_i(t) -\bar{x}(t) =0$, it follows directly that for all $t\geq0$,
\begin{equation}\label{eq2-8}
\|\mathbf{x}(t) - \mathbf{x}_*\|^2= \|\mathbf{x}(t) -\bar{\mathbf{x}}(t)\|^2 + \|\bar{\mathbf{x}}(t) - \mathbf{x}_*\|^2.
\end{equation}
%We will use the quantities $\|\mathbf{x}(t) - \mathbf{x}_*\|^2$ and $\|\mathbf{x}(t) -\bar{\mathbf{x}}(t)\|^2 + \|\bar{\mathbf{x}}(t) - \mathbf{x}_*\|^2$, interchangeably.

Now we introduce our results on the projected decentralized gradient descent \eqref{scheme}. The first result provides the conditions for the uniform boundedness of the sequence $\{x_i (t)\}_{t \geq 0}$ in the sense that $\|\mathbf{x}(t)-\mathbf{\bar{x}}(t)\|^2$ is uniformly bounded for all $t\geq0$.
\begin{thm}\label{thm-2-11}
Suppose that Assumptions 1,3,4 hold. Also, assume that one of the following statements holds true:
\begin{enumerate}
\item $\Omega$ is bounded.
\item Each cost $f_i$ is convex and the stepsize is constant, i.e., $\alpha (t) \equiv \alpha$, satisfying $\alpha \leq \frac{1 + \lambda_n (W)}{L}$.  
\item Assumption 2 holds. Also the stepsize $\{\alpha (t)\}_{t \geq 0}$ is non-increasing and $\alpha (0)$ satisfies
$$
\alpha(0) < \min\bigg\{Z, \frac{\mu}{4c_1}, \frac{2}{L+\mu}\bigg\}.
$$ 
 Here   we have set the positive constant   $Z$ by  
$$
Z := \frac{1}{2c_3} \Big[ - \Big(c_4 + \frac{\mu}{4}\Big) + \sqrt{\Big(c_4 +\frac{\mu}{4}\Big)^2 + 4c_3 (1-\tilde{\beta})}\Big].
$$
\end{enumerate}
Then there exists a constant $R>0$ such that
\begin{equation*}\label{eq-2-8}
\|\mathbf{x}(t)-\mathbf{\bar{x}}(t)\|^2 \leq R 
\end{equation*}
holds for all $t\geq 0$.
\end{thm}

%Also we have
%\begin{equation}
%\tilde{Z} = \frac{2(1-\beta)}{  \frac{4L^2}{n\mu} + \frac{\mu}{4} + \sqrt{ \Big( \frac{4L^2}{n\mu}  + %\frac{\mu}{4}\Big)^2 + 48 L^2 \beta^2 + \frac{4L^2}{n}(1-\beta)}.}
%\end{equation}
The following assumption formulates the above uniform boundedness property:
\begin{ass}\label{ass-5}
There exists a constant $R>0$ such that
\begin{equation*}\label{eq-2-8}
\|\mathbf{x}(t)-\mathbf{\bar{x}}(t)\|^2 \leq R 
\end{equation*}
holds for all $t\geq 0$.
\end{ass}
Although the result of this assumption is proved in Theorem \ref{thm-2-11}, we proposed this assumption because the result may hold for larger ranges of $\alpha(t)$ than that guaranteed by Theorem \ref{thm-2-11}. Proving a sharper range of $\alpha (t)$ for the uniform boundedness property would be an interesting future work.

Now we state the consensus and convergence results for \eqref{scheme} both for the constant stepsize and decreasing stepsize. We first introduce the following consensus results based on the estimates for the consensus error $\|\mathbf{x}(t) - \mathbf{\bar{x}}(t)\|^2$.
\begin{thm}\label{thm2-5}
Suppose that Assumptions \ref{LS}-\ref{ass-5} hold. If $\{\alpha(t)\}_{t\geq0}$ satisfies $\alpha(0)\leq \frac{2}{L+\mu}$, then we have
\begin{equation*}
\|\mathbf{x}(t)-\bar{\mathbf{x}}(t)\|^2\leq \beta^t \|\mathbf{x}(0)-\bar{\mathbf{x}}(0)\|^2 + \frac{J\alpha (t)^2}{(1-\beta)^2}.
\end{equation*}
Here for constant stepsize, i.e. $\alpha(t)\equiv \alpha$, we set
\begin{equation*}
J := 3(L^2R^2 + nD^2).
\end{equation*}
For a decreasing stepsize, we set
\begin{equation*}
J:=3(L^2R^2 + nD^2)\cdot\sup_{s\geq0}\frac{\alpha(0)^2\beta^s + \alpha([s/2])^2}{\alpha(s)^2}.
\end{equation*}
\end{thm}
%Theorem \ref{thm2-5} demonstrates that the consensus is reached through the convergence of each agent's state, which exponentially converges to an $O(\alpha^2)$ neighborhood of the averaging point with a constant stepsize, and converges to the averaging point at a rate that depends on the stepsize. 
Theorem \ref{thm2-5} demonstrates that the consensus is reached exponentially up to  an $O(\alpha (t))$ error. Next we state the convergence results for the sequence $\{\bar{x}(t)\}_{t\geq0}$ towards the optimal point. Before stating the results, we introduce the following constants $G_1$ and $G_2$:
\begin{equation*}
\begin{split}
G_1 & =  2R\Big( \tilde{c}_3 \alpha (0)^2 +  \tilde{c}_4 \alpha (0) + \beta\Big) 
\\
G_2 & = \Big[ ( \tilde{c}_3 \alpha (0)^2 +  \tilde{c}_4 \alpha (0) + \beta) \frac{2J^2}{(1-\beta)^2}  + \tilde{c}_1 R+  \tilde{c}_2 \Big],
\end{split}
\end{equation*}
where
\begin{equation*}\label{eq-2-52}
\tilde{c}_1 := \frac{3L^2 }{1-\beta},\ \tilde{c}_2 := \frac{3nD^2 }{1-\beta},\ \tilde{c}_3 := \tilde{c}_1 + L^2 , \ \tilde{c}_4 := \frac{4L^2}{\mu}.
\end{equation*}
These constants are obtained by setting $\delta = (1-\beta)/\beta$ in the constants of \eqref{eq-2-51}. 
The following convergence result holds when the stepsize is given by a constant. 
\begin{thm}\label{thm2-3} 
Suppose that Assumptions \ref{LS}-\ref{ass-5} hold. If the stepsize is given by a constant  $\alpha >0$ such that $\alpha\leq\frac{2}{L+\mu}$, then
we have
\begin{equation*}
 \|\bar{\mathbf{x}}(t) - \mathbf{x}_*\|^2 \leq  \Big(1 - \frac{\mu \alpha}{2}\Big)^{t} \|\bar{\mathbf{x}}(0) - \mathbf{x}_*\|^2+ \frac{2 G_2}{\mu}\alpha +\frac{2}{\mu\alpha}\left(\left(1-\frac{\mu\alpha}{2}\right)^{t-1} + \beta^{\frac{t-1}{2}}\right).
\end{equation*}
\end{thm}
Theorem \ref{thm2-3} implies the sequence generated by \eqref{scheme} converges to an $O(\sqrt{\alpha})$-neighborhood of the optimal point exponentially fast. We recall from \cite{YLY} that the sequence of the algorithm \eqref{scheme} on the whole space $\mathbb{R}^d$ converges to an $O(\alpha)$-neighborhood of $x_*$. This naturally leads us to pose the following question.

\medskip

\noindent \emph{\textbf{Question}: Is the convergence error $O(\sqrt{\alpha})$ in Theorem \ref{thm2-3} optimal? or can we improve the convergence error to $O(\alpha)$?}

\medskip

We give a partial answer to this question in Section  \ref{sec9}. Precisely, we find a one-dimensional example such that the algorithm \eqref{scheme} converges to an $O(\alpha)$ neighborhood of the optimal point. 

In the below, we provide the convergence results when the stepsize is given by the decreasing stepsize $\alpha(t) = v/(t+w)^p$ for $0 < p \leq 1$. %Theorems \ref{thm2-6} and \ref{thm2-7} correspond to $0\leq p \leq 1$ and $p=1$, respectively.
\begin{thm}\label{thm2-6}
Suppose that Assumptions \ref{LS}-\ref{ass-5} hold.   Let $p\in(0,1)$ and assume that $\alpha(t) = \frac{v}{(t+w)^p}$ with $v, w>0$ satisfying
$$  
\alpha(0)=\frac{v}{w^p} \leq  \frac{2}{L+\mu}. 
$$
Then we have
\begin{equation*}
  \|\bar{\mathbf{x}}(t) - \mathbf{x}_*\|^2 
\leq \frac{2e Q \Big( \rho G_1 + G_2\Big)  v}{\mu} ([t/2]+w-1)^{-p} + \mathcal{R}_1(t) + \mathcal{R}_2(t),  
\end{equation*} 
where $Q = \Big( \frac{w+1}{w}\Big)^{2p}$, $\rho = \sup_{t \geq 0} \beta^{t}/\alpha (t)^2$ and
\begin{equation*}
\begin{split}
\mathcal{R}_1(t) &= e^{-\sum^{t-1}_{s=0}\frac{\mu v}{2(s+w)^p}}\|\bar{\mathbf{x}}(0) - \mathbf{x}_*\|^2,\\
\mathcal{R}_2(t) &= Q\Big( \rho G_1 + G_2\Big) v^2 e^{-\frac{\mu vt}{4(t+w)^p}}\sum^{[t/2]-1}_{s=1}\frac{1}{(s+w)^{2p}}.
\end{split}
\end{equation*}
\end{thm}
In the above result, we easily see that for any fixed $N>0$, there exists a constant $C_N>0$ independent of $t\geq0$ such that
$$
\mathcal{R}_1(t) + \mathcal{R}_2(t) \leq C_Nt^{-N}.
$$


\begin{thm}\label{thm2-7}
Suppose that Assumptions \ref{LS}-\ref{ass-5} hold. Let  $\alpha(t) = \frac{v}{(t+w) }$
with $v, w>0$ satisfying
$$  
\alpha(0)=\frac{v}{w } \leq \frac{2}{L+\mu}. 
$$
Also, choose $v>0$ such that $  \mu v/2 >1$. Then we have
\begin{equation*}
 \|\bar{\mathbf{x}}(t) - \mathbf{x}_*\|^2\leq \left(\frac{w}{t+w}\right)^{ \mu v/2 }  \|\bar{\mathbf{x}}(0) - \mathbf{x}_*\|^2  + \mathcal{R}_3(t),
\end{equation*}
where $Q = \Big( \frac{w+1}{w}\Big)^2$, $\rho = \sup_{t \geq 0} \beta^{t}/\alpha (t)^2$  and 
\begin{equation*}
\mathcal{R}_3(t)= \frac{Q}{( \mu v/2 ) -1} \Big( \frac{w+1}{w}\Big)^{ \mu v/2  } \frac{ \Big( \rho G_1 + G_2\Big)  v^2}{(t+w-1)}.
\end{equation*}  
\end{thm}

\begin{comment}
Next, we propose a general condition on the stepsize which guarantees the convergence of the projected distributed gradient descent. 


\begin{ass}\label{ass-2-12}
The sequence of stepsize $\{\alpha(t)\}_{t\geq0}$ is monotonically non-increasing and $\sum_{t=0}^{\infty}\alpha (t) = \infty$. Also,  the function
$$
G(t) = \min_{0 \leq N \leq t-1} \Big(e^{-\sum^t_{k=N}\alpha(k)}\sum^{N}_{s=0} \alpha(s)^2 +\sum^{t}_{s=N+1} \alpha(s)^2\Big),
$$
satisfies $\lim_{t\rightarrow \infty} G(t) =0$.
\end{ass}
In many papers, the sequence of stepsize satisfies the condition
\begin{equation}\label{eq-2-30}
\sum^{\infty}_{t=0} \alpha(t) = \infty\quad \text{and} \quad \sum^\infty_{t=0} \alpha(t)^2 < \infty.
\end{equation}
We prove that Assumption \ref{ass-2-12} still holds for any stepsize satisfying \eqref{eq-2-30}. Under Assumption \ref{ass-2-12} we establish the convergence result in the following theorem.
\begin{thm}\label{thm2-9}
Suppose that Assumptions \ref{LS}-\ref{ass-1-1} and \ref{ass-2-12} hold. The sequence $\{x_i(t)\}_{t\geq0}$ generate by \eqref{scheme} for all $1\leq i \leq n$.  Assume that
$$ 
\alpha(0) < \min\bigg\{Z, \frac{\mu}{2c_1}, \frac{2}{L+\mu}\bigg\}. 
$$
Then $x_i(t)$ is convergent to $x_*$ for all $1\leq i \leq n$.  
\end{thm}
\end{comment}
\section{Preliminary results}\label{sec3}
In this section, we prepare several estimates which will be used to derive two sequential estimates %of $\|\mathbf{x}(t)-\mathbf{\bar{x}}(t)\|^2$ and $\|\mathbf{x}(t)-\mathbf{\bar{x}}(t)\|^2+\|\mathbf{\bar{x}}(t)-\mathbf{x}_*\|^2$ 
in the next section. We first study the projection operator $\mathcal{P}_{\Omega}$ in \eqref{scheme} defined by
$$
\mathcal{P}_\Omega[x] = \arg \min_{y\in\Omega} \|y-x\|.
$$
This projection operator has the following property:
\begin{lem}\label{lem-1-2}
Let $\Omega\subset \mathbb{R}^d$ be convex and closed. 
\begin{enumerate}
\item For any $x, y \in \mathbb{R}^d$, we have
\begin{equation}\label{eq-2-2}
\|\mathcal{P}_{\Omega} [x]-\mathcal{P}_{\Omega}[y]\| \leq \|x-y\|.
\end{equation}
\item For any $x\in \mathbb{R}^d$ and $y\in\Omega$, we have
\begin{equation}\label{eq-2-1}
\|\mathcal{P}_{\Omega}[x] - y\| \leq \|x-y\|.
\end{equation}
\end{enumerate}
\end{lem}
\begin{proof}
We refer to Theorem 1.5.5 in \cite{FP} for the proof of estimate \eqref{eq-2-2}. Taking $y\in\Omega$ in \eqref{eq-2-2} leads to the estimate \eqref{eq-2-1}. 
\end{proof}
The following lemma will be used to obtain the sequential estimates in Section \ref{sec4}.
\begin{lem}\label{lem-1-7}
Let $\Omega\subset \mathbb{R}^d$ be convex and closed. Then, for any $x_1,\cdots, x_n\in \mathbb{R}^d$, we have
\begin{equation*}
\sum^n_{i=1} \Bigg\| \mathcal{P}_{\Omega}[x_i] - \frac{1}{n}\sum^n_{j=1}\mathcal{P}_{\Omega}[x_j]\Bigg\|^2\leq \sum^n_{i=1} \left\| x_i - \frac{1}{n}\sum^n_{j=1}x_j \right\|^2.
\end{equation*}
\begin{proof}
For given $\{a_i\}_{i=1}^n \subset \mathbb{R}^d$, consider the function $F: \mathbb{R}^d \rightarrow \mathbb{R}$ defined by
\begin{equation*}
F(x) = \sum_{i=1}^n \|a_i -x\|^2,
\end{equation*}
This function is minimized when $x = \frac{1}{n} \sum_{i=1}^n a_i$, and using this we find
\begin{equation*}
\sum^n_{i=1} \Bigg\| \mathcal{P}_{\Omega}[x_i] - \frac{1}{n}\sum^n_{j=1}\mathcal{P}_{\Omega}[x_j]\Bigg\|^2\leq \sum^n_{i=1} \Bigg\| \mathcal{P}_{\Omega}[x_i] - \mathcal{P}_{\Omega}\bigg[\frac{1}{n}\sum^n_{j=1}x_j\bigg]\Bigg\|^2.
\end{equation*}
Combining this with \eqref{eq-2-2}, we get the desired inequality.
\end{proof}
\end{lem}

\begin{lem}\label{lem-1-4}
Suppose that Assumptions \ref{LS} and \ref{sc} hold. If $\alpha(t)\leq \frac{2}{L+\mu}$ for all $t\geq 0 $, then we have
\begin{equation}\label{eq3-6}
\left\| \bar{x}(t)-x_*- \frac{\alpha(t)}{n}\sum^n_{i=1}( \nabla f_i(\bar{x}(t))- \nabla f_i(x_*))\right\|^2    \leq \bigg(1-\frac{\mu\alpha(t)}{2} \bigg)^2\| \bar{x}(t)-x_*\|^2 .
\end{equation}
\end{lem}

\begin{proof}
We expand the left hand side in \eqref{eq3-6} as
\begin{equation}\label{eq3-5}
\| \bar{x}(t)-x_*\|^2 - 2\alpha(t)\Big\langle \bar{x}(t)-x_*, \nabla f(\bar{x}(t))- \nabla f(x_*)\Big\rangle + \alpha(t)^2\left\| \nabla f(\bar{x}(t))- \nabla f(x_*)\right\|^2,
\end{equation}
where $f(x) = \frac{1}{n}\sum^n_{i=1} f_i(x).$ Note that $f$ is $L$-smooth and $\mu$-strongly convex by Assumptions \ref{LS} and \ref{sc}, and so we have the following  inequality (see e.g., \cite[Lemma 3.11]{B}):
$$
\Big\langle\bar{x}(t)-x_*,~\nabla f(\bar{x}(t))-\nabla f(x_*)\Big\rangle \geq \frac{L\mu}{L+\mu}\|\bar{x}(t)-x_*\|^2 + \frac{1}{L+\mu}\|\nabla f(\bar{x}(t)) - \nabla f(x_*)\|^2.
$$
Putting the above inequality in \eqref{eq3-5}, we get
\begin{align*}
&\left \| \bar{x}(t)-x_*- \frac{\alpha(t)}{n}\sum^n_{i=1} ( \nabla f_i(\bar{x}(t))- \nabla f_i(x_*))\right\|^2 \\
&\leq \bigg(1-\frac{2L\mu\alpha(t)}{L+\mu} \bigg)\| \bar{x}(t)-x_*\|^2  + \alpha(t)\bigg(\alpha(t)-\frac{2}{L+\mu}\bigg)\|\nabla f (\bar{x}(t))-\nabla f(x_*)\|^2.
\end{align*}
Using the assumption $\alpha(t)\leq \frac{2}{L+\mu}$ and $2L\geq L+\mu$, we then have
\begin{equation*}
\begin{split}
\left\| \bar{x}(t)-x_*- \frac{\alpha(t)}{n}\sum^n_{i=1}( \nabla f_i(\bar{x}(t))- \nabla f_i(x_*))\right\|^2 & \leq \bigg(1-\frac{2L\mu\alpha(t)}{L+\mu} \bigg)\| \bar{x}(t)-x_*\|^2 
\\
&\leq \bigg(1-\frac{\mu\alpha(t)}{2} \bigg)^2\| \bar{x}(t)-x_*\|^2 .
\end{split}
\end{equation*}
The proof is done.
\end{proof}

\begin{lem}\label{lem-1-5}
Suppose that Assumption \ref{LS} holds. For $(x_1, \cdots, x_n ) \in \mathbb{R}^{dn}$ and $\bar{x} = \frac{1}{n} \sum_{k=1}^n x_k$  we have
\begin{equation}\label{eq3-7}
\sum^n_{i=1}\|  \nabla f_i({x}_i) - \nabla f_i(\bar{x})  \|^2 \leq L^2\|\mathbf{x} - \bar{\mathbf{x}}\|^2\end{equation}
and
\begin{equation}\label{eq3-8}
\sum_{i=1}^n \Big\| \nabla f_i({x}_i) - \frac{1}{n} \sum_{l=1}^n \nabla f_l ({x}_l) \Big\|^2 \leq  3L^2 \| \mathbf{x} - \bar{\mathbf{x}}\|^2 +3L^2 \| \bar{\mathbf{x}}- \mathbf{x}_*\|^2 + 3nD^2.
\end{equation}
\end{lem}
\begin{proof}
By Assumption \ref{LS}, we have
\begin{equation*}
\sum^n_{i=1}\|  \nabla f_i({x}_i) - \nabla f_i(\bar{x})  \|^2 \leq L^2 \sum^n_{i=1}\|  {x}_i-\bar{x} \|^2,
\end{equation*}
which directly implies \eqref{eq3-7}.
Next, we prove \eqref{eq3-8}.  Note that for any $a=(a_1,\cdots,a_n)\in\mathbb{R}^n$, we have 
\begin{equation*}
\begin{split}
\sum^n_{i=1}\left\|a_i -\frac{1}{n}\sum^n_{l=1}a_l\right\|^2&=\sum^n_{i=1}\left(\|a_i\|^2 -2\left\langle a_i, \frac{1}{n}\sum^n_{l=1}a_l\right\rangle +\frac{1}{n^2}\left\|\sum^n_{l=1}a_l\right\|^2\right)\\
&=\sum^n_{i=1} \|a_i\|^2 +\left(\frac{1}{n^2}-\frac{2}{n}\right)\left\|\sum^n_{l=1}a_l\right\|^2 \\
&\leq \sum^n_{i=1} \|a_i\|^2.
\end{split}
\end{equation*}
Using this, it follows that
\begin{equation*}\label{eq-3-11}
\begin{split}
\sum_{i=1}^n \left\| \nabla f_i({x}_i)  - \frac{1}{n} \sum_{l=1}^n \nabla f_l ({x}_l)\right\|^2 &\leq \sum^n_{i=1}  \| \nabla f_i({x}_i(t))\|^2.
\end{split}
\end{equation*}
By the triangle inequality, one has
\begin{align*}
\|\nabla f_i({x}_i)\|^2 &\leq 3\|\nabla f_i({x}_i) - \nabla f_i(\bar{x}) \|^2 + 3\|  \nabla f_i(\bar{x}) - \nabla f_i(x_*)\|^2 + 3\|\nabla f_i(x_*) \|^2.
\end{align*}
This, together with \eqref{eq3-7} and Assumption \ref{LS}, gives
\begin{equation*}
\sum^n_{i=1}\|\nabla f_i({x}_i)\|^2\leq 3L^2 \| \mathbf{x} - \bar{\mathbf{x}}\|^2 +3L^2 \| \bar{\mathbf{x}}- \mathbf{x}_*\|^2 + 3nD^2.
\end{equation*}
Combining this with \eqref{eq-3-50} gives the desired estimate.
\end{proof}
  
%We denote
%$$\mathbf{x} = [x_1, x_2,  \cdots, x_n ]^T, \nabla F(\mathbf{x}) =[ \nabla f_1( x_1) ,\ \nabla f_2( x_2), \cdots ,\nabla f_n(x_n)]^T$$  


\begin{lem}\label{lem-3-8}
Suppose that Assumptions \ref{LS} and \ref{sc} hold.  If the diminishing sequence $\{\alpha(t)\}_{t\geq0}$  satisfy $\alpha(0)\leq \frac{2}{L+\mu}$, then the sequence $\{x_i(t)\}_{t\geq0}$ generating by \eqref{scheme} for all $1\leq i \leq n$ satisfies the following inequality 
\begin{equation}\label{eq-3-50}
\begin{split}
& n\left\|\bar{x}(t)-x_* -\alpha(t)\left(\frac{1}{n}\sum^n_{i=1} \nabla f_i({x}_i(t)) -  \nabla f(x_*)\right) \right\|^2\\
&\quad \leq \bigg(1- \frac{\mu \alpha (t)}{2}\bigg)\|\bar{\mathbf{x}}(t) - \mathbf{x}_* \|^2 + \bigg(L^2\alpha(t)^2+\frac{4L^2\alpha(t)}{\mu}\bigg) \|\mathbf{x}(t)-\bar{\mathbf{x}}(t) \|^2.
\end{split}
\end{equation}
%where $\mathbf{\bar{x}}(t)$, $\mathbf{x}_*$, $\mathbf{x}(t)\in \mathbb{R}^n$ such that
%$$
%\bar{\mathbf{x}}(t) = [\bar{x}(t),\cdots,\bar{x}(t)]^T, \ \mathbf{x}_*=[x_*,\cdots,x_*]^T, \ \mathbf{x}%(t)=[x_1(t),\cdots,x_n(t)]^T
%$$
\end{lem}
\begin{proof} 
By Young's inequality, for any $x$ and $y$ in $\mathbb{R}^n$ we have
\begin{equation}\label{eq3-11}
\|x+y\|^2 \leq (1+\eta) \|x\|^2 + \Big( 1+ \frac{1}{\eta}\Big) \|y\|^2
\end{equation}
for all $\eta >0$.
For notational convenience, we let $H(t)= \frac{1}{n}\sum^n_{i=1} \nabla f_i({x}_i(t))$. Using \eqref{eq3-11}, we obtain the following inequality:
\begin{align*}
\nonumber&\quad \left\|\bar{x}(t)-x_* -\alpha(t) \left(H(t) -  \nabla f(x_*)\right) \right\|^2 \\
\nonumber&= \left\|\bar{x}(t) - x_* - \alpha(t) \left(\nabla f(\bar{x}(t))-\nabla f(x_*)\right) + \alpha(t) \left( \nabla f(\bar{x}(t)) - H(t) \right) \right\|^2\\
&\leq  (1+\eta)\left\|\bar{x}(t) - x_* - \alpha(t) \left(\nabla f(\bar{x}(t))-\nabla f(x_*)\right)\right\|^2
\\&\quad\quad\qquad\qquad\qquad+ \left(1+\frac{1}{\eta}\right)\alpha(t)^2 \left\|  \nabla f(\bar{x}(t)) -H(t) \right\|^2.
\end{align*}
 Now we estimate the right hand side of the last inequality. By Lemma \ref{lem-1-4}, it follows that
\begin{equation}\label{eq3-15}
(1+\eta)\left\|\bar{x}(t) - x_* - \alpha(t) \left(\nabla f(\bar{x}(t))-\nabla f(x_*)\right)\right\|^2 \leq (1+\eta)\bigg(1-\frac{\mu\alpha(t)}{2} \bigg)^2\| \bar{x}(t)-x_*\|^2 .
\end{equation} 
Note that by the Cauchy-Schwarz inequality, we get 
\begin{equation*}
\begin{split}
\left\| \nabla f(\bar{x}(t)) -H(t) \right\|^2 &= \frac{1}{n^2}\left\|\sum^n_{i=1}\left(\nabla f_i(\bar{x}(t)) - \nabla f_i({x}_i(t)) \right) \right\|^2\\
&\leq \frac{1}{n}\sum^n_{i=1} \left\| \nabla f_i(\bar{x}(t)) - \nabla f_i({x}_i(t))\right\|^2.
\end{split}
\end{equation*}
Using this inequality and Lemma \ref{lem-1-5}, we obtain
\begin{equation}\label{eq3-16}
\begin{split}
\left(1+\frac{1}{\eta}\right)\alpha(t)^2\left\| \nabla f(\bar{x}(t)) - H(t) \right\|^2 &\leq \left(1+\frac{1}{\eta}\right)\frac{\alpha(t)^2}{n}\sum^n_{i=1} \| \nabla f_i(\bar{x}(t)) - \nabla f_i({x}_i(t))\|^2
\\
&\leq \left(1+\frac{1}{\eta}\right)\frac{L^2\alpha (t)^2}{n} \sum_{i=1}^n \|\bar{x}(t) - x_i (t)\|^2.
\end{split}
\end{equation}
Now setting $\eta = \frac{\mu\alpha(t)}{4}$ and combining \eqref{eq3-15} and \eqref{eq3-16}, we have
\begin{align*}
 &n \left\|\bar{x}(t)-x_* -\frac{\alpha(t)}{n}\sum^n_{l=1} \left(\nabla f_l({x}_l(t)) -  \nabla f_l(x_*)\right) \right\|^2\\
 &\leq n\bigg(\bigg(1+\frac{\mu\alpha(t)}{4}\bigg)\bigg(1-\frac{\mu\alpha(t)}{2} \bigg)^2\| \bar{x}(t)-x_*\|^2  +\bigg(1+\frac{4}{\mu\alpha(t)}\bigg)\frac{L^2\alpha(t)^2}{n}\sum^n_{i=1} \| \bar{x}(t) - x_i(t)\|^2\bigg)\\
 &=\bigg(1+\frac{\mu\alpha(t)}{4}\bigg)\bigg(1-\frac{\mu\alpha(t)}{2} \bigg)^2\|\bar{\mathbf{x}}(t) - \mathbf{x}_* \|^2 + \bigg(1+\frac{4}{\mu\alpha(t)}\bigg)L^2\alpha(t)^2 \|\mathbf{x}(t) - \bar{\mathbf{x}}(t) \|^2.
\end{align*}
Here we used
\begin{equation*}
n\|\bar{x}(t) - x_*\|^2 = \|\bar{\mathbf{x}}(t)-\mathbf{x}_*\|^2 \ \text{and} \ \sum^n_{i=1}\|\bar{x}(t) -x_i(t)\|^2 = \|\mathbf{x}(t) - \mathbf{\bar{x}}(t)\|^2
\end{equation*}
for the last equality.
Using $\alpha (t) \leq \frac{2}{\mu}$ we have
\begin{equation*}
\begin{split}
\Big( 1+ \frac{\mu \alpha (t)}{4}\Big) \Big( 1- \mu \alpha (t) + \frac{\mu^2 \alpha (t)^2}{4}\Big)& = 1-\frac{3}{4} \mu \alpha (t) + \frac{\mu^3}{16}\alpha (t)^3
\\
& = 1- \frac{\mu \alpha (t) }{2} - \Big( \frac{\mu \alpha (t)}{4} - \frac{\mu^3 \alpha (t)^3}{16}\Big)
\\
&\leq 1- \frac{\mu \alpha (t)}{2}.
\end{split}
\end{equation*}
Using this, we obtain the desired estimate. The proof is done.
\end{proof}
\section{Sequential estimates}\label{sec4}
In this section, we establish sequential estimates which will be used importantly to derive the convergence results of the algorithm \eqref{scheme}.
As discussed in the end of Section \ref{sec1}, the projection operator makes it difficult to average the equation \eqref{scheme} to obtain \eqref{centralized}. To get around this difficulty, we estimate the quantity $\|\mathbf{x}(t+1)-\mathbf{x}_*\|^2$ instead of $\|\bar{\mathbf{x}}(t+1)-\mathbf{x}_*\|$ to analyze the sequence of \eqref{scheme}.
 
 Precisely, we aim to establish an estimate of $\|\mathbf{x}(t+1)-\mathbf{x}_*\|^2$ in terms of $\|\mathbf{x}(t)-\bar{\mathbf{x}}(t)\|^2$ and $\|\bar{\mathbf{x}}(t)-\mathbf{x}_*\|^2$ by applying the contraction property of the projection operator. We  start this section by deriving an estimate of $\|\mathbf{x}(t) - \bar{\mathbf{x}}(t)\|^2$. For the reader's convenience, we recall the constant $c_1$ and $c_2$ as 
\begin{equation*}
c_1 := 3L^2\bigg(1+\frac{1}{\delta}\bigg),\ c_2 := 3nD^2\bigg(1+\frac{1}{\delta}\bigg).
\end{equation*}  
%Before stating the results, we briefly explain a standard argument for the decentralized gradient descent on the whole space  (\cite{YLY, CK})  and explain that the argument is not easy to apply for the algorithm \eqref{scheme} involving the projection operator. 
%Since $\frac{1}{n} \sum_{i=1}^{n} \mathcal{P}_\Omega[a_i] \neq \mathcal{P}_\Omega\Big[ \frac{1}{n} \sum_{i=1}^n a_i\Big]$ for $a_i \in \mathbb{R}^d$ in general, the above argument is difficult to apply for in a direct way if we first average the algorithm with respect to $1 \leq j \leq n$. Instead, we get around the difficulty by applying the projection inequality for each $x_i(t+1)$ and $\bar{x}(t+1)$ first, and then adding their squares.
\begin{prop}\label{lem-3-1}
Suppose that Assumptions \ref{LS}-\ref{ass-1-1} hold.  If $\{\alpha(t)\}_{t\geq0}$  satisfies $\alpha(0)\leq \frac{2}{L+\mu}$, then the sequence $\{x_i(t)\}_{t\geq0}$ generated by \eqref{scheme} for all $1\leq i \leq n$ satisfies the following inequality. 
\begin{equation*}
\begin{split}
\| \mathbf{x}(t+1) -\bar{\mathbf{x}}(t+1)\|^2 \leq (c_1\alpha(t)^2 + \tilde{\beta})\| \mathbf{x}(t) - \bar{\mathbf{x}}(t)\|^2 + c_1\alpha(t)^2 \|\bar{\mathbf{x}}(t)-\mathbf{x}_*\|^2 + c_2\alpha(t)^2.
\end{split}
\end{equation*}
\end{prop}
\begin{proof}
For the reader's convenience, we recall the algorithm \eqref{scheme} as 
\begin{equation*}
x_i(t+1)=\mathcal{P}_{\Omega}\bigg[\sum^n_{j=1}w_{ij}x_j(t) -\alpha(t)\nabla f_i({x}_i(t))\bigg],
\end{equation*}
and note that $\| \mathbf{x}(t+1) -\bar{\mathbf{x}}(t+1)\|^2= \sum^n_{i=1} \|x_i(t+1) - \bar{x}(t+1)\|^2$ by definition. Using this equality, we derive the following estimate.
\begin{align*}
&\sum^n_{i=1} \|x_i(t+1) - \bar{x}(t+1)\|^2\\
&= \sum^n_{i=1}\left\|\mathcal{P}_{\Omega}\bigg[\sum^n_{j=1}w_{ij}x_j(t) -\alpha(t)\nabla f_i({x}_i(t))\bigg] - \frac{1}{n}\sum^n_{k=1}\mathcal{P}_{\Omega}\bigg[\sum^n_{j=1}w_{kj}x_j(t) -\alpha(t)\nabla f_i({x}_k(t))\bigg]\right\|^2 \\
&\leq \sum^{n}_{i=1}\left\| \sum^n_{j=1}w_{ij}\left(x_j(t) - \bar{x}(t)\right) -\alpha(t)\nabla f_i({x}_i(t)) + \frac{\alpha(t)}{n}\sum^n_{l=1}\nabla f_l({x}_l(t)) \right\|^2\\
&\leq\sum^n_{i=1}(1+\delta)\left\| \sum^n_{j=1}w_{ij}\left(x_j(t) - \bar{x}(t)\right)\right\|^2 \\ 
&\qquad\qquad\qquad\qquad\qquad\qquad+ \sum^n_{i=1} \left(1+\frac{1}{\delta}\right)\alpha(t)^2 \left\| \nabla f_i({x}_i(t)) + \frac{1}{n}\sum^n_{l=1}\nabla f_l({x}_l(t))  \right\|^2.
\end{align*}
Here we used Lemma \ref{lem-1-7} for the first inequality and Young's inequality for the last inequality for $\delta >0$. We can further write the right hand side of the last inequality as
\begin{equation}\label{3-12}
\begin{split}
&(1+\delta)\left\|W(\mathbf{x}(t)-\bar{\mathbf{x}}(t))\right\|^2 + \sum^n_{i=1} \left(1+\frac{1}{\delta}\right)\alpha(t)^2 \left\| \nabla f_i({x}_i(t)) + \frac{1}{n}\sum^n_{l=1}\nabla f_l({x}_l(t))  \right\|^2 \\
&\leq (1+\delta)\beta^2\|\mathbf{x}(t)- \bar{\mathbf{x}}(t)\|^2+
\sum^n_{i=1} \left(1+\frac{1}{\delta}\right)\alpha(t)^2 \left\| \nabla f_i({x}_i(t)) + \frac{1}{n}\sum^n_{l=1}\nabla f_l({x}_l(t))  \right\|^2.
\end{split}
\end{equation}
Here we applied Lemma \ref{lem-1-1} to the last inequality. Combining Lemma \ref{lem-1-5} with \eqref{3-12}, we obtain the desired estimate. The proof is done.
\end{proof}
Next, we give an estimate of the quantity $\|\mathbf{x}(t) - \mathbf{x}_*\|^2$. Before starting the statement, we recall the constants $c_3$ and  $c_4$:
\begin{equation*}
c_3 := c_1 + L^2 , \ c_4 := \frac{4L^2}{\mu}.
\end{equation*}
Using the relation \eqref{eq2-8}, we state the following propostion.  
\begin{prop}\label{prop-3-2}
Suppose that Assumptions \ref{LS}-\ref{ass-1-1} hold.  If the diminishing sequence $\{\alpha(t)\}_{t\geq0}$  satisfy $\alpha(0)\leq \frac{2}{L+\mu}$, then the sequence $\{x_i(t)\}_{t\geq0}$ generated by \eqref{scheme} for all $1\leq i \leq n$ satisfes the following inequality. 
\begin{equation*}
\begin{split}
&\|\mathbf{x}(t+1) -\bar{\mathbf{x}}(t+1)\|^2 + \|\bar{\mathbf{x}}(t+1) - \mathbf{x}_*\|^2\\
&\leq \left(c_3\alpha(t)^2 + c_4\alpha(t) + \tilde{\beta}\right) \| \mathbf{x}(t) - \bar{\mathbf{x}}(t)\|^2 +\bigg(1-\frac{\mu}{2}\alpha(t) + c_1\alpha(t)^2\bigg) \|\bar{\mathbf{x}}(t)-\mathbf{x}_*\|^2 + c_2\alpha(t)^2.
\end{split}
\end{equation*}
\end{prop}
\begin{proof}
Note that since $x_* = \arg\min_{x\in\Omega} f(x)$, it follows that $x_* = \mathcal{P}_\Omega\left[ x_* -\alpha(t)\nabla f(x_*)\right]$. By the algorithm \eqref{scheme} and using \eqref{eq-2-2} we deduce
\begin{equation*}
\begin{split}
\| x_i(t+1) - x_* \|^2&= \bigg\| \mathcal{P}_\Omega\bigg[\sum^n_{j=1}w_{ij} x_j(t) -\alpha(t)\nabla f_i({x}_i(t))\bigg] - \mathcal{P}_\Omega\left[ x_* -\alpha(t)\nabla f(x_*)\right]\bigg\|^2 \\
&\leq \bigg\|\sum^n_{j=1}w_{ij} x_j(t) -x_* -\alpha(t)\left(\nabla f_i({x}_i(t))-\nabla f(x_*)\right)\bigg\|^2.
\end{split}
\end{equation*}
Summing up the above inequality from $i=1$ to $n$, we have
\begin{equation}\label{relation}
\sum^{n}_{i=1}\| x_i(t+1) - x_* \|^2  \leq \sum^{n}_{i=1}\bigg\|\sum^n_{j=1}w_{ij} x_j(t) -x_*-\alpha(t)\left(\nabla f_i({x}_i(t))-\nabla f(x_*)\right) \bigg\|^2.
\end{equation}
We find the following identity of the right hand side of \eqref{relation}:
\begin{equation}\label{relation2}
\begin{split}
 &\quad \sum^{n}_{i=1}\bigg\|\sum^n_{j=1}w_{ij} x_j(t) -x_*-\alpha(t) \left(\nabla f_i({x}_i(t))-\nabla f(x_*)\right) \bigg\|^2\\
 &= \sum^{n}_{i=1} \bigg\|\bar{x}(t)-x_* -\frac{\alpha(t)}{n}\sum^n_{l=1} \left(\nabla f_l({x}_l(t)) -  \nabla f(x_*)\right) 
\\
 &\quad\quad + \sum^n_{j=1}w_{ij}\Big(x_j(t) - \bar{x}(t)\Big) -\alpha(t)\Big(\nabla f_i({x}_i(t))- \frac{1}{n}\sum^n_{l=1}\nabla f_l({x}_l(t))\Big) \bigg\|^2\\
&= \sum^{n}_{i=1} \left\|\bar{x}(t)-x_* -\frac{\alpha(t)}{n}\sum^n_{l=1} \left(\nabla f_l({x}_l(t)) -  \nabla f(x_*)\right) \right\|^2 \\  
&\quad + \sum^{n}_{i=1}\bigg\|  \sum^n_{j=1}w_{ij}\Big(x_j(t) - \bar{x}(t)\Big) -\alpha(t)\Big(\nabla f_i({x}_i(t))- \frac{1}{n}\sum^n_{l=1}\nabla f_l({x}_l(t))\Big) \bigg)\bigg\|^2.
\end{split}
\end{equation} 
Here we used
\begin{equation*}
\sum^n_{i=1}\left(\sum^n_{j=1}w_{ij}x_j(t) - \bar{x}(t) -\alpha(t)\Big(\nabla f_i({x}_i(t))- \frac{1}{n}\sum^n_{l=1}\nabla f_l({x}_l(t))\Big) \right) =0 
\end{equation*}
for the second equality. 

Now we estimate the second term of the right hand side of the last equality in \eqref{relation2}. %Note that
%\begin{equation}
%\begin{split}
%&\| \mathbf{x}(t+1) -\bar{\mathbf{x}}(t+1)\|^2\\
%&=\sum^{n}_{i=1}\bigg\|  \sum^n_{j=1}w_{ij}\Big(x_j(t) - \bar{x}(t)\Big) -\alpha(t)\Big(\nabla f_i({x}_i(t))- \frac{1}{n}\sum^n_{l=1}\nabla f_l(x_l(t))\Big)\bigg\|^2.
%\end{split}
%\end{equation}
By the proof of Proposition \ref{lem-3-1}, we have
\begin{align*}
&\sum^{n}_{i=1}\bigg\|  \sum^n_{j=1}w_{ij}\Big(x_j(t) - \bar{x}(t)\Big) -\alpha(t)\Big(\nabla f_i({x}_i(t))- \frac{1}{n}\sum^n_{l=1}\nabla f_l(x_l(t))\Big)\bigg\|^2\\
&\leq (c_1 \alpha (t)^2 + \tilde{\beta})\| \mathbf{x}(t) - \bar{\mathbf{x}}(t)\|^2 +c_1 \alpha (t)^2 \|\bar{\mathbf{x}}(t)-\mathbf{x}_*\|^2 + c_2 \alpha(t)^2.
\end{align*}
By Lemma \ref{lem-3-8}, the first term of the right hand side of the last equality in \eqref{relation2} is bouned by
$$
\bigg(1- \frac{\mu \alpha (t)}{2}\bigg)\|\bar{\mathbf{x}}(t) - \mathbf{x}_* \|^2 + \bigg(L^2\alpha(t)^2+\frac{4L^2\alpha(t)}{\mu}\bigg) \|\bar{\mathbf{x}}(t) - \mathbf{x}(t)\|^2.
$$
%where
%\begin{equation}
%a_1 (t) =(c_3\alpha(t)^2 + c_4\alpha(t) + \tilde{\beta}) \quad\textrm{and}\quad b_1 (t)= \bigg(1-\frac{\mu}{2}\alpha(t) + c_1\alpha(t)^2\bigg). 
%\end{equation}
Putting the above two estimates in the last term of \eqref{relation2}, we get
\begin{align*}
\sum^{n}_{i=1}\| x_i(t+1) - x_* \|^2&= \|\mathbf{x}(t+1) -\bar{\mathbf{x}}(t+1)\|^2 + \|\bar{\mathbf{x}}(t+1) - \mathbf{x}_*\|^2.\\
& \leq\bigg(\Big( c_1 + L^2\Big) \alpha (t)^2 +\frac{4L^2\alpha(t)}{\mu} + \tilde{\beta}\bigg) \| \mathbf{x}(t) - \bar{\mathbf{x}}(t)\|^2
\\
&\qquad \qquad + \bigg(1- \frac{\mu \alpha (t)}{2} + c_1 \alpha(t)^2\bigg)\|\bar{\mathbf{x}}(t)-\mathbf{x}_*\|^2 + c_2 \alpha(t)^2.
\end{align*}
The proof is done.
\end{proof}

\section{Uniform boundedness of the sequence}\label{sec5}
In this section, we prove the uniform boundedness of the sequence $\{x_i (t)\}_{t \geq 0}$ stated in Theorem \ref{thm-2-11}. We note that the uniform boundedness of the sequence is trivial for the first case where $\Omega$ is assumed to be bounded. Next we prove the theorem for the second case. 
\begin{proof}[Proof of Theorem \ref{thm-2-11} for case 2]
Consider the following functional $E_{\alpha}:(\mathbb{R}^{d})^n \rightarrow \mathbb{R}$ defined as
\begin{equation*}
E_{\alpha} (x) = \frac{1}{2} \Big( \sum_{k=1}^n \|x_k\|^2 - \sum_{k=1}^n \sum_{j=1}^n w_{kj} \langle x_k, x_j \rangle \Big) + \alpha \sum_{k=1}^n f_k (x_k).
\end{equation*}
Then 
\begin{equation*}
x(t+1) = P_{\Omega^n} \Big(x(t) -\nabla E_{\alpha}(x(t))\Big).
\end{equation*}
The function $E_{\alpha}$ is convex and smooth with constant $1-\lambda_n (W) + \alpha L$, where $\lambda_n (W)$ is the smallest eigenvalue of $W$ (refer to \cite{YLY}). Then, we may use the general result for the projected gradient descent (see e.g., \cite{B}) to conclude that
 the sequence $\{x(t)\}_{t \geq 0}$ is uniformly bounded if
\begin{equation*}
1 \leq \frac{2}{1- \lambda_n (W) +\alpha L},
\end{equation*}
which is equivalent to $\alpha \leq \frac{1+\lambda_n (W)}{L}$.
\end{proof}

To hanlde the third case of Theorem \ref{thm-2-11}, we set
\begin{equation}\label{eq5-4}
A(t) = \|\mathbf{x}(t) -\bar{\mathbf{x}}(t)\|^2,\quad  B(t) =\|\bar{\mathbf{x}}(t) - \mathbf{x}_*\|^2,   \quad C(t)=\|\mathbf{x}(t) - \mathbf{x}_*\|^2.
\end{equation}
Using \eqref{eq2-8}, it follows that $C(t) = A(t) + B(t)$. In the following lemma, we find a sequential inequality fo the sequence $\{C(t)\}_{t \in \mathbb{N}_0}$ and find the uniform boundedness of $\{C(t)\}_{t\geq0}$, which also implies the uniform boundedness of $\{A(t)\}_{t\geq0}$ and $\{B(t)\}_{t\geq0}$. It contains the proof of Theorem \ref{thm-2-11} for case 3.
\begin{lem}\label{lem-3-3}
Suppose that Assumptions \ref{LS}-\ref{ass-1-1} hold and the stepsize $\{\alpha(t)\}_{t\geq0}$ is nonincreasing and satisfies
\begin{equation}\label{eq5-7}
\alpha(0) \leq \min\bigg\{Z, \frac{\mu}{4c_1}, \frac{2}{L+\mu}\bigg\}
\end{equation}
where
\begin{equation*}
 Z := \frac{1}{2c_3} \left[ - \Big(c_4 + \frac{\mu}{4}\Big) + \sqrt{\Big(c_4 +\frac{\mu}{4}\Big)^2 + 4c_3  (1-\tilde{\beta})}\right].
\end{equation*} 
Suppose also that $\tilde{\beta}:= (1+\delta) \beta^2 <1$. Then the sequence $\{x_i(t)\}_{t\in\mathbb{N}_0}$ generated by \eqref{scheme} satisfies the following statements. 
\begin{enumerate}
\item We have
\begin{equation}\label{4-11}
C(t+1) \leq \Big(1-\frac{\mu}{4}\alpha(t)\Big)C(t) + c_2\alpha(t)^2.
\end{equation}
\item There exists $R>0$ such that
\begin{equation*}
C(t) \leq R, \ \text{for all $t\in\mathbb{N}$.} 
\end{equation*}
In fact, we may set $R = \max\left\{\frac{4c_2 \alpha (0)}{\mu}, C(0)\right\}$
\end{enumerate} 

\end{lem}
\begin{proof}
We first prove \eqref{4-11}. By Proposition \ref{prop-3-2}, we have the following estimate
\begin{equation*}\label{eq5-10}
C(t) \leq (c_3\alpha(t)^2 + c_4\alpha(t) + \tilde{\beta})A(t)+\bigg(1-\frac{\mu}{2}\alpha(t) + c_1\alpha(t)^2\bigg) B(t) + c_2\alpha(t)^2.
\end{equation*}
Note that since $\alpha(0)  \leq \frac{\mu}{4c_1}$ by \eqref{eq5-7}, it follows that $\mu/2 -c_1\alpha(0)\geq \frac{\mu}{4}$.
Suppose that the following inequality hold.
\begin{equation}\label{eq-4-1}
1-\frac{\mu}{2}\alpha(t) + c_1\alpha(t)^2 < 1-\frac{\mu}{4}\alpha(t),
\end{equation}
\begin{equation}\label{eq-4-2}
c_3\alpha(t)^2 + c_4\alpha(t) +\tilde{\beta} < 1- \frac{\mu}{4}\alpha(t).
\end{equation}
Then we obtain \eqref{4-11} as follows.
\begin{equation*}\label{4-9}
\begin{split}
C(t+1) &\leq (c_3\alpha(t)^2 + c_4 \alpha (t) +\tilde{\beta})A(t) + \bigg(1-\frac{\mu}{2}\alpha(t) + c_1\alpha(t)^2\bigg) B(t) + c_2\alpha(t)^2\\
&\leq  \bigg(1-\frac{\mu}{4} \alpha (t)\bigg)C(t) + c_2\alpha(t)^2.
\end{split}
\end{equation*}
Now we show that \eqref{eq-4-1} and \eqref{eq-4-2} hold under the assumption \eqref{eq5-7}.
% We first choose $\delta>0$ such that $\tilde{\beta} = (1+\delta)\beta^2 <1$. 
Since $\{ \alpha(t)\}_{t\geq0}$ is diminishing sequence and $\alpha (0) \leq \frac{\mu}{4c_1}$, we obtain \eqref{eq-4-1} as follows:
\begin{align*}
1-\frac{\mu}{2}\alpha(t) + c_1\alpha(t)^2 &= 1 - \left(\frac{\mu}{2}-c_1\alpha(t)\right)\alpha(t)\leq 1 - \frac{\mu}{4}\alpha(t).
\end{align*}
To obtain \eqref{eq-4-2}, we note that \eqref{eq-4-2} is equivalent to 
\begin{equation}\label{eq5-14}
c_3 \alpha(t)^2 + \Big(c_4 + \frac{\mu}{4}\Big)\alpha(t) + \tilde{\beta} -1 \leq 0.
\end{equation}
Therefore, the following inequality is a sufficient condition for \eqref{eq5-14}:
\begin{equation*}
\alpha(t) \leq \alpha (0) \leq Z:= \frac{1}{2c_3} \left[ - \Big(c_4 +\frac{\mu}{4}\Big) + \sqrt{\Big(c_4 +\frac{\mu}{4}\Big)^2 + 4c_3 (1-\tilde{\beta})}\right].
\end{equation*}
Since $\tilde{\beta}<1$, we have $Z>0$.
This proves the first estimate of the lemma. 

In order to show the second estimate, we argue by induction. Fix a value $R>0$ and assume that $C(t) \leq R$ for some $t \in \mathbb{N}_0$. Then, it follows from \eqref{4-11} that
\begin{equation*}
\begin{split}
C(t+1)&\leq (1-\frac{\mu}{4}\alpha(t)) R +c_2\alpha(t)^2= R - (\frac{\mu}{4}R -c_2\alpha(t))\alpha(t).
\end{split}
\end{equation*}
If we set
$$
R = \frac{4c_2\alpha(0)}{ \mu },
$$
then we have 
$$
\frac{\mu}{4}R -c_2\alpha(t)\geq \frac{\mu}{4}R - c_2\alpha(0) = 0.
$$ 
This implies $C(t+1)\leq R$. Therefore we have $C(t) \leq R$ for any $t \geq 0$.
The proof is done.
\end{proof}

\section{Consensus estimates}\label{sec6}
%In section \ref{sec4}, we obtain the sequential estimates of $\|\mathbf{x}(t)-\mathbf{\bar{x}}(t)\|^2$ and $\|\mathbf{x}(t)-\mathbf{\bar{x}}(t)\|^2 + \|\mathbf{\bar{x}}(t)-\mathbf{x_*}\|^2$. 
In this section, we establish the consensus result of Theorem \ref{thm2-5} by using Proposition \ref{lem-3-1} and \ref{prop-3-2} in Section \ref{sec4}. 

\begin{comment}
\begin{lem}\label{lem-4-1}
Suppose that $\{\alpha (t)\}_{t\in \mathbb{N}_0}$ is a positive nondecreasing sequence and  the sequence $\{H(t)\}_{t\in\mathbb{N}_0}$ satisfies
\begin{equation}\label{eq-4-20}
H(t+1) \leq \beta H(t) + d\alpha(t)^2
\end{equation}
for some $d>0$ and $\beta\in (0,1)$.
Then we have
\begin{equation}\label{4-1}
H(t) \leq \beta^t H(0) + \frac{d\alpha(0)^2 \beta^{t/2}}{1-\beta} + \frac{d\alpha ([t/2])^2}{1-\beta}.
\end{equation}
\end{lem}
\begin{proof}
Since the inequality \eqref{4-1} holds trivially for $t \in \{0,1\}$, we consider $t \geq 2$. By expanding \eqref{eq-4-20}, for $t\geq 2$ we have
\begin{equation}
H(t) \leq \beta^t H(0) + d \left[ \sum_{s=0}^{t-1} \beta^{t-s-1}\alpha (s)^2\right].
\end{equation}
We split the right hand side to estimate it as
\begin{equation}
\begin{split}
H(t) &\leq \beta^t H(0) + d \sum_{s=0}^{[t/2]-1} \beta^{t-1-s}\alpha (s)^2 + d \sum_{s=[t/2]}^{t-1} \beta^{t-1-s}\alpha (s)^2
\\
&\leq \beta^t H(0) + \frac{d\alpha(0)^2 \beta^{t/2}}{1-\beta} + \frac{d\alpha ([t/2])^2}{1-\beta}.
\end{split}
\end{equation}
The proof is done. 
\end{proof} 
\end{comment}
\begin{comment}
\begin{rem}
We remark that $\mathcal{R}(t)$ is bounded as follows.
Note that
\begin{equation}
\begin{split}
\sum_{s=0}^{t-1} \frac{C_1}{(s+w)^{p}} & \geq \int_0^{t} \frac{C_1}{(s+w)^p} ds
\\
& = \frac{C_1}{1-p} \Big[ (t+w)^{1-p} -w^{1-p}\Big].
\end{split}
\end{equation}
Using this we get
\begin{equation}\label{eq-5-1}
e^{-\sum_{s=0}^{t-1} \frac{C_1}{(s+w)^p}} A(0) \leq A(0) e^{\frac{C_1}{1-p}w^{1-p}} e^{-\frac{C_1}{1-p}(t+w)^{1-p}}.
\end{equation}
Next, we observe that for $a>b$ we have
\begin{equation}
a^{1-p} -b^{1-p} = (1-p) s^{-p} (a-b) \geq (1-p)\frac{a-b}{b^p}
\end{equation}
where $s \in (a,b)$. Using this, 
\begin{equation}
\Big[ (t+w)^{1-p} -  ([t/2]-1 +w)^{1-p}\Big] \geq \frac{(1-p)}{(t+w)^p} (t+1 - [t/2]) \geq \frac{(1-p)}{2} \frac{t}{(t+w)^p}.
\end{equation}
This enables us to estimate
\begin{equation}
-\frac{C_1}{1-p}[(t+w )^{1-p} - ([t/2]-1+w)^{1-p}] \leq - \frac{C_1 t}{2 (t+w)^p}.
\end{equation}
Therefore
\begin{equation}
QC_2 e^{-\frac{C_1}{1-p}[(t+w )^{1-p} - ([t/2]-1+w)^{1-p}]} \sum_{s=1}^{[t/2]-1} \frac{1}{(w+s)^{p+q}}  \leq QC_2 [t/2] e^{-\frac{C_1 t}{2(t+w)^p}}. 
\end{equation}
Combining this with \eqref{eq-5-1} we get
\begin{equation}
\mathcal{R}(t) \leq A(0) e^{\frac{C_1}{1-p}w^{1-p}} e^{-\frac{C_1}{1-p}(t+w)^{1-p}} +QC_2 [t/2] e^{-\frac{C_1 t}{2(t+w)^p}}. 
\end{equation}
This yields that $\mathcal{R}(t) = O(t^{-N})$ for any fixed $N \in \mathbb{N}$.
\end{rem}
\end{comment}
 
%Then, by Proposition \ref{prop-3-2}
%\begin{align}\label{estimate1}
%A(t+1) + B(t+1)&\leq\bigg( (1+\frac{1}{\eta})\frac{L^2\alpha(t)^2}{n} + (1+\delta)\beta^2 + 12L(1 + \frac{1}{\delta})\alpha(t)^2\bigg) A(t)\\
%&\quad+\bigg((1+\eta)\bigg(1-\frac{\mu\alpha(t)}{2} \bigg)^2 +12L(1 + \frac{1}{\delta})\alpha(t)^2 \bigg)B(t)\\
%&\quad+ 12nD^2(1 + \frac{1}{\delta})\alpha(t)^2.
%\end{align}
%Note that by choosing $\eta = \frac{\mu \alpha(t)}{4}$, we obtain
%\begin{equation}
%\begin{split}
%(1+\eta)\bigg(1-\frac{\mu\alpha(t)}{2} \bigg)^2 & = \Big( 1+ \frac{\mu \alpha (t)}{4}\Big) \Big( 1- \mu \alpha (t) + \frac{\mu^2 \alpha (t)^2}{4}\Big)
%\\
%& = 1-\frac{3}{4} \mu \alpha (t) + \frac{\mu^3}{16}\alpha (t)^3
%\\
%& = 1- \frac{1}{2} \mu \alpha (t) - \Big( \frac{\mu \alpha (t)}{4} - \frac{\mu^3 \alpha (t)^3}{16}\Big)
%\\
%&\leq 1- \frac{\mu \alpha (t)}{2},
%\end{split}
%\end{equation}
%provided that $\alpha (t)$ satisfies $\alpha (t) \leq \frac{2}{\mu}$. Then we have  
%\begin{align}
%A(t+1) + B(t+1) &\leq \bigg(\bigg(\frac{L^2}{n}+CL\bigg)\alpha(t)^2 + \frac{4L^2}{\mu n}\alpha(t) +(1+\delta) \beta^2 \bigg)A(t)\\
%&+ \Bigg[\bigg(1-\frac{\mu\alpha(t)}{2}\bigg) + CL\alpha(t)^2 \Bigg]B(t) + CnD^2\alpha(t)^2,
%\end{align}
%where $C=12(1+\frac{1}{\delta}).$ Now we want to show $A(t+1) + B(t+1) \leq R$ under the assumption, $A(t) + B(t) \leq R$.
%So we neet to obtain the following estimate.
%\begin{align}
%A(t+1) + B(t+1) &\leq \bigg(\bigg[\bigg(\frac{L^2}{n}+CL\bigg)\alpha(t) + \frac{4L^2}{\mu n}\bigg]\alpha(t) +(1+\delta) \beta^2 \bigg) A(t)\\
%&+ \Big(1-\frac{\mu\alpha(t)}{2}+ CL\alpha(t)^2\Big) B(t) + CnD^2\alpha(t)^2 \leq R
%\end{align}
%To obtain the above inequality, we consider the following esitmates
%\begin{equation}
%\bigg(\frac{L^2}{n}+CL\bigg)\alpha(t)^2 + \frac{4L^2}{\mu n} \alpha(t) + \beta + \alpha(t)^2 \leq 1
%\end{equation}
%and
%\begin{equation}
%1-\frac{\mu\alpha(t)}{4} + CL\alpha(t)^2 +\alpha(t)^2 \leq 1.
%\end{equation}
%We choose $\delta >0$ such that $(1+\delta)\beta^2 <1$. Next we choose $\alpha (t) \leq \alpha_0$ with $\alpha_0 >0$ satisfying
%\begin{equation}
%\Big( \frac{L^2}{n} + CL\Big)\alpha_0^2 + \frac{4L^2}{\mu n} \alpha_0 + (1+\delta) \beta^2 <1 \quad \textrm{and}\quad 
%q  := \frac{\mu}{2}- CL\alpha_0 >0.
%\end{equation}

\begin{proof}[Proof of Theorem \ref{thm2-5}]
Let $\delta = \frac{1}{\beta}-1$ so that $\tilde{\beta} = \beta<1$. Then, the estimate of Proposition \ref{lem-3-1} reads as
\begin{equation*}
\begin{split}
\| \mathbf{x}(t+1) -\bar{\mathbf{x}}(t+1)\|^2 &\leq (\tilde{c}_1\alpha(t)^2 + \beta)\| \mathbf{x}(t) - \bar{\mathbf{x}}(t)\|^2 + \tilde{c}_1\alpha(t)^2 \|\bar{\mathbf{x}}(t)-\mathbf{x}_*\|^2 + \tilde{c}_2\alpha(t)^2.
\end{split}
\end{equation*} 
Since $\| \mathbf{x}(t) -\bar{\mathbf{x}}(t)\|^2 <R$, it follows that
\begin{equation}\label{eq6-33}
\begin{split}
\| \mathbf{x}(t+1) -\bar{\mathbf{x}}(t+1)\|^2&\leq \beta\| \mathbf{x}(t) - \bar{\mathbf{x}}(t)\|^2 + \frac{3\alpha(t)^2}{1-\beta}(L^2R^2+nD^2)\\
&\leq \beta^{t+1}\|\mathbf{x}(0)-\bar{\mathbf{x}}(0)\|^2 + \frac{3}{1-\beta}(L^2R^2+nD^2)\sum^t_{s=0}\alpha(s)^2\beta^{t-s}.
\end{split}
\end{equation}
For a decreasing stepsize, we have
\begin{equation}\label{eq6-4}
\begin{split}
\sum^{t}_{s=0}\alpha(s)^2\beta^{t-s} &= \sum^{[t/2]-1}_{s=0}\alpha(s)^2\beta^{t-s} + \sum^{t}_{s=[t/2]}\alpha(s)^2\beta^{t-s}  \\
&\leq \alpha(0)^2\frac{\beta^t}{1-\beta} + \alpha([t/2])^2\frac{1}{1-\beta}.
\end{split}
\end{equation}
Inserting this inequality to \eqref{eq6-33}, we have
\begin{equation*}
\|\mathbf{x}(t+1)-\bar{\mathbf{x}}(t+1)\|^2 \leq \beta^{t+1}\|\mathbf{x}(0)-\bar{\mathbf{x}}(0)\|^2  + \frac{J\alpha(t)^2}{(1-\beta)^2},
\end{equation*}
where
\begin{equation*}
J=3(L^2R^2 + nD^2)\cdot\sup_{s\geq0}\frac{\alpha(0)^2\beta^s + \alpha([s/2])^2}{\alpha(s)^2}.
\end{equation*}
For a constant stepszie $\alpha(t)\equiv\alpha$, we can estimate \eqref{eq6-33} as
\begin{equation*}\label{eq6-3}
\begin{split}
\| \mathbf{x}(t+1) -\bar{\mathbf{x}}(t+1)\|^2
&\leq \beta^{t+1}\|\mathbf{x}(0)-\bar{\mathbf{x}}(0)\|^2 + \frac{3\alpha^2}{1-\beta}(L^2R^2+nD^2)\sum^t_{s=0}\beta^{t-s}\\
&\leq \beta^{t+1}\|\mathbf{x}(0)-\bar{\mathbf{x}}(0)\|^2 + \frac{3\alpha^2}{(1-\beta)^2}(L^2R^2+nD^2) \\
&=  \beta^{t+1}\|\mathbf{x}(0)-\bar{\mathbf{x}}(0)\|^2 + \frac{J\alpha^2}{(1-\beta)^2},
\end{split}
\end{equation*}
where $J=3(L^2R^2+nD^2)$.
 The proof is done.
\end{proof}



\section{Convergence results}\label{sec7}
In this section, we prove the convergence results, Theorems \ref{thm2-3}, \ref{thm2-6} and \ref{thm2-7} using the estimates in Section \ref{sec4} and the following lemma.
\begin{lem}\label{lem-4-3} 
 Let $ p \in (0,1]$ and $q>0$. Take $C_1 >0$ and $w \geq 1$ such that $C_1/w^p <1.$ 
Suppose that the sequence $\{H(t)\}_{t\geq0}$ satisfies
\begin{equation*}\label{4-4}
H(t) \leq \bigg(1-\frac{C_1}{(t+w-1)^p}\bigg) H(t-1) + \frac{C_2}{(t+w-1)^{p+q}} \quad \text{for all $t\geq1$}.
\end{equation*}
Set $Q = \Big( \frac{w+1}{w}\Big)^{p+q}$. Then $H(t)$ satisfies the following bound.

\medskip 

\noindent Case 1. If $p<1$, then we have
\begin{equation*}
 H(t) \leq   \delta \cdot([t/2]+w-1)^{-q} + \mathcal{R}(t),
\end{equation*}
where  $\delta =  \frac{QC_2}{C_1} e^{\frac{C_1}{w^p}}$ and
%\begin{equation*}
%\mathcal{R}(t) =e^{-\sum^{t-1}_{s=0}\frac{C_1}{(s+w)^p}}A(0)  + QC_2 e^{-\frac{C_1}{1-p}[(t+w )^{1-p} - ([t/2]-1+w)^{1-p}]} \sum_{s=1}^{[t/2]-1} \frac{1}{(w+s)^{p+q}},
%\end{equation*}
\begin{equation*}
\mathcal{R}(t) =e^{-\sum^{t-1}_{s=0}\frac{C_1}{(s+w)^p}}H(0)  + QC_2e^{- \frac{C_1t}{2(t+w)^p}}  \sum_{s=1}^{[t/2]-1} \frac{1}{(s+w)^{p+q}}.
\end{equation*}
Here the second term on the right hand side is assumed to be zero for $1\leq t \leq 3$.
%We also have 
%$$
%\mathcal{R}(t) = O\big(\min\{(C_1^{\frac{1}{1-p}}t)^{-N}, 1)\}\big)\ \text{for any fixed $N \in \mathbb{R}^N$}.
%$$
\medskip

\noindent Case 2. If $p=1$, then we have
\begin{equation*}
 H(t) \leq \Big( \frac{w}{t+w}\Big)^{C_1} H(0) + \mathcal{R} (t),
\end{equation*}
where
\begin{equation*}
  \mathcal{R}(t)   = \left\{\begin{array}{ll}  \frac{w^{C_1 -q}}{q-C_1}\cdot\frac{QC_2}{(t+w)^{C_1}}& \textrm{if}~q>C_1
\\
\log \left(\frac{t+w}{w}\right)\cdot\frac{QC_2}{(t+w)^{C_1}} &\textrm{if}~ q=C_1
 \\
\frac{1}{C_1-q}\cdot\left( \frac{w+1}{w}\right)^{C_1}\cdot \frac{QC_2}{(t+w+1)^q}& \textrm{if}~ q<C_1.
\end{array}\right.
\end{equation*} 
\end{lem}

\begin{proof}[Proof of Theorems \ref{thm2-3}, \ref{thm2-6} and \ref{thm2-7}]
Using the notation \eqref{eq5-4}, we can rewrite Theorem \ref{thm2-5} and Proposition \ref{prop-3-2} as
\begin{equation}\label{eq7-2}
A(t)\leq \beta^t A(0) + \frac{J\alpha (t)^2}{(1-\beta)^2}
\end{equation}
and
\begin{equation}\label{eq7-3}
\begin{split}
&A(t+1)+B(t+1)\\
&\leq (c_3\alpha(t)^2 + c_4\alpha(t) + \tilde{\beta}) A(t) +\bigg(1-\frac{\mu}{2}\alpha(t) + c_1\alpha(t)^2\bigg) B(t) + c_2\alpha(t)^2.
\end{split}
\end{equation}
Combining \eqref{eq7-2} and \eqref{eq7-3}, we get
\begin{equation*}
\begin{split}
B(t+1)&\leq A(t+1) + B(t+1) \\
&\leq \left(1-\frac{\mu}{2}\alpha(t)\right)B(t) +  (c_3\alpha(t)^2 + c_4\alpha(t) + \tilde{\beta})A(0)\beta^t \\
&\quad + \left((c_3\alpha(t)^2 + c_4\alpha(t) + \tilde{\beta})\frac{J}{(1-\beta)^2} + c_1B(t) + c_2\right)\alpha(t)^2.
\end{split}
\end{equation*}
Since $A(t)+B(t)<R$ and $\alpha(t)\leq \alpha(0)$, it follows that
\begin{equation}\label{eq-7-20}
B(t+1) \leq \left(1-\frac{\mu}{2}\alpha(t)\right) B(t) + G_1\beta^t + G_2\alpha(t)^2,
\end{equation}
where
\begin{equation*}
\begin{split}
G_1 &= (c_3\alpha(0)^2 + c_4\alpha(0) + \beta)R, \\
G_2 &= (c_3\alpha(t)^2 + c_4\alpha(t) + \tilde{\beta})\frac{J}{(1-\beta)^2} + c_1R + c_2.
\end{split}
\end{equation*}

\noindent  \textbf{Case $\alpha(t)\equiv\alpha$.} To estimate the sequence $B(t)$  we consider the following two sequences $\{q_1 (t)\}_{t\geq0}$ and $\{q_2(t)\}_{t\geq0}$ satisfying
\begin{equation}
\begin{split}
q_1 (t+1)& = \Big( 1- \frac{\mu}{2}\alpha \Big) q_1 (t) + G_1 \beta^{t},\quad q_1 (0) =0,
\\
q_2(t+1)& = \Big( 1- \frac{\mu}{2}\alpha \Big) q_2(t) + G_2 \alpha^2,\quad q_2(0) = B(0).
\end{split}
\end{equation}
It then easily follows that $B(t) \leq q_1 (t) + q_2 (t)$ for all $t \geq 0$. 
Similarly as in \eqref{eq6-4}, we have
\begin{equation*}
\begin{split}
q_1 (t)&= \sum_{s=0}^{t-1} G_1 \beta^{s} \Big( 1- \frac{\mu \alpha}{2}\Big)^{t-1-s} \leq \frac{2}{\mu\alpha}\left(\left(1-\frac{\mu\alpha}{2}\right)^{t-1} + \beta^{\frac{t-1}{2}}\right)
\end{split}
\end{equation*}
Note that
\begin{equation*}
\sum^{t-1}_{s=0} \left(1-\frac{\mu\alpha}{2}\right)^{t-1-s} \leq \frac{2}{\mu\alpha}.
\end{equation*} 
Using this, we estimate $q_2(t)$ as 
\begin{equation*}
q_2(t) \leq \Big(1 - \frac{\mu \alpha}{2}\Big)^{t} q_2(0) + \frac{2 G_2}{\mu}\alpha.
\end{equation*}
Combining the above estimates, we obtain
\begin{equation*}
B(t) \leq \Big(1 - \frac{\mu \alpha}{2}\Big)^{t} B(0) + \frac{2 G_2}{\mu}\alpha+ \frac{2}{\mu\alpha}\left(\left(1-\frac{\mu\alpha}{2}\right)^{t-1} + \beta^{\frac{t-1}{2}}\right)
\end{equation*}

We next consider decreasing stepsize $\alpha (t) = \frac{v}{(t+w)^p}$, where $0<p\leq1$. 
For decreasing stepsize, we set
\begin{equation*}
\rho := \sup_{t \geq 0} \frac{\beta^{t}}{\alpha (t)^2}.
\end{equation*}
Then it follows that
\begin{equation*}
B(t+1) \leq \Big( 1- \frac{\mu}{2}\alpha (t)\Big) B(t) + \Big( \rho G_1 + G_2\Big) \alpha (t)^2,
\end{equation*}
which verifies the result of Theorem \ref{thm2-3}.
\medskip 


\noindent \textbf{Case $\alpha (t) = \frac{v}{(t+w)^p}$.}  The estimate \eqref{eq-7-20} reads as
\begin{equation*}
B(t+1) \leq \Big( 1- \frac{\mu v}{2(t+w)^p}\Big) B(t) + \Big( \rho G_1 + G_2\Big)\frac{  v^2}{(t+w)^{2p}}.
\end{equation*}
By applying Lemma \ref{lem-4-3}, we have
\begin{equation*}
B(t) \leq \frac{4Q \Big( \rho G_1 + G_2\Big)  v}{\mu} ([t/2]+w-1)^{-p} + \mathcal{R}_1(t) + \mathcal{R}_2(t),
\end{equation*}
where 
\begin{equation*}
\begin{split}
\mathcal{R}_1(t) &= e^{-\sum^{t-1}_{s=0}\frac{\mu v}{2(s+w)^p}}B(0)\\
\mathcal{R}_2(t) &= Q\Big( \rho G_1 + G_2\Big) v^2 e^{-\frac{\mu vt}{4(t+w)^p}}\sum^{[t/2]-1}_{s=1}\frac{1}{(s+w)^{2p}}
\end{split}
\end{equation*}
with constant $Q = \Big( \frac{w+1}{w}\Big)^{2p}$. The proof of Theorem \ref{thm2-6} is done.

\medskip 
 \noindent \textbf{Case  $\alpha (t) = \frac{v}{t+w}$.} The estimate \eqref{eq-7-20} gives
\begin{equation*}
B(t+1) \leq \Big( 1-\frac{\mu v}{2 (t+w)}\Big)B(t) +\Big( \rho G_1 + G_2\Big) \frac{v^2}{(t+w)^2}.
\end{equation*}
Choose $v>0$ so that $C_1 = \mu v/2 >1$. Then we use Lemma \ref{lem-4-3} to derive the following estimate 
\begin{equation*}
B(t) \leq \Big( \frac{w}{t+w}\Big)^{C_1} B(0) + \frac{1}{C_1 -1} \Big( \frac{w+1}{w}\Big)^{C_1} \frac{Q\Big( \rho G_1 + G_2\Big)  v^2}{(t+w-1)},
\end{equation*}
where $Q = \Big( \frac{w+1}{w}\Big)^2$. It proves  Theorem \ref{thm2-7}.
\end{proof}
\section{Improved convergence estimate  for one dimensional example}\label{sec9}
In this section, we show that the convergence result of Theorem \ref{thm2-3} can be improved for one dimensional example. Recall the following estimate in Theorem \ref{thm2-3}: 
\begin{equation*}
 \|\bar{\mathbf{x}}(t) - \mathbf{x}_*\|^2 \leq  \Big(1 - \frac{\mu \alpha}{2}\Big)^{t} \|\bar{\mathbf{x}}(0) - \mathbf{x}_*\|^2+ \frac{2 G_2}{\mu}\alpha +\frac{2}{\mu\alpha}\left(\left(1-\frac{\mu\alpha}{2}\right)^{t-1} + \beta^{\frac{t-1}{2}}\right).
\end{equation*}
This implies that the sequence $\{\bar{x}(t)\}_{t\geq0}$ converges to an $O(\sqrt{\alpha})$-neighborhood of the optimal point. However, if $\Omega = \mathbb{R}^n$, it is known by the work  \cite{YLY} that the sequence converges to an $O(\alpha)$-neighborhood of the optimal point. Hence, it is natural to ask the following question: While our approach for Theorem \ref{thm2-3} has led to convergence to an $O(\sqrt{\alpha})$-neighborhood, can we improve empirical performance up to an $O(\alpha)$-neighborhood? We answer this question by constructing a specific example. Let us consider the functions $g_1, g_2 : [0,\infty)\rightarrow \mathbb{R}$ defined by
\begin{equation}\label{eq8-2}
g_1 (x) = 5x^2 \quad \textrm{and}\quad g_2 (x) = -3x^2, \quad x\in [1,\infty]
\end{equation}
and the mixing matrix $\tilde{W}$ defined by
\begin{equation}\label{eq8-3}
\tilde{W} = \begin{pmatrix} 2/3 & 1/3 \\ 1/3 & 2/3 \end{pmatrix}
\end{equation}
satisfying Assumption \ref{ass-1-1}. We note that the total cost function $g = (g_1+g_2)/2$ has the optimal point at $x=1$. Then we can represent the projected decentralized gradient descent algorithm with a constant stepsize $\alpha$ explicitly as follows:
\begin{equation}\label{eq8-4}
\begin{split}
x_1(t+1) & =  \max \Big[\frac{2}{3}x_1(t) + \frac{1}{3}x_2(t) - 10\alpha x_1(t),~ 1\Big],
\\
x_2(t+1) & = \max \Big[ \frac{1}{3}x_1(t) + \frac{2}{3}x_2(t) +6\alpha x_2(t),~1\Big].
\end{split}
\end{equation}
We first establish that the state $(x_1(t),x_2(t))$ generated by the algorithm \eqref{eq8-4} will be confined to a certain region after a finite number of iterations. The proof for the following lemma will be provided at the end of this section. 
\begin{lem}\label{lem8-1}
Let $g_1(x)$, $g_2(x)$ and the mixing matrix $\tilde{W}$ be defined by \eqref{eq8-2} and \eqref{eq8-3} and let $\mathbf{x}(t) = (x_1(t), x_2(t))$ be the state at $t\geq0$ generated by \eqref{eq8-4}. Then for any initial state $\mathbf{x}(0) = (x_1(0), x_2(0))$ and $\alpha \in (0,1/45)$, there exists $t_0 \leq \log \|\mathbf{x}(0)\|_2/ \log(1/\lambda_{+})-1$ such that $x_1 (t_0+1) =1$ and $x_2 (t_0 +1) \leq 1 + 30\alpha$, where $\lambda_{+}\in (0,1)$ is defined as
\begin{equation*}
\lambda_{+} = \frac{2}{3} - 2\alpha + \sqrt{\frac{1}{9} +64\alpha^2}. 
\end{equation*}
\end{lem}
Now, we demonstrate that the state $(x_1(t),x_2(t))$ generated by the algorithm \eqref{eq8-4} converges to an $O(\alpha)$ neighborhood of the optimal point $(1,1)$.   
\begin{thm}\label{thm8-1}
Let $g_1(x)$, $g_2(x)$ and the mixing matrix $\tilde{W}$ be defined by \eqref{eq8-2} and \eqref{eq8-3} and let $\mathbf{x}(t) = (x_1(t), x_2(t))$ be the state at $t\geq0$ generated by \eqref{eq8-4}. Then for any initial state $\mathbf{x}(0) = (x_1(0), x_2(0))$ and $\alpha \in (0,1/45)$, the state $\mathbf{x}(t)$ converges exponentially fast to the point $(1, 1/(1-18\alpha))$ which belongs to an $O(\alpha)$ neighborhood of the optimal point $(1,1)$. 
\end{thm}
\begin{proof}
By Lemma \eqref{lem8-1}, we can choose $t_0$ satisfying $x_1 (t_0+1) =1$ and $x_2 (t_0 +1) \leq 1 + 30\alpha$. Note that if $\alpha<1/45$ then $1/(1-18\alpha)<1+30\alpha$. Since $x_1(t_0+1) = 1$ and $x_2(t_0+1)<1+30\alpha$, it follows that
\begin{equation*}
\frac{2}{3} + \frac{1}{3}x_2(t_0+1)-10\alpha<1,
\end{equation*}
which implies $x_1(t_0+2)=1$. We have
\begin{equation}\label{eq8-6}
\begin{split}
x_2(t_0+2) &= \frac{1}{3}x_1(t_0+1) + \frac{2}{3}x_2(t_0+1)+6\alpha x_2(t_0+1)
\\
&= \frac{1}{3}x_1(t_0+1) + \frac{2+18\alpha}{3}x_2(t_0+1).
\end{split}
\end{equation}
We can further write \eqref{eq8-6} as 
\begin{equation}\label{eq8-7}
x_2(t_0+2) -  \frac{1}{1-18\alpha} = \frac{2+18\alpha}{3}\left[x_2(t_0+1)-\frac{1}{1-18\alpha}\right].
\end{equation}
Since $(2+18\alpha)/3<1$, it follows that $x_2(t_0+2)< x_2(t_0+1)<1+30\alpha$ for $x_2(t_0+1)>\frac{1}{1-18\alpha}$, and $1\leq x_2(t_0+2)\leq \frac{1}{1-18\alpha}$ for $1\leq x_2(t_0+1)\leq \frac{1}{1-18\alpha}$. We can conclude that $x_1(t)=1$ and $x_2(t)<1+30\alpha$ for all $t\geq t_0+1$. In addition, \eqref{eq8-7} implies that $x_2(t)$ converges to $1/(1-18\alpha)$. The proof is done.
\end{proof}
The above result will be also verified by numerical test in the next section. This result suggests that the sequence $\{\bar{x}(t)\}$ converges to an $O(\alpha)$-neighborhood of the optimal point which is a stronger  result than the convergence result to an $O(\sqrt{\alpha})$-neighborhood of Theorem \ref{thm2-3}. We guess that the result of Theorem \ref{thm8-1} could be extended to more general examples.%this conjecture that with the constant stepsize, the points generated by the projected decentralized gradient descent converge to an $O(\alpha)$-neighborhood of the optimal solution.% even though our method only obtains an $O(\sqrt{\alpha})$-neighborhood.

Before ending this section, we give a proof of Lemma \ref{lem8-1}.
\begin{proof}[Proof of Lemma \ref{lem8-1}]
Notice that if $x_1 (t+1) >1$ and $x_2 (t+1)>1$, it should hold that
\begin{equation}\label{eq8-9}
\begin{pmatrix} x_1 (t+1) \\ x_2 (t+1) \end{pmatrix} = \begin{pmatrix} \frac{2}{3}-10\alpha & \frac{1}{3} \\ \frac{1}{3} & \frac{2}{3} + 6\alpha \end{pmatrix} \begin{pmatrix} x_1 (t) \\ x_2 (t) \end{pmatrix}.
\end{equation}
The eigenvalues of the matrix in the right hand side of \eqref{eq8-9} are
\begin{equation*}
\lambda_{\pm} = \frac{2}{3} - 2\alpha \pm \sqrt{\frac{1}{9} +64\alpha^2} 
\end{equation*}
which are positive and less than $1$ for $\alpha \in (0, 1/45)$. Therefore
\begin{equation*}
\|(x_1 (t+1), x_2 (t+1))\|_2 \leq \lambda_{+} \|(x_1 (t), x_2 (t))\|_2,
\end{equation*}
and so 
\begin{equation*}
\|(x_1 (t), x_2 (t))\|_2 \leq \lambda_{+}^{t} \|(x_1 (0), x_2 (0))\|_2.
\end{equation*}
Thus we can find a smallest integer $t_0 \leq \log \|\mathbf{x}(0)\|_2/ \log(1/\lambda_{+})-1$ such that $x_1 (t_0+1) =1$. By \eqref{eq8-4}, it follows that
\begin{equation*}\label{eq-9-1}
\frac{2}{3} x_1 (t_0) + \frac{1}{3} x_2 (t_0) - 10 \alpha x_1 (t_0) \leq 1.
\end{equation*}
This leads to
\begin{equation*}
\begin{split}
\frac{1}{3} x_2 (t_0)& \leq 1 - \Big( \frac{2}{3} - 10\alpha \Big) x_1 (t_0) 
\\
&\leq 1- \Big( \frac{2}{3} -10\alpha\Big) = \frac{1}{3} + 10\alpha,
\end{split}
\end{equation*}
which implies that $x_2 (t_0) \leq 1 + 30 \alpha$. Now we want to show that $x_2 (t_0+1) \leq 1+30 \alpha$.
Note that
\begin{equation*}
\begin{split}
\frac{1}{3} x_1 (t_0) + \Big( \frac{2}{3} + 6\alpha\Big) x_2 (t_0) 
&\leq \frac{1}{3} \Big( \frac{3-x_2 (t_0)}{2-30\alpha}\Big) + \Big(\frac{2}{3} + 6\alpha\Big) x_2 (t_0)
\\
& = \frac{1}{2-30\alpha} +\Big( \frac{2}{3} + 6\alpha - \frac{1}{6-90\alpha}\Big) x_2 (t_0). 
\end{split}
\end{equation*}
Here we used $x_1 (t_0) \leq (3-x_2 (t_0))/(2-30\alpha)$
from \eqref{eq-9-1} for the first inequality.
Inserting this into \eqref{eq8-4} we find 
\begin{equation*}
\begin{split}
x_2 (t_0+1)& = \max\left[\frac{1}{3} x_1 (t_0) + \Big( \frac{2}{3} + 6\alpha\Big) x_2 (t_0), 1\right]
\\
& = \max\left[\frac{1}{2-30\alpha} +\Big( \frac{2}{3} + 6\alpha - \frac{1}{6-90\alpha}\Big) x_2 (t_0), 1\right]. 
\end{split}
\end{equation*}
Combining this with $x_2 (t_0) \leq 1+30\alpha$, we get
\begin{equation*}
\begin{split}
x_2 (t_0+1) & \leq \max\left[\frac{1}{2-30\alpha} + \Big( \frac{2}{3} + 6\alpha - \frac{1}{6-90\alpha}\Big) (1+30\alpha),1\right]
\\
& \leq 1+30\alpha.
\end{split}
\end{equation*}
Here the second inequality follows by observing the first component in the max operator equivalent form
\begin{equation*}
1 \leq (1-22\alpha +180\alpha^2) (1+30\alpha).
\end{equation*}
This can be rewritten as 
\begin{equation*}
0 \leq \alpha (1-45\alpha)(1-15\alpha),
\end{equation*}
which holds true for $ \alpha \in (0, 1/45)$.
\end{proof}

 




\section{Simulations}\label{sec8}
In this section, we conduct numerical experiments for the projected distributed gradient descent algorithm \eqref{scheme}  supporting the convergence results of this paper, both for the constant and decreasing stepsize cases. 

\subsection{Regression problem} We consider the following constrained decentralized least squares problem, with $n$ agents
$$
\min_{x\in\Omega} \sum^n_{i=1}\|q_i - p_i^Tx\|^2
$$ 
where $\Omega = \{ x\in\mathbb{R}^d | \|x\|\leq3\}$. Here the variable $p_i\in \mathbb{R}^{d\times p}$  is randomly chosen from the uniform distribution on $[0,1]$ and the variable $q_i\in\mathbb{R}^{p}$ is generated according to the linear regression model $q_i = p_i^T \tilde{x} + \varepsilon_{i}$ where $\tilde{x}\in \mathbb{R}^d$ is the true weight vector and $\varepsilon_i$'s are jointly Gaussian and zero mean. In this case, the projection operator $\mathcal{P}_{\Omega}$ is defined by
\begin{equation*}
\mathcal{P}_{\Omega}[x] =
\left\{\begin{array}{ll}
\frac{3}{\|x\|}x \ \text{if $\|x\|>3$},&\\
x, \ \text{otherwise}.& 
\end{array}\right.
\end{equation*}
We initialized the points $x_i(0)$ as independent random variables generated from a standard Gaussian distribution and then apply the projection operator. In this simulation, we set the problem dimensions and the number of agents as $d= 5$, $p=2$, and $n=50$. We use a connected graph constructed based on the Watts and Strogatz model where each node has four out-neighbors. Then we consider the relative convergence error $R_1(t)$ and the relative consensus error $R_2(t)$ which are defined as follows:
\begin{equation*}
R_1(t) = \frac{\| \bar{x}(t) -x_*\|}{\| \bar{x}(0) -x_*\|}, \quad R_2(t) = \frac{\sum^{n}_{i=1}\|x_i(t)- \bar{x}(t)\|}{\sum^{n}_{i=1}\|x_i(0)- \bar{x}(0)\|}.
\end{equation*}

We first consider the following constant stepsizes:
\begin{equation*}
\alpha_1(t) \equiv \frac{4}{\mu+L}, \ \alpha_2(t) \equiv \frac{3}{\mu+L}, \ \alpha_3(t) \equiv\frac{1}{\mu+L},\ \alpha_4(t) \equiv \frac{1}{2(\mu+L)}. 
\end{equation*}
We measure the relative convergence error $R_1(t)$ and the relative convergence error $R_2(t)$ for each constant stepsize and presented in Figure \ref{figure1}.
\begin{figure}[!htbp]
\centering
\includegraphics[width=7.4cm]{test_for_convergence_constant.png}
\includegraphics[width=7.4cm]{test_for_consensus_constant.png}
\caption{The left figure corresponds to the value $R_1(t)$ and the right figure corresponds to the value $R_2(t)$.}
\label{figure1}
\end{figure}
From Figure \ref{figure1}, we observe that the black and blue lines in the plot represent cases where the conditions in Theorems \ref{thm2-5} and \ref{thm2-3} are violated, while the other lines satisfy the conditions. Although both the black and blue lines do not converge to zero, they still appear to be bounded and oscillate within a certain range. On the other hand, for the other lines, both $R_1(t)$ and $R_2(t)$ properly converge to a small neighborhood of zero.

Next, we consider the decreasing stepsize $\alpha(t)=0.5/(t+w)^p$ with $w$ satisfying $w^{p} = L+\mu$ for $p\in\{0.25,0.5,0.75,1\}$. The graphs of $R_1(t)$ and $R_2(t)$ are presented in \mbox{Figure \ref{figure2}.}  The numerical result shows that the values of $R_1(t)$ and $R_2 (t)$ converge to zero as expected in Theorem \ref{thm2-6} and Theorem \ref{thm2-7}.
\begin{figure}[!htbp]
\centering
\includegraphics[width=7.4cm]{test_for_convergence_decreasing.png}
\includegraphics[width=7.4cm]{test_for_consensus_decreasing.png}
\includegraphics[width=7.4cm]{test_for_convergence_decreasing_part.png}
\includegraphics[width=7.4cm]{test_for_consensus_decreasing_part.png}
\caption{The values for $R_1(t)$ are listed in the first column and those for $R_2(t)$ are listed in the second columns. Each figure in the second row displays the first 100 iterations for each of the corresponding figures in the first row.}
\label{figure2}
\end{figure}
%In contrast to Figure \ref{figure1}, we observe that each line in \ref{figure2} has decreasing fluctuation. This is because the decreasing step size allows the algorithm to make larger steps at the beginning of the optimization process and gradually decrease the step size as it approaches the optimal solution.
\subsection{The example in Section \ref{sec9}}
Here we provide a numerical test for the example considered in Section \ref{sec9}. Namely, we test the convergence property of the algorithm \eqref{eq8-4}. To verify the result of Theorem \ref{thm8-1}, we consider the sequence $(x_1 (t), x_2(t))$ of \eqref{eq8-4} and the following measure
\begin{equation}
R(t) = (x_1 (t)-1)^2 + \Big(x_2 (t) - \frac{1}{1-18\alpha}\Big)^2\quad\textrm{for}~ t \geq 0.
\end{equation}
We test with the stepsizes $\{1/200, 1/100, 1/46, 1/43\}$ and the initial values given as
 \begin{equation}
 (x_1 (0), x_2 (0)) \in \{(5,10), (100,5)\}.
\end{equation}
The graph of the normalized measure $R(t)/R(0)$ is provided in Figure  \ref{figure5}. The result shows that the algorithm \eqref{eq8-4} converges to the value $(1, 1/(1-18\alpha))$ for the stepsizes $\{1/200, 1/100, 1/46\}$ as expected by Theorem \ref{thm8-1}. Meanwhile, the algorithm \eqref{eq8-4} diverges for the stepsize $\alpha =1/43$ which is not supported in the interval $(0,1/45)$ guaranteed by Theorem \ref{thm8-1}. These results verify the sharpness of the result of Theorem \ref{thm8-1}.


\begin{figure}[!htbp]
\centering
\includegraphics[width=7.4cm]{5-10.jpg}
\includegraphics[width=7.4cm]{100-5.jpg}
\caption{The left and right graphs represent the value $R(t)/R(0)$ for the initial values $(x_1 (0), x_2(0))=(5,10)$ and $(100,5)$ respectively.}
\label{figure5}
\end{figure}

\noindent \textbf{Conclusion.} In this paper, we established new convergence estimates for the decentrlized projected gradient method. The result guarantees that the algorithm with stepsize $\alpha (t)\equiv \alpha >0$ converges to an $O(\sqrt{\alpha})$-neighborhood of the optimization, provided that $\alpha$ is less than a threshold. We also proved that this result can be improved to the convergence to an $O(\alpha)$-neighborhood for a specific example in dimension one. It remains an open question to extend the $O(\alpha)$ convergence result for general cases.

\begin{comment}
\textbf{Discussion}

\begin{enumerate}
\item $\alpha (t) = \frac{2}{\mu +L}$ is really excluded in the theorems? If not, we may change the experiments a  little bit.
\item Do we use $\hat{x}(t)$?
\end{enumerate}
\end{comment}
\begin{thebibliography}{20}  


\bibitem{AGL} M. Akbari, B. Gharesifard, T. Linder,  Distributed online convex optimization on time-varying directed graphs. IEEE Transactions on Control of Network Systems, 4(3), 417428 (2015).


\bibitem{ALBR} M. Assran, N. Loizou, N. Ballas, and M. Rabbat, Stochastic gradient push
for distributed deep learning, in Proc. 36th Int. Conf. Mach. Learn., 2019,
pp. 344353.

\bibitem{Boyd Gossip} S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah, Randomized gossip
algorithms, IEEE/ACM Transactions on Networking (TON), {\bf14},
no. SI, pp. 25082530, 2006.

\bibitem{BCN} L. Bottou, F. E. Curtis, and J. Nocedal, Optimization methods for large-scale machine learning, SIAM Review, vol. 60, no. 2, pp.
223-311, 2018.

\bibitem{B} S. Bubeck,  Convex optimization: Algorithms and complexity. Foundations and Trends
R in Machine
Learning, 8(3-4):231357, 2015.


\bibitem{BCM} F. Bullo, J. Cortes, and S. Martinez,  Distributed Control of Robotic Networks: A Mathematical Approach to Motion Coordination Algorithms, Princeton Series in Applied Mathematics (2009).

\bibitem{CB} X. Cao, T. Basar (change s),
Decentralized online convex optimization based on signs of relative states. (English summary)
Automatica J. IFAC 129 (2021), Paper No. 109676, 13 pp.

\bibitem{CYRC} Y. Cao, W. Yu, W. Ren, G. Chen,  An overview of recent progress in the study of distributed multiagent coordination. IEEE Trans. Ind. Inform. 9(1), 427-438 (2013)

\bibitem{CK} W. Choi, J. Kim, On the convergence of decentralized gradient descent with diminishing stepsize, revisited, arXiv:2203.09079.

\bibitem{DMR} T. Doan, S. Maguluri, and J. Romberg, Finite-time analysis of distributed
TD(0) with linear function approximation on multi-agent
reinforcement learning, in Proc. Int. Conf. Mach. Learn., 2019,
pp. 16261635.


%\bibitem{DAW} J. C. Duchi, A. Agarwal, and M. J. Wainwright, "Dual averaging for distributed optimization: Convergence analysis and network scaling," IEEE Trans. Autom. Control, vol. 57, no. 3, pp. 592--606, Mar. 2012.

\bibitem{I} I.-A. Chen et al., Fast distributed first-order methods, Masters thesis, Massachusetts Institute of Technology, 2012.

\bibitem{FCG} P. A. Forero, A. Cano, and G. B. Giannakis, Consensus-based distributed support vector machines, Journal of Machine Learning Research, vol. 11, pp. 1663-1707, 2010. 

\bibitem{FP} F. Facchinei and J.-S. Pang, Finite-Dimensional Variational Inequalities and Complementarity Problems. New York: Springer-Verlag, 2003


%\bibitem{FMGP} A. Falsone, K. Margellos, S. Garatti, and M. Prandini, Dual decomposition for multi-agent distributed optimization with coupling constraints, Automatica, vol. 84, pp. 149158, 2017.
 
\bibitem{HCM} S. Hosseini, A. Chapman, M. Mesbahi, Online distributed convex optimization on dynamic networks. IEEE Trans. Automat. Control 61 (2016), no. 11, 35453550. 


\bibitem{KHT}  Y. Kajiyama, N. Hayashi, and S. Takai,  Distributed subgradient method with edge-based event-triggered communication, IEEE Trans. Autom. Control, vol. 63, no. 7, pp. 22482255, Jul. 2018. 


\bibitem{JLYC} B. Jin, H. Li, W. Yan, M. Cao, Distributed model predictive control and optimization for linear systems with global constraints and time-varying communication, IEEE Trans. Automat. Control 66 (7) (2021) 33933400

 \bibitem{LLSX}  C. Liu, H. Li, Y. Shi, and D. Xu,  Distributed event-triggered gradient method for constrained convex optimization,  IEEE Trans. Autom. Control, vol. 65, no. 2, pp. 778785, Feb. 2020.
 
 \bibitem{LQX} S. Liu, Z. Qiu, L. Xie, Convergence rate analysis of distributed optimization with projected subgradient algorithm. Automatica J. IFAC 83 (2017), 162169. 

\bibitem{KLBJ} A. Koloskova, N. Loizou, S. Boreiri, M. Jaggi, and S. Sebastian, A unified theory of decentralized SGD with changing topology and local updates. In International
Conference on Machine Learning, 2020.

%\bibitem{MJ} M. Maros, J. Jald\'{e}n,: On the Q-Linear Convergence of Distributed Generalized ADMM Under Non-Strongly Convex Function Components. 
%IEEE Transactions on Signal and Information Processing over Networks {\bf 5} (3) 442--453, 2019

\bibitem{NO1} A. Nedi{\'c} and A. Ozdaglar, Distributed subgradient methods for multi-agent optimization, 	IEEE Trans. Autom. Control 54 (2009), pp. 4861.

\bibitem{NO2} A. Nedi{\'c} and A. Olshevsky, Distributed optimization over time-varying directed graphs,
IEEE Trans. Autom. Control {\bf60} (2015), pp. 601615.

\bibitem{NO3} A. Nedi{\'c}, A. Olshevsky,  Stochastic gradient-push for strongly convex functions on time-varying directed graphs. IEEE Trans. Automat. Control 61 (2016), no. 12, 39363947.  


%\bibitem{Nedic review} A. Nedi{\'c},   Distributed gradient methods for convex machine learning problems in networks. IEEE Signal Processing Magazine {\bf37} (3) 92--101 (2020)

 
\bibitem{PN}S. Pu and A. Nedi{\'c}, Distributed stochastic gradient tracking methods,
Math. Program, pp. 1--49, 2018

\bibitem{QT} Q. Ling and Z. Tian, Decentralized sparse signal recovery for compressive sleeping wireless sensor networks, IEEE Trans. Signal Process., 58 (2010), pp. 3816--3827.

\bibitem{RB} H. Raja and W. U. Bajwa, Cloud K-SVD: A collaborative dictionary learning algorithm for big, distributed data, IEEE Transactions on Signal Processing, vol. 64, no. 1, pp. 173-188, Jan. 2016. 

\bibitem{RNV} S. S. Ram, A. Nedi{\'c}, and V. V. Veeravalli, Distributed Stochastic
Subgradient Projection Algorithms for Convex Optimization, Journal
of Optimization Theory and Applications, {\bf 147}, no. 3, pp. 516-545, 2010. 

%\bibitem{SJ} A. Simonetto and H. Jamali-Rad, "Primal recovery from consensus-based dual decomposition for distributed convex optimization," J. Optim. Theory Appl., vol. 168, pp. 172197, 2016.

%\bibitem{SKKM} S. Safavi, U. A. Khan, S. Kar, and J. M. F. Moura, Distributed localization: A linear theory, Proceedings of the IEEE, vol. 106, no.
%7, pp. 1204-1223, Jul. 2018.


%\bibitem{SLWY - Extra} W. Shi, Q. Ling, G. Wu, and W. Yin, Extra: An exact first-order
%algorithm for decentralized consensus optimization, SIAM Journal on
%Optimization, {\bf25} , no. 2, pp. 944-966, 2015.


%\bibitem{SLYW} W. Shi, Q. Ling, K. Yuan, G. Wu, and W. Yin, On the linear convergence of the ADMM in decentralized consensus optimization, IEEE Trans. Signal Process., vol. 62, no. 7, pp. 17501761, Apr. 2014. 

 
\bibitem{YLY} K. Yuan, Q. Ling, W. Yin, On the convergence of decentralized gradient descent. 
SIAM J. Optim., {\bf26} (3), 18351854. 

%\bibitem{YYWY} T. Yang, X. Yi, J. Wu, Y. Yuan, D. Wu, Z. Meng, Y. Hong, H. Wang, Z. Lin, and K. H. Johansson, A survey of distributed optimization, Annual Reviews in Control, vol. 47, pp. 278 - 305, 2019.

 
  

\end{thebibliography}

\end{document}

