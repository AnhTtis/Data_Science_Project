\documentclass[aop,preprint,reqno]{imsart}
%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage[numbers]{natbib}
%\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
%\RequirePackage{graphicx}
\usepackage{enumitem}
\usepackage{booktabs} %表格环境
\usepackage[ruled]{algorithm2e} %算法环境
\usepackage{caption}
\usepackage{subcaption}
\RequirePackage{graphicx,ulem}
\startlocaldefs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothesis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{axiom}{Axiom}
\newtheorem{claim}[axiom]{Claim}
\newtheorem{remark}{Remark}[section]
\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{remark}
\newtheorem{Assumption}{Assumption}
\newtheorem{example}{Example}
\newtheorem*{fact}{Fact}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\endlocaldefs

\begin{document}
	
	\begin{frontmatter}
		\title{A spectral based goodness-of-fit test for stochastic block models}
		%\title{A sample article title with some additional note\thanksref{t1}}
		%\runtitle{A sample running head title}
		%\thankstext{T1}{A sample additional note to the title.}
		
		\begin{aug}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%			
			%% Only one address is permitted per author. %%
			%% Only division, organization and e-mail is %%
			%% included in the address.                  %%
			%% Additional information can be included in %%
			%% the Acknowledgments section if necessary. %%
			%% ORCID can be inserted by command:         %%
			%% \orcid{0000-0000-0000-0000}               %%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\author{\fnms{Qianyong} \snm{Wu}\ead[label=e1,mark]
				{wuqy923@nenu.edu.cn}},
			\and
			\author{\fnms{Jiang} \snm{Hu}\ead[label=e2,mark]{huj156@nenu.edu.cn}},
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%			
			%% Addresses                                %%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\address{KLASMOE and School of Mathematics and Statistics, Northeast Normal University, China.
				\printead{e1,e2}}
		\end{aug}
		
		\begin{abstract}
	Community detection in complex networks has attracted considerable attention, however, most existing methods need the number of communities to be specified beforehand. In this paper, a  goodness-of-fit test based on the linear spectral statistic of the centered and rescaled adjacency matrix for the stochastic block model is proposed.  We prove that the proposed test statistic converges in distribution to the standard Gaussian distribution under the null hypothesis. The proof uses some recent advances in generalized Wigner matrices. Simulations and real data examples show that our proposed test statistic performs well. This paper extends the work of Dong et al. [Information Science  512 (2020) 1360-1371].
		\end{abstract}
		
	%	\begin{keyword}[class=MSC]
%			\kwd[Primary]{60B10}
%			\kwd{60F05}
%			\kwd[; secondary ]{62B15}
%		\end{keyword}
		
		\begin{keyword}
			\kwd{Stochastic block model}
			\kwd{Goodness-of-fit test}
			\kwd{Random matrix theory}
			\kwd{Linear spectral statistics}
		\end{keyword}
	\end{frontmatter}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% Please use \tableofcontents for articles %%
	%% with 50 pages and more                   %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\tableofcontents
	
	\section{Introduction}\label{intro}
	Network data can be found in diverse areas,
	such as social networks, gene regulatory networks, food webs, and many others \cite{jalanRandomMatrixAnalysis2007,jiCoauthorshipCitationNetworks2016,newmanNetworks2018,pontesBiclusteringExpressionData2015,westveldMixedEffectsModel2011}. One of the fundamental problems in network analysis is detecting community structure. 
	%where communitiesare groups of nodes that are more similar to each other than to other nodes.
	%``What is the community?'' there is no precise answer, but the most commonly used
	%concept of community is that the links are dense within communities and
	%relatively sparse between communities. 
	The stochastic block model(SBM)  \cite{hollandStochasticBlockmodelsFirst1983,karrerStochasticBlockmodelsCommunity2011}  is probably the most studied tool to model large networks with community structures. For an undirected network $G=(V,E)$, we give a brief introduction about how to generate the adjacency matrix by the SBM. We suppose all the $n$ nodes split into $K$ disjoint communities as follows:
	$$
	V=V^{1}\cup V^{2}\cup \cdots \cup V^{K}.
	$$
	Let $A$ be the $n\times n$ adjacency matrix of $G$. In the SBM the entries $A_{ij}(i > j)$  of the symmetric adjacency $A$ are independent
	Bernoulli random variables satisfying
	$$
	P(A_{ij}=1)=B_{g_{i}g_{j}}
	$$
	where $g \in	\left\{ 1,2,3 \cdots K \right\}^{n} $ is the membership vector and $B = [B_{ab}]$ is a $K\times K$ symmetric matrix.  Here $B$ is a probability matrix which controls edge probabilities between communities. In this paper self-loops are not allowed so that all the diagonal entries of A are 0.
	
	Community detection is recovering the label vector $g$ while giving a single observation of $A$. This problem has received considerable attention from different research areas, many methods have been proposed such as spectral clustering \cite{jinFastCommunityDetection2015,maSemisupervisedSpectralAlgorithms2018,leiConsistencySpectralClustering2015}, likelihood methods \cite{aminiPseudolikelihoodMethodsCommunity2013,bickelAsymptoticNormalityMaximum2013,newmanMixtureModelsExploratory2007} and modularity maximization \cite{newmanFindingCommunityStructure2006}, see  \cite{abbeCommunityDetectionStochastic2018} for a review. However, most of these methods assume we know the number of clusters which we do not. To solve this problem, Bickel et al. \cite{bickelHypothesisTestingAutomated2016} proposed to test $K = 1$ vs $K > 1$, under null hypothesis the proposed test statistic converges to the Tracy-Widom law. Lei \cite{leiGoodnessoffitTestStochastic2016} extend this hypothesis test to a more general situation that is to test $K = K_{0}$ vs $K > K_{0}$. However, both the test statistics they proposed approach the limiting Tracy–Widom distribution slowly. In order to deal with the low convergence rate they adopt an additional bootstrap step. As a result it is time-consuming especially when the size of the network is large. Recently Dong et al. \cite{dongSpectralBasedHypothesis2020} proposed a linear spectral statistic
	to test $K = 1$ vs $K > 1$.  Under the null hypothesis the proposed test statistic converges to the standard normal distribution very fast even when $n$ is small, so one can omit the bootstrap step. Inspired by Lei \cite{leiGoodnessoffitTestStochastic2016}, in this paper we extend Dong et al.'s \cite{dongSpectralBasedHypothesis2020} work to test $K = K_{0}$ vs $K > K_{0}$.  
	
	This paper is organized as follows. In Section  \ref{MR} we introduce the linear spectral statistic and derive its asymptotic null distribution.  A sequential testing algorithm is proposed to determine the number of communities. To illustrate the performance of the test statistic, some simulations and real data examples are given in Section \ref{numexp} and \ref{realdata}. The technical proofs of the results are shown in Section \ref{pf}. Finally, we conclude this paper in section \ref{conclusion}.
	
	
	\section{Main results}\label{MR}
	\subsection{A goodness-of-fit test for stochastic block model}
	Suppose we have a network of $n$ vertices and its adjacency matrix is represented mathematically by $A$. There is a question of whether we can fit this network by a stochastic block model with $K_{0}$ communities. Assume $K$ is the true cluster number which is unknown in advance, this question can be solved in the following
	hypothesis test framework:
	\begin{align}\label{eq1} 
		H_0:K=K_{0}  \quad vs \quad H_1:K\textgreater K_{0}.
	\end{align}
	To derive the goodness-of-fit statistic, we need to introduce some definitions and recent progress in random matrix theory (RMT).
	
	
	Consider the $n\times n$ matrix $P$ given by 
	$P_{ij}=B_{g_{i}g_{j}}$ we  center each $A_{ij}$ by subtracting $P_{ij}$ and  rescale it by dividing $ \sqrt{nP_{ij}(1-P_{ij})}$. 
	Denote $\widetilde{A}$ be
	\begin{align}\label{eq2} 
		\widetilde{A}_{ij}=\left\{
		\begin{array}{rcl}
			\frac{A_{ij}-P_{ij}}{\sqrt{nP_{ij}(1-P_{ij})}}     &      & {i     \neq     j},\\
			0   &      & {i=j}.\\
		\end{array} \right.
	\end{align}
	Now we introduce our  test statistic
	\begin{align}\label{eq3} 
		T = \frac{1}{\sqrt{6}}trace(\widetilde{A}^{3}),
	\end{align}
	where $trace$ represents the  trace operator.
	
	The centered and scaled adjacency matrix $\widetilde{A}$  is the so-called generalized Wigner matrix, satisfying $E(\widetilde{A}_{ij}) =0$ and $Var(\widetilde{A}_{ij}) =\frac{1}{\sqrt{n}}$ for all $(i\neq j)$.
	%The eigenvalues of $\widetilde{A}$ are denoted by $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n$. 
	The limiting spectral distribution(LSD) and linear spectral statistics(LSS) of $W$ have been well studied in random matrix theory. In particular, combining recent developments in  \cite{wangGeneralizationCLTLinear2021} and  \cite{baiSpectralAnalysisLarge2010a}, we
	have the following theorem, whose proof is postponed to Section \ref{pf}.
	\begin{thm}\label{thm1}
		$\quad T = \frac{1}{\sqrt{6}}trace(\widetilde{A}^{3}) \stackrel{d}{\longrightarrow} N(0,1)$, where $ \stackrel{d}{\longrightarrow} $ means convergence 
		in distribution.
	\end{thm}
	\begin{remark}
		Theorem \ref{thm1} is a nontrivial generalization of Theorem 1 in  Dong et al. \cite{dongSpectralBasedHypothesis2020}.  Next, we outline the main differences between the two results. First, 
        Dong et al. \cite{dongSpectralBasedHypothesis2020} introduce new random variables $C_{ii}$ and  let $A_{ii}=C_{ii}$, where $C_{ii}$ are i.i.d random variables such that $P(C_{ii} = -\frac{1}{\sqrt{n}})=P(C_{ii} = \frac{1}{\sqrt{n}})=\frac{1}{2}$,  for all $i$. In this paper, we do not require such unnecessary random variables. Second, the proof in  \cite{dongSpectralBasedHypothesis2020} needs the so-called homogeneity of fourth moments: $E[\widetilde{A}_{ij}^4]=M<\infty$, for $i\neq j$. However,
		the entries of $\widetilde{A}$ do not satisfy this condition when $K>1$. Thus, in this paper, we adopt the recent RMT results in  \cite{wangGeneralizationCLTLinear2021} to avoid the homogeneity of fourth moments condition.
		Most importantly, to determine the number of communities,   Dong et al. need a multiple tests procedure. However, when multiple tests are conducted simultaneously, one has to resort to some corrections (e.g., Bonferroni correction) to control the overall Type I error rate.  It is known that the corrections may be conservative if there are a large number of tests \cite{johnsonAppliedMultivariateStatistical2007}. 
		Through our Theorem \ref{thm1}, we can get an algorithm (Algorithm \ref{alg1} below) that does not require multiple test.  
		
	\end{remark}
	
	In real network datasets we do not know  the true parameter $P$. As a result we cannot  use
	$T$ as our test statistic. In the following we give an estimated  $\hat{T}$ by plugging in an estimated $\hat{P}$ and prove that the estimated test statistic $\hat{T}$ still holds asymptotic normality in Theorem \ref{thm2}.
	
	Let $\hat{g}$ be a consistently estimated community membership vector with the 
	communities number being $K=K_0$. Define $\hat{n}_{k}= \#( \hat{V}_{k} )$, and $\hat{V}_{k}=\left\{i:1\leq i \leq n, \hat{g}(i)=k\right\},$ for all
	$1\leq k \leq K_0$. We consider the plug-in estimator of $B$  \cite{leiGoodnessoffitTestStochastic2016}:
	
	$$
	\hat{B}_{kl}=\left\{
	\begin{array}{rcl}
		\frac{\sum\nolimits_{i\in \hat{\mathcal{N}}_{k},j\in \hat{\mathcal{N}}_{l}}A_{ij}}{\hat{n}_{k}\hat{n}_{l}}, & & {k     \neq     l} ,   \\
		\frac{\sum\nolimits_{i,j\in \hat{\mathcal{N}}_{k},i\leq j }A_{ij}}{\hat{n}_{k}(\hat{n}_{k}-1)/2},  &      & {k=l}.\\
	\end{array} \right.
	$$
	By plugging in $
	\hat{B}$ we get an new centered and rescaled $\widetilde{A}{'}$: 
	$$
	\widetilde{A}^{'}_{ij}=\left\{
	\begin{array}{rcl}
		\frac{A_{ij}-\hat{P}_{ij}}{\sqrt{n\hat{P}_{ij}(1-\hat{P}_{ij})}}     &      & {i     \neq     j},\\
		0    &      & {i=j}.\\
	\end{array} \right.
	$$
	
	For establishing the theoretical results, we need the following assumption:
	\begin{assumption}\label{as1}
		There exists a constant $c_0 > 0$ such that $\min\limits_{1 \leq k \leq K} n_k\geq \frac{c_0n}{K}$ for all $n$.
	\end{assumption}
	\begin{thm} \label{thm2}
		Suppose that Assumption \ref{as1} holds, then under the null hypothesis $H_0:K=K_{0}$, if $K_0 = o({\sqrt{n}})$, the estimated test statistic  
		$\hat{T} = \frac{1}{\sqrt{6}}trace(\widetilde{A}^{'3}) \stackrel{d}{\longrightarrow} N(0,1).$
	\end{thm}
	
	We defer the proof to section \ref{pf}. Given Theorem \ref{thm2}, for the testing problem
	\ref{eq1}, we reject $H_0$ when $\hat T \geq| t_{1-\alpha/2}|$, where $t_{\alpha}$ is the $\alpha$-th quantile of the standard normal distribution.
	
	\subsection{Hypothesis testing algorithm}
	In this section, we put forward a sequential testing algorithm to determine the number of communities. We can see from Theorem \ref{thm2}  our test statistic $\hat{T}$ converges to the standard normal distribution. Given this result, we 
	propose the following algorithm \ref{alg1} to find the number of communities. Note that in the third step of algorithm \ref{alg1} we use the spectral clustering that was introduced in Von Luxburg \cite{vonluxburgTutorialSpectralClustering2007}. The choice of spectral clustering is not connected to the hypothesis test. One can use any other community detection method.
	
	\begin{algorithm}[h]
		%\SetAlgoRefName{} % no count number
		\caption{Hypothesis testing algorithm}
		\label{alg1}
		%	$\bf{Input :}$ Data matrix $A\in \{0,1\}^{n\times n}$, the maximum number of clusters $K_{max}$.
		
		%	$\bf{Output :}$ The estimated number of clusters $\hat{K}$.
		
		1: $\bf{Function}$  HypothesisTest $(A,K_{max})$\\
		2: For $ 1 \leq K_0 \leq K_{max}$ do 3-5\\
		3: $\bf{Function}$ CommunityDetect $(A,K_0)$\\
		4: Compute $\hat{T}(K_0) = \frac{1}{\sqrt{6}}trace(\widetilde{A}^{'3})$\\
		5: Compute p-value($K_0$) under the standard normal distribution.\\
		6: $\hat{K} =\arg \max_{K_0}$ p-value($K_0$).
		
		
		
	\end{algorithm}
	
	
	\section{Numerical experiments} \label{numexp}
	In this section, we illustrate the performance of our  proposed
	test statistic. 
	%and the estimator of K in extensive simulation study.
	Note that in the following simulations spectral clustering algorithm \cite{vonluxburgTutorialSpectralClustering2007} is used to derive the estimated label $\hat{g}$.
	
	\subsection{The null distribution}
	In this simulation, we examine the finite sample null distribution
	of the test statistic and verify the results in  Theorem \ref{thm1} and Theorem \ref{thm2}.
	We follow the same set-ups in Lei \cite{leiGoodnessoffitTestStochastic2016}: there are two equal-sized
	communities, with $B_{11} = B_{22 }= 0.7$ and $B_{12 }= B_{21 }= 0.3$,
	as to sample sizes we consider a small network with $n = 50$.
	
	In Figure \ref{fig1}  we present the histogram plots of our test statistics from 1000 independent realizations under the null hypothesis. The standard normal
	density curve(red line) is also plotted as the reference. It visually confirms the
	results in Theorem \ref{thm1} and Theorem \ref{thm2}.
	
	
	\begin{figure}[h]
		\centering
		\begin{minipage}[t]{0.48\textwidth}
			\centering
			\includegraphics[height= 5.6cm,width=7.3cm]{T.pdf}
			
		\end{minipage}
		\begin{minipage}[t]{0.48\textwidth}
			\centering
			\includegraphics[height= 5.5cm,width=7.3cm]{hatT.pdf}
			
		\end{minipage}
		\caption{ Histogram plots of the test statistic and the standard normal distribution. Left: $P$ is known; Right: $P$ is unknown.
		}\label{fig1}
	\end{figure}
	\subsection{Type I and type II errors}
	In this simulation, we examine the type I error of the proposed test under the null hypothesis and the power against the alternative distributions. 
	The edge probability between communities $k$ and $l$ is 0.2+0.4$\times I(k = l)$. The membership vector $g$ is generated by sampling each entry independently from $\left\{1, . . . , K \right\}$ with equal probability.
	The network size $n = 1000$, after repeatedly performing the proposed hypothesis test 200 times, the proportion of rejection
	at nominal level 0.05 is summarized in Table \ref{tab1}. It can be seen from this table that the Type I error is close to the nominal level and our test statistic is powerful.
	
	\begin{table}[h]
		\centering
		\setlength{\tabcolsep}{7mm}
		\caption{\label{tab1}Proportion of rejection at nominal level 0.05 over 200 realizations.}
		\begin{tabular}{lccl}
			\toprule
			$K_0$ & $K=K_0$ &$K=K_0+1$  \\
			\midrule
			2 & 0.04 & 1 \\
			3 & 0.06 & 1 \\
			4 & 0.05 & 1 \\
			\bottomrule
		\end{tabular}
	\end{table}
	Next, we compare the performance of our test statistic $\hat{T}$ with Lei \cite{leiGoodnessoffitTestStochastic2016}. For their test statistic, we also use the bootstrap correction procedure suggested in his paper. The test statistic is referred to as $TW_1$.
	We fix the network size at $n = 1000$ and let both $K$ and $K_0$ vary from 2 to 5. We take the same parameter settings as in  \cite{leiGoodnessoffitTestStochastic2016}. The edge probabilities between communities $k$ and $l$ are $B_{kl}=0.3+4\times I\{k = l\}$. The membership vector $g$ is generated by sampling each entry independently from $\{1,2,\cdots, K\}$ with equal probability.
	
	Under 100 independent replications, the proportion of rejection at the nominal significance level of
	0.05 can be seen in Table \ref{tab2}. It can be seen from Table \ref{tab2} that the two tests have comparable Type I and Type II errors. Here * represents alternatives that are not considered since we only consider a one-sided test with the alternative $H_1 : K > K_0$.
	
	\begin{table}[h]
		\caption{\label{tab2}Proportion of rejection at nominal level 0.05 over 100 realizations.}
		\centering
		\setlength{\tabcolsep}{4mm}
		\begin{tabular}{c|cccc|cccc}
			\hline
			& \multicolumn{4}{c|}{$\hat{T}$} & \multicolumn{4}{c}{$TW_1$ } \\
			\hline
			$K$      & 2 & 3 & 4 & 5                  & 2 & 3 & 4 & 5   \\
			\hline
			
			$K_0=2$     &$\bf{0.05}$&1.00&1.00&1.00
			&$\bf{0.04}$&1.00&1.00&1.00\\
			$K_0=3$     &*&$\bf{0.05}$&1.00&1.00
			&*&$\bf{0.07}$&1.00&1.00\\
			$K_0=4$     &*&*&$\bf{0.02}$&1.00
			&*&*&$\bf{0.03}$&1.00\\
			$K_0=5$     &*&*&*&$\bf{0.05}$&
			*&*&*&$\bf{0.06}$\\
			\hline
		\end{tabular}
	\end{table}
	
	\subsection{Estimating $K$ by algorithm \ref{alg1}}
	In the third simulation, we examine the performance of algorithm \ref{alg1}. The edge probabilities between communities $k$ and $l$ are $B_{kl}=r(3+4\times I\{k = l\})$, where $r$ controls the sparsity of the network. We consider $r \in \{0.01, 0.05, 0.1 \}$, and values of $K$ vary from 2 to 5. The membership vector $g$ is generated by sampling each entry independently from $\{1, 2, \cdots, K\}$ with equal probability. For each of $K$ and $r$, we generate 200 independent adjacency matrices $A$ with network size $n = 1000$. The proportion of correct estimates can be seen in Table \ref{tab3}. It can be seen from Table \ref{tab3} that algorithm \ref{alg1} works well for $K = 2, 3$ at all sparsity levels. When $K$ gets larger, $K = 4, 5$, algorithm \ref{alg1} requires a dense network to have good performance.
	
	
	\begin{table}[h]
		\centering
		\setlength{\tabcolsep}{7mm}
		\caption{\label{tab3}Proportion of correct estimates of $K$ over 200 repetitions under different sparsity levels indexed by $r$.}
		\begin{tabular}{lcccl}
			\toprule
			$r$ & 0.01 &0.05  & 0.1 \\
			\midrule
			$K = 2$ & 1 & 1 & 1  \\
			$K = 3$ & 1 & 1 &0.99 \\
			$K = 4$ & 0.26 & 0.95 &0.99 \\
			$K = 5$ & 0.00 & 0.87 &0.93 \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	
	
	\section{Real Data Example} \label{realdata}
	\subsection{The dolphin network}
	In this subsection we turn our attention to a popularly studied network collected by Lusseau et al. \cite{lusseauBottlenoseDolphinCommunity2003} 
	(The original data can be downloaded from \url{http://www-personal.umich.edu/~mejn/netdata/}). 
	The nodes are the bottlenose dolphins of a bottlenose dolphin community living off Doubtful Sound, a fjord in New Zealand, 
	it has 62 nodes and 159 edges.  Every time a school of dolphins was encountered in the fjord between 1995 and 2001, each adult member of the school was photographed and identified from natural markings on the dorsal fin. This information was utilized to determine how often two individuals were seen together. Lusseau et al. then built a social network with 62 dolphins and 159 undirected ties representing preferred companionship.
	At first it is well believed that this network can be divided into two groups. Now it was argued in  \cite{liuDiscoveringCommunitiesComplex2016} that $K = 4$ is also reasonable. The p-values corresponding to $K\in\{2,3,4\}$ are listed in Table \ref{tab4}. In Figure \ref{comde} we present the community detection results that are obtained by using different $K$.
	
	\begin{table}[h]
		\centering
		\setlength{\tabcolsep}{7mm}
		\caption{\label{tab4} p-values corresponding to $K\in\{2,3,4\}$. }
		\begin{tabular}{lcccl}
			\toprule
			$K$ & 2&3  & 4 \\
			\midrule
			p-value & $1.6 \times 10^{-6}$ & $2.5 \times 10^{-6}$ & 0.0014  \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	
	
	
	\begin{figure}[h]
		\centering
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[height= 4cm,width=\textwidth]{K=2.pdf}
			\caption{$K=2$}
			\label{}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[height= 4cm,width=\textwidth]{K=3.pdf}
			\caption{$K=3$}
			\label{}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[height= 4cm, width=\textwidth]{K=4.pdf}
			\caption{$K=4$}
			\label{}
		\end{subfigure}
		\caption{Community detection results for different $K$.}
		\label{comde}
	\end{figure}
	
	
	
	
	
	
	
	
	
	
	\subsection{The political blog data}
	In this subsection we consider a well-known network collected by Adamic \cite{adamicPoliticalBlogosphere20042005} (The original data is freely available from the website  \url{http://www-personal.umich.edu/~mejn/netdata/}). The nodes in this network represent political blogs and edges represent hyperlinks between  blogs. The network data records hyperlinks between web blogs shortly before the 2004 US presidential election. As it is commonly done in the literature \cite{wangLikelihoodbasedModelSelection2017,zhaoConsistencyCommunityDetection2012} we only consider the largest connected component of this network, which consists of 1222 nodes.
	This network can be divided into two groups: the liberal and conservative communities, but there exist high degree nodes (see Figure \ref{figDegree}), so it is more proper to fit this network by the degree corrected block model \cite{leiGoodnessoffitTestStochastic2016,karrerStochasticBlockmodelsCommunity2011}.  Under $K=2$, we derived the $p$-value is 0. As result it is inappropriate to fit this network by the SBM with two communities. 
	\begin{figure}[h]
		\centering         
		\includegraphics[height= 6cm,width=10cm]{PblogD.pdf} 
		\caption{Degree histograms for the political blog network.}
		\label{figDegree}
	\end{figure}
	
	\section{Technical proofs}\label{pf}
	In this section, we will prove Theorems \ref{thm1}-\ref{thm2}. We first introduce some definitions and recent progress in random matrix theory(RMT).
	
	Let $W$ be an $n\times n$  Wigner matrix with eigenvalues $\lambda_{i}$, $i=1,2,\cdots,n$. The empirical spectral distribution (ESD) of $W$ is defined as
	$$
	F_{n}(x)=\frac{1}{n}\sum_{i=1}^{n}I(\lambda_{i}\leq x ),
	$$
	It has been proved in Bai et al \cite{baiSpectralAnalysisLarge2010a} that $F_{n}(x)$ converges to the semicircular law $F(x)$ almost surely, see the following Lemma \ref{lem1}, whose density is given by
	$$
	F(dx)=\frac{1}{2\pi}\sqrt{4-x^{2}},\quad -2 \leq x \leq 2.
	$$
	\begin{lem}(Theorem 2.9 in Bai et al. \cite{baiSpectralAnalysisLarge2010a}).\label{lem1}
		Suppose that $W_n = \frac{1}{\sqrt{n}} X_n$ is a Wigner matrix and the entries above or on the diagonal of $X_n$ are independent but may be dependent
		on $n$ and may not necessarily be identically distributed. Assume that all the
		entries of $X_n$ are of mean zero and variance 1 and satisfy the condition that,
		for any constant $\eta > 0$,
		$$
		(A.1)\quad\quad \lim\limits_{n\to\infty} \frac{1}{n^{2}}\sum_{i,j}E(|X_{i,j}|^{2})I(|X_{i,j}|\geq \eta\sqrt n)=0.
		$$
		Then, the ESD of $W_n$ converges to the semicircular law almost surely.
	\end{lem}
	
	Let $\mathcal{U}$ be an open set of the real line that contains the interval $[-2,2]$, which is the support of the semicircular law $F$. Next define $\mathcal{A}$ to be the set of analytic functions $f:\mathcal{U}\rightarrow \mathbb{R} $,
	where $\mathbb{R} $ is the set of real numbers. We then consider the empirical process $G_n={G_n(f)}$
	given by:
	$$	(A.2)\quad\quad {G_n(f)}=n \int_{-\infty}^{\infty} f(x)[F_n-F]\, (dx),\qquad f\in \mathcal{A}. $$
	
	Under the following moment conditions(1)-(3), Wang et al. \cite{wangGeneralizationCLTLinear2021} proved that $G_n$ converges weakly to a Gaussian process, see the following Lemma \ref{lem2}.
	
	(1) For all $i$, $E(|X_{i,i}|^{2})=\sigma^{2}$, for all $i < j$, $E(|X_{i,j}|^{2})=1.$
	
	(2) $\lim_{n \rightarrow \infty}\sum_{i,j}E|X_{i,j}|^{4}=L$.
	
	(3) For any $\eta \geq 0$ as ${n\to\infty},$  
	$$
	\frac{1}{\eta^{4}n^{2}}\sum_{i,j}E[|X_{i,j}|^{4}I(|X_{i,j}|\geq \eta\sqrt n)]=o(1).
	$$
	
	\begin{lem}(Theorem 2.1 in  Wang et al. \cite{wangGeneralizationCLTLinear2021})\label{lem2}.
		Under conditions (1)–(3), the spectral empirical process
		$G_n = (G_{n}(f))$ indexed by the set of analytic functions $\mathcal{A}$ converges weakly
		in finite dimension to a Gaussian process $G := \left\{G(f) : f\in \mathcal{A}\right\}$ with mean
		function $E[G(f)]$ given by
		$$
		\frac{1}{4}(f(2)+f(-2))-\frac{1}{2}\tau_0(f)+(\sigma^{2}-2)\tau_2(f)+(L-3)\tau_4(f),
		$$
		and the covariance function $c(f, g) :=E((G(f)-E(G(f))(G(g)-E(G(g)))$ given by
		$$cov(f,g)=\frac{1}{4\pi^2}\int_{-2}^{2} \int_{-2}^{2}f^{'}(t)g^{'}(s)V(t,s)\, dtds
		,$$
		where $$
		V(t,s)=(\sigma^{2}-2+\frac{L-3}{2} ts)\sqrt{(4-t^2)(4-s^2)}+2\log(\frac{4-ts+\sqrt{(4-t^2)(4-s^2)}}{4-ts-\sqrt{(4-t^2)(4-s^2)}}),
		$$	
		
		$$
		\tau_l(f)=\frac{1}{2\pi}\int_{-\infty}^{\infty} f(2cos\theta)cos(l\theta)\, d\theta.
		$$
	\end{lem}
	
	
	
	
	
	Proof of Theorem \ref{thm1}
		First, we show that the ESD of $\widetilde{A}$ converges to the semicircular law $F(x)$ almost surely. Let $X=\sqrt{n}\widetilde{A}$. From Lemma \ref{lem1}, it is sufficient to prove that condition (A.1) is satisfied.
		For any $\eta \geq 0$, we have
		
		
		\begin{equation}
			\begin{aligned}\nonumber
				&\frac{1}{n^2}\sum_{i,j}E(|X_{i,j}|^{2})I(|X_{i,j}|\geq \eta\sqrt{n})
				=\frac{1}{n^2}\sum_{i,j}I(|X_{i,j}|\geq \eta\sqrt(n))\\
				&\leq\mathop{max}\limits_{ij}\left\{I(|X_{i,j}|\geq \eta\sqrt{n})\right\}
				=\mathop{max}\limits_{i\neq j}\left\{I(|X_{i,j}|\geq \eta\sqrt{n})\right\}
				\rightarrow 0.
			\end{aligned}
		\end{equation}
		the last equality is due to $|X_{ii}|=0$, and $|X_{ij}|=\frac{A_{ij}-P_{ij}}{\sqrt{P_{ij}(1-P_{ij})}}$ is bounded.
		
		Second, we prove that $\widetilde{A}$ satisfies conditions (1)-(3) of Lemma \ref{lem2}.
		Since $E(|X_{i,j}|^{2})=nE(|\widetilde{A}_{i,j}|^{2})=1$ for all $ i,j\in[n],$ as 
		a result condition (1) holds. Condition (2) holds because of $E|X_{i,j}|^{4}=O(\frac{1}{n^2}).$
		Last, we verify that condition(3) holds. For any $\eta \geq 0$, we have
		\begin{equation}
			\begin{aligned}\nonumber
				&\frac{1}{\eta^{4}n^2}\sum_{i,j}E[|X_{i,j}|^{4}I(|X_{i,j}|\geq \eta\sqrt{n})]
				\leq\frac{1}{\eta^{4}n^2}\sum_{i,j}(E(|X_{i,j}|^{8}))^{\frac{1}{2}}E(I(|X_{i,j}|\geq \eta\sqrt{n}))^{\frac{1}{2}}\\
				&\leq\frac{C_{1}}{\eta^{4}n^2}\sum_{i,j}(P(|X_{i,j}|\geq \eta\sqrt{n}))^{\frac{1}{2}}
				\leq\frac{C_{1}}{\eta^{4}}\mathop{max}\limits_{i, j}\left\{(P(|X_{i,j}|\geq \eta\sqrt{n}))^{\frac{1}{2}}\right\}\\
				&=\frac{C_{1}}{\eta^{4}}\mathop{max}\limits_{i\neq j}\left\{(P(|\frac{A_{i,j}-P_{ij}}{P_{ij}(1-P_{ij})}|\geq \eta\sqrt{n}))^{\frac{1}{2}},\right\}\\
				&\rightarrow 0.
			\end{aligned}
		\end{equation}
		where $C_{1}$ is the upper bound of $(E(|X_{i,j}|^{8}))^{\frac{1}{2}}$, the second  inequality holds because of H\"older inequality.
		
		Let $f(x)=x^{3}$, then we have
		\begin{equation}
			\begin{aligned}\nonumber
				&{G_n(f)}=n \int_{-\infty}^{\infty} f(x)[F_n-F]\, (dx)
				=n \int_{-\infty}^{\infty} x^3F_n\, (dx)-n \int_{-\infty}^{\infty} x^3F\, (dx)\\
				&~~\quad \quad=n \int_{-\infty}^{\infty} x^3F_n\, (dx)
				=\sum_{i=1}^{n}\lambda_{i}^{3}
				=trace(\widetilde{A}^{3}).\\
			\end{aligned}
		\end{equation}
		From Lemma \ref{lem2} we derive
		$$ 
		E(G (x^{3} )) = 0 , Var(G(x^{3})) = 6.
		$$
		This completes the proof of Theorem \ref{thm1}.
	
	
	Next we will prove Theorem \ref{thm2} in matrix form. For simplicity, we introduce some notations used in the proof.
	
	For two matrices A and B of the same dimension $m \times n$, the Hadamard product $ A\circ B$ is a matrix  with elements given by $(A\circ B)_{ij}=(A)_{ij}(B)_{ij},$  the Hadamard division  $A\oslash B$ is a matrix  with elements given by $(A\oslash B)_{ij}=\frac{(A)_{ij}}{(B)_{ij}},$ the Hadamard root is defined as $(A^{\circ\frac{1}{2}})_{ij}= A_{ij}^{\frac{1}{2}}$. 
	
	Proof of Theorem \ref{thm2}
		We first denote 
		\begin{align}\label{eq4} 
			\widetilde{A}_{1}=\widetilde{A}+(P-\hat{P})\oslash(nP\circ(J_n-P))^{\circ\frac{1}{2}},
		\end{align}
		\begin{align}\label{eq5} 
			\widetilde{A_{1}^{'}}=(A-\hat{P}+\hat{P}\circ I_n)\oslash(nP\circ(J_n-P))^{\circ\frac{1}{2}}.
		\end{align}
		where  $\circ$ represents the Hadamard product, $\oslash$ represents the Hadamard division, ${\circ\frac{1}{2}}$  represents the Hadamard root, $J_n$ is the $n\times n$ matrix of ones, $I_n$ is the $n\times n$ identify matrix.
		
		Note that $\widetilde{A}=(A-P+P\circ I_{n})\oslash(nP\circ(J_n-P))^{\circ\frac{1}{2}}$, then we have
		\begin{align}\label{eq6} 
			\widetilde{A_{1}}=\widetilde{A_{1}^{'}}+((P-\hat{P})\circ I_n)\oslash(nP\circ(J_n-P))^{\circ\frac{1}{2}}.
		\end{align}
		
		From assumption \ref{as1} we have $P_{ij}-\hat{P}_{ij}=O_p(\frac{k}{n})$, thus (\ref{eq6}) can be expressed as
		\begin{align}\label{eq7} 
			\widetilde{A_{1}}=\widetilde{A_{1}^{'}}+O_p(\frac{k}{n^\frac{3}{2}})I_n.	
		\end{align}
		Similarly, we have
		\begin{align}\label{eq8} 
			\widetilde{A}=\widetilde{A_{1}}+O_p(\frac{k}{{n^\frac{3}{2}}})J_n.
		\end{align}
		As a result, we have
		$$
		trace([\widetilde{A_{1}}]^{3}-[\widetilde{A_{1}^{'}}]^{3})=
		O_p(\frac{k}{{n^\frac{3}{2}}})trace([\widetilde{A_{1}^{'}}]^{2})+O_p(\frac{k^{2}}{{n^{3}}})trace([\widetilde{A_{1}^{'}}])+O_p(\frac{k^{3}}{{n^\frac{9}{2}}})trace(I_{n}).
		$$
		First, we have
		$
		trace(\widetilde{A^{'}_{1}})=0.$  Second, one can prove that
		$$
		trace([\widetilde{A_{1}^{'}}]^{2})=\sum_{i=1}^{n}\widetilde{[A_{1}^{'}}(i,i)]^{2}
		+\sum_{i \neq j}[\widetilde{A_{1}^{'}}(i,j)]^{2}
		=\sum_{i \neq j}[\widetilde{A_{1}^{'}}(i,j)]^{2}=O_p(n).
		$$	
		the last equality holds because of 
		\begin{equation}
			\begin{aligned}\nonumber
				&\sum_{i \neq j}[\widetilde{A_{1}^{'}}(i,j)]^{2}=\sum_{i \neq j}\frac{(A_{ij}-P_{ij})^{2}}{nP_{ij}(1-P_{ij})}
				+2\sum_{i \neq j}\frac{(A_{ij}-P_{ij})(P_{ij}-\hat{P}_{ij})}{nP_{ij}(1-P_{ij})}
				+\sum_{i \neq j}\frac{(P_{ij}-\hat{P}_{ij})^2}{nP_{ij}(1-P_{ij})}\\
				&~~~\quad \quad \quad \quad \quad=O_p(n)+O_p(k) +O_p(\frac{k^2}{n}).
			\end{aligned}
		\end{equation}	
		Finally, we have
		\begin{align}\label{eq9} 
			trace([\widetilde{A_{1}}]^{3}-[\widetilde{A_{1}^{'}}]^{3})=
			O_p(\frac{k}{{\sqrt n}}).
		\end{align}
		Similarly to equation (\ref{eq8}), one can derive
		$$
		trace([\widetilde{A_{1}}]^{3}-[\widetilde{A}]^{3})=
		O_p(\frac{k}{{n^\frac{3}{2}}})trace([\widetilde{A}]^{2}J_{n})+O_p(\frac{k^{2}}{{n^{3}}})trace([\widetilde{A}J_{n}^{2}])+O_p(\frac{k^{3}}{{n^\frac{9}{2}}})trace(J_{n}^{3}).
		$$
		Note that
		$$
		trace([\widetilde{A}]^{2}J_{n})\leq \lambda_{max}([\widetilde{A}]^{2})n=O_P(n).
		$$
		$$
		trace([\widetilde{A}]J_{n}^{2})\leq\lambda_{max}(\widetilde{A})n^{2}=O_P(n^{2}).
		$$
		As a result
		\begin{align}\label{eq10} 
			trace([\widetilde{A_{1}}]^{3}-[\widetilde{A}]^{3})=
			O_p(\frac{k}{{\sqrt n}}).  
		\end{align}
		Recall that
		\begin{align}\label{eq11} 
			\widetilde{A}^{'}=(P\circ(J_n-P)\oslash(\hat{P}(J_n-\hat{P})))^{\circ\frac{1}{2}}\circ\widetilde{A_{1}^{'}},
		\end{align} 
		because of
		\begin{align}\label{eq12} 
			(P\circ(J_n-P)))^{\circ\frac{1}{2}}=(\hat{P}\circ(J_n-\hat{P})))^{\circ\frac{1}{2}}\circ(1+O_P(\frac{k}{n}))J_n.
		\end{align}
		we have
		\begin{align}\label{eq13} 
			\widetilde{A}^{'}=(1+O_P(\frac{k}{n}))\widetilde{A_{1}^{'}},
		\end{align} 
		As a result,
		\begin{align}\label{eq14} 
			trace([\widetilde{A^{'}}]^{3})=(1+O_P(\frac{k}{n}))^{3}trace([\widetilde{A^{'}_{1}}]^{3}).
		\end{align}
		Combing equation (\ref{eq9}) and (\ref{eq10}), we have
		\begin{align}\label{eq15} 
			trace([\widetilde{A^{'}}]^{3})=(1+O_P(\frac{k}{n}))^{3}(trace([\widetilde{A}]^{3})+O_p(\frac{k}{{\sqrt n}})).
		\end{align}
		This completes our proof.		

	
	
	%Next we consider the power of the test based on $\hat{\theta}$. The following
	%theorem provides a lower bound of the growth rate of the test statistic $\hat{\theta}$
	%under the alternative model $K >K_0$.	
	
	%\begin{thm} 
	%	Under the alternative hypothesis $H_1:K\textgreater K_{0}$, the estimated test statistic  
	%	$\hat{\theta} = O_P(k\sqrt n).$
	%\end{thm}
	%\begin{pf}
	%	Under the alternative hypothesis $P_{ij}-\hat{P}_{ij}=O_p(1)$, as result
	%	$$\widetilde{A_{1}}=\widetilde{A_{1}^{'}}+O_p(\frac{k}{\sqrt n})I_n.$$
	%	$$\widetilde{A}=\widetilde{A_{1}}+O_p(\frac{k}{{\sqrt n}})J_n.$$
	%	$$
	%	\widetilde{A}^{'}=(1+O_P(1))\widetilde{A_{1}^{'}}+O_P(1) C.
	%	$$
	%	Similar to Theorem 3.5, we have
	%	\begin{equation}
	%	\begin{aligned}\nonumber
	%	&trace([\widetilde{A^{'}}]^{3})=(1+O_P(1))^{3}trace([\widetilde{A^{'}_{1}}]^{3})+(1+O_P(1))O_P(\sqrt n)\\
	%	&\quad \quad \quad \quad \quad  =(1+O_P(1))^{3}(trace([\widetilde{A}]^{3})+O_p(k\sqrt n))+O_P(\sqrt n).
	%	\end{aligned}
	%	\end{equation}
	%which completes our proof.
	%\end{pf}
	
	\section{Conclusion}\label{conclusion}
	In this paper, we propose a linear spectral statistic to test whether a network
	can be fitted by the SBM with $K_0$ communities. This test statistic is the trace of the third order for a centered and scaled adjacency matrix $A$.  With some recent progress in random matrix theory, we derive the asymptotic distribution of the test statistic under the null hypothesis. Simulations and real data examples validate our theoretical results. In many real-world networks, the nodes often exhibit degree heterogeneity.  This paper examines random networks using the framework of random matrix theory. Recently, there have been some results on block random matrices\cite{baoSpectralStatisticsSample2022,wangCentralLimitTheorem2021a}, we hope those results can help us extend the test statistic in this paper to the degree corrected SBM in the future.
	
	
	\section*{Acknowledgments}
	Jiang Hu was supported by National Natural Science Foundation of China (Grant Nos. 12171078 and
	11971097).
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% Support information, if any,             %%
	%% should be provided in the                %%
	%% Acknowledgements section.                %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\begin{acks}[Acknowledgments]
	%The authors would like to thank the anonymous referees, an %Associate
	%Editor and the Editor for their constructive comments that improved the
	%quality of this paper.
	%\end{acks}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% Funding information, if any,             %%
	%% should be provided in the                %%
	%% funding section.                         %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\begin{funding}
	%The first author was supported by NSF Grant DMS-??-??????.
	
	%The second author was supported in part by NIH Grant ???????????.
	%\end{funding}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% Supplementary Material, including data   %%
	%% sets and code, should be provided in     %%
	%% {supplement} environment with title      %%
	%% and short description. It cannot be      %%
	%% available exclusively as external link.  %%
	%% All Supplementary Material must be       %%
	%% available to the reader on Project       %%
	%% Euclid with the published article.       %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\begin{supplement}
	%\stitle{Title of Supplement A}
	%\sdescription{Short description of Supplement A.}
	%\end{supplement}
	%\begin{supplement}
	%\stitle{Title of Supplement B}
	%\sdescription{Short description of Supplement B.}
	%\end{supplement}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%                  The Bibliography                       %%
	%%                                                         %%
	%%  imsart-???.bst  will be used to                        %%
	%%  create a .BBL file for submission.                     %%
	%%                                                         %%
	%%  Note that the displayed Bibliography will not          %%
	%%  necessarily be rendered by Latex exactly as specified  %%
	%%  in the online Instructions for Authors.                %%
	%%                                                         %%
	%%  MR numbers will be added by VTeX.                      %%
	%%                                                         %%
	%%  Use cite{...} to cite references in text.             %%
	%%                                                         %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%% if your bibliography is in bibtex format, uncomment commands:
	%\bibliographystyle{imsart-number} % Style BST file (imsart-number.bst or imsart-nameyear.bst)
	%\bibliography{bibliography}       % Bibliography file (usually '*.bib')
	
	%% or include bibliography directly:
	\bibliographystyle{imsart-number}
	\bibliography{MyLibrary} 
	
	
\end{document}
