

\section{Discussion}
\label{sec:conclusions}
\paragraph{Conclusion.}
In this work, we provide a comprehensive study of post-training quantization (\ptq) on large language models with different \ptq methods (e.g., \rtn, \gptq, ZeroQuant), and with different quantization coverage (weight-only and weight-and-activation quantization), etc. 
We find that \ptq methods are critical to improving the quantized model quality, and that fine-grained quantization (FGQ) can bring acceptable accuracy and model size trade-off. Finally, we introduced an optimization technique called Low Rank Compensation (LoRC), which works synergistically with PTQ and FGQ, playing a crucial role in enhancing full model quality recovery with a minimal increase in model size.

\paragraph{Limitation.} Despite quantizing over 10,000 experiments, our study was constrained by our computing resources. This restriction made us choose between diversifying the model sizes and varying the tasks. We strategically limited our datasets to WikiText, PTB, and C4 to concentrate on a broad range of quantization methods. Consequently, our general findings are more robust concerning the two model families and three datasets examined in this paper. However, caution should be exercised when generalizing these findings to tasks that are dissimilar to those covered in this study.
% We also list several potential future directions and hope our work sheds some light on \llms compression.
\paragraph{Future Opportunity.}
Throughout the paper, we see several unresolved problems from current quantization schemes and/or algorithms, and we find potential directions for \llm compression: (1) Although we use fine-grained quantization schemes in the paper, the real implementation is missing. Moreover, how to efficiently implement odd bit precision is  challenging. 
    \cite{frantar2022gptq} demonstrated that 3-bit can achieve better throughput in the generation phase by packing all 3-bit numbers in continuous memory space. 
    However, this method is sub-optimal as the dequantization step needs to connect bits from different bytes.
    One possible way to implement odd bits, e.g., 5 bits, is to use two integer matrices with INT4 and INT1.
    During the dequantization stage, we couple the two matrices together. 
    (2) How to combine \ptq with other lightweight compression techniques, e.g., post-training pruning~\cite{kwon2022fast,frantar2023massive}, is an interesting direction to further reduce the memory consumption and compute cost. 
