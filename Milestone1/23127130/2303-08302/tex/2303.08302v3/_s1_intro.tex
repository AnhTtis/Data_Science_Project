\section{Introduction}
\label{sec:intro}
% Large language models (LLMs) have been shown breakthrough performance on various benchmarks, e.g., natural language understanding and generation, and have been adopted for daily usage, e.g., Codex~\cite{copilot} and ChatGPT~\cite{chatgpt}.
% However, how to \textit{efficiently} serve those LLMs becomes urgent due to their large memory consumption and heavy computation requirement. %% about the main challengens

% Unlike classification models or diffusion models, \llms have their own serving challenge.
% Generally, classification models run inference once per query and diffusion models have the same inference behavior for every time step.
% However, \llms have two phases, i.e., prompt and generation:
% the prompt stage takes the query/question (a sequence of tokens) from the user and runs one forward pass, 
% then the generation stage auto-regressively (token-by-token) generates the corresponding answer by running the model for multiple steps. 
% The fundamental bottlenecks for prompt and generation phases are different. 
% Particularly, for a normal prompt stage (e.g., sequence length $\geq256$), the forward pass is primarily compute bounded, i.e., higher compute brings better latency;
% for the normal generation phase (low batch size) with KV cache, the forward pass is mainly memory bounded, i.e., higher memory bandwidth brings better performance ~\cite{pope2022efficiently}    %% about the inference challenges
% % See~\cite{pope2022efficiently} for a more detailed analysis.

% Meanwhile, as mentioned in~\cite[Figure 3]{gholami2020ai}, the bandwidth of hardware increases about 1.4x every two years while the compute increases about 3.1x every two years.
% Additionally, multiple nodes are now required to serve extra large models, e.g., 2 A100-80G nodes for MT-NLG-530B~\cite{smith2022using} and 2 A100-40G nodes for GPT-3-175B~\cite{brown2020language}, which introduces the extra bandwidth challenge between cross-node communication.
% As such, reducing the model size for LLMs is an urgent request. 
% Meanwhile, if we can also reduce the compute cost, it will cover both prompt and generation phases to further alleviate the serving challenge for \llms.  %why using PTQ

 
%  Considering the forbidden training/finetuning cost for those \llms, an effective way to alleviate those memory/compute challenges is post-training quantization (\ptq), where no/minimal training is required to reduce the bit precision for weights and/or activations to INT4 or INT8.  %talle about INT4/8



Large language models (LLMs) like Codex~\cite{copilot} and ChatGPT~\cite{chatgpt} have demonstrated breakthrough performance across various benchmarks, such as natural language understanding and generation, and are now integrated into everyday applications. However, efficiently serving LLMs has become a pressing concern due to their significant memory consumption and computational demands. Unlike classification or diffusion models, LLMs present unique challenges, as they involve two distinct phases: prompt and generation. The prompt phase is primarily compute-bound, while the generation phase, with low batch size and KV cache, is mainly memory-bound~\cite{pope2022efficiently}.


As the progression of hardware bandwidth lags behind that of computational demand \cite{gholami2020ai}, the resource demands of extra-large models such as MT-NLG-530B~\cite{smith2022using}—which necessitates the deployment of multiple nodes for operation—escalate, adding to the complexities of cross-node communication. This has emphasized the urgency to curtail both the size and computational expense of Large Language Models (LLMs). An increasingly effective solution to these issues is post-training quantization (PTQ). This method aids in the reduction of training prerequisites while simultaneously lowering the bit precision of weights and activations to either INT4 or INT8.

\begin{figure}
\centering
\includegraphics[width=0.49\textwidth]{figures/model-tradeoff-opt.pdf}
\includegraphics[width=0.49\textwidth]{figures/model-tradeoff-bloom.pdf}
\vspace{-0.42cm}
\caption{The model size and quality trade-off of different quantization methods on models from \opt and \bloom families.
Here \ptq (with fine-grained quantization) represents the method from~\cite{yao2022zeroquant,frantar2022gptq},
\rtn means the naive round-to-nearest baseline (with fine-grained quantization as well),
and FP16/INT8 is used as the no-accuracy-loss baseline. \lorc is our proposed method that works seamless with \ptq.
Note that we drop all diverged points for better visualization.
For all detailed numbers, please see~\appref{sec:full_tables_results}.}
\label{fig:main_figure}
\vspace{-0.5cm}
\end{figure}

% Despite the demonstrated success of PTQ in recent studies~\cite{yao2022zeroquant,frantar2022gptq,xiao2022smoothquant,dettmers2022case}, the existing literature seldom offers  systematic exploration of several crucial aspects such as the functional coverage of different PTQ methods and the sensitivity of different models. Furthermore, although current quantization methods have delivered encouraging outcomes in model size reduction, it remains an open question whether these methods are optimally harnessing the potential to minimize LLM sizes. Consequently, this paper aims to address two primary questions: first, do LLMs of different sizes and pretraining data exhibit similar behavior when quantized? Second, are the existing quantization methods truly maximizing their potential in minimizing LLM sizes?



% While the effectiveness of post-training quantization (PTQ) has been underscored in a number of recent studies~\cite{yao2022zeroquant,frantar2022gptq,xiao2022smoothquant,dettmers2022case}, a comprehensive, systematic investigation into several key dimensions of this technique remains to be undertaken. Specifically, the extant literature falls short in providing thorough coverage of the functionality of various PTQ methods or the sensitivity of disparate models. In addition, despite the promising results exhibited by current quantization methods in terms of reducing model sizes, it remains ambiguous whether these methods are being leveraged to their fullest potential for minimizing Large Language Models (LLMs) sizes. In light of these gaps in our understanding, this paper sets forth to address two primary queries: initially, we seek to determine whether LLMs, with varying sizes and pretraining data, display consistent behavior when subjected to quantization. Subsequently, we aim to investigate whether the prevailing quantization techniques are truly optimizing their capacity to shrink LLM sizes.
% Hardware bandwidth increases at a slower rate than computation \cite{gholami2020ai}, and extra large models now require multiple nodes for serving (i.e., two A100-80G nodes for MT-NLG-530B~\cite{smith2022using}), introducing additional bandwidth challenges for cross-node communication. Reducing model size and compute cost for LLMs is therefore essential. One effective approach to address these challenges is post-training quantization (PTQ), which minimizes training requirements while reducing the bit precision of weights and activations to INT4 or INT8.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.49\textwidth]{figures/model-tradeoff-opt.pdf}
%     \includegraphics[width=0.49\textwidth]{figures/model-tradeoff-bloom.pdf}
%     \caption{The model size and quality trade-off of different quantization methods on models from \opt and \bloom families.
%     Here  \ptq (with fine-grained quantization) represents the method from~\cite{yao2022zeroquant,frantar2022gptq}, 
%     \rtn means the naive round-to-nearest baseline (with fine-grained quantization as well),
%     and FP16/INT8 is used as the no-accuracy-loss baseline. \lorc is our proposed method that works seamless with \ptq.
%     Note that we drop all diverged points for better visualization. 
%     For all detailed numbers, please see~\appref{sec:full_tables_results}.}
%     \label{fig:main_figure}
% \end{figure}

% Recent studies~\cite{yao2022zeroquant,frantar2022gptq,xiao2022smoothquant,dettmers2022case} have shown the effectiveness of \ptq. However, none of them gives a systematic study, e.g., the functional coverage for different \ptq methods, the sensitivity of different models, etc.  Moreover, while existing quantization methods have shown promising results in reducing the model sizes, it is not clear if there is still room for investigation and improvement to push the limits of generative model size using advanced quantization techniques. 
% Thus, we set goals to answer:
% \begin{center}
% When quantized, do LLMs with different sizes and pretraining data have similar behavior? \\

% Are existing quantization methods optimally harnessing the potential to minimize  LLMs sizes?
% \end{center}
% \xiaoxia{need to think how to naturally  raise the above two question }

%  In this work, we provide a comprehensive study on the quantization effect for both weigh-only quantization and weight-and-activation quantization using different quantization schemes with various \ptq methods, including round-to-nearest (\rtn), \gptq~\cite{frantar2022gptq}, ZeroQuant~\cite{yao2022zeroquant} and its variants, on two different model families \opt~\cite{zhang2022opt} and \bloom~\cite{scao2022bloom} across model sizes from 125M to 176B.
% With better understanding of the existing methods, we propose an innovative method -- \textbf{Lo}w \textbf{R}ank \textbf{C}ompensation (LORC), where we  apply low rank matrix factorization on the quantization error matrix to compensate the accuracy loss.

% In this work, we offer a thorough investigation of the \ptq quantization impact for both weight-only, activation-only, and weight-and-activation quantization, utilizing various PTQ methods such as round-to-nearest (\rtn), GPTQ~\cite{frantar2022gptq}, ZeroQuant~\cite{yao2022zeroquant}, and their variants. We analyze two different model families, OPT~\cite{zhang2022opt} and BLOOM~\cite{scao2022bloom}, across model sizes ranging from 125M to 176B. 

While the effectiveness of post-training quantization (PTQ) has been underscored in a number of recent studies~\cite{yao2022zeroquant,frantar2022gptq,xiao2022smoothquant,dettmers2022case}, a comprehensive, systematic investigation into several key dimensions of this technique remains to be undertaken. Specifically, the extant literature falls short in providing thorough coverage of the functionality of various PTQ methods or the sensitivity of disparate models. Moreover, despite current quantization methods demonstrating promising results in the reduction of model sizes, the question persists as to whether these methods are achieving their optimal potential in minimizing Large Language Models (LLMs) sizes.

With these observations in mind, our study sets forth to address two salient questions: (1) When subjected to quantization, do LLMs of varying sizes and pretraining data exhibit similar behavior? (2) Are existing quantization methods truly leveraging their full potential in reducing the sizes of LLMs?

\paragraph{Contribution.} To elucidate these queries, we undertake an exhaustive examination of the impact of PTQ on weight-only, activation-only, and combined weight-and-activation quantization. This investigation incorporates a range of PTQ methods, including round-to-nearest (\rtn), GPTQ~\cite{frantar2022gptq}, ZeroQuant~\cite{yao2022zeroquant}, and their respective variants. To broaden the scope of our analysis, we focus on two distinct model families, OPT~\cite{zhang2022opt} and BLOOM~\cite{scao2022bloom}, spanning model sizes from 125M to a massive 176B. Our code will be made available for reproduction. In summary, we make the following contributions:

% \begin{itemize}
(1) We provide a thorough \textbf{sensitivity analysis} 
to demonstrate that a) Activation quantization is generally more sensitive to weight quantization; Smaller models usually have better activation quantization performance than the relative larger model. b) Different model families show different INT8 activation quantization behaviors; Particularly for large models, \bloom-176B has  small accuracy drops (about 1 perplexity or PPL) but \opt-30B and -66B experience worse performance. 

(2) We carry out a detailed evaluation and comparison of current PTQ methods, utilizing optimal configurations to maximize model size reduction while minimizing accuracy impact. 
We found that the current existing method can barely achieve less than 0.1 \ppl points degradation for quantization with either INT4-weight or INT4-weight-and-INT8-activation (W4A8). To recover the 0.1 \ppl, we strive to push the boundaries of employing \textbf{fine-grained quantization} (FGQ) techniques. We observe FGQ is able to recovered points degradation of <0.1 \ppl  for large models (>13B) for INT4 weight quantization, but there are still non-negligible model quality drops.

(3) Based on the above understanding, we further optimize existing methods and introduce a technique called \textbf{Lo}w \textbf{R}ank \textbf{C}ompensation (LoRC), which employs low-rank matrix factorization on the quantization error matrix. Complementary to FGQ, LoRC plays a crucial role in enhancing the full model quality recovery, while there is little increase of the model size.
% \end{itemize}



In~\fref{fig:main_figure}, we provide model size and quality trade-offs for both \opt and \bloom families. 
As can be seen, using \lorc on top of \ptq  methods from~\cite{yao2022zeroquant,frantar2022gptq} and fine-grained quantization, we set a new quantization Pareto frontier for \llms.   
Meanwhile, we recommend the following setting for quantizing \llms with LoRC (Note that activation quantization should be only applied if necessary): 
(1) For larger models (>10B), fine-grained (block size 64--256) 4-bit weight quantization plus 8-bit activation quantization (block size 64--256) with \ptq  can be used for real deployment;
(2) For middle-size models (<10B and >1B), per-row INT8 quantization plus fine-grained (block size 64--256) INT8 activation quantization can be used with \ptq  from~\cite{frantar2022gptq,yao2022zeroquant};
(3) For smaller models (<1B),  per-row W8A8 (INT8 weight and INT8 activation) \rtn is enough based on~\cite{yao2022zeroquant}.% \xiaoxia{need to rephrase}
