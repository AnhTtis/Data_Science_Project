\section{Are existing quantization methods optimally harnessing the potential to minimize LLMs sizes?}
\label{sec:evaluation_of_existing_methods}
% Several lightweight optimization-based (weight of the model will be updated during quantization) methods have been proposed. 
% Different than quantization-aware training, those methods~\cite{yao2022zeroquant,frantar2022gptq,xiao2022smoothquant} only require a small portion of the training data and a short range of training time. Among them, two types of methods are demonstrated to be both effective and efficient (based on GPU resource, time cost, and data) for INT4 weight quantization, GPTQ~\cite{frantar2022gptq} and ZeroQuant~\cite{yao2022zeroquant}. 
% For this work, we focus on the variants of \gptq and ZeroQuant as well as the most straightforward baseline, round-to-nearest neighborhood (\rtn). 
Numerous lightweight optimization-based methods have been proposed, which update the model weights during quantization. These methods such as \cite{yao2022zeroquant,frantar2022gptq,xiao2022smoothquant}, unlike quantization-aware training, only require a small portion of the training data and a limited training time. Particularly, GPTQ~\cite{frantar2022gptq} and ZeroQuant~\cite{yao2022zeroquant}, have proven to be effective and efficient in terms of GPU resources, time cost, and data usage for INT4 weight quantization.\footnote{We tested the method proposed by \cite{xiao2022smoothquant} but did not find it better than others for INT4 weight quantization.} In this work, we focus on the variants of GPTQ and ZeroQuant as well as the most straightforward baseline, round-to-nearest neighborhood (RTN).

\textbf{\rtn} directly applies \ptq on the trained data and follows the procedure detailed in Section~\ref{sec:background_of_quantization} to perform the quantization. Specifically, for symmetric quantization, we set $S=max(abs(x))$ and $Z=0$; for asymmetric quantization, we set $S=max(x)-min(x)$ and $Z=min(x)$.

\textbf{\gptq} extends the OBQ~\cite{frantar2022optimal}. It  tries to optimize the following non-linear least square problem,
    $\min_{\hat W} \|Wx - \hat Wx\|_2^2$
where $W$ is the weight, $x$ is the activation, and $\hat W$ is a quantized weight. GPTQ employs second-order methods to obtain a closed-form solution. In addition, the quantization for each weight matrix is performed column-/row-wisely and the quantization errors from previous columns will be passed to those columns not yet quantized. See\cite{frantar2022optimal,frantar2022gptq} for more details.

\textbf{\zqglobal} is the original method proposed in~\cite{yao2022zeroquant}, where authors treat each layer as a small neural network (a.k.a., subnetwork) and use the FP16 subnetwork as the teacher model to distill the quantized one with a few hundred iterations, i.e., $ \min_{\hat \theta} |f_{\theta}(x) - f_{\hat\theta}(x)|2^2,$ where $\theta$ is a set of weights, $\hat \theta$ is the quantized version, $f{\theta}$ is the subnetwork with parameters $\theta$, and $x$ is the input. Thus, it can significantly reduce the GPU resource requirement and time cost.

% \textbf{\zqglobal} is the original method proposed in~\cite{yao2022zeroquant}, where authors treat each transformer layer as a small neural network (a.k.a., subnetwork) and use the unquantized FP16 subnetwork as the teacher model to distill the quantized one with a few hundred iterations, i.e.,
% $ \min_{\hat \theta} \|f_{\theta}(x) - f_{\hat\theta}(x)\|_2^2,$ where $\theta$ is a set of weight, $\hat \theta$ is the quantized version of it, $f_{\theta}$ is the subnetwork with parameters $\theta$, and $x$ is the input.
% Thus, it can significantly 
% reduce the GPU resource requirement and time cost.

\textbf{\zqlocal} is an extension mode of \zqglobal for further GPU requirement reduction and training cost reduction. 
Particularly, instead of using each transformer layer as the subnetwork, we treat each linear layer as the subnetwork. 
This method can be viewed as an iterative first-order optimization method (e.g., SGD) to solve~ $\min_{\hat W} \|Wx - \hat Wx\|_2^2$. 

\textbf{Experimental Setup.} We compare the four methods mentioned above on weight-only and weight-and-activation quantization. As weight quantization is always static (i.e., it does not change during inference), there is virtually no system performance difference between symmetric and asymmetric quantization.\footnote{The bias term (a.k.a., the zero point) can be simply fused into the previous activation quantization kernel~\cite{yao2022zeroquant}.} We use asymmetric quantization for better accuracy, and the conclusions would hold similarly for symmetric quantization. For parameters used for \gptq, \zqlocal, and \zqglobal, please refer to Appendix~\ref{sec:hyperparameter_used_in_exisiting_method_evaluation}. An interesting finding for ZeroQuant is that the hyperparameters (e.g., learning rate and its scheduler) provided in the original work~\cite{yao2022zeroquant} are sub-optimal. In this work, we find the best configurations for \zqlocal and \zqglobal and denote them as \zqlocalstar and \zqglobalstar, respectively, with the best tuned results. To ensure consistent and comparable results, we set a fixed random seed for our experiments. In the context of post-training quantization, varying the random seed has minimal impact on the final results, as indicated in more detail in \tref{tab:mean}.


\textbf{Evaluation of Weight-only Quantization.}\label{sec:weight_only_quantization_existing_method} The results from weight-only quantization using OPT and Bloom are presented in Table~\ref{tab:weight_only_quantization_opt_existing_method_average_in_main_text}. The findings indicate that the larger models tend to be less sensitive to INT4 weight-only quantization. This observation holds true across all methods (\rtn, \gptq, \zqlocalstar, and \zqglobalstar) with the exception of OPT-66B, which shows greater degradation than OPT-30B. It is noteworthy that light-weight optimization-based methods significantly outperform the \rtn baseline in terms of accuracy. For instance, these methods substantially reduce the degradation in perplexity of OPT-30B/66B compared to baseline. Most quantized models with parameters greater than 6.7B fall under Class II, indicating their potential for real-world applications. For instance, the quality of INT4 OPT-30B (66B) is superior to that of INT8 OPT-13B (30B).

% The weight-only quantization results of \opt and \bloom are shown in~\tref{tab:weight_only_quantization_opt_existing_method_average_in_main_text}.
% Similar to \rtn (in~\sref{sec:ptq_challenge}), \gptq, \zqlocalstar, and \zqglobalstar have the same observation, i.e., larger models are less sensitive to INT4 weight-only quantization except for \opt-66B, which has larger degradation than \opt-30B.
% Overall, optimization-based methods have significantly better accuracy performance than the baseline method, \rtn. 
% For instance, optimization-based methods significantly reduce \ppl degradation of \opt-30B/66B compared to \rtn. 
% Most quantized models (>6.7B) lie in \classtwo, which can be potentially deployed for real application (e.g., INT4 \opt-30B (66B) has better quality than INT8 \opt-13B (30B)).

Among the optimization-based methods, \zqglobalstar  generally performs better on smaller models (those with fewer than 1B parameters), while \gptq excels on larger models. \zqlocalstar does not outperform \gptq or \zqglobalstar -â€” a reasonable outcome given that  \gptq  employs a closed-form solution to solve the non-linear quadratic problem and \zqglobalstar optimizes a larger subnetwork. The inferior performance of \zqglobalstar compared to \gptq for larger models is unexpected since \zqglobalstar  optimizes an entire transformer layer while \gptq only optimizes a single linear layer. A plausible explanation is that larger models are more sensitive to weight updates, necessitating more advanced fine-tuning methods.
% \footnote{We also tried to freeze different components of the subnetwork, e.g., layer norm and/or bias, to see if we could get better results.
% However, they all exhibited similar performances.
% }

% However, the worse performance of \zqglobalstar than \gptq for larger models is not initially expected as \zqglobalstar optimizes an entire transformer layer while \gptq only optimizes a single linear layer. 
% One possible reason is that large models are more sensitive to the weight update and a more advanced finetuning method is needed.\footnote{
% We also tried to freeze different components of the subnetwork, e.g., layer norm and/or bias, to see if we could get better results.
% However, they all exhibited similar performances.
% }

\begin{table}[t] %#[t]
\vspace{-0.5cm}
\caption{
The evaluation results of different \ptq methods on \opt and \bloom (BLM) with asymmmetric quantization on weight or (and) activation.
See more details in ~\tref{tab:weight_only_quantization_opt_existing_method_full_in_appendix} and~\tref{tab:weight_only_quantization_bloom_existing_method_full_in_appendix}.
}\centering
\label{tab:weight_only_quantization_opt_existing_method_average_in_main_text}
\begin{adjustbox}{width=0.999\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Precision &    Method               & OPT-6.7b & OPT-13b & OPT-30b & OPT-66b & BLM-1.7b & BLM-3b & BLM-7.1b & BLM-176b \\\midrule
\multicolumn{2}{l}{W16A16}                      & 11.90     & 11.22   & 10.70    & 10.33   & 20.43    & 17.58  & 14.96    & 10.90     \\\midrule
\multirow{4}{*}{W4A16} &\rtn          & 13.44    & 12.09   & 11.52   & 31.52   & 22.47    & 19.01  & 15.90     & 11.20     \\
&\gptq         & 12.28    & 11.42   & 10.78   & 10.52   & 21.58    & 18.33  & 15.50     & 11.02    \\
&\zqlocalstar  & 12.46    & 11.64   & 11.05   & 10.79   & 21.70     & 18.50   & 15.55    & 11.11    \\
&\zqglobalstar & 12.38    & 11.62   & 11.04   & 10.68   & 21.38    & 18.33  & 15.52    & 11.05  \\ \midrule
\multirow{4}{*}{W4A8}
&\rtn          & 14.80     & 26.36   & 86.26   & 815.00     & 22.75    & 19.17  & 16.19    & 12.22    \\
&\gptq         & 13.88    & 17.28   & 20.71   & 648.69  & 21.71    & 18.44  & 15.75    & 11.86    \\
&\zqlocalstar  & 13.24    & 14.23   & 18.53   & 16.32   & 21.86    & 18.66  & 15.75    & 11.19    \\
&\zqglobalstar & 13.17    & 13.07   & 14.65   & 37.82   & 21.43    & 18.39  & 15.58    & 11.49   \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-0.5cm}
\end{table}



\textbf{Evaluation of Weight and Activation Quantization.}
\label{sec:weightactivation_quantization_existing_method}
The evaluation results for existing methods using W4A8 quantization are presented in Table~\ref{tab:weight_only_quantization_opt_existing_method_average_in_main_text}. The three light-weight optimization-based methods  outperform \rtn significantly, underscoring their efficacy. However, all of the results fall into either  \classtwo or \classthree. This suggests that for certain applications, it might be more beneficial to use smaller models with fewer parameters rather than larger, quantized models.

Among quantization-based methods, \zqglobalstar and \zqlocalstar generally outperform GPTQ, which is anticipated given that GPTQ was originally designed for weight-only quantization. \zqglobalstar performs better than  \zqlocalstar in most cases except for the two largest models, OPT-66B and Bloom-176B, despite having larger trainable parameters in one step. This again signifies the need for a more suitable and advanced optimization method for large language models (LLMs).

% work generally better than \gptq, which is expected since \gptq was originally proposed for weight-only quantization. 
% Compared to \zqlocalstar, \zqglobalstar has better performance for most cases except for the two largest models, i.e., \opt-66B and \bloom-176B, even though \zqglobalstar has larger trainable parameters in one step, which again reflects that for \llms, a more suitable and advanced optimization method is needed.

\begin{tcolorbox}
\textbf{Finding 2 on Comparisons.}   \textbf{(1)} \gptq typically performs better for weight-only quantization, while ZeroQuant (including both \zqglobalstar and \zqlocalstar) yields superior results for weight and activation quantization. 
    \textbf{(2)} The tested optimization-based methods cannot achieve \classone quantization error for either INT4 weight-only or W4A8 quantization  with the exception of \gptq on OPT-30B with weight-only quantization.
\end{tcolorbox}

