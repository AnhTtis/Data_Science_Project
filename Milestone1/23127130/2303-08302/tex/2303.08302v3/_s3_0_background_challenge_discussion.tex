\section{Would different model families behave similarly on quantization?}
\label{sec:ptq_challenge}


%\subsection{Post Training Quantization for Large Language Models}
There are mainly two categories of \ptq for \llms, i.e., weight-only quantization~\cite{frantar2022gptq} and weight-and-activation quantization~\cite{dettmers2022llm,yao2022zeroquant,xiao2022smoothquant}. 
In the latter, it is uniformly observed across all studies that activation quantization demonstrates greater sensitivity than weight quantization. However, prior research tends to concentrate on a single (family) model to emphasize the necessity of their proposed quantization technique. A comprehensive and systematic evaluation of this PTQ methodology, particularly the sensitivity of weight/activation quantization for varying model sizes and distinct model families, has yet to be undertaken. Hence, we conduct an examination on both the OPT~\cite{zhang2022opt} and BLOOM~\cite{scao2022bloom} families to elucidate the quantization sensitivity of weight and activation.

\begin{wraptable}{r}{8cm}
%\begin{table}[t]
% \vspace{-0.5cm}
\caption{
Classification of quantization sensitivity (or quantization loss). The sensitivity increases from \classone to \classthree.
}\centering
\label{tab:quantization-loss-table}
\begin{adjustbox}{width=1.0\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Class     & \classone & \classtwo & \classthree \\
\midrule
\ppl Degradation & $\le$0.1 & $>$0.1 \& $\le$0.5 & $>$0.5 \\ 
\bottomrule
\end{tabular}
\end{adjustbox}
% \end{table}
\vspace{-0.2cm}
\end{wraptable}
\textbf{Sensitivity setting.} 
% We use the zero-shot validation perplexity (\ppl) difference on three datasets, i.e., Wikitext-2~\cite{merity2016pointer}, PTB~\cite{marcinkiewicz1994building}, and C4~\cite{colin2019t5}, before and after quantization of those \llms to demonstrate their sensitivity as the \ppl is highly related to zero-shot/few-shot accuracy measurement~\cite{dettmers2022case}. 
% Particularly, a larger \ppl drop means higher quantization sensitivity.
% For simplicity, we also categorize quantization sensitivity (or quantization loss) into 3 different classes as shown in~\tref{tab:quantization-loss-table}.\footnote{
% The threshold is selected since when the model size is about doubled (e.g., 13B vs. 30B, and 30B vs. 66B), the \ppl improvement is about 0.5 (see~\tref{tab:ptq_challenge_opt_average_in_maintext}).
% }
We use the zero-shot validation perplexity (\ppl) differential on three datasets, namely, Wikitext-2~\cite{merity2016pointer}, PTB~\cite{marcinkiewicz1994building}, and C4~\cite{colin2019t5}, before and after the quantization of these LLMs to illustrate their sensitivity, as \ppl is significantly correlated to zero-shot/few-shot accuracy measurement~\cite{dettmers2022case}. Specifically, a higher \ppl drop indicates enhanced quantization sensitivity. For simplicity, we also categorize quantization sensitivity (or quantization loss) into three different classes as depicted in~\tref{tab:quantization-loss-table}. Notably,
the threshold is chosen because when the model size approximately doubles (e.g., 13B vs. 30B, and 30B vs. 66B), the \ppl improvement is about 0.5 (see~\tref{tab:ptq_challenge_opt_average_in_maintext}).  The sensitivity (or loss) incrementally increases as the class number ascends. From a practical standpoint, we favor lower quantization sensitivity (accuracy loss), making \classone the optimal-loss post-training quantization. 

We employ both symmetric and asymmetric quantization to gauge the quantization sensitivity and highlight the advantage of asymmetric quantization. Particularly, we implement per-row quantization~\cite{frantar2022gptq} for weight quantization and per-token quantization for activation~\cite{yao2022zeroquant}.

\textbf{Robustness of Weight-only Quantization for Large Models.} 
The results of weight-only quantization in \opt and \bloom models are summarized in~\tref{tab:ptq_challenge_opt_average_in_maintext}. INT8 weight-only quantization, either symmetric or asymmetric, results in negligible accuracy loss (less than 0.05, i.e., \classone). Consequently, for tasks oriented towards generation, FP16 weight can simply be replaced with INT8 weight to reduce memory usage. For INT4 quantization, the asymmetric method outperforms the symmetric approach in accuracy, attributable to its superior utilization of the quantization range. Interestingly, larger models exhibit better tolerance to low-precision quantization (i.e., INT4) than smaller models, with a few exceptions such as \opt-66B.\footnote{\cite{frantar2022gptq} discovered that \opt-66B has a high proportion of dead neurons in the early layers, which might influence the compression capability. We also identify another potential reason: the Layer Norm of the \opt-family is not well trained (except \opt-350M), with the weight and the bias being all 1's and 0's, respectively.} Particularly, \bloom-176B shows \ppl degradation (around 0.3 points) in \classtwo, which could explain why the large GLM-130B~\cite{zeng2022glm} can operate with INT4 weight-only quantization out of the box with acceptable accuracy impact.

\begin{table}[H]
\caption{
Average \ppl of \opt and \bloom (BLM). 
See~\tref{tab:ptq_challenge_opt_full_in_appendix} for all results.
}\centering
\label{tab:ptq_challenge_opt_average_in_maintext}
\begin{adjustbox}{width=0.999\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Precision     &  OPT-6.7b	& OPT-13b &OPT-30b	& OPT-66b   & BLM-1.7b  & BLM-3b & BLM-7.1b & BLM-176b\\
\midrule
W16-A16     & 11.90 & 11.22   & 10.70   & 10.33   & 20.43 & 17.58 & 14.96 & 10.90 \\\midrule
W8\sym-A16  & 11.90 & 11.22   & 10.70   & 10.33   & 20.43 & 17.59 & 14.97 & 10.90 \\
W8\asym-A16 & 11.90 & 11.22   & 10.70   & 10.33   & 20.45 & 17.59 & 14.97 & 10.90 \\ \midrule
W4\sym-A16  & 14.36 & 12.73   & 11.77   & 97.05   & 23.18 & 19.36 & 16.27 & 11.28 \\
W4\asym-A16 & 13.44 & 12.09   & 11.52   & 31.52   & 22.47 & 19.01 & 15.90  & 11.20  \\ \midrule
W16-A8\sym  & 26.04 & 3171.49 & 2048.21 & 2638.09 & 20.68 & 17.73 & 15.28 & 12.10  \\
W16-A8\asym & 12.62 & 15.36   & 23.57   & 561.35  & 20.52 & 17.65 & 15.14 & 11.62\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\textbf{Challenge Encountered in Activation Quantization for Large Models.}
Activation quantization has consistently proven more difficult than weight quantization~\cite{yao2022zeroquant,dettmers2022llm}, as illustrated in \tref{tab:ptq_challenge_opt_average_in_maintext}. When compared to weight-only quantization, activation-only quantization indicates that asymmetric quantization can significantly improved performance over symmetric quantization. Moreover, contrary to weight-only quantization, smaller models typically exhibit better tolerance to activation quantization, as their hidden dimension is smaller and the activation dynamic range is also narrower than larger models~\cite{yao2022zeroquant}. It should be noted that for models larger than 10B, all fall into \classthree, indicating a degradation of more than 0.5 \ppl points. 

% In the lat two rows of \tref{tab:ptq_challenge_opt_average_in_maintext}, we see that  different model families have significantly different behaviors. 
% \bloom has no divergence issues up to 176B model size but \opt has very poor performance from 6.7B model size (the larger models with INT8 activation have even worse \ppl). 
% This may be caused again by the layer norm issue of \opt-family\samethanks. 

The last two rows of \tref{tab:ptq_challenge_opt_average_in_maintext} show that different model families exhibit significantly different behaviors. \bloom does not exhibit divergence issues even up to a model size of 176B, whereas \opt displays very poor performance from a model size of 6.7B (larger models with INT8 activation have even worse \ppl). This could again be attributed to the Layer Norm issue within the \opt-family\samethanks.

 
% \begin{tcolorbox}
%  \textbf{Findings 1 on Sensitivity Analysis.} \textbf{(1)} INT8 weight-only quantization can be used as a standard (almost) no-accuracy-degradation way to help reduce memory cost for \llms.  \textbf{(2)} INT4 weight-only quantization for small models leads to significant accuracy degradation (\classthree) and this effect diminishes as the model size becomes larger (\classtwo).
% \textbf{(3)} In contrast to (2), INT8 activation leads to minimal accuracy drops for small models (\classone) and the drops become larger for larger models (\classthree). \textbf{(4)} For INT8 activation, \bloom has no divergence issues up to 176B model size but \opt has very poor performance from $\geq$ 6.7B model sizes.
% \end{tcolorbox}

\begin{tcolorbox}
\textbf{Findings 1 on Sensitivity Analysis.} \textbf{(1)} INT8 weight-only quantization can serve as a standard method for reducing memory costs in \llms, with negligible degradation in accuracy. \textbf{(2)} INT4 weight-only quantization for small models results in substantial accuracy degradation (\classthree), but this effect lessens as the model size increases (\classtwo).
\textbf{(3)} Contrary to (2), INT8 activation results in minimal accuracy drops for small models (\classone) but larger models exhibit greater drops (\classthree). \textbf{(4)} With INT8 activation, \bloom shows no divergence issues up to a model size of 176B, whereas \opt performs poorly from $\geq$ 6.7B model sizes.
\end{tcolorbox}