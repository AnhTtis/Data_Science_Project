\subsection{Fine-grained Quantization and Its Evaluation}
With \ptq and row-wise quantization, achieving \classone quantization error is challenging for both weight-only and weight-and-activation quantization. Generally, utilizing a smaller model with INT8 weight is more advantageous than employing a model that is twice as large with INT4 weight.

One potential solution to this issue is the implementation of finer-grained quantization schemes~\cite{darvish2020pushing}, where every $k$ elements possess their own scaling factor and/or zero point. This approach can significantly reduce quantization error. In the extreme case, where every single element has its own scaling factor, the original FP16 number can be precisely recovered. Importantly, block-k quantization can be implemented on modern GPUs, one of the most prevalent deep learning architectures, since the compute unit (streaming multiprocessor) of GPUs processes tiles of data (e.g., 128 by 128 tiling size) for matrix computation.
% One way to solve this problem is to use finer-grained quantization schemes~\cite{darvish2020pushing}, i.e., every $k$ elements have their own scaling factor and/or zero point. 
% This can significantly reduce the quantization error. 
% For the extreme case, i.e., every $1$ element has its own scaling factor, we can exactly recover the original FP16 number.
% More importantly, such block-k quantization can be implemented on modern GPU (one of the most popular deep learning architectures) since the compute unit (streaming multiprocessor) of GPU process tiles of data (e.g., 128 by 128 tiling size) for matrix computation.


% \subsection{Settings}
% Although fine-grained quantization can greatly close the gap between the quantized tensor and its floating point counterpart, later we show that it still leaves a non-trivial accuracy gap if \rtn is applied. 
% Therefore, built upon fine-grained quantization, we also apply the existing optimization-based methods to further boost the accuracy. 
% Particularly, we use \gptq and \zqglobal for all models and settings, and use \zqlocal for \opt-66B and \bloom-176B.
% For hyper-parameters used for \zqglobal and \zqlocal, we choose the top three found in~\sref{sec:evaluation_of_existing_methods} for all models except for \bloom-176B, for which we only use the top-one hyperparameter, to reduce the training cost.

Although fine-grained quantization can substantially narrow the gap between the quantized tensor and its floating-point counterpart, the application of \rtn still results in a non-trivial accuracy gap. Consequently, we build upon fine-grained quantization by employing existing optimization-based methods to further enhance accuracy. Specifically, we utilize \gptq and \zqglobal for all models and settings and apply \zqlocal to OPT-66B and Bloom-176B. For the hyperparameters used in \zqglobal and \zqlocal, we select the top three identified in Section~\ref{sec:evaluation_of_existing_methods} for all models, except for Bloom-176B, for which we only use the top-performing hyperparameter  to reduce training costs.