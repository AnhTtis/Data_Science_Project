\section{Related Work}
\label{sec:related_work}
Different quantization methods~\cite{shen2020q,zafrir2019q8bert,fan2020training,zhang2020ternarybert,bai2020binarybert,esser2019learned,tao2022compression,kim2021bert} for transformer-based models~\cite{vaswani2017attention} have been explored for a while.
However, most of those works need quantization-aware finetuning or even expensive quantization-aware knowledge distillation~\cite{hinton2015distilling}.
Due to the cost of training/finetuning \llms \cite{polino2018model,jiao2019tinybert,tao2022compression,wu2022extreme,wu2023understanding}, it is a challenge for practitioners/researchers to do finetuning/distillation on those \llms, particularly for models like GPT-3-175B~\cite{brown2020language} and \bloom-176B~\cite{scao2022bloom}. 

Post-training quantization (\ptq)~\cite{zadeh2020gobo,bondarenko2021understanding} is an alternative way to quantize the model with no/minimal finetuning requirement.
Along this line, several recent works focus on \llms (beyond the million-parameter scale).
\cite{yao2022zeroquant} proposes vector-based INT8 quantization with layer-by-layer knowledge distillation to overcome the training cost and quantization error introduced by \llms. 
\cite{dettmers2022llm} uses similar vector-based INT8 quantization weight plus mixed-precision (INT8/FP16) quantization for activation to overcome the sensitivity of activation quantization. 
However, the inference speed of \cite{dettmers2022llm} is generally even slower than FP16 baseline~\cite{bloom_inference} due to the difficulty of implementing mixed-precision calculation within a single tensor.
More recently, \cite{frantar2022gptq} extends OBQ~\cite{frantar2022optimal,hassibi1993second, lecun1990optimal}    on \llms for INT4 weight-only quantization and shows great efficiency on quantization and latency,
and \cite{xiao2022smoothquant} shows the outliers from activations can be smoothed out by migrating the quantization difficulty from activations to its associated weights. 
However, \cite{xiao2022smoothquant} can only work for W8A8 quantization as lower weight precision (INT4) itself already leads to significant accuracy degradation, and the accuracy drop is larger than 0.1 \ppl points, which as discussed in the later section is sub-optimal.
\cite{dettmers2022case} shows the scaling law of weight-only quantization with the simplest round-to-nearest baseline, but it does not consider the weight-and-activation quantization and/or the above \ptq optimization methods.
As can be seen from~\fref{fig:main_figure}, by using \ptq optimization methods, the model quality can be significantly improved. 
Please also see~\appref{sec:full_tables_results} for more detailed numbers.

Different than existing works, our paper extensively tests the effect of (1) different quantization schemes, e.g., symmetric and asymmetric quantization, (2) different \ptq methods, e.g.,~\cite{yao2022zeroquant,frantar2022gptq}, (3) different model families, e.g.,~\cite{scao2022bloom,zhang2022opt}, (4) different quantization coverage, e.g., weight-only and weight-and-activation quantization, and (5) other discussions, e.g., the effect of quantization granularity. 
As such, we provide a much more comprehensive understanding of post-training quantization for large language models compared to the previous works.
% \xiaoxia{do we need to add something in LoRC?}