\section{Proposed Method to Further Push the Limit of Post-training Quantization}
\label{sec:design}
% Based on the studies and findings in previous sections, we now see the necessary for a better method to further optimize the current existing method such that we fully achieve the original fp16 \ppl quality. Here we proposed an innovative method called \textbf{LoRC}: \textbf{Lo}w \textbf{R}ank \textbf{C}ompensation, which employs low-rank matrix factorization on the quantization error matrix (denoted as $E$). Recall in \sref{sec:evaluation_of_existing_methods} that $W$ and $\hat W$ are  origial and quantized weight matrices, respectively. We have 
% $W = \hat{W} + E$. LORC with $m$ dimension low-rank matrices to approximate the error matrix $E\in\mathbb{R}^{d_\text{in}\times d_\text{out}}$ can be summarized into two steps: \textbf{(1)} Applying the singular value decomposition (SVD) on $E=U\Sigma^{\text{sorted}} V$ where 
% $U\in\mathbb{R}^{d_\text{in}\times d_\text{in}}$ and $V\in \mathbb{R}^{d_\text{out} \times d_\text{out}}$ are unitary matrices, and $\Sigma^{\text{sorted}} \in\mathbb{R}^{d_\text{in}\times d_\text{out}}$ is a diagonal matrix with the diagonal elements sorted from large to small; \textbf{(2)} Obtain an low-rank compensation matrix $\hat{E} = U^{\sigma}_{m} V^{\sigma}_{m}$ where we define the two low-rank matrices as $U^{\sigma}_{m}:= U_m(\Sigma_m^{\text{sorted}})^{\frac{1}{2}}$ and $V^{\sigma}_{m}:=(\Sigma_m^{\text{sorted}})^{\frac{1}{2}} V_m$. Here $U_m=U_{:, 1:m}\in\mathbb{R}^{d_\text{in}\times m}$ and $V_m=V_{1:m,:}\in\mathbb{R}^{ m \times d_\text{out}}$ 
Building on the investigation and conclusions drawn from previous sections, it has become apparent that there is still a need for an advanced methodology to further refine the existing methods, with the objective of fully realizing the original fp16 \ppl quality. In this section, we introduce a simple yet effective method called \textbf{LoRC} (Low Rank Compensation) to optimize the current existing quantization error and further bridge the gap between the quality of the original model and its quantized counterparts.

LoRC is inspired by the employment of low-rank matrix factorization on the quantization error matrix $E:=W- \hat{W}$, where $W$ represents the original weight and $\hat{W}$ is the quantized weight. LoRC approximates the error $E$ with $\hat{E}=\hat{U}\hat{V}$ by using two low-rank matrices $\hat{U}$ and $\hat{V}$. This results in a more accurate approximation of the original weight matrix $W$ by $\hat{W}_{\text{lorc}} = \hat{W} + \hat{E}$, thereby reducing quantization errors: $\|W-\hat{W}\|\geq \|W-\hat{W}_{\text{lorc}}\|$. LoRC consists of two  steps:

\textbf{Step I:} Implement Singular Value Decomposition (SVD) on the error matrix $E = U \Sigma V$, where $U \in\mathbb{R}^{d_\text{in}\times d_\text{in}}$ and $V \in \mathbb{R}^{d_\text{out} \times d_\text{out}}$ are unitary matrices, and $\Sigma \in\mathbb{R}^{d_\text{in}\times d_\text{out}}$ is a diagonal matrix with its diagonal elements ordered in a descending manner.

\textbf{Step II:} We formulate the matrix $\hat{E} = \hat{U} \hat{V}$ where $\hat{U}= U_m(\Sigma_m)^{\frac{1}{2}}$ and $\hat{V}= (\Sigma_m)^{\frac{1}{2}} V_m$. Here, $U_m = U_{:, 1:m} \in\mathbb{R}^{d_\text{in}\times m}$, $V_m = V_{1:m, :} \in\mathbb{R}^{ m \times d_\text{out}}$, and $\Sigma_m = \Sigma_{1:m, 1:m} \in\mathbb{R}^{ m \times m}$.

The objective of LoRC is to achieve a good approximation of the error matrix $E$ using low-rank matrices, with minimal impact on the increase in model size. For instance, consider the standard transformer models~\cite{vaswani2017attention}, where each layer is comprised of a multi-headed attention (MHA) module and a multi-linear perception (MLP) module. Let $h$ represent the hidden dimension and $l$ the number of layers. The total number of parameters is $12lh^2$ as each layer contains $4h^2$ for MHA (for key, query, value, and projection matrices), and $8h^2$ for MLP (two matrices of sizes $h\times 4h$ and $4h\times h$). With the addition of low-rank LoRC to the six matrices in each layer, the total number of parameters for $l$ layers would amount to $18hml$.\footnote{In the MHA module, LoRC contributes $2hm$ to each of key, query, value, and the projection matrices. In the MLP module, LoRC contributes $8hm$ and $2hm$ respectively to the matrices of dimensions $h\times 4h$ and $4h\times h$.} Consequently, the ratio of parameters added to the existing model is $3m/2h$. It's important to note that the low-rank dimension $m$ can be as small as $4$ or $8$ (which we will discuss in detail in a later section) while the standard hidden dimension $h\ge 768$, making the number $3m/2h\leq 0.016$. 

 % LoRC aims to find a better approximation of the error matrix $E$ by using low-rank matrices, while impacting little on the model-size increase. For example, in the standard transformer models where each layer consists of multi-headed attention (MHA) module  and multi-linear perception (MLP) module. Denote $h$ the hidden dimension and $l$ the number of layers. In total, the number of parameter is $12lh^2$ as  in each layer, there are $4h^2$ for MHA (key, query, values and the projection matrices), and $8h^2$ for MLP (two matrices with size  $h\times 4h$ and $4h\times h$). Now with low-rank LoRC added to the six matrices in each layer, the total number of parameter for $l$ layers would be $18hml$.\footnote{In MHA module, lorc counts $2hm$ for each of key, query, values and the projection matrices. In MLP model, lorc counts $8hm$ and $2hm$  respectively for the matreces of dimension $h\times 4h$ and $4h\times h$ respectively.} Thus, the ratio of parameter added to the existing model is $3m/2h$.  Note the low-rank dimension $m$ can be as small as $4$ or $8$ (we will have more discussion in the later section) while the standard hidden dimension $h\ge 768$ making the number  $3m/2h\leq 0.016$.
 % More over, we find that the two low-rank matrices $\hat{U}$ and $\hat{V}$ can be represented by $8$-bit.
 % Thus, in the experiments,  we by default approximate $\hat{E}=\hat{U}_{\text{int8}}\hat{V}_{\text{int8}}$ where $\hat{U}_{\text{int8}}$ and $\hat{V}_{\text{int8}}$ are 8-bit matrices reducing the size of LoRC parameters by two times compare to their FP16 counterpart. 

 
Significantly, LoRC can be viewed as a supplementary feature to existing quantization methodologies such as RTN, GPTQ, and ZeroQuant-Local/Global, and can be seamlessly integrated with FGQ. We have conducted experiments to evaluate the performance of LoRC on both \opt and \bloom, applying 4-bit, 3-bit, and 2-bit weights by setting the activation to FP16.\footnote{For INT8 Activation, please see \tref{tab:LORC-int8}, the observation for FP16 holds similarly for INT8 Activation.} Based on the discoveries in the preceding sections, we utilize the GPTQ quantization strategy. To gain a comprehensive understanding of LoRC, we include the results with and without the application of FGQ. The datasets and hyperparameters are consistent with those detailed in earlier sections.

 % Meanwhile, it is important to see that our LoRC is complementary to the current existing quantization methods such as RTN, GPTQ and ZeroQuant-local/Global. Moreover, it can be seamless combined with FGQ. 






\begin{table}
\caption{
 W\#\asym-A16 quantization with \# being 4-bit, 3-bit and 2-bit on \opt and \bloom (BLM). 
}\centering
\label{tab:lorc-4bit-blocksize}
\begin{adjustbox}{width=1.01\linewidth}
\centering
\begin{tabular}{lc|ccccc|ccccccc }
\toprule
 \multirow{2}{*}{Bits}            & \multirow{2}{*}{\small \textbf{LoRC}}       & \multicolumn{5}{c|}{Coarse-grained weight quantization (per-row block-size)}     & \multicolumn{5}{c}{Fine-grained quantization on weight (256 block-size )}                \\
                  &   &  \small OPT-6.7b & \small OPT-13b &  \small OPT-30b &  \small OPT-66b & \small BLM-176b & \small OPT-6.7b & \small OPT-13b &  \small OPT-30b &  \small OPT-66b & \small BLM-176b \\\midrule
    \multicolumn{2}{c|}{W8A16}                      & 11.90     & 11.22   & 10.70    & 10.33   & 10.90       & 11.90     & 11.22   & 10.70    & 10.33   & 10.90       \\\midrule
\multirow{2}{*}{W4A16} & \xmark     & 12.28    & 11.42   & 10.78   & 10.78   & 11.02      & 12.05    & 11.28   & 10.74   & 10.50    & 10.95      \\
                       & \cmark    & 12.10    & 11.36   & 10.76   & 10.34   & 10.98      & 11.99    & 11.29   & 10.70    & 10.29   & 10.93      \\ \midrule
\multirow{2}{*}{W3A16} & \xmark     & 14.18    & 12.43   & 11.28   & 17.77   & 49.46      & 12.79    & 11.63   & 10.9    & 11.34   & 11.13      \\
                       & \cmark    & 13.00     & 11.90    & 11.14   & 10.63   & 11.30       & 12.40    & 11.57   & 10.83   & 10.42   & 11.08      \\\midrule
\multirow{2}{*}{W2A16} & \xmark    & 120.56   & 40.17   & 25.74   & 225.45  & Explode    & 23.13    & 15.55   & 12.68   & 308.49  & 12.64      \\
                       & \cmark    & 24.17    & 18.53   & 14.39   & 13.01   & 14.15      & 16.27    & 14.30   & 12.37   & 11.54   & 12.21    \\
\bottomrule
\end{tabular}
\end{adjustbox}
% \vspace{-0.5cm}
\end{table}

\textbf{Evaluation Results.} The findings are showcased in \tref{tab:lorc-4bit-blocksize}, split into two sections: coarse-grained weight quantization (per-row) and fine-grained quantization (block-size 256). Notably, we observe that the two low-rank matrices, $\hat{U}$ and $\hat{V}$, can be quantized to 8-bit without any performance discrepancy (\tref{tab:lorc-int8}). Thus, the two low-rank matrices for LoRC in \tref{tab:lorc-4bit-blocksize} are INT8 with a low-rank dimension of $m=8$. 

\begin{wraptable}{r}{8cm}
% \vspace{-0.5cm}
\caption{
\small Results of W4\asym A16 quantization with LoRC approximating $\hat{E}=\hat{U}\hat{V}$ on OPT model family.  $\hat{U}$ and $\hat{V}$ can be represented with FP16 or INT8, of which the performance are represented below. There is hardly any difference between FP16 and INT8.  
}\centering
\label{tab:lorc-int8}
\begin{adjustbox}{width=1.01\linewidth}
\centering
\begin{tabular}{l|cccc|ccccccccc }
\toprule
LoRC          & \multicolumn{4}{c|}{Coarse-grained weight quantization} & \multicolumn{3}{c}{Fain-grained weight Quantization} \\
$\hat{U},\hat{V}$           & 6.7b           & 13b          & 30b          & 66b          & 6.7b                & 13b                & 30b                \\\midrule
 FP16 & 12.08              & 11.35            & 10.76            & 10.31            & 11.993                  & 11.290                 & 10.703                 \\
INT8 & 12.10              & 11.36            & 10.76            & 10.34            & 11.987                  & 11.290                 & 10.700   \\
\bottomrule
\end{tabular}
\end{adjustbox}
% \vspace{-0.5cm}
\end{wraptable}

Several key observations can be made. Firstly, LoRC consistently boosts performance across all bit sizes and block sizes, as indicated by the lower perplexity scores when LoRC is activated. Secondly, the enhancement brought about by LoRC becomes more substantial as the bit size diminishes, especially noticeable for W2A16, which displays a markedly greater impact compared to W4A16 and W3A16 in most scenarios. Lastly, the combination of fine-grained quantization with LoRC yields the most impressive results, underscoring the efficacy of LoRC when integrated with FGQ.
Overall, the results emphasize the benefits of using LoRC for enhanced performance in weight quantization and its compatibility with FGQ. Notably, recovering the last 0.05-0.1 perplexity can be challenging, but with LoRC, we are able to nearly recover the original model quality for INT4 quantization.
\begin{table}[t]
\vspace{-0.05cm}
\begin{minipage}[c]{0.4\textwidth}
     \centering
     % \begin{subfigure}[b]{0.59\textwidth}
% \begin{wraptable}{r}{6cm}
\vspace{-0.55cm}
\caption{ \small W4A16 quantization with LoRC by varying the low-rank dimension $m$. }\centering
\label{tab:lorc-dim}
\begin{adjustbox}{width=0.99\linewidth}
\centering
% \vspace{-10.cm}
\begin{tabular}{l|cccccccc }
\toprule
% LoRC-dim $m$ &  baseline     & $m=1$     & $m=4$     & $m=8$     & $m=16$    & $m=32$    \\\midrule
% OPT-1.3b       & 15.95 & 15.93 & 15.73 & 15.76 & 15.74 & 15.71 \\
% OPT-6.7b       & 12.06 & 12.01 & 12.00 & 11.99 & 12.00 & 12.01 \\
% OPT-30b        & 10.73 & 10.73 & 10.72 & 10.70 & 10.69 & 10.69 \\
LoRC-dim $m$ & OPT-1.3b  & OPT-6.7b  &  OPT-30b   \\\midrule 
$m=0$ basline          & 15.95 & 12.06 & 10.73 \\\midrule 
$m=1$          & 15.93 & 12.01 & 10.73 \\
 $m=4$            & 15.73 & 12.00 & 10.72 \\
$m=8$         & 15.76 & 11.99 & 10.70 \\
$m=16$         & 15.74 & 12.00 & 10.69 \\
$m=32$          & 15.71 & 12.01 & 10.69\\
\bottomrule
\end{tabular}
\end{adjustbox}
 % \end{subfigure}
% 
\end{minipage}
 % \end{varwidth}%
\begin{minipage}[c]{0.59\textwidth}
\centering
\includegraphics[width=0.85\textwidth]{figures/eigenvalues.png}
 \captionof{figure}{ \small Eigenvalues of the Error matrix $E$ for W4A16}\label{fig:Eigenvalues}
\end{minipage}
  % \end{subfigure}
  % \vspace{-0.75cm}
\end{table}
% \end{figure}

\textbf{Ablation Study on the Low Rank Dimension $m$.} An essential aspect of the LoRC method is on the optimal low-rank dimension, denoted as $m$, explained in \textbf{Step II}. To explore this, we varied $m$ in the range of 1, 4, 8, 16, and 32 for OPT-1.3b/6.7b/30b models, and applied W4A16 GPTQ quantization. The outcomes are depicted in Table \ref{tab:lorc-dim}, indicating that the enhancements achieved through LoRC begin to plateau as the dimension $m$ surpasses 4. The most optimal performance for OPT-6.7b is realized when $m=8$.

This observation may seem counterintuitive initially, as one might anticipate that larger LoRC dimensions would yield more significant improvements. To gain a more comprehensive understanding, we conducted an analysis of the eigenvalues of the actual error matrix $E=W-\hat{W}$ for each matrix. By randomly selecting 20 matrices from MHA and MLP layers, we plotted the eigenvalues of $E$ as a curve, depicted in \fref{fig:Eigenvalues}. The two plots reveal a rapid flattening of eigenvalues after index 8, which elucidates why increasing the LoRC dimension does not considerably enhance performance. Hence, a sensible dimension for $\hat{U}$ and $\hat{V}$ in the LoRC methodology could be 8.\footnote{Please note that this observation is only true for PTQ. If one uses quantize-aware training (QAT) and let $\hat{U}$ and $\hat{V}$ updated during QAT, we arrive at contrasting conclusions. For more details, please refer to \appref{sec:qat-lorc}.}


% An important question regarding the LoRC method is determining the optimal dimension $m$. To investigate this, we vary $m$ across 1, 4, 8, 16, and 32 for OPT-1.3b/6.7b/30b, and implemented W4A16 GPTQ quantization. The results can be found in Table \ref{tab:lorc-dim}, which shows that improvements using LoRC plateau as the dimension $m$ exceeds 4. The best performance for OPT-6.7b is achieved when $m=8$. 

% The above observation might be counterintuitive because one would expect larger LoRC dimensions to yield greater improvements. To better understand this, an analysis of the eigenvalues of the true Error matrix $E=W-\hat{W}$ for each matrix was performed. By randomly selecting 20 matrices for attention and MLP layers, the eigenvalues of $E$ were plotted as a curve, as shown in \fref{fig:Eigenvalues}. The two plots reveal that the eigenvalues flatten quickly after index 8, which explains why increasing the LoRC dimension does not significantly improve performance.
% Thus, the reasonable dimensions for $\hat{U}_{\text{int8}}$ and $\hat{V}_{\text{int8}}$ in the LoRC method  could be 8.
