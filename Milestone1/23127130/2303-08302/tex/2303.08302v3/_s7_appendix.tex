\appendix
\section{Background of Quantization}
\label{sec:background_of_quantization}
Quantization maps floating point (e.g., FP16/FP32) numbers to integer numbers (e.g., INT4/INT8) so that lower memory usage (weight quantization) and faster integer arithmetic (weight-and-activation quantization) can be achieved compared to the floating point format. 
In this work, we are focusing on uniform quantization, i.e., 
\begin{equation}
\small
\label{eq:quantization_formula}
Q(x) = \text{INT}\big({(x-Z)}/{S}\big)-Z,
\end{equation}
where $Q$ is the quantization function, $x$ is a floating point input vector/tensor, $S$ is a real valued scaling factor, and $Z$ is an integer zero point. 
Based on different settings, the quantization method can be viewed as (1) symmetric vs. asymmetric quantization ($Z=0$ or not), (2) fine-grained vs. coarse-grained quantization (how to partition the input x and get its associated scaling factor, e.g., matrix wise or row wise). 
See~\cite{gholami2021survey} for more details. 

Throughout this work, we focus on post-training quantization (\ptq), i.e., no or minimal training effort is applied after quantization, for which large accuracy degradation usually exhibits for coarse-grained quantization (per matrix/tensor) due to their large quantization error.
As such, we focus on fine-grained quantization.
Particularly, we use the per-row quantization (one row of the weight matrix or one token for the activation) from~\cite{yao2022zeroquant} as our coarsest-grained quantization method, 
and we use block-k quantization (for every k elements, they have their own scaling factor and/or zero point) as our finer-grained quantization scheme. 

%%%%%%%%%%%%%%%%%%%
% Re-count the Figure/Algorithm/Tables after this point. 
%%%%%%%%%%%%%%%%%%%
\counterwithin{figure}{section}
\counterwithin{table}{section}


\section{Detailed Setting Used in~\sref{sec:evaluation_of_existing_methods}}
\label{sec:hyperparameter_used_in_exisiting_method_evaluation}
Same as~\cite{frantar2022gptq}, for all methods, we use C4 dataset to randomly select 128 sentences for training and each of them has 2048 tokens.

For \gptq, we check its main hyperparameter, i.e., the dampening factor, and find out the method is not sensitive to it.
As such, we use the hyparameter suggested by the author for all of our experiments. 
For \zqglobal and \zqlocal, as mentioned the in main text, the hyperparameters suggested by the original work~\cite{yao2022zeroquant} is suboptimal. 
We find that a linear decay learning rate schedule is very helpful in our initial test.
As such, we add this as our default setting.
Meanwhile, we extensively test a wide range (1e-3 to 5e-8) of learning rate for different models until we find the best learning rate (i.e., larger or smaller learning rate leads to worse accuracy performance).We employed the Adam optimizer and set the default batch size to 1 for our experiments. 

We conducted tests to assess whether changes in random seeds would introduce substantial variations in the outcomes. As per the findings detailed in Table \tref{tab:mean}, the modifications in random seeds resulted in only minimal effects on the final quality of the models. This effect was particularly negligible in the context of larger models, such as OPT-30b, where the standard deviation was only 0.01. Therefore, in consideration of these results, we elected to standardize the random seed for the subsequent experiments presented in this paper, setting it uniformly at 123 or 0. The code will be made publicly available to facilitate reproducibility of our results. 

For all three methods, we run them on a single GPU (either V100-32GB or A100-80GB). 
For the largest model tested in the paper, i.e., \bloom-176B, the cost of all methods is lower than one GPU-day on A100-80G.


\begin{table}[H]
\caption{The table on the left illustrates the outcomes of each task, evaluated using three different random seeds. On the right, we present a table detailing the mean and standard deviation of the Task-mean values (which can be found in the final column of the left table) over the three random seeds, accompanied by additional quantization results. The quantization methodologies employed in this context are based on the \gptq algorithm.
}\centering
\label{tab:mean}
\begin{adjustbox}{width=0.99\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
% Precision     & 560m   &1.1b   & 1.7b  & 3b & 7.1b & 176b \\
Precision              & Random Seed & WikiText  & PTB  & C4    & Task-mean \\\midrule
\multirow{2}{*}{OPT-13b} &123  & 10.31 & 12.62 & 11.35 & 11.43     \\
\multirow{2}{*}{W4A16}&234  & 10.25 & 12.57 & 11.35 & 11.39     \\
&456  & 10.37 & 12.61 & 11.36 & 11.44     \\\midrule
\multirow{2}{*}{OPT-30b} &123  & 9.56  & 11.95 & 10.79 & 10.77     \\
\multirow{2}{*}{W4A16}&234  & 9.6   & 11.95 & 10.79 & 10.78     \\
&456  & 9.52  & 11.97 & 10.79 & 10.76  \\
\bottomrule
\end{tabular}
\begin{tabular}{lcccccccccccccc }
\toprule
% Precision     & 560m   &1.1b   & 1.7b  & 3b & 7.1b & 176b \\

Precision              & Items & OPT-1.3b & OPT-13b & OPT-30b \\\midrule
\multirow{2}{*}{W4A16} & mean over three random seeds  & 16.39    & 11.42   & 10.77   \\
                       & standard deviation   & 0.019    & 0.027   & 0.010   \\\midrule
\multirow{2}{*}{W4A8}  & mean over three random seeds  & 16.76    & 17.16   & 21.64   \\
                       & standard deviation    & 0.048    & 0.048   & 1.277  \\
\bottomrule
\end{tabular}

\end{adjustbox}
\end{table}

\section{Best \ptq Methods with Per-row Quantization}
Table~\ref{tab:opt-quantization-method} and~\ref{tab:bloom-quantization-method} summarize the best \ptq methods with per-row optimization.

\begin{table}[t]
\caption{
Best optimization method of \opt family in~\sref{sec:evaluation_of_existing_methods}.
}\centering
\label{tab:opt-quantization-method}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Precision     & 125m	& 350m 	& 1.3b	& 2.7b	& 6.7b	& 13b &30b	& 66b \\
\midrule
Weight Only (INT4) &\zqglobal &\zqglobal &\gptq &\gptq &\gptq &\gptq &\gptq &\gptq\\
\midrule
Weight \& Activation (W4A8) &\zqglobal &\zqglobal &\zqglobal &\gptq &\zqglobal &\zqglobal &\zqglobal &\zqlocal\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[t]
\caption{
Best optimization method of \bloom family in~\sref{sec:evaluation_of_existing_methods}.
}\centering
\label{tab:bloom-quantization-method}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc}
\toprule
Precision     & 560m   &1.1b   & 1.7b  & 3b & 7.1b & 176b \\
\midrule
Weight Only (INT4) &\gptq &\zqglobal &\zqglobal &\zqglobal/\gptq &\gptq &\gptq\\
\midrule
Weight \& Activation (W4A8) &\zqglobal &\zqglobal &\zqglobal &\zqglobal &\zqglobal &\zqlocal \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


% \section{3-bit Weight-only Quantization}
% \label{sec:3bit_weightonly_quantization}
% \begin{table}[t]
% \caption{
% Results of W3\asym-A16 quantization on \opt with various block-size out of the best result from optimization-based methods. 
% See~\tref{tab:opt-3bit-blocksize-full} for full results including \rtn.
% N/A means that the block size is not divisible by the hidden size.
% }\centering
% \label{tab:opt-3bit-blocksize}
% \begin{adjustbox}{width=0.9\linewidth}
% \centering
% \begin{tabular}{lcccccccccccccc }
% \toprule
% Block-size     & 125m	& 350m 	& 1.3b	& 2.7b	& 6.7b	& 13b &30b	& 66b \\
% \midrule 
% W16-A16 &28.27 &22.93 &15.44 &13.58 &11.90 &11.22 &10.70 &10.33\\

% \midrule
% Per-row &46.82 &30.30 &22.36 &17.06 &14.18 &12.43 &11.28 &17.77 \\
% 1024 &N/A &29.62 &20.16 &N/A &12.90 &11.74 &11.03 &12.95 \\
% 512 &N/A &28.65 &18.94 &15.47 &12.82 &11.67 &10.97 &12.33 \\
% 256 &38.85 &27.92 &17.95 &15.10 &12.79 &11.63 &10.90 &11.34 \\
% 128 &36.80 &26.97 &17.61 &15.05 &12.69 &11.59 &10.91 &11.27 \\
% 64 &35.48 &26.76 &17.40 &14.85 &12.58 &11.62 &10.92 &10.97\\
% 32 &33.75 &26.38 &17.11 &14.73 &12.64 &11.70 &10.99 &10.95\\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{table}

% \begin{table}[t]
% \caption{
% Results of W3\asym-A16 quantization on \bloom with various block-size out of the best result from optimization-based methods. 
% See~\tref{tab:bloom-3bit-blocksize-full} for full results including \rtn.
% N/A means that the block size is not divisible by the hidden size.
% }\centering
% \label{tab:bloom-3bit-blocksize}
% \begin{adjustbox}{width=0.9\linewidth}
% \centering
% \begin{tabular}{lcccccccccccccc }
% \toprule
% Block-size     & 560m   &1.1b   & 1.7b  & 3b & 7.1b & 176b \\
% \midrule 
% W16-A16  &29.35 &28.32 &20.43 &17.58 &14.96 &10.90 \\
% \midrule
% Per-row &43.37 &54.48 &25.59 &24.10 &271.31 &49.46\\
% 1024    &38.10 &N/A   &24.24 &N/A   &16.68 &11.15\\
% 512     &35.20 &33.75 &23.58 &19.58 &16.21 &11.15\\
% 256     &34.43 &32.46 &23.08 &19.31 &16.15 &11.13\\
% 128     &33.49 &31.95 &22.62 &18.98 &15.96 &11.10\\
% 64      &33.26 &31.51 &22.41 &18.91 &15.86 &11.10\\
% 32      &32.93 &31.34 &22.15 &18.95 &15.85 &11.12\\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{table}

% We report W3A16 results of \opt and \bloom in~\ref{tab:opt-3bit-blocksize} and \ref{tab:bloom-3bit-blocksize} with various quantization block sizes, respectively.
% Similar to 4-bit quantization, smaller block size brings better accuracy. 
% However, none of the models can achieve \classone quantization error, and more importantly, 3-bit with block size 32, which has similar actually bits as 4-bit per-row quantization (since block size 32 has one FP16 scaling factor and one FP16 zeropoint), has worse performance than 4-bit per-row quantization, which demonstrates that fine-grained quantization might be able to close the gap from the reduction of bits.


\input{_s7_finetuning}

\section{Tables and Figures}
\label{sec:full_tables_results}
We put the full results of our evaluations in this section.
% \subsection{Full results of \sref{sec:ptq_challenge}}
\input{_s7_appendix_challenge_table.tex}


% \subsection{Full Results of~\sref{sec:weight_only_quantization_existing_method}}
\input{_s7_appendix_existing_weightonly.tex}

% \subsection{Full Results of~\sref{sec:weightactivation_quantization_existing_method}}
\input{_s7_appendix_existing_weightandactivation.tex}

% \subsection{Full Resuts of~\sref{sec:main_result_weightonly_quantization}}
\input{_s7_appendix_main_result_w4a16.tex}
\input{_s7_appendix_main_result_w4a8.tex}

