\label{sec:main_result_weightonly_quantization}
\paragraph{4-bit Weight Quantization.} We hereby present the W4A16 results for \opt and \bloom, as delineated in \tref{tab:opt-4bit-blocksize}, corresponding to an array of quantization block sizes. The performance sees a significant improvement with smaller block sizes compared to per-row quantization. The point of diminishing returns, however, varies for different model sizes. For example, smaller models (such as \opt-6.7B and \bloom-1.7b) continue to see substantial gains until the block size reduces to 32. In contrast, for larger models (those exceeding 10B, with \opt-66B as the exception), the benefits derived from smaller block sizes wane rapidly around block-256/512.  Most crucially, for models equal to or larger than 13B, a smaller quantization block size results in quantization error being classified under \classone, indicating virtually negligible degradation in accuracy.
\begin{table}[t]
\caption{
Results of \textbf{W4\asym-A16} quantization with various block-size out of the best result from optimization-based methods on \opt and \bloom (BLM). 
See~\tref{tab:opt-4bit-blocksize-full}  and~\tref{tab:bloom-4bit-blocksize-full} 
 for full results including \rtn. 
N/A means that the block size is not divisible by the hidden size.
}\centering
\label{tab:opt-4bit-blocksize}
\begin{adjustbox}{width=0.999\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Block-size  & OPT-6.7b & OPT-13b & OPT-30b & OPT-66b & BLM-1.7b & BLM-3b & BLM-7.1b & BLM-176b \\ \midrule 
W16A16   & 11.90     & 11.22   & 10.70    & 10.33   & 20.43    & 17.58  & 14.96    & 10.90     \\
Per-row   & 12.28    & 11.42   & 10.78   & 10.52   & 21.38    & 18.33  & 15.50     & 11.02    \\\midrule
1024      & 12.16    & 11.36   & 10.75   & 10.52   & 31.03    & N/A    & 15.24    & 10.96    \\
512       & 12.08    & 11.32   & 10.73   & 10.52   & 20.93    & 17.99  & 15.20     & 10.95    \\
256       & 12.05    & 11.28   & 10.74   & 10.50    & 20.95    & 17.97  & 15.18    & 10.95    \\
128       & 12.10     & 11.28   & 10.74   & 10.44   & 20.92    & 17.90   & 15.17    & 10.94    \\
32        & 12.03    & 11.28   & 10.72   & 10.41   & 20.82    & 17.88  & 15.16    & 10.95   \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}
\begin{table}[H]
\caption{
\opt W4\asym-A8 with various block-size out of the best result from \gptq, \zqlocal, and \zqglobal on \opt and \bloom (BLM). 
See~\tref{tab:opt-4bit8bit-blocksize-full} for full results including \rtn.
}\centering
\label{tab:opt-4bit8bit-blocksize}
\begin{adjustbox}{width=0.999\linewidth}
\centering
\begin{tabular}{llccccccccccccc }
\toprule
Precision &  block-size (W|A)  & OPT-6.7b & OPT-13b & OPT-30b & OPT-66b & BLM-1.7b & BLM-3b & BLM-7.1b & BLM-176b \\\midrule
W4A16   &   128 | NA     & 12.10     & 11.28   & 10.74   & 10.44   & 20.92    & 17.90   & 15.17    & 10.94     \\\midrule
\multirow{3}{*}{W4A8}    & Case-1: per-row | per-row & 13.17    & 13.07   & 14.65   & 16.32   & 21.43    & 18.39  & 15.58     & 11.19    \\
    % & 128  & per-row     & 12.10     & 11.28   & 10.74   & 10.44   & 20.92    & 17.90   & 15.17    & 10.94    \\
     & Case-2:  per-row  | 128  & 12.29    & 11.45   & 10.80    & 10.61   & 21.59    & 18.31  & 15.52    & 11.03    \\
     & Case-3: 128 | 128   & 12.04    & 11.31   & 10.75   & 10.45   & 21.27    & 17.86  & 15.19    & 10.96   \\
\bottomrule
\end{tabular}
\end{adjustbox}
% \vspace{-0.75cm}
\end{table}


\begin{wraptable}{r}{8cm}
%\begin{table}[t]
\vspace{-0.15cm}
% \begin{table}[t]
\caption{
\bloom-176B with different quantization block sizes on activation. 
Here weight is asymmetrically quantized with block size 128.
See more in~\tref{tab:bloom-176-different-blocks-full}.
}\centering
\label{tab:bloom-176-different-blocks}
\begin{adjustbox}{width=1.0\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
A8 Block Size  & 1024   &512    &256 &128 &32 \\
\midrule
\ppl &10.98 &10.97 &10.95 &10.95  &10.95\\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-0.5cm}
\end{wraptable}

\textbf{Activation Quantization (W4A8).} To comprehend the benefits of fine-grained quantization on activation, we analyze the quantization between per-row and a block size of 128, with INT4 weight, as highlighted in \tref{tab:opt-4bit8bit-blocksize}. For models of considerable size, specifically those equal to or exceeding 1B, the application of such fine-grained activation quantization (Case-1) results in a substantial reduction in quantization error compared to per-row activation (Case-2). By implementing fine-grained activation quantization with weight quantization (Case-3), we are able to almost restore the performance to the level of their W4A16 counterparts. 

Furthermore, we detail the impacts of varying activation quantization block sizes in \tref{tab:bloom-176-different-blocks} on \bloom-176B, with INT4 weight. A trend of superior accuracy is observed with smaller block sizes in contrast to larger ones. However, the enhancement in performance reaches a saturation point when the size smaller or equal to 256, which corresponds to the range of values INT8 can represent. Despite INT8's capability to signify 256 distinct values, activation quantization errors persist due to the application of uniform quantization.

\begin{tcolorbox}
\textbf{Finding 3 on FGQ. }   \textbf{(1)} Larger models ($\geq$10B) are capable of attaining \classone error for 4-bit quantization. These models can leverage low-precision quantization as the model size with INT4 is similar to an INT8 model that is half its size, with improved accuracy. On the other hand, smaller models ($\leq$10B) typically reach only \classtwo or \classthree error levels.
    \textbf{(2)}  For larger models (>10B), the difference between fine-grained weight-and-activation quantization and fine-grained weight-only quantization is insignificant. 
    \textbf{(3)} The advantage of fine-grained activation quantization fades for larger models when the block size reaches 256.
\end{tcolorbox}
  \vspace{-0.3cm}