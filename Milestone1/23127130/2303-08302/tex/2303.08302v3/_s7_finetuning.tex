\section{Quantization-aware training with LoRC}\label{sec:qat-lorc}
In order to better understand our proposed algorithm, \lorc, particularly in relation to the dimensions of low-rank matrices, we applied quantize-aware training alongside knowledge distillation. This approach builds upon the methodology of row-wise weight quantization and token-wise quantization. For the optimization process, we employed the Adam optimizer, setting the learning rate at 1e-4 and a dropout rate of 0.05. These settings were identified as the most effective in our context (additional details can be found in \cite{wu2023understanding}). We performed fine-tuning on the WikiText dataset using pre-trained GPT2 models with 125M and 350M parameters, which were obtained from Hugging Face as our initial models. \footnote{\url{https://huggingface.co/gpt2}}

The results are illustrated in Figure \fref{fig:lorc-despription}. As observed, the quantized models tend to overfit swiftly. However, implementing higher dropout values, such as 0.1, does not result in a significantly improved performance with regards to the best perplexity over the entire training duration. Now when examining the best perplexity associated with each dimension of \lorc (also indicated in the figure's legend), it becomes evident that the larger the dimension, the better the W4A8 models perform. This suggests that augmenting the dimension of \lorc can enhance the model quality for QAT, a finding that deviates from the trends observed in PTQ.
\begin{figure}[H]
\centering
\includegraphics[width=0.35\textwidth]{figures/model-gpt2.png}
\includegraphics[width=0.35\textwidth]{figures/model-gpt2-medium.png}
 \captionof{figure}{The graph on the left represents the results for a smaller model size (GPT2-125M), while the one on the right corresponds to the GPT2-350M model. The dimension (refer to the legend) in the LoRC algorithm, which is represented by different color curves, plays a pivotal role in approximating the original quality of the fp16 model.}\label{fig:lorc-despription}
\end{figure}