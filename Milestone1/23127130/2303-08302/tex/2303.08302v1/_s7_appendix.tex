\appendix

%%%%%%%%%%%%%%%%%%%
% Re-count the Figure/Algorithm/Tables after this point. 
%%%%%%%%%%%%%%%%%%%
\counterwithin{figure}{section}
\counterwithin{table}{section}


\section{Detailed Setting Used in~\sref{sec:evaluation_of_existing_methods}}
\label{sec:hyperparameter_used_in_exisiting_method_evaluation}
Same as~\cite{frantar2022gptq}, for all methods, we use C4 dataset to randomly select 128 sentences for training and each of them has 2048 tokens.

For \gptq, we check its main hyperparameter, i.e., the dampening factor, and find out the method is not sensitive to it.
As such, we use the hyparameter suggested by the author for all of our experiments. 

For \zqglobal and \zqlocal, as mentioned the in main text, the hyperparameters suggested by the original work~\cite{yao2022zeroquant} is suboptimal. 
We find that a linear decay learning rate schedule is very helpful in our initial test.
As such, we add this as our default setting.
Meanwhile, we extensively test a wide range (1e-3 to 5e-8) of learning rate for different models until we find the best learning rate (i.e., larger or smaller learning rate leads to worse accuracy performance).
We use Adam optimizer and a default batch size 1.

For all three methods, we run them on a single GPU (either V100-32GB or A100-80GB). 
For the largest model tested in the paper, i.e., \bloom-176B, the cost of all methods is lower than 1 GPU-day on A100-80G.

\section{Best \ptq Methods with Per-row Quantization}
Table~\ref{tab:opt-quantization-method} and~\ref{tab:bloom-quantization-method} summarize the best \ptq methods with per-row optimization.

\begin{table}[t]
\caption{
Best optimization method of \opt family in~\sref{sec:evaluation_of_existing_methods}.
}\centering
\label{tab:opt-quantization-method}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Precision     & 125m	& 350m 	& 1.3b	& 2.7b	& 6.7b	& 13b &30b	& 66b \\
\midrule
Weight Only (INT4) &\zqglobal &\zqglobal &\gptq &\gptq &\gptq &\gptq &\gptq &\gptq\\
\midrule
Weight \& Activation (W4A8) &\zqglobal &\zqglobal &\zqglobal &\gptq &\zqglobal &\zqglobal &\zqglobal &\zqlocal\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[t]
\caption{
Best optimization method of \bloom family in~\sref{sec:evaluation_of_existing_methods}.
}\centering
\label{tab:bloom-quantization-method}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc}
\toprule
Precision     & 560m   &1.1b   & 1.7b  & 3b & 7.1b & 176b \\
\midrule
Weight Only (INT4) &\gptq &\zqglobal &\zqglobal &\zqglobal/\gptq &\gptq &\gptq\\
\midrule
Weight \& Activation (W4A8) &\zqglobal &\zqglobal &\zqglobal &\zqglobal &\zqglobal &\zqlocal \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


\section{3-bit Weight-only Quantization}
\label{sec:3bit_weightonly_quantization}
\begin{table}[t]
\caption{
Results of W3\asym-A16 quantization on \opt with various block-size out of the best result from optimization-based methods. 
See~\tref{tab:opt-3bit-blocksize-full} for full results including \rtn.
N/A means that the block size is not divisible by the hidden size.
}\centering
\label{tab:opt-3bit-blocksize}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Block-size     & 125m	& 350m 	& 1.3b	& 2.7b	& 6.7b	& 13b &30b	& 66b \\
\midrule 
W16-A16 &28.27 &22.93 &15.44 &13.58 &11.90 &11.22 &10.70 &10.33\\

\midrule
Per-row &46.82 &30.30 &22.36 &17.06 &14.18 &12.43 &11.28 &17.77 \\
1024 &N/A &29.62 &20.16 &N/A &12.90 &11.74 &11.03 &12.95 \\
512 &N/A &28.65 &18.94 &15.47 &12.82 &11.67 &10.97 &12.33 \\
256 &38.85 &27.92 &17.95 &15.10 &12.79 &11.63 &10.90 &11.34 \\
128 &36.80 &26.97 &17.61 &15.05 &12.69 &11.59 &10.91 &11.27 \\
64 &35.48 &26.76 &17.40 &14.85 &12.58 &11.62 &10.92 &10.97\\
32 &33.75 &26.38 &17.11 &14.73 &12.64 &11.70 &10.99 &10.95\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[t]
\caption{
Results of W3\asym-A16 quantization on \bloom with various block-size out of the best result from optimization-based methods. 
See~\tref{tab:bloom-3bit-blocksize-full} for full results including \rtn.
N/A means that the block size is not divisible by the hidden size.
}\centering
\label{tab:bloom-3bit-blocksize}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Block-size     & 560m   &1.1b   & 1.7b  & 3b & 7.1b & 176b \\
\midrule 
W16-A16  &29.35 &28.32 &20.43 &17.58 &14.96 &10.90 \\
\midrule
Per-row &43.37 &54.48 &25.59 &24.10 &271.31 &49.46\\
1024    &38.10 &N/A   &24.24 &N/A   &16.68 &11.15\\
512     &35.20 &33.75 &23.58 &19.58 &16.21 &11.15\\
256     &34.43 &32.46 &23.08 &19.31 &16.15 &11.13\\
128     &33.49 &31.95 &22.62 &18.98 &15.96 &11.10\\
64      &33.26 &31.51 &22.41 &18.91 &15.86 &11.10\\
32      &32.93 &31.34 &22.15 &18.95 &15.85 &11.12\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

We report W3A16 results of \opt and \bloom in~\ref{tab:opt-3bit-blocksize} and \ref{tab:bloom-3bit-blocksize} with various quantization block sizes, respectively.
Similar to 4-bit quantization, smaller block size brings better accuracy. 
However, none of the models can achieve \classone quantization error, and more importantly, 3-bit with block size 32, which has similar actually bits as 4-bit per-row quantization (since block size 32 has one FP16 scaling factor and one FP16 zeropoint), has worse performance than 4-bit per-row quantization, which demonstrates that fine-grained quantization might be able to close the gap from the reduction of bits.


\section{Full results of Our Evaluation}
\label{sec:full_tables_results}
We put the full results of our evaluations in this section.
% \subsection{Full results of \sref{sec:ptq_challenge}}
\input{_s7_appendix_challenge_table.tex}


% \subsection{Full Results of~\sref{sec:weight_only_quantization_existing_method}}
\input{_s7_appendix_existing_weightonly.tex}

% \subsection{Full Results of~\sref{sec:weightactivation_quantization_existing_method}}
\input{_s7_appendix_existing_weightandactivation.tex}

% \subsection{Full Resuts of~\sref{sec:main_result_weightonly_quantization}}
\input{_s7_appendix_main_result_w4a16.tex}
\input{_s7_appendix_main_result_w4a8.tex}