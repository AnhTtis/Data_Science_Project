\section{Introduction}
\label{sec:intro}
Large language models (LLMs) have been shown breakthrough performance on various benchmarks, e.g., natural language understanding and generation, and have been adopted for daily usage, e.g., Codex~\cite{copilot} and ChatGPT~\cite{chatgpt}.
However, how to \textit{efficiently} serve those LLMs becomes urgent due to their large memory consumption and heavy computation requirement. 

Different than classification models or diffusion models, \llms has its own serving challenge.
Generally, classification models run inference once per query and diffusion models have the same inference behavior for every time steps.
However, \llms have two phases, i.e., prompt and generation:
the prompt stage takes the query/question (a sequence of tokens) from the user and run one forward pass, 
then the generation stage auto-regressively (token-by-token) generates the corresponding answer by running the model for multiple steps. 
The fundamental bottlenecks for prompt and generation phases are different. 
Particularly, for a normal prompt stage (e.g., sequence length $\geq256$), the forward pass is primarily compute bounded, i.e., higher compute brings better latency;
for the normal generation phase (low batch size) with KV (key and value for attention) cache, the forward pass is mainly memory bounded, i.e., higher memory bandwidth brings better performance. 
See~\cite{pope2022efficiently} for a more detailed analysis.

% Using GPT-3-175B~\cite{brown2020language} (with 350GB model size in FP16) and A100 (with 2TB/s bandwidth and 312 TFLOPS compute in FP16) as an example\footnote{Here, we simply assume one GPU is able to hold the model. 
% If multiple GPUs are used, we can linearly scale down the time.}, 
% if we are able to hit the peak performance, it takes \zhewei{0.14}s to prompt a query with 128 tokens (ignore memory movement time) and \zhewei{22.4}s to generate 128 tokens (ignore compute time) by loading the weight for 128 times.
% As can be seen, the generation cost is \zhewei{xx} higher than prompt phase to process the same amount of tokens. \zhewei{@Cheng for double chech the numbers.}

Meanwhile, as mentioned in~\cite[Figure 3]{gholami2020ai}, the bandwidth of hardware increases about 1.4x every two years while the compute increases about 3.1x every two years.
Additionally, multiple nodes are now required to serve extra large models, e.g., 2 A100-80G nodes for MT-NLG-530B~\cite{smith2022using} and 2 A100-40G nodes for GPT-3-175B~\cite{brown2020language}, which introduces the extra bandwidth challenge between cross-node communication.
%
As such, reducing the model size for LLMs is a urgent. 
% \zhewei{I also want to mention computation cost reduction here as in this paper we also show W4A8 results. 
% Not sure how to massage this paragraph.}
Meanwhile, if we can also reduce the compute cost, it will cover both prompt and generation phases to further alleviate the serving challenge for \llms. 
% it would be even better if both goals (i.e., model size and computation cost reductions) can be simultaneously achieved. 

Considering the forbidden training/finetuning cost for those \llms, one of the most effective ways to alleviate those memory/compute challenge is post training quantization (\ptq), where no/minimal training is required to reduce the bit precision for weights and/or activations to INT4 or INT8.
Several works, e.g.,~\cite{yao2022zeroquant,frantar2022gptq,xiao2022smoothquant,dettmers2022case} have shown the effectiveness of \ptq, but none of them gives a systemic study, e.g., the functional coverage for different \ptq methods, the sensitivity of different models, etc.  

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/opt-tradeoff.pdf}
    \includegraphics[width=0.49\textwidth]{figures/bloom-tradeoff.pdf}
    \caption{The model size and quality trade-off of different quantization methods on models from \opt and \bloom families.
    Here \ptq (with fine-grained quantization) represents the method from~\cite{yao2022zeroquant,frantar2022gptq}, 
    \rtn means the naive round-to-nearest baseline (with fine-grained quantization as well),
    and FP16/INT8 is used as the no accuracy-loss baseline. 
    Note that we drop all diverged points for a better visualization. 
    For all detailed numbers, please see~\appref{sec:full_tables_results}.}
    \label{fig:main_figure}
\end{figure}

In this work, we provide a comprehensive study on the quantization effect for both weigh-only quantization and weight-and-activation quantization using different quantization schemes, e.g., symmetric and asymmetric quantization, with various \ptq methods, including round-to-nearest (\rtn), \gptq~\cite{frantar2022gptq}, ZeroQuant~\cite{yao2022zeroquant} and its variants, on two different model families \opt~\cite{zhang2022opt} and \bloom~\cite{scao2022bloom} across model sizes from 125M to 176B.
In summary, our observations are as follows.
\begin{itemize}
\item[] \textbf{Sensitivity Analysis}(\tref{tab:ptq_challenge_opt_average_in_maintext} and~\ref{tab:ptq_challenge_bloom_average_in_maintext})
    \begin{itemize}
        \item 
        We demonstrate that INT8 weight-only quantization does not have any model quality effect. 
        For INT4 weight-only quantization, larger models usually exhibit better quantization tolerance as compared to relative smaller models. 
        \item Activation quantization is generally more sensitive to quantization as compared to weight quantization. 
        Smaller models usually have better activation quantization performance than relative larger model. 
        \item Different model families show entirely different INT8 activation quantization behaviors. 
        particularly for large models, \bloom-176B still has meaningful accuracy (about 1 perplexity, \ppl in short, point drop) but \opt-30B and -66B have much worse performance. 
    \end{itemize}
\item[] \textbf{Existing \ptq Method Analysis}(\tref{tab:weight_only_quantization_opt_existing_method_average_in_main_text}, \ref{tab:weight_only_quantization_bloom_existing_method_average_in_main_text}, \ref{tab:weightactivation_quantization_opt_existing_method_average_in_main_text}, and~\ref{tab:weightactivation_quantization_bloom_existing_method_average_in_main_text})
    \begin{itemize}
        \item Existing methods can significantly reduce the quantization error as compared to the round-to-the-nearest baseline. 
        Different \ptq methods has their own best working scenarios.
        \item Current existing method can barely achieve less than 0.1 \ppl points degradation for either INT4 weight-only or W4A8 weight-and-activation (i.e., INT4 weight and INT8 activation) quantization. 
    \end{itemize}
\item[] \textbf{Fine-grained Quantization Effect}(\tref{tab:opt-4bit-blocksize}, \ref{tab:bloom-4bit-blocksize}, \ref{tab:opt-4bit8bit-blocksize}, \ref{tab:bloom-4bit8bit-blocksize}, \ref{tab:bloom-176-different-bits}, and~\ref{tab:bloom-176-different-blocks})
    \begin{itemize}
        % \item  For large enough models (>13B), with \ptq methods and finegrained quantization, <0.1 \ppl points degradation is achievable using either weight-only quantization or weight-and-activation quantization.
        \item With the further help from fine-grained quantization, \ptq is able to achieve <0.1 \ppl points degradation for large models (>13B) with either weight-only quantization or weight-and-activation quantization.
        
        \item Larger models can use relative coarse-grained weight quantization (e.g., block size 128/256 for \bloom-176B) to achieve good quantization error as compared to smaller models (e.g., block size 32/64 for \opt-30B).
        \item For \bloom-176B, coarse-grained (per-row) weight quantization with higher bits (e.g., 5 bits) always leads to better accuracy as compared to fine-grained quantization with lower bits (e.g., 4 bits with 32 elements as quantization block size), even if the real bit precision is similar.
        % (2) the benefit from fine-grained quantization for activation quantization diminishes when the block size is 256 or smaller.
    \end{itemize}

\end{itemize}



We provide model size and model quality trade-off of models from \opt and \bloom families in~\fref{fig:main_figure}. 
As can be seen, using \ptq optimization methods from~\cite{yao2022zeroquant,frantar2022gptq} and fine-grained quantization, we setup a new quantization Pareto frontier for \llms.   
Meanwhile, we recommend the following setting for quantizing \llms (note that activation quantization should be only applied if necessary): 
(1) For larger models (>10B), fine-grained (block size 64--256) 4-bit weight quantization plus 8-bit activation quantization (block size 64--256) with \ptq methods can be used for real deployment;
(2) For middle-size models (<10B and >1B), per-row INT8 quantization plus fine-grained (block size 64--256) INT8 activation quantization can be used with \ptq methods from~\cite{frantar2022gptq,yao2022zeroquant};
(3) For smaller models (<1B), directly apply per-row W8A8 (INT8 weight and INT8 activation) \rtn is enough based on~\cite{yao2022zeroquant}.


% \zhewei{Create figures to compare \rtn and our final results with model-size vs. ppl.}

% a lot of existing compression methods, e.g., layer reduction~\cite{wu2022extreme}, structure pruning~\cite{he2019filter,michel2019sixteen}, ultra-low-precision quantization~\cite{dong2019hawq,liu2022bit} etc, are not easy to be applied. 
% As such, recently, post-training compression~\cite{kwon2022fast,yao2022zeroquant} techniques attract increasing attentions.