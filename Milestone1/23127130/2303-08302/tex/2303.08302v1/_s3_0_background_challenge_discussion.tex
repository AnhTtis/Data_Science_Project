\section{Background and Challenges}
\label{sec:ptq_challenge}

% \zhewei{Also discuss weight/activation quantization for memory and compute, the limitations/or unanswered questions...}
% \zhewei{
% 1. W-asym/sym + A-sym/asym quantization
% \\
% 2. W-perrow/blockwise + A-perrow/blockwise
% \\
% 3. W-quantized-asym + A-16bit (another one already did it)
% }
\subsection{Background of Quantization}
\label{sec:background_of_quantization}
Quantization maps floating points (e.g., FP16/FP32) numbers to integer numbers (e.g., INT4/INT8) so that lower memory usage (weight quantization) and faster integer arithmetic (weight-and-activation quantization) can be achieved compared to the floating point format. 
In this work, we are focusing on uniform quantization, i.e., 
\begin{equation}
\small
\label{eq:quantization_formula}
Q(x) = \text{INT}\big({(x-Z)}/{S}\big)-Z,
\end{equation}
where $Q$ is the quantization function, $x$ is a floating point input vector/tensor, $S$ is a real valued scaling factor, and $Z$ is an integer zero point. 
Based on different settings, the quantization method can be viewed as (1) symmetric vs. asymmetric quantization ($Z=0$ or not), (2) fine-grained vs. coarse-grained quantization (how to partition the input x and its associated scaling factor, e.g., matrix wise or row wise). 
See~\cite{gholami2021survey} for more details. 

Throughout this work, we focus on post training quantization (\ptq), i.e., no or minimal training effort is applied after quantization, for which large accuracy degradation usually exhibits for coarse-grained quantization (per matrix/tensor) due to their large quantization error.
As such, we focus on fine-grained quantization.
Particularly, we use the per-row quantization (one row of the weight matrix or one token for the activation) from~\cite{yao2022zeroquant} as our coarsest-grained quantization, 
and we use block-k quantization (for every k elements, they have their own scaling factor and/or zero point) as our finer-grained quantization scheme. 

\subsection{Post Training Quantization for Large Language Models}
% Usually, there are two normal quantization configurations for LLMs, i.e., weight-only quantization, for which only the weight of the neural network got quantized and the activation is left in FP16, and weight \& activation quantization, for which both the weight and activation are quantized.
% In this paper, we give a comprehensive analysis and provide the pros/cons for both methods.

There are mainly two categories of \ptq for \llms, i.e., weight-only quantization~\cite{frantar2022gptq} and weight-and-activation quantization~\cite{dettmers2022llm,yao2022zeroquant,xiao2022smoothquant}. 
For the latter case, all works found that activation quantization is more sensitive than weight quantization. 
However, none of them gives a systematic view, e.g., the sensitivity of weight/activation quantization for different model sizes and different model families. 
Therefore, we here perform a study on both the OPT~\cite{zhang2022opt} and BLOOM~\cite{scao2022bloom} families to illustrate the quantization sensitivity of weight and activation. 

\subsubsection{Settings}
\paragraph{Quantization setting.} 
We use both symmetric and asymmetric quantization to measure the quantization sensitivity and show the benefit of asymmetric quantization. 
Particularly, we use per-row quantization~\cite{frantar2022gptq} for weight quantization and use per-token quantization for activation~\cite{yao2022zeroquant}.

\paragraph{Sensitivity setting.} 
We use the zero-shot validation perplexity (\ppl) difference on three datasets, i.e., Wikitext-2~\cite{merity2016pointer}, PTB~\cite{marcinkiewicz1994building}, and C4~\cite{colin2019t5} , before and after quantization of those \llms to demonstrate their sensitivity as the \ppl is highly related to zero-shot/few-shot accuracy measurement~\cite{dettmers2022case}. 
Particularly, larger \ppl drop means higher quantization sensitivity.
For simplicity, we also categorize quantization sensitivity (or quantization accuracy loss) into 3 different classes as shown in~\tref{tab:quantization-loss-table}.\footnote{
The threshold is selected since when the model size is about doubled (e.g., 13B vs. 30B, and 30B vs. 66B), the \ppl improvement is about 0.3 (see~\tref{tab:ptq_challenge_opt_average_in_maintext}).
}
The sensitivity (or loss) gradually increases as the class number becomes larger. 
From a practical perspective, we prefer lower quantization sensitivity (accuracy loss) and \classone can be (almost) viewed as optimal-loss post training quantization.

\begin{table}[t]
\caption{
Quantization sensitivity (or quantization accuracy loss) categorization. 
From \classone to \classthree, the sensitivity (or loss) becomes larger.
}\centering
\label{tab:quantization-loss-table}
\begin{adjustbox}{width=0.5\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Class     & \classone & \classtwo & \classthree \\
\midrule
\ppl Degradation & $\le$0.1 & $>$0.1 \& $\le$0.5 & $>$0.5 \\ 
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[t]
\caption{
Average \ppl of \opt. 
See~\tref{tab:ptq_challenge_opt_full_in_appendix} for all results.
}\centering
\label{tab:ptq_challenge_opt_average_in_maintext}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Precision     & 125m	& 350m 	& 1.3b	& 2.7b	& 6.7b	& 13b &30b	& 66b \\
\midrule
W16-A16 &28.27 &22.93 &15.44 &13.58 &11.90 &11.22 &10.70 &10.33\\
\midrule
W8\sym-A16 &28.27 &22.96 &15.44 &13.59 &11.90 &11.22 &10.70 &10.33 \\
W8\asym-A16 &28.31 &22.96 &15.46 &13.60 &11.90 &11.22 &10.70 &10.33 \\
W4\sym-A16 &45.42 &27.00 &20.79 &25.06 &14.36 &12.73 &11.77 &97.05 \\
W4\asym-A16 &37.46 &26.76 &19.75 &19.58 &13.44 &12.09 &11.52 &31.52 \\
\midrule
W16-A8\sym &28.40 &23.14 &16.40 &14.29 &26.04 &3171.49 &2048.21 &2638.09 \\
W16-A8\asym &28.37 &23.02 &16.06 &13.76 &12.62 &15.36 &23.57 &561.35 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[t]
\caption{
Average \ppl of \bloom. 
See~\tref{tab:ptq_challenge_bloom_full_in_appendix} for all results.
}\centering
\label{tab:ptq_challenge_bloom_average_in_maintext}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Precision     & 560m   &1.1b   & 1.7b  & 3b & 7.1b & 176b \\
\midrule
W16-A16  &29.35 &28.32 &20.43 &17.58 &14.96 &10.90 \\
\midrule
W8\sym-A16 &29.37 &28.33 &20.43 &17.59 &14.97 &10.90 \\
W8\asym-A16 &29.36 &28.33 &20.45 &17.59 &14.97 &10.90 \\
W4\sym-A16 &34.73 &33.24 &23.18 &19.36 &16.27 &11.28 \\
W4\asym-A16 &33.06 &39.40 &22.47 &19.01 &15.90 &11.20 \\
\midrule
W16-A8\sym &29.52 &28.48 &20.68 &17.73 &15.28 &12.10 \\
W16-A8\asym &29.41 &28.36 &20.52 &17.65 &15.14 &11.62 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


\subsubsection{Robustness of Weight-only Quantization for Large Models}
The results of weight-only quantization of \opt and \bloom are shown in~\tref{tab:ptq_challenge_opt_average_in_maintext} and~\ref{tab:ptq_challenge_bloom_average_in_maintext}. 
INT8 (either symmetric or asymmetric) weight-only quantization leads to almost no accuracy loss (smaller than 0.05, i.e., \classone).
As such, for generation-oriented tasks, we can simply replace FP16 weight with INT8 weight to save memory consumption. 
For INT4 quantization, asymmetric method has better accuracy than symmetric method since asymmetric quantization has better utilization of the quantization range.
Also, larger models have better tolerance to low-precision quantization (i.e. INT4) than smaller models, except for several models, e.g., \opt-66B.\footnote{\cite{frantar2022gptq} found that \opt-66B has a high ratio of dead neurons in the early layers, which may affect the compression ability. 
We also find another possible reason that layer norm of \opt-family is not well trained (except \opt-350M): the weight and the bias are all 1's and 0's, respectively.}
Particularly, for \bloom-176B, the \ppl degradation (about 0.3 points) is in \classtwo,
and this may explain why the large GLM-130B~\cite{zeng2022glm} can work with INT4 weight-only quantization out of the box with acceptable accuracy impact. 

\subsubsection{Challenge of Activation Quantization for Large Models}
Activation quantization has generally been shown to be more challenging than weight quantization~\cite{yao2022zeroquant,dettmers2022llm}. 
As such, throughout the paper, we only focus on INT8 activation quantization. 

Similar to weight-only quantization, asymmetric quantization here has better performance than symmetric quantization, e.g., \opt-6.7B has totally different behavior with asymmetric and symmetric quantization.
Different than weight-only quantization,  smaller models usually have better activation quantization tolerance as their hidden dimension is smaller and the activation dynamic range is also narrower than larger models~\cite{yao2022zeroquant}.
Note that for model size larger than 10B, all models belong to \classthree, which has more than 0.5 \ppl points degradation. 
 
Also, note that different model families have significantly different behaviors. 
\bloom does not have divergence issues up to 176B model size but \opt has very poor performance from 6.7B model size (the larger models with INT8 activation have even worse \ppl). 
This may be caused again by the layer norm issue of \opt-family\samethanks. 


\subsubsection{Summary}
In a short summary, 
\begin{itemize}
\item INT8 weight-only quantization can be used as a standard (almost) no-accuracy-degradation way to help reduce memory cost for \llms.

\item INT4 weight-only quantization for small models leads to significant accuracy degradation (\classthree) and this effect diminishes as the model size becomes larger (\classtwo).
However, even for the larger models, the accuracy degradation might be higher than the gain from using the larger model, e.g., 4-bit asymmetric quantized \opt-30B has worse performance than 8-bit quantized \opt-13B in~\tref{tab:ptq_challenge_opt_average_in_maintext}.

\item INT8 activation leads to minimal accuracy degradation for small models (\classone) and the trend becomes larger for larger models (\classthree). 
Another interesting thing is that the activation quantization sensitivity is highly related to the model family, e.g., the result of \bloom in~\tref{tab:ptq_challenge_bloom_average_in_maintext} is much better than that of \opt in~\tref{tab:ptq_challenge_opt_average_in_maintext}.
\end{itemize}