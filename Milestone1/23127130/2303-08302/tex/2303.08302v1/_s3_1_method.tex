\section{Evaluation of Existing Methods for \ptq}
\label{sec:evaluation_of_existing_methods}
Several lightweight optimization-based (weight of the model will be updated during quantization) methods have been proposed. 
Different than quantization-aware training, those methods~\cite{yao2022zeroquant,frantar2022gptq,xiao2022smoothquant} only require a small portion of the training data and a short range of training time.
Among them, two types of methods are demonstrated to be both effective and efficient for INT4 weight quantization (of GPU resource, time cost, and data), GPTQ~\cite{frantar2022gptq} and ZeroQuant~\cite{yao2022zeroquant}. 
For this work, we focus on the variants of \gptq and ZeroQuant as well as the most straightforward baseline, round-to-nearest neighborhood (\rtn). 

\paragraph{\rtn} directly applies \ptq on the trained data and follows~\sref{sec:background_of_quantization} to do the quantization.
Particularly, 
for symmetric quantization, we set $S=max(abs(x))$ and $Z=0$; 
for asymmetric quantization, we set $S=max(x)-min(x)$ and set $Z=min(x)$.

\paragraph{\gptq} extends the OBQ~\cite{frantar2022optimal} by column-/row-wisely quantizing weight matrix instead of element-by-element. 
On a high level, it directly optimizes the following non-linear least square problem,
\begin{equation}
\label{eq:main_eq}
    \min_{\hat W} \|Wx - \hat Wx\|_2^2
\end{equation}
where $W$ is the weight, $x$ is the activation, and $\hat W$ is a quantized weight. 
There are multiple ways to solve this problem, e.g., using second-order methods to get a closed-form solution as \gptq does or using the \zqlocal which is discussed below.
See~\cite{frantar2022gptq} for more details.

\paragraph{\zqglobal} is the original method proposed in~\cite{yao2022zeroquant}, where authors treat each transformer layer as a small neural network (a.k.a., subnetwork) and use the unquantized FP16 subnetwork as the teach model to distill the quantized one with a few hundred iterations, i.e.,
\begin{equation}
    \min_{\hat \theta} \|f_{\theta}(x) - f_{\theta}(x)\|_2^2,
\end{equation}
where $\theta$ is the set of weight, $\hat \theta$ is the quantized version of it, $f_{\theta}$ is the subnetwork with parameters $\theta$, and $x$ is the input.
As such, it can significantly 
reduce the GPU resource requirement and time cost.
See~\cite{yao2022zeroquant} for more details.

\paragraph{\zqlocal} is an extension mode of \zqglobal for further GPU requirement reduction and training cost reduction. 
Particularly, instead of using each transformer layer as the subnetwork, we treat each linear layer as the subnetwork. 
This method can be viewed as an iterative first-order optimization method (e.g., SGD) to solve~\eref{eq:main_eq}. 
% The rest setting of \zqlocal is the same as \zqglobal. 

% Make two variants \zqglobal and \zqlocal


% \begin{table}[t]
% \caption{
% Comparison of \gptq, \zqlocal, and \zqglobal about speed and memory requirements.
% }\centering
% \label{tab:quantization-loss-table}
% \begin{adjustbox}{width=0.9\linewidth}
% \centering
% \begin{tabular}{lcccccccccccccc }
% \toprule
% Method     & \gptq & \zqlocal & \zqglobal\\
% \midrule
% Speed & Fa & \\
% Memory \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{table}

\subsection{Settings}
We compare four different methods on weight-only and weight-and-activation quantization: \rtn, \gptq, \zqlocal, and \zqglobal. 
As weight quantization is always static (i.e., it does not change during inference), there is almost no system performance difference between symmetric and asymmetric quantization.\footnote{The bias term (a.k.a., the zero point) can be simply fused into the previous activation quantization kernel~\cite{yao2022zeroquant}.} 
we directly use asymmetric quantization for weight for better accuracy.
For activation quantization, since we use dynamic quantization as~\cite{yao2022zeroquant} (i.e. the bias term dynamically changes and cannot be simply fused into other operators), symmetric quantization would provide better system performance but worse accuracy than asymmetric quantization. 
As such, in this section, we provide both results to demonstrate the trade-off.
We use the quantization error/sensitivity as~\sref{sec:ptq_challenge} to demonstrate the effectiveness of these methods.
% , and we use the time to compress the largest model we have (i.e., \bloom-176B) to illustrate the efficiency of each method. 

For parameter used for \gptq, \zqlocal, and \zqglobal, please see~\appref{sec:hyperparameter_used_in_exisiting_method_evaluation}.  
One interesting thing we find for ZeroQuant is that the hyperparameters (e.g., learning rate and learning-rate scheduler) provided in the original work~\cite{yao2022zeroquant} are sub-optimal.
In this work, for \zqlocal and \zqglobal, we use the best configuration we find to report the result. 
For simplicity, we mark \zqlocal and \zqglobal as \zqlocalstar and \zqglobalstar, respectively, with tuned results.

\subsection{Evaluation of Weight-only Quantization}
\label{sec:weight_only_quantization_existing_method}

\begin{table}[t]
\caption{
The evaluation results of different \ptq methods on \opt with W4\asym-A16.
See~\tref{tab:weight_only_quantization_opt_existing_method_full_in_appendix} for the full table.
}\centering
\label{tab:weight_only_quantization_opt_existing_method_average_in_main_text}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Precision     & 125m	& 350m 	& 1.3b	& 2.7b	& 6.7b	& 13b &30b	& 66b \\
\midrule
W16-A16 &28.27 &22.93 &15.44 &13.58 &11.90 &11.22 &10.70 &10.33 \\
\midrule
\rtn &37.46 &26.76 &19.75 &19.58 &13.44 &12.09 &11.52 &31.52 \\
\gptq &33.52 &25.02 &16.42 &14.19 &12.28 &11.42 &10.78 &10.52 \\
\zqlocalstar &33.50 &25.48 &16.74 &14.45 &12.46 &11.64 &11.05 &10.79 \\
\zqglobalstar &31.77 &24.45 &16.48 &14.30 &12.38 &11.62 &11.04 &10.68\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[t]
\caption{
The evaluation results of different \ptq methods on \bloom with W4\asym-A16. Please see~\tref{tab:weight_only_quantization_bloom_existing_method_full_in_appendix} for the full table.
}\centering
\label{tab:weight_only_quantization_bloom_existing_method_average_in_main_text}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc}
\toprule
Precision     & 560m   &1.1b   & 1.7b  & 3b & 7.1b & 176b \\
\midrule
W16-A16  &29.35 &28.32 &20.43 &17.58 &14.96 &10.90 \\
\midrule
\rtn &33.06 &39.40 &22.47 &19.01 &15.90 &11.20 \\
\gptq &31.08 &39.67 &21.58 &18.33 &15.50 &11.02 \\
\zqlocalstar &31.74 &31.06 &21.70 &18.50 &15.55 &11.11  \\
\zqglobalstar &31.21 &30.85 &21.38 &18.33 &15.52 &11.05 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

The weight-only quantization results of \opt and \bloom are shown in~\tref{tab:weight_only_quantization_opt_existing_method_average_in_main_text} and~\ref{tab:weight_only_quantization_bloom_existing_method_average_in_main_text}, respectively.

Similar to \rtn (which has been shown in~\sref{sec:ptq_challenge}), \gptq, \zqlocalstar, and \zqglobalstar have the same observation, i.e., larger models are less sensitive to INT4 weight-only quantization except for \opt-66B, which has larger degradation than \opt-30B.
Overall, optimization-based methods have significantly better accuracy performance than the baseline method, \rtn. 
For instance, optimization-based methods significantly reduce \ppl point degradation of \opt-30B/66B compared to \rtn. 
Meanwhile, most quantized large models (>6.7B) belong to \classtwo, which has the potential to be deployed for real application (e.g., INT4 \opt-30B (66B) has better quality than INT8 \opt-13B (30B)).
% \zhewei{add 30B 4-bit and 13B 8-bit example to say it might be Okay for deployment}

Among the three optimization-based methods, \zqglobalstar usually shows better performance than the other two on smaller models (smaller than 1B parameters), and \gptq demonstrates better performance on larger models. 
\zqlocalstar does not give better results than \gptq and \zqglobalstar, which is understandable since \gptq utilizes a ``closed'' form to solve the non-linear quadratic problem and \zqglobalstar optimizes a larger subnetwork.

However, the worse performance of \zqglobalstar than \gptq for larger models is not initially expected as \zqglobalstar optimizes an entire transformer layer while \gptq only optimizes a single linear layer. 
One possible reason is that large models are more sensitive to weight update and more advanced finetuning method is needed.\footnote{
We also tried freezing different components of the subnetwork, e.g., layer normalization and/or bias, to see if we could get even better results.
However, they all show similar performance.
}
\subsection{Evaluation of weight and activation quantization}
\label{sec:weightactivation_quantization_existing_method}

\begin{table}[t]
\caption{
\opt ppl on wikitext/opt/c4 with W4\asym-A8\sym/A8\asym.
Please see~\tref{tab:weightactivation_quantization_opt_existing_method_full_in_appendix} for the full table.
}\centering
\label{tab:weightactivation_quantization_opt_existing_method_average_in_main_text}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Precision     & 125m	& 350m 	& 1.3b	& 2.7b	& 6.7b	& 13b &30b	& 66b \\
\midrule
W16-A16 &28.27 &22.93 &15.44 &13.58 &11.90 &11.22 &10.70 &10.33\\
W8\asym-A16 &28.31 &22.96 &15.46 &13.60 &11.90 &11.22 &10.70 &10.33 \\
\midrule
W4\asym-A8\sym Block\\
\rtn &37.21 &27.84 &24.73 &31.86 &146.10 &3953.99 &3238.68 &2990.32 \\
\gptq &32.72 &25.80 &17.55 &15.46 &51.78 &3409.66 &1889.45 &4822.68 \\ 
\zqlocalstar &33.10 &26.29 &18.04 &16.40 &18.67 &2536.44 &1612.07 &504.19 \\
\zqglobalstar &32.25 &25.13 &17.17 &15.52 &43.43 &118.76 &430.42 &1687.28 \\ 
\midrule 
W4\asym-A8\asym Block \\
\rtn &37.24 &27.07 &21.32 &25.39 &14.80 &26.36 &86.26 &815.00 \\ 
\gptq &32.82 &25.28 &16.81 &14.52 &13.88 &17.28 &20.71 &648.69 \\ 
\zqlocalstar &33.40 &25.61 &17.11 &14.84 &13.24 &14.23 &18.53 &16.32\\
\zqglobalstar &31.90 &24.81 &16.74 &14.55 &13.17 &13.07 &14.65 &37.82\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[t]
\caption{
\bloom ppl on wikitext/opt/c4 with W4\asym-A8\sym/A8\asym.
Please see~\tref{tab:weightactivation_quantization_bloom_existing_method_full_in_appendix} for the full table.
}\centering
\label{tab:weightactivation_quantization_bloom_existing_method_average_in_main_text}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Precision     & 560m   &1.1b   & 1.7b  & 3b & 7.1b & 176b \\
\midrule
W16-A16  &29.35 &28.32 &20.43 &17.58 &14.96 &10.90 \\
W8\asym-A16 &29.36 &28.33 &20.45 &17.59 &14.97 &10.90 \\
\midrule
W4\asym-A8\sym Block\\
\rtn &33.47 &40.83 &23.07 &19.31 &16.36 &12.91 \\
\gptq &31.59 &40.47 &22.10 &18.48 &15.95 &12.54 \\ 
\zqlocalstar &32.13 &31.30 &22.01 &18.69 &15.86 &11.41 \\
\zqglobalstar &31.31 &31.18 &21.51 &18.41 &15.67 &11.60\\ 
\midrule 
W4\asym-A8\asym Block \\
\rtn &33.18 &39.73 &22.75 &19.17 &16.19 &12.22 \\ 
\gptq &31.35 &39.50 &21.71 &18.44 &15.75 &11.86 \\ 
\zqlocalstar &31.86 &31.22 &21.86 &18.66 &15.75 &11.19 \\
\zqglobalstar &31.21 &31.02 &21.43 &18.39 &15.58 &11.49\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

The evaluation results of existing methods with W4A8 quantization are presented in~\tref{tab:weightactivation_quantization_opt_existing_method_average_in_main_text} and~\ref{tab:weightactivation_quantization_bloom_existing_method_average_in_main_text}.
Optimization-based methods achieve significantly better accuracy than \rtn for both asymmetric and symmetric activation quantization schemes, demonstrating their effectiveness.
However, all of the results are in \classtwo or \classthree, i.e., it might be better to use smaller models with fewer parameters than larger models with quantization.

Among quantization-based methods, \zqglobalstar and \zqlocalstar work generally better than \gptq, which is expected since \gptq was originally proposed for weight-only quantization. 
Compared to \zqlocalstar, \zqglobalstar has superb performance for most of cases except for the two largest models, i.e., \opt-66B and \bloom-176B, even though \zqglobalstar has larger trainable parameters in one step, which again reflects that for \llms, a more suitable and advanced optimization method is needed.



\subsection{Summary of Existing Methods}
In a short summary,
\begin{itemize}
    \item \gptq generally works better for weight-only quantization, and ZeroQuant (including both \zqglobal and \zqlocal) has better performance for weight \& activation quantization. 
    We also summarize the best Optimization-based  method for different models and different settings in~\tref{tab:opt-quantization-method} and~\ref{tab:bloom-quantization-method}.
    \item The tested optimization-based methods cannot achieve \classone quantization error for either INT4 weight-only or W4A8 weight-and-activation quantization except for \gptq on \opt-30B with weight-only quantization. 
\end{itemize}

