\subsection{Evaluation of Weight-only Quantization}
\label{sec:main_result_weightonly_quantization}

\begin{table}[t]
\caption{
Results of W4\asym-A16 quantization on \opt with various block-size out of the best result from optimization-based methods. 
See~\tref{tab:opt-4bit-blocksize-full} for full results including \rtn. 
N/A means that the block size is not divisible by the hidden size.
}\centering
\label{tab:opt-4bit-blocksize}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Block-size     & 125m	& 350m 	& 1.3b	& 2.7b	& 6.7b	& 13b &30b	& 66b \\
\midrule 
W16-A16 &28.27 &22.93 &15.44 &13.58 &11.90 &11.22 &10.70 &10.33\\
\midrule
Per-row &31.77 &24.45 &16.42 &14.19 &12.28 &11.42 &10.78 &10.52 \\
1024 &N/A &24.39 &16.17 &N/A &12.16 &11.36 &10.75 &10.52 \\
512 &N/A &24.34 &15.97 &13.93 &12.08 &11.32 &10.73 &10.52  \\
256 &30.68 &24.17 &15.84 &13.89 &12.05 &11.28 &10.74 &10.50\\
128 &30.04 &23.99 &15.85 &13.83 &12.10 &11.28 &10.74 &10.44\\
64 &29.88 &23.99 &15.76 &13.84 &12.02 &11.27 &10.72 &10.40\\
32 &29.62 &23.86 &15.71 &13.82 &12.03 &11.28 &10.72 &10.41\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[t]
\caption{
Results of W4\asym-A16 quantization on \bloom with various block-size out of the best result from optimization-based methods. 
See~\tref{tab:bloom-4bit-blocksize-full} for full results including \rtn.
N/A means that the block size is not divisible by the hidden size.
}\centering
\label{tab:bloom-4bit-blocksize}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Block-size     & 560m   &1.1b   & 1.7b  & 3b & 7.1b & 176b \\
\midrule
W16-A16  &29.35 &28.32 &20.43 &17.58 &14.96 &10.90 \\
\midrule
Per-row &31.08 &30.85 &21.38 &18.33 &15.50 &11.02 \\
1024    &30.98 &N/A   &31.03 &N/A   &15.24 &10.96 \\
512     &30.75 &29.40 &20.93 &17.99 &15.20 &10.95 \\
256     &30.49 &29.26 &20.95 &17.97 &15.18 &10.95\\
128     &30.35 &29.13 &20.92 &17.90 &15.17 &10.94\\
64      &30.24 &29.01 &20.82 &17.90 &15.16 &10.94\\
32      &30.18 &28.91 &20.82 &17.88 &15.16 &10.95\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\paragraph{4-bit Quantization.}
We report the W4A16 results of \opt and \bloom in~\tref{tab:opt-4bit-blocksize} and \ref{tab:bloom-4bit-blocksize} with various quantization block sizes, respectively. 
Smaller block sizes improve the performance by a non-trivial margin as compared to per-row quantization.

For different model sizes, the diminishing return points are different. 
For instance, small models (e.g., \opt-125m and \bloom-560m) can achieve great gain until the block size becomes 32.
However, for large models (>10B, except \opt-66B), the gain from smaller block sizes quickly vanishes around block-256/512. 
More importantly, for those $\ge$13B models, small quantization block size makes the quantization error belong to \classone, which means the accuracy degradation is almost negligible.

We also report full 3-bit quantization results in~\appref{sec:3bit_weightonly_quantization}.


\begin{table}[t]
\caption{
Results of \bloom-176B with different quantization bits
See~\tref{tab:bloom-176-different-bits-full} for full results.
N/A means that we did not perform the evaluation.
}\centering
\label{tab:bloom-176-different-bits}
\begin{adjustbox}{width=0.6\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Bits     & 3   &4    &5 &6 &7 &8 \\
\midrule
Per-row &49.46 &11.02 &10.93 &10.90 &10.90 &10.90\\
1024    &11.15 &10.96 &10.91 &10.90 &10.90 &10.90\\
32      &11.12 &10.95 &10.91 &N/A&N/A&N/A\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\paragraph{Fine-grained VS. Higher Bits}
Would higher (more) bits with coarse-grained quantization be better than lower bits with finer-grained quantization? 
To answer this, we select the most robust model in our study, i.e., \bloom-176B, to perform 3 to 8 bits asymmetric weight-only quantization.
We use 32 as the smallest block size, since with such a small block size, the real effective bit precision is N+1 bits (for every 32 numbers, we need 2 FP16 numbers, scaling and bias values).

The results are presented in~\tref{tab:bloom-176-different-bits}.
As can be seen, finer-grained quantization cannot have better performance than higher bits quantization with relatively coarser granularity for all cases. 
Another interesting noticeable point is that 6-bit quantization can achieve no-loss quantization.
However, how to achieve good system performance when using non-standard bit precision (e.g., 6 bits) is a big challenge (we give potential solutions in~\sref{sec:conclusions}).


\subsubsection{Summary}
By testing the optimization-based \ptq method with a fine-grained quantization scheme, here is a short summary:
\begin{itemize}
    \item Larger models ($\ge$10B) can achieve \classone quantization error for 4-bit quantization.
    They can benefit from low-precision quantization as the model size with INT4 is similar to an INT8 2x smaller model and the accuracy is better.
    \item Smaller models ($\le$10B) usually can only achieve \classtwo or \classthree quantization error.
    As such, the usage of 4-bit quantization needs to be carefully evaluated for those models.
    \item Fine-grained quantization cannot match the accuracy of more-bit quantization even if the real model size is similar.
    However, how to utilize non-standard bit-precision is still challenging.
\end{itemize}

\subsection{Evaluation of Weight and Activation Quantization}

\begin{table}[t]
\caption{
\opt W4\asym-A8 with various block-size out of the best result from \gptq, \zqlocal, and \zqglobal. 
See~\tref{tab:opt-4bit8bit-blocksize-full} for full results including \rtn.
}\centering
\label{tab:opt-4bit8bit-blocksize}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Quantization Scheme     & 125m	& 350m 	& 1.3b	& 2.7b	& 6.7b	& 13b &30b	& 66b \\
\midrule 
W16-A16 &28.27 &22.93 &15.44 &13.58 &11.90 &11.22 &10.70 &10.33\\
W4\asym Per-row and A16  &31.77 &24.45 &16.42 &14.19 &12.28 &11.42 &10.78 &10.52 \\
W4\asym 128 and A16  &30.04 &23.99 &15.85 &13.83 &12.10 &11.28 &10.74 &10.44\\
\midrule
W4\asym full row and A8\sym 128 &31.85 &24.56 &16.48 &14.22 &12.31 &11.42 &10.76 &10.63\\
\midrule
W4\asym 128 and A8\sym 128 &30.06 &24.07 &15.84 &13.86 &12.05 &11.31 &10.73 &10.43\\
\midrule
W4\asym full row and A8\asym 128 &32.10 &24.58 &16.40 &14.20 &12.29 &11.45 &10.80 &10.61\\
\midrule
W4\asym 128 and A8\asym 128 &30.16 &24.02 &15.86 &13.84 &12.04 &11.31 &10.75 &10.45\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[t]
\caption{\bloom W4\asym-A8 with various block-size out of the best result from \gptq, \zqlocal, and \zqglobal. 
See~\tref{tab:bloom-4bit8bit-blocksize-full} for full results including \rtn.
}\centering
\label{tab:bloom-4bit8bit-blocksize}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Quantization Scheme     & 560m   &1.1b   & 1.7b  & 3b & 7.1b & 176b \\
\midrule
W16-A16  &29.35 &28.32 &20.43 &17.58 &14.96 &10.90 \\
W4\asym Per-row and A16 &31.08 &30.85 &21.38 &18.33 &15.50 &11.02 \\
W4\asym 128 and A16     &30.35 &29.13 &20.92 &17.90 &15.17 &10.94\\
\midrule
W4\asym full row and A8\sym 128 &31.28 &34.58 &21.57 &18.32 &15.49 &11.03\\
\midrule
W4\asym 128 and A8\sym 128 &30.45 &29.29 &20.95 &17.92 &15.19 &10.95\\
\midrule
W4\asym full row and A8\asym 128 &31.24 &34.64 &21.59 &18.31 &15.52 &11.03\\
\midrule
W4\asym 128 and A8\asym 128 &30.42 &29.25 &21.27 &17.86 &15.19 &10.96\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\paragraph{W4A8 Quantization}
We show four different settings of W4A8 quantization of \opt and \bloom in~\tref{tab:opt-4bit8bit-blocksize} and \ref{tab:bloom-4bit8bit-blocksize}, respectively. 
We restrict the activation quantization block size to 128.

Thanks to the small activation quantization block size, there is no accuracy difference between symmetric and asymmetric quantization schemes.
For large enough models (e.g., $\ge$10B), using such fine-grained activation quantization does not introduce much quantization error as compared to weight-only (either per row or per 128 elements) quantization, except for full-row weight quantization on \opt-66B\samethanks[3]. 
For smaller models, fine-grained activation quantization plus per-row weight quantization usually has a larger accuracy drop (around $0.1$ \ppl drop) than per-row weight-only quantization. 

\begin{table}[t]
\caption{
Results of \bloom-176B with different quantization block sizes on activation. 
Here weight is always asymmetrically quantized with block size 128.
See~\tref{tab:bloom-176-different-blocks-full} for full results.
}\centering
\label{tab:bloom-176-different-blocks}
\begin{adjustbox}{width=0.6\linewidth}
\centering
\begin{tabular}{lcccccccccccccc }
\toprule
Block Size  & 1024   &512    &256 &128 &64 &32 \\
\midrule
\ppl &10.98 &10.97 &10.95 &10.95 &10.95 &10.95\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\paragraph{Different Quantization Block Sizes}
We report the effects of different activation quantization block sizes in~\tref{tab:bloom-176-different-blocks} on \bloom-176B. 

As expected, smaller block sizes bring better accuracy compared to larger block sizes.
The performance improvement plateaus after the size reaches 256, which matches the numbers that INT8 can represent. 
Note that although INT8 can represent 256 different numbers, there is still an activation quantization error since we use uniform quantization.

\subsubsection{Summary}
With the comprehensive test on \opt and \bloom model families, here is the summary:
\begin{itemize}
    \item With fine-grained activation quantization, the quality degradation of symmetric and asymmetric schemes is similar.
    For larger models (>10B), the difference between weight-and-activation quantization and weight-only quantization is negligible. 
    \item The benefit from fine-grained activation quantization vanishes when the block size reaches 256.
\end{itemize}