\section{Fine-grained Quantization and Its Evaluation with \ptq}
With \ptq and row-wise quantization, we can hardly achieve \classone quantization error for either weight-only or weight-and-activation quantization.
As such, it is generally better to use a smaller model with INT8 weight quantization than a 2x larger model with INT4 weight quantization. 

One way to solve this problem is to use finer-grained quantization schemes~\cite{darvish2020pushing}, i.e., every $k$ elements have their own scaling factor and/or zero point. 
This can significantly reduce the quantization error. 
For the extreme case, i.e., every $1$ element has its own scaling factor, we can exactly recover the original FP16 number.
More importantly, such block-k quantization can be implemented on modern GPU (one of the most popular deep learning architectures) since the compute unit (streaming multiprocessor) of GPU process tiles of data (e.g., 128 by 128 tiling size) for matrix computation.


\subsection{Settings}
Although fine-grained quantization can greatly close the gap between the quantized tensor and its floating point counterpart, later we show that it still leaves a non-trivial accuracy gap if \rtn is applied. 
Therefore, built upon fine-grained quantization, we also apply the existing optimization-based methods to further boost the accuracy. 
Particularly, we use \gptq and \zqglobal for all models and settings, and use \zqlocal for \opt-66B and \bloom-176B.
For hyper-parameters used for \zqglobal and \zqlocal, we choose the top three found in~\sref{sec:evaluation_of_existing_methods} for all models except for \bloom-176B, for which we only use the top-one hyperparameter, to reduce the training cost.