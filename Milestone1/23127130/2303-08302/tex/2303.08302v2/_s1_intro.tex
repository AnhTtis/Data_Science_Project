\section{Introduction}
\label{sec:intro}
Large language models (LLMs) have been shown breakthrough performance on various benchmarks, e.g., natural language understanding and generation, and have been adopted for daily usage, e.g., Codex~\cite{copilot} and ChatGPT~\cite{chatgpt}.
However, how to \textit{efficiently} serve those LLMs becomes urgent due to their large memory consumption and heavy computation requirement. 

Different than classification models or diffusion models, \llms have their own serving challenge.
Generally, classification models run inference once per query and diffusion models have the same inference behavior for every time step.
However, \llms have two phases, i.e., prompt and generation:
the prompt stage takes the query/question (a sequence of tokens) from the user and runs one forward pass, 
then the generation stage auto-regressively (token-by-token) generates the corresponding answer by running the model for multiple steps. 
The fundamental bottlenecks for prompt and generation phases are different. 
Particularly, for a normal prompt stage (e.g., sequence length $\geq256$), the forward pass is primarily compute bounded, i.e., higher compute brings better latency;
for the normal generation phase (low batch size) with KV (key and value for attention) cache, the forward pass is mainly memory bounded, i.e., higher memory bandwidth brings better performance. 
See~\cite{pope2022efficiently} for a more detailed analysis.

Meanwhile, as mentioned in~\cite[Figure 3]{gholami2020ai}, the bandwidth of hardware increases about 1.4x every two years while the compute increases about 3.1x every two years.
Additionally, multiple nodes are now required to serve extra large models, e.g., 2 A100-80G nodes for MT-NLG-530B~\cite{smith2022using} and 2 A100-40G nodes for GPT-3-175B~\cite{brown2020language}, which introduces the extra bandwidth challenge between cross-node communication.
As such, reducing the model size for LLMs is an urgent request. 
Meanwhile, if we can also reduce the compute cost, it will cover both prompt and generation phases to further alleviate the serving challenge for \llms. 

Considering the forbidden training/finetuning cost for those \llms, one of the most effective ways to alleviate those memory/compute challenges is post-training quantization (\ptq), where no/minimal training is required to reduce the bit precision for weights and/or activations to INT4 or INT8.
Several works, e.g.,~\cite{yao2022zeroquant,frantar2022gptq,xiao2022smoothquant,dettmers2022case} have shown the effectiveness of \ptq, but none of them gives a systematic study, e.g., the functional coverage for different \ptq methods, the sensitivity of different models, etc.  

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/opt-tradeoff.pdf}
    \includegraphics[width=0.49\textwidth]{figures/bloom-tradeoff.pdf}
    \caption{The model size and quality trade-off of different quantization methods on models from \opt and \bloom families.
    Here \ptq (with fine-grained quantization) represents the method from~\cite{yao2022zeroquant,frantar2022gptq}, 
    \rtn means the naive round-to-nearest baseline (with fine-grained quantization as well),
    and FP16/INT8 is used as the no-accuracy-loss baseline. 
    Note that we drop all diverged points for better visualization. 
    For all detailed numbers, please see~\appref{sec:full_tables_results}.}
    \label{fig:main_figure}
\end{figure}

In this work, we provide a comprehensive study on the quantization effect for both weigh-only quantization and weight-and-activation quantization using different quantization schemes, e.g., symmetric and asymmetric quantization, with various \ptq methods, including round-to-nearest (\rtn), \gptq~\cite{frantar2022gptq}, ZeroQuant~\cite{yao2022zeroquant} and its variants, on two different model families \opt~\cite{zhang2022opt} and \bloom~\cite{scao2022bloom} across model sizes from 125M to 176B.
In summary, our observations are as follows.
\begin{itemize}
\item[] \textbf{Sensitivity Analysis}(\tref{tab:ptq_challenge_opt_average_in_maintext} and~\ref{tab:ptq_challenge_bloom_average_in_maintext})
    \begin{itemize}
        \item 
        We demonstrate that INT8 weight-only quantization does not have any model quality effect. 
        For INT4 weight-only quantization, larger models usually exhibit better quantization tolerance as compared to relative smaller models. 
        \item Activation quantization is generally more sensitive to quantization as compared to weight quantization. 
        Smaller models usually have better activation quantization performance than the relative larger model. 
        \item Different model families show entirely different INT8 activation quantization behaviors. 
        Particularly for large models, \bloom-176B still has meaningful accuracy (about 1 perplexity, \ppl in short, point drop) but \opt-30B and -66B have much worse performance. 
    \end{itemize}
\item[] \textbf{Existing \ptq Method Analysis}(\tref{tab:weight_only_quantization_opt_existing_method_average_in_main_text}, \ref{tab:weight_only_quantization_bloom_existing_method_average_in_main_text}, \ref{tab:weightactivation_quantization_opt_existing_method_average_in_main_text}, and~\ref{tab:weightactivation_quantization_bloom_existing_method_average_in_main_text})
    \begin{itemize}
        \item Existing methods can significantly reduce the quantization error as compared to the round-to-the-nearest baseline. 
        Different \ptq methods have their own best working scenarios.
        \item The current existing method can barely achieve less than 0.1 \ppl points degradation for either INT4 weight-only or W4A8 weight-and-activation (i.e., INT4 weight and INT8 activation) quantization. 
    \end{itemize}
\item[] \textbf{Fine-grained Quantization Effect}(\tref{tab:opt-4bit-blocksize}, \ref{tab:bloom-4bit-blocksize}, \ref{tab:opt-4bit8bit-blocksize}, \ref{tab:bloom-4bit8bit-blocksize}, \ref{tab:bloom-176-different-bits}, and~\ref{tab:bloom-176-different-blocks})
    \begin{itemize}
        \item With further help from fine-grained quantization, \ptq is able to achieve <0.1 \ppl points degradation for large models (>13B) with either weight-only quantization or weight-and-activation quantization.
        
        \item Larger models can use relative coarse-grained weight quantization (e.g., block size 128/256 for \bloom-176B) to achieve good quantization error as compared to smaller models (e.g., block size 32/64 for \opt-30B).
        \item For \bloom-176B, coarse-grained (per-row) weight quantization with higher bits (e.g., 5 bits) always leads to better accuracy as compared to fine-grained quantization with lower bits (e.g., 4 bits with 32 elements as the quantization block size), even if the real bit precision is similar.
    \end{itemize}

\end{itemize}



We provide model size and model quality trade-offs of models from \opt and \bloom families in~\fref{fig:main_figure}. 
As can be seen, using \ptq optimization methods from~\cite{yao2022zeroquant,frantar2022gptq} and fine-grained quantization, we set up a new quantization Pareto frontier for \llms.   
Meanwhile, we recommend the following setting for quantizing \llms (note that activation quantization should be only applied if necessary): 
(1) For larger models (>10B), fine-grained (block size 64--256) 4-bit weight quantization plus 8-bit activation quantization (block size 64--256) with \ptq methods can be used for real deployment;
(2) For middle-size models (<10B and >1B), per-row INT8 quantization plus fine-grained (block size 64--256) INT8 activation quantization can be used with \ptq methods from~\cite{frantar2022gptq,yao2022zeroquant};
(3) For smaller models (<1B), directly apply per-row W8A8 (INT8 weight and INT8 activation) \rtn is enough based on~\cite{yao2022zeroquant}.
