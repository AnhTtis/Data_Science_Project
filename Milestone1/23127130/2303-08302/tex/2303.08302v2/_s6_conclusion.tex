\section{Future Opportunity and Conclusion}
\label{sec:conclusions}
\paragraph{Future Opportunity}
Throughout the paper, we see several important but unresolved problems from current quantization schemes and/or algorithms, and we find new potential directions for \llm compression:
\begin{itemize}
    \item \zqglobal used in the paper has worse accuracy than \gptq even though it uses a larger training subnetwork. 
    This is very counter-intuitive since if we increase the subnetwork to the full network, \qat (quantization-aware training) is performed which should have better performance. 
    A further understanding is needed and/or a better algorithm is needed.
    \item Although we use fine-grained quantization schemes in the paper, the real implementation is missing. 
    \item How to efficiently implement odd bit precision is also challenging. 
    \cite{frantar2022gptq} demonstrated that 3-bit can achieve better throughput in the generation phase by packing all 3-bit numbers in continuous memory space. 
    However, this method is sub-optimal as the dequantization step needs to connect bits from different bytes.
    One possible way to implement odd bits, e.g., 5 bits, is to use two integer matrices with INT4 and INT1.
    During the dequantization stage, we couple the two matrices together. 
    \item How to combine \ptq with other lightweight compression techniques, e.g., post-training pruning~\cite{kwon2022fast,frantar2023massive}, is an interesting direction to further reduce the memory consumption and compute cost. 
\end{itemize}

\paragraph{Conclusion}
In this work, we provide a comprehensive study (tens of thousands of zero-shot evaluations) of post-training quantization (\ptq) on large language models with different quantization schemes (symmetric vs. asymmetric), different \ptq methods (e.g., \rtn, \gptq, ZeroQuant), and different quantization coverage (weight-only and weight-and-activation quantization), etc. 
We find that \ptq methods are critical to improving the quantized model quality.
Our results show that although fine-grained quantization can bring acceptable accuracy and model size trade-off, the best way to maintain model quality is to use higher bits.
We also list several potential future directions and hope our work sheds some light on \llms compression.