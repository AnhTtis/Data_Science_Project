\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
%\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{subfigure}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{multirow}
%\usepackage{algorithm}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{bbding}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Subjective and Objective Quality Assessment for in-the-Wild Computer Graphics Images}

\author{Zicheng Zhang, Wei Sun, Tao Wang, Wei Lu, Quan Zhou, Jun he, Qiyuan Wang,  \\ Xiongkuo Min, \emph{Member, IEEE,} and Guangtao Zhai, \emph{Senior Member, IEEE}

        % <-this % stops a space

\IEEEcompsocitemizethanks {\IEEEcompsocthanksitem Zicheng Zhang, Wei Sun, Tao Wang, Wei Lu, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, 200240 Shanghai, China. E-mail:\{zzc1998,sunguwei,f1603011.wangtao,SJTU-Luwei,minxiongkuo,zhaiguangtao\}@sjtu.edu.cn.\protect 

\IEEEcompsocthanksitem Quan Zhou, Jun he, and Qiyuan Wang are with the Department of Video Cloud Technology, Bilibili Inc, Shanghai 200433, China. E-mail:\{zhouquan,hejun,wangqiyuan\}@bilibili.com.\protect }

\thanks{Corresponding author: Guangtao Zhai.}% <-this % stops a space
}

% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Computer graphics images (CGIs) are artificially generated by means of computer programs and are widely perceived under various scenarios, such as games, streaming media, etc. In practical, the quality of CGIs consistently suffers from poor rendering during the production and inevitable compression artifacts during the transmission of multimedia applications. However, few works have been dedicated to dealing with the challenge of computer graphics images quality assessment (CGIQA). Most image quality assessment (IQA) metrics are developed for natural scene images (NSIs) and validated on the databases consisting of NSIs with synthetic distortions, which are not suitable for in-the-wild CGIs. To bridge the gap between evaluating the quality of NSIs and CGIs, we construct a large-scale in-the-wild CGIQA database consisting of 6,000 CGIs (CGIQA-6k) and carry out the subjective experiment in a well-controlled laboratory environment to obtain the accurate perceptual ratings of the CGIs. Then, we propose an effective deep learning-based no-reference (NR) IQA model by utilizing multi-stage feature fusion strategy and multi-stage channel attention mechanism. The major motivation of the proposed model is to make full use of inter-channel information from low-level to high-level since CGIs have apparent patterns as well as rich interactive semantic content. Experimental results show that the proposed method outperforms all other state-of-the-art NR IQA methods on the constructed CGIQA-6k database and other CGIQA-related databases. The database along with the code will be released to facilitate further research.
\end{abstract}


% What's more, the experimental results reveal that the authentic distortions are harder to evaluate than synthetic distortions, which again demonstrate the practical value and contributions of the constructed in-the-wild CGIQA-6k database. 


\begin{IEEEkeywords}
Computer graphics images, in-the-wild distortions, image quality assessment, no-reference, multi-stage feature fusion, multi-stage channel attention
\end{IEEEkeywords}

%\vspace{0.3cm}
\section{Introduction}
\IEEEPARstart{W}{ith} the rapid development of computer graphics rendering techniques, billions of computer graphics images (CGIs) have been generated and perceived in various applications. Unlike natural scene images (NSIs) that are captured with cameras in the real world, CGIs are rendered through a 2D or 3D model described by means of a computer program \cite{min2017unified,pharr2016physically}, which has been widely used in architecture, video games, simulators, movies, etc. Different from the generative models based on deep neural networks such as GAN \cite{goodfellow2014generative}, Nerf \cite{mildenhall2020nerf}, etc., computer graphics directly synthesize images using mathematical and computational techniques without learning from the specific training data. According to the rendering techniques and application scenarios, CGIs can be divided into photorealistic images and non-photorealistic images. Photorealistic images are generated to achieve photorealism by modeling the real world \cite{greenberg1997framework}, while non-photorealistic images promote the prosperity of digital art by enabling a wide variety of
expressive styles. 
Regardless of the rendering techniques and purposes, both photorealistic and non-photorealistic images inevitably suffer from various distortions, such as texture loss caused by the limited computation resources, poor visibility caused by wrong exposure setting, and blur caused by low rendering accuracy, etc. Additionally, a large part of CGIs are transmitted through the network services like cloud gaming and live broadcasting \cite{laghari2019quality}. In most practical cases, CGIs are constantly influenced by the compression distortion, which damages the user's Quality of Experience (QoE) \cite{chen2013quality}. 







In the last decade, many image quality assessment (IQA) models have been proposed to tackle quality assessment issues. According to the involving extent of reference images, image quality assessment can be categorized into full-reference (FR), reduced-reference (RR), and no-reference (NR) methods \cite{zhai2020perceptual}. A variety of IQA measures have been proposed \cite{mittal2012no,mittal2012making,li2015no,saad2012blind,min2018blind,narvekar2011no,min2017unified,gu2014using,zhang2018blind,ke2021musiq,su2020blindly,wang2021multi,sun2021blind} and achieved huge success on IQA tasks for natural scene content. 
However, several studies have proven that the state-of-the-art (SOTA) IQA models designed for NSIs are not suitable for images with different statistics distributions from natural scene content \cite{yang2015perceptual,wang2016subjective,ling2020subjective,yu2022subjective}.  Compared with NSIs, CGIs often contain more regular geometric shapes, simpler texture, and relatively less content diversity.  To verify the statistical difference between NSIs and CGIs, we present 5 quality-related attributes distributions of over 10,073 in-the-wild NSIs and  18,031 in-the-wild CGIs for comparison. The NSIs are sourced from the in-the-wild KonIQ-10k IQA database \cite{koniq10k} and the CGIs are collected through the LSCGB database \cite{bai2021robust}. The quality-related attributes include light, contrast, colorfulness, blur, and spatial information (SI), the details description of which can be referred to in \cite{hosu2017konstanz}. Fig. \ref{fig:diff} illustrates the quality-related distributions of the NSIs and CGIs, from which we can see that the normalized probability distributions of NSIs' quality attributes tend to be more centered at zero while the normalized probability distributions of CGIs' quality attributes are relatively irregular in skewness. Specifically, the CGIs contain relatively lower illumination levels and lack colorfulness. 


Thus, it is unreasonable to simply transfer IQA models based on NSIs scope to computer graphics IQA (CGIQA). Moreover, the mainstream well-performing IQA methods are mostly designed in the data-driven manner, which highly depends on the quality of corresponding databases. To promote the development of natural scene IQA (NSIQA), various NSI databases have been carried out \cite{sheikh2006statistical,ponomarenko2015image,larson2010most,lin2019kadid}. However, the progress of CGIQA database falls behind: 1) A large part of the publicly available CGIQA-related databases are organized in former times. The selected CGIs are sourced from limited types of media and are generated by outdated rendering techniques, thus such CGIs may not be able to cover the range of current CGIQA tasks. 2) The existing CGIQA-related databases are relatively small in scale, which is difficult for supporting the designing and training of algorithms based on the deep neural networks, which have been the dominant methods for the IQA tasks. 3) Many previous works mainly focus on full-reference (FR) methods and the distortions are manually introduced to the reference images. However, the pristine reference CGIs are usually not available in practical situations and the distortions are complex and unpredictable, which therefore increases the need for evaluating the quality of in-the-wild CGIs in the NR manner. 




Therefore, to address the above challenges, we first establish a large-scale CGIQA database containing 6,000 CGIs.  In order to construct the benchmark for the CGIQA task, we select several SOTA NR IQA models including handcrafted-based and deep learning-based methods for comparison. What's more, we propose a deep learning-based NR IQA model through the multi-stage feature fusion and the multi-stage channel attention to help promote the performance on the proposed CGIQA database. The experimental results show that the proposed method is more effective for predicting the quality scores of CGIs. The major contributions of our work are summarized as follows:
\begin{itemize}
    \item To the best of our knowledge, we construct the largest in-the-wild CGIQA database (CGIQA-6k), which consists of 6,000 CGIs from games and movies along with the subjective quality scores. Our database covers a wider range of resolutions (480p$\sim$4K) and contains more diverse content as well as authentic distortions. 
    \item We especially design a deep learning-based model by using the multi-stage feature fusion and the multi-stage channel attention. These two modules are utilized to fuse and strengthen features from low-level to high-level, which enables the model to better understand the quality levels of CGIs. 
    \item The proposed method and some mainstream NR IQA models are validated on the CGIQA-6k database along with other CGIQA-related databases to provide the benchmark for CGIQA tasks. The statistical test and ablation study are conducted for further analysis. In-depth discussions about the experimental performance are given as well.
\end{itemize}

The remainder of this paper is organized as follows. Section \ref{sec:related} gives a brief introduction of the related work. Section \ref{sec:subjective} describes the establishment of the CGIQA-6k database and the subjective experiment. Section \ref{sec:method} presents the details of the proposed IQA model. Section \ref{sec:experiment} gives the experimental performance of the proposed method and other SOTA NR IQA models. The ablation study and statistical test are also conducted and discussed. Section \ref{sec:conclusion} concludes this paper. 



\begin{figure}[t]
    \centering
    \subfigure[Sample NSI]{\includegraphics[width = 3.8cm]{nsi_mscn.png}}\hspace{0.18cm}
    \subfigure[Sample CGI]{\includegraphics[width = 3.8cm ]{cgi_mscn.jpg}}
    \subfigure[NSI distributions]{\includegraphics[width = 4cm,height = 2.5cm]{nsi_diff.pdf}}
    \subfigure[CGI distributions]{\includegraphics[width = 4cm,height = 2.5cm]{cgi_diff.pdf}}
    \caption{The normalized probability distributions of the quality-related attributes for NSIs and CGIs. The distributions are obtained from 10,073 NSIs in the KonIQ-10k IQA database \cite{koniq10k} and 18,031 CGIs in the LSCGB database \cite{bai2021robust} respectively. The 'color' indicates the colorfulness of the images and the 'SI' (spatial information) stands for the content diversity of the images.}
    \label{fig:diff}
    \vspace{-0.4cm}
\end{figure}



\section{Related Work}
\label{sec:related}
In the section, we briefly summarize the development of NR IQA models as well as the construction of CGIQA-related databases.

\subsection{No-reference Image Quality Assessment}
\vspace{0.1cm}
Generally speaking, the NR IQA models can be categorized into handcrafted-based methods (extracting features in handcrafted manners) and deep learning-based methods (extracting features using deep neural networks), both of which have been confirmed to be effective for common IQA tasks.
BRISQUE \cite{mittal2012no} employs natural scene statistics (NSS) in the spatial field for quality analysis. NIQE \cite{mittal2012making} makes use of measurable deviations from statistical regularities observed in natural images for evaluation. BLIINDS2 \cite{saad2012blind} further uses NSS approach in the discrete cosine transform (DCT) domain for quality assessment.  CPBD \cite{narvekar2011no} calculates the blur levels by computing the cumulative probability of blur detection. BIBLE \cite{li2015no} focuses on blur-specific distortions based on discrete orthogonal moments. NFERM \cite{gu2014using} studies the quality of images by using free energy principle. UCA \cite{min2017unified}  is a unified content-type adaptive blind IQA measure aiming at compression distortion.

With the development of deep neural networks, many deep learning-based IQA methods have been proposed. DBCNN \cite{zhang2018blind} constitutes two streams of deep neuron networks and deals with both synthetic and authentic distortions.  HyperIQA \cite{su2020blindly} uses a self-adaptive hyper network to deal with the challenges of distortion diversity and content variation for IQA issues. MUSIQ \cite{ke2021musiq} employs a multi-scale image quality transformer to represent image quality levels at different granularities. MGQA \cite{wang2021multi} and StairIQA \cite{sun2021blind} both hierarchically integrate the features extracted from intermediate layers to take advantage of both low-level and high-level visual information. Most of the mentioned IQA measures mentioned above have shown strong ability of predicting quality levels for NSIs on the traditional IQA databases such as LIVE \cite{sheikh2006statistical}, TID2013 \cite{ponomarenko2015image}, CSIQ \cite{larson2010most}, Kadid10K \cite{lin2019kadid}, etc. 



\begin{table*}[t]
\renewcommand\arraystretch{1.2}
\renewcommand\tabcolsep{5pt}
\centering
% \renewcommand\arraystretch{1.2}
\caption{The comparison of previous CGIQA-related databases and our database. The scale indicates the number of stimuli with mean opinion scores (MOSs).}
\vspace{-0.2cm}
\begin{tabular}{ccccccc}
\toprule
Database    &Year   & Scale & Source                & Scope             & Resolution Range   &Public  \\
\midrule
CCT \cite{min2017unified}    &2017       & 528   & PC game images        & Compression distortion       & 720P$\sim$1080P    &Yes       \\
GamingVideoSET \cite{barman2018gamingvideoset}&2018 & 90 & PC game videos & Compression distortion  &480P$\sim$1080P &Yes\\
KUGVD \cite{barman2019no} &2019 & 90    & PC game videos      & Downsampling \& Bitrates control  &480P$\sim$1080P   & Yes    \\
CGVDS \cite{zadtootaghaj2020quality} & 2020 & 225 & PC game videos & Compression distortion  &480P$\sim$1080P   & Yes\\
TGQA \cite{ling2020subjective}   &2020       & 1091  & Mobile game images    & Aesthetic evaluation         & 1080P   &Yes               \\

TGV \cite{wen2021subjective}    &2021       & 1293  & Mobile game videos    & Stull \& Bitrates control              & 480P$\sim$1080P  & No\\
LIVE-YT-Gaming \cite{yu2022subjective} &2021 & 600 & PC game videos   & UGC distortions & 360P$\sim$1080P   &Yes \\
NBU-CIQAD \cite{chen2021perceptual} & 2021 & 2700 & Cartoon images & Brightness, Saturation, Contrast & 1080P & Yes\\
CGIQA-6k(ours)     &2022      & 6000  & Games \& Movies & in-the-wild distortion      & 480P$\sim$4K  &Yes \\
\bottomrule
\end{tabular}
% \vspace{-0.3cm}

\label{tab:compare}
\end{table*}



\begin{figure*}[t]
    \centering
    \includegraphics[width = 18.15 cm]{overview.jpg}
    \caption{Sample images from the CGIQA-6k database. Top three rows: CGIs from games. Bottom three rows: CGIs from movies.}
    \label{fig:overview}
    %\vspace{-0.3cm}
\end{figure*}



\subsection{CGIQA-Related Databases}
The CCT database \cite{min2017unified} is a cross-content-type database including natural scene images, screen content images, and computer graphics images, which mainly focuses on compression distortion. The GamingVideoSET database \cite{barman2018gamingvideoset} contains 576 compressed gaming videos from 24 raw gaming videos and conducts the subjective quality assessment experiment on 90 compressed gaming videos. Similarly, the KUGVD database \cite{barman2019no} obtains 144 distorted gaming videos from 6 reference gaming videos and provides subjective ratings for 90 distorted gaming videos. The CGVDS database \cite{zadtootaghaj2020quality} is carried out for cloud gaming applications and provides 225 gaming videos along with quality scores. The TGQA database \cite{ling2020subjective} focuses on the aesthetic assessment of mobile game images and presents the subjective study for 1,091 mobile game images on multi-dimensional aesthetic factors. The TGV database \cite{wen2021subjective} generates 1,293 mobile gaming sequences encoded with three codecs along with quality scores. The LIVE-YT-Gaming database \cite{yu2022subjective} pays more attention to the user-generated content (UGC) gaming videos and consists of 600 authentic UGC gaming videos with subjective ratings. The NBU-CIQAD database \cite{chen2021perceptual} deals with the quality assessment of cartoon images and contains 2,600 distorted cartoon images with quality scores.
A detailed comparison of the CGIQA-related quality assessment database is given in Table \ref{tab:compare}. Through further observation, it can be clearly found that our database is the largest in scale and covers the most common resolutions.










% extend our previous proposed database CGIQA-1k2 (containing 1,200 CGIs) \cite{subjective2022wang} and 



\section{Subjective Quality Assessment for Computer Graphics Images}
\label{sec:subjective}
Few large-scale databases for CGIs have been developed in the quality assessment field. To further facilitate research, we construct a large-scale database called CGIQA-6k and conduct the corresponding subjective experiment to derive the subjective quality scores. 

% The overview of the proposed CGIQA-6k database is exhibted in Fig. \ref{fig:overview}. 

% focuses on distinguishing computer-generated images from natural-scene images, thus providing 9,213 3D game images and 8,818 3D movie images



\subsection{Data Collection}
The previously published LSCGB database for CGI and NSI detection \cite{bai2021robust} contained 18,031 CGIs sourced from popular 3D games and 3D movies. To cover more range of resolutions, we personally collect 40,000 CGIs through the frames of stream videos from YouTube, Netflix, and Bilibili with different playback settings. Furthermore, we obtain about 10,000 CGIs by recording the screenshots of local game demos. To reduce the effect of uncorrelated distractions, we specifically eliminate the life bars, mini maps, and subtitles. To ensure the diversity of content, we also manually remove the CGIs with close content. Considering different viewpoints can provide different experiences even in the interactive games, we include both first-person view (POV) CGIs and third-person view (TOV) CGIs. The POV CGIs allow the viewers to perceive the scene through the characters' eyes, providing the most immersive feelings \cite{denisova2015first}.  The TOV CGIs give a broader view of the environment and enable the viewers to have a clear sight of the main character \cite{voorhees2012guns}. These mentioned two views make up the majority views of mainstream video games on the market. As the CGIs focus more on storylines rather than interactivity, we choose mostly portrait CGIs and landscape CGIs for evaluation, which are commonly found in movies and game cutscenes. 

Following the process described above, we carefully select a total of 6,000 CGIs, which consists of 3,000 game CGIs and 3,000 movie CGIs. Samples of the selected CGIs are exhibited in Fig. \ref{fig:overview}, where the top three rows are game CGIs and the bottom three rows are movie CGIs.
Generally speaking, most CGIs have been processed through compression or transmission systems, we do not introduce new distortions in addition and pay most attention to the in-the-wild distortions of CGIs. Examples of typical distortions are shown in Fig. \ref{fig:distortions}. 

\begin{figure*}[t]
    \centering
    \subfigure[Game]{\includegraphics[width=.3\linewidth]{game.pdf}}
    \subfigure[Movie]{\includegraphics[width=.3\linewidth]{movie.pdf}}
    \subfigure[All]{\includegraphics[width=.3\linewidth]{all.pdf}}
    \caption{The distribution of the MOSs of the CGIQA-6k database. (a) represents the MOS distribution for game CGIs while (b) indicates the MOS distribution for movie CGIs. (c) exhibits the MOS distribution for all CGIs. Additionally, the mean and standard deviation values of the MOSs are marked on the top right as well.}
    \label{fig:mos}
    \vspace{-0.4cm}
\end{figure*}


\begin{figure}[t]
    \centering
    \subfigure[Confusing exposure]{\includegraphics[width = 4cm]{game_2769.jpg}\label{fig:distortions-a}}
    \subfigure[Texture loss]{\includegraphics[width = 4cm]{game_0811.jpg}\label{fig:distortions-b}}
    \subfigure[Low rendering accuracy]{\includegraphics[width = 4cm]{game_2860.jpg}\label{fig:distortions-c}}
    \subfigure[Compression distortion]{\includegraphics[width = 4cm]{game_2797.jpg}\label{fig:distortions-d}}
    \caption{Examples of some typical distortions of CGIs. The content of (a) is poorly visible due to the confusing exposure and the hair texture is quite ambiguous in (b). The components of (c) exhibit relatively simple geometry shapes with jagged edges caused by low rendering accuracy and the compression distortion damages the perceptual quality of (d). The subjective quality scores for (a), (b), (c), and (d) are marked on the top right. }
    \label{fig:distortions}
\end{figure}









\subsection{Subjective Experiment Methodology}
To obtain the perceptual quality scores of CGIs, the subjective experiment is conducted with the recommendations of ITU-R BT.500-13 \cite{bt2002methodology}. All CGIs are shown in random order with an interface designed by Python Tkinter on an iMac monitor which supports the resolution up to 4096 $\times$ 2304. The screenshot of the designed interface is illustrated in Fig. \ref{fig:interface}, from which we can see that the interface allows users to freely browse the previous and next CGIs, and record the quality scores through a scroll bar. A total of 60 graduate students (38 males and 22 females) are invited to participate in the subjective experiment. The viewers are seated at a distance of around 1.5 times the screen height (45cm) in a laboratory environment with normal indoor illumination. The quality scale scores from 0 to 5, with a minimum interval of 0.1.
The whole experiment is split into 20 sessions, each of which includes the subjective quality evaluation for 300 CGIs. In this way, we limit the experiment time for each session to less than half an hour. Every session is attended by at least 20 viewers so that every CGI is evaluated by at least 20 subjects, which generates more than 20$\times$6,000=120,000 quality ratings in total. 


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{interface.png}
    \caption{Screenshot of the interface for subjective experiments.}
    \label{fig:interface}
\end{figure}

After the subjective experiment, we collect all the quality ratings from the subjects. Let $r_{ij}$ denote the raw rating judged by the $i$-th subject on the $j$-th image, the z-scores are obtained from the raw ratings as follows:
\begin{equation}
z_{i j}=\frac{r_{i j}-\mu_{i}}{\sigma_{i}},
\end{equation}
where $\mu_{i}=\frac{1}{N_{i}} \sum_{j=1}^{N_{i}} r_{i j}$, $\sigma_{i}=\sqrt{\frac{1}{N_{i}-1} \sum_{j=1}^{N_{i}}\left(r_{i j}-\mu_{i}\right)}$, and $N_i$ is the number of images judged by subject $i$.
Then we remove the ratings from unreliable subjects by employing the recommended subject rejection procedure described in the ITU-R BT.500-13 \cite{bt2002methodology}.
The corresponding z-scores are linearly rescaled to $[0,100]$ and the mean opinion score (MOS) of the image $j$ is computed by averaging the rescaled z-scores: 
% \vspace{-0.1cm}
\begin{equation}
M O S_{j}=\frac{1}{M} \sum_{i=1}^{M} z_{i j}^{'},
\end{equation}
where $M O S_{j}$ indicates the MOS for the $j$-th CGI, $M$ is the number of the valid subjects, and $z_{i j}^{'}$ are the rescaled z-scores. The corresponding MOS distributions are exhibited in Fig. \ref{fig:mos}. It can be evidently seen that the MOS distributions follow Gaussian-like distribution. With closer inspections, the movie CGIs tend to gain higher quality scores than the game CGIs, which is consistent with the practical situations that games are usually rendered in real-time manners while movies often have more resources for rendering. 



\begin{figure*}
    \centering
    \includegraphics[width = 18.15cm]{method.pdf}
    \caption{The framework of the proposed method, where $\oplus$ indicates the element-wise summation and $\otimes$ indicates the element-wise multiplication. The proposed MFF and MCA modules are arranged in a series manner.}
     \label{fig:framework}
\end{figure*}



\section{Proposed Method}
\label{sec:method}
In this paper, we propose an effective deep learning-based method to evaluate the quality of CGIs, the framework of which is clearly shown in Fig. \ref{fig:framework}. The framework consists of the feature extraction backbone, the multi-stage feature fusion (MFF) module, the multi-stage channel attention (MCA) module, and the feature regression module. The MFF module is utilized to incorporate useful information from different stages of the feature extraction networks. The MCA module is employed to further improve the feature representation ability through modeling inter-channel relationships of features.



%  In many previous quality assessment tasks \cite{}, both low-level and high-level feature maps have been proven effective for better understanding the quality of images.For example, game CGIs require viewers to understand the character status in time and give effective feedback.
\subsection{Multi-stage Feature Fusion}
\label{sec:MFF}
Games and movies are among the most important artistic mediums since they are believed to pass conceptual ideas, spread emotional powers, and inspire creative thinking \cite{gintere2019new,patton2013games,mackay2017fantasy,mendiburu20123d}. Therefore, the CGIs obtained from games and movies require the viewers to identify the semantic contents before fully understanding the images. In previous quality assessment studies \cite{dhar2011high,kao2017deep}, the semantic information is stated to be closely correlated with the high-level attributes. At the same time, due to the poor rendering and transmission loss, CGIs also suffer from low-level distortions such as blur and texture damage. Therefore, we propose a multi-stage feature fusion (MFF) module to make full use of the quality-aware information from low-level to high-level in CGIQA tasks as well.

Assume that there are $N_{s}$ stages, and $F_{i}$ is the extracted feature map from the $i$-th stage, where $i \in \{0,1, \ldots, N_{s}-1\}$. Considering that the feature maps obtained from different stages have different resolutions, we first apply the 2D adaptive average pooling with the target output size of 7$\times$7 to keep all feature maps having the same resolution:

\begin{equation}
    \hat{F_{i}} = A_{7\times7} F_{i}, 
\end{equation}
where $A_{7\times7}$ represents the adaptive average pooling operations with target output size of 7$\times$7. Then the pooled feature maps are concatenated to form the combined feature maps $\hat{F} \in \mathbf{R}^{C \times 7 \times 7} $ ($C$ is the sum of the number of channels):

\begin{equation}
    \hat{F} = \mathop{\mathbf{Cat}}\limits_{i=0}^{N_{s}-1} F_{i},  
\end{equation}
where $\mathbf{Cat}$ indicates the cumulative concatenation process. To dynamically explore the spatial information across different channels, we use a convolution block which consists of 3 convolution layers to generate the fused feature maps:

\begin{equation}
    \hat{F_{s}} = W_{1\times1}\times W_{3\times3}\times W_{1\times1} \times \hat{F},
\end{equation}
where $W_{1\times1}$ and $W_{3\times3}$ represent the 1$\times$1 and 3$\times$3 convolution operation. The first $1\times1$ convolution layer is used to reduce the channels to a quarter and generates feature maps $W_{1\times1} \times \hat{F} \in \mathbf{R}^{\frac{C}{4} \times 7 \times 7}$, which helps lower the computational complexity. Then the $3\times3$ convolution layer is applied to fuse the spatial information across channels and generates feature maps $ W_{3\times3} \times W_{1\times1} \times \hat{F} \in \mathbf{R}^{\frac{C}{4} \times 7 \times 7}$. Finally, the $1\times1$ convolution layer is employed to adjust the number of channels and we set the number of output channels as the same of the last stage's feature maps and we can get the fused feature maps $\hat{F_{s}} \in \mathbf{R}^{C' \times 7 \times 7}$ ($C'$ is the number of adjusted channels).










\subsection{Multi-stage Channel Attention}
\label{sec:MCA}
 Each channel of the feature maps is believed to function as a feature detector and focuses on representative meanings of the image \cite{woo2018cbam}. For example, the channels may represent specific aspects, possibly such as  
texture, color, illumination (low-level), and interactivity, understandability (high-level), etc, in CGIQA tasks. It is apparent that the features reflected by different channels have different impacts on human perception. Inspired by many channel attention works \cite{hu2018squeeze,li2019selective,woo2018cbam,park2018bam}, we introduce a typical channel attention module to improve the representation of the multi-stage fused feature maps as the multi-stage channel attention (MCA) module.  

At the beginning of this module, we simply squeeze the spatial dimension of the obtained feature maps $\hat{F_{s}}$ using both average-pooling and max-pooling. It is argued that average-pooling together with max-pooling can gather both global and distinctive features, which can help improve the representation power of channel-wise attention \cite{woo2018cbam}. The process can be derived as:

\begin{equation}
\begin{aligned}
    \hat{F_{a}} &= A_{1\times1} \hat{F_{s}}, \\
    \hat{F_{m}} &= M_{1\times1} \hat{F_{s}},
\end{aligned}
\end{equation}
where $A_{1\times1}$ and $M_{1\times1}$ stand for the average-pooling and max-pooling, and such squeeze process generates channel descriptors $\hat{F_{a}} \in \mathbf{R}^{C' \times 1 \times 1}$ and $\hat{F_{m}} \in \mathbf{R}^{C' \times 1 \times 1}$. Then the descriptors are put through multi-layer perceptron (MLP) with one hidden layer, the parameters of which are shared for $\hat{F_{a}}$ and $\hat{F_{m}}$. Similarly, to reduce the computation complexity, the hidden activation size of the MLP is set as $\mathbf{R}^{\frac{C'}{r} \times 1 \times 1}$, where $r$ is the reduction ratio. The MLP output size is still set the same as the input channel descriptors ($\mathbf{R}^{C' \times 1 \times 1}$). The channel attention feature maps can be computed as the sum of forwarded channel descriptors:

\begin{equation}
    \hat{F_{c}} = \delta (MLP(\hat{F_{a}}) \oplus MLP(\hat{F_{m}})),
\end{equation}
where $\oplus$ indicates the element-wise summation operation and $\delta$ denotes the sigmoid function. 
The final strengthened quality-wise feature vector can then be computed with the element-wise multiplication and average pooling:

\begin{equation}
    \mathbf{{F}} = Avg(\hat{F_{c}} \otimes \hat{F_{s}}),
\end{equation}
where $\otimes$ indicates the element-wise multiplication, the final quality-wise feature vector $\mathbf{{F}} \in \mathbf{R}^{C'}$, the multi-stage channel attention feature maps $\hat{F_{c}} \in \mathbf{R}^{C' \times 1 \times 1}$, and multi-stage fused feature maps $\hat{F_{s}} \in \mathbf{R}^{C' \times 7 \times 7}$.
   

\subsection{Feature Regression}
With the extracted features, two Fully Connected (FC) layers are adopted as the feature regression module, which consists of 1024 and 128 neurons respectively:
\begin{equation}
    \hat{Q} = FC(\mathbf{F}),
\end{equation}
where $\hat{Q}$ represents the predicted quality score. The mean squared error (MSE) is utilized as the loss function:
\begin{equation}
    Loss = \frac{1}{m}\sum_{i=1}^{m}\left(\hat{Q}_{i}-Q_{i}\right)^{2}
\end{equation}
where $Q$ represents the ground-truth quality score obtained from subjective experiments and $m$ indicates the number of images in a mini batch.



\section{Experiment}
\label{sec:experiment}
In this section, we conduct experiments on the proposed CGIQA-6k and other CGIQA-related databases. Ablation experiments are carried out to demonstrate the contributions and effect of the proposed modules, the features from different stages, and different backbones. In-depth performance discussions are given as well.
\subsection{Comparing Models}
Since there are no pristine reference images in the proposed CGIQA-6k database, we only select NR IQA models for comparison. The chosen models can be divided into handcrafted-based models and deep learning-based models:
\begin{itemize}
    \item Handcrafted-based models  : These models include BRISQUE \cite{mittal2012no}, NIQE \cite{mittal2012making}, BIBLE \cite{li2015no}, BLIINDS2 \cite{saad2012blind}, BMPRI \cite{min2018blind}, CPBD \cite{narvekar2011no}, UCA \cite{min2017unified}, and NFERM \cite{gu2014using}. Such models extract handcrafted features through prior knowledge, and then employ the extracted features to evaluate the quality levels of images.
    \item Deep learning-based models: These models consist of DBCNN \cite{zhang2018blind}, MUSIQ \cite{ke2021musiq}, HyperIQA \cite{su2020blindly}, MGQA \cite{wang2021multi}, and StairIQA \cite{sun2021blind}. These mentioned models obtain quality-aware information by learning a feature representation from the labeled data and have achieved competitive performance on previous IQA tasks. 
\end{itemize}





\subsection{ Evaluation Criteria}
Four mainstream consistency evaluation criteria are used to measure the correlation between the predicted scores and MOS, which include Spearman Rank Correlation Coefficient (SRCC), Pearson Linear Correlation Coefficient (PLCC), Kendallâ€™s Rank Correlation Coefficient (KRCC),  Root Mean Squared Error (RMSE). The SRCC values represent the similarity between two groups of rankings, the PLCC values describe the linear correlation of two sets of rankings, the KRCC values denote the ordinal association between two measured quantities, and the RMSE values stand for the average distance between the predicted scores and labels. 

Before obtaining the criteria values, a five-parameter logistic function is applied to map the predicted scores according to the practices in \cite{sheikh2006statistical}:
\begin{equation}
\hat{y}=\beta_{1}\left(0.5-\frac{1}{1+e^{\beta_{2}\left(y-\beta_{3}\right)}}\right)+\beta_{4} y+\beta_{5},
\end{equation}
% \begin{equation}
% \hat{y}=\frac{\beta_{1}-\beta_{2}}{1+e^{-\left(\frac{y-\beta_{3}}{\left|\beta_{4}\right|}\right)}}+\beta_{2}
% \end{equation}
where $\left\{\beta_{i} \mid i=1,2, \ldots, 5\right\}$ are parameters to be fitted, $y$ and $\hat{y}$ are the predicted scores and mapped scores respectively.
Besides, the MOSs for all databases are rescaled to a five-point scale for validation.



\begin{table*}[t]
\renewcommand\arraystretch{1.4}
\renewcommand\tabcolsep{4.2pt}
\setlength{\abovecaptionskip}{-5pt}
  \caption{Performance comparison on the CGIQA-6k database and the corresponding subsets.}
  \vspace{-0.05cm}
  \begin{center}
  \begin{tabular}{c|c|c|cccc|cccc|cccc}
    \toprule
    \multirow{2}{*}{Index} & \multirow{2}{*}{Model} & \multirow{2}{*}{Type} & \multicolumn{4}{c|}{Game} & \multicolumn{4}{c|}{Movie} & \multicolumn{4}{c}{All}\\ \cline{4-15}
     &&& SRCC &  PLCC & KRCC & RMSE & SRCC &  PLCC & KRCC & RMSE  & SRCC &  PLCC & KRCC & RMSE\\
    \hline
    A&BRISQUE & \multirow{8}{*}{Handcarfted-based} & 0.4231 & 0.4246 & 0.2855 & 0.5460 & 0.4434 & 0.4316 & 0.3083 & 0.5590  & 0.3979 & 0.4126 & 0.2735 & 0.6531  \\
    B&NIQE &  & 0.1185 & 0.1016 & 0.0907 & 0.6188 & 0.0326 & 0.0114 & 0.0245 & 0.6423  & 0.1091 & 0.0777 & 0.0821 & 0.7035  \\
    C&BIBLE &  & 0.0828 & 0.0704 & 0.0542 & 0.6265 & 0.0268 & 0.0334 & 0.0171 & 0.6448 & 0.0064 & 0.0077 & 0.0031 & 0.7025  \\
    D&BLIINDS2 &  & 0.3696 & 0.3509 & 0.2499 & 0.5872 & 0.3670  & 0.3693 & 0.2490 & 0.5927  & 0.3530 & 0.3449 & 0.2373 & 0.6704  \\
    E&BMPRI &  & 0.2305 & 0.2378 & 0.1548 & 0.6010 & 0.2245 & 0.2514 & 0.1536 & 0.6046  & 0.1927 & 0.1933 & 0.1292 & 0.6944  \\
    F&CPBD &  & 0.1952 & 0.1857 & 0.1307 & 0.6331 & 0.1643 & 0.1321 & 0.1119 & 0.6204  & 0.1519 & 0.1536 & 0.1014 & 0.7006  \\
    G&UCA &  & 0.3044 & 0.3211 & 0.2118 & 0.5822 & 0.3595 & 0.3807 & 0.2484 & 0.5613  & 0.2126 & 0.2254 & 0.1409 & 0.6921 \\
    H&NFERM &  & 0.3660 & 0.4241 & 0.2513 & 0.5543 & 0.3801 & 0.3897 & 0.2592 & 0.6035  & 0.3978 & 0.3679 & 0.2666 & 0.6747  \\ \hline
    I&DBCNN & \multirow{7}{*}{Deep learning-based} & 0.5907 & 0.6439 & 0.4423 & 0.5055 & 0.6533 & 0.6402 & 0.4668 & 0.5492  & 0.6437 & 0.6568 & 0.4593 & 0.5460   \\
    J&MUSIQ &  & 0.6067 & 0.6158 & 0.4347 & 0.5256 & 0.7181 & 0.7140 & 0.5293 & 0.4442  & 0.7104 & 0.7250 & 0.5251 & 0.4870 \\
    K&HypherIQA &  & 0.6271 & 0.6371 & 0.4485 & 0.5533 & 0.6465 & 0.6822 & 0.4666 & 0.4902  & 0.7003 & 0.7224 & 0.5160 & 0.5140 \\ 
    L&MGQA &  & 0.7472 & 0.7586 & 0.5579 & 0.3816 & 0.7494 & 0.7573 & 0.5648 & 0.3992 & 0.7999 & 0.8053 & 0.5865 & 0.4110\\
    M&StairIQA & & 0.7461 & 0.7601 & 0.5567 & 0.3806 & 0.7410 & 0.7493 & 0.5552 & 0.3938 & 0.8117 & 0.8186 & 0.6191 & 0.4082\\
    N&Proposed &    & \bf{0.8001} & \bf{0.8068} & \bf{0.6068} & \bf{0.3583} & \bf{0.8103} & \bf{0.8158} & \bf{0.6216} & \bf{0.3677} & \bf{0.8358} & \bf{0.8393} & \bf{0.6398} & \bf{0.3971}\\
    %\cline{3-6}
    
    \bottomrule
  \end{tabular}
  \end{center}
  \label{tab:cgiqa}

\end{table*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=18.1cm]{error_bar.pdf}
    \caption{Mean SRCC values and standard error bars for methods compared on the CGIQA-6k database and its corresponding subsets. A-N are of the same order as in Table \ref{tab:cgiqa}. }
    \label{fig:errorbar}
\end{figure*}


\subsection{Experiment Setup}
The details of the experiment setup are shown as follows:
\begin{itemize}
    \item Input Size \& Backbone: The 448$\times$448 views are adopted as inputs by center-cropping the resized images with dimensions of 480$\times$480. The ResNet50 \cite{he2016deep} is utilized as the backbone structure for the proposed method and the initial weights are loaded from the model pre-trained on ImageNet \cite{deng2009imagenet}. Specifically, the number $C'$ of adjusted channels mentioned in Section \ref{sec:MFF} and the reduction ratio $r$ mentioned in Section \ref{sec:MCA} are set as 2048 and 16 respectively. For other comparing deep learning-based methods, the default training parameters are utilized.
    \item Optimizer: The Adam optimizer \cite{kingma2014adam} is employed with an initial learning rate set as 1e-5. The default batch size is set as 32. Each training process lasts for 40 epochs.
    \item Training and Testing set: We randomly separate the databases into the training sets and testing sets with a ratio of 8:2. We repeat the training process 10 times and the average results are recorded as the final performance.
\end{itemize}


% Additionally, the distorted images of the CCT-CGI and the NBU-CIQAD databases are synthesised from reference images. The images of the LIVE-YT-gaming database are extracted from videos, therefore we avoid the content overlap between the training and testing set. 



\begin{table*}[t]
\renewcommand\arraystretch{1.4}
\renewcommand\tabcolsep{4.2pt}
\setlength{\abovecaptionskip}{-5pt}
  \caption{Performance results on the CCT-CGI, NBU-CIQAD, and LIVE-YT-Gaming databases.}
  \vspace{-0.05cm}
  \begin{center}
  \begin{tabular}{c|c|c|cccc|cccc|cccc}
    \toprule
    \multirow{2}{*}{Index} & \multirow{2}{*}{Model}  & \multirow{2}{*}{Type} & \multicolumn{4}{c|}{CCT-CGI} & \multicolumn{4}{c|}{NBU-CIQAD} & \multicolumn{4}{c}{LIVE-YT-Gaming} \\ \cline{4-15}
     &&& SRCC &  PLCC & KRCC & RMSE & SRCC &  PLCC & KRCC & RMSE & SRCC &  PLCC & KRCC & RMSE \\
    \hline
    A&BRISQUE  & \multirow{8}{*}{Handcarfted-based} & 0.8468 & 0.8609 & 0.6633 & 0.3605 & 0.4858 & 0.4579 & 0.3744 & 0.6860 & 0.4815 & 0.4986 & 0.3397 & 0.7437  \\
    B&NIQE  & & 0.6888 & 0.6862 & 0.5078 & 0.6760 & 0.4932 & 0.4985 & 0.3412 & 0.8329 & 0.5412 & 0.5864 & 0.3852 & 0.6990\\
    C&BIBLE  & & 0.5941 & 0.6272 & 0.4195 & 0.7204 & 0.1452 & 0.1071 & 0.0950 & 1.0008 &  0.1647 & 0.1344 & 0.1108 & 0.9782 \\
    D&BLIINDS2 & & 0.8894 & 0.8790 & 0.7082 & 0.3124 & 0.3915 & 0.3825 & 0.2700 & 0.9216 & 0.4204 & 0.4839 & 0.2915 &  0.7294  \\
    E&BMPRI  & & 0.8700 & 0.8770 & 0.6751 & 0.3292 & 0.4001 & 0.4140 & 0.2703 & 0.9258 & 0.4461 & 0.4844 & 0.3118 & 0.7746 \\
    F&CPBD  & & 0.6612 & 0.6490 & 0.4799 & 0.4579 & 0.2295 & 0.2414 & 0.1860 & 0.8958 & 0.0184 & 0.0422 & 0.0112 & 0.8232 \\
    G&UCA  & & 0.8459 & 0.8854 & 0.6450 & 0.5822 & 0.2916 & 0.1642 & 0.0817 & 0.9554 & 0.2503 & 0.2910 & 0.1734 & 0.8891 \\
    H&NFERM  & & 0.3570 & 0.3340 & 0.2413 & 0.6094 & 0.0848 & 0.0736 & 0.0580 & 1.0048 & 0.2035 &0.1684 & 0.1385 &0.8782 \\\hline
    I&DBCNN  & \multirow{7}{*}{Deep learning-based} & 0.9300 & 0.9473 & 0.7765 & 0.2562 & 0.7295 & 0.7301 & 0.5296 & 0.7162  & 0.6832 & 0.5240 & 0.5035 & 0.7989 \\
    J&MUSIQ  & & 0.9427 & 0.9458 & 0.7817 & 0.6071 & 0.8419 & 0.8531 & 0.6481 & 0.5792 & 0.6107 & 0.4095 & 0.4435 & 0.8094\\
    K&HypherIQA  && 0.9607 & 0.9696 & 0.8343 & 0.1699 & 0.6297 & 0.6771 & 0.5383 & 0.5831 & 0.5200 & 0.5840 & 0.3705 & 0.7183    \\ 
    
    L&MGQA  && 0.9612 & 0.9627 & 0.8257 & 0.1901 & 0.8916 & 0.8842 & 0.7113 & 0.4712  & 0.7266 & 0.7791 & 0.5376 & 0.5312\\
    M&StairIQA  && 0.9736 & 0.9742 & 0.8609 & 0.1544 & 0.9070 & 0.9041 & 0.7277 & 0.4311  & 0.6808 & 0.7024 & 0.4903 & 0.6032\\
    N&Proposed   && \bf{0.9839} & \bf{0.9838} & \bf{0.8932} & \bf{0.1225} & \bf{0.9337} & \bf{0.9322} & \bf{0.7762} & \bf{0.3610} & \bf{0.7503} & \bf{0.7833} & \bf{0.5511} & \bf{0.5373}\\
    %\cline{3-6}
    
    \bottomrule
  \end{tabular}
  \end{center}
  \label{tab:others}
  \vspace{-0.3cm}
\end{table*}







% b) Among the handcrafted-based methods, BRISQUE \cite{mittal2012no} achieves the best performance, which indicates that the statistic distributions can reflect the quality of CGIs to a certain extent.


\subsection{Performance on the CGIQA-6k Database}
The experimental performance on the CGIQA-6k database and corresponding subsets is clearly summarized in the Table \ref{tab:cgiqa}. The best performance is marked in bold for each column. To further verify the effectiveness and stability of different IQA methods, we also show the mean SRCC values and standard error bars in Fig. \ref{fig:errorbar}. From the above results, we can make several useful observations. a) The deep learning-based methods exhibit significant advantages over the handcrafted-based methods on both mean and standard deviation values. It is because the features extracted by the handcrafted-based methods are designed for natural scene content rather than computer graphics content. The deep learning-based methods, however, are capable of learning a better feature representation for CGIs, thus being more effective.  b) Most deep learning-based methods (except DBCNN \cite{zhang2018blind} and MUSIQ \cite{ke2021musiq}) gain better performance on the all CGIQA-6k database than separate subsets, which reveals that learning from more samples can help improve the model's understanding of CGIs. c) All deep learning-based methods obtain better performance on the movie subset than the game subset. It can be explained that the movie CGIs are more refined, making the perceived distortion levels more obvious. While the game CGIs obtain relatively low initial quality levels limited to the computing resources, thus making the distortions relatively inconspicuous. d) The proposed method achieves the best performance on the CGIQA-6k database (including both subsets) and gains the smallest standard deviation values among all comparing IQA methods, which suggests that the proposed method performs more effectively and consistently.




\subsection{Performance on other CGIQA-related Databases}
\subsubsection{Databases Selection}
To further demonstrate the effectiveness of the proposed method, we also select several existing CGIQA-related databases to compare the performance of the proposed model and the mainstream IQA models, which include the CCT \cite{min2017unified},
NBU-CIQAD \cite{chen2021perceptual}, LIVE-YT-Gaming \cite{yu2022subjective}. A detailed introduction of the selected databases is given as follows:
\begin{itemize}
    \item CCT: The CCT database is a cross-content-type IQA database that deals with the quality assessment for natural scene images (NSIs), computer graphic images (CGIs), and screen content images (SCIs). We only adopt the 528 distorted CGIs for validation and such subset of the CCT database is referred to as the CCT-CGI database in this paper. The distortions are generated by HEVC \cite{sullivan2012overview} and HEVC-SCC \cite{xu2015overview} standards.
    \item NBU-CIQAD: The NBU-CIQAD database focuses on the quality assessment of cartoon images, which has a high correlation with CGIs. The NBU-CIQAD database consists of  1,800 cartoon images corrupted by single type of distortion and 800 cartoon images suffering from multiple types of distortions. The distortions are caused by the manual adjustment of brightness, saturation, and contrast. 
    \item LIVE-YT-Gaming: The LIVE-YT-Gaming database includes 600 authentic user-generated content (UGC) gaming videos and 18,600 subjective quality ratings collected from an online subjective study. We extract frames from the gaming videos with a fixed interval of 60 frames and obtain 3,501 game images for validation. The images share the same quality scores as the corresponding videos.
    
\end{itemize}
We maintain the default experimental setup. Additionally, the distorted images of the CCT-CGI and the NBU-CIQAD databases are synthesized from the corresponding reference images. Therefore, we separate the training and testing sets without content overlap.
\subsubsection{Performance discussion}
The performance on other CGIQA-related IQA databases is given in Table \ref{tab:others}. The best performance is marked in bold for each column.
The detailed analysis is as follows: a) The CCT-CGI database manually employs compression distortions for evaluation. Both handcrafted-based and deep learning-based methods obtain relatively good performance. The SRCC values of all deep learning-based methods are even higher than 0.9. b) The NBU-CIQAD database specifically deals with the color distortions. The deep learning-based methods are superior to handcrafted-based methods on such tasks since the handcrafted features have a low correlation with color information. c) The LIVE-YT-Gaming collects authentic distorted contents. Therefore, all the IQA models exhibit a clear performance drop compared with other databases because assessing authentic distortions is believed to be more difficult.

% In all, we find that the authentic distortions are more difficult to evaluate than synthetic distortions. Assessing authentic distorted CGIs is more challenging and gain more practical interest, which again validates the value of the proposed CGIQA-6k database.  



\begin{figure*}[t]
    \centering
    \subfigure[Game]{\includegraphics[width = 5cm]{heatmap_game.pdf}} \hspace{0.5cm}
    \subfigure[Movie]{\includegraphics[width = 5cm]{heatmap_movie.pdf}}\hspace{0.5cm}
    \subfigure[All]{\includegraphics[width = 5cm]{heatmap_all.pdf}}
    \subfigure[CCT-CGI]{\includegraphics[width = 5cm]{heatmap_cct.pdf}} \hspace{0.5cm}
    \subfigure[NBU-CIQAD]{\includegraphics[width = 5cm]{heatmap_nbu.pdf}}  \hspace{0.5cm}
    \subfigure[LIVE-YT-Gaming]{\includegraphics[width = 5cm]{heatmap_live.pdf}}
    \caption{Statistical test results on the proposed CGIQA-6k, CCT-CGI, NBU-CIQAD, and LIVE-YT-Gaming databases. A black/white block means the row method is statistically worse/better than the column one. A gray block means the row method and the column method are statistically indistinguishable. The metrics are denoted by the same index as in Table \ref{tab:cgiqa} and Table \ref{tab:others} respectively.}
    \label{fig:heatmap}
    %\vspace{-0.4cm}
\end{figure*}






\subsection{Statistical Test}
To further verify the effectiveness of the proposed method, we carry out the statistical test in this section. We follow the same experiment setup as in \cite{statistic-test} and compare the difference between the predicted quality scores with the subjective ratings. All possible pairs of models are tested and the results are listed in Fig. \ref{fig:heatmap}. By further inspections, it can be found that the proposed method is statistically superior to all compared NR IQA models on the CGIQA-6k database and the corresponding subsets.  On the other CGIQA-related databases, our method achieves competitive performance as well. More specifically, the proposed method statistically outperforms 10, 12, and 13 NR IQA models on the CCT-CGI, NBU-CIQAD, and LIVE-YT-Gaming databases respectively.
% It can be seen that our method is significantly superior to 7 compared PCQA metrics on the SJTU-PCQA and the WPC databases while our method also significantly outperforms 7 compared MQA metrics on the CMDM database. Specifically, the FR metric PCQM achieves significantly better performance than our method on both SJTU-PCQA and WPC databases, the FR metric CMDM is insignificantly distinguishable from our method on the CMDM database.


\begin{table}[t]
\caption{Performance Results of the ablation study on the CGIQA-6k database. The mark \checkmark indicates that the referred module is applied while $\times$ indicates that the referred module is not applied.}
\renewcommand\arraystretch{1.4}
\renewcommand\tabcolsep{8pt}
    \centering
    \begin{tabular}{cc|cccc}
    \toprule
         MFF         & MCA          & SRCC & PLCC & KRCC & RMSE \\ \hline
         \checkmark  & \checkmark     & \bf{0.8358} & \bf{0.8393} & \bf{0.6398} & 0.3971 \\
        \checkmark  & $\times$        & 0.8166  & 0.8227 & 0.6257 & 0.3980\\
       $\times$   & \checkmark        & 0.8231  & 0.8293 & 0.6332 & \bf{0.3934}\\  
        $\times$   &  $\times$       & 0.7919  & 0.7968 & 0.5977 & 0.4442\\
    \bottomrule
    \end{tabular}
    \label{tab:ablation}
\end{table}
\subsection{Ablation Study}
Since the proposed method utilizes two modules, we try to quantificationally reveal the contributions of each module. Therefore, the ablation study is carried out on the CGIQA-6k database with ResNet50 as the backbone and keeping the default experiment setup. The experimental results are clearly shown in Table \ref{tab:ablation}, from which we can get several useful conclusions. First, separately attaching the MFF and MCA modules is superior to the backbone, which indicates that both of the proposed modules make contributions to the final results. Second, utilizing both modules gains the highest performance, which further validates the effectiveness of the proposed framework. Third,
employing only MFF module gets lower performance than employing only MCA module, which reveals that the MCA module seems to contribute more in the proposed framework. It may indicate that the channel attention is more important and human perception of CGIs is more easily influenced by specific feature attributes. 







\subsection{Backbone Effectiveness}
The proposed modules can be easily adapted to other CNN modules which contain multiple stages. To further test the performance of different backbones, we select VGG16 \cite{simonyan2014very}, ResNet34, ResNet101 \cite{he2016deep}, and MobileNetV2 \cite{sandler2018mobilenetv2} for comparison. The performance results on the CGIQA-6k database are clearly exhibited in Table \ref{tab:backbone}. From Table \ref{tab:backbone}, we can make several useful observations. First, all backbones with the proposed modules achieve higher performance comparing with the corresponding baseline models (i.e. backbones). Second, benefiting from more trainable parameters, the bare ResNet101 model is slightly superior to the bare ResNet50 model. However, the proposed modules help ResNet50 gain more performance improvement over ResNet101, which shows that ResNet50 is more suitable for this task. Third, as for the light-weight MobileNetV2 model, the designed modules also make improvements to the performance, which demonstrates the potential for use on mobile devices. In all, the proposed modules can effectively incorporate multi-stage features and are capable of improving many popular CNN models, which enables a wider application of such structure for CGIQA tasks with different requirements.   



\begin{table}[t]
\caption{Performance results of the different backbones on the CGIQA-6k database. The mark \checkmark in the 'Modified' column indicates that the proposed modules are applied.}
\renewcommand\arraystretch{1.4}
%\renewcommand\tabcolsep{7pt}
    \centering
    \begin{tabular}{c|c|cccc}
    \toprule
         Backbone     & Modified    & SRCC    & PLCC   & KRCC   & RMSE \\ \hline
        %  VGG13 &$\times$     & 0.7778  & 0.7910 & 0.5848 & 0.4284\\
        %  VGG13 &\checkmark      & 0.7991  & 0.8021 & 0.5970 & 0.4241\\ \hline
         \multirow{2}{*}{VGG16} &$\times$     & 0.7675  & 0.7818 & 0.5744 & 0.4339\\
          &\checkmark      & 0.7831  & 0.7907 & 0.5887 & 0.4254\\\hline
        %  ResNet18     & $\times$  & 0.7664  & 0.7811 & 0.5748 & 0.4339 \\
        %  ResNet18     & \checkmark    & 0.7708  & 0.7858 & 0.5707 & 0.4385\\ \hline
        \multirow{2}{*}{ResNet34}      & $\times$  & 0.7893  & 0.7968 & 0.6332 & 0.4199\\  
        & \checkmark     & 0.8019  & 0.8068 & 0.6377 & 0.4142\\   \hline
         \multirow{2}{*}{ResNet50}  & $\times$  & 0.8002  & 0.8060 & 0.6035 & 0.4114\\
         & \checkmark     & \bf{0.8358} & \bf{0.8393} & \bf{0.6398} & \bf{0.3971}\\ \hline
         \multirow{2}{*}{ResNet101}   & $\times$   & 0.8023  & 0.8105 & 0.6215 & 0.3972\\
        &\checkmark      & 0.8223  & 0.8225 & 0.6365 & 0.3998\\ \hline
         \multirow{2}{*}{MobileNetV2} &$\times$     & 0.7675  & 0.7818 & 0.5744 & 0.4339\\
         &\checkmark      & 0.7991  & 0.7993 & 0.5983 & 0.4276\\
        %  MobileNetV3 &$\times$     & 0.7686  & 0.7779 & 0.5755 & 0.4366\\
        %  MobileNetV3 &\checkmark      & 0.7899  & 0.7929 & 0.6081 & 0.4201\\
    \bottomrule
    \end{tabular}
    \vspace{-0.25cm}
    \label{tab:backbone}
\end{table}



\begin{table}[t]
\caption{Performance Results of the features from different stages on the CGIQA-6k database. The 'S' represents the stage and the mark \Checkmark indicates features from the corresponding stage are included.}
\renewcommand\arraystretch{1.4}
\renewcommand\tabcolsep{7.5pt}
    \centering
    \begin{tabular}{c|ccccc|cc}
    \toprule
     Model & S0 & S1 & S2 & S3 & S4 & SRCC & PLCC \\ \hline
     $None$  &$\times$ &$\times$ &$\times$ &$\times$ &$\times$ & 0.8002 & 0.8060 \\
     $Path0$  &$\times$   &\checkmark &\checkmark &\checkmark &\checkmark & 0.8322 & 0.8340 \\
     $Path1$  &\checkmark &$\times$   &\checkmark &\checkmark &\checkmark & 0.8342 & 0.8347\\
     $Path2$  &\checkmark &\checkmark &$\times$   &\checkmark &\checkmark & 0.8278 & 0.8236\\
     $Path3$  &\checkmark &\checkmark &\checkmark &$\times$   &\checkmark & 0.8233 & 0.8241\\
     $Path4$  &\checkmark &\checkmark &\checkmark &\checkmark &$\times$ & 0.8177 & 0.8225\\
     $All$  &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark & \bf{0.8358} & \bf{0.8351}\\
    
    \bottomrule
    
    \end{tabular}
    \vspace{-0.3cm}
    \label{tab:stage}
    
\end{table}


\subsection{Stages Comparison}
As discussed in Section \ref{sec:MFF}, the proposed modules incorporate low-level and high-level features from different stages. In this section, we try to quantify the contributions of different stages by retraining the model without the features from the specific stage. The default experiment setup is maintained and we use the ResNet50 as the backbone. The performance results are shown in Table \ref{tab:stage}. First, with higher stage's features absent, the models tend to experience a larger performance drop, which indicates that the features from higher stages make more contributions to the performance. Second, among all $Path$ models, $Path4$ achieves the lowest performance while $Path1$ achieves the highest performance, which points out that the features from the 4th stage count most and the features from the 1st stage count least. This phenomenon suggests that the high-level information have a bigger impact on humans' preference for CGIs compared with low-level information. Finally, $path0\sim path1$ models are all inferior to the proposed model, which suggests that features from all stages contribute to the final performance.    



\section{Conclusion}
\label{sec:conclusion}
In this paper, we construct a large-scale in-the-wild CGIQA database named CGIQA-6k. The database consists of 3,000 game CGIs and 3,000 movie CGIs and covers a wider range of resolutions. The subjective quality scores are provided through a well-organized subjective experiment in the laboratory environment. Moreover, we propose a deep learning-based no-reference IQA model via multi-stage feature fusion and multi-stage channel attention. The main purpose of the proposed model is to take advantage of inter-channel relationships from low-level to high-level, which is because CGIs have apparent patterns and rich interactive semantic information. Experimental results show that the proposed method achieves the best performance among all other state-of-the-art IQA methods on the constructed CGIQA-6k database and other CGIQA-related databases. Furthermore, the ablation study, statistical test, backbone comparison, and stage comparison experiments are conducted to give a detailed analysis of the proposed framework. The database along with the code will be released to facilitate further research.



% \newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}





% \vfill
\bibliographystyle{IEEEtran}
%argument is your BibTeX string definitions and bibliography database(s)
\bibliography{output}
\end{document}


