\section{Experiments}\label{sec:exp}
We verify the effectiveness of the proposed DCO meta-learning framework on some illustrative tasks. Specifically, we leverage the DCO optimizer instantiations introduced in Section \ref{sec:dco_meta} to solve linear least squares, system identification, and smooth function interpolation tasks.
%Throughout our experiments, we measure performance or effectiveness of a method by the number of epochs used to convergence to the optimal validation loss.

\subsection{Meta-Training Setup}\label{ssec:meta_tr_instantiation}
%We now outline how we instantiated Algorithm \ref{alg:method} to effectively train the DCO optimizers introduced in Sections \ref{sssec:DCOg}, \ref{sssec:DCOgd}, \ref{sssec:DCOm}.

Meta-parameters $\phi$ were initialized such that the DCO optimizers resemble existing first-order update rules. $\Lambda$ and $M$ were set to constant vectors in \eqref{eq:DCO_gradient} and \eqref{eq:DCO_momentum} to mimic the GD update rules introduced in Section \ref{ssec:firstordermethods}. Similarly, we initialized $B$ as the identity matrix in formulation \eqref{eq:DCO_general_descent}.
    
One potential challenge in training DCO optimizers is in ensuring that the proposed formulations remain well-posed for the entirety of the unrolling of the computational graph represented by Algorithm \ref{alg:method}. While the formulations as unconstrained QPs are by themselves well-posed, from a practical viewpoint potentially ill-posed inputs need to be handled. This is especially true for the initial meta-training epochs, where suboptimal meta-parameters may give rise undesirably small or large inner loop gradients. This was overcome by normalizing inner loop gradients before feeding them into the DCO optimizers.

In our experiments we set $T=1$ with $T$ as defined in Algorithm \ref{alg:method}. Restricting $T$ has the effect of explicitly training the DCO optimizer to perform an aggressive inner loop descent step. From a compututational standpoint, this restriction of $T$ allows to reallocate compute resources from solving several DCOs in the inner loop to performing more meta-parameter updates.

Note that Algorithm \ref{alg:method} allows for any first-order meta-optimizer to perform updates on $\phi$. For simplicity we restrict ourselves to using RMSProp with default hyperparameter settings as suggested in the \texttt{PyTorch} library. 

The DCO optimizers were implemented on a 2.2 GHz single-core CPU using the \texttt{CVXPYLayers} library \cite{cvxpylayers2019} and were solved using general-purpose interior-point solvers. While the implementation could be made more efficient, it suffices to outline the potential of the DCO meta-learning framework to outperform existing first-order baselines.

Throughout the experiments a comparison baseline of first-order methods Adam, SGD and RMSProp was considered due to their prevalence in solving unconstrained minimizations. For each baseline optimizer the learning rate was tuned and the best validation performance was reported.


\subsection{Least Squares Task}\label{ssec:ls}
 We first focus on solving least-squares (LS) problems
\begin{align}\label{eq:ls_problem}
    \min_{\theta\in \R^{100}} \|X\theta - y\|_2^2,
\end{align}
where $X\in\R^{100\times 100}$, $y\in\R^{100}$ with $X_{ij}, y_i\sim \mathcal{N}(0, 1)\ \forall i,j\in [100]$. The meta-training set was constructed by sampling 100 tasks according to \eqref{eq:ls_problem}. For each task, a LS objective was sampled which acts as both as $l_i$ and $\lossv$, i.e. $l_i=\lossv$ for $i\in[100]$. Meta-training was run for $M=20$ epochs. Then 100 new tasks were sampled and the evolution of the average loss across tasks over 30 training epochs was compared with existing first-order methods.  Figure \ref{fig:LS} shows the results. The DCO optimizers exhibit substantially faster convergence compared to classical baselines.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{Figures/Least_Squares_Task1.pdf}
\caption{Optimization performance on 100-dimensional LS tasks. Validation curves are averaged across 100 new tasks.}
\label{fig:LS}
\end{figure}

\subsection{System Identification Task}\label{ssec:system_id}
Next, we consider the task of identifying the underlying nonlinear discrete-time dynamics for population growth. We approximate the Beverton–Holt model given by
\begin{align}
    n_{t+1} = f(n_t):=\frac{R_0 n_t}{K+n_t},
\end{align}
where $n_t$ represents the population density in generation $t$, $R_0>0$ is the proliferation rate per generation, and $K>0$ is the carrying capacity of the environment. To introduce stochasticity into the model we include additive disturbance $d \sim \mathcal{N}(0, 0.1)$. In this context, we define a particular task by sampling a system with $R_0, K\sim \mathcal{U}(1, 2)$ and then generating training and validation samples $\{n, f(n)\}$ with $n\sim \mathcal{U}(0, 10)$. For each task we sample 500 training points and 100 validation points. The goal is to learn the underlying discrete nonlinear dynamics using a feedforward architecture with design ($1$-$5$-$5$-$1$), i.e. 2 hidden layers with 5 units each. The training of each network is carried out on the training set sampled for each task, and the final performance for that task is measured using the mean-square error (MSE) metric on the associated validation set. Meta-training was run for $M=20$ epochs. Figure \ref{fig:SI} presents the performance comparison between considered methods on 100 newly sampled tasks. The DCO optimizers continue to outperform baselines.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{Figures/System_Identification_Task1.pdf}
\caption{Optimization performance on approximating the Beverton–Holt dynamics. Validation curves are averaged across 100 new tasks.}
\label{fig:SI}
\end{figure}


\subsection{Smooth Function Interpolation Task}\label{ssec:function_interpolation}
We finally consider the task of interpolating a real, nonlinear, smooth, univariate function via regression. As an illustrative example, we consider the smooth function
\begin{align}\label{eq:smoothfn}
    g(x) = a\cos(b x)\textrm{exp}(-c|x|)
\end{align}
where $a, b, c \sim \mathcal{U}(0, 1)$. A particular task is constructed by sampling 500 training points and 100 validation points from an instance of $g(x)$. The goal of the task was to learn a feed-forward network (FFN) with architecture ($1$-$10$-$10$-$1$) consisting of $2$ hidden layers with $10$ units each that yields low validation loss. As before, meta-training was run for $M=20$ epochs. The performance comparison with first-order methods on a new set of tasks is shown in Figure \ref{fig:FI}. The validation loss learning curves are averaged over 10 tasks. Similar to previous settings, we have obtained an improved convergence of DCO optimizers over baselines.
\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{Figures/Function_Interpolation_Task1.pdf}
\caption{Optimization performance on smooth interpolation tasks. Validation curves are averaged across 10 new tasks.}
\label{fig:FI}
\end{figure}

