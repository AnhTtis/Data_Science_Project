\section{Meta-Optimization Framework}
Consider the setting where we have $N$ \textit{training} tasks $\task = (\losst, \lossv)$ for $i\in [N]$, where each task consists of a tuple containing a training loss function $\losst$ and an associated performance metric $\lossv$. Each of these tasks is sampled from an underlying distribution $\mathcal{D}$, i.e $\task \sim \mathcal{D}\ \forall i\in[N]$. For task $\task$, we consider the optimization 
\begin{align}\label{eq:optbasic}
    \min_{\theta_i \in\Theta} \losst(\theta_i)
\end{align}
where we aim to minimize loss $\losst$ over the decision variable $\theta$ constrained to the set $\Theta\subset \R^p$. We let $\phi$ denote the set of meta-parameters that configure the method used to solve optimization \eqref{eq:optbasic}, e.g. $\phi$ could include the learning rate in a gradient-based algorithm. The validation loss $\lossv$ is used to evaluate the final $\theta^\star_i$ recovered from solving \eqref{eq:optbasic}. As motivation for this setup, we consider the general training-validation procedure seen in ML. Here $\losst$ can be seen as the loss on training data with respect to model parameters $\theta$ and $\lossv$ denotes the loss on validation data. Note that for problems where the metric of interest is in fact the objective of \eqref{eq:optbasic}, we can trivially define $\lossv:=\losst$.
                                 
In this meta-learning framework, the goal is to perform well on a new task $\tasktarget = (\lossttarget, \lossvtarget) \sim \mathcal{D}$ using previous experience from tasks $\tasks$. Since $\tasktarget$ is sampled from the same distribution $\mathcal{D}$ as the training tasks, it has structural similarities that can be exploited by meta-learning. 


\subsection{Inner Optimization Loop}\label{ssec:training_loop}
Depending on the structure of \eqref{eq:optbasic}, several iterative methods exist to solve the considered problem. The chosen algorithm has an update rule that yields a sequence of parameter updates $\{\thetait{i}{t}\}_{t=1}^{T}$ where $T$ is defines the total number of updates and $i$ indexes the associated task $\task$. Within the class of first-order methods, these update rules require computing or estimating (e.g. within reinforcement learning) the gradient $\grad{i}{t}:=\nabla_{\theta}l_i(\thetait{i}{t})$ to solve the inner optimization of task $\task$. The algorithm $\Alg$ applies the computed first-order and zeroth-order information at time step $t$ along with an abstraction of past information encapsulated by state $\hist{}{t}$ to yield both an updated parameter $\thetait{i}{t+1}$ and state $\hist{}{t+1}$: 

%\textcolor{red}{Shouldn't this take the feature map $d$ instead? For stuff that depends on history, etc.}\textcolor{green}{I'm keeping this general here. It could use what you mention but the notation is simply saying to take in zeroth and first order info. We don't want to explicitly introduce a feature map in the general meta-learning intro yet.}

\begin{align}\label{eq:update_rule}
(\thetait{i}{t+1}, \hist{i}{t+1}) :=\Alg(\thetait{i}{t}, \hist{i}{t}, \grad{i}{t}; \phi).
\end{align}

Here we characterize the optimizer with meta-parameters $\phi$. Solving the inner loop problem to completion involves recursively applying \eqref{eq:update_rule} $T$ times from an initial condition $\thetait{i}{1}$ and history state $\hist{i}{1}$, which we denote by $\Alg_T(\thetait{i}{1}, \hist{i}{1}; \phi)$. Note that moving forward, unless made explicit, we suppress the return argument of the next state, i.e. we utilize the shorthand $\thetait{i}{t+1} :=\Alg(\thetait{i}{t}, \hist{i}{t}, \grad{i}{t}; \phi)$.

\subsection{Meta-Learning Loop}\label{ssec:metatraining_loop}
The meta-learning loop wraps around the inner loop. It aims to find optimal meta-parameters $\phi^\star$ that ensure that for each task $\mathcal{T}_i$ in distribution $\mathcal{D}$, the inner loop optimizer $\Alg$ produces $\theta_i^\star$ that performs well on metric $l_i^{\textrm{val}}(\theta_i^\star)$:

\begin{align}\label{eq:training}
    \min_{\phi} E_{\task\sim\mathcal{D}}[ \lossv(\theta_i^\star(\phi))]
\end{align}
where $E_{\task \sim\mathcal{D}}$ denotes the expectation over task distribution $\mathcal{D}$. An empirical version of this meta-learning process with training tasks $\tasks$ can be formulated as

\begin{align}\label{eq:meta_opt}
    \phi^\star &= \argmin_{\phi} \frac{1}{N}\sum_{i=1}^N \lossv(\theta_i^\star) \nonumber\\
    &= \argmin_{\phi} \frac{1}{N}\sum_{i=1}^N \lossv(\argmin_{\theta\in\Theta} \losst(\theta)) \nonumber\\
    &\approx \argmin_{\phi} \frac{1}{N}\sum_{i=1}^N \lossv(\Alg_T(\thetait{i}{1}, \hist{i}{1}; \phi)), \\
    &\defeq  \argmin_{\phi} \losstotal(\phi)
    %&\approx \argmin_{\phi} \sum_{i=1}^n l_i^{\textrm{val}}(\Alg(\theta_{T-1}, \nabla_{\theta}l_i(\theta_{T-1}); \phi))
\end{align}
where the inner optimization is approximated by running algorithm $\Alg$ for $T$ time steps. Optimization \eqref{eq:meta_opt} can be approximated by another iterative gradient-based scheme that estimates $\nabla_{\phi} \lossv(\theta_i^\star)$. This requires differentiation through the inner loop update rule 
$\Alg$ with respect to meta-parameters $\phi$. More specifically, we require differentiation with respect to $\phi$ through a trajectory of parameter updates with horizon $T$. The meta-parameters will then be updated using a meta-optimizer of choice that uses first-order information on the meta-parameters:

\begin{align}\label{eq:meta_update_rule}
\phit{t+1} :=\MetaAlg\left(\phit{t}, \nabla_{\phi} \losstotal\left(\phit{t}\right)\right).
\end{align}

 
\begin{remark}\label{remark:practicalmetalearning}
Note that an approximated attempt at meta-learning is ubiquitous in practice. More specifically, the notion of hyperparameter selection (e.g. learning rate) for a first-order method is an instance of approximated meta-learning. In this context, we let hyperparameters be viewed as meta-parameters. Given a task, the goal in hyperparameter selection is to identify these such that the algorithm $\Alg$ generates $\theta_i^\star$ with low $\lossv(\theta_i^\star)$. In practice, the selection of hyperparameters (i.e. $\MetaAlg$) is restricted to crude rules of thumb or grid search guided by previous experience of similar problems. It is clear how such approximations can often fall short especially when considering high-dimensional or even continuous meta-parameter search spaces. Moreover, it does not accommodate parameterizing $\Alg$ to describe novel update rules. The meta-learning framework in \eqref{eq:meta_opt} generalizes the hyperparameter selection problem and makes it more rigorous.
\end{remark}

\subsection{Meta-Training}\label{ssec:meta-training}
The meta-training algorithm for an arbitrary optimizer $\Alg$ with meta-parameters $\phi$ is presented in Algorithm \ref{alg:method}. For each meta-parameter update, average validation losses across training tasks $\tasks$ are accumulated in $\losstotal$. For each task $\task$, these validation losses are measured after running the inner loop optimization using $\Alg(\cdot; \phi)$ for $T$ iterations. $\MetaAlg(\cdot, \cdot)$ then uses first-order information on $\losstotal$ with respect to $\phi$ to update the meta-parameters.
\begin{remark}\label{remark:horizon}
    For a specific task $\task$, the role of the meta-optimizer can be viewed as trying to learn the loss landscape of the inner problem locally around $\thetait{i}{t}$ for $t\in[T]$ and adapt the optimizer accordingly to encourage efficient descent. Thus, the updates within the inner loop help the meta-optimizer get a better gauge of the loss landscape. In turn, $T$ should be selected based on how complicated or spurious the inner problem's loss landscape is. For more complicated inner problems, more information (i.e. updates) are necessary to gauge the loss landscape. For simpler problems, a smaller horizon should suffice.
\end{remark}
\begin{remark}\label{remark:stochastic}
    Algorithm \ref{alg:method} allows for flexibility when choosing $\MetaAlg$. Stochastic first-order methods can be employed to solve the meta-training problem. That is, rather than using the entire batch of $N$ tasks $\tasks$, a random minibatch can be selected to perform a meta-parameter update. This strategy becomes particularly useful in settings where $N$ is prohibitively large. Furthermore, adding stochasticity in the $\MetaAlg$ procedure may reap some known benefits of SGD such as not succumbing to local minima. 
\end{remark}


\begin{algorithm}[tb]
   \caption{Meta-Training Framework}
   \label{alg:method}
\begin{algorithmic}
   \STATE {\bfseries Input:} Training set consisting of $N$ tasks $\tasks$
   \STATE {\bfseries Design choices:} Inner loop horizon $T$, meta-training epochs $M$, optimizer $\Alg(\cdot; \phi)$, meta-optimizer $\MetaAlg(\cdot, \cdot)$
   \STATE {\bfseries Return:} Meta-parameters $\phi$
   \vspace*{\baselineskip}
   \STATE {\bfseries begin training}
   \STATE 1. Initialize meta-parameters $\phi^{(1)}$
   \STATE 2. Initialize inner loop parameters and initial optimizer states $\{\thetait{i}{1}, \hist{i}{1}: i\in[N]\}$
   \FOR{$k\in[M]$}
   \STATE 3. Initialize $\losstotal \leftarrow0$
   \FOR{$i\in[N]$}
   \FOR{$t\in[T]$}
   \STATE 4. Compute inner loop gradient $\grad{i}{t}\leftarrow  \nabla_{\theta}l_i(\thetait{i}{t})$
   \STATE 5. $(\thetait{i}{t+1}, \hist{i}{t+1}) \leftarrow\Alg(\thetait{i}{t}, \hist{i}{t}, \grad{i}{t}; \phi)$
   \ENDFOR{}
   \STATE 6. $\losstotal \leftarrow \losstotal + \lossv(\thetait{i}{T+1})/N$
   \ENDFOR{}
   \STATE 7. $\phit{k+1} \leftarrow\MetaAlg(\phit{k}, \nabla_{\phi} \losstotal)$
   \ENDFOR{}
   \STATE {\bfseries end training} 
\end{algorithmic}
\end{algorithm}



\section{Differentiable Convex Optimizers}\label{sec:dco_meta}
We now propose various instantiations of the the inner loop optimization step \eqref{eq:update_rule} as differentiable convex optimizations. More generally, our proposed DCO meta-learning framework parameterizes optimizer $\Alg$ as a DCO introduced in \eqref{eq:dco}:
\begin{align}\label{eq:dco_meta}
    \Alg(\cdot; \phi):= D(\cdot; \phi).
\end{align}
As discussed in Section~\ref{ssec:firstordermethods}, this formulation contains a range of well-known first-order update rules as special cases.

To demonstrate the representational capacity of general DCOs as formulated in \eqref{eq:dco} within the meta-learning context, we focus on the subclass of unconstrained differentiable QPs. Note that this is a narrower subclass of DCO as we no longer have an arbitrary convex objective but rather a convex quadratic one. However, as we will demonstrate, this narrower formulation lends itself naturally to generalize the structure of update rules of existing gradient-based methods. While the formulations themselves admit closed-form solutions, we treat these as convex optimizations in our implementations to stay true to the DCO framework.  

\subsubsection{DCO Gradient}\label{sssec:DCOg}
We propose DCO Gradient based on the convex optimization \eqref{eq:gd_update} that encodes the vanilla GD update rule. The formulation discards the optimizer state $\hist{i}{t}$ and simply encodes the update rule:
\begin{align}\label{eq:DCO_gradient}
    \thetait{i}{t+1} \defeq \argmin_{\theta} \{\big(\grad{i}{t}\big)^\top \theta + \frac{1}{2}||\Lambda \odot (\theta - \theta_i^{(t)})||_2^2\},
\end{align}
where the parameterization is given by $\phi:=\Lambda\in\R^p$. In this formulation, learning the parameter $\Lambda$ can be viewed as optimizing the per-weight learning rate within vanilla GD.

\subsubsection{DCO General Descent (DCOGD)}\label{sssec:DCOgd}
We introduce a generalization of the previous approach that enables a general linear transformation of the update gradient:
\begin{align}\label{eq:DCO_general_descent}
    \thetait{i}{t+1} &\defeq \argmin_{\theta} \{\big( B \grad{i}{t} \big) ^\top \theta + \frac{1}{2} ||\theta - \theta_i^{(t)}||_2^2\},
\end{align}
where $\phi:=B\in\R^{p\times p}$.
\subsubsection{DCO Momentum (DCOM)}\label{sssec:DCOm}
Finally, we extend formulation \eqref{eq:DCO_gradient} to include momentum information:
\begin{align}\label{eq:DCO_momentum}
    \hist{i}{t+1} &\defeq M \odot \grad{i}{t} + (\ones - M) \odot \hist{i}{t}, \\
    \thetait{i}{t+1} &\defeq \argmin_{\theta} \{ \big(\hist{i}{t+1}\big) ^\top \theta + \frac{1}{2} ||\Lambda \odot (\theta - \theta_i^{(t)})||_2^2\},
\end{align}
where the parameterization is given by $\phi:=\{\Lambda, M\in\R^p\}$. Here, the DCO learns both the learning rate and the momentum averaging mechanism on a per-weight basis.
