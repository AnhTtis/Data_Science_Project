\section{Background}\label{sec:background}
This section contextualizes our proposed framework. Section~\ref{ssec:firstordermethods} illustrates how conventional first-order update rules can be typically expressed as the solution to a convex optimization problem. Section~\ref{ssec:optlayers} then elaborates on the differentiable convex optimization methods that can be used to differentiate through the aforementioned inner loop gradient steps to update meta-parameters.

\subsection{First-order methods}\label{ssec:firstordermethods}
We consider a generic unconstrained optimization problem
\begin{align}\label{eq:optbasic_convex}
    \min_{x} f(x)
\end{align}
with differentiable objective $f$. First-order methods are a popular means to solve optimization problems of the form \eqref{eq:optbasic_convex}. The first-order property refers to the underlying methods' use of gradient information to generate a sequence of parameter iterates. Next we briefly survey a subset of important first-order methods that solve optimization problems of the form \eqref{eq:optbasic_convex}. We highlight how the update rules of these algorithms can be formulated as convex optimization problems themselves. This motivates the formulation of a generic parameterized convex optimizer to yield optimal parameter updates. 

%\textcolor{red}{Change proximal gradient and mirror to one-line sentence, and elaborate more on gradient descent. Write regular gradient descent update rule then compare to (2).}
\subsubsection{Gradient descent}\label{sssec:gd}
Gradient descent (GD) is a standard first-order method used to solve a variety of unconstrained optimization problems. For an unconstrained optimization problem, GD updates aim to reconcile the notion of minimizing a linear approximation of the objective while simultaneously maintaining proximity to the current parameter iterate. This can be cast as the convex optimization
\begin{align}\label{eq:gd_update}
    x^{(t+1)} = \argmin_x \{\nabla f(x^{(t)})^\top (x-x^{(t)})+\frac{\lambda}{2}||x-x^{(t)}||_2^2\}
\end{align}
where $\lambda>0$ is the step size. Solving \eqref{eq:gd_update} in closed-form yields the well-known GD update.

\subsubsection{Gradient descent with Momentum}\label{sssec:gdmomentum}
A popular practical variation of GD is to utilize the history first-order information within the parameter update rule. This is referred to as GD with momentum. The contribution of historic first-order information is captured by the notion of a state. More concretely, we define state update for $t>1$ as
\begin{align}
    \hist{}{t+1} = \beta \hist{}{t} + (1-\beta) \nabla f(x^{(t)}),    
\end{align}
where $\beta\in [0, 1]$ is an averaging parameter and we initialize $\hist{}{1}:=\nabla f(x^{(1)})$. The convex update rule in this method substitutes $\nabla f(x^{(t)})$ with $\hist{}{t+1}$:
\begin{align}\label{eq:gdmomentum_update}
    x^{(t+1)} = \argmin_x \{(\hist{}{t+1})^\top (x-x^{(t)})+\frac{\lambda}{2}||x-x^{(t)}||_2^2\}
\end{align}
 

Other notable first-order methods whose updates are defined via convex optimization problems are the proximal gradient (PG) \cite{boyd2004convex, Wright-Ma-2021} and mirror descent (MD) \cite{BECK2003167, doi:10.1137/1027074} methods. The former addresses unconstrained nondifferentiable problems whose objective is a composite function that can be decomposed into the sum of a differentiable and nondifferentiable part. The latter targets potentially constrained problems with updates that simultaneously minimize a linear approximation of the objective and a proximity term between parameter updates.

    
\subsection{Differentiable Optimization Layers}\label{ssec:optlayers}
We now present the formulation for a general DCO \cite{cvxpylayers2019}:
\begin{align}\label{eq:dco}
    D(x ; \phi):=\argmin_{y\in\R^n}\quad &f_0(x, y;\phi)\nonumber\\
    \textrm{s.t.}\quad &f_i(x,y;\phi)\leq 0 \quad \textrm{for } i\in[q],\nonumber\\
    &g_j(x,y;\phi) = 0 \quad \textrm{for } j\in[r],
\end{align}
where $x\in\R^d$ is the optimization input and $y\in\R^n$ is the solution. Here optimization parameters are defined by a vector $\phi$. The functions $f_i$ parameterize inequality constraint functions which are convex in $y$ and $g_j$ parameterize affine equality constraints. As with the constraint functions, the objective $f_0$ is convex in the optimization variable $y$.

Note that this formulation defines a general parameterized convex optimization problem in the output $y$. The solution to the optimization is a function of the input $x$.

When embedding DCO as a layer within the deep learning context, we require the ability to differentiate through $D$ with respect to $\phi$ when performing backpropagation. This is achieved via implicit differentiation through the Karush-Kuhn-Tucker (KKT) optimality conditions as proposed in \cite{amos2017optnet, amos2017icnn}. Particular instantiations of DCO, such as parameterized QPs, can enable more efficient backpropagation of gradients \cite{amos2017optnet}. 