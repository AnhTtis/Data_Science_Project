@article{garrigos2023handbook,
  title={Handbook of Convergence Theorems for (Stochastic) Gradient Methods},
  author={Garrigos, Guillaume and Gower, Robert M},
  journal={arXiv preprint arXiv:2301.11235},
  year={2023}
}
@misc{Petersen2008,
  abstract = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.},
  added-at = {2011-01-17T12:52:58.000+0100},
  author = {Petersen, K. B. and Pedersen, M. S.},
  biburl = {https://www.bibsonomy.org/bibtex/263c840382cc4b1efb8cefe447465b7ac/hkayabilisim},
  file = {:home/hkaya/Projeler/diagnus/Screener/doc/literature/Petersen2008.pdf:PDF},
  interhash = {6368b9b490c0225e22334ea0a0841a33},
  intrahash = {63c840382cc4b1efb8cefe447465b7ac},
  keywords = {matrixderivative inverse Matrixidentity matrixrelations},
  month = oct,
  note = {Version 20081110},
  publisher = {Technical University of Denmark},
  review = {Matrix Cookbook},
  timestamp = {2011-01-17T12:52:58.000+0100},
  title = {The Matrix Cookbook},
  url = {http://www2.imm.dtu.dk/pubdb/p.php?3274},
  year = 2008
}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    year={2016}
},
@misc{simonyan2015deep,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
},
@inproceedings{alexnet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {}}
@inproceedings{
    madry2018towards,
    title={Towards Deep Learning Models Resistant to Adversarial Attacks},
    author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
    booktitle={International Conference on Learning Representations},
    year={2018}
}

@Book{Sutton+Barto:1998,
  author =       "Sutton, Richard S. and Barto, Andrew G.",
  title =        "Reinforcement Learning: An Introduction",
  publisher =    "MIT Press",
  year =         "1998",
  ISBN =         "0-262-19398-1",
  address =   "Cambridge, MA, USA",
  bib2html_rescat = "Function Approximation, Partial Observability, Learning Methods, General RL, Applications",
}
@article{Mnih2015HumanlevelCT,
  title={Human-level control through deep reinforcement learning},
  author={V. Mnih and K. Kavukcuoglu and D. Silver and Andrei A. Rusu and J. Veness and Marc G. Bellemare and A. Graves and Martin A. Riedmiller and A. Fidjeland and Georg Ostrovski and Stig Petersen and Charlie Beattie and A. Sadik and Ioannis Antonoglou and Helen King and D. Kumaran and Daan Wierstra and S. Legg and D. Hassabis},
  journal={Nature},
  year={2015},
  volume={518},
  pages={529-533}
}
@misc{silver2017mastering,
      title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}, 
      author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
      year={2017},
      eprint={1712.01815},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}
@article{sgd,
	author = {Herbert Robbins and Sutton Monro},
	doi = {10.1214/aoms/1177729586},
	journal = {The Annals of Mathematical Statistics},
	number = {3},
	pages = {400 -- 407},
	publisher = {Institute of Mathematical Statistics},
	title = {{A Stochastic Approximation Method}},
	url = {https://doi.org/10.1214/aoms/1177729586},
	volume = {22},
	year = {1951},
	Bdsk-Url-1 = {https://doi.org/10.1214/aoms/1177729586}}

 @article{fista,
author = {Beck, Amir and Teboulle, Marc},
year = {2009},
month = {01},
pages = {183-202},
title = {A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems},
volume = {2},
journal = {SIAM J. Imaging Sciences},
doi = {10.1137/080716542}
}

@inproceedings{nlp_attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{gautam2022sequential-long,
	title={A Sequential Greedy Approach for Training Implicit Deep Models},
	author={Gautam, Tanmay and Anderson, Brendon G.\ and  Sojoudi, Somayeh and El Ghaoui, Laurent},
	year={2022},
	journal={Technical report},
	url={https://people.eecs.berkeley.edu/~tgautam23/publications/ImplicitSequential_Preprint.pdf}
}








@article{ghaoui2020implicit,
author = {El Ghaoui, Laurent and Gu, Fangda and Travacca, Bertrand and Askari, Armin and Tsai, Alicia},
title = {Implicit Deep Learning},
journal = {SIAM Journal on Mathematics of Data Science},
volume = {3},
number = {3},
pages = {930-958},
year = {2021},
doi = {10.1137/20M1358517},
eprint = { 
        https://doi.org/10.1137/20M1358517
    
}
,
    abstract = { Implicit deep learning prediction rules generalize the recursive rules of feedforward neural networks. Such rules are based on the solution of a fixed-point equation involving a single vector of hidden features, which is thus only implicitly defined. The implicit framework greatly simplifies the notation of deep learning, and opens up many new possibilities in terms of novel architectures and algorithms, robustness analysis and design, interpretability, sparsity, and network architecture optimization. }
}

@article{bai2019deep,
  title={Deep equilibrium models},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{gu2020implicit,
  title={Implicit graph neural networks},
  author={Gu, Fangda and Chang, Heng and Zhu, Wenwu and Sojoudi, Somayeh and El Ghaoui, Laurent},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11984--11995},
  year={2020}
}

@article{bai2020multiscale,
  title={Multiscale deep equilibrium models},
  author={Bai, Shaojie and Koltun, Vladlen and Kolter, J Zico},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5238--5250},
  year={2020}
}

@inproceedings{pabbaraju2020estimating,
  title={Estimating {L}ipschitz constants of monotone deep equilibrium models},
  author={Pabbaraju, Chirag and Winston, Ezra and Kolter, J Zico},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@InProceedings{amos2017optnet,
  title = {{O}pt{N}et: Differentiable Optimization as a Layer in Neural Networks},
  author = {Brandon Amos and J. Zico Kolter},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages = {136--145},
  year = {2017},
  volume = {70},
  series = {Proceedings of Machine Learning Research},
  publisher ={PMLR},
}

@inproceedings{NEURIPS2018_ba6d843e,
 author = {Amos, Brandon and Jimenez, Ivan and Sacks, Jacob and Boots, Byron and Kolter, J. Zico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Differentiable MPC for End-to-end Planning and Control},
 url = {https://proceedings.neurips.cc/paper/2018/file/ba6d843eb4251a4526ce65d1807a9309-Paper.pdf},
 volume = {31},
 year = {2018}
}
@inproceedings{Wang2019SATNetBD,
  title={SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver},
  author={Po-Wei Wang and Priya L. Donti and Bryan Wilder and J. Zico Kolter},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@inproceedings{cvxpylayers2019,
  author={Agrawal, A. and Amos, B. and Barratt, S. and Boyd, S. and Diamond, S. and Kolter, Z.},
  title={Differentiable Convex Optimization Layers},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019},
}
@InProceedings{amos2017icnn,
  title = {Input Convex Neural Networks},
  author = {Brandon Amos and Lei Xu and J. Zico Kolter},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages = {146--155},
  year = {2017},
  volume = {70},
  series = {Proceedings of Machine Learning Research},
  publisher = {PMLR},
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{Rumelhart1986LearningRB,
  title={Learning representations by back-propagating errors},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986}
}

@ARTICLE {mtlsurvey,
author = {T. Hospedales and A. Antoniou and P. Micaelli and A. Storkey},
journal = {IEEE Transactions on Pattern Analysis \& Machine Intelligence},
title = {Meta-Learning in Neural Networks: A Survey},
year = {2022},
volume = {44},
number = {09},
issn = {1939-3539},
pages = {5149-5169},
abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
keywords = {task analysis;optimization;training;machine learning algorithms;predictive models;neural networks;deep learning},
doi = {10.1109/TPAMI.2021.3079209},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {sep}
}

@article{lrmtl,
  author    = {Zhenguo Li and
               Fengwei Zhou and
               Fei Chen and
               Hang Li},
  title     = {Meta-SGD: Learning to Learn Quickly for Few Shot Learning},
  journal   = {CoRR},
  volume    = {abs/1707.09835},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.09835},
  eprinttype = {arXiv},
  eprint    = {1707.09835},
  timestamp = {Tue, 05 Jul 2022 11:22:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LiZCL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{improvemaml,
  doi = {10.48550/ARXIV.1810.09502},
  
  url = {https://arxiv.org/abs/1810.09502},
  
  author = {Antoniou, Antreas and Edwards, Harrison and Storkey, Amos},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {How to train your MAML},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{learningtooptimize,
  doi = {10.48550/ARXIV.1606.01885},
  
  url = {https://arxiv.org/abs/1606.01885},
  
  author = {Li, Ke and Malik, Jitendra},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Learning to Optimize},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{learningtolearnbygd,
author = {Andrychowicz, Marcin and Denil, Misha and Colmenarejo, Sergio G\'{o}mez and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
title = {Learning to Learn by Gradient Descent by Gradient Descent},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3988–3996},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}


@inproceedings{maml,
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1126–1135},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{Ravi2016OptimizationAA,
  title={Optimization as a Model for Few-Shot Learning},
  author={Sachin Ravi and H. Larochelle},
  booktitle={International Conference on Learning Representations},
  year={2016}
}
@InProceedings{l2lbygdoriginal,
author="Hochreiter, Sepp
and Younger, A. Steven
and Conwell, Peter R.",
editor="Dorffner, Georg
and Bischof, Horst
and Hornik, Kurt",
title="Learning to Learn Using Gradient Descent",
booktitle="Artificial Neural Networks --- ICANN 2001",
year="2001",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="87--94",
abstract="This paper introduces the application of gradient descent methods to meta-learning. The concept of ``meta-learning'', i.e. of a system that improves or discovers a learning algorithm, has been of interest in machine learning for decades because of its appealing applications. Previous meta-learning approaches have been based on evolutionary methods and, therefore, have been restricted to small models with few free parameters. We make meta-learning in large systems feasible by using recurrent neural networks with their attendant learning routines as meta-learning systems. Our system derived complex well performing learning algorithms from scratch. In this paper we also show that our approach performs non-stationary time series prediction.",
isbn="978-3-540-44668-2"
}

@inproceedings{Mishra2017ASN,
  title={A Simple Neural Attentive Meta-Learner},
  author={Nikhil Mishra and Mostafa Rohaninejad and Xi Chen and P. Abbeel},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{hypernetwork,
author = {Qiao, Siyuan and Liu, Chenxi and Shen, Wei and Yuille, Alan},
year = {2018},
month = {06},
pages = {7229-7238},
title = {Few-Shot Image Recognition by Predicting Parameters from Activations},
doi = {10.1109/CVPR.2018.00755}
}

@book{Wright-Ma-2021,
author = {John Wright and Yi Ma},
title = {High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications},
publisher = {Cambridge University Press},
year = {2021}
}

@article{BECK2003167,
title = {Mirror descent and nonlinear projected subgradient methods for convex optimization},
journal = {Operations Research Letters},
volume = {31},
number = {3},
pages = {167-175},
year = {2003},
issn = {0167-6377},
doi = {https://doi.org/10.1016/S0167-6377(02)00231-6},
url = {https://www.sciencedirect.com/science/article/pii/S0167637702002316},
author = {Amir Beck and Marc Teboulle},
keywords = {Nonsmooth convex minimization, Projected subgradient methods, Nonlinear projections, Mirror descent algorithms, Relative entropy, Complexity analysis, Global rate of convergence},
abstract = {The mirror descent algorithm (MDA) was introduced by Nemirovsky and Yudin for solving convex optimization problems. This method exhibits an efficiency estimate that is mildly dependent in the decision variables dimension, and thus suitable for solving very large scale optimization problems. We present a new derivation and analysis of this algorithm. We show that the MDA can be viewed as a nonlinear projected-subgradient type method, derived from using a general distance-like function instead of the usual Euclidean squared distance. Within this interpretation, we derive in a simple way convergence and efficiency estimates. We then propose an Entropic mirror descent algorithm for convex minimization over the unit simplex, with a global efficiency estimate proven to be mildly dependent in the dimension of the problem.}
}

@article{doi:10.1137/1027074,
author = {Blair, Charles},
title = {Problem Complexity and Method Efficiency in Optimization (A. S. Nemirovsky and D. B. Yudin)},
journal = {SIAM Review},
volume = {27},
number = {2},
pages = {264-265},
year = {1985},
doi = {10.1137/1027074},

URL = { 
        https://doi.org/10.1137/1027074
    
},
eprint = { 
        https://doi.org/10.1137/1027074
    
}

}


@article{Snderhauf2018TheLA,
  title={The limits and potentials of deep learning for robotics},
  author={Niko S{\"u}nderhauf and Oliver Brock and Walter J. Scheirer and Raia Hadsell and Dieter Fox and J. Leitner and Ben Upcroft and P. Abbeel and Wolfram Burgard and Michael Milford and Peter Corke},
  journal={The International Journal of Robotics Research},
  year={2018},
  volume={37},
  pages={405 - 420}
}


@inproceedings{NIPS2016_90e13578,
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and kavukcuoglu, koray and Wierstra, Daan},
	booktitle = {Advances in Neural Information Processing Systems},
	date-modified = {2023-03-15 17:33:59 -0700},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	read = {0},
	title = {Matching Networks for One Shot Learning},
	url = {https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf},
	volume = {29},
	year = {2016},
	Bdsk-Url-1 = {https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf}}

 @inproceedings{snell,
author = {Snell, Jake and Swersky, Kevin and Zemel, Richard},
title = {Prototypical Networks for Few-Shot Learning},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4080–4090},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{Hochreiter2001LearningTL,
  title={Learning to Learn Using Gradient Descent},
  author={Sepp Hochreiter and Arthur Steven Younger and Peter R. Conwell},
  booktitle={International Conference on Artificial Neural Networks},
  year={2001}
}

@misc{howtotrainmaml,
  doi = {10.48550/ARXIV.1810.09502},
  
  url = {https://arxiv.org/abs/1810.09502},
  
  author = {Antoniou, Antreas and Edwards, Harrison and Storkey, Amos},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {How to train your MAML},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{grefenstette2019generalized,
      title={Generalized Inner Loop Meta-Learning}, 
      author={Edward Grefenstette and Brandon Amos and Denis Yarats and Phu Mon Htut and Artem Molchanov and Franziska Meier and Douwe Kiela and Kyunghyun Cho and Soumith Chintala},
      year={2019},
      eprint={1910.01727},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}