\section{Introduction}\label{sec:introduction}
First-order optimization methods underpin a wide range of modern control and machine learning (ML) techniques. The field of deep learning, including domains such as computer vision \cite{alexnet,simonyan2015deep}, natural language processing \cite{nlp_attention}, deep reinforcement learning \cite{Sutton+Barto:1998}, and robotics \cite{Snderhauf2018TheLA}, has yielded revolutionary results when trained with variants of gradient descent such as stochastic gradient descent (SGD) \cite{sgd} and Adam \cite{Kingma2015AdamAM}. Algorithms like projected and conditional gradient descent extend the class of first-order methods to accommodate problems with constraints such as matrix completion, or training well-posed implicit deep models \cite{ghaoui2020implicit,gautam2022sequential-long}.
%The class of proximal gradient descent algorithms has found application in the context of regularized least squares problems like LASSO \cite{fista}.

While this proliferation of methods has facilitated rapid advances across the control and ML communities, designing update rules tailored to specific problems still remains a challenge. This challenge is exacerbated by the fact that different domains are tasked with solving distinct problem types. The deep learning community is, for instance, tasked with solving high-dimensional non-convex problems whereas the optimal control community often deals with constrained convex problems where the constraints encode restrictions on the state space and system dynamics. Moreover, even different problem instances within a particular problem class may require significantly varying update rules. As an example, within deep learning, effective hyperparameter (e.g. learning rate) selection for algorithms such as Adam and SGD is highly dependent on the underlying model that is to be trained. 
%In such cases, practitioners are restricted to resorting to rules of thumb or hyperparameter sweeps.

\subsection{Contributions}\label{ssec:contributions}
This work proposes a new data-driven approach for optimization algorithm design based on differentiable convex optimization (DCO). This approach enables the use of previous optimization experience to propose update rules that efficiently solve new optimization tasks sampled from the same underlying problem class. We start by introducing the notion of DCO as a means to parameterize optimizers within the meta-learning framework. We then propose an efficient instantiation of meta-training that can be leveraged by the DCO optimizer to learn appropriate meta-parameters. To illustrate the generality of the DCO meta-learning framework, we then formulate concrete differentiable quadratic optimizations to solve unconstrained optimization problems, namely, DCO Gradient (DCOG), DCO Momentum (DCOM) and DCO General Descent (DCOGD). These DCO instantiations are generalizations of existing first-order update rules, which in turn demonstrates that existing methods can be thought of special cases of the DCO meta-learning framework.

DCO also provides sufficient structure conducive to rigorous theoretical analysis for the meta-learning problem. We establish convergence guarantees for the DCOGD optimizer to the optimal first-order update rule that leads to one step convergence when considering a family of linear least squares (LS) problems. Finally, we illustrate the potential of our proposed DCO optimizer instantiations by comparing convergence speed with popular existing first-order methods on illustrative regression and system identification tasks. 


\subsection{Related Works}\label{ssec:relatedworks}
\subsubsection{Meta-Learning}\label{sssec:meta}
Deep learning has been shown to be particularly performant in scenarios where there is an abundance of training data and computational resources \cite{alexnet, Goodfellow-et-al-2016, nlp_attention, silver2017mastering}. This, however, excludes many important applications where there is an inherent lack of data or where computation is very expensive. Meta-learning attempts to address this issue by gaining learning process experience on similarly structured tasks \cite{mtlsurvey}. This learning-to-learn paradigm is aligned with the human and animal learning process which tends to improve with greater experience. Moreover, by making the learning process more efficient meta-learning targets the aforementioned issues of data and compute scarcity.

Meta-learning methods can be categorized into three broad classes. In \cite{grefenstette2019generalized}, authors introduce a unifying framework that encapsulates a wide class of existing approaches. 

Optimizer-focused methods aim to improve the underlying optimizer in the inner loop used to solve the tasks at hand by meta-learning optimizer initialization or hyperparameters. Within few-shot learning, Model Agnostic Meta Learning (MAML) and its variants use prior learning experience to meta-learn a model/policy initialization that requires just a few inner gradient steps to adapt to a new task \cite{maml, howtotrainmaml}. Other works aim to meta-learn optimizer hyperparameters. In \cite{lrmtl, improvemaml}, authors attempt to identify optimal learning rate scheduling strategies. Another strategy within this category is to directly learn a parameterization of the optimizer. Due to the sequential structure of inner loop parameter updates, recurrent architectures have been considered in this space \cite{Hochreiter2001LearningTL, learningtolearnbygd}. The inner loop optimization has also been viewed as a sequential decision-making problem and consequently optimizers have also been characterized as policies within an RL setting \cite{learningtooptimize}. 

Black-box methods represent the inner loop via a forward-pass of a single model. The learning process of the inner loop is captured by the activation layers of the underlying model. The inner loop learning can be instantiated as RNNs \cite{Ravi2016OptimizationAA, l2lbygdoriginal}, convolutional neural networks (CNN) \cite{Mishra2017ASN} or hyper-networks \cite{hypernetwork}. The meta-learning loop finds the hyperparameters of the inner loop network yielding good performance. 

In non-parametric methods, the inner loop aims to identify feature extractors that enable the matching of validation and training samples to yield an accurate prediction using the matched training label. The meta-loop aims to identify the class of feature extractors that transform the data samples into an appropriate space where matching is viable \cite{NIPS2016_90e13578, snell}. 

\subsubsection{Implicit Layers}\label{sssec:implicit}
Recent work has proposed a novel viewpoint wherein deep learning can be instantiated using implicit prediction rules rather than as conventional explicit feedforward architectures \cite{ghaoui2020implicit, chen2018neural, bai2019deep}. In \cite{ghaoui2020implicit} and \cite{bai2019deep} authors formalize how deep equilibrium models, characterized by nonlinear fixed point equations, represent weight-tied, infinite-depth networks. In this framework, \cite{ghaoui2020implicit} demonstrates how the aforementioned  models are able to generalize most of the popular deep learning architectures. In \cite{chen2018neural}, authors propose neural ordinary differential equations (ODE): an alternative instantiation of an implicit layer where the layer output is the solution to an ODE. This is shown to be an expressive model class yielding particularly impressive results when processing sequential data. Implicit layers have also been characterized as differentiable optimization layers. The work \cite{amos2017optnet} introduces differentiable quadratic optimization (QP) layers that can be incorporated within deep learning architectures. In \cite{cvxpylayers2019} authors develop software to differentiate through defined convex optimization problems. Some notable applications of differentiable optimization layers include parameterizing model predictive control policies \cite{NEURIPS2018_ba6d843e} and representing a maximum satisfiability (MAXSAT) solver \cite{Wang2019SATNetBD}. 
\subsection{Notation}\label{ssec:notation}
Throughout this work, we consider the problem of solving a task $\task[]$ which consists of an optimization problem and an evaluation step. The optimization problems are characterized with a loss function $\losst[](\theta)$ over decision variables $\theta$ belonging to some parameter space $\Theta\subseteq \R^p$. For evaluation, we denote a validation criterion $\lossv[]$ that assesses the optimizer $\theta^\star$ found in the associated problem. We refer to solving the optimization over $\theta$ as the \textit{inner loop} problem. At the meta-level we consider an algorithm $\Alg(\ \cdot\ ; \phi):\Theta\rightarrow \Theta$ with meta-parameters $\phi$ which generates a sequence of parameter updates using first-order information to solve the inner loop problem. We denote the horizon of the parameter update sequence by $T$. By meta-training, we refer to the optimization over the meta-parameters over a training set of $N$ tasks $\{\task\}_{i=1}^N$. For a vector-valued function $f(x): \R^d\rightarrow \R^p$ we let the operator $\nabla_x f(\cdot): \R^d\rightarrow \R^{d\times p}$ denote the gradient. If $f: \R^d \rightarrow \R$ is a scalar function, the Hessian of $f$ is denoted by $\nabla^2_x f(\cdot): \R^d \rightarrow \R^{d \times d}$. We denote that a square symmetric matrix $A$ is positive definite (all eigenvalues strictly positive) by $A \succ 0$. The vectorization of a matrix $A \in \R^{m \times n}$ is denoted by $\matvec(A) \in \R^{mn}$ and is constructed by stacking the columns of $A$. The Kronecker product of two matrices $A$ and $B$ is denoted $A \otimes B$. For a vector $x\in\R^n$ and $p\geq 1$, $\|x\|_p$ denotes the $\ell_p$-norm of $x$. For $m \in \mathbb{N}_+$, we define $[m]$ to be the set $\{a \in \mathbb{N}_+ \mid a \leq m\}$, where $\mathbb{N}_+$ is the set of positive integer numbers. We define the operator $\odot$ as an elementwise multiplication. $\mathcal{U}(a, b)$ denotes the uniform probability distribution with support $[a, b]$ and $\mathcal{N}(\mu, \sigma^2)$ represents a univariate normal distribution centered at $\mu$ with standard deviation $\sigma$. Finally, we define $E_{\mathcal{D}}[\cdot]$ as the expectation operator  over distribution $\mathcal{D}$.

