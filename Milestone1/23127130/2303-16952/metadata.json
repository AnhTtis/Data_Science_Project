{
    "arxiv_id": "2303.16952",
    "paper_title": "Meta-Learning Parameterized First-Order Optimizers using Differentiable Convex Optimization",
    "authors": [
        "Tanmay Gautam",
        "Samuel Pfrommer",
        "Somayeh Sojoudi"
    ],
    "submission_date": "2023-03-29",
    "revised_dates": [
        "2023-03-31"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
    ],
    "abstract": "Conventional optimization methods in machine learning and controls rely heavily on first-order update rules. Selecting the right method and hyperparameters for a particular task often involves trial-and-error or practitioner intuition, motivating the field of meta-learning. We generalize a broad family of preexisting update rules by proposing a meta-learning framework in which the inner loop optimization step involves solving a differentiable convex optimization (DCO). We illustrate the theoretical appeal of this approach by showing that it enables one-step optimization of a family of linear least squares problems, given that the meta-learner has sufficient exposure to similar tasks. Various instantiations of the DCO update rule are compared to conventional optimizers on a range of illustrative experimental settings.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16952v1"
    ],
    "publication_venue": "9 pages, 3 figures"
}