\documentclass[preprint,authoryear,10pt,3p]{elsarticleFTP}
\renewcommand{\baselinestretch}{1.25}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{color}
\usepackage{tabulary}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{eqparbox}
\usepackage{tabularx}
\usepackage{tabulary}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{kpfonts}
\usepackage{rotating}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{breakurl}
\usepackage{setspace}
\usepackage{epigraph}
\usepackage{longtable}
\usepackage{pdflscape}
%\usepackage{paralist}
%\usepackage{adjustbox}
%\usepackage{blindtext}

\hyphenation{net-works a-na-ly-sis con-si-de-ring hi-sto-ry appro-xi-ma-tors e-vi-de-nce a-ve-ra-ge le-vels re-gu-la-ri-zed ba-la-n-ce do-mi-na-te re-gu-la-ri-za-tion ba-la-n-ce ta-king pos-si-ble e-xa-mi-ne com-bi-ning met-hods or-ga-ni-sa-tions e-xa-mi-ned ma-xi-mum time-tabling time-slot opti-mi-sa-tion}

%\usepackage{draftwatermark}
%\SetWatermarkText{DRAFT}
%\SetWatermarkScale{1}

%\biboptions{sort&compress,square,comma}
 
\journal{Journal of the Operational Research Society}

\begin{document}

\begin{frontmatter}
\title{Operational Research: Methods and Applications\footnote{This is an original manuscript of an article published by Taylor \& Francis in the \textit{Journal of the Operational Research Society} on 27th December 2023, available at: \url{https://doi.org/10.1080/01605682.2023.2253852}}}

\author[label1,label1b]{Fotios~Petropoulos\corref{cor1}}
\cortext[cor1]{Corresponding author: f.petropoulos@bath.ac.uk; fotios@bath.edu}
\author[label7a,label1,label401]{Gilbert~Laporte}
\author[label100]{Emel~Aktas}
\author[label2]{Sibel~A.~Alumur}
\author[label3]{Claudia~Archetti}
\author[label4]{Hayriye~Ayhan}
\author[label1]{Maria~Battarra}
\author[label5]{Julia~A.~Bennell}
\author[label1001]{Jean-Marie~Bourjolly}
\author[label6]{John~E.~Boylan}
\author[label7a]{Michèle~Breton}
\author[label9]{David~Canca}
\author[label7a,label7b]{Laurent~Charlin}
\author[label10]{Bo~Chen}
\author[label301]{Cihan~Tugrul~Cicek}
\author[label11a,label11b]{Louis~Anthony~Cox,~Jr}
\author[label12]{Christine~S.M.~Currie}
\author[label13]{Erik~Demeulemeester}
\author[label14]{Li~Ding}
\author[label15]{Stephen~M.~Disney}
\author[label16]{Matthias~Ehrgott}
\author[label17]{Martin~J.~Eppler}
\author[label1]{Güneş~Erdoğan}
\author[label18c,label18a,label18b]{Bernard~Fortz}
\author[label19a,label19b]{L.~Alberto~Franco}
\author[label20]{Jens~Frische}
\author[label21a,label21b]{Salvatore~Greco}
\author[label1002]{Amanda~J.~Gregory}
\author[label22]{Raimo~P.~Hämäläinen}
\author[label13]{Willy~Herroelen}
\author[label23]{Mike~Hewitt}
\author[label24]{Jan~Holmström}
\author[label25]{John~N.~Hooker}
\author[label26]{Tuğçe~Işık}
\author[label27]{Jill~Johnes}
\author[label28]{Bahar~Y.~Kara}
\author[label28]{Özlem~Karsu}
\author[label29]{Katherine~Kent}
\author[label31]{Charlotte~Köhler}
\author[label32]{Martin~Kunc}
\author[label33a,label33b]{Yong-Hong~Kuo}
\author[label16]{Adam~N.~Letchford}
\author[label34]{Janny~Leung}
\author[label19a]{Dong~Li}
\author[label35]{Haitao~Li}
\author[label201]{Judit~Lienert}
\author[label36]{Ivana~Ljubić}
\author[label37]{Andrea~Lodi}
\author[label38]{Sebastián~Lozano}
\author[label40]{Virginie~Lurkin}
\author[label41]{Silvano~Martello}
\author[label42]{Ian~G.~McHale}
\author[label43a,label43ab,label43b,label43c,label43f]{Gerald~Midgley}
\author[label44]{John~D.W.~Morecroft}
\author[label45]{Akshay~Mutha}
\author[label46]{Ceyda~Oğuz}
\author[label47]{Sanja~Petrovic}
\author[label61]{Ulrich~Pferschy}
\author[label48]{Harilaos~N.~Psaraftis}
\author[label49]{Sam~Rose}
\author[label50]{Lauri~Saarinen}
\author[label102]{Said~Salhi}
\author[label51]{Jing-Sheng~Song}
\author[label56]{Dimitrios~Sotiros}
\author[label52]{Kathryn~E.~Stecke}
\author[label20]{Arne~K.~Strauss}
\author[label46]{İstenç~Tarhan}
\author[label62]{Clemens~Thielen}
\author[label41]{Paolo~Toth}
\author[label58]{Tom~Van~Woensel}
\author[label8]{Greet~Vanden~Berghe}
\author[label1]{Christos~Vasilakis}
\author[label53]{Vikrant~Vaze}
\author[label101]{Daniele~Vigo}
\author[label54a,label54b]{Kai~Virtanen}
\author[label55]{Xun~Wang}
\author[label56]{Rafał~Weron}
\author[label57]{Leroy~White}
\author[label15]{Mike~Yearworth}
\author[label59]{E.~Alper~Yıldırım}
\author[label7a]{Georges~Zaccour}
\author[label60]{Xuying~Zhao}

% Countries (XX): UK, 

\address[label1]{School of Management, University of Bath, Bath, UK}
\address[label1b]{Makridakis Open Forecasting Center, University of Nicosia, Nicosia, Cyprus}
\address[label7a]{Department of Decision Sciences, HEC Montréal, Montreal, Canada}
\address[label401]{Molde University College, Molde, Norway}
\address[label100]{Cranfield School of Management, Cranfield University, Cranfield, UK}
\address[label2]{Department of Management Sciences, University of Waterloo, Waterloo, Canada}
\address[label3]{ESSEC Business School in Paris, Department of Information Systems, Decision Sciences and Statistics, Cergy, France}
\address[label4]{H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, USA}
\address[label5]{Centre for Decision Research, Leeds University Business School, University of Leeds, Leeds, United Kingdom}
\address[label1001]{Université du Québec à Montréal, Montreal, Canada}
\address[label6]{Centre for Marketing Analytics and Forecasting, Lancaster University Management School, Lancaster University, Lancaster, UK}
\address[label9]{School of Engineering, Department of Industrial Engineering and Management Science I, University of Seville, Seville, Spain}
\address[label7b]{Mila-Quebec AI Institute, Montreal, Canada}
\address[label10]{Warwick Business School, University of Warwick, Coventry, UK}
\address[label301]{Department of Industrial Engineering, Atilim University, Ankara, Turkey}
\address[label11a]{Department of Business Analytics, University of Colorado School of Business, Denver, USA}
\address[label11b]{Cox Associates, Denver, USA}
\address[label12]{Mathematical Sciences, University of Southampton, Southampton, UK}
\address[label13]{Faculty of Economics and Business, Research Center for Operations Management, KU Leuven, Leuven, Belgium}
\address[label14]{Durham University Business School, Durham University, Durham, UK}
\address[label15]{Center for Simulation, Analytics, and Modelling, University of Exeter Business School, Exeter, UK}
\address[label16]{Department of Management Science, Lancaster University Management School, Lancaster University, Lancaster, UK}
\address[label17]{University of St. Gallen, St. Gallen, Switzerland}
\address[label18c]{HEC - Management School of the University of Liège, Liège, Belgium}
\address[label18a]{Département d'Informatique, Université Libre de Bruxelles, Brussels, Belgium}
\address[label18b]{Inria Lille-Nord Europe, Villeneuve d'Ascq, France}
\address[label19a]{School of Business and Economics, Loughborough University, Loughborough, UK}
\address[label19b]{Universidad del Pacífico, Lima, Perú}
\address[label20]{WHU – Otto Beisheim School of Management, Vallendar, Germany}
\address[label21a]{Department of Economics and Business, University of Catania, Catania, Italy}
\address[label21b]{Portsmouth Business School, Centre of Operations Research and Logistics (CORL), University of Portsmouth, Portsmouth, UK}
\address[label1002]{Centre for Systems Studies, Faculty of Business, Law and Politics, University of Hull, Hull, UK}
\address[label22]{Systems Analysis Laboratory, Aalto University, Finland}
\address[label23]{Department of Information Systems and Supply Chain Management, Loyola University, Chicago, USA}
\address[label24]{Department of Industrial Engineering and Management, Aalto University, Aalto, Finland}
\address[label25]{Tepper School of Business, Carnegie Mellon University, Pittsburgh, USA}
\address[label26]{Department of Industrial Engineering, Clemson University, Clemson, USA}
\address[label27]{Department of Accounting, Finance and Economics, University of Huddersfield, Huddersfield, UK}
\address[label28]{Department of Industrial Engineering, Bilkent University, Ankara, Turkey}
\address[label29]{Office for National Statistics, Newport, UK}
\address[label31]{Europa Universität Viadrina, Frankfurt (Oder), Germany}
\address[label32]{Southampton Business School, University of Southampton, UK}
\address[label33a]{Department of Industrial and Manufacturing Systems Engineering, The University of Hong Kong, Pokfulam, Hong Kong, China}
\address[label33b]{HKU Musketeers Foundation Institute of Data Science, The University of Hong Kong, Pokfulam, Hong Kong, China}
\address[label34]{State Key Laboratory of Internet of Things for Smart City, Choi Kai Yau College, University of Macau, Taipa, Macau, China}
\address[label35]{Supply Chain \& Analytics Department, University of Missouri-St. Louis, St. Louis, USA}
\address[label201]{Eawag: Swiss Federal Institute of Aquatic Science and Technology, Switzerland}
\address[label36]{ESSEC Business School, Cergy-Pontoise, France}
\address[label37]{Jacobs Technion-Cornell Institute, Cornell Tech and Technion - IIT, USA}
\address[label38]{Department of Industrial Management, Escuela Superior de Ingenieros, University of Seville, Sevilla, Spain}
\address[label40]{HEC Lausanne Faculty of Business and Economics, University of Lausanne, Lausanne, Switzerland}
\address[label41]{DEI ``Guglielmo Marconi'', Alma Mater Studiorum Università di Bologna, Bologna, Italy}
\address[label42]{Centre for Sports Business, University of Liverpool Management School, UK}
\address[label43a]{Centre for Systems Studies, Faculty of Business, Law and Politics, University of Hull, Hull, UK}
\address[label43ab]{Birmingham Leadership Institute, University of Birmingham, Birmingham, UK}
\address[label43b]{Department of Informatics, Faculty of Technology, Linnaeus University, Växjö, Sweden}
\address[label43c]{School of Innovation, Design and Engineering, Mälardalen University, Eskilstuna, Sweden}
\address[label43f]{School of Agriculture and Food Sciences, University of Queensland, Brisbane, Queensland, Australia}
\address[label44]{London Business School, London, UK}
\address[label45]{Grossman School of Business, University of Vermont, Burlington, Vermont, USA}
\address[label46]{Department of Industrial Engineering, Koç University, İstanbul, Turkey}
\address[label47]{Nottingham University Business School, University of Nottingham, Nottingham, UK}
\address[label61]{Department of Operations and Information Systems, University of Graz, Graz, Austria}
\address[label48]{Department of Technology, Management and Economics, Technical University of Denmark, Lyngby, Denmark}
\address[label49]{Department for Transport, London, UK}
\address[label50]{Department of Industrial Engineering and Management, Aalto University, Aalto, Finland}
\address[label102]{Centre for Logistics and Heuristic optimisation, Kent Business School, University of Kent, Kent, UK}
\address[label51]{The Fuqua School of Business, Duke University, Durham, North Carolina, USA}
\address[label56]{Department of Operations Research and Business Intelligence, Wrocław University of Science and Technology, Wrocław, Poland}
\address[label52]{Naveen Jindal School of Management, The University of Texas at Dallas, Richardson, Texas, USA}
\address[label62]{TUM Campus Straubing for Biotechnology and Sustainability, Technical University of Munich, Straubing, Germany}
\address[label58]{School of Industrial Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands}
\address[label8]{Department of Computer Science, KU Leuven, Gent, Belgium}
\address[label53]{Thayer School of Engineering, Dartmouth College, Hanover, USA}
\address[label101]{Department of Electrical, Electronic, and Information Engineering ``G. Marconi'' and CIRI-ICT, University of Bologna, Bologna, Italy}
\address[label54a]{Department of Mathematics and Systems Analysis, Aalto University, Helsinki, Finland}
\address[label54b]{Department of Military Technology, National Defence University, Helsinki, Finland}
\address[label55]{Logistics Systems Dynamics Group, Cardiff Business School, Cardiff University, UK}
\address[label57]{Management Department, University of Exeter Business School, Exeter, UK}

\address[label59]{School of Mathematics, The University of Edinburgh, Edinburgh, UK}
\address[label60]{Mendoza College of Business, University of Notre Dame, Notre Dame, Indiana, USA}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{frontmatter}

\epigraph{Operations research is neither a method nor a technique; it is or is becoming a science and as such is defined by a combination of the phenomena it studies.}{\cite{Ackoff1956-qh}}

\section*{Abstract}
\label{sec:Abstract}

\noindent Throughout its history, Operational Research has evolved to include a variety of methods, models and algorithms that have been applied to a diverse and wide range of contexts. This encyclopedic  article consists of two main sections: methods and applications. The first aims to summarise the up-to-date knowledge and provide an overview of the state-of-the-art methods and key developments in the various subdomains of the field. The second offers a wide-ranging list of areas where Operational Research has been applied. The article is meant to be read in a nonlinear fashion. It should be used as a point of reference or first-port-of-call for a diverse pool of readers: academics, researchers, students, and practitioners. The entries within the methods and applications sections are presented in alphabetical order.

\noindent The authors dedicate this paper to the 2023 Turkey/Syria earthquake victims. We sincerely hope that advances in OR will play a role towards minimising the pain and suffering caused by this and future catastrophes.

\noindent \textbf{Keywords:} review; encyclopedia; theory; practice; principles; optimisation; programming; systems; simulation; decision making; models.

\clearpage
\setcounter{tocdepth}{3}
\small
\tableofcontents
\normalsize

\clearpage

\section[Introduction (Gilbert Laporte)]{Introduction\protect\footnote{This subsection was written by Gilbert Laporte.}}
\label{sec:introduction}

The year 2024 marks the 75\textsuperscript{th} anniversary of the \textit{Journal of the Operational Research Society}, formerly known as \textit{Operational Research Quarterly}. It is the oldest Operational Research (OR) journal worldwide. On this occasion, my colleague Fotios Petropoulos from University of Bath proposed to the editors of the journal to edit an encyclopedic article on the state of the art in OR. Together, we identified the main methodological and application areas to be covered, based on topics included in the major OR journals and conferences. We also identified potential authors who responded enthusiastically and whom we thank wholeheartedly for their contributions.

Modern OR originated in the United Kingdom during World War II as a need to support the operations of early radar-detecting systems and was later applied to other operations \citep{McCloskey1987-gw_GL}. However, one could argue that it precedes this period in history since it is partly rooted in several mathematical fields such as probability theory and statistics, calculus, and linear algebra, developed much earlier. For example, the Fourier-Motzkin elimination method \citep{Fourier1826b-hx_GL,Fourier1826a-hx_GL} constitutes the main basis of linear programming.  Queueing theory, which plays a central role in telecommunications and computing, already existed as a distinct field of study since the early 20\textsuperscript{th} century \citep{Erlang1090-er_GL}, and other concepts, such as the economic order quantity \citep{harris1913many_JSS} were developed more than one century ago. Interestingly, while many recent advances in OR are rooted in theoretical or algorithmic concepts, we are now witnessing a return to the practical roots of OR through the development of new disciplines such as business analytics.

After the war ended, several industrial applications of OR arose, particularly in the manufacturing and mining sectors which were then going through a renaissance. The transportation sector is without doubt the field that has most benefited from OR, mostly since the 1960s. The aviation, rail, and e-commerce industries could simply not operate at their current scale without the support of massive data analysis and sophisticated optimisation techniques. The application of OR to maritime transportation is more recent, but it is fast gaining in importance. Other areas that are less visible, such as telecommunications, also deeply depend on OR. The success of OR in these fields is partly explained by their network structures which make them amenable to systematic analysis and treatment through mathematical optimisation techniques. In the same vein, OR also plays a major role in various branches of logistics and project management, such as facility location, forecasting, inventory planning, scheduling, and supply chain management. 

The public sector and service industries also benefit greatly from OR. Healthcare is the first area that comes to mind because of its very large scale and complexity. Decision making in healthcare is more decentralised than in transportation and manufacturing, for example, and the human issues involved in this sector add a layer of complexity. OR methodologies have also been applied to diverse areas such as education, sports management, natural resources, environment and sustainability, political districting, safety and security, energy, finance and insurance, revenue management, auctions and bidding, and disaster relief, most of which are covered in this article.

Among OR methodologies, mathematical programming occupies a central place. The simplex method for linear programming, conceived by Dantzig in 1947 but apparently first published later \citep{Dantzig1951-qp_GL}, is arguably the single most significant development in this area. Over time, linear programming has branched out into several fields such as nonlinear programming, mixed integer programming, network optimisation, combinatorial optimisation, and stochastic programming. The techniques most frequently employed for the exact solution of mathematical programs are based on branch-and-bound, branch-and-cut, branch-and-price (column generation), and dynamic programming. Game theory and data envelopment analysis are firmly rooted in mathematical programming. Control theory is also part of continuous mathematical optimisation and relies heavily on differential equations.

Complexity theory is fundamental in optimisation. Most problems arising in combinatorial optimisation are $\mathcal{NP}$-hard and typically require the application of heuristics for their solution. Much progress has been made in the past 40 years or so in the development of metaheuristics based on local search, genetic search, and various hybridisation schemes. Many problems in fields such as vehicle routing, location analysis, cutting and packing, set covering, and set partitioning can now be solved to near optimality for realistic sizes by means of modern heuristics. A recent trend is the use of open-source software which not only helps disseminate research results, but also contributes to ensuring their accuracy, reproducibility and adoption.

Several modelling paradigms such as systems thinking and systems dynamics approach problems from a high-level perspective, examining the inter-relationships between multiple elements. Complex systems can often be analysed through simulation, which is also commonly used to assess the performance of heuristics. Decision analysis provides a useful framework for structuring and solving complex problems involving soft and hard criteria, behavioural OR, stochasticity, and dynamism. Recently, issues related to ethics and fairness have come to play an increasing role in decision making.

Because the various topics of this review paper are listed in alphabetical order, the subsection on “Artificial intelligence, machine learning and data science” comes first, but this topic constitutes one of the latest developments in the field. It holds great potential for the future and is likely to reshape parts of the OR discipline. Already, machine learning-based heuristics are competitive for the solution of some hard problems.

This paper begins with a quote from Russell L. Ackoff who has been a pioneer of OR. In 1979, he published in this journal two articles \citep{Ackoff1979-zx_GL,Ackoff1979-vt_GL} that presented a rather pessimistic view of our discipline. The author complained about the lack of communications between academics and practitioners, and about the fact that some OR curricula in universities did not sufficiently prepare students for practice, which is still true to some extent. One of his two articles is entitled “The Future of Operational Research is Past”, which may be perceived as an overreaction to this diagnosis. In my view, the present article provides clear evidence to the contrary. Soon after the publication of the two Ackoff papers, we witnessed the development of micro-computing, the Internet and the World Wide Web. It has become much easier for researchers in our community to access information, software and computing facilities, and for practitioners to access and use our research results. We are now fortunate to have access to sophisticated open-source software, data bases, bibliographic sources, editing and visualisation tools, and communication facilities. Our field is richer than it has ever been, both in terms of theory and applications. It is constantly evolving in interaction with other disciplines, and it is clearly alive and well and has a promising future. 

\clearpage

\section{Methods}
\label{sec:methods}

\subsection[Artificial intelligence, machine learning, and data science (Laurent~Charlin \& Andrea~Lodi)]{Artificial intelligence, machine learning, and data science\protect\footnote{This subsection was written by Laurent~Charlin and Andrea~Lodi.}}
\label{sec:Artificial_intelligence_machine_learning_data_science}
Machine learning (ML) comprises  techniques for modelling predictive tasks, i.e.\ tasks that involve the prediction of an unknown quantity from other observed quantities. Ideas of learning in an artificial system and the term machine learning were first discussed in the 1950s \citep{Samuel_IBM_5392560_LCAL} and their development and popularity have seen enormous growth over the last two decades in part due to the availability of large-scale datasets and increased computational resources to model them.

\cite{mitchell1997machine_LCAL} provides this concrete definition of machine learning ``A computer program is said to learn from experience E with respect to some class of tasks T, and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E''. The program is a model or a function and its experience E is the type of data it has access to. There are three types of experiences supervised, unsupervised, and reinforcement learning. The performance measure (P) allows for model evaluation and comparison including model selection.

Supervised learning is an experience where a model aims at predicting one or more unobserved target (dependent) variables given observed ibackprout (independent) variables. In other words, a supervised model is a function that map inputs to outputs. The process of solving a supervised problem involves first learning a model, that is adjusting its parameters using a training dataset with both input and target variables. The training set is drawn IID (independently and identically distributed) from an underlying distribution over inputs and targets. Once trained, the model can provide target predictions for new unseen samples from the same distribution. The most common tasks in supervised learning are regression (real dependent variable) and classification (categorical dependent variable). Evaluating a supervised system is usually performed using held-out data referred to as the test data while held-out validation data is used for model development and selection using procedures such as $k$-fold cross-validation.

Supervised models can be dichotomised into linear and nonlinear models. Linear models perform a linear mapping from inputs to outputs (e.g., linear regression). Machine learning mostly investigates nonlinear supervised models including deep neural network (DNN) models \citep{Goodfellow-et-al-2016_LCAL}. DNNs are composed of a succession of parametrised nonlinear transformations called layers and each layer contains a set of transformations called neurons. Layers successively transform an input datum into a target. The parameters of the layers are adjusted to iteratively obtain better predictions using a procedure called backpropagation, a form of gradient descent \cite[\S 6.5]{Goodfellow-et-al-2016_LCAL}. DNNs are state-of-the-art methods for many large-scale non-structured datasets across domains (see also \S\ref{sec:Power_markets_and_systems}). DNNs can be adapted to different sizes of inputs and targets as well as variable types. They can also be specialised for specific types of data. Recurrent neural networks (RNNs) are auto-regressive models for sequential data \citep{Rumelhart86_LCAL}. The sequential data are tokenised and an RNN transforms each token sequentially along with a transformation of the previous tokens. Convolutional neural networks (CNNs) are specialised networks for modelling data that is arranged on a grid \citep[e.g., an image][]{LeCun89_LCAL}. Their layers contain a convolution operation between an input and a parameterised filter followed by a nonlinear transformation, and a pooling operation. Each layer processes data locally and so requires fewer parameters compared to vanilla DNNs. As a result, CNNs can model higher-dimensional data. Graphical neural networks (GNNs) are specialised architectures for modelling graph data \citep[e.g., a social network;][]{Scarselli09_LCAL}. In GNNs, the data are transformed by following the topology of the graph. 
Last, attention layers dynamically combine their inputs (tokens) based on their values. Transformer models use successions of attention and feed-forward layers to model sequential input and output data \citep{NIPS2017_3f5ee243_LCAL}. Transformers are more efficient to train than RNNs and can be trained on internet-scale data given enormous computational power. The availability of such broad datasets especially in the text and image domains has given rise to a class of very-large-scale models (also referred to as foundation models) that display an ability to adapt to and obtain high performance across a diversity of downstream supervised tasks \citep{Bommasani2021FoundationModels}

Last, attention is a mechanism that considers data to be unordered and uses transformations dynamically. Transformers are models based on attention. They provide more efficient training than RNNs for very large-scale datasets \citep{NIPS2017_3f5ee243_LCAL}.  

Neural networks currently outperform other methods when learning from unstructured data (e.g., images and text). For tabular data, data that is naturally encoded in a table and that has heterogeneous features \citep{NEURIPS2022_Varoquaux_LCAL}, best-performing methods use ideas first proposed in tree-based classifiers, bagging, and boosting. They include random forests \citep{Breiman01_LCAL}, XGBoost \citep{Chen16_LCAL} which both scale to large-scale datasets as well as kernel methods including support vector machines \citep[SVMs see, e.g.,][]{Schlkopf18_LCAL} and probabilistic Gaussian Processes  \citep[GPs see, e.g.,][]{Rasmussen05_LCAL}. These methods are used across regression and classification tasks.

In unsupervised learning, the second type of experience, the data consist of independent variables (features or covariates) alone. The aim of unsupervised learning is to model the structure of the data to better understand their properties. As a result, evaluating an unsupervised model is often task and application-dependant \cite[\S 1.3.4]{pml1Book_LCAL}. The prototypical unsupervised-learning task is clustering. It involves learning a function that groups similar data together according to a similarity measure and desiderata often expressed as an objective function. Several standard algorithms divided into hierarchical and non-hierarchical methods exist. The former uses the similarity between all pairs of data and finds a hierarchy of clustering solutions with a different number of clusters using either a bottom-up or top-down approach. Agglomerative clustering is a standard hierarchical approach. Non-hierarchical methods tend to be more computationally efficient in terms of dataset size. 
For example, $K$-means clustering is a well-known non-hierarchical method that finds a single solution using $K$ clusters \citep{MacQueen1967_LCAL}. Other unsupervised learning tasks include dimensionality reduction for example for visualisation or to prepare data for further analysis. Density modelling is another unsupervised task where a probabilistic model learns to assign a probability to each datum \cite[\S 1.3]{pml1Book_LCAL}. Probabilistic models can be used to learn the hidden structure in large quantities of data \citep[e.g.,][]{hoffman13_LCAL}. Further, probabilistic models are also used to generate high-dimensional data (e.g., images of human faces or English text) with high fidelity \citep{Karas19_LCAL} and often referred in this context as generative models. Large Language Models are examples of such generative models \citep{Bommasani2021FoundationModels}.

Reinforcement learning (RL) is the third type of experience. RL models collect their own data by executing actions in their environment to maximise their reward. RL is a sequential decision-making task and is formalised using Markov decision processes (MDPs) \cite[\S 3.8]{sutton2018reinforcement_LCAL}. An MDP encodes a set of states, available actions, distribution over next states given current states and action, a reward function, and a discount factor. Partially observable MDPs (or POMDPs) extend the formalism to environments where the exact current state is unknown \citep{KAELBLING199899_LCAL}. In RL, an agent's objective is to learn a policy, a distribution over actions for each state in an environment. Tasks are defined by rewards attached to different states. Exact and approximate methods exist for solving RL problems. Whereas exact solutions are appropriate for smaller tabular problems only, deep neural networks are widely used for solving larger-scale problems that require approximate solutions yielding a set of techniques known as deep reinforcement learning \citep{Mnih_DRL_2015_LCAL}. An RL agent can also learn to imitate an expert either by learning a mapping from states/observations to actions as in supervised learning \citep[a technique known as imitation learning; for a survey, see][]{Hussein17_LCAL} or by trying to learn the expert's reward function \citep[inverse reinforcement learning][]{Russell98_LCAL}.  

In addition to learning models for solving prediction tasks using one of the three experiences above, machine learning also studies methods for enabling the reuse of information learned from one or multiple datasets and environments to other similar ones. Representation learning studies how to learn such reusable information and it can use both supervised and unsupervised experiences \cite[\S32]{pml2Book_LCAL}. When using a deep learning model, a representation is obtained after one or more layer transformations of the data. Representation learning is used in a variety of situations including for transfer learning tasks, where a trained model is reused to solve a different supervised task \cite[for a survey, see][]{Zhuang_TransferLearning21_LCAL}. 

In the last decade, machine learning models have achieved high performance on a variety of tasks including perceptual ones (e.g.\ recognising objects in images and words from speech) as well as natural language processing ones thereby becoming a core component of artificial intelligence (AI) methods. The goal of AI methods is to develop intelligent systems. Some of these advances shine a bright light on the ethical aspects of machine learning techniques and are active areas of study \citep[see, e.g.,][]{Dignum19_LCAL,barocas19_LCAL}. Another area of active study is explainability \citep{XAI_NIST_21}. Some of the most effective ML tools make predictions and recommendations that are hard to explain to users (for example when neural networks are employed). Clearly, lack of explainability slows down ML use in those contexts where decisions made due to those predictions and recommendations are life changing and involve a human in the loop, healthcare (applying a treatment), finance (refusing a mortgage), or justice (granting parole) to mention a few. So, explainability is currently one of the most crucial challenges for ML and AI and, at the same time, a tremendous opportunity for their wider applicability. 

Further, advances in machine learning alongside statistics, data management, and data processing, as well as the wider availability of datasets from a variety of domains have led to the popularisation and development of data science (DS), a  discipline whose goal is to extract insights and knowledge from these data. DS uses statistics and machine-learning techniques for inference and prediction, but it also aims at enabling and systematising the analysis of large quantities of data. As such, it includes components of data management, visualisation, as well as the design of (efficient) data processing algorithms \citep{grus2019data_LCAL}.
 
\subsubsection*{Resources} 
\citet{pml1Book_LCAL} provides a thorough introduction to the field following a probabilistic approach and its sequel \citep{pml2Book_LCAL} introduces advanced topics. \cite{Goodfellow-et-al-2016_LCAL} provide a self-contained introduction to the field of deep learning \citep[the field evolves rapidly and more advanced topics are covered through recent papers and in][]{pml2Book_LCAL}. 
Open-source software packages in Python and other languages are essential. They include data-wrangling libraries such as pandas \citep{mckinney-proc-scipy-2010_LCAL} and plotting ones such as matplotlib \citep{Hunter:2007_LCAL}. The library scikit-learn \citep{scikit-learn_LCAL} in Python offers an extensive API that includes data processing, a toolbox of standard supervised and unsupervised models, and evaluation routines. For deep learning, PyToch \citep{PyToch_NEURIPS2019_9015_LCAL} and TensorFlow \citep{tensorflow2015-whitepaper_LCAL} are the standard. 

\subsubsection*{Learning for combinatorial optimisation} 
The impressive success of machine learning in the last decade made it natural to explore its use in many scientific disciplines, such as drug discovery and material sciences. Combinatorial optimisation (CO; \S\ref{sec:Combinatorial_optimisation}) is no exception to this trend and we have witnessed an intense exploration (or, better, revival) of the use of machine learning for CO. Two lines of work have strongly emerged. On the one side, ML has been used to learn crucial decisions within CO algorithms and solvers. This includes imitating an algorithmic expert that is computationally expensive like in the case of strong branching for branch and bound, the single application that has attracted the largest amount of interest \citep{lodi2017learning_LCAL,gasse2019exact_LCAL}. The interested reader is referred to two recent surveys \citep{bengio2021machine_JLYHK,cappart2021combinatorial_LCAL}, the latter highlighting the relevance of GNNs for effective CO representation. On the other side, ML has been used end to end, i.e., for solving CO problems directly or leveraging ML to devise hybrid methods for CO. The area is surveyed in \cite{fioretto2021_LCAL}.

\subsection[Behavioural OR (L.~Alberto~Franco \& Raimo~P.~Hämäläinen)]{Behavioural OR\protect\footnote{This subsection was written by L.~Alberto~Franco and Raimo~P.~Hämäläinen.}}
\label{sec:Behavioural_OR}

Behavioural OR (BOR) is concerned with the study of human behaviour in OR-supported settings. Specifically, BOR examines how the behaviour of individuals affects, or is affected by, an OR-supported intervention\footnote{We use the term ‘intervention’ to describe a structured process comprised of designed OR-related activities such as, for example, modelling, model use, data collection, interviews, meetings, workshops, and presentations.}. The individuals of interest are those who, acting in isolation or as part of a team, design, implement and engage with OR in practice. These individuals include OR practitioners playing specific intervention roles (e.g., modellers, facilitators, consultants), and other individuals with varying interests and stakes in the intervention (e.g., users, clients, domain experts, sponsors). 

A concern with the behavioural aspects of the OR profession can be traced back to past debates in the 1960s, 1970s and 1980s \citep{Churchman1970-hv_GM,Dutton1964-wf_AFRH,Jackson1989-fk_AFRH}. Although these debates dwindled down in subsequent years, the emergence of BOR as a field of study represents a return to these earlier concerns \citep{Franco2016-qk_MYLW,Hamalainen2013-zr_AFRH}. What motivates this resurgence is the recognition that the successful deployment of OR in practice relies heavily on our understanding of human behaviour. For example, overconfidence, competing interests, and the willingness to expend effort in searching, sharing, and processing information are three behavioural issues that can negatively affect the success of OR activities. Attention to behavioural issues has been central in disciplines such as economics, psychology and sociology for decades, and BOR studies draw heavily from these reference disciplines \citep{Franco2021-sm_JL}.

It is important to distinguish between the specific focus of BOR and the broader focus of behavioural modelling. The creation of models that capture human behaviour has a long tradition within OR, but it is not necessarily concerned with the study of human behaviour in OR-supported settings. For example, in the last 20 years operational researchers have produced an increasing number of robust analytical models that describe behaviour in, and predict its impact on, operations management settings \citep{Cui2018-xl_AFRH,Donohue2020-mw_AFRH,Loch2007-ij_AFRH}. Operational researchers also have produced simulation models that capture human behaviour within a system with different levels of complexity. For example, systems dynamics models incorporate high-level variables representing average behaviour \citep[][\S\ref{sec:Systems_dynamics}]{Morecroft2015-vr_MCJM,Sterman2000_SMD}, and discrete event simulation models capture human processes controlled by simple behavioural rules \citep[][\S\ref{sec:Simulation}]{Brailsford2003-rs_AFRH,Robinson2014-bd_AFRH}. More complex agent-based simulation models represent behaviour as emergent from the interactions of agents with particular behavioural attributes \citep[][\S\ref{sec:Simulation}]{Sonnessa2017-la_AFRH,Utomo2018-uy_AFRH}. Overall, behavioural modelling within the OR field is concerned with examining human behaviour in a system of interest in order to improve that system\footnote{To date, the practice of studying OR-supported intervention as a system of interest has been somewhat overlooked by behavioural modellers, with the notable exception of behavioural forecasting researchers; see \S\ref{sec:forecasting}, and also \cite{Arvan2019-xy_AFRH} and \cite{Petropoulos2016-uk_FP}.}. In contrast, BOR takes an OR-supported intervention as the core system of interest where human behaviour is examined. The ultimate goal of BOR is to generate an improved understanding of the behavioural dimension of OR practice, and use this understanding to design and implement better OR-supported interventions. 

Another important distinction worth stating is that between BOR and Soft OR. At first glance, this distinction may seem unnecessary as BOR is a field of study within OR, while Soft OR refers to a specific family of problem structuring approaches (\S\ref{sec:Soft_OR_and_problem_structuring_methods}). Soft OR approaches have been developed to help groups reach agreements on problem structure and, often, appropriate responses to a problem of concern \citep{Franco2022-mn_AFRH,Rosenhead2001-ai_JL}. However, while Soft OR intervention design and implementation typically require the consideration of behavioural issues, this is not the same as choosing human behaviour in a Soft OR intervention context as the unit of analysis. Of course, a study with such a focus would certainly fall within the BOR remit \citep[e.g.,][]{Tavella2021-mj_AFRH}. But note that BOR is also concerned with the study of human behaviour in other OR-supported settings, such as those involving the use of ‘hard’ and ‘mixed-method’ OR approaches.

Studies of behaviour in OR-supported settings assume implicitly or explicitly that human behaviour is either influenced by cognitive and external factors, or is in itself an influencing factor \citep{Franco2021-sm_JL}.  In the first case, observed individual and collective action is taken to be guided by cognitive structures (e.g., personality traits, cognitive styles) manifested during OR-related activity -- behaviour is \textit{influenced}. In contrast, the second case assumes that individuals and collectives are responsible for determining how OR-related activity will unfold -- behaviour is \textit{influencing}. This raises the practical possibility that the same OR methodology, technique, or model could be used in distinctive ways by various individuals or groups according to their cognitive orientations, goals and interests \citep{Alberto_Franco2013-yw_MYLW}. Whilst behaviour in practice is likely to lie somewhere between the influenced and influencing assumptions, BOR studies tend to foreground one of the extremes as the focus, while backgrounding the other.

BOR studies can adopt three different research methodologies to examine behaviour: variance, process, and modelling.  A \textit{variance methodology} uses variables that represent the important aspects or attributes of the OR-supported activity being examined. Variance explanations of behavioural-related phenomena take the form of causal statements captured in a theoretically-informed research model that incorporates these variables (e.g., A causes B, which causes C). The research model is then tested with data generated by the activity, and the research findings are assessed in terms of their generality \citep{Poole2004-cq_AFRH}. Adopting a variance research methodology typically requires the implementation of experimental, quasi-experimental, or survey research designs\footnote{It should be noted that a variance approach could also be implemented through field research designs where pre and post intervention measures of key variables are used to assess changes in behaviour or surrogates of behaviour. Studies adopting this approach are common in the System Dynamics field \citep[see, for example,][]{Scott2013-fg_AFRH}.}. This involves careful selection of independent variables, which might be either manipulated or left untreated, and of dependent variables that act as surrogates for specific behaviours. Once information about all variables is collected, data is quantitatively analysed using a wide range of variance-based methods (e.g., analysis of variance, regression, structural equation modelling).

Behavioural studies that use a variance research methodology can produce a good picture of the generative mechanisms underpinning behavioural processes if they test hypotheses about those mechanisms. For example, variance studies in BOR have examined the impact of individual differences in cognitive motivation and cognitive style on the conduct of OR-supported activity \citep{Fasolo2014-mp_AFRH,Franco2016-yn_AFRH,Lu2001-ow_AFRH}. There is also a long tradition of testing the behavioural effects of reconfiguring different aspects of OR-supported activity such as varying model or information displays \citep{Bell1995-lw_AFRH,Gettinger2013-mi_AFRH}, and preference elicitation procedures \citep{Cavallo2019-sj_AFRH,Hamalainen2016-gr_AFRH,Poyhonen2001-dw_AFRH,Von_Nitzsch1993-hh_AFRH}.

A \textit{process methodology} is used to examine OR-supported activity as a series of events that bring about or lead to some behaviour-related outcome. Specifically, it considers as the unit of analysis an evolving individual or group whose behaviour is led by, or leading, the occurrence of events \citep{Poole2004-cq_AFRH}. Process explanations take the form of theoretical narratives that account for how event dynamics lead to a final outcome \citep{Poole2007-cg_AFRH}. These narratives are often derived from observation, but it is also possible to use an established narrative (e.g., a theory) to guide observation that further specifies the narrative. 

Diverse and eclectic research designs are used to implement a process research methodology. Central to these designs is the task of identifying or reconstructing the process through the analysis of events taking place over time. For example, there is an important stream of BOR studies that examines the process of building models by experts and novices \citep{Tako2015-dk_AFRH,Tako2010-ua_AFRH,Waisel2008-uw_AFRH,Willemain1995-qz_AFRH,Willemain2007-hj_AFRH}. There is also an increasing interest to use process methodologies to take a closer look at actual behaviour in OR-supported settings both, before, during and after OR-related activity is undertaken \citep{Franco2018-nk_AFRH,Kaki2019-bo_AFRH,Velez-Castiblanco2016-kh_AFRH,White2016-sv_MYLW}.

The variance and process approaches may seem opposite to each other, but instead they should be seen as complementary \citep{Franco2021-sm_JL,Van_de_Ven2005-ug_AFRH}. BOR studies using a variance research methodology can explore and test the mechanisms that drive process explanations of behaviour, while BOR studies adopting a process research methodology can explore and test the narratives that ground variance explanations of behaviour. One way of combining a variance and process approach within a single BOR study is by adopting \textit{modelling} as a research methodology. A modelling approach would create models that capture the mechanisms that generate a process of interest such as, for example, trust on an OR-derived solution, and the model can be run to generate the characteristics of that process. Model parameters and structure can then be varied systematically to enable variance-based comparisons of trust levels. Furthermore, the trajectory of trust levels over time can be used to gain insights into the nature of the trust development process.  As already mentioned, there is a long behavioural modelling tradition within OR but, as far as we know, its potential as a research methodology tool to specifically examine behaviour in OR-supported settings is yet to be realised. 

In sum, the variance, process and modelling methodologies offer rich possibilities for the study of human behaviour in OR-supported settings. Which is best for a particular study will depend on the types of question being addressed by BOR researchers, their assumptions about human behaviour, and the data they have access to. Ultimately, a thorough understanding of behaviour in OR-supported settings is likely to require all three research methodologies. 

For  a detailed review of BOR studies the reader is referred to \cite{Franco2021-sm_JL}. A review of behavioural studies in the context of OR in health has been written by \cite{Kunc2020-fn_MCJM}. There are also two collections edited by \cite{Kunc2016-gy_AFRH} and \cite{White2020-fw_AFRH}. The \textit{European Journal of Operational Research} published a feature cluster on BOR edited by \cite{Franco2016-ns_AFRH}.  Finally, BOR-related news and events can be found on the sites of the European Working Group on Behavioural OR\footnote{\url{https://www.euro-online.org/websites/bor/}}, and the UK BOR Special Interest Group\footnote{\url{https://www.theorsociety.com/get-involved/society-groups/special-interest-groups-and-networks/behavioural-or/}}. 

\subsection[Business analytics (John~E.~Boylan)]{Business analytics\protect\footnote{This subsection was written by John~E.~Boylan.}}
\label{sec:Business_analytics}

Business Analytics has its origins in practice, rather than theory, as illustrated by some of the earliest publications on the subject \citep[e.g.,][]{Kohavi2002-rl_JEB}. Senior executives began to realise the importance of analytics in the first decade of the new millennium because of the ready availability of large amounts of data, the maturity of business performance management, the emergence of self-service analytics and business intelligence, and the declining cost of computing power, data storage and bandwidth \citep{Acito2014-my_JEB}.

\cite{Davenport2007-jn_JEB} gave examples of companies becoming ‘analytical competitors’ by using analytics to support distinctive organisational capabilities. To achieve this level of maturity, it was argued that analytics needs to become a strategic competency. In the 1990s, \cite{Fildes1997-bo_JEB} reported on the closure or dispersal of Operational Research groups. \cite{Davenport2010-vu_JEB} reflected a reversal of that trend, by focusing on how analytical talent can be organised as an internal resource. They suggested that there are four categories of people to be considered when finding, developing and managing analysts: champions, professionals, semi-professionals and amateurs. In 2012/13, the Institute for Management Science and Operations Research (INFORMS) introduced the Certified Analytics Professional program and examination. This covers the broad spectrum of skills required of analytics professionals, including business problem framing, analytics problem framing, data (handling), methodology selection, model building, deployment and lifecycle management \citep{INFORMS2022}.

The development of talent is just one of the prerequisites for Business Analytics to create value. \cite{Vidgen2017-vp_JEB} recommended ‘coevolutionary change’, aligning their analytics strategy with their strategies for Information and Communications Technology, human resources and the whole business. This helps to ensure that the necessary data assets are available, the right culture is developed to build data and analytics skills, and that there is alignment with the business strategy for value creation. \cite{Hindle2018-zo_JEB} proposed a Business Analytics Methodology based on four activities, namely problem situation structuring, business model mapping, business analytics leverage and analytics implementation. They advocated a soft OR approach, Soft Systems Methodology \citep{Checkland2006-tv_JEB}, to support structuring and mapping activities.

Many definitions of Business Analytics have been proposed; for a review of early definitions, see \cite{Holsapple2014-gf_JEB}. According to \cite{Davenport2013-xh_JEB}, “\textit{By analytics we mean the extensive use of data, statistical and quantitative analysis, explanatory and predictive models, and fact-based management to drive decisions and actions}” (p. 7). \cite{Mortenson2015-ak_JEB} suggested that analytics is at the intersection of quantitative methods, technologies and decision making. \cite{Rose2016-lq_JEB} considered analytics as the union of Data Science (which is data centric) and Operational Research (which is problem centric). \cite{Power2018-kx_JEB} proposed the following definition: “\textit{Business Analytics is a systematic thinking process that applies qualitative, quantitative and statistical computational tools and methods to analyse data, gain insights, inform and support decision-making}". \cite{Delen2018-hf_JEB} pointed out that, although analytics includes analysis, it also involves synthesis and subsequent implementation. These broad perspectives, emphasising synthesis as well as analysis, and qualitative as well as quantitative approaches, are consistent with earlier writings on the use of a broad range of methods in Management Science \citep[e.g.,][]{Mingers1997-nd_MYLW,Pidd2009-ny_JEB}.

Business Analytics can be viewed from different orientations. From a methodological viewpoint, the subject covers descriptive, predictive and prescriptive methods \citep{Lustig2010-ty_JEB}. These three  categories are sometimes extended to four, with a distinction being drawn between ‘descriptive’ and ’diagnostic’ analytics, following the Gartner analytics ascendancy model \citep{Maoz2013-zz_JEB}. \cite{Lepenioti2020-mq_JEB} argue that it is preferable to maintain the threefold categorisation to ensure consistency, with each category addressing both  ‘What?’ and ‘Why’ questions. (Descriptive: ‘What happened?’, ‘Why did it happen?’; Predictive: ‘What will happen?’, ‘Why will it happen?’; Prescriptive: ‘What should I do to make it happen?’, ‘Why should I make it happen?’).  For detailed literature reviews on descriptive, predictive and prescriptive analytics, the reader is directed to \cite{Duan2015-bh_JEB}, \cite{Lu2017-oe_JEB}, and \cite{Lepenioti2020-mq_JEB}, respectively.

From a technological viewpoint, Business Analytics is facilitated by the integration of transactional data with big data streaming from social media platforms and the Internet of Things into a unified analytics system \citep{Shi2018-eb_JEB}. These authors suggest that this integration can be achieved in two stages, starting with integration of traditional Enterprise Resource Planning (ERP) and big data, and proceeding to integration of big-data ERP with Business Analytics. \cite{Ruivo2020-kq_JEB} reported that analytics ranked second in extended ERP capabilities (behind collaboration) according to the views of 20 experts engaged in a Delphi study. \cite{Romero2022-nw_JEB} suggested that cloud-based big data analytics software will not provide competitive advantage to firms that have not installed a large ERP system, although it will ensure that they do not lag further behind their sector-leading competitors.

From an ethical viewpoint, Business Analytics faces a number of challenges. \cite{Davenport2010-vu_JEB} recognised that issues of data privacy can be difficult to address, especially if an organisation operates in a wide range of territories or industries. \cite{Ram_Mohan_Rao2018-jb_JEB} summarised major privacy threats in data analytics, namely surveillance, disclosure, discrimination, and personal embarrassment and abuse, and reviewed privacy preservation methods, including randomisation and cryptographic techniques. A further ethical issue is that AI algorithms are likely to replicate and reinforce existing social biases \citep{ONeil2016-rs_JEB}. Such algorithmic bias is said to occur when the outputs of an algorithm benefit or disadvantage certain individuals or groups more than others without a justified reason. \cite{Kordzadeh2022-yp_JEB} reviewed the literature on algorithmic bias and showed that most studies had examined the issue from a conceptual standpoint, with only a limited number of empirical studies. Similarly, \cite{Vidgen2020-vy_JEB} reviewed papers on ethics in Business Analytics and found that most were at the level of guiding principles and frameworks, with little of direct applicability for the practitioner. Their case study demonstrated how ethical principles (utility, rights, justice, common good and virtue) can be embedded in analytics development. For further discussions on ethics and OR, the reader is referred to \cite{Ormerod2013-km_JH}, \cite{Le_Menestrel2004-wy}, and \cite{Mingers2011-id} but also \S\ref{sec:Ethics_and_fairness}.

Analytics maturity models have been developed to describe, explain and evaluate the development of analytics in an organisation. \cite{Krol2020-yp_JEB} reviewed 11 maturity models and assessed them in terms of the number of assessment dimensions, scoring mechanism, number of maturity levels, and the public availability of the methodology. They found that the most common assessment dimensions were technical infrastructure, analytics culture and human resources, including staff’s analytics competencies. \cite{Lismont2017-qz_JEB} undertook a survey of companies, based on the DELTA maturity model \citep{Davenport2010-vu_JEB} of data, enterprise, leadership, targets and analysts. They identified four analytics maturity levels from their survey. The most advanced companies tended to use a wider variety of analytics techniques and applications, to organise analytics more holistically, and to have a more mature data governance policy.

A crucial empirical question is whether Business Analytics adds value to an organisation. An early study on the effect of Business Analytics on supply chain performance was conducted by \cite{Trkman2010-nx_JEB}. They examined over 300 companies, showing a statistically significant relationship between self-assessed analytical capabilities and performance. \cite{Oesterreich2022-ez_JEB} conducted a meta-analysis of 125 firm-level studies, spanning ten years of research in 26 countries.  They found evidence of Business Analytics having a positive impact on operational, financial and market performance. They also found that human resources, management capabilities and organisational culture were major determinants of value creation, whereas technological factors were less important.   

\subsection[Combinatorial optimisation (Silvano~Martello \& Paolo~Toth)]{Combinatorial optimisation\protect\footnote{This subsection was written by Silvano~Martello and Paolo~Toth.}}
\label{sec:Combinatorial_optimisation}

A {\em Combinatorial Optimisation} (CO) problem consists of searching for the optimal element in a finite collection of elements. More formally, given a set of elements and a family of its subsets, each defining a feasible solution and having an associated value, a CO problem is to find  a subset having the minimum (or, alternatively, the maximum) value. The subsets may be proper, like, e.g., in the knapsack problem, or represented by permutations, like, e.g., in the assignment problem (see below). Typically, the  feasible solutions are not explicitly listed, but are described in a concise manner (like a set of equalities and inequalities, or a  graph structure) and their number is huge, so scanning all feasible solutions to select the optimal one is intractable. A CO problem can usually be modelled as an {\em Integer Program} (IP, see also \S\ref{sec:Mixed_integer_programming}) with linear or nonlinear objective function and constraints, in which the variables can take a finite number of integer values.

Consider for example the problem of assigning $n$ tasks to $n$ agents, by knowing the time that each agent needs to complete each task, with the objective of finding  a solution that minimises the overall time needed to complete all tasks ({\em Assignment Problem}, AP). The solution could obviously be found by enumerating all permutations of the integers $1, 2\dots, n$ and selecting the best one. However, this number is so huge that such approach is ruled out even for small-size problem instances: for $n = 30$, we have $n! \cong 2.6\cdot10^{30}$, and the fastest supercomputer on hearth would need millions of years to scan all solutions. The challenge is thus to find more efficient methods. For example, one of the most famous CO algorithms (the {\em Hungarian algorithm}) can solve assignment problem instances with millions of variables in few seconds on a standard PC.

The algorithm mentioned above can be implemented so as to solve any AP instance in a time of order $n^3$, i.e., in a time bounded by a polynomial function of the input size. Unfortunately, only for relatively few CO problems we know algorithms with such property, while for most of them (${\cal NP}$-hard problems) the best known algorithms can take, in the worst case, a time that grows exponentially in the size of the instance. In addition, Complexity theory (see also \S\ref{sec:Computational_complexity}) suggests that the existence of polynomial-time algorithms for such problems is unlikely. On the other hand, CO problems arise in many industrial sectors (manufacturing, crew scheduling, telecommunication, distribution, to mention a few) and hence there is the prominent and practical need to obtain good quality solutions, especially to large-size instances, in reasonable times.

\subsubsection*{Origins}
Many problems arising on graphs and networks (see \S\ref{sec:Graphs_and_networks}) belong to CO (the AP discussed above can be described as that of finding a minimum weight perfect matching in a bipartite graph), and hence the origins of CO date back to the eighteen century. In the following, we narrow our focus to modern CO \citep[see][]{S05_SMPT}. Its roots can be found in the first decades of the past century, when Central European mathematicians developed seminal studies on \textit{matching problems} \citep{K16_SMPT}, \textit{paths} \cite{M27_SMPT}, and {\em Shortest Spanning Trees} (SST) (\citeauthor{J30_SMPT}, \citeyear{J30_SMPT}; \citeauthor{B26_SMPT}, \citeyear{B26_SMPT}, results independently rediscovered by \citeauthor{P57_SMPT}, \citeyear{P57_SMPT} and \citeauthor{K57_SMPT}, \citeyear{K57_SMPT}).
The Fifties produced major results on the AP (\citeauthor{K55_SMPT}, \citeyear{K55_SMPT,K56_SMPT}, on the basis of the results by \citeauthor{K16_SMPT}, \citeyear{K16_SMPT} and \citeauthor{E31_SMPT}, \citeyear{E31_SMPT}, also see \citeauthor{M10_SMPT}, \citeyear{M10_SMPT}), the {\em Travelling Salesman Problem} \citep{DFJ:1954_IL}, and {\em Network Flows} \citep{Ford-Fulkerson:1962_IL},
as well as fundamental studies on basic methodologies: {\em dynamic programming} \citep[DP;][see \S \ref{sec:Dynamic_programming}]{Bellman1957-ue_HL}, {\em cutting planes} \citep[][see \S \ref{sec:Mixed_integer_programming}]{Go58_ALAL}, and {\em branch-and-bound} \citep{LD60_ALAL}.

\subsubsection*{Problems and complexity}
The most important CO problems, for which we know there are polynomial algorithms, are the basic graph-theory problems mentioned in the previous section. Other important problems, which are relevant both from the theoretical point of view and from that of real-world applications, are instead ${\cal NP}$-hard. The main ${\cal NP}$-hard CO problems arise in the following areas.

{\em Scheduling}. Given a set of tasks which must be processed on a set of processors, a scheduling problem asks to find a processing schedule that satisfies prescribed conditions and minimises (or maximises) an objective function, frequently related to the time needed to complete all tasks. This huge area, that includes literally hundreds of problems and variants (mostly ${\cal NP}$-hard), is also discussed in \S \ref{sec:Timetabling}.

{\em Travelling Salesman Problem} (TSP). Given a weighted (directed or undirected) graph, the problem is to find a circuit that visits each vertex exactly once (Hamiltonian tour) and has minimum total weight. This is one of the most intensively studied problems of CO, and is treated in detail in \S \ref{sec:Graphs_and_networks}.

{\em Vehicle Routing Problems} (VRP). A VRP is a generalisation of the TSP which consists of finding a set of routes for a fleet of vehicles, based at one or more depots, to deliver goods to a given set of customers by satisfying a set of conditions and minimising the overall transportation cost.

{\em Facility Location}. These problems require to find the best placement of facilities on the basis  of geographical demands, installation costs, and transportation costs, so as to satisfy a set of conditions and to minimise the total cost (see \S \ref{sec:Location} for a detailed treatment).

{\em Steiner Trees}. Given a weighted graph and a subset $S$ of vertices, it is requested to find an SST connecting all vertices in $S$ (possibly containing additional vertices). These problems, which generalise both the shortest path problem and the SST, are treated in detail in \S \ref{sec:Graphs_and_networks}.

{\em Set Covering}. Given a set of elements and a collection of its subsets, each having a cost, we want to find the least cost sub-collection whose union includes (covers) all the elements.

{\em Maximum Clique} (MC). A clique is a {\em complete} subgraph of a graph (i.e., it is defined by a subset of vertices all adjacent to each other). Given a graph, the MC problem is to find a clique of maximum cardinality (or, if the graph is weighted, a clique of maximum weight). We refer the reader to \S \ref{sec:Graphs_and_networks} for a  detailed analysis.

{\em Cutting and Packing} (C\&P). Given a set of  ``small" items, and a set of ``large" containers, a problem in this area asks for an optimal arrangement of the items into the containers. Items and containers can be in one dimension ({\em Knapsack Problems} (KP), Bin Packing problems) or in more - usually two or three – dimensions (C\&P). See \S \ref{sec:Cutting_and_packing} for more details.

{\em Quadratic Variants of CO problems}. A currently hot research area concerns CO problems whose ``normal" linear objective function is replaced by a quadratic one. This greatly increases difficulty: in most cases problems which, in their linear formulation, can be solved in polynomial time (e.g., the AP) or in pseudo-polynomial time (e.g., the KP) become strongly ${\cal NP}$-hard.

\subsubsection*{Exact methods for ${\cal NP}$-hard problems}
For heuristic and approximation algorithms, we refer the reader to \S \ref{sec:Heuristics} and \S \ref{sec:Computational_complexity}. With the exception of DP methods (\S \ref{sec:Dynamic_programming}), most exact algorithms for $\mathcal{NP}$-hard CO problems, as well as most commonly used ILP solvers, are based on implicit enumeration. In the worst case, they can require the evaluation of all feasible solutions, and hence computing times growing exponentially with the problem size.
The most common methods can be classified as
\begin{itemize}[noitemsep]
\item {\em Branch-and-Bound} (B\&B);
\item {\em Branch-and-Cut} (B\&C);
\item {\em Branch-and-Price} (B\&P).
\end{itemize}
We will describe B\&B, the other methods (and their combinations, as B\&C-and-Price) being its extensions described in \S \ref{sec:Mixed_integer_programming}.

We consider a {\em maximisation} CO problem having an IP model with inequality constraints of `$\leq$' type. For a problem $P$, having feasible solution set $F(P)$, $z(P)$ denotes its optimal solution value, and $ub(P)$ an upper bound on $z(P)$. The main ingredients of B\&B are branching scheme and upper bound computation. \\

\noindent {\em Branching scheme}. The solution is obtained as follows:
\begin{itemize}[noitemsep]
\item[(i)] subdivide $P$ into $m$ subproblems, each having the same objective function as $P$ and a feasible solution set contained in $F(P)$, such that the union of their feasible solution sets is $F(P)$. The optimal solution of $P$ is thus given by the optimal solution of the subproblem having the maximum objective function value;
\item[(ii)] iteratively, if a subproblem cannot be immediately solved, subdivide it into additional subproblems.
\end{itemize}
The resulting method can be represented through a {\em branch-decision tree}, where the root node corresponds to $P$ and each node corresponds to a subproblem.

A node of the tree can be eliminated if the feasible solution set of the corresponding subproblem is empty, or its upper bound is not greater than the value of the best feasible solution to $P$ found so far.\\ 

\noindent {\em Upper bound computation}.
A valid upper bound $ub(P)$ can be computed as the optimal solution value of a {\em Relaxation} of the IP model of $P$, defined by:
\begin{itemize}[noitemsep]
\item[(i)] a feasible solutions set containing $F(P)$;
\item[(ii)] an objective function whose value is not smaller than that of $P$ for any solution in $F(P)$.
\end{itemize}
A relaxation is ``good" if the resulting upper bound $ub(P)$ is ``close" to $z(P)$ (i.e., if the {\em gap} between the two values, $z(P) - ub(P)$, is ``small''), and the relaxed problem is ``easy'' to solve, i.e., its optimal solution can be obtained with a computational effort much smaller than that required to solve $P$.

\subsubsection*{Relaxations}
The most common relaxation methods are:
\begin{itemize}[noitemsep]
\item {\em Constraint elimination}: a subset of constraints is removed from the IP model of $P$, so that the resulting problem is easy to to solve. The most widely used case is the linear relaxation;
\item {\em Linear relaxation}: when the model is an {\em Integer Linear Problem} (ILP), removing the constraints that impose integrality of the variables leads to a {\em Linear Program} (LP), which is polynomially solvable, commonly used in ILP solvers (see \S \ref{sec:Mixed_integer_programming});
\item {\em Surrogate relaxation}: a subset $\Sigma$ of inequality constraints is replaced by a single {\em surrogate} inequality, so that the corresponding relaxed problem is easy to solve. The surrogate inequality is obtained by multiplying both sides of each inequality of $\Sigma$ by a non-negative constant, and summing, respectively, the left-hand and right-hand sides of the resulting inequalities;
\item {\em Lagrangian relaxation}: a subset $\Lambda$ of inequality constraints is removed from the model and ``embedded'', in a Lagrangian fashion, into the objective function. For each inequality of $\Lambda$, the difference between left-hand and right-hand sides ({\em slack}) multiplied by a non-negative constant is added to the objective function.
\end{itemize}

The relaxations can be strengthened by adding one or more valid inequalities ({\em cuts}) to the IP model of $P$, such that they are redundant for the IP model, but can become active when the IP model is relaxed (see \S\ref{sec:Mixed_integer_programming}).

\subsubsection*{Further readings}
We refer the reader to the following selection of references for more details on the topics covered in this section. Well known, pre-1990 books are those by \citeauthor{GN72_SMPT} (\citeyear{GN72_SMPT}, IP), \citeauthor{C75_SMPT} (\citeyear{C75_SMPT}, algorithmic graph theory), \citeauthor{GJ79_SMPT} (\citeyear{GJ79_SMPT}, complexity), 
\citeauthor{BD80_SMPT} (\citeyear{BD80_SMPT}, AP), \citeauthor{LLRS85_SMPT} (\citeyear{LLRS85_SMPT}, TSP), and the CO specific volumes by  \cite{L76_SMPT}, \cite{CMTS79_SMPT}, \cite{PS82_SMPT},
 \cite{MLMR87_SMPT}, and \cite{NW88_SMPT}. We list more recent contributions in the order in which the topics were introduced:
\begin{itemize}[noitemsep]
\item CO: \cite{CCPS98_SMPT}, \cite{Schrijver:2003_IL};
\item AP: \cite{BDM12_SMPT} for linear and quadratic AP, \cite{C13_SMPT} for quadratic AP;
\item Network Flows: \cite{ahuja1993network_IL};
\item Scheduling: \cite{BEPSW01_SMPT,BEPSW07_SMPT}, \cite{P12_SMPT};
\item TSP: \cite{GP06_SMPT}, \cite{ABCC07_SMPT}, \cite{Cook:2011_IL};
\item VRP: \cite{GoldenRW2008_CA_MB}, \cite{TV14_SMPT};
\item Facility Location: \cite{MPL90_SMPT}, \cite{LNS15_SMPT};
\item Steiner trees: \cite{HRW92_SMPT}, \cite{PS12_SMPT}. Also see the recent survey by \cite{Ljubic:2021_IL};
\item Cutting and packing: \cite{MT90_SMPT}, \cite{KPP04_SMPT}. Also see the recent survey by \cite{CILM22A_SMPT,CILM22B_SMPT}.
\end{itemize}

\subsection[Computational complexity (Ulrich~Pferschy \& Clemens~Thielen)]{Computational complexity\protect\footnote{This subsection was written by Ulrich~Pferschy and Clemens~Thielen.}}
\label{sec:Computational_complexity}

Operational Research develops models and solution methods for problems arising from practical decision making scenarios. Often, these solution methods are \emph{algorithms}. The difficulty of a problem can be assessed empirically by evaluating the running times of corresponding algorithms, which requires careful implementations and meaningful test data. Moreover, this can be time-consuming and yields insights that depend on the skills of the programmer and are limited to the available test instances. 
\emph{Computational complexity} represents an alternative approach that allows for a more general assessment of a problem's difficulty that is independent of specific problem instances or solution algorithms. 

\subsubsection*{Problem encoding and running times of algorithms}

In complexity theory, the running time of an algorithm is expressed in terms of the size of the input, i.e., the amount of data necessary to encode an instance of the problem. Since computers store data in the form of binary digits (bits), the standard \emph{binary encoding} represents all data of a problem instance in the form of binary numbers. The number of required bits (the \emph{encoding length}) of an integer is roughly given by the binary logarithm of its absolute value. As an example, consider the binary encoding of instances of the well-known 0-1 knapsack problem (KP). An instance of KP consists of $n$~items -- each with a non-negative, integer weight and profit -- and a positive, integer knapsack capacity~$c$.
We can assume that all $n$~item weights are bounded by the capacity~$c$ and denote the value of the largest item profit by~$p_{\max}$. Then, the encoding length of a KP instance is bounded by $(n+1)\cdot\log_2(c) + n\cdot\log_2(p_{\max})\leq (2n+1)\cdot\log_2(\max\{c,p_{\max}\})$.

Rational numbers can be straightforwardly represented by their (integer) numerator and denominator, but their presence in the input might already influence a problem's computational complexity \citep{Woi18_UPCT}. Irrational numbers cannot be encoded in binary without rounding them appropriately, which means that a different kind of complexity theory is required when general real numbers are part of the input \citep[see][for details]{BCSS98_UPCT}. Hence, the following exposition is restricted to the case of integer inputs, where the encoding length of an instance can be bounded by the number of integers needed to represent it multiplied with the binary logarithm of the largest among their absolute values (see the bound for KP instances provided above as an example).

To allow universal running time analyses of algorithms that are independent of specific computer architectures, asymptotic running time bounds described using the so-called $\mathcal{O}$-notation \citep{Cor09_UPCT} are used. Informally, every polynomial in~$n$ with largest exponent~$k$ is in $\mathcal{O}(n^k)$. All terms with exponents smaller than~$k$ and the constant coefficient of~$n^k$ are ignored. One is then often interested in \emph{po\-ly\-no\-mi\-al-time algorithms} whose running time is in $\mathcal{O}(|I|^k)$ for some constant~$k$, where~$|I|$ denotes the encoding length of instance~$I$. A less preferred outcome would be a \emph{pseudopolynomial-time algorithm}, where the running time is only required to be polynomial in the number of integers in the input and the largest among their absolute values (or, equivalently, in the exponentially larger encoding length of the input when using \emph{unary encoding}, where the encoding length of an integer is roughly its absolute value). 

\subsubsection*{The complexity classes $\mathcal{P}$ and $\mathcal{NP}$}

Most application scenarios encountered in Operational Research finally lead to an optimisation problem (often a combinatorial problem -- see \S\ref{sec:Combinatorial_optimisation}), where a feasible solution is sought that minimises or maximises a given objective function. Every optimisation problem immediately yields an associated decision problem, asking a yes-no question. For example, a minimisation problem consisting of a set~$\mathcal{X}$ of feasible solutions and an objective function~$f$ can be written as $\min \{f(x): x \in \mathcal{X}\}$. For a given target value~$v$, the associated decision problem then asks: Does there exist a feasible solution~$x \in \mathcal{X}$ such that $f(x) \leq v$? Solving an optimisation problem to optimality trivially answers the associated decision problem for any given~$v$. On the other hand, every algorithm for the decision problem can be used to solve the underlying optimisation problem. Given upper and lower bounds, the optimal solution value can be identified in polynomial time by performing binary search between these bounds using the decision problem to answer the query in every iteration of the binary search (assuming that the range of objective function values and the encoding lengths of the bounds are polynomially bounded).

Motivated by the above, the computational complexity of an optimisation problem follows from the complexity of its associated decision problem. Here, the most relevant complexity classes in Operational Research are probably $\mathcal{P}$ and $\mathcal{NP}$, which are often used to draw the line between ``easy'' and ``hard'' problems in this context. Formally, the class $\mathcal{P}$ (``polynomial'') consists of all decision problems for which a polynomial-time solution algorithm exists on a deterministic Turing machine (or, equivalently, in any other ``reasonable'' deterministic model of computation), while the class $\mathcal{NP}$ (``nondeterministic polynomial'') consists of all decision problems for which the same holds on a \emph{nondeterministic} Turing machine. Equivalently, $\mathcal{NP}$ is the class of all decision problems such that, for any yes instance~$I$, there exists a certificate with encoding length polynomial in~$|I|$ and a deterministic algorithm that, given the certificate, can verify in polynomial time that the instance is indeed a yes instance. Since the most natural certificate is often a (sufficiently good) solution of the problem, $\mathcal{NP}$ can informally be defined as the class of decision problems for which solutions can be verified in polynomial time. For example, when considering the travelling salesman problem (TSP) on a given edge-weighted graph, the associated decision problem asks whether or not there exists a tour (Hamiltonian cycle) of at most a given length~$v$. While no polynomial-time algorithm for this decision problem is known to date, the problem can easily be seen to be in $\mathcal{NP}$ since the natural certificate to provide for a yes instance is simply a tour with length at most~$v$, whose feasibility and length can be easily verified in polynomial time.

Observe that these definitions directly imply that $\mathcal{P}\subseteq\mathcal{NP}$. Most researchers believe that $\mathcal{P}\subsetneq\mathcal{NP}$ or, equivalently, that there are problems in $\mathcal{NP}$ that do not admit polynomial-time solution algorithms. However, formally proving that $\mathcal{P}\neq\mathcal{NP}$ (or that $\mathcal{P}=\mathcal{NP}$) is still one of the most famous open problems in theoretical computer science to date.
            
This so-called \emph{$\mathcal{P}$ versus $\mathcal{NP}$ problem} can be equivalently expressed using the well-known notion of $\mathcal{NP}$-completeness \citep[see, e.g.,][]{GJ79_SMPT}. Intuitively, $\mathcal{NP}$-complete problems are the hardest problems in $\mathcal{NP}$ in the sense that, if one of these problems admits a polynomial-time solution algorithm, then so does every problem in $\mathcal{NP}$ (and, thus, we would obtain $\mathcal{P}=\mathcal{NP}$). A decision problem (not necessarily in $\mathcal{NP}$) with this property is also called $\mathcal{NP}$-hard. This means that a problem is $\mathcal{NP}$-complete if and only if it is both $\mathcal{NP}$-hard and contained in $\mathcal{NP}$. 
The first problem shown to be $\mathcal{NP}$-complete in Cook's famous theorem \citep{Cook:STOC71_UPCT} is the (Boolean) satisfiability problem (SAT). Shortly after, \cite{Karp:complexity_UPCT} gave a list of 21~fundamental problems that are $\mathcal{NP}$-complete. While Cook's proof that SAT is $\mathcal{NP}$-complete required considerable effort, proving that further problems are $\mathcal{NP}$-complete became significantly easier with this knowledge and hundreds -- if not thousands -- of problems were shown to be $\mathcal{NP}$-complete.

A decision problem is $\mathcal{NP}$-complete if and only if (1) it is contained in $\mathcal{NP}$ and (2) some $\mathcal{NP}$-complete problem (and, therefore, all problems in $\mathcal{NP}$) can be reduced to it via a \emph{polynomial-time reduction}. Such a polynomial-time reduction works as follows: For any instance of the known $\mathcal{NP}$-complete problem (e.g., SAT or TSP), one has to construct an instance of the investigated problem in polynomial time such that the two instances are equivalent, i.e., the constructed instance is a yes instance if and only if the given instance is a yes instance. Note that the requirement that the instance must be constructed in polynomial time (and, therefore, have encoding length polynomial in the encoding length of the original instance) is crucial. A common error in reductions is that the encoding length of the constructed instance depends polynomially on the size of numerical values in the given instance (instead of their encoding length).

The importance of the encoding can be illustrated by the 0-1 knapsack problem (KP), which is $\mathcal{NP}$-hard if binary encoding is used, but can be solved in polynomial time (via dynamic programming) if unary encoding is used (so $\mathcal{NP}$-hardness of the unary-encoded version would imply that $\mathcal{P}=\mathcal{NP}$). Problems like this, i.e., problems whose binary-encoded version is $\mathcal{NP}$-hard, but whose unary-encoded version can be solved in polynomial time, are called \emph{weakly $\mathcal{NP}$-hard}, while problems (such as SAT) that remain $\mathcal{NP}$-hard also under unary encoding are called \emph{strongly $\mathcal{NP}$-hard}. The existence of a pseudopolynomial-time algorithm is possible for weakly $\mathcal{NP}$-hard problems, but not for strongly $\mathcal{NP}$-hard problems (unless $\mathcal{P}=\mathcal{NP}$).
            
\subsubsection*{Approximation algorithms}

While some realistic-size instances of $\mathcal{NP}$-hard problems might still be solvable in reasonable time, this is not the case for all instances. In general, one can deal with 
$\mathcal{NP}$-hardness by relaxing the requirement of finding an optimal solution and instead settling for a ``good-enough'' solution. This leads to heuristics, whose aim is producing good-enough solutions in reasonable time (see \S\ref{sec:Heuristics} for details) and approximation algorithms \citep{Vazi10_UPCT, SW11_UPCT, Ausi13_UPCT}. Given~$\alpha\geq 1$, an \emph{$\alpha$-approximation algorithm} for an optimisation problem is a polynomial-time algorithm that, for each instance of the problem, produces a solution whose objective value is at most a factor~$\alpha$ worse than the optimal objective value. The factor~$\alpha$, which can be a constant or a function of the instance size, is then called the \emph{approximation ratio} or \emph{performance guarantee} of the approximation algorithm. While it is standard to use $\alpha\geq1$ for minimisation problems, there is no clear consensus in the literature as to whether $\alpha\geq1$ or $\alpha\leq1$ should be used for maximisation problems. For example, the simple extended greedy algorithm for the knapsack problem produces a solution with at least half of the optimal objective value on each instance, i.e., it is a $1/2$- or a $2$-approximation algorithm.

While \emph{inapproximability results} can be shown for some $\mathcal{NP}$-hard problems \citep[see][ch.~10]{Hoch96_UPCT}, others allow for approximation algorithms with approximation ratios arbitrarily close to one, i.e., they admit a \emph{polynomial-time approximation scheme} (PTAS). A PTAS is a family of algorithms that contains a $(1+\varepsilon)$-approximation algorithm for every $\varepsilon>0$. If the running time is additionally polynomial in $1/\varepsilon$, the PTAS is called a \emph{fully polynomial-time approximation scheme} (FPTAS). If all objective function values are integers, every FPTAS can be turned into a pseudopolynomial-time exact algorithm, so strongly $\mathcal{NP}$-hard problems do not admit an FPTAS (unless $\mathcal{P}=\mathcal{NP}$). Conversely, pseudopolynomial-time algorithms, in particular dynamic programming algorithms, often serve as a starting point for designing an FPTAS \citep{Woe00_UPCT,PW07_UPCT}.

\subsubsection*{Further complexity classes}
Theoretical computer science developed a wide range of complexity classes far beyond the $\mathcal{P}$ vs.\ $\mathcal{NP}$ dichotomy. Considering algorithms requiring polynomial space, i.e., for which the encoding length of the data stored at any time during the algorithm's execution is polynomial in the encoding length of the input (but no bound on the running time is required), gives rise to the class $\textsf{PSPACE}$. It is widely believed that $\mathcal{NP}\subsetneq\textsf{PSPACE}$, but even whether $\mathcal{P} \neq \textsf{PSPACE}$ holds is not known.

In the theoretical analysis of bilevel optimisation problems \citep[see, e.g.,][]{Lab16_UPCT} the complexity class~$\Sigma_2^p$ plays an important role \citep[see][]{Woe21_UPCT}. 
Here, a yes instance~$I$ is characterised by the existence of a certificate of encoding length polynomial in~$|I|$ such that a certain polynomial-time-verifiable property holds true \emph{for all elements of a given set~$\mathcal{Y}$}. As an example, consider the $2$-quantified (Boolean) satisfiability problem. Here, an instance consists of two sets~$X$ and~$Y$ of Boolean variables and a Boolean formula over $X\cup Y$. The question then is whether there exists a truth assignment of the variables in~$X$ such that the formula evaluates to true \emph{for all possible truth assignments of the variables in~$Y$}. This definition immediately sets the stage for a bilevel problem, where the decision~$x$ of the upper level (the leader) should guarantee a certain outcome \emph{for every possible decision~$y$ at the lower level} (the follower). It is widely believed that $\mathcal{NP}\subsetneq \Sigma_2^p$, although $\Sigma_2^p$-hardness does not rule out the existence of a PTAS \citep{Cap14_UPCT}. Under this assumption, $\Sigma_2^p$-hardness does, however, rule out the existence of a compact ILP-formulation, which can be a valuable finding for bilevel optimisation problems.

For some $\mathcal{NP}$-hard problems, one can construct algorithms with running time $\mathcal{O}(f(k)\cdot \mbox{poly}(|I|))$ for an arbitrary computable function~$f$, where the parameter~$k$ describes a property of the instance~$I$. Such problems are called \emph{fixed-parameter tractable}.
For example, the satisfiability problem SAT is fixed-parameter tractable with respect to the parameter~$k$ that represents the tree-width of the primal graph of the SAT instance.
In this graph, the vertices are the variables and two vertices are joined by an edge if the associated variables occur together in at least one clause, see \cite{Szei03}.
This parametric point of view is captured in the $W$-hierarchy of complexity classes -- see \cite{Nied06_UPCT} and the seminal book by \cite{DF99_UPCT}.


\subsection[Control theory (Xun~Wang)]{Control theory\protect\footnote{This subsection was written by Xun~Wang.}}
\label{sec:Control_theory}

Control theory deals with designing a control signal so that the state or output variables of the system meet certain criteria. It is a broad umbrella term that covers a variety of theories and techniques. Control theory has been widely applied in the studies of economics \citep{tustin1953_XW, grubbstrom1967_XW}, operations management \citep[][also see \citealt{Sarimveis2008_XW} for a recent review]{Simon1952_XW, Vassian1955_XW}, and finance \citep{Sethi1970_XW}. Here, we do not intend to provide an exhaustive or comprehensive review. Instead, we try to structurally organise the concepts and techniques commonly applied in operations research, which means that technical details will be omitted. We direct interested readers to a number of textbooks in the reference list, and an excellent review by \cite{aastrom2014_XW} for those interested in the development of control theory.

The major distinction between control theory and other optimisation theories is that the control variable to be designed is normally a time-varying, dynamic function. The control signal can either be dependent on the state variables (which is referred to as feedback control or closed-loop control) or independent (feedforward control or open-loop control). The design of control signals and control policies (defined as the function between the state of the system and the control, also known as ``control laws'' or ``decision rules'') is based on the structure of the system to be controlled (sometimes called the ``plant'' in the control engineering literature). Thus, the type of the dynamical system often define the type of control problem. In continuous systems, the time variable is defined on the real axis, suitable to describe continuous processes such as fluid processing and finance. In discrete systems, time is defined as integers, suitable in cases such as production and inventory control, where the production quantity is released every day. Linear systems are comprised of linear (or affine) state equations, while nonlinear systems contain nonlinear elements. Nonlinear systems are more difficult to analyse and control, and may lead to complex system behaviours such as bifurcation, chaos and fractals \citep{strogatz2018_XW}. But there are linearisation strategies which approximate the nonlinear system locally as linear systems \citep{Slotine1991_XW}. Based on whether random input is present, the dynamical system can be categorised into deterministic and stochastic.

There are two fundamental methods in the analysis of the system and control. The first relies on time-frequency transformations (Laplace transform for continuous systems and $z$-transform for discrete systems). A transfer function in the frequency domain can be used to represent and analyse the system \citep{towill1970_XW}. This method saves computational effort; however, it can only deal with linear system models and each transfer function only describes the relation between a single input and a single output (SISO). The second method directly tackles the state equations in the time domain and describes the movement of system state in the state space. It is suitable for nonlinear systems and multi-input-multi-output (MIMO) systems. With the advancement of computing technology, the computational burden faced by the time-based method becomes less significant. The literature refers to the frequency-based method as \emph{classic} control theory \citep{Ogata2010_XW} and the time-based method as \emph{modern} control theory.

The system under the effect of the control policy must be examined with respect to its properties and dynamic performance. \emph{Stability} is a property of the dynamical system, that the system can return to its steady state after receiving a finite external disturbance. Stability is a fundamental precondition that almost all control designs must meet, with few exceptions such as clocks and metronomes, where a periodic or cyclic response is desired. The stability criterion is straightforward to derive for linear systems, where both frequency-based \citep[e.g., Routh-Hurwitz stabibility criteria and Jury's inners approach,][]{jury1975_XW} and time-based (e.g., the eigenvalue approach) methods exist. However, stability analysis for nonlinear systems is more challenging \citep{Bacciotti2005_XW}. Other important properties of the control system include \emph{controllability}, defined as the ability to move the system to preferred state using only the control signal; and \emph{observability}, defined as the ability to infer the system state using the observable output signals \citep{Gilbert1963_XW}.

In addition to these intrinsic properties, the system can also be evaluated by the system's response to some characteristic input functions. The \emph{step function} (sometimes referred to as the Heaviside function) takes the value of zero before the reference time point, and one thereafter. The \emph{impulse function} (the Dirac $\delta$ function) takes the value of infinity at the reference time point and zero otherwise. These two input functions usually represent an abrupt change in the external environment. The \emph{sinusoidal function} can be used to describe the periodic and seasonal externalities. The Bode plot describes the amplitude and phase shift between the sinusoidal input and output. For stochastic environments, the \emph{white noise} is used to mimic random disturbances. It is a random signal that follows an identical and independent Gaussian distribution and has a constant power spectrum. The noise bandwidth of the system determines the ratio between output and input variances when the input is iid. The value of the noice bandwidth can be derived from either the transfer function or the state space representation. This concept is used in analysing the amplification phenomena in supply chains (see \S\ref{sec:Supply_chain_management}).

In practise, the system state and even the system structure may be unknown. Therefore, statistical techniques, known as \emph{state estimation} and \emph{system identification}, have been developed. State estimation uses observable output data to estimate the unobservable system states. A popular technique for this purpose is the Kalman filter \citep{kalman1960_XW}, essentially an adaptive estimator that can be applied not only in linear, time-invariant cases (LTI, where the system is linear and does not change over time), but also non-linear and time-variant cases. For example, it has been applied to estimate the demand process from the observed sales data \citep{Aviv2003_XW}. System identification attempts to ``guess'' the structure of the system from the input and output. 

Along with the development of control theory, various control strategies have been proposed. They are designed to fit the structure of the system, the objective of the control, and most importantly, to offer a paradigm to design the control policy. In what follows, we provide a brief summary of control strategies. \emph{Linear} control strategies can be represented linearly (in the form of transfer function). They offer great analytical tractability and satisfactory performance, especially when the open-loop system is also linear. Two widely adopted policies in this family are proportional-integral-derivative (PID) control and full-state feedback (FSF) control. In PID control, an error signal between the output and the reference input (e.g., a Heaviside function) is computed. The control signal is a linear combination of the error, the integral, and the derivative of the error. These three components can appear separately. The proportional control has been applied in mechanical and managerial mechanisms such as the centrifugal governor and production planning \citep{Chen2007_XW}. The full-state feedback control defines the control signal as a linear combination of the full system state vector, where the coefficient vector (the ``gain'') shares the same dimensionality as the state vector. By tuning the gain, the poles of the closed-loop system (the eigenvalue of the transition matrix or the roots of the characteristic equation) will change their position in the complex plane, adjusting the system performance. The full-state feedback policy can also be applied in production and inventory control \citep{Gaalman2006_XW}.

In contrast to the linear strategies, the \emph{nonlinear} control strategies are defined as policies where the control signal cannot be represented by a linear function of the system state \citep{Slotine1991_XW}. These policies are primarily used when the open loop system is also nonlinear. One such policy is sliding mode control, where the control signal is a switching function of the state, dependent on some switching rules. The system is then maintained near a hypersurface of the state space (sliding), where the dynamic behaviour of the system is desired. It should be ensured that the hypersurface is reachable from any initial state and that the system state can be maintained on the hypersurface by the policy. In practise, bang-bang control is adopted frequently as a special case of sliding mode control, where the control signal can take only two possible values. The rocket engines and domestic thermostats are examples of such (with on and off states). 

\emph{Optimal control} aims at finding the control signal or control policy that allows an objective function to reach its extreme point \citep{Sethi2009_XW, bertsekas2012dynamic_DLLD}. The objective function could be dependent on the state, output and/or control. Many control policies mentioned above, e.g. full-state feedback control and sliding mode control, have been proved to be the optimal control of some control problems. Optimal control in the special sense is based on Pontryagin's Maximum (or equivalently Minimum) Principle and mainly deals with the design of the open-loop control signal. When equipped with the Hamilton-Jacobi-Bellman (HJB) theory, it can be used to design optimal feedback control policies. Optimal control is closely connected with dynamic programming, which will be reviewed in \S [dynamic programming]. The optimal control technique has been widely applied in operations management \citep[e.g.,][]{Kumar2003_XW}.

When random external disturbances are present, the \emph{stochastic control} techniques are necessary \citep{aastrom2012_XW}. In these situations, objective functions are usually statistical functions of the state or the output, such as the absolute mean or variance. The most well-studied stochastic control problem is the Linear Quadratic Gaussian (LQG) problem, where the system is linear, the objective function is of quadratic form, and the noise signal follows a Gaussian distribution. The optimal control policy in this case is a linear one. Many supply chain management problems can be modelled in LQG form \citep{Lee1997_XW}. For more complex problems involving nonlinearity or an unspecified system structure, the \emph{model predictive control} (MPC) approach can be used \citep{Camacho2013_XW}. This approach transforms the infinite-horizon problem into a finite-horizon problem by focusing only on $T$ periods in the future, deriving the control signal for these $T$ periods, and adopting the most recent control. In the next period, the prediction is updated, and this process is repeated. MPC is not an optimal control method due to the finite-horizon approximation, yet it works very well in practise \citep{Doganis2008_XW}. To deal with parametrical uncertainties in the disturbance, \emph{robust control} provides guaranteed performance \citep{Zhou1998_XW}. The well-known $H_\infty$ control (H infinity) is one of such examples. It minimises the largest singular value of the transition matrix function, which in SISO systems equates to the peak value of the frequency response curve. This minimax strategy ensures that any frequency component in the input will not be amplified too much. Finally, if the system parameters vary over time, \emph{adaptive control} allows the control policy to update according to the estimated parameters \citep{aastrom2013_XW}. The difference between adaptive and robust control is that the policy is dynamic in the former and static in the latter. 

Recent development of control theory can be seen in the controlling of complex, large scale and network system; the use of artificial intelligence in control engineering; and the application of control theory in areas of physics, biology and economics.

\subsection[Data envelopment analysis (Sebastián~Lozano)]{Data envelopment analysis\protect\footnote{This subsection was written by Sebastián~Lozano.}}
\label{sec:Data_envelopment_analysis}

Data Envelopment Analysis (DEA) is a non-parametric frontier analysis methodology mainly used to assess the relative efficiency of a set of homogeneous operating units (termed Decision Making Units, DMUs). DMUs are assumed to consume inputs (i.e., resources) to produce outputs (e.g., goods and services). The production function that indicates the amount of outputs that can be produced from a given input vector is unknown. DEA does not make any assumption about the functional form of that dependency. Instead, DEA uses the observed data to infer the Production Possibility Set (PPS), also called the DEA technology, which contains all the operating points that are deemed feasible. This is achieved on the basis of a few assumptions (like envelopment of the observations, free disposability of inputs and outputs, convexity and returns to scale) and invoking the Minimum Extrapolation Principle. The resulting PPS contains all linear combinations of the observations along with all the operating points that they dominate. This leads to Linear Programming models whose main decision variables are the intensity variables used to compute the target operating point (projection). This target operating point must dominate the DMU being projected and represents maximal improvements (i.e., input reduction and output increase) with respect to the latter. Hence, the computed target belongs to the efficient frontier (which is the non-dominated subset of the PPS) and the efficiency score is a decreasing function of the distance from the DMU to the computed efficient target. There are different ways of measuring this distance, which, ultimately, depends on the potential input and output improvements (i.e., slacks) computed by DEA. Before diving into the DEA methodology note that, as \cite{Cook2014-xo_SL} point out, although DEA has a strong link with production theory in economics, it is often used to benchmark the performance of manufacturing and service operations. In such benchmarking exercises, the efficient DMUs, as defined by DEA, may not necessarily form a ``production frontier'', but rather a ``best-practice frontier''. Thus, the purpose of the performance measurement exercise affects the classification of the different variables considered into inputs or outputs. 

\subsubsection*{Efficiency assessment and target setting DEA models}
The seminal DEA models by \cite{Charnes1978-nm_SL} and \cite{Banker1984-no_SL}were oriented (i.e., gave priority to reducing the inputs or to increasing the outputs) and looked for a uniform (i.e., radial) improvement in all the input or output dimensions. The projection can also be estimated using a given direction, giving rise to Directional Distance Function (DDF) DEA models \citep{Wang2019-sv_SL}. However, most DEA approaches are non-radial and non-oriented \citep[e.g.,][]{Fukuyama2009-mo_SL}. Actually, because DEA aims at simultaneously improving inputs and outputs, it is inherently a multiobjective optimisation approach. Hence, taking into account the preferences of a decision maker, any Pareto optimal point can be selected as efficient target \citep{Soltani2020-xv_SL}.

Most DEA models compute targets that can be sometimes far away from the observed DMU. This increases the difficult and effort required to achieve the target. Hence, DEA models that compute closest efficient targets have been developed \citep{Aparicio2007-fb_SL}. An alternative is to compute stepwise efficiency improvement approaches that may eventually achieve ambitious efficient targets but after several gradual improvement steps \citep{Lozano2005-hz_SL}.

DEA models for handling non-discretionary variables \citep{Banker1986-ai_SL}, undesirable outputs \citep{Kuosmanen2005-og_SL}, integer variables \citep{Kazemi_Matin2009-ha_SL}, ratio variables \citep{Olesen2022-is_SL}, negative data \citep{Sharp2007-xj_SL}, and fuzzy data \citep{Arana-Jimenez2022-nv_SL} have also been proposed. Each of the above ``complications'' requires specific adaptations of the methodology and being capable of taking them into account is a proof of the power and flexibility of DEA.

The DEA models based on the PPS concept are labelled as envelopment formulations. There are also dual multiplier formulations in which the decision variables are not the intensity variables used to compute the target inputs and outputs but the corresponding input and output shadow prices. Multiplier formulations let each DMU choose these input and output weights so that its efficiency is maximised. This freedom often leads to DMUs choosing idiosyncratic or unreasonable weights. Imposing Assurance Regions (AR) and other types of weight restrictions has been proposed \citep{Allen1997-fa_SL} as well as measuring the efficiency of the DMUs as the average of the cross-efficiency scores computed with the input and output weights chosen by the different DMUs \citep{Doyle1994-of_DEA,Chen2022-ms_SL}. Another alternative that has been proposed is using a Common Set of Weights (CSW) instead of letting each DMU choose its own \citep{Salahi2021-eh_SL}.

In addition to computing efficiency scores, DEA can be used to rank the DMU. The problem here is that in conventional DEA all the DMUs labelled as efficient are tied and cannot be ranked. In addition to the CSW or cross-efficiency approaches mentioned above, there are other DEA-based full ranking methods, like the super-efficiency approach \citep{Tone2002-ha_SL}.  Alternatively, instead of fully ranking the DMUs, ranking intervals and dominance relations can be established \citep{Salo2011-io_DEA}.

\subsubsection*{Dynamic and Network DEA models}
DEA views DMUs as input-output black boxes. However, it is often the case that DMUs have an internal structure with different stages or processes (sometimes labelled subDMUs). Many different Network DEA (NDEA) models have been developed to address these scenarios \citep{Tone2009-ph_SL}. The key features of NDEA models are that each process has its own technology and that, except in the case of parallel processes, there exist intermediate product flows between the processes. Some NDEA models can compute an efficiency score for each process and relate the overall efficiency score to the scores of the individual processes \citep{Kao2016-sl_SL}. It must be noted that the NDEA configuration most studied and most commonly used in practice involves two stages in series \citep[see, e.g.,][]{Cook2010-bk_SL,Halkos2014-de_SL}.

Multi-period and dynamic scenarios can be modelled in a manner similar to NDEA simply by considering each time period as a subDMU. The difference between multi-period approaches \citep{Kao2014-dj_SL} and Dynamic DEA \citep{Tone2010-gh_SL} is that in the latter there are flows between consecutive periods (i.e., carryovers). Dynamic NDEA (DNDEA) models, in which there are carryovers between periods as well as intermediate product flows between the processes, have also been developed \citep{Tone2014-hp_SL}.

\subsubsection*{Centralised DEA models}
DEA generally projects each DMU separately onto the efficient frontier. There are situations in which the DMUs belong to the same organisation and there is a Central Decision Maker (CDM) that is interested in the overall system performance and therefore in projecting all the DMUs simultaneously. This type of Centralised DEA (CDEA) models are commonly used for resource allocation \citep{Lozano2004-mz_SL} and for centralised production planning \citep{Lozano2014-bb_SL}. Also, an approach to measure the centralised efficiency of the individual DMUs in CDEA scenarios has been proposed \citep{Davtalab-Olyaie2022-yb_SL}.

DEA models for allocating a fixed input or common revenue \citep{Li2021-ia_SL} or for fixed-sum-outputs \citep[FSO;][]{Zhu2017-ol_SL} also share with CDEA the need to project all the DMUs simultaneously to take into account their interrelationships. These models, same as CDEA, can use an envelopment or a multiplier formulation. While the key feature of the former is that all DMUs are projected simultaneously, that of the latter is that, same as in CSW, a single set of input and output weights is considered. 

\subsubsection*{DEA and Total Factor Productivity (TFP) growth}
DEA can be used to compute the Malmquist Productivity Index (MPI) by projecting the DMU in two consecutive periods onto the efficient frontier of each period and computing the geometric mean of the change in the corresponding radial efficiency scores between the two periods \citep{Fare1992-gf_SL}. The Malmquist-Luenberger Productivity Indicator (MLPI) is analogous but it employs the arithmetic average and an additive decomposition of DDF efficiency scores \citep{Chambers1996-zd_SL}. In both cases, the TFP growth of each DMU can be decomposed into an efficiency change and a technological change component. Other alternative decompositions of the MPI and MLPI have been developed \citep{Epure2011-yw_SL}.

Other approaches compute a global MPI \citep{Pastor2005-rg_SL,Kao2014-dj_SL}. These have the circularity property, missing in the adjacent-periods MPI. Changes in prices can be also incorporated to compute and decompose a global cost MPI \citep{Tohidi2012-nr_SL}. MPI variants that take into account the projections of all the observations or of different groups of observations as well as approaches to compute and decompose the aggregate productivity growth index of a whole industry and input-specific productivity growth indexes have also been proposed \citep{Aparicio2017-dh_SL,Kapelko2015-wf_SL}.

\subsubsection*{Metafrontier analysis}
In scenarios where the DMUs are heterogeneous and belong to different groups, not necessarily disjoint, the DMUs can be projected onto its group frontier as well as onto the metafrontier that results from enveloping all the group frontiers. Measuring the difference between the corresponding efficiency scores can be used to estimate the distance between both frontiers and hence the corresponding technology gap of each group. Although the group technologies are generally convex, the metatechnology is generally non-convex \citep{Afsharian2018-ah_SL}.

The metafrontier approach can be used in DNDEA \citep{See2021-yy_SL} and CDEA \citep{Gan2021-fz_SL} contexts. Also, using metafrontier concepts with each group of observations corresponding to a different time period, meta-MPI and meta-MLPI can be computed and appropriately decomposed \citep{Portela2010-zl_SL}.

\subsubsection*{Other DEA approaches}
There are other interesting DEA approaches that have not been covered above, like congestion \citep{Ren2021-lh_SL}, window analysis \citep{Peykani2021-qb_SL}, etc. Moreover, the field, although mature, is still expanding, with promising new developments, like Efficiency Analysis Trees (EAT) \citep{Esteve2020-ud_SL}, Support Vector Frontiers (SVF) \citep{Valero-Carreras2022-gd_SL}, or big data DEA \citep{Dellnitz2022-ei_SL}. This is not to mention the large and increasing number of DEA applications (see \S\ref{sec:Education}, \S\ref{sec:Environment}, and \S\ref{sec:Power_markets_and_systems}). For further learning on DEA the interested reader is referred to existing textbooks \citep{Cooper2007-pc_SL}, handbooks \citep{Cooper2011-sx_SL,Cook2014-fa_SL,Zhu2015-co_SL} and review papers \citep{Kao2014-jt_SL,Contreras2020-rn_SL,Peykani2020-zz_SL}.

\subsection[Decision analysis (Matthias~Ehrgott \& Salvatore~Greco)]{Decision analysis\protect\footnote{This subsection was written by Matthias~Ehrgott and Salvatore~Greco.}}
\label{sec:Decision_analysis}

The term decision analysis was introduced by \cite{howarddecision_MESG} as “a logical procedure for the balancing of the factors that influence a decision”, pointing out that “the procedure incorporates uncertainties, values and preferences in a basic structure that models the decision”.  According to \cite{keeney1982decision_MESG} decision analysis is a “formalisation of common sense for decision problems which are too complex for informal use of common sense” and, in more technical form “a philosophy, articulated by a set of logical axioms, and a methodology and collection of procedures, based upon those axioms, for responsibly analysing the complexities inherent in decision problems”. In a slighty different  perspective, \cite{roy1993decision_MESG} proposed the concept of decision aiding as “the activity of one who, in ways we call scientific, helps to obtain elements of answers to questions asked by actors involved in a decision-making process, elements helping to clarify this decision in order to provide actors with the most favourable conditions possible for that type of behaviour which will increase coherence between the evolution of the process, on the one hand, and the goals and/or systems of values within which these actors operate on the other”.

For \cite{howarddecision_MESG} “the essence of the procedure is the construction of a structural model of the decision in a form suitable for computation and manipulation”. For \cite{keeney1982decision_MESG}  “the foundations of decision analysis are provided by a set of axioms ... which provide principles for analysing decision problems”. Moreover, “the philosophical implications of the axioms are that all decisions require subjective judgements and that the likelihoods of various consequences and their desirability should be separately estimated using probabilities and utilities, respectively”. In this perspective, the key components of a decision problem are the set of alternatives to be taken into consideration; the set of consequences describing outcomes of alternatives, possibly in terms of a plurality of attributes or criteria; if the consequences are uncertain, the beliefs about their possible realisations expressed in terms of a probability distribution; the preferences of the decision maker.  The objective of the decision analysis is to construct a value function representing the preferences of the decision maker by assigning each alternative an evaluation of its desirability. In case of uncertainty of the consequences, the value function is expressed in terms of expected value with respect to the probability of the consequences. The basic methodology to induce the value function is based on the pioneering work of \cite{Vomo1944_GZ} that showed that a small set of axioms imply that the “utility” of an outcome $x$ is defined as the probability of getting the most-preferred outcome and otherwise the least-preferred outcome that would be indifferent to receiving outcome $x$ with certainty. For \cite{roy1993decision_MESG}, the decision aiding procedure should be developed in a constructive approach in which “concepts, models, procedures and results are here seen as suitable tools for developing convictions and allowing them to evolve, as well as for communicating with reference to the bases of these convictions”. In this perspective the “object is not to know or to approximate the best possible decision but to develop a corpus of conditions and means on which we can base our decisions in favour of what we believe to be most suitable”.

Decision Analysis is mainly based on concepts and tools related to the subjective probability of \cite{RePEc:hay:hetcha:ramsey1926_MESG} and \cite{de1937prevision_MESG}, the theory of expected utility of  \cite{Vomo1944_GZ} and subjective expected utility of \cite{savage1954foundations_MESG}, the Multiple Attribute Utility Theory (MAUT) of \cite{KeeneyRaiffa1976_MESG} and the psychology of judgement and decision-making of  \cite{tversky1974judgment_MESG}. The general idea is to try to evaluate each alternative by assigning a value based on the utilities of the outcomes obtained in each state of the nature multiplied by their probabilities. Delayed consequences may be discounted according to the time at which they are obtained. Each outcome may be evaluated by considering value trade-offs among multiple attributes. Decision analysis techniques include Utility Function Elicitation techniques, Probability Elicitation protocols, Net Present Value, Decision Trees, Influence Diagrams, and Monte Carlo simulation-based decision analysis \citep{clemen1996making_MESG}; Value-Focused Thinking \citep{keeney1996value_MESG};  Portfolio Decision Analysis \citep{salo2011portfolio_MESG}, Bayesian Networks \citep{pearl1988probabilistic_MESG}, and multi-stage decision optimization techniques such as dynamic rogramming and reinforcement learning.

Considering the distinction between normative, descriptive and prescriptive approaches \citep{bell1988descriptive_MESG}, the general perspective of the decision analysis is prescriptive rather than normative or descriptive \citep{edwards2007advances_MESG}. Descriptive analysis concerns the representation and prediction of observed decisions and normative analysis concerns the decisions that ideally coherent and rational individuals should take. Instead, prescriptive analysis tries to propose methods and techniques that will help real people make better decisions with lower regret and greater coherence of values and behaviors.  In this context, decision analysis takes a prescriptive approach that, focusing on the few basic axioms underlying subjective expected utility, adopts ``pragmatically''  the aspiration to the rationality of the normative approach, trying to correct all the heuristics and biases discovered and investigated by descriptive analysis \citep{tversky1974judgment_MESG}. The decision aiding approach \citep{roy1993decision_MESG} takes a different perspective that, criticising the idea that there is an objectively optimal decision to be discovered or at least approximated, aims to provide a recommendation consisting in a set of convictions constructed in the course of a decision process based on multiple interactions between the analyst and the decision maker. The decision aiding approach leads directly to a multi-criteria perspective \citep{belton2002multiple_MESG,greco2016multiple_MESG} taking explicitly into consideration the multiple attributes or criteria (e.g., related to finance, resources, time, and environmental impacts) to be considered in the decision problem at hand. This avoids the risk of a fictitious, not reasoned and arbitrary conversion of evaluations on different criteria  to a common unit, facilitating the discussion on the respective role of each criterion  \citep{roy2005paradigms_MESG,roy1996multicriteria_MESG}. To compare alternatives in a multicriteria decision procedure four main approaches can be adopted:
\begin{itemize}[noitemsep]
	\item aggregating criteria assigning a single value to each alternative: this is the case of above mentioned MAUT, as well as of some of the most well known multicriteria methods such as SMART \citep{edwards1994smarts_MESG}, and UTA \citep{jacquet1982assessing_MESG}; a specific mention deserves in this context the AHP approach \citep{saaty1977scaling_MESG}, that is probably the most adopted \citep[although controversial; see, e.g.,][]{dyer1990remarks} multicriteria method. It is based on the comparison of "importance" of criteria and of evaluation of alternatives with respect to considered criteria by means of a nine point qualitative scale; another specific class in this family are the distance-based methods which, following the main principle of TOPSIS \citep{Hwang1981multiple_MESG}, the first and most famous of these methods, evaluate each alternative on the basis of their distance from the positive ideal solution and the negative ideal solution (the fictitious alternatives that have the best and the worst evaluation on each criterion, respectively); two other well-known methods in this class are VIKOR \citep{opricovic2004compromise} and TODIM \citep{gomes1991todim}.
	\item aggregating criteria by means of one or more synthesising preference relations: the most well known methods based on this approach are the ELECTRE methods \citep{figueira2013overview_MESG,figueira2016electre_MESG}, that build a crisp or valued preference relation called outranking for which an alternative $a$ is at least as good as another alternative $b$ if a $a$ is not worse than $b$ for a majority of important criteria (concordance) and there is no criterion for which the advantage of $b$ over $a$ is so large that it prevents the possibility to declare $a$ at least as good as $b$ (non-discordance); 
	\item aggregating criteria through ``if $\ldots$, then $\ldots$'' decision rules \citep{greco2001rough_MESG}: the alternatives obtain an overall evaluation by matching decision rules with a syntax ``if the alternative is at least at level $l_{j_1}$ on criterion $g_{j_1}$ and $\ldots$ at least at level $l_{j_r}$ on criterion $g_{j_r}$, then the alternative is globally at least at level $l_{tot}$'', such as ``if the student has an evaluation at least good on mathematics and at least medium on literature, then the student is globally at least medium''; these rules are induced from a set of examples of decisions supplied by the decision maker. The advantage of this approach is its explainability due to the fact that the decision rules are expressed in natural language;  
	\item aggregating criteria through an interactive multiobjective optimisation \citep{branke2008multiobjective_MESG}: with this approach one can handle decision problems in which a set of objectives have to be optimised under given constraints \citep[see][]{sawnata85_MESG,steuer_MESG,miett99_MESG,ehrgott05_MESG}. In this context, the concept of  Pareto efficient solution is fundamental: it is a solution for which one cannot improve one objective without deteriorating some others. Several algorithms have been proposed for Pareto set generation and among them let us remember the weighted sum method, the lexicographic method, the achievement secularising function, the epsilon constraint method (for surveys see \citeauthor{marler2004survey}, \citeyear{marler2004survey}, or Chapters 18 and 19 in \citeauthor{greco2016multiple_MESG}, \citeyear{greco2016multiple_MESG}). Dealing with a multiobjective optimisation problem, it is important to discover the set of Pareto efficient solutions most preferred by the decision maker. Recently, beyond many exact methods, some heuristic methods have been proposed for these problems, such as some hybridisation between evolutionary multiobjective optimisation algorithms aiming to approximate the whole set of Pareto efficient solutions \citep{kdeb01_MESG} and some multicriteria preference elicitation methods to guide the optimisation algorithm toward the most interesting set of Pareto efficient solutions \citep[see, e.g.,][]{phelps2003interactive_MESG,branke2016using_MESG}.  
\end{itemize}

\subsection[Dynamic programming (Dong~Li \& Li~Ding)]{Dynamic programming\protect\footnote{This subsection was written by Dong~Li and Li~Ding.}}
\label{sec:Dynamic_programming}

Dynamic programming (DP) was the brainchild of Richard Bellman \citep{bellman1953introduction_DLLD}, who wrote ``DP is a mathematical theory devoted to the study of multistage processes''. Indeed, in the seven decades since his seminal work, the uses of DP have grown substantially thanks to its algorithmic nature in solving sequential decision-making problems, where the preceding actions and their realisation (in terms of consequences) will impact on the course of futures. Examples of such problems include multiperiod inventory management, or asset allocation (portfolio management) over a given time horizon. 
The central idea of DP is to break down the original multistage problem into a number of tail sub-problems by stages. For each stage, the tail sub-problem is a truncated version of the original problem starting from this stage. These tail sub-problems are then recursively solved one by one from the last stage backwards to the first one, at which point the original problem is solved. The solution of such a procedure is guaranteed to be optimal when the problem concerned satisfies a sufficient condition, i.e., the \textit{Principle of Optimality} \citep{bellman1953introduction_DLLD,Puterman2005-lr_HL}, which states “an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision” \citep{bellman1953introduction_DLLD}. Throughout this section, we focus our attention on discrete time systems. For continuous time dynamic systems, the readers are referred to the Hamilton-Jacobi-Bellman equations in optimal control \citep[see, for example,][]{bertsekas2012dynamic_DLLD}.

In particular, for a finite time horizon problem, the decisions are made over a number of \textit{stages} or \textit{decision epochs}, denoted by $t=0,\dots,T-1$. At each decision epoch, after observing the current system \textit{state} $x_t$ (comprised of one or more information variables that characterise how the system progresses), an \textit{action} $a_t$ is taken that leads to an immediate \textit{reward (cost)} of $r_t(x_t,a_t,w_t)$, where $w_t$ is the random disturbance at time $t$ with a known probability distribution. The system then evolves to state $x_{t+1}$ at the next decision epoch, following the \textit{transition function} $x_{t+1}=f_t(x_t, a_t, w_t)$ with the transition probability $p_t(x_{t+1}|x_t, a_t, w_t)$. After the last decision is made at epoch $T-1$, the system evolves to $x_T$ in the terminal stage with the salvage value $r_T(x_T)$. The objective of the problem is to find a \textit{policy} $\pi$, or a sequence of actions $(a_0, a_1,\cdots, a_{T-1})$ prescribed by $a_t = \pi(x_t)$, that maximises (minimises) the total expected reward (cost) across the entire time horizon. Note that for the expected total reward optimisation criterion (or additive reward functions) the Principle of Optimality is always satisfied \citep{Puterman2005-lr_HL}. To avoid the technical subtleties, in what follows we focus on discrete state space $S$ and action space $A$, and assume the random disturbance at an epoch is independent of those in the previous epochs. Define the dimension of the state space $S$ as the number of the information variables in the state. The mathematically inclined readers are referred to \cite{Puterman2005-lr_HL} for discussions on more general situations. Before proceeding, it is worth mentioning that when the random disturbance $w_t$ takes only a single value, the problem reduces to a deterministic problem. Perhaps the two most well known deterministic sequential decision-making problems solvable by DP are the Shortest Path problem \citep{dreyfus1969appraisal_DLLD} and the Knapsack problem \citep{KPP04_SMPT}.

Under the Principle of Optimality, the above-mentioned problem can be solved by backward induction. Denote by $V_t(x_t)$ the value function, or the optimal expected value-to-go from state $x_t$ at epoch $t$ until the end of the time horizon. The value function (for maximisation problems) satisfies the following optimality equations \citep[or Bellman Equations, see e.g.,][]{Puterman2005-lr_HL},
\begin{equation}
    V_t(x_t) = \max_{a_t \in A}\mathbb{E}\left[r_t(x_t,a_t,w_t) + V_{t+1}(f_t(x_t, a_t, w_t)) \right],
    \forall x_t \in S, t=0,\cdots,T-1, \label{Bellman}
\end{equation}
with the boundary condition $V_T(x_T) = r_T(x_T)$. 
By recursively solving the optimality equations from the last stage backwards to time zero, we obtain the optimal value functions and, at the same time, an optimal policy. For this method to work, however, at each stage one has to solve the value function for all states before proceeding to the previous stage. For problems with high dimensional state variables, the solution via this method is simply not practical due to the prohibitive amount of computational time and memory required. The recent development on DP research has been essentially trying to overcome this so called \textit{curse of dimensionality} \citep{Powell2011-km_HL}, which is discussed in the last paragraph of this section.

Many sequential decision-making problems in practice do not have a natural termination stage, leading to a rich body of literature studying infinite horizon problems, for which the total expected reward becomes unbounded as the time horizon tends to infinity. To this end, two alternative criteria have been widely used in the literature  \citep{Puterman2005-lr_HL, bertsekas2012dynamic_DLLD}. The first one applies a discount factor between 0 and 1, say $\beta$, to the future reward, which can be understood as the depreciation of monetary values over time. The total discounted reward is well defined as it is bounded by the sum of a decreasing infinite geometric sequence. In situations where discounting is not appropriate, a meaningful criterion is to consider the long run average reward, or the reward rate per stage. Assuming a stationary system (in which the transition function/probability, the reward function, and random disturbance do not change over time), the Bellman Equations for the total discounted reward criterion take the following form:
\begin{equation}
    V(x) = \max_{a \in A}\mathbb{E}\left[r(x,a,w) + \beta V(f(x, a, w)) \right], \forall x \in S,
    \label{BellmanInfinite}
\end{equation}
where the value function $V(x)$ is the optimal discounted value-to-go from state $x$ over an infinite time horizon. Note that there are no more boundary conditions. There is no more dependency on time either under the assumption of stationary systems, which is often satisfied in practice \citep{bertsekas2012dynamic_DLLD}. When such an assumption is not satisfied, a periodic or cyclic DP can be developed \citep{li2022bid_DLLD}. For brevity we do not include the Bellman equations for the long run average reward criterion but direct the readers to \cite{bertsekas2012dynamic_DLLD} and \cite{Puterman2005-lr_HL}.

There are mainly three solution algorithms \citep{tijms1994stochastic_DLLD, Puterman2005-lr_HL} for infinite horizon problems. The most widely used and understood algorithm is value iteration, or successive approximations as it was called in the early days. Starting from an arbitrary bounded value function vector (e.g., $V_0(x)=0, \forall x \in S$), this method iteratively updates value functions via the recursive equation below until the successive gaps between iterations $k+1$ and $k$ are within a predefined threshold. 
\begin{equation}
    V_{k+1}(x) = \max_{a \in A}\mathbb{E}\left[r(x,a,w) + \beta V_k(f(x, a, w)) \right], \forall x \in S.
    \label{VI}
\end{equation}
An alternative algorithm is policy iteration, which starts with an arbitrary policy and then iteratively improves it until no further improvements are possible. Each iteration includes two steps: firstly the expected value-to-go under the current policy is evaluated via a system of equations similar to (\ref{BellmanInfinite}) but for the actions prescribed by the policy; after that a policy improvement step is undertaken to find an improved action for each state that leads to a better value-to-go \citep{Puterman2005-lr_HL}. In the last algorithm, the system of Bellman Equations (\ref{BellmanInfinite}) are reformulated into a vary large scale linear program, which has one decision variable for each state and one constraint for each state-action pair. Regardless of the solution algorithms, just as in finite horizon problems, the curse of dimensionality remains the biggest hurdle for the implementation of DP. 

Various approximation methods have been proposed to improve the scalability of DP, leading to an important and thriving research field called Approximate Dynamic Programming (ADP). According to \cite{bertsekas2012dynamic_DLLD}, most of the ADP approaches fall into either the value space or policy space. We concentrate on the approaches in the value space (see also \S\ref{sec:Stochastic_models}) while we direct readers to \cite{bertsekas2012dynamic_DLLD} for the policy space counterparts. The basic idea of the value space approaches is to develop efficient methods to approximate the value functions or the expected value-to-go for a given policy. The most studied methods approximate the value functions via a linear or nonlinear combination of a set of handcrafted feature vectors (functions of the state) weighted by a set of parameters, which are calibrated by a suitable method \citep{bertsekas2012dynamic2_DLLD,ding2008allocation_DLLD}. Feature vectors are not always available, in which case Neural Networks have been used to construct feature vectors automatically \citep{Powell2011-km_HL,bertsekas2012dynamic_DLLD,he2018vehicle_DLLD}. Decomposition is also a popular method, which decomposes the original problem into a number of sub-problems each of which has a much smaller state space and can be solved efficiently by the exact algorithms mentioned above. The assembly of the value functions of these sub-problems provides an approximation to the original value functions \citep{kunnumkal2010new_DLLD, li2017dynamic_DLLD}. A distinct decomposition approach is \textit{Whittle's Restless Bandit} framework \citep{whittle1988restless_DLLD, glazebrook2014stochastic_DLLD, li2020switch_DLLD}, which decomposes the original problem via Lagrangian relaxation, calculates a state dependent index value for each sub-problem and uses these index values directly to derive policies for the original problem. Another method in the value space approximates the value functions of a specific policy via Monte Carlo simulation \citep{chang2007simulation_DLLD,bertsekas2012dynamic2_DLLD}, which are then used to find an improved policy. An alternative method is called \textit{Q-Learning} \citep{sutton2018reinforcement_LCAL}, which approximates the \textit{Q-factor} for each state-action pair. The Q-factor for $(x,a)$ is the expected value-to-go by taking action $a$ at state $x$ and then following either a given policy or the optimal policy thereafter. Due to the large number of combinations of state-action pairs, Q-Learning is more suitable for problems with a small state space \citep{bertsekas2012dynamic2_DLLD}. For an in-depth account on ADP we refer to two seminal books of \cite{Powell2011-km_HL} and \cite{bertsekas2012dynamic2_DLLD}. 

\subsection[Forecasting (Fotios~Petropoulos)]{Forecasting\protect\footnote{This subsection was written by Fotios~Petropoulos.}}
\label{sec:forecasting}

Forecasting is concerned with the prediction of unknown/future values of one or multiple variables of interest. If the values of these variables are collected over time, especially/in particular at regular intervals, the corresponding problem is referred to as time-series forecasting. The outputs of forecasting models include point estimates as well as expressions of uncertainty of such estimates in terms of probabilistic forecasts, prediction intervals, or path forecasts. Forecasting is applied in a wide range of applications. In this subsection, we offer an overview of established forecasting approaches that are useful in social settings \citep{Makridakis2020-eu_FP}, such as forecasts produced to support decision making in operations and supply chain management (\S \ref{sec:Inventory}; \S \ref{sec:Supply_chain_management}, finance (\S \ref{sec:Finance}), energy (\S \ref{sec:Power_markets_and_systems}), and other domains.

\textit{Exponential smoothing} is one of the most popular families of models for univariate time-series forecasting. The underlying principle of exponential smoothing models is that, at every step, the forecast is updated such that the most recent information is taken into account by exponentially discounting information from previous periods. The estimates for the exponential smoothing parameters are based on in-sample fits. The first and simplest exponential smoothing method, simple (or single) exponential smoothing, was developed by \cite{Brown56_FP}. This method was able to handle level-only data (no trend nor seasonal patterns). Soon after, it was extended to handle trended and seasonal data \citep{Holt20045_FP,Winters1960_FP}. Forty years later, \cite{Hyndman2002-jp_FP} introduced a fully fledged family of exponential smoothing models that are represented in a state-space framework. Usually, three states are considered: level, trend, and seasonality. The way that these three states interact to produce the final forecast determines the types of trend and seasonality (such as additive or multiplicative). Exponential smoothing models are fast to compute and perform well in a wide range of data \citep{Makridakis2019-oy_FP}, rendering them ideal benchmarks for forecasting applications. Detailed reviews of exponential smoothing models are offered by \cite{Gardner2006-bv_FP} and \cite{Hyndman2008-iu_FP}.

\textit{Autoregressive integrated moving average} (ARIMA) is another very popular family of univariate forecasting models \citep[for a seminal work on ARIMA, see][]{Box1976-af_FP}. In ARIMA, the data are first rendered stationary through transformations and differencing. The stationary data are then fitted in linear regression models (see also the next paragraph on regression models) in which the predictors are either past values of the data (autoregressive terms) or past errors (moving average terms). ARIMA models are theoretically appealing as they can depict a wide range of data generation processes. While manually identifying an optimal ARIMA model can be sometimes challenging, nowadays automated approaches exist \citep[see, for example,][]{Hyndman2008_FP,Franses2014-kl_FP}

When the variable of interest is known to be affected by other factors (also called ``exogenous variables''), then causal modelling can be applied. In its simplest form, causal models can be linear or nonlinear \textit{regression models} that regress the values of the dependent variable on the values of the independent variable(s). Apart from the ordinary least squares regression models, other types of regression models exist, such as the ordinal, logistic, Poisson, negative binomial regression models as well as the Generalised Linear Models (GLMs). The dependent variable (variable of interest) is usually continuous, however specific regression models exist for ordinal or binary dependent variables, such as the ordinal logistic regression model.” But of course there are also regression approaches for count data, like Poisson regression or negative binomial regression (Hilbe’s textbook of the same name is my go-to reference on this), or more generally Generalized Linear Models (GLMs). I would assume these to be more relevant to OR than binary or ordinal logistic regressions.

A common rule for using regression models for forecasting purposes is that the values of the independent variables are either known or can be predicted, as is very common in energy forecasting; see \cite{wer:14_DSRW} and \S\ref{sec:Power_markets_and_systems}. Transformations of the dependent or independent variables are sometimes necessary so that assumptions regarding normality of errors and constancy of the error variance are satisfied \citep{lag:mar:sch:wer:21_DSRW}. Another common issue in regression models is that of multicollinearity between independent variables. Linear regression models can also be used to produce time-series forecasts when no exogenous variables are available. In these cases, we can construct predictors for trend and seasonality and use these predictors as independent variables to model the time-series patterns. Finally, it is also worth mentioning that ARIMA models can be extended to ARIMAX models that can include the effects of exogenous variables, just like autoregressive (AR) models can be extended to ARX.

Instead of forecasting each time series separately, several approaches exist in order to forecast time series data as a collection. \textit{Multivariate models} (also known as structural models) are designed to model cross-sectional data, producing forecasts for many variables of interest at the same time. Such forecasts take into account interactions between all series. A common example is the vector autoregressive (VAR) models \citep{sims1980_FP,hasbrouck95_FP}. Another very popular cross-sectional approach is \textit{hierarchical forecasting} \citep{Athanasopoulos2020-cx_FP}. Hierarchical forecasting deals with time-series data that are naturally arranged in hierarchical structures (for example, product or geographical hierarchies). Forecasts for each node of the hierarchy are first produced independently using standard univariate forecasting approaches (such as exponential smoothing or ARIMA); then, forecasts across the hierarchy are reconciled to achieve coherency \citep{Wickramasuriya2019_FP,Hollyman2021-wj_FP}. Hierarchical forecasts offer better accuracy and are directly relevant for decision makers at multiple levels of an organisation. A different form of forecasting using multiple series, which is widely applied in machine-learning methods, is called \textit{cross-learning}. This approach implies learning \citep[usually through features;][]{Montero-Manso2020_FP,Wang2022comb_FP} from other series to be able to predict the variable of interest. Compared to other cross-sectional approaches, cross-learning requires access to a set of ``reference'' data which, though, do not have to be concurrent to the target data.

Given the plethora of available modelling options, we need ways to help us decide on the best approach for the target data. Two popular approaches for \textit{model selection} are information criteria and cross-validation. Information criteria select the best model amongst a pool of candidate models based on how well the in-sample forecasts fit the actual data (model fit), penalising at the same time for model complexity (Occam's razor). Information criteria are fast to compute and widely applied, mostly due to their implementations in open-source forecasting packages \citep{Hyndman2008_FP}. Cross-validation is based on the comparison of the out-of-sample performance between different models. To achieve this, the available data are split into ``training'' and ``validation'' data. The validation follows a rolling-origin process, where the forecasts of the candidate models are compared for multiple forecast origins \citep{tashman2000out_FP,Bergmeir2012use_FP}. A more recent approach to forecast selection is based on the concept of representativeness \citep{Petropoulos2022REP_FP}. Out-of-sample forecasts with higher representativeness to the past data patterns are preferred to ones with lower representativeness. Regardless of how one selects between forecasts and models, the values of the selection criteria can also be used to \textit{combine forecasts} \citep{kolassa2011combining_FP}. In fact, multiple studies have shown that combining forecasts, using equal or unequal weights, can significantly boost the forecasting performance of individual models \citep{Bates1969combination_FP,now:liu:wer:hon:16_DSRW,Wang2022_FP}. \cite{Claeskens2016-hr} offer a possible explanation on why the performance of forecast combinations is better than that of the individual forecasts.

Apart from statistical, algorithmic and computational approaches, the forecasting process can also be infused by \textit{judgement} (see, also, \S \ref{sec:Behavioural_OR} and \S \ref{sec:Soft_OR_and_problem_structuring_methods}). It is not unusual for forecasts to be directly produced in a judgemental way, without the support of any systematic approaches. Research suggests that such forecasts suffer from several biases \citep{Lawrence2006-ey_FP}. However, managers may sometimes have a unique appreciation of the situation, one that the hard data cannot communicate through models. In such cases, systematic approaches to elicit expert knowledge include the Delphi method \citep{Rowe1999-vs_FP}, structured analogies \citep{Green2007-ts_FP}, prediction markets \citep{Wolfers2004-mq_FP}, and interaction groups \citep{Van_de_Ven1971-vd_FP}; see also \cite{Graefe2011-ha_FP} and \cite{Nikolopoulos2015-mz_FP} for a comparison between these approaches. Apart from producing forecasts directly, judgement may also be used to adjust the formal/statistical forecasts. Judgemental interventions and their efficacy have been well-studied in the literature \citep[see, for example:][]{Fildes2009-tc_FP,Petropoulos2016-uk_FP,Fildes2019-uj_FP}. The main takeaways are: (\textit{i}) negative adjustments are generally more beneficial than positive ones; (\textit{ii}) larger adjustments should be preferred to smaller ones; and (\textit{iii}) the use of feedback and support will limit and improve the role of such judgemental adjustments. Finally, managerial judgement may be applied in other stages of the forecasting process, such as judgementally selecting between statistical models \citep{Petropoulos2018-mt_FP,De_Baets2020-du_FP} or setting their (hyper)parameters.

Forecasts produced in previous periods need to be evaluated once the corresponding actual values become available. Through feedback, \textit{forecast evaluation} allows analysts  to improve the forecasting process and, thus, forecasting performance. The main rule of forecast evaluation is that performance should be measured on data that were not used to fit the models or produce the forecasts. Solely measuring the in-sample performance will inevitably lead to over-fitting and the use of complex forecasting models. There exist a wide array of evaluation metrics. Some of them are suitable for measuring the accuracy or bias of the point forecasts, while others focus on how well the uncertainty around the forecasts is estimated. In the former category, popular metrics are the mean absolute error (MAE), the root mean squared error (RMSE), the mean absolute percentage error (MAPE, which is very popular in practice) and the mean absolute scaled error (MASE, which is theoretically more elegant and popular in academia). It should be noted that \cite{Kolassa2020-jh} showed that different error metrics are minimised by different (point) forecasts and that it makes little sense to evaluate one point forecast using multiple KPIs. For detailed overviews of forecasting metrics for point forecasts and their proper use, the reader is referred to \cite{HYNDMAN2006_FP}, \cite{Davydenko2013_FP}, and \cite{Koutsandreas2022_FP}. In the latter category, a popular metric is the interval score (IS), which is a proper scoring rule and considers both the calibration and the sharpness of the prediction intervals, as well as the pinball score, the continuous ranked probability score (CRPS), and the energy score. \cite{Gneiting2007-oj_FP} offer a review of (strictly) proper scoring rules. Finally, we should mention that nowadays it is common to go beyond strict forecasting performance and measure the performance of the forecasts on the utility \citep{hon:pin:etal:20_DSRW,Yardley2021_FP}.

For a detailed encyclopedic overview of the forecasting field, both in terms of theory and practice, we refer the reader to the work of \cite{PETROPOULOS2022705_FP}; a live version of this encyclopedia is available at \href{https://forecasting-encyclopedia.com/}{forecasting-encyclopedia.com}. \cite{HyndmanAthanasopoulos2021_FP} and \cite{FildesOrd2017_FP} have written comprehensive textbooks on forecasting and its applications. Notable open-source packages with implementations of the most popular forecasting models include the \texttt{forecast} \citep{Hyndman2022Rforecast_FP} and \texttt{smooth} \citep{Svetunkov2022smooth_FP} packages for R statistical software.

\subsection[Game theory (Georges~Zaccour)]{Game theory\protect\footnote{This subsection was written by Georges~Zaccour.}}
\label{sec:Game_theory}

Game theory is a branch of mathematics that studies strategic interactions between decision makers, called players. Strategic interactions means that a player's payoff depends not only on her own decision (action or choice), but also on the decisions made by the other players. The book by \cite{Vomo1944_GZ} is often considered the starting date of game theory, though some of its roots can be traced back to much earlier. Games can be classified along a series of features. In a static game, each player acts only once, whereas in a dynamic game, interactions are repeated over time. In a one-person game, the decision maker plays against a nonstrategic (or dummy) player, often referred to as ``nature'', whose action is the outcome of a probabilistic event with a fixed (known) distribution. Two-player games focus on one-on-one interactions. Duopolistic competition and management-union negotiations are situations that can be modelled as two-person games. Extending the model to $n>2$ players is conceptually easy but may be computationally challenging because each player needs to determine all the possible sequences of actions and reactions for all players. When the number of interacting players is very large, e.g., an economy with many small agents, the analysis shifts from individual-level decisions to understanding the group's behavioural dynamics. An illustration of this is traffic congestion: when an agent attempts to minimise her travel time on a route from A to B, her travel speed depends on the traffic density on that route. What matters is the number of drivers, not their identity. Population and evolutionary games \citep{Hosi1998_GZ,Cr2003_GZ,Sa2010_GZ} and mean-field games \citep{Huetal2003_GZ,Huetal2006_GZ,Huetal2007_GZ,Lali2006a_GZ,Lali2006b_GZ,Lali2007_GZ,Gosa2014_GZ} are branches of game theory that study situations with large numbers of players.

A game can be defined in three forms, namely, in strategic, extensive, or in coalitional form. To formulate a one-shot game in strategic form, we have to specify (\textit{i}) the set of \textit{players} and, for each player, (\textit{ii}) the set of \textit{actions}, and (\textit{iii}) a \textit{payoff function} measuring the desirability of the game's possible outcomes, which depends on the actions chosen by all players. The set of actions can be finite, e.g., to bid on a contract or not, or continuous, e.g., the amount bid. If the players intervene more than once in the game, then we should additionally define (\textit{iv}) the order of play, (\textit{v}) the information acquired by the players over time (stages), and (\textit{vi}) whether or not nature is involved in the game.

In a one-shot game, an action (move) and a strategy mean the same thing. In games where players intervene more than once, the two concepts no longer coincide. A strategy is then a \textit{decision rule} that associates a player's action with the information available to her at the time she selects her move. So an action, e.g., spending advertising dollars, is a result of the strategy. The word strategy comes from Greek (strategia) and has a military sense. An army general's main task is to design a plan that takes into account (adapts to) all possible contingencies. This is precisely the meaning of strategy in game theory. Whether in war, business or politics, it is never wise to allow yourself to be surprised by the enemy. This does not imply that a winning strategy always exists. Sometimes we must be content with a draw or even a reasonable loss.

One-shot games are a useful representation of strategic interactions when the past and the future are irrelevant to the analysis. However, if today's decisions also affect future outcomes and are dependent on past moves, then a dynamic game is needed. In a \textit{repeated game}, the agents play the same game in each round, that is, the set of actions and the payoff structures are the same in all stages \citep{Metal2015_GZ}. The number of stages can be finite or infinite, and this distinction matters in terms of achievable outcomes. In a \textit{stochastic game}, the transition between states depends on the players' actions \citep{Sh1953_GZ,Mene1981_GZ,Jano2018b_GZ,Jano2018a_GZ}. In a \textit{multistage game}, the players share the control of a discrete-time dynamic system (state equations) observed over stages \citep{Baol1999_GZ,En2005_GZ,Krpe2018_GZ}. Their choice of control levels, e.g., investments in production capacity or advertising, affects the evolution of the state variables (e.g., production capacity, reputation of the firm), as well as current payoffs. \textit{Differential games} are continuous-time counterparts of multistage games \citep{Is1975_GZ,Baetal2018_GZ}.

Information plays an important role in any decision process. In a game, the \textit{information structure} refers to what the players know about the game and its history when they choose an action. A player has \textit{complete information} if she knows who the players are, which set of actions is available to each one, what each player's information structure is, and what the players' possible outcomes can be. Otherwise, the player has \textit{incomplete information}. If, for instance, competing firms do not know their rivals' production costs, then the game is an incomplete-information game. The game can also have perfect or imperfect information. Roughly speaking, in a game of \textit{perfect information}, each player knows the other players' moves when she chooses her own action, as in, e.g., chess or a manufacturer-retailer game where the upstream player first announces a product's wholesale price, and then the downstream player reacts by selecting the retail price. The archetype of an imperfect-information game is the prisoner's dilemma, where (in the original story) the players have to simultaneously choose between confessing or denying a crime. A Cournot oligopoly, where each firm chooses its own production level without knowing its competitors' choices, is another instance of an imperfect-information game.

The outcome of a game depends on the players' behaviour. In a noncooperative game, e.g., R\&D competition to develop a vaccine, each player optimises her own payoff, whereas in a cooperative game, the players seek a collectively optimal solution. For instance, the members of a supply chain could agree to coordinate their strategies to maximise the chain's total profit. The
fundamental solution concept in a noncooperative game is the \textit{Nash equilibrium} \citep{Na1950_GZ,Na1951_GZ}. Let $I=\left\{ 1,\ldots ,n\right\} $ be the set of players, $S_{i}$ the set of strategies of player $i\in I$, and let her payoff function be given by $g_{i}\left( s_{1},\ldots,s_{n}\right)$ $:\prod\limits_{i\in I}S_{i}\rightarrow \mathbb{R}$, where $s=\left( s_{1},\ldots ,s_{n}\right) $. Assuming the players are maximisers, the strategy profile $s^{N}=\left(s_{1}^{N},\ldots ,s_{n}^{N}\right) $ is a Nash equilibrium if $g_{i}\left( s_{1}^{N},\ldots ,s_{n}^{N}\right) \geq $ $g_{i}\left( s_{1}^{N},\ldots s_{i-1}^{N},s_{i},s_{i+1}^{N}\ldots ,s_{n}^{N}\right) $ for all $s_{i}\in S_{i}$ and all $i\in I$. At an equilibrium, no player has an interest in deviating unilaterally to any other admissible strategy. Put differently, if all other players stick to their equilibrium values, then player $i$ does not regret implementing her equilibrium value too, which is obtained by best-replying to the choice of the others. That is, $s_{i}^{N}=\arg \max_{s_{i}\in S_{i}}g_{i}\left( s_{1}^{N},\ldots s_{i-1}^{N},s_{i},s_{i+1}^{N}\ldots ,s_{n}^{N}\right) .$ A Nash equilibrium does not always exist, and there may be multiple equilibria, raising the question of which one to select \citep{Selten1975-rl_GZ}. Existence and uniqueness conditions for Nash equilibrium typically rely on fixed-point theorems. If the game is one of incomplete information, then the solution concept is a \textit{Bayesian Nash equilibrium} \citep{Ha1967_GZ_1,Ha1967_GZ_2,Ha1967_GZ_3}. Another noncooperative equilibrium solution concept, which predates the Nash equilibrium, is the \textit{Stackelberg equilibrium}, introduced in a two-player framework by \citet{vo1934_GZ}. There is a hierarchy in decision-making between the two players: the \textit{leader} first announces her action, and next the follower makes a decision that takes the leader's action as given. Before announcing her action, the leader would of course anticipate the follower's response and selects the action that gives her the most favourable outcome. The framework has been extended to several followers and leaders \citep{Sh1984_GZ}.

In a \textit{cooperative game}, the players coordinate their strategies in view of optimising a collective outcome, e.g., a weighted sum of their payoffs, and must agree on how to share the dividend of their cooperation \citep{Mo1988_GZ,Ow1995_GZ}. Different solution concepts have been proposed, each based on some desirable properties, typically stated as axioms, such as fairness, uniqueness of allocation, and stability of cooperation. The most-used solutions in applications are the \textit{core} \citep{Gi1953_GZ}, and the \textit{Shapley value} \citep{Sh1953_GZ}. In any solution, the set of acceptable allocations only includes those that are individually rational. Individual rationality means that a player will agree to cooperate only if she can get a better outcome in the cooperative agreement than she would by acting alone. In a dynamic cooperative game, the agreement must specify, at the outset, the decisions that must be implemented by each player throughout the planning horizon. One concern in such games is the durability of the agreement over time. Clearly, it is rational for a player to leave the agreement at an intermediate time if she can achieve a better outcome. The literature on dynamic games has followed two streams in its quest to sustain cooperation over time, namely, building cooperative equilibria or defining time-consistent solutions. Through the implementation of some (punishing) strategies, the first stream seeks to make the cooperative solution an equilibrium of an associated noncooperative game. If this is achieved, then the result will be at once collectively optimal and stable, as no player will find it optimal to deviate unilaterally from the equilibrium. See \citet{Osru1994_GZ} for repeated games, \citet{Du1995_GZ} and \citet{Paza2015_GZ} for different types of stochastic games; and \cite{Hato1985_GZ}, \cite{Toetal1986_GZ}, and \citet{Hapo1987_GZ} for multistage and differential games. The second stream looks for time-consistent solutions, which are achieved by allocating the cooperative payoffs over time in such a way that, along the cooperative state trajectory, no player will find it optimal to switch to her noncooperative strategies. The idea was initiated in \cite{Pe1977_GZ} and has since been further developed \citep[see][]{Yepe2018_GZ,Peza2018_GZ}.

Game theory has found applications in biology, economics, engineering, management, Operational Research, and political and social sciences.

\subsection[Graphs and networks (Ivana~Ljubi\'c)]{Graphs and networks\protect\footnote{This subsection was written by Ivana~Ljubi\'c.}}
\label{sec:Graphs_and_networks}

Graphs and networks are used to represent interactions, connections or relationships between objects. In network optimisation problems, numerical attributes representing features such as costs, weights or capacities are assigned to objects (also called \emph{vertices}) or to connections between them. If connections are directed, we refer to them as \emph{arcs}, otherwise we call them \emph{edges}. Given an input graph with $n$ vertices and $m$ arcs (or edges), the goal is to find a subgraph that exhibits desired properties (described by a given set of constraints) and that optimises the given objective function (usually measured as the sum of edge or vertex ``weights'' of the solution's subgraph). In the following, we focus on some of the most fundamental and most studied problems in network optimisation. 

The \emph{shortest path problem} in arc-weighted graphs, for example, seeks to find a least costly path from the given source vertex $s$ to the given target $t$. When the arc costs are non-negative, one can use the algorithm of \cite{Dijkstra:1959_IL}, the efficient implementation of which uses Fibonacci heaps and runs in $\mathcal{O}(m + n\log n)$ time. For graphs with possible negative arc costs, in $\mathcal{O}(mn)$ time the Bellman-Ford algorithm either finds the shortest path from $s$ to all other vertices, or it proves that such a path does not exist due to the presence of a negative cost cycle reachable from $s$. The shortest path algorithms are explained in many textbooks, see e.g., \cite{cormen01introduction_IL,Kleinberg-Tardos:2006_IL,Schrijver:2003_IL,Williamson:2019_IL}. 
 
In the \emph{maximum flow problem} (MF), in a given network with arc capacities, we want to send as much flow as possible from the given source $s$ to the given sink $t$ without violating the arc capacities. The problem was motivated by the conflict between East and West during the Cold War \citep{schrijver2002history_JLYHK}. \cite{Ford-Fulkerson:1957_IL} develop the first exact algorithm that searches for augmenting paths in the \emph{residual network}. Their fundamental result, known as the \emph{max-flow/min-cut theorem} states that the maximum flow passing from the source to the sink is equal to the total capacity of the arcs in a \emph{minimum cut}, i.e., the network that indicates how much more flow is allowed in each arc., which is a subset of arcs of the smallest total capacity, the removal of which disconnects the source from the sink. The same result using the duality theory of LPs is given in \cite{Dantzig-Fulkerson:1955_IL}. The famous results from graph theory such as Menger's theorem, K\"{o}nig-Egeváry theorem, or Hall's theorem, follow from the max-flow/min-cut theorem \citep{Ford-Fulkerson:1962_IL}. The method of \cite{Ford-Fulkerson:1957_IL} is pseudo-polynomial when arc capacities are integral, however it may fail to find the optimal solution and need not terminate if some of the arc capacities are irrational \citep{Ford-Fulkerson:1962_IL}. An algorithm that overcomes this issue was independently discovered in the 1970s by \cite{Edmonds-Karp:1972_IL} and  \cite{Dinic:1970_IL}, see also \cite{Dinitz:2006_IL}.  Augmenting the flow along shortest paths (that is, along the paths with fewest edges) guarantees a polynomial-time complexity. Instead of augmenting the flow along a single augmenting path as in \cite{Edmonds-Karp:1972_IL}, the algorithm of \cite{Dinic:1970_IL} finds all shortest augmenting paths in a single phase. Another stream of MF algorithms exploits the \emph{preflow} idea of \cite{Karzanov:1974_IL} in which the vertices are overloaded with the excess flow (i.e., more incoming flow than the outgoing flow is allowed). Subsequent improvements are obtained in the following years. An important breakthrough is achieved by Goldberg \& Tarjan with the introduction of \emph{push-relabel} algorithms \citep{Goldberg-Tarjan:1988_IL}. A \emph{pseudoflow} algorithm for the maximum flow is introduced by  \cite{Hochbaum:2008_IL} and it is later improved in \cite{Hochbaum-Orlin:2013_IL}. The recent implementation by  \cite{Goldberg-et-al:2015_IL} is competitive with the  \cite{Boykov-Kolmogorov:2004_IL} method and the pseudoflow approach. Further historical details and a more complete list of references can be found in \cite{ahuja1993network_IL,Dinitz:2006_IL,Goldberg-Tarjan:2014_IL,Williamson:2019_IL}. Currently, the best strongly polynomial bounds are obtained by  \cite{Orlin:2013_IL} and  \cite{King-et-al:1994_IL}. However, new and improved MF algorithms continue to be discovered. The most recent trends use the idea of electrical flows for obtaining faster (exact or approximate) algorithms, see, e.g., Chapter 8 of \cite{Williamson:2019_IL}. 

In the \emph{minimum cost flow problem} (MCF), for each arc of the graph, a cost is incurred per unit of flow that traverses it. The goal is to send units of a good that reside at one or more \emph{supply} vertices to some other \emph{demand} vertices, without violating the given arc capacities at minimum possible cost. \cite{Edmonds-Karp:1972_IL} introduce the \emph{scaling technique} for the MCF. The technique is later improved by \cite{Orlin:1993_IL}. The algorithms of \cite{Vygen:2002_IL} and \cite{Orlin:1993_IL} have the best-known strongly polynomial complexity bounds for the MCF. \cite{Kovacs:2015_IL} provides a comprehensive literature overview and gives an experimental evaluation of MCF algorithms based on \emph{network-simplex, scaling} or \emph{cycle-cancelling} techniques. The MCF is treated in detail in many textbooks,  \cite{ahuja1993network_IL,Korte-Vygen:2008_IL,Williamson:2019_IL}. One of the important results is the  \emph{integrality of flow} property: if all demands/supplies and arc capacities are integers, then there exists an optimal MCF solution with integer flow on each arc. The result follows from the \emph{totally unimodular} property of the constraint matrix when the MCF is modelled as a linear program. 

In the \emph{minimum cut problem} (MC), one searches for a proper subset of vertices $S$ of a given arc-capacitated graph, such that the total capacity of arcs leaving $S$ is minimised.  For directed graphs, the algorithm of \cite{Hao-Orlin:1994_IL} is based on MF calculations between chosen pairs/subsets of vertices and exploits the push-relabel ideas. For undirected graphs, the \emph{Gomory–Hu tree}, which is a weighted tree that represents the minimum $s$-$t$ cuts for every $s$-$t$ pair in the graph, is introduced in \cite{Gomory-Hu:1961_IL}. This tree is constructed after $n-1$ MF computations, and a simpler procedure has been later given by  \cite{Gusfield:1990_IL}. The algorithm of   \cite{Padberg-Rinaldi:1990_IL} improves the ideas of \cite{Gomory-Hu:1961_IL}  and is widely used within branch-and-cut schemes for solving the travelling salesperson problem (TSP) and related problems. The \emph{maximum adjacency ordering} together with Fibonacci heaps is used in \cite{Nagamochi-et-al:1994_IL}. Randomised approaches can be found in \cite{Karger-Stein:1996_IL,Karger:2000_IL}. The method of   \cite{Karger:2000_IL} is de-randomised by   \cite{Li:2021_IL}. Practical performance of some of these algorithms is evaluated in \cite{Chekuri-et-al:1997_IL,Junger-et-al:2000_IL}.  For additional and more recent references, see the book by  \cite{Williamson:2019_IL}.

The problems mentioned so far all belong to the class $\mathcal{P}$, however most of the network optimisation problems that are relevant for practical applications are $\mathcal{NP}$-hard. We highlight two of them that serve as drivers for discovering new algorithms and methodologies that can be easily adapted to other difficult optimisation problems.

Given an undirected graph with non-negative edge costs, the \emph{Steiner tree problem in graphs} (STP) asks for finding a subtree that interconnects a given set of vertices (referred to as \emph{terminals}) at minimum cost. Two special cases can be solved in polynomial time: when all vertices are terminals (the minimum spanning tree problem), or when there are only two terminals (the shortest path problem). In general, however, the decision version of the STP is $\mathcal{NP}$-complete \citep{Karp:1972_IL}. Older surveys covering developments of first MIP formulations, Lagrangian relaxations, branch-and-bound methods and heuristics can be found in \cite{Maculan:1987_IL,Winter:1987_IL}. The research on the STP was marked by polyhedral studies in the 1990s \citep{Goemans:1994related_IL,Chopra:1994part1_IL}. Exact solution methods for the STP are based on a sophisticated combination of: \emph{reduction techniques} \citep{gamrathscip_IL,Rehfeldt-Koch:2021_IL}, \emph{dual and primal heuristics} \citep{Pajor:2018_IL} embedded within branch-and-cut or branch-and-bound frameworks, see \citep{Polzin:2003_IL,Daneshmand:2003_IL,Polzin:2009approaches_IL,gamrathscip_IL,Fischetti:2016_IL}. Currently best approximation ratio for the STP is 1.39 \citep{Goemans:2012matroids_IL}. A comprehensive survey of the results obtained in the last three decades is given by  \cite{Ljubic:2021_IL}. State-of-the-art computational techniques for the STP are due to   \cite{Rehfeldt:2021_IL}.  

The \emph{Travelling salesperson problem} (TSP) aims at finding the answer to the following question: If a travelling salesperson wishes to visit all $n$ cities from a given list exactly once, and then return to the home city, what is the cheapest route they need to take? For the history of the problem, see \cite{Applegate-et-al:2011_IL} and the book by \cite{Cook:2011_IL}. Since 1954, when \cite{DFJ:1954_IL} found a provably optimal solution for a 49-city problem instance, many important improvements in the development of exact methods have been achieved\footnote{\url{www.math.uwaterloo.ca/tsp/history/milestone.html}}. Facet-defining inequalities are investigated in  \cite{Padberg-Rinaldi:1990a_IL,JungerReineltRinaldi:1995_IL}. MIP formulations, including the famous \emph{subtour-elimination constraints} model by  \cite{DFJ:1954_IL},  are compared in \cite{Padberg-Sung:1991_IL}. Branch-and-cut methods are developed in   \cite{Applegate-et-al:2011_IL,JungerReineltRinaldi:1995_IL,Padberg-Rinaldi:1991_IL}. For the most recent overview on approximation algorithms for the TSP see \cite{Traub:2020_IL}. \cite{Helsgaun:2000_IL}\footnote{\url{akira.ruc.dk/~keld/research/LKH/}} provides an efficient implementation of the  \emph{$k$-opt heuristic} of \cite{lin1973effective_COIT}. \cite{Cook-et-al:2022_IL} extend the algorithm of \cite{Helsgaun:2000_IL} to deal with additional constraints in routing applications and win the Amazon Last Mile Routing Challenge in 2021. The TSP solver Concorde\footnote{\url{www.math.uwaterloo.ca/tsp/concorde}} \citep{Applegate-et-al:2011_IL} incorporates best algorithmic ideas from the past 60 years of  research on the topic. By combining techniques of \cite{Helsgaun:2000_IL} and \cite{Applegate-et-al:2011_IL}, instances with millions of vertices can be solved to within 1\% of optimality, see e.g., TSP solutions on graphs with up to 1.33 billion of vertices\footnote{\url{www.math.uwaterloo.ca/tsp/star}}.

\subsection[Heuristics (Ceyda~Oğuz \& İstenç~Tarhan)]{Heuristics\protect\footnote{This subsection was written by Ceyda~Oğuz and İstenç~Tarhan.}}
\label{sec:Heuristics}

Etymologically meaning to find/discover, heuristics make use of previous experience and intuition to solve a problem. A heuristic algorithm is designed to solve a problem in a shorter time than exact methods, by using different techniques ranging from simple greedy rules to complex structures, which could be dependent on the problem characteristics; however it does not guarantee to find the optimal solution (\S\ref{sec:Combinatorial_optimisation}; \S\ref{sec:Dynamic_programming}). Heuristics have been used in the operational research area extensively with respect to the applications (see, for example, \S\ref{sec:Inventory}, \S\ref{sec:Logistics}, \S\ref{sec:Manufacturing}, and \S\ref{sec:Transportation_Vehicle_routing}). In this subsection, we review the methods employed in the development of heuristics. 

Classifications and strategies provided in the literature guide us for the methods employed in heuristics. Below we provide a thorough classification and explain briefly the basic methods used under each class.

\textit{Induction}, being the simplest method to be applied with an analogy to the mathematical induction, is to solve the original complex problem by extending the results and insights obtained from small and simpler versions of the problem \citep{silver1980tutorial_COIT, silver2004overview_COIT,laguna2013heuristics_COIT}.

\textit{Restriction} methods primarily focus on explicitly eliminating some parts of the solution space so that the problem will be solved given a restricted set of solutions \citep{silver1980tutorial_COIT,zanakis1989heuristic_COIT,silver2004overview_COIT,laguna2013heuristics_COIT}. One way of doing this is to identify common attributes of the optimal solution and search among the solutions having these attributes only \citep{glover1977heuristics_COIT}. Another restriction can be applied by eliminating infeasible solutions considering a combination of decision variables which dictates incompatible values. Beam Search \citep{morton1993heuristic_COIT} is a good example of this class of heuristics which works with a truncated tree structure using strategies similar to a branch-and-bound algorithm (\S\ref{sec:Combinatorial_optimisation}). The trimming of the tree is utilised by a parameter called beam width to indicate how many nodes to have at every level of the tree. 

Heuristics using \textit{decomposition/partitioning} method employ different approaches to divide the problem into smaller and tractable parts, solve these parts separately and combine their solutions to give the solution to the original problem \citep{foulds1983heuristic_COIT,zanakis1989heuristic_COIT,silver2004overview_COIT,laguna2013heuristics_COIT}. The methods used to divide and then combine the solutions are usually dictated by the nature of the problem. For example, Hierarchical Planning proposed by \cite{hax1973hierarchical_COIT} considers the organisational level breakdown and the output of one decomposed problem becomes the input for the other. Rolling horizon also falls under this category \citep{stadtler2003multilevel_COIT}. A problem with a sequence of decisions that span a long planning horizon is solved by dividing the planning horizon into smaller planning intervals. The problem with these small planning intervals is solved continually by fixing the decisions for the first time period and moving into the next time period to solve the next problem. Another approach takes the characteristics of the input data into account and divides the problem such that each part includes only tractable amount of data. For example, data showing clusters of geographically close customers is suitable for this type of partitioning. The decomposed problems are solved independently, and their solutions are combined with a certain rule. Divide and Conquer algorithm heuristically clusters vertices on a given graph, generates a smaller graph for each cluster and solves the original problem for each cluster independently \citep{akhmedov2016divide_COIT}. Decomposition can be made based on an element of the problem, for example solving a logistics problem after dividing it into parts per vehicle. Other decomposition approaches benefit from the structure of the mathematical model developed for the problem. Examples of this sort are Lagrangian Relaxation \citep{fisher1981lagrangian_COIT}, in which complicating constraints are lifted to the objective function with a penalty, and Benders Decomposition \citep{Benders1962-sf_HL,rahmaniani2017benders_COIT}, in which once complicating variables are fixed, the remaining problem can be divided into problems to be solved independently.  

\textit{Approximation} methods focus on the mathematical models and utilise different strategies to make the problem tractable which results in a reduced size of the problem \citep{silver1980tutorial_COIT,silver2004overview_COIT}. One strategy widely used is the aggregation over variables or stages. Another common strategy is to modify the variables, the objective function or the constraints of the mathematical model in different ways, such as converting discrete variables into continuous variables, using a linear objective function instead of the non-linear objective function,, linearising nonlinear constraints, and either eliminating or weakening some of the constraints \citep{glover1977heuristics_COIT}. Kernel Search \citep{angelelli2010kernel_COIT}, which combines relaxation with decomposition over the decision variables, demonstrates that a heuristic may use more than one class of methods in its design. 

\textit{Constructive} heuristics start from an empty solution and build a complete solution by adding an element of the problem following a rule at every step, such as the nearest neighbour algorithm \citep{bellmore1968traveling_COIT} for the travelling salesman problem. Usually constructive heuristics are of greedy nature by making the decision for local optimum in every step. These algorithms can be enhanced by adding a look-ahead mechanism that is by estimating the future effects of a decision rather than just the current effect to avoid pitfalls of being greedy.

\textit{Improvement} heuristics start with a complete solution and improve it by modifying one or more elements of the solution in every iteration until a predetermined stopping condition is achieved. Improvement heuristics in their simplest form utilise a local search which is defined over a neighbourhood structure to express how the moves are performed from one iteration to the next. $k$-opt is an example of this sort which replaces $k$ elements of a solution with another set of $k$ elements in every step if it is beneficial \citep{lin1973effective_COIT}. The parameter $k$ determines the size of the local search and implicitly applies the restriction method discussed above. A neighbourhood is defined by a set of solutions which are reachable form the current solution. A local search is performed by moving from the current solution to another solution in the neighbourhood of it (next solution). Selection of the next solution is done by accepting either the one among random choices that improves the objective function value first (random descent if it is a minimisation problem) or the one resulting in the best objective function, i.e., the local optimum, with respect to that neighbourhood (steepest descent for a minimisation problem). This simple structure focuses on the local information (exploitation of the accumulated search experience) and is known as intensification \citep{glover1990tabu_COIT}. While it will be useful if the structure of the problem is appropriate, it may result in not good enough solutions otherwise. Hence, the improvement heuristic will benefit if it can explore other parts of the solution space, which is known as diversification \citep{glover1990tabu_COIT}. Two immediate strategies to be employed are either to start the search from different initial solutions and choose among the final solutions obtained (multi-start algorithms) or to allow moving to worse solutions if this direction will provide a better path for the future selections (hill-climbing strategy for a minimisation problem). 

Even though \textit{metaheuristics} \citep{glover1986future_COIT} are improvement methods, since they advance notably, considering them as a separate class is worthwhile. Metaheuristics utilise a local search together with intensification and diversification mechanisms and aim at eliminating the problem-dependent and domain-specific nature of other heuristics. Simulated Annealing \citep{kirkpatrick1983optimization_COIT} is one of the most popular metaheuristics which uses a single solution in its local search with a random descent and utilises hill-climbing strategy for diversification. Tabu Search \citep{glover1986future_COIT} is an example of deterministic metaheuristic working with a single solution throughout the search. It explicitly uses history of search in both intensification and diversification mechanisms. Genetic Algorithm \citep{Holland1975-qj_HL} is another popular metaheuristic comprising of random components for intensification and diversification but working with a set of solutions during the search. Variable Neighbourhood Search \citep{hansen1999introduction_COIT} is an excellent example of a design in which diversification is provided by systematically changing neighbourhood structures.     

\textit{Matheuristics} are heuristic approaches that exploit exact approaches (and their complementary strengths) without guaranteeing to find the optimal solutions. While matheuristics are designed with different strategies, we summarise the most widely used three strategies.

Those matheuristics which are originally exact approaches yet are implemented heuristically are overlapping with what is described under \textit{restriction} and \textit{decomposition/partitioning} methods in this subsection. Apart from those overlapping works, in the context of dynamic programming, the corridor method constructs neighbourhoods as corridors around the state trajectory of the incumbent solution \citep{sniedovich2006corridor_COIT}. Defined (preferably large) neighbourhoods can be searched with exact approaches. Dynasearch algorithm uses dynamic programming to search an exponential size neighbourhood stemming from compound moves in polynomial time \citep{congram2002iterated_COIT}.

Another group of matheuristics benefits from multiple exact models collectively within a heuristic mechanism. \cite{tarhan2022matheuristic_COIT} decompose the scheduling planning horizon into a set of buckets, solve a time-indexed model to generate a restricted model for each bucket and solve the restricted models sequentially to construct a complete feasible solution. \cite{della2014matheuristic_COIT} solve a restricted time-indexed model and a model with positional variables iteratively to search the neighbourhood of the incumbent solution. \cite{solyali2022effective_COIT} propose a matheuristic algorithm by sequentially solving different mixed integer linear programs.

Third strategy is to incorporate exact models into different components of the heuristics. This approach may have several variations. First version includes those matheuristics having a constant interaction between heuristics and mathematical programming models. \cite{manerba2014effective_COIT} use the Variable Neighbourhood Search to decide which variables to fix in their fix-and-optimise algorithm. \cite{Archetti2015_COIT} use different integer programming models in both the intensification and the diversification phases of their Tabu Search algorithm to improve the objective function value and/or restore feasibility. \cite{Adouani2022_COIT} apply exact and heuristic approaches respectively to change the value of so-called upper and lower level variables in the neighbourhood search. Other variations include matheuristics that sequentially call heuristics and the models; e.g., exact approaches following heuristics for post-optimisation \citep{Pillac2013_COIT}, exact approaches generating the initial solutions from heuristics \citep{macrina2019energy_COIT}, exact approaches supporting heuristics at their both beginning and end to provide an initial solution and to improve the final solution, respectively \citep{archetti2017matheuristic_COIT}.

We refer the reader for a detailed overview of heuristics to the works of \cite{muller1981heuristics_COIT} and \cite{silver2004overview_COIT}, of metaheuristics to the work of \cite{blum2003metaheuristics_COIT}, and of matheuristics to the work of \cite{boschetti2022matheuristics_COIT}. The most recent book by \cite{marte2018handbook_COIT} on heuristics is another invaluable resource. Finally, the progress of metaheuristics is discussed by \cite{swan2022metaheuristics_COIT}. This work provides a critical analysis of the current state of metaheuristics by focusing on cultural and technical barriers. 

For the future studies in the area of heuristics, new techniques and powerful mechanisms could be derived from practical problems to address complex systems of today's world. Another contribution can be to explore and integrate applications of artificial intelligence to deal with large scale data. Matheuristics are especially often applied for single-objective problems and accordingly, their implementation for multi-objective optimisation is a promising future research direction. For practical purposes, such as to be used within commercial solvers, it is also worthwhile to develop generic matheuristic frameworks that can address specific classes of optimisation problems. Parallel computing (i.e., parallel solution of mathematical models) and integration with machine learning (to, for example, manage the interaction with mathematical models and heuristics) are some other invaluable research directions for matheuristics.

\subsection[Linear programming (Jean-Marie~Bourjolly)]{Linear programming\protect\footnote{This subsection was written by Jean-Marie~Bourjolly.}}
\label{sec:Linear_programming}

Linear programming (LP) offers a framework for modelling the problem of extremising a linear economic function under a set of linear inequality constraints. Solving such models can be approached algebraically as well as geometrically: finding an extreme point of a polyhedron at which a given economic function is maximised or minimised. Since its inception in 1947 by Dantzig, the simplex method has been the standard algorithm for solving linear programs. A precursor, unbeknownst then to Dantzig, was a set of ideas exposed by Fourier in 1826 and 1827, and partly rediscovered by Motzkin in 1936, hence the now famous Fourier-Motzkin elimination method (\citeauthor{Dantzig1963-sy_JMB}, \citeyear{Dantzig1963-sy_JMB}, p. 84–85; \citeauthor{S98_SMPT}, \citeyear{S98_SMPT}, p. 155–157) that solves a set of linear inequalities by sequentially eliminating variables, at the cost though of exponentially increasing the number of constraints.

But since the 1930s, several researchers had been making a headway. Working independently from one another, they had grappled with specific problems: balancing the distribution of revenue (output) with the distribution of outlays (input) in the economic activity of a whole country \citep{Leontief1936-gg_JMB}; general economic equilibrium \citep{Neumann1945-wq_JMB}; production planning \citep{Kantorovich1960-ky_JMB}; transportation planning \citep{Hitchcock1941-mc_JMB,Kantorovitch1958-xh_JMB,Koopmans1949-qx_JMB}; deployment planning and logistics \citep{Dantzig1991-tc}. \cite{Dantzig1982-rx_JMB} said he had been “fascinated” by Leontief’s interindustry input-output model and wanted to generalise it by considering many alternative activities. He also credits von Neumann with the duality theory of linear programming, which parallels the work the latter did with Morgenstern on the theory of games. 

A linear program can always be expressed (in standard form) as \{minimise $cx$, subject to $Ax = b, x \geq 0$\}, where $x$ is an $n$-vector of decision variables, $A$ is an $m$ by $n$ constraint matrix that somehow weighs the variables, $b$ is an $m$-vector that puts limits on the possible values of $x$, and $cx$ is an economic function, called the objective function, that measures the quality of a given solution $x$. It is customary to assume, without loss of generality, that matrix $A$ is of rank $m$ and that $m$ is smaller than $n$ \citep[see, e.g.,][]{PS82_SMPT}. Since $\text{min}(cx) = -\text{max}(-cx)$, one can minimise a “cost” as well maximise a “profit”. Linear programs come in pairs: \{minimise $cx$, subject to $Ax = b, x \geq 0$\} and \{maximise $yb$, subject to $yA \leq c$\}. The former is called the primal, the latter is the dual. The duality theorem of linear programming has been proved to be equivalent to Farkas's lemma that was published in 1902 \citep[see, e.g.,][]{Dantzig1963-sy_JMB}. This implies that finding an optimal solution to a linear program is equivalent to finding a feasible solution to a system made of the primal and dual constraints with the additional inequality $cx \leq yb$. 

With the introduction of duality, we now have three algorithms for solving LPs: the (primal) simplex method that maintains (primal) feasibility throughout and tries to achieve optimality; the dual simplex method that maintains dual feasibility and moves toward primal feasibility; and the primal-dual algorithm that starts with a feasible solution to the dual and keeps improving it by solving an associated restricted primal. The primal-dual algorithm is the favoured simplex tool for solving most network flow problems, for instance, the famous algorithm of \cite{Ford-Fulkerson:1962_IL} for maximum flow. The dual simplex method together with column generation may come in handy when the number of constraints is huge in comparison with the number of variables \citep{DDS06_ALAL}.

A set of linear inequalities defines a convex polyhedron $P$. Therefore, since the objective function is linear, there are only three possibilities: no feasible solution (if and only if $P$ is empty); exactly one optimal solution, located at some extreme point of $P$; infinitely many optimal solutions, located at the points of a face of $P$ of dimension 1 or more, including its extreme points. The simplex method moves sequentially along the edges of $P$ from one extreme point to another. Algebraically, it moves from one set of $m$ linearly independent columns, called a basis, to another. Each basis induces a basic solution defined by setting to zero all the $n - m$ variables that do not correspond to its columns. A basic solution is feasible if all its components are non-negative. The move from one basis to another goes as follows: one column is dropped and replaced by a new one. This exchange, called a pivot, follows a set of rules for choosing the column that enters the basis and the one that exits. It is such that, barring degeneracy, the objective function decreases strictly in value at each pivot. 

Degeneracy is rooted in the fact that an extreme point of $P$ may correspond to several bases. The algebraic expression of this defectiveness is a basic feasible solution with more than $n - m$ zero components. This occurs when the number of hyperplanes intersecting at an extreme point is greater than the minimum necessary to define it. (Think of the tip of a pyramid that has a square base.) Pivoting in the presence of degeneracy may cause the simplex method to cycle. Several schemes have been devised to avoid cycling by carefully choosing the entering and leaving columns. Bland’s rule, considered as both simple and elegant, has been widely adopted \citep{Bland1977-dy_JMB}. As for finding an initial solution, if the problem is feasible, this can be done by introducing artificial non-negative variables that one then tries to drive down to zero.

Evidence shows that the simplex method is very fast in practice \citep[see][]{Shamir1987-pu_JMB}, but \cite{Klee1972-oq_JMB} designed an LP for which it must visit each one of the $2^n$ or so extreme points, which proved that it is not “good” in the sense of \cite{Edmonds1965-zj_JMB}. A “good algorithm” having been defined as one for which the worst-case complexity is polynomial with respect to the dimension of any instance, an important open question became “Is LP in $\mathcal{P}$?”. Khachiyan answered by the affirmative in 1979 when he adapted to the specific case of linear programming a known approach in convex optimisation that had been contributed to by several Soviet mathematicians \citep[see][]{Gacs1981-kz_JMB,Bland1981-fy_JMB,Chvatal1983-ri_JMB}. The argument goes as follows: given an LP, start with an ellipsoid that is big enough to contain the set $S$ of feasible solutions if it is not empty. At each iteration, check whether the centre of the ellipsoid is a solution. If it is not, there is a hyperplane $H$ separating it from $S$. Cut the ellipsoid in half by the hyperplane parallel to $H$ that goes through the centre. Then determine the smallest ellipsoid that contains the half-ellipsoid where one is trying to locate $S$, and repeat. Stop either with a solution (located at a centre) or with an ellipsoid that is too small to contain $S$. This is an important theoretical result \citep[see][]{GLS81_ALAL}, but with very little practical use as far as solving actual LPs goes. 

The same cannot be said, however, of the interior point algorithm introduced by \cite{Karmarkar84_EAY}, in which the moves happen strictly inside the set of feasible solutions instead of taking place on the envelope. Indeed, Karmarkar’s algorithm is polynomial and often competitive with the simplex method. It assumes a canonical form for linear programming in which the variables are constrained to $Ax = 0, x \geq 0$ and $x \in S = {x: x_1 + x_2 + \dots + x_n = 1}$; it further assumes, without loss of generality, that the point $e/n = (1/n, 1/n, \dots, 1/n)$ is feasible and that the minimum value of the objective function $cx$ is zero. As it seeks to stay away from the envelope of the solution polyhedron, the algorithm builds a sequence of strictly feasible solutions, i.e., that have strictly positive components, and makes a repetitive use of $e/n$. The gist of the algorithm is the following: given a strictly feasible solution $x^k$, one can define a simple bijective scaling function $f$ that maps $S$ onto itself so that $x^k$ is mapped onto $e/n$, away from the envelope, and so that $f$ has the following property: if, for any variable $x$, $f(x)$ is strictly feasible in the “new” space, then so too is $x$ in the initial space. In the “new” space, the gradient of the transformed objective function is projected on the null space of the transformed matrix $A$ augmented of a row of 1s, to account for $S$. If $p$ denotes that projection, one then moves in the direction of $-p$, i.e., in the direction of the steepest descent, while feasibility is maintained. The algorithm stops at a point $y^{k+1}$ before reaching the envelope of the feasible region. That point is transformed to $x^{k+1}$ by $f^{-1}$, and this is repeated with a new scaling bijection \citep[see][]{Strang1987-zh_JMB,Goldfarb1989-bz_JMB,Fang1993-bg_JMB,Winston2003-ls_JMB}. Important links between Karmarkar’s algorithm and the ellipsoid method have been pointed out \citep{Todd1988-uj_JMB,Ye1987-kd_JMB}. 

Integer Linear Programming (ILP), i.e., linear programs in which the variables are restricted to being integer-valued, is arguably the most challenging and beautiful expression of LP. Unfortunately, whereas LP is in $\cal{P}$, ILP is not, unless $\cal{P}=\cal{NP}$ \citep{Karp:complexity_UPCT}. However, there are classes of LPs for which, if there exists a solution at all, an integer solution is guaranteed without having to make it a requirement. This is the case, e.g., of most network flow models. And there are classes of ILPs for which any extreme point of the polyhedron of integer solutions can be obtained by “shaving off” non-integer extreme points of the outer polyhedron of real-valued solutions with hyperplanes the number of which is bounded by a polynomial in the dimension of the instance \citep{Edmonds1965-xd_JMB,Edmonds1965-zj_JMB,GLS81_ALAL,Grotschel_1988-ic_JMB,CCPS98_SMPT}. Furthermore, tackling $\cal{NP}$-complete problems has benefited greatly from this approach \citep[see, e.g.,][]{DFJ:1954_IL,ABCC07_SMPT}. 

\subsection[Mixed-integer programming (Adam~N.~Letchford \& Andrea~Lodi)]{Integer programming\protect\footnote{This subsection was written by Adam~N.~Letchford and Andrea~Lodi.}}
\label{sec:Mixed_integer_programming}

\emph{Mixed-integer programming} (MIP) is an $\cal{NP}$-hard generalisation of linear programming (LP; \S\ref{sec:Linear_programming}), in which some or all of the variables are required to take whole-number values. Way back in the late 1950s, it was already realised that a wide variety of important practical problems could be modelled as MIPs \citep{Da60_ALAL,MM57_ALAL}. Of course, at the time, there were no good algorithms, or indeed computers, to enable one to solve MIPs from real-life applications. Since then, however, dramatic progress has been made in theory, algorithms and software. Indeed, it is now possible to solve many real-life MIPs to proven optimality (or at least near-optimality) on a laptop. In this subsection, we review the main developments in this area. For more details, we refer the reader to the textbooks by \cite{CBD11_ALAL} and \cite{CCZ14_ALAL}.

In 1958, \cite{Go58_ALAL} developed the first finitely-convergent exact algorithm for pure IPs (i.e., MIPs in which all variables are restricted to whole-number values). His method was based on \emph{cutting planes}, i.e.
additional linear constraints which cut off fractional LP  solutions. Shortly after, \cite{LD60_ALAL} invented the \emph{branch-and-bound} method, in which a sequence of LP relaxations is embedded within a tree
structure. A few years later, \cite{Ba65_ALAL} devised a simpler branch-and-bound algorithm, for pure 0-1 LPs, which did not rely on LPs at all.

In the 1960s and 1970s, researchers invested considerable effort into deriving ``deep" cutting planes. This led to the discovery of Gomory mixed-integer cuts \citep{Go60_ALAL}, corner polyhedra \citep{Go69_ALAL}, intersection cuts \citep{Ba71_ALAL}, Chv\'atal-Gomory cuts \citep{Ch73_ALAL}, disjunctive cuts \citep{Ba79_ALAL,Ow73_ALAL}, and cuts derived from a study of the so-called knapsack polytope \citep{Ba75_ALAL,Wo75_ALAL}. These topics are still being studied to this day  \citep[see, e.g.,][]{CCZ14_ALAL,Co08_ALAL}.

In 1980, \cite{BM80_ALAL} developed a general-purpose heuristic for 0-1 LPs, called ``pivot-and-complement". This initiated a line of work on so-called ``primal heuristics", which also continues to this day. We will mention this again below.

A major step forward occurred in 1983, with the publication of an award-winning paper by \cite{CJP83_ALAL}. Basically, they did the following before running branch-and-bound: (\textit{i}) ``pre-process" the
formulation in order to make the LP relaxation stronger, (\textit{ii}) automatically generate  knapsack cuts to further improve the relaxation, (\textit{iii}) run a simple primal heuristic in order to obtain a feasible integer solution early on, and (\textit{iv}) permanently fix some variables to 0 or 1 based on reduced-cost arguments. In this way, they were able to solve ten real-life 0-1 LPs that had previously been regarded as unsolvable. The largest of these instances had 2756 variables and 756 constraints, a phenomenal achievement at the time. The approach of \cite{CJP83_ALAL} is now called ``cut-and-branch".

Around the same time, there were several major theoretical advances, such as the proof of the ``polynomial equivalence of separation and optimisation" \citep{GLS81_ALAL} and the development of a polynomial-time algorithm for pure IPs with a fixed number of variables \citep{Le83_ALAL}. For details, we recommend \cite{Sc86_ALAL}.

Coming back to a more practical perspective, several improvements were made to the basic cut-and-branch scheme in the 1980s and 1990s. For brevity, we just mention some highlights. Several authors proposed more powerful
pre-processing procedures \citep[e.g.,][]{DEC93_ALAL,HP91_ALAL,Sa94_ALAL}. \cite{GNS98_ALAL} developed more effective algorithms for generating knapsack cuts. Researchers also began to study cutting planes for \emph{mixed} 0-1 LPs  \citep[e.g.,][]{PVW84_ALAL,VW86_ALAL}, which eventually led to effective cut-and-branch algorithms for such problems \citep[e.g.,][]{VW87_ALAL}.

The next milestone was the invention of \emph{branch-and-cut} by \cite{PR87_ALAL}. In branch-and-cut, one has the option of generating cutting planes at \emph{any} node of the branch-and-bound tree, rather than only at the root node (as in cut-and-branch). Although this is a fairly simple idea, Padberg and Rinaldi added several ingredients to turn it into a highly effective tool. For example, (\textit{i}) care is taken to ensure that cutting planes generated at one node of the tree remain valid at all other nodes, (\textit{ii}) whenever a cutting plane is generated, it is stored in a so-called ``cut pool", (\textit{iii}) when visiting a new node of the tree, one can check the cut pool to see if it contains any useful cuts, (\textit{iv}) one uses a heuristic rule to decide when to stop cutting and start branching at any given node.

Several developments in the 1990s are also worth mentioning. First, there were some interesting works on methods to construct ``hierarchies" of relaxations for 0-1 and mixed 0-1 LPs \citep[e.g.,][]{BCC93_ALAL,LS91_ALAL,SA90_ALAL}. The method in \cite{BCC93_ALAL}, called \emph{lift-and-project}, turned out to be useful when embedded within a branch-and-cut algorithm for mixed 0-1 LPs \citep{BCC96_ALAL}. Shortly after that, \cite{Ba96_ALAL} obtained good results using Gomory mixed-integer cuts instead. This last result was a big surprise: up to then, researchers had thought that Gomory cuts were of theoretical interest only.

By the end of the 1990s, researchers were routinely solving real-life MIPs with thousands of variables and hundreds of constraints to proven optimality. Of course, MIP in general is ${\cal NP}$-hard, so one could not expect to solve \emph{all} instances so quickly. Indeed, \cite{CD99_ALAL} found a family of 0-1 LPs, called ``market split" problems, which proved to be especially challenging for branch-and-cut. This led to the development of a new class of specific algorithms called \emph{basis reduction} methods, see, e.g., \cite{Aa00_ALAL}.

In the period 2000-2010, there was a flurry of impressive works concerned with primal heuristics for MIP. For brevity, we mention just a few examples. \cite{FL03_ALAL} devised a method called \emph{local branching}, which is essentially a form of neighbourhood search in which the neighbourhoods -- being of exponential size -- are searched by solving auxiliary MIPs. Shortly after, \cite{DRL05_ALAL} presented \emph{relaxation-induced neighbourhood search} or RINS, which solves a series of small MIPs to search for integer solutions that are ``close" to the solution of the LP relaxation. Both local branching and RINS are improving heuristics, i.e., the neighbourhoods are defined with respect to a reference feasible solution to be improved. Remarkably, they solve auxiliary MIPs by simply calling a MIP solver in a black-box fashion (with work limits), thus witnessing the maturity of the field. In the same year of RINS, \cite{FGL05_ALAL} introduced the \emph{feasibility pump}, which is highly effective for MIPs where even finding a feasible solution is challenging.

The development of the branch-and-cut technology has been so impressive that many of the above-mentioned developments have been incorporated in software packages. This includes major commercial packages, such as {\tt CPLEX}, {\tt Gurobi} and {\tt FICO Xpress}, and non-commercial ones that are free to academics, such as {\tt SCIP}. We remark that this continual development in algorithms and software has been greatly enhanced by the creation and constant maintenance of {\tt MIPLIB}, a library of MIP instances on which all new methods are now routinely tested 
\citep[see][]{BBI92_ALAL,Gl17_ALAL}.

We end this section by briefly mentioning three other areas of constant development. First, there has been great progress on \emph{decomposition} approaches to MIPs that have special structure, with \emph{branch-and-price} being a particularly effective method \citep[e.g.,][]{DDS06_ALAL}. Second, there is also by now a substantial literature on \emph{stochastic} MIPs \citep[e.g.,][]{KS17_ALAL}. Third, considerable effort has been made to extend the MIP algorithmic technology to cope with nonlinearities, leading to the blossoming field of \emph{mixed-integer nonlinear programming} or MINLP \citep[e.g.,][]{LL12_ALAL}. Particularly effective algorithms and software packages are now available for \emph{convex} MINLP \citep[e.g.,][]{Kr19_ALAL}, and one of its important special cases, mixed-integer second order cone programming \citep[e.g.,][]{BS13_ALAL}.

\subsection[Nonlinear programming (E.~Alper~Y{\i}ld{\i}r{\i}m)]{Nonlinear programming\protect\footnote{This subsection was written by E.~Alper~Y{\i}ld{\i}r{\i}m.}}
\label{sec:Nonlinear_programming}

Nonlinear programming is a generalisation of linear programming (\S\ref{sec:Linear_programming}), in which the objective function or the constraints can be given by general nonlinear functions. Mathematically, a nonlinear programming problem is represented 
as 
\[
\textrm{(P)} \quad \min\{f(x): x \in \mathcal{S}\}.
\]
Here, $\mathcal{S} = \{x \in \mathbb{R}^n: g_i (x) \leq 0, \quad i = 1,\ldots,m\}$ denotes the feasible region, where, $g_i: \mathbb{R}^n \to \mathbb{R},~i = 1,\ldots,m$, and $f: \mathbb{R}^n \to \mathbb{R}$ denotes the objective function.

In comparison with linear programming, nonlinear programming problems have much more expressive power. As such, nonlinear programming problems naturally arise in almost every setting, ranging from investment planning to machine learning; from engineering to medicine; and from energy to sustainability (\S\ref{sec:Logistics}; \S\ref{sec:Ecommerce}; \S\ref{sec:Power_markets_and_systems}; \S\ref{sec:Finance}; \S\ref{sec:Healthcare}; \S \ref{sec:Location}).

In this subsection, we will give a brief overview of theory and algorithms. While we will not cover the modelling aspect, we will mention some classes of optimisation problems with desirable properties, which should imply that using an optimisation model from such classes would significantly increase the likelihood of solving it. 

The difficulty of the generic optimization problem (P) is largely determined by the properties of the objective function $f: \mathbb{R}^n \to \mathbb{R}$ and of the functions $g_i,~i = 1,\ldots,m$ that define the feasible region $\mathcal{S} \subseteq \mathbb{R}^n$. Generally speaking, increasingly more restrictive assumptions on $f$ and on $g_i,~i = 1,\ldots,m$ give rise to increasingly more structured optimisation problems with stronger and more desirable properties. For instance, the special case in which each of $f$ and $g_i,~i = 1,\ldots,m$ is a linear function, referred to as \emph{linear programming} (\S\ref{sec:Linear_programming}), is arguably the most structured class of optimization problems with very appealing theoretical properties, which lay the groundwork for several effective solution methods such as the simplex method \citep[see, e.g.,][]{Dantzig1990_EAY} and interior-point methods \citep{Karmarkar84_EAY,Wright97_EAY,Ye1997_EAY}. In contrast, general nonlinear programming problems usually enjoy fewer desirable properties.

The class of \emph{convex optimisation problems} is comprised of optimisation problems in which each of $f: \mathbb{R}^n \to \mathbb{R}$ and $g_i,~i = 1,\ldots,m$ is a convex function, which implies that $\mathcal{S} \subseteq \mathbb{R}^n$ is a convex set, and includes linear programming as a special case. Any optimisation problem that does not belong to this class is a \emph{nonconvex optimisation problem}. On the other hand, (P) is called an \emph{unconstrained optimisation problem} if $\mathcal{S} = \mathbb{R}^n$, and a \emph{constrained optimisation problem} otherwise. 

A useful notion in nonlinear programming is that of \emph{local optimality}. A point $\hat x \in \mathbb{R}^n$ is said to be a \emph{local minimiser} of (P) if there exists an open ball $\mathcal{B} \subset \mathbb{R}^n$ of positive radius centred at $\hat x$ such that $\hat x$ is a minimiser of $f$ over the potentially smaller feasible region $\mathcal{B} \cap \mathcal{S}$. In contrast, $\hat x$ is a global minimiser of (P) if $\hat x$ is a minimiser of $f$ over the entire feasible region $\mathcal{S}$. Note that a global minimiser is also a local minimiser.

We next briefly give an overview of optimality conditions for each aforementioned class of optimisation problems. We start with unconstrained optimisation problems in the one-dimensional setting (i.e., $n = 1$). If $\hat x \in \mathbb{R}$ is a local minimiser of (P), then $f$ should be neither decreasing nor increasing at $\hat x$. Assuming that $f$ is a continuously differentiable function, we therefore obtain $f^\prime(\hat x) = 0$. This geometric interpretation carries over to the higher-dimensional setting (i.e., $n \geq 2$) by simply viewing a mutivariate function as a collection of one-dimensional functions along \emph{feasible directions} at each $\hat x \in \mathbb{R}^n$, i.e., directions along which one can move starting from $\hat x \in \mathbb{R}^n$ and still remain in the feasible region. In the unconstrained case, every direction $d \in \mathbb{R}^n$ is a feasible direction at every $x \in \mathbb{R}^n$. Using the result from the one-dimensional case, if $\hat x \in \mathbb{R}^n$ is a local minimiser of (P), then the partial derivatives of $f$ with respect to each variable should be zero, or equivalently, that $\nabla f(\hat x) = 0 \in \mathbb{R}^n$, where $\nabla f: \mathbb{R}^n \to \mathbb{R}^n$ is the gradient of $f$. Such a point is called a \emph{stationary point}.

For the special case of convex unconstrained optimisation problems, the convexity of the objective function $f: \mathbb{R}^n \to \mathbb{R}$ implies that the aforementioned necessary conditions are also sufficient, i.e., a point is a local minimiser if and only if it is a stationary point. Furthermore, for convex functions, every local minimiser is, in fact, a global minimiser. Therefore, we obtain the equivalence between global minimisers and stationary points. On the other hand, for a nonconvex optimisation problem, there may be stationary points that may not correspond to a local minimiser of (P) (e.g., if $f(x) = x^3$, then $\hat x = 0$ is a stationary point but not a local minimiser). As illustrated by this example, the complete characterisation of global optimality does not carry over from convex optimisation to nonconvex optimisation, even in the unconstrained setting.

For constrained optimisation problems, we first consider the convex optimisation case. By the convexity of the feasible region $\mathcal{S} \subseteq \mathbb{R}^n$, for any $\hat x \in \mathcal{S}$, the set of all feasible directions is given by $\tilde x - \hat{x} \in \mathbb{R}^n$, where $\tilde x \in \mathcal{S}$. Arguing similarly to the unconstrained case and using the convexity of $f$, a point $\hat x \in \mathcal{S}$ is a global minimiser of (P) if and only if $f$ does not decrease along any feasible direction, i.e., if and only if $\nabla f(\hat x)^T (\tilde x - \hat x) \geq 0$ for all $\tilde x \in \mathcal{S}$. Therefore, as in the unconstrained case, we once again have the equivalence between local and global minimisers.

Next, consider a nonconvex constrained optimisation problem. If the feasible region $\mathcal{S} \subseteq \mathbb{R}^n$ is a convex set but $f$ is a nonconvex function, a similar argument as in the convex case gives rise to the following necessary condition: If $\hat x \in \mathcal{S}$ is a local minimiser of (P), then $\nabla f(\hat x)^T (\tilde x - \hat x) \geq 0$ for all $\tilde x \in \mathcal{S}$. As in the unconstrained case, simple examples show that this condition is no longer sufficient for local optimality. If, on the other hand, $\mathcal{S} \subseteq \mathbb{R}^n$ is a nonconvex set, then we instead rely on a more general notion of \emph{tangent directions} to the feasible region $\mathcal{S}$ at $\hat x$. Therefore, if $\hat x \in \mathcal{S}$ is a local minimiser, then $\nabla f(\hat x)^T d \geq 0$ for every tangent direction $d \in \mathbb{R}^n$ to the feasible region $\mathcal{S}$ at $\hat x$. In general, the set of such tangent directions may not be easy to characterise. Under certain additional assumptions about the geometry of the feasible region $\mathcal{S} \subseteq \mathbb{R}^n$, referred to as \emph{constraint qualifications} \cite[Chapter 5]{BSS2005_EAY}, explicit necessary optimality conditions can be derived. 

Having reviewed optimality conditions, we finally give a brief overview of methods for solving optimisation problems. Nonlinear optimisation algorithms are generally \emph{iterative} in nature, i.e., they generate a sequence of points $x_k \in \mathbb{R}^n,~k = 1,2,\ldots$ that satisfies certain properties. For instance, the sequence may either converge to a local or global minimiser of an optimisation problem, or may simply have a limit point that satisfies the necessary conditions for local optimality. As illustrated by the discussion on optimality conditions, one can establish considerably weaker properties for nonconvex optimisation problems in comparison with convex optimisation problems. In fact, most classes of nonconvex optimisation problems are provably difficult in a formal complexity sense, even when restricted to minimising a quadratic function over a polyhedron \citep{MK87_EAY,PV91_EAY}. As such, it would not be reasonable to expect an algorithm to solve every optimisation problem to global optimality in a reasonable amount of time. 

Therefore, different performance metrics are employed for assessing algorithms for different classes of optimisation problems. While, for convex optimisation problems, one usually expects a ``good'' algorithm to compute a global optimal solution, an algorithm for a nonconvex optimisation problem could be deemed ``effective'' if it always converges to a local (rather than a global) optimal solution. 

In the unconstrained case, given an iterate $x_k \in \mathbb{R}^n$, the main idea is to identify a feasible direction $d \in \mathbb{R}^n$ along which the objective function will decrease. Such a direction $d \in \mathbb{R}^n$, called a \emph{descent direction}, would necessarily satisfy $\nabla f(x_k)^T d < 0$. Then, a step size in this direction is determined according to certain criteria that would guarantee a decrease in the objective function. Therefore, this family if algorithms is referred to as \emph{gradient descent methods} and includes \emph{steepest descent} as a special case (i.e., the case where $d = - \nabla f(x_k)$). Under mild assumptions, this class of algorithms converges to a stationary point of $f$. Recall that such a point is a global minimiser if $f$ is a convex function. Other methods in this class are Newton methods, conjugate gradient methods, and quasi-Newton methods, each of which generates iterates that converge to a stationary point under appropriate assumptions. 

Considering the constrained case, while general convex optimisation problems do not retain all desirable properties of the simpler class of linear programming problems, they still have a sufficiently rich structure that pave the way for provably efficient solution algorithms. In fact, every convex optimisation problem, in theory, can be solved to global optimality by the ellipsoid method \citep{YN76_EAY,Shor77_EAY} or by interior-point methods in polynomial time \citep{NN94_EAY}. Furthermore, a variety of highly effective commercial and non-commercial solvers are available for solving several classes of convex optimisation problems such as linear programming, second-order cone programming, and semidefinite programming that frequently arises
in applications (see, e.g., https://neos-server.org/neos/solvers/index.html).

For the nonconvex constrained case, one approach is based on approximating a constrained optimisation problem by a sequence of unconstrained optimisation problems by either using a penalty function, based on penalising violation of constraints (\emph{penalty methods}), or using a barrier function, based on preventing the violation of constraints by keeping the iterates strictly in the relative interior of the feasible region $\mathcal{S} \subseteq \mathbb{R}^n$ (\emph{barrier methods}). Other methods include Augmented Lagrangian methods, based on combining Lagrangian relaxation with penalty methods, and Sequential Quadratic Programming methods, based on approximating the optimisation problem by a quadratic programming problem.

Finally, various real-life applications in machine learning and data science give rise to very large-scale problems that are beyond the capability of current solvers and computing platforms. For such problems, there exist a variety of heuristic optimisation methods that can be employed to find a good solution in a reasonable amount of time (\S\ref{sec:Heuristics}). However, in contrast with exact methods, such methods usually do not provide any guarantees on the quality of the solution.

Nonlinear optimisation is a very active area of research. The reader is referred to excellent textbooks for further information \cite[e.g.,][]{FM68_EAY,Man94_EAY,BSS2005_EAY,JW06_EAY,Bert16_EAY,LY2016_EAY}.

\subsection[Queueing (Hayriye~Ayhan \& Tuğçe~Işık)]{Queueing\protect\footnote{This subsection was written by Hayriye~Ayhan and Tuğçe~Işık.}}
\label{sec:Queueing}

Queueing systems arise in many real life applications including production, service systems, finance, logistics and transportation. As mentioned in \cite{stidham_HATI}, many queueing models have been studied even before the introduction of Operational Research in the 1950s. We provide a brief overview of methodologies used in queueing systems analysis. We start with exact methods and then continue with approximations and asymptotic analysis. 

The classical analysis of queueing systems involved modelling the single stage Markovian queues as birth-death processes and computing their steady state performances using Markov chain theory. These earlier theoretical contributions were initially summarised in Feller's two volume books \cite{feller1_HATI,feller2_HATI}, then in classical textbooks such as \cite{cooper_HATI}, \cite{grossharris_HATI} and  Kleinrock's two volumes \cite{kleinrock_HATI,kleinrock_HATI2}, and more recently in \cite{gautam_HATI}, \cite{mor_HATI}, and many other books. \cite{takas_HATI} focused on using transforms and generating functions for steady state and transient behaviour of queueing systems. In the early days, transforms and generating functions were considered to be exact expressions but one had to invert these generating functions in order to obtain the actual performance measures which is in general difficult. Marcel Neuts was the first one who approached this inversion problem algorithmically. In his 1981 book, \cite{neuts_HATI} focused on queues that generalise the G/M/1 structure, whereas in his second book, \cite{neuts2_HATI} generalised the structure of the M/G/1 queue. The main idea in these books is to approximate the non-exponential distributions with a phase type distribution (convolution and mixture of exponentials) which yields a continuous time Markov chain model for the original system that could be analysed, at least numerically. This line of research resulted in many contributions on the so-called {\em matrix-geometric} methods \citep[see also][]{latouche_HATI}. Arguably the most well known  result in queueing theory is Little's law ($L=\lambda W$ or its generalisation $H=\lambda G$) which provides a relationship between the mean steady state number of customers and the mean sojourn time in a system. For a thorough survey of the Little's result and its extensions, the reader is referred to \cite{whitt_HATI}. There are numerous proofs of Little's law but \cite{eltaha_HATI} provide an elegant sample path proof. On the other hand, \cite{bertsimas_HATI} relate the steady-state distribution of the number in the system (or in the queue) to the steady state distribution of the time spent in the system (or in the queue) in a queueing system under FIFO (First In First Out). 

While there has been a lot of interest in stationary queues, Massey's 1981 dissertation \cite{massey_HATI} drew attention to the analysis of non-stationary queues (i.e., queues with time dependent arrival and service processes). Massey's dissertation started with the analysis of M(t)/M(t)/1 queue and then extended to other non-stationary Markovian systems. Many subsequent papers, such as \cite{massey4_HATI}, focused on queueing models with time-dependent arrival rates, especially infinite-server ``offered-load'' models which describe the load that would be on the system if there were no limit to the available resources. The main idea of these papers is to provide algorithms (approximations) to solve the Poisson equation. On the other hand, \cite{bertsimas2_HATI} derived a set of transient distributional laws that relate the number of customers in the system (queue) at time t to the system (waiting) time of a customer that arrived to the system (queue) at time $t$.

Networks of queues have been of interest to researchers since 1950s. \cite{jackson1_HATI} was the first one to observe that joint steady state distribution of the number of customers at the nodes of a network of Markovian queues with single server (at each node) is the product of individual distribution of M/M/1 queues. \cite{jackson2_HATI} generalised this result to networks of queues with multiple servers at the nodes. \cite{gordon_HATI} discovered that the stationary distribution again has a product form in closed Markovian networks but in this case a normalisation constant is required. \cite{bcmp_HATI} proved that the product form is insensitive to the service time distribution if the service discipline satisfies certain assumptions. This and other insensitivity results in networks were also considered by \cite{kelly_HATI} and \cite{dick_HATI} which also has results on other networks such as those with blocking and rerouting. \cite{daduna_HATI} focused on obtaining explicit expressions for the steady behaviour of discrete time queueing networks and gave a moderately positive answer to the question of whether there can be a product form calculus in discrete time. In recent years, a number of models involving different compatibilities between jobs and servers in queueing systems, or between agents and resources in matching systems, have been studied, and, under Markovian assumptions and appropriate stability conditions, the stationary distributions were again shown to have product forms \citep[see][and the references therein]{rhonda_HATI}. \cite{francois_HATI} modelled a class of networks using the so-called (max,+) linear systems. In their pioneering work, using (max,+) algebra techniques, \cite{baccellischmidt_HATI} derived Taylor series expansions for the mean waiting times in Poisson driven queueing networks  that belong to the class of (max,+) linear systems. Even though these expansions are sometimes referred to as light traffic approximations, in some cases all coefficients of the series expansion can be computed yielding an exact expression. These results were generalised to transient performance measures by \cite{sven_HATI} and  joint characteristics by \cite{ayhan1_HATI}.

Exact analysis of general queueing systems is often challenging, making the characterisation of performance measures difficult. Thus, asymptotic analyses are commonly carried out via various approximation methods. We next provide an overview of such methods.

Many of the earlier works on the asymptotic analysis of queueing systems focused on heavy traffic and many server approximations for single stage queues. In his pioneering work, \cite{kingman1961single_HATI,kingman1962queues_HATI,kingman1965heavy_HATI} have asymptotically characterised the waiting time distribution for the single server queue with general interarrival and service time distributions under heavy traffic conditions (i.e., when the traffic intensity $\rho \to 1$). Several others have developed heavy traffic approximations for the G/G/s queue, where a sequence of systems with fixed number of servers and traffic intensities $\{\rho_n\}$ approaching one are considered \citep[see, for example,][]{kollerstrom1974heavy_HATI}. In these approximations, the sequence of normalised (i.e., scaled) queue length processes converge to a reflected Brownian motion with negative drift  \citep[see][]{whitt2002stochastic_HATI}, and the associated sequence of scaled stationary queue-length distributions (i.e., the stationary distribution of the limiting diffusion process) converges to an exponential distribution. We refer the reader to \cite{harrison1985brownian_HATI} for a detailed technical treatment of heavy traffic limits and diffusion approximations. Asymptotic analysis was also considered for  multi-class and multi-stage queueing networks. Defining the stability region of these networks using fluid limit analysis was considered in \cite{chen1995fluid_HATI, dai1995positive_HATI,dai1995stability_HATI}. Many of the works that considered heavy traffic analysis of multi-class queueing networks focus on achieving the so-called state space collapse. \cite{bramson1998state_HATI} demonstrated the state space collapse for first-in first-out queueing networks of Kelly type and head-of-the-line proportional processor sharing queueing networks. His framework has been used to prove state space collapse results in several other works including \cite{stolyar2004maxweight_HATI} and \cite{mandelbaum2004scheduling_HATI}. For a more comprehensive review of heavy traffic analysis of multi-class queueing networks, we refer the reader to \cite{chen2001fundamentals_HATI}.

Many-server approximations were also considered for asymptotic analyses of  queueing systems. In these approximations, the traffic intensity can be kept constant while letting the arrival rate and the number of servers go to infinity. \cite{iglehart1965limiting_HATI} showed that the resulting sequence of normalised queue length processes converges to an Ornstein-Uhlenbeck process in the many server setting when the service time distributions are exponential. Later on, \cite{whitt1982heavy_HATI} generalised this result for systems with non-exponential service times.  For a more comprehensive overview of results in this area, see \cite{whitt2002stochastic_HATI}. In their seminal work, \cite{halfin1981heavy_HATI} defined the so called Halfin-Whitt regime for the GI/M/s queue where the traffic intensities converge to one from below, the number of servers and arrival rates tend to infinity, but steady-state probability that all servers are busy remains fixed. They showed that under the appropriate scaling, the queue length processes converge to a diffusion process. In the past decades, many other asymptotic results have been obtained for many server queues in the Halfin-Whitt regime. \cite{reed2009g_HATI} studied the G/GI/s queue and obtained fluid and diffusion limit results for the queue length process.  We refer the reader to \cite{van2019economies_HATI} for a further review of the various asymptotic results obtained in the Halfin-Whitt regime. 

Although heavy traffic approximations for queues have been popular in recent decades, light traffic (as the traffic intensity $\rho\rightarrow 0$) and interpolation approximations have also been developed. \cite{bloomfield1972low_HATI} developed light traffic approximations for a single server queue. \cite{burman1983light_HATI} developed approximations for the expected delay in M/G/s queue both for heavy and light traffic, and showed that as traffic intensity goes to zero, probability of delay depends only on mean service time distributions. \cite{daley1992light_HATI} used light traffic approximations to study the limiting properties of the waiting time in many-server queues. Light traffic approximations have also been used to study the limiting processes in queueing networks  \citep[see, for example,][]{simon1992simple_HATI}. 

As mentioned earlier, approximation methods were commonly used in the asymptotic analysis of time-varying (i.e., non-stationary) queues. \cite{mandelbaum1999time_HATI} developed a fluid approximation for the queue length process in time-varying multiserver queue with abandonments and retrials. \cite{pang2010two_HATI} have developed heavy traffic approximations for infinite server queues with time-varying arrivals. The reader is referred to \cite{whitt2018time_HATI} for a recent review of the literature on non-stationary queues. 

Due to the interest in communication/telecommunication systems, in late 1990s and early 2000s, there was a lot of research on queues with heavy tailed interarrival and/or service times. Intuitively, heavy tailed distributions decay slower than an exponential distribution \citep[see][for a thorough discussion]{resnick2007heavy_HATI}. \cite{boxma2000single_HATI} provided an overview of results for single service queues with heavy tailed interarrival and/or service time distributions. \cite{sabine_HATI} and \cite{ayhan2_HATI} showed that the asymptotics of response time was dominated by the station with the heavy tailed service time in a class of open and closed networks, respectively. \cite{foss2012large_HATI} developed upper and lower bounds on the tail distribution of the stationary waiting time in the GI/GI/s queue with heavy tailed service times.

\subsection[Risk analysis (Louis~Anthony~Cox,~Jr)]{Risk analysis\protect\footnote{This subsection was written by Louis~Anthony~Cox,~Jr.}}
\label{sec:Risk_analysis}

Risk analysis is a discipline that seeks to inform people about what might happen and how to reduce the probability and severity of undesired outcomes. It draws on decision analysis, game theory, and other areas of Operational Research but is distinguished from them by the questions it asks, the frameworks it provides for answering them, and the uses to which its answers are put \citep{Aven2020-tq_TC,Greenberg2020-xl_TC}. Where decision analysis focuses on principles for identifying logically coherent choices that make preferred outcomes more likely based on a decision maker’s beliefs and value trade-offs, risk analysis seeks to inform \textit{analytic-deliberative decision-making} by multiple stakeholders –- possibly with conflicting worldviews, values, and beliefs –- for managing critically important matters ranging from the safe operation of nuclear power plants to priority-setting for public and occupational health and safety measures. 

Risk analysis is often subdivided into \textit{risk perception}, \textit{risk assessment}, \textit{risk communication}, \textit{risk management}, and \textit{risk governance and policy-making} \citep{Greenberg2021-vu_TC}. The following sections describe these components.

\subsubsection*{Risk Perception}
Public concerns and political appetite to address them are shaped by \textit{perceived} risks, whether or not they are accurate.  Several frameworks have been developed to help understand the technical, psychological, and social drivers of risk perceptions \citep{Siegrist2020-ul_TC}.  The \textit{psychometric paradigm} \citep{Slovic2000-zr_TC} explains many aspects of risk perceptions in terms of a few underlying factors such as as \textit{dread risk} (associated with a lack of control, dreaded consequences, catastrophic potential, inequity in the distribution of risks, risks increasing over time, and fatal consequences) and \textit{unknown risk} (associated with unobservability, novelty, unknown exposure, being unknown to science, and delayed consequences). The cognitive \textit{heuristics and biases} literature positions risk perceptions within a “dual process” framework in which rapid emotional evaluations (“System 1”) can be modified by slower, more effortful cognition (“System 2”) \citep{Kahneman2011-vb_TC,Skagerlund2020-cs_TC}. The \textit{cultural theory of risk} \citep{Douglas1983-td_TC,McEvoy2017-zp_TC,Bi2021-gl_TC} posits that individual perceptions of risk are shaped by social and ideological processes that emphasise or suppress perceptions of risks depending on the respondent’s values and preferred form of social order. The \textit{social amplification of risk framework} (SARF) \citep{Kasperson2022-dv_TC} describes the social amplification or attenuation of perceived risks as risk information is communicated among people with different worldviews. 

Major lessons from the study of risk perception are that experts and members of the public often view risks quite differently; that experts often focus on the probability and consequence severity dimensions of risk while members of the public consider many other aspects; that most people tend to overestimate the frequencies of rare but vivid events (e.g., terrorist attacks, murders) and underestimate the frequencies of common but familiar ones (e.g., car accidents, heart attack fatalities); and that risk perceptions of both experts and lay people are predictably shaped and distorted by cognitive heuristics and biases and are amplified or attenuated by media reports and other communications in ways that reflect the recipients’ worldviews. System 1 tends to be innumerate, responding emotionally to possibilities and categories of harm while underweighting or ignoring relevant frequencies and magnitudes. System 2 often fails to sufficiently adjust or correct the promptings of System 1 leading to decisions with predictable regrets. These findings help to explain why expert and actuarial assessments of risk often differ from lay perceptions of risk. In a democratic society, perceptions affect decisions. A major challenge for risk analysis is to assess and communicate risks to help inform and improve collective decisions in ways that understand and respect the realities of risk perception. 

\subsubsection*{Risk Assessment}
\textit{Risk assessment} addresses how large and uncertain risks are. It begins with qualitative questions about what might go wrong and proceeds to quantitative assessments of how likely adverse events are to occur and what their possible consequences and their probabilities would be \citep{Kaplan1981-vg_TC}.  Probabilistic risk assessment (PRA) and quantitative risk assessment (QRA) methods apply probability models and statistical methods to data and modelling assumptions to quantify or bound the predicted frequencies and severities of losses and to estimate how their joint probability distribution would be changed by different risk management policies or interventions. Quantitative measures of risk can be derived from the full probability distribution or stochastic process descriptions of uncertain outcomes \citep{Smidts1997-ma_TC}, including dynamic coherent risk measures used in financial risk analysis \citep{Bielecki2017-kr_TC}. Stochastic process models of the occurrence frequencies of adverse events (such as accidents at power plants or tornadoes in cities) and the probability distribution of losses for each event can also be used to estimate entire cumulative probability distributions for losses over a stated time interval for different scenarios or sets of assumptions \citep[][see also \S\ref{sec:forecasting} and \S\ref{sec:Power_markets_and_systems}]{Kaplan1981-vg_TC}.

In the past decade, PRA techniques such as causal Bayesian networks (BNs), dynamic Bayesian networks (DBNs), and related probabilistic graphical models have increasingly been used to predict the probabilistic effects caused by interventions in engineering systems \citep{Ruiz-Tagle2022-ts_TC} and public health applications \citep{Butcher2021-zo_TC}. They have largely supplanted older and less general PRA techniques such as fault tree analysis and Markov decision processes \citep{Hanea2022-ls_TC,Cox2018-ui_TC}. Together with discrete-event stochastic simulation models and continuous (systems dynamics) simulation, they provide constructive methods to predict how risk management interventions would change the probabilities of outcomes over time. This information can be used for simulation-optimisation of risk management decisions \citep{Better2011-mq_TC}.  

PRA techniques have been extended to address adversarial risks in which intelligent adaptive adversaries rather than chance events threaten the safety and values that a risk manager seeks to protect \citep{Banks2022-fi_TC}; and \textit{unknown risks} (or risks under radical uncertainty, sometimes called \textit{Knightian uncertainty} by economists), in which relevant probabilities are unknown, e.g., by using uncertainty sets that replace precise probability distributions by (usually convex) sets of possible probability distributions or by scenarios of possibilities that are not necessarily exhaustive \citep{Gilboa2017-hp_TC}.  Recent artificial intelligence and machine learning (AI/ML) methods are now being applied to natural hazards and disasters \citep{Guikema2020-pw_TC}, cybersecurity \citep{Nifakos2021-jk_TC}, power markets \citep{mar:nar:wer:zie:22_DSRW}, and financial portfolio risk management problems where new, changing, and unknown conditions make it necessary to learn effective risk prediction and management decision rules from data and experience without the guidance of well-validated PRA models \citep{Cox2020-sj_TC}.  

\subsubsection*{Risk Management, Governance, Communication, and Risk-Cost-Benefit Analysis}
Given public perceptions and technical estimates of risks, what should be done about them? Who should decide, and how? Managing risks to human health, safety, or the environment often involves “wicked” decision problems and “deep” uncertainties, meaning that there are no clear, widely agreed-to definitions of the decision problem and solutions to it \citep{Lempert2021-os_TC}. Although multiobjective and risk-sensitive or risk-constrained optimisation problems can be formulated for some risk management problems, such as routing hazardous cargo, for many wicked risk management problems, relevant decision variables, constraints, possible outcomes, and objective functions may be unknown or not widely agreed to. Risk management in such challenging cases usually involves issues of causation (what can be done and how much difference in outcome probabilities would different feasible choices make?), \textit{collective choice} (how should the disparate perceptions and preferences of individuals be resolved or aggregated for purposes of collective decision-making?) and \textit{risk governance} (who should be responsible for making, implementing, obeying, enforcing, and revising risk management decisions; how should stakeholders participate in risk management decisions; what institutions and processes should guide, restrain, and integrate collective risk management; and how should conflicts be resolved and collective decisions be made at individual, organisational, community, local, national, and international levels?) \citep{Klinke2021-zn_TC}. 

The most immediate decisions for risk managers responding to potential or actual crises are often about \textit{risk communication}. For example, if a pandemic or natural disaster such as a tsunami, volcanic eruption or hurricane, seems possible but not necessarily imminent or certain, then what should scientists and government officials tell policy-makers and the public about the uncertain risks? Who should say what to whom, and how soon?  Different risk communication goals such as informing and empowering individual and community decisions, persuading individuals to change their behaviours, instructing citizens what to do, and informing or shaping policy deliberations and decisions, require different communication approaches. Risk communication frameworks for addressing these challenges overlap with risk perception frameworks but also emphasise the roles of trust in information sources and of \textit{outrage} in mobilising public engagement and changing behaviours \citep{Malecki2020-cm_TC}.   

\textit{Risk-cost-benefit analysis} provides a simple-sounding approach to collective risk management: take risk management actions to maximise expected social utility or net societal benefits \citep[expressed as expected net present value, possibly with a risk-adjusted discount rate;][]{Eliashberg1981-dt_TC,Hammond1992-gv_TC}. However, mathematical impossibility theorems have shown that when different people have sufficiently different beliefs and preferences, there may be no coherent way to aggregate them to make collective decisions that respect normative principles such as Pareto efficiency \citep{Nehring2007-zh_TC}. Trade-offs between measures of accuracy and fairness have recently been identified for machine-learning algorithms used in risk assessment in areas such as mortgage lending and criminal justice \citep{Corbett-Davies2017-ls_TC}. Societal risk management is now often viewed less as a top-down or centralised decision and control process in which experts provide estimated probabilities and social utilities or net benefits to use in risk-cost-benefit or social utility maximisation calculations than as a participatory democratic deliberative process \citep{Rad2021-yr_TC}. Experts in risk analysis can provide useful technical information in this process but should not dominate it \citep{Pellizzoni2000-fl_TC,Greenberg2021-vu_TC,Klinke2021-zn_TC}.

Risk analysis poses intellectual, technical, and practical implementation challenges that are likely to engage and challenge Operational Research and risk analysis professionals for the foreseeable future. A more detailed review of the accomplishments, current state, and remaining challenges for much of the field of risk analysis can be found in the 40\textsuperscript{th} Anniversary Special Issue of \textit{Risk Analysis} \citep{Greenberg2020-xl_TC}, in specialised books such as \cite{Aven2015-wq_TC}, and in online resources\footnote{For example, \url{https://www.sra.org/risk-analysis-introduction/}}.

\subsection[Simulation (Christine~S.M.~Currie)]{Simulation\protect\footnote{This subsection was written by Christine~S.M.~Currie.}}
\label{sec:Simulation}
A simulation aims to reproduce the important behaviour of a real system. Our focus here is on the use of computer simulation models within operational research (OR), whilst acknowledging that the field is much wider and ranges from computer models of sub-atomic particles to simulations involving real human actors, particularly prevalent in medicine and health sciences. Three main \textit{flavours} of simulation are used in this context: discrete event simulation, agent-based modelling and system dynamics. After discussing the uses of simulation, we continue this subsection by introducing these three main flavours before going on to discuss four important areas in simulation research: conceptual modelling, input modelling and parameterisation, simulation optimisation, and finally the newer area of data-driven simulation, linking to Industry 4.0 and digital twins. A selection of open source tools for simulation are given in \S\ref{sec:Open_source_software_for_OR}.

A simulation model, built on a computer, has a number of potential functions. Principally it is used for experimentation because testing out new settings or ways of working on a simulation results in fewer negative implications than experimenting with the real system. This can allow simulation to be used for optimisation of complex stochastic systems and there has been considerable research in this area in recent years, as we discuss below. Simulation can also be used for predicting future behaviour, and the COVID-19 pandemic showcased the predictive power of simulation modelling in a very high-profile situation \citep[e.g., the agent-based model used to advise the UK government and described in][]{Ferguson2020_CC}. The process of building a simulation model results in a better understanding of the real system because of the need to identify and model the important relationships between different entities. Running the model can also help with estimating the sensitivity of model outputs to system parameters. Another use of simulation models is for training. Within the OR context, this most often takes the form of strategic game-playing \citep[e.g., the beer game developed by MIT and described in][]{Sterman1989_XW} to practice decision-making under different scenarios in a safe environment.

\textit{Discrete event simulation} (DES) is typically used to model systems in which entities move through a set of activities. Where these activities require resources, entities will queue until the resource becomes available. Such simulations are described as discrete event because the system state only varies at discrete time points, known as events. For example, a DES model might be used to describe a manufacturing line and in this case the events could include an item starting or finishing processing by a machine on the line. Usually in DES, the simulation clock will jump from one event to the next rather than moving in equal time steps. 

\textit{System dynamics} (SD; \S\ref{sec:Systems_dynamics}) was first developed in the 1950s by Jay Forrester to help with the understanding of industrial problems. SD models deal with stocks and flows, where the dynamics are dictated by a set of connected differential equations. Describing a system using an SD model can help with detecting feedback loops and delay effects and SD modelling is useful for strategic decision-making. 
 
\textit{Agent based modelling} (ABM) describes the behaviour of individual entities or \textit{agents} within a population. As \cite{Macal2016_CC} states, one of the key differences between ABM and both DES and SD is that it takes an agent perspective of a system. Each agent in the simulation will follow its own set of rules dictating its behaviour and how it interacts with the environment and other agents. Agent behaviour is typically stochastic, allowing natural variability to be included in the model. An ABM can be used as a \textit{bottom up} approach to determine \textit{emergent behaviour} where individual actions lead to a system level response.  

Regardless of the simulation modelling technique, the first part of any simulation project is to gain an understanding of the system being modelled, the objectives of the work, and the key components that should be included, referred to as \textit{conceptual modelling}. There is some discussion of the exact definition of conceptual modelling in \cite{Robinson2008_CC} but some key points are made to support the process. First, the conceptual model can be thought of as separate from the final computer model that is built and serves as an abstraction of the real system that describes what is going to be modelled and gives an indication of how that might be done. Second, the development of the conceptual model requires input from the modeller and the system owners. Third, a conceptual model does not remain constant through a simulation project but is revisited and adapted as the project continues. Recent research in conceptual modelling is reviewed by \cite{Robinson2020_CC} and has focused on designing modelling frameworks. The work described tends to be related to DES models but the core principles can also apply to the building of both ABS and SD models.

Like any other model, the utility of a simulation is very much dependent on its inputs: the garbage-in-garbage-out principle holds true here. Setting up the probability distributions used for inputs of a stochastic simulation model or parameterisation of a deterministic simulation model is referred to as \textit{input modelling} and is typically achieved through fitting statistical models to available data and eliciting expert opinion. When estimating the inputs for a simulation model from data there is some uncertainty in their true values. With a different set of data, the estimates of the inputs would likely be different. Any uncertainties setting the model inputs will propagate through to the model outputs, resulting in \textit{input uncertainty}. This is influenced both by the accuracy of the estimates of the inputs and the sensitivity of the model output to that particular distribution or parameter. \cite{Corlu2020_CC} provide a review of the current state of the art in input uncertainty research for simulation, while \cite{SongWSC2014_CC} provide practical suggestions on how to estimate the impact of unput uncertainty on the output results.

Often simulation models are used to experiment with different system set-ups. \textit{Simulation optimisation}, sometimes referred to as \textit{optimisation via simulation} describes the use of a simulation model to find the optimal value for one or more decision variables. Typically it is used in the design of stochastic systems that are too complex to be effectively described by an analytical model. Practical examples of problems that can be solved using simulation optimisation include finding the optimal number and configuration of beds in a hospital ward; determining the appropriate number of repair staff on a production line; choosing between a selection of different configurations for a system.

The problem can be represented mathematically as 
\[
\min g(\mathbf{x}), \mbox{  } \mathbf{x} \in \mathbf{\Theta},
\]
where the function we are optimising $g(\mathbf{x})$ is generally the expected value of the output of a stochastic simulation model, $g(\mathbf{x}) = E[Y(\mathbf{x}, \xi)]$; $\mathbf{x}$ is a vector of decision variables; $\mathbf{\Theta}$ denotes the feasible region for $\mathbf{x}$; and $\xi$ indicates the randomness inherent in the model. The majority of research in simulation optimisation aims to improve the efficiency of the optimisation algorithms by reducing the number of simulation replications needed to estimate the optimal values of $\mathbf{x}$. Where a complex and slow-running simulation model is used to generate the $Y(\mathbf{x}, \xi)$ this efficiency is particularly important. \cite{HongNelsonWSC2009_CC}  classify simulation optimisation problems into three categories:
\begin{enumerate}[noitemsep]
\item \textbf{Small number of solutions:} $\mathbf{\Theta}$ contains only a small number of solutions and the decision variable $\mathbf{x}$ might define a particular system configuration. In this case the problem is one of \textit{ranking and selection}.
\item \textbf{Decision variables are continuous:} $\mathbf{\Theta}$ is a convex subset of the set of $d$-dimensional real numbers and the problem is \textit{continuous optimisation via simulation}.
\item \textbf{Decision variables $\mathbf{x}$ are discrete and ordered:} $\mathbf{\Theta}$ is a subset of the set of $d$-dimensional integers and the problem is \textit{discrete optimisation via simulation}.
\end{enumerate}
A set of algorithms exists for solving each category of problem. There has also been significant work on multi-objective optimisation via simulation; for example, see \cite{Hunter2019_CC} for a detailed description of the problem and different solution approaches.

In recent years, sensing has become more widespread and the transfer of data from physical systems to control systems is now happening in close to real time. This has allowed modellers to design simulation models that are automatically fed data from the real system allowing them to either predict the future \citep[e.g., prediction of emergency departments crowding][]{Hoot2008_CC} or to use the simulation models to optimise system parameters as part of a dynamic control process. Such models are sometimes referred to as a \textit{digital twin} or \textit{symbiotic simulation} \cite[see][for a definition]{Onggo_chapter_2019_CC}. \cite{Xu2016_CC} describe how simulation can be incorporated into the Industry 4.0 framework using the example of a semiconductor fab operation. The use of simulation in dynamic control is still in its infancy and requires fast and reliable simulation optimisation algorithms as well as mechanisms for enabling the simulation model to evolve based on new input data. Industry 5.0 is intended to complement Industry 4.0 by putting societal goals at the heart of industrial decision-making\footnote{\url{https://msu.euramet.org/current_calls/documents/EC_Industry5.0.pdf}}. This is a potential growth area for simulation optimisation, particularly multi-objective simulation optimisation \citep[e.g., see ][]{Hunter2019_CC} which enables solutions to be found that support several competing objectives.

Being a huge topic with many different facets, there is no single article that provides an overview of simulation but there are several excellent textbooks covering simulation techniques including \cite{Law2015_CC} and \cite{Banks2004_CC}. The archive of the Winter Simulation Conference\footnote{\url{www.wintersim.org}} is also an extensive resource in understanding the state-of-the-art in the field and its tutorial papers provide more basic tuition in the effective implementation of simulation. Recently, the history track at the conference has also provided an overview of the evolution of the simulation field. 

\subsection[Soft OR and problem structuring methods (Mike~Yearworth \& Leroy~White)]{Soft OR\protect\footnote{Henceforth in this subsection we just refer to PSMs to avoid the obvious dual of ‘Hard OR’ and therefore to manufacture, or at least to continue to propagate, an unhelpful distinction. We retain its use here for signposting.} and problem structuring methods\protect\footnote{This subsection was written by Mike~Yearworth and Leroy~White.}}
\label{sec:Soft_OR_and_problem_structuring_methods}

Problem Structuring Methods (PSMs) are concerned with addressing problem formulation in OR. Following definitions of \cite{Mingers2004-qf_MYLW} and \cite{Rosenhead1996-xa_MYLW} they consist of a set of rigorous but not mathematical methods based on qualitative, diagrammatic modelling. They allow for a range of distinctive stakeholder views of a problem to be expressed, explored and accommodated. They encourage active participation of stakeholders in the modelling process, through facilitated workshops and the cognitive accessibility of the modelling approach. PSMs afford negotiating a joint agenda and ownership of actions. The aim is for exploration, learning and commitment from stakeholders, rather than optimisation or prediction. PSMs thus are vital and constitute a significant developmental direction for OR. See \cite{Smith2019-pw_JL} and \cite{Franco2022-mn_AFRH} for recent reviews.

Understanding the contribution of PSMs to OR requires some knowledge of their evolution. We characterise the development of the field into three phases: (\textit{i}) origins, (\textit{ii}) growth (only noted here through the increased publication rate of PSM related articles), and (\textit{iii}) maturity, covering the diffusion of PSMs to fields outside of OR, and re-integration of problem structuring into mainstream OR. Looking at the last first, we see PSMs at an important turning point, as recent work by \cite{Dyson2021-sb_MYLW} specifically identify the centrality of problem structuring in the origins of OR and lead us towards the important question of why PSMs are not seen as an essential element of every OR engagement. 

The origins of PSMs as a set of formal methods in OR arose as a consequence of the broad critique of the process of OR in the 1970-80s; the label itself was pioneered by \cite{Woolley1981-go_MYLW}. Ackoff became a trenchant critic of the sole pursuit of objectivity and optimisation in OR describing it as an “\textit{opt-out}” \citep{Ackoff1977-kh_MYLW} and set out an agenda for reconceptualising OR practice \citep{Ackoff1979-zx_GL}. \cite{Dando1981-hv_MYLW} described the situation as a “\textit{Kuhnian crisis in management science}”. In Rosenhead’s “\textit{Rational Analysis for a Problematic World: Problem Structuring Methods for Complexity, Uncertainty and Conflict}” their prescription in OR engagements was associated with dealing with problem contexts identified variously as wicked, messy, or swampy \citep[][pp. 3-11]{Rosenhead1989-nu_MYLW}. These can be summarised as problem situations that are not well-defined, involving many interested parties with different perspectives (worldviews), where there is difficulty agreeing objectives and the meaning of success, and that require creating agreement amongst the parties involved for actions to be taken. The implication of the dichotomous framing of problem contexts – i.e., wicked/tame, swamp/high ground, hard/soft, tactical/strategic – was to set out a clear critique for the whole field of OR and to suggest that to retain its relevance in dealing with the messiness of real-world problems PSMs were required to bring some rigour – and indeed a reminder of the importance – to the process of problem formulation. Importantly, the pioneers of PSMs were concerned that traditional (i.e., ‘Hard OR’) processes for problem formulation were practitioner-free \citep{Checkland1983-st_MYLW,Rosenhead1986-ti_MYLW}. 

The main PSMs set out by \cite{Rosenhead1989-nu_MYLW} were Strategic Options Development and Analysis \citep[SODA;][]{Eden1989-rz_MYLW}, arising from cognitive mapping; Soft Systems Methodology \citep[SSM;][]{Checkland1989-fc_MYLW}, emerging from the failure of Hard Systems Thinking approaches (e.g., Systems Engineering, RAND-style Systems Analysis) when applied to messy problems; and the Strategic Choice Approach \citep[SCA;][]{Friend1989-yj_MYLW}, arising from planning. In addition, Robustness Analysis, Metagame Analysis, and Hypergame Analysis were also included. However, setting the boundary of PSMs has always been an open question \citep{mingers_soft_2011_JHLS}. In the main, the core methods (SODA, SSM, SCA) are seen as exemplary and provide sufficient coherence for OR scholars and practitioners to be provided with a clear view of a common theme. 

The methodology of PSMs has long been associated with contextual matters through Systems Thinking \citep[\S\ref{sec:Systems_thinking}][]{Checkland1983-st_MYLW,Mingers2010-ei_MYLW}, Community OR \citep{Johnson2018-sj,Jones1981-xt_MYLW,Parry1991-px_MYLW}, and large group processes \citep{Shaw2004-gw_MYLW,White2002-ck_MYLW}. Methodological individualism has been addressed through Behavioural OR \citep[\S\ref{sec:Behavioural_OR};][]{Franco2016-qk_MYLW,White2016-jg_MYLW}. There has also been long-standing relevance to Multi-Criteria Decision Analysis \citep[MCDA;][]{Marttunen2017-yk_JL}, value-focused thinking \citep{Keeney1996-yo}, policy analysis \citep{Eden2004-ws_MYLW}, and strategy making \citep{Ackermann2011-as_MYLW,Dyson2000-gz_MYLW}. Bridging between PSMs and other techniques in OR has been developed as multimethodology \citep{Mingers1997-nd_MYLW}; for example, integration with Simulation \citep[\S\ref{sec:Simulation};][]{Kotiadis2006-gv_MYLW,Tako2015-wi_MYLW}. Some approaches to using the Viable Systems Model (VSM), System Dynamics (\S\ref{sec:Systems_dynamics}), and Decision Analysis (\S\ref{sec:Decision_analysis}) would also be considered as PSMs too \citep[pp. 267-288]{Rosenhead2001-ai_JL}, e.g., VSM \citep{Lowe2020-ai_MYLW} and System Dynamics \citep{Lane1998-ff_MYLW}.  We also see developments in Group Model Building (GMB) from the System Dynamics community making a significant contribution to PSMs \citep{Andersen2007-pg}. In their growth and mature phase, applications of SSM, SCA, and SODA have extended the reach of PSMs into, e.g., project management \citep{Franco2004-qi_MYLW} and environment, sustainability, and energy policy, e.g., SCA \citep{Fregonara2013-yd_MYLW}, SODA \citep{Hjortso2004-aa_MYLW}, SSM \citep{Pahl-Wostl2007-br_MYLW}, and the Drivers, Pressures, State, Impact and Response framework \citep[DPSIR][]{Bell2012-gn_MYLW}.

The state of the art and research agenda for PSMs has been the subject of periodic reflection e.g., reviews by \cite{Rosenhead1996-xa_MYLW} and \cite{Mingers2004-qf_MYLW}. A Special Issue of JORS in 2006 questioned where PSMs were heading \citep{Rosenhead2006-ih_MYLW} – variously argued as a “grassroots revolution” \citep{Westcombe2006-vm_MYLW}, an appeal to common principles \citep{Eden2006-as_MYLW}, and observations that “form and content have evolved through interaction between the ideas and their practical use” \citep{Checkland2006-qc_MYLW}. A more recent viewpoint debate “whither PSMs” again questioned their direction of travel \cite{Harwood2019-kp_MYLW,Lowe2019-gm_MYLW}. 

The qualitative nature of PSM methods raises questions about evaluating both effectiveness and value. \cite{Midgley2013-ec_MYLW}, \cite{White2006-wo_MYLW}, and \cite{Franco2022-mn_AFRH} have addressed the question of effectiveness, and whilst White goes some way towards defining the value of PSMs it is important to note the reservations expressed by \citeauthor{Checkland1990-ip_MYLW} (\citeyear{Checkland1990-ip_MYLW}, p. 299) – that measuring value in a unique problem context, the ‘messy’ realm of PSMs, is unlikely to be meaningful. \cite{Tully2019-dd_MYLW} examine this conundrum in depth from the perspective of a consulting business and make some practical suggestions for its resolution.

Theory provides an important basis for PSM development. The range of PSM practice reported has been explained by the constitutive rules that underpin specific methods. First articulated by \citeauthor{Checkland1981-op_MYLW} (\citeyear{Checkland1981-op_MYLW}, pp. 252-254), constitutive rules are generative of method rather than prescriptive and account for the range of practices that emerge, even when adopting a specific methodology such as SSM i.e., adaptation is always necessary to address the specifics of the application context. The idea was developed further by \citeauthor{Jackson2003-ln_MYLW} (\citeyear{Jackson2003-ln_MYLW}, pp. 307-311) and then by \cite{Yearworth2014-xf_MYLW} into a generic constitutive definition for PSMs. Another significant development has been a focus on PSMs as practice and drawing on practice theories. These theories provide a means of understanding OR practices by “zooming-in” to the detailed, fine-grained, scale and by “zooming-out”, looking at how specific practices affect the broader context \citep{Ormerod2022-qp_MYLW}. Together these theoretical strands provide sufficient basis on the one hand, to liberate PSMs from the pigeon-hole of the dichotomous framing of their origins, and on the other, to address OR practice as a whole and to see problem structuring as a normal, indeed necessary, part of every OR intervention. For instance, Actor Network Theory (ANT) provides a lens to look at the processes of problematisation (i.e., problem formulation) in OR practice \citep{White2009-di_MYLW}. \cite{Callon1981-tn_MYLW} draws specific attention to the “abundance of problematisations” facing expert practitioners – that there is no single specific way of problematising. Strands of ANT focus on the performative idiom; \citep{Ormerod2014-ax_MYLW} draws attention to the “mangle of practice” and the need for more informative case studies of OR practice. Other theoretical underpinnings are relevant to PSM developments e.g., PSMs as technology \citep{Keys1998-bo_MYLW}, Critical Realism \citep{Mingers2000-oa_MYLW}, Activity Theory \citep{White2016-sv_MYLW}, and the specific role of models as boundary objects \citep{Alberto_Franco2013-yw_MYLW} in facilitated workshops \citep{Franco2010-pf_MYLW}.

From a practitioner point of view, the recent report “Reinvigorating Soft OR for Practitioners” by \cite{Ranyard2021-kw_MYLW} to the Heads of OR and Analytics Forum (HORAF) and the inclusion of the knowledge requirement “How to select and apply, a range of problem structuring methods to understand complex problems” in the Operational Research Specialist Degree Apprenticeship specification by the \cite{Institute_for_Apprenticeships_Technical_Education_undated-av_MYLW} are a welcome development. 

In conclusion, for PSMs we see a return to the roots of OR as a discipline – encompassing both practice and academic scholarship – through the centrality of problem formulation to the process of OR \citep[][p. 13]{Churchman1957-ax_MYLW} and a reminder that the seeds of problem structuring can be seen in the work of the ‘founders’ of OR as uncovered by \cite{Dyson2021-sb_MYLW}. We have identified a number of research gaps that indicate future research directions for the development of PSMs. In the area of the impact of new digital technologies, \cite{Yearworth2019-qi_MYLW} propose greater use of online “same time/different places” problem structuring workshops in order to meet the requirements for fast meeting setup times, reducing carbon emissions, enabling the scale-up to large group participation, and supporting new post-pandemic working patterns. The need to address complex policy issues in the context of wicked problems is highlighted by \cite{Howick2017-of_MYLW} and \cite{Ferretti2019-qt_MYLW}, who argue for a re-invigorated engagement for PSM practice in policy analysis. Finally, \cite{Ormerod2014-kb_MYLW}, \cite{Ranyard2015-xx_MYLW} and \cite{Ormerod2023-qp} remind us that we need to see a renewed practitioner-led orientation for OR scholarship that grounds future development in solid empirical work.

\subsection[Stochastic models (Haitao~Li)]{Stochastic models\protect\footnote{This subsection was written by Haitao~Li.}}
\label{sec:Stochastic_models}

Many decision problems involve uncertainty, e.g., network design with disruption risk, portfolio selection with uncertain return, resource planning with unknown resource availability, crop planting with uncertain yield, inventory control with varying demand, and project scheduling with random task duration, etc. While the effect of uncertain parameters on the optimal solution and objective value can be studied through the well-known sensitivity analysis, or what-if analysis, in a deterministic optimisation approach, such post-optimality analysis does not prescribe solutions under uncertainty \textit{a priori}. This subsection provides an overview of a suite of optimisation models and methods that seek to obtain optimal or near-optimal solutions for the class of decision problems where some parameters are \textit{stochastic} with known probability distribution\footnote{Decision problems with \textit{incomplete information} about random parameter’s probability distribution go beyond the scope of this review.}.

Originated in the seminal work of \cite{Dantzig1955-um_HL}, stochastic programming is one of the earliest and most prominent approaches to deal with optimisation problems with stochastic parameters. The basic stochastic programming model has a two-stage framework, called two-stage stochastic programming with recourse \citep[2S-SPR;][]{Birge2011-ut_HL}. In the first stage, the \textit{here-and-now} decision is made. Then in the second stage, the \textit{recourse} decision is prescribed for each scenario of stochastic parameters after their realisation. The objective function minimises the total cost as the summation of the first-stage cost and the expected second-stage cost given the probability distribution of the stochastic parameters. It is often insightful to compute the value of stochastic solution \citep[VSS;][]{Birge1982-ix_HL} as the difference between the optimal objective function value of the deterministic counterpart (by substituting the stochastic parameters with their point estimates) and that of the stochastic programming model. We refer to \cite{Birge2011-ut_HL} and \cite{Shapiro2021-wq_HL} for a systematic and updated treatment on the modelling and theory of stochastic programming, and to \cite{Wallace2005-lf_HL} for a collection of applications of stochastic programming. Recent applications include disaster relief management \citep{Grass2016-rg_HL}, transit network design \citep{Zhao2017-jt_HL}, portfolio selection \citep{Masmoudi2018-ya_HL}, treatment plant placement in drinking water systems \citep{Schwetschenau_S_E2019-ws_HL}, process systems \citep{Li2021-qy_HL}, multi-product aggregate planning \citep{Gomez-Rocha2022-kp_HL}, and resource allocation for infrastructure planning \citep{Zhang2022-su_HL}, among others. 

A stochastic programming model may also include a constraint that is satisfied with a probability. This model is is known as the chance constrained programming model introduced by \cite{Charnes1958-nv_HL}. The probabilistic constraint can often be transformed into a deterministic constraint given the known probability distribution of a stochastic parameter. Detailed coverage on the chance constrained programming models and methods is available in \cite{Prekopa2013-du_HL}. Notable applications include farm management \citep{Moghaddam2011-yh_HL}, broadband wireless network design \citep{Clasen2014-ki_HL}, supply chain network design \citep{Shaw2016-sh_HL}, equity trading server allocation \citep{Sun2019-lq_HL}, and power system planning \citep{Geng2019-ph_HL}, among others.  

A stochastic programming model can be formulated as a deterministic mathematical programming model by associating its decision variables with the scenarios of stochastic parameters, an approach often referred to as deterministic equivalent formulation (DEF). Solving a stochastic programming model via its DEF can be computationally challenging as the size of DEF grows rapidly with the number of scenarios of stochastic parameters. Thus custom designed algorithms are often needed to obtain quality solutions for medium- and large-size stochastic programming models. Assuming the set of random parameters has finite support, the DEF of a 2S-SPR has a block structure with L-shape, which motivates the well-known L-shape method \citep{Van_Slyke1969-qz_HL} based on Benders decomposition \citep{Benders1962-sf_HL}. For problems with a large number of random scenarios, it can be computationally intractable for the exact decomposition method to obtain an optimal solution. One may resort to various sampling-based methods to obtain approximate solutions. The stochastic decomposition method proposed by \cite{Dantzig1991-yh_HL} and \cite{Higle1991-lz_HL} employs Monte Carlo simulation and importance sampling to compute sampling cuts instead of generating the exact cuts in the L-shape method. The other successful approach is sample average approximation \citep[SAA;][]{Kleywegt2002-mi_HL,Shapiro2003-sm_HL}, which approximates the second-stage objective function by an expected value function corresponding to a set of scenarios of the random parameters. Numerical experiments and results of the SAA method on various benchmark instances are available in \cite{Linderoth2006-ff_HL}. 

Another well-known stochastic modelling and solution approach is the integrated simulation-optimisation \citep{Fu2005-qb_HL}, especially used for solving problems involving discrete decision variables, widely encountered in applications in management science, operations and supply chain management. A typical integrated simulation-optimisation framework consists of two inter-related components: \textit{search} and \textit{sampling}. The search component deals with the solution space, often combinatorial in nature with large size, for which various metaheuristics \citep{Glover2003-cz_HL} can be applied. These include local search based metaheuristics such as simulated annealing \citep{kirkpatrick1983optimization_COIT}, tabu search \citep{Glover1997-bi_HL} and scatter search \citep{Glover1998-hd_HL}, as well as population-based metaheuristics, e.g., genetic algorithm \citep{Holland1975-qj_HL}. The sampling component evaluates a candidate solution via simulation, e.g., Monte Carlo or discrete event simulation. Thus an integrated simulation-optimisation approach can be viewed as an augmented deterministic metaheuristic that employs simulation to evaluate/estimate solutions in an uncertain environment. Recent applications include maritime logistics \citep{Zhou2021-vs_HL}, pooled ride-hailing operators \citep{Bischoff2018-fr_HL}, and staffing for service operations \citep{Solomon2022-ou_HL}. 

Many real-world applications need decisions to be made sequentially under uncertainty, e.g., production planning, inventory control, resource allocation, and project scheduling, etc. One approach to this type of applications is the multi-stage stochastic programming \citep{Birge2011-ut_HL}, which is a generalisation of the 2S-SPR. In a typical multi-stage stochastic programming framework, a decision is made in a stage, based on the observed realisation of random parameters and the decisions made in the previous stage, to minimise the total expected future cost. The random parameters are assumed to evolve according to some known stochastic process. We refer to \cite{Zhang2023-xu_HL} for an updated and comprehensive treatment on various stochastic processes.  A nested decomposition as a generalisation of the L-shape method can be applied to obtain exact solutions to a multi-stage stochastic programming model \citep{Birge1985-hx_HL}. Conceptually, it applies Benders decomposition or the L-shape method recursively to a series of nested two-stage subproblems. Although theoretically sound, it can be computationally challenging to handle reasonably large instances as the number of scenarios grows exponentially with the number of stages and random parameters. Thus one often resorts to various approximation algorithms for obtaining quality solutions efficiently, including value function approximation, constraint relaxation, scenario reduction, and Monte Carlo methods, among others \citep{Birge2011-ut_HL}. 

An alternative approach to sequential decision making under uncertainty is stochastic dynamic programming \citep{Ross1983-yc_HL} or as a Markov decision processes \citep[MDP;][]{Puterman2005-lr_HL}. See \S\ref{sec:Dynamic_programming} for more details.

There are two general approaches for solving an MDP model: open-loop and closed-loop \citep{bertsekas2012dynamic_DLLD}. An open-loop approach obtains a solution to all the decision variables upfront, which is \textit{static} in nature without updating during execution of the sequential decision-making process. The integrated simulation-optimisation approach introduced above is a successful way to obtain an open-loop policy, e.g., using genetic algorithms \citep{Ballestin2007-ma_HL}, tabu search \citep{Tsai1998-tz_HL}, or the greedy randomised adaptive search procedure \citep[GRASP;][]{Ballestin2009-rp_HL} with simulation for the Stochastic Resource-Constrained Project Scheduling Problem (SRCPSP). 

Instead of optimising the entire problem upfront, a closed-loop approach seeks to obtain an optimal decision rule (\textit{policy}) to map the state at a stage to a decision, given the information available to the decision-maker at the current stage. A closed-loop policy is \textit{dynamic} and \textit{adaptive} in nature, thus is more flexible than an open-loop policy \citep{Dreyfus1977-sl_HL,bertsekas2012dynamic_DLLD}. Although theoretically attractive, to obtain an optimal closed-loop policy through the well-known Bellman equation in recursive way \citep{Bellman1957-ue_HL} is computationally intractable due to the curse-of-dimensionalities of MDP in state space, solution space of decision variables, and sample space of random parameters. 

Recent advances advocate the design and implementation of approximate dynamic programming (ADP) for solving large-scale MDPs. ADP has its roots in neural dynamic programming \citep[NDP;][]{Bertsekas1996-qb_HL} and reinforcement learning \citep[RL;][]{sutton2018reinforcement_LCAL}. Its key idea is to replace the exact cost-to-go function with some sort of approximation. We refer to \cite{Si2004-mp_HL} and \cite{Powell2011-km_HL} for comprehensive coverage on ADP and its applications. There are two approximation paradigms for the design of an ADP algorithm. The value function approximation approach works directly on the cost-to-go function to replace it with an alternative functional form that is computationally tractable. Using sample path simulation, a forward iteration procedure can be implemented to solve a deterministic sub-problem with the approximated objective function subject to the set of constraints corresponding to the state of the current stage \citep{Powell2011-km_HL}. This approach has been successfully applied to the multicommodity network flow problem \citep{Topaloglu2006-bn_HL}, dynamic fleet management \citep{Simao2009-ag_HL}, and dynamic resource planning \citep{Solomon2019-cu_HL}. 

While the value function approximation approach works well for problems with structures amenable to efficient mathematical programming methods such as linear programming or network optimisation, many combinatorial optimisation problems are $\mathcal{NP}$-hard themselves, and can be computationally demanding for mathematical programming to handle. We refer to \S\ref{sec:Computational_complexity} for a review on the topic of computational complexity and $\mathcal{NP}$-hardness. This calls for an alternative approximation paradigm known as the rollout policy \citep{Bertsekas1997-qm_HL}. A rollout policy estimates the cost-to-go function using some heuristic via simulation, which can be either an efficient problem-specific heuristic or a custom-designed metaheuristic for the problem at hand. It can be viewed as a look-ahead policy that estimates the cost of a decision-state pair under uncertainty about the future, which can be in contrast to the lookup table approach in RL \citep{sutton2018reinforcement_LCAL} where the cost of a decision-state pair is learned through simulation in a look-back fashion. A hybrid look-ahead and look-back ADP algorithm has been developed by \cite{Li2015-ey_HL} to take advantage of the complementary strengths of the pure rollout approach and the lookup table approach alone. Successful applications of rollout policy have been reported for stochastic vehicle routing \citep{Secomandi2001-lw_HL,Goodson2013-fd_HL}, SRCPSP with stochastic activity durations \citep{Li2015-ey_HL}, RCPSP with multiple-overlapping modes \citep{Chu2019-bw_HL}, ride-hailing system planning \citep{Al-Kanj2020-rn_HL}, and attended home delivery \citep{Koch2020-bf_HL}. 

All the aforementioned models and methods assume that the probability distribution of random parameters is known or can be properly estimated. This assumption may not hold in some situations where there is lack of knowledge about the uncertain parameters, or error in measurement or implementation. Optimisation with uncertain parameters without probability distribution calls for the robust optimisation (RO) approach. Although the origin of RO can be dated back to the 1970s \citep{Soyster1973-hb_HL}, RO has been growing as an active research field since the last two decades. In an RO model, one assumes that uncertain parameters are within a user-specified uncertainty set. A robust feasible solution satisfies the set of uncertain constraints for all realisations of the uncertain parameters in the uncertainty set. One main technique to solve an RO model is the robust reformulation approach to obtain a computationally tractable robust counterpart (RC) with a finite number of deterministic constraints \citep{Bertsimas2011-fv_HL}. When choosing the type of uncertainty set for the model, one often needs to trade-off between robustness against realisations of the uncertain parameters and computational tractability, i.e., size of the uncertainty set \citep{Gorissen2015-zv_HL}. We refer to \cite{Ben-Tal2002-as_HL} and \cite{Ben-Tal2009-lm_HL} for systematic treatment on robust optimisation. RO has been applied in various fields including finance \citep{Georgantas2021-nj_HL}, energy and utility \citep{Sun2021-pm_HL}, supply chain \citep{Ben-Tal2005-bz_HL,Pishvaee2011-hu_HL}, healthcare \citep{Meng2015-dg_HL}, and marketing \citep{Wang2012-qi_HL}. 

\subsection[System dynamics (Martin~Kunc \& John~D.W.~Morecroft)]{System dynamics\protect\footnote{This subsection was written by Martin~Kunc and John~D.W.~Morecroft.}}
\label{sec:Systems_dynamics}

System Dynamics (SD), founded by \cite{Forrester1961-px_MCJM}, is a ``rigorous method for qualitative description, exploration and analysis of complex systems in terms of their processes, information, organisational boundaries and strategies; which facilitates quantitative simulation modelling and analysis for the design of system structure and control'' \citep{Wolstenholme1990-df_MCJM}. SD modelling involves (as extracted from the System Dynamics Society website - \href{www.systemdynamics.org}{www.systemdynamics.org}):
\begin{itemize}[noitemsep]
   \item ``Defining problems dynamically, in terms of graphs over time.
   \item Striving for an endogenous, behavioural view of the significant dynamics of a system, a focus inward on the characteristics of a system that themselves generate or exacerbate the perceived problem.
   \item Thinking of all concepts in the real system as continuous quantities interconnected in loops of information feedback and circular causality.
   \item Identifying independent stocks or accumulations (levels) in the system and their inflows and outflows (rates).
   \item Formulating a behavioural model capable of reproducing, by itself, the dynamic problem of concern. The model is usually a computer simulation model expressed in nonlinear equations or can be left without quantities as a diagram capturing the stock-and-flow/causal feedback structure of the system.
   \item Deriving understandings and applicable policy insights from the resulting model.
   \item Implementing changes resulting from model-based understandings and insights.''
\end{itemize}

SD can be employed for both qualitative and quantitative modelling. On the one hand, tools and methods employed for qualitative SD modelling are also considered Soft Operational Research or Problem Structuring methods. On the other hand, quantitative SD modelling shares many aspects of traditional simulation methods or Hard Operational Research. Using SD quantitatively implies the development of a 5-steps process \citep{Sterman2000_SMD} that starts with a dynamic hypothesis about a structure responsible for the performance over time observed in the system followed by the model formulation, testing and experimentation. The next section discusses both approaches in detail.

One interesting characteristic of SD models is the spectrum of model fidelity they cover \citep{Morecroft2012-zf_MCJM}. Figure 1 illustrates a spectrum of model fidelity and realism. Models range in size from large and detailed to small and metaphorical.  On the left-hand side are analogue, high-fidelity models epitomised by aircraft flight simulators used to train pilots and to rehearse crisis scenarios.  They are constructed with realistic detail and accurate scaling to provide a vivid and lifelike experience of flying the aircraft they represent. People typically expect business and social models to be similarly realistic; the more realistic the better. Realistic high-fidelity models are discussed later in this subsection. But very often smaller models are extremely useful, particularly when their purpose is to aid communication and to build shared understanding of contentious problem situations in business and society. As Figure 1 suggests, the spectrum of useful models can include illustrative models (of limited detail yet plausible scaling) or even tiny metaphorical models (of minimal detail yet transferable insight).

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=10cm]{figures/MCJM_fig1.pdf} 
\caption{Modelling and Realism: A Spectrum of Model Fidelity. Adapted from \cite{Morecroft2015-vr_MCJM}, Chapter 10.}
\label{fig:MCJM_fig1}
\end{center} 
\end{figure}

At the other end of the spectrum, on the far right, is a low fidelity Romeo and Juliet simulator \citep{Morecroft2010-ui_MCJM}. This particular simulation model contains just four main concepts: Romeo's love for Juliet, Juliet’s love for Romeo and the corresponding rates of change of their love.  It is used as a metaphorical model or transitional object to help undergraduates and high school students to better understand something complex and abstract -- differential equations or even Shakespeare's play. Clearly, a simulator cannot possibly replicate Shakespeare’s play, but it can encourage students to study the play more closely than they otherwise would. It is this metaphorical property of small models -- to attract people's attention, to encourage them to reflect and debate -- that often underpins their value to model users. Sometimes metaphorical models enable client engagement. One could say that `small is beautiful' in the world of policy and strategy modelling. Over the years SD studies have included models and simulators that cover the entire range. See \cite{Kunc2017-pw_MCJM} for a sample of papers published in the Journal of the Operational Research Society and \cite{Kunc2018-uj_MCJM} for a study on published SD models. 

\subsubsection*{Qualitative System Dynamics}
The main objective of qualitative SD involves discovering the structure, in terms of feedback loops, driving the dynamic behaviour of key variables, usually with clients through facilitated workshops. The main tool employed in qualitative SD modelling is causal loop diagram (CLD). The steps for developing a CLD are \citep[based on][]{Kunc2017-bq_MCJM}:
\begin{enumerate}[noitemsep]
   \item Understanding the direction of causality between two variables. Interestingly, it is a source of important discussion among participants in facilitated qualitative SD modelling. 
   \item Defining polarities involves identifying the relationships between two variables as either positive (same sense of direction of change) or negative (opposite sense of direction of change).
   \item Identifying feedback processes responsible for the dynamic behaviour of variables. They originate from connecting variables in a circular chain of cause-and-effect. There are two types of feedback process: reinforcing and balancing.
\end{enumerate}

Finally, table \ref{tab:MVJM_t1} shows description of the modelling process

\begin{table}[h]
\centering
\caption{Qualitative  SD modelling process based on \cite{Kunc2017-bq_MCJM}.}
\label{tab:MVJM_t1}
\medskip
\begin{tabular}{ p{2cm} p{13cm} }
 \hline 
\textbf{Modelling process}	& \textbf{Qualitative SD} \\
 \hline
\textit{Objective} & Understand the feedback structure of the system. \\
\textit{Inputs} &	Text data obtained through facilitated face-to-face meetings, interview or the interpretation of causal mechanism in reports and from theories. \\
\textit{Process}	& The modelling process implies the construction of CLD to represent individual and/or group-level interpretations of causal links. Facilitation processes are critical to uncover the causal links. \\
\textit{Outputs}	& There are three main outputs: learning about the structure of the system, changes in participants perspectives, and agreement on future policies. \\
 \hline
\end{tabular}
\end{table}

\subsubsection*{Quantitative System Dynamics}
Quantitative System Dynamics characterises the system behaviour using a set of accumulation processes linked through feedback processes. The structure of the model is represented through stocks and flows diagrams. The numerical results, which are deterministic and continuous, aim to replicate past system behaviour through calibration and testing processes before the model is used to test interventions in the system. Table \ref{tab:MVJM_t2} presents a summary of the modelling process.

\begin{table}[h]
\centering
\caption{Quantitative SD modelling process based on \cite{Kunc2017-bq_MCJM}.}
\label{tab:MVJM_t2}
\medskip
\begin{tabular}{ p{2cm} p{13cm} }
 \hline 
\textbf{Modelling process}	& \textbf{Quantitative SD} \\
 \hline
\textit{Objective}	& Test a hypothesis about the structure driving the reference mode of a variable.  \\
\textit{Inputs}	& Text data obtained through facilitated face-to-face meetings, interview or the interpretation of causal mechanism in reports and from theories to determine the structure.
Numerical data for the model can come from three sources: judgement from experts or managers, numerical data sets and facilitation processes for nonlinear functions. \\
\textit{Process}	& After defining the boundary of the model, a stock and flow diagram is developed to represent the structure of the system. Equations are formulated for each variable and parameters entered. Testing of the structure and outputs are performed to confirm the model structure replicates the behaviour observed in the key variables. \\
\textit{Outputs}	& There are three outputs: time series showing performance over time; performance over time of policies or interventions in the system; and learning about the dynamic behaviour of the system.  \\
 \hline
\end{tabular}
\end{table}

\subsubsection*{Application areas}
\begin{enumerate}[noitemsep]
\item[a)] \textit{Behavioural modelling}: There are three main areas of application. Firstly, research in decision making under dynamic complexity focused on identifying and documenting systematic misperceptions of feedback in decision making processes across multiple industries and environmental conditions using SD models \citep{Gary2008-uu_MCJM,Atkinson2016-wl_MCJM}. Secondly, experimental studies explore decision making and performance using management flight simulators or microworlds based on SD models \citep{Gary2008-uu_MCJM,Sterman2014-du_MCJM}. Thirdly, individual experimental work using SD models examines how differences in mental model accuracy and decision rules lead to differences in the performance \citep{Torres2017-vm_MCJM}. Recently, scholars have advocated for a practice of behavioural system dynamics \citep{Lane2022-md_MCJM}.
\item[b)] \textit{Group model building}: There is a wide body of research on  model conceptualisation in groups; see \cite{Rouwette2010-sc_MCJM}, where the outcome is either qualitative or quantitative SD models. Researchers have assessed the effects on communication, learning, consensus and commitment in the behaviour of groups, as well as measuring the changes in mental models and understanding the impact of group model building in terms of persuasion and attitudes \citep{Rouwette2016-su_MCJM}. 
\item[c)] \textit{Multi-scale high-fidelity systems modelling}: SD modellers are embracing new approaches to improve the scale and fidelity of their models to move from aggregate conceptual models into realistic detailed models supported with specific data. There are multiple considerations to develop high-fidelity models \citep{Sterman2018-cg_MCJM}. Firstly, these models represent heterogeneous actors in the system, which involves disaggregating single stocks into multiple stocks reflecting their differences across dimensions (e.g., age). While this solution increases granularity in the model, it also implies increasing computational burden and long simulation times, which can limit the ability to perform sensitivity analysis, change structure and test interventions by stakeholders. Secondly, high-fidelity models reflect business and social processes in detail fitting the data. Therefore, models may move from the traditional representation of time, continuous, to discrete; from state continuous variables or discrete variables; and include uncertainty using stochastic variables. Consequently, SD models can employ ordinary differential equations, stochastic differential equations, discrete event simulations, agent-based models and dynamic network models. Thirdly, multiscale modelling involves integrating models working at different temporal scales (e.g., fast and slow dynamics). Fourthly, since SD models tend to employ qualitative data (e.g., decision making rules), modellers should identify and mitigate biases in sample selection and data elicitation to collect robust qualitative data. Fifthly, quantitative data should have a clear purpose in terms of the model, so specific data related to the problem the model is solving has to be collected rather than accepting only available numerical data. Sixthly, high-fidelity models should consider parameter estimation and model analysis extremely necessary to replicate historical data.
\item[d)] \textit{SD and Artificial Intelligence (AI)}: Since abundant information is available in different forms (images, text, and numbers), there is a need for technologies that not only predict data but also learn from the environment such as AI \citep{Baryannis2019-vp_MCJM}. AI can be used for cognitive thinking, learning from behaviour, recalling, and drawing inferences \citep{Min2010-tg_MCJM}. SD models use inferences of the casual structure in system to predict future trends or test interventions (e.g., new policies). SD can combine with AI to generate AI-driven simulations based on machine-learned and mathematical rules to make more accurate models \citep{Li2022_MC}. Another use is the employment of AI methods to interpret the results of simulations, especially feedback loop dominance in complex SD models. 
\end{enumerate}

\subsubsection*{Future of System Dynamics research} 
The future of SD may be driven by developments in several on different areas. Firstly, SD can be used as a problem structuring or systems thinking method (in terms of qualitative SD) so improvements in terms of facilitation will be critical. Secondly, when SD is an aggregated simple model that helps modeller and client to learn about dynamic complexity, improvements in terms of impact on behaviour from using the model \citep{Kunc2020-fn_MCJM} will be expected. Thirdly, SD can be high fidelity systems models using all the toolkit available in terms of simulation methods and AI. The next section on systems thinking (\S\ref{sec:Systems_thinking}) looks at other systems methodologies for different purposes.

\subsection[Systems thinking (Gerald~Midgley)]{Systems thinking\protect\footnote{This subsection was written by Gerald~Midgley.}}
\label{sec:Systems_thinking}

`Systems thinking' involves us viewing complex problem situations and possible human responses to them using systems theories, methodologies, methods and concepts. We will start this section by presenting a contemporary understanding of what a ‘system’ is. An explanation of how systems thinkers use this understanding to support action to address or prevent complex problems will then follow. Subsequently, we will review 70$+$ years of systems thinking to show how we got to this contemporary understanding via three ‘waves’ of methodological development. 

\subsubsection*{What is a System?} 
A system is made up of a set of \textit{interrelated parts}, with \textit{emergent properties} \citep{Emmeche1997-cj_GM}. An emergent property is a feature that cannot be traced back to any single part of the system, so can only be understood as arising from the whole (all the parts and interrelations together). Systems have \textit{boundaries}: we can say what is inside and outside the system \citep{Ulrich1994-fw_GM}, although some interactions may cross these boundaries \citep{Von_Bertalanffy1968-yb_GM}. However, systems are always seen from the \textit{perspective} of an observer/participant \citep{Churchman1979_GM,Cabrera2015-rd_GM}. Indeed, there can be multiple perspectives on the boundaries of the system, what interrelationships (within the system and with its environment) need to be considered, what emergent properties matter, and what other perspectives should be heard. 

\subsubsection*{What is Systems Thinking?} 
Based on the above understanding of systems, we can now explain \textit{systems thinking}. It is taking a systems approach to \textit{rethinking the taken-for-granted assumptions} of decision makers, OR practitioners and stakeholders on what perspectives, boundaries, interrelationships and/or emergent properties matter in a given situation, and what the implications are for action. Many systems thinkers use the adjective ‘critical-systemic’: thinking is \textit{systemic} because of the use of the above systems concepts, but it is also \textit{critical} because it involves rethinking options for understanding and action in relation to the deployment of these concepts \citep{Ulrich1994-fw_GM,Gregory2020-zf_JL}.

\subsubsection*{Three Waves of Systems Methodology} 
Since the 1950s, there have been three ‘waves’ (or successive paradigms) of systems methodology, although the second and third waves didn't fully replace their predecessors: some groups of practitioners stuck with earlier ideas. The first wave was typified by early work in systems engineering \citep[e.g.,][]{Hall1962-by_GM,Jenkins1969-vq_GM}, systems analysis \citep[e.g.,][]{Miser1985-yx_GM,Miser1988-ev_GM}, system dynamics \citep[e.g.,][see also \S\ref{sec:Systems_dynamics}]{Forrester1961-px_MCJM}, and organisational cybernetics \citep[e.g.,][]{Beer1966-aw_GM,Beer1981-fe_GM}. The first wave emphasised quantitative computer modelling by experts serving clients. These experts explained emergent properties of systems by understanding interrelatedness, and then deployed these explanations to make recommendations to clients on the possible consequences of strategic and tactical decisions.

In terms of the definition of a system presented earlier, systems were seen as real-world entities; the emphases were on interrelationships and emergent properties; boundaries were relevant because modelling had to account for all the parts and interrelationships in a system that are needed to understand given emergent properties; but multiple perspectives were often bypassed, rather than listened to, in the interests of objectivity or impartiality.

Almost all the first-wave methodologies regarded models as representations of reality, with people often being viewed as deterministic parts of systems being modelled rather than self-conscious actors who can change their purposes \citep{Ackoff1979-zx_GL}. Indeed, stakeholder purposes can differ significantly from those of the systems modeller and his/her client, and ignoring this can create conflict that undermines an OR project \citep{Checkland1985-mi_GM}. Some critics \citep[e.g.,][]{Hoos1972-cq_GM,Lee1973-us_GM} argued that massive investments in large-scale modelling were wasted because systems practitioners tried to be comprehensive (e.g., modelling all interacting problems at the city scale), yet they didn’t sufficiently account for the actual questions that decision makers wanted to address –- more modest modelling for specific purposes would have been better. Worse, the typical response to project failures was to say that the models were \textit{not comprehensive enough}, so the ideal of comprehensiveness remained unquestioned \citep{Lilienfeld1978-kd_GM}. 

These criticisms led to a second wave of systems methodologies focused on stakeholder participation, qualitative modelling and dialogue for collaborative learning. The idea of producing expert recommendations was replaced by a facilitation role for the practitioner, so multiple stakeholders could develop and integrate their ideas into proposals for change. Modelling shifted from a focus on real-world systems to understanding stakeholder perspectives, which could help people develop better mutual understanding and agree broadly-acceptable ways forward. Second-wave methodologies included soft systems methodology \citep{Checkland1981-op_MYLW}, strategic assumption surfacing and testing \citep{Mason1981-wa_GM}, interactive planning \citep{Ackoff1981-rt_GM} and interactive management \citep{Warfield1994-fa_GM}. Several earlier, first-wave methodologies were transformed in the second wave to become more participative, most notably system dynamics \citep[e.g.,][]{Vennix1996-ei_GM} and organisational cybernetics \citep[e.g.,][]{Espejo1989-fd_GM}. 

It was during the second wave that the definition of a system was expanded to recognise that all systems are understood \textit{from a perspective}. Boundaries were no longer considered the real-world edges of systems, but instead marked what people include in or exclude from their deliberations \citep{Churchman1970-hv_GM}. There was a shift away from seeing systems as real-world entities to viewing them as useful \textit{ways of thinking} to structure interpretations, either of the world or of prospective actions to change that world \citep{Checkland1981-op_MYLW}.

However, this second wave came to be critiqued by a third wave of systems thinkers. Two issues came to the fore. First, a bitterly-entrenched paradigm war between first- and second-wave systems thinkers was sparked by the emergence of the second wave \citep{Jackson1984-zt_GM}. In response, there were many third-wave proposals for \textit{methodological pluralism}: drawing creatively from both first- and second-wave methodologies, and reinterpreting methods through new frameworks or guidelines for choice. The idea was to refuse the forced choice between first- and second-wave thinking, and embrace the best of both. This gave us a more flexible and responsive practice than either of the previous two waves could deliver \citep[e.g.,][]{Jackson1991-pi_GM,Mingers1997-vb_GM}. Much of the work on methodological pluralism was developed under the banner of ‘critical systems thinking’ \citep{Flood1991-ej_GM,Flood1996-pg_GM,Jackson2019-eb_GM}.

The second issue identified in the third wave was that earlier approaches were relatively naïve with respect to power relations. The first-wave assumption that the practitioner and/or client knows best could result in the coercive imposition of ‘solutions’ and/or a lack of stakeholder buy-in, which would frustrate implementation of recommendations for change \citep[e.g.,][]{Jackson1991-pi_GM,Rosenhead2001-ai_JL}. Also, there was a second-wave, practice-limiting belief that stakeholder participation in dialogue, in and of itself, allows the better argument to prevail. This overly minimises problems of bias, coercion, groupthink, deceit, ideological framing and disempowerment \citep{Mingers1980-ya_GM,Mingers1984-yt_GM,Jackson1982-rr_GM}. 

A seminal, third-wave response to the power issue was Ulrich’s (\citeyear{Ulrich1987-hh_GM,Ulrich1994-fw_GM}) critical systems heuristics. Ulrich’s central idea is being critical of the boundary judgements made by decision makers, including the OR practitioner him/herself. Nobody can have a comprehensive viewpoint, so boundaries are inevitably set with reference to the purposes and values of decision makers. However, too often, boundary judgements are taken for granted, so decision makers (often unknowingly) foist their normative assumptions on those affected by their decisions, and the latter’s voices are not heard. Ulrich encourages those involved in and affected by an OR project to reach agreement in dialogue on the key assumptions upon which that project should be based. However, when dialogue is avoided by decision makers, those affected by their ideas have the right to make a ‘polemical’ case to embarrass the decision makers into accepting discussion. The key principle is preventing powerful stakeholders (decision makers and ‘experts’, including the OR practitioner) from simply taking their boundaries and values for granted and imposing them on others. 

Following \cite{Ulrich1994-fw_GM}, \cite{Midgley1998-rw_GM} then reviewed all the second- and third-wave work on boundaries, and proposed a broader theory and practice of \textit{boundary critique}. This encourages the practitioner to explore different possible boundaries, purposes and values in an OR project, and also to uncover conflicts \citep{Midgley2011-gm_GM} and processes of marginalisation \citep{Midgley1992-cr_GM}. \cite{Midgley1998-rw_GM} argue that boundary critique is necessary in all projects dealing with complex issues, as there are likely to be initially-hidden elements of the situation that need to be accounted for. Indeed, even deciding whether a problem situation should be viewed as complex or not requires some up-front boundary critique.

In terms of the definition of a system given earlier and its implications for systems thinking, this work deepened our understanding of boundaries: taken-for-granted boundaries can reflect the structural entrenchment of power relations in our organisations, institutions and wider society \citep{Jackson1985-ws_GM}, which can cause major socio-political and environmental issues \citep{Midgley1994-cj_GM}. Therefore, third-wave systems thinkers started talking about evolving stakeholder perspectives \textit{and} structural relationships: doing either without the other can result in systemic resistance to change \citep{Gregory2000-mo_GM}. However, the starting point for intervention (following an initial boundary critique) is usually stakeholder perspectives because it is the stakeholders themselves who can then turn their attention to structural reform \citep{Boyd2007-am_GM}. Here we see the co-existence of both the first-wave understanding of real-world systems and the second-wave emphasis on stakeholder perspectives. Methodological pluralism makes perfect sense in this context, as some approaches are particularly useful for evolving stakeholder perspectives \citep[e.g.,][]{Checkland1981-op_MYLW}, and others support intervention in organisational and institutional structures \citep[e.g.,][]{Beer1981-fe_GM}. Both can be integrated into an OR project design \citep[e.g.,][]{Sydelko2021-qp_GM,Sydelko2023-pa_GM}.

Eventually, research on methodological pluralism and boundary critique was integrated into a new ‘systemic intervention’ approach by \cite{Midgley2000-gr_GM}. He recognised that boundary critique could support deep diagnoses of problem contexts, and these diagnoses could then inform the design of OR projects, drawing creatively upon methods from both previous waves of systems thinking \textit{and} from other traditions. This work unified the different strands of third-wave methodology.

Recently, however, there have been discussions about whether a fourth wave is forming. Current research foci include whether a universal theory of systems thinking is possible and necessary \citep{Cabrera2023-nw_GM}; how to construct a simple narrative of systems thinking to effectively communicate our work \citep{Midgley2021-hy_GM}; how arts-based methods can enhance practice \citep{Rajagopalan2021-cv_GM}; and what we can learn from neuroscience to inform methodological development \citep{Lilley2022-ud_GM}. It remains to be seen whether addressing these issues will extend the third wave or launch a fourth wave of systems thinking. 

\subsection[Visualisation (Martin~J.~Eppler)]{Visualisation\protect\footnote{This subsection was written by Martin~J.~Eppler.}}
\label{sec:Visualisation}

Visualisation, the graphic (and often interactive) display of quantitative or qualitative information, has established itself not only as a powerful working modality for many management and engineering contexts \citep{Basole2021-ma_MJE,Lindner2022-lk_MJE}, but also as a research field (and research method) in its own right \citep{Eppler2008-gh_MJE}. In this subsection we briefly review the visualisation field, its relevance for Operational Research (including its benefits and caveats), its various types and application contexts, its theoretical perspectives and approaches, as well as its likely future evolution.

Why care about the graphic representation of information, especially for Operational Research? The short answer that research has provided over the last decades to this question is that it provides numerous cognitive, emotional, and social benefits and thus improves our individual and collective ability to make use of information. These benefits include a quicker comprehension of information \citep{Kress2006-ug_MJE}, the detection of important patterns \citep{Bendoly2016-cx_MJE}, the ability to better discuss information \citep{Bendoly2016-cx_MJE,Meyer2018-ui_MJE}, or the greater recall of information \citep{Paivio1990-mw_MJE,Childers1984-iq_MJE}.

The visual representation of information is not, however, without risks or potential disadvantages \citep[see][]{Bresciani2015-ac_MJE,Basole2021-ma_MJE}. Visualisations can be misleading, manipulating, oversimplified, biased, or simply confusing or overwhelming. If, for example, the y-axis of a line chart has been cropped, a small improvement may be mistakenly interpreted as a substantial one. The correct interpretation of information may also require what is often referred to as visual literacy \citep[including data and information literacy, see][]{Locoro2021-dc_MJE} on behalf of the viewers. A diagram is sometimes worth ten thousand words \citep{Larkin1987-se_MJE}, but at times it requires that many words of explanation to properly understand it.

To avoid such risks, professionals need to choose the right visualisation format for the task at hand and use it diligently and in line with existing guidelines \citep[such as those made popular by][]{Tufte2001-bh_MJE} and our perceptual preferences \citep{Ware2020-sj_MJE}. There is research on both of these questions, i.e., on the available types of visualisation \citep{Shneiderman1996-an_MJE,Chi2000-og_MJE} and on their proper use \citep[see for example][]{Ware2020-sj_MJE}.

In terms of segmenting the different kinds of graphic representations for operations management contexts, one can, at the highest level, differentiate between quantitative and qualitative information visualisation. This distinction is based on the type of information that is represented: in the case of numbers or data this is referred to as quantitative visualisation. Typical examples of this genre of visualisation are business intelligence dashboards or simple overhead slides with bar and pie charts. Pie charts, however, are perceptually problematic, as we cannot visually distinguish pie section sizes accurately, let alone compare them in different pie charts. In the case of concepts, arguments, ideas, or issues this is often labelled as qualitative visualisation. Argument mapping \citep{Bresciani2018-xj_MJE} is one approach within this group that is already used in different management contexts. Whereas quantitative visualisation is mostly software-based, qualitative visualisation can be done on paper, walls, flipcharts, and other physical media.

There are, of course, also instances of mixed visualisations that combine quantitative information and qualitative insights in a single image \citep[see][for such combined representations]{Eppler2016-tb_MJE}. An example of such a hybrid visualisation would be a business intelligence dashboard (consisting of charts) that reveals conceptual diagrams through mouse-over comments (or vice versa). 

The aforementioned distinctions are part of one tradition of visualisation research, namely the classificatory or taxonomic approach \citep[see, for example,][]{Shneiderman1996-an_MJE}. This research stream or visualisation perspective aims at providing a systematic and comprehensive overview on all forms of information visualisation that are useful for the engineering or management sector. 

Another theoretical framing of the visualisation field comes from the literature on graphic representations as boundary objects that span professional frontiers and connect expertise across disciplines -- through the help of joint visual displays \citep{Black2012-ym_MJE}. This stream of literature emphasises the dual nature of visualisations to be simultaneously fixed and fluid, clear and open to multiple interpretations or functions (for example the blueprint chart of a building or the Gantt chart for a project). 

A third influential approach to make sense of the use and impact of visualisation in workplace settings is the cognitive or collaborative dimensions approach \citep{Green2006-cm_MJE,Bresciani2018-xj_MJE}. This theoretical lens sheds light on the different qualities of graphic representation that make them more or less suited to be collaboration catalysts –- for example based on their (procedural or representational) clarity, unevenness or facilitated insight. 

A similar theoretical perspective is the affordance approach \citep{Meyer2018-ui_MJE}, that highlights the different cognitive `invitations' or incentives that visualisations can provide, such as their attention grabbing effect, their interpretive flexibility or their story telling potential. 

Another influential research stream focuses not on how images are best designed for their application contexts, but on how they are appropriated and interpreted. Many researchers with this research stream employ a semiotic approach to the study of visualisation based on the seminal work by \cite{Kress2006-ug_MJE}. This approach is also informed by (research-based) insights into our perception of visual information, but additionally enriched by the conventions and (cultural) traditions that govern our interpretation processes of graphic symbols.

There are of course many other research streams discussing the design and use of visualisation in management or engineering contexts. While some of them focus on particular visualisation formats, such as diagrams, maps, 3D models, or sketches, others focus on certain application contexts, such as (big data) analytics, creativity and innovation, production simulation, or planning. This brings us to the actual application contexts of visualisation.

When are the visualisation formats and perspectives described above mostly used? Typical application contexts for information visualisation are strategising \citep{Eppler2009-qw_MJE} and planning sessions of managers and experts (for example with the help of Gantt charts or technology roadmaps \citep{Blackwell2008-gs_MJE}, risk analysis \citep{Eppler2009-qy_MJE}, ideation and problem solving workshops (using mind mapping, argument mapping, or simple sketching) in product development and business model innovation contexts (using canvases and other visual framerworks), training and development (including knowledge transfer and retention), as well as for performance management, simulations and forecasting or scenario workshops. Last but not least, visual methods are also used as research tools in their own right \citep{Comi2014-rt_MJE} to enable better access to practitioners' expectations, experiences, or priorities \citep{Bell2013-eb_MJE}. 

Many new application contexts are currently emerging within the realm of Operational Research and management, including new forms of visualisation. These novel forms include trainings and simulations in three dimensional immersive settings such as the Metaverse or augmented reality visualisations for simulations or assisted on-site decision making or operations. Another fascinating recent phenomenon consists of images created by artificial intelligence based on user instructions (such as DALL-E or similar systems). Such artificially created, at times photo-realistic images, can help (for example) in the ideation, service innovation, or product development context. The rise of artificial intelligence also impacts the interpretation of information visualisation: A case in point are data visualisation packages (such as Tableau or PowerBI) that (through AI) already assist the user in the exploration and interpretation of the provided data charts and suggest areas for deeper analysis. The visualisation field is thus a highly dynamic area with great promise, both in terms of its methodological repertoire as well as its application scope.


\clearpage

\section{Applications}
\label{sec:applications}

\subsection[Auctions and bidding (Bo~Chen)]{Auctions and bidding\protect\footnote{This subsection was written by Bo~Chen.}}
\label{sec:Auctions_and_bidding}

The 2020 Nobel Prize in Economics was awarded to Paul Milgrom and Robert Wilson for their improvements to auction theory and inventions of new auction formats. Their theoretical discoveries have improved auctions in practice and benefited sellers, buyers and taxpayers around the world \citep{RSAS20_BC}.

An \emph{auction} is usually a process of selling and/or buying goods or services that are up for bids. A \emph{bid} is a competitive offer of a price and/or quantity tag for a good or service. Auction is a particular way to determine prices and allocation of goods or services.

Auctions have been used since antiquity for the sale of a variety of objects. Today, both the range and the value of objects sold by auction have grown to staggering proportions \citep{Krishna10_BC}. The contexts within which auctions are applied include art objects, antiques, rare collectibles, expensive wines, numerous kinds of commodities, livestock, radio spectrum, used cars, real estate, online advertising, vacation packages, wholesale electricity and emission trading, and many more.

In the basic economic model, the price of a good or service is obtained when the supply and demand meet and it is normally an equilibrium value after adjustments over time. However, in some situations such adjustments cannot be made to reach an equilibrium. As \citet{Haeringer18_BC} points out, auctions are commonly used when (\textit{a}) sellers and/or buyers have little knowledge of what would be the ``right'' price (e.g., a tract of land with an unknown amount of oil underground); (\textit{b}) the supply is scarce (e.g., an art painting); (\textit{c}) the quantity or quality of the good changes very frequently (e.g., electricity or fish); and (\textit{d}) transaction frequency is low (e.g., radio spectrum).

Bidders behave strategically. Based on the available information, what they know themselves and what they believe other bidders to know, it is difficult to analyse the outcomes of different bidding rules. This is where auction theory comes in, which is closely linked to many other domains of operational research, such as game theory (\S\ref{sec:Game_theory}), behavioural OR (\S\ref{sec:Behavioural_OR}), combinatorial optimisation (\S\ref{sec:Combinatorial_optimisation}), computational complexity (\S\ref{sec:Computational_complexity}), linear programming (\S\ref{sec:Linear_programming}) and integer programming (\S\ref{sec:Mixed_integer_programming}).

\subsubsection*{Key concepts and results}
As detailed by \citet{Haeringer18_BC}, an auction consists of the following component rules: (\textit{a}) bidding format (e.g., a price, a price and a quantity, a quantity only, or a list of items if more than one item are for sale); (\textit{b}) bidding process (e.g., auction stopping criteria and information for bidders); and (\textit{c}) price and allocation (i.e., auction winner(s) and the final price(s)).

In studying auctions, it is important to bear in mind the underlying model of \emph{valuation}, the values attached to the objects by individual buyers and/or sellers. If the value, though unknown at the time of bidding, of an object is the same for all bidders, then the evaluation is of a \emph{common value}. More generally, in situations of \emph{private values}, the value of an object varies from one bidder to another. These values can be \emph{independent} or \emph{interdependent}.

If there is only one item to be sold, we have the most basic auction. Some common forms of such simple auctions are well known. In an open-outcry auction, an auctioneer takes bids from the participants and at some point of time a winner is declared, who will then pay for the item at some price related to the bids. If all bids follow the dynamics of ascending prices and the winner is the highest bidder, who pays his bidding price, then we have an \emph{English auction}. In the case of private values, the English auction is strategically equivalent to the \emph{second-price sealed-bid auction} \citep{Krishna10_BC}, in which bidders submit written bids without knowledge of other bids. The highest bidder wins but pays the price that is the second highest in the auction. On the other hand, if the auctioneer in an open-outcry auction begins with a high asking price (in the case of selling) and lowers it until some participant accepts the price (or until it reaches a predetermined reserve price), then we have a \emph{Dutch auction}. This type of open-outcry descending-price auction is most commonly used for goods that are required to be sold quickly such as flowers, or fresh produce \citep{Mishra09_BC}, as it has the advantage of speed since a sale never requires more than one bid. The Dutch auction is strategically equivalent to the \emph{first-price sealed-bid auction} \citep{Krishna10_BC}, which is the same as the second-price sealed-bid auction except that the winner pays his bidding price.

If there are multiple homogeneous (resp., heterogeneous) items to be sold, we have a \emph{multiunit} (resp.\ \emph{combinatorial}) auction.

One of the most important results in auction theory is the \emph{revenue equivalence theorem} \citep{Heydenreich09_BC,Nisan07_BC}, which in its simple form states that when bidders' valuations are private and uniformly distributed, the expected revenue of the seller is the same in the English (or second-price) and Dutch (or first-price) auctions.

In a \emph{forward auction}, a number of buyers compete to obtain goods or services from one seller  (e.g., spectrum auction). In contrast, in a \emph{reverse auction}, a number of sellers compete to obtain business from one buyer (e.g., electricity capacity market). In a \emph{double auction}, there are multiple sellers and multiple buyers (e.g., wholesale electricity market). Potential buyers submit their bid prices and potential sellers submit their ask prices to the market institution, which then chooses the price that clears the market. At this price $p$, all the sellers who asked no more than $p$ sell and all buyers who bid at least $p$ buy.

The main issues that guide auction theory involve a comparison of the performance of different auction formats. Naturally \emph{revenue} is by far the most common yardstick from the seller's perspective. However, if the auction concerns the sale of a publicly held asset to the private sector, such as the case of spectrum auction, \emph{efficiency} may be more important -- the object ends up in the hands of whoever values it most \emph{ex post}, or in the more general case of multiple items, the \emph{sum} of realised values for all participants is maximised. Besides, \emph{simplicity} and \emph{susceptibility} to collusion among bidders are among other criteria for the choice of an auction format \citep{Krishna10_BC}.

\subsubsection*{Some best practices}
One of the most important applications of auction theory is the implementation of \textit{spectrum auctions} to allocate licenses to mobile phone carriers, who act as buyers in the forward auction. One of the auction formats introduced by \citet{Milgrom87_BC} and \citet{Wilson98_BC} was first used in 1994 by the US authorities to sell radio frequencies. This practice has since spread globally and led to great benefit to society.

There can be many ways for allocating licences in general. In addition to an auction, it can proceed either with a lottery in which any interested party would just have to sign up possibly with an entry fee, or with a beauty contest in which all those wishing to obtain a licence are required to present a case and the final winners would be selected by a committee. \citet{Haeringer18_BC} provides a detailed argument why a lottery or a beauty contest is inappropriate
in the case of radio frequencies and why an auction offers a more attractive solution. There are a number of issues in selling licences of spectrum, such as those concerning collusion, demand reduction and lack of entry. Of particular relevance for common-value auctions is the so-called \emph{winner's curse} -- the winner pays too much and loses out. \citet{Haeringer18_BC} discusses how a suitable format of an auction can be used to address these issues.

A \textit{wholesale electricity market} exists when competing generators offer their electricity output to retailers. Double auctions are normally used for such a market \citep{may:tru:18_DSRW}. By its nature electricity is difficult to store and has to be available on demand. Consequently, unlike other products, it is not possible, under normal operating conditions, to keep it in stock, ration it or have customers queue for it, so the supply should match the demand very closely at any time despite the continuous variations of both \citep[][\S\ref{sec:Power_markets_and_systems}]{wer:14_DSRW}. The supply uncertainty becomes particularly relevant with an increased use of green energy (such as solar, tidal, wind energy).

The \textit{electricity capacity market} becomes necessary to build and maintain electricity capacity that may be called upon in time of need to maintain the grid balance. In the UK’s system for purchasing Short Term Operating Reserve (STOR) for electricity supply \citep{NG22_BC}, the National Grid maintains a reserve generation ability in case of sudden demand or supply variations. Part of the operating reserve is made up by contracts through auctions. In this market, the bids come as electricity capacity, so the National Grid determines the right amount of capacity to reserve from a competitive tender. Tenders are assessed on the basis of availability prices and utilisation prices together with a consideration of response times and geographical locations. In this reverse auction, the National Grid acts as the buyer, while individual electricity operators act as sellers. Extensive studies on such auctions can be found in \citet{Chao02_BC} and \citet{Schummer03_BC}. More recently, \citet{Anderson17_BC, Anderson22_BC} investigate the problem under more general settings. They show that a natural equilibrium is not only efficient but also optimal for individual bidders.

The Internet is a new exciting venue for auctions and eBay is certainly the most well-known auction place on the Internet. Auctions on eBay face new challenges due to the nature of the Internet, where an auction can take days or even weeks and potential buyers can bid whenever they want. In response, eBay uses \emph{proxy bidding} wherein a computer programme is used to bid on behalf of the bidder, who enters an auction effectively with a \emph{maximum} bid. The computer programme raises rival bids by the minimum increment set beforehand as long as it is below the maximum bid. It is easy to see that such an auction is effectively a second-price auction in which the amount entered by the bidder serves as the bidding amount. \cite{Ariely2003-eb_BC} propose an analytical framework for studying bidding behaviour in online auctions. \cite{Chothani2015-eb_BC} provide an overview of online auctions. \citet{Hickman08_BC} analyses significant differences between electronic auctions and non-electronic auctions.

\subsubsection*{Closing remarks}
There are many excellent surveys of auction theory and applications. \citet{Milgrom85_BC} and \citet{McAfee87_BC} provide a cogent account of the theory of single-object auctions and explain many extensions and applications of the theory. \citet{Milgrom04_BC} provides a comprehensive introduction to modern auction theory and its important new applications. \citet{Samuelson14_BC} examines the use of auctions, paying equal attention to theory and practice. \citet{Haeringer18_BC} and \citet{Kagel20_BC} give respectively an overview of empirical and experimental studies on auctions. \citet{Cassady67_BC} provides a colourful and insightful overview of real-world auction institutions.

\subsection[Community Operational Research (Amanda~J.~Gregory)]{Community Operational Research\protect\footnote{This subsection was written by Amanda~J.~Gregory.}}
\label{sec:Community_Operational_Research}

Community Operational Research (COR) reflects the aspirations of OR's early theorists and practitioners of “science helping society” (\citeauthor{Cook1973-fz_AJG}, \citeyear{Cook1973-fz_AJG}/\citeyear{Cook1984-cu_AJG}, p.36). There is a long tradition of COR practice that includes Ackoff's \citeyear{Ackoff1970-if_AJG} engagement with members of the Mantua ghetto in Philadelphia, Cook's projects with inner-city community organisations \citep{Cook1973-fz_AJG,Luck1984-sj_AJG}, Beer's work with the Allende Government in Chile \citep{Beer1981-fe_GM}, and numerous projects undertaken from the University of Bath \citep{Jones1981-xt_MYLW,Sims1982-sn_AJG}. See \cite{Jackson2004-qf_AJG} and \cite{Rosenhead1993-dj_AJG} for a discussion of such work. Although these early examples of COR are significant, they were far from the norm as a focus on “science helping the establishment” predominated \citep[p.36]{Cook1973-fz_AJG,Cook1984-cu_AJG}. In recognition of this, \cite{Rosenhead1986-ti_MYLW} posed the question of “who O.R. worked for (“custom”)” (p.335) in his inaugural address as President of the UK's Operational Research Society. Rosenhead answered his own question in stating that the customers were, in the main, “big business, public utilities, the military and central government departments, with a thin scatter of local governments and health and other public authorities” (p.336) to the neglect of other groups “located outside the power structure” (p.337). \cite{Rosenhead1986-ti_MYLW} not only discussed the custom of OR, but also tackled the related issue of practice in asserting that “The evolved forms of tools reflect the circumstances of their use” (p.338). Hence, mainstream OR's focus on quantification and modelling reflected its customers’ privileging of technical matters over dialogue between stakeholders and issues of emancipation (\citeauthor{Rosenhead1993-dj_AJG}, \citeyear{Rosenhead1993-dj_AJG}, drawing on \citeauthor{Habermas1972-yr_AJG}, \citeyear{Habermas1972-yr_AJG}), and involved the use of OR methods “beyond the comprehension of most people” \citep[p.339]{Rosenhead1986-ti_MYLW}, effectively masking the social and value-laden nature of much decision making. In contrast, concerns for better mutual understanding in society and freedom from oppressive power relationships inspired the call for a more transparent OR to support “a more lively, complex and elaborate social process of decision-making” \citep[p.339]{Rosenhead1986-ti_MYLW}.

Such was the impact of Rosenhead's inaugural speech and his efforts within the OR Society that engagement with non-traditional clients quickly became legitimised and formalised through the founding of a research centre, the Community Operational Research Unit, located at Northern College, which later moved to its present location at the University of Lincoln, UK. In the 1980s, the OR Society also provided support for the Centre for Community OR at the University of Hull (later to be merged into the Centre for Systems Studies), and the Community OR Network of around 300 OR practitioners. The universities of Lincoln and Hull continue to actively practice and promote COR. More recently, in 2011, the OR Society created a Pro Bono OR scheme that connects volunteer analysts with good causes\footnote{\url{https://www.theorsociety.com/get-involved/pro-bono-or/}, accessed 2023-01-19}\textsuperscript{,}\footnote{\url{https://www.theorsociety.com/get-involved/society-groups/special-interest-groups-and-networks/or-in-the-third-sector/}, accessed 2023-01-19}.

Given the multi-faceted and often complex nature of COR projects, there appears to be no one particular OR approach that has emerged as dominant. There are, though, three streams of complementary, sometimes overlapping, approaches that have proven useful in multiple reported cases of COR:

\begin{enumerate}[noitemsep]
\item Problem Structuring Methods (PSMs) are a collection of approaches that offer decision support by “way of representing the situation (that is, a model or models) that will enable participants to clarify their predicament, converge on a potentially actionable mutual problem or issue within it, and agree commitments that will at least partially resolve it” \citep[p.531]{Mingers2004-qf_MYLW}. The modelling effort may involve clarification of normative agendas through dialogue, as PSMs are largely founded on interpretivist or social constructivist epistemologies \citep{Jackson2006-cb_AJG}. For more on PSMs see \S\ref{sec:Soft_OR_and_problem_structuring_methods}.
\item Critical Systems Thinking (CST) and Critical Systems Practice (CSP) focus on the distinction of a broad range of problem contexts and the development of systems-based methods appropriate to those contexts \citep{Flood1991-fb_AJG,Flood1991-ej_GM,Mingers1997-nd_MYLW}. Having a broad range of systems methodologies to draw on is necessary but not sufficient for good practice. Consequently, \cite{Jackson2000-ei_AJG} encapsulated the notion of good practice in his statement of three commitments of CSP: critical awareness, relating to critique of the different systems methodologies, and social awareness of the societal and organisational context; improvement, referring to the achievement of something beneficial, reflecting a cautious approach to the aspiration of universal liberation; and pluralism, the need to work with multiple paradigms without recourse to some unifying metatheory. For more on systems thinking, see \S\ref{sec:Systems_thinking}.
\item Systemic Intervention (SI) developed out of CST and took as its two primary concerns critical reflection on boundaries of inclusion and exclusion \citep{Churchman1970-hv_GM,Ulrich1983-ki_AJG,Ulrich1987-hh_GM,Midgley2000-gr_GM} and methodological pluralism. \cite{Midgley2000-gr_GM} defines SI thus: “If intervention is purposeful action by an agent to create change, then systemic intervention is purposeful action by an agent to create change in relation to reflection on boundaries” (p.129). He shows how exploring boundaries informs the methodological design of a project, with the meaningful engagement of communities built in. For more on SI see section \S\ref{sec:Systems_thinking}.
\end{enumerate}

These three streams of approaches have much in common with action research \citep[AR;][]{Levin1994-yg_AJG,Midgley2000-gr_GM,Mingers2004-qf_MYLW} and, perhaps not surprisingly, AR has been a focus of a lot of COR work. Indeed, the Community Operational Research Unit explicitly articulated a working philosophy of AR following the traditions established in Latin America and Scandinavia \citep{Thunhurst1992-vg_AJG}. Over the years, a considerable and diverse body of COR work has amassed, with some contemporary and notable examples including conference papers \citep[e.g.,][]{Wong2020-so_AJG}, case-based research papers \citep[e.g.,][]{Rosenhead1996-zi_AJG,Deutsch2022-ns_AJG,Paucar-Caceres2022-dy_AJG,Pinzon-Salcedo2022-im_AJG,Chowdhury2023-th_AJG}, journal special issues \citep[e.g.,][]{Johnson2018-sj}, project reports \citep[e.g.,][]{Stephens2018-ov_AJG} and edited books \citep[e.g.,][]{Bowen1995-to_AJG,Ritchie1994-rw_AJG,Midgley2004-xx_AJG,Johnson2012-jm_AJG}.

What counts as COR is not a simple matter though, and there have been several papers over the years that have critically discussed this (see for example the different understandings reflected in \citeauthor{Midgley2018-jv_AJG}, \citeyear{Midgley2018-jv_AJG}, and \citeauthor{White2018-rh_AJG}, \citeyear{White2018-rh_AJG}). Importantly, \cite{Johnson2012-vr_AJG} suggest that some examples of COR might be more appropriately classed as capacity-building instead of “applications based on analytic models intended to provide specific policy and operational guidance to decision-makers in a way that extends existing theory and methods” (p.39). While some COR might indeed be classed as capacity building (for example, \citeauthor{Boyd2007-am_GM}, \citeyear{Boyd2007-am_GM}, are explicit that capacity building was part of their project), it is important not to confuse such interventions with those that are based on the use of models of a qualitative rather than quantitative nature. Indeed, the commitment to knowledge being embedded within the client organisation \citep{Klein2007-dj_AJG}, handing over tools and techniques \citep{Gregory1992-ab_AJG,Gregory1992-yb_AJG,Boyd2007-am_GM,Gregory2015-co_AJG} and self-organised learning \citep{Herron2018-ff_AJG} serve to bring about capability-building alongside model building and the use of analytical approaches at the local level, which does not rule out modelling and data analysis. The needs and skills of citizens and associated groups have moved on since the 1980s, such that the tools of OR (data and models) are not so incomprehensible as they might previously have been regarded, and are familiar to most if not all citizens \citep{Caulkins2008-xg_AJG}. Indeed, \citeauthor{Hindle2018-zo_JEB}'s (\citeyear{Hindle2018-zo_JEB}) work with the Trussell Trust on mapping food bank data demonstrates that charities can make good use of big data and data visualisation.

Although there are examples of COR projects being undertaken world-wide, sustained organised support has been most evident in the UK and US. \cite{Johnson2012-jm_AJG,Johnson2012-iq_AJG} brought renewed interest to the field in the US with his promotion of a stream of activity that goes by the title of Community-Based Operations Research (CBOR). \cite{Johnson2012-vr_AJG} define CBOR as “a subfield of public-sector OR... that emphasizes most strongly the needs and concerns of disadvantaged human stakeholders in well-defined neighborhoods. Within these neighborhoods, localized characteristics vary over space and exert a strong influence over relevant analytic models and policy or operational prescriptions” (p.38). Complementary to the remit of CBOR is the Institute for Operations Research and the Management Sciences (INFORMS) Pro Bono Analytics® initiative\footnote{\url{https://connect.informs.org/probonoanalytics/home}, accessed 2023-02-08}.

Whilst COR, CBOR and pro-bono OR may be said to have a related remit, it is worth mentioning a key difference, “COR takes as its remit to work with (i.e., to take as its clients) disadvantaged community groups themselves” \citep[p.610]{Rosenhead2013-zp_AJG}, whereas CBOR and pro-bono OR are more focused on making OR and analytics available to third sector and public organisations. This distinction is not undisputed \citep{Midgley2018-jv_AJG}, but the important thing is that such efforts, geared to meaningful community engagement, have not only enabled community access to OR, but have also provided a strong impetus for its theoretical and methodological development in a way that honours the legacy of OR's early founders.

As we have an increasing number of ways to connect with others and form communities, it would be easy to assume that, going forwards, COR merely needs to develop new forms of practice to support communities in these different realms. But, in a VUCA (volatile, uncertain, complex and ambiguous) world \citep{Bennis1986-ia_AJG}, we must be alert to the need to challenge simple assumptions. Rather, there is a good argument for a critical turn in COR involving the explicit examination of underpinning values and ethics \citep{Cordoba2006-tv_AJG,Jackson2006-cb_AJG}. \cite{Midgley2004-xx_AJG} have already claimed that “if practitioners do not reflect on the different visions that it is possible to promote, then there is a danger that they will default to the understanding of community that is implicit in the liberal/capitalist tradition” (p.259). This brings with it missed opportunities to pursue more challenging and empowering practices that enable political activism and give some of the most marginalised people in our society a meaningful voice in OR projects. Much of COR has arguably been quite tame, doing good in local communities without challenging the political status quo \citep{Wong1994-lb_AJG}, but in an era of climate change, biodiversity loss, rising nationalism, insecure employment, mass migration, and increasing wealth inequality, a new, more critical agenda for COR is urgently needed.

\subsection[Cutting and packing (Julia~A.~Bennell)]{Cutting and packing\protect\footnote{This subsection was written by Julia~A.~Bennell.}}
\label{sec:Cutting_and_packing}

Cutting and packing (C\&P) problems are geometric assignment problems, in which \textit{small items} are assigned to \textit{large objects} such that a given objective function is optimised and two basic geometric feasibility conditions hold, specifically containment and non-overlap. They appear in a wide range of settings, but are most commonly investigated for applications in manufacturing and transportation. For example, cutting pattern pieces from material or packing boxes into containers. These are combinatorial optimisation problems and $\mathcal{NP}$-hard. Depending on the size or geometry of the problem, there exists strong formulations that can be solved using exact methods. However, there remains many open problems that have instances that cannot be solved to optimality, or computational times are impractical for applications in practice. Moreover, there are problems where bounds are weak and only toy instances can be solved to optimality. As a result, heuristics remain an important tool in C\&P.

Given the wide variety of C\&P problems, \cite{Dyckhoff1990-lr_JB} and later \cite{Wascher2007-bg_JB} defined a typology of problems using the following dimensions:\\

\noindent Objective function
\begin{itemize}[noitemsep]
\item Output maximisation: packing the greatest value of items in a given fixed dimension finite number of large object(s).
\item Input minimisation: pack all items using the minimal number of fixed dimension objects or the minimum size large object with at least one unconstrained dimension.
\end{itemize}

\noindent Assortment of small items (items to be packed)
\begin{itemize}[noitemsep]
\item All items are identical. 
\item Weakly heterogeneous: few item types given the total number of items. 
\item Strongly heterogeneous: many item types that are unique or have few copies.
\end{itemize}

\noindent Assortment of large items
\begin{itemize}[noitemsep]
\item Single large object: fixed dimension for output maximisation, open dimension(s) for input minimisation.
\item Multiple large objects: fixed dimension, either identical or heterogeneous.
\end{itemize}

These distinctions lead to named problem types, e.g., bin packing problems (BPP) are input minimisation problems, with strongly heterogeneous small items and multiple large objects, while a knapsack problem (KP) shares the same characteristics but is an output maximisation problem. Note that problem names and their definitions are not universally accepted or consistently used, so researchers should check the articulated problem definition in the paper when selecting literature. 

The following focuses on two-dimensional (2D) and three-dimensional packing (3D) as these include the unique challenges of the geometric constraints associated with C\&P problems. One-dimensional (1D) problems remain interesting and challenging \citep[see][]{Martinovic2018-by_JB,Munien2021-sh_JB}. For an introduction to C\&P, see \cite{Scheithauer2018-xm_JB}.

\subsubsection*{Geometry}
Handling the geometric characteristic of C\&P problems adds significantly to the computational burden and the number of variables needed in a model. These increase with the spatial dimensions and with the irregularity of the shape of the small items. For 1D problems, the geometric constraints of overlap and containment are trivial. Regular shapes (rectangles, boxes, circles, spheres) add complexity through additional item location variables $x$, $y$ (and $z$) co-ordinates, and dimensions: length, width (and depth). However, the common characteristics of the shape mean these are straightforward to model. Pairwise constraints between items and between each item and the boundary of the large object ensure feasibility.

In the case of irregular shaped items, accurate non-overlap constraints cannot be reduced to comparing a set of common dimensions. While the item location is still determined by a defined origin, the arbitrary nature of the shape significantly increase the complexity of assessing geometric feasibility. At a basic level, it requires testing for edge intersections between items and containment of one item inside another. Methods to reduce the complexity include the nofit polygon, raster method and phi-functions in 2D and voxels and phi-functions in 3D. \cite{Bennell2008-nv_JB} provide a tutorial in geometric methods for 2D nesting problems. \cite{Lamas-Fernandez2022-fe_JB} describe approaches for modelling 3D geometry. Developing solution methods for irregular packing problems requires a comprehensive, fast and robust geometry library.

\subsubsection*{Constraints}
There exists a wide range of practical constraints arising from the applications. These may relate to the material being cut having defects or quality variability, the cutting tool requiring space between items or constraints on the types of cut. There may be sequencing constraints or assignment constraints that include precedence or prevent/require the packing of items together. In 2D rectangle C\&P, a common requirement is guillotine cuts where all cuts must be orthogonal and span the entire width or breadth of the rectangular material sheet. Furthermore, the number of alternate cuts (e.g., a switch from vertical to horizontal cuts) may be restricted.  Applications in 3D container loading provide a challenging set of constraints on the arrangement of boxes to ensure weight distribution, horizontal and vertical stability of load and consider the weight baring strength of the stacked boxes. \cite{Bortfeldt2013-wd_JB} describe the different types of constraints and their adoption by researchers.

\subsubsection*{Two-dimensional problems}
In two dimensions, research has focused on rectangle packing problems, and irregular shape packing problems, often called nesting problems. There is also a smaller body of literature on circle packing \citep{Hifi2009-ce_JB}.

Exact solution approaches to the 2D rectangle packing problem are reviewed by \cite{Iori2021-hs_JB} and cover the main problem types. The paper evidences the recent advances in exact methods for these problems while identifying a number of open problems that remain very challenging. Specifically, they identify the open-dimension problem, some specific instances of BPP, and problems with multiple heterogeneous large objects. Moreover, nesting problems remain a rich area of research for developing high fidelity and scalable exact methods.

Heuristics and metaheuristics are a natural choice, particularly for problems with a large number of small items. Early research focused on placement heuristics such as First Fit and Next Fit for BPP \citep{Coffman1980-vj_JB}, Bottom Left and Bottom Left Fill for open dimension problems \citep{Albano1980-jw_JB,Chazelle1983-fp_JB,Burke2004-ia_JB}. These place items into the large object in a given sequence according to the placement rule and observing any additional placement constraints. A natural evolution of this approach is to apply a metaheuristic to re-sequence the packing order to obtain better solutions, of which there are many examples. 

The 2D rectangle \textit{identical item packing} problem is known as the manufacturer's pallet loading problem. Research is mature with exact methods and heuristics that perform well across benchmark instances. \cite{Silva2016-jj_JB} comprehensively review these problems. Fewer papers have looked at the case where the small item is irregular, for example cutting metal blanks \citep[e.g.,][]{Costa2009-il_JB}.

Output maximisation problems focus on the 2D rectangle \textit{knapsack problem}, with few articles considering problems with weakly heterogeneous data. Problems cover guillotine and non-guillotine cutting and item values may be equivalent to area or have an assigned value. Furthermore, the constrained variant places upper and lower bounds on the number of each item type placed. The guillotine variant where area and value align has a fast exact method \citep{Oliveira1990-cd_JB}. Non-aligned item values and non-guillotine cutting is still challenging. \cite{CILM22B_SMPT} includes a summary of 2DKP. Packing a single large object is often a component of a larger practical problem, where the decision problem of whether a set of rectangles will fit into a fixed dimension rectangle, referred to as a 2D orthogonal packing problem, is of interest \citep{Clautiaux2007-ti_JB}. 

\textit{Cutting stock problems} have been studied for over 60 years with the seminal paper by \cite{Gilmore1965-mm_JB} that described the column generation approach for 2D rectangle cutting stock problem with guillotine cuts. Most papers focus on ILP/MILP approaches. A notable feature of these problems and how they differ from the BPP, is the way solutions are constructed arising from the data instances. Since there are few item types, but many items of each type, the solutions are composed of pattern types that are repeated across multiple stock sheets leaving a residual problem of unmet demand. 

\textit{Bin packing problems} have been extensively studied and include the guillotine and non-guillotine variant. \cite{Lodi2014-jq_JB} provide a review of BPPs. Early heuristic approaches \citep{Berkey1987-lb_JB} include two-phase algorithms that pack multiple bin width strips and then solve a 1D BPP where the item size is the height of the strip, while single-phase algorithms pack directly into the bins either using a level packing approach or a placement heuristic such as bottom-left. Increasingly, researchers are focusing on exact methods; see for example \cite{Pisinger2007-rx_JB} who use branch and price for variable size and fixed size bins. There are very few examples of bin packing with irregular shapes, where one example is glass cutting \citep{Bennell2018-fm_JB}.

The \textit{open dimension problem} variant is often called the strip packing problem. This can be formulated as a linear mixed integer programme, although practical size problems are still very challenging. \cite{Martello2003-mo_JB} develop bounds by relaxing the problem so it can be solved as a one dimensional BPP. Placement heuristics (bottom left and bottom left fill) based on sequencing of items within a (meta)heuristic framework are widely used. \cite{Hopper2001-dz_JB} undertook an extensive analysis of this type of approach. 

\textit{Nesting problems}, where the small items are irregular, are commonly formulated as open dimension problems. Solution approaches are dominated by the use of heuristics and metaheuristics. \cite{Bennell2009-kx_JB} provide a review of these methodologies including using exact models to improve local optima. This approach is also used by \cite{Stoyan2016-fk_JB} who use phi-functions, which allow orientation as a decision variable. In the last decade, researchers have developed formulations that can be solved to a global optimum for small problems. \cite{Toledo2013-xy_JB} approximates the items and the packing area to a discrete set of points allowing the problem to be solved as a MIP model. \cite{Alvarez-Valdes2013-fs_JB} used the nofit polygon to define a finite set of convex spaces and used binary variables to activate constraints. 

\subsubsection*{Three dimensional problems}
These problems are solved across the range of problem types, largely considering single container output maximisation, multi-container input minimisation and the open dimension problem. For packing boxes, the mix of constraints addressed across the literature is inconsistent and frequently not congruent with industry standards. Solutions to the problem focus on building walls, layers or blocks of identical boxes. See \cite{Zhao2016-nt_JB} for a comparative review of algorithms including exact methods. Recent papers are now looking at the additional constraints arising from a vehicle, such as axle load and stability under breaking and acceleration \cite[see][]{Ali2022-ow_JB}. 3D packing of irregular shapes is an open problem that had increasing relevance in areas such as additive manufacturing. Efficient handling of the geometry is a significant factor in the solution approach along with the level of fidelity required for the application. 

\subsubsection*{Data}
Across all problem types there are standard data sets and data generators that provide a useful means to test the effectiveness of solution approaches. Many of these are listed on the EURO Special Interest Group on Cutting and Packing (ESICUP) website\footnote{\url{https://www.euro-online.org/websites/esicup/data-sets/}}.

\subsection[Disaster relief and humanitarian logistics (Bahar~Y.~Kara \& Özlem~Karsu)]{Disaster relief and humanitarian logistics\protect\footnote{This subsection was written by Bahar~Y.~Kara and Özlem~Karsu.}}
\label{sec:Disaster_relief_and_humanitarian_logistics}

Humanitarian logistics (HL) is one of the key application areas that Operational Research (OR) has been offering solutions to improve the welfare of the society under difficult circumstances. Humanitarian logistics problems are highly relevant in today's world due to various challenges including but not limited to, climate change and its consequences (increases in extreme weather events such as heatwaves, floods), natural disasters (e.g., earthquakes, tsunamis), man-made conflicts (e.g., Syria and Ukraine crises) and health-related catastrophic events (e.g., pandemics). Humanitarian logistics operations involve  complex systems with multiple stakeholders such as victims, planners, public/private service providers, volunteers, general public and media, each with their own preferences and priorities; and the inherently challenging decisions of scarce resource allocation have to be made over a long time span, under high uncertainty. We use humanitarian logistics as an umbrella term, which covers relief logistics, disaster logistics, and development logistics. The humanitarian literature uses all these terms interchangeably. Actually, disaster logistics and relief logistics should refer to the cases where a disaster is/was/is expected to be in action whereas development logistics refers to cases which aim to improve daily life. 

In relief logistics, the operations require advanced planning, hence the authorities are constantly facing challenges in the four main stages of: \textit{mitigation, preparedness, response, and recovery} \citep{altay2006or_BYKOK,ccelik2012humanitarian_BYKOK,kara2017humanitarian_BYKOK}. In Table \ref{tab:prob_BYKOK}, we list some of the most frequently considered problems, categorised based on the phase that they arise. As seen in the table, mitigation and preparedness phases mostly consist of activities related to planning, which involves network design, location, allocation and routing operations as well as provisioning processes that include inventory and other supply chain-related decisions \citep[see, e.g.,][for applications in location and prepositioning, respectively]{balcik2008facility_BYKOK,rawls2010pre_BYKOK}. 

Response activities occur after the crisis or the disaster hits. In this phase, the aim is providing a rapid response, prioritising the survival needs. This, however, does not preclude considering efficiency in these operations as the system requires scarce resource allocation such as personnel, equipment and supplies across demand points, invoking a need for good decision support mechanisms. In line with this need, a large body of work is devoted to the application problems arising in this phase. Finally, recovery phase focuses mainly on debris management and infrastructure repair and restoration. Most of the mentioned operations involve additional decisions regarding workforce planning and scheduling and require structured methods for data management, information sharing and coordination, which are key for effective response \citep{altay2014challenges_BYKOK}. Some of these models require quantification of human suffering due to lack of services or goods and the deprivation cost can be used for this purpose, as discussed in detail in \cite{holguin2013appropriate_BYKOK}. 

\begin{table}[htbp]
	\centering
	\caption{Problems in Relief Logistics}
\resizebox{\textwidth}{!}{	\begin{tabular}{clll}
		\textbf{Phase} & \multicolumn{1}{l}{\textbf{Main Problems considered}} & \multicolumn{1}{l}{\textbf{Main decisions involved}} & \multicolumn{1}{l}{\textbf{Main concerns}} \\
				\hline
		Preparedness and mitigation & \multicolumn{1}{l}{Infrastructure  and network structuring } & Network design & \multicolumn{1}{l}{Connectivity} \\
				&  &  (mainly for strengthening purposes)  &  \\
		\cline{2-4}
		& \multicolumn{1}{l}{Risk assessment } & Prioritization  & \multicolumn{1}{l}{Data preparation } \\
			&  & (of roads, buildings, arcs)  & for further analysis \\
		\cline{2-4}
		& \multicolumn{1}{l}{Evacuation planning } & Location of gathering points & \multicolumn{1}{l}{Traffic} \\
		&       & Allocation of evacuees & \multicolumn{1}{l}{Accessibility} \\
		&       & Routing  & \multicolumn{1}{l}{Evacuation Time} \\
		&       &       & \multicolumn{1}{l}{Behavioral factors} \\
		\cline{2-4}
		& \multicolumn{1}{l}{Shelter location (and allocation)} & Selecting among potential locations  & \multicolumn{1}{l}{Ensuring accesibility} \\
		&       &       & \multicolumn{1}{l}{Shelter utilization} \\
		&       &       & \multicolumn{1}{l}{Infrastructure (Risk)} \\
		&       &       & \multicolumn{1}{l}{Behavioral factors} \\
		\cline{2-4}
		& \multicolumn{1}{l}{Prepositioning} & Locating point of distributions  & \multicolumn{1}{l}{Budget} \\
		&       & Provisioning (of supplies) & \multicolumn{1}{l}{Accessibility} \\
		&       & Related supply chain decisions  & \multicolumn{1}{l}{Fairness} \\
		&       &    (supplier selection, allocation and routing)   & \multicolumn{1}{l}{Speed} \\
		&       &       & \multicolumn{1}{l}{Efficiency} \\
		\cline{2-4}
		& \multicolumn{1}{l}{Supply chain (procurement) planning } & Contract design  & \multicolumn{1}{l}{Quality} \\
			&  &   &  Efficiency\\
		\hline
		Response & \multicolumn{1}{l}{Damage assessment } & Demand assessment & \multicolumn{1}{l}{Speed} \\
		&       & Infrastructure assessment & \multicolumn{1}{l}{Accuracy} \\
		\cline{2-4}
		& \multicolumn{1}{l}{Search and rescue operations} & Team formation and allocation  & \multicolumn{1}{l}{Speed} \\
		&       &       & \multicolumn{1}{l}{Fairness} \\
		&       &       & \multicolumn{1}{l}{Capacity} \\
		\cline{2-4}
		& \multicolumn{1}{l}{Evacuation } & Location & \multicolumn{1}{l}{Speed} \\
		&       & Allocation & \multicolumn{1}{l}{Risk} \\
		&       & Routing &  \\
		\cline{2-4}
		& \multicolumn{1}{l}{Shelter management} & Location & \multicolumn{1}{l}{Physical environment} \\
		&       & Allocation & \multicolumn{1}{l}{Risk} \\
		&       & Related supply chain decisions  & \multicolumn{1}{l}{Accessibility} \\
		&       &    (distribution, routing)   & \multicolumn{1}{l}{Fairness} \\
		&       &       & \multicolumn{1}{l}{Utilization} \\
		\cline{2-4}
		& \multicolumn{1}{l}{Donation management/ } & Allocation & \multicolumn{1}{l}{Fairness } \\
		&   Resource allocation    & Routing  & \multicolumn{1}{l}{Efficiency} \\
		&       & Inventory Management  &  \\
		\hline
		Recovery & \multicolumn{1}{l}{Debris management } & Network design & \\
		&       & Prioritization and scheduling  &  \\
			&  & (deciding which nodes/arcs to clean first)  &  \\
		\hline
	\end{tabular}}%
	\label{tab:prob_BYKOK}%
\end{table}%

Not all humanitarian operations are triggered by challenges stemming from a single well-defined event. There are crises that can not be attributed to a single cause e.g., famine in under-developed countries. There are well-established efforts in the development logistics literature to alleviate the effects of such crises. Some examples are global health projects for increasing access to health coverage  and fighting diseases that occur in low and middle-income countries on a wide-scale, such as malaria and AIDS, through distribution of effective tools and/or medication.  Vaccine development and distribution to poorer regions as well as distribution of other basic needs to the deprived populations: food aid distribution \citep{foodaid2015_BYKOK, mahmoudi2022decision_BYKOK}, clean water network design and distribution \citep{LAPORTE2022_BYKOK}, energy, education and hygiene provision are also widely considered. A recent trend is utilising cash and voucher distribution whenever possible, since it is a method that respects human dignity, avoids complications of relief item logistics and supports the local market \citep{karsu2019refugee_BYKOK}. 

The recent COVID-19 pandemic has also motivated a wide range of HL applications such as personnel protective equipment allocation and distribution, frontline workforce planning and system design for testing, tracing and vaccination \citep{FARAHANI20231_BKOK}.

The HL literature also integrates newer technologies to the delivery systems: There has been recent attempts to  use drones in the last mile distribution as they constitute a convenient tool to reach remote areas in short time \citep[examples include delivery of blood samples, vaccines and food aid; see also][]{GENTILI2022108057_BKOK,GHELICHI2021105443_BKOK,alfandari2022tailored_BKOK}.

OR offers decision support for humanitarian settings based on a wide range of quantitative and qualitative tools. Mathematical modelling and optimisation is used in almost all problems arising in HL to make the related location, allocation, routing and network design decisions. The models are shaped by the priorities in the phase and constraints imposed by the physical infrastructure, resource availability as well as the social, economic and cultural environment. 
As the underlying technical problems are difficult to solve, various mathheuristic and metaheuristic approaches are employed. There is also an increasing trend in using system dynamics \citep{besiou2021system_BYKOK} and empirical analysis \citep{pedraza2016empirically_BYKOK}.

There is no one-size-fits-all methodology but some key properties require specific methods to be used. In most relief logistics problems the environment is highly stochastic, calling for applications of forecasting and stochastic programming. The uncertain factors include but are not limited to the number of affected individuals, the extent of the effect, types of needs, and usability status of the infrastructure and other resources. Multiple stakeholders and conflicting criteria are involved, requiring multicriteria decision making \citep{ferrer2018multi_BYKOK} approaches. Unlike commercial logistics systems, fairness is a key concern in humanitarian settings. Fairness or equity, however, is hard-to-quantify and is context dependent: a rule that is considered fair under some circumstances may not be deemed so in others. The policy makers may want to prioritise beneficiaries based on attributes such as socio-economic status and hence ensure vertical equity or may consider them indistinguishable and seek horizontal equity \citep{KARSU2015_BYKOK}. 
  
HL has been receiving attention with an increasing rate, which led to many review studies that the interested reader  can refer to: see, e.g., \cite{luis2012disaster_BYKOK, CELIK201647_BYKOK, aringhieri2017emergency_BYKOK, besiou2018or_BYKOK,8935364_BYKOK,DONMEZ20211_BYKOK}. See also \cite{kunz2017relevance_BYKOK} for a discussion on how to make humanitarian research more impactful for humanitarian organisations and beneficiaries. 
	
Next we categorise the future of the humanitarian applications to motivate and direct new researchers:
\begin{itemize}[noitemsep]
\item 	OR is responsive to the difficulties the world is facing and humanitarian challenges are no exception to this. The recent COVID-19 pandemic has shown that relief logistics can be applied in health-related crisis management to provide quick, effective, efficient and fair responses to health care problems.  WHO ``urges countries to build a fairer, healthier world post-COVID-19" and this is doable with good humanitarian practices relying on OR.  The recent efforts in designing fair and efficient systems using OR would contribute in addressing inequities in health and welfare, which have been exacerbated by the pandemics. We believe that there is still room for improvement in adopting a holistic approach and conducting multidisciplinary work when designing such systems. One example is the vaccine implementation and roll-out problem: conceptualising  this problem as a sole logistics problem may not be the best practice as the success of any design highly depends on human behaviour. People have different views, risk attitudes and preferences over available options, which affects how any proposed policy will perform.  Incorporating such behavioural factors is an important yet scarcely studied issue. 
\item The underlying technical problems in the HL domain are hard to solve due to uncertainty in various parts of the system, lack of (reliable) data and multiple criteria that are involved. Moreover, a significant portion of these problems are combinatorial optimisation problems, i.e. they require choosing from a prohibitively large set of solutions that are implicitly defined by constraints of the system. Therefore advances in OR methodology to obtain better, quicker solutions to optimisation problems, and in data analytics on handling big data such as the one obtained through geographic information systems (GIS), would pave the way for quicker and better response. Effective data analysis would especially help when learning from past practice. Indeed, lessons learned from humanitarian supply chain practice can also be used in managing supply chain disruptions in other sectors, as discussed in \citep{kovacs2021lessons_BYKOK}. 
\item UN's Sustainable Development Goals emphasise the global challenges faced including  poverty, inequality, climate change, environmental degradation, peace and justice \citep{SDG_BYKOK}. As stated in \citep{report_BYKOK}, ``Increases in extreme weather events and  climate change can compound risks of international food shocks, water insecurity, conflict and other humanitarian emergencies and crises. Difficulty of access to critical resources such water and food may trigger migrations or	exacerbate conflict risks.''  All these areas are, by definition, related to humanitarian operations, hence humanitarian logistics has a lot to offer in these domains. 
\item The Turkey/Syria earthquakes in February 2023 have clearly demonstrated the importance of effective coordination and strategic planning. Thus, we would like to emphasise the need for collaborative research that brings field expertise (of e.g,. municipalities, NGOs and volunteers) and academic know-how together. 
\end{itemize}

\subsection[E-commerce (Charlotte~Köhler \& Tom~Van~Woensel)]{E-commerce\protect\footnote{This subsection was written by Charlotte~Köhler and Tom~Van~Woensel.}}
\label{sec:Ecommerce}

\subsubsection*{What is E-Commerce about?}
E-commerce deals with the transactions of goods and services through online communications (computers, tablets, smartphones, etc.). Both business-to-business (B2B) and business-to-consumer (B2C) realisations are observed in practice. In B2B, companies operate their supply chains through online networks. In B2C, products and services are sold directly to consumers. E-commerce sales steadily increased for years and amounted to \$5,211 billion worldwide in 2021, with the pandemic being a major contributor.\footnote{https://bit.ly/3dfiwDW, 2022-08-08}$^{,}$\footnote{https://bit.ly/3SzAWzf, 2022-08-08}

\textit{E-fulfilment} describes all fulfilment activities for e-commerce. All necessary steps for a customer to receive an order after placing are thus referred to as the e-fulfilment process. Due to the nature of the e-commerce domain, these e-fulfilment activities often occur in a city context \citep{savelsbergh201650th_CKTVW}. E-fulfilment processes are planning intensive, and creating a profitable business in this environment is challenging. Customer service expectations are high, however, and the customer is more and more in the lead on how and where their orders need to be delivered \citep[the ``logsumer'' takes an active role in time, price, quality, and sustainability decisions of logistic services][]{DHL_CKTVW}.

The e-fulfilment process can be divided into three steps, namely (\textit{i}) order acceptance, (\textit{ii}) order assembly, and (\textit{iii}) order delivery \citep{campbell2005decision_CKTVW}. For most online companies, these steps take place separately, one after the other. However, new on-demand companies have considerably shortened lead times and perform these steps simultaneously \citep{wassmuth2022demand_CKTVW}.

During \textit{order acceptance}, customer requests arrive on a retailer's website and ask for service. As fulfilment capacities are limited (for example, delivery capacities), the retailer wants to accept the most profitable subset of all customer requests. However, customer requests arrive one at a time. Thus, the retailer does not know the total delivery costs until all customers are accepted and the final delivery route is planned. In addition, when a request is accepted, the retailer does not know whether requests with higher revenues will arrive afterward for which capacity should have been reserved. To estimate costs, vehicle routing methods are adapted for usage as customer acceptance mechanisms \citep[e.g.,][]{ehmke2014customer_CKTVW,kohler2019evaluation_CKTVW}). Revenue management methods are used to allocate capacities to high revenue requests \citep[e.g.,][]{cleophas2014deliveries_CKTVW,klein2019differentiated_CKTVW}). However, since decisions in the online environment must be made instantly, the use of complex and, thus, computationally intensive solution methods is limited. 

The warehouse picking and consolidating ordered goods are summarised under \textit{order assembly}. Before this, the retailer must decide on the location and design of the warehouses. Choosing the location is closely linked to the fulfilment capacity of the retailer and must be well-planned. The design of the warehouse determines the efficiency of picking the ordered items. Finding efficient picking strategies to reduce retailer costs is studied in, for example, \cite{Schiffer2022_CA_MB}. Lastly, the retailer must determine the optimal stock level of items. Given the short lead times in e-commerce, this task must be completed before customer requests arrive. The task is closely linked to the research field of inventory management, where techniques such as forecasting \citep{ulrich2021distributional_TVWCK} or artificial intelligence \citep{albayrak2023applications_TVWCK} are commonly used to address this challenge effectively.

For \textit{order delivery}, routes are planned for all accepted orders. For e-commerce, the last-mile delivery is usually towards the customer's location, i.e., the consumer's home or company site \citep{agatz2008fulfillment_CKTVW}, leading to a magnitude of fragmented delivery locations with small drop sizes. Significant challenges arise from how these last-mile deliveries (routes) are designed. Delivery route planning is closely related to the established field of vehicle routing, and approaches are being adapted for use in e-fulfilment \citep[e.g.,][]{emec2016adaptive_CKTVW}. Two-echelon routing systems are often considered to maintain economies of scale and satisfy the emission zone requirements in the cities \citep{sluijk2022chance_CKTVW,sluijk2022two_CKTVW}). In most cases, delivery is made by conventional delivery vehicles. However, individual retailers are also starting to bring orders to customers in the city centre using bikes. We also see drones \citep[e.g.,][]{ulmer2018same_CKTVW,dayarian2020crowdshipping_CKTVW}) and robots \citep[e.g.,][]{simoni2020optimization_CKTVW}.

\subsubsection*{E-fulfilment Challenges}
E-fulfilment processes present several challenges. 
For \textit{unattended deliveries}, delivery is possible without the customer being present. Pick-up point delivery enhances the efficiency of the delivery operations via consolidation opportunities. Consumers can also find it a more convenient delivery option than waiting for the delivery at home. There is a need for incentive mechanisms to increase the attractiveness of pick-up points (e.g., reduced delivery price). \cite{Galiullina2022_CKTVW} study this problem as a trade-off between routing cost savings gained from steering the customer demand and the investments required to influence customer behaviour. Another challenge is to find the optimal locations for pick-up points, such that delivery costs are minimised and customers still have convenient access, which is, for example, considered in  \cite{lin2020last_CKTVW} and \cite{wang2020locating_CKTVW}. The customer must accept the delivery herself for \textit{attended deliveries}, e.g., to prevent grocery spoiling. To avoid delivery failures, the customer and the retailer usually agree on a delivery time window.

Customers expect short time windows, which increase the retailer's delivery costs  \citep{kohler2020flexible_CKTVW}. As the time windows assignment to orders is crucial for the retailer's profitability, several approaches consider balancing demand along the offered time windows. One possibility is to withhold specific time windows from customers and only offer a subset of beneficial time windows. \cite{campbell2005decision_CKTVW} and \cite{cleophas2014deliveries_CKTVW} consider routing costs and customer value and only offer time windows to customers that are expected to maximise the profit. Another possibility is to assign prices to time windows to nudge customers to specific time window options \citep{campbell2006incentive_CKTVW,yang2016choice_CKTVW,klein2019differentiated_CKTVW}. Some approaches consider adapting the time window design to increase routing flexibility. \cite{kohler2020flexible_CKTVW} only offer short time windows to customers when it does not impact the routing costs too much, and \cite{strauss2021dynamic_CKTVW} hand out time window bundles to customers that are only narrowed down to one option once more customers requests are known. 

Recently, many online retailers began offering \textit{on-demand deliveries}, so customers can receive their orders the same day (some grocery stores promise delivery times within a few minutes\footnote{https://bit.ly/3B1H1ww, 2022-09-09}). Shortening lead times poses another challenge as there is almost no time for planning or consolidation of orders available. The approach presented by \cite{klapp2020request_CKTVW} hence supports retailers in deciding which customers can be promised an immediate delivery and which can only be served from the next delivery day. \cite{ulmer2018same_CKTVW} investigate how the number of same-day deliveries can be increased if delivery is not only done by vehicles but additionally by drones. In \cite{banerjee2022fleet_CKTVW}, the authors examine how retailers must allocate their delivery capacity to cover same-day delivery needs per service area. 

For delivery in an urban context, high demand in densely populated areas often goes hand in hand with high traffic and unreliable travel times, and vice versa. \cite{ehmke2014customer_CKTVW} therefore create acceptance mechanisms that present the customer with a time window offer that is as reliable as possible so that the customer does not notice an unforeseen change in travel times. \cite{kohler2019evaluation_CKTVW} test the suitability of customer acceptance mechanisms for more and less densely populated areas to derive how well different routing mechanisms approximate delivery times. 

Another ongoing challenge is the increasing prevalence of customers being granted the option to \textit{return ordered items} free of charge by many companies. As a result, the e-fulfillment process expands beyond the three steps outlined earlier to include the management of returns. Despite the typically high return rates that result in substantial additional costs for retailers, offering a return option is still profitable due to the subsequent improvement in customer satisfaction and retention \citep{rintamaki2021customers_TVWCK}. The management of returns can be perceived as reverse order delivery, leading to routing challenges related to those presented earlier. To mitigate costs, several studies, such as \citet{mahar2017store_TVWCK} and \citet{yan2022whether_TVWCK}, explore the implementation of in-store returns.

\subsubsection*{Operational Research challenges: Time, Timing, and Data}

The \textit{time} dimension involves all dimensions to how key elements are (conceptually) modelled with regards to the time (e.g., travel times or handling times). Identifying the time features in modelling and solution methodologies are essential qualifiers for realistic model representations. 

The \textit{timing} dimension involves all actions at a particular point or in a period when something happens (e.g., a new order arrives). Timing considers synchronisation issues where, for example, vehicles need to meet at a certain point in time and geographical location. \cite{drexl2012synchronization_CKTVW} presents a survey of vehicle routing problems with multiple synchronisation constraints. Synchronisation requirements between the vehicles relate to spatial, temporal, and load aspects. Synchronisation is a challenge, for example, in heterogeneous fleets \citep{ulmer2018same_CKTVW} or, in the case of battery-powered vehicles that must be charged in time. 
\begin{itemize}[noitemsep]
    \item \textit{Offline} means that we do the planning and scheduling before the execution, often assigned to tactical planning. Data is estimated (forecast) based on past observations, and the operations are planned based on that. For example, \cite{agatz2011time_CKTVW} use expected demand to decide which time windows should be offered within different parts of the delivery area. \cite{lang2021anticipative_CKTVW} propose a preparation offline phase that serves as input to speed up decisions during later online customer acceptance. The data considered could be either time-independent (i.e., independent of time) or time-dependent (i.e., the data has a time-stamp). For example, travel times can be modelled time-independent (i.e., constant speed) or time-dependent \citep[e.g.,][]{spliet2018time_CKTVW}.
    \item \textit{Online} refers to the optimisation in real-time, where revealing new data and planning and scheduling operations happen simultaneously. The terms ``dynamic'' or ``operational planning'' is also often used. As time is critical in online planning, methods are always limited by their solution time. Instead of finding a routing solution, delivery costs are approximated \citep[e.g.,][]{yang2017approximate_CKTVW,lebedev2021dynamic_CKTVW} or a simple routing heuristic is applied \citep[e.g.,][]{mackert2019choice_CKTVW,klein2018model_CKTVW}). Alternatively, customer choice is estimated simply \citep[e.g.,][]{campbell2006incentive_CKTVW} instead of complex and time-consuming customer choice modelling. \cite{vanderhagen2022machine_CKTVW} uses a machine learning approach to fasten up feasibility checks of time windows offered during order acceptance. 
\end{itemize}

The \textit{data dimension} refers to how the data and observations are modelled. The data can be handled deterministic or stochastic, or we observe the realised data. Most models assume deterministic data and build their solution approach around this notion. More and more researchers, however, recognise the challenge of adequately representing reality in their models. \cite{yang2016choice_CKTVW} use booking data of an online grocer to estimate realistic customer behaviour. \cite{kohler2022data_CKTVW} investigate how to accept high revenue requests by applying a sampling procedure with booking data from an e-grocer in Germany. 

\subsubsection*{Relevant literature}
\cite{agatz2013revenue_CKTVW} provide the first overview of how retailers can manage e-fulfilment processes. A recent review on e-fulfilment for attended home deliveries can be found in \cite{wassmuth2022demand_CKTVW}. We refer the reader to \cite{fleckensteinRecentAdvancesIntegrating2022_AKSJF} and \cite{snoeck2020revenue_CKTVW} for a focus on routing and revenue management methods in e-fulfilment, respectively.

\subsection[Education (Jill~Johnes)]{Education\protect\footnote{This subsection was written by Jill~Johnes.}}
\label{sec:Education}

Education spans activity from kindergarten, through primary and secondary schooling, to higher education. The earlier years of education are often compulsory reflecting the premise that an educated workforce is crucial to economic performance. The extent to which education is publicly funded varies from one level of education to another, as well as from one country to another depending on the local view concerning the social return on investment. Public funding for education alongside the role education and training play in the performance of an economy therefore make education a prime context for application of operational research (OR) tools. This section provides a brief overview of some of the main areas.

Many OR methods can be useful to the policy maker for macro-planning and financial allocation purposes. Forecasting student numbers can be done using Markov chain models \citep{Nicholls2009-vc_JJ,Brezavscek2017-cj_JJ} or machine learning (ML) and artificial intelligence (AI) \citep{Yan2021_JJ} – the importance of AI applications to education will be further expanded later. Allocation of finances is typically supported by multi-objective decision analysis \citep{Cobacho2010-mn_JJ}. 

One important aspect of resource allocation relates to the efficient use of resources. Availability of published education data in many countries provides an opportunity to examine the “black box” of education production. Consequently, there is a long-standing literature surrounding efficiency in education, typically a not-for-profit context where conventional measures of performance are inappropriate. 

Early studies of efficiency in higher education applied deterministic ordinary least squares methods to university-level data to examine efficiency in the production of specific outputs \citep{Jauch1975-hd_JJ,Johnes1990-zt_JJ} while schools adopted multilevel modelling methods to derive performance insights from pupil-level as opposed to school-level data \citep{Woodhouse1988-fr_JJ}. But the multi-product nature of production in education establishments means that looking at inputs separately provides only a partial picture. The tools of multiple-criteria decision analysis such as principle components, the analytic hierarchy process and co-plot have therefore been adopted to examine and visualise the many dimensions more easily \citep{Johnes1996-pk_JJ,Paucar-Caceres2005-kz_JJ,Mar-Molinero2007-pi_JJ}.

Two frontier estimation approaches to analysing efficiency, both of which derive from Farrell (1957), have evolved to address various shortcomings of early approaches. The non-parametric data envelopment analysis (DEA) easily handles the multi-input multi-output nature of production observed in education and provides easily-interpreted measures of efficiency \citep{Charnes1978-nm_SL}. DEA shows each observation in its best possible light (in efficiency terms) by computing a distinct set of input and output weights. This permits the derivation of benchmark observations for each inefficient institution, i.e., the establishment(s) the observation should be looking to emulate to become more efficient. Non-parametric frontier estimation techniques have been applied in the context of education at all levels, providing management information at the institution level, and policy insights at the macro-level \citep{Thanassoulis2011-yg_JJ,Portela2012-bw_JJ,Burney2013-iw_JJ}.

Network DEA provides a more forensic examination of the “black box” \citep{Fare2000-wo_JJ} by breaking down the production process into its component parts, and overall efficiency can be decomposed into efficiency in each of the stages \citep{Wang2019-qo_JJ,Lee2022-an_JJ}. 

When longitudinal data are available, DEA can be used to analyse changes in efficiency using the \cite{Malmquist1953-ve_JJ} productivity index which decomposes productivity change into efficiency and technological change components \cite{Wolszczak-Derlacz2018-xo_JJ}. The method can be used to make comparisons between groups rather than (or as well as) between time periods \citep{Aparicio2017-dh_SL}.

The deterministic non-parametric nature of DEA has been addressed in numerous extensions including by introducing bootstrapping and significance tests \citep{Johnes2006-id_JJ,Essid2010-at_JJ,Papadimitriou2019-ap_JJ}. Second stage analyses which examine the determinants of efficiency also abound \citep{Haug2017-bv_JJ}. This approach is only valid if the hypothesis of separability holds i.e. the variables used in the second stage should only influence the efficiency scores and not the determination of the efficiency frontier \citep{Simar2011-oz_JJ}. The development of separability tests \citep{Daraio2018-ut_JJ} and the robust conditional estimation approach address these issues \citep{Daraio2007-qr_JJ}; their application in education provide more robust and insightful results \citep{Lopez-Torres2021-yl_JJ}.

Stochastic frontier analysis (SFA) provides both parameter estimates (with significance tests) and efficiency estimates which allow for stochastic errors \citep{Aigner1977-df_JJ,Meeusen1977-gq_JJ,Jondrow1982-fq_JJ}. Compared to DEA it is more difficult to model multi-input, multi-output production; most SFA applications in education therefore focus on cost efficiency \citep{Agasisti2016-bg_JJ}, or a single output model \citep{Kirjavainen2012-cr_JJ}, although there are some exceptions \citep{Abbott2009-xv_JJ,Johnes2014-jm_JJ}. The parameter estimates have made SFA popular in the cost function context as scope and scale economies can be estimated and these have useful policy implications \citep{Johnes2005-ue_JJ,Johnes2013-eg_JJ}.

In its basic form, SFA parameter estimates apply to every observation in the dataset. Extensions of the technique include latent class SFA and random parameters SFA which allow the parameters to vary by specific groups (latent class) or by each observation (random parameter). These approaches benefit from the advantages of DEA and SFA although are computationally demanding but have been applied in education to interesting effect \citep{Johnes2011-hd_JJ,Johnes2016-di_JJ}.

The interested reader is referred to comprehensive reviews of the relevant literature \citep{Kao2014-jt_SL,Thanassoulis2016-du_JJ,Witte2017-xg_JJ,Johnes2022-jv_JJ}.

All these OR methods can be applied in the contexts of macro- and micro-level planning and budget allocation. One area at the micro-level for which OR techniques are useful is timetabling. 
Timetabling of examinations and/or teaching is most complex at secondary and tertiary levels and can be viewed as a scheduling problem whereby resources, limited in supply, are allocated to a constrained number of times and locations, with the allocation satisfying stated objectives. Timetabling differs from scheduling in that the resources (staff members) are typically specified in advance rather than being a part of the allocation problem; and while scheduling aims to minimise costs, the objective of timetabling is to realise desirable objectives (e.g., no clashes) as closely as possible
\citep{Petrovic2004-ly_JJ}. Timetablers face both hard and soft constraints in constructing the timetable \citep{Asmuni2009-yb_JJ} and this is therefore a problem which lends itself to solution by various possible OR techniques in the field of combinatorial optimisation. The main approaches are briefly summarised below.

Mathematical programming (particularly integer linear programming) is commonly used in timetabling \citep{Cataldo2017-la_JJ} but often leads to  computationally demanding problems. Heuristics (see below) are introduced for increased efficiency \citep{Dimopoulou2001-rz_JJ}. Case-based reasoning approaches use a past solution (stored in the case base) as the starting point for a new timetable and use similarity measures to identify the optimal solutions \citep{Burke2006-wo_JJ}. These approaches are often problem-specific making them non-transferable. Their computational demands can be addressed by using heuristics \citep{Petrovic2007-ls_JJ}. The multi-criteria approach assumes that there are solutions to the timetabling problem satisfying the hard constraints and then the quality of these solutions is assessed on the basis of how well each one satisfies the soft constraints \citep{Burke2002-dp_JJ}. As with other methods it is often combined with heuristics. 

Heuristics are an increasingly common method for application to timetabling either on their own or in combination with other methods. Low level construction heuristics include largest degree, largest weighted degree, largest colour degree, largest enrolment, saturation degree and random. Extensions include meta-heuristics which work in the search space guiding neighbourhood moves to a solution \citep{Qu2015-oi_JJ}; fuzzy heuristics which can find a best approach in the initial timetable construction phase \citep{Asmuni2009-yb_JJ}; and hyper-heuristics which find or generate appropriate heuristics to solve complex search problems as encountered in timetabling \citep{Qu2015-oi_JJ}. Given their focus, hyper-heuristics have the potential to provide more generalised solutions to timetabling problems than other approaches \citep[see][for a review]{Pillay2016-pp_JJ}.

The interested reader is referred to reviews of educational timetabling approaches \citep{Oude_Vrielink2019-qh_JJ,Tan2021-ba_JJ}.

Finally, an emerging area of interest is the application of AI and ML to education. AI and ML are, as already highlighted,  useful for forecasting as they can analyse rich data on, for example, student numbers, retention, achievement, teaching and quality to derive better predictions and/or understanding of the challenges \citep{Alyahyan2020-eh_JJ,Bates2020-bc_JJ,Teng2022-ra_JJ}. They can also be used in the teaching and learning process itself by personalising each student’s experience for example through use of chatbots, by creating exercises for students which address their weaknesses, and by reviewing assessments highlighting strengths and weaknesses \citep{Teng2022-ra_JJ}. In the growing distance learning education arena where it is more difficult to manage participants who have more freedom to learn when they want and may encounter more distractions, AI can be used to support teachers in gauging student engagement. Thus AI algorithms can be used to develop an online education classroom management system \citep{Wang2021-rm_JJ}. AI and ML have much to offer in education but their potential across all disciplines has yet to be properly explored \citep{Bates2020-bc_JJ}. See \cite{Zawacki-Richter2019-ki_JJ} for further literature.

\subsection[Environment (Judit~Lienert)]{Environment\protect\footnote{This subsection was written by Judit~Lienert.}}
\label{sec:Environment}

Environmental problems are at the centre of societal concerns, and of many research activities, also in Operational Research (OR). It is impossible to comprehensively present this literature. Instead, we first introduce characteristics of environmental problems, then present some insights from specific OR fields, mainly citing review articles. Thereafter, we discuss \textit{Decision Analysis} (\S\ref{sec:Decision_analysis}) methods applied to environmental problems.

Environmental problems are usually multi-faceted and complex \citep{French2005-pc_JL,Gregory2012-zh_JL,Reichert2015-jd_JL}. Since 50 years, such public policy issues are known as ``wicked problems'' \citep{Rittel1973-wn_JL}. In many environmental cases, uncertainties are high. It may be difficult to establish scientific knowledge and adequately model environmental systems. They usually span all sustainability dimensions, which requires making trade-offs between achieving environmental, economic, and societal objectives. Various decision-makers and stakeholders with different world-views are affected, sparking conflicts of interest. Any action may have irreversible or far-reaching consequences over long time horizons. Additionally to the temporal dimension, spatial considerations over varying geographic regions may be important. As wicked problems are typically unique, we might need to find new solutions in each case. OR methods can be highly suitable to disentangle and structure complex environmental problems, and can certainly contribute to problem solving. Below, we present some viewpoints.

\textit{Soft OR} methodologies, and \textit{problem structuring methods} (PSMs; see also \S\ref{sec:Soft_OR_and_problem_structuring_methods}) have been developed to tackle complex real-world problems in interaction with stakeholders \citep{Rosenhead2001-ai_JL,Smith2019-pw_JL}. However, most (review) articles are not specific to environmental problems. Using an applied example, \cite{White2009-dd_JL} explored the potential of soft OR for a city development case. \cite{Marttunen2017-yk_JL} reviewed the combination of PSMs with Multi-Criteria Decision Analysis (MCDA) methods. More complex PSMs seem to be under-utilised, suggesting that their benefits cannot sufficiently inform real-world issues, including environmental decision-making. Similarly, \cite{French2022-kr_JL} argued that literature of quantitative and qualitative OR approaches has developed in silos, and that an intertwined, cyclic understanding of soft and hard OR methods is needed to address complex problems. This author was also concerned that behavioural issues are less well understood in qualitative compared to quantitative model building. Related to problem structuring, \textit{stakeholder analysis and participation} is central to environmental problems. Such research is recently gaining increased interest by OR \citep{De_Gooyert2017-yt_JL,Gregory2020-zf_JL,Hermans2009-rp_JL}. \textit{Behavioural OR} (BOR; \S\ref{sec:Behavioural_OR}) is also gaining momentum \citep{Franco2021-sm_JL}. BOR strongly focuses on interventions, and could increase the understanding of societal and psychological issues in environmental problems. However, to date an environmental perspective is rarely taken. One exception is a conceptual paper about behavioural issues in environmental modelling \citep{Hamalainen2015-fv_JL}. A meta-analysis of 61 environmental and energy cases analysed patterns and \textit{biases} that may occur in the problem structuring phase of decision-making \citep{Marttunen2018-qs_JL}.

\textit{Sustainable supply chains} (see also \S\ref{sec:Supply_chain_management}) have been recently reviewed by \cite{Barbosa-Povoa2018-dw_JL}. These authors took a multi-stakeholder perspective along the supply chain to achieve sustainability goals. They found a predominance of \textit{optimisation} methods applied to strategic decision levels. Most of the 220 reviewed articles focused on economic and environmental aspects, leaving behind the social aspects. Similarly, another review focused on \textit{combinatorial optimisation} (\S\ref{sec:Combinatorial_optimisation}), integrating \textit{reverse logistics} (see also \S\ref{sec:Logistics}) and waste management \citep{Van_Engeland2020-qq_JL}. Among other aspects, the authors emphasised the importance of environmental, social and performance indicators, and stakeholder integration, when dealing with flows of waste products. Taking a life-cycle perspective, usually addressed with \textit{life cycle sustainability assessment} (LCSA), \cite{Thies2019-by_JL} reviewed advanced OR methods for \textit{sustainability assessment of products}. While most articles used ecological indicators, the integration of economic and social indicators is emerging. They concluded that improved systematic procedures for uncertainty treatment are needed, and better integration of qualitative social indicators as well as spatially explicit data.

Other authors reviewed specific OR methods. For instance, \cite{Zhou2018-et_JL} reviewed \textit{Data Envelopment Analysis} (DEA; \S\ref{sec:Data_envelopment_analysis}) for sustainability assessments. Again, economic and environmental measures were well included, but the literature lacked social measures such as customer satisfaction. New DEA methods should be developed that include \textit{social network relationships}. \textit{Mathematical programming} and \textit{optimisation} methods to support biodiversity protection were reviewed by \cite{Billionnet2013-hw_JL}. Some of these difficult combinatorial optimisation problems were well solved, but further research is needed to satisfactorily address real-world biodiversity issues. For conservation management, spatial aspects are central, for example creating biological corridors in the landscape to increase biodiversity. Future research should include the temporal dimension and needs of practitioners. \textit{Robust optimisation} (\S\ref{sec:Stochastic_models}) could be a research avenue to handle uncertainty. A review of invasive species also took a mathematical perspective \citep{Buyuktahtakin2018-bb_JL}. Among other conclusions, research should develop more realistic models to capture spatial and temporal dynamics of invasive species, improve uncertainty treatment and coordination among stakeholders, and include holistic approaches for addressing trade-offs between conservation management and costs of such programs.

\textit{Multi-criteria decision analysis} (MCDA; \S\ref{sec:Decision_analysis}) provides a rich literature addressing environmental decision problems. \cite{French2005-pc_JL} discussed properties of wicked environmental problems from a conceptual point of view (introduced above), and implications for decision support. \cite{Cinelli2014-gf_JL} analysed MCDA methods for sustainability assessments. They voiced some concern that choosing the MCDA methods is rather based on preferences, not analytic considerations. Indeed, text-mining of 3,000 articles provided little evidence that particular environmental application fields used certain methods more frequently, possibly because researchers are unaware of specific method merits \citep{Cegan2017-kf_JL}. To overcome this, \cite{Cinelli2014-gf_JL} classified five MCDA methods using ten criteria important for sustainability assessments, e.g., uncertainty management and testing robustness of results, software, and user-friendliness. We know of two general articles for systematically choosing a suitable MCDA method \citep{Cinelli2020-lq_JL,Roy2013-ii_JL}. Several articles reviewed \textit{decision support systems} (DSS) to identify features and best practices for supporting environmental problems \citep{Mustajoki2017-eq_JL,Walling2020-kn_JL}. Moreover, there are many reviews of MCDA applied to a specific environmental field, but only few were published in OR journals \citep[e.g.,][]{Colapinto2020-mc_JL,Kandakoglu2019-pm_JL}. There is a pronounced increase of articles applying MCDA in all environmental areas \citep[e.g., water, air, energy, natural resources, and waste management;][]{Cegan2017-kf_JL,Huang2011-hy_JL}. Below, we introduce some important findings from \textit{decision analysis}.

Some authors defined frameworks for environmental assessments taking a method perspective. \cite{Gregory2012-zh_JL} proposed \textit{structured decision making} (SDM) to tackle real-world environmental decision problems. Based on \textit{multi-attribute value theory} (MAVT), SDM can be applied without much (mathematical) formalisation. This textbook discusses many practical environmental issues, highlighting solutions from international decision cases. \cite{Reichert2015-jd_JL} proposed a framework for environmental decisions that emphasises uncertainty of scientific knowledge and societal preferences. They argued that theoretical requirements are best met by combining \textit{multi-attribute utility theory} (MAUT) with \textit{scenario planning} and \textit{probability theory}, illustrated with a river management case. Scenario planning has been advocated by various authors for tackling wicked problems \citep{Wright2019-av_JL}. The combination of scenario planning with MCDA has been reviewed by \cite{Stewart2013-ez_JL}, and applied to e.g., nuclear remediation management \citep{Geldermann2009-cz_JL}, coastal engineering under climate change \citep{Karvetski2011-sk_JL}, or water infrastructure planning \citep{Scholten2015-ja_JL}. Scenario analysis has also been combined with probabilistic statements and \textit{mathematical optimisation} for \textit{risk assessment} (see also \S\ref{sec:Risk_analysis}) of nuclear waste repositories \citep{Salo2022-yz_JL}. A climate policy review illustrates the importance of integrating various OR methods to effectively support decision-making \citep{Doukas2020-vl_JL}. The currently predominant evaluation of policy strategies with \textit{climate-economy} or \textit{integrated assessment models} (IAMs) fails to incorporate all relevant uncertainties and stakeholders, and sufficiently address system complexity. These authors proposed integrated approaches, including participatory stakeholder processes with \textit{fuzzy cognitive maps}, combined with MCDA and \textit{portfolio analysis} (PA). PA is especially useful as meta-analysis, and has been reviewed by \cite{Liesio2021-zv_JL}. A PA-framework for environmental decision-making has been proposed by \cite{Lahtinen2017-sh_JL}.

To address \textit{spatial aspects} of environmental problems, \textit{geographic information systems} (GIS) are often combined with MCDA, sometimes also developing DSSs \citep{Keenan2019-mp_JL}. \textit{Risk analysis} (\S\ref{sec:Risk_analysis}) and OR research increasingly focuses on spatial planning \citep{Ferretti2019-jm_JL,Malczewski2020-gs_JL}. One example is the axiomatic foundation of spatial multi-attribute value functions \citep{Harju2019-mb_KVRH,Keller2019-uq_JL}.

Many reviews found that \textit{stakeholder integration} throughout the decision-making process was insufficiently considered, e.g., in flood risk management \citep{Madruga_de_Brito2016-ul_JL} or nature conservation \citep{Esmail2018-wp_JL}. This reflects generally found deficits in \textit{problem structuring} (\S\ref{sec:Soft_OR_and_problem_structuring_methods}), for instance insufficient consideration of social objectives \citep{Kandakoglu2019-pm_JL}, or systematic underestimation of the importance of economic objectives \citep{Marttunen2018-qs_JL,Walling2020-kn_JL}. Moreover, there is a tendency to choose too many objectives in environmental cases \citep{Diaz-Balteiro2017-tl_JL}, potentially inducing \textit{biases} in later stages of MCDA \citep{Marttunen2019-nl_JL}. 

Many reviews emphasised the importance of \textit{uncertainty analyses} in environmental decisions, but this is strongly ignored in practice. One review found that only 19\% of 271 articles included uncertainty analysis, 17\% using \textit{fuzzy techniques} to capture imprecise numbers \citep{Diaz-Balteiro2017-tl_JL}. In another review, 34\% of 343 articles dealt with the imprecision of predictions, 70\% using fuzzy sets, and 20\% \textit{stochastic modelling} \citep{Kandakoglu2019-pm_JL}. In both reviews, only 20\%–30\% of the articles performed \textit{sensitivity analysis}. Additionally, only 5\% of 343 reviewed papers included \textit{temporal aspects} of the environmental decision \citep{Kandakoglu2019-pm_JL}.

As conclusion, OR researchers are widely engaging in environmental problems. Environmental problems are intriguingly complex, thus offering opportunities for inspiring research. Although our evaluation is neither comprehensive nor systematic, some general research needs appear across all OR fields. Many articles emphasised the importance of better integrating practitioners and stakeholders in environmental problems, and of better considering societal objectives. Various fields require improved methods to address the complexities of environmental problems, including appropriately dealing with many types of uncertainties, time, and space. Combining soft with hard OR, improving problem structuring, and integrating questions from behavioural OR will increase the chances of finding sustainable solutions for our worlds' environmental problems. This can also spark cross-disciplinary research over different fields of OR.

\subsection[Ethics and fairness (John~N.~Hooker)]{Ethics and fairness\protect\footnote{This subsection was written by John~N.~Hooker.}}
\label{sec:Ethics_and_fairness}

There is substantial literature on the ethical practice of operational research, surveyed in \cite{Brans2007-xi_JH}, \cite{Ormerod2013-km_JH}, \cite{Tsoukias2021-pz_JH}, and \cite{Bellenguez2023-hc_JH}.  While this is a vitally important discussion, it is useful to consider how the \mbox{science} of operational research can contribute to ethics, as well as how ethics can contribute to the practice of operational research.  It has accomplished this primarily through the development of modelling techniques and algorithms that embody ethical concepts, notably distributive justice.

An operational research model that aims simply to minimise total cost or maximise total benefit may unfairly distribute costs or benefits across stakeholders.  This concern arises in a number of application areas, including healthcare (\S\ref{sec:Healthcare}), disaster relief (\S\ref{sec:Disaster_relief_and_humanitarian_logistics}), facility location (\S\ref{sec:Location}), task assignment, telecommunications (\S\ref{sec:Telecommunications}), and machine learning (\S\ref{sec:Artificial_intelligence_machine_learning_data_science}). It
poses the problem of finding a suitable formulation of equity or fairness that can be incorporated into a mathematical model.

For example, if donated organs are allocated in the most economically efficient fashion, patients with certain medical conditions may wait far longer for a transplant than other patients \citep{McElfresh2018_JH}.  If earthquake shelters are located so as to minimise average distance from residents, persons living in less densely populated areas may have much further to travel \citep{Sibel2019inequity_JH}.  If a machine learning algorithm awards mortgage loans so as to maximise expected earnings, members of a \mbox{minority} group may find themselves unable to obtain loans even when they are financially responsible 
\citep{SaxHuaDefRadParLiu20_JH}. If traffic signals at intersections are timed to maximise traffic throughput, motorists on side streets may have to wait forever for a green light \citep{CheShaTse13_JH}.  

We provide here a brief overview of mathematical formulations of fairness that have been proposed for OR and AI models. Comprehensive treatments can be found in \cite{KARSU2015_BYKOK} and \cite{CheHoo23_JH}. In addition, \cite{OgrLusPioNacTom14_JH} review formulations developed for telecommunications and facility location, two major users of fairness models.  Recent years have seen an enormous surge of interest in fairness criteria for machine learning, many of which are surveyed in \cite{MehMorSaxLerGal22_JH}.

We suppose that the model into which one wishes to incorporate fairness allocates {\em utilities} to a collection of {\em stakeholders}, and we are concerned about the fairness of this allocation.  Utility could take the form of wealth, resources, negative cost, health outcomes, or some other type of benefit.  Stakeholders can be individuals, organisations, demographic groups, geographic regions, or other entities for which distributive justice is a concern.

Fairness models can be divided into three broad categories.  
{\em Inequality measures} are normally used to constrain the degree of inequality in solutions obtained by maximising total benefit or minimising total cost.   Some of these focus on inequalities across individuals, and others on inequalities across groups.  Various statistics for measuring the former are discussed in \cite{Cow00_JH} and \cite{JenVanKer11_JH}.  Perhaps the best known is the {\em Gini coefficient}, widely used to measure income or wealth inequality \citep{Gin12_JH,yitzhaki2013more_JH}. The {\em Hoover index} \citep{Hoov36_JH} is proportional to the relative mean deviation of utilities and represents the fraction of total utility that must be redistributed to achieve perfect equality.  Both the Gini coefficient and the Hoover index can be given linear formulations (\S\ref{sec:Linear_programming}) in an optimisation model by means of linear-fractional programming \citep{ChaCoo62_JH}. {\em Jain's index} \citep{JaiChiHaw84_JH}, well known in telecommunications, is a strictly monotone function of the coefficient of variation.

Inequality between groups, generally referred to as {\em group disparity}, is by far the most discussed type of inequality metric in the machine learning field \citep[\S\ref{sec:Artificial_intelligence_machine_learning_data_science};][]{VerRub18_JH,MehMorSaxLerGal22_JH}.  It assesses whether AI-based decisions (e.g., mortgage loan awards, job interviews, parole, college admission) are biased against a designated group, perhaps defined by race, ethnic background, or gender. Fairness implementations in machine learning typically strive to minimise loss (due to defaults on loans, etc.) while placing a bound on some measure of resulting group disparities. The best known measures are {\em demographic parity} \citep{dwork2012fairness_JH}, {\em equalised odds} \citep{HarPriSre16_JH}, and {\em predictive rate parity} \citep{dieterich2016compas_JH,chouldechova2017fair_JH}, and {\em counterfactural fairness} \citep{KusLofRusSil17_JH,RusKusLofSil17_JH}.  The first two have mixed \mbox{integer}/\mbox{linear} programming (MILP) formulations (\S\ref{sec:Mixed_integer_programming}), and the third a mixed \mbox{integer}/nonlinear formulation.  Weaknesses of group parity measures include a lack of consensus on which one is suitable for a given application \citep{CasCruGreRegPenCos22_JH}, as well as on which groups should be monitored for bias.

A second category of models is concerned with {\em fairness for the disadvantaged}.  They strive for equality, but with greater emphasis on the lower end of the distribution.  The {\em maximin} criterion, based on the famous difference principle of John Rawls, maximises the welfare of the worst-off individual or social class \citep{Raw99_JH}.  It is defended with a social contract argument that has been intensely discussed in the philosophical literature \citep[as surveyed in][]{Fre03_JH,RicWei99_JH}. A more sophisticated form of the principle is lexicographic maximisation ({\em leximax}), which maximises the worst-off, then the second worst-off, and so forth. The {\em McLoone index} compares the total utility of stakeholders at or below the median utility to the utility they would enjoy of all were brought up to the median.  It is based on a concern that no one be disadvantaged but tolerates inequality in the top half of the distribution.  It has been used to assess the allocation of public services, particularly education \citep{Ver96_JH} and can be given an MILP formulation \citep{CheHoo23_JH}.

Criteria that {\em balance efficiency and fairness} can be placed in three categories: convex combinations of efficiency and fairness, criteria from classical social choice theory, and threshold criteria.  Convex combinations provide the simplest approach, as for example a combination of total utility and a fairness measure \citep[e.g.,][]{Sibel2019inequity_JH}.  Other formulations are given by \cite{Yager1997_JH}, \cite{ogryczak2003solving_JH}, and \cite{ReaFroMasSteFerPan21_JH}.  Convex combinations and other weighted averages pose the general problem of justifying a choice of weights, particularly when utility and equity are measured in different units, although \cite{ArgKarYav22_JH} propose a means of avoiding this issue.

The task of balancing fairness and efficiency gave rise to one of the oldest research streams in social choice theory, beginning with the {\em Nash bargaining solution}, also known as {\em proportional fairness} \citep{Nas50_JH}.  Proportional fairness has seen application in such engineering contexts as telecommunication and traffic signal timing \citep{Mazumdar1991_JH,Kelly1998_JH} and elsewhere.  Nash gave an axiomatic argument for the criterion, while \cite{Har77_JH}, \cite{Rub82_JH}, and \cite{BinRubWol86_JH} have shown that it is the outcome of certain bargaining procedures.  {\em Alpha fairness} generalises proportional fairness by introducing a parameter $\alpha$ that governs the importance of fairness, where $\alpha=0$ corresponds to a purely utilitarian criterion, $\alpha=1$ to proportional fairness, and $\alpha=\infty$ to the maximin criterion \citep{MoWal00_JH,VerAyeBor10_JH}.  Alpha fairness has been derived from a set of axioms \citep{Lan2010_JH,LanChi11_JH}, including an ``axiom of partition'' that is largely responsible for the result. It provides an objective function to be maximised that is nonlinear but concave (\S\ref{sec:Nonlinear_programming}). Another criterion, {\em Kalai-Smorodinsky bargaining}, likewise has an axiomatic defence \citep{KalSmo75_JH} and addresses what one might see as a weakness in Nash bargaining, namely that it can result in reduced utility for some stakeholders when the feasible set is enlarged.  The Kalai-Smorodinsky criterion can be viewed as a kind of normalised maximin, as it calls for allocating to each stakeholder the largest possible fraction of his or her potential utility (ignoring other stakeholders) on the condition that this fraction be the same for everyone. This criterion has received support from \cite{Tho94_JH} as well as the ``contractarian'' ethical philosophy of \cite{Gau87_JH} and has been recommended for wage negotiations and similar applications \citep{Ale92_JH}.

{\em Threshold criteria} are of two types.  One, based on an {\em efficiency threshold}, imposes a maximin objective until the efficiency cost becomes unacceptably great, at which point some stakeholders are switched to a utilitarian criterion.  The other, based on an {\em \mbox{equity} threshold}, imposes a utilitarian criterion until inequity becomes unacceptably great, at which point a maximin criterion is introduced.  Originally proposed for two stakeholders \citep{WilliamsCookson2000_JH}, the threshold criteria have been extended to $n$ persons, using an MILP formulation for the former \citep{HooWil12_JH} and a linear programming model for the latter \citep{ElcHooZha22_JH}.  A parameter $\Delta$ regulates the equity/efficiency trade-off in both models, in that stakeholders with utility within $\Delta$ of the worst-off are given special priority.  Thus, the parameter $\Delta$ may be interpretable in a practical situation in a way that $\alpha$ in the alpha fairness criterion is not.  Both threshold criteria inherit a weakness of the maximin criterion, namely that they may be insensitive to the equity position of disadvantaged stakeholders other than the very worst-off.  This has been addressed for the efficiency threshold by combining a utilitarian criterion with a {\em leximax} rather than a maximin criterion.  \cite{McElfresh2018_JH} accomplish this by assuming there is a pre-existing priority ordering of stakeholders.  \cite{CheHoo21_JH} avoid this assumption by giving greater priority to stakeholders with utilities closer to the lowest, and by solving a sequence of MILP models to balance the leximax element with total utility.

Fairness modelling is a relatively recent research program in operational research that may forge new connections with other fields.  Much as interactions between OR and economics, management, and engineering have been mutually beneficial on both a theoretical and practical level, collaboration with ethicists on the precise formulation of fairness concepts may bring similar benefits to both ethical philosophy and operational research.

\subsection[Finance (Michèle~Breton)]{Finance\protect\footnote{This subsection was written by Michèle~Breton.}}
\label{sec:Finance}

The use of mathematical models and numerical algorithms to solve an extensive range of problems in finance is widespread, by both researchers and practitioners. In this subsection, we offer an overview of some established models and discuss a selection of the corresponding OR approaches and techniques.

\subsubsection*{Resource allocation models}
As in any other industry, the optimal allocation of resources to activities is a central problem in finance. Prototype models include short-term cash flow management (a linear program), portfolio dedication and immunisation (linear programs), capital budgeting (knapsack problem), asset/liability management (stochastic program with recourse), and portfolio selection (quadratic program).

The \emph{portfolio selection} model introduced in \cite{Markowitz52_MB} and discussed in \cite{markowitz00_MB} is one of the best known optimisation models in finance. This mean-variance model consists of determining the composition of a portfolio of risky assets -- a vector of weights -- where the performance (to be maximised) is measured by the expected portfolio return, a linear function of the assets' weights, while the risk (to be minimised) is measured by the variance of the portfolio return, a quadratic function of the weight vector. The resulting optimisation problem gives rise to a convex quadratic program. This model and its analytical properties led to a formalisation of diversification as a strategy to mitigate risk and to important developments in financial theory.

While the Markowitz model represents a considerable simplification of the portfolio management problem, mean-variance optimisation models are still very much applied in practice. Straightforward variations of the Markowitz model can account for various constraints on the asset weights (e.g., bounds, minimum participation, regulatory or operational restrictions, logical constraints, etc.), yielding mixed integer quadratic programs.

Mean-variance models rely on sets of parameters describing the expected returns and their correlation matrix in the universe of the set of considered assets. Various forecasting approaches (\S\ref{sec:forecasting}) have been proposed to obtain estimates of these parameters, often relying on some assumptions about the correlation structure. One important issue related to the use of mean-variance optimisation models is the sensitivity of their solutions to the estimated parameter values \citep{michaud89_MB}, specifically when the feasible region is relatively unconstrained. Robust optimisation (see also \S\ref{sec:Stochastic_models}) is increasingly used to limit the estimation risk of mean-variance portfolio solutions \citep{ismail19_MB, yin21_MBl, blanchet22_MB}.

Another related limitation of mean-variance models is the fact that they are static models, that is, expectations and correlations of asset returns are assumed to be known and constant over the planning horizon. In practice, estimations are updated periodically to reflect changes in data, and portfolios are rebalanced to the optimal composition corresponding to the new set of estimates. Small perturbations in the values of the input parameters may lead to significant changes in the composition of the portfolio from one period to the next (for instance, when groups of assets have similar characteristics). When the costs associated with changing the composition of the portfolio are significant, a static model may be far from optimal. The portfolio selection problem can be readily extended to a multi-period context, allowing to account for transaction costs and/or to use a dynamic model of the evolution of asset prices over time \citep{li00_MB}. Dynamic models can also account for additional frictions, such as taxes on capital gains or losses \citep{dammon01_MB}. The resulting dynamic portfolio selection problem may be a large-scale stochastic dynamic program (\S\ref{sec:Dynamic_programming}; \S\ref{sec:Stochastic_models}). Moreover, risk measures based on portfolio variance are not additively separable, precluding the efficient use of dynamic programming. \cite{Steinbach01_MB} proposes a solution approach based on scenario decomposition.

\subsubsection*{Risk management}
While, in OR, the classical way to deal with decisions under risk is utility theory, finance models usually take a different approach by directly measuring and/or pricing risk. Various measures, such as variance, semi-variance, Value at Risk (VaR) or Conditional value at risk (CVaR) have been proposed to characterise risk\footnote{VaR is a quantile of the distribution of investment losses over a specified period, while CVaR is the amount of the expected losses, conditional to being above the VaR threshold}. VaR is effectively concerned with computing quantiles of the predictive distrubution (see also \S\ref{sec:forecasting} and \S\ref{sec:Power_markets_and_systems}). In the following paragraphs, we present two contrasting families of approaches to financial risk management.

\emph{Diversification and hedging }approaches are closely related to the resource allocation models presented above. They consist in setting up and managing portfolios of securities with desirable properties. Diversification is effective in reducing risk that is uncorrelated across securities, while hedging is used to reduce systematic risk, for instance by holding securities exposed to the same risk factors to eliminate uncertainty, or by buying insurance in the form of derivative contracts. In general, hedging positions must be continuously adjusted to account for the time evolution of risk factors and security prices. In addition, investment portfolios are
often required to satisfy institutional or regulatory constraints. Risk mitigation portfolio planning problems give rise to dynamic stochastic mathematical programs. In recent years, CVaR has become prominent for measuring portfolio risk; CVaR is well-suited to measure down-side risk in skewed distribution and, as shown in \cite{artzner99_MB}, it has the desirable properties of a \emph{coherent} risk measure. Moreover, the use of CVaR in optimisation models gives rise to convex or linear programs, allowing to efficiently solve the large-scale problems encountered in practice \citep{rockafellar00_MB, andersson01_MB, rockafellar02_MB}.

\emph{Risk pricing} approaches rather seek to evaluate the consequence of unpredictable events and are notably used for the management of credit and counterparty risk, that is, the risk that the issuer of a security (for instance, a corporate bond) will not be able to meet its future obligations. A variety of models have been proposed to evaluate the VaR of debt instruments, mainly for the purpose of assessing regulatory requirements ensuring that financial institutions put aside sufficient capital to sustain eventual losses. \cite{crouhy00_MB} presents a review of methodologies currently proposed by the industry to evaluate the probability and consequences of default events. Most approaches used in the industry to price credit and counterparty risk are based on probabilistic models or
Monte-Carlo simulation (\S\ref{sec:Simulation}) and, as such, cannot account for strategic behaviour by the debtor or the lender \citep{breton18_1_MB}.

\subsubsection*{Asset pricing}
Most asset pricing models are founded on an absence of arbitrage assumption, which is usually motivated by the efficiency of markets. Under this assumption, the value of a financial asset is equal to the expected value of its future payoffs, under a suitable probability measure. One specific application is the valuation and replication of contingent claims, such as financial options. The contribution of OR to this area lies in the development and implementation of efficient numerical pricing methods for complex financial securities.

Starting from the binomial tree model of \cite{cox79_MB}, \emph{numerical methods} for option pricing include Monte-Carlo (\S\ref{sec:Simulation}) and quasi-Monte-Carlo approaches \citep{acworth98_MB, lEcuyer09_MB}; dynamic programming (\S\ref{sec:Dynamic_programming}) and approximate dynamic programming models accounting for optimal exercise strategies \citep{ben02_MB,longstaff01_MB}; and robust control models (\S\ref{sec:Stochastic_models}) accounting for transaction costs and model uncertainty \citep{davis93_MB,bernhard05_MB, bandi14_MB}. Numerical algorithms developed for option pricing have also been applied to the valuation of numerous instruments, including corporate bonds, credit derivatives, contracts, and, under the designation of \emph{real options}, managerial flexibility \citep{trigeorgis96_MB, schwartz04_MB, dixit09_MB}.

In the context of \emph{algorithmic trading}, asset pricing algorithms have been revisited using artificial intelligence approaches, for instance by using machine learning to identify factor models or reinforcement learning to compute optimal exercise strategies \citep{dixon20_MB, gu20_MB}, or by augmenting the set of covariates with textual data \citep{algaba20_MB}.

\subsubsection*{Strategic interactions}
Decisions made by investors, firms, financial institutions, and regulators have a direct impact on asset values, returns and risk. Players in the financial sector have competing interests and interact strategically over time, and these interactions are recognised in many game-theoretic models of investment and corporate finance (\S\ref{sec:Game_theory}). Important issues include market impact and market manipulation, option games, strategic exercise of real options, agency conflicts, corporate investment, dividend and capital structure policies, financial distress, and mergers and acquisitions.

\emph{Optimal execution} refers to the determination of a trading strategy minimising the expected cost of trading a given volume over a fixed period, accounting for the impact of the trades on the price of the security. This problem is addressed in \cite{bertsimas98_MB}, using a stochastic dynamic program minimising the execution costs, and in \cite{almgren01_MB}, where a combination of volatility risk and transaction costs is minimised. Optimal execution and market impact are particularly significant issues in the context of algorithmic trading and have been addressed by the recently developed mean-field game theory, acknowledging the fact that price is impacted by the trades of many atomic players \citep{Firoozi17_MB, cardaliaguet18_MB, Mojtaba19_MB}.

\emph{Option games} appear in asset pricing models when a security gives interacting optional rights to more than one holder, that is, when the exercise of an optional right by one holder modifies those of the others. Examples include callable, putable and convertible bonds, warrants, and, especially, instruments subject to credit or counterparty risk. In general, the pricing of such financial instruments corresponds to the solution of a non-zero-sum stochastic game where players use feedback strategies \citep{ben2007_MB}.

\emph{Financial distress} models are used to price corporate debt, according to various assumptions about strategic default, debt service and bankruptcy procedures \citep{fan00_MB, broadie07_MB,annabi12_MB}.

Finally, a large literature in corporate finance uses game-theoretic models to deal with financial decisions made by firms, such as the choice between debt and equity when financing operations, the amount of dividends paid out to shareholders, and decisions about whether to invest in risky projects.

\subsubsection*{Further readings}
The recognition of finance as an thriving application area for OR methods developed about thirty years ago \citep[see, for instance, ][for an introduction to optimisation problems underlying risk management strategies and instruments]{dahl93I_MB,dahl93II_MB}. A review of practical applications of OR methods in finance appeared in \cite{board03_MB}. For a comprehensive textbook covering optimisation models in finance, we refer the reader to \cite{cornuejols06_MB}. A unified framework for asset pricing can be found in \cite{cochrane09_MB} and a review of applications of dynamic games in finance in \cite{breton18_2_MB}. Recent discussions about the interface of operations, risk management and finance as a promising research area are presented in \cite{wang21_MB} and \cite{babich21_MB}.

\subsection[Government and public sector (Katherine~Kent \& Sam~Rose)]{Government and public sector\protect\footnote{This subsection was written by Katherine~Kent and Sam~Rose.}}
\label{sec:Government_and_public_sector}

This subsection will present some OR applications within the UK’s government operational research service (GORS). GORS represents over 26 departments and agencies across Great Britain and Northern Ireland with analysts working in multi-disciplinary teams to find workable solutions to real life problems. The outbreak of the Coronavirus pandemic in 2020 introduced a new global backdrop and we were faced with the challenge of producing appropriate analysis to answers questions during an ever-changing landscape where time was of the essence. This led to collaborations across a wide range of departments across the nations.

A few examples of where this collaborative approach was adopted successfully are highlighted by the work carried out by the Department for Transport (DfT) and the Office for National Statistics (ONS). The ONS worked with other government departments such as Department of Health and Social Care (DHSC) and schools across the UK to monitor infection rates. They also applied their expertise in artificial intelligence (AI) in the form of semantic maps to gather insight into the pandemic. Additionally, the DfT along with other government departments used agent based modelling and discrete event simulation to unpick the issues around border disruptions and international travel. 

\subsubsection*{Coronavirus (COVID-19) Infection survey and Schools Infection Survey}
The Office for National Statistics (ONS) played a vital role during the pandemic in monitoring infection rates. The Coronavirus (COVID-19) infection survey estimates how many people across England, Wales, Northern Ireland, and Scotland would have tested positive for a COVID-19 infection, regardless of whether they report experiencing symptoms. This study was a collaboration with academic partners and funded by Department of Health and Social Care.
This major study involved asking people up and down the country to provide nose and throat swabs on a regular basis. These are analysed to see if they have contracted COVID-19. In addition, some adults are also asked to provide blood samples to determine what proportion of the population has antibodies to COVID-19. Further details of the methodology can be found in \cite{Office_for_National_Statistics2022-ey}.

Estimates of the total national proportion of the population testing positive for COVID-19 are weighted to be representative of the population that live in private-residential households in terms of age (grouped), sex and region. The analysis for the infection study is complex, the model generates estimated daily rates of people testing positive for COVID-19 controlling for age, sex, and region. This technique is known as dynamic Bayesian multi-level regression post-stratification (MRP). Details about the methodology are also provided by \cite{Pouwels2021-xu}.

Estimates from the ONS survey are published weekly, a critical element was how best to communicate the uncertainty, for dissemination estimates were translated into, for example, 1 in 50 people, with appropriate visuals including the ONS insights tool \citep{Office_for_National_Statistics2022-rx}. A complementary piece of work was monitoring transmission and antibody levels within schools, enabling the government to accurately assess the risk of different policy options.

The Schools Infection Survey (SIS) was a longitudinal study which collected data through polymerase chain reaction (PCR) tests, antibody tests and questionnaires. As well as monitoring transmission within a school environment, this data was used to assess the wider impacts of the pandemic and repeat lockdowns on our children and young people, including long covid, mental health and physical activity levels. Further detail can be seen in \cite{Office_for_National_Statistics2022-gy} and \cite{Hargreaves2022-yt}.

The Daily Contact Testing (DCT) trial was a blind medical trial which compared infection rates across two groups subject to different policies: the control group where children were in “bubbles”, and after one child testing positive the entire bubble would be sent home from school, and the intervention group where after a child tested positive, close contacts would then test daily and were allowed to remain in school as long as their results were negative. The study was a success and led to a policy change that resulted in schools being kept open for longer. Further detail can be seen in \cite{Young2021-ws}.

\subsubsection*{Semantic Maps and their use for understanding regional disparities}
Semantic Maps are a type of knowledge graph, championed in the world of robotics and Artificial Intelligence as a way to provide infrastructure to exploit all kinds of potentially even crowdsourced data and information in such a way as to provide dynamic, online, interactive visualisations that support the controlled and secure use of live data. The can be geospatial in nature, but they can also reflect connections through semantic relationships. These maps populated with data would provide users with many different ways to consume the underlying data and help inspire citizens about the potential power of data to drive understanding and generate insights. 

During the development of the Levelling Up White Paper the evidence base needed to be developed across government in order to define the key metrics and measures to focus policies in areas that would drive change. The white paper itself was delivered by the Levelling Up Taskforce in the Cabinet Office, however, ONS worked with the geospatial commission in convening a group of chief analysts from all departments on a regular basis. ONS and the Levelling Up Taskforce used the group to commission and collate existing evidence and then worked with officials in His Majesty’s Treasury (HMT) to develop a systems thinking model from that evidence. This systems mapping was the basis of the theory of change that underpins the white paper. Subsequently, the metrics and missions were developed and refined with this group in recognition of the fact that there needs to be a focus across the system to reduce disparities that are often larger within areas such as local authorities or regions than they are between them. 

ONS took this and developed a semantic map that identifies potential data sources for various aspects of the knowledge graph, using this to both prioritise filling evidence gaps where data and evidence do not currently exist and developing an integrated data asset for Levelling Up. This data is in the process of being acquired and engineered to be able to be easily linked through a set of linking ‘spines’ referred to as the reference management database. This engineering and architecture is key for supporting the sharing of information in a way that ensures privacy. It is intended that in future this asset will be made available across government and in a secure integrated data service. 

\subsubsection*{Agent Based Modelling}
The COVID-19 pandemic presented challenges for the international travel community. Government officials in transport and health needed to model the preventative effect of the various policy options including testing and isolation on importation of infections from international travel.  

The approach chosen in the Department for Transport to support this fast-moving policy area was agent based modelling which built upon the more scientific epidemiological modelling undertaken by colleagues in Health and academia. This allowed for the incorporation of the various differing parameters of international travellers including, where they were coming from and their risk of being infected and infectious, the uncertainty over incubation and infectious periods, and their likely behavioural response to various isolation and testing regulations. 

Whilst not designed to be a scientific forecast, the modelling allowed the cross-government community to estimate the relative effectiveness of policy options. This work supported policy making during a highly uncertain and changing environment when Government had to balance risk with the wider impacts on the aviation sector and the second order impacts on the economy.

\subsubsection*{Discrete Event Simulation}
As part of the EU Exit preparations, it was important for both the Department for Transport, the Home office and regional resilience teams to understand the impact of the expected border disruption at UK ports for roll-on roll-off freight traffic travelling to EU Member States. As the issue was around changes to the time taken and resource available to carry out additional border processes, the natural choice was discrete event simulation. Analysts in government developed a detailed model of the Short Strait crossings 
\citep[Port of Dover and the Channel Tunnel to France accounting for 84\% of accompanied heavy goods vehicles travelling to continental Europe in 2019;][]{Department_for_Transport2018-vw}. Regional models were developed to cover other ports. These allowed government officials to understand the likely queues and flow of vehicles and to understand the impact of changes to the system, which was vital to supporting contingency planning.

\subsubsection*{Statistical analysis and forecasting}
The COVID-19 lockdowns of 2020 accelerated the uptake of new and novel data sources for understanding mobility. Government analysts rapidly ingested new data sources such as that provided by Google mobility as open source data as well as procuring in additional anonymised and aggregated mobile network operator data. By analysing these new data sets alongside traditional demographic and geographic data sets it was possible to generate insights into the changes in mobility being seen across the country as a result of the various national and regional restrictions. Regression analysis was undertaken to produce a predictive model. It was then possible to forecast the impact of later changes to restrictions on population mobility.

\subsubsection*{Net zero - system thinking}
In 2020 the UK Prime Minister’s Council for Science and Technology advised on the following: ‘a whole systems approach can provide the framework that government requires to lead change across public and private sectors and \dots enables decision makers to understand the complex challenges posed by the net-zero target and devise solutions and innovations that are more likely to succeed’ \citep{GOVUK_2020-yk}. The Prime minister agreed.

As transport represents a huge portion of the challenge, ORs have run participatory systems mapping workshops in the Department for Transport with modal subject matter experts to identify the key causes and effects in the Transport Net Zero system. This aims to enable those working on Transport policy to explore the evidence, gain new insights and visibility of interdependencies within the system, and to understand the likely wider impact of their policy choices.

\subsubsection*{Conclusions}
All these examples help to illustrate the breadth of analysis undertaken across central government during the global pandemic to tackle real life issues; and the ranges of techniques we have as OR analysts to find workable solutions in an ever changing world.

\subsection[Healthcare (Christos~Vasilakis)]{Healthcare\protect\footnote{This subsection was written by Christos~Vasilakis.}}
\label{sec:Healthcare}

Why is the organisation and delivery of health and care services so difficult to manage, plan for and improve? Difficulties and delays in accessing care services, cancellations and increasing costs have a negative impact on all of us: patients, carers, and care professionals. Despite the attention and resources invested in addressing these problems, many health systems face increasing pressure to improve the effectiveness and efficiency of their operations. Part of the problem is the complexity inherent in the organisation of care services and our limited understanding of how changes will affect their delivery. Another problem is the intrinsic uncertainty and variability in many aspects of care service delivery. Add the multifaceted dynamics arising in this very complex socio-technical system involving professionals, patients and existing and new technologies, against a background of increased demand and budgetary constraints, and it is no surprise the effort to improve healthcare has been termed ‘rocket science’ \citep{Berwick2005-hf_CV}.

Operational research has a long established history in this area with the first application (scheduling outpatient hospital appointments) reported in the early 1950s \citep{Bailey1952-xt_CV}. Since then, there has been a proliferation of OR applications reported in the literature \citep{Katsaliaki2010-ae_CV}, and evidence of use to support policy making and care delivery \citep{Royston2009-ae_CV}. This is not a surprise given the importance of healthcare in our lives and that many of the problems faced by those managing and delivering care services are amenable to the methods and ethos of OR \citep{utley_crowe_pagel_2022_CV}. In the short review that follows, which is by no means exhaustive and draws primarily (but not exclusively) from the UK academic community and the National Health Service (NHS), I have attempted to give examples of review and individual studies grouped in a few broad areas of healthcare. 

\subsubsection*{Applications to hospital settings}
Hospital care has been the setting of a large number of OR studies \citep{Jun1999-oz_CV}. Hospitals typically survive reorganisations and funding cuts (unlike management, policy and other statutory bodies), and are large enough to be able to engage meaningfully in research projects (unlike, for example,  many primary care practices). Some (mainly teaching) hospitals host large biomedical research centres, with many of the professionals working in them active researchers.  

A specific area that has attracted the attention of operational researchers is the Emergency Department (ED). A recent review article identified 21 studies that used a computer simulation method to capture patient progression through the ED of an established UK NHS hospital, mainly focusing on service redesign \citep{Mohiuddin2017-zh_CV}. Individual studies have addressed the micro (single hospital) level \citep{Baboolal2012-ci_CV}, as well as the meso-level of emergency and on-demand healthcare within a region \citep{Brailsford2004-ib_CV}. The study by \cite{Lane2000-qz_CV} used System Dynamics to model the interaction of demand patterns and resources deployed in ED and other parts of the hospital to examine the link between emergency and elective operations in hospitals.

Another hospital area that has been the focal point of OR is peri-operative care. \cite{Sobolev2011-xv_CV} in their systematic review identified 34 studies modelling the flow of surgical patients. Various forms of optimisation have also been applied to surgical scheduling problems including operating room \citep{Fairley2019-eu_CV}, staffing \citep{Bandi2020-xy_CV} and nurse rostering \citep{Xiang2015-qe_CV} among others. \cite{Cardoen2010-rw_CV} identified almost 250 papers, with the rate of published studies accelerating at around the start of the new millennium (similar trends have been observed across many disciplines). The review revealed that most of the research was directed towards the planning and scheduling of elective patients in highly stylised scenarios – although many operational challenges are triggered by factors such as the arrival of non-elective (emergency) patients. More recently, the problems tackled have become more realistic to include considerations of downstream resource availability such as critical care and general ward beds \citep{Fugener2014-jk_CV}, and scheduling elective operations in such a way that randomly arriving emergency patients can be accommodated without excessive delays \citep{Jung2019-ls_CV}.

An area OR has demonstrably made a beneficial impact is the organisation of acute stroke services. Several studies have attempted to address the rate and speed with which patients with suspected acute ischemic stroke go through the initial diagnostic steps and receive treatment \citep{Meretoja2014-he_CV}. \cite{Monks2012-ec_CV} made a number of recommendations for improving treatment rates in a rural hospital. In a follow-up study that evaluated the results of their recommendations, mean door-to-needle times (a key performance metric with direct impact on patient survival and recovery) fell from 100 min to 55 min while thrombolysis rates increased to 14.5\% \citep{Monks2015-yy_CV}. More recently, the focus has shifted to supporting decision around the centralisation of regional acute stroke services \citep{Wood2020-jf_CV,Wood2022-ot_CV}, as well as the supporting the introduction of endovascular thrombectomy, a new and very effective treatment for ischemic stroke \citep{Maas2022-ty_CV}.

\subsubsection*{Applications to non-acute hospital care settings}
Much of healthcare is delivered outside of large hospital facilities. Primary care, home and community care, social care are significant components of the healthcare ecosystem. Primary care, whether provided by physicians, nurse practitioners or pharmacists, is typically concerned with providing a first contact and principal point of continuing care and/or coordinating other specialist care. There are early examples of theoretical work to assist primary care planning by estimating the coverage achieved by staff and facilities, using antenatal care as an example \citep{Kemball-Cook1983-yd_CV}. More recently in the area of maternity care provided in community as well as hospital facilities, \cite{Erdogan2019-ms_CV} developed and empirically tested an open source facility location solver to assist with a decision on the number and location of regional maternity facilities.

Home-based care has attracted considerable attention from operational researchers \citep{Grieco2021-le_CV}. This review identified studies proposing models and solution methods for operational decisions on staff rostering, the allocation of staff to patient visits, the scheduling of visits and the routing of staff. An example of impactful OR project is the Swedish study by \cite{eveborn_operations_2009_JHLS}, where a set of algorithms and accompanying software tool were developed to provide solutions to staff-to-patient allocations, staff scheduling and staff routing problems. Having deployed the tool to more than 200 units/organisations, operational efficiency was increased by up to 15\%, resulting in annual savings of 20-30 million euros. More recently, modelling work has supported the effort to address the timely discharging of hospital patients by using a combination of home-based and bedded ‘step-down’ community care \citep{Harper2021-qk_CV}.

Mental health, one of the leading causes of disease burden internationally, has also received the attention of operational researchers \citep{Long2018-ji_CV}. Specific areas of application varied from psychiatric ICUs \citep{Moss2022-ao_CV} to system design \citep{Smits2010-dq_CV} and planning \citep{Vasilakis2013-sn_CV}, medical decision making \citep{Haji_Ali_Afzali2012-jh_CV}, and epidemiology \citep{Ciampi2011-fi_CV}.

\subsubsection*{Public health, health system preparedness and resilience, and pandemic response}
Public health, the science and practice of helping people stay healthy and protecting them from threats to their health, is another area of OR applications. The review article by \cite{Fone2003-uq_CV}, identified OR studies of infection and communicable disease, screening, and several epidemiological and health policy studies. Microsimulation, a type of simulation which models individual life trajectories through a number of healthy and disease states, has found wide applicability in the area of public health \citep{Krijkamp2018-iz_CV}, such as forecasting the long-term care needs of the older population in England \citep{Kingston2018-fi_CV}. Multicriteria decision analysis (MCDA) methods have also been used extensively to address questions of health policy or health technology assessment \citep{Glaize2019-jz_CV}. 

An area that has seen increased attention over the last two decades is that of emergency preparedness and health system resilience \citep{Tippong2022-ec_CV}. Emergency preparedness studies include a study of red blood cell provision following mass casualty events \citep{Glasgow2018-th_CV}. Examples of health system resilience studies include the paper by \cite{Crowe2014-gr_CV}, which examined the feasibility of using modelling to assess the capacity of a care system to continue operating in the face of major disruption. 

The COVID-19 pandemic not only gave rise to a large number of modelling studies, it also raised the profile of mathematical modelling with the general public and policy makers. \cite{Pagel2022-vg_CV}, in their excellent article on the role of modelling in the pandemic response, discussed the early lessons learnt from this experience including the poor understanding of policy makers and the public of key concepts such as exponential growth. They argue that infection disease modelling, which generated much of the evidence used to support decisions of pandemic response \citep{Brooks-Pollock2021-lw_CV}, is intrinsically difficult given the complex relationships between the model parameters, and the difficulties associated with quantifying these parameters. 

The possible benefits of modelling in addressing the challenges presented by the pandemic were outlined by \cite{Currie2020-tz_CV}. Indeed, several studies emerged early in the pandemic including, for example, an attempt to forecast the number of infected and recovered cases used univariate time series models \citep{Petropoulos2020-qm_CV}. \cite{Wood2020-wx_CV} published one of the first OR studies that examined the likely impact of increases in critical care capacity as a means to reduce the COVID-19 death toll. In a follow-up study, the sophistication of the model was increased to capture notions of triaging access of patients to critical care beds during periods of intense demand \citep{Wood2021-jv_CV}. The operation of large vaccination centres was also the topic of several modelling studies, both theoretical \citep{Franco2022-dx_CV} and empirical \citep{Wood2021-jt_CV,Valladares2022-yk_CV}.

\subsubsection*{Concluding remarks}
Despite the large body of literature, the role and impact of OR on improving care systems is less clear. Hospitals have “largely failed to use one of the most potent methods currently available for improving the performance of complex organisations” \citep{Buhaug2002-ck_CV} and “staff may be largely unaware of the potential applications and benefits of OR” \citep{utley_crowe_pagel_2022_CV}. A systematic review found that only half of the included studies reported models that were constructed to address the needs of policy-makers, and only a quarter reported some involvement of stakeholders \citep{Sobolev2011-xv_CV}. Recent positive developments include the introduction of guidelines to improve the reporting of OR studies \citep[e.g.,][]{Monks2019-wt_CV}, studies that recognise the importance of behavioural factors in attempts to influence practice and decision making with OR \citep{Crowe2022-bc_CV} and attempts to systematically generate evidence on the value and impact of OR on patient and system outcomes \citep{Monks2015-yy_CV,Soorapanth2022-jt_CV}. The research agenda should continue to evolve with the aim of addressing the challenges around engagement, implementation and evidencing the impact of OR applied to healthcare problems.

\subsection[Inventory (Jing-Sheng~Song)]{Inventory\protect\footnote{This subsection was written by Jing-Sheng~Song.}}
\label{sec:Inventory}

Inventories are the materials, parts, and finished goods held by an organisation for future use or sale. Not having enough inventory is costly. Shortages of materials and parts cause interruptions in production processes, delays in product delivery, and stockouts of finished goods. On the other hand, carrying inventory is costly, too, involving the cost of capital due to tied-up capital, storage cost, insurance, taxes, and spoilage and obsolescence costs.  

Inventory theory studies analytical models and solution techniques to help organisations meet the service requirement most cost-effectively or minimise the total expected costs of ordering, inventory-holding, and shortage. It does so by quantifying the tradeoffs driven by economies of scale, lead time (the time it takes to receive the ordered quantity after placing an order), and supply and demand uncertainties. It prescribes effective inventory-control policies that govern when to order an item (called \textit{reorder point}) and how much to order (called \textit{order quantity}). 

Inventory models distinguish from each other along several features: single or multiple planning periods, discrete- or continuous-time inventory monitoring, single- or multi-product, single- or multi-stage (or location), demand nature (deterministic or stochastic, stationary or nonstationary, distribution known or unknown), product perishability, lost sales or backlogging when shortages occur, deterministic or stochastic lead time, supply system (single- or dual-source, exogenous or endogenous, a finite or infinite capacity), and cost structure (with or without a fixed ordering cost, etc.). The following research-based textbooks and handbooks offer more detailed coverage and references:  \cite{arrow1958studies_JSS}, \cite{axsater2006inventory_JSS}, \cite{de2003supply_JSS}, \cite{graves1993logistics_JSS},
\cite{hadley1963within_JSS}, \cite{nahmias2011perishable_JSS}, \cite{porteus2002foundations_JSS}, \cite{silver1988inventory_JSS}, \cite{simchi2014logic_JSS}, \cite{snyder2019fundamentals_JSS}, \cite{song2023handbook_JSS}, and \cite{Zipkin2000_JSS}.

One class of models focuses on characterising the optimal inventory-control policy under a given supply and demand environment and cost structure. A common approach is formulating a multi-period inventory decision problem as a dynamic program and transforming the original formulation into a simpler one through state reduction. Next, identify the structural properties of the single-period cost function to determine the optimal policy form for a single-period problem. Then, show that these properties are preserved by the (Bellman) optimality equation, so the policy form is optimal for each period. The optimal policy parameters may not be easy to compute; hence some works develop efficient algorithms to calculate the optimal policy parameters. 

Another class of models focuses on developing efficient performance evaluation tools for a given type of inventory policy that is either commonly used in practice or of simple structure and easy to implement. This is particularly important for systems where state reduction is not viable and the dimension of the system state grows exponentially in the number of periods (the so-called curse of dimensionality), so the optimal policy has no simple form. Typically, this type of work analyses a continuous-review system in which demand follows a stochastic process and derives steady-state performance measures of any given policy, such as average inventory, average backorders, and stockout rate, as well as the long-run average cost. Then, optimisation tools can be developed to find the optimal policy parameters that minimise the long-run average cost. 

The third class of models conducts asymptotic analysis to establish asymptotic optimality of some simple-structured policies for less tractable inventory systems with unknown and complex optimal policies. 

The following are several classic models where the optimal policies are shown to have simple forms. Unless otherwise stated, the models assume a single stage, a single source, and a single nonperishable product. 

\textit{The EOQ (Economic Order Quantity) Model} was first developed by \cite{harris1913many_JSS} \citep[see the reprint][]{harris1990many_JSS} and popularised by \cite{wilson1934scientific_JSS}. It concerns the balancing of holding and ordering costs due to economies of scale in procurement or production. It is a continuous-review model over an infinite planning horizon, assuming the annual demand for the stocked item is a constant $\lambda$. There is a fixed procurement cost $k$ independent of the order size, accounting for administrative, material handling, and transportation-related costs. The annual per-unit inventory-holding cost is $h$. The optimal order quantity (EOQ) that minimises the annual order and holding costs equals $\sqrt{2k\lambda/h}$, which is insensitive to small perturbations of the model parameters. Variations of this model can accommodate finite production rate, planned backlogs, random yield, a quantity discount, and time-varying demand \citep[also known as the dynamic lot sizing problem; see][]{wagner1958dynamic_JSS,silver1973heuristic_JSS}. It also forms the basis for the development of efficient multi-item joint replenishment policies and multiechelon coordinated replenishment policies such as the power-of-two policies; see \cite{roundy198598_JSS}, \cite{roundy198698_JSS}. For major developments and references, see \cite{axsater2006inventory_JSS}, \cite{muckstadt1993analysis_JSS}, \cite{silver1988inventory_JSS}, \cite{simchi2014logic_JSS}, and \cite{Zipkin2000_JSS}.

\textit{The Newsvendor Model}, which originated from \cite{edgeworth1888mathematical_JSS} in a banking application, was formalised by \cite{arrow1951optimal_JSS} in the general inventory context. It optimises the tradeoff between too much and too little inventory caused by demand uncertainty for a seasonal product. It is a single-period model with only one ordering opportunity before the selling season, assuming an estimated demand distribution. The fixed order cost is negligible. After the ordered quantity arrives, the selling season begins, and demand realises. At the end of the season, there will be either unsold units (overage) or unmet demand (underage). The unit overage cost ($o$) = purchasing cost less - salvage value, while the unit underage cost ($u$) is the lost profit. The optimal \textit{newsvendor order quantity} equals to the fractile of the demand distribution at the critical ratio $u/(u+o)$. The model can be generalised in many ways, including random yield, different cost structures, pricing, and distribution-free bounds \citep{gallego1993distribution_JSS, petruzzi1999pricing_JSS, porteus1990stochastic_JSS, qin2011newsvendor_JSS} and multi-location with risk-pooling effect \citep{bimpikis2016inventory_JSS,eppen1979note_JSS}.

\textit{Dynamic Backlogging Models}. The most tractable and developed setting for multi-period models with stochastic demand and a constant lead time is full backlogging. When stockouts are rare, this model is a reasonable approximation for the lost-sales system. An important concept (due to state reduction) is \textit{inventory position}, which is the sum of the on-hand inventory plus total pipeline inventory minus backorders. This is the total system inventory available to satisfy future demand if we do not order again. 

Assume demand is independent over time. A \textit{base-stock policy} is optimal if the order cost is linear (no fixed order cost). Each period has a target inventory position called the \textit{base-stock level}. If the inventory position before ordering is below this level, order up to this level; otherwise, do not order. If the demand is stationary, the myopic base-stock level that minimises a single-period expected cost is optimal. The base-stock level has the same form as the newsvendor quantity, with the holding cost as the overage cost, the backorder cost as the underage cost, and the demand during a lead time replacing the single-period demand. For nonstationary demand, as long as the myopic base-stock levels are nondecreasing in time, the myopic base-stock level is still optimal. See \cite{veinott1965optimal_JSS} and \cite{porteus1990stochastic_JSS}. 

When the order cost is linear plus a fixed cost $k$, the optimal policy is an \textit{$(s, S)$ policy}. In each period, if the inventory position before ordering is below a threshold $s$, order up to $S$; otherwise, do not order. The key enabler of this result is that the single-period cost is $k$-convex, a property discovered by \cite{scarf1960optimality_JSS}. When the demand is stationary, the policy is also stationary. In a continuous-review system with Poisson demand, the optimal policy is an \textit{$(r,q)$ policy}: When the inventory position reaches $r$, order $q$ units. It is equivalent to the $(s,S)$ policy with $r=s$ and $q=S-s$. A simple yet effective heuristic policy is to use the optimal base-stock level to approximate $r$, and use the EOQ formula to approximate $q$; see \cite{zheng1992properties_JSS} and \cite{axsater1996using_JSS}.

These policy structures have been extended to more complex models, such as Markov modulated demand \citep{iglehart1960optimal_JSS, song1993inventory_JSS, sethi1997optimality_JSS}, exogenous and sequential stochastic lead times \citep{kaplan1970dynamic_JSS, nahmias1979simple_JSS, ehrhardt1984s_JSS, song1994effect_JSS, song1996inventory_JSS}, capacity constraints \citep{federgruen1986ainventory_JSS, federgruen1986binventory_JSS}, unknown demand distribution \citep{scarf1959bayes_JSS, scarf1960some_JSS,  azoury1985bayes_JSS}, and a dual-source problem where the lead times of the two sources differ by one period \citep{fukuda1964optimal_JSS} or the lead times are stochastic and endogenous \citep{song2017optimal_JSS}. See \cite{veinott1966status_JSS}, \cite{perera2022asurvey_JSS}, \cite{perera2022bsurvey_JSS}, \cite{porteus1990stochastic_JSS}, and \cite{Zipkin2000_JSS} for more detail. 

Multiechelon (or multi-stage) inventory systems are common in supply chains where the stages are interrelated, such as production facilities, warehouses, and retail locations. The literature focuses on understanding three basic system structures: series, assembly, and distribution systems. 

In a series system with $N$ stages and backlogging, random customer demand arises at stage 1, stage 1 orders from stage 2, and so on, and stage N orders from an outside supplier with ample supply. There is a constant transportation time between two consecutive stages. Define the echelon inventory of each stage to be the inventory at the stage plus all downstream inventories (including those in transit). Assuming no fixed order costs, \cite{clark1960optimal_JSS} establish that an echelon base-stock policy is optimal for all stages. That is, we can treat each echelon as a single location and order the echelon inventory position up to a target base-stock level. \cite{axsater1993installation_JSS} show that for any echelon base-stock policy, there is an equivalent local base-stock policy; therefore, the implementation of the optimal policy is simple. \cite{federgruen1984computational_JSS} find that the optimal echelon base-stock policy for the infinite horizon problem can be efficiently obtained. \cite{rosling1989optimal_JSS} proves that under certain mild conditions, an assembly system can be transformed into an equivalent series system, so the Clark-Scarf result applies. \cite{chen1994lower_JSS} further streamline the proofs of these results. \cite{shang2003newsvendor_JSS} construct effective single-stage newsvendor solutions to approximate the optimal echelon base-stock levels. \cite{chen2001optimal_JSS} show that a state-dependent echelon base-stock policy is optimal for Markov-modulated demand. See \cite{axsater1993continuous_JSS}, \cite{axsater2003supply_JSS}, \cite{axsater2006inventory_JSS}, \cite{angelus2023generalization_JSS}, \cite{federgruen1993centralized_JSS}, \cite{kapuscinski2023capacitated_JSS}, and \cite{shang2023single_JSS} for more developments, including batch ordering, capacity limits, distribution systems, transshipment, and expediting. 

Many other features are much less tractable, such as lost-sales systems \citep{bijvank2023lost_JSS}, censored demand data, perishable products \citep{li2023perishable_JSS}, general dual-sourcing systems \citep{xin2023dual_JSS}, distribution systems, and assemble-to-order systems \citep{atan2017assemble_JSS,song2003supply_JSS, de2023assemble_JSS}. Nonetheless, significant progress has been made in recent years on structural properties of the optimal policy, asymptotic optimal policies, and effective heuristics, thanks to more analytical tools such as discrete convexity, asymptotic analysis, and machine learning algorithms. See \cite{chao2023online_JSS}, \cite{cheung2023statistical_JSS}, \cite{shi2023approximation_JSS}, and other chapters in \cite{song2023handbook_JSS}.

\subsection[Location (Sibel~A.~Alumur)]{Location\protect\footnote{This subsection was written by Sibel~A.~Alumur.}}
\label{sec:Location}

In the domain of operations research, location problems are concerned with determining the location of a facility or multiple facilities to optimise one or more objective functions under constraints. 
Location problems seek answers to questions such as how many facilities should be located, where should each facility location be, how large should each facility be, and how should the demand for the facilities’ services be allocated to these facilities \citep{daskin1995network_SAA}. 
An example of a facility to be located is a factory, distribution centre, warehouse, cross-dock, or hub, where demand can be for raw materials, components, products, passengers, data, etc. 

Location decisions arise in a variety of public and private sector decision-making problems. Some examples from different sectors include locating landfills where demand is for disposal of household waste \citep{ERKUT1989275_SAA},  ambulances where demand is for transporting emergency patients to hospitals \citep{brotcorne2003ambulance_SAA}, warehouses where demand is for storing products arriving from factories \citep{aghezzaf2005capacity_SAA}, schools where demand is for students \citep{haase2013management_SAA}, regenerators in optical networks where demand is for data \citep{yildiz2017regenerator_SAA}, shelter sites where demand is for refugees \citep{bayram2018shelter_SAA}, and charging stations where demand is for electric vehicles that need to charge \citep{kinay2021full_SAA}. More applications of location problems from practice can be found in \cite{eiselt2015applications_SAA}.

Location decisions refer to the placement of a facility considering its interactions with demand points (e.g., customers, suppliers, retailers, households) and possibly with other facilities to be located. It includes selecting the location and determining how this location supports meeting a decision-maker or organisation's objective. It is important to note that facility location decisions are different from facility \textit{design} decisions. Facility design decisions usually consist of facility layout and material handling systems design. The layout entails all equipment, machinery, and furnishing within the building, whereas material handling systems comprise the mechanism needed to satisfy the required facility interactions. Facilities planning and design are extensively discussed in \cite{tompkins2010facilities_SAA}.

Several factors influence facility location decisions, the most prominent ones being transportation costs and the availability of the transportation infrastructure. Among other important factors are the availabilities and costs of land, market, labour, materials, equipment, energy, government incentives, and competitors as well as geographical factors and weather conditions. 
Distance is usually considered to be one of the most important criteria in facility location models. Several distance metrics can be used in location models such as Euclidean (straight-line), rectilinear (Manhattan), Cheybyshev (Tchebychev), and network distance. Network distance is the distance that is calculated on an existing transportation network, for example, through using Google or Bing maps. 

An important criterion to be considered in location problems is how demands are to be satisfied by the facilities to be located. In some applications, the whole demand of each customer must be satisfied from a single facility (``non-divisible" demand) which is referred to as a \textit{single source} or \textit{single allocation}. Single allocation location problems are also referred to as \textit{location-allocation} problems as each demand point is allocated to a single facility. On the other hand, in \textit{multiple source}/\textit{allocation} problems, the demand of a single customer can be served from several facilities.  

Location decisions are usually classified according to their decision space. In \textit{continuous} or \textit{planar} location problems, the facilities can be located anywhere in the decision space. The search is for the optimal coordinates; i.e, latitude and longitude. In \textit{discrete} location problems, a finite set of potential facility locations is provided, possibly determined through a pre-selection process. In \textit{network} location problems, on the other hand, there is a given network and the facilities are to be located on this network. In network location problems, facilities can further be restricted to be placed only on the vertices or nodes of this network and not on the edges or arcs, referred to as \textit{vertex-} or \textit{node-restricted} location problems.

Continuous location problems focus on minimising some function related to the distance between the facilities to be located and the existing facilities or demand points, such as suppliers and customers, where minisum (minimising the total weighted distance) and minimax (minimising the maximum or worst weighted distance) are among the most commonly employed objectives. Special cases of continuous single-facility location problems with commonly used distance metrics (e.g., rectilinear and Euclidean) are well-studied and polynomial time solution algorithms exist \citep{francis2004facility_SAA}. In the case of multi-facility continuous location problems, the facilities to be located can be homogeneous or non-homogeneous; in the latter, there are different types (e.g., a factory and a warehouse) or sizes of facilities to locate. 

One of the most studied discrete location problems is the \textit{$p$-median} problem. The goal is to pick a subset $p$ of (homogeneous) facilities to open from among a given set of potential locations that minimise the total transportation cost of satisfying each demand point from the (nearest) facility it takes service from. There is a well-known node optimality theorem by \cite{hakimi1965optimum_SAA} for the $p$-median problem on networks that proves that at least one optimal solution to the $p$-median problem consists of locating the facilities only on the nodes of the network (even though a facility is allowed to be located anywhere on the network including any point on an edge between the nodes). Possible applications of the $p$-median problem are clustering, transit network timetabling and scheduling, placement of cache proxies in a computer network, diversity management, cell formation and much more \citep{marin2019p_SAA}. 

An important related problem is the \textit{uncapacitated facility location problem} (UFLP) which is also referred to as the \textit{simple plant location problem}. Unlike the $p$-median problem, in  UFLP, the number of facilities to be located is no longer known and determined by optimising an objective function that considers the trade-off between the fixed costs of locating facilities and the transportation costs. Numerous extensions of UFLP with uncertainty, multiple commodities (e.g., products or services), multi-period planning horizon, multiple objectives, and network design decisions have been studied with applications in several domains such as supply chain and distribution systems design. 

A nice structure of $p$-median and UFLP is that since the facilities to be located are assumed to have enough capacity (e.g., space or labour hour), all demands of each customer can be served from a single facility with minimum allocation costs. This is no longer the case for \textit{capacitated} versions of the facility location problems, where single- and multiple-allocation versions are both extensively studied. For multiple allocation capacitated (fixed-charge) facility location problems, when the set of open facilities is given, the resulting subproblem of finding the best allocations is a transportation problem. In the single allocation case, on the other hand, when the set of open facilities is pre-determined the resulting allocation subproblem is a generalised assignment problem \citep{fernandez2015fixed_SAA}.  

When the worst-case is more important than the average, it might be better to consider the furthest or most disadvantaged demand point to ensure equity in servicing the demand. Accordingly, the $p$-centre problem aims to locate $p$ facilities such that the maximum distance (or travel time/cost) from a demand point to its nearest facility is minimised (minmax). The $p$-centre problem can be used to locate public schools and various emergency service facilities such as police stations, hospitals, and fire stations. Different variations of this problem have been studied such as the capacitated, conditional, continuous, fault-tolerant, and probabilistic $p$-centre problems \citep{ccalik2019p_SAA}. 

In covering location problems, the aim is to locate facilities so as to cover demand. Typically, a demand point is considered to be covered if it is within a certain distance or travel time of a facility. Unlike in the previous models, the demand points are not assigned to facilities in covering location problems. The two most common covering location problems are \textit{set covering} and \textit{maximal covering} location problems. In the set covering location problem, the aim is to minimise the total cost of locating facilities to cover all demand points, whereas, in the maximal covering location problem, the aim is to maximise the total demand covered subject to a budget constraint or a constraint on the total number of facilities to locate. Continuous variants of these covering location problems are also studied \citep{plastria2002continuous_SAA}. Several different versions of covering location problems have been studied in the literature, including, but not limited to weighted, redundant, hierarchical, backup covering problems with applications in emergency services, crew scheduling, mail advertising, archaeology, metallurgy, and nature reserve selections \citep{garcia2015covering_SAA}. 

In general, facility location problems consider and model only a single echelon; i.e., either the flows of commodities  (e.g., products, customers) coming into or out from the facilities to be located are negligible, for instance, when one of those transportation costs is borne by another decision-maker and somehow not related to the current decision-making problem. An example would be a manufacturing company determining the location of its new factory for delivering products to its customers with minimum total cost, where the company is not directly involved in the delivery of raw materials from their suppliers to the factory. When the flow of commodities coming into the facilities to be located as well as the flow going out of those facilities are simultaneously considered in the models, these location problems are referred to as \textit{two-echelon} location problems. For example, while locating a distribution centre, the transportation cost of products from the factory to this distribution centre as well as the transportation costs from the distribution centre to the retailers may need to be considered in the model. Sometimes there are facilities to be located at several echelons where flows of commodities in and out of all those facilities need to be considered. These \textit{multi-echelon} types of location problems are encountered for several applications of supply chain network design \citep{melo2009facility_SAA}. Another related category is when there is a hierarchical network structure among the facilities to be located, referred to as \textit{hierarchical} facility location problems \citep{csahin2007review_SAA}. An example of a hierarchical location problem is designing a postal delivery network where the locations of the sorting centres as well the locations of the post offices that are to be allocated to those sorting centres need to be determined. 

There might also be interactions among the facilities to be located. This is the case, for example, for \textit{hub location} problems where the demand is defined between pairs of demand points (origin-destination pairs) as opposed to having the demand of an individual point. 
In that case, to satisfy the demand from an origin to a destination point, flow can be transported between the facilities to be located en route to the destination, where those facilities can act as switching, transshipment, sorting, connection, consolidation, or break-bulk points. Hub location models have several applications in passenger and freight airlines, express shipment, postal delivery, trucking, public transit, and telecommunication network design \citep{alumur2021perspectives_SAA}.

Location problems have been a testbed for many algorithmic and methodological advances in operations research. Most discrete location problems commonly belong to a class of $\mathcal{NP}$-hard decision problems (\S\ref{sec:Computational_complexity}) and they can usually be formulated with mixed-integer programming (MIP) models (see \S\ref{sec:Mixed_integer_programming}). In addition to using commercial MIP solvers, several exact and (meta)heuristic algorithms (\S\ref{sec:Heuristics}) have been developed and tested on benchmark instances from the literature. Some of those benchmark instances can be obtained from \cite{beasley1990or_SAA}, \cite{posta2014exact_SAA}, and \cite{fischetti2017redesigning_SAA}.

Location science is a very broad field of research that encompasses geography, continuous and discrete optimisation (\S\ref{sec:Combinatorial_optimisation}), graph theory (\S\ref{sec:Graphs_and_networks}), logistics (\S\ref{sec:Logistics}), and supply chain management (\S\ref{sec:Supply_chain_management}). This section only highlights the basic and most well-known location models. For a more detailed overview of the field of location science, we refer the reader to several books written in this field, such as \cite{drezner2004facility_SAA}, \cite{eiselt2011foundations_SAA}, and \cite{LNS15_SMPT}. 

\subsection[Logistics (Janny~Leung \& Yong-Hong~Kuo)]{Logistics\protect\footnote{This subsection was written by Janny~Leung and Yong-Hong~Kuo.}}
\label{sec:Logistics}

Logistics refers to the organisation and implementation of the processes related to the procurement, transport and maintenance of materials, personnel and facilities. The application of operational research to logistics dates back to 1930 \citep{schrijver2002history_JLYHK}, where \cite{tolstoi1930metody_JLYHK} solved to optimality the problem of transporting salt, cement, and other cargo on the railway network of the Soviet Union. In general, the objective of logistics management can be summed up as ``getting the right thing/people to the right place at the right time in the right quantity at the right cost''. For materials, logistics operations require the co-ordination of forecasting, purchasing, inventory control, warehousing, distribution, transportation, delivery and installation. Logistics management of personnel involves, in addition, skills matching,   capabilities training, labour rules and worker preferences. At a strategic level, logistics involves the design of the transport network and facilities. In this subsection, we discuss several major domains of logistics applications, namely, military, inventory, time-sensitive, reverse and humanitarian logistics. We also mention some new technologies for emerging logistics applications.

\subsubsection*{Military Logistics}

Logistics play an important role in military operations. Indeed, the word ``logistics" itself is derived from the position {\it{Mar\'echal des logis}} created in the French army in the 17\textsuperscript{th} century, whose responsibilities of establishing camps and arranging transport/supplies were referred as ``la logistique" \citep{Jomini1862_JLYHK}. Many historians credited logistics as the success factor in wars from ancient to modern times. During World War II, the need for large-scale logistics planning accelerated the development of operational research. The ability to sustain the convoy of supply ships was a major factor in the Battle of the Atlantic \citep{Kirby2003_JLYHK}. The 1948-1949 Berlin Airlift, where over 2.3 million tons of goods were flown to besieged West Berlin, is well-known as the first use of logistics as a military and political strategy \citep{Tine2005_JLYHK}.

The North Atlantic Treaty Organisation defines logistics as the science of planning and carrying out the movement and maintenance of forces, and covers acquisition, transport, maintenance and evacuation of materiel, personnel and facilities, and provision of services and medical support. Operational research methodologies are extensively used \citep{Scala2020_JLYHK}. Reliability and operability of the supply lines are a major concern in military logistics \citep{McConnell2021_JLYHK}, and simulation is much utilised. \cite{Cioppa2004_JLYHK} review agent-based simulation for military applications. Emerging technologies -- such as additive manufacturing \citep{Boer2020_JLYHK} and unmanned transport \citep{Jotrao2021_JLYHK} -- have also sparked research in smart military logistics \citep{Schutz2017_JLYHK}. The reader is also referred to \S\ref{sec:Military_and_Homeland_security}.

\subsubsection*{Inventory Logistics}

In modern logistics, most activities are related to products and goods, where their availability to customers or users is a key concern. Inventory, thus, plays an important role in this respect. A classic problem related to inventory logistics is the inventory-routing problem (IRP), introduced by \cite{bell1983improving_JLYHK} for the distribution of industrial gases. Since then, various IRP applications have been studied, including those related to automobile components \citep{blumenfeld1985analyzing_JLYHK}, groceries \citep{gaur2004periodic_JLYHK}, cement \citep{christiansen2011maritime_JLYHK}. Typically, IRP arises in vendor-managed inventory systems as the supplier monitors the inventory and makes replenishment decisions for its retailers \citep{archetti2007branch_JLYHK}. Because inventory can be carried from one period to the next, IRP considers {\it joint} decisions of inventory and routing across multiple periods and aims to minimise the total transportation and inventory holding costs over the planning horizon, subject to all demands being satisfied. \cite{speranza1994minimizing_JLYHK} extended the IRP to settings with multiple products. When demands are uncertain, IRP becomes stochastic IRP \citep{federgruen1984combined_JLYHK,trudeau1992stochastic_JLYHK}, where the objective function includes additional shortage cost. \cite{coelho2014heuristics_JLYHK} investigated the stochastic dynamic IRP where decisions are made as customers' demand become realised. Inventory logistics is even more timely today due to e-commerce \citep{archetti2021recent_JLYHK}. The main challenge of these inventory logistics problems is due to the computational complexity of solving multiple $\mathcal{NP}$-hard problems simultaneously. The reader is also referred to \S\ref{sec:Inventory}.

\subsubsection*{Time-sensitive Logistics}

The quality and functionality of items, in storage or transit, deteriorate over time. For items such as fresh vegetables, the value degrades continuously. Other perishables, such as blood, have fixed lifetimes and cannot be used beyond expiry. Logistics management of time-sensitive goods must consider production, distribution and transport jointly. \cite{Federgruen1986_JLYHK} was one of the first papers to consider jointly inventory allocation and transportation for fixed-lifetime perishables with probabilistic demand. Since then, there has been much research exploring additional issues, such as freight consolidation \citep{Hu2018_JLYHK}, storage/transport capacities \citep{Crama2022_JLYHK} and environmental concerns \citep{GOVINDAN2014_JLYHK}. \cite{Shaabani2022_JLYHK} gives a comprehensive literature review.

For continually decaying food items, delivery costs must be traded off with freshness-upon-arrival which may lead to lost sales or revenue \citep{Mirzaei2015_JLYHK}. The overall network design -- especially decisions on where along the supply chain processing occurs -- is important, since deterioration rates differ for unprocessed vs.~finished/packaged goods, and for items in transport vs.~in storage \citep{DeKeizer2017_JLYHK}.

An important category of perishable goods is blood. Integer-programming models were developed by \cite{Hemmel2009_JLYHK} for collection and distribution of blood products to Austrian hospitals, and by \cite{Araujo2020_JLYHK} for blood delivery in south Portugal. \cite{Piraban2019_JLYHK} survey research on blood supply chain management. 

\subsubsection*{Reverse Logistics}

Due to increased sustainability awareness and legislation, reducing the environmental impact of production and distribution has become important. Twenty years ago, \cite{Beamon1999_JLYHK} advocated that supply chains must be extended from one-way to a closed loop, where used products and materials are recovered for {\em re-use, recycle or re-manufacture}. Reverse logistics, thus, refer to the material flow from the point of consumption back upstream for regenerating value \citep{Rogers2001_JLYHK}. Compared to a forward supply chain, reverse logistics processes are more complicated. Firstly, the source, quality and quantity of recoverable used products/materials from end-users are highly unpredictable. There is an added decision-stage for inspection, evaluation and sorting of the collected materials, and streaming them into various processes (re-use, re-manufacture, disassembly, disposal, etc.). These re-purposing processes may be expensive, so trade-offs must be made between recovery cost and salvage value.

Re-manufacturing, where items are repaired to serviceable (like-new) condition, is an important aspect of reverse logistics. \cite{Simpson1978_JLYHK} was the first to address a multi-period repairable inventory problem with random demand and returns supply; using dynamic programming, he found the optimal policy structure which specified the repair, purchase and scrap levels for each period. Later, the model was extended to consider side-sales \citep{Calmon2017-gn_AM} and warranty demands \citep{Lin2020_JLYHK}.  Nowadays, the concept of reverse logistics is broadened holistically to closed-loop supply chains and the circular economy \citep{SG2019_JLYHK}. See \cite{Van_Engeland2020-qq_JL} for a recent review.

\subsubsection*{Humanitarian Logistics}

When disasters strike, speedy evacuation and prompt delivery of resources to affected areas are critical.
From some sparse early studies \citep{Sherali1991_JLYHK}, humanitarian logistics research grew rapidly since 2000. The research stream yielded insights that have changed how humanitarian agencies plan and manage disaster relief. A key concept is {\em inventory pre-positioning} where depots are set up already stocked with supplies {\em in anticipation} of disaster occurrences, instead of scrambling for procurement in the aftermath. \cite{Duran2011_JLYHK} developed a facility-location and supply pre-positioning plan for CARE. See also \cite{Rawls2011_JLYHK}. Many of the models used are large-scale mixed-integer-programming models.

Humanitarian logistics involve multiple objectives: costs, response urgency and fairness are all important.
\cite{Huang2012_JLYHK} considered {\em equity} in last-mile distribution;  \cite{Sheu2014_JLYHK} incorporated perceptions of people awaiting rescue. Other researchers considered decision under uncertainty:  \cite{Mete2010_JLYHK} developed a stochastic model for location and delivery of medical supplies. Yet other research took an interdiction approach and anticipated post-disaster deployment \citep{OHanley2011_JLYHK, Irohara2013_JLYHK}. Recent technological advances have stimulated new research and practices. \cite{Maharjan2020_JLYHK} investigated pre-positioning of mobile logistics/ telecommunications hubs for Nepal. See \cite{Behl2019_JLYHK} for a survey. The reader is also referred to \S\ref{sec:Disaster_relief_and_humanitarian_logistics}.

\subsubsection*{Emerging Technologies}

As technologies advance, the role of logistics has become more important in the Industry 4.0 era \citep{tang2019strategic_JLYHK}. Tracking and locating technologies (RFID, GPS, IoT, etc.) enable organisations and companies to acquire information in real time. Powerful computing facilities can perform analytics of massive volumes of historical data to support near real-time solutions for large-scale problems -- essential for city logistics involving thousands of orders to fulfil within a day, or even an hour. Exciting emerging applications include TSP/VRP for routing of drones \citep{masmoudi2022vehicle_JLYHK} and/or autonomous vehicles \citep{reed2022impact_JLYHK}, risk analysis powered by block-chain technology \citep{choi2019mean_JLYHK}, flow-based optimisation for crowd-sourcing logistics \citep{sampaio2020delivery_JLYHK} and cargo hitching \citep{fatnassi2015planning_JLYHK}, demand-driven optimisation for car/bike sharing systems \citep{wang2022dynamic_JLYHK}, and queuing and simulation for robotic warehouses \citep{fragapane2021planning_JLYHK}.

These more complex and larger-scale problems with tighter response times require new solution methodologies. Most of these new approaches are combinations of operational research and data science techniques, for example, robust optimisation \citep{zhang2021robust_JLYHK}, reinforcement learning \citep{yan2022reinforcement_JLYHK} and other machine learning-based optimisation approaches \citep{bengio2021machine_JLYHK}.

\subsection[Manufacturing (Kathryn~E.~Stecke \& Xuying~Zhao)]{Manufacturing\protect\footnote{This subsection was written by Kathryn~E.~Stecke and Xuying~Zhao.}}
\label{sec:Manufacturing}

Manufacturing is the production process from materials to goods. Such goods can be finished goods sold to end consumers or components sold to other manufacturers for the production of other more complex products. Manufacturing has gone through several different phases (Industry 2.0 to 4.0) in the twentieth and twenty-first centuries. Here we offer an overview of important manufacturing topics in different time periods. 

In Industry 2.0 (from the end of nineteenth century to the 1980s), demand was relatively stable. Important manufacturing systems include the Toyota production system (TPS) and cellular manufacturing. The aim of these systems is to increase productivity with lower production cost, which fits the needs of a stable market during this time period. 

Taichi Ohno published Toyota Seisan Hoshiki describing the TPS in 1978. TPS is an integrated production system that can supply products to meet both requirements of product volumes and product varieties. Research and practical papers, reports, and books were published in various media to describe TPS. The underlying management principles and theoretical mechanisms of TPS are well-known. A TPS is an integrated production system that generates products to satisfy requirements of volumes and varieties simultaneously with minimum resource waste. A large number of TPS enablers have been reported and include just-in-time material system (JIT-MS), seven wastes, heijunka, multi-skilled workers, quick set-up and changeover, and keiretsu. Excellent analysis and review papers on the TPS are \cite{Treville2006-ak_KESXZ}, \cite{Hines2004-fo_KESXZ}, and \cite{Narasimhan2006-qt_KESXZ}. 

Cellular manufacturing (CM) uses group technology to efficiently produce a high variety of parts. Cells are converted from job shops with functional layouts to improve efficiency \citep{Yin2006-vm_KESXZ}. A cell consists of a machine group and a part family. The first step in CM system design is cell formation. Part families and machine groups are identified to form manufacturing cells such that the intercell movements of parts are minimised. Parts in the same family have similar machining sequences. Machines in a cell are arranged to follow this sequence. In this way, parts flow from machine to machine in their processing sequence, resulting in an efficient machining flow that is similar to an assembly line. For each part family, the volume of any particular part type may not be high enough to utilise a dedicated cell. The total volume of all part types in a part family should be high enough to utilise a machine cell well. CM attempts to flexibly accommodate high variety and simultaneously efficiently take advantage of flow lines \citep{Celikbilek2015-dz_KESXZ}.

In Industry 3.0 (from the 1980s to today), demand is relatively volatile because of technological innovations, higher product variety, and shorter product life cycles. The important manufacturing topics include flexible manufacturing systems (FMSs) and \textit{seru} production system. The theme of these topics is to meet the increased demand for high variety and short delivery time. Product life cycles decreased during this time period, which drives manufacturers to focus on responsiveness and delivery time. Short changeover time between different product types is useful. 

An FMS is an integrated, computer-controlled manufacturing system of automated material handling and computer numerically controlled machine tools that can simultaneously process medium-sized volumes of a variety of part types. A fully automated FMS can attain the efficiency of well-balanced, machine-paced transfer lines, while utilising the flexibility that job shops have to simultaneously machine multiple part types \citep{Stecke1981-eb_KESXZ,Stecke1983-lc_KESXZ,Browne1984-ec_KESXZ}.

A seru production system is an assembly system that has been adopted by many Japanese electronics companies. 
\cite{Yin2008-jw_KESXZ} is the first English language paper on seru production. They describe and analyse the success of seru production systems in Canon and other Japanese companies. It is more flexible than TPS, which cannot achieve the required responsiveness in this innovative time period. A seru production system consists of one or more serus. Serus within a seru system are quickly reconfigurable, i.e. they can be constructed, modified, dismantled, and reconstructed frequently in a short time. There are three types of serus, called divisional serus, rotating serus, and yatais. They represent the evolutionary development of serus. A divisional seru is a short, often U-shaped, assembly line staffed with several partially cross-trained workers. Tasks within a divisional seru are divided into different sections. One worker is in charge of each section. A rotating seru is often arranged in a U-shaped short line with several workers. Each worker performs all required tasks from start to finish without interruption. Tasks are performed on fixed stations, so workers walk from station to station. A worker follows the worker ahead of her or him, and is also followed by the worker behind her or him. A seru with only one worker is called a yatai. An important performance of the seru production system is that it can quickly respond to product varieties with fluctuated volumes. By applying seru, delivery time is reduced. Variety and volume are easily handled. 

The TPS-based assembly line became inefficient because of an inability to change very frequently to produce small-volume demands. The typical seru creation process in Sony and Canon can be summarised as follows \citep{Yin2017-gh_KESXZ}. Assembly lines were dismantled and replaced with divisional seru systems through resource co-location and removal/replacement, cross training, and autonomy. The technique of karakuri (involves procedures to discover and appropriate the useful functions of expensive equipment into inexpensive self-made equipment) is applied to replace expensive dedicated equipment by inexpensive self-made and/or general-purpose equipment that can be duplicated and redeployed as needed by serus. As cross-training progresses, divisional serus evolve into rotating serus and yatais. 

More details about the underlying management and control principles of seru can be found in \cite{Stecke2012-rg_KESXZ}, \cite{Yin2008-jw_KESXZ}, \cite{Yin2017-gh_KESXZ}, and \cite{Liu2014-mk_KESXZ}. \cite{Roth2016-pg_KESXZ} reviews the last 25 years of OM research and provides eight promising research directions, one of which is seru production systems. 

Today manufacturing has entered a new age (Industry 4.0) because of the emergence of disruptive technological innovations. Examples of important manufacturing topics include smart manufacturing, mass-customisation, sustainable manufacturing, and additive layer manufacturing. \cite{Strozzi2017-ff_KESXZ} examines the evolution, trends, and emerging topics of a smart factory and provides topics for future research. \cite{Hughes2022-jg_KESXZ} provides a review for manufacturing in the Industry 4.0 era. 

Smart manufacturing refers to flexible and adaptable manufacturing processes through integrated systems and using advanced technologies such as sensors, IoT, cloud computing, big data, artificial intelligence, automation, robots, cyber-physical systems, and additive layer manufacturing. Some detailed discussions can be found in \cite{Ivanov2016-qo_KESXZ}, \cite{Kersten2017-zq_KESXZ}, \cite{Liao2017-qi_KESXZ}, \cite{Theorin2017-ae_KESXZ}, \cite{Thoben2017-il_KESXZ}, and \cite{Hughes2022-jg_KESXZ}.   

One important benefit of smart manufacturing is that it aids the capability of mass customisation and short lead time to quickly meet changing demands. \cite{Zawadzki2016-ek_KESXZ} suggested smart product design and production control for efficient operations in a smart factory to enable mass customisation. \cite{Brettel2014-ho_KESXZ} show that self-improving smart manufacturing systems can utilise data and quickly react (e.g., reconfigure) to personalised customer orders, which helps realise mass customisation. Some efficient mathematical models that use big data to manage and control manufacturing processes can be applied in smart factories \citep{Ivanov2016-qo_KESXZ,Ivanov2017-rv_KESXZ}. 

Sustainable manufacturing aims to minimise negative environmental impacts while conserving energy and natural resources. Sustainable manufacturing also enhances employee, community, and product safety. The emergence of blockchain technology and its potential disruption within the manufacturing and supply chain industries present opportunities for greater levels of sustainability in Industry 4.0. The immutability and smart contract capability of blockchain technology allow the provenance and integrity of products to be monitored more effectively. These factors contribute to reducing verification costs and the provision of real-time status information on the quality of materials throughout the supply chain \citep{Ko2018-pi_KESXZ}. The disintermediation attributes of blockchain can directly contribute to manufacturing sustainability by effectively reducing complexity, and improving efficiency with less waste via the streamlining of the supply chain \citep{Hughes2019-iv_KESXZ}. 

Additive layer manufacturing may generate a disruptive and revolutionary impact on manufacturing \citep{Garrett2014-on_KESXZ}. It enables a manufacturer to further increase responsiveness by reducing lead time and increasing customisation levels. \cite{Long2017-ie_KESXZ} provide a definition, characteristics, and mainstream technologies of 3D printing. \cite{Dong2016-aq_KESXZ} compared the optimal assortment strategies under traditional flexible technology and 3D printing to find that 3D printing may allow a larger set of product assortment. \cite{Song2020-ti_KESXZ} and \cite{Ivan2017-ux_KESXZ} examined the use of 3D printing on a logistics system for spare parts inventory design. 3D printing tends to be slower than other manufacturing methods, which currently limits its use in practice.  

For a detailed encyclopedic overview of the manufacturing field, both in terms of theory and practice, see \cite{Yin2017-gh_KESXZ}. They discuss and compare production systems from Industry 2.0 to Industry 4.0. The demand dimensions of each industry era are analysed and provided as the driving force for the changes in the production systems over time.

\subsection[Military and homeland security (Kai~Virtanen \& Raimo~P.~Hämäläinen)]{Military and Homeland security\protect\footnote{This subsection was written by Kai~Virtanen and Raimo~P.~Hämäläinen.}}
\label{sec:Military_and_Homeland_security}

The birth of OR is related to the use of optimisation modelling for military operations and resource planning during the Second World War. The early linear programming (\S\ref{sec:Linear_programming}) problems ranged from the efficient use of weapon systems to logistics and strategy planning. Today, the arena of defence has expanded extensively with new areas including information and cyber warfare. The need to counter terrorism has created the field of homeland security. OR has a role in all these emerging topics. One can say that all OR methods are applied in military and homeland security problems.

Optimisation methods are used in a wide range of defence and security applications. For instance, assigning weapons to targets \citep{Kline2019-fx_KVRH} using integer programming (\S\ref{sec:Mixed_integer_programming}; \S\ref{sec:Combinatorial_optimisation}) has been addressed with a variety of optimisation algorithms. Other integer programming studies include, for example scheduling of training for military personnel \citep{Fauske2016-fi_KVRH} as well as military workforce and capital planning \citep{Brown2004-mg_KVRH}. Mixed integer linear programming is utilised in diverse applications such as path planning of unmanned ground and areal vehicles including drones, mission planning, acquisition decisions of military systems as well as load planning in transportation. Optimisation of vehicles’ routes is also carried out by solving network optimisation problems (\S\ref{sec:Graphs_and_networks}) with shortest path algorithms \citep{Royset2009-ps_KVRH}. Network optimisation is also used, e.g., in developing military countermeasures. Examples of bilevel and robust optimisation (\S\ref{sec:Stochastic_models}) formulations cover positioning of defensive missile interceptors \citep{Brown2005-ao_KVRH} and design of a supply chain for medical countermeasures against bioattacks \citep{Simchi-Levi2019-pv_KVRH}. Multiobjective optimisation has been applied, for example in optimising boat resources of coast guard \citep{Wagner2012-rt_KVRH} and planning of airstrikes against terrorist organisations \citep{Dillenburger2019-ir_KVRH}. Inherent structures of specific military optimisation problems have motivated the development of new solution techniques \citep{Boginski2015-iz_KVRH} including, for example, metaheuristics (\S\ref{sec:Heuristics}). Such techniques are used, e.g., for solving nonlinear military optimisation tasks (\S\ref{sec:Nonlinear_programming}) such as design of projectiles.

Game theoretic modelling (\S\ref{sec:Game_theory}) is used in many defence studies. Information related topics include misinformation in warfare \citep{Chang2022-xf_KVRH} and public warnings against terrorist attacks \citep{Bakshi2018-hw_KVRH}. Examples of game theoretic strategy design problems cover the optimal use of missiles and the validation of combat simulations \citep{Poropudas2010-jx_KVRH}. Designing security and counter strategies against enemies, terrorists and adversarial countries naturally lead to the use of game models. Interdiction network game models arise in security applications \citep{Holzmann2021-ib_KVRH}, and they are used, e.g., in route planning through a minefield.

Military simulation models (\S\ref{sec:Simulation}) are classified into constructive, virtual and live simulations \citep{Tolk2012-fx_KVRH}. Constructive simulations do not involve real-time human participation. They are based on well-known modelling methodologies such as Monte Carlo, discrete event and agent-based simulations. Applications of constructive models cover, e.g., the development and use of weapons, sensor and communications systems, planning of operations and campaigns, improving maintenance processes of military systems, and evaluating effects of fire. In addition, cyber-defence analyses have been conducted \citep{Damodaran2020-us_KVRH}. Constructive simulations have also been used in simulation-optimisation studies such as scheduling maintenance activities of aircraft, military workforce planning, and aircraft fleet management \citep{Mattila2014-mm_KVRH,Jnitova2017-vp_KVRH}.

The complexity of modelling human behaviour generates a major challenge for constructive simulation. This issue is avoided in virtual simulations, i.e. simulators in which real people operate simulated systems and in live simulations where real people operate real systems with simulated weapon effects. These practices are typically used, e.g., in military exercises and training of personnel. An emerging trend is to combine live, virtual and constructive simulations into a single simulation activity \citep{Mansikka2021-is_KVRH}. Applications of this simulation type vary from training to testing large-scale systems and mission rehearsals \citep{Hodson2014-ft_KVRH}. In a combined simulation, new ways to measure performance are introduced \citep{Mansikka2021-rw_KVRH} by complementing traditional measures such as loss exchange or kill ratio by human measures such as participants' situation awareness, mental workload and normative performance \citep{Mansikka2019-ol_KVRH}.

Features of virtual simulation can be recognised in wargaming \citep{Turnitsa2021-dz_KVRH} that has been used for military training and educating since the early 19\textsuperscript{th} century. Other wargaming areas are, for example, examination of warfighting tactics as well as evaluation of military operations and scenarios. Nowadays, wargames are also applied in studies of international relations and security as well as in analyses of government policy, international trade, and supply-chain mechanics \citep{Reddie2018-zs_KVRH}. The implementations of wargames range from manual tabletop map exercises to computer-supported setups in which different OR and artificial intelligence techniques are utilised \citep{Davis2022-kx_KVRH}.

Dynamic phenomena regarding military and defence are often represented with differential or difference equations. Examples of these models are Lanchester attrition equations that describe the evolution of strengths of opposing forces in gunfire combat \citep[e.g.,][]{Jaiswal2012-md_KVRH}. There are also several modifications of these equations aiming to model, e.g., asymmetrical combat, tactical restrictions and even morale issues. Another example of simple combat models is the salvo model that represents naval combat of warships involving missiles \citep{Hughes1995-za_KVRH}. Optimal control (see also \S\ref{sec:Control_theory}) has been utilised, for example in planning optimal paths of military vehicles as well as in guidance systems of unmanned aerial vehicles, drones and missiles \citep{Karelahti2007-jb_KVRH}. For a recent overview, see for example \cite{Israr2022-woKAI}. Another type of optimal control application is the assignment of resources to counter-terror policies and measures \citep{Seidl2016-ga_KVRH}. Markov decision processes  and approximate dynamic programming (\S\ref{sec:Dynamic_programming}) have recently emerged as important techniques for analysing dynamic military decision-making problems related to, e.g., missile defence interceptors and military medical evacuation \citep{Jenkins2021-nx_KVRH}.

The need for multicriteria evaluation is common in military decision-making. Example applications of multi-criteria decision analysis (MCDA; see also \S\ref{sec:Decision_analysis}) are acquisition of military systems and equipment procurement, military unit realignments and base closures, locating military bases, and assessment of future military concepts and technologies \citep{Ewing2006-as_KVRH,Geis2011-zd_KVRH,Harju2019-mb_KVRH}. Public procurement even for the military is regulated in many countries, and directives require it to consider multiple criteria \citep{Lehtonen2022-yc_KVRH}. It is interesting to notice that the recent acquisition decision of a 5\textsuperscript{th} generation multirole fighter aircraft in Finland was, indeed, supported by MCDA \citep{Keranen2018-ok_KVRH}. MCDA weighting methods have also been used to create measures of mental workload in military tasks \citep{Virtanen2021-ft_KVRH}. In portfolio decision analysis problems, the goal is to find the best set of components, e.g., in weapons systems or in force mix for reconnaissance, with respect to multiple criteria \citep{Burk2011-ns_KVRH}. The evaluation of the effectiveness of military systems calls also for the use of cost-benefit analysis  \citep[\S\ref{sec:Risk_analysis};][]{Melese2015-zf_KVRH}. Data envelopment analysis (\S\ref{sec:Data_envelopment_analysis}) is a multicriteria approach helping to seek efficiency also in military problems such as personnel planning.

MCDA studies in homeland security is a broad area ranging from the design of countermeasure portfolios to threat analysis and cyber-security \citep{Wright2006-ug_KVRH}. The questions of interest include, e.g., identification of terrorists' goals and preferences, estimation of attacks' consequences, and comparison of countermeasure actions \citep{Abbas2017-sv_KVRH}. Cost-benefit models are also relevant in terrorism research \citep{Hausken2018-gv_KVRH}.

Today, we are witnessing the vast growth of the use of machine learning and artificial intelligence (\S\ref{sec:Artificial_intelligence_machine_learning_data_science}) in military and security problems \citep{Dasgupta2022-mj_KVRH,Galan2022-xx_KVRH}. Such problem areas are, e.g., wargaming and simulation, command and control of autonomous unmanned vehicles including drones, air surveillance, and cyber-security only to mention a few. Data analytics (see also \S\ref{sec:Business_analytics}) is naturally also used in military OR \citep{Hill2020-sv_KVRH}, e.g., for supporting logistics planning. Considering uncertainty is essential, e.g., in intelligence analysis and risk analysis related to terrorism (see also \S\ref{sec:Risk_analysis}) . Adversarial risk analysis \citep{Rios_Insua2021-yx_KVRH} uses Bayesian approaches (see also \S\ref{sec:Risk_analysis}) for taking into account information, beliefs and goals of adversaries. A similar approach is also applied in the modelling of pilot decision-making in air combat with influence diagrams \citep{Virtanen2004-su_KVRH}. Markov models and Bayesian networks are used to evaluate risks and conduct time dependent probabilistic reasoning related to military missions \citep{Poropudas2011-ns_KVRH}. \cite{Kaplan2010-dt_KVRH} studies the infiltration and interdiction of terror plots using queueing theory (\S\ref{sec:Queueing}).

In the future, combat models need to include socio-cultural and behavioural factors \citep{Numrich2012-ov_KVRH}. We are also likely to see an increase in the modelling of individual and group behaviour as well as the consideration of behavioural issues in military and homeland security contexts. Behavioural game theory can give insights into military strategy and conflict situations. Behavioural OR (\S\ref{sec:Behavioural_OR}), which studies the impacts of the human modeller and model users including cognitive biases in decision support, is likely to receive increasing attention in military applications as well.

For further readings, we refer to the military OR textbooks by \cite{Fox2019-ky_KVRH} and \cite{Jaiswal2012-md_KVRH}. The recently edited volume by \cite{Scala2020_JLYHK} describes various OR methods and how to apply them in military problems. \cite{Abbas2017-sv_KVRH} and \cite{Herrmann2012-cg_KVRH} focus on homeland security modelling.

\subsection[Natural Resources (Emel~Aktas)]{Natural Resources\protect\footnote{This subsection was written by Emel~Aktas.}}
\label{sec:Natural_Resources}

Climate change and natural resource management require different quantitative and qualitative models that support public policy \citep{Ackermann2021_EA}. One of the early papers on the use of modelling for natural resource utilisation describes a resource analysis simulation procedure to assess the environmental impact of human activities \citep{Bryant1978_EA}. The procedure comprises a structural model to express the complex network of interacting human activity systems and a parametric model to determine the scale of the activity being modelled. 

An integrated decision support system for water distribution and management was built to generate alternative water allocation and agricultural production scenarios for a semi-arid region \citep{Datta1995_EA}. The model considers ground and surface water sources as the supply. The water demand is a combination of the need for drinking, irrigation, household and public utility, natural vegetation, industrial use, and ecological balance. The decision support tool visualises water allocation to competing crops under a range of simulation scenarios, providing a wider set of options to the department taking decisions how water is distributed. 

A web-based decision support system developed for the US Fish and Wildlife Service and the US Geological Survey initiative facilitates cross-organisational data sharing and performs analyses to improve conservation delivery \citep{Hunt2016_EA}. Situation-specific management actions such as controlled burn or prescribed graze required by this decision support tool improves ecological outcomes of other conservation efforts. Buffelgrass is an invasive species that causes significant damage to the native desert ecosystem. A multi-period multi-objective integer programming model was proposed to find optimal treatment strategies to control the buffelgrass population in the Arizona desert \citep{Buyuktahtakin2014_EA}. The multiple objectives minimise damage to threatened resources: a native cactus species (saguaros), buildings, and vegetation subject to budget and labour constraints. The results show the necessity of cooperation between different interest groups to establish reasonable treatment strategies and the need for a policy change because current resources cannot stop an ecological disaster in the future.

A mixed-integer programming model is developed to evaluate fishery management policies over an infinite horizon by incorporating steady state levels of variables into a multi-period analysis framework \citep{Glen1997_EA}. This model is intended to be used annually with updated stock estimates to set a dynamic total allowable catch per year depending on the stock estimates over several years. Statistical and fuzzy multiple criteria analysis establishes which materials contribute the most to the presence and the abundance of species in artificial reef structures \citep{Shipley2018_EA}. Managers of fisheries can use this model to screen different species without loss of rigour and validity of results. Multiple-criteria analysis is used in conjunction with integer programming to assist complex management plans in ecology and natural resources \citep{miranda2020_EA}. A case study on the Mitchell River catchment (Australia) shows the trade-offs between ecological, spatial, and cost criteria, enabling decision-makers to explore and analyse a broad range of conservation plans. The use of catastrophe theory in management of natural resource systems are described with cases on forestry and fishery management \citep{Wright1983_EA}. Catastrophe theory applies the mathematical theory of structural stability to practical systems. It allows modelling of ecosystems with low and high levels of predator and prey populations. It helps model a catastrophic jump from one level to the other, supporting decision making for management of natural resources. 

As a natural resource, wind provides clean, renewable, and sustainable energy. A multi-objective model minimises cost and idle time under reliability thresholds, maintenance priority, and opportunism \citep{Ma2022_EA}. Reliability thresholds trigger maintenance activity. Maintenance priority indicates which maintenance tasks need to be performed under limited maintenance resources. Opportunistic approach indicates whether additional maintenance should be performed when a maintenance team is already out to service several turbines. The proposed multi-objective optimisation model is tested using a stochastic simulation model of a wind farm and confirmed to keep the wind system at a higher performance level with lower cost and higher availability.

Natural resource exploration is frequently subject to real options analysis \citep{Nishihara2012_EA, Martzoukos2009_EA}. A stochastic mixed integer nonlinear programme is proposed to incorporate geological and market uncertainty into mineral value chain optimisation \citep{Zhang2018_EA}. Simulation of mine deposits and commodity market informs the profitability of strategic and tactical plans. A range of real options applied to natural resource management include investments in infrastructure, use of land, and management of natural resources \citep{Trigeorgis2018_EA}. Firms require high output price levels to invest in environmental technologies, because they would not want to commit to an investment that could turn out to be unprofitable in the event of a price fall \citep{Cortazar1998_EA}.

Several papers are published on the use of operational research for natural resource management. Typical operational research problems and actors in agricultural supply chains informs strategic investment and operations management under increased pressure on natural resources \citep{pla2014_EA}. The contribution of operational research applications to agricultural value chain sustainability and resilience call for applications of complex systems methods such as agent-based modelling, systems modelling, and network analysis \citep{Higgins2010_EA}. A review of environmental management and sustainability papers in major management science/operational research and systems journals revealed dominance of hard optimisation methods \citep{paucar2011_EA}. 

The environment-development problem concerns reconciling industrial development and environmental protection. A methodological framework is proposed to model the environmental impact of development under uncertainty arising from the degree of unpredictability arising from decision makers and environmental processes \citep{Dzidonu1993_EA}. Natural resource development contracts depend on the bargaining power of transnational corporations and host country governments \citep{Anandalingam1987_EA}. Contracts that stipulate sharing of the net income from resource development between the developing corporation and the government show that government would receiver higher income if many corporations are involved and if the government agrees to contribute to production costs. A review of Operational Research in mine planning reports optimisation and simulation applied to surface and underground mine planning problems, including mine design, long- and short-term production scheduling, and equipment selection \citep{Newman2010_EA}. The operational research on mining is evolving to solve larger and more detailed and realistic models.

A series of cases studies from Asia, Africa, and Latin America presents principles and applications of an integrated approach to natural resources management, including the complexity of systems and redirecting research towards participatory approaches, multi-scale analysis, and tools for systems analysis, information management, and impact assessment \citep{Campbell2003_EA}. A specialised book on the Baltic region presents scientific research on activities depleting natural resources, emissions from energy use, pollution, and strategies for environmental management \citep{Fenger1991_EA}. Stochastic Models and Option Values: Applications to Resources, Environment and Investment Problems presents a collection of research papers on the use of control theoretic methods to address problems that arise in natural resource development \citep{Lund1991_EA}. Strategic Planning in Energy and Natural Resources contain innovative and methodologically rigorous operational research applications \citep{Lev1987_EA}. The Handbook of Operations Research in Natural Resources collate research papers that address optimal allocation of scarce resources in agriculture, fisheries, forestry, mining, and water resources \citep{Weintraub2007_EA}. Operations Research and Environmental Management organises its content by regional and global policies \citep{Haurie2012_EA}. Models help local and regional authorities optimise their energy distribution and minimise natural resource waste.  

\subsection[Open-source software for OR (Cihan~Tugrul~Cicek \& Güneş~Erdoğan)]{Open-source software for OR\protect\footnote{This subsection was written by Cihan~Tugrul~Cicek and Güneş~Erdoğan.}}
\label{sec:Open_source_software_for_OR}

Commercial solvers for solving Operational Research (OR) problems have been used for several decades and have provided both practitioners and academics with access to the state-of-the-art OR techniques. Mathematical programming solvers IBM ILOG CPLEX \citep{Cplex_CTCGE}, Gurobi \citep{Gurobi_CTCGE}, BARON \citep{Baron_CTCGE}, and discrete event simulation software Arena \citep{Arena_CTCGE} and Simul8 \citep{Simul8_CTCGE} are among the best-known examples. There also exists ad-hoc software for particular problems raising in manufacturing \citep[e.g., AIMMS;][]{Aimms_CTCGE}, healthcare \citep[e.g., SimCAD Pro Health;][]{Simcad_CTCGE}, and logistics \citep[e.g., AnyLogistix][]{Anylogistix_CTCGE}. The strength of commercial software is primarily based on the fact that they provide users with a simple interface to declare a problem, utilise state-of-the-art solution algorithms, and visualise the result with minimal effort. 

These software do not only solve problems but also provide modelling, debugging, and scenario analysis to improve the solution process \citep{Dagkakis2016_CTCGE}. However, the lack of access to the source code and knowledge of how these tools work internally inhibit users from customisation. It is difficult to contribute to the development of commercial software as it is a black-box to the end-user. The high licence costs of those software has been one of the most prominent factors blocking many companies, especially small and medium enterprises, from integrating them into their tactical and operational planning \citep{Linderoth2005_CTCGE}. \cite{Dagkakis2016_CTCGE} argued that the lack of reusability and modularity have been the additional factors impeding the use of commercial OR software.

Open-source software, on the other hand, enable users to solve OR problems without a significant initial investment. Although using open-source software does not require licensing fees, the effort to deploy it may require a significant amount of effort and time. Nevertheless, the opportunity to access the core components of a solver (or simulator) and ease of development has driven the OR community to shift from a strict, slow-pace black-box software development to modular, flexible, and quick open-source software development. 

In this section, we discuss open-source OR software, categorising them into two main groups: (\textit{i}) open-source solvers and (\textit{ii}) open-source simulators. The former category covers the software focusing on solving mathematical programming problems. The latter includes software for simulating a real-world environment and helping decision makers to understand and analyse the system without consuming physical resources. Note that we neither provide the specific features of such software nor the characteristics in terms of programming languages, etc. Interested readers are referred to the comprehensive reviews in \cite{Linderoth2005_CTCGE} and \cite{Dagkakis2016_CTCGE} for some of the software we mention below.

\subsubsection*{Open-source Solvers}
A solver can be defined as a set of computationally efficient analytical tools that can find optimal (or near-optimal) solutions to a mathematical programming model. In 2000, a public initiative was built by the IBM Research Division \citep{Coinor_CTCGE} to promote and support community-driven development of open-source solvers that utilise the state-of-the-art research in OR. Subsequently, a public project called COIN-OR \citep{Coinor2_CTCGE} has been initiated to host a range of open-source solvers with an open-source interface that enables contributors, users, and developers to implement their own algorithms. The repository has been expanded to provide different open-source solvers for different programming problems such as CLP \citep{CLP_CTCGE} and HiGHS \citep{Huangfu2018_CTCGE} for linear programming (LP); ABACUS \citep{Abacus_CTCGE}, BCP \citep{Bcp_CTCGE}, CBC \citep{Cbc_CTCGE}, Pyomo \citep{Pyomo_CTCGE}, and SYMPHONY \citep{Symphony_CTCGE} for mixed integer linear programming (MILP); Bonmin \citep{Bonmin_CTCGE}, Couenne \citep{Couenne_CTCGE}, DisCO \citep{Disco_CTCGE}, Ipopt \citep{Ipopt_CTCGE}, and SHOT \citep{Shot_CTCGE} for linear and mixed integer nonlinear programming (MINLP); SMI \citep{Smi_CTCGE} and Pyomo \citep{Pyomo_CTCGE} for Stochastic Programming (SP). COIN-OR also includes several other projects that would help users to improve their experience with modelling like PuLP \citep{Pulp_CTCGE} and visualising such as GiMPy \citep{Gimpy_CTCGE}.

SCIP Optimization Suite \citep{Scipopt_CTCGE} can be used as a framework for mixed-integer linear or nonlinear programming as well as a standalone solver for such problems. A recent initiative commenced by the introduction of Julia language \citep{Julia_CTCGE}, which is a high-level, high-performance dynamic language for technical computing, is JuMP \citep{Jump_CTCGE} which helps users to solve a variety of problem classes including linear, mixed-integer linear, and nonlinear programming. It allows developers to use its framework and introduce new open-source solvers for particular problem classes.

We would like to also mention GLPK \citep{Glpk_CTCGE}, which is the default linear programming solver behind some of the aforementioned mixed integer linear programming solvers. GLPK can also be used as a standalone linear programming solver. Finally, a suite of open-source solvers has been developed by Google \citep{ORTools_CTCGE} to tackle integer programming and constraint programming problems. The OR-Tools provide a modelling interface and allow users to select different commercial or open-source solvers to generate solutions.

\subsubsection*{Open-source Simulation Software}
Simulation software can be categorised into three based on the methods that they use to define the system and its resources. We should note that we cover the software that has been applied particularly in OR domains. We opt to omit open-source software that focus on specific domains, e.g. OMNet++ \citep{Omnet_CTCGE} for communication networks, for the sake of brevity. We refer interested readers to the works of \citep{Dagkakis2016_CTCGE} and \citep{Lang2021_CTCGE} and references therein for a broader review. An experimental comparison of some of the software presented here can be found in \cite{Kristiansen2022_CTCGE}.

The first method, Discrete Event Simulation (DES), is based on the processes of a system. In DES, the processes are defined as hosts of resources that run different operations on entities. For instance, one can define a part to be manufactured as an entity and create a manufacturing process with three machines to shape the part. DES software can be used to model and visualise complex queuing systems in order to help decision makers better understand the interactions between entities and processes.

JaamSim \citep{Jaamsim_CTCGE} is one of the most popular open-source DES with its user-friendly interface, easy-to-use drag and drop facilities, and continuous maintenance support. JaamSim provides a standalone executable which allows users to start using the software without technical knowledge on installations. Another DES framework is SimPy \citep{Simpy_CTCGE} which is based on standard Python functions. Its simple structure enables users to quickly obtain results for their simulation problems. SimPy has also initiated two other DESs, SimSharp \citep{Simsharp_CTCGE} and SimJulia \citep{SimJulia_CTCGE}, which are the implementations of SimPy on C\# and Julia languages, respectively. The last DES we would like to mention is Facsimile \citep{Facsimile_CTCGE} which uses Scala as its basis scripting language. The main purpose of Facsimile, is to provide a high-quality discrete-event simulation library that can be used for industrial projects.

The second method, known as System Dynamics (SD), is based on representing a system as a \emph{causal loop diagram} to define interactions between different components of a system. Some of the open-source SD software are PySD \citep{Pysd_CTCGE}, InsightMaker \citep{Insightmaker_CTCGE}, SysDyn \citep{Sysdyn_CTCGE}, and OpenModelica \citep{Openmodelica_CTCGE}. PySD can convert the well-known commercial SD software Vensim \citep{Vensim_CTCGE} input and allow user to configure. SysDyn uses the OpenModelica environment for simulation but provides an alternative built-in environment to speed up the simulation process. All these software have their own visualisation and reporting tools. 

The third method is called Agent Based Simulation (ABS) and focuses on the autonomous individuals, i.e., agents, in a system. Each agent in ABS has its own characteristics and its way to interact with the other agents and the surrounding environment can differ. One of the open-source ABS software is Gama \citep{Gama_CTCGE}, which provides users a modelling language, a cross-platform to reproduce simulations, as well as a visualisation tool. InsightMaker \citep{Insightmaker_CTCGE} is another open-source ABS software that allows users to create their own model on a web-based interface. Lastly, NetLogo \citep{NetLogo_CTCGE} provides a modelling environment together with different applications to interact with other scripting languages.

We would like to complete this section with a brief summary of the application areas of both solvers and simulation software. Table~\ref{tab:application_areas} provides examples of areas on which the OR software can be used.

\begin{table}[!htb]
    \centering
    \caption{Application areas of solvers and simulation software.}
    \begin{tabular}{p{0.12\textwidth}p{0.12\textwidth}p{0.65\textwidth}}
        \toprule
        Subject & Methodology & Application Areas \\
        \midrule
        Solver & LP & Transportation, agriculture, manufacturing \\
        & MILP & Logistics, healthcare, network design, pooling, disaster response  \\
        & MINLP & Scheduling, telecommunication, energy systems, layout design, network design, portfolio optimization, water systems \\
        & SP & Supply chain planning, production planning, process control and optimisation \\
        \midrule
        Simulation & DES & Manufacturing, network design, healthcare operations, financial systems, inventory management \\
        & SD & Telecommunication, macro- and micro-economics, social systems, ecological systems \\
        & ABS & Stock markets, robotics, epidemiology, game theory, evacuation planning \\
        \bottomrule
    \end{tabular}
    \label{tab:application_areas}
\end{table}

\subsubsection*{Discussion}
For the sake of completeness, we should also mention that there are also several ad-hoc software that address specific OR problems. For instance, OptaPlanner \citep{Optaplanner_CTCGE} can solve staff rostering, scheduling, timetabling, and Vehicle Routing Problems (VRPs). Another example is VRP Spreadsheet Solver \citep{Erdogan2017_CTCGE}, which is an Excel-based solver. Although these software provide easy and fast access to solutions, the lack of generalisation to more complex OR problems and limited development opportunities can be seen as barriers to widespread impact.

As a final discussion point, we would like to list some of the essential features of an open-source OR software. First and foremost is the performance of the software. A user would expect a comparable level of performance from an open-source software with respect to commercial software. Secondly, the scalability of a solver, i.e. its performance when the problem size increases, is one of the factors desired by practitioners. Finding an optimal solution to a VRP instance with 20 customers does not guarantee that a VRP solver will achieve the same performance when the number of customers increases to 2000. Thirdly, technical support for a software has a crucial role in attracting users. Continuous development, documentation, and clear descriptions to change requests are some of the aspects that an open-source software should address to improve its maintainability. Finally, integration with existing libraries would help an open-source software widen its community and attract more developers to contribute.

\subsection[Power markets and systems (Dimitrios~Sotiros \& Rafał~Weron)]{Power markets and systems\protect\footnote{This subsection was written by Dimitrios~Sotiros and Rafał~Weron.}}
\label{sec:Power_markets_and_systems}

The energy industry relies on forecasts (\S\ref{sec:forecasting}) and decision support tools (\S\ref{sec:Decision_analysis}) for operations and planning. While long-term demand forecasts -- with lead times measured in months, quarters or years -- have been used for planning purposes for over a century, contemporary energy forecasting literature focuses more on the short- (minutes, hours) and mid-term (days, weeks) horizons \citep{hon:pin:etal:20_DSRW}. 
Since the late 1990s, the workhorse of power trading and a typically used reference point for long-term contracts is the day-ahead market \citep{may:tru:18_DSRW}, where prices for all load periods of the next day are determined at the same time during a uniform-price auction \citep[][see also \S\ref{sec:Auctions_and_bidding}]{wer:14_DSRW}. No wonder, the majority of studies focus on predicting intermittent generation from \textit{renewable energy sources} (RES), electric load (or demand) and prices for the 24 hours of the next day \citep{mac:nit:wer:21_DSRW}. Two classes of approaches dominate: regression-based models and \textit{artificial neural networks} \cite[ANN;][see also \S\ref{sec:Artificial_intelligence_machine_learning_data_science}]{lag:mar:sch:wer:21_DSRW}.

Regression and ANN models of the 1990s and 2000s were built on expert knowledge, often independently for each hour of the day. Their inputs included past values of (depending on the context) RES generation, loads or prices from the last few days, day-ahead forecasts of exogenous variables (e.g., temperatures for load, load for prices) and a seasonal component captured by sinusoidal functions or weekday dummies \citep{hon:14_DSRW,wer:14_DSRW}. Their sub-optimal  performance could be readily improved by combining forecasts across different models \citep{bor:bun:lis:nan:13_DSRW}, calibration sets \citep{now:liu:wer:hon:16_DSRW} or calibration windows \citep{hub:mar:wer:19_DSRW}. Interestingly, combining is not only a remedy for time-varying point forecasting performance. Together with quantile regression it provides a simple, yet powerful tool for probabilistic predictions -- \textit{Quantile Regression Averaging} \cite[QRA;][]{now:wer:18_DSRW}. During the Global Energy Forecasting Competition 2014, teams using variants of QRA were ranked 1\textsuperscript{st} and 2\textsuperscript{nd} in the price track \citep{gai:gou:ned:16_DSRW,mac:now:16_DSRW}. QRA can be also used to construct dynamic strategies aiming at finding the optimal trade-off between risk and return when trading the intraday and day-ahead markets \citep{jan:woj:22_DSRW}.

With the advent of easily accessible computational power, the models became more complex and expert knowledge was no longer enough to handle them. A major breakthrough came with the introduction of regularisation methods to energy forecasting in the 2010s. Although regularisation is a much older concept, its use in load \citep{cha:hor:hwa:lee:16_DSRW,zie:liu:16_DSRW}, price \citep{uni:now:wer:16_DSRW,zie:16_DSRW,zie:wer:18_DSRW}, wind \citep{mes:pin:19_DSRW} and solar forecasting \citep{yan:18_DSRW} began only recently. Ridge regression has not seen many applications in energy forecasting, however, the \textit{least absolute shrinkage and selection operator} (LASSO) and elastic nets \citep{has:tib:wai:15_DSRW} have been shown to yield extremely competitive predictive models. \textit{LASSO-estimated autoregressive} (LEAR) models often have hundreds of inputs, e.g., spanning all hours of the past week, but LASSO can shrink redundant coefficients to zero and, thus, perform variable selection. Despite their ability to handle only linear relationships between variables, LEAR models tend to be only slightly inferior to the much more complex and much harder to estimate deep ANNs \citep{lag:mar:sch:wer:21_DSRW}. 

The availability of high-performance GPUs and advances in optimisation algorithms  made it possible to efficiently train ANNs with hundreds of inputs and outputs, multiple hidden layers and recurrent connections (\S\ref{sec:Artificial_intelligence_machine_learning_data_science}). This led to a wave of deep learning and hybrid energy forecasting models in the late 2010s \citep{gao:etal:19_DSRW,wan:etal:17_DSRW}. A prominent, yet relatively parsimonious example is the deep neural network (DNN) model proposed for price forecasting \citep{lag:rid:sch:18_DSRW}. It uses a feedforward architecture with two hidden layers, 24 outputs (one for each hour of the next day) and ca.\ 250 inputs: past prices from the previous week, day-ahead forecasts of fundamental variables (demand, RES generation) and weekday dummies. To decrease the computational burden, its hyperparameters (number of neurons per layer, activation functions, optimisation algorithm, etc.) and inputs (treated as binary hyperparameters -- either selected or discarded) are jointly optimised once every few weeks, while the weights are recalibrated every day to account for the most recent market data. Despite this simplification, daily recalibration of the DNN model is two orders of magnitude slower than of the LEAR model with the same inputs \cite[minutes vs.\ seconds on a quadcore i7 CPU; see][]{lag:mar:sch:wer:21_DSRW}.

The increased complexity of deep ANNs is a major obstacle in understanding the underlying processes. Partial remedy provide recently proposed architectures like the neural basis expansion analysis for interpretable time series forecasting \citep[NBEATS;][]{ore:dud:pel:tur:21_DSRW,oli:cha:mar:wer:dub:22_DSRW}, which project the time series onto basis functions in the fundamental blocks of the network structure. The final forecasts can be decomposed into interpretable components returned by groups of blocks (called stacks). Separate stacks can account for the trend, seasonality and exogenous variables. Another recent innovation in energy forecasting is a distributional ANN \citep{mas:etal:21_DSRW}. It only requires a small change in the architecture -- instead of the 24 hourly forecasts, the network can return the parameter sets of 24 probability distributions (e.g., the mean and standard deviation for the Gaussian). The benefits are clear. The downside, however, is that the distribution itself has to be estimated  (it is a hyperparameter). Somewhat surprisingly, distributional ANNs not only can yield more accurate probabilistic predictions, but also better point forecasts \citep{mar:nar:wer:zie:22_DSRW}.

For horizons beyond the next 48 hours other  approaches have been proposed \citep{wer:14_DSRW}, not necessarily forecasting per se. Structural models define the functional relationships between physical (weather, generation, consumption, etc.) and economic (bidding, trading) variables that set the price, then utilise -- typically -- parsimonious statistical or machine learning techniques to provide the stochastic inputs. Due to the nature of fundamental data, often of weekly or monthly granularity, such models are more suitable for medium-term risk management, portfolio optimisation and derivatives pricing \citep{kie:kus:16_DSRW}, than for short-term forecasting \citep{mah:gir:kar:22_DSRW}.
In the class of multi-agent approaches, \cite{ven:bai:ram:riv:05_DSRW} identify three trends:  equilibrium, simulation and optimisation models. The former (Nash-Cournot framework, supply function equilibrium, strategic production-cost models) have seen limited application in oligopoly markets \citep{rui:maz:08_DSRW}. Agent-based simulations are used when the problem is too complex to be addressed within an equilibrium framework \citep{fra:kra:kel:fic:21_DSRW}. 

Optimisation models address profit maximisation from the point of view of a firm competing in the market. One of the simplest settings is that of the price clearing process being exogenous to electricity generation optimisation -- as the price is fixed, the market revenue is a linear function of the production and linear programming (\S\ref{sec:Linear_programming}) or \textit{mixed integer linear programming} (MILP) can be employed \citep{ven:bai:ram:riv:05_DSRW}. On the other hand, \textit{Virtual Power Plant} (VPP) operations constitute a more complex problem of decision-making under uncertainty. A VPP is a cluster of dispersed generating units (e.g., intermittent rooftop solar panels on residential houses), flexible loads and battery storage that operates as a single entity. Robust optimisation and stochastic programming can be used to derive the optimal VPP trading strategy
\citep{mor:con:mad:pin:zug:14_DSRW}.

To support broader regulatory decisions at the firm or country level, frontier analysis methods are employed. Such methods aim to estimate the efficient frontier of the evaluated production units, to measure their relative efficiency (against the frontier) and to provide targets that can support policymakers. The benchmarking nature of these methods has established them as a flexible and multifaceted decision-making tool. In particular, \textit{Data Envelopment Analysis} (DEA, \S\ref{sec:Data_envelopment_analysis}) has been employed in a wide spectrum of energy applications. Early DEA studies relied only on a few factors (labour, fuel, capital, electricity production) to assess the technical efficiency of electric utilities \citep{Fare_1983_DSRW}. Later studies took into account sustainable practices by including environmental variables. Such factors are commonly treated as undesirable outputs that arise as by-products of the production process \citep{Fare_1996_DSRW} or as non-controllable variables, which reflect external factors that the unit under evaluation cannot control \citep{Hattori_2005_DSRW}. When price information is available, DEA allocation models can be used to evaluate revenue, cost and profit efficiency. \cite{Ederer_2015_DSRW} argued that sophisticated cost efficiency assessment methods should be employed to evaluate RES, and relied on DEA models to assess the capital and the operating cost efficiency of offshore wind farms. Notably, DEA is often combined with multi-criteria decision-making techniques to incorporate decision maker's preferences into the assessment \citep{Lee_2011_DSRW,Wang_a_2022_DSRW} and econometric techniques to study causal effects \citep{Shah_2022_DSRW}.

For a review and outlook into the future of energy (load, price, wind, solar) forecasting see
\cite{hon:pin:etal:20_DSRW}. \cite{hon:fan:16_DSRW} offer a tutorial review on probabilistic load forecasting. The standard reference for electricity price forecasting is \cite{wer:14_DSRW}. \cite{lag:mar:sch:wer:21_DSRW} offer a more recent viewpoint focusing on deep learning and hybrid models. They also provide a set of guidelines/best practices and make freely available the \textit{epftoolbox} with Python codes for two highly competitive benchmark models (LEAR, DNN). Two thorough treatments of probabilistic price forecasting are \cite{now:wer:18_DSRW} and \cite{zie:ste:18_DSRW}. \cite{swe:bes:bro:pin:20_DSRW} present a brief overview of the state-of-the-art in RES forecasting, whereas \cite{yan:etal:22_DSRW} jointly discuss atmospheric science and power system engineering in the context of solar forecasting. Finally, for detailed literature reviews on energy related applications of DEA see \cite{Mardani_2017_DSRW}, \cite{Sueyoshi_2017_DSRW} and \cite{Yu_2020_DSRW}.

\subsection[Project management (Willy~Herroelen \& Erik~Demeulemeester)]{Project management\protect\footnote{This subsection was written by Willy~Herroelen and Erik~Demeulemeester.}}
\label{sec:Project_management}

Operational Research methods play a fundamental role in managing a portfolio of projects, in project selection and in the management of each individual project. \textit{Project portfolio management} is concerned with the optimal mix and prioritisation of proposed projects in order to maximise the organisation’s overall goals \citep{Levine2007-bi_WH_ED}. At the strategic level, \textit{project selection} deals with the selection of and resource allocation among a group of projects \citep{Kavadias2012-mr_WH_ED}. Static models rely on mathematical programming, scoring and sorting, financial modelling, graphical and charting techniques. Dynamic models for selecting projects from a stream of arrivals may rely on queueing theory (\S\ref{sec:Queueing}), simulation (\S\ref{sec:Simulation}), decision theory (\S\ref{sec:Decision_analysis}) and stochastic dynamic programming (\S\ref{sec:Dynamic_programming}; \S\ref{sec:Stochastic_models}).

At the tactical and operational levels,  \textit{project management} \citep{Meredith2011-uu_WH_ED} basically involves the planning, scheduling and control of project activities to achieve performance, cost and time objectives for a given scope of work, while using resources efficiently and effectively. The scope of a project is the magnitude of the work that must be performed to make sure that the product or items to be provided (the project result or the project deliverables) meet the requirements or acceptance criteria agreed upon at the onset of a project. Once the project is properly defined in terms of its scope and objectives, the planning phase may start through the identification of the project activities, the estimation of time and resources, the identification of relationships and dependencies between the activities and the identification of the schedule constraints. The activities can be graphically portrayed in the form of a project network showing the necessary interdependencies of the activities. Based on the type and quantities of resources required, cost estimates can be made. \textit{Project scheduling} \citep{Demeulemeester2002-an_WH_ED} then involves the construction of a project base plan which specifies for each activity the precedence and resource feasible start and completion dates, the amounts of the various resource types that will be needed during each time period, and as a result the budget. Once a baseline schedule has been established, it must be implemented. This involves performing the work according to the plan and controlling the work by monitoring the progress and taking necessary corrective action when the project is on its way to run behind schedule, to overrun the budget, or to violate the original technical specifications.

\subsubsection*{Construction of the project network}
A project network is a graph consisting of a set of nodes and a set of arcs. In the activity-on-arc representation (AoA), the nodes represent the events and the arcs represent the activities. AoA networks form the basis of the Project Evaluation and Review Technique \citep[PERT;][]{Malcolm1959-yn_WH_ED} and the Critical Path Method \citep[CPM;][]{Kelley1961-ko_WH_ED}. The precedence relationship used is the finish-start relationship with a zero time lag: an activity can start as soon as all its predecessor activities have finished. In the mostly used activity-on-node representation (AoN) the nodes represent the activities and the arcs denote the precedence relations. The AoN representation allows for the specification of generalised precedence relations of four types: start-start, start-finish, finish-start and finish-finish with minimal and/or maximal time lags. A minimal time lag specifies that an activity can only start (finish) when its predecessor activity has already started (finished) for a certain time period, whereas a maximal time lag specifies that an activity should be started (finished) at the latest a number of time periods beyond the start (finish) of another activity.

\subsubsection*{Temporal analysis for deterministic unconstrained project scheduling}
In this case a single deterministic duration estimate is used for the activities. Basically, the temporal analysis then involves the computation of the activity start times under the objective of minimising the project duration. In the presence of strict finish-start precedence relations, this can be achieved by simple forward and backward pass calculations. Generalised precedence relations with maximal time lags call for the use of graph algorithms for computing the longest path (critical path) in networks. 

The temporal analysis may also be performed under the objective of maximising the net present value of the project. The deterministic max-npv problem can be formulated as a nonlinear problem. An efficient recursive solution procedure has been developed for AoN networks and has been extended to deal with the case of time-dependent cash flows \citep{Vanhoucke2001-kp_WH_ED}. 

Another non-regular performance measure is the minimisation of the weighted earliness-tardiness penalty costs of the project activities, where activities have an individual due date with associated unit earliness and unit tardiness penalty costs. The problem can be solved by an exact recursive search procedure \citep{Vanhoucke2001-nb_WH_ED}.

\subsubsection*{The deterministic resource-constrained project scheduling problem}
Project activities require resources for their execution. Renewable resources (e.g., manpower, machines) are available on a per-period basis. Their introduction into the analysis complicates matters considerably. Computing a precedence- and resource-feasible deterministic schedule that minimises the project duration, the resource-constrained project scheduling problem (RCPSP) is $\mathcal{NP}$-hard in the strong sense (\S\ref{sec:Computational_complexity}). Both exact and suboptimal procedures have been presented in the literature.

Many mathematical programming formulations (\S\ref{sec:Mixed_integer_programming}), either binary or mixed integer linear programs, have been developed \citep{Demassey2010-pt_WH_ED}. The RCPSP may also be solved through constraint-based scheduling \citep{Laborie2010-li_WH_ED}. Also a number of branch-and-bound algorithms have been presented for optimally solving the RCPSP \citep{Brucker1998-qg_WH_ED,Demeulemeester1992-xs_WH_ED}. 

Heuristic procedures broadly fall into two categories: constructive heuristics and improvement heuristics. Constructive heuristics start from an empty schedule and add activities one by one until a feasible schedule is obtained. Activities are ranked by priority rules which determine the order in which the activities are added to the schedule. Improvement heuristics start from a feasible schedule that was obtained using a constructive heuristic. Operations are then performed on a schedule which transforms a solution into an improved one. These operations are repeated until a locally optimal solution is obtained.

Project scheduling metaheuristics come in a wide variety and broadly include tabu search \citep{Baar1999-ni_WH_ED}, simulated annealing \citep{Bouleimen2003-ud_WH_ED}, genetic algorithms \citep{Hartmann2002-vg_WH_ED}, ant colony optimisation \citep{Merkle2002-xi_WH_ED}, scatter search and electromagnetic approaches \citep{Debels2006-di_WH_ED}. 

\subsubsection*{Resource problem variants and generalisations}
Branch-and-bound may be used for solving the RCPSP with generalised precedence relations \citep{Demeulemeester1997-bt_WH_ED,De_Reyck1998-sh_WH_ED}, when activities may be preempted \citep{Demeulemeester1996-yl_WH_ED}, when the problem has to be solved under the objective of maximising the net present value \citep{Vanhoucke2001-kp_WH_ED} or with the earliness-tardiness objective \citep{Vanhoucke2001-fb_WH_ED}.

The resource levelling problem aims at completing the project within its deadline with a resource usage that is leveled as much as possible over the entire project duration. Exact solution procedures based on integer or dynamic programming and branch-and-bound as well as heuristic procedures have been developed \citep{Neumann2000-yl_WH_ED}. The resource availability cost problem, that consists of scheduling the project activities such that the total cost of acquiring the necessary resources is minimised, assuming that a resource is assigned to the project for the total project duration, can be solved optimally \citep{Demeulemeester1995-gy_WH_ED}. The resource renting problem \citep{Nubel2001-ou_WH_ED} which assumes that resources can be added or removed from the resource pool over the project life, can be solved optimally using branch-and-bound or heuristically using genetic algorithms and scatter search \citep{Ballestin2007-xx_WH_ED,Kerkhove2017-et_WH_ED}. 

The multi-mode RCPSP assumes limited availability of renewable and nonrenewable (e.g., money) resource types and assumes that a project activity may be executed in multiple modes, where an activity mode corresponds to the assignment of a mode-specific number of units of a (non)renewable resource type to the activity with correspondingly resulting activity duration. The project decisions then involve the decisions to start and perform the activities in a specific mode in order to minimise the project duration. Branch-and-bound \citep{Hartmann1998-ny_WH_ED}, branch-and-cut, local search \citep{De_Reyck1999-oz_WH_ED} and genetic algorithms \citep{Hartmann2001-bu_WH_ED} are available. 

For projects with a flexible project structure, where activities to be performed are not known in advance, decisions for the implementation of optional activities can be made using genetic algorithms and tabu search \citep{Kellenbrink2015-av_WH_ED,Servranckx2019-zs_WH_ED,Servranckx2019-vw_WH_ED}.

\subsubsection*{Dealing with uncertainty}
Risk analysis involves the identification of the qualitative and quantitative assessment of the risk factors for the project through the estimation of the probability of the risk factors (activity duration, cost and resource requirement increases, start time delays) as well as their potential impact. The impact of each risk is best assessed individually and mapped to the duration of a project activity \citep{Creemers2014-lb_WH_ED}. Risk responses may then involve risk avoidance by performing an alternative approach without the risk, taking actions to reduce the risk, and risk impact reduction by switching to a different execution mode, adding additional resources, etc.

Stochastic scheduling does not generate a baseline schedule before the start of the project, but deals with time uncertainty by viewing the scheduling problem as a multi-stage decision process where scheduling policies are used to decide at each of the stages which occur serially through time at random decision points, which activities selected from the set of precedence and resource feasible activities have to be started under the objective of minimising the expected project duration \citep{Demeulemeester2011-eb_WH_ED}. 

Proactive project scheduling generates a robust baseline schedule through solving the RCPSP and subsequently tries to protect it as well as possible against time and resource disruptions that may occur during project execution. This protection can be achieved by deciding on a clever way to transfer the renewable resources between the activities \citep{Leus2004-kp_WH_ED}.  Both branch-and-bound and heuristics are available for the minimisation of the weighted sum of the difference between the planned and the realised activity start times \citep{Van_de_Vonder2008-hv_WH_ED,Lambrechts2008-fp_WH_ED}. Another way involves the insertion of time buffers that should prevent the propagation of distortions throughout the schedule. The critical chain methodology \citep{Goldratt1997-tq_WH_ED,Herroelen2001-vl_WH_ED,Newbold1998-xg_WH_ED} defines the critical chain as that set of tasks which determines the overall project duration. Protection is then realised through the insertion of feeding buffers and resource buffers in combination with a project buffer at the end of the critical chain. 

When during the actual execution of the project disruptions occur that cause deviations from the protected baseline schedule or even render this schedule infeasible, reactive scheduling procedures may be deployed.

For reviews and comprehensive textbooks on project management and scheduling we refer the reader to \cite{Demeulemeester2002-an_WH_ED},
\cite{Demeulemeester2011-eb_WH_ED},
\cite{Hartmann2010-dc_WH_ED},
\cite{Herroelen2007-hh_WH_ED},
\cite{Herroelen2005-me_WH_ED},
\cite{Meredith2011-uu_WH_ED},
\cite{Neumann2002-ve_WH_ED},
\cite{Shtub2004-ob_WH_ED}, and
\cite{Vanhoucke2018-vv_WH_ED}.

\subsection[Revenue management (Arne~K.~Strauss \& Jens~Frische)]{Revenue management\protect\footnote{This subsection was written by Arne~K.~Strauss and Jens~Frische.}}
\label{sec:Revenue_management}

The discipline of revenue management (RM) deals, in the widest sense, with demand management decisions to improve overall revenue or profit. Demand management decisions aim at influencing demand, such as pricing and availability control. Occasionally, demand management decisions can also take different forms like ranking lists (e.g., when showing customers of a meal delivery platform a rank order of restaurants that offer home delivery) or green van icons to denote which time slots for grocery home delivery are more environment-friendly (because there is a delivery planned to take place already anyway). RM is about IT-supported decision-making, mostly on the operational level, in contrast to strategic pricing theory encountered in the marketing domain. 
            
Such decision support systems, referred to as RM systems, have been first developed in the airline industry in the 1970s when deregulation introduced competition in the US airline market. They were so successful that the practice of RM soon spread to other industry domains, particularly to those that sell services or perishable goods (perishability creates pressure to sell within a limited selling horizon).  Examples include restaurant tables, hotel rooms, rental cars, or airline seats, among many other applications. In these industries, the supply is usually fairly inflexible, fixed costs are high and variable costs are relatively low (which also makes revenue maximisation mostly equivalent to profit maximisation, hence the name \emph{revenue} management).

In this section, we first outline recent research trends on demand models (and their estimation) that are required to provide an input to the RM optimisation system. Then, we present recent research on efficient optimisation of demand management decisions. Finally, we outline further reading suggestions including some current popular application areas. We mostly use the passenger airline industry throughout this subsection to illustrate developments in the field of RM.
        
\subsubsection*{Modelling demand}
In order to make good demand management decisions, we first need to  have a model of demand to describe the response to specific RM actions (such as pricing or changing the availability of products or services). The first demand models used in RM assumed that demand for a given product is independent of what else is offered. These so-called `independent demand models' are relatively easy to estimate. However, this independence assumption usually only holds in applications where rate fences (such as advance purchase requirements) strongly limit customers' abilities to substitute a product with one another. 

One way to relax the -- often unrealistic -- independent demand assumption is by considering that the customer looks at all alternatives available and chooses one. The requires modelling of customers’ choice behaviour; the seminal paper by \cite{talluriRevenueManagementGeneral2004_AKSJF} introduced discrete choice modelling to the domain of revenue management. In choice-based RM, demand for a product is assumed to depend on the available purchase alternatives and their attributes. These models tend to be more accurate in predicting demand if the independent demand assumption is not met, at the cost of being more difficult to estimate and implement \citep{kleinReviewRevenueManagement2020_AKSJF}. Much research has been carried out on choice-based RM since 2005; for a recent review, see \cite{straussReviewChoicebasedRevenue2018_AKSJF}.
            
Among the most recent trends -- building on the aforementioned choice-based RM literature with fixed choice model parameters -- is a stream of work on personalisation and choice model parameter learning. For example, \cite{cheungThompsonSamplingOnline2017_AKSJF} solve an online personalised assortment optimisation problem formulated as a multi-armed bandit problem. Demand learning models balance the trade-off between gathering new samples (and thereby learning more about the true customer behaviour) and applying the RM decision that, based on the current belief of customer behaviour, looks to be the best. In the short term, this means that we occasionally make decisions that seem not very promising, yet that will gain us insights into customer behaviour (for instance, by offering price points that were never offered before). For our airline example, a potential application is the ongoing learning of model parameters governing the choice of ancillary products (like seat upgrades, extra luggage, etc.). Models like the one by \cite{agrawalMNLbanditDynamicLearning2019_AKSJF} can be used for this purpose.

Demand models in RM may be biased if they are estimated on constrained data, meaning that the sales data does not necessarily reflect the actual demand. For example, if a flight is fully booked, we observe no further sales transactions for that flight. Yet demand may well exceed flight capacity and, as such, should be estimated somehow. Methods to statistically unconstrained demand data are reviewed in \cite{guoUnconstrainingMethodsRevenue2012_AKSJF}. 

Another dimension of demand modelling is represented by strategic versus myopic customer behaviour. One of the earliest papers on this topic is the work by \cite{suIntertemporalPricingStrategic2007_AKSJF}. He considers customers who may delay their purchase when they expect lower-priced offers in the future. With RM mostly focusing on myopic customers (meaning customers who do not anticipate future developments in their purchase decision), the behaviour of strategic customers leads to inefficiency. \cite{suIntertemporalPricingStrategic2007_AKSJF} proposes an intertemporal pricing component to adjust the offering based on the market composition between these customer types, and more work has built on this since.
            
\subsubsection*{Optimisation advances}
A central element of an RM system is the decision of what to offer whenever a customer arrives. Decision policies (essentially a mapping from the state space of available information to the action space) are used to determine which products are made available (i.e., availability control), or at which price (i.e., dynamic pricing) -- and sometimes, a combination of both. Using dynamic prices to manage demand can be very similar to availability control: when there are products defined with identical features but different price tags such that there is a discrete set of prices to choose from for a product, it can be considered a special instance of the aforementioned availability control \citep{straussReviewChoicebasedRevenue2018_AKSJF}. This can be observed in airline's implementations of differently priced booking classes for the same seat such that a customer can only purchase that seat for the fare of the booking class made available to them. The groundbreaking papers by \cite{gallegoOptimalDynamicPricing1994_AKSJF,gallegoMultiproductDynamicPricing1997_AKSJF} also featured a dynamic pricing concept for a single-commodity and a network-level problem, respectively. Both papers also considered the effect of significant cancellations and no-shows (meaning bookings that are not actually being used in the end). In that context, it can be valuable to accept more reservations than physical capacity would allow. This practice is called overbooking and is widespread in many industries where the risk of having to reject a customer with a valid reservation is not overly costly \citep[with examples such as][showing it being applied already before RM but now usually integrated into systems to manage demand for available capacity]{simonAlmostPracticalSolution1968_AKSJF}. For an overview of recent contributions on this matter, see \cite{kleinReviewRevenueManagement2020_AKSJF}.
            
Decision policies in RM are trading off the immediate reward of having a customer buy a product versus the so-called opportunity cost associated with this purchase, stemming from having to commit some resources to a given sale. For example, a resource might be a flight with a specific seat capacity. Selling a ticket for a seat on this flight requires us to commit a seat to this customer, which otherwise might have still gotten sold in the future at a potentially larger fare. Therefore, by having a customer buy the product, we incur the cost of losing the opportunity to sell the associated resource units in the future. This value (or at least an approximation thereof) is sometimes used as a revenue threshold defining which products shall be shown as available; such special versions of availability control are known as bid price policies, with the bid price being this threshold, and only products with revenues that exceed the bid price being shown.
            
There are two major challenges in deriving optimal decision policies: first, we need to somehow obtain the opportunity costs involved with having a customer book a particular product at a given point in time; second, we need to solve the resulting optimisation problem to give us the actual decision to be implemented. 
            
The latter decision problem, given opportunity cost, may be as simple as a comparison of two numbers (traditionally used in independent-demand settings), but can be non-trivial in the presence of sophisticated models of customer behaviour (dependent demand settings). Much research over the past few years has been devoted to studying properties of choice models that can be exploited to efficiently solve -- or at least closely approximate -- the online RM decision problem. This work is further motivated by the need to solve these RM decision problems quickly to ensure an acceptable user experience. Within availability control, assortment optimisation under various choice models has received particularly much attention because this problem becomes $\mathcal{NP}$-hard for many customer choice models unless a certain structure can be exploited; for a review, see  \cite{straussReviewChoicebasedRevenue2018_AKSJF}. 
            
The other challenge in deriving optimal decision policies is the computation of opportunity costs. This is usually the more difficult task for real applications because the opportunity costs depend on time, the current state (of inventories), and future demand and actions. Dynamic programming (DP; \S\ref{sec:Dynamic_programming}) is usually being applied to solve or at least characterise the optimal decision policy over a given booking horizon. 

However, obtaining opportunity costs using DP is often only possible when dealing with problems that have a single resource \citep[like optimising for a single flight only, for example in][]{wollmerAirlineSeatManagement1992_AKSJF}. When there are products that use more than one resource (like a itinerary of multiple flights connecting in a hub), then we speak of network RM problems. These require much more effort to solve (so as to get opportunity cost estimates for our decision policy) due to the fact that decisions on one product may affect many others that are using the same resource. Therefore, to reach at least an approximate solution for a network RM problem, one usually needs to resort to deterministic linear programming \citep[][\S\ref{sec:Linear_programming}]{liuChoicebasedLinearProgramming2008_AKSJF} or approximate dynamic programming (\citeauthor{gallegoRevenueManagementPricing2019_AKSJF}, \citeyear{gallegoRevenueManagementPricing2019_AKSJF}, describe how approximate dynamic programming can be used in RM). In practical applications, the network-level optimisation problem is often decomposed into a collection of single-resource problems such as in \cite{kemmerDynamicSimultaneousFare2012_AKSJF} who were motivated by methods deployed by Lufthansa Systems in their RM optimisation module. In older RM systems, booking control was typically implemented using versions of the so-called Expected Marginal Seat Revenue (EMSR) heuristics \citep{belobabaAirTravelDemand1987_AKSJF}, which are in turn rooted in the work of \cite{littlewoodSpecialIssuePapers2005_AKSJF}, originally written in 1972.
            
Once a decision policy has been obtained (by first obtaining opportunity costs and then solving the corresponding decision problem), we then need to evaluate the decision policy using simulation or even in real-world trials. \cite{bertsimas2005simulation_VLVV} give an overview of different decision rules for airline RM that are evaluated with a simulation study. Further details on simulation techniques can be found in \S\ref{sec:Simulation}. An example of testing a dynamic pricing policy in live trials is the work by \cite{fisherCompetitionbasedDynamicPricing2018_AKSJF}.
        
\subsubsection*{Further reading}
RM is also applied in retailing, both for e-commerce and offline shopping. \cite{agatz2013revenue_CKTVW} provide a practical overview of ways online retailers implement RM in their business. But there are also retail RM applications outside of online shopping. For example,  \cite{caroClearancePricingOptimization2012_AKSJF} describe how brick-and-mortar fashion stores optimise their price markdowns during season clearing sales.
                     
In particular, linking RM to general transportation problems has received significant attention over recent years. An overview of advances in that field is given by \cite{fleckensteinRecentAdvancesIntegrating2022_AKSJF}. Applications thereof can be seen here especially in business models with delivery constraints, such as same-day deliveries \citep{ulmerDynamicPricingRouting2020_AKSJF} or for attended home delivery (AHD) which is common for online grocery shopping. Customers' desire for short and guaranteed time windows in AHD leads to less than optimal routings. \cite{yang2016choice_CKTVW} show how RM can be used to steer demand for delivery time slots towards a routing solution closer to the optimum, thereby increasing overall profit for businesses shipping goods that require AHD. Another example of applying RM ideas to new transportation problems is the work by \cite{kunnenValueFlexibleFlighttoroute2022_AKSJF}. They analyse how an air traffic network manager could reduce overall delays for all airspace users (i.e., airlines) by offering dynamically priced flight trajectories.
            
For more detailed readings about the development of the RM domain, the techniques being used, and more applications, we refer the reader to the books by \cite{talluriTheoryPracticeRevenue2004a_AKSJF} and \cite{gallegoRevenueManagementPricing2019_AKSJF}.

\subsection[Service industries (Jan~Holmström \& Lauri~Saarinen)]{Service industries\protect\footnote{This subsection was written by Jan~Holmström and Lauri~Saarinen.}}
\label{sec:Service_industries}

\textit{Service industry from the perspective of operations}: Service industry is a concept from economics originally defined by what it is not. It is not a manufacturing industry that produces tangible goods (cars, clothes, equipment), but industry that provides intangible outputs, such as hospitality, healthcare, and education.  In service research, services are also defined by additional characteristics. In addition to intangibility, the so-called IHIP characteristics, recognises heterogeneity, inseparability, and perishability as the defining characteristics of service industries.

In operations and Operational Research, service industry is approached not from its characteristics but operationally to support actionable insights \citep{burger_developing_2019_JHLS}. For Operational Research applications, service industry can be approached through the FTU-framework, defining services as a particular type of transformation (Figure \ref{fig:FTU_JHLS}). Service industries are distinguished from goods industries through the direct provision and integrative decision making. In services the decision making of customers and providing companies is intertwined, while in goods industries customer and providers make autonomous decisions. In service industries the value is directly provided to the customer, while in goods industries indirectly through the product.

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=13.5cm]{figures/Figure_1_service_indurstries_JHLS.png} 
    \caption{An actionable framework for service industries: Facilities-Transformation-Usage \citep[adapted from ][]{moeller_characteristics_2010_JHLS}}
\label{fig:FTU_JHLS}
\end{center} 
\end{figure}

However, operational reality is not this clear-cut. In manufacturing industries, through servitisation \citep{kohtamaki_practices_2018_JHLS}, some companies seek to make their products more like services to differentiate their offering, and directly create value to their customers. In service industries, managers seek to make services more like goods, to be able to run service facilities more like factories and improve productivity \citep{levitt_production-line_1972_JHLS, schmenner_service_2004_JHLS}. OR methods that were initially developed in manufacturing industries (e.g., forecasting, queuing, scheduling, simulation), are increasingly applied in service industries to make service facilities operate more like factories \citep[cf. ][]{eveborn_operations_2009_JHLS}. For servitisation, OR presents a more limited range of methods. Methods supporting the servitisation of products are for example, value constellation modelling 
\citep{holmstrom_comparing_2010_JHLS, brax_meta-model_2017_JHLS}, and ecosystem modelling to support innovation and new business model design in an open environment \citep{talmar_mapping_2020_JHLS}.

The challenge in service industries is that service systems tend to be open, problems wicked, and optimising solutions difficult to develop and apply. Value provision often requires interaction with customers (customisation) limiting the situations where facilities can be organised for flow and efficiency as service factories. Also, servitisation of products occurs in an open systems environment, requiring responsiveness to influences and disturbances from the outside, as will be seen for our application examples from industrial services and home healthcare.

\textit{Service industry applications}: In the following we will present two examples on the use of OR methods for creative insight and novel solutions in service provision. The examples are homecare of elderly patients \citep{groop_improving_2017_JHLS}, and line maintenance of commercial aircraft \citep{ohman_frontlog_2021_JHLS}. In the first example systems thinking, in the form of soft OM methods from Theory of Constraints \citep{davies_theory_2005_JHLS}, is used in combination with design science research (implementation and evaluation). In the second example design and simulation are used in combination, uncovering an unexpected new way of simultaneously improving resilience and reducing costs in a commercial airline.

\textit{Homecare of elderly patients} \citep{groop_improving_2017_JHLS}: Nurses, team leaders, and healthcare management had distinct and diverging views on what is the problem with the homecare operations. Strongly held and divergent views are an indication of a possibly wicked problem \citep{Sydelko2021-qp_GM}. The divergent views in the case were uncovered through engagement (following actors in their work) and interviewing, with the purpose of articulating what different stakeholders identified as undesirable effects (UDE) of the current solution.  UDE is thinking tools terminology from Theory of Constraints \citep{dettmer_goldratts_1997_JHLS}. These UDEs of the current operation were pruned for overlaps and narrowed down to a list of seven (including the seeming contradiction between low utilisation of full-time employed nurses, stressed-out nurses, and chronic under-staffing requiring frequent use of temporary staff). Using effect-cause logic, the interconnections between effects, and mechanisms behind the effects were specified and then evaluated by all stakeholder groups in joint workshops. In this case, the first effect-cause analysis pointed towards a core problem, a contradiction, which when addressed, would improve efficiency. The needed change was in the way the nurse visits are scheduled to improve effectiveness. Instead of scheduling nearby patients after each other to save travelling time, the home care organisation should focus on only scheduling nurses for time-critical visits (visits that must be performed at a specific time) at the peak-demand in the morning. This way the time of full-time nurses will be more effectively used.

However, when implemented, the scheduling change had next to no effect. With the initial solution a failure, the evaluation of the implemented change pointed to issues with the problem framing. Going back, considering the stated problems (UDEs) and initial solution, the field researchers found that they had missed an important undesirable effect originating from the way the organisation operated. In the mapping nobody had raised as a problem that full-time nurses, when not busy, do not help-out in other teams. When nurses stay within their teams, work is evenly divided between everybody in the team, which is not a problem for nurses, nor for team leaders. Instead, when there is need for more nurses, outside temporary nurses are called in, and they move between teams if needed, but not the full-time employed nurses.

Management, for whom the low utilisation of full-time employed nurses is a cost issue (with payment of salaries both for idling full-time nurses and busy temporary nurses), were not aware of how nurses staying with their team was a mechanism behind the low utilisation. Nor had the researchers working in the field realised this before failing with the first solution design. Re-framing the problem once more, another solution changing the scheduling for employed nurses was proposed. Instead of dividing work equally between all nurses in their teams, the team leaders should seek to schedule work so that one, or even two nurses in their teams have no work, and can be made available to help-out in other teams. 

\textit{Line-maintenance of commercial aircraft} \citep{ohman_frontlog_2021_JHLS}:  The second example illustrates the use of simulation in problem reframing and finding a new type of solution. The service operations are line-maintenance of aircraft in an airline. Initially the problem was framed by management as improving departure reliability without increasing the number of maintenance technicians. The intended solution was introducing lean in the turn-around of aircraft. 

However, in line maintenance there are no material and time buffers for which lean approaches have been so impactful in manufacturing. The minimum frequency and content of maintenance tasks are regulated. Departures are delayed by technical problems that add unplanned tasks, which need to be carried out. Here, lean principles can increase productivity but not reduce the unplanned tasks. To reduce the delays caused by unplanned tasks a resource buffer of maintenance technicians appears necessary.

In this example, the same method of engagement was applied as in the homecare example. Observing and interviewing different actors involved, field researchers sought to articulate a set of undesirable effects of the current way of operating. However, no agreement on a core problem to address could here be reached. Instead, problem framing ended with a question and a puzzling response that pointed in a new direction. Line-maintenance scheduling assumed that maximising the interval for planned tasks is optimal, also when there are unplanned tasks and constrained resources. Engaging and interviewing maintenance planners for both long-haul and short-haul fleets and operations, production, and resource planning, field researchers began to gain an in-depth understanding of the airline maintenance planning function. Heuristics and principles related to dealing with over-maintenance not visible in the operational documentation were encountered.

To explore the implications, the researchers first modelled the relationship between over-maintenance and planned workload variance in a deterministic setting, focusing solely on scheduled maintenance. The model indicated a promising relationship: an increase of one percent in the total planned workload (over-maintenance) could result in up to a six percent reduction in workload variance. Next, simulation of the airline operation and maintenance included the unplanned events according to their historical distribution. The simulation surprisingly indicated that increasing over-maintenance could reduce over-all costs and improve departure reliability, if combined with a re-scheduling solution for maintenance task. Re-scheduling introduces a new type of time buffer, a frontlog of planned maintenance tasks that can be postponed to allow technicians to address unplanned tasks without disruptions to departure schedule.

\textit{Summary and conclusion}: In service industry applications problem framing methods are particularly important. The openness of service operations and wicked problems often require the Operational Researcher in service industry applications to go outside their comfort zone regarding methods \citep{mingers_soft_2011_JHLS, mingers_helping_2015_JHLS} to search for actionable insight \citep{burger_developing_2019_JHLS}. In the examples provided, a combination of approaches, tools, and methods were contingently employed in the search for a good problem framing as the basis for an effective solution design. For the application of OR methods in service industries, the homecare example illustrates the use of a soft OR method in framing the problem \citep[from the practice of Theory of Constraints, ][]{davies_theory_2005_JHLS}, the use of scheduling from hard OR as a solution component, and implementation as a method of design science for evaluation and redesign \citep{holmstrom_bridging_2009_JHLS, sein_action_2011_JHLS}. The second example illustrates the use of simulation as a method of explorative design. In the empirical grounding of the simulation model we encountered the good problem, which is the key to success in simulation projects \citep{law_how_2003_JHLS}. Through simulation we developed and explored the effect of the dynamic re-scheduling and buffer management approach, with surprising outcomes. Before the simulation study nobody knew about the opportunity to both improve departure reliability and reduce costs.

The example multi-method approach combined soft OR, simulation, and systems thinking for framing the problem. As in cross-agency problem solving in government and public administration, service industry problem solving benefits from mapping different actor perspectives, as the purposes, perspectives and values of the service supply chain actors can easily be in conflict \citep{Sydelko2021-qp_GM}. However, in addition to methods for actionable insight, methods for turning insights into solution proposals are also needed. For proposing and developing a solution design, the two examples relied on explorative design science \citep{holmstrom_bridging_2009_JHLS}, relying on OR methods in evaluation when implementation is possible, and simulation for substituting implementation. In the search of effective solution designs, OR methods such as scheduling, and forecasting were applied as potential solution components in both examples. 

\subsection[Sports (Ian~G.~McHale)]{Sports\protect\footnote{This subsection was written by Ian~G.~McHale.}}
\label{sec:Sports}

Moneyball \citep{Lewis2003-hl_IM} told the story of how the Oakland Athletics Major League Baseball team was able to leverage an inefficiency in the labour market for baseball players, and perform above expectations (given the team’s salary spend). Its impact on how quantitative analysis is viewed within sport and wider society is unprecedented. We have moved from an age when society tended to undervalue quantitative skills to a post-Moneyball era where analytics is generally accepted as being ``cool''. Told in both a best-selling book \citep{Lewis2003-hl_IM}, and a Hollywood movie of the same name, Moneyball has driven a rapid expansion of interest in the field of sports analytics. For an analysis of Moneyball, see, for example, \cite{Hakes2006-tt_IM}.

The history of quantitative analysis in sports dates back to centuries before the Moneyball story, and to the conception of probability itself. The concepts of chance are as old as the first dice games, but they did not evolve into the mathematical principles of probability until the 17\textsuperscript{th} century when Pascal and Fermat exchanged ideas in a series of letters during 1654. The letters were written in response to the following problem: \textit{two players, A and B, each stake 32 pistoles on a first-to-three-point game. When A has 2 points and B has 1 point, the game is interrupted and cannot continue. How should the stakes of 64 pistoles be fairly distributed?} Fast forward three centuries and the similarities of this problem with the problem encountered in limited-overs cricket, when a match is cut short because of rain, are uncanny. Indeed, the solution offered by \cite{Duckworth1998-cu_IM} is one of the great success stories of OR in sport, or arguably OR in any field. That sports fans routinely use the names of a statistician, and an operational researcher should be the source of great pride to the OR community.

The field of sports analytics now boasts specialist journals, regular special issues in top-rated mainstream journals, large departments in sports teams, and many stories of success and over-achievement in professional sport.

\subsubsection*{What is `sports analytics'?}
Analytics is largely an umbrella term for data science, statistics, operational research, and nowadays, machine learning. A simple definition of sports analytics is \textit{the use of analytics to gain a competitive edge in sport}. A wider definition would be \textit{the use of analytics to improve decision making in sport}.

Research has been published on the use of analytics in almost all popular sports including: football, tennis, cricket, golf, American football, baseball, motor sport, martial arts, and many more. Rather than review the field by sport, it is more logical to consider the field by task. The following is not a comprehensive list of such tasks, but provides an overview of the more common objectives of sports analytics.

\subsubsection*{Ranking and Rating}
Ranking of competitors is, to a large extent, the entire purpose of organised sport, and rating is a popular area of research. There are several families of models used to rate competitors. Paired comparisons models are used when two competitors play in each contest. For example, Elo ratings were first developed for use in chess, but have since been used by, for example, \cite{Hvattum2010-nn_IM} for forecasting football results. \cite{Glickman2001-vc_IM} presented a more general Elo model based on a Bayesian updating system and applied it to the problem of dynamic ratings of chess players. Another paired comparisons model is the Bradley-Terry model and this was used in \cite{McHale2011-qs_IM} to forecast the results of tennis matches. Multiple comparisons models are used when several competitors play in each contest and \cite{Baker2015-cv_IM} use a time-varying multiple comparisons model to rate golfers from different eras. \cite{Langville2013-cf_IM} provide an excellent overview of rankings models.

Rating individuals in team sports is a somewhat more complex task than the examples given above, especially when the individuals have different objectives, as is the case in football for example, where some players are mainly responsible for defending, whilst others are mainly responsible for attacking. Basketball, ice-hockey and football all fall into this category. In such circumstances plus-minus ratings are useful. At its most basic level, a player’s plus-minus rating is a comparison of a team’s performances with and without the player. \cite{Rosenbaum2004-ut_IM} presented a method for calculating plus-minus player ratings in basketball, before extensions were added by \cite{Macdonald2012-xc_IM} and \cite{Kharrat2020-gg_IM} to account for the intricacies of ice-hockey and football, respectively.

The availability of more granular data, such as event data (detailing each and every event in a game, e.g., the timing, coordinates and players involved in a pass) and player tracking data (the coordinates of all players on the field of play recorded at several times per second), has enabled more advanced measures of player performance to be calculated. One such measure is that of \textit{expected value of possession} (EVP) for valuing individual actions in team sports. The concept of EVP was first presented in \cite{Cervone2016-gk_IM} and asks the question “what is the probability of the objective happening before an action, compared to the probability after an action?”. The objective may be to score a goal in football. If an action is a good one – the probability of a goal should increase, whilst if it is a bad one, the probability of a goal will likely decrease. The change in the probability of the objective occurring is then the value of the action. Recent applications of deep reinforcement learning have seen EVP calculated for football \citep[e.g.,][]{Liu2020-ps_IM,Fernandez2021-do_IM}. Indeed, it is likely that the EVP concept will be used in many sports in the future.

\cite{Akhtar2015-lj_IM} uses the change in probability of a team winning a Test match to rate cricketers. The idea is similar to the EVP idea proposed by \cite{Cervone2016-gk_IM}, but uses multinomial regression to calculate probabilities of a team winning/drawing/losing the match, before and after each ball of the match.

The idea of monitoring the change in an expected value is also used in golf’s ‘strokes gained’ metric \citep{Broadie2012-eo_IM}. Strokes gained measures how good an individual shot is, and by aggregating over many shots, one can identify how good a player is overall, or how good certain areas (e.g., putting and driving) of a player’s game are.

\subsubsection*{Decision making}
A core tenet of sports analytics is that it should drive improvement, indeed improving decision making is central to the OR paradigm. There are many papers looking to use analytics to improve decision making in a sports context.

Perhaps the costliest consequences of decision making in sport concern the recruitment of athletes. Indeed, the Moneyball premise is built on the idea of avoiding overpaying for talent.

Football clubs exchange huge sums of money to acquire the services of players. These transfer fees were studied in \cite{Coates2022-sm_IM} who consider the issue of the wisdom of the crowd in estimating the fees. \cite{McHale2022-vx_IM} use machine learning techniques to model transfer fees as a function of performance metrics and contract status, amongst other things.

In lucrative team sports such as American football, football and basketball, recruitment of young talent with high potential is of potentially great value, but it appears a relatively little researched area. In one of only a handful of papers on this issue, \cite{Craig2021-fc_IM} present a model to predict the potential of college quarterbacks to one day play in the NFL. 

In addition to making good decisions around player recruitment, sports teams must make good decisions about their coaches. \cite{Peeters2020-yb_IM} consider the impact of coaches on the performance of Major League Baseball teams, whilst \cite{Muehlheusser2018-no_IM} rate coaches in German football. Identifying good coaches is just one dimension of decision-making surrounding running a sports team, and it is often the case that team owners are faced with the decision of whether or not to fire a coach. The impact of managerial dismissals has been the focus of attention in the economics literature. In football, \cite{De_Dios_Tena2007-dp_IM} measure the consequences of mid-season managerial dismissals on a team’s performance and find that there is a short-term improvement in results, but only in home matches.

The final area of decision making we note is that of team selection. In cricket the ordering of the batting line-up was considered in \cite{Perera2016-iw_IM}, whilst \cite{Watson2021-od_IM} use machine learning to optimise team selection in rugby union. \cite{Cao2022-dj_IM} look at optimising team selection in soccer.

\subsubsection*{Other areas of sports analytics}
Sport has attracted the attention of quantitative analysis in numerous other areas, though some do not have the objective of improving performance and/or decision making. For example, OR has been used to inform scheduling of tournaments (see also \S\ref{sec:Timetabling}). 

The popularity of sports betting means forecasting results has received a great deal of attention in the literature. As the sport with the largest global betting market, football has attracted the most attention in the forecasting literature. A notable contribution was that of \cite{Dixon1997-cf_IM}, whose Poisson model has been used as the basis of subsequent work for over two decades. More recently, machine learning techniques have begun to outperform Poisson-type models. See \cite{Dubitzky2019-kc_IM} for details of the results of the ‘Soccer Prediction Challenge’.

Tournament design has been the subject of research in, for example, \cite{Scarf2009-lc_IM}. The idea is that tournaments should maintain excitement. On a similar theme, \cite{Friesl2017-ji_IM} and \cite{Scarf2019-mk_IM} looked at the rules of ice-hockey and rugby and considered how they might be adjusted to increase excitement. By lowering scoring rates, the outcome of a game is more uncertain, and according to the uncertainty of outcome hypothesis this is what drives interest. However, there is conflicting evidence on the uncertainty of outcome hypothesis \citep[see, for example,][]{Forrest2002-rg_IM}. Understanding what drives the interest of fans was the subject of \cite{Buraimo2020-ur_IM} who looked at how suspense, surprise and shock during a match drives in-match television viewing figures.

To find more articles on sports analytics, the interested reader has several options including specialist journals (the \textit{Journal of Quantitative Analysis in Sports}, the \textit{Journal of Sports Economics}, and the \textit{Journal of Sports Analytics}), and discipline journals such as the \textit{European Journal of Operational Research}, the \textit{Journals of the Royal Statistical Society}, the \textit{Journal of the Operational Research Society}, and the \textit{International Journal of Forecasting}, together with a plethora of blogs and websites all focused on sports analytics.

\subsection[Supply chain management (Stephen~M.~Disney)]{Supply chain management\protect\footnote{This subsection was written by Stephen~M.~Disney.}}
\label{sec:Supply_chain_management}

The field of supply chain management (SCM) is concerned with the information, material, and cash flows within and between supply chain members. Materials generally flow down a supply chain (like water in a river); information and money flow up the supply chain. The way we design, source, produce, move, store, schedule, communicate, collaborate, and compete are important factors in SCM.

\subsubsection*{Lean production} 
SCM is built on the foundations of good \emph{industrial engineering}. The pioneering industrial engineers Frank and Lilian Gilbreth provided us with \emph{time and motion studies} \citep{Gilbreth1911_SMD}, \emph{human factors}, and \emph{scientific management}. During the 1920s scientific management techniques were imported into Japan's Imperial Navy's shipyards and factories to improve efficiency and quality. Initially, some table top management games learnt from the Gilbreths in the United States were taken to the Kure Navel Arsenal (a navel shipyard) in 1923 \citep{Robinson1994_SMD}. The table top management games were used demonstrate the efficient flow, and organisation, of work. \cite{Robinson1994_SMD} claims these table top games facilitated Japan in general, and Toyota in particular, to become highly efficient at producing high quality, low cost, reliable products. The \emph{Toyota Production System} (TPS) became the world standard in the highly efficient \emph{lean production} technique \citep{Ohno1988_SMD}. Western companies soon sought to emulate the success of TPS \citep{Womack1990_SMD}, hunting high and low for the \emph{seven lean wastes} \citep{Hines1997_SMD}. \cite{Holweg2007_SMD} provides an excellent summary of the genealogy of lean production.

\subsubsection*{Value stream mapping} 
One of the best ways to document and understand a supply chain is to draw a \emph{value stream map} \citep[VSM;][]{Rother1999_SMD}. VSMs detail how the material flow is controlled by the information flow and decision-making activities.  Key is to determine the point in the material flow where the customer order directly regulates the production cadence. This point is known as the \emph{pacemaker process} or the \emph{customer order decoupling point} \citep[CODP;][]{Olhager2010_SMD}. The pacemaker is often the process that separates the work that is pulled through the system by a \emph{Kanban} system, and the work that flows out to the customer in a \emph{first-in-first-out} (FIFO) queue. 

\subsubsection*{Agile and leagile supply chains}
Lean supply chains are characterised by \emph{just-in-time} inventories and \emph{high capacity utilisation}. But not all supply chains should be lean. Some supply chains need to be responsive, with extra inventory and spare capacity held in reserve so the system can quickly respond to unexpected demand \citep{Fisher1997_SMD}. This has become known as \emph{agile production}. The lean and agile paradigms can integrated in together in a concept known as \emph{leagility} \citep{Naylor1999_SMD}. In leagile supply chains, the material flow is set up to follow lean principles upstream from the CODP; downstream from the CODP, agile principles are followed.

\subsubsection*{Bullwhip and supply chain dynamics}
The \emph{bullwhip effect} is one of the biggest areas of SCM research. The moniker, coined by \cite{Lee1997_XW}, refers to the tendency of the slowly changing consumer demand (the bullwhip handle) to create wildly fluctuating fast moving demand at the raw material processors (the bullwhip popper). This \emph{variance amplification} effect is caused by the decision-making activities  \citep{Forrester1958_SMD}. The seminal paper by \cite{Lee1997_XW} highlights four causes of the bullwhip effect: \emph{demand signal processing}, \emph{order batching}, \emph{shortage gaming}, and \emph{price fluctuations}.

Demand signal processing has been the most studied cause of the bullwhip effect. Demand signal processing refers to the activity of \emph{forecasting} the demand over the lead time (and review period), so that one may determine production and/or replenishment order quantities to maintain \emph{finished goods inventory} and \emph{raw material} levels close to a target. Setting target inventory levels is a problem similar to the \emph{newsvendor problem} \citep{Churchman1957-ax_MYLW}. As orders eventually turn into the inventory here is a \emph{feedback loop} in the decision; there is also a \emph{work-in-progress} feedback loop in the system \citep{Sterman2000_SMD}.  Both these feedback loops contain delays.  This creates a complex system whose dynamics are in part driven by the external demand, but are mostly an internally generated effect caused by the fundamental structure of the supply chain \citep{Sterman2000_SMD}.

Control engineers have developed a large toolkit to understand and manipulate the dynamics of feedback systems. \cite{Towill1982_SMD} and \cite{John1994_SMD} studied the dynamics of continuous time replenishment rules with the \emph{Laplace transform}. \cite{Dejonckheere2003_SMD} studied discrete time replenishment rules via the \emph{z-transform} and the \emph{Fourier transform}. They showed the \emph{order-up-to replenishment policy} with \emph{moving average} and \emph{exponential smoothing} forecasts, for all lead-times and all possible demand patterns, always created bullwhip. 

\cite{Michna2020_SMD} studied \emph{stochastic lead times}, revealing the forecasting of lead times is an important cause of the bullwhip effect. \cite{Gaalman2022_SMD} explores the interaction between the lead time and bullwhip under \emph{general order auto-regressive moving average} demand. They reveal the interaction between demand, lead times, and bullwhip is complex and subtle; bullwhip does not always increase in the lead time. \cite{Wang2016_SMD} provides a recent review of the bullwhip effect, its causes, solution approaches, and thoughts on future research directions.

\subsubsection*{Location and localisation}
The number, and location, of distribution centres (DC) is an important problem in \emph{distribution network design}.  Too few DCs result in longer travel distances (and times) to customers; too many result in high amounts of distribution inventory. The \emph{square root law for inventory}  \citep{Maister1976_SMD} shows the amount of inventory in a distribution network falls by $1\slash\sqrt{n}$ when $n$ DC's are consolidated into a single DC.  The transportation costs involved in delivering customer demand from $n$ DCs can be accurately modelled using \emph{transportation planning software}  \citep{Hammant1999_SMD}. This software typically includes: road maps, speed limits, tolls, congestion, as well as various methods for modelling transport costs.

\emph{Postponement} can also reduce inventory in supply chains. Postponement involves delaying final assembly until demand reveals itself; products are then quickly customised to meet the consumer's desires. For example, HP build generic printers in Mexico to ship to Europe. Upon arrival, they are assigned to a country and the correct power pack is ``assembled'' into the product  \citep{Feitzinger1997_SMD}. With postponement HP holds less generic inventory to buffer the shipping lead time compared to the amount of country specific inventory it would need if the power packs were assembled in Mexico. 

Another important SCM decision is where to produce?  Should you produce locally where perhaps labour cost is high, or should you \emph{outsource}, or \emph{off-shore}, to a low labour cost country? Sometimes, offshore production is supplemented with a local factory or a \emph{near-shored supplier} in a \emph{dual sourcing} arrangement \citep{Allon2010_SMD}. A \emph{tailored base surge} policy sends constant orders to the offshore supplier with the long lead time, while the near-shore supplier flexes production quantities with a short lead time. A small local \emph{SpeedFactory} may be able to correct for the forecast errors and gain enough of an inventory advantage to offset the increased local labour costs \citep{Boute2022_SMD}. 

\subsubsection*{Information flows in supply chains}
Changing the information used in replenishment decisions can improve the dynamics of supply chains. The sharing of demand information with upstream suppliers is often referred to as the \emph{information sharing} \citep{Lee2000_SMD}, or \emph{information enrichment} strategy \citep{Dejonckheere2004_SMD}. Knowing the end consumer demand allows upstream members to base their demand forecasts on the real demand information, removing one of the potential causes of the bullwhip effect. Indeed, information sharing allows for a linear, rather than a geometrically, increasing bullwhip effect as orders go echelon-to-echelon up the supply chain \citep{Chen2000_SMD}. \cite{Kaipia2017_SMD} considers the practicalities of implementing the information sharing strategy.

Sharing both demand and inventory information with your supplier can enable the \emph{vendor managed inventory} (VMI) strategy  \citep{Dong2002_SMD}. In the VMI strategy, the consumer demand and downstream inventory information is used by the supplier (the vendor) to make replenishment  decisions on behalf of his customer. This allows two supply chain echelons to behave dynamically as one echelon, removing a bullwhip generating decision from the supply chain \citep{Holweg2005_SMD}.

\subsubsection*{Coordinating supply chain contracts}
Supply chains often consist of many different organisations, each operating to maximise their own profit. Due to the \emph{double marginalisation} problem, if each player acts solely in their own interests, the supply chain will not be able to reach the \emph{first best} solution; money will be left on the table. Sometimes, the first best solution can be reached by a \emph{centralised decision-maker} coordinating the supply chain; at other times the altruistic behaviour of one supply chain member, in return for a \emph{transfer payment}, can coordinate. 

There are many different types of contracts \citep{Cachon2005_SMD}: revenue sharing, buy-back, price-discount, quantity-flexibility, sales-rebate, franchise, and quantity discount contracts to name just a few. All have their strengths and weakness and are applicable in different settings. Many contracts are based on \emph{newsvendor} principles \citep{Lariviere2016_SMD}. Another important concept in contract design is the idea of \emph{Pareto improving} contracts, where no player is worse off than the (locally optimised) \emph{base case}, but at least one other player is better off. Other contracts allow for the \emph{arbitrary allocation of profits} between players, and for the delegation of decision-making activities to others \citep{Chintapalli2017_SMD}.

\subsubsection*{Emerging topics in the field of supply chain management}
Emerging topics in the SCM field include: 
\begin{itemize}[noitemsep]
    \item The distributed ledger technology behind \emph{block chains} \citep{Babich2020_SMD} and \emph{cryptocurrencies} \citep{Choi2020_SMD} can be used to create a permanent record of provenance and ownership. Ensuring your cotton has not been produced by slaves, your diamonds are not conflict, and children did not mine your Lithium is vital now as UK Directors can face prison time under the Proceeds of Crime Act for crimes committed in their supply chains.
    \item \emph{Opaque pricing} is a technique used to sell last minute travel industry inventory (e.g., hotel rooms) at discounted prices.  The traveller books a room without knowing the exact hotel brand. Cost sensitive travellers are happy because they get a bargain. The hotel is happy because they get extra income without damaging their brand. Opaque pricing can be used for products as well; for example, a red pen sells for \$10, and a blue pen sells for \$10, but if you don't care which colour you have, a red-or-blue pen is offered at the opaque price of \$8. The vendor is able to use the customer's lack of preference to reduce inventory requirements \citep{Ren2022_SMD}.
    \item \emph{Quantum computing} allows one to solve $\mathcal{NP}$-hard problems (such as the \emph{travelling salesman} problem) to optimality instantaneously, rather than waiting for months with regular computers \citep{Srinivasan2018_SMD}. This technology has the potential to make supply chains more efficient. 
\end{itemize}

\subsection[Sustainability (Akshay~Mutha)]{Sustainability\protect\footnote{This subsection was written by Akshay~Mutha.}}
\label{sec:Sustainability}

In this subsection, we focus on the area of sustainable operations from the perspective of \textit{closed-loop supply chains (CLSC)}. We consider literature that focuses on product-, module/part-, and material-level recovery and reuse activities. These activities provide economic and environmental benefits. CLSC entail transportation and acquisition of used products; sorting, grading and disposition for different recovery methods; disassembly and reassembly (i.e., remanufacturing operations); and marketing of remanufactured products. \cite{Guide2003-qd_AM} and \cite{Ferguson2010-pb_AM} provide comprehensive overviews of the strategic, tactical, and operational aspects of CLSC. 

The supply side in CLSC differs from traditional supply chains in the following ways. The quantity of used products being returned is uncertain; the timing of when they are returned is uncertain; and the condition (quality) in which they are returned is also uncertain. These differences lead to uncertain recovery rates and processing times, uncertain cost of recovery, and imperfect matching between supply of used products and demand for remanufactured products, and hence the subsequent demand for new parts needed to make the remanufactured (finished) product. Below, we provide a brief overview of some of the methods used to optimise the different activities in CLSC, while managing these uncertainties. 

The \textit{reverse logistics (RL) network} (see also \S\ref{sec:Logistics}) handles the collection of used products from end-users, and their transportation between collection points, consolidation centres, testing, sorting, and grading facilities, and recovery (e.g., remanufacturing, reuse, recycling) facilities and landfill locations. Stylised and game-theoretic models are developed to determine the optimal collection strategy for producers (if they choose to, or are required to collect used products). The collection strategy includes decisions on whether producers should collect directly from end-users, or use the retail network as collection points, or use third-party collectors \citep[e.g.,][]{Savaskan2006-ou_AM}. In further analysis of the collection strategy, the continuous approximation method is used to determine whether the producer (or business) should offer to pick-up, or have end-users drop-off the used product \citep[e.g.,][]{Fleischmann2003-fx_AM}. 

Several quantitative models are developed to determine the optimal RL network design. An extensive discussion of these models and solution approaches can be found in \cite{Akcali2009-zg_AM}. Linear programming, mixed-integer linear programming (MILP), and stochastic programming are widely used to determine optimal network structures. \cite{Fleischmann2004-wv_AM} provide and excellent overview of MILP and stochastic programming models for facility location and network design for dedicated reverse, and integrated (forward and reverse) logistics networks. Mixed-integer nonlinear programming models are also sometimes used to determine the optimal RL network structure \citep[e.g.,][]{De_Figueiredo2008-rt_AM}. In addition to optimal network design, vehicle routing models (\S\ref{sec:Transportation_Vehicle_routing}) are used to determine optimal collection and pick-up routes. These vehicle routing problems are often $\mathcal{NP}$-hard, and are based on location of demand: node, arc, and general. The models are extended to include vehicle routing with backhaul, routing with simultaneous delivery and pick-up, and routing with partially mixed deliveries and pick-ups \citep[see][and references therein]{Beullens2004-nf_AM}.  

One way of managing the supply uncertainty in CLSC is to forecast the return of used products. Different methods are used to compute the product return probability. These include modelling returns as a function of past sales (via a known delay distribution), regression models \citep{Samorani2019-rw_AM}, simulation models, and queueing models \citep[e.g.,][]{Toktay2000-iy_AM}

Buyers of used products (i.e., producers or their contract-remanufacturers, and third-party remanufacturers) actively manage the supply uncertainty (timing, quantity, and quality) by using incentive mechanisms such as quality-based pricing, trade-ins, and buybacks. Buyers acquire used products either in sorted (i.e., known quality-levels) or unsorted (i.e., unknown quality-levels) form. Lot-sizing models are developed to determine the optimal acquisition quantity when the used products are available in unsorted form \citep[e.g.,][]{Galbreth2009-ar_AM}; and when they are available in sorted (continuous or discrete quality-levels) form \citep[e.g.,][]{Mutha2016-dc_AM}. The acquisition process has also been analysed in the context of buyer-supplier contracts. The objective of these analyses is to determine the optimal contract structure with known or unknown quality levels, e.g., quality-dependant acquisition costs and quantities \citep{Mutha2019-nv_AM}, and coordination mechanisms \citep{Debo2004-xh_AM,Vedantam2021-zj_AM,Li2022-ad_AM}. Several models are also developed to determine the optimal acquisition cost for used products and selling price for remanufactured products for an exogenous set of discrete quality-levels of used products \citep[e.g.,][]{Guide2003-nv_AM}. 

Testing, sorting, and grading of the acquired used products are important activities in product recovery operations. \cite{Hahler2017-qt_AM} provide a detailed description of these operations for used consumer electronics. Sorting defective, and economically and technically infeasible-to-remanufacture units from the acquired quantity streamlines the subsequent operations (e.g., transportation, disassembly, and reassembly). Knowing the quality of the incoming units before scheduling recovery operations significantly improves the performance of the system. The benefit of yield information (i.e., information on the quality distribution of the incoming units) has been analysed using lot-sizing models and simulation models \citep[e.g.,][]{Ketzenberg2003-ek_AM}. Several models are developed to optimise the different decisions in the grading process, e.g., the optimal number of grades \citep[e.g.,][]{Ferguson2009-vt_AM}, the resulting optimal grade-wise remanufacturing cost and selling price \citep[e.g.,][]{Mutha2022-ry_AM}, and the optimal location and timing of the sorting and grading process, for example at the point of collection/return or at the disassembly stage \citep[e.g.,][]{Guide2006-eo_AM,Zikopoulos2008-sq_AM}. 

The disposition of sorted and graded used products typically involve a problem of optimal assignment of the economically and technically recoverable units to different recovery options, e.g., product-level recovery (i.e., remanufacturing); module/part-level recovery (i.e., reuse for making remanufactured and new products, or for spares); and the non-recoverable products for material-level recovery (i.e., recycling). The assignment decisions are usually based on considerations of supply (yield information, processing times, and costs) and demand (revenue, opportunity cost, and inventory cost). Optimal control models \citep[e.g.,][]{Inderfurth2001-ye_AM} and revenue management-based models \citep[e.g.,][]{Pince2016-ol_AM,Calmon2017-gn_AM,Calmon2021-vf_AM} are widely used to determine optimal disposition decisions. Depending on the type of the product, single-period models (for products with short lifecycles, e.g., cellphones) and multiperiod models (for products with long lifecycles, e.g., engines) are used in the disposition analyses. For example, \cite{Ozdemir-Akyildirim2014-by_AM} formulate the optimisation problem as a multiperiod Markov decision process (MDP) and provide a linear-programming model for solving the deterministic approximation of the MDP model. 

Within the production planning and control literature in CLSC, a relatively small part has focused on disassembly planning and sequencing, and material requirement planning (MRP). \cite{Inderfurth2004-qp_AM} provide an extensive overview of the various optimisation models developed to optimise these elements, including shop floor control rules, in remanufacturing-only and hybrid (joint manufacturing and remanufacturing) systems. Disassembly sequencing is mainly analysed using direct graphs \citep[see][and \S\ref{sec:Graphs_and_networks}]{Lambert2003-au_AM}, and MRP decisions are analysed from an inventory control perspective \citep[e.g.,][]{Inderfurth1999-kv_AM,Ferrer2001-yn_AM}. A significant part of the literature on CLSC has focused on inventory management. Optimal inventory control policies are derived using periodic-review models \citep[e.g.,][]{R_Teunter2004-cp_AM,Zhou2011-zd_AM} and continuous-review models \citep[e.g.,][]{Van_der_Laan1999-kp_AM,Toktay2000-iy_AM,Jia2016-vy_AM}. The single-period newsvendor-like models are largely analysed as acquisition lot-sizing models (discussed in the preceding paragraphs). 

The research on market (selling)-related aspects of CLSC is focused around understanding the profit and pricing implications due to the co-existence of new and remanufactured products in the (same) market, and on understanding the customers of remanufactured products. Optimisation models are developed to determine the pricing and profitability of remanufactured products \citep[e.g.,][]{Ovchinnikov2011-qd_AM,Abbey2015-dl_AM}. Game-theoretic models are developed to determine optimal market-segments (based on pricing) for new and remanufactured products \citep[e.g.,][]{Debo2005-vu_AM,Atasu2008-za_AM}. The market for- and customers of- remanufactured products are mostly analysed using empirical methods, e.g., using sales data from websites selling used and remanufactured products, usually accompanied by customer surveys \citep[e.g.,][]{Guide2010-ey_AM,Subramanian2012-om_AM}. Behavioural experiments are used to understand consumer perceptions (e.g., quality, functionality), their acceptance (and rejection), and willingness to pay for remanufactured products \citep[e.g.,][and references therein]{Abbey2017-cn_AM}.

\subsection[Telecommunications (Bernard~Fortz)]{Telecommunications\protect\footnote{This subsection was written by Bernard~Fortz.}}
\label{sec:Telecommunications}

Operational Research plays a key role in the design and management of telecommunication networks. A large variety of applications of both exact methods and heuristics can be found in the literature. We focus here on the applications for wired networks. 

\subsubsection*{Topological network design}
The earliest works on telecommunication networks focused on wired fixed line telephony. For the long-term planning of these networks, clients' demands are not known in advance, or with a lot of uncertainty.  This often gives rise to two-stage approaches where only the fixed cost of opening links are considered first, and the decisions on routing and capacity allocation taken in a second (later) stage. This approach is relevant when the fixed costs are very high compared to routing and capacity costs, and/or when topological decisions do not affect capacity decisions too much . For example, digging a trench to install fiber optic cables is very costly, while increasing capacity can be done by adding or upgrading equipment into nodes, which is relatively simple and cheap. The objective is to build a network at minimum cost, considering only the fixed cost associated with opening a link, ignoring capacity and routing costs.

Two main issues appear in the planning process of such networks: economy and survivability.  Economy refers to the construction cost, while survivability refers to the restoration of services in the event of equipment failure. A network is called a tree if it is connected (i.e., there exists a path between all pairs of nodes), and removing any link disconnects at least one pair of nodes. Trees satisfy the primary goal of minimising the total cost while connecting all nodes. The minimum cost spanning tree problem therefore received a lot of attention, see e.g.,\citet{magnanti.wolsey:95_BF}.

However, only one node or edge breakdown causes a tree network to become disconnected and therefore to fail in its main objective of enabling communication between all pairs of nodes. This means that some survivability constraints have to be considered while building the network. Usually, these constraints come in the form with $k$-connectivity requirements, i.e. the ability to restore network service in the event of a failure of at most $k-1$ components of the network. In their earliest work on the subject, \cite{grotschel.monma:90_BF} introduced a general model for survivability requirements, and studied the polytope associated with an integer programming formulation of the problem. 

The minimum-cost two-connected spanning network problem, that consists in finding a network with minimal total cost for which two node-disjoint paths are available between every pair of nodes, was studied extensively, starting with the work of \citet{monma.shallcross:89_BF}. Such networks have been found to provide a sufficient level of survivability in most cases, but it turns out that the optimal solution of this problem is often very sparse. In such a topology, primary routing paths and re-routing paths in case of failure might become very long, introducing large delays in the network.

Two kinds of solutions have been proposed to remedy this problem: The first one imposes a constraint on the length of the paths (in terms of number of links crossed), the so-called \emph{hop-constrained} models. The second approach consists of imposing that each edge belongs to at least one cycle (or {\em ring}) whose length is bounded by a given constant.

Hop-constraints were first considered by \cite{balakrishnan.altinkemer:92_BF} in order  to generate alternative solutions for a network design problem. Later on, \citet{gouveia:98_BF} presented a layered network flow reformulation that has since been used in many network design applications involving hops-constraints.

The second approach to avoid long re-routing paths in case of failure is based on the technology of {\em self-healing rings}. These are cycles in the network equipped in such a way that any link failure in the ring is automatically detected and the traffic rerouted by the alternative path in the cycle. Many problems involve setting a bound on the length of the ring including each edge. Network design problems with bounded rings were first studied in \citet{fortz.labbe.ea:00_BF}.

\subsubsection*{Location problems}

Location problems play a central role in telecommunications network design. We focus here on problems arising in wired (optical) telecommunications networks. These problems are mostly concerned with decisions related to the placement of specific equipments into nodes of the network, and are closely related to hub location problems \citep{alumur.kara:08_BF}.

The \emph{Concentrator Location Problem} is probably the most basic application of equipment placement. The problem consists of determining the number and location of concentrators that are used to aggregate end-user demands before sending them on the backbone network. The allocation of end-users demands to the concentrators has also to be determined such that the capacities of the concentrators are not exceeded. This problem has received much attention in the literature, starting with the work of 
\citet{pirkul:87_BF}.

Another classical problem arises with the replacement of an old technology by a new one, e.g., when telecommunications companies replace outdated copper twisted cable connections by fiber optic connections. The \emph{Connected Facility Location Problem} (ConFL) aims at optimising the building cost for networks involving the two technologies, which is modeled as \emph{tree-star} networks: the core network, made of fiber optic connections, has a tree topology and interconnects multiplexers that switch traffic between fiber optic and copper connections. Each multiplexer is the centre of a star-network of copper connections to the customers. Early work on ConFL concentrated on approximation algorithms, such as the primal-dual procedures proposed by \cite{swamy.kumar:04_BF}. The currently best-known constant approximation ratio is given by the 4-approximation algorithm of \cite{eisenbrand.grandoni.ea:10_BF}. Heuristic approaches have been proposed by \cite{ljubic:07_BF} and \cite{bardossy.raghavan:10_BF}. Different Mixed Integer Programming models for ConFL were proposed by \cite{gollowitzer.ljubic:11_BF}. 

In addition to these long-term design problems, operational short-term decisions are related to the routing of demands in the network, with a focus on avoiding congestion. Most networks nowadays operate the \emph{Internet Protocol}. The internet is a collection of inter-connected networks called autonomous systems, that operates under a hierarchy of layers. An \emph{Autonomous System} (AS) is defined as a set of routers under a single
technical administration, such as an internet service provider or a country. As of July 2022, over $100,000$ ASes were registered\footnote{\url{https://www-public.imtbs-tsp.eu/~maigron/RIR_Stats/RIR_Delegations/World/ASN-ByNb.html}}, connecting over 5 billion internet users worldwide\footnote{\url{https://www.statista.com/statistics/617136/digital-population-worldwide/}}. 

\subsubsection*{Traffic engineering}
Traffic engineering (TE) addresses the problem of efficiently allocating resources in the network so that user constraints are met. Several criteria can be used to measure the effectiveness of a routing configuration. The selection of the objective function may drastically change the quality of the resulting routing. This distinction has been illustrated in \citet{pioro.medhi:04_BF}. \citet{balon.skivee.ea:06_BF} discuss various TE objective functions and evaluate how well these objective functions meet TE requirements.

The internet routing protocols can be clustered into two main groups: \emph{inter-domain} and \emph{intra-domain}. While inter-domain are used to route traffic between ASes, \emph{Interior Gateway Protocols} (IGPs) handle the routing within ASes. As inter-domain protocols are mostly governed by administrative and political considerations, there is not much room for Operational Research techniques to be applied for performing TE. On the other hand, the optimisation of IGPs have received a lot of attention. The most popular IGPs
are based on shortest path routing: shortest paths are calculated using a \emph{link metric system}, which corresponds to the set of link weights or link metrics that belong to the same AS. The network operator controls the routing of the traffic indirectly by setting the link metrics. This gives rise to very challenging optimisation that have mostly been tackled heuristically by many authors, starting with the seminal work of \citet{fortz.thorup:00_BF}. Some exact models have also been proposed, e.g., by \citet{pioro.szentesi.ea:00_BF}.

Recently, \cite{filsfils.kumar-nainar.ea:15_BF} proposed Segment Routing (SR), a new routing protocol developed to address known limitations of traditional routing protocols in IP networks. SR offers the possibility to deviate from the shortest path by using detours in the form of nodes or links respectively called node segments and adjacency segments. Optimisation of SR is a very active field of research and has been already addressed in
\cite{bhatia.hao.ea:15_BF,hartert.vissicchio.ea:15_BF,jadin.aubry.ea:19_BF}.

\subsubsection*{Further readings}
For surveys on survivable network design, we refer the reader to \cite{christofides.whitlock:81_BF,kerivin.mahjoub:05_BF,fortz.labbe:06_BF,fortz:21_BF}. Location problems in telecommunications are surveyed in \cite{skorin-kapov.skorin-kapov.ea:06_BF,fortz:15_BF} and a unified view on location and network design problems was proposed by \cite{contreras.fernandez:12_BF}. For a detailed survey on the Concentrator Location Problem, see Chapter~2 in \cite{yaman:05*1_BF}. Traffic engineering with shortest paths routing protocols is covered in the surveys of \cite{bley.fortz.ea:09_BF,fortz:11_BF,altn.fortz.ea:13_BF}.

\subsection[Timetabling (Greet~Vanden~Berghe \& Sanja~Petrovic)]{Timetabling\protect\footnote{This subsection was written by Greet~Vanden~Berghe and Sanja~Petrovic.}}
\label{sec:Timetabling}

Timetabling  represents a particular subgroup of scheduling problems, namely the set of problems for which activities must be assigned to resources within a set of fixed timeslots. Nevertheless, the two disciplines, scheduling and timetabling, are tightly related and benefit from mutual advancements in both modelling and method development. 

Practical timetabling problems appear in many sectors, for example, in education, healthcare, sports and public transportation. They have been drawing academic attention for a few decades, partly because they are easy to grasp but challenging to solve. The timetabling community gathered at its first international conference in Edinburgh in 1995, one year before the Association of European Operational Research Societies (EURO) established a EURO Working Group on the Practice and Theory of Automated Timetabling \citep{EWG_PATAT_GVBSP}.  Ever since the third conference, which took place in 2000, the timetabling community has gathered every two years\footnote{The 25th anniversary of PATAT conferences had to be postponed till 2022, due to the COVID-19 pandemic.} to share ideas on both theoretical and practical aspects of timetabling.

This subsection provides a brief overview of timetabling history, while highlighting what makes timetabling problems computationally challenging, which initiatives have boosted timetabling research and how state-of-the-art knowledge, models and algorithms can be applied in practice. We restrict the discussion to timetabling problems involving human resources, such as students, teachers, healthcare workers and sports teams. 

\subsubsection*{Problem definition}
Let us consider a set of timeslots $T = \{1,...,|T|\}$, a set of activities $A = \{1, ..., |A|\}$ and a set of resources $R = \{1,...,|R|\}$. A timetabling problem then consists in assigning (all) the activities in $A$ to resources in $R$ and timeslots in $T$ in such a way that a set of constraints is met.  Constraints may apply to resources, timeslots and activities.  They usually restrict the number of assignments to certain resources within subsets of $T$. 

Constraints are usually divided into two categories: hard constraints, which must be strictly satisfied, and soft constraints, for which violations may be tolerated but should be avoided if possible. Weights may be set on the soft constraints, denoting their relative importance. A common timetabling objective is to minimise the weighted sum of soft constraint violations.  This objective sometimes has to be combined with other timetabling objectives, for example, to minimise the cost associated with the employed resources. 

\subsubsection*{Educational timetabling} 
Educational timetabling problems can be split into three major groups: university examination timetabling, university course timetabling and high-school timetabling. In {\it examination timetabling}, the task is to assign examinations in $A$ to a limited number of timeslots in $T$ and rooms in $R$ such that no student has more than one exam at a time.  Each student's exams should be spread out in time as much as possible. Additional constraints may include precedence constraints between exams, special room requirements, and limited room capacities. {\it Course timetabling} involves the assignment of course sections  (lectures, tutorials, lab sessions, seminars) to specific days of the week and times of the day. Real-world problems may require sectioning, when students have to be split into separate subgroups for different sections. Typically, the objective is to minimise the number of students' conflicts. {\it High-school timetabling} assumes that students are split into classes and each class has to take a set of resources. Given a set of timeslots, each activity (involving both a student group and a teacher) must be assigned to a timeslot so that no teacher and no student group are participating in more than one activity at a time. Most practical problems have additional constraints;  for example, teachers may have limited  availability and some activities may require more than one timeslot. In general, educational timetabling problems are $\mathcal{NP}$-hard \citep{DeWerra_GVBSP}. Additionally, the constraints often pose a feasibility challenge. 

The educational timetabling community made a considerable effort to create rich sets of benchmark instances to be used for comparing methods. The first set of examination timetabling instances was defined by \citet{carter1996_GVBSP}.  Four competitions on educational timetabling, entitled  ITC-2002 \citep{ITC-2002_GVBSP}, ITC-2007 \citep{McCollumEtal2010_GVBSP}, ITC-2011 \citep{post2016_GVBSP} and ITC-2019 \citep{muller_GVBSP}, further advanced the development of timetabling algorithms. \cite{post2012_GVBSP} developed a general format and benchmark instances for high-school timetabling, which were extended later by \cite{post2014_GVBSP}. \citet{CESCHIA2022_GVBSP} published a review of educational timetabling, presenting detailed characteristics of all benchmark instances and state-of-the-art results. \textsc{OptHub}\footnote{\hyperlink{https://OPTHUB.uniud.it}{https://OPTHUB.uniud.it}} provides a common platform for storing problem instances and solutions to selected optimisation problems, including educational timetabling. 

\subsubsection*{Personnel timetabling}
Personnel timetabling, also referred to as employee timetabling or rostering, concerns the construction of a timetable for personnel in $R$ in such a way as to satisfy coverage constraints throughout a time horizon \citep{Ernst_GVBSP}. The timeslots in $T$ often represent shifts, which correspond to tasks or duties in $A$. Some activities may require certain skills, and hence can only be conducted by a subset of $R$. Many {\it work-rest}-related objectives are formulated in terms of {\it time-related constraints}, restricting, for example, the number of hours worked, the number of weekends worked, the number of consecutive night shifts \citep{JOS_GVBSP}. Additionally, personnel rostering problems typically consider {\it personal preferences} as regards working time or days off. Whereas the problem is generally considered $\mathcal{NP}$-hard, \cite{smet2016polynomially_GVBSP} showed that some personnel rostering problems are polynomially solvable, provided they do not contain a particular class of  constraints. \cite{categorisation_GVBSP} developed a categorisation of personnel rostering problems, based on the characterisation of resources, objectives and constraints.  \cite{kingston2018unifed_GVBSP} complemented this work by providing a unified notation for nurse rostering problems. 

The Practice and Theory of Automated Timetabling (PATAT) community organised two International Nurse Rostering Competitions, entitled INRC I and INRC II. The problem definition of INRC I \citep{INRCI_GVBSP} was based on the instances collected by \cite{curtois_instances_GVBSP}. INRC II \citep{ceschia2019second_GVBSP} incorporated real-world constraints concerning subsequent rostering horizons. The competition datasets have been collected and published\footnote{\url{https://patatconference.org/}}.

Apart from the constraints and objective functions considered in the two INRCs, some sectors expect their personnel rosters to be cyclic \citep{musliu_GVBSP,cyclic_GVBSP}. Recent trends also include  objectives related to  fairness \citep{fairness_GVBSP} and well-being \citep{well-being_GVBSP}.  Objective priorities set by the users may lead to unwanted solutions. To address this issue, \cite{weights_GVBSP} developed an approach to automatically set acceptable weights which avoid conflicting objectives from leading to poor solutions. 

\subsubsection*{Sports timetabling}
Sports timetabling problems often address tournament or competition scheduling. They require assigning sports activities in $A$, represented by {\it pairs} of teams in $R$, to timeslots in $T$ in such a way that each team meets all the other teams. Constraints depend on the competition's rules, which may differ in different  parts of the world \citep{ribeiro_GVBSP,duran2021sports_GVBSP}. Specific sports timetabling constraints prescribe that teams must not meet the same opponent within consecutive timeslots, or that the number of consecutive home or away games is restricted. The travelling tournament problem (TTP), introduced by \cite{Easton2001_GVBSP}, is an academic adaptation of the Major League Baseball competition in the United States.  The objective of the TTP is to minimise the sum of travelling distances for each  team.  Travelling umpire scheduling \citep{TUP_GVBSP} is subject to similar constraints, but it assumes that the tournament is fixed and that each game is assigned an umpire.

\cite{Rasmussen2008_GVBSP} provided a review on round robin sports timetabling, where each team plays against each other team twice, once at home and once away. \citeauthor{Drexl2007_GVBSP}'s (\citeyear{Drexl2007_GVBSP}) review focused on graph-theoretical approaches to sports timetabling.  \cite{Briskorn2010_GVBSP} investigated the complexity of several variants of the round-robin tournament problem, and similarly, \cite{DEOLIVEIRA2015101_GVBSP} studied the complexity of travelling umpire scheduling problems. The characteristic sports timetabling constraints, which forbid the assignment of activities to subsets of $T$, can be  challenging in terms of feasibility. 

\cite{TTP_GVBSP} and \cite{TUP_Website_GVBSP} boosted sports timetabling research by publishing challenging benchmark instances and monitoring best known and/or optimal results. \cite{sports_competition_GVBSP} organised the first international sports timetabling competition, for which the instances are available at the website of \cite{UGENT_STT_competition_GVBSP}.

\subsubsection*{Timetabling and related problems}
Academic timetabling problems are often considered in isolation from other problems. However, many real-world situations face timetabling entangled with other optimisation problems. Solutions for one of them have an impact on the solution for the other problems. For example, the {\it staffing} problem is concerned with optimising a group of human resources and their characteristics such as skills and contracts in an organisation, across a relatively large time horizon.  From a staffing perspective, the personnel structure should adequately cover the organisation’s anticipated workload while respecting the available budget. On the other hand, from a rostering perspective, the personnel structure should enable computing good quality rosters across many subsequent rostering periods \citep{Komarudin_GVBSP}. Similarly, {\it task scheduling} usually assumes personnel rosters are fixed, but both problems can also be addressed in an integrated manner \citep{paul2015classification_GVBSP}. The {\it workforce routing and scheduling} problem is related to vehicle routing. Apart from scheduling a fleet of vehicles to serve a set of customers, timetabling issues, such as temporal constraints, contracts and skills are also imposed on the problem \citep{Dario_GVBSP}. Some {\it production scheduling and inventory} problems are subject to additional timetabling restrictions which apply to their employees \citep{altachem_GVBSP}.

\subsubsection*{Where do we stand and what is the future}
Academic timetabling has made good progress and instances, models and algorithms have been shared and published.  For example, the heuristic search strategies Step  Counting Hill-climbing \citep{step_GVBSP} and Late Acceptance Hill-climbing \citep{LA_GVBSP} were initially developed for solving timetabling problems.  Due to their simplicity and effectiveness, they continue to be used in a much wider application domain by many computational experts. 

So long as some instances remain unsolved, or solutions for instances have not been proven optimal, algorithm development  remains open for improvement. Future challenges may also apply to new combinatorial optimisation problems encompassing a timetabling component. They may not necessarily map to any of the three timetabling categories detailed in this chapter. However, they may gain importance due to either increased practical need or academic initiatives, such as the publication of benchmarks or the organisation of competitions.

Apart from these future computational challenges, timetabling research should also focus on how to address {\it human resources'} considerations. Besides the traditional work-rest constraints and objectives, academia should also reconcile personnel well-being with their perception of fair workload within a team and with their level of autonomy in determining their personal timetables.  Research should also focus on how to address the increasing personnel resignation in human-centric working environments such as education and healthcare. Robust timetabling, for example, has a lot of potential and at the same time induces scientifically interesting modelling questions.

\subsection[Transportation: Rail (David~Canca)]{Transportation: Rail\protect\footnote{This subsection was written by David~Canca.}}
\label{sec:Transportation_Rail}

The transportation of goods and passengers by rail has played an important role in the evolution of industrialised societies, contributing to their development and prosperity. Rail freight transport still holds critical importance in supporting the economic growth of many countries around the world due to its contribution to guaranteeing an efficient flow of goods internally and across borders. Furthermore, rail transportation is also essential for the movement of people, being the preferred transportation mode for commuters in many large urban areas. This preponderant role also affects the internal mobility of cities. First, a differentiation must be made between freight and passenger transport. Freight trains are longer and heavier than passenger trains, and can often have multiple propulsion units. Compared to that, passenger trains are much lighter and have more horsepower per tonne. There are also important planning and operational differences, whereas passengers decide freely where they will travel, each load of freight must be managed and routed from a specific origin to its destination. These differences originate very different problems in both areas. Even in passenger transportation, different problems arise depending on the type of service; long- and medium-distance, commuter rail, urban rapid transit, and scenic and sightseeing train transportation; see, for instance, \citet{Caprara2007_DC}.

Despite all these differences, a set of common hierarchical stages can be highlighted in the process of planning and operating a rail transportation system  \citep{Bussieck1997a_DC}: network design and/or line planning, timetabling, platforming, rolling stock circulation, shunting, and crew planning. 

At a strategic level, the problems are characterised by long planning horizons and typically involve resource acquisition. This level includes network design and line planning problems. The first  refers to the construction or modification of existing railway infrastructure and mainly concerns urban rapid transit systems.  For a railway company or agency, the line planning problem consists of defining a set of lines and determining their frequencies, and it is usually the first stage in planning medium and long-distance passenger rail networks. 

\citet{Bussieck2004_DC} considered the design of line plans in public transport with the objective of minimising the total cost.  \citet{Goossens2006_DC} presented several models for solving line planning problems in which lines can have different halting patterns. \citet{Laporte2007_DC} proposed a first railway rapid transit network design model to maximise the expected trip coverage. \citet{GutierrezJarpa2013_DC} presented a model to minimise travel cost while maximising the captured demand. See also \cite{Laporte2015_DC} for an extension where  the idea consists of first building a set of segments within broad corridors connecting some vertex sets to later assemble the segments into lines.

A different set of works pays attention to the formulation of network design models from scratch. Starting from an underlying network, these models construct lines by joining edges, incorporating topological constraints to guarantee connectivity between consecutive edges of each line.  This approach gives rise to complex models which are quite difficult to solve using exact procedures; see, for instance, the work by \citet{Szeto2014_DC}, or the recent works by \citet{Canca2017_DC} and \citet{Canca2019b_DC} which concern the design of a railway rapid transit network.

For a comprehensive review of the different methodologies used in practice to solve this problem, the readers can consult the review of \citet{Guihaire2008_DC}. More recent reviews of \citet{Shoebel2012_DC} and \citet{IbarraRojas2015_DC} present a systematic classification of problem variants, considered objectives and solving methodologies. 

At the tactical level, the next stage in planning a railway system consists of several problems, starting with scheduling and timetabling, followed by rolling stock planning, crew rostering, and crew scheduling. The timetabling problem concerns the determination of the arrival and departure times of trains to stations. When overtaking and overlapping are allowed, the timetabling problem becomes a train scheduling problem. Timetables can be cyclic, regular, hybrid, and demand-driven. 
Concerning the design of cyclic timetables, \citet{Caprara2002_DC} proposed a graph-theoretic formulation for the train timetabling problem using a directed multigraph in which nodes correspond to departures and arrivals at a certain station at a given time instant.  \citet{Liebchen2002_DC} used a Periodic Event Scheduling problem (PESP) with several add-ons concerning problem reduction and strengthening. \citet{Chierici2004_DC} extended the classical timetabling model to take into account the reciprocal influence between the quality of a timetable and the transport demand captured by the railway with respect to alternative means of transport. \citet{Cacchiani2008_DC} proposed heuristic and exact algorithms for the (periodic and non-periodic) train timetabling problem on a corridor.

Regular timetables have been commonly used in the case of railway rapid transit systems, especially at relatively short time planning horizons where demand can be considered approximately constant.  \citet{Canca2016a_DC} proposed a sequential optimisation approach to determine the best regular timetable for a railway rapid transit network where lines share tracks. \citet{Canca2017a_DC} incorporated aspects of energy consumption in the design of a two-way rapid rail transit line. Later, \citet{Canca2018_DC} extended the previous work to a full network, taking into account transfers between lines. \citet{Robenek2017_DC} proposed a new type of timetable combining both the regularity of the cyclic timetables and the flexibility of the non-cyclic ones. 

During recent years, starting from the works of \citet{Canca2014_DC} and \citet{Niu2013_DC} many researchers have paid attention to the design of demand-driven timetables \citep[see, for instance,  ][]{Barrena2014a_DC, Barrena2014_DC}. The design of a specific train timetable can be combined by using different acceleration strategies such as stop-skipping and short-turning. For example, given predetermined train skip-stop patterns, \citet{Niu2015_DC} proposed a quadratic integer programming model with linear constraints to synchronise effective passenger loading and train arrival and departure times at stations. \citet{Zhou2022_DC} proposed a mixed integer linear programming model to jointly optimise the train timetable and the rolling stock circulation plan, allowing rolling stock to change its composition through coupling/decoupling operations at the terminal stations of a metro line. \cite{Yuan2022_DC} introduced a new integrated optimisation model for train timetabling that also considered rolling stock assignment and incorporated a short turn strategy on a bidirectional metro line. 

Several authors have also proposed methods to increase the transport capacity of a given timetable \citep[see ][]{Burdett2009_DC}. \citet{Cacchiani2010_DC} studied the problem of incorporating freight transport trains in railway networks, where both passenger and freight trains are running. To finish this description of the OR contributions for the train timetabling problem, a special mention of the work by \citet{Kroon2009_DC} is convenient. In this research, the authors generated several real timetables using Operational Research techniques for the Dutch railway network.

Rolling stock management is probably the most complex stage in the classical sequential railway planning process and plays a key role in the efficient operation of railway networks. At a tactical level, the rolling stock circulation plan consists of a set of interrelated subproblems such as train composition decisions (coupling and decoupling operations involving locomotives and carriages), selection of rest locations, the design of vehicle circulations (specific paths that vehicles must follow to guarantee an efficient and safe operation), and the definition of maintenance policies \citep{Caprara2007_DC}. In a general rolling stock circulation problem, every train circulation has a variable length (distance and number of days) and incorporates information about the allowed specific rolling stock types, composition, coupling/decoupling operations, maintenance and cleaning activities. \citep{Maroti2005_DC, Maroti2007_DC}. Other practical considerations such as rolling stock availability, depot capacity \citep{Lai2015_DC}, coupling and decoupling activities \citep{Fioole2006_DC}, turnaround times, maintenance \citep{Maroti2007_DC}, and track and platform capacities are simultaneously considered depending on the specific problem. Given the importance of this topic within the set of planning tasks, other contributions have been proposed for different problems concerning rolling stock management, as, for instance, determining a set of minimum cost equipment cycles such that the most convenient rolling material is assigned to each planned trip \cite{Cordeau2000_DC} or obtaining the optimal circulation of rolling stock considering order in train compositions \citep{Alfieri2006_DC, Peeters2008_DC}. Maintenance also plays an important role in several rolling stock management contributions, see, for instance, the works by \citet{Maroti2005_DC, Giacco2014_DC} and \citet{Dariano2019_DC}. Robustness is another topic of interest in the related literature. Interested readers can consult the works by \citet{Cacchiani2008a_DC, Cacchiani2012_DC}.

After rolling stock management, the crew scheduling process determines the set of duties that covers all programmed services \citep{Caprara1998_DC}. Finally, the crew is assigned to serve the crew schedule and the corresponding train services \citep{Huisman2005_DC}. The rostering process aims at determining an optimal sequencing of a given set of duties into rosters satisfying operational constraints deriving from union contract and company regulations \citep{Caprara2003_DC}. 

To finish this section, two important problems of rail freight transportation are briefly commented. The first concerns the strategic design of freight transport networks and the second concerns the tactical operation of marshalling yards. Concerning the design of service networks, \citet{Crainic1984_DC} analysed the problems of routing freight traffic, scheduling train services, and allocating classification activities between yards on a rail network. \cite{Crainic1990_DC} developed a model of rail freight transportation adapted for the strategic planning of freight traffic considering other transportation modes. \citet{Zhu2014_DC} addressed the problem of scheduled service network design for freight rail transportation integrating service selection and scheduling, car classification and blocking, train makeup, and shipments routing based on a three-layer cyclic space-time network representation. 

Shunting yards, also known as marshalling or classification yards, play a key role in rail freight transport networks, acting as hubs where inbound trains are first disassembled and the carriages are then to form new convoys, generating new trains which transport the load towards the correct destinations. This procedure allows carriages to be sent through the network according to their destinations without the need for many connections. Therefore, time savings in shunting operations \citep{Jaehn2015_DC} have a great impact on cost savings in the movement of freight through the rail network \citep{Boysen2012_DC}. In passenger transportation, shunting operations focus on train units that are not necessary to operate a schedule and must be parked at shunt yards. Since different types of trains use the rail infrastructure, the specific type of a unit restricts the set of shunt tracks where they can be parked. The aim of this problem is to assign train locations to the shunt tracks while minimising routing costs from platforms to the corresponding shunt tracks \citep{Huisman2005_DC, Kroon2008_DC}. For a more detailed description of the optimisation problems involved in shunting operations, we refer the reader to the works by \citet{Jaehn2016_DC} and \cite{Ruf2021_DC}.

\subsection[Transportation: Maritime (Harilaos~N.~Psaraftis)]{Transportation: Maritime\protect\footnote{This subsection was written by Harilaos~N.~Psaraftis.}}
\label{sec:Transportation_Maritime}

Maritime transportation carries more than 80\% of the world’s trade and some 70\% of the value of that trade \citep{Unctad2022-sd_HP}. The spectrum of Operational Research (OR) applications in maritime transportation is broad. Following the classification of \cite{Christiansen2013-cg_HP}, these problems can be broken down into three levels: \textit{strategic}, \textit{tactical} and \textit{operational}. Some typical problems in each of these levels will be described in this section.

It is important to note that, in much of the OR maritime transportation literature, traditional economic criteria such as cost minimisation or profit maximisation are the norm, and environmental criteria (for instance emissions minimisation) are less frequent. However, with the quest to decarbonise shipping \citep{Imo2018-zj_HP}, the body of knowledge that includes environmental criteria is growing very fast in recent years. Sometimes environmental criteria map directly into economic criteria: if for instance \textit{fuel cost} is the criterion, and since it is directly proportional to ship \textit{emissions}, if fuel cost is to be minimised as an objective, so will emissions, and the solution is \textit{win-win}. However, for other objectives this direct relationship may cease to exist and one would need to look at environmental criteria in their own right. 

In conceptual terms, if $x$ is a vector of the decision variables of the problem at hand, $f(x)$ is the fuel cost associated with $x$, $c(x)$ is the cost other than fuel and $m(x)$ are the associated maritime emissions (carbon, sulphur, or other), then a generic optimisation problem is the following:
\begin{gather*} 
\text{Minimise } \alpha(f(x)+c(x))+\beta m(x) \\
\text{s.t. }  x \in X
\end{gather*}
where $\alpha$ and $\beta$ are user-defined weights (both  $\geq0$) representing the relative importance the decision maker assigns to cost versus emissions, and $X$ represents the feasible solution space, usually defined by a set of constraints.

One can safely say and without loss of generality that if $d(x)$ is the amount of fuel consumed, $p$ is the fuel price, and $e$ is the emissions coefficient (kg of emissions per kg of fuel), then $f(x)=pd(x)$ and $m(x)= ed(x)$. Therefore $f(x)= km(x)$ with $k=p/e$, as both $f(x)$ and $m(x)$ are proportional to the amount of fuel consumed $d(x)$. The cases that different fuels are used onboard the ship, for instance in the main engine vs the auxiliary engines, or if fuel is switched from high to low sulphur along the ship's trip, represent straightforward generalisations of the above formulation. Then the above problem can also be written as
\begin{gather*} 
\text{Minimise } \alpha c(x)+(\alpha k+\beta)m(x) \\
\text{s.t. }  x \in X
\end{gather*}

\noindent The following special cases of the above problem are important:

\begin{enumerate}[noitemsep]
   \item The case $\alpha=0$, $\beta>0$, in which the problem is to minimise emissions.
   \item The case $\alpha>0$, $\beta=0$, in which the problem is to minimise total cost.
   \item The case $c(x)=0$, in which fuel cost is the only component of the cost.
\end{enumerate}

\noindent A solution $x$* is called \textit{win-win} if both case (1) and case (2) have $x$* as an optimal solution. It is important to realise that such a solution may not necessarily exist.

It is also straightforward to see that in case (3), cost and emissions are minimised at the same time and we have a \textit{win-win} solution. It is clear that $c(x)=0$ is a \textit{sufficient condition} for a win-win solution. But this is not a \textit{necessary} condition, as it is conceivable to have the same solution being the optimal solution under two different objective functions. An interesting question is to what extent policy makers can introduce either (\textit{a}) a Market Based Measure (MBM) such as a fuel tax and/or (\textit{b}) a set of constraints, that would make win-win solutions possible. 

Let us now examine some typical OR problems in the 3-level hierarchy.

\textit{Strategic level problems} involve planning horizons of several years (from 1 to 25). Among them, \textit{fleet size and mix} problems involve the basic questions, what is the best mix for a shipping company's fleet in the years ahead? How large should these ships be? How many should they be, and how fast they should go? See \cite{Alvarez2011-do_HP}, \cite{Zeng2007-xf_HP} and \cite{Pantuso2014-bk_HP} for some work in this area.

\textit{Network design} problems also belong to the strategic problem category and are special to liner shipping. They involve the design of a liner company's network, which comprises the ports it would serve, the routes it will use, which ports will be chosen as hub ports, how are the company's feeder networks configured, and whether the company will use the hub-and-spoke concept or direct calls. See \cite{Agarwal2008-vx_HP}, \cite{Reinhardt2012-do_HP}, and \cite{Brouer2014-ue_HP} for more on these problems.

\textit{Tactical level problems} involve intermediate planning horizons, from a few days to a year. Among them, \textit{ship routing and scheduling} is perhaps the most important problem class, mainly for tramp shipping, with works by \cite{Christiansen2013-cg_HP}, \cite{Andersson2011-qg_HP}, \cite{Fagerholt2010-lr_HP}, and \cite{Lin2011-yp_HP}. Routing and scheduling of offshore supply vessels belongs also to this area \citep{Halvorsen-Weare2011-fm_HP,Norlund2013-qg_HP}. All of these problems call for the determination of the best set of ship routes under some predefined criteria.

\textit{Fleet deployment} is also included in the class of tactical level problems, calling for the allocation of ships to routes \citep[see][among others]{Meng2011-of_HP,Andersson2015-rj_HP,Lai2022-nz_HP}. \textit{Speed optimisation} problems are also tactical level problems and have received increased attention in recent years, due to the pivotal role of ship speed with regard to both economic and environmental criteria. Due to the fact that fuel consumption is a nonlinear function of ship speed, these problems are typically nonlinear. Related formulations attempt to find best vessel speeds along the legs of the route, according to specific criteria \citep[see][]{Psaraftis2013-zv_HP,Fagerholt2013-on_HP,Magirou2015-bq_HP}. These problems may also involve \textit{flexible frequencies} \citep{Giovannini2019-fb_HP}.

Speed and route decisions may also be combined \citep{Psaraftis2014-lu_HP,Wen2017-ku_HP}. One of the perhaps counter-intuitive results of these combined scenarios is that sailing the minimum distance route at minimum speed does not necessarily minimise fuel consumption and hence emissions. This may be so whenever the minimum distance route involves a heavier load profile for the ship. Depending on ship type, the difference in fuel consumption between a fully loaded and a ballast (empty) condition can be up to 40\%. A result that is less surprising is that expensive cargoes sail faster and hence induce more emissions. This is to be expected if cargo in-transit inventory costs are taken into account.

\textit{Modal split/discrete choice models} examine scenarios in which shippers may choose a transportation mode that is alternative to the maritime mode as a result of unfavourable time, cost, or other considerations. As a result, cargoes from the Far East to Europe may prefer the rail vs the maritime mode, or cargoes in European short sea trades may choose the road mode as opposed to shipping. Such modal shifts may increase the overall level of CO2 and may warrant mitigation measures by the shipping lines and the policy makers. Papers that look into this problem include \cite{Psaraftis2010-rt_HP} and \cite{Zis2017-ka_HP,Zis2019-ne_HP}. A multi-commodity network flow formulation in the context of China’s Belt and Road initiative is given by \cite{Qi2022-le_HP}. 

\textit{Operational level problems} concern problems with planning horizons from a few hours to a few days. Among them, a very important class of problems concerns \textit{weather routing} scenarios. The important difference vis-à-vis the ship routing and scheduling problems described earlier is that weather routing problems are typically \textit{path} problems defined as trying to optimise a ship's track from a specified origin to a specified destination, under a prescribed objective and under time varying and maybe also stochastic weather conditions. Decision variables include the selection of the ship’s path and the speeds along the path, and typical objectives include minimum transit time and minimum fuel consumption. Several constraints such as time windows, or constraints to accommodate a feasible envelope on ship motions, vertical and transverse accelerations and ship loads such as shear forces, bending moments and torsional moments can be introduced. The influence of currents, tides, winds and waves, which may be varying in both time and space should be taken into account. See \cite{Perakis1989-zi_HP}, \cite{Lo1998-ix_HP}, and \cite{Zis2020-uk_HP} for some references on this topic.

\textit{Disruption management} is also another important operational level problem class and typically refers to liner shipping. It entails actions that can help the shipping company manage its recovery from possible disruptions of its schedule. Such disruptions may be the result of bad weather, port strikes, equipment malfunction, or more recently, the COVID-19 pandemic that caused massive congestion in many ports worldwide or the Ever Given incident that disrupted traffic in the Suez Canal and the Far East to Europe route in 2021. See \cite{Qi2015-rp_HP} and \cite{Asghari2022-pq_HP} for work in this area.

\textit{Terminal management, berth allocation, and stowage planning} problems also belong to the class of operational level problems, as they deal with an important part of the overall maritime supply chain, that of the coordination between a ship and a port. See \cite{Moccia2006-cl_HP}, \cite{Goodchild2007-rb_HP}, and \cite{Zhen2015-mh_HP} for some related work. 

To conclude, maritime transportation constitutes an important application area for OR, and the related problems are interesting and significant, both from a methodological perspective and from a business and policy perspective. This is so both for traditional economic performance criteria and for environmental criteria, the importance of the latter getting higher in recent years. 

\subsection[Transportation: Aviation (Virginie~Lurkin \& Vikrant~Vaze)]{Transportation: Aviation\protect\footnote{This subsection was written by Virginie~Lurkin and Vikrant~Vaze.}}
\label{sec:Transportation_Aviation}
According to the Air Transport Action Group, in 2019, the world's 1,478 airlines transported 4.5 billion passengers to 3,780 airports, generating 11.3 million direct jobs. Today's airlines are sophisticated businesses making aviation a worldwide economic engine. Yet, aviation is a competitive industry, vulnerable to exogenous shocks, e.g., oil prices, infectious diseases or terrorism. This leads to high costs, and low profit margins, even in the best of times. To tackle these challenges, the industry relies heavily on Operational Research (OR) for decision-making. Prominent OR application domains within aviation include revenue management, airline schedule planning, airline operations recovery, airport flight scheduling, and air traffic flow management. Additionally, some recent OR studies focus on modelling delay propagation through aviation networks

\subsubsection*{Revenue management (RM)}
RM is broadly defined as the strategies and tactics to increase revenues by optimally matching demand for products/services with the available capacity. Seat allocation and pricing are the two main decisions to \textit{control} ticket sales of different fare-classes. Models using capacity allocation as the control variable are called quantity-based RM models. They allocate seats to fare-classes with exogenously determined prices. In contrast, price-based RMs uses pricing policies to maximise revenues. Early RM models focused on overbooking -- the practice of selling more tickets than seats to hedge against cancellations or no-shows. Though various static and dynamic models have been presented since the pioneering work of \cite{rothstein1971airline_VLVV}, airlines mostly use simpler static policies in practice.

Static and dynamic models have been proposed for both single-leg and network-wide seat allocation. Static models optimise seat allocation at a certain time, typically the beginning of the booking period. Dynamic models monitor and adjust to the booking process over time. The earliest static leg-based approach \citep{littlewood1972forecasting_VLVV} considered two fare-classes. \cite{brumelle1990allocation_VLVV} relaxed the assumption of statistical independence between demands. For the multi-class problem, \cite{belobaba1987survey_VLVV} introduced the Expected Marginal Seat Revenue heuristic, a widely used approach in practice. Many studies \citep[e.g.,][]{brumelle1993airline_VLVV} provided optimality conditions for static models, while others developed methods to compute optimal protection levels in the absence of demand information, using optimality conditions \citep{van2000revenue_VLVV} or stochastic approximations \citep{kunnumkal2009stochastic_VLVV}. Dynamic formulations allow time-based controls, but require restrictive demand assumption for tractability, limiting practical impact. Solving network models exactly is computationally hard. Accordingly, most studies on network models use approximations, based on deterministic linear programming \citep{talluriRevenueManagementGeneral2004_AKSJF}, randomised linear programming \citep{talluri1999randomized_VLVV} or decomposition into single-resource problems, as well as solutions using simulation-based optimisation \citep{bertsimas2005simulation_VLVV}. Seat inventory control usually assumes capacity to be fixed, an assumption relaxed by \cite{busing2019capacity_VLVV} integrating capacity uncertainty in leg-based RM. Others integrated inventory control and pricing \citep{you1999dynamic_VLVV}.

Simplest deterministic pricing models are price-sensitive versions of the well-known newsvendor problem \citep{gallego1994optimal_VLVV}. This allows mathematical derivations of optimal prices. Several studies, such as, \cite{feng1995optimal_VLVV}, generalised this problem to include demand dynamics and/or multiple products. Stochastic dynamic programming is a natural way to tackle dynamic pricing. Dynamic models depict reality more accurately, but are harder to solve \citep{gallego1994optimal_VLVV}. Interestingly, solutions to deterministic models are usually good approximations for their stochastic counterparts, and are often used in practice. Traditional RM assumed independent demand, ignoring product substitutability. With the seminal paper of \cite{talluriRevenueManagementGeneral2004_AKSJF}, the RM field has shifted toward including customer choice behaviors within pricing and capacity decisions. \S\ref{sec:Revenue_management} provides a detailed overview of RM concepts and trends beyond aviation.

\subsubsection*{Airline schedule planning (ASP)}
ASP is the process of designing airline schedules maximising profits subject to resource constraints. Taking demand, airport and aircraft characteristics, and maintenance and personal requirements as inputs, ASP outputs selected flight timetables, aircraft schedules and crew duty plans. Most ASP steps typically occur before RM actions and thus constrain the set of decisions available to RM systems. Key ASP steps include fleet planning, route planning, frequency planning, timetable design, fleet assignment, aircraft routing and crew scheduling.  Fleet planning involves decisions regarding purchasing, selling, and leasing of aircraft fleet, while route planning selects airport pairs to operate nonstop flights. Early studies, e.g., \cite{hane1995fleet_VLVV}, matched a predetermined set of flights with aircraft types, developing a fleet assignment model (FAM). The FAM specifically focuses on fleet assignment, which is one particular step within the overall ASP process. The basic FAM, a mixed-integer linear program, minimised costs of operating aircraft and passengers unserved, given passenger demand for individual flight legs. This leg-based approach ignores that passengers often fly on multiple flights in connecting itineraries. 

\cite{barnhart2002itinerary_VLVV} overcame this limitation via an itinerary-based FAM to explicitly model network effects. Some studies developed tractable solution approaches. \cite{barnhart2009airline_VLVV} proposed a subnetwork-based decomposition for capturing FAM's revenue implications, an approach recently extended by \cite{yan2022choice_VLVV} to solve a FAM incorporating passenger choice. Others extended FAM by incorporating incremental timetable design decisions, e.g., changes to flight timings \citep{desaulniers1997daily_VLVV} or selection of optional flights \citep{lohatepanont2004airline_VLVV}. \cite{wei2020airline_VLVV} developed a \textit{clean slate} heuristic optimising entire timetables and fleet assignments under choice-based demand. Frequency planning, which optimises the number of flights operated during a day or part of a day, rather than deciding exact timetables, has also received attention, with an emphasis on capturing affects of competition from other airline and high-speed rail operators \citep[e.g.,][]{cadarso2017integrated_VLVV}.

The last two steps in schedule planning are conceptually similar. Aircraft routing assigns individual aircraft to flights while ensuring that each aircraft undergoes periodic maintenance, and crew scheduling assigns crew to operate flights while satisfying a myriad of crew regulations. Early studies individually optimised aircraft routing \citep{gopalan1998aircraft_VLVV} or crew scheduling \citep{graves1993flight_VLVV}. \cite{lavoie1988new_VLVV} used column generation, an effective solution approach for both problems, to crew scheduling, while \cite{cordeau2001benders_VLVV} used Benders decomposition to jointly solve both problems.

Good schedules not only minimise planned costs, but are also robust to disruptions, to keep the actual costs low. Researchers in early 2000s optimised robustness proxies, e.g., station purity, short cycles, crew swapping opportunities, and crew schedule slack \citep{schaefer2005airline_VLVV}. Later studies directly minimised total planned and unplanned costs of aircraft routing \citep{lan2006planning_VLVV} and crew scheduling \citep{yen2006stochastic_VLVV} separately, and also jointly \citep{dunbar2012robust_VLVV}. Recent studies have used robust optimisation to solve the aircraft routing \citep{yan2018robust_VLVV} and crew scheduling \citep{antunes2019robust_VLVV} problems.

\subsubsection*{Airline operations recovery (AOR)}
AOR encompasses the actions undertaken to repair schedules, when disruptive events such as inclement weather, equipment failures, etc., take place. \cite{rosenberger2003rerouting_VLVV} developed a model and a solution heuristic for repairing aircraft routing, whereas \cite{lettovsky2000airline_VLVV} tackled crew recovery. For the integrated recovery problem, \cite{petersen2012optimization_VLVV} developed a decomposition strategy, while \cite{maher2016solving_VLVV} used column-and-row-generation. Recent recovery studies incorporated other key elements, including flight planning \citep{marla2017integrated_VLVV} and passenger no-shows \citep{cadarso2022passenger_VLVV}.

\subsubsection*{Airport flight scheduling (AFS)}
Beyond airline decision-making, OR is also used to improve decision-making of central authorities and air traffic managers. Research over the past decade demonstrated the potential for enhancing social welfare by constraining schedules at busy airports via slot-control mechanisms \citep{swaroop2012more_VLVV}. Some studies balanced strategic cost of scheduling changes against tactical cost of delays, for a single airport \citep{jacquillat2015integrated_VLVV} or multiple airports \citep{wang2020stochastic_VLVV}. \cite{zografos2012dealing_VLVV} used an integer program for allocating slots to airlines under administrative controls. \cite{fairbrother2020slot_VLVV} attempted to balance the often-conflicting goals of efficiency, equity and the incorporation of airline preferences in optimising slot-scheduling mechanism.

\subsubsection*{Air traffic flow management (ATFM)}
The tactical side of airport and airspace capacity management has received considerable OR attention since the 1990s. ATFM is a broad term used to define key interventions, such as ground holding of airplanes, that ensure safe and efficient flight operations by restricting flow of aircraft into congested airspaces. \cite{terrab1993strategic_VLVV} and \cite{vranas1994multi_VLVV} proposed the single-airport and multi-airport ground holding problems, respectively. The latter was extended to include enroute capacities by \cite{bertsimas1998air_VLVV}. \cite{bertsimas2011integer_VLVV} additionally incorporated flight rerouting and solved larger-scale problems. Adoption of the collaborative decision-making (CDM) paradigm in practice ushered in a new era of research. Advocating increased agency to airlines, \cite{vossen2006slot_VLVV} provided an integer program for slot trading mechanism design under CDM. Recent studies \citep[e.g.,][]{starita2020air_VLVV} are increasingly focused on explicit handling of uncertainty on both demand and capacity side within the ATFM optimisation problems.

\subsubsection*{Modelling delay propagation}
Tightly coupled aviation networks make disruption management particularly challenging. Delays and disruptions in one part of the network propagate to other parts, through aircraft, crew and passenger connections. Recent studies quantified these propagation effects. First, \cite{pyrgiotis2013modelling_VLVV} proposed an analytical queuing and network decomposition model for aircraft-based delay propagation. \cite{barnhart2014modeling_VLVV} presented discrete choice models for passenger itinerary estimation and a reaccommodation heuristic for passenger delay calculations. \cite{wei2018modeling_VLVV} solved inverse optimisation for estimating crew itineraries and crew-based delay propagation. These studies attempted bridging the gap between sparse and aggregate public datasets, and the detailed and disaggregated data needs for aviation OR research.

\subsubsection*{Further reading}
Readers interested in aviation OR are referred to the second edition of the book by \cite{belobaba2015global_VLVV}. In particular, Chapters 4 and 5 focus on pricing and RM, Chapters 8 and 10 on schedule optimisation, robustness and recovery, and Chapter 14 on air traffic management and control. Looking ahead, it is apparent that OR will keep finding natural applications within aviation, especially given the exciting disruptive innovations within urban air mobility. Rapidly growing fields of passenger air taxi operations and drone operations for parcel deliveries are giving rise to new variants of well-known OR problems, e.g., network design \citep{wang2022vertiport_VLVV}, travelling salesperson \citep{roberti2021exact_VLVV}, vehicle routing \citep{dayarian2020same_VLVV}, and facility location \citep{chen2022scalable_VLVV}. 

\subsection[Transportation: Network design (Mike~Hewitt)]{Transportation: Network design\protect\footnote{This subsection was written by Mike~Hewitt.}}
\label{sec:Transportation_Network_design}

In a transportation context, the term \emph{Network Design} \citep{magnanti1984network_MH} generally refers to planning the \emph{supply} side of a transportation system so that it efficiently satisfies some estimate of \emph{demand} within the quality standards of the customers using the system. The planning decisions typically prescribe the movements of vehicles, or convoys (e.g., a railroad train or tug and barges), between stations/terminals in the network to transport people or goods. Network design is typically undertaken for situations wherein what is transported, be it people or goods, is small relative to vehicle capacity. Thus, one primary measure of efficiency is vehicle utilisation, with high utilisation achieved through consolidation. Quality is typically measured based on on-time delivery. 

Network design is relevant to passenger transportation systems such as urban public-transport \citep{Mauttone2021_MH} by bus \citep{ceder1986bus_MH} or light rail \citep{farahani2013review_MH}, as well as systems providing interurban transport by train \citep{hooghiemstra1999decision_MH} or airplane \citep{franke2017network_MH}. It is also relevant to a wide range of goods transportation markets, such as parcel and small-package \citep{barnhart1996air_MH} and less-than-truckload freight \citep{powell/sheffi:1989_MH}. A network design case study for a postal carrier can be found in \cite{winkenbach2016strategic_MH}. Transportation carriers serving these markets may rely on one or more modes, including motor carrier \citep{Bakir2021_MH}, rail \citep{Chouman2021_MH}, ocean \citep{Christiansen2021_MH}, and inland waterway \citep{konings2003network_MH}. The planning of vehicle and goods movements by each mode and synchronisation of goods moving from one mode to the next (e.g., intermodal) can be assisted by network design \citep{arnold2004modelling_MH}.

For different modes the scope of design decisions prescribed by network design models may be broadened in different ways. For example, modes such as rail and inland waterway involve multiple layers of consolidation. For rail \citep{Zhu2014_DC}, goods are consolidated into rail cars, which are then consolidated into blocks that are transported by the same locomotive. For motor carriers, vehicles can not yet move without a driver, whose movements and schedules are restricted by governmental safety regulations and potentially labour management practices that dictate the driver return periodically to a specific physical location in the network (e.g., his/her domicile). Network design models for motor carriers may build schedules for drivers that observe safety regulations \citep{Crainic2018_MH} as well as determine how many drivers should be associated with each physical location \citep{HEWITT2019324_MH}.

The network design problem is typically modelled as a Mixed Integer Program (MIP) formulated on a directed graph \citep{Crainic2021MIP_MH}. Nodes in such a graph model physical locations, potentially at different points in time. Directed edges between such nodes model transportation that begins in one physical location and ends at another. Edges may encode a scheduling dimension, such as when a vehicle departs from one location and arrives at another, that depends in part on the travel time required for the physical move \citep{erera2013improved_MH}. Associated with an edge is a function that maps the amount of vehicle capacity made available on that edge to cost. Typically, it is a step function with each step modelling an increase in capacity due to dispatching an extra vehicle. Commodities model people or goods that are to be transported; associated with each commodity is an origin node, a destination node, and a size. 

The classical network design problem seeks to find a path for each commodity that begins at its origin node, ends at its destination node, and potentially visits one or more intermediate nodes. The problem evaluates these paths with respect to the total cost of capacity made available to support them and seeks to minimise that total cost. Some network design models \citep{Frangioni2021_MH} instead minimise costs that are a function of the amount of goods transported on an edge, as opposed to the capacity made available to transport them. Network design is an optimisation problem that has received significant attention both for its practical relevance and the computational challenges \citep{johnson1978complexity_MH} associated with solving it. 

Most MIP formulations of the network design problem involve commodity flow variables that model the transportation of goods within the network and another set of edge-based variables that model the transportation of vehicles. Typically, commodity flow variables are continuous when a shipper's goods can be divided and routed on multiple paths or binary when they cannot.  Commodity flow variables are typically edge-based, but some models involve paths from shipment origin to shipment destination. The use of a path formulation typically necessitates column generation \citep{HEWITT2019324_MH}. However, unlike the vehicle routing problem, extended, path-based formulations of the network design problem do not provide stronger linear relaxations than compact, arc-based formulations. Depending on the context and mode the vehicle edge variables may either be binary or integer. Linking constraints are included in the formulation to ensure sufficient vehicle capacity travels on an edge to carry the commodities making that transportation move.  Typically, much larger cost coefficients are associated with vehicle edge variables than commodity flow variables.

The majority of literature on network design focuses on deterministic models wherein it is presumed all parameter values (costs, capacities, demands) are known with certainty. However, given that network design models are often solved as part of a tactical planning exercise, uncertainty has been studied \citep{Hewitt2021SND_MH}. Much of that work focuses on uncertainty in commodity sizes and models such problems as two stage stochastic programs wherein vehicle movements are planned in the first stage and commodities are routed in the second stage given the vehicle movements prescribed in the first. There has been limited work on robust optimisation models \citep{Koster2021_MH} or those that view network design in a dynamic context \citep{ALHAJJHASSAN2022102885_MH}. 

Both exact \citep{Crainic2021Exact_MH} and heuristic \citep{Crainic2021Heuristic_MH} solution methods for deterministic network design models have been proposed.  One challenge associated with solving MIP formulations of network design problems is that the linking constraints often lead to fractional vehicle edge variables. Thus, the linear programming relaxations of network design MIPs often yield weak bounds on the objective function value of an optimal solution to the MIP. As a result, much of the literature that focuses on speeding up the solution of MIP formulations of the network design problem focuses on strengthening formulations with \textit{valid inequalities} \citep{NW88_SMPT}. Such inequalities are typically either based on classical ideas such as flow covers from integer programming \citep{gu1999lifted_MH} or leveraging the network structure of the problem \citep{raack2011cut_MH}. Another approach taken to solve network design problems is Benders decomposition \citep{benders2005partitioning_MH,costa2005survey_MH}, particularly when second stage variables are continuous and the optimisation problem resulting from fixing the network design is a linear program.

Another challenge associated with solving MIP formulations of the network design problem is due to the size of the network on which the MIP is formulated  when that network encodes time. The classical approach to representing time in network design is to formulate a MIP on a network wherein multiple nodes represent the same physical location, albeit at different points in time \citep{HewittVuCrainic2016_MH}. Similarly, multiple edges represent the same physical transportation move, albeit at different departure and arrival times. Such networks are typically referred to as time-expanded networks and the overall solution procedure in contexts that require the modelling of time is to construct such a network, formulate a MIP on that network, and then solve that MIP. \cite{BOLAND2019195_MH} study the impact on solution quality of modelling time at different granularities and observe that the finer the representation the higher the quality of the resulting solution. However, such an approach can be computationally challenging when long planning horizons must be modelled or fine representations of time are used, as both cases lead to networks and resulting MIPs that are very large. An alternate approach, called \textit{Dynamic Discretisation Discovery} \citep{DDD2017_MH,EnhancedDDD_MH} proposed to instead generate time-expanded networks in a dynamic and iterative manner.
 
Heuristic methods for deterministic network design models can be classified into one of two categories. The first category focuses on metaheuristics \citep{hussain2019metaheuristic_MH} and  neighbourhood structures. Early heuristics \citep{POWELL1983471_MH} proposed for network design models searched neighbouring solutions by reducing the capacity on one edge in the network and, if necessary, increasing the capacity on another. However, more recent and  effective methods have proposed more complex neighbourhood structures such as cycles or paths \citep{ghamlouche2003cycle_MH}. The second category focuses on what is generally called matheuristics \citep{maniezzo2021matheuristics_MH}. In these heuristics, a neighbourhood of a solution is searched by formulating and solving the MIP of the network design problem, albeit with the values of subsets of variables fixed to their values in the solution at hand \citep{hewitt2010combining_MH}. This is repeatedly done and with different mechanisms used for selecting subsets of variables to fix. 

Similarly, both exact and heuristic solution methods have been proposed for stochastic network design models that take the form of scenario-based two stage stochastic programs. The vast majority of such stochastic programs studied to date involve continuous commodity flow variables in the second stage. As a result, the second stage subproblems are linear programs and the overall stochastic program is amenable to Benders decomposition \citep{Birge2011-ut_HL}. Thus, much of the methodological work on solving such stochastic programs has focused on techniques for speeding up or rendering more impactful different steps in the Benders scheme \citep{magnanti1981accelerating_MH,crainic2021partial_MH}. While Progressive Hedging \citep{RockWetsPH_MH} is an exact method for two stage stochastic programs with continuous variables in both stages, it has been used as the basis of heuristic methods for stochastic network design \citep{crainic2011progressive_MH,crainic2014scenario_MH}. 

\cite{crainic2021network_MH} contains deeper dives into the subjects touched on here as well as discussions of those not discussed. 

\subsection[Transportation: Vehicle routing (Claudia~Archetti \& Maria~Battarra)]{Transportation: Vehicle routing\protect\footnote{This subsection was written by Claudia~Archetti and Maria~Battarra.}}
\label{sec:Transportation_Vehicle_routing}

The Capacitated Vehicle Routing Problem (CVRP) was first proposed by \cite{DantzigR1959_CA_MB}, and named the Truck Dispatching Problem. The goal was that of routing a fleet of identical gasoline delivery trucks from a central depot to service stations (often referred as `customers'). Each truck had to return to the central depot, after visiting an ordered subset of the customers. All customers had to be visited once by a vehicle delivering all their gasoline requirements in the one delivery. The objective was the minimisation of the routing costs, as the sum of the travelling distances of every truck. The CVRP classical definition is the same as that proposed by \cite{DantzigR1959_CA_MB} more than 60 years ago. Introducing a capacitated fleet of vehicles makes the CVRP for a much harder generalisation of the Travelling Salesman Problem \citep{Flood1956_CA_MB}. 
    
The CVRP definition has been enriched over the decades to take into account all the delivery requirements of the customers and of the transportation providers, as well as the characteristics of the available fleet of vehicles, and the increasing availability of technology (i.e., GIS and real time mapping, autonomous vehicles, shared mobility systems and so on). The research literature has flourished with new variants, as well as more  sophisticated  and flexible solution approaches. This chapter aims at providing pointers to key milestones achieved in the last 60 years of the CVRP literature, identifying the latest and most successful exact and metaheuristic algorithms, as well as referencing the most famous online challenges and standard techniques for benchmarking CVRP solution algorithms. 
    
The CVRP `classical' variants and solution approaches are well summarised in  \cite{TothV2002_CA_MB}. 
This book provides key references and definitions for critical application features, as for the CVRP with Time Windows, the CVRP with Backhauls and the CVRP with Pickup and Delivery, the CVRP with vehicle/site dependencies, the CVRP with inventory and the stochastic CVRP. \cite{GoldenRW2008_CA_MB} extends the definition of the classical variants to routing problems with heterogenous fleets, periodic routing problems, split routing problems, dynamic and online routing problems. 
\cite{TV14_SMPT} further widen the remit of application of routing algorithms to maritime applications, disaster relief distribution problems, and considers up-to-date objective functions different than minimising the distance travelled. More recently, fleets of electric vehicles \citep{PelletierJL2016_CA_MB}, problems over time \citep{mor2022vehicle_CA_MB}, drones \citep{OttoACGP2018_CA_MB}, cargo boats \citep{Christiansen2013-cg_HP} and warehouse pickers  \citep{Schiffer2022_CA_MB} have been embedded in routing settings. The new dynamic environment inspired research on stochastic \citep{Gendreau2016_CA_MB}, dynamic \citep{soeffker2021stochastic_CA_MB} and time-dependent \citep{gendreau2015time_CA_MB} routing problems.
    
An up-to-date survey on recent trends can be found in \cite{VIDAL2020401_CA_MB}, in which the CVRP extensions due to richer objective functions, the integration with other optimisation problems, and application-oriented transportation requirements are surveyed. \cite{PartykaH2014_CA_MB} discuss routing algorithms from the practitioners' perspective, and surveys which are the requirements of a logistics company when they acquire a  routing software. 
    
Next the most successful CVRP solution algorithms are summarised, first discussing exact methods. Formulations with a polynomial number of variables and constraints were the first proposed mathematical models, as for the two-commodity formulation  by \cite{laporte1992vehicle_CA_MB} and \cite{Baldacci2Flow2004_CA_MB}. They have the advantage of being easy to use (as they just require encoding in the syntax of the solver). The disadvantage of them however is their poor performance due to high dimension of the formulations, and the weakness of the continuous relaxation. Better results were obtained from formulations with an exponential number of constraints, such as those in which subtour elimination constraints are added dynamically to the formulations in a branch\&cut fashion \citep{Padberg-Rinaldi:1991_IL}. The CVRPSEP library by \cite{LysgaardLE2004_CA_MB} provides separation procedures for subtour elimination constraints, as well as other strengthening additional inequalities. The most successful exact solution framework is up-to-date the branch\&cut\&price \citep{DDS06_ALAL,laporte2009fifty_CA_MB}. This method is based on the Dantzig-Wolfe decomposition \citep{Desrosiers2005_CA_MB}. Binary variables model if a route is used or not in the solution, thus their corresponding set is exponential in size. As a consequence, a restricted set of variables is used to initiate the formulation and only profitable routes are iteratively generated solving a subproblem, called the pricing problem. The CVRP pricing problem is a shortest path with resource constrains, and it is typically solved through dynamic programming \citep{Irnich2005_CA_MB}.  Some of the most relevant milestones in developing branch\&cut\&price algorithms for the CVRP are combining branch\&cut and column generation into the first branch\&cut\&price \citep{Fukasawa2006_CA_MB}, applying bi-directional search in the subproblem \citep{RighiniS2008_CA_MB}, introducing subset row cuts \citep{JepsenPSP2008_CA_MB},   using ng-routes to speed up the subproblem solution \citep{Roberti2011_CA_MB},  using stabilisation techniques for dual values \citep{Gschwind2016_CA_MB,Pessoa2018_CA_MB}, and  proposing primal heuristics based on the restricted master problem \citep{Sadykov2019_CA_MB}. The reader might refer to \cite{desaulniers2002accelerating_CA_MB} for the most widely used acceleration techniques for the solution of the pricing problem.
   
Lately, the work of \cite{PessoaSUV2020_CA_MB} provides an impressive open-source branch\&cut\&price algorithm, based on \cite{Pecin2017_CA_MB}. This algorithm provides state-of-the-art exact solutions for the CVRP and, using a flexible solution representation, for most of the well known routing variants and other sequencing problems. The tool incorporates the algorithmic components previously mentioned, as well as other recent developments \cite[see for example,][]{Sadykov2021_CA_MB}, and compares favourably to other  branch\&cut\&price implementations.  Some of the most powerful exact algorithms for the CVRP, available in different programming languages, are publicly available at \cite{Sadykov2022_CA_MB}.
   
Metaheuristics are  capable of solving very large CVRP instances in limited computing time, however there is no proof of optimality for the solutions found. They are typically initialised with solutions generated by constructive heuristics (the Clarke and Wright is a famous example, \citealp{ClarkeW1964_CA_MB}). Metaheuristics rely heavily on local search procedures to improve the solution quality and intensify the search, and on a metaheuristic framework to obtain a good balance of diversification and intensification \citep{gendreau2010handbook_CA_MB}. In chronological order, popular CVRP frameworks have been the Tabu Search \citep{Cordeau2005_CA_MB}, the Adaptive Large Neighbourhood Search \citep{PISINGER20072403_CA_MB}, the Iterated Local Search \citep{SUBRAMANIAN20132519_CA_MB}, and the Hybrid Genetic algorithm \citep{VIDAL2022105643_CA_MB}. 
The latter two examples of metaheuristic frameworks are particularly relevant to the CVRP literature  due to their high performance,  their flexibility in solving effectively many VRP variants, and because their code had been made publicly available to the research community (the code presented in \citealp{VIDAL2022105643_CA_MB} is, for example, available at \citealp{Vidal2022_CA_MB}). \cite{VIDAL20131_CA_MB} provide a very good summary of the features that make a CVRP metaheuristic successful.

More recently, examples of algorithms producing very high quality solutions for the CVRP have been:\begin{itemize}[noitemsep]
    \item \cite{arnold2019makes_CA_MB}: data mining is used to identify solution features, and these features are used to effectively guide the search algorithms;
    \item \cite{christiaens2020slack_CA_MB}: SISRs is a ruin and recreate algorithm based on an innovative string removal operator;
    \item \cite{QUEIROGA2021105475_CA_MB}: POPMUSIC is a matheuristic that iteratively solves smaller subproblems by means of the branch\&cut\&price by \cite{PessoaSUV2020_CA_MB};
    \item  \cite{AccorsiVigo2021_CA_MB}: FILO is an Iterated Local Search with acceleration techniques and annealing-based neighbour acceptance criteria;
    \item    \cite{MAXIMO20211108_CA_MB}: AILS-PR is an Iterated Local Search metaheuristic hybridised with Path Relinking; and, 
    \item \cite{CavaliereBF2022_CA_MB}: a refinement heuristic using a penalty-based extension of the Lin and Kerninghan heuristic is combined with a restricted column generation to iteratively select meaningful routes.
\end{itemize} 
  
Clear standards have been set by the CVRP community around which benchmark instances should be used for testing the performance of an algorithm, and which are ways of testing a computer code for a fair comparison with other previously proposed algorithms. \cite{UCHOA2017845_CA_MB} discuss the most widely used instances and provides a link to the repository, in which the input data, as well as the best known solutions, are provided and kept up-to-date by the authors. A more recent set of instances and best known solutions is available in \cite{queiroga2022_CA_MB}, where the authors provide data enabling the use of machine learning approaches to solve the CVRP. \cite{ACCORSI2022229_CA_MB} present the standard practices to test CVRP algorithms: how to determine computing time (typically on a single thread), common ways of tuning parameters, and providing best and average solutions on a specified number of executions, among others.
   
Finally, another popular and flourishing avenue for boosting research on the development of effective solutions approaches for the CVRP and variants is represented by competitions.  Some of the most famous CVRP and routing challenges are:
\begin{itemize}[noitemsep]
   \item the DIMACS challenge \citep{DIMACS_CA_MB}, where the goal was to promote research on challenging routing problem variants;
   \item the Amazon Last Mile Routing Research Challenge \citep{AMAZON_CA_MB}, where a specific problem was tackled, namely, the challenge of embedding driver knowledge into route optimisation;
   \item the recently launched EURO Meets NeurIPS 2022 Vehicle Routing Competition \citep{EURO_CA_MB}, with the goal of developing and comparing machine learning techniques for the CVRP.
\end{itemize}

The Vehicle Routing problem has inspired an incredible amount of research. This is due to the challenges it poses when it comes to solving it, to the many variants related to it and to the relevant practical applications. Despite the decades of research efforts and achievements, interest continues to grow mainly thanks to the emerging topics raised by the ever changing application environment. This chapter provides a brief, but hopefully sufficiently comprehensive overview of the techniques, problem variants and emerging trends which will inspire further research.

\clearpage

\section[Conclusions (Daniele~Vigo \& Said~Salhi)]{Conclusions\protect\footnote{This subsection was written by Daniele~Vigo and Said~Salhi.}}
\label{sec:Conclusions}

This encyclopedic article, dedicated to the 75\textsuperscript{th} anniversary of the \textit{Journal of the Operational Research Society}, is made up of an \textit{Introduction} and two distinct though related sections: \textit{Methods} and \textit{Applications}. The introduction section gives an interesting overview of OR with an emphasis on its origin in the UK and highlights the methods and applications that are covered in this paper. A brief summary of the two sections is given below.

In the first main section (\S\ref{sec:methods}), 24 OR-based methods are presented by experts in their respective areas. These methods, which are given in alphabetical order, are concisely described, each starting with the basics, then moving to advanced and contemporary aspects. The authors also pinpoint challenging limitations while highlighting promising research directions.

As OR is rooted in the need to solve decision problems either through optimisation, statistics, visualisation and information technology tools, or through soft system methodologies, we aim to retain this historical flavor in summarising these methods by adopting a simple three-group categorisation.  

The first category covers optimisation-related topics and includes 10 out of the 24 subsections. It ranges from the original optimisation model of linear programming (LP; \S\ref{sec:Linear_programming}) in the late 1940s to its various extensions. One is obtained  by restricting the decision variables to discrete elements including  binary ones (\S\ref{sec:Mixed_integer_programming}), allowing uncertainty in the input (\S\ref{sec:Stochastic_models}), or relaxing the objective function or the constraints not to be  necessarily linear (\S\ref{sec:Nonlinear_programming}). An interesting area that had been dormant for more than 30 years was revived in the late 1970s and early 1980s by studying  a special case of fractional LP which defines relative efficiency and is  known as data envelopment analysis (\S\ref{sec:Data_envelopment_analysis}). Combinatorial optimisation (\S\ref{sec:Combinatorial_optimisation}), a topic that has fascinated and intrigued many mathematicians of the 18\textsuperscript{th} century, seeks an optimal subset or values from a large finite set of elements. These problems can be defined and solved through graphs and networks (\S\ref{sec:Graphs_and_networks}),  some of which are relatively more difficult  than others. To measure the performance of algorithms in terms of time and space complexity, computational complexity (\S\ref{sec:Computational_complexity}) emerged as a solid foundation for distinguishing between classes defined as $\mathcal{P}$ and $\mathcal{NP}$ and studying $\alpha$-approximation algorithms. One methodology  can be traced back to the Ancient Greek times, and is based on the ‘find and discover principle’, now known as ‘heuristic search’ (\S\ref{sec:Heuristics}), which has experienced a phenomenal growth in the late 1980s and early 1990s. This is a major development since  these methodologies provide the best way  to reduce  not only the risk of getting stuck at poor local optima, but also have the power to  yield practical solutions for complex discrete and global optimisation problems that could not have been solved otherwise. A methodology that  is free from restrictions of linearity and convexity is the study of multi-stage process, as given in \S\ref{sec:Dynamic_programming}. 

The next category includes statistics and decision-based tools and also covers 10 of the 24 subsections. For example, business analytics (\S\ref{sec:Business_analytics}), decision analysis (\S\ref{sec:Decision_analysis}) and visualisation (\S\ref{sec:Visualisation}), though they previously existed under different names, have grown significantly while retaining their simplicity. Machine learning, including artificial intelligence (\S\ref{sec:Artificial_intelligence_machine_learning_data_science}), which borrowed its principles from heuristic search and statistics, has taken off very rapidly in teaching, research and applications. This is mainly due to computer power, sophisticated algorithms, freely available  computer languages such as R and Python, and their ability to handle massive amount of data that are now easily available to the public. Other older topics, though still relevant and widely applicable, have also seen a surge in new developments. These include queueing (\S\ref{sec:Queueing}), forecasting (\S\ref{sec:forecasting}), control theory (\S\ref{sec:Control_theory}),  and game theory (\S\ref{sec:Game_theory}). Given the uncertainty and risk involved in many decisions, risk analysis (\S\ref{sec:Risk_analysis}) is evolving fast so as to handle such environments alongside computer simulation (\S\ref{sec:Simulation}), especially discrete event simulation.  The latter, which has a wide spectrum of applications in both the private and public sectors, has recently been enriched by incorporating  multi-objective optimisation within its evaluation component. 

The last category covers the remaining four subsections.  Although some of these research areas existed in other fields such as system engineering in the 1950s, they have become contemporary OR topics especially in the UK in the late 1970s. Soft OR and problem structuring methods (\S\ref{sec:Soft_OR_and_problem_structuring_methods}) question the problem definition and aim to involve stakeholders for a better understanding, with system thinking (\S\ref{sec:Systems_thinking}) analysing the interactions between people, machines and systems  while also questioning the system boundaries.  A related area is system dynamics (\S\ref{sec:Systems_dynamics})  where the dynamism is incorporated throughout and found to suit better applications with limited but plausible scaling. An interesting, though relatively recent OR area, but with a long history rooted in social psychology, is behavioural OR (\S\ref{sec:Behavioural_OR}), where people's behaviour and culture are incorporated into the decision making process. Although the methodologies included in this category usually do not directly aim to solve problems, they can be complementary  to the harder OR techniques.

The second section covers applications that have been, since the very beginning, strongly interconnected with the development of OR methodologies. This section is very rich in examples coming from many fields. For the sake of brevity, we will not refer to each subsection individually but mention just a few. By reading the section it is evident that, on the one hand OR provides appropriate modelling and solution tools to practical problems that  arise in the real world and are nowadays crucial in the design and management of most systems, from healthcare and other public services, to transportation and manufacturing. On the other hand, the complexity and size of practical problems has always stimulated the progress of OR  towards more efficient and flexible techniques which are capable to cope with the challenges posed by the applications. This mutual and virtuous connection is well reflected by the richness of the Applications section of this work. It highlights not only the traditional areas which saw tremendous research efforts and successful implementations, as the traditional fields of transportation, manufacturing, cutting and packing, and inventory management, but also relatively new and interesting sectors such as sports and education.

It is worth noting that in the \textit{Applications} section (\S\ref{sec:applications}), several dimensions of OR impact in the real world clearly emerge. The first one is the broad range of fields to which OR techniques have already successfully been applied and offer an even larger potential yet to be exploited. These range from \textit{vertical} sectors,  such as supply chain management, disaster relief and recovery, or military applications, where a wide array of  problems are defined and solved through appropriate and varied methodologies, to more \textit{horizontal} domains which may impact  several vertical sectors, like vehicle routing or facility location, for which highly specialised methods have been developed. The second dimension is related to the great variety of methodologies applied to the different contexts. These span the whole tool set of OR, including exact and heuristic methods developed to solve specific optimisation problems, to techniques created to handle uncertainty and multi-criteria and, more recently, integrating artificial intelligence methods. Indeed, the great improvements achieved in the last decades in integer and nonlinear programming now allow  to effectively model and solve many problems arising at the operational and tactical levels, where data are more available and reliable. The uncertainty in the data and the modelling typical of strategic decisions are  successfully handled by a variety of methodologies that have proven to be effective in the solution of real applications which are well reviewed in this work. A third very interesting dimension is represented by the development of new broad research perspectives which may have a strong impact in all fields of OR and  are deeply motivated by  applications. An excellent example is the inclusion of fairness and ethics  in  optimisation which, on the one hand allow for considering important issues favouring the acceptability and usability of the results, and on the other hand pose new methodological challenges.

As a general conclusion, thanks to the advances in computer technology, the availability of massive amount of live data, and novel developments, in both optimisation and statistics,   effective optimisation software, powerful machine learning techniques and visualisation tools now exist to solve problems that were considered practically unsolvable just a decade ago.  Applications have always been a main driver for OR development, and the successes achieved increase the appetite for further improvements. 

In the more classical area of exact and heuristic techniques, there is clearly a need to improve the capability of handling efficiently large and very large-scale instances to cope with more complex and demanding scenarios. This  increase in scale is not only generated by the need to solve larger problems, but also to incorporate various steps of the planning processes into integrated and more comprehensive methods. A  field that still deserves further research efforts is the  consideration of uncertainty in OR methods. Important methodological obstacles have yet to be surmounted and there is clearly a need for the development of simple and pragmatic methods, possibly resulting from the integration of artificial intelligence techniques, which can be applied to the solution of large-scale problems arising in  several important application domains. However, it is also worth stressing that these advances, though they are welcome, may  suffer from shortcomings, such as the local optimality trap, biased data,  and impractical assumptions. These hidden aspects could yield poor outcomes on which  academics and practitioners ought to keep an open eye.

\clearpage

\section*{Disclaimer}
The views expressed in this paper are those of the authors and do not necessarily reflect the views of their affiliated institutions and organisations.

\section*{Acknowledgements}
Fotios Petropoulos would like to thank all the co-authors of this article for their very enthusiastic response and participation in this initiave. He is also indebted to his lead advisor for this project, Gilbert Laporte, as well as Christos Vasilakis, Güneş~Erdoğan, Stephen Disney and Maria Battarra for their help and suggestions. Finally, Fotios is grateful to John Boylan and the other Editors-in-Chief of the \textit{Journal of the Operational Research Society} for inviting this paper to be part of the 75\textsuperscript{th} issue of the journal. Fotios dedicates this article to Professor John Boylan: John, your kindness will always be remembered.

Maria Battarra's work reported in this paper was undertaken as part of the Made Smarter Innovation: Centre for People-Led Digitalisation, at the University of Bath, University of Nottingham, and Loughborough University.

David Canca's work was supported by the University of Sevilla, the Regional Government of Andalucia (Spain) and the European Regional Development Fund (ERDF) under grant US-1381656.

Laurent Charlin and Andrea Lodi would like to thank Didier Chételat and Mizu Nishikawa-Toomey for reading and commenting on drafts of their subsection (\S\ref{sec:Artificial_intelligence_machine_learning_data_science}) and the CIFAR AI Chair and the CERC programs for funding.

Salvatore Greco wishes to acknowledge the support of the Ministero dell'Istruzione, dell'Universita e dellaRicerca (MIUR) - PRIN 2017, project ``Multiple Criteria Decision Analysis and Multiple Criteria Decision Theory'', grant 2017CY2NCA.

Katherine Kent and Sam Rose thank Mithu Norris for the help and coordination with \S\ref{sec:Government_and_public_sector} but also Emma Hickman and Ffion Lelii for their contribution.

Silvano Martello, Paolo Toth and Daniele Vigo were supported by Air Force Office of Scientific Research under Grants no. FA8655-20-1-7012, FA8655-20-1-7019, FA9550-17-1-0234 and FA8655-21-1-7046.

Dimitrios Sotiros's work was partially supported by the National Science Center (NCN, Poland) grant no. 2020/37/B/HS4/03125.

Greet~Vanden~Berghe and Sanja~Petrovic acknowledge the advice provided by Andrea Schaerf (University of Udine).

Rafał Weron's work was partially supported by the National Science Center (NCN, Poland) grant no. 2018/30/A/HS4/00444.

\clearpage

\appendix

\section{List of acronyms}\label{sec:acronyms}

\noindent 
1D: One-Dimensional \\
2D: Two-Dimensional \\
2DKP: Two-Dimensional Knapsack Problem \\
2S-SPR: Two-Stage Stochastic Programming with Recourse \\
3D: Three-Dimensional \\
ABM: Agent Based Modelling \\
ABS: Agent Based Simulation \\
ADP: Approximate Dynamic Programming \\
AFS: Airport Flight Scheduling \\
AHD: Attended Home Delivery \\
AHP: Analytic Hierarchy Process \\
AI: Artificial Intelligence \\
ANN: Artificial Neural Network \\
ANT: Actor Network Theory \\
AoA: Activity-on-Arc \\
AoN: Activity-on-Node \\
AOR: Airline Operations Recovery \\
AP: Assignment Problem \\
ARIMA: AutoRegressive Integrated Moving Average (model) \\
AR: Assurance Region \\
AR: Action Research \\
ARIMAX: AutoRegressive Integrated Moving Average with eXogenous variables (model) \\
AS: Autonomous System \\
ASP: Airline Schedule Planning \\
ATFM: Air Traffic Flow Management \\
B2B: Business-To-Business \\
B2C: Business-To-Consumer \\
B\&B: Branch-and-Bound \\
B\&C: Branch-and-Cut \\
B\&P: Branch-and-Price \\
BN: Bayesian Network \\
BOR: Behavioural OR \\
BPP: Bin Packing Problem \\
C\&P: Cutting and Packing \\
CBOR: Community-Based Operations Research \\
CDEA: Centralised DEA \\
CDM: Central Decision Maker or Collaborative Decision-Making \\
CLD: Causal Loop Diagram \\ 
CLSC: Closed-Loop Supply Chains \\
CM: Cellular Manufacturing \\
CNN: Convolutional Neural Network \\
CO: Combinatorial Optimisation \\
CODP: Customer Order decoupling point \\
ConFL: Connected Facility Location Problem \\
COR: Community Operational Research \\
CPM: Critical Path Method \\
CRPS: Continuous Ranked Probability Score \\
CST: Critical Systems Thinking \\
CSW: Common Set of Weights \\
CVaR: Conditional Value at Risk \\
CVRP: Capacitated Vehicle Routing Problem \\
DBN: Dynamic Bayesian Network \\
DC: Distribution Centre \\
DCT: Daily Contact Testing \\
DDF: Directional Distance Function \\
DEA: Data Envelopment Analysis \\
DEF: Deterministic Equivalent Formulation \\
DES: Discrete Event Simulation \\
DfT: Department for Transport \\
DHSC: Department of Health and Social Care \\
DMU: Decision Making Unit \\
DNDEA: Dynamic Network DEA \\
DNN: Deep Neural Network \\
DP: Dynamic Programming \\
DPSIR: Drivers, Pressures, State, Impact and Response \\
DS: Data Science \\
DSS: Decision Support Systems \\
EAT: Efficiency Analysis Trees \\
ED: Emergency Department \\
EMSR: Expected Marginal Seat Revenue \\
EOQ: Economic Order Quantity \\
ERP: Enterprise Resource Planning \\
ESICUP: EURO Special Interest Group on Cutting and Packing \\
EURO: European Operational Research Societies \\
EVP: Expected Value of Possession \\
FAM: Fleet Assignment Model \\
FIFO: First-In-First-Out \\
FMS: Flexible Manufacturing Systems \\
FPTAS: Fully Polynomial-Time Approximation Scheme \\
FSF: Full-State Feedback \\
FSO: Fixed-sum output \\
FTU: Facilities-Transformation-Usage (framework) \\
GIS: Geographic Information Systems \\
GLM: Generalised Linear Model \\
GMB: Group Model Building \\
GNN: Graphical Neural Network \\
GORS: Government Operational Research Service \\
GP: Gaussian Process \\
GPS: Global Positioning System \\
GPU: Graphics Processing Unit \\
GRASP: Greedy Randomised Adaptive Search Procedure \\
HJB: Hamilton-Jacobi-Bellman \\
HMT: His Majesty’s Treasury \\
HL: Humanitarian Logistics \\
HORAF: Heads of OR and Analytics Forum \\
IAM: Integrated Assessment Model \\
ICU: Intensive Care Unit \\
IGP: Interior Gateway Protocol \\
IHIP: Intangibility, Heterogeneity, Inseparability, and Perishability \\
IID: Independently and Identically Distributed \\
INFORMS: Institute for Management Science and Operations Research \\
INRC: International Nurse Rostering Competition \\
ILP: Integer Linear Problem \\
ILP: Integer Linear Programming \\
IoT: Internet of Things \\
IP: Integer Programming \\
IRP: Inventory-Routing Problem \\
JIT-MS: Just-In-Time Material System \\
KP: Knapsack Problem \\
LASSO: Least Absolute Shrinkage and Selection Operator \\
LCSA: Life Cycle Sustainability Assessment \\
LEAR: LASSO-Estimated AutoRegressive (model)\\
LP: Linear Programming \\
LQG: Linear Quadratic Gaussian \\
MAE: Mean Absolute Error \\
MAPE: Mean Absolute Percentage Error \\
MASE: Mean Absolute Scaled Error \\
MAUT: Multi-Attribute Utility Theory \\
MAVT: Multi-Attribute Value Theory \\
MBM: Market Based Measure \\
MC: Maximum Clique or Minimum Cut (problem)\\
MCDA: Multi-Criteria Decision Analysis \\
MCF: Minimum Cost Flow (problem) \\
MDP: Markov Decision Process \\
MF: Maximum Flow (problem) \\
MILP: Mixed-Integer Linear Programming \\
MINLP: Mixed-Integer NonLinear Programming \\
MIMO: Multi-Input-Multi-Output \\
MIP: Mixed-Integer Programming \\
ML: Machine Learning \\
MLPI: Malmquist Luenberger Productivity Indicator \\
MPC: Model Predictive Control \\
MPI: Malmquist Productivity Index \\
MRP: Material Requirement Planning \\
MRP: Multi-level Regression Post-stratification \\
NBEATS: Neural Basis Expansion Analysis for interpretable Time Series forecasting \\
NDEA: Network DEA \\
NDP: Neural Dynamic Programming \\
NFL: National Football League \\
NHS: National Health Service \\
NGO: Non-Governmental Organisation \\
OM: Operations Management \\
ONS: Office for National Statistics \\
OR: Operational (or Operations) Research \\
PA: Portfolio Analysis \\
PATAT: Practice and Theory of Automated Timetabling \\
PCR: Polymerase Chain Reaction \\
PERT: Project Evaluation and Review Technique \\
PESP: Periodic Event Scheduling Problem \\
PID: Proportional Integral Derivative \\
POMDP: Partially Observable Markov Decision Process \\
PPS: Production Possibility Set \\
PRA: Probabilistic Risk Assessment \\
PSM: Problem Structuring Method \\
PTAS: Polynomial-Time Approximation Scheme \\
QRA: Quantile Regression Averaging or Quantitative Risk Assessment \\
R\&D: Research and Development \\
RCPSP: Resource-Constrained Project Scheduling Problem \\
RES: Renewable Energy Sources \\
RFID: Radio-Frequency IDentification \\
RINS: Relaxation-Induced Neighbourhood Search \\
RL: Reinforcement Learning or Reverse Logistics\\
RM: Revenue Management \\
RMSE: Root Mean Squared Error \\
RNN: Recurrent Neural Network \\
SAA: Sample Average Approximation \\
SARF: Social Amplification of Risk Framework \\
SAT: SATisfiability (problem) \\
SCA: Strategic Choice Approach \\
SCM: Supply Chain Management \\
SD: Systems Dynamics \\
SDM: Structured Decision Making \\
SFA: Stochastic Frontier Analysis \\
SI: Systemic Intervention \\
SIS: Schools Infection Survey \\
SISO: Single-Input-Single-Output \\
SODA: Strategic Options Development and Analysis \\
SR: Segment Routing \\
SRCPSP: Stochastic Resource-Constrained Project Scheduling Problem \\
SSM: Soft Systems Methodology \\
SST: Shortest Spanning Trees \\
STP: Steiner Tree Problem (in graphs) \\
SVF: Support Vector Frontiers \\
SVM: Support Vector Machine \\
TE: Traffic Engineering \\
TFP: Total Factor Productivity \\
TPS: Toyota Production System \\
TSP: Travelling Salesman Problem \\
TTP: Travelling Tournament Problem \\
UDE: UnDesirable Effects \\
UFLP: Uncapacitated Facility Location Problem \\
VaR: Value at Risk \\
VAR: Vector AutoRegressive (model) \\
VMI: Vendor Managed Inventory \\
VPP: Virtual Power Plant \\
VRP: Vehicle Routing Problems \\
VSM: Viable Systems Model or Value Stream Map \\
VSS: Value of Stochastic Solution \\
VUCA: Volatile, Uncertain, Complex and Ambiguous \\
WHO: World Health Organisation \\

\clearpage

%% References
\bibliographystyle{elsarticle-harv}
%\bibliographystyle{elsarticle-num}
\bibliography{refs}

\end{document}