%
%


%
%

%
%



\documentclass[a4paper,fleqn]{cas-sc}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[numbers,square,sort&compress]{natbib}
\usepackage{array}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{lineno}
\usepackage{multirow}
%

\usepackage{afterpage}

\usepackage{amsmath, amsfonts, amsthm, latexsym}
\usepackage{algorithm}
\usepackage{algpseudocode}

%
%
%
%
%
%
%
%
%
%
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
\tsc{EP}
\tsc{PMS}
\tsc{BEC}
\tsc{DE}
%

%
%

\makeatletter
\newtheoremstyle{definition}
{3ex}%
{3ex}%
{\upshape}%
{}%
{\bfseries}%
{.}%
{.5em}%
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\makeatother

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{fact}[theorem]{Fact}
\DeclareMathOperator{\Enc}{ENC}
\DeclareMathOperator{\Dec}{DEC}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\tr}{tr}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
%



%
\ExplSyntaxOn
\keys_set:nn { stm / mktitle } { nologo }
\ExplSyntaxOff
%


\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

%

\shortauthors{Brzozowski, Siudem, Gagolewski}

\shorttitle{Community detection in complex networks}
\title[mode = title]{Community detection in complex networks via node similarity, graph representation learning, and~hierarchical clustering}



\author[1]{\L{}ukasz Brzozowski}[orcid=0000-0002-3625-3312]    %
\ead{lukasz.brzozowski@pw.edu.pl}
\cormark[1]
\cortext[cor1]{Corresponding author}
\credit{Conceptualisation, Methodology, Data Curation, Software, Visualisation, Writing -- Original Draft, Revision}  %

\author[2]{Grzegorz Siudem}[orcid=0000-0002-9391-6477]
\ead{grzegorz.siudem@pw.edu.pl}
\ead[URL]{http://if.pw.edu.pl/~siudem}
\credit{Conceptualisation, Methodology, Writing -- Revision}  %

\author[3]{Marek Gagolewski}[orcid=0000-0003-0637-6028]
\ead{m.gagolewski@deakin.edu.au}
\ead[url]{https://www.gagolewski.com}
\credit{Conceptualisation, Methodology, Writing -- Revision}  %





%
\address[1]{Warsaw University of Technology,
Faculty of Mathematics and Information Science,
ul. Koszykowa 75, 00-662 Warsaw, Poland}

\address[2]{Warsaw University of Technology, Faculty of Physics,
ul. Koszykowa 75, 00-662 Warsaw, Poland}

\address[3]{Deakin University, Data to Intelligence Research Centre, School of IT, Geelong, VIC 3220, Australia}

%
%


%
%
%
%
%
%
%



%
%
%


\begin{abstract}
%
%
Community detection is a critical challenge in the analysis of real-world graphs and complex networks, including social, transportation, citation, cybersecurity networks, and food webs. Motivated by many similarities between community detection and clustering in Euclidean spaces, we propose three algorithm frameworks to apply hierarchical clustering methods for community detection in graphs. We show that using our methods, it is possible to apply various linkage-based (single-, complete-, average- linkage, Ward, Genie) clustering algorithms to find communities based on vertex similarity matrices, eigenvector matrices thereof, and Euclidean vector representations of nodes. We convey a comprehensive analysis of choices for each framework, including state-of-the-art graph representation learning algorithms, such as Deep Neural Graph Representation, and a vertex proximity matrix known to yield high-quality results in machine learning -- Positive Pointwise Mutual Information. Overall, we test over a hundred combinations of framework components and show that some -- including Wasserman-Faust and PPMI proximity, DNGR representation -- can compete with algorithms such as state-of-the-art Leiden and Louvain and easily outperform other known community detection algorithms. Notably, our algorithms remain hierarchical and allow the user to specify any number of clusters a priori.
\end{abstract}


\begin{keywords}
community detection \sep
Genie hierarchical clustering \sep
representation learning \sep
deep autoencoders \sep
complex network analysis
\end{keywords}


\maketitle


%
%
%


\section{Introduction}\label{sec:intro}
%


%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

More and more structures become available in the form of graphs and networks. The problem of community detection therein is a long-standing task in data science: a~plethora of solutions have already been developed (see, e.g., \cite{fortunato, su, ZHANG202286, BALTSOU2023354, cd1}). Still, the topic seems far from exhausted \cite{fortunato202220}. What constitutes a community can be understood very broadly \cite{fortunato} Therefore, many relatively different tools will aim to provide us with their own solutions to this task: is a community a group of tightly connected vertices, or is it a group of vertices that play similar roles in the graph's structure? Further, the sole existence of some mathematically definable relation allows us to construct a network in a given environment. However, such networks may be, for example, structured, hierarchical, temporal, evolving, or attributed. The various generalisations of the concept of a graph call for different community detection solutions tailored to each specific context~\cite{fortunato, cd2}.

Researchers in pattern recognition and statistical/machine learning have long known a similar problem under a different name: clustering~\cite{Xu}. At some level, we can say that the only difference between clustering and community detection is the data space considered -- the former is defined on graphs and the latter on other metric spaces, such as the Euclidean one~\cite{Xu}. This similarity resulted in some domain-specific approaches stemming from a common theoretical basis. For example, hierarchical algorithms may be successfully employed to solve both community detection and clustering tasks with little change in the implementation.

However, with the need of more specialised methods~\cite{su}, the approaches developed in the two aforementioned fields began to diverge.

Taking into account the growing popularity of graph machine learning (e.g., \cite{hamilton, DeepWalk, gcn}), which includes graph representation learning specifically,
we wish to help bridge the between-domain gap by exploring new ways of connecting community detection and clustering solutions.

Specifically, in this paper, we develop a framework for applying hierarchical clustering algorithms (e.g., \cite{murtagh,genie, cd3}) to the task of community detection. While perhaps not the newest by itself, the hierarchical approach turns out robust and easily applicable on graphs, because it relies solely on similarity metrics between the pairs of nodes. As the problem of choosing the right measures for our task is crucial, we will perform an extensive comparative analysis, on both synthetic and real-life network examples, of 25 different graph models falling into the three following classes:
\begin{enumerate}
    \item distance matrices obtained by considering various node similarity measure:
        the Wasserman--Faust measure~\cite{wassermanFaust},
        Neighbourhood overlap~\cite{fortunato},
        $K$-step walk probability~\cite{fortunato},
        Katz index~\cite{katz},
        Rooted PageRank~\cite{hope},
        Adamic--Adar index~\cite{adamicAdar},
        Blondel--Gajardo measure~\cite{blondel}, and
        Positive Pointwise Mutual Information~\cite{dngr},
    \item spectral versions of the above,
    \item distances on automatically discovered spaces using
    the representation learning algorithms:
        DeepWalk~\cite{DeepWalk},
        DNGR~\cite{dngr},
        Graph Factorisation~\cite{GF},
        GraRep~\cite{grarep},
        HARP~\cite{HARP},
        HOPE~\cite{hope},
        Laplacian Eigenmaps~\cite{LE}, and
        Node2Vec~\cite{Node2Vec}.
\end{enumerate}
Further, we shall study how the tuning of the underlying parameters affects the quality of clustering. In each case, we will determine the best linkage function. This will also provide us with a good opportunity to consider a new graph extension of the recently proposed Genie algorithm \cite{genie}, whose performance on many benchmark datasets in the Euclidean spaces turned out to be above par \cite{genieclust}.


%
%

\medskip
The paper is set out as follows. In Section~\ref{sec:background},
we introduce the node similarity measures and graph representation learning methods that will serve as graph models in the task at hand. In Section~\ref{sec:methodology}, we present a framework for community detection based on hierarchical clustering algorithms (including Genie). In Section~\ref{sec:results}, we evaluate all the algorithms on synthetic datasets from the stochastic block model in order to determine the best method combinations. These are then carefully evaluated, including on real-world data, in Section~\ref{sec:results-details}. Finally, Section~\ref{sec:final} concludes the article.


%
%
%

\section{Graph models}\label{sec:background}

\subsection{Community detection}

A common way of conducting mathematical research is to find generalisations of the existing concepts to explain them better in connection with the definitions from other fields. In that sense, Network Science is not that different -- why should we define a ``small'' graph, such as a citation network of machine learning papers, when we could define a more general citation network of all academic research? While this approach allows us to work on larger networks, which usually means that we obtain better statistical approximations of some sought values, that also inevitably indicates that our network becomes less homogeneous. In fact, while the whole citation network contains all the domains, we should expect that its subgraph induced by the vertices representing ML should be connected more densely than the whole network on average. Such an induced subgraph is called a \textit{community}~\cite{fortunato}. In general, communities can be found in many naturally occurring networks, e.g., they may be based on shared interests of social media users in a social network~\cite{socialcommunity}, clustering of delays in railway networks \cite{dekker2022} or similarity between protein functions in biological networks~\cite{Girvan}.

Therefore, to properly understand the behaviour of a network, it is essential to detect whether it contains communities and, if so, find which nodes belong to which clusters. However, the above description of a community is far from strict. In fact, there is no universally accepted definition~\cite{fortunato,fortunato202220}. Here we would like to present some metrics that could help define the community.

\begin{definition}
Let $H = (V_H, E_H)$ be an induced subgraph of graph $G = (V_G, E_G)$ and let $n_H = |V_H|$. We say that $e$ is an \textit{internal} edge of $H$ if it connects two vertices in $V_H$, and $e$ is an \textit{inter-cluster} edge of $H$, if exactly one of its ends lie in $V_H$. As the maximum number of edges in $H$ is equal to

\begin{equation}
\binom{n_H}{2} = \frac{n_H (n_H - 1)}{2},
\end{equation}

\noindent
we define the \textit{intra-cluster density} of $H$ as~\cite{fortunato}:

\begin{equation}
\delta_\text{int}(H) = \frac{\text{the number of internal edges of $H$}}{\binom{n_H}{2}},
\end{equation}

\noindent
and the \textit{inter-cluster} density of $H$ as:

\begin{equation}
\delta_\text{ext}(H) = \frac{\text{the number of inter-cluster edges of $H$}}{n_H (n-n_H)}.
\end{equation}
\end{definition}

Intuitively, a community, regarded as an induced subgraph of $G$, should have higher inter-cluster density and lower intra-cluster density than the average link density of the graph $G$. To detect communities, one could look for a partition that either maximises the average value of $\delta_\text{int}$, minimises the average value of $\delta_\text{ext}$, or combines the two; for example, we could maximise the average value of $\delta_\text{int}-\delta_\text{ext}$~\cite{mancoridis}. However, the problem with the algorithms relying simply on the above-defined concepts of cluster density is that the posed tasks are often NP-complete. For example, finding all clusters in $G$ that have intra-cluster density $\delta_\text{int}$ higher than a given threshold $\zeta$ cannot be performed in polynomial time~\cite{gareyjohnson}.

Another issue with the above definitions of cluster density is that we calculate them only relative to the observed network $G$. If $G$ is heterogeneous, it becomes difficult to precisely define the requirements for a subgraph to be a community. For example, we would require a community to have an extremely high density if it lies in a densely connected region of $G$, but we would accept a subgraph with a lower density in a sparse region in $G$. Such differences could lead to problems with proper task definition and unreliable results. On the other hand, we know that the Erdős–Rényi (ER) model~\cite{er} should not contain communities, as the edges are positioned randomly with equal probabilities. Based on that fact, Newman and Girvan introduced a concept of \textit{modularity}~\cite{newmanGirvanModularity}.


\begin{definition}
Let $G = (V, E)$ and let us assume that we have partitioned $G$ into clusters and let $A$ be its adjacency matrix. Let $\tilde{G}$ be a \textit{null model}, i.e., a graph that is structurally similar to $G$ but does not contain communities. Let $\tilde{A}_{i, j} $ be the expected number of edges between $i, j$ in $\tilde{G}$ and let $C_i$ denote the cluster in $G$ containing the vertex $i$. Then, we define a partition quality function called \textit{modularity} as:

\begin{equation}
Q = \frac{1}{2|E|} \sum_{i, j \in V} (A_{i, j} - P_{i, j})\, \delta(C_i, C_j),
\end{equation}

\noindent
where $\delta$ denotes the Kronecker delta.
\end{definition}

Intuitively, given some partition of $G$, modularity compares the observed number of edges in the communities with the expected number of such edges in the null model. Various candidates for the null model have been investigated: the ER model is the simplest of them all~\cite{newmanGirvanModularity}, but we could derive specific null models for particular classes of networks~\cite{nullmodels}. While explicitly maximising modularity is again NP-complete, it has become a popular evaluation metric for community detection algorithms~\cite{fortunato}.



\subsection{Node similarity}\label{sec:nodesim}

In the previous section, we have defined the community to be an induced subgraph, which is, in some sense, connected more strongly than its neighbourhood. However, we could look at the community detection problem more locally -- a community could be a subgraph of \textit{somewhat} similar vertices. In such an approach, we would like the communities to be subgraphs such that the inter-cluster edges connect similar vertices and the intra-cluster edges connect different ones~\cite{fortunato}. Assuming that two vertices are similar if they are connected in the graph, we naturally return to the previous definition. However, the flexibility of similarity allows us to introduce other approaches to community detection. In this section, we will present some selected node similarity measures.

\paragraph{Embedding-based measures~\cite{fortunato}.}
%
Let us assume that we have embedded the graph $G$ in the space $\mathbb{R}^d$ for some $d$, i.e., that we have vector representations $X_v = (x^1_v, x^2_v, \dots, x^d_v) \in \mathbb{R}^d$ for each node $v$ of the graph $G$. Regardless of how the embedding was obtained, we can now easily define the node similarity between two vertices $u$ and $v$ as a distance  between $X_u$ and $X_v$. To the most popular ones belong
the Euclidean, Manhattan, and Chebyshev metric.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
Applying any non-increasing transformation thereon turns them into a similarity function.

Particular graph embedding methods will be discussed in Section~\ref{grl}.

\paragraph{Relationship-based measures~\cite{fortunato,burt}.}
Another class of similarity measures is based on the relationships encoded within the graph. Assuming a graph $G$, the most straightforward function of the similarity between the vertices $u$ and $v$ is $A_{u, v}$, i.e., the element of the graph's adjacency matrix indicating whether or not $u$ and $v$ are adjacent. However, we could also define the similarity between $u$ and $v$ in terms of the structure of their neighbourhoods. The most common similarity measure is based on the Wasserman--Faust distance~\cite{wassermanFaust}:

\begin{equation}
d^{WF}_{u, v} = \sqrt{\sum_{k \neq u, v} (A_{u, k} - A_{v, k})^2}.
\end{equation}

The Wasserman--Faust distance is one of the first measures based on the structural equivalence of the vertices, as it does not rely on the adjacency of $u$ and $v$. Similarly, the Adamic--Adar index~\cite{adamicAdar} also incorporates structural equivalence. It is given by:

\begin{equation}
d^{AA}_{u, v} = \sum_{w \in N(u) \cap N(v)} \frac{2}{\log |N(w)|}.
\end{equation}

The intuition behind the Adamic--Adar index is that the common neighbours having large neighbourhoods are less significant to the similarity between the vertices. Also, another variant of the similarity measure based on neighbourhoods of $u$ and $v$ is the Jaccard, or Overlap, similarity:

\begin{equation}
w_{u, v} = \frac{|N_G(u) \cap N_G(v)|}{|N_G(u) \cup N_G(v)|}.
\end{equation}

%
%
%
%

\paragraph{Walk-based measures.}
In another approach, we could measure similarity between the vertices based on walks in the graph. Intuitively, if many paths connect two vertices, they would seem similar. Fortunately, the adjacency matrix of a graph captures all information about paths. It is known that for an unweighted graph $G$, the element $A_{u, v}^k$ counts the number of paths between the vertices $u, v$ of length exactly $k$~\cite{godsil}. Following that fact, the Katz index matrix~\cite{katz} is defined as:

\begin{equation}
S^\text{Katz} = \sum_{l=1}^{\infty}\beta A^l = \beta \cdot (I - \beta \cdot A)^{-1} \cdot A,
\end{equation}

\noindent
where $\beta$ is a decay parameter, and $I$ denotes the identity matrix. Then, the value $S^\text{Katz}_{u, v}$ gives the walk-based similarity between the vertices $u$ and $v$.

On the other hand, we may be interested in whether the two vertices $u$ and $v$ often co-occur on random walks in the graph. This approach differs slightly from the previous one, as it assumes that the co-occurrence in random walks gives more practical information about the similarity between the two vertices than the Katz index.

Let $D$ and $A$ be the degree and adjacency matrix of the graph, respectively. Then, the $K$-step transition probability between the vertices $u$ and $v$ may be retrieved as~\cite{revesz}:

\begin{equation}
d^K_{i, j} = ((D^{-1}A)^K)_{i, j}.
\end{equation}

Similarly, instead of using unbiased random walks, we could turn to more sophisticated tools like the Rooted PageRank algorithm~\cite{pagerank,hope}. It assumes that we ``surf'' through the vertices and walk to some neighbour with probability $\alpha$ or return to the starting vertex with the probability $1-\alpha$. If $P$ is the transition probability matrix, we get that

\begin{equation}
S^\text{RPR} = \alpha \cdot S^\text{RPR}\cdot P + (1-\alpha)\cdot I.
\end{equation}

\noindent
The recurrence is solved to obtain:

\begin{equation}
S^\text{RPR} = (1-\alpha) (I -\alpha P)^{-1}.
\end{equation}

\noindent
The elements of this matrix correspond to node similarity based on the Rooted PageRank algorithm.

Finally, we can use random walks to estimate the value of Pointwise Mutual Information (PMI)~\cite{pmi} between the vertices. That is, we identify the similarity between $u$ and $v$ as:

\begin{equation}
\text{PMI}(u, v) = \log\frac{p(u, v)}{p(u)p(v)},
\end{equation}

\noindent
where $p(u)$ denotes the probability of the occurrence of $u$ in some random walk of a fixed length. Analogically, $p(u, v)$ denotes the probability that both vertices occur on the same random walk, calculated using powers of a transition matrix of a stochastic random walk on the graph. Usually, as the PMI values may be negative, we use the Positive Pointwise Mutual Information (PPMI), i.e., we set all of the negative elements in the matrix to be equal to $0$.
An example usage may be found in \cite{dngr}.





\subsection{Graph representation learning}\label{grl}

In our work, we will use several algorithms,
which can be loosely divided into the five following categories:

\begin{itemize}
    \item direct optimisation methods: Laplacian Eigenmaps~\cite{LE}, Graph Factorisation~\cite{GF},
    \item Singular Value Decomposition (SVD)~\cite{SVD} methods: GraRep~\cite{grarep}, HOPE~\cite{hope},
    \item random walks methods: DeepWalk~\cite{DeepWalk}, Node2Vec~\cite{Node2Vec}, HARP~\cite{HARP},
    \item autoencoder methods: DNGR~\cite{dngr}, SDNE~\cite{SDNE},
    \item structural similarity methods: GraphWave~\cite{GraphWave}, Struc2Vec~\cite{Struc2Vec}.
\end{itemize}

They have been implemented in the ReLeGy package for Python, see \cite{relegy}.

Here we present a short description of each of the categories and algorithms. For the sake of clarity, throughout this section, we assume that we embed the nodes of a graph $G = (V, E)$ in $\mathbb{R}^d$, i.e., we obtain a matrix $Z \in \mathbb{R}^{|V| \times d}$, where each row corresponds to a single node. We also denote the adjacency matrix of $G$ with $A$, its Laplacian as $L$, and its degree matrix as $D$.


\paragraph{Direct optimisation methods.}
This class of methods generates the graph embedding by solving a specific optimisation task. For example, in the Laplacian Eigenmaps~\cite{LE} algorithm, we select $Z$ such that:

\begin{equation}
Z = \argmin_{Z^TDZ = I} \tr(Z^TLZ),
\end{equation}

\noindent
where $\tr$ is the trace of a matrix. The authors justify that this optimisation task corresponds to a representation where adjacent nodes are possibly close in the resulting space while the non-adjacent nodes lie far from each other. On the other hand, in the Graph Factorisation (GF)~\cite{GF} method, the similarity between the nodes $v_i$ and $v_j$ is approximated with the dot product of the corresponding representations, i.e.,

\begin{equation}
s(v_i, v_j) \approx \langle Z_i, Z_j \rangle.
\end{equation}

\noindent
As such, the GF method aims to capture node adjacency in the product of the representation.


\paragraph{SVD methods.}
The second class of methods uses the Singular Value Decomposition (SVD), which allows for a lower-dimensional approximation of higher-dimensional structures~\cite{SVD}. To obtain the embedding, we utilise the SVD on a matrix representation of a graph and again rely on the dot product $\langle Z_i, Z_j \rangle$ approximation of the similarity. However, neither of the two methods performs the SVD on the adjacency or the Laplacian matrix as, according to the authors, these matrices are incapable of accurately representing node similarity. Thus, GraRep~\cite{grarep} performs the decomposition on a $K$-step transition probability matrix, and HOPE~\cite{hope} allows to use of either the aforementioned Katz Index, Adamic--Adar, Rooted PageRank, or Jaccard similarity matrices. It must be noted that HOPE is a tool developed specifically for directed graphs with asymmetric similarity matrices.


\paragraph{Random walk methods.} The algorithms belonging to this class train the embedding model based on random walks generated on the graph. Their common feature is using a tool developed for Natural Language Processing: the Skip-Gram model~\cite{skipgram}. This design decision can be justified by the similarity of node sequences in a random walk and the word sequences in a sentence. Here, DeepWalk~\cite{DeepWalk} is the most basic but efficient solution, in which first-order Markov walks in a graph are fed to the neural network to generate the embedding. Node2Vec~\cite{Node2Vec} was developed to improve DeepWalk by replacing the walks with biased second-order Markov walks.

Finally, the authors of HARP~\cite{HARP} noticed inefficient weight propagation in the SkipGrap~\cite{skipgram} neural network for large graphs. Therefore, they developed a meta-algorithm where the Skip-Gram model is trained iteratively on a family of larger and larger graphs derived from $G$. The weights corresponding to the nodes are passed between the iterations, improving the final representation. The HARP algorithm may work with either DeepWalk or Node2Vec as the underlying model.


\paragraph{Autoencoder methods.}
These methods use an autoencoder neural network to generate an embedding. Specifically, a matrix representation of the graph $G$ is passed as input to both the neural network and the loss calculation. In that sense, autoencoder methods use a unary encoder in the described encoder-decoder architecture. Then, after the network is trained, the matrix $Z$ is retrieved from the middle of the autoencoder network. The two methods: SDNE~\cite{SDNE} and DNGR~\cite{dngr}, besides slight architecture changes, differ primarily in the input matrix representation. SDNE uses the graph adjacency matrix $A$, and DNGR uses the Positive Pointwise Mutual Information matrix.


\paragraph{Structural similarity methods.}
The last class of methods differs significantly from the others. In the previously described algorithms, each method aimed to capture the node similarity somehow derived from the distance of the nodes in the graph. The solutions belonging to the current class aim to capture some structural similarity, i.e., the similarity of the neighbourhoods of the vertices.
%
Struc2Vec~\cite{Struc2Vec} generates a multigraph capturing the structural similarity of the nodes and then trains a Skip-Gram network similarly to Node2Vec to generate representations. GraphWave~\cite{GraphWave} uses graph signal processing methods on the graph Laplacian and converts the signals to random distributions of energy transferred between nodes.
%



\subsection{Representation learning for community detection}

As the primary goal of our work is to develop a general framework for community detection, we have reviewed recent works that use representation learning tools to solve the task at hand. However, as the domain is relatively young, few representation learning-based algorithms have been introduced so far. We present the most notable  and representative ones below.


\paragraph{vGraph: A generative model for joint community detection
and node representation learning~\cite{vgraph}.}
The presented solution -- vGraph -- is not precisely a representation-based community detection tool but a unified framework performing both community detection and node embedding. The authors heavily rely  on node proximity theory derived for LINE~\cite{LINE} and introduce changes that account for better community detection. We assume that the network may be described with two prior probability distributions:

\begin{itemize}
    \item $p(z|w)$, which represents the community distribution, $z$, for a given node $w$,
    \item $p(c|z)$, which represents the node distribution, $c$, for a given community $z$.
\end{itemize}

Given the above distributions, the authors describe a generative process of the network as drawing edges based on the social context of the neighbours. That is, an edge $(w, c)$ is generated with the probability:

\begin{equation}
p(c|w) = \sum_z p(c|z)p(z|w).
\end{equation}

Given the above, we may estimate the probabilities over the network to then detect communities. The probabilities for each node and community are parametrised with Euclidean representations of the nodes, which connects both tasks. A neural network is then trained to fit the probabilities to the observed network, resulting in both community detection and Euclidean node representations.

According to the authors' results, vGraph can achieve competitive results in terms of modularity maximisation and  state-of-the-art performance when used with supervised learning models for node classification. However, a possible drawback of the solution is that it allows for overlaps between communities, rendering it unsuitable for some use-cases.


\paragraph{Community detection based on graph representation
learning in evolutionary networks~\cite{lcden}.}
The LCDEN algorithm aims to generate Euclidean node representations of temporal networks, i.e., networks whose structure change over time. This trait prohibits us from using standard ``static'' community detection tools as the communities change when the graph evolves. Its authors incorporate the same loss as the Laplacian Eigenmaps method to account for node information. However, to effectively deal with the network evolution, we assume time slices $t$ and the loss function given as:

\begin{equation}
L_1 = 2\tr\Big(Z_t^T L_{t-1} Z_t\Big).
\end{equation}

The authors use a deep sparse autoencoder network instead of performing a direct optimisation. This approach is motivated by the fact that sparse autoencoders may be less vulnerable to overfitting and still achieve satisfactory results for sparse networks due to the smaller volume of the overall structural information. To train the network, the authors employ an SE-based loss function with sparsely constrained parameters:

\begin{equation}
L_2 = \sum \| (x_i' - x_i) \cdot ( x_i (\beta - 1) + 1) \|_2^2,
\end{equation}

\noindent
where $x_i'$ and $x_i$ are the input and output representations of the node $i$, respectively, and $\beta$ is a tunable parameter. The  loss function is then given as:

\begin{equation}
L_{min} = \theta L_1 + \gamma L_2,
\end{equation}

\noindent
where $\theta$ and $\gamma$ are some regularisation parameters.



\paragraph{Structural deep clustering network~\cite{sdcn}.}
Structural Deep Clustering Network (SDCN) is a solution incorporating selected state-of-the-art deep learning frameworks for graph community detection. The primary motivation of the authors was based on two observations: that the autoencoder models tend to capture the possible community structure successfully and that Graph Convolutional Networks~\cite{gcn} outperform many other solutions in graph-related machine learning tasks. Therefore, the authors' idea is to combine the two components into one large solution for community detection. To achieve that, they train a Deep Neural Network Autoencoder (DNN) on the consecutive layers of the GCN. To ensure the coherence of the components, a Dual Self-Supervised Module is introduced, which guides the training of the networks. Finally, the $K$-means clustering is performed on the resulting Euclidean for direct community detection. The reader is referred to the main article for further description of the implemented improvements and technical details. SDCN is currently one of the leading solutions for community detection in large networks, significantly outperforming other solutions, e.g., Variational Graph Autoencoders~\cite{vgae}.

We refer the reader to the recent survey \cite{su} for descriptions of other recent solutions, including deep learning models.




%
%
%


\section{Distance-based graph clustering framework}\label{sec:methodology}



\subsection{Hierarchical clustering}

Hierarchical clustering methods are either divisive or agglomerative. DIANA~\cite{DIANA} is an example of a divisive algorithm: starting from a single cluster $C_1$, we find the data point having maximal average dissimilarity to all the others and move it to a new cluster $C_2$. Then, all points that are now, on average, more similar to $C_2$ than $C_1$ are moved to $C_2$. The algorithm progresses iteratively until a predefined stopping requirement is met.

However, agglomerative algorithms are more popular due to better computational complexity and thus lie at the heart of our paper. Assuming a data set of points in a Euclidean space $S = \{s_1, s_2, \dots, s_n\}$, we first partition $S$ such that each point belongs to its own cluster, i.e., we define a partition consisting of singletons, $C = \{C_1, C_2, \dots, C_n\}$. Then, we merge clusters iteratively until a stop criterion is met: the desired number of clusters was reached.
The merging strategy is what differentiates various algorithms. Denoting $d(s_i, s_j)$ as the selected distance measure from $s_i$ to $s_j$, we merge clusters $C_m$ and $C_n$ if they minimise a selected linkage criterion $D(C_m, C_n)$. The most popular ones include~\cite{murtagh}:

\begin{enumerate}
    \item \textit{single-linkage clustering}

    \begin{equation}
    D(C_m, C_n) = \min\{d(s_i, s_j): s_i \in C_m, s_j \in C_n\}
    \end{equation}

    \item \textit{complete-linkage clustering}

    \begin{equation}
    D(C_m, C_n) = \max\{d(s_i, s_j): s_i \in C_m, s_j \in C_n\}
    \end{equation}

    \item \textit{average-linkage clustering}

    \begin{equation}
    D(C_m, C_n) = \frac{1}{|C_m|\cdot |C_n|} \sum_{s \in C_m} \sum_{t \in C_n} d(s, t)
    \end{equation}

    \item \textit{Ward linkage clustering}

    \begin{equation}
    D(C_m, C_n) = \frac{1}{|C_m|\cdot |C_n|}\sum_{s, t \in C_m \cup C_n} d(s, t)^2
    \end{equation}
\end{enumerate}

It must be noted that hierarchical clustering methods may also be based on graphs. For example, we could generate a graph using $k$-nearest neighbours methods and define a criterion using the degrees of nodes~\cite{graphDegreeLinkage}.

The single-linkage criterion is relatively naive and often does not produce suitable partitions. The remaining ones have higher time complexity. One way to bypass the complexity problem is to add additional requirements. For example, the Genie algorithm~\cite{genie,genieclust} uses a single-linkage criterion but simultaneously monitors the partition's Gini Index value, i.e., a measure of inequality of cluster sizes. Only the cluster of the smallest cardinality can be merged when the algorithm reaches a pre-set threshold of the Gini Index value.


\begin{algorithm}[p!]
\caption{Node dissimilarity-based community detection}\label{alg:alg1}
%
 \textbf{Input:} \\
 $G \text{ -- the input graph}$ \\
 $K \text{ -- the number of communities to detect}$ \\
 $\mathit{Dissimilarity} \text{ -- a method to compute the pairwise node dissimilarity matrix of a graph}$ \\
$\mathit{Clustering} \text{ -- a clustering algorithm}$ \\
%
 \textbf{Output:} \\
 $C \text{ -- the partitioning of the nodes of the graph into communities}$ \\
%
  \textbf{Algorithm:} \\
$D \gets \mathit{Dissimilarity}(G)$\\
$C \gets \mathit{Clustering}(D, K)$ \\
\textbf{return} $C$
\end{algorithm}

\begin{algorithm}[p!]
\caption{Spectral similarity-based community detection}\label{alg:alg2}
%
 \textbf{Input:} \\
 $G \text{ -- the input graph}$ \\
 $K \text{ -- the number of communities to detect}$ \\
 $\mathit{Similarity} \text{ -- a method to compute the pairwise node similarity matrix of a graph}$ \\
$\mathit{Clustering} \text{ -- a clustering algorithm}$ \\
%
 \textbf{Output:} \\
 $C \text{ -- the partitioning of the nodes of the graph into communities}$ \\
%
  \textbf{Algorithm:} \\
%
%
$M \gets \mathit{Similarity}(G)$\\
$Z \gets \text{Matrix consisting of } K \text{ eigenvectors of } M$\\
$D \gets \text{Matrix of pairwise distances (Euclidean) between all rows in }Z$\\
$C \gets \mathit{Clustering}(D, K)$ \\
\textbf{return} $C$
%
\end{algorithm}


\begin{algorithm}[p!]
\caption{Representation-based community detection}\label{alg:alg3}
%
 \textbf{Input:} \\
 $G \text{ -- the input graph}$ \\
 $K \text{ -- the number of communities to detect}$ \\
 $\mathit{Representation} \text{ -- a method to generate the vector representations of the graph nodes}$ \\
$\mathit{Clustering} \text{ -- a clustering algorithm}$ \\
%
 \textbf{Output:} \\
 $C \text{ -- the partitioning of the nodes of the graph into communities}$ \\
%
  \textbf{Algorithm:} \\
%
%
$Z \gets \mathit{Representation}(G)$ \\
$D \gets \text{Matrix of pairwise distances (Euclidean) between all rows in }Z$\\
$C \gets \mathit{Clustering}(D, K)$ \\
\textbf{return} $C$
%
\end{algorithm}

\afterpage{\clearpage}



\subsection{Solution framework}


We focus on the following three main sources from which we can obtain the communities:
\begin{itemize}
    \item the node dissimilarity matrices,
    \item the eigenvectors of node similarity matrices,
    \item the Euclidean representations of the nodes.
\end{itemize}
As each of the above sources requires a slightly different approach to obtain the final community detection, we developed three frameworks suitable for each of the above. We present the frameworks below.


\paragraph{Node dissimilarity-based community detection.}
The great strength of hierarchical algorithms is that they can work directly on the distance matrix of the sample. Therefore, in this approach, we compute the node dissimilarity matrix using one of the previously mentioned methods and generate the clustering based on this matrix. Algorithm~\ref{alg:alg1} presents the outline of the method.





\paragraph{Eigenvector-based community detection.}
In the standard spectral community detection, the clustering is performed on the rows of the matrices consisting of the eigenvectors of the graph's Laplacian matrix. To generalise this approach, we substitute the Laplacian matrix with a node similarity matrix. Algorithm~\ref{alg:alg2} presents the outline of the method.




\paragraph{Representation-based community detection.}
In our final approach, we first generate vector representations of the nodes and then apply the hierarchical clustering directly onto the representations. Algorithm~\ref{alg:alg3} presents the outline of the method.


\paragraph{Components.}
We may notice that all three approaches are greatly modular, i.e., we may use any appropriate algorithm for calculating the similarity and dissimilarity matrices, the clustering, and the node vector representations. Therefore, to develop the best possible final solutions, we have implemented and tested many possible scenarios listed below.

    \begin{itemize}
        \item Similarity and dissimilarity measures\footnote{The measures are obtained from the respective (dis)similarity measure by a composition with a strictly decreasing function, if necessary.}:
        \begin{itemize}
            \item Wasserman--Faust measure~\cite{wassermanFaust},
            \item Neighbourhood overlap~\cite{fortunato},
            \item $K$-step walk probability~\cite{fortunato},
            \item Katz index~\cite{katz},
            \item Rooted PageRank~\cite{hope},
            \item Adamic--Adar index~\cite{adamicAdar},
            \item Blondel--Gajardo measure~\cite{blondel},
            \item Positive Pointwise Mutual Information~\cite{dngr},
        \end{itemize}
        \item Hierarchical clustering algorithms~\cite{murtagh,genie,genieclust}:
        \begin{itemize}
            \item single-linkage clustering,
            \item complete-linkage clustering,
            \item average-linkage clustering,
            \item Ward linkage clustering,
            \item Genie,
        \end{itemize}
        \item Representation learning algorithms:
        \begin{itemize}
            \item DeepWalk~\cite{DeepWalk},
            \item DNGR~\cite{dngr},
            \item Graph Factorisation~\cite{GF},
            \item GraRep~\cite{grarep},
            \item HARP~\cite{HARP},
            \item HOPE~\cite{hope},
            \item Laplacian Eigenmaps~\cite{LE},
            \item Node2Vec~\cite{Node2Vec}.
        \end{itemize}
    \end{itemize}





%
%
%

\section{Parameter tuning}\label{sec:results}

We have noted that the number of possible method-parameter-clustering
algorithm combinations is overwhelming. Therefore, we need a systematic
approach that will enable us to identify the most promising community
detection procedures.


\subsection{Experiment setup}\label{sec:expsetup}

Let us describe the benchmark battery, the assumed method parameters,
as well as the performance metrics that we examine in this section.

\paragraph{Benchmark dataset.}
To compare all the methods, we will exercise a community detection task on a
benchmark battery that consists of graphs generated via the well-known
stochastic block model~\cite{sbm}; see Figure~\ref{fig:sbm} for an illustration.

We generated 10 graphs for each set of parameters on the following grid:

\begin{itemize}
    \item number of clusters: 3, 4, 5, 10,
    \item number of vertices in a cluster: 5, 10, 20,
    \item cluster in-density: 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,
    \item cluster out-density: 0.05, 0.1, 0.2, 0.3,
\end{itemize}


\noindent
In other words, we test each algorithm on $2{,}880$
synthetic graphs. As we will see later, some graphs will turn out not dense
enough (particularly those with few vertices per clusters) to guarantee the
convergence of all the community detection approaches.



\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\linewidth]{alpha0_05.png}
    \includegraphics[width=0.4\linewidth]{alpha0_2.png}

    \caption{\label{fig:sbm}
    Example graphs with three clusters generated
    by the stochastic block model with the cluster out-density $0.05$
    (left subfigure) and $0.2$ (right subfigure).}
\end{figure}


We should note that this benchmark battery consists of very regular graphs.
Therefore, it promotes the community detection methods that specialise in
detecting such network types (e.g., the Louvain algorithm mentioned below). This is more
or less an equivalent of detecting Gaussian blobs in an Euclidean space,
where we know that such algorithms as $k$-means and average or complete linkage
will outperform the more flexible (nonparametric) methods.

Also, the generated graphs are quite small. We must thus stress that
our purpose right now is merely to select the best
model-parameter-clustering algorithm combinations and to inspect their
run times. In the next section, we will run the best performing algorithms on
a few real-world datasets to get a picture from a slightly different angle.





\paragraph{Method parameters.}
Some of the selected similarity measures have additional parameters that must
be set. We assume the following values thereof:

\begin{itemize}
    \item the number of steps in the $K$-step walk probability: 1, 3, 5,
    \item the decay parameter of the Katz index: 0.1, 0.3, 0.5,
    \item the return probability of the Rooted PageRank: 0, 0.1, 0.3,
    \item the approximation limit of the Blondel--Gajardo measure: 1, 10, 100,
    \item the continuation probability of the Positive Pointwise Mutual
    Information measure: 0.7, 0.9, 1.
\end{itemize}


The classical hierarchical clustering algorithms do not have any
modifiable parameters. For the Genie algorithm, we considered the following
values as suggested in \cite{genie}:

\begin{itemize}
    \item threshold $g$ for the Gini index of the cluster sizes: 0.1, 0.3, 0.5.
%
\end{itemize}


Finally, we use the default values of the representation learning algorithms'
parameters, as assumed in the ReLeGy package for Python~\cite{relegy} except:

\begin{itemize}
    \item the output dimension: 6, 12.
\end{itemize}


\paragraph{Evaluation.}
In the current scenario, we know the true community membership of each node
as it is directly implied by the Stochastic Block Model.
Thus, we can use the adjusted Rand score
(AR-index, see~\cite{HubertArabie1985:partitionscomp})
as a measure the clustering quality.
Assuming that $\mathbf{M}$ is a confusion matrix,
where $m_{u,v}$ gives the number of elements in the $u$-th reference cluster
classified by an algorithm as actually members of the $v$-th community,
\begin{equation}
\text{AR-index}(\mathbf{M}) =
\frac{{n \choose 2} \sum_{u=1}^K\sum_{v=1}^K {m_{u,v}\choose 2} - \sum_{u=1}^K {m_{u,\boldsymbol{\cdot}}\choose 2}\sum_{v=1}^K {m_{\boldsymbol{\cdot},v}\choose 2}}
{\frac{1}{2}{n \choose 2}\Big(\sum_{u=1}^K {m_{u,\boldsymbol{\cdot}}\choose 2}+\sum_{v=1}^K {m_{\boldsymbol{\cdot},v}\choose 2}\Big) -\sum_{u=1}^K {m_{u,\boldsymbol{\cdot}}\choose 2}\sum_{v=1}^K {m_{\boldsymbol{\cdot},v}\choose 2}},
\end{equation}
where $m_{u,\boldsymbol{\cdot}}=\sum_{v=1}^{K} m_{u,v}$
and   $m_{\boldsymbol{\cdot},v}=\sum_{u=1}^{K} m_{u,v}$.
The adjusted Rand score attains the values between a~soft lower limit equal to 0 and a hard upper limit equal to 1 -- a partition may yield values slightly lower than 0 if its performance is worse than a purely random assignment.
Naturally, many other measures exists (e.g., \cite{psi,aaa}), but the results they
yield are usually highly correlated.

We shall also record the times to compute each graph model.
Note that on such small graphs, the time to apply the hierarchical clustering
algorithms on an already constructed distance matrix is negligible.




\subsection{Reference algorithms}\label{sec:reference-algos}

As a point of reference, let us inspect the following five quite well-known algorithms:
\begin{itemize}
    \item Greedy Modularity Optimisation~\cite{greedyModularity},
    \item Label Propagation Algorithm~\cite{labelpropagation},
    \item the Leiden algorithm~\cite{leiden},
    \item the Louvain algorithm~\cite{louvain},
    \item spectral community detection~\cite{Filippone}.
\end{itemize}

The overall results for the reference methods are listed in~\autoref{tab:reference_results}.
%

In line with our expectations, the Leiden and Louvain algorithms achieved the best results, as they are considered state-of-the-art methods for community detection for ``regular'' graphs. Greedy Modularity Optimisation was also able to yield good results, albeit significantly worse than the mentioned former methods, while the Label propagation algorithm and spectral community detection performed rather unsatisfactorily. We may notice that all of the results are relatively low when compared with the theoretical maximal value of the Rand score equal to 1 -- this is due to the presence of particularly challenging test examples in our test set. Specifically, graphs with  low both in- and out-density, e.g., in-density equal to 0.4 and out-density equal to 0.3, pose a challenge even for the best community detection algorithms. Therefore, we expect that the average performance of any method will be quite low, and we focus on a pairwise relative comparison of the methods.

When it comes to the run-time comparison, most of the methods generate the community partitioning almost instantaneously, with the spectral community detection being the only exception.
%


\begin{table}[!ht]
    \centering
    \caption{Average ($\pm$ standard deviation) Rand score and computation time of the reference community detection algorithms on the sample of $2{,}880$ test graphs.
%
    }
%

    \begin{tabular}{|l|r|r|}
    \hline
        \textbf{Community detection algorithm} & \textbf{Rand score} & \textbf{Time [s]} \\ \hline
        Greedy Modularity & 0.51$\pm $0.35 & 0.02$\pm $0.04 \\ \hline
        Label propagation & 0.15$\pm $0.26 & 0.00$\pm $0.00 \\ \hline
        Leiden & \textbf{0.66$\pm $0.37} & 0.00$\pm $0.00 \\ \hline
        Louvain & 0.64$\pm $0.37 & 0.01$\pm $0.02 \\ \hline
        Spectral & 0.29$\pm $0.23 & 0.20$\pm $0.22 \\ \hline
    \end{tabular}
    \label{tab:reference_results}
\end{table}



%

%
%
%



\begin{table}[p!]
    \centering
    \caption{Aggregated Rand scores for the considered node dissimilarity-based community detection methods with the Genie clustering. If a certain combination is not reported, it means that the results were unobtainable due to numerical instabilities. ``Parameter'' denotes the dissimilarity-specific parameter, as listed in Section \ref{sec:expsetup}. Only the best-performing combinations, with respect to the dissimilarity parameter, are reported.
    }
%

    \begin{tabular}{l|r|r|rH}
    \hline
        \textbf{Dissimilarity} & \textbf{Gini Index Threshold} & \textbf{Parameter} & \textbf{Rand score} & \textbf{Time [s]} \\ \hline
%
%
%
        Blondel--Gajardo &  0.3 & 100 & 0.02$\pm $0.02 & 0.02$\pm $0.03 \\ \hline
        Blondel--Gajardo &  0.5 & 100 & 0.02$\pm $0.02 & 0.02$\pm $0.03 \\ \hline
%
%
%
        K-step probability &  0.1 & 3 & 0.48$\pm $0.37 & 0.03$\pm $0.05 \\ \hline
        K-step probability &  0.3 & 3 & 0.44$\pm $0.37 & 0.03$\pm $0.05 \\ \hline
        K-step probability &  0.5 & 3 & 0.35$\pm $0.34 & 0.03$\pm $0.05 \\ \hline
%
%
%
        Katz index & 0.1 & 0.1 & 0.30$\pm $0.34 & 0.03$\pm $0.06 \\ \hline
        Katz index & 0.3 & 0.1 & 0.25$\pm $0.30 & 0.03$\pm $0.04 \\ \hline
        Katz index & 0.5 & 0.1 & 0.17$\pm $0.24 & 0.02$\pm $0.04 \\ \hline
%
%
%
        Overlap & 0.1 & - & 0.44$\pm $0.40 & 0.11$\pm $0.19 \\ \hline
        Overlap & 0.3 & - & 0.42$\pm $0.40 & 0.11$\pm $0.19 \\ \hline
        Overlap & 0.5 & - & 0.36$\pm $0.38 & 0.11$\pm $0.18 \\ \hline
%
%
%
        PPMI & 0.1 & 1.0 & 0.50$\pm $0.37 & 0.03$\pm $0.04 \\ \hline
        PPMI & 0.3 & 1.0 & 0.43$\pm $0.36 & 0.03$\pm $0.04 \\ \hline
        PPMI & 0.5 & 1.0 & 0.33$\pm $0.34 & 0.03$\pm $0.04 \\ \hline
        Rooted PageRank & 0.1 & 0.3 & 0.37$\pm $0.29 & 0.12$\pm $0.16 \\ \hline
%
%
%
        Wasserman--Faust & 0.1 & - & \textbf{0.52$\pm $0.39} & 0.13$\pm $0.23 \\ \hline
        Wasserman--Faust & 0.3 & - & 0.47$\pm $0.39 & 0.13$\pm $0.22 \\ \hline
        Wasserman--Faust & 0.5 & - & 0.37$\pm $0.37 & 0.13$\pm $0.23 \\ \hline
    \end{tabular}
    \label{tab:node_genie}
\end{table}


\begin{table}[p!]
    \centering
    \caption{Aggregated Rand scores for  the considered node dissimilarity-based community detection methods with the remaining clustering methods.
    %
    }

%
    \begin{tabular}{l|l|r|rH}
    \hline
        \textbf{Dissimilarity} & \textbf{Clustering method} & \textbf{Parameter} & \textbf{Rand score} & \textbf{Time [s]} \\ \hline
        Adamic--Adar & Average linkage & - & 0.45$\pm $0.43 & 0.01$\pm $0.01 \\ \hline
        Adamic--Adar & Complete linkage & - & 0.35$\pm $0.43 & 0.01$\pm $0.01 \\ \hline
        Adamic--Adar & Single linkage & - & 0.16$\pm $0.32 & 0.04$\pm $0.06 \\ \hline
        Adamic--Adar & Ward linkage & - & 0.53$\pm $0.40 & 0.01$\pm $0.01 \\ \hline
        Blondel--Gajardo & Average linkage & 1 & 0.00$\pm $0.01 & 0.01$\pm $0.01 \\ \hline
        Blondel--Gajardo & Complete linkage & 1 & 0.00$\pm $0.01 & 0.01$\pm $0.01 \\ \hline
        Blondel--Gajardo & Single linkage & 1 & 0.00$\pm $0.00 & 0.01$\pm $0.01 \\ \hline
        Blondel--Gajardo & Ward linkage & 1 & 0.01$\pm $0.02 & 0.01$\pm $0.01 \\ \hline
        K-step probability & Average linkage & 3 & 0.55$\pm $0.37 & 0.01$\pm $0.01 \\ \hline
        K-step probability & Complete linkage & 3 & 0.52$\pm $0.37 & 0.01$\pm $0.01 \\ \hline
        K-step probability & Single linkage & 3 & 0.24$\pm $0.37 & 0.01$\pm $0.01 \\ \hline
        K-step probability & Ward linkage & 3 & 0.61$\pm $0.33 & 0.01$\pm $0.01 \\ \hline
        Katz index & Average linkage & 0.1 & 0.33$\pm $0.36 & 0.01$\pm $0.01 \\ \hline
        Katz index & Complete linkage & 0.1 & 0.34$\pm $0.35 & 0.01$\pm $0.01 \\ \hline
        Katz index & Single linkage & 0.1 & 0.07$\pm $0.19 & 0.01$\pm $0.03 \\ \hline
        Katz index & Ward linkage & 0.1 & 0.42$\pm $0.38 & 0.01$\pm $0.01 \\ \hline
        Overlap & Average linkage & - & 0.50$\pm $0.41 & 0.09$\pm $0.16 \\ \hline
        Overlap & Complete linkage & - & 0.34$\pm $0.43 & 0.09$\pm $0.16 \\ \hline
        Overlap & Single linkage & - & 0.26$\pm $0.39 & 0.09$\pm $0.16 \\ \hline
        Overlap & Ward linkage & - & 0.52$\pm $0.40 & 0.09$\pm $0.16 \\ \hline
        PPMI & Average linkage & 1 & \textbf{0.63$\pm $0.36} & 0.01$\pm $0.01 \\ \hline
        PPMI & Complete linkage & 1 & 0.31$\pm $0.42 & 0.01$\pm $0.01 \\ \hline
        PPMI & Single linkage & 1 & 0.22$\pm $0.35 & 0.01$\pm $0.01 \\ \hline
        PPMI & Ward linkage & 1 & 0.62$\pm $0.36 & 0.01$\pm $0.01 \\ \hline
        Rooted PageRank & Average linkage & 0.3 & 0.58$\pm $0.33 & 0.01$\pm $0.01 \\ \hline
        Rooted PageRank & Complete linkage & 0.3 & 0.54$\pm $0.33 & 0.01$\pm $0.01 \\ \hline
        Rooted PageRank & Single linkage & 0.3 & 0.08$\pm $0.19 & 0.02$\pm $0.04 \\ \hline
        Rooted PageRank & Ward linkage & 0.3 & 0.56$\pm $0.33 & 0.01$\pm $0.01 \\ \hline
        Wasserman--Faust & Average linkage & - & 0.52$\pm $0.41 & 0.11$\pm $0.19 \\ \hline
        Wasserman--Faust & Complete linkage & - & 0.54$\pm $0.39 & 0.11$\pm $0.20 \\ \hline
        Wasserman--Faust & Single linkage & - & 0.23$\pm $0.38 & 0.11$\pm $0.20 \\ \hline
        Wasserman--Faust & Ward linkage & - & 0.60$\pm $0.37 & 0.11$\pm $0.19 \\ \hline
    \end{tabular}
    \label{tab:node_hier}
\end{table}

%


\subsection{Node dissimilarity-based community detection}

Let us proceed with the analysis of the overall results of the node dissimilarity-based methods. Tables~\ref{tab:node_genie} and~\ref{tab:node_hier} present the results for the Genie-based implementations and the remaining hierarchical methods, respectively.

The Wasserman--Faust dissimilarity produces the best results when combined with the Genie clustering, but the PPMI-based dissimilarity and the K-step transition probability-based dissimilarity allow the method to achieve relatively high results as well. Overall, only the Blondel--Gajardo matrix seems unsuitable for the dissimilarity-based community detection, as it results in the same performance as random partitioning.

When it comes to the parameters of the Genie clustering, we observe that all of the methods work better for lower values of the inequity threshold. As our test graphs have equal-sized communities, this is expected -- any noticeable inequity in the cluster sizes indicates an error in the clustering, and the algorithms aim to fix the error during the process by merging minimum communities.

%

%

Analogously, all algorithms have a single parameter value for which they perform the best, regardless of the selected inequity measure and the threshold value. Interestingly, the dissimilarity based on K-step transition probability yields the best results for the lowest value of K, i.e., for the shortest theoretical random walks. This observation suggests that local neighbourhood-sourced dissimilarity conveys the community structure better than the global neighbourhood-sourced one, as would be the case for higher values of K. Similarly, the PPMI-based dissimilarity works best with the continuation probability equal to 1.0, i.e., when a random walk does not randomly return to the source. This result is consistent with intuition, as random returns to the source should only cause undesirable disruptions in conveying the graph's structure.

By looking at the remaining hierarchical clustering algorithms in Table~\ref{tab:node_hier}, we observe that the results are relatively consistent, i.e., methods which performed well with Genie clustering also perform well with linkage criteria. In most cases, the average and Ward linkage criteria generate the best results, which is also consistent with our expectations, as they should capture the community similarities the best. We also observe the high performance of the average linkage with PPMI-based dissimilarity, which yields the best performance amongst all node dissimilarity-based methods. We believe it may compete with the state-of-the-art Louvain and Leiden algorithms.

The time comparison shows that most algorithms can generate results almost as quickly as the reference methods. The only exceptions are the methods based on the Wasserman--Faust  and the Overlap dissimilarity. This is because these are the only measures for which the dissimilarity matrix must be calculated element-by-element. We can obtain the dissimilarity matrix with algebraic transformations of other graph-related matrices in the remaining cases.

%






\begin{table}[p!]
    \centering
        \caption{Aggregated Rand scores for the considered spectral-based community detection methods with Genie clustering.
%
        }
%

    \begin{tabular}{l|r|r|rH}
    \hline
        \textbf{Similarity} & \textbf{Gini Index Threshold} & \textbf{Parameter} & \textbf{Rand score} & \textbf{Time [s]} \\ \hline
        Adamic--Adar & 0.1 & - & -0.02$\pm $0.04 & 1.28$\pm $1.76 \\ \hline
        Adamic--Adar & 0.3 & - & -0.01$\pm $0.03 & 1.55$\pm $2.21 \\ \hline
        Adamic--Adar & 0.5 & - & -0.01$\pm $0.03 & 1.60$\pm $2.15 \\ \hline
        Blondel--Gajardo & 0.1 & 1 & 0.03$\pm $0.06 & 0.10$\pm $0.38 \\ \hline
        Blondel--Gajardo & 0.3 & 1 & 0.02$\pm $0.05 & 0.80$\pm $1.83 \\ \hline
        Blondel--Gajardo & 0.5 & 1 & 0.01$\pm $0.04 & 0.50$\pm $1.25 \\ \hline
        K-step probability & 0.1 & 5 & \textbf{0.20$\pm $0.17} & 1.44$\pm $2.00 \\ \hline
        K-step probability & 0.3 & 5 & 0.18$\pm $0.16 & 1.69$\pm $2.40 \\ \hline
        K-step probability & 0.5 & 5 & 0.13$\pm $0.13 & 1.11$\pm $1.67 \\ \hline
        Katz index & 0.1 & 0.3 & 0.19$\pm $0.32 & 1.03$\pm $1.61 \\ \hline
        Katz index & 0.3 & 0.3 & 0.18$\pm $0.31 & 1.32$\pm $1.93 \\ \hline
        Katz index & 0.5 & 0.3 & 0.15$\pm $0.28 & 1.02$\pm $1.56 \\ \hline
        Overlap & 0.1 & - & 0.00$\pm $0.05 & 2.22$\pm $2.61 \\ \hline
        Overlap & 0.3 & - & 0.00$\pm $0.04 & 0.91$\pm $1.41 \\ \hline
        Overlap & 0.5 & - & 0.00$\pm $0.03 & 0.89$\pm $1.50 \\ \hline
        PPMI & 0.1 & 1.0 & 0.01$\pm $0.06 & 0.65$\pm $1.43 \\ \hline
        PPMI & 0.3 & 1.0 & 0.01$\pm $0.06 & 0.17$\pm $0.63 \\ \hline
        PPMI & 0.5 & 1.0 & 0.01$\pm $0.04 & 0.70$\pm $1.76 \\ \hline
        Rooted PageRank & 0.1 & 0.0 & 0.02$\pm $0.02 & 1.44$\pm $2.22 \\ \hline
        Rooted PageRank & 0.3 & 0.0 & 0.02$\pm $0.02 & 2.03$\pm $2.92 \\ \hline
        Rooted PageRank & 0.5 & 0.0 & 0.02$\pm $0.02 & 1.16$\pm $2.08 \\ \hline
        Wasserman--Faust & 0.1 & - & 0.04$\pm $0.08 & 0.63$\pm $0.82 \\ \hline
        Wasserman--Faust & 0.3 & - & 0.04$\pm $0.07 & 0.63$\pm $0.79 \\ \hline
        Wasserman--Faust & 0.5 & - & 0.03$\pm $0.06 & 0.58$\pm $0.79 \\ \hline
    \end{tabular}
    \label{tab:spectral_genie}
\end{table}

\begin{table}[p!]
    \centering
        \caption{Aggregated Rand scores for the considered spectral-based community detection methods with the remaining clustering methods.
%
        }
%

    \begin{tabular}{l|l|r|rH}
    \hline
        \textbf{Similarity} & \textbf{Clustering method} & \textbf{Parameter} & \textbf{Rand score} & \textbf{Time [s]} \\ \hline
        Adamic--Adar & Average linkage & - & -0.01$\pm $0.03 & 0.00$\pm $0.02 \\ \hline
        Adamic--Adar & Complete linkage & - & -0.02$\pm $0.03 & 0.00$\pm $0.02 \\ \hline
        Adamic--Adar & Single linkage & - & 0.00$\pm $0.01 & 0.00$\pm $0.02 \\ \hline
        Adamic--Adar & Ward linkage & - & -0.02$\pm $0.03 & 0.00$\pm $0.02 \\ \hline
        Blondel--Gajardo & Average linkage & 1 & 0.01$\pm $0.03 & 0.00$\pm $0.02 \\ \hline
        Blondel--Gajardo & Complete linkage & 1 & 0.02$\pm $0.04 & 0.00$\pm $0.02 \\ \hline
        Blondel--Gajardo & Single linkage & 1 & 0.01$\pm $0.02 & 0.00$\pm $0.02 \\ \hline
        Blondel--Gajardo & Ward linkage & 1 & 0.02$\pm $0.04 & 0.00$\pm $0.02 \\ \hline
        K-step probability & Average linkage & 5 & 0.03$\pm $0.06 & 0.00$\pm $0.01 \\ \hline
        K-step probability & Complete linkage & 5 & 0.09$\pm $0.10 & 0.00$\pm $0.01 \\ \hline
        K-step probability & Single linkage & 5 & 0.01$\pm $0.03 & 0.01$\pm $0.01 \\ \hline
        K-step probability & Ward linkage & 5 & 0.16$\pm $0.15 & 0.00$\pm $0.01 \\ \hline
        Katz index & Average linkage & 0.3 & 0.18$\pm $0.32 & 0.00$\pm $0.00 \\ \hline
        Katz index & Complete linkage & 0.3 & 0.17$\pm $0.31 & 0.00$\pm $0.00 \\ \hline
        Katz index & Single linkage & 0.3 & 0.13$\pm $0.27 & 0.00$\pm $0.00 \\ \hline
        Katz index & Ward linkage & 0.3 & \textbf{0.19$\pm $0.32} & 0.00$\pm $0.00 \\ \hline
        Overlap & Average linkage & - & 0.00$\pm $0.02 & 0.18$\pm $0.31 \\ \hline
        Overlap & Complete linkage & - & -0.01$\pm $0.03 & 0.18$\pm $0.30 \\ \hline
        Overlap & Single linkage & - & 0.00$\pm $0.01 & 0.18$\pm $0.31 \\ \hline
        Overlap & Ward linkage & - & -0.01$\pm $0.03 & 0.17$\pm $0.30 \\ \hline
        PPMI & Average linkage & 1.0 & 0.00$\pm $0.02 & 0.00$\pm $0.01 \\ \hline
        PPMI & Complete linkage & 1.0 & 0.00$\pm $0.03 & 0.01$\pm $0.01 \\ \hline
        PPMI & Ward linkage & 1.0 & 0.00$\pm $0.03 & 0.00$\pm $0.01 \\ \hline
        Rooted PageRank & Average linkage & 0.0 & 0.02$\pm $0.02 & 0.00$\pm $0.02 \\ \hline
        Rooted PageRank & Complete linkage & 0.0 & 0.02$\pm $0.02 & 0.00$\pm $0.02 \\ \hline
        Rooted PageRank & Single linkage & 0.0 & 0.02$\pm $0.02 & 0.00$\pm $0.02 \\ \hline
        Rooted PageRank & Ward linkage & 0.0 & 0.02$\pm $0.02 & 0.00$\pm $0.02 \\ \hline
        Wasserman--Faust & Average linkage & - & 0.00$\pm $0.01 & 0.23$\pm $0.40 \\ \hline
        Wasserman--Faust & Complete linkage & - & 0.00$\pm $0.02 & 0.22$\pm $0.39 \\ \hline
        Wasserman--Faust & Single linkage & - & 0.00$\pm $0.01 & 0.23$\pm $0.39 \\ \hline
        Wasserman--Faust & Ward linkage & - & 0.00$\pm $0.03 & 0.22$\pm $0.39 \\ \hline
    \end{tabular}
    \label{tab:spectral_hier}
\end{table}

%



\subsection{Spectral-based community detection}

In this section, we summarise the results of our spectral-based community methods, which are present in Tables~\ref{tab:spectral_genie} and~\ref{tab:spectral_hier}, analogously to the previous section.

None of the methods achieved satisfactory results when compared with our node dissimilarity-based as well as the reference methods. However, this is a somewhat expected phenomenon. When the clustering is performed on the eigenvectors of the graph's Laplacian matrix, the standard spectral community detection has a~proper theoretical justification. Our replacements of the Laplacian matrix do not have the same theoretical justification, but they are still heuristics worth considering.


%
Most methods gave the same performance quality as a random partitioning -- the only exceptions here are the K-step transition probability matrix and the Katz index matrix. More interestingly, the transition matrix's best parameter value changed from 3 to 5. While we cannot directly deduce why such a change occurred or how the random walk length contributes to the final clustering, it implies that K remains a tunable parameter that impacts the final performance. We repeat the same conclusion for the Katz decay parameter, for whom the best decay parameter value also changed from 0.1 to 0.3.

If we were to try to explain the behaviour of the transition probability-based method, we could notice one fundamental similarity to the Laplacian matrix used for the standard spectral community detection: both the K-step probability matrix and the Laplacian matrix are row-stochastic.
While this reasoning has a firm basis, to the best of the authors' knowledge, this is not the case for the Katz index matrix.
%
Nonetheless, we believe that further research into these two methods can be profitable in terms of understanding spectral community detection better and may yield a new well-performing spectral algorithm in the future.

When we compare the computation time of the methods,
%
only the Overlap and Wasserman--Faust similarity-based methods do not produce the results almost instantaneously due to the need for element-by-element computation. Interestingly, the standard Laplacian matrix can also be obtained from the graph's adjacency and degree matrix in linear time with respect to the number of vertices. However, when we measure the computation time of the reference spectral method, it requires time similar to the Wasserman--Faust and Overlap-based methods, which in our case is quadratic with respect to the number of vertices. This observation indicates a possibility for improvement in the reference implementation.





\begin{table}[!t]
    \centering
            \caption{Aggregated Rand score and computation times for the considered representation-based community detection methods with the Genie clustering.
%
        }
%

    \begin{tabular}{l|r|r|r|r}
    \hline
        \textbf{Embedding}  & \textbf{Gini Index Threshold} & \textbf{Dimension} & \textbf{Rand score} & \textbf{Time [s]} \\ \hline
        DNGR     & 0.1 & 6 & \textbf{0.57$\pm $0.39} & \multirow{3}{*}{0.87 -- 17.45} \\
        DNGR     & 0.3 & 6 & 0.54$\pm $0.39 & \\
        DNGR     & 0.5 & 6 & 0.47$\pm $0.39 & \\
        \hline
        DeepWalk & 0.1 & 6 & 0.47$\pm $0.37 & \multirow{3}{*}{0.44 -- 4.85} \\
        DeepWalk & 0.3 & 6 & 0.43$\pm $0.35 & \\
        DeepWalk & 0.5 & 6 & 0.34$\pm $0.32 & \\ \hline
        GraRep   & 0.1 & 6 & 0.41$\pm $0.40 & \multirow{3}{*}{0.00 -- 1.53} \\
        GraRep   & 0.3 & 6 & 0.36$\pm $0.38 & \\
        GraRep   & 0.5 & 6 & 0.26$\pm $0.33 & \\ \hline
        GraphFactorisation  & 0.1 & 12 & 0.41$\pm $0.36 & \multirow{3}{*}{1.18 -- 2.81} \\
        GraphFactorisation  & 0.3 & 12 & 0.36$\pm $0.35 & \\
        GraphFactorisation  & 0.5 & 12 & 0.27$\pm $0.30 & \\ \hline
        HARP     & 0.1 & 6 & 0.41$\pm $0.36 & \multirow{3}{*}{1.27 -- 17.38} \\
        HARP     & 0.3 & 6 & 0.37$\pm $0.34 & \\
        HARP     & 0.5 & 6 & 0.28$\pm $0.31 & \\ \hline
        HOPE     & 0.1 & 6 & 0.30$\pm $0.32 & 0.00 -- 2.81 \\ \hline
        Node2Vec & 0.1 & 6 & 0.47$\pm $0.37 & \multirow{3}{*}{0.48 -- 6.66} \\
        Node2Vec & 0.3 & 6 & 0.43$\pm $0.35 & \\
        Node2Vec & 0.5 & 6 & 0.33$\pm $0.32 & \\ \hline
    \end{tabular}
    \label{tab:euclid_genie}
\end{table}

\begin{table}[t!]
    \centering
            \caption{Aggregated Rand scores and computation times for the considered representation-based community detection methods with the remaining clustering methods.
%
}
%

    \begin{tabular}{l|l|r|r|r}
    \hline
        \textbf{Embedding} & \textbf{Clustering} & \textbf{Dimension} & \textbf{Rand score} & \textbf{Time [s]} \\ \hline
        DNGR & Average linkage & 6 & 0.53$\pm $0.40 & \multirow{4}{*}{0.83 -- 20.82} \\
        DNGR & Complete linkage & 6 & 0.54$\pm $0.38 &\\
        DNGR & Single linkage & 6 & 0.39$\pm $0.41 & \\
        DNGR & Ward linkage & 6 & \textbf{0.59$\pm $0.39} & \\ \hline
        DeepWalk & Average linkage & 6 & 0.42$\pm $0.38 & \multirow{4}{*}{0.42 -- 4.85} \\
        DeepWalk & Complete linkage & 6 & 0.44$\pm $0.35 &\\
        DeepWalk & Single linkage & 6 & 0.20$\pm $0.32 &\\
        DeepWalk & Ward linkage & 6 & 0.51$\pm $0.36 & \\ \hline
        GraRep & Average linkage & 6 & 0.25$\pm $0.37 & \multirow{4}{*}{0.00 -- 1.10}\\
        GraRep & Complete linkage & 6 & 0.31$\pm $0.34 &\\
        GraRep & Single linkage & 6 & 0.13$\pm $0.29 &\\
        GraRep & Ward linkage & 6 & 0.44$\pm $0.40 & \\ \hline
        GraphFactorisation & Average linkage & 12 & 0.28$\pm $0.34 & \multirow{4}{*}{1.17 -- 3.21} \\
        GraphFactorisation & Complete linkage & 12 & 0.34$\pm $0.34 &\\
        GraphFactorisation & Single linkage & 12 & 0.16$\pm $0.28 &\\
        GraphFactorisation & Ward linkage & 12 & 0.41$\pm $0.35 & \\ \hline
        HARP & Average linkage & 6 & 0.35$\pm $0.37 & \multirow{4}{*}{1.25 -- 17.91} \\
        HARP & Complete linkage & 6 & 0.38$\pm $0.34 &\\
        HARP & Single linkage & 6 & 0.17$\pm $0.30 &\\
        HARP & Ward linkage & 6 & 0.44$\pm $0.36 &\\ \hline
        HOPE & Average linkage & 6 & 0.13$\pm $0.20 & \multirow{4}{*}{0.00 -- 1.13} \\
        HOPE & Complete linkage & 12 & 0.14$\pm $0.16 &\\
        HOPE & Single linkage & 6 & 0.06$\pm $0.17 &\\
        HOPE & Ward linkage & 12 & 0.29$\pm $0.31 & \\ \hline
        LaplacianEigenmaps & Average linkage & 6 & 0.04$\pm $0.12 & \multirow{4}{*}{0.18 -- 334.79} \\
        LaplacianEigenmaps & Complete linkage & 6 & 0.06$\pm $0.15 &\\
        LaplacianEigenmaps & Single linkage & 6 & 0.02$\pm $0.09 &\\
        LaplacianEigenmaps & Ward linkage & 6 & 0.08$\pm $0.17 & \\ \hline
        Node2Vec & Average linkage & 6 & 0.45$\pm $0.38 &\multirow{4}{*}{0.48 -- 6.90} \\
        Node2Vec & Complete linkage & 6 & 0.45$\pm $0.35 &\\
        Node2Vec & Single linkage & 6 & 0.21$\pm $0.32 &\\
        Node2Vec & Ward linkage & 6 & 0.52$\pm $0.36 &  \\ \hline
    \end{tabular}
    \label{tab:euclid_hier}
\end{table}





\subsection{Representation-based community detection}

Finally, we consider the representation-based community detection methods. The results are presented in Tables~\ref{tab:euclid_genie} and~\ref{tab:euclid_hier}, analogously to the previous sections.

We observe that DNGR-based representations yield the best results.
DNGR paired with a suitable clustering algorithm is able to achieve only a slightly lower Rand score than the state-of-the-art methods, which we believe is entirely satisfactory.

On the other hand, most of the remaining representation learning algorithms give mediocre results. While they outperformed the Label Propagation Algorithm and spectral clustering, they are far behind the Leiden and Louvain algorithms. Among the well-performing algorithms, almost all worked better with six-dimensional representations, which may be explained by the relatively small size of the test graphs. Higher dimensions could be required for larger graphs, where the representation algorithms cannot correctly encode the pairwise node relations.

Comparing the performance as a function of the Gini index threshold and parameter values, we observe the same dependence as in the previous examples: the algorithms work better for lower threshold values, and the best parameter value is independent of the threshold value.

Regarding the computation times, we observe a somewhat expected dependence -- factorisation-based algorithms (GraRep, HOPE) generate a partitioning almost instantaneously, while the algorithms requiring explicit optimisation of a given task or training of an underlying neural network need much more time.




\section{Detailed evaluation}\label{sec:results-details}

Based on the results presented in the previous section,
we select the following best-performing combinations of methods for further,
more detailed analysis:

\begin{itemize}
    \item \textit{Node Genie PPMI} -- node dissimilarity-based Genie with PPMI-based dissimilarity, the Gini index threshold equal to 0.1, and continuation probability equal to 1,
    \item \textit{Node Genie W-F} -- node dissimilarity-based Genie with Wasserman--Faust measure-based dissimilarity, and the Gini index threshold equal to 0.1,
    \item \textit{Node Average PPMI} -- node dissimilarity-based average linkage with PPMI-based dissimilarity, and continuation probability equal to 1,
    \item \textit{Node Average RPR} -- node dissimilarity-based average linkage with Rooted PageRank-based dissimilarity, and return probability equal to 0.3,
    \item \textit{Spectral Genie K-Step} -- spectral-based Genie with K-step probability based similarity, threshold equal to 0.1, and number of steps equal to 5,
    \item \textit{Spectral Ward Katz} -- spectral-based Ward linkage with Katz index-based similarity, and decay parameter equal to 0.3,
    \item \textit{Euclidean Genie DNGR} -- representation-based Genie with DNGR representation, threshold equal to 0.1, and embedding dimension equal to 6,
    \item \textit{Euclidean Ward DNGR} -- representation-based Ward linkage with DNGR representation, and embedding dimension equal to 6,
    \item \textit{Euclidean Ward Node2Vec} -- representation-based Ward linkage with Node2Vec representation, and embedding dimension equal to 6.
\end{itemize}

We include the spectral-based methods because while they performed relatively poorly, their analysis is rather interesting based on their observed behaviour.
%


Let us compare the nine selected methods against the five reference methods listed in \ref{sec:reference-algos}. Specifically, we study:
\begin{itemize}
    \item the  performance for various values of the in- and out-densities of the clusters,
    \item the performance for various values of the vertex and the cluster number of the graph.
    \item the computation time.
\end{itemize}




\begin{figure}[p!]
    \centering
    \includegraphics[width=0.95\textwidth]{Final_methods_maps.png}
    \caption{Rand scores with respect to in- and out-density for the selected designed methods.}
    \label{fig:in_my_rand}
\end{figure}

\begin{figure}[p!]
    \centering
    \includegraphics[width=0.95\textwidth]{Reference_methods_maps.png}
    \caption{Rand scores with respect to in- and out-density for the selected reference methods.}
    \label{fig:in_ref_rand}
\end{figure}

%


\subsection{Graph density-based comparison}

Firstly, we analyse how our methods behave with respect to various values of in- and out-densities of the clusters in the graph. Figures \ref{fig:in_my_rand} and \ref{fig:in_ref_rand} present the results for the selected and the reference methods, respectively.

All node dissimilarity-based and representation-based algorithms behave according to our expectations. They tend to perform better for higher in-densities and lower out-densities of the clusters. We observe that these methods achieve a near-perfect score in the most uncomplicated cases and then gradually decrease in performance as the task complexity increases. Notably, Node Genie PPMI and Node Genie W-F seem to perform relatively poorly for the edge cases, when both in- and out-densities are low -- the remaining methods result in the Rand score equal to at least 0.6, while these two produce 0.46 and 0.33, respectively.

Overall, we believe that the most noteworthy results are those related to the spectral-based methods. First, the Spectral Ward Katz seems to work in the same manner as the previously mentioned methods but scored significantly worse. On the other hand, Spectral Genie K-step behaves significantly different -- it produces better results when the in-densities are low, and the out-densities are high. Let us stress again, however, that detecting communities with in-densities equal to 0.4 and out-densities equal to 0.3 is exceptionally challenging, as the communities are nearly indistinguishable from their neighbourhood. Therefore, while the Rand score remains low, it is particularly of interest that Spectral Genie K-step outperformed all of the methods, including the reference methods.

Also, looking at the reference methods, we see that the general pattern still holds. The only algorithm that perhaps works somewhat differently is Label Propagation, whose performance seems to depend more firmly on the clusters' out-densities and less on in-densities. Nonetheless, it performed somewhat poorly, so the grounds for these observations may be unreliable.

In general, we believe that our best method, Node Average PPMI, achieves as good performance as the best reference methods, i.e., the Louvain and Leiden algorithms, as the differences are hardly noticeable. We should also note that our method results in a slightly lower standard deviation of the Rand score for best cases, i.e., that our method correctly detects communities more stably.


\begin{figure}[p!]
    \centering
    \includegraphics[width=0.95\textwidth]{Final_methods_vncn.png}
    \caption{Rand scores with respect to the number of clusters and the number of vertices in a single cluster for the selected designed methods.}
    \label{fig:cnvn_my_rand}
\end{figure}

\begin{figure}[p!]
    \centering
    \includegraphics[width=0.95\textwidth]{Reference_methods_vncn.png}
    \caption{Rand scores with respect to the number of clusters and the number of vertices in a single cluster for the selected reference methods.}
    \label{fig:cnvn_ref_rand}
\end{figure}



\subsection{Cluster structure-based comparison}

Secondly, we compare our methods and the reference algorithms with respect to the numbers of  vertices in the clusters and the number of clusters. The results are presented in Figures~\ref{fig:cnvn_my_rand} and \ref{fig:cnvn_ref_rand}, respectively.

Similarly as in the previous case, if we leave out the spectral-based methods, there are configurations for which the algorithms work better in general. Specifically, all these routines tend to fit better to a low number of large communities. This behaviour is consistent with our expectations, as such configurations are intuitively the easiest to detect: there is a low number of sparse regions in the graph separating the communities, and the communities themselves are clearly distinguishable due to their sizes.

We also once again observe that the spectral-based methods behave in a significantly different manner. Spectral Ward Katz seems to give good results only for some values of the number of vertices and the number of clusters. While this is quite challenging to explain without further analysis reliably, we expect that the parameters of the matrix generation seem to impact the method's performance significantly for various parameters of the graph. This behaviour is somewhat discouraging when applying this method to an arbitrary dataset, as the results may vary heavily, and we would have no way to distinguish the best partitioning into communities properly. On the other hand, Spectral Genie K-step once again seems to work better in the edge case, where the remaining algorithms work poorly. Specifically, it obtains an almost acceptable Rand score for a large number of big clusters; that is, it can distinguish large communities even when there are many. In this case, it works as good as, e.g., Node Genie PPMI and is better than Greedy Modularity Optimisation but significantly worse than the best algorithms.

If we compare our best method, Node Average PPMI, with the Leiden and Louvain algorithms, we observe that it performs slightly worse in most cases but obtains the best results for many small clusters. As this is the most challenging case, the results suggest that our method may be preferred for some datasets with many communities.


\subsection{Real-world datasets}

In the last part of the analysis of the results, we compare the performance of our selected methods and reference methods on two previously described datasets, i.e., Zachary's Karate Club~\cite{zachary} and Lusseau's Bottlenose Dolphins~\cite{dolphins}. As the Dolphins dataset may be partitioned into communities in several ways and there is no one agreed-upon partitioning, we use modularity to evaluate the community detection. The results are presented in Table~\ref{tab:real_results}.

When we compare the results on the Karate dataset, we observe that four of our methods significantly outperformed all of the reference methods. It is interesting that the state-of-the-art algorithms, i.e., Louvain and Leiden, could not adequately capture the graph's community structure, while our algorithms did so very satisfactorily. Also, the spectral-based methods worked poorly, which was expected.

Regarding the Dolphins dataset, the reference methods are better in terms of modularity maximisation. Let us note that this does not indicate that they detected the communities objectively better, only that they seem to yield higher overall modularity, which can be used to evaluate community detection when no ground truth is given. Nonetheless, many of our algorithms achieved only a slightly lower value of the measure.

%
%
%
%
%
%
%
%
%


\begin{table}[t!]
    \centering
            \caption{Results of our methods and the reference methods on two real-world datasets. The columns denote the algorithm, Rand score on the Karate task, computation time of the Karate task, modularity on the Dolphins task, and computation time on the Dolphins task, respectively.}
%

    \begin{tabular}{|l||r|r||r|r|}
    \hline
        \textbf{Method} & \textbf{$\text{Rand}_{\text{Karate}}$} & \textbf{$\text{T}_{\text{Karate}}$[s]} & \textbf{$\text{Mod}_{\text{Dolphins}}$} & \textbf{$\text{T}_{\text{Dolphins}}$[s]} \\ \hline
        Node Genie PPMI & 0.77 & 0.01 & 0.49 & 0.02 \\ \hline
        Node Genie W-F & \textbf{0.88} & 0.03 & 0.37 & 0.02 \\ \hline
        Node Average PPMI & 0.77 & 0.00 & 0.49 & 0.01 \\ \hline
        Node Average RPR & 0.77 & 0.00 & 0.38 & 0.01 \\ \hline
        Spectral Genie K-step & 0.40 & 0.00 & -0.05 & 0.11 \\ \hline
        Spectral Ward Katz & -0.01 & 0.00 & 0.24 & 0.01 \\ \hline
        Euclidean Genie DNGR & \textbf{0.88} & 13.86 & 0.48 & 19.15 \\ \hline
        Euclidean Ward DNGR & \textbf{0.88} & 11.28 & 0.48 & 22.14 \\ \hline
        Euclidean Ward Node2Vec & \textbf{0.88} & 1.21 & 0.43 & 1.95 \\ \hline
        \hline
        Leiden & 0.46 & 0.00 & \textbf{0.52} & 0.00 \\ \hline
        Louvain & 0.43 & 0.00 & \textbf{0.52} & 0.00 \\ \hline
        Greedy modularity & 0.57 & 0.00 & 0.50 & 0.01 \\ \hline
        Spectral & 0.77 & 0.05 & 0.38 & 0.10 \\ \hline
        Label propagation & 0.38 & 0.00 & 0.50 & 0.00 \\ \hline
    \end{tabular}
    \label{tab:real_results}
\end{table}




%
%
%

\section{Conclusion}\label{sec:final}

We have developed three general pipelines in which agglomerative hierarchical clustering algorithms may be used for community detection in graphs:
node dissimilarity-, spectral-, and  representation-based approaches. Each of the pipelines is unique in its own way:

\begin{enumerate}
    \item Node dissimilarity-based approach aims to generalise and build upon the structure of the earliest clustering and community detection algorithms.
%
    \item Spectral-based approach tries to experimentally verify whether there is a place for improvement of the standard spectral community detection algorithms.
    \item Representation-based approach uses the newest, ``sophisticated''
    (black-box)     methods based on Graph Representation Learning to propose new ways of performing community detection.
\end{enumerate}

We have tested each pipeline with numerous components and performed a comprehensive analysis of their applicability in the community detection task. To reiterate the most important outcomes of our analysis.

\begin{itemize}
    \item Several methods can achieve similar performance as the current state-of-the-art ``dedicated'' methods, and a majority of our methods significantly outperform older reference algorithms, such as Label Propagation Algorithm or Greedy Modularity Optimisation. Also, there are instances of graphs on which we obtain better results than the current state-of-the-art.
    \item In our analysis, we have confirmed the hypothesis regarding the usability of Graph Representation Learning methods for community detection. Specifically, we have shown that some methods, such as Deep Neural Graph Representation (DNGR), can capture the community structure of synthetic and real-world graphs successfully.
    \item Thanks to developing spectral-based approach, we have observed unexpectedly good performance of selected combinations of tools. We believe that our preliminary results strongly motivate further research into including K-step walk transition matrices in the spectral community detection process. As we observed, there is a strong premise that such a tool, if improved, may constitute a state-of-the-art algorithm for community detection in sparse graphs with sparse communities.
\end{itemize}

%
Much work remains to be done in the future, specifically related to spectral-based methods and their applicability.
%
Nonetheless, we have found a number of new approaches to produce competitive results to the current state-of-the-art while remaining hierarchical and allowing the user to pre-specify the number of clusters.

%

%



%
%
%
%
%
%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
%


\section*{Conflict of interest}

All authors certify that they have no affiliations with or involvement in any
organisation or entity with any financial interest or non-financial interest
in the subject matter or materials discussed in this manuscript.




%
%


\section*{Acknowledgements}

This research was supported by the Australian Research Council Discovery
Project ARC DP210100227 (MG).
%

The research was carried out with the support of the Laboratory of Bioinformatics and Computational Genomics and the High Performance Computing Center of the Faculty of Mathematics and Information Science Warsaw University of Technology under computational grant number A-21-04.

ŁB would like to thank Kacper Siemaszko and Paweł Rzążewski for stimulating discussion.

%
%

\printcredits

%
%

%

%
%
%

\begin{thebibliography}{63}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\href}[2]{#2}
\providecommand{\path}[1]{#1}
\providecommand{\DOIprefix}{doi:}
\providecommand{\ArXivprefix}{arXiv:}
\providecommand{\URLprefix}{URL: }
\providecommand{\Pubmedprefix}{pmid:}
\providecommand{\doi}[1]{\href{http://dx.doi.org/#1}{\path{#1}}}
\providecommand{\Pubmed}[1]{\href{pmid:#1}{\path{#1}}}
\providecommand{\bibinfo}[2]{#2}
\ifx\xfnm\relax \def\xfnm[#1]{\unskip,\space#1}\fi
%
\bibitem[{Adamic and Adar(2003)}]{adamicAdar}
\bibinfo{author}{Adamic, L.A.}, \bibinfo{author}{Adar, E.},
  \bibinfo{year}{2003}.
\newblock \bibinfo{title}{Friends and neighbors on the web}.
\newblock \bibinfo{journal}{Social Networks} \bibinfo{volume}{25},
  \bibinfo{pages}{211--230}.
%
\bibitem[{Ahmed et~al.(2013)Ahmed, Shervashidze, Narayanamurthy, Josifovski and
  Smola}]{GF}
\bibinfo{author}{Ahmed, A.}, \bibinfo{author}{Shervashidze, N.},
  \bibinfo{author}{Narayanamurthy, S.M.}, \bibinfo{author}{Josifovski, V.},
  \bibinfo{author}{Smola, A.J.}, \bibinfo{year}{2013}.
\newblock \bibinfo{title}{Distributed large-scale natural graph factorization},
  in: \bibinfo{booktitle}{WWW '13: Proceedings of the 22nd international
  conference on World Wide Web}, \bibinfo{publisher}{International World Wide
  Web Conferences Steering Committee / {ACM}}. pp. \bibinfo{pages}{37--48}.
%
\bibitem[{Baltsou et~al.(2023)Baltsou, Gounaris, Papadopoulos and
  Tsichlas}]{BALTSOU2023354}
\bibinfo{author}{Baltsou, G.}, \bibinfo{author}{Gounaris, A.},
  \bibinfo{author}{Papadopoulos, A.N.}, \bibinfo{author}{Tsichlas, K.},
  \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Explaining causality of node (non-)participation in
  network communities}.
\newblock \bibinfo{journal}{Information Sciences} \bibinfo{volume}{621},
  \bibinfo{pages}{354--370}.
\newblock \URLprefix
  \url{https://www.sciencedirect.com/science/article/pii/S0020025522013792},
  \DOIprefix\doi{https://doi.org/10.1016/j.ins.2022.11.088}.
%
\bibitem[{Belkin and Niyogi(2001)}]{LE}
\bibinfo{author}{Belkin, M.}, \bibinfo{author}{Niyogi, P.},
  \bibinfo{year}{2001}.
\newblock \bibinfo{title}{Laplacian eigenmaps and spectral techniques for
  embedding and clustering}, in: \bibinfo{booktitle}{NIPS'01: Proceedings of
  the 14th International Conference on Neural Information Processing Systems:
  Natural and Synthetic}, \bibinfo{publisher}{{MIT} Press}. pp.
  \bibinfo{pages}{585--591}.
%
\bibitem[{Blondel et~al.(2004)Blondel, Gajardo, Heymans, Senellart and
  Van~Dooren}]{blondel}
\bibinfo{author}{Blondel, V.}, \bibinfo{author}{Gajardo, A.},
  \bibinfo{author}{Heymans, M.}, \bibinfo{author}{Senellart, P.},
  \bibinfo{author}{Van~Dooren, P.}, \bibinfo{year}{2004}.
\newblock \bibinfo{title}{A measure of similarity between graph vertices:
  Applications to synonym extraction and web searching}.
\newblock \bibinfo{journal}{SIAM Review} \bibinfo{volume}{46},
  \bibinfo{pages}{647--666}.
\newblock \DOIprefix\doi{10.2307/20453570}.
%
\bibitem[{Blondel et~al.(2008)Blondel, Guillaume, Lambiotte and
  Lefebvre}]{louvain}
\bibinfo{author}{Blondel, V.D.}, \bibinfo{author}{Guillaume, J.L.},
  \bibinfo{author}{Lambiotte, R.}, \bibinfo{author}{Lefebvre, E.},
  \bibinfo{year}{2008}.
\newblock \bibinfo{title}{Fast unfolding of communities in large networks}.
\newblock \bibinfo{journal}{Journal of Statistical Mechanics: Theory and
  Experiment} \bibinfo{volume}{2008}, \bibinfo{pages}{P10008}.
%
\bibitem[{Bo et~al.(2020)Bo, Wang, Shi, Zhu, Lu and Cui}]{sdcn}
\bibinfo{author}{Bo, D.}, \bibinfo{author}{Wang, X.}, \bibinfo{author}{Shi,
  C.}, \bibinfo{author}{Zhu, M.}, \bibinfo{author}{Lu, E.},
  \bibinfo{author}{Cui, P.}, \bibinfo{year}{2020}.
\newblock \bibinfo{title}{Structural deep clustering network}, in:
  \bibinfo{booktitle}{WWW '20: Proceedings of The Web Conference 2020},
  \bibinfo{publisher}{International World Wide Web Conferences Steering
  Committee / {ACM}}. pp. \bibinfo{pages}{1400--1410}.
%
\bibitem[{Brzozowski and Siemaszko(2021)}]{relegy}
\bibinfo{author}{Brzozowski, L.}, \bibinfo{author}{Siemaszko, K.},
  \bibinfo{year}{2021}.
\newblock \bibinfo{title}{{ReLeGy} -- {R}epresentation learning of graphs in
  {P}ython}.
\newblock \URLprefix \url{https://github.com/lukaszbrzozowski/ReLeGy}.
%
\bibitem[{Burt(1976)}]{burt}
\bibinfo{author}{Burt, R.S.}, \bibinfo{year}{1976}.
\newblock \bibinfo{title}{Positions in networks}.
\newblock \bibinfo{journal}{Social Forces} \bibinfo{volume}{55},
  \bibinfo{pages}{93--122}.
%
\bibitem[{Cao et~al.(2015)Cao, Lu and Xu}]{grarep}
\bibinfo{author}{Cao, S.}, \bibinfo{author}{Lu, W.}, \bibinfo{author}{Xu, Q.},
  \bibinfo{year}{2015}.
\newblock \bibinfo{title}{Grarep: Learning graph representations with global
  structural information}, in: \bibinfo{booktitle}{CIKM '15: Proceedings of the
  24th ACM International on Conference on Information and Knowledge
  Management}, \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{891--900}.
%
\bibitem[{Cao et~al.(2016)Cao, Lu and Xu}]{dngr}
\bibinfo{author}{Cao, S.}, \bibinfo{author}{Lu, W.}, \bibinfo{author}{Xu, Q.},
  \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Deep neural networks for learning graph
  representations}, in: \bibinfo{booktitle}{AAAI'16: Proceedings of the
  Thirtieth AAAI Conference on Artificial Intelligence},
  \bibinfo{publisher}{{AAAI} Press}. pp. \bibinfo{pages}{1145--1152}.
%
\bibitem[{Chen et~al.(2021)Chen, Nie, Wang, Kong, Wang and Huang}]{lcden}
\bibinfo{author}{Chen, D.}, \bibinfo{author}{Nie, M.}, \bibinfo{author}{Wang,
  J.}, \bibinfo{author}{Kong, Y.}, \bibinfo{author}{Wang, D.},
  \bibinfo{author}{Huang, X.}, \bibinfo{year}{2021}.
\newblock \bibinfo{title}{Community detection based on graph representation
  learning in evolutionary networks}.
\newblock \bibinfo{journal}{Applied Sciences} \bibinfo{volume}{11}.
%
\bibitem[{Chen et~al.(2018)Chen, Perozzi, Hu and Skiena}]{HARP}
\bibinfo{author}{Chen, H.}, \bibinfo{author}{Perozzi, B.}, \bibinfo{author}{Hu,
  Y.}, \bibinfo{author}{Skiena, S.}, \bibinfo{year}{2018}.
\newblock \bibinfo{title}{Harp: Hierarchical representation learning for
  networks}, in: \bibinfo{booktitle}{AIES '18: Proceedings of the 2018 AAAI/ACM
  Conference on AI, Ethics, and Society}, \bibinfo{publisher}{{AAAI} Press}.
  pp. \bibinfo{pages}{2127--2134}.
%
\bibitem[{Chen et~al.(2022)Chen, Yu, Yang and Shao}]{cd2}
\bibinfo{author}{Chen, H.}, \bibinfo{author}{Yu, Z.}, \bibinfo{author}{Yang,
  Q.}, \bibinfo{author}{Shao, J.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Community detection in subspace of attribute}.
\newblock \bibinfo{journal}{Information Sciences} \bibinfo{volume}{602},
  \bibinfo{pages}{220--235}.
\newblock \URLprefix
  \url{https://www.sciencedirect.com/science/article/pii/S0020025522003991},
  \DOIprefix\doi{https://doi.org/10.1016/j.ins.2022.04.047}.
%
\bibitem[{Church and Hanks(1989)}]{pmi}
\bibinfo{author}{Church, K.W.}, \bibinfo{author}{Hanks, P.},
  \bibinfo{year}{1989}.
\newblock \bibinfo{title}{Word association norms, mutual information and
  lexicography}, in: \bibinfo{booktitle}{ACL '89: Proceedings of the 27th
  annual meeting on Association for Computational Linguistics},
  \bibinfo{publisher}{{ACL}}. pp. \bibinfo{pages}{76--83}.
%
\bibitem[{Clauset et~al.(2004)Clauset, Newman and Moore}]{greedyModularity}
\bibinfo{author}{Clauset, A.}, \bibinfo{author}{Newman, M.E.J.},
  \bibinfo{author}{Moore, C.}, \bibinfo{year}{2004}.
\newblock \bibinfo{title}{Finding community structure in very large networks}.
\newblock \bibinfo{journal}{Phys. Rev. E} \bibinfo{volume}{70},
  \bibinfo{pages}{066111}.
\newblock \DOIprefix\doi{10.1103/PhysRevE.70.066111}.
%
\bibitem[{Cordasco and Gargano(2011)}]{labelpropagation}
\bibinfo{author}{Cordasco, G.}, \bibinfo{author}{Gargano, L.},
  \bibinfo{year}{2011}.
\newblock \bibinfo{title}{Community detection via semi-synchronous label
  propagation algorithms}.
\newblock \bibinfo{journal}{2010 IEEE International Workshop on Business
  Applications of Social Network Analysis, BASNA 2010} , \bibinfo{pages}{1--8}.
%
\bibitem[{Dekker et~al.(2022)Dekker, Medvedev, Rombouts, Siudem and
  Tupikina}]{dekker2022}
\bibinfo{author}{Dekker, M.M.}, \bibinfo{author}{Medvedev, A.N.},
  \bibinfo{author}{Rombouts, J.}, \bibinfo{author}{Siudem, G.},
  \bibinfo{author}{Tupikina, L.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Modelling railway delay propagation as diffusion-like
  spreading}.
\newblock \bibinfo{journal}{EPJ Data Sci.} \bibinfo{volume}{11},
  \bibinfo{pages}{44}.
\newblock \URLprefix \url{https://doi.org/10.1140/epjds/s13688-022-00359-1},
  \DOIprefix\doi{10.1140/epjds/s13688-022-00359-1}.
%
\bibitem[{Donnat et~al.(2018)Donnat, Zitnik, Hallac and Leskovec}]{GraphWave}
\bibinfo{author}{Donnat, C.}, \bibinfo{author}{Zitnik, M.},
  \bibinfo{author}{Hallac, D.}, \bibinfo{author}{Leskovec, J.},
  \bibinfo{year}{2018}.
\newblock \bibinfo{title}{Learning structural node embeddings via diffusion
  wavelets}, in: \bibinfo{booktitle}{KDD '18: Proceedings of the 24th ACM
  SIGKDD International Conference on Knowledge Discovery and Data Mining},
  \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{1320--1329}.
%
\bibitem[{Erd\"{o}s and R\'{e}nyi(1959)}]{er}
\bibinfo{author}{Erd\"{o}s, P.}, \bibinfo{author}{R\'{e}nyi, A.},
  \bibinfo{year}{1959}.
\newblock \bibinfo{title}{On random graphs {I}}.
\newblock \bibinfo{journal}{Publicationes Mathematicae Debrecen}
  \bibinfo{volume}{6}, \bibinfo{pages}{290}.
%
\bibitem[{Filippone et~al.(2008)Filippone, Camastra, Masulli and
  Rovetta}]{Filippone}
\bibinfo{author}{Filippone, M.}, \bibinfo{author}{Camastra, F.},
  \bibinfo{author}{Masulli, F.}, \bibinfo{author}{Rovetta, S.},
  \bibinfo{year}{2008}.
\newblock \bibinfo{title}{A survey of kernel and spectral methods for
  clustering}.
\newblock \bibinfo{journal}{Pattern Recognition} \bibinfo{volume}{41},
  \bibinfo{pages}{176--190}.
%
\bibitem[{Fortunato(2009)}]{fortunato}
\bibinfo{author}{Fortunato, S.}, \bibinfo{year}{2009}.
\newblock \bibinfo{title}{Community detection in graphs}.
\newblock \bibinfo{journal}{CoRR} \bibinfo{volume}{abs/0906.0612}.
%
\bibitem[{Fortunato and Newman(2022)}]{fortunato202220}
\bibinfo{author}{Fortunato, S.}, \bibinfo{author}{Newman, M.E.},
  \bibinfo{year}{2022}.
\newblock \bibinfo{title}{20 years of network community detection}.
\newblock \bibinfo{journal}{Nature Physics} \bibinfo{volume}{18},
  \bibinfo{pages}{848--850}.
%
\bibitem[{Gagolewski(2021)}]{genieclust}
\bibinfo{author}{Gagolewski, M.}, \bibinfo{year}{2021}.
\newblock \bibinfo{title}{{genieclust}: {F}ast and robust hierarchical
  clustering}.
\newblock \bibinfo{journal}{SoftwareX} \bibinfo{volume}{15},
  \bibinfo{pages}{100722}.
\newblock \URLprefix \url{https://genieclust.gagolewski.com},
  \DOIprefix\doi{10.1016/j.softx.2021.100722}.
%
\bibitem[{Gagolewski(2022)}]{aaa}
\bibinfo{author}{Gagolewski, M.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Adjusted asymmetric accuracy: {A} well-behaving
  external cluster validity measure}.
\newblock \URLprefix \url{https://arxiv.org/pdf/2209.02935.pdf},
  \DOIprefix\doi{10.48550/arXiv.2209.02935}. \bibinfo{note}{under review
  (preprint)}.
%
\bibitem[{Gagolewski et~al.(2016)Gagolewski, Bartoszuk and Cena}]{genie}
\bibinfo{author}{Gagolewski, M.}, \bibinfo{author}{Bartoszuk, M.},
  \bibinfo{author}{Cena, A.}, \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Genie: A new, fast, and outlier-resistant
  hierarchical clustering algorithm}.
\newblock \bibinfo{journal}{Information Sciences} \bibinfo{volume}{363},
  \bibinfo{pages}{8--23}.
%
\bibitem[{Garey and Johnson(1979)}]{gareyjohnson}
\bibinfo{author}{Garey, M.R.}, \bibinfo{author}{Johnson, D.S.},
  \bibinfo{year}{1979}.
\newblock \bibinfo{title}{Computers and Intractability: {A} Guide to the Theory
  of NP-Completeness}.
\newblock \bibinfo{publisher}{W. H. Freeman}.
%
\bibitem[{Girvan and Newman(2002)}]{Girvan}
\bibinfo{author}{Girvan, M.}, \bibinfo{author}{Newman, M.E.J.},
  \bibinfo{year}{2002}.
\newblock \bibinfo{title}{Community structure in social and biological
  networks}.
\newblock \bibinfo{journal}{Proceedings of the National Academy of Sciences}
  \bibinfo{volume}{99}, \bibinfo{pages}{7821--7826}.
%
\bibitem[{Godsil and Royle(2001)}]{godsil}
\bibinfo{author}{Godsil, C.D.}, \bibinfo{author}{Royle, G.F.},
  \bibinfo{year}{2001}.
\newblock \bibinfo{title}{Algebraic Graph Theory}.
\newblock \bibinfo{publisher}{Springer}.
%
\bibitem[{Golub and Reinsch(1970)}]{SVD}
\bibinfo{author}{Golub, G.H.}, \bibinfo{author}{Reinsch, C.},
  \bibinfo{year}{1970}.
\newblock \bibinfo{title}{Singular value decomposition and least squares
  solutions}.
\newblock \bibinfo{journal}{Numer. Math.} \bibinfo{volume}{14},
  \bibinfo{pages}{403–420}.
%
\bibitem[{Grover and Leskovec(2016)}]{Node2Vec}
\bibinfo{author}{Grover, A.}, \bibinfo{author}{Leskovec, J.},
  \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Node2vec: Scalable feature learning for networks},
  in: \bibinfo{booktitle}{KDD '16: Proceedings of the 22nd ACM SIGKDD
  International Conference on Knowledge Discovery and Data Mining},
  \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{855--864}.
%
\bibitem[{Hamdaqa et~al.(2014)Hamdaqa, Tahvildari, LaChapelle and
  Campbell}]{socialcommunity}
\bibinfo{author}{Hamdaqa, M.}, \bibinfo{author}{Tahvildari, L.},
  \bibinfo{author}{LaChapelle, N.}, \bibinfo{author}{Campbell, B.},
  \bibinfo{year}{2014}.
\newblock \bibinfo{title}{Cultural scene detection using reverse {L}ouvain
  optimization}.
\newblock \bibinfo{journal}{Sci. Comput. Program.} \bibinfo{volume}{95},
  \bibinfo{pages}{44--72}.
%
\bibitem[{Hamilton et~al.(2017)Hamilton, Ying and Leskovec}]{hamilton}
\bibinfo{author}{Hamilton, W.L.}, \bibinfo{author}{Ying, R.},
  \bibinfo{author}{Leskovec, J.}, \bibinfo{year}{2017}.
\newblock \bibinfo{title}{Representation learning on graphs: Methods and
  applications}.
\newblock \bibinfo{journal}{{IEEE} Data Eng. Bull.} \bibinfo{volume}{40},
  \bibinfo{pages}{52--74}.
%
\bibitem[{He et~al.(2022)He, Zheng, Cheng, Tang, Chen and Liu}]{cd1}
\bibinfo{author}{He, C.}, \bibinfo{author}{Zheng, Y.}, \bibinfo{author}{Cheng,
  J.}, \bibinfo{author}{Tang, Y.}, \bibinfo{author}{Chen, G.},
  \bibinfo{author}{Liu, H.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Semi-supervised overlapping community detection in
  attributed graph with graph convolutional autoencoder}.
\newblock \bibinfo{journal}{Information Sciences} \bibinfo{volume}{608},
  \bibinfo{pages}{1464--1479}.
\newblock \URLprefix
  \url{https://www.sciencedirect.com/science/article/pii/S0020025522007253},
  \DOIprefix\doi{https://doi.org/10.1016/j.ins.2022.07.036}.
%
\bibitem[{Holland et~al.(1983)Holland, Laskey and Leinhardt}]{sbm}
\bibinfo{author}{Holland, P.W.}, \bibinfo{author}{Laskey, K.B.},
  \bibinfo{author}{Leinhardt, S.}, \bibinfo{year}{1983}.
\newblock \bibinfo{title}{Stochastic blockmodels: First steps}.
\newblock \bibinfo{journal}{Social Networks} \bibinfo{volume}{5},
  \bibinfo{pages}{109--137}.
%
\bibitem[{Ikotun et~al.(2023)Ikotun, Ezugwu, Abualigah, Abuhaija and
  Heming}]{cd3}
\bibinfo{author}{Ikotun, A.M.}, \bibinfo{author}{Ezugwu, A.E.},
  \bibinfo{author}{Abualigah, L.}, \bibinfo{author}{Abuhaija, B.},
  \bibinfo{author}{Heming, J.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{K-means clustering algorithms: A comprehensive
  review, variants analysis, and advances in the era of big data}.
\newblock \bibinfo{journal}{Information Sciences} \bibinfo{volume}{622},
  \bibinfo{pages}{178--210}.
\newblock \URLprefix
  \url{https://www.sciencedirect.com/science/article/pii/S0020025522014633},
  \DOIprefix\doi{https://doi.org/10.1016/j.ins.2022.11.139}.
%
\bibitem[{Katz(1953)}]{katz}
\bibinfo{author}{Katz, L.}, \bibinfo{year}{1953}.
\newblock \bibinfo{title}{A new status index derived from sociometric
  analysis}.
\newblock \bibinfo{journal}{Psychometrika} \bibinfo{volume}{18},
  \bibinfo{pages}{39--43}.
%
\bibitem[{Kipf and Welling(2016)}]{vgae}
\bibinfo{author}{Kipf, T.N.}, \bibinfo{author}{Welling, M.},
  \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Variational graph auto-encoders}.
\newblock \bibinfo{journal}{CoRR} \bibinfo{volume}{abs/1611.07308}.
%
\bibitem[{Kipf and Welling(2017)}]{gcn}
\bibinfo{author}{Kipf, T.N.}, \bibinfo{author}{Welling, M.},
  \bibinfo{year}{2017}.
\newblock \bibinfo{title}{Semi-supervised classification with graph
  convolutional networks}, in: \bibinfo{booktitle}{ICLR '17: Proceedings of the
  5th International Conference on Learning Representations},
  \bibinfo{publisher}{OpenReview.net}.
%
\bibitem[{Lawrence and Phipps(1985)}]{HubertArabie1985:partitionscomp}
\bibinfo{author}{Lawrence, H.}, \bibinfo{author}{Phipps, A.},
  \bibinfo{year}{1985}.
\newblock \bibinfo{title}{Comparing partitions}.
\newblock \bibinfo{journal}{Journal of Classification} \bibinfo{volume}{2},
  \bibinfo{pages}{193--218}.
%
\bibitem[{Lusseau(2003)}]{dolphins}
\bibinfo{author}{Lusseau, D.}, \bibinfo{year}{2003}.
\newblock \bibinfo{title}{The emergent properties of a dolphin social network}.
\newblock \bibinfo{journal}{Proc. Biol. Sci.} \bibinfo{volume}{270 Suppl 2},
  \bibinfo{pages}{S186--8}.
%
\bibitem[{Mancoridis et~al.(1998)Mancoridis, Mitchell, Rorres, Chen and
  Gansner}]{mancoridis}
\bibinfo{author}{Mancoridis, S.}, \bibinfo{author}{Mitchell, B.S.},
  \bibinfo{author}{Rorres, C.}, \bibinfo{author}{Chen, Y.F.},
  \bibinfo{author}{Gansner, E.R.}, \bibinfo{year}{1998}.
\newblock \bibinfo{title}{{Using Automatic Clustering to Produce High-Level
  System Organizations of Source Code}}, in: \bibinfo{booktitle}{IWPC '98:
  Proceedings of the 6th International Workshop on Program Comprehension},
  \bibinfo{publisher}{{IEEE}}. pp. \bibinfo{pages}{45--52}.
%
\bibitem[{Mikolov et~al.(2013)Mikolov, Chen, Corrado and Dean}]{skipgram}
\bibinfo{author}{Mikolov, T.}, \bibinfo{author}{Chen, K.},
  \bibinfo{author}{Corrado, G.}, \bibinfo{author}{Dean, J.},
  \bibinfo{year}{2013}.
\newblock \bibinfo{title}{Efficient estimation of word representations in
  vector space}, in: \bibinfo{booktitle}{ICLR '13: Proceedings of the 1st
  International Conference on Learning Representations}.
%
\bibitem[{Murtagh(1983)}]{murtagh}
\bibinfo{author}{Murtagh, F.}, \bibinfo{year}{1983}.
\newblock \bibinfo{title}{A survey of recent advances in hierarchical
  clustering algorithms}.
\newblock \bibinfo{journal}{The Computer Journal} \bibinfo{volume}{26},
  \bibinfo{pages}{354--359}.
%
\bibitem[{Newman and Girvan(2004)}]{newmanGirvanModularity}
\bibinfo{author}{Newman, M.E.J.}, \bibinfo{author}{Girvan, M.},
  \bibinfo{year}{2004}.
\newblock \bibinfo{title}{Finding and evaluating community structure in
  networks}.
\newblock \bibinfo{journal}{Phys. Rev. E} \bibinfo{volume}{69},
  \bibinfo{pages}{026113}.
%
\bibitem[{Ou et~al.(2016)Ou, Cui, Pei, Zhang and Zhu}]{hope}
\bibinfo{author}{Ou, M.}, \bibinfo{author}{Cui, P.}, \bibinfo{author}{Pei, J.},
  \bibinfo{author}{Zhang, Z.}, \bibinfo{author}{Zhu, W.}, \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Asymmetric transitivity preserving graph embedding},
  in: \bibinfo{booktitle}{KDD '16: Proceedings of the 22nd ACM SIGKDD
  International Conference on Knowledge Discovery and Data Mining},
  \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{1105--1114}.
%
\bibitem[{Page et~al.(1999)Page, Brin, Motwani and Winograd}]{pagerank}
\bibinfo{author}{Page, L.}, \bibinfo{author}{Brin, S.},
  \bibinfo{author}{Motwani, R.}, \bibinfo{author}{Winograd, T.},
  \bibinfo{year}{1999}.
\newblock \bibinfo{title}{The PageRank Citation Ranking: Bringing Order to the
  Web}.
\newblock \bibinfo{type}{Technical Report} \bibinfo{number}{1999-66}. Stanford
  InfoLab.
%
\bibitem[{Patnaik et~al.(2016)Patnaik, Bhuyan and {Krishna Rao}}]{DIANA}
\bibinfo{author}{Patnaik, A.K.}, \bibinfo{author}{Bhuyan, P.K.},
  \bibinfo{author}{{Krishna Rao}, K.}, \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Divisive analysis (diana) of hierarchical clustering
  and gps data for level of service criteria of urban streets}.
\newblock \bibinfo{journal}{Alexandria Engineering Journal}
  \bibinfo{volume}{55}, \bibinfo{pages}{407--418}.
%
\bibitem[{Paul and Chen(2016)}]{nullmodels}
\bibinfo{author}{Paul, S.}, \bibinfo{author}{Chen, Y.}, \bibinfo{year}{2016}.
\newblock \bibinfo{title}{{Null Models and Modularity Based Community Detection
  in Multi-Layer Networks}}.
\newblock \bibinfo{journal}{CoRR} \bibinfo{volume}{abs/1608.00623}.
%
\bibitem[{Perozzi et~al.(2014)Perozzi, Al{-}Rfou and Skiena}]{DeepWalk}
\bibinfo{author}{Perozzi, B.}, \bibinfo{author}{Al{-}Rfou, R.},
  \bibinfo{author}{Skiena, S.}, \bibinfo{year}{2014}.
\newblock \bibinfo{title}{Deepwalk: Online learning of social representations},
  in: \bibinfo{booktitle}{KDD '14: Proceedings of the 20th ACM SIGKDD
  International Conference on Knowledge Discovery and Data Mining},
  \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{701--710}.
%
\bibitem[{R{\'{e}}v{\'{e}}sz(1990)}]{revesz}
\bibinfo{author}{R{\'{e}}v{\'{e}}sz, P.}, \bibinfo{year}{1990}.
\newblock \bibinfo{title}{Random Walk in Random and Non-Random Environments}.
\newblock \bibinfo{publisher}{World Scientific}.
%
\bibitem[{Rezaei and Fränti(2016)}]{psi}
\bibinfo{author}{Rezaei, M.}, \bibinfo{author}{Fränti, P.},
  \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Set matching measures for external cluster validity}.
\newblock \bibinfo{journal}{IEEE Transactions on Knowledge and Data
  Engineering} \bibinfo{volume}{28}, \bibinfo{pages}{2173--2186}.
\newblock \DOIprefix\doi{10.1109/TKDE.2016.2551240}.
%
\bibitem[{Ribeiro et~al.(2017)Ribeiro, Saverese and Figueiredo}]{Struc2Vec}
\bibinfo{author}{Ribeiro, L.F.R.}, \bibinfo{author}{Saverese, P.H.P.},
  \bibinfo{author}{Figueiredo, D.R.}, \bibinfo{year}{2017}.
\newblock \bibinfo{title}{Struc2vec: Learning node representations from
  structural identity}, in: \bibinfo{booktitle}{KDD '17: Proceedings of the
  23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
  Mining}, \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{385--394}.
%
\bibitem[{Su and \textit{et al.}(2021)}]{su}
\bibinfo{author}{Su, X.}, \bibinfo{author}{\textit{et al.}},
  \bibinfo{year}{2021}.
\newblock \bibinfo{title}{A comprehensive survey on community detection with
  deep learning}.
\newblock \bibinfo{journal}{CoRR} \bibinfo{volume}{abs/2105.12584}.
%
\bibitem[{Sun et~al.(2019)Sun, Qu, Hoffmann, Huang and Tang}]{vgraph}
\bibinfo{author}{Sun, F.Y.}, \bibinfo{author}{Qu, M.},
  \bibinfo{author}{Hoffmann, J.}, \bibinfo{author}{Huang, C.},
  \bibinfo{author}{Tang, J.}, \bibinfo{year}{2019}.
\newblock \bibinfo{title}{{vGraph: {A} Generative Model for Joint Community
  Detection and Node Representation Learning}}, in:
  \bibinfo{booktitle}{NIPS'18: Proceedings of the 32nd International Conference
  on Neural Information Processing Systems}, pp. \bibinfo{pages}{512--522}.
%
\bibitem[{Tang et~al.(2015)Tang, Qu, Wang, Zhang, Yan and Mei}]{LINE}
\bibinfo{author}{Tang, J.}, \bibinfo{author}{Qu, M.}, \bibinfo{author}{Wang,
  M.}, \bibinfo{author}{Zhang, M.}, \bibinfo{author}{Yan, J.},
  \bibinfo{author}{Mei, Q.}, \bibinfo{year}{2015}.
\newblock \bibinfo{title}{Line: Large-scale information network embedding}, in:
  \bibinfo{booktitle}{WWW '15 Companion: Proceedings of the 24th International
  Conference on World Wide Web}, \bibinfo{publisher}{International World Wide
  Web Conferences Steering Committee / {ACM}}. pp. \bibinfo{pages}{1067--1077}.
%
\bibitem[{Traag et~al.(2019)Traag, Waltman and van Eck}]{leiden}
\bibinfo{author}{Traag, V.}, \bibinfo{author}{Waltman, L.},
  \bibinfo{author}{van Eck, N.J.}, \bibinfo{year}{2019}.
\newblock \bibinfo{title}{From {L}ouvain to {L}eiden: {G}uaranteeing
  well-connected communities}.
\newblock \bibinfo{journal}{Scientific Reports} \bibinfo{volume}{9},
  \bibinfo{pages}{5233}.
\newblock \DOIprefix\doi{10.1038/s41598-019-41695-z}.
%
\bibitem[{Wang et~al.(2016)Wang, Cui and Zhu}]{SDNE}
\bibinfo{author}{Wang, D.}, \bibinfo{author}{Cui, P.}, \bibinfo{author}{Zhu,
  W.}, \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Structural deep network embedding}, in:
  \bibinfo{booktitle}{KDD '16: Proceedings of the 22nd ACM SIGKDD International
  Conference on Knowledge Discovery and Data Mining},
  \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{1225--1234}.
%
\bibitem[{Wasserman and Faust(1994)}]{wassermanFaust}
\bibinfo{author}{Wasserman, S.}, \bibinfo{author}{Faust, K.},
  \bibinfo{year}{1994}.
\newblock \bibinfo{title}{Social Network Analysis: Methods and Applications}.
\newblock \bibinfo{publisher}{Cambridge University Press}.
%
\bibitem[{Xu and Tian(2015)}]{Xu}
\bibinfo{author}{Xu, D.}, \bibinfo{author}{Tian, Y.}, \bibinfo{year}{2015}.
\newblock \bibinfo{title}{A comprehensive survey of clustering algorithms}.
\newblock \bibinfo{journal}{Annals of Data Science} \bibinfo{volume}{2},
  \bibinfo{pages}{165--193}.
%
\bibitem[{Zachary(1977)}]{zachary}
\bibinfo{author}{Zachary, W.W.}, \bibinfo{year}{1977}.
\newblock \bibinfo{title}{An information flow model for conflict and fission in
  small groups}.
\newblock \bibinfo{journal}{Journal of Anthropological Research}
  \bibinfo{volume}{33}, \bibinfo{pages}{452--473}.
%
\bibitem[{Zhang et~al.(2012)Zhang, Wang, Zhao and Tang}]{graphDegreeLinkage}
\bibinfo{author}{Zhang, W.}, \bibinfo{author}{Wang, X.}, \bibinfo{author}{Zhao,
  D.}, \bibinfo{author}{Tang, X.}, \bibinfo{year}{2012}.
\newblock \bibinfo{title}{Graph degree linkage: Agglomerative clustering on a
  directed graph}, in: \bibinfo{booktitle}{ECCV'12: Proceedings of the 12th
  European conference on Computer Vision}, \bibinfo{publisher}{Springer}. pp.
  \bibinfo{pages}{428--441}.
%
\bibitem[{Zhang et~al.(2022)Zhang, Wan, Zhou, Lu, Chen and Liao}]{ZHANG202286}
\bibinfo{author}{Zhang, Z.}, \bibinfo{author}{Wan, J.}, \bibinfo{author}{Zhou,
  M.}, \bibinfo{author}{Lu, K.}, \bibinfo{author}{Chen, G.},
  \bibinfo{author}{Liao, H.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Information diffusion-aware likelihood maximization
  optimization for community detection}.
\newblock \bibinfo{journal}{Information Sciences} \bibinfo{volume}{602},
  \bibinfo{pages}{86--105}.
\newblock \URLprefix
  \url{https://www.sciencedirect.com/science/article/pii/S0020025522003334},
  \DOIprefix\doi{https://doi.org/10.1016/j.ins.2022.04.009}.

\end{thebibliography}

%
%

\end{document}
