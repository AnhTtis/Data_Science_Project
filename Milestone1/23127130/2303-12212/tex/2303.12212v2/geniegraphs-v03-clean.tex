%
%


%
%
%
%


%

%
%



%

\documentclass[a4paper,fleqn]{cas-sc}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[numbers,square,sort&compress]{natbib}
\usepackage{array}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{lineno}
\usepackage{multirow}
%

\usepackage{afterpage}

\usepackage{amsmath, amsfonts, amsthm, latexsym}
\usepackage{algorithm}
\usepackage{algpseudocode}

%
%
%
%
%
%
%
%
%
%
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
\tsc{EP}
\tsc{PMS}
\tsc{BEC}
\tsc{DE}
%

%
%

\makeatletter
\newtheoremstyle{definition}
{3ex}%
{3ex}%
{\upshape}%
{}%
{\bfseries}%
{.}%
{.5em}%
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\makeatother

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{fact}[theorem]{Fact}
\DeclareMathOperator{\Enc}{ENC}
\DeclareMathOperator{\Dec}{DEC}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\tr}{tr}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
%



%
\ExplSyntaxOn
\keys_set:nn { stm / mktitle } { nologo }
\ExplSyntaxOff
%


\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

\shortauthors{Brzozowski, Siudem, Gagolewski}

\shorttitle{Community detection in complex networks}
\title[mode = title]{Community detection in complex networks via node similarity, graph representation learning, and~hierarchical clustering}



\author[1]{\L{}ukasz Brzozowski}[orcid=0000-0002-3625-3312]    %
\ead{lukasz.brzozowski@pw.edu.pl}
\cormark[1]
\cortext[cor1]{Corresponding author}
\credit{Conceptualisation, Methodology, Data Curation, Software, Visualisation, Writing -- Original Draft, Revision}  %

\author[2]{Grzegorz Siudem}[orcid=0000-0002-9391-6477]
\ead{grzegorz.siudem@pw.edu.pl}
\ead[URL]{http://if.pw.edu.pl/~siudem}
\credit{Conceptualisation, Methodology, Writing -- Revision}  %

\author[3]{Marek Gagolewski}[orcid=0000-0003-0637-6028]
\ead{m.gagolewski@deakin.edu.au}
\ead[url]{https://www.gagolewski.com}
\credit{Conceptualisation, Methodology, Writing -- Revision}  %





%
\address[1]{Warsaw University of Technology,
Faculty of Mathematics and Information Science,
ul. Koszykowa 75, 00-662 Warsaw, Poland}

\address[2]{Warsaw University of Technology, Faculty of Physics,
ul. Koszykowa 75, 00-662 Warsaw, Poland}

\address[3]{Deakin University, Data to Intelligence Research Centre, School of IT, Geelong, VIC 3220, Australia}

%
%




%
%
%


\begin{abstract}
%
%
%
%
Community detection is a critical challenge in analysing real graphs, including social, transportation, citation, cybersecurity, and many other networks. This article proposes three new, general, hierarchical frameworks to deal with this task. The introduced approach supports various linkage-based clustering algorithms, vertex proximity matrices, and graph representation learning models. We compare over a hundred module combinations on the Stochastic Block Model graphs and real-life datasets. We observe that our best pipelines (Wasserman--Faust and the mutual information-based PPMI proximity, as well as the deep learning-based DNGR representations) perform competitively to the state-of-the-art Leiden and Louvain algorithms. At the same time, unlike the latter, they remain hierarchical. Thus, they output a series of nested partitions of all possible cardinalities which are compatible with each other. This feature is crucial when the number of correct partitions is unknown in advance.
%
\end{abstract}


\begin{keywords}
community detection \sep
complex network analysis \sep
deep autoencoders \sep
representation learning \sep
Genie hierarchical clustering
\end{keywords}


\maketitle


%
%
%


\section{Introduction}\label{sec:intro}
%


\subsection{Background}

More and more structures become available in the form of graphs and networks. The problem of community detection therein is a long-standing task in data science: a~plethora of solutions have already been developed (see, e.g., \cite{fortunato, su,wilinski2019detectability, cd1}). Still, the topic seems far from exhausted \cite{fortunato202220, IPM2}. What constitutes a community can be understood very broadly \cite{fortunato}. Therefore, many relatively different tools will aim to provide us with their own solutions to this task: is a community a group of tightly connected vertices, or is it a group of vertices that play similar roles in the graph's structure? Further, the sole existence of some mathematically definable relation allows us to construct a network in a given environment. However, such networks may be, for example, structured, hierarchical, temporal, evolving, or attributed. The various generalisations of the concept of a graph call for different community detection solutions tailored to each specific context~\cite{fortunato, cd2, IPM1}.

Researchers in pattern recognition and statistical/machine learning have long known a similar problem under a different name: clustering~\cite{Xu}. At some level, we can say that the only difference between clustering and community detection is the data space considered -- the former is defined on graphs and the latter on other metric spaces, such as the Euclidean one~\cite{Xu}. This similarity resulted in some domain-specific approaches stemming from a common theoretical basis. For example, hierarchical algorithms may be successfully employed to solve both community detection and clustering tasks with little change in the implementation.
However, with the need of more specialised methods~\cite{su}, the approaches developed in the two aforementioned fields began to diverge.



%


%



\subsection{Research objectives}
Taking into account the growing popularity of graph machine learning (e.g., \cite{hamilton, DeepWalk, gcn}), which includes graph representation learning specifically, we wish to help bridge the between-domain gap by exploring new ways of connecting community detection and clustering solutions.

Specifically, in this paper, we develop a framework for applying hierarchical clustering algorithms (e.g., \cite{murtagh,genie, IPM3}) to the task of community detection. While perhaps not the newest by itself, the hierarchical approach turns out robust and easily applicable on graphs because it relies solely on similarity metrics between the pairs of nodes. As the problem of choosing the right measures for our task is crucial, we will perform an extensive comparative analysis of twenty five vertex similarity modelling techniques falling into the three following classes:
\begin{enumerate}
    \item distance matrices obtained by considering various node similarity measure:
        the Wasserman--Faust measure~\cite{wassermanFaust},
        Neighbourhood overlap~\cite{fortunato},
        $K$-step walk probability~\cite{fortunato},
        Katz index~\cite{katz},
        Rooted PageRank~\cite{hope},
        Adamic--Adar index~\cite{adamicAdar},
        Blondel--Gajardo measure~\cite{blondel}, and
        Positive Pointwise Mutual Information~\cite{dngr},
    \item spectral versions of the above, i.e., eigendecompositions of the corresponding vertex similarity matrices,
    \item distances on automatically discovered spaces using
    the representation learning algorithms:
        DeepWalk~\cite{DeepWalk},
        DNGR~\cite{dngr},
        Graph Factorisation~\cite{GF},
        GraRep~\cite{grarep},
        HARP~\cite{HARP},
        HOPE~\cite{hope},
        Laplacian Eigenmaps~\cite{LE}, and
        Node2Vec~\cite{Node2Vec}.
\end{enumerate}
Both synthetic and real-life network examples will be used.
Further, we shall study how the tuning of the underlying parameters affects the quality of clustering. In each case, we will determine the best linkage function. This will also provide us with a good opportunity to consider a new graph extension of the recently-proposed Genie algorithm \cite{genie}, whose performance on many benchmark datasets in the Euclidean spaces turned out to be above par \cite{genieclust}.

We should emphasise that hierarchical approaches come with a set of advantages. Firstly, the communities are nested, i.e., depending on the selected number of clusters we can observe how a large community can be decomposed into smaller ones. Also, hierarchical approaches allow us to select any number of clusters, which is a feature often lacking in other methods.

\bigskip
The paper is set out as follows. In Section~\ref{sec:background},
we introduce the node similarity measures and graph representation learning methods that will serve as graph models in the task at hand. In Section~\ref{sec:methodology}, we present a framework for community detection based on hierarchical clustering algorithms (including Genie). In Section~\ref{sec:results}, we evaluate all the algorithms on synthetic datasets from the stochastic block model in order to determine the best method combinations. They are then carefully evaluated, including on real-world data, in Section~\ref{sec:results-details}. Finally, Section~\ref{sec:final} concludes the article.


%
%
%

\section{Graph models}\label{sec:background}

In this section, we present selected vertex similarity measures we investigate in our framework.


\subsection{Community detection}

A common way of conducting mathematical research is to find generalisations of the existing concepts to explain them better in connection with the definitions from other fields. In that sense, Network Science is not that different: why should we define a ``small'' graph, such as a citation network of machine learning papers, when we could define a more general citation network of all academic research? This approach allows us to work on larger structures, which usually means that we obtain better statistical approximations of some sought values. Nevertheless this also inevitably indicates that our network becomes less homogeneous. In fact, while the whole citation network contains all the domains, we should expect that its subgraph induced by the vertices representing ML should be connected more densely than the whole network on average. Such an induced subgraph is called a \textit{community}~\cite{fortunato}. In general, communities can be found in many naturally occurring networks, e.g., they may be based on shared interests of social media users in a social network~\cite{socialcommunity, IPM4}, finding patterns in delays in railway networks \cite{dekker2022}, or similarity between protein functions in biological networks~\cite{Girvan}.

Therefore, to understand the behaviour of a network correctly, it is essential to detect whether it contains communities and, if so, find which nodes belong to which clusters. However, the above description of a community is far from strict. In fact, there is no universally accepted definition~\cite{fortunato,fortunato202220}. Here we would like to present some metrics that could help define the community.

\begin{definition}
Let $H = (V_H, E_H)$ be an induced subgraph of graph $G = (V_G, E_G)$ and let $n_H = |V_H|$. We say that $e$ is an \textit{internal} edge of $H$, if it connects two vertices in $V_H$. Also, $e$ is an \textit{inter-cluster} edge of $H$, if exactly one of its ends lies in $V_H$. As the maximum number of edges in $H$ is equal to
$\binom{n_H}{2} = \frac{n_H (n_H - 1)}{2}$,
we define the \textit{intra-} and \textit{inter-cluster} densities of $H$,
respectively, as~\cite{fortunato}:
\begin{eqnarray}
\delta_\text{int}(H) &=& \frac{\text{the number of internal edges of $H$}}{\binom{n_H}{2}},
\\
\delta_\text{ext}(H) &=& \frac{\text{the number of inter-cluster edges of $H$}}{n_H (n-n_H)}.
\end{eqnarray}
\end{definition}

Intuitively, a community, regarded as an induced subgraph of $G$, should have higher inter-cluster density and lower intra-cluster density than the average link density of the graph $G$. To detect communities, we could look for a partition that either maximises the average value of $\delta_\text{int}$, minimises the average value of $\delta_\text{ext}$, or combines the two; for example, we could maximise the average value of $\delta_\text{int}-\delta_\text{ext}$ \cite{mancoridis}. However, the problem with algorithms simply relying on the above-defined concepts of cluster density is that the posed tasks are often NP-complete. For example, finding all clusters in $G$ that have intra-cluster density $\delta_\text{int}$ higher than a given threshold $\zeta$ cannot be performed in polynomial time~\cite{gareyjohnson}.

Another issue with the above definitions of cluster density is that we calculate them only relative to the observed network $G$. If $G$ is heterogeneous, it becomes difficult to precisely define the requirements for a subgraph to be a community. For example, we would require a community to have an extremely high density if it lies in a densely connected region of $G$, but we would accept a subgraph with a lower density in a sparse region in $G$. Such differences could lead to problems with proper task definition and unreliable results. On the other hand, we know that the Erdős–Rényi (ER) model~\cite{er} should not contain communities, as the edges are positioned randomly with equal probabilities. Based on that fact, Newman and Girvan introduced a concept of \textit{modularity}~\cite{newmanGirvanModularity}.


\begin{definition}
Let $G = (V, E)$ and let us assume that we have partitioned $G$ into clusters and let $A$ be its adjacency matrix. Let $\tilde{G}$ be a \textit{null model}, i.e., a graph that is structurally similar to $G$ but does not contain communities. Let $\tilde{A}_{i, j} $ be the expected number of edges between $i, j$ in $\tilde{G}$ and let $C_i$ denote the cluster in $G$ containing the vertex $i$. Then,
denoting the Kronecker delta with $\delta$,
we define a partition quality function called \textit{modularity} as:
\begin{equation}
Q = \frac{1}{2|E|} \sum_{i, j \in V} (A_{i, j} - P_{i, j})\, \delta(C_i, C_j).
\end{equation}
\end{definition}

Intuitively, given some partition of $G$, modularity compares the observed number of edges in the communities with the expected number of such edges in the null model. Various candidates for the null model have been investigated: the ER model is the simplest of them all~\cite{newmanGirvanModularity}, but we could derive specific null models for particular classes of networks~\cite{nullmodels}. While explicitly maximising modularity is again NP-complete, it has become a popular evaluation metric for community detection algorithms~\cite{fortunato}.



\subsection{Node similarity}\label{sec:nodesim}

In the previous section, we have defined the community to be an induced subgraph, which is, in some sense, connected more strongly than its neighbourhood. However, we could look at the community detection problem more locally: a community could be a subgraph of \textit{somewhat} similar vertices. In such an approach, we would like the communities to be subgraphs such that the inter-cluster edges connect similar vertices and the intra-cluster edges connect different ones~\cite{fortunato}. Assuming that two vertices are similar if they are connected in the graph, we naturally return to the previous definition. However, the flexibility of similarity allows us to introduce other approaches to community detection. In this section, we will present some selected node similarity measures.

\paragraph{Embedding-based measures~\cite{fortunato}.}
%
Let us assume that we have embedded the graph $G$ in the space $\mathbb{R}^d$ for some $d$, i.e., that we have vector representations $X_v = (x^1_v, x^2_v, \dots, x^d_v) \in \mathbb{R}^d$ for each node $v$ of $G$. Regardless of how the embedding was obtained, we can now easily define the node similarity between two vertices $u$ and $v$ as a distance  between $X_u$ and $X_v$, e.g., being
the Euclidean, Manhattan, and Chebyshev metric.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
Applying any non-increasing transformation thereon turns them into a similarity function.
We will discuss particular graph embedding methods in Section~\ref{grl}.

\paragraph{Relationship-based measures~\cite{fortunato,burt}.}
Another class of similarity measures is based on the relationships encoded within the graph. The most straightforward function of the similarity between the vertices $u$ and $v$ of $G$ is $A_{u, v}$, i.e., the element of the graph's adjacency matrix indicating whether or not $u$ and $v$ are adjacent. However, we could also define their similarity in terms of the structure of their neighbourhoods, e.g., based on the Wasserman--Faust distance~\cite{wassermanFaust}:
\begin{equation}
d^{WF}_{u, v} = \sqrt{\sum_{k \neq u, v} (A_{u, k} - A_{v, k})^2}.
\end{equation}

The Wasserman--Faust distance is one of the first measures based on the structural equivalence of the vertices. It does not rely on the adjacency of $u$ and $v$. The Adamic--Adar index~\cite{adamicAdar} also incorporates the structural equivalence:
\begin{equation}
d^{AA}_{u, v} = \sum_{w \in N(u) \cap N(v)} \frac{2}{\log |N(w)|}.
\end{equation}
The intuition behind this index is that the common neighbours having large neighbourhoods are less significant to the similarity between the vertices. Another variant of the similarity measure based on neighbourhoods of $u$ and $v$ is the Jaccard (Overlap), similarity:
\begin{equation}
w_{u, v} = \frac{|N_G(u) \cap N_G(v)|}{|N_G(u) \cup N_G(v)|}.
\end{equation}

%
%
%
%

\paragraph{Walk-based measures.}
In another approach, we could measure similarity between the vertices based on walks in the graph. Intuitively, if many paths connect two vertices, they would seem similar. Fortunately, the adjacency matrix of a graph captures all information about paths. It is known that for an unweighted graph $G$, the element $A_{u, v}^k$ counts the number of paths between the vertices $u, v$ of length exactly $k$~\cite{godsil}. For instance, the Katz index matrix~\cite{katz} is:
\begin{equation}
S^\text{Katz} = \sum_{l=1}^{\infty}\beta A^l = \beta \cdot (I - \beta \cdot A)^{-1} \cdot A,
\end{equation}
\noindent
where $\beta$ is a decay parameter, and $I$ denotes the identity matrix. Then, the value $S^\text{Katz}_{u, v}$ gives the walk-based similarity between the vertices $u$ and $v$.

We may also be interested in whether the two vertices $u$ and $v$ often co-occur in the graph's random walks. This approach differs slightly from the previous one, as it assumes that the co-occurrence in random walks gives more practical information about the similarity between the two vertices than the Katz index.
If $D$ is the degree matrix of $G$, then the $K$-step transition probability between the vertices $u$ and $v$ may be retrieved cia~\cite{revesz}:
\begin{equation}
d^K_{i, j} = ((D^{-1}A)^K)_{i, j}.
\end{equation}

Similarly, instead of using unbiased random walks, we could turn to more sophisticated tools like the Rooted Page\-Rank algorithm~\cite{pagerank,hope}. It assumes that we ``surf'' through the vertices and walk to some neighbour with probability $\alpha$, or return to the starting vertex with the probability $1-\alpha$. If $P$ is the transition probability matrix, we get that
$S^\text{RPR} = \alpha \cdot S^\text{RPR}\cdot P + (1-\alpha)\cdot I$,
which  can be solved to obtain:
\begin{equation}
S^\text{RPR} = (1-\alpha) (I -\alpha P)^{-1}.
\end{equation}
\noindent
The elements of this matrix correspond to node similarity based on the Rooted PageRank algorithm.

Finally, we can use random walks to estimate the value of Pointwise Mutual Information (PMI)~\cite{pmi} between the vertices. That is, we identify the similarity between $u$ and $v$ as:
\begin{equation}
\text{PMI}(u, v) = \log\frac{p(u, v)}{p(u)p(v)},
\end{equation}
\noindent
where $p(u)$ denotes the probability of the occurrence of $u$ in some random walk of a fixed length. Analogically, $p(u, v)$ denotes the probability that both vertices occur on the same random walk, calculated using powers of a transition matrix of a stochastic random walk on the graph. Usually, as the PMI values may be negative, we use the Positive Pointwise Mutual Information (PPMI), i.e., we replace all the negative elements with $0$;
see \cite{dngr} for an example use.





\subsection{Graph representation learning}\label{grl}

We will use several algorithms
which can be loosely divided into five categories: direct optimisation-based, matrix factorisation-based, random walk-based, and autoencoder-based methods capturing vertex proximity, and a separate class of algorithms capturing  structural similarity of vertices. Let us present a short description of each of the categories and their noteworthy instances. For the sake of clarity, throughout this section, we assume that we embed the nodes of a graph $G = (V, E)$ in $\mathbb{R}^d$, i.e., we obtain a matrix $Z \in \mathbb{R}^{|V| \times d}$ whose rows correspond to individual nodes. We also denote the adjacency matrix of $G$ with $A$, its Laplacian with $L$, and its degree matrix with $D$.


\paragraph{Direct optimisation methods.}
This class of methods generates the embedding of $G$ by solving a specific optimisation task. For example, in the Laplacian Eigenmaps~\cite{LE} algorithm, we select $Z$ which minimises $\tr(Z^TLZ)$
subject to $Z^TDZ = I$,
where $\tr$ is the trace of a matrix. The authors of \cite{LE} justify that this optimisation task corresponds to a representation where adjacent nodes are possibly close in the resulting space while the non-adjacent nodes lie far from each other. Furthermore, in the Graph Factorisation (GF)~\cite{GF} method, the similarity between the nodes $v_i$ and $v_j$ is approximated by the dot product of the corresponding representations, i.e.,
$
s(v_i, v_j) \approx \langle Z_i, Z_j \rangle.
$

%
%


\paragraph{Matrix factorisation methods.}
The second class of methods uses the Singular Value Decomposition (SVD), which creates a lower-dimensional approximation of higher-dimensional structures~\cite{SVD}. To obtain the embedding, we utilise the SVD on a matrix representation of a graph and again rely on the dot product $\langle Z_i, Z_j \rangle$ approximation of the similarity. However, neither of the two methods performs the SVD on the adjacency or the Laplacian matrix as, according to the authors, these matrices are incapable of accurately representing node similarity. Thus, GraRep~\cite{grarep} performs the decomposition on a $K$-step transition probability matrix, and HOPE~\cite{hope} allows the use of the aforementioned Katz Index, Adamic--Adar, Rooted PageRank, or Jaccard similarity matrices. It must be noted that HOPE is a tool developed specifically for directed graphs with asymmetric similarity matrices. We also should mention separately some SVD-based clustering algorithms, e.g., Lingo~\cite{lingo}, which could be applied to graph matrices to directly perform community detection.


\paragraph{Random walk methods.} The next group of algorithms train an embedding model based on random walks in the graph. They all use the Skip-Gram model~\cite{skipgram}, which is a tool developed for Natural Language Processing. This design decision can be justified by the similarity of node sequences in a random walk and the word sequences in a sentence. Here, DeepWalk~\cite{DeepWalk} is the most basic but efficient solution, in which first-order Markov walks in a graph are fed to the neural network to generate the embedding. Node2Vec~\cite{Node2Vec} was developed to improve DeepWalk by replacing the walks with biased second-order Markov walks.
Let us note that the authors of HARP~\cite{HARP} noticed inefficient weight propagation in the SkipGrap~\cite{skipgram} neural network for large graphs. Therefore, they developed a meta-algorithm where the Skip-Gram model is trained iteratively on a family of graphs of increasing sizes derived from $G$. The weights corresponding to the nodes are passed between the iterations, improving the final representation. The HARP algorithm may work with either DeepWalk or Node2Vec as the underlying model.


\paragraph{Autoencoder methods.}
These methods use an autoencoder neural network to generate an embedding. Specifically, a matrix representation of the graph $G$ is passed as input to both the neural network and the loss calculation. In that sense, autoencoder methods use a unary encoder in the described encoder-decoder architecture. Then, after the network is trained, the matrix $Z$ is retrieved from the middle of the autoencoder network. The two methods: SDNE~\cite{SDNE} and DNGR~\cite{dngr}, besides slight architecture changes, differ primarily in the input matrix representation. SDNE uses the graph adjacency matrix $A$, and DNGR uses the Positive Pointwise Mutual Information matrix.


\paragraph{Structural similarity methods.}
In the previously described algorithms, each method aimed to capture the node similarity derived from the distance of the nodes in the graph. In contrast, the solutions belonging to the current class aim to capture some structural similarity, i.e., the similarity of the neighbourhoods of the vertices.
%
Struc2Vec~\cite{Struc2Vec} generates a multigraph capturing the structural similarity of the nodes and then trains a Skip-Gram network to generate representations similarly to Node2Vec. GraphWave~\cite{GraphWave} uses graph signal processing methods on the graph Laplacian and converts the signals to random distributions of energy transferred between nodes.
%



\subsection{Representation learning for community detection}

As the primary goal of our work is to develop a general framework for community detection, we have reviewed recent works that use representation learning tools to solve the task at hand. However, as the domain is relatively young, very few representation learning-based algorithms have been introduced so far~\cite{IPM5}. Below we present the most notable and representative ones.


\paragraph{vGraph: A generative model for joint community detection
and node representation learning~\cite{vgraph}.}
The presented solution -- vGraph -- is not precisely a representation-based community detection tool but a unified framework performing both community detection and node embedding. The authors heavily rely  on node proximity theory derived for LINE~\cite{LINE} and introduce changes that account for better community detection. We assume that the network may be described with two prior probability distributions:

\begin{itemize}
\setlength\itemsep{-0.1em}
    \item $p(z|w)$, which represents the community distribution, $z$, for a given node $w$,
    \item $p(c|z)$, which represents the node distribution, $c$, for a given community $z$.
\end{itemize}

Given the above distributions, the authors describe a generative process of the network as drawing edges based on the social context of the neighbours. That is, an edge $(w, c)$ is generated with the probability:
\begin{equation}
p(c|w) = \sum_z p(c|z)p(z|w).
\end{equation}

Given the above, we may estimate the probabilities over the network to detect communities. The probabilities for each node and community are parametrised with Euclidean representations of the nodes, which connects both tasks. A neural network is then trained to fit the probabilities to the observed network, resulting in both community detection and Euclidean node representations.

According to the authors' results, vGraph can achieve competitive results in terms of modularity maximisation and  state-of-the-art performance when used with supervised learning models for node classification. However, a possible drawback of the solution is that it allows for overlaps between communities, rendering it unsuitable for some use-cases.


\paragraph{Community detection based on graph representation
learning in evolutionary networks~\cite{lcden}.}
LCDEN aims to generate Euclidean node representations of temporal networks, i.e., whose structure changes over time. This trait prohibits us from using standard ``static'' community detection tools as the communities change with the graph's evolution. Its authors incorporate the same loss as the Laplacian Eigenmaps method to account for node information. However, to deal with the network evolution effectively, we assume time slices $t$ and the loss function given as:
\begin{equation}
L_1 = 2\tr\Big(Z_t^T L_{t-1} Z_t\Big).
\end{equation}

The authors use a deep sparse autoencoder network instead of performing a direct optimisation. This approach is motivated by the fact that sparse autoencoders may be less vulnerable to overfitting and still achieve satisfactory results for sparse networks due to the smaller volume of the overall structural information. To train the network, the authors employ an SE-based loss function with sparsely constrained parameters:
\begin{equation}
L_2 = \sum \| (x_i' - x_i) \cdot ( x_i (\beta - 1) + 1) \|_2^2,
\end{equation}
\noindent
where $x_i'$ and $x_i$ are the input and output representations of the node $i$, respectively, and $\beta$ is a tunable parameter. The  loss function is then given as $L_{min} = \theta L_1 + \gamma L_2$,
where $\theta$ and $\gamma$ are some regularisation parameters.



\paragraph{Structural deep clustering network~\cite{sdcn}.}
Structural Deep Clustering Network (SDCN) is a solution incorporating selected state-of-the-art deep learning frameworks for graph community detection. The primary motivation of the authors was based on two observations: that the autoencoder models tend to capture the possible community structure successfully and that Graph Convolutional Networks~\cite{gcn} outperform many other solutions in graph-related machine learning tasks. Therefore, the authors' idea is to combine the two components into one large solution for community detection. To achieve that, they train a Deep Neural Network Autoencoder (DNN) on the consecutive layers of the GCN. To ensure the coherence of the components, a Dual Self-Supervised Module is introduced, which guides the training of the networks. Finally, the $K$-means clustering is performed on the resulting Euclidean for direct community detection.
%
SDCN is currently one of the leading solutions for community detection in large networks, significantly outperforming other solutions, e.g., Variational Graph Autoencoders~\cite{vgae}.

\smallskip
We refer the reader to the recent survey \cite{su} for descriptions of other recent solutions, including deep learning models.




%
%
%


\section{Distance-based graph clustering framework}\label{sec:methodology}



\subsection{Hierarchical clustering}

Most hierarchical clustering methods are either divisive or agglomerative. DIANA~\cite{DIANA} is an example of a divisive algorithm: starting from a single cluster $C_1$, we find the data point having maximal average dissimilarity to all the others and move it to a new cluster $C_2$. Then, all points that are now, on average, more similar to $C_2$ than $C_1$ are moved to $C_2$. The algorithm progresses iteratively until a predefined stopping requirement is met.

However, agglomerative algorithms are more popular due to better computational complexity and thus lie at the heart of our paper. Assuming a data set of points in a Euclidean space $S = \{s_1, s_2, \dots, s_n\}$, we first partition $S$ such that each point belongs to its own cluster, i.e., we define a partition consisting of singletons, $C = \{C_1, C_2, \dots, C_n\}$. Then, we merge clusters iteratively until a stop criterion is met: the desired number of clusters was reached.
The merging strategy is what differentiates various algorithms. Denoting $d(s_i, s_j)$ as the selected distance measure from $s_i$ to $s_j$, we merge clusters $C_m$ and $C_n$ if they minimise a selected linkage criterion $D(C_m, C_n)$,
%
with the single (minimum), complete (maximum), average, and Ward (variance)
linkages being the most common choices; see, e.g., \cite{murtagh}.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%

%

%
%
%

%

%
%
%

%

%
%
%
%

It must be noted that hierarchical clustering methods may also be based on graphs. For example, we could generate a graph using $k$-nearest neighbours methods and define a criterion using the degrees of nodes~\cite{graphDegreeLinkage}.

The single-linkage criterion is relatively naïve and often does not yield suitable partitions. The remaining ones have higher time complexity. One way to bypass the complexity problem is to add additional requirements. For example, the Genie algorithm~\cite{genie,genieclust} uses a single-linkage criterion but simultaneously monitors the partition's Gini index value, which a measure of inequality of cluster sizes. Only the cluster of the smallest cardinality can be merged when the algorithm reaches a predefined threshold of the Gini index value.



\subsection{Proposed solution frameworks}

We focus on the following three main sources from which we can obtain the communities: the node dissimilarity matrices, the eigenvectors of node similarity matrices, and the Euclidean representations of the nodes. As each of the above sources requires a slightly different approach to obtain the final community detection, we developed three frameworks  which we detail below.



\begin{algorithm}[ht!]
\caption{The proposed generic approaches to community detection
via hierarchical clustering:
a) node dissimilarity-based,
b) spectral similarity-based,
c) representation-based.}\label{alg:alg123}
%
 \textbf{Input:} \\
 $G \text{ -- the input graph}$ \\
 $K \text{ -- the number of communities to detect}$ \\
%
 \textbf{Output:} \\
 $C_K \text{ -- the partitioning of the nodes of the graph into} K \text{ communities}$ \\
%
  \textbf{Algorithm:} \\
\begin{minipage}[t]{0.32\linewidth}
a)\\
$D \gets \mathit{Dissimilarity}(G)$\\
\end{minipage}
\begin{minipage}[t]{0.32\linewidth}
b)\\
$M \gets \mathit{Similarity}(G)$\\
$Z \gets \text{Matrix consisting of } K\\ \text{ eigenvectors of } M$\\
$D \gets \text{Matrix of pairwise distances}\\ \text{(Euclidean) between all rows in }Z$\\
\end{minipage}
\begin{minipage}[t]{0.32\linewidth}
c)\\
$Z \gets \mathit{GraphRepresentation}(G)$ \\
$D \gets \text{Matrix of pairwise distances}\\ \text{(Euclidean) between all rows in }Z$\\
\end{minipage}
\\
$C_1,\dots,C_N \gets \mathit{HierarchicalClustering}(D)$ \\
\textbf{return} $C_K$
\end{algorithm}





\paragraph{Node dissimilarity-based community detection.}
Hierarchical algorithms can work directly on the distance matrix of the sample. Therefore, in this approach, we compute the node dissimilarity matrix using one of the previously mentioned methods and generate the clustering based on this matrix. Algorithm~\ref{alg:alg123}a presents the outline of the method.





\paragraph{Eigenvector-based community detection.}
In the standard spectral community detection, the clustering is performed on the rows of the matrices consisting of the eigenvectors of the graph's Laplacian matrix. To generalise this approach, we substitute the Laplacian matrix with a node similarity matrix; see Algorithm~\ref{alg:alg123}b.




\paragraph{Representation-based community detection.}
In the third approach, we first generate vector representations of the nodes and then apply the hierarchical clustering directly onto the representations;
compare Algorithm~\ref{alg:alg123}c.


\paragraph{Components.}
We may notice that all three approaches are greatly modular, i.e., we may use any appropriate algorithm for calculating the similarity and dissimilarity matrices, the clustering, and the node vector representations. Therefore, to develop the best possible final solutions, we have tested many combinations of components listed in Table~\ref{tab:components}.

\begin{table}[t!]
    \caption{Framework components tested in our experiments.}
    \label{tab:components}
    \centering
    \begin{tabular}{p{5cm}|p{10.5cm}}
        Component type & Tested solutions \\
        \hline
        Similarity and dissimilarity measures\footnote{The measures are obtained from the respective (dis)similarity measure by a composition with a strictly decreasing function, if necessary.} & Wasserman--Faust measure~\cite{wassermanFaust}, Neighbourhood overlap~\cite{fortunato}, $K$-step walk probability~\cite{fortunato}, Katz index~\cite{katz}, Rooted PageRank~\cite{hope},  Adamic--Adar index~\cite{adamicAdar}, Blondel--Gajardo measure~\cite{blondel}, Positive Pointwise Mutual Information~\cite{dngr}. \\
        \hline
        Clustering algorithms & Single-linkage clustering,
            complete-linkage clustering, average-linkage clustering, Ward linkage clustering, Genie~\cite{murtagh,genie,genieclust}. \\
            \hline
        Representation learning algorithms & DeepWalk~\cite{DeepWalk}, DNGR~\cite{dngr}, Graph Factorisation~\cite{GF}, GraRep~\cite{grarep}, HARP~\cite{HARP}, HOPE~\cite{hope}, Laplacian Eigenmaps~\cite{LE}, Node2Vec~\cite{Node2Vec}.
    \end{tabular}
\end{table}

    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %





%
%
%

\section{Methodology and results}\label{sec:results}

\subsection{Experiment setup}\label{sec:expsetup}

We have noted that the number of possible method-parameter-clustering
algorithm combinations is overwhelming. Therefore, we need a systematic
approach that will enable us to identify the most promising community
detection procedures. Let us describe the benchmark battery, the assumed method parameters,
as well as the performance metrics that we examine in this section.

\paragraph{Benchmark dataset.}
To compare all the methods, we will exercise a community detection task on a
benchmark battery that consists of graphs generated via the well-known
stochastic block model~\cite{sbm}; see Figure~\ref{fig:sbm} for an illustration.

We generated ten graphs for each combination of parameters presented in  Table~\ref{tab:parametergrid}. This resulted in a total of $2{,}880$ synthetic graphs. As we will see later, some graphs will turn out not dense
enough (particularly those with few vertices per cluster) to guarantee the convergence of all the community detection approaches.






We should note that this benchmark battery consists of very regular graphs.
Therefore, it promotes community detection methods that specialise in
detecting such network types (e.g., the Louvain algorithm mentioned below). This is more or less an equivalent of detecting Gaussian blobs in a Euclidean space,
where we know that such algorithms as $k$-means and average or complete linkage
will outperform the more flexible (nonparametric) methods.

Also, the generated graphs are quite small. We must thus stress that
our purpose right now is merely to select the best
model-parameter-clustering algorithm combinations and to inspect their
run times. In the next section, we will run the best performing algorithms on
a few real-world datasets to get a picture from a slightly different angle -- namely, we conduct experiments on the Zachary's Karate Club~\cite{zachary} and Dolphins~\cite{dolphins} datasets.





\paragraph{Method parameters.}
Some of the selected similarity measures have additional parameters that must be set; see Table~\ref{tab:parametergrid} again. Also, while the classical hierarchical clustering algorithms do not have any modifiable parameters, we considered the values suggested in \cite{genie} for the Genie algorithm. Finally, we assumed the default values for all representation learning algorithms (as in the ReLeGy package for Python~\cite{relegy}) but the output dimension.


\paragraph{Evaluation.}
In the current scenario, we know the true community membership of each node
as it is directly implied by the Stochastic Block Model.
Thus, we can use the adjusted Rand score
(AR-index, see~\cite{HubertArabie1985:partitionscomp})
as a measure the clustering quality.
Assuming that ${M}$ is a confusion matrix,
where $m_{u,v}$ gives the number of elements in the $u$-th reference cluster
classified by an algorithm as members of the $v$-th community,
\begin{equation}
\text{AR-index}({M}) =
\frac{{n \choose 2} \sum_{u=1}^K\sum_{v=1}^K {m_{u,v}\choose 2} - \sum_{u=1}^K {m_{u,\boldsymbol{\cdot}}\choose 2}\sum_{v=1}^K {m_{\boldsymbol{\cdot},v}\choose 2}}
{\frac{1}{2}{n \choose 2}\Big(\sum_{u=1}^K {m_{u,\boldsymbol{\cdot}}\choose 2}+\sum_{v=1}^K {m_{\boldsymbol{\cdot},v}\choose 2}\Big) -\sum_{u=1}^K {m_{u,\boldsymbol{\cdot}}\choose 2}\sum_{v=1}^K {m_{\boldsymbol{\cdot},v}\choose 2}},
\end{equation}
\noindent
where $m_{u,\boldsymbol{\cdot}}=\sum_{v=1}^{K} m_{u,v}$
and   $m_{\boldsymbol{\cdot},v}=\sum_{u=1}^{K} m_{u,v}$.
The adjusted Rand score attains the values between a~soft lower limit equal to 0 and a hard upper limit equal to 1. A partition may yield values slightly lower than 0 if its performance is worse than a purely random assignment.
Naturally, many other measures exists (e.g., \cite{psi,aaa}), but the results they
yield tend to be highly correlated.

We shall also record the times to compute each graph model.
Note that on such small graphs, the time to apply the hierarchical clustering
algorithms on an already constructed distance matrix is negligible.


\begin{table}[t!]
    \caption{Parameter grid of all of the tested components.}
    \label{tab:parametergrid}
    \centering
    \begin{tabular}{l | l | l}
    Family of parameters & Parameter & Values \\
    \hline
    \multirow{4}{*}{Graph parameters} &
       Number of clusters & 3, 4, 5, 10 \\
       & Cluster size & 5, 10, 20 \\
       & Cluster in-density & 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 \\
       & Cluster out-density & 0.05, 0.10, 0.20, 0.30 \\
       \hline
    \multirow{5}{*}{Component parameters} & Length of $K$-step walk & 1, 3, 5 \\
    & Katz index decay parameter & 0.1, 0.3, 0.5 \\
    & Return probability of Rooted PageRank & 0.0, 0.1, 0.3 \\
    & Approximation limit of Blondel-Gajardo measure & 1, 10, 100 \\
    & PPMI continuation probability & 0.7, 0.9, 1.0 \\
    \hline
    Clustering algorithm parameters & Gini index threshold in the Genie algorithm & 0.1, 0.3, 0.5 \\
    \hline
    Representation parameters & Representation dimension & 6, 12
    \end{tabular}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\linewidth]{alpha0_05.png}
    \includegraphics[width=0.4\linewidth]{alpha0_2.png}

    \caption{\label{fig:sbm}
    Example graphs with three clusters generated
    by the stochastic block model (SBM) with the cluster out-density $0.05$
    (left subfigure) and $0.2$ (right subfigure).}
\end{figure}





\begin{table}[t!]
    \centering
    \caption{Average ($\pm$ standard deviation) Rand score and computation time of the reference community detection algorithms on the sample of $2{,}880$ test graphs.
%
    }
%

    \begin{tabular}{l|r|r}
    \hline
        \textbf{Community detection algorithm} & \textbf{Rand score} & \textbf{Time [s]} \\ \hline
        Greedy Modularity & 0.51$\pm $0.35 & 0.02$\pm $0.04 \\ \hline
        Label propagation & 0.15$\pm $0.26 & 0.00$\pm $0.00 \\ \hline
        Leiden & \textbf{0.66$\pm $0.37} & 0.00$\pm $0.00 \\ \hline
        Louvain & 0.64$\pm $0.37 & 0.01$\pm $0.02 \\ \hline
        Spectral & 0.29$\pm $0.23 & 0.20$\pm $0.22 \\ \hline
    \end{tabular}
    \label{tab:reference_results}
\end{table}



\subsection{Reference results}\label{sec:reference-algos}

As a point of reference, let us inspect the following five well-known algorithms: Greedy Modularity Optimisation~\cite{greedyModularity}, Label Propagation  \cite{labelpropagation}, the Leiden algorithm~\cite{leiden}, the Louvain algorithm~\cite{louvain}, and standard spectral community detection~\cite{Filippone}. The overall results for the reference methods are listed in~\autoref{tab:reference_results}.
%

In line with our expectations, the Leiden and Louvain algorithms achieved the best results, as they are considered state-of-the-art methods for community detection for ``regular'' graphs. Greedy Modularity Optimisation was also able to yield good results, albeit significantly worse than the mentioned former methods, while the Label propagation algorithm and spectral community detection performed rather unsatisfactorily. We may notice that all of the results are relatively low when compared with the theoretical maximal value of the Rand score equal to 1 -- this is due to the presence of particularly challenging test examples in our test set. Specifically, graphs with  low both in- and out-density, e.g., in-density equal to 0.4 and out-density equal to 0.3, pose a challenge even for the best community detection algorithms. Therefore, we expect that the average performance of any method will be quite low, and we focus on a pairwise relative comparison of the methods.

When it comes to the run-time comparison, most of the methods generate the community partitioning almost instantaneously, with the spectral community detection being the only exception.
%



%

%
%
%



\begin{table}[t!]
    \centering
    \caption{Aggregated Rand scores for the considered node dissimilarity-based community detection methods with the Genie clustering. If a certain combination is not reported, it means that the results were unobtainable due to numerical instabilities. ``Parameter'' denotes the dissimilarity-specific parameter, as listed in Section \ref{sec:expsetup}. Only the best-performing combinations, with respect to the dissimilarity parameter, are reported.
    }
%
    \begin{minipage}[t]{0.49\linewidth}
        \begin{tabular}{l|r|r|rH}
    \hline
        \textbf{Dissimilarity} & \textbf{Gini} & \textbf{Param.} & \textbf{Rand score} \\ \hline
%
%
%
        Blondel--Gajardo &  0.3 & 100 & 0.02$\pm $0.02 & 0.02$\pm $0.03 \\ \hline
        Blondel--Gajardo &  0.5 & 100 & 0.02$\pm $0.02 & 0.02$\pm $0.03 \\ \hline
%
%
%
        K-step probability &  0.1 & 3 & 0.48$\pm $0.37 & 0.03$\pm $0.05 \\ \hline
        K-step probability &  0.3 & 3 & 0.44$\pm $0.37 & 0.03$\pm $0.05 \\ \hline
        K-step probability &  0.5 & 3 & 0.35$\pm $0.34 & 0.03$\pm $0.05 \\ \hline
%
%
%
        Katz index & 0.1 & 0.1 & 0.30$\pm $0.34 & 0.03$\pm $0.06 \\ \hline
        Katz index & 0.3 & 0.1 & 0.25$\pm $0.30 & 0.03$\pm $0.04 \\ \hline
        Katz index & 0.5 & 0.1 & 0.17$\pm $0.24 & 0.02$\pm $0.04 \\ \hline
%
%
%
        Overlap & 0.1 & - & 0.44$\pm $0.40 & 0.11$\pm $0.19 \\ \hline
        \end{tabular}
    \end{minipage}
    \begin{minipage}[t]{0.49\linewidth}
        \begin{tabular}{l|r|r|rH}
    \hline
        \textbf{Dissimilarity} & \textbf{Gini} & \textbf{Param.} & \textbf{Rand score} \\ \hline
        Overlap & 0.3 & - & 0.42$\pm $0.40 & 0.11$\pm $0.19 \\ \hline
        Overlap & 0.5 & - & 0.36$\pm $0.38 & 0.11$\pm $0.18 \\ \hline
%
%
%
        PPMI & 0.1 & 1.0 & 0.50$\pm $0.37 & 0.03$\pm $0.04 \\ \hline
        PPMI & 0.3 & 1.0 & 0.43$\pm $0.36 & 0.03$\pm $0.04 \\ \hline
        PPMI & 0.5 & 1.0 & 0.33$\pm $0.34 & 0.03$\pm $0.04 \\ \hline
        Rooted PageRank & 0.1 & 0.3 & 0.37$\pm $0.29 & 0.12$\pm $0.16 \\ \hline
%
%
%
        Wasserman--Faust & 0.1 & - & \textbf{0.52$\pm $0.39} & 0.13$\pm $0.23 \\ \hline
        Wasserman--Faust & 0.3 & - & 0.47$\pm $0.39 & 0.13$\pm $0.22 \\ \hline
        Wasserman--Faust & 0.5 & - & 0.37$\pm $0.37 & 0.13$\pm $0.23 \\ \hline
    \end{tabular}
    \end{minipage}
    \label{tab:node_genie}
\end{table}


\begin{table}[t!]
    \centering
    \caption{Aggregated Rand scores for  the considered node dissimilarity-based community detection methods with the remaining clustering methods.
    %
    }

%
\begin{minipage}[t]{0.49\linewidth}
    \begin{tabular}{l|l|r|rH}
    \toprule
        \textbf{Dissimilarity} & \textbf{Clustering} & \textbf{Par.} & \textbf{Rand score} \\ \hline
        Adamic--Adar & Average & - & 0.45$\pm $0.43 & 0.01$\pm $0.01 \\ \hline
        Adamic--Adar & Complete  & - & 0.35$\pm $0.43 & 0.01$\pm $0.01 \\ \hline
        Adamic--Adar & Single  & - & 0.16$\pm $0.32 & 0.04$\pm $0.06 \\ \hline
        Adamic--Adar & Ward  & - & 0.53$\pm $0.40 & 0.01$\pm $0.01 \\ \hline
        Blondel--Gajardo & Average  & 1 & 0.00$\pm $0.01 & 0.01$\pm $0.01 \\ \hline
        Blondel--Gajardo & Complete  & 1 & 0.00$\pm $0.01 & 0.01$\pm $0.01 \\ \hline
        Blondel--Gajardo & Single  & 1 & 0.00$\pm $0.00 & 0.01$\pm $0.01 \\ \hline
        Blondel--Gajardo & Ward  & 1 & 0.01$\pm $0.02 & 0.01$\pm $0.01 \\ \hline
        K-step probability & Average  & 3 & 0.55$\pm $0.37 & 0.01$\pm $0.01 \\ \hline
        K-step probability & Complete  & 3 & 0.52$\pm $0.37 & 0.01$\pm $0.01 \\ \hline
        K-step probability & Single  & 3 & 0.24$\pm $0.37 & 0.01$\pm $0.01 \\ \hline
        K-step probability & Ward  & 3 & 0.61$\pm $0.33 & 0.01$\pm $0.01 \\ \hline
        Katz index & Average  & 0.1 & 0.33$\pm $0.36 & 0.01$\pm $0.01 \\ \hline
        Katz index & Complete  & 0.1 & 0.34$\pm $0.35 & 0.01$\pm $0.01 \\ \hline
        Katz index & Single  & 0.1 & 0.07$\pm $0.19 & 0.01$\pm $0.03 \\ \hline
        Katz index & Ward  & 0.1 & 0.42$\pm $0.38 & 0.01$\pm $0.01 \\ \bottomrule
        \end{tabular}
        \end{minipage}
        \begin{minipage}[t]{0.49\linewidth}
            \begin{tabular}{l|l|r|rH}
            \toprule
        \textbf{Dissimilarity} &
        \textbf{Clustering} & \textbf{Par.} & \textbf{Rand score} \\ \hline
        Overlap & Average  & - & 0.50$\pm $0.41 & 0.09$\pm $0.16 \\ \hline
        Overlap & Complete  & - & 0.34$\pm $0.43 & 0.09$\pm $0.16 \\ \hline
        Overlap & Single  & - & 0.26$\pm $0.39 & 0.09$\pm $0.16 \\ \hline
        Overlap & Ward  & - & 0.52$\pm $0.40 & 0.09$\pm $0.16 \\ \hline
        PPMI & Average  & 1 & \textbf{0.63$\pm $0.36} & 0.01$\pm $0.01 \\ \hline
        PPMI & Complete  & 1 & 0.31$\pm $0.42 & 0.01$\pm $0.01 \\ \hline
        PPMI & Single  & 1 & 0.22$\pm $0.35 & 0.01$\pm $0.01 \\ \hline
        PPMI & Ward  & 1 & 0.62$\pm $0.36 & 0.01$\pm $0.01 \\ \hline
        Rooted PageRank & Average  & 0.3 & 0.58$\pm $0.33 & 0.01$\pm $0.01 \\ \hline
        Rooted PageRank & Complete  & 0.3 & 0.54$\pm $0.33 & 0.01$\pm $0.01 \\ \hline
        Rooted PageRank & Single  & 0.3 & 0.08$\pm $0.19 & 0.02$\pm $0.04 \\ \hline
        Rooted PageRank & Ward  & 0.3 & 0.56$\pm $0.33 & 0.01$\pm $0.01 \\ \hline
        Wasserman--Faust & Average  & - & 0.52$\pm $0.41 & 0.11$\pm $0.19 \\ \hline
        Wasserman--Faust & Complete  & - & 0.54$\pm $0.39 & 0.11$\pm $0.20 \\ \hline
        Wasserman--Faust & Single  & - & 0.23$\pm $0.38 & 0.11$\pm $0.20 \\ \hline
        Wasserman--Faust & Ward  & - & 0.60$\pm $0.37 & 0.11$\pm $0.19 \\ \bottomrule
    \end{tabular}
    \end{minipage}
    \label{tab:node_hier}
\end{table}

%





\subsection{Node dissimilarity-based community detection}

Let us proceed with the analysis of the overall results of the node dissimilarity-based methods. Tables~\ref{tab:node_genie} and~\ref{tab:node_hier} present the results for the Genie-based implementations and the remaining hierarchical methods, respectively.

The Wasserman--Faust dissimilarity produces the best results when combined with the Genie clustering, but the PPMI-based dissimilarity and the K-step transition probability-based dissimilarity allow the method to achieve relatively high results as well. Overall, only the Blondel--Gajardo matrix seems unsuitable for the dissimilarity-based community detection, as it results in the same performance as random partitioning.

When it comes to the parameters of the Genie clustering, we observe that all of the methods work better for lower values of the inequity threshold. As our test graphs have equal-sized communities, this is expected -- any noticeable inequity in the cluster sizes indicates an error in the clustering, and the algorithms aim to fix the error during the process by merging minimum communities.

%

%

Analogously, all algorithms have a single parameter value for which they perform the best, regardless of the selected inequity measure and the threshold value. Interestingly, the dissimilarity based on K-step transition probability yields the best results for the lowest value of K, i.e., for the shortest theoretical random walks. This observation suggests that local neighbourhood-sourced dissimilarity conveys the community structure better than the global neighbourhood-sourced one, as would be the case for higher values of K. Similarly, the PPMI-based dissimilarity works best with the continuation probability equal to 1.0, i.e., when a random walk does not randomly return to the source. This result is consistent with intuition, as random returns to the source should only cause undesirable disruptions in conveying the graph's structure.

By looking at the remaining hierarchical clustering algorithms in Table~\ref{tab:node_hier}, we observe that the results are relatively consistent, i.e., methods which performed well with Genie clustering also perform well with linkage criteria. In most cases, the average and Ward linkage criteria generate the best results, which is also consistent with our expectations, as they should capture the community similarities the best. We also observe the high performance of the average linkage with PPMI-based dissimilarity, which yields the best performance amongst all node dissimilarity-based methods. We believe it may compete with the state-of-the-art Louvain and Leiden algorithms.

The time comparison shows that most algorithms can generate results almost as quickly as the reference methods. The only exceptions are the methods based on the Wasserman--Faust  and the Overlap dissimilarity. This is because these are the only measures for which the dissimilarity matrix must be calculated element-by-element. We can obtain the dissimilarity matrix with algebraic transformations of other graph-related matrices in the remaining cases.

%







\subsection{Spectral-based community detection}

In this section, we summarise the results of our spectral-based community methods, which are present in Tables~\ref{tab:spectral_genie} and~\ref{tab:spectral_hier}, analogously to the previous section.

None of the methods achieved satisfactory results when compared with our node dissimilarity-based as well as the reference methods. However, this is a somewhat expected phenomenon. When the clustering is performed on the eigenvectors of the graph's Laplacian matrix, the standard spectral community detection has a~proper theoretical justification. Our replacements of the Laplacian matrix do not have the same theoretical justification, but they are still heuristics worth considering.


%
Most methods gave the same performance quality as a random partitioning -- the only exceptions here are the K-step transition probability matrix and the Katz index matrix. More interestingly, the transition matrix's best parameter value changed from 3 to 5. While we cannot directly deduce why such a change occurred or how the random walk length contributes to the final clustering, it implies that K remains a tunable parameter that impacts the final performance. We repeat the same conclusion for the Katz decay parameter, for whom the best decay parameter value also changed from 0.1 to 0.3.

If we were to try to explain the behaviour of the transition probability-based method, we could notice one fundamental similarity to the Laplacian matrix used for the standard spectral community detection: both the K-step probability matrix and the Laplacian matrix are row-stochastic.
While this reasoning has a firm basis, to the best of the authors' knowledge, this is not the case for the Katz index matrix.
%
Nonetheless, we believe that further research into these two methods can be profitable in terms of understanding spectral community detection better and may yield a new well-performing spectral algorithm in the future.

When we compare the computation time of the methods,
%
only the Overlap and Wasserman--Faust similarity-based methods do not produce the results almost instantaneously due to the need for element-by-element computation. Interestingly, the standard Laplacian matrix can also be obtained from the graph's adjacency and degree matrix in linear time with respect to the number of vertices. However, when we measure the computation time of the reference spectral method, it requires time similar to the Wasserman--Faust and Overlap-based methods, which in our case is quadratic with respect to the number of vertices. This observation indicates a possibility for improvement in the reference implementation.






\begin{table}[t!]
    \centering
        \caption{Aggregated Rand scores for the considered spectral-based community detection methods with Genie clustering.
%
        }
%
    \begin{minipage}[t]{0.49\linewidth}
        \begin{tabular}{l|r|r|rH}
    \hline
        \textbf{Similarity} & \textbf{Gini} & \textbf{Param.} & \textbf{Rand score}\\ \hline
        Adamic--Adar & 0.1 & - & -0.02$\pm $0.04 & 1.28$\pm $1.76 \\ \hline
        Adamic--Adar & 0.3 & - & -0.01$\pm $0.03 & 1.55$\pm $2.21 \\ \hline
        Adamic--Adar & 0.5 & - & -0.01$\pm $0.03 & 1.60$\pm $2.15 \\ \hline
        Blondel--Gajardo & 0.1 & 1 & 0.03$\pm $0.06 & 0.10$\pm $0.38 \\ \hline
        Blondel--Gajardo & 0.3 & 1 & 0.02$\pm $0.05 & 0.80$\pm $1.83 \\ \hline
        Blondel--Gajardo & 0.5 & 1 & 0.01$\pm $0.04 & 0.50$\pm $1.25 \\ \hline
        K-step probability & 0.1 & 5 & \textbf{0.20$\pm $0.17} & 1.44$\pm $2.00 \\ \hline
        K-step probability & 0.3 & 5 & 0.18$\pm $0.16 & 1.69$\pm $2.40 \\ \hline
        K-step probability & 0.5 & 5 & 0.13$\pm $0.13 & 1.11$\pm $1.67 \\ \hline
        Katz index & 0.1 & 0.3 & 0.19$\pm $0.32 & 1.03$\pm $1.61 \\ \hline
        Katz index & 0.3 & 0.3 & 0.18$\pm $0.31 & 1.32$\pm $1.93 \\ \hline
        Katz index & 0.5 & 0.3 & 0.15$\pm $0.28 & 1.02$\pm $1.56 \\ \hline
        \end{tabular}
    \end{minipage}
    \begin{minipage}[t]{0.49\linewidth}
        \begin{tabular}{l|r|r|rH}
    \hline
        \textbf{Similarity} & \textbf{Gini} & \textbf{Param.} & \textbf{Rand score} & \textbf{Time [s]} \\ \hline
        Overlap & 0.1 & - & 0.00$\pm $0.05 & 2.22$\pm $2.61 \\ \hline
        Overlap & 0.3 & - & 0.00$\pm $0.04 & 0.91$\pm $1.41 \\ \hline
        Overlap & 0.5 & - & 0.00$\pm $0.03 & 0.89$\pm $1.50 \\ \hline
        PPMI & 0.1 & 1.0 & 0.01$\pm $0.06 & 0.65$\pm $1.43 \\ \hline
        PPMI & 0.3 & 1.0 & 0.01$\pm $0.06 & 0.17$\pm $0.63 \\ \hline
        PPMI & 0.5 & 1.0 & 0.01$\pm $0.04 & 0.70$\pm $1.76 \\ \hline
        Rooted PageRank & 0.1 & 0.0 & 0.02$\pm $0.02 & 1.44$\pm $2.22 \\ \hline
        Rooted PageRank & 0.3 & 0.0 & 0.02$\pm $0.02 & 2.03$\pm $2.92 \\ \hline
        Rooted PageRank & 0.5 & 0.0 & 0.02$\pm $0.02 & 1.16$\pm $2.08 \\ \hline
        Wasserman--Faust & 0.1 & - & 0.04$\pm $0.08 & 0.63$\pm $0.82 \\ \hline
        Wasserman--Faust & 0.3 & - & 0.04$\pm $0.07 & 0.63$\pm $0.79 \\ \hline
        Wasserman--Faust & 0.5 & - & 0.03$\pm $0.06 & 0.58$\pm $0.79 \\ \hline
    \end{tabular}
    \end{minipage}
    \label{tab:spectral_genie}
\end{table}

\begin{table}[t!]
    \centering
        \caption{Aggregated Rand scores for the considered spectral-based community detection methods with the remaining clustering methods.
%
        }
%
    \begin{minipage}[t]{0.49\linewidth}
        \begin{tabular}{l|l|r|rH}
    \hline
        \textbf{Similarity} & \textbf{Clustering} & \textbf{Par.} & \textbf{Rand score} \\ \hline
        Adamic--Adar & Average & - & -0.01$\pm $0.03 & 0.00$\pm $0.02 \\ \hline
        Adamic--Adar & Complete & - & -0.02$\pm $0.03 & 0.00$\pm $0.02 \\ \hline
        Adamic--Adar & Single & - & 0.00$\pm $0.01 & 0.00$\pm $0.02 \\ \hline
        Adamic--Adar & Ward & - & -0.02$\pm $0.03 & 0.00$\pm $0.02 \\ \hline
        Blondel--Gajardo & Average & 1 & 0.01$\pm $0.03 & 0.00$\pm $0.02 \\ \hline
        Blondel--Gajardo & Complete & 1 & 0.02$\pm $0.04 & 0.00$\pm $0.02 \\ \hline
        Blondel--Gajardo & Single & 1 & 0.01$\pm $0.02 & 0.00$\pm $0.02 \\ \hline
        Blondel--Gajardo & Ward & 1 & 0.02$\pm $0.04 & 0.00$\pm $0.02 \\ \hline
        K-step probability & Average & 5 & 0.03$\pm $0.06 & 0.00$\pm $0.01 \\ \hline
        K-step probability & Complete & 5 & 0.09$\pm $0.10 & 0.00$\pm $0.01 \\ \hline
        K-step probability & Single & 5 & 0.01$\pm $0.03 & 0.01$\pm $0.01 \\ \hline
        K-step probability & Ward & 5 & 0.16$\pm $0.15 & 0.00$\pm $0.01 \\ \hline
        Katz index & Average & 0.3 & 0.18$\pm $0.32 & 0.00$\pm $0.00 \\ \hline
        Katz index & Complete & 0.3 & 0.17$\pm $0.31 & 0.00$\pm $0.00 \\ \hline
        Katz index & Single & 0.3 & 0.13$\pm $0.27 & 0.00$\pm $0.00 \\ \hline
        Katz index & Ward & 0.3 & \textbf{0.19$\pm $0.32} & 0.00$\pm $0.00 \\ \hline
        \end{tabular}
    \end{minipage}
    \begin{minipage}[t]{0.49\linewidth}
        \begin{tabular}{l|l|r|rH}
    \hline
        \textbf{Similarity} & \textbf{Clustering} & \textbf{Par.} & \textbf{Rand score} \\ \hline
        Overlap & Average & - & 0.00$\pm $0.02 & 0.18$\pm $0.31 \\ \hline
        Overlap & Complete & - & -0.01$\pm $0.03 & 0.18$\pm $0.30 \\ \hline
        Overlap & Single & - & 0.00$\pm $0.01 & 0.18$\pm $0.31 \\ \hline
        Overlap & Ward & - & -0.01$\pm $0.03 & 0.17$\pm $0.30 \\ \hline
        PPMI & Average & 1.0 & 0.00$\pm $0.02 & 0.00$\pm $0.01 \\ \hline
        PPMI & Complete & 1.0 & 0.00$\pm $0.03 & 0.01$\pm $0.01 \\ \hline
        PPMI & Ward & 1.0 & 0.00$\pm $0.03 & 0.00$\pm $0.01 \\ \hline
        Rooted PageRank & Average & 0.0 & 0.02$\pm $0.02 & 0.00$\pm $0.02 \\ \hline
        Rooted PageRank & Complete & 0.0 & 0.02$\pm $0.02 & 0.00$\pm $0.02 \\ \hline
        Rooted PageRank & Single & 0.0 & 0.02$\pm $0.02 & 0.00$\pm $0.02 \\ \hline
        Rooted PageRank & Ward & 0.0 & 0.02$\pm $0.02 & 0.00$\pm $0.02 \\ \hline
        Wasserman--Faust & Average & - & 0.00$\pm $0.01 & 0.23$\pm $0.40 \\ \hline
        Wasserman--Faust & Complete & - & 0.00$\pm $0.02 & 0.22$\pm $0.39 \\ \hline
        Wasserman--Faust & Single & - & 0.00$\pm $0.01 & 0.23$\pm $0.39 \\ \hline
        Wasserman--Faust & Ward & - & 0.00$\pm $0.03 & 0.22$\pm $0.39 \\ \bottomrule
        \multicolumn{4}{c}{}
    \end{tabular}
    \end{minipage}

    \label{tab:spectral_hier}
\end{table}

%





\begin{table}[!t]
    \centering
            \caption{Aggregated Rand score and computation times for the considered representation-based community detection methods with the Genie clustering.
%
        }
%
    \begin{minipage}[t]{0.49\linewidth}
        \begin{tabular}{l|r|r|r|r}
    \hline
        \textbf{Method}  & \textbf{Gini} & \textbf{Dim.} & \textbf{Rand score} & \textbf{Time [s]} \\ \hline
        DNGR     & 0.1 & 6 & \textbf{0.57$\pm $0.39} & \multirow{3}{*}{0.87--17.45} \\
        DNGR     & 0.3 & 6 & 0.54$\pm $0.39 & \\
        DNGR     & 0.5 & 6 & 0.47$\pm $0.39 & \\
        \hline
        DeepWalk & 0.1 & 6 & 0.47$\pm $0.37 & \multirow{3}{*}{0.44--4.85} \\
        DeepWalk & 0.3 & 6 & 0.43$\pm $0.35 & \\
        DeepWalk & 0.5 & 6 & 0.34$\pm $0.32 & \\ \hline
        GraRep   & 0.1 & 6 & 0.41$\pm $0.40 & \multirow{3}{*}{0.00--1.53} \\
        GraRep   & 0.3 & 6 & 0.36$\pm $0.38 & \\
        GraRep   & 0.5 & 6 & 0.26$\pm $0.33 & \\ \hline
        HOPE     & 0.1 & 6 & 0.30$\pm $0.32 & 0.00--2.81 \\ \hline
        \end{tabular}
    \end{minipage}
    \begin{minipage}[t]{0.49\linewidth}
        \begin{tabular}{l|r|r|r|r}
    \hline
        \textbf{Method}  & \textbf{Gini} & \textbf{Dim.} & \textbf{Rand score} & \textbf{Time [s]} \\ \hline
        GF  & 0.1 & 12 & 0.41$\pm $0.36 & \multirow{3}{*}{1.18--2.81} \\
        GF  & 0.3 & 12 & 0.36$\pm $0.35 & \\
        GF  & 0.5 & 12 & 0.27$\pm $0.30 & \\ \hline
        HARP     & 0.1 & 6 & 0.41$\pm $0.36 & \multirow{3}{*}{1.27--17.38} \\
        HARP     & 0.3 & 6 & 0.37$\pm $0.34 & \\
        HARP     & 0.5 & 6 & 0.28$\pm $0.31 & \\ \hline
        Node2Vec & 0.1 & 6 & 0.47$\pm $0.37 & \multirow{3}{*}{0.48--6.66} \\
        Node2Vec & 0.3 & 6 & 0.43$\pm $0.35 & \\
        Node2Vec & 0.5 & 6 & 0.33$\pm $0.32 & \\ \hline
        \multicolumn{5}{c}{}
    \end{tabular}
    \end{minipage}

    \label{tab:euclid_genie}
\end{table}

\begin{table}[t!]
    \centering
            \caption{Aggregated Rand scores and computation times for the considered representation-based community detection methods with the remaining clustering methods.
%
}
%
    \begin{minipage}[t]{0.49\linewidth}
        \begin{tabular}{l|l|r|r|r}
    \hline
        \textbf{Method} & \textbf{Clust.} & \textbf{D.} & \textbf{Rand s.} & \textbf{Time [s]} \\ \hline
        DNGR & Average & 6 & 0.53$\pm $0.40 & \multirow{4}{*}{0.83--20.82} \\
        DNGR & Complete & 6 & 0.54$\pm $0.38 &\\
        DNGR & Single & 6 & 0.39$\pm $0.41 & \\
        DNGR & Ward & 6 & \textbf{0.59$\pm $0.39} & \\ \hline
        DeepWalk & Average & 6 & 0.42$\pm $0.38 & \multirow{4}{*}{0.42--4.85} \\
        DeepWalk & Complete & 6 & 0.44$\pm $0.35 &\\
        DeepWalk & Single & 6 & 0.20$\pm $0.32 &\\
        DeepWalk & Ward & 6 & 0.51$\pm $0.36 & \\ \hline
        GraRep & Average & 6 & 0.25$\pm $0.37 & \multirow{4}{*}{0.00--1.10}\\
        GraRep & Complete & 6 & 0.31$\pm $0.34 &\\
        GraRep & Single & 6 & 0.13$\pm $0.29 &\\
        GraRep & Ward & 6 & 0.44$\pm $0.40 & \\ \hline
        GF & Average & 12 & 0.28$\pm $0.34 & \multirow{4}{*}{1.17--3.21} \\
        GF & Complete & 12 & 0.34$\pm $0.34 &\\
        GF & Single & 12 & 0.16$\pm $0.28 &\\
        GF & Ward & 12 & 0.41$\pm $0.35 & \\ \hline
        \end{tabular}
    \end{minipage}
    \begin{minipage}[t]{0.49\linewidth}
        \begin{tabular}{l|l|r|r|r}
    \hline
        \textbf{Method} & \textbf{Clust.} & \textbf{D.} & \textbf{Rand s.} & \textbf{Time [s]} \\ \hline
        HARP & Average  & 6 & 0.35$\pm $0.37 & \multirow{4}{*}{1.25--17.91} \\
        HARP & Complete  & 6 & 0.38$\pm $0.34 &\\
        HARP & Single  & 6 & 0.17$\pm $0.30 &\\
        HARP & Ward  & 6 & 0.44$\pm $0.36 &\\ \hline
        HOPE & Average  & 6 & 0.13$\pm $0.20 & \multirow{4}{*}{0.00--1.13} \\
        HOPE & Complete  & 12 & 0.14$\pm $0.16 &\\
        HOPE & Single  & 6 & 0.06$\pm $0.17 &\\
        HOPE & Ward  & 12 & 0.29$\pm $0.31 & \\ \hline
        LE & Average  & 6 & 0.04$\pm $0.12 & \multirow{4}{*}{0.18--334.79} \\
        LE & Complete  & 6 & 0.06$\pm $0.15 &\\
        LE & Single  & 6 & 0.02$\pm $0.09 &\\
        LE & Ward  & 6 & 0.08$\pm $0.17 & \\ \hline
        Node2Vec & Average  & 6 & 0.45$\pm $0.38 &\multirow{4}{*}{0.48--6.90} \\
        Node2Vec & Complete  & 6 & 0.45$\pm $0.35 &\\
        Node2Vec & Single  & 6 & 0.21$\pm $0.32 &\\
        Node2Vec & Ward  & 6 & 0.52$\pm $0.36 &  \\ \hline
    \end{tabular}
    \end{minipage}


    \label{tab:euclid_hier}
\end{table}





\subsection{Representation-based community detection}

Finally, we consider the representation-based community detection methods. The results are presented in Tables~\ref{tab:euclid_genie} and~\ref{tab:euclid_hier}, analogously to the previous sections.

We observe that DNGR-based representations yield the best results.
DNGR paired with a suitable clustering algorithm is able to achieve only a slightly lower Rand score than the state-of-the-art methods, which we believe is entirely satisfactory.
On the other hand, most of the remaining representation learning algorithms give mediocre results. While they outperformed the Label Propagation Algorithm and spectral clustering, they are far behind the Leiden and Louvain algorithms. Among the well-performing algorithms, almost all worked better with six-dimensional representations, which may be explained by the relatively small size of the test graphs. Higher dimensions could be required for larger graphs, where the representation algorithms cannot correctly encode the pairwise node relations.

Comparing the performance as a function of the Gini index threshold and parameter values, we observe the same dependence as in the previous examples: the algorithms work better for lower threshold values, and the best parameter value is independent of the threshold value.

Regarding the computation times, we observe a somewhat expected dependence -- factorisation-based algorithms (GraRep, HOPE) generate a partitioning almost instantaneously, while the algorithms requiring explicit optimisation of a given task or training of an underlying neural network need much more time.




\section{Detailed evaluation}\label{sec:results-details}

Based on the results presented in the previous section,
we select the following best-performing combinations of methods for further, more detailed analysis:
\vspace{-0.2cm}
\begin{itemize}
\setlength\itemsep{-0.1em}
    \item \textit{Node Genie PPMI} -- node dissimilarity-based Genie with PPMI-based dissimilarity, the Gini index threshold equal to 0.1, and continuation probability equal to 1,
    \item \textit{Node Genie W-F} -- node dissimilarity-based Genie with Wasserman--Faust measure-based dissimilarity, and the Gini index threshold equal to 0.1,
    \item \textit{Node Average PPMI} -- node dissimilarity-based average linkage with PPMI-based dissimilarity, and continuation probability equal to 1,
    \item \textit{Node Average RPR} -- node dissimilarity-based average linkage with Rooted PageRank-based dissimilarity, and return probability equal to 0.3,
    \item \textit{Spectral Genie K-Step} -- spectral-based Genie with K-step probability-based similarity, threshold equal to 0.1, and number of steps equal to 5,
    \item \textit{Spectral Ward Katz} -- spectral-based Ward linkage with Katz index-based similarity, and decay parameter equal to 0.3,
    \item \textit{Euclidean Genie DNGR} -- representation-based Genie with DNGR representation, threshold equal to 0.1, and embedding dimension equal to 6,
    \item \textit{Euclidean Ward DNGR} -- representation-based Ward linkage with DNGR representation, and embedding dimension equal to 6,
    \item \textit{Euclidean Ward Node2Vec} -- representation-based Ward linkage with Node2Vec representation, and embedding dimension equal to 6.
\end{itemize}

We include the spectral-based methods because while they performed relatively poorly, their analysis is rather interesting based on their observed behaviour.
%


Let us compare the nine selected methods against the five reference methods listed in Section \ref{sec:reference-algos}. Specifically, we study the  performance for various values of the in- and out-densities of the clusters, the performance for various values of the vertex and the cluster number of the graph, and the computation time.







\subsection{Graph density-based comparison}

Firstly, we analyse how our methods behave with respect to various values of in- and out-densities of the clusters in the graph. Figures \ref{fig:in_my_rand} and \ref{fig:in_ref_rand} present the results for the selected new and the reference methods, respectively.

All node dissimilarity-based and representation-based algorithms behave according to our expectations. They tend to perform better for higher in-densities and lower out-densities of the clusters. We observe that these methods achieve a near-perfect score in the most uncomplicated cases and then gradually decrease in performance as the task complexity increases. Notably, Node Genie PPMI and Node Genie W-F seem to perform relatively poorly for the edge cases, when both in- and out-densities are low -- the remaining methods result in the Rand score equal to at least 0.6, while these two produce 0.46 and 0.33, respectively.

Overall, we believe that the most noteworthy results are those related to the spectral-based methods. First, the Spectral Ward Katz seems to work in the same manner as the previously mentioned methods but scored significantly worse. On the other hand, Spectral Genie K-step behaves significantly different -- it produces better results when the in-densities are low, and the out-densities are high. Let us stress again, however, that detecting communities with in-densities equal to 0.4 and out-densities equal to 0.3 is exceptionally challenging, as the communities are nearly indistinguishable from their neighbourhood. Therefore, while the Rand score remains low, it is particularly of interest that Spectral Genie K-step outperformed all of the methods, including the reference methods.

Also, looking at the reference methods, we see that the general pattern still holds. The only algorithm that perhaps works somewhat differently is Label Propagation, whose performance seems to depend more firmly on the clusters' out-densities and less on in-densities. Nonetheless, it performed somewhat poorly, so the grounds for these observations may be unreliable.

In general, we believe that our best method, Node Average PPMI, achieves as good a performance as the best reference methods, i.e., the Louvain and Leiden algorithms: the differences are hardly noticeable. We should also note that our method results in a slightly lower standard deviation of the Rand score for best cases, i.e., that our method correctly detects communities in a more stable manner.


\subsection{Cluster structure-based comparison}

Secondly, we compare our methods and the reference algorithms with respect to the numbers of  vertices in the clusters and the number of clusters. The results are presented in Figures~\ref{fig:cnvn_my_rand} and \ref{fig:cnvn_ref_rand}, respectively.

Similarly as in the previous case, if we leave out the spectral-based methods, there are configurations for which the algorithms work better in general. Specifically, all these routines tend to fit better to a low number of large communities. This behaviour is consistent with our expectations, as such configurations are intuitively the easiest to detect: there is a low number of sparse regions in the graph separating the communities, and the communities themselves are clearly distinguishable due to their sizes.

We also once again observe that the spectral-based methods behave in a significantly different manner. Spectral Ward Katz seems to give good results only for some values of the number of vertices and the number of clusters. While this is quite challenging to explain without further analysis reliably, we expect that the parameters of the matrix generation seem to impact the method's performance significantly for various parameters of the graph. This behaviour is somewhat discouraging when applying this method to an arbitrary dataset, as the results may vary heavily, and we would have no way to distinguish the best partitioning into communities properly. On the other hand, Spectral Genie K-step once again seems to work better in the edge case, where the remaining algorithms work poorly. Specifically, it obtains an almost acceptable Rand score for a large number of big clusters; that is, it can distinguish large communities even when there are many. In this case, it works as good as, e.g., Node Genie PPMI and is better than Greedy Modularity Optimisation but significantly worse than the best algorithms.

If we compare our best method, Node Average PPMI, with the Leiden and Louvain algorithms, we observe that it performs slightly worse in most cases but obtains the best results for many small clusters. As this is the most challenging case, the results suggest that our method may be preferred for some datasets with many communities.


\subsection{Real-world datasets}

In the last part of the analysis of the results, we compare the performance of our selected methods and reference methods on two previously described datasets, i.e., Zachary's Karate Club~\cite{zachary} and Lusseau's Bottlenose Dolphins~\cite{dolphins}. As the Dolphins dataset may be partitioned into communities in several ways and there is no one agreed-upon partitioning, we use modularity to evaluate the community detection. The results are presented in Table~\ref{tab:real_results}.

When we compare the results on the Karate dataset, we observe that four of our methods significantly outperformed all of the reference methods. It is interesting that the state-of-the-art algorithms, i.e., Louvain and Leiden, could not adequately capture the graph's community structure, while our algorithms did so very satisfactorily. Also, the spectral-based methods worked poorly, which was expected.

Regarding the Dolphins dataset, the reference methods are better in terms of modularity maximisation. Let us note that this does not indicate that they detected the communities objectively better, only that they seem to yield higher overall modularity, which can be used to evaluate community detection when no ground truth is given. Nonetheless, many of our algorithms achieved only a slightly lower value of the measure.

%
%
%
%
%
%
%
%
%


\begin{table}[t!]
    \centering
            \caption{Results of our methods and the reference methods on two real-world datasets: the Rand score and computation time in the Karate task as well as modularity and computation time in the Dolphins task.}
%

    \begin{tabular}{|l||r|r||r|r|}
    \hline
        \textbf{Method} & \textbf{$\text{Rand}_{\text{Karate}}$} & \textbf{$\text{T}_{\text{Karate}}$[s]} & \textbf{$\text{Mod}_{\text{Dolphins}}$} & \textbf{$\text{T}_{\text{Dolphins}}$[s]} \\ \hline
        Node Genie PPMI & 0.77 & 0.01 & 0.49 & 0.02 \\ \hline
        Node Genie W-F & \textbf{0.88} & 0.03 & 0.37 & 0.02 \\ \hline
        Node Average PPMI & 0.77 & 0.00 & 0.49 & 0.01 \\ \hline
        Node Average RPR & 0.77 & 0.00 & 0.38 & 0.01 \\ \hline
        Spectral Genie K-step & 0.40 & 0.00 & -0.05 & 0.11 \\ \hline
        Spectral Ward Katz & -0.01 & 0.00 & 0.24 & 0.01 \\ \hline
        Euclidean Genie DNGR & \textbf{0.88} & 13.86 & 0.48 & 19.15 \\ \hline
        Euclidean Ward DNGR & \textbf{0.88} & 11.28 & 0.48 & 22.14 \\ \hline
        Euclidean Ward Node2Vec & \textbf{0.88} & 1.21 & 0.43 & 1.95 \\ \hline
        \hline
        Leiden & 0.46 & 0.00 & \textbf{0.52} & 0.00 \\ \hline
        Louvain & 0.43 & 0.00 & \textbf{0.52} & 0.00 \\ \hline
        Greedy modularity & 0.57 & 0.00 & 0.50 & 0.01 \\ \hline
        Spectral & 0.77 & 0.05 & 0.38 & 0.10 \\ \hline
        Label propagation & 0.38 & 0.00 & 0.50 & 0.00 \\ \hline
    \end{tabular}
    \label{tab:real_results}
\end{table}




%
%
%

\section{Discussion on Theoretical and Practical Implications}\label{sec:final}

We have developed three general pipelines in which agglomerative hierarchical clustering algorithms may be used for community detection in graphs:
node dissimilarity-, spectral-, and  representation-based approaches. Each of the pipelines is unique in its own way:
\vspace{-0.05cm}
\begin{enumerate}
\setlength\itemsep{-0.1em}
    \item node dissimilarity-based approach aims to generalise and build upon the structure of the earliest clustering and community detection algorithms;
%
    \item spectral-based approach tries to verify experimentally
    whether there is a place for improvement of the standard spectral community detection algorithms;
    \item representation-based approach uses the newest, ``sophisticated''
    (black-box)     methods based on Graph Representation Learning to propose new ways of performing community detection.
\end{enumerate}

We have tested each pipeline with numerous components and performed a comprehensive analysis of their applicability in the community detection task. Here are the most important outcomes of our analysis.
\vspace{-0.05cm}
\begin{itemize}
\setlength\itemsep{-0.1em}
    \item Several methods can achieve similar performance as the current state-of-the-art ``dedicated'' methods, and a majority of our methods significantly outperform older reference algorithms, such as the Label Propagation Algorithm or Greedy Modularity Optimisation. Also, there are instances of graphs on which we obtain better results than the current state-of-the-art.
    \item In our analysis, we have confirmed the hypothesis regarding the usability of Graph Representation Learning methods for community detection. Specifically, we have shown that some methods, such as Deep Neural Graph Representation (DNGR), can capture the community structure of synthetic and real-world graphs successfully.
    \item Thanks to developing spectral-based approach, we have observed unexpectedly good performance of selected combinations of tools. We believe that our preliminary results strongly motivate further research into including the K-step walk transition matrices in the spectral community detection process. As we observed, there is a strong premise that such a tool, if improved, may constitute a state-of-the-art algorithm for community detection in sparse graphs with sparse communities.
\end{itemize}

Moreover, let us emphasise that all of our algorithms are hierarchical. Hence, they present the same advantages as other clustering algorithms from this class. More precisely, the user may select \textit{any} number of communities, and the communities are interpretable in the sense that we can observe how they decompose into smaller subgraphs. Other standalone methods (e.g., Leiden and Leuven) do not have this property: in their case, the number of clusters must be known in advance, and the $k$-clustering is usually not properly nested or even related to the $l$-clustering, $k\neq l$.

%
Much work remains to be done in the future, specifically related to spectral-based methods and their applicability. An analysis of synthetic data generated from different models should also be considered.
%
%

%

%



%
%
%
%
%
%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
%


\section*{Conflict of interest}

All authors certify that they have no affiliations with or involvement in any
organisation or entity with any financial interest or non-financial interest
in the subject matter or materials discussed in this manuscript.


%
%


\section*{Data availability}

Zachary's Karate Club and Dolphins datasets are freely available online. Our battery of synthetic SBM graphs will be made available upon request.

%
%


\section*{Acknowledgements}

This research was supported by the Australian Research Council Discovery
Project ARC DP210100227 (MG).
%

The research was carried out with the support of the Laboratory of Bioinformatics and Computational Genomics and the High Performance Computing Centre of the Faculty of Mathematics and Information Science Warsaw University of Technology under computational grant number A-21-04.

ŁB would like to thank Kacper Siemaszko and Paweł Rzążewski for stimulating discussion.

%
%

\printcredits

%
%

%

%
%
%
\begin{thebibliography}{67}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\href}[2]{#2}
\providecommand{\path}[1]{#1}
\providecommand{\DOIprefix}{doi:}
\providecommand{\ArXivprefix}{arXiv:}
\providecommand{\URLprefix}{URL: }
\providecommand{\Pubmedprefix}{pmid:}
\providecommand{\doi}[1]{\href{http://dx.doi.org/#1}{\path{#1}}}
\providecommand{\Pubmed}[1]{\href{pmid:#1}{\path{#1}}}
\providecommand{\bibinfo}[2]{#2}
\ifx\xfnm\relax \def\xfnm[#1]{\unskip,\space#1}\fi
%
\bibitem[{Adamic and Adar(2003)}]{adamicAdar}
\bibinfo{author}{Adamic, L.A.}, \bibinfo{author}{Adar, E.},
  \bibinfo{year}{2003}.
\newblock \bibinfo{title}{Friends and neighbors on the web}.
\newblock \bibinfo{journal}{Social Networks} \bibinfo{volume}{25},
  \bibinfo{pages}{211--230}.
%
\bibitem[{Ahmed et~al.(2013)Ahmed, Shervashidze, Narayanamurthy, Josifovski and
  Smola}]{GF}
\bibinfo{author}{Ahmed, A.}, \bibinfo{author}{Shervashidze, N.},
  \bibinfo{author}{Narayanamurthy, S.M.}, \bibinfo{author}{Josifovski, V.},
  \bibinfo{author}{Smola, A.J.}, \bibinfo{year}{2013}.
\newblock \bibinfo{title}{Distributed large-scale natural graph factorization},
  in: \bibinfo{booktitle}{WWW '13: Proceedings of the 22nd international
  conference on World Wide Web}, \bibinfo{publisher}{International World Wide
  Web Conferences Steering Committee / {ACM}}. pp. \bibinfo{pages}{37--48}.
%
\bibitem[{Belkin and Niyogi(2001)}]{LE}
\bibinfo{author}{Belkin, M.}, \bibinfo{author}{Niyogi, P.},
  \bibinfo{year}{2001}.
\newblock \bibinfo{title}{Laplacian eigenmaps and spectral techniques for
  embedding and clustering}, in: \bibinfo{booktitle}{NIPS'01: Proceedings of
  the 14th International Conference on Neural Information Processing Systems:
  Natural and Synthetic}, \bibinfo{publisher}{{MIT} Press}. pp.
  \bibinfo{pages}{585--591}.
%
\bibitem[{Blondel et~al.(2004)Blondel, Gajardo, Heymans, Senellart and
  Van~Dooren}]{blondel}
\bibinfo{author}{Blondel, V.}, \bibinfo{author}{Gajardo, A.},
  \bibinfo{author}{Heymans, M.}, \bibinfo{author}{Senellart, P.},
  \bibinfo{author}{Van~Dooren, P.}, \bibinfo{year}{2004}.
\newblock \bibinfo{title}{A measure of similarity between graph vertices:
  Applications to synonym extraction and web searching}.
\newblock \bibinfo{journal}{SIAM Review} \bibinfo{volume}{46},
  \bibinfo{pages}{647--666}.
\newblock \DOIprefix\doi{10.2307/20453570}.
%
\bibitem[{Blondel et~al.(2008)Blondel, Guillaume, Lambiotte and
  Lefebvre}]{louvain}
\bibinfo{author}{Blondel, V.D.}, \bibinfo{author}{Guillaume, J.L.},
  \bibinfo{author}{Lambiotte, R.}, \bibinfo{author}{Lefebvre, E.},
  \bibinfo{year}{2008}.
\newblock \bibinfo{title}{Fast unfolding of communities in large networks}.
\newblock \bibinfo{journal}{Journal of Statistical Mechanics: Theory and
  Experiment} \bibinfo{volume}{2008}, \bibinfo{pages}{P10008}.
%
\bibitem[{Bo et~al.(2020)Bo, Wang, Shi, Zhu, Lu and Cui}]{sdcn}
\bibinfo{author}{Bo, D.}, \bibinfo{author}{Wang, X.}, \bibinfo{author}{Shi,
  C.}, \bibinfo{author}{Zhu, M.}, \bibinfo{author}{Lu, E.},
  \bibinfo{author}{Cui, P.}, \bibinfo{year}{2020}.
\newblock \bibinfo{title}{Structural deep clustering network}, in:
  \bibinfo{booktitle}{WWW '20: Proceedings of The Web Conference 2020},
  \bibinfo{publisher}{International World Wide Web Conferences Steering
  Committee / {ACM}}. pp. \bibinfo{pages}{1400--1410}.
%
\bibitem[{Brzozowski and Siemaszko(2021)}]{relegy}
\bibinfo{author}{Brzozowski, L.}, \bibinfo{author}{Siemaszko, K.},
  \bibinfo{year}{2021}.
\newblock \bibinfo{title}{{ReLeGy} -- {R}epresentation learning of graphs in
  {P}ython}.
\newblock \URLprefix \url{https://github.com/lukaszbrzozowski/ReLeGy}.
%
\bibitem[{Burt(1976)}]{burt}
\bibinfo{author}{Burt, R.S.}, \bibinfo{year}{1976}.
\newblock \bibinfo{title}{Positions in networks}.
\newblock \bibinfo{journal}{Social Forces} \bibinfo{volume}{55},
  \bibinfo{pages}{93--122}.
%
\bibitem[{Cao et~al.(2023)Cao, Liu, Fang, Xu and Wang}]{IPM4}
\bibinfo{author}{Cao, R.}, \bibinfo{author}{Liu, X.F.}, \bibinfo{author}{Fang,
  Z.}, \bibinfo{author}{Xu, X.K.}, \bibinfo{author}{Wang, X.},
  \bibinfo{year}{2023}.
\newblock \bibinfo{title}{How do scientific papers from different journal tiers
  gain attention on social media?}
\newblock \bibinfo{journal}{Information Processing \& Management}
  \bibinfo{volume}{60}, \bibinfo{pages}{103152}.
\newblock \DOIprefix\doi{10.1016/J.IPM.2022.103152}.
%
\bibitem[{Cao et~al.(2015)Cao, Lu and Xu}]{grarep}
\bibinfo{author}{Cao, S.}, \bibinfo{author}{Lu, W.}, \bibinfo{author}{Xu, Q.},
  \bibinfo{year}{2015}.
\newblock \bibinfo{title}{Grarep: Learning graph representations with global
  structural information}, in: \bibinfo{booktitle}{CIKM '15: Proceedings of the
  24th ACM International on Conference on Information and Knowledge
  Management}, \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{891--900}.
%
\bibitem[{Cao et~al.(2016)Cao, Lu and Xu}]{dngr}
\bibinfo{author}{Cao, S.}, \bibinfo{author}{Lu, W.}, \bibinfo{author}{Xu, Q.},
  \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Deep neural networks for learning graph
  representations}, in: \bibinfo{booktitle}{AAAI'16: Proceedings of the
  Thirtieth AAAI Conference on Artificial Intelligence},
  \bibinfo{publisher}{{AAAI} Press}. pp. \bibinfo{pages}{1145--1152}.
%
\bibitem[{Chen et~al.(2021)Chen, Nie, Wang, Kong, Wang and Huang}]{lcden}
\bibinfo{author}{Chen, D.}, \bibinfo{author}{Nie, M.}, \bibinfo{author}{Wang,
  J.}, \bibinfo{author}{Kong, Y.}, \bibinfo{author}{Wang, D.},
  \bibinfo{author}{Huang, X.}, \bibinfo{year}{2021}.
\newblock \bibinfo{title}{Community detection based on graph representation
  learning in evolutionary networks}.
\newblock \bibinfo{journal}{Applied Sciences} \bibinfo{volume}{11}.
%
\bibitem[{Chen et~al.(2018)Chen, Perozzi, Hu and Skiena}]{HARP}
\bibinfo{author}{Chen, H.}, \bibinfo{author}{Perozzi, B.}, \bibinfo{author}{Hu,
  Y.}, \bibinfo{author}{Skiena, S.}, \bibinfo{year}{2018}.
\newblock \bibinfo{title}{Harp: Hierarchical representation learning for
  networks}, in: \bibinfo{booktitle}{AIES '18: Proceedings of the 2018 AAAI/ACM
  Conference on AI, Ethics, and Society}, \bibinfo{publisher}{{AAAI} Press}.
  pp. \bibinfo{pages}{2127--2134}.
%
\bibitem[{Chen et~al.(2022)Chen, Yu, Yang and Shao}]{cd2}
\bibinfo{author}{Chen, H.}, \bibinfo{author}{Yu, Z.}, \bibinfo{author}{Yang,
  Q.}, \bibinfo{author}{Shao, J.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Community detection in subspace of attribute}.
\newblock \bibinfo{journal}{Information Sciences} \bibinfo{volume}{602},
  \bibinfo{pages}{220--235}.
\newblock \DOIprefix\doi{10.1016/j.ins.2022.04.047}.
%
\bibitem[{Church and Hanks(1989)}]{pmi}
\bibinfo{author}{Church, K.W.}, \bibinfo{author}{Hanks, P.},
  \bibinfo{year}{1989}.
\newblock \bibinfo{title}{Word association norms, mutual information and
  lexicography}, in: \bibinfo{booktitle}{ACL '89: Proceedings of the 27th
  annual meeting on Association for Computational Linguistics},
  \bibinfo{publisher}{{ACL}}. pp. \bibinfo{pages}{76--83}.
%
\bibitem[{Clauset et~al.(2004)Clauset, Newman and Moore}]{greedyModularity}
\bibinfo{author}{Clauset, A.}, \bibinfo{author}{Newman, M.E.J.},
  \bibinfo{author}{Moore, C.}, \bibinfo{year}{2004}.
\newblock \bibinfo{title}{Finding community structure in very large networks}.
\newblock \bibinfo{journal}{Phys. Rev. E} \bibinfo{volume}{70},
  \bibinfo{pages}{066111}.
\newblock \DOIprefix\doi{10.1103/PhysRevE.70.066111}.
%
\bibitem[{Cordasco and Gargano(2011)}]{labelpropagation}
\bibinfo{author}{Cordasco, G.}, \bibinfo{author}{Gargano, L.},
  \bibinfo{year}{2011}.
\newblock \bibinfo{title}{Community detection via semi-synchronous label
  propagation algorithms}.
\newblock \bibinfo{journal}{2010 IEEE International Workshop on Business
  Applications of Social Network Analysis, BASNA 2010} , \bibinfo{pages}{1--8}.
%
\bibitem[{Dekker et~al.(2022)Dekker, Medvedev, Rombouts, Siudem and
  Tupikina}]{dekker2022}
\bibinfo{author}{Dekker, M.M.}, \bibinfo{author}{Medvedev, A.N.},
  \bibinfo{author}{Rombouts, J.}, \bibinfo{author}{Siudem, G.},
  \bibinfo{author}{Tupikina, L.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Modelling railway delay propagation as diffusion-like
  spreading}.
\newblock \bibinfo{journal}{EPJ Data Sci.} \bibinfo{volume}{11},
  \bibinfo{pages}{44}.
\newblock \DOIprefix\doi{10.1140/epjds/s13688-022-00359-1}.
%
\bibitem[{Donnat et~al.(2018)Donnat, Zitnik, Hallac and Leskovec}]{GraphWave}
\bibinfo{author}{Donnat, C.}, \bibinfo{author}{Zitnik, M.},
  \bibinfo{author}{Hallac, D.}, \bibinfo{author}{Leskovec, J.},
  \bibinfo{year}{2018}.
\newblock \bibinfo{title}{Learning structural node embeddings via diffusion
  wavelets}, in: \bibinfo{booktitle}{KDD '18: Proceedings of the 24th ACM
  SIGKDD International Conference on Knowledge Discovery and Data Mining},
  \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{1320--1329}.
%
\bibitem[{Erd\"{o}s and R\'{e}nyi(1959)}]{er}
\bibinfo{author}{Erd\"{o}s, P.}, \bibinfo{author}{R\'{e}nyi, A.},
  \bibinfo{year}{1959}.
\newblock \bibinfo{title}{On random graphs {I}}.
\newblock \bibinfo{journal}{Publicationes Mathematicae Debrecen}
  \bibinfo{volume}{6}, \bibinfo{pages}{290}.
%
\bibitem[{Fani et~al.(2020)Fani, Jiang, Bagheri, Al-Obeidat, Du and
  Kargar}]{IPM1}
\bibinfo{author}{Fani, H.}, \bibinfo{author}{Jiang, E.},
  \bibinfo{author}{Bagheri, E.}, \bibinfo{author}{Al-Obeidat, F.},
  \bibinfo{author}{Du, W.}, \bibinfo{author}{Kargar, M.}, \bibinfo{year}{2020}.
\newblock \bibinfo{title}{User community detection via embedding of social
  network structure and temporal content}.
\newblock \bibinfo{journal}{Information Processing \& Management}
  \bibinfo{volume}{57}, \bibinfo{pages}{102056}.
\newblock \DOIprefix\doi{10.1016/j.ipm.2019.102056}.
%
\bibitem[{Filippone et~al.(2008)Filippone, Camastra, Masulli and
  Rovetta}]{Filippone}
\bibinfo{author}{Filippone, M.}, \bibinfo{author}{Camastra, F.},
  \bibinfo{author}{Masulli, F.}, \bibinfo{author}{Rovetta, S.},
  \bibinfo{year}{2008}.
\newblock \bibinfo{title}{A survey of kernel and spectral methods for
  clustering}.
\newblock \bibinfo{journal}{Pattern Recognition} \bibinfo{volume}{41},
  \bibinfo{pages}{176--190}.
%
\bibitem[{Fortunato(2010)}]{fortunato}
\bibinfo{author}{Fortunato, S.}, \bibinfo{year}{2010}.
\newblock \bibinfo{title}{Community detection in graphs}.
\newblock \bibinfo{journal}{Physics Reports} \bibinfo{volume}{486},
  \bibinfo{pages}{75--174}.
\newblock \DOIprefix\doi{10.1016/j.physrep.2009.11.002}.
%
\bibitem[{Fortunato and Newman(2022)}]{fortunato202220}
\bibinfo{author}{Fortunato, S.}, \bibinfo{author}{Newman, M.E.},
  \bibinfo{year}{2022}.
\newblock \bibinfo{title}{20 years of network community detection}.
\newblock \bibinfo{journal}{Nature Physics} \bibinfo{volume}{18},
  \bibinfo{pages}{848--850}.
%
\bibitem[{Gagolewski(2021)}]{genieclust}
\bibinfo{author}{Gagolewski, M.}, \bibinfo{year}{2021}.
\newblock \bibinfo{title}{{genieclust}: {F}ast and robust hierarchical
  clustering}.
\newblock \bibinfo{journal}{SoftwareX} \bibinfo{volume}{15},
  \bibinfo{pages}{100722}.
\newblock \DOIprefix\doi{10.1016/j.softx.2021.100722}.
%
\bibitem[{Gagolewski(2022)}]{aaa}
\bibinfo{author}{Gagolewski, M.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Adjusted asymmetric accuracy: {A} well-behaving
  external cluster validity measure}.
\newblock \URLprefix \url{https://arxiv.org/pdf/2209.02935.pdf},
  \DOIprefix\doi{10.48550/arXiv.2209.02935}. \bibinfo{note}{under review
  (preprint)}.
%
\bibitem[{Gagolewski et~al.(2016)Gagolewski, Bartoszuk and Cena}]{genie}
\bibinfo{author}{Gagolewski, M.}, \bibinfo{author}{Bartoszuk, M.},
  \bibinfo{author}{Cena, A.}, \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Genie: A new, fast, and outlier-resistant
  hierarchical clustering algorithm}.
\newblock \bibinfo{journal}{Information Sciences} \bibinfo{volume}{363},
  \bibinfo{pages}{8--23}.
%
\bibitem[{Garey and Johnson(1979)}]{gareyjohnson}
\bibinfo{author}{Garey, M.R.}, \bibinfo{author}{Johnson, D.S.},
  \bibinfo{year}{1979}.
\newblock \bibinfo{title}{Computers and Intractability: {A} Guide to the Theory
  of NP-Completeness}.
\newblock \bibinfo{publisher}{W. H. Freeman}.
%
\bibitem[{Girvan and Newman(2002)}]{Girvan}
\bibinfo{author}{Girvan, M.}, \bibinfo{author}{Newman, M.E.J.},
  \bibinfo{year}{2002}.
\newblock \bibinfo{title}{Community structure in social and biological
  networks}.
\newblock \bibinfo{journal}{Proceedings of the National Academy of Sciences}
  \bibinfo{volume}{99}, \bibinfo{pages}{7821--7826}.
%
\bibitem[{Godsil and Royle(2001)}]{godsil}
\bibinfo{author}{Godsil, C.D.}, \bibinfo{author}{Royle, G.F.},
  \bibinfo{year}{2001}.
\newblock \bibinfo{title}{Algebraic Graph Theory}.
\newblock \bibinfo{publisher}{Springer}.
%
\bibitem[{Golub and Reinsch(1970)}]{SVD}
\bibinfo{author}{Golub, G.H.}, \bibinfo{author}{Reinsch, C.},
  \bibinfo{year}{1970}.
\newblock \bibinfo{title}{Singular value decomposition and least squares
  solutions}.
\newblock \bibinfo{journal}{Numer. Math.} \bibinfo{volume}{14},
  \bibinfo{pages}{403–420}.
%
\bibitem[{Grover and Leskovec(2016)}]{Node2Vec}
\bibinfo{author}{Grover, A.}, \bibinfo{author}{Leskovec, J.},
  \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Node2vec: Scalable feature learning for networks},
  in: \bibinfo{booktitle}{KDD '16: Proceedings of the 22nd ACM SIGKDD
  International Conference on Knowledge Discovery and Data Mining},
  \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{855--864}.
%
\bibitem[{Hamdaqa et~al.(2014)Hamdaqa, Tahvildari, LaChapelle and
  Campbell}]{socialcommunity}
\bibinfo{author}{Hamdaqa, M.}, \bibinfo{author}{Tahvildari, L.},
  \bibinfo{author}{LaChapelle, N.}, \bibinfo{author}{Campbell, B.},
  \bibinfo{year}{2014}.
\newblock \bibinfo{title}{Cultural scene detection using reverse {L}ouvain
  optimization}.
\newblock \bibinfo{journal}{Sci. Comput. Program.} \bibinfo{volume}{95},
  \bibinfo{pages}{44--72}.
%
\bibitem[{Hamilton et~al.(2017)Hamilton, Ying and Leskovec}]{hamilton}
\bibinfo{author}{Hamilton, W.L.}, \bibinfo{author}{Ying, R.},
  \bibinfo{author}{Leskovec, J.}, \bibinfo{year}{2017}.
\newblock \bibinfo{title}{Representation learning on graphs: Methods and
  applications}.
\newblock \bibinfo{journal}{{IEEE} Data Eng. Bull.} \bibinfo{volume}{40},
  \bibinfo{pages}{52--74}.
%
\bibitem[{He et~al.(2022)He, Zheng, Cheng, Tang, Chen and Liu}]{cd1}
\bibinfo{author}{He, C.}, \bibinfo{author}{Zheng, Y.}, \bibinfo{author}{Cheng,
  J.}, \bibinfo{author}{Tang, Y.}, \bibinfo{author}{Chen, G.},
  \bibinfo{author}{Liu, H.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Semi-supervised overlapping community detection in
  attributed graph with graph convolutional autoencoder}.
\newblock \bibinfo{journal}{Information Sciences} \bibinfo{volume}{608},
  \bibinfo{pages}{1464--1479}.
\newblock \DOIprefix\doi{10.1016/j.ins.2022.07.036}.
%
\bibitem[{Holland et~al.(1983)Holland, Laskey and Leinhardt}]{sbm}
\bibinfo{author}{Holland, P.W.}, \bibinfo{author}{Laskey, K.B.},
  \bibinfo{author}{Leinhardt, S.}, \bibinfo{year}{1983}.
\newblock \bibinfo{title}{Stochastic blockmodels: First steps}.
\newblock \bibinfo{journal}{Social Networks} \bibinfo{volume}{5},
  \bibinfo{pages}{109--137}.
%
\bibitem[{Hu et~al.(2022)Hu, Feng and Xie}]{IPM5}
\bibinfo{author}{Hu, D.}, \bibinfo{author}{Feng, D.}, \bibinfo{author}{Xie,
  Y.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Egc: A novel event-oriented graph clustering
  framework for social media text}.
\newblock \bibinfo{journal}{Information Processing \& Management}
  \bibinfo{volume}{59}, \bibinfo{pages}{103059}.
\newblock \DOIprefix\doi{10.1016/J.IPM.2022.103059}.
%
\bibitem[{Katz(1953)}]{katz}
\bibinfo{author}{Katz, L.}, \bibinfo{year}{1953}.
\newblock \bibinfo{title}{A new status index derived from sociometric
  analysis}.
\newblock \bibinfo{journal}{Psychometrika} \bibinfo{volume}{18},
  \bibinfo{pages}{39--43}.
%
\bibitem[{Kipf and Welling(2016)}]{vgae}
\bibinfo{author}{Kipf, T.N.}, \bibinfo{author}{Welling, M.},
  \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Variational graph auto-encoders}.
\newblock \bibinfo{journal}{CoRR} \bibinfo{volume}{abs/1611.07308}.
%
\bibitem[{Kipf and Welling(2017)}]{gcn}
\bibinfo{author}{Kipf, T.N.}, \bibinfo{author}{Welling, M.},
  \bibinfo{year}{2017}.
\newblock \bibinfo{title}{Semi-supervised classification with graph
  convolutional networks}, in: \bibinfo{booktitle}{ICLR '17: Proceedings of the
  5th International Conference on Learning Representations},
  \bibinfo{publisher}{OpenReview.net}.
%
\bibitem[{Lawrence and Phipps(1985)}]{HubertArabie1985:partitionscomp}
\bibinfo{author}{Lawrence, H.}, \bibinfo{author}{Phipps, A.},
  \bibinfo{year}{1985}.
\newblock \bibinfo{title}{Comparing partitions}.
\newblock \bibinfo{journal}{Journal of Classification} \bibinfo{volume}{2},
  \bibinfo{pages}{193--218}.
%
\bibitem[{Lusseau(2003)}]{dolphins}
\bibinfo{author}{Lusseau, D.}, \bibinfo{year}{2003}.
\newblock \bibinfo{title}{The emergent properties of a dolphin social network}.
\newblock \bibinfo{journal}{Proc. Biol. Sci.} \bibinfo{volume}{270 Suppl 2},
  \bibinfo{pages}{S186--8}.
%
\bibitem[{Mancoridis et~al.(1998)Mancoridis, Mitchell, Rorres, Chen and
  Gansner}]{mancoridis}
\bibinfo{author}{Mancoridis, S.}, \bibinfo{author}{Mitchell, B.S.},
  \bibinfo{author}{Rorres, C.}, \bibinfo{author}{Chen, Y.F.},
  \bibinfo{author}{Gansner, E.R.}, \bibinfo{year}{1998}.
\newblock \bibinfo{title}{{Using Automatic Clustering to Produce High-Level
  System Organizations of Source Code}}, in: \bibinfo{booktitle}{IWPC '98:
  Proceedings of the 6th International Workshop on Program Comprehension},
  \bibinfo{publisher}{{IEEE}}. pp. \bibinfo{pages}{45--52}.
%
\bibitem[{Mikolov et~al.(2013)Mikolov, Chen, Corrado and Dean}]{skipgram}
\bibinfo{author}{Mikolov, T.}, \bibinfo{author}{Chen, K.},
  \bibinfo{author}{Corrado, G.}, \bibinfo{author}{Dean, J.},
  \bibinfo{year}{2013}.
\newblock \bibinfo{title}{Efficient estimation of word representations in
  vector space}, in: \bibinfo{booktitle}{ICLR '13: Proceedings of the 1st
  International Conference on Learning Representations}.
%
\bibitem[{Murtagh(1983)}]{murtagh}
\bibinfo{author}{Murtagh, F.}, \bibinfo{year}{1983}.
\newblock \bibinfo{title}{A survey of recent advances in hierarchical
  clustering algorithms}.
\newblock \bibinfo{journal}{The Computer Journal} \bibinfo{volume}{26},
  \bibinfo{pages}{354--359}.
%
\bibitem[{Newman and Girvan(2004)}]{newmanGirvanModularity}
\bibinfo{author}{Newman, M.E.J.}, \bibinfo{author}{Girvan, M.},
  \bibinfo{year}{2004}.
\newblock \bibinfo{title}{Finding and evaluating community structure in
  networks}.
\newblock \bibinfo{journal}{Phys. Rev. E} \bibinfo{volume}{69},
  \bibinfo{pages}{026113}.
%
\bibitem[{Osi{\'{n}}ski et~al.(2004)Osi{\'{n}}ski, Stefanowski and
  Weiss}]{lingo}
\bibinfo{author}{Osi{\'{n}}ski, S.}, \bibinfo{author}{Stefanowski, J.},
  \bibinfo{author}{Weiss, D.}, \bibinfo{year}{2004}.
\newblock \bibinfo{title}{Lingo: Search results clustering algorithm based on
  singular value decomposition}, in: \bibinfo{editor}{K{\l}opotek, M.A.},
  \bibinfo{editor}{Wierzcho{\'{n}}, S.T.}, \bibinfo{editor}{Trojanowski, K.}
  (Eds.), \bibinfo{booktitle}{Intelligent Information Processing and Web
  Mining}, \bibinfo{publisher}{Springer Berlin Heidelberg}. pp.
  \bibinfo{pages}{359--368}.
%
\bibitem[{Ou et~al.(2016)Ou, Cui, Pei, Zhang and Zhu}]{hope}
\bibinfo{author}{Ou, M.}, \bibinfo{author}{Cui, P.}, \bibinfo{author}{Pei, J.},
  \bibinfo{author}{Zhang, Z.}, \bibinfo{author}{Zhu, W.}, \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Asymmetric transitivity preserving graph embedding},
  in: \bibinfo{booktitle}{KDD '16: Proceedings of the 22nd ACM SIGKDD
  International Conference on Knowledge Discovery and Data Mining},
  \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{1105--1114}.
%
\bibitem[{Page et~al.(1999)Page, Brin, Motwani and Winograd}]{pagerank}
\bibinfo{author}{Page, L.}, \bibinfo{author}{Brin, S.},
  \bibinfo{author}{Motwani, R.}, \bibinfo{author}{Winograd, T.},
  \bibinfo{year}{1999}.
\newblock \bibinfo{title}{The PageRank Citation Ranking: Bringing Order to the
  Web}.
\newblock \bibinfo{type}{Technical Report} \bibinfo{number}{1999-66}. Stanford
  InfoLab.
%
\bibitem[{Patnaik et~al.(2016)Patnaik, Bhuyan and {Krishna Rao}}]{DIANA}
\bibinfo{author}{Patnaik, A.K.}, \bibinfo{author}{Bhuyan, P.K.},
  \bibinfo{author}{{Krishna Rao}, K.}, \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Divisive analysis (diana) of hierarchical clustering
  and gps data for level of service criteria of urban streets}.
\newblock \bibinfo{journal}{Alexandria Engineering Journal}
  \bibinfo{volume}{55}, \bibinfo{pages}{407--418}.
%
\bibitem[{Paul and Chen(2016)}]{nullmodels}
\bibinfo{author}{Paul, S.}, \bibinfo{author}{Chen, Y.}, \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Null models and modularity based community detection
  in multi-layer networks}.
\newblock \bibinfo{journal}{Sankhya A} \bibinfo{volume}{84}.
\newblock \DOIprefix\doi{10.1007/s13171-021-00257-0}.
%
\bibitem[{Perozzi et~al.(2014)Perozzi, Al{-}Rfou and Skiena}]{DeepWalk}
\bibinfo{author}{Perozzi, B.}, \bibinfo{author}{Al{-}Rfou, R.},
  \bibinfo{author}{Skiena, S.}, \bibinfo{year}{2014}.
\newblock \bibinfo{title}{Deepwalk: Online learning of social representations},
  in: \bibinfo{booktitle}{KDD '14: Proceedings of the 20th ACM SIGKDD
  International Conference on Knowledge Discovery and Data Mining},
  \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{701--710}.
%
\bibitem[{R{\'{e}}v{\'{e}}sz(1990)}]{revesz}
\bibinfo{author}{R{\'{e}}v{\'{e}}sz, P.}, \bibinfo{year}{1990}.
\newblock \bibinfo{title}{Random Walk in Random and Non-Random Environments}.
\newblock \bibinfo{publisher}{World Scientific}.
%
\bibitem[{Rezaei and Fränti(2016)}]{psi}
\bibinfo{author}{Rezaei, M.}, \bibinfo{author}{Fränti, P.},
  \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Set matching measures for external cluster validity}.
\newblock \bibinfo{journal}{IEEE Transactions on Knowledge and Data
  Engineering} \bibinfo{volume}{28}, \bibinfo{pages}{2173--2186}.
\newblock \DOIprefix\doi{10.1109/TKDE.2016.2551240}.
%
\bibitem[{Ribeiro et~al.(2017)Ribeiro, Saverese and Figueiredo}]{Struc2Vec}
\bibinfo{author}{Ribeiro, L.F.R.}, \bibinfo{author}{Saverese, P.H.P.},
  \bibinfo{author}{Figueiredo, D.R.}, \bibinfo{year}{2017}.
\newblock \bibinfo{title}{Struc2vec: Learning node representations from
  structural identity}, in: \bibinfo{booktitle}{KDD '17: Proceedings of the
  23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
  Mining}, \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{385--394}.
%
\bibitem[{Sieranoja and Fränti(2022)}]{IPM2}
\bibinfo{author}{Sieranoja, S.}, \bibinfo{author}{Fränti, P.},
  \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Adapting k-means for graph clustering}.
\newblock \bibinfo{journal}{Knowledge and Information Systems}
  \bibinfo{volume}{64}, \bibinfo{pages}{115--142}.
\newblock \DOIprefix\doi{10.1007/S10115-021-01623-Y}.
%
\bibitem[{Su et~al.(2022)Su, Xue, Liu, Wu, Yang, Zhou, Hu, Paris, Nepal, Jin,
  Sheng and Yu}]{su}
\bibinfo{author}{Su, X.}, \bibinfo{author}{Xue, S.}, \bibinfo{author}{Liu, F.},
  \bibinfo{author}{Wu, J.}, \bibinfo{author}{Yang, J.}, \bibinfo{author}{Zhou,
  C.}, \bibinfo{author}{Hu, W.}, \bibinfo{author}{Paris, C.},
  \bibinfo{author}{Nepal, S.}, \bibinfo{author}{Jin, D.},
  \bibinfo{author}{Sheng, Q.Z.}, \bibinfo{author}{Yu, P.S.},
  \bibinfo{year}{2022}.
\newblock \bibinfo{title}{A comprehensive survey on community detection with
  deep learning}.
\newblock \bibinfo{journal}{IEEE transactions on neural networks and learning
  systems} \bibinfo{volume}{PP}.
\newblock \DOIprefix\doi{10.1109/TNNLS.2021.3137396}.
%
\bibitem[{Sun et~al.(2019)Sun, Qu, Hoffmann, Huang and Tang}]{vgraph}
\bibinfo{author}{Sun, F.Y.}, \bibinfo{author}{Qu, M.},
  \bibinfo{author}{Hoffmann, J.}, \bibinfo{author}{Huang, C.},
  \bibinfo{author}{Tang, J.}, \bibinfo{year}{2019}.
\newblock \bibinfo{title}{{vGraph: {A} Generative Model for Joint Community
  Detection and Node Representation Learning}}, in:
  \bibinfo{booktitle}{NIPS'18: Proceedings of the 32nd International Conference
  on Neural Information Processing Systems}, pp. \bibinfo{pages}{512--522}.
%
\bibitem[{Tang et~al.(2015)Tang, Qu, Wang, Zhang, Yan and Mei}]{LINE}
\bibinfo{author}{Tang, J.}, \bibinfo{author}{Qu, M.}, \bibinfo{author}{Wang,
  M.}, \bibinfo{author}{Zhang, M.}, \bibinfo{author}{Yan, J.},
  \bibinfo{author}{Mei, Q.}, \bibinfo{year}{2015}.
\newblock \bibinfo{title}{Line: Large-scale information network embedding}, in:
  \bibinfo{booktitle}{WWW '15 Companion: Proceedings of the 24th International
  Conference on World Wide Web}, \bibinfo{publisher}{International World Wide
  Web Conferences Steering Committee / {ACM}}. pp. \bibinfo{pages}{1067--1077}.
%
\bibitem[{Traag et~al.(2019)Traag, Waltman and van Eck}]{leiden}
\bibinfo{author}{Traag, V.}, \bibinfo{author}{Waltman, L.},
  \bibinfo{author}{van Eck, N.J.}, \bibinfo{year}{2019}.
\newblock \bibinfo{title}{From {L}ouvain to {L}eiden: {G}uaranteeing
  well-connected communities}.
\newblock \bibinfo{journal}{Scientific Reports} \bibinfo{volume}{9},
  \bibinfo{pages}{5233}.
\newblock \DOIprefix\doi{10.1038/s41598-019-41695-z}.
%
\bibitem[{Wang et~al.(2016)Wang, Cui and Zhu}]{SDNE}
\bibinfo{author}{Wang, D.}, \bibinfo{author}{Cui, P.}, \bibinfo{author}{Zhu,
  W.}, \bibinfo{year}{2016}.
\newblock \bibinfo{title}{Structural deep network embedding}, in:
  \bibinfo{booktitle}{KDD '16: Proceedings of the 22nd ACM SIGKDD International
  Conference on Knowledge Discovery and Data Mining},
  \bibinfo{publisher}{{ACM}}. pp. \bibinfo{pages}{1225--1234}.
%
\bibitem[{Wasserman and Faust(1994)}]{wassermanFaust}
\bibinfo{author}{Wasserman, S.}, \bibinfo{author}{Faust, K.},
  \bibinfo{year}{1994}.
\newblock \bibinfo{title}{Social Network Analysis: Methods and Applications}.
\newblock \bibinfo{publisher}{Cambridge University Press}.
%
\bibitem[{Wilinski et~al.(2019)Wilinski, Mazzarisi, Tantari and
  Lillo}]{wilinski2019detectability}
\bibinfo{author}{Wilinski, M.}, \bibinfo{author}{Mazzarisi, P.},
  \bibinfo{author}{Tantari, D.}, \bibinfo{author}{Lillo, F.},
  \bibinfo{year}{2019}.
\newblock \bibinfo{title}{Detectability of macroscopic structures in directed
  asymmetric stochastic block model}.
\newblock \bibinfo{journal}{Physical Review E} \bibinfo{volume}{99},
  \bibinfo{pages}{042310}.
%
\bibitem[{Xu and Tian(2015)}]{Xu}
\bibinfo{author}{Xu, D.}, \bibinfo{author}{Tian, Y.}, \bibinfo{year}{2015}.
\newblock \bibinfo{title}{A comprehensive survey of clustering algorithms}.
\newblock \bibinfo{journal}{Annals of Data Science} \bibinfo{volume}{2},
  \bibinfo{pages}{165--193}.
%
\bibitem[{Zachary(1977)}]{zachary}
\bibinfo{author}{Zachary, W.W.}, \bibinfo{year}{1977}.
\newblock \bibinfo{title}{An information flow model for conflict and fission in
  small groups}.
\newblock \bibinfo{journal}{Journal of Anthropological Research}
  \bibinfo{volume}{33}, \bibinfo{pages}{452--473}.
%
\bibitem[{Zhang et~al.(2012)Zhang, Wang, Zhao and Tang}]{graphDegreeLinkage}
\bibinfo{author}{Zhang, W.}, \bibinfo{author}{Wang, X.}, \bibinfo{author}{Zhao,
  D.}, \bibinfo{author}{Tang, X.}, \bibinfo{year}{2012}.
\newblock \bibinfo{title}{Graph degree linkage: Agglomerative clustering on a
  directed graph}, in: \bibinfo{booktitle}{ECCV'12: Proceedings of the 12th
  European conference on Computer Vision}, \bibinfo{publisher}{Springer}. pp.
  \bibinfo{pages}{428--441}.
%
\bibitem[{Zhang et~al.(2022)Zhang, Pietrasik, Xu and Reformat}]{IPM3}
\bibinfo{author}{Zhang, Y.}, \bibinfo{author}{Pietrasik, M.},
  \bibinfo{author}{Xu, W.}, \bibinfo{author}{Reformat, M.},
  \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Hierarchical topic modelling for knowledge graphs}.
\newblock \bibinfo{journal}{Lecture Notes in Computer Science (including
  subseries Lecture Notes in Artificial Intelligence and Lecture Notes in
  Bioinformatics)} \bibinfo{volume}{13261 LNCS}, \bibinfo{pages}{270--286}.
\newblock \DOIprefix\doi{10.1007/978-3-031-06981-9_16}.

\end{thebibliography}


\clearpage

%
%


\begin{figure}[p!]
    \centering
    \includegraphics[width=0.95\textwidth]{IPM_Final_methods_maps_presentation.pdf}
    \caption{Rand scores with respect to in- and out-density for the selected new methods.}
    \label{fig:in_my_rand}
\end{figure}

\begin{figure}[p!]
    \centering
    \includegraphics[width=0.95\textwidth]{IPM_Reference_methods_maps_presentation.pdf}
    \caption{Rand scores with respect to in- and out-density for the selected reference methods.}
    \label{fig:in_ref_rand}
\end{figure}

\clearpage

\begin{figure}[p!]
    \centering
    \includegraphics[width=0.95\textwidth]{IPM_Final_methods_vncn.pdf}
    \caption{Rand scores with respect to the number of clusters and the number of vertices in a single cluster for the selected new methods.}
    \label{fig:cnvn_my_rand}
\end{figure}

\begin{figure}[p!]
    \centering
    \includegraphics[width=0.95\textwidth]{IPM_Final_reference_vncn.pdf}
    \caption{Rand scores with respect to the number of clusters and the number of vertices in a single cluster for the selected reference methods.}
    \label{fig:cnvn_ref_rand}
\end{figure}



\end{document}
