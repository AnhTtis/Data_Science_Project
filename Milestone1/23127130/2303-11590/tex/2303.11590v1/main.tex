%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.
\renewcommand{\baselinestretch}{0.955}


%%% BEGIN YTC shortcuts
\newcommand{\ub}{\underline{\bf{}}}
% \renewcommand{\baselinestretch}{1}

\newtheorem{Theorem}{Theorem}[section]
\newtheorem{Lemma}[Theorem]{Lemma}
\newtheorem{Proposition}[Theorem]{Proposition}
\newtheorem{Definition}[Theorem]{Definition}
\newtheorem{Corollary}[Theorem]{Corollary}
\newtheorem{Notation}{Notation}
\newtheorem{Remark}[Theorem]{Remark}
% \numberwithin{equation}{section}

\def\no{\noindent} \def\p{\partial} \def\nb{\nonumber}
\def \Vh0{\stackrel{\circ}{V}_h} \def\to{\rightarrow}
\def\cl{\centerline}   \def\ul{\underline}
\def\Om{\mathcal{T}}  \def\om{\mathcal{T}} \def\I{ {\rm (I) } }
\newcommand{\q}{\quad}   \newcommand{\qq}{\qquad} \def\R{{\mathbb R}}
\def\l{\label}  \def\f{\frac}  \def\fa{\forall}
%\def\D{\end{document}}
\def\b{\beta}  \def\a{\alpha} \def\eps{\varepsilon}
\def\m{\mbox} \def\t{\times}  \def\lam{\lambda}
\def\Box{\sharp}
\def\fn{\footnote}

\def\u{{\bf u}} \def\v{{\bf v}} \def\w{{\bf w}}
\def\n{{\bf n}}  \def\x{{\boldsymbol x}} \def\A{{\bf A}}
\def\E{{\bf E}} \def\H{{\bf H}} \def\J{{\bf J}}

\newcommand{\lc}
{\mathrel{\raise2pt\hbox{${\mathop<\limits_{\raise1pt\hbox
{\mbox{$\sim$}}}}$}}}

\newcommand{\gc}
{\mathrel{\raise2pt\hbox{${\mathop>\limits_{\raise1pt\hbox{\mbox{$\sim$}}}}$}}}

\newcommand\zgnote[1]{\textcolor{blue}{[ZG: #1]}}

\newcommand{\ec}
{\mathrel{\raise2pt\hbox{${\mathop=\limits_{\raise1pt\hbox{\mbox{$\sim$}}}}$}}}

\def\bb{\begin{equation}} \def\ee{\end{equation}}
\def\beqn{\begin{eqnarray}}  \def\eqn{\end{eqnarray}}
\def\beqnx{\begin{eqnarray*}} \def\eqnx{\end{eqnarray*}}
\def\bn{\begin{enumerate}} \def\en{\end{enumerate}}
\def\i{\item}
\def\bd{\begin{description}} \def\ed{\end{description}}
%%% END YTC shortcuts


\input{packages}

\title{\LARGE \bf
Koopman-Hopf Hamilton-Jacobi Reachability and Control % tempting
% Koopman-Hopf for Approximate Hamilton-Jacobi Reachability in \\ High-Dimensional, Nonlinear Systems % OG
% Koopman-Hopf Procedure for Hamilton-Jacobi Reachability and Control \\ in High-Dimensional, Nonlinear Systems % yes but bleh
% A Koopman-Hopf approach for Hamilton-Jacobi Reachability in High Dimensonal Non-Linear Systems % too long
}

\author{Will Sharpless, Nikhil U. Shinde, Matthew Kim, Yat Tin Chow and Sylvia Herbert
\thanks{Sharpless, Shinde, Kim, and Herbert are with University of California, San Diego. Chow is with the University of California, Riverside. 
\{\href{mailto:wsharpless@ucsd.edu}{wsharpless}, \href{mailto:nshinde@ucsd.edu}{nshinde}, \href{mailto:mak009@ucsd.edu}{mak009}, \href{mailto:sherbert@ucsd.edu}{sherbert}\}@ucsd.edu, \href{mailto:yattinc@ucr.edu}{yattinc}@ucr.edu
This work is supported by ONR YIP N00014-22-1-2292.}
}

%
%, 

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}
\begin{abstract}
The Hopf formula for Hamilton-Jacobi Reachability analysis has been proposed for solving viscosity solutions of high-dimensional differential games as a space-parallelizeable method. In exchange, however, a complex, potentially non-convex optimization problem must be solved, limiting its application to linear time-varying systems. With the intent of solving Hamilton-Jacobi backwards reachable sets (BRS) and their corresponding online controllers, we pair the Hopf solution with Koopman theory, which can linearize high-dimensional nonlinear systems. We find that this is a viable method for approximating the BRS and performs better than local linearizations. Furthermore, we construct a Koopman-Hopf controller for robustly driving a 10-dimensional, nonlinear, stochastic, glycolysis model and find that it significantly out-competes both stochastic and game-theoretic Koopman-based model predictive controllers against stochastic disturbance.
\end{abstract}

% \section{Outline}
% \begin{enumerate}
%     \item Intro
%     \item Hopf Background
%     \item Koop Background
%     % \item Convexity Issue
%     \item Exact Linearization
%     \begin{itemize}
%         \item 2D Plot of BRS w/ \textit{hj-reachability.py}
%         \item Plot Dim vs. Compute w/ both
%     \end{itemize}
%     \item Inexact Linearization 
%     \begin{itemize}
%         \item Autonomous and low-d: using \textit{AutoKoopman.py} \\ comp w/ Koop-Zonotoping, \textit{hj-reachability.py}
%         \item Cntrolled and high-d: Glycolysis + EDMD \\ comp w/ Koop-Conformant Controller (?)
%     \end{itemize}
%     \item Brief \textit{HopfReachability.jl} paragraph
%     \item Conclusion
    
% \end{enumerate}

\section{Introduction}

There are several active directions for designing safe autonomous systems and all are united by the overarching difficulty of the problem. The technical rigor involved in planning for success while simultaneously avoiding failure tends to force methods to sacrifice guarantees (e.g. data-driven methods where success is higher but failure is too) or feasibility (e.g. differential inclusion where solutions are often over-conservative or burdensome to solve).  

Among these approaches, Hamilton Jacobi reachability (HJR) is well known for being a robust approach to optimal control and safe path planning \cite{bansal2017hamilton}. It has taken seat as the golden standard in robotics, autonomous driving and other stochastic control because of its connection to the fundamental theory underlying differential games \cite{evans1984differential,lions1986hopf} which describe how to optimally posture a system to withstand antagonistic or stochastic disturbances \cite{bacsar1998dynamic}. HJR is also, however, infamous for being a burdensome computational approach because of its dependency on spatial gradient approximations, making it sensitive to the \textit{curse of dimensionality} \cite{bansal2017hamilton}. If this theory could be extended to higher dimensional systems, engineering efforts in diverse domains, particularly in medicine, finance and other large systems, could make strides where simpler (dimension-robust) controllers are unable to overcome stochasticity. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{BRS_duffing_2s_scatter_nice.png}    
    \caption{Here the $\pm \epsilon$ boundary of a target (black), the target's true reachable set (blue), local linearization Hopf BRS (green), and our Koopman-Hopf BRS solved in a 15 dimensional Koopman space (gold) for the Duffing oscillator at $t=2s$ with control and disturbance. %\zgnote{for this figure, seems like the gold one approximates the best right? Maybe try to use bigger points to emphisize the good results and use smaller points to weak the 'not so good' resutls. (or maybe darker vs lighter color) }
    \vspace{-2em}}
    \label{fig:BRSapprox} 
\end{figure}

Toward this end, several works have been put forth, including set-based propagation, often called the method of zonotopes \cite{althoff2011zonotope}, to over-approximate the Hamilton Jacobi reachable states with linear systems \cite{althoff2011zonotope,Bak} and in some special classes of nonlinear systems \cite{majumdar2014convex}. The only shortcomings with these methods are that they don't inherently provide an optimal controller and also tend to be overly-conservative.

Another direction has been pursued in the face of this dimensionality obstacle: the method of decomposition and system reduction for HJR. Several works have defined the structures of systems that can be decomposed into independent low-dimension problems solved exponentially faster \cite{chen2017exact}. There also exists a work discussing how projections to lower dimensional systems can be formulated such that the inverted solution is conjectured to be an over approximation \cite{Ian03}; this is profoundly relevant to our later discussion.

Yet another route for solving HJR, the \textit{generalized Hopf formula} was recognized \cite{darbon2016algorithms,chow2017algorithm,chow2019algorithm} as an alternative method to solving HJR without sensitivity to dimension. This method involves interchanging a dynamic programming problem for an abstract optimization problem over the characteristic curves of the Lagrange multiplier \cite{darbon2016algorithms}. Note, it does not lend a ``free lunch", instead the difficulty appears now as a so-called \textit{curse of complexity} because the optimization problem is difficult to solve. Nonetheless, certain classes of systems, namely linear time-varying systems, can be robustly solved in both one-player \cite{doggn2020hopf,donggun19iterativehopf} and two-player optimal cases \cite{Kirchner_2018,chow2017algorithm}. These attempts have been applied with simple and local linearization methods to drive nonlinear systems with success, although without random disturbance. 

In this work, we build on the aforementioned strides by novely pairing the Hopf solution with Koopman theory for global linearization. Koopman theory involves 'lifting' nonlinear dynamics to high dimensional spaces in order to linearize them with higher accuracy. Many have rushed to the application of Koopman which in practice deviates from its theoretical guarantees but often yields impressive results nonetheless. Many papers have found that the procedure can yield highly accurate predictions over a short horizon, suitable for Model Predictive Control (MPC) and LQR \cite{proctor2014dynamic,yeung2019learning,CompKoop,kaiser2021data}. Furthermore, we are inspired by recent and impressive work into Koopman theory applied to the method of Zonotopes for general (non-optimal) reachability \cite{Bak}.

Our resulting Koopman-Hopf method can be used to solve HJ backwards reachable sets (BRSs); a BRS is the set of states from which the system can reach a target set at a desired time. A BRS generally may expand in all directions of the state space, and we hypothesized a spatially-balanced linearization would be necessary for accuracy. We find that this has much to do with \cite{Ian03} and solving HJR in a ``parallel" space. Although optimizing the resulting Hopf problem is not simple, we find that with sophisticated formulations such as the alternating direction method of multipliers (ADMM), close approximations of the true BRS can be achieved (see Fig.~\ref{fig:BRSapprox}). Moreover, we show that the resulting controller fares better than a stochastic Koopman-MPC or game-theoretic Koopman-MPC for a 10-dimensional glycolysis model. Ultimately, the accuracy of the BRS's and the robust navigation through a high dimensional, stochastic nonlinear system defend that the proposed pairing is not only valid, but a prudent choice when juxtaposed with other linearization methods or controllers in the Koopman space.

% \textbf{Related work:}

% \begin{enumerate}
%     \item HJR
%     \item Linear reachability (zonotopes etc)
%     \item Hopf, 1-Player or naive linearizations 
%     \item Koopman, Koopman-MPC, Koopman-LQR
%     \item Koopman Reachability (stanley bak)
% \end{enumerate}


% \subsection{Hopf Reachability Analysis of 2-Player Games}
% \begin{enumerate}
%     \item great for LTV systems with bounds on input, disturbance
%     \item if nonlinear, optimization of the 2-Player Hopf solution suffers from the curse of complexity
% \end{enumerate}

% \subsection{Koopman Lifting of Nonlinear Systems}
% \begin{enumerate}
%     \item can capture nonlinearities by lifting to higher dim
%     \item since hopf can handle higher dimensions, we propose merging these
% \end{enumerate}

\section{Preliminaries}\label{sec:prelims}
This paper focuses on control-affine and disturbance-affine systems of the form
\beqn
\dot x = f(x, t) + B_1(t)u + B_2(t)d,
\label{Dynamics}
\eqn
%where $x(\cdot) \subset \mathcal{X} := \mathbb{R}^{n_x}$ is the state trajectory evolving from initial state $x$, $u(\cdot) \subset \mathcal{U} \subset \mathbb{R}^{n_u}$ is the control trajectory and $d(\cdot) \subset \mathcal{D} \subset \mathbb{R}^{n_d}$ is the disturbance trajectory, and $\mathcal{U}$ and $\mathcal{D}$ are convex sets constraining the respective inputs. 
where control and disturbance inputs $u$ and $d$ are drawn from convex sets $\mathcal{U} \subset \R^{n_u}$, $\mathcal{D} \subset \R^{n_d}$, 
and the control and disturbance functions $u(\cdot)$ and $d(\cdot)$ are assumed to be of the set of measurable functions $\mathbb U : [t,0] \mapsto \mathcal{U}$, $\mathbb D : [t,0] \mapsto \mathcal{D}$.
Assuming that the dynamics \eqref{Dynamics} is Lipschitz continuous in $(x,u)$ and continuous in $t$, there exists a unique trajectory $x(\cdot) \subset \mathcal{X} := \mathbb{R}^{n_x}$ of the system given initial state $x$, control function $u(\cdot)$, and disturbance function $d(\cdot)$. 

% \zgnote{Here, $\mathcal {X,U,D}$ are sets, denoting the control you take at a specific time right? and the corresponding trajectory should be a function: $u(\cdot): [t,T] \mapsto \mathcal U$ right? Then probably need another set denote mesurable function, i.e. $\mathbb U$ to denote these trajectory.}

\subsection{Hamilton-Jacobi Reachability}\label{sec:HJR}
% Consider a system,
% \beqn
% \dot x(\tau) = f(x(\tau)), u(\tau), d(\tau), \tau) \quad \forall \tau \in [t, T],\: x(t)= x
% \label{Dynamics}
% \eqn

% \beqn
% ||f(x, u, d, t) - f(x', u, d, t)|| \leq L_f||x - x'|| \quad x, x' \in \mathbb{R}^{n_x}.
% % \label{Lipchitz}
% \eqn

To design a safe autonomous controller, HJ reachability solves for the optimal control that counters an adversarial disturbance in a differential game. Here, the control player's objective is to minimize the game cost while the disturbance player seeks to maximize it \cite{chow2017algorithm}. 
The game is defined by the cost functional
\beqn
P(x, u(\cdot), d(\cdot), t) = J(x(T)) + \int_t^T L(u(\tau), d(\tau)) d\tau ,
\label{GameCost}
\eqn
where $x(T)$ is the solution of (\ref{Dynamics}) at time $T$. The terminal cost $J:\mathbb{R}^{n_x} \rightarrow \mathbb{R}$ is a convex, proper, lower semicontinuous function chosen such that
\beqnx
\begin{cases}
J(x) < 0 \:\: \text{ for } \:\:x \in \mathcal{T} \setminus \partial\mathcal{T} \\
J(x) = 0 \:\: \text{ for }\:\:x \in \partial\mathcal{T} \\
J(x) > 0 \:\: \text{ for } \:\:x \notin \mathcal{T} \\
\end{cases}
\label{InitialValue}
\eqnx
where $\mathcal{T} \subset \mathcal X$ is a user-defined closed set representing the target to reach (or avoid, if the max/min are switched) and $\partial \mathcal{T}$ its boundary. The running cost $L:\mathbb{R}^{n_u} \times \mathbb{R}^{n_d} \rightarrow \mathbb{R}$ serves only to constrain the inputs and, thus, takes the form,
\beqn
L(u, d) = \mathcal{I}_\mathcal{U}(u) - \mathcal{I}_\mathcal{D}(d)
\label{RunningCost}
\eqn
where $\mathcal{I}_\mathcal{C}$ are the indicator functions of $\mathcal{U}$ and $\mathcal{D}$,
\beqn
\mathcal{I}_\mathcal{C}(c) = \{0 \text{ if } c \in \mathcal{C}, +\infty \text{ else} \}.
\label{Indicator}
\eqn
We have now defined the game such that for trajectory $x(\cdot)$ arising from a given $x$, $u(\cdot)\subset \mathcal{U}$, $d(\cdot)\subset \mathcal{D}$, $t$, and (\ref{Dynamics}), 
\beqn
P(x, u(\cdot), d(\cdot), t) \leq 0 \iff x(T) \in \mathcal{T}.
\label{GameMeaning}
\eqn


% where $p\in\mathbb{R}^{n_x}$ is the Lagrange multiplier of the dynamics constraint. In this paper we assume that $\forall t$ the Hamiltonian obtains a unique Nash equilibrium (saddle point) for the control and disturbance strategies (Isaac's condition). While restrictive, this assumption applies to a large class of systems, namely linearly-affine systems of the form,
% \beqn
% \dot x = f(x, t) + B_1(t)u + B_2(t)d.
% \label{DynamicsAffine}
% \eqn
% Isaac's condition guarantees that the game has value, i.e. that the lower and upper value of the game coincide and, thus, can be defined unanimously by
The value function  $V:\mathbb{R} \times R \rightarrow \mathbb{R}$ encodes the optimal cost of trajectories,
\beqn
V(x,t) = \inf_{u(\cdot) \subset \mathcal{U}} \sup_{d(\cdot) \subset \mathcal{D}} P(x, u(\cdot), d(\cdot), t).
\label{GameValue}
\eqn
This function is useful because by the same logic of (\ref{GameMeaning}),
\beqn
V(x,t) \leq 0 \iff x \in \mathcal{R}(\mathcal{T},t)
\label{ValueMeaning}
\eqn
where $\mathcal{R}(\mathcal{T},t)$, the Backwards Reachable Set (BRS), is the set of all points which can be driven to the target despite worst-case disturbance, formally defined by
\beqn
\mathcal{R}(\mathcal{T},t) =\{x \:\:  | \:\:  \exists u(\cdot)\subset \mathcal{U} \:\: \forall d(\cdot)\subset \mathcal{D} \text{ s.t. } \\ x(\cdot) \text{ satisfies } (\ref{Dynamics}) \land x(T) \in \mathcal{T}\}.
\label{BRS}
\eqn
Notably, applying Bellman's principle of optimality to this time-varying value function $V$ leads to the following well known theorem.
\begin{mytheorem}
[Evans 84] \cite{evans1984differential}
Given the assumptions (2.1)-(2.5) in [Evan 84], the value function $V$ defined in (\ref{GameValue}) is the viscosity solution to the following Hamilton-Jacobi Partial Differential Equation,
\beqn
\begin{aligned}
\frac{\partial V}{\partial t}  + H(x, \nabla_x V, t) &= 0 \qquad \text{ on } \mathbb{R}^{n_x} \times [t,T], \\
V(x,T) &= J(x(T)) \:\: \text{ on } \mathbb{R}^{n_x} 
\end{aligned}
\label{HJPDE-V}
\eqn
where the Hamiltonian $H:\mathbb{R}^{n_x} \times \mathbb{R}^{n_x} \times [t,T] \rightarrow \mathbb{R}$ is defined as 
\beqn
H(x, p, t) = \min_{u \in \mathcal{U}} \max_{d \in \mathcal{D}} p \cdot f(x, u, d, t)
\label{Hamiltonian}
\eqn
\end{mytheorem}

To solve this PDE, therefore, yields the value function and corresponding BRS. Additionally, the value function can be used to derive the optimal control and disturbance strategy for any point in space and time with,
\beqn
\begin{aligned}
u^*(t) = \arg \min_{u \in \mathcal{U}} \nabla_x V(x,t) \cdot B_1(t)u \\
d^*(t) = \arg \max_{d \in \mathcal{D}} \nabla_x V(x,t) \cdot B_2(t)d
\end{aligned}
\label{HJoc}
\eqn

The main challenge of HJR lies in solving this PDE in (\ref{HJPDE-V}); numerical methods propagate $V(x, t)$ by finite-differences over a grid of points that grows exponentially with respect to $n_x$ \cite{bansal2017hamilton}. In practice, this method is computationally intractable for systems of $n_x \geq 6$ and typically reserved for offline planning.
    
\subsection{The Hopf Solution to HJ-PDE's}\label{subsec:hopf}

A recent alternative to the brute force grid solving of $V(x,t)$ is via the Hopf formula, which offers a solution to (\ref{HJPDE-V}) in the form of a space-parallelizeable optimization problem. 
First, we define $\phi(x,t):=V(x, T-t)$ to change the aforementioned final-value problem into an initial-value problem. $\phi$ is now the viscosity solution of
\beqn
\begin{aligned}
- \frac{\partial \phi}{\partial t}  + H(x, \nabla_x \phi, t) &= 0 \qquad \text{ on } \mathbb{R}^{n_x} \times [0,t], \\
\phi(x,0) &= J(x) \:\: \text{ on } \mathbb{R}^{n_x} 
\end{aligned}
\label{HJPDE-phi}
\eqn
with Hamiltonian,
\beqn
H(x, p, t) = \max_{u \in \mathcal{U}} \min_{d \in \mathcal{D}} -p \cdot f(x, u, d, T-t).
\label{HamiltonianPhi}
\eqn
and $f$ defined in (\ref{Dynamics}).

Note, for systems $f(u,d,t)$ without state dependence, the Hamiltonian $H$ also lacks state-dependence i.e. $H(x,p,t) = H(p,t)$ and under such conditions the following Hopf formula was proved to solve the viscosity solution for 1-player games in \cite{lions1986hopf,rublev2000generalized,kurzhanski2014dynamics} and extended to 2-player games in \cite{darbon2016algorithms}, .


% {\color{red}
% You probably want to cite [11, 21, 35] in [CDOY 17], especially [35], for this time dependant formula.
% Please look into Section 2.3 for the rigorous statement of the theorem right after (2.7).  You see a technical assumption that H is required to be pseudo-convex w.r.t.the argument p in the set of minimum arguments S(t,x) in [CDOY 17]
% I think you would like to shorten it, so perhaps again you may refer to the conditions in [35] in [CDOY 17] and [CDOY 17] both for reference of the conditions.

% [35] in [CDOY 17]: 
% I.V. Rublev, Generalized Hopf formulas for the nonautonomous Hamilton-Jacobi equation, Computational
% Mathematics and Modeling 11.4,pp. 391-400 (2000).

% In there, again there will be Lipschitz conditions etc, and the above "pseudo-convex" assumption.  

% P.S. the formula gives minimax-viscosity solution, which is slightly different from viscosity solution, the two definitions are not the sme.  (Again See section 2.3 in [CDOY 17].)  The two coincides when H is convex, but in general they may differ.
% You may want to weaken this point though.

% In the "control" case instead of game, when H is convex, another citation you may have is [21, Section 5.3.2, p. 215] from the [Kirchner] paper for IEEE Control Systems Letters

% A. B. Kurzhanski and P. Varaiya. Dynamics and Control of Trajectory
% Tubes: Theory and Computation, volume 85. Springer, 2014.

% You may want to follow the Kirchner paper to weaken the difference if you want.

% }

\begin{mytheorem} [Lions 1986, Darbon and Osher 2016] 
Assuming $J$ convex, proper and lower-semicontinuous as in (\ref{InitialValue}) and $H$ smooth and state-independent, the viscosity solution $\phi(x,t)$ of (\ref{HJPDE-phi}) is given by the Hopf formula,
\beqn
\phi(x,t) = -\min_{p \in \mathbb{R}^{n_x}} \bigg\{ J^\star(p) - x \cdot p + \int_0^t H(p, \tau) d\tau \bigg\}
\label{HopfFormula}
\eqn
where $J^\star(p):\mathbb{R}^{n_x} \rightarrow \mathbb{R} \cup \{+\infty\}$ is the Fenchel-Legendre transform (i.e. convex-conjugate) of a convex, proper, lower semicontinuous function $J:\mathbb{R}^{n_x} \rightarrow \mathbb{R}$ defined by
\beqn
J^\star(p) = \sup_{x\in \mathbb{R}^{n_x}} \{ p \cdot x - J(x) \}.
\label{FL}
\eqn
\end{mytheorem}
We may note, from the definition of the Fenchel-Legendre (FL) transform, (\ref{HopfFormula}) may be rewritten as \cite{lions1986hopf}
\beqn
\phi(x,t) = \bigg(J^\star + \small\int_0^t H\bigg)^\star (x)
\label{FLphi}
\eqn
illustrating that the value function is itself an FL transform and from the well known property of FL transforms \cite{lions1986hopf}, $\nabla_x \phi(x,t)$ is the minimizing argument of (\ref{HopfFormula}), which we may use to compute the optimal control strategy.

The strength of this method is that now we may derive the solution to (\ref{HJPDE-V} \& \ref{HJPDE-phi}) by solving a space-parallizeable optimization problem, avoiding the so called \textit{curse of dimensionality}. However, requiring a state-independent Hamiltonian, limits this method greatly. Furthermore, we can not guarantee we have converged to the globally minimizing argument ($=\nabla_x\phi$) if the Hamiltonian is non-convex, which can occur based on the relative sizes of $\mathcal{U}$ and $\mathcal{D}$.

However, it was noted in \cite{darbon2016algorithms,chow2017algorithm} that any linear time-varying system can be mapped to a state-independent system $\dot z$ 
\beqn
\begin{aligned}
\dot x = A(t)x + B_1(t) u + B_2(t) d \\ \rightarrow \dot z = \Phi(t)(B_1(t) u + B_2(t) d)
\end{aligned}
\label{LinearStateIndependent}
\eqn
with the linear time-varying mapping $z(t):=\Phi(t)x(t)$ defined by the fundamental matrix $\dot \Phi = A(t) \Phi(t), \Phi(0) = I $, which we note solves the linear solution of the system. The Hamiltonian for $\phi$ thus becomes,
\beqn
\begin{aligned}
H_\mathcal{Z} (p, t) =& \max_{u \in \mathcal{U}} -p \cdot \Phi(T-t) B_1(T-t) u \\ 
& - \max_{d \in \mathcal{D}} -p \cdot \Phi(T-t) B_2(T-t) d .
\end{aligned}
\label{HamiltonianLinear}
\eqn
Finally, since the mapping $x\rightarrow z$ is one-to-one, we know $\phi_\mathcal{Z}(z,t) = \phi(x,t)$ then
\beqn
\phi_\mathcal{Z}(x,t) = -\min_{p \in \mathbb{R}^{n_x}} \bigg\{ J_\mathcal{Z}^\star(p) - z \cdot p + \int_0^t H_z(p, \tau) d\tau \bigg\}
\label{HopfFormulaZ}
\eqn
where
\beqn
\begin{aligned}
z = z(0) = x(0) = x &\implies J_\mathcal{Z}(z) = J(x) \\
&\implies J_\mathcal{Z}^\star(p) = J^\star(p).
% \label{eqn}
\end{aligned}
\eqn

Given the convexity of $\mathcal{U}$ and $\mathcal{D}$, the Hamiltonian in (\ref{HamiltonianLinear}) can be rewritten as the difference of two positively homogeneous Hamiltonians corresponding to the convex-conjugates of their Indicator functions  \cite{chow2017algorithm},
\beqn
\begin{aligned}
H(p,t) = \mathcal{I}_\mathcal{U}^\star(R_1 p) - \mathcal{I}_\mathcal{D}^\star(R_2 p) \\ R_i:=-B_i(T-t)^\dagger \Phi(T-t)^\dagger 
\end{aligned}
\label{HamiltonianIndicator} 
\eqn
which allows for rapid and efficient computation of (\ref{HopfFormulaZ}) that has been observed to scale linearly with $n_x$ for a fixed $t$ \cite{chow2017algorithm} and demonstrated in online guidance with naive linearizations \cite{Kirchner_2018}. Notably, when $\mathcal{U}$ and $\mathcal{D}$, are constrained by norms, these functions have analytical solutions, namely the dual norms. Given $Q\in\mathbb{R}^{n \times n}$, consider two popular input constraints \cite{chow2017algorithm} and their $\mathcal{I}^\star$:
\beqn
\begin{aligned}
 \mathcal{C}_{\mathcal{E}}(Q) := \{c\:\:|\:\:||c||_Q \leq 1\} \implies \:\: \mathcal{I}_\mathcal{C}^\star(\cdot) = ||\cdot||_{Q^{-1}}, \\
 \mathcal{C}_{\mathcal{R}}(Q) := \{c\:\:|\:\:||Qc||_\infty \leq 1\} \implies \:\: \mathcal{I}_\mathcal{C}^\star(\cdot) = ||Q\cdot||_1.
\end{aligned}
\label{InputConstraint} 
\eqn
% where $||\cdot||_Q = ||\Sigma V^T \cdot||_2$ is the elliptical norm described by $Q^{-1}$ and $||Q\cdot||_\infty$ the rectangular norm. 
Note, if $\mathcal{U}$ and $\mathcal{D}$ are defined by the same set type in (\ref{InputConstraint}) with $Q_u$ and $Q_d$ respectively, then convexity of the Hamiltonian is given when $Q_u \succeq Q_d$.


\subsection{Koopman Theory}\label{sec:Koop}

We would like to apply dimension-robust Hopf theory to high-dimensional, nonlinear systems with a highly accurate linearization to approximate BRS's and synthesize safe control. Moreover, we would like a non-local linearization method due to the need for global analysis across the state space. Koopman theory, although imperfect, is known for outperforming other linearization methods in these regards \cite{proctor2014dynamic}, \cite{yeung2019learning}.

Consider the discretized mapping of a nonlinear system,
$
x(t_{i+1}) = F(x(t_i))$.
The Koopman operator \cite{koopman1931hamiltonian} $\mathcal{K}:\mathcal{F} \rightarrow \mathcal{F}$ is defined as
% \beqn
$\mathcal{K} g:=g \circ F$,
% \label{KoopDef}
% \eqn
where $\mathcal{F}$ is the collection of all functions that form an infinite Hilbert space, often called observables, and $g \in \mathcal{F}:\mathcal{X} \rightarrow \mathbb{R}$ \cite{CompKoop}. By definition, the operator has the property,
\beqn
(\mathcal{K}g)(x(t_i)) = g(F(x(t_i))) = g(x(t_{i+1}))
\label{KoopProperty}
\eqn
and in practice, we assume this holds for a finite space such that,
\beqn
Kg \in \mathcal{G} \quad \forall x\in \mathcal{X}\:\:s.t.\:\:g = \Psi(x).
\label{KoopInvariance}
\eqn
where $\mathcal{G}$ is an invariant subspace of the Koopman space spanned by a finite basis of observables $\Psi(x):= \{ \psi_1(x),\: \dots, \psi_{n_k}(x)$ and $K \in \mathbb{R}^{n_k \times n_k}$ is a finite matrix.

%Despite that Koopman and Von Neumann did not consider a system with external inputs, several group 
Recent works have found this theory can be extended to systems with external inputs \cite{proctor2014dynamic,kaiser2021data,bruder2019modeling,CompKoop} such that for $x(t_{i+1}) = F(x(t_i),u(t_i),d(t_i))$,
\beqn
g(t_{i+1}) \approx Kg(t_i) + L_1 u(t_i) + L_2 d(t_i).
\label{KoopControl}
\eqn
where $L_1$ and $L_2$ are the Koopman control and disturbance matrices. Moreover, although absent from the original theory, we have found it useful, like others \cite{proctor2014dynamic,yeung2019learning}, to consider a return mapping $\widetilde{\psi}:\mathcal{G} \rightarrow \mathcal{X}$ which is not injective in general because of the approximate nature of (\ref{KoopControl}). 

\section{Koopman-Hopf Reachability}\label{sec:koopman_hopf}

% \subsection{Reachability in Lifted Spaces}\label{sec:lifted_reach}

We now seek to approximate the value and BRS (\ref{sec:HJR}) of our differential game by uniting the Hopf solution with approximate dynamics derived from Koopman methods. We will first show how to define the target set in the lifted Koopman space. We will then discuss the lifting functions to the Koopman space that are well-suited to Hopf reachability analysis. Finally, we will describe how to solve the resulting Hopf reachability analysis using the lifted target set and dynamics.

\subsection{Defining the lifted target set}Given a target $\mathcal{T}$ defined in (\ref{sec:HJR}), our target can be lifted directly to the Koopman space with $\Psi$,
\beqn
\mathcal{T}_\mathcal{G} := \{ g \:\: |  \:\: g = \Psi(x), \: x \in \mathcal{T}\}.
\label{KoopTarget}
\eqn
However, given that we may not have an exact linearization i.e. $g(t_{i+1}) \neq Kg(t_i)$, we also propose an approximation of our target,
\beqn
\widetilde{\mathcal{T}}_\mathcal{G} := \{ g \:\: | \:\: \widetilde{\psi} (g) \in \mathcal{T}\}
\label{KoopTargetApprox}
\eqn
as the preimage of the target in the Koopman space under the return mapping $\widetilde{\psi}$. This captures the Koopman trajectories which might evolve off of the manifold defined by the true states $g = \Psi(x) \subset \mathcal{G}$ to $g' \in \mathcal{G}$ for which $\nexists x'$ such that $g' = \Psi(x')$.

%On the contrary, we are only concerned with finding the value of the states $\phi_\mathcal{G}(g,t)$ in the Koopman space such that $\exists x$ mapping to $g = \Psi(x)$. The value of states off of the true-state manifold might be the result of backwards-propagating a trajectory which may not ever have a real initial condition.

\subsection{Lifting the target set and dynamics}
The choice of lifting functions will impact the shape of the lifted target sets $\mathcal{T}_\mathcal{G}$ and $\widetilde{\mathcal{T}}_\mathcal{G}$. To provide guarantees that the Hopf analysis will return the optimal solution, the cost function (and therefore target set) must be convex. This can be enforced by either (a) using lifting functions $\Psi$ that are convex, or (b) taking the inner (for reach) or outer (for avoid) ball or convex-hull of $\Psi(\mathcal{T})$\footnote{alternatively, one could use the Lax formula \cite{chow2019algorithm}, which swaps the convexity assumption between $J$ and $H$. This approach has its own challenges for implementation beyond the scope of this paper.}.
% An important requirement for utilization of the Hopf solution is convexity of $J(x)$. This is true if the Target $\mathcal{T}$ is convex, thus, we need $\mathcal{T}_\mathcal{G}$ convex. We propose a couple solutions: \zgnote{Here you are saying that the target is convex is the sufficient condition for a convex cost right? Is this always true? Or maybe you want to say if the target is convex, we can always define a convex cost?}
% \begin{itemize}
%     \item Take the inner (for reach) or outer (for avoid) ball or convex-hull of $\Psi(\mathcal{T})$
%     \item Use the Lax formula [YTC19], which swaps the convexity assumption between $J$ and $H$
%     \item Use lifting functions $\Psi$ that are convex
% \end{itemize}
Using convex lifting functions is challenging, given that the difference of convex functions is not necessarily convex.  On the other hand, simply ``convexifying'' a lifted target from nonconvex lifting functions may be drastically conservative.
%(not to mention convex-hull methods also suffer from the \textit{curse of dimensionality}), the non-convex Lax formula would ultimately trouble our numerical optimization all the same, 
%and the notion of a convex basis is troubled as the difference of convex functions is not necessarily convex.

We were thus motivated to demonstrate the proposed method with two simple cases of lifting functions:
\begin{enumerate}
    \item the identity mapping, $I:\mathbb{R}^{n_x} \rightarrow \mathbb{R}^{n_x}$
    \item the multivariate polynomial of degree $l$, 
$P_l:\mathbb{R}^{n_x} \rightarrow \mathbb{R}^{M}, M = {n_x + l \choose l}  - 1$.
\end{enumerate}
Both cases have been extensively studied in Koopman literature; the former corresponds to Dynamic-Mode-Decomposition with control (DMDc) \cite{proctor2014dynamic} while the latter is a case of Extended-DMD with control \cite{kaiser2021data}. 

The identity mapping resolves the convexity issue, as
\beqn
I(\mathcal{X}) = \mathcal{X} \implies \mathcal{T} = \mathcal{T}_\mathcal{G} = \widetilde{\mathcal{T}}_\mathcal{G} \implies \phi_\mathcal{G}(g,t) = \phi(x,t),
\label{IdentityTarget}
\eqn
assuming we also define $\widetilde{\psi} = I$; this implies that our lifted target will be convex if our initial target is convex. 

%Although, we find it successful for one example of Koopman-based high-dimensional nonlinear control (\ref{sec:ctrl}), the 
The polynomial mapping $P_l(x)$ is more expressive and therefore produces more accurate linearizations of dynamics.
%identity mapping is known to be less "expressive" and yields less accurate linearizations than EDMD procedures [cites]. %On the other hand, the polynomial mapping  is better able to capture nonlinear dynamics with standard Koopman methods [cites], but requires some caution. 
The trouble with the polynomial mapping is in the negative domain $\mathbb{R}^{n_x}_<0:=\{x|x_i\le 0\}$. On this region, the mapping is not-invertible and non-convex, and therefore impacts the Hopf reachability solution. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{Glycolysis_experiment_2.png}    
    \caption{The four controlled evolutions of the 10D glycolysis model with the same random disturbance trajectory and initial condition. `Auto' signifies the controller which is disturbed but always chooses the trivial input. Its possible to see from the example trajectory the behavior which allows Hopf to succeed; it amplifies the cycle of the phosphate, glucose and ATP trajectories to achieve the ATP target.}
    \label{fig:glycolysis}\vspace{-1em}
\end{figure*}
%\zgnote{if the legend can be bigger, this will be a great figure}

% However, with a polynomial mapping, the following lemma proves that the mapping of the target set can be bounded to over-approximate the set. Here we consider the mapping of a spherical target $\mathcal{T}:=\mathcal{B}_r(x_c)=\{ x\:|\: ||x - x_c||_2 \le r\}$, however, the results might be extended to other norms.

% \begin{mytheorem}[\shnote{ name}]
% Given the boundary of a spherical target $\partial \mathcal{B}_r(x_c):=\{ x\:|\: ||x - x_c||_2 = r\}$, the $x_c$ shifted polynomial mapping, $g(x) := P_l(x-x_c)$ will be bounded by,
% \beqn
% \begin{aligned}
% % ||g(x) - g(x_c)||^2 \le \sum_k^L r^{2k} \quad x \in \partial \mathcal{B} \\
% ||\bar g(x) - \bar g(x_c)||^2 = \sum_k^l r^{2k} =: r_l \quad x \in \partial \mathcal{B}
% \end{aligned}
% \eqn
% if $\bar g(x) := T g(x) $ and $T := \text{Diag}(\{ \sqrt{{n_i \choose l_j}} \}_{i \in [1, l], j \in [1, n_x]})$.
% \label{PolynomialBound}
% \end{mytheorem}
% Hence, the surface of the ball will map to the surface of a larger ball of radius $r_l$ if we linearly scale the polynomial map by the square root of the binomial coefficients.
% This provides us with a method to approximate the non-convex mapping of the target $P_l(\mathcal{T})$ with a convex $\mathcal{T}_\mathcal{G} := \mathcal{B}_{r_l}$ such that 
% %\beqn
% $\forall g \in \mathcal{T}_\mathcal{G}, \quad (g = \Phi(x), x \in \mathcal{T}) \lor (g \notin \text{Im}(\Psi)).$ 
% %\eqn

To resolve this issue, one can consider reachable problems on the complement of $\mathbb{R}^{n_x}_<0$, the all-positive numbers $\mathbb{R}_{\ge 0}^{n_x}$, where the map is one-to-one.
However, in applications for safety, this would require a temporal bound on the BRS growth to know when we might be incorporating ambiguous trajectories in the solution of our game. Furthermore, this would constrain our lifting procedure undesirably. 

It is common to circumvent the issues above by using the first-degree polynomial terms \cite{proctor2014dynamic} to return to $\mathcal{X}$ such that
%\beqn
$\widetilde{\psi}(g) := \text{Proj}_\mathcal{X}(g).$
%\label{PolynomialReturn}
%\eqn
This would imply that $\widetilde{\mathcal{T}}_\mathcal{G}$ is the infinite extrusion of our target, and if our target were defined by either of the sets in (\ref{InputConstraint}) and a PSD matrix $A\in\mathbb{R}^{n_x \times n_x}$ (e.g. an ellipsoid or box), then
\beqn
\widetilde{\mathcal{T}}_{\mathcal{G}, \mathcal{C}} = \mathcal{C}(\hat A), \:\: \hat A:=\begin{bmatrix}A & 0 \\ 0 & 0\end{bmatrix} \in \mathbb{R}^{M \times M}
\label{PolynomialTargetApprox}
\eqn
is an infinite cylinder or prism. In practice, we find that finite-relaxations of this set defined by $\mathcal{C}(\hat A + \epsilon I)$ for $\epsilon << 1$ yield better solutions.

Finally, we note that to guarantee convergence to the global optimum, we also need convexity of the Hamiltonian. If the control and disturbance enter the Koopman space as in (\ref{KoopControl}), the sets will remain convex, however, there is a chance the $L$ matrices may change the relative sizes and break convexity. We could assume disturbances on the control input only to guarantee this, but we also find in practice that a powerful solver such as ADMM is robust to finding the global optimizer even in dimensions up to 15.

% Requirements
% \begin{enumerate}
%     \item For hard guarantees, need exact linearization and maintain convexity. Only in specific cases, i.e. with convex lifting functions and with disturbance on control only can we guarantee convexity.
%     \item If relax these requirements, lose guarantees but using a powerful solver which is known to perform well in non-convex cases, we gain approximations for high-dim nonlinear systems. Can be used as-is or to guide e.g. training.
%     \item how to lift target set / function to high-dim space and back.
% \end{enumerate}

\section{Results}\label{sec:results}

All results are computed using our codebase \href{https://github.com/UCSD-SASLab/HopfReachability}{\textit{HopfReachability.jl}}, an open-source package designed for solving 2-player linear differential games.

% \subsection{Exactly Linearizeable Systems}\label{subsec:EL}

% We begin by considering the well-known slow manifold system \cite{proctor2014dynamic}, which can be Koopman-linearized exactly. Although simple, we use this example to validate our codebase \href{https://github.com/UCSD-SASLab/HopfReachability}{\textit{HopfReachability.jl}}, an open-source package designed for solving 2-player linear differential games. Let us specifically consider an $N$-dimensional generalization of the slow manifold system, with control and disturbance, defined by
% \beqn
% \begin{aligned}
% \dot x_i =& \mu_i x_i + x_i^{-1}(u_i +d_i) \quad (i < N), \\
% \dot x_N =& \lambda \big(x_N - \frac{1}{N-1}\sum^{N-1} x_i^{k_i} \big) + u_i + d_i.
% \label{ExactLinearizeable}
% \end{aligned} 
% \eqn
% Akin to the traditional model, it can be linearized by defining $g_i(x) := x_i^{k_i} \implies \dot g_i = k_ix_i^{k_i - 1} \dot x_i = k_i \mu_i g_i$, leading to, $\dot g = Kg + Bu + Bd$. 

% We might note, however, that the system is bizarre near the origin; both control and disturbance tend to $\infty$. Moreover, we are faced with the previously discussed ambiguous lack of invertibility near the origin. Hence, to study Koopman-HJR in this canonical example, we consider a target defined at $5*\mathbf{1}$. Given that our mapping is locally one-to-one and exactly linearizeable, we use the direct lifted target $\mathcal{T}_\mathcal{G}$ as defined in (\ref{KoopTarget}). We compare our results with an HJR dynamic programming method, namely \textit{hj\_reachability.py}, considered to be close to ground truth. We quantify the error with the Jaccard Index over a common descritized grid,
% \beqn
% JI(\mathcal{R}_1,\mathcal{R}_2)= \frac{|\mathcal{R}_1 \cap \mathcal{R}_2|}{|\mathcal{R}_1 \cup \mathcal{R}_2|}.
% \label{Jaccard}
% \eqn
%  The results for the index and compute time versus dimension $N$ are displayed in Table~\ref{}.\shnote{where is this table}



\subsection{Approximate BRS of the lifted Duffing Oscillator}\label{subsec:BRSapprox}

Here, we test the Koopman-Hopf approach on the nonlinear Duffing oscillator in both the cases with disturbance on the control input (convex Hamiltonian) and disturbance on the full state (non-convex Hamiltonian) with the previously discussed ellipsoidal target (\ref{KoopTarget}). We solve the BRS's evolving from the lifted targets, backwards in time with Koopman dynamics derived from a 4-degree polynomial $P_4(x)$ (with 15 dimensions) that was hyper-tuned by the \textit{autokoopman.py} package written by the authors of \cite{Bak}. 

We compare our results with an HJR dynamic programming method, namely \textit{hj\_reachability.py}, considered to be close to ground truth. Figure~\ref{fig:BRSapprox} compares the BRS's at $t = 2$ in the problem with disturbance on all states.

We quantify the error with the Jaccard Index over a common discritized grid,
\beqn
JI(\mathcal{R}_1,\mathcal{R}_2)= \frac{|\mathcal{R}_1 \cap \mathcal{R}_2|}{|\mathcal{R}_1 \cup \mathcal{R}_2|}.
\label{Jaccard}
\eqn
The Jaccard index is used to quantify the similarity of these sets to the \textit{hj\_reachability.py} BRS, and we include a baseline derived from the Taylor series approximate dynamics with a localization point at the center of the target in $\mathcal{X}$. The numerical results can be viewed in Table~\ref{table:BRSresults}.

\begin{table}[h]
\caption{BRS similarity results compared to true BRS, $JI(\mathcal{R}(\mathcal{T}), \cdot)$ for disturbance on control and convex (left), disturbance on all states, nonconvex (right)}
\label{table:BRSresults}
\begin{center}
\begin{tabular}{|c||c|c||c|c|}
\hline
t & $\mathcal{R}(\mathcal{T}_{Taylor})$ & $\mathcal{R}(\widetilde{\mathcal{T}}_\mathcal{G})$ &  $\mathcal{R}(\mathcal{T}_{Taylor})$ & $\mathcal{R}(\widetilde{\mathcal{T}}_\mathcal{G})$  \\
\hline
\hline
  0.0  & 1.0 & 0.97    & 1.0 & 0.97   \\
-0.33  & 0.90 & 0.96   & 0.92 & 0.91    \\
-0.66  & 0.80 & 0.88   & 0.85 & 0.80    \\
-0.99  & 0.61 & 0.82   & 0.71 & 0.66    \\
-1.32  & 0.47 & 0.78   & 0.50 & 0.60    \\
-1.65  & 0.37 & 0.79   & 0.37 & 0.59    \\
-1.98  & 0.30 & 0.76   & 0.30 & 0.58    \\
\hline
\end{tabular}
\end{center}
\end{table}

%We do not include $\tilde{\mathcal{T}_G}$ here as it proved to be a massive under approximation of the reachable space, likely a consequence of the inexact nature of the linearization. 
Note, in this non-convex case we lose guaranteed convergence to the global optimum, which may explain why the Jacard index is similar between the Koopman and Taylor series results.% the equal $JI$ with the Taylor series and worse performance compared with the convex case, but overall, the similarity is decent.

\subsection{Comparison of Koopman controllers on the 10D Glycolysis Model}\label{subsec:Glycolysis}

Next, we compare the ability of a Koopman-Hopf controller to navigate the true nonlinear dynamics of a 10D glycolysis model extended from \cite{yeung2019learning,ruoff2003temperature,daniels2015efficient} given its 10-D DMDc linearization derived from the \textit{pykoopman.py} package. The Koopman-Hopf controller uses the standard Hopf controller formulation  \cite{donggun19iterativehopf,doggn2020hopf,Kirchner_2018} to solve for the minimum time $T^*$ for which the target is reachable given the current state $x$,
\beqn
T^* = \text{argmin}_T \phi(x, t) = \text{argmin}_T \phi(g, t), \quad g = \Psi(x).
\label{MinT}
\eqn
The controller then applies the optimal control at this time which is derived from,
\beqn
\nabla_p H(\nabla_{z_g} \phi(z_{g_0}, T^*), T^*) = e^{-tK} L_1 u^* + e^{-tK} L_2 d^*.
\label{HJocHopf}
\eqn

In Fig.~\ref{fig:glycolysis} we compare this Koopman-Hopf controller with two basic MPC formulations that also evolve their dynamics in the Koopman space \cite{bemporad2002model}. The MPC game (MPCg) solves the optimal control for a random fixed disturbance with a one step horizon, and then the disturbance does the same for the previous control, and this iterates for 3 iterations (where improvement plateaued). The standard MPC (MPCs) is a stochastic MPC \cite{mesbah2016stochastic} where the controller samples 20 random disturbance trajectories and minimizes the expectation of the cost given evolution with those samples. %In both cases, the MPC's are defined by a terminal cost alone to match the Hopf controller.

The goal given to all controllers is to achieve a target ATP concentration, as a bioengineer might desire for cell growth, by manipulating the external glucose, total NAD+/NADH pool, and ADP/ATP pool. After the controllers compute a $u(t)$, the system is then propagated with the true nonlinear dynamics and Radau. The system is complex because of the highly-stiff nonlinearities, state constraints ($x_i > 0$) and interconnectivity of the metabolic network; naively driving the ATP/ADP pool up leads to counter-productive results, as seen by MPCg and MPCs.

The simulation was run 50 times, with each controller subjected to the same random initial conditions sampled from the realistic concentration bounds given in \cite{ruoff2003temperature,daniels2015efficient} and same random disturbance trajectory. The success of the Koopman-Hopf controller can be observed in Table~\ref{MPCresults}. The Koopman-Hopf controller appears to overcome the nonlinearity of the system by amplifying the oscillations of ATP to reach the target.

\begin{table}[h]
\caption{Controller Results for 50 random $x_0$ bounded by \cite{daniels2015efficient} and random $d(\cdot)$}
\label{MPCresults}
\begin{center}
\begin{tabular}{|c||c|c|c|c|}
\hline
Controller & Success \% & Mean, Max ATP [m] & Mean $t_c$/step [s]\\
\hline
\hline
Auto & 20 & 2.07 & $\emptyset$ \\
Hopf & 96 & 3.06 & 0.81 \\
MPCg & 22 & 2.14 & 0.02 \\
MPCs & 16 & 1.96 & 0.36 \\
\hline
\end{tabular}
\end{center}
\end{table}



\section{Conclusion}\label{sec:conclusion}

We propose here the union of Koopman and Hopf methods in order to approximate Hamilton-Jacobi Reachability in high-dimensional, nonlinear systems. Although the work is brief and our examples limited, we find that this approach works well in the cases, one for approximating BRS's and the other for driving stochastic, high-dimensional, nonlinear systems. We hope to extend this work along several exciting directions, including but not limited to expanding to more expressive but analytically complicated lifting functions such as random Fourier features, radial basis functions and neural networks, lifting the input states for more accurate control, applying this to black box systems for which we have no model, and exploring a modular linearization approach for multi-agent Koopman-Hopf procedures. We believe that the proposed method will be very useful ultimately for controllng a wide range of otherwise intractable systems.

\section*{Acknowledgements}

We would like to thank Steven Brunton, Stanley Bak and Masih Haseli for discussions about Koopman spaces and control in them. We would also like to thank Somil Bansal for thought-provoking discussions that bettered the work. Finally, we would like to thank Zheng Gong and Sander Tonkens for valuable discussions and feedback on the paper.


\bibliographystyle{IEEEtran}
\bibliography{main}
\end{document}







