\begin{proof}[Proof of Theorem \ref{thm: lower_bd}]
For sake of contradiction, assume there is an online algorithm  $A$ that is a $c$-approximation for $c\leq \frac{k-1}{2} -\epsilon$ and any $\epsilon >0$. The algorithm is given a budget $\textsf{OPT}$.  The sequence of points that arrives will lie in $\mathbb{R}^k$. First there are $10k^2$ points $P_0$ that arrive at location $ (0,0,\dots, 0)$.  We may assume that $A$ assigns each of these the same label.  

There are $k-1$ remaining phases for how points are released. Index these phases as $1,2, \ldots k-1$. At the end of the $i$th phase, $A$ will have accumulated cost at least $\frac{i}{2} \textsf{OPT}$.  In the end, $A$'s cost will be at least $\frac{k-1}{2}\textsf{OPT}$, contradicting the assumption that $A$ is a $c$-approximation. Additionally, by the end of the $i$th phase, the algorithm will have used at least $i+1$ labels.  All points in the $i$th phase will arrive at location $(0, 0, \ldots \textsf{OPT}, \ldots 0)$ where the $i$th dimension is non-zero.  The number of points that arrive will be $10 k^2$ at this location.  The majority of these points will have to be labelled with a single label by the algorithm. 

Consider phase $i$.  The adversary releases one point $q_i$ at location $(0, 0, \ldots \textsf{OPT}, \ldots 0)$.  First \emph{assume} the algorithm the algorithm does not give this  point a new label. We will revisit this assumption at the end of the proof. Then regardless of what label the algorithm gives this point, it is distance at least $\textsf{OPT}$ from all other points that have arrived.  Thus, this point will add an additional $\textsf{OPT}/2$ cost to the algorithm.  Inductively, the total algorithm cost is at least $\frac{i}{2} \textsf{OPT}$.  

Next $10k^2 -1$ points arrive at this same location.  If the algorithm does not label half of these points with a new label, then these points will be grouped with points that arrived previously.  Since prior points are distances at least $\textsf{OPT}$ away, this will make the algorithm's total cost larger than $\Omega(k^2 \textsf{OPT})$. Thus, the algorithm must label half of these points with a new label.  Now the inductive invariants are satisfied.

At the end, the optimal solution labels the points that arrive in each phases a different label. The total optimal cost is $0$ and the algorithm has cost $\Omega(k \textsf{OPT})$.

Now we revisit the assumption that $q_i$ must not be given a new label by $A$.  Indeed, say it is given a new label.  Then the above procedure terminates at this time.  Instead, we know the algorithm has used $i+1$ labels.  There is a clustering of the points that have arrived whose cost is below \textsf{OPT} and only uses $i$ labels.  Intuitively, this is a large mistake. 

Indeed, consider the following. There will be $k-i$ points $P^*$ that will arrive.  The $\ell$th point arrives at location $(0, 0, \ldots L \cdot \textsf{OPT}, \ldots 0)$ where $L$ is a parameter. Here $\ell \in [k-i]$. The non-zero dimension is the $(i+\ell)th$ dimension. 

The optimal solution labels $q_i$ the same label as the points in $P_0$ and this cluster has cost \textsf{OPT}.  All other points get a unique label depending on which dimension is non-zero.  $k-1$ dimensions are used, combined with $P_0$ this is $k$ labels. The cost of these points are $0$ and the only cost is $q_i$ paying cost $\textsf{OPT}$.  The optimal cost is bounded. 

On the other hand, $A$  only has $k-i-1$ unused labels when points in $P^*$ begin arriving.  One of them must be given a label as some other point whose distance is at least $L\textsf{OPT}$ away.  The cost of this cluster is at least $\frac{L}{2} \textsf{OPT}$.  Setting $L$ large, contradicts the bounded approximation ratio of $A$. 






\end{proof}
