
\section{Introduction}

Clustering problems, such as $k$-means clustering
and $k$-median clustering, are a 
classic genre of learning / data mining problems~\cite{bishop}. 
Typically the input consists of a collection $X=\{ x_1, \ldots, x_n\}$ 
of points in some metric space $\mathcal M$ (typically $\Re^d$ with the 1-norm or 2-norm)
and a positive integer $k$. 
The output for a \textbf{center-based} clustering problem is a collection
$c_1, \ldots, c_k$ of $k$
points from $X$, called centers, that succinctly summarize the data points.
The implicit cluster $C_i$
corresponding to the center $c_i$ is the collection of points in
$X$ whose closest center is $c_i$, that is $C_i =\{ x_j \mid \argmin_{h \in [k]} d(x_j, c_h) = i  \}$, where $d(\cdot, \cdot)$ is the distance function for the metric space.
The output for a \textbf{cluster-based} clustering problem is 
a partition $C_1, \ldots C_k$ of
$X$ into $k$ parts, called clusters.
The implicit center
of each cluster $C_i$ is then  $c_i = \argmin_{x_h \in C_i} \sum_{x_j \in C_i} d(x_h, x_j)$.
For both center-based clustering and cluster-based clustering, the objective is to minimize the cost
of the clustering.  This paper considers the $k$-median objective which is 
the aggregate  distance from each point to the center of its cluster, that is 
$\sum_{i=1}^k \sum_{x_j \in C_i} d(x_j, c_i)$. 





Here we consider applications where the data points in $X$
arrive online over time. In a center-based clustering problem, the online algorithm
maintains a collection of centers.   In a cluster-based problem, 
the online algorithm needs to assign the data points  to a cluster
when they arrive, that is each point $x_j$ needs to be assigned 
a label $\ell_j \in [k]$ when $x_j$ arrives. 

An application of online clustering given by \cite{LSSTalk} is the task of
clustering news articles that arrive online.  For example at 
Yahoo news or Google news. We will refer to these outlets as the news providers. 
The  news provider selects some (approximately) fixed number $k$ of articles to feature
on the news homepage, and has a ``view complete coverage'' link 
next to each article to see all the news stories
on this topic. 
The problem of selecting  the best $k$ articles that summarize all current news articles is better modeled as a center-based clustering.
The  problem of partitioning all news  articles into clusters of similar articles is better modeled as a cluster-based clustering. 
Other applications can be
found in \cite{liberty2016algorithm,LattanziV17}, but we will use the
news story clustering application as our running canonical example.

A line of research~\cite{pmlr-v130-guo21a,FLN21,LattanziV17,liberty2016algorithm} within online clustering goes by moniker of consistent clustering.  Research on consistent clustering studies the trade-offs and relationship between the following objectives:
\begin{itemize}
    \item {\bf Maximizing the Quality of the Clustering:} One seeks
    the cost of the clustering to be small. 
    The most common metric to measure the quality of a
    solution is the ratio of the cost of this   solution to cost of the optimal solution.  
The most common metric to measure the quality of an online algorithm
is the competitive ratio, which is the maximum (over all inputs) of the ratio of the cost of
the online algorithm's solution to the optimal cost. 
    \item {\bf Maximizing Consistency:} Ideally one would like the centers in
    a center-based problem, or
   the clusters in a cluster-based problem, to be consistent over time.
 That is, they should change as little as possible. So for example,
the news provider doesn't want the clusters to completely change
every time a new news article is written.
\end{itemize}


\subsection{Prior Work on Consistent Clustering}


$k$-median clustering is 
NP-hard, but constant factor approximation polynomial-time algorithms
are known~\cite{CGST99,JV99,AGKMMP01,LS13,BPRST17}. 

All prior algorithmic research on consistent clustering that we are aware of~\cite{pmlr-v130-guo21a,FLN21,LattanziV17,liberty2016algorithm} is
center-based. That is, the  online algorithm  explicitly maintains a collection of
centers, and the clustering is implicit. That is, each point is assumed to be associated with the closest center, but there are no restrictions on how often points' associated centers can change.

 The first paper~\cite{liberty2016algorithm} in this line of research
 gave a lower bound that  
 showed that one can not simultaneously
 have both high quality and maximum consistency.
 That is, they showed that if a center cannot be changed once it is established, 
then there is no algorithm whose competitive ratio 
can be bounded by any function of $n$ and $k$.

Thus various ``beyond worst-case analysis'' (see~\cite{roughgarden_2021}) approaches have been used in the literature to 
attempt to circumvent
the obstacle presented by this lower bound. 
One approach is to use bi-criteria 
analysis or \emph{resource augmentation} analysis.  This analysis   allows the online algorithm to use more than $k$ centers,
and then compares the cost of the algorithm's clustering 
to the optimal one using $k$ 
centers~\cite{liberty2016algorithm}.
A second approach is to allow the algorithm \emph{recourse}, which 
in this setting means allowing the algorithm to  
change the centers (or clusters) a small number of times~\cite{LattanziV17,FLN21,pmlr-v130-guo21a}. 
Another approach is to consider learning augmented algorithms,
which assumes that the algorithm receives some advice/information a priori about the input.
For example, in the  news application, the news provider  presumably has prior data
that it could use to predict with some reasonable accuracy some properties of the input. 
 


\cite{liberty2016algorithm}  gives a randomized algorithm for $k$-means clustering  and
analyzes this algorithm using \emph{resource augmentation} analysis. 
 \cite{liberty2016algorithm} shows that 
the expected number of clusters/centers used by their algorithm is 
$O( k \log n \log n \Delta )$ and at all times 
 the expected cost of the clustering using these centers
 is at most $O(\log n)$ times
the optimal cost using $k$ 
clusters. Here $\Delta$ is the aspect ratio of the data points,
which is the ratio between the distance between the furthest pair
of points and the distance between the closest pair of points.
The algorithm 
leverages 
a randomized online algorithm for facility location from \cite{Meyerson2001} 
to decide whether to create a new center at a newly arriving data point. 
Once a center is established, it is maintained throughout the course of the algorithm. 
Finally, \cite{liberty2016algorithm} gives a randomized algorithm
that requires a priori knowledge of
$n$ and a lower bound of the optimal with $k$ centers, 
and that maintains a  collection of $O(k \log n \log \alpha)$ centers in expectation
that has expected cost 
$O(1)$ times
the optimal cost with $k$
centers. Here $\alpha$ is the ratio between actual optimal cost with $k$
centers and the lower bound provided a priori to the algorithm. 



\cite{LattanziV17} give  a randomized algorithm for $k$-means or $k$-median
clustering that uses \emph{recourse}.
The algorithm maintains the invariant that the cost of the current centers is
always $O(1)$-competitive with the optimal clustering of the data points seen \textit{to date}.
To maintain this invariant, the expected number of cluster center changes used by
the algorithm is 
 $O(k^2 \log^4 n \Delta)$. 
\cite{LattanziV17} show a similar lower bound, that is they show that every
algorithm requires 
 $\Omega(k \log_c  \frac{ \Delta}{k})$ center changes  to maintain $O(c)$-competitiveness.
Further, \cite{LattanziV17} show that it possible to maintain
$O(1)$-competitiveness with $O(k \log^2 n \Delta)$ center changes,
but this is just an existential result, and no algorithm to achieve
this was given. In followup paper \cite{FLN21} gave
a randomized algorithm that maintains $O(1)$-competitiveness with
$O(k \polylog( n \Delta)$ cluster changes. 
The results in \cite{LattanziV17} were extended to $k$-median
clustering with outliers (so one could opt to not
cluster a pre-specified number of points) in \cite{pmlr-v130-guo21a}.
\cite{LattanziV17}  also observes
that for $k$-center clustering an algorithm from \cite{CharikarCFM04} 
can be used to maintain
an $O(1)$-competitive clustering with $O(k \log  n \Delta)$ center changes. 

While not directly germane for the work in this paper, there is also research on online clustering in the streaming setting, where the emphasis
is more on the algorithm using a small amount of memory, or quickly responding to
the arrival of a new data point~(e.g. \cite{chanKDD,Cohen-AddadHPSS19}).


\subsection{Our Contribution}

Our initial research goal was to investigate consistent clustering for
cluster-based problems (recall that all the past algorithmic consistent
clustering publications that we are aware of focus on center-based clustering). 
We are interested in applications
where the focus is on explicitly maintaining consistent clusters (and not necessarily on maintaining consistent centers). 
The application where Google or Yahoo news is trying to maintain collections
of similar news articles is an example of such an application. 
Note that even the algorithms from \cite{liberty2016algorithm} that are
 perfectly consistent from a center perspective, in that once 
a center is established it persists until the end of the algorithm, 
are not necessarily consistent from a cluster perspective in that
a data point could  change clusters every time a  new center
is established. All one can say (at least naively) about 
the cluster consistency of the algorithms from \cite{liberty2016algorithm}
is that no data point changes clusters more than $O( k \log n \log n \Delta )$ times. 


Specifically, our research goal is to determine whether some reasonable
competitiveness with respect to cost can be achieved if each data point has to be irrevocably 
assigned a cluster, or equivalently a label, when it arrives. 

The lower bound from \cite{liberty2016algorithm} implies  that
we will need to take some beyond worst-case analysis approach as otherwise there is no algorithm with bounded approximation ratio.
For reasons we explain in Section \ref{sect:overview},
we will take a learning augmented algorithms approach, and
assume that the algorithm is provided a priori with 
an estimated upper bound $B$ of the cost of the final optimal clustering.
Our algorithms will then try maintain
a clustering of low cost relative to $B$, \textit{not} the current optimal cost for
the points that have arrived to date. Thus we will say an algorithm
is $c$-competitive with respect to the budget if the algorithms cost
is at most $c \cdot B$ on instances where the optimal cost is at most $B$ (after all points arrive).


We  develop a lower bound in Section \ref{appendix: lower_bd}  that the competitiveness of any algorithm relative to the budget
must be $\Omega(k)$. In light of this observation, a natural question is whether
one can find  an algorithm whose competitiveness with respect to the budget is
$O(f(k))$ for some function $f$, that depends only on the number of centers $k$, and not on the number of points $n$.  Such an algorithm is constant competitive when $k$ is fixed.  We answer this in the affirmative by
giving a (polynomial-time deterministic) online  algorithm that is $O(k^5 3^k)$-competitive with respect to the budget.  Thus, we know the competitive ratio must depend on $k$ by the lower bound and we give an upper-bound that only depends on $k$.
Note that in most clustering applications while the number of points $n$ to be 
clustered may be large, the number $k$ of clusters is typically a modestly
small constant, say 10 for example~\cite{bishop}. 
    



