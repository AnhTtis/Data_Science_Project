\section{Technical Overview}

\label{sect:overview}



To understand the motivation for learning-augmented approach, 
let us consider the lower bound instance from~\cite{liberty2016algorithm}.
It is sufficient to assume $k=2$. The first point $x_1$ arrives and is assigned some irrevocable label. Then assume the second data point $x_2$
arrives a unit distance from $x_1$. If the online algorithm assigns $x_2 $ the same label as  
$x_1$ then the cost of the algorithm's clustering is 1, and the optimal
cost is 0 (which is the cost if each of these data points were given a different labels). 
This results in the algorithm having in unbounded competitiveness. In contrast, if the algorithm gave
$x_2$ the a different label as $x_1$ then the third data point $x_3$ could arrive very far
away. In which case, the algorithm's clustering would necessarily have very high
cost (as $x_3$'s label would have to be either the same as $x_1$'s or the same as $x_2$'s). 
However, the optimal clustering would have cost 1 (by giving $x_1$ and $x_2$ the same label
and giving $x_3$ the remaining label).  Again, this results in competitiveness that can only be bounded by the aspect ratio of metric space (much larger than $n$ or $k$).


Intuitively, the dilemma faced by the online algorithm when $x_2$ arrives is
that it does not know whether the distance between $x_1$ and $x_2$ is small or large. 
Equipped with an estimate of the optimal cost $B$, the algorithm could 
 resolve this dilemma by giving $x_2$  a different label than $x_1$ if their distance is larger than $B$ and the same label otherwise. Thus we will assume
 that the algorithm is provided a priori 
 with a budge $B$ that is an upper bound on
 the optimal cost (and ideally is not too much larger than the optimal cost).

To better understand 
the design of our algorithm it is useful to understand some instances that illustrate some properties
that a competitive algorithm must have.


The first observation is that any reasonably competitive algorithm can 
never use $t+1$ labels if it is the case that the points to date could be
clustered with cost at most $B$ using at most $t$ labels.
If the algorithm ever allowed this to happen,  it could be that the
next $k-t$ data points could arrive very far from the previous data points,
and very far from each other. Thus after these data points arrive,
the algorithm's cost would
increase to something like the diameter of the metric space, while there
would still be a clustering of cost at most $B$, since the clustering that
used $t$ labels could be extended with no additional cost by giving each
new $k-t$ data points a new label. 


In light of this last observation, a natural greedy algorithm would maintain the
invariant that the number of labels it uses is always equal to the minimum
number of labels necessary for a clustering of cost at most $B$,
and then give each new data point the label that minimizes the increase in cost.
To see why such an algorithm (and other similar algorithms) can have unbounded cost even when $k=2$, 
and the metric space is the real line,
consider the following instance (see Figure \ref{fig: greedy_tricky_ex}). 
Let $\alpha$ be an arbitrarily large positive integer. We construct an example in which the budget $B=2$.  
The first point arrives  at location $-2$. Say the algorithm gives this point 
the label blue. The next point arrives at location $1$. Now, we know that any offline clustering with cost at most $2$ must use at least 2 clusters. So the greedy algorithm would give
this new point a second label, say green, as that would minimize the increase in the objective. Then a total of $\alpha$ additional points arrive at location $1$. The algorithm labels these points green. Then $\alpha$ points arrive at the origin 0. It is still the case that only 2 clusters are required in order  to have cost at most $2$, since we may place the points at location $-2$ and the origin in one cluster, and
the  points at location 1 in the other cluster.  However the algorithm would assign each point arriving at the origin the label green, since this increases the objective by at most $1$ while assigning such a point the label blue increases the objective by $2$.   Yet, this results in a solution for the algorithm in which the contribution of green points towards the objective is $\alpha $. 

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/greedy_tricky_ex.png}
\caption{An example in which the natural greedy algorithm incurs arbitrarily large cost.}
\label{fig: greedy_tricky_ex}
\end{figure}


Upon reflection of this lower bound instance for the natural greedy algorithm,
there appear to us to be two natural hypotheses as to the ``mistake'' that this algorithm is making,
and corresponding two natural paths towards a remedy:
\begin{itemize}
    \item One hypothesis is that greedy assignment is a mistake,
    and then the natural remedy would be to use some label assignment rule that is more sophisticated than greedy.
    \item
    Another hypothesis is that the algorithm was too hasty in using a new label.
    Thus the natural remedy  would then be to delay using a new label  until it is more clear as to
a region where arriving data points should be given this new label. 
Note in the example in Figure \ref{fig: greedy_tricky_ex} that if the algorithm had waited until some reasonable
number of data points had arrived at the origin before using the second label, then the algorithm might reasonably have been able to 
see that the right choice was to give the  remaining points arriving at the origin the second label of green.
\end{itemize}

Here we primarily adopt the second remedy/approach (although
we also consider an alternate greedy assignment policy). 
To apply this remedy we must address the following two questions:\begin{itemize}
    \item 
 Under what conditions can the algorithm
justify the use of an additional label, say going from $t-1$ labels to $t$ labels?
\item
And when this can be justified, how should we modify our prior partition of space into $t-1$ parts into a partition into $t$ parts?
\end{itemize}
At a high level our answer to the first question is that we do not use $t$ labels until there  exist $t$ well-separated points $x_{\alpha(1)}, \ldots, x_{\alpha(t)}$. 
We will say that a collection of points $x_{\alpha(1)}, \ldots, x_{\alpha(t)}$ from a collection $S$ of points is $\beta$-\textbf{well-separated} with respect to $w_S$ if
\begin{equation} \label{eq: well-sep-def}
\min\{w_S(x_{\alpha(i)}),w_S(x_{\alpha(j)})\}\cdot d(x_{\alpha(i)},x_{\alpha(j)}) \geq \beta \cdot B \hspace{0.3cm} \text{for all } i,j \in [t], i \neq j \tag{$\star$}
\end{equation}
Here $w_S(x_h)$ is what we call the \textbf{natural weight} of point $x_h$ in $S$, which
is the maximum number of points in $S$ whose distances to $x_h$ sum to at most $2B$. (If $S$ is clear, we may drop it from the notation).
Intuitively, if we have $t$ well-separated points then 
we then know that not only must any near optimal solution use $t$ labels, but such a solution cannot combine
the points near $x_{\alpha(i)}$ and the points near $x_{\alpha(j)}$ into a single cluster (assuming $i \ne j$). 


To drill down a bit further, we maintain a collection of points $p_1, \ldots, p_t$ from the online stream $X$ which we call \textit{pivots}.
When a new point arrives, it is assigned the label $i$ of the pivot $p_i$ nearest to it (so we still maintain a form of greedy label assignment). 
While one might reasonably think that the pivots are intuitively centers for the clusters, 
this intuition is only partially correct. In fact there are scenarios where some pivots are
in fact poor centers for the corresponding cluster. What is critical is that the pivots are located
so as to guarantee that using greedy assignment in the future results in a relatively low cost assignment. 
In order for our cost analysis to be tractable  we want the algorithm to maintain the following invariants:\begin{itemize}
    \item Each pivot $p_i$  is reasonably located in a region where it would not be too costly to assign points arriving in this region the label $i$.   
    \item The pivots are well-separated (for some appropriate choice of $\beta$).\footnote{$\beta$ will have to both be initialized sufficiently large and also decrease as the number of pivots increases. We show this is necessary in Appendix \ref{appendix: decreasing_well_sep}.}
    \item There is no other point that is well-separated from the pivots.
    \item The locations of the pivots should not move very often.
\end{itemize}
Note that some of these invariants can intuitively be in opposition to each other, which seemingly requires
that the design of the algorithm
is a bit complicated, as there are several different scenarios where
maintaining this invariant requires different actions. 
But we will now try to give a brief, definitely over-simplified, overview of how the algorithm maintains these invariants.
As the pivots are not necessarily good centers (for example pivot $p_1$ at location $-2$ as the points arrive at location 1 in Figure \ref{fig: greedy_tricky_ex}), the algorithm also maintains a collection
$c_1, \ldots, c_t$ of estimated centers for the $t$ labels that have been used to date. 
The pivots and the estimated centers are updated in two scenarios. The first scenario is when there is an applicable Add Operation, and the second
is when there is an applicable Exchange Operation. 
In both scenarios the estimated centers are recomputed from the previously computed estimated centers and the newly computed (near) optimal centers. 
Then a sequence of Add and Exchange Operations are executed. 

An Add Operation is applicable when there is a point $x_\alpha$ that is well-separated from
the current pivots. Intuitively, this means a new label can be justified, but the implementation requires the consideration of several possible scenarios. 
 In the simplest scenario $x_\alpha$ is near a cluster of new points that are all far from
previous points, and the pivot $p_{t+1}$ for the new label ($t+1$) is set to $x_\alpha$. In some scenarios  an old pivot $p_i$ ($i \le t$) is set to $x_\alpha$ and $p_{t+1}$ is set to $p_i$ (so the new pivot 
inherits an old label and an old pivot location gets the new label). Intuitively, this occurs when the estimated center $c_i$ for cluster $i$ is near the location of $x_{\alpha}$.
Also there are scenarios where an old pivot $p_i$ ($i \le t$) is set to the estimated center $c_i$ of cluster $i$ 
and $p_{t+1}$ is set to $p_i$ (so again the new pivot inherits an old label and an old pivot location gets the new label). 
Finally, there are scenarios where two new pivots are created.\footnote{This is needed to avoid label conflicts. See Appendix \ref{appendix: label_conflicts}.}



An Exchange Operation is applicable when there are two  points $x_\alpha$ and $x_\gamma$ 
 near a pivot $p_j$ that are well-separated from each other and 
the other pivots (besides $p_j$). So intuitively the cluster of points labeled $j$ appear to be splitting
into two clusters. In the simplest scenario the location of pivot $p_j$ is set to the location of one of
$x_\alpha$ or $x_\gamma$, and the location of the new pivot $p_{t+1}$ is set to the location of the other one. 
This scenario occurs in the instance depicted in Figure \ref{fig: greedy_tricky_ex}.
The first pivot $p_1$ is initially set to location $-2$. The points arriving at
location $1$ would all be assigned the label 1 (blue) as there is no  point well-separated
from $p_1$ (the points located at $1$ are not separated from $p_1$ because the  points at
$p_1$ can be cheaply moved to location $1$). When enough points have arrived at the origin,
then the points $x_\alpha = 0$  and  at $x_\gamma = 1$ are near $p_1$ (because the point at 
$p_1$ can be cheaply moved to either $x_\alpha$ or $x_\gamma$), and are well-separated from each other
and the pivots other than $p_1$. Thus our algorithm would locate $p_1$ at $1$ and $p_2$ at the origin.
While this scenario gives some intuition,
there are several other more complicated scenarios.




The analysis is broken into two parts. The first part is to show that certain stuctural invariants are maintained through carefully engineered algorithmic choices.  The well-separated invariant is used to show we do not use more than $k$ labels and for bounding the cost. The second part of the analysis is to bound the cost (Section \ref{sec:cost}). We inductively assume that the cost of points that were labelled using the first $t-1$ labels is bounded. Then, we show that the points that were labelled when exactly $t$ labels were in use are clustered near optimally under our greedy procedure. The key challenge is showing the points given the same label combined have bounded cost. 



