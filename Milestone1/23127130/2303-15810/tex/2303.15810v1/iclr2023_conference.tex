
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage[colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue,]{hyperref}
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}%
\usepackage{algpseudocode}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{dsfont}
\usepackage{lipsum}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{enumerate}
\newcommand{\ljx}[1]{\textcolor{blue}{\small{\bf [ljx: #1]}}}
\newcommand{\jl}[1]{\textcolor{blue}{\small{\bf [jl: #1]}}}
\newcommand{\hoyin}[1]{\textcolor{purple}{\small{\bf [haoran: #1]}}}

\usepackage[flushleft]{threeparttable}
\usepackage{vcell}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{xcolor}
% \usepackage[table,xcdraw,dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage[font=small]{caption}
% \usepackage{tcolorbox}
\usepackage{booktabs}
\usepackage{colortbl}

\usepackage{listings}

\usepackage{tcolorbox}
\definecolor{mine}{RGB}{205, 232, 248}%
\definecolor{my_g}{RGB}{128, 128, 128}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\usepackage{hyperref}
\usepackage{cleveref}
% \Crefname{equation}{Eqn.}{Eqns.}
\Crefname{figure}{Fig.}{Figs.}
\Crefname{tabular}{Tab.}{Tabs.}

% \usepackage{subcaption}
\usepackage{graphicx}

% \title{Implicit Value Regularization is the Key to SOTA Offline RL Methods}
% \title{Sparse $Q$-Learning: Offline Reinforcement Learning with Implicit Value Regularization}
% \title{Regularization with No OOD Actions: In-Sample Offline RL via Implicit Value Regularization}
\title{Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Haoran Xu$^\spadesuit$\footnotemark[1] \quad Li Jiang$^\spadesuit$ \quad Jianxiong Li$^\spadesuit$ \quad Zhuoran Yang$^\clubsuit$ \quad Zhaoran Wang$^\diamondsuit$ \\ \textbf{Victor Wai Kin Chan}$^\spadesuit$ \quad \textbf{Xianyuan Zhan}$^\spadesuit$$^\heartsuit$ \\ 
% $^\spadesuit$Tsinghua University \ 
% $^\clubsuit$Yale University \ 
% $^\diamondsuit$Northwestern University \ 
% $^\heartsuit$Shanghai AI Laboratory\\
% \texttt{\{ryanxhr,jiangli3859,zhanxianyuan\}@gmail.com}
% }

\author{Haoran Xu$^{1}$\footnotemark[1] \quad Li Jiang$^{2}$ \quad Jianxiong Li$^{1}$ \quad Zhuoran Yang$^{3}$ \quad Zhaoran Wang$^{4}$ \\ \textbf{Victor Wai Kin Chan}$^{2}$ \quad \textbf{Xianyuan Zhan}$^{1}$$^{5}$ \\ 
$^{1}$Institute for AI Industry Research (AIR), Tsinghua University \\ 
$^{2}$Tsinghua-Berkeley Shenzhen Institute (TBSI), Tsinghua University \\ 
$^{3}$Yale University \
$^{4}$Northwestern University \ 
$^{5}$Shanghai Artificial Intelligence Laboratory\\
\texttt{\{ryanxhr,jiangli3859,zhanxianyuan\}@gmail.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Work done while at JD Technology. Correspondence to Haoran Xu, Xianyuan Zhan.}

\begin{abstract}
% The recent proposed \textit{In-sample Learning} paradigm, which improvement step implicitly by quentile regression, shows great promise to offline reinforcement learning (RL) because it learn an optimal policy without querying the value function of any unseen actions.
% While straight forward, it remains unclear about why so as to avoid errors due to distributional shift
% In this work, we find that some form of Implicit Value Regularization (IVR) will result in the In-sample Learning paradigm in offline RL. This gives a theoretical explantion of why it will work.
Most offline reinforcement learning (RL) methods suffer from the trade-off between improving the policy to surpass the behavior policy and constraining the policy to limit the deviation from the behavior policy as computing $Q$-values using out-of-distribution (OOD) actions will suffer from errors due to distributional shift. 
The recent proposed \textit{In-sample Learning} paradigm (i.e., IQL), which improves the policy by quantile regression using only data samples, shows great promise because it learns an optimal policy without querying the value function of any unseen actions. However, it remains unclear how this type of method handles the distributional shift in learning the value function.
In this work, we make a key finding that the in-sample learning paradigm arises under the \textit{Implicit Value Regularization} (IVR) framework. This gives a deeper understanding of why the in-sample learning paradigm works, i.e., it applies implicit value regularization to the policy. Based on the IVR framework, we further propose two practical algorithms, Sparse $Q$-learning (SQL) and Exponential $Q$-learning (EQL), which adopt the same value regularization used in existing works, but in a complete in-sample manner. 
Compared with IQL, we find that our algorithms introduce sparsity in learning the value function, making them more robust in noisy data regimes. We also verify the effectiveness of SQL and EQL on D4RL benchmark datasets and show the benefits of in-sample learning by comparing them with CQL in small data regimes. 
% Under all settings, SQL and EQL achieve better results and owns faster convergence compared to other baselines. 
% Compared with IQL, we find that our algorithm introduces sparsity in learning the value function, we thus dub our method Sparse $Q$-learning (SQL). We verify the effectiveness of SQL on D4RL benchmark datasets. We also show the benefits of sparsity by comparing SQL with IQL in noisy data regimes and show the robustness of in-sample learning by comparing SQL with CQL in small data regimes. Under all settings, SQL achieves better results and owns faster convergence compared to other baselines. 
Code is available at \url{https://github.com/ryanxhr/IVR}.
\end{abstract}


\section{Introduction}
\label{intro}
Reinforcement learning (RL) is an increasingly important technology for developing highly capable AI systems, it has achieved great success in game-playing domains \citep{mnih2013playing,silver2017mastering}. However, the fundamental online learning paradigm in RL is also one of the biggest obstacles to RL's widespread adoption, as interacting with the environment can be costly and dangerous in real-world settings. 
Offline RL, also known as batch RL, aims at solving the abovementioned problem by learning effective policies solely from offline data, without any additional online interactions.
It is a promising area for bringing RL into real-world domains, such as robotics \citep{kalashnikov2021mt}, healthcare \citep{tang2021model} and industrial control \citep{zhan2022deepthermal}. In such scenarios, arbitrary exploration with untrained policies is costly or dangerous, but sufficient prior data is available. 

While most off-policy RL algorithms are applicable in the offline setting by filling the replay buffer with offline data, improving the policy beyond the level of the behavior policy entails querying the $Q$-function about values of actions produced by the policy, which are often not seen in the dataset. Those out-of-distribution actions can be deemed as adversarial examples of the $Q$-function, which cause extrapolation errors of the $Q$-function \citep{kumar2020conservative}. 
To alleviate this issue, prior model-free offline RL methods typically add pessimism to the learning objective, in order to be pessimistic about the distributional shift. Pessimism can be achieved by policy constraint, which constrains the policy to be close to the behavior policy \citep{kumar2019stabilizing,wu2019behavior,nair2020accelerating,fujimoto2021minimalist}; or value regularization, which directly modifies the $Q$-function to be pessimistic \citep{kumar2020conservative,kostrikov2021offline,an2021uncertainty,bai2021pessimistic}. Nevertheless, this imposes a trade-off between accurate value estimation (more regularization) and maximum policy performance (less regularization).


In this work, we find that we could alleviate the trade-off in \textit{out-of-sample learning} by performing \textit{implicit value regularization}, this bypasses querying the value function of any unseen actions, allows learning an optimal policy using \textit{in-sample learning}\footnote{The core difference between in-sample learning and out-of-sample learning is that in-sample learning uses only dataset actions to learn the value function while out-of-sample learning uses actions produced by the policy.}.
More specifically, we propose the Implicit Value Regulazization (IVR) framework, in which a general form of behavior regularizers is added to the policy learning objective.
% (maximizing the expected discounted cumulative reward). 
Because of the regularization, the optimal policy in the IVR framework has a closed-form solution, which can be expressed by imposing weight on the behavior policy. The weight can be computed by a state-value function and an action-value function, the state-value function serves as a normalization term to make the optimal policy integrate to 1. It is usually intractable to find a closed form of the state-value function, however, we make a subtle mathematical transformation and show its equivalence to solving a convex optimization problem.
In this manner, both of these two value functions can be learned by only dataset samples.

Note that the recently proposed method, IQL \citep{kostrikov2021iql}, although derived from a different view (i.e., approximate an upper expectile of dataset actions given a state), remains pretty close to the learning paradigm of our framework. Furthermore, our IVR framework explains why learning the state-value function is important in IQL and gives a deeper understanding of how IQL handles the distributional shift: it is doing implicit value regularization, with the hyperparameter $\tau$ to control the strength. This explains one disturbing issue of IQL, i.e., the role of $\tau$ does not have a perfect match between theory and practice. In theory, $\tau$ should be close to 1 to obtain an optimal policy while in practice a larger $\tau$ may give a worse result.

Based on the IVR framework, we further propose some practical algorithms. We find that the value regularization terms used in CQL \citep{kumar2020conservative} and AWR \citep{peng2019advantage} are two valid choices in our framework. However, when applying them to our framework, we get two complete in-sample learning algorithms.
The resulting algorithms also bear similarities to IQL. However, we find that our algorithm introduces sparsity in learning the state-value function, which is missing in IQL. The sparsity term filters out those bad actions whose $Q$-values are below a threshold, which brings benefits when the quality of offline datasets is inferior.
% we thus dub our method Sparse $Q$-learning (SQL).
We verify the effectiveness of SQL on widely-used D4RL benchmark datasets and demonstrate the state-of-the-art performance, especially on suboptimal datasets in which value learning is necessary (e.g., AntMaze and Kitchen). 
We also show the benefits of sparsity in our algorithms by comparing with IQL in noisy data regimes and the robustness of in-sample learning by comparing with CQL in small data regimes. 
% Under all settings, our algorithms achieve better results and own faster convergence compared with other baselines. 

% Highlight contributions.
To summarize, the contributions of this paper are as follows:
\begin{itemize}[leftmargin=*]
\item We propose a general implicit value regularization framework, where different behavior regularizers can be included, all leading to a complete in-sample learning paradigm.
\item Based on the proposed framework, we design two effective offline RL algorithms: Sparse $Q$-Learning (SQL) and Exponential $Q$-learning (EQL), which obtain SOTA results on benchmark datasets and show robustness in both noisy and small data regimes.
\end{itemize}



\section{Related Work}
\label{related}
% Model-free offline RL: behavior regularization, value regularization, in-sample learning.
To tackle the distributional shift problem, most model-free offline RL methods augment existing off-policy methods (e.g., $Q$-learning or actor-critic) with a behavior regularization term. 
% The primary ingredient of this class of methods is to propose various regularizers to ensure that the learned policy does not stray too far from the behavior policy, i.e., stays in distribution.
Behavior regularization can appear explicitly as divergence penalties \citep{wu2019behavior,kumar2019stabilizing,fujimoto2021minimalist}, implicitly through weighted behavior cloning \citep{wang2020critic,nair2020accelerating}, or more directly through careful parameterization of the policy \citep{fujimoto2018addressing,zhou2020latent}. 
Another way to apply behavior regularization is via modification of the critic learning objective to incorporate some form of regularization, to encourage staying near the behavioral distribution and being pessimistic about unknown state-action pairs \citep{nachum2019algaedice,kumar2020conservative,kostrikov2021offline,xu2022constraints}. 
There are also several works incorporating behavior regularization through the use of uncertainty \citep{wu2021uncertainty,an2021uncertainty,bai2021pessimistic} or distance function \citep{li2022data}.

However, in-distribution constraints used in these works might not be sufficient to avoid value function extrapolation errors.
Another line of methods, on the contrary, avoid value function extrapolation by performing some kind of imitation learning on the dataset.
When the dataset is good enough or contains high-performing trajectories, we can simply clone or filter dataset actions to extract useful transitions \citep{xu2022discriminator,chen2020bail}, or directly filter individual transitions based on how advantageous they could be under the behavior policy and then clones them \cite{brandfonbrener2021offline,xu2021offline,xu2022policy}. While alleviating extrapolation errors, these methods only perform single-step dynamic programming and lose the ability to "stitch" suboptimal trajectories by multi-step dynamic programming.

Our method can be viewed as a combination of these two methods while sharing the best of both worlds: SQL and EQL implicitly control the distributional shift and learns an optimal policy by in-sample generalization.
% with no out-of-sample value extrapolation. 
SQL and EQL are less vulnerable to erroneous value estimation as in-sample actions induce less distributional shift than out-of-sample actions.
Similar to our work, IQL approximates the optimum in-support policy by fitting the upper expectile of the behavior policy's action-value function, however, it is not motivated by remaining pessimistic to the distributional shift.

Our method adds a behavior regularization term to the RL learning objective. In online RL, there are also some works incorporating an entropy-regularized term into the learning objective \citep{haarnoja2018soft,nachum2017bridging,lee2019tsallis,neu2017unified,geist2019theory,ahmed2019understanding}, this brings multi-modality to the policy and is beneficial for the exploration.
Note that the entropy-regularized term only involves the policy, it could be directly computed, resulting in a similar learning procedure as in SAC \citep{haarnoja2018soft}. 
% In online RL, one also add a entropy-regularized term to the maximizing reward objective, however, this is for multi-modal for exploration and obtain a smoother of the max Bellman operator. Solving the problem is now easier than max operator.
While our method considers the offline setting and provides a different learning procedure to solve the problem by jointly learning a state-value function and an action-value function.

\section{Preliminaries}
\label{pre}
We consider the RL problem presented as a Markov Decision Process (MDP) \citep{sutton1998introduction}, which is specified by a tuple $\mathcal{M}=\langle \mathcal{S}, \mathcal{A}, T, r, \rho, \gamma \rangle$ consisting of a state space, an action space, a transition probability function, a reward function, an initial state distribution, and the discount factor. 
The goal of RL is to find a policy $\pi(a|s): \mathcal{S} \times \mathcal{A} \rightarrow[0,1]$ that maximizes the expected discounted cumulative reward (or called return) along a trajectory as
% A policy $\pi$ interacts with the environment by starting at an initial state $s_{0} \sim \mu_{0}$, and iteratively producing a sequence of distributions $\pi(\cdot | s_{t})$ over $A$, at steps $t=0,1, \ldots$, from which actions $a_{t}$ are sampled and successively applied to the environment. The environment produces a scalar reward $r_{t}=r(s_{t}, a_{t})$ and a next state $s_{t+1} \sim T(s_{t}, a_{t})$.
% We use $d^{\pi}(s)$ to denote the distribution of states that a policy $\pi(a|s)$ generates.
% In RL, one wishes to learn a max-return policy, i.e., one wishes to optimize
\begin{align}
\label{eq_rl}
\max_{\pi} \ \mathbb{E} \left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}, a_{t}\right) \bigg| s_{0}=s, a_{0}=a, s_{t} \sim T\left(\cdot | s_{t-1}, a_{t-1}\right), a_{t} \sim \pi\left(\cdot|s_{t}\right) \text { for } t \geq 1\right].
\end{align}
% where $V^{\pi}(s)$ describes the future rewards accumulated by $\pi$ from any state $s$,
% \begin{equation}
% V^{\pi}(s)=\mathbb{E} \left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}, a_{t}\right) \bigg| s_{0}=s, a_{t} \sim \pi\left(\cdot|s_{t}\right), s_{t+1} \sim T\left(\cdot | s_{t}, a_{t}\right)  \right],
% \end{equation}

In this work, we focus on the offline setting. Unlike online RL methods, offline RL aims to learn an optimal policy from a fixed dataset $\mathcal{D}$ consisting of trajectories that are collected by different policies. The dataset can be heterogenous and suboptimal, we denote the underlying behavior policy of $\mathcal{D}$ as $\mu$, which represents the conditional distribution $p(a|s)$ observed in the dataset.

RL methods based on approximate dynamic programming (both online and offline) typically maintain an action-value function ($Q$-function) and, optionally, a state-value function ($V$-function), refered as $Q(s, a)$ and $V(s)$ respectively \citep{haarnoja2017reinforcement,nachum2017bridging,kumar2020conservative,kostrikov2021iql}.
These two value functions are learned by encouraging them to satisfy single-step Bellman consistencies.
Define a collection of policy evaluation operator (of different policy $\mathbf{x}$) on $Q$ and $V$ as
\begin{equation*}
(\mathcal{T}^{\mathbf{x}} Q)(s, a) := r(s, a) + \gamma \mathbb{E}_{s'|s,a}\mathbb{E}_{a'\sim \mathbf{x}}\left[Q(s',a')\right]
\end{equation*}
\begin{equation*}
(\mathcal{T}^{\mathbf{x}} V)(s) := \mathbb{E}_{a\sim \pi} \left[ r(s, a) + \gamma \mathbb{E}_{s'|s,a} \left[ V(s') \right] \right],
\end{equation*}
then $Q$ and $V$ are learned by $\min_{Q} J(Q)=\frac{1}{2} \mathbb{E}_{(s, a) \sim \mathcal{D}}\left[(\mathcal{T}^{\mathbf{x}} Q - Q)(s, a)^2 \right]$ and $\min_{V} J(V)=\frac{1}{2} \mathbb{E}_{s \sim \mathcal{D}}\left[(\mathcal{T}^{\mathbf{x}} V - V)(s)^2\right]$, respectively.
Note that $\mathbf{x}$ could be the learned policy $\pi$ or the behavior policy $\mu$, if $\mathbf{x}=\mu$, then $a \sim \mathbf{\mu}$ and $a' \sim \mathbf{\mu}$ are equal to $a \sim \mathcal{D}$ and $a' \sim \mathcal{D}$, respectively. 
In offline RL, since $\mathcal{D}$ typically does not contain all possible transitions $\left(s, a, s^{\prime}\right)$, one actually uses an empirical policy evaluation operator that only backs up a single $s'$ sample, we denote this operator as $\hat{\mathcal{T}}^{\mathbf{x}}$.


% Given a dataset $\mathcal{D}=\left\{\left(s, a, r, s'\right)\right\}$ of tuples, $\hat{\mathcal{T}}^{\mathbf{x}}$ is defined as:
% \begin{equation}
% (\hat{\mathcal{T}}^{\mathbf{x}} Q)(s, a) = r(s, a) + \gamma \mathbb{E}_{a'\sim \mathbf{x}}\left[Q(s',a')\right]
% \end{equation}
% \begin{equation}
% (\hat{\mathcal{T}}^{\mathbf{x}} V)(s) = \mathbb{E}_{a\sim \mathbf{x}} \left[ r(s, a) + \gamma V(s') \right]
% \end{equation}



% Hard policy regularization:
% \begin{equation}
% \pi(s):=\underset{a_{i}+\xi_{\phi}\left(s, a_{i}, \Phi\right)}{\arg \max } Q\left(s, a_{i}+\xi \left(s, a_{i}, \Phi\right)\right), \quad
% \left\{a_{i} \sim \mu(\cdot | s)\right\}_{i=1}^{n}
% \end{equation}

% Soft policy regularization:
% \begin{equation}
% \max _{\pi} \underset{s \sim \mathcal{D}}{\mathbb{E}}\left[\underset{a \sim \pi(\cdot | s)}{\mathbb{E}}\left[Q(s, a)\right]-\beta \cdot D(\mu(\cdot | s) , \pi(\cdot | s))\right]
% \end{equation}



\textbf{In-sample Learning via Expectile Regression} \quad 
Instead of adding explicit regularization to the policy evaluation operator to avoid out-of-distribution actions, IQL uses only in-sample actions to learn the optimal $Q$-function. IQL uses an asymmetric $\ell_{2}$ loss (i.e., expectile regression) to learn the $V$-function, which can be seen as an estimate of the maximum $Q$-value over actions that are in the dataset support, thus allowing implicit $Q$-learning:
\begin{equation}
\label{v_iql}
\min_{V} \ \mathbb{E}_{(s, a) \sim \mathcal{D}} \big[ \big|\tau -\mathds{1} \big(Q(s, a) - V(s) < 0 \big) \big|  \big(Q(s, a) - V(s) \big)^2 \big]
\end{equation}
\begin{equation*}
\label{q_iql}
\min_{Q} \ \mathbb{E}_{(s, a, s') \sim \mathcal{D}} \big[ \big(r(s, a)+\gamma V(s')-Q(s, a)\big)^2 \big],
\end{equation*}
where $\mathds{1}$ is the indicator function. After learning $Q$ and $V$, IQL extracts the policy by advantage-weighted regression \citep{peters2010relative,peng2019advantage,nair2020accelerating}:
\begin{equation}
\label{pi_iql}
\min_{\pi} \ \mathbb{E}_{(s, a) \sim \mathcal{D}}\big[\exp\left(\beta\left(Q(s, a)-V(s)\right)\right) \log \pi(a|s)\big].
\end{equation}

While IQL achieves superior D4RL benchmark results, several issues remain unsolved:
\begin{itemize}[leftmargin=*]
\item The hyperparameter $\tau$ has a gap between theory and practice: in theory $\tau$ should be close to 1 to obtain an optimal policy while in practice a larger $\tau$ may give a worse result.
% \item Why IQL allows multi-step dynamic programming?
\item In IQL the value function is estimating the optimal policy instead of the behavior policy, how does IQL handle the distributional shift issue?
\item Why should the policy be extracted by advantage-weighted regression, does this technique guarantee the same optimal policy as the one implied in the learned optimal $Q$-function?
\end{itemize}



\section{Offline RL with Implicit Value Regularization}
\label{method}
In this section, we introduce a framework where a general form of value regularization can be implicitly applied. 
We begin with a special MDP where a behavior regularizer is added to the reward, we conduct a full mathematical analysis of this regularized MDP and give the solution of it under certain assumptions, which results in a complete in-sample learning paradigm.
We then instantiate a practical algorithm from this framework and give a thorough analysis and discussion of it.

\subsection{Behavior-regularized MDPs}
Like entropy-regularized RL adds an entropy regularizer to the reward \citep{haarnoja2018soft}, in this paper we consider imposing a general behavior regularization term to objective (\ref{eq_rl}) and solve the following \textit{behavior-regularized} MDP problem
\begin{equation}
\label{eq_reg_rl}
\max_\pi \ \mathbb{E}\bigg[\sum_{t=0}^{\infty} \gamma^t\Big(r(s_t, a_t) - \alpha \cdot f\Big( \frac{\pi(a_t | s_t)}{\mu(a_t | s_t)} \Big)\Big) \bigg],
\end{equation}
where $f(\cdot)$ is a regularization function.
It is known that in entropy-regularized RL the regularization gives smoothness of the Bellman operator \citep{ahmed2019understanding,chow2018path}, e.g., from greedy max to softmax over the whole action space when the regularization is Shannon entropy. While in our new learning objective (\ref{eq_reg_rl}), we find that the smoothness will transfer the greedy max from policy $\pi$ to a softened max (depending on $f$) over behavior policy $\mu$, this enables an in-sample learning scheme, which is appealing in the offline RL setting.


% Standard RL maximizes the expected sum of rewards $\sum_{t} \mathbb{E}_{(s_{t}, a_{t}) \sim d^{\pi}}\left[r(s_{t}, a_{t})\right]$. We will consider a more general maximum entropy objective (see e.g. Ziebart (2010)), which favors stochastic policies by augmenting the objective with the expected entropy of the policy over $d^{\pi}(s_{t})$ :
% \begin{align*}
% J(\pi) &=\sum_{t=0}^{T} \mathbb{E}_{s_{t} \sim d^{\pi}}\mathbb{E}_{a_{t} \sim \pi} \left[r(s_{t}, a_{t})  - \alpha \cdot f \left(\frac{\pi(a_{t} | s_{t})}{\mu(a_{t} | s_{t})}\right) \right] 
% \end{align*}
% where $f$ is any twice differentiable convex function with $f(1) = 0$. \hoyin{Is other distance $D$ valid? (e.g., optimal transport distance)}

In the behavior-regularized MDP, we have a modified policy evaluation operator $\mathcal{T}_{f}^{\pi}$ given by
\begin{equation*}
(\mathcal{T}_{f}^{\pi}) Q(s, a) := r(s, a) + \gamma \mathbb{E}_{s'|s,a} \left[V(s') \right]
\end{equation*}
where
\begin{equation*}
V(s) = \mathbb{E}_{a\sim \pi} \bigg[ Q(s, a) - \alpha f \Big( \frac{\pi(a|s)}{\mu(a|s)} \Big) \bigg].
\end{equation*}

The policy learning objective can also be expressed as $\max_{\pi} \mathbb{E}_{s \sim \mathcal{D}} \left[ V(s) \right]$.
Compared with the origin policy evaluation operator $\mathcal{T}^{\pi}$, $\mathcal{T}_{f}^{\pi}$ is actually applying a value regularization to the $Q$-function.
% rather than adding a policy constraint in the policy improvement.
% A disadvantage of \textit{behavior regularization} is that it does little to regularize the critic itself, and thus, it is common for the critic to take on wildly extrapolated values on actions unseen in the training data, which may dominate any behavior regularization applied to the actor.
% Although policy regularization methods demonstrate impressive performance on some tasks, they share a common problem. The Q-function learned via standard TD-error minimization on $\mathcal{D}$ receives no learning signal for actions not observed in the replay buffer, while this same Q-function is nevertheless queried on out-of-distribution actions during the policy update - i.e., when $Q_{\theta}(s, a)$ is evaluated on $a \sim \pi(\cdot | s)$ in eq. (8) - and for bootstrapping critic targets in the TD-loss. Thus, issues with critic extrapolation can still dominate divergence regularizers applied to the policy.
However, the regularization term is hard to compute because the behavior policy $\mu$ is unknown. Although we can use Fenchel-duality \citep{boyd2004convex} to get a sampled-based estimation if $f$ belongs to the $f$-divergence \citep{wu2019behavior}, this unnecessarily brings a min-max optimization problem, which is hard to solve and results in a poor performance in practice \citep{nachum2019algaedice}. 

% There are two ways to accomplish equation (15) implicitly, both without 1) directly calculating $f$, 2) invloving min-max optimization problem (dual form of $f$-divergence), and 3) knowing $\mu$.


\subsection{Assumptions and Solutions}
We now show that we can get the optimal value function $Q^{*}$ and $V^{*}$ without knowing $\mu$. 
First, in order to make the learning problems (\ref{eq_reg_rl}) analyzable, two basic assumptions are required as follows:
\begin{assumption}
Assume $\pi(a|s) > 0 \Rightarrow \mu(a|s)>0$ so that $\pi/\mu$ is well-defined.
\label{assumption: 1}
\end{assumption}
\begin{assumption}
Assume the function $f(x)$ satisfies the following conditions on $(0,\infty)$ : (1) $f(1)=0$; (2) $h_{f}(x) = x f(x)$ is strictly convex; (3) $f(x)$ is differentiable.
% (3) $f(x)$ is monotonic and differentiable.
\label{assumption: 2}
\end{assumption}

The assumptions of $f(1)=0$ and $x f(x)$ strictly convex make the regularization term be positive due to the Jensenâ€™s inequality as $\mathbb{E}_{\mu} \big[\frac{\pi}{\mu}f\big(\frac{\pi}{\mu}\big)\big] \geq 1 f(1) = 0$. This guarantees that the regularization term is minimized only when $\pi=\mu$.
Because $h_{f}(x)$ is strictly convex, its derivative, $h_f^{\prime}(x)=f(x)+x f^{\prime}(x)$ is a strictly increasing function and thus $(h_f^{\prime})^{-1}(x)$ exists. For simplicity, we denote $g_f(x)=(h_f^{\prime})^{-1}(x)$.
The assumption of differentiability facilitates theoretic analysis and benefits practical implementation due to the widely used automatic derivation in deep learning.

% There are plenty of options for the regularizer $\phi(\cdot)$ that satisfy Assumption 1 . First, entropy can be recovered by $H_\phi(\pi)$ with specific $\phi(\cdot)$. For example, when $\phi(x)=-\log x$, the Shannon entropy is recovered; when $\phi(x)=\frac{k}{q-1}\left(1-x^{q-1}\right)$ with $k>0$, the Tsallis entropy is recovered. Second, there are many instances that are not viewed as an entropy but can serve as a regularizer. We find two families of such functions, namely, the exponential function family $q-x^k q^x$ with $k \geq 0, q \geq 1$ and the trigonometric function family $\cos (\theta x)-\cos (\theta)$ and $\sin (\theta)-\sin (\theta x)$ both with hyper-parameter $\theta \in\left(0, \frac{\pi}{2}\right]$. Since these functions are simple, we term them basic functions.

Under these two assumptions, we can get the following two theorems:
\begin{theorem}
In the behavior-regularized MDP, any optimal policy $\pi^*$ and its optimal value function $Q^*$ and $V^*$ satisfy the following optimality condition for all states and actions:
\begin{equation*}
\label{q_ivr}
Q^*(s, a) =r(s, a)+\gamma \mathbb{E}_{s^{\prime} | s, a} \left[ V^*\left(s^{\prime}\right) \right] 
\end{equation*}
\begin{equation}
\label{pi_ivr}
\pi^*(a | s) = \mu(a|s) \cdot \max \bigg\{g_f \Big(\frac{Q^*(s, a)-U^*(s)}{\alpha}\Big), 0\bigg\}
\end{equation}
\begin{equation}
\label{v_ivr}
V^*(s) =U^*(s)+\alpha \mathbb{E}_{a \sim \mu} \bigg[ \Big( \frac{\pi^*(a | s)}{\mu(a | s)}\Big)^2 f^{\prime}\Big(\frac{\pi^*(a | s)}{\mu(a | s)}\Big) \bigg]
\end{equation}
where $U^*(s)$ is a normalization term so that $\sum_{a \in \mathcal{A}} \pi^*(a | s)=1$.
\label{theorem: 2}
\end{theorem}

The proof is provided in Appendix \ref{proof: 2}. The proof depends on the KKT condition where the derivative of a Lagrangian objective function with respect to policy $\pi(a | s)$ becomes zero at the optimal solution. 
Note that the resulting formulation of $Q^{*}$ and $V^{*}$ only involves $U^{*}$ and action samples from $\mu$. $U^*(s)$ can be uniquely solved from the equation obtained by plugging Eq.(\ref{pi_ivr}) into $\sum_{a \in \mathcal{A}} \pi^*(a | s)=1$, which also only uses actions sampled from $\mu$. 
In other words, now the learning of $Q^{*}$ and $V^{*}$ can be realized in an in-sample manner.
% Also, the optimal policy $\pi^{*}$ can be extracted using only dataset samples by 


Theorem \ref{theorem: 2} also shows how the behavior regularization influences the optimality condition.
If we choose $f$ such that there exists some $x$ that $g_f(x)<0$, then it can be shown from Eq.(\ref{pi_ivr}) that the optimal policy $\pi^*$ will be sparse by assigning zero probability to the actions whose $Q$-values $Q^*(s, a)$ are below the threshold $U^*(s)+\alpha h_f^{\prime}(0)$ and assigns positive probability to near optimal actions in proportion to their $Q$-values (since $g_f(x)$ is increasing).
% The threshold involves $U^*(s)$, which can be uniquely solved from the equation obtained by plugging Eq.(\ref{pi_ivr}) into $\sum_{a \in \mathcal{A}} \pi^*(a | s)=1$. 
Note that $\pi^*$ could also have no sparsity, for example, if we choose $f=\log(x)$, then $g_f=\exp(x-1)$ will give all elements non-zero values.

\begin{theorem}
Define $\mathcal{T}_{f}^{*}$ the case where $\pi$ in $\mathcal{T}_{f}^{\pi}$ is the optimal policy $\pi^{*}$, then $\mathcal{T}_{f}^{*}$ is a $\gamma$-contraction.
\label{theorem: 1}
\end{theorem}

The proof is provided in Appendix \ref{proof: 1}. This theorem means that by applying $Q^{k+1} =\mathcal{T}_{f}^{*} Q^{k}$ repeatedly, then sequence $Q^{k}$ will converge to the $Q$-value of the optimal policy $\pi^{*}$ when $k \rightarrow \infty$.
% As a consequence, only one Q-value satisfies the soft Bellman equation, and thus the optimal policy presented in Theorem 1 is unique.

After giving the closed-form solution of the optimal value function. We now aim to instantiate a practical algorithm.
In offline RL, in order to completely avoid out-of-distribution actions, we want a \textit{zero-forcing} support constraint, i.e., $\mu(a|s) = 0 \Rightarrow \pi(a|s) = 0$. 
This reminds us of the class of $\alpha$-divergence \citep{boyd2004convex}, which is a subset of $f$-divergence and takes the following form ($\alpha \in \mathbb{R} \backslash\{0, 1\}$):
\begin{equation*}
D_{\alpha}(\mu, \pi)=\frac{1}{\alpha(\alpha-1)} \mathbb{E}_{\pi}\bigg[\Big(\frac{\pi}{\mu}\Big)^{-\alpha}-1\bigg].
\end{equation*}

$\alpha$-divergence is known to be mode-seeking if one chooses $\alpha \leq 0$. 
Note that the Reverse KL divergence is the limit of $D_{\alpha}(\mu, \pi)$ when $\alpha \rightarrow 0$. We can also obtain Helinger distance and Neyman $\chi^{2}$-divergence as $\alpha=1 / 2$ and $\alpha=-1$, respectively.
One interesting property of $\alpha$-divergence is that $D_{\alpha}(\mu, \pi) = D_{1-\alpha}(\pi, \mu)$.

% Note that CQL \citep{kumar2020conservative} applies the following conservative policy evaluation operator to learn the $Q$-function (according to Appendix C, equation 9):
% \begin{equation*}
% Q(s, a)=\mathcal{T}^\pi Q(s, a)-\beta\bigg[\frac{\pi(a | s)}{\mu(a | s)}-1\bigg],
% \end{equation*}
% which is the same as $\alpha = -1$. Hence, CQL is implicitly doing value regularization by applying the Neyman $\chi^{2}$-divergence.
\subsection{Sparse $Q$-Learning (SQL)}
% Note that CQL applies the following conservative policy evaluation operator to learn the $Q$-function (according to Appendix C, equation 9):
We first consider the case where $\alpha = -1$, which we find is the regularization term CQL adds to the policy evaluation operator (according to Appendix C in CQL): $Q(s, a)=\mathcal{T}^\pi Q(s, a)-\beta[\frac{\pi(a | s)}{\mu(a | s)}-1]$.
In this case, we have $f(x) = x-1$ and $g_f(x) = \frac{1}{2}x+\frac{1}{2}$. Plug them into Eq.(\ref{pi_ivr}) and Eq.(\ref{v_ivr}) in Theorem \ref{theorem: 2}, we get the following formulation:
\begin{equation}
\label{q_sql_u}
Q^*(s, a) =r(s, a)+\gamma \mathbb{E}_{s^{\prime} | s, a} \left[ V^*\left(s^{\prime}\right) \right]
\end{equation}
\begin{equation}
\label{pi_sql_u}
\pi^*(a | s) = \mu(a|s) \cdot \max \bigg\{\frac{1}{2} + \frac{Q^*(s, a)-U^*(s)}{2\alpha}, 0\bigg\}
\end{equation}
\begin{equation}
\label{v_sql_u}
V^*(s) = U^*(s) + \alpha \mathbb{E}_{a \sim \mu} \bigg[ \Big( \frac{\pi^*(a | s)}{\mu(a | s)}\Big)^2  \bigg],
\end{equation}
% \begin{align}
% Q^*(s, a) &=r(s, a)+\gamma \mathbb{E}_{s^{\prime} | s, a} \left[ V^*\left(s^{\prime}\right) \right] \\
% \pi^*(a | s) &= \mu(a|s) \cdot \max \left\{\frac{1}{2} + \frac{Q^*(s, a)-U^*(s)}{2\alpha}, 0\right\} \\
% V^*(s) &= U^*(s) + \alpha \mathbb{E}_{a \sim \mu} \left[ \left( \frac{\pi^*(a | s)}{\mu(a | s)}\right)^2  \right],
% \end{align}
where $U^{*}(s)$ needs to satisfy the following equation to make $\pi^{*}$ integrate to 1:
\begin{equation}
\label{eq_u}
\mathbb{E}_{a \sim \mu} \bigg[ \max \Big\{\frac{1}{2} + \frac{Q^*(s, a)-U^*(s)}{2\alpha}, 0\Big\} \bigg] = 1
\end{equation}

It is usually intractable to get the closed-form solution of $U^{*}(s)$ from Eq.(\ref{eq_u}), however, here we make a mathematical transformation and show its equivalence to solving a convex optimization problem.

\begin{lemma}
We can get $U^{*}(s)$ by solving the following optimization problem:
\begin{equation}
\label{min_u}
\min_{U} \ \mathbb{E}_{a \sim \mu} \bigg[ \mathds{1} \Big( \frac{1}{2} + \frac{Q^{*}(s, a) - U(s)}{2\alpha} >0 \Big) \Big( \frac{1}{2} + \frac{Q^{*}(s, a) - U(s)}{2\alpha} \Big)^2 \bigg] + \frac{U(s)}{\alpha}
\end{equation}
\label{proposition: 1}
\end{lemma}
The proof can be easily got if we set the derivative of the objective to 0 with respect to $U(s)$, which is exactly Eq.(\ref{eq_u}).
Now we obtain a learning scheme to get $Q^{*}$, $U^{*}$ and $V^{*}$ by iteratively updating $Q$, $U$ and $V$ following Eq.(\ref{v_sql_u}), objective (\ref{min_u}) and Eq.(\ref{q_sql_u}), respectively. We refer to this learning scheme as SQL-U, however, SQL-U needs to train three networks, which is a bit computationally expensive. 

Note that the term $\mathbb{E}_{a \sim \mu} \big[ \big( \frac{\pi^*(a | s)}{\mu(a | s)} \big)^{2}  \big]$ in Eq.(\ref{v_sql_u}) is equal to $\mathbb{E}_{a \sim \pi^{*}} \big[ \frac{\pi^*(a | s)}{\mu(a | s)}  \big]$. As $\pi^{*}$ is optimized to become mode-seeking, for actions sampled from $\pi^{*}$, its probability $\pi^{*}(a|s)$ should be close to the probability under the behavior policy, $\mu(a|s)$. 
Note that for actions sampled from $\mu$, $\pi^{*}(a|s)$ and $\mu(a|s)$ may have a large difference because $\pi^{*}(a|s)$ may be 0.

Hence in SQL we \textbf{make an approximation} by assuming $\mathbb{E}_{a \sim \pi^{*}} \big[ \frac{\pi^*(a | s)}{\mu(a | s)}  \big]=1$, this removes one network as $U^{*} = V^{*}-\alpha$. Replacing $U^{*}$ with $V^{*}$, we get the following learning scheme that only needs to learn $V$ and $Q$ iteratively to get $V^{*}$ and $Q^{*}$:
\begin{equation}
\label{v_sql}
\min_{V} \ \mathbb{E}_{(s, a) \sim \mathcal{D}} \bigg[ \mathds{1} \Big( 1 + \frac{Q(s, a) - V(s)}{2\alpha} >0 \Big) \Big( 1 + \frac{Q(s, a) - V(s)}{2\alpha} \Big)^2 + \frac{V(s)}{\alpha} \bigg]
\end{equation}
\begin{equation}
\label{q_sql}
\min_{Q} \ \mathbb{E}_{(s, a, s') \sim \mathcal{D}} \Big[\big(r(s, a)+\gamma V(s')-Q(s, a)\big)^2 \Big]
\end{equation}
% \begin{align}
% &\min_{V} \ \mathbb{E}_{(s, a) \sim \mathcal{D}} \left[ \mathds{1} \left( 1 + \frac{Q(s, a) - V(s)}{2\alpha} >0 \right) \left( 1 + \frac{Q(s, a) - V(s)}{2\alpha} \right)^2 + \frac{V(s)}{\alpha} \right] \\
% &\min_{Q} \ \mathbb{E}_{(s, a, s') \sim \mathcal{D}} \left[\big(r(s, a)+\gamma V(s')-Q(s, a)\big)^2 \right]
% \end{align}

After getting $V$ and $Q$, following the formulation of $\pi^{*}$ in Eq.(\ref{pi_sql_u}), we can get the learning objective of policy $\pi$ by minimizing the KL-divergence between $\pi$ and $\pi^{*}$:
\begin{equation}
\label{pi_sql}
\max_{\pi} \ \mathbb{E}_{(s, a) \sim \mathcal{D}}\bigg[ \mathds{1} \Big(1+ \frac{Q(s, a) - V(s)}{2\alpha} >0 \Big) \Big(1+\frac{Q(s, a) - V(s)}{2\alpha} \Big) \log \pi(a|s)\bigg].
\end{equation}

\subsection{Exponential $Q$-Learning (EQL)}
Now let's consider another choice, $\alpha \rightarrow 0$ which is the Reverse KL divergence. 
Note that AWR also uses Reverse KL divergence, however, it applies it to the policy improvement step and needs to sample actions from the policy when learning the value function.
In this case, we get $f(x) = \log(x)$ and $g_f(x) = \exp(x-1)$. Plug them into Eq.(\ref{pi_ivr}) and Eq.(\ref{v_ivr}) in Theorem \ref{theorem: 2}, we have
\begin{equation*}
Q^*(s, a) =r(s, a)+\gamma \mathbb{E}_{s^{\prime} | s, a} \left[ V^*\left(s^{\prime}\right) \right]
\end{equation*}
\begin{equation*}
\pi^*(a | s) = \mu(a|s) \cdot \exp \left(\frac{Q^*(s, a)-U^*(s)}{\alpha} - 1 \right)
\end{equation*}
\begin{equation*}
V^*(s) = U^*(s) + \alpha \mathbb{E}_{a \sim \mu} \bigg[ \Big( \frac{\pi^*(a | s)}{\mu(a | s)}\Big)^2 \frac{\mu(a | s)}{\pi^*(a | s)} \bigg],
\end{equation*}
note that $\mathbb{E}_{a \sim \mu} [ ( \frac{\pi^*(a | s)}{\mu(a | s)})^2 \frac{\mu(a | s)}{\pi^*(a | s)} ]$ is equal to 1, so we get $V^*(s) = U^*(s) + \alpha$, this eliminates the existence of $U^{*}$ \textbf{without any approximation}.
Replacing $U^{*}$ with $V^{*}$, we get the following formulation:
\begin{equation*}
Q^*(s, a) =r(s, a)+\gamma \mathbb{E}_{s^{\prime} | s, a} \left[ V^*\left(s^{\prime}\right) \right]
\end{equation*}
\begin{equation*}
\pi^*(a | s) = \mu(a|s) \cdot \exp \left(\frac{Q^*(s, a)-V^*(s)}{\alpha} \right)
\end{equation*}

Note that $\pi^{*}$ should be integrated to 1, we use the same mathematical transformation did in SQL and get the closed-form solution of $V^{*}(s)$ by solving the following convex optimization problem.

\begin{lemma}
We can get $V^{*}(s)$ by solving the following optimization problem:
\begin{equation*}
\min_{V} \ \mathbb{E}_{a \sim \mu} \bigg[ \exp \Big(\frac{Q^*(s, a)-V(s)}{\alpha} \Big) \bigg] + \frac{V(s)}{\alpha}
\end{equation*}
\label{proposition: 2}
\end{lemma}
Now the final learning objective of $Q$, $V$ and $\pi$ is:
\begin{equation}
\label{v_eql}
\min_{V} \ \mathbb{E}_{(s, a) \sim \mathcal{D}} \bigg[ \exp \Big(\frac{Q(s, a)-V(s)}{\alpha} \Big) + \frac{V(s)}{\alpha} \bigg]
\end{equation}
\begin{equation}
\label{q_eql}
\min_{Q} \ \mathbb{E}_{(s, a, s') \sim \mathcal{D}} \Big[\big(r(s, a)+\gamma V(s')-Q(s, a)\big)^2 \Big]
\end{equation}
\begin{equation}
\label{pi_eql}
\max_{\pi} \ \mathbb{E}_{(s, a) \sim \mathcal{D}}\bigg[ \exp \Big(\frac{Q(s, a)-V(s)}{\alpha} \Big) \log \pi(a|s)\bigg],
\end{equation}
we name this algorithm as EQL (Exponential Q-Learning) because there is an exponential term in the learning objective. Note that one concurrent work, XQL \citep{garg2023extreme}, derives the same learning objective as EQL, but from the perspective of Gumbel Regression.
Although EQL/XQL derives its learning objective without any approximation, one drawback is the exponential term, which causes unstable gradients when learning $V$ and is also more vulnerable to hyperparameter choices than SQL.

To summarize, our final algorithm, SQL and EQL, consist of three supervised stages: learning $V$, learning $Q$, and learning $\pi$. 
We use target networks for $Q$-functions and use clipped double $Q$-learning (take the minimum of two $Q$-functions) in learning $V$ and $\pi$.
We summarize the training procedure in Algorithm \ref{alg:sql}. 

\subsection{Discussions}
\label{sec:discussion}


\begin{wrapfigure}{R}{0.5\textwidth}
\vspace{-0.9cm}
    \begin{minipage}{0.5\textwidth}
        \begin{algorithm}[H]
        \small
            \caption{Sparse or Exponential $Q$-Learning}
            \label{alg:sql}
            \begin{algorithmic}[1]
                \Require $\mathcal{D}$, $\alpha$.
                \State Initialize $Q_{\phi}$, $Q_{\phi^{\prime}}$, $V_{\psi}$, $\pi_{\theta}$
                \For{$t=1, 2,\cdots, N$}
                \State Sample transitions $(s, a, r, s') \sim \mathcal{D}$
                \State Update $V_{\psi}$ by Eq.(\ref{v_sql}) or Eq.(\ref{v_eql}) using $V_{\psi}$, $Q_{\phi^{\prime}}$
                \State Update $Q_{\phi}$ by Eq.(\ref{q_sql}) or Eq.(\ref{q_eql}) using $V_{\psi}$, $Q_{\phi}$
                \State Update $Q_{\phi^{\prime}}$ by $\phi^{\prime} \leftarrow \lambda \phi + (1-\lambda) \phi'$
                \State Update $\pi_{\theta}$ by Eq.(\ref{pi_sql}) or Eq.(\ref{pi_eql}) using $V_{\psi}$, $Q_{\phi^{\prime}}$
                \EndFor
            \end{algorithmic}
        \end{algorithm}
    \end{minipage}
\vspace{-0.5cm}
\end{wrapfigure}

SQL and EQL establishes the connection with several prior works such as CQL, IQL and AWR. 

Like CQL pushes down policy $Q$-values and pushes up dataset $Q$-values, in SQL and EQL, the first term in Eq.(\ref{v_sql}) and Eq.(\ref{v_eql}) pushes up $V$-values if $Q-V>0$ while the second term pushes down $V$-values, and $\alpha$ trades off these two terms.
SQL incorporates the same inherent conservatism as CQL by adding the $\chi^{2}$-divergence to the policy evaluation operator. However, SQL learns the value function using only dataset samples while CQL needs to sample actions from the policy. In this sense, SQL is an "implicit" version of CQL that avoids any out-of-distribution action.
Like AWR, EQL applies the KL-divergence, but implicitly in the policy evaluation step. In this sense, EQL is an "implicit" version of AWR that avoids any OOD action. 
% The difference between EQL and AWR is that EQL is doing implicit value regularization while AWR (also AWAC) is doing implicit policy regularization. 

Like IQL, SQL and EQL learn both $V$-function and $Q$-function. However, IQL appears to be a heuristic approach and the learning objective of $V$-function in IQL has a drawback. We compute the derivative of the $V$-function learning objective with respect to the residual ($Q-V$) in SQL and IQL (see Figure \ref{fig: sql} in Appendix \ref{stat_view}). We find that SQL keeps the derivative unchanged when the residual is below a threshold, while IQL doesn't. In IQL, the derivative keeps decreasing as the residual becomes more negative, hence, the $V$-function will be over-underestimated by those bad actions whose $Q$-value is extremely small. 
Note that SQL and EQL will assign a zero or exponential small probability mass to those bad actions according to Eq.(\ref{pi_sql}) and Eq.(\ref{pi_eql}), the sparsity is incorporated due to the mode-seeking behavior of $\chi^{2}$-divergence and KL-divergence.

Also, IQL needs two hyperparameters ($\tau$ and $\beta$) while SQL only needs one ($\alpha$). The two hyperparameters in IQL may not align well because they represent two different regularizations. Note that objective (\ref{pi_eql}) is exactly how IQL extracts the policy! However, the corresponding optimal $V$-function learning objective (\ref{v_eql}) is not objective (\ref{v_iql}). This reveals that the policy extraction part in IQL gets a different policy from the one implied in the optimal $Q$-function.

% Like OptiDICE \citep{lee2021optidice} and SMODICE \citep{ma2022versatile}, SQL derives the solution by solving a regularized optimization problem. However, SMODICE solves an upper bound of the learning objective while SQL solves the exact one. Additionally, OptiDICE and SMODICE solve for the state visitation distribution of the optimal policy, $d^{*}$, rather than $\pi^{*}$, this results a residual learning scheme, which is known to have slower convergence than fitted TD-learning \citep{baird1995residual}.



\begin{table}[t]
\centering
% \small
% \begin{threeparttable}
\caption{
Averaged normalized scores of SQL against other baselines. The scores are taken over the final 10 evaluations with 5 seeds. SQL or EQL achieves the highest scores in 14 out of 18 tasks.}
% Our method outperforms prior methods on the challenging Ant Maze tasks, which require dynamic programming, and is competitive with the best prior methods on the locomotion tasks.  %
\label{tab:d4rl}
% \scriptsize
\resizebox{\linewidth}{!}{
\begin{tabular}{l||rrrrrrrr|rr}
\toprule
Dataset                      & BC    & 10$\%$BC & BCQ   & DT    & One-step & TD3+BC & CQL   & IQL                  & SQL  & EQL  \\
\hline
halfcheetah-m        & 42.6  & 42.5   & 47.0  & 42.6  & \colorbox{mine}{48.4}       & 48.3   & 44.0 \color{my_g}{$\pm$0.8}  & 47.4 \color{my_g}{$\pm$0.2}         &  48.3\color{my_g}{$\pm$0.2} &  47.2\color{my_g}{$\pm$0.3} \\
hopper-m            & 52.9  & 56.9   & 56.7  & 67.6  & 59.6       & 59.3   & 58.5 \color{my_g}{$\pm$2.1} & 66.3 \color{my_g}{$\pm$5.7}                &  \colorbox{mine}{75.5}\color{my_g}{$\pm$3.4} &  74.6\color{my_g}{$\pm$2.6} \\
walker2d-m           & 75.3  & 75.0   & 72.6  & 74.0  & 81.8       & 83.7   & 72.5 \color{my_g}{$\pm$0.8} & 72.5 \color{my_g}{$\pm$8.7}               &  \colorbox{mine}{84.2}\color{my_g}{$\pm$4.6}  &  83.2\color{my_g}{$\pm$4.4}\\
halfcheetah-m-r & 36.6  & 40.6   & 40.4  & 36.6  & 38.1       & 44.6   & \colorbox{mine}{45.5} \color{my_g}{$\pm$0.5}  & 44.2 \color{my_g}{$\pm$1.2}                &   44.8\color{my_g}{$\pm$0.7} &   44.5\color{my_g}{$\pm$0.5}\\
hopper-m-r      & 18.1  & 75.9   & 53.3  & 82.7  & 97.5       & 60.9   & 95.0 \color{my_g}{$\pm$6.4}  & 95.2  \color{my_g}{$\pm$8.6}               &  \colorbox{mine}{99.7}\color{my_g}{$\pm$3.3} &  98.1\color{my_g}{$\pm$3.6}\\
walker2d-m-r    & 26.0  & 62.5   & 52.1  & 66.6  & 49.5       & \colorbox{mine}{81.8}   & 77.2\color{my_g}{$\pm$5.5}  & 76.1 \color{my_g}{$\pm$7.3}                &  81.2\color{my_g}{$\pm$3.8} &  76.6\color{my_g}{$\pm$4.2}\\
halfcheetah-m-e & 55.2  & 92.9   & 89.1  & 86.8  & 93.4       & 90.7   & 90.7\color{my_g}{$\pm$4.3}  & 86.7\color{my_g}{$\pm$5.3}                 & \colorbox{mine}{94.0}\color{my_g}{$\pm$0.4} &   90.6\color{my_g}{$\pm$0.5}\\
hopper-m-e      & 52.5  & 110.9  & 81.8  & 107.6 & 103.3      & 98.0   & 105.4\color{my_g}{$\pm$6.8} & 101.5 \color{my_g}{$\pm$7.3}                    &\colorbox{mine}{111.8}\color{my_g}{$\pm$2.2} &  105.5\color{my_g}{$\pm$2.1}\\
walker2d-m-e    & 107.5 & 109.0  & 109.0 & 108.1 & \colorbox{mine}{113.0}      & 110.1  & 109.6\color{my_g}{$\pm$0.7} & 110.6\color{my_g}{$\pm$1.0}                & 110.0\color{my_g}{$\pm$0.8} &  110.2\color{my_g}{$\pm$0.8}\\ 
\hline
% mujoco mean           & 51.9  & 74.0   & 66.9  & 74.8  & 76.1       & 75.2   & 77.5\color{my_g}{$\pm$3.1}  &  77.8 \color{my_g}{$\pm$5.0}                  &  \colorbox{mine}{82.3} \color{my_g}{$\pm$1.4}\\
\hline 
antmaze-u             & 54.6  & 62.8   & 78.9  & 59.2  & 64.3       & 78.6   & 84.8\color{my_g}{$\pm$2.3}  & 85.5 \color{my_g}{$\pm$1.9}                & 92.2\color{my_g}{$\pm$1.4}  & \colorbox{mine}{93.2}\color{my_g}{$\pm$2.2} \\
antmaze-u-d     & 45.6  & 50.2   & 55.0  & 53.0  & 60.7       & 71.4   & 43.4\color{my_g}{$\pm$5.4}  & 66.7 \color{my_g}{$\pm$4.0}               & \colorbox{mine}{74.0}\color{my_g}{$\pm$2.3} &  65.4\color{my_g}{$\pm$2.7} \\
antmaze-m-p       & 0     & 5.4    & 0     & 0.0   & 0.3        & 10.6   & 65.2\color{my_g}{$\pm$4.8} & 72.2  \color{my_g}{$\pm$5.3}               & \colorbox{mine}{80.2}\color{my_g}{$\pm$3.7}   &  77.5\color{my_g}{$\pm$4.3} \\
antmaze-m-d    & 0     & 9.8    & 0     & 0.0   & 0.0        & 3.0    & 54.0\color{my_g}{$\pm$11.7}  & 71.0  \color{my_g}{$\pm$3.2}               & \colorbox{mine}{79.1}\color{my_g}{$\pm$4.2} &  70.0\color{my_g}{$\pm$3.7} \\
antmaze-l-p        & 0     & 0.0    & 6.7   & 0.0   & 0.0        & 0.2    & 38.4\color{my_g}{$\pm$12.3}  & 39.6 \color{my_g}{$\pm$4.5}               & \colorbox{mine}{53.2}\color{my_g}{$\pm$4.8}  &  45.6\color{my_g}{$\pm$4.2}  \\
antmaze-l-d     & 0     & 6.0    & 2.2   & 0.0   & 0.0        & 0.0    & 31.6\color{my_g}{$\pm$9.5}  & 47.5 \color{my_g}{$\pm$4.4}                & \colorbox{mine}{52.3}\color{my_g}{$\pm$5.2}  &  42.5\color{my_g}{$\pm$4.7} \\ 
% \hline
% antmaze mean              & 16.7  & 21.3   & 23.8  & 18.7  & 20.9       & 27.3   & 61.9\color{my_g}{$\pm$7.6}  &  63.7\color{my_g}{$\pm$3.6}                  & \colorbox{mine}{71.3}\color{my_g}{$\pm$4.2} \\
\hline
kitchen-c          & 33.8  & -      &   -    & -     &   -        &    -    & 43.8 \color{my_g}{$\pm$11.2} & 61.4  \color{my_g}{$\pm$9.5}               &   \colorbox{mine}{76.4}\color{my_g}{$\pm$8.7 } &   70.3\color{my_g}{$\pm$7.1}\\
kitchen-p           & 33.9  & -      &   -    & -     & -          &     -   & 49.8\color{my_g}{$\pm$10.1}  & 46.1 \color{my_g}{$\pm$8.5}               &  72.5\color{my_g}{$\pm$7.4}  &   \colorbox{mine}{74.5}\color{my_g}{$\pm$3.8}   \\
kitchen-m               & 47.5  & -       &  -     &  -     &  -          &     -   & 51.0\color{my_g}{$\pm$6.5}  & 52.8  \color{my_g}{$\pm$4.5}               &  \colorbox{mine}{67.4}\color{my_g}{$\pm$5.4} &  55.6\color{my_g}{$\pm$5.2}\\
\hline
% kitchen mean              &   38.4    & -      &       & -     & -          & -      & 48.2 \color{my_g}{$\pm$9.2}  & 53.4\color{my_g}{$\pm$7.5}                     &   \colorbox{mine}{67.8}\color{my_g}{$\pm$6.8}    \\
% \multicolumn{1}{l}{}         &       &        &       &       &            &        &       & \multicolumn{1}{r}{} &                
\bottomrule
\end{tabular}}
% \end{threeparttable}
\end{table}


\section{Experiments}
\label{exp}
We present empirical evaluations of SQL and EQL in this section. 
We first evaluate SQL and EQL against other baseline algorithms on benchmark offline RL datasets.
We then show the benefits of sparsity introduced in SQL and EQL by comparing them with IQL in noisy data regimes.
We finally show the robustness of SQL and EQL by comparing them with CQL in small data regimes.

% Our experiments try to make empirical studies of SQL comparatively, in contrast with previous offline RL algorithms, and in particular to understand the effect of sparsity in in-sample learning manner and the difference between implicit and explicit value regularization. We first compare our proposed algorithm SQL with other representative algorithms on D4RL benchmarks \cite{fu2020d4rl} in the same manner. We then illustrate the benefit of introducing sparsity in the in-sample learning manner and implicit value regularization, respectively. 


\subsection{Benchmark Datasets}
% Intro to the dataset, Antmaze, Kitchen: rely on value function. Kitchen: realistic/small dataset/non-navigation goal tasks "stitching" non-trivial/This dataset requires the highest degree of generalization in order to succeed.
%intro the compared algos. (some details)
%explain the results

We first evaluate our approach on D4RL datasets \citep{fu2020d4rl}. It is worth mentioning that Antmaze and Kitchen datasets include few or no near-optimal trajectories, and highly require learning a value function to obtain effective policies via "stitching". 
We compare SQL with prior state-of-the-art offline RL methods, including BC \citep{pomerleau1989alvinn}, 10\%BC \citep{chen2021decision}, BCQ \citep{fujimoto2018addressing}, DT \citep{chen2021decision}, TD3+BC \citep{fujimoto2021minimalist}, One-step RL \citep{brandfonbrener2021offline}, CQL \citep{kumar2020conservative}, and IQL \citep{kostrikov2021offline}. 
Aggregated results are displayed in \Cref{tab:d4rl}.
In MuJoCo tasks, where performance is already saturated, SQL and EQL show competitive results to the best performance of prior methods. In more challenging AntMaze and Kitchen tasks, SQL and EQL outperform all other baselines by a large margin. This shows the effectiveness of value learning in SQL and EQL.
% Moreover, SQL convergences much faster than all prior methods, e.g., 0.2 million training steps for most tasks. 
We show learning curves and performance profiles generated by the rliable library \citep{agarwal2021deep} in Appendix \ref{appendix: exp}.

We then compare our approach with other baselines on high-dimensional image-based Atari datasets in RL Unplugged \citep{gulcehre2020rl}. Our approach also achieves superior performance on these datasets, we show aggregated results, performance profiles and experimental details in Appendix \ref{appendix: exp}.
% The proposed algorithm SQL shows superior performance on all ranges of datasets, from near-optimal ones in which simple behavior cloning is sufficient, to suboptimal datasets in which value learning is necessary. 

% SQL outperforms all prior approaches, especially a large margin in Kitchen tasks that requires the highest degree of "stitching" and generalization in order to succeed. 


% Examining the learning curves, we can see that our approach achieves a similar learning speed and stability, without requiring any pre-training phase.

% The results are shown in Table \ref{table:results}, in MuJoCo locomotion tasks, \texttt{POR} performs competitively to the best performance of prior methods in high-quality datasets, while achieving much better performance than other methods in low-quality datasets (e.g., \texttt{random} and \texttt{medium} datasets). 
% On the more challenging AntMaze tasks, \texttt{POR} outperforms all other baselines by a large margin. It even surpassed RvS that uses the oracle goal information to learn the policy.
% We conjecture that the success in all those datasets should be credited to the out-of-distribution generalization ability of \texttt{POR}.
\subsection{Noisy Data Regime}
\label{subsec: noisy}
\begin{wrapfigure}{r}{0.6\textwidth}
\vspace{-1cm}
  \begin{center}
    \includegraphics[width=0.56\textwidth]{figs/mix_data.pdf}
  \end{center}
  \vspace{-10pt}
  \caption{Performance of different methods in noisy data regimes.}
  \vspace{-10pt}
\label{fig: noisy}
\end{wrapfigure}
In this section, we try to validate our hypothesis that the sparsity term our algorithm introduced in learning the value function will benefit when the datasets contain a large portion of noisy transitions.
To do so, we make a "mixed" dataset by combining random datasets and expert dataset with different expert ratios. 
We test the performance of SQL, EQL and IQL under different mixing ratios in \Cref{fig: noisy}.

It is shown that SQL and EQL outperforms IQL under all settings. The performance of IQL is vulnerable to the expert ratio, it has a sharp decrease from 30\% to 1\% while SQL and EQL still retain the expert performance. For example, in walker2d, SQL and EQL reaches near 100 performance when the expert ratio is only 5\%; in halfcheetah, IQL is affected even with a high expert ratio (30\%).

% With a decrease in the expert data ratio, as the sparsity filters noisy signals, SQL outperforms IQL with a growing margin and remains competitive in the extremely hard task (1\% expert data). Notably, The performance of IQL begins to catastrophic deteriorate from 10\% to 5\% expert data, while SQL remains firm by involving sparsity.

% The resulting algorithm also bears similarities to IQL However, we find that our algorithm introduces sparsity in learning the state-value function, which is missing in IQL. The sparsity term filters out those bad actions whose Q-values are below a threshold, which brings benefits when the quality of offline datasets is inferior, we thus dub our method Sparse Q-learning (SQL).


% \textbf{Is there any advantage to introducing sparsity in in-sample learning manners?} We hypothesize that the learning policies from in-sample learning manners without sparsity, e.g., IQL, will still be influenced by the noisy signals even though they are weak, which leads to a decreased performance on evaluation.



\subsection{Small Data Regime}
\label{subsec: small data}
\begin{wraptable}{r}{7cm}
 % \begin{table}[t]\centering
% \begin{threeparttable}
% \renewcommand\arraystretch{1.1}
\scriptsize
\centering
\vspace{-15pt}
\caption{The normalized return (NR) and Bellman error (BR) of CQL, SQL and EQL in small data regimes.} 
\label{tab:few data regime}
% \begin{tabular}{cc||rr|rr} 
% \toprule
% \multicolumn{2}{l||}{\multirow{2}{*}{Dataset (AntMaze)}} & \multicolumn{2}{c|}{CQL}                         & \multicolumn{2}{c}{SQL}                          \\ 
% \cline{3-6}
% \multicolumn{2}{l||}{}                                   & \multicolumn{1}{c}{NR} & \multicolumn{1}{c|}{BE} & \multicolumn{1}{c}{NR} & \multicolumn{1}{c}{BE}  \\ 
% \hline
% \multirow{4}{*}{Medium} & Vanilla                        & 65.2                   & 13.1                    & 70.0                   & 1.6                     \\
%                         & Easy                           & 48.2                   & 14.8                    & 56.2                   & 1.7                     \\
%                         & Medium                         & 14.5                   & 14.7                    & 43.3                   & 2.1                     \\
%                         & Hard                           & 9.3                    & 64.4                    & 24.2                   & 1.9                     \\ 
% \hline
% \multirow{4}{*}{Large}  & Vanilla                        & 38.4                   & 13.5                    & 49.8                   & 1.4                     \\
%                         & Easy                           & 28.1                   & 12.8                    & 40.5                   & 1.5                     \\
%                         & Medium                         & 6.3                    & 30.6                    & 36.7                   & 1.3                     \\
%                         & Hard                           & 0                      & 300.5                   & 34.2                   & 2.6                     \\
% \bottomrule
% \end{tabular}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{cc||rr|rr|rr} 
\toprule
\multicolumn{2}{l||}{\multirow{2}{*}{Dataset (AntMaze)}} & \multicolumn{2}{c|}{CQL}                         & \multicolumn{2}{c|}{SQL}                         & \multicolumn{2}{c}{EQL}                          \\ 
\cline{3-8}
\multicolumn{2}{l||}{}                                   & \multicolumn{1}{c}{NR} & \multicolumn{1}{c|}{BE} & \multicolumn{1}{l}{NR} & \multicolumn{1}{l|}{BE} & \multicolumn{1}{c}{NR} & \multicolumn{1}{c}{BE}  \\ 
\hline
\multirow{4}{*}{Medium} & Vanilla                        & 65.2                   & 13.1                    & 75.1                   & 1.6                     & 74.0                   & 2.2                     \\
                        & Easy                           & 48.2                   & 14.8                    & 56.2                   & 1.7                     & 57.5                   & 1.1                     \\
                        & Medium                         & 14.5                   & 14.7                    & 43.3                   & 2.1                     & 39.7                   & 2.3                     \\
                        & Hard                           & 9.3                    & 64.4                    & 24.2                   & 1.9                     & 19.6                   & 1.8                     \\
\hline
\multirow{4}{*}{Large}  & Vanilla                        & 38.4                   & 13.5                    & 50.2                   & 1.4                     & 49.6                   & 1.7                     \\
                        & Easy                           & 28.1                   & 12.8                    & 40.5                   & 1.5                     & 40.4                   & 1.7                     \\
                        & Medium                         & 6.3                    & 30.6                    & 36.7                   & 1.3                     & 35.3                   & 1.8                     \\
                        & Hard                           & 0                      & 300.5                   & 34.2                   & 2.6                     & 31.6                   & 1.6                     \\
\bottomrule
\end{tabular}}
\vspace{-10pt}
% \end{threeparttable}
% \end{table}   
\end{wraptable}
% To check the effect of in-sample learning. SQL vs CQL. Use the same regularization term, 1) SQL keeps smaller TD error because only uses datasets samples, 2) the performance drop ratio of SQL is small while CQL is large. 

In this section, we try to explore the benefits of in-sample learning over out-of-sample learning. 
We are interested to see whether in-sample learning brings more robustness than out-of-sample learning when the dataset size is small or the dataset diversity of some states is small, which are challenges one might encounter when using offline RL algorithms on real-world data.


% Based on the Vanilla dataset, for each transition, the closer to the target location, the higher probability that will be discarded. We expect these datasets to simulate the situation where the dataset is fewer and has limited state coverage near the target location because the data generation policies maybe be not satisfied and more determined when they get closer to the target location \cite{kumar2022WhenShould}.

To do so, we make custom datasets by discarding some transitions in the AntMaze datasets. For each transition, the closer it is to the target location, the higher probability it will be discarded from the dataset. 
This simulates the scenarios (i.e., robotic manipulation) where the dataset is fewer and has limited state coverage near the target location because the (stochastic) data generation policies maybe not be successful and are more determined when they get closer to the target location \citep{kumar2022WhenShould}.
We use a hyperparameter to control the discarding ratio and build three new tasks: \texttt{Easy}, \texttt{Medium} and \texttt{Hard}, with dataset becomes smaller. For details please refer to Appendix \ref{appendix: exp}.  
We compare SQL with CQL as they use the same inherent value regularization but SQL uses in-sample learning while CQL uses out-of-sample learning, 

We demonstrate the final normalized return (NR) during evaluation and the mean squared Bellman error (BE) during training in \Cref{tab:few data regime}. 
It is shown that CQL has a significant performance drop when the difficulty of tasks increases, and the Bellman error also exponentially grows up, indicating that the value extrapolation error becomes large in small data regimes.
SQL and EQL remain a stable yet good performance under all difficulties, the Bellman error of SQL is much smaller than that of CQL. 
This justifies the benefits of in-sample learning, i.e., it avoids erroneous value estimation by using only dataset samples while still allowing in-sample generalization to obtain a good performance.


% \textbf{Is implicit a better choice than explicit value regularization in the small data regime?} We see that SQL significantly outperforms CQL in small dataset Kitchen tasks in D4RL benchmarks \Cref{tab:d4rl}. \jl{In order to validate our conjecture in \Cref{sec:discussion}. ..haoran...} We build more realistic and challenging small datasets in the AntMaze suite (medium and large) as our test board. Based on the Vanilla dataset, for each transition, the closer to the target location, the higher probability that will be discarded. We expect these datasets to simulate the situation where the dataset is fewer and has limited state coverage near the target location because the data generation policies maybe be not satisfied and more determined when they get closer to the target location \cite{kumar2022WhenShould}. For example, \jl{a robot picking up an object has many good options to approach the object, but when the robot grasps the object, its behavior should be more deterministic to ensure successful grasp without damaging or dropping the object [33].} We use a hyper-parameter to control the difficulty degree and build three small datasets in each suite, details are in Appendix \ref{appendix: exp}. 

% We intend this dataset to be representative of situations where the data generation policies are more deterministic in some states than others [26] â€“ a robot picking up an object has many good options to approach the object, but when the robot grasps the object, its behavior should be more deterministic to ensure successful grasp without damaging or dropping the object [33].
% It is worth noting that our way to generate the small dataset goes as far as possible to produce data in a form that matches the real-world setting, instead of randomly reducing the amount of data.
% Intuitively, the closer the transition is to the \texttt{GOAL}, the higher the probability that it will be discarded.

% We demonstrate the final normalized return (NR) and mean squared Bellman error (BE) in \Cref{tab:few data regime}. CQL shows a significant performance drop by increasing the degree of difficulty, as well as an exponentially growing Bellman error. On the other hand, the performance drop of SQL remains stable, and the Bellman error is close to zero. We speculate that the reason for the above results comes from the different value regularization: implicit and explicit. Implicit value regularization, e.g., SQL, only needs the generalization within the dataset distribution; explicit value regularization, on the other hand, highly replies on the outside. In the small dataset regime, generalization outside the dataset distribution is extremely implausible \cite{cxcx} and results in a considerable Bellman error.


% With the growing degree of difficulty, SQL consistently outperforms CQL in normalized return and keeps a more smooth performance drop. Additionally, we reveal that the Bellman error of CQL is bigger than SQL with a large margin and increases substantially from medium to hard levels in all tasks. 



\section{Conclusions and Future Work}
\label{con}
% In this work, we propose a novel offline RL approach, \texttt{POR}, which leverages the training stability of imitation-style methods while still encouraging logical out-of-distribution generalization. \texttt{POR} allows \textit{state-compositionality} rather than \textit{action-compositionality} from the dataset. Through theoretical analysis and extensive experiments, we show that \texttt{POR} outperforms prior methods on a variety of datasets, especially those with low quality. We also empirically demonstrate the additional benefits of \texttt{POR} in terms of improving with supplementary suboptimal data and easily adapting to new tasks. 
% One limitation of \texttt{POR} is that the prediction error of the guide-policy may be large when the state space is high-dimensional. However, this can be alleviated with the help of representation learning \cite{laskin_srinivas2020curl, zhang2020learning}.
In this paper, we propose a general Implicit Value Regularization framework, which builds the bridge between behavior regularized and in-sample learning methods in offline RL.
Based on this framework, we propose two practical algorithms, which use the same value regularization in existing works, but in a complete in-sample manner.  
We verify the effectiveness of our algorithms on both the D4RL benchmark and customed noisy and small data regimes by comparing it with different baselines. 
% Under all settings, SQL achieves better results and owns faster convergence compared to other baselines. 
% SQL is easy to implement, shows fast convergence, and has the promise to become the new SOTA offline RL algorithm.
One future work is to scale our proposed framework to online RL \citep{garg2023extreme}, offline-to-online RL or offline IL \citep{li2023mind}.
% anymore, what are the pros and cons? Whatâ€™s the theoretical effect of different $\phi$?
Another future work is, instead of only constraining action distribution, constraining the state-action distribution between $d^{\pi}$ and $d^{D}$ as considered in \citet{nachum2019algaedice}.
% as considered in \citet{nachum2019algaedice,lee2021optidice}.


% \textbf{Future work} \quad 
% What about other choices of $\phi$ or other formulations? For example, $\log(x)$, this is no sparsity anymore, what are the pros and cons? Whatâ€™s the theoretical effect of different $\phi$?

% Another future work is, instead of only constrain action distribution, consider to regularize between the stationary distribution $d^{\pi}$ and the dataset distribution $d^{D}$ as considered in \citet{nachum2019algaedice,lee2021optidice}.

% {\color{blue}
% \section*{Reproducibilty Statement}
% We ensure the reproducibility of our method via a) releasing the code used to obtain our results upon publication, b) releasing the customed dataset that we used in our experiments to allow the community to conduct further research on this domain upon publication and c) detailing the hyperparameters and design choices for the implementation of SQL and other baselines in Appendix \ref{appendix: exp}.
% }
\section*{Acknowledgement}
This work is supported by funding from Intel Corporation. The authors would like to thank the anonymous reviewers for their feedback on the manuscripts.

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}


\clearpage
\appendix


\section{A Statistical View of Why SQL and EQL Work}
\label{stat_view}
Inspired by the analysis in IQL, we give another view of why SQL and EQL could learn the optimal policy.
Consider estimating a parameter $m_{\alpha}$ for a random variable $X$ using samples from a dataset $\mathcal{D}$, we show that $m_{\alpha}$ could fit the extrema of $X$ by using the learning objective of $V$-function in SQL:
\begin{equation*}
\arg \min_{m_{\alpha}} \ \mathbb{E}_{x \sim \mathcal{D}} \bigg[ \mathds{1} \Big( 1 + \frac{x - m_{\alpha}}{2\alpha} >0 \Big) \Big( 1 + \frac{x - m_{\alpha}}{2\alpha} \Big)^2 + \frac{m_{\alpha}}{\alpha} \bigg],
\end{equation*}
or using the learning objective of $V$-function in EQL:
\begin{equation*}
\arg \min_{m_{\alpha}} \ \mathbb{E}_{x \sim \mathcal{D}} \bigg[ \exp \Big(\frac{x-m_{\alpha}}{\alpha} \Big) + \frac{m_{\alpha}}{\alpha} \bigg]
\end{equation*}

In Figure \ref{fig: sql} and Figure \ref{fig: eql}, we give an example of estimating the state conditional extrema of a two-dimensional random variable, as shown, $\alpha \rightarrow 0$ approximates the maximum operator over in-support values of $y$ given $x$. This phenomenon can be justified in our IVR framework as the value function becomes more optimal with less value regularization. However, less value regularization also brings more distributional shift, so we need a proper $\alpha$ to trade-off optimality against distributional shift.

\begin{figure}[h]
\centering
\resizebox{1\linewidth}{!}{
		\centering

			\begin{minipage}[b]{0.4\textwidth}
				\includegraphics[width=1\textwidth]{figs/SQL_loss.pdf}
			\end{minipage}
			\begin{minipage}[b]{0.4\textwidth}
				\includegraphics[width=1\textwidth]{figs/SQL_two_dims_RVs.pdf}
			\end{minipage}
			\begin{minipage}[b]{0.4\textwidth}
				\includegraphics[width=1\textwidth]{figs/SQL_IQL_grad.pdf}
			\end{minipage}
		}
\caption{\textbf{Left:} The loss with respect to the residual ($Q-V$) in the learning objective of $V$ in SQL with different $\alpha$. \textbf{Center:} 
An example of estimating state conditional extrema of a two-dimensional random variable (generated by adding random noise to samples from $y=\sin(x)$). Each $x$ corresponds to a distribution over $y$. The loss fits the extrema more with $\alpha$ becoming smaller.
\textbf{Right:} The comparison of the derivative of loss of SQL and IQL. In SQL, the derivative keeps unchanged when the residual is below a threshold. }
\label{fig: sql}
\end{figure}


\begin{figure}[h]
\centering
\resizebox{1\linewidth}{!}{
		\centering

			\begin{minipage}[b]{0.4\textwidth}
				\includegraphics[width=1\textwidth]{figs/EQL_loss.pdf}
			\end{minipage}
			\begin{minipage}[b]{0.4\textwidth}
				\includegraphics[width=1\textwidth]{figs/EQL_two_dims_RVs.pdf}
			\end{minipage}
			\begin{minipage}[b]{0.4\textwidth}
				\includegraphics[width=1\textwidth]{figs/EQL_IQL_grad.pdf}
			\end{minipage}
		}
\caption{\textbf{Left:} The loss with respect to the residual ($Q-V$) in the learning objective of $V$ in EQL with different $\alpha$. \textbf{Center:} 
An example of estimating state conditional extrema of a two-dimensional random variable (generated by adding random noise to samples from $y=\sin(x)$). Each $x$ corresponds to a distribution over $y$. The loss fits the extrema more with $\alpha$ becoming smaller.
\textbf{Right:} The comparison of the derivative of loss of EQL and IQL. In EQL, the derivative softly decreases and keeps (nearly) unchanged when the residual is below a threshold. }
\label{fig: eql}
\end{figure}



\section{How Does Sparsity Benefit Value Learning in SQL?}
In this section, we add more experiments about the sparsity characteristic of SQL. 
We use a toy example in the tabular setting to demonstrate how sparsity benefits value learning in SQL. 
We study the relationship of normalized score and sparsity ratio with $\alpha$ in the continuous action setting to clearly show that sparsity plays an important role in the performance of SQL. 


\begin{figure}[h]
\centering
\resizebox{1\linewidth}{!}{
\centering
\includegraphics[width=1\textwidth]{figs/toy_case_modify.pdf}
}
\caption{Evaluation of IQL and SQL on the Four Rooms environment. SQL learns a more optimal value function and produces a better policy than IQL when the dataset is heavily corrupted by suboptimal actions.}
\label{fig: toy_case}
\end{figure}


\begin{table}[!h]
\centering
\caption{The relationship of the normalized score (left) and non-sparsity ratio (right) with $\alpha$ in MuJoCo datasets.}
\label{tab:sql_alpha_mujoco}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{l||rr|rr|rr|rr|rr} 
\toprule
$\alpha$        & \multicolumn{2}{c|}{0.5}                               & \multicolumn{2}{c|}{1}                                 & \multicolumn{2}{c|}{2}                                 & \multicolumn{2}{c|}{5}                                 & \multicolumn{2}{c}{10}                                 \\ 
\hline
            metrics    & \multicolumn{1}{l}{score} & \multicolumn{1}{l|}{ratio} & \multicolumn{1}{l}{score} & \multicolumn{1}{l|}{ratio} & \multicolumn{1}{l}{score} & \multicolumn{1}{l|}{ratio} & \multicolumn{1}{l}{score} & \multicolumn{1}{l|}{ratio} & \multicolumn{1}{l}{score} & \multicolumn{1}{l}{ratio}  \\ 
\hline
halfcheetah-m   & 48.3 & 0.72              & \colorbox{mine}{48.7} & 0.91 & 48.1           & 0.96          & 47.4           & 0.97          & 47.1          & 1.00           \\
hopper-m        & 62.5 & 0.84              & \colorbox{mine}{74.5} & 0.90 & 74.1           & 0.98          & 68.5           & 0.99          & 62.4          & 0.99           \\
walker2d-m      & 22.3 & 0.03              & 65.3          & 0.93          & \colorbox{mine}{84.2}  & 0.98 & 83.7           & 0.99          & 84.1          & 0.99           \\
halfcheetah-m-r & 43.2 & 0.65              & 44.2          & 0.84          & 44.8           & 0.89          & \colorbox{mine}{44.9}  & 0.97 & 44.8          & 0.99           \\
hopper-m-r      & 43.5 & 0.53              & 95.5          & 0.74          & \colorbox{mine}{100.7} & 0.78 & 63.3           & 0.94          & 74.2          & 0.99           \\
walker2d-m-r    & 5.9  & 0.51              & 38.2          & 0.70          & \colorbox{mine}{82.2}  & 0.89 & 80.3           & 0.97          & 81.3          & 1.00           \\
halfcheetah-m-e & 40.2 & 0.38              & 39.3          & 0.33          & 35.8           & 0.27          & 94.2           & 0.99          & \colorbox{mine}{94.8} & 0.99  \\
hopper-m-e      & 18.6 & 0.07              & 106.3         & 0.89          & 106.6          & 0.94          & \colorbox{mine}{111.9} & 0.96 & 111.5         & 0.99           \\
walker2d-m-e    & 9.2  & 0.01              & 110.2         & 0.95          & \colorbox{mine}{111.3} & 0.96 & 109.5          & 0.99          & 111.2         & 0.99           \\ 
\hline
mujoco-mean     & 30.4 &  0.41            & 69.2           &   0.80        & 76.4               &  0.85             &   78.1             &  0.97             &  \colorbox{mine}{79.0}             &  0.99              \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[!h]
\centering
\caption{The relationship of the normalized score (left) and non-sparsity ratio (right) with $\alpha$ in AntMaze datasets.}
\label{tab:sql_alpha_antmaze}
\resizebox{\textwidth}{!}{
\begin{tabular}{l||rr|rr|rr|rr|rr|rr|rr} 
\toprule
$\alpha$     & \multicolumn{2}{c|}{0.2}        & \multicolumn{2}{c|}{0.3} & \multicolumn{2}{c|}{0.5} & \multicolumn{2}{c|}{0.7} & \multicolumn{2}{c|}{0.9} & \multicolumn{2}{c|}{1.0} & \multicolumn{2}{c}{2.0}  \\ 
\hline
metrics      & \multicolumn{1}{l}{score} & \multicolumn{1}{l|}{ratio} & score & ratio            & score & ratio            & score & ratio            & score & ratio            & score & ratio            & score & ratio            \\ 
\hline
antmaze-m-d  & 40.3 & 0.01                     & 62.6  & 0.39             &  \colorbox{mine}{75.1}     & 0.35             & 68.6  & 0.70             & 61.6  & 0.82             & 63.4  & 0.77             & 17.5  & 0.92             \\
antmaze-m-p  & 0.0    & 0.03                     & 70.2  & 0.32             &  \colorbox{mine}{80.2}     & 0.62             & 67.5  & 0.81             & 69.3  & 0.83             & 67.3  & 0.75             & 2.0   & 0.88             \\
antmaze-l-d  & 35.3 & 0.0                        &  \colorbox{mine}{55.2}     & 0.56             & 50.2  & 0.70             & 39.5  & 0.75             & 18.3  & 0.91             & 20.5  & 0.88             & 4.1   & 0.98             \\
antmaze-l-p  & 0    & 0.48                     & 38.3  & 0.33             &  \colorbox{mine}{52.3}     & 0.51             & 18.2  & 0.70             & 20.3  & 0.89             & 11.5  & 0.80             & 2.1   & 0.95             \\ 
\hline
antmaze-mean & 18.9 & 0.13                     & 56.6  & 0.40             &  \colorbox{mine}{64.5}     & 0.79             & 48.5  & 0.74             & 42.4  & 0.86             & 40.7  & 0.80             & 7.5   & 0.94             \\
\bottomrule
\end{tabular}}

\end{table}


\subsection{Sparsity in the Tabular Setting}

In Appendix \ref{stat_view}, we show the comparison of loss's derivative of
SQL and IQL, we found that SQL keeps the derivative unchanged when the residual is below a threshold while IQL doesn't, the $V$-function in IQL will be over-underestimated by those bad actions whose $Q$-value is small.

To justify this claim, we use the Four Rooms environment, where the agent starts from the bottom-left and needs to navigate through the four rooms to reach the goal in the up-right corner in as few steps as possible. There are four actions: $\mathcal{A} = \{up, down, right, left\}$. The reward is zero on each time step until the agent reaches the goal-state where it receives +10.
The offline dataset is collected by a \textbf{random} behavior policy which takes each action with equal probability.
We collect 30 trajectories and each trajectory is terminated after 20 steps if not succeed, $\gamma$ is 0.9. 

It can be seen from \Cref{fig: toy_case} that the value learning in IQL is corrupted by those suboptimal actions in this dataset. Those suboptimal actions prevent IQL from propagating correct learning signals from the goal location to the start location, resulting underestimated $V$-values and some mistaken $Q$-values. Particularly, incorrect $Q$-values at $(1, 1)$ and $(5, 9)$ make the agent fail to reach the goal. While in SQL, $V$-values and $Q$-values are more identical to the true optimal ones and the agent succeeds in reaching the goal location. This reveals that the sparsity term in SQL helps to alleviate the effect of bad dataset actions and learn a more optimal value function.






\subsection{Sparsity in the Continuous Action Setting}

The value of non-sparsity ratio (i.e., $\mathbb{E}_{(s, a) \sim \mathcal{D}}[\mathds{1} (1+ (Q(s, a) - V(s))/2\alpha >0 )]$) is controlled by the hyperparameter $\alpha$. 
In the continuous action setting, we show the relationship of the normalized score and non-sparsity ratio with $\alpha$ in \Cref{tab:sql_alpha_mujoco} and \Cref{tab:sql_alpha_antmaze}. It can be seen that typically a larger $\alpha$ gives less sparsity, sparsity plays an important role in the performance of SQL and we need to choose a proper sparsity ratio to achieve the best performance. The best sparsity ratio depends on the composition of the dataset, for example, the best sparsity ratios in MuJoCo datasets (around 0.1) are always larger than those in AntMaze datasets (around 0.4), this is because AntMaze datasets are kind of multi-task datasets (the start and goal location are different from the current ones), there is a large portion of useless transitions contained so it is reasonable to give those transitions zero weights by using more sparsity.




\section{Proofs}
\label{proofs}

\subsection{Proof of Theorem \ref{theorem: 2}}
\label{proof: 2}

In this section, we give the detailed proof for Theorem \ref{theorem: 2}, which states the optimality condition of the behavior regularized MDP. 
The proof follows from the Karush-Kuhn-Tucker (KKT) conditions where the derivative of a Lagrangian objective function with respect to policy $\pi(a | s)$ is set zero. 
Hence, our main theory is necessary and sufficient.

\begin{proof}
The Lagrangian function of (\ref{eq_reg_rl}) is written as follows
\begin{align*}
L(\pi, \beta, u)=& \sum_s d^\pi(s) \sum_a \pi(a | s)\left(Q(s, a) - \alpha f\left(\frac{\pi(a | s)}{\mu(a | s)}\right) \right) \\
&-\sum_s d^\pi(s)\left[u(s)\left(\sum_a \pi(a | s)-1\right)-\sum_a \beta(a | s) \pi(a | s)\right],
\end{align*}
where $d^{\pi}$ is the stationary state distribution of the policy $\pi$, $u$ and $\beta$ are Lagrangian multipliers for the equality and inequality constraints respectively. 

Let $h_f(x)=x f(x)$. Then the KKT condition of (\ref{eq_reg_rl}) are as follows, for all states and actions we have
\begin{align}
&0 \leq \pi(a | s) \leq 1 \text { and } \sum_a \pi(a | s)=1 \label{kkt_1} \\
&0 \leq \beta(a | s) \label{kkt_2} \\
&\beta(a | s) \pi(a | s)=0 \label{kkt_3} \\
&Q(s, a) - \alpha h_f^{\prime}\left(\frac{\pi(a | s)}{\mu(a | s)} \right) - u(s) + \beta(a | s)=0 \label{kkt_4}
\end{align}
where (\ref{kkt_1}) is the feasibility of the primal problem, (\ref{kkt_2}) is the feasibility of the dual problem, (\ref{kkt_3}) results from the complementary slackness and (\ref{kkt_4}) is the stationarity condition. 
We eliminate $d^\pi(s)$ since we assume all policies induce an irreducible Markov chain.

From (\ref{kkt_4}), we can resolve $\pi(a | s)$ as
$$
\pi(a | s)= \mu(a|s) \cdot g_f \left(\frac{1}{\alpha}\left(Q(s, a) -u(s) +\beta(a | s)\right)\right)
$$

Fix a state $s$. For any positive action, its corresponding Lagrangian multiplier $\beta(a | s)$ is zero due to the complementary slackness and $Q(s, a)>u(s) + \alpha h_f^{\prime}(0)$ must hold. 
For any zero-probability action, its Lagrangian multiplier $\beta(a | s)$ will be set such that $\pi(a | s)=0$. 
Note that $\beta(a | s) \geq 0$, thus $Q(s, a) \leq u(s) + \alpha h_f^{\prime}(0)$ must hold in this case. From these observations, $\pi(a | s)$ can be reformulated as
\begin{equation}
\label{eq_pi_ref}
\pi(a | s)=\mu(a | s) \cdot \max \left\{g_f \left(\frac{1}{\alpha}\left(Q(s, a) - u(s)\right)\right), 0\right\}
\end{equation}

By plugging (\ref{eq_pi_ref}) into (\ref{kkt_1}), we can obtain an new equation
\begin{equation}
\label{eq_pi_sum}
\mathbb{E}_{a \sim \mu} \left[ \max \left\{g_f \left(\frac{1}{\alpha}\left(Q(s, a) - u(s)\right)\right), 0\right\} \right] = 1
\end{equation}

Note that (\ref{eq_pi_sum}) has and only has one solution denoted as $u^{*}$ (because the LHS of (\ref{eq_pi_sum}) can be seen as a continuous and monotonic function of $u$), so $u^{*}$ can be solved uniquely. We denote the corresponding policy $\pi$ as $\pi^*$.

Next we aim to obtain the optimal state value $V^*$. It follows that
$$
\begin{aligned}
&V^*(s)=\mathcal{T}_f^* V^*(s) \\
&=\sum_a \pi^*(a | s)\left(Q^*(s, a)-\alpha f\left(\frac{\pi^*(a | s)}{\mu(a | s)}\right)\right) \\
&=\sum_a \pi^*(a | s)\left(u^*(s)+\alpha \frac{\pi^*(a | s)}{\mu(a | s)} f^{\prime}\left(\frac{\pi^*(a | s)}{\mu(a | s)}\right)\right) \\
&=u^*(s)+\alpha \sum_a \frac{\pi^*(a | s)^2}{\mu(a | s)} f^{\prime}\left(\frac{\pi^*(a | s)}{\mu(a | s)}\right) \\
&=u^*(s)+\alpha \mathbb{E}_{a \sim \mu}\left[\left(\frac{\pi^*(a | s)}{\mu(a | s)}\right)^2 f^{\prime}\left(\frac{\pi^*(a | s)}{\mu(a | s)}\right)\right]
\end{aligned}
$$

The first equality follows from the definition of the optimal state value. The second equality holds because $\pi$ maximizes $\mathcal{T}^{*}_f V^*(s)$. The third equality results from plugging (\ref{kkt_4}).

To summarize, we obtain the optimality condition of the behavior regularized MDP as follows
\begin{equation*}
Q^*(s, a) =r(s, a)+\gamma \mathbb{E}_{s^{\prime} | s, a} \left[ V^*\left(s^{\prime}\right) \right] 
\end{equation*}
\begin{equation*}
\pi^*(a | s) = \mu(a|s) \cdot \max \bigg\{g_f \Big(\frac{Q^*(s, a)-u^*(s)}{\alpha}\Big), 0\bigg\}
\end{equation*}
\begin{equation*}
V^*(s) =u^*(s)+\alpha \mathbb{E}_{a \sim \mu} \bigg[ \Big( \frac{\pi^*(a | s)}{\mu(a | s)}\Big)^2 f^{\prime}\Big(\frac{\pi^*(a | s)}{\mu(a | s)}\Big) \bigg]
\end{equation*}

\end{proof}



\subsection{Proof of Theorem \ref{theorem: 1}}
\label{proof: 1}
\begin{proof}
For any two state value functions $V_1$ and $V_2$ , let $\pi_i$ be the policy that maximizes $\mathcal{T}_f^* V_i$, $i \in {1, 2}$. Then it follows that for any state s in $\mathcal{S}$,
$$
\begin{aligned}
&\left(\mathcal{T}_f^* V_1\right)(s)-\left(\mathcal{T}_f^* V_2\right)(s) \\
&=\sum_a \pi_1(a | s)\left[r+\gamma \mathbb{E}_{s^{\prime}} \left[ V_1\left(s^{\prime}\right) \right] -\alpha f\left(\frac{\pi_1(a | s)}{\mu(a|s)} \right)\right]-\max _\pi \sum_a \pi(a | s)\left[r+\gamma \mathbb{E}_{s^{\prime}} \left[ V_2\left(s^{\prime}\right) \right] - \alpha f\left(\frac{\pi(a | s)}{\mu(a|s)} \right) \right] \\
&\leq \sum_a \pi_1(a | s)\left[r+\gamma \mathbb{E}_{s^{\prime}} \left[ V_1\left(s^{\prime}\right) \right] -\alpha f\left(\frac{\pi_1(a | s)}{\mu(a|s)} \right) \right]-\sum_a \pi_1(a | s)\left[r+\gamma \mathbb{E}_{s^{\prime}} \left[ V_2\left(s^{\prime}\right) \right]  -\alpha f\left(\frac{\pi_1(a | s)}{\mu(a|s)} \right) \right] \\
&=\gamma \sum_a \pi_1(a | s) \mathbb{E}_{s^{\prime}}  \left[V_1\left(s^{\prime}\right)-V_2\left(s^{\prime}\right)\right] \leq \gamma\left\|V_1-V_2\right\|_{\infty}
\end{aligned}
$$

By symmetry, it follows that for any state $s$ in $\mathcal{S}$,
$$
\left(\mathcal{T}_f^* V_1\right)(s)-\left(\mathcal{T}_f^* V_2\right)(s) \leq \gamma \left\|V_1-V_2\right\|_{\infty}
$$
Therefore, it follows that
$$
\left\|\mathcal{T}_f^* V_1-\mathcal{T}_f^* V_2\right\|_{\infty} \leq \gamma\left\|V_1-V_2\right\|_{\infty}
$$

\end{proof}


\section{Experimental Details}
\label{appendix: exp}
\textbf{More about the policy extraction part} \quad 
SQL and EQL extract the policy by Eq.(\ref{pi_sql}) and Eq.(\ref{pi_eql}), respectively. However, in practice, we found the scale of $\alpha$ does affect the performance as the behavior cloning weight involves $(Q-V) / \alpha$. To make the residual less affected by the scale of $\alpha$, we multiply a constant value to the residual, this can be seen as a kind of reward normalization and doesn't change the optimal policy. In EQL, we multiply 10 to the residual term and extract the policy by $\max_{\pi} \ \mathbb{E}_{(s, a) \sim \mathcal{D}}\left[ \exp \left(10 \cdot \left(Q(s, a) - V(s) \right) / \alpha \right) \log \pi(a|s)\right]$. In SQL, we remove the "1+" term in Eq.(\ref{pi_sql}) (this amounts to multiply a large value to the residual term) and extract the policy by $\max_{\pi} \ \mathbb{E}_{(s, a) \sim \mathcal{D}}\left[ \mathds{1} \left(Q(s, a) - V(s) >0 \right) \left(Q(s, a) - V(s) \right) \log \pi(a|s)\right]$. Note that XQL also applies a similar trick, it extracts the policy by using temperature $\beta$ (3.0 for MuJoCo tasks and 10.0 for AntMaze tasks) instead of $\alpha$.

\textbf{D4RL experimental details} \quad 
For MuJoCo locomotion and Kitchen tasks, we average mean returns over 10 evaluations every 5000 training steps, over 5 seeds. 
For AntMaze tasks, we average over 100 evaluations every 0.1M training steps, over 5 seeds.
Followed by IQL, we standardize the rewards by dividing the difference in returns of the best and worst trajectories in MuJoCo and Kitchen tasks, we subtract 1 to rewards in AntMaze tasks. 

In SQL and EQL, we use 2-layer MLP with 256 hidden units, we use Adam optimizer \citep{Kingma2014-az} with a learning rate of $3 \cdot 10 ^{-4}$ for MuJoCo and Kitchen tasks, and a learning rate of $2 \cdot 10 ^{-4}$ for AntMaze tasks. We use a target network with soft update weight $5 \cdot 10 ^{-3}$ for $Q$.
% We implement our method in the framework of JAX. 
The only hyperparameter $\alpha$ used in SQL and EQL is listed in \Cref{tab:sql_alpha}. 
The sensitivity of $\alpha$ in SQL can be found in \Cref{tab:sql_alpha_mujoco} and \Cref{tab:sql_alpha_antmaze}.
In EQL, to avoid numerical instability, we use the commonly used exp-normalize trick to compute the loss of $V$ and clip the loss input of $V$ by 5.0.

% Our implementation of 10\%BC is as follows, we first filter the top 10 $\%$ trajectories in terms of the trajectory return, and then run behaviour cloning on those filtered data. 
We re-run IQL on all datasets and report the score of IQL by choosing the best score from $\tau$ in $[0.5, 0.6, 0.7, 0.8, 0.9, 0.99]$, using author-provided implementation. 
The sensitivity of $\tau$ in IQL can be found in \Cref{tab:sql_alpha_mujoco}.
We re-run CQL on AntMaze datasets as we find the performance can be improved by carefully sweeping the hyperparameter \texttt{min-q-weight} in $[0.5, 1, 2, 5, 10]$, using a PyTorch-version implementation\footnote{\url{https://github.com/young-geng/CQL}}.
Other baseline results are taken directly from their corresponding papers.
The runtime of different algorithms can be found in \Cref{tab:run_time}.

\begin{table}[h]
\centering
\caption{$\alpha$ used for SQL and EQL}
\label{tab:sql_alpha}
\resizebox{0.45\linewidth}{!}{
\begin{tabular}{l||rr} 
\toprule
Env                          & $\alpha$ (SQL) & $\alpha$ (EQL) \\ 
\hline
halfcheetah-medium-v2        & 1.0  & 2.0    \\
hopper-medium-v2             & 1.0  & 2.0    \\
walker2d-medium-v2           & 1.0  & 2.0    \\
halfcheetah-medium-replay-v2 & 1.0  & 2.0    \\
hopper-medium-replay-v2      & 1.0  & 2.0    \\
walker2d-medium-replay-v2    & 1.0  & 2.0    \\
halfcheetah-medium-expert-v2 & 1.0  & 2.0    \\
hopper-medium-expert-v2      & 1.0  & 2.0    \\
walker2d-medium-expert-v2    & 1.0  & 2.0    \\
\hline
antmaze-umaze-v2             & 0.1 & 0.5   \\
antmaze-umaze-diverse-v2     & 3.0 & 5.0  \\
antmaze-medium-play-v2       & 0.1 & 0.5   \\
antmaze-medium-diverse-v2    & 0.1 & 0.5   \\
antmaze-large-play-v2        & 0.1 & 0.5   \\
antmaze-large-diverse-v2     & 0.1 & 0.5   \\
\hline
kitchen-complete-v0 & 0.5  & 2.0  \\
kitchen-partial-v0 & 0.5  & 2.0  \\
kitchen-mixed-v0 & 0.5  & 2.0  \\
\bottomrule
\end{tabular}
}
\end{table}


\begin{table}[h]
\centering
\caption{The sensitivity of $\tau$ in IQL.}
\label{tab:iql_tau}
\resizebox{0.55\textwidth}{!}{
\begin{tabular}{l||rrrrrrrrr} 
\toprule
$\tau$        & 0.5    & 0.6            & 0.7            & 0.8    & 0.9             & 0.99  \\ 
\hline
hopper-m-r   & 57.1   & 71.3           & \colorbox{mine}{95.2}  & 74.4   & 59.5            & 2.8   \\
hopper-m-e   & 99.7   & \colorbox{mine}{101.1} & 94.5           & 22.5   & 13.5            & 30.4  \\
walker2d-m-r & 74.3   & \colorbox{mine}{76.1}  & 72.3           & 41.7   & 20.3             & 4.3   \\
walker2d-m-e & 109.94 & 106.7          & \colorbox{mine}{109.6} & 109.3  & 78.2           & 50.3  \\
antmaze-m-d  & 0      & 0              & 2.5               & 51.6   & \colorbox{mine}{71.0}   & 12.1  \\
antmaze-m-p  & 0      & 0              & 8.0              & 51.2     & \colorbox{mine}{72.1}   & 11.5  \\
antmaze-l-d  & 0      & 0              & 1.2              & 12.4     & \colorbox{mine}{47.5}   & 7.6   \\
antmaze-l-p  & 0      & 0              & 1.3               & 10.4   & \colorbox{mine}{39.6}  & 5.3   \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[h]
\centering
\caption{The runtime of different algorithms.}
\label{tab:run_time}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{l||llllllllll} 
\toprule
algorithms & BC                      & 10\%BC                  & BCQ                     & DT                       & One-step                & TD3+BC                  & CQL                     & IQL                     & SQL         & EQL       \\ 
\hline
runtime   & \multicolumn{1}{r}{20m} & \multicolumn{1}{r}{20m} & \multicolumn{1}{r}{60m} & \multicolumn{1}{r}{950m} & \multicolumn{1}{r}{20m} & \multicolumn{1}{r}{25m} & \multicolumn{1}{r}{80m} & \multicolumn{1}{r}{20m} & \multicolumn{1}{r}{20m}  & \multicolumn{1}{r}{20m} \\
\bottomrule
\end{tabular}}
\end{table}

\textbf{RL Unplugged experimental details} \quad 
We use d3rlpy \citep{seno2021d3rlpy}, a modularized offline RL library that contains several SOTA offline RL algorithms and provides an easy-to-use wrapper for the offline Atari datasets introduced in \citep{agarwal2020optimistic}.
There are three types of Atari datasets in d3rlpy: \texttt{mixed}: datasets collected at the first 1M training steps of an online DQN agent, \texttt{medium}: datasets collected at between 9M steps and 10M training steps of an online DQN agent, \texttt{expert}: datasets collected at the last 1M training steps of an online DQN agent. To make the task more challenging, we use only 10\% or 5\% of origin datasets. We choose three image-based Atari games: Breakout, Qbert and Seaquest.

We implement the discrete version of SQL (D-SQL) and IQL (D-IQL) based on d3rlpy. The implementation of discrete CQL (D-CQL) and discrete BCQ (D-BCQ) are directly taken from d3rlpy. We use consistent preprocessing and network structures to ensure a fair comparision.
We report the score of D-IQL by choosing the best score from $\tau$ in $[0.5, 0.7, 0.9]$, we report the score of D-CQL by choosing the best score from \texttt{min-q-weight} in $[1, 2, 5]$, we report the score of D-BCQ by choosing the best score from $\tau$ in $[0.1, 0.3, 0.5]$. For D-SQL, we use $\alpha=1.0$ for all datasets.


\textbf{Nosiy data regime experimental details} \quad In this experiment setting, we introduce the \texttt{noisy} dataset by mixing the \texttt{expert} and \texttt{random} dataset with different expert using MuJoCo locomotion datasets. The number of total transitions of the noisy dataset is $100,000$. We provide details in Table \ref{tab:noisy_info}. We report the score of IQL by choosing the best score from $\tau$ in $[0.5, 0.6, 0.7, 0.8, 0.9]$. 

% A mixed dataset can be constructed by first loading an expert dataset and then replacing the final 10 percent transitions with tuples from random trajectories (the first 10 percent in the corresponding random dataset). We provide statistics of the mixed D4RL MuJoCo datasets used in our evaluations:

% \usepackage{multirow}


\begin{table}[htbp] \centering
% \begin{threeparttable}
% \centering
\caption{Noisy dataset of MuJoCo locomotion tasks with different expert ratios.}
\label{tab:noisy_info}
% \resizebox{0.9\textwidth}{!}{
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{c||crrr}
\toprule
Env                          & Expert ratio  & \multicolumn{1}{c}{Total transitions} & \multicolumn{1}{c}{Expert transitions} & \multicolumn{1}{c}{Random transitions}  \\ 
\hline
\multirow{5}{*}{Walker2d}    & 1\%               & 100,000                               & 1,000                                  & 99,000                                  \\
                             & 5\%               & 100,000                               & 5,000                                  & 95,000                                  \\
                             & 10\%              & 100,000                               & 10,000                                 & 90,000                                  \\
                             & 20\%              & 100,000                               & 20,000                                 & 80,000                                  \\
                             & 30\%              & 100,000                               & 30,000                                 & 70,000                                  \\ 
\hline
\multirow{5}{*}{Halfcheetah} & 1\%               & 100,000                               & 1,000                                  & 99,000                                  \\
                             & 5\%               & 100,000                               & 5,000                                  & 95,000                                  \\
                             & 10\%              & 100,000                               & 10,000                                 & 90,000                                  \\
                             & 20\%              & 100,000                               & 20,000                                 & 80,000                                  \\
\bottomrule                            
\end{tabular}
}
% \end{threeparttable}
\end{table}

\textbf{Small data regime experimental details} \quad 
% In this experiment setting, we introduce the small data regime in AntMaze tasks with different levels. It is worth noting that our way to generate the small dataset goes as far as possible to produce data in a form that matches the real-world setting, instead of randomly reducing the amount of data. 
We generate the small dataset using the following psedocode \ref{lst:code}, its hardness level can be found at Table \ref{tab:small data detail}. 
We report the score of CQL by choosing the best score from \texttt{min-q-weight} in $[0.5, 1, 2, 5, 10]$.

\begin{lstlisting}[label={lst:code}, language=Python, caption={The sketch of generation procedure of small data regimes with different hard levels. Given an AntMaze environment and a hardness level, we discard some transitions by following the rule in the Coding List. Intuitively, the closer the transition is to the \texttt{GOAL}, the higher the probability that it will be discarded.}]
# LEVEL = {'easy', 'medium', 'hard'}
obs = dataset['observations']
length = dataset['observations'].shape[0]
POSITIONS = env.get_position(obs)
GOAL = env.get_goal()
MINIMAL_POSITION = env.get_minimal_position()
# get maximal Euclidean distance
MAX_EU_DIS = (GOAL - MINIMAL_POSITION)**2
DIS = ((POSITIONS - MINIMAL_POSITION)**2) / MAX_EU_DIS
save_idx = np.random.random(size=length) > (DIS * hardness['LEVEL'])
small_data = collections.defaultdict()
for key in dataset.keys():
    small_data[key] = dataset[key][save_idx]
\end{lstlisting}

\begin{table}[htbp]\centering
% \begin{threeparttable}
% \centering
\caption{Details of small data regimes with different task difficulties.}
\label{tab:small data detail}
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{cl||rrr} 
\toprule
\multicolumn{2}{l||}{Dataset (AntMaze)} & \multicolumn{1}{l}{Hardness} & \multicolumn{1}{l}{Total transitions} & \multicolumn{1}{l}{Reward signals}  \\ 
\hline
\multirow{4}{*}{medium-play} & Vanilla  & NA                           & 100, 000                              & 10,000                              \\
                             & Easy     & 0                            & 56,000                                & 800                                 \\
                             & Medium   & 0.07                         & 48,000                                & 150                                 \\
                             & Hard     & 0.1                          & 45,000                                & 10                                  \\ 
\hline
\multirow{5}{*}{large-play}  & Vanilla  & NA                           & 100,000                               & 12500                               \\
                             & Easy     & 0                            & 72,000                                & 5,000                               \\
                             & Medium   & 0.3                          & 42,000                                & 1,000                               \\
                             & Medium   & 0.35                         & 37,000                                & 500                                 \\
                             & Hard     & 0.38                         & 35,000                                & 100                                 \\
\bottomrule
\end{tabular}}
% \end{threeparttable}
\end{table}


\begin{table}
\centering
\caption{Performance in setting with 10\% (top) and 5\% (bottom) Atari dataset. SQL achieves the best performance in 10 out of 12 games.}
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{l||rrr|r} 
\toprule
Task           & \multicolumn{1}{c}{D-BCQ} & \multicolumn{1}{c}{D-IQL} & \multicolumn{1}{c|}{D-CQL} & \multicolumn{1}{c}{D-SQL}  \\ 
\hline
breakout-medium-v0 ($10$\%) & 3.5                     & 20.1                    & 15.1                     & \colorbox{mine}{28.0}                            \\
qbert-medium-v0    & 395.4                   & 3717.5                  & 4141.5                   & \colorbox{mine}{5213.4}                          \\
seaquest-medium-v0 &   438.1                     & 404.9                   & 359.4                    & \colorbox{mine}{465.3}                           \\
breakout-mixed-v0  & 8.1                     & 10.9                    & 9.3                     & \colorbox{mine}{13.3}                            \\
qbert-mixed-v0     & 557.5                   & 1000.3                  & 890.2                   & \colorbox{mine}{1244.3}                          \\
seaquest-mixed-v0  & 300.3                   & 326.1                & \colorbox{mine}{337.5}                    & 330.4                           \\ 
\hline
breakout-medium-v0 ($5$\%)     &   1.6                 &    13.9          &    13.3                 &      \colorbox{mine}{16.5}                                \\
qbert-medium-v0        &         301.6           &    2788.7          &           \colorbox{mine}{3147.3}          &        2970.6                              \\
seaquest-medium-v0      &        301.9            &    294.5          &      272.5               &       \colorbox{mine}{361.3}                             \\
breakout-mixed-v0     &            5.3        &          10.5    &           8.7          &       \colorbox{mine}{11.8}                              \\
qbert-mixed-v0    &      576.4              &   931.1           &     925.3                &          \colorbox{mine}{965.0}                               \\
seaquest-mixed-v0 &          275.5          &     292.2         &      321.4               &       \colorbox{mine}{336.7 }          \\ 
\bottomrule
\end{tabular}}
\end{table}


% \textbf{Learning curves} \quad
% We present learning curves of SQL D4RL benchmarks in the following. 

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\columnwidth]{figs/ivr_d4rl_mujoco.pdf}
\caption{Learning curves of SQL and EQL on D4RL MuJoCo locomotion datasets.}
\label{fig: learning curve locomotion}
\end{figure}


\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\columnwidth]{figs/ivr_d4rl_antmaze.pdf}
\caption{Learning curves of SQL and EQL on D4RL AntMaze datasets.}
\label{fig: learning curve antmaze}
\end{figure}


\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\columnwidth]{figs/ivr_d4rl_kitchen.pdf}
\caption{Learning curves of SQL and EQL on D4RL Kitchen datasets.}
\label{fig: learning curve kitchen}
\end{figure}


\begin{figure}[tbp]
\centering
\resizebox{1\linewidth}{!}{
		\centering
			\begin{minipage}[b]{0.45\textwidth}
				\includegraphics[width=1\textwidth]{figs/rliable_runs.jpg}
			\end{minipage}
			\begin{minipage}[b]{0.45\textwidth}
				\includegraphics[width=1\textwidth]{figs/rliable_tasks.jpg}
			\end{minipage}
		}
\caption{Performance proï¬les of CQL, IQL and SQL generated by the rliable library \citep{agarwal2021deep} on D4RL datasets based on score distributions (left), and average score distributions (right). Shaded regions show pointwise 95\% conï¬dence bands based on percentile bootstrap with stratiï¬ed sampling.}
\label{fig: rliable_mujoco}
\end{figure}

% \usepackage{booktabs}




\end{document}



