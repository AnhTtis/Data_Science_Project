\section{Feature Selection}
\label{sec:feature_selection}
With the proposed algorithm achieving the desired fnr, fpr and minimum escape time across all folds, the next step is to try and improve its performance by selecting better features.  Feature selection is important as the wrong choice of features can decrease the performance of the algorithm. For instance,  using the features shown in Equation \ref{eq:features_f1_slow}

\begin{equation}
    \begin{bmatrix}
        \text{\rm knee~angle} \\
        \text{\rm hip~angle} \\
        \text{\rm vel~hip~angle} \\
        (L_{cop} + L_{com})* sgn(p_{com}^x- p_{f_\mid}^x) 
    \end{bmatrix}
    \label{eq:features_f1_slow}
\end{equation}

where $L_{cop}$ is the angular momentum about the contact point to train the SVM model on just the incipient faults with 0.9 training escape time results in 0 false positive and negative rates and an average escape time of 0.4, 0.37 and 0.38 for the training, validation, and testing sets. Whereas using the features shown in Equation \ref{eq:features_0.9_corr}
\begin{equation}
    \begin{bmatrix}
        L_{com}\\
        \text{\rm knee~angle} \\
        \text{\rm hip~angle} \\
        \text{\rm vel~knee~angle} \\
        \text{\rm vel~hip~angle} \\
        \text{\rm vel~torso} \\
        p_{com}^x - p_{cop}^x \\
        p_{com}^z- p_{f_\mid}^z
    \end{bmatrix}
    \label{eq:features_f1_slow}
\end{equation}

also results in 0 false positive and negative rates but only 0.2,0.16 and 0.18 average escape times for training, validation, and testing data. The goal of feature selection is to select features based on their redudancy and relevance. Feature selection algorithms are typically split into 3 methods, wrapper, filter and embedded methods. Wrapper methods use greedy search algorithms to add or remove features based on the chosen model's performance. Filter methods use statistical scores such as Pearson's correlation to select features. In embedded methods, feature selection is built in the chosen model. \eva{cite the following papers https://sciendo.com/article/10.2478/cait-2019-0001, https://www.frontiersin.org/articles/10.3389/fbinf.2022.927312/}. Here, we try to select features using the wrapper and filter methods. Since we are using the radial basis function kernel with our SVM classifiers, we can't explicitly compute the weight vector which would aid in determining the relevant features. \eva{cite this paper https://link.springer.com/article/10.1007/s10462-011-9205-2 } Note that there are researchers such as \eva{cite this paper https://link.springer.com/article/10.1007/s10462-011-9205-2 } who have put forward methods for feature selection for SVM with RBF kernel, however, as we are using sklearn's SVM implementation, we choose to forego embedded feature selection with the SVM classifier.  For the wrapper method we use the forward and backward sequential feature selection. The features are selected from commonly used features in literature, and are displayed in Equation \ref{eq:features_raw}. 

\begin{equation}
    \begin{bmatrix}
        L_{cop} - L_{com}\\
        L_{com} \\
        L_{cop} \\
        p_{cop}^x\\
        p_{com} \\
        v_{com} \\
        p_{com} - p_{heel} \\
        p_{toe} - p_{com}\\
        q \\
        \dot{q}\\
        p_{com} - p_{cop} \\
        (L_{cop} + L_{com})* sgn(p_{com}^x- p_{f\_mid}^x) \\
        p_{f\_mid} \\
        torso \\
        dtorso        
    \end{bmatrix}
    \label{eq:features_raw}
\end{equation}


In keeping up with the example from the beginning of this section, the SVM is trained on the selected features using the incipient and abrupt faults individually with 0.9 and 0.6 training escape times respectively. The training escape times are chosen from the first fold results in Figure \ref{fig:classifier_results} \eva{cite table when it becomes available}. 

\subsection{Distance Correlation Feature Selection}
The features are selected such that they have a correlation of greater than 0 with the escape time and less than 1 with each other. To reduce the number of features selected, the correlation thresholds can be increased and decreased respectively. The second requirement of non redundancy requires one to make a design choice between features. As a result, there are multiple features which can be derived from the same conditions. However, the performance of these resulting features can differ as can be seen in Figure \ref{fig:features_distance_corr}. As a result, this feature selection method does not give a clear cut way of selecting the features that can distinguish between our two classes. Therefore, we don't proceed to use this method with the abrupt fault trajectories. 


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/fall_detection/feature_selection/distance_corr_features.png}
    \caption{Resulting Features from Distance Correlation Feature Selection}
    \label{fig:features_distance_corr}
\end{figure}


\subsection{Sequential Feature Selection}
The sequential feature selection from sklearn is used to select the features. We got the best results by using forward featurer selection with a training escape time of 2. These features are displayed in Equation \ref{eq:features_f1_slow}. \eva{confirm how these features were found} However, when the training escape time is decreased to 0.9, the resulting features don't perform achieve as high of escape time with 0 false positive and negative rates. The results are displayed in Figure \ref{fig:features_seq_feat}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/fall_detection/feature_selection/seq_feat_selection_features.png}
    \caption{Resulting Features from Sequential Feature Selection}
    \label{fig:features_seq_feat}
\end{figure}

Even though sequential feature selection is able to find features with a higher escape time that still achieve 0 false positive and negative rate, it is time consuming. 





\section{Deep Learning}

Given the fact that feature selection is important, sequential feature selection is time consuming, and our goal of applying fall detection to more complex robots such as digit, we propose using deep neural networks such as a 1D CNN. Deep learning is capable of abstracting features from raw data. Here we take a look at the performance of a  multilayer perceptron (MLP) and 1D CNN on the fall detection data of the planar 4 link robot. The MLP and 1D CNN models used are displayed in Figure \ref{fig:mlp_model} and Figure \ref{fig:cnn_model}. Given that the fall detection problem for the planar four link robot is not complex, the neural networks perform equivalently or better than the SVM. These results are displayed in Figure   \ref{fig:neural_net_deep}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/fall_detection/feature_selection/mlp_model}
    \caption{Results from CNN and MLP}
    \label{fig:mlp_model}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/fall_detection/feature_selection/cnn_model}
    \caption{Results from CNN and MLP}
    \label{fig:cnn_model}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/fall_detection/feature_selection/cnn_mlp_performance.png}
    \caption{Results from CNN and MLP}
    \label{fig:neural_net_deep}
\end{figure}

