
\section{Fall Detection Methods}
As a baseline, we use the SVM-based classification algorithm of \cite{ref:scikit-learn}, while the nearest-neighbor algorithm is based on the Ward minimum variance method \cite{ref:ward1963hierarchical}. To prioritize recent data points over previous ones, both methods make use of sliding windows. The number of data points in a window is referred to as $N_{window}$. It is important to note that both methods are supervised algorithms.

\subsection{SVM Classifier}

%
The radial basis function is chosen as the kernel and the soft margin formulation is implemented for the SVM classifier. The training data for the classifier is defined as 
\begin{align*}
D&=\{X_i,y_i\}_{i=1}^n \\
&~~\text{where}  \\
&\begin{array}{rcl}
    n &=& \text{number of windows across all training data}\\
    m &=& \text{number of time steps in a window}\\
    x_{ij}&=&\text{features at time step j in window i} \\
        X_i &=& \begin{bmatrix} x_{i1} && x_{i2} && \cdots && x_{im}  \end{bmatrix}^\intercal \\
    y_i &\in&\  \left\{
    \begin{array}{rcl}
           -1  &\quad& X_i ~\in ~\text{faulty trajectory} \\
            1  &\quad& X_i ~\in ~\text{normal trajectory}
    \end{array}
    \right\}
    \end{array}
\end{align*}


\subsection{Nearest-Neighbor Algorithm}
\label{sec:classifier_intro}
 The selected nearest-neighbor algorithm determines distance using the Ward minimum variance and a weighted Euclidean distance. Given two clusters A and B, the Ward minimum variance method calculates the effort, $E_{AB}$, it takes to join the two clusters together, as determined by the sum of squared errors, specifically,  
 \begin{equation}
    \label{eq:ward_method}
    E_{AB} = SSE_{AB} - SSE_{A} - SSE_{B},
    \end{equation} 
 where
\begin{align*}
    &SSE_{AB} =(A\cup B- \mu_{A\cup B})^\intercal R^{-1}_{A\cup B}(A\cup B- \mu_{A\cup B})\\
    &SSE_{A} = (A- \mu_{A})^\intercal R^{-1}_{A}(A- \mu_{A})\\
    &SSE_{B} =  (B- \mu_{B})^\intercal R^{-1}_{B}(B- \mu_{B})\\
    &R = \text{correlation coefficient matrix}\\
    &\mu = \text{mean vector}
\end{align*}


In our application, cluster B contains the features at the current time step while cluster A contains all the features in the previous time steps included in the window. Given, that B is a single data point, the Ward minimum variance simplifies to
\begin{equation}
    \label{eq:our_ward_method}
    E_{AB} = SSE_{AB} - SSE_{A}. 
\end{equation}

 The nearest-neighbor algorithm detects a potential fall if the effort it takes to join the two clusters A and B is higher than a threshold determined from the training data. The threshold is calculated offline as the maximum $E_{AB}$ value for the safe data while $R$ is determined using distance correlation. Note that the underlying assumption for the nearest-neighbor algorithm is that cluster A only contains safe data points. 

\subsection{Data Labeling}
As one of our objectives is to maximize the lead time, we propose the use of a training lead time to label windows in a trajectory. Training lead time is defined as the difference between the time of the actual fall and the time when a  sliding window of a trajectory can be labeled as faulty. Therefore, training lead time is a subset of the maximum lead time that can be achieved in a faulty trajectory. While labeling all windows in a faulty trajectory as faulty would achieve the maximum lead time, it would also increase the rate of false positives. For instance, given two faults that are close in magnitude but where one results in a fall and the other is safe, the safe trajectory could be mistaken as a faulty trajectory. 

 If a trajectory does not contain a fall, all windows derived from the trajectory are labeled as 1. If a trajectory ends in a fall, all windows containing data points after the desired training lead time are labeled as -1. Note that for an abrupt fault. the training lead time is only defined after the push is introduced, and only the data points before a fall are kept for the training data of both faults.

 The desired training lead time is determined by a grid search algorithm that trains the algorithm of interest using a range of training lead times from 0 to 2s and evaluates the results on the training and/or validation data. The training data is included in the evaluation process for cases where the algorithm is allowed to make mistakes, such as when using a soft margin in SVM. A training lead time of 2s would label all the data points in a faulty trajectory as faulty.   



\subsection{Performance of Fall Detection Methods} 
\label{sec:fault_comparison}
 % To determine how to split the training data along faults, an experiment is conducted with each of the proposed fall detection algorithms trained using just incipient faults, just abrupt faults, and both faults. The algorithms are evaluated on the validation data. The evaluation metrics are false positive and negative rate (fpr and \fnr respectively), and the maximum average F\_LT achieved across all the trajectories. The desired fpr and \fnr is set to 0,  $N_{window}=25$, and $N_{monitor} = 1$ . The value of $N_{window}$ is set based on evaluating the performance of the algorithms across various values. Note that for both algorithms, a low value for $N_{window}$ results in higher fpr while a high value can reduce the maximum average F\_LT. $N_{monitor} = 1$ because we found that for our dataset, $N_{monitor}$ had to be set to a large value, e.g. 100 in order to result in 0 fpr.

In this section, we analyze the performance of the proposed nearest-neighbor algorithm and the SVM classifier. The algorithms are trained and evaluated on testing data across all 5 folds using just abrupt trajectories, just incipient trajectories, and both trajectories together. The evaluation metrics are false positive and negative rates, and the average lead time achieved. The desired false positive and false negative rates are set to 0. The training lead time chosen is the maximum that meets the given bounds on the false positive and false negative rates when evaluated on the training and validation data. Based on previous experiments, we set the values of the remaining hyper-parameters as $N_{window}=10$ and $N_{monitor} = 1$. 

 From \ref{tab:fault_comp_clustering} and \ref{tab:fault_comp_classification} we see that the nearest-neighbor algorithm and the classification algorithm perform similarly when trained and evaluated on the abrupt and incipient faults separately. The nearest-neighbor algorithm achieves an average escape time of 0.46s and 0.91s, respectively, for the abrupt and incipient faults, while the classification algorithm achieves an average escape time of 0.48s and 0.97s. Because our sampling time is 0.03s, the difference in the performance of both algorithms is 1 and 2 data points for the abrupt and incipient fault respectively . 

 % Even though the nearest-neighbor algorithm performs similarly to the classification algorithm, it should be noted that it requires a sampling time 'large' enough to detect faulty data from normal data. For instance, with a sampling time of 0.01s, the nearest-neighbor algorithm did not attain good results even for abrupt faults. To overcome this limitation if finer sampling times are desired, we suggest having Cluster B be further removed from Cluster A in time. For instance we are able to achieve similar results for the nearest-neighbor algorithm with a sampling time of 0.01s and Cluster B being 0.08s away from the last data point in Cluster A. 

 When both faults are trained and evaluated together, the classification algorithm achieves an average lead time 0.15s higher in comparison to the nearest-neighbor algorithm. In comparison to its average performance on the abrupt and incipient fault, the classification algorithm achieves an average lead time of 0.08s less when trained on both faults together. Similarly, the nearest-neighbor algorithm achieves an average lead time of 0.19s less. As a result, the classification algorithm outperforms the nearest-neighbor algorithm when both faults are trained together. However, because both algorithms are able to achieve lead times higher than the 0.2s, which is the lead time reflexive algorithms such as \cite{ref:hohn2009probabilistic} and \cite{ref:wu2021falling} require, both algorithms are viable options. As the nearest-neighbor algorithm learns the safe/good model, it should be used when faulty data is sparse. 

 

 

 

 % \jwg{I do not know what this first sentence means.} To determine how to split the training data along faults, an experiment is conducted with each of the proposed fall detection algorithms trained using incipient faults only, abrupt faults only, and both faults. The algorithms are evaluated on the validation data. The evaluation metrics are false positive and negative rates (\fpr and \fnrNS, respectively), and the average F\_LT achieved across all the trajectories. The desired \fpr and \fnr are set to 0.0,  $N_{window}=25$, and $N_{monitor} = 1$ . The value of $N_{window}$ is set based on evaluating the performance of the algorithms across various values \jwg{values of what?}. Note that for both algorithms, a low value for $N_{window}$ results in higher \fpr while a high value can reduce the maximum average F\_LT. $N_{monitor} = 1$ because we found that for our dataset, $N_{monitor}$ had to be set to a large value, e.g. 100 in order to result in 0 \fpr. \jwg{Last sentence must have a typo because it cannot be set to 1 because it needs to be at least 100.}
 
% Both algorithms are able to attain a higher average F\_LT with zero \fpr and \fnr when trained on the faults individually, as shown in Table  \ref{tab:fault_comp_clustering} and \ref{tab:fault_comp_classification}. A reason for this is that the optimal \jwg{not sure what optimal means here.} training F\_LT varies per fault. For instance, the training F\_LT for the classifier during the first fold that achieves the maximum average F\_LT while meeting the desired \fpr and \fnr is  0.6 for abrupt faults (only), 2.0 for incipient faults (only), and 0.2 for both faults together. A similar phenomenon can be observed for the nearest-neighbor algorithm. 

% % The nearest-neighbor algorithm is able to detect incipient faults because when the robot starts to lose balance, the foot starts to rotate, and the length of the window is able to capture the rotation. If the foot rotation is not included in the features, the nearest-neighbor algorithm is no longer able to detect incipient faults. For instance, using the following features:

% The nearest-neighbor algorithm relies on foot rotation to detect incipient faults. If foot rotation is not included in the features, the nearest-neighbor algorithm is no longer able to detect incipient faults. For instance, the following features,
% \begin{equation}
%     \begin{bmatrix}
%         \text{\rm knee~angle} \\
%         \text{\rm hip~angle} \\
%         \text{\rm vel~hip~angle} \\
%         (L_{cop} + L_{com})* sgn(p_{com}^x- p_{f_\mid}^x), 
%     \end{bmatrix}
%     \label{eq:features_f1_slow}
% \end{equation}
% result in a \fpr of 1.0 regardless of the chosen training F\_LT. On the other hand, for the classifier, these features result in a training F\_LT of 0.37s, with \fpr and \fnr equal to zero. 
\subsection{Categorizing Faults}
A means to decrease the difference in performance for both algorithms when trained on both faults together vs separately is to implement a multi-class classification problem. The labels for this multi-class classification can be identified as: abrupt fault safe (AS), abrupt fault fall (AF), incipient fault safe (IS) and incipient fault fall (IF). Using these labels with the one-vs-one or one-vs-rest multi-class classification techniques typically implemented \cite{ref:bishop2006pattern}, results in six and four detectors, respectively. However, as one-vs-rest can result in ambiguities and class imbalances and using one-vs-one can result in ambiguities and higher computational times, we seek a different approach \cite{ref:bishop2006pattern}.



% If the problem is decomposed into identifying fault and detecting falls, the number of detectors needed is only 3: a detector for identifying faults, another for detecting falls in incipient trajectories and another for detecting falls in abrupt trajectories. Furthermore, using this technique resolves the ambiguity problem as the fault identifier detector can be used to determine the operational space (abrupt vs incipient fault). 

If the problem is decomposed into classifying trajectories first into the incipient versus abrupt categories, and secondly, detecting falls (or not) within these categories of trajectories, the number of detectors needed is only three: a detector for identifying types of trajectories, a second for detecting falls in incipient trajectories, and a third for detecting falls in abrupt trajectories. Furthermore, using this technique resolves the ambiguity problem as the incipient vs. abrupt classifier can be used to determine the operational space (abrupt vs incipient fault). 

To achieve this, we propose using SVM to categorize the trajectories into incipient vs abrupt. The training data for this SVM are taken as the joint velocities, and the labels 1 and -1 are used for the incipient and abrupt faults, respectively. For the training data, the windows in abrupt trajectories before a force is applied and windows uniformly distributed throughout the incipient trajectories are labeled as incipient and only the windows containing the force are labeled as abrupt. The rest of the pre-processing process follows steps similar to those in Section \ref{sec:data_preparation}. 

\begin{table}
        \centering
        \caption{ A comparison of the nearest-neighbor algorithm's performance when trained with (1) just the abrupt fault, (2) just the incipient fault, and (3) both faults together. Note that the false positive and negate rates are 0.}
        % \begin{tabular}{|p{0.2\linewidth}|p{0.75cm}|p{0.75cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|  }
        \tabulinesep=0.5mm
           \begin{tabu}{|c|c|c|c|  }
     \hline\hline
      &\multicolumn{1}{c|}{\makecell{Abrupt Fault\\Only}} & \multicolumn{1}{c|}{\makecell{Incipient Fault\\Only}}& \multicolumn{1}{c|}{\makecell{Both Faults\\Together}} \\
    \hline
           Fold                                                 &\makecell{Average\\Escape\\Time}         &\makecell{Average\\Escape\\Time} 
                  &\makecell{Average\\Escape\\Time}  \\                                             \hline
        1 & 0.48 &  0.93   & 0.49 \\
        \hline
        2    & 0.46  & 0.91  & 0.49 \\
        \hline
        3    & 0.44  & 0.9  & 0.51 \\    
        \hline
        4    & 0.43  & 0.89  & 0.51  \\    
        \hline
        5   & 0.5  & 0.89  & 0.51  \\ 
        \hline
        Average   & 0.46 & 0.91 &  0.50 \\       
    \hline                                               
    \end{tabu}
        \label{tab:fault_comp_clustering}
    \end{table}

\begin{table}
        \centering
        \caption{ A comparison of the SVM classification's performance when trained with (1) just the abrupt fault, (2) just the incipient fault, and (3) both faults together. Note that the false positive and negate rates are 0. }
        % \begin{tabular}{|p{0.2\linewidth}|p{0.75cm}|p{0.75cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|  }
        \tabulinesep=0.5mm
           \begin{tabu}{|c|c|c|c|  }
     \hline\hline
     & \multicolumn{1}{c|}{\makecell{Abrupt Fault\\Only}} & \multicolumn{1}{c|}{\makecell{Incipient Fault\\Only}}& \multicolumn{1}{c|}{\makecell{Both Faults\\Together}} \\
    \hline
           Fold                                                 &\makecell{Average\\Escape\\Time}         &\makecell{Average\\Escape Time} 
                  &\makecell{Average\\Escape Time}  \\                                             \hline
        1 & 0.5 &  1.0   & 0.65 \\
        \hline
        2    & 0.47  & 0.96  & 0.66 \\
        \hline
        3    & 0.49  & 0.98  & 0.66 \\    
        \hline
        4    & 0.44  & 0.97  & 0.65  \\    
        \hline
        5   & 0.51  & 0.96  & 0.63  \\ 
        \hline
        Average   & 0.48 & 0.97 & 0.65 \\       
    \hline                                               
    \end{tabu}
        \label{tab:fault_comp_classification}
    \end{table}


 % \begin{center}
%     \begin{table*}
%         \centering
%         \caption{ The maximum average F\_LT yielding a \fpr and \fnr of 0 that can be achieved by the clustering algorithm trained and evaluated on the validation data with (1) just the abrupt fault, (2) just the incipient fault, and (3) both faults together. $N_{monitor}$ =1 and $N_{window}$ =25. The maximum average F\_LT that can be achieved across all folds for the validation data is 0.34s, 0.8s and 0.7s for the abrupt, incipient, and both faults respectively}
%         % \begin{tabular}{|p{0.2\linewidth}|p{0.75cm}|p{0.75cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|  }
%         \tabulinesep=0.5mm
%            \begin{tabu}{|c|c|c|c|c|c|c|  }
%      \hline\hline
%      & \multicolumn{2}{c|}{\makecell{Abrupt\\Fault\\Only}} & \multicolumn{2}{c|}{\makecell{Incipient\\Fault\\Only}}& \multicolumn{2}{c|}{\makecell{Both\\Faults\\Together}} \\
%     \hline
%            Fold                                         &\makecell{Training\\Escape\\Time}        &\makecell{Average\\Escape\\Time}  &\makecell{Training\\Escape\\Time}        &\makecell{Average\\Escape\\Time} 
%             &\makecell{Training\\Escape\\Time}        &\makecell{Average\\Escape\\Time}  \\                                             \hline
%         1   & 0.7 & 0.33 & ~0.3 & 0.24  & ~0.3 & 0.05 \\
%         \hline
%         2   & 0.7 & 0.29 & ~0.3 & 0.23 & ~0.3 & 0 \\
%         \hline
%         3   & 0.7 & 0.24 & ~0.3 & 0.29 & ~0.3 & 0.01 \\    
%         \hline
%         4   & 0.5 & 0.21 & ~0.3 & 0.23 & ~0.3 & 0.04  \\    
%         \hline
%         5   & 0.5 & 0.22 & ~0.3 & 0.26 & ~0.3 & 0.04  \\ 
%         \hline
%         Average   &  & 0.25 &  & 0.25 &  & 0.03 \\       
%     \hline                                               
%     \end{tabu}
%         \label{tab:fault_comp_clustering}
%     \end{table*}
% \end{center}


%  \begin{center}
%     \begin{table*}
%         \centering
%         \caption{ The maximum average F\_LT yielding a \fpr and \fnr of 0 that can be achieved by the classification algorithm trained and evaluated on the validation data with (1) just the abrupt fault, (2) just the incipient fault, and (3) both faults together. $N_{monitor}$ =1 , $N_{window}$ =25, and a weight of 2 is used for normal data. The maximum average F\_LT that can be achieved across all folds for the validation data is 0.34s, 0.8s and 0.7s for the abrupt, incipient, and both faults respectively}
%         % \begin{tabular}{|p{0.2\linewidth}|p{0.75cm}|p{0.75cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|  }
%         \tabulinesep=0.5mm
%            \begin{tabu}{|c|c|c|c|c|c|c|  }
%      \hline\hline
%      & \multicolumn{2}{c|}{\makecell{Abrupt\\Fault\\Only}} & \multicolumn{2}{c|}{\makecell{Incipient\\Fault\\Only}}& \multicolumn{2}{c|}{\makecell{Both\\Faults\\Together}} \\
%     \hline
%            Fold                                         &\makecell{Training\\Escape\\Time}        &\makecell{Average\\Escape\\Time}  &\makecell{Training\\Escape\\Time}        &\makecell{Average\\Escape\\Time} 
%             &\makecell{Training\\Escape\\Time}        &\makecell{Average\\Escape\\Time}  \\                                             \hline
%         1   & 0.6 & 0.31 & 2.0 & 0.36  & ~0.2 & 0.13 \\
%         \hline
%         2   & 0.9 & 0.27 & ~0.5 & 0.32 & ~0.2 & 0.14 \\
%         \hline
%         3   & 0.5 & 0.22 & ~0.4 & 0.28 & ~0.2 & 0.12 \\    
%         \hline
%         4   & 2.0 & 0.2 & ~2 & 0.34 & ~0.2 & 0.2  \\    
%         \hline
%         5   & 2.0 & 0.29 & ~0.3 & 0.24 & ~0.2 & 0.13  \\ 
%         \hline
%         Average   &  & 0.25 &  & 0.31 &  & 0.03 \\       
%     \hline                                               
%     \end{tabu}
%         \label{tab:fault_comp_classification}
%     \end{table*}
% \end{center}


% \subsection{Varying the training F\_LT}
% The output of the algorithms are computed for a range of training F\_LT, 0-2s with $N_{window}=25$ (equivalent to 0.25s). 

% From Figures \ref{fig:fastActingFaults}  we see that the classification and clustering algorithm perform similarly for abrupt faults. However, as depicted in Figure \ref{fig:slowActingFaults}, the classification algorithm attains the desired \fpr and \fnr with a maximum average escape time of about 0.3s for the incipient faults while the clustering algorithm is unable to achieve both the \fnr,\fpr, and minimum average escape time. When comparing the performance of the classifier trained on both faults (Figure \ref{fig:classifier_both})  with its performance trained with just the abrupt fault (Figure \ref{fig:classifier_fast})  or just the incipient fault (Figure \ref{fig:classifier_slow}), we see that the classifier performs better when trained on each fault individually. More specifically, the classifier is able to attain a higher escape time with 0 false positive and negative rates, when it's trained on the faults individually. This conclusion is also reflected in the clustering algorithm as depicted in Figure \ref{fig:clustering_both}. 











% \subsection{Varying the number of data points in a window}
% Here, the training escape time is set to 2s and $N_{window}$ is varied from 3 - 150 data points. 

% Both algorithms are able to achieve the desired \fpr, \fnr and minimum escape time for the abrupt faults. The clustering algorithm attains a maximum average escape time of 0.26s with 66 data points while the classification algorithm has a maximum escape time of 0.28s with 3 data points.  For the incipient faults, both algorithms are able to achieve the desired \fpr, \fnr, and minimum average escape time but only the classification algorithm meets the desired minimum escape time threshold with a maximum  average escape time of 0.42s. None of the algorithms are able to achieve the desired \fpr, \fnr, and minimum average escape time threshold for both faults together. Figures \ref{fig:slowActingFaults_dataPoints}, \ref{fig:fastActingFaults_dataPoints}, and \ref{fig:bothActingFaults_dataPoints} display the results for the abrupt, incipient and both faults respectively. Note that the results of the clustering algorithm are not collected after 66 data points and 93 data points for the abrupt and incipient faults respectively because of its assumption that the first window contains only safe data points. 










%*****************************************************************************************************************************************
%  \begin{center}
%     \begin{table}
%         \centering
%         \caption{ The maximum average escape time for the incipient fault that can be achieved by both the clustering and classification algorithm using the training escape time that results in a \fpr and \fnr of 0.  $N_{monitor}$ =1. $N_{window} =25 $ for all the folds in the clustering algorithm. $N_{window}$ is set to 25 and 15 for all the folds 1-3 and 4-5 respectively in the classification algorithm}
%         % \begin{tabular}{|p{0.2\linewidth}|p{0.75cm}|p{0.75cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|  }
%         \tabulinesep=0.5mm
%            \begin{tabu}{|c|c|c|c|c|  }
%      \hline\hline
%      & \multicolumn{2}{c}{\makecell{Clustering\\Algorithm}} & \multicolumn{2}{c}{\makecell{Classification\\Algorithm}} \\
%     \hline
%            Fold                                         &\makecell{Training\\Escape\\Time}         &\makecell{Average\\Escape\\Time}   &\makecell{Training\\Escape\\Time}        &\makecell{Average\\Escape\\Time} \\                                       
%      \hline
%         1   &  ~0.3 & 0.22  &   ~~2 & 0.36 \\
%         \hline
%         2   &  ~0.3 & 0.23 &  ~0.5 & 0.32 \\
%         \hline
%         3   &  ~0.4 & 0.29 &  ~0.4 & 0.28 \\    
%         \hline
%         4   &  ~0.3 & 0.23 &  ~~2 & 0.38 \\    
%         \hline
%         5   &  ~0.3 & 0.16 &  ~0.3 & 0.25 \\ 
%         \hline
%         Average &    & 0.23 &  & 0.32 \\       
%     \hline                                               
%     \end{tabu}
%         \label{tab:incipient_trainEscapeTime}
%     \end{table}
% \end{center}


%  \begin{center}
%     \begin{table}
%         \centering
%         \caption{ The maximum average escape time for the abrupt fault that can be achieved by both the clustering and classification algorithm using the training escape time that results in a \fpr and \fnr of 0 when evaluated on the validation data. Note that $N_{monitor}$ =1. $N_{window} =25 $ for all the folds in the clustering algorithm. $N_{window}=15$ for all the folds in the classification algorithm   }
%         % \begin{tabular}{|p{0.2\linewidth}|p{0.75cm}|p{0.75cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|  }
%         \tabulinesep=0.5mm
%            \begin{tabu}{|c|c|c|c|c| }
%      \hline\hline
%      & \multicolumn{2}{c}{\makecell{Clustering\\Algorithm}} & \multicolumn{2}{c}{\makecell{Classification\\Algorithm}} \\
%     \hline
%            Fold                                        &\makecell{Training\\Escape\\Time}         &\makecell{Average\\Escape\\Time}   &\makecell{Training\\Escape\\Time}        &\makecell{Average\\Escape\\Time} \\                                              
%      \hline
%         1   &  ~0.6 & 0.33 &  ~~2 & 0.32 \\
%         \hline
%         2   &  ~0.6 & 0.29 &  ~0.9 & 0.28 \\
%         \hline
%         3   &  ~0.6 & 0.23 &  ~0.5 & 0.23 \\    
%         \hline
%         4   &  ~0.5 & 0.21 &  ~~2 & 0.2 \\    
%         \hline
%         5   &  ~0.5 & 0.22 &  ~~2 & 0.30 \\ 
%         \hline
%         Average   &  & 0.26 &  & 0.27 \\       
%     \hline                                               
%     \end{tabu}
%         \label{tab:abrupt_trainEscapeTime}
%     \end{table}
% \end{center}

%  \begin{center}
%     \begin{table}
%         \centering
%         \caption{ The minimum average escape time for the abrupt fault that can be achieved by both the clustering and classification algorithm using the training escape time that results in a fpr and \fnr of 0.  $N_{monitor}$ =1}
%         % \begin{tabular}{|p{0.2\linewidth}|p{0.75cm}|p{0.75cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|  }
%         \tabulinesep=0.5mm
%            \begin{tabu}{|c|c|c|c|c|  }
%      \hline\hline
%      & \multicolumn{2}{c}{\makecell{Clustering\\Algorithm}} & \multicolumn{2}{c}{\makecell{Classification\\Algorithm}} \\
%     \hline
%            Fold                                         &\makecell{Training\\Escape\\Time}        &\makecell{Average\\Escape\\Time}  &\makecell{Training\\Escape\\Time}        &\makecell{Average\\Escape\\Time}  \\                                       
%      \hline
%         1   & ~0.3 & 0.05 &  ~~0.2 & 0.13 \\
%         \hline
%         2   & ~0.3 & 0 &  ~0.2 & 0.14 \\
%         \hline
%         3   & ~0.3 & 0.01 &  ~0.2 & 0.12 \\    
%         \hline
%         4   & ~0.3 & 0.04 &  ~0.2 & 0.2 \\    
%         \hline
%         5   & ~0.3 & 0.04 &  0.2 & 0.13 \\ 
%         \hline
%         Average   &  & 0.03 &  & 0.14 \\       
%     \hline                                               
%     \end{tabu}
%         \label{tab:both_trainEscapeTime}
%     \end{table}
% \end{center}
