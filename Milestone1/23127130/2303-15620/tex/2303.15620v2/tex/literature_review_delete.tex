\subsection{Literature Review}\label{sec:literature_review}
 Fall detection algorithms, however, have been implemented in some bipedal robots such as Digit \cite{ref:agility}. Most algorithms in literature tend to focus only on abrupt faults.faults is more challenging for closed loop systems due to the feedback controller's effort to mitigate the effects of the faults. \eva{cite You-jin park here}.  Generally focusing on abrupt faults does not accomplish the goal of preventing or mitigating the impact of all falls as falls can also caused by either incipient or intermittent faults. For instance, a fall detection algorithm meant for abrupt faults, might not be able to detect incipient faults 'early' enough due to the crowding phenomenon. The crowding phenomenon is the similarity between the normal and incipient faulty data which makes it difficult to separate normal data from faulty data. \eva{cite Chen et al 2020} Note that the 
 


 \eva{dive into lit review where we highlight some papers}
 
 That being said, fall detection algorithms have been implemented in some bipedal robots such as Digit\cite{ref:agility}. However, most if not all of these algorithms focus only on "fast acting" faults. As discussed in Section \ref{sec:fastSlowActingFaults} an algorithm that's trained using "fast acting" faults will still detect "slow acting" faults but with little escape time. 
 
 Escape time or lead time \cite{ref:kalyanakrishnan} is defined as the difference between the time of the actual fall and predicted fall, and is used to inform whether or not sufficient time is left for the implementation of recovery/reflexive motions. Therefore, it is important to maximize the escape time. The minimum amount of escape time needed depends on the chosen recovery algorithm and the robot dynamics. It is important to note that a large escape time can result in high false positives.
 
 False positive and false negative rates are utilized to determine the reliability of the fall detection algorithms. Threshold methods such as checking the CoM height, and orientation of the feet have been utilized to minimize the percentage of false negative rates. \cite{ref:kalyanakrishnan}.

To reduce the false positive rate the output of the fall detection algorithm is monitored for a certain number of windows ($N_{monitor}$) and a fall is declared if and only if the number of detected falls in $N_{monitor}$ is above a chosen threshold. Note that the value of $N_{monitor}$ has an inverse relationship to the false positive rate. $N_{monitor}$ and its fall detection threshold are hyperparameters. \cite{ref:kalyanakrishnan}\cite{ref:subburuman}\cite{ref:khalastchi}.


The process of designing fall detection algorithms can be divided into two parts, feature engineering and the method used for detection. The purpose of feature engineering is to choose the minimal set of features that can differentiate between the anomalous and normal data while the chosen detection method relies on the chosen features to predict falls.
 
 In literature, a combination of stability metrics such as the zero moment point(zmp)\cite{ref:zmp}, capture points/regions\cite{ref:capture_regions}, and centroidal angular momentum(L$_{\text{com}}$)\cite{ref:orin}  along with other measurements such as torso angular velocity and acceleration are utilized as features for fall detection. \footnote{ even though stability metrics are used to increase the robusteness of controllers, they individually are not a sufficient condition for falling in general cases \cite{ref:subburuman}}. There, however, isn't a systematic way of choosing the features.
 
 The fall detection methods used in literature fall into two  categories classification (feature representation)\cite{ref:park}\cite{ref:hornung}\cite{ref:hohn}\cite{ref:wu}\cite{ref:kalynakrishnan}\cite{ref:marcilino} and  nearest-neighbor (relative density)\cite{ref:khalestchi}\cite{ref:subburaman}\cite{ref:amri}\cite{ref:solar}\cite{khan}. Each of the aforementioned categories can further be classified as either shallow or deep (using neural networks), and can be used in combination with dimension reduction algorithms such as PCA \cite{ref:suetani}\cite{ref:karseen}\cite{ref:alos}.  

Classification models try to learn a model from labeled training data and then classify a data
point into one of the classes based on the learned model. A disadvantage of these algorithms is that they can output incorrect predictions if the input data is outside the training data parameters. 

Nearest-neighbor based algorithms especially those based on density assume that the normal data exist in
highly dense spaces whereas the neighborhood of anomalous data is sparse. The disadvantages of
these methods is that they tend to have high false positives if the normal instances do not exist in
dense enough neighborhoods. \cite{ml1}\cite{ml4} 
 





