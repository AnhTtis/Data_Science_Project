
\section{Fall Detection Methods}
As a baseline, we use the SVM-based classification algorithm of \cite{ref:scikit-learn}, while the nearest-neighbor classification algorithm is based on the Ward minimum variance method \cite{ref:ward1963hierarchical}. To prioritize recent data points over previous ones, both methods make use of sliding windows. The number of data points in a window is referred to as $N_{window}$. It is important to note that both methods are supervised algorithms.

\subsection{SVM Classifier}

%
The radial basis function is chosen as the kernel and the soft margin formulation is implemented for the SVM classifier (SVM classification algorithm). The training data for the classifier is defined as 
\begin{align*}
D&=\{X_i,y_i\}_{i=1}^n \\
&~~\text{where}  \\
&\begin{array}{rcl}
    n &=& \text{number of windows across all training data}\\
    m &=& \text{number of time steps in a window}\\
    x_{ij}&=&\text{features at time step j in window i} \\
        X_i &=& \begin{bmatrix} x_{i1} && x_{i2} && \cdots && x_{im}  \end{bmatrix}^\intercal \\
    y_i &\in&\  \left\{
    \begin{array}{rcl}
           -1  &\quad& X_i ~\in ~\text{faulty trajectory} \\
            1  &\quad& X_i ~\in ~\text{normal trajectory}
    \end{array}
    \right\}
    \end{array}
\end{align*}


\subsection{Nearest-Neighbor Classification Algorithm}
\label{sec:classifier_intro}
 The selected nearest-neighbor classification algorithm determines distance using the Ward minimum variance and a weighted Euclidean distance. Given two clusters A and B, the Ward minimum variance method calculates the effort, $E_{AB}$, it takes to join the two clusters together, as determined by the sum of squared errors, specifically,  
 \begin{equation}
    \label{eq:ward_method}
    E_{AB} = SSE_{AB} - SSE_{A} - SSE_{B},
    \end{equation} 
 where
\begin{align*}
    &SSE_{AB} =(A\cup B- \mu_{A\cup B})^\intercal R^{-1}_{A\cup B}(A\cup B- \mu_{A\cup B})\\
    &SSE_{A} = (A- \mu_{A})^\intercal R^{-1}_{A}(A- \mu_{A})\\
    &SSE_{B} =  (B- \mu_{B})^\intercal R^{-1}_{B}(B- \mu_{B})\\
    &R = \text{correlation coefficient matrix}\\
    &\mu = \text{mean vector}
\end{align*}


In our application, cluster B contains the features at the current time step while cluster A contains all the features in the previous time steps included in the window. Given, that B is a single data point, the Ward minimum variance simplifies to
\begin{equation}
    \label{eq:our_ward_method}
    E_{AB} = SSE_{AB} - SSE_{A}. 
\end{equation}

 The nearest-neighbor classification algorithm detects a potential fall if the effort it takes to join the two clusters A and B is higher than a threshold determined from the training data. The threshold is calculated offline as the maximum $E_{AB}$ value for the safe data while $R$ is determined using distance correlation. Note that the underlying assumption for the nearest-neighbor classification algorithm is that cluster A only contains safe data points. 

\subsection{Data Labeling}
As one of our objectives is to maximize the lead time, we propose the use of a training lead time to label windows in a trajectory. Training lead time is defined as the difference between the time of the actual fall and the time when a  sliding window of a trajectory can be labeled as faulty. Therefore, training lead time is a subset of the maximum lead time that can be achieved in a faulty trajectory. While labeling all windows in a faulty trajectory as faulty would achieve the maximum lead time, it would also increase the rate of false positives. For instance, given two faults that are close in magnitude but where one results in a fall and the other is safe, the safe trajectory could be mistaken as a faulty trajectory. 

 If a trajectory does not contain a fall, all windows derived from the trajectory are labeled as 1. If a trajectory ends in a fall, all windows containing data points after the desired training lead time are labeled as -1. Note that for an abrupt fault, the training lead time is only defined after the push is introduced, and only the data points before a fall are kept for the training data of both faults.

 The desired training lead time is determined by a grid search algorithm that trains the algorithm of interest using a range of training lead times from 0 to 2s and evaluates the results on the training and/or validation data. The training data is included in the evaluation process for cases where the algorithm is allowed to make mistakes, such as when using a soft margin in SVM. A training lead time of 2s would label all the data points in a faulty trajectory as faulty.   



\subsection{Performance of Fall Detection Methods} 
\label{sec:fault_comparison}
 % To determine how to split the training data along faults, an experiment is conducted with each of the proposed fall detection algorithms trained using just incipient faults, just abrupt faults, and both faults. The algorithms are evaluated on the validation data. The evaluation metrics are false positive and negative rate (fpr and \fnr respectively), and the maximum average F\_LT achieved across all the trajectories. The desired fpr and \fnr is set to 0,  $N_{window}=25$, and $N_{monitor} = 1$ . The value of $N_{window}$ is set based on evaluating the performance of the algorithms across various values. Note that for both algorithms, a low value for $N_{window}$ results in higher fpr while a high value can reduce the maximum average F\_LT. $N_{monitor} = 1$ because we found that for our dataset, $N_{monitor}$ had to be set to a large value, e.g. 100 in order to result in 0 fpr.

In this section, we analyze the performance of the proposed nearest-neighbor classification algorithm and the SVM classifier. The algorithms are trained and evaluated on testing data across all 5 folds using just abrupt trajectories, just incipient trajectories, and both trajectories together. The evaluation metrics are false positive and negative rates, and the average lead time achieved. The desired false positive and false negative rates are set to 0. The training lead time chosen is the maximum that meets the given bounds on the false positive and false negative rates when evaluated on the training and validation data. Based on previous experiments, we set the values of the remaining hyper-parameters as $N_{window}=10$ and $N_{monitor} = 1$. 

 From Table \ref{tab:fault_comp_clustering} and \ref{tab:fault_comp_classification} we see that the nearest-neighbor and the SVM classification algorithms perform similarly when trained and evaluated on the abrupt and incipient faults separately. The nearest-neighbor classification algorithm achieves an average lead time of 0.46s and 0.91s, respectively, for the abrupt and incipient faults, while the SVM classification algorithm achieves an average lead time of 0.48s and 0.97s. Because our sampling time is 0.03s, the difference in the performance of both algorithms is 1 and 2 data points for the abrupt and incipient fault respectively.  Fig.~\ref{fig:class_results} displays the classification results for several trajectories. 


 When both faults are trained and evaluated together, the SVM classification algorithm achieves an average lead time 0.15s higher compared to the nearest-neighbor classification algorithm. In comparison to its average performance on the abrupt and incipient fault, the SVM classification algorithm achieves an average lead time of 0.08s less when trained on both faults together. Similarly, the nearest-neighbor classification algorithm achieves an average lead time of 0.19s less. As a result, the SVM classification algorithm outperforms the nearest-neighbor classification algorithm when both faults are trained together. However, because both algorithms can achieve lead times higher than the 0.2s, which is the lead time required by reflexive algorithms such as \cite{ref:hohn2009probabilistic} and \cite{ref:wu2021falling}, both algorithms are viable options. As the nearest-neighbor classification algorithm learns the safe/good model, it should be used when faulty data is sparse. 

 
\subsection{Categorizing Faults}
A means to decrease the difference in performance for both algorithms when trained on both faults together vs. separately is to implement a multi-class classification problem. The labels for this multi-class classification can be identified as: abrupt fault safe (AS), abrupt fault fall (AF), incipient fault safe (IS), and incipient fault fall (IF). Using these labels with the one-vs-one or one-vs-rest multi-class classification techniques typically implemented \cite{ref:bishop2006pattern}, results in six and four detectors, respectively. However, as one-vs-rest can result in ambiguities and class imbalances and using one-vs-one can result in ambiguities and higher computational times, we seek a different approach \cite{ref:bishop2006pattern}.


If the problem is decomposed into classifying trajectories first into the incipient versus abrupt categories, and secondly, detecting falls (or not) within these categories of trajectories, the number of detectors needed is only three: a detector for identifying types of trajectories, a second for detecting falls in incipient trajectories, and a third for detecting falls in abrupt trajectories. Furthermore, using this technique resolves the ambiguity problem as the incipient vs. abrupt classifier can be used to determine the operational space (abrupt vs incipient fault). 

To achieve this, we propose using SVM to categorize the trajectories into incipient vs abrupt. The training data for this SVM are taken as the joint velocities, and the labels 1 and -1 are used for the incipient and abrupt faults, respectively. For the training data, the windows in abrupt trajectories before a force is applied and windows uniformly distributed throughout the incipient trajectories are labeled as incipient and only the windows containing the force are labeled as abrupt. The rest of the pre-processing process follows steps similar to those in Section \ref{sec:data_preparation}. 

\begin{table}
        \centering
        \caption{ A comparison of the nearest-neighbor classification algorithm's performance when trained with (1) just the abrupt fault, (2) just the incipient fault, and (3) both faults together. Note that the false positive and negative rates are 0.}
        % \begin{tabular}{|p{0.2\linewidth}|p{0.75cm}|p{0.75cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|  }
        \tabulinesep=0.5mm
           \begin{tabu}{|c|c|c|c|  }
     \hline\hline
      &\multicolumn{1}{c|}{\makecell{Abrupt Fault\\Only}} & \multicolumn{1}{c|}{\makecell{Incipient Fault\\Only}}& \multicolumn{1}{c|}{\makecell{Both Faults\\Together}} \\
    \hline
           Fold                                                 &\makecell{Average\\Lead Time}         &\makecell{Average\\Lead Time} 
                  &\makecell{Average\\Lead Time}  \\                                             \hline
        1 & 0.48 &  0.93   & 0.49 \\
        \hline
        2    & 0.46  & 0.91  & 0.49 \\
        \hline
        3    & 0.44  & 0.9  & 0.51 \\    
        \hline
        4    & 0.43  & 0.89  & 0.51  \\    
        \hline
        5   & 0.5  & 0.89  & 0.51  \\ 
        \hline
        Average   & 0.46 & 0.91 &  0.50 \\       
    \hline                                               
    \end{tabu}
        \label{tab:fault_comp_clustering}
    \end{table}

\begin{table}
        \centering
        \caption{ A comparison of the SVM classification algorithm's performance when trained with (1) just the abrupt fault, (2) just the incipient fault, and (3) both faults together. Note that the false positive and negative rates are 0. }
        % \begin{tabular}{|p{0.2\linewidth}|p{0.75cm}|p{0.75cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|  }
        \tabulinesep=0.5mm
           \begin{tabu}{|c|c|c|c|  }
     \hline\hline
     & \multicolumn{1}{c|}{\makecell{Abrupt Fault\\Only}} & \multicolumn{1}{c|}{\makecell{Incipient Fault\\Only}}& \multicolumn{1}{c|}{\makecell{Both Faults\\Together}} \\
    \hline
           Fold                                                 &\makecell{Average\\Lead Time}         &\makecell{Average\\Lead Time} 
                  &\makecell{Average\\Lead Time}  \\                                             \hline
        1 & 0.5 &  1.0   & 0.65 \\
        \hline
        2    & 0.47  & 0.96  & 0.66 \\
        \hline
        3    & 0.49  & 0.98  & 0.66 \\    
        \hline
        4    & 0.44  & 0.97  & 0.65  \\    
        \hline
        5   & 0.51  & 0.96  & 0.63  \\ 
        \hline
        Average   & 0.48 & 0.97 & 0.65 \\       
    \hline                                               
    \end{tabu}
        \label{tab:fault_comp_classification}
    \end{table}

\begin{figure*}
\subfloat[Nearest-Neighbor Incipient Fault]{\resizebox{.40\linewidth}{!}{\includegraphics[]{img/fall_detection/classification_results/cluster_incipient_noTitle.pdf}}}
\hfill
\subfloat[SVM Incipient Fault]{\resizebox{.40\linewidth}{!}{\includegraphics[]{img/fall_detection/classification_results/svm_incipient_noTitle.pdf}}}
    \hfil
    \newline
\subfloat[Nearest-Neighbor Abrupt Fault]{\resizebox{.40\linewidth}{!}{\includegraphics[]{img/fall_detection/classification_results/cluster_abrupt_noTitle.pdf}}}
\hfill
\subfloat[SVM Abrupt Fault]{\resizebox{.40\linewidth}{!}{\includegraphics[]{img/fall_detection/classification_results/svm_abrupt_noTitle.pdf}}}    
\caption{Plots displaying the classification results of the nearest-neighbor and SVM classification algorithms for several trajectories. The red solid line is the threshold for the decision function. The dots are the last data point in a window. Positive and negative values for the SVM decision function result in safe and faulty classifications, respectively. On the other hand, values below and above the decision function threshold are classified as safe and faulty for the nearest-neighbor classification algorithm.}
\label{fig:class_results}
\end{figure*}

