%!TEX root = response.tex

\section*{Reviewer 2 (2AE)}

\textbf{[c1] One concern is that the proposed idea is mainly built up from prior art (esp., ApproxTuner) and the novelty is not clearly shown throughout the paper. The main approximate techniques presented in Section 3.1 have already been studied in ApproxTuner. The main addition that the authors made in this paper is mostly about Android implementation and porting as shown in Figure 2. I could see that the authors have put significant efforts in this direction, but the research contribution is not clearly shown (i.e., which unique/specific challenges were addressed to realize approximate computing on mobile devices).}

\paragraph{}

[r1]  Please see [r5] to Reviewer 1's comments.

%To the best of our knowledge, Mobiprox is the only solution that enables dynamic approximation of an existing neural network on actual mobile device. Other solutions, such as common quantization, pruning, distillation, DeepX, etc., are either \textit{not dynamic}, i.e. leave the model permanently impaired, or \textit{not applicable to an existing network} that was not trained with approximation in mind (e.g. SPINN, MSDNN, etc.), or additionally, \textit{ not supported by mobile devices} (e.g. Slimmable Neural Networks, AnyPrecision networks). 
%\hashim{Approxtuner doesn't impair the NNs structure. The argument is weak imo. Approxtuner can very well apply to Mobile systems - it just requires developers to add a new hardware backend specific to Android. Repeating the earlier point, the best differentiating novel contribution in Mobiprox is of "context-aware approximation". If there is something more fundamental about Mobiprox in supporting mobile systems it should be made clear. For instace, what new (not existing before) algorithms, methodologies, etc are proposed in Mobiprox that make it work for mobile systems}

%We also need to clarify that our work is not about porting ApproxTuner to Android. Indeed, ApproxTuner remains as is and runs on the server. Instead, we \textit{i)} develop a completely new runtime library supporting dynamic execution of approximate operations on Android, \textit{ii)} implement a new Android-based profiler of approximate configurations, \textit{iii)} devise three different strategies for runtime approximation configuration adaptation on mobile devices, and \textit{iv)} implement an end-to-end compiler framework that enables dynamic approximation of pre-trained neural networks on Android devices.

%The challenges we faced include: \textit{i)} the difficulty of implementing approximate operations on a sufficiently low level of the mobile computing stack; as mobile deep learning frameworks (e.g. TensorFlow Lite) and even libraries of specialized functions for mobile deep learning (e.g. ARM Compute Library) aggregate tensor operations, we had to implement approximate, as well as precise, tensor operations from basic linear algebra subprograms (BLAS) primitives; \textit{ii)} the issue of modifying neural network operation at runtime on a mobile device; mobile DL frameworks do not support dynamic graph reconfiguration, thus even the existing dynamic approximation schemes (such as Slimmable Neural Networks) do not work on mobiles; to overcome this, we implemented our custom approximations at a low level and exposed the calls for setting the approximation level at runtime through Java Native Interface; \textit{iii)} the lack of algorithms and tools for context-aware adaptation of mobile DL; a certain classification accuracy level might be acceptable in some situations, but not in others; in addition, an approximated DL model that works well for certain inputs, might not provide correct classification for some other inputs; finally, gauging model performance at runtime is challenging; we first devise two proxies for measuring classification performance (the same-class instance counting-based and the SoftMax confidence-based) and then develop two algorithms (state-based and confidence-based) for dynamically adapting the approximation.

%We modified Section 1 to clarify this (please see pg 3).

\paragraph{}

\textbf{[c2] the benefit and advantages of Mobiprox are not clearly shown, compared to existing model compression techniques.}

\paragraph{}

[r2] Please see [r7] to Reviewer 1's comments.

%Mobiprox provides a series of benefits compared to existing model compression techniques. First of all, it represents a framework that enables low-level approximation/compression techniques to be deployed on mobiles -- until Mobiprox, there was no library that would allow for such approximations to be implemented on mobile devices.
%\hashim{Not true imo. Many frameworks support compilation with approximations for mobile. For instance, TVM: https://tvm.apache.org/docs/how\_to/deploy/android.html. Also approximations such as structured pruning (removing full filters) would only be applied at the Pytorch level while the performance benefits would translate to mobile systems - or any hardware platform for that matter. }
%Second, Mobiprox is the first framework to allow for changing between running with or without approximation on the same neural network. Existing network compression techniques normally leave the network permanently impaired (e.g. quantization normally gives you a permanently quantized network), whereas Mobiprox enables running inference with the both uncompressed/unapproximated network and the network with various compression techniques applied. Finally, one of the main strongpoints of Mobiprox is its ability to adjust the level of approximation/compression in real time to achieve a variable trade-off between QoS and energy consumption. This context-aware dynamic adjustment of the compression level is innovative and was, prior to Mobiprox, not available for use in the case of traditional model compression techniques.

\paragraph{}

\textbf{[c3] The scope of the paper is not clear, and the generalizability of the proposed system is less convincing. First, it seems that the authors mainly target CNN-based architecture, but there are also other types of deep learning architecture that are actively used.}

\paragraph{}

[r3] Please see [r6] to Reviewer 1's comments. 

%Mobiprox supports all layer types in deep learning, but the approximation of an individual layer is limited by the approximation techniques that are actually implemented for this particular layer type at a low level of the mobile computing stack. Thus, for all layer types we support quantization from 32b to 16b floats (as long as on-device GPUs support such quantization). For convolutional layers, as they are the most computationally hungry in most DL applications, we implemented additional approximation techniques (i.e. the row and column perforation, and filter pruning). To the best of our knowledge, there are no other approximation primitives for, say, LSTM layers. Should they appear in the future, they could be added to Mobiprox. In the revised manuscript, we emphasize better this support for any NN architecture/layer type in Section 8 (Discussion and limitations).

\paragraph{}

\textbf{[c4] Second, the adaptation strategies work only for the model with the time-series data.}

\paragraph{}

[r4] Please see [r6] to Reviewer 1's comments. 

%Mobiprox is not limited to time series data only. Indeed, we show that an image classification network can be dynamically approximated with Mobiprox. The framework’s dynamic adaptation strategies, however, will perform well only if the link between the context and the required level of adaptation can be established. This does not mean that the approach is limited to time series data or even data where the processes change slowly. To demonstrate this, in addition to the MobileNet architecture used for human activity recognition, in the expanded evaluation section (Section 7) of the new version of the manuscript we also implement a different neural network architecture for spoken keyword recognition. Experiments detailed in Section 7 demonstrate that Mobiprox enables substantial energy savings with an algorithm that adapts the approximation as the levels of the background noise vary. 


\paragraph{}

\textbf{[c5] The evaluation was conducted only with the motion models, but the effect of the adaptation was not evaluated for other cases (e.g., vision models or audio models). }

\paragraph{}

[r5] Please see [r8] to Reviewer 1's comments. 


\paragraph{}

\textbf{[c6] Third, I am not sure, in practice, how many types of approximation can be supported by this system, other than balancing the energy and accuracy. Accordingly, the need for adopting general adaptation strategies is not clear. }

\paragraph{}

[r6] Mobiprox makes use of the approximations which can be controlled in real time for achieving a tradeoff between accuracy and energy according to the contextual factors and the requirements of the user/developer. The process of balancing between accuracy and energy at each inference point is controlled by the adaptation strategy which is in use. Such strategies can vary from ``maximize the accuracy using just as much energy as needed'' to ``use higher approximation when battery level falls below 10\%'' or even to strategies which take the broader context of the user into account (e.g. ``use more accurate HAR models when a user is exercising'') or even business models (e.g. ``use input-adaptable approximation for premium users''). We discuss the possibility of using different adaptation strategies at the beginning of Section 5 in the paper.

Defining the strategy is handled by the developer and Algorithm~2 (pg. 10) shows the generic integration of the adaptation strategy within Mobiprox. Programming such strategies is trivial, yet, one can envision a more challenging-to-achieve goal, such as ``minimize the energy usage without sacrificing the inference accuracy''. In Section 5 we devise three strategies demonstrating that such a widely applicable goal can be met with Mobiprox.

%\hashim{Good response.}


\paragraph{}

\textbf{[c7] Fourth, it is unclear how developers can access all possible devices for ADB-energy profiling. It is well known that power profile is quite different depending on many factors such as smartphone model, Android version, etc.}

\paragraph{}

[r7] Indeed the power profile is different for each smartphone model, however, according to our testing on  widely different devices, the relative ordering of the configurations remains the same with respect to the QoS loss -- Speedup space. We clarify this in Section 6:

\begin{quote}
    \emph{"While each Android device has its own specific power profile, the experimental measurements we perform are still relevant to identify the ordering of the approximate configurations in the QoS loss – Speedup space. This ordering directly impacts the adaptation strategies and, at least over the devices we had access to (Samsung Galaxy S21, Samsung Galaxy M21, Xiaomi Pocophone F1, and ASUS Tinkerboard S), the ordering remains the same regardless of the device specifics."}
\end{quote}

This means, that, for instance, \emph{“achieve the highest possible accuracy with the least amount of energy” }strategy, such as the one used in the evaluation section, works on all devices, despite the configurations being profiled on only one device. In the discussion section (Section 8), we also present our ides for strategies that rely on absolute performance numbers, such as, for example \emph{“ensure that the execution runs 20\% faster when a user is in a vehicle”}: 

\begin{quote}
 \emph{``the power consumption profiling we conduct in this paper remains relevant for the ordering of approximate configurations in the QoS loss -- Speedup space across many devices, although it is performed on a single device. This makes any configuration ordering-based adaptation strategy feasible and generalizable. However, the absolute speedup values achieved by approximate configurations may not stay the same across different devices. Thus, adaptation strategies relying on absolute speedup values would require that the profiling devices is of the same architecture (in a narrow sense) as the target device. To avoid the impact on generalizability, in the future we plan to integrate a runtime performance measurement method based on heartbeats [16] that will ensure runtime re-profiling of the configurations on a given execution platform.''}

\end{quote}

%We note that strategies that rely on absolute performance numbers, such as \emph{“ensure that the execution runs 20\% faster when a user is in a vehicle”} require that the profiling device is of the same architecture (in a narrow sense) as the client device. To avoid this, in the future we plan to integrate a runtime performance measurement method based on heartbeats \emph{(H. Hoffmann, J. Eastep, M. D. Santambrogio, J. E. Miller, and A. Agarwal. Application Heartbeats: A Generic Interface for Specifying Program Performance and Goals in Autonomous Computing Environments. In 7th International Conference on Autonomic Computing, ICAC, 2010.)} that will ensure re-profiling of the configurations on a given execution platform. In the revised manuscript, we justify why our power profiling experiments can be generalized for other devices in section 6 and discuss its potential limitation in section 8.

%Third, we note that while our power consumption profiling, although performed on a single device, is relevant for the ordering of approximate configurations in the QoS loss – Speedup space, for those adaptation strategies that rely on absolute performance numbers, we require that the profiling device is of the same architecture (in a narrow sense) as the target device. To avoid the impact on generalizability , in the future we plan to integrate a runtime performance measurement method based on heartbeats [16] that will ensure runtime re-profiling of the configurations on a given execution platform.

\paragraph{}

\textbf{[c8] It is a bit disappointing that the experiment was conducted on a dev bod, not actual smartphones. Was there any limitations of the Mobiprox implementation to be adopted on smartphones?}

\paragraph{}

[r8] Please see [r9] to Reviewer 1's comments. 

\paragraph{}

\textbf{[c9] Also, only one model was used for the adaptation evaluation (which is the main experiment of this paper). }

\paragraph{}

[r9] Please see [r8] to Reviewer 1's comments. 


\paragraph{}
