%!TEX root = response.tex

\section*{Reviewer 3}

\textbf{[c1] My main concern is novelty.}

%\hashim{This point has been addressed earlier for other reviewers. The responses to multiple reviewers can be merged to avoid redundancy}

\paragraph{}

[r1]  Please see [r5] to Reviewer 1's comments.

%To the best of our knowledge, Mobiprox is the only solution that enables dynamic approximation of an existing neural network on actual mobile device. Other solutions, such as common quantization, pruning, distillation, DeepX, etc., are either \textit{not dynamic}, i.e. leave the model permanently impaired, or \textit{not applicable to an existing network} that was not trained with approximation in mind (e.g. SPINN, MSDNN, etc.), or additionally, \textit{ not supported by mobile devices} (e.g. Slimmable Neural Networks, AnyPrecision networks). 

%In addition, our work is highly innovative since Mobiprox provides solutions to a series of challenges which have not been successfully addressed until now:
%\begin{enumerate}
%    \item The lack of support for low level approximation on the mobiles -- until Mobiprox, there was no library that would allow for such approximations to be implemented on mobile devices;
%    \item Missing support for dynamic runtime adaptation of neural networks. Mobiprox is the first framework to allow for changing between running with or without approximation on the same neural network;
%    \item Missing mechanism for controlling the approximation in a context-aware manner; Compared to other compression techniques which produce a permanently impaired network, Mobiprox enables adjusting the level of approximation in real time to achieve a variable trade-off between QoS and energy consumption.  
%\end{enumerate}
%In Section 1 we added a description of these challenges and the way Mobiprox addresses them. 

\paragraph{}

\textbf{[c2] Another question is whether the proposed optimizations will work well on other large datasets (for example, ImageNet), as Cifar10 is too small to detect the genunarity. }

\paragraph{}

[r2] We thank the reviewer for raising this issue. Indeed, the appropriate variability of the difficulty of the data (in terms of how difficult it is to classify an instance) is what our adaptation strategies are counting on. Very high difficulty would preclude the use of approximation. However, the approximations that Mobiprox implements are guided by an adaptation strategy, which, according to contextual factors (including the difficulty of the input) fine-tunes the level of approximation to meet the QoS requirements. In the case of having inputs that are very challenging to classify, an adaptation strategy would lower the level of approximation down to the point of using the non-approximated network. Hence, in such a corner case, Mobiprox would perform no worse than the unapproximated network.

\paragraph{}

\textbf{[c3] The accuracy-speedup tradeoff between approximate computation and other model compression techniques, such as low-bit quantization or model pruning, would be interesting to compare. Therefore, I would be more convincing in evaluating the efficacy of the solutions proposed in this paper.}

\paragraph{}

[r3] Please see [r7] to Reviewer 1's comments.

%While Mobiprox provides a series of benefits compared to existing model compression techniques, it does not compete directly with these. 

%First of all, Mobiprox represents a framework that enables low-level approximation/compression techniques to be deployed on mobiles -- until Mobiprox, there was no library that would allow for such approximations to be implemented on mobile devices. Second, Mobiprox is the first framework to allow for changing between running with or without approximation on the same neural network. Existing network compression techniques normally leave the network permanently impaired (e.g. quantization normally gives you a permanently quantized network), whereas Mobiprox enables running inference with the both uncompressed/unapproximated network and the network with various compression techniques applied. Finally, one of the main strongpoints of Mobiprox is its ability to adjust the level of approximation/compression in real time to achieve a variable trade-off between QoS and energy consumption. This context-aware dynamic adjustment of the compression level is innovative and was, prior to Mobiprox, not available for use in the case of traditional model compression techniques.

\paragraph{}
