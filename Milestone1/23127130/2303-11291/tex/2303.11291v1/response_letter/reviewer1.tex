%!TEX root = response.tex

\section*{Reviewer 1 (1AE)}

%\sasa{This response seems to be to all the reviewers, not just R1?!}

\textbf{[c1] Discussing more on the selection of approximation techniques (R4).}

\paragraph{}

[r1]  With Mobiprox we take the stance that “data scientists are not mobile system experts”. %\hashim{Should we say "mobile system experts" or "optimization experts". The claim we make about data scientists is more general.} 
Mobiprox's key contribution is decoupling neural network training (the task of a data scientist) from network adaptation and optimized execution on the mobile platforms (the task of a mobile system expert). This decoupling allows for greater portability of existing deep learning models specialized for various tasks (e.g. activity recognition, spoken keyword detection, etc.) that are becoming common in mobile computing. 

Approaches such as Slimmable Neural Networks (SNN), suggested by R4, go against this philosophy and require that a model is trained as a SNN from the start, i.e. the data scientist has to be aware in advance of the compression that will be used during the runtime. 
%
We add a clarification in Section 8: 
\begin{quote}
    \emph{“We note that techniques requiring up-front modification in order to support approximation, such as Slimmable neural networks, remain unsuitable, as with Mobiprox we provide a service that allows the integration of pre-built networks oblivious to approximation, for instance, those acquired through Google Cloud AutoML, into mobile apps.”}

\end{quote}

%\hashim{Portability argument is strong and comparison against SNN is appropriate}

In addition, we observe that SNNs are far from mobile-ready. For the evaluation purposes we attempted to experiment with SNNs on the mobiles, yet this proved to be prohibitively difficult to implement. More specifically, SNNs require dynamic NN graph reconfiguration, which, to the best of our knowledge, is not supported by any of the mobile deep learning frameworks (TensorFlow Lite, Pytorch Mobile, etc.). This we clarify in the last paragraph of Section 2:

\begin{quote}
    \emph{“...unlike our approach, the above methods were not originally planned with mobile platform restrictions in mind. SNNs, for instance, rely on dynamic neural network graph reconfiguration, something that none of the mobile DL frameworks (e.g. TensorFlow Lite, Pytorch Mobile, etc.) supports at the moment.”}

\end{quote}

%\sasa{This last point should also be in the related work paragraph!}


\paragraph{}

\textbf{[c2] Evaluating the system implementation in comparison to other existing mobile deep learning libraries or code generation tools (R4).}

\paragraph{}

[r2] We thank R4 for the suggestion to compare with the existing mobile deep learning libraries. However, we believe that a direct comparison is not possible. Mobiprox has different goals and is not a direct competitor to the systems such as TensorFlow Lite or Pytorch Mobile. With Mobiprox, our goal is to demonstrate \textit{the feasibility of dynamic on-device approximation}. Mobiprox allows for a  neural network, implemented in either PyTorch or TensorFlow, to be used for mobile on-device inference using various approximate configurations, each yielding a different trade-off ratio between accuracy and energy usage. In addition, Mobiprox has the key advantage of enabling a dynamic, real-time adjustment of the approximation level, according to the context of use. PyTorch Mobile or TF Lite\textit{ simply do not support this}. What these frameworks do produce, however, is a single version of a network thoroughly optimized (through e.g. operation fusion, weight prepacking, etc.) for running on a target platform. 

%TF lite and PyTorch Mobile, on the other hand, aim to convert a network to enable it to run on a mobile (something Mobiprox does as well), but do not support approximation adaptation.

%\sasa{We can emphasize this part of the response more:}

In Section 8, however, we add the following insight about the potential synergy between Mobiprox and other mobile DL compiler stacks:
\begin{quote}
    \emph{"To unlock further benefits, we plan to examine integration with mobile DL compiler stacks that are already hand-optimized by large engineering teams in production environments, such as TVM, Pytorch Mobile, or TF Lite. Since the approximations in Mobiprox reduce both the number of compute operations and memory loads and stores, the performance improvements of these approximations should seamlessly translate, if efficient library and compiler implementations listed above are used. Not only would this likely lead to improved speedup gains, but would also ameliorate the need for time-inefficient development of custom approximable tensor runtimes for various architectures."}

\end{quote}

%Frameworks, such as Pytorch Mobile and TF Lite, have to compete on the market, and are hand-optimized by large engineering teams in production environments. Mobiprox is a research project demonstrating a novel concept, and as such, it has not been fine-tuned in production environments. Nevertheless, since the approximations in Mobiprox reduce both the number of compute operations and memory loads and stores, the performance improvements of these approximations should extend well to efficient library and compiler implementations in future.

%\sasa{let's emphasize here that they don't do adaptation.} \sasa{We should make a stronger argument why we don't go into minimizing resource usage; Hashim's comment at the end of this Q may be a good start.} 

%\sasa{These numbers below don't look good. We are better with just the argument above why the direct comparison is not necessary and remove the next paragraph from the response. Perhaps we should also say in the paper that we will emphasize that our focus is on showing the potential for dynamic approximation, not building the most optimized system in the world (i.e. the difference between science and engineering). }
%Nevertheless, we performed additional experiments to juxtapose the two approaches. While an un-approximated MobileNet V2 network implemented through Mobiprox consumes $25.6uAh$ for classifying an input instance, the same network implemented through PyTorch Mobile consumes $4uAh$ of energy for classifying an input instance on the same device. \hashim{So we are conceding Mobiprox unapproximated is much worse than Pytorch mobile?} As explained above, however, these numbers should not be directly compared, but should be used to assess the potential of absolute performance of Mobiprox, once it is optimized with orthogonal techniques used in frameworks such as PyTorch mobile. \hashim{The purpose of these last 2 sentences is vague - we should make a stronger argument}

%\hashim{An alternative argument could be: Since NN-specific libraries are hand-optimized by large engineering teams in production environments, it is impractical to match performance offered by these systems. However, since the approximations in Mobiprox reduce both the number of compute operations and memory loads and stores, the performance improvements of these approximations should extend well to efficient library and compiler implementations. }

%Finally, Mobiprox has another advantage over the above-mentioned frameworks: it represents an alternative solution for deep neural networks to be executed on an Android device, which could prove an advantage given the existing limitations of both Pytorch Mobile and TF Lite in terms of converting some particular neural network architectures or networks with custom layers, etc...


\paragraph{}

\textbf{[c3] Please elaborate on why the proposed adaptation strategies might work on image classification tasks (R4).}

\paragraph{}

[r3] We thank R4 for this observation. Indeed, Mobiprox adaptation algorithms are not suited for situations where the target concept fluctuates rapidly, or as we state in Section~8:
\begin{quote}
    \emph{``our adaptation strategies may not be suitable for tasks such as anomaly detection where sudden changes of the target phenomena are expected''}
\end{quote}
This, however, does not limit Mobiprox’s applicability to computer vision. Indeed, we envision Mobiprox being useful for tasks such as visual object tracking in live video, where the difficulty of tracking, and consequently the opportunity for deep learning approximation, may vary with the object occlusion, scene complexity, brightness, etc. 

We add the following text to Section 8 to clarify this:
\begin{quote}
\emph{``Note that this does not restrict the general domain in which Mobiprox can be applied. Indeed, with minor modifications, the strategies presented in this paper can be used for adapting approximation of models built for tracking objects in live video [20], for instance.''}
\end{quote}

%With respect to CIFAR10, this dataset is used in Sections 7.1. and 7.2. exclusively to demonstrate that approximate versions of networks built for image processing can be successfully identified with Mobiprox. This dataset is not used in Sections 7.3. and 7.4. where we discuss and evaluate the dynamic adaptation of approximation. 

The reviewer also raised the remark about the applicability of Mobiprox to image classification on the CIFAR10 dataset. We have to clarify that this dataset is used only in Sections 7.1. and 7.2. to demonstrate that approximate versions of networks built for image processing can be successfully identified with Mobiprox. This dataset is not used in Sections 7.3. and 7.4. where we discuss and evaluate the dynamic adaptation of approximation and use the human activity recognition-related dataset UCI-HAR and the Google Speech Commands (GSC) v0.01 dataset for spoken keyword detection. These two datasets are described in detail in Section 6.

% \sasa{Say here which datasets we used (it should be 2 with the new experiment?). } \sasa{Did we update the text in the main paper to make this point clearer?}

\paragraph{}

\textbf{[c4] Present the evaluation results in a more appealing format (R4).}

\paragraph{}

[r4] We thank R4 for this comment. In the revised version of the paper we improved the presentation of the evaluation results. First, we underline the evaluation metrics - the QoS: in Section 4.2 we define the quality of service (QoS) loss as a real number equal to the difference between the classification accuracy, over a representative validation dataset, of a non-approximated and an approximated DL model. Second, we replaced the previous Figure 5 with a new plot which better illustrates the key results of evaluating Mobiprox, namely the system-wide energy consumption of Mobiprox when running two different NNs on two different inference tasks, versus the QoS loss. 

\paragraph{}

\textbf{[c5] Clarify the novelty of the work (R2,R3)}
%\sasa{This question should be moved forward}

\paragraph{}

[r5] Our work is highly innovative since Mobiprox provides solutions to a series of challenges that have not been successfully addressed in the mobile realm until now:
%\sasa{we should say 'for mobile' here}
\begin{enumerate}
    \item Missing mechanism for controlling the approximation in a context-aware manner; Compared to other compression techniques which produce a permanently impaired network, Mobiprox enables adjusting the level of approximation in real time to achieve a variable trade-off between QoS and energy consumption.  %\hashim{This is the most compelling argument! Mobiprox is the first framework for "context-aware approximation"}
    %\sasa{I agree. This bullet should be the first one in the list.}
    \item Missing support for dynamic runtime adaptation of mobile neural networks. Mobiprox is the first framework to allow for changing between running with or without approximation on the same neural network on a mobile; %\hashim{Approxtuner also provided this capability. I have no objection that we make this claim but reviewers could complain} 
    %\sasa{Quantifying this point 'as providing the capability for mobile platforms' may help with this point. }
    \item The lack of support for low-level approximation on the mobiles -- until Mobiprox, there was no library that would allow for such approximations to be implemented on mobile devices; 
    %\hashim{Unclear what is fundamental? As in, couldn't existing systems be applied to mobile platforms, what was missing in those systems? Not a strong argument unless we justify better.}
    %\sasa{The argument is good as a supporting one: indeed, the implementation is not trivial and }
\end{enumerate}

In Section 1, page 3, we added a description of these challenges and the way Mobiprox addresses them. 
%\sasa{page number}

%We further discuss the novelty of Mobiprox with respect to network compression approaches (e.g. quantization, pruning, etc.) and the  state of the art dynamic adaptation research (e.g. SPINN, MSDNN, Slimmable Neural Networks, etc.) in answer to Reviewer 2's comment 1.  \sasa{This is different numebr now. Should it be c2? }

\paragraph{}

\textbf{[c6] Clarify the scope and the generalizability of the proposed system (R2,R3)}

\paragraph{}

[r6] Mobiprox is currently applicable to all kinds of convolutional and GEMM-based neural network layers, and is further potentially applicable to other tensor-based programs, such as image and video processing applications. In this work we implemented a number of approximations that are targeting different NN layers. Thus, for all layer types we support quantization from 32b to 16b floats (as long as on-device GPUs support such quantization). For convolutional layers, as they are the most computationally hungry in most DL applications, we implemented additional approximation techniques (i.e. the row and column perforation, and filter pruning). Nevertheless, Mobiprox is extensible with all kinds of tensor-based dynamically-tunable approximations. Adding approximations requires implementing the support primitives, while the existing modular structure of Mobiprox makes it straightforward to include new approximation techniques in network tuning and dynamic adaptation. Our immediate next step in this direction is elaborated in Section 8:

\begin{quote}
    \emph{"To expand the range of approximation techniques, in future we plan to investigate the integration of dynamic pruning [27] of fully-connected layers in Mobiprox"}
\end{quote}

%With respect to generalizabilty, Mobiprox supports all layer types in deep learning, but the approximation of an individual layer is limited by the approximation techniques that are actually implemented for this particular layer type at a low level of the mobile computing stack. Thus, for all layer types we support quantization from 32b to 16b floats (as long as on-device GPUs support such quantization). For convolutional layers, as they are the most computationally hungry in most DL applications, we implemented additional approximation techniques (i.e. the row and column perforation, and filter pruning). To the best of our knowledge, there are no other approximation primitives for, say, LSTM layers. Should they appear in the future, they could be added to Mobiprox. \sasa{Perhaps we can deflect the question here that Mobiprox supports the kinds of approximations that exist in ApproxHPVM. Adding more approximations depends on the support of the primitives and the existing modular structure of the uderlying framework makes it easy to support them. Perhaps Hashim can help here?}

%\hashim{I think we should say Mobiprox is extensible to all kinds of tensor-based approximations that are "dynamically-tunable" - not all approximations are dynamically tunable, e.g., one-shot pruning.}

%\hashim{Also, I think generality should be more clearly spelled out. We can say Mobiprox applies to all kinds of convolution and gemm-based neural networks and is potentially applicable to other tensor-based programs such as image and video processing applications. }

Scope-wise, Mobiprox is not limited to time series data only. %Indeed, we show that an image classification network can be dynamically approximated with Mobiprox (Section 7.2, Figure 5, for example). 
However, \textit{the framework’s dynamic adaptation strategies will perform well only if the link between the context and the required level of adaptation can be established}. Temporal ordering is natural in many mobile computing applications, including the human activity recognition and sound processing domains we experimented with in Section 7.3. Consequently, we designed the most generally applicable adaptation strategies (i.e. the naive, state-based and confidence-based) that, we demonstrated, work well in these domains. We clarify this in Section 5:
\begin{quote}
    \emph {"We harness the natural temporal dependence of the instances of sensed data that is characteristic in many mobile computing applications, and devise three strategies demonstrating that a widely applicable goal of energy minimization can be met with Mobiprox."}
\end{quote}

However, one can envision more specific adaptation strategies that do not count on the temporal dependence on the data, but harness a different contextual dimension. For instance, the approximation level of the network we use in Section 7.3.2 for spoken keyword recognition could have been adapted according to the measured background noise level. This adaptation strategy, however, would both require more engineering work (for explicit noise measurement) and would also be 
of little use outside this immediate application. 


%This does not mean that the approach is limited to time series data or even data where the processes change slowly. To demonstrate this, in addition to the MobileNet architecture used for human activity recognition, in the expanded evaluation section (Section 7) of the new version of the manuscript we also implement a different neural network architecture for spoken keyword recognition. Experiments detailed in Section 7 demonstrate that Mobiprox enables substantial energy savings with an algorithm that adapts the approximation as the levels of the background noise vary. 

\paragraph{}

\textbf{[c7] Discuss accuracy-speedup tradeoff between approximate computation and other model compression techniques, such as low-bit quantization or model pruning (R3)}

\paragraph{}

[r7] Mobiprox does not directly compete with individual model compression techniques. Instead, it represents a wider framework that enables such low-level approximation/compression techniques (including quantization, which the reviewer also mentioned) to be deployed on mobiles in a context-aware dynamically adaptive way. %Prior to Mobiprox, there was no library that would allow for such approximations to be implemented on mobile devices. 
%\hashim{This is not true imo. DeepCompression, Intel Neural compressor tools, Neural magic compression tools, Nvidia Tensor RT, TVM, are all equally applicable to mobile devices. A structured pruning/quantization framework that works for the desktop is equally applicable to Mobile. Does Mobiprox provide something fundamentally different or better?}
%Second, Mobiprox is the first framework to allow for changing between running with or without approximation (with or without such model compression techniques) on the same neural network. \hashim{ApproxTuner did this}\sasa{add 'for mobiles' qualifier}  Existing network compression techniques normally leave the network permanently impaired (e.g. quantization normally gives you a permanently quantized network), \hashim{Dynamic quantization and dynamic pruning approaches have been proposed} whereas Mobiprox enables running inference with the both uncompressed/unapproximated network and the network with various compression techniques applied. Finally, 
Consequently, one of the main strongpoints of Mobiprox is its ability to adjust the level of approximation/compression in real time to achieve a variable trade-off between QoS and energy consumption. This context-aware dynamic adjustment of the compression level is innovative and was not, prior to Mobiprox, available for use in the case of traditional model compression techniques. 
%\hashim{This is a strong argument and a sufficient argument. We can leave out the weaker arguments imo} \sasa{I agree with Hashim that this is the main argument to push.}

To conclude, the approximate computation that Mobiprox enables actually includes \emph{model compression techniques} such as low-bit quantization, but it also includes much more: the ability to dynamically switch between running inference with or without such compression techniques enabled, and the possibility to \textit{adapt the intensity of the approximation (the level of compression) according to the context of use}.

\paragraph{}

\textbf{[c8] Consider more than one model for the adaptation evaluation (R2).}

\paragraph{}

[r8] Based on the R2's comment, we performed additional experimental validation of Mobiprox using a different NN model and an additional application domain (spoken keyword recognition). Regarding the model, we used a compact keyword recognition network from a well-cited paper. The network consists of two convolutional layers and one fully-connected layer with the softmax output (more details on the model are provided in Section 7.3.2) and is significantly different than MobileNet architecture used in the HAR experiments. The network was trained on a subset of Google Speech Commands v0.01 dataset (detailed in Section 6). For the validation of this new scenario we constructed a trace and ran it using the confidence-based adaptation engine, with the results showing Mobiprox achieving a 15\% saving in system-wide energy consumption with the inference accuracy remaining on-par with the non-approximated model (additional details in Section 7.3.2). 

\paragraph{}

\textbf{[c9] Clarify if there is any limitations of the Mobiprox implementation to be adopted on smartphones (R2).}
%\sasa{The summary of the q is vague. Was the reviewer complaining explicitly about using board vs mobile (as the answer implies) or also looking for a broader limitation set?}

\paragraph{}

[r9]  Mobiprox's dynamic adaptation of approximate deep learning indeed runs on Android phones. As stated in Section 7.4.:
\begin{quote}
    \emph{``Furthermore, to confirm that Mobiprox can be used on a smartphone in real time, we run approximated HAR inference, guided by a confidence-based adaptation strategy, directly on the phone as the data is getting collected.'' }
\end{quote}
%\sasa{We should also make the point in the intro that we evaluate on the board for energy and mobile for functionality. }

%\sasa{Do we want to draw attention to those numbers again? I forgot if the reviewers found the numbers sufficiently good; but if they see them out of the context may cause more questions. We should have these sentences in the camera-ready version though. }
%To additionally clarify this, we add the following to the manuscript abstract:
%\begin{quote}
%\emph{``We implement Mobiprox in Android and through a 10-user experiment with commodity smartphones demonstrate that it can achieve up to 91\% agreement with a non-approximated neural network for human activity recognition while saving up to 15\% energy.''}
%\end{quote}

%We perform a series of experiments on both a development board (for precise energy measurements) and commodity smartphones (for realistic user experience), using different NN architectures and multiple application domains, including human activity classification and spoken keyword recognition. Our evaluation demonstrates that in both scripted and unscripted scenarios, by adapting to the varying context (i.e. input data difficulty), Mobiprox can bring significant energy savings while preserving the inference accuracy.

The development board was used exclusively to evaluate the energy consumption. To the best of our knowledge there is no reliable way to assess the energy consumption of a modern unibody smartphone that does not allow for a battery to be removed. Software tools that report battery consumption do not operate at the (temporal and measurement precision) granularity that is sufficient for assessing the savings enabled by dynamic deep learning approximation. Therefore, we used a development board that runs a standard Android OS stack and powered the board through a high-fidelity power meter. %\hashim{Is this para needed to answer the question?}

We clarify this in Section 6 where we now state:
\begin{quote}
    \emph{``Mobiprox is fully compliant with consumer off-the-shelf Android devices. Yet, modern unibody smartphones do not allow for batteries to be easily removed, precluding the use of high-accuracy power metering, which is essential for evaluating the energy savings enabled by Mobiprox. Therefore, when energy consumption is examined, we use ASUS TinkerBoard S development board running Android 7 OS.''}
\end{quote}

We also modify the last bullet point of the Introduction, which clarifies that:
\begin{quote}
    \emph{`` We perform a series of experiments on both adevelopment board (for precise energy measurements) and commodity smartphones (for realistic user experience)''}
\end{quote}

Finally, we confirm that the execution of approximate operations remains comparable across different hardware and in Section 7 we state:

\begin{quote}
    \emph{`` While each Android device has its own specific power profile, the experimental measurements we perform are still relevant to identify the ordering of the approximate configurations in the QoS loss – Speedup space. This ordering directly impacts the adaptation strategies and, at least over the devices we had access to (Samsung Galaxy S21, Samsung Galaxy M21, Xiaomi Pocophone F1, and ASUS TinkerBoard S), the ordering remains the same regardless of the device specifics.''}
\end{quote}

%\sasa{We should add which Android phones we tested the funcionality of Mobiprox for  Sec 7.3/7.4 . Is there some set of results that we can draw that confirm that the phone's execution is similar to board's (e.g. app-level accuracy) ? }

\paragraph{}


%\sasa{Read the points for the following reviewers but it looks most of those are already integrated in the previous answers. }
