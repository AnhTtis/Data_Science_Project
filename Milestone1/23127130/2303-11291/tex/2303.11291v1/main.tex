%%
%% This is file `sample-acmlarge.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmlarge')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmlarge.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
%\documentclass[acmlarge,anonymous]{acmart}
\documentclass[acmlarge]{acmart}
%\documentclass[manuscript,screen,review]{acmart}
%\documentclass[sigconf, 10pt]{acmart}
%% NOTE that a single column version is required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen,review]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%\pagenumbering{arabic}
%\settopmatter{printfolios=true}
%\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[inline]{enumitem}
\floatname{algorithm}{\footnotesize Algorithm}
\usepackage[section]{placeins}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2022}
\acmYear{2022}
\acmDOI{10.1145/1122445.1122456}


%%
%% These commands are for a JOURNAL article.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


\newcommand{\hashim}[1]{\textcolor{cyan}{Hashim: #1}}

\newcommand{\sasa}[1]{{\color{blue} #1}}

\newcommand{\veljko}[1]{{\color{olive} #1}}

\newcommand{\blue}[1]{{\color{black} #1}}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Mobiprox: Supporting Dynamic Approximate Computing on Mobiles}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Matevž Fabjančič}
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Faculty of Computer and Information Science, University of Ljubljana}
  \country{Slovenia}
}

\author{Octavian Machidon}
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Faculty of Computer and Information Science, University of Ljubljana}
  \country{Slovenia}
}

\author{Hashim Sharif}
\affiliation{%
 \institution{University of Illinois at Urbana-Champaign}
 \city{Urbana-Champaign}
 \state{Illinois}
 \country{United States}}

 \author{Yifan Zhao}
\affiliation{%
 \institution{University of Illinois at Urbana-Champaign}
 \city{Urbana-Champaign}
 \state{Illinois}
 \country{United States}}

 \author{Saša Misailović}
\affiliation{%
 \institution{University of Illinois at Urbana-Champaign}
 \city{Urbana-Champaign}
 \state{Illinois}
 \country{United States}}
 
\author{Veljko Pejović}
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Faculty of Computer and Information Science, University of Ljubljana}
  \country{Slovenia}
}
\affiliation{%
  \institution{Department of Computer Systems, Institute ``Jožef Stefan''}
  \country{Slovenia}
}




%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Fabjančič et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

Runtime-tunable context-dependent network compression would make mobile deep learning adaptable to often varying resource availability, input ``difficulty'', or user needs. The existing compression techniques significantly reduce the memory, processing, and energy tax of deep learning, yet, the resulting models tend to be permanently impaired, sacrificing the inference power for reduced resource usage. The existing tunable compression approaches, on the other hand, require expensive re-training, seldom provide mobile-ready implementations, and do not support arbitrary strategies for adapting the compression. 

In this paper we present Mobiprox, a framework enabling flexible-accuracy on-device deep learning. Mobiprox implements tunable approximations of tensor operations and enables runtime adaptation of individual network layers. A profiler and a tuner included with Mobiprox identify the most promising neural network approximation configurations leading to the desired inference quality with the minimal use of resources. Furthermore, we develop control strategies that depending on contextual factors, such as the input data difficulty, dynamically adjust the approximation level of a model. \blue{We implement Mobiprox in Android OS and through experiments in diverse mobile domains, including human activity recognition and spoken keyword detection, demonstrate that it can save up to 15\% system-wide energy with a minimal impact on the inference accuracy.}
%run it on a recent trace of smartphone sensors' data, and show that it can save up to 15\% of the system-level energy with only 2\% of the inference accuracy loss, when used for continuous human activity recognition.

%The affordances of deep learning are often counterbalanced by the burden that deep learning imposes on resource-constrained edge devices. While a range of compression techniques significantly reduce the memory, processing, and energy tax of deep learning, the resulting models tend to be permanently impaired, sacrificing the inference power for reduced resource usage. Mobile computing, on the other hand, is highly dynamic and while compressed models perform sufficiently well most of the time, occasions when high-accuracy models are necessary may arise. In this paper we present Mobiprox, a framework enabling flexible-accuracy on-device deep learning. Mobiprox implements tunable approximations of basic deep learning operations and enables runtime-adaptable approximation of individual neural network layers. Mobiprox includes a profiler that, in concert with an external tuner, identifies the most promising neural network approximation configurations leading to the desired inference quality with the minimal use of a mobile’s resources. Finally, with Mobiprox we develop a suite of control algorithms that depending on the context-dependent factors, such as the input data’s difficulty, dynamically adapt the approximation level of a deep learning model. Through a 21-person user study we show Mobiprox can save up to 15\% of the system-level energy with only 2\% of the inference accuracy loss, when used for continuous human activity recognition from on-body sensor data.  
 
% \hashim{The last part of the abstract talks about the novel component. The "context-dependent" runtime tuning to me is the new exciting idea here. The start of the abstract focuses on dynamically tunable approximations - this is less novel given other systems have done it (ApproxTuner, Green, MCDNN, Dynamic pruning techniques) }
 
\end{abstract}


%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003138</concept_id>
       <concept_desc>Human-centered computing~Ubiquitous and mobile computing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010293.10010294</concept_id>
       <concept_desc>Computing methodologies~Neural networks</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Ubiquitous and mobile computing}
\ccsdesc[500]{Computing methodologies~Neural networks}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{approximate computing, context-awareness, mobile deep learning, ubiquitous computing}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\newcommand{\mnCifar}{\texttt{mobilenet\_cifar10}}
\newcommand{\mnUci}{\texttt{mobilenet\_uci-har}}
\newcommand{\anCifar}{\texttt{alexnet2\_cifar10}}
\newcommand{\vggCifar}{\texttt{vgg16\_cifar10}}
\newcommand{\resUci}{\texttt{resnet50\_uci-har}}


\input{tex/introduction}

\input{tex/related_work}

\input{tex/preliminaries}

\input{tex/mobiprox}

\input{tex/strategies}

\input{tex/methodology}

\input{tex/evaluation}

\input{tex/discussion}

\input{tex/conclusion}


\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}
\endinput
%%
%% End of file `sample-acmlarge.tex'.
