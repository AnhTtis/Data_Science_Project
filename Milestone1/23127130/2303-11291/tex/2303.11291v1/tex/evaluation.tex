\section{Evaluation}
\label{sec:evaluation}

%\sasa{Consider having a section Methodology that will contain all the details of your experimental setup, hardware, inputs, benchmarks, accuracy metrics, etc.}

%\sasa{Also, consider having explicit research questions that you want to present. I.e. direct readers: what questions do sections 5.1 and 5.2 answer?h}

In our evaluation of the Mobiprox framework we aim to provide an answer to the following research questions:
\begin{itemize}
    \item \emph{RQ1:} Usability: time to find configurations and generalizability across different devices?
    \item \emph{RQ2:} Generalizability across NN models: how does Mobiprox perform across different neural network models and classification tasks, in terms of accuracy, speedup, and energy saved?
    \item \emph{RQ3:} What energy savings would Mobiprox bring in a real world scenario and at what trade-off with regard to inference accuracy?
\end{itemize}
    
\subsection{Configuration identification time and generalizability}

In Mobiprox, the identification of suitable approximation configurations is split into two phases: \textit{i)} identifying the candidate Pareto front among all possible configurations and \textit{ii)} measuring each configuration's speedup and inference accuracy on the target platform. The first part of the configuration identification process relies on ApproxTuner and is performed on a CUDA GPU-enabled machine. The second part of the process must be executed on a machine (e.g. a PC or a laptop) that connects to the target mobile platform via Android Debug Bridge (ADB). In this step, the candidate configurations get executed directly on the mobile, thus, what matters are the capabilities of the connected mobile platform, not the machine that controls the execution.

In our experiments we use a single node of a grid supercomputer equipped with Nvidia Quadro GV100 GPUs for the first phase of the configuration identification process. The node uses a single GPU for the tuning task, which is executed in a batch processing mode. In Table~\ref{tab:profiling_time_server} we list the times needed for finding the Pareto front of the configurations for different networks used in our experiments. The heuristic search done by ApproxTuner is not deterministic, thus different runs may be completed in slightly different amounts of time. In addition, subsequent tunings of the same network often take significantly less time, as they build upon already cached results. In any case, we observe that the even the most complex tuning (for resnet50\_uci-har) completes in less than 30 minutes. 

\begin{table}[!htbp]
    \caption{Time needed for identifying Pareto-optimal configurations using a single GPU on a supercomputer. Standard deviations are in the parentheses. Note that the duration may vary with different search parameters\protect\footnotemark.}
    \begin{center}
    \begin{tabular}{lll}
DL model  & Tuning time -- initial [s]  & Tuning time -- subsequent [s]\\ \hline
alexnet2\_cifar & 574 (97)  & 184 (6)\\ 
mobilenet\_cifar10 & 1301 (226) & 370 (18)\\ 
vgg16\_cifar10 & 1216 (62) & 264 (2) \\
resnet50\_uci-har & 1602 (316) & 276 (14)\\ 
mobilenet\_uci-har & 1203 (246) & 253 (1)\\ 
\end{tabular}
    \end{center}
\label{tab:profiling_time_server}
\end{table}
\footnotetext{The ApproxTuner search parameters are described at \url{https://predtuner.readthedocs.io/en/latest/reference/index.html}}

For the second phase of the configuration identification process we use an ASUS TinkerBoard S, which, with its Rockchip RK3288 system-on-chip released in 2014, represents a lower-end mobile platform. In Table~\ref{tab:profiling_time_client} we list the times needed for measuring the classification accuracy and the speedup for all Pareto front configurations found in the first phase of the identification process. The size of the test dataset, the batch size, as well as the number of configurations in the Pareto front varied for different networks. Nevertheless, this one-off process completes in less than an hour even on our low-end mobile platform, thus, we conclude that Mobiprox can be comfortably used within the existing Android application compilation process. 

\begin{table}[!htbp]
    \caption{Time needed for profiling Pareto-optimal configurations' speedup and accuracy on a low-end mobile device connected to a laptop.}% Non-approximated network is compared with dynamically adapted approximation provided by Mobiprox.}
    \begin{center}
    \begin{tabular}{lllll}

DL model  & Batch size & Dataset size & Num. of configs. & Profiling time [mins] \\ \hline
alexnet2\_cifar & 100 & 800     & 20 & 36 \\ 
mobilenet\_cifar10 & 100 & 800     & 20 & 58 \\ 
vgg16\_cifar10 & 25 & 200     & 20 & 42 \\ 
resnet50\_uci-har & 50 & 250     & 20 & 26 \\ 
mobilenet\_uci-har & 145 & 1450     & 10 & 42 \\ 
\end{tabular}
    \end{center}
\label{tab:profiling_time_client}
\end{table}

To fully answer RQ1, we now discuss the generalizability of an application compiled with Mobiprox. Mobiprox requires an actual mobile hardware for the second part of the configuration identification process. Tens of thousands of different Android devices exist on the market, however, the choice of which to use should not have a major impact on the approximate configuration profiling results. First, despite different hardware, the platforms use the same OpenCL-based primitives we developed in Section~\ref{sec:android-rt}. Second, while the speed at which neural network will be executed may differ among different devices, there is no reason to expect that speedups (relative to a non-approximated network) will be different for the same configuration ran on different devices. This is especially true if these devices belong to the same architectural category. Currently, Android apps can be built for four such categories, i.e. Application Binary Interfaces (ABIs). Yet, 99\% of the smartphones rely on one of the two ARM-based ABIs\footnote{\url{https://stackoverflow.com/questions/46453457/which-android-abis-cpu-architectures-do-i-need-to-serve}}, armeabi-v7a and arm64-v8a, both of which we successfully tested through ASUS TinkerBoard S and Samsung Galaxy \blue{phones} we used in our experiments\footnote{It should be noted that the APK can contain builds for different ABIs even if these ABIs were not used during the profiling.}. While  the above assumptions can be confirmed only through experimentation with a very large number of devices, we note that, for the deep learning models presented in this paper we have not observed any differences in ordering among speedups obtained by different configurations on the platforms we have experimented with. 

\subsection{Popular CNN benchmarks}
\label{sec:evaluation_microbenchmarks}
% \newcommand{\MPM}{Monsoon Power Monitor}
% \newcommand{\tboard}{ASUS TinkerBoard S}

% To assess the efficacy of our approximations, we have modified ApproxHPVM's profiler to use ADB\footnote{Android Debug Bridge: \url{https://developer.android.com/studio/command-line/adb}} to run compiled approximated neural networks on the {\tboard}\footnote{\tboard: \url{https://tinker-board.asus.com/product/tinker-board-s.html}} development board. We used the \MPM{} as the board's power supply, which allowed measuring energy consumption accurately. 

% According to the \tboard{}'s specification, the board requires a 5V power supply capable of providing current of up to 3A. However, our configuration of the \MPM{} at 5V was not sufficient for the board. We have detected issues that were not present when using a conventional power supply -- poor classification accuracy of various neural networks and an overall unpredictable behaviour of our software. These issues were resolved after increasing the voltage of the \MPM{} to 5.25V.

% We ran the HPVM profiler on our selection of neural networks.
% Profiling for each neural network was executed on a predefined fraction of the data in 8 batches for CIFAR-10 networks and in 10 batches for UCI-HAR.
% These fractions are shown in Table~\ref{tab:data-fracs}.
% This was done to 
%     \textit{i)} reduce overall time requirement,
%     and
%     \textit{ii)} obtain more robust measurements by measuring each batch separately.
% When we obtained energy-consumption traces given by the \MPM{}, we used the PowerTool\footnote{PowerTool: \url{https://www.msoon.com/hvpm-software-download}} software to extract energy consumption for each approximation configuration. By measuring each batch separately, we obtained multiple measurements for each approximation configuration. From this data we calculated the mean and standard deviation of energy consumption.

% \begin{table}
%     \centering
%     \caption{Data-set sizes used for energy profiling.}
%     \begin{tabular}{llcc}
%         \toprule
%         Neural Network & Data set & No. images & Batch size \\
%         \midrule
%         \vggCifar{} & CIFAR-10 & 200  & 25  \\
%         \anCifar{}  & CIFAR-10 & 800  & 100 \\
%         \mnCifar{}  & CIFAR-10 & 800  & 100 \\
%         \mnUci{}    & UCI-HAR  & 1450 & 145 \\
%         \resUci{}   & UCI-HAR  & 250  & 50  \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:data-fracs}
% \end{table}

To answer \emph{RQ2} we obtain approximation configuration sets for NNs trained on CIFAR-10 and UCI-HAR data sets through Mobiprox tuner\footnote{For clarity we limit the number of configurations and direct the tuner to not consider configs. that lead to QoS loss below a given threshold.} and plot the profiling results in Figures~\ref{fig:energy-cifar} and~\ref{fig:energy-uci} respectively. Note that the reported energy consumption reduction is \textit{system-level}, i.e. the idle consumption was not subtracted. We can observe a key difference between approximation configurations of different NN architectures -- larger networks, such as VGG and AlexNet, are more amenable to approximation, and Mobiprox brings higher energy savings with these networks. This may indicate that certain models that are prohibitively expensive for mobile DL could, with the help of Mobiprox, be made mobile-ready even without any retraining necessary. 

%overall, the QoS (accuracy) loss variation on CIFAR-10 neural networks is greater compared to UCI-HAR neural networks. 

%We can observe a key difference between approximation configurations of neural networks on the two data sets -- overall, the QoS (accuracy) loss variation on CIFAR-10 neural networks is greater compared to UCI-HAR neural networks. 

%Additionally, the density of trade-off points at QoS loss of $3.0$ is greater for UCI-HAR networks. This shows how different tuning techniques (predictive, empirical) impact the actual accuracy losses of approximated neural networks. In both cases the tuner was guided into producing neural networks with an accuracy loss of $3.0$. However, the predictor's inability to predict accuracy loss is particularly evident for \anCifar.
%\hashim{I'm not following how that is true. Also we should talk in more detail about this over call sometime. Our predictive tuning generally works quite well and the above statement gives a strong verdict against it. Let's understand your findings more. Some questions and points are: a) how many tuning iterations you used? I ask because if you give large number of iterations with predictive you'll start to get comparable results to empirical and the overall time budget would be much lower, b) the graph don't show the difference of predicted vs empirical tuning on the same benchmarks, 5a is showing predictive on CIFAR-10 and 5b is showing empirical tuning result on UCI-HAR, c) the open-source implementation doesn't yet include our on-tuning tuning (of linear regressor predictor) functionality -- this could be causing relatively poor results for you. }
%\hashim{Given these confusions, my suggestion is to not go into details of predictive vs empirical because its not as thoroughly investigated yet. It's not necessary for the story of this paper anyway, or is it? Predictive tuning doesn't seem to be a major theme - and evaluating a technique of an already published paper (ApproxTuner) doesn't add too much new exciting factor}

\begin{figure}[!htb]
    \newcommand{\w}{0.45\linewidth}
    \centering
    \begin{subfigure}{\w}
        \centering
        \includegraphics[width=\linewidth]{figures/fig-cifar10_v2.png}
        \caption{CIFAR-10}
        \label{fig:energy-cifar}
    \end{subfigure}
    \hspace{1cm}
    \begin{subfigure}{\w}
        \centering
        \includegraphics[width=\linewidth]{figures/fig-uci-har_v2.png}
        \caption{UCI-HAR}
    \label{fig:energy-uci}
    \end{subfigure}
    
    \caption{\blue{System-wide energy consumption (relative to no approximation) of an \tboard{} running inference on NNs trained on CIFAR-10 and UCI-HAR datasets. Different point types correspond to different NN architectures; each point represents a single approx. configuration. The x-axis represents the actual QoS loss from the model deployed on a mobile device.}}
    \label{fig:energy}
\end{figure}


% \begin{figure}[!htb]
%     \newcommand{\w}{\linewidth}
%     \centering
%     \begin{subfigure}{\w}
%         \centering
%         \includegraphics[width=\linewidth]{figures/pareto-all.pdf}
%         \caption{Filtered Pareto frontier of generated approximation configurations. Red line shows the approximations generated by running ApproxTuner at various QoS thresholds.}
%         \label{fig:combined-pareto}
%     \end{subfigure}\\
%     \vspace{1cm}
%     \begin{subfigure}{\w}
%         \centering
%         \includegraphics[width=\linewidth]{figures/pareto-android.pdf}
%         \caption{Configurations from figure~\ref{fig:combined-pareto} profiled on \tboard{}. Configurations that yield worse speedup at the cost of accuracy are marked as outliers.}
%         \label{fig:combined-pareto-android}
%     \end{subfigure}
%     \caption{Analysis of combined configuration sets. Note different scaling of y-axis.}
% \end{figure}

%These clusters of configurations at similar QoS loss values do not improve our trade-off curve, as only the best performing configuration would be selected.
%We improve the distribution of configurations by running separate tuning steps with various QoS loss targets given to ApproxTuner: $1.0$, $2.0$, $4.0$, $6.0$, $8.0$ and $10.0$. Once we have separate trade-off curves generated at these thresholds, we apply a filtering algorithm on the union of new configurations, described by Algorithm~\ref{alg:pareto-construciton}.

% \begin{algorithm}
% \caption{Configuration set filtering}
% \label{alg:pareto-construciton}

% \begin{algorithmic}[1]
% \STATE sort input configurations with increasing QoS loss
% \STATE kept configurations = [\;]
% \FORALL{$C$ in input configurations}
%     \IF{$C$ yields higher speedup than last kept configuration}
%         \STATE keep configuration $C$
%     \ENDIF
% \ENDFOR
% \RETURN kept configurations
% \end{algorithmic}

% \end{algorithm}

%Once combined and filtered, 
It is interesting to juxtapose the measured energy savings with the speedup expected at the tuning time. In Figure~\ref{fig:combined-pareto} in Section~\ref{sec:mobiprox:charting} we show that the server-based profiling indicates that our approximations can lead to more than  $2.5\times$ speedup of inference on the MobileNet architecture trained on the UCI-HAR dataset. The actual energy consumption reduction is much lower and is in line with the speedup measurements presented in Figure~\ref{fig:combined-pareto-android}. We believe that unlocking the full potential of approximation on the mobile platform requires careful consideration of the mobile processing hardware. The overheads and the inefficiency of thread scheduling in the ARM Cortex-A17 computing architecture on which the experiments were performed could be a likely culprit~\cite{wang2021asymo}.

%Using these configurations we obtain a new Pareto frontier of approximation configurations as shown in Figure~\ref{fig:combined-pareto}. We again used the \tboard{} to measure how these approximations impact performance. The results are shown in Figure~\ref{fig:combined-pareto-android}. We observe equal $X$-axis distribution of trade-off points, which is expected. However, similar to our previous measurements, we measure only speedups of up to $1.25$ at highest levels of approximations. Across the board, our approximations yield speedups far from the speedups of $2.0$ and more measured on an Nvidia~V100~GPU.
In Figure~\ref{fig:energy-eff-uci} we analyse how speedups translate to energy consumption reduction. We show that energy consumption is reduced by a larger factor compared to computation time reduction. This result is surprising and indicates that it is not only the inference time that is reduced, but the power consumption as well. We suspect that either the automatic voltage-frequency scaling or thread scheduling (i.e. approximated NNs keeping fewer threads active) could be the cause.
%can be explained by the heterogeneous computing architecture on which the experiments were performed. Such architectures allow for higher energy savings when using the little cluster cores, with a more limited speed-up compared to big cluster execution~\cite{hahnel2014heterogeneity}.
%This is a surprising result, as energy consumption measurement includes energy required by an idle device, but any constant, continuous energy consumption is expected to negatively impact relative energy savings.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/energy-replot-combined.pdf}
    \caption{Relative energy consumption compared to relative inference time reduction for \mnUci{} at various trade-off points (approximation configurations). The x-axis represents the actual QoS loss from the model deployed on a mobile device.}
    \label{fig:energy-eff-uci}
\end{figure}

%\FloatBarrier

In the end, we note that the adaptation strategy calculation, i.e. deciding which approximation level to use, plays virtually no role in the overall energy consumption. Irrespective of which of the three strategies presented in Section~\ref{sec:strategies} we employ, the process boils down to either \textit{i)} comparing the current and the past predicted class (``naive'' strategy), \textit{ii)} assessing the equality of $M$ predictions, where $M$ is a small integer, and comparing the updated integer reliability metric $V$ with a constant threshold  (``state-driven'' strategy), or \textit{iii)} comparing the softmax confidence with a constant threshold (``confidence-driven'' strategy). Each of these calculations is performed in a constant time that is negligible compared to an execution time of even a single neural network operation. The choice of the strategy, however, impacts the levels that the 
neural network will be approximated with. Thus, in the remainder of the evaluation we examine the mobile deep learning accuracy and energy efficiency afforded by different approximation adaptation strategies.

%\subsection{User Study}
\subsection{Adaptation strategy evaluation}

%\hashim{The experiment is quite interesting but this description and text doesn't do justice. It just talks about the high-level takeaways. You can dive more deep into what kind of insights you observe when say human activity changes from X to Y -- is it harder to detect some scenarios (walking up the stairs) and how do different strategies behave in those kind of difficult scenarios? I'm sure you have that data. It'll add a lot of value to add more depth and insights to this section. I won't even call it a "user study". IMO, this is the main evaluation. Calling it "user study" makes it sound less important and a reviewer may even skip over it. }

%\sasa{It would be good to discuss here (and in the intro) what is the 'quality' that matters to the user. In this context, this should answer the question what an acceptable accuracy threshold is -- .e.g, why is 0.65 acceptable to the user}
% We conducted a study to evaluate the performance of Mobiprox in terms of inference accuracy vs. energy savings on a human activity detection task using real-world data. Our study involved 21 volunteers, 13 male and 8 female participants, with an average age of 29 and a standard deviation of 12 years.  Given that we initially validated the Mobiprox framework in-lab on CNNs trained on the UCI HAR dataset, we ensure that our study is conducted under similar conditions to those described in the original study. Consequently, the study participants were equipped with a UDOO Neo Full board battery-powered and waist-mounted to accurately replicate the positioning of the smartphone in the original UCI HAR experiment. In addition, we instructed the study participants to perform the same physical activities with the ones present in the original UCI HAR experiment. 

% The UDOO Neo Full board is a compact IoT embedded computing device equipped with a 3-axis accelerometer and a digital gyroscope. We chose this platform as its processing hardware corresponds to that found in today's low-end smartphones, yet, the platform itself runs Linux enabling a quick prototyping of DL pipelines. We sample the acceleration and angular velocity in all three axes from the board's sensors with a frequency of 50 Hz using the Neo.GPIO library~\cite{neogpio}.

% The study experiments took place in a university campus building at our institution's premises. All volunteers performed the six activities in a row, first the static ones (sitting in a chair, standing still and lying down) for 2 minutes each. Then, they performed the dynamic activities: walking up and down a hallway (summing up to a total time of two to three minutes for each participant) and walking down and up the stairs (about 45 seconds in each direction, the duration being limited by the total number of stairs).


\blue{To answer \emph{RQ3} we assess the Mobiprox's ability to deliver significant energy savings when adaptation, according to the strategies developed in Section~\ref{sec:strategies}, is performed in realistic dynamic scenarios. This we evaluate in two unrelated domains of mobile deep learning: human-activity recognition from accelerometer samples and keyword recognition from microphone recordings. }





%To answer \emph{RQ3} we assess the Mobiprox's ability to deliver significant energy savings when adaptation, according to the strategies developed in Section~\ref{sec:strategies}, is performed in an environment different from the one in which the original training data was obtained. 

\subsubsection{\blue{Human activity recognition}}

We first analysed the pre-collected HAR traces described in Section~\ref{sec:methodology} by computing descriptive statistics (mean and standard deviation) across all inertial signals on all three axes. This statistical analysis revealed that for 6 of the 21 users the angular velocity data was corrupted (deviating from the norm by an order of magnitude more than for the other users) which prevented even the non-approximated classifier to produce meaningful inferences on these users' data. Further communication with a dataset author revealed that the corrupt data was collected at a separate day from the rest of the dataset indicating a systematic error, thus we remove these 6 users from further experimentation.  

We then used the Mobiprox pipeline to perform inference on the pre-collected HAR traces. We evaluated all three adaptation strategies from Section~\ref{sec:strategies} and compared the results with the ones obtained by the non-approximated MobileNet-V2 (Table~\ref{tab:all_engines}). For each strategy we choose the option for moving to more aggressive approximations (linear vs. exponential) that yielded the best results. 

\begin{table}[!htbp]
    \caption{Inference accuracy and energy consumption on the HAR traces from~\cite{electronics10232958} for MobileNet-V2 trained on the UCI-HAR dataset.}% Non-approximated network is compared with dynamically adapted approximation provided by Mobiprox.}
    \begin{center}
    \begin{tabular}{llll}

Adaptation  & Incr. & Accuracy & Relative Energy \\ \hline
Non-approximated & - & 0.65     & 1.0        \\ 
Confidence-based    & Expon.   & 0.63     & 0.854        \\ 
State-based ($V_{L}=2$)         & Linear   & 0.63     & 0.867        \\ 
%Kalman          & Linear & 0.61     & 210.7        \\ 
Naive       & Expon. & 0.64     & 0.946        \\ 
\end{tabular}
    \end{center}
\label{tab:all_engines}
\end{table}

These results show that all adaptation engines are more energy efficient than the vanilla MobileNet-V2 with a small drop in average accuracy. The optimal trade-off between the energy saved and the drop in accuracy is obtained using the Confidence-based adaptation engine, which is ~15\% more energy efficient with just a ~2\% drop in overall average accuracy. The accuracy results are modest (the accuracy of the non-approximated network on the UCI-HAR test set was 90\%), which is to be expected given that we used a network trained on a dataset collected in one environment for performing human activity inference on data collected in a completely different environment. Thus, the results are more in line with other efforts involving HAR on free-living data for using networks trained on the UCI-HAR dataset~\cite{cruciani2020feature}.
%free living data~\cite{10.1145/2526667.2526685}.

Finally, to understand whether the accuracy-energy impact is uniform across the users, in Figure~\ref{fig:acc-vs-energy} we show the average accuracy vs. average energy consumption for each user trace for both the non-approximated network and the approximated network using the confidence-based adaptation engine. It is observable from this plot that there is a general trend in the reduction of the energy consumption while maintaining the same performance in terms of accuracy of the classification.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/output4.png}
    \caption{Average accuracy vs. average energy consumption for each user for the non-approximated network and the confidence-based adaptation.}%\sasa{presentation: redo the plot in the same tool as other plots.} \hashim{I'm not following why they are multiple blue points. As in how can the non-approximate CNN have differing accuracy values? Since its the baseline with no approximation it would have a single accuracy value, right?}  }
    \label{fig:acc-vs-energy}
\end{figure}


\subsubsection{Spoken keyword recognition}  

\blue{Mobiprox approximation strategies are not restricted to a particular domain. Thus, we also demonstrate approximation adaptation of a DL model in the domain of spoken keyword recognition. Understanding voice commands is a critical affordance in many ubiquitous computing settings, such as, for providing driving assistance, or smart home functionalities. 

From a number of DL models have been crafted for spoken keyword recognition we focus on CNN-based models introduced by Sainath and Parada~\cite{sainath2015convspeech}. The family of models presented in this work is light-weight, both in terms of memory usage and computation requirements, thus, well-suited for mobile devices. We adopt a particular PyTorch implementation of a model from this family consisting of two convolutional layers and one fully-connected layer with the softmax output presented in~\cite{tang2017honk}. The code accompanying~\cite{tang2017honk} already contains the DL model parameters obtained through training on the 80\% of the GSC dataset (validation on 10\%), and we reuse these parameters in our network instance. This model is then funneled to Mobiprox's on-server and later on-device tuning on the ASUS Tinkerboard S to obtain a 10-point Pareto front of approximate configurations of the network. For tuning we use a half of the 10\% of the GSC that was not used for the training/validation.


%Investigating a minimum-footprint network specifically built for mobile devices helps us understand both the ability of Mobiprox to identify promising approximations beyond the optimizations uncovered through hand tuning, but also the potential of Mobiprox dynamic adaptation in case the ``dynamic range'' of configurations is modest. 

%We use the Google Speech Commands (GSC) v0.01 Dataset~\cite{warden2018speech} containing 65,000 one-second long utterances of 30 short words by thousands of different speakers. Interested in the recognition of keywords in realistic situations, where a word has to be spotted in sound segments that contain both words we are not interested in as well as recordings of silence, we follow the approach presented in ~\cite{tang2017honk} and use twelve classes for ten selected keywords (yes, no, up, down, left, right, on, off, stop and go) and two extra classes: ``unknown'' (for the remaining 20 words in the dataset) and ``silence''. 

Opportunities for dynamic approximation in the spoken keyword recognition domain come with a naturally-varying level of background noise. For instance, it has been show that when different levels of noise are present, a different complexity of a DL model is needed to successfully recognize spoken keywords~\cite{machidon2022keyword}. In our experiments we examine how the adaptation strategies developed in Section~\ref{sec:strategies} cope with time-varying noise levels. For this, we first construct a trace consisting of 160 word utterances from a previously unseen part of the GSC dataset mixed with time-varying white noise level corresponding to the level measured in a realistic environment~\cite{flamme2012typical} (Figure~\ref{fig:noise}). The trace is then used as an input to our mobile DL model for keyword recognition, while a Mobiprox's adaptation strategy decides on which approximation configuration to use at subsequent inference step. Unlike in the HAR experiment, in this experiment there is no notion of ``state'' (e.g. a period of time during which a user is likely to keep performing the same activity), rather, keywords are randomly distributed in the trace. Thus, we do not evaluate the state-based adaptation nor the naive method, but focus on the confidence-based adaptation from Section~\ref{sec:strategies}).}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/noise_distribution.png}
    \caption{\blue{Noise distribution for the keyword recognition trace: 160 noise level values, one for each of the samples from the trace. Each sample contains one random utterance (from the 12 classes) on which noise is added with the level specified according to the distribution of the ambient noise during a regular day~\cite{flamme2012typical}.}}
    \label{fig:noise}
\end{figure}


\begin{table}[!htbp]
    \caption{\blue{Inference accuracy and energy consumption on the spoken keyword recognition task for a network introduced in~\cite{tang2017honk}.}}% Non-approximated network is compared with dynamically adapted approximation provided by Mobiprox.}
    \begin{center}
    \begin{tabular}{llll}
Adaptation  & Incr. & Accuracy & Relative Energy \\ \hline
Non-approximated & - & 0.96     & 1.0        \\ 
Confidence-based    & Expon.   & 0.96     & 0.852        \\ 
Confidence-based         & Linear   & 0.96     & 0.852        \\ 
%Kalman          & Linear & 0.61     & 210.7        \\ 
\end{tabular}
    \end{center}
\label{tab:all_engines_sound}
\end{table}

\blue{
We run the above trace on an ASUS Tinkerboard S device connected to a Monsoon power meter. We run both the original compact keyword recognition network from~\cite{tang2017honk}, as well as the same network dynamically approximated with two flavors of our confidence-based adaptation scheme -- with a linear and an exponential increase in approximation level. The results are shown in Table~\ref{tab:all_engines_sound}. Both the original network, as well as the two flavors of the approximated network, achieve the same accuracy $96.3\%$, while Mobiprox adaptation leads to 15\% system-wide energy savings. Thus, we confirm that such adaptation is highly suitable for slowly changing noise levels observed in realistic situations.}

%The spoken keyword trace classification performed on an ASUS Tinkerboard S device showed that Mobiprox's adaptable approximation manages to reduce the energy consumption by up to 15\%, despite the DL model being already highly optimized for mobile deployment. We also observe that the confidence-based approximation is suitable for slowly changing noise levels observed in realistic situations, as almost no loss of accuracy is incurred, Mobiprox achieving an accuracy of $96.3\%$, on par with the non-approximated network model.



\subsection{Adaptation in an unscripted scenario}
\label{sec:evaluation_unscripted}

We investigate how Mobiprox adapts approximation of the \mnUci{} when the movement scenario is not prescribed. The ten users we recruited (details in Section~\ref{sec:methodology}) carried a waist-mounted smartphone that sampled and stored accelerometer and gyroscope signals as the users freely performed six activities (walking, walking upstairs, walking downstairs, sitting, standing and lying) during a ten-minute period. Afterwards, we re-run the collected traces on the same phone through a non-approximated \mnUci{} model and Mobiprox with state-based approximation strategy employed. Furthermore, to confirm that Mobiprox can be used on a smartphone in real time, we run approximated HAR inference, guided by a confidence-based adaptation strategy, directly on the phone as the data is getting collected. Analyzing the logs we have not observed any significant discrepancies (i.e. in terms of inference delay or inferred class mismatch) between on-device sampling and inference, and trace-based inference, confirming that Mobiprox affords smooth real-time approximation of mobile deep learning. 


\begin{table}[!htbp]
    \caption{Average relative accuracy (agreement with baseline) and relative energy consumption on 10 user traces collected in an unscripted scenario for MobileNet-V2 trained on the UCI-HAR dataset. Standard deviations across users are in parenthesis.}
    \begin{center}
    \begin{tabular}{llll}

Adaptation  & Incr. & Agreement w. baseline & Relative energy (\%) \\ \hline
Confidence-based    & Expon.   & 0.83 (0.04)     & 0.85 (0.01)        \\ 
State-based ($V_{L}=2$)         & Linear   & 0.91 (0.02)    & 0.88 (0.01)        \\ 
\end{tabular}
    \end{center}
\label{tab:unscripted}
\end{table}

In Table~\ref{tab:unscripted} we compare the inference performance and energy savings for the confidence-based adaptation engine with exponential increase of approximation and the state-driven adaptation strategy with the linear increase. Since the experiments were unscripted and we do not have the ground truth labels, we show the relative accuracy (i.e., the agreement with the non-approximated baseline neural network) and relative energy consumption (i.e.,  compared to the consumption of the non-approximated model). From the table we observe that the state-based adaptation engine achieves a higher average agreement with the baseline non-approximated model -- 91\%, while consuming 12\% less energy than the baseline. The confidence-based engine allows for more energy savings -- up to 15\% -- but with the downside of a lower agreement \mbox{with the non-approximated network -- 83\%.}

To further understand the functioning of the approximation adaptation strategy, in Figure~\ref{fig:adaptation-timeline-bestenergy} we show an example of the adaptation timeline for one of the user traces collected in this experiment. The black dotted line presents the adaptation, where the higher approximation configuration number (right y-axis) indicates a higher level of approximation. Since we do not have the ground truth information, we compare the activities inferred (left y-axis) by the non-approximated model (green dots) with the activities inferred by the currently used approximation configuration. Should these differ, we plot a red triangle indicating the mismatched activity inferences. Finally, we show the cumulative energy savings (compared to the non-approximated model and normalised to the graph dimensions) extrapolated from the currently used configuration and the precise energy measurements from the ASUS Tinkerboard. 

The figure confirms that the Mobiprox adaptation strategy harnesses the \textit{accuracy-energy consumption} trade-off points determined during the tuning phase. The state-driven strategy remains cautious (i.e. uses more accurate configurations) when a user performs dynamic activities that are more difficult to classify (e.g. walking, walking up or down the stairs), thus when long periods of uniform classification results are not present. The strategy jumps to more aggressive approximation configurations when a user lingers in an easier-to-classify static activities (e.g. standing, lying, sitting). 

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/adaptation-StateAdaptation_x1-new_trace.pdf}
    \caption{
    State-driven adaptation timeline with linear increase of approximation. }
    \label{fig:adaptation-timeline-bestenergy}
\end{figure}

