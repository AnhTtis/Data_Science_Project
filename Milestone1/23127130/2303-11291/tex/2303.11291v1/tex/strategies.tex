\section{Approximation adaptation strategies}
\label{sec:strategies}

%\sasa{This is a very interesting section (from my perspective). It would be nice if the context of the adaptation within the system and its importance was pointed a bit more prominently. Another suggestion would be to extract 'the interface' for the adaptation strategies (e.g., the objective they try to accomplish, the prediction problem, inputs) -- if this is extracted at the beginning of the section, then the presentation of each may be guided by these bullet points. }
%\hashim{100\% agreed!! This is the new novel contribution of this paper and its getting lost by it being too late in the paper. In my opinion if the story revolved around these application-specific/context-specific strategies for tuning, the rest of the technical details on how you use Approxtuner would make more sense given you motivated why you want to dynamic tuning. }

%\sasa{This section needs some work to highlight the novelty -- consider writing a small algorithm here for how it does runtime adaptation. This is probably what you want to advertise the most}

Mobiprox's key strength is its support for context-based runtime adaptation of mobile deep learning approximation. The framework itself deliberately does not prescribe the adaptation strategy allowing a developer to implement an arbitrary set of rules driven by energy needs (e.g. ``use higher approximation when battery level falls below 10\%''), the purpose of use (e.g. ``use more accurate HAR models when a user is exercising''), or even business models (e.g. ``use input-adaptable approximation for premium users''). Algorithm~\ref{alg:adapt-RT} shows the generic integration of the adaptation strategy within Mobiprox. Programming such strategies is trivial, yet, one can envision a more challenging-to-achieve goal, such as ``minimize the energy usage without sacrificing the inference accuracy''. In this section \blue{we harness the natural temporal dependence of the instances of sensed data that is characteristic in many mobile computing applications, and devise three strategies demonstrating that a widely applicable goal of energy minimization can be met with Mobiprox.}


\renewcommand{\algorithmicrequire}{\textbf{Parameter:}}

\begin{algorithm}[!htb]
\caption{Real-time approximation adaptation}
\label{alg:adapt-RT}

\begin{algorithmic}[1]
\REQUIRE{adaptationStrategy}
\COMMENT{designed to satisfy a certain objective}
\WHILE{true}
    \STATE $\mathrm{inputs}=\mathrm{gatherInputs()}$ \COMMENT{e.g. sensor data; user req.}
    \STATE $\mathrm{approxConfig}=\mathrm{adaptationStrategy(inputs)}$
    \STATE $\mathrm{load(approxConfig)}$
    \STATE $\mathrm{\textbf{with}\ approxConfig\ \textbf{do}\ inference(inputs)}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}


\subsection{Naive}

%One of the key contributions of Mobiprox is the ability to dynamically adapt the approximation configuration during runtime, targeting computation and energy efficiency while at the same time maximizing the inference accuracy. The goal is to select the more ``aggresive'' approximation configurations when the classification difficulty of the input is lower, and switch to more accurate and computationally expensive configurations when the input is more difficult to correctly classify (and more approximated configurations fail). Determining the ``difficulty'' of an input for a neural network (the likelihood that a particular approximate configuration would yield a correct inference result or not for a given input) is not a trivial task, since no ground truth information is available in the case of a deployed deep learning model. Consequently, we explored several strategies for designing an adaptation engine capable of choosing the most suitable approximation configuration for each inference datapoint.

The idea behind all the approximation strategies we present in this section is that inputs that are less difficult to classify can be processed with more ``aggresive'' energy-saving approximation configurations, whereas more difficult-to-classify inputs require computationally more expensive, more accurate configurations. Determining the ``difficulty'' of an input for a NN (i.e. the likelihood that a certain network will not be able to produce a correct inference result for that input) is not a easy task, in particular since no ground truth information is available in the case of a deployed model. 

Our baseline naive adaptation strategy \blue{assumes that the target class does not change throughout time and} uses a simple, heuristic model to predict the correctness of classification $\hat{C} \in \{0,1\}$. After each prediction $\hat{P}_{t}$ at time $t$, the model checks if the network predicted the same class as prediction $\hat{P}_{t-1}$. If the predictions are the same, the model assumes that the current approximation configuration of the network is yielding correct results ($\hat{C}=1$), signaling Mobiprox to approximate more and select a more ``aggressive'' approximation configuration. In the other case, when the current and previous predictions differ, the model assumes the current configuration is giving incorrect predictions and consequently Mobiprox switches to a ``milder'' approximation configuration\footnote{For all adaptation strategies we experiment with two options for moving to more aggressive approximations -- linear and exponential -- while moving to milder approximations is always done in the exponential fashion.}.


%\subsection{Kalman filtering}

%\sasa{The motivation was not clear here for what Kalman filter does}
% Our naive adaptation strategy considers only two consecutive predictions $\hat{P}_{t}$ and $\hat{P}_{t-1}$ to make a binary prediction of the correctness of classification $\hat{C}$ either 0 (wrong) or 1 (correct). This leads to two main drawbacks: high sensitivity to wrong predictions, as a single incorrect prediction by the network would downgrade to a less approximated configuration, and predicting the correctness as discrete values.

% To address these drawbacks, we develop an adaptation strategy that relies on a Kalman filter. Kalman filters~\cite{10.1115/1.3662552, 10.1115/1.3658902} are the statistically optimal sequential estimation procedure for dynamic systems~\cite{LOUKA20082348}. The Kalman filter allows for the recursive estimation of an unknown state $x_{t}$ based on observation values $y$ up to time $t$, by recursively combining observations with recent predictions with weights that minimize the corresponding biases~\cite{LOUKA20082348}. 

% We design the Kalman filter using a high measurement uncertainty value of $1.0$. This makes the Kalman gain small, causing the filter to suppress single events of differences between predictions $\hat{P}_{t}$ and $\hat{P}_{t-1}$. By filtering a sequence
% of discrete values $\hat{C}$, correctness predictions are now values $\hat{C}_{K}\in [0,1]$. We increase approximations when $\hat{C}_{K} > 0.95$ and reduce approximations when $\hat{C}_{K} < 0.5$. Threshold $0.95$ is chosen to counteract floating-point arithmetic errors, and threshold $0.5$ is chosen as the average of valid values of $\hat{C}$.


\subsection{State-driven}

%\sasa{This was an interesting one.}
Many mobile sensing domains deal with the recognition of states that do not vary rapidly over time: human physiological signals do not change erratically, people have conversations, not random utterances, movement is continuous in space, etc. Our state-driven adaptation strategy is based on the observation that rapid variations, especially in human behavior, are rare (e.g.~\cite{jabla2019balancing, reyes2016transition}).

%takes as an inspiration the problem of human activity recognition for which the following is true:

% \begin{itemize}
%     \item Natural human mobility is not characterised by rapid variations~\cite{jabla2019balancing, reyes2016transition}, so a user's physical activity is changing at a much slower rate than the sampling period of the activity classification;
%     \item Certain sequences of activities are intuitively less common or infeasible (e.g. walking downstairs and then lying) while others are very common and to be expected in everyday life (e.g. running followed by walking)
% \end{itemize}

Starting from this assumptions we implement an adaptation algorithm that adjusts the approximation configuration based on the reliability of classification, which in turn is determined by looking at a subset of the most recent predictions made by the network. After each inference, a vote is cast on the measure of reliability $V$, which is increased by $1$ if all previous $N$ predictions are equal, and decreased by $1$ otherwise. The functionality of this approach is described in detail in Algorithm~\ref{alg:adapt-SM}. %\sasa{Would be good to include the algo!}

In this algorithm, $V_{L}$ refers to the number of required votes that need to be cast consecutively in order to change the approximation configuration -- this parameter avoids the situation where the configuration is changed at every inference point. %For the experiments described in this work, we used $V_{L}=2$. 
The second parameter $N$ defines the capacity of the FIFO memory $M$. A larger memory would increase the robustness of the algorithm to classification errors (since it will consider a larger subset of previous predictions), but at the same time would hinder switching to more approximate configurations after a change in the observed/modeled phenomenon.


\begin{algorithm}[!htb]
\caption{State-driven adaptation engine}
\label{alg:adapt-SM}

\begin{algorithmic}[1]
\STATE $M = [ \; ]$ \COMMENT{FIFO memory with maximal capacity $N$}
\STATE $V = 0$  \COMMENT{Reliability index, always on interval $[-V_L, V_L]$}
\WHILE{$p$ = nextPrediction()}
    \STATE $\mathrm{push}(M, p)$
    \IF{$\mathrm{len}(M) < N$}
        \STATE continue
    \ENDIF
    
    \IF{all predictions in $M$ are equal}
        \STATE $V = \mathrm{max}(0, V) + 1$
    \ELSE
        \STATE $V = \mathrm{min}(0, V) - 1$
    \ENDIF
    
    \IF{$V \leq -V_L$}
        \STATE Approximate \textit{less}
    \ELSIF{$V \geq V_L$}
        \STATE Approximate \textit{more}
    \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{Confidence-driven}

% Finally, we exploit the classifier's confidence as a proxy to accuracy. We are using the Softmax confidence of the network, which in a Deep Learning classifier is represented by the normalized classification scores generated by the network's final layer (the Softmax layer). In the case of a $N$-class classification task, this is represented by an $N$-dimensional vector $z$ with the probability scores for all classes. For any class $i$, its Softmax confidence can be computed as:

% \begin{equation}
%     \sigma_i(z) = \frac{e^{z_i}}{\sum_{j=1}^{N} e^{z_j}}
% \end{equation}


Finally, we exploit the classifier's confidence as a proxy to accuracy. Recent research shows that the softmax layer probability values can accurately reflect the actual confidence of the classifier~\cite{mahmoud2021optimizing}. However, Guo et al.~\cite{guo2017calibration} point out that to achieve a high correlation between the softmax confidence and the expected inference accuracy, calibration is needed. 

Hence, we perform calibration by applying the temperature scaling during softmax confidence calculation. More specifically, for a $N$-class classification task where the $N$-dimensional vector $z$ contains class scores, for any class $i$, its calibrated softmax confidence is computed as:
 
\begin{equation}
    \sigma_i(z; T) = \frac{e^{z_i/T}}{\sum_{j=1}^{N} e^{z_j/T}}
\end{equation}
 
Where $T>0$ is a scalar temperature parameter, which ``softens'' the softmax (raises the output entropy) when $T>1$ and is optimized with respect to negative log likelihood on the validation dataset~\cite{guo2017calibration}. The goal is to tune the value of $T$ such that the confidence value for the datapoints classified with $p$ accuracy is as close a possible to $p$. Ideally, $T$ should be distinctly optimized for every approximation configuration. However, to avoid the computational impact on the tuning process, we apply temperature scaling by tuning a single instance of $T$ to already approximated NNs. 

\newcommand{\cCorrect}{C_{\mathrm{+}}^{(i)}}
\newcommand{\cWrong}{C_{\mathrm{-}}^{(i)}}
\newcommand{\tMore}{T_{\mathrm{more}}}
\newcommand{\tLess}{T_{\mathrm{less}}}
\newcommand{\CMore}{C_{\mathrm{more}}^{(i)}}
\newcommand{\CLess}{C_{\mathrm{less}}^{(i)}}

Our adaptation strategy then uses the calibrated softmax confidence to identify incorrect classifications. Our Android profiler (Section~\ref{sec:hpvm-profiler-android}) also reports per-class confidence averages for correct ($\cCorrect{}$) and incorrect ($\cWrong{}$) predictions and adds this information to approximation configuration files. The algorithm is then driven by a hysteresis outlined by two thresholds $\CLess$ and $\CMore$, where $\cWrong{}>\CLess>\CMore>\cCorrect{}$. If the classification confidence of the predicted class of the immediately preceding instance is higher than $\CMore$, the algorithm moves towards more aggressive approximation. If it is lower than $\CLess$, the algorithm moves towards less approximated configuration. We empirically find that the values of $\CLess$ halfway and $\CMore$ three-quarters-way between $\cWrong{}$ and $\cCorrect{}$, respectively, perform well in our experiments.


% \begin{figure}[!htb]
%     \newcommand{\sw}{0.48\linewidth}
%     \newcommand{\w}{\linewidth}
%     \newcommand{\hs}{\hspace{0cm}}
%     \newcommand{\vs}{\vspace{1cm}}
%     \centering
%     \begin{subfigure}{\sw}
%         \centering
%         \includegraphics[width=\w]{figures/confidence/combined.android-profiled.pareto.confidence.pdf}
%         \caption{Average over all classes}
%         \label{fig:confidences-average}
%     \end{subfigure}
%     \hs
%     \begin{subfigure}{\sw}
%         \centering
%         \includegraphics[width=\w]{figures/confidence/combined.android-profiled.pareto.confidence-class0.pdf}
%         \caption{Walking}
%         \label{fig:confidences-walking}
%     \end{subfigure}
    
%     \vs
    
%     \begin{subfigure}{\sw}
%         \centering
%         \includegraphics[width=\w]{figures/confidence/combined.android-profiled.pareto.confidence-class1.pdf}
%         \caption{Walking upstairs}
%         \label{fig:confidences-upstairs}
%     \end{subfigure}
%     \hs
%     \begin{subfigure}{\sw}
%         \centering
%         \includegraphics[width=\w]{figures/confidence/combined.android-profiled.pareto.confidence-class5.pdf}
%         \caption{Lying}
%         \label{fig:confidences-lying}
%     \end{subfigure}
%     \vs
%     \caption{SoftMax Confidence values of predicted class at various approximation levels (indicated by the QoS loss, defined by equation ~\ref{eq:qosloss}). Figures show confidence values when predictions were correct, wrong and average over both correct and wrong predictions.}
%     \label{fig:confidences}
% \end{figure}
