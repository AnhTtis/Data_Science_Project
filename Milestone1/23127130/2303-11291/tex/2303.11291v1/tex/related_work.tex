\section{Related Work}
\label{sec:related_work}

\textbf{Resource-efficient deep learning on mobiles.} The expansion of mobile deep learning (DL) applications has been hindered by the high resource consumption of DL models and the difficulty of the edge computing devices, such as battery-powered smartphones, to meet the resource and energy requirements of such applications~\cite{chen2019deep}. Model representations may including hundreds of millions of parameters and  performing the classification of a single input vector can easily overwhelm the available computing and memory resources of edge computing platforms~\cite{LaneSqueezing2017}. 


Efforts have, thus, focused on reducing the complexity, while preserving the inference power of DL models through weight quantization~\cite{wu2016quantized}, pruning~\cite{niu2020patdnn},  knowledge distillation~\cite{hinton2015distilling} and other methods~\cite{cheng2017survey}. High-level DL frameworks, such as PyTorch, do not readily support mobile platforms, thus, there are relatively few demonstrations of an on-device DL optimization. Among these, DeepX compresses the network's layers using singular value decomposition and supports execution on heterogeneous mobile hardware. Its downside, common to many DL compression techniques, is a permanent decrease in inference accuracy (of $\approx 5\%$). On the pruning front, PatDNN enables real-time inference using large-scale DL models (e.g., VGG-16, ResNet-50) on mobile devices by harnessing pattern-based model pruning~\cite{10.1145/3373376.3378534}, while DeepIoT~\cite{yao2017deepiot} uses reinforcement learning to guide the pruning process. Both solutions lead to significant model size reductions ($90\%$ to $98.9\%$ in case of DeepIoT) and speedups (up to $44.5\times$ in case of PatDNN) with no inference accuracy degradation in certain settings, demonstrating vast opportunities for mobile DL optimisation. Parameter quantization, on the other hand, despite being actively researched~\cite{wu2016quantized, Jin_2020_CVPR,zhou2018adaptive}, sees only limited implementation in the mobile realm. The main reason is the lack of support for arbitrary bit-width computation in today's mobile hardware. 




%. These efforts target various aspects of deep learning neural networks, from model architecture and learning algorithms to compiler- and hardware-level optimizations. In this section, we review some of the most relevant existing contributions to resource-efficient deep learning on mobile devices, focusing on dynamic neural network complexity adaptation strategies and compiler-level support for approximate computing, and discuss how Mobiprox relates to the existing works in this field.


%The efforts to make deep learning more resource-efficient and suitable for mobile and embedded settings range from model-level techniques (aimed at reducing the model size through methods such as weight quantization, pruning or knowledge distillation) to techniques involving lower precision arithmetic (for reducing the memory footprint and the data transfer times across buses and interconnections) and implementation-level techniques (such as early exit, network slimming)~\cite{lee2021resourceefficient}.

%Deep model quantization is a technique widely used for reducing the computation and memory costs of DNNs, and deploying complex DNNs on mobile and embedded devices~\cite{wu2016quantized, Jin_2020_CVPR}. The work by Zhou et al.~\cite{zhou2018adaptive} proposes an adaptive optimization approaches which estimates the effects of parameter quantization errors in the network's individual layers on the overall model prediction accuracy and then optimizes the quantization bit-width for each layer accordingly. The authors report their quantization algorithm achieves $20\%$-$40\%$ higher compression rates compared to equal bit-width quantization at the same model prediction accuracy~\cite{zhou2018adaptive}. Another work by Peng et al.~\cite{PENG2021194} aims to optimize the quantization of a CNN for mobile inference through a novel approach which ensures that only integer-based arithmetic is needed during the inference stage of the quantized model. The authors report that their method can achieve comparable prediction accuracy to other state-of-the-art methods while reducing the run-time latency by a large margin (a maximum of 4.33 the speed of the original full-precision version on an ARMv8 CPU)~\cite{PENG2021194}.

%Pruning
%Pruning is another model-level technique for making deep-learning inference on mobile devices more energy-efficient. By pruning unimportant neurons, filters, and channels, significant computational resources can be saved with a minimal decrease in inference accuracy, improving the accuracy per parameter and per operation~\cite{lee2021resourceefficient}. A discimination-aware channel pruning (DCP) method for the compression of deep neural networks is proposed in~\cite{9384353}. The authors first introduce additional losses to improve the
%discriminative power of the network and then perform channel selection in a layer-wise manner by formulating the channel pruning as a sparsity induced optimization problem and proposing a greedy algorithm to solve it. The method is evaluated on a smartphone device and the pruned networks (MobileNetV1 and MobileNetV2) achieve $1.93\times$ and $1.42\times$ inference acceleration, respectively, with a negligible decrease in inference accuracy.

%PatDNN~\cite{10.1145/3373376.3378534} is a novel end-to-end mobile DNN acceleration framework that can generate highly accurate DNN models with the help of a novel pattern-based pruning model compression technique and a set of thorough architecture-aware compiler- and code generation-based optimizations (filter kernel reordering, compressed weight storage, register load redundancy elimination, and parameter auto-tuning). PatDNN is reported to outperform three state-of-the-art end-to-end DNN frameworks, TensorFlow Lite, TVM, and Alibaba Mobile Neural Network with speedup up to $44.5\times$, $11.4\times$, and $7.1\times$, respectively, with no accuracy degradation, thus enabling real-time inference of large-scale DNNs (e.g., VGG-16, ResNet-50) using mobile devices~\cite{10.1145/3373376.3378534}.

%PatDNN is a DNN acceleration framework that enables real-time inference of large-scale DNNs (e.g., VGG-16, ResNet-50) using mobile devices with the help of a pattern-based pruning model compression technique, outperforming state-of-the-art end-to-end DNN frameworks (TensorFlow Lite, TVM, and Alibaba Mobile Neural Network) with speedups up to $44.5\times$, $11.4\times$, and $7.1\times$, respectively, with no accuracy degradation~\cite{10.1145/3373376.3378534}. DeepIoT~\cite{yao2017deepiot} is a similar pruning-based compression technique dedicated for running DNNs on edge devices which reduces the size of the DNN models by $90\%$ to $98.9\%$ and their execution time by $71.4\%$ to $94.5\%$ compared to non-compressed models. These improvements, achieved without degrading the accuracy, lead to a decrease in energy consumption by $72.2\%$ to $95.7\%$.


%A pruning-based compression technique dedicated for running DNNs on edge devices is reported also by the authors of~\cite{yao2017deepiot}. This technique, entitled DeepIoT, is able to compress most DNN architectures such as fully-connected, convolutional, and recurrent neural networks, as well as their combinations. The DeepIoT compressed models can run on embedded and mobile devices using the existing deep learning libraries and the authors showed that DeepIoT reduces the size of these models by $90\%$ to $98.9\%$ and their execution time by $71.4\%$ to $94.5\%$ compared to non-compressed models. These improvements, achieved without degrading the accuracy, lead to a decrease in energy consumption by $72.2\%$ to $95.7\%$.

%Knowledge distillation is based on the principle that a compact deep neural network (DNN) can successfully capture the knowledge of a larger, high-preforming model (teacher network), if coached by it~\cite{hinton2015distilling}. By training the compact DNN with the output predictions of the teacher network, the former approximates the function learned by the latter, leading to improved resource efficiency such as accuracy per parameter and per operation of inference tasks~\cite{hinton2015distilling,lee2021resourceefficient}. 


%An implementation-level technique for improved DNN resource efficiency is early exit, which involves reducing the inference computational complexity by not computing all network layers. Practical observations have shown that a DNN may correctly infer many samples at an intermediate point in the network, when the features learned at that particular intermediate layer are sufficient for an accurate classification~\cite{teerapittayanon2016branchynet}. Consequently, this technique allows a DNN to classify an input as early as possible by having multiple exit classifier points along the network (cascaded early exists), enabling a finer control over the accuracy vs approximation trade-off and improving the compute resources and the inference latency, improving the accuracy per Joule, per operation, and per core utilization~\cite{lee2021resourceefficient}.

%Enabling deep learning models to run on edge computing devices can also be achieved through model compression techniques such as DeepX~\cite{lane2016deepx}. This method compresses the network's layers using singular value decomposition (SVD) and the experiments reported by the authors show it can allow even efficient execution of even large-scale deep learning models on mobile processors. The downside is, as expected, a decrease in inference accuracy (of $\approx 5\%$), a price to be paid in case of nearly any neural network compression technique.

%One solution to limit the accuracy degradation by optimizing the neural network compression is using Neural Architectural Search (NAS), a technique that is traditionally used to seek optimal DNN models in the space of hyperparameters of network width, depth, and resolution by exploiting the trade-off between accuracy and latency to maximize resource efficiency given compute resource budget~\cite{lee2021resourceefficient}. ProxylessNAS~\cite{cai2018proxylessnas} is a framework that can directly learn the architectures for large-scale target tasks and target hardware platforms, which saves memory consumption by one order of magnitude by using path-level binarization of the network's parameters and provides a new path-level pruning perspective for NAS, showing a close connection between NAS and model compression. The evaluation of ProxylessNAS on ImageNet showed a $3.1\%$ higher top-1 accuracy than state-of-the-art architecture MobileNetV2, being $1.2\times$ faster with meaasured GPU latency. On CIFAR-10, ProxylessNAS achieves $2.08\%$ test error with only 5.7M parameters, outpeforming the previous state-of-the-art architecture AmoebaNet-B ($2.13\%$ test error), while using 6Ã— fewer parameters. 

%FastDeepIoT~\cite{yao2018fastdeepiot} too exploits the relationship between the amount of compression and the model performance, by aiming to find the network configurations that considerably improve the trade-off between execution time and accuracy on edge devices, resulting in reduced execution time and energy consumption with minimal performance drop.
%DeepMon, a mobile deep learning inference system, integrates a suite of optimization techniques, including combining quantization with caching of results from intermediate layers on GPUs, to efficiently offload convolutional layers to mobile GPUs and accelerate the processing~\cite{huynh2017deepmon}.
%Other similar works~\cite{scheidegger2019constrained, minhas2021leveraging} include NAS-based frameworks such as MnasNet~\cite{tan2019mnasnet}, an automated mobile neural architecture search (MNAS) approach utilizing reinforcement learning with a balanced reward function between the accuracy and the latency to seek a compact neural network model. Experimental results showed MnasNet to consistently outperform state-of-the-art mobile CNN models across multiple computer vision tasks. Reinforcement learning is also used by AdaDeep~\cite{liu2018demand}, to determine the optimal combination of compression techniques (including pruning and special filter structures borrowed from MobileNet and SqueezeNet) that should be employed for the model to meet application requirements and device constraints (energy consumption, accuracy, or latency).

\textbf{Dynamic neural network compression adaptation.} All the above approaches suffer from a common drawback: once the compression is applied, the resulting ``impaired'' network remains fixed during runtime. Thus, such approaches enable operation at a single fixed point on the\textit{ accuracy-resource usage} trade-off curve regardless of how the context in which the inference is performed changes during runtime. This, however, is inappropriate for the mobile domain, since the changing context of use is a defining trait of mobile computing and the one that may significantly impact the requirements imposed on the DL inference. For instance, a smartphone may or may not be connected to a charger calling for more or less energy-efficient operation; sensed data may be more or less noisy, calling for more or less complex DL models; depending on the purpose of use, a user may require more or less accurate inference results from a mobile app. Recent research therefore focuses on enabling dynamic network adaptation in order to cover the entire accuracy-resource usage trade-off curve and can adapt the network compression level without the need for re-training.

The initial solutions enabling dynamic adaptivity, such as MCDNN~\cite{han2016mcdnn}, relied on having several differently-compressed candidate DL models in the cloud and downloading the most appropriate model on the device according to the current context. While enabling context-adaptation, this strategy adds substantial overheads stemming from model transfer. Early exit networks can dynamically reduce the computational complexity of a single model by not traversing all network layers and halting the computation at an intermediate point in the network instead~\cite{teerapittayanon2016branchynet}. With multiple exit points along the network (cascaded early exits), a finer control can be gained over the accuracy vs approximation trade-off~\cite{lee2021resourceefficient}. SPINN~\cite{laskaridis2020spinn} introduces a scheduler that co-optimises the early-exit policy and DL model splitting at run time, in order to adapt to dynamic conditions and meet user-defined service-level requirements in a cloud-edge environment. The drawbacks of early-exit schemes include the need for off-the-shelf models to be re-structured and re-trained and the complexity of developing exiting policies that will be suitable for a particular operational domain.

Traditional compression techniques, such as pruning and quantization, have also been revised to support dynamic adaptation. Runtime Neural Pruning framework~\cite{10.5555/3294771.3294979} enables bottom-up, layer-by-layer pruning guided by a Markov decision process and reinforcement learning. The importance of each convolutional kernel is assessed and based on it channel-wise pruning is performed, where the network is pruned more when classifying an ``easy-to-classify'' input instance. The influence of different features, computed by convolutional layers, on the classification success varies with different inputs. This was exploited by Gao et al.~\cite{gao2018dynamic} who propose feature boosting and suppression to predictively amplify salient convolutional channels and skip unimportant ones at run-time. This method preserves the full network structure and accelerates convolution by dynamically skipping unimportant input and output channels. A different approach for dynamic compression adaptation is the Slimmable Neural Network (SNN)~\cite{yu2018slimmable}. The method trains a single CNN and then executes it at different widths (number of channels in a layer), permitting runtime adaptive accuracy-efficiency trade-offs at runtime. There is no need for re-training or loading different models: the network adjusts its width on the fly, based on the resource constraints, input difficulty, or other contextual factors. Any-Precision approach~\cite{yu2020any} proposes a CNN training method that allows the network to adapt its numerical precision during inference to support dynamic speed and accuracy trade-off.
Yet, neither SNNs nor Any-Precision apply to already trained networks, nor have these techniques been implemented in the mobile realm. \blue{The reason for this is that, unlike our approach, the above methods were not originally planned with mobile platform restrictions in mind. SNNs, for instance, rely on dynamic neural network graph reconfiguration, something that none of the mobile DL frameworks  (e.g. TensorFlow Lite, Pytorch Mobile, etc.) supports at the moment.}


%An implementation-level technique for improved DNN resource efficiency is early exit, which involves reducing the inference computational complexity by not computing all network layers. Practical observations have shown that a DNN may correctly infer many samples at an intermediate point in the network, when the features learned at that particular intermediate layer are sufficient for an accurate classification~\cite{teerapittayanon2016branchynet}. Consequently, this technique allows a DNN to classify an input as early as possible by having multiple exit classifier points along the network (cascaded early exists), enabling a finer control over the accuracy vs approximation trade-off and improving the compute resources and the inference latency, improving the accuracy per Joule, per operation, and per core utilization~\cite{lee2021resourceefficient}.




%The previously discussed pruning methods produce a fixed model for deployment -- i.e. once pruned, the network remains unchanged during inference. Recently, efforts are focused on enabling \emph{dynamic} pruning techniques, which adapts the pruning during runtine, according to the input or other contextual factors. One such approach is the Runtime Neural Pruning (RNP) framework~\cite{10.5555/3294771.3294979}, where pruning is performed in a bottom-up, layer-by-layer manner, modeled as a Markov decision process and using reinforcement learning for training. The importance of each convolutional kernel is assessed and based on it channel-wise pruning is performed conditioned on different samples, where the network is pruned more when the input is easier for the task. The authors claim their approach can be applied to off-the-shelf network structures and reach a better tradeoff between speed and accuracy, especially with a large pruning rate. The evaluation results show RNP achieving up to $5.9\times$ speed-up with under $4.9\%$ increase in top-5 error on the ILSVRC2012 dataset~\cite{ILSVRC15}, compared to the state-of-the-art VGG-16 network.

%The dependency of the features computed by the convolutional layers on the input is also exploited by Gao et al.~\cite{gao2018dynamic} who propose feature boosting and suppression (FBS) to predictively amplify salient convolutional channels and skip unimportant ones at run-time. This method preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. The authors report FBS achieves $5\times$ and $2\times$ MAC savings on the ILSVRC2012 dataset compared to VGG-16 and ResNet-18, respectively, in both cases  with less than $0.6\%$ top-5 accuracy loss.

%The conventional quantization techniques (some of which where mentioned above) try to quantize the NN models based on the static distribution of the weight values, but fail to dynamically capture
%the sensitivity and variability in the input feature map values, which also impact the quantized network's accuracy vs. energy consumption trade-off. A solution to this issue is proposed through dynamic region-based quantization (DRQ)~\cite{9138970}, a method which can change the precision of a DNN model dynamically based on the sensitive regions in the feature map to achieve greater acceleration while reserving better NN accuracy. In their evaluation, the authors report their method achieves $21\%$ performance gain and $33\%$ energy reduction with $3\%$ prediction accuracy improvement compared to the state-of-the-art mixed-precision quantization accelerator OLAccel~\cite{8416865}.

%A dynamic quantization technique using an Adaptive Floating Point (AFP) format is proposed in~\cite{liu2021improving}, enhancing the model compression rate without accuracy degradation and model re-training. The framework also automatically optimizes and chooses the adequate AFP configuration for each layer, thus maximizing the compression efficacy. The paper reports that their AFP-encoded ResNet-50/MobileNet-v2 outperforms the state-of-the-art works by $1.1\%$ in accuracy using the same bit-width while reducing the energy consumption by $11.2\times$.

%A different approach for dynamic NN complexity adaptation is the Slimmable Neural Network (SNN) proposed by Yu et al.~\cite{yu2018slimmable}. This is a general method to train a single CNN and then execute it at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime. There is no need for re-training or storing/loading different models: the network adjusts its width on the fly, based on the resource constraints, input difficulty and other contextual factors. Any-Precision Deep Neural Networks approach~\cite{yu2020any} proposes the same runtime adaptivity, in this case the CNN training method allows the network to adapt its numerical precision during inference to support dynamic speed and accuracy trade-off.

%\subsection{Compiler support for approximate computing}

%Resource-efficient inference on edge computing devices can also be achieved through approximate mobile computing: using controllable approximations that sacrifice accuracy for energy-efficiency when executing deep learning models on mobile devices. Approximate computing started to be widely researched after 2010, with researchers focusing on a wide array of software and hardware techniques that can trade energy and computing time with accuracy of output. One of the critical elements for supporting software-level approximations is the compiler tool chain.

%Some of the first research efforts targeting approximate compilers were conducted by the Sampa research group from the University of Washington~\cite{Barua2019}, with their major contributions in the approximate compiler area being EnerJ~\cite{10.1145/1993498.1993518}, a Java application-based compiler tool chain that adds support for approximate data types, storage and computation, and ACCEPT~\cite{sampson2015accept}, an approximate compiler based on C/C++. ACCEPT includes C/C++ type qualifiers for constraining approximation, a compiler analysis library that identifies regions of approximable code, an autotuning system that automatically chooses the best approximation strategies, and a feedback mechanism that explains how annotations can be improved for better approximation opportunities~\cite{sampson2015accept}.

%In the recent years, important contributions to the field of appromximate compilers have been brought by the research group frin the University of Illinois at Urbana-Champaign, who first introduced  ApproxHPVM~\cite{sharif2019approxhpvm}, a compiler IR and system designed to enable accuracy-aware performance and energy tuning on heterogeneous systems with multiple compute units and approximation methods, followed by ApproxTuner~\cite{sharif2021approxtuner}, an automatic framework for accuracy-aware optimization of tensor-based applications while requiring only high-level end-to-end quality specifications.

%However, none of the above approximate compilers and frameworks natively support mobile execution of approximated neural networks: there is no dedicated support for dynamic compression or approximation of a NN on mobile devices and no tools for for building such adaptable NNs. In addition, the promised theoretical energy savings by approaches such as AnyPrecision or Slimmable Neural Networks do not translate in practice to resource efficient deep learning on mobiles: the approaches either lack compiler support of their operations (AnyPrecision) or there is no framework supporting their execution under Android OS (Slimmable Neural Networks)~\cite{electronics10232958}. This is the gap that Mobiprox attempts to address, by by extending the state-of-the-art approximate deep learning compiler pipeline, ApproxTuner, with an approximable tensor back-end for mobile devices and integrating a context- and input-aware approximation adaptation engine.



