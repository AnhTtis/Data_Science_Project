\section{Introduction}
\label{sec:introduction}

Powerful services enabled by deep learning, such as real-time camera-based object detection, online translation, and human activity recognition (HAR), are becoming increasingly available on mobile devices. Indeed, DL is an integral part of more than 12\% of application downloads from the Android store platform~\cite{10.1145/3308558.3313591}. However, the new affordances do not come for free -- large DL models may overload the limited memory of mobile devices, the computational burden may lead to significant delays before the results are available, and the power needed for processing may quickly deplete the mobile's battery. 

Reducing the complexity of neural networks (NNs) is the primary means of making DL mobile friendly. Such complexity reduction may be inherent to the network design -- MobileNet~\cite{howard2017mobilenets}, EfficientNet~\cite{tan2019efficientnet}, and ShuffleNet~\cite{zhang2018shufflenet} represent some of the architectures that are specifically designed for the mobile's limited memory resources. Yet, the computational burden of these networks may still be overwhelming for a wide range of heterogeneous edge devices~\cite{almeida2019embench}.
%\hashim{a very specific point regarding these netwrorks -- needs some statistic to back this up or some citation} 
Both memory and computational complexity can be further reduced by a gamut of NN compression techniques. These include parameter quantization~\cite{wu2016quantized}, network weight pruning~\cite{niu2020patdnn}, network distillation~\cite{hinton2015distilling},  to name a few. The key issue with such complexity reduction is that it often leads to a certain loss of the inference accuracy and leaves the network permanently impaired. 

On mobiles, on the other hand, \textit{deep learning compression needs to be adaptable to the context of use}: a compressed model that reliably recognises a user's speech commands when used in a quiet indoor location, might completely fail in noisy outdoor environments; similarly, a user might be fine with a more compressed model that occasionally misclassifies her physical activity during her daily routine, but would require a more accurate model while exercising. A rigid approach to DL compression is against the often dynamic nature of mobile computing, where both a user's requirements with respect to the result accuracy~\cite{machidon2020watching} well as the difficulty of given NN input~\cite{machidon2021queen} may vary as the context of use changes. 
%\hashim{"against the dynamic nature" might be strong wording because not sure all kinds of mobile computing strictly need to be dynamic}

Recently, proposals have been made to enable dynamic accuracy/complexity adaptation of neural networks. Examples of adaptable networks include dynamic quantization enabled by AnyPrecision~\cite{yu2020any}, dynamic layer width adaptation via Slimmable Neural Networks~\cite{yu2018slimmable}, or dynamic pruning proposed in~\cite{10.5555/3294771.3294979}. Common for all of the above approaches is that they do not support pre-built networks, but require specialized training, which for large datasets and architectures can take days or weeks, before real-time adaptation can be harnessed. Furthermore, despite targeting dynamic environments, the above works do not actually provide mobile-ready implementations. Translating benefits provided by high-level demonstrations (often implemented in PyTorch) to mobile energy savings requires a significant engineering effort as modern mobile DL frameworks, such as TensorFlow Lite, do not support the versatility of high-level frameworks, such as PyTorch.%~\cite{8746691}.
%\hashim{good point that pruning and similar approaches need retraining but need to emphasize more than retraining requires days/weeks/months and so can be prohibitively expensive to do for large datasets and so its a cool capability to be able to use pretrained models in Mobiprox}

Advances in a different research area -- heterogeneous systems' compilers -- have recently tackled the issue of ``optimal'' NN compilation, where individual tensor operations are implemented in accordance with the underlying hardware capabilities. Along these lines, an LLVM-based compiler ApproxHPVM~\cite{sharif2019approxhpvm} enables the execution of convolutional neural network (CNN) operations with a different level of approximation, should the hardware/OS support approximate computing. %For a given inference accuracy threshold, multifold speedup can be achieved. %\hashim{"multifold execution speedup" is strange wording}
However, ApproxHPVM targets server environments, produces CUDA-ready binaries only, and does not support compilation to mobile (Android or iOS) hardware. With the help of ApproxTuner~\cite{sharif2021approxtuner}, approximation levels within ApproxHPVM can be dynamically adapted, yet, the provided adaptation method is simple, reactive, and context-oblivious.

%settings and, thus, it does not support compilation to mobile hardware nor does it provide a means for intelligent adaptation that would match the approximation levels to potentially changing contexts of use. 
%\hashim{This last sentence is the main motivation for this work in my understand. My suggestion is to bring this point up much earlier and adding more motivation for it. This could actually be the start of paragraph 2 where you introduce that "context-dependent" approximation is really important because otherwise results can be pretty bad for certain important scenarios/runtime conditions/inputs. You can give examples of such scenarios and argue that these are common in mobile computing. Then in the 3rd para you can talk about how current solutions, e.g., pruning does not address this problem and why you need Mobiprox}


In this paper we present \textbf{Mobiprox} -- a novel framework that enables context-adaptable approximation of DL operations executed on the mobile platform.
%\hashim{"adaptable" approximation in what sense? context-dependent adaptable? Important that this single sentence summarizes the uniqueness of Mobiprox}
%\veljko{Actually in any way - you can drive it with the context, e.g. sense the context and adapt accordingly or you can drive it with business policies, etc.}
Our guiding vision is that \textit{data scientists are not mobile system experts}. Therefore, deep learning modeling should be disentangled from system-level performance optimization. Mobiprox aims to support efficient on-device execution of an arbitrary pre-trained mobile network architecture. Furthermore, we do not require that a developer knows which optimizations (in our case -- execution approximations) are available on the device. Still, we give a developer an option of (dynamically) setting an operational point along the inference accuracy vs. resource usage trade-off curve, yet, in the limit case, the developer need not even set this point, but merely let Mobiprox tune the execution according to its internal approximation adaptation algorithms.

Mobiprox adapts the abstractions from ApproxHPVM/ApproxTuner in a general way to support the Andorid mobile platform. We implement Mobiprox at low levels of the computing stack to support a wide range of NN architectures and embrace various approximation techniques exposed by the underlying hardware and the OS\footnote{The specific implementation presented in this paper supports perforated convolution, filter sampling, and half-precision quantisation.}. To support context-sensitive runtime adaptation Mobiprox identifies Pareto-optimal approximation configurations during the off-line tuning stage. The system then enables the network to glide across different speedup/accuracy trade-off points during the runtime. The key novelty of Mobiprox are also the adaptation algorithms that guide the runtime approximation adaptation according to a given goal, e.g. maximal energy savings. 

%Designed in the most general way and implemented at low levels of the computing stack, our framework is not limited to a particular deep learning architecture, nor to a single approximation dimension, but can be applied to a virtually unlimited number of architectures composed of convolutional and densely connected layers, and supports any approximation technique exposed by the underlying hardware and the OS\footnote{The specific implementation presented in this paper supports perforated convolution, filter sampling, and half-precision quantisation.}. Among all possible approximations per each layer of a network, Mobiprox identifies Pareto-optimal approximation configurations enabling the network to glide across different speedup/accuracy trade-off points during the runtime. Finally, Mobiprox introduces adaptation algorithms that guide the runtime approximation adaptation according to a given goal, e.g. maximal energy savings. 

With Mobiprox, we address multiple challenges that stand in the way towards adaptable \mbox{approximate mobile DL:}
\begin{itemize} 

\item \blue{\textbf{The difficulty of implementing approximate operations at an appropriate level of the mobile computing stack;} The last two decades witnessed a range of approximate computing techniques being developed -- from approximate adders and multipliers, to loop perforation and task skipping~\cite{mittal2016survey}. However, due to their small form factor mobile devices seldom can host both approximate and accurate versions of hardware circuits. Software techniques, on the other hand, often require a strong involvement from the developer, for example, to label loops that are candidates for perforation and select the fraction of loops to skip. Therefore, we focus on software-level tensor operation approximation. As mobile DL frameworks (e.g. TensorFlow Lite) and even libraries of specialized functions for mobile DL (e.g. ARM Compute Library) aggregate tensor operations, we had to implement approximate, as well as precise, tensor operations from basic linear algebra subprograms (BLAS) primitives;}

\item \blue{\textbf{The issue of modifying neural network operation at runtime on a mobile device;} mobile DL frameworks do not support dynamic graph reconfiguration, thus even the existing dynamic approximation schemes (such as Slimmable Neural Networks~\cite{yu2018slimmable}) do not work on mobiles; to overcome this limitation, we implemented our custom approximations at a fine-grained level and exposed the calls for setting the approximation level at runtime through Java Native Interface;} 


\item \blue{\textbf{The lack of algorithms and tools for context-aware adaptation of mobile DL;} a certain classification accuracy level might be acceptable in some situations, but not in others; in addition, an approximated DL model that works well for certain inputs, might not provide correct classification for some other inputs; finally, gauging model performance at runtime is challenging; we first devise proxies for measuring classification performance (the same-class instance counting-based and the softmax confidence-based) and then develop algorithms (naive, state-based, and confidence-based) for dynamically adapting the approximation.}

%To harness the existing state-of-the-art solutions in the area of heterogeneous approximate computing (i.e. ApproxHPVM), we have to overcome the challenge of the integration of a mobile (i.e. Android) and to-date desktop-centric (i.e. HPVM~\cite{kotsifakou2018hpvm}) compiler in a single functioning pipeline supporting translation of approximate configurations.  

%\item The need to gauge the opportunities for approximation in different contexts of use represents a particular challenge -- an approximated network that works well for certain inputs, might not provide correct classification for some other inputs.
\end{itemize}

%Numerous challenges stand in the way towards adaptable approximate mobile deep learning. To enable wide support for approximating different mobile neural networks, in Mobiprox we first have to enable low-level support for approximate computing on mobile CPU and GPU hardware. 
%The last two decades witnessed a range of approximate computing techniques being developed -- from approximate adders and multipliers, to loop perforation and task skipping~\cite{mittal2016survey}. However, due to their small form factor mobile devices seldom can host both approximate and accurate versions of hardware circuits. Software techniques, on the other hand, often require a strong involvement from the developer, for example, to label loops that are candidates for perforation. %Therefore, in Mobiprox we focus on implementing basic software-based approximations at the lowest possible level -- the individual DL operation level -- and do not even require that the developer knows about these approximations. 
%Second, in order to harness the existing state-of-the-art solutions in the area of heterogeneous approximate computing (i.e. ApproxHPVM), we have to overcome the challenge of the integration of a mobile (i.e. Android) and to-date desktop-only (i.e. HPVM~\cite{kotsifakou2018hpvm}) compiler in a single functioning pipeline supporting translation of approximate configurations.  In the end, the need to gauge the opportunities for approximation in different contexts of use represents a particular challenge -- an approximated network that works well for certain inputs, might not provide correct classification for some other inputs. 
%\hashim{This last sentence is great but should come earlier in my opinion. This very nicely conveys the unique capability of Mobiprox}


%\vspace{0.05in}
\noindent Towards this end, the paper presents the following contributions:
\begin{itemize}
    \item \textbf{We develop an end-to-end approximate configuration search, selection, and compilation pipeline for mobile devices.} Our solution integrates state-of-the-art heterogeneous compilation infrastructure, approximate configuration search framework, and a widely used LLVM compiler into an Android-ready pipeline; furthermore, our solution supports dynamic configuration loading;
    \item \textbf{We devise novel strategies for runtime approximation configuration adaptation}; based on the problem properties or the classifier confidence, our solutions 
    ensure that the desired inference accuracy is achieved with the minimal use of a mobile's resources;
    \item \textbf{We implement selected approximate computing primitives at a low-level of the mobile computing stack}, supporting both on-CPU and on-GPU approximate execution of different tensor operations for mobile devices using OpenCL.
    \item \textbf{Our evaluation shows the energy savings, speedup, and classification accuracy enabled by approximated configurations generated by Mobiprox.}
    \blue{We perform a series of experiments on both a development board (for precise energy measurements) and commodity smartphones (for realistic user experience), using different NN architectures and multiple application domains, including human activity classification and spoken keyword recognition. Our evaluation demonstrates that in both scripted and unscripted scenarios, by adapting to the varying context (i.e. input data difficulty), Mobiprox can bring significant energy savings while preserving the inference accuracy.}

\end{itemize}

    %\item \textbf{We implement selected approximate computing primitives at a low-level of the mobile computing stack}, supporting both on-CPU and on-GPU approximate execution of different neural network operations; 
    %furthermore, their use does not require any engagement from the developer, who does not even need to know about these approximations;
    %uncovering memory usage issues connected with approximate operation implementations directly translated from the desktop domain, we optimise our mobile implementations to minimize memory usage;
    %\item \textbf{We construct an end-to-end approximate configuration search, selection, and compilation pipeline for mobile devices.} Our solution integrates state-of-the-art heterogeneous compilation infrastructure, approximate configuration search framework, and a widely used LLVM compiler into an Android-ready pipeline; furthermore, our solution  supports dynamic configuration loading;
    %\hashim{"widely used" LLVM compiler?}
   % \item \textbf{We devise strategies for runtime approximation configuration adaptation}; based on the problem properties or the classifier confidence, our solutions 
    %is driven by the softmax layer confidence and 
    %ensure that the desired inference accuracy is achieved with the minimal use of a mobile's resources.



%\sasa{It is probably better to merge bullets 2 and 3 in terms of the novelty -- both are saying system support for android. In addition the evaluation can be added as a bullet in contribution}


%Second, we confirm that the softmax layer confidence correlates with the inference accuracy in case of a MobileNet deep learning network trained to recognize human activity. Finally, in a 21-user study we test how Mobiprox performs in a real world HAR application and comparatively evaluate how the 3 adaptation engines perform in terms of energy saved vs. inference accuracy.

%\sasa{Some things regarding the result are unclear here: in particular as you later say 2 percent quality drop -- what is the metric for quality; second the impact and the result of the user study (which is very welcome!) is not clear}