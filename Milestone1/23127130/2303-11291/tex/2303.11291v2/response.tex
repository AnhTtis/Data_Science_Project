%% !TEX TS-program = latex+dvips+ps2pdf
%% ACM Conference on Computer and Communications Security (CCS) 2012

%\documentclass{IEEEtran}
\documentclass{llncs}%{svjour3}
%\documentclass{svjour3}
\usepackage{balance}
\usepackage{cite}
\usepackage[usenames,dvipsnames]{color}
\usepackage{enumerate}
\usepackage{graphicx}
	\DeclareGraphicsExtensions{.eps}
\usepackage{hyperref}
\usepackage[hyphenbreaks]{breakurl} % this needs to be included after hyperref
\usepackage{multirow}
\usepackage{times}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{epstopdf}
\usepackage{psfrag}
\usepackage{url}
\usepackage{amsmath}
\usepackage{color}
\usepackage{soul}
\usepackage[dvipsnames]{xcolor}

\newcommand{\hashim}[1]{\textcolor{cyan}{\textbf{HS: #1}}}
\newcommand{\sasa}[1]{\textcolor{blue}{\textbf{#1}}}




\def\ptitle{Response to Reviewers' Comments on Paper:\\ Mobiprox: Supporting Dynamic Approximate Computing on Mobiles}

\def\pkeywords{}

\hypersetup{
%	bookmarks	= false,
	colorlinks	= true,
	citecolor	= black,
	linkcolor	= black,
	filecolor	= black,
	urlcolor	= black,
%	pagebackref	= true,
	pdftitle 	= {\ptitle},
	pdfkeywords	= {\pkeywords},
}

\def\eg{{\it e.g.,}\xspace}
\def\etc{{\it etc.}\xspace}
\def\ie{{\it i.e.,}\xspace}
\def\etal{{\it et al.}\xspace}
	\def\Section{{Section}\xspace}
\def\Table{{Table}\xspace}
\def\Figure{{Figure}\xspace}

\def\mytexttt{\bgroup\hyphenchar\font=45\relax\let\next=}

\newcommand{\tcircle}[1]{\textcircled{\scriptsize{#1}}}
\newcommand{\hilight}[1]{\colorbox{yellow}{#1}}

% author comments

% enable
\newif\ifcomments
\commentstrue

\ifcomments
	\newcommand{\dgen}[1]{{|\textcolor{blue}{#1}|}}
	\newcommand{\tania}[1]{{[[\textcolor{red}{#1}]]}}
	\newcommand{\stephanie}[1]{{--\textcolor{Fuchsia}{#1}--}}
	\newcommand{\ioannis}[1]{{--\textcolor{green}{#1}--}}
    \newcommand{\yifan}[1]{{\textcolor{PineGreen}{[[#1]]}}}
\else
	\newcommand{\dgen}[1]{}
	\newcommand{\tania}[1]{}
	\newcommand{\stephanie}[1]{}
	\newcommand{\ioannis}[1]{}
    \newcommand{\yifan}[1]{}
\fi

% correct bad hyphenation
\hyphenation{op-tical net-works semi-conduc-tor light-weight heavy-weight
fun-ction fun-ctionality post-authen-tication  OpenSSH authen-ticate
}

\begin{document}

\title{\ptitle}


\author{Matevž Fabjančič, Octavian Machidon, Hashim Sharif, Yifan Zhao, Saša Misailović, and Veljko Pejović}


\institute{
}




\maketitle



We would like to thank the reviewers for their valuable comments.
We have made various corrections and clarifications throughout the paper
to improve our work. All edits are highlighted blue in the revised version of 
the paper. 

Our core effort was on addressing the following issues:
\begin{itemize}
\item Verifying Mobiprox in a ten-user experiment with commodity smartphones in a realistic usage scenario;
\item Discussing the effect of different tensor operations on resource consumption of a mobile device and clarifying the applicability of Mobiprox approximations on different layer types;
\item Positioning Mobiprox within the related work on parameter pruning;
\item  Clarifying the relationship between Mobiprox and the existing deep learning frameworks, such as Pytorch, and Tensorflow Lite;
\end{itemize}

%\sasa{This rubric has changed it seems?}
Our specific answers to the reviewers' comments can be found in the following pages.
Each reviewer's comment is referred to as c\# and the related response we have 
provided with r\# (i.e., reviewers comment [c1] and our response in [r1]).


\newpage

\section*{Reviewer 1}

%\sasa{This response seems to be to all the reviewers, not just R1?!}

\textbf{[c1] First, only convolution is considered in this paper. My understanding is that convolution is the most energy-consuming operator. However, the authors need to provide some clarification as to why other operators are not included.}

\paragraph{}

[r1]  Measurements of CPU consumption conducted in Lie et al., \textit{DeepRebirth: Accelerating Deep
Neural Network Execution on Mobile Devices} and Li et al. \textit{An architecture-level analysis on deep learning models for low-impact computations} for example, demonstrate that convolutional layers remain responsible for anything from 50\% to 90\% of the CPU utilization during neural network inference. Thus, in Mobiprox we primarily focus on convolutional layer approximation. This is also in line with architectural improvements (e.g. Mobilenet, Squeezenet) targeting mobile computing, which primarily focus on reducing the computational burden of convolutional layers (for instance, through depth-wise separable convolutions). Note, however, that Mobiprox supports approximation of other network layer types as well. Indeed, its quantization approximation technique can be applied to any layer type. 
%
This is clarified in Section VIII: 

\begin{quote}
    \emph{Mobiprox is general and can be applied to any neural network architecture, yet, the richness of the approximate configurations and the efficiency of the approximation depends on the presence of convolutional layers in the network. Mobiprox specifically targets these layers with convolution perforation and filter sampling approximations, as convolutional layers tend to consume the majority of computational time and energy in mobile neural networks [47]. For non-convolutional layers, Mobiprox allows only one type of approximation -- half-precision quantization -- leading to at most $2^L$ possible approximation configurations in an L-layer network.}
\end{quote}

% \begin{quote}
%     \emph{About 80\%-90\% of inference time within modern DL architectures is spent on convolutional layers calculation [30]. This is why, through the above approximation techniques, we focus on making convolution dynamically approximable. Nevertheless, Mobiprox also provides an optional half-precision quantization that can be used to approximate any floating point tensor operation.}
% \end{quote}


\paragraph{}

\textbf{[c2] Energy consumption can vary for different hardware or operators with different configurations, and more details are needed here.}

\paragraph{}


[r2] While the absolute values of energy consumption may vary across different hardware devices, the ratio of savings stays the same, as the same Mobiprox's approximation configuration results in the same reduction of the number of FLOPS and impacts the same parameters in the same manner, irrespective of the underlying hardware, as long as the computer architecture remains comparable. We confirm this experimentally over multiple smartphone models and an ASUS Thinkerboard S single-board computer. The following text in Section VII clarifies this:
%Furthermore, targeting Android devices, we note that 99\% of the smartphones rely on one of the two ARM-based architectures -- armeabi-v7a and arm64-v8a -- both of which we experimented with. This is clarified in Section X:

\begin{quote}
    \emph{Tens of thousands of different Android devices exist on the market, however, the choice of which to use should not have a major impact on the approximate configuration profiling. First, despite different hardware, the platforms use the same OpenCL-based primitives we developed in Section IV-B2. Second, while the speed at which NN will be executed may differ among different devices, there is no reason to expect that speedups (relative to a non-approximated network) will be different for the same configuration ran on different devices. This is especially true if these devices belong to the same architectural category. Currently, Android apps can be built for four such categories, i.e. Application Binary Interfaces (ABIs). Yet, 99\% of the smartphones rely on one of the two ARM-based ABIs: armeabi-v7a and arm64-v8a\footnote{\url{https://stackoverflow.com/questions/46453457/which-android-abis-cpu-architectures-do-i-need-to-serve}}, both of which we successfully tested on ASUS TinkerBoard S and a range of phones (Samsung Galaxy M21, Samsung Galaxy S21, Xiaomi Pocophone) in our lab. For the deep learning models presented in this paper we have not observed any differences in ordering among speedups obtained by different configurations on the platforms we have experimented with. 
}
\end{quote}

%TF lite and PyTorch Mobile, on the other hand, aim to convert a network to enable it to run on a mobile (something Mobiprox does as well), but do not support approximation adaptation.

%\sasa{We can emphasize this part of the response more:}

\paragraph{}

\textbf{[c3] Second, Approx-Tunner is used to find the Pareto frontier between configurations and latency. However, long latency does not always lead to high energy consumption. It's not a linear relation on mobile devices because different chipsets have different working voltage and current, which means the power consumption varies.
}

\paragraph{}

[r3] We thank the reviewer for a very interesting observation. We agree that long latency does not, in theory, imply high energy consumption. However, NN calculations represent one of the most computationally demanding workloads on modern mobile devices. Thus, there is virtually no space for frequency scaling CPU governors to optimize the consumption through dynamic voltage-frequency scaling (DVFS). Consequently, the time spent running NN inference is \textit{directly proportional} to the energy spent for that computation. We confirm this in our experiments and show it in Figure 5. We further clarify this with the following text added to Section VII: 

\begin{quote}
    \emph{In Figure 5 we analyse how speedups translate to energy consumption reduction. We observe a clear relationship between the two lines indicating that a higher speedup indeed reduces energy consumption. Note that due to dynamic voltage-frequency scaling this relationship does not necessarily hold for any general computing task, as at light loads the CPU/GPU governor might lower the CPU/GPU frequency, and thus, the power consumption. This would lead to a more complex relationship among power, energy, and speedup. However, DL computation is, based on our experience with mobile devices, highly demanding leaving no space for the governor to reduce the frequency and make the speedup -- energy consumption relationship non-trivial. }

\end{quote}

\paragraph{}

\textbf{[c4] Third, the authors performed the experiment using a development board due to hardware limitations. As far as I know, there are already some experiences using Monsoon to power and measure the power consumption of edge devices. This could make the experiment more aligned with realistic scenarios. }

\paragraph{}

[r4] We are sorry that our initial wording caused misinterpretation. First, there is nothing specific about ASUS Thinkerboard -- it is a single-board computer that has the equivalent computing capabilities and the same computer architecture as a (lower-end) smartphone. To clarify this, we now refer to ``a development board'' as ``a single-board computer''. Second, it is convenience, not limitations that lead us to using ASUS Thinkerboard. Connecting Monsoon power meter to a smartphone is indeed possible in case of older phones with removable batteries and our team has done this in the past (see, e.g. in O. Machidon, J. Asprov, T. Fajfar, and V. Pejovic, \textit{Context-Aware Adaptation of Mobile Video Decoding Resolution}, (2023), Multimedia Tools and Applications). However, with newer unibody phones opening the device voids the warranty and leads to the phone being permanently damaged. The single-board computer, on the other hand, allows seamless energy measurements. 

However, to prove that Mobiprox indeed runs on commodity smartphones without any issues,\textbf{ we now include a ten-user experiment with Samsung Galaxy M21 smartphone running Mobiprox}. The app was tasked with inferring human activity in real time with adaptive dynamic approximation. In Section 6 we describe the experimental setup:

\begin{quote}
    \emph{\textbf{Live smartphone-based human activity recognition.} We
recruit ten users (all students or staff at University of Ljubljana, 7 female/3 male) to perform a 10-minute experiment during which they are given an option of conducting six activities – sitting, standing still, lying, walking, going up the stairs and going down the stairs. Unlike with the lab-based studies, such as [35], the order and the duration in which the activites were to be performed, was not in any way prescribed in our experiment. A Samsung Galaxy M21 smartphone, in the portrait orientation with the screen facing forward, was attached to each user’s waist. The phone sampled accelerometer and gyroscope at 50Hz, and ran Mobiprox for live on-device inference of human activity using dynamically approximated neural network. The model used was mobilenet-uci-har pretrained on UCI-HAR dataset without any further re-training. Finally, accelerometer and gyroscope samples were stored and re-ran through a non-approximated mobilenet-uci-har model to obtain the baseline activity prediction.}
\end{quote}

\noindent We then present the results of the above experiment in a newly-added Section VII D. 

\newpage

\section*{Reviewer 2}

\textbf{[c1] The paper states that “The key issue with such complexity reduction is that it often leads to a certain loss of the inference accuracy and leaves the network permanently impaired”. 
However, recent research (e.g., lottery ticket hypothesis [1]) has demonstrated that even with the removal of more than 90\% of parameters, pruned models can still maintain good accuracy. Thus, it is not accurate to claim that pruned models are permanently impaired. }

\paragraph{}

[r1] We thank the reviewer for the comment. We agree that the claim is too strong, thus have rewritten the phrase in question to emphasise that only in case the loss of accuracy happens, such a loss is permanent:

\begin{quote}
    \emph{The key issue with such complexity reduction is that the network parameters are permanently changed. Thus, in case that the resulting inference accuracy is reduced, that reduction remains permanent.}
\end{quote}

We also note that the Lottery Ticket Hypothesis uses
an unstructured pruning technique,
which allows for high sparsity ratio (e.g., more than 90\%)
without accuracy loss but introduces unstructured sparsity.
% but the unstructured sparsity also cancels out
% much of the performance and energy benefits
% unless the sparsity ratio is very high.
Structural pruning often provide
more performance and energy benefits at a lower sparsity ratio,
such as convolution perforation and filter sampling used in Mobiprox,
and filter and channel pruning (for convolutions).
The amount of accuracy loss required
to achieve a given amount of speedup / energy saving
depends greatly on the specific network architecture,
learning task, and pruning technique used.

\paragraph{}

\textbf{[c2] The paper mentions that “Common for all of the above approaches is that they do not support prebuilt networks, but require specialized training that can take days or weeks for large datasets and architectures before real-time adaptation can be used.” 
However, some methods, like Post-Training Quantization, do not necessitate prolonged training periods. }

\paragraph{}

[r2] We agree with the reviewer that the previous version of the manuscript was vague about which methods the need for training refers to. We now clarify that we refer to the dynamic adaptation approaches from the same paragraph. The text now reads:

\begin{quote}
    \emph{Common for all of the above dynamic adaptation approaches is that they do not support prebuilt networks, but require specialized training that can take days or weeks for large datasets and architectures before real-time adaptation can be used.}
\end{quote}

We note that standard post-training quantization is not a dynamic adaptation approach. We believe that the revised wording makes it unambiguous that other related approaches, sucha as dynamic quantization by AnyPrecision, do require extensive training. 

\paragraph{}


\textbf{[c3] The authors discuss the limitation of existing works by stating, "DL frameworks such as TensorFlow Lite do not support the versatility of high-level frameworks such as PyTorch." 
This paper also implements all networks in PyTorch. Does this paper solve this problem? }

\paragraph{}

[r3] The focus of our work is not to port a versatile mature framework, such as PyTorch, to mobiles. Thus, Mobiprox does not provide a general solution for the issue of mobile DL frameworks, such as TensorFlow Lite, not supporting on-demand reconfigurability. However, for a particular use case -- the approximation of NN operations through quantization, filter sampling, and perforated convolution -- Mobiprox indeed provides runtime adaptability which is not present in TensorFlow Lite. We also note that Mobiprox supports DL models written in either PyTorch or Keras. To clarify this, the intro paragraph to Section VIII now reads:

\begin{quote}
    \emph{Mobiprox builds upon the existing efforts
towards dynamic DL optmisation [11], [12], [27], [13], yet differs from them in three important ways: i) being implemented at the compiler-level and exposed as an end-to-end pipeline, Mobiprox is not limited to a particular network architecture, supports various layer types present in NNs, supports models written in either PyTorch or Keras, and does not require any previous involvement from a data scientist designing and training the network; ii) Mobiprox supports quantization, perforated convolutions, and filter sampling, but is created to be easily extensible to a wide range of approximation techniques; iii) Mobiprox allows context-dependent runtime adaptation of the approximation level; while we include two adaptation algorithms with the Mobiprox codebase, virtually any policy can be used. To facilitate future research and usage, we release Mobiprox as open-source software.}
\end{quote}

\paragraph{} 



\textbf{[c4]  Evaluation can be more comprehensive, if 
1) more NN architectures can be evaluated, like ShuffleNet and EfficientNet; 2) more complicated datasets can be evaluated, like ImageNet; 3) some related works can be compared since some works also deploy their compressed model to the edge devices, like AMC[2] and AAP[3]. 
}

\paragraph{}

[r4] Regarding 1) and 2) -- The manuscript already includes five NN architectures (ResNet, AlexNet, MobileNet, VGG16, and a custom CNN model by Sainath and Parada for spoken keyword recognition) as well as four different datasets (CIFAR-10, UCI-HAR, HAR dataset collected by Knez et al., and Google Speech Commands dataset). While these are chosen to cover a representative set of possible applications of NNs expected to work in mobile and IoT domains, they are, in a sense, generic and artificial. Therefore, for the revised version of the manuscript we decided to go a step further and, also in accordance to R1's comments, perform a real-world unscripted experiment on commodity smartphones. The details of the experiment are presented in Section VI:

\begin{quote}
    \emph{\textbf{Live smartphone-based human activity recognition.} We
recruit ten users (all students or staff at University of Ljubljana, 7 female/3 male) to perform a 10-minute experiment during which they are given an option of conducting six activities – sitting, standing still, lying, walking, going up the stairs and going down the stairs. Unlike with the lab-based studies, such as [35], the order and the duration in which the activites were to be performed, was not in any way prescribed in our experiment. A Samsung Galaxy M21 smartphone, in the portrait orientation with the screen facing forward, was attached to each user’s waist. The phone sampled accelerometer and gyroscope at 50Hz, and ran Mobiprox for live on-device inference of human activity using dynamically approximated neural network. The model used was mobilenet-uci-har pretrained on UCI-HAR dataset without any further re-training. Finally, accelerometer and gyroscope samples were stored and re-ran through a non-approximated mobilenet-uci-har model to obtain the baseline activity prediction.}
\end{quote}

\noindent The results of the above experiment are presented in a newly-added Section VII D. We are also ready to release the collected dataset, should the reviewers find it useful for future research. 



Regarding the point 3) -- The key benefit of Mobiprox is that it allows \emph{dynamic adaptation of approximation} as the application runs on Android devices. More specifically, Mobiprox allows the DNNs to operate along the whole Pareto front of \textit{accuracy loss -- speedup }shown in Figure 3 of the manuscript. In Mobiprox, the cost of moving from one DNN configuration to another requires negligible time and does no require additional memory.

Both AMC and AAP optimize models for a particular operating point in this space, but do not allow  the dynamic reconfiguration. Specifically, AMC leverages reinforcement learning to find appropriately pruned model for given constraints, such as the number of FLOPS. While such compression leads to a significant speedup (1.43x when executed on GPU) with a small loss of inference accuracy (0.4\%) on a MobileNet with ImageNet, it does not allow for the compression to be sped up, nor made more accurate at runtime. Similarly, AAP presents an innovative pruning approach, but does not allow dynamic configuration change. 
Both approaches would require storing multiple copies of the network (and loading the weights in memory) if one would need to support dynamic adaptation. Consequently, to the best of our understanding, there is no meaningful way of comparing Mobiprox with AMC and AAP, as these approaches do not share the same objectives. To clarify this, in Section II we included the following text:

\begin{quote}
    \emph{All of the above approaches share a common drawback: once the approximation is applied, the resulting network remains unchanged during runtime. Thus, such approaches enable operation at a single fixed point on the\textit{ accuracy-resource usage} trade-off curve regardless of how the context in which inference is performed changes during runtime.}
\end{quote}

\noindent In the same section we have also included references to both AMC and AAP. 


\paragraph{}



\newpage
%\input{reviewer3}
\newpage
%\input{reviewer4}






% FIXME %
% the bibliography needs to be inlined
%\bibliographystyle{IEEEtran}
%\bibliography{response}


\end{document}
