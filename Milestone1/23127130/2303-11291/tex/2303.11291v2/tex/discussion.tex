\section{Discussion and Limitations}
\label{sec:discussion}

%Need for making computing efficient

%Making this more versatile, adaptable to different configurations. 

%Dynamic adaptation that learns behavioral patterns.

%Learning when to approximate.

% Integration with existing frameworks TVM

% Adaptability using state prediction (Markov chain). Reinforcement learning to select the best strategy

Static training-time optimization is stifling further proliferation of mobile DL. Mobiprox builds upon the existing efforts towards dynamic DL optmisation~\cite{yu2020any, yu2018slimmable,laskaridis2020spinn,10.5555/3294771.3294979}, yet differs from them in three important ways: 
\textit{i)} being implemented at the compiler-level and exposed as an end-to-end pipeline, Mobiprox is not limited to a particular network architecture, supports various layer types present in NNs, \blue{supports models written in either PyTorch or Keras}, and does not require any previous involvement from a data scientist designing and training the network; \textit{ii)} Mobiprox supports quantization, perforated convolutions, and filter sampling, but is created to be easily extensible to a wide range of approximation techniques; \textit{iii)} Mobiprox allows context-dependent runtime adaptation of the approximation level; while we include two adaptation algorithms with the Mobiprox codebase, virtually any policy can be used. To facilitate future research and usage, we release Mobiprox as open-source software\footnote{\url{https://gitlab.fri.uni-lj.si/lrk/mobiprox/}}. 

% LIMITATIONS:
% - No guarantees on how far off we will get, just like in other compression methods. 
% Moreover, we could have sacrificed reliability. Further studies are needed.
% - The savings are relatively modest. With TVM we might get better. Also, this would make 
% it easier to support different architectures 
Mobiprox, is subject to certain limitations. First, despite implementing an on-device profiler to better gauge the effect of different approximations on the QoS loss, Mobiprox cannot provide guarantees that the expected QoS will indeed be achieved, nor can it predict the maximal expected QoS loss on yet-to-be-seen data. Somewhat related is the issue of potentially reduced reliability of approximated models. While compression can, in certain situations, improve the generalizability of a model~\cite{giles1994pruning}, different compression levels can lead to widely varying reliability outcomes~\cite{cygert2021robustness}. 

Second, our measurements show the maximum speedup Mobiprox achieves on a mobile device remains relatively modest at $1.25\times$, while the same NN architecture achieves twice the speedup on a server. This discrepancy likely stems from the lack of optimised support for running (approximate) deep learning on mobile devices. The goal of the  prototype version of Mobiprox presented in this paper is to, for the first time, demonstrate dynamic mobile DL approximation adaptation. To unlock further benefits, we plan to examine integration with mobile DL compiler stacks that are already hand-optimized by large engineering teams in production environments, such as TVM~\cite{chen2018tvm}, Pytorch Mobile, or TF Lite\footnote{This is also the key reason why a direct comparison between Mobiprox and current mobile DL compression implementations (e.g. quantization in TFLite) is impossible -- neither do these approaches provide dynamic approximation adaptation, nor is Mobiprox optimized for performance.}. Since the approximations in Mobiprox reduce both the number of compute operations and memory loads and stores, the performance improvements of these approximations should seamlessly translate, if efficient library and compiler implementations listed above are used. Not only would this likely lead to improved speedup gains, but would also ameliorate the need for time-inefficient development of custom approximable tensor runtimes for various architectures. 

%Frameworks, such as Pytorch Mobile and TF Lite, have to compete on the market, and are hand-optimized by large engineering teams in production environments. Mo- biprox is a research project demonstrating a novel concept, and as such, it has not been fine-tuned in production environments. Nevertheless, since the approximations in Mo- biprox reduce both the number of compute operations and memory loads and stores, the performance improvements of these approximations should extend well to efficient library and compiler implementations in future.


%\blue{Third, the power consumption profiling we conduct in this paper remains relevant for the ordering of approximate configurations in the QoS loss -- Speedup space across many devices, although it is performed on a single device. This makes any configuration ordering-based adaptation strategy feasible and generalizable. However, the absolute speedup values achieved by approximate configurations may not stay the same across different devices. Thus, adaptation strategies relying on absolute speedup values would require that the profiling devices is of the same architecture (in a narrow sense) as the target device. To avoid the impact on generalizability, in the future we plan to integrate a runtime performance measurement method based on heartbeats~\cite{hbeats2010} that will ensure runtime re-profiling of the configurations on a given execution platform.}

Third, Mobiprox is general and can be applied to any neural network architecture, yet, the richness of the approximate configurations and the efficiency of the approximation depends on the presence of convolutional layers in the network. Mobiprox specifically targets these layers with convolution perforation and filter sampling approximations, as convolutional layers tend to consume the majority of computational time and energy in mobile neural networks~\cite{li2018deeprebirth}. For non-convolutional layers, Mobiprox allows only one type of approximation -- half-precision quantization -- leading to at most $2^L$ possible approximation configurations in an L-layer network. To expand the range of approximation techniques, in future we plan to investigate the integration of dynamic pruning~\cite{10.5555/3294771.3294979} of fully-connected layers in Mobiprox. Techniques requiring up-front modification or specialized training to support approximation, such as Slimmable neural networks~\cite{yu2018slimmable}, remain unsuitable, as with Mobiprox we provide a service that allows the integration of pre-built networks oblivious to approximation, for instance, those acquired through Google Cloud AutoML, into mobile apps.

Finally, with respect to Mobiprox's adaptation algorithms (Section~\ref{sec:strategies}), these were developed for commonly encountered situations where the context does not fluctuate rapidly. Such behavior is present in numerous domains, including two examined in the previous section -- the human activity recognition where an activity a person is performing often stays the same over a certain time period, and the spoken keyword recognition, where the background noise gradually changes throughout the day. However, harnessing the slow-changing nature of many real-world phenomena, our adaptation strategies may not be suitable for tasks such as anomaly detection where sudden changes of the target phenomena are expected~\cite{pang2021deep}. Note that this does not restrict the general domain in which Mobiprox can be applied. Indeed, with minor modifications, the strategies presented in this paper can be used for adapting approximation of models built for tracking objects in live video~\cite{8003302}, for instance.
%Nevertheless, the approximation adaptation strategy is not ``hardcoded'' in Mobiprox and a developer is free to implement an arbitrary set of rules to drive the approximation configuration changes. Besides according to the volatility of the target phenomenon value, one can envision different strategies based on the available energy, device temperature, different contextual factors and others. As an immediate future work we plan to develop an adaptation strategy that considers the probability of contextual changes inferred by the model itself. More specifically, we will profile the likelihood of activity transitions and adapt our HAR inference model towards less-approximated configurations should it yield an unlikely transition. Another Mobiprox extension planned is a meta-method for selecting among possible adaptation strategies. We will implement a reinforcement learning framework that given a list of adaptation strategies and a reward function finds the most suitable strategy at runtime. 

%Mobiprox opens wide space for developing real-time approximation adaptation strategies. Strategies presented in this paper (Section~\ref{sec:strategies}) are relatively simple and guided by the properties of the input data, 


%adheres to this research trend and enables adaptable approximations of deep neural networks to run on mobile devices. The contribution of Mobiprox to the field of resource-efficient mobile deep learning is threefold: it is generic and implemented at the compiler-level (not limited to a particular network architecture) \hashim{the text in the paranthesis can be in the sentence}, supports a plethora of approximation techniques (quantization, perforated convolutions, filter sampling, and reduction sampling) \hashim{can instead say it is "extensible" to all kinds of approximation techinques - or rather a wide range of approximation techniques} and allows a dynamic and real-time adaptation of the approximation level during runtime, resulting in a variable speedup/accuracy trade-off. 
%\hashim{this last point about dynamic tuning is not novel - it is novel when you talk about dynamic tuning in a "context-dependent" fashion.}
%\hashim{These three capabilities highlight what's novel about Mobiprox but my impression is that very few reviewers read discussion sections. Shouldn't we talk about these in the intro as well? To make sure these are highlighted}

%One of the key features of Mobiprox is versatility: not only it allows running approximations on virtually any deep neural network architecture, it includes 3 different approximation adaptation strategies that were experimentally validated and which are capable of choosing the most suitable approximation configuration for each inference datapoint; thus, Mobiprox allows selecting the most suitable adaptation strategy based on the application and the particularities of the input on which inference is performed. This enables Mobiprox to ``learn'' from the previously infered samples behavioral patterns which impact the `difficulty'' level of the input at each inference point and decide on when and how to approximate (and what approximations to choose at each step) in order to optimize computation and energy efficiency while at the same time maximizing the inference accuracy.

%\hashim{Generally (not necessarily) discussion section has some text on "future work" do you want to add few sentences about the implications of Mobiprox on future mobile systems?}

%

% FUTURE WORK:
% - Predicting when approximation is useful. Markov model of human activity to 
% guide adaptation... 
% - reinforcement learning
% - Opened space for developing different strategies, for instance those based on the 
% energy availability. 