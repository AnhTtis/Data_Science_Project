\section{Mobiprox Framework}
\label{sec:mobiprox}

%\sasa{this is a good motivation -- may be emphasized in the intro more.}
%\textbf{Data scientists are not mobile system experts.} Our guiding vision is that deep learning modeling should be disentangled from system-level performance optimization. Mobiprox aims to support efficient on-device execution of any pre-trained mobile network architecture based on convolutional and fully-connected layers. Furthermore, we do not require that a developer knows which optimizations (in our case -- execution approximations) are available on the device. Still, we give a developer an option of (dynamically) setting an operational point along \textit{the inference accuracy -- resource usage} trade-off line, yet, in the limit case, the developer need not even set this point, but merely let Mobiprox tune the execution according to its internal approximation adaptation algorithms.


%\subsection{Overview}
%\label{sec:mobiprox-overview}
Mobiprox, our novel framework for enabling dynamic approximation of mobile DL, is sketched in Figure~\ref{fig:amc-pipeline}. An Android app compiled with Mobiprox can use an arbitrary runtime approximation adaptation strategy for its DL models (e.g., ``run low quality network when battery is low'', ``run high quality inference when user is at a specific location'', etc.). To achieve this, Mobiprox operates with approximation configurations, i.e. combinations of per-layer approximations of a pre-trained DL model. Mobiprox first uses ApproxTuner to examine the impact of different configurations on the inference accuracy and the speedup. Each approximation configuration yields a point in the accuracy--speedup space, and Mobiprox identifies the most promising configurations that form the Pareto front in this space and then profiles their actual performance on the mobile platform using the novel \textit{HPVM Profilier for Android}. Mobiprox's Android-based \textit{OpenCL runtime} then enables execution of and dynamic switching between approximation configurations on a mobile device. Using the \textit{JNI interface library} generated by the framework, the mobile application can control the approximation level of the NN. Finally, as part of Mobiprox, we also devise \textit{Approximation adaptation strategies }that leverage the generated trade-off curves to match the required and delivered quality of computation, thus enable energy-efficient DL on mobile devices. 
%\hashim{This last point is the new contribution in Mobiprox IMO. It's buried too deep in the para. I would suggest starting with it and reversing where you talk about ApproxTuner at the end - you want to say ApproxTuner is just a tool you use for approximation-selection - and you could use any other approximation tool similarly. The key capability of context-dependent optimization in Mobiprox seems agnostic of the choice of approximation-tuning tool used. }

%Our solution harnesses ApproxTuner (AT), a compiler and optimiser that investigates the effect of different  per-operation approximations on the inference accuracy of a neural network. In addition, AT identifies energy-accuracy trade-off points achieved by different sets of approximations applied to layers of a neural network. Yet, AT does not natively support mobile devices, nor does it provide any guidance on which approximation trade-off point to operate on at a particular time, which is crucial, having in mind that user requirements and available resources may vary in the mobile context.

%In Mobiprox we addresses these limitations by first extending ApproxTuner's runtime with a tensor operation back-end for mobile devices that is capable of executing approximated neural network inference; second, we devise new tools for profiling approximated neural networks, so their true performance on Android devices can be characterised; finally, we devise new adaptation methods that leverage generated approximation trade-off curves to match the required and the delivered quality of computation, thus enabling energy-efficient DL on mobile devices. 




\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/system-diagram.png}
    \caption{Mobiprox overview. OpenCL run-time supports running the inference binary (controlled either directly from the C code, or via JNI from the main Java/Kotlin app) with a varying level of approximation. The HPVM Profiler for Android helps us chart the \textit{approximation -- resource usage} space, so that the Approximation adaptation strategy wihtin the Android app can set the approximation level dynamically at runtime. Main Mobiprox modules are colored green, while the supporting pre-existing modules are grayed out.}
    \label{fig:amc-pipeline}
\end{figure}

\subsection{Charting approximation space} 
\label{sec:mobiprox:charting}
%Neural Network approximation using ApproxHPVM and ApproxTuner}

%\sasa{the next 3 paragraphs can be condensed. Put more emphasis on what is new in Mobiprox -- I like the discussion starting from "However, the above method does not readily translate...". It's better to present first your method and then say why the existing one did not work, for readers' attention. }

Each of the approximation techniques described in Section~\ref{sec:preliminaries-acts} exposes one or more \textbf{approximation knobs} that can change the level of approximation and thus adjust the accuracy and the execution time (consequently the energy efficiency) of a tensor operation. These knobs are \ttit{offset} and \ttit{stride} for convolution perforation and filter sampling, and an indicator \texttt{\_fp16} of whether an operation is executed using half-precision quantization. An \textbf{approximation configuration} is a set of pairs $\langle \mathrm{Op.}, \mathrm{KnobValue} \rangle$ for all operations in a given NN. Each of the configurations leads to a single \textbf{trade-off point} on an speedup-accuracy trade-off curve. 

%An \textbf{HPVM approximation configuration file} is a collection of approximation configurations, or a \textbf{trade-off curve}. This file is read at the initialisation stage of an ApproxHPVM-transformed neural network.

%Table~\ref{tab:knobs} shows the available approximation knobs for these approximation techniques. The behaviour of each knob is defined by its knob name and its parameters. For the three described approximations, only parameters \ttit{stride} and \ttit{offset} define the approximation. Parameter~\ttit{stride} defines the order of approximations. Parameter~\ttit{offset} defines the index of the first omitted row or column in the case of row and column perforations respectively and the index of the first eliminated component in the case of filter sampling. Additionally, each operation can be run at reduced precision with 16-bit floating point data (indicated by \texttt{\_fp16} extension).

% \begin{table}[H]
%     \caption{Convolution approximation knobs and their parameters. Parameters \ttit{stride} and \ttit{offset} define the approximation; each operation can be run at reduced precision with 16-bit floating point data (indicated by \texttt{\_fp16} extension).}
%     \begin{center}
%     \begin{tabular}{lll}
%         \toprule
%         Approximation type & Knob name & Knob parameters \\
%         \midrule
%         Row perforation     & {{\tt perf}[{\tt\_fp16}]} & \ttit{stride}, \texttt{1}, \ttit{offset} \\
%         Column  perforation & {{\tt perf}[{\tt\_fp16}]} & \texttt{1}, \ttit{stride}, \ttit{offset} \\
%         Filter sampling     & {{\tt samp}[{\tt\_fp16}]} & \ttit{stride}, \ttit{offset}, \textit{unused} \\
%         \bottomrule
%         \end{tabular}
%     \end{center}
% \label{tab:knobs}
% \end{table}

%Once a neural network is compiled using ApproxHPVM, 
The tuner heuristically searches the space of possible approximations and determines a Pareto frontier of approximation configurations that maximise the execution speedup at different \textbf{quality of service (QoS) loss} points. This loss is a real number defined as a difference between the classification accuracy, over a representative validation dataset, of a non-approximated and an approximated DL model. %Two different methods of determining the approximated model's loss are offered: 1) {empirical tuning} that determines the loss by testing the inference accuracy of a full approximated DL model, and a faster 2) {predictive tuning} where the impact of an approximation configuration is estimated through a compositional error model that analytically combines the accuracy loss observed at individual network layers. 


%However, the amount of different configurations is too large to search exhaustively, thus, OpenTuner\cite{opentuner} is used to heuristically search the space of supported approximation parameters. 
%The goal of the tuning is to maximise the execution speedup 
% 
%\newcommand{\qos}{\mathrm{QoS}\relax}
%\newcommand{\qosLoss}{\qos_\mathrm{loss}\relax}
%\newcommand{\qosBase}{\qos_\mathrm{base}\relax}
%\newcommand{\qosApprox}{\qos_\mathrm{approx}\relax}
% 
%\begin{equation}
%    \label{eq:qosloss}
%    \qosLoss = \qosBase - \qosApprox,    
%\end{equation}
% 
%$\qosLoss = \qosBase - \qosApprox$,  where $\qosBase$ is the classification accuracy of a non-approximated model and $\qosApprox$ is the classification accuracy of an approximated model. 

%ApproxTuner offers two different methods of determining $\qosApprox$. When using \textbf{empirical tuning}, $\qosApprox$ is determined by executing a complete neural network inference using the ApproxHPVM-compiled network. However, such inference is computationally expensive. ApproxTuner speeds up the optimisation metric evaluation by introducing \textbf{predictive tuning}. With predictive tuning, the impact of all \textit{approximation knobs} on $\qosApprox$ is measured separately for each layer in the neural network. Different combinations of approximation knob values yield approximation configurations and the search for optimal approximation configurations is guided by error composition models, which define how to combine per-layer information of approximations into the final $\qosApprox$. An example of such a composition model would be a linear combination of QoS losses contributed by approximation knobs used in an approximation configuration. 

However, the method described above for determining the optimal approximation configurations does not readily translate to mobile devices. The mobile platform is substantially different from the server used for fast heuristic-based approximation configuration profiling. The specifics of GPU-based execution (e.g., CUDA vs OpenCL), heterogeneous CPUs with fewer cores, and other factors mean that the results of the configuration search performed on a server are a rather poor representation of the actual approximated NN performance on a mobile. In Figure~\ref{fig:combined-pareto-android}, on the example of a MobileNetV2 model used for HAR (detailed in Section~\ref{sec:methodology}), we show the actual on-mobile-device speedup and QoS loss achieved by the approximation configurations that ApproxTuner identified as the most promising. While the Pareto points obtained on a server generally remain relevant, the achieved on-device speedup is about 50\% lower on the mobile.

% \begin{figure}[!htb]
%     \newcommand{\w}{0.45\linewidth}
%     \centering
%     \begin{subfigure}{\w}
%         \centering
%         \includegraphics[width=\linewidth]{figures/pareto-all.pdf}
%         \caption{Pareto frontier (blue line) of generated approximation configurations (red dots) as determined by tuning on a server.}
%         \label{fig:combined-pareto}
%     \end{subfigure}
%     \hspace{1cm}
%     \begin{subfigure}{\w}
%         \centering
%         \includegraphics[width=\linewidth]{figures/pareto-android.pdf}
%         \caption{Configurations from Figure~\ref{fig:combined-pareto} Pareto frontier profiled on Android ASUS TinkerBoard S. Configurations that defy the Pareto front principle are marked as outliers.
%         }%\sasa{The difference between the predicted and obtained speedups on android. Hashim can chime in here}}
%         \label{fig:combined-pareto-android}
%     \end{subfigure}
%     \caption{Comparison of the achieved speedup and the resulting QoS (inference accuracy) loss for approximation configurations selected by the on-server tuning with the same configurations ran on a mobile platform. Note the different scaling of the y-axis.}
%     \label{fig:pareto}
% \end{figure}


\begin{figure} 
    \centering
  \subfloat[Tuning on server\label{fig:combined-pareto}]{%
       \includegraphics[width=\linewidth]{figures/pareto-all.pdf}}
    \hfill
  \subfloat[Tuning on mobile\label{fig:combined-pareto-android}]{%
        \includegraphics[width=\linewidth]{figures/pareto-android.pdf}}
    
  \caption{Comparison of the achieved speedup and the resulting QoS (inference accuracy) loss for approximation configurations selected by the on-server tuning with the same configurations ran on a mobile platform. Note the different scaling of the y-axis.}
  \label{fig:pareto} 
\end{figure}


% NOTE: Taken out in the IMWUT submission only, as the figure does not agree with this text:
%
%Second, with Mobiprox we aim to support a range of neural networks used for diverse tasks on mobile devices, such those that rely on processing multimodal sensor data. ApproxTuner, on the other hand, predominantly targets computer vision tasks. This, unfortunately, limits the applicability of its tuning. We compare the predicted and the actual performance of configurations identified by ApproxTuner for two neural networks of the same architecture (MobileNetV2) but used for different tasks -- image recognition and HAR from accelerometer data. With the same tuning algorithm we achieved vastly different results. In the case of images, ApproxTuner gives accurate performance predictions. However, tuning on the HAR network yields approximations exhibiting an order of magnitude larger QoS losses than predicted. 
%

%
%In Figure~\ref{fig:approxtuner-compare-ucihar-cifar10} we compare the predicted and the actual performance of configurations identified by ApproxTuner's predictive tuning for two neural networks of the same architecture (MobileNetV2) but used for different tasks -- image recognition ({\mnCifar}) and human activity recognition from accelerometer data ({\mnUci}). With the same tuning algorithm we achieved vastly different results. In the case of {\mnCifar}, ApproxTuner achieves accurate predictions. However, predictive tuning {\mnUci} yielded approximations exhibiting an order of magnitude larger QoS losses than predicted.

% \begin{figure}[!htb]
%      \newcommand{\w}{\linewidth}
%     \newcommand{\sw}{0.43\linewidth}
%     \centering
%     \begin{subfigure}{\sw}
%         \centering
%         \includegraphics[width=\w]{figures/tuning/mncifar-pred-p1.pdf}
%         \caption{{\mnCifar}}
%         \label{fig:conf-cifar10-p1}
%     \end{subfigure}
%     \hspace{1cm}
%     \begin{subfigure}{\sw}
%         \centering
%         \includegraphics[width=\w]{figures/tuning/mnuci-pred-p1.pdf}
%         \caption{{\mnUci}}
%         \label{fig:conf-ucihar-p1}
%     \end{subfigure}
%     \caption{Visualisations of \textbf{predictive tuning} for two neural networks. Points represent energy-accuracy trade-off points (approximation configurations).\sasa{I still don't think we should keep this plot in the paper. Fine to say in the words only what was said as the justification of empirical tuning}}
%     \label{fig:approxtuner-compare-ucihar-cifar10}
%     %\sasa{This plot should go away -- the second uci-har plot does not show a good result. }%octavian: I removed the left-sided subfigures in each case, they were confusing. We think leaving this might be of use to discuss how Mobiprox behaves on image data vs. time-domain data%
% \end{figure}

Mobiprox therefore introduces a novel configuration identification approach. First, we perform tuning on a computer cluster to identify candidate approximation configurations. Then, we develop an Android-based profiler (described in Section~\ref{sec:hpvm-profiler-android}) that runs each candidate configuration on a mobile device and obtains a realistic picture of the approximated neural network performance. The resulting picture of the speedup -- QoS loss space charted by these configurations is then used to guide the dynamic adaptation of the approximation. As a final result, the profiler creates a file listing configurations that will be switched during the mobile app runtime (according to a strategy, e.g. from Section~\ref{sec:strategies}), yet\textit{ only a single network model definition gets deployed on a mobile}.

%Table~\ref{tab:knobs} shows the available approximation knobs for these approximation techniques. The behaviour of each knob is defined by its knob name and its parameters. For the three described approximations, only parameters \ttit{stride} and \ttit{offset} define the approximation. Parameter~\ttit{stride} defines the order of approximations. Parameter~\ttit{offset} defines the index of the first omitted row or column in the case of row and column perforations respectively and the index of the first eliminated component in the case of filter sampling. Additionally, each operation can be run at reduced precision with 16-bit floating point data (indicated by \texttt{\_fp16} extension).


\subsection{Mobiprox -- Android implementation}

Mobiprox, as a concept, is not tied to a particular mobile platform. Yet, amassing 75\% of the smartphone market share Android is the most common mobile deep learning platform and that stands to gain the most from dynamically adaptable approximation, thus, in this section we develop a full Mobiprox compilation pipeline targeting Android devices. 

\subsubsection{Mobiprox Android Compiler}
\label{sec:ndk-integration}

\newcommand{\ndkversion}{21.4.7075529}

Mobile application development with Mobiprox involves compiling the tuning binary and the inference binary (Figure~\ref{fig:amc-pipeline}). While the tuning binary is confined to the server environment and is handled by the ApproxHPVM compilation pipeline, the inference binary is cross-compiled from a server to a mobile (Android). We implement a mechanism for turn-taking between ApproxHPVM and Android NDK LLVM compiler toolchains (Algorithm~\ref{alg:llvm-compile}). We enable this by clearly partitioning the compilation steps and harnessing the fact that LLVM-based compilers apply transformations to an intermediate representation termed LLVM-IR. Note that ApproxHPVM extends LLVM-IR by defining HPVM-IR to which approximation-related transformations are applied. This clear division allows us to use Android NDK for generating the initial LLVM-IR suitable for Android applications and for generating the machine code containing approximate NN operations suitable for mobile GPUs in the final compilation step, while using HPVM-IR transformations for the internal part of the compilation pipeline to insert the description of the desired approximate tensor operations.

 %However, it is crucial that versions of both compilers match. This limits advancements of both compilers -- using newer Android NDK versions or upgrading the base of ApproxHPVM would likely result in compilation errors. %Furthermore, we deem our usage of both compilers case-specific and experimental.


%-- ApproxHPVM and the Android NDK -- to compile neural networks into libraries suitable for execution on an Android device. The ApproxHPVM compiler is implemented as a collection of patches and extensions to the LLVM source code. After these patches are applied, the build process is similar to how LLVM is built.

%At the time of writing, LLVM 9.0.0 is used as the foundation. Since Android NDK's adaptation of LLVM (from now on Android-LLVM) is an open-source project\footnote{Android Open Source Project: \url{https://source.android.com/}} we were leaning towards replacing LLVM 9.0.0 with Android-LLVM. This would enable ApproxHPVM to compile high-level descriptions of neural networks directly into native libraries for Android. However, due to the vast size of both LLVM 9.0.0 and Android-LLVM, this approach did not seem feasible.

%Instead, we chose a different approach.
%Since Android NDK version {\ndkversion}  is based on LLVM 9.0.9, we adapted the HPVM compilation pipeline to use \textbf{both} compilers (ApproxHPVM and Android-LLVM), each at different steps during compilation. This was made possible by clear partitioning of the compilation steps: LLVM-based compilers apply transformations to an intermediate representation called LLVM-IR. HPVM and ApproxHPVM extend LLVM-IR by defining HPVM-IR, on which their transformations are applied. We present this partitioning with Algorithm~\ref{alg:llvm-compile}.

% \begin{figure}
% \caption{ApproxHPVM compilation. Compilers used at each step are shown within curly braces.}
% \label{alg:llvm-compile}
% \newcommand{\IRL}{\mathrm{IR}_\mathrm{LLVM}}
% \newcommand{\IRH}{\mathrm{IR}_\mathrm{HPVM}}

% \begin{algorithmic}[1]
% \STATE $\IRL \gets$ Transform source code into LLVM-IR \COMMENT{\textbf{Android LLVM}}
% \STATE $\IRH \gets$ Transform $\IRL$ code into HPVM-IR \COMMENT{\textbf{ApproxHPVM}}
% \label{alg:llvm-compile:for}
% \FOR{\textbf{each} IR transformation $T_i$ of the compiler} 
% \STATE $\IRH \gets T_i(\IRH)$ \COMMENT{\textbf{ApproxHPVM}}
% \ENDFOR
% \STATE $\IRL \gets$ Transform $\IRH$ into LLVM-IR \COMMENT{\textbf{ApproxHPVM}}
% \label{alg:llvm-compile:endfor}
% \STATE Compile $\IRL$ to machine code \COMMENT{\textbf{Android LLVM}}
% \end{algorithmic}
% \end{figure}


% \begin{figure}[!t]
% \newcommand{\IRL}{\mathrm{IR}_\mathrm{LLVM}}
% \newcommand{\IRH}{\mathrm{IR}_\mathrm{HPVM}}
%  \removelatexerror
%   \begin{algorithm}[H]
% \caption{ApproxHPVM compilation. Compilers used at each step are shown within curly braces.}
% \STATE: $\IRL \gets$ Transform source code into LLVM-IR \COMMENT{\textbf{Android LLVM}}
% \STATE $\IRH \gets$ Transform $\IRL$ code into HPVM-IR \COMMENT{\textbf{ApproxHPVM}}
% \label{alg:llvm-compile:for}
% \FOR{\textbf{each} IR transformation $T_i$ of the compiler} 
% \STATE $\IRH \gets T_i(\IRH)$ \COMMENT{\textbf{ApproxHPVM}}
% \ENDFOR
% \STATE $\IRL \gets$ Transform $\IRH$ into LLVM-IR \COMMENT{\textbf{ApproxHPVM}}
% \label{alg:llvm-compile:endfor}
% \STATE Compile $\IRL$ to machine code \COMMENT{\textbf{Android LLVM}}
%   \end{algorithm}
% \end{figure}

\begin{algorithm}[h]
\SetAlgoLined
\caption{Mobiprox compilation. Compilers used at each step are shown in comments.}
\label{alg:llvm-compile}
\newcommand{\IRL}{IR_{LLVM}}
\newcommand{\IRH}{{IR}_{HPVM}}
\begin{scriptsize}
   
$\IRL \gets$ Transform source code into LLVM-IR \tcp*{\textbf{Android LLVM}}
$\IRH \gets$ Transform $\IRL$ into HPVM-IR \tcp*{\textbf{ApproxHPVM}}
\For{\textbf{each} IR transformation $T_i$ of the compiler} {
$\IRH \gets T_i(\IRH)$ \tcp*{\textbf{ApproxHPVM}}
}
$\IRL \gets$ Transform $\IRH$ into LLVM-IR \tcp*{\textbf{ApproxHPVM}}
Compile $\IRL$ to machine code \tcp*{\textbf{Android LLVM}}
\end{scriptsize}
\end{algorithm}

%This clear partitioning allows us to use Android NDK for generating LLVM-IR and the final compilation step (generating machine code), while using ApproxHPVM's LLVM transformations for the internal part of the compilation pipeline (e.g. inserting approximate tensor operations). However, it is crucial that versions of both compilers match. This limits advancements of both compilers -- using newer Android NDK versions or upgrading the base of ApproxHPVM would likely result in compilation errors. %Furthermore, we deem our usage of both compilers case-specific and experimental.
%Ideally, we would be able to use a single compiler for the whole compilation process. 



% LLVM IR, compiler compatibility, required adaptations.

%\subsubsection{HPVM Runtime for Android}
%\label{sec:android-hpvm-rt}

%The next step in the HPVM compilation pipeline (before machine code generation) is linking the LLVM IR with the HPVM Runtime library (\texttt{hpvm-rt}). This library implements the logic for initialising HPVM, executing HPVM's dataflow graphs on heterogeneous hardware and cleaning up resources upon program termination. The Mobiprox pipeline does not use this library directly, however, ApproxHPVM relies on the functionality implemented by this library.

%To enable successful compilation for Android devices, we had to enable the use of this library on Android devices. This was accomplished by defining patches to the original source code of \texttt{hpvm-rt}, which fixed incompatibilities between LLVM-IR generated by Android-LLVM and HPVM itself. However, following the release of ApproxHPVM and increments of both the LLVM used in HPVM and the version of Android-LLVM, these patches are no longer required.
%Instructions and required source files for building the library for Android are published on GitHub\footnote{HPVM RT for Android: \url{https://github.com/MatevzFa/hpvm-rt-android}}.

\subsubsection{OpenCL Tensor Runtime for Android}
\label{sec:android-rt}

A core component of Mobiprox Android is a Tensor Runtime, which implements tuneable approximable tensor operations for NN inference. The existing support for approximate NN operations for Nvidia CUDA GPUs~\cite{sharif2019approxhpvm} is not suitable for mobiles, which seldom host such hardware. Instead, Mobiprox implements an own tensor runtime using OpenCL, an open standard for GPU-accelerated computation which is available on a wide variety of hardware, including mobile platforms.

%Our first version of the HPVM Tensor Runtime for Android was based on \textbf{ARM ComputeLibrary}\footnote{ARM ComputeLibrary:  \url{https://github.com/ARM-software/ComputeLibrary}}. It is a library optimised for ARM Mali graphics processing units. We chose it as it implements a wide variety of machine learning functions in OpenCL. Furthermore, it is an open source library, which made it easier to implement approximate variants of convolution algorithms. A key optimisation in ARM ComputeLibrary revolves around memory management:
%
%\begin{itemize}[itemsep=-0.2em, topsep=0.2em]
%    \item As described in section~\ref{sec:cnn}, convolutions are implemented as a matrix multiplication of transformed input images and kernel matrices. This introduces additional memory requirements into a neural network computation algorithm.
    
%\item In GPU accelerated algorithms, memory access optimisation is crucial. To make it possible to use vectorised memory reads regardless of matrix size, ARM ComputeLibrary adds padding to all matrices.
%\end{itemize}
%
%The library reduces these additional memory requirements by reusing already allocated blocks of memory that are no longer needed. Thus, it requires knowledge on relationships between NN operations for  successful memory management. However, NN operations in the HPVM Tensor Runtime are implemented as independent operations, which makes aforementioned optimisations impossible without redesigning ApproxHPVM's compilation process.

To enable an enhanced control over low-level concepts (such as memory allocation), we implemented the tensor runtime for Android using CLBlast~\cite{clblast}, an OpenCL implementation of basic linear algebra subprograms (BLAS). However, this library is not intended for deep learning: it does not implement operations commonly used in NNs. Therefore we extended CLBlast with the following operators: \textit{i)} Point-wise tensor addition, \textit{ii)} Bias addition, \textit{iii)} Activation functions (ReLU, clipped ReLU, $tanh$), \textit{iv)} FP-16 -- FP-32 tensor conversion, \textit{v)} Batch normalisation, \textit{vi)} Pooling ($min$, $max$, $average$), \textit{vii)} Convolution approximations operators optimized with tiling and vectorization: \textit{Image-to-Column} ($im2col$) transformations with row perforation, column perforation, and filter sampling, \textit{Kernel-to-Row} ($kn2row$) transformation with filter sampling, and \textit{Interpolation} of missing values in convolution perforation. % \textit{viii}, \textit{ix}, \textit{x} 
%     \item Point-wise tensor addition,
%     \item Bias addition,
%     \item Activation functions (ReLU, clipped ReLU, $tanh$),
%     \item FP-16 -- FP-32 tensor conversion,
%     \item Batch normalisation,
%     \item Pooling ($min$, $max$, $average$),
%     \item Convolution approximations operators optimised with tiling and vectorisation:
%     \begin{itemize}
%         \item \textit{Image-to-Column} ($im2col$) transformations with row perforation, column perforation, and filter sampling,
%         \item \textit{Kernel-to-Row} ($kn2row$) transformation with filter sampling,
%         \item Interpolation of missing values in convolution perforation.
%     \end{itemize}
% \end{itemize}
%
%We validate our implementation of the HPVM Tensor Runtime by comparing the results of running each operation with the results of running the same operation through the reference implementation targeting desktop CPUs. 
Finally, during the mobile app compilation Java Native Interface (JNI) is exposed, enabling the tensor runtime initialization and destruction, NN inference invocation, and dynamic approximation configuration loading.

\subsubsection{HPVM Profiler for Android}
\label{sec:hpvm-profiler-android}

To assess the speedups and consequently the energy efficiency of approximated NNs we implement a profiler tool. The profiler, in the form of a Python library, for a given NN binary measures the accuracy, softmax confidence, and execution time of NN inference on a given test dataset. Due to a high discrepancy between the speedup observed on a mobile device and on a server for the same approximated network (Figure~\ref{fig:pareto}), the profiler uses the Android Debug Bridge (ADB)~\cite{adb} to run measurements on an actual Android device and to transfer the profiling information files back to the host machine for analysis.

%The ApproxTuner project features a profiler tool (\texttt{hpvm-profiler}) which is used to measure speedups and consequently the energy efficiency of compiled neural networks. The profiler is implemented as a Python library that enables selecting an inference binary for profiling and recording profiling information (accuracy, execution time) while executing NN inference on a test data set. It does so by running the inference binary in a sub-process, waiting for its termination, and parsing profiling information files produced by the inference binary. However, the library is designed to run on the same device as the inference binary, and is therefore not usable in our use-case of running the neural networks on Android devices.

%To run profiling in the same manner on Android, Mobiprox implements an adapted version of the profiler (\texttt{hpvm-profiler-android}) that uses Android Debug Bridge (ADB)~\cite{adb} to run measurements on the target Android device and transfer profiling information files back to the host machine for analysis.

% \subsection{Case study: MobileNet-V2 approximation using ApproxTuner for HAR classification}

% \sasa{this can be a section on its own --- also consider moving it to the front instead of related work... }

% We use ApproxTuner to determine a pareto-frontier of approximation configurations for a MobileNet-V2 convolutional neural network trained to perform Human Activity Recognition in Pytorch.

% \subsubsection{MobileNet for Human Activity Recognition}
% \label{sec:mnUci}

% MobileNet~\cite{howard2017mobilenets} is a class of neural network architectures proposed in 2017 that focuses on efficient computer vision applications in embedded systems. MobileNet was chosen for our classification since it was designed to be efficient when executed on power-limited devices and is therefore challenging to optimise further. In addition, it has been used for human activity recognition before~\cite{machidon2021queen}.

% Being designed for computer vision applications, MobileNet is natively works with $32\times32$ 3-channel images. However, human activity recognition data sets, such as the UCI-HAR data set~\cite{anguita2013public}, are commonly constructed on time-domain sensor data (3-axial acceleration, 3-axial rotation). Each data point in the UCI-HAR data set represents a 2.5 second time frame of 128 sensor readings per axis ($x$, $y$, $z$) for each of the 3 sensors (body acceleration, total body acceleration, body rotation). %A more in-depth description of these sensors is presented in section~\ref{sec:android-signalimage}.

% To use the UCI-HAR data set with MobileNet, a transformation of sensor data is required. In our signal images, image channels represent different sensors (body acceleration, total body acceleration, body rotation). Figure~\ref{fig:signal-image} shows how we arranged the feature vectors within each image channel. Feature vectors for axes $x$, $y$ and $z$ were vertically stacked into a $8\times 128$ matrix. Feature vectors for each axis were repeated multiple times, as shown in Figure~\ref{fig:signal-image-in}. Following a matrix transformation, the final $32~\times~32~\times~3$ signal image (Figure~\ref{fig:signal-image-out}) has the following property: neighbouring features in the signal image carry information that is neighbouring in the time domain\footnote{Due to signal image construction technique, this does not hold at the borders of each 32-row block, as visible from colour changes shown in figure~\ref{fig:signal-image-out}.}. This property is highlighted in~\cite{jiang2015human}, which inspired our signal image composition technique. The importance of this aspect and caveats of such signal image composition are detailed in section~\ref{sec:mnUci-tuning}.

% \begin{figure}
%     \newcommand{\w}{\linewidth}
%     \newcommand{\sw}{0.45\linewidth}
%     \centering
    
%     \begin{subfigure}{0.47\w}
%         \centering
%         \includegraphics[width=\w]{figures/signal-image.pdf}
%         \caption{Matrix of stacked feature vectors with size $128~\times~8~\times~3$.}
%         \label{fig:signal-image-in}
%     \end{subfigure}
%     \hspace{1cm}
%     \begin{subfigure}{0.38\w}
%         \centering
%         \includegraphics[width=\w]{figures/signal-image-final.pdf}
%         \caption{Final signal image with size $32~\times~32~\times~3$.}
%         \label{fig:signal-image-out}
%     \end{subfigure}
%     \caption{Signal image composition. Horizontal blocks of data in figure (a) are arranged vertically in figure (b).}
%     \label{fig:signal-image}
% \end{figure}

% We implemented the MobileNet-V2 network architecture (\mnUci{}) using PyTorch. The NN was trained on the UCI-HAR training data set composed of 7352 data points (signal images). We tested our model on the test data set containing 2947 data points. The model achieved a classification accuracy of 90\% on the test data set.
% We also investigated per-class accuracies with a confusion matrix shown in Figure~\ref{fig:mnuci-confusion}.

% \begin{figure}
%     \centering
%     \includegraphics[width=3.5in]{figures/mobilenet_uci-har_0.90.pth.pdf}
%     \hspace{1.3cm}
%     \caption{Confusion matrix for \mnUci{}.}
%     \label{fig:mnuci-confusion}
% \end{figure}


% \begin{itemize}
%     \item Tuning a MobileNet for HAR task turns out to be a unique task
%     \item Hypotesis: CNNs on signal images behave differently as RGB images. RGB images are locally smooth in all directions, singal images are not.
%     \item Show how kernels look like in the trained MobileNET on UCI-HAR and on CIFAR-10.
% \end{itemize}

% \subsubsection{Approximating MobileNet for HAR using ApproxTuner}
% \label{sec:mnUci-tuning}

% To use an approximated neural network for the HAR classification task, we used ApproxTuner to determine approximation configurations suitable for this particular neural network. As described in section~\ref{sec:approxtuner-about}, ApproxTuner offers two avenues of determining approximation configurations. \textit{Predictive} tuning is based on knowledge about the behaviour of various NN approximations that is obtained as a pre-processing step to the iterative optimisation, while \textit{empirical} tuning is evaluating configurations during the iterative optimisation.

% We identified a unique property of our NN ({\mnUci}) compared to image classification networks used in evaluation of ApproxTuner. In our case, predictive tuning predicted accuracy losses that poorly resemble real accuracy losses. In Figure~\ref{fig:approxtuner-compare-ucihar-cifar10}, we compare these tuning results with those for {\mnCifar}. We can observe that with the same tuning algorithm, we achieved vastly different results. In the case of {\mnCifar}, ApproxTuner achieved accurate predictions, which resulted in valid approximation configurations. However, predictive tuning {\mnUci}  yielded approximations that yield QoS losses of up to 60\%, which makes these approximations unusable.


% To confirm our hypothesis that prediction models poorly resemble behaviour of NN {\mnUci}, we used ApproxTuner's \textit{empirical tuning}. Figure~\ref{fig:approxtuner-ucihar-empirical} shows results of this tuning approach. We can see that with empirical tuning ApproxTuner successfully finds. Furthermore, in Figure~\ref{fig:filters} we take a look at the filters of both neural networks. We can see that component values in convolution filters of {\mnCifar} transition smoothly, while in {\mnUci} the differences between neighbouring components are greater. Additionally, we can observe similarity between neighbouring components among the horizontal dimension in the case of {\mnUci}, which we believe is a consequence of the signal-image composition described in section~\ref{sec:mnUci}.

% \begin{figure}[!htb]
%     \newcommand{\sw}{0.75\linewidth}
%     \newcommand{\w}{\linewidth}
%     \centering
%     \includegraphics[width=\sw]{figures/tuning/mnuci-empirical.pdf}
%     \caption{Empirical tuning visualisation for {\mnUci}.}
%     \label{fig:approxtuner-ucihar-empirical}
% \end{figure}


% \begin{figure}[!htb]
%     \newcommand{\w}{\linewidth}
%     \newcommand{\sw}{0.45\linewidth}
%     \centering
%     \begin{subfigure}{0.42\w}
%         \centering
%         \includegraphics[width=\w]{figures/weights-mobilenet_cifar10-conv0.pdf}
%         \caption{\mnCifar}
%     \end{subfigure}
%     \hspace{1cm}
%     \begin{subfigure}{0.42\w}
%         \centering
%         \includegraphics[width=\w]{figures/weights-mobilenet_uci-har-conv0.pdf}
%         \caption{\mnUci}
%     \end{subfigure}
%     \caption{Visualisation of first 16 filters of the first convolutional layer in MobileNet. Filters were globally normalised to values on interval $[0, 1]$, for each network separately. }
%     \label{fig:filters}
% \end{figure}



% We believe that these are the properties that make approximations such as filter sampling unfeasible. The idea behind filter sampling is that neighbouring values in CNN filters and images are similar. This allows for accurate interpolation of missing values. However, neither of these assumptions hold for {\mnUci} (see section~\ref{sec:mnUci}).

