\section{Preliminaries}
\label{sec:preliminaries}

Mobiprox builds upon the existing work on heterogeneous and approximate computing compilers:

\textbf{HPVM} (\textit{Heterogeneous Parallel Virtual Machine})~\cite{kotsifakou2018hpvm} is a compiler infrastructure targeting heterogeneous hardware. It introduces HPVM-C, a programming language for defining \textit{data flow graphs} (DFGs), directed graphs in which nodes represent a computation task and edges represent inputs and outputs of a computation task. Computation workloads are defined using HPVM's intrinsic functions used to specify the {target device} the node will be executed on, node {inputs}, node {outputs}, and any compute {operations} (e.g. addition). HPVM compiler achieves parallel execution of produced binaries by identifying dependencies among the nodes in a DFG and generating compute code for specified target devices (CPU, GPU) for each node. %To orchestrate DFG execution, HPVM inserts appropriate calls to the {HPVM Runtime} library. %which is explained in section~\ref{sec:android-rt}.

\textbf{ApproxHPVM}~\cite{sharif2019approxhpvm} expands HPVM by introducing support for NN tensor operations: multiplication, convolution, addition, pooling, and activation functions.  Additionally, ApproxHPVM enables transforming high level descriptions of convolutional neural networks (in frameworks such as Keras, PyTorch) into DFGs in the form of generated HPVM-C source files. However, while HPVM generates code for computation nodes in a DFG, ApproxHPVM's tensor operations are mapped to functions defined in the \textbf{HPVM Tensor Runtime} library. In ApproxHPVM individual tensor operations can be marked with the maximum allowed level of approximation, and the compiler then ensures that these are mapped to the appropriate underlying approximate computing techniques (either software or hardware-based). Yet, ApproxHPVM's tensor operations are supported for Nvidia CUDA-enabled devices only. %and the PROMISE hardware simulator~\cite{srivastava2018promise}. 
In this paper we introduce a novel OpenCL implementation that enables approximate tensor operation execution on Android devices (Section~\ref{sec:android-rt}).

\textbf{ApproxTuner}~\cite{sharif2021approxtuner} delivers heuristic-based search of the space of possible approximations of each individual network layer, so that a comprehensive \textit{speedup-inference accuracy} trade-off curve is charted and the list of the most promising sets of approximations is identified. Yet, ApproxTuner does not take into account the peculiarities of the mobile platform and the predicted trade-off curves it draws do not reflect the actual performance observed on the mobiles. Consequently, in this paper we build a new cross-platform approximation profiler based on ApproxTuner (Section~\ref{sec:hpvm-profiler-android}).

\subsection{Approximation techniques}
\label{sec:preliminaries-acts}

\newcommand{\ttit}[1]{\textit{\texttt{#1}}}

%\sasa{This section should more clearly state the distinction between the previous work (ApproxTuner) and the new work in Mobiprox. I suggest we move the description of approximation techniques in the background (while also shortenting them if space becomes tight) and discuss new things here (e.g. half precision quantization)} 

%The last two decades witnessed a range of approximate computing techniques being developed -- from approximate adders and multipliers, to loop perforation and task skipping~\cite{Mittal2015}. However, due to their small form factor mobile devices seldom can host both approximate and accurate versions of hardware circuits. Software techniques, on the other hand, often require a strong involvement from the developer, for example, to label loops that are candidates for perforation. Therefore, in Mobiprox we focus on implementing basic software-based approximations (described below) at the lowest possible level -- the individual DL operation level -- and do not even require that the developer knows about these approximations. 
%\sasa{the key message from this paragraph should be moved to the intro. here is too buried. }

% presented in~\cite{sharif2021approxtuner}

We identified the following generally-applicable approximation techniques that can be employed at a level of a single NN operation and are supported by commodity mobile hardware, and we implemented them in Mobiprox:

%\hashim{"additional hardware" should be "specialized hardware"}
%\hashim{I think here you want to emphasize more strongly that you implemented these approximations and optimized them based on the ARM compute library. I know you spent significant effort doing this and should come through as such. For instance you can mention these routines were optimized with "vectorization", "tiling", etc etc.}

\textbf{Convolution perforation}~\cite{figurnov2016perforatedcnns} is an approximation that skips certain input matrix coordinates when calculating convolution, as shown in Figure~\ref{fig:convperf}. %\hashim{can make it more clear that this is "output" perforation - in that some output tensor elements are not computed and later interpolated with neighbour averaging.} 
Due to the nature of convolutions, this does not necessarily mean that the inputs at skipped coordinates are never used -- indeed, the inputs are used in neighboring convolutions. This, in turn, makes it feasible to interpolate convolution results at skipped coordinates by computing the average of computed neighboring cells. We support two types of convolution perforation -- \textbf{row perforation} and \textbf{column perforation}. The parameter \ttit{offset} defines the index of the first omitted row or column, while parameter \ttit{stride} defines the interval between the skipped rows/columns.  In Figure~\ref{fig:convperf}, parameters \ttit{stride}=2 and \ttit{offset}=1 were used.

%The difference between the two stems from the axis along which perforation takes place. Row perforation with \ttit{stride} $k$ skips calculation of convolutions in every $k$-th row of the input image, while column perforation with  \ttit{stride} $k$ skips convolutions at every $k$-th column of the input. Parameter \ttit{offset} defines the index of the first omitted row or column. In Figure~\ref{fig:convperf}, parameters \ttit{stride}=2 and \ttit{offset}=1 were used.


% \begin{figure}[h]
%     \centering
%     \subfigure[Row perforation]
%     {
%         \includegraphics[width=1.0in]{figures/convperf-row}
%         \label{fig:convperf-row}
%     }
%     \subfigure[Column perforation]
%     {
%         \includegraphics[width=1.0in]{figures/convperf-col}
%         \label{fig:convperf-col}
%     }
%     \caption{Perforated convolution. Coloured sections indicate convolution coordinates. Dashed squares indicate the area of the first and the final convolution.}
%     \label{fig:convperf}
% \end{figure}


\begin{figure} 
    \centering
  \subfloat[Row perforation\label{fig:convperf-row}]{%
       \includegraphics[width=0.45\linewidth]{figures/convperf-row.pdf}}
    \hfill
  \subfloat[Column perforation\label{fig:convperf-column}]{%
        \includegraphics[width=0.45\linewidth]{figures/convperf-col.pdf}}
    
  \caption{Perforated convolution. Coloured sections indicate convolution coordinates. Dashed squares indicate the area of the first and the final convolution.}
  \label{fig:convperf} 
\end{figure}

% \begin{figure}
%     \newcommand{\w}{\linewidth}
%     \newcommand{\sw}{0.3\linewidth}
%     \centering

%     \subfigure[]{\includegraphics[width=0.45\linewidth]{figures/convperf-row.pdf}}
%     %\begin{subfigure}{\sw}
%     %    \centering
%     %    \includegraphics[width=\w]{figures/convperf-row.pdf}
%         %\caption{Row perforation}
%         %\label{fig:convperf-row}
        
% %    \hspace{1cm}
%     \subfigure[]{\includegraphics[width=0.45\linewidth]{figures/convperf-col.pdf}}

%     %\begin{subfigure}{\sw}
%     %    \centering
%     %    \includegraphics[width=\w]{figures/convperf-col.pdf}
%         %\caption{Column perforation}
%         %\label{fig:convperf-col}
 
%     \caption{Perforated convolution. Coloured sections indicate convolution coordinates. Dashed squares indicate the area of the first and the final convolution.}
%     \label{fig:convperf}
% \end{figure}




\newcommand{\nelm}{n_\mathrm{elm}}
\newcommand{\nelmPerf}{n_\mathrm{elm-samp}}

\textbf{Filter sampling} approximates the filters that the convolutions are performed with.
%\hashim{Perforation is actually perforating the "output". Filter sampling is perforating the "filter". In both cases, a subset of inputs are used, so both do input sampling in some sense.}
In CNNs filters are 4-dimensional tensors with dimensions $[N, C, H, W]$. $N$ represents the number of filters in the convolution, $C$ is the number of feature channels in the input and the filter, and $H$ and $W$ represent the height and width of the filter, respectively. Each filter is therefore composed of $\nelm{} = C \cdot H \cdot W$ components. Filter sampling with \ttit{stride} $k$ removes every $k$-th component of the filter's $\nelm{}$ components, starting at element specified by \ttit{offset}. The technique, thus, reduces the amount of computation by keeping only $\nelmPerf{} = \nelm{} - \frac{\nelm{} - \ttit{offset}}{\ttit{stride}}$ filter components at the cost of the overall convolution accuracy. To interpolate missing values, each retained filter component is multiplied by a factor of $\nelm{}/\nelmPerf{}$.


%\begin{equation}\label{eq:n_elm}
%    \nelm{} = C \cdot H \cdot W
%\end{equation}

%\begin{equation}\label{eq:n_elm_perf}
%    \nelmPerf{} = \nelm{} - \frac{\nelm{} - %\ttit{offset}}{\ttit{stride}}
%\end{equation}

%\blue{About 80\%-90\% of inference time within modern DL architectures is spent on convolutional layers calculation~\cite{li2023architecture}. This is why, through the above approximation techniques, we focus on making convolution dynamically approximable. Nevertheless, Mobiprox also provides an optional \textbf{half-precision quantization} }

Finally, Mobiprox also provides an optional \textbf{half-precision quantization} that can be used to approximate any floating point tensor operation. While such quantization is meaningful only if the underlying hardware supports it, we opted for enabling it as modern mobile GPUs, such as those of Arm Mali series, natively support the IEEE FP16 16-bit format.

%Additionally, each operation can be run at reduced precision with 16-bit floating point data (indicated by \texttt{\_fp16} extension).


% \begin{figure}[h]
%     \newcommand{\w}{\linewidth}
%     \newcommand{\sw}{0.3\linewidth}
%     \centering
    
%     \begin{subfigure}{\sw}
%         \centering
%         \includegraphics[width=\w]{figures/convperf-filter.pdf}
%         \caption{First step}
%         \label{fig:convperf-filter-0}
%     \end{subfigure}
%     % \begin{subfigure}{\sw}
%     %     \centering
%     %     \includegraphics[width=\w]{figures/convperf-filter-1.pdf}
%     %     \caption{}
%     %     \label{fig:convperf-filter-1}
%     % \end{subfigure}
%     \begin{subfigure}{\sw}
%         \centering
%         \includegraphics[width=\w]{figures/convperf-filter-2.pdf}
%         \caption{Intermediate step}
%         \label{fig:convperf-filter-2}
%     \end{subfigure}
%     \begin{subfigure}{\sw}
%         \centering
%         \includegraphics[width=\w]{figures/convperf-filter-3.pdf}
%         \caption{Final step}
%         \label{fig:convperf-filter-3}
%     \end{subfigure}

%     \caption{First, intermediate and final steps of convolution with filter sampling. The filter traverses all input coordinates. Coloured filter components were retained in filter sampling. A non-perforated filter would have all nine squares shaded. All figures depict convolution of a single channel $6\times6$ image and a $3\times3$ filter. Filter sampling was done with \ttit{stride} 2 and \ttit{offset} 1.}
%     \label{fig:convperf-filter}
% \end{figure}
