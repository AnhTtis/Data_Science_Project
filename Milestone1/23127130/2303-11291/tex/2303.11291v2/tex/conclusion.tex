\section{Conclusion}
\label{sec:conclusion}

In this paper we introduced Mobiprox, to the best of our knowledge, the first end-to-end framework that enables dynamically adaptable, rather than static, approximation of mobile DL. Furthermore, Mobiprox works with arbitrary architectures, even with networks not initially designed with approximation in mind. To accomplishing this, we first implemented low-level support for approximate computing on mobile CPU and GPUs through compiler-level primitives. We then integrated the heterogeneous compilation infrastructure, the approximate configuration search framework, and our novel profiler into an Android-ready end-to-end approximate configuration search, selection, and compilation pipeline. We ran different deep learning architectures %, such as MobileNet, AlexNet, VGG, and ResNet 
through the pipeline and %in two different domains -- human activity recognition and computer vision -- 
demonstrated that Mobiprox identifies approximation configurations that enable a trade-off between inference accuracy and energy consumption. Finally, we implemented approximation adaptation strategies for dynamic selection of energy-preserving DL configurations while ensuring that the quality of the resulting classification is not hurt. Experiments in human activity and spoken keyword recognition domains demonstrate that the adaptation strategies successfully accommodate varying context, reducing the system-wide energy usage by 15\% in both domains, while sacrificing only 2\% of the accuracy in the HAR domain, and leading to no loss of accuracy in the spoken keyword recognition domain. %With its general applicability to different NN architectures, demonstrated energy savings, and the decoupling of the model approximation and the approximation adaptation strategies, Mobiprox represents a versatile tool for bringing efficient deep learning on mobile devices. 

%Experiments with real-world hardware and on-device profiling demonstrate that Mobiprox can generate a Pareto front of approximation configurations on the accuracy vs. speedup curve. While the maximum speedup remains relatively modest, measurements show that the system-level energy consumption is reduced by a larger factor compared to the reduction in computation time. %We believe that further optimisations on the mobile compiler front can enable even higher energy savings and speedup. 
