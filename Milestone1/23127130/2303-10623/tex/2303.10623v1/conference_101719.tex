\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage{subcaption}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Active hypothesis testing in unknown environments using recurrent neural networks and model free reinforcement learning
}

\author{\IEEEauthorblockN{1\textsuperscript{st} George Stamatelis}
\IEEEauthorblockA{\textit{dept. of Informatics and Telecommunications} \\
\textit{National and Kapodistrian University of Athens}\\
Athens, Greece \\
sdi1800185 at di.uoa.gr}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Nicholas Kalouptsidis}
\IEEEauthorblockA{\textit{dept. of Informatics and Telecommunications} \\
\textit{National and Kapodistrian University of Athens}\\
Athens, Greece \\
kalou at di.uoa.gr}

}

\maketitle

\begin{abstract}
A combination of deep reinforcement learning and supervised learning is proposed for the problem of active sequential hypothesis testing in completely unknown environments. We make no assumptions about the prior probability, the action and observation sets, and the observation generating process. Our method can be used in any environment even if it has continuous observations or actions, and performs competitively and sometimes better than the Chernoff test, in both finite and infinite horizon problems, despite not having access to the environment dynamics.
\end{abstract}

\begin{IEEEkeywords}
Active hypothesis testing, POMDPs, Reinforcement learning, Neural networks, Controlled sensing, Supervised learning, Sensor networks, Anomaly detection
\end{IEEEkeywords}

\section{Introduction}
In active sequential hypothesis testing (ASHT) problems,  a decision maker actively conducts a sequence of experiments in order to infer the underlying hypothesis from a finite set of hypotheses. The problem has been extensively studied. It finds numerous applications including anomaly detection \cite{activeHTforAD}, medical diagnosis \cite{Berry2010BayesianAM}, radar assisted target search
\cite{chernoff-radar}, feedback channel coding \cite{polyanski}, and content search applications \cite{fabioAdaptiveAHT}. It has been  extended to scenarios against passive eavesdroppers \cite{evasive-active} and active adversaries \cite{aaht}. 
Recently, deep reinforcement learning (DRL) was successfully applied to ASHT in \cite{KatrikEtAll} and \cite{ActorCriticHT}, \cite{aaht}. The DRL agent selects  actions based on their beliefs over all possible hypotheses. The DQN  \cite{mnih2015humanlevel} and deep actor critic algorithms were employed.
Multiagent DRL methods for the anomaly detection problem were proposed in \cite{decHTKohen} and \cite{decADRL}.

In this work we consider belief free methods. The states of Deep recurrent neural networks (RNN)s are used as proxies for beliefs. We combine  RNNs with reinforcement learning (RL) methods  to address ASHT in environments with unknown observation probabilities and possible infinite or continuous observations. At each time instance, based on the history of past observations and actions, the decision maker selects which experiment to conduct. This is performed by a RNN network called RNNpolicy.  Once the episode has terminated, the inference network, RNNinference  is used to infer the hypothesis  from the actions and observation sequence. In the infinite horizon setting, an additional monitoring RNN network, RNNmonitor monitors the decision maker's interaction with the environment and determines whether the experiments should terminate or not. We assume that a training environment or a large dataset is available to train the networks. The actual environment in which the networks will be deployed can be slightly different, and the observation probabilities are unknown. Therefore, the agent can not take advantage of the belief vectors. In the anomaly detection over sensor networks problem \cite{activeHTforAD}, the size of the belief vector increases exponentially with the number of sensors. Consequently, if we target at quick decision making on large sensor networks, recursively updating the belief at each time period  and then solving a mathematical optimization problem or passing it through a large neural network to select the next query may prove infeasible.

The policy RNN is trained on an artificial simulation environment using a popular DRL algorithm. Then a large dataset of actions and observations is created using the above RNNpolicy network. The monitoring RNN is trained on that dataset using backpropagation \cite{backpropagation}. Then the pair RNNpolicy-RNNmonitor is used to create a second large dataset. The inference RNN  is trained on the new dataset using backpropagation. 

\subsection{Related work}
The first attempt to solve the binary passive sequential hypothesis testing goes back to Wald \cite{waldSeq}. It was later extended to the active setting by Chernoff in \cite{ChernoffHT}. Chernoff proposed a heuristic called the Chernoff test that is asymptotically optimal under certain assumptions. In \cite{controlledSensForMH} the problem of multi-hypothesis testing is studied and a new variant of the Chernoff test that relaxes the previous assumptions is proposed. In \cite{Naghshvar_2013} ASHT is modeled as a cost minimisation  POMDP. The cost depends on the stopping time and the error probability. In \cite{KatrikEtAll} ASHT is modeled as a confidence maximisation belief MDP and a recurrent DQN  \cite{mnih2015humanlevel} procedure is proposed. It is shown to perform better than other heuristics in numerical experiments. Similarly, in \cite{ActorCriticHT} a deep actor critic procedure is shown to perform better than the Chernoff test in environments that require high confidence levels. Again the problem is modeled as a confidence maximization belief MDP.
\subsection{Our contribution}
We propose a combination of DRL algorithms and RNNs to  solve ASHT problems in completely unknown environments. We make no assumptions about the prior probability of the hypotheses, the  observation generating process or the type of observations. In fact our approach can also work in continuous action spaces. The only requirement is access to a training environment, or a large training dataset.    We compare our approach to the modified Chernoff test of \cite{controlledSensForMH} in environments with discrete, finite actions and observations, and we show that the proposed method achieves competitive results and sometimes performs slightly better.

The rest of this paper is organised as follows. In section II, ASHT   is defined. In section III, we discuss the proposed method, both for the finite and infinite horizon setting. Comparisons with the Chernoff test are given in section IV using an example of anomaly detection. In section V, we examine a larger problem and we conclude in section VI.

\section{Problem statement}
Let $\mathcal{X}=\{0,1,...,N\}$ be a finite set of hypotheses. The random variable $X$ defines the true hypothesis. At each time step $t>0$ a decision maker chooses an action $a_t \in \mathcal{A}$. Based on $a_t$ and $X$ an observation $y_t \in \mathcal{Y}$ is generated. It is sampled from 
\begin{equation}
    \label{obsProb}
   p_X^{a_t} \triangleq P[Y|X,a_t].
\end{equation}
Usually, the observations are assumed to be  i.i.d., given the state and action. Also, the set $\mathcal{Y}$ is taken to be finite. Our method does not require these assumptions. The choice of action $a_t$ relies on 
the information available at time $t$
\begin{equation}
    \label{infoSet}
    I_t=\{a_{1:t-1},y_{1:t}\}.
\end{equation}
The purpose of these actions is to help inference of  the underlying hypothesis. Actions are sampled from a policy which in turn is based on beliefs.
For a given action $a_t$ and observation $y_t$ the belief on each hypothesis $i \in \mathcal{X}$ is updated recursively as \begin{equation}
    \label{belup}
    \rho_{t+1}(i)= \rho_t(i) \frac{p_i^{a_t}(y_t)}{\sum_j \rho_t(j) p_j^{a_t}(y_t)}.
\end{equation}
The prior probability is $\rho_0$.

The hypothesis inferred at time $t$ is denoted by $\hat {X}_t$ and is the result of the inference policy acting on the information set. 

The quality of inference is measured by several indicators. The error probability 
\begin{equation}
 \gamma_t=P[\hat{X}_t\neq X] ,  
\end{equation} is an important example, which is expressed in terms of beliefs as 
\begin{equation}
\label{errorProb}
    \gamma_t=1-\max_{i}\rho_t(i).
\end{equation}
Other examples include the average confidence level 
\begin{equation}
    \label{confDef}
    C(\rho)=\sum_{i \in \mathcal{X}} \rho(i) \log \bigg(\frac{\rho(i)}{1-\rho(i)}\bigg).
\end{equation}
In the infinite horizon setting the following index 
\begin{equation}
    \label{LLRatio}
    LL_t=log \frac{p_{\hat{i}_t}(y_{1:t},a_{1:t})}{\max_{j \neq i} p_j(y_{1:t},a_{1:t})},
\end{equation}
monitors performance. It provides an asymptotically optimal stopping rule: the agent terminates the experiment the first time t for which
\begin{equation}
    \label{chernoffStop}
    LL_t > -\log c,
\end{equation}
where c is a positive real valued parameter.
Upon termination, the inference strategy selects the hypothesis $\hat{i}_t$ that maximizes the aposteriori probability (MAP decoding) or the likelihood (ML decoding). Often a uniform prior is employed, in which case MAP and ML decoding coincide. For further details please see \cite{KatrikEtAll} and \cite{aaht}.

An asympotically optimal policy is given by the Chernoff test. At each time instance $t$,  the policy samples actions from the distribution 
\begin{equation}
    \label{chernoffTest}
    g_t = \max_{g} \min_{j \neq \hat{i}_t} \sum_a g(a) D(p_{\hat{i}}^a||p_j^a),
\end{equation} 
 where $\hat{i}_t$ is the most likely hypothesis at time $t$.
\section{The proposed method}
In this section, we combine DRL and supervised RNNs to tackle the ASHT problem without the use of belief vectors. We assume that we have access to a training environment or alternatively, to a large dataset, based on which we can build a simulator. 

\subsection{Decoder structure}
Under the infinite horizon setting we employ two recurrent neural networks for decoding, RNNmonitor and RNNinference. The RNNmonitor monitors the environment at each time instance. The value determined by its  output is used to decide whether the process is continued or terminated on the basis of a threshold rule such as (\ref{chernoffStop}). The RNNinference network is activated when the RNNmonitor decides termination. It then produces an estimate of the underlying true hypothesis. 
Under the fixed finite horizon case, the RNNmonitor becomes redundant. The structure of the two recurrent networks is identical except from the final layer which performs the regression and classificatin tasks respectively. 

We have chosen to employ recurrent neural networks rather than multilayer neural networks to take advantage of their capacity to model dynamic input output systems. Simple RNNs are prone to numerical instability problems, most notably vanishing and exploding gradients. For that reason, many variations have been proposed throughout the years, most notably, long short term memory networks (LSTM)s \cite{lstm} and Gated recurrent units (GRU)s \cite{gru}. Recently recurrent  variants of popular model free DRL algorithms have been successfully applied to POMDP problems such as \cite{recurrentDQNStone} and \cite{recurrentModelFreeCan}. In this work we consider both LSTM and GRU networks. Each network is fed by the information sequences of action and observation pairs. At each time period t, a tuple $(a_t,y_t)$ passes through the RNN cell and an output $o_t$ is generated. The mean of all outputs goes through a feed forward layer that outputs the prediction. In case of classification, the feed forward layer is followed by a softmax activation function. All decoder networks in this paper are developed using the pytorch framework \cite{pytorchCite} and trained with the adam optimizer \cite{adam}.
\subsection{Training procedure}
Training of  the  neural networks takes place in  three phases. First, we train the RNNpolicy network using reinforcement learning on an artificial simulation environment. Then the RNNmonitor and subsequently the RNNinference are trained using the adam optimizer \cite{adam}. In the fixed horizon setting, training of the RNNmonitor is deactivated. 

The RNNpolicy is initialised with random parameters. The artificial simulation environment is created as follows. At each training episode the underlying hypothesis $i$ is sampled from $\mathcal{X}$ according to $\rho_0$.
At each time step $t$ the agent chooses an action $a_t$ based on the hidden state of the recurrent neural network and its most recent observation $y_{t-1}$ (action $a_1$ is chosen randomly). An observation $y_t$ is sampled from $\mathcal{Y}$ according to $P[Y|X=i,a=a_t]$. The training environment internally updates the belief vector $\rho_{t}$.
The reward provided to the agent is the error improvement  $\gamma_{t-1}-\gamma_{t}$. After $T$ time steps the training episode terminates. In the finite horizon setting, $T$ is the value of the horizon. In the infinite horizon setting, we set $T$ to a large value to ensure sufficient exploration.   We use a reliable open source implementation of recurrent PPO \cite{stable-baselines3} (contributed version).

Upon completion of phase 1 and training of the  RNNpolicy network, a new dataset is generated by randomly selecting the horizon of each episode. Each data point consists of a sequence of actions and observations labeled by the error probability at the final time step. The resulting dataset is used to train RNNmonitor in stage 2. 

Finally training of RNNinference is realized via a new dataset generated as follows. At each time instance $t$ of each training episode, RNNmonitor outputs an approximation of the error probability $\bar{\gamma}_t$. If $\bar{\gamma}_t<c$, where the hyperparameter $c$ is a user defined threshold, the episode terminates, else RNNpolicy continues with the experiment selection. The label of each data point is the underlying true hypothesis. The desired dataset is obtained by repeating this procedure for multiple episodes. 

In the next sections, the three networks are deployed and evaluated on a testing environment which is slightly different from the one they were trained on.

\section{Case study: Anomaly detection}
We consider an anomaly detection example. There are two sensors $\mathcal{A}$ and $\mathcal{B}$ that detect anomalies in their proximity. There are 4 possible hypotheses. $X=0$ means there is no anomaly near any sensor (the system is in a safe state). $X=1$ and $X=2$ means there is an anomaly near sensors $\mathcal{A}$ and $\mathcal{B}$ respectively. Finally, $X=3$ means there are anomalies near both sensors. We assume a uniform prior. We create two slightly different environments, one for training the DRL agent and the RNN decoders and one for testing their performance.  The observation models for both environments are summarized in tables  \ref{tab:obs1} and \ref{tab:obs2}. 
\begin{table}
    \centering
    \caption{$P[Y=1|a_t,X]$ for the training environment}
    \label{tab:obs1}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
         &$X=0$ & $X=1$ & $X=2$& $X=3$  \\
    \hline
         $\mathcal{A}$&0.2&0.8&0.2&0.8\\
         \hline
         $\mathcal{B}$&0.2&0.2&0.8&0.8\\
         \hline
    \end{tabular}
    
\end{table}
\begin{table}
    \centering
    \caption{$P[Y=1|a_t,X]$ for the testing environment}
    \label{tab:obs2}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
         &$X=0$ & $X=1$ & $X=2$& $X=3$  \\
    \hline
         $\mathcal{A}$&0.25&0.75&0.25&0.75\\
         \hline
         $\mathcal{B}$&0.15&0.15&0.85&0.85\\
         \hline
    \end{tabular}
    
\end{table}

\subsection{Comparing decoding measures}
Before actually evaluating our method, we check how efficiently LSTMs and GRUs learn to map action and observation sequences to $\gamma_t$, $LL_t$, $C_t$ and $\hat{i}_t$, where $\hat{i}_t$ is the most likely hypothesis. 

We follow the modified Chernoff strategy as described in \cite{controlledSensForMH} in the training environment, and collect a dataset of 60000 sequences. The first 50000 are used for training and the rest for validation. We also build a test set of 10000 sequences using the testing environments.  We consider a fixed horizon in the beginning of each episode, randomly in the range 5-50. 

We consider unidirectional and  bidirectional  RNNs with two hidden units. We examine hidden sizes from 50-400. The one with the best score on the validation set is used in the test set. In case of ties, the simpler architecture is preferred. The results for all 5 metrics are shown in table 
\ref{tab:supervRes}.  

We note that as the error probability becomes small, both $LL_t$ and $C_t$ can take very large values. Moreover, they might take small and sometimes negative values for small horizons. Therefore training a neural network to learn these indicators can be problematic. In fact, when learning $LL_t$, we discarded all the samples where $LL_t>100$, because the maximum absolute error (MAE) was very large. On the other hand $\gamma_t$ is always in the range $[0,1]$. It can be seen from table \ref{tab:supervRes} that both LSTMs and  GRUs learn $\hat{j}_t$ and $\gamma_t$ very accurately. For that reason, the stopping rule in the infinite horizon setting employs the error probability.
\begin{table}
\caption{Test scores and structures of LSTM and GRU based decoders for $LL_t,C_t,\gamma_t$ and $\hat{i}_t$.}
    \label{tab:supervRes}
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
         & LSTM &GRU  \\
    \hline
         best hidden size for $LL_t$& 400(BI)&300 (BI) \\
         \hline
        best hidden size for $\hat{j}_t$&200 &200 \\
\hline
best hidden size for $C(\rho_t)$ &250(BI) &250(BI) \\
\hline
best hidden size for $\gamma_t$& 200&250 \\
\hline
    precision on $\hat{j}_t$&0.9996 & 0.9967\\
    \hline
    recall on $\hat{j}_t$&0.9996 & 0.996\\
    \hline
    f1 score on $\hat{j}_t$&0.9996 &0.9996 \\
    \hline
     MAE for $LL_t$& 13.25& 14.397\\
     \hline
     MAE for $C(\rho_t)$& 0.952& 0.945\\
     \hline
     MAE for $\gamma_t$&0.00085 &0.0019 \\
     \hline
         
    \end{tabular}
    
\end{table}
\subsection{Fixed horizon } 
\label{sec:fixedH}
We considered four different horizons 10, 25, 50, and 100. For each horizon, we trained the PPO agent and then collected a dataset that has 60000 different sequences of actions and observations.
Both decoders are unidirectional RNNs with 2 hidden layers of the same size. The GRU decoder has 250 hidden units and the LSTM decoder 200 units.
When paired with a GRU or an LSTM decoder, for horizons 10-50, PPO's performance is comparable to that of the Chernoff test. In fact, for  horizon $T=10$ it achieves a smaller error probability. For  horizon $T=100$ the Chernoff test achieves a significantly smaller error probability. The GRU decoder  performs slightly better than the LSTM decoder.
\begin{table}
\caption{Average error probability out of 10000 episodes for different horizons}
    \label{tab:resFixedH}
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         T&PPO-LSTM&PPO-GRU&Chernoff  \\
    \hline
        10 &0.152 & 0.151&0.162 \\
        \hline
        25 &0.0396 &0.0341
 & 0.0312\\
        \hline
        50&0.0078&0.008& 0.0017\\
        \hline 
        100&0.0008&0.0007&0.00005 \\
        \hline
    \end{tabular}
    
\end{table}
\subsection{Infinite horizon}

We used the PPO agent  previously trained for $T=50$. We collected a dataset of 60000 training examples. The horizon of each episode is chosen uniformly in the range [1,50]. Then we trained an LSTM network that approximates the error probability. Once the new network detects that the error probability is below a user defined threshold $c$ the episode terminates. Using the agent and the network we collected a new dataset of 60000 examples and trained another LSTM that infers the underlying hypothesis. We tested the PPO agent combined with the LSTM networks in the test environments for 10000 episodes. We repeated the procedure for GRU networks.
The test results for different tolerance levels are demonstrated in  table \ref{tab:resInfH}.

The LSTM networks used in this section are bidirectional LSTMs with 2 hidden units of 200 neurons. They also employ  dropout with probability $p=0.2$. The GRUs are bidirectional with 4 hidden units of 250 neurons. Again, dropout is used with $p=0.2$. 

Contrary to the previous section, LSTM decoders perform  better than GRU decoders. When the PPO agent is combined with LSTM decoders it terminates with the desired error probability level slightly faster than the Chernoff test in 2/4 experiments.

\begin{table}
\centering
\caption{Average stopping time and error probability out of 10000 episodes} \label{tab:resInfH}
\begin{subtable}[b]{0.3\textwidth}
\caption{stopping time}
\begin{tabular}{|c|c|c|c|}
\hline
   c&PPO-LSTM & PPO-GRU&Chernoff    \\
   \hline
   0.3 & 5.85  & 6 & 4.58\\
   \hline
   0.2 &7.41 &8 &7.73 \\
   \hline
   0.1 &11.58 & 13&12.61 \\
   \hline
   0.05 &12.66& 16.14 &12.164\\
   \hline
   \end{tabular}
\end{subtable}\\

\begin{subtable}[b]{0.3\textwidth}
\caption{error probability}
\begin{tabular}{|c|c|c|c|}
\hline
c&PPO-LSTM & PPO-GRU&Chernoff    \\
   \hline
   0.3 & 0.198  & 0.205 & 0.248\\
   \hline
   0.2 & 0.134& 0.164 & 0.131\\
   \hline
   0.1 &0.075 &0.092 & 0.046\\
   \hline
   0.05 & 0.043 &0.051 & 0.038\\
   \hline
   \end{tabular}
\end{subtable}
\end{table}
\section{A larger example}
We now  consider a larger anomaly detection problem. There are 4 sensors and each of them monitors an independent random process. Any number of processes can be abnormal, therefore there are 16 different hypotheses. Proceeding as in the previous chapter, we  build two slightly different environments, one for training the models, and one for testing. 

When a process is abnormal In the training environment,the sensor outputs 1 with probability 0.8 and 0 with probability 0.2. When it is normal the numbers are reversed. In the testing environment, the first two sensors output 1 with probability 0.85 when the process is  abnormal, and the last two output 1 with probability 0.75. 

We repeat the experiments of \ref{sec:fixedH}, only this time, the training dataset consists of 150000 training examples. The network pairs are tested for 10000 episodes. The results  for different horizons can be seen in table  \ref{tab:resLarge}.
\begin{table}[]
\caption{Average error probability out of 10000 episodes for the larger environment}
    \label{tab:resLarge}
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         horizons& PPO+LSTM & PPO+GRU & Chernoff \\
        \hline
        10 & 0.48 &0.49 &0.53\\
        \hline
        25&0.247 & 0.245 & 0.234\\
        \hline
        50&0.0698&0.0699&0.071\\
        \hline
        100&0.0238 &0.0157&0.008\\
        \hline
    \end{tabular}
    
\end{table}
Both network pairs perform better than the Chernoff test for horizons 10 and 50. For horizons 25 and 100, the Chernoff test performs slightly better. 

\section{Conclusion} 
We have established a proof of concept. Model free recurrent DRL combined with RNN decoders can successfully be applied to ASHT problems in environments with unknown models. We have made no assumptions about the observation and action spaces, or the observation generating processes. Our approach does not assume any prior knowledge about the environment.

The proposed method  performs competitively with the asymptotically optimal  Chernoff test, both in finite and infinite horizon problems. For smaller horizons it actually performs better. In future work we will extend the experiments to different applications and larger examples. We will also consider replacing LSTMs and GRUs with transformers \cite{attentionAllYouNeed}.
\bibliographystyle{ieeetr} % We choose the "plain" reference style
\bibliography{references}

\textbf{Nicholas Kalouptsidis} received the BS degree in mathematics from the
University of Athens, in 1973 and the M.S and PhD degrees in systems
science and mathematics from Washington University at St. Louis, MO,
in 1975 and 1976, respectively. From 1989 until today he is professor
of Communications and Signal Processing in the Department of Computer
Science and Communications, at the National and Kapodistrian University of
Athens. He has held visiting positions at Washington University at St. Louis, Politecnico Di Torino,Northeastern University and University of Utah at Salt Lake.  He was a visiting scholar at Harvard University in 2008 and a visiting
professor at Stanford University in 2015. He has more than 200 publications
and 3 books.

\vspace{2cm}


\textbf{Georgios Stamatelis} received the BS degree in Computer Science and Communications with honours  from the University of Athens in 2022. He plans to pursue doctoral studies. His research interests  include detection, control and  reinforcement learning.\\
\end{document}
