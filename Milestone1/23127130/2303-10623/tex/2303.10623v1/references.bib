@ARTICLE{controlledSensForMH,  author={Nitinawarat, Sirin and Atia, George K. and Veeravalli, Venugopal V.},  journal={IEEE Transactions on Automatic Control},   title={Controlled Sensing for Multihypothesis Testing},   year={2013},  volume={58},  number={10},  pages={2451-2464},  doi={10.1109/TAC.2013.2261188}}

@misc{stable-baselines3,
  author = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah},
  title = {Stable Baselines3},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/stable-baselines3}},
}

@inproceedings{pytorchCite,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}
@article{adam,
author = {Kingma, Diederik and Ba, Jimmy},
year = {2014},
month = {12},
pages = {},
title = {Adam: A Method for Stochastic Optimization},
journal = {International Conference on Learning Representations}
}

@ARTICLE{chernoff-radar,  author={Franceschetti, Massimo and Marano, Stefano and Matta, Vincenzo},  journal={IEEE Transactions on Signal Processing},   title={Chernoff Test for Strong-or-Weak Radar Models},   year={2017},  volume={65},  number={2},  pages={289-302},  doi={10.1109/TSP.2016.2616323}}

@ARTICLE{evasive-active,  author={Chang, Meng-Che and Bloch, Matthieu R.},  journal={IEEE Journal on Selected Areas in Information Theory},   title={Evasive Active Hypothesis Testing},   year={2021},  volume={2},  number={2},  pages={735-746},  doi={10.1109/JSAIT.2021.3074156}}

@ARTICLE{activeHTforAD,  author={Cohen, Kobi and Zhao, Qing},  journal={IEEE Transactions on Information Theory},   title={Active Hypothesis Testing for Anomaly Detection},   year={2015},  volume={61},  number={3},  pages={1432-1450},  doi={10.1109/TIT.2014.2387857}}

@INPROCEEDINGS{vaid,  author={Vaidhiyan, Nidhin Koshy and Arun, S. P. and Sundaresan, Rajesh},  booktitle={2012 IEEE International Symposium on Information Theory Proceedings},   title={Active sequential hypothesis testing with application to a visual search problem},   year={2012},  volume={},  number={},  pages={2201-2205},  doi={10.1109/ISIT.2012.6283844}}

@inproceedings{fabioAdaptiveAHT,
 author = {Cecchi, Fabio and Hegde, Nidhi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Active Hypothesis Testing under Limited Information},
 url = {https://proceedings.neurips.cc/paper/2017/file/9f44e956e3a2b7b5598c625fcc802c36-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{Berry2010BayesianAM,
  title={Bayesian Adaptive Methods for Clinical Trials},
  author={Scott M. Berry and Bradley P. Carlin and Jiun-Kae Jack Lee and Peter M{\"u}ller},
  year={2010}
}

@article{customerService,
author = {Hui, S. C. and Jha, G.},
title = {Data Mining for Customer Service Support},
year = {2000},
issue_date = {Oct. 2000},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {38},
number = {1},
issn = {0378-7206},
url = {https://doi.org/10.1016/S0378-7206(00)00051-3},
doi = {10.1016/S0378-7206(00)00051-3},
journal = {Inf. Manage.},
month = {oct},
pages = {1–13},
numpages = {13},
keywords = {data mining, knowledge discovery in databases, customer service support, decision support, machine fault diagnosis}
}

@techreport{polyanski,
  added-at = {2020-05-23T12:35:08.000+0200},
  author = {Polyanskiy, Yury and Wu, Yihong},
  biburl = {https://www.bibsonomy.org/bibtex/252de367a219ee9ce6dec7ee671d80845/shayari321},
  interhash = {0580768c21fb5daa566185e990985b08},
  intrahash = {52de367a219ee9ce6dec7ee671d80845},
  keywords = {},
  timestamp = {2020-05-23T12:35:08.000+0200},
  title = {LECTURE NOTES ON INFORMATION THEORY},
  url = {http://people.lids.mit.edu/yp/homepage/data/itlectures_v5.pdf},
  year = 2019
}
@INPROCEEDINGS{KatrikEtAll,  author={Kartik, Dhruva and Sabir, Ekraam and Mitra, Urbashi and Natarajan, Prem},  booktitle={2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},   title={Policy Design for Active Sequential Hypothesis Testing using Deep Learning},   year={2018},  volume={},  number={},  pages={741-748},  doi={10.1109/ALLERTON.2018.8636086}}

@INPROCEEDINGS{ActorCriticHT,  author={Zhong, Chen and Gursoy, M. Cenk and Velipasalar, Senem},  booktitle={2019 IEEE Global Communications Conference (GLOBECOM)},   title={Deep Actor-Critic Reinforcement Learning for Anomaly Detection},   year={2019},  volume={},  number={},  pages={1-6},  doi={10.1109/GLOBECOM38437.2019.9013223}}

@inproceedings{DRL-change-point-det,
author = {Puzanov, Anton and Cohen, Kobi},
title = {Deep Reinforcement One-Shot Learning for Change Point Detection},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ALLERTON.2018.8635928},
doi = {10.1109/ALLERTON.2018.8635928},

booktitle = {2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
pages = {1047–1051},
numpages = {5},
location = {Monticello, IL, USA}
}

@article{ChernoffHT,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2237415},
 author = {Herman Chernoff},
 journal = {The Annals of Mathematical Statistics},
 number = {3},
 pages = {755--770},
 publisher = {Institute of Mathematical Statistics},
 title = {Sequential Design of Experiments},
 urldate = {2022-06-25},
 volume = {30},
 year = {1959}
}
@book{waldSeq,
  added-at = {2017-06-29T07:13:07.000+0200},
  address = {New York},
  author = {Wald, Abraham},
  biburl = {https://www.bibsonomy.org/bibtex/257097c3007511d528fc4a2fd3969c680/gdmcbain},
  citeulike-article-id = {2462406},
  comment = {(private-note)Cited by:
== Miller \& Freund (1965, pp. 97, 385)
== Wikipedia article of same name: http://en.wikipedia.org/wiki/Sequential\_analysis
---=note-separator=---
(private-note)Holdings: pers.; ordered 2008-03-04T1005 from betterworld.com, arr. 2008-04-02.},
  interhash = {3f1f3e6f93af5ab51edd8ed1a39278b7},
  intrahash = {57097c3007511d528fc4a2fd3969c680},
  keywords = {62l05-sequential-design, 62l10-sequential-analysis, 62l12-sequential-estimation},
  posted-at = {2008-03-03 23:00:14},
  priority = {2},
  publisher = {John Wiley \& Sons},
  timestamp = {2017-06-29T07:13:07.000+0200},
  title = {{Sequential Analysis}},
  year = 1947
}
@article{Naghshvar_2013,
	doi = {10.1214/13-aos1144},
  
	url = {https://doi.org/10.1214%2F13-aos1144},
  
	year = 2013,
	month = {dec},
  
	publisher = {Institute of Mathematical Statistics},
  
	volume = {41},
  
	number = {6},
  
	author = {Mohammad Naghshvar and Tara Javidi},
  
	title = {Active sequential hypothesis testing},
  
	journal = {The Annals of Statistics}
}
@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@ARTICLE{lstm,  author={Hochreiter, Sepp and Schmidhuber, Jürgen},  journal={Neural Computation},   title={Long Short-Term Memory},   year={1997},  volume={9},  number={8},  pages={1735-1780},  doi={10.1162/neco.1997.9.8.1735}}

@ARTICLE{aaht,  author={Kalouptsidis, Nicholas and Stamatelis, George},  journal={under review},   title={Deep reinforcement learning and adaptive strategies for adversarial active hypothesis testing},   year={2022}, }

@InProceedings{gru,
  title = 	 {Gated Feedback Recurrent Neural Networks},
  author = 	 {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2067--2075},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/chung15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/chung15.html},
  abstract = 	 {In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.}
}
@misc{recurrentModelFreeCan,
  doi = {10.48550/ARXIV.2110.05038},
  
  url = {https://arxiv.org/abs/2110.05038},
  
  author = {Ni, Tianwei and Eysenbach, Benjamin and Salakhutdinov, Ruslan},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs },
  
  publisher = {arXiv},
  
  year = {2021, Available at:  Arxiv:2110.05038},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@InProceedings{recurrentDQNStone,
  author = {Matthew Hausknecht and Peter Stone},
  title = {Deep Recurrent Q-Learning for Partially Observable MDPs},
  booktitle = {AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents (AAAI-SDMIA15)},
  location = {Arlington, Virginia, USA},
  month = {November},
  year = {2015},
  abstract={
Deep Reinforcement Learning has yielded proficient controllers for
complex tasks. However, these controllers have limited memory and rely
on being able to perceive the complete game screen at each decision
point. To address these shortcomings, this article investigates the
effects of adding recurrency to a Deep Q-Network (DQN) by replacing
the first post-convolutional fully-connected layer with a recurrent
LSTM. The resulting Deep Recurrent Q-Network (DRQN), although
capable of seeing only a single frame at each timestep, successfully
integrates information through time and replicates DQN's performance
on standard Atari games and partially observed equivalents featuring
flickering game screens. Additionally, when trained with partial
observations and evaluated with incrementally more complete
observations, DRQN's performance scales as a function of
observability. Conversely, when trained with full observations and
evaluated with partial observations, DRQN's performance degrades less
than DQN's. Thus, given the same length of history, recurrency is a
viable alternative to stacking a history of frames in the DQN's input
layer and while recurrency confers no systematic advantage when learning
to play the game, the recurrent net can better adapt at evaluation
time if the quality of observations changes.
  },
}

@article{schulman2017ppo,
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. },
  added-at = {2019-12-16T18:31:56.000+0100},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  biburl = {https://www.bibsonomy.org/bibtex/24bbcce6aa1c42ae7f61ef8cf5475aa85/lanteunis},
  ee = {http://arxiv.org/abs/1707.06347},
  interhash = {f57ff463a90dbafb77d55a25aea8355c},
  intrahash = {4bbcce6aa1c42ae7f61ef8cf5475aa85},
  journal = {CoRR},
  keywords = {DRLAlgoComparison ppo reinforcement_learning},
  timestamp = {2019-12-18T21:15:59.000+0100},
  title = {Proximal Policy Optimization Algorithms.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1707.html#SchulmanWDRK17},
  volume = {abs/1707.06347},
  year = 2017
}
@inproceedings{attentionAllYouNeed,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}
@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}


@InProceedings{trpo,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schulman15.html},
  abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}
@article{schulman2017ppo,
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. },
  added-at = {2019-12-16T18:31:56.000+0100},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  biburl = {https://www.bibsonomy.org/bibtex/24bbcce6aa1c42ae7f61ef8cf5475aa85/lanteunis},
  ee = {http://arxiv.org/abs/1707.06347},
  interhash = {f57ff463a90dbafb77d55a25aea8355c},
  intrahash = {4bbcce6aa1c42ae7f61ef8cf5475aa85},
  journal = {CoRR},
  keywords = {DRLAlgoComparison ppo reinforcement_learning},
  timestamp = {2019-12-18T21:15:59.000+0100},
  title = {Proximal Policy Optimization Algorithms.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1707.html#SchulmanWDRK17},
  volume = {abs/1707.06347},
  year = 2017
}
@article{Papadimitriou1987TheCO,
  title={The Complexity of Markov Decision Processes},
  author={Christos H. Papadimitriou and John N. Tsitsiklis},
  journal={Math. Oper. Res.},
  year={1987},
  volume={12},
  pages={441-450}
}

@misc{pomdpnotscary,
  doi = {10.48550/ARXIV.2204.08967},
  
  url = {https://arxiv.org/abs/2204.08967},
  
  author = {Liu, Qinghua and Chung, Alan and Szepesvári, Csaba and Jin, Chi},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Systems and Control (eess.SY), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {When Is Partially Observable Reinforcement Learning Not Scary? },
  
  publisher = {arXiv},
  
  year = {2022 Available at: \href{https://arxiv.org/abs/2204.08967 }},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{bellemare2012ale,
 
  added-at = {2019-12-30T13:47:50.000+0100},
  author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/20ab3dc05ba807da8085ca30679390cde/lanteunis},
  doi = {10.1613/jair.3912},
  interhash = {ad133244b4e33eda52c994d9b6174f27},
  intrahash = {0ab3dc05ba807da8085ca30679390cde},
  journal = {Journal of Artificial Intelligence Research},
  keywords = {ALE Atari DRLAlgoComparison ReinforcementLearning reinforcement},
  note = {cite arxiv:1207.4708},
  pages = {253-279},
  timestamp = {2019-12-30T13:55:38.000+0100},
  title = {The Arcade Learning Environment: An Evaluation Platform for General
  Agents},
  url = {http://arxiv.org/abs/1207.4708},
  volume = {Vol. 47},
  year = 2012
}


@misc{dota2RL,
  doi = {10.48550/ARXIV.1912.06680},
  
  url = {https://arxiv.org/abs/1912.06680},
  
  author = {{OpenAI} and {:} and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dębiak, Przemysław and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and Józefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d. O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Dota 2 with Large Scale Deep Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{backpropagation,
  title={Learning representations by back-propagating errors},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536}
}
@misc{decADRL,
  doi = {10.48550/ARXIV.2112.04912},
  
  url = {https://arxiv.org/abs/2112.04912},
  
  author = {Joseph, Geethu and Zhong, Chen and Gursoy, M. Cenk and Velipasalar, Senem and Varshney, Pramod K.},
  
  keywords = {Machine Learning (cs.LG), Signal Processing (eess.SP), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Scalable and Decentralized Algorithms for Anomaly Detection via Learning-Based Controlled Sensing},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@INPROCEEDINGS{decHTKohen,  author={Szostak, Hadar and Cohen, Kobi},  booktitle={2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},   title={Decentralized Anomaly Detection via Deep Multi-Agent Reinforcement Learning},   year={2022},  volume={},  number={},  pages={1-4},  doi={10.1109/Allerton49937.2022.9929423}}
@article{TENEKETZIS198723,
title = {The decentralized wald problem},
journal = {Information and Computation},
volume = {73},
number = {1},
pages = {23-44},
year = {1987},
issn = {0890-5401},
doi = {https://doi.org/10.1016/0890-5401(87)90038-1},
url = {https://www.sciencedirect.com/science/article/pii/0890540187900381},
author = {Demosthenis Teneketzis and Yu-Chi Ho},
abstract = {Two detectors making independent observations must decide which one of two hypotheses is true. The decisions are coupled through a common cost function. It is shown that the detectors' optimal decisions are characterized by thresholds which are coupled and whose computation requires the solution of two coupled sets of dynamic programming equations. An approximate computation of the thresholds is proposed and numerical results are presented.}
}