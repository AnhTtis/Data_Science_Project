\documentclass[twoside,11pt]{article}

%\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
\usepackage[abbrvbib,preprint]{jmlr2e}

%\usepackage{jmlr2e}
\usepackage{algorithm}
\usepackage{amsmath,amssymb,amsfonts,mathrsfs}
\usepackage{breakcites}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{pdfpages}
\usepackage{bm}
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs}  
\usepackage{lipsum}
\usepackage{makecell}
\usepackage{rotating}
\usepackage{tablefootnote}
%\usepackage{cleveref}


\usepackage{threeparttable}

% Definitions of handy macros can go here

\input{defs}

\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
%\newcommand{\dataset}{{\cal D}}
%\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\newcommand{\peiyuan}[1]{{\color{blue} Peiyuan's comment: #1}}
\newcommand{\jingzhao}[1]{{\color{red} Jingzhao's comment: #1}}
\newcommand{\jiaye}[1]{{\color{magenta} Jiaye's comment: #1}}


% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
%\jmlrheading{23}{2022}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Peiyuan Zhang, Jiaye Teng and Jingzhao Zhang}

% Short headings should be running head and authors last names

\ShortHeadings{}{Zhang, Teng and Zhang}
\firstpageno{1}

\begin{document}

\title{Lower Generalization Bounds for GD and SGD \\
in Smooth Stochastic Convex Optimization}

\author{\name Peiyuan Zhang\thanks{Partly done when the author was an intern at Shanghai Qizhi Institute.} \email peiyuan.zhang@yale.edu \\
       \addr Yale University%\\
       %New Haven, CT
       \AND
       \name Jiaye Teng  \email tjy20@mail.tsinghua.edu.cn \\
       \addr Tsinghua University%\\
       %Beijing, China
       \AND
       \name Jingzhao Zhang \email jingzhaoz@mail.tsinghua.edu.cn \\
       \addr Tsinghua University \& Shanghai Qizhi Institute
       %Beijing, China
       }

\editor{My editor}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
Recent progress was made in characterizing the generalization error of gradient methods for general convex loss by the learning theory community. In this work, we focus on how training longer might affect generalization in \emph{smooth} stochastic convex optimization (SCO) problems. We first provide tight lower bounds for \emph{general non-realizable} SCO problems. Furthermore, existing upper bound results suggest that sample complexity can be improved by assuming the loss is \emph{realizable}, i.e. an optimal
solution simultaneously minimizes all the data points. However, this improvement is compromised when training time is long and lower bounds are lacking. Our paper examines this observation by providing excess risk lower bounds for gradient descent (GD) and stochastic gradient descent (SGD) in two \emph{realizable} settings: 1) realizable with $T = \bigO{n}$, and (2) realizable with $T = \bigOmega{n}$, where $T$ denotes the number of training iterations and $n$ is the size of the training dataset. These bounds are novel and informative in characterizing the relationship between $T$ and $n$. In the first small training horizon case, our lower bounds almost tightly match and provide the \emph{first} optimal certificates for the corresponding upper bounds. However, for the realizable case with $T = \bigOmega{n}$, a gap exists between the lower and upper bounds. We provide a conjecture to address this problem, that the gap can be closed by improving upper bounds, which is supported by our analyses in one-dimensional and linear regression scenarios. 
%\lipsum[1]
\end{abstract}

\begin{keywords}
  generalization, excess risk, stochastic convex optimization, realizable setting, lower bounds
\end{keywords}

\input{texts/sec_1_intro}
\input{texts/sec_2_setting}
\input{texts/sec_3_lower_bounds}
\input{texts/sec_4_infinite}
\input{texts/sec_5_proofs}
\input{texts/sec_6_related}

%\acks{All acknowledgements go at the end of the paper before appendices and references. Moreover, you are required to declare funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work). More information about this disclosure can be found on the JMLR website.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.


%\input{texts/appendix_4(upper bound 1-dim)}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:



\vskip 0.2in
\bibliography{bib}

\newpage

\appendix

\input{texts/appendix_1}
\input{texts/appendix_2}
\input{texts/appendix_3}
%\input{texts/appendix_5(ub-gd)}

\end{document}