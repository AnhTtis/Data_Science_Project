



@article{lyu2021gradient,
  title={Gradient descent on two-layer nets: Margin maximization and simplicity bias},
  author={Lyu, Kaifeng and Li, Zhiyuan and Wang, Runzhe and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12978--12991},
  year={2021}
}

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}

@article{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{jiang2019fantastic,
  title={Fantastic generalization measures and where to find them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  journal={arXiv preprint arXiv:1912.02178},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{schliserman2022stability,
  title={Stability vs implicit bias of gradient methods on separable data and beyond},
  author={Schliserman, Matan and Koren, Tomer},
  booktitle={Conference on Learning Theory},
  pages={3380--3394},
  year={2022},
  organization={PMLR}
}

@inproceedings{pillaud2018exponential,
  title={Exponential convergence of testing error for stochastic gradient methods},
  author={Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={250--296},
  year={2018},
  organization={PMLR}
}

@article{sekhari2021sgd,
  title={Sgd: The role of implicit regularization, batch-size and multiple-epochs},
  author={Sekhari, Ayush and Sridharan, Karthik and Kale, Satyen},
  journal={Advances In Neural Information Processing Systems},
  volume={34},
  pages={27422--27433},
  year={2021}
}

@inproceedings{zou2021benign,
  title={Benign overfitting of constant-stepsize sgd for linear regression},
  author={Zou, Difan and Wu, Jingfeng and Braverman, Vladimir and Gu, Quanquan and Kakade, Sham},
  booktitle={Conference on Learning Theory},
  pages={4633--4635},
  year={2021},
  organization={PMLR}
}

@article{zou2022risk,
  title={Risk Bounds of Multi-Pass SGD for Least Squares in the Interpolation Regime},
  author={Zou, Difan and Wu, Jingfeng and Braverman, Vladimir and Gu, Quanquan and Kakade, Sham M},
  journal={arXiv preprint arXiv:2203.03159},
  year={2022}
}

@article{pillaud2018statistical,
  title={Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes},
  author={Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{lei2021generalization,
  title={Generalization performance of multi-pass stochastic gradient descent with convex loss functions},
  author={Lei, Yunwen and Hu, Ting and Tang, Ke},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={1145--1185},
  year={2021},
  publisher={JMLRORG}
}

@book{DBLP:books/daglib/0033642,
  author    = {Shai Shalev{-}Shwartz and
               Shai Ben{-}David},
  title     = {Understanding Machine Learning - From Theory to Algorithms},
  publisher = {Cambridge University Press},
  year      = {2014},
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{bassily2019private,
  title={Private stochastic convex optimization with optimal rates},
  author={Bassily, Raef and Feldman, Vitaly and Talwar, Kunal and Guha Thakurta, Abhradeep},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{ma2022dimension,
  title={Dimension independent generalization of dp-sgd for overparameterized smooth convex optimization},
  author={Ma, Yi-An and Marinov, Teodor Vanislavov and Zhang, Tong},
  journal={arXiv preprint arXiv:2206.01836},
  year={2022}
}

@article{aubin2020generalization,
  title={Generalization error in high-dimensional perceptrons: Approaching bayes error with convex optimization},
  author={Aubin, Benjamin and Krzakala, Florent and Lu, Yue and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12199--12210},
  year={2020}
}

@inproceedings{DBLP:conf/nips/AubinKLZ20,
  author    = {Benjamin Aubin and
               Florent Krzakala and
               Yue M. Lu and
               Lenka Zdeborov{\'{a}}},
  title     = {Generalization error in high-dimensional perceptrons: Approaching
               Bayes error with convex optimization},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
}

@article{feldman2016generalization,
  title={Generalization of erm in stochastic convex optimization: The dimension strikes back},
  author={Feldman, Vitaly},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@inproceedings{DBLP:conf/icml/HarutyunyanRSG20,
  author    = {Hrayr Harutyunyan and
               Kyle Reing and
               Greg Ver Steeg and
               Aram Galstyan},
  title     = {Improving generalization by controlling label-noise information in
               neural network weights},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML} 2020, 13-18 July 2020, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {4071--4081},
  publisher = {{PMLR}},
  year      = {2020},
}

@inproceedings{harutyunyan2020improving,
  title={Improving generalization by controlling label-noise information in neural network weights},
  author={Harutyunyan, Hrayr and Reing, Kyle and Ver Steeg, Greg and Galstyan, Aram},
  booktitle={International Conference on Machine Learning},
  pages={4071--4081},
  year={2020},
  organization={PMLR}
}

@article{song2019does,
  title={How does early stopping help generalization against label noise?},
  author={Song, Hwanjun and Kim, Minseok and Park, Dongmin and Lee, Jae-Gil},
  journal={arXiv preprint arXiv:1911.08059},
  year={2019}
}

@article{DBLP:journals/corr/abs-1911-08059,
  author    = {Hwanjun Song and
               Minseok Kim and
               Dongmin Park and
               Jae{-}Gil Lee},
  title     = {Prestopping: How Does Early Stopping Help Generalization against Label
               Noise?},
  journal   = {CoRR},
  volume    = {abs/1911.08059},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.08059},
  eprinttype = {arXiv},
  eprint    = {1911.08059},
}

@article{wen2022realistic,
  title={Realistic Deep Learning May Not Fit Benignly},
  author={Wen, Kaiyue and Teng, Jiaye and Zhang, Jingzhao},
  journal={arXiv preprint arXiv:2206.00501},
  year={2022}
}

@inproceedings{DBLP:conf/iclr/TengMY22,
  author    = {Jiaye Teng and
               Jianhao Ma and
               Yang Yuan},
  title     = {Towards Understanding Generalization via Decomposing Excess Risk Dynamics},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
}

@article{DBLP:journals/corr/abs-2206-00501,
  author    = {Kaiyue Wen and
               Jiaye Teng and
               Jingzhao Zhang},
  title     = {Realistic Deep Learning May Not Fit Benignly},
  journal   = {CoRR},
  volume    = {abs/2206.00501},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2206.00501},
  doi       = {10.48550/arXiv.2206.00501},
  eprinttype = {arXiv},
  eprint    = {2206.00501},
}

@article{nikolakakis2022beyond,
  title={Beyond Lipschitz: sharp generalization and excess risk bounds for full-batch GD},
  author={Nikolakakis, Konstantinos E and Haddadpour, Farzin and Karbasi, Amin and Kalogerias, Dionysios S},
  journal={arXiv preprint arXiv:2204.12446},
  year={2022}
}

@article{taheri2023generalization,
  title={Generalization and Stability of Interpolating Neural Networks with Minimal Width},
  author={Taheri, Hossein and Thrampoulidis, Christos},
  journal={arXiv preprint arXiv:2302.09235},
  year={2023}
}

@inproceedings{grunwald2021pac,
  title={PAC-Bayes, MAC-Bayes and Conditional Mutual Information: Fast rate bounds that handle general VC classes},
  author={Grunwald, Peter and Steinke, Thomas and Zakynthinou, Lydia},
  booktitle={Conference on Learning Theory},
  pages={2217--2247},
  year={2021},
  organization={PMLR}
}

@inproceedings{DBLP:conf/colt/Grunwald0Z21,
  author    = {Peter Gr{\"{u}}nwald and
               Thomas Steinke and
               Lydia Zakynthinou},
  title     = {PAC-Bayes, MAC-Bayes and Conditional Mutual Information: Fast rate
               bounds that handle general {VC} classes},
  booktitle = {Conference on Learning Theory, {COLT} 2021, 15-19 August 2021, Boulder,
               Colorado, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {134},
  pages     = {2217--2247},
  publisher = {{PMLR}},
  year      = {2021},
}

@inproceedings{DBLP:conf/nips/YangS019,
  author    = {Jun Yang and
               Shengyang Sun and
               Daniel M. Roy},
  title     = {Fast-rate PAC-Bayes Generalization Bounds via Shifted Rademacher Processes},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {10802--10812},
  year      = {2019},
}

@article{yang2019fast,
  title={Fast-rate PAC-Bayes generalization bounds via shifted Rademacher processes},
  author={Yang, Jun and Sun, Shengyang and Roy, Daniel M},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{dalalyan2018exponentially,
  title={On the exponentially weighted aggregate with the laplace prior},
  author={Dalalyan, AS and Grappin, E and Paris, Q},
  journal={Annals of Statistics},
  volume={46},
  number={5},
  pages={2452--2478},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}

@article{chesneau2009adapting,
  title={Adapting to unknown smoothness by aggregation of thresholded wavelet estimators},
  author={Chesneau, Christophe and Lecu{\'e}, Guillaume},
  journal={Statistica Sinica},
  pages={1407--1417},
  year={2009},
  publisher={JSTOR}
}

@article{tsybakov2004optimal,
  title={Optimal aggregation of classifiers in statistical learning},
  author={Tsybakov, Alexander B},
  journal={The Annals of Statistics},
  volume={32},
  number={1},
  pages={135--166},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}

@phdthesis{bousquet2002concentration,
  title={Concentration inequalities and empirical processes theory applied to the analysis of learning algorithms},
  author={Bousquet, Olivier},
  year={2002},
  school={{\'E}cole Polytechnique: Department of Applied Mathematics Paris, France}
}

@article{panchenko2002some,
  title={Some extensions of an inequality of Vapnik and Chervonenkis.},
  author={Panchenko, Dmitriy},
  journal={Electronic Communications in Probability [electronic only]},
  volume={7},
  pages={55--65},
  year={2002},
  publisher={University of Washington}
}

@inproceedings{lee1996importance,
  title={The importance of convexity in learning with squared loss},
  author={Lee, Wee Sun and Bartlett, Peter L and Williamson, Robert C},
  booktitle={Proceedings of the Ninth Annual Conference on Computational Learning Theory},
  pages={140--146},
  year={1996}
}

@inproceedings{DBLP:conf/colt/LeeBW96,
  author    = {Wee Sun Lee and
               Peter L. Bartlett and
               Robert C. Williamson},
  title     = {The Importance of Convexity in Learning with Squared Loss},
  booktitle = {Proceedings of the Ninth Annual Conference on Computational Learning
               Theory, {COLT} 1996, Desenzano del Garda, Italy, June 28-July 1, 1996},
  pages     = {140--146},
  publisher = {{ACM}},
  year      = {1996},
}

@inproceedings{DBLP:conf/uai/ZhangZBP0022,
  author    = {Yikai Zhang and
               Wenjia Zhang and
               Sammy Bald and
               Vamsi Pingali and
               Chao Chen and
               Mayank Goswami},
  title     = {Stability of {SGD:} Tightness analysis and improved bounds},
  booktitle = {Uncertainty in Artificial Intelligence, Proceedings of the Thirty-Eighth
               Conference on Uncertainty in Artificial Intelligence, {UAI} 2022,
               1-5 August 2022, Eindhoven, The Netherlands},
  series    = {Proceedings of Machine Learning Research},
  volume    = {180},
  pages     = {2364--2373},
  publisher = {{PMLR}},
  year      = {2022},
}

@inproceedings{DBLP:conf/colt/FeldmanV19,
  author    = {Vitaly Feldman and
               Jan Vondr{\'{a}}k},
  title     = {High probability generalization bounds for uniformly stable algorithms
               with nearly optimal rate},
  booktitle = {Conference on Learning Theory, {COLT} 2019, 25-28 June 2019, Phoenix,
               AZ, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {99},
  pages     = {1270--1279},
  publisher = {{PMLR}},
  year      = {2019},
}

@inproceedings{feldman2019high,
  title={High probability generalization bounds for uniformly stable algorithms with nearly optimal rate},
  author={Feldman, Vitaly and Vondrak, Jan},
  booktitle={Conference on Learning Theory},
  pages={1270--1279},
  year={2019},
  organization={PMLR}
}

@inproceedings{DBLP:conf/icml/HardtRS16,
  author    = {Moritz Hardt and
               Ben Recht and
               Yoram Singer},
  title     = {Train faster, generalize better: Stability of stochastic gradient
               descent},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning,
               {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {48},
  pages     = {1225--1234},
  publisher = {JMLR.org},
  year      = {2016},
}


@article{DBLP:journals/corr/abs-2112-04470,
  author    = {Lijia Zhou and
               Frederic Koehler and
               Danica J. Sutherland and
               Nathan Srebro},
  title     = {Optimistic Rates: {A} Unifying Theory for Interpolation Learning and
               Regularization in Linear Regression},
  journal   = {CoRR},
  volume    = {abs/2112.04470},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.04470},
  eprinttype = {arXiv},
  eprint    = {2112.04470},
}

@article{zhou2021optimistic,
  title={Optimistic rates: A unifying theory for interpolation learning and regularization in linear regression},
  author={Zhou, Lijia and Koehler, Frederic and Sutherland, Danica J and Srebro, Nathan},
  journal={arXiv preprint arXiv:2112.04470},
  year={2021}
}

@article{steinwart2007fast,
  title={Fast rates for support vector machines using gaussian kernels},
  author={Ingo Steinwart and Clint Scovel},
  journal={Annals of statistics},
  volume={35},
  number={2},
  pages={575--607},
  year={2007}
}

@article{srebro2010optimistic,
  title={Optimistic rates for learning with a smooth loss},
  author={Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1009.3896},
  year={2010}
}

@article{bartlett2005local,
  title={Local Rademacher complexities},
  author={Bartlett, Peter and Bousquet, Olivier and Mendelson, Shahar},
  journal={Annals of Statistics},
  volume={33},
  number={4},
  pages={1497--1537},
  year={2005},
  publisher={Institute of Mathematical Statistics}
}

@article{glasgow2022max,
  title={Max-Margin Works while Large Margin Fails: Generalization without Uniform Convergence},
  author={Glasgow, Margalit and Wei, Colin and Wootters, Mary and Ma, Tengyu},
  journal={arXiv preprint arXiv:2206.07892},
  year={2022}
}

@article{DBLP:journals/corr/abs-2206-07892,
  author    = {Margalit Glasgow and
               Colin Wei and
               Mary Wootters and
               Tengyu Ma},
  title     = {Max-Margin Works while Large Margin Fails: Generalization without
               Uniform Convergence},
  journal   = {CoRR},
  volume    = {abs/2206.07892},
  year      = {2022},
}

@inproceedings{DBLP:conf/nips/NagarajanK19,
  author    = {Vaishnavh Nagarajan and
               J. Zico Kolter},
  title     = {Uniform convergence may be unable to explain generalization in deep
               learning},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {11611--11622},
  year      = {2019},
}

@article{nagarajan2019uniform,
  title={Uniform convergence may be unable to explain generalization in deep learning},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{DBLP:conf/colt/HopkinsKLM22,
  author    = {Max Hopkins and
               Daniel M. Kane and
               Shachar Lovett and
               Gaurav Mahajan},
  title     = {Realizable Learning is All You Need},
  booktitle = {Conference on Learning Theory, 2-5 July 2022, London, {UK}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {178},
  pages     = {3015--3069},
  publisher = {{PMLR}},
  year      = {2022}
}

@article{DBLP:journals/iandc/Haussler92,
  author    = {David Haussler},
  title     = {Decision Theoretic Generalizations of the {PAC} Model for Neural Net
               and Other Learning Applications},
  journal   = {Inf. Comput.},
  volume    = {100},
  number    = {1},
  pages     = {78--150},
  year      = {1992}
}

@article{DBLP:journals/jacm/BlumerEHW89,
  author    = {Anselm Blumer and
               Andrzej Ehrenfeucht and
               David Haussler and
               Manfred K. Warmuth},
  title     = {Learnability and the Vapnik-Chervonenkis dimension},
  journal   = {J. {ACM}},
  volume    = {36},
  number    = {4},
  pages     = {929--965},
  year      = {1989}
}

@misc{vapnik1974theory,
  title={Theory of pattern recognition},
  author={Vapnik, Vladimir and Chervonenkis, Alexey},
  year={1974},
  publisher={Nauka, Moscow}
}

@article{shalev2010learnability,
  title={Learnability, stability and uniform convergence},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={2635--2670},
  year={2010},
  publisher={JMLR. org}
}

@article{DBLP:journals/jmlr/Shalev-ShwartzSSS10,
  author    = {Shai Shalev{-}Shwartz and
               Ohad Shamir and
               Nathan Srebro and
               Karthik Sridharan},
  title     = {Learnability, Stability and Uniform Convergence},
  journal   = {J. Mach. Learn. Res.},
  volume    = {11},
  pages     = {2635--2670},
  year      = {2010}
}

@book{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskij, Arkadij Semenovi{\v{c}} and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley-Interscience}
}


@inproceedings{zhang2022stability,
  title={Stability of sgd: Tightness analysis and improved bounds},
  author={Zhang, Yikai and Zhang, Wenjia and Bald, Sammy and Pingali, Vamsi and Chen, Chao and Goswami, Mayank},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={2364--2373},
  year={2022},
  organization={PMLR}
}


@inproceedings{ji2019implicit,
  title={The implicit bias of gradient descent on nonseparable data},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1772--1798},
  year={2019},
  organization={PMLR}
}

@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30063--30070},
  year={2020},
  publisher={National Acad Sciences}
}

@inproceedings{fang2020fast,
  title={Fast convergence of stochastic subgradient method under interpolation},
  author={Fang, Huang and Fan, Zhenan and Friedlander, Michael},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{vaswani2019painless,
  title={Painless stochastic gradient: Interpolation, line-search, and convergence rates},
  author={Vaswani, Sharan and Mishkin, Aaron and Laradji, Issam and Schmidt, Mark and Gidel, Gauthier and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{bassily2018exponential,
  title={On exponential convergence of sgd in non-convex over-parametrized learning},
  author={Bassily, Raef and Belkin, Mikhail and Ma, Siyuan},
  journal={arXiv preprint arXiv:1811.02564},
  year={2018}
}

@inproceedings{ma2018power,
  title={The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning},
  author={Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  booktitle={International Conference on Machine Learning},
  pages={3325--3334},
  year={2018},
  organization={PMLR}
}

@article{yun2021minibatch,
  title={Minibatch vs local SGD with shuffling: Tight convergence bounds and beyond},
  author={Yun, Chulhee and Rajput, Shashank and Sra, Suvrit},
  journal={arXiv preprint arXiv:2110.10342},
  year={2021}
}

@inproceedings{ma2018power,
  title={The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning},
  author={Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  booktitle={International Conference on Machine Learning},
  pages={3325--3334},
  year={2018},
  organization={PMLR}
}

@inproceedings{golowich2020last,
  title={Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems},
  author={Golowich, Noah and Pattathil, Sarath and Daskalakis, Constantinos and Ozdaglar, Asuman},
  booktitle={Conference on Learning Theory},
  pages={1758--1784},
  year={2020},
  organization={PMLR}
}

@article{mokhtari2020convergence,
  title={Convergence Rate of O(1/k) for Optimistic Gradient and Extragradient Methods in Smooth Convex-Concave Saddle Point Problems},
  author={Mokhtari, Aryan and Ozdaglar, Asuman E and Pattathil, Sarath},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={4},
  pages={3230--3251},
  year={2020},
  publisher={SIAM}
}

@article{bassily2020stability,
  title={Stability of stochastic gradient descent on nonsmooth convex losses},
  author={Bassily, Raef and Feldman, Vitaly and Guzm{\'a}n, Crist{\'o}bal and Talwar, Kunal},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4381--4391},
  year={2020}
}


@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International conference on machine learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}

@inproceedings{vaswani2019fast,
  title={Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron},
  author={Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1195--1204},
  year={2019},
  organization={PMLR}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={The Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002},
  publisher={JMLR. org}
}


@inproceedings{kuzborskij2018data,
  title={Data-dependent stability of stochastic gradient descent},
  author={Kuzborskij, Ilja and Lampert, Christoph},
  booktitle={International Conference on Machine Learning},
  pages={2815--2824},
  year={2018},
  organization={PMLR}
}

@article{moulines2011non,
  title={Non-asymptotic analysis of stochastic approximation algorithms for machine learning},
  author={Moulines, Eric and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}


@inproceedings{amir2021sgd,
  title={SGD generalizes better than GD (and regularization doesnâ€™t help)},
  author={Amir, Idan and Koren, Tomer and Livni, Roi},
  booktitle={Conference on Learning Theory},
  pages={63--92},
  year={2021},
  organization={PMLR}
}



@article{amir2021never,
  title={Never go full batch (in stochastic convex optimization)},
  author={Amir, Idan and Carmon, Yair and Koren, Tomer and Livni, Roi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25033--25043},
  year={2021}
}

@inproceedings{lei2020fine,
  title={Fine-grained analysis of stability and generalization for stochastic gradient descent},
  author={Lei, Yunwen and Ying, Yiming},
  booktitle={International Conference on Machine Learning},
  pages={5809--5819},
  year={2020},
  organization={PMLR}
}

@inproceedings{shalev2009stochastic,
  title={Stochastic Convex Optimization.},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  booktitle={COLT},
  volume={2},
  number={4},
  pages={5},
  year={2009}
}