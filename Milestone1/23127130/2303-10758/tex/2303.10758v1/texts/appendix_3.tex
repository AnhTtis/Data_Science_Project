\section{Missing proof in Section~\ref{sec: infinite}} \label{appendix: infinite}
Here we provide Lemma~\ref{lemma: dim-1-ub-gd}, the GD version of Lemma~\ref{lemma: dim-1-ub-sgd}: in dimension one, GD is also able to achieve $\bigO{1/n}$ bound under the regime $T = \bigOmega{n}$.
\begin{lemma} [Restated Lemma~\ref{lemma: dim-1-ub-gd}]
In dimension one, if $f(w,z)$ is convex, $1$-smooth and realizable with $z \sim D$, then for every $\eta = \Theta(1)$, there exists $T_0 = \Theta(n)$ such that for $T \geq T_0$, the output $w_{\gd}$ of GD satisfies
\begin{equation*}
    \bbE[ F(w_{\gd})] - F(w^*) = \bigO{\frac{1}{n}}.
\end{equation*} 
\end{lemma}
\begin{proof}
From Theorem~10 in \citet{nikolakakis2022beyond}, it holds that for realizable cases (we rescale it to $f(w^*, z) = 0$ for each $z$) with step size $\eta =  \Theta(1)$, it holds that
\begin{equation}
    \bbE[F(w_{\gd})] = \bigO{\frac{1}{T_0} + \frac{1 + T_0/n}{n}}.
\end{equation}
Therefore, for $T_0= \Theta(n)$, it holds that
\begin{equation*}
    \bbE[F(w_{\gd})] = \bigO{\nfrac{1}{n}}.
\end{equation*}
For SGD, the iteration formulates the iterate 
\begin{equation*}
    w_{t+1} = w_t - \eta \nabla F_S(w_t).
\end{equation*}
Under the realizable and convex assumption, the iteration becomes
\begin{equation*}
    w_{t+1} - w^* = (1 - \eta \nabla^2 F_S(\xi)) (w_t - w^*),
\end{equation*}
using mean value theorem, where $\xi$ is a point between $w_t$ and $w^*$. This indicates that the distance $w_t - w^*$ shrinks in each step. Due to the convexity of $F$, it holds that 
$F(w_{t+1}) \leq F(w_t)$. In summary, for any $T \geq T_0$, it holds that 
\begin{equation*}
    \bbE[F(w_{\gd})] - F(w^*) = \bigO{\nfrac{1}{n}}.
\end{equation*} 
which is the desired result.
\end{proof}

\section{Minor proofs} \label{appendix: minor}
\subsection{Lower bound of term \texorpdfstring{$1/n$}{1/n}} \label{appendix: lb-sample}
\begin{lemma} \label{lemma: lb-sample}
    For every $\eta > 0$, $T \geq 1$, there exists a convex, $1$-smooth and realizable $f(w, z): \bbR^{2n} \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, it holds for the output of any gradient-based algorithm $\cA[S]$
    \begin{align*}
        \bbE[F(\cA[S])] - F(w^*) = \bigOmega{\nfrac{1}{n}}.  
    \end{align*}
\end{lemma}
\begin{proof}
    We consider the following instance
    \begin{equation*}
        f(w, z = i) = \frac{1}{2} w(i)^2, \qquad z \sim \textrm{Uniform}([2n]),
    \end{equation*}
    where $w(i)$ denotes the $i$-th coordinate of $w$. Then the population risk is
    \begin{align*}
        F(w) = \bbE_{z\sim\text{Uniform}([2n])}[f(w,z)] = \frac{1}{4n} \|w\|^2,
    \end{align*}
    which achieves minimum at $w^* = 0$. It is easy to reckon that $f(w,z)$ is smooth, convex and realizable. Now consider any dataset $S$ of $n$ samples. Since  $z \sim \textrm{Uniform}(2n)$, with probability $\Omega(1)$,  $\Theta(n)$ coordinates are not observed. For any gradient-based algorithm with initialization $w_0 = \frac{1}{\sqrt{2n}} \cdot \bm{1}_d$, the unobserved $\Theta(n)$ coordinates will remain unchanged for any step-size $\eta$ and $T$. Then we have the following lower bound:
    \begin{align*}
        \bbE[F(\cA[S])] - F(w^*) = F(w_{\gd}) - F(w^*) = \Omega\left(\frac{1}{4n} \cdot \frac{n}{2n}\right) = \Omega\left(\frac{1}{n}\right),
    \end{align*}
    which is the desired result.
\end{proof}


\subsection{Lower bound of term \texorpdfstring{$1/\eta T$}{1/eta T}}  \label{appendix: lb-suboptimality}
\begin{lemma} \label{lemma: lb-suboptimality}
For every $\eta > 0$, $T \geq 2$, there exists a convex, $\bigO{1}$-smooth and realizable $f(w, z): \bbR^2 \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, the output $w_{\gd}$ for GD satisfies
\begin{align*}
 \bbE[F(w_{\gd})] - F(w^*) = \Omega\left(\frac{1}{\eta T}\right). 
\end{align*}
The same result also holds for SGD.
\end{lemma}
\begin{proof}
    We define the deterministic convex and $\bigO{1}$-smooth function as
    \begin{align}
        f(w) = \frac{1}{2} w^2(1) + \frac{\lambda}{2} w^2(2)
    \end{align}
    with $0 < \lambda < 1$, $w(1)$ and $w(2)$ are the value of first and second coordinate of $w$. Then GD formulates the iteration
    \begin{align*}
        w_{t+1} = w_t - \eta \nabla f(w_t),
    \end{align*}
    with initialization $w_0 = (1,1)$. This is then precisely:
    \begin{align*}
        w_{t+1}(1) & = (1 -\eta) w_{t+1}(1), \qquad w_{t+1}(2) = (1 -\lambda\eta) w_{t+1}(2).
    \end{align*}
    At iteration $t \in [T]$, we can upper bound 
    \begin{align*}
        w_{t}(1) \geq \frac{1}{4} e^{-\eta t} \cdot x_0(1), \qquad w_{t}(2) \geq \frac{1}{4} e^{- \lambda \eta t} \cdot x_0(2) = \frac{e^{-t/T}}{4}.
    \end{align*}
    The averaged output is then
    \begin{align*}
        \hat{w}_T(2) = \sum_{t=1}^T \hat{w}_t(2) \geq \sum_{t=1}^T \frac{e^{-t/T}}{4T} \geq \frac{1-e^{-1}}{4} > \frac{1}{8}.
    \end{align*}
    With $\lambda = \tfrac{1}{\eta T}$, the suboptimality is then
    \begin{equation}
        f(w_{\gd}) - f(w^*) \geq \frac{\lambda}{2} |w_{\gd}(2)|^2 \geq \frac{1}{128\eta T}.
    \end{equation}
    Then the following result holds:
    \begin{equation*}
        \bbE[F(w_{\gd})] - F(w^*) = f(w_T) - f(w^*) \geq \Omega\left(\frac{1}{\eta T}\right)
    \end{equation*}
    because $f(w)$ is a deterministic function. Since the instance is deterministic, then the suboptimality lower bound $\bigOmega{\frac{1}{\eta T}}$ also holds for SGD.
\end{proof}


\subsection{GD upper bound for realizable smooth SCO} \label{appendix: gd-ub}
Here we derive the upper bound for GD under realizable smooth SCO, as in Table~\ref{tab: summary}. The derivation is based on Theorem~10 in \citet{nikolakakis2022beyond}.
In the realizable cases, it holds that (see \citet{nikolakakis2022beyond} for the notations):
\begin{equation*}
    \begin{split}
        &\epsilon_{\text{opt}} = \frac{\bbE \| w_1 - w_S^*\|^2}{\eta T} \\
        &\epsilon_{\text{path}} = \beta (\bbE \| w_1 - w_S^*\|^2 + \epsilon_{{\boldsymbol{c}}} \eta T) \\
        &\epsilon_{{\boldsymbol{c}}} = 0 
    \end{split}
\end{equation*}
Plug them in Theorem~10, it holds that for some constant c,
\begin{equation*}
    |\epsilon_{\text{gen}}| \leq c \frac{\beta \bbE \| w_1 - w_S^*\|^2}{n} + \frac{\beta^2 \eta T \bbE \| w_1 - w_S^*\|^2}{n^2}.
\end{equation*}
Combined with the optimization upper bound $\bigO{1/{\eta T}}$, we obtain the upper bound
\begin{equation*}
    \bbE[F(w_{\gd})] - F(w^*) = \bigO{\frac{1}{\eta T} + \frac{1}{n} + \frac{\eta T}{n^2}}.
\end{equation*}