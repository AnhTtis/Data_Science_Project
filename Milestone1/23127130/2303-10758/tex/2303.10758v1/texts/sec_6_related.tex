\section{Related Work}
Convex optimization has a rich history and its generalization error has been extensively explored in the literature~\citep{boyd2004convex,shalev2014understanding}, with one-pass SGD~\citep{pillaud2018exponential}, multi-pass SGD~\citep{pillaud2018statistical,sekhari2021sgd,lei2021generalization}, DP-SGD~\citep{bassily2019private,ma2022dimension}, ERM solution~\citep{feldman2016generalization,aubin2020generalization} and so on. 
One of the most famous results is that one-pass SGD can achieve an optimal error rate of $\cO(1/\sqrt{n})$ in convex optimization, even in the presence of non-smooth loss functions~\citep{nemirovskij1983problem}.

However, in the case of convex regimes with realizability constraints, existing analyses typically focus only on upper bounds and lack corresponding lower bounds~\citep{lei2020fine,nikolakakis2022beyond,schliserman2022stability,taheri2023generalization}. 
Realizability is closely related to label noise, which can have a substantial impact on generalization performance~\citep{song2019does,harutyunyan2020improving,DBLP:conf/iclr/TengMY22,wen2022realistic}. 

From a perspective of lower bounds,
\cite{amir2021sgd} show that no less than $\bigOmega{1/\epsilon^4}$ steps is needed to generalize for GD. \cite{sekhari2021sgd} further indicate that GD suffers from a $\bigOmega{1/n^{5/12}}$ sample complexity, which is slower than the well-established bound $\bigOmega{1/\sqrt{n}}$ for SGD \citep{nemirovskij1983problem}.
Besides the upper/lower bound mentioned above, a line of lower bounds in generalization analysis typically focuses on the failure of techniques. 
For instance, despite the optimal rate of $\cO(1/\sqrt{n})$ in convex optimization, uniform convergence only returns a lower bound of $\Omega(\sqrt{d/n})$~\citep{shalev2010learnability,feldman2016generalization}, leading to a constant lower bound in overparameterized regimes. 
A line of works further illustrate the inherent weakness of uniform convergence~\citep{nagarajan2019uniform,glasgow2022max}.
Regarding stability-based bounds, \citet{bassily2020stability} presents a lower bound under non-smooth convex losses. 


To bridge the gap between lower and upper bound, a fast rate upper bound in order $O(1/n)$ is required. 
One of the most well-known fast-rate bound is local Rademacher complexity, which works well under low-noise regimes~\citep{bartlett2005local}. 
However, it typically relies on a specific function class and may not be directly applied into the general convex optimization regimes~\citep{steinwart2007fast,srebro2010optimistic,zhou2021optimistic}. 
Alternatively, stability-based analyses have shown promise and work well in convex optimization regimes, which provide fast-rate generelization bound~\citep{bousquet2002stability,hardt2016train,feldman2019high,zhang2022stability}.
In addition to these bounds, one can also derive fast rate bound for finite-dimensional cases~\citep{lee1996importance,bousquet2002concentration}, aggregation~\citep{tsybakov2004optimal,chesneau2009adapting,dalalyan2018exponentially}, PAC-Bayesian and information-based analysis~\citep{yang2019fast,grunwald2021pac}.



% To close the gap between upper and lower bound in realizable regimes, a fast rate upper bound is required.
% Local Rademacher complexity is among the most famous fast-rate bound, which \citep{}. 
% However, the local Rademacher complexity usually relies on 
% Recently, stability stands out due to 

% It is known that one-pass SGD can attain the optimal error $\cO(1/\sqrt{n})$ in convex optimization,  with smoothness~\citep{nemirovskij1983problem}. 
% Researchers also focus on the multi-pass SGD regimes using 
% Under convex optimization problems, \citet argues that $O(1/\sqrt{n})$ convergence rate is optimal for one-pass SGD \jiaye{I did not check the details of this paper because it is hard to download, it is in paper \url{https://proceedings.neurips.cc/paper/2020/file/2e2c4bf7ceaa4712a72dd5ee136dc9a8-Paper.pdf}}. 
% \jiaye{It might be in non-smooth convex regimes. }
% For multi-pass SGD, Hardit et al, any new results?



% \textbf{Other types of lower bounds.}
% A line of works makes progress on the generalization lower bounds under different regimes. 
% For uniform-convergence bound, a lower bound of $\Omega(\sqrt{d/n})$ is derived in  SCO settings~\citep{DBLP:journals/jmlr/Shalev-ShwartzSSS10,feldman2016generalization}, leading to a constant lower bound in overparameterized regimes.
% Besides, \citet{} further show the inherent weakness of uniform convergence. 
% For stability-based bounds, \citet{bassily2020stability} presents the lower bound under non-smooth convex losses. 
% Different from these approaches, we derive problem-intrinsic lower bounds which do not rely on any techniques. 

% \textbf{Other types of lower bounds.}
% Analyzing lower bounds helps understand generalization. 
% In convex settings, stability and uniform convergence both fails.
% In general settings, there are still lower bounds.

% \textbf{Fast-rate upper bounds.}
% Since our ultimate goal is to close the gap between upper and lower bound, a fast rate upper bound is required.
% Local Rademacher complexity is among the most famous fast-rate bound, which \citep{}. 
% However, the local Rademacher complexity usually relies on some specific function class and therefore cannot directly apply into the SCO regimes. 
% Recently, stability stands out due to 

% Under convex training regimes, the stability-based bound usually 

% \textbf{Realizable settings.}
% Despite the equivalence of realizable and agnostic learnability under the PAC-learning framework~\citep{vapnik1974theory,DBLP:journals/jacm/BlumerEHW89,DBLP:journals/iandc/Haussler92,DBLP:conf/colt/HopkinsKLM22},
% researchers find that the realistic generalization performances can be pretty different where the label noise matters~\citep{}.
% The key to the gap comes from 


\section{Conclusion}
In this work, we focus on generalization bounds under the smooth SCO setting. In particular, we investigate the relationship between sample complexity and iteration $T$ under three settings: (1) non-realizable, (2) realizable with $T = \bigO{n}$, and (3) realizable with $T =  \bigOmega{n}$. We provide novel excess population risk lower bounds for Gradient Descent and Stochastic Gradient Descent methods in all the three cases. For the first two cases, our lower bounds match the corresponding upper bounds and certificate the optimal sample complexity. Nevertheless, under the realizable case with $T = \bigO{n}$, we observe a gap between existing sample complexity upper bounds $\bigO{1/n^{2/3}}$ and lower bounds $\bigOmega{1/n}$ derived from our results. We conjecture that this gap can be closed by improving the upper bound under the long time horizon regime and provide evidence in the particular one-dimensional and regressional setting to support our hypothesis.