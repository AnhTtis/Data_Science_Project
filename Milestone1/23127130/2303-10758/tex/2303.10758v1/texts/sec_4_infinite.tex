\section{Infinite Time Horizon Instances} \label{sec: infinite}
In Section~\ref{sec: lb}, we provide lower bounds for both realizable and non-realizable cases. For non-realizable losses, both the upper and lower bounds are consistent within different time horizon. The result differs for realizable cases: for small time horizon $T = \bigO{n}$, our lower bounds match the upper bound in sample complexity $\Theta(1/n)$; nevertheless, for large time horizon $T = \bigOmega{n}$, there exists a gap between the sample complexity lower bounds $\bigO{1/n}$ and upper bounds $\bigOmega{1/n}$. It is natural to ask
\begin{center}
    \it Can we close the gap between upper and lower bounds\\ for realizable SCO when $T$ goes to infinity?
\end{center}
We conjecture that the above problem can be tackled by proving GD and SGD can achieve $\bigO{1/n}$ even for large time horizon $T = \bigOmega{n}$. In the section, we provide evidences to support the conjecture: we consider the example of one-dimensional function and linear regression. On both examples, $\Theta(1/n)$ is achieved for GD and SGD when $T = \bigOmega{n}$.
\subsection{One-dimensional feasibility}
 We showcase the reasoning behind our conjecture by providing a first evidence in dimension one: under $d = 1$, we close the gap between upper and lower bound by establishing $\Theta(1/n)$ sample complexity in the rest part of the subsection.
\paragraph{Upper bounds.} We begin by presenting Lemma~\ref{lemma: dim-1-ub-sgd}, which establishes an upper bound for SGD based on the result of \citet{lei2020fine}.
\begin{lemma} \label{lemma: dim-1-ub-sgd}
In dimension one, if $f(w,z)$ is convex, $1$-smooth and realizable with $z \sim D$, then for every $\eta = \Theta(1)$, there exists $T_0 = \Theta(n)$ such that for $T \geq T_0$, the output $w_{\sgd}$ of SGD satisfies
\begin{equation*}
    \bbE[ F(w_{\sgd})] - F(w^*) = \bigO{\frac{1}{n}}.
\end{equation*} 
\end{lemma}
\begin{proof}
From Theorem~4 in \citet{lei2020fine}, it holds that for realizable cases (we rescale it to $f(w^*, z) = 0$ for each $z$) with step size $\eta =  \Theta(1)$, it holds that
\begin{equation}
    \bbE[F(w_{\sgd})] = \bigO{\frac{1}{T_0} + \frac{1 + T_0/n}{n}}.
\end{equation}
Therefore, for $T_0= \Theta(n)$, it holds that
\begin{equation*}
    \bbE[F(w_{\sgd})] = \bigO{\nfrac{1}{n}}.
\end{equation*}
For SGD, the iteration formulates the iterate 
\begin{equation*}
    w_{t+1} = w_t - \eta \nabla f(w_t, z_i).
\end{equation*}
Under the realizable and convex assumption, for any $z_i \in \cZ$, the iteration becomes
\begin{equation*}
    w_{t+1} - w^* = (1 - \eta \nabla^2 f(\xi, z_i)) (w_t - w^*),
\end{equation*}
using mean value theorem, where $\xi$ is a point between $w_t$ and $w^*$. This indicates that the distance $w_t - w^*$ shrinks in each step for any $z_i \in \cZ$. Due to the convexity of $F$, it holds that 
$F(w_{t+1}) \leq F(w_t)$. In summary, for any $T \geq T_0$, it holds that 
\begin{equation*}
    \bbE[F(w_{\sgd})] - F(w^*) = \bigO{\nfrac{1}{n}}.
\end{equation*} 
\end{proof}
A similar result can be established for GD, as in the next lemma. Its proof is postponed to Appendix~\ref{appendix: infinite}.
\begin{lemma} \label{lemma: dim-1-ub-gd}
In dimension one, if $f(w,z)$ is convex, $1$-smooth and realizable with $z \sim D$, then for every $\eta = \Theta(1)$, there exists $T_0 = \Theta(n)$ such that for $T \geq T_0$, the output $w_{\gd}$ of GD satisfies
\begin{equation*}
    \bbE[ F(w_{\gd})] - F(w^*) = \bigO{\frac{1}{n}}.
\end{equation*} 
\end{lemma}
\paragraph{Lower bound.}
Besides upper bound, we present a hard instance in dimension one case, which leads to sample complexity lower bound of $\bigOmega{1/n}$.
\begin{lemma} \label{lemma: dim-1-lb}
    In dimension one, for every $\eta > 0$, $T \geq 1$, there exists a convex, $1$-smooth and realizable $f(w, z): \bbR \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, %with high probability $\Theta(1)$,
    it holds that for the output of any gradient-based algorithm $\cA[S]$
    \begin{align*}
        \bbE[F(\cA[S])] - F(w^*) = \bigOmega{\nfrac{1}{n}}.  
    \end{align*}  
\end{lemma}
\begin{proof}
    We consider the following instance
    \begin{equation*}
        f(w, z) = \begin{cases} \frac{1}{2}w^2, \qquad z = 1, \\0, \qquad\quad\ z = 0, \end{cases}
    \end{equation*}
    where $z \sim \text{Bern}(1/n)$. The population risk is
    \begin{align*}
        F(w) = \bbE_{z\sim\text{Bern}(1/n)}[f(w,z)] = \frac{1}{2n} w^2,
    \end{align*}
    which achieves minimum at $w^* = 0$. It is easy to reckon that $f(w,z)$ is smooth, convex and realizable. Now consider any dataset $S$ of $n$ samples. With probability
    $ \left(1 - \nfrac{1}{n}\right)^n = \Theta(1)$, dataset $S$ of size $n$ does not observe stochastic function $f(w,z=1) = w^2/2$. So with initialization $w_0 = 1$, $w_t$ remains unchanged for any gradient-base algorithm and any $t \in [T]$. Then we have the following lower bound:
    \begin{align*}
        \bbE[F(\cA[S])] - F(w^*) =  \frac{1}{2n} = \bigOmega{\frac{1}{n}},
    \end{align*}
    which is the desired result.
\end{proof}
The lemma holds for any gradient-based algorithm and provides a sample complexity lower bound of $\bigOmega{1/n}$ for any $T \geq 1$. This matches the upper bound of GD and SGD in Lemma~\ref{lemma: dim-1-ub-gd} and Lemma~\ref{lemma: dim-1-ub-sgd} under large time horizon regime $T = \bigOmega{1/n}$. Unfortunately, we cannot employ the same technique to extend the result to high dimensional case. However, we show that the gap can be closed for a special case in the high-dimensional regime in next subsection.

\subsection{Linear Regression}
In this subsection, we demonstrate that when $T = \bigOmega{n}$, $\Theta(1/n)$ can be achieved on \emph{linear regression} problem, whatever underparameterized ($ d < n$) or overparameterized ($ d \geq n$). 
In realizable linear regression problems, the $i$-th sample $z_i = (x_i, y_i)$ in dataset $S= \{z_1, \dots, z_n \}$ satisfies that $y_i = x_i^\top w^*$ and $x_i$ is i.i.d. drawn from an unknown distribution. Under the linear predictor $x_i^\top w$, the loss term is defined as $f(w,z) = (y_i - x_i^\top w)^2$. Under this regime, a bounded feature $\|x\| \leq 1$ suffices to guarantee that $f(w, z)$ is convex, $1$-smooth, and realizable. 
In this case, the upper bound would be $\bigO{1/n}$ and the lower bound would be ${\cO}(\log^3n/n)$, which is optimal up to a $\log$-factor. 

\paragraph{Upper bounds.}
We begin by presenting Lemma~\ref{lemma: ub-regression}, which establishes an upper bound using local Rademacher Complexity.
\begin{lemma}[From \citet{srebro2010optimistic}]
\label{lemma: ub-regression}
    In the realizable linear regression cases, for every $\eta > 0$ and $T\geq 1$, if the feature $x_i$ is bounded, it holds that for the output of SGD
\begin{align*}
    & \bbE[F(w_{\sgd})] - F(w^*) = \bigO{\frac{1}{\eta T} + \frac{\log^3 n}{n} }, 
    \end{align*}
    and also the output for GD
    \begin{align*}
    & \bbE[F(w_{\gd})] - F(w^*) = \bigO{\frac{1}{\eta T} + \frac{\log^3 n}{n}}.
\end{align*}
\end{lemma}
\begin{proof}
    One could directly apply Theorem~1 in \citep{srebro2010optimistic}.
    Specifically, we plug in the realizability assumption and the Rademacher complexity of linear function class, which is in order $\bigO{1/n}$ in bounded norm cases. 
\end{proof}
%One can prove similar results for GD in Lemma~\ref{lem: upper bound for linear regression GD}.\jiaye{Do we need to merge the two lemmas?}
%\begin{lemma}[From \citet{srebro2010optimistic}]
%\label{lem: upper bound for linear regression GD}
%    For realizable linear regression cases, if the feature $x$ is bounded, then for every $\eta$, it holds that 
%\begin{equation*}
%\end{equation*}
%\end{lemma}
\paragraph{Lower bounds.}
We now proceed to a corresponding lower bound for regressional problems.
\begin{lemma}[Lower bound in linear regression.]
\label{lemma: lb-regression}
    For any $\eta > 0$, $T \geq 1$, there exists a linear regressional instance such that, if the feature $x_i \sim D$ is bounded, it holds that for the output of any gradient-based algorithm $\cA[S]$
\begin{equation*}
    \bbE[F(\cA[S])] - F(w^*) = \bigOmega{\frac{1}{n}}.
\end{equation*}
\end{lemma}
\begin{proof}
    We consider the following regression problem: we generate $x_i = e_i \in \bbR^{2n}$, where $i \sim \text{Unif}([2n])$. The ground truth weight is set to $w^* = 0$ and $y_i = x_i^\top w^*$. Hence, the loss function $f(w, z): \bbR^{2n} \times \cZ \to \bbR$ is 
    \begin{align*}
        f(w, z = i) = w(i)^2, \qquad z \sim \text{Unif}([2n]).
    \end{align*}
    From Lemma~\ref{lemma: lb-sample} in Appendix~\ref{appendix: lb-sample}, we know that for any gradient-based algorithm, the lower bound is $\bigOmega{1/n}$.
\end{proof}
Lemma~\ref{lemma: ub-regression} and Lemma~\ref{lemma: lb-regression} establish a sample complexity rate of $\Theta(1/n)$ for linear regression when $T$ grows large. Our evidence on both dimension one case and regression suggests the gap in the regime $T = \bigOmega{n}$ might be closed by improving the upper bounds of excess risk or sample complexity. We hope our analysis can motivate future exploration into the topic.