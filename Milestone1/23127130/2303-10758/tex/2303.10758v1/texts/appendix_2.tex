\section{Missing proofs from Section~\ref{sec: t_equal_n} and Section~\ref{sec: t_larger_than_n} } \label{appendix: t_equal_n}

\subsection{Proof of Theorem~\ref{thm: lb-1}} \label{appendix: lb-gd-tn}
The proof of GD is immediate from combining the lower bound constructions in Lemma~\ref{lemma: lb-sample}, Lemma~\ref{lemma: lb-suboptimality}, and most importantly, Lemma~\ref{lemma: lb-gd-tn} to be stated below. In precise, Lemma~\ref{lemma: lb-gd-tn} is the core part of our result and gives a lower bound of $\bigOmega{\eta T/n}$ when $T = \bigO{n}$. We postpone its proof to Appenidx~\ref{appendix: lb-gd}. The proof of the rest two lemmas can be found in Appendix~\ref{appendix: lb-sample}, \ref{appendix: lb-suboptimality}.
\begin{lemma} \label{lemma: lb-gd-tn}
For every $\eta > 0$, $T \geq 1$, if $T =  \bigO{n}$, then there exists a convex, $1$-smooth and realizable $f(w, z): \bbR^d \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with initialization $\| w_0 - w^* \| = \bigO{1}$, the output $w_{\gd}$ for GD satisfies
\begin{align*}
 \bbE[F(w_{\gd})] - F(w^*) = \bigOmega{\frac{\eta T}{n^2}}.  
\end{align*}
Similarly, if $T =  \bigOmega{n}$, then 
the output $w_{\gd}$ for GD satisfies
\begin{align*}
 \bbE[F(w_{\gd})] - F(w^*) = \bigOmega{\frac{1}{\eta T}}.  
\end{align*}
\end{lemma}
Similar to the proof of GD, the result on SGD is also obtained by combining lower bound construction in following lemma and Lemma~\ref{lemma: lb-sample}, \ref{lemma: lb-suboptimality} in Appendix~\ref{appendix: lb-sample}, \ref{appendix: lb-suboptimality}. Lemma~\ref{lemma: lb-sgd-tn} establishes a lower bound of $\bigOmega{\eta T/n}$ when $T = \bigO{n}$. Its proof can be found in Appendix~\ref{appendix: lb-sgd}.
\begin{lemma} \label{lemma: lb-sgd-tn}
For every $\eta > 0$, $T \geq 1$, if $T =  \bigO{n}$, then there exists a convex, $2$-smooth and realizable $f(w, z): \bbR^d \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with initialization $\| w_0 - w^* \| = \bigO{1}$, the output $w_{\sgd}$ for SGD satisfies
\begin{align*}
 \bbE[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{\eta T}{n^2}}.  
\end{align*}
Similarly, if $T =  \bigOmega{n}$, then 
the output $w_{\sgd}$ for SGD satisfies
\begin{align*}
 \bbE[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{1}{\eta T}}.  
\end{align*}
\end{lemma}


\subsection{Proof of Theorem~\ref{thm: lb-2}} \label{appendix: lb-gd-t}
Similar to the proof of Theorem~\ref{thm: lb-1}, the proof of GD is immediate from combining the lower bound constructions in Lemma~\ref{lemma: lb-sample}, Lemma~\ref{lemma: lb-suboptimality}, and Lemma~\ref{lemma: lb-gd-tn}. In particular, Lemma~\ref{lemma: lb-gd-tn} gives a lower bound of $\bigOmega{1/{\eta T}}$ when $T = \bigOmega{n}$. 

Concurrently, the SGD proof is obtained from combining the lower bounds in Lemma~\ref{lemma: lb-sample}, Lemma~\ref{lemma: lb-suboptimality}, and Lemma~\ref{lemma: lb-sgd-tn}. In particular, Lemma~\ref{lemma: lb-sgd-tn} gives a lower bound of $\bigOmega{1/{\eta T}}$ when $T = \bigOmega{n}$. 

\subsection{Proof of Lemma~\ref{lemma: lb-gd-tn}} \label{appendix: lb-gd}
\begin{lemma}[Restated Lemma~\ref{lemma: lb-gd-tn}]
For every $\eta > 0$, $T \geq 1$, if $T =  \bigO{n}$, then there exists a convex, $1$-smooth and realizable $f(w, z): \bbR^d \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with initialization $\| w_0 - w^* \| = \bigO{1}$, the output $w_{\gd}$ for GD satisfies
\begin{align*}
 \bbE[F(w_{\gd})] - F(w^*) = \bigOmega{\frac{\eta T}{n^2}}.  
\end{align*}
Similarly, if $T =  \bigOmega{n}$, then 
the output $w_{\gd}$ for GD satisfies
\begin{align*}
 \bbE[F(w_{\gd})] - F(w^*) = \bigOmega{\frac{1}{\eta T}}.  
\end{align*}
\end{lemma}

\begin{proof}
    The hard instance $f(w,z)$ employed in the proof is constructed by \emph{glueing} parallelly in dimension multiple copies of basic instance $g(w,z): \bbR^{n+1} \times \cZ \to \bbR$. So we start with the construction of $g(w, z)$ and slightly abuse the notation of $w, z$ to denote the parameter and random variable of $g$. $w$ is defined as a tuple of two variables, i.e. $w = (x, y)$, where $x \in \bbR$ and $y \in \bbR^n$. Instance $g: \bbR^{w+1} \times \cZ \to \bbR$ is defined as
    \begin{equation} \label{eq: lb-instance}
        g(w, z = i) = \frac{\alpha}{2} x^2 + \frac{1}{2} \big(y(i)\big)^2  - \sqrt{\alpha} x \cdot y(i) = \frac{1}{2} \Big(\sqrt{\alpha}x - y(i)\Big)^2
    \end{equation}
    where $y(i)$ is the $i$-th coordinate of $y$ and $z \sim \text{Unif}([n])$.  Parameter $\alpha$ is set to $C/(\eta T)$ where $C$ is a constant. When condition $\eta T = \Omega(1)$ holds, it is easy to check that $g(w,z)$ is $\bigO{1}$-smooth and convex. The population risk $G$ is 
    \begin{align*}
        G(w) = \bbE_{z\sim\text{Unif}([n])}[g(w,z)] = \frac{\alpha}{2} x^2 + \frac{1}{2n} \|y\|^2 - \frac{\sqrt{\alpha}}{n}x \cdot \vone^\top y = \frac{1}{2n} \left\| y - \sqrt{\alpha} x \cdot \vone \right\|^2,
    \end{align*}
    which attains minimum at $(x^*, y^*) = (0, 0)$. So $g(w,z)$ satisfies the realizable condition.
    
    Before we proceed to describe the method of obtaining $f(w,z)$, we define the following probability event: given dataset $S = \{ z_1, \dots, z_n\}$, for one copy of $g(w, z)$, event $\cE$ is defined as 
    \begin{equation*}
        \cE = \left\{ z_i = i \text{ for any } \vpi(i) \in [n] \right\}
    \end{equation*}
    where $\vpi:[n] \to [n]$ is any permutation on $[n]$. Intuitively, when $\cE$ happens, in dataset $S$, each coordinates of $y$ is selected only for once. The probability of $\cE$ is calculated from the without-replacement sampling: 
    \begin{align*}
        p = \pr[\text{Event } \cE_1 \text{ happens}] = 1 \cdot \frac{n-1}{n} \cdots \cdot  \frac{1}{n} = \frac{n!}{n^n} = \Theta(\sqrt{n}\cdot e^{-n})
    \end{align*}
    where the last step is from Stirling approximation.

    Clearly for dataset $S$ and single copy $g(w,z)$, the probability of  events $\cE$ is small. Nevertheless, if we consider $m = \Theta(1/p)$ copies of such $g(w,z)$, then with high probability there exists at least one copy such event happen. Denote $R$ to be the random variable counting such copies out of total $m$ copies. Using second moment method, we upper bound the following probability: 
    \begin{equation}
        \pr[R > 0] \geq \frac{(\bbE[R])^2}{\bbE[R^2]} = \frac{m^2p^2}{mp(1-p)} \geq \frac{mp}{2} = \frac{1}{2}.
    \end{equation}

    This allows us to define $f: \bbR^{(n+1) \times m} \times \cZ^m \to \bbR$ as the sum of $m$ independent copies of $g(w,z)$:
    \begin{equation}
        f(w, z) = \sum_{i=1}^m g(w^{(i)}, z^{(i)}),
    \end{equation}
    where $w = ( w^{(1)}, \cdots, w^{(m)})$, $z = ( z^{(1)}, \cdots, z^{(m)})$, and each $w^{(i)} \in \bbR^{n+1}$, $z^{(j)} \sim \text{Unif}([n])$. This is to say, we define $f$ as the summation over $m$ copies of $g(w^{(j)},z^{(j)})$ and the variable $w^{(j)}$ are put parallel in coordinates to form a large vector $w$. The total dimension of $f$ is then $\Theta(\sqrt{n} \cdot e^{n} )$ and it is easy to check $f$ is still $1$-smooth, convex and realizable. Then with probability at least $\Omega\left(\nfrac{1}{2}\right)$, there exists $j \in [m]$ such that $\cE$ happens for $j$-copy. Without the loss of generalization, we set $\vpi(i) = i$ for any $i \in [n]$. This suggests in dataset $S$, we have
    \begin{equation}
        g(w^{(j)}, z^{(j)}_i) = \frac{\alpha}{2} (x^{(j)})^2 + \frac{1}{2} \|y^{(j)}\|^2 - \frac{x^{(j)}\sqrt{\alpha}}{n} y^{(j)}(i), \qquad \forall i \in [n].
    \end{equation}

    Next, we consider GD trajectory on the instance $f$ and dataset $S$, with the following initialization 
    \begin{align*}
        x_0^{(j)} = 1, \quad x_0^{(i)} = 0, \quad \forall i \ne j \in [m], \quad \text{and} \quad y_0^{(i)} = 0, \quad \forall i \in [m]. 
    \end{align*}
    The initialization satisfies $\| w_0 - w^* \| = \bigO{1}$. This allows us to focus on the $j$-th copy, i.e. $w^{(j)}$ and $z^{(j)}$. For simplicity, we suppress the upscript of $j$. From the above construction, GD formulates the following update on $j$-th copy:
    \begin{align*}
        w_{t+1} = w_t - \frac{\eta}{n} \sum_{i=1}^n \nabla_w g(w_t, z_i)
    \end{align*}
    with initialization $x_0 = 1$, $y_0 = 0$. The stochastic gradient is computed as 
    \begin{align}
        \nabla_x g(w, z_i) = \alpha x - \sqrt{\alpha}y(i), \qquad \nabla_{y} g(w, z_i) = (y(i) - \sqrt{\alpha}x) \cdot \ve_i.
    \end{align}
    Since all coordinates in $y$ are equivalent in the construction, we suppress the index of $i$ and write $y_t = y_t(i)$ for any $i \in [n]$, $t \in [T]$. Then it formulates
    \begin{align*}
        & x_{t+1} = x_t - \eta \alpha x_t + \frac{\eta \sqrt{\alpha}}{n}\sum_{i=1}^ny_t(i) = (1-\alpha \eta)x_t + \eta \sqrt{\alpha}y_t, \\
        & y_{t+1} = y_t - \frac{\eta}{n}y_t + \frac{\eta\sqrt{\alpha}}{n}x_t = \left(1 - \frac{\eta}{n} \right)y_t + \frac{\eta\sqrt{\alpha}}{n}x_t.
    \end{align*}
    We give an upper bound for $x_t$ and $y_t$ by the following induction. If condition
    \begin{equation} \label{eq: induction-GD-2}
        x_{t} \leq 1, \qquad y_t \leq \sqrt{\alpha}
    \end{equation} 
    holds for $t$, then the above condition also holds for $t+1$:
    \begin{align*}
        x_{t+1} & \leq (1 - \alpha \eta) + \eta\sqrt{\alpha} \cdot \sqrt{\alpha} = 1 - \frac{\eta}{\eta T} + \frac{\eta }{\eta T}  \leq 1, \\
        y_{t+1} & \leq \left(1 - \frac{\eta}{n} \right) \sqrt{\alpha} + \eta \frac{\sqrt{\alpha}}{n} = \sqrt{\alpha}.
    \end{align*}
    Then by induction we conclude that \eqref{eq: induction-GD-2} is true.
    For any $t \in [T]$, the lower bound for $x_t$ is much simpler to compute under our choice of parameter $\alpha = 1/(\eta T)$:
    \begin{align*}
        x_{t} & \geq (1 - \alpha \eta)  x_{t-1} \geq (1 - \alpha \eta )^tx_0 = e^{-Ct/T} \geq e^{-C}.
    \end{align*}
    Hence $\hat{x}_T = \frac{1}{T} \sum_{t=1}^T x_t = \Theta(1)$. This then allows us to lower bound $y$ at iteration $t \in [T]$:
    \begin{align*}
        y_t & \geq \left(1-\frac{\eta}{n}\right) y_{t-1} + \frac{\eta\sqrt{\alpha}}{e^Cn} \\
        & \geq \frac{\eta\sqrt{\alpha}}{e^Cn} \cdot \left( 1 + (1-\eta/n) + \cdots (1-\eta/n)^{t-1} \right) \\
        & \geq \frac{\eta\sqrt{\alpha}}{e^Cn} \cdot \frac{1 - (1-\eta/n)^t}{1 - (1 - \eta/n)} .
     \end{align*}
    Now, we discuss two cases: $T = \bigO{n}$ and $T = \bigOmega{n}$.
    \paragraph{Case $T = \bigO{n}$.} We decompose $t = n \cdot \tfrac{t}{n}$ and obtain
    \begin{align*}
        y_t & \geq \frac{\eta\sqrt{\alpha}}{e^Cn} \cdot \frac{1 - (1-\eta/n)^t}{1 - (1 - \eta/n)} = \frac{\eta\sqrt{\alpha}}{e^Cn} \cdot \frac{1 - (1-\eta/n)^{\tfrac{t}{n} \cdot n}}{1 - (1 - \eta/n)} \\
        & \overset{\text{(A)}}{\geq} \frac{\eta \sqrt{\alpha}}{e^C}\left( \frac{t}{n} - \frac{\eta t^2}{2n^2}\right)  \overset{\text{(B)}}{=} \frac{\eta t \sqrt{\alpha}}{2e^Cn} = \sqrt{\frac{\eta}{CT}} \cdot \frac{t}{2e^Cn}%= \Omega\left( \frac{\sqrt{\eta t}}{n}\right).
    \end{align*}
    where $\text{(A)}$ is due to Taylor expansion, $\text{(B)}$ is due to the condition $\eta t \leq \eta T \leq T = \bigO{n}$ and $\alpha = C/(\eta T)$. We then calculate the average output
    \begin{align*}
        \hat{y}_T = \frac{1}{T} \sum_{t=1}^T y_t = \frac{1}{T} \sum_{t=1}^T \sqrt{\frac{\eta}{CT}} \cdot \frac{t}{2e^Cn} \geq \frac{1}{4e^C} \cdot \sqrt{\frac{\eta T}{C}}.
    \end{align*}
    We return to the original $f(w,z)$ by inserting the above analysis on the $j$-th copy:
    \begin{align*}
        \bbE[F(w_{\gd})] \geq \bbE[G(w^{(j)}_{\gd})] \geq \frac{1}{n} \sum_{i=1}^n \left(\frac{x^{(j)}_{\gd}}{\sqrt{\eta T}} - y^{(j)}_{\gd}(i)\right)^2 \geq \bigOmega{\max\left\{ \frac{1}{\eta T,} \ \frac{\eta T}{n^2 }\right\}} = \bigOmega{\frac{\eta T}{n^2 }}
    \end{align*}
    where the last inequality is due to the fact $e^{-C} \leq x_{\gd} \leq 1$. We can always choose a proper $C$ such that the difference is non-vanishing.
    \paragraph{Case $T = \bigOmega{n}$.} We can directly lower bound $y_t$ as
    \begin{align*}
        y_t \geq \frac{\eta \sqrt{\alpha}}{e^C n} \cdot \frac{n}{2\eta} = \frac{1}{2e^C\sqrt{C \eta T}} = \bigOmega{\frac{1}{\sqrt{\eta T}}}
    \end{align*}
    since $(1 - \eta/n)^t \leq 1/2$ when $\eta = \bigO{1}$ and $T = \bigOmega{n}$. We then calculate the average output
    \begin{align*}
        \hat{y}_T = \frac{1}{T} \sum_{t=1}^T y_t = \frac{1}{T} \sum_{t=1}^T \bigOmega{\frac{1}{\sqrt{\eta T}}} \geq \bigOmega{\frac{1}{\sqrt{\eta T}}}.
    \end{align*}
    Similarly, we return to the original $f(w,z)$ by inserting the above analysis on the $j$-th copy and obtain a non-vanishing lower bound by choosing proper $C$:
    \begin{align*}
        \bbE[F(w_{\gd})] \geq \bbE[G(w^{(j)}_{\gd})] \geq \frac{1}{n} \sum_{i=1}^n \left(\frac{x^{(j)}_{\gd}}{\sqrt{\eta T}} - y^{(j)}_{\gd}(i)\right)^2 \geq \bigOmega{\frac{1}{\eta T}}.
    \end{align*}
    This completes our proof.
\end{proof}

\subsection{Proof of Lemma~\ref{lemma: lb-sgd-tn}} \label{appendix: lb-sgd}
\begin{lemma}[Restated Lemma~\ref{lemma: lb-sgd-tn}]
For every $\eta > 0$, $T \geq 1$, if $T =  \bigO{n}$, then there exists a convex, $1$-smooth and realizable $f(w, z): \bbR^d \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with initialization $\| w_0 - w^* \| = \bigO{1}$, the output $w_{\sgd}$ for SGD satisfies
\begin{align*}
 \bbE[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{\eta T}{n^2}}.  
\end{align*}
Similarly, if $T =  \bigOmega{n}$, then 
the output $w_{\sgd}$ for SGD satisfies
\begin{align*}
 \bbE[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{1}{\eta T}}.  
\end{align*}
\end{lemma}

\begin{proof}
    We utilize the similar strategy employed in Lemma~\ref{lemma: lb-gd-tn}: we glue multiple copies of basic construction $g: \bbR^{n+1} \times \cZ \to \bbR$ defined in Eq.~\eqref{eq: lb-instance} with $z \sim \text{Unif}([n])$. With probability $\bigOmega{1}$, event $\cE$ happens for at least one copy, i.e. each coordinates We label as the $j$-copy and consider the trajectory of $j$-th copy only. Without the loss of generalization, we set $\vpi(i) = i$ for any $i \in [n]$. This suggests in dataset $S = \{z_1, \dots, z_n\}$, we have
    \begin{equation}
        g(w^{(j)}, z^{(j)}_i) = \frac{\alpha}{2} (x^{(j)})^2 + \frac{1}{2} \|y^{(j)}\|^2 - \frac{x^{(j)}\sqrt{\alpha}}{n} y^{(j)}(i), \qquad \forall i \in [n].
    \end{equation}
    Again, for simplicity, we suppress the upscript of $j$. SGD formulates the update:
    \begin{align*}
        w_{t+1} = w_{t+1} - \eta g(w_t, z_{i_t}) 
    \end{align*}
    where $z_{i_t} \sim \text{Unif}([n])$. The initialization is $x_0 = 1$, $y_0 = 0$. Based on the current value of $w_t$, under expectation, we have
    \begin{align*}
        \bbE[w_{t+1}] = w_t - \frac{\eta}{n} \sum_{i=1}^n \nabla_w g(w_t, z_i).
    \end{align*}
    We write the update of $\bbE[x_t]$ and $\bbE[y_t]$ by plugging stochastic gradients: it easy to see all coordinates in $\bbE[y_t]$ are equivalent, we suppress the index of $i$ and write $y_t = y_t(i)$ for any $i \in [n]$, $t \in [T]$. Then it formulates
    \begin{align*}
        & \bbE[x_{t+1}] = x_t - \eta \alpha x_t + \frac{\eta \sqrt{\alpha}}{n}\sum_{i=1}^ny_t(i) = (1-\alpha \eta)x_t + \eta \sqrt{\alpha}y_t, \\
        & \bbE[y_{t+1}] = y_t - \frac{\eta}{n}y_t + \frac{\eta\sqrt{\alpha}}{n}x_t = \left(1 - \frac{\eta}{n} \right)y_t + \frac{\eta\sqrt{\alpha}}{n}x_t.
    \end{align*}
    We give an upper bound for $\bbE[x_t]$ and $\bbE[y_t]$ by the following induction. If condition
    \begin{equation} \label{eq: induction-SGD-2}
        \bbE[x_{t}] \leq 1, \qquad \bbE[y_t] \leq \sqrt{\alpha}
    \end{equation} 
    holds for $t$, then the above condition also holds for $t+1$:
    \begin{align*}
        \bbE[x_{t+1}] & \leq (1 - \alpha \eta) + \eta\sqrt{\alpha} \cdot \sqrt{\alpha} = 1 - \frac{\eta}{\eta T} + \frac{\eta }{\eta T}  \leq 1, \\
        \bbE[y_{t+1}] & \leq \left(1 - \frac{\eta}{n} \right) \sqrt{\alpha} + \eta \frac{\sqrt{\alpha}}{n} = \sqrt{\alpha}.
    \end{align*}
    Then by induction we conclude that \eqref{eq: induction-SGD-2} is true. For any $t \in [T]$, the lower bound for $x_t$ is much simpler to compute under our choice of parameter $\alpha = C/(\eta T)$:
    \begin{align*}
        \bbE[x_{t+1}] & \geq (1 - \alpha \eta)  x_{t} \geq (1 - \alpha \eta )^tx_0 = e^{-t/T} \geq e^{-C}.
    \end{align*}
    Hence $\bbE[\hat{x}_T] = \frac{x_1}{T} + \frac{1}{T} \sum_{t=2}^T \bbE[x_t|w_{t-1}] = \Theta(1)$. This then allows us to lower bound $y$ at iteration $t \in [T]$:
    \begin{align*}
        \bbE[y_{t+1}] & \geq \left(1-\frac{\eta}{n}\right) y_{t} + \frac{\eta\sqrt{\alpha}}{e^Cn} \\
        & \geq \frac{\eta\sqrt{\alpha}}{e^Cn} \cdot \left( 1 + (1-\eta/n) + \cdots (1-\eta/n)^{t} \right) \\
        & \geq \frac{\eta\sqrt{\alpha}}{e^Cn} \cdot \frac{1 - (1-\eta/n)^{t+1}}{1 - (1 - \eta/n)} .
     \end{align*}
     Now, we discuss two cases: $T = \bigO{n}$ and $T = \bigOmega{n}$.
    \paragraph{Case $T = \bigO{n}$.} We decompose $t = n \cdot \tfrac{t}{n}$ and obtain
    \begin{align*}
        \bbE[y_t] & \geq \frac{\eta\sqrt{\alpha}}{e^Cn} \cdot \frac{1 - (1-\eta/n)^t}{1 - (1 - \eta/n)} = \frac{\eta\sqrt{\alpha}}{e^Cn} \cdot \frac{1 - (1-\eta/n)^{\tfrac{t}{n} \cdot n}}{1 - (1 - \eta/n)} \\
        & \overset{\text{(A)}}{\geq} \frac{\eta \sqrt{\alpha}}{e^C}\left( \frac{t}{n} - \frac{\eta t^2}{2n^2}\right)  \overset{\text{(B)}}{=} \frac{\eta t \sqrt{\alpha}}{2e^Cn} = \sqrt{\frac{\eta}{CT}} \cdot \frac{t}{2e^Cn}%= \Omega\left( \frac{\sqrt{\eta t}}{n}\right).
    \end{align*}
    where $\text{(A)}$ is due to Taylor expansion, $\text{(B)}$ is due to the condition $\eta t \leq \eta T \leq T = \bigO{n}$ and $\alpha = C/(\eta T)$. We then calculate the average output
    \begin{align*}
        \bbE[\hat{y}_T] = \frac{1}{T} \sum_{t=1}^T \bbE[y_t] = \frac{1}{T} \sum_{t=1}^T \sqrt{\frac{\eta}{CT}} \cdot \frac{t}{2e^Cn} \geq \frac{1}{4e^C} \cdot \sqrt{\frac{\eta T}{C}}.
    \end{align*}
    We return to the original $f(w,z)$ by inserting the above analysis on the $j$-th copy:
    \begin{align*}
        \bbE[F(w_{\sgd})] \geq \bbE[G(w^{(j)}_{\sgd})] \geq \frac{1}{n} \sum_{i=1}^n \left(\frac{x^{(j)}_{\sgd}}{\sqrt{\eta T}} - y^{(j)}_{\sgd}(i)\right)^2 \geq \bigOmega{\max\left\{ \frac{1}{\eta T,} \ \frac{\eta T}{n^2 }\right\}} = \bigOmega{\frac{\eta T}{n^2 }}
    \end{align*}
    where the last inequality is due to the fact $e^{-C} \leq x_{\gd} \leq 1$. We can always choose a proper $C$ such that the difference is non-vanishing.
    \paragraph{Case $T = \bigOmega{n}$.} We can directly lower bound $\bbE[y_t]$ as
    \begin{align*}
        \bbE[y_t] \geq \frac{\eta \sqrt{\alpha}}{e^C n} \cdot \frac{n}{2\eta} = \frac{1}{2e^C\sqrt{C \eta T}} = \bigOmega{\frac{1}{\sqrt{\eta T}}}
    \end{align*}
    since $(1 - \eta/n)^t \leq 1/2$ when $\eta = \bigO{1}$ and $T = \bigOmega{n}$. We then calculate the average output
    \begin{align*}
        \bbE[\hat{y}_T] = \frac{1}{T} \sum_{t=1}^T \bbE[y_t] = \frac{1}{T} \sum_{t=1}^T \bigOmega{\frac{1}{\sqrt{\eta T}}} \geq \bigOmega{\frac{1}{\sqrt{\eta T}}}.
    \end{align*}
    Similarly, we return to the original $f(w,z)$ by inserting the above analysis on the $j$-th copy and obtain a non-vanishing lower bound by choosing proper $C$:
    \begin{align*}
        \bbE[F(w_{\sgd})] \geq \bbE[G(w^{(j)}_{\sgd})] \geq \frac{1}{n} \sum_{i=1}^n \left(\frac{x^{(j)}_{\sgd}}{\sqrt{\eta T}} - y^{(j)}_{\sgd}(i)\right)^2 \geq \bigOmega{\frac{1}{\eta T}}.
    \end{align*}
    This completes our proof.
\end{proof}


