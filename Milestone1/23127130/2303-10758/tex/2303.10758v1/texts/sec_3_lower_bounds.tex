\section{Main Result: Lower Bounds in Smooth SCO} \label{sec: lb}
In this section we proceed to state our main results on lower bounds for the smooth SCO.
We aim at filling the gaps mentioned in Section~\ref{sec: setting} for all the above settings. To this end, we will split our discussion to three parts: (1) non-realizable, (2) realizable under $T = \bigO{n}$ and (3) realizable under $T = \bigOmega{n}$.

\subsection{Non-realizable setting} \label{sec: nonrealizable}
We first discuss the non-realizable setting and provide a novel lower bound for the excess risk of GD in the following theorem.
\begin{theorem} \label{thm: lb-sco-gd}
    For any $\eta > 0$, $T \geq 1$ with $\bigOmega{1/T} = \eta = \bigO{1}$\footnote{This is a regular condition in optimization and generalization. This is because (1) step-size cannot exceed $\bigO{1}$, in order to make the optimization method converge for $\bigO{1}$-smooth function and (2) $T$ is arbitrarily large to ensure $\eta T = \bigOmega{1}$. We will assume this holds everywhere and do not repeat the regularity in the statement of rest theorems and lemmas.}, there exists a convex, $1$-smooth $f(w,z): \bbR \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with probability $\Theta(1)$, the output $w_{\gd}$ for GD satisfies
    \begin{align*}
        \bbE[F(w_T)] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{\eta T}{n}}.
    \end{align*}
\end{theorem}
The lower bound in Theorem~\ref{thm: lb-sco-gd} tightly matches the corresponding upper bound established in \cite{hardt2016train} (see Table~\ref{tab: summary}). It can be translated to a lower bound of sample complexity: for any $T \geq 1$, by setting $\eta = \sqrt{n}/T$, we derive a $\bigOmega{1/\sqrt{n}}$ bound which certifies the optimality of existing upper bound of $\bigO{1/\sqrt{n}}$. To the best of our knowledge, this is the first such result for GD. A recent work provides lower bound for the uniform stability of (S)GD \citep{zhang2022stability}, but it does not trivially imply a bound in excess risk.

The crucial step in the proof of Theorem~\ref{thm: lb-sco-gd} is to find a hard instance that gives an overfitting lower bound $\bigOmega{\eta T/n}$. To this end, we employ a technique inspired by Theorem 3 and Lemma 7 in \citet{sekhari2021sgd}: in non-realizable setting, the stochastic gradient does not necessarily scale down with the value of $f(w,z)$. As a result, by utilizing an \emph{anti-concentration} argument, we show that with non-vanishing probability $\bigOmega{1}$, the absolute value of $w_t$ increases by a rate of $\bigOmega{\eta/\sqrt{n}}$ in each step. Subsequently, calculation suggests a $\bigOmega{\eta T/n}$ bound for the function value. The details can be found in Appendix~\ref{appendix: lb-sco-gd}. In the meanwhile, the term $\bigOmega{1/\eta T}$ reflects the optimization error and the proof is provided in Lemma~\ref{lemma: lb-suboptimality}, Appendix~\ref{appendix: lb-suboptimality}. 

Subsequently, we proceed to a lower bound for SGD that is similar to GD. We state it in the following theorem.
\begin{theorem} \label{thm: lb-sco-sgd}
    For any $\eta > 0$, $T \geq 1$, there exists a convex, $1$-smooth $f(w,z): \bbR \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, %with probability $\Theta(1)$, 
    the output $w_{\sgd}$ for SGD satisfies
      \begin{align*}
        \bbE[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{\eta T}{n}}.
    \end{align*}
\end{theorem}
This also matches the SGD upper bound in \cite{hardt2016train} and implies a sample complexity bound $\bigOmega{1/\sqrt{n}}$ if we set $\eta = \sqrt{n}/T$ for any $T$.  

We emphasize the bound for SGD is novel compared with existing works: it is a forklore that in \citet{nemirovskij1983problem}, single-pass SGD achieves a sample complexity lower bounds for Lipschitz convex functions (where a smooth function within a bounded domain is automatically Lipschitz). Nevertheless, our result is first to provide such results in the multi-pass or large time horizon $T = \bigOmega{n}$ setting and reflects a hardship between $T$ and $n$ since it suggests overfitting and an increase of error when the iteration becomes large.

\subsection{Realizable: case \texorpdfstring{$T = O(n)$}{T=O(n)}} \label{sec: t_equal_n}
In the subsection we provide our lower bounds for the realizable setting when condition $T = \bigO{n}$ is satisfied. The next theorem  characterizes the lower bounds for GD and SGD.
\begin{theorem} \label{thm: lb-1}
For every $\eta > 0$, $T > 1$, if condition $T = \cO(n)$ holds, then there exists a convex, $1$-smooth and realizable $f(w, z): \bbR^d \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with a bounded initialization $\| w_0 - w^* \| = \bigO{1}$, the output $w_{\gd}$ for GD satisfies
\begin{align*}
    \bbE[F(w_{\gd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{1}{n} + \frac{\eta T}{n^2}}.
\end{align*}
Similarly, the output $w_{\sgd}$ for SGD satisfies
\begin{align*}
    \bbE[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{1}{n} + \frac{\eta T}{n^2}}.
\end{align*}
\end{theorem}
Theorem~\ref{thm: lb-1} suggests that there is an hard instance for both GD and SGD under the realizable smooth SCO. It is worth noting that we assume bounded initialization $\| w_0 - w^* \| = \bigO{1}$. This is standard and necessary in the generalization literature: the bound will be vacuous and arbitrarily bad if initial point is away from the optimal point with infinite distance.

Under regime $T = \bigO{n}$, the lower bound for GD tightly matches the upper bound in \cite{nikolakakis2022beyond}, and the lower bound for SGD almost tightly matches the lower bound in \cite{lei2020fine} up to a $\eta$ factor in the second term. Please refer to Table~\ref{tab: summary} for a comparison. We will combine the discussion for GD and SGD due to the similarity. Both upper and lower bounds are non-vacuous only when $T \leq n$ for $\eta = \Theta(1)$. As a result, to obtain the optimal sample complexity $\bigOmega{1/n}$ from Theorem~\ref{thm: lb-1}, we need to restrict the number of steps to $T = \Theta(n)$ and set $\eta = \Theta(1)$. In this way, it matches the sample complexity upper bound $\bigO{1/n}$ under the regime of $T = \bigO{n}$.
% \jiaye{I am confused, does the lower bound hold for $\cO(n)$ or $\Theta(n)$?} \peiyuan{Jingzhao suggested using $\bigO{n}$ instead of $\Theta(n)$.}

The major difficulty in lower bound construction and our major novelty is the proof of term $\bigOmega{\eta T/n^2}$. Similar to the non-realizable setting, the term $\bigOmega{1/(\eta T)}$ reflects the optimization error. In the meanwhile, the term $\bigOmega{1/n}$ comes from a universal hard instance that holds for any deterministic or stochastic gradient methods. Notice that the term $\bigOmega{1/n}$ does not suggest the rest two terms are vacuous since they are hard in the sense of characterizing the relationship between $\eta$, $T$ and $n$. Therefore, we emphasize that our lower bound construction does not only provide the worst instance on sample complexity but also reflects possibly worst growth of generalization error along time. 

%Theorem~\ref{thm: lb-sgd-1} shows that under realizable smooth SCO, SGD enjoys the same hardship with GD. The lower bound matches the upper bound of Eq.~\ref{eq: ub-sgd-tn} in \cite{lei2020fine} up to a factor of $\eta$ in the second term. It implies a sample complexity lower bound when $T = \bigO{n}$: when we select $T = n$, $\eta = 1$, it achieves the optimal sample complexity $\bigOmega{1/n}$. This also matches the corresponding upper bound.

\subsection{Realizable: case \texorpdfstring{$T = \bigOmega{n}$}{T=Omega(n)}} \label{sec: t_larger_than_n}
In this subsection we focus on the case that allows large or infinite time horizon $T = \bigOmega{n}$. We provide lower bounds for different algorithms and discuss their relationship with upper bounds. This is stated in the following theorem.
\begin{theorem} \label{thm: lb-2}
For every $\eta > 0$, $T > 1$, if condition $T = \bigOmega{n}$ holds, then there exists a convex, $1$-smooth and realizable $f(w, z): \bbR^d \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with a bounded initialization $\| w_0 - w^* \| = \bigO{1}$, the output $w_{\gd}$ for GD satisfies
\begin{align*}
    \bbE[F(w_{\gd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{1}{n}}.
\end{align*}
Similarly, the output $w_{\sgd}$ for SGD satisfies
\begin{align*}
    \bbE[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{1}{n}}.
\end{align*}
\end{theorem}
Again, we combine the discussion of GD and SGD because they have the same upper and lower bounds. Theorem~\ref{thm: lb-2} indicates that, different from the case $T = \bigO{n}$, our lower bound for both GD and SGD does not match the corresponding upper bounds in \cite{schliserman2022stability} (see Table~\ref{tab: summary}). And a gap exists between the sample complexity upper and lower bounds: for lower bound, we may achieve $\bigOmega{1/n}$ for any given $T$ when we select $\eta = n/T$; in contrast, we only obtain the rate $\bigO{1/n^{2/3}}$ from the upper bound  by setting $\eta = n^{2/3}/T$. 

To obtain the lower bound in Theorem~\ref{thm: lb-2} we employ a strategy similar to the proof of Theorem~\ref{thm: lb-1}. Term $\bigOmega{1/n}$ comes from the universal sample hardness for any algorithm, and term $\bigOmega{1/(\eta T)}$ is obtained from the construction used to prove $\bigOmega{\eta T/n}$ in Theorem~\ref{thm: lb-1}. The details are postponed to Appendix~\ref{appendix: lb-gd-t}.

We conjecture the sample complexity bound under a large or infinite time horizon can be closed by proving upper bound $\bigO{1/n}$ is achievable for GD. We will discuss the conjecture and provide several evidences in Section~\ref{sec: infinite}.% In the next theorem, we proceed to the lower bound for SGD.
%\begin{theorem} \label{thm: lb-sgd-2}
%For every $\eta > 0$, $T > 1$, there exists a convex, $1$-smooth and realizable $f(w, z): \bbR^d \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with a bounded initialization $\| w_0 - w^* \| = \bigO{1}$, the output $w_{\sgd}$ for SGD satisfies
%\begin{align*}
%    \bbE_{S\sim D^n}[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{1}{n}}.
%\end{align*}
%Similarly, the output $w_{\sgd}$ for SGD satisfies
%\begin{align*}
%   \bbE_{S\sim D^n}[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{1}{n}}.
%\end{align*}
%\end{theorem}
%Similar to GD, generalization error for SGD has a sample complexity lower bound of $\bigOmega{1/n}$ when we set $\eta = 1$, for any $T = \bigOmega{n}$. This does not match the upper bound $\bigO{1/n^{2/3}}$ from Eq.~\eqref{eq: ub-sgd-t}. We also conjecture the gap can be closed by proving upper bound $\bigO{1/n}$ for SGD, which we will discuss in next section. The proof to the theorem can be found in Appendix~\ref{appendix: t_larger_than_n}.

