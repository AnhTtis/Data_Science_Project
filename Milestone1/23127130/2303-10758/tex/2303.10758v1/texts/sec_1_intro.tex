\section{Introduction}

Gradient methods have become the predominant algorithms for training neural networks. These methods are not only computationally and space efficient, but more importantly, produce solutions that generalize well~\citep{he2016deep,huang2017densely,vaswani2017attention}. Understanding why neural networks trained with gradients perform well on test data can be very challenging, as the phenomenon results from an interplay between the network architecture, the data distribution as well as the training algorithm~\citep{jiang2019fantastic,zhang2021understanding}. In this work, we aim to shed some light on the role of (stochastic) gradient descent, and take a step by considering generalization in smooth convex problems.  

Much work has been done for analyzing gradient descent in convex learning problems. The early approach exploits the convex structure and shows that gradient methods find approximate empirical risk minimizers. Then the generalization can be bounded by uniform convergence~\citep{shalev2014understanding}. However, this approach is limited in scalability to high dimensions and can be provably vacuous even for the simple task of linear regression~\citep{shalev2010learnability,feldman2016generalization}. An alternative approach \citep{nemirovskij1983problem} that addresses the high-dimension problem is via online-to-batch conversion. This approach achieves minimax sample complexity, but it only applies to single-pass training, whereas in practice, models trained for longer periods can generalize better~\citep{hoffer2017train}.

Several recent explanations have been proposed to bridge the gap and bound generalization in multi-pass settings~\citep{soudry2018implicit,ji2019implicit,lyu2021gradient,bartlett2020benign}. These works demonstrate that gradient descent benefits from implicit bias and finds max-margin solutions for classification problems, as well as min-norm solutions for regression problems. However, characterizing the implicit bias for other loss functions or non-linear models remains a challenging task.

One method that generalizes to a broader range of loss functions and models is the stability argument~\citep{hardt2016train}. This argument shows that if the model and the training method is not overly sensitive to data perturbations, the generalization error can be bounded. However, this argument suffers from a large number of training updates, while in practice, large neural networks often benefit from longer training times. 
\textbf{It remains unclear whether longer training time truly hurts generalization in smooth convex learning problems, as it is unclear whether the growing upper bounds are tight or just result from an artifact of the analysis. }

In this work, we focus on the setting of smooth \emph{stochastic convex optimization} (SCO), where a finite sample of smooth convex functions i.i.d. drawn from an unknown distribution is employed to learn a parameterized model that minimizes the expected risk. This simple setting has drawn significant attention over the past few decades and has yielded many remarkable results.

Our aim is to understand whether longer training inevitably leads to overfitting in smooth SCO. Several recent works have shown fast convergence rates in test error when the number of iterations is not too large under the \emph{realizable} condition~\citep{lei2020fine,nikolakakis2022beyond,schliserman2022stability}. Our work provides the first tight lower bounds in these scenarios.

\paragraph{Our contributions.}
We now summarize the major contributions in our paper. Let $\eta$ represent the step size in gradient methods, $T$ denote the iteration number, and $n$ denote the sample size.
\begin{itemize}
    \item We first provide a tight lower bound $\bigOmega{\frac{1}{\eta T} + \frac{\eta T}{n}}$ for smooth non-realizable SCO. 
    \item For realizable SCO, we notice a gap between two types of analysis, as shown in Table~\ref{tab: summary}. When $T = \bigO{n}$, we prove matching lower bounds for the excess population risk for GD and SGD under the smooth and realizable SCO setting.
    \item When $T = \bigOmega{n}$, we provide a lower bound construction that suggests a gap exists between upper and lower bound. 
    \item We conjecture that the upper bound when $T = \bigO{n}$ is not tight. We provide evidence for the conjecture in two special scenarios: (1) one-dimensional and (2) linear regression.
\end{itemize}

Our results offer insights and answers to the question of how longer training can impact generalization error. For non-realizable cases, our lower bound suggests that training for a longer time can provably lead to overfitting, even for smooth convex problems. For realizable cases, our lower bounds suggest that longer training might actually improve generalization for general smooth convex problems. We conjecture that the existing upper bound for realizable smooth SCO is not tight. This conjecture is supported by matching upper bounds in two special settings.

% Furthermore, our lower bounds extend to large training iterations and suggest that training longer in this setup will not hurt generalization. This leads to a gap between upper and lower bounds in the multi-pass training scheme. We  conjecture that under the infinite horizon condition GD and SGD can achieve an upper bound of $\bigO{1/n}$, which meets our lower bounds. We support our conjecture by providing evidences on special settings. In precise, we establish matching upper and lower bounds for (1) dimension one case and (2) linear regression. We believe it to be a promising direction to explore the more general case and leave the problem of closing the gap when $T = \bigOmega{n}$ as a open problem. 

% \paragraph{Overview of structure.} We give a brief overview of the whole paper. We start by introducing our basic setting and reviewing related backgrounds in Section~\ref{sec: setting}. In Section~\ref{sec: lb}, we present our main result: the lower bound of excess population risk for the realizable smooth SCO. In particular, we divide into two cases $T = \bigO{n}$ and $T = \bigOmega{n}$ and discuss them separately in Section~\ref{sec: t_equal_n}, Section~\ref{sec: t_larger_than_n}. In Section~\ref{sec: infinite}, we provide evidences to endorse our conjecture on the gap between upper and lower bound for large time horizon. Section~\ref{sec: proof} is then an overview of our proof technique used in Section~\ref{sec: lb}. We finish with a discussion on related literature and several important problems.

\paragraph{Notations.} 
We denote set $[n] = \{1, 2, \dots, n\}$. $\| \cdot\|$ denotes the $l_2$ norm for vectors. We use $\text{Bern}(p)$ to denote the Bernoulli distribution with probability $p$ to be $1$ and $\text{Unif}(S)$ to denote uniform distribution over set $S$.