\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
\usepackage[abbrvbib,preprint]{jmlr2e}

%\usepackage{jmlr2e}
\usepackage{algorithm}
\usepackage{amsmath,amssymb,amsfonts,mathrsfs}
\usepackage{breakcites}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{pdfpages}
\usepackage{bm}
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs}  
\usepackage{lipsum}
\usepackage{makecell}
\usepackage{rotating}
\usepackage{tablefootnote}
%\usepackage{cleveref}

%\displaybreak
\allowdisplaybreaks

\usepackage{threeparttable}

% Definitions of handy macros can go here

\input{defs}

\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
%\newcommand{\dataset}{{\cal D}}
%\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\newcommand{\peiyuan}[1]{{\color{blue} Peiyuan's comment: #1}}
\newcommand{\jingzhao}[1]{{\color{red} Jingzhao's comment: #1}}
\newcommand{\jiaye}[1]{{\color{magenta} Jiaye's comment: #1}}


% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}
\jmlrheading{24}{2023}{1-\pageref{LastPage}}{04/03; Revised XX/XX}{XX/XX}{XX-XXXX}{Zhang, Teng and Zhang}
\usepackage{lastpage}
%\jmlrheading{23}{2022}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Peiyuan Zhang, Jiaye Teng and Jingzhao Zhang}

% Short headings should be running head and authors last names

\ShortHeadings{Lower Generalization Bound in Smooth SCO}{Zhang, Teng and Zhang}
\firstpageno{1}

\begin{document}

\title{Lower Generalization Bounds for GD and SGD \\
in Smooth Stochastic Convex Optimization}

\author{\name Peiyuan Zhang%\thanks{Partly done when the author was an intern at Shanghai Qizhi Institute.} 
       \email peiyuan.zhang@yale.edu \\
       \addr Yale University%\\
       %New Haven, CT
       \AND
       \name Jiaye Teng  \email tjy20@mail.tsinghua.edu.cn \\
       \addr Tsinghua University%\\
       %Beijing, China
       \AND
       \name Jingzhao Zhang\thanks{Corresponding author.} \email jingzhaoz@mail.tsinghua.edu.cn \\
       \addr Tsinghua University \& Shanghai Qizhi Institute
       %Beijing, China
       }

\editor{My editor}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
This work studies the generalization error of gradient methods. More specifically, we focus on how training steps $T$ and step-size $\eta$ might affect generalization in \emph{smooth} stochastic convex optimization (SCO) problems. We first provide tight excess risk lower bounds for Gradient Descent (GD) and Stochastic Gradient Descent (SGD) under the general \emph{non-realizable} smooth SCO setting, suggesting that existing stability analyses are tight in step-size and iteration dependence, and that overfitting provably happens. 
Next, we study the case when the loss is \emph{realizable}, i.e. an optimal
solution minimizes all the data points. Recent works show better rates can be attained but the improvement is reduced when training time is long. Our paper examines this observation by providing excess risk lower bounds for GD and SGD in two \emph{realizable} settings: 1)  $\eta T = \bigO{n}$, and (2)  $\eta T = \bigOmega{n}$, where $n$ is the size of dataset. In the first  case $\eta T = \bigOmega{n}$, our lower bounds tightly match and certify the respective upper bounds. However, for the case $\eta T = \bigOmega{n}$, our analysis indicates a gap between the lower and upper bounds. A conjecture is proposed that the gap can be closed by improving upper bounds, supported by analyses in two special scenarios. 
%\lipsum[1]
\end{abstract}

\begin{keywords}
  generalization, gradient methods, stochastic convex optimization, realizable setting, lower bounds
\end{keywords}

\input{texts/sec_1_intro}
\input{texts/sec_2_setting}
\input{texts/sec_3_lower_bounds}
\input{texts/sec_4_infinite}
\input{texts/sec_5_proofs}
\input{texts/sec_6_related}

%\acks{All acknowledgements go at the end of the paper before appendices and references. Moreover, you are required to declare funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work). More information about this disclosure can be found on the JMLR website.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.


%\input{texts/appendix_4(upper bound 1-dim)}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

\newpage

\appendix

\input{texts/appendix_1}
\input{texts/appendix_2}
\input{texts/appendix_3}
%\input{texts/appendix_5(ub-gd)}

\vskip 0.2in
\bibliography{bib}

%\input{texts/test}


\end{document}