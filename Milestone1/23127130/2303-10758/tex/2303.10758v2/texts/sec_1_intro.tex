\section{Introduction}

Gradient methods are the predominant algorithms for training neural networks. These methods are not only efficient in time and space, but more importantly, produce solutions that generalize well~\citep{he2016deep,vaswani2017attention}. Understanding why neural networks trained with gradient methods perform well on test data can be very challenging, as the phenomenon results from an interplay between the network architecture, the data distribution as well as the training algorithm~\citep{jiang2019fantastic,zhang2021understanding}. In this work, we aim to shed some light on the role of (stochastic) gradient descent, and take a humble step by considering generalization in smooth convex problems.  

Much work has been done for analyzing gradient descent in convex learning problems. The early approach exploits the convex structure and shows that gradient methods find approximate empirical risk minimizers. Then the generalization can be bounded by uniform convergence~\citep{shalev2014understanding}. However, this approach is limited in scalability to high dimensions and can be provably vacuous even for the simple task of linear regression~\citep{shalev2010learnability,feldman2016generalization}. An alternative approach \citep{nemirovskij1983problem} that addresses the high-dimension problem is via online-to-batch conversion. This approach achieves minimax sample complexity, but it only applies to single-pass training, whereas in practice, models trained for longer periods can generalize better~\citep{hoffer2017train}.

Several recent explanations have been proposed to bridge the gap and bound generalization in multi-pass settings~\citep{soudry2018implicit,ji2019implicit,lyu2021gradient,bartlett2020benign}. These works demonstrate that gradient descent benefits from implicit bias and finds max-margin solutions for classification problems, as well as min-norm solutions for regression problems. However, characterizing the implicit bias for other loss functions or non-linear models remains a challenging task.

One method that generalizes to a broader range of loss functions and models is the stability argument~\citep{bousquet2002stability,hardt2016train}. This argument shows that if the model and the training method is not overly sensitive to data perturbations, the generalization error can be effectively bounded. However, this argument suffers from a large number of training updates especially when the step-size is sufficiently large, while in practice, generalization often benefits from longer training time. 
\emph{It is unclear whether longer training truly hurts generalization in smooth convex learning problems, because the tightness of the growing upper bounds remains unknown, and might just result from an artifact of the analysis. }

In this work, we analyze the above problem in smooth \emph{stochastic convex optimization} (SCO). More specifically, we focus on how the \emph{training horizon} $\eta T$ might affect the generalization property: the product of the step size $\eta$ and the number of iteration $T$ is a better measure for training intensity as large number of iterations may not even train a model if $\eta$ is close to zero. 
While several recent works have established fast convergence rates in test error when $\eta T$ is not too large under the \emph{realizable} condition~\citep{lei2020fine,nikolakakis2022beyond,schliserman2022stability}, our work provides the first tight lower bounds in these scenarios and suggest that known bounds are tight in some settings but likely not when $\eta T$ is large. 


% In this work, we focus on the setting of smooth \emph{stochastic convex optimization} (SCO), where a finite sample of smooth convex functions i.i.d. drawn from an unknown distribution is employed to learn a parameterized model that minimizes the expected risk. This simple setting has drawn significant attention over the past few decades and has yielded many remarkable results.

% Our aim is to understand whether longer training inevitably leads to overfitting in smooth SCO. Several recent works have shown fast convergence rates in test error when the number of iterations is not too large under the \emph{realizable} condition~\citep{lei2020fine,nikolakakis2022beyond,schliserman2022stability}. Our work provides the first tight lower bounds in these scenarios.

\paragraph{Our contributions.}
Let $\eta$ represent the step-size in gradient methods, $T$ denote the iteration number, and $n$ denote the sample size. Our contributions are as follows and presented in Table~\ref{tab: summary},
\begin{itemize}
    \item We first provide a tight lower bound $\bigOmega{\frac{1}{\eta T} + \frac{\eta T}{n}}$ for the smooth non-realizable SCO. 
    \item For realizable SCO, we notice a gap between two types of analysis, as shown in Table~\ref{tab: summary}: 
      \begin{enumerate}
        \item when $\eta T = \bigO{n}$, we prove matching lower bounds for the excess population risk for GD and SGD under the smooth and realizable SCO setting; 
        \item  when  $\eta T = \bigOmega{n}$, we provide a lower bound construction that suggests a gap exists between upper and lower bound. 
      \end{enumerate}
    \item We conjecture that the upper bound when $\eta T = \bigOmega{n}$ is not tight for large step-sizes. We provide evidences for the conjecture in two special scenarios: (1) one-dimensional convex problems and (2) high-dimensional linear regression.
\end{itemize}

Our results offer insights and answers to the question of how longer training can impact generalization error. For non-realizable cases, our lower bound suggests that training for a longer time can provably lead to overfitting, even for smooth convex problems. For realizable cases, our lower bounds suggest that longer training might actually reduce the generalization error. Moreover, our new lower bounds in the smooth setting, compared with those known in the nonsmooth setting, suggest that smoothness and realizability together might explain why training longer does not lead to overfitting.

% Furthermore, our lower bounds extend to large training iterations and suggest that training longer in this setup will not hurt generalization. This leads to a gap between upper and lower bounds in the multi-pass training scheme. We  conjecture that under the infinite horizon condition GD and SGD can achieve an upper bound of $\bigO{1/n}$, which meets our lower bounds. We support our conjecture by providing evidences on special settings. In precise, we establish matching upper and lower bounds for (1) dimension one case and (2) linear regression. We believe it to be a promising direction to explore the more general case and leave the problem of closing the gap when $T = \bigOmega{n}$ as a open problem. 

% \paragraph{Overview of structure.} We give a brief overview of the whole paper. We start by introducing our basic setting and reviewing related backgrounds in Section~\ref{sec: setting}. In Section~\ref{sec: lb}, we present our main result: the lower bound of excess population risk for the realizable smooth SCO. In particular, we divide into two cases $T = \bigO{n}$ and $T = \bigOmega{n}$ and discuss them separately in Section~\ref{sec: t_equal_n}, Section~\ref{sec: t_larger_than_n}. In Section~\ref{sec: infinite}, we provide evidences to endorse our conjecture on the gap between upper and lower bound for large time horizon. Section~\ref{sec: proof} is then an overview of our proof technique used in Section~\ref{sec: lb}. We finish with a discussion on related literature and several important problems.

\paragraph{Notations.} 
For any positive integer $n$, we denote the set $[n] := \{1, 2, \dots, n\}$. $\| \cdot\|$ denotes the $l_2$ norm for vectors. We use $\text{Bern}(p)$ to denote the Bernoulli distribution with probability $p$ to be $1$ and $\text{Unif}(S)$ to denote the uniform distribution over set $S$. Occasionally we will use capital letters, i.e. $W$, to denote "large" vectors, in contrast to usual vectors like $v,w$. Also, Let $\vone$ be all-one vector and $\ve_i$ be the vector with a $1$ in the $i$-th coordinate and $0$ elsewhere. 