\section{Proof Overviews} \label{sec: proof}
In this section we provide a brief overview regarding our technique used in the proofs of theorems for realizable cases in Section~\ref{sec: lb}. In particular, we will focus on the lower bound construction for the output of GD when $\eta T = \bigO{n}$, i.e. first part in Theorem~\ref{thm: lb-1}, to showcase the major intuition and idea behind our constructions. 

As discussed in the above section, the main technical difficulty in the GD part of Theorem~\ref{thm: lb-1} lies in proving
\begin{equation} \label{eq: lb-major}
    \bbE[F(w_{\gd})] - F(w^*) = \bigOmega{\frac{\eta T}{n^2}}.
\end{equation}
The proof of rest terms is based on easier constructions and we recommend referring to Lemma~\ref{lemma: lb-sample} and Lemma~\ref{lemma: lb-suboptimality} in Appendix~\ref{appendix: minor}. These lemmas are general and hold for any deterministic or stochastic gradient methods. Here we focus on the proof of \eqref{eq: lb-major}. Our technique is novel and inspired by the work of \citet{amir2021sgd, sekhari2021sgd}. However, their construction critically relies on nonsmoothness and nonrealizability. 

We start by considering running GD on the following 2-dimensional quadratic function
\begin{equation*}
    h(x, y) = \frac{\alpha x^2}{2} + \frac{y^2}{2} - 2\sqrt{\alpha} xy = \frac{1}{2} \left| \sqrt{\alpha} x -  y \right|^2
\end{equation*}
with step-size $\eta$ and initialization $x_1 = 1$ and $y_1 = 0$. We choose a small enough $\alpha = \nfrac{1}{\eta T} \ll 1$. In every iteration, since $\alpha$ is small, $x$ is pulled back to zero slowly: it is easy to lower bound the value since 
\begin{equation*}
    x_{t+1} \geq (1 - \alpha\eta) x_{t} \geq e^{-\alpha \eta t} x_1  \geq e^{-t/T} x_1 \geq 1/e = \Theta(1).
\end{equation*}
Hence $x_t = \bigOmega{1}$ for any $t \in [T]$. Meanwhile, 
coordinate $y$ is simultaneously (1) pushed away from zero by $x$ on the scale of $\bigOmega{\eta\sqrt{\alpha}}$ and (2) pulled back towards zero by itself. As a result, despite the pulling influence, we can still guarantee that $y_t$ is bounded away from zero for all $t \in [T]$.

We now want to improve over the naive two-dimensional quadratic example to make sure that multiple coordinates are bounded away from zero. This intuitively might provide a hard instance for the GD algorithm. Also, we hope stochasticity plays a role in the hard instance such that we can introduce the factor of $n$.  We then devise the following instance $g(w, z): \bbR^{n+1} \times \cZ \to \bbR$ belonging to the realizable smooth SCO setting:  
\begin{equation}
    g(w, z = i) = \frac{\alpha}{2} x^2 + \frac{1}{2} \big(y(i)\big)^2  - \sqrt{\alpha} x \cdot y(i) = \frac{1}{2} \left|\sqrt{\alpha}x - y(i)\right|^2
\end{equation}
where $w = (x,y)$, $x \in \bbR$, $y \in \bbR^n$ and $z \sim \text{Unif}([n])$.  We still set parameter $\alpha$ to be $1/(\eta T)$. We are given a dataset $S$ of $n$ examples i.i.d. from the distribution. This leads to population loss
\begin{equation}
    G(w) = \bbE_{z\sim\text{Unif}([n])}[g(w,z)] =  \frac{1}{2n} \left\| y - \sqrt{\alpha} x \right\|^2.
\end{equation}

We generalize the idea from the two-dimensional case to $n+1$ dimension. To this end, we need every example $z_i \in S$ corresponds to one coordinate $y(i)$. This is, however, an improbable event that occurs with probability $\Theta(\sqrt{n} \cdot e^{-n})$. We use the intuition from \citet{amir2021sgd,sekhari2021sgd}: if we consider multiple independent copies of $g(w,z)$, then with probability $\Theta(1)$, there exists at least one copy that satisfies the condition.

We focus on the particular copy only. Our calculations shows that under assumption $\eta T = \bigO{n}$, it holds that, for any $t \in [T]$, (1) $x_t = \Theta(1)$ and (2) $y_t(i) = \bigOmega{\sqrt{\eta t}/n}$ for any coordinates $i \in [n]$. With a slight abuse of the notation $w$, we put everything together and guarantee that 
\begin{align*}
    F(w_{\gd}) - F(w^*) = \bigOmega{\frac{1}{2n} \cdot \| y_T \|^2} = \bigOmega{\frac{\eta T}{n^2}}.
\end{align*}

The details in the proof of Theorem~\ref{thm: lb-1} can be found in Appendix~\ref{appendix: lb-gd-tn}. The idea behind the proof for the case $\eta T = \bigOmega{n}$ (Theorem~\ref{thm: lb-2}) differs only in calculations and hence we omit the repetition. Proof for SGD is also similar. The details of proof for other theorems can be found in Appendix~\ref{appendix: t_equal_n}.