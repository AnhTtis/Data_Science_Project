\section{Missing Proofs from Section~\ref{sec: nonrealizable}} \label{appendix: nonrealizable}
\subsection{Proof of Theorem~\ref{thm: lb-sco-gd}} \label{appendix: lb-sco-gd}
The theorem provides an excess risk lower bound $\bigOmega{\nfrac{1}{\eta T} + \nfrac{\eta T}{n}}$ for GD under the \emph{non-realizable} smooth SCO scenario. The result is obtained by combining a $\bigOmega{1/{\eta T}}$ bound in Lemma~\ref{lemma: lb-suboptimality} and a $\bigOmega{\eta T/n}$ bound in Lemma~\ref{lemma: lb-gd-overfit} stated below. The first bound reflects an optimization error and is postponed to Appendix~\ref{appendix: lb-suboptimality}. In the rest part, we present the proof of the latter lemma.
\begin{lemma} \label{lemma: lb-gd-overfit}
    For any $\eta > 0$, $T > 1$, there exists a convex, $1$-smooth $f(w,z): \bbR \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with probability $\Theta(1)$, the output $w_{\gd}$ for GD satisfies
    \begin{align*}
        F(w_\gd) - F(w^*) = \bigOmega{\frac{\eta T}{n}}.
    \end{align*}
\end{lemma}

\begin{proof}
We define loss function $f: \bbR \times \cZ \to \bbR$ as
\begin{align*}
    f(w, z) = \frac{w^2}{2\eta T}  + zw
\end{align*}
where $z \sim \text{Unif}(\{ \pm 1\})$. It is obvious that $f(w,z)$ is $1$-smooth and convex since $\eta T \geq 1$. The population risk is computed as
\begin{align*}
    F(w) = \bbE_{z\sim\text{Unif}(\{\pm 1\})}[f(w, z)] = \frac{w^2}{2\eta T}.
\end{align*}
The minimizer is then $w^* = 0$. GD formulates the following recurrence on dataset $S$ with initialization $w_1=0$:
\begin{align*}
    w_{t+1} = w_t - \frac{\eta}{n} \sum_{i=1}^n\left( \frac{w_t}{\eta T} + z_i \right) = \left(1-\frac{1}{T} \right) w_t - \frac{\eta}{n}\sum_{i=1}^nz_i,
\end{align*}
where each $z_i \sim \text{Unif}(\{\pm 1\})$ for $i \in [n]$. We want to use an anti-concentration result to lower bound the recurrence: from Lemma 7 in \citet{sekhari2021sgd}, with probability $\bigOmega{1}$, it holds that
\begin{align*}
    \sum_{i=1}^n z_i \leq - \frac{\sqrt{n}}{2}.
\end{align*}
We get lower bound
\begin{align*}
    w_{t+1} \geq  \left(1-\frac{1}{T} \right) w_t + \frac{\eta}{2\sqrt{n}}.
\end{align*}
Then we have for any $t \in [T]$
\begin{align*}
    w_t & \geq \frac{\eta}{2\sqrt{n}} \left( 1 + \left( 1 - \frac{1}{T}\right) + \cdots + \left( 1 - \frac{1}{T}\right)^{t-1} \right) \\
    & \geq \frac{\eta t}{8\sqrt{n}}
\end{align*}
where the second inequality is due to the fact 
\begin{align*}
    1 > 1 - \frac{1}{T} > \cdots  \left( 1 - \frac{1}{T}\right)^t > \cdots > \left( 1 - \frac{1}{T}\right)^{T} \geq \frac{1}{4}
\end{align*}
for any $t \in [T]$ and $T \geq 2$. Then the average is lower bounded as
\begin{align*}
    \bar{w}_T = \frac{1}{T} \sum_{t=1}^T w_t \geq \sum_{t=1}^T \frac{\eta t}{8\sqrt{n}} = \frac{\eta (T-1)}{16\sqrt{n}}. 
\end{align*}
As a result, we have
\begin{align*}
    F(w_{\gd}) - F(w^*) = \frac{w_{\gd}^2}{2\eta T}  = \bigOmega{\frac{\eta T}{n}},
\end{align*}
which is the desired result.
\end{proof}
%Combined with the suboptimality lower bound in Lemma~\ref{lemma: lb-suboptimality}, we have lower bound $$ \bbE[F(w_{\gd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{\eta T}{n}}.$$

\subsection{Proof of Theorem~\ref{thm: lb-sco-sgd}}
% We consider this to be a forklore knowledge. Nevertheless, we fail to find any reference to such a result and present a short proof here. We first give a lower bound $\bigOmega{\eta T/n}$ by the following construction.
Similar to the proof of Theorem~\ref{thm: lb-sco-gd}, we prove the excess risk lower bound for SGD by combining Lemma~\ref{lemma: lb-suboptimality} and the following lemma.
\begin{lemma} \label{lemma: lb-sgd-overfit}
    For any $\eta > 0$, $T > 1$,  there exists a convex, $1$-smooth $f(w,z): \bbR \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with probability $\Theta(1)$, the output $w_{\sgd}$ for SGD satisfies
    \begin{align*}
        \bbE[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{\eta T}{n}}.
    \end{align*}
\end{lemma}

\begin{proof}
We use the same construction in Lemma~\ref{lemma: lb-gd-overfit}. Consider dataset $S=\{z_1,\dots,z_n\}$ where $z_i \sim \text{Bern}(\{\pm 1\})$. Given $x_t$, SGD formulates the following recurrence on dataset $S$ with initialization $w_1=0$:
\begin{align*}
    \bbE[w_{t+1}] = w_t - \frac{\eta}{n} \sum_{i=1}^n\left( \frac{w_t}{\eta T} + z_i \right) = \left(1-\frac{1}{T} \right) w_t - \frac{\eta}{n}\sum_{i=1}^nz_i,
\end{align*}
where $z_i \sim \text{Unif}(\{\pm 1\})$. From Lemma 7 in \citet{sekhari2021sgd}, with probability $\bigOmega{1}$, it holds that
\begin{align*}
    \sum_{i=1}^n z_i \leq - \frac{\sqrt{n}}{2}.
\end{align*}
Then we get lower bound
\begin{align*}
    \bbE[w_{t+1}] \geq  \left(1-\frac{1}{T} \right) w_t + \frac{\eta}{2\sqrt{n}}.
\end{align*}
Similar to the proof of Lemma~\ref{lemma: lb-gd-overfit}, we have for any $t \in [T]$
\begin{align*}
    \bbE[w_t] & \geq \frac{\eta}{2\sqrt{n}} \left( 1 + \left( 1 - \frac{1}{T}\right) + \cdots + \left( 1 - \frac{1}{T}\right)^{t-1} \right) \\
    & \geq  \frac{\eta t}{8\sqrt{n}}.
\end{align*}
Then the average is lower bounded as
\begin{align*}
    \bbE[\bar{w}_T] = \frac{1}{T} \sum_{t=1}^T \bbE[w_t] \geq \sum_{t=1}^T \frac{\eta t}{8\sqrt{n}} = \frac{\eta (T-1)}{16\sqrt{n}}. 
\end{align*}
As a result, we have
\begin{align*}
    \bbE[F(w_{\sgd})] - F(w^*) \geq F(\bbE[w_{\sgd}]) - F(w^*) = \frac{(\bbE[w_{\sgd}])^2}{2\eta T}  = \bigOmega{\frac{\eta T}{n}},
\end{align*}
by Jensen's inequality.
\end{proof}


