\section{Missing Proofs from Section~\ref{sec: t_equal_n} and Section~\ref{sec: t_larger_than_n} } \label{appendix: t_equal_n}

\subsection{Proof of Theorem~\ref{thm: lb-1}} \label{appendix: lb-gd-tn}
The proof of GD is immediate from combining lower bound $\bigOmega{1/{\eta T}}$ in Lemma~\ref{lemma: lb-suboptimality}, lower bound $\bigOmega{1/n}$ in Lemma~\ref{lemma: lb-sample}, and most importantly, lower bound $\bigOmega{\eta T/n^2}$ in Lemma~\ref{lemma: lb-gd-tn} to be stated below. In precise, Lemma~\ref{lemma: lb-gd-tn} is the core part of our result and gives a lower bound of $\bigOmega{\eta T/n}$ when $\eta T = \bigO{n}$, and a lower bound of $\bigOmega{1/\eta T}$ when $\eta T = \bigOmega{n}$ for GD. The latter part is used in the proof of Theorem~\ref{thm: lb-2}. We postpone its proof to Appendix~\ref{appendix: lb-gd}. The proof of the rest two lemmas can be found in Appendix~\ref{appendix: lb-sample}, \ref{appendix: lb-suboptimality}.
\begin{lemma} \label{lemma: lb-gd-tn}
For every $\eta > 0$, $T > 1$, if $\eta T =  \bigO{n}$, then there exists a convex, $1$-smooth and realizable $f(W, Z): \bbR^{(n+1) \times m} \times \cZ^m \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with initialization $\| W_1 - W^* \| = \bigO{1}$, the output $W_{\gd}$ for GD satisfies
\begin{align*}
 \bbE[F(W_{\gd})] - F(W^*) = \bigOmega{\frac{\eta T}{n^2}}.  
\end{align*}
In specific, $m$ is an integer with $m = \Theta(e^n/\sqrt{n})$. Similarly, if $\eta T =  \bigOmega{n}$, then 
it satisfies
\begin{align*}
 \bbE[F(W_{\gd})] - F(W^*) = \bigOmega{\frac{1}{\eta T}}.  
\end{align*}
\end{lemma}
Similar to the proof of GD, the result on SGD is also obtained by combining lower bound constructions in the following lemma and Lemma~\ref{lemma: lb-sample}, \ref{lemma: lb-suboptimality} in Appendix~\ref{appendix: lb-sample}, \ref{appendix: lb-suboptimality}. Lemma~\ref{lemma: lb-sgd-tn} establishes a lower bound of $\bigOmega{\eta T/n}$ when $\eta T = \bigO{n}$, and a lower bound of $\bigOmega{1/\eta T}$ when $\eta T = \bigOmega{n}$ for SGD. Its proof can be found in Appendix~\ref{appendix: lb-sgd}.
\begin{lemma} \label{lemma: lb-sgd-tn}
For every $\eta > 0$, $T > 1$, if $\eta T =  \bigO{n}$, then there exists a convex, $1$-smooth and realizable $f(W, Z): \bbR^{(n+1) \times m} \times \cZ^m \to \bbR$ for every $Z \in \cZ^m$, and a distribution $D$ such that, with initialization $\| W_1 - W^* \| = \bigO{1}$, the output $W{\sgd}$ for SGD satisfies
\begin{align*}
 \bbE[F(W_{\sgd})] - F(W^*) = \bigOmega{\frac{\eta T}{n^2}}.  
\end{align*}
In specific, $m$ is an integer with $m = \Theta(e^n/\sqrt{n})$. Similarly, if $\eta T =  \bigOmega{n}$, then 
it satisfies
\begin{align*}
 \bbE[F(W_{\sgd})] - F(W^*) = \bigOmega{\frac{1}{\eta T}}.  
\end{align*}
\end{lemma}

\subsection{Proof of Theorem~\ref{thm: lb-2}} \label{appendix: lb-gd-t}
Similar to the proof of Theorem~\ref{thm: lb-1}, the proof of GD is obtained from combining the lower bounds in Lemma~\ref{lemma: lb-sample}, Lemma~\ref{lemma: lb-suboptimality}, and Lemma~\ref{lemma: lb-gd-tn}. In particular, Lemma~\ref{lemma: lb-gd-tn} gives a lower bound of $\bigOmega{\eta T/n}$ when $\eta T = \bigOmega{n}$. 

Concurrently, the proof of SGD is obtained from combining the lower bounds in Lemma~\ref{lemma: lb-sample}, Lemma~\ref{lemma: lb-suboptimality}, and Lemma~\ref{lemma: lb-sgd-tn}. In particular, Lemma~\ref{lemma: lb-sgd-tn} gives a lower bound of $\bigOmega{1/{\eta T}}$ when $\eta T = \bigOmega{n}$. 

\subsection{Proof of Lemma~\ref{lemma: lb-gd-tn}} 

This subsection contains the proof of Lemma~\ref{lemma: lb-gd-tn}, along with another two supportive lemmas. We first present the proof of the major lemma. 
\label{appendix: lb-gd}
%\begin{lemma}[Restated Lemma~\ref{lemma: lb-gd-tn}]
%For every $\eta > 0$, $T > 1$, if $T =  \bigO{n}$, then there exists a convex, $1$-smooth and realizable $f(w, z): \bbR^d \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with initialization $\| w_1 - w^* \| = \bigO{1}$, the output $w_{\gd}$ for GD satisfies
%\begin{align*}
% \bbE[F(w_{\gd})] - F(w^*) = \bigOmega{\frac{\eta T}{n^2}}.  
%\end{align*}
%Similarly, if $T =  \bigOmega{n}$, then 
%the output $w_{\gd}$ for GD satisfies
%\begin{align*}
% \bbE[F(w_{\gd})] - F(w^*) = \bigOmega{\frac{1}{\eta T}}.  
%\end{align*}
%\end{lemma}

\begin{proof} We construct the following instance to obtain the lower bound, where $f: \bbR^{(n+1) \times m} \times \cZ^m \to \bbR$ is
    \begin{align} \label{eq: lb-instance1}
        & f(W,Z) = \sum_{j=1}^m g(w^{(i)}, z^{(i)}) 
    \end{align}
    with a positive integer $m$, and $g$ defined as
    \begin{align} \label{eq: lb-instance2}
        & g(w, z = i) = \frac{\alpha}{2} x^2 + \frac{1}{2} \big(y(i)\big)^2  - \sqrt{\alpha} x \cdot y(i) = \frac{1}{2} \Big(\sqrt{\alpha}x - y(i)\Big)^2,
    \end{align}
where $W = ( w^{(1)}, \cdots, w^{(m)})$ is a large vector formed by concatenating by $m$ vectors $\{w^{(j)} \ |\ w^{(j)} \in \bbR^{n+1}\}_{j=1}^m$, and $Z = ( z^{(1)}, \cdots, z^{(m)})$ denotes a large sample concatenated by $m$ copies of independent samples $\{ z^{(j)}\ |\ \ z^{(j)} \in  [n]\}$. We will omit the upscript of $j$ when it does not lead to confusion. Each $w$ is split into $w = (x, y)$ with $x \in \bbR$ and $y \in \bbR^n$ in the function $g$, and we define $y(i)$ as the $i$-th coordinate of $y$. We assume $z \sim \text{Unif}([n])$ i.i.d., and set parameters $m = \Theta(e^n/\sqrt{n})$, $\alpha = C/(\eta T)$, where $C \leq 1$ is a constant. 
Intuitively, $f$ can be regarded as the summation over $m$ copies of $g(w^{(j)},z^{(j)})$. 
Such a construction $f$ satisfies the conditions in the statement of this lemma (see Lemma~\ref{lemma: supp-1} below).


Lemma~\ref{lemma: supp-2} (also see below) shows that: with constant probability, there exists at least one copy of $\{z^{(j)}_i\}_{i\in[n]}$ (for clarification, $z^{(j)}_i$ is the $j$-th component in the $i$-th sample $Z_i$ within the dataset $S=\{Z_1,\dots,Z_n\}$) satisfying
\begin{align*}
    z^{(j)}_i = i, \quad \text{for all} \quad i \in [n],
\end{align*}
without the loss of generality, we consider the identity permutation $\vpi(i) = i$. We use the following initialization:
\begin{align*}
    x_1^{(k)} = \begin{cases} 1, \qquad k = j,\\ 0, \qquad k \ne j;\end{cases} \quad\text{and}\qquad y_1^{(k)} = 0, \qquad \forall k \in [m].
\end{align*}
We have then $\| W_1 - W^*\| = \bigO{1}$. This allows us to focus on the $j$-th component only and hence we suppress the upscripts. In this context, the stochastic loss function $g$ on this copy is written as
    \begin{equation}
        g(w, z_i) = \frac{\alpha}{2} (x )^2 + \frac{1}{2} \|y \|^2 - \frac{x \sqrt{\alpha}}{n} y (i), \qquad \forall i \in [n].
    \end{equation}
From the above construction, GD formulates the following update
\begin{align*}
        w_{t+1} = w_t - \frac{\eta}{n} \sum_{i=1}^n \nabla_w g(w_t, z_i)
    \end{align*}
    with initialization $x_1 = 1$, $y_1 = 0$.
The stochastic gradient is computed as 
    \begin{align}
        \nabla_x g(w, z_i) = \alpha x - \sqrt{\alpha}y(i), \qquad \nabla_{y} g(w, z_i) = (y(i) - \sqrt{\alpha}x) \cdot \ve_i.
    \end{align}

    Since all coordinates in $y$ are equivalent in the construction, we suppress the index of $i$ and write $y_t = y_t(i)$ for any $i \in [n]$, $t \in [T]$. Then it formulates
    \begin{align*}
        & x_{t+1} = x_t - \eta \alpha x_t + \frac{\eta \sqrt{\alpha}}{n}\sum_{i=1}^ny_t(i) = (1-\alpha \eta)x_t + \eta \sqrt{\alpha}y_t, \\
        & y_{t+1} = y_t - \frac{\eta}{n}y_t + \frac{\eta\sqrt{\alpha}}{n}x_t = \left(1 - \frac{\eta}{n} \right)y_t + \frac{\eta\sqrt{\alpha}}{n}x_t.
    \end{align*}

    We next provide both upper and lower bounds for $x_t$ and $y_t$. 
    We give an upper bound for $x_t$ and $y_t$ by the following induction. If condition
    \begin{equation} \label{eq: induction-GD-2}
        x_{t} \leq 1, \qquad y_t \leq \sqrt{\alpha}
    \end{equation} 
    holds for $t$, then the above condition also holds for $t+1$:
    \begin{align*}
        x_{t+1} & \leq (1 - \alpha \eta) + \eta\sqrt{\alpha} \cdot \sqrt{\alpha} = 1 - \frac{\eta}{\eta T} + \frac{\eta }{\eta T}  \leq 1, \\
        y_{t+1} & \leq \left(1 - \frac{\eta}{n} \right) \sqrt{\alpha} + \eta \frac{\sqrt{\alpha}}{n} = \sqrt{\alpha}.
    \end{align*}
    Then by induction we conclude that \eqref{eq: induction-GD-2} is true.
    For any $t \in [T]$ with $T \geq 2$, the lower bound for $x_t$ is much simpler to compute under our choice of parameter $\alpha = C/(\eta T)$:
    \begin{align*}
        x_{t+1} & \geq (1 - \alpha \eta)  x_{t} \geq (1 - \alpha \eta )^tx_1 = 4^{-Ct/T} \geq 4^{-C}.
    \end{align*}
    Hence $\bar{x}_T = \frac{1}{T} \sum_{t=1}^T x_t = \Theta(1)$. This then allows us to lower bound $y$ at iteration $t \in [T]$:
    \begin{align*}
        y_t & \geq \left(1-\frac{\eta}{n}\right) y_{t-1} + \frac{\eta\sqrt{\alpha}}{4^Cn} \\
        & \geq \frac{\eta\sqrt{\alpha}}{4^Cn} \cdot \left( 1 + (1-\eta/n) + \cdots (1-\eta/n)^{t-1} \right) \\
        & \geq \frac{\eta\sqrt{\alpha}}{4^Cn} \cdot \frac{1 - (1-\eta/n)^t}{1 - (1 - \eta/n)} .
     \end{align*}
    Now, we discuss two cases: $\eta T = \bigO{n}$ and $\eta T = \bigOmega{n}$.
    \paragraph{Case $\eta T = \bigO{n}$.} We decompose $t = n \cdot \tfrac{t}{n}$ and obtain
    \begin{align*}
        y_t & \geq \frac{\eta\sqrt{\alpha}}{4^Cn} \cdot \frac{1 - (1-\eta/n)^t}{1 - (1 - \eta/n)} = \frac{\eta\sqrt{\alpha}}{4^Cn} \cdot \frac{1 - (1-\eta/n)^{\tfrac{t}{n} \cdot n}}{1 - (1 - \eta/n)} \\
        & \overset{\text{(A)}}{\geq} \frac{\eta \sqrt{\alpha}}{4^C}\left( \frac{t}{n} - \frac{\eta t^2}{2n^2}\right)  \overset{\text{(B)}}{=} \frac{\eta t \sqrt{\alpha}}{2\cdot 4^Cn} = \sqrt{\frac{\eta}{CT}} \cdot \frac{t}{2\cdot 4^Cn}%= \Omega\left( \frac{\sqrt{\eta t}}{n}\right).
    \end{align*}
    where $\text{(A)}$ is due to Taylor expansion, $\text{(B)}$ is due to the condition $\eta t \leq \eta T = \bigO{n}$ and $\alpha = C/(\eta T)$. We then calculate the average output
    \begin{align*}
        \bar{y}_T = \frac{1}{T} \sum_{t=1}^T y_t = \frac{1}{T} \sum_{t=1}^T \sqrt{\frac{\eta}{CT}} \cdot \frac{t}{2\cdot 4^Cn} \geq \frac{1}{4\cdot4^Cn} \cdot \sqrt{\frac{\eta T}{C}}.
    \end{align*}
    We return to the original $f(w,z)$ by inserting the above analysis on the $j$-th component:
    \begin{align*}
        \bbE[F(W_{\gd})] \geq \bbE[G(w^{(j)}_{\gd})] \geq \frac{1}{n} \sum_{i=1}^n \left(\frac{x^{(j)}_{\gd}}{\sqrt{\eta T}} - y^{(j)}_{\gd}(i)\right)^2 \geq \bigOmega{\max\left\{ \frac{1}{\eta T,} \ \frac{\eta T}{n^2 }\right\}} = \bigOmega{\frac{\eta T}{n^2 }}
    \end{align*}
    where the last inequality is due to the fact $4^{-C} \leq x_{\gd} \leq 1$. We can always choose a proper $C$ such that the difference is non-vanishing.
    \paragraph{Case $\eta T = \bigOmega{n}$.} We can directly lower bound $y_t$ as
    \begin{align*}
        y_t \geq \frac{\eta \sqrt{\alpha}}{4^C n} \cdot \frac{n}{2\eta} = \frac{1}{2\cdot4^C\sqrt{C \eta T}} = \bigOmega{\frac{1}{\sqrt{\eta T}}}
    \end{align*}
    since $(1 - \eta/n)^t \leq 1/2$ when $\eta T = \bigOmega{n}$. We then calculate the average output
    \begin{align*}
        \bar{y}_T = \frac{1}{T} \sum_{t=1}^T y_t = \frac{1}{T} \sum_{t=1}^T \bigOmega{\frac{1}{\sqrt{\eta T}}} \geq \bigOmega{\frac{1}{\sqrt{\eta T}}}.
    \end{align*}
    Similarly, we return to the original $f(w,z)$ by inserting the above analysis on the $j$-th component and obtain a non-vanishing lower bound by choosing proper $C$:
    \begin{align*}
        \bbE[F(W_{\gd})] \geq \bbE[G(w^{(j)}_{\gd})] \geq \frac{1}{n} \sum_{i=1}^n \left(\frac{x^{(j)}_{\gd}}{\sqrt{\eta T}} - y^{(j)}_{\gd}(i)\right)^2 \geq \bigOmega{\frac{1}{\eta T}}.
    \end{align*}
    This completes our proof.
\end{proof}
We proceed to prove the supporting lemmas.
\begin{lemma}
    \label{lemma: supp-1}
    Suppose $\alpha = \Theta(1/\eta T)$, then $f$ is $1$-smooth, convex and realizable over $D$.
\end{lemma}
\begin{proof}
    When condition $\eta T \geq 1$ holds, it is easy to check that $g(w,z)$ is $1$-smooth and convex for any $w \in \bbR^{n+1}$ and $z \in [n]$. The population risk $G$ is 
    \begin{align*}
        G(w) = \bbE_{z\sim\text{Unif}([n])}[g(w,z)] = \frac{\alpha}{2} x^2 + \frac{1}{2n} \|y\|^2 - \frac{\sqrt{\alpha}}{n}x \cdot \vone^\top y = \frac{1}{2n} \left\| y - \sqrt{\alpha} x \cdot \vone \right\|^2,
    \end{align*}
    which attains minimum at $(x^*, y^*) = (0, 0)$. So $g(w,z)$ satisfies the realizable condition.  It is easy to conclude $f$ is also $1$-smooth, convex and realizable.
\end{proof}

\begin{lemma}
\label{lemma: supp-2}
    Consider dataset $S = \{ Z_1, \dots, Z_n\}$ defined in Lemma~\ref{lemma: lb-gd-tn}. Suppose $m$ is a positive integer satisfying $ m = \Theta(e^n/\sqrt{n})$, then there exists at least one component $z^{(j)}, j \in [m]$ such that
    \begin{align*}
        z^{(j)}_i = \vpi(i), \quad \text{for all} \quad i \in [n]
    \end{align*}  
    where $\vpi:[n] \to [n]$ is any permutation on $[n]$.
\end{lemma}
\begin{proof}
    We define the following probability event: given dataset $S = \{ Z_1, \dots, Z_n\}$, we focus on the $j$-th component $\{z^{(j)}_1, \dots ,z^{(j)}_n\}$ and define event $\cE_j$ as 
    \begin{equation*}
        \cE_j = \left\{ z^{(j)}_i = \vpi(i) \text{ for any } i \in [n] \right\}
    \end{equation*}
    where $\vpi:[n] \to [n]$ is any fixed permutation on $[n]$. Intuitively, when $\cE_j$ happens, each coordinates of $y^{(j)}$ is selected only for once in dataset $S$. For any fixed $j \in [m]$, the probability of $\cE_j$ happens is calculated from the without-replacement sampling: 
    \begin{align*}
        p := \pr[\text{Event } \cE_j \text{ happens}] = 1 \cdot \frac{n-1}{n} \cdots \cdot  \frac{1}{n} = \frac{n!}{n^n} = \Theta(\sqrt{n}\cdot e^{-n})
    \end{align*}
    where the last step is from Stirling approximation $\sqrt{2\pi n } \left(\frac{n}{e}\right)^n e^{\tfrac{1}{12n+1}} < n! < \sqrt{2\pi n } \left(\frac{n}{e}\right)^n e^{\tfrac{1}{12n}}$ for any $n \geq 1$. ensures event $\cE_j$ to happen.

    We now prove that with $\bigOmega{1}$ probability, there exists at least a $j \in [m]$ such that $\cE_j$ happens using the second moment method. Denote $R$ to be the random variable counting the number of $\{\cE_j\}_{j \in [m]}$ happens. Using second moment method, we upper bound the following probability: 
    \begin{equation}
        \pr[R > 0] \geq \frac{(\bbE[R])^2}{\bbE[R^2]} = \frac{m^2p^2}{mp(1-p)} \geq \frac{mp}{2} = \frac{1}{2}.
    \end{equation}
    So with probability $\bigOmega{\frac{1}{2}}$, we have at least one copy fulfilling the statement.
\end{proof}

\subsection{Proof of Lemma~\ref{lemma: lb-sgd-tn}} \label{appendix: lb-sgd}
\begin{proof}
    We utilize the similar strategy employed in Lemma~\ref{lemma: lb-gd-tn} and consider the same $f(W,Z)$ defined in Eq.~\eqref{eq: lb-instance1}, Eq.\eqref{eq: lb-instance2}. Lemma~\ref{lemma: supp-2} (see below) shows that: with constant probability, there exists at least one copy of $\{z^{(j)}_i\}_{i\in[n]}$ satisfying (without the loss of generality, we consider the identical permutation $\vpi(i) = i$)
    \begin{align*}
        z^{(j)}_i = i, \quad \text{for all} \quad i \in [n].
    \end{align*}
    We use the following initialization:
    \begin{align*}
        x_1^{(k)} = \begin{cases} 1, \qquad k = j,\\ 0, \qquad k \ne j;\end{cases} \quad\text{and}\qquad y_1^{(k)} = 0, \qquad \forall k \in [m].
    \end{align*}
    We have then $\| W_1 - W^*\| = \bigO{1}$. This allows us to focus on the $j$-th component only and hence we suppress the upscripts. In this context, the stochastic loss function $g$ on this copy is written as
    \begin{equation}
        g(w, z_i) = \frac{\alpha}{2} (x )^2 + \frac{1}{2} \|y \|^2 - \frac{x \sqrt{\alpha}}{n} y (i), \qquad \forall i \in [n].
    \end{equation}
    SGD formulates the update:
    \begin{align*}
        w_{t+1} = w_{t+1} - \eta g(w_t, z_{i_t}) 
    \end{align*}
    where $z_{i_t} \sim \text{Unif}([n])$. The initialization is $x_1 = 1$, $y_1 = 0$. Based on the current value of $w_t$, under expectation, we have
    \begin{align*}
        \bbE[w_{t+1}] = w_t - \frac{\eta}{n} \sum_{i=1}^n \nabla_w g(w_t, z_i).
    \end{align*}
    We write the update of $\bbE[x_t]$ and $\bbE[y_t]$ by plugging stochastic gradients: it easy to see all coordinates in $\bbE[y_t]$ are equivalent, we suppress the index of $i$ and write $y_t = y_t(i)$ for any $i \in [n]$, $t \in [T]$. Then it formulates
    \begin{align*}
        & \bbE[x_{t+1}] = x_t - \eta \alpha x_t + \frac{\eta \sqrt{\alpha}}{n}\sum_{i=1}^ny_t(i) = (1-\alpha \eta)x_t + \eta \sqrt{\alpha}y_t, \\
        & \bbE[y_{t+1}] = y_t - \frac{\eta}{n}y_t + \frac{\eta\sqrt{\alpha}}{n}x_t = \left(1 - \frac{\eta}{n} \right)y_t + \frac{\eta\sqrt{\alpha}}{n}x_t.
    \end{align*}
    We give an upper bound for $\bbE[x_t]$ and $\bbE[y_t]$ by the following induction. If condition
    \begin{equation} \label{eq: induction-SGD-2}
        \bbE[x_{t}] \leq 1, \qquad \bbE[y_t] \leq \sqrt{\alpha}
    \end{equation} 
    holds for $t$, then the above condition also holds for $t+1$:
    \begin{align*}
        \bbE[x_{t+1}] & \leq (1 - \alpha \eta) + \eta\sqrt{\alpha} \cdot \sqrt{\alpha} = 1 - \frac{\eta}{\eta T} + \frac{\eta }{\eta T}  \leq 1, \\
        \bbE[y_{t+1}] & \leq \left(1 - \frac{\eta}{n} \right) \sqrt{\alpha} + \eta \frac{\sqrt{\alpha}}{n} = \sqrt{\alpha}.
    \end{align*}
    Then by induction we conclude that \eqref{eq: induction-SGD-2} is true. For any $t \in [T]$ and $T \geq 2$, the lower bound for $x_t$ is much simpler to compute under our choice of parameter $\alpha = C/(\eta T)$:
    \begin{align*}
        \bbE[x_{t+1}] & \geq (1 - \alpha \eta)  x_{t} \geq (1 - \alpha \eta )^tx_1 = 4^{-t/T} \geq 4^{-C}.
    \end{align*}
    Hence $\bbE[\bar{x}_T] = \frac{x_1}{T} + \frac{1}{T} \sum_{t=2}^T \bbE[x_t|w_{t-1}] = \Theta(1)$. This then allows us to lower bound $y$ at iteration $t \in [T]$:
    \begin{align*}
        \bbE[y_{t}] & \geq \left(1-\frac{\eta}{n}\right) y_{t-1} + \frac{\eta\sqrt{\alpha}}{4^Cn} \\
        & \geq \frac{\eta\sqrt{\alpha}}{4^Cn} \cdot \left( 1 + (1-\eta/n) + \cdots (1-\eta/n)^{t-1} \right) \\
        & \geq \frac{\eta\sqrt{\alpha}}{4^Cn} \cdot \frac{1 - (1-\eta/n)^{t}}{1 - (1 - \eta/n)} .
     \end{align*}
     Now, we discuss two cases: $\eta T = \bigO{n}$ and $\eta T = \bigOmega{n}$.
    \paragraph{Case $\eta T = \bigO{n}$.} We decompose $t = n \cdot \tfrac{t}{n}$ and obtain
    \begin{align*}
        \bbE[y_t] & \geq \frac{\eta\sqrt{\alpha}}{4^Cn} \cdot \frac{1 - (1-\eta/n)^t}{1 - (1 - \eta/n)} = \frac{\eta\sqrt{\alpha}}{4^Cn} \cdot \frac{1 - (1-\eta/n)^{\tfrac{t}{n} \cdot n}}{1 - (1 - \eta/n)} \\
        & \overset{\text{(A)}}{\geq} \frac{\eta \sqrt{\alpha}}{4^C}\left( \frac{t}{n} - \frac{\eta t^2}{2n^2}\right)  \overset{\text{(B)}}{=} \frac{\eta t \sqrt{\alpha}}{2\cdot4^Cn} = \sqrt{\frac{\eta}{CT}} \cdot \frac{t}{2\cdot4^Cn}%= \Omega\left( \frac{\sqrt{\eta t}}{n}\right).
    \end{align*}
    where $\text{(A)}$ is due to Taylor expansion, $\text{(B)}$ is due to the condition $\eta t \leq \eta T = \bigO{n}$ and $\alpha = C/(\eta T)$. We then calculate the average output
    \begin{align*}
        \bbE[\bar{y}_T] = \frac{1}{T} \sum_{t=1}^T \bbE[y_t] = \frac{1}{T} \sum_{t=1}^T \sqrt{\frac{\eta}{CT}} \cdot \frac{t}{2\cdot4^Cn} \geq \frac{1}{4\cdot 4^Cn} \cdot \sqrt{\frac{\eta T}{C}}.
    \end{align*}
    We return to the original $f(w,z)$ by inserting the above analysis on the $j$-th component:
    \begin{align*}
        \bbE[F(W_{\sgd})] \geq \bbE[G(w^{(j)}_{\sgd})]  \geq G(\bbE[w_{\sgd}^{(j)}]) & = \frac{1}{n} \sum_{i=1}^n \left(\frac{x^{(j)}_{\sgd}}{\sqrt{\eta T}} - y^{(j)}_{\sgd}(i)\right)^2 \\
        & \geq \bigOmega{\max\left\{ \frac{1}{\eta T,} \ \frac{\eta T}{n^2 }\right\}} = \bigOmega{\frac{\eta T}{n^2 }}
    \end{align*}
    where the second inequality comes from Jensen's inequality and the last inequality is due to the fact $4^{-C} \leq x_{\gd} \leq 1$. We can always choose a proper $C$ such that the difference is non-vanishing.
    \paragraph{Case $\eta T = \bigOmega{n}$.} We can directly lower bound $\bbE[y_t]$ as
    \begin{align*}
        \bbE[y_t] \geq \frac{\eta \sqrt{\alpha}}{4^C n} \cdot \frac{n}{2\eta} = \frac{1}{2\cdot4^C\sqrt{C \eta T}} = \bigOmega{\frac{1}{\sqrt{\eta T}}}
    \end{align*}
    since $(1 - \eta/n)^t \leq 1/2$ when $\eta T = \bigOmega{n}$. We then calculate the average output
    \begin{align*}
        \bbE[\bar{y}_T] = \frac{1}{T} \sum_{t=1}^T \bbE[y_t] = \sum_{t=1}^T \bigOmega{\frac{1}{\sqrt{\eta T}}} \geq \bigOmega{\frac{1}{\sqrt{\eta T}}}.
    \end{align*}
    Similarly, we return to the original $f(w,z)$ by inserting the above analysis on the $j$-th component and obtain a non-vanishing lower bound by choosing proper $C$:
    \begin{align*}
       \bbE[F(W_{\sgd})] \geq \bbE[G(w^{(j)}_{\sgd})]  & \geq G(\bbE[w_{\sgd}^{(j)}]) = \frac{1}{n} \sum_{i=1}^n \left(\frac{x^{(j)}_{\sgd}}{\sqrt{\eta T}} - y^{(j)}_{\sgd}(i)\right)^2 \geq \bigOmega{\frac{1}{\eta T}}.
    \end{align*}
    This completes our proof.
\end{proof}


