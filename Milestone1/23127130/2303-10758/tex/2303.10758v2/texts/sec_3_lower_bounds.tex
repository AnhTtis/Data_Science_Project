\section{Main Theorems: Lower Bounds in Smooth SCO} \label{sec: lb}
In this section we present our main results on lower bounds for the smooth SCO.
We aim to reduce the gaps mentioned in Section~\ref{sec: setting} for all the above settings. To this end, we will split our discussion to three parts: (1) non-realizable, (2) realizable with $\eta T = \bigO{n}$ and (3) realizable with $\eta T = \bigOmega{n}$.

\subsection{Non-realizable Setting} \label{sec: nonrealizable}
We first discuss the non-realizable setting and provide a novel lower bound for the excess risk of GD in the following theorem.
\begin{theorem} \label{thm: lb-sco-gd}
    For any $\eta > 0$, $T > 1$ with $1/T \leq \eta = \bigO{1}$\footnote{This is a mild condition because (1) step-size cannot exceed $\bigO{1}$, in order to make the optimization method converge for $\bigO{1}$-smooth function, (2) an overly small step-size $\eta$ cannot even guarantee the convergence in the optimization sense and $T$ is arbitrarily large to ensure $\eta T \geq 1$. We will assume this holds in the statement of rest theorems and lemmas.}, there exists a convex, $1$-smooth $f(w,z): \bbR \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with a bounded initialization $\| w_1 - w^* \| = \bigO{1}$, the output $w_{\gd}$ for GD satisfies
    \begin{align*}
        \bbE[F(w_T)] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{\eta T}{n}}.
    \end{align*}
\end{theorem}
The lower bound in Theorem~\ref{thm: lb-sco-gd} tightly matches the corresponding upper bound established in \cite{hardt2016train} (see Table~\ref{tab: summary}). It can be translated to a lower bound of sample complexity: for any $T > 1$, by setting $\eta = \sqrt{n}/T$, we derive a $\bigOmega{1/\sqrt{n}}$ bound which certifies the optimality of existing upper bound  $\bigO{1/\sqrt{n}}$. To the best of our knowledge, this is the first such result for GD. A recent work provides lower bound for the uniform stability of (S)GD \citep{zhang2022stability} under smooth SCO, but it does not directly imply a bound on the excess risk.

The key step in the proof of Theorem~\ref{thm: lb-sco-gd} is to find a hard instance that gives an overfitting lower bound $\bigOmega{\eta T/n}$. To this end, we employ a technique inspired by Theorem 3 and Lemma 7 in \citet{sekhari2021sgd}: in non-realizable setting, the stochastic gradient does not necessarily scale down with the value of $f(w,z)$. As a result, by utilizing an \emph{anti-concentration} argument, we show that with non-vanishing probability $\bigOmega{1}$, the absolute value of $w_t$ increases by a rate of $\bigOmega{\eta/\sqrt{n}}$ in each step. Then, calculation suggests a $\bigOmega{\eta T/n}$ bound for the function value. The details can be found in Appendix~\ref{appendix: lb-sco-gd}. In the meanwhile, the term $\bigOmega{1/\eta T}$ reflects the optimization error and the proof is provided in Lemma~\ref{lemma: lb-suboptimality}, Appendix~\ref{appendix: lb-suboptimality}. 

A similar result holds for SGD in the following theorem.
\begin{theorem} \label{thm: lb-sco-sgd}
    For any $\eta > 0$, $T > 1$, there exists a convex, $1$-smooth $f(w,z): \bbR \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with a bounded initialization $\| w_1 - w^* \| = \bigO{1}$, %with probability $\Theta(1)$, 
    the output $w_{\sgd}$ for SGD satisfies
      \begin{align*}
        \bbE[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{\eta T}{n}}.
    \end{align*}
\end{theorem}
This also matches the SGD upper bound in \cite{hardt2016train} and implies a sample complexity bound $\bigOmega{1/\sqrt{n}}$ if we set $\eta = \sqrt{n}/T$ for any $T$.  

We emphasize the bound for SGD is novel compared with existing works: it is a forklore that in \citet{nemirovskij1983problem}, single-pass SGD ($ T = n $) achieves a sample complexity lower bounds for Lipschitz convex functions (where a smooth function within a bounded domain is automatically Lipschitz). Yet, our result is the first to provide an explicit dependence on $T$ and $\eta$ and applies to an arbitrary number of updates. It shows that training longer can provably lead to overfitting, and answers the question raised in the introduction for the non-realizable setting.

\subsection{Realizable Setting: \texorpdfstring{$\eta T = O(n)$}{eta T=O(n)}} \label{sec: t_equal_n}
Better stability-based generalization upper bound are known for realizable problems. However, we will see that our lower bounds suggest known results may not be tight. In this subsection we first provide our lower bounds for the realizable setting when condition $\eta T = \bigO{n}$ is satisfied. The next theorem  characterizes the lower bounds for GD and SGD.
\begin{theorem} \label{thm: lb-1}
For every $\eta > 0$, $T > 1$, if condition $T = \cO(n)$ holds, then there exists a convex, $1$-smooth and realizable $f(w, z): \bbR^d \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with a bounded initialization $\| w_1 - w^* \| = \bigO{1}$, the output $w_{\gd}$ for GD satisfies
\begin{align*}
    \bbE[F(w_{\gd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{1}{n} + \frac{\eta T}{n^2}}.
\end{align*}
Similarly, the output $w_{\sgd}$ for SGD satisfies
\begin{align*}
    \bbE[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{1}{n} + \frac{\eta T}{n^2}}.
\end{align*}
\end{theorem}

It is worth noting that we assume bounded initialization $\| w_1 - w^* \| = \bigO{1}$. This is standard and necessary in the generalization literature: the bound will be vacuous and arbitrarily bad if initial point is away from the optimal point with infinite distance.

Similar to the non-realizable setting, the term $\bigOmega{1/(\eta T)}$ reflects the optimization error. In the meanwhile, the term $\bigOmega{1/n}$ comes from a universal hard instance that holds for any deterministic or stochastic gradient methods. The major challenge in lower bound construction is the proof for the term $\bigOmega{\eta T/n^2}$.  Notice that the term $\bigOmega{1/n}$ does not suggest the rest two terms are vacuous since they are hard in the sense of characterizing the relationship between $\eta$, $T$ and $n$. 

Theorem~\ref{thm: lb-1} suggests that known upper bounds are tight not only in sample complexity but also in $T$ and $\eta$ dependence. More specifically, the lower bound for GD tightly matches the upper bound in \cite{nikolakakis2022beyond}, and the lower bound for SGD almost tightly matches the lower bound in \cite{lei2020fine} up to a $\eta$ factor in the second term. Please refer to Table~\ref{tab: summary} for a comparison.


 We will combine the discussion for GD and SGD due to their similarity. Both upper and lower bounds are non-vacuous only when $T = \Theta(n)$ and $\eta = \Theta(1)$: under this configuration, we obtain the optimal sample complexity lower bound $\bigOmega{1/n}$ from Theorem~\ref{thm: lb-1}, which matches the sample complexity upper bound $\bigO{1/n}$ under the regime of $\eta T = \bigO{n}$. We will see in the next subsection that the conclusion is different when $\eta T = \bigOmega{n}$.
% \jiaye{I am confused, does the lower bound hold for $\cO(n)$ or $\Theta(n)$?} \peiyuan{Jingzhao suggested using $\bigO{n}$ instead of $\Theta(n)$.}



%Therefore, we emphasize that our lower bound construction does not only provide the worst instance on sample complexity but also reflects possibly worst growth of generalization error along time. 

%Theorem~\ref{thm: lb-sgd-1} shows that under realizable smooth SCO, SGD enjoys the same hardship with GD. The lower bound matches the upper bound of Eq.~\ref{eq: ub-sgd-tn} in \cite{lei2020fine} up to a factor of $\eta$ in the second term. It implies a sample complexity lower bound when $T = \bigO{n}$: when we select $T = n$, $\eta = 1$, it achieves the optimal sample complexity $\bigOmega{1/n}$. This also matches the corresponding upper bound.

\subsection{Realizable Setting:  \texorpdfstring{$\eta T = \bigOmega{n}$}{eta T=Omega(n)}} \label{sec: t_larger_than_n}
In this subsection we focus on the case that allows large or infinite training horizon $\eta T = \bigOmega{n}$. We provide lower bounds for different algorithms and discuss their relationship with upper bounds, as stated in the following theorem.
\begin{theorem} \label{thm: lb-2}
For every $\eta > 0$, $T > 1$, if condition $\eta T = \bigOmega{n}$ holds, then there exists a convex, $1$-smooth and realizable $f(w, z): \bbR^d \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with a bounded initialization $\| w_1 - w^* \| = \bigO{1}$, the output $w_{\gd}$ for GD satisfies
\begin{align*}
    \bbE[F(w_{\gd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{1}{n}}.
\end{align*}
Similarly, the output $w_{\sgd}$ for SGD satisfies
\begin{align*}
    \bbE[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{1}{n}}.
\end{align*}
\end{theorem}
Here,  GD and SGD again have the same upper and lower bounds. Theorem~\ref{thm: lb-2} indicates that, different from the case $\eta T = \bigO{n}$, our lower bound for both GD and SGD does not match the corresponding upper bounds in \cite{lei2020fine,nikolakakis2022beyond} (see Table~\ref{tab: summary}). For lower bound, we achieve best sample complexity $\Omega(1/n)$ as long as $\eta T \geq n$, whereas for upper bound, the best sample complexity $O(1/n)$ is obtained \emph{only} when we set $\eta T = n$. To conclude, despite the lower bound of sample complexity certifies the optimality of its upper bound, the upper and lower bound still suggest different behavior of generalizarion error: while the upper bound indicates longer training leads to overfit, the lower bound suggests the opposite under the realizable setting.

To obtain the lower bound in Theorem~\ref{thm: lb-2} we employ a strategy similar to the proof of Theorem~\ref{thm: lb-1}. Term $\bigOmega{1/n}$ comes from the universal sample hardness for any algorithm, and term $\bigOmega{1/(\eta T)}$ is obtained from the construction used to prove $\bigOmega{\eta T/n}$ in Theorem~\ref{thm: lb-1}. Albeit the identical construction, the difference between bounds comes from lower bounding $1 - (1-\eta/n)^T$ in two regimes: when $\eta T = \bigO{n}$, we have $1 - (1-\eta/n)^T = \bigOmega{1}$ and when $\eta T = \bigOmega{n}$, we have $1 - (1-\eta/n)^T = \bigOmega{\eta T/n}$. This then leads to a difference in the absolute value of each  coordinates and in the end the difference of overall lower bounds. The details are postponed to Appendix~\ref{appendix: lb-gd-t}.

We conjecture the sample complexity bound under a large or infinite time horizon can be closed by proving upper bound $\bigO{1/n}$ is achievable for GD even when $\eta T$ goes to infinity. We will discuss the conjecture and provide several evidences in Section~\ref{sec: infinite}.% In the next theorem, we proceed to the lower bound for SGD.
%\begin{theorem} \label{thm: lb-sgd-2}
%For every $\eta > 0$, $T > 1$, there exists a convex, $1$-smooth and realizable $f(w, z): \bbR^d \to \bbR$ for every $z \in \cZ$, and a distribution $D$ such that, with a bounded initialization $\| w_1 - w^* \| = \bigO{1}$, the output $w_{\sgd}$ for SGD satisfies
%\begin{align*}
%    \bbE_{S\sim D^n}[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{1}{n}}.
%\end{align*}
%Similarly, the output $w_{\sgd}$ for SGD satisfies
%\begin{align*}
%   \bbE_{S\sim D^n}[F(w_{\sgd})] - F(w^*) = \bigOmega{\frac{1}{\eta T} + \frac{1}{n}}.
%\end{align*}
%\end{theorem}
%Similar to GD, generalization error for SGD has a sample complexity lower bound of $\bigOmega{1/n}$ when we set $\eta = 1$, for any $T = \bigOmega{n}$. This does not match the upper bound $\bigO{1/n^{2/3}}$ from Eq.~\eqref{eq: ub-sgd-t}. We also conjecture the gap can be closed by proving upper bound $\bigO{1/n}$ for SGD, which we will discuss in next section. The proof to the theorem can be found in Appendix~\ref{appendix: t_larger_than_n}.

