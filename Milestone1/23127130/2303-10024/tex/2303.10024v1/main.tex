\documentclass[letterpaper, 10 pt, journal, twosid]{ieeetran}

\IEEEoverridecommandlockouts    
\usepackage[hyphens]{url}
\usepackage{hyperref}
\hypersetup{colorlinks=false,breaklinks=true}
\usepackage{amsfonts,amsthm,amssymb,mathrsfs}
\usepackage{amsmath,mathtools}
\usepackage{algorithmic}
\usepackage{booktabs,tabularx}
\usepackage{cases}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{array}
\usepackage{xcolor}
%\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{marginnote}
\usepackage{multirow}
%\usepackage{lmodern}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore cyber-se-cu-ri-ty}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem{standing}{Standing Assumption}
%%\newtheorem*{proof*}{Proof}
%%\newtheorem{pf}{pf}%[section]
\newtheorem{problem}{Problem}
\newtheorem{iteration}{Algorithm}


\newcommand{\DM}[2]{  \textit{ \color{magenta} #1} \marginnote{\color{teal}#2}[-2cm]  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Filippo's commands
\newcommand{\mc}{\mathcal}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\X}{\mathbb{X}}
\newcommand{\U}{\mathbb{U}}

\usepackage{stfloats}
\usepackage{soul}
%\usepackage{caption}
%\usepackage{subcaption}
\usepackage{subfigure}


% NICER TABLES
\usepackage{booktabs}
% TABLES
\usepackage{footnote}
\usepackage{tabularx}
\usepackage{threeparttable} %Table
\usepackage{multirow} %Table: multirow
\usepackage{rotating} %Table: rotate text
\usepackage{bigstrut} % Table: extra row height
\usepackage{pifont}

\newcommand*\OK{\ding{51}}
\usepackage{adjustbox}

\newcolumntype{R}[2]{%
	>{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
	l%
	<{\egroup}%
}
\newcommand*\rot{\multicolumn{1}{R{60}{0.7em}}}

\renewcommand{\qedsymbol}{$\blacksquare$}

% COMMENTS
\setlength {\marginparwidth }{2cm}
\usepackage{todonotes}
\newboolean{showcomments}
\setboolean{showcomments}{true}         %\setboolean{showcomments}{false} 

\ifthenelse{\boolean{showcomments}}
{\newcommand{\nb}[2]{
		\fbox{\bfseries\sffamily\scriptsize#1}
		{\sf\small$\blacktriangleright$\textit{\textcolor{red}{#2}}$\blacktriangleleft$}
	}
}
{\newcommand{\nb}[2]{}
	\newcommand{\cvsversion}{}
}

\newcommand\dmnote[1]{\nb{DM}{#1}}
\newcommand\ffnote[1]{\nb{FF}{#1}}
\newcommand\ggnote[1]{\nb{GG}{#1}}
\newcommand\abnote[1]{\nb{AB}{#1}}
\newcommand{\lambdamin}{\lambda_{\mathrm{min}}}
\newcommand{\lambdamax}{\lambda_{\mathrm{max}}}
%OTHER COMMENTS

\usepackage{scalerel}
\usepackage{tikz}

% \usetikzlibrary{svg.path}
% \definecolor{orcidlogocol}{HTML}{A6CE39}
% \tikzset{
% 	orcidlogo/.pic={
% 		\fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
% 		\fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
% 		svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
% 		svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
% 	}
% }
% \newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
% 				\begin{tikzpicture}[yscale=-1,transform shape]
% 					\pic{orcidlogo};
% 				\end{tikzpicture}
% 			}{|}}}}


% \author{Simone~Soderi\authorrefmark{1}\authorrefmark{3}$^{\textsuperscript{\orcidicon{0000-0002-1024-9470}}}$, Senior Member, IEEE, Daniele~Masti\authorrefmark{1}\authorrefmark{3}$^{\textsuperscript{\orcidicon{0000-0002-2889-2309}}}$, \\
%  Matti~H\"am\"al\"ainen\authorrefmark{4}$^{\textsuperscript{\orcidicon{0000-0002-6115-5255}}}$, Senior Member, IEEE, and
% Jari~Iinatti\authorrefmark{4}$^{\textsuperscript{\orcidicon{0000-0001-6420-1547}}}$, Senior Member, IEEE
% }
% \affil{IMT School for Advanced Studies, Lucca, Italy.}
% \affil{Centre for Wireless Communications, University of Oulu, Oulu, Finland.}
% \affil{CINI Cybersecurity Laboratory, Roma, Italy.}
% \corresp{CORRESPONDING AUTHOR: Simone Soderi (e-mail: simone.soderi@imtlucca.it).}
% \authornote{A preliminary version of this paper was presented at Seventh Nordic Workshop on System and Network Optimization for Wireless (SNOW'16). Authors have been partially supported by Consorzio Interuniversitario Nazionale per l'Informatica (CINI)  through  Research Project under Grant CA 01/2021 a.i. 2.}
% \markboth{Cybersecurity considerations for Communication Based Train Control}{Soderi \textit{et al.}}


\begin{document}

\title{Counter-example guided inductive synthesis of control Lyapunov functions for uncertain systems}
\author{Daniele Masti, Filippo Fabiani, Giorgio Gnecco, and Alberto Bemporad
\thanks{
The authors are with the IMT School for Advanced Studies Lucca, Piazza San Francesco 19, 55100, Lucca, Italy 
({\tt \footnotesize \{daniele.masti, filippo.fabiani, giorgio.gnecco, alberto.bemporad\}@imtlucca.it}).
% and with CINI Cybersecurity Laboratory, Roma, Italy. 
% email:~
D.~Masti is partially supported by Consorzio Interuniversitario Nazionale per l'Informatica (CINI)  through  Research Project under Grant CA 01/2021 a.i. 2.
}
}

% \markboth{Journal of \LaTeX\ Class Files,~Vol.~18, No.~9, September~2020}%
% {How to Use the IEEEtran \LaTeX \ Templates}

\maketitle         
\thispagestyle{empty}
\pagestyle{empty}    

% \authornote{A preliminary version of this paper was presented at Seventh Nordic Workshop on System and Network Optimization for Wireless (SNOW'16). Authors have been partially supported by Consorzio Interuniversitario Nazionale per l'Informatica (CINI)  through  Research Project under Grant CA 01/2021 a.i. 2.}
% \markboth{Cybersecurity considerations for Communication Based Train Control}{Soderi \textit{et al.}}

\begin{abstract}
We propose a counter-example guided inductive synthesis (CEGIS) scheme for the design of control Lyapunov functions and associated state-feedback controllers for linear systems affected by parametric uncertainty with arbitrary shape. 
% At each iteration of the scheme, t
In the CEGIS framework, a learner iteratively proposes a candidate control Lyapunov function and a tailored controller by solving a linear matrix inequality (LMI) feasibility problem, while a verifier either falsifies the current candidate by producing a counter-example to be considered at the next iteration, or it certifies that the tentative control Lyapunov function actually enjoys such feature. We investigate the Lipschitz continuity property of the global optimization problem solved by the verifier, which is key to establish the convergence of our method in a finite number of iterations. Numerical simulations %finally
confirm the effectiveness of the proposed approach.  
\end{abstract}

\begin{IEEEkeywords}
Uncertain systems, Computer-aided control design, Lyapunov methods.
\end{IEEEkeywords}

%\IEEEspecialpapernotice{(Invited Paper)}

\maketitle

\section{Introduction}


\input{intro.tex}
\section{Problem definition and preliminaries}
\label{sec:background}
% \subsection{Problem definition}
% In this paper, we discuss how a CEGIS approach can be used to learn control Lyapunov functions for uncertain linear systems, also accompanied with linear state-feedback controllers. 
% In particular, we focus on discrete-time dynamical systems with state-space evolution described as follows: 
% In the work, we aim at designing quadratic common 
% control Lyapunov functions and related linear state-feedback controllers for %discrete-time
% uncertain linear systems of the form:
In this paper we aim at designing control Lyapunov functions and related state-feedback controllers for discrete-time
uncertain linear systems of the form:
\begin{align}
\Sigma : 
x(k+1)=A(k) x(k) +B(k) u(k),
\label{eq:Sigma}
\end{align}
where $x \in \mathbb{R}^n$ and $u \in \mathbb{R}^m$ denote the state and control vectors, respectively,
and $x$ is assumed fully available.
The dynamical matrices are so that $(A(k),B(k)) \in \Omega \subseteq \mc A \times \mc B$, for all $k \ge 0$, where $\mc A \subseteq \R^{n\times n}$ and $\mc B \subseteq \R^{n\times m}$. %It is worth remarking that systems of the form (\ref{eq:Sigma}) are able to represent also nonlinear discrete time-varying systems of the form $x(k + 1) = f (x(k), u(k), k)$, in cases in which the Jacobian $[\partial f/\partial x \,\, \partial f/\partial u]$ is known to belong to the set $\Omega$ for every pair $(x(k), u(k))$ \cite{kothare1996robust}.
Next, we introduce the main assumption adopted throughout:

\begin{standing}\label{standing:ass}
    % The following conditions hold:
    % \begin{enumerate}
    %     \item[i)] $\mc A$ and $\mc B$ are compact sets;

    %     \item[ii)] For each $(A(k),B(k)) \in \Omega$, $k\in \N$, $\Sigma$ is controllable. 
    % \end{enumerate}
    $\mc A$ and $\mc B$ are compact sets.
    \hfill$\square$
\end{standing}

As made evident later in the paper, a distinct feature of our approach will be the possibility to efficiently handle uncertain systems as in \eqref{eq:Sigma} i) without necessarily imposing any polytopic or convex structure to characterize the uncertainty,
and ii) sampling a (possibly large) number of matrices in $\mc A$ and $\mc B$.
Even though our CEGIS-based scheme will require the availability of $\Omega$ to solve the verifier task, we remark here that it is non-conservative as it makes neither inner nor outer approximations of such a set. 

%\subsubsection{Structure of this work}

%In this note, we explore an approach based on Common Lyapunov Functions to synthesize an AG to empower data-driven controllers with robust stability properties.

% \subsection{Notions of Lyapunov stability}\label{subsec:Lyap_stab}
% In the following, we recall some basic facts on Lyapunov stability for discrete-time dynamical systems.
We now recall some key notions on Lyapunov stability for discrete-time linear uncertain systems as $\Sigma$ in \eqref{eq:Sigma}. 
% Specifically, we start with the autonomous version of $\Sigma$ in \eqref{eq:Sigma} obtained by setting $B(k)=0$, for all $k$.
% , for all $j=1,\ldots,M$. 
% In addition, 
In particular, such a system is said to be \emph{stable} in some set $\mc X \subseteq \R^n$, with $0 \in \mc X$, if there exist a function $V : \mathbb{R}^n \to \mathbb{R}$ and some control law $u:\R^n\to\R^m$ so that \cite[Ch.~2.3]{blanchini2015set}:
% Let us then characterize the resulting dynamics through a state-successor pair $(x, x^+)$ so that $x^+ = A x$, for some $A\in\Omega=\mc A$.
% \ggnote{ma la matrice non stava nel conv di quelle? Scritto così non  sembra comparire un conv}. 
% Such system is \emph{stable} in $\mc X \subseteq \mathbb{R}^n$, with $0 \in \mc X$, if there exists $V : \mathbb{R}^n \to \mathbb{R}$ so that \cite[Ch.~2.3]{blanchini2015set}:
\begin{equation}
\begin{aligned}
%
V(x)> 0, & ~\forall x \in \mc X \setminus \{0\},\\
V(0) = 0 & \iff x = 0, \\
V(x^+)-V(x)\leq 0, & \; \forall (x, x^+),
%
\end{aligned}
\label{eq:LyapunovCondition}
\end{equation}
%\ggnote{Secondo me bisogna chiarire se vogliamo cercare una Lyapunov function che garantisca simple stability oppure asymptotic stability..la definizione di sopra (con V quadratica strettamente convessa) garantisce solo simple stability, mentre la risoluzione del sistema (3), nel caso politopico convesso, garantisce invece asymptotic stability..ho inserito alcuni commenti nel sorgente relativamente a come si potrebbe formulare la cosa in modo uniforme nel testo, a seconda che ci interessi alla fine garantire simple stability o asymptotic stability}
where $(x, x^+)$ denotes a generic state-successor pair, i.e., $x^+ = A x + B  u(x)$, for any possible $(A,B) \in \Omega$. %\abnote{[Chiarire meglio: qui sembra la formulazione sia per sistemi LTI. Se $V$ \`e control-Lyapunov per ogni sistema $(A,B)$ LTI in $\Omega$, allora lo \`e anche per il sistema LTV $A(k),B(k)$ variabili nel tempo, in maniera noto o incerta, in $\Omega$]}.
In this case, $V(\cdot)$ represents a (common)\emph{control Lyapunov function} with associated state-feedback controller $u(\cdot)$, and ensures closed-loop stability in $\mc X$
% , both for each Linear Time-Invariant (LTI) dynamical system with a given matrix pair $(A,B) \in \Omega$, and for the Linear Time-Varying (LTV) dynamical system 
to the trajectories originating from $\Sigma$ in \eqref{eq:Sigma} with matrices
% time-varying matrix pair 
$(A(k),B(k))$ varying in $\Omega$. Note that, in case the third condition holds as strict inequality, one may additionally recover convergence to the origin of the underlying closed-loop trajectories.
% , where its variation in time can be either known or uncertain.   
% When a control action is also considered, if for all $x\in\mc X$ there exists some feedback control law $\bar u:\R^n\to\R^m$ so that conditions \eqref{eq:LyapunovCondition} are met, then $V$ is called \emph{control Lyapunov function} 
% \ggnote{la scelta del controllo qui non può dipendere da $A_k$ e $B_k$ (che magari non si sa nemmeno quali siano), vero?} 
% and ensures closed-loop stability in $\mc X$. With a slight abuse of notation, the successor state in this case is the one produced by a state-input pair $(x,\bar u(x))$.

To preserve mathematical tractability while designing functions and controllers so that conditions \eqref{eq:LyapunovCondition} are met, a common choice is to rely on a quadratic form for $V(\cdot)$, i.e.,
% which turns \eqref{eq:LyapunovCondition} conditions for \emph{quadratic stability}, i.e.,
$
V(x)=x^\top Px,
$
with $P \in \mathbb{S}^{n}_{\succ 0}$, and a linear one for the controller, $u(x)=Kx$, for some gain matrix $K\in\R^{m\times n}$. Such a linear-quadratic parametrization for $V(\cdot)$ and $u(\cdot)$ is the one that will also be employed in the remainder. 
% is so that $P=P^\top \succ 0$.

We finally recall some facts regarding the well-known convex polytopic case, namely when $\Omega =\textrm{conv}(\{(A_1,B_1), \ldots, (A_M,B_M)\})$ with $M$ given generator matrices, that will be instrumental for the learner task.
% \ggnote{notazione un po' ambigua. Non si capisce dal papero se $\Omega$ è  un conv (quindi un insieme infinito) oppure un insieme finito (come sembra essere a p. 1}.
In that case, a control Lyapunov function with associated robust control law for $\Sigma$ can be designed by solving an LMI feasibility problem \cite[Ch.~7.3.3.2]{duan2013LMIs}:
% \\\ffnote{Mi sa che c'era un problema di segno sull'elemento $(1,2)$}
\begin{equation}\label{eq:baseLineK}
\left\{
\begin{aligned}
%\begin{array}{rcl}
&\underset{X,W}{\mathrm{min}} && 0\\
& ~\mathrm{ s.t.} &&
\begin{bmatrix}
X& X A^\top_j +W^\top B_j^\top\\ \star & X
\end{bmatrix} \succ 0,~j = 1,\ldots,M.
% &~~X  \succcurlyeq \varepsilon I, \notag
%\end{array}
\end{aligned}
\right.
\end{equation}

Given a solution pair $(X^\star, W^\star)$ to \eqref{eq:baseLineK} one hence obtains:
\begin{itemize}
\item A linear controller robustly stabilizing $\Sigma$ to the origin:
% i.e.,
\begin{equation}
u(x)=Kx=W^\star(X^\star)^{-1}x;
\label{eq:staticFeedback}
\end{equation}
\item A control Lyapunov function $V(x)=x^\top Px$, $P=(X^\star)^{-1}$, for the system \eqref{eq:Sigma} with linear controller \eqref{eq:staticFeedback}.
% , i.e., $\{(A_1,B_1), \ldots, (A_M,B_M)\}$. 
% \ggnote{Cioè, è una common Lyapunov function per i sistemi autonomi che si ottengono inserendo l'espressione 
% (\ref{eq:staticFeedback}) del controllo dentro l'equazione di stato?} \DM{yep}{}
\end{itemize}

% \DM{ho modificato il codice per imporre il vincolo di (5) invece che quello di contrazione di khotare, lasciando inalterato il resto (quello della funzione di costo). Funziona lo stesso. Di conseguenza tutto il resto dovrebbe tornare, fatto salvo lo sbattimento di doverlo scrivere.}{}

% This can be easily shown by nothing that the constraint in Problem \eqref{eq:baseLineK} on the one hand, guarantees that $X \succ 0$ , but also that 
% \begin{equation}
%     X-\varepsilon I - (X A_j^\top +W^\top B^\top)(X-\varepsilon I)^{-1}(X A_j^\top  +W^\top B^\top)^\top\succcurlyeq 0.
%     \label{eq:intermediate}
% \end{equation}
% By pre and post multiplying \eqref{eq:intermediate} by $X^{-1}$ and applying the Woodbury's matrix identity~\cite{dacitare} X , it is possible to obtain 
% \DM{Controllate pure e correggete i conti. Io ho semplicemente sbobinato, ma gli errori stanno lì}{} \ggnote{qual è il riferimento da dove sei partito per la sbobinatura? Così almeno lo si guarda e non c'è da rifare i conti..}
% \begin{align}
%     P-\varepsilon I - (A_j+BK)^\top(X-\varepsilon I)^{-1}(A_j+BK)= \\
%     P-\varepsilon I- (A_j+BK)^\top P(A_j+BK) + \notag \\
%     -(A_j+BK)^\top \underbrace{(I-P \varepsilon I)^{-1}P \varepsilon I P}_{\Gamma}  (A_j+BK) \succeq 0.
%     \label{eq:intermediate2}
% \end{align}
% As a whole, this implies that
% \begin{equation}
%     P- (A_j+BK)^\top P(A_j+BK) \succeq \varepsilon I +(A_j+BK)^\top \Gamma (A_j+BK)
%     \label{eq:intermediate3}
%     \end{equation}
% Being $\Gamma \succ 0$, it follows that $P- (A_j+BK)^\top P(A_j+BK) \succeq 0$  $\forall A_j$ when the feedback policy $u=Kx$ is applied. In other words, Problem~\eqref{eq:baseLineK} ensures that a stabilizing feedback policy is found in all the provided vertexes.

% \DM{Ad essere sinceri che $\Gamma \succ $ non è proprio banale. Credo segua dal fatto che il max autovalore di X sia  $1/\varepsilon$, e di conseguenza la parentesi tonda non diventa mai negativo definita}{} \ggnote{Secondo me dipende dal fatto che le due matrici $I-P \varepsilon I$ e $P \varepsilon I P$ hanno la stessa base ortonormale di autovettori (gli stessi di $P$), il che dovrebbe rendere la verifica semplice da fare (si guarda che cosa succede per ogni autovalore).}
% Once Lyapunov Function has been obtained, it can be transformed into a ``Control Lyapunov Function'' (CLF) by reintroducing and explicit control function  $u_k$ in place of the state feedback $u_k=Kx_k$. Doing so we obtain that
% \begin{equation}
% V_u(x,u)= (Ax+Bu)^\top P(Ax+Bu)
% \end{equation}
% is a CLF for $\Sigma$.


% \textbf{From vertices to Constraint points}
% \vspace{0.5cm}

% To preserve mathematical tractability, a commonly-chosen form for (control) Lyapunov functions is therefore the quadratic one, i.e.,
% % which turns \eqref{eq:LyapunovCondition} conditions for \emph{quadratic stability}, i.e.,
% $
% V(x)=x^\top Px
% $
% where $P \in \mathbb{R}^{n \times n}$ is so that $P=P^\top \succ 0$. 
% Also in this work, we will still stick on such a parametrization, thus determining the following hypothesis space adopted throughout the paper:
% $$
%     \mathcal{H} = \{f : \mathbb{R}^n \to \mathbb{R} \mid f(x) = x^\top P x,~P=P^\top \succ 0 \},
% $$
% which hence reduces to $\mathbb S^n_{\succ 0}$, namely the cone of symmetric, positive definite, $n\times n$ matrices.
% \ffnote{Credo $\mathcal{H}$ alla fine riduca al cono delle matrici $n\times n$ pd, i.e., $\mathbb{S}_{\succ 0}^n$}

\section{Control Lyapunov functions design through counter-examples}
\label{sec:cegis}
%We now detail both the learner' and verifier's tasks and characterize the technical properties of the problems they are asked to solve, which are instrumental to finally establish the finite-time convergence of the overall procedure.

\subsection{The learner task}\label{subsec:learner}

The considerations above regarding \eqref{eq:baseLineK} directly apply to the CEGIS-based framework we are about to introduce. In fact, let $\mc C_i$ be the set of \textit{counter-examples} (or simply \textit{samples}) of dynamical matrices $(\hat A_j,\hat B_j)$ considered at the $i$-th iteration, $j=1,\ldots,i$. We rewrite the LMI constraints in \eqref{eq:baseLineK} to account for the elements of $\mc C_i$ as follows:
% \subsection{From polytopic uncertainty to CEGIS}
% \begin{subequations}
% \label{eq:baseLineK}
\begin{equation}\label{eq:LMICon2}
\left\{
\begin{aligned}
%\begin{array}{rcl}
&\underset{X,W}{\mathrm{min}}&&0\\
&~\mathrm{ s.t. }&&
\begin{bmatrix}
X&X \hat A_h^\top+ W^\top \hat B_h^\top\\ \star & X
\end{bmatrix} \succcurlyeq \varepsilon I,~h=1,\ldots,M_i, \\
&&& X \preccurlyeq \eta I,~ W \in \mc W,
% \begin{bmatrix}
% X&X \hat A_h^\top+ W^\top \hat B_h^\top & 0\\ \star & X & 0\\0&0&-X
% \end{bmatrix} \succcurlyeq 
% \begin{bmatrix}
% \varepsilon I_{2n} & 0\\ 0 & -\eta I_{n}
% \end{bmatrix},\\
% &&&\hspace{4.2cm} \text{for all }h=1,\ldots,M_i, \\
% &  \forall A_l \in C \\
% \label{eq:learnerFindK}
% &~~X \succeq  \varepsilon I, \notag
%\end{array}
\end{aligned}
% \end{subequations}
\right.
\end{equation}

% \ggnote{Secondo me la differenza fondamentale tra il problema~\eqref{eq:baseLineK} e il problema~\eqref{eq:LMICon2} è che nel primo caso si considerano tutti i vertici del politopo, mentre nel secondo caso si considerano solo gli ex-controesempi trovati nelle iterazioni precedenti.}\DM{è l'intero senso del paper, come spiegato nella prima sezione}

%\ggnote{non è che $X \succeq  \varepsilon I$ sia già implicato dalla~\eqref{eq:LMICon2}?} 

%\ggnote{Potrebbe essere utile chiarire (magari riportando un riferimento) che risolvendo la LMI~\eqref{eq:baseLineK} si fa in modo che i contro esempi non siano più tali (se effettivamente è così, come mi sembra di capire). Inoltre, nel formulare la LMI~\eqref{eq:baseLineK}, non è che anziché considerare tutti gli $A_l \in C$ sia sufficiente considerare tutti gli $A_l$ che sono vertici di ${\rm conv}(C)$?}\ffnote{Sono d'accordo sulla seconda parte -- ho riscritto LMI sopra in questo senso.}
\noindent where, in this case, the index $h$ allows us to enumerate the vertex matrices of $\mc C_i$, with $M_i \coloneqq |\textrm{vert}(\textrm{conv}(\mc C_i))|$, where clearly $M_i\le i$ and, for any $h=1,\ldots,M_i$, $(\hat A_h, \hat B_h) \in \textrm{vert}(\textrm{conv}(\mc C_i)) \subseteq \Omega$. The hyperparameters $\eta \ge \varepsilon>0$ can be chosen arbitrarily and are meant to upper (respectively, lower) bound the largest (smallest) eigenvalue of matrix $X$. Besides imposing some sort of contraction through strict positive-definiteness with $\varepsilon$, we will see that 
 $\eta$ and $\epsilon$ are hyperparameters of our CEGIS methodology that strongly affect its performance. 
Finally, we assume $\mc W \subset \R^{m\times n}$ to be a compact set for technical reasons clarified later.
% strictly convex and hence added to single out a unique solution from \eqref{eq:LMICon2}.
% \begin{remark}
% For the first step of the CEGIS algorithm, the $C$ set can be initialized with any valid realization of the matrix $A$.
% \hfill$\square$
% \end{remark}

At each iteration $i$ of our CEGIS scheme, the learner accomplishes two tasks: first, given the current set $\mc C_i$ of samples of matrix pairs, it identifies those counter-examples that are vertices of its convex hull, for a total of $M_i$ vertices; leveraging this information, it then solves the resulting convex problem \eqref{eq:LMICon2} with LMI constraints. 

Note that the same discussion made in \S \ref{sec:background} also applies here. Specifically, solving \eqref{eq:LMICon2} produces a linear gain $K_i$ as in \eqref{eq:staticFeedback} that makes $V_i=x^\top (X^\star)^{-1} x$ a control Lyapunov function for the surrogate convex polytopic system identified by generator matrices $(\hat A_h,\hat B_h) \in \textrm{vert}(\textrm{conv}(\mc C_i))$.
On the other hand, since \eqref{eq:LMICon2} heavily depends on the choice of hyperparameters $\eta \ge \varepsilon>0$, in case the synthesis process fails, this implies that the parametrization adopted does not allow to design a control Lyapunov function with associated controller for $\Sigma$, with the assigned $(\eta, \varepsilon)$. We will discuss possible solutions to this issue later in \S \ref{subsec:cegis}.

% \begin{remark}
% In case the synthesis process fails, this obviously implies that a common Lyapunov function for $\Omega$ does not exists in $\mathcal{H}$. \ggnote{non è che invece $\varepsilon$ è stato scelto troppo grande, oppure che magari potrebbe anche esistere una common Lyapunov function, ma non quadratica?} \DM{Non mi pare di aver scritto il contrario}{}
% \hfill$\square$
% \end{remark}

% \begin{remark}
% The same reasoning discussed in the previous Section can be also applied here to see that solving Problem \eqref{eq:LMICon2} indeed generates a feedback policy and a Lyapunov function valid for $ \forall~A_h \in \textrm{vert}(\textrm{conv}(C))$. \ggnote{perché invece non è più valida per gli altri elementi di $\textrm{conv}(C)$?}\DM{L'uno implica l'altro}{}
% \end{remark}



\subsection{The verifier task}\label{subsec:verifier}
% We now detail the construction of the verification procedure, which has as its primary goal to certify that a given candidate function is indeed a (control) Lyapunov function for $\Sigma$, namely $\Theta= \{f:\mathbb{R}^n\to\mathbb{R} \mid f \text{ is a common Lyapunov function for }\Sigma\}$.
The verifier aims at certifying that a given candidate function and associated controller, designed by exploiting samples in $\mc C_i$, define a control Lyapunov function with tailored control action for $\Sigma$ with $(A(k),B(k)) \in \Omega$.
% , for any $\Sigma$ with $(A,B) \in \Omega$. %\abnote{[for any $\Sigma$ with $A(k),B(k)\in\Omega$ ?]}
% \ggnote{questo punto non mi è molto chiaro..forse è over the whole of $\Omega$, però bisogna rivedere un attimo com'è scritta la definizione}.
% To do so, we first consider the autonomous case only, i.e, by setting $\mc B = \emptyset$:
% for all $j=1,\ldots,M$ \ggnote{controllare poi alla fine quando $M$ ci vuole e quando no..forse supponiamo che $\Omega$ (l'insiemone i cui vertici esploriamo solo in parte durante l'applicazione delle varie LMI) abbia $M$ vertici?}. 
% the extension to the controlled case with linear state-feedback, indeed, follows immediately.
% by repeating the same discussion we are about to introduce focusing on the set of closed-loop matrices originating from all pair matrices $(A,B) \in \Omega$ and some linear gain $K$. 
Thus, by specializing the third condition in \eqref{eq:LyapunovCondition} in case of a quadratic function and linear controller, we obtain:
\begin{align}\label{eq:quadratic}
&V(x^+)- V(x)= (x^+)^\top P x^+ - x^\top Px \notag \\
=& ~ x^\top ((A^\textrm{cl})^\top P A^\textrm{cl}-P)x \leq 0,~\forall x \in \mc X,~ (A,B) \in \Omega,
\end{align} 
where $A^\textrm{cl}\coloneqq A+BK$. In particular, for a given $P$ and gain matrix $K$, condition~\eqref{eq:quadratic} is verified for all $x \in \mc X$ whenever $(A^\textrm{cl})^\top P A^\textrm{cl}-P$ is negative semi-definite for all $(A,B) \in \Omega$. 
Thus, fixing $(A,B) \in \Omega$ and applying the Schur complement to \eqref{eq:quadratic} yields the 
 %following
symmetric matrix
% \ggnote{attenzione che non è ancora una LMI, manca il secondo membro}:
\begin{equation}\label{eq:Schur}
\Xi(A^\textrm{cl})\coloneqq\begin{bmatrix}
P & (A^\textrm{cl})^\top P \\
\star &P
\end{bmatrix},
\end{equation}
% \DM{Nonostante i miei tentantivi,per ora non sono riuscito a mettere nessun bound sull'autovalore minimo di $\Xi_P$, fatto salvo che per l'interlacing theorem è bounded da sopra dall'autovalore più piccolo di P}{}
whose eigenvalues are therefore all real. %Moreover, we note that the elements of $\Xi(A^\textrm{cl})$ depend % (at most) \ggnote{perché "at most"? forse meglio "affinely"? sempre che sia giusto come termine?} 
%affinely on the elements of $(A,B)$. 
% This fact will be useful later.
% \begin{remark}
% As $\Xi(\cdot)$ is symmetric, its eigenvalues are all real.
% \hfill$\square$
% \end{remark}

In view of the relation in \eqref{eq:Schur}, checking whether a given matrix $P=P^\top\succ0$ produces a control Lyapunov function for $\Sigma$ is thus equivalent to checking if the optimal value $\lambda^\star$ of the following optimization problem is non-negative:
% \ggnote{oppure weakly positive?}:
%the global minimizer(s) $m^*$ of
\begin{align}\label{eq:smallestEig}
\lambda^\star=\underset{(A,B) \in \Omega}{ \mathrm{min}} & \;  \lambda_{\mathrm{min}}(\Xi(A^\textrm{cl})),
% & \mathrm{vec}(A) \in \Omega \notag
\end{align}
where $\lambdamin(\Xi(\cdot))$ denotes the smallest eigenvalue of $\Xi(\cdot)$.
% , %, is smaller than zero.
% and here allows us to identify the set as $\Theta = \{P \in \mathbb{S}^n_{\succ0} \mid \lambda_{\mathrm{min}}(\Xi(A))\ge0,~\forall A\in\Omega\}$.

Therefore, at the $i$-th iteration of the procedure, the verifier takes the matrices $P_i = (X^\star)^{-1}$ and $K_i=W^\star(X^\star)^{-1}$ proposed by the learner solving \eqref{eq:LMICon2}, defines $A^\textrm{cl}_i \coloneqq A+BK_i$, and
% $
% \Xi(A)\coloneqq\begin{bmatrix}
% P_i & A^\top P_i \\
% \star &P_i
% \end{bmatrix},
% $
$
\Xi(A^\textrm{cl}_i) =
\left[
\begin{smallmatrix}
    P_i & (A^\textrm{cl}_i)^\top P_i\\
    \star & P_i
    \label{eq:closedLoopCondition}
\end{smallmatrix}\right]
$
 as in \eqref{eq:Schur}, where the subscript $i$ makes explicit the dependence on the learner tentative solution, and finally  solves \eqref{eq:smallestEig}. If $\lambda^\star <0$, then a minimizer pair $(A^\star, B^\star) = (\hat A_{i+1}, \hat B_{i+1})$ is taken as a counter-example. Otherwise, no counter-example exists. 

% For the controlled case, instead, the verifier shall also accounts for the linear gain $K_i=W^\star(X^\star)^{-1}$ proposed by the learner in order to define the following augmented matrix involving $A^\textrm{cl}_i \coloneqq A+BK_i$ for some pair $(A,B)$:
% $
% \Xi(A^\textrm{cl}_i) =
% \begin{bmatrix}
%     P_i & (A^\textrm{cl}_i)^\top P_i\\
%     \star & P_i
%     \label{eq:closedLoopCondition}
% \end{bmatrix}.
% $
% This latter hence requires to solve \eqref{eq:smallestEig} over all pairs $(A,B) \in \Omega$. If some counter-example exists so that $\lambda^\star <0$, in this case it will be of the type $(A^\star, B^\star) = (\hat A_{i+1}, \hat B_{i+1})$.
% \ggnote{Magari qui, anche a costo di essere un po' pedanti, si potrebbe accennare a che cosa si fa quando c'è anche la matrice $B$}
% \ffnote{$\Sigma$ è il sistema che stiamo considerando, forse si vuole dire che $\Omega$ è composto da matrici simmetriche?}
% \DM{sbagliato $\Sigma$ con $\Xi_P$. Decisamente una pessima scelta di nomi comunque}{}

% In both cases
Note that, however, \eqref{eq:smallestEig} amounts to solving a generally non-convex optimization problem that requires the availability of the set $\Omega$. Moreover, a global minimum must be attained to prove that no counter-example exists. On the other hand, it also features several properties that make finding a globally optimal solution an affordable task. To see this, we need to exploit the following result borrowed from traditional matrix perturbation theory:

\begin{lemma}[\hspace{-.01cm}\cite{hornjohnson1991}]%\cite{helmke1995eigenvalue}]
Consider two symmetric matrices $K$ and $L$ of the same dimension. Then, it holds that
\begin{equation}\label{eq:eig_lip}
| \lambdamin(K)-\lambdamin(K+L)| \leq || L||_{\mathrm{op}},
\end{equation}
where $|| \cdot||_{\mathrm{op}}$ is the operator norm of a matrix,  induced by the $l_2$-norm.
\hfill$\square$
\label{lemma:liplamdamin}
\end{lemma}
In other words, the minimum eigenvalue $\lambdamin(\cdot)$ of a symmetric matrix is a Lipschitz continuous map of the elements of the matrix. This follows as a by-product of the so-called Weyl's inequalities \cite{hornjohnson1991}. %, stating that the eigenvalues of  symmetric matrix are Lipschitz continuous with respect to their entries. %, and the pointwise operator $\mathrm{min}$ preserves Lipschitz continuity.

It is recalled here that any linear operator between two finite-dimensional normed spaces is bounded (this applies, in particular, to the operator $\Xi(\cdot)$, which maps some $A^\textrm{cl}$ to $\Xi(A^\textrm{cl})$). Since bounded linear operators preserve Lipschitz continuity, we are then able to prove the following result: 
% \ggnote{attenzione che nell'enunciato del prossimo risultato forse bisognerebbe ricordare di nuovo che stiamo considerando il caso senza $B$ (il problema è che se uno guarda la formulazione iniziale del problema di sintesi, lì c'è anche $B$)..meglio scrivere nell'enunciato che per il momento togliamo $B$, e poi semmai lo reintroduciamo in un remark successivo)}:
\begin{theorem}
Let $P = (X^\star)^{-1}$ and $K=W^\star(X^\star)^{-1}$ with $(X^\star,W^\star)$ be the solution to \eqref{eq:LMICon2} with a given $\eta\ge\varepsilon>0$. Then, there exists some constant $\ell = \ell(\varepsilon) \ge 0$ such that  
$$
| \lambdamin(\Xi(A^\mathrm{cl}))-\lambdamin(\Xi(A^\mathrm{cl}+\Delta A^\mathrm{cl}))| \leq \ell || \Delta A^\mathrm{cl}||_{\mathrm{op}},
$$
with $A^\mathrm{cl}=A+BK$ for $(A,B)\in\Omega$ and perturbation $\Delta A^\mathrm{cl}$ induced by some pair $(\Delta A, \Delta B)$.
\hfill$\square$
\label{theo:LipSigma}
\end{theorem}
\begin{proof}
 In view of the relation in \eqref{eq:eig_lip}, we have:
$$|\lambdamin(\Xi(A^\textrm{cl}))-\lambdamin(\Xi(A^\textrm{cl}+ \Delta A^\textrm{cl}))| \leq \left \|\begin{bmatrix}
0 & (\Delta A^\textrm{cl})^\top P \\
\star &0
\end{bmatrix} \right\|_{\rm op}$$
where %Without loss of generality, focusing on the
$\|\cdot \|_{\rm op}$-norm is the matrix norm induced by the $l_2$ one. This leads to:
$$
\begin{aligned}
&\left\|\begin{bmatrix}
0 & (\Delta A^\textrm{cl})^\top P \\
\star &0
\end{bmatrix} \right\|_{\rm op} =\underset{\|x\|_2=1}{\textrm{sup}} \left\|\begin{bmatrix}
0 & (\Delta A^\textrm{cl})^\top P& \\
\star &0
\end{bmatrix} \begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}\right\|_2 & \\
%&=\underset{\|x\|_2=1}{\textrm{sup}}\left\|\begin{bmatrix}
%(\Delta A^\textrm{cl})^\top P x_2 \\
%P^\top \Delta A^\textrm{cl} x_1
%\end{bmatrix}\right\|_2 \\
&= \underset{\|x\|_2=1}{\textrm{sup}} \sqrt{\|(\Delta A^\textrm{cl})^\top P\|_{\rm op}^2 \|x_2\|_2^2 + \|P^\top \Delta A^\textrm{cl}\|_{\rm op}^2 \|x_1\|_2^2} & \\
&=\|(\Delta A^\textrm{cl})^\top P\|_{\rm op} \leq \|\Delta A^\textrm{cl}\|_{\rm op} \|P\|_{\rm op} \leq \varepsilon^{-1}\|\Delta A^\textrm{cl}\|_{\rm op},
\end{aligned}
$$
where we have first used the fact that $\|(\Delta A^\textrm{cl})^\top P\|_{\rm op}=\|P^\top \Delta A^\textrm{cl}\|_{\rm op}$ to resolve the sup operator and, successively, of $\|P\|_{\rm op}=\lambda_{\mathrm{max}}(P) \leq \varepsilon^{-1}$, which follows by choosing some $P=P^\top\succ0$ as $P=(X^\star)^{-1}$, for any $X^\star$ solving~\eqref{eq:LMICon2}. Thus, setting $\ell = \ell(\varepsilon) \coloneqq \varepsilon^{-1}$ concludes the proof.
\end{proof}

% \begin{remark}
    
% We discussed only the autonomus systems case, but it is trivial to see that the same would hold for full state-feedback systems. In such a case, one would consider $
% A_{cl}=A+BK.
% $
% A similar reasoning would also apply in the case of an uncertain $B$ matrix.
% \end{remark}
% \DM{ho fatti due conti con la $B$ incerta. Mi sembra venga la stessa identica cosa che hai scritto te, perchè tanto la K è una costante in questa fase}

% {\color{red} %Si potrebbe cercare di studiare la costante di Lipschitz di $\lambda_{\mathrm{min}}(\Xi(A))$ (che in realtà dipende anche da $P$). Nel modello in cui c'è anche $K$, è possibile che $K$ non influisca sulla stima della costante di Lipschitz, perché  $K$ non moltiplica $A$. Update: facendo qualche conto iniziale (riportato di seguito), sembra che la costante di Lipschitz di $\lambda_{\mathrm{min}}(\Xi(A))$ sia maggiorata da proprio da $\|P\|_{\rm op}$ (che in questo caso coincide con il suo autovalore più grande), e che questo valga anche quando c'è la $K$ di mezzo. Dal momento che si ha $0 \preceq P \preceq \varepsilon^{-1} I$ per costruzione, sembra che la costante di Lipschitz in questione sia maggiorata da $\varepsilon^{-1}$. Conclusione: sembra che si possa prendere $\ell=\varepsilon^{-1}$.

% Verifica: variando $A$, si ottiene

% $$|\lambdamin(\Xi(A))-\lambdamin(\Xi_P(A+ \Delta A))| \leq \left \|\begin{bmatrix}
% 0 & {\Delta A}^\top P \\
% \star &0
% \end{bmatrix} \right\|_{\rm op}$$
% Nel seguito, considero la norma $\|\cdot \|_{\rm op}$ indotta dalla norma $l_2$. Pertanto, prendendo un vettore $x$ con $\|x\|_2=1$ e spezzandolo in due parti $x_1$ e $x_2$ (che raccolgono le sue prime componenti e le sue ultime componenti, rispettivamente), ottengo:
% \begin{align}
% &\,\,\,\,\,\,\left\|\begin{bmatrix}
% 0 & {\Delta A}^\top P \\
% \star &0
% \end{bmatrix} \right\|_{\rm op} &\nonumber \\
% &=\sup_{x: \|x\|_2=1} \left\|\begin{bmatrix}
% 0 & {\Delta A}^\top P& \\
% \star &0
% \end{bmatrix} \begin{bmatrix}
% x_1 \\
% x_2
% \end{bmatrix}\right\|_2 &\nonumber \\
% &=\sup_{x: \|x\|_2=1}\left\|\begin{bmatrix}
% {\Delta A}^\top P x_2 \\
% P^\top \Delta A x_1
% \end{bmatrix}\right\|_2 \nonumber \\
% &= \sup_{x: \|x\|_2=1} \sqrt{\|{\Delta A}^\top P\|_{\rm op}^2 \|x_2\|_2^2 + \|P^\top \Delta A\|_{\rm op}^2 \|x_1\|_2^2} & \nonumber \\
% &=\|{\Delta A}^\top P\|_{\rm op} \leq \|{\Delta A}\|_{\rm op} \|P\|_{\rm op} \leq \|{\Delta A}\|_{\rm op} \varepsilon^{-1} &
% \end{align}
% dove si è sfruttato il fatto che $\|{\Delta A}^\top P\|_{\rm op}=\|P^\top  {\Delta A}\|_{\rm op}$ e $\|P\|_{\rm op}=\lambda_{\mathrm{max}}(P) \leq \varepsilon^{-1}$, che segue dall'ultima formula nella LMI~\eqref{eq:LMICon2}. Concludendo, si ha
% $$|\lambdamin(\Xi(A))-\lambdamin(\Xi_P(A+ \Delta A))| \leq 
% \|{\Delta A}\|_{\rm op} \varepsilon^{-1} $$
% che mostra che si può prendere $\ell= \varepsilon^{-1}$. Il caso in cui sia presente anche la $K$ si tratta allo stesso modo.
% }

%{\color{red} Appena potete, controllate se i conti di sopra sono giusti...}
%\ffnote{Rifatto i conti e formalizzato, mi sembra tutto corretto!}

% \begin{remark}
%     The extension to the controlled case of the result above requires one to first fix the linear gain $K$ (not necessarily as in \eqref{eq:staticFeedback}), and then to focus on the perturbations $(\Delta A, \Delta B)$ affecting the closed-loop matrix $A+BK$, which takes values in some $\Omega_K \subseteq \R^{n\times n}$. In particular, this latter set is directly obtained from $\Omega$ once the linear control action has been fixed. Accordingly, the perturbations of interests are of the form $(\Delta A, \Delta B)$ so that $(A+\Delta A)+(B+\Delta B)K$ still lives in $\Omega_K$.
%     \hfill$\square$
% \end{remark}
% \ggnote{Temo che in presenza di $B$ (e di $\Delta B$) bisognerà rifare i conti sulla costante di Lipschitz (sperando che si trovi ancora una funzione Lipschitz-continua). Mi aspetto che l'upper bound $\|{\Delta A}\|_{\rm op} \varepsilon^{-1}$ sia sostituito da $\|{\Delta A}\|_{\rm op} \varepsilon^{-1} + \|{\Delta B}\|_{\rm op} \|W^\star\|_{\rm op} \varepsilon^{-2}$ (intanto vediamo se è corretto). Il problema è che non vorrei che $W^\star$ avesse una norma matriciale enorme. Comunque se non altro al passo $i$-esimo è fissata. E se invece assumessimo che $B$ non è incerta? Almeno ci semplificheremmo la vita, e butteremmo via il termine in $\|{\Delta B}\|_{\rm op}$. Così almeno potremmo scrivere effettivamente che il Theorem~\ref{theo:LipSigma} si estende pari pari al caso in cui c'è anche $B$ (ma non è incerta).}
% \ffnote{Sono d'accordo col bound che suggerisci considerando $\Delta A$ e $\Delta B$ ``separatamente''. Tuttavia, per una data matrice di controllo $K$, credo uno si debba focalizzare su $\Delta A^{\textrm{cl}} = \Delta A + \Delta B K$, ed ottenere precisamente il bound del teorema, in quanto si sta studiando $|\lambdamin(\Xi_P(A^\textrm{cl}))-\lambdamin(\Xi_P(A^\textrm{cl}+ \Delta A^\textrm{cl}))|$ per quindi avere una disuguaglianza con $\Delta A^\textrm{cl}$ nel RHS. Cosa mi sto perdendo?} \ggnote{L'unico problema è che se si include anche una variazione di $B$, allora si ottengono due costanti di Lipschitz (una per $A$, l'altra per $B$), ma soprattutto la costante di Lipschitz relativa a $B$ ha un'espressione più complicata (non sappiamo a priori quanto possa essere grande $\|W^\star\|_{\rm op}$, e come possa dipendere da $\varepsilon$). Però, ho visto che successivamente prendi $A^{\rm cl}$ come variabile. Forse si ottiene un'espressione semplice della costante di Lipschitz rispetto a quella variabile (magari la stessa costante di Lipschitz che abbiamo già trovato, rispetto ad $A$ soltanto). Forse questo potrebbe risolvere il problema che ho tirato fuori.}


% Thus, we note that Lemma~\ref{lemma:liplamdamin} and Theorem~\ref{theo:LipSigma} together not only suggest that global optimization techniques, tailored for Lipschitz optimization, can be used to verify if a candidate function $V_i$ is a Lyapunov function for $\Sigma$, but also provide a termination criterion for the CEGIS algorithm, as will be described in the next subsection.
% \ggnote{Da qualche parte (forse dopo?) è meglio scrivere che il risultato precedente vale anche in presenza di $B$ (magari proviamo a controllare un attimo se è vero).}.
We finally note that Lemma~\ref{lemma:liplamdamin} and Theorem~\ref{theo:LipSigma} together suggest that global optimization techniques, tailored for Lipschitz optimization, can be used to verify if a candidate function $V_i$ is a Lyapunov function for $\Sigma$. We give more details on this crucial point in the next section.

\subsection{The proposed CEGIS scheme}\label{subsec:cegis}

Algorithm~\ref{algo:OverallCegis} summarizes the main steps of the proposed CEGIS-based iterative scheme for the design of a control Lyapunov function, accompanied with a suitable linear state-feedback controller, for the linear uncertain system $\Sigma$ in \eqref{eq:Sigma}. In particular, black-filled bullets refer to the tasks that have to be performed by the learner, while the empty bullet to the one performed by the verifier. 
% \ggnote{per come è scritto, l'algoritmo sembra mostrare che, nella formulazione originaria del CEGIS riportata all'inizio, è meglio non mettere un $N$ fissato, in quanto l'algoritmo potrebbe non terminare mai}. 

%\DM{Questo è super sketched}
 \begin{algorithm}[h!t]
	\caption{CEGIS-Lyapunov learning method}\label{algo:OverallCegis}
	\DontPrintSemicolon
	\SetArgSty{}
	\SetKwFor{ForAll}{for all}{do}{end forall}
	\smallskip
	\textbf{Initialization:} Set $\eta \ge \varepsilon>0$, fill $\mc C_1 = \{(\hat A_1, \hat B_1)\}$ with some $(\hat A_1, \hat B_1)\in\Omega$. \\
	\smallskip
	\textbf{Iteration $(i \in \N^+)$:} 
 % \ggnote{meglio $\N^+$, così non c'è ambiguità e togliamo lo zero}:}
        \begin{itemize}\setlength{\itemindent}{-.2cm}
            \item[$\bullet$] Identify $\textrm{vert}(\textrm{conv}(\mc C_i))$
        \end{itemize}
        \begin{itemize}\setlength{\itemindent}{-.2cm}
            % \smallskip
            \item[$\bullet$] Solve \eqref{eq:LMICon2}, set $P_i=(X^\star)^{-1}$, $K_i=W^\star(X^\star)^{-1}$
            
            \smallskip
            \textbf{If} \eqref{eq:LMICon2} \texttt{infeasible} \textbf{then} \texttt{exit}
        \end{itemize}
        \begin{itemize}\setlength{\itemindent}{-.2cm}
            \item[$\circ$] Solve \eqref{eq:smallestEig} using a Lipschitz global optimization tool
            
            \smallskip
            \textbf{If} $\lambda^\star<0$ \textbf{then} $\mc C_{i+1}\leftarrow \mc C_i \cup \{(A^\star,B^\star)\}$, \texttt{repeat}\\
            \textbf{If} $\lambda^\star\ge0$ \textbf{then} $V=x^\top P_i x$, $K=K_i$, \texttt{exit}
        \end{itemize}
\end{algorithm}

Note that the Algorithm~\ref{algo:OverallCegis} may terminate in two different points with two possible outcomes. According to the discussion in \S \ref{subsec:learner}, the learner has to solve an LMI feasibility problem, which is not only susceptible to the parametrization adopted and the quality of the samples the verifier produces, but also on the choice of the hyperparameters $\eta \ge \varepsilon>0$. For these reasons, \eqref{eq:LMICon2} may thus not be feasible, and this coincides with the notion of infeasibility mentioned in Theorem~\ref{th:convergence}, namely that a quadratic control Lyapunov function and associated linear controller do not exist for the uncertain system $\Sigma$ in \eqref{eq:Sigma} and the assigned $\eta$ and $\epsilon$. In this case, one might want to reinitialize the procedure with, e.g., a smaller value for $\varepsilon$, a larger one for $\eta$, a larger $\mc W$, or a different initial sample populating $\mc C_1$. 

Finally, in case the verifier
% global optimization solver, which in view of the properties of $\lambdamin(\cdot)$ can coincide with any suitable solver for Lipschitz optimization, 
simply terminates without founding any counter-example, Algorithm~\ref{algo:OverallCegis} has converged. Such a convergence property is itself a certification of the correctness of the control Lyapunov function and associated linear controller found. Otherwise, in case some counter-example has been found, the set $\mc C_{i+1}$ includes also the new pair $(A^\star,B^\star)=(\hat A_{i+1}, \hat B_{i+1})$ obtained from \eqref{eq:smallestEig} and the iterative procedure continues, although not indefinitely, as shown in the following result establishing the finite-time convergence properties of Algorithm~\ref{algo:OverallCegis}:
%\ggnote{Quando $\lambdamin(\Xi_P(A_l))$ è piccolo, l'insieme $S(A_l)$ potrebbe avere raggio piccolo. Questo potrebbe rendere la condizione di sopra difficile da verificare (se non con valori enormi di $i$). Inoltre, potrebbe essere interessante capire se, considerando un caso in cui un ``oracolo'' permetta di sapere che $\lambda^\star$ è maggiore o uguale ad una certa costante $c>0$, si riesca a stabilire dopo quanti passi sia possibile soddisfare la condizione~\eqref{eq:terminationCondition}...la speranza è che ogni $S_{A_l}$ riesca a rimuovere da $\Omega$ una sua porzione di volume maggiore o uguale ad una data costante maggiore di $0$. Però la cosa mi sembra complicata da studiare, in quanto la forma dell'insieme $\underset{l=1,\ldots,i}\bigcup \mathrm{vec}(S_{A_l})$ potrebbe essere complicata, anche perché la funzione obiettivo cambia da un'iterazione all'altra (in quanto cambia la $P$)...forse è meglio lasciar perdere quest'analisi (cancelliamo pure questo commento).}
% \begin{remark}
% In general, estimating the Lipschitz constant $\ell$ (but also computing the operator norm) might be non trivial. In such a case, it may be easier to resort on techniques like DIRECT that are able to estimate such a constant. \ggnote{Aggiungere qualche dettaglio su 
%  che cosa fa DIRECT}
% \end{remark}
%\DM{Reggerà? Lo farà solo all'infinito in casi patologici?}{}
%\DM{come la sbrogliamo nei casi in cui, sebbene esistano punti fuori dai safe set, l'ottimizzatore non trova più controesempi?
%
%
%L'unica idea che ho sarebbe,
%\begin{itemize}
%\item restrigere la ricerca di direct solo fuori dai safe set (ma diventa una geometria bruttissima)
%\item nell'ottimo trovato, calcolare comunque un safe set ed aggiungerlo a quelli noti.
%\end{itemize}
%}{}
% \begin{remark}
    % It might also happen that the global optimization solver simply terminates without founding any counter-examples. In such a case, its convergence properties are themselves a certification of the correctedness of the found controller.
% \end{remark}
%{\color{red} Spiegare meglio il punto di sopra..Se ottimizzando si trova che il minimo dei $\lambdamin(\Xi(A))$ è maggiore di $0$, non si ha la ``prova'' che non esistono controesempi (sempre che il minimo sia stato calcolato correttamente)?}

%\ggnote{Comunque sì, in teoria potrebbe essere utile cercare di avere degli insiemi $\mc C_i$ sempre più grandi, nella speranza di riusciere ad imporre la condizione ~\eqref{eq:terminationCondition} (almeno dal punto di vista teorico).}

% \subsection{Overall CEGIS algorithm for the construction of a common Lyapunov function}
% % \DM{NDR: Io con l'ambiente algorithm ci ho discusso. Visto poi che ogni template ha la sua variante personale ovviamente incompatibile con tutte le altre, fino proprio all'ultimo mi rifiuto di usarlo}{}

% We can now define the overall CEGIS learning synthesis algorithm for the construction of a common Lyapunov function.

% \begin{enumerate}
% \item Initialize $C(1)$ with a realization of $\mathrm{vec}(A) \in \Omega$.
% \item {For $i=1,...,N$:
% \begin{enumerate}
% \item {Synthesize a candidate Lyapunov function $V_i$ and the relative stabilizing feedback policy $K_i$. 
% \begin{itemize}
% \item If the process fails, return with a global infeasibility certificate.
% \end{itemize}
% }
% \item { Define $\Xi(A)= \begin{bmatrix}
% P &(A+BK)^\top P \\ \star & P
% \end{bmatrix}$}, where $K$ is chosen as in Equation~\eqref{eq:staticFeedback}.
% \item Check if~\eqref{eq:terminationCondition} holds, in such a case, exit the loop.
% \item Solve Problem~\eqref{eq:smallestEig} using a Lipschitz global optimization tool. \label{step:check}
% \begin{itemize}
% \item if $\lambda^\star<0$, set $\mc C_{(i+1)}\longleftarrow \mc C_i \cup A^\star$, set $i   \leftarrow i+1$ and repeat.
% \item if  $\lambda^\star>0$, terminate.
% \end{itemize}

% \end{enumerate}
% }
% \item Set $V=V_i$, $K=K_i$, and return.
% \end{enumerate}

% \begin{remark}
% For the first step of the CEGIS algorithm, the $C$ set can be initialized with any valid realization of the matrix $A$.
% \hfill$\square$
% \end{remark}
% \begin{remark}
%     In case the procedure terminates due to the infeasbility of the Problem~\eqref{eq:LMICon2}, one might want to consider restarting the procedure with a smaller $\varepsilon$.
% \end{remark}

% Armed with these considerations, we establish next the finite-time convergence properties of Algorithm~\ref{algo:OverallCegis}:
\begin{theorem}\label{th:convergence}
    Let $\eta \ge \varepsilon>0$ be given. Then, in a finite number of iterations Algorithm~\ref{algo:OverallCegis} either declares infeasibility
    % (i.e., that a quadratic common control Lyapunov function and associated linear controller do not exist for the given linear uncertain system $\Sigma$ in \eqref{eq:Sigma} and the given $\eta$ and $\epsilon$), %\abnote{[be more specific of what infeasibility means, e.g., that a quadratic control Lyapunov function and associated linear controller do not exist for the given uncertain set of linear systems and given hyperparameteds $\eta$, $\epsilon$?]}, 
    or produces a pair $(\bar P, \bar K)$ so that $V=x^\top \bar P x$ is a control Lyapunov function for the linear uncertain system $\Sigma$ in \eqref{eq:Sigma} with linear controller $u=\bar Kx$.
    \hfill$\square$
\end{theorem}
\begin{proof}
    We split the proof in two parts. First, we show that the verifier can only generate counter-examples through \eqref{eq:smallestEig} %strictly
    outside the set of matrices considered by the learner to %compute
    find a solution to \eqref{eq:LMICon2}. This condition %This is a necessary condition, 
    is then exploited %in the second part of the proof
    to prove that the verifier can not produce counter-examples indefinitely, %and
    therefore Algorithm~\ref{algo:OverallCegis} either returns infeasibility, or a solution pair $(\bar P, \bar K)$ in a finite number of steps.
    
    i) 
    % At the generic iteration index $i \in \N^+$, we note that any solution $(X^\star,W^\star)$ to the learner problem \eqref{eq:LMICon2} would still be a feasible if $(\hat A_{i+1}, \hat B_{i+1}) \in \textrm{conv}(\mc C_i)$, where $(\hat A_{i+1}, \hat B_{i+1})$ is the counter-example produced by the verifier. Therefore, $(\hat A_{i+1}, \hat B_{i+1}) \notin \textrm{conv}(\mc C_i)$ represents a necessary condition to claim that Algorithm~\ref{algo:OverallCegis} can return infeasibility in a finite number of steps. We will show next that such a condition actually holds true.
    Assume that \eqref{eq:LMICon2} is feasible at the generic $i$-th iteration, yielding a solution pair $(X^\star,W^\star)$. Then, by pre and post-multiplying by $\mathrm{diag}((X^\star)^{-1}, (X^\star)^{-1})$ both sides of the first 
    % $2n\times2n$ diagonal block of the 
    set of LMI constraints in \eqref{eq:LMICon2}, which is always possible in view of $X^\star \succcurlyeq \varepsilon I$ with $\varepsilon>0$, we obtain:
    $$
    \begin{aligned}
    &\begin{bmatrix}
        (X^\star)^{-1}&(\hat A_h+ \hat B_h W^\star (X^\star)^{-1})^\top(X^\star)^{-1} \\
        \star & (X^\star)^{-1}
    \end{bmatrix} \\
    &\hspace{1.8cm}
    \succcurlyeq  \varepsilon
    \mathrm{diag}((X^\star)^{-1}, (X^\star)^{-1})^{2} %\\
    %&%\hspace{4.2cm}
    \succcurlyeq  \varepsilon \lambdamin^2((X^\star)^{-1}) I \\
    &\hspace{1.8cm}
    \succcurlyeq  (\varepsilon/\lambdamax^2(X^\star)) I \succcurlyeq  (\varepsilon/\eta^2) I,
    \end{aligned}
    $$
    for all $h=1,\ldots,M_i$.
    % with $\lambdamax(X^\star) > 0$, since $X^\star \succcurlyeq \varepsilon I$ 
    This happens by construction of \eqref{eq:LMICon2}, since $\lambdamax(X^\star) \le \eta$ in view of $X^\star \preccurlyeq \eta I$,
    % the second
    % $n\times n$ diagonal block
    % LMI constraint, 
    and $\lambdamin(X^\star)\ge\varepsilon$, thus yielding $\varepsilon/\eta^2 > 0$ as $\eta\ge\varepsilon>0$. Then, in view of relations $P_i=(X^\star)^{-1}$ and $K_i=W^\star (X^\star)^{-1}$, from the previous inequalities we have:
    \begin{equation}\label{eq:lower_bound}
    \Xi(A^{\textrm{cl}}_{i,h}) \succcurlyeq  (\varepsilon/\eta^2) I, \text{ for all } h=1,\ldots,M_i,
    \end{equation}
    with $A^{\textrm{cl}}_{i,h} = \hat A_h + \hat B_h K_i$, which implies that $\lambdamin(\Xi(A^{\textrm{cl}}_{i,h})) \ge \varepsilon/\eta^2 > 0$ for all $h=1,\ldots,M_i$. However, since each pair $(\hat A_h, \hat B_h) \in \textrm{vert}(\textrm{conv}(\mc C_i))$, standard arguments in robust control of polytopic systems \cite[Chapter~7.3.3.2]{duan2013LMIs} imply that this latter relation not only holds true for all vertex matrices, but also for all matrices contained in their convex hull, thus yielding $\lambdamin(\Xi(A^{\textrm{cl}}_i)) \ge \varepsilon/\eta^2 > 0$ where $A^{\textrm{cl}}_i$ is now constructed by making use of any pair of matrices $(A,B) \in \textrm{conv}(\{(\hat A_h,\hat B_h)\}_{h=1}^{M_i})=\textrm{conv}(\mc C_i)$. This implicitly means that the verifier is only able to find counter-examples through \eqref{eq:smallestEig} strictly outside $\textrm{conv}(\{(\hat A_h,\hat B_h)\}_{h=1}^{M_i})$, i.e., $(\hat A_{i+1}, \hat B_{i+1}) \in \Omega \setminus \textrm{conv}(\mc C_i)$. 
    % Therefore we have that $\mc C_{i+1} \supset \mc C_i$ and, in view of the properties of the convex hull \cite{kiselman2002semigroup}, also that $\textrm{conv}(\mc C_{i+1}) \supset \textrm{conv}(\mc C_i)$, which means that the learner has to consider a larger pool of samples at every iteration while solving \eqref{eq:LMICon2}. 
    % Then, either it produces a new tentative solution pair $(P_{i+1},K_{i+1})$, or Algorithm~\ref{algo:OverallCegis} terminates at the second step.
    % , which can hence only happen in a finite number of iterations.
    
    ii) Armed with the previous result, once the learner has obtained from the verifier a new counter-example $(\hat A_{i+1}, \hat B_{i+1}) \in \Omega \setminus \textrm{conv}(\mc C_i)$, either it computes a new tentative solution pair $(P_{i+1},K_{i+1})$ considering a larger pool of samples, or Algorithm~\ref{algo:OverallCegis} terminates at the second step of the $(i+1)$-th iteration. If the latter happens, then Algorithm~\ref{algo:OverallCegis} returns infeasibility in a finite number of iterations, as claimed.
    On the other hand, what could happen instead is that the verifier may produce counter-examples indefinitely, thus invalidating both the statements.
    We will show in the following that this can 
 not be the case.

    In particular, we start by assuming that \eqref{eq:LMICon2} is feasible for all the considered iterations $i\in\N^+$, otherwise Algorithm~\ref{algo:OverallCegis} declares infeasibility in a finite number of steps. Then, by relying on Theorem~\ref{theo:LipSigma}, at the generic $i$-th iteration we can quantify how far from $\textrm{conv}(\mc C_i)$ the new counter-example $(\hat A_{i+1}, \hat B_{i+1})$ will be. Precisely, it will fall outside 
    % More precisely, at the generic $i$-th iteration we can resort on Theorem~\ref{theo:LipSigma} to define 
    the following convex set of matrices generating closed-loop eigenvalues that are guaranteed to be at most $(\varepsilon/\eta^2)$-distant from those belonging to $\textrm{conv}(\mc C_i)$:
    % which hence amounts to a ``safe'' set where no counter-examples can be found:
    $$
    \begin{aligned}
        \mc S_{\bar A^{\textrm{cl}}_i} \coloneqq &\{A^{\textrm{cl}} \in \R^{n\times n}\mid A^{\textrm{cl}} = A+BK_i, A\in\R^{n\times n}, \\ &B\in\R^{n\times m}, ||\bar A^{\textrm{cl}}_i- A^{\textrm{cl}}||_\mathrm{op} \le \varepsilon^2/\eta^2, \\
        &\text{ for all } \bar A^{\textrm{cl}}_i = A_j+B_jK_i,~(A_j, B_j) \in \textrm{conv}(\mc C_i) \}.
    \end{aligned}
    $$

    % Armed with these considerations, we now show by contradiction that the verifier will not be able to produce any counter-example in a finite number of iterations.
    % a fact that directly depends on the behaviour of $\textrm{conv}(\mc C_i)$ that satisfies, as already observed, the inclusion $\textrm{conv}(\mc C_{i+1}) \supset \textrm{conv}(\mc C_i)$. 
    % Our goal is thus to show that $\Omega \setminus \mc S_{\bar A^{\textrm{cl}}_i} \to \emptyset$ as $i\to\infty$, which directly depends on the behaviour of $\textrm{conv}(\mc C_i)$. 
    % In particular, we note that in view of the properties of the convex hull \cite{kiselman2002semigroup}, since $\mc C_{i+1} \supset \mc C_i$, then also $\textrm{conv}(\mc C_{i+1}) \supset \textrm{conv}(\mc C_i)$.
    % , and hence $\textrm{cl}(\textrm{conv}(\mc C_{i+1})) \supset \textrm{cl}(\textrm{conv}(\mc C_i))$. 
    For the sake of contradiction, assume that the verifier generates an infinite sequence $\{(\hat A_i, \hat B_i)\}_{i\in\N^+}$, with $(\hat A_i, \hat B_i) \in \Omega$. From the first part of the proof, we know that $\mc C_{i+1} \supset \mc C_i$, and therefore the infinite, monotonically increasing sequence of sets $\{\mc C_i\}_{i\in\N^+}$ admits the 
    % convergent subsequence whose 
    limit set $\bar{\mc C} = \cup_{i\in\N^+} \mc C_i$, which enables us to extract from $\{(\hat A_i, \hat B_i)\}_{i\in\N^+}$ a subsequence of counter-examples converging to some $(\bar A, \bar B) \in \textrm{cl}(\textrm{conv}(\bar{\mc C})) \cap \Omega$. 
    % , is contained in $\Omega$ itself. 
    Since the learner iteratively generates tentative pairs $\{(P_i, K_i)\}_{i\in\N^+}$, produced by optimal solutions $\{(X^\star, W^\star)\}_{i\in\N^+}$ to \eqref{eq:LMICon2} living in compact sets, we can additionally extract a convergent subsequence from $\{(P_i, K_i)\}_{i\in\N^+}$ with limit pair $(\bar P, \bar K)$. Let us now consider a limiting ``safe'' set $\mc S_{\bar A^{\textrm{cl}}}$, defined as:
    $$
    \begin{aligned}
        &\mc S_{\bar A^{\textrm{cl}}} \coloneqq \{A^{\textrm{cl}} \in \R^{n\times n}\mid A^{\textrm{cl}} = A+B\bar K, A\in\R^{n\times n}, \\ &\hspace{1.3cm}B\in\R^{n\times m}, ||\bar A^{\textrm{cl}}- A^{\textrm{cl}}||_\mathrm{op} \le \varepsilon^2/\eta^2, \\
        &\hspace{1.3cm}\text{ for all } \bar A^{\textrm{cl}} = A_j+B_j \bar K,~(A_j, B_j) \in \textrm{conv}(\bar{\mc C}) \}.
    \end{aligned}
    $$
    % Accordingly, since \eqref{eq:LMICon2} is assumed to be feasible for the assigned $\eta \ge \varepsilon>0$ and all $i \in \N^+$, the 
    % sequence of unique optimal solutions $\{(P_i, K_i)\}_{i \in \N^+}$ generated by the 
    % learner generates the tentative pair $(\bar P, \bar K)$ by considering $\textrm{vert}(\bar{\mc C})$. 
    % and introduce a limiting
    % Assume now that $\Omega \setminus \mc S_{\bar A^{\textrm{cl}}} \neq \emptyset$.
    % , where a limiting ``safe'' set $\mc S_{\bar A^{\textrm{cl}}}$ is defined as:
    % $$
    % \begin{aligned}
    %     &\mc S_{\bar A^{\textrm{cl}}} \coloneqq \{A^{\textrm{cl}} \in \R^{n\times n}\mid A^{\textrm{cl}} = A+B\bar K, A\in\R^{n\times n}, \\ &\hspace{1.3cm}B\in\R^{n\times m}, ||\bar A^{\textrm{cl}}- A^{\textrm{cl}}||_\mathrm{op} \le \varepsilon^2/\eta^2, \\
    %     &\hspace{1.3cm}\text{ for all } \bar A^{\textrm{cl}} = A_j+B_j \bar K,~(A_j, B_j) \in \textrm{conv}(\bar{\mc C}) \}.
    % \end{aligned}
    % $$
    % with $\lambdamin(\Xi(\bar A^{\textrm{cl}})) \ge \varepsilon/\eta^2>0$ for all $A^{\textrm{cl}}$ constructed with pair of matrices $(A_j, B_j) \in \textrm{cl}(\textrm{conv}(\bar{\mc C}))$. 
    % Condition $\Omega \setminus \mc S_{\bar A^{\textrm{cl}}} \neq \emptyset$ means that the verifier either has concluded its task, as it is not able to find any other counter-example in the nonempty set $\Omega \setminus \mc S_{\bar A^{\textrm{cl}}}$, and hence Algorithm~\ref{algo:OverallCegis} has converged, 
    % Now, in case the verifier produces the new counter-example shall belong outside $\mc S_{\bar A^{\textrm{cl}}}$, thus contradicting the fact that $\bar{\mc C}$ is a limit point for a convergent subsequence of sets $\{\mc C_i\}_{i\in\N^+}$. Thus, what can only happen is that the sequence $\{\mc C_i\}_{i\in\N^+}$ converges to some set $\bar{\mc C}$ in a finite number of iterations, with $\bar{\mc C}$ so that $\Omega \setminus \mc S_{\bar A^{\textrm{cl}}} = \emptyset$, concluding the proof.
    Leveraging this limiting set, we can finally conclude that not all the points of the convergent subsequence extracted from $\{(\hat A_i, \hat B_i)\}_{i\in\N^+}$ can be counter-examples. In fact, by definition of limit, there will exist some large enough index $i_c$ associated to the underlying subsequence so that, by starting from $(\hat A_{i_{c+1}}, \hat B_{i_{c+1}})$, all the counter-examples generate closed-loop eigenvalues, obtained with the limit pair $(\bar P, \bar K)$, 
    strictly closer than $(\varepsilon/\eta^2)$ from those generated by $(\hat A_{i_{c}}, \hat B_{i_{c}})$ with the same pair limit $(\bar P, \bar K)$, which is clearly in contradiction with the first part of the proof, i.e., with the assumption that they are counter-examples. 
    % This clearly yields a contradiction.
    
    Therefore, under the feasibility of \eqref{eq:LMICon2}, what can only happen is that the sequence $\{(\hat A_i, \hat B_i)\}$ converges to some pair $(\bar A, \bar B) \in \Omega$ in a finite number of iterations, say $N\in\N^+$, entailing that also $\{\mc C_i\}$ converges to some set $\mc C_N=\bar{\mc C}$ in $N$ steps, with $\bar{\mc C}$ so that $\Omega \setminus \mc S_{\bar A^{\textrm{cl}}} = \emptyset$. This means that, after $N$ iterations, the verifier is not able to produce any counter-example by evaluating the tentative pair $(P_N, K_N)=(\bar P, \bar K)$ proposed by the learner. Since the verifier can not produce counter-examples indefinitely, both outcomes of Algorithm~\ref{algo:OverallCegis} can only be returned in a finite number of iterations, thus concluding the proof.
\end{proof}

\begin{remark}
% One can now see the fundamental difference between the proposed method and standard ray-shooting ones, as in~\cite{alessio2007squaring}. Whilst these latter build an outer approximation of $\Omega$ leveraging geometric considerations, our approach instead can set also for an inner approximation of $\Omega$ due to the additional presence of the $\mc S_{\bar A^{\textrm{cl}}_i}$ sets and still guarantee stability.
Compared to standard ray-shooting methods~\cite{alessio2007squaring}, which build a conservative outer-approximation of $\Omega$ through geometric considerations, our approach covers the whole set $\Omega$ of possible counter-examples by exploiting samples contained in it, along with the ``safety'' guarantees provided by $\mc S_{\bar A^{\textrm{cl}}_i}$, which follow from \eqref{eq:LMICon2} and \eqref{eq:smallestEig}.
\hfill$\square$
\end{remark}

\section{Computational aspects}\label{sec:comp_aspect}

%We now discuss few computational aspects of Algorithm~\ref{algo:OverallCegis}, including considerations on the tradeoff induced by the hyperparameters $\eta$ and $\varepsilon$ to both \eqref{eq:LMICon2} and \eqref{eq:smallestEig}. 

\subsection{Tradeoff induced by $\eta$ and $\varepsilon$}
\label{sec:rolevareps}
Besides strongly affecting the learner convex optimization problem \eqref{eq:LMICon2}, we note that  $\eta$ and $\epsilon$, with $\eta\ge\varepsilon>0$, assume a key role also in characterizing the performance of both Algorithm~\ref{algo:OverallCegis} and the verification task \eqref{eq:smallestEig}. 
% Particularly $\varepsilon$, whose inverse coincides with the estimate of the Lipschitz constant characterizing the minimum eigenvalue of the matrix $\Xi(A_i^{\textrm{cl}})$, according to the proof of Theorem~\ref{theo:LipSigma}.

In fact, from Theorem~\ref{th:convergence}, specifically the lower bound \eqref{eq:lower_bound}, it is evident that setting $\eta$ as close as possible to $\varepsilon$, with this latter large enough, the convergence of Algorithm~\ref{algo:OverallCegis} may require a smaller number of steps, as this combination allows to ``erode'' (by means of the safe set $\mc S_{\bar A^{\textrm{cl}}_i}$) a larger portion of the set $\Omega\setminus \textrm{conv}(\mc C_i)$ at each iteration. In addition, this choice is also favourable to improve the performance of the technique employed to solve \eqref{eq:smallestEig}. Indeed, convergence of algorithms for Lipschitz-continuous global optimization problems, as the one in \eqref{eq:smallestEig}, is well-studied in the literature. As a general rule of thumb inferred, the smaller the Lipschitz constant, the better the guarantee on the performance of the underlying algorithm~\cite{malherbe2017global}. In view of Theorem~\ref{theo:LipSigma}, this hence suggests to choose larger values for the hypeparameter $\varepsilon$.
% , aiming at improving the performance of the adopted algorithm to solve the verifier's task \eqref{eq:smallestEig}. 

However, following the considerations above may undesirably complicate the learner's optimization problem, as the feasible set of \eqref{eq:LMICon2} would shrink, thus making the design of quadratic control Lyapunov functions and associated linear controllers a challenging task. This naturally induces a tradeoff in the choice of the pair $(\eta,\varepsilon)$, which shall hence be set according to the problem data at hand.

% % , as they characterizes the lower bound \eqref{eq:lower_bound} for each pair of matrices belonging to $\textrm{conv}(\mc C_i)$, 
% we note that by exploiting the Lipschitz condition in Theorem~\ref{theo:LipSigma} dictated by $\varepsilon$,
% \ggnote{mi sa che prima le abbiamo dato un altro nome} 
% for the controlled case), according to Theorem~\ref{theo:LipSigma}. %\ggnote{il caso con $B$ in realtà va ancora riportato..come dicevo, secondo me ci semplifichiamo un po' le cose quando $B$ non è incerta}.
% This fact may be exploited to identify a suitable termination criterion for Algorithm~\ref{algo:OverallCegis} prior to solving \eqref{eq:smallestEig}. At the generic $i$-th step of the verifier, indeed, for all $(\hat A_h, \hat B_h)\in \mc C_i$ and for given candidates $P_i$ and $K_i$ passed by the learner, define matrices $\hat A_h+\hat B_hK_i\eqqcolon\hat A^{\textrm{cl}}_h(i)$ and
% $$
% \Xi(\hat A^{\textrm{cl}}_h(i))=%%\\
% %&
% \begin{bmatrix}
% P_i & (\hat A^{\textrm{cl}}_h(i))^\top P_i \\ \star &
% P_i
% \end{bmatrix}.
% $$
% This fact can be exploited to accelerate the convergence of Algorithm~\ref{algo:OverallCegis}. While it is clear from Theorem~\ref{th:convergence}, specifically the lower bound \eqref{eq:lower_bound} for each pair of matrices belonging to $\textrm{conv}(\mc C_i)$, that setting $\eta$ as close as possible to $\varepsilon$, with this latter large enough to ``erode'' larger portions of the set $\Omega\setminus \textrm{conv}(\mc C_i)$,
% % , as they characterizes the lower bound \eqref{eq:lower_bound} for each pair of matrices belonging to $\textrm{conv}(\mc C_i)$, 
% we note that by exploiting the Lipschitz condition in Theorem~\ref{theo:LipSigma} dictated by $\varepsilon$, 
% for each $A^{\textrm{cl}}_{i,h}$ we can additionally define a ``safe set'' of matrices for which $\lambdamin(\Xi(A^{\textrm{cl}}_{i,h}))$ is guaranteed to be nonnegative, i.e.,
% $$
% \begin{aligned}
%     &\mc S_{A^{\textrm{cl}}_{i,h}} \coloneqq \{A^{\textrm{cl}} \in \R^{n\times n}\mid A^{\textrm{cl}} = A+BK_i, A\in\R^{n\times n}, \\ &\hspace{1.75cm}B\in\R^{n\times m}, ||A^{\textrm{cl}}_{i,h}- A^{\textrm{cl}}||_\mathrm{op} \le \varepsilon \lambdamin(\Xi(A^{\textrm{cl}}_{i,h})) \}.
% \end{aligned}
% $$
% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\columnwidth]{cegis.pdf}
%     \caption{The region identified by the union of green elements, i.e., the ``balls'' $\mc S_{\bar A^{\textrm{cl}}_i}$ centred on each vertex pair of matrices $\{(\hat A_h, \hat B_h)\}_{h=1}^{M_i}$ (red dots, among all the other samples) and the associated set of matrices with eigenvalues $\varepsilon/\lambdamax^2(P^{-1})$-distant from those belonging to $\textrm{cl}(\textrm{conv}(\mc C_i))$ (green lines and its interior, with $\textrm{conv}(\mc C_i)$ represented by the blue dashed line), denotes the actual portion of volume where it is guaranteed that no counter-example can be found.}
%     \label{fig:cegis}
% \end{figure}
% Moreover, from Theorem~\ref{th:convergence} we shall consider also the set of matrices with eigenvalues $\varepsilon/\lambdamax^2(X^\star)$-distant from those belonging to $\textrm{cl}(\textrm{conv}(\mc C_i))$ as a ``safe'' one:
% $$
% \begin{aligned}
%     &\mc S_{\bar A^{\textrm{cl}}_i} \coloneqq \{A^{\textrm{cl}} \in \R^{n\times n}\mid A^{\textrm{cl}} = A+BK_i, A\in\R^{n\times n}, \\ &\hspace{.5cm}B\in\R^{n\times m}, ||\bar A^{\textrm{cl}}_i- A^{\textrm{cl}}||_\mathrm{op} \le \varepsilon^2 \lambdamin(\Xi(\bar A^{\textrm{cl}}_i))/\lambdamax^2(X^\star), \\
%     &\hspace{.5cm}\text{ for all } \bar A^{\textrm{cl}}_i = A_j+B_jK_i,~(A_j, B_j) \in \textrm{cl}(\textrm{conv}(\mc C_i)) \}.
% \end{aligned}
% $$
% Therefore, the actual portion for which we are guaranteed that no counter-examples can be found is identified by the  set 
% $
% \{\cup_{h=1,\ldots,M_i} \mc S_{A^{\textrm{cl}}_{i,h}}\}\cup\mc S_{\bar A^{\textrm{cl}}_i},
% $
% which can hence be enlarged by tuning $\varepsilon>0$ to ``cover'' the original set $\Omega$ with a reasonable number of samples, i.e., iterations.
% so that no possible counter-example can be found.
% \ffnote{Questo dovrebbe essere vero nel caso autonomo, non sicuro nel caso controllato}\\
% Note that condition \eqref{eq:terminationCondition} can be checked by the verifier sequentially thorugh the iterations, i.e., by computing $S_{\hat A^{\textrm{cl}}_i(i)}$ once that a new sample is added to the set $\mc C_i$, and then taking the union with $\bigcup_{h=1,\ldots,i-1} \mc S_{\hat A^{\textrm{cl}}_h(i)}$. \ggnote{Non mi è chiaro perché ci si fermi a $i-1$. Poi un altro problema è che mi sembra di capire che gli insiemi stessi dipendano da $i$ (come evidenziato dalla notazione), anche per esempi che sono già stati inseriti in passato, per cui potrebbero ingrandirsi o ridursi all'aumentare di $i$ (boh). Inltre non ho capito che cosa cambi quando si inserisce anche $B$ (cioè perché il commento sia vero in assenza di $B$, e perché potrebbe non esserlo in presenza di $B$).}
% In conclusion, the design of the hyperparameter $\varepsilon>0$ then happens to be crucial, as it allows us to strike a balance between that sort of contraction imposed through the LMI in \eqref{eq:LMICon2}, and the possibility to cover large enough regions of perturbation matrices during the verification process.
% \begin{remark}
%     The convergence of Algorithm~\ref{algo:OverallCegis} is also affected by $\lambdamax(X^\star)$, whose value depends on the set of samples considered at each iteration, $\mc C_i$. One may hence want to slightly modify the LMI in \eqref{eq:LMICon2} to upper bound also the largest eigenvalue of the matrix $X$ so that the portion of volume eaten away from $\Omega$ can be fixed, thus possibly enabling for finite-time convergence of Algorithm~\ref{algo:OverallCegis}.
%     \hfill$\square$
% \end{remark}
% \subsection{Lipschitz-continuos global optimization}
%\begin{remark}
% This goes hand-in-hand with the fact that convergence of algorithms for Lipschitz-continuous global optimization, as the one in \eqref{eq:smallestEig}, is well-studied in the literature and, in general, the smaller the Lipschitz constant, the better the guarantee on the performance of the algorithm~\cite{malherbe2017global}. 
% In the specific case of the verifier task, it may be possible to achieve an improved estimate of the Lipschitz constant $\ell=\ell(\varepsilon)$ (i.e., smaller than $\varepsilon^{-1}$). To do this, one may resort on adaptive techniques, based on sampling the set $\Omega$ in suitable ways. %This is generally not problematic with respect to the convergence properties of the overall optimization methods~\cite{malherbe2017global}.
%\DM{Giorgio: se vuoi metterci te le mani, aggiungi qualche riga di più. Altrimenti sarei quasi per levare ogni cosa}{}\ggnote{lasciamo perdere..ci potremmo limitare a scrivere (e forse l'abbiamo già scritto da qualche parte) che abbiamo un trade-off tra facilità a risolvere il problema del verifier (più semplice se la stima della costante di Lipschitz è piccola, cioè se $\varepsilon$ è grande, in quanto il safe set sembra essere grande, ed anche la funzione obiettivo cambia meno velocemente) e possibilità di risolvere il problema del learner (più semplice se $\varepsilon$ è piccola) 
%\end{remark}

\subsection{Speeding up the verification process}
% Despite the fact that computing the smallest eigenvalue of a matrix is not an expensive procedure, it is undeniable that 
%The considerations below focus on the autonomous version of $\Sigma$ in \eqref{eq:Sigma}, i.e., $\mc B = \emptyset$, although they can immediately be generalized to the controlled case.
% The verification process shown in \S \ref{subsec:verifier} can undeniably be time consuming. 
To partially overcome the computational requirements of the verification process, one might want to rely on sensitivity-based methods.
% \ggnote{qui meglio riscrivere al solito che stiamo considerando per semplicità il solo caso in cui c'è $A$ e basta, ma che queste considerazioni valgono anche quando c'è $B$}. 
 Indeed, while in general the partial derivative 
$
\partial \lambda_i(\Xi(A^\textrm{cl}))/\partial A^\textrm{cl}_{(h,k)}
$ of the $i$-th eigenvalue $\lambda_i(\Xi(A^\textrm{cl}))$ with respect to the element  in position $(h,k)$ of $A^\textrm{cl}$ is typically not defined in  those cases in which $\lambda_i$ changes multiplicity, as noted in~\cite{kato2013perturbation}, such an happenstance is quite uncommon. This fact suggests that exploiting such an information and trying to solve \eqref{eq:smallestEig} using a sensitivity-based optimization algorithm like an SQP or interior-point solver might be a valid heuristic to speed-up the overall verification process, by possibly accelerating the process of finding a new counter-example. In this case, however, we loose any kind of convergence guarantee, since finding a local minimum in \eqref{eq:smallestEig} may still lead to a counter-example, but if no counter-example can be found by a local solver, then nothing can be concluded. 
% and a global optimizer is needed. %\abnote{[Better explain here. I guess due to local minima? I understand that a local minimum may still lead to a counter-example, but if no counter-example can be found by a local solver, then nothing can be concluded and a global optimizer is needed]}.
Nevertheless, a sensitivity-based approach may lead to a substantial speed improvement in the evaluation of \eqref{eq:smallestEig}. This motivates the following modification of the verifier's task in Algorithm~\ref{algo:OverallCegis}. Given some number of trials $N_t\in\N^+$:

% $\texttt{counter}=0$ and initial conditions for the sensitivity-based algorithm: 1) Solve \eqref{eq:smallestEig} using a sensitivity-based method to obtain $\lambdamin^{\star,  \textrm{sens}}$; 2) If $\lambdamin^{\star,  \textrm{sens}} <0$ then $\lambdamin^\star \leftarrow \lambdamin^{\star,  \textrm{sens}}$, otherwise reinitialize the algorithm with different initial condition and increase $\texttt{counter}$.;
% 3) if $\texttt{counter}==N_t$ then solve \eqref{eq:smallestEig} as in Algorithm~\ref{algo:OverallCegis}.

\smallskip
\noindent \textbf{Set} $\texttt{counter}=0$\\
\noindent\textbf{While} $\texttt{counter}<N_t$
\begin{itemize}
\item[$\circ$] Solve \eqref{eq:smallestEig} by a sensitivity-based method to get $\lambdamin^{\star,  \textrm{sens}}$\\
    \smallskip
    \textbf{If} $\lambdamin^{\star,  \textrm{sens}} <0$ \textbf{then} $\lambdamin^\star \leftarrow \lambdamin^{\star,  \textrm{sens}}$, \texttt{exit while}\\
    \textbf{Else} $\texttt{counter} \leftarrow \texttt{counter}+1$, set different initial conditions for sensitivity-based method
\end{itemize}
\noindent\textbf{End}\\
\noindent\textbf{If} $\texttt{counter}==N_t$ \textbf{then} Solve \eqref{eq:smallestEig} as in Algorithm~\ref{algo:OverallCegis}
\smallskip

The same intuition also holds for the \textit{refinement} of the minimizer found by the global optimization procedure, as it is well-known that the \textit{quality} of the solution provided by most global optimization tools is sometimes subpar. 
%(in the sense that such algorithms are good in finding approximately an optimal solution if low accuracy is needed, but bad in finding it approximately if, instead, high accuracy is required)
%a warm-started sensitivity method can be used to refine the already found solution.



%\subsubsection{On polytopic approximations}

%The verifier task requires the knowledge of the set $\Omega$ characterizing the generator pairs of matrices determining the dynamical evolution of $\Sigma$. Given that availability, one may na\"ively be induced to solve the learner task in one shoot by, e.g., considering a convex-hull-based approximation of $\Omega$ and hence implementing an LMI in the spirit of \eqref{eq:baseLineK}. We note that, however, this choice is typically overly conservative for sets $\Omega$ with generic shape, as it leads to some outer approximation. In addition, it is well known that the number of vertices of the convex hull grows with both the number of generator matrices populating $\Omega$ and the dimension we are considering, i.e., $n(n+m)$ \cite{raynaud1970enveloppe}, since $\mc A \subseteq \R^{n\times n}$ and $\mc B \subseteq \R^{n\times m}$.
%As an undesired consequence, since \eqref{eq:baseLineK} is based on the vertex pairs of matrices $(A_j,B_j)$, one can easily incur in a prohibitive LMI to solve numerically, thus making such an approximation-based approach not viable from a computational perspective.

%With the proposed approach, instead, the learner first tries to solve tractable optimization problems with LMI constraints with moderate (or even small) number of vertices dictated at most by the cardinality of $\mc C_i$,
%whereas the verifier has only to compare the minimum eigenvalues of symmetric matrices \eqref{eq:Schur} once the candidates $P_i$ and $K_i$ have been disclosed by the learner.


\section{Numerical examples}
\label{sec:numerical}
\input{experimental.tex}


\section{Conclusion}
\label{sec:conclusions}
We have introduced a CEGIS-based methodology to design control Lyapunov functions, accompanied by suitable controllers, for linear systems with parametric uncertainties living in an arbitrary compact set.
% of dynamical systems subject to polytopic parametric uncertainty. 
%The proposed CEGIS-based approach is non-conservative and allows one to practically extend standard results on robust control.
By building upon available results in linear operator theory, our technique features appealing Lipschitz-continuity properties that enable the adoption of efficient solvers for global optimization, as well as guaranteeing finite-time convergence. 
% Several numerical considerations have also been been given. 
% We foresee several extension for the proposed method to other classes of dynamical systems and to include additional objectives into the synthesis part. Ideas regarding the design of an especially crafted global Lipschitz  optimization method %especially
% geared on the proposed verification process are also being considered.
Future work will focus on conditions guaranteeing an estimate on the number of iterations needed for convergence.

\bibliographystyle{IEEEtran}
\bibliography{biblio}



\end{document}