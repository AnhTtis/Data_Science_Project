% Don't touch this %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage[left=1in,top=1in,right=1in,bottom=1in,headheight=3ex,headsep=3ex]{geometry}
\usepackage{graphicx}
\usepackage{float}

\newcommand{\blankline}{\quad\pagebreak[2]}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Modify Course title, instructor name, semester here %%%%%%%%

\title{COMP 514: Optimization: Algorithms, Complexity \& Approximations}
\author{Final Project Abstract \\ Kai Malcolm \& Josue Casco-Rodriguez}
\date{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Don't touch this %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[sc]{mathpazo}
\linespread{1.05} % Palatino needs more leading (space between lines)
\usepackage[T1]{fontenc}
\usepackage[mmddyyyy]{datetime}% http://ctan.org/pkg/datetime
\usepackage{advdate}% http://ctan.org/pkg/advdate
\newdateformat{syldate}{\twodigit{\THEMONTH}/\twodigit{\THEDAY}}
\newsavebox{\MONDAY}\savebox{\MONDAY}{Mon}% Mon
\newcommand{\week}[1]{%
%  \cleardate{mydate}% Clear date
% \newdate{mydate}{\the\day}{\the\month}{\the\year}% Store date
  \paragraph*{\kern-2ex\quad #1, \syldate{\today} - \AdvanceDate[4]\syldate{\today}:}% Set heading  \quad #1
%  \setbox1=\hbox{\shortdayofweekname{\getdateday{mydate}}{\getdatemonth{mydate}}{\getdateyear{mydate}}}%
  \ifdim\wd1=\wd\MONDAY
    \AdvanceDate[7]
  \else
    \AdvanceDate[7]
  \fi%
}
\usepackage{setspace}
\usepackage{multicol}
%\usepackage{indentfirst}
\usepackage{fancyhdr,lastpage}
\usepackage{url}
\pagestyle{fancy}
\usepackage{hyperref}
\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{layout}
\usepackage{amssymb}

\lhead{}
\chead{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Modify header here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\rhead{\footnotesize COMP 514}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Don't touch this %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lfoot{}
\cfoot{\small \thepage/\pageref*{LastPage}}
\rfoot{}

\usepackage{array, xcolor}
\usepackage{color,hyperref}
\definecolor{clemsonorange}{HTML}{EA6A20}
\hypersetup{colorlinks,breaklinks,linkcolor=clemsonorange,urlcolor=clemsonorange,anchorcolor=clemsonorange,citecolor=black}

%% I added this for code
\usepackage{listings}
\usepackage{color}

%% I added this for strikethrough
\usepackage[normalem]{ulem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\newcommand\josue[1]{{\color{red}\sf{[Josue: #1]}}}
\newcommand\kai[1]{{\color{blue}\sf{[Kai: #1]}}}










\begin{document}

\maketitle

\bigskip 

What is the narrative we are trying to weave from all the different papers we could pursue? \\
\josue{I'm thinking that we should (since 514 is about optimization) focus on the optimization / gradient / sparsity methods in Spiking Neural Networks (SNNs). While the following questions in SNN research aren't the only ones we should focus on, here are a few that immediately come to mind for me:}
\\
\kai{Nice.} \\
\josue{"SNNs: How do you: 1) \textit{work} with them? (what frameworks are available? what are their limitations?) 2) \textit{optimize} them? (what advances have been made in SNN optimization? Sparsity? Also what are the top performing SNN architectures? Which datasets?) 3) \textit{understand} them? (geometric understandings of SNNs? biological plausibility?)"} \\
\\

\noindent \textbf{\textit{Rough Outline:}} \\
\begin{enumerate}
    \item What our project is \\
    
    \\
    \noindent \textbf{\textit{Potential Ideas to Pursue:}}
    \begin{enumerate}
        \item \textbf{Energy savings from sparse firing.  Efficiency?}
        \item Redundancy and plasticity? Geometrical Understanding of SNNs?
        \item Target encoding via firing rates?
        \item \textbf{Biologically plausible/inspired learning}
        \item SNN Architectures
        \item \textbf{Applications of optimization / gradient methods for SNNs}
    \end{enumerate}
    \\
    \\
    
    \begin{itemize}
    \item There aren't many meta-analyses on SNN (although these two are good places to start: \textit{Spiking Neural Networks and Their Applications: A Review} (this one is very good, we will have to borrow from it) and also \textit{Spiking Neural Networks for Computational Intelligence: An Overview}), especially ones that clearly and concisely compare the performances of SNNs on various datasets in one figure (i.e., like a leaderboard or like. Also would be good to explain the various datasets available---for example, what is the "MNIST of SNNs"? "CIFAR of SNNs"? How well have previous methods performed on such datasets?
    \item New SNN libraries are being developed independently (e.g., sNNTorch and SpykeTorch), and there's no clear consensus as to which library(ies) are available / which library(ies) one should use, unlike in traditional ML where we have TF + PyTorch (+ Jax, etc.). Lack of knowledge about existing libraries can be especially problematic when trying to work with sparse spiking, for example, since no-one would want to develop CUDA implementations for sparse layers if they knew such implementations already existed. \textbf{In the SNN field, it's unclear what has/hasn't been implemented already}.
    \item Would definitely be good to be able to go into detail about each of the new (in 2022, 2021, and 2020 there have already been several good new papers) optimization methods / assumptions in SNNs and how they are different---i.e., compare and contrast.
    \end{itemize}
    
    \item "Your opinion" / Describe why this is interesting/important \\

    % This last sentence isn't great, I'll make it make more sense when I come back in the morning
    
    % Probably ought to talk more about actual optimization, I know way more about the neuroscience than the optimization tho so I'll have to come back to that ig lol
    
   % "I have always been convinced that the only way to get artificial intelligence to work is to do the computation in a way similar to the human brain. That is the goal I have been pursuing.  We are making progress, though we still have lots to learn about how the brain actually works." - Geoffrey Hinton
    
    \item What needs to be done \\
    \\
    \kai{So I'm generally hearing: 
    \begin{enumerate}
        \item A general meta-analysis/overview (focused on extensions from recent papers in the last 3 years or so)
        \item A quantitative analysis (or maybe aggregation is a better way to describe it) comparing how state of the art SNN methods have performed, and specifically on what datasets, and thus:
        \item Identifying a standard framework to allow comparisons between different SNNs: both in terms of what people are tending to benchmark against but also perhaps what would theoretically be a good benchmark for SNNs.
        \item Maybe scroll through some documentation and compile a (somewhat general probably) list of available methods/attributes in different frameworks.  Could be good to note what (framework) most people are typically using in research as well, if they are using one and they say what it is.
    \end{enumerate} }

    \item Ideas to improve the ideas involved? \\
    
    \kai{Essentially the above bullet points.  And also:}
    \\
    
    \\
    
    \noindent \textbf{\textit{Identified Open Questions:}} \\
\begin{enumerate}
    \item Deriving learning rules for the discrete / sparse / non-linear nature of spikes \josue{A huge challenge in SNNs!}\\
    \item Adapting to the time-encoding features of spikes \josue{Also important, i.e., for encoding}\\
    \item Finding more efficient training approaches / scaling up to higher dimensions / more complex networks \josue{Important, too! SNNs are always playing catch-up to ANNs}\\
\end{enumerate}
    
    
\end{enumerate}


\end{document}
