% Don't touch this %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage[left=1in,top=1in,right=1in,bottom=1in,headheight=3ex,headsep=3ex]{geometry}
\usepackage{graphicx}
\usepackage{float}

\newcommand{\blankline}{\quad\pagebreak[2]}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Modify Course title, instructor name, semester here %%%%%%%%

\title{COMP 514: Optimization: Algorithms, Complexity \& Approximations}
\author{Final Project 1 Page Proposal \\ Kai Malcolm \& Josue Casco-Rodriguez}
\date{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Don't touch this %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[sc]{mathpazo}
\linespread{1.05} % Palatino needs more leading (space between lines)
\usepackage[T1]{fontenc}
\usepackage[mmddyyyy]{datetime}% http://ctan.org/pkg/datetime
\usepackage{advdate}% http://ctan.org/pkg/advdate
\newdateformat{syldate}{\twodigit{\THEMONTH}/\twodigit{\THEDAY}}
\newsavebox{\MONDAY}\savebox{\MONDAY}{Mon}% Mon
\newcommand{\week}[1]{%
%  \cleardate{mydate}% Clear date
% \newdate{mydate}{\the\day}{\the\month}{\the\year}% Store date
  \paragraph*{\kern-2ex\quad #1, \syldate{\today} - \AdvanceDate[4]\syldate{\today}:}% Set heading  \quad #1
%  \setbox1=\hbox{\shortdayofweekname{\getdateday{mydate}}{\getdatemonth{mydate}}{\getdateyear{mydate}}}%
  \ifdim\wd1=\wd\MONDAY
    \AdvanceDate[7]
  \else
    \AdvanceDate[7]
  \fi%
}
\usepackage{setspace}
\usepackage{multicol}
%\usepackage{indentfirst}
\usepackage{fancyhdr,lastpage}
\usepackage{url}
\pagestyle{fancy}
\usepackage{hyperref}
\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{layout}
\usepackage{amssymb}

\lhead{}
\chead{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Modify header here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\rhead{\footnotesize COMP 514}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Don't touch this %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lfoot{}
\cfoot{\small \thepage/\pageref*{LastPage}}
\rfoot{}

\usepackage{array, xcolor}
\usepackage{color,hyperref}
\definecolor{clemsonorange}{HTML}{EA6A20}
\hypersetup{colorlinks,breaklinks,linkcolor=clemsonorange,urlcolor=clemsonorange,anchorcolor=clemsonorange,citecolor=black}

%% I added this for code
\usepackage{listings}
\usepackage{color}

%% I added this for strikethrough
\usepackage[normalem]{ulem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newcommand\josue[1]{{\color{red}\sf{[Josue: #1]}}}
\newcommand\kai[1]{{\color{blue}\sf{[Kai: #1]}}}










\begin{document}
\maketitle
%\bigskip 
%\textbf{ONE PAGER: Submit a one-page description of the project, what is it about, your opinion, what needs to be done (related papers to read) and whether you have any ideas to improve the ideas involved. Describe why they are important or interesting, and provide some appropriate references.}
% BACKGROUND / WHAT OUR PROJECT IS
Spiking neural networks (SNNs) offer a perspective on machine learning different than traditional artificial neural networks (ANNs): SNNs encode data in a series of discrete time spikes, taking cue from the brain's action potentials arising from neuronal dynamics.  The human brain, which inspired today's artificial neural networks as advanced by Geoffrey Hinton, consists of nearly 100 billion neurons, each of which can have up to 15,000 synapses, has a massive parameter space, but only consumes about 15 W and takes up just 3 pounds.  This is accomplished in part by the fact that most neurons are inactive, thus exposing the brain as a sparse network: SNNs attempt to combine more biologically plausible network encoding mechanisms and architectures in order to achieve the performance of ANNs while also achieving the energy efficiency and robustness of the human brain.  Today, the development of SNNs is a growing field but lacks clear benchmarks and well-known toolsets, complete optimization of such nonlinear spiking dynamics, and robust mechanisms in encoding targets via a binary spike train, all of which are necessary in order to catch up with and potentially surpass ANNs. The main goal of the proposed literature review will be to chronicle the advances relevant to SNN optimization, with particular focused applied to relevant topics such as how sparsity can be leveraged and how such architectures can benefit from following biologically plausible models.\\
\\
% OPTIMIZATION FOCUS / INSPIRATION
From an optimization point of view, SNNs pose an interesting problem as, when compared to the corresponding relevant equations from ANNs, they lack interpretable models (with an added challenge of differentiating over the discrete spike trains, which thus prevent gradient calculation), and frequently rely on methods translated from ANNs which do not leverage the inherent advantages of SNNs.
% WHAT WE PROPOSE TO DO
In this literature review, we hope to compile a meta-analysis of recent SNN advances, providing a quantitative comparison between said implementations when possible.  Inherent to this process is determining standard datasets to act as benchmarks between different implementations: thus, the types of problems and datasets that previous works have experimented on will be considered, and proposals will be made for how to better showcase differences and advancements between recent innovations. For performance comparisons, we envision finding and establishing benchmark datasets so that we can rank existing SNN architectures in an accessible and  understandable format. \comment{provides an accessible GUI to immediately see what the best performing models for a vast number of datasets}.  In a similar vein of work, we believe it would be beneficial to pair such an analysis with a similar comparison of what tools / frameworks were used to create each model, and generally to catalogue what attributes various frameworks offer (and more importantly, what each framework is compatible with).  We believe such a characterization of tools available to researchers would be a valuable resource for unifying the field, making SNNs more accessible to new researchers. \\
\\
% CURRENT OPEN QUESTIONS
Current open questions in the field of interest include how to derive learning rules for the spike-based encoding of SNNs, adapting to the time-encoded features of spikes for information propagation, and generally how to find more efficient training approaches that leverage the inherent benefits of SNNs (e.g. the sparsity for improving training time, accuracy, and energy consumed).
\end{document}