
@article{willi_identifying_2019,
	title = {Identifying animal species in camera trap images using deep learning and citizen science},
	volume = {10},
	doi = {10.1111/2041-210X.13099},
	abstract = {Ecologists often study wildlife populations by deploying camera traps. Large datasets are generated using this approach which can be difficult for research teams to manually evaluate. Researchers increasingly enlist volunteers from the general public as citizen scientists to help classify images. The growing number of camera trap studies, however, makes it ever more challenging to find enough volunteers to process all projects in a timely manner. Advances in machine learning, especially deep learning, allow for accurate automatic image classification. By training models using existing datasets of images classified by citizen scientists and subsequent application of such models on new studies, human effort may be reduced substantially. The goals of this study were to (a) assess the accuracy of deep learning in classifying camera trap data, (b) investigate how to process datasets with only a few classified images that are generally difficult to model, and (c) apply a trained model on a live online citizen science project. Convolutional neural networks (CNNs) were used to differentiate among images of different animal species, images of humans or vehicles, and empty images (no animals, vehicles, or humans). We used four different camera trap datasets featuring a wide variety of species, different habitats, and a varying number of images. All datasets were labelled by citizen scientists on Zooniverse. Accuracies for identifying empty images across projects ranged between 91.2\% and 98.0\%, whereas accuracies for identifying specific species were between 88.7\% and 92.7\%. Transferring information from CNNs trained on large datasets (“transfer-learning”) was increasingly beneficial as the size of the training dataset decreased and raised accuracy by up to 10.3\%. Removing low-confidence predictions increased model accuracies to the level of citizen scientists. By combining a trained model with classifications from citizen scientists, human effort was reduced by 43\% while maintaining overall accuracy for a live experiment running on Zooniverse. Ecology researchers can significantly reduce image classification time and manual effort by combining citizen scientists and CNNs, enabling faster processing of data from large camera trap studies.},
	language = {en},
	number = {1},
	urldate = {2022-02-24},
	journal = {Methods in Ecology and Evolution},
	author = {Willi, Marco and Pitman, Ross T. and Cardoso, Anabelle W. and Locke, Christina and Swanson, Alexandra and Boyer, Amy and Veldthuis, Marten and Fortson, Lucy},
	year = {2019},
	keywords = {deep learning, machine learning, animal identification, camera trap, citizen science, convolutional neural networks},
	pages = {80--91},
	file = {Snapshot:/Users/ludwigbothmann/Zotero/storage/APVH7FQT/2041-210X.html:text/html;Willi et al. - 2019 - Identifying animal species in camera trap images u.pdf:/Users/ludwigbothmann/Zotero/storage/7MXJKDDL/Willi et al. - 2019 - Identifying animal species in camera trap images u.pdf:application/pdf},
}

@article{beery_efficient_2019,
    doi = {10.48550/arXiv.1907.06772},
	title = {Efficient {Pipeline} for {Camera} {Trap} {Image} {Review}},
	urldate = {2022-06-01},
	journal = {arXiv},
	author = {Beery, Sara and Morris, Dan and Yang, Siyu},
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: From the Data Mining and AI for Conservation Workshop at KDD19},
}

@article{beery_efficient_2019_note,
    doi = {10.48550/arXiv.1907.06772},
	title = {Efficient {Pipeline} for {Camera} {Trap} {Image} {Review}},
	urldate = {2022-06-01},
	journal = {arXiv},
	author = {Beery, Sara and Morris, Dan and Yang, Siyu},
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: From the Data Mining and AI for Conservation Workshop at KDD19},
        note = {doi: 10.48550/arXiv.1907.06772}
}

@article{carl_automated_2020,
	title = {Automated detection of {European} wild mammal species in camera trap images with an existing and pre-trained computer vision model},
	volume = {66},
	doi = {10.1007/s10344-020-01404-y},
	abstract = {The use of camera traps is a nonintrusive monitoring method to obtain valuable information about the appearance and behavior of wild animals. However, each study generates thousands of pictures and extracting information remains mostly an expensive, time-consuming manual task. Nevertheless, image recognition and analyzing technologies combined with machine learning algorithms, particularly deep learning models, improve and speed up the analysis process. Therefore, we tested the usability of a pre-trained deep learning model available on the TensorFlow hub–FasterRCNN+InceptionResNet V2 network applied to images of ten different European wild mammal species such as wild boar (Sus scrofa), roe deer (Capreolus capreolus), or red fox (Vulpes vulpes) in color as well as black and white infrared images. We found that the detection rate of the correct region of interest (region of the animal) was 94\%. The classification accuracy was 71\% for the correct species’ name as mammals and 93\% for the correct species or higher taxonomic ranks such as “carnivore” as order. In 7\% of cases, the classification was incorrect as the wrong species’ name was classified. In this technical note, we have shown the potential of an existing and pre-trained image classification model for wildlife animal detection, classification, and analysis. A specific training of the model on European wild mammal species could further increase the detection and classification accuracy of the models. Analysis of camera trap images could thus become considerably faster, less expensive, and more efficient.},
	language = {en},
	number = {4},
	urldate = {2022-05-31},
	journal = {European Journal of Wildlife Research},
	author = {Carl, Christin and Schönfeld, Fiona and Profft, Ingolf and Klamm, Alisa and Landgraf, Dirk},
	month = jul,
	year = {2020},
	keywords = {Camera trap, Computer vision, Image analysis, Pre-trained model, Wild mammal species},
	pages = {62},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/ADQXQYWE/Carl et al. - 2020 - Automated detection of European wild mammal specie.pdf:application/pdf},
}

@inproceedings{choinski_first_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {First} {Step} {Towards} {Automated} {Species} {Recognition} from {Camera} {Trap} {Images} of {Mammals} {Using} {AI} in a {European} {Temperate} {Forest}},
	doi = {10.1007/978-3-030-84340-3_24},
	abstract = {Camera traps are used worldwide to monitor wildlife. Despite the increasing availability of Deep Learning (DL) models, the effective usage of this technology to support wildlife monitoring is limited. This is mainly due to the complexity of DL technology and high computing requirements. This paper presents the implementation of the light-weight and state-of-the-art YOLOv5 architecture for automated labeling of camera trap images of mammals in the Białowieża Forest (BF), Poland. The camera trapping data were organized and harmonized using TRAPPER software, an open-source application for managing large-scale wildlife monitoring projects. The proposed image recognition pipeline achieved an average accuracy of 85\% F1-score in the identification of the 12 most commonly occurring medium-size and large mammal species in BF, using a limited set of training and testing data (a total of 2659 images with animals).},
	language = {en},
	booktitle = {Computer {Information} {Systems} and {Industrial} {Management}},
	publisher = {Springer International Publishing},
	author = {Choiński, Mateusz and Rogowski, Mateusz and Tynecki, Piotr and Kuijper, Dries P. J. and Churski, Marcin and Bubnicki, Jakub W.},
	editor = {Saeed, Khalid and Dvorský, Jiří},
	year = {2021},
	keywords = {Deep learning, Camera trap, Computer vision, TRAPPER, Wildlife, YOLOv5},
	pages = {299--310},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/4CYJC28H/Choiński et al. - 2021 - A First Step Towards Automated Species Recognition.pdf:application/pdf},
}

@article{christin_applications_2019,
	title = {Applications for deep learning in ecology},
	volume = {10},
	doi = {10.1111/2041-210X.13256},
	abstract = {A lot of hype has recently been generated around deep learning, a novel group of artificial intelligence approaches able to break accuracy records in pattern recognition. Over the course of just a few years, deep learning has revolutionized several research fields such as bioinformatics and medicine with its flexibility and ability to process large and complex datasets. As ecological datasets are becoming larger and more complex, we believe these methods can be useful to ecologists as well. In this paper, we review existing implementations and show that deep learning has been used successfully to identify species, classify animal behaviour and estimate biodiversity in large datasets like camera-trap images, audio recordings and videos. We demonstrate that deep learning can be beneficial to most ecological disciplines, including applied contexts, such as management and conservation. We also identify common questions about how and when to use deep learning, such as what are the steps required to create a deep learning network, which tools are available to help, and what are the requirements in terms of data and computer power. We provide guidelines, recommendations and useful resources, including a reference flowchart to help ecologists get started with deep learning. We argue that at a time when automatic monitoring of populations and ecosystems generates a vast amount of data that cannot be effectively processed by humans anymore, deep learning could become a powerful reference tool for ecologists.},
	language = {en},
	number = {10},
	urldate = {2022-02-24},
	journal = {Methods in Ecology and Evolution},
	author = {Christin, Sylvain and Hervet, Eric and Lecomte, Nicolas},
	year = {2019},
	keywords = {deep learning, neural network, artificial intelligence, automatic monitoring, ecology, pattern recognition},
	pages = {1632--1644},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/SM55C5IL/Christin et al. - 2019 - Applications for deep learning in ecology.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/P2MZMN33/2041-210X.html:text/html},
}

@article{conway_frame-by-frame_2021,
	title = {Frame-by-frame annotation of video recordings using deep neural networks},
	volume = {12},
	doi = {10.1002/ecs2.3384},
	abstract = {Video data are widely collected in ecological studies, but manual annotation is a challenging and time-consuming task, and has become a bottleneck for scientific research. Classification models based on convolutional neural networks (CNNs) have proved successful in annotating images, but few applications have extended these to video classification. We demonstrate an approach that combines a standard CNN summarizing each video frame with a recurrent neural network (RNN) that models the temporal component of video. The approach is illustrated using two datasets: one collected by static video cameras detecting seal activity inside coastal salmon nets and another collected by animal-borne cameras deployed on African penguins, used to classify behavior. The combined RNN-CNN led to a relative improvement in test set classification accuracy over an image-only model of 25\% for penguins (80\% to 85\%), and substantially improved classification precision or recall for four of six behavior classes (12–17\%). Image-only and video models classified seal activity with very similar accuracy (88 and 89\%), and no seal visits were missed entirely by either model. Temporal patterns related to movement provide valuable information about animal behavior, and classifiers benefit from including these explicitly. We recommend the inclusion of temporal information whenever manual inspection suggests that movement is predictive of class membership.},
	language = {en},
	number = {3},
	urldate = {2022-02-24},
	journal = {Ecosphere},
	author = {Conway, Alexander M. and Durbach, Ian N. and McInnes, Alistair and Harris, Robert N.},
	year = {2021},
	keywords = {deep learning, neural networks, animal-borne video, automated detection, image classification, video classification},
	pages = {e03384},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/33YQMJTZ/Conway et al. - 2021 - Frame-by-frame annotation of video recordings usin.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/9D4QBG54/ecs2.html:text/html},
}

@techreport{cunha_filtering_2021,
	title = {Filtering {Empty} {Camera} {Trap} {Images} in {Embedded} {Systems}},
	abstract = {Monitoring wildlife through camera traps produces a massive amount of images, whose a significant portion does not contain animals, being later discarded. Embedding deep learning models to identify animals and filter these images directly in those devices brings advantages such as savings in the storage and transmission of data, usually resource-constrained in this type of equipment. In this work, we present a comparative study on animal recognition models to analyze the trade-off between precision and inference latency on edge devices. To accomplish this objective, we investigate classifiers and object detectors of various input resolutions and optimize them using quantization and reducing the number of model filters. The confidence threshold of each model was adjusted to obtain 96\% recall for the nonempty class, since instances from the empty class are expected to be discarded. The experiments show that, when using the same set of images for training, detectors achieve superior performance, eliminating at least 10\% more empty images than classifiers with comparable latencies. Considering the high cost of generating labels for the detection problem, when there is a massive number of images labeled for classification (about one million instances, ten times more than those available for detection), classifiers are able to reach results comparable to detectors but with half latency.},
	number = {arXiv:2104.08859},
	urldate = {2022-05-24},
	institution = {arXiv},
	author = {Cunha, Fagner and Santos, Eulanda M. dos and Barreto, Raimundo and Colonna, Juan G.},
	month = apr,
	year = {2021},
	note = {arXiv:2104.08859 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2021 (Mobile AI workshop and challenges)},
	file = {arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/XJU4AA82/Cunha et al. - 2021 - Filtering Empty Camera Trap Images in Embedded Sys.pdf:application/pdf;arXiv.org Snapshot:/Users/ludwigbothmann/Zotero/storage/24CZ4GLT/2104.html:text/html},
}

@article{delisle_next-generation_2021,
	title = {Next-{Generation} {Camera} {Trapping}: {Systematic} {Review} of {Historic} {Trends} {Suggests} {Keys} to {Expanded} {Research} {Applications} in {Ecology} and {Conservation}},
	volume = {9},
	shorttitle = {Next-{Generation} {Camera} {Trapping}},
	abstract = {Camera trapping is an effective non-invasive method for collecting data on wildlife species to address questions of ecological and conservation interest. We reviewed 2,167 camera trap (CT) articles from 1994 to 2020. Through the lens of technological diffusion, we assessed trends in: (1) CT adoption measured by published research output, (2) topic, taxonomic, and geographic diversification and composition of CT applications, and (3) sampling effort, spatial extent, and temporal duration of CT studies. Annual publications of CT articles have grown 81-fold since 1994, increasing at a rate of 1.26 (SE = 0.068) per year since 2005, but with decelerating growth since 2017. Topic, taxonomic, and geographic richness of CT studies increased to encompass 100\% of topics, 59.4\% of ecoregions, and 6.4\% of terrestrial vertebrates. However, declines in per article rates of accretion and plateaus in Shannon's H for topics and major taxa studied suggest upper limits to further diversification of CT research as currently practiced. Notable compositional changes of topics included a decrease in capture-recapture, recent decrease in spatial-capture-recapture, and increases in occupancy, interspecific interactions, and automated image classification. Mammals were the dominant taxon studied; within mammalian orders carnivores exhibited a unimodal peak whereas primates, rodents and lagomorphs steadily increased. Among biogeographic realms we observed decreases in Oceania and Nearctic, increases in Afrotropic and Palearctic, and unimodal peaks for Indomalayan and Neotropic. Camera days, temporal extent, and area sampled increased, with much greater rates for the 0.90 quantile of CT studies compared to the median. Next-generation CT studies are poised to expand knowledge valuable to wildlife ecology and conservation by posing previously infeasible questions at unprecedented spatiotemporal scales, on a greater array of species, and in a wider variety of environments. Converting potential into broad-based application will require transferable models of automated image classification, and data sharing among users across multiple platforms in a coordinated manner. Further taxonomic diversification likely will require technological modifications that permit more efficient sampling of smaller species and adoption of recent improvements in modeling of unmarked populations. Environmental diversification can benefit from engineering solutions that expand ease of CT sampling in traditionally challenging sites.},
	urldate = {2022-02-24},
	journal = {Frontiers in Ecology and Evolution},
	author = {Delisle, Zackary J. and Flaherty, Elizabeth A. and Nobbe, Mackenzie R. and Wzientek, Cole M. and Swihart, Robert K.},
	year = {2021},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/FAVJFCSB/Delisle et al. - 2021 - Next-Generation Camera Trapping Systematic Review.pdf:application/pdf},
}

@article{elhamod_hierarchy-guided_2022,
	title = {Hierarchy-guided neural network for species classification},
	volume = {13},
	doi = {10.1111/2041-210X.13768},
	abstract = {Species classification is an important task which is the foundation of industrial, commercial, ecological and scientific applications involving the study of species distributions, dynamics and evolution. While conventional approaches for this task use off-the-shelf machine learning (ML) methods such as existing Convolutional Neural Network (ConvNet) architectures, there is an opportunity to inform the ConvNet architecture using our knowledge of biological hierarchies among taxonomic classes. In this work, we propose a new approach for species classification termed hierarchy-guided neural network (HGNN), which infuses hierarchical taxonomic information into the neural network's training to guide the structure and relationships among the extracted features. We perform extensive experiments on an illustrative use-case of classifying fish species to demonstrate that HGNN outperforms conventional ConvNet models in terms of classification accuracy, especially under scarce training data conditions. We also observe that HGNN shows better resilience to adversarial occlusions, when some of the most informative patch regions of the image are intentionally blocked and their effect on classification accuracy is studied.},
	language = {en},
	number = {3},
	urldate = {2022-03-03},
	journal = {Methods in Ecology and Evolution},
	author = {Elhamod, Mohannad and Diamond, Kelly M. and Maga, A. Murat and Bakis, Yasin and Bart Jr., Henry L. and Mabee, Paula and Dahdul, Wasila and Leipzig, Jeremy and Greenberg, Jane and Avants, Brian and Karpatne, Anuj},
	year = {2022},
	keywords = {neural networks, machine learning, species classification, science guided machine learning, taxonomy},
	pages = {642--652},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/XC8A44TZ/Elhamod et al. - 2022 - Hierarchy-guided neural network for species classi.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/IDL9SFAQ/2041-210X.html:text/html},
}

@article{gimenez_trade-off_2021,
	title = {Trade-off between deep learning for species identification and inference about predator-prey co-occurrence: {Reproducible} {R} workflow integrating models in computer vision and ecological statistics},
	urldate = {2022-03-03},
	author = {Gimenez, Olivier and Kervellec, Maëlis and Fanjul, Jean-Baptiste and Chaine, Anna and Marescot, Lucile and Bollet, Yoann and Duchamp, Christophe},
        doi = {10.48550/arXiv.2108.11509},
    journal = {arXiv},
	year = {2021},
	keywords = {Statistics - Applications, 62P12, 68T07, Quantitative Biology - Quantitative Methods},
	annote = {Comment: 22 pages, 5 figures},
}

@article{green_innovations_2020,
	title = {Innovations in {Camera} {Trapping} {Technology} and {Approaches}: {The} {Integration} of {Citizen} {Science} and {Artificial} {Intelligence}},
	volume = {10},
	shorttitle = {Innovations in {Camera} {Trapping} {Technology} and {Approaches}},
	doi = {10.3390/ani10010132},
	abstract = {Simple Summary
Camera traps, also known as “game cameras” or “trail cameras”, have increasingly been used in wildlife research over the last 20 years. Although early units were bulky and the set-up was complicated, modern camera traps are compact, integrated units able to collect vast digital datasets. Some of the challenges now facing researchers include the time required to view, classify, and sort all of the footage collected, as well as the logistics of establishing and maintaining camera trap sampling arrays across wide geographic areas. One solution to this problem is to enlist or recruit the public for help as ‘citizen scientists’ collecting and processing data. Artificial Intelligence (AI) is also being used to identify animals in digital photos and video; however, this process is relatively new, and machine-based classifications are not yet fully reliable. By combining citizen science with AI, it should be possible to improve efficiency and increase classification accuracy, while simultaneously maintaining and promoting the benefits associated with public engagement with, and awareness of, wildlife.

Abstract
Camera trapping has become an increasingly reliable and mainstream tool for surveying a diversity of wildlife species. Concurrent with this has been an increasing effort to involve the wider public in the research process, in an approach known as ‘citizen science’. To date, millions of people have contributed to research across a wide variety of disciplines as a result. Although their value for public engagement was recognised early on, camera traps were initially ill-suited for citizen science. As camera trap technology has evolved, cameras have become more user-friendly and the enormous quantities of data they now collect has led researchers to seek assistance in classifying footage. This has now made camera trap research a prime candidate for citizen science, as reflected by the large number of camera trap projects now integrating public participation. Researchers are also turning to Artificial Intelligence (AI) to assist with classification of footage. Although this rapidly-advancing field is already proving a useful tool, accuracy is variable and AI does not provide the social and engagement benefits associated with citizen science approaches. We propose, as a solution, more efforts to combine citizen science with AI to improve classification accuracy and efficiency while maintaining public involvement.},
	number = {1},
	urldate = {2022-02-24},
	journal = {Animals : an Open Access Journal from MDPI},
	author = {Green, Siân E. and Rees, Jonathan P. and Stephens, Philip A. and Hill, Russell A. and Giordano, Anthony J.},
	month = jan,
	year = {2020},
	pmid = {31947586},
	pmcid = {PMC7023201},
	pages = {132},
	file = {PubMed Central Full Text PDF:/Users/ludwigbothmann/Zotero/storage/E37EVMZ4/Green et al. - 2020 - Innovations in Camera Trapping Technology and Appr.pdf:application/pdf},
}

@article{jia_domain-aware_2022,
	title = {Domain-{Aware} {Neural} {Architecture} {Search} for {Classifying} {Animals} in {Camera} {Trap} {Images}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	doi = {10.3390/ani12040437},
	abstract = {Camera traps provide a feasible way for ecological researchers to observe wildlife, and they often produce millions of images of diverse species requiring classification. This classification can be automated via edge devices installed with convolutional neural networks, but networks may need to be customized per device because edge devices are highly heterogeneous and resource-limited. This can be addressed by a neural architecture search capable of automatically designing networks. However, search methods are usually developed based on benchmark datasets differing widely from camera trap images in many aspects including data distributions and aspect ratios. Therefore, we designed a novel search method conducted directly on camera trap images with lowered resolutions and maintained aspect ratios; the search is guided by a loss function whose hyper parameter is theoretically derived for finding lightweight networks. The search was applied to two datasets and led to lightweight networks tested on an edge device named NVIDIA Jetson X2. The resulting accuracies were competitive in comparison. Conclusively, researchers without knowledge of designing networks can obtain networks optimized for edge devices and thus establish or expand surveillance areas in a cost-effective way.},
	language = {en},
	number = {4},
	urldate = {2022-05-31},
	journal = {Animals},
	author = {Jia, Liang and Tian, Ye and Zhang, Junguo},
	month = jan,
	year = {2022},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {convolutional neural network, camera trap images, neural architecture search},
	pages = {437},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/E4QFLCHB/Jia et al. - 2022 - Domain-Aware Neural Architecture Search for Classi.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/SQFYTK9C/437.html:text/html},
}

@inproceedings{koh_wilds_2021,
	title = {{WILDS}: {A} {Benchmark} of in-the-{Wild} {Distribution} {Shifts}},
	shorttitle = {{WILDS}},
	language = {en},
	urldate = {2022-05-31},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and Lee, Tony and David, Etienne and Stavness, Ian and Guo, Wei and Earnshaw, Berton and Haque, Imran and Beery, Sara M. and Leskovec, Jure and Kundaje, Anshul and Pierson, Emma and Levine, Sergey and Finn, Chelsea and Liang, Percy},
	year = {2021},
	file = {koh21a.pdf:/Users/ludwigbothmann/Zotero/storage/SX6ANASL/koh21a.pdf:application/pdf},
}

@article{mac_aodha_presence-only_2019,
	title = {Presence-{Only} {Geographical} {Priors} for {Fine}-{Grained} {Image} {Classification}},
	abstract = {Appearance information alone is often not sufﬁcient to accurately differentiate between ﬁne-grained visual categories. Human experts make use of additional cues such as where, and when, a given image was taken in order to inform their ﬁnal decision. This contextual information is readily available in many online image collections but has been underutilized by existing image classiﬁers that focus solely on making predictions based on the image contents.},
	language = {en},
	urldate = {2022-03-03},
	journal = {arXiv:1906.05272 [cs]},
	author = {Mac Aodha, Oisin and Cole, Elijah and Perona, Pietro},
	month = oct,
	year = {2019},
	note = {arXiv: 1906.05272},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICCV 2019},
	file = {Mac Aodha et al. - 2019 - Presence-Only Geographical Priors for Fine-Grained.pdf:/Users/ludwigbothmann/Zotero/storage/HCWGL63K/Mac Aodha et al. - 2019 - Presence-Only Geographical Priors for Fine-Grained.pdf:application/pdf},
}

@article{miao_insights_2019,
	title = {Insights and approaches using deep learning to classify wildlife},
	volume = {9},
	doi = {10.1038/s41598-019-44565-w},
	language = {en},
	number = {1},
	urldate = {2022-02-24},
	journal = {Scientific Reports},
	author = {Miao, Zhongqi and Gaynor, Kaitlyn M. and Wang, Jiayun and Liu, Ziwei and Muellerklein, Oliver and Norouzzadeh, Mohammad Sadegh and McInturff, Alex and Bowie, Rauri C. K. and Nathan, Ran and Yu, Stella X. and Getz, Wayne M.},
	month = dec,
	year = {2019},
	pages = {8137},
	file = {Miao et al. - 2019 - Insights and approaches using deep learning to cla.pdf:/Users/ludwigbothmann/Zotero/storage/YT6UY45A/Miao et al. - 2019 - Insights and approaches using deep learning to cla.pdf:application/pdf},
}

@article{miao_iterative_2021,
	title = {Iterative human and automated identification of wildlife images},
	volume = {3},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	doi = {10.1038/s42256-021-00393-0},
	language = {en},
	number = {10},
	urldate = {2022-03-03},
	journal = {Nature Machine Intelligence},
	author = {Miao, Zhongqi and Liu, Ziwei and Gaynor, Kaitlyn M. and Palmer, Meredith S. and Yu, Stella X. and Getz, Wayne M.},
	year = {2021},
        keywords = {Biodiversity, Computer science, Conservation biology, Scientific data},
	pages = {885--895},
	file = {Eingereichte Version:/Users/ludwigbothmann/Zotero/storage/6YPT3UH8/Miao et al. - 2021 - Iterative human and automated identification of wi.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/S6TQ8VXN/s42256-021-00393-0.html:text/html},
}

@article{norouzzadeh_deep_2021,
	title = {A deep active learning system for species identification and counting in camera trap images},
	volume = {12},
	doi = {10.1111/2041-210X.13504},
	abstract = {A typical camera trap survey may produce millions of images that require slow, expensive manual review. Consequently, critical conservation questions may be answered too slowly to support decision-making. Recent studies demonstrated the potential for computer vision to dramatically increase efficiency in image-based biodiversity surveys; however, the literature has focused on projects with a large set of labelled training images, and hence many projects with a smaller set of labelled images cannot benefit from existing machine learning techniques. Furthermore, even sizable projects have struggled to adopt computer vision methods because classification models overfit to specific image backgrounds (i.e. camera locations). In this paper, we combine the power of machine intelligence and human intelligence via a novel active learning system to minimize the manual work required to train a computer vision model. Furthermore, we utilize object detection models and transfer learning to prevent overfitting to camera locations. To our knowledge, this is the first work to apply an active learning approach to camera trap images. Our proposed scheme can match state-of-the-art accuracy on a 3.2 million image dataset with as few as 14,100 manual labels, which means decreasing manual labelling effort by over 99.5\%. Our trained models are also less dependent on background pixels, since they operate only on cropped regions around animals. The proposed active deep learning scheme can significantly reduce the manual labour required to extract information from camera trap images. Automation of information extraction will not only benefit existing camera trap projects, but can also catalyse the deployment of larger camera trap arrays.},
	language = {en},
	number = {1},
	urldate = {2022-02-24},
	journal = {Methods in Ecology and Evolution},
	author = {Norouzzadeh, Mohammad Sadegh and Morris, Dan and Beery, Sara and Joshi, Neel and Jojic, Nebojsa and Clune, Jeff},
	year = {2021},
	keywords = {active learning, deep learning, computer vision, deep neural networks, camera trap images},
	pages = {150--161},
	file = {Methods Ecol Evol - 2020 - Norouzzadeh - A deep active learning system for species identification and counting in camera.pdf:/Users/ludwigbothmann/Zotero/storage/HL73TMGF/Methods Ecol Evol - 2020 - Norouzzadeh - A deep active learning system for species identification and counting in camera.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/JW7ULQSK/2041-210X.html:text/html},
}

@article{norouzzadeh_automatically_2018,
	title = {Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning},
	volume = {115},
	doi = {10.1073/pnas.1719367115},
	abstract = {Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences. Motion-sensor “camera traps” enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with {\textgreater}93.8\% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3\% of the data while still performing at the same 96.6\% accuracy as that of crowdsourced teams of human volunteers, saving {\textgreater}8.4 y (i.e., {\textgreater}17,000 h at 40 h/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.},
	language = {en},
	number = {25},
	urldate = {2022-02-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Norouzzadeh, Mohammad Sadegh and Nguyen, Anh and Kosmala, Margaret and Swanson, Alexandra and Palmer, Meredith S. and Packer, Craig and Clune, Jeff},
	year = {2018},
	pages = {E5716--E5725},
	file = {Norouzzadeh et al. - 2018 - Automatically identifying, counting, and describin.pdf:/Users/ludwigbothmann/Zotero/storage/VUELTHCU/Norouzzadeh et al. - 2018 - Automatically identifying, counting, and describin.pdf:application/pdf},
}

@article{schneider_three_2020,
	title = {Three critical factors affecting automated image species recognition performance for camera traps},
	volume = {10},
	doi = {10.1002/ece3.6147},
	abstract = {Ecological camera traps are increasingly used by wildlife biologists to unobtrusively monitor an ecosystems animal population. However, manual inspection of the images produced is expensive, laborious, and time-consuming. The success of deep learning systems using camera trap images has been previously explored in preliminary stages. These studies, however, are lacking in their practicality. They are primarily focused on extremely large datasets, often millions of images, and there is little to no focus on performance when tasked with species identification in new locations not seen during training. Our goal was to test the capabilities of deep learning systems trained on camera trap images using modestly sized training data, compare performance when considering unseen background locations, and quantify the gradient of lower bound performance to provide a guideline of data requirements in correspondence to performance expectations. We use a dataset provided by Parks Canada containing 47,279 images collected from 36 unique geographic locations across multiple environments. Images represent 55 animal species and human activity with high-class imbalance. We trained, tested, and compared the capabilities of six deep learning computer vision networks using transfer learning and image augmentation: DenseNet201, Inception-ResNet-V3, InceptionV3, NASNetMobile, MobileNetV2, and Xception. We compare overall performance on “trained” locations where DenseNet201 performed best with 95.6\% top-1 accuracy showing promise for deep learning methods for smaller scale research efforts. Using trained locations, classifications with {\textless}500 images had low and highly variable recall of 0.750 ± 0.329, while classifications with over 1,000 images had a high and stable recall of 0.971 ± 0.0137. Models tasked with classifying species from untrained locations were less accurate, with DenseNet201 performing best with 68.7\% top-1 accuracy. Finally, we provide an open repository where ecologists can insert their image data to train and test custom species detection models for their desired ecological domain.},
	language = {en},
	number = {7},
	urldate = {2022-02-24},
	journal = {Ecology and Evolution},
	author = {Schneider, Stefan and Greenberg, Saul and Taylor, Graham W. and Kremer, Stefan C.},
	year = {2020},
	keywords = {monitoring, deep learning, computer vision, camera traps, convolutional networks, density estimation, population dynamics, species classification},
	pages = {3503--3517},
	file = {Schneider et al. - 2020 - Three critical factors affecting automated image s.pdf:/Users/ludwigbothmann/Zotero/storage/KVEMJG5W/Schneider et al. - 2020 - Three critical factors affecting automated image s.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/HHUMGBP5/ece3.html:text/html},
}

@inproceedings{schneider_similarity_2020,
	address = {Snowmass, CO, USA},
	title = {Similarity {Learning} {Networks} for {Animal} {Individual} {Re}-{Identification} - {Beyond} the {Capabilities} of a {Human} {Observer}},
	doi = {10.1109/WACVW50321.2020.9096925},
	abstract = {Deep learning has become the standard methodology to approach computer vision tasks when large amounts of labeled data are available. One area where traditional deep learning approaches fail to perform is one-shot learning tasks where a model must correctly classify a new category after seeing only one example. One such domain is animal re-identiﬁcation, an application of computer vision which can be used globally as a method to automate species population estimates from camera trap images. Our work demonstrates both the application of similarity comparison networks to animal re-identiﬁcation, as well as the capabilities of deep convolutional neural networks to generalize across domains. Few studies have considered animal reidentiﬁcation methods across species. Here, we compare two similarity comparison methodologies: Siamese and Triplet-Loss, based on the AlexNet, VGG-19, DenseNet201, MobileNetV2, and InceptionV3 architectures considering mean average precision (mAP)@1 and mAP@5. We consider ﬁve data sets corresponding to ﬁve different species: humans, chimpanzees, humpback whales, fruit ﬂies, and Siberian tigers, each with their own unique set of challenges. We demonstrate that Triplet Loss outperformed its Siamese counterpart for all species. Without any speciesspeciﬁc modiﬁcations, our results demonstrate that similarity comparison networks can reach a performance level beyond that of humans for the task of animal re-identiﬁcation. The ability for researchers to re-identify an animal individual upon re-encounter is fundamental for addressing a broad range of questions in the study of population dynamics and community/behavioural ecology. Our expectation is that similarity comparison networks are the beginning of a major trend that could stand to revolutionize animal reidentiﬁcation from camera trap data.},
	language = {en},
	urldate = {2022-02-24},
	booktitle = {2020 {IEEE} {Winter} {Applications} of {Computer} {Vision} {Workshops} ({WACVW})},
	publisher = {IEEE},
	author = {Schneider, Stefan and Taylor, Graham W. and Kremer, Stefan C.},
	month = mar,
	year = {2020},
	pages = {44--52},
	file = {Schneider et al. - 2020 - Similarity Learning Networks for Animal Individual.pdf:/Users/ludwigbothmann/Zotero/storage/9L2NPRML/Schneider et al. - 2020 - Similarity Learning Networks for Animal Individual.pdf:application/pdf},
}

@article{schneider_past_2019,
	title = {Past, present and future approaches using computer vision for animal re-identification from camera trap data},
	volume = {10},
	doi = {10.1111/2041-210X.13133},
	abstract = {The ability of a researcher to re-identify (re-ID) an individual animal upon re-encounter is fundamental for addressing a broad range of questions in the study of ecosystem function, community and population dynamics and behavioural ecology. Tagging animals during mark and recapture studies is the most common method for reliable animal re-ID; however, camera traps are a desirable alternative, requiring less labour, much less intrusion and prolonged and continuous monitoring into an environment. Despite these advantages, the analyses of camera traps and video for re-ID by humans are criticized for their biases related to human judgement and inconsistencies between analyses. In this review, we describe a brief history of camera traps for re-ID, present a collection of computer vision feature engineering methodologies previously used for animal re-ID, provide an introduction to the underlying mechanisms of deep learning relevant to animal re-ID, highlight the success of deep learning methods for human re-ID, describe the few ecological studies currently utilizing deep learning for camera trap analyses and our predictions for near future methodologies based on the rapid development of deep learning methods. For decades, ecologists with expertise in computer vision have successfully utilized feature engineering to extract meaningful features from camera trap images to improve the statistical rigor of individual comparisons and remove human bias from their camera trap analyses. Recent years have witnessed the emergence of deep learning systems which have demonstrated the accurate re-ID of humans based on image and video data with near perfect accuracy. Despite this success, ecologists have yet to utilize these approaches for animal re-ID. By utilizing novel deep learning methods for object detection and similarity comparisons, ecologists can extract animals from an image/video data and train deep learning classifiers to re-ID animal individuals beyond the capabilities of a human observer. This methodology will allow ecologists with camera/video trap data to reidentify individuals that exit and re-enter the camera frame. Our expectation is that this is just the beginning of a major trend that could stand to revolutionize the analysis of camera trap data and, ultimately, our approach to animal ecology.},
	language = {en},
	number = {4},
	urldate = {2022-02-24},
	journal = {Methods in Ecology and Evolution},
	author = {Schneider, Stefan and Taylor, Graham W. and Linquist, Stefan and Kremer, Stefan C.},
	year = {2019},
	keywords = {monitoring, deep learning, computer vision, camera traps, convolutional networks, density estimation, animal reidentification, object detection},
	pages = {461--470},
	file = {Methods Ecol Evol - 2018 - Schneider - Past  present and future approaches using computer vision for animal.pdf:/Users/ludwigbothmann/Zotero/storage/F6ILIRVW/Methods Ecol Evol - 2018 - Schneider - Past  present and future approaches using computer vision for animal.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/6WQBDU5P/2041-210X.html:text/html},
}

@article{swinnen_novel_2014,
	title = {A {Novel} {Method} to {Reduce} {Time} {Investment} {When} {Processing} {Videos} from {Camera} {Trap} {Studies}},
	volume = {9},
	doi = {10.1371/journal.pone.0098881},
	abstract = {Camera traps have proven very useful in ecological, conservation and behavioral research. Camera traps non-invasively record presence and behavior of animals in their natural environment. Since the introduction of digital cameras, large amounts of data can be stored. Unfortunately, processing protocols did not evolve as fast as the technical capabilities of the cameras. We used camera traps to record videos of Eurasian beavers (Castor fiber). However, a large number of recordings did not contain the target species, but instead empty recordings or other species (together non-target recordings), making the removal of these recordings unacceptably time consuming. In this paper we propose a method to partially eliminate non-target recordings without having to watch the recordings, in order to reduce workload. Discrimination between recordings of target species and non-target recordings was based on detecting variation (changes in pixel values from frame to frame) in the recordings. Because of the size of the target species, we supposed that recordings with the target species contain on average much more movements than non-target recordings. Two different filter methods were tested and compared. We show that a partial discrimination can be made between target and non-target recordings based on variation in pixel values and that environmental conditions and filter methods influence the amount of non-target recordings that can be identified and discarded. By allowing a loss of 5\% to 20\% of recordings containing the target species, in ideal circumstances, 53\% to 76\% of non-target recordings can be identified and discarded. We conclude that adding an extra processing step in the camera trap protocol can result in large time savings. Since we are convinced that the use of camera traps will become increasingly important in the future, this filter method can benefit many researchers, using it in different contexts across the globe, on both videos and photographs.},
	language = {en},
	number = {6},
	urldate = {2022-02-24},
	journal = {PLoS ONE},
	author = {Swinnen, Kristijn R. R. and Reijniers, Jonas and Breno, Matteo and Leirs, Herwig},
	editor = {Goodrich, John},
	month = jun,
	year = {2014},
	pages = {e98881},
	file = {swinnen_et_al2014.pdf:/Users/ludwigbothmann/Zotero/storage/PFICYLHM/swinnen_et_al2014.pdf:application/pdf},
}

@article{tabak_cameratrapdetector_2022,
	type = {preprint},
	title = {{CameraTrapDetectoR}: {Automatically} detect, classify, and count animals in camera trap images using artificial intelligence},
	shorttitle = {{CameraTrapDetectoR}},
	language = {en},
	urldate = {2022-03-18},
	institution = {Ecology},
	author = {Tabak, Michael A. and Falbel, Daniel and Hamzeh, Tess and Brook, Ryan K. and Goolsby, John A. and Zoromski, Lisa D. and Boughton, Raoul K. and Snow, Nathan P. and VerCauteren, Kurt C. and Miller, Ryan S.},
        journal = {bioRxiv},
	year = {2022},
	doi = {10.1101/2022.02.07.479461},
	file = {Tabak et al. - 2022 - CameraTrapDetectoR Automatically detect, classify.pdf:/Users/ludwigbothmann/Zotero/storage/EQDZ8H69/Tabak et al. - 2022 - CameraTrapDetectoR Automatically detect, classify.pdf:application/pdf},
}

@article{tabak_machine_2019,
	title = {Machine learning to classify animal species in camera trap images: {Applications} in ecology},
	volume = {10},
	shorttitle = {Machine learning to classify animal species in camera trap images},
	doi = {10.1111/2041-210X.13120},
	abstract = {Motion-activated cameras (“camera traps”) are increasingly used in ecological and management studies for remotely observing wildlife and are amongst the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analysed, typically by visually observing each image, in order to extract data that can be used in ecological analyses. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or “out-of-distribution” in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98\% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82\% accuracy and correctly identified 94\% of images containing an animal in the dataset from Tanzania. We provide an r package (Machine Learning for Wildlife Image Classification) that allows the users to (a) use the trained model presented here and (b) train their own model using classified images of wildlife from their studies. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analysing images. Our r package makes these methods accessible to ecologists.},
	language = {en},
	number = {4},
	urldate = {2022-02-24},
	journal = {Methods in Ecology and Evolution},
	author = {Tabak, Michael A. and Norouzzadeh, Mohammad S. and Wolfson, David W. and Sweeney, Steven J. and Vercauteren, Kurt C. and Snow, Nathan P. and Halseth, Joseph M. and Di Salvo, Paul A. and Lewis, Jesse S. and White, Michael D. and Teton, Ben and Beasley, James C. and Schlichting, Peter E. and Boughton, Raoul K. and Wight, Bethany and Newkirk, Eric S. and Ivan, Jacob S. and Odell, Eric A. and Brook, Ryan K. and Lukacs, Paul M. and others},
	year = {2019},
	keywords = {machine learning, artificial intelligence, image classification, remote sensing, camera trap, convolutional neural network, deep neural networks, r package},
	pages = {585--590},
	file = {Methods Ecol Evol - 2018 - Tabak - Machine learning to classify animal species in camera trap images  Applications in.pdf:/Users/ludwigbothmann/Zotero/storage/HP29SDV9/Methods Ecol Evol - 2018 - Tabak - Machine learning to classify animal species in camera trap images  Applications in.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/75HF6NA9/2041-210X.html:text/html},
}

@article{tabak_improving_2020,
	title = {Improving the accessibility and transferability of machine learning algorithms for identification of animals in camera trap images: {MLWIC2}},
	volume = {10},
	shorttitle = {Improving the accessibility and transferability of machine learning algorithms for identification of animals in camera trap images},
	doi = {10.1002/ece3.6692},
	abstract = {Motion-activated wildlife cameras (or “camera traps”) are frequently used to remotely and noninvasively observe animals. The vast number of images collected from camera trap projects has prompted some biologists to employ machine learning algorithms to automatically recognize species in these images, or at least filter-out images that do not contain animals. These approaches are often limited by model transferability, as a model trained to recognize species from one location might not work as well for the same species in different locations. Furthermore, these methods often require advanced computational skills, making them inaccessible to many biologists. We used 3 million camera trap images from 18 studies in 10 states across the United States of America to train two deep neural networks, one that recognizes 58 species, the “species model,” and one that determines if an image is empty or if it contains an animal, the “empty-animal model.” Our species model and empty-animal model had accuracies of 96.8\% and 97.3\%, respectively. Furthermore, the models performed well on some out-of-sample datasets, as the species model had 91\% accuracy on species from Canada (accuracy range 36\%–91\% across all out-of-sample datasets) and the empty-animal model achieved an accuracy of 91\%–94\% on out-of-sample datasets from different continents. Our software addresses some of the limitations of using machine learning to classify images from camera traps. By including many species from several locations, our species model is potentially applicable to many camera trap studies in North America. We also found that our empty-animal model can facilitate removal of images without animals globally. We provide the trained models in an R package (MLWIC2: Machine Learning for Wildlife Image Classification in R), which contains Shiny Applications that allow scientists with minimal programming experience to use trained models and train new models in six neural network architectures with varying depths.},
	language = {en},
	number = {19},
	urldate = {2022-02-24},
	journal = {Ecology and Evolution},
	author = {Tabak, Michael A. and Norouzzadeh, Mohammad S. and Wolfson, David W. and Newton, Erica J. and Boughton, Raoul K. and Ivan, Jacob S. and Odell, Eric A. and Newkirk, Eric S. and Conrey, Reesa Y. and Stenglein, Jennifer and Iannarilli, Fabiola and Erb, John and Brook, Ryan K. and Davis, Amy J. and Lewis, Jesse and Walsh, Daniel P. and Beasley, James C. and VerCauteren, Kurt C. and Clune, Jeff and Miller, Ryan S.},
	year = {2020},
	keywords = {machine learning, image classification, computer vision, deep convolutional neural networks, motion-activated camera, R package, remote sensing, species identification},
	pages = {10374--10383},
	file = {Snapshot:/Users/ludwigbothmann/Zotero/storage/EJS7QE43/ece3.html:text/html;Tabak et al. - 2020 - Improving the accessibility and transferability of.pdf:/Users/ludwigbothmann/Zotero/storage/II82EIKJ/Tabak et al. - 2020 - Improving the accessibility and transferability of.pdf:application/pdf},
}

@article{tuia_perspectives_2022,
	title = {Perspectives in machine learning for wildlife conservation},
	volume = {13},
	doi = {10.1038/s41467-022-27980-y},
	abstract = {Abstract
            Inexpensive and accessible sensors are accelerating data acquisition in animal ecology. These technologies hold great potential for large-scale ecological understanding, but are limited by current processing approaches which inefficiently distill data into relevant information. We argue that animal ecologists can capitalize on large datasets generated by modern sensors by combining machine learning approaches with domain knowledge. Incorporating machine learning into ecological workflows could improve inputs for ecological models and lead to integrated hybrid modeling tools. This approach will require close interdisciplinary collaboration to ensure the quality of novel approaches and train a new generation of data scientists in ecology and conservation.},
	language = {en},
	number = {1},
	urldate = {2022-03-18},
	journal = {Nature Communications},
	author = {Tuia, Devis and Kellenberger, Benjamin and Beery, Sara and Costelloe, Blair R. and Zuffi, Silvia and Risse, Benjamin and Mathis, Alexander and Mathis, Mackenzie W. and van Langevelde, Frank and Burghardt, Tilo and Kays, Roland and Klinck, Holger and Wikelski, Martin and Couzin, Iain D. and van Horn, Grant and Crofoot, Margaret C. and Stewart, Charles V. and Berger-Wolf, Tanya},
	year = {2022},
	pages = {792},
	file = {Tuia et al. - 2022 - Perspectives in machine learning for wildlife cons.pdf:/Users/ludwigbothmann/Zotero/storage/DWNKJERI/Tuia et al. - 2022 - Perspectives in machine learning for wildlife cons.pdf:application/pdf;Tuia et al. - 2022 - Perspectives in machine learning for wildlife cons.pdf:/Users/ludwigbothmann/Zotero/storage/IRPIEKAA/Tuia et al. - 2022 - Perspectives in machine learning for wildlife cons.pdf:application/pdf},
}

@article{velez_choosing_2022,
	title = {An evaluation of platforms for processing camera-trap data using artificial intelligence},
	author = {Vélez, Juliana and McShea, William and Shamon, Hila and Castiblanco-Camacho, Paula J. and Tabak, Michael A. and Chalmers, Carl and Fergus, Paul and Fieberg, John},
	year = {2023},
        doi = {10.1111/2041-210X.14044},
    journal = {Methods in Ecology and Evolution},
    volume = {14},
    pages = {459--477}
}

@article{waldchen_machine_2018,
	title = {Machine learning for image based species identification},
	volume = {9},
	doi = {10.1111/2041-210X.13075},
	abstract = {Accurate species identification is the basis for all aspects of taxonomic research and is an essential component of workflows in biological research. Biologists are asking for more efficient methods to meet the identification demand. Smart mobile devices, digital cameras as well as the mass digitisation of natural history collections led to an explosion of openly available image data depicting living organisms. This rapid increase in biological image data in combination with modern machine learning methods, such as deep learning, offers tremendous opportunities for automated species identification. In this paper, we focus on deep learning neural networks as a technology that enabled breakthroughs in automated species identification in the last 2 years. In order to stimulate more work in this direction, we provide a brief overview of machine learning frameworks applicable to the species identification problem. We review selected deep learning approaches for image based species identification and introduce publicly available applications. Eventually, this article aims to provide insights into the current state-of-the-art in automated identification and to serve as a starting point for researchers willing to apply novel machine learning techniques in their biological studies. While modern machine learning approaches only slowly pave their way into the field of species identification, we argue that we are going to see a proliferation of these techniques being applied to the problem in the future. Artificial intelligence systems will provide alternative tools for taxonomic identification in the near future.},
	language = {en},
	number = {11},
	urldate = {2022-02-24},
	journal = {Methods in Ecology and Evolution},
	author = {Wäldchen, Jana and Mäder, Patrick},
	year = {2018},
	keywords = {deep learning, computer vision, automated species identification, convolutional neural network, images},
	pages = {2216--2225},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/E273PRHA/Wäldchen and Mäder - 2018 - Machine learning for image based species identific.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/L7BA36J3/2041-210X.html:text/html},
}

@article{whytock_robust_2021,
	title = {Robust ecological analysis of camera trap data labelled by a machine learning model},
	volume = {12},
	doi = {10.1111/2041-210X.13576},
	abstract = {Ecological data are collected over vast geographic areas using digital sensors such as camera traps and bioacoustic recorders. Camera traps have become the standard method for surveying many terrestrial mammals and birds, but camera trap arrays often generate millions of images that are time-consuming to label. This causes significant latency between data collection and subsequent inference, which impedes conservation at a time of ecological crisis. Machine learning algorithms have been developed to improve the speed of labelling camera trap data, but it is uncertain how the outputs of these models can be used in ecological analyses without secondary validation by a human. Here, we present our approach to developing, testing and applying a machine learning model to camera trap data for the purpose of achieving fully automated ecological analyses. As a case-study, we built a model to classify 26 Central African forest mammal and bird species (or groups). The model generalizes to new spatially and temporally independent data (n = 227 camera stations, n = 23,868 images), and outperforms humans in several respects (e.g. detecting ‘invisible’ animals). We demonstrate how ecologists can evaluate a machine learning model's precision and accuracy in an ecological context by comparing species richness, activity patterns (n = 4 species tested) and occupancy (n = 4 species tested) derived from machine learning labels with the same estimates derived from expert labels. Results show that fully automated species labels can be equivalent to expert labels when calculating species richness, activity patterns (n = 4 species tested) and estimating occupancy (n = 3 of 4 species tested) in a large, completely out-of-sample test dataset. Simple thresholding using the Softmax values (i.e. excluding ‘uncertain’ labels) improved the model's performance when calculating activity patterns and estimating occupancy but did not improve estimates of species richness. We conclude that, with adequate testing and evaluation in an ecological context, a machine learning model can generate labels for direct use in ecological analyses without the need for manual validation. We provide the user-community with a multi-platform, multi-language graphical user interface that can be used to run our model offline.},
	language = {en},
	number = {6},
	urldate = {2022-02-24},
	journal = {Methods in Ecology and Evolution},
	author = {Whytock, Robin C. and Świeżewski, Jędrzej and Zwerts, Joeri A. and Bara-Słupski, Tadeusz and Koumba Pambo, Aurélie Flore and Rogala, Marek and Bahaa-el-din, Laila and Boekee, Kelly and Brittain, Stephanie and Cardoso, Anabelle W. and Henschel, Philipp and Lehmann, David and Momboua, Brice and Kiebou Opepa, Cisquet and Orbell, Christopher and Pitman, Ross T. and Robinson, Hugh S. and Abernethy, Katharine A.},
	year = {2021},
	keywords = {artificial intelligence, biodiversity, birds, Central Africa, mammals},
	pages = {1080--1092},
	file = {Snapshot:/Users/ludwigbothmann/Zotero/storage/C7V2R5JT/2041-210X.html:text/html;Whytock et al. - 2021 - Robust ecological analysis of camera trap data lab.pdf:/Users/ludwigbothmann/Zotero/storage/E7RNPB2D/Whytock et al. - 2021 - Robust ecological analysis of camera trap data lab.pdf:application/pdf},
}

@article{gal_deep_2017,
	title = {Deep {Bayesian} {Active} {Learning} with {Image} {Data}},
	abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
	author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
	month = mar,
	year = {2017},
	keywords = {active learning, 2\_read},
}

@misc{houlsby_bayesian_2011,
	title = {Bayesian {Active} {Learning} for {Classification} and {Preference} {Learning}},
	abstract = {Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.},
	author = {Houlsby, Neil and Huszár, Ferenc and Ghahramani, Zoubin and Lengyel, Máté},
	year = {2011},
	keywords = {active learning, 2\_read},
}

@article{budd_survey_2021,
	title = {A {Survey} on {Active} {Learning} and {Human}-in-the-{Loop} {Deep} {Learning} for {Medical} {Image} {Analysis}},
	volume = {71},
	doi = {doi.org/10.1016/j.media.2021.102062},
	abstract = {Fully automatic deep learning has become the state-of-the-art technique for many tasks including image acquisition, analysis and interpretation, and for the extraction of clinically useful information for computer-aided detection, diagnosis, treatment planning, intervention and therapy. However, the unique challenges posed by medical image analysis suggest that retaining a human end user in any deep learning enabled system will be beneficial. In this review we investigate the role that humans might play in the development and deployment of deep learning enabled diagnostic applications and focus on techniques that will retain a significant input from a human end user. Human-in-the-Loop computing is an area that we see as increasingly important in future research due to the safety-critical nature of working in the medical domain. We evaluate four key areas that we consider vital for deep learning in the clinical practice: (1) Active Learning to choose the best data to annotate for optimal model performance; (2) Interaction with model outputs - using iterative feedback to steer models to optima for a given prediction and offering meaningful ways to interpret and respond to predictions; (3) Practical considerations - developing full scale applications and the key considerations that need to be made before deployment; (4) Future Prospective and Unanswered Questions - knowledge gaps and related research fields that will benefit human-in-the-loop computing as they evolve. We offer our opinions on the most promising directions of research and how various aspects of each area might be unified towards common goals.},
	journal = {Medical Image Analysis, Volume 71, 2021, 102062, ISSN 1361-8415},
	author = {Budd, Samuel and Robinson, Emma C. and Kainz, Bernhard},
	year = {2021},
	keywords = {active learning, 1\_skimmed},
}

@inproceedings{auer_minimizing_2021,
	title = {Minimizing the {Annotation} {Effort} for {Detecting} {Wildlife} in {Camera} {Trap} {Images} with {Active} {Learning}},
	doi = {10.18420/informatik2021-042},
	booktitle = {{INFORMATIK} 2021},
	publisher = {Gesellschaft für Informatik, Bonn},
	author = {Auer, Daphne and Bodesheim, Paul and Fiderer, Christian and Heurich, Marco and Denzler, Joachim},
	year = {2021},
	pages = {547--564},
	file = {Auer et al. - Minimizing the Annotation Effort for Detecting Wil.pdf:/Users/ludwigbothmann/Zotero/storage/RF8RKJ5K/Auer et al. - Minimizing the Annotation Effort for Detecting Wil.pdf:application/pdf},
}

@article{bischl_hyperparameter_2023,
	title = {Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges},
     doi = {10.1002/widm.1484},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and Deng, Difan and Lindauer, Marius},
	year = {2023},
}

@article{ahmed_semantic_2019,
	title = {Semantic region of interest and species classification in the deep neural network feature domain},
	volume = {52},
	issn = {1574-9541},
	doi = {10.1016/j.ecoinf.2019.05.006},
	abstract = {In this paper, we focus on animal object detection and species classification in camera-trap images collected in highly cluttered natural scenes. Using a deep neural network (DNN) model training for animal- background image classification, we analyze the input camera-trap images to generate a multi-level visual representation of the input image. We detect semantic regions of interest for animals from this representation using k-mean clustering and graph cut in the DNN feature domain. These animal regions are then classified into animal species using multi-class deep neural network model. According the experimental results, our method achieves 99.75\% accuracy for classifying animals and background and 90.89\% accuracy for classifying 26 animal species on the Snapshot Serengeti dataset, outperforming existing image classification methods.},
	language = {en},
	urldate = {2022-06-21},
	journal = {Ecological Informatics},
	author = {Ahmed, Ahmed and Yousif, Hayder and Kays, Roland and He, Zhihai},
	month = jul,
	year = {2019},
	keywords = {Animal species classification, Deep neural networks, Graph cut, Semantic regions},
	pages = {57--68},
	file = {ScienceDirect Snapshot:/Users/ludwigbothmann/Zotero/storage/9KI88ISN/S1574954118301481.html:text/html},
}

@inproceedings{beery_recognition_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Recognition in {Terra} {Incognita}},
	doi = {10.1007/978-3-030-01270-0_28},
	abstract = {It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.(The dataset is available at https://beerys.github.io/CaltechCameraTraps/)},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Beery, Sara and Van Horn, Grant and Perona, Pietro},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Benchmark, Context, Dataset, Domain adaptation, Recognition, Transfer learning},
	pages = {472--489},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/WL2JDE2I/Beery et al. - 2018 - Recognition in Terra Incognita.pdf:application/pdf},
}

@article{swanson_data_2015,
	title = {Data from: {Snapshot} {Serengeti}, high-frequency annotated camera trap images of 40 mammalian species in an {African} savanna},
	journal = {Dryad Digital Repository},
	author = {Swanson, AB and Kosmala, M and Lintott, CJ and Simpson, RJ and Smith, A and Packer, C},
	year = {2015},
	doi = {10.5061/dryad.5pt92},
}

@article{shorten_survey_2019,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	number = {1},
	urldate = {2022-06-21},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	year = {2019},
	keywords = {Big data, Data Augmentation, Deep Learning, GANs, Image data},
	pages = {60},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/S3XYJLKV/Shorten und Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/3ZTLCMJ2/s40537-019-0197-0.html:text/html},
}

@article{szegedy_inception-v4_2016,
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
	year = {2016},
    journal = {arXiv},
        doi = {10.48550/arXiv.1602.07261}
}

@inproceedings{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2015},
        doi = {10.48550/arXiv.1409.1556},
}

@inproceedings{chollet_xception_2017_note,
	address = {Honolulu, HI},
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	doi = {10.1109/CVPR.2017.195},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and signiﬁcantly outperforms Inception V3 on a larger image classiﬁcation dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efﬁcient use of model parameters.},
	language = {en},
	urldate = {2022-06-23},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chollet, Francois},
	year = {2017},
	pages = {1800--1807},
        note = {doi: 10.1109/CVPR.2017.195},
	file = {Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf:/Users/ludwigbothmann/Zotero/storage/R78288BA/Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf:application/pdf},
}

@inproceedings{chollet_xception_2017,
	address = {Honolulu, HI},
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	doi = {10.1109/CVPR.2017.195},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and signiﬁcantly outperforms Inception V3 on a larger image classiﬁcation dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efﬁcient use of model parameters.},
	language = {en},
	urldate = {2022-06-23},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chollet, Francois},
	year = {2017},
	pages = {1800--1807},
	file = {Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf:/Users/ludwigbothmann/Zotero/storage/R78288BA/Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf:application/pdf},
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@inproceedings{zoph_learning_2018,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	doi = {10.1109/CVPR.2018.00907},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	year = {2018},
}

@inproceedings{huang_densely_2017,
	title = {Densely {Connected} {Convolutional} {Networks}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
    doi={10.1109/CVPR.2017.243},
	year = {2017},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	keywords = {Neural networks, Training, Complexity theory, Degradation, Image recognition, Image segmentation, Visualization},
	file = {Eingereichte Version:/Users/ludwigbothmann/Zotero/storage/H24MFIJC/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/ludwigbothmann/Zotero/storage/CP4WG8KH/7780459.html:text/html},
}

@inproceedings{tan_survey_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Survey} on {Deep} {Transfer} {Learning}},
	doi = {10.1007/978-3-030-01424-7_27},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2018},
	publisher = {Springer International Publishing},
	author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
	editor = {Kůrková, Věra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
	year = {2018},
	keywords = {Transfer learning, Deep transfer learning, Survey},
	file = {Eingereichte Version:/Users/ludwigbothmann/Zotero/storage/C87C6VL3/Tan et al. - 2018 - A Survey on Deep Transfer Learning.pdf:application/pdf},
}

@inproceedings{bozinovski_influence_1976,
	title = {The influence of pattern similarity and transfer learning upon training of a base perceptron b2},
	volume = {3},
	booktitle = {Proceedings of {Symposium} {Informatica}},
	author = {Bozinovski, Stevo and Fulgosi, Ante},
	year = {1976},
	pages = {121--126},
}

@article{ottoni_hyperparameter_2022,
	title = {Hyperparameter tuning of convolutional neural networks for building construction image classification},
	doi = {10.1007/s00371-021-02350-9},
	language = {en},
	urldate = {2022-06-23},
	journal = {The Visual Computer},
	author = {Ottoni, André Luiz Carvalho and Novo, Marcela Silva and Costa, Dayana Bastos},
	year = {2022},
	keywords = {Deep learning, Hyperparameter tuning, Building construction image classification, Convolutional neural networks, Scott–Knott method},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/WR6KNUVW/Ottoni et al. - 2022 - Hyperparameter tuning of convolutional neural netw.pdf:application/pdf},
}

@book{hutter_automated_2019,
	title = {Automated {Machine} {Learning} - {Methods}, {Systems}, {Challenges}},
	publisher = {Springer},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
        doi = {10.1007/978-3-030-05318-5},
}

@techreport{settles_active_2009,
	type = {Computer {Sciences} {Technical} {Report}},
	title = {Active {Learning} {Literature} {Survey}},
	number = {1648},
	institution = {University of Wisconsin–Madison},
	author = {Settles, Burr},
	year = {2009},
}

@article{yang_active_2018,
	title = {Active {Learning} for {Visual} {Image} {Classification} {Method} {Based} on {Transfer} {Learning}},
	volume = {6},
	doi = {10.1109/ACCESS.2017.2761898},
	abstract = {The active learning method involves searching for the most informative unmarked samples by query function, submitting them to the expert function for marking, then using the samples to train the classification model in order to improve the accuracy of the model and use the newly acquired knowledge to inquire into the next round, with the aim of getting the highest accuracy of classification using minimal training samples. This paper details the various principles of active learning and develops a method that combines active learning with transfer learning. Experimental results prove that the active learning method can cut back on samples redundancy and promote the accuracy of classifier convergence quickly in small samples. Combining active learning and transfer learning, while taking advantage of knowledge in related areas, could further improve the generalization ability of classification models.},
	author = {Yang, Jihai and Li, Shijun and Xu, Wenning},
	year = {2018},
	keywords = {Uncertainty, Data mining, image classification, Learning systems, Training, Data models, Support vector machines, Active learning, field adaptation, Redundancy, transfer learning},
	pages = {187--198},
    journal = {IEEE Access},
	file = {IEEE Xplore Abstract Record:/Users/ludwigbothmann/Zotero/storage/A8KDKT5J/8100722.html:text/html;IEEE Xplore Full Text PDF:/Users/ludwigbothmann/Zotero/storage/PZXEKPC4/Yang et al. - 2018 - Active Learning for Visual Image Classification Me.pdf:application/pdf},
}

@article{shahinfar_how_2020,
	title = {“{How} many images do {I} need?” {Understanding} how sample size per class affects deep learning model performance metrics for balanced designs in autonomous wildlife monitoring},
	volume = {57},
	shorttitle = {“{How} many images do {I} need?},
	doi = {10.1016/j.ecoinf.2020.101085},
	abstract = {Deep learning (DL) algorithms are the state of the art in automated classification of wildlife camera trap images. The challenge is that the ecologist cannot know in advance how many images per species they need to collect for model training in order to achieve their desired classification accuracy. In fact there is limited empirical evidence in the context of camera trapping to demonstrate that increasing sample size will lead to improved accuracy. In this study we explore in depth the issues of deep learning model performance for progressively increasing per class (species) sample sizes. We also provide ecologists with an approximation formula to estimate how many images per animal species they need for certain accuracy level a priori. This will help ecologists for optimal allocation of resources, work and efficient study design. In order to investigate the effect of number of training images; seven training sets with 10, 20, 50, 150, 500, 1000 images per class were designed. Six deep learning architectures namely ResNet-18, ResNet-50, ResNet-152, DnsNet-121, DnsNet-161, and DnsNet-201 were trained and tested on a common exclusive testing set of 250 images per class. The whole experiment was repeated on three similar datasets from Australia, Africa and North America and the results were compared. Simple regression equations for use by practitioners to approximate model performance metrics are provided. Generalizes additive models (GAM) are shown to be effective in modelling DL performance metrics based on the number of training images per class, tuning scheme and dataset. Overall, our trained models classified images with 0.94 accuracy (ACC), 0.73 precision (PRC), 0.72 true positive rate (TPR), and 0.03 false positive rate (FPR). Variation in model performance metrics among datasets, species and deep learning architectures exist and are shown distinctively in the discussion section. The ordinary least squares regression models explained 57\%, 54\%, 52\%, and 34\% of expected variation of ACC, PRC, TPR, and FPR according to number of images available for training. Generalised additive models explained 77\%, 69\%, 70\%, and 53\% of deviance for ACC, PRC, TPR, and FPR respectively. Predictive models were developed linking number of training images per class, model, dataset to performance metrics. The ordinary least squares regression and Generalised additive models developed provides a practical toolbox to estimate model performance with respect to different numbers of training images.},
	language = {en},
	urldate = {2022-06-27},
	journal = {Ecological Informatics},
	author = {Shahinfar, Saleh and Meek, Paul and Falzon, Greg},
	month = may,
	year = {2020},
	keywords = {Deep learning, Wildlife, Camera traps, Ecological informatics, Generalised additive models, Learning curves, Predictive modelling},
	pages = {101085},
	file = {Accepted Version:/Users/ludwigbothmann/Zotero/storage/JWGXZKBU/Shahinfar et al. - 2020 - “How many images do I need” Understanding how sam.pdf:application/pdf;ScienceDirect Snapshot:/Users/ludwigbothmann/Zotero/storage/8DWRRXFF/S1574954120300352.html:text/html},
}

@article{curry_application_2021,
	title = {Application of deep learning to camera trap data for ecologists in planning / engineering -- {Can} captivity imagery train a model which generalises to the wild?},
	doi = {10.48550/arXiv.2111.12805},
	urldate = {2022-06-27},
	journal = {arXiv},
	author = {Curry, Ryan and Trotter, Cameron and McGough, Andrew Stephen},
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Submitted to Big Data 2021},
	file = {arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/LFL4ZVZ2/Curry et al. - 2021 - Application of deep learning to camera trap data f.pdf:application/pdf;arXiv.org Snapshot:/Users/ludwigbothmann/Zotero/storage/EAJ628V2/2111.html:text/html},
}

@article{shepley_automated_2021,
	title = {Automated location invariant animal detection in camera trap images using publicly available data sources},
	volume = {11},
	abstract = {A time-consuming challenge faced by camera trap practitioners is the extraction of meaningful data from images to inform ecological management. An increasingly popular solution is automated image classification software. However, most solutions are not sufficiently robust to be deployed on a large scale due to lack of location invariance when transferring models between sites. This prevents optimal use of ecological data resulting in significant expenditure of time and resources to annotate and retrain deep learning models. We present a method ecologists can use to develop optimized location invariant camera trap object detectors by (a) evaluating publicly available image datasets characterized by high intradataset variability in training deep learning models for camera trap object detection and (b) using small subsets of camera trap images to optimize models for high accuracy domain-specific applications. We collected and annotated three datasets of images of striped hyena, rhinoceros, and pigs, from the image-sharing websites FlickR and iNaturalist (FiN), to train three object detection models. We compared the performance of these models to that of three models trained on the Wildlife Conservation Society and Camera CATalogue datasets, when tested on out-of-sample Snapshot Serengeti datasets. We then increased FiN model robustness by infusing small subsets of camera trap images into training. In all experiments, the mean Average Precision (mAP) of the FiN trained models was significantly higher (82.33\%–88.59\%) than that achieved by the models trained only on camera trap datasets (38.5\%–66.74\%). Infusion further improved mAP by 1.78\%–32.08\%. Ecologists can use FiN images for training deep learning object detection solutions for camera trap image processing to develop location invariant, robust, out-of-the-box software. Models can be further optimized by infusion of 5\%–10\% camera trap images into training data. This would allow AI technologies to be deployed on a large scale in ecological applications. Datasets and code related to this study are open source and available on this repository: https://doi.org/10.5061/dryad.1c59zw3tx.},
	language = {en},
	number = {9},
	urldate = {2022-06-27},
	journal = {Ecology and Evolution},
	author = {Shepley, Andrew and Falzon, Greg and Meek, Paul and Kwan, Paul},
	year = {2021},
	keywords = {deep learning, artificial intelligence, deep convolutional neural networks, animal identification, camera trap images, camera trapping, infusion, location invariance, wildlife ecology, wildlife monitoring},
	pages = {4494--4506},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/J48JGFRD/Shepley et al. - 2021 - Automated location invariant animal detection in c.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/GYRZWCQ2/ece3.html:text/html},
}

@article{kellenberger_aide_2020,
	title = {{AIDE}: {Accelerating} image-based ecological surveys with interactive machine learning},
	volume = {11},
	shorttitle = {{AIDE}},
	doi = {10.1111/2041-210X.13489},
	abstract = {Ecological surveys increasingly rely on large-scale image datasets, typically terabytes of imagery for a single survey. The ability to collect this volume of data allows surveys of unprecedented scale, at the cost of expansive volumes of photo-interpretation labour. We present Annotation Interface for Data-driven Ecology (AIDE), an open-source web framework designed to alleviate the task of image annotation for ecological surveys. AIDE employs an easy-to-use and customisable labelling interface that supports multiple users, database storage and scalability to the cloud and/or multiple machines. Moreover, AIDE closely integrates users and machine learning models into a feedback loop, where user-provided annotations are employed to re-train the model, and the latter is applied over unlabelled images to e.g. identify wildlife. These predictions are then presented to the users in optimised order, according to a customisable active learning criterion. AIDE has a number of deep learning models built-in, but also accepts custom model implementations. Annotation Interface for Data-driven Ecology has the potential to greatly accelerate annotation tasks for a wide range of researches employing image data. AIDE is open-source and can be downloaded for free at https://github.com/microsoft/aerial\_wildlife\_detection.},
	language = {en},
	number = {12},
	urldate = {2022-06-24},
	journal = {Methods in Ecology and Evolution},
	author = {Kellenberger, Benjamin and Tuia, Devis and Morris, Dan},
	year = {2020},
	keywords = {statistics, applied ecology, conservation, monitoring (population ecology), population ecology, surveys},
	pages = {1716--1727},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/75DLNFVS/Kellenberger et al. - 2020 - AIDE Accelerating image-based ecological surveys .pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/J3BD3PUR/2041-210X.html:text/html},
}

@misc{noauthor_surveys_nodate,
	title = {Surveys using camera traps: are we looking to a brighter future? - {Rowcliffe} - 2008 - {Animal} {Conservation} - {Wiley} {Online} {Library}},
	urldate = {2022-06-29},
	file = {Surveys using camera traps\: are we looking to a brighter future? - Rowcliffe - 2008 - Animal Conservation - Wiley Online Library:/Users/ludwigbothmann/Zotero/storage/DPUVNIGD/j.1469-1795.2008.00180.html:text/html},
}

@article{trolliet_use_2014,
	title = {Use of camera traps for wildlife studies. {A} review},
	volume = {18},
	abstract = {Utilisation des pièges photographiques pour l’étude de la faune sauvage (synthèse bibliographique). Alors que les pressions anthropiques continuent de dégrader les habitats naturels, le besoin de suivre régulièrement les tendances des populations de grands vertébrés augmente. Les efforts de conservation doivent être de plus en plus ciblés mais les travaux de terrains nécessaires à la récolte de données sont souvent limités par le temps et le nombre de personnes disponibles. Les pièges photographiques apparaissent ainsi comme une méthode efficace pour assurer un échantillonnage continu et dans des zones difficilement accessibles. Nous illustrons ici la manière dont cet outil est utilisé pour une diversité de thèmes d’études de terrain tels que le comportement animal, le suivi de populations et les interactions faune-flore. En analysant les aspects techniques et matériels permettant d’assurer différents types de travaux d’écologie animale, nous mettons en évidence la nécessité de sélectionner du matériel et de mettre en place un protocole d’échantillonnage adapté à l’espèce et aux objectifs fixés de l’étude.},
	language = {en},
	number = {3},
	urldate = {2022-06-29},
	journal = {BASE},
	author = {Trolliet, Franck and Huynen, Marie-Claude and Vermeulen, Cédric and Hambuckers, Alain},
	year = {2014},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/92BBCTQL/Trolliet et al. - 2014 - Use of camera traps for wildlife studies. A review.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/N9HAJK68/index.html:text/html},
}

@article{moeller_three_2018,
	title = {Three novel methods to estimate abundance of unmarked animals using remote cameras},
	volume = {9},
	doi = {10.1002/ecs2.2331},
	abstract = {Abundance and density estimates are central to the field of ecology and are an important component of wildlife management. While many methods exist to estimate abundance from individually identifiable animals, it is much more difficult to estimate abundance of unmarked animals. One step toward noninvasive abundance estimation is the use of passive detectors such as remote cameras or acoustic recording devices. However, existing methods for estimating abundance from cameras for unmarked animals are limited by variable detection probability and have not taken full advantage of the information in camera trapping rate. We developed a time to event (TTE) model to estimate abundance from trapping rate. This estimate requires independent estimates of animal movement, so we collapsed the sampling occasions to create a space to event (STE) model that is not sensitive to movement rate. We further simplified the STE model into an instantaneous sampling (IS) estimator that applies fixed-area counts to cameras. The STE and IS models utilize time-lapse photographs to eliminate the variability in detection probability that comes with motion-sensor photographs. We evaluated the three methods with simulations and performed a case study to estimate elk (Cervus canadensis) abundance from remote camera trap data in Idaho. Simulations demonstrated that the TTE model is sensitive to movement rate, but the STE and IS methods are unbiased regardless of movement. In our case study, elk abundance estimates were comparable to those from a recent aerial survey in the area, demonstrating that these new methods allow biologists to estimate abundance from unmarked populations without tracking individuals over time.},
	language = {en},
	number = {8},
	urldate = {2022-06-29},
	journal = {Ecosphere},
	author = {Moeller, Anna K. and Lukacs, Paul M. and Horne, Jon S.},
	year = {2018},
	keywords = {abundance, Cervus canadensis, density, elk, exponential distribution, Poisson point process, remote camera, space to event, time to event, time-lapse photography, unmarked population},
	pages = {e02331},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/E8EC8A3B/Moeller et al. - 2018 - Three novel methods to estimate abundance of unmar.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/93ZBUIL8/ecs2.html:text/html},
}

@article{royle_n-mixture_2004,
	title = {N-{Mixture} {Models} for {Estimating} {Population} {Size} from {Spatially} {Replicated} {Counts}},
	volume = {60},
	doi = {10.1111/j.0006-341X.2004.00142.x},
	abstract = {Summary. Spatial replication is a common theme in count surveys of animals. Such surveys often generate sparse count data from which it is difficult to estimate population size while formally accounting for detection probability. In this article, I describe a class of models (N-mixture models) which allow for estimation of population size from such data. The key idea is to view site-specific population sizes, N, as independent random variables distributed according to some mixing distribution (e.g., Poisson). Prior parameters are estimated from the marginal likelihood of the data, having integrated over the prior distribution for N. Carroll and Lombard (1985, Journal of American Statistical Association80, 423–426) proposed a class of estimators based on mixing over a prior distribution for detection probability. Their estimator can be applied in limited settings, but is sensitive to prior parameter values that are fixed a priori. Spatial replication provides additional information regarding the parameters of the prior distribution on N that is exploited by the N-mixture models and which leads to reasonable estimates of abundance from sparse data. A simulation study demonstrates superior operating characteristics (bias, confidence interval coverage) of the N-mixture estimator compared to the Caroll and Lombard estimator. Both estimators are applied to point count data on six species of birds illustrating the sensitivity to choice of prior on p and substantially different estimates of abundance as a consequence.},
	language = {en},
	number = {1},
	urldate = {2022-06-29},
	journal = {Biometrics},
	author = {Royle, J. Andrew},
	year = {2004},
	keywords = {Avian point counts, Binomial population size estimation, North American Breeding Bird Survey},
	pages = {108--115},
	file = {Snapshot:/Users/ludwigbothmann/Zotero/storage/FK9NCHDP/j.0006-341X.2004.00142.html:text/html},
}

@article{rowcliffe_estimating_2008,
	title = {Estimating animal density using camera traps without the need for individual recognition},
	volume = {45},
	doi = {10.1111/j.1365-2664.2008.01473.x},
	abstract = {1 Density estimation is of fundamental importance in wildlife management. The use of camera traps to estimate animal density has so far been restricted to capture–recapture analysis of species with individually identifiable markings. This study developed a method that eliminates the requirement for individual recognition of animals by modelling the underlying process of contact between animals and cameras. 2 The model provides a factor that linearly scales trapping rate with density, depending on two key biological variables (average animal group size and day range) and two characteristics of the camera sensor (distance and angle within which it detects animals). 3 We tested the approach in an enclosed animal park with known abundances of four species, obtaining accurate estimates in three out of four cases. Inaccuracy in the fourth species was because of biased placement of cameras with respect to the distribution of this species. 4 Synthesis and applications. Subject to unbiased camera placement and accurate measurement of model parameters, this method opens the possibility of reduced labour costs for estimating wildlife density and may make estimation possible where it has not been previously. We provide guidelines on the trapping effort required to obtain reasonably precise estimates.},
	language = {en},
	number = {4},
	urldate = {2022-06-29},
	journal = {Journal of Applied Ecology},
	author = {Rowcliffe, J. Marcus and Field, Juliet and Turvey, Samuel T. and Carbone, Chris},
	year = {2008},
	keywords = {camera traps, density estimation, abundance estimation, animal movement, day range, detection probability, trapping rate},
	pages = {1228--1236},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/M6P9B9NN/Rowcliffe et al. - 2008 - Estimating animal density using camera traps witho.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/5L5NB8RG/j.1365-2664.2008.01473.html:text/html},
}

@article{rowcliffe_surveys_2008,
	title = {Surveys using camera traps: are we looking to a brighter future?},
	volume = {11},
	shorttitle = {Surveys using camera traps},
	doi = {10.1111/j.1469-1795.2008.00180.x},
	language = {en},
	number = {3},
	urldate = {2022-06-29},
	journal = {Animal Conservation},
	author = {Rowcliffe, J. M. and Carbone, C.},
	year = {2008},
	pages = {185--186},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/8TVTQPX7/Rowcliffe und Carbone - 2008 - Surveys using camera traps are we looking to a br.pdf:application/pdf},
}

@article{rigoudy_deepfaune_2022,
	title = {The {DeepFaune} initiative: a collaborative effort towards the automatic identification of the {French} fauna in camera-trap images},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {The {DeepFaune} initiative},
	doi = {10.1101/2022.03.15.484324},
	language = {en},
	urldate = {2022-06-29},
	journal = {bioRxiv},
	author = {Rigoudy, Noa and Benyoub, Abdelbaki and Besnard, Aurélien and Birck, Carole and Bollet, Yoann and Bunz, Yoann and Backer, Nina De and Caussimont, Gérard and Delestrade, Anne and Dispan, Lucie and Elder, Jean-François and Fanjul, Jean-Baptiste and Fonderflick, Jocelyn and Garel, Mathieu and Gaudry, William and Gérard, Agathe and Gimenez, Olivier and Hemery, Arzhela and Hemon, Audrey and Jullien, Jean-Michel and  others},
	year = {2022},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2022-07-05},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/ZQPB6AVN/LeCun et al. - 2015 - Deep learning.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/64HTWIBR/nature14539.html:text/html},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	annote = {http://www.deeplearningbook.org},
}

@article{krizhevsky_imagenet_2012,
	title = {Imagenet classification with deep convolutional neural networks},
	volume = {25},
	journal = {Advances in neural information processing systems},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
}

@article{bodesheim_pre-trained_2022,
	title = {Pre-trained models are not enough: active and lifelong learning is important for long-term visual monitoring of mammals in biodiversity research—{Individual} identification and attribute prediction with image features from deep neural networks and decoupled decision models applied to elephants and great apes},
	volume = {102},
	shorttitle = {Pre-trained models are not enough},
	doi = {10.1007/s42991-022-00224-8},
	abstract = {Animal re-identification based on image data, either recorded manually by photographers or automatically with camera traps, is an important task for ecological studies about biodiversity and conservation that can be highly automatized with algorithms from computer vision and machine learning. However, fixed identification models only trained with standard datasets before their application will quickly reach their limits, especially for long-term monitoring with changing environmental conditions, varying visual appearances of individuals over time that differ a lot from those in the training data, and new occurring individuals that have not been observed before. Hence, we believe that active learning with human-in-the-loop and continuous lifelong learning is important to tackle these challenges and to obtain high-performance recognition systems when dealing with huge amounts of additional data that become available during the application. Our general approach with image features from deep neural networks and decoupled decision models can be applied to many different mammalian species and is perfectly suited for continuous improvements of the recognition systems via lifelong learning. In our identification experiments, we consider four different taxa, namely two elephant species: African forest elephants and Asian elephants, as well as two species of great apes: gorillas and chimpanzees. Going beyond classical re-identification, our decoupled approach can also be used for predicting attributes of individuals such as gender or age using classification or regression methods. Although applicable for small datasets of individuals as well, we argue that even better recognition performance will be achieved by improving decision models gradually via lifelong learning to exploit huge datasets and continuous recordings from long-term applications. We highlight that algorithms for deploying lifelong learning in real observational studies exist and are ready for use. Hence, lifelong learning might become a valuable concept that supports practitioners when analyzing large-scale image data during long-term monitoring of mammals.},
	language = {en},
	number = {3},
	urldate = {2022-11-30},
	journal = {Mammalian Biology},
	author = {Bodesheim, Paul and Blunk, Jan and Körschens, Matthias and Brust, Clemens-Alexander and Käding, Christoph and Denzler, Joachim},
	month = jun,
	year = {2022},
	keywords = {Deep learning, Neural networks, Active learning, Animal re-identification, Attribute prediction, Continuous learning, Human-in-the-loop, Lifelong learning},
	pages = {853--875},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/M7KPMTU3/Bodesheim et al. - 2022 - Pre-trained models are not enough active and life.pdf:application/pdf},
}

@inproceedings{deng_imagenet_2009,
    doi={10.1109/CVPR.2009.5206848},
	title = {Imagenet: {A} large-scale hierarchical image database},
	booktitle = {2009 {IEEE} conference on computer vision and pattern recognition},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = {2009},
	pages = {248--255},
}

@article{redlich_disentangling_2022,
	title = {Disentangling effects of climate and land use on biodiversity and ecosystem services—{A} multi-scale experimental design},
	volume = {13},
	doi = {10.1111/2041-210X.13759},
	abstract = {Climate and land-use change are key drivers of environmental degradation in the Anthropocene, but too little is known about their interactive effects on biodiversity and ecosystem services. Long-term data on biodiversity trends are currently lacking. Furthermore, previous ecological studies have rarely considered climate and land use in a joint design, did not achieve variable independence or lost statistical power by not covering the full range of environmental gradients. Here, we introduce a multi-scale space-for-time study design to disentangle effects of climate and land use on biodiversity and ecosystem services. The site selection approach coupled extensive GIS-based exploration (i.e. using a Geographic information system) and correlation heatmaps with a crossed and nested design covering regional, landscape and local scales. Its implementation in Bavaria (Germany) resulted in a set of study plots that maximise the potential range and independence of environmental variables at different spatial scales. Stratifying the state of Bavaria into five climate zones (reference period 1981–2010) and three prevailing land-use types, that is, near-natural, agriculture and urban, resulted in 60 study regions (5.8 × 5.8 km quadrants) covering a mean annual temperature gradient of 5.6–9.8°C and a spatial extent of 310 × 310 km. Within these regions, we nested 180 study plots located in contrasting local land-use types, that is, forests, grasslands, arable land or settlement (local climate gradient 4.5–10°C). This approach achieved low correlations between climate and land use (proportional cover) at the regional and landscape scale with {\textbar}r ≤ 0.33{\textbar} and {\textbar}r ≤ 0.29{\textbar} respectively. Furthermore, using correlation heatmaps for local plot selection reduced potentially confounding relationships between landscape composition and configuration for plots located in forests, arable land and settlements. The suggested design expands upon previous research in covering a significant range of environmental gradients and including a diversity of dominant land-use types at different scales within different climatic contexts. It allows independent assessment of the relative contribution of multi-scale climate and land use on biodiversity and ecosystem services. Understanding potential interdependencies among global change drivers is essential to develop effective restoration and mitigation strategies against biodiversity decline, especially in expectation of future climatic changes. Importantly, this study also provides a baseline for long-term ecological monitoring programs.},
	language = {en},
	number = {2},
	urldate = {2022-12-20},
	journal = {Methods in Ecology and Evolution},
	author = {Redlich, Sarah and Zhang, Jie and Benjamin, Caryl and Dhillon, Maninder Singh and Englmeier, Jana and Ewald, Jörg and Fricke, Ute and Ganuza, Cristina and Haensel, Maria and Hovestadt, Thomas and Kollmann, Johannes and Koellner, Thomas and Kübert-Flock, Carina and Kunstmann, Harald and Menzel, Annette and Moning, Christoph and Peters, Wibke and Riebl, Rebekka and Rummler, Thomas and Rojas-Botero, Sandra and others},
	year = {2022},
	keywords = {biodiversity, climate change, ecosystem functioning, insect monitoring, land use, space-for-time approach, spatial scales, study design},
	pages = {514--527},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/2IW8N926/Redlich et al. - 2022 - Disentangling effects of climate and land use on b.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/HGSVZ7TK/2041-210X.html:text/html},
}

@article{oquab2023dinov2,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={arXiv preprint arXiv:2304.07193},
  year={2023}
}

@article{bothmann_automated_2023,
	title = {Automated wildlife image classification: {An} active learning tool for ecological applications},
	shorttitle = {Automated wildlife image classification},
	url = {http://arxiv.org/abs/2303.15823},
	doi = {10.48550/arXiv.2303.15823},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Bothmann, Ludwig and Wimmer, Lisa and Charrakh, Omid and Weber, Tobias and Edelhoff, Hendrik and Peters, Wibke and Nguyen, Hien and Benjamin, Caryl and Menzel, Annette},
	year = {2023},
        journal = {arXiv},
	note = {doi: 10.48550/arXiv.2303.15823},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Applications},
	file = {arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/WTEM5SAA/Bothmann et al. - 2023 - Automated wildlife image classification An active.pdf:application/pdf;arXiv.org Snapshot:/Users/ludwigbothmann/Zotero/storage/24F27MR2/2303.html:text/html},
}

@misc{abadi_tensorflow_2015,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Systems}},
	url = {https://www.tensorflow.org/},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2015},
}

@article{bolger_computer-assisted_2012,
	title = {A computer-assisted system for photographic mark–recapture analysis},
	volume = {3},
	doi = {10.1111/j.2041-210X.2012.00212.x},
	number = {5},
	urldate = {2023-07-05},
	journal = {Methods in Ecology and Evolution},
	author = {Bolger, Douglas T. and Morrison, Thomas A. and Vance, Bennet and Lee, Derek and Farid, Hany},
	year = {2012},
	pages = {813--822},
}

@article{yu_automated_2013,
	title = {Automated identification of animal species in camera trap images},
	volume = {2013},
	issn = {1687-5281},
	doi = {10.1186/1687-5281-2013-52},
	number = {1},
	urldate = {2023-07-05},
	journal = {EURASIP Journal on Image and Video Processing},
	author = {Yu, Xiaoyuan and Wang, Jiangping and Kays, Roland and Jansen, Patrick A. and Wang, Tianjiang and Huang, Thomas},
	year = {2013},
	pages = {52},
}


@inproceedings{chen_deep_2014,
	title = {Deep convolutional neural network based species recognition for wild animal monitoring},
	doi = {10.1109/ICIP.2014.7025172},
	booktitle = {2014 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Chen, Guobin and Han, Tony X. and He, Zhihai and Kays, Roland and Forrester, Tavis},
	year = {2014},
	pages = {858--862},
}

@article{gomez_villa_towards_2017,
	title = {Towards automatic wild animal monitoring: {Identification} of animal species in camera-trap images using very deep convolutional neural networks},
	volume = {41},
	shorttitle = {Towards automatic wild animal monitoring},
	doi = {10.1016/j.ecoinf.2017.07.004},
	language = {en},
	urldate = {2023-07-05},
	journal = {Ecological Informatics},
	author = {Gomez Villa, Alexander and Salazar, Augusto and Vargas, Francisco},
	year = {2017},
	pages = {24--32},
}

@article{kellenberger_half_2019,
	title = {Half a {Percent} of {Labels} is {Enough}: {Efficient} {Animal} {Detection} in {UAV} {Imagery} {Using} {Deep} {CNNs} and {Active} {Learning}},
	volume = {57},
	shorttitle = {Half a {Percent} of {Labels} is {Enough}},
	doi = {10.1109/TGRS.2019.2927393},
	language = {English},
	number = {12},
	urldate = {2023-07-05},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Kellenberger, Benjamin and Marcos, Diego and Lobry, Sylvain and Tuia, Devis},
	month = dec,
	year = {2019},
	pages = {9524--9533},
}

The Nature Conservancy (2021): Channel Islands Camera Traps 1.0. The Nature Conservancy. Dataset.
@misc{channel_island_2021,
	title = {Channel Islands Camera Traps 1.0},
	author = {{The Nature Conservancy}},
	year = {2021},
	notes = {Dataset},
        url = {https://lila.science/datasets/channel-islands-camera-traps/}
}
