\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission - SINGLE BLIND!!!
%\usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{bm}             % Bold greek letters
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}        % for "align"
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[pdftex]{graphicx}
%\usepackage{ngerman} % für deutsche Anführungszeichen
\usepackage[inline]{enumitem} % inline enumeration
%\usepackage{dsfont}         % Real numbers symbol etc. => wieder entfernt, nehme stattdessen \mathbb{R} aus dem amsfonts package wie von neurips vorgeschlagen
\usepackage{caption}        % For subfigures and captions
\usepackage{subcaption}     % For subfigures and captions

\usepackage{fancyhdr}
\pagestyle{fancy}
\addtolength{\headwidth}{-1cm}
\lhead{Bothmann et al. (2023)}
\rhead{Wildlife image classification with active learning}

\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{version}
\excludeversion{kurz}
\includeversion{tune}
\excludeversion{fig_publish}
\includeversion{fig_count_words}

\setcitestyle{authoryear}
%\setcitestyle{numbers,square}
\bibliographystyle{abbrvnat} %rusnat %abbrvnat


\title{Automated wildlife image classification: An active learning tool for ecological applications}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Ludwig Bothmann$^{1,2,*}$, Lisa Wimmer$^{1,2}$, Omid Charrakh$^3$, Tobias Weber$^{1,2}$, 
  \And Hendrik Edelhoff$^4$, Wibke Peters$^{4,5}$, Hien Nguyen$^4$, 
  \And Caryl Benjamin$^{6}$, Annette Menzel$^{6,7}$
}


\begin{document}

\include{basic_math}
\include{basic_ml}
\include{macros}
\include{ml-svm}

\maketitle
%\tableofcontents
$^1$Department of Statistics, LMU Munich, Germany\\
$^2$Munich Center for Machine Learning (MCML), Germany\\
$^3$Munich Center for Mathematical Philosophy (MCMP), LMU Munich, Germany\\
$^4$Wildlife Biology and Management Research Unit, Bavarian State Institute of Forestry (LWF), Freising, Germany\\
$^5$Wildlife Biology and Management Unit, Technical University of Munich, Freising, Germany\\
$^6$Ecoclimatology, TUM School of Life Sciences, Technical University of Munich, Freising, Germany\\
$^7$TUM Institute for Advanced Study, Garching, Germany\\
$^*$Corresponding author: \texttt{ludwig.bothmann@stat.uni-muenchen.de}\\[2ex]

\begin{abstract}





\begin{enumerate}
    \item Wildlife camera trap images are being used extensively to investigate animal abundance, habitat associations, and behavior, which is complicated by the fact that experts must first classify the images to retrieve relevant information. 
    Artificial intelligence systems can take over this task but usually need a large number of already-labeled training images to achieve sufficient performance.
    This requirement necessitates human expert labor and poses a particular challenge for projects with few cameras or short durations.
    We propose a label-efficient learning strategy that enables researchers with small or medium-sized image databases to leverage the potential of modern machine learning, thus freeing crucial resources for subsequent analyses.
    \item Our methodological proposal is two-fold: On the one hand, we improve current strategies of combining object detection and image classification by tuning the hyperparameters of both models. On the other hand, we provide an active learning system that allows training deep learning models very efficiently in terms of required human-labeled training images. We supply a software package that enables researchers to use these methods without specific programming skills and thereby ensure the broad applicability of the proposed framework in ecological practice.
    \item We show that our tuning strategy improves predictive performance, emphasizing that tuning can and must be done separately for a new data set. We demonstrate how the active learning pipeline reduces the amount of pre-labeled data needed to achieve a specific predictive performance and that it is especially valuable for improving out-of-sample predictive performance.
    \item We conclude that the combination of tuning and active learning increases the predictive performance of automated image classifiers substantially. Furthermore, we argue that our work can broadly impact the community through the ready-to-use software package provided. Finally, the publication of our models tailored to European wildlife data enriches existing model bases mostly trained on data from Africa and North America.  
\end{enumerate}

\end{abstract}

Keywords: active learning, camera traps, deep learning, European animal species, hyperparameter tuning, object detection, out-of-sample prediction, wildlife image classification

\section{Introduction}
\label{sec:intro}

Wildlife camera traps have been increasingly used for estimating the abundance and distribution of animal communities, studying animal behavior, and assessing animal-plant interactions over the last decade 
\citep{delisle_next-generation_2021, trolliet_use_2014, tuia_perspectives_2022}. 
Since human annotation of large image data sets is time-consuming and expensive, machine learning (ML) techniques, especially from the field of deep learning (DL) %\cite[DL,][]{goodfellow_deep_2016}, 
have been developed, customized, and applied to wildlife image data sets %\citep{ahmed_semantic_2019, auer_minimizing_2021, beery_efficient_2019, christin_applications_2019, choinski_first_2021, cunha_filtering_2021, elhamod_hierarchy-guided_2022, gimenez_trade-off_2021, miao_insights_2019, miao_iterative_2021, norouzzadeh_automatically_2018, norouzzadeh_deep_2021, schneider_past_2019, schneider_three_2020,tabak_machine_2019, tabak_improving_2020, tabak_cameratrapdetector_2022, waldchen_machine_2018, whytock_robust_2021, willi_identifying_2019}. 
\citep{auer_minimizing_2021, beery_efficient_2019, christin_applications_2019, gimenez_trade-off_2021,  miao_iterative_2021, norouzzadeh_automatically_2018, norouzzadeh_deep_2021, schneider_past_2019, schneider_three_2020,tabak_machine_2019, tabak_improving_2020, tabak_cameratrapdetector_2022, whytock_robust_2021}. 
These techniques allow for high detection rates in the investigated data sets. 

However, the broad applicability of the proposed methods is hindered for different reasons: First, models are often trained on data sets from specific regions of the earth, e.g., the Serengeti Snapshot data from Africa \citep{swanson_data_2015}, the North American Camera Trap Images  \citep{tabak_machine_2019} and Caltech Camera Traps \citep{beery_recognition_2018} from North America, the LILA database\footnote{https://lila.science/datasets} with images from outside Europe, and DeepFaune \citep{rigoudy_deepfaune_2022} from France. Applying such models to other animal species or habitats, which are not representative of the data used for model training, leads to a drop in  
predictive performance, 
see also \cite{beery_recognition_2018, koh_wilds_2021, schneider_three_2020, shepley_automated_2021, tuia_perspectives_2022}. Second, training such models for new animal species or new habitats demands a large amount of manually annotated training images, which may not be available for smaller research groups that operate in regions where models trained on the above-mentioned data sets result in poor classification performance.
Third, highly customizable software targeted at experienced programmers may be an obstacle for applied researchers from ecology who would benefit much more from an easy-to-use software package that yields high-performing ML models across a range of tasks, see, e.g., \cite{velez_choosing_2022, gimenez_trade-off_2021}. 



We aim at filling this gap by proposing an active learning pipeline that yields high-performing models, demands only small training data sets, and is readily usable -- via accompanying example code and tutorials -- for researchers in ecology without any specific background in ML or computer science. Our main research goals are:

\begin{itemize}
    \item achieving high out-of-sample (OOS) performance, i.e., good transferability to new domains,
    \item dealing with both existing and new, previously unseen animal species,
    \item providing solutions for small data sets ($< 50.000$ images),
    \item adding models pre-trained on European wildlife images to the existing pre-trained models that are mainly from Africa and North America, and 
    \item supplying highly usable software that can be used off-the-shelf at a low computational cost.
\end{itemize}

\subsection{Main contributions}

\subsubsection{Active learning pipeline}


Our pipeline consists of four main building blocks: We combine the benefits of \myemph{object detection} -- to locate parts of an image that most likely contain animals -- and \myemph{image classification} -- to classify the animal species found by the object detector. A thorough \myemph{tuning process} optimizes the hyperparameters of both the object detector and image classifier, including the architecture of the deep neural network used for transfer learning. Finally, an \myemph{active learning} component allows training a new model or adapting a pre-trained model to new animal classes or habitats with a substantially reduced need for training data from that new domain. We will explain the central components of the pipeline in the following and refer to ``Materials and Methods'' for a thorough explanation of all steps.


\paragraph{Combining object detection and image classification}

A key factor for creating a powerful pipeline is the combination of object detection and image classification, see also \cite{curry_application_2021}. 
First, using a pre-trained object detection network, as proposed, e.g., by \cite{beery_efficient_2019}, allows for rapid identification of empty images \citep{velez_choosing_2022}.  
Otherwise, the task of locating empty images is either very time-consuming --  \cite{gimenez_trade-off_2021} mention that doing this manually ``took several weeks of labor full time'' for a rather small data set of $<50.000$ images --
or technically complicated without yielding convincing results (e.g., \cite{auer_minimizing_2021} try to use active learning to filter out empty images).
Second, image backgrounds and camera-added information (such as a header or footer) are removed and the image classifier can directly focus on the parts of the image where the object detector has detected an animal.
As a result, fewer training images are necessary to adapt pre-trained models to new domains; furthermore, the number of animals on a given image can be estimated, which, in turn, is desirable from an ecological perspective for wildlife abundance estimation methods \citep{rowcliffe_estimating_2008, royle_n-mixture_2004, moeller_three_2018}.

We enrich the pipeline with the following features:

\begin{itemize}
    \item \textbf{Tuned confidence threshold for object detection:} When using an object detection method such as the \textit{Megadetector} (MD) \citep{beery_efficient_2019}, each predicted bounding box for class ``animal'' is accompanied by a confidence score.
While others use a fixed confidence threshold above which an image is deemed non-empty (e.g., $0.9$ as in \cite{norouzzadeh_deep_2021}), we consider this optimal threshold a performance-relevant hyperparameter and propose to tune it. Thereby, we optimize the trade-off between (i) ending up with many empty images left after object detection (i.e.,  threshold for class ``animal'' is too low) and (ii) overlooking animal images (i.e., threshold for class ``animal'' is too high).


\item \textbf{Augmentation:} Data augmentation, i.e., enlarging the training set by random modifications of the existing samples, is known to improve results of image classifiers \citep{shorten_survey_2019}. 
We include this step such as \cite{schneider_three_2020, tabak_machine_2019, whytock_robust_2021} -- in contrast to other proposed pipelines such as \cite{norouzzadeh_automatically_2018, norouzzadeh_deep_2021, tabak_cameratrapdetector_2022, tabak_improving_2020}.

\item \textbf{Transfer learning:} Transfer learning \citep{tan_survey_2018} %bozinovski_influence_1976
refers to a technique where large pre-trained models are fine-tuned on the data at hand, hence leveraging universal knowledge these models have gained from learning on millions of images before. 
We support transfer learning with different models  
pre-trained on the ImageNet database \citep{deng_imagenet_2009} that consists of more than 14 million images from over 20.000 classes and consider Xception \citep{chollet_xception_2017}, InceptionResNetV2 \citep{szegedy_inception-v4_2016}, and DenseNet121 
\citep{huang_densely_2017} 
in this work; other such pre-trained networks can be easily included using our software.

\item \textbf{Hyperparameter tuning:} Hyperparameter tuning is a common technique to optimize the hyperparameters of an ML model \citep{bischl_hyperparameter_2023}. 
However, the adoption of hyperparameter tuning in the field of DL is still in its infancy \citep{ottoni_hyperparameter_2022, hutter_automated_2019} due to the higher computational burden compared to smaller tabular data sets. 
We argue that this cost is worth bearing in the light of the expected performance gains. 
Besides the threshold of the bounding-box confidence, we optimize the choice of the best pre-trained model for the data at hand, which further improves the results.


\end{itemize}

\paragraph{Active learning}

Adapting an existing model to a new task -- be it a new habitat, new animal species, or new classes (as in moving from ImageNet objects to animals during transfer learning) -- involves manually labeling data and re-training the model with this labeled data. A practical problem is that we do not know beforehand how many and which images should be labeled manually in order to achieve a certain predictive performance. Active learning (AL) \citep{settles_active_2009, yang_active_2018} tackles both challenges at the same time: The model is trained with a human-in-the-loop who sequentially labels batches of data and terminates the process when the model has reached sufficient predictive performance.
In each iteration, the AL system requests labels for the images it deems most informative for its learning process.
This careful iterative data selection reduces the amount of required human labels considerably, as our results in Section~\ref{sec:oosample} confirm.


\subsubsection{Software package}

The presented pipeline can be used from different perspectives, depending on the use case at hand and the desired level of customizability. All of them allow training models for new domains and new animal species.

\begin{itemize}
    \item \textbf{AL for everybody:}
For users who want to train a model on their wildlife image data in order to automatically label large parts of their image database, we offer a command line interface (CLI). With this CLI, users can iterate the AL loop until a satisfying predictive performance is reached, and eventually predict the class labels of all remaining unlabeled images. No programming skills are required. We provide a step-by-step example of how to use the CLI.\footnote{https://github.com/slds-lmu/wildlife-experiments, also includes code for reproducing our results}


\item \textbf{Full customizability:}
Users who like to have full flexibility and be able to customize the code base for their more specific needs may access the Python code base directly.\footnote{https://github.com/slds-lmu/wildlife-ml}

\end{itemize}



\subsubsection{Pre-trained model for European wildlife images}

We trained the pipeline on a set of European wildlife images classified into the six most abundant species (European hare (Lepus europaeus), red deer (Cervus elaphus), red fox (Vulpes vulpes), red squirrel (Sciurus vulgaris), roe deer (Capreolus capreolus) and wild boar (Sus scrofa)) detected in camera traps in the state of Bavaria in southeastern Germany. The training data additionally includes a class ``empty'' as well as a class ``others'' to account for other animals.  
The trained models are publicly available$^2$ and interested researchers can use those either as-is or as a starting point for fine-tuning them with the AL pipeline implemented in our software package. 
\begin{kurz}
In the results section, we show that ``warm-starting'' with our models improves the OOS predictive performance and decreases the number of labeled images from the new domain necessary for achieving a certain predictive performance.
\end{kurz}

\subsection{Related work}


Using DL methods for automatic classification of wildlife camera trap images has widely spread in the last years, the work of \cite{norouzzadeh_automatically_2018} being an early key contribution. Subsequently, several research groups incorporated an object detection component, e.g., \cite{norouzzadeh_deep_2021, shepley_automated_2021, tabak_cameratrapdetector_2022}, proposed active learning components, e.g.,  \cite{auer_minimizing_2021, miao_iterative_2021, norouzzadeh_deep_2021}, 
or discussed OOS performance, e.g., \cite{auer_minimizing_2021, curry_application_2021, gimenez_trade-off_2021, miao_iterative_2021, schneider_three_2020, shepley_automated_2021, whytock_robust_2021, tabak_improving_2020}. A variety of software frameworks have been proposed in the context of automated analysis of camera trap images; a recent discussion can be found in \cite{velez_choosing_2022}.

Perhaps the closest contribution to ours is \cite{norouzzadeh_deep_2021}, who also make use of object detection, transfer learning, and active learning. However, we extend their pipeline and analysis by the following points: We show empirically that an object detection component improves predictive performance -- compared to the simpler alternative of only using image classification; we consider hyperparameter tuning and show the related improvements in predictive performance; we support multiple transfer learning alternatives and decide on the best architecture in a data-driven manner during the tuning process; we systematically analyze in-sample and out-of-sample errors for small data sets and quantify the benefit of using active learning; we operate in a different and so far underrepresented domain -- European wildlife images -- and publish the pre-trained model.
Finally, we provide ecologists with a software package and example code for Python, which enables users to actually apply the proposed methods without computational barriers.

Some work has been conducted on European wildlife images, e.g., \cite{rigoudy_deepfaune_2022, gimenez_trade-off_2021, auer_minimizing_2021} 
use image data from France and Germany, respectively. To the best of our knowledge, there has not been a contribution so far that resulted in a pre-trained model for European wildlife images and an active learning pipeline comparable to ours.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and methods}

\subsection{Data sets}

We used camera trap images from 37 wildlife camera traps installed in forests across the state of Bavaria in southeastern Germany (see Fig.~\ref{fig:study_sites}). 
The data set was collected along a gradient of human and climatic variation as part of a comprehensive study focusing on biodiversity along these gradients \citep{redlich_disentangling_2022}. 
The images used in this study are from the time period September 2019 to February 2020 (autumn-winter). All images containing humans, vehicles, and domesticated animals were removed in compliance with privacy laws. The total number of 48,116 images was divided into in-sample and out-of-sample data by camera stations, i.e., 18 camera stations along with their 24,368 images were declared as in-sample and the remaining 19 camera stations along with their 23,748 images were declared as out-of-sample. Table \ref{tab:train_data} summarizes the number of images per class for both data sets. Fig.~\ref{fig:day_night} shows ratios of day/night images. Note that for some classes, the ratios for in-sample and out-of-sample images are quite different, which reflects typical real-world situations. A desideratum of a good pipeline is to perform well also on these classes.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth, trim=0 120 0 0, clip]{fig/ML_studysitesv2.pdf}%11.4
    \caption{Study sites of the camera trap images used in this work. Two stations (black square) have almost the same location, the respective camera had to be moved during the study.}
    \label{fig:study_sites}
\end{figure}



\begin{table}%[tbhp]
\centering
\caption{Number of images per class.}
\begin{tabular}{lrrr}
\label{tab:train_data}
Species & in-sample & out-of-sample & total \\
\midrule
european hare & 485 & 255 & 740\\
red deer     &26 &   135 & 161\\
red fox      &708 &  106 & 814\\
red squirrel & 297 &  13 & 310\\
roe deer     & 6,074& 7,010 &13,084\\
wild boar  &210 & 904 &  1,114\\
others     &941 & 1,051&   1,992\\
empty       &15,627 & 14,274 &  29,901\\
\midrule
total & 24,368 & 23,748 & 48,116\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{fig/Fig2_day_night.pdf}%11.4
    \caption{Distribution of day and night images per class for in-sample and out-of-sample images.}
    \label{fig:day_night}
\end{figure}

\subsection{Pipeline for tuning and model training}
\label{sec:methods-train-tune}


We consider a data set $\D = \Dset$ of labeled images, where $\yi \in \gset$ is the label of image $\xi$ and $g$ is the number of classes. 
The goal is to train a classification pipeline $\pihf$ that maps images $\xv$ to probability scores $\pixh$ of predicted hard labels $\yh = \argmax_{k \in \gset} \pikxh[k]$. Our pipeline consists of two major parts: In the first part, bounding boxes of animals in the images are detected via object detection. In the second part, bounding-box images are classified with respect to animal species. 


\subsubsection{Classification pipeline with object detection and image classification} \label{sec:ppl}

\paragraph{Object detection -- from image level to bounding-box level} Often, a considerable fraction of each image does not show an animal (see Fig.~\ref{fig:misclass} for some examples). If the entire image is passed to an image classifier, the classifier needs to cope with the challenge of detecting the important pixels and focusing on those. 
Predictive performance can be increased by providing bounding boxes of animals. Different such object detection pipelines have been proposed in the literature; we use the \textit{Megadetector} (MD) pipeline \citep{beery_efficient_2019} for its high usability and good performance in prior studies \citep[see][for a comparison]{velez_choosing_2022}. 
MD produces (among other results which we do not use in the following) for every image $\xi$ a (possibly empty) set of $m$ bounding-box coordinates %$\bm{b}^{(i)} = (\bm{b}^{(i)}_1, \dots, \bm{b}^{(i)}_m)$ 
with corresponding confidence values $\bm{c}^{(i)} =(c^{(i)}_1, \dots, c^{(i)}_m) \in [0,1]^m$ 
%\sum_{j=1}^m c^{(i)}_j = 1 \ \forall i \in \nset$ 
of capturing an animal. 
All bounding boxes with confidence $c^{(i)}_j \geq \alpha$ (\textit{high-conf BB}) are passed on to the next step -- the image classifier on bounding-box level. All images that have no bounding boxes or only bounding boxes with a low confidence $c^{(i)}_j < \alpha$ (\textit{other BB}) are considered empty. 
The confidence threshold $\alpha$ is either set to a fixed value beforehand 
or can be chosen in a data-driven manner and specifically tailored for the data set at hand. 
We opt for the latter approach and consider the threshold $\alpha$ to be a hyperparameter optimized during tuning. 



\paragraph{Image classification at the level of bounding boxes} We consider the following preprocessing on the level of bounding-box images (bb-images). 

\textit{(1) Cropping and resizing:} For all bounding boxes whose confidence threshold is at least $\alpha$, we crop the respective part from the original image and resize it, such that we obtain $s$ square bb-images $\bm{z}^{(i)}, i \in \{1, \dots, s\}$ of $224 \times 224$ pixels, which is the optimal input size for networks pre-trained on ImageNet \citep{deng_imagenet_2009}. 
We assign each bb-image the label of the original image that it is part of; this only makes sense if images are pure in the sense that if an image shows animals, it only shows animals of a unique species. In our data, we did not encounter any image with mixed animal species, 
but this assumption needs to be asserted critically for other data sets -- demanding to label on bounding-box level if the purity assumption does not hold. 



\textit{(2) Augmentation:} We augment the bb-images of the training data by 
optional rotation, flipping, and changing the contrast, and use up to three randomly sampled augmentations per image for the analyses in this paper \footnote{
Technically speaking, augmentation is realized on-the-fly during training to avoid increased memory consumption.
}. 

\textit{(3) Transfer learning:} As a last preprocessing step, we carry out a forward pass through a pre-trained image classification network, choosing between the available architectures, where we exclude the final fully-connected layers.
The resulting data representation, whose structure the network has learned from millions of pre-training samples, encapsulates latent image properties from which classes can be predicted quite easily.



With the thus preprocessed data, we train an image classification model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Predicting new images with the pipeline}
\label{sec:methods-predict}

Consider a set $\D_{new} = (\xi[1], \dots, \xi[w])$ of new images: 
First, we apply object detection, cropping and resizing, and the forward pass through the pre-trained model. 
The resulting data is used to predict the class scores 
of the bb-images $\D_{z,new}$ with the image classifier $\pixh$.
Then, the results of object detection and image classification are merged: The predicted label is set to ``empty'' for all images which (i) have no bounding boxes with a confidence of at least $\alpha$ or (ii) have bounding boxes that pass the threshold but are classified as ``empty'' by the image classifier.
For all other images, the predicted label is set to the non-empty class with the highest weighted average of the bounding box predictions, where the MD confidences are used as weights.
This weighted average is retained as final confidence about the class. 
This yields

\begin{enumerate}[label=(\roman*)]
    \item predicted labels $\yih$ for each image $\xi[i] \in \D_{new}$,
    \item average weighted predicted class scores $\hat{\pi}_k^{(i)}$ for each class $k \in \gset$ per image, 
    \item the total predicted number $o_k^{(i)}$ of each animal class per image, and
    \item a final confidence score $d^{(i)} = \max_{k \in \gset} \hat{\pi}_k^{(i)}$ per image, reflecting the highest class-wise score.
\end{enumerate}

Optionally, images whose highest confidence for any class  
does not exceed a user-specified threshold, i.e., $d^{(i)} \leq \beta, $ may still remain ``unlabeled'', thus giving the user the opportunity to label it manually in a later iteration of the AL process. 


\subsubsection{Tuning pipeline} 
\label{sec:tuning}

\paragraph{Resampling strategy}

The above pipeline comprises hyperparameters (\begin{enumerate*}[label=(\roman*)]
    \item megadetector threshold
    \item choice of the pre-trained network
\end{enumerate*}) which cannot be learned during training but need to be optimized via hyperparameter tuning. 
Fig.~\ref{fig:resampling} visualizes the resampling strategy and pipeline for tuning and evaluation:
We divide $\D$ randomly (stratified by camera station) into data for tuning and training ($\Dtune$) and data for evaluating the model performance ($\DTest$), where $15\%$ are used for $\DTest$. Tuning data $\Dtune$ is further split into training data $\DTrain$ and validation data $\Dval$, such that those sets comprise $70\%$ and $15\%$ of $\D$, respectively. Training data $\DTrain$ is then used to train a classification model on the level of bounding boxes for each combination of hyperparameters; the best hyperparameter combination is chosen with respect to the performance of the pipeline on validation data $\Dval$. Finally, generalization errors of the classifier and the pipeline, endowed with the chosen hyperparameters, are estimated on the untouched test data $\DTest$.

\begin{figure}[t]
    \centering
    \includegraphics[width=14.4cm]{fig/resampling_230327.png}%11.4
    \caption{Resampling scheme. Hyperparameter tuning is carried out on tuning data $\Dtune$. Generalization errors using the best hyperparameters are estimated on untouched test data $\DTest$.}
    \label{fig:resampling}
\end{figure}

\begin{tune}
\paragraph{Tuning}
We train a model $\pih_{\lamv}(\cdot)$ for each hyperparameter combination $\lamv \in \Lam$ -- where $\Lam$ is the corresponding search space -- on $\D_{z,train}$ and evaluate the pipeline of object detection and image classification with this model $\pih_{\lamv}(\cdot)$ on $\Dval$. %it on $\D_{z,val}$ 


Predicted labels are computed by combining the results of object detection and image classification -- as explained above for new images.
Evaluation metrics $\rho_{\lamv}$ are computed 
for each hyperparameter combination separately. 
For our tuning process, one of the four alternatives recall, precision, F1-score, or accuracy can be chosen by the user -- for the results presented in this paper we choose the weighted F1-score.
The hyperparameter combination yielding the highest $\rho_{\lamv}$ on $\Dval$, i.e., 
$\bm{\lambda}^* = \argmax_{\lamv \in \Lam} \rho_{\lamv}$, is considered the best hyperparameter combination, the corresponding model is 
$\pi^*(\cdot)$. 


\paragraph{Evaluation at the level of bounding boxes}  
We first carry out the object detection and the above preprocessing steps of cropping, resizing, and the forward-pass through the pre-trained model -- as indicated by the respective value in $\bm{\lambda}^*$ -- on $\DTest$. The labels of the resulting bb-images are predicted by $\pi^*(\xv)$ and an evaluation metric can be computed for the high-conf bb-images in $\D_{z,test}$. 

\paragraph{Evaluation at the level of original images} As above, we merge the results of object detection and image classification for the images in $\DTest$ and compute the respective evaluation metric.



\end{tune}


\subsection{Pipeline for active learning}
\label{sec:methods-active}

We propose the following active learning (AL) pipeline (see also Algorithm \ref{alg:al}).

\textit{(1) Initialization:} A data set of unlabeled images is provided by the user. If available, information about which images belong to the same camera station should be provided, too, to allow for stratification in the resampling process.

\textit{(2) AL loop:} As long as the user is unsatisfied with the prediction performance of the current model (as reported by the evaluation step -- and optionally by visual inspection), the active learning loop is carried out: 

\textit{(2a) Image selection:} A user-specified number of unlabeled images is selected by the AL engine. 
These are the images that have the highest value of a so-called ``acquisition function''; the value of this function reflects the potential of an image for improving the model -- we use softmax entropy. 
Optionally, this can be done stratified by camera stations. 
In the first iteration of the AL loop, where no model is available, image selection is done randomly.

\textit{(2b) Manual labeling:} The selected images are labeled by a human expert.

\textit{(2c) Model training:} A full step of tuning, training, and evaluation as described above is carried out, based on the available labeled data. 
If the time budget is limited, the user may consider skipping tuning here.

\textit{(3) Predict remaining unlabeled images:} As the last step of the pipeline, all remaining unlabeled images are predicted by the final model.

For evaluation of the trained models, a sufficiently large subset of the labeled images needs to be set apart as a test set. This test set is used only for evaluating the performance of the model and remains unchanged during the AL loop. 

\begin{algorithm}
\caption{Active Learning Workflow (H: Human -- M: Machine)}
\label{alg:al}
\begin{algorithmic}
\State H: Load data (unlabeled or partly labeled) \Comment{\textit{(1)}}
\If{No or not enough labeled images} 
    \State M: Select random images to be labeled \Comment{\textit{(2a)}}
    \State H: Label images manually \Comment{\textit{(2b)}}
\EndIf
\State M: Tune, train, evaluate initial model \Comment{\textit{(2c)}}
\While{H: Not satisfied with test performance} \Comment{\textit{(2)}}
    \State M: Select best images to be labeled via acquisition function \Comment{\textit{(2a)}}
    \State H: Label images manually \Comment{\textit{(2b)}}
    \State M: Tune, train, evaluate \Comment{\textit{(2c)}}
\EndWhile
\State M: Predict all remaining unlabeled images with the last model \Comment{\textit{(3)}}
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}


\subsection{In-sample results}

For tuning, training, and evaluating the models, we use a $70\%$-$15\%$-$15\%$ train-val-test split and report evaluation metrics only for the test set which is not seen during training or tuning of the models. As data, we use the 24,368 ``in-sample'' images shown in Table \ref{tab:train_data}. 
As tuning search space $\Lam$ we use a grid consisting of all combinations of architecture $\in$ \{InceptionResNetV2, Xception, DenseNet121\}, which are all state of the art across many image classification tasks and parameter-efficient at the same time, and MD threshold $\alpha \in \{0.1, 0.3, 0.5, 0.7, 0.9\}$. 
All reported numbers represent the average over results from three independent runs.
        
\subsubsection{Benefit of tuning hyperparameters}
\label{sec:tuning_results}

Table \ref{tab:tuning} shows the weighted F1-score on $\Dval$ for the 8-class classification for different choices of the hyperparameters. 
As can be seen, tuning the hyperparameters properly has an impact on the predictive performance of the model. 
The bold first row reflects the optimal choice of hyperparameters which is used for the following experiments.
At the same time, we observe that our pipeline is fairly robust also under other values of the hyperparameters, underlining its usability for a broad range of applications.

\begin{table}[h]%[tbhp]
\centering
\caption{Hyperparameter tuning: best and worst configurations sorted by F1-score}
\begin{tabular}{lrr}
\label{tab:tuning}
Confidence & Architecture & F1-score\\
\midrule
\textbf{0.5} & \textbf{Xception} & \textbf{0.909} \\
0.7 & Xception & 0.908 \\
0.9 & InceptionResNetV2 & 0.908 \\
$\vdots$ & $\vdots$ & $\vdots$ \\
0.3 & DenseNet121 & 0.899 \\
0.1 & InceptionResNetV2 & 0.894 \\
0.1 & DenseNet121 & 0.888 \\
\bottomrule
\end{tabular}
\end{table}


\subsubsection{Empty vs. non-empty images}

During tuning, the best confidence threshold was 
$\alpha=0.5$. 
We can now compare results for the binary classification of ``empty'' vs. ``non-empty'' for different choices of $\alpha$ in Table \ref{tab:empty_results}.
The empty recall increases with $\alpha$ due to the fact that the MD is considering more images as empty.
On the other hand, the non-empty recall decreases with $\alpha$, meaning that more non-empty images (i.e., animals) are retrieved for smaller $\alpha$. 
In fact, with tuned $\alpha=0.5$, the false-empty rate is $14.2\%$ less than with $\alpha=0.9$ (a common choice, e.g., \cite{norouzzadeh_deep_2021}) ($10.9\%$ vs. $12.7\%$), meaning that $14.2\%$ fewer animal images are overlooked. From a practical perspective, this is highly desirable: While it is no big effort to manually discard some images that are empty but labeled as non-empty, it is not feasible to detect non-empty images that are labeled as empty without screening all the images again -- which is exactly what we wish
to avoid by using an automatic image classifier.
This shows that tuning $\alpha$ is also beneficial for the ``empty'' vs. ``non-empty'' performance and increases the number of detected animals overall.

\begin{table}[h]%[tbhp]
\centering
\caption{Empty vs. non-empty images, ``no MD'' shows metrics for not using object detection at all, hence only image classification on entire images.}
\begin{tabular}{l|c|ccc|ccc}
\label{tab:empty_results}
& & & empty & & & non-empty & \\
\hline
Confidence & Accuracy & Precision & Recall & F1 & Precision & Recall & F1 \\[.1ex]
\hline
no MD  &  0.920 & 0.913 & 0.969 & 0.940 & 0.936 & 0.830 & 0.880\\
0.3  &  0.952 & 0.947 & 0.981 & 0.964 & 0.963 & 0.900 & 0.930\\
0.5 & 0.949 & 0.943 & 0.982 & 0.962 & 0.964 & 0.891 & 0.926 \\
0.7 &  0.952 & 0.940 & 0.990 & 0.964 & 0.979 & 0.885 & 0.930\\
0.9 &  0.950 & 0.934 & 0.991 & 0.962 & 0.981 & 0.873 & 0.924\\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Multi-class model performance}
\begin{kurz}
Table \ref{tab:multi_results} shows the results of the image classifier on the test data for the multi-class classification of 7 animal classes and class ``empty'' for different choices of $\alpha$. Again, the tuned confidence threshold of $\alpha=0.5$ leads to better results in all the metrics but for precision.

\begin{table}[h]%[tbhp]
\centering
\caption{Multi-class classification. Precision and F1-score are weighted averages of 8 classes, (weighted recall is not shown since equal to accuracy by definition).}
\begin{tabular}{lrrr}
\label{tab:multi_results}
Confidence & Accuracy & Precision &  F1\\
\midrule
0.3 & 0.926 & 0.922 &  0.922 \\
0.5 & 0.924 & 0.920 &  0.920 \\
0.7  & 0.928 & 0.923 & 0.923 \\
0.9 & 0.923 & 0.917 &  0.916 \\
\bottomrule
\end{tabular}
\end{table}

\end{kurz}



Table \ref{tab:ins-oos}, first and second row, lists overall metrics of the classification performance on $\DTest$. The first row contains results of omitting object detection and directly performing image classification on the entire images; a comparison with the second row reveals a clear benefit of using the pipeline of object detection and image classification.
Fig.~\ref{fig:multi_conf} shows the detailed predictive performance of the tuned pipeline.  
Performance for larger classes such as ``roe deer'' is better than for smaller classes such as ``wild boar''.
The class ``other'' proves difficult to detect, which might be due to the fact that this class is rather heterogeneous as it consists of (i) other animal species than considered here (hence low numbers per class) and (ii) images where the true animal class could not be determined by the human annotators with sufficient confidence. 

\begin{table}[h]%[tbhp]
\centering
\caption{Multi-class classification. Precision and F1-score are weighted averages of 8 classes, (weighted recall is not shown since equal to accuracy by definition).}
\begin{tabular}{lrrrr}
\label{tab:ins-oos}
{} & Accuracy & Precision &  F1\\
\midrule
no MD (in-sample) &  0.865 & 0.869 &  0.856 \\
In-sample &  0.924 & 0.920 &  0.920 \\
Out-of-sample & 0.881 & 0.874 &  0.873 \\
Active learning -- 40 $\%$ & 0.922 & 0.917 &  0.916 \\
Active learning -- 100 $\%$& 0.926 & 0.921 &  0.920 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]%[tbhp]
\centering
\begin{fig_publish}
\includegraphics[width=.7\linewidth]{fig/cm_insample.eps}
\end{fig_publish}
\begin{fig_count_words}
\includegraphics[width=.7\linewidth]{fig/cm_insample.png}
\end{fig_count_words}
\caption{In-sample performance}
\label{fig:multi_conf}
\end{figure}

Fig.~\ref{fig:misclass} displays some misclassified images and points to cases where the classification is wrong. Reasons seem to be, among others: Extreme close-up (\ref{fig:wrong_close}), difficulties due to partial occlusion (\ref{fig:occlusion}), poor lighting (\ref{fig:bad_light}), very small objects (\ref{fig:wrong_tiny}), wrong bounding box by MD (\ref{fig:wrong_bb}), and wrong human label (\ref{fig:wrong_label}). 

\begin{figure}%[tbhp]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fig/miscl_close.png}
         \caption{\centering Extreme close-up. \textit{Label}: roe deer, \textit{predicted}:~others, \textit{confidence}: 0.9
         \vspace{0.5cm}
         }
         \label{fig:wrong_close}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fig/miscl_partial_occlusion.png}
         \caption{\centering Partial occlusion. \textit{Label}: others, \textit{predicted}:~red squirrel, \textit{confidence}: 1.0
         \vspace{0.5cm}
         }
         \label{fig:occlusion}
     \end{subfigure}
     \hfill
          \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fig/miscl_dark.png}
         \caption{\centering Poor light. \textit{Label}: roe deer, \textit{predicted}:~empty, \textit{confidence}: 0.9
         \vspace{0.5cm}
         }
         \label{fig:bad_light}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fig/miscl_tiny.png}
         \caption{\centering Small objects. \textit{Label}: red squirrel, \textit{predicted}:~empty, \textit{confidence}: 1.0
         \vspace{0.5cm}
         }
         \label{fig:wrong_tiny}
     \end{subfigure}
     \hfill
    \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fig/miscl_wrong_bbox.png}
         \caption{\centering Wrong bounding box. \textit{Label}: empty, \textit{predicted}:~roe deer, \textit{confidence}: 0.5}
         \label{fig:wrong_bb}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fig/miscl_wrong_true.png}
         \caption{\centering Wrong human label. \textit{Label}: empty, \textit{predicted}:~others, \textit{confidence}: 0.7}
         \label{fig:wrong_label}
     \end{subfigure}
    \caption{Misclassified images}
    \label{fig:misclass}
\end{figure}


\subsection{Out-of-sample results} \label{sec:oosample}

For a sensible comparison of OOS results, we sampled a random test set from the OOS images, consisting of 15\% of the images, stratified by camera station. This test set comprises 3,563 images, from which 1,264 images were identified as non-empty by the MD at confidence $\alpha = 0.5$, is never used for training or tuning, and remains unchanged over all model comparisons.

\subsubsection{Model performance out-of-sample without active learning}

Table \ref{tab:ins-oos}, third row, shows the predictive performance of predicting the OOS test data with the model trained on the entire in-sample data; the drop in predictive performance is expected and in line with results in the literature. Fig.~\ref{fig:oos_multi} shows detailed results of the image classifier for the OOS test data. A comparison with the class distribution between in-sample and out-of-sample data reveals that performance is especially low where the distribution has shifted most. For example, there are only 26 red deers in the entire training data; it is hence unsurprising that the predictive performance for this class will be rather low. This distribution shift between train and test data is exactly the problem of OOS prediction which shall be remedied by using active learning: By adding more and more training data from the new distribution, the performance should increase until a satisfying level is reached.





\begin{figure}%[tbhp]
\centering
\begin{fig_publish}
\includegraphics[width=.7\linewidth]{fig/cm_oosample.eps}
\end{fig_publish}
\begin{fig_count_words}
\includegraphics[width=.7\linewidth]{fig/cm_oosample.png}
\end{fig_count_words}
\caption{Out-of-sample performance without AL.}
\label{fig:oos_multi}
\end{figure}

\subsubsection{Model performance out-of-sample with active learning}

\begin{figure}%[tbhp]
\centering
\begin{fig_publish}
\includegraphics[width=\linewidth]{fig/active_learning.eps}
\end{fig_publish}
\begin{fig_count_words}
\includegraphics[width=\linewidth]{fig/active_learning.png}
\end{fig_count_words}
\caption{Active-learning performance (8-class accuracy) for cold and warm start as compared to out-of-sample prediction (lower horizontal line) and training on entire data set (upper horizontal line). Error bars indicate variability over 3 different random seeds.}
\label{fig:oos_active}
\end{figure}

Fig.~\ref{fig:oos_active} shows the results of the active learning pipeline.
For a proper assessment of the benefit of active learning, we compare it with two baselines: The lower baseline is the model trained on the in-sample data (Table \ref{tab:ins-oos}, third row). The upper baseline is a model trained on the entire OOS data (Table \ref{tab:ins-oos}, last row). We compare two active learning strategies; for both strategies we use the architecture that was found to be optimal in the above tuning procedure, i.e., the Xception architecture. 
The classification head (i.e., the set of final linear layers) is replaced by a new classification head with freshly initialized weights.
Note that this enables tailoring the pipeline to tasks with an arbitrary set of classes by simply adjusting the number of output units in the last layer.
The two strategies differ in the following:

\begin{itemize}
    \item \textbf{Cold-start:} For the rest of the neural net (the so-called backbone responsible for the task of learning a suitable data representation), the weights pre-trained on ImageNet are used.
    \item \textbf{Warm-start:} For the backbone, the weights pre-trained on ImageNet and on the in-sample data are used.
\end{itemize}

\begin{figure}%[tbhp]
\centering
\begin{fig_publish}
\includegraphics[width=.7\linewidth]{fig/cm_active.eps}
\end{fig_publish}
\begin{fig_count_words}
\includegraphics[width=.7\linewidth]{fig/cm_active.png}
\end{fig_count_words}
\caption{Out-of-sample performance with AL using $40\%$ of available data, i.e., 3,968 images.}
\label{fig:al_multi}
\end{figure}

As can be seen in the very left part of Fig.~\ref{fig:oos_active}, both active learning strategies are worse than the lower baseline (which has never encountered samples from the OOS data) in the initial iterations. This is due to the fact that the model has not yet seen enough of the new data to classify them correctly -- while the lower baseline was at least trained on a sufficient number of wildlife images (if only images from a different domain).
With an increasing amount of images, the active learning models improve and become better than the lower baseline when using more than 128 (warm-start) and 384 (cold-start) images, respectively. After that, the advantage of warm-starting vanishes, which means that the information in the OOS data is eventually better than the information in the in-sample data.
Using a relative sample size of $40\%$, i.e. 3,968 images, we already reach $99.6\%$ of the performance of the upper baseline, showing the benefit of using active learning.

Note the general applicability of the active learning approaches: Since we replace the classification head, the strategy can be used to learn models predicting any set of classes. This allows learning well-performing models with just a small number of manually annotated images. A further advantage is that the number of images to be labeled by a human does not have to be fixed beforehand: The active learning procedure can be carried out iteratively until a satisfying performance is reached, thus using the human annotation labor very efficiently. 
Furthermore, if the new images are expected to be rather similar to the existing ones (e.g., by setting up a new camera station in a territory comprising the same fauna), warm-starting may be beneficial and can decrease the number of images needed even more.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}


We presented two methodological advances in using deep learning methods for wildlife image classification. First, a multi-step pipeline that combines the ideas of object detection and image classification with a thorough tuning procedure of the involved hyperparameters. Second, an active learning component that enables efficient training of a high-performing model on new data, potentially from a new monitoring location or involving previously unseen animal species. We accompany the methodological developments with ready-to-use software which does not require programming skills of the user. Thereby, we leverage the potential of deep learning and active learning for a broad target group including all researchers in ecology, even with different analytical backgrounds.

While the methods are indeed easy to use, it is helpful to have access to sufficient computing resources. 
Although it is possible to use the methods on a local machine with only CPUs, working on a machine with a GPU is substantially faster. A rather time-consuming step is the application of the \textit{Megadetector} for finding the bounding boxes, which, however, needs to be carried out only once before starting the active learning loop. There are attempts to approximate the \textit{Megadetector} with a far smaller object detection method \citep[e.g.,][]{rigoudy_deepfaune_2022} saving computing time. We still opt for the \textit{Megadetector} due to its very convincing performance. The other time-consuming step is the repeated tuning of the hyperparameters which can slow down the progress in the active learning loop; we, therefore, allow to optionally skip the tuning inside the active learning.
In many applications, though, human labor will remain the scarcest resource, and the need for it is greatly alleviated with our pipeline.

\paragraph{Future work}
There are some directions of future work that we would like to pursue:
\begin{enumerate*}
    \item Labeling bounding boxes instead of original images could further improve the performance of the models.
    \item More complex acquisition functions in the active learning pipeline could improve efficiency.
    \item We already have first results on incorporating methods from explainable AI in the pipeline. Explaining the predictions of the model can help to find systematic errors.
    \item Quantifying the predictive uncertainty of the model and the derived population count might give more realistic insights into the data. 
    \item Using metadata such as time, date, etc.\ as additional input has the potential to further improve the results. 
    \item Combining machine learning and domain knowledge could be a promising next step, see, e.g., \cite{tuia_perspectives_2022}.
    \item Finally, we plan on developing an R package that helps researchers with downstream analyses of the information from the image classification pipeline.
\end{enumerate*}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Acknowledgements}

We thank Michael Jeske for helping with labeling the images. We thank Holger Löwe for helping with visualizations. CB, WP, and AM received funding within the project LandKlif, funded by the Bavarian State Ministry of Science and the Arts in the context of the Bavarian Climate Research Network (bayklif). HE, WP, and CB received funding from the Bavarian State Ministry of Agriculture and Forestry (grant ST375). TW received funding from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) as part of BERD@NFDI - grant number 460037581.
LW received funding from the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the Federal Ministry of Education and Research.

\section*{Conflict of Interest statement}

The authors declare no conflicts of interest.

\section*{Author Contributions}

Conceptualization, LB, HE, WP, CB, and AM;
Methodology, LB, OC, LW, and TW;
Software, LB, OC, LW, and TW;
Formal analysis, LB, OC, and LW;
Resources, WP and AM;
Data curation, CB, HE and HN;
Writing - Original Draft, LB;
Writing - Review \& Editing, all authors;
Visualization, OC and LW


\section*{Data Availability}

The image data set will be archived on Dryad upon acceptance. 
\begin{kurz}
The code base for reproducing the results of this paper and for using the methods for other use cases is available on Github: \url{https://github.com/slds-lmu/wildlife-experiments} \url{https://github.com/slds-lmu/wildlife-ml}
\end{kurz}

% Bibliography
%\bibliography{mybibliography} %=> export from zotero
\bibliography{mybib-manually} % manually modified

\end{document}