\section{Introduction}
In this work, the problem under investigation is \emph{expected risk minimization}
\begin{equation} \label{eq:expected-risk-minimization}
    \begin{alignedat}{2}
        & \minimize_{\theta \in \Theta} &\quad& \E[ \ell(\theta, \xi)].
    \end{alignedat}
\end{equation}
Here $\Theta \subseteq \Re^{n_\theta}$ and $\xi \colon \Omega \to \Xi \subseteq \Re^{n_{\xi}}$ a random vector defined 
on sample space $(\Omega, \F, \prob)$. 
Problems like \cref{eq:expected-risk-minimization} are omnipresent in machine-learning and statistics \cite{Shalev-Shwartz2013,Vapnik1998}. 

In practice the expectation in \cref{eq:expected-risk-minimization} cannot be evaluated exactly. Instead 
a sample $\{\xi_i\}_{i=1}^n$ is available. Let $\ell_i(\theta) = \ell(\theta, \xi_i)$ and $L(\theta) = (\ell_1(\theta), \dots, \ell_{n}(\theta))$.
We then solve 
\begin{equation} \label{eq:robustified-erm}
    \begin{alignedat}{2}
        & \minimize_{\theta \in \Theta} \, \maximize_{\mu \in \amb} &\quad& \<\mu, L(\theta)\>,
    \end{alignedat}
\end{equation}
for some convex, non-empty and closed \emph{ambiguity set} $\amb \subset \Re^n$ of probability vectors. 
The usual \emph{empirical risk minimization} or \emph{sample average approach (SAA)} amounts to $\amb = \{\one_n/n\}$. 
We denote the value of the maximization in \cref{eq:robustified-erm} as $\rho[L(\theta)]$
and refer to it as a \emph{risk measure} \cite[\S6.3]{Shapiro2021}, \cite{Pichler2021}. When $\amb$ is permutation invariant 
(i.e. for any $\mu \in \amb$ and permutation $\pi$, $\pi\mu \in \amb$) we show
\begin{equation} \label{eq:well-callibrated}
    \prob\left[\rho[L(\theta)] \dfn \max_{\mu \in \amb} \<\mu, L(\theta)\> \geq \E[\ell(\theta, \xi)]\right] \geq 1 - \varepsilon
\end{equation}
for all $\theta \in \Theta$ when $\ell_n(\theta)$ is replaced by $\bar{\ell}(\theta)$ which upper bounds $\ell(\theta, \xi)$ with probability one. 
The value of $1 - \varepsilon$ is given by the volume of a so-called \emph{distortion set}, which is related to $\rho$ through a modified convex conjugate. 
Whenever $\rho$ satisfies \cref{eq:well-callibrated} we say it is \emph{$\varepsilon$-calibrated}. Note that \cref{eq:well-callibrated} defines two problems:
\emph{(i)} an analysis problem, where given some $\rho$ a $\varepsilon$ should be determined; \emph{(ii)} a synthesis/calibration problem where we want to determine some (parametric) $\rho$
such that \cref{eq:well-callibrated} holds for some user provided $\varepsilon$. We solve both problems in this work and it turns out that calibrated risk measures solving \emph{(ii)} serve as 
good proxy costs for \cref{eq:expected-risk-minimization}. 

Solving \cref{eq:robustified-erm} instead of a sample average is nothing new and several sets $\amb$ have been considered under the framework of \emph{distributionally robust optimization (DRO)} \cite{Pichler2021}.
Specifically $\amb$ can be based on moments as in \cite{Delage2010}, divergence from the empirical distribution as in \cite{Ben-Tal2013},
Wasserstein distance \cite{Esfahani2018b} or other statistical hypothesis tests \cite{Bertsimas2018}. See \cite{Lin2022,Rahimian2019} for recent surveys.
There are however some issues with classical DRO. Sets often depend on some parameters (e.g. some radius). These are sometimes determined using concentration inequalities \cite{Delage2010} 
or using inexact asymptotic bounds \cite{Ben-Tal2013} to make sure the true distribution is contained within $\amb$ with high confidence. This can often be 
conservative in the case fo concentration inequalities, either due to loose constants in the concentration inequalities or due to the shape of the ambiguity set. So instead one often resorts to either bootstrapping or 
cross validation (cf. \cite{Esfahani2018b}), which can be computationally expensive and lack statistical guarantees for finite samples, similarly to the asymptotic bounds. 

Recently, in the asymptotic setting, this conservativeness has been tackled by looking at \emph{cost-aware} bounds like \cref{eq:well-callibrated} \cite{Duchi2021b,Lam2019}
with good results. They also discuss the gap between \cref{eq:well-callibrated} and the usual uniform bounds (i.e. $\prob[\rho[L(\theta)] \geq \E[\ell(\theta, \xi)], \forall \theta] \geq 1 - \varepsilon'$)
provided by DRO. The finite sample case has not been considered much. We take the first steps in that direction in this work. 

To summarize, our approach is novel as follows:
\begin{enumerate}
    \item Our scheme provides \emph{cost-aware}, point-wise statistical guarantees as in \cref{eq:well-callibrated} for any number of samples;
    \item The bounds are tight for any distribution, 
            replacing the involved analytical derivations of concentration inequalities with efficient numerical procedures;
    \item The framework applies to many risks from literature, including distortion \cite{Bertsimas2009b} and $\phi$-divergence risks \cite{Ben-Tal2013};
    \item We present the \emph{distortion representation} of risks.
\end{enumerate}
The resulting scheme also outperforms SAA in both regression and classification as illustrated in \cref{sec:case-studies}, while being less sensitive to tuning parameters.

We exploit results from convex duality \cite{Rockafellar1998}, order statistics \cite{Wilks1964,David2003}, Kolmogorov-Smirnov statistics \cite{Moscovich2020}, 
majorization \cite{Marshall2011,Steerneman1990} and isotonic regression \cite{Best2000}. 








% The classical approach of \emph{empirical risk minimization}, where a cost $\sum_{i=1}^{n} \ell(\theta, \xi_i)/n$
% is minimized, often does not suffice due to the well known problem of over-fitting. Instead \emph{distributionally robust optimization (DRO)} 
% or \emph{risk averse stochastic programming} has been considered recently. Here, one minimizes a cost 
% \begin{equation} \label{eq:ambiguity-representation-continuous}
%     \rho[\ell(\theta, \xi)] = \sup_{\mu \in \set{A}} \int \ell(\theta, \xi) \di \mu(\xi). 
% \end{equation}
% The function $\rho$, referred to as a \emph{risk measure}, maps (scalar) random variables to real values and is \emph{coherent} whenever the \emph{ambiguity set} $\set{A}$ is a non-empty, closed and 
% convex set of probability distributions \cite{Ruszczynski2006}. 

% In classical DRO, the ambiguity set is calibrated such that 
% \begin{equation}
%     \prob[\mu_{\star} \in \amb] \geq 1 - \varepsilon,
% \end{equation}
% with $\mu_{\star}$ the true distribution of $\xi$. Such sets can be based on moments \cite{Delage2010}, divergence from the empirical distribution \cite{Ben-Tal2013},
% Wasserstein distance \cite{Esfahani2018b} or other statistical hypothesis tests \cite{Bertsimas2018}. 
% If this distribution is within $\amb$ then clearly the maximum is larger than $\E[\ell(\theta, \xi)]$ 
% uniformly over $\theta$. That is 
% \begin{equation} \label{eq:uniform-confidence-guarantee}
%     \prob\left[\rho[\ell(\theta, \xi)] \geq \E[\ell(\theta, \xi)], \forall \theta \in \Theta\right] \geq 1 - \varepsilon.
% \end{equation}
% As previously observed in \cite{Duchi2021b,Lam2019}, guaranteeing that $\mu_{\star} \in \mathcal{A}$ can be overly conservative.
% This is why, in practice, the ambiguity set $\amb$ is either calibrated 
% using bootstrapping \cite{Delage2010}, asymptotic versions of \cref{eq:uniform-confidence-guarantee} \cite{Bertsimas2018, Ben-Tal2013} or using cross-validation \cite{Esfahani2018b}. 
% These methods are either computationally expensive or lack guarantees for low sample counts -- the case where over-fitting is likely to occur.

% Instead it can be helpful to initially focus on a point-wise guarantee instead of the uniform one of \cref{eq:uniform-confidence-guarantee}. 
% That is 
% \begin{equation} \label{eq:point-confidence-guarantee}
%     \prob[\rho[\ell(\theta, \xi)] \geq \E[\ell(\theta, \xi)]] \geq 1 - \varepsilon, \quad \forall \theta \in \Theta.
% \end{equation}
% This is the approach of \cite{Duchi2021b,Lam2019}. 
% They start with a DRO scheme based on $\phi$-divergence balls for $\amb$, guaranteeing \cref{eq:point-confidence-guarantee} in the limit $n \to \infty$. They show that a slight 
% increase in the radius of the ball $\amb$ suffices when both sides of the event in \cref{eq:point-confidence-guarantee} are minimized over $\theta$. 
% These guarantees however only hold for sufficiently large $n$. So over-fitting can still occur.% (cf. Figure~1(b) in \cite{Duchi2021b}). 

% We similarly produce guarantees like \cref{eq:point-confidence-guarantee}, but ours are applicable for any number of samples. 
% To do so we begin by providing an alternative representation of \cref{eq:ambiguity-representation-continuous} when the 
% distributions are only supported on the samples and when the risk measure is \emph{law-invariant}. That is, it produces the same result for
% identically distributed random variables. We call this alternative representation a \emph{distortion representation}, which
% relates to distortion risk measures \cite{Bertsimas2009b} and involves a modified support function over a \emph{distortion set}. 

% Results from order statistics then enable computing $\varepsilon$ in \cref{eq:point-confidence-guarantee} 
% for any such risk measure in terms of the volume of the distortion set. 
% For a subset of these risk measures we conversely illustrate how, given some $\varepsilon$,
% the risk measure can be configured to satisfy \cref{eq:point-confidence-guarantee}. This subset includes the \emph{conditional value-at-risk}
% and the $\phi$-divergences from \cite{Duchi2021b,Lam2019}. 


% We show how the large class 
% of \emph{law-invariant, coherent risk measures} -- which are dual to DRO -- serve as high-confidence bound of the expectation.
% This suggest a large class of proxy costs satisfying \cref{eq:point-confidence-guarantee}, the value of $\varepsilon$ of which can be computed 
% tightly for every $n$, contrasting other work which only supports sufficiently large values of $n$. 
% For a subset of these law-invariant, coherent risk measures we conversely illustrate how given some $\varepsilon$ 
% the risk measure can be configured to satisfy \cref{eq:point-confidence-guarantee}. This subset includes the well known \emph{conditional value-at-risk}
% and the $\phi$-divergences considered in \cite{Duchi2021b,Lam2019}. 

% Problems like \cref{eq:expected-risk-minimization} are omnipresent in machine-learning and statistics in general \cite{Shalev-Shwartz2013,Vapnik1998}. 
% The loss function $\ell \colon \Theta \times \Xi \to \Re$ can model some estimation error 
% in the case of regression or classification \cite[\S9]{Shalev-Shwartz2013}; it can model the 
% log-likelihood of the data in the case of density estimation \cite[\S1.6]{Vapnik1998}; the return on an investment in case of 
% portfolio optimization \cite[\S1.4]{Shapiro2021}; the profit given some uncertain demand in the case of multi-product assembly \cite[\S1.3]{Shapiro2021}
% or it can model some (relaxed) chance constraint in stochastic optimal control problems \cite{Lorenzen2017}.

% In practice the expectation in \cref{eq:expected-risk-minimization} cannot be evaluated explicitly. Instead 
% a sample $\{\xi_i\}_{i=1}^n$ is available. Often \cref{eq:expected-risk-minimization} is then approximated 
% using \emph{empirical risk}
% \begin{equation}\label{eq:empirical-risk-minimization}
%     \begin{alignedat}{2}
%         & \minimize_{\theta \in \Theta} &\quad& \frac{1}{n} \sum_{i=1}^{n }\ell(\theta, \xi_i).
%     \end{alignedat}        
% \end{equation}
% Under this approximation, over-fitting will often occur. This can be mitigated through regularization -- e.g. by adding a term $\lambda \cdot \nrm{\theta}_1$ with hyperparameter $\lambda$ --
% which should render the problem stable with respect to variability in the samples. Thus over-fitting is reduced at the cost of a worse fit  \cite[\S13.2]{Shalev-Shwartz2013}. 
% This fitting-stability tradeoff (or alternatively bias-complexity tradeoff \cite[\S13.4]{Shalev-Shwartz2013}) is challenging to evaluate in practice, 
% making tuning of hyper-parameters difficult. 

% As an alternative to \cref{eq:empirical-risk-minimization} \emph{distributionally robust optimization (DRO)} considers the proxy-cost 
% \begin{equation} \label{eq:coherent-risk-functional}
%         \rho[\ell(\theta, \xi)] \dfn \max_{\mu \in \mathcal{A}} \, \int_{\xi} \ell(\theta, \xi) \di \mu(\xi),
% \end{equation}
% with the so-called  \emph{ambiguity set} $\amb$ a closed, convex set of distributions selected such that $\prob[\mu_{\star} \in \mathcal{A}] \geq 1-\varepsilon$,
% with $\mu_{\star}$ the true distribution of $\xi$. Such sets can be based on moments \cite{Delage2010}, divergence from the empirical distribution \cite{Ben-Tal2013},
% Wasserstein distance \cite{Esfahani2018b} or other statistical hypothesis tests \cite{Bertsimas2018}. 
% If this distribution is within $\amb$ then clearly the maximum is larger than $\E[\ell(\theta, \xi)]$ 
% uniformly over $\theta$. Hence 
% \begin{equation} \label{eq:uniform-confidence-guarantee}
%         \prob[\rho[\ell(\theta, \xi)] \geq \E[\ell(\theta, \xi)], \forall \theta \in \Theta] \geq 1 - \varepsilon.
% \end{equation}
% As previously observed in \cite{Duchi2021b,Lam2019}, guaranteeing that $\mu_{\star} \in \mathcal{A}$ can be overly conservative.
% This is why, in practice, the ambiguity set $\amb$ is either calibrated 
% using bootstrapping \cite{Delage2010}, asymptotic versions of \cref{eq:uniform-confidence-guarantee} \cite{Bertsimas2018, Ben-Tal2013} or using cross-validation \cite{Esfahani2018b}. 
% These methods are either computationally expensive or lack guarantees for low sample counts -- the case where over-fitting is likely to occur.

% Instead it can be helpful to initially focus on a point-wise guarantee instead of the uniform one of \cref{eq:uniform-confidence-guarantee}. 
% That is 
% \begin{equation} \label{eq:point-confidence-guarantee}
%     \prob[\rho[\ell(\theta, \xi)] \geq \E[\ell(\theta, \xi)]] \geq 1 - \varepsilon, \quad \forall \theta \in \Theta.
% \end{equation}
% This is the approach of \cite{Duchi2021b,Lam2019}. 
% They start with a DRO scheme based on $\phi$-divergence balls for $\amb$, guaranteeing \cref{eq:point-confidence-guarantee} in the limit $n \to \infty$. They show that a slight 
% increase in the radius of the ball $\amb$ suffices when both sides of the event in \cref{eq:point-confidence-guarantee} are minimized over $\theta$. 
% These guarantees however only hold for sufficiently large $n$. So over-fitting can still occur.% (cf. Figure~1(b) in \cite{Duchi2021b}). 

% We take a similar approach, viewing $\ell(\theta, \xi)$ as a scalar random variable. We show how the large class 
% of \emph{law-invariant, coherent risk measures} -- which are dual to DRO -- serve as high-confidence bound of the expectation.
% This suggest a large class of proxy costs satisfying \cref{eq:point-confidence-guarantee}, the value of $\varepsilon$ of which can be computed 
% tightly for every $n$, contrasting other work which only supports sufficiently large values of $n$. 
% For a subset of these law-invariant, coherent risk measures we coversely illustrate how given some $\varepsilon$ 
% the risk measure can be configured to satisfy \cref{eq:point-confidence-guarantee}. This subset includes the well known \emph{conditional value-at-risk}
% and the $\phi$-divergences considered in \cite{Duchi2021b,Lam2019}. 



% We suggest an alternative proxy-cost to \cref{eq:empirical-risk-minimization}, motivated by finite-sample statistics, 
% that requires less tuning. Specifically we minimize
% \begin{equation} \label{eq:proxy-cost}
% \begin{alignedat}{2}
%         \varphi^{\gamma}_n(\theta) \dfn (1 - \gamma) \CVAR^{\gamma}_n[\ell(\theta, \xi)] + \gamma \bar{\ell}(\theta).
% \end{alignedat}           
% \end{equation}
% where $\CVAR^{\gamma}_n$ denotes the empirical conditional value-at-risk -- also known as average value-at-risk, expected shortfall and the super-quantile \cite{Rockafellar2014} -- defined as 
% \begin{equation} % \label{eq:conditional-value-at-risk}
%      \CVAR^{\gamma}_n[\ell(\theta, \xi)] \dfn \inf_{\tau} \left\{ \tau + \sum_{i=1}^{n} \frac{[\ell(\theta, \xi_i) - \tau]_+}{n(1-\gamma)} \right\}.
% \end{equation}
% Here $\bar{\ell} \colon \Re^{n_\theta} \to \Re$ serves as the regularization term. Ideally it should be such that $\bar{\ell}(\theta) \geq \ell(\theta, \xi)$ for almost every $\xi$. 
% The confidence level $\gamma \in \Re$ is not viewed as a regularization parameter in this scheme, but instead is tuned automatically based on the number of samples and 
% a confidence level $\epsilon$ such that,
% for fixed $\theta \in \Re^{n_{\theta}}$,
% \begin{equation} \label{eq:confidence-guarantee}
%    \prob[\varphi^{\gamma}_n(\theta) \geq \E[\ell(\theta, \xi)]] \geq 1 - \varepsilon,
% \end{equation}
% where $\varepsilon$ is a tuning parameter that can be tuned a-priori independently of the size of the sample.\todoright{sensitivity?}

% Achieving guarantees akin to \cref{eq:confidence-guarantee} is also the objective of \emph{distributionally robust optimization (DRO)}. 
% Here one considers the proxy-cost 
% \begin{equation*}
%         \bar{\varphi}_n^{\varepsilon}(\theta) \dfn \max_{\mu \in \mathcal{A}} \, \int_{\xi} \ell(\theta, \xi) \di \mu(\xi),
% \end{equation*}
% with the so-called \emph{ambiguity set} $\amb$ of distributions selected such that $\prob[\mu_{\star} \in \mathcal{A}] \geq 1-\varepsilon$,
% with $\mu_{\star}$ the true distribution of $\xi$. Such sets can be based on moments \cite{Delage2010}, divergence from the empirical distribution \cite{Ben-Tal2013},
% Wasserstein distance \cite{Esfahani2018b} or other statistical hypothesis tests \cite{Bertsimas2018}. 
% If this distribution is within $\amb$ then clearly the maximum is larger than $\E[\ell(\theta, \xi)]$ 
% uniformly over $\theta$. Hence 
% \begin{equation} \label{eq:uniform-confidence-guarantee}
%         \prob[\bar{\varphi}^{\varepsilon}_n(\theta) \geq \E[\ell(\theta, \xi)], \forall \theta \in \Theta] \geq 1 - \varepsilon.
% \end{equation}
% This guarantee is somewhat stronger than \cref{eq:confidence-guarantee}, since the first holds uniformly over $\theta$ 
% while the second only holds pointwise. Nonetheless we observe that our method generalizes well, without any of the 
% conservativeness rampant in DRO. This is why, in practice, the ambiguity set $\amb$ is either calibrated 
% using bootstrapping \cite{Delage2010}, asymptotic versions of \cref{eq:uniform-confidence-guarantee} \cite{Bertsimas2018, Ben-Tal2013} or using cross-validation \cite{Esfahani2018b}. 
% These methods are either computationally expensive or lack guarantees for low sample counts -- the case where over-fitting is likely to occur. 

% This limitation of DRO is caused by the requirement that $\mu_{\star} \in \amb$. This is much stronger than requiring $\bar{\varphi}_{n}^{\epsilon}$
% upper bounds the expected risk. Hence other authors have started investigating this second problem directly, viewing $\ell(\theta, \xi)$ 
% as a scalar random variable, the mean of which should be bounded. In the asymptotic setting (i.e. for large $n$) progress has been made by \cite{Duchi2021b,Lam2019}. 
% They start with a DRO scheme based on $f$-divergence balls for $\amb$, guaranteeing \cref{eq:confidence-guarantee} in the limit $n \to \infty$. They then show that a slight 
% increase in the radius of the ball $\amb$ is sufficient to make the guarantee hold when both sides of \cref{eq:confidence-guarantee} are minimized over $\theta$. The fact 
% that these guarantees only hold for sufficiently large $n$ however means that over-fitting still occurs (cf. Figure~1(b) in \cite{Duchi2021b}). 

% In this work we similarly view $\ell(\theta, \xi)$ as a scalar random variable and compute a bound for the (scalar) mean, which
% holds for all $n$. The bound was first conceived by Anderson \cite{Anderson1969} and corresponds to 
% \cref{eq:proxy-cost}. The relation to $\CVAR$ is novel to the authors knowledge. 

% Unlike \cite{Duchi2021b,Lam2019}, investigation of the gap between \cref{eq:confidence-guarantee} and \cref{eq:uniform-confidence-guarantee}
% is left to future work. We instead provide a number of experimental results to illustrate that \cref{eq:confidence-guarantee} is often 
% sufficient for good generalization.

The rest of the paper proceeds as follows. In \cref{sec:statistical-framework} we derive mean bounds linked to law-invariant coherent risk measures through the distortion representation in \cref{sec:distortion-representation}. 
The problem of calibrating 
risk measures given $\varepsilon$ is answered in \cref{sec:calibration} and in \cref{sec:case-studies} numerical case 
studies are presented, showing the performance of these calibrated risks.

\paragraph*{Notation} Let $\Re$ denote the reals and $\eRe$ the extended reals\todo[yshift=3em]{appendix}. 
For some convex function $\phi \colon \Re^n \to \eRe$ let $\partial \phi$ denote the subgradient\todo{appendix}, $\phi^*$ the convex conjugate and
$\dom \phi$ its domain. 
For a set $\set{X}$ let $\iota_{\set{X}}(x) = 0$ if $x \in \set{X}$ and $+\infty$ otherwise be the indicator function\todo[yshift=2em]{appendix} of $\set{X}$ and $\ri \set{X}$ the relative interior\todo[yshift=1em]{appendix}. 
For integers $a, b$ let $[a, b] = \{a, \dots, b\}$ and $[b] = \{1, \dots, b\}$. Let $[x]_+ = \max(0, x)$. 
For real vectors $x, y \in \Re^n$ we use $\< x, y\>$ to denote the Euclidean inner product and $\one_n \in \Re^n$ is the vector of all ones. 
For a cone $\set{K}$ let $\set{K}^* \dfn \{y \colon \<x, y\> \geq 0, \forall x \in \set{K}\}$ denote its dual\todo{appendix} and $\set{K}^\circ \dfn -\set{K}^*$ its polar cone. 
Let $\Pi^n$ denote the permutations of $[n]$ (i.e. all bijections $[n] \to [n]$). We write $\pi x = (x_{\pi(1)}, \dots, x_{\pi(n)})$
for any $x \in \Re^n$ and similarly for sets $\pi \set{X} = \{\pi x \colon x \in \set{X}\}$. 
Similarly let $\Pi^n y = \{\pi y \colon \pi \in \Pi^n\}$ denote the orbit of $y$ under $\Pi^n$.
Let $\Re^n_{\uparrow} \dfn \{x \colon x_{1} \leq x_2 \leq \dots \leq x_n\}$ 
denote the monotone cone and $(\M^n)^\circ$ its polar (cf. \cref{prop:dual-monotone-cone}). Let $\Delta^n \dfn \{\mu \colon \sum_{i=1}^n \mu_i = 1, \mu_i \geq 0, i \in [n]\}$
denote the probability simplex.
For a vector $x \in \Re^n$ let $x_{(1)} \leq x_{(2)} \leq \dots \leq x_{(n)}$ be the increasing permutation of the elements of $x$
with $x_{\uparrow} = (x_{(1)}, x_{(2)}, \dots, x_{(n)})$. For $x, y \in \Re^n$ let $x \slt y$ denote that $y$ majorizes $x$ \cite{Marshall2011}\todo{appendix}. For sets let $A + B \dfn \{a + b \colon a \in \set{A}, b \in \set{B}\}$
denote the Minkowski sum and for $x \in \Re^n$, $x + B = \{x\} + B$. For random variables $X, Y$ we write $X \deq Y$ to say $X$ is identically distributed to $Y$. Let $\esssup[X]$ 
denote the essential supremum. Let $X \deq \mathcal{U}(\ell, u)$ imply $X$ is uniformly distributed over $[\ell, u]$ and $X \deq \mathcal{N}(\mu, \Sigma)$ 
denote $X$ is normally distributed with mean $\mu$ and covariance $\Sigma$. 