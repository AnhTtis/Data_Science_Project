\section{Case Studies} \label{sec:case-studies} 

To illustrate the validity and potential of our method we provide several simple case studies. These 
are convex for maximum interpretability, as in the non-convex case the presence of local optima might affect the performance in
unexpected ways. Nonetheless our method is also applicable in non-convex settings, where stochastic gradient
descent methods are applicable (cf. \cite{Mehta2022} for simple distortions and \cite{Chouzenoux2019} for divergences).
Such cases are deferred to future work. Solution of \cref{eq:robustified-erm} for the convex case is detailed in \cref{app:conic-risk}.

Throughout this section we will consider the following risk measures: \emph{(SAA)} takes $\rho(X) = \sum_{i=1}^{n} X_i/n$; 
\emph{($\bar{\CVAR}$)} takes $\rho = \bar{\CVAR}^n_{\gamma}$ with $\gamma$ determined such that $\rho$ is $\varepsilon$-calibrated
using \cref{rem:order-statistics-bound} and \cite{Moscovich2020} or using \cref{eq:asymptotic} when $n \geq 100$; \emph{(TV)} and \emph{(KL)} take $\rho$ 
as a \emph{total-variation} and a \emph{Kullback-Leibler} $\phi$-divergence risk respectively and both are $\varepsilon$-calibrated as in \cref{thm:divergence-bound} with $\beta = 0.005$ and $m = 10\,000$. 

\subsection{Newsvendor}
We begin with a toy problem to illustrate the behavior of our method in low-sample settings. 
Let $\xi \colon \Omega \to \Re$ be beta distributed with parameters $\alpha = 0.1$, $\beta = 0.2$, scaled by a factor $\bar{D} \dfn 100$. 
Consider a newsvendor problem \cite[\S1.2.1]{Shapiro2021}:
\begin{equation*} \label{eq:newsvendor}
    \begin{alignedat}{2}
        &\minimize_{\theta \in \Re} & \quad & \E\left[\smashunderbracket{cx + b[\xi - x]_+ +  h[x - \xi]_+}{\ell(\theta, \xi)}\right],
    \end{alignedat}
\end{equation*}
with $b = 14$, $h = 2$ amd $c = 1$. For samples $\{\xi_i\}_{i=1}^{n-1}$ with $n=20$ let $\ell_i(\theta) = \ell(\theta, \xi_i)$ 
for $i \in [n-1]$, $\ell_n(\theta) = \max\{(c - b)\theta + b \bar{D}, (c + h)\theta\}$ a robust upper bound
and $L(\theta) = (\ell_i(\theta))_{i=1}^{n}$. We then solve \cref{eq:robustified-erm} for the risk measures specified above.
Note that $\rho(L(\theta)) = \sum_{i=1}^{n-1} \ell_i(\theta)/(n-1)$ for the sample average (SAA) has no robust term.

The risks are calibrated at $\varepsilon = 0.2$. Their performance is compared over $200$ sampled data sets in \cref{fig:newsvendor-example}.
The left plot shows the actual expected cost for the minimizers of \cref{eq:robustified-erm}
The blue dashed line is the true optimum of \cref{eq:newsvendor}. See \cite[\S1.2.1]{Shapiro2021} for details on how to compute these values. 
Note how the SAA performs decently in the median, but has significantly more variance. The outliers above $240$ were omitted, the largest of which was $428.2$. 
Moreover, the right plot depicts the difference between the predicted cost, i.e. the optimum value of \cref{eq:robustified-erm}, and the true cost. 
The SAA often underestimates its true cost, while our methods overestimate it. The dashed red line depicts the behavior when taking $\amb = \Delta^n$
in \cref{eq:robustified-erm} (cf. \cite[Eq.~1.9]{Shapiro2021}). As we almost never perform worse than this robust method, this shows that our methods learn from data without being overly sensitive to the sample. 

In large sample cases we can use \cref{rem:robust-sample}. In combination with some regularization, this significantly boosts 
the performance of our method, as shown in the next examples.

\begin{figure}
    \centering
    \input{assets/boxes.tex}\vspace{-0.5em}
    \caption{Box plots showing newsvendor expected cost (left); and difference predicted cost and expected cost (right). The colored area is the \emph{inter-quartile range (IQR)}, while the whiskers show the 
    range of samples truncated to $1.5$ times the IQR. Outliers are depicted as diamonds. The red dashed lines depict the robust performance. The blue dashed line is the optimal cost.} \label{fig:newsvendor-example}\vspace{-1.5em}
\end{figure}

\subsection{Regression}
Let $T_k \colon \Re \to \Re$ denote the Chebychev polynomials of the first kind for $k \geq 0$ and $f_d(x) = (T_k(x))_{k=0}^{d} \in \Re^{d+1}$ 
a feature vector. Consider a lasso regression problem:
\begin{equation} \label{eq:regression}
    \begin{alignedat}{2}
        &\minimize_{\theta \in \Re^{d+1}} &\qquad & \E\left[(\<f_d(X), \theta\> - Y)^2\right] + \lambda \nrm{\theta}_1.
    \end{alignedat}    
\end{equation}
Assuming access to samples $\{(X_i, Y_i)\}_{i=1}^n$, we replace the expectation as in \cref{eq:robustified-erm} with the risks described above,
where $L(\theta) = ((\<f_d(X_i), \theta\> - Y_i)^2)_{i=1}^{n}$. 
Note that we do not include a robust term, approximating it with the largest sample as discussed in \cref{rem:robust-sample}.

For the parameters $\theta_\star = (0, 0, 0.2, 0.5, 1.0)$ the data is generated as $Y_i = \<f_4(X_i), \theta_\star\> + E_i$ with $X_i \deq \mathcal{U}(-1, 1)$ 
and $E_i \deq \mathcal{U}(-0.2, 0.2)$ for $i \in [n]$. We over-parametrize the problem, taking $d = 20$, to illustrate the regularizing effect of our method.
A fit is plotted for $\lambda = 0.2$ and $n = 50$ in \cref{fig:regression-example}. Note how the risk measures all perform similarly, 
while SAA has a worse fit.

\begin{figure}
    \centering
    \input{assets/regression.tex}
    \caption{Regression using $n = 50$ samples with $d=20$ and $\lambda = 0.2$ for different risk measures.} \label{fig:regression-example}
    \vspace{-0.5em}
\end{figure}

The methods are evaluated quantitatively by sampling an additional $100\,000$ data points and computing a sample approximation of the cost of \cref{eq:regression}.
The resulting performance is compared for several tunings in \cref{tab:regression-quantitative}, where any parameters not mentioned are kept as specified above. 
It is of note that our methods are significantly less sensitive to tuning parameters compared to the SAA. In fact, our methods outperform SAA for all tunings investigated. 


{\setlength{\tabcolsep}{4pt}\tiny
\begin{table}
   \input{assets/regression-table.tex}
   \centering
   \caption{Regression generalization performance for various tuning parameters. Values are reported as \emph{mean (standard deviation $\cdot\, 10^{3}$)}
   computed over $10$ training sets. The same $10$ sets were used for every selection of parameters and method. Note that $\varepsilon$ 
   does not affect SAA.} \label{tab:regression-quantitative} \vspace{-1.5em}
\end{table}
}


\subsection{Support Vector Machines}
Consider a classification problem with $X \deq \mathcal{N}(0, I_2)$ normally distributed and $Y = 1$ if $X_{1} X_{2} \geq 0$ 
and $Y = -1$ otherwise%\footnote{Inspired by \url{https://scikit-learn.org/stable/auto_examples/svm/plot_svm_nonlinear.html}}
. A \emph{Support Vector Machine (SVM)}
solves:
\begin{equation*}
    \begin{alignedat}{2}
        &\minimize_{(f, b) \in \set{H} \times \Re} &\qquad & \frac{1}{2} \nrm{f}_{\set{H}}^2 + \lambda\E\left[ 1 - Y (f(X) - b) \right]_+
    \end{alignedat}
\end{equation*}
with $\lambda > 0$ and $\set{H}$ some \emph{reproducing kernel Hilbert Space (RKHS)} \cite[Def.~2.9]{Scholkopf2002}. The 
resulting classifier is then given by $\mathrm{sign}(f(X) - b)$. Henceforth $\set{H}$ is the RKHS associated with the \emph{radial basis function} kernel 
\cite[\S2.3]{Scholkopf2002} with some standard deviation $\sigma$.
Solving the primal problem is difficult for two reasons: \emph{(i)} 
the true expectation is often unknown; \emph{(ii)} optimizing over the infinite dimensional $\set{H}$ is intractable in general. 
We resolve \emph{(i)} by replacing the expectation with a risk measure $\rho$, calibrated as in \cref{sec:calibration} and \emph{(ii)} through the usual duality trick \cite[\S7.4]{Scholkopf2002}.
Details are deferred to \cref{app:svm}. 

For three risks of the ones described above -- SAA, TV and $\bar{\CVAR}$ -- the associated ambiguity set is polyhedral and the dual problem is a QP. 
The sample average -- C-SVC in \cite[\S7.5]{Scholkopf2002} -- is the usual choice. We illustrate how our calibrated risks perform better. 

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{assets/svm.tex}
    \caption{SVM classifiers trained using $n = 250$ samples with $\sigma=0.25$ and $\lambda = 10^4$ for different risks. The red and blue markers are samples for $Y=1$ and $-1$ respectively. 
    The line is the decision boundary and the color axis depicts $f(X) - b$.} \label{fig:svm-example}
    \vspace{-1em}
\end{figure}

First observe \cref{fig:svm-example}, where the three classifiers produced by the three risk measures above are depicted. Note how both TV and $\bar{\CVAR}$ perform similarly 
and both visibly better than the usual SAA. Quantitative performance is compared through the fraction of incorrectly labeled samples in a test set of $10^5$ samples, which 
we refer to as the misclassification rate. The performance is compared for several tunings in \cref{tab:svm-quantitative}, where any parameters not mentioned are kept as specified above. 
It is of note that our methods are significantly less sensitive to tuning parameters compared to the SAA.

{\setlength{\tabcolsep}{4pt}
\begin{table}
   \input{assets/svm-table.tex}
   \centering
   \caption{SVM misclassification rates for various tuning parameters. Values are reported as \emph{mean (standard deviation)}
   computed over $10$ training sets. The same $10$ sets were used for every selection of parameters and method. Note that $\varepsilon$ 
   does not affect SAA.} \label{tab:svm-quantitative} \vspace{-2em}
\end{table}
}

We can also examine the effect of varying the sample count $n$. For each such value we train the classifiers, again using the parameters used to produce \cref{fig:svm-example}, 
for $30$ training sets. The resulting misclassification rates are depicted in \cref{fig:svm-complexity}. Again note that $\bar{\CVAR}$ and TV both outperform SAA. 

\begin{figure}
    \centering
    \input{assets/svm-complexity.tex}\vspace{-0.5em}
    \caption{SVM misclassification rates for varying sample counts $n$. 
    The center line depicts the mean, while the colored region depicts the empirical $0.2$-confidence interval.} \label{fig:svm-complexity} \vspace{-2em}
\end{figure}