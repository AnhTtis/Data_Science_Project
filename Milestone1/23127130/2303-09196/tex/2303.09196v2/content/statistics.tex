\section{Statistical Framework} \label{sec:statistics}
The analysis of this section investigates upper bounds for the mean of a scalar \emph{random variable (rv)} $Z \colon \Omega \to \Re$, defined on some probability space $(\Omega, \F, \prob)$. 
These findings remain applicable to the previous section, when considering $Z = \ell(\theta, \xi)$ for fixed $\theta$. Let $F(z) = \prob[Z \leq z]$ 
denote the \emph{cumulative distribution function (cdf)} of $Z$. 
We assume access to iid samples $Z_{1}, \dots, Z_{n-1}$ and an upper bound denoted as $Z_{n} = \esssup[Z]$ for notational convenience, 
which satisfies $F(Z_{n}) = 1$. 

We then introduce the \emph{coverages}
\begin{equation} \label{eq:coverages}
    W_i = F(Z_{(i)}) - F(Z_{(i-1)}), \quad \forall i \in [n],
\end{equation}
where $-\infty = Z_{(0)} \leq Z_{(1)} \leq Z_{(2)} \leq \dots \leq Z_{(n)}$ denotes
an increasing permutation of $Z_1, \dots, Z_{n}$ called the \emph{order statistics} of $Z$ and
with $Z_{(0)}$ added for convenience. 

% We study the distribution of \cref{eq:coverages}. We do so through the following construction. 
% Let $U_{(1)} \leq  \dots \leq U_{(n-1)}$ denote the \emph{uniform order statistics} and $U_{(n)} = 1$ 
% for notational convenience. Then $Z_{(i)} \deq F^{-1}(U_{(i)})$ by \cite[Eq.~1.2.4]{Reiss1989}. Therefore 
% \begin{equation} \label{eq:order-statistics-construction}
%     W_i \deq F(F^{-1}(U_{(i)})) - F(F^{-1}(U_{(i-1)})), \quad \forall i \in [n].
% \end{equation}
% When $F$ is continuous $F(F^{-1}(U_{(i)})) = U_{(i)}$. So the coverages would be distributed like differences between 
% uniform order statistics, which corresponds to them being uniformly distributed over $\Delta^n$ \cite[\S6.4]{David2003}.
% Whenever $F$ has discontinuities however, things are more involved. In that case, we invert the linear relation in \cref{eq:order-statistics-construction}, resulting in:
% \begin{equation} \label{eq:order-statistics-construction:b}
%     \ssum_{i=1}^k W_i \deq F(F^{-1}(U_{(i)})), \quad \forall k \in [n-1].
% \end{equation}
% and $\sum_{i=1}^n W_i = U_{(n)} = 1$. We introduce $\nu$ with $\nu_i = U_{(i)} - U_{(i-1)}$ for $i \in [n]$, which (i) 
% is uniformly distributed over $\Delta^n$ and (ii) satisfies $\sum_{i=1}^{k} \nu_i = U_{(k)}$ for $k \in [n]$. 
% Therefore 
% \begin{equation*}
%     \ssum_{i=1}^k W_i \deq F(F^{-1}(\ssum_{i=1}^{k} \nu_i)), \quad \forall k \in [n-1]
% \end{equation*}
% and $\sum_{i=1}^n W_i = \ssum_{i=1}^{k} \nu_i = 1$.
% For a discontinuous distribution we then have $F(F^{-1}(p)) \geq p$ for all $p \in (0, 1)$ \cite[Eq.~1.2.11]{Reiss1989}, 
% where the inequality is strict at points of discontinuity. Combining this with the previous display gives 
% \begin{equation} \label{eq:stoch-dominance}
%     \sum_{i=1}^{k} \nu_i \leq \sum_{i=1}^k W_i, \, \forall k \in [n-1] \text{ and } \sum_{i=1}^n \nu_i = \sum_{i=1}^{n} W_i.
% \end{equation}
% This corresponds to a conic inequality under $\Re^n_{\uparrow}$ (cf.\ \cref{prop:divergence-bound-pre}). 


% In statistics literature, it is well known that a lower bound on the cdf implies an upper bound on the expectation (cf. \cite{Anderson1969}, \cite[Eq.~1.A.5]{Shorack2009}).
% Similarly, the event in \cref{cor:coverages-stochastic-order} implies an upper bound on the expectation as well. The following is then the main result of this paper:

The coverages will be used to bound the expectation. To do so, we require the following cone in $\Re^n$:
\begin{equation} \label{eq:majorization-cone}
    \major{n} = \left\{ x \colon \sum_{i=1}^k x_i \geq 0, \forall k \in [n-1], \sum_{i=1}^n x_i = 0 \right\}. 
\end{equation}
In \cref{prop:dual-monotone-cone} we prove that it corresponds to the polar of the monotone cone $\Re^n_{\uparrow}$. It has a history in isotonic regression \cite{Barlow1972},
majorization \cite{Steerneman1990} and also describes a stochastic order between discrete distributions \cite[p.~4]{Shaked2007} as we illustrate later.
% \begin{equation} \label{eq:majorization-cone}
%     \major{n} = \left\{ x \colon \sum_{i=1}^k x_i \geq 0, \forall k \in [n-1], \sum_{i=1}^n x_i = 0 \right\}. 
% \end{equation}

The expectation bound is then as follows:
\begin{proposition} \label{prop:mean-bound-main}
    Suppose that $\amb \subseteq \Delta^n$ is permutation invariant\footnote{%
        Specifically, for any $\pi \in \Pi^n$ and $\mu \in \amb$, $\pi \mu = (\mu_{\pi(1)}, \dots, \mu_{\pi(n)}) \in \amb$. Moreover we assume sets $\amb$ are Lebesgue measurable.}.
    Take $W$ as in \cref{eq:coverages}. 
    Then, for any rv $Z \colon \Omega \to \Re$ with iid samples $\{Z_i\}_{i=1}^{n-1}$ and $Z_n = \esssup[Z] < +\infty$, 
    \begin{equation*}
        \prob\left[ \sup_{\mu \in \amb} \, \sum_{i=1}^{n} \mu_i Z_{i} \geq \E[Z] \right] \geq \prob[W \in \amb + \major{n}]. 
    \end{equation*}
\end{proposition}
\begin{proof}
    We split up the expectation\footnote{For details on the integral notation see \cite[Eq.~17.22]{Billingsley1995}.} as follows:
    \begin{align}
        \E[Z] = \int_{-\infty}^{Z_{(n)}} z \di F(z) = \sum_{i=1}^{n} \int_{Z_{(i-1)}}^{Z_{(i)}} z \di F(z),
    \end{align}
    with $Z_{(1)} \leq \dots \leq Z_{(n-1)}$ the order statistics and $Z_{(n)} = \esssup[Z]$. 
    The first equality follows from $\prob[Z \leq Z_{(n)}] = 1$ and the second from \cite[Thm.~16.9]{Billingsley1995}. 
    For each term
    $\int_{Z_{(i-1)}}^{Z_(i)} z \di F(z) \leq Z_{(i)} \int_{Z_{(i-1)}}^{Z_(i)} \di F(z) = Z_{(i)} (F(Z_{(i)}) - F(Z_{(i-1)}))$. Hence
    $\E[Z] \leq \sum_{i=1}^{n} Z_{(i)} W_i$.

    Condition on $W \in \amb + \major{n}$.
    Then,
    \begin{equation*}
        \E[Z] \leq \sum_{i=1}^{n} W_i Z_{(i)} \leq \sup_{\mu \in \amb + \major{n}} \ssum_{i=1}^{n} \mu_i Z_{(i)}. 
    \end{equation*}
    The expression on the right can be simplified by noting that
    \begin{align*}
        {\sup}_{\mu \in \amb + \major{n}} {\ssum_{i=1}^{n}} \mu_i Z_{(i)} = {\sup}_{\mu \in \amb} \ssum_{i=1}^{n} \mu_i Z_{(i)},
    \end{align*}
    where we use $(Z_{(1)}, \dots, Z_{(n)}) \in \Re^n_{\uparrow}$, the definition of the polar cone and \cref{prop:dual-monotone-cone}, 
    which implies $\ssum_{i=1}^{n} s_i Z_{(i)} \leq 0$ for any $s \in \major{n}$ with equality for $s = 0$. 

    Finally, let $\pi \in \Pi^n$ be the permutation such that $Z_{\pi(i)} = Z_{(i)}$ for $i \in [n]$ and let $\pi^{-1}$ denote its inverse (which exists, since permutations are bijections). 
    Then
    \begin{align*}
        &\sup_{\mu \in \amb} \, \ssum_{i=1}^{n} \mu_i Z_{i} = {\displaystyle\sup_{\mu \in \amb}} \, \ssum_{i=1}^{n} \mu_{\pi^{-1}(i)} Z_{(i)} \\
        &\quad = \sup_{\pi \mu \in \amb} \, \ssum_{i=1}^{n} \mu_i Z_{(i)} \labelrel={step:permutation} {\displaystyle\sup_{\mu \in \amb}} \, \ssum_{i=1}^{n} \mu_i Z_{(i)},
    \end{align*} 
    where \ref{step:permutation} uses permutation invariance. Hence, we showed $\prob[\sup_{\mu \in \amb} \, \ssum_{i=1}^{n} \mu_i Z_{(i)} \geq \E[Z]] \geq \prob[W \in \amb + \major{n}]$.
\end{proof}

From \cref{prop:mean-bound-main}, it is clear that the distribution of $W$ is important. 
Interestingly, when $F$ is continuous, then $W$ is always uniformly distributed over $\Delta^n$ \cite[Thm.~8.7.4]{Wilks1964}. 
For general distributions however, we can still establish a type of \emph{stochastic order} between the two distributions using $\major{n}$:
\begin{lemma} \label{cor:coverages-stochastic-order}
    Take $W = (W_1, \dots, W_n) \in \Delta^n$ as in \cref{eq:coverages}. Then, for any (Lebesgue measurable) $\amb \subseteq \Delta^n$, 
    \begin{equation*}
        \prob[W \in \amb + \major{n}] \geq \prob[\nu \in \amb + \major{n}],
    \end{equation*}
    with $\nu \deq \mathrm{U}[\Delta^n]$ uniformly distributed over $\Delta^n$. 
    
    For continuous cdf we have $\prob[W \in \amb] = \prob[\nu \in \amb]$. 
\end{lemma}
\begin{proof}
    For continuous cdf we refer to \cite[Thm.~8.7.4]{Wilks1964}. For discontinuous cdf
    we first introduce a construction of the joint distribution of random vectors 
    $W'$ and $\nu'$, such that $W' \deq W$ and $\nu' \deq \nu$ (i.e., the marginals are as specified in the lemma). 
    For this construction, we show that $\nu' \in \amb + \major{n}$ implies $W' \in \amb + \major{n}$ almost surely.
    So $\prob[W' \in \amb + \major{n}] \geq \prob[\nu' \in \amb + \major{n}]$. 
    From $W' \deq W$ and $\nu' \deq \nu$ we then get the required result. 
    % This method is similar to one used in stochastic orders (cf.\ \cite[Thm.~6.B.1]{Shaked2007}).

    The construction starts by taking $U_i \deq \mathrm{U}[0, 1]$ as uniform random variables, for $i \in [n - 1]$, with 
    $U_{(1)} \leq U_{(2)} \leq \dots \leq U_{(n-1)} \leq U_{(n)} = 1$ the uniform order statistics.
    Let $\sum_{i=1}^{k} \nu_i' = U_{(k)}$ for $k \in [n]$. Then, by \cite[\S6.4]{David2003}, $\nu' \deq \mathrm{U}[\Delta^n]$ or $\nu' \deq \nu$. 
    Meanwhile, the \emph{quantile transform} \cite[Lem.~1.2.4(i)]{Reiss1989} states that $Z_i' = F^{-1}(U_i)$ has cdf $F$, where $F^{-1}$ is the quantile function of $Z$. 
    Since $F$ (and therefore $F^{-1}$) is nondecreasing, we can apply \cite[Lem.~1.2.1]{Reiss1989} to claim that $F^{-1}(U_{(i)})$ is distributed as the $i$-th order statistic $Z_{(i)}$. 
    So with $Z_{(i)}' = F^{-1}(U_{(i)})$ we take $W'$ analogously to \cref{eq:coverages}. From $Z_{(i)}' \deq Z_{(i)}$ we then have $W' \deq W$.

    Both marginals are related as, for $k \in [n]$, 
    \begin{equation} \label{eq:relationship-distributions}
        \begin{aligned}
        &\ssum_{i=1}^{k} W_i' = F(Z_{(k)}') \\
        &\quad= F(F^{-1}(U_{(k)})) = F(F^{-1}(\ssum_{i=1}^{k} \nu_i')), 
        \end{aligned}
    \end{equation}
    where the first equality follows by summing \cref{eq:coverages} for $i \in [k]$ with $W_i'$, $Z_{(i)}'$ in place of $W_i$, $Z_i$, 
    the second by construction of $Z_{(k)}'$ and the third by construction of $\nu'$.
    Note that, for general distributions, we have $F(F^{-1}(p)) \geq p$ for all $p \in [0, 1]$ \cite[Ex.~3.2]{Shorack2017}, %\cite[Eq.~1.2.11]{Reiss1989},
    with strict inequality iff $p \in (0, 1)$ is not in the range of $F$. Applying this to \cref{eq:relationship-distributions} gives
    \begin{equation} \label{eq:stoch-dominance}
        \sum_{i=1}^{k} \nu_i' \leq \sum_{i=1}^k W_i', \, \forall k \in [n-1] \text{ and } \sum_{i=1}^n \nu_i' = \sum_{i=1}^{n} W_i'.
    \end{equation}
    Observe how \cref{eq:stoch-dominance} corresponds to a conic inequality under $\major{n}$. The inequalities are strict iff 
    $\sum_{i=1}^{k} \nu_i'$ is not in the range of $F$ (i.e., it lies in a discontinuous jump of $F$). In that sense, \cref{eq:stoch-dominance} 
    models the gap between $\mathrm{U}[\Delta^n]$ and the coverages $W$. 

    To complete the proof, assume that $\nu' \in \amb + \major{n}$. By definition of the Minkowski sum, this is equivalent to there 
    being some $\mu \in \amb$ such that $\nu - \mu \in \major{n}$ or,
    $\sum_{i=1}^{k} \mu_i \leq \sum_{i=1}^{k} \nu_i'$ for $k \in [n-1]$ and $\sum_{i=1}^{n} \mu_i = \sum_{i=1}^{n} \nu_i'$ (cf.\ \cref{eq:majorization-cone}).
    Thus, from \cref{eq:stoch-dominance}, we have $\sum_{i=1}^{k} \mu_i \leq \sum_{i=1}^{k} W_i'$ for $k \in [n-1]$ and $\sum_{i=1}^{n} \mu_i = \sum_{i=1}^{n} W_i'$. 
    By definition of $\amb + \major{n}$ and \cref{eq:majorization-cone} this shows that $W' \in \amb + \major{n}$. So, by our arguments at the start of the 
    proof, we showed the required result. 
\end{proof}

% \begin{undecided}
%     To further describe the result in \cref{cor:coverages-stochastic-order} we can consider an example discontinuous distribution, with a range 
%     equal to $\rg(F) = [0, \ubar{p}) \cup [\bar{p}, 1]$. 
% \end{undecided}


\begin{figure}
    \input{assets/onesided/onesideddominated.tex}
    \caption{Lower bounds of the cumulative distribution for nonnegative $Z$ with cdf $F$. 
    The tightest lower bound supported on the samples is depicted in blue, while a feasible lower bound is depicted in red. 
    The green area is the expectation.\vspace{-1em}} \label{fig:cdf-bounds}
\end{figure}

The result in \cref{cor:coverages-stochastic-order} can be interpreted in terms of lower bounding the cdf of $Z$. To illustrate this, we introduce 
the weighted empirical cdf
\begin{equation} \label{eq:weighted-cdf}
    F_\mu^n(x) = \ssum_{i=1}^{n} \mu_i \one_{[Z_{(i)}, +\infty)},
\end{equation}
where $\mu \in \Delta^n$ and $\one_{[Z_{(i)}, +\infty)}(z) = 1$ when $z \geq Z_{(i)}$ and zero otherwise. 
Note that $W \in \{\mu\} + \major{n}$ holds iff
\begin{equation*}
    F_{\mu}^n(Z_{(i)}) = \ssum_{i=1}^{k}\mu_i  \leq \ssum_{i=1}^{k} W_i = F(Z_{(i)}), \, \forall k \in [n],
\end{equation*}
with $\sum_{i=1}^{n} \mu_i = \sum_{i=1}^{n} W_i = 1$. This relationship relates to \cref{eq:stoch-dominance} and \cref{eq:majorization-cone}
and implies that $F_{\mu}^n$ should lower bound the cdf $F$ everywhere, as depicted in \cref{fig:cdf-bounds}. This inequality between cdfs is
the usual stochastic order \cite[\S1.A.1]{Shaked2007}. 
The event $W \in \amb + \major{n}$ is then equivalent to the existence of a cdf $F_\mu^n$
with weights $\mu \in \amb$ that lower bounds the true cdf. In terms of this interpretation, \cref{prop:mean-bound-main} 
follows from the fact that a lower bound on the cdf implies an upper bound on the expectation (cf. \cite{Anderson1969}, \cite[Eq.~1.A.5]{Shorack2009}).


By combining \cref{prop:mean-bound-main} and \cref{cor:coverages-stochastic-order} we directly prove
the main contribution of this paper:
\begin{theorem} \label{cor:mean-bound-main}
    Assuming the setting of \cref{prop:mean-bound-main} and taking $\nu \deq \mathrm{U}[\Delta^n]$, then
    \begin{equation*}
        \prob\left[ \sup_{\mu \in \amb} \, \sum_{i=1}^{n} \mu_i Z_{i} \geq \E[Z] \right] \geq \prob[\nu \in \amb + \major{n}]. 
    \end{equation*}
\end{theorem}

Note that, the important requirement of $\amb$ in \cref{cor:mean-bound-main} is that it is permutation invariant and a subset of 
the probability simplex. The resulting support functions are related to \emph{law-invariant, coherent risk measures} in literature \cite[\S6.3.5]{Shapiro2021}, \cite{Bertsimas2009b}, 
where the \emph{law} in our case is a permutation of the random vector. The $\phi$-based ambiguity sets considered below are the most frequently 
studied case of such risk measures. 

\section{Calibration Problem} \label{sec:calibration}
This section considers calibrating ambiguity sets $\amb$ such that \cref{eq:calibration} holds.
To do so, we first consider the $\phi$-divergence parametrization in \cref{eq:ambiguity-phi-div} and try to upper bound 
the smallest $\alpha$ such that \cref{eq:calibration} still holds for $\amb = \amb_{\alpha}$, which we denote as $\alpha_\star$.
Later we also provide an alternative parametrization similar to the conditional value-at-risk (cf.\ \cref{cor:simple-distortion} and the discussion below).

We can use the previous result to simplify \cref{eq:calibration}. 
Note that, for a fixed $\theta$, $\ell(\theta, \xi)$ is simply a scalar random variable, which we will denote as $Z$. So we consider
\begin{equation} \label{eq:calibration-scalar}
    \inf \left\{ \alpha \colon \prob\left[ \sup_{\mu \in \amb_{\alpha}} \sum_{i=1}^{n} \mu_i Z_{i} \geq \E[Z] \right] \geq 1 - \delta, \, \forall Z \right\},
\end{equation}
where we inherit the notation from the previous section. It's solution will upper bound $\alpha_\star$.
Note that all previous results were distribution-free. They hold for all (bounded) random variables $Z$, invariant of their underlying distribution. 
As such, by using \cref{cor:mean-bound-main}, the constraint in \cref{eq:calibration-scalar} can be conservatively approximated by 
\begin{equation}\label{eq:calibration-scalar:b}
    \prob\left[ \nu \in \amb_{\alpha} + \major{n} \right] \geq 1 - \delta, \quad \text{with } \nu \deq \mathrm{U}[\Delta^n].
\end{equation}
We use this to approximate the calibration problem:
\begin{proposition} \label{prop:divergence-bound-pre}
    Let $I_{\phi}$ denote the $\phi$-divergence, with $\amb_{\alpha}$ the associated ambiguity set as in \cref{eq:ambiguity-phi-div}. 
    Let $\alpha_\star$ denote the smallest $\alpha$ such that \cref{eq:calibration} holds for $\amb_\alpha$. Then
    \begin{align} \label{eq:quantile-function}
        \alpha_\star \leq \inf \left\{ \alpha \in \Re \colon \prob\left[ I^\diamond_{\phi}(\nu) \leq \alpha \right] \geq 1-\delta \right\},
    \end{align}
    with $\nu \deq \mathrm{U}[\Delta^n]$ and, for $\phi^*(s) = \sup_{t \geq 0} \, \{ts - \phi(t)\}$, 
    \begin{equation} \label{eq:div-adj}
        I^\diamond_{\phi}(\nu) \dfn \sup_{\lambda \in \Re^n_{\uparrow}} \left\{ \ssum_{i=1}^{n} \nu_i \lambda_i - \frac{1}{n} \phi^*(\lambda_i) \right\}.
    \end{equation}
\end{proposition}
\begin{proof}
    Note that $\nu \in \amb_{\alpha} + \major{n}$ holds iff there is some $\mu \in \amb_{\alpha}$ such that 
    $\nu - \mu \in \major{n}$. Equivalently, there should exist a $\mu \in \Delta^n$, which satisfies
    $I_{\phi}(\mu, \one_n/n) \leq \alpha$ and $\nu - \mu \in \major{n}$. We already showed that 
    the smallest $\alpha$ satisfying \eqref{eq:calibration-scalar:b} upper bounds $\alpha_\star$. The left-hand side of \eqref{eq:calibration-scalar:b} equals
    \begin{equation*}
        \prob\left[ 
            \inf_{\mu \in \Delta^n} \, \left\{\ssum_{i=1}^{n} \frac{1}{n} \phi(n \mu_i) \colon \nu - \mu \in \major{n} \right\} \leq \alpha \right].
    \end{equation*}
    Since $\nu \deq \mathrm{U}[\Delta^n]$, the constraint $\nu - \mu \in \major{n}$, together with $\sum_{i=1}^{n} \nu_i = 1$ 
    implies (cf.~\cref{eq:majorization-cone}) $\sum_{i=1}^{n} \mu_i = 1$. So the infimum can be taken over $\Re^n_+$. We place the constraint inside of the cost by noting that 
    the indicator function of a polar cone equals the support function of its dual \cite[Ex.~2.26]{Beck2017}. So we can rewrite \cref{eq:calibration-scalar:b} as follows:
    \begin{align*}
        &\inf_{\mu \in \Re^n_+} \, \sup_{\lambda \in \Re^n_{\uparrow}} \, \left\{\ssum_{i=1}^{n} \frac{1}{n} \phi(n \mu_i) + \<\lambda, \nu - \mu\> \right\}, \\
        & \leq  \sup_{\lambda \in \Re^n_{\uparrow}} \, \ssum_{i=1}^{n} \inf_{t_i \geq 0} \left\{ \phi(t_i) - t_i \lambda_i \right\} / n + \lambda_i \nu_i,
    \end{align*}
    The inequality follows by weak duality, the substitution $t_i = n\mu_i$ and separability.  
    The infima inside the sum are $-\phi^*(\lambda_i)$, completing the proof by the argument preceding \cref{eq:calibration-scalar:b}.
\end{proof}

Observe that the right-hand side of \cref{eq:quantile-function} is essentially the $1-\delta$ quantile of the scalar random variable $I^\diamond_{\phi}(\nu)$. 
Unfortunately its distribution is unknown. However, we can sample from it. To do so note that $\nu \deq \mathrm{U}[\Delta^n]$ is Dirichlet distributed with parameters $(1, \dots, 1) \in \Re^n$ \cite[\S6.4]{David2003}. 
So we can sample it by either sampling from a Dirichlet distribution, or by sampling $n-1$ uniform order statistics $U_{(1)} \leq U_{(2)} \leq \dots \leq U_{(n-1)}$
and using $(U_{(1)}, U_{(2)} - U_{(1)}, \dots, 1 - U_{(n-1)})$ as a sample from $\mathrm{U}[\Delta^n]$ (cf. \cite[\S6.4]{David2003}). 
We can then evaluate $I^\diamond_{\phi}(\nu)$ by solving \cref{eq:div-adj}, which is a special case of the 
optimization problem in \cite{Best2000}. That paper presents the \emph{pool-adjacent violator (PAV)} algorithm.
It solves \cref{eq:div-adj} with complexity $\mathcal{O}(n)$. 

The fact that we can sample $I^\diamond_{\phi}(\nu)$ efficiently implies that \cref{eq:calibration-scalar} can be estimated
through data-driven means.
\begin{theorem} \label{thm:divergence-bound}
    Let $\alpha_1, \dots, \alpha_m$ denote $m$ iid samples from $I^\diamond_{\phi}(\nu)$ with $\nu \deq \mathrm{U}[\Delta^n]$ and 
    $\alpha_{(1)} \leq \dots \leq \alpha_{(m)}$ the associated order statistics. Then, for any $\delta \in [0, 1]$ 
    and $k \in [m]$, 
    \begin{equation} \label{eq:confidence-upper-bound}
        \prob\left[ \alpha_\star \leq \alpha_{(k)} \right] \geq 1 - \beta
    \end{equation}
    with $\alpha_\star$ as in \cref{prop:divergence-bound-pre}, and $\beta = I_{1-\delta}(k, m-k+1)$ 
    the regularized incomplete beta function (i.e., the cdf of a beta distribution) at level $1-\delta$.
\end{theorem}
\begin{proof}
    The result follows directly from \cref{prop:divergence-bound-pre} and a one-sided data-driven bound of a quantile 
    stated in \cite[\S{}G.2.2]{Meeker2017} applied to \cref{eq:quantile-function}.
    Alternatively \cref{eq:quantile-function} can be interpreted as a \emph{scenario program}, 
    for which \cite[Thm.~3.7]{Campi2018} holds.
\end{proof}

In practice, the user would select $m$ (the number of samples from $I^\diamond_{\phi}(\nu)$ computed using PAV).
A larger value gives a tighter upper bound for $\alpha_\star$ at the cost of additional computation time. The confidence level is then determined 
by fixing some $\beta \in [0, 1]$ and then finding the smallest $k$ such that $I_{1-\delta}(k, m-k+1) \leq \beta$. 
Such a $k$ is determined with a scalar root finder\footnote{We use \texttt{brentq} as implemented in \texttt{scipy 1.10.0}\label{foot:brentq}}.

We can establish a connection with other results in literature, through the following corollary of \cref{cor:mean-bound-main}:
\begin{corollary} \label{cor:simple-distortion}
    Let $\mu \in \Re^n_{\uparrow} \cap \Delta^n$. Then, for uniform order statistics $U_{(1)} \leq U_{(2)} \leq \dots \leq U_{(n-1)} \leq U_{(n)} = 1$
    and $Z_{(1)} \leq Z_{(2)} \leq \dots Z_{(n-1)}$ and $Z_{(n)} = \esssup[Z]$ the order statistics of rv $Z \colon \Omega \to \Re$,
    \begin{equation} \label{eq:distortion-confidence}
        \prob\left[ \sum_{i=1}^{n} \mu_i Z_{(i)} \geq \E[Z] \right] \geq \prob\left[ \sum_{i=1}^{k} \mu_i \leq U_{(k)}, \, \forall k \in [n] \right].
    \end{equation}
\end{corollary}
\begin{proof}
    Using \cite{Bertsimas2009b} gives $\sum_{i=1}^{n} \mu_i Z_{(i)} = \sup_{\mu \in \amb} \, \sum_{i=1}^{n} \mu_i Z_{i}$,
    with $\amb = \hull{\Pi^n \mu}$ the convex hull of all permutations of $\mu$.
    Invoking \cref{cor:mean-bound-main} then gives $\prob\left[ \sum_{i=1}^{n} \mu_i Z_{(i)} \geq \E[Z] \right] \geq \prob[\nu \in \amb + \major{n}]$,
    with $\nu$ distributed according to $\mathrm{U}[\Delta^n]$. 
    Assume that  $\sum_{i=1}^{k} \mu_i \leq U_{(k)}$ for all $k \in [n-1]$.
    Noting that $\sum_{i=1}^{k} \nu_i \deq U_{(k)}$ by \cite[\S6.4]{David2003}
    and using \cref{eq:majorization-cone}, this implies that 
    $\nu - \mu \in \major{n}$ and, since $\mu \in \amb$, $\nu \in \amb + \major{n}$. 
    So $\prob[\nu \in \amb + \major{n}] \geq \prob[\sum_{i=1}^{k} \mu_i \leq U_{(k)}, \, \forall k \in [n]]$.
\end{proof}
 
Evaluating the right-hand side of \cref{eq:distortion-confidence} was studied in the context of 
lower bounding the cdf of a random variable (cf.\ \cite{Moscovich2020} for an efficient and numerically stable algorithm). 

The expression $\sum_{i=1}^{n} \mu_i Z_{(i)}$ is called a \emph{distortion risk} \cite{Bertsimas2009b},
which is a convex function of $(Z_1, \dots, Z_n)$ iff $\mu \in \Re^n_{\uparrow}$. Since the uniform order statistics form 
a uniform distribution over a convex set (i.e., $\Re^n_{\uparrow} \cap [0, 1]^n$), their density is quasi-concave \cite[Ex.~4.10]{Shapiro2021}. 
Hence we can leverage \cite[Ex.~4.17, Cor.~4.42]{Shapiro2021} to claim that the calibration problem
\begin{equation*}
    \inf_{\mu \in \Re^n_{\uparrow} \cap \Delta^n} \, \left\{\varphi(\mu) \colon \prob\left[\sum_{i=1}^k \mu_i \leq U_{(k)}, \, \forall k \in [n]\right] \geq 1 - \delta \right\}
\end{equation*}
is a convex problem, whenever $\varphi$ is convex. We will study the choice of $\varphi$, 
which should measure the size of $\amb$, in future work as well as the tractability of the calibration problem. 
For now, we instead consider a specific parametrization:
\begin{equation} \label{eq:cvar-weights}
    \mu^{(\gamma)}\dfn \left(0, \dots, 0, \tfrac{\lceil (n-1)\gamma \rceil}{n-1} - \gamma, \tfrac{1}{n-1}, \dots, \tfrac{1}{n-1}, \gamma\right),
\end{equation}
with $\gamma \geq 1/(n-1)$, such that $\mu \in \Delta^n \cap \Re_\uparrow^n$ holds. Then 
\begin{equation} \label{eq:biased-cvar}
    \sum_{i=1}^{n} \mu_i^{(\gamma)} Z_{(i)} =  \left( \tfrac{d}{n-1} - \gamma \right) Z_{(d)} + \sum_{i=d+1}^{n-1} \tfrac{Z_{(i)}}{n-1} + \gamma Z_{(n)},
\end{equation}
with $d = \lceil (n-1)\gamma\rceil$. The final expression is a well-known bound for the expectation of $Z$ due to Anderson \cite{Anderson1969}.
The re-interpretation in terms of a distortion risk is novel to the authors' knowledge. In \ilpub{the technical report \cite[App.~A-B]{TR}}\ilarxiv{\cref{app:cvar}}
we also show that the final expression is an affine transformation of the well-known conditional value-at-risk. Hence we denote it as $\bar{\CVAR}$ 
in the experiments. The value of $\gamma$ in the context of \cite{Anderson1969} is usually determined using an 
asymptotic bound (cf.\ \cite[Thm. 11.6.2]{Wilks1964}). We find an accurate value by solving:
\begin{equation} \label{eq:calibration-cvar}
    \inf_{\gamma \geq 1/(n-1)} \, \left\{\gamma \colon \prob\left[ \sum_{i=1}^{k} \mu_i^{(\gamma)} \leq U_{(k)}, \, \forall k \in [n] \right] \geq 1- \delta\right\},
\end{equation}
for a user-specified $\delta \in [0, 1]$ with a scalar root finder\footref{foot:brentq}.
The probability is evaluated numerically using \cite{Moscovich2020}. The calibration problem \cref{eq:calibration-cvar} in \cite{Anderson1969}
is interpreted as moving the empirical cdf down as little as possible, while still guaranteeing that it lower bounds the true cdf with high 
probability. It is also comparable to producing the tightest mean bound as in the calibration problem \cref{eq:calibration-scalar}. 


% \begin{align*}
%     &\inf \left\{ \alpha \in \Re \colon \prob\left[ \max_{\mu \in \amb_{\alpha}} \sum_{i=1}^{n} \mu_i Z_{(i)} \geq \E[Z] \right] \geq 1 - \delta \right\} \\
%     &\quad \leq \inf \left\{ \alpha \in \Re \colon \prob\left[ \mathrm{U}[\Delta^n] \in \amb_{\alpha} + \major{n} \right] \geq 1 - \delta \right\} \\
%     &\quad \leq \inf \left\{ \alpha \in \Re \colon \prob\left[  \right] \right\}
% \end{align*}



% This equivalent characterization can be used to show:
% \begin{proposition} \label{prop:atomic-coverage-result}
%     Following the notation of \cref{cor:coverages-stochastic-order}: 
%     \begin{equation*}
%         \prob\left[ \sum_{i=1}^{n} \mu_i Z_{(i)} \geq \E[Z]\right] \geq \prob[D \in \mu + \major{n}].
%     \end{equation*}
% \end{proposition}
% \begin{proof}
%     We split up the expectation as follows:
%     \begin{align}
%         \E[Z] = \int_{-\infty}^{Z_{(n)}} Z \di F(Z) = \sum_{i=1}^{n} \int_{Z_{(i-1)}}^{Z_{(i)}} Z \di F(Z).
%     \end{align}
%     The first equality follows from $\prob[Z \leq Z_{(n)}] = 1$ and the second from \cite[Thm.~16.9]{Billingsley1995}. 
%     For each term
%     $\int_{Z_{(i-1)}}^{Z_(i)} Z \di F(Z) \leq Z_{(i)} \int_{Z_{(i-1)}}^{Z_(i)} \di F(Z) = Z_{(i)} (F(Z_{(i)}) - F(Z_{(i-1)}))$. Hence
%     $\E[Z] \leq \sum_{i=1}^{n} Z_{(i)} W_i$.
%     Note that $W \in \major{n} + \mu$ has greater probability that $D \in \major{n} + \mu$ by \cref{cor:coverages-stochastic-order}. 
%     The first implies that $\<W, x\> \leq \<\mu, x\>$ for any $x \in \Re^{n}_{\uparrow}$ by definition of the polar cone.
%     So $\E[Z] \leq \sum_{i=1}^{n} Z_{(i)} W_i \leq \sum_{i=1}^{n} Z_{(i)} \mu_i$ holds with greater probability than $D \in \major{n} + \mu$. 
% \end{proof}

% We can interpret \cref{prop:atomic-coverage-result} in terms of cdf bounds. In \cref{fig:cdf-bounds}, three cdfs are shown. 
% The true cdf, the tightest lower bound supported on the sample with expectation $\sum_{i=1}^{n} W_i Z_{(i)}$ and a 
% lower bound with expectation $\sum_{i=1}^{n} \mu_i Z_{(i)}$. These expectations are ordered in reverse since 
% a lower bound on a cdf implies an upper bound on the associated expectation, which equals the area above the cdf (cf.\ \cite[Eq.~21.9]{Billingsley1995}). 
% The probability that the cdf with weights $\mu_i$ lower bounds the true cdf has been studied a lot in literature, usually in terms of 
% $u_k = \sum_{i=1}^{k} \mu_i$ as in \cref{prop:cdf-stochastic-order}. Its link to uniform order statistics is well known\todo{cite}. Our re-characterization
% in terms of $\major{n}$ will be useful in the next section. 



% The event $W \in \major{n} + \mu$ (which is equivalent to $\sum_{i=1}^{k} \mu_i \leq F(Z_{(k)})$ as shown above)
% is of crucial importance. To make this explicit we introduce
% \begin{equation} \label{eq:weighted-cdf}
%     F_\mu(x) = \sum_{i=1}^{n} \mu_i \one_{[Z_{(i)}, +\infty)},
% \end{equation}
% where $\mu \in \Delta^n$ and $\one_{[Z_{(i)}, +\infty)}(z) = 1$ when $z \geq Z_{(i)}$ and zero otherwise.
% The mean associated with this cdf is $\ssum_{i=1}^{n} \mu_i Z_{(i)}$. We depict it in \cref{fig:cdf-bounds}, together with the true cdf $F$ and 
% $F_W$, which is the tightest approximation of $F$ using a discrete cdf of the form \cref{eq:weighted-cdf}. 


% Following the proof of \cref{cor:coverages-stochastic-order} shows that $W \in \major{n} + \mu$ implies $F_W \geq F_{\mu}$. Since 
% the expectation is related to the area above the cdf (cf.\ \cite[Eq.~21.9]{Billingsley1995}), this implies that $\E[Z] \leq \sum_{i=1}^{n} \mu_i Z_{(i)}$
% as can be inferred from \cref{fig:cdf-bounds}. We make this argument formal in a different way below: 
% \begin{proposition}
%     Take $W = (W_1, \dots, W_n) \in \Delta^n$ as in \cref{eq:coverages} and $\mu \in \Delta^n$. Then 
%     \begin{equation*}
%         W \in \major{n} + \mu \quad \Rightarrow \quad \sum_{i=1}^{n} \mu_i Z_{(i)} \geq \E[Z]. 
%     \end{equation*}
% \end{proposition}
% \begin{proof}
%     We split up the expectation as follows:
%     \begin{align}
%         \E[Z] = \int_{-\infty}^{Z_{(n)}} Z \di F(Z) = \sum_{i=1}^{n} \int_{Z_{(i-1)}}^{Z_{(i)}} Z \di F(Z).
%     \end{align}
%     The first equality follows from $\prob[Z \leq Z_{(n)}] = 1$ and the second from \cite[Thm.~16.9]{Billingsley1995}. 
%     For each term
%     $\int_{Z_{(i-1)}}^{Z_(i)} Z \di F(Z) \leq Z_{(i)} \int_{Z_{(i-1)}}^{Z_(i)} \di F(Z) = Z_{(i)} (F(Z_{(i)}) - F(Z_{(i-1)}))$. Hence
%     $\E[Z] \leq \sum_{i=1}^{n} Z_{(i)} W_i$.
%     Note that $W \in \major{n} + \mu$ implies that $\<\mu, x\> \geq \<W, x\>$ for any $x \in \Re^{n}_{\uparrow}$ by definition of the polar cone.
%     So $\E[Z] \leq \sum_{i=1}^{n} Z_{(i)} W_i \leq \sum_{i=1}^{n} Z_{(i)} \mu_i$. 
% \end{proof}




% Our core idea uses \emph{stochastic orders} \cite[Eq.~1.A.5]{Shaked2007}:
% \begin{lemma} \label{lem:stochastic-orders}
%     Given random variables $Z, Z'$ such that $\prob[Z \leq z] \geq \prob[Z' \leq z]$ for all $z \in \Re$.
%     Then $\E[Z] \leq \E[Z']$. 
% \end{lemma}
% Here the inequality in terms of the \emph{cumulative distribution function (cdf)} $F(z) = \prob[Z \leq z]$ is the definition of the \emph{usual stochastic order}. 
% Clearly \cref{lem:stochastic-orders} can be used to construct an upper bound of the expectation, whenever we can find a
% random variable $Z'$ with a cdf that lower bounds $F$.  We assume access to iid samples $Z_{1}, \dots, Z_{n-1}$ and an upper bound denoted as $Z_{n} = \esssup[Z]$ for notational convenience, 
% which satisfies $\prob[Z \leq Z_{n}] = 1$. Also suppose $Z_{1}^{n} = (Z_1, \dots, Z_{n-1}, Z_n) \in \Re^{n}$. Now consider: 
% \begin{equation} \label{eq:weighted-lower-bound}
%     F_{\mu}(z) = \sum_{i=1}^{n} \mu_i \one_{[Z_{(i)}, +\infty)}(z),
% \end{equation}
% where $\mu \in \Delta^n$ and $\one_{[z_{(i)}, +\infty)}(z) = 1$ when $z \geq z_{(i)}$ and zero otherwise. A feasible lower bound of the type $F_\mu$ 
% is depicted in \cref{fig:cdf-bounds}. From this figure we can conclude that $F_{\mu} \leq F$ ifftodo{proof?}
% \begin{equation} \label{eq:cdf-lower-bound}
%     \ssum_{i=1}^{k} \mu_i \leq F(Z_{(i)}) \quad \text{for all } k \in [n-1].
% \end{equation}
%  The expectation associated with $F_\mu$ is $\sum_{i=1}^{n} \mu_i Z_{(i)}$. 
% So we can apply \cref{lem:stochastic-orders} to conclude that $\rho(Z_{1}^{n}) \geq \E[Z]$
% holds if there is some $\mu \in \amb_{\alpha}$ such that \cref{eq:cdf-lower-bound} holds. Plugging in the definition of $\amb_{\alpha}$ implies 
% this is equivalent to 
% \begin{equation*}
%     \inf_{\mu \in \Delta^{n}} \, \left\{ I_{\phi}(\mu, \one_n/n) \colon \cref{eq:cdf-lower-bound} \right\} \leq \alpha.  
% \end{equation*}

% \begin{figure}
%     \includegraphics[]{assets/onesided/onesideddominated.pdf} \centering
%     \caption{Lower bounds of the cumulative distribution. 
%     The tightest lower bound supported on the samples is depicted in blue, while a feasible lower bound is depicted in red. 
%     The green area is the expectation.\vspace{-1em}} \label{fig:cdf-bounds}
% \end{figure}

% The remainder of this section proceeds in two steps. We first investigate how to evaluate the probability that \cref{eq:cdf-lower-bound} holds for a single
% vector $\mu_i$. Then, we show how to check whether there is some $\mu \in \amb_{\alpha}$ such that \cref{eq:cdf-lower-bound} holds with high probability. 

% \subsection{Quantile Transform}

% \subsection{Convex Duality}


% We assume access to iid samples $Z_{1}, \dots, Z_{n-1}$ and an upper bound denoted as $Z_{n} = \esssup[Z]$ for notational convenience, 
% which satisfies $\prob[Z \leq Z_{n}] = 1$. Also suppose $Z_{1}^{n} = (Z_1, \dots, Z_{n-1}, Z_n) \in \Re^{n}$. The core of our result relies on the following:
% \begin{align}
%     \E[Z] = \int_{-\infty}^{Z_{(n)}} Z \di F(Z) = \sum_{i=1}^{n} \int_{Z_{(i-1)}}^{Z_{(i)}} Z \di F(Z),
% \end{align}
% where $F(z) = \prob[Z \leq z]$ denotes the \emph{cumulative distribution function} of $Z$ and $0 = Z_{(0)} \leq Z_{(1)} \leq Z_{(2)} \leq \dots \leq Z_{(n)}$
% are an increasing permutation of $Z_1, \dots, Z_{n}$ called the \emph{order statistics} of $Z$. 
% The first equality follows from $\prob[Z \leq Z_{(n)}] = 1$ and the second from \cite[Thm.~16.9]{Billingsley1995}. 
% For each term
% $\int_{Z_{(i-1)}}^{Z_(i)} Z \di F(Z) \leq Z_{(i)} \int_{Z_{(i-1)}}^{Z_(i)} \di F(Z) = Z_{(i)} (F(Z_{(i)}) - F(Z_{(i-1)}))$. Hence
% \begin{equation} \label{eq:scal-known-distortion-bound}
%     \E[Z] \leq \sum_{i=1}^{n} Z_{(i)} W_i,
% \end{equation}
% where $W_i = F(Z_{(i)}) - F(Z_{(i-1)})$ for all $i \in [n]$. In practice, $W_i$ depend on the true distribution through $F$, so they are unknown. 
% Instead imagine replacing them by some estimate $\mu_i$. We first answer when the associated upper bound $\sum_{i=1}^{n} Z_{(i)} \mu_i$ is still valid. 
% \begin{lemma} \label{lem:stochastic-orders}
%     Given two vectors $\nu, \mu \in \Delta^n$. Then
%     \begin{equation} \label{eq:polar-mon-cone}
%         \sum_{i=1}^{k} \nu_i \leq \sum_{i=1}^{k} \mu_i, \quad \text{for all } k \in [n]
%     \end{equation}
%     if and only if $\sum_{i=1}^{n} z_{(i)} \nu_i \geq \sum_{i=1}^{n} z_{(i)} \mu_i$ for all sequences of nondecreasing 
%     real values $z_{(1)} \leq z_{(2)} \leq \dots \leq z_{(n)}$. 
% \end{lemma}
% \begin{proof}
%     We begin by rewriting the weighted sum:
%     \begin{align}
%         &\ssum_{i=1}^{n} \nu_i z_{(i)} = \ssum_{k=1}^{n} \left( \ssum_{i=1}^{k} \nu_i - \ssum_{i=1}^{k-1} \nu_i \right) z_{(k)} \nonumber\\
%         &\,\,= z_{(n)} + \ssum_{k=1}^{n-1} \left( \ssum_{i=1}^{k} \nu_i \right) z_{(k)} - \ssum_{k=2}^{n} \left( \ssum_{i=1}^{k-1} \nu_i \right) z_{(k)} \nonumber\\
%         &\,\,= z_{(n)} - \ssum_{k=1}^{n-1}  \left( \ssum_{i=1}^{k} \nu_i \right) (z_{(k+1)} - z_{(k)}), \label{eq:sum-by-parts}
%     \end{align}
%     where we used $\sum_{i=1}^{n} \nu_i = 1$ and $\sum_{i=1}^{0} \nu_i = 0$ for the first equality. The same is true for $\ssum_{i=1}^{n} \mu_i$. 
%     The result follows by noting that $(z_{(k+1)} - z_{(k)}) \geq 0$ for any $k \in [n-1]$. So \cref{eq:polar-mon-cone} is sufficient for $\sum_{i=1}^{n} z_{(i)} \nu_i \geq \sum_{i=1}^{n} z_{(i)} \mu_i$. 
%     For the reverse inequality, simply take $z_{(1)} \leq \dots \leq z_{(n)}$ such that $z_{(k+1)} - z_{(k)} = 1$ and apply \cref{eq:sum-by-parts} to both
%     sides of $\sum_{i=1}^{n} z_{(i)} \nu_i \geq \sum_{i=1}^{n} z_{(i)} \mu_i$.
% \end{proof}

% \begin{remark}
%     The inequality \cref{eq:polar-mon-cone} relates. First note that $\sum_{i=1}^{n} \mu_i z_{(i)}$ is 
%     the mean associated with a random variable $Z$, with $\prob[Z = z_{(i)}] = \mu_i$. The same is true for $\sum_{i=1}^{n} \nu_i z_{(i)}$. 
%     Then \cref{eq:polar-mon-cone} holds iff the cdf associated with the weights $\nu_i$ lower bounds the cdf with the weights $\mu_i$ at the points $z_{(k)}$ for $k \in [n-1]$. 
%     So \cref{lem:stochastic-orders} becomes a consequence of a result in \emph{stochastic orders} (cf.\ \cite[Eq.~1.A.5]{Shaked2007}). 

%     Moreover, \cref{eq:polar-mon-cone} describes a polyhedral cone. Specifically the polar of the monotone cone $\Re^{n}_{\uparrow} \dfn \{x \colon x_{(1)} \leq \dots \leq x_{(n)}\}$. 
%     This cone relates to \emph{majorization} (cf.\ \cite[\S14]{Marshall2011}). 
% \end{remark}