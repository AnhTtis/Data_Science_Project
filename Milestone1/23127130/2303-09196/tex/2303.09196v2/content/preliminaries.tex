\section{Preliminaries}
\subsection{Monotone Cone}
Let $\Re^n_{\uparrow} \dfn \{x \in \Re^n \colon x_1 \leq x_2 \leq \dots \leq x_n\}$ 
denote the monotone cone. This cone and its polar have a history in isotonic regression \cite{Barlow1972}
and majorization \cite{Steerneman1990}. 

We show that $\major{n}$ and $\Re_\uparrow^n$ are related.
\begin{lemma} \label{prop:dual-monotone-cone}
    Let $\Re_\uparrow^n$ be the monotone cone and $\major{n}$ as in \cref{eq:majorization-cone}.
    Then $\major{n}$ is the polar of $\Re_\uparrow^n$.
\end{lemma}
\begin{proof}
    The monotone cone is polyhedral with $\Re_\uparrow^n = \{x \colon Mx \leq 0\}$ for 
    $M \in \Re^{n-1 \times n}$ with $Mx = (x_1 - x_2, x_2 - x_3, \dots, x_{n-1} - x_n)$. 
    The definition of the polar cone is thus 
    \begin{align*}
        (\Re_\uparrow^n)^\circ &= \left\{ y \in \Re^n \colon \<x, y\> \leq 0, \forall x \text{ s.t. } M x \leq 0 \right\}.
    \end{align*}
    By Farkas' lemma \cite[p.~263]{Boyd2004} we have either $Mx \leq 0$ and $\<x, y\> > 0$ or $\trans{M} \lambda = y$ and $\lambda \geq 0$. 
    So
    \begin{align*}
        (\Re_\uparrow^n)^\circ = \left\{ y \in \Re^n \colon y = \trans{M} \lambda, \lambda \geq 0 \right\}.
    \end{align*}
    Note that $\<\lambda, Mx\> = \sum_{i=1}^{n-1} \lambda_i (x_i - x_{i+1}) = \sum_{j=1}^{n} x_j (\lambda_{j} - \lambda_{j-1}) = \<\trans{M} \lambda, x\>$,
    where $\lambda_0 = \lambda_{n} = 0$. Thus $y \in (\Re_\uparrow^n)^\circ$ iff $y = \trans{M} \lambda$ for $\lambda \geq 0$. Here $y = \trans{M} \lambda$ holds iff 
    \begin{align*}
        y_j &= \lambda_j - \lambda_{j-1}, &&\forall j \in [n].  \\
        \Leftrightarrow \quad \ssum_{j=1}^k y_j &= \ssum_{j=1}^k \lambda_j - \lambda_{j-1} = \lambda_k, &&\forall k \in [n]. 
    \end{align*}
    Since $\lambda \geq 0$ we have $\sum_{j=1}^k y_j = \lambda_k \geq 0$ for $k \in [n-1]$ and $\sum_{j=1}^{n} y_j = \lambda_n = 0$. 
    These are the constraints in \cref{eq:majorization-cone}. 
\end{proof}

% In this paper, we use the polar of the monotone cone to compare discrete random variables. 
% Consider a \emph{cumulative distribution function (cdf)} of a discrete random variable, supported on $x_{1}, \dots, x_n$:
% \begin{equation} \label{eq:weighted-cdf}
%     F_\mu(x) = \sum_{i=1}^{n} \mu_i \one_{[x_{(i)}, +\infty)},
% \end{equation}
% where $\mu \in \Delta^n$ and $\one_{[x_{(i)}, +\infty)}(z) = 1$ when $z \geq z_{(i)}$ and zero otherwise.
% The polar cone $\major{n}$ then gives us the following 
% characterization of inequalities between such cdfs.
% \begin{corollary}
%     For $\mu, \nu \in \Delta^n$, let $F_{\mu}$, $F_{\nu}$ as in \cref{eq:weighted-cdf}. Then 
%     \begin{equation*}
%         F_{\mu} \leq F_{\nu} \, \Leftrightarrow \, \mu - \nu \in \major{n} \, \Leftrightarrow \, \<\mu, x_{\uparrow}\> \geq \<\nu, x_{\uparrow}\>, \, \forall x \in \Re^n.
%     \end{equation*}
% \end{corollary}
% \begin{proof}
%     Note that $F_{\mu}$ and $F_{\nu}$ only have jumps at $x_{(i)}$ for $i \in [n]$. As such $F_{\mu} \leq F_{\nu}$ iff 
%     $F_{\mu}(x_{(k)}) = \ssum_{i=1}^{k} \mu_i \leq \ssum_{i=1}^{k} \nu_i = F_{\nu}(x_{(k)})$. By \cref{prop:dual-monotone-cone},
%     this holds iff $\mu - \nu \in \major{n}$. The final equivalence holds by definition of the polar cone. 
% \end{proof}
% The usefulness of this result becomes clear by noting that $\<\mu, x_{\uparrow}\>$ is the expectation associated 
% with $F_{\mu}$. 

\begin{arxiv}
\subsection{Details on $\bar{\CVAR}$} \label{app:cvar}
We first characterize the conditional value-at-risk in terms of order statistics as a \emph{distortion risk} below.
\begin{lemma} \label{lem:cvar-rewritten}
    Consider $\CVAR_{n}^{\gamma} \colon \Re^{n} \to \Re$ as
    \begin{equation} \label{eq:cvar-definition}
        \CVAR_{n}^{\gamma}[X] = \inf_{\tau} \left\{ \tau + \frac{1}{(1-\gamma) n} \sum_{i=1}^{n} [X_i - \tau]_+ \right\},
    \end{equation}        
    for $\gamma \in [0, 1]$. Then
    \begin{equation*}
        (1 - \gamma)\CVAR_n^{\gamma}[X] = \left( \frac{d}{n} - \gamma \right) X_{(d)} + \sum_{i=d+1}^{n} \frac{X_{(i)}}{n},
    \end{equation*}
    with $d \dfn \lceil n\gamma \rceil$. So $\CVAR_n^\gamma$ is a distortion risk. 
\end{lemma}
\begin{proof}
    Consider the minimizers in the definition of $\CVAR$:
    \begin{equation*}
        \argmin_\tau \left\{ \tau + \frac{1}{(1-\gamma) n} \sum_{i=1}^{n} [X_i - \tau]_+ \right\}.
    \end{equation*} 
    By \cite[Thm.~1]{Rockafellar2000}, this set is a closed bounded interval with the left endpoint being
    \begin{align*}
        \VAR_{n}^{\gamma}[X] &\dfn \inf_x \{x \colon F_n(x) \geq \gamma\} \\
                            &= \inf_x \left\{ x \colon \sum_{i=1}^{n }\bm{1}_{(-\infty, x]}(X_i) \geq \gamma n \right\} = X_{(d)},
    \end{align*}
    with $F_n$ the empirical cdf, the definition of which we plugged in for the second equality.
    For the third equality note that the left-hand side counts the number of values $X_i$ smaller than or equal to $x$. 
    Assume 
    \begin{equation}
        X_{(d-k-1)} < X_{(d-k)} = X_{(d-k+1)} = \dots = X_{(d)},
    \end{equation}
    for $k \geq 0$. Then clearly there are at least $d = \lceil n \gamma \rceil > n\gamma$ values smaller than or equal to $X_{(d-k)}$. 
    For any $z < X_{(d-k)}$ 
    there are at most $d-k-1$ samples values than or equal. Hence 
    $\VAR_{n}^{\gamma}[X] = X_{(d-k)} = X_{(d)}$. 

    Plugging into the cost of \cref{eq:cvar-definition} gives 
    \begin{align}
        &X_{(d)} + \frac{1}{(1-\gamma) n} \sum_{i=1}^{n} [X_i - X_{(d)}]_+ \nonumber \\
        &\quad = X_{(d)} + \frac{1}{(1-\gamma) n} \sum_{i=d + 1}^{n} (X_{(i)} - X_{(d)}),\label{eq:cvar-rewritten}
    \end{align}
    where we used $X_{(i)} \leq X_{(d)}$ for $i \leq d$. The stated result follows from some 
    basic algebraic manipulation. Finally note that $d/n - \gamma = (\lceil n\gamma \rceil - n\gamma)/n \leq 1/n$. 
    So the monotonicity constraint on $\mu$ in \cref{cor:simple-distortion} is also satisfied.
\end{proof}

We can then rewrite \cref{eq:biased-cvar} as 
\begin{equation*}
    \bar{\CVAR}_n^\gamma[X] \dfn (1 - \gamma) \CVAR_{n-1}^{\gamma}[(X_{(i)})_{i=1}^{n-1}] + \gamma X_{(n)}.
\end{equation*}
The associated distortion risk has weights $\mu^{(\gamma)}$ as in \cref{eq:cvar-weights}.
So we need $\gamma \geq 1/(n-1)$ for $\mu \in \Delta^n \cap \Re_\uparrow^n$ to hold. The advantage of $\bar{\CVAR}$ 
as a well-calibrated risk measure over $\CVAR$ is that additional weight is placed on the largest sample. 
This often makes the mean bound associated with $\bar{\CVAR}$ less conservative compared to $\CVAR$ 
for the same confidence level. 
\end{arxiv}