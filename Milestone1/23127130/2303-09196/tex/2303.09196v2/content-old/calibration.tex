\section{Calibrating Risk Functionals} \label{sec:calibration}
Previously, we showed that law-invariant, coherent risk measures $\rho$ are $\varepsilon$-calibrated with $\varepsilon = \prob[W \notin \distort]$. 
For certain risk measures, numerical procedures that compute $\prob[W \in \distort]$
are available. These are \emph{distortion risks}, the distortion set of which is \emph{simple} as specified below. 
We then introduce a modified $\CVAR$ risk of this type. Its single parameter is calibrated such that 
$\prob[W \notin \distort] \leq \varepsilon$ for some $\varepsilon$. 

Another wide class of law-invariant, coherent risk measures are the $\phi$-divergences. For these, exact numerical procedures are unavailable.
Instead, efficient sampling procedures to calibrate the $\phi$-divergence at some level $\varepsilon$ are derived.

The proofs in this section are deferred to \ilarxiv{\cref{app:calibraton}}\ilpub{\cite[App.~D]{TR}}. Since our contribution is focused on calibration, we omit details related to solving \cref{eq:robustified-erm} and
refer to \ilarxiv{\cref{app:conic-risk}}\ilpub{\cite[App.~E]{TR}} instead. 
Optimizing distortion risks \cite{Bertsimas2009b, Mehta2022} and $\phi$-divergences \cite{Ben-Tal2013, Chouzenoux2019} 
is studied extensively in literature after all.

\subsection{Distortion risk} \label{sec:calibration:a}
We consider \emph{simple} distortion sets, which are generated by one vector. 
That is $\distort = \mu_\uparrow + (\M^n)^\circ$ with $\mu_{\uparrow} \in \Delta^n \cap \M^n$. \cref{fig:graphical-equivalence} depicts one such set.
Simple distortion sets correspond to \emph{distortion risk measures} 
(also called \emph{spectral risk measures} \cite[\S6.3.4]{Shapiro2021}, \cite[\S2.4.2]{Pflug2007})
which are law-invariant, coherent and \emph{comonotonic} \cite[Def.~4.1]{Bertsimas2009b}.
\begin{proposition} \label{lem:simple-distortion}
    For simple distortion sets $\distort = \mu_{\uparrow} + (\M^n)^\circ$ with $\mu \in \Delta^n$, the associated risk measure $\rho$ given by \cref{eq:distortion-representation}
    is a \emph{distortion risk measure} \cite[Def.~4.5]{Bertsimas2009b} and
    \begin{equation} \label{eq:distortion-risk}
        \rho(X) = \<\mu_{\uparrow}, X_{\uparrow}\>.
    \end{equation}
\end{proposition}

The coverage probability of simple distortion sets is expressed in terms of order statistics.

\begin{corollary} \label{lem:order-statistics-bound}
    For simple distortion sets $\distort = \mu_{\uparrow} + (\M^n)^\circ$ with $\mu \in \Delta^n$, the coverage probability 
    \begin{equation} \label{eq:order-statistics-bound}
        \prob[W \in \distort] = \prob\left[\sum_{i=1}^{k} \mu_{(i)} \leq U_{(k)}, \, \forall k \in [n-1]\right],
    \end{equation}
    where $U_{(1)} \leq U_{(2)} \leq \dots \leq U_{(n-1)}$ are order statistics associated with the uniform distribution over $[0, 1]$. 
\end{corollary}

\begin{remark} \label{rem:order-statistics-bound}
Bounds like the one in \cref{lem:order-statistics-bound} have been extensively studied due to the link with the 
\emph{uniform empirical process} and Kolmogorov-Smirnov statistics \cite[\S1]{Shorack2009}. 
An efficient method for the computation of \cref{eq:order-statistics-bound} is given in \cite{Moscovich2020}.
\end{remark}

We now present an example of a distortion risk that can be calibrated efficiently.
\begin{example} \label{ex:cvar}
    For $\gamma \in [0, 1]$, consider the following:
    \begin{equation} \label{eq:biased-cvar}
        \bar{\CVAR}^\gamma_n[X] = \left( \tfrac{d}{n-1} - \gamma \right) X_{(d)} + \sum_{i=d+1}^{n-1} \tfrac{X_{(i)}}{n-1} + \gamma X_{(n)},
    \end{equation}
    which is a \emph{distortion risk} with $d = \lceil (n-1)\gamma\rceil$ and $\gamma \geq 1/(n-1)$. 
    It is a known bound for the mean \cite{Anderson1969}, relating directly to a one-sided Kolmogorov-Smirnov
    bound. In \ilarxiv{\cref{app:calibraton}}\ilpub{\cite[App.~D]{TR}} we also establish a novel link with the conditional value-at-risk, explaining 
    the notation $\bar{\CVAR}$. 

    The exact method described in \cref{rem:order-statistics-bound} is applicable to \cref{eq:biased-cvar} and enables computing $\varepsilon$ 
    for a given $\gamma$. This dependency is monotone, with $\gamma = 0, 1$ giving $\varepsilon = 1, 0$ respectively. So
    scalar root finders\footnote{We use \texttt{brentq} as implemented in \texttt{scipy~1.10.0}} can find $\gamma$ for a given value of $\varepsilon$. 

    Alternatively, asymptotic bounds for \cref{eq:order-statistics-bound} exist in literature. For sufficiently large $n$ \cite[Thm.~11.6.2]{Wilks1964}
    \begin{equation} \label{eq:asymptotic}
            \varepsilon = e^{-2 (n-1) \gamma^2} \quad \Leftrightarrow \quad \gamma = \sqrt{\tfrac{\log(1/\varepsilon)}{2n - 2}}.
    \end{equation}    
\end{example}


% \section{Calibrating Risk Functionals} \label{sec:calibration}
% In this section we describe how to find distortion sets $\distort$ satisfying \cref{eq:confidence-distortion}. 
% To do so we consider two classes of parametric distortion sets and the associated risk measures.
% In both cases the parameter such that \cref{eq:confidence-distortion} holds for some user selected $\varepsilon$ can be determined efficiently. 

% \subsection{distortion risk} \label{sec:calibration:a}
% We first start by considering a special type of distortion set $\distort$, for which $\prob[W \in \distort]$ can be evaluated exactly. 
% Specifically we consider \emph{simple} distortion sets, which are generated by one vector. 
% That is $\distort = \{\mu\} + (\M^n)^\circ$ with $\mu \in \Delta^n$. \cref{fig:graphical-equivalence} depicts one such set.
% Simple distortion sets correspond to \emph{distortion risk measures} 
% (also called \emph{spectral risk measures} \cite[\S6.3.4]{Shapiro2021}, \cite[\S2.4.2]{Pflug2007})
% which are law-invariant, coherent and \emph{comonotonic} \cite[Def.4.1]{Bertsimas2009b}.
% \begin{lemma} \label{lem:simple-distortion}
%     For simple distortion sets $\distort = \{\mu\} + (\M^n)^\circ$ with $\mu \in \Delta^n \cap \M^n$, the associated risk measure $\rho$ given by \cref{eq:distortion-representation}
%     is a \emph{distortion risk measure} \cite[Def.~4.5]{Bertsimas2009b} and
%     \begin{equation} \label{eq:distortion-risk}
%         \rho[X] = \<\mu, X_{\uparrow}\>.
%     \end{equation}
% \end{lemma}
% \begin{proof}
%     Note that $\<\mu + s, X_{\uparrow}\> \leq \<\mu, X_{\uparrow}\>$ for all $s \in (\M^n)^\circ$ since it is the polar cone of $\Re_{\uparrow}^n$. 
%     Hence the supremum in \cref{eq:distortion-representation} is achieved at $\mu$. The characterization of $\rho$ as a distortion risk measure
%     then follows from \cite[Thm.~4.2]{Bertsimas2009b} -- keeping in mind that their risk measures are defined as $X \mapsto \rho(-X)$,
%     where $\rho$ follows our convention (cf. \cite[Thm.~2.1]{Bertsimas2009b}).
% \end{proof}

% Simple distortion sets are not only interesting from a computational point of view -- several efficient optimization algorithms exist for them \cite{Mehta2022} --
% but also from the statistical point of view. Their coverage probability can be computed exactly by expressing it in 
% terms of order statistics. 

% \begin{lemma} \label{lem:order-statistics-bound}
%     For simple distortion sets $\distort = \{\mu\} + (\M^n)^\circ$ with $\mu \in \Delta^n$, the coverage probability 
%     \begin{equation} \label{eq:order-statistics-bound}
%         \prob[W \in \distort] = \prob\left[\sum_{i=1}^{k} \mu_i \leq U_{(k)}, \, \forall k \in [n-1]\right],
%     \end{equation}
%     where $U_{(1)} \leq U_{(2)} \leq \dots \leq U_{(n-1)}$ are order statistics associated with the uniform distribution over $[0, 1]$. 
% \end{lemma}
% \begin{proof}
%     We have $W \in \{\mu\} + (\M^n)^\circ$ iff. $W - \mu \in (\M^n)^\circ$. This holds iff, for all $k \in [2, n]$ (cf. \cref{lem:dual-monotone-cone})
%     \begin{align*}
%         \sum_{i=k}^n W_i - \mu_i \leq 0 \, \Leftrightarrow \, 1 - \sum_{i=k}^n \mu_i \leq 1 - \sum_{i=k}^n W_i.
%     \end{align*}
%     Note that $W \in \Delta^n$ and $\mu \in \Delta^n$. Hence the final statement holds iff 
%     \begin{equation*}
%         \sum_{i=1}^k \mu_i \leq \sum_{i=1}^k W_i, \quad \forall k \in [n-1].
%     \end{equation*}
%     The fact that $\sum_{i=1}^k W_i \deq U_{(k)}$ follows from \cite[\S6.4]{David2003}.
% \end{proof}

% \begin{remark} \label{rem:order-statistics-bound}
% Bounds like the one in \cref{lem:order-statistics-bound} have been extensively studied due to the link with the 
% \emph{uniform empirical process} and Kolmogorov-Smirnov statistics \cite[\S1]{Shorack2009}. 
% An efficient method for the computation of \cref{eq:order-statistics-bound} is given in \cite{Moscovich2020}.
% \end{remark}

% We can characterize the ambiguity set associated with a distortion risk as follows:
% \begin{proposition} \label{prop:distortion-ambiguity}
%     Take some distortion risk $\rho$ with $\distort \dfn \dom \rho^{\diamond} = \mu + (\M^n)^\circ$ for some $\mu \in \Delta^n \cap \Re_{\uparrow}^n$. 
%     Then the ambiguity set $\amb \dfn \dom \rho^*$ is given as:
%     \begin{align*}
%         \amb = \left\{ S \nu \colon S \one_d = \one_n, \trans{\one}_n S = c, S \geq 0 \right\},
%     \end{align*}
%     with $\nu \in \Re^d$ containing the $d \leq n$ unique elements of $\mu$ and $c \in \Re^d$ the number of copies of each element of $\nu$ in $\mu$. 
% \end{proposition}
% \begin{proof}
%     This follows from \cref{prop:fundamental-equivalence-lemma} and \cref{lem:simple-distortion-characterization}.
% \end{proof}

% So we can evaluate risk measures as support functions of the polyhedral ambiguity set described above. 

% One special distortion risk measure is the \emph{conditional value-at-risk}, which we characterize 
% in terms of \cref{eq:distortion-risk} below.
% \begin{lemma} \label{lem:cvar-rewritten}
%     Consider $\CVAR_{n}^{\gamma} \colon \Re^{n} \to \Re$ as
%     \begin{equation} \label{eq:cvar-definition}
%         \CVAR_{n}^{\gamma}[X] = \inf_{\tau} \left\{ \tau + \frac{1}{(1-\gamma) n} \sum_{i=1}^{n} [X_i - \tau]_+ \right\},
%     \end{equation}        
%     for $\gamma \in [0, 1]$. Then
%     \begin{equation*}
%         (1 - \gamma)\CVAR_n^{\gamma}[X] = \left( \frac{d}{n} - \gamma \right) X_{(d)} + \sum_{i=d+1}^{n} \frac{X_{(i)}}{n},
%     \end{equation*}
%     with $d \dfn \lceil n\gamma \rceil$ is a distortion risk. 
% \end{lemma}
% \begin{proof}
%     Consider the minimizers in the definition of $\CVAR$:
%     \begin{equation*}
%         \argmin_\tau \left\{ \tau + \frac{1}{(1-\gamma) n} \sum_{i=1}^{n} [X_i - \tau]_+ \right\}.
%     \end{equation*} 
%     By \cite[Thm.~1]{Rockafellar2000}, this set is a closed bounded interval with the left endpoint being
%     \begin{align*}
%         \VAR_{n}^{\gamma}[X] &\dfn \inf_x \{x \colon F_n(x) \geq \gamma\} \\
%                             &= \inf_x \left\{ x \colon \sum_{i=1}^{n }\bm{1}_{(-\infty, x]}(X_i) \geq \gamma n \right\} = X_{(d)},
%     \end{align*}
%     with $F_n$ the empirical cdf, the definition of which we plugged in for the second equality.
%     For the third equality note that the left-hand side counts the number of values $X_i$ smaller than or equal to $x$. 
%     Assume 
%     \begin{equation}
%         X_{(d-k-1)} < X_{(d-k)} = X_{(d-k+1)} = \dots = X_{(d)},
%     \end{equation}
%     for $k \geq 0$. Then clearly there are at least $d = \lceil n \gamma \rceil > n\gamma$ values smaller than or equal to $X_{(d-k)}$. 
%     or any $z < X_{(d-k)}$ meanwhile 
%     there are at most $d-k-1$ samples values than or equal. Hence 
%     $\VAR_{n}^{\gamma}[X] = X_{(d-k)} = X_{(d)}$. 

%     Plugging into the cost of \cref{eq:cvar-definition} gives 
%     \begin{align}
%         &X_{(d)} + \frac{1}{(1-\gamma) n} \sum_{i=1}^{n} [X_i - X_{(d)}]_+ \nonumber \\
%         &\quad = X_{(d)} + \frac{1}{(1-\gamma) n} \sum_{i=d + 1}^{n} (X_{(i)} - X_{(d)}),\label{eq:cvar-rewritten}
%     \end{align}
%     where we used $X_{(i)} \leq X_{(d)}$ for $i \leq d$. The stated result follows from some 
%     basic algebraic manipulation. Finally note that $d/n - \gamma = (\lceil n\gamma \rceil - n\gamma)/n \leq 1/n$. 
%     So the monotonicity constraint on $\mu$ in \cref{lem:simple-distortion} is also satisfied.
% \end{proof}

% So $\CVAR_n^\gamma$ has a simple distortion set as in \cref{lem:simple-distortion} with 
% $\mu = (0, \dots, 0, \lceil n\gamma \rceil / n - \gamma, 1/n, \dots, 1/n)$. We can define a
% similar distortion risk that treats $Z_{n} \geq \esssup[Z]$ in $\hat{Z} = (Z_i)_{i=1}^{n}$ differently compared to the other samples. 
% \begin{equation} \label{eq:anderson-bound}
%     \bar{\CVAR}_n^\gamma[\hat{Z}] \dfn (1-\gamma) \CVAR_{n-1}^\gamma[(Z_i)_{i=1}^{n-1}] + \gamma Z_{n}.
% \end{equation}
% The associated simple distortion set in \cref{lem:simple-distortion} has
% \begin{equation} \label{eq:anderson-bound-vertex}
%     \mu = \left(0, \dots, 0, \tfrac{\lceil (n-1)\gamma \rceil}{n-1} - \gamma, \tfrac{1}{n-1}, \dots, \tfrac{1}{n-1}, \gamma\right),
% \end{equation}
% where we need $\gamma \geq 1/(n-1)$ for $\mu \in \Delta^n \cap \M^n$ to hold. 


% The expression in \cref{eq:anderson-bound} has been used as a high confidence bound for the mean before. It was first discovered 
% by \cite{Anderson1969} and relates directly to the Kolmogorov-Smirnov statistic. It was not related to $\CVAR$ as in \cref{eq:anderson-bound} before however.

% \begin{proposition}[Anderson {\cite{Anderson1969}}] \label{prop:anderson}
%         Consider an \iid{} sample $\{Z_i\}_{i=1}^{n-1}$ from $Z$ with $Z_{n} \geq \esssup[Z]$ and cdf $F$. Then
%         \begin{equation} \label{eq:anderson}
%                 \prob\left[ \E[Z] \leq \bar{\CVAR}_n^\gamma[Z]  \right] \geq 1 - \varepsilon,
%         \end{equation}
%         with $d = \lceil n\gamma\rceil$, $\bar{\CVAR}_n^\gamma[Z]$ as in \cref{eq:anderson-bound} and with
%         \begin{equation} \label{eq:bound}
%         \varepsilon = \prob\left[\exists z \in (-\infty, \bar{Z}] \colon F_n(z) - \gamma > F(z)\right],
%         \end{equation}
%         where $F(z) \dfn \prob[Z \leq z]$ the cumulative distribution function (cdf) and $F_n(z) \dfn \sum_{i=1}^{n} \bm{1}_{(-\infty, z]}(Z_i) / (n)$
%         the empirical cdf.
% \end{proposition}

% \begin{remark} \label{rem:asymptotic}
%         For large $n$ an asymptotic bound for \cref{eq:order-statistics-bound} in the case of \cref{eq:anderson-bound-vertex}
%         is given as \cite[Thm.~11.6.2]{Wilks1964}
%         \begin{equation*}
%                 \varepsilon = e^{-2 (n-1) \gamma^2} \quad \Leftrightarrow \quad \gamma = \sqrt{\tfrac{\log(1/\varepsilon)}{2n - 2}}.
%         \end{equation*}
%         For small $n$ the previous method in \cref{rem:order-statistics-bound} is preferred. The dependency of 
%         $\gamma$ on the confidence level is logarithmic. Hence its choice will not affect the result much. The $1/\sqrt{n}$ 
%         also occurs in many other DRO schemes (cf. \cite[Lem.~1]{Duchi2021b}).
% \end{remark}

% Calibrating $\bar{\CVAR}_\gamma^n$ involves finding a $\gamma \in [0, 1]$ such that the right-hand side of
% \cref{eq:order-statistics-bound} (computed using \cite{Moscovich2020}) equals $1-\varepsilon$.
% For this, standard scalar root finders\footnote{We use \texttt{brentq} as implemented in \texttt{scipy 1.10.0}} can be used. 
% Alternatively \cref{rem:asymptotic} can be used to calibrate $\bar{\CVAR}_{\gamma}^n$ 
% to act as an asymptotic high-confidence upper bound for $\E[Z]$. 



\subsection{$\phi$-divergence risk} \label{sec:calibration:b}
The $\phi$-divergence \cite{Ben-Tal2013} between $\mu, \nu \in \Delta^n$ is defined as 
\begin{equation} \label{eq:phi-divergence}
    I_{\phi}(\mu, \nu) = \sum_{i=1}^{n} \nu_i \phi\left( \mu_i/\nu_i \right)
\end{equation}
where $\phi(t)$ is convex for $t \geq 0$, $0\phi(a/0) \dfn a \lim_{t\to\infty} \phi(t)/t$, 
$0\phi(0/0) \dfn 0$ and $\phi(1) = 0$. A (centered) $\phi$-divergence ambiguity set is then
\begin{equation} \label{eq:phi-ambiguity}
    \amb_{\alpha}^n \dfn \left\{ \mu \colon I_{\phi}^n(\mu) \dfn I_{\phi}(\mu, \one_n/n) \leq \alpha\right\},
\end{equation}
with $\one_n/n \dfn (1/n, \dots, 1/n)$. The distortion set associated with it is $\distort_{\alpha}^n \dfn \amb_{\alpha}^n + (\M^n)^\circ$,
by \cref{thm:distortion-representation}. The set $\amb_{\alpha}^n$ is clearly permutation invariant, since \eqref{eq:phi-divergence}
is when $\nu = \one_n/n$.

Calibration of the $\phi$-divergence risk corresponds to solving 
\begin{equation} \label{eq:scenario-program}
    \argmin_{\alpha \geq 0} \left\{ \alpha \colon \prob\left[ W \in \distort_{\alpha}^n \right] \geq 1 - \epsilon \right\}.
\end{equation}
Unlike in the previous section, the probability cannot be evaluated exactly. Instead we 
compute a sample approximation to \cref{eq:scenario-program} as follows:

\begin{theorem} \label{thm:divergence-bound}
    Let $\{W^{(i)}\}_{i=1}^m$ be an \iid{} sample uniformly from $\Delta^n$ and $\alpha_i \dfn ({I_{\phi}^n}^*)^\diamond(W^{(i)})$
    for $i \in [m]$ with associated order statistics $\alpha_{(1)} \leq \dots \leq \alpha_{(m)}$. Then, for some $\epsilon \in [0, 1]$ 
    and some $u \in [n]$, 
    \begin{equation*}
        \prob\left[ \prob\left[ ({I_{\phi}^n}^*)^\diamond(W) \leq \alpha_{(u)} \mid W^{(i)} \right] \geq 1 - \varepsilon \right] \geq 1 - \beta
    \end{equation*}
    with $\beta = I_{1-\varepsilon}(u, m-u+1)$ the regularized incomplete beta function (i.e., the cdf of a beta distribution) at level $1-\epsilon$.
\end{theorem}

In practice, sample $m$ values of $\alpha_i$ and sort them. 
For given $\beta$ and $\varepsilon$, take $u \in [n]$ such that $\beta = I_{1-\varepsilon}(u, m-u+1)$
or increase $m$ if no such $u$ exists. Then \cref{thm:divergence-bound} 
says $\rho$ with radius $\alpha_{(u)}$ is $\varepsilon$-calibrated with probability at least $1-\beta$. 

\begin{remark}
    Note that \cref{eq:scenario-program} can be viewed as a chance-constrained program. So instead of relying on classical bounds from order statistics
    as we did in the proof of \cref{thm:distortion-bound} in \ilarxiv{\cref{app:calibraton}}\ilpub{\cite[App.~D]{TR}},
    we could use \cite[Thm.~3.7]{Campi2018}. We evaluate $({I_{\phi}^n}^*)^\diamond(W)$ 
    with a modified \emph{pool adjacent violators (PAV)} algorithm, which has worst-case computational complexity 
    of $\mathcal{O}(n)$ \cite{Best2000}. Details are provided in \ilarxiv{\cref{rem:pav}}\ilpub{\cite[Rem.~D.3]{TR}}.
\end{remark}

% For the sake of making the sample approximations computationally efficient we need the following.
% \begin{proposition} \label{prop:phi-containment}
%     Consider a $\phi$-distortion set $\distort_{\alpha}^n \dfn \amb_{\alpha}^n + (\M^n)^\circ$ with $\amb_{\alpha}^n$ as in \cref{eq:phi-ambiguity}. 
%     Then $\mu \in \distort_{\alpha}^n$ if and only if 
%     \begin{equation} \label{eq:phi-containment}
%         \left( {I_{\phi}^n}^* \right)^\diamond(\mu) = \sup_{\lambda \in \Re_{\uparrow}^n} \left\{ \sum_{i=1}^{n} \mu_i \cdot \lambda_i -  \frac{1}{n} \phi^*(\lambda_i) \right\}\leq \alpha.
%     \end{equation}
% \end{proposition}
% \begin{proof}
%     By construction of $\distort_{\alpha}^n = \amb_{\alpha}^n + (\M^n)^\circ$ we have $\mu \in \distort_{\alpha}^n$ 
%     if and only if there is some $s \in (\M^n)^\circ$ such that $\mu - s \in \amb_{\alpha}^n$ or 
%     \begin{align*}
%         \exists s \in (\M^n)^\circ\colon I_{\phi}^n(\mu - s) \leq \alpha \Leftrightarrow \inf_{s} \left\{ I_{\phi}^n(\mu - s) + \iota_{(\M^n)^\circ}(s) \right\} \leq \alpha.
%     \end{align*}
%     The infimum equals $I_{\phi}^n \episum \iota_{(\M^n)^\circ}$ by definition. 
%     Applying \cite[Thm.~11.23]{Rockafellar1998}, using $\dom I_{\phi}^n = \Delta^n$ and $\intr(\Delta^n) \cap (\M^n)^\circ \neq \emptyset$,
%     gives ${I_{\phi}^n} \episum \iota_{(\M^n)^\circ} = ({I_{\phi}^n}^* + \iota_{((\M^n)^\circ)^\circ})^* = ({I_{\phi}^n}^* + \iota_{(\Re^{n}_{\uparrow})})^*
%     = ({I_{\phi}^n}^*)^\diamond$, where we used \cite[Ex.~11.4]{Rockafellar1998} for the second equality.
%     For the equality in \cref{eq:phi-containment} we apply the tricks for the convex conjugate of a seperable sum and the conjugate of $\alpha f(x/\alpha)$
%     listed in \cite[App.~B, Table~3]{Beck2017}.
% \end{proof}

% \begin{remark}
%     To compute the supremum in \cref{eq:phi-containment} we specialize\footnote{
%         All that was required to specialize the algorithm of \cite{Best2000} is to note that 
%         \begin{equation*}
%             \argmax_{\lambda \in \Re} \quad \sum_{i=p}^{q} \left(\frac{1}{n}\right) \phi^*(\lambda) - \mu_i \cdot \lambda = \partial \phi\left( \frac{n \bar{\mu}_{[p, q]}}{q-p} \right),
%         \end{equation*}
%         which follows from simple algebra and \cite[Prop.~11.3]{Rockafellar1998}.
%     } the pool adjacent violators (PAV) algorithm from \cite{Best2000}. 
%     This requires a partition of $[n]$ given as $\set{J}$ consisting of blocks $[p, q] \subseteq [n]$. 
%     Let $[p, q]^+$ denote the block starting with $q+1$ and $[p, q]^-$ the one ending at $p-1$.
%     If no $[p, q]^+$ or $[p, q]^-$ exists then the output is $\emptyset$. 
%     For $\lambda, \mu \in \Re^n$ let $\lambda_{[p, q]} = (\lambda_p, \dots, \lambda_q)$ and $\bar{\mu}_{[p, q]} = \sum_{i=p}^q \mu_i$. 
%     Using this notation \hyperref[alg:pav]{Alg.~\ref{alg:pav}} computes the $\argmax$ of \cref{eq:phi-containment}. We give expressions for $\partial \phi$ 
%     for some $\phi$-divergences in \cref{tab:phi-table}.

%     {\SetAlgoNoLine\begin{algorithm}
%         \caption{The PAV algorithm for computing \cref{eq:phi-containment}}\label{alg:pav}
%         \textbf{initialization}: \\
%         \quad pick a partition $\set{J} \gets \{[i, i]\}_{i=1}^{n}$ \\
%         \quad\,\, let $\lambda_{[i, i]}^\star \in \partial \phi(n \bar{\mu}_{[i, i]})$ and $[p, q] \gets [1, 1]$.

%         \While{$[p, q]^+ \neq \emptyset$}{
%             \eIf{$\lambda^\star_{[p, q]} > \lambda^\star_{[p, q]^+}$}{
%                 \vspace{3pt}$J \gets (J \setminus \{[p, q], [p, q]^+\}) \cup ([p, q] \cup [p, q]^+)$ and $[p, q] \gets [p, q] \cup [p, q]^+$.\\
%                 let $\lambda_{[p, q]}^{\star} \in \partial \phi(n \bar{\mu}_{[p, q]}/(q-p))$.
%             }{
%                 $J \gets (J \setminus \{[p, q]^{-}, [p, q]\}) \cup ([p, q]^{-} \cup [p, q])$ and $[p, q] \gets [p, q]^{-} \cup [p, q]$.\\
%                 let $\lambda_{[p, q]}^{\star} \in \partial \phi(n \bar{\mu}_{[p, q]}/(q-p))$.                
%             }
%         }
%         \textbf{return}: $\lambda^\star$.\vspace{7pt}
%     \end{algorithm}}

%     Keeping these tools in mind, calibration of the $\phi$-divergence risk corresponds to solving 
%     \begin{equation} \label{eq:scenario-program}
%         \argmin_{\alpha \geq 0} \left\{ \alpha \colon \prob\left[ \left( {I_{\phi}^n}^* \right)^\diamond(W) \leq \alpha\right] \geq 1 - \epsilon \right\}.
%     \end{equation}
%     Letting $F_{\phi}$ denote the cdf of $({I_{\phi}^n}^*)^\diamond(W)$ then we can express this as $\alpha = F^{-1}_{\phi}(1-\epsilon)$.
%     That is we want to compute the quantile of a scalar random variable. 

%     \begin{theorem} \label{thm:divergence-bound}
%         Let $\{W^{(i)}\}_{i=1}^m$ be an \iid{} sample uniformly from $\Delta^n$ and $\alpha_i \dfn ({I_{\phi}^n}^*)^\diamond(W^{(i)})$
%         for $i \in [m]$ with associated order statistics $\alpha_{(1)} \leq \dots \leq \alpha_{(m)}$. Then, for some $\epsilon \in [0, 1]$ 
%         and some $u \in [n]$, 
%         \begin{equation*}
%             \prob\left[ \prob\left[ ({I_{\phi}^n}^*)^\diamond(W) \leq \alpha_{(u)} \mid W^{(i)} \right] \geq 1 - \epsilon \right] \geq 1 - \beta
%         \end{equation*}
%         with $\beta = I_{1-\epsilon}(u, m-u+1)$ the regularized incomplete beta function (i.e., the cdf of a beta distribution) at level $1-\epsilon$.

%         % Moreover 
%         % \begin{equation*}
%         %     \prob\left[ ({I_{\phi}^n}^*)^\diamond(W) \leq \alpha_{(u)} \right] \geq 
%         % \end{equation*}
%     \end{theorem}
%     \begin{proof}
%         The one-sided bound is stated in \cite[\S{}G.2.1]{Meeker2017}.% and the second in \cite[\S{}G.4.2]{Meeker2017}. 
%     \end{proof}

%     \begin{remark}
%         Note that \cref{eq:scenario-program} can be viewed as a scenario program. So instead of relying on classical bounds from order statistics,
%         we could use \cite[Thm.~3.7]{Campi2018}. 
%     \end{remark}

%     % The ordered conjugate is often more complex to compute analytically compared to the convex conjugate. However, efficient numerical procedures are available
%     % for convex seperable functions. 
    
%     % Specifically let $\varphi(x) = \sum_{i=1}^{n} f(x_i)$. Then
%     % \begin{equation*}
%     %     \begin{alignedat}{2}
%     %         -\varphi^{\diamond}(y) = &\min_{x \in \Re^n} & \quad &\sum_{i=1}^{n} f(x_i) - \<y_i, x_i\> \\
%     %                               &\sttshort & & x_1 \leq x_2 \leq \dots \leq x_n.
%     %     \end{alignedat}
%     % \end{equation*}
%     % The cost of the problem is separable and convex in terms of $x$. The monotonicity constraint (or simple chain constraint)
%     % is common in literature. Specifically \cite{Best2000} describes an algorithm of worst-case complexity $\mathcal{O}(n)$
%     % referred to as the pool adjacent violators (PAV) algorithm. To implement PAV one needs to evaluate 
%     % \begin{equation*}
%     %     \argmax_{x} yx - f^*(x) = \partial f(y).
%     % \end{equation*}
%     % We state these minimizers for the problem in \cref{prop:phi-containment} for several $\phi$-divergences in \cref{tab:phi-table} below.
% \end{remark}

% \begin{table*}
%     \begin{tabular}[t]{>{\bfseries}l c c c}
%         \toprule
%         Divergence & $\phi(t)$ & $\phi^{*}(\lambda)$ & $\partial \phi(t)$ \\
%         \midrule
%         Kullback-Leibler        & $t\log t - t + 1$     & $e^\lambda - 1$                           & $\log(t)$ \\
%         Burg Entropy            & $t-\log t - 1$        & $-\log(1-\lambda), \lambda < 1$           & $1 - t^{-1}$ \\
%         Hellinger distance      & $(\sqrt{t} - 1)^2$    & $\lambda/(1-\lambda), \lambda < 1$        & $1 - 1/\sqrt{t}$ \\
%         $\chi^2$-distance       & $(t-1)^2/t$           & $2 - 2\sqrt{1-\lambda}, \lambda < 1$      & $1 - t^2$ \\
%         Variation distance      & $|t-1|$               & $\max(-1, \lambda), \lambda < 1$          & $\begin{cases}-1, & t < 1 \\ [-1, 1], & t = 1 \\ \phantom{-}1, & 1 < t \end{cases}$ \\
%         \bottomrule
%     \end{tabular}
%     \centering
%     \caption{Some $\phi$-divergences, their conjugates and subgradients required for PAV.} \label{tab:phi-table}
% \end{table*}

% \section{Statistical Framework}
% We assume all random variables are defined on a sample space $(\Omega, \F, \prob)$ and consider $Z \colon \Omega \to (-\infty, \bar{Z}]$ 
% with $n$ iid. copies $\{Z_i\}_{i=1}^{n}$. These samples serve to model $\ell(\theta, \xi_i)$ for a fixed value of $\theta$,
% while $\bar{Z}$ replaces $\bar{\ell}(\theta)$. Under this setting we are interested in bounding the mean $\E[Z]$. 

% To do so we use the following:
% \begin{proposition}[Anderson {\cite{Anderson1969}}] \label{prop:anderson}
%         Let $Z \colon \Omega \to (-\infty, \bar{Z}]$ with sample $\{Z_i\}_{i=1}^{n}$. Then
%         \begin{equation} \label{eq:anderson}
%                 \prob\left[ \E[Z] \leq \left(\frac{d}{n} - \gamma \right) Z_{(d)} + \sum_{i=d+1}^{n} \frac{Z_{(i)}}{n} + \gamma \bar{Z}  \right] \geq 1 - \varepsilon,
%         \end{equation}
%         with $d = \lceil n\gamma\rceil$, $Z_{(1)} \leq \dots \leq Z_{(n)}$ the order statistics associated with the sample and for
%         \begin{equation} \label{eq:bound}
%         \varepsilon = \prob\left[\exists z \in (-\infty, \bar{Z}] \colon F_n(z) - \gamma > F(z)\right],
%         \end{equation}
%         where $F(z) \dfn \prob[Z \leq z]$ the cumulative distribution function (cdf) and $F_n(z) \dfn \sum_{i=1}^{n+1} \bm{1}_{(-\infty, z]}(Z_i) / (n+1)$
%         the empirical cdf, with $Z_{i+1} = \bar{Z}$ for notational convenience
% \end{proposition}

% \begin{remark}
%         The value \cref{eq:bound} is related to one-sided Kolmogorov-Smirnov statistics and is independent of the distribution of $Z$ when $F$
%         is continuous (cf. \cite[\S1]{Shorack2009}).

%         For an analytic expression for \cref{eq:bound}, see \cite[Thm.~9.1.1]{Shorack2009} or \cite[Thm.~11.6.1]{Wilks1964}. Such expressions are 
%         however numerically unstable. Instead we use the scheme proposed in \cite{Moscovich2020}, which is both numerically stable 
%         and has computational complexity $\mathcal{O}(n^2)$. To the authors knowledge this is the best available.

%         Also note that $\varepsilon$ is clearly non-increasing $\gamma$ with $\epsilon = 0$ and $\epsilon = 1$ for $\gamma = 1$ and $-1$ respectively. 
%         So we invert it using standard root finders\footnote{We use \texttt{brentq} as implemented in \texttt{scipy 1.10.0}}.
% \end{remark}

% \begin{remark}
%         For large $n$ an asymptotic bound for \cref{eq:bound} is given as \cite[Thm.~11.6.2]{Wilks1964}
%         \begin{equation*}
%                 \varepsilon = e^{-2n \gamma^2} \quad \Leftrightarrow \quad \gamma = \sqrt{\frac{\log(1/\varepsilon)}{2n}}.
%         \end{equation*}
%         Due to inaccuracies for small $n$ the previous method is preferred. It is informative that the dependency of 
%         $\gamma$ on the confidence level is logarithmic. Hence its choice will not affect the result much. The $1/\sqrt{n}$ 
%         is also of note as it occurs in many DRO schemes (cf. \cite[Lem.~1]{Duchi2021b}).
% \end{remark}

% We now relate the bound in \cref{eq:anderson} to $\CVAR$. 
% \begin{lemma} \label{lem:cvar-rewritten}
%         Let $\CVAR_{n}^{\gamma}[Z]$ be defined as in \cref{eq:conditional-value-at-risk}. That is 
%         \begin{equation} \label{eq:cvar-definition}
%                 \CVAR_{n}^{\gamma}[Z] = \inf_{\tau} \left\{ \tau + \frac{1}{(1-\gamma) n} \sum_{i=1}^{n} [Z_i - \tau]_+ \right\},
%         \end{equation}
%         for a sample $\{Z_i\}_{i=1}^{n}$ of the random variable $Z$.         
%         Then
%         \begin{equation*}
%                 (1 - \gamma) \CVAR_n^{\gamma}[Z] = \left( \frac{d}{n} - \gamma \right) Z_{(d)} + \sum_{i=d+1}^{n} \frac{Z_{(i)}}{n},
%         \end{equation*}
%         with $d \dfn \lceil n\gamma \rceil$ and $Z_{(1)} \leq \dots \leq Z_{(n)}$ the order statistics. 
% \end{lemma}
% \begin{proof}
%         Consider the minimizers in the definition of $\CVAR$:
%         \begin{equation*}
%                 \argmin_\tau \left\{ \tau + \frac{1}{(1-\gamma) n} \sum_{i=1}^{n} [Z_i - \tau]_+ \right\}.
%         \end{equation*} 
%         By \cite[Thm.~1]{Rockafellar2000}, this set is a closed bounded interval with the left endpoint being
%         \begin{align*}
%                 \VAR_{n}^{\gamma}[Z] &\dfn \inf_z \{z \colon F_n(z) \geq \gamma\} \\
%                                      &= \inf_z \left\{ z \colon \sum_{i=1}^{n }\bm{1}_{(-\infty, z]}(Z_i) \geq \gamma n \right\} = Z_{(d)},
%         \end{align*}
%         with $F_n$ the empirical cdf, the definition of which we plugged in for the second equality.
%         For the third equality note that the left-hand side counts the number of samples $Z_i$ smaller than or equal to $z$. 
%         Assume 
%         \begin{equation}
%                 Z_{(d-k-1)} < Z_{(d-k)} = Z_{(d-k+1)} = \dots = Z_{(d)},
%         \end{equation}
%         for $k \geq 0$. Then clearly there are at least $d = \lceil n \gamma \rceil > n\gamma$ samples smaller than or equal to $Z_{(d-k)}$. For any $z < Z_{(d-k)}$ meanwhile 
%         there are at most $d-k-1$ samples smaller than or equal. Hence 
%         $\VAR_{n}^{\gamma}[Z] = Z_{(d-k)} = Z_{(d)}$. 

%         Plugging into the cost of \cref{eq:cvar-definition} gives 
%         \begin{align}
%                 &Z_{(d)} + \frac{1}{(1-\gamma) n} \sum_{i=1}^{n} [Z_i - Z_{(d)}]_+ \nonumber \\
%                 &\quad = Z_{(d)} + \frac{1}{(1-\gamma) n} \sum_{i=d + 1}^{n} (Z_{(i)} - Z_{(d)}),\label{eq:cvar-rewritten}
%         \end{align}
%         where we used $Z_{(i)} \leq Z_{(d)}$ for $i \leq d$. The stated result follows from some 
%         basic algebraic manipulation.
% \end{proof}

% Note that $\CVAR$ has often been written in terms of order statistics (e.g. \eqref{eq:cvar-rewritten} is identical to \cite[Eq.~32]{Chun2012}). The connection to \cref{eq:anderson}
% has not been made before to our knowledge. 

% \begin{theorem}
%         Let $Z \colon \Omega \to (-\infty, \bar{Z}]$ with sample $\{Z_i\}_{i=1}^{n}$ and $\CVAR_n^\gamma$
%         as in \cref{eq:cvar-definition}. Then 
%         \begin{equation} \label{eq:cvar-bound}
%                 \prob\left[ (1 - \gamma) \CVAR_{n}^{\gamma}[Z] + \gamma \bar{Z} \geq \E[Z] \right] \geq 1 - \varepsilon,
%         \end{equation}
%         with $\varepsilon$ as given in \cref{eq:bound}.
% \end{theorem}
% \begin{proof}
%         This follows from \cref{lem:cvar-rewritten} and \cref{prop:anderson}.
% \end{proof}

% Plugging in $Z = \ell(\theta, \xi)$ implies \cref{eq:confidence-guarantee} from the introduction.
% So the bound in \cref{eq:cvar-bound} serves as a good data-driven proxy cost that replaces the unknown expected risk. 

