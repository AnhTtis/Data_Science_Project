\section{Case Studies} \label{sec:case-studies} 

To illustrate the validity and potential of our method we provide several simple case studies. These 
are convex for maximum interpretability, as in the non-convex case a worse generalization performance might be caused by local optima. 
Nonetheless our method is also applicable in non-convex settings, where stochastic gradient
descent methods can be used (cf. \cite{Mehta2022} for simple distortions and \cite{Chouzenoux2019} for divergences).
In the convex case we use duality to reformulate the proxy cost in \cref{eq:ordered-risk}. See \cite{Schuurmans2023,Bertsimas2009b} for details.

We present problems of the form \cref{eq:erm} and employ ordered risk minimization \cref{eq:ordered-risk} 
or using \cref{eq:biased-cvar}, which we refer to as $\bar{\CVAR}$. For divergences we use either the \emph{total variation (TV)} or
\emph{Kullback-Leibler (KL)} and the radius is calibrated using \cref{thm:divergence-bound} (with $\beta = 0.005$ and $m=10\,000$).
The value of $\gamma$ in \cref{eq:biased-cvar} is calibrated using \cref{eq:calibration-cvar}.

% Throughout this section we will consider the following risk measures: \emph{(SAA)} takes $\rho(X) = \sum_{i=1}^{n} X_i/n$; 
% \emph{($\bar{\CVAR}$)} takes $\rho = \bar{\CVAR}^n_{\gamma}$ with $\gamma$ determined such that $\rho$ is $\varepsilon$-calibrated
% using \cref{rem:order-statistics-bound} and \cite{Moscovich2020} or using \cref{eq:asymptotic} when $n \geq 100$; \emph{(TV)} and \emph{(KL)} take $\rho$ 
% as a \emph{total-variation} and a \emph{Kullback-Leibler} $\phi$-divergence risk respectively and both are $\varepsilon$-calibrated as in \cref{thm:divergence-bound} with $\beta = 0.005$ and $m = 10\,000$. 

\subsection{Newsvendor}
We begin with a toy problem, illustrating the behavior of our method in low-sample settings. 
Let $\xi \colon \Omega \to \Re$ be Beta distributed with $\alpha = 0.1$, $\beta = 0.2$, scaled by a factor $\bar{D} \dfn 100$. 
Consider a newsvendor problem \cite[\S1.2.1]{Shapiro2021}:
\begin{equation*} \label{eq:newsvendor}
    \begin{alignedat}{2}
        &\minimize_{\theta \in \Re} & \quad & \E\left[\smashunderbracket{c\theta + b[\xi - \theta]_+ +  h[\theta - \xi]_+}{\ell(\theta, \xi)}\right],
    \end{alignedat}
\end{equation*}
with $b = 14$, $h = 2$ amd $c = 1$. For samples $\{\xi_i\}_{i=1}^{n-1}$ with $n=20$ let $\ell_i(\theta) = \ell(\theta, \xi_i)$ 
for $i \in [n-1]$, $\ell_n(\theta) = \max\{(c - b)\theta + b \bar{D}, (c + h)\theta\}$ a robust upper bound. 
We replace the expectation by a data-driven proxy as described at the start of the section.
For the \emph{sample average approach (SAA)} we take $\sum_{i=1}^{n-1} \ell_i(\theta)/(n-1)$.

\begin{figure}
    \centering
    \vspace{0.2em}
    \input{assets/boxes.tex}\vspace{-0.5em}
    \caption{Box plots showing newsvendor expected cost (left); and difference between the predicted cost and expected cost (right). The colored area is the \emph{inter-quartile range (IQR)}, while the whiskers show the 
    range of samples truncated to $1.5$ times the IQR. Outliers outside of this range are depicted as diamonds. The red dashed lines depict the robust performance. The blue dashed line is the optimal cost.} \label{fig:newsvendor-example}%
    \vspace{-1em}
\end{figure}


The calibration problems are solved for $\delta = 0.2$. 
Their performance is compared over $200$ sampled data sets in \cref{fig:newsvendor-example}.
The left plot shows the actual expected cost for the minimizers.
The blue dashed line is the true optimum of \cref{eq:newsvendor}. See \cite[\S1.2.1]{Shapiro2021} for details on how to compute these values. 
Note how the SAA performs decently in the median, but has significantly more variance. The outliers above $240$ were omitted, the largest of which was $428.2$. 
Moreover, the right plot depicts the difference between the optimum value of the proxy cost, and the true cost. 
The SAA often underestimates its true cost, while our methods overestimate it. The dashed red line depicts the behavior when taking $\amb = \Delta^n$
in \cref{eq:ordered-risk} (cf. \cite[Eq.~1.9]{Shapiro2021}). As we almost never perform worse than this robust method, 
this shows that our methods learn from data without over-fitting on the sample. 

In large sample cases, we can use the largest sample as an approximation of $\ell_n(\theta)$. 
This heuristic is similar to the one used in the scenario approach, the consequences of which have been studied in detail (cf.\ \cite{Ramponi2018}).
In combination with some regularization, this significantly boosts 
the performance of our method, as shown in the next \ilarxiv{examples}\ilpub{example}.


\begin{arxiv}
\subsection{Regression}
Let $T_k \colon \Re \to \Re$ denote the Chebychev polynomials of the first kind for $k \geq 0$ and $f_d(x) = (T_k(x))_{k=0}^{d} \in \Re^{d+1}$ 
a feature vector. Consider a lasso regression problem:
\begin{equation} \label{eq:regression}
    \begin{alignedat}{2}
        &\minimize_{\theta \in \Re^{d+1}} &\qquad & \E\left[(\<f_d(X), \theta\> - Y)^2\right] + \lambda \nrm{\theta}_1.
    \end{alignedat}    
\end{equation}
Assuming access to samples $\{(X_i, Y_i)\}_{i=1}^n$, we replace the expectation with the proxy costs described above,
where $\ell_i(\theta) = (\<f_d(X_i), \theta\> - Y_i)^2$ for $i \in [n]$. 
So we approximate the robust term with the largest sample.

For the parameters $\theta_\star = (0, 0, 0.2, 0.5, 1.0)$ the data is generated as $Y_i = \<f_4(X_i), \theta_\star\> + E_i$ with $X_i \deq \mathcal{U}(-1, 1)$ 
and $E_i \deq \mathcal{U}(-0.2, 0.2)$ for $i \in [n]$. We over-parametrize the problem, taking $d = 20$, to illustrate the regularizing effect of our method.
A fit is plotted for $\lambda = 0.2$ and $n = 50$ in \cref{fig:regression-example}. Note how the risk measures all perform similarly, 
while SAA has a worse fit.

\begin{figure}
    \centering
    \input{assets/regression.tex}
    \caption{Regression using $n = 50$ samples with $d=20$ and $\lambda = 0.2$ for different risk measures.} \label{fig:regression-example}
    \vspace{-1em}
\end{figure}

The methods are evaluated quantitatively by sampling an additional $100\,000$ data points and computing a sample approximation of the cost of \cref{eq:regression}.
The resulting performance is compared for several tunings in \cref{tab:regression-quantitative}, where any parameters not mentioned are kept as specified above. 
It is of note that our methods are significantly less sensitive to tuning parameters compared to the SAA. In fact, our methods outperform SAA for all tunings investigated. 


{\setlength{\tabcolsep}{4pt}\tiny
\begin{table}
    \vspace{1.2em}
   \input{assets/regression-table.tex}
   \centering
   \caption{Regression generalization performance for various tuning parameters. Values are reported as \emph{mean (standard deviation $\cdot\, 10^{3}$)}
   computed over $10$ training sets. The same $10$ sets were used for every selection of parameters and method. Note that $\varepsilon$ 
   does not affect SAA.} \label{tab:regression-quantitative} \vspace{-1em}
\end{table}
}
\end{arxiv}

\subsection{Support Vector Machines}
Consider a classification problem with $X \deq \mathcal{N}(0, I_2)$ normally distributed and $Y = 1$ if $X_{1} X_{2} \geq 0$ 
and $Y = -1$ otherwise%\footnote{Inspired by \url{https://scikit-learn.org/stable/auto_examples/svm/plot_svm_nonlinear.html}}
. A \emph{Support Vector Machine (SVM)}
solves:
\begin{equation*}
    \begin{alignedat}{2}
        &\minimize_{(f, b) \in \set{H} \times \Re} &\qquad & \frac{1}{2} \nrm{f}_{\set{H}}^2 + \lambda\E\left[ 1 - Y (f(X) - b) \right]_+
    \end{alignedat}
\end{equation*}
with $\lambda > 0$ and $\set{H}$ some \emph{reproducing kernel Hilbert Space (RKHS)} \cite[Def.~2.9]{Scholkopf2002}. The 
resulting classifier is then given by $\mathrm{sign}(f(X) - b)$. Henceforth $\set{H}$ is the RKHS associated with the \emph{radial basis function} kernel 
\cite[\S2.3]{Scholkopf2002} with some standard deviation $\sigma$.
Solving the primal problem is difficult for two reasons: \emph{(i)} 
the true expectation is often unknown; \emph{(ii)} optimizing over the infinite dimensional $\set{H}$ is intractable in general. 
We resolve \emph{(i)} by replacing the expectation with a proxy-cost as described above and \emph{(ii)} through the usual duality trick \cite[\S7.4]{Scholkopf2002}.
Details are deferred to \ilarxiv{\cref{app:svm}}\ilpub{\cite[App.~B]{TR}}. 

The proxy cost of three of the risks above -- SAA, TV and $\bar{\CVAR}$ -- is a maximum of linear functions and the dual problem is a QP. 
The sample average -- C-SVC in \cite[\S7.5]{Scholkopf2002} -- is the usual choice. We illustrate the superior performance our calibrated risks. 

\begin{figure}
    \vspace{1em}
    \centering
    \def\svgwidth{0.85\columnwidth}
    \input{assets/svm.tex}
    \caption{SVM classifiers trained using $n = 250$ samples with $\sigma=0.25$ and $\lambda = 10^4$ for different risks. The red and blue markers are samples for $Y=1$ and $-1$ respectively. 
    The line is the decision boundary and the color axis depicts $f(X) - b$.} \label{fig:svm-example}
    \vspace{-0.5em}
\end{figure}

In \cref{fig:svm-example}, the three classifiers produced by the three proxy costs above are depicted. 
Note how both TV and $\bar{\CVAR}$ perform similarly and both visibly better than the usual SAA. 
% \ilpub{In further experiments in the technical report \cite{TR}, the performance is compared for several tunings. It is of note that our methods are
% significantly less sensitive to tuning parameters compared to the SAA and outperform it for most parameter values.}%
% \begin{arxiv}%
Quantitative performance is compared through the fraction of 
incorrectly labeled samples in a test set of $10^5$ samples, which 
we refer to as the misclassification rate. The performance is compared for several tunings in \cref{tab:svm-quantitative}, 
where any parameters not mentioned are kept as specified above. It is of note that our methods are significantly less 
sensitive to tuning parameters compared to the SAA. In fact, even for the tunings where SAA performs best, our methods 
perform better for the same tuning, for reasonable choices of $\delta$. 

{\setlength{\tabcolsep}{4pt}
\begin{table}
    \vspace{1.2em}
    \centering
    \input{assets/svm-table.tex}
    \caption{%
%    \ilarxiv{%
    SVM misclassification rates for various tuning parameters. Reported values are the \emph{mean (standard deviation)}
    over $10$ training sets. The same $10$ sets were used for every parameter selection and method. Note that $\delta$ 
    does not affect SAA. The lowest values in a column are bold. Observe that in the rows where SAA achieves its best performance, our methods still perform better.%
%    }%
    } \label{tab:svm-quantitative} \vspace{-1.5em}
\end{table}
}
% \end{arxiv}

\begin{figure}
    \centering
    \input{assets/svm-complexity.tex}\vspace{-0.5em}
    \caption{SVM misclassification rates for varying sample counts $n$. 
    The center line depicts the mean, while the intervals depicts the empirical $0.2$-confidence interval.} \label{fig:svm-complexity} \vspace{-1.5em}
\end{figure}

We can also examine the effect of varying the sample count $n$. For each such value we train the classifiers, again using the parameters used to produce \cref{fig:svm-example}, 
for $30$ training sets. The resulting misclassification rates are depicted in \cref{fig:svm-complexity}. Again note that $\bar{\CVAR}$ and TV both outperform SAA. 

