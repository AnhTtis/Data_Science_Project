
\section{Support Vector Machines} \label{app:svm}
\newcommand{\threshold}{b}
Let $\set{H}$ be some \emph{reproducing kernel Hilbert Space (RKHS)} \cite[Def.~2.9]{Scholkopf2002} with reproducing kernel $\kappa \colon \Re^d \times \Re^d \to \Re$ for some $d \in \N$. 
Here $\<f, g\>_{\set{H}}$ denotes the inner product associated with $\set{H}$ and $\nrm{f}_{\set{H}}^2 = \<f, f\>$ for $f, g \in \set{H}$. 
The \emph{primal problem} for learning a support vector machine is usually given in terms of the \emph{hinge loss}:
\begin{equation*}
    \begin{alignedat}{2}
        &\minimize_{(f, \threshold) \in \set{H} \times \Re} &\quad & \frac{1}{2} \nrm{f}_{\set{H}}^2 + \lambda \E\left[ 1 - Y (f(X) - \threshold) \right]_+.
    \end{alignedat}
\end{equation*}
with $\lambda > 0$, $X \colon \Omega \to \Re^d$ and $Y \colon \Omega \to \{-1, 1\}$. Using the reproducing property of $\kappa$ (cf. \cite[Def.~2.9.1]{Scholkopf2002}) we have $f(X) = \<\kappa(X, \cdot), f\>$. 
Given a sample $\{(X_i, Y_i)\}_{i=1}^{n}$, let $\hat{Y} = \mathrm{diag}(Y_1, \dots, Y_n) \in \Re^{n \times n}$ and $\hat{K} \colon \set{H} \to \Re^n$ 
a linear operator such that $(\hat{K} f)_{i} = \<\kappa(X_i, \cdot), f\>$. We do not include a data-point modeling the $\esssup$ 
of the random loss. Instead the largest sample will act as a replacement. 

We then replace the expectation with a proxy cost, as in \cref{eq:ordered-risk}. 
The robustified, data-driven problem then becomes
\begin{equation} \label{eq:primal-problem}
    \begin{alignedat}{2}
        &\minimize_{(f, \threshold) \in \set{H} \times \Re} &\quad & \frac{1}{2} \nrm{f}_{\set{H}}^2 + \lambda \rho\left[ \one_n - \hat{Y} \left(\hat{K} f - \threshold \one_n \right) \right]_+,
    \end{alignedat}
\end{equation}
where $\rho(x) = \sup_{\mu \in \amb} \, \<\mu, x\>$ is a support function.

\begin{proposition} \label{prop:svm-reformulation}
    The value of \cref{eq:primal-problem} equals
    \begin{equation} \label{eq:svm-dual-problem}
        \begin{alignedat}{2}
            &\maximize_{(\alpha, \beta) \in \Re^n_{+} \times \Re^n_+} &\qquad & \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i, j=1}^{n} \alpha_i \alpha_j Y_i Y_j \kappa(X_i, X_j) \\
            &\stt && \sum_{i=1}^{n} \alpha_i Y_i = 0, \, \frac{\alpha + \beta}{\lambda} \in \amb.
        \end{alignedat}
    \end{equation}
    Moreover, let $\alpha^\star, \beta^\star$ denote the optimizers and $\set{J} \dfn \{j \in [n] \colon \alpha_j > 0, \, \beta_j > 0\}$.
    Then
    \begin{align*}
        f^\star &= \sum_{i=1}^{n} \alpha_i^\star Y_i \kappa(X_i, \cdot) \\
        b^\star &= \sum_{i=1}^{n} \alpha_i^\star Y_i \kappa(X_i, X_j) - Y_j, \quad \forall j \in \set{J} \, \text{when } \set{J} \neq \emptyset
    \end{align*}
    are the optimizers\footnote{In practice, we pick $b^\star$ as the average of the values over all $j \in \set{J}$.} of \cref{eq:primal-problem} when $\set{J} \neq \emptyset$.
    When $\set{J} = \emptyset$, then $b^\star$ can be determined by solving \cref{eq:primal-problem}, keeping $f = f^\star$ fixed.
\end{proposition}
\begin{proof}
    We write \cref{eq:primal-problem} with slack variables first 
    \begin{equation*}
        \begin{alignedat}{2}
            &\minimize_{(f, \threshold, s) \in \set{H} \times \Re \times \Re^n} & \qquad & \frac{1}{2} \nrm{f}_{\set{H}}^2 + \lambda\rho(s) \\
            &\stt && \hat{Y} (\hat{K} f - \one_n \threshold) - \one_n + s \geq 0 \\
            &&& s \geq 0.
        \end{alignedat}
    \end{equation*}

    The next step is to apply Lagrangian duality over Hilbert spaces as presented in \cite[Prop.~19.18]{Bauschke2011}.   
    We first bring the problem in the standard form:
    \begin{equation*}
        \minimize_{x \in \set{G}} \qquad h(x) + g(L x),
    \end{equation*}
    with $\set{G} \dfn \set{H} \times \Re \times \Re^n$ a Hilbert space, elements of which we partition as $x = (f, \threshold, s)$.
    Let $L \colon \set{G} \to \Re^{2n}$ denote the linear operator defined as 
    \begin{equation} \label{eq:ell-definition}
        L x = \left(\hat{Y} \hat{K} f - \hat{Y} \one_n \threshold + s, s\right). 
    \end{equation}
    Its adjoint -- defined implicitly as $L^* \colon \Re^{2n} \to \set{G}$ such that $\<Lx, v\> = \<x, L^* v\>$ -- is given as
    \begin{equation*}
        L^* v = (\hat{K}^* \hat{Y} \alpha,  -\one_n^T \hat{Y} \alpha, \alpha + \beta),
    \end{equation*}
    for $v = (\alpha, \beta)$ and 
    $\hat{K}^* \hat{Y} \alpha = \sum_{i=1}^{n} \alpha_i Y_i \kappa(X_i, \cdot)$. 
    The functions $h \colon \set{G} \to \eRe$ and $g \colon \Re^n \to \eRe$ are given as 
    \begin{align*}
        h(x) &\dfn  \nrm{f}_{\set{H}}^2/2 + \lambda\rho(s)\\
        g(Lx)&\dfn \indi_{\Re^{2n}_+}(Lx - \delta),
    \end{align*}
    with $\delta = (\one_n, 0) \in \Re^{2n}$. 

    First we prove that strong duality holds, a sufficient condition for which is $\mathrm{int} (\dom g) \cap L \dom h$ (cf. \cite[Thm.~15.23, Prop.~6.19(vii)]{Bauschke2011}).
    Note that $\dom g = \Re^{2n}_+ + \delta$ and $\dom h = \set{H} \times \Re \times \Re^n$, keeping in mind that $\dom \rho = \Re^n$ 
    since $\rho$ is coherent. Note that $0 \in \set{H}$ and $0 \in \Re$ and that $L(0, 0, s) = (s, s)$.
    So $\distort = \{(s, s) \colon s \in \Re^n\} \subset L \dom h$. Hence $\mathrm{int} (\dom g) \cap L \dom h \supset (\mathrm{int} (\Re^{2n}_+) + \delta) \cap \distort \neq \emptyset$.

    Consider the convex conjugates $h^*$ and $g^*$. Then
    \begin{align*}
        h^*(\bar{x}) = \nrm{f}_{\set{H}}^2/2 + \indi_{\{0\}}(\bar{\threshold}) + \lambda \rho^*(\bar{s}/\lambda),
    \end{align*}
    where we used the definition of the convex conjugate, the partitioning $\bar{x} = (\bar{f}, \bar{\threshold}, \bar{s}) \in \set{G}$,
    \cite[Prop.~13.16, Prop.~13.20(i)]{Bauschke2011} and seperability of $h$. 
    Since $\rho$ is a support function we have $\rho^* = \indi_{\amb}$. 
    % Noting that $\dom \rho = \Re^n$ enables applying \cite[Thm.~15.27]{Bauschke2011} showing that 
    % \begin{equation*}
    %     (\indi_{\Re^n_+} + \rho)^* = \indi_{\Re^n_+}^* \episum \rho^* = \indi_{\Re^n_{-}} \episum \indi_{\amb} = \indi_{\Re^n_- + \amb},
    % \end{equation*}
    % where we used coherence of $\rho$ (specifically \cref{eq:ambiguity-representation}) for the second equality and \cite[Prop.~12.6]{Bauschke2011} for the third. 
    Also, again letting $v = (\alpha, \beta)$,
    \begin{equation*}
        g^*(v) = \indi_{\Re^{2n}_-}(v) + \trans{\one_n} \alpha
    \end{equation*}
    by \cite[Prop.~13.20(ii)]{Bauschke2011}. The dual problem, the value of which equals minus the primal by strong duality, is then given as \cite[Prop.~19.18]{Bauschke2011}:
    \begin{equation*}
        \begin{alignedat}{2}
            &\minimize_{(\alpha, \beta) \in \Re^n \times \Re^n} &\qquad& \frac{1}{2} \nrm{\hat{K}^* \hat{Y} \alpha}_{\set{H}}^2 - \trans{\one_n} \alpha \\
            &\stt && \trans{\one_n} \hat{Y} \alpha = 0, \, (\alpha + \beta)/\lambda \in \amb, \, (\alpha, \beta) \geq 0
        \end{alignedat}
    \end{equation*}
    where we already integrated the indicator functions in the constraints. 
    After adding the minus sign, this is equivalent to the problem in the theorem. 

    We next compute the subgradients. Note that,
    \begin{align} \label{eq:subgradient-h}
        \partial h^*(\bar{x}) = \{\bar{f}\} \times (\Re \cap \{\bar{\threshold}\}^\bot) \times \mathcal{N}_{\amb}(\bar{s}/\lambda)/\lambda,
    \end{align}
    with $\mathcal{N}_{\set{C}}$ denotes the \emph{normal cone} for set $\set{C}$ \cite[Def.~6.37]{Bauschke2011} and 
    $\set{C}^\bot = \{u \colon \<x, u\> = 0, \forall x \in \set{C}\}$ denotes the \emph{orthogonal complement} of $\set{C}$. 
    The first term follows from differentiability. The second term follows from \cite[Ex.~16.12, Ex.~6.39]{Bauschke2011}
    which states $\partial \indi_{\{0\}}(\bar{\threshold}) = \mathcal{N}_{\{0\}}(\bar{\threshold}) = \Re^n \cap \{\bar{\threshold}\}^{\bot}$. 
    The third follows from applying \cite[Ex.~16.12, Cor.~16.42]{Bauschke2011} to $\indi_{\amb} \circ \lambda^{-1} I_n$. 
    Similarly 
    \begin{align}
        \partial g^*(v) &= \Re^n_+ \cap \{v\}^\bot + \delta \nonumber \\
                             &= \{u + \delta \colon u \geq 0, \, \trans{u} v = 0\}, \label{eq:subgradient-g}
    \end{align}
    where we apply \cite[Cor.~16.38]{Bauschke2011}, differentiability of the second term and again \cite[Ex.~16.12, Ex.~6.39]{Bauschke2011}
    to deal with the indicator. 

    By \cite[Prop.~19.17(v), Prop.~19.18(v)]{Bauschke2011} the optimizers $v^\star = (\alpha^\star, \beta^\star)$ and $
    x^\star = (f^\star, \threshold^\star, s^\star)$
    must satisfy
    \begin{align*}
        && L^* \alpha^\star &\in \partial h(x^\star) &&\text{and}& -\alpha^\star &\in \partial g(L x^\star) \\
        &\Leftrightarrow & x^\star &\in \partial h^*(L^* \alpha^\star) &&\text{and}& L x^\star &\in \partial g^*(-\alpha^\star),
    \end{align*}
    where we used \cite[Cor.~16.24]{Bauschke2011} to express the optimality conditions in terms of the subgradients of the conjugates. 
    Plugging in the subgradient determined in \cref{eq:subgradient-h} gives 
    \begin{align*}
        f^* = \hat{K}^* \hat{Y} \alpha^\star = \sum_{i=1}^{n} \alpha^\star_i Y_i \kappa(X_i, \cdot). 
    \end{align*}
    Comparing with \cite[Eq.~7.25]{Scholkopf2002} shows that this is the usual SVM solution. 
    For the threshold $\threshold^\star$ we use \cref{eq:subgradient-g}, giving
    \begin{align*}
        &&&L x^\star \in \partial g^*(-\alpha^\star) \\
        &\Leftrightarrow && \sum_{i=1}^{n} \alpha_i^\star \left( Y_i (f^\star(X_i) - \threshold^\star) - 1 + s_i^\star \right) + \sum_{i=1}^{n} \beta_i^\star s_i^\star = 0,\\
        &&& Y_i (f^\star(X_i) - \threshold^\star) - 1 + s_i \geq 0, \quad \forall i \in [n],\\
        &&& s^\star \geq 0.
    \end{align*}
    Let $\set{J} = \{j \in [n] \colon \alpha_j^\star > 0, \, \beta_j^\star > 0\}$. From the above conditions we have 
    $s_j^\star = 0$ for all $j \in \set{J}$. Similarly we require 
    \begin{equation*}
        Y_j (f^\star(X_j) - \threshold^\star) - 1 = 0, \quad \forall j \in \set{J}.
    \end{equation*}
    Using $Y_j \in \{-1, 1\}$ to mulyiply both sides with $Y_j$ and the characterization of $f^\star$ above results in 
    \begin{equation*}
        b^\star = \sum_{i=1}^n \alpha^\star_i Y_i \kappa(X_i, X_j) - Y_j, \quad \forall j \in \set{J}.
    \end{equation*}
    We can again compare with the classical SVM setting (cf. \cite[Eq.~47.32]{Scholkopf2002} and the discussion at \cite[p.~206]{Scholkopf2002}),
    seeing that the condition is similar. When $\set{J} = \emptyset$, we cannot generate trivial constraints on $b^\star$. In that case, we note the $f^\star$
    is still a valid minimizer, thus $b^\star$ must minimize \cref{eq:primal-problem} for $f = f^\star$.
\end{proof}

For $\bar{\CVAR}$ as in \cref{eq:biased-cvar} we can characterize the ambiguity set $\amb$ efficiently in terms of a polyhedral set as in \cite{Bertsimas2009b}.
So \cref{eq:svm-dual-problem} is a quadratic program. For divergence-based risk measures the ambiguity set $\amb_{\alpha}$ in \cref{eq:ambiguity-phi-div} 
is convex so we can implement it directly. It is polyhedral for the total variation.

