\section{Calibrating Risk Functionals} \label{app:calibraton}

\subsection{Distortion Risks}
\paragraph*{Proof of \cref{lem:simple-distortion}}
Note that $\<\mu_\uparrow + s, X_{\uparrow}\> \leq \<\mu_\uparrow, X_{\uparrow}\>$ for all $s \in (\M^n)^\circ$ since it is the polar cone of $\Re_{\uparrow}^n$. 
Hence the supremum in \cref{eq:distortion-representation} is achieved at $\mu_\uparrow$. The characterization of $\rho$ as a distortion risk measure
then follows from \cite[Thm.~4.2]{Bertsimas2009b} -- where their risk measures are defined as $X \mapsto \rho(-X)$,
where $\rho$ follows our convention (cf. \cite[Thm.~2.1]{Bertsimas2009b}). \qed

\paragraph*{Proof of \cref{lem:order-statistics-bound}}
We have $W \in \mu_\uparrow + (\M^n)^\circ$ iff. $W - \mu_\uparrow \in (\M^n)^\circ$. This holds iff, 
$\forall k \in [n-1]$ (cf. \cref{prop:dual-monotone-cone})
\begin{align*}
    \ssum_{i=1}^k W_i - \mu_{(i)} \geq 0 \quad \Leftrightarrow \quad \ssum_{i=1}^k W_i \geq \ssum_{i=1}^k \mu_{(i)}.
\end{align*}
The fact that $\sum_{i=1}^k W_i \deq U_{(k)}$ follows from \cite[\S6.4]{David2003}.\qed

\subsection{Details for \cref{ex:cvar}}
We first characterize the conditional value-at-risk in terms of order statistics as a \emph{distortion risk} below.
\begin{lemma} \label{lem:cvar-rewritten}
    Consider $\CVAR_{n}^{\gamma} \colon \Re^{n} \to \Re$ as
    \begin{equation} \label{eq:cvar-definition}
        \CVAR_{n}^{\gamma}[X] = \inf_{\tau} \left\{ \tau + \frac{1}{(1-\gamma) n} \sum_{i=1}^{n} [X_i - \tau]_+ \right\},
    \end{equation}        
    for $\gamma \in [0, 1]$. Then
    \begin{equation*}
        (1 - \gamma)\CVAR_n^{\gamma}[X] = \left( \frac{d}{n} - \gamma \right) X_{(d)} + \sum_{i=d+1}^{n} \frac{X_{(i)}}{n},
    \end{equation*}
    with $d \dfn \lceil n\gamma \rceil$. So $\CVAR_n^\gamma$ is a distortion risk. 
\end{lemma}
\begin{proof}
    Consider the minimizers in the definition of $\CVAR$:
    \begin{equation*}
        \argmin_\tau \left\{ \tau + \frac{1}{(1-\gamma) n} \sum_{i=1}^{n} [X_i - \tau]_+ \right\}.
    \end{equation*} 
    By \cite[Thm.~1]{Rockafellar2000}, this set is a closed bounded interval with the left endpoint being
    \begin{align*}
        \VAR_{n}^{\gamma}[X] &\dfn \inf_x \{x \colon F_n(x) \geq \gamma\} \\
                            &= \inf_x \left\{ x \colon \sum_{i=1}^{n }\bm{1}_{(-\infty, x]}(X_i) \geq \gamma n \right\} = X_{(d)},
    \end{align*}
    with $F_n$ the empirical cdf, the definition of which we plugged in for the second equality.
    For the third equality note that the left-hand side counts the number of values $X_i$ smaller than or equal to $x$. 
    Assume 
    \begin{equation}
        X_{(d-k-1)} < X_{(d-k)} = X_{(d-k+1)} = \dots = X_{(d)},
    \end{equation}
    for $k \geq 0$. Then clearly there are at least $d = \lceil n \gamma \rceil > n\gamma$ values smaller than or equal to $X_{(d-k)}$. 
    For any $z < X_{(d-k)}$ 
    there are at most $d-k-1$ samples values than or equal. Hence 
    $\VAR_{n}^{\gamma}[X] = X_{(d-k)} = X_{(d)}$. 

    Plugging into the cost of \cref{eq:cvar-definition} gives 
    \begin{align}
        &X_{(d)} + \frac{1}{(1-\gamma) n} \sum_{i=1}^{n} [X_i - X_{(d)}]_+ \nonumber \\
        &\quad = X_{(d)} + \frac{1}{(1-\gamma) n} \sum_{i=d + 1}^{n} (X_{(i)} - X_{(d)}),\label{eq:cvar-rewritten}
    \end{align}
    where we used $X_{(i)} \leq X_{(d)}$ for $i \leq d$. The stated result follows from some 
    basic algebraic manipulation. Finally note that $d/n - \gamma = (\lceil n\gamma \rceil - n\gamma)/n \leq 1/n$. 
    So the monotonicity constraint on $\mu$ in \cref{lem:simple-distortion} is also satisfied.
\end{proof}

We can then rewrite \cref{eq:biased-cvar} as 
\begin{equation*}
    \bar{\CVAR}_n^\gamma[X] \dfn (1 - \gamma) \CVAR_{n-1}^{\gamma}[(X_{(i)})_{i=1}^{n-1}] + \gamma X_{(n)}.
\end{equation*}
The associated simple distortion set in \cref{lem:simple-distortion} has
\begin{equation} \label{eq:anderson-bound-vertex}
    \mu = \left(0, \dots, 0, \tfrac{\lceil (n-1)\gamma \rceil}{n-1} - \gamma, \tfrac{1}{n-1}, \dots, \tfrac{1}{n-1}, \gamma\right),
\end{equation}
So we need $\gamma \geq 1/(n-1)$ for $\mu \in \Delta^n \cap \M^n$ to hold. The advantage of $\bar{\CVAR}$ 
as a well-calibrated risk measure over $\CVAR$ is that additional weight is placed on the largest sample. 
This often makes the mean bound associated with $\bar{\CVAR}$ less conservative compared to $\CVAR$ 
for the same confidence level. \qed

\subsection{$\phi$-divergence risk}

We first require the following lemma, that enables checking whether a 
vector lies in the distortion set associated with some $\phi$-divergence risk measure.
\begin{lemma} \label{prop:phi-containment}
    Consider a $\phi$-distortion set $\distort_{\alpha}^n \dfn \amb_{\alpha}^n + (\M^n)^\circ$ with $\amb_{\alpha}^n$ as in \cref{eq:phi-ambiguity}. 
    Then $\mu \in \distort_{\alpha}^n$ if and only if 
    \begin{equation} \label{eq:phi-containment}
        \left( {I_{\phi}^n}^* \right)^\diamond(\mu) = \sup_{\lambda \in \Re_{\uparrow}^n} \left\{ \sum_{i=1}^{n} \mu_i \cdot \lambda_i -  \frac{1}{n} \phi^*(\lambda_i) \right\}\leq \alpha.
    \end{equation}
\end{lemma}
\begin{proof}
    By construction of $\distort_{\alpha}^n = \amb_{\alpha}^n + (\M^n)^\circ$ we have $\mu \in \distort_{\alpha}^n$ 
    iff there is some $s \in (\M^n)^\circ$ such that $\mu - s \in \amb_{\alpha}^n$ or 
    \begin{align*}
        &\exists s \in (\M^n)^\circ\colon I_{\phi}^n(\mu - s) \leq \alpha \\
        &\qquad\Leftrightarrow \quad \inf_{s} \left\{ I_{\phi}^n(\mu - s) + \iota_{(\M^n)^\circ}(s) \right\} \leq \alpha.
    \end{align*}
    The infimum equals $I_{\phi}^n \episum \iota_{(\M^n)^\circ}$ by definition. 
    Applying \cite[Thm.~11.23]{Rockafellar1998}, using $\Delta^n \subseteq \dom I_{\phi}^n$ and $\Delta^n \cap \intr (\M^n)^\circ \neq \emptyset$,
    gives ${I_{\phi}^n} \episum \iota_{(\M^n)^\circ} = ({I_{\phi}^n}^* + \iota_{((\M^n)^\circ)^\circ})^* = ({I_{\phi}^n}^* + \iota_{(\Re^{n}_{\uparrow})})^*
    = ({I_{\phi}^n}^*)^\diamond$, where we used \cite[Ex.~11.4]{Rockafellar1998} for the second equality.
    For the equality in \cref{eq:phi-containment} we apply the tricks for the convex conjugate of a seperable sum and the conjugate of $\alpha f(x/\alpha)$
    listed in \cite[App.~B, Table~3]{Beck2017}.
\end{proof}

\begin{remark} \label{rem:pav}
    To compute the supremum in \cref{eq:phi-containment} we specialize\footnote{
        All that was required to specialize the algorithm of \cite{Best2000} is to note that 
        \begin{equation*}
            \argmax_{\lambda \in \Re} \quad \sum_{i=p}^{q} \left(\frac{1}{n}\right) \phi^*(\lambda) - \mu_i \cdot \lambda = \partial \phi\left( \frac{n \bar{\mu}_{[p, q]}}{q-p} \right),
        \end{equation*}
        which follows from simple algebra and \cite[Prop.~11.3]{Rockafellar1998}.
    } the pool adjacent violators (PAV) algorithm from \cite{Best2000}. 
    This requires a partition of $[n]$ given as $\set{J}$ consisting of blocks $[p, q] \subseteq [n]$ with $p \leq q$. 
    Let $[p, q]^+$ denote the block starting with $q+1$ and $[p, q]^-$ the one ending at $p-1$.
    If no $[p, q]^+$ or $[p, q]^-$ exists then the output is $\emptyset$. 
    For example let 
    \begin{equation*}
        \set{J} = \{[1, 5], [6, 8], [9, 10]\}. 
    \end{equation*}
    Then $[6, 8]^+ = [9, 10]$, $[6, 8]^- = [1, 5]$ and $[1, 5]^- = \emptyset$.

    For $\lambda, \mu \in \Re^n$ let $\lambda_{[p, q]} = (\lambda_p, \dots, \lambda_q)$ and $\bar{\mu}_{[p, q]} = \sum_{i=p}^q \mu_i$. 
    Using this notation \hyperref[alg:pav]{Alg.~\ref{alg:pav}} computes the $\argmax$ of \cref{eq:phi-containment}. We give expressions for $\partial \phi$ 
    for some $\phi$-divergences in \cref{tab:phi-table}. The steps~\ref{step:merge-right} and \ref{step:merge-left} simply perform a merge operation. Say 
    the current partition is as above and $[p, q] = [6, 8]$. Then $\set{J} = \{[1, 5], [6, 10]\}$ and $[p, q] = [6, 10]$ after step~\ref{step:merge-right}. 

    {\SetAlgoNoLine\begin{algorithm}
        \caption{The PAV algorithm for computing \cref{eq:phi-containment}}\label{alg:pav}
        \textbf{initialization}: \\
        \quad initialize the partition $\set{J} \gets \{[i, i]\}_{i=1}^{n}$ \\
        \quad let $\lambda_{[i, i]}^\star \in \partial \phi(n \bar{\mu}_{[i, i]})$ and $[p, q] \gets [1, 1]$.

        \While{$[p, q]^+ \neq \emptyset$}{
            \eIf{$\lambda^\star_{[p, q]} > \lambda^\star_{[p, q]^+}$}{
                \vspace{3pt}$J \gets (J \setminus \{[p, q], [p, q]^+\}) \cup ([p, q] \cup [p, q]^+)$ and $[p, q] \gets [p, q] \cup [p, q]^+$.\label{step:merge-right}\\
                let $\lambda_{[p, q]}^{\star} \in \partial \phi(n \bar{\mu}_{[p, q]}/(q-p))$.\\
                \While{$[p, q]^- \neq \emptyset$ and $\lambda^\star_{[p, q]^-} > \lambda^\star_{[p, q]}$}{
                    $J \gets (J \setminus \{[p, q]^{-}, [p, q]\}) \cup ([p, q]^{-} \cup [p, q])$ and $[p, q] \gets [p, q]^{-} \cup [p, q]$.\label{step:merge-left}\\
                    let $\lambda_{[p, q]}^{\star} \in \partial \phi(n \bar{\mu}_{[p, q]}/(q-p))$.
                }
            }{                
                $[p, q] \gets [p, q]^+$.
            }
        }
        \textbf{return}: $\lambda^\star$.\vspace{7pt}
    \end{algorithm}}
\end{remark}

\begin{table*}
    \begin{tabular}[t]{>{\bfseries}l c c c}
        \toprule
        Divergence & $\phi(t)$ & $\phi^{*}(\lambda)$ & $\partial \phi(t)$ \\
        \midrule
        Kullback-Leibler        & $t\log t - t + 1$     & $e^\lambda - 1$                           & $\log(t)$ \\
        Burg Entropy            & $t-\log t - 1$        & $-\log(1-\lambda), \lambda < 1$           & $1 - t^{-1}$ \\
        Hellinger distance      & $(\sqrt{t} - 1)^2$    & $\lambda/(1-\lambda), \lambda < 1$        & $1 - 1/\sqrt{t}$ \\
        $\chi^2$-distance       & $(t-1)^2/t$           & $2 - 2\sqrt{1-\lambda}, \lambda < 1$      & $1 - t^{-2}$ \\
        Variation distance      & $|t-1|$               & $\max(-1, \lambda), \lambda < 1$          & $\begin{cases}-1, & t < 1 \\ [-1, 1], & t = 1 \\ \phantom{-}1, & 1 < t \end{cases}$ \\
        \bottomrule
    \end{tabular}
    \centering
    \caption{Some $\phi$-divergences, their conjugates and subgradients required for PAV.} \label{tab:phi-table}
\end{table*}

Using \cref{prop:phi-containment} to efficiently check containment in $\distort_{\alpha}^n$ we can compute the data-driven 
approximation of \cref{eq:scenario-program} given in \cref{thm:divergence-bound}. 

\paragraph*{Proof of \cref{thm:divergence-bound}}
By \cref{prop:phi-containment}, \cref{eq:scenario-program} is equivalent to
\begin{equation*}
    \argmin_{\alpha \geq 0} \left\{ \alpha \colon \prob\left[ \left( {I_{\phi}^n}^* \right)^\diamond(W) \leq \alpha\right] \geq 1 - \epsilon \right\}.
\end{equation*}
So, letting $F_{\phi}$ denote the cdf of the scalar random variable
$({I_{\phi}^n}^*)^\diamond(W)$ then we can express this as $\alpha = F^{-1}_{\phi}(1-\epsilon)$.
The result then follows from a one-sided data-driven bound of a quantile stated in \cite[\S{}G.2.2]{Meeker2017}. \qed


