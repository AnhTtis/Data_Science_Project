\section{Methodology}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Global Model Training}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Data Collection}

Our global model approach is highly data driven. 
In order to learn a generalized and transferable SAR feature extractor, intuition says we must train it on data from different sensors, imaging modes, polarizations, resolutions, target/scene types, etc.
Thus, the first step in our methodology is to aggregate data from the following public sources: SAR-Ships \cite{sarship_dataset}; HRSID \cite{hrsid_dataset}; FUSAR \cite{fusar_dataset}; SSDD \cite{ssdd_dataset}; LS-SSDD \cite{lsssdd_dataset}; Dual-Pol Ships \cite{dualpolships_dataset}; SRSDD \cite{srsdd_dataset}; and CVDome \cite{cvdome_dataset}.
After the necessary pre-processing (e.g., chipping from a full frame), we are left with $\sim$100k unlabeled SAR chips which we refer to as $\mathcal{D}_{pretrain}$.
For reference, Fig.~\ref{fig:pretrain_data} shows a handful of samples from this set. 
An important note is that $90\%$ of $\mathcal{D}_{pretrain}$ chips contain ship-like targets and \textit{none} of the chips contain MSTAR land-vehicle targets. 
Given that MSTAR will be our downstream task for FSL evaluation, this sets up for a true test of generalization.
However, in practice if data resembling the anticipated downstream task were available it could easily be leveraged here.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Representation Learning}

With $\mathcal{D}_{pretrain}$ curated we can execute Stage 1, which is to train the global model feature extractor $f_\phi$. 
We use the powerful SimCLR \cite{ChenK0H20} contrastive learning algorithm, which trains by enforcing that two ``views'' of the same image lie near each-other in a feature space, while views of different images lie far away.
Each view is created via aggressive stochastic augmentations that preserve the underlying semantic features but force the model to learn an informative feature set via minimizing the contrastive objective.  
We specifically leverage the normalized temperature-scaled cross-entropy loss (NT-Xent) from \cite{ChenK0H20}, and refer the reader to that work for a more technical description of the algorithm and \cite{simclr_github} for a PyTorch implementation.




Perhaps the most unique part about the application of SimCLR (and many other SSL algorithms) to the SAR ATR setting is the composition of augmentations needed to create informative views. 
Since the SimCLR algorithm was developed and tested in the natural imagery domain, the standard augmentation pipeline contains transforms that are relevant to RGB images. 
However, several of the key augmentations such as color jittering and random gray-scaling \cite{simclr_github} do not fit with our magnitude-only gray-scale SAR modality.
Thus, we create a new augmentation pipeline made of seven image-level transformations that work to create translation invariance, and to manipulate the signal-to-noise ratio and frequency content of the chips. 
Included are random resized cropping, random horizontal flipping, element-wise power-scaling, Gaussian noising and filtering, and linear re-scaling.
See Supplemental A for more information about our transformation pipeline.
Lastly, we mention that we do not believe our design choices made in this section are ``optimal.'' 
Important areas of future work are to experiment with other SSL algorithms (see Supplemental C) and to develop more domain-relevant augmentations that may lead to higher quality learned features.


%%% D_pretrain Examples Figure
\begin{figure}[t]
  \centering
  %\vspace{-2mm}
  \includegraphics[width=0.96\linewidth]{figures/pretrain_data_small.png}
  \vspace{-1mm}
  \caption{Samples from $\mathcal{D}_{pretrain}$, sourced from: SAR-Ships \cite{sarship_dataset}; HRSID \cite{hrsid_dataset}; FUSAR \cite{fusar_dataset}; SSDD \cite{ssdd_dataset}; LS-SSDD \cite{lsssdd_dataset}; Dual-Pol Ships \cite{dualpolships_dataset}; SRSDD \cite{srsdd_dataset}; and CVDome \cite{cvdome_dataset}.}
  \vspace{-1mm}
  \label{fig:pretrain_data}
\end{figure}

%%% MSTAR SOC Accuracy Table
\input{tables/table_mstar_soc}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Classifier Training}

In Stage 2 (ref. Fig.~\ref{fig:overview}) we use the global model $f_\phi$ as a fixed feature extractor and learn a classifier $c_\theta$ for the given (M-way, N-shot) FSL task, described by $\mathcal{D}_{support}=\{(x_i,y_i)\}_{i=1 \ldots (M \times N)}$.
To be clear, ``ways'' refers to the number of categories in the label-space and ``shots'' is number of labeled training chips per category. 
Unlike related works in the natural imagery domain that use simple linear regression or nearest neighbor-style classifiers \cite{goodembedding_fsl}, we find it beneficial to utilize a 2-layer neural network which has the flexibility to learn non-linear decision boundaries.

We specifically consider two ways to train $c_\theta$. 
The first, called the \textit{basic} method, is to train under a vanilla supervised learning objective which minimizes empirical risk on $\mathcal{D}_{support}$ samples only. 
Mathematically, the basic training method is described as
\begin{equation}
\min_{\theta} \mathop{\mathbb{E}}_{(x,y)\sim\mathcal{D}_{support}}\big [ L(c_{\theta}(f_{\phi}(x)), y) \big ], 
\end{equation}
where $L$ is the cross-entropy loss between the classifier's prediction and the ground truth label $y$.
The intuition for this training method is to learn a classifier that produces accurate and confident predictions on the ID data.



While the basic method can achieve high ID accuracy, it gives no guidance to the model for how to behave when an OOD input is encountered. 
Without violating or adding any assumptions, we can do something clever to greatly improve the classifier's ability to handle OOD inputs. 
Specifically, we can re-purpose the $\mathcal{D}_{pretrain}$ dataset from Stage 1 as an \textit{outlier exposure} (OE) set in Stage 2.
The intuition for OE \cite{HendrycksOE} is to teach the model a confidence calibration on ID and OOD inputs -- it should make accurate and confident predictions on ID data and minimally confident predictions on OOD data.
Functionally, the minimum confidence state is when the predicted probability is \sfrac{1}{\# classes}, so to achieve the desired effect we set the training targets of the OE data to be a Uniform distribution over the classes $\mathcal{U}_{\mathcal{C}}$, while the targets of the ID data remain 1-hots.
The complete OE objective is
\begin{multline}
   \min_{\theta} \mathop{\mathbb{E}}_{(x,y)\sim\mathcal{D}_{support}}\big [ L(c_{\theta}(f_{\phi}(x)), y) \big ] \\ + \lambda \mathop{\mathbb{E}}_{\tilde{x}\sim\mathcal{D}_{pretrain}}\big [ L(c_{\theta}(f_{\phi}(\tilde{x})), \mathcal{U}_{\mathcal{C}}) \big ], 
\end{multline}
where $\lambda$ is a weighting factor between the ID- and OOD-focused terms that we set to 0.5 here \cite{HendrycksOE}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{OOD Detection}

The final detail of our methodology is how we detect OOD samples during deployment.
To conceptualize the process, think about each test input $x$ being assigned a real-valued score $\mathcal{S}_{ID}(x)$ as a measure of its ID-ness.
During operation, if $\mathcal{S}_{ID}(x) \geq \beta_{thresh}$ (an application dependent threshold), then the input would be considered ID and the classifier would release its prediction over the set of known classes. Else, if $\mathcal{S}_{ID}(x) < \beta_{thresh}$, the input is considered OOD and the system abstains from releasing the prediction.
In this work we use a temperature-scaled Maximum Softmax Probability detector \cite{odin_ood} to produce $\mathcal{S}_{ID}(x)$ scores, described as
\begin{equation}
\mathcal{S}_{ID}(x) = \max_{i\in\mathcal{C}} \frac{exp \big( c_{\theta}^{(i)}(f(x))/\tau \big )}{\sum_{j}^{\mathcal{|C|}} exp \big( c_{\theta}^{(j)}(f(x))/\tau \big )}.
\end{equation}
Here, $\tau$ is a temperature hyperparameter which we set to 100 via validation. 
The intuition for this detection scheme is straightforward: ID samples should be predicted by the classifier with higher confidence than OOD samples. 
This fits naturally with the OE training goal, making it a logical choice in our system.
We note that OOD detection is currently a popular research topic and future works may investigate alternative scoring mechanisms.


