\section{Conclusion}

We now confirm our initial hypothesis (Sec.~\ref{sec:intro}) that modern SSL techniques like SimCLR are capable of learning highly generalizable SAR feature extractors from large pools of diverse and unlabeled SAR data. 
As evidence, our experiments show that a model trained mostly on unlabeled SAR ships is able to provide an informative-enough feature space to do few-shot classification of MSTAR targets.
This result provides merit to the under-studied global model approach to SAR ATR and confirms that SAR features can be highly transferable, even across sensors, imaging modes, target types, etc.
At a more concrete level, we provide a method for performing robust few-shot SAR ATR in a limited environment where there is no labeled data for pretraining.
We also provide a methodology that boosts OOD detection performance of the classifier without adding significant assumptions or overhead.
We hope that this work motivates further study on few-shot SAR ATR and representation learning for SAR, with specific considerations for robustness to OOD inputs and the trade-off between OOD detection and generalization.


Lastly, we provide some suggestions for future work.
We believe there are potential gains in developing improved domain-relevant augmentations for both Stage 1 and Stage 2 training. 
We also believe that adding synthetically generated data crafted to be relevant to the downstream task may improve the quality of learned features.
Next, one may investigate the use of different SSL algorithms and backbone model architectures. 
Finally, an interesting study would be to evaluate when it becomes beneficial to fine-tune the feature extractor instead of keeping it fixed during Stage 2 training.

\noindent \textbf{Public Release Number:}~AFRL-2022-3418









