\section{Experiments}

To show the merit of our approach to the robust few-shot SAR ATR problem we perform three primary experiments and one analysis. 
The first experiment (Sec.~\ref{sec:soc}) shows results in MSTAR's Standard Operating Condition (SOC) and the second (Sec.~\ref{sec:eoc}) investigates generalization performance in an Extended Operating Condition (EOC). 
The third experiment (Sec.~\ref{sec:ood}) measures OOD detection performance against a spectrum of granularities and the analysis (Sec.~\ref{sec:tradeoff}) discusses a critical trade-off between OOD detection and generalization.


All of the experiments follow a similar setup.
In Stage 1, we initialize the feature extractor as a ResNet-18 (RN18) \cite{resnet} backbone with output dimension 512, and the Projector network as a 3-layer neural net \cite{simclr_github} with output dimension 128. 
These components are trained with an ADAM optimizer for 200 epochs, with 1024 batch size, weight decay of 1e-4, and a learning rate of 3e-4 following a cosine decay schedule. 
For the SimCLR NT-Xent loss we use a temperature of 0.01. 
In Stage 2, we discard the Projector network, fix the weights of the RN18, and initialize our 2-layer neural net classifier. 
We train the classifier for 500 iterations, with label smoothing, using an ADAM optimizer with cosine decayed learning rate starting at 1e-3. 
On the data side, we train with $64\times64$~px crops and use Gaussian noise and random flipping augmentations.
Finally, the results in this document are averages over 250 randomized runs.


% SimCLR training: 
% - RN18 architecture
% - bsize=1024
% - lr = 3e-4 with cos decay
% - ADAM optimizer with weight\_decay=1e-4
% - num\_epochs = 200
% - projection dimension = 128
% - NT-Xent temp = 0.01
% Classifier Training
% - 500 iters
% - 5x OE bsize
% - lblsmoothing=0.1
% - centercrop=64
% - ADAM optimizer with lr=1e-3 and cosine decay
% - random hflip
% - gaus noise = 0.1
%All numbers we report are computed averages over 250 randomized runs 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Standard Operating Conditions} \label{sec:soc}


To measure SOC performance on MSTAR we train the models on 17$^{\circ}$ elevation imagery and test the accuracy of classifying 15$^{\circ}$ data \cite{aconvnet}.
As mentioned, because we do not assume to have labeled pretraining data, meta-learning-based FSL methods are inappropriate for comparison (also see Supplemental B). 
For baselines we instead train supervised models from scratch on the $\mathcal{D}_{support}$ samples only. 
We examine two architectures of varying complexities: A-ConvNet \cite{aconvnet} and RN18 (displayed as Scratch-\{AConv,RN18\}).
The MSTAR SOC results are shown in Table~\ref{tab:mstar_soc} for \#-ways=\{2,5,10\} and \#-shots=\{1,5,10,20,25\}, covering a gamut of potential scenarios.



To start, we find several intuitive takeaways. 
First, accuracy increases with the number of shots; and second, as the number of ways increases the models need more shots to achieve the same performance. 
In all cases, our SimCLR-based models significantly outperform the models trained from scratch, and in most cases the OE classifier has a slight edge over the basic classifier. 
When only given 10-shots, our average margin of improvement over the best baseline is +9.8\%.
On full 10-class MSTAR, we reach 90\% average accuracy at $\sim$25-shots, which is a 6\% improvement over the best baseline.
Lastly, we want to emphasize the implications of these results. 
The self-supervised SimCLR backbone, which has been trained on mostly ships, is able to separate the MSTAR data by class even though it has never been trained on such data!






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extended Operating Conditions} \label{sec:eoc}

For the MSTAR EOC test we train the classification models on 17$^{\circ}$ elevation imagery and test on 30$^{\circ}$ data \cite{aconvnet}. 
The large difference in collection geometry causes a sizable change in the target signatures and offers a more challenging test of generalization.
Our results are reported in Table~\ref{tab:mstar_eoc}.
We see several similar trends to the SOC results w.r.t. the \#-shots and \#-ways.
In all cases, the SimCLR-based models outperform the train-from-scratch baselines, while the basic and OE classifiers perform very similarly to each other.
Interestingly, the largest margins over the baselines are at the lowest number of shots. For example, at 5-shots our average improvement over the best baseline is +9\%.

%%% MSTAR EOC Accuracy Table
\input{tables/table_mstar_eoc}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Out-of-Distribution Detection} \label{sec:ood}

Our goal in the third experiment is to examine OOD detection performance across a spectrum of difficulties/granularities.
To do this we leverage OOD data from various sources. 
For a particularly hard OOD test we use a holdout scheme. 
Starting with a 7-way classifier, where in each test iteration 7 of the 10 MSTAR classes are randomly selected for use in the ID label space, we use data from the remaining 3 classes as \textit{Holdout} OOD samples. 
For medium difficulty OOD, we use data from \textit{SARSIM-Roads}, \textit{SARSIM-Medium}, and \textit{SARSIM-Grass} \cite{sarsim_dataset}. 
This is synthetic SAR data which has been generated to specifically match the collection conditions of MSTAR with different backgrounds (note, we remove the Bulldozer and Tank classes to avoid potential ID/OOD ambiguities).
For coarse-grained OOD we generate \textit{FakeData} by creating random Uniform noise chips and use the \textit{MNIST} test set which contains hand-written digits. Both are clearly OOD w.r.t. any SAR classifier.
Fig.~\ref{fig:ood_data} shows examples from each of these datasets. Importantly, keep in mind that none of the test OOD data is in $\mathcal{D}_{pretrain}$ (the OE set), meaning the model has never seen it before.




Table~\ref{tab:mstar_ood} shows the average OOD detection performance (as measured by \% AUROC \cite{odin_ood}) of a 7-way MSTAR SOC classifier at different \#-shots.
Note, the AUROC value can be interpreted as the probability that an ID test input would have a greater $\mathcal{S}_{ID}$ score than an OOD input \cite{smaxthresh}, where 100\% is a perfect detector.
At each setting we examine the performance of the basic versus OE classifier, which we know both achieve high ID accuracy from Sec.~\ref{sec:soc}.
The most important takeaway from Table~\ref{tab:mstar_ood} is that the OE classifier is better than the basic classifier in \textit{all} cases by a large margin.
Interestingly, for the basic model the coarse-grained OOD sets are the hardest to detect, while the OE classifier detects these almost perfectly.
This surprising behavior is something we plan to study in a future work.
Another interesting finding is that detection of \textit{Holdout} samples improves significantly as \#-shots increases, while detection of the other OOD types stays relatively constant w.r.t.~\#-shots. 
Finally, a somewhat expected result is that for OE models, the \textit{Holdout} samples are the most difficult to detect, while the medium- and coarse-grained OOD inputs are less challenging.
Given that these results still show room for improvement, we highly encourage future system designers (whether in few-shot scenarios or not) to use OOD-aware classifier training schemes such as OE to build robustness to the many forms of OOD inputs.


%%% OOD Examples Figure
\begin{figure}[t]
  \centering
  %\vspace{-2mm}
  \includegraphics[width=1.0\linewidth]{figures/ood_data.png}
  %\vspace{-3mm}
  \caption{Sample test chips from each dataset used in the OOD experiment.}
  \label{fig:ood_data}
\end{figure}

%%% MSTAR OOD Table
\input{tables/table_mstar_ood}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalization vs. Detection Trade-off} \label{sec:tradeoff}

Our final experiment is an analysis that deliberates the question: can we have it all? 
Specifically, can we have high accuracy on ID data in SOC \textit{and} high accuracy on ID classes in EOC \textit{and} reliable OOD detection?
To answer this question we create Fig.~\ref{fig:density_plot}, which shows density plots of $\mathcal{S}_{ID}(x)$ scores measured on a SimCLR+OE model for ID test data in SOC ({\color{blue}{SOC-ID}}), ID test data in EOC ({\color{orange}{EOC-ID}}), and Holdout OOD data in SOC ({\color{codegreen}{Holdout-OOD}}).


Firstly, we observe that the {\color{blue}{SOC-ID}} and {\color{codegreen}{Holdout-OOD}} are relatively separable, which matches the findings in Table~\ref{tab:mstar_ood}.
However, the {\color{orange}{EOC-ID}} and {\color{codegreen}{Holdout-OOD}} are not very separable, as both are predicted with relatively low scores. 
Thus, with our method if you require good {\color{codegreen}{Holdout-OOD}} detection then most of the {\color{orange}{EOC-ID}} would also get flagged as OOD.
Conversely, if you require high recall on {\color{orange}{EOC-ID}} then many {\color{codegreen}{Holdout-OOD}} would pass through the detector.
These results indicate that we \textit{cannot} have it all with this design, and highlight a serious trade-off to be considered in future work.
Finally, while this trade-off may not be completely unique to the FSL setting, our previous observation that less shots makes Holdout-OOD detection harder (Sec.~\ref{sec:ood}) means this trade-off may be more dire in the few-shot setting.


%%% Generalization vs Detection Density Plot Figure
\begin{figure}[t]
  \centering
  %\vspace{-2mm}
  \includegraphics[width=1.0\linewidth]{figures/density_plot_small.png}
  %\vspace{-3mm}
  \caption{Density plots of $\mathcal{S}_{ID}$ scores for ID data in SOC, ID data in EOC, and Holdout OOD data. Also shown are hypothetical thresholds to achieve 80\% True Positive Rates on ID data in SOC and EOC.}
  \label{fig:density_plot}
\end{figure}
