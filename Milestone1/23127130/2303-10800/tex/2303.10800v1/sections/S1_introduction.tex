
\section{Introduction} \label{sec:intro}

%%% Background

Most modern Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) algorithms use Deep Neural Networks (DNNs) to perform the underlying recognition function \cite{tom_book}. 
While DNNs have shown to be very effective in ideal laboratory-like conditions, they present several challenges when being considered for deployment in ``real-world'' scenarios.
One challenge is that DNNs are data-hungry, and often utilize hundreds to thousands of training samples per class to achieve state-of-the-art accuracy.
Another key challenge is that DNNs are notoriously susceptible to producing erroneous predictions on anomalous/out-of-distribution (OOD) inputs (i.e., inputs that fall outside of the categories in the training distribution). 
These challenges spark two main concerns. 
First, what if we, as the model practitioners, cannot obtain hundreds of labeled samples per class for training, and instead can only come up with $\sim$5 (i.e., a few shots)?
And second, in these few-shot settings, does OOD detection become harder because the models are primarily seeking generalization and have not been given enough information to learn highly detailed/nuanced representations of the in-distribution (ID) classes?

%%% Overview Figure
\begin{figure}[t]
  \centering
  %\vspace{-2mm}
  \includegraphics[width=1.0\linewidth]{figures/overview.png}
  \vspace{-5mm}
  \caption{Overview of our two-stage approach. In Stage 1 we train the feature extractor on a pool of unlabeled data and in Stage 2 we learn a classifier on top of the feature extractor for a given few shot learning problem.}
  \vspace{-4mm}
  \label{fig:overview}
\end{figure}

%%% APPROACH / METHOD

Our goal in this paper is to address the stated concerns by developing an accurate and robust SAR target classifier given only a few labeled samples per class. 
We also strive to keep a minimal assumption set to prioritize applicability and extendability.
Our high level approach involves two stages and is shown in Fig.~\ref{fig:overview}. 
In the first stage, a global representation model is trained on a diverse and unlabeled pool of SAR data such that it is capable of extracting quality features from (nearly) any SAR chip. 
In Stage 2, for an arbitrary downstream (M-way, N-shot) few-shot learning (FSL) task we train a light-weight classifier using the global model as a fixed feature extractor.


A key part of our methodology is how we train the global model.
Importantly, we do \textit{not} assume to have labeled data in this step, which eliminates the option for a meta-learning approach \cite{protonet,dktsn}.
To learn quality representations we instead aggregate publicly available data from recent papers and leverage a modern unsupervised learning paradigm called Self-Supervised Learning (SSL). 
Specifically, we use the SimCLR \cite{ChenK0H20} method to represent the SSL class of algorithms in this work.
%SSL algorithms such as SimCLR 
SSL has shown great promise for transfer and few-shot learning in the natural imagery domain \cite{goodembedding_fsl}, but has not been studied widely in the context of SAR ATR.
Our hypothesis is that these modern algorithms can learn a highly flexible~/~expressive~/~useful~/~transferable feature extractor from unlabeled SAR data sourced from different sensors, imaging modes, polarizations, resolutions, and target types. Further, the unlabeled data does not necessarily have to represent the anticipated downstream conditions.
This is contrary to several lines of concurrent research whose goal is to generate high-fidelity synthetic imagery as close to the downstream task conditions as possible for supervised pre-training \cite{sarsim_dataset}.


The second key part of our methodology is how we improve robustness to OOD inputs.
This is accomplished in the classifier training stage and given our stated operating environment it is nearly a ``free-lunch.''
Practically, we can use the SSL model's pretraining dataset as an OOD dataset for outlier exposure (OE) training \cite{HendrycksOE, inkawhich_advoe}. 
Intuitively, the classifier is taught to be accurate and confident on labeled task data while being minimally confident on the OOD data.
This objective creates a confidence calibration effect which can be exploited to detect novel OOD samples during deployment \cite{inkawhich_onlineOOD}.


%%% Experiments & Contributions

Using MSTAR \cite{Ross1998MSTAR} as our primary few-shot test environment, we find that our methodology can generate highly accuracy models in both standard and extended operating conditions.
We also find that by OE training in Stage 2, our models can reliably detect a spectrum of fine- to coarse-grained OOD types unseen during training. 
Finally, we uncover and discuss an important trade-off between OOD generalization and detection. 
Overall, we make the following contributions:
\begin{itemize}
    \item We show a proof-of-concept that a highly transferable global SAR feature extractor can be trained without labels on diverse data and used to great effect in downstream FSL tasks;
    \item We develop a robust FSL classification scheme that can reliably detect and reject OOD inputs at test time without adding overhead or assumptions;
    \item We uncover a critical trade-off between OOD detection and generalization that directly motivates future work.
\end{itemize}


%%% Random thoughts:

%- Few-shot SAR-ATR is also particularly challenging because the target signatures can change drastically given minimal shifts in viewing angle and any target motion.

%- Mention that we are not aware of many other FSL approaches that do not require labeled pretraining data from a domain relevant dataset.

%%% End





