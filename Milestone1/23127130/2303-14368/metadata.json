{
    "arxiv_id": "2303.14368",
    "paper_title": "FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views",
    "authors": [
        "Vinoj Jayasundara",
        "Amit Agrawal",
        "Nicolas Heron",
        "Abhinav Shrivastava",
        "Larry S. Davis"
    ],
    "submission_date": "2023-03-25",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
    ],
    "abstract": "We present FlexNeRF, a method for photorealistic freeviewpoint rendering of humans in motion from monocular videos. Our approach works well with sparse views, which is a challenging scenario when the subject is exhibiting fast/complex motions. We propose a novel approach which jointly optimizes a canonical time and pose configuration, with a pose-dependent motion field and pose-independent temporal deformations complementing each other. Thanks to our novel temporal and cyclic consistency constraints along with additional losses on intermediate representation such as segmentation, our approach provides high quality outputs as the observed views become sparser. We empirically demonstrate that our method significantly outperforms the state-of-the-art on public benchmark datasets as well as a self-captured fashion dataset. The project page is available at: https://flex-nerf.github.io/",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14368v1"
    ],
    "publication_venue": "CVPR 2023"
}