Free-viewpoint rendering of a scene is an important problem often attempted under constrained settings: on subjects demonstrating simple motion carefully captured with multiple cameras \cite{Liu2020NeuralHV, MartinBrualla2018LookinGoodEP, MartinBrualla2021NeRFIT}. However, photorealistic \textit{free-viewpoint} rendering of moving humans captured from a \textit{monocular} video still remains an unsolved challenging problem, especially with sparse views.

Neural radiance fields (NeRF) have emerged as a popular tool to learn radiance fields from images/videos for novel view-point rendering. Previous approaches assume multiple view-points and often fail on non-rigid human motions. Human-specific NeRFs have recently become popular for learning models using input videos\cite{Weng2022HumanNeRFFR,Peng2021AnimatableNR}. The current state-of-art approaches such as HumanNeRF\cite{Weng2022HumanNeRFFR} have shown impressive progress in this domain. However, there remain several challenges. Firstly, approaches such as HumanNeRF\cite{Weng2022HumanNeRFFR} utilize a pose prior and use a canonical configuration (\eg T-pose) for optimization, which may be well outside the set of observed poses. The underlying optimization becomes challenging especially as the number of observed views become sparse. In contrast, we select a pose from the available set of poses as the canonical pose-configuration, similar to previous pose-free approaches such as D-NeRF\cite{Pumarola2021DNeRFNR}. This enables best of both worlds; it becomes easier to learn a motion field mapping due to smaller deformations while using a pose prior. In addition, having the canonical view in the training data provides a strong prior for the optimization of the canonical pose itself. Finally, it allows us to optimize the canonical configuration with our novel \textit{pose-independent temporal deformation}. We demonstrate that this architectural change provides significantly better results compared to existing approaches \cite{Weng2022HumanNeRFFR, Ouyang2022RealTimeNC}.

In addition, approaches such as HumanNeRF\cite{Weng2022HumanNeRFFR} depend on the estimated pose for the canonical configuration optimization. Errors in the initial pose estimation, for example, due to strong motion blur cause challenges in pose correction. The underlying assumption that the non-rigid motion is pose-dependent often fails in scenarios with complex clothing and accessories, hair styles, and large limb movements. Our proposed \textit{pose-independent temporal deformation} helps to supplement the missing information in its pose-dependent counterpart.


% We We see little value in choosing a canonical-configuration that is not in the set of observed poses, such as the T-pose. Instead, we select a pose from the available set of poses as the canonical pose-configuration due to three reasons: a) easier to learn a motion field mapping for a smaller deformation (we assume that a pose chosen fro

% On the other hand, approaches such as D-NerF use an observed frame as canonical reference for dynamic scenes without any priors. depends on the pose for canonical configuration optimization.

% Our approach builds upon these architectures. We first highlight challenges in The current state-of-art approaches such as HumanNeRF\cite{Weng2022HumanNeRFFR} which is perhaps the closest work to ours.


% Recent attempts at this problem include HumanNeRF \cite{Weng2022HumanNeRFFR}, which highlights several challenges. One such key issue is that it solely depends on the pose for canonical configuration optimization. Albeit the pose correction in Sec. \ref{sec:method}, erroneous initial pose estimation or the presence of strong motion blur causes this method to fail. Furthermore, the assumption that the non-rigid motion is pose-dependent often fails when it comes to human subjects especially in circumstances with complex clothing and accessories, hair styles, and hand/finger movements. Hence, it is clear that a pose-independent deformation must be learnt to supplement the missing information in its pose-dependent counterpart. 

% Another key issue with HumanNeRF is that they attempt to optimize a canonical pose that is well outside of the set of observed poses. This optimization formulation becomes quite challenging especially as the number of observed views becomes sparser. We see little value in choosing a canonical-configuration that is not in the set of observed poses, such as the T-pose. Instead, we select a pose from the available set of poses as the canonical pose-configuration due to three reasons: a) easier to learn a motion field mapping for a smaller deformation (we assume that a pose chosen from the observed set of poses is closer to themselves than one that is not from the observed set), b) having one view in the training data is a strong prior for the optimization of the canonical pose, and c) it allows us to optimize the canonical configuration with non-pose guided temporal deformation, which is lacking in existing approaches \cite{Weng2022HumanNeRFFR, Ouyang2022RealTimeNC}. 

To this end, we introduce FlexNeRF, a novel approach for jointly learning a \textit{pose-dependent motion field} and \textit{pose-independent temporal deformation} within the NeRF framework for modeling human motions.
Moreover, we introduce a novel cycle consistency loss in our framework, further capitalizing on the fact that our canonical pose corresponds to one of the captured frames. The consistency regularizes the estimated deformation fields by mapping back and forth between each view and the canonical pose. Moreover, the information content of any frame in a motion sequence has a strong similarity to that of its neighbours. Hence, we propose to utilize this contextual information present in a consecutive set of frames to aid learning by imposing a temporal consistency loss. We additionally regularize the training by adding a supplementary loss based on the segmentation masks. Our approach allows photorealistic rendering of a moving human even when sparse views are available, by supplementing the pose-dependent motion field with additional information during learning: (i) pose-independent temporal deformation with ample pixel-wise correspondences beyond the (typically 24) pose point-correspondences, and (ii) consistency constraints/losses. In summary, our paper makes the following contributions: 

% We also introduce consistency constraints and supplementary losses based on intermediate representations such as segmentation to improve the training. 




\begin{itemize}
    \item We propose a novel approach to learn  pose-independent temporal deformation to complement the pose-dependent motion for modeling humans in video, using one of the views as the canonical view.
    
    \item We propose a novel cyclic-consistency loss to regularize the learned deformations.
    
    \item We propose a temporal-consistency loss to aid learning with contextual information present in neighbouring frames, as well as to maintain consistency across consecutive rendered frames.
    
    \item Our approach outperforms the state-of-the-art approaches, with significant improvement in case of sparse views.
    
\end{itemize}
% \begin{itemize}
%     \item We propose a novel approach to learn for optimizing a combined canonical time and pose configuration. Given an observed pose, the combined approach relies on learning the pose-guided skeletal and non-skeletal motion, as well as the pose-independent time-guided motion complementing each other. To the best of our knowledge, FlexNeRF is the first approach to propose such a combined approach for modeling moving humans from monocular videos.
    
%     \item Proposing a novel cyclic-consistency based approach paired with temporal consistency loss. Furthermore, we introduce a post-canonical decoder network for finer-refinement of the rendered views.
    
%     \item Proposing a novel binary segmentation network that attempts to predict the binary segmentation mask coupled with imposing an additional loss on the predicted mask. Furthermore, the segmentation network deploys a learnable threshold designed to eliminate the halo effect of the rendered images.
    
%     \item Empirically proving that the proposed combined approach outperforms the state-of-the-art significantly. We further demonstrate that the our approach yields photorealistic results even when the available views are highly sparse. Furthermore, we explore applications of our approach such as rendering re-posed continuous videos. 
    
% \end{itemize}