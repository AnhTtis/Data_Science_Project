\subsection{Neural Radiance Fields (NeRFs)}
% H-NeRF
NeRFs attempt to learn a scene representation for novel-view synthesis by modeling the radiance field with learnable functions. A variety of approaches have been proposed recently for neural rendering, exploiting voxel grids~\cite{Sitzmann2019DeepVoxelsLP, Dellaert2021NeuralVR}, neural textures~\cite{Shysheya2019TexturedNA, Thies2019DeferredNR}, point-clouds~\cite{Meshry2019NeuralRI, Aliev2020NeuralPG}, and neural implicit functions~\cite{Chen2019LearningIF, Park2019DeepSDFLC}.

The landscape of neural rendering changed with NeRF \cite{Mildenhall2020NeRFRS}, which proposed a simple, yet revolutionary approach for photorealistic novel-view synthesis of static scenes. NeRF attempts to map from the 5-d light fields to 4-d space consisting of color $c$ (RGB) and density $\sigma$: likelihood that the light ray at this 5-d co-ordinate is terminated by occlusion. Since the introduction of original NeRF formulation, several variations and improvements \cite{Srinivasan2021NeRVNR, Zhang2021NeRFactorNF, Rebain2021DeRFDR, Liu2020NeuralSV} have been proposed.

\vspace{-0.5mm}
\subsection{Neural Rendering of Dynamic Scenes}

While originally proposed for static scenes, NeRF based approaches have been recently extended to dynamic scenes, for both rigid and non-rigid objects. These approaches can be divided into two main categories: a) optimizing a canonical configuration, and b) directly optimizing the 4-D spatio-temporal scenes. D-NeRF~\cite{Pumarola2021DNeRFNR} is an example of the first category, which attempts to map each observed frame to a given canonical frame. Once the canonical scene has been optimized for all available views, the novel-view can be rendered from the canonical space, and mapped back to the observed space. The same approach can be seen applied to videos with simple motion and other settings \cite{Park2020DeformableNR, Chen2021AnimatableNR, Tretschk2021NonRigidNR}. In contrast, the approaches that directly estimate spatio-temporal scene representations \cite{Xian2021SpacetimeNI, Li2021NeuralSF} takes positional-encoded or latent-coded time $t$ as an input in-addition to the spatial inputs, and attempts to predict the color and the density along each ray.

\vspace{-0.5mm}
\subsection{Neural Rendering of Human Subjects}
Compared to general rendering of dynamic scenes, human subject-specific rendering has additional challenges in terms of complex non-rigid human motions. Priors such as human pose that can provide additional information for successful scene representation. Hence, most methods \cite{Wu2020MultiViewNH, Peng2021AnimatableNR, Peng2021NeuralBI} begin with assuming \textit{SMPL} template as a prior \cite{Loper2015SMPLAS}. Furthermore, most methods use multi-view videos \cite{Xu2021HNeRFNR, Noguchi2021NeuralAR, Liu2021NeuralAN, Weng2020Vid2ActorFA}. A few recent methods including HumanNeRF \cite{Weng2022HumanNeRFFR} and others \cite{Gao2021DynamicVS, Chen2021AnimatableNR, Tretschk2021NonRigidNR} use monocular videos, whereas only the former attempts free-viewpoint rendering. However, these approaches have challenges rendering photorealistic outputs with sparse input views. We consider HumanNeRF\cite{Weng2022HumanNeRFFR} as the closet work to ours and address the aforementioned challenges.