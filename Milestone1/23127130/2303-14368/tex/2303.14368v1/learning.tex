\input{results_figs_internal}

In this section we describe the loss functions used to learn the FlexNeRF model and discuss details with respect to optimization and ray-sampling.
\vspace{-1mm}
\subsection{Loss Functions}
NeRFs are typically trained with a combination of losses between the rendered and observed frames. In addition, FlexNeRF also uses a combination of segmentation loss, cyclic consistency loss and temporal consistency loss as defined below.

\noindent\textbf{Segmentation Loss:} We apply the BCE-Dice loss between the predicted and ground truth binary segmentation masks
\begin{equation}
\begin{aligned}
    \mathbb{L}_{S} &= \frac{1}{N} \sum \left[ M \text{log} \hat{M} + (1-M)\text{log}(1-\hat{M}) \right] \\
    &+ \frac{2|M \cap \hat{M}|}{|M|+|\hat{M}|},
\end{aligned}
\end{equation}
% \begin{equation}
% \begin{aligned}
%     \mathbb{L}_{S} &= \frac{1}{N} \sum^N_{n=1} \left[ m_n \text{log} \hat{m}_n + (1-m_n)\text{log}(1-\hat{m}_n) \right] \\
%     &+ \frac{2|M \cap \hat{M}|}{|M|+|\hat{M}|},
% \end{aligned}
% \end{equation}
where $N$ is the number of pixels in the segmentation mask. 

\begin{figure}[t]
    \vspace*{-3ex}
  \centering
   \includegraphics[width=0.85\linewidth]{figs/supp/views_comp_flipped.png}
   \caption{LPIPS metric comparison on ZJU-MoCap between HumanNeRF \cite{Weng2022HumanNeRFFR} and our method with decreasing number of views.}
   \label{fig:views_comp}
   \vspace*{-4ex}
\end{figure}

\noindent\textbf{Cyclic Consistency Loss:} We introduce a cyclic consistency constraint on the canonical to observation space transformation, using Mean Squared Error (MSE) between $\hat{x}^o$ and $x^o$ defined by,
\begin{equation}
    \mathbb{L}_{CCL} = \frac{1}{L} \sum_{i=1}^{L} (\hat{x}^o_i- x^o_i)^2,
\end{equation}
where $L$ is the number of positional samples. 
\vspace{1mm}

\input{results_tab_all}

% \input{results_tab_mocap}

\noindent\textbf{Temporal Consistency Loss (TCL):} We identify that imposing temporal consistency constraints can be valuable at two instances: a) while rendering consecutive training frames $\{\hat{I}^o\}_{t=-k}^{k}$ and b) while applying temporal deformation from consecutive training frames to the canonical frame $\{{\Delta x}_T\}_{t=-k}^{k}$. To this end, we employ the cycle-back regression consistency loss proposed in \cite{Dwibedi2019TemporalCL}. The cycle-back regression attempts to determine the temporal proximity of rendered frames or deformation vectors, and penalize the model if they are not in close temporal proximity. Given a rendered frame or a deformation vector $u$, and neighbors $\{v_k\}$, we compute the similarity vector $\beta_k$,
\begin{equation}
    \vspace{-1ex}
    \beta_k = \frac{\text{exp}(-\|u-v_k\|^2)}{\sum_j \text{exp}(-\|u-v_j\|^2)},
\end{equation}
where $u, v_k \in \{v_k\}$ and $\beta$ is a discrete distribution of similarities over time. We impose a Gaussian prior on $\beta$ by minimizing the normalized square distance, 
\begin{align}
    \mu = \sum_k k \beta_k \ \ \ \ \sigma^2 = \beta_k (k-\mu)^2
\end{align}
\vspace{-9mm}
\begin{align}
    \mathbb{L}_{TCL} = \frac{|i-\mu|^2}{\sigma^2} + \lambda log(\sigma),
\end{align}

where $\lambda$ is a regularization parameter. 
% Intuitively, it should peak around the index of $u$. 
Finally, the rendering loss
$\mathbb{L}_{rend}$, the canonical loss  $\mathbb{L}_{can}$, and the overall loss $\mathbb{L}$ are defined as 
\vspace{-3mm}
\begin{align}
\mathbb{L}_{rend} &= \mathbb{L}_{LPIPS}(\hat{I}^o,I^o) + \mathbb{L}_{MSE}(\hat{I}^o,I^o) \\
\nonumber &+ \mathbb{L}_{TCL}(\{\hat{I}^o\}_{t=-k}^{k})\\
\mathbb{L}_{can} &= \mathbb{L}_{MSE}(\hat{x}^c,x^c) + \mathbb{L}_{TCL}(\{{\Delta x}_T\}_{t=-k}^{k})\\
\mathbb{L} &= \mathbb{L}_{rend} + \mathbb{L}_{can} + \mathbb{L}_{CCL} + \mathbb{L}_{S}
\end{align}

% the combination of the LPIPS loss $\mathbb{L}_{LPIPS}$, MSE loss $\mathbb{L}_{MSE\_rend}$ between the ground truth frames $\hat{I}^o$ and predicted frames $I^o$, and the TCL $\mathbb{L}_{TCL\_rend}$ amongst the consecutive rendered frames $\{\hat{I}^o\}_{t=-k}^{k}$. Similarly, the canonical loss $\mathbb{L}_{can}$ can be defined as the combination of the MSE loss $\mathbb{L}_{MSE\_can}$ between the predicted canonical point positions $\hat{x}^c$ and $x^c$ and the TCL $\mathbb{L}_{TCL\_can}$ amongst the temporal deformations $\{{\Delta x}_T\}_{t=-k}^{k}$.

\subsection{Optimization Details}
\vspace{-1mm}
\noindent\textbf{Delayed Modular Optimization:} We follow a delayed-optimization approach similar to \cite{Weng2022HumanNeRFFR} to optimize the non-rigid motion, binary segmentation, and the refinement modules of our method. Optimizing these modules from the beginning yields lower performance as they rely on adequate inputs from the rest of the system. Hence, we freeze these modules initially, and unfreeze them gradually during the course of training.

\noindent\textbf{Ray Sampling:} Since LPIPS use a convolution-based approach to extract features, we use patch-based ray sampling following \cite{Weng2022HumanNeRFFR, Schwarz2020GRAFGR} instead of random ray sampling~\cite{Mildenhall2020NeRFRS} from the whole image. 
% We segregate each input image into a fixed number of patches of uniform pre-defined size, and use rays from a given patch for training. 

% \subsection{Optimization Details}