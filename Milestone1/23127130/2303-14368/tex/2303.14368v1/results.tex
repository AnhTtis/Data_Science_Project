\vspace{-0.5mm}
\subsection{Benchmark Datasets and Metrics}
\vspace{-0.5mm}
We evaluate the proposed method on two public datasets: ZJU-MoCap \cite{Peng2021NeuralBI, fang2021mirrored} and People Snapshot \cite{alldieck2018video}, and one Self-Captured Fashion (SCF) dataset. The People Snapshot dataset has 7 sequences of monocular videos of human subjects displaying rotating motions in front of a static camera. For the ZJU-MoCap dataset, we use 6 sequences to be compatible with \cite{Weng2022HumanNeRFFR} for comparison purposes. We use only the views from the first camera to simulate monocular video settings for training, and use the views from rest of the cameras for evaluation. Videos from both ZJU-MoCap and People Snapshot datasets are carefully captured under lab settings. 

% (chosen subject IDs: 377, 386, 387, 392, 393, 394)

For the SCF dataset, we captured 7 sequences of monocular videos freely where the movements are solely up to the discretion of the consenting subjects. In contrast to the public datasets: a) the subjects are wearing complex clothing and accessories, and the movements are fast, b) videos are captured without any controlled settings, and c) the captured videos are brief with only one full rotation. Due to the absence of ground truths for the SCF and People Snapshot datasets, we evaluate the rendered frames by holding random frames from training. We use LPIPS, PSNR, and SSIM \cite{Zhang2021NeRFactorNF} as evaluation metrics.

\begin{table}[t]
\begin{center}
\begin{tabular}{c}
\hspace{-6mm} HumanNeRF \hspace{1mm} Ours \hspace{2mm} HumanNeRF \hspace{6mm} Ours \hspace{8mm}  \\
\hline \\
% \vspace*{-1ex}
\includegraphics[width=0.75\linewidth, trim={0.4cm 0 1.4cm 0},clip]{figs/supp/tpose_1.png} 
\vspace*{-5ex}
\end{tabular}

\end{center}
  \captionof{figure}{Comparing the rendering of the canonical view for SCF (left) and ZJU-Mocap (right) datasets. Our approach is able to learn a higher quality canonical view.}
%   (Left) HumanNeRF (Middle) Our approach without temporal deformation and cyclic consistency (Right) Our full approach
  \label{fig:analysis}
  \vspace*{-3ex}
\end{table}
\vspace{-0.5mm}
\subsection{Results and Analysis}
\vspace{-0.5mm}
\input{tab_ablations}

\begin{table}[t]
\begin{center}
\begin{tabular}{c}
 \hspace{-6mm} Worst case \hspace{14mm} Best Case  \\
\hline \\
\includegraphics[width=0.7\linewidth, trim={0.6cm 0 0.4cm 0},clip]{figs/supp/fails.png}
\vspace*{-5ex}
\end{tabular}
\end{center}
  \captionof{figure}{Each cell shows the original training frame (left) with rendered frame using the \textit{same} viewpoint after training. Note that for difficult/challenging poses, HumanNeRF (right) fails to minimize the training loss compared to ours (middle). }
  \label{fig:render_training_frames}
  \vspace*{-4ex}
\end{table}

%H-NeRF \cite{Xu2021HNeRFNR}
\vspace{-0.75mm}
\noindent\textbf{Quantitative Results:} Table~\ref{tab:all_datasets} compares our method against HumanNeRF~\cite{Weng2022HumanNeRFFR} and Neural-Body~\cite{Peng2021NeuralBI} across the three datasets. We consider two settings: \textit{Full}, using all the frames and \textit{Sparse}, using a sparse number of views. To generate the \textit{Sparse} setting, we remove stationary frames from the video and any subsequent frames after the first  complete rotation of the subject. Subsequently, we re-sample every $k^{\text{th}}$ frame, where $k$ is chosen such that $\sim8-10\%$ of the original number of frames remain. Our approach outperforms HumanNeRF and NeuralBody across all metrics in the \textit{Full} setting. Since HumanNeRF is significantly better than Neural Body using all the frames, we only compare with HumanNeRF for sparse view setting. Our approach also outperforms HumanNeRF in the \textit{Sparse} setting for all three datasets as shown in Table~\ref{tab:all_datasets}.



% We surpass the state-of-the-art for the People Snapshot dataset by $40\%$, ZJU-MoCap dataset by $9\%$, and SCF dataset by $15\%$, across the three metrics. 
  


% We surpass the state-of-the-art in all the datasets in the sparse setting as well. Interestingly, even our method trained with sparse datasets surpass the state-of-the-art trained on the full dataset (except for one metric in the ZJU-MoCap dataset).

\noindent\textbf{Analysis of Number of Views:} To further analyze the effect of the number of views, we train various models using different number of input views. Figure~\ref{fig:views_comp} compares the performance of LPIPS metric for our method against HumanNeRF \cite{Weng2022HumanNeRFFR} with varying number of views. Our approach is better than HumanNeRF for all settings, with significant reduction in LPIPS metric as the number of views decreases.

\noindent\textbf{Quality of Canonical View:} An interesting analysis is to visualize the quality of the canonical view rendering itself. This indicates how well the model can learn deformations and fuse information from various frames. As shown in Figure~\ref{fig:analysis}, our approach can produce significantly higher quality of rendering for canonical view, thanks to our proposed pose-independent temporal deformation.

\noindent\textbf{Quality of Rendered Training View:} We can infer how well the NeRF model is minimizing the training loss by re-rendering the trained model using the same viewpoint as input training frames. Figure~\ref{fig:render_training_frames} shows the best and worst frames for training loss minimization. Notice that for easy poses (frontal), the rendered training frame from both HumanNeRF and our method are visually similar to the original training frame. However, for challenging poses (side view), the quality and accuracy of rendered training frame is significantly higher for our method compared to HumanNeRF. Infact, in the example shown in Figure~\ref{fig:render_training_frames} (top row), HumanNeRF totally fails to render the correct pose of the training frame itself, hindering overall learning. This again highlights the pitfalls of pose-dependent learning, and indicates that our method can better transform input frames to canonical frame, thanks to the proposed cyclic consistency constraint and pose-independent temporal deformation. Moreover, our method can render neighbouring frames that are not used for training in the \textit{sparse} setting by passing the interpolated frame index. However, both our method and HumanNeRF share similar challenges generating completely unseen poses \textit{beyond} the range in the input video.

% In fact, the first row shows that the recovered pose is different 

% Another interesting analysis is to visualize the quality of the canonical view rendering itself. This

% and Neural Body \cite{Peng2021NeuralBI} for the 6 sequences of the ZJU-MoCap datasest. Our method improves the metrics compare to the state-of-the-art approaches across sequences.

\noindent\textbf{Qualitative Results:} Figs.~\ref{fig:mocap} and~\ref{fig:itw} show qualitative results for rendering from novel view-points for the ZJU-MoCap and SCF datasets respectively. Our approach renders higher quality novel-views compared to HumanNerf (on faces, buttons on t-shirts, etc.).

\noindent\textbf{Ablations}: Table~\ref{tab:ablation} shows the effect of removing various modules from our full approach. We observe that all the proposed losses and constraints contribute to the performance improvement.

