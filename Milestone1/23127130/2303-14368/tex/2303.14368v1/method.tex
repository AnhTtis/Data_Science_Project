\begin{figure*}[t]
  \centering
   \includegraphics[width=0.8\linewidth]{figs/supp/overviews.png}
   \caption{Overview of our approach. Pose-independent temporal deformation is used in conjunction with pose-dependent motion fields (rigid and non-rigid). We choose  one of the input frames as the canonical view, allowing us to use cyclic-consistency for regularization. }
   \label{fig:overview}
   \vspace{-1.25em}
\end{figure*}

Given a sequence of frames of a monocular video with a human manifesting complex motions, our goal is to achieve photo-realistic free-viewpoint rendering and reposing. We choose a frame as the canonical configuration (\eg the mid-point of the motion sequence) and learn it via: a) pose-dependent (rigid and non-rigid) motion fields and b) pose-independent temporal deformations. 

\subsection{Pose-Dependent Motion Fields}

Given a canonical pose-configuration $p^c = (J^c,\Omega^c)$ and the observed pose $p = (J,\Omega)$, where $\Omega$ represents the local joint rotations and $J$ represents the joint locations in 3D, we define a pose-guided motion field mapping between the observed and canonical spaces. We first compute the transformation $M_k(p^c, p)$, and hence the translation $t_k$ and rotation $R_k$ matrices between the joint coordinates in observed and canonical spaces, for a given body part $k$. $Y(w_i, j_i)$ computes the exponent of the local joint rotation $w_i$ of the joint location $j_i$ using the Rodrigues's formula \cite{Sorgi2011TwoViewGE},
\begin{align}
    Y(\omega_i, j_i) = \prod_{i \in \tau(k)} \begin{bmatrix} exp(\omega_i) & j_i \\ 0 & 1 \end{bmatrix},
\end{align}
where $\tau(k)$ denotes the ordered set of parents of the $k^{\text{th}}$ local joint. Subsequently, we compute the corresponding translation $t_k$ and rotation $R_k$ matrices,
\begin{align}
    \vspace{-3mm}
    M_k(p^c, p) = Y(\omega_i^c, j_i^c) \left\{ Y(\omega_i, j_i)  \right\}^{-1} = \begin{bmatrix} R_k & t_k \\ 0 & 1 \end{bmatrix}.
    \vspace{-3mm}
\end{align}

Given the translation and rotation matrices, we compute the rigid deformation $x_R$ between the observed and canonical spaces by defining
\begin{equation}
\vspace{-0.25em}
    \label{eq: l_x}
    \mathcal{L}(x) = \sum^K_{k=1} w^c_k (R_kx+t_k),
    \vspace{-0.25em}
\end{equation}
% In this CNN, the output of the first fully-connected layer with $1024$ units is reshaped in to $4$ dimensions, followed by $5$ transposed convolutions layers with LeakyReLU activation \cite{Xu2020ReluplexMM}, producing the motion weight volume $W^c(x) \in \mathbb{R}^4$.
% 
which represents the likelihood that the position $x$ is a part of the subject. We obtain the set of blend weight volumes in the canonical space $\{w_c^k\}^K_{k=1}$, where $K$ is the total number of 3D joint locations. To this end, starting from a constant random latent vector $z$, we generate the motion weight volume $W^c(x) = CNN_{\theta_{R}}(x;z) \in \mathbb{R}^4$ by optimizing the parameters $\theta_R$ of the $CNN_{\theta_R}$~\cite{Weng2022HumanNeRFFR}. We add a computed approximate Gaussian bone volume as a motion weight volume prior to the output of the last transposed convolution layer before activation. Subsequently, we compute the rigid deformation $x_R$ with the obtained $\mathcal{L}(x)$ and $W^c(x)$,
\begin{equation}
    \vspace{-1mm}
    \label{eq: x_r}
    x_{R}=\frac{\sum_{k=1}^{K}w^c_k(R_kx^o+t_k)^2}{\mathcal{L}(x)}.
\end{equation}

% The likelihood function $\mathcal{L}(x)$ and the rigid deformation $x_R$ defined in Eqn.~\ref{eq: l_x} and~\ref{eq: x_r} can be collectively represented by the rigid transformation $G$, where $(\mathcal{L}(x), x_{R}) = G(W^c(x), x)$. 
The non-rigid deformation between the observed and canonical spaces is then computed as a pose-guided offset $\delta x_{NR}$ to the rigid deformation $x_R$. We feed the positional encoding $\tau(x_R)$ to the non-rigid motion MLP as,
\begin{equation}
\vspace{-1mm}
    \delta x_{NR} = MLP_{\theta_{NR}}(\gamma(x_{R});\Omega).
\end{equation}

We follow the approach defined in \cite{Mildenhall2020NeRFRS} to obtain the positional encoding $\tau(x)$ of the position $x$. The non-rigid motion MLP consists of six fully-connected layers with the positional encoding $\tau(x_R)$ and the local joint rotations $\Omega$ (without global rotation) as the inputs with $\tau(x_R)$ skip-connected to the fifth layer to generate the offset. Since the initial pose estimate $p$ obtained from off-the-shelf techniques such as SPIN \cite{Kolotouros2019LearningTR} or VIBE \cite{Kocabas2020VIBEVI} can be erroneous, we perform a pose correction following \cite{Weng2022HumanNeRFFR}. 
\vspace{-0.25em}
\subsection{Pose-Independent Temporal Deformation}

We strategically set the canonical configuration to an observed frame in the training set, allowing us to access observed ($x^o$) and canonical ($x^c$) positions as a source of information. Furthermore, having a common canonical anchor when learning a dynamic setting ensures that the scene is inter-connected across frames and no longer independent between time instances, which is intuitive and known to provide quality performance \cite{Pumarola2021DNeRFNR}. This grounding aids the model to learn preserving temporal consistency of the dynamic scene. Nevertheless, such an approach which optimizes a canonical time configuration does not work well alone for free-viewpoint rendering where we are required to render a $360^0$ camera path with complex motion. Hence, we utilize a combined approach of pose-guided and pose-independent (time-guided) canonical configuration optimization. Results in Sec.~\ref{sec:results} show that the combined approach allows high quality photorealistic rendering with sparse views. 

% Instances with such sparse views include rapid motion with only one frame available per view (i.e. the subject is only rotating once). 

We compute the pose-independent temporal deformation between a point position in the observation space $x^o$ to the canonical space $x^c$ with a temporal deformation MLP, similar to D-NeRF \cite{Pumarola2021DNeRFNR}. This temporal deformation $\Delta x_T$ is defined by,
\begin{equation}
    \vspace{-1mm}
    \Delta x_{\scriptscriptstyle T} = MLP_{\theta_{TD}}(\gamma(x^{o}),\gamma(x^{c}); (t^o, t^c)),
\end{equation}
where $t^o$ is the observed time stamp defined by $(t^o = \tau(v_t^o))$, and $t^c$ is the canonical time stamp defined by $(t^c = \tau(v_t^c))$. $v_t \in R^5$ is a learnable vector representation initialised proportional to the frame sequence index of the monocular video. 

%Learnable \tau(v_t^o), instead of v_t^o.
In contrast to D-NeRF \cite{Pumarola2021DNeRFNR}, we set the temporal vectors to be learnable due to several reasons. Even though the progression of frame sequence indices are linear, the progression of temporal information throughout a video is highly non-linear. For instance, there can be rapid motion between two consecutive frames in a video, whereas there can be no motion between another two consecutive frames in the same video. Hence, it is not intuitive to allocate a linear representation to the temporal vectors $\{v_t\}$. Furthermore, albeit being a contrasting approach to ours, DyNeRF \cite{Li2022Neural3V} presents strong evidence that trainable (latent) codes can better handle complex scene dynamics such as large deformations and topological changes. We heavily regularize the training of these temporal vectors in order to ensure that they are contained within practical limits.

The temporal deformation MLP, $MLP_{\theta_{TD}}$ consists of 8 fully connected layers with the positional encoded temporal vectors $\tau(v_t^o))$ and $\tau(v_t^c))$, and the positional encoded point position vectors $\tau(x^o)$ and $\tau(x^c)$ as inputs. The observed encodings $\tau(v_t^o))$ and $\tau(x^o)$ are skip connected to the fifth layer to generate the deformation $\Delta x_T$. Finally, we aggregate the pose-guided rigid motion $x_R$, pose-guided non-rigid motion $\delta x_{NR}$ (as an offset to $x_R$), and the pose-independent temporal deformation $\Delta x_T$ to produce the predicted canonical configuration $\hat{x}^c$,
\begin{equation}
    \hat{x}^c = \underbrace{(x_R+\delta x_{NR})}_{\substack{\text{pose-guided motion}\\\text{field}}}  \ + \underbrace{\Delta x_T}_{\substack{\text{pose-independent}\\\text{temporal deformation}}}
\end{equation}

\subsection{Cyclic Consistency}

Having obtained the predicted canonical configuration $\hat{x}^c$, we predict the RGB color $c$ and the density $\sigma$ at a given spatial location. Rather than directly predicting ($c, \sigma$) from the canonical space similar to the existing approaches \cite{Weng2022HumanNeRFFR, Pumarola2021DNeRFNR, Xu2021HNeRFNR}, we propose to break the prediction process in two steps: a) transformation from canonical ($\hat{x}^c$) to observed ($\hat{x}^o$) space, and b) $(c, \sigma)$ prediction from $\hat{x}^o$. 

The proposed approach yields the opportunity to enforce a cyclic consistency constraint (observed $x^o$ $\rightarrow$ canonical $\hat{x}^c$ $\rightarrow$ observed $\hat{x}^o$) on the output of the canonical to observed transformation MLP, $\hat{x}_o = MLP_{\theta_{CO}}(\gamma(\hat{x}^{c}))$. Furthermore, having two separate specialized networks rather than one network to map from the rays in the canonical space to $(c,\sigma)$ in the observation space is more flexible and is empirically more effective as shown in Sec. \ref{sec:results}. 

The $MLP_{\theta_{CO}}$ has a similar architecture to $MLP_{\theta_{TD}}$, without the temporal vectors as inputs. The subsequent scene representation MLP, $(c,\sigma) = MLP_{\theta_{c}}(\gamma(\hat{x}^{o}))$ has a similar architecture to the network proposed in \cite{Mildenhall2020NeRFRS}. Prior to feeding $\hat{x}^c$ and $\hat{x}^o$ to the corresponding networks, each vector is positional encoded.
% $(c,\sigma) = MLP_{\theta_{o}}(\gamma(\hat{x}^{o}))$
\input{results_figs_mocap}

\subsection{Volume Rendering and Refinement Network}

We follow the volume rendering approach described in NeRF \cite{Mildenhall2020NeRFRS} by defining the expected alpha (density) mask $\mathcal{A}(r)$ and the expected color $\mathbf{C}(r)$ for a give ray $r$,
\begin{align}
    \mathcal{A}(r) &= \sum^{D}_{i=1} \left\{\prod^{i-1}_{j=1}(1-\alpha_j)\right\}\alpha_i \\
    C(r) &= \sum^{D}_{i=1} \left\{\prod^{i-1}_{j=1}(1-\alpha_j)\right\}\alpha_i c(x_i) \\
    \alpha_i &= \mathcal{L}(x_i)\{1-\text{exp}(-\sigma(x_i)\Delta z_i)\},
\end{align}
where $D$ is the number of samples, and $\Delta z_i$ is the interval between consecutive samples. We employ the same stratified sampling approach described in \cite{Mildenhall2020NeRFRS}. 
% With the aid of the computed  $\mathbf{C}(r)$ and $\mathrm{A}(r)$, we render the resulting image in the observed RGB space $\hat{I}^o$.  

To further enhance the photorealism of the rendered images, we use a refinement network $\hat{I}_o = CNN_{\theta_{FT}}(C(r),\mathcal{A}(r))$ to add fine-grained details to the rendered image, similar to latent diffusion approaches \cite{Rombach2022HighResolutionIS}. The refinement network $CNN_{\theta_{FT}}$ consists three transposed convolution layers and outputs the final rendered image $\hat{I}_o$.

%Can be extended to up-sample the outputs, if ground truth is available. 

% With the aid of the computed $\mathbf{C}(r)$ and $\mathrm{A}(r)$, the rendered image is fed to 

% The refinement network $CNN_{\theta_{FT}}$ is built using three transposed convolution layers and outputs the final rendered image $\hat{I}_o $.


\subsection{Rendering Segmentation Mask}
The segmentation masks for the input frames can be obtained using an off-the-shelf segmentation network~\cite{He2020GrapyMLGP}. We use them to apply an additional loss to improve the density estimation. Note that rendering $\mathcal{A}(r)$ results in the predicted segmentation $\hat{M} =\mathcal{A}(r)$, which is compared against the real segmentation mask $M$. This helps to eliminate the halo effects~\cite{Weng2022HumanNeRFFR, Peng2021NeuralBI} and provide sharper boundaries. Empirically, we observed that thresholding the predicted segmentation mask, $\hat{M} =\mathcal{A}(r)H(\mathcal{A}(r),b)$ works better, where $b$ is a threshold value and \begin{equation} 
    H(\mathcal{A}(r),b)=\begin{cases} 1 \ \text{if} \ \mathcal{A}(r) > b \\ 0 \ \text{otherwise.} \end{cases}
    \label{eq: thresh_raw}
    \vspace{-0.25em}
\end{equation}

However, using a fixed threshold $b$ makes learning difficult at the start of training. To ease learning, we make $b$ a learnable parameter and re-define
$\hat{M} =(\mathcal{A}(r)+b)H(\mathcal{A}(r),b)$, so that it is differentiable with respect to $b$. We observe that $b$ goes to $0$ as training progresses, as in the ideal case. Compared to previous approaches such as \cite{Jiang2022NeuManNH}, this does not require us to depend on estimated depths, which could themselves be erroneous due to complex non-rigid motions.


% To this end, we propose a novel thresholding function $H$ with a learnable threshold $b$ for the second and final layer of $MLP_{\theta_{BS}}$,

% \begin{equation}
%     H(\mathcal{A},b)=\begin{cases} 1 \ \text{if} \ \mathcal{A}(r) > b \\ 0 \ \text{otherwise.} \end{cases}
%     \label{eq: thresh_raw}
% \end{equation}

% It is critical that the proposed thresholding function should be: a) differentiable: in order for the threshold $b$ to be learnable and the $\mathcal{A}(r)$ and $\sigma$ to be updated, as well as to make use of the error signal generated from the predicted segmentation mask, the thresholding function must be differentiable and b) has regularized $b$: when the model is perfectly trained, the threshold should ideally be $0$. Hence, we randomly initialize the learnable threshold with a smaller value, and regularize it towards $0$. 

% Simple thresholding as shown in Eq. \ref{eq: thresh_raw} is non-differentiable with respect to $b$. To this end, we propose the following modification,

% \begin{equation}
%     \label{eq: thresh_mod}
%     \hat{M} = (\mathcal{A}(r)+b)*H(\mathcal{A}(r),b).
% \end{equation}

% %initial training is hard. 

% Consider the function $(\mathcal{A}(r)+x)*H(\beta(r),y)$, where $x,y$ are random variables. Due to the nature of $H$, the function is non-differentiable with respect to $y$, but differentiable with respect to $x$. However, since $y$ is implicitly following $x$, we set it to the threshold $b$, making Eq. \ref{eq: thresh_mod} differentiable with respect to $b$. 

% Furthermore, updating ${\alpha_i}$ set should be updated accordingly, in order for the backpropagation update loop to be completed, an important feature missing in approaches that attempt to leverage predicting segmentation mask \cite{Jiang2022NeuManNH}. Without this ${\alpha_i}$ correction, the full value of the binary segmentation error signal is not reaped. 

%Depth value sampling can be erraneous, RGB and mask are aligned to render with color and without color. 

% \subsection{Rendering re-posed continuous videos}

% FlexNeRF has the ability to render continuous re-posed videos starting from a given monocular video. We update the observed pose sequence $\{p^o_i\}$ with either an arbitrary perturbation $\{\eta_i\}$ or given a target pose sequence, with the deformation from the observed to the target pose sequences  $\{\delta p^o_i\}$. We re-render the observed frames by querying with the perturbed poses $\{p^o_i+\eta_i\}$ or $\{p^o_i+\delta p^o_i\}$ to produce the re-posed continuous video. Furthermore, given an outlier target pose from the observed pose sequence $\{p^o_i\}$, we can interpolate the pose transition from the outlier pose to the observed sequence, to query a continuous smooth-transitioning video to re-render the video starting from an arbitrary pose.