\documentclass[letterpaper, 10 pt, journal, twoside]{IEEEtran}
\pdfminorversion=4
\usepackage[noadjust]{cite}
\let\proof\relax
\let\endproof\relax
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithm}
\usepackage[algo2e]{algorithm2e}
\usepackage{algorithmic}
\usepackage{wasysym,xcolor}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{graphicx,float}
\usepackage{capt-of}
\usepackage{textcomp,soul,comment}
\usepackage{pgfplots}
\pgfplotsset{compat=1.10}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\anna}[1]{{\color{teal} #1}}
\newcommand{\rahel}[1]{{\color{blue} #1}}
\newcommand{\JK}[1]{{\color{magenta} #1}}

\newtheorem{remark}{Remark}

%\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}


\allowdisplaybreaks

\begin{document}
\title{Active Learning-based \\
Model Predictive Coverage Control}
\author{Rahel Rickenbach, Johannes K\"ohler, Anna Scampicchio, Melanie N. Zeilinger, Andrea Carron
\thanks{
All authors are members within the Institute for Dynamic Systems and Control (IDSC), ETH Z{\"u}rich.
{\tt\footnotesize [rrahel|jkoehle|ascampicc|\\mzeilinger|carrona]@ethz.ch}}}

\maketitle

\begin{abstract}
The problem of coverage control, i.e., of coordinating multiple agents to optimally cover an area, arises in various applications. 
However, existing coverage algorithms face two major challenges: (1) dealing with nonlinear dynamics while respecting system and safety critical constraints, and (2) performing the task in an initially unknown environment. We solve the coverage problem by using a hierarchical framework, where references are calculated at a central server and passed to the agents' local model predictive control (MPC) tracking schemes.
Furthermore, a probabilistic exploration-exploitation trade-off is deployed to ensure that the environment is actively explored by the agents. In addition, we derive a control framework that avoids the hierarchical structure by integrating the reference optimization in the MPC formulation.  
Active learning is then performed drawing inspiration from Upper Confidence Bound (UCB) approaches. We guarantee recursive feasibility and convergence to an optimal configuration for all developed control architectures. Furthermore, all methods are tested and compared on hardware using a miniature car platform.
\end{abstract}

\begin{IEEEkeywords}
Coverage Control, NL Predictive Control, Cooperative Control, Machine Learning, Agents and Autonomous Systems%, Centroidal Voronoi Partitions.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
 
A key problem in multi-agent systems consists in optimally covering a finite area with respect to environmental demands, which  
are reflected by a density function of measurable values of interest. 
This task goes under the name of coverage control, and it can be rephrased as placing the agents at the centroids of their Voronoi partition induced by the density function~\cite{Du1999}. Applications are versatile. One is, e.g.,  
the autonomous re-positioning of self-driving taxis according to the population density, providing a faster and more environmentally friendly service. Another example, illustrated in Figure~\ref{fig:explanatoryfigure}, are firefighting planes that are autonomously and optimally distributing themselves with respect to the heat map of a certain area.
From these examples, the challenges that emerge are twofold and intertwined. The first is designing a coverage control architecture for agents with nonlinear dynamics while ensuring collision avoidance and respecting safety constraints. The second consists in dealing with initially unknown environments, i.e., unknown density functions characterizing the optimal coverage problem. In this case, the considered area needs to be explored during the process relying on the agents' sensing capabilities. 
\begin{figure} [t]
\centering
\includegraphics[width=0.45\textwidth]{figures/coverage_control_explained_9-compressed.pdf}
\caption{Illustration of coverage control problem and partitioning of environment at the example of firefighting planes and environmental demands defined by the resulting heat map.}
\label{fig:explanatoryfigure}
\vspace{-1.2em}
\end{figure} 
The goal of this paper is to design a coverage control framework that addresses both of the aforementioned challenges. 

\subsubsection*{Related Work}
\label{subsubsec:relatedwork}
In classical works such as~\cite{Cortes2004,Bullo2012}, the coverage control problem is rigorously solved under the assumptions that the environment is perfectly known and that dynamics are single integrators. Nonlinear dynamics and state/input constraints can be taken into account by using a  model predictive control (MPC) scheme~\cite{Grune2011,rawlings2017model}, which was pursued in~\cite{Carron2017,Kohler2018,Farina2015}. In all the aforementioned references, the implemented coverage algorithm is hierarchical, i.e., a reference is calculated before being passed to a tracking MPC~\cite{Limon2018}. Moreover, they all require some non-trivial offline design for the terminal ingredients in MPC to prove convergence and recursive feasibility. Furthermore, none of them addressed coverage control in an unknown environment.\\
To cope with this second challenge, ``exploitation" targeted to the coverage control task needs to be combined with ``exploration" given by data collection performed via active learning~\cite{Li2006,Campbell1990} to improve the estimate of the initially unknown density function. The problem of coverage control in an unknown environment is investigated in~\cite{Schwager2009,Todescato2017,McDonald2021,prajapat2022}, where different strategies to balance exploration and exploitation are proposed. 
However, in these works the agents' dynamics are once again assumed to be single integrators. To the best of the authors' knowledge, the general coverage control problem encompassing an unknown environment and nonlinear constrained dynamics has not yet been addressed in the literature. 

\begin{figure} [t]
\centering
\includegraphics[trim={0cm 0cm 0.8cm 0cm},clip,width=0.45\textwidth]{figures/coverage_clarification_diagram_vert-compressed.pdf}
\caption{Simplified illustration of the developed two- and one-layers algorithm. Considering density $\phi$, partitions $\mathbb{O}$, references $r$, state $x$, inputs $u$, positions $p$, as well as setpoint positions $\bar{p}$.} 
\label{fig:coverageclarification}
\vspace{-1.5em}
\end{figure}

\subsubsection*{Contributions}
\label{subsubsec:contributions}
We propose an MPC-based framework to perform coverage control under nonlinear dynamics and constraints, and extend it to consider an unknown environment. In particular, we first consider a coverage control strategy inspired by~\cite{Carron2017}, passing references, calculated by the server, to each agent's individual tracking MPC~\cite{Limon2018}. In this way, the multi-agent architecture is assumed centralized when computing the partitions of the considered area, but decentralized to calculate each agent's control policy. In the remainder of the paper, we will refer to this implementation strategy as ``two-layers approach''. A visual explanation is presented in the scheme on the left of Figure~\ref{fig:coverageclarification}. In order to facilitate application to more complex, nonlinear systems, we use the tools in~\cite{Grune2011,Soloperto2021} to derive a tracking MPC scheme that does not rely on offline designed terminal ingredients. Moreover, thanks to this predictive control set-up we are able to directly include collision avoidance constraints. We extend this approach to an initially unknown environment by leveraging the probabilistic exploration-exploitation decision
proposed in~\cite{Todescato2017} and include it in the hierarchical control set-up. \\
We also demonstrate how to simplify and overcome the hierarchical structure of the latter and directly integrate the calculation of the next optimal configuration into the MPC cost of each agent. To the best of the authors' knowledge, this has not been addressed in the literature. In the following, we refer to this non-hierarchical method, schematized on the right of Figure~\ref{fig:coverageclarification}, as ``one-layer approach". Moreover, to provide a potentially more efficient exploration behaviour, we address unknown environments by using an Upper Confidence Bound (UCB) approach, similar to Bayesian optimization methods~\cite{Auer2002,Srinivas2010}.\\
We prove recursive feasibility, constraint satisfaction and convergence to an optimal configuration for both control architectures (one-layer and two-layers, Figure~\ref{fig:coverageclarification}) and for both known and unknown environments. 
Moreover, all approaches are tested and compared on hardware, using the miniature racing cars Chronos in combination with CRS, an open-source software framework for control and robotics~\cite{carron2022,Froehlich2021,Tearle2021}%\footnote{An accompanying video detailing the contributions of the paper is available at the following link: \url{https://youtu.be/ej0WAizJKVM}}.

\subsubsection*{Outline}
\label{subsubsec:roadmap}
The remainder of the paper is structured as follows. Section~\ref{sec:problemFormulation} states the coverage control problem. This is followed by a presentation of  preliminaries regarding MPC and Bayesian learning in Section~\ref{sec:controlandlearningframework}.  Sections~\ref{sec:twolayers} and~\ref{sec:onelayer} detail the proposed two-layers and one-layer methods, respectively. In addition, we state the main theoretical results on convergence, satisfaction of safety-critical constraints and recursive feasibility for both set-ups with known and unknown environment. The proofs are deferred to the Appendix. Section~\ref{sec:experiments} gathers all the experimental results, and Section~\ref{sec:conclusions} ends the paper by discussing the benefits of the proposed approaches and drawing conclusions.
 
\subsubsection*{Notation}
\label{subsubsec:notation}
Throughout the paper, $\Vert \! \cdot \! \Vert$ indicates the Euclidean norm and $\mathcal{N}$ the normal distribution. The set of all non-negative real numbers is given by $\mathbb{R}_{+}$ and the set of natural numbers by $\mathbb{N}$. We define the ball $\mathbb{B}_{r}^{b} = \{x \in \mathbb{R}^b\vert \,\Vert x \Vert\leq r \}$ and indicate with $\ominus$ the Pontryagin difference.  Further, we consider some arbitrary small but fixed constant $\epsilon > 0$ and for each set $\mathbb{V}\subset\mathbb{R}^b$ we denote $\mathbb{V}^{\mathrm{int}}:= \mathbb{V} \ominus \mathbb{B}_{\epsilon}^{b}$. Given a compact set $\mathbb{D}\subset\mathbb{R}^b$ and a  continuous function $f: \mathbb{D} \rightarrow \mathbb{R}$, we define
\begin{equation}
    f(s)_{\mathbb{D}} := \min_{\tilde{s} \in \mathbb{D}}f(s - \tilde{s}).
    \label{eq:minsetdistance}
\end{equation}

\section{Problem Formulation}
\label{sec:problemFormulation} 
This section is devoted to the statement of the coverage control problem, together with the specification of the nonlinear constrained dynamics that are considered in the paper.

\subsection{Dynamics and Constraints}
We consider $M$ agents on a convex area \mbox{$\mathbb{A}\in \mathbb{R}^{D}$} and the agents are described using time-invariant, discrete-time, nonlinear dynamics. For agent $i$, the dynamics are given by
\begin{equation}
\begin{split}
    &x_{i,k+1} = f_{i}(x_{i,k}, u_{i,k}) \\
    &p_{i,k} = C_{i}x_{i,k},
\end{split}
\label{eq:nonlineardynamics}
\end{equation}
where $x_{i,k} \in \mathbb{R}^{n_{i}}$ and $u_{i,k} \in \mathbb{R}^{m_{i}}$ denote the state and input at time $k$, respectively. The vector $p_{i,k} \in \mathbb{R}^D$ indicates the position of agent $i$ at time $k$, which is usually chosen as Cartesian coordinates in D=2 or D=3. Additionally, all agents are subject to state and input constraints, i.e., $x_{i,k} \in \mathbb{X}_{i}$ and $u_{i,k} \in \mathbb{U}_{i}$ for $k \in \mathbb{N}$. Note that we consider relatively general heterogeneous dynamics for the different agents. Additionally, we make the subsequent assumption.
\begin{assumption} The agent's dynamics $f_{i}$ are assumed to be known and Lipschitz continuous with Lipschitz constant $\mathcal{L}_{i}$. Moreover, each agent's state can be measured perfectly and its state and input constraints are compact.
\label{assumption:dynamics}
\end{assumption}

\subsection{Coverage Control}
\label{subsec:coverage}
We indicate the agents' position with \mbox{$p = [p_{1}, \hdots, p_{M}]^{T} \in \mathbb{R}^{M \times D}$} and denote with $\mathbb{O} = \{\mathbb{O}_{1}, \hdots, \mathbb{O}_{M}\}$ an arbitrary collection of polytopes partitioning $\mathbb{A}$. The problem of coverage control can be mathematically stated as the following optimization problem:
\begin{equation}
\begin{split}
    \min_{p,\,\mathbb{O}} \:\underbrace{\sum_{i=1}^{M} \int_{\mathbb{O}_{i}} g(\Vert q - p _{i} \Vert)\phi(q)dq}_{H(p,\mathbb{O})},\, \ p_{i} \in \mathbb{O}_{i},
\end{split}
\label{eq:cortescost}
\end{equation}
where $\phi: \mathbb{A} \rightarrow \mathbb{R}_+$ is the so-called density function which acts as a measure of information on the environment~$\mathbb{A}$, and \mbox{$g$: $\mathbb{R}_{+} \rightarrow \mathbb{R}_{+}$} is an arbitrary function representing the agents' sensing capability that needs to be continuous, non-decreasing, and is typically chosen as the quadratic function. The function $H(p, \mathbb{O})$ is known as the locational optimization cost. It was shown in~\cite{Du1999} that the optimal set  $\mathbb{O}$ in~\eqref{eq:cortescost} is given by the Voronoi tessellation~\cite{Senechal1995} denoted by \mbox{$\mathbb{W}_{p} = \{\mathbb{W}_{p,1}, \hdots, \mathbb{W}_{p,M}\}$}, with
\begin{equation}
\begin{split}
    \mathbb{W}_{p,i} = \{ q \in \mathbb{A} \; \vert\; \Vert q - p_{i} \Vert \leq \Vert q - p_{j} \Vert, \, \forall j \neq i \},
\end{split}
\label{eq:voronoidef}
\end{equation}
and each agent's optimal position is given by the centroids $c(\mathbb{W}_{p}, \phi) = [c_1(\mathbb{W}_{p,1}, \phi), \hdots, c_M(\mathbb{W}_{p,M}, \phi)]$, with
\begin{equation}
\begin{split}
     c_i(\mathbb{W}_{p,i}, \phi) = \left(\int_{\mathbb{W}_{p,i}}\phi(q)dq\right)^{-1} \left(\int_{\mathbb{W}_{p,i}}q\phi(q)dq\right).
\end{split}
\label{eq:voronoicenterdef}
\end{equation}
The optimal solution in terms of both partition and positions is called centroidal Voronoi configuration. Note that all the sets in $\mathbb{W}_{p}$ are convex provided that $\mathbb{A}$ is convex~\cite{Du1999}. The coverage control problem can hence be reduced to ensuring $p$ converges to $c(\mathbb{W}_{p}, \phi)$ with $\mathbb{W}_{p}$ according to~\eqref{eq:voronoidef}. Classically, for integrator dynamics this is achieved using the Lloyd algorithm~\cite{Lloyd1982}, which consists in an iterative update of the form $p_{i, k+1} = c(\mathbb{W}_{p_{k},i}, \phi)$. We study this problem for more general dynamics, and both for known environments, i.e., a known density~$\phi$, and in case $\phi$ is unknown and needs to be learned online.

\subsection{Collision Avoidance}
\label{subsec:collisionavoidance}
To prevent agents from colliding with each other, it needs to be ensured that, for all time instances $k \in \mathbb{N}$, 
\begin{equation}
\Vert p_{i,k} - p_{j,k}\Vert \geq (r_{i,\max} + r_{j,\max}), \ \forall i\neq j, 
\label{eq:colavoidancerequirement}
\end{equation}
where $r_{i,\max}$ indicates a radius that covers its respective agent. For later use, we define
$r_{\max} = \max\{r_{1,\max},\hdots,r_{M,\max}\}$.
The developed collision avoidance strategy will deploy 
the constructed Voronoi partitions~\eqref{eq:voronoidef}. 

\vspace{-0.7em}
\section{Control and Learning Framework}
\label{sec:controlandlearningframework}
In this section, we introduce the core control and learning tool of the developed methods that allow for a safe coverage movement while respecting the 
given dynamics. Therefore, we recall a nonlinear tracking MPC formulation and complement such a set-up with 
collision avoidance constraints based on the Voronoi partition (Section~\ref{subsec:nonlineartrackingmpc}). Additionally, we define the Bayesian linear regression (BLR) strategy that will be deployed in all the learning-based approaches to deal with an unknown environment (Section~\ref{subsec:measurementcol}).

\subsection{Nonlinear Tracking MPC}
\label{subsec:nonlineartrackingmpc}

To allow for the inclusion of state, input and collision avoidance constraints, each agent $i$ is controlled by a nonlinear tracking MPC~\cite{Limon2018,Soloperto2021}.
Its cost reads as
\begin{equation}
\begin{split}
&J_{i}(x_{i,\cdot \vert k}, u_{i,\cdot \vert k}, s_{i,k}, r_{i,k}) \\ & = V_{N,i}(x_{i,\cdot \vert k},u_{i,\cdot \vert k},s_{i,k}) + \ell_{T,i}(\bar{p}_{i,k} - r_{i,k} )\\
& = \sum_{l=0}^{N-1}\ell_{i}(x_{i,l\vert k}, u_{i,l\vert k}, s_{i,k}) + \ell_{T,i}(\bar{p}_{i,k} - r_{i,k} ),
\end{split}
\label{eq:generalnonlineartrackingmpccost}
\end{equation}
where we use the symbol ``$\cdot$" to denote all values of the index $l=\{0,\dots,N-1\}$. Similar to standard MPC~\cite{Grune2011}, the tracking cost~$V_{N,i}$ sums a continuous stage cost $\ell_i$ over a finite horizon~$N$. Thereby, the stage cost should steer the system to the aritifical setpoint~$s_{i,k} = (\bar{x}_{i,k},\bar{u}_{i,k}) \in \mathbb{S}_{i}$, with
\begin{equation}
\begin{split}
    \mathbb{S}_{i} \! = \! \{(x,u) \! \in \! \mathbb{R}^{n_{i}+m_{i}}\,\vert\, x \! \in \mathbb{X}^{\mathrm{int}}_{i}, u \! \in \mathbb{U}^{\mathrm{int}}_{i}, x \! = \! f_{i}(x,u) \},
\end{split}
\label{eq:steadystateset}
\end{equation} which enables the tracking of piece-wise constant references~$r_{i,k}$. The according steady-state position is obtained as \mbox{$\bar{p}_{i,k} = C_{i}s_{i,k}$} and its difference to the reference is penalized in the continuous target cost $\ell_{T,i}$. To ensure~\eqref{eq:colavoidancerequirement}, we consider the current Voronoi partitions, indicated by $\mathbb{W}$, and define \mbox{$\bar{\mathbb{W}}_{i} := \mathbb{W}_{i} \ominus \mathbb{B}_{r_{\max}}^D$}  for all $i = \{1,\hdots,M\}$. Then, for all $i \neq j$ and all $p_{i} \in \bar{\mathbb{W}}_{i}, p_{j} \in \bar{\mathbb{W}}_{j}$, inequality~\eqref{eq:colavoidancerequirement} holds. \\
Consequently, a set of  position and steady-state constraints is added. The first can be encoded as $p_{i,k} \in \bar{\mathbb{W}}_{i}$, while the latter is given by modifying the set of setpoints as follows:
\begin{equation}
\begin{split}
    \mathbb{S}_{\mathbb{W}_{i}} = & \{(x,u) \in \mathbb{S}_{i}\;\vert \; C_{i}x \in \bar{\mathbb{W}}_{i}^{\mathrm{int}} \},
\end{split}
\label{eq:steadystatesetwithcollavoidance}
\end{equation}
and define its projection on the position space as 
\begin{equation}
\begin{split}
    \mathbb{S}_{\mathbb{W}_{i}}^{\mathrm{p}} = & \{p \in \mathbb{W}_i^{\mathrm{int}} \;\vert \; p = C_{i}x, \forall x : \exists u \text{ s.t. } (x,u) \in \mathbb{S}_{\mathbb{W}_{i}}\} \notag.
\end{split}
\label{eq:steadystatesetwithcollavoidancepositionspace}
\end{equation}
The resulting MPC problem, of agent $i$ at time $k$, given state~$x_i$, reference~$r_i$ and partition $\mathbb{W}_i$, reads as
\begin{subequations}
\begin{align}
\min_{x_{i,\cdot \vert k}, u_{i,\cdot \vert k},s_{i,k}}&J_{i}(x_{i,\cdot \vert k}, u_{i,\cdot \vert k}, s_{i,k}, r_{i,k}) \label{eq:generalcostwithcolavoidance} \\ &\qquad x_{i,0\vert k} = x_{i,k} \label{eq:generalinit}\\
&\qquad x_{i, l+1\vert k} = f_{i}(x_{i,l\vert k}, u_{i,l\vert k}) \label{eq:generaldyn} \\
&\qquad x_{i,\cdot\vert k} \in \mathbb{X}_{i},\; u_{i,\cdot\vert k} \in \mathbb{U}_{i}  \label{eq:generalstateconst}\\
&\qquad V_{N,i}(x_{i,\cdot \vert k},u_{i,\cdot \vert k}, s_{i,k}) \leq V_{\max,i} \label{eq:generalvarbound}\\
&\qquad p_{i,l\vert k} = C_{i}x_{i,l\vert k} \in \bar{\mathbb{W}}_{i} \label{eq:generalvoronoiconst}\\
&\qquad s_{i,k} \in \mathbb{S}_{\mathbb{W}_{i}}. \label{eq:generalsteadystatesetconstwithcolavoidance} \\
& \qquad l = 1, \hdots, N-1 \label{eq:1toN-1withcolavoidance}.
\end{align}
\label{eq:nonlineartrackingmpcwithcolavoidance}
\end{subequations}
Condition~\eqref{eq:generalvarbound} imposes an additional user-chosen bound~$V_{\max,i}$ on the tracking cost $V_{N,i}$, which is a technical condition to ensure closed-loop properties under reasonable assumptions and without terminal ingredients, see~\cite{Soloperto2021,Boccia2014}. For each agent, a solution of~\eqref{eq:nonlineartrackingmpcwithcolavoidance} are the optimal state and input trajectories~$x_{i,\cdot \vert k}^{*}$ and $u_{i,\cdot \vert k}^{*}$, as well as the optimal setpoint  \mbox{$\bar{s}_{i,k}^{*} = (\bar{x}_{i, k}^{*}$, $\bar{u}_{i,k}^{*})$}. It exists, given continuity of $\ell_{i},\ell_{T,i}$, due to the compact constraints. The problem is solved in a receding horizon fashion, i.e., at every time $k$ we solve~\eqref{eq:nonlineartrackingmpcwithcolavoidance} and apply the first input of the obtained input sequence, that is $u_{i,k}=u^*_{i,0|k}$. We denote by $V_{N,i}^{*}(x_{i,k},s_{i,k}^{*},\mathbb{W}_i) = V_{N,i}(x_{i,\cdot \vert k}^{*},u_{i,\cdot \vert k}^{*}, s_{i,k}^{*})$ the optimal tracking cost for the considered partition, by $\ell^*_i(x_k,s_k^*) = \min_{u \in \mathbb{U}_{i}} \ell_{i}(x_{i,k},u,s_{i,k}^*)$ the one step optimal stage cost, and by $u^*_{x_{i,k},s_{i,k}^*}$ its minimizing input, which is assumed to be equal to $\bar{u}_{i,k}^*$.
In the following, we discuss the stability properties of the MPC scheme~\eqref{eq:nonlineartrackingmpcwithcolavoidance} using the theory in~\cite{Boccia2014}. To this end, we require a local stabilizability condition, also known as \textit{exponential cost controllability} (\cite[Assumption 1]{Boccia2014}) in the literature.
\begin{assumption} For every agent $i \in \{1,\hdots,M\}$ there exist constants $c_{i}, \gamma_{i} > 0$ such that for any $\mathbb{W}_{i} \subset \mathbb{A}$, with $\bar{\mathbb{W}}^{\mathrm{int}}_{i} \neq \emptyset$, $s \in \mathbb{S}_{\mathbb{W}_{i}}$, $N \in \mathbb{N}$ and any $x \in \mathbb{R}^{n_{i}}$ satisfying $\ell_{i}^{*}(x,s) \leq c_{i}$, we have
\begin{equation}
    V_{N,i}^{*}(x,s) \leq \gamma_{i} \ell_{i}^{*}(x,s). 
\label{eq:assumption2gammavmax}
\end{equation}
\label{assumption:expocostcontrollability}
\end{assumption}
\vspace{-1.4em}
In addition, we require that the stage cost $\ell_{i}$ is positive definite w.r.t. the setpoint $s_{i}$, which is often achieved using a quadratic stage cost $\ell_{i}$. However, in~\cite{Muller2017}, it was shown that Assumption~\ref{assumption:expocostcontrollability} cannot be satisfied for non-holonomic systems using a quadratic stage cost. To avoid this issue, we use the results in~\cite{Rosenfelder2021, Coron2020, Worthmann2016} to devise a polynomial stage cost for the experiments with non-holonomic robots (cf. Section~\ref{sec:experiments}). To properly characterize such polynomial stage costs we introduce the following distance function
\begin{equation}
d_i(\zeta) = \sqrt{\zeta_1^{a_{i,1}} + \zeta_2^{a_{i,2}} + \cdots + \zeta_{n_i+m_i}^{a_{i,n_i+m_i}}}.\label{eq:distance}
\end{equation} which uses even exponents $a_{i,j} \in \mathbb{N}, j = 1,\hdots,n_i+m_i$.
\begin{assumption} For every agent $i \in \{1,\hdots,M\}$ there are constants $\alpha_{1,i}, \alpha_{2,i} > 0$ such that, for all $x \in \mathbb{X}_{i}$, $s \in \mathbb{S}_{\mathbb{W}_{i}}$, and their respective $u^*_{x,s} \in \mathbb{U}_{i}$
, the following holds: 
\begin{equation}
\begin{split}
    \alpha_{1,i}d_{i}((x,u_{x,s}^{*}) \! - \! s)^{2} \! \leq  \! \ell_{i}^{*}(x,s) \! \leq \! \alpha_{2,i}d_{i}((x,u_{x,s}^{*}) \! - \! s)^{2}.
\end{split}
\label{eq:boundstagecosttwolayer}
\end{equation}
Moreover, there exist constants $\xi_{1,i}, \xi_{2,i} \geq 0$ such that for any two admissible setpoints \mbox{$s_{1} \! = \! (\bar{x}_{1},\bar{u}_{1})$ and $s_{2} \! = \! (\bar{x}_{2},\bar{u}_{2})$ $\in \mathbb{S}_{\mathbb{W}_{i}}$,}
\begin{equation}
\begin{split}
    \ell_{i}(x,u,s_{1}) \leq  \xi_{1,i}\cdot \ell_{i}(x,u,s_{2}) + \xi_{2,i}\cdot d_{i}(s_{1}-s_{2})^{2}.
\end{split}
\label{eq:boundtwosteadystatestwolayer}
\end{equation}
\label{assumption:boundedbyd}
\end{assumption}
\vspace{-1.3em}
\begin{remark}\label{remark:commentAssumption23}
By applying recursively the argument in~\cite[Section A]{Soloperto2021}, it is easily shown that Assumption~\ref{assumption:boundedbyd} holds for polynomial stage costs with only even exponents.\label{remark:remarkgammamax}
\end{remark}
\noindent Given these requirements, it is proven in~\cite{Boccia2014,Soloperto2021} that, for a sufficiently long prediction horizon, the tracking cost is a valid Lyapunov function with respect to a fixed steady state.
\begin{theorem}\cite[Theorem 4]{Boccia2014} Let Assumptions~\ref{assumption:expocostcontrollability} and~\ref{assumption:boundedbyd} hold. Then, for any $\mathbb{W}_{i}\subset \mathbb{A}$, with $\bar{\mathbb{W}}^{\mathrm{int}}_{i} \neq \emptyset$, any $V_{\max,i} > 0$ and any $\alpha_{N,i} \in (0,1)$, there exists a horizon $N^{*} \in \mathbb{N}_{\geq 0}$ such that for all $k \in \mathbb{N}$, $N \geq N^{*}$, $s_k \in \mathbb{S}_{\mathbb{W}_{i}}$ and all $x_k$ with $V_{N,i}^{*}(x_{k},s_k) \leq V_{\max,i}$ 
\begin{equation}
\begin{split}
    &V_{N,i}^{*}(f_i(x_{k},u_{0\vert k}^{*}),s_k^*,\mathbb{W}_i) - V_{N,i}^{*}(x_{k},s_k^*,\mathbb{W}_i) \\ &\leq  - \bar{\alpha}_{N,i} \ell_{i}^{*}(x_{k},s_k^*).
\end{split}
\label{eq:theorem5twolayer}
\end{equation}
\label{theorem:theo4boccia}
\end{theorem}
\vspace{-1.0em}
\noindent Note that the developed approaches, see Figure~\ref{fig:coverageclarification}, will use a Voronoi partion update to ensure a coverage cost decrease. However, feasibility of the nonlinear tracking MPC explicitly depends on the partition $ \bar{\mathbb{W}}_{i}^{\mathrm{int}}$: hence, any update might cause infeasibility of optimization problem~\eqref{eq:nonlineartrackingmpcwithcolavoidance} or invalidate the closed-loop properties in Theorem~\ref{theorem:theo4boccia}. A procedure for preserving feasibility under the time-varying collision avoidance constraints, i.e., taking Voronoi partition updates into account, is introduced in the following. For this purpose, let $\mathbb{W}'$ be the candidate Voronoi update and consider the following input and state sequences
\begin{subequations}
    \begin{align}
    \hat{u}_{i, \cdot \vert k} = &[u^{*}_{i,1\vert k}, \hdots, u^{*}_{i,N-2\vert k}, \bar{u}_{i,k}^{*}, \bar{u}_{i,k}^{*}]^{\top}\label{eq:uhat}, \\
    \hat{x}_{i, \cdot \vert k} = &[x^{*}_{i,1\vert k}, \hdots, x^{*}_{i,N-1\vert k}, f(x^{*}_{i,N-1\vert k},\bar{u}_{i,k}^{*}), \label{eq:xhat} \\ &f(f(x^{*}_{i,N-1\vert k},\bar{u}_{i,k}^{*}),\bar{u}_{i,k}^{*})]^{\top}.\nonumber
    \end{align}
    \label{eq:lemma1proposal}
\end{subequations}
\noindent The idea is to only update the partitions if the candidate sequence remains feasible, i.e., if, for all $l=\{1,...,N+1\}$ and all $i=\{1,...,M\}$, 
\begin{equation}
    \hat{x}_{i,l \vert k}\in\mathbb{X}_i, \ C_{i}\hat{x}_{i,l \vert k} \in \bar{\mathbb{W}}_{i}' \ \text{and} \  C_{i}\bar{x}_{i,k}^{*} \in \bar{\mathbb{W}}'^{\mathrm{int}}_{i},
    \label{eq:feasibilitycond}
\end{equation}
which will be later used to ensure closed-loop properties (cf. proof of Theorem~\ref{theorem:twolayers} and~\ref{theorem:onelayer} in the Appendix).
To ensure that the problem is well-posed, we define a feasible configuration as a configuration $p$ for which the distance between all agents is bigger than or equal to $2(r_{\max} + \epsilon)$.
\begin{lemma}
    Take any feasible configuration $p_w$ and its according Voronoi tessellation $\mathbb{W}_{p_w}$. Then, for any configuration $p$ with $p_{i} \in \bar{\mathbb{W}}^{\mathrm{int}}_{p_w,i}, \forall i \in \{1,\hdots,M\}$, it holds that $p_{i} \in \bar{\mathbb{W}}^{\mathrm{int}}_{p,i}$
\label{lemma:inclusioninnewinterior}
\end{lemma}
\vspace{-0.0em}
\emph{Proof:} %Lemma~\ref{lemma:inclusioninnewinterior} follows, given the fact that 
For all $p_i \in \bar{\mathbb{W}}^{\mathrm{int}}_{p_w,i}$ and $p_j \in \bar{\mathbb{W}}^{\mathrm{int}}_{p_w,j}$, with $i \neq j$, it holds by definition of ${\mathbb{W}}^{\mathrm{int}}_{p_w}$ that $\|p_i-p_j\| \leq 2(r_{\max} + \epsilon)$. Furthermore, following the definition of Voronoi partitions in~\eqref{eq:voronoidef}, the distance of each $p_i$ to its closest Voronoi border is bigger than or equal to half the distance to the surrounding seeds, which indeed is
$(r_{\max} + \epsilon)$.  \\

\noindent 
We also assume that, for an update of its Voronoi partition with respect to a sufficiently close position $p_i$, the steady state stays within the interior of $\bar{\mathbb{W}}_{p_i}$. 
\begin{assumption} 
There exists an $\tilde{\epsilon}>0$ s.t., for any $\bar{p}_i \in \mathbb{S}_{\mathbb{W}_i}^{\mathrm{p}}$ and 
%any
$p_i \in \bar{\mathbb{W}}_i^{int}$ with $\| \bar{p}_i - p_i \|<\tilde{\epsilon}$, it holds: 
$\bar{p}_i \in \bar{\mathbb{W}}_{p,i}^{\mathrm{int}} \text{ and } p_i \in \bar{\mathbb{W}}_{\bar{p},i}$.
\label{assumption:voronoisetneverempty}
\end{assumption}
\vspace{-0.6em}

\subsection{Data Collection and Bayesian Linear Regression}
\label{subsec:measurementcol}
In an unknown environment, the agents are assumed to be equipped with sensors and able to take a noisy measurement of the density $\phi$ at their current position. Note that measurements do not necessarily have to be taken at each time step, but their collection can be e.g. conditioned on reaching a predefined location or triggered after a fixed number of time steps $k$. Indexing by $h$ the number of collected data, and assuming that the noise is independent and identically distributed (i.i.d.) and zero mean Gaussian with variance $\sigma^2$, the measurements model for the $i-$th agent is
\begin{equation}
   m_{i,h} = \phi(p_{i,h}) + \nu_{i,h},\; \text{with } \nu_{i,h} \sim \mathcal{N} (0,\sigma^{2}). 
   \label{eq:measmod}
   \vspace{-0.3em}
\end{equation}
We assume that each agent's data is communicated to a server, so that the estimation of $\phi$ is centralized. Accordingly, we define the data-set collected by all $M$ agents after $t$ measurement steps as
\begin{equation}
\begin{split}
    I_{t} = \{ (p_{i,h},m_{i,h})\: \vert \: i=1,\cdots, M,\; h=1, \cdots, t\}.
\end{split}
\label{eq:setofmeasurements}
\vspace{-0.3em}
\end{equation}
In this set-up, we make use of the following assumption.
\begin{assumption}
The density function $\phi$ is represented by a linear combination of $\upsilon$ known, Lipschitz continuous features collected in a vector $\Phi: \mathbb{A} \rightarrow \mathbb{R}^{1 \times \upsilon}$, i.e., there exists an unknown parameter vector  $\theta \in \mathbb{R}^{\upsilon}$, s.t., $\phi(p) = \Phi(p)\theta$, $\forall p\in\mathbb{A}$. Furthermore, the features are linearly independent on the partition $\mathbb{W}_{p,i}$ in the sense that $\max_{p\in\mathbb{W}_{p,i}}\Phi(p)\Sigma\Phi(p)^\top\geq c_\Phi\|\Sigma\|$ for any positive semi-definite matrix $\Sigma\in\mathbb{R}^{\upsilon\times \upsilon}$ and some constant $c_\Phi>0$.
\label{assumption:bayesianconvergence}
\end{assumption}
Thus, the goal is to estimate the unknown vector $\theta$ from collected data. We solve the problem within the Bayesian linear regression framework~\cite{Sarkka2013}. To this aim, we endow the unknown vector with a Gaussian prior, such that \mbox{$\theta \sim \mathcal{N}(\mu_0,\Sigma_0)$}, and adopt a recursive scheme to update mean and covariance matrix in view of new data. Specifically, denoting by $\bar{\Phi}_{t+1} = [\Phi(p_{1,t+1})^{\top} \: \cdots \: \Phi(p_{M,t+1})^{\top}]^{\top} \in \mathbb{R}^{M \times \upsilon}$ the matrix of features measured at $t+1$, and by $\bar{m}_{t+1} = [m_{1,t+1} \: \cdots \: m_{M,t+1}]^{\top} \in \mathbb{R}^{M}$ the column vector of respective measurements, the updating rule reads as follows:
\begin{subequations}
\begin{align}
    \theta_{t+1} & \sim \mathcal{N}(\mu_{t+1}, \Sigma_{t+1})\\
    \mu_{t+1} & = \Sigma_{t+1}\Big(\Sigma_{t}^{-1}\mu_{t} + \frac{1}{\sigma^{2}}\bar{\Phi}^{\top}_{t+1}\bar{m}_{t+1}\Big)\\
    \Sigma_{t+1} & = \Big(\frac{1}{\sigma^{2}}\bar{\Phi}^{\top}_{t+1}\bar{\Phi}_{t+1} + \Sigma_{t}^{-1}\Big)^{-1}. \label{eq:bayessigmaupdate}
\end{align}
\label{eq:bayesupdate}
\vspace{-0.2em}
\end{subequations}
\noindent Accordingly, at any point $p \in \mathbb{A}$ we can define the estimated density $\hat{\phi}_{t+1}(p)$ and the variance $\text{Var}_{t+1}(p)$ as
\begin{subequations}
\begin{align}
    \hat{\phi}_{t+1}(p) & = \Phi(p)\mu_{t+1} \label{eq:thetaupdate}\\
    \text{Var}_{t+1}(p) & = \Phi(p)\Sigma_{t+1}\Phi(p)^{\top} \label{eq:variancephiblr}.
\end{align}
\label{eq:densityupdateblr}
\end{subequations}
\vspace{-1.5em}
\begin{remark}
Other coverage control approaches in an unknown environment rely on Gaussian Processes (GPs) for estimation of the unknown density function, i.e., in~\cite{Todescato2017} and~\cite{prajapat2022}, but they suffer from increasing complexity and memory requirements. The parametric set-up stated in Assumption~\ref{assumption:bayesianconvergence} thereby allows for a recursive update with fixed computational complexity. This is particularly relevant for the one-layer approach, where the GP would otherwise enter the MPC formulation. 
\end{remark}

\section{Two-layers Coverage MPC Algorithm}
\label{sec:twolayers}
The two-layers approach presented in this section builds upon the MPC scheme proposed in~\cite{Carron2017}. However, the set-up below does not leverage terminal ingredients, allowing for the consideration of more complex dynamical systems and the inclusion of collision avoidance constraints. Section~\ref{subsec:twolayers} studies the solution with known density, while in~\ref{subsec:twolayerslearning} the set-up is adjusted to include learning.

\vspace{-0.3em}
\subsection{Known Environment}
\label{subsec:twolayers}
In this subsection we consider the density $\phi$ as known. The overall here presented strategy encompasses two main tasks: (i) centralized computation of Voronoi partitions according to the agents' current position, and the respective centroids; (ii) solution of a tracking MPC scheme presented in Section~\ref{subsec:nonlineartrackingmpc} having the centroids as references. After solving task (ii), a new Voronoi partition can be computed, and the procedure can be repeated. \\
With $w \leq k$ indicating the time step of the last partition update, we now discuss the conditions that ensure convergence of this iterative set-up to the centroidal Voronoi partition solving problem~\eqref{eq:cortescost}. For the first task, we require a suitable update rule on the partitions. This is built upon~\cite[Proposition 3.3]{Cortes2004}, where sufficient conditions on position updates are given: 
\begin{proposition}\cite[Proposition 3.3]{Cortes2004} Assume that $p_{0} \in \mathbb{A}^{M}$ and the Voronoi partition is updated in finite time.
Further consider a continuous mapping $T: \mathbb{A}^{M} \rightarrow \mathbb{A}^{M}$, with $p_{w_{+}} = T(p_w)$, where $w_{+}$ indicates the next update step. If the mapping $T(p_w)$ fulfills the following properties at each update step $w$:
%time step $k$: 
\begin{itemize}
    \item $ \Vert p_{i,w_{+}} - c_i(\mathbb{W}_{p_{w},i}, \phi) \Vert \leq \Vert p_{i,w} - c_i(\mathbb{W}_{p_{w},i}, \phi) \Vert,$ \newline  $\forall i \in \{1, ..., M\}$;
    \item As long as the positions do not describe a centroidal Voronoi partition,
    $\exists j \in \{1, ..., M\}$ such that$ \newline \Vert p_{j,w_{+}} - c_j(\mathbb{W}_{p_{w},j}, \phi) \Vert < \Vert p_{j,w} - c_j(\mathbb{W}_{p_{w},j}, \phi) \Vert$,
\end{itemize}
then, $p_{w}$ converges to a centroidal Voronoi partition.
\label{proposition:cortesconvergence}
\end{proposition}

Hence, given the ability of integrator dynamics to reduce their Euclidean distance to a given reference at each time-step $k$, their Voronoi partition can be updated accordingly. However, as arbitrary nonlinear dynamics do not generally decrease the distance of agents to their corresponding centroid for all instances in time (see, e.g., the case presented in Figure~\ref{pics:singleintvsarbdyn}),  
\begin{figure}[t]
\begin{minipage}[t]{0.20\textwidth}
\includegraphics[width = \textwidth]{figures/single_int_mov_own_picto.pdf}
\end{minipage}
\hfill
\begin{minipage}[t]{0.20\textwidth}
\includegraphics[width = \textwidth]{figures/complex_dyn_mov_own_picto.pdf}
\end{minipage}
\caption{Left: agent able to decrease Euclidean distance to reference at each time step, indicated by a drone. Right: agent not able to decrease Euclidean distance to reference at each time step, represented by a car.}
\label{pics:singleintvsarbdyn}
\vspace{-0.8em}
\end{figure} a partition update rule has to be applied in order to meet the requirements of Proposition~\ref{proposition:cortesconvergence} and allow for the construction of a suitable mapping $T$. For this purpose, we rely on the conditions proposed in~\cite[Section 4]{Carron2017}: i.e., defining $e_{i,w} \! = \! \Vert p_{i,w} \! - \! c_i(\mathbb{W}_{p_{w},i}, \phi) \Vert$, the partition is updated only if
\begin{subequations}
\begin{align}
    & \bullet \; \Vert p_{i,k} - c_i(\mathbb{W}_{p_{w},i}, \phi) \Vert \leq e_{i,w} \;\: \forall i \in \{1,..., M\}, \label{eq:updatereq1} \\
    & \bullet \; \exists j \! \in \! \{1,..., M\} \text{ s.t. } \Vert p_{j,k} \! - \! c_j(\mathbb{W}_{p_{w},j}, \phi) \Vert \! < \! e_{j,w}. \label{eq:updatereq2} 
\end{align}
\label{eq:updatereq}%
\end{subequations}
Furthermore, the vector combining all errors is indicated with $e_{w}=[e_{1,w}, \hdots, e_{M,w}]$. Before discussing convergence within the second task, i.e., of the tracking MPC having the above computed centroids as reference, we detail its formulation. The optimization problem at time $k$ is defined as in~\eqref{eq:nonlineartrackingmpcwithcolavoidance}, with $r = c(\mathbb{W}_{p_{w}}, \phi)$ and  $p_{w}$ set to the most recent position configuration that fulfilled~\eqref{eq:updatereq} and~\eqref{eq:feasibilitycond}. The cost function $J_{i}(x_{i}, \bar{x}_{i}, u_{i}, \bar{u}_{i}, r_{i})$ follows the definition in~\eqref{eq:generalnonlineartrackingmpccost}. Within this set-up, convergence can be proven leveraging the subsequent assumptions. 
\begin{assumption}  
For any feasible configuration $p \in \mathbb{A}^{M}$ and according reference vector $r \in \mathbb{A}^{M}$ define the set \begin{equation}
\begin{split}
    \mathbb{T}_{\mathbb{W}_{p,i},r} = \{s\in \mathbb{S}_{\mathbb{W}_{p,i}} \: \vert \: \ell_{T,i}(C_{i}\bar{x} - r_{i}) = l_{\mathbb{W}_{p,i},r,\min}\},
\end{split}
\label{eq:Tid2layer}
\end{equation}
which is the union of all setpoints resulting in a target cost value equal to $l_{\mathbb{W}_{p,i},r,\min} = \min_{s\in \mathbb{S}_{\mathbb{W}_{p,i}}} \ell_{T,i}(C_i\bar{x}_i-r_i)$. Then, for every agent $i \in \{1,\hdots,M\}$ there are constants  \mbox{$\beta_{1,i}, \beta_{2,i} > 0$} such that, for any $\bar{s} := (\bar{x},\bar{u}) \in \mathbb{S}_{\mathbb{W}_{p,i}}$ and any $\epsilon \in [0,1]$, there exists a setpoint $\hat{s} := (\hat{x},\hat{u}) \in \mathbb{S}_{\mathbb{W}_{p,i}}$ satisfying 
\begin{subequations}
\begin{align}
    d_{i}(\hat{s}-\bar{s}) &\leq \beta_{1,i}\epsilon d_{i}(\bar{s})_{\mathbb{T}_{\mathbb{W}_{p,i},r}} \label{eq:assumption31twolayer} \\
    \ell_{T,i}(C_{i}\hat{x} \! - \! r_{i}) \! - \! \ell_{T,i}(C_{i}\bar{x}-r_{i}) &\leq \! - \! \beta_{2,i}\epsilon d_{i}(\bar{s})_{\mathbb{T}_{\mathbb{W}_{p,i},r}}^{2}, \label{eq:assumption32twolayer}%
\end{align}
\end{subequations} 
where $d_i(\cdot)$ is the distance function defined in~\eqref{eq:distance} and $d_{i}(\cdot)_{\mathbb{T}_{\mathbb{W}_{p,i},r}}$ follows definition~\eqref{eq:minsetdistance}. 
\label{assumption:steadystatedecreasetwolayers}%
\end{assumption}
\begin{assumption}
Consider any feasible configuration $p \in \mathbb{A}^{M}$ and according reference vector $r \in \mathbb{A}^{M}$. Then, for  every agent $i \in \{1,\hdots,M\}$ there is a constant  \mbox{$\beta_{T,i} > 0$} such that, for any $s := (\bar{x},\bar{u}) \in \mathbb{S}_{\mathbb{W}_{p,i}}$ it holds: 
\begin{align}
    \ell_{T,i}(C\bar{x}-r_i) - l_{\mathbb{W}_{p,i},r,\min} \leq \beta_{T,i}\epsilon d_{i}(s)_{\mathbb{T}_{\mathbb{W}_{p,i},r}}
    \label{eq:upperboundtargetcost}
\end{align}
\label{assumption:upperboundtargetcost}%
\end{assumption}
\vspace{-0.5em}
\noindent Assumption~\ref{assumption:steadystatedecreasetwolayers} states that, for any setpoint $\bar{s}$ not being a minimizer of the target cost, i.e., resulting in a target cost value not equal to its attainable minimal value $l_{\mathbb{W}_{p,i},r,\min}$, there exists another setpoint in its neighborhood such that the target cost can be actually reduced. In Assumption~\ref{assumption:upperboundtargetcost} the target cost of an arbitrary setpoint is upper bounded by a function of its distance to the target cost's minimizing setpoint. Assumption~\ref{assumption:steadystatedecreasetwolayers} and~\ref{assumption:upperboundtargetcost} are generally satisfied by choosing a target cost of the same powers as the stage cost, and by having $\mathbb{S}_{\mathbb{W}_{p,i}}$ and $\mathbb{A}$ as polytopes. The latter condition is not restrictive for most dynamical systems that are of interest in this work. \\
The overall procedure consists in constructing the Voronoi partitions with respect to the agents' position and setting the references equal to their centroids. Then, the MPC defined in~\eqref{eq:nonlineartrackingmpcwithcolavoidance} is applied recursively until conditions~\eqref{eq:updatereq1},~\eqref{eq:updatereq2} and~\eqref{eq:feasibilitycond} are fulfilled and the partitions as well as the centroids are updated. It is summarized in Algorithm~\ref{alg:twolayermpcalg} and we state our main result in Theorem~\ref{theorem:twolayers}, whose proof can be found in Appendix~\ref{subsec:twolayersproof}.

\begin{algorithm}[h!]
\SetAlgoLined
Set $k = 0$, $w = 0$ and construct $\mathbb{W}_{p_{w}}$. \par
Set $r = c(\mathbb{W}_{p_{w}},\phi)$ and calculate $e_{w}$.\par
 \For{k=0,1,\dots}{
 \ForAll{$i \in \{1, \hdots,M\}$}{
 Solve~\eqref{eq:nonlineartrackingmpcwithcolavoidance} and obtain $u^{*}_{i,0\vert k}, \hat{u}_{i,\cdot \vert k}$, $\hat{x}_{i,\cdot \vert k}$. \par
 Apply $u^{*}_{i,0\vert k}$ to obtain  $x_{i,k+1}$. \par}
 Construct $\mathbb{W}^{'}_{p_{k+1}}$. \par
\If{\textup{conditions~\eqref{eq:updatereq1},~\eqref{eq:updatereq2} and~\eqref{eq:feasibilitycond} are fulfilled}}{
   Set $w = k+1$ and update $\mathbb{W}_{p_{w}}$ = $\mathbb{W}^{'}_{p_{k+1}}$.\par
   Update $r = c(\mathbb{W}_{p_{w}},\phi)$ and calculate $e_{w}$.}
   }
 \caption{Two-Layers Coverage MPC}
 \label{alg:twolayermpcalg}
\end{algorithm}

\begin{theorem} 
Let Assumptions~\ref{assumption:dynamics},~\ref{assumption:expocostcontrollability},~\ref{assumption:boundedbyd},~\ref{assumption:voronoisetneverempty},~\ref{assumption:steadystatedecreasetwolayers} and~\ref{assumption:upperboundtargetcost} hold and consider a horizon length $N \geq N^{*}$. Additionally, suppose that at time step $k = 0$ the MPC problem described in~\eqref{eq:nonlineartrackingmpcwithcolavoidance} is feasible for all agents. Then, the overall control problem according to Algorithm~\ref{alg:twolayermpcalg} is recursively feasible, the agents do not collide and satisfy $x_{i,k} \in \mathbb{X}_{i},\; u_{i,k} \in \mathbb{U}_{i}, \forall i \in \{1,\hdots,M\}$ and $\forall k \in \mathbb{N}$. Furthermore, the partition update condition given by~\eqref{eq:updatereq1},~\eqref{eq:updatereq2} and~\eqref{eq:feasibilitycond} is fulfilled after a finite amount of time, and the agents' position configuration converges to a centroidal Voronoi partition.
\label{theorem:twolayers}
\end{theorem}

\vspace{-0.7em}
\subsection{Unknown Environment}
\label{subsec:twolayerslearning}
To deal with an initially unknown $\phi$, the motion planning scheme in the tracking MPC considered above needs to be adjusted to balance density learning and steering towards the current partition's centroids. In particular, the reference of each agent during exploration is chosen as the point of maximal variance within the Voronoi region\footnote{Finding the point of maximal variance over the whole Voronoi region implies evaluating~\eqref{eq:densityupdateblr} on an infinite number of points. In practice, we perform a sufficiently dense gridding of each $\mathbb{W}_{p,i}$ and evaluate variances on those points. For ease of notation, we still refer to such set of points as $\mathbb{W}_{p,i}$.} $\bar{\mathbb{W}}_{p,i}^{\mathrm{int}}$, denoted by $v_i(\mathbb{W}_{p,i}, \text{Var}) = \text{arg}\!\max_{\tilde{p} \in \bar{\mathbb{W}}_{p,i}^{\mathrm{int}}} \text{Var}(\tilde{p})$ and belonging to the vector $v(\mathbb{W}_{p}, \text{Var}) = [v_1(\mathbb{W}_{p,1}, \text{Var}), \hdots, v_M(\mathbb{W}_{p,M}, \text{Var})]$. In case of exploitation, the reference is set to the current partition's centroid with respect to the available density estimate $\hat{\phi}_t$, i.e., $r_{i,k} = c_i(\mathbb{W}_{p_{w},i}, \hat{\phi}_t)$. As proposed in~\cite{Todescato2017}, denoting with $\text{Var}_{\max} = \max_{p \in \mathbb{A}} \text{Var}(p)$ the maximal variance value computed over the set $\mathbb{A}$, and with $F: [0,1] \rightarrow [0,1]$ an arbitrary strictly monotonically increasing function such that \mbox{$F(\epsilon)=0 \Leftrightarrow \epsilon=0$}, the decision between exploration and exploitation is performed according to a Bernoulli random variable $\mathcal{B}(F(\text{Var}_{\max})) \in \{0,1\}$. The rationale is the following: at each time step, a sample from $\mathcal{B}$ is drawn whose probability of success, i.e. of returning a value equal to 1 is $F(\text{Var}_{\max})$. In this case, exploration is selected and both the Voronoi tessellation and the references are kept fixed for all agents. The exploration modality continues regardless of the new samples of $\mathcal{B}(F(\text{Var}_{\max}))$ and the data-set is expanded as $I_{t+1} = I_{t} \cup [(m_{1,k},p_{1,k}), \hdots, (m_{M,k},p_{M,k})]$ at each time-step until the agents' positions are sufficiently close to the point of maximum variance of their Voronoi region, i.e., when $e_{v,i,k} = \Vert v_i(\mathbb{W}_{p,i}, \text{Var}_t) - p_{i,k} \Vert \leq \rho$ for an arbitrarily small $\rho >0$. At this point, exploration modality is left. As for the exploitation phase, Voronoi partitions are updated only if conditions~\eqref{eq:updatereq1},~\eqref{eq:updatereq2} and~\eqref{eq:feasibilitycond} are met. The overall procedure is summarized in Algorithm~\ref{alg:twolayermpcalglearningimp}. The main result is given in the following theorem that is proven in Appendix~\ref{subsec:twolayerslearningproof}.
\begin{algorithm}[h!]
\SetAlgoLined
 Set $k = 0$, $w = 0$ and construct $\mathbb{W}_{p_{w}}$. \par
 Set $t = 1$, $\rho$, $\mu_{0}, \Sigma_{0}$ and $I_{t} = [(m_{1,k},p_{1,k}),..., (m_{M,k},p_{M,k})]$.\par
 Compute $\hat{\phi}_{t}(p), \text{Var}_{t}(p) \ \ \forall p \in \mathbb{A}$. \par
 Compute $c(\mathbb{W}_{p_{w}}, \hat{\phi}_{t}), v(\mathbb{W}_{p_{w}}, \text{Var}_{t}), \text{Var}_{\max,t}$ and $e_{w}$.\par
 Set exploration flag = false. \par
  \For{k=0,1, \dots}{
  \eIf{\textup{exploration flag} $\Vert$ \textup{(}$\mathcal{B}(F(\textup{Var}_{\textup{max},t})) == 1$\textup{)}}{
   Set exploration flag = true. \par
   \ForAll{$i \in \{1, \hdots, M \}$}{
   Set $r_{i,k} = v_i(\mathbb{W}_{p_{w},i}, \text{Var}_{t})$.\par
   Solve~\eqref{eq:nonlineartrackingmpcwithcolavoidance} and obatin $u^{*}_{i,0\vert k}.$ \par
   Apply $u^{*}_{i,0\vert k}$ to obtain $x_{i,k+1}$. \par 
   $e_{v,i,k+1} = \Vert v_i(\mathbb{W}_{p_{w},i}, \text{Var}_{t}) - p_{i,k+1} \Vert$. \par}
   $I_{t+1} \! = \! {\footnotesize I_{t} \! \cup \![(m_{1,k+1},p_{1,k+1}),..., (m_{M,k+1},p_{M,k+1})]}$ \par
   Update~\eqref{eq:thetaupdate}-\eqref{eq:variancephiblr} $\forall p \in \mathbb{A}$. \par
   Update $c(\mathbb{W}_{p_{w}}, \hat{\phi}_{t+1})$,$v(\mathbb{W}_{p_{w}}, \! \text{Var}_{t+1}),\text{Var}_{\max,t+1}$.\par
   $t=t+1$ \par
   \If{$\|e_{v,i,k+1}\|\leq\rho$, 
   $\forall i \in \{1, \hdots, M \}$}
   {
   Set exploration flag = false. \par}
  }
  {\ForAll{$i \in \{1, \hdots, M \}$}{
  Set $r_{i,k} = c(\mathbb{W}_{p_{w},i}, \hat{\phi}_{t})$. \par
  Solve~\eqref{eq:nonlineartrackingmpcwithcolavoidance} and obtain $u^{*}_{i,0\vert k}, \hat{u}_{i,\cdot \vert k}$, $\hat{x}_{i,\cdot \vert k}$. \par
  Apply $u^{*}_{i,0\vert k}$ to obtain $x_{i,k+1}$. \par}
 Construct $\mathbb{W}^{'}_{p_{k+1}}$. \par
\If{\textup{conditions~\eqref{eq:updatereq1},~\eqref{eq:updatereq2} and~\eqref{eq:feasibilitycond} are fulfilled}}{
   $w = k+1$, $\mathbb{W}_{p_{w}}$ = $\mathbb{W}^{'}_{p_{k+1}} \xrightarrow{update}$$c(\mathbb{W}_{p_{w}},\hat{\phi}_t)$, $e_{w}$.}}
   }
  \caption{Two-Layers, Learning-Based Coverage MPC}
 \label{alg:twolayermpcalglearningimp}
\end{algorithm}

\begin{theorem} 
Let Assumptions~\ref{assumption:dynamics},~\ref{assumption:expocostcontrollability},~\ref{assumption:boundedbyd},~\ref{assumption:voronoisetneverempty},~\ref{assumption:bayesianconvergence},~\ref{assumption:steadystatedecreasetwolayers} and~\ref{assumption:upperboundtargetcost} hold and consider a horizon length $N \geq N^{*}$ and $\rho>0$ sufficiently small. Additionally, suppose that at time step $k = 0$ the MPC problem described in~\eqref{eq:nonlineartrackingmpcwithcolavoidance} is feasible for all agents. Then, the overall control problem according to Algorithm~\ref{alg:twolayermpcalglearningimp} is recursively feasible and the agents do not collide and satisfy $x_{i,k} \in \mathbb{X}_{i}, u_{i,k} \in \mathbb{U}_{i}, \forall i \in \{1,\hdots,M\}$ and $\forall k \in \mathbb{N}$. Furthermore, the partition update condition given by~\eqref{eq:updatereq1},~\eqref{eq:updatereq2} and~\eqref{eq:feasibilitycond} is fulfilled after a finite amount of time, and the agents' position configuration converges to a centroidal Voronoi partition.
\label{theorem:twolayerslearning}
\end{theorem}
\section{One-Layer Coverage MPC Algorithm}
\label{sec:onelayer}
In the one-layer approach, the reference is not pre-calculated and passed to the MPC, but the locational optimization function is jointly optimized with the MPC cost, cf. Figure~\ref{fig:coverageclarification}. Furthermore, the Voronoi partitions are constructed with respect to the optimal setpoint positions $\bar{p}_{i,k}^* = C_{i}s_{i,k}^{*}$. The combined optimization is then expected to reduce the time and energy required for exploration.
\vspace{-0.5em}
\subsection{Known Environment}
\label{subsec:onelayer}
For the case of a known density $\phi$, the MPC optimization cost of the one-layer framework will encompass the same tracking cost $V_{N,i}$ considered in the previous sections, but the target cost $\ell_{T,i}$ will be equal to the corresponding summand of the locational optimization cost defined in~\eqref{eq:cortescost}. In particular, the latter is to be optimized with respect to the steady-state position $\bar{p}_{i,k} = C_i\bar{x}_{i,k} \in \mathbb{S}^{\mathrm{p}}_{\mathbb{W}_{\bar{p}^{*}_{w},i}}$ (see~\eqref{eq:steadystatesetwithcollavoidance}), and the integral is evaluated over the current Voronoi partition $\mathbb{W}_{\bar{p}^{*}_{w},i}$.
Thus, the resulting continuous objective reads as follows:
\begin{align}
&J_{i}(x_{i,\cdot \vert k}, u_{i,\cdot \vert k}, s_{i,k},\mathbb{W}_{\bar{p}^{*}_{w},i}) = \label{eq:fullnonlineartrackingmpccostonelayer} \\ & V_{N,i}(x_{i,\cdot \vert k}, u_{i,\cdot \vert k}, s_{i,k}) +   \ell_{T,i}(\bar{p}_{i,k},\mathbb{W}_{\bar{p}^{*}_{w},i}) = \notag\\ 
& \sum_{l=0}^{N-1}\ell_{i}(x_{i,l\vert k}, u_{i,l \vert k}, s_{i,k}) + \lambda \int_{\mathbb{W}_{\bar{p}^{*}_{w},i}} g(\Vert q - \bar{p}_{i,k} \Vert)\phi(q)dq,\notag
\end{align}
with $\lambda \in \mathbb{R}_{+}$ representing a scaling factor. The overall MPC program is then stated as
\begin{equation}
\begin{split}
\min_{x_{i,\cdot \vert k}, u_{i,\cdot \vert k},s_{i,k}}&J_{i}(x_{i,\cdot \vert k}, u_{i,\cdot \vert k}, s_{i,k},\mathbb{W}_{\bar{p}^{*}_{w},i}, \phi) \\
&\eqref{eq:generalinit}-\eqref{eq:1toN-1withcolavoidance} ,
\end{split}
\label{eq:fullnonlineartrackingmpconelayer}
\end{equation}
and is solved within the iterative coverage scheme presented in Algorithm~\ref{alg:onelayeralg}.To show convergence to a centroidal Voronoi configuration while dealing with non-convex target costs, Assumptions~\ref{assumption:ballwithminconvex} and~\ref{assumption:steadystatedecreaseonelayer} are imposed.

\begin{algorithm}[h!]
\caption{One-Layer Coverage MPC}
\SetAlgoLined
Set $k = 0$, $w=0$ and 
construct $\mathbb{W}_{\bar{p}_{w}^{*}}$ with $\bar{p}_0^*=p_0$\par
\For{k=0,1, \dots}{\ForAll{$i \in \{1, \hdots, M \}$}{
 Solve~\eqref{eq:fullnonlineartrackingmpconelayer} and obtain  $\hat{x}_{i,\cdot \vert k}$, $\hat{u}_{i,\cdot \vert k}$,  $\bar{x}^{*}_{i,k}$,  $u^{*}_{i,0\vert k}$. \par
 Apply $u^{*}_{i,0\vert k}$ to obtain $x_{i,k+1}$. \par}
 %$x_{i,k+1}$ = $f_{i}(x_{i,k},u^{*}_{i,0\vert k})$. \par}
 Construct $\mathbb{W}^{'}_{\bar{p}_{k}^{*}}$ according to $\bar{p}_{k}^{*}$. \par
  \If{\textup{condition~\eqref{eq:feasibilitycond} is fulfilled}}{
  $w = k$, $ \mathbb{W}_{\bar{p}_{w}^{*}} = \mathbb{W}_{\bar{p}_{k}^{*}}^{'}$.}
 }
\label{alg:onelayeralg}
\end{algorithm}

\begin{assumption} For every agent $i \in \{1, ..., M\}$, given any $\bar{p} \in \mathbb{S}^{p}_{i}$ and any $\bar{p}' \in \mathbb{S}^{\mathrm{p}}_{\mathbb{W}_{\bar{p},i}}$ there exist an $r > 0$ such that the set $\mathrm{argmin}_{p \in S^{\mathrm{p}}_{\bar{p}',\mathbb{W}_{\bar{p},i}}} \ell_{T,i}(p,\mathbb{W}_{\bar{p},i}), \; \text{with } S^{\mathrm{p}}_{\bar{p}',\mathbb{W}_{\bar{p},i}} := \mathbb{B}_{r}^{D}(\bar{p}') \cap \mathbb{S}^{\mathrm{p}}_{\mathbb{W}_{\bar{p},i}},$ is convex.
\label{assumption:ballwithminconvex}
\end{assumption}
\begin{assumption} 
Let $l_{\bar{p}',\mathbb{W}_{\bar{p},i},\min} \! = \! \min_{p \in S^{\mathrm{p}}_{\bar{p}',\mathbb{W}_{\bar{p},i}}}\ell_{T,i}(p,\mathbb{W}_{\bar{p},i})$, and define the corresponding set of local minimizers as
\begin{equation}
\begin{split}
    \mathbb{T}_{\bar{p}',\mathbb{W}_{\bar{p},i}} \! = \! \{p \in S^{\mathrm{p}}_{\bar{p}',\mathbb{W}_{\bar{p},i}}\,\vert\, \ell_{T,i}(p,\mathbb{W}_{\bar{p},i}) \! = \! l_{\bar{p}',\mathbb{W}_{\bar{p},i},\min}\}.
\end{split}
\label{eq:Tid}
\end{equation}
Then, for every agent $i \in \{1,\hdots,M\}$, any $\bar{p} \in \mathbb{S}^{p}_{i}$ and any $\bar{s}' := (\bar{x}',\bar{u}') \in \mathbb{S}_{\mathbb{W}_{\bar{p},i}}$ there are constants $\beta_{1,i}, \beta_{2,i} > 0$ as well as a $\mathcal{K}$-function $\kappa$ such that, for any $\epsilon \in [0,1]$, there exists a setpoint $\hat{s} := (\hat{x},\hat{u}) \in S_{\bar{p}',\mathbb{W}_{\bar{p},i}}$ satisfying
\begin{subequations}
\begin{align}
    d_{i}(\hat{s}-\bar{s}') &\leq \beta_{1,i}\epsilon \kappa(\Vert \bar{p}' \Vert)_{\mathbb{T}_{\bar{p}',\mathbb{W}_{\bar{p},i}}}
    \label{eq:assumption31},\\
    \ell_{T,i}(\hat{p},\mathbb{W}_{\bar{p},i}) \! - \! \ell_{T,i}(\bar{p}',\mathbb{W}_{\bar{p},i}) &\leq \! - \! \beta_{2,i}\epsilon \kappa(\Vert \bar{p}' \Vert)_{\mathbb{T}_{\bar{p}',\mathbb{W}_{\bar{p},i}}}^{2} \label{eq:assumption32},
\end{align}
\end{subequations}
with $\hat{p} = C_i\hat{x}$ and $\bar{p}' = C_i\bar{x}'$
\label{assumption:steadystatedecreaseonelayer}
\end{assumption}
\begin{remark} 
Assumption~\ref{assumption:ballwithminconvex} builds upon the fact that the constructed Voronoi partitions are never empty (see Section~\ref{subsec:collisionavoidance}) and provides a convenient structure on the set of solutions in view of Assumption~\ref{assumption:steadystatedecreaseonelayer}. Note that, in the considered case of the function $g(\cdot)$ in~\eqref{eq:cortescost} being equal to the square function, $\ell_{T,i}(\bar{p},\mathbb{W}_{\bar{p},i})$ is convex in $p$~\cite[Lemma 6.1]{Bullo2012}, and the set of minimizers is trivially convex: accordingly, Assumption~\ref{assumption:ballwithminconvex} holds with $r = \infty \ \text{such that} \ S^{\mathrm{p}}_{\bar{p}',\mathbb{W}_{\bar{p},i}} =\mathbb{S}^{\mathrm{p}}_{\mathbb{W}_{\bar{p},i}}$.  Finally, Assumption~\ref{assumption:steadystatedecreaseonelayer} is a generalization of Assumption~\ref{assumption:steadystatedecreasetwolayers} to the case of non-convex target costs. In accordance to~\cite[Remark 1]{Soloperto2021}, this condition holds taking functions $d$ and $\kappa$ quadratic if $\ell_{T,i}$ is strongly convex quadratic and sets are polytopic.
\end{remark}
\noindent Next, we state the theorem collecting all the theoretical guarantees of the proposed scheme. The proof is presented in Appendix~\ref{subsubsec:onelayerknownenvironmenttheory}.
\begin{theorem} Let Assumptions~\ref{assumption:dynamics},~\ref{assumption:expocostcontrollability},~\ref{assumption:boundedbyd},~\ref{assumption:voronoisetneverempty},~\ref{assumption:ballwithminconvex} and~\ref{assumption:steadystatedecreaseonelayer} hold, and consider a horizon length $N \geq N^{*}$. Additionally, suppose that at time step $k = 0$ the MPC problem described in~\eqref{eq:fullnonlineartrackingmpconelayer} is feasible for all agents. Then, in accordance to Algorithm~\ref{alg:onelayeralg} it remains recursively feasible, the agents do not collide and satisfy $x_{i,k} \in \mathbb{X}_{i},\; u_{i,k} \in \mathbb{U}_{i}, \forall i \in \{1,\hdots,M\}$ and $\forall k \in \mathbb{N}$. Furthermore, the partition update condition given by~\eqref{eq:feasibilitycond} is fulfilled after a finite amount of time, and the agents' position configuration converges to a centroidal Voronoi partition.
\label{theorem:onelayer}
\end{theorem}

\vspace{-0.8em}
\subsection{Unknown Environment}
\label{subsec:onelayerlearning}
The adaptation of the one-layer approach to an unknown environment involves the following modification of the target cost $\ell_{T,i}$ entering~\eqref{eq:fullnonlineartrackingmpconelayer} inspired by the Upper Confidence Bound method in Bayesian Optimization~\cite{Auer2002,Srinivas2010}. Specifically, the uncertainty on a potential setpoint $\bar{p}_i$, quantified by the variance $\text{Var}(\bar{p}_i)$ and scaled by a parameter $S > 0$, is subtracted from~\eqref{eq:fullnonlineartrackingmpconelayer}. Hence, the target cost now reads as
\begin{equation}
\begin{split}
&\ell_{T,i}(\bar{p}_{i}, \mathbb{W}_{\bar{p}^{*}_{w},i}, \hat{\phi}, \text{Var}) = \\ & \lambda \left(\int_{\mathbb{W}_{\bar{p}^{*}_{w},i}} g(\Vert q - \bar{p}_{i} \Vert)\hat{\phi}(q)dq - S\cdot \text{Var}(\bar{p}_{i}) \right).
\end{split}
\label{eq:fullnonlineartrackingmpccostonelayerlearning}
\end{equation}
Apart from this modification of the cost and the data collection, the overall procedure follows the one proposed in Section~\ref{subsec:onelayer} and is summarized in Algorithm~\ref{alg:onelayerlearningalg}. Note that~\eqref{eq:fullnonlineartrackingmpccostonelayerlearning} is non-convex, so Assumptions~\ref{assumption:ballwithminconvex} and~\ref{assumption:steadystatedecreaseonelayer} become crucial. However, they are considered to hold with respect to the modified target cost $\ell_{T,i}$ defined in~\eqref{eq:fullnonlineartrackingmpccostonelayerlearning}, given its continuity with respect to $\bar{p}_i$, which is proven below. \\

\noindent \textit{Proof of Continuity.} Considering a fixed Voronoi partition $\mathbb{W}_{\bar{p},i}$. By continuity of $g$, it holds that $\lim_{p_{1} \rightarrow p_{2}}g(p_{1}) - g(p_{2}) = 0$ for all $p_{1},\,p_{2} \in \mathbb{A}.$ Accordingly,
\begin{align*}
    &\lim_{p_{1} \rightarrow p_{2}} H_{i}(p_{1}, \mathbb{W}_{\bar{p},i}) - H_{i}(p_{2}, \mathbb{W}_{\bar{p},i}) \\
    &=\lim_{p_{1} \rightarrow p_{2}}\int_{\mathbb{W}_{\bar{p},i}}\Big(g(\Vert q - {p}_{1} \Vert) - g(\Vert q - {p}_{2} \Vert)\Big)\phi(q)dq\\
    &= \int_{\mathbb{W}_{\bar{p},i}} \lim_{p_{1} \rightarrow p_{2}} (g(\Vert q - {p}_{1} \Vert) -  g(\Vert q - {p}_{2} \Vert))\phi(q)dq = 0,
\label{eq:cortescostcontinuity}
\end{align*}
where the last step follows from the fact that the limit and the integration can be interchanged if the limit of the function exists and is integrable. The continuity of the variance follows by its definition.
\hfill{$\blacksquare$} \\
\label{proposition:coveragecostcontinuity}

\noindent Also, Assumption~\ref{assumption:ballwithminconvex} seems natural in such a scenario, ensuring the existence of a direction in which $\ell_{T,i}(\bar{p}',\mathbb{W}_{\bar{p},i})$ is decreasing and fulfilling equation~\eqref{eq:assumption32} as long as $\bar{p}' \notin \mathbb{T}_{\bar{p}',\mathbb{W}_{\bar{p},i}}$.

\begin{algorithm}[h!]
\caption{One-Layer, Learning-Based Coverage MPC}
\SetAlgoLined
Set $k = 0$, $w = 0$ and construct $\mathbb{W}_{\bar{p}^{*}_{w}}$ with $\bar{p}_0^*=p_0$\par
Set $t = 1$, $\mu_{0}$, $\Sigma_{0}$ and $I_{t} = [(m_{1,k},p_{1,k}), \hdots, (m_{M,k},p_{M,k})]$. \par 
Update $\hat{\phi}_{t}(p), \text{Var}_{t}(p) \ \ \forall p \in \mathbb{A}$. \par
 \For{k = 0,1,\dots}{\ForAll{$i \in \{1, \hdots, M \}$}{
  Set $\ell_{T,i}$ = $\ell_{T,i}(\bar{p}, \mathbb{W}_{\bar{p}^{*}_{w},i}, \hat{\phi}_{t}, \text{Var}_{t})$ \par
  Solve \eqref{eq:fullnonlineartrackingmpconelayer} and obtain  $\bar{x}^{*}_{i,k}, u^{*}_{i,0\vert k}, \hat{u}_{i,\cdot \vert k}$, $\hat{x}_{i,\cdot \vert k}$. \par
  Apply $u^{*}_{i,0\vert k}$ to obtain $x_{i,k+1}$. \par}
 $I_{t+1} = I_{t} \cup [(m_{1,k},p_{1,k}), \hdots, (m_{M,k},p_{M,k})]$ \par
 Update~\eqref{eq:thetaupdate}-\eqref{eq:variancephiblr} $\forall p \in \mathbb{A}$. \par
 Construct $\mathbb{W}^{'}_{\bar{p}_{k}^{*}}$ according to $\bar{p}_{k}^{*}$. \par
  \If{\textup{condition~\eqref{eq:feasibilitycond} is fulfilled}}{
  $w = k$, $ \mathbb{W}_{\bar{p}_{w}^{*}} = \mathbb{W}_{\bar{p}_{k}^{*}}^{'}$.} $t=t+1$
 }
\label{alg:onelayerlearningalg}
\end{algorithm}

\noindent Define the set of feasible positions in the MPC problem of agent $i$ at time $k$ by $\mathbb{F}_{i,k}=\{p\in\mathbb{A}|~\exists (x_{\cdot|k},u_{\cdot|k},s_k)$ s.t.~\eqref{eq:fullnonlineartrackingmpconelayer} is feasible with $[C,0]s_k=p\}$ and accordingly $\mathbb{F}_{k}=\bigcup_{i=1}^{M}\mathbb{F}_{i,k}$. 
The theoretical results are summarized in the following theorem, which is proven in Appendix~\ref{subsec:onelayerunknownenvironmenttheory}. 
\begin{theorem} Let  Assumptions~\ref{assumption:dynamics},~\ref{assumption:expocostcontrollability},~\ref{assumption:boundedbyd},~\ref{assumption:voronoisetneverempty},~\ref{assumption:bayesianconvergence},~\ref{assumption:ballwithminconvex} and~\ref{assumption:steadystatedecreaseonelayer} hold regarding the target cost defined in~\eqref{eq:fullnonlineartrackingmpccostonelayerlearning} and consider a horizon length $N \geq N^{*}$. Additionally, suppose that at time step $k = 0$, the MPC problem described in~\eqref{eq:fullnonlineartrackingmpconelayer} is feasible for all agents, Then, the overall control problem according to Algorithm~\ref{alg:onelayerlearningalg} is recursively feasible and the agents do not collide and satisfy $x_{i,k} \in \mathbb{X}_{i},\; u_{i,k} \in \mathbb{U}_{i}, \forall i \in \{1,\hdots,M\}$ and $\forall k \in \mathbb{N}$. The partition update condition given by~\eqref{eq:feasibilitycond} is fulfilled after a finite amount of time, the density estimate converges in probability and the agents' position configuration converges to a centroidal Voronoi partition with respect to the converged density estimate $\hat{\phi}_{\infty}$. Furthermore, there exists a uniform constant $\Delta H\geq 0$, such that
$\sup_{p\in\mathbb{F}_k}\lim_{k\rightarrow\infty} \mathrm{Var}_k(p)\leq \frac{ \Delta H}{S}$, in probability. 
\label{theorem:onelayerlearning}
\end{theorem}
\vspace{-0.1em}
\noindent In accordance to Theorem~\ref{theorem:onelayerlearning}, the remaining uncertainty by the time of convergence can be tuned by altering the variance scaling factor $S$ introduced in equation~\eqref{eq:fullnonlineartrackingmpccostonelayerlearning}, as well as the controller horizon length $N$, and hence the set of feasible positions $\mathbb{F}_{i,k}$.
\vspace{-0.3em}
\section{Experimental Results}
\label{sec:experiments}
In the following section, we introduce the mathematical set-up of our experiments (Section~\ref{subsec:mathematicalsetup} and~\ref{subsec:expermpccost}) and provide details on the used software and hardware framework (Section~\ref{subsec:hardwaredetails}). Furthermore, the obtained experimental hardware results for both control architectures (one-layer and two-layers) and for both known and unknown environments are presented in Sections~\ref{subsec:experimenttwolayer}-~\ref{subsec:experimentonelayerlearning}.
\vspace{-0.6em}
\subsection{Mathematical Set-up}
\label{subsec:mathematicalsetup}
We consider a fleet of 4 cars that covers area of $\mathbb{A}= [-1.55,1.55] \times [-1.85,1.85]$ meters. The nonlinear continuous dynamics for the $i-$th car is modeled using the kinematic bicycle model~\cite{Rajamani2012}, 
\begin{equation}
\begin{bmatrix}
    \dot{x}_{p,i} \\
    \dot{y}_{p,i} \\
    \dot{\psi_{i}}\\
    \dot{\delta_{i}}
    \end{bmatrix} = 
\begin{bmatrix}
    \cos{(\psi_{i})} \\
    \sin{(\psi_{i})} \\
    \frac{1}{L} \cdot \tan{(\delta_{i})}\\
    0
\end{bmatrix} v_{i} + 
\begin{bmatrix}
    0 \\
    0 \\
    0\\
    1
\end{bmatrix} u_{d,i} 
= g_{1,i} v_{i} + g_{2,i} u_{d,i}
\label{eq:dynamics}
\end{equation}
where $p_{i} =[x_{p,i},y_{p,i}]^{\top} \in \mathbb{R}^{2}$ is the position in Cartesian coordinates and~$\psi_{i}$ its orientation with respect to the $x$-axis. The orientation of its wheels with respect to their neutral position is given by~$\delta_{i}$. As an input, steering and velocity are available, indicated as~$v_{i}$ and~$u_{d,i}$. Lastly, we denote with $L$ the wheelbase length. The discrete model is obtained using the forward Euler method, with a sampling time $T_{s}$. It is set to 0.05 seconds for all experiments except for the one-layer, learning-based approach, where due to the increased complexity of its target cost, it has been set to 0.1 seconds. The coverage is performed in accordance to density $\phi_{1}$ in case of a known environment and $\phi_{2}$ for the experiments in which the environmental information is initially unknown, where
\begin{align}
    &\phi_{1}(x_p,y_p) = 5.0e^{-0.5((x_{p}-1.4)^2 + (y_{p}-1.7)^2)} \\
    &\phi_{2}(x_p,y_p) = -0.5x^{2} - 0.5y^{2} - 0.5x - 0.5y + 12. 
\end{align}
The cars are able to collect noisy measurements of the density at their current location according to~\eqref{eq:measmod}, with $\nu_{i,h} \sim \mathcal{N} (0,0.1) \ \forall i \in \{1,...,4\}$. For the application of the Bayesian linear regression, the feature vector is chosen as $\Phi = [x_p^2  \: y_p^2  \: x_p  \: y_p  \: 1]^{\top}$ and the Gaussian prior is set to $\mu_0 = [0  \: 0  \: 0  \: 0  \: 0]^{\top}$ and $\Sigma_0 = \mathbb{I}$.

\subsection{MPC Cost Function}
\label{subsec:expermpccost}
In consideration of the non-holonomic system dynamics presented in Section~\ref{subsec:mathematicalsetup} the stage cost of the MPC is designed according to~\cite{Rosenfelder2021}. Therefore, the Lie brackets of the previously introduced vectors $g_{1,i}$ and $g_{2,i}$, given by
\begin{align}
    & g_{3,i} := [g_{1,i},g_{2,i}] = \begin{bmatrix} 0 & 0 & \frac{1}{L}(\tan^{2}{(\delta_{i})} + 1) & 0 \end{bmatrix}^{\top} \notag \\
    & g_{4,i} := [g_{1,i},g_{3,i}] = \begin{bmatrix} \frac{\sin{(\psi_{i})}}{L(\sin^{2}{(\delta_{i})}-1)} & \frac{\cos{(\psi_{i})}}{L\cos^{2}{(\delta_{i})}} & 0 & 0 \end{bmatrix}^{\top}, \notag
\end{align}
are used to build a stage cost that allows for parallel parking maneuvers. Concerning a full dimensional state and input reference, indicated by $x_{r,i}$ and $u_{r,i}$, it reads as
\begin{align}
    &\ell_{i} = \sum_{j = 1}^{4}Q_{j}(g_{j,i}^{\top}(x_{r})(x_{i}-\bar{x}_{i}))^{\rho_{j}} + \sum_{z = 1}^{2}R_{z}(u_{z,i}-\bar{u}_{z,i})^{\rho_{u}}. \notag 
\end{align}
By setting~$\rho_{1}= \rho_{2} = \rho_{u} = 12$,~$\rho_{3} = 6 $ and~$\rho_{4} = 4$, the stage cost satisfies the cost-controllability condition in Assumption~\ref{assumption:expocostcontrollability}, as shown in~\cite{Rosenfelder2021} based on~\cite{Coron2020}. Hence, the derived stability guarantees are applicable. The reference location is either set to the pre-calculated reference, for the two-layers approaches, or the previously calculated setpoint, for the one-layer approaches. The reference in orientation,~$\psi_{r,i}$, is set to the angle enclosed by the x-axis and the error vector to the reference and~$\delta_{r,i}$ is chosen equal to zero for all agents. While the stage cost is used for all of the presented approaches, the implemented target costs differ. In consideration of Assumption~\ref{assumption:steadystatedecreasetwolayers}, the target cost of the one-layer approaches is formed using the same structure,
\begin{align}
    &\ell_{T,i} \! = \! \sum_{j = 1}^{4}Q_{j}(g_{j,i}^{\top}(x_{r})(\bar{x}_{i}-x_{r,i}))^{\rho_{j}} \! + \! \sum_{z = 1}^{2}R_{n}(\bar{u}_{z,i}-u_{r,z,i})^{\rho_{u}}. \notag
\end{align}
For the one-layer approaches, the target cost follows the structures described in Sections~\ref{subsec:onelayer} and~\ref{subsec:onelayerlearning}. To speed up computation we approximate the included integral with a quadratic function whose coefficients are learned using BLR. The controller's horizon length is set to $N=30$ for all conducted experiments except for the one-layer learning-based approach where, due to the doubled sampling time, it is divided in half and set to $N=15$.

\subsection{Software and Hardware Details}
\label{subsec:hardwaredetails}
For the experiments, the miniature RC cars of Chronos in combination with the CRS software framework are used~\cite{carron2022,Froehlich2021,Tearle2021}. The vehicles' position, velocity and orientation feedback is provided by a VICON motion capture system, and the control inputs are transmitted using WiFi-connected micro-controllers. A Macbook Air with 8 GB RAM and a 1.4GHz Dual-Core Intel Core i5 processor is used as a server. Its operating system is Ubuntu 18.04. The closed-loop system is implemented using ROS (Robotics Operating System) embedded in a C++/ Python framework using ACADOS as a solver~\cite{Verschueren2019,Verschueren2018}. The approximate initial positions and orientations, as well as the car's wheelbase length are given in the following table. The radius covering each agent $i$ is set to its length, i.e., $r_{i,\max} = L_i$.

\begin{table}[h]
\begin{center}
\vspace{1ex}
\begin{tabular}{l|cccccc}
\hline car & $x_{p,i,0}$ & $y_{p,i,0}$ & $\psi_{i,0}$ & $\delta_{i,0}$ & $L_{i}$ \\ \hline  \hline
1  & -1.26m & -1.4m & 0.785 rad & 0 rad & 0.115m \\
2 & -0.86m & -1.1m & 0.785 rad & 0 rad & 0.099m \\
3 & -1.25m & -1.1m & 0.000 rad & 0 rad & 0.115m \\
4  & -0.85m & -1.4m & 1.570 rad & 0 rad & 0.093m  \\
\hline
\end{tabular}
\caption{Values of initial configuration and model parameters of minature RC cars for all conducted experiments.}
\label{tab:model_param}
\end{center}
\end{table}

\vspace{-1.6em}
\subsection{Results Two-Layers Coverage MPC}
\label{subsec:experimenttwolayer}
The application of Algorithm~\ref{alg:twolayermpcalg} in the described set-up using $\phi_{1}$ results in a decrease of the locational optimization cost $H(p,\mathbb{W})$, defined in equation~\eqref{eq:cortescost}, shown in Figure~\ref{fig:twolayercoveragecost}. For a more detailed visualization of the obtained solution, in Figure~\ref{pics:twolayersconfig} the agents' location, their travelled paths, predicted trajectory, as well as the current Voronoi partitions and their centroids, are shown for three instances in time.
\begin{figure} [h]
\centering
\input{figures/coverage_cost_two_layer_6_220316.tex}
\vspace{-0.7em}
\caption{Locational optimization cost decrease over time, applying Algorithm~\ref{alg:twolayermpcalg} with respect to a known density $\phi_{1}$.}
\label{fig:twolayercoveragecost}
\vspace{-0.6em}
\end{figure} 
\begin{figure}[h!]
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[trim={5.2cm 8.1cm 4.6cm 7.7cm},clip, width = 0.8\textwidth]{figures/config_7sec_two_layer_6_220316_1215-compressed.pdf}
\centering
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[trim={5.2cm 8.1cm 4.6cm 7.7cm},clip, width = 0.8\textwidth]{figures/config_13sec_two_layer_6_220316_1215-compressed.pdf}
\centering
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[trim={5.2cm 8.1cm 4.6cm 7.7cm},clip, width = 0.8\textwidth]{figures/config_31sec_two_layer_6_220316_1215-compressed.pdf}
\centering
\end{minipage}
\caption{Configurations of cars at 2, 8, and 26 seconds for the application of Algorithm~\ref{alg:twolayermpcalg} in the described set-up. The agents' location and their predicted trajectory are given in red, the Voronoi partitions in green, their centroids in blue, and the traveled paths are visualized in light grey.}
\label{pics:twolayersconfig}
\vspace{-0.8em}
\end{figure}While the general trend of the locational optimization cost is pointing downwards, its decrease is not monotonic. Small increases are caused by movements not fulfilling the partition update conditions presented in~\eqref{eq:updatereq} and accordingly not causing a partition and centroid update, i.e. reorientation movements (see Figure~\ref{pics:singleintvsarbdyn}).
\vspace{-0.9em}
\subsection{Results Two-Layers, Learning-Based Coverage MPC}
\label{subsec:experimenttwolayerlearning}
Figure~\ref{fig:twolayerlearningcoveragecost} shows the locational optimization cost and the estimated cost when applying Algorithm~\ref{alg:twolayermpcalglearningimp} with the initially unknown density~$\phi_2$. Further, Figure~\ref{pics:twolayerslearningconfig} shows the according configurations at three instances in time.  
\begin{figure} [h!]
\centering
\input{figures/coverage_cost_and_estimate_two_layer_learning_3_220316.tex}
\vspace{-0.7em}
\caption{Locational optimization cost (green) as well as estimated locational optimization cost versus time (rose), applying Algorithm~\ref{alg:twolayermpcalglearningimp} in consideration of an initially unknown $\phi_{2}$. The grey background indicates time instances for which the agents are exploring.}
\label{fig:twolayerlearningcoveragecost}
\vspace{-0.8em}
\end{figure}

\begin{figure}[h!]
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[trim={5.2cm 8.1cm 4.6cm 7.7cm},clip, width = 0.8\textwidth]{figures/config_7sec_two_layer_learning_3_220316_1215-compressed.pdf}
\centering
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[trim={5.2cm 8.1cm 4.6cm 7.7cm},clip, width = 0.8\textwidth]{figures/config_42sec_two_layer_learning_3_220316_1215-compressed.pdf}
\centering
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[trim={5.2cm 8.1cm 4.6cm 7.7cm},clip, width = 0.8\textwidth]{figures/config_64sec_two_layer_learning_3_220316_1215-compressed.pdf}
\centering
\end{minipage}
\caption{Configurations of cars at 2, 37, and 59 seconds for the application of Algorithm~\ref{alg:twolayermpcalglearningimp} in the described set-up. The agents' location and their predicted trajectory are given in red, the Voronoi partitions in green, the references in blue, and the traveled paths are visualized in light grey. While the first two instances visualize the initial exploration movement, the third shows a covering behavior.}
\label{pics:twolayerslearningconfig}
\vspace{-0.4em}
\end{figure}
Defining~$F(\text{Var}_{\max,t}) = \frac{\text{Var}_{\max,t}}{\text{Var}_{\max,0}}$, the first decision in Algorithm~\ref{alg:twolayermpcalglearningimp} is guaranteed to be exploration and the agents drive towards the point of maximal variance within their initial Voronoi partition. Allowing for a better insight into the learning progress, the decreasing behaviour of the maximal variance over time is presented in Figure~\ref{fig:twolayerslearningmaxvar}. In Figure~\ref{fig:twolayerslearningcoef}, the development of estimated parameter vector $\theta$ is visualized. \\
In Figure~\ref{fig:twolayerlearningcoveragecost}, as well as in Figures~\ref{fig:twolayerslearningmaxvar} and~\ref{fig:twolayerslearningcoef}, the time instances for which the cars are exploring are indicated with a light grey background. Hence, it can be seen that, for the first 40 seconds, the agents keep exploring: correspondingly, any decrease or increase in cost is only accidental. However, by collecting measurements at each time step during exploration, after 25 seconds the mean estimate of $\theta$ is quite precise and further improvements are minor. This is also reflected in the small remaining maximal variance of the estimate. By the time the agents leave exploration mode all mean estimates show a maximal error of less than 0.05 concerning the true coefficient value and a maximal variance of approximately $10^{-3}$. Due to the remaining small uncertainty, the probability of another exploration movement for future instances is strictly positive, but very small, by the time the experiment is interrupted.
\begin{figure} [h!]
\centering
\input{figures/max_var_two_layer_learning_3_220316.tex}
\vspace{-0.7em}
\caption{Maximal variance decrease over time applying Algorithm~\ref{alg:twolayermpcalglearningimp} in consideration of an initially unknown $\phi_{2}$. The grey background indicates time instances for which the agents are exploring.}
\label{fig:twolayerslearningmaxvar}
\vspace{-0.7em}
\end{figure}

\begin{figure} [h!]
\centering
\input{figures/coef_two_layer_learning_3_220316.tex}
\vspace{-0.7em}
\caption{Coefficient estimation over time applying Algorithm~\ref{alg:twolayermpcalglearningimp} in consideration of an initially unknown $\phi_{2}$. The grey background indicates time instances for which the agents are exploring.}
\label{fig:twolayerslearningcoef}
\vspace{-1.0em}
\end{figure}
\vspace{-0.4em}
\subsection{Results One-Layer Coverage MPC}
\label{subsec:experimentonelayer}
For the application of Algorithm~\ref{alg:onelayeralg} in the described set-up using $\phi_{1}$, we obtain the locational optimization cost as well as the cumulative target cost decrease presented in Figure~\ref{fig:onelayercoveragecost}. 
\begin{figure} [h!]
\centering
\input{figures/coverage_cost_and_target_cost_one_layer_2_220323.tex}
\vspace{-0.7em}
\caption{Locational optimization cost decrease (green) as well as over all agents summed up target cost decrease over time (rose), applying Algorithm~\ref{alg:onelayeralg} with respect to a known density $\phi_{1}$.}
\label{fig:onelayercoveragecost}
\vspace{-0.5em}
\end{figure}


\begin{figure}[h!]
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[trim={5.2cm 8.1cm 4.6cm 7.7cm},clip, width = 0.8\textwidth]{figures/config_6sec_one_layer_2_220323_1215-compressed.pdf}
\centering
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[trim={5.2cm 8.1cm 4.6cm 7.7cm},clip, width = 0.8\textwidth]{figures/config_13sec_one_layer_2_220323_1215-compressed.pdf}
\centering
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[trim={5.2cm 8.1cm 4.6cm 7.7cm},clip, width = 0.8\textwidth]{figures/config_30sec_one_layer_2_220323_1215-compressed.pdf}
\centering
\end{minipage}
\caption{Configurations of cars at 2, 8, and 25 seconds for the application of Algorithm~\ref{alg:onelayeralg} in the described set-up. The agents' location and their predicted trajectory are given in red, the Voronoi partitions in green, the artificial setpoints in blue, and the traveled paths are visualized in light grey.}
\label{pics:onelayerconfig}
\vspace{-0.7em}
\end{figure}

It shows a comparable behavior to the two-layers approach regarding the time required until convergence, as well as the final configuration. While at certain instances in time the locational optimization cost increases slightly, the summed up target cost is strictly decreasing. 
\subsection{Results One-Layer, Learning-Based Coverage MPC}
\label{subsec:experimentonelayerlearning}
In a final experiment, Algorithm~\ref{alg:onelayerlearningalg} is applied, with its corresponding density $\phi_{2}$ initially unknown and $S$=2.5. Allowing for comparability, the same values are investigated as in the previous sections.  
\begin{figure} [h!]
%\centering
\input{figures/test_fig.tex}
%\centering
\vspace{-0.7em}
\caption{Locational optimization cost (green) as well as estimated locational optimization cost over time (rose), applying Algorithm~\ref{alg:onelayerlearningalg} for an initially unknown $\phi_{2}$.}
\label{fig:onelayerlearningcoveragecost}
\vspace{-0.7em}
\end{figure}

\begin{figure}[h!]
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[trim={5.2cm 8.1cm 4.6cm 7.7cm},clip, width = 0.8\textwidth]{figures/config_7sec_one_layer_learning_2_220322_1215-compressed.pdf}
\centering
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[trim={5.2cm 8.1cm 4.6cm 7.7cm},clip, width = 0.8\textwidth]{figures/config_14sec_one_layer_learning_2_220322_1215-compressed.pdf}
\centering
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[trim={5.2cm 8.1cm 4.6cm 7.7cm},clip, width = 0.8\textwidth]{figures/config_25sec_one_layer_learning_2_220322_1215-compressed.pdf}
%\input{figures/config_25sec_one_layer_learning_2_220322.tex}
\centering
\end{minipage}
\caption{Configurations of cars at 2, 9, and 20 seconds for the application of Algorithm~\ref{alg:onelayerlearningalg} in the described set-up. The agents' location and their predicted trajectory are given in red, the Voronoi partitions in green, the artificial setpoints in blue, and the traveled paths are visualized in light grey.}
\label{pics:onelayerlearningconfig}
\vspace{-0.7em}
\end{figure}

\begin{figure} [h]
\centering
\input{figures/max_var_one_layer_learning_2_220322.tex}
\vspace{-0.7em}
\caption{Maximal variance over time applying Algorithm~\ref{alg:onelayerlearningalg} for an initially unknown $\phi_{2}$.}
\label{fig:onelayerlearningmaxvar}
\vspace{-0.7em}
\end{figure}

\begin{figure} [h!]
\centering
\input{figures/coef_one_layer_learning_2_220322.tex}
\vspace{-0.7em}
\caption{Coefficient estimation over time applying Algorithm~\ref{alg:onelayerlearningalg} for an initially unknown $\phi_{2}$.}
\label{fig:onelayerlearningcoef}
\vspace{-0.7em}
\end{figure}
For the first half of the considered time-span the variance dominates the target cost and the setpoints are placed close to the locations of maximal variance within the partitions. The influence of the variance diminishes with its decreasing maximal value and the goal continuously shifts to the coverage problem. It is important to note that the timescale of the graphics highly differs from the two-layers learning approach presented in Section~\ref{subsec:experimenttwolayerlearning}. For the conducted experiments the one-layer learning approach takes approximately one-third of the time to steer its agents in a nearly optimal configuration. However, by the time the experiment has been interrupted, all mean estimates show a maximal error of less than 0.085 concerning the true coefficient value, and maximal variance reads as approximately 0.007. Therefore, both of them are slightly higher than in the two-layers learning-based approach and the obtained estimates of the parameter vector $\theta$ are slightly less certain and precise, due to a reduction in exploration movements. Both values would decrease if a large variance scaling factor $S$ is used.
\vspace{-0.7em}


\section{Discussion and conclusions}
\label{sec:conclusions}
This work presented a framework, consisting of two methods, to optimally cover a predefined convex area  using a non-homogeneous fleet of agents whose movements are determined by their nonlinear dynamics.  To this end, a tracking MPC without terminal constraints was deployed in both approaches to allow collision avoidance, as well as the consideration of additional constraints. While the first method relies on a two-layered structure and passes an online calculated reference to the individual MPC of each agent, the second method overcomes the hierarchical structure and directly integrates the calculation of the succeeding optimal configuration into the cost function of each agent's MPC. The developed methods are proposed for the case with known environment, represented by a known density function $\phi$, and are individually extended to the scenario in which the density function needs to be actively learned by the agents. 
Recursive feasibility and convergence to an optimal configuration were formally proven for both methods in each scenario. Furthermore, hardware results were obtained for all the proposed methods using a miniature racing car platform. With the performance of the two methodologies being rather comparable in a known environment, one of the benefits of the one-layer approach is its simplicity. Furthermore, the one-layer learning approach allows for a more efficient operation in the considered experiments, since it does \textit{not} require an explicit exploration vs. exploitation decision, but automatically adjusts it based on the remaining uncertainty. However, given the integration in its MPC cost function, the one layer approach is computationally more expensive than its two layer opponent. An interesting challenge for further research is a decentralized implementation for the learning and the Voronoi partition calculation~\cite{Bullo2012}, e.g., relying only on gossiping between the agents. 
\vspace{-0.7em}


\bibliographystyle{docstyle/IEEEtran} % official IEEE transaction referencing style
\bibliography{main}


\appendix
\section{Proofs}
\label{sec:proofs}
In this section, the proofs of Theorems~\ref{theorem:twolayers}-\ref{theorem:onelayerlearning} are given. Note that the dependency of the optimal costs, e.g. $J^{*}$ and $V_{N}^{*}$, on their current Voronoi partition will be dropped for notational ease if clear from the context. In all theorems, Theorem~\ref{theorem:theo4boccia} ensures the partitions are updated, while recursive feasibility is ensured by only updating the partitions if the condition presented in~\eqref{eq:feasibilitycond} is fulfilled. Furthermore, recall that $w$ indicates the timestep of the most recent partition update. 

\subsection{Proof of Theorem~\ref{theorem:twolayers}}
\label{subsec:twolayersproof}The proof of Theorem~\ref{theorem:twolayers} is divided into two steps. First, convergence of each agent to its reference $r$ is shown for a fixed Voronoi partition. This is followed by the proof that the fulfillment of the partition update requirement described in equations~\eqref{eq:updatereq1},~\eqref{eq:updatereq2} and~\eqref{eq:feasibilitycond} is satisfied after a finite time. Subsequently, convergence to a centroidal Voronoi configuration follows by  Proposition~\ref{proposition:cortesconvergence}.

\subsubsection{Convergence to Reference} 
The result will be proven by first introducing two intermediate results (Prop.~\ref{proposition:notminsteadynotcosetwolayers} and~\ref{proposition:upperlowerboudJtilde}).
\begin{proposition} Let Assumptions~\ref{assumption:dynamics},~\ref{assumption:expocostcontrollability},~\ref{assumption:boundedbyd} and~\ref{assumption:steadystatedecreasetwolayers} hold. Then there exist a constant $\bar{a}_{i} > 0$ such that, for any $p_w \in \mathbb{A}^M$ and according reference $r$, any feasible state $x_{k} \in \mathbb{X}^{\mathrm{int}}: Cx_k \in \bar{\mathbb{W}}_{p_w,i}^{\mathrm{int}}$, the optimal solution of problem~\eqref{eq:nonlineartrackingmpcwithcolavoidance} satisfies
\begin{equation}
\begin{split}
    \ell_{i}^*(x_{k}, s_{k}^{*}) \geq \bar{a}_{i}d_{i}(s_{k}^{*})_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2}.
\end{split}
\label{eq:prop12layer}
\end{equation}
\label{proposition:notminsteadynotcosetwolayers}
\end{proposition}
\vspace{-1.9em}
\noindent \emph{Proof:} For time step $k\in \mathbb{N}$, assume for contradiction that $\ell_{i}^*(x_{k}, s_{k}^{*}) < \bar{a}_{i}d_{i}(s_{k}^{*})_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2}$. Following~\cite[Proposition 1]{Soloperto2021} using compact constraints and $\bar{a}_{i}$ small enough, we can define a setpoint $s'_{k} \in \mathbb{S}_{\mathbb{W}_{p_w,i}}$ according to Assumption~\ref{assumption:steadystatedecreasetwolayers}, such that
\begin{align*}
    &V_{N,i}^{*}(x_{k},s'_{k},\mathbb{W}_{p_w,i}) - V_{N,i}^{*}(x_{k},s_{k}^{*},\mathbb{W}_{p_w,i}) + \ell_{T,i}(\bar{p}'_{k} - r_{i,k})\\
    & -\ell_{T,i}(\bar{p}^*_{k} - r_{i,k}) \overset{\eqref{eq:assumption2gammavmax},~\eqref{eq:assumption32twolayer}}{\leq} \gamma_{i} \ell_{i}^{*}(x_{k},s'_{k}) - \beta_{2,i}\epsilon d_{i}( s_{k}^{*} )_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2} \\
    &\overset{\eqref{eq:boundtwosteadystatestwolayer}}{\leq} \! \gamma_{i} \xi_{1,i} \ell_{i}^*(x_{k},s_{k}^{*}) \! +  \! \gamma_{i}\xi_{2,i} d_{i}(s'_{k} \! - \! s_{k}^{*})^{2} \! - \! \beta_{2,i}\epsilon d_{i}( s_{k}^{*} )_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2} \\ 
    &\overset{\eqref{eq:assumption31twolayer}}{<} (\gamma_{i} \xi_{1,i}\bar{a}_i + \gamma_{i} \xi_{2,i}\beta_{1,i}^{2}\epsilon^{2}- \beta_{2,i}\epsilon) d_{i}( s_{k}^{*} )_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2} \\ &:= - \bar{c}_id_{i}( s_{k}^{*} )_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2}. 
\label{eq:prop1}
\end{align*}
Given $\bar{a}_i$ small enough, an $\epsilon > 0$ can be found such that $\bar{c}_i > 0$. This implies that the new setpoint $s^{\prime}_k$ yields a smaller cost, which contradicts the optimality of $s_k^*$.\hfill{$\blacksquare$}

\begin{proposition}
Let Assumptions~\ref{assumption:dynamics},~\ref{assumption:expocostcontrollability},~\ref{assumption:boundedbyd},~\ref{assumption:steadystatedecreasetwolayers} and~\ref{assumption:upperboundtargetcost} hold. Defining  $a_{i} = \frac{1}{2} \min{(\bar{a}_{i},\alpha_{1,i})}$ and recalling that $u^*_{x,s}$ describes the minimizer of $\min_{u \in \mathbb{U}_{i}} \ell_{i}(x,u,s)$. Then for for any $p_w \in \mathbb{A}^M$, according reference $r$ and setpoint $s_k^* \in \mathbb{S}_{\mathbb{W}_{p_w,i}}$ and any feasible state $x_{k} \in \mathbb{X}^{\mathrm{int}}: Cx_k \in \bar{\mathbb{W}}_{p_w,i}^{\mathrm{int}}$ it holds
\begin{equation}
    \tilde{J}_{i}^{*}(x_{k}, \mathbb{W}_{p_w,i}) \! \geq \! a_{i}(d_{i}(s_{k}^{*})_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2} \! + \! d_{i}((x_{k},u_{x_k,s_k^*}^{*}) \! - \! s_{k}^{*})^2).
\label{eq:lowerbound2layer}
\end{equation}
Further, denote by $b_{i} \! = \! \max{(\gamma_{i,V_{\max,i}}\alpha_{2,i}, \beta_{T,i})}$. Then, we have 
\begin{equation}
     \tilde{J}_{i}^{*}(x_{k}, \mathbb{W}_{p_w,i}) \! \leq \! b_{i} (d_{i}(s_{k}^{*})_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2} \! + \! d_{i}((x_{k},u_{x_k,s_k^*}^{*}) \! - \! s_{k}^{*})^2).
\label{eq:upperbound2layer}
\end{equation}
\label{proposition:upperlowerboudJtilde}
\end{proposition}
\vspace{-1.9em}
\noindent \emph{Proof:} As regards the lower bound, we have 
\begin{align}
     &\tilde{J}_{i}^{*}(x_{k}, \mathbb{W}_{p_w,i}) \geq  V_{N,i}^{*}(x_{k},s_{k}^{*},\mathbb{W}_{p_w,i}) \geq \ell_{i}(x_{k},u_{x_k,s_k^*}^{*},s_{k}^{*}) \notag \\ &\overset{\eqref{eq:boundstagecosttwolayer},\eqref{eq:prop12layer}}{\geq} \frac{\bar{a}_{i}}{2}d_{i}(s_{k}^{*})_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2} + \frac{\alpha_{1,i}}{2}d_{i}((x_{k},u_{x_k,s_k^*}^{*}) - s_{k}^{*})^2 \label{eq:lowerbound2layertmp} \\ & \geq a_{i}(d_{i}(s_{k}^{*})_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2} + d_{i}((x_{k},u_{x_k,s_k^*}^{*}) - s_{k}^{*})^2). \notag
\end{align}
To show the existence of an upper bound, we leverage Assumptions~\ref{assumption:expocostcontrollability} and~\ref{assumption:upperboundtargetcost}. In consideration that combining~\cite{Boccia2014} with the constraint $V_{N,i} \leq V_{\max,i}$,  results in $V_{N,i}^{*}(x,s) \leq \gamma_{i,V_{\max,i}} \ell_{i}^{*}(x,s)$, where $\gamma_{i,V_{\max,i}} = \max{(\gamma_{i}, \frac{V_{\max,i}}{c_{i}},2)}$, it follows: 
\begin{align}
     &\tilde{J}_{i}^{*}(x_{k}, \mathbb{W}_{p_w,i}) \notag \\ & = V_{N,i}^{*}(x_{k},s_{k}^{*},\mathbb{W}_{p_w,i}) + \ell_{T,i}(s_{k}^{*}-r) -   l_{\mathbb{W}_{p_w,i},r,\min} \label{eq:upperbound2layer1} \\ & \leq
     %\gamma_{i,V_{\max,i}}\alpha_{2,i} d_{i}((x_{k},u_{x,s}^{*}) - s)^{2} =
     \gamma_{i,V_{\max,i}}\alpha_{2,i} d_{i}((x_{k},u_{x_k,s_k^*}^{*}) - s_{k}^{*}))^2 + \beta_{T,i}d_{i}(s_{k}^{*})_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2} \notag \\ &  \leq b_{i}(d_{i}(s_{k}^{*})_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2} + d_{i}((x_{k},u_{x_k,s_k^*}^{*}) - s_{k}^{*})^2). \notag
     \hspace{5.1em}\hfill\ensuremath{\blacksquare}
\end{align}
\vspace{-0.1em}

Prop.~\ref{proposition:notminsteadynotcosetwolayers} follows \cite[Proposition 1]{Soloperto2021} and shows that the optimal setpoint cannot be arbitrarily close to the current state-input pair, unless the latter belongs to the set $\mathbb{T}_{\mathbb{W}_{p_w,i},r}$ defined in~\eqref{eq:Tid2layer}; Prop.~\ref{proposition:upperlowerboudJtilde} later provides a lower and upper bound for $\tilde{J}_{i}^{*}(x_{k}, \mathbb{W}_{p_Wi}) = J_{i}^{*}(x_{k}, \mathbb{W}_{p_w,i}) - l_{\mathbb{W}_{p_w,i},r,\min}$. At this point, considering a fixed partition $\mathbb{W}_{p_w,i}$ and using the candidate $s_{k+1}=s_{k}^{*}$, Theorem~\ref{theorem:theo4boccia} in combination with Proposition~\ref{proposition:notminsteadynotcosetwolayers} and Assumption~\ref{assumption:boundedbyd} yields \begin{align*}
    &\tilde{J}_{i}^{*}(x_{k+1},\mathbb{W}_{p_w,i}) - \tilde{J}_{i}^{*}(x_{k}, \mathbb{W}_{p_w,i})\\ 
    &\leq V^{*}_{N,i}(x_{k+1},s_{k}^{*},\mathbb{W}_{p_w,i}) + \ell_{T,i}(s_{k}^{*} - r_{i}) - l_{\mathbb{W}_{p_w,i},r,\min} \\ 
    &- V^{*}_{N,i}(x_{k},s_{k}^{*},\mathbb{W}_{p_w,i}) - \ell_{T,i}(s_{k}^{*} - r_{i}) + l_{\mathbb{W}_{p_w,i},r,\min} \\
    &\overset{\eqref{eq:theorem5twolayer}}{\leq} - \bar{\alpha}_{N,i}\ell_{i}^{*}(x_{k},s_{k}^{*})\\ &\overset{\eqref{eq:prop12layer}}{\leq} -\frac{\bar{\alpha}_{N,i}}{2}(\ell_{i}^{*}(x_{k},s_{k}^{*}) + \bar{a}_{i}d_{i}(s_{k}^{*})_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2}) \\ 
    &\overset{\eqref{eq:boundstagecosttwolayer}}{\leq} -\frac{\bar{\alpha}_{N,i}}{2}(\alpha_{1,i}d_{i}((x_{k},u_{x_{k},s_{k}^{*}}^{*})-s_{k}^{*})^{2} + \bar{a}_{i}d_{i}(s_{k}^{*})_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2})\\  & \leq - \tilde{\alpha}_{N,i}(d_{i}((x_{k},u_{x_{k},s_{k}^{*}}^{*})-s_{k}^{*})^{2} + d_{i}(s_{k}^{*})_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2}),
    \label{eq:2theorem2twolayer}
\end{align*}
where we defined $\tilde{\alpha}_{N,i}=\frac{\bar{\alpha}_{N,i}}{2}\text{min}\{\alpha_{1,i},\bar{\alpha}_i\}$. Using Proposition~\ref{proposition:upperlowerboudJtilde}, we have 
\begin{equation*}
    \begin{aligned}
    &\tilde{J}_{i}^{*}(x_{k+1},\mathbb{W}_{p_w,i}) - \tilde{J}_{i}^{*}(x_{k}, \mathbb{W}_{p_w,i}) \\
    &\leq - \tilde{\alpha}_{N,i}(d_{i}(s_{k}^{*})_{\mathbb{T}_{\mathbb{W}_{p_w,i},r}}^{2} + d_{i}((x_{k},u_{x_{k},s_{k}^{*}}^{*})-s_{k}^{*})^{2})\\ 
    & \leq  -\frac{\tilde{\alpha}_{N,i}}{b_{i}}\tilde{J}_{i}^{*}(x_{k}, \mathbb{W}_{p_w,i})
    \end{aligned}
\end{equation*}

\noindent and obtain the exponential convergence 
\begin{equation}
\begin{split}
    \tilde{J}_{i}^{*}(x_{k+\tilde{k}},\mathbb{W}_{p_w,i})
    \leq \Big(1 - \frac{\tilde{\alpha}_{N,i}}{b_{i}} \Big)^{\tilde{k}} \tilde{J}_{i}^{*}(x_{k}, \mathbb{W}_{p_w,i})
\end{split}
\label{eq:4fintitetimecovergencetwolayer}
\end{equation}
which is guaranteed to converge, because $b_{i} >\tilde{\alpha}_{N,i}>0$.

\subsubsection{Finite Time Update Condition Fulfillment, Part 1}\label{subsection:finitetimeconditiontwolayersp1}

Given \eqref{eq:4fintitetimecovergencetwolayer}, and the fact that $\tilde{J}_{i}^{*}(x_{k}, \mathbb{W}_{p,i})$ admits a uniform bound due to compact constraints and continuity of the cost, it follows that
for any $\epsilon > 0$ there exists a finite time step $\tilde{k}'>0$ such that $\tilde{J}_{i}^{*}(x_{k+\tilde{k}'},\mathbb{W}_{p,i}) < \epsilon$. 
Hence, by choosing $\epsilon > 0$ small enough and considering the lower bound~\eqref{eq:lowerbound2layer}, the state at time step $\tilde{k}'$ is arbitrary close to the set $\mathbb{T}_{\mathbb{W}_{p_w,i},r}$, and thus the update conditions~\eqref{eq:updatereq1},~\eqref{eq:updatereq2} hold (assuming we are not already in a centroidal Voronoi configuration).

\subsubsection{Finite Time Update Condition Fulfillment, Part 2}\label{subsection:finitetimeconditiontwolayersp2}We now need to ensure that the remaining Voronoi partition update condition~\eqref{eq:feasibilitycond} is fulfilled in finite time with the candidate state and input sequences proposed in~\eqref{eq:lemma1proposal}. For this we leverage the initial feasibility of the problem, the construction of the Voronoi partitions with respect to the agents positions, resulting in a position sequence $C_{i}x_{i,k} \in \bar{\mathbb{W}}_{p_{w},i}^{\mathrm{int}}, \forall k \in \mathbb{N}, \forall i \in \{1, \hdots, M\}$, as well as Assumption~\ref{assumption:voronoisetneverempty}. Specifically, 
 we need to find an upper bound for 
\begin{align}
    & V_{N,i}(x_{k+\tilde{k}'}, \hat{u}, s_{k}^{*}, \mathbb{W}_{p_w}) = \sum_{l=0}^{N-2}\ell_{i}(x_{l\vert k+\tilde{k}'}^{*}, u_{l \vert k+\tilde{k}'}^{*},s_{k}^{*}) + \notag \\ &  \ell_{i}(x_{N-1\vert k+\tilde{k}'}^{*},\bar{u}_{k+\tilde{k}'}^{*},s_{k}^{*}) + \notag \\ & \ell_{i}(f(x_{N-1\vert k+\tilde{k}'}^{*},\bar{u}_{k+\tilde{k}'}^{*}),\bar{u}_{k+\tilde{k}'}^{*},s_{k}^{*}), 
    \label{eq:trackingcostcandidate}
\end{align}
and such a bound is required to be decreasing as $\tilde{k}'$ increases, along the lines of~\eqref{eq:4fintitetimecovergencetwolayer}. To this aim, we will make use of $\epsilon$ as introduced above that can be seen as a decreasing function of $\tilde{k}'$.
The first term of~\eqref{eq:trackingcostcandidate} can be upper bounded by~\eqref{eq:4fintitetimecovergencetwolayer} as 
\begin{align}
    &\epsilon \geq V_{N,i}^{*}(x_{k+\tilde{k}'},s_{k}^{*},\mathbb{W}_{p_w})  \geq \sum_{l=0}^{N-2}\ell_{i}(x_{l\vert k+\tilde{k}'}^{*}, u_{l \vert k+\tilde{k}'}^{*},s_{k}^{*}). \notag %\notag \\ & \geq \sum_{l=0}^{N-2}\alpha_{1}d(({x}^{*}_{l \vert k+\tilde{k}'},u_{x_{l},s_{k}^{*}}^{*}) - s_{k}^{*}).
\label{eq:7finitetimeconvergence}
\end{align}
The upper bound for the last two terms is given by the following proposition.
\begin{proposition} Let Assumption~\ref{assumption:dynamics} and~\ref{assumption:boundedbyd} hold. 
Then, for any $x \in \mathbb{R}^{n_{i}}$ and any $s = (\bar{x},\bar{u}) \in \mathbb{S}_i$, it holds that
\begin{equation}
    \ell_{i}(x,\bar{u},s) + \ell_{i}(f(x,\bar{u}),\bar{u},s)\leq \gamma_{i}' \cdot \ell_{i}^{*}(x,s),
\label{eq:assumption22}
\end{equation}
where $\gamma_{i}'=\frac{2\alpha_{2,i}}{\alpha_{1,i}}\max_j\{1+\mathcal{L}_{i}^{\alpha_j}\}$.
\label{proposition:upperboundsuccedingstagecost}
\end{proposition}

\noindent \emph{Proof:} 
In accordance to equation~\eqref{eq:boundstagecosttwolayer} it holds that
\begin{align}
    &\ell_{i}(x,\bar{u},s) + \ell_{i}(f_{i}(x,\bar{u}),\bar{u},s)\leq \label{eq:fulfillassumption22}\\ 
    &2\alpha_{2,i} {\sum_{j=1}^{n_i}  \Big(\vert x_{j}-\bar{x}_j \vert^{a_j} + \vert f_{i,j}(x,\bar{u})-\bar{x}_j \vert^{a_j}\Big)}.\notag 
\end{align}
Because $\bar{x}$ is a steady state, we have that $f_{i,j}(x,\bar{u}) - \bar{x}_j = f_{i,j}(x,\bar{u}) - f_{i,j}(\bar{x},\bar{u})$. Thus, by Lipschitz continuity and reusing \eqref{eq:boundstagecosttwolayer}, we have that~\eqref{eq:fulfillassumption22} is upper bounded by 
\begin{align*}
    \hspace{3.3em}&2\alpha_{2,i}\sum_{j=1}^{n_i} (1 + \mathcal{L}_{i}^{a_j})\vert x_{j}-\bar{x}_j \vert^{\alpha_j} \leq \gamma_i'\ell_i^*(x,s).
    \hspace{3.3em}\hfill\ensuremath{\blacksquare}
\end{align*}
\vspace{-1.0em}

\noindent Finally, noting also that
\begin{equation}
{\displaystyle \epsilon \geq \ell_{i}(x_{N-1 \vert k+\tilde{k}'}^{*}, u_{N-1 \vert k+\tilde{k}'}^{*}, s_{k}^{*}) \geq \ell^{*}_{i}(x_{N-1 \vert k+\tilde{k}'}^{*},s_{k}^{*})},\notag 
\label{eq:8finitetimeconvergence}
\end{equation}
we obtain the desired bound
\begin{align}
     & V_{N,i}(x_{k+\tilde{k}'}, \hat{u}, s_{k}^{*}, \mathbb{W}_{\bar{p}}) \qquad \text{(see~\eqref{eq:trackingcostcandidate})} \notag \\
     &  \leq \sum_{l=0}^{N-2}\ell_{i}(x_{l\vert k+\tilde{k}'}^{*}, u_{l \vert k+\tilde{k}'}^{*},s_{k}^{*}) + \gamma_{i}' \ell^{*}_{i}(x_{N-1\vert k+\tilde{k}'}^{*},s_{k}^{*}) \notag \\ &
     \leq (1+ \gamma_{i}')\epsilon.
\label{eq:9finitetimeconvergence}
\end{align}
Since it holds for any of the addenda, which in turn are lower-bounded by a function describing the distance between state and setpoint, they will become arbitrarily close. Given Assumption~\ref{assumption:voronoisetneverempty} and that $\bar{\mathbb{W}}_{p_{k+\tilde{k}'}}^{\mathrm{int}}$ (the interior of the candidate partition at $\tilde{k}'$) contains $p_{k+\tilde{k}'}$,~\eqref{eq:feasibilitycond} will hold for a finite~$\tilde{k}'$.

\subsection{Proof of Theorem~\ref{theorem:twolayerslearning}}
\label{subsec:twolayerslearningproof}
The result of Theorem~\ref{theorem:twolayerslearning} builds upon~\cite[Proposition 1]{Todescato2017}. It ensures that the learning-based solution converges to the centroidal Voronoi partition if the estimate $\hat{\phi}_t$ converges in probability to the true density $\phi$. Then, convergence to a centroidal Voronoi configuration follows by Theorem~\ref{theorem:twolayers}. 

\begin{proposition}~\cite[Proposition 1]{Todescato2017} Assume $\hat{\phi}_{t}(p) \xrightarrow{\mathbb{P}} \phi(p)$, where $\mathbb{P}$ denotes convergence in probability in the space of continuous functions. Moreover, assume it is possible to start the Lloyd algorithm from the configuration reached by Algorithm~\ref{alg:twolayermpcalglearningimp} at time $\bar{t}$. Then the centroids $c^{L}_{\bar{t}}$ obtained from the Lloyd algorithm and the estimated ones $\hat{c}_{\bar{t}}$ coincide. Furthermore, for any $0<\delta < 1$, $\epsilon > 0$ an integer $N$ can be picked, such that there exists a $\bar{t}$ sufficiently large such that
\begin{equation}
     \mathbb{P}[ \Vert \hat{c}_{\bar{t} + t} - c^{L}_{\bar{t} + t} \Vert < \epsilon ] > 1 - \delta, \ \ t = 0, \hdots, N,
\label{eq:learningprop1}
\end{equation}
where $\mathbb{P}$ is associated to the Gaussian distribution of the estimated coefficients.
\label{proposition:convergenceofcentroidstwolayerslearning}
\end{proposition}
\noindent As by construction of the algorithm the locations of the agents never coincide, it is possible to start Lloyd's algorithm from an arbitrary instant in time. Then, it remains to show that $\hat{\phi}_{t}(p) \xrightarrow{\mathbb{P}} \phi(p)$ holds by applying Algorithm~\ref{alg:twolayermpcalglearningimp}.

\begin{lemma}
Assumptions~\ref{assumption:dynamics},~\ref{assumption:expocostcontrollability},~\ref{assumption:boundedbyd},~\ref{assumption:voronoisetneverempty},~\ref{assumption:bayesianconvergence},~\ref{assumption:steadystatedecreasetwolayers} and~\ref{assumption:upperboundtargetcost} hold
and consider a horizon length $N \geq N^{*}$ and $\rho>0, r_{i,\max} >0, \epsilon>0$ sufficiently small. Then for any $p \in \mathbb{A}$, $\hat{\phi}_t(p) \xrightarrow{\mathbb{P}} \phi(p)$. 
\end{lemma}
\noindent \emph{Proof:} We prove this by showing that, given any partition configuration $\mathbb{W}_p$, the maximum variance value inside the $i-$th partition, $\text{Var}_{i,\max,t} = \max_{p \in \mathbb{W}_{p,i}} \text{Var}_t(p)$, converges to zero for each $i$. This implies that $\text{Var}_t(p)$ at any point $p$ inside the partition
converges to zero with increasing $t$: this is equivalent to proving $\mathscr{L}^2$-convergence of $\phi_t(p)$, from which convergence in probability follows.\\
For a time instant $t$, define $p^{\text{maxvar}}_{t,i} = \arg\max_{p \in \mathbb{W}_{p,i}} \text{Var}_t(p)$, and consider the position of the $i-$th agent, $p_{t,i}$, such that $\|p^{\text{maxvar}}_{t,i} - p_{t,i}\| \leq (\rho + r_{\max} + \epsilon)$. Next, we show $\text{Var}(p_{t,i}) \geq c\text{Var}(p^{\text{maxvar}}_{t,i})$ for some $c>0$. To this aim, we have
\begin{align*}
    \text{Var}(p^{\text{maxvar}}_{t,i}) &\leq 2\Phi(p_{t,i})\Sigma_{t}\Phi(p_{t,i})^{\top}  \\
    + &2(\Phi(p_{t,i}) - \Phi(p^{\text{maxvar}}_{t,i}))\Sigma_t(\Phi(p_{t,i}) - \Phi(p^{\text{maxvar}}_{t,i}))^{\top}\\
    &\leq 2 \text{Var}(p_{t,i}) + 2\mathcal{L}_{\Phi}^2(\rho + r_{\max} + \epsilon)^2\|\Sigma_t\|,
\end{align*}
with some Lipschitz constant $\mathcal{L}_\Phi>0$ from Assumption~\ref{assumption:bayesianconvergence}. Moreover, by the linear independence of the features in $\Phi$ stated in the same Assumption, we have $\text{Var}(p^{\text{maxvar}}_{t,i}) \geq c_{\Phi}\|\Sigma_t\|$. Applying this inequality yields
\begin{equation*}
   \text{Var}(p_{t,i}) \geq \underbrace{(1 - 2\mathcal{L}_{\Phi}^2(\rho+r_{\max} + \epsilon)^2/c_{\Phi})/2}_{=:c} \text{Var}(p^{\text{maxvar}}_{t,i}), 
\end{equation*}
with $c>0$ for $\gamma>0, r_{i,\max} >0, \epsilon>0$ sufficiently small.\\
The proof is concluded by showing that $\text{Var}(p_{t,i})$ converges to zero. Assume for contradiction that \mbox{$\lim\sup_{t \rightarrow +\infty} \Phi(p_{t,i})\Sigma_t\Phi(p_{t,i})^{\top} \! = \! \tilde{c} > 0$.} We can isolate a subsequence $\{t_{\tau}\}_{\tau}$ such that $\lim_{\tau \rightarrow +\infty} \Phi(p_{t_{\tau},i})\Sigma_{t_{\tau}}\Phi(p_{t_{\tau},i})^{\top} \! = \! \tilde{c}$. \\ However, from recursive application of equation~\eqref{eq:bayessigmaupdate} we have that \mbox{$\lim_{\tau\rightarrow\infty}\Sigma_{t_\tau}^{-1} \! \succeq \! \sum_{\tau=1}^{\infty}\Phi(p_{t_{\tau},i})^{\top}\Phi(p_{t_{\tau},i})$}. 
This implies that $\Phi(p_{t_\tau,i})\Sigma_{t_\tau}\Phi(p_{t_\tau,i})^\top$ tends to 0, contradicting the starting hypothesis. 
\hfill$\blacksquare$

\subsection{Proof of Theorem~\ref{theorem:onelayer}}
\label{subsubsec:onelayerknownenvironmenttheory}
We first present two preliminary results that will be needed in the main proof. In particular, Proposition~\ref{proposition:notminsteadynotcoseonelayer} generalizes Proposition~\ref{proposition:notminsteadynotcosetwolayers} to non-convex target costs and Proposition~\ref{proposition:voronoiupdatecostdecrease} ensures that an update of the partitions in accordance to the current steady state does not increase the cost~\eqref{eq:cortescost}.

\begin{proposition} Let Assumptions~\ref{assumption:dynamics},~\ref{assumption:expocostcontrollability},~\ref{assumption:boundedbyd},~\ref{assumption:ballwithminconvex} and~\ref{assumption:steadystatedecreaseonelayer} hold. Then there exists a constant $\bar{a}_{i} > 0$ such that, for any $\bar{p}_w \in \mathbb{S}_{i}^{\mathrm{p}}$ any feasible state $x_{k} \in \mathbb{X}^{\mathrm{int}}:Cx_k \in \bar{\mathbb{W}}_{i}^{\mathrm{int}}$, the optimal solution of the MPC problem~\eqref{eq:fullnonlineartrackingmpconelayer} satisfies
\begin{equation}
    \ell_{i}^*(x_{k}, s_{k}^{*}) \geq \bar{a}_{i}\kappa(\Vert \bar{p}_{k}^{*} \Vert)_{\mathbb{T}_{\bar{p}_{k}^{*},\mathbb{W}_{\bar{p}_w,i}}}.
\label{eq:prop1}
\end{equation}
\label{proposition:notminsteadynotcoseonelayer}
\end{proposition}
\vspace{-1.2em}
\noindent \emph{Proof:} Follows the same steps as the proof of Proposition 2, with Assumption~\ref{assumption:steadystatedecreasetwolayers} replaced by Assumptions~\ref{assumption:ballwithminconvex} and~\ref{assumption:steadystatedecreaseonelayer}.\hfill{$\blacksquare$} 

\begin{proposition} 
For any $\bar{p},\bar{p}_k\in\mathbb{A}^M$, it holds
\begin{equation}
    H(\bar{p}_{k},\mathbb{W}_{\bar{p}_{k}}) \leq  H(\bar{p}_{k},\mathbb{W}_{\bar{p}}).
\label{eq:condassumption6}
\end{equation}
\label{proposition:voronoiupdatecostdecrease}
\end{proposition}
\vspace{-1.5em}
\noindent \emph{Proof:} 
Given setpoint positions $\bar{p}$, each partition $\mathbb{W}_{\bar{p},i}$ admits itself a partition $\{\mathbb{W}_{\bar{p} \rightarrow \bar{p}_{k},i \rightarrow j}\}_{j=1}^M$, explicitly depending on $\bar{p}_{k}$, where each $\mathbb{W}_{\bar{p} \rightarrow \bar{p}_{k},i \rightarrow j}$ describes the set of points which are currently assigned to the steady-state of agent $i$, but would be assigned to the steady-state of agent $j$ in case of a Voronoi partition update in accordance to $\bar{p}_{k}$. 
Therefore, with $g$ non-decreasing, it holds:
\begin{align*}
    &H(\bar{p}_{k},\mathbb{W}_{\bar{p}})  = \sum_{i=1}^{M}\int_{\mathbb{W}_{\bar{p},i}} g(\Vert q - \bar{p}_{i} \Vert)\phi(q)dq \\ 
    &= \sum_{i=1}^{M}\sum_{j=1}^{M}\int_{\mathbb{W}_{\bar{p}\rightarrow\bar{p}_{k},i\rightarrow j}} g(\Vert q - \bar{p}_{i} \Vert)\phi(q)dq \\ 
    &\overset{\eqref{eq:voronoidef}}{\geq} \sum_{i=1}^{M}\sum_{j=1}^{M}\int_{\mathbb{W}_{\bar{p}\rightarrow\bar{p}_{k},i\rightarrow j}} g(\Vert q - \bar{p}_{j} \Vert)\phi(q)dq \\
    &= \sum_{i=1}^{M}\int_{\mathbb{W}_{\bar{p}_{k},i}} g(\Vert q - \bar{p}_{i} \Vert)\phi(q)dq = H(\bar{p}_{k},\mathbb{W}_{\bar{p}_{k}}).
    \label{eq:assumption6}
    \hspace{3.0em}\hfill\ensuremath{\blacksquare}
\end{align*}

\noindent
The remainder of the proof amounts to showing the following facts: 1) convergence to a fixed setpoint, taking into account a possible update of the Voronoi tessellation; 2) convergence to a centroidal Voronoi partition; and 3) finite-time partition update. The following results thereby rely on a Lyapunov like decrease of the storage function for the MPC in~\eqref{eq:fullnonlineartrackingmpconelayer}.\\
The storage function we are considering is the summation of the optimal MPC cost over all agents, i.e., that is,
\begin{equation}
\begin{aligned}
&J^{*}(x_{k}, \mathbb{W}_{\bar{p}^{*}_{w}}) = V_{N}^{*}(x_{k}, s_{k}^{*},\mathbb{W}_{\bar{p}^{*}_{w}}) + \lambda  H(\bar{p}_{k},\mathbb{W}_{\bar{p}^{*}_{w}}). 
\end{aligned}
\label{eq:lyapunovonelayer}
\end{equation}
When the subscript $i$ is dropped, then we refer to quantities of the overall system. The storage function~\eqref{eq:lyapunovonelayer} has an upper and lower bound, whereas the first exists thanks to the compactness of $\mathbb{A},\mathbb{X},\mathbb{U}$ and the continuity of the cost. Given that the locational optimization cost is always greater or equal to zero the latter follows by
\begin{align}
     &J^{*}(x_{k},\mathbb{W}_{\bar{p}_w}) \geq V_{N}^{*}(x_{k},s_{k}^{*},\mathbb{W}_{\bar{p}_w}) \geq \ell^{*}(x_{k},s_{k}^{*}) \notag \\ &\geq \alpha_{1}d((x_{k},u_{x_{k},s_{k}^{*}}^{*})-s_{k}^{*})^{2} \geq 0.
\label{eq:lowerbound}
\end{align}

\subsubsection{Convergence to Setpoint} We now prove that the optimal cost at time $k+1$ on a candidate update partition $\mathbb{W}_{\bar{p}_{k}^{*}}$ decreases with respect to its value corresponding to the Voronoi partition induced by the setpoint positions $\bar{p}$ at time $w$. In other words, we show that $J^{*}(x_{k+1},\mathbb{W}_{\bar{p}_{k}^{*}}) < J^{*}(x_{k},\mathbb{W}_{\bar{p}_w})$. By definition of the involved storage function, this in turn implies that $x_k$ converges to the current steady-state $s_k^*$.\\
Recalling the definitions of $\gamma_{i}'$ and $\gamma_{i,V_{\max,i}}$ given in Proposition~\ref{proposition:upperboundsuccedingstagecost} and the proof of Proposition~\ref{proposition:upperlowerboudJtilde}, respectively, let $\gamma_{\max} := \max_{i=1,...,M}\max{\{\gamma_{i}',\gamma_{i,V_{\max,i}}\}}$ and $\gamma' = \max_{i=1,...,M}{\{\gamma_{i}'\}}$. Moreover, we will make use of the following result taken from the proof of Theorem~\ref{theorem:theo4boccia}:
\begin{equation}
\begin{split}
    &\left( \frac{\gamma_{i,V_{\max,i}}}{\gamma_{i,V_{\max,i}}-1} \right)^{N-1} \ell_{i}^{*}(\hat{x}_{N-1\vert k},s_{k}^*) \\ & \leq V_{N,i}(x_{k},s_k^*,\mathbb{W}_{\bar{p}_w,i}) \leq \gamma_{i,V_{\max,i}} \ell^{*}_{i}(x_{k},s_k^*).
\end{split}
\label{eq:prooftheom4}
\end{equation}

Next we get the following storage function decrease 
\begin{align}
    &J^{*}(x_{k+1},\mathbb{W}'_{\bar{p}_{k}^{*}}) \leq J^{*}(x_{k+1}, s^{*}_{k},\mathbb{W}'_{\bar{p}_{k}^{*}}) \label{eq:intermediateJ1} \\ & = 
    V_{N}^{*}(x_{k+1},s_{k}^{*},\mathbb{W}'_{\bar{p}_{k}^{*}}) + \lambda H(\bar{p}_{k}^{*},\mathbb{W}'_{\bar{p}_{k}^{*}}) %- \ell_{T,min} 
    \notag \\ &\overset{\eqref{eq:lemma1proposal},\eqref{eq:feasibilitycond}}{\leq} V_{N}(\hat{x}_{\cdot \vert k}, \hat{u}_{\cdot \vert k},s_{k}^{*}) + \lambda H(\bar{p}_{k}^{*},\mathbb{W}'_{\bar{p}_{k}^{*}})  %-\ell_{T,min} 
    \notag \\ &\overset{\eqref{eq:condassumption6},\eqref{eq:feasibilitycond}}{\leq} V_{N}(\hat{x}_{\cdot \vert k}, \hat{u}_{\cdot \vert k},s_{k}^{*}) + \lambda H(\bar{p}_{k}^{*},\mathbb{W}_{\bar{p}_w}) %- \ell_{T,min} 
    \notag \\
    &\overset{\eqref{eq:lemma1proposal}}{=} J^{*}(x_{k},\mathbb{W}_{\bar{p}_w}) - \ell(x_{k},u_{0\vert k}^{*},s_{k}^{*}) - \ell(x_{N-1\vert k}^{*},u_{N-1\vert k}^{*},s_{k}^{*}) \notag \\ & + \ell(\hat{x}_{N-1 \vert k},\bar{u}^{*}_{k},s_{k}^{*}) + \ell(f(\hat{x}_{N-1 \vert k},\bar{u}^{*}_{k}),\bar{u}^{*}_{k},s_{k}^{*}) \notag \\
    & \leq J^{*}(x_{k},\mathbb{W}_{\bar{p}_w}) - \ell^{*}(x_{k},s_{k}^{*}) - \ell^{*}(x_{N-1\vert k}^{*},s_{k}^{*}) \notag \\ & + \ell(\hat{x}_{N-1\vert k},\bar{u}^{*}_{k},s_{k}^{*}) + \ell(f(\hat{x}_{N-1 \vert k},\bar{u}^{*}_{k}),\bar{u}^{*}_{k},s_{k}^{*}) \notag \\
    &\overset{\eqref{eq:assumption22}}{\leq} J^{*}(x_{k},\mathbb{W}_{\bar{p}_w}) - \ell^{*}(x_{k},s_{k}^{*}) + (\gamma' -1)\ell^{*}(x_{N-1\vert k}^{*},s_{k}^{*}) \notag \\
    &\overset{\eqref{eq:prooftheom4}}{\leq} J^{*}(x_{k},\mathbb{W}_{\bar{p}_w}) - \ell^{*}(x_{k},s_{k}^{*})  \notag \\ & +(\gamma_{\max} - 1)\left( \frac{\gamma_{\max}-1}{\gamma_{\max}} \right)^{N-1} \gamma_{\max} \ell^{*}(x_{k},s_{k}^{*}). \notag
\end{align}
Thus, we obtain 
\begin{equation}
 J^{*}(x_{k+1},\mathbb{W}'_{\bar{p}_{k}^{*}}) \leq J^{*}(x_{k},\mathbb{W}_{\bar{p}_w}) - \bar{\alpha}_{N}\ell^{*}(x_{k},s_{k}^{*}),
 \label{eq:1theorem2}
\end{equation}
where $\bar{\alpha}_{N} = 1-\left( \dfrac{(\gamma_{\max}-1)}{\gamma_{\max}} \right)^{N} \gamma_{\max}^{2} > 0$ by choosing $N>N^{*}$ large enough. Thus, inequality~\eqref{eq:1theorem2} in combination with lower and upper bounds on $J^{*}$ and Assumption~\ref{assumption:boundedbyd} ensures that $\lim_{k\rightarrow \infty} \|  x_{k} - s_{k}^{*} \|  = 0$.

\subsubsection{Convergence to Centroidal Voronoi partition} Combining Proposition~\ref{proposition:notminsteadynotcoseonelayer} with the result seen in~\eqref{eq:1theorem2}, it follows that $J^{*}$ decreases as long as $x_{k} \neq s_{k}^{*}$ or $\bar{p}_{k}^{*} \notin \mathbb{T}_{\bar{p}_{k}^{*},\mathbb{W}_{\bar{p}_w,i}}$. In fact,
\begin{align}
    &J^{*}(x_{k+1}, \mathbb{W}_{\bar{p}_w}) - J^{*}(x_{k}, \mathbb{W}_{\bar{p}_w})
    \leq - \bar{\alpha}_{N}\ell^{*}(x_{k},s_{k}^{*}) \notag \\
    &\leq -\frac{\bar{\alpha}_{N}}{2}(\ell^{*}(x_{k},s_{k}^{*}) + \bar{a}\kappa(\Vert \bar{p}_{k}^{*} \Vert)_{\mathbb{T}_{\bar{p}_{k}^{*},\mathbb{W}_{\bar{p},i}}}^{2}).
\label{eq:2theorem2}
\end{align}
Accordingly, it can be concluded that $\lim_{k \to \infty}p_{k} \in \mathbb{T}_{\bar{p}_{k}^{*},\mathbb{W}_{\bar{p},i}}$. 

\subsubsection{Finite Time Partition Update} Finally, it has to be shown that equation~\eqref{eq:feasibilitycond} is fulfilled in finite time and the Voronoi partition can be updated. For this purpose, we leverage the result in~\eqref{eq:1theorem2} with respect to a fixed set of partitions $\mathbb{W}_{\bar{p}_w}$ and candidate $s_{k+1} = s_k^*$ and combine them with Assumption~\ref{assumption:expocostcontrollability}. Then, we obtain 
\begin{equation}
\begin{split}
     V_{N}^{*}(x_{k+\tilde{k}},s_{k}^{*},\mathbb{W}_{\bar{p}_w}) 
     \leq \Big(1 - \frac{\bar{\alpha}_{N}}{\gamma_{\max}} \Big)^{\tilde{k}} V_{N}^{*}(x_{k},s_{k}^{*},\mathbb{W}_{\bar{p}_w}),
\end{split}
\label{eq:4fintitetimecovergence}
\end{equation}
that converges because by definition $\gamma_{\max}> \bar{\alpha}_{N} > 0$. At this point, the same reasoning carried out in Appendix~\ref{subsection:finitetimeconditiontwolayersp2} can be used to show that condition~\eqref{eq:feasibilitycond} is met in finite time. However, we leverage Assumption~\ref{assumption:voronoisetneverempty} in combination with the fact that by construction, starting from an initially feasible configuration, the interior of the candidate partition at $\tilde{k}'$, $\bar{\mathbb{W}}_{\bar{p}_{k+\tilde{k}'}}^{\mathrm{int}}$ contains $\bar{p}_{k+\tilde{k}'}$.

\noindent In conclusion, the algorithm does converge to a configuration in which each agent's position defines a local minimum of $H(p, \mathbb{W})$ and the Voronoi partition does not update anymore: that is, a centroidal Voronoi configuration is obtained. 

\subsection{Proof of Theorem~\ref{theorem:onelayerlearning}}
\label{subsec:onelayerunknownenvironmenttheory}
For this proof we first show convergence to a steady state describing a local minimum of the locational optimization function considering the converged estimate. In a second step, an upper bound for the remaining variance at each reachable position is defined.
\subsubsection{Convergence} Similar to the proof of Theorem~\ref{theorem:onelayer}, we define the storage function as
\begin{align}
&J^{*}(x_{k}, \mathbb{W}_{\bar{p}^{*}_{w}},\hat{\phi}_{k}) \notag \\ & =  V_{N}^{*}(x_{k}, s_{k}^{*},\mathbb{W}_{\bar{p}^{*}_{w}}) + \lambda (H(\bar{p}_{k}^{*},\mathbb{W}_{\bar{p}^{*}_{w}}, \hat{\phi}_{k}) - S\text{Var}_{k}(\bar{p}_{k}^{*}))\notag \\
& = \sum_{i=1}^{M} \bigg( \sum_{l=0}^{N-1}\ell_{i}(x_{i,l\vert k}^{*}, u_{i,l \vert k}^{*}, s_{i,k}^{*}) \label{eq:lyapunovonelayerlearning} \\  &+ \lambda (H_{i}(\bar{p}_{i,k}^{*},\mathbb{W}_{\bar{p}^{*}_{w},i},\hat{\phi}_{k}) - S\text{Var}_{k}(\bar{p}_{i,k}^{*}) \bigg), \notag
\end{align}
Following the reasoning in equations~\eqref{eq:intermediateJ1} and~\eqref{eq:1theorem2} for the adapted storage function, we end up with
\begin{align}
&J^{*}(x_{k+1}, \mathbb{W}'_{\bar{p}^{*}_{k}},\hat{\phi}_{k+1}) \leq J^{*}(x_{k}, \mathbb{W}_{\bar{p}_w^*},\hat{\phi}_{k}) - \underbrace{\bar{\alpha}_{N}\ell^{*}(x_{k},s_{k}^{*})}_{\geq 0} 
\notag \\ &- \lambda\int_{\mathbb{W}_{\bar{p}_w^*}} g(\Vert q - \bar{p}^{*}_{k} \Vert)(\hat{\phi}(q)_{k}-\hat{\phi}(q)_{k+1})dq \notag \\ &- \underbrace{\lambda S (\text{Var}_{k}(\bar{p}^{*}_{k}) - \text{Var}_{k+1}(\bar{p}^{*}_{k}))}_{\geq 0}. \label{eq:lyth5}
\end{align}
Using the Bernstein-von Mises Theorem~\cite[Chapter 10.2]{vanderVaart1998} we have that $\hat{\phi}_{k}(q_k)$ converges in probability to the Gaussian distribution with mean equal to the maximum likelihood estimate, and with variance equal to $1/k$ times the asymptotic variance of the maximum likelihood estimate (i.e., the inverse of the Fisher information matrix). Using~\eqref{eq:bayesupdate}, this convergence implies that $(\hat{\theta}_{k}-\hat{\theta}_{k+1}) \xrightarrow{\mathbb{P}} 0$. Thus, the storage function decrease in~\eqref{eq:lyth5} implies $\lim_{k\rightarrow\infty}\|x_k-s_k^*\|=0$ in probability. Applying Proposition~\ref{proposition:notminsteadynotcoseonelayer} ensures convergence of the steady state to a local minimum of the target cost with respect to the current density~$\phi_k$, similarly to Equation~\eqref{eq:2theorem2}.

\subsubsection{Remaining Variance} 
Given the converged state, the following proposition provides a bound on the variance for all feasible position $p\in\mathbb{F}:=\lim_{k\rightarrow\infty}\mathbb{F}_k$.  

\begin{proposition} 
Suppose the closed loop converges to a setpoint $\overline{s}_i^*$ with state $\bar{x}_{i}^{*}$ and position $\bar{p}_{i}^{*}$. 
Then, each agent $i \in \{1,\hdots,M\}$, there exists a uniform constant $\Delta H_i\geq 0$, such that for any $p\in \mathbb{F}_i$ with corresponding $s_{p}\in\mathbb{S}_i$, the variance is bounded by
\begin{align}
    &\lim_{k \rightarrow \infty}\mathrm{Var}_{k}(p) \leq  \frac{\Delta H_i}{S}. 
\label{eq:upperboundvaronelayerlearningsimplified}
\end{align}
\label{proposition:onelayerlearning}
\end{proposition}
\vspace{-1.5em}
\noindent \emph{Proof:} 
Convergence ensures $\lim_{k\rightarrow\infty} V_N(\overline{x}^*,\overline{s}^*)=0$, $\text{Var}_\infty(\overline{p}^*)=0$ and hence the minimum cost in the MPC problem is given by $\lambda H(\overline{p}^*,\mathbb{W}_{\overline{p}^*,i},\hat{\phi}_\infty)$. 
Note that the cost of any feasible position $p\in\mathbb{F}_i$ with corresponding setpoint $s_p\in\mathbb{S}_i$ is larger than or equal to this minimum, i.e.,
\begin{align}
    &\lambda H(\overline{p}^*,\mathbb{W}_{\overline{p}^*,i},\hat{\phi}_\infty) \label{eq:contraditiononelayerlearning2}\\
    &\leq V_{N}^{*}(\bar{x}_{i}^{*}, s_{p},\mathbb{W}_{\bar{p},i}) +\lambda (H(\overline{p},\mathbb{W}_{\overline{p}^*,i},\hat{\phi}_\infty)-S\text{Var}_\infty({p})).
    \nonumber
\end{align}
Note that $V_N(\overline{x}_i^*,p)\leq V_{\max,i}$; moreover, the definition of the coverage cost $H$ in~\eqref{eq:cortescost}   
with $g$ continuous and $\mathbb{W}_{\overline{p}^*,i}\subseteq\mathbb{A}$ compact ensures that there exists a uniform constant $\Delta H_i\geq 0$ such that
\begin{align*}
    %&S\text{Var}_\infty(p)
    &\dfrac{1}{\lambda}V_{N}^{*}(\bar{x}_{i}^{*}, s_{p},\mathbb{W}_{\bar{p},i})  + H(\overline{p},\mathbb{W}_{\overline{p}^*,i},\hat{\phi}_\infty)-H(\overline{p}^*,\mathbb{W}_{\overline{p}^*,i},\hat{\phi}_\infty)\\
    & \leq \Delta H_i.
    %\label{eq:contraditiononelayerlearning2}
\end{align*}
Inequality~\eqref{eq:upperboundvaronelayerlearningsimplified} follows by applying this bound to inequality~\eqref{eq:contraditiononelayerlearning2}. 
$\blacksquare$\\

\noindent Therefore, for every $\epsilon > 0$, a scaling $S>0$ can be found such that
$\lim_{k\rightarrow\infty}\text{Var}_k(p) \leq \frac{\Delta H}{S} \leq \epsilon$, $\ \forall p \in \mathbb{F}$, whereas $\Delta H = \max_{\Delta H_i}\{\Delta H_1,\hdots,\Delta H_M\}$ 
\vspace{-3.7em}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/rahel_rickenbach-min.jpg}}]{Rahel Rickenbach} is a PhD candidate at ETH Zrich. She received her bachelors degree in mechanical engineering in 2019 and her masters degree in robotics, systems and control in 2021, both from ETH Zrich. In 2022 she was awarded the SGA-Frderpreis for her master thesis. Her main research interests lie in the areas of multi agent and coverage control, as well as inverse optimal control. 
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/koehler-min.jpg}}]{Johannes K\"ohler} received his Master degree in Engineering Cybernetics from the University of Stuttgart, Germany, in 2017. 
In 2021, he obtained a Ph.D. in mechanical engineering, also from the University of Stuttgart,
Germany, for which he received the 2021 European Systems \& Control PhD award.
He is currently a postdoctoral researcher at the Institute for Dynamic Systems and Control (IDSC) at ETH Zrich.
His research interests are in the area of model predictive control and control and estimation for nonlinear uncertain systems. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/scampicchio.jpg}}]{Anna Scampicchio} was born in 1993. She received in 2015 the Bachelor degree in Information Engineering and in 2017 the Masters degree in Automation Engineering, both cum laude, from the University of Padova. In 2017 she was awarded with the Roberto Rocca scholarship for her career during the Masters Degree. She held a visiting position at the Department of Applied Mathematics of University of Washington, Seattle, in 2019. In 2021 she received the Ph.D. in Information Engineering from the University of Padova, and now she is a
postdoctoral researcher at the Institute for Dynamic Systems and Control, ETH Z{\"u}rich. Her research interests lie at the interplay among system identification, machine learning and control design.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/melanie_zeilinger.jpg}}]{Melanie N. Zeilinger} 
is an Associate Professor at ETH Zurich, Switzerland. She received the Diploma degree in engineering cybernetics from the University of Stuttgart, Germany, in 2006, and the Ph.D. degree with honors in electrical engineering from ETH Zurich, Switzerland, in 2011. From 2011 to 2012 she was a Postdoctoral Fellow with the Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland. She was a Marie Curie Fellow and Postdoctoral Researcher with the Max Planck Institute for Intelligent Systems, Tbingen, Germany until 2015 and with the Department of Electrical Engineering and Computer Sciences at the University of California at Berkeley, CA, USA, from 2012 to 2014. From 2018 to 2019 she was a professor at the University of Freiburg, Germany. Her current research interests include safe learning-based control, as well as distributed control and optimization, with applications to robotics and human-in-the-loop control.
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/andrea_carron-min.jpg}}]{Andrea Carron} is a Senior Lecturer at ETH Zrich. He received the bachelors, masters, and Ph.D. degrees in control engineering from the University of Padova, Padova, Italy. During his master and Ph.D. studies, he spent three stays abroad as a Visiting Researcher: the first at the University of California at Riverside, Riverside, CA, USA, the second at the Max Planck Institute, Tubingen, Germany, and the third at the University of California at Santa Barbara, Santa Barbara, CA, USA. From 2016 to 2019, he was a Post-Doctoral Fellow with the Intelligent Control Systems Group, ETH Zrich, Zrich, Switzerland. His is research interests include safe-learning, learning-based control, multiagent systems, and robotics.
\end{IEEEbiography}


\end{document}