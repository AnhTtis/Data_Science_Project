\documentclass{article}



\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[final]{neurips_2022}

\input{math_commands.tex}

\usepackage[dvipsnames,table,xcdraw]{xcolor}
\definecolor{TableGreen}{RGB}{90, 175, 51}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}






\usepackage{xr-hyper}
\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}
\hypersetup{
  citecolor=mydarkblue,
}
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %
\usepackage{xspace}
 \usepackage{float}
\newcommand\mypara[1]{\vspace{1.mm}\noindent\textbf{#1}}

\newcommand{\methodlong}{Rote Domain Adaptation\xspace}
\newcommand{\methodshort}{Rote-DA\xspace}
\newcommand{\postfiltershort}{PO-F\xspace}
\newcommand{\ppfiltershort}{FB-F\xspace}
\newcommand{\ppsupervisionshort}{FB-S\xspace}

\newcommand{\postfilterlong}{Posterior Filtering\xspace}
\newcommand{\ppfilterlong}{Foreground Background Filtering\xspace}
\newcommand{\ppsupervisionlong}{Foreground Background Supervision\xspace}

\newcommand{\lyft}{Lyft\xspace}
\newcommand{\ith}{Ithaca-365\xspace}
\newcommand{\kitti}{KITTI\xspace}
\newcommand{\waymo}{WOD\xspace}
\newcommand{\lidar}{LiDAR\xspace}

\title{Unsupervised Adaptation  from Repeated Traversals for Autonomous Driving}





\author{
Yurong You\thanks{Denotes equal contribution.} \thanks{Correspondences could be directed to \url{yy785@cornell.edu}} $^{1}$\hspace{10pt}
Cheng Perng Phoo\footnotemark[1] $^{1}$\hspace{10pt}
Katie Z Luo\footnotemark[1] $^{1}$\hspace{10pt}
Travis Zhang$^{1}$\hspace{10pt} \\
\textbf{Wei-Lun Chao}$^{2}$ \hspace{10pt}
\textbf{Bharath Hariharan}$^{1}$\hspace{10pt}
\textbf{Mark Campbell}$^{1}$\hspace{10pt}
\textbf{Kilian Q. Weinberger}$^{1}$\\
$^1$Cornell University, Ithaca NY \hspace{14pt}$^2$The Ohio State University, Columbus, OH
}


\begin{document}


\maketitle

\input{sections/abstract}


\input{sections/introduction}
\input{sections/related}
\input{sections/method}
\input{sections/experiment}
\input{sections/discussion}
\input{sections/acknowledgement}






\bibliography{main}
\bibliographystyle{plain}





















\newpage
\vbox{
\centering
    {\LARGE\bf
    Supplementary Material for \\
Unsupervised Adaptation from Repeated Traversals\\
for Autonomous Driving
    \par}
}

\section{Implementation Details}
The parameters that we used in this work were $\beta=0.333$, and $N_c^{\mathcal{S}}$ values are 14357, 2207, and 734 for Cars, Pedestrians, and Cyclists, respectively.
We include an ablation table for different values of $\beta$ in \autoref{tab:lyft-results-beta-ablation}.
For the focal loss, we set $\alpha=0.25$ and $\gamma=2.0$ which are the default values. For the \postfilterlong, we set $\alpha_{\text{\ppfiltershort}}=20$ and $\gamma_{\text{\ppfiltershort}}=0.5$. We selected the best hyperparameters based on the performance on \kitti $\rightarrow$ \lyft and used the same hyperparameters for the rest of the settings.

\input{tables/beta-ablation}

\section{Additional Detection Evaluation on the \lyft dataset}

\subsection{On different metrics.} We include additional evaluations on the Lyft dataset.
In Tables  \ref{tab:lyft-results-07-3d}, \ref{tab:lyft-results-05-bev}, and \ref{tab:lyft-results-05-3d} we show the results with metrics \AP at IoU 0.7 (cars) / 0.5 (pedestrian and cyclists), \APBEV at IoU 0.7 / 0.5, and \AP at IoU 0.5 / 0.25, respectively. This corresponds to \autoref{tab:lyft-results-main} in the main paper.

\input{tables/lyft-results-07-3d}
\input{tables/lyft-results-05-bev}
\input{tables/lyft-results-05-3d}

\subsection{On a different detection model.} In Tables \ref{tab:lyft-pvrcnn-07-bev}, \ref{tab:lyft-pvrcnn-07-3d}, \ref{tab:lyft-pvrcnn-05-bev} and \ref{tab:lyft-pvrcnn-05-3d}, we include additional adaptation results on the PVRCNN~\cite{shi2020pv} model. We use the same hyperparameters as those in the main paper. Since PVRCNN does not have the point-proposal module as in PointRCNN, we apply only PO-F and / or FB-F for adaptation. We observe our method is consistently better than baseline methods.

\input{tables/lyft-pvrcnn-07-bev}
\input{tables/lyft-pvrcnn-07-3d}
\input{tables/lyft-pvrcnn-05-bev}
\input{tables/lyft-pvrcnn-05-3d}

\subsection{Additional adaptation scenario.} In \autoref{tab:waymo2ith365-results} we show adaptation results for different adaptation methods adapting a PointRCNN detector trained on the Waymo Open Dataset~\cite{Sun_2020_CVPR} to the \ith dataset. We observe that our conclusion holds in this case as well, especially in the class of pedestrians with a marked improvement over direct adaptation of the source model.

\input{tables/waymo2ith365}



\section{Additional Qualitative Visualization}
Similar to \autoref{fig:qualitative}, in \autoref{fig:qualitative_supp} we show extra qualitative visualization of the adaptation results of various adaptation strategies in both \lyft and \ith datasets.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/qualitative_vis_supp.pdf}
    \caption{\textbf{Qualitative visualization of adaptation results.} We visualize two more example scenes in the \lyft and \ith test split. Please refer to \autoref{fig:qualitative} for more details.}
    \label{fig:qualitative_supp}
\end{figure}
\end{document}