\section{\methodlong (\methodshort)}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/model_figure.jpg}
    \caption{A schematic layout of \methodshort{}. The PointRCNN Proposal network classifies each input LiDAR point as car/pedestrian/cyclist (Class Predictions) with three binary classifiers. The PP-Score is used during fine-tuning in an auxiliary loss function $L^{cls}_{prop}$ to reduce false-negatives.  The Refinement network produces bounding boxes for the target data (Raw Detection Outputs). For the next self-training round, these are filtered with posterior and foreground/background filtering to reduce false positives, giving rise to the next pseudo-labels (bottom right).  }
    \vspace{-10px}
    \label{fig:model}
\end{figure}

We seek to adapt a 3D object detector pretrained on a certain area/domain (\eg \kitti{}~\citep{geiger2013vision} in Germany) for reliable deployment to a different target area/domain (\eg, \lyft{}~\citep{lyft2019} in the USA). Without loss of generality, we assume all objects of interest are \emph{dynamic} (\eg, cars, pedestrians, and cyclists). Similar to prior work \citep{yang2021st3d,xu2021spg,luo2021MLC-Net,wang2020train}, we assume access to unlabeled target data for adaptation. Crucially different from previous work, we assume that the unlabeled target data are collected from the same routes repeatedly, and the localization information is available for adaptation. We note that such an additional assumption is highly realistic, since with current localization technology \citep{chong2013synthetic,you2022hindsight} these data could be easily collected by the end-users going about their daily lives. %
For simplicity, we will focus on adapting point-based detectors \citep{vora2020pointpainting,you2022exploiting,you2022hindsight,you2022learning,mao2021one,saltori2020sf}, specifically PointRCNN \citep{shi2019pointrcnn} which is one of the current state-of-the-art 3D object detectors. 







\subsection{Background}
\label{section:background}
Our work leverages persistence prior score (PP-score) from multiple traversals~\citep{you2022learning} to adapt PointRCNN to a new, target domain. We review key concepts relevant to the understanding of our approach. 

\mypara{Persistence prior score (PP-score) from multiple traversals.} The PP-score~\citep{you2022learning,barnesephemerality} is an entropy-based measure that quantifies how persistent a single LiDAR point is across multiple traversals. We assume access to unlabeled LiDAR data that are collected from multiple traversals of a set of location $L$; each traversal contains a series LiDAR scans. To calculate the PP-score, we further assume that these LiDAR scans of a traversal $t$ have been pre-processed, such that the LiDAR points around a location $g\in L$ are aggregated to form a dense point cloud $\bm{S}_g^t$. We note that $\bm{S}_g^t$ is only used for PP-score computation, not as an input to 3D object detectors.

Given a single 3D point $\bm{q}$ around location $g$, we can calculate its PP-score by the following steps. First, we count the number of its neighboring points within a certain radius $r$ (say $0.3$m) in each $\bm{S}_g^t$: 
\begin{equation}
    N_t(\bm{q}) = \left|\{\bm{p}_i\hspace{2pt}|\hspace{2pt}\|\bm{p}_i - \bm{q}\|_2 < r, \bm{p}_i \in \bm{S}_g^t\}\right|.
\end{equation}
We then normalize $N_t(\bm{q})$ across traversals $t\in\{1,\cdots, T\}$ into a categorical probability:
\begin{equation}
    P(t;\bm{q}) = \frac{N_t(\bm{q})}{\sum_{t'=1}^T N_{t'}(\bm{q})}.
\end{equation}
With $P(t;\bm{q})$, we can then compute the PP-score $\tau(\vq)$ by
\begin{align}
\tau(\vq) = \begin{cases} 0 & \textrm{if } N_t(\vq) = 0 \;\;\forall t; \\  \frac{H\left(P(t;\vq)\right)}{\log(T)} & \textrm{ otherwise, } \end{cases}
\end{align}
where $H$ is the information entropy. Essentially, the more uniform $P(t;\bm{q})$ is across traversals, the higher the PP-score is. This happens when the neighborhood of $\bm{q}$ is stationary across traversals; \ie, $\bm{q}$ is likely a background point. 
In contrast, a low PP-score indicates that some traversals $t$ have much higher $P(t;\bm{q})$ than some other traversals. This suggests that the neighborhood of $\bm{q}$ is sometimes empty (so low probability) and sometimes occupied (\eg, by a foreground car, so high probability), and when $\bm{q}$ is detected by LiDAR, it is likely reflected from a foreground object. 
  






\mypara{PointRCNN.} PointRCNN~\citep{shi2019pointrcnn} is a two-stage detector. In the first stage, each LiDAR point is classified into a foreground class or background, and a 3D box proposal is generated around each foreground point. The proposals are then passed along to the second stage for bounding box refinement, which refines both the class label and box pose. It is worth noting that this two-stage pipeline is widely adopted in many other detectors~\citep{fernandes2021point,qian20213d,wu2020deep,arnold2019survey}. An understanding and solution to the error patterns of PointRCNN, especially when it is applied to new environments, are thus very much applicable to other detectors. 

By taking a deeper look at the inner working of PointRCNN, we found that if a foreground LiDAR point is misclassified as the background in the first stage, then it is removed from consideration for the refinement (\ie, bound to be a false negative). In our approach, we thus propose to incorporate the PP-score to correct this error during iterative fine-tuning. In the following, we describe the original loss function used to train PointRCNN's first stage.

Let us denote by $N_c$ the number of foreground classes and by ${N_p}$ the number of points in a scene. An annotated point cloud can be represented by a set of tuples $\{(\bm{q_i}, \bm{y_i} , \bm{b_i})\}_{i=1} ^ {N_p}$, where $\bm{y_i}$ is a one-hot $N_c$-dimensional class label vector and $\bm{b_i}$ is the bounding box pose that encapsulates $\bm{q_i}$. The loss function can be decomposed into two terms:
\begin{align}
    L(\{\bm{q_i}, \bm{y_i}, \bm{b_i}\}_{i=1} ^ {N_p}) = \sum_{i=1}^{N_p} L_\text{cls} (\bm{q_i}, \bm{y_i}) + L_\text{reg} (\bm{q_i}, \bm{b_i}).
\end{align}
The first term $L_\text{cls}$ is for per-point classification (or equivalently, \emph{segmentation} of the point cloud). The second term $L_\text{reg}$ is for proposal regression. For the former, a focal loss \citep{lin2017focal} is used: 
\begin{align}
    \frac{1}{\alpha} L_\text{cls}(\bm{q_i}, \bm{y_i}) = \sum_{c=1}^{N_c} y_{ic}(1 - p_{c})^{\gamma} \log(p_{c}) +(1 - y_{ic})  (p_c)^{\gamma} \log(1 - p_c) \label{eq_focal}
\end{align}
where $p_c$ is the one-vs-all probability of class $c$, produced by PointRCNN's first stage; $y_{ic}$ indexes the $c$-th position of $\bm{y_i}$; $\alpha$ and $\gamma$ are hyperparameters for the focal loss (we use default value $\alpha=0.25$ and $\gamma=2.0$).  




\subsection{Adaptation Strategy}
\label{sec:adaptation-strategy}


\mypara{Approach overview.} Our adaptation approach is built upon the conceptually-simple but highly effective self-training for adaptation \citep{lee2013pseudo,you2022exploiting}. The core idea is to iteratively apply the current model to obtain \emph{pseudo-labels} on the unlabeled target data, and use the pseudo-labels to fine-tune the current model. The current model is initialized by the source model (\ie, a pre-trained PointRCNN). Self-training works when the pseudo-labels are of high quality --- in the ideal case that the pseudo-labels are exactly the ground truths, self-training is equivalent to supervised fine-tuning.
In practice, the pseudo-labels can be refined through the iterative process, but errors may also get reinforced. It is therefore important to have a ``quality control'' mechanism on pseudo-labels.

In this section, we propose a set of novel approaches to improve the quality of pseudo-labels, taking advantage of the PP-scores, illustrated in Figure~\ref{fig:model}. 







\mypara{Pseudo-label refinement for false positives removal.} 
Pseudo-labels generated by the source model are often noisy when the source model is first applied to the target data that are different from the source data. As shown in~\citep{wang2020train} and our experiments, PointRCNN suffers a serious performance drop in new environments. Many of the detected boxes are false positives or false negatives. To control the quality, we first leverage the PP-score to identify and filter out false positives.

To assess the quality of a bounding box $b$, we first crop out the points $\{\vq_j\}_{j \in b}$ in it, and query their PP-scores $\{\tau(\vq_j)\}_{j \in b}$. A bounding box is highly likely to be a false positive if 
it contains so many \emph{persistent} points, \ie, points with high PP-scores. To this end, we summarize the PP-scores $\{\tau(\vq_j)\}_{j \in b}$ of a box by the $\alpha_{\text{\ppfiltershort}}$ percentile, and remove the box if the value is larger than a threshold $\gamma_{\text{\ppfiltershort}}$. In our experiments, we set $\alpha_{\text{\ppfiltershort}}=20$ and $\gamma_{\text{\ppfiltershort}}=0.5$ (We find these values are not sensitive).  Since we are effectively filtering out boxes that do not respect the foreground/background segmentation obtained from multiple traversals, we term this filtering approach \emph{\ppfilterlong}(\textbf{\ppfiltershort}).

In addition, we present another complementary way to identify another kind of false positives. Essentially, \ppfiltershort can effectively identify false positives that should have been detected as background. However, it cannot identify false positives that result from wrong classification or size estimates.
Indeed, after \ppfiltershort, we still see a decent amount of this kind of false positives; the remaining pseudo-labels are more numerous than the ground-truth boxes. One naive way to remove them is by thresholding the model's confidence on them. However, setting a suitable threshold is nontrivial in self-training, since the model's confidence will get higher along the iterations.

We thus propose to directly set a cap on the average number of pseudo-labels per class $c$ in a scene. 
We make the following assumption: as long as the source and target domains are not from drastically different areas (\eg, a city vs. barren land), the object frequency in the source domain can serve as a good indicator for what a well-performing object detector should see in the target domain. To this end, we set the cap by $\beta \times \frac{N_{c}^{\gS}}{N_\text{scenes}^{\gS}}$, where $N_\text{scenes}^{\gS}$ and $N_{c}^{\gS}$ are the total number of source training scenes and the ground-truth objects of class $c$ in them, respectively. The value $\beta\in [0,1]$ is a hyperparameter that controls the tightness of the cap. With this cap, after creating pseudo-labels on $N_\text{scenes}^{\gT}$ target scenes we keep the top $\beta \times \frac{N_{c}^{\gS}}{N^{\gS}_\text{scenes}} \times {N_\text{scenes}^{\gT}}$ of them for each class $c$ according to the model's confidence. Given we control the distribution of objects (similar to posterior regularization \citep{ganchev2010posterior_reg}), we term this filtering step \postfilterlong (\postfiltershort). 



\mypara{\ppsupervisionlong (\ppsupervisionshort) for false negatives reduction.} 
\ppfiltershort, as discussed above, can effectively filter out false positives that should have been background. Now we show that the PP-scores are also useful for correcting false negatives.
As mentioned in \autoref{section:background}, the first stage of PointRCNN is the key to false negatives: if a foreground point is misclassified as background, then it is bound to be a false negative. To rectify this, we incorporate the PP-score into the fine-tuning process. Specifically, we modify the pseudo-class-label $\bm{\hat{y}_i}$ of a point $\bm{q_i}$ in \autoref{eq_focal} with PP-score $\tau(\bm{q_i})$:
\begin{align}
    {\bm{y_i}} = 
        \begin{cases}
            \bm{0} & \quad \text{ if }\tau(\bm{q_i}) > \tau_U, \\
            \bm{1} & \quad \text{ if } \tau(\bm{q_i}) < \tau_L \text{ and } \bm{\hat{y}_i} =\bm{0}, \\
            \bm{y_i} &  \quad \text{otherwise.}
        \end{cases}
\end{align}
where $\bm{0}$ is a zero vector and $\bm{1}$ is an all-one vector. Essentially, if a point is persistent (\ie, high $\tau(\bm{q_i})$), we set the the pseudo-class-label as background $\bm{0}$. On the contrary, for a non-persistent point (\ie, low $\tau(\bm{q_i})$) that is deemed as background  (\ie, $\bm{\hat{y}_i} =\bm{0}$) by the current model, we encourage the scores of all the foreground classes to be as high as possible, so that a foreground proposal can be generated. We note that while this foreground class label may be wrong, the subsequent refinement by PointRCNN's second stage can effectively correct it. 













