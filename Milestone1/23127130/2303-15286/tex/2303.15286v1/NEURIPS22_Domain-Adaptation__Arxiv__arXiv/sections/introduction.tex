\section{Introduction}

Autonomous vehicles and driver-assist systems require 3D object detectors to accurately identify and locate other traffic participants (cars, pedestrians and so on) to drive safely~\citep{shi2019pointrcnn, wang2019pseudo,lang2019pointpillars,shi2020pv,qi2018frustum}.
Modern 3D object detectors achieve high accuracy on benchmark datasets~\citep{geiger2012we,lyft2019,zheng2021se,xu2021behind,xu2021spg,wu2022sparse,zhu2021vpfnet}. 
However, most benchmark data sets train and test classifiers on essentially the same locations (city, country), time, and  weather conditions, and therefore represent the ``best case'' of an end-user using the self-driving car in precisely the same conditions it was trained on.
A more realistic scenario is that self-driving cars trained in, for example, Germany will be driven in the USA.
Unfortunately, past work has shown that this domain gap results in a catastrophic drop in accuracy~\citep{wang2020train}.
Given that an end-user may choose to operate their car wherever they please, adapting the perception pipeline effectively to such domain shifts is a critical challenge.




An obvious solution is to retrain the detector in the target domain (\ie, the end-user's location/environment).
Unfortunately, this requires large amounts of labeled data, where expert human annotators painstakingly locate every object in LiDAR scans in every conceivable location.
Such labeled data is all but impossible to obtain in sufficient quantity. However, \emph{unlabeled} data is not.
No matter where the end-user intends to use their car, likely thousands of cars drive there every day already. 
By simply logging the data collected by cars with adequate drive-assist sensors, 
one obtains a wealth of information about the local environment, which should be useful to adapt a detector to this new domain.
But it is unclear how to use this data: in particular, how can the detector learn to correct its many mistakes in this new domain if it has no labels at all?

The key here is the fact that this unlabeled data is not just an arbitrary collection of unrelated scenes.
If we look at a population of cars driving around in a city, we observe that they all visit a shared set of roads and intersections.
Indeed, as pointed out by~\cite{you2022learning}, any single vehicle will probably be driven on the same route, day in and day out (e.g., commute, grocery shopping, patrol routes).
Even when one end-user takes their car on a new route, it is likely that other cars have taken that very route not long before.
This fact implies that the unlabeled data obtained from cars will typically contain \textit{multiple traversals of the same route}, obtained for free without any targeted data collection.
Previous work has already shown that aggregating data from multiple traversals can aid visual odometry~\citep{barnesephemerality} and unsupervised object discovery~\citep{you2022learning}.


In this paper we argue that multiple traversals are particularly suited for end-user domain adaptation.
We assume the existence of unlabeled LiDAR data from several repeated traversals of routes within the target domain (e.g. collected a few hours or days apart).
For a LiDAR point captured in any one of these traversals, we use the other traversals to 
compute a persistency prior (PP-score)~\citep{you2022learning}, capturing how persistent this LiDAR point has been across traversals: persistent points are likely static background.
The PP-score thus yields a proxy signal for foreground vs background.
This provides a powerful signal to correct both false positives and false negatives: detector outputs that mostly capture background points are likely false positives, and foreground points that are not captured by any detection reveal false negatives.
To formalize this intuition, we propose a new iterative fine-tuning approach.
We use the detector to generate 3D bounding boxes along the recorded traversals but remove boxes with lots of persistent (and thus static) points as false positives.
We fine-tune the detector on this filtered data, and then ``rinse and repeat''.
To reduce false negatives during this training, we introduce a new auxiliary loss that forces the detector to classify non-persistent LiDAR points as foreground.
We refer to our method as \emph{\methodlong{} (\methodshort{})}. 



The resulting approach is a simple modification of existing object detectors, but offers substantial accuracy gains in unsupervised domain adaptation.
We demonstrate on the Lyft~\citep{lyft2019} and Ithaca-356~\citep{carlos2022ithaca365} benchmark data sets that our approach consistently leads to drastic improvements when adapting a detector trained on KITTI~\citep{geiger2012we} to the local environments --- in some categories even outperforming a dedicated model trained on hand labeled data in the target domain (which we intended as an upper bound). 









