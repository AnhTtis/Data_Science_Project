\vspace{-5pt}
\section{Experiments}
\vspace{-5pt}
\label{sec:exp}
\mypara{Datasets.} We validate our approach on a single source dataset, the KITTI dataset~\citep{geiger2013vision} and two target datasets: the Lyft Level 5 Perception dataset~\citep{lyft2019} and the Ithaca-365 dataset~\citep{carlos2022ithaca365}. The \kitti dataset is collected in Karlsruhe, Germany, while the \lyft and \ith dataset is collected in Palo Alto (California) and Ithaca (New York) in the US respectively. Such setup is chosen to simulate large domain difference~\citep{wang2020train}. To show good generalizability, we use \emph{exactly the same} hyper-parameters for adaptation experiments on these two target datasets.

To the best of our knowledge\footnote{
We note that though there are some scenes with multiple traversals in the nuScenes dataset~\cite{caesar2020nuscenes} as used in \cite{you2022hindsight,you2022learning}, the localization in $z$-axis is not accurate (\url{https://www.nuscenes.org/nuscenes\#data-format}).}, \lyft and \ith are the only two publicly available autonomous driving datasets that have both bounding box annotations and multiple traversals with accurate 6-DoF localization. 
We use these two datasets to test out two different adaptation scenarios.
The first scenario is that the detector is trained on data from nearby locations, \emph{but not from the roads and intersections it will be driven on}. 
Thus, following \citep{you2022learning}, we split the \lyft dataset so that the ``train''/test set are \emph{geographically disjoint}; we also discard locations with less than 2 traversals in the ``train'' set. This results a ``train''/test split of 11,873/4,901 point clouds for the \lyft dataset. We use all traversals available (2-10 in the dataset) to compute PP-score for each scene.

The second adaptation scenario is when the detector uses unlabeled data \emph{from the same routes that it sees at test time}.
This scenario is highly likely in practice since, as mentioned before, a self-driving car can leverage data collected by other cars on the same route previously.
To test this, we split the \ith dataset based on the data  collection date, keeping the same geographical locations in both train and test.
This results in 4445/1644 point clouds. The ``train'' sets of these two target datasets are used without labels. We use the roof LiDAR (40/60-beam in \lyft; 128-beam in \ith), and the global 6-DoF localization with the calibration matrices directly from the raw data. We do not use the intensity channel of the LiDAR data due to drastic difference in sensor setups between datasets. We use 5 traversals to compute PP-score for each scene.

We pre-train the 3D object detection models on the train split (3,712 point clouds) of \kitti datasets to detect \emph{Car}, \emph{Pedestrian} and \emph{Cyclist} classes, and adapt them to detect the same objects in the \lyft and \emph{Car}, \emph{Pedestrian} in the \ith since there too few Cyclist in the dataset to provide reasonable performance estimate. Since KITTI only provides 3D object labels within frontal view, we focus on frontal view object detection only during adaption and evaluation.

\mypara{Evaluation metric.} On the \lyft dataset, we follow \cite{you2022hindsight} to evaluate object detection in the bird's-eye view (BEV) and in 3D for the mobile objects by \kitti~\citep{geiger2012we} metrics and conventions: we report average precision (AP) with the intersection over union (IoU) thresholds at 0.7/0.5 for Car and 0.5/0.25 for Pedestrian and Cyclist. We further follow \cite{wang2020train} to evaluate the AP at various depth ranges. Due space constraint, we present \APBEV at IoU=0.7 for Car and 0.5 for Pedestrian and Cyclist in the main text and defer the rest of the results to the supplementary materials. On the \ith dataset, the default match criterion is by the minimum distance to ground-truth bounding boxes. We evaluate the mean of AP with match thresholds of \{0.5, 1, 2, 4\} meters for Car and Pedestrian. We follow \cite{you2022hindsight} to evaluate only detection in frontal view.

\mypara{Implementation of PointRCNN.} 
We use the default implementation/configuration of PointRCNN~\citep{shi2019pointrcnn} from OpenPCDet~\citep{openpcdet2020}. For fine-tuning, we fine-tune the model for 10 epochs with learning rate $1.5 \times 10^{-3}$ (pseudo-labels are regenerated and refined after each epoch). All models are trained/fine-tuned with 4 GPUs (NVIDIA 2080Ti/3090/A6000). 

\mypara{Comparisons.} We compare the proposed method against two methods with publicly available implementation: Statistical Normalization (SN) \citep{wang2020train} and ST3D \citep{yang2021st3d}. SN \emph{assumes access to mean car sizes of target domain}, and applies object sizes scaling to address the domain gap brought by different car sizes. Since there is less variability on box sizes among pedestrians and cyclists, we only scale the car class using the target domain statistics. ST3D achieves adaptation via self-training on the target data with stronger augmentation and maintaining a memory bank of high quality pseudolabels. 

\subsection{Adaptation performance on \kitti $\rightarrow$ \lyft and \ith}
\input{tables/lyft-results-07-bev}
\input{tables/ith365-results}
In \autoref{tab:lyft-results-main} and \autoref{tab:ith365-results-main} we show the adaptation performance of adapting a KITTI pre-trained PointRCNN detection model to the \lyft and the \ith datasets. 
We observe that despite its simplicity, \methodshort outperforms all baselines on almost all metrics, across both datasets and across object types, confirming the potent learning signal from multiple traversals. 
Note that the hyper-parameters are kept as exactly the same between experiments in these two datasets, showing the strong generalizability of \methodshort.  
While SN is more accurate than \methodshort for cars on \lyft, it uses external information about car sizes that is unavailable to the other techniques, and that is not useful for other classes.


\methodshort works especially well on the challenging categories of pedestrians and cyclists, almost doubling the performance on cyclists and even outperforming an in-domain detector in some scenarios (pedestrians, 0-30 m range). In contrast, prior domain adaptation strategies actually \emph{hurt} performance for these categories.
For e.g., ST3D through the course of self-training gradually over-fits to cars and ``forgets'' pedestrians and cyclists (comparing row ST3D(R10) and ST3D(R30), see also \autoref{fig:qualitative}). 

Interestingly, when \methodshort has access to unlabeled data from past traversals of the test routes (as on the \ith dataset), the performance gains are even more significant, especially on the mid-to-far ranges (30-80m), improving accuracy by more than 10$\times$ for cars in the 50-80m range. 



\subsection{Analysis}
Unless otherwise stated, we conduct the following study on the Lyft dataset.

\input{tables/lyft-p2-ablations}
\mypara{Effects of different components. } We ablate different components of \methodshort: pseudo-labels refinement and \ppsupervisionlong (\ppsupervisionshort) in \autoref{tab:ablation-bev}. To start, vanilla self-training without any of the components would only yield marginal improvements to detecting cars whereas performance of the adapted detectors for the rarer classes (pedestrians, cyclists) degrade significantly compared to no adaptation. \postfilterlong (\postfiltershort) is an effective strategy to prevent performance degradation. Combining \ppfilterlong (\ppfiltershort) with \postfiltershort would always yield significant improvements regardless of classes, showing usefulness of using PP-score for filtering and the efficacy of our filtering pseudo-label refinement strategy. Combining the \ppsupervisionlong(\ppsupervisionshort)  with only \postfiltershort would not be effective always but combining \ppsupervisionshort with the full pseudo-label refinement procedure would would bring forth significant improvements especially on cyclist. 

\mypara{Effects of different rounds of iterative fine-tuning. } As customary to any iterative approach, we analyze the effect of the number of rounds of self-training in \autoref{fig:rounds_analysis}. One conclusion is immediate: vanilla self-training degrades  (even underperforms no adaptation) over more rounds of self-training potentially due to learning from erroneous pseudo-labels. \methodshort (and its variants) improves for the first few rounds of training (before the 10th round), and experience little to no performance degradation over more rounds of training.  



\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/ablate_rounds_analysis.pdf}
    \caption{Performance of various detectors on \kitti $\rightarrow$ \lyft for different rounds of self-training (averaged across 3 runs with mean and one standard deviation reported). Van. ST stands for vanilla self-training without any modification; Dir. Apply stands for direct applying the source detector without any adaptation. We observe that the performance for vanilla self-training degrades over more rounds of self-training whereas variants of \methodshort experience little to no degradation in performance after 10 rounds of training. }
    \label{fig:rounds_analysis}
    \vspace{-10pt}
\end{figure}

\mypara{Effect of \ppsupervisionlong (\ppfiltershort). } \ppfiltershort seeks to reduce false negatives by correcting the foreground predictions by the model. To validate this claim, we plot the precision-recall curves of various detectors in \autoref{fig:prcurve}. Comparing \methodshort and \postfiltershort +\ppfiltershort, we observe that the max recall for \methodshort is much higher than \postfiltershort +\ppfiltershort, suggesting  \ppfiltershort is encouraging the detector to produce more meaningful boxes at foreground regions, thus reducing false negatives. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/PR_0-80.pdf}
    \caption{\textbf{Precision-recall curves on the \kitti $\rightarrow$ \lyft with 10 rounds of self-training.} We show the P-R curves of ablated \methodshort, please refer to \autoref{fig:rounds_analysis} for naming. The precision and recall is calculated under \APBEV with IoU=0.7 for Cars, 0.5 for Pedestrians and Cyclists.}
    \label{fig:prcurve}
\end{figure}












\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/qualitative_vis_v2.pdf}
    \caption{\textbf{Qualitative visualization of adaptation results.} We visualize one example scene (up:LiDAR, bottom: image (not used in the adaptation)) in the \lyft and \ith test split, and plot the detection results with various adaptation strategies. Ground-truth bounding boxes are shown in green, detection boxes of no adaptation, ST3D, SN and \methodshort are shown in yellow, red, orange and cyan, respectively. Zoom in for details. Best viewed in color.}
    \label{fig:qualitative}
\end{figure}

\mypara{Qualitative visualization.} In \autoref{fig:qualitative}, we visualize the adaptation results of various adaptation strategies in both \lyft and \ith datasets. We observe that, aligning with quantitative results, ST3D has a good coverage of cars but usually ignores pedestrians and cyclists and generates many false positive cars; SN successfully corrects the car size bias, but can hardly improve the recall of the detection; \methodshort adapts to the object size bias in the target domain while having a good recall rate of all three object classes.

\mypara{Additional results, analyses, qualitative visualization.} Please refer to the supplementary material for evaluation with more metrics, results on a different detection model (PVRCNN~\cite{shi2020pv}) and on a different adaptation scenario (Waymo Open Dataset~\cite{Sun_2020_CVPR} to \ith), and more qualitative results.