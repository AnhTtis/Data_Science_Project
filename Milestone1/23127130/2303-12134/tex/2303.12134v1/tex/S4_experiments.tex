\section{Datasets and  Experiments}

A key challenge in acquiring training data for the SML network is the lack of RGB-D+IMU datasets. In our pipeline, IMU data is needed to run VIO to generate sparse metric depth. While simulators allow recording synchronized RGB-D and IMU data~\cite{AirSim2017}, manually gathering sufficient training data is difficult. We select TartanAir \cite{TartanAir2020} for its large size and variety of outdoor and indoor sequences. IMU data is not provided in this dataset. To proxy sparse depth map generation, we run the VINS-Mono feature tracker front-end~\cite{Lusk2018} to obtain sparse feature locations and then sample ground truth depth at those locations. We use a 70\%-30\% train-test split for TartanAir, with 172K training and 73K test samples taken from both easy and hard sequences.

% The training split (172K samples) includes scenes from abandoned factory, amusement, carwelding, endofworld, hospital, japanesealley, oldtown, office, seasidetown, and soulcity. The test split (73K samples) includes scenes from office, neighborhood, and westerndesert. Both easy and hard sequences are used.

In addition to the synthetic TartanAir dataset, we benchmark on VOID \cite{Wong2020void}, which offers real-world data collected using an Intel RealSense D435i camera and the VIO system XIVO \cite{Fei2019xivo}. This dataset is smaller than TartanAir, with only 47K training and 800 test samples. We use the published train-test split. 

\mypara{Setup.} We use MiDaS-small \cite{Ranftl2020} to construct our SML network. The encoder backbone is initialized with pretrained ImageNet \cite{Deng2009imagenet} weights, while other layers are initialized randomly. We use AdamW \cite{Loshchilov2019} with $\beta_{1}=0.9$, $\beta_{2}=0.999$, and $\lambda=0.001$. We set an initial learning rate of $5\times10^{-4}$ when training on TartanAir and $3\times10^{-4}$ on VOID. We additionally use a step-based scheduler that halves the learning rate after 5 epochs on TartanAir and after 8 epochs on VOID. We train for 20 epochs on a node with 8 GeForce RTX 2080 Ti GPUs, with a batch size of 256, and with mixed-precision training enabled. Input data is resized and cropped to a training resolution of $384\timess384$. Training takes up to 4 hours.

\mypara{Metrics.} We mainly evaluate in inverse depth space $\mathbf{z}=1/\mathbf{d}$ (in km$^{-1}$), as doing so penalizes error at closer depth ranges more significantly. We compute inverse mean absolute error iMAE $= \frac{1}{M}\sum_{i=1}^{M}|\mathbf{z}^{*}_{i}-\hat{\mathbf{z}}_{i}|$, inverse root mean squared error iRMSE $= [\frac{1}{M}\sum_{i=1}^{M}|\mathbf{z}^{*}_{i}-\hat{\mathbf{z}}_{i}|^{2}]^\frac{1}{2}$, and inverse absolute relative error iAbsRel $= \frac{1}{M}\sum_{i=1}^{M}|\mathbf{z}^{*}_{i}-\hat{\mathbf{z}}_{i}|/\mathbf{z}^{*}_{i}$. On VOID, we also compute MAE and RMSE in regular depth space $\mathbf{d}$ (in mm). 

We follow the VOID evaluation protocol of Wong et al.~\cite{Wong2020void,Wong2021kbnet} and consider ground truth depth to be valid between 0.2 and 5.0 meters. The minimum and maximum depth prediction values in these works are set to 0.1 and 8.0 meters, respectively. We clamp depth predictions, both after global alignment and after applying regressed dense scale maps, to this range. In contrast to the mostly-indoor scenes in VOID, outdoor scenes in TartanAir exhibit larger depth ranges. For TartanAir, we define ground truth depth to be valid between 0.2 and 50 meters and clamp predictions between 0.1 and 80 meters.

\subsection{Evaluation on TartanAir}

We first evaluate on \emph{synthetic} samples from the TartanAir dataset, where inertial data is unknown. To proxy sparse depth generation from VIO, we preprocess TartanAir data with a sparsifier that samples depth from the ground truth at locations determined via the VINS-Mono-based feature tracker implemented in \cite{Lusk2018}. We run monocular depth estimation with DPT-Hybrid, perform global alignment against metric sparse depth, and generate a scale map scaffolding for every sample prior to SML training. We define our baseline as global alignment only and show that performing dense scale alignment with SML improves metric depth accuracy. Table \ref{tab:tartanair_eval} shows that SML achieves 30\%, 17\%, and 26\% reduction in iMAE, iRMSE, and iAbsRel, respectively. Metrics are aggregated across a set of 690 samples taken from our TartanAir test split.

\begin{table}
  \footnotesize
  \centering
    \caption{Evaluation on TartanAir. Lower is better for all metrics.} 
    \label{tab:tartanair_eval}
    \vspace{-4pt}
    \begin{tabular}{@{}l@{\hspace{5mm}}c@{\hspace{5mm}}*{2}{S[table-format=2.2]@{\hspace{2mm}}}*{1}{S[table-format=1.3]@{\hspace{0mm}}}@{}}
    \toprule
    Method  & Depth Model                   & {iMAE}          & {iRMSE}         & {iAbsRel}       \\
    \midrule
    GA only & \multirow{2}{*}{DPT-Hybrid}   &           22.94 &           35.49 &           0.126 \\
    GA+SML  &                               & \bfseries 16.11 & \bfseries 29.48 & \bfseries 0.093 \\
    \midrule
    GA only & \multirow{2}{*}{MiDaS v2.0}   & 58.11           & 79.34           & 0.299 \\
    GA+SML  &                               & \bfseries 28.79 & \bfseries 46.67 & \bfseries 0.156 \\
    \bottomrule
  \end{tabular}
  \vspace{-12pt}
\end{table}

Figure \ref{fig:vis_tartanair} provides a visualization of our approach on several TartanAir samples. Performance is qualitatively evaluated by comparing metric depth error for globally-aligned depth (GA error) versus densely-scaled depth (SML error). A whiter region in the error map indicates that SML improved metric depth accuracy there. The first sample depicts a neighborhood scene where the building towards center-right is pushed further back under dense scale alignment; this is confirmed by a reduction in negative (blue) error in inverse depth. The tree behind the pool is brought closer, as shown by the reduction in positive (red) error. The latter two samples depict significantly more challenging scenes due to low light as well as proximity to walls and the ground. In both, the SML still aligns surfaces towards the correct metric depth.

We note that DPT-Hybrid was trained on a large mixed dataset containing TartanAir. To remove any potential bias this contributes to SML evaluation on TartanAir, we swap in MiDaS v2.0~\cite{Ranftl2020} that has not seen any TartanAir data during training. Table \ref{tab:tartanair_eval} shows that MiDaS v2.0 still yields the same trends as DPT-Hybrid, with SML improving all metrics. 

\begin{figure}
\centering
% \footnotesize
  \begin{tabular}{@{}*{6}{c@{\hspace{0.5mm}}}c@{}}
    {\scriptsize RGB Image} & {\scriptsize GA Depth} & {\scriptsize SML Depth} & {\scriptsize GT Depth} & {\scriptsize GA Error} & {\scriptsize SML Error}\\
    \vspace{-0.75mm}
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_0_col_0.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_0_col_2.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_0_col_3.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_0_col_4.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_0_col_5.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_0_col_6.png}&
    \includegraphics[width=0.035\linewidth]{fig/vis_tartanair/row_0_cbar.pdf}\\
    \vspace{-0.75mm}
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_1_col_0.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_1_col_2.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_1_col_3.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_1_col_4.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_1_col_5.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_1_col_6.png}&
    \includegraphics[width=0.035\linewidth]{fig/vis_tartanair/row_1_cbar.pdf}\\
    \vspace{-0.75mm}
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_2_col_0.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_2_col_2.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_2_col_3.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_2_col_4.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_2_col_5.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_tartanair/row_2_col_6.png}&
    \includegraphics[width=0.035\linewidth]{fig/vis_tartanair/row_2_cbar.pdf}\\
  \end{tabular}
  \vspace{-8pt}
  \caption{Our method tested on TartanAir samples. In depth maps, brighter is closer and darker is farther. In error maps, red is positive inverse depth error (farther than ground truth GT) and blue is negative inverse depth error (closer than GT). Whiter regions in error indicate improved metric depth accuracy.} 
  \label{fig:vis_tartanair}
  \vspace{-12pt}
\end{figure}

\subsection{Evaluation on VOID}

We additionally evaluate on real-world data from the VOID dataset. We preprocess VOID data in the same manner as the TartanAir data, but using the sparse depth provided in the published dataset \cite{Wong2020void}. The first two rows of Table \ref{tab:void_eval} summarize our results when training SML directly on VOID. SML again improves over global alignment, with a 38\%, 30\%, and 39\% reduction in iMAE, iRMSE, and iAbsRel, respectively.

\begin{table}
  \footnotesize
  \setlength{\tabcolsep}{1pt} % cols space (default: 6pt)
  \renewcommand{\arraystretch}{1} % rows space (default: 1)
  \centering
  \caption{Evaluation on VOID. All methods use DPT-H as the depth model and 150 sparse depth points. Lower is better.}
  \label{tab:void_eval}
  \vspace{-4pt}
  \begin{tabular}{@{}l@{\hspace{2mm}}l@{\hspace{2mm}}*{2}{S[table-format=3.2]@{\hspace{1.5mm}}}*{1}{S[table-format=2.2]@{\hspace{1mm}}}*{1}{S[table-format=3.2]@{\hspace{1mm}}}*{1}{S[table-format=1.3]@{\hspace{0mm}}}@{}} %{@{}lc@{} c@{}}
    \toprule
    Method  & Training Set                  & {MAE}             & {RMSE}            & {iMAE}           & {iRMSE}          & {iAbsRel} \\
    \midrule
    GA only &                               &           165.33  &           243.11  &           75.74  &          106.37  & 0.103 \\
    GA+SML  & VOID                     &   \myuline{97.03} &  \myuline{167.82} &           46.62  &           74.67  & 0.063 \\
    GA+SML  & TA (zero-shot)    &            98.49  &           175.04  &  \myuline{45.55} &  \myuline{74.28} & \myuline{0.062} \\
    GA+SML  & TA + VOID     &  \bfseries 82.65  & \bfseries 153.51  & \bfseries 38.56  & \bfseries 66.23  & \bfseries 0.051 \\
    \bottomrule
  \end{tabular}
  \vspace{-12pt}
\end{table}

\mypara{TartanAir-to-VOID transfer.} We investigate the performance of SML when trained on TartanAir and evaluated on VOID without any finetuning (i.e., zero-shot cross-dataset transfer). This can be interpreted as a sim-to-real transfer experiment, since TartanAir consists solely of synthetic data and VOID contains real-world data samples. We observe that zero-shot testing on VOID achieves very similar error as when training directly on VOID. If evaluating in inverse depth space, zero-shot transfer even slightly outperforms direct training on VOID. This is particularly notable since it demonstrates that training on a large quantity of diverse synthetic data can indeed translate to improved real-world performance. It also shows the generalizability of our pipeline. DPT-Hybrid is already known to generalize well after having been trained on a massive mixed dataset with scale- and shift-invariant loss functions. The SML network is trained using metric loss terms; however, some metric information is provided to SML via the globally-aligned depth and scale map scaffolding inputs. Since SML only needs to learn to refine this scaffolding, it is less likely to memorize or overfit to a specific metric scale.

\mypara{Pretraining.} Pretraining on TartanAir and fine-tuning on VOID yields the lowest error across all metrics. We use this combination to produce the results visualized for samples in Figure \ref{fig:vis_void}. The first sample suffers from blurriness in the RGB input and depicts a cluttered scene. With global alignment only, depth predictions appear flattened: the table is aligned to be farther than ground truth (red error), while background surfaces such as walls and the floor are aligned to be closer than ground truth (blue error). Dense scale alignment with SML helps to rectify this, with noticeable reduction (whiter regions) throughout the error map. The second sample shows a staircase; in addition to reducing depth error on the steps, SML is able to correctly realign the handrail on the left. This is impressive as pixels near the image boundary fall outside the convex hull of known sparse depth points, and the scale map scaffolding that we input to SML signals no information at pixels outside the convex hull. The last sample shows a challenging viewpoint of the floor leading to a staircase in the top right corner. Global alignment alone misjudges the depth gradient at the staircase edge. SML corrects this and also reduces depth error elsewhere on the floor surface.

\begin{figure}
\centering
% \footnotesize
  \begin{tabular}{@{}*{6}{c@{\hspace{0.5mm}}}c@{}}
    {\scriptsize RGB Image} & {\scriptsize GA Depth} & {\scriptsize SML Depth} & {\scriptsize GT Depth} & {\scriptsize GA Error} & {\scriptsize SML Error}\\
    % \vspace{-0.75mm}
    %\includegraphics[width=0.155\linewidth]{fig/vis_void/row_0_col_0.pdf}&
    %\includegraphics[width=0.155\linewidth]{fig/vis_void/row_0_col_2.pdf}&
    %\includegraphics[width=0.155\linewidth]{fig/vis_void/row_0_col_3.pdf}&
    %\includegraphics[width=0.155\linewidth]{fig/vis_void/row_0_col_4.pdf}&
    %\includegraphics[width=0.155\linewidth]{fig/vis_void/row_0_col_5.pdf}&
    %\includegraphics[width=0.155\linewidth]{fig/vis_void/row_0_col_6.pdf}&
    %\includegraphics[width=0.035\linewidth]{fig/vis_void/row_0_cbar.pdf}\\
    \vspace{-0.75mm}
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_1_col_0.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_1_col_2.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_1_col_3.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_1_col_4.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_1_col_5.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_1_col_6.png}&
    \includegraphics[width=0.035\linewidth]{fig/vis_void/row_1_cbar.pdf}\\
    \vspace{-0.75mm}
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_2_col_0.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_2_col_2.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_2_col_3.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_2_col_4.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_2_col_5.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_2_col_6.png}&
    \includegraphics[width=0.035\linewidth]{fig/vis_void/row_2_cbar.pdf}\\
    \vspace{-0.75mm}
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_4_col_0.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_4_col_2.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_4_col_3.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_4_col_4.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_4_col_5.png}&
    \includegraphics[width=0.155\linewidth]{fig/vis_void/row_4_col_6.png}&
    \includegraphics[width=0.035\linewidth]{fig/vis_void/row_4_cbar.pdf}\\
  \end{tabular}
  \vspace{-8pt}
  \caption{Our method tested on VOID samples. SML is pretrained on TartanAir and fine-tuned on VOID. Color coding is the same as in Figure \ref{fig:vis_tartanair}.}
  \label{fig:vis_void}
  \vspace{-8pt}
\end{figure}

\begin{table}
  \centering
  \footnotesize
%   \sisetup{detect-weight=true,detect-inline-weight=math,table-align-text-post=false}
  \caption{Comparison on VOID. Lower is better for all metrics.}
  \label{tab:void_comparison}
  \vspace{-4pt}
  \begin{tabular}{@{}
  l@{\hspace{1mm}}|
  l@{\hspace{2mm}}
  S[table-format=3.2]@{\hspace{2mm}} 
  S[table-format=3.2]@{\hspace{2mm}} 
  S[table-format=2.2]@{\hspace{2mm}} 
  S[table-format=3.2]@{\hspace{0mm}} @{}}
    \toprule
    & Method               & {MAE} & {RMSE}  & {iMAE} & {iRMSE} \\
    \midrule
    \multirow{5}{*}{\rot{150 points}} & VOICED-S \cite{Wong2020void}    & 174.04 & 253.14 & 87.39 & 126.30 \\
    & KBNet \cite{Wong2021kbnet}      & 131.54 & 263.54 & 66.84 & 128.29 \\
    & GA+SML (DPT-BEiT-Large)      & \bfseries 76.95 & \bfseries 142.85 & \bfseries 34.25 & \bfseries 57.13 \\
    & GA+SML (DPT-Hybrid)      & \myuline{97.03} & \myuline{167.82} & \myuline{46.62} & \myuline{74.67} \\
    & GA+SML (MiDaS-small)    & 113.27 & 193.38 & 53.86 &  84.82 \\
    \midrule
    \multirow{5}{*}{\rot{500 points}} & VOICED-S \cite{Wong2020void}    & 118.01 & 195.32 & 59.29 & 101.72 \\
    & KBNet \cite{Wong2021kbnet}  & \myuline{77.70} & 172.49 & 38.87 &  85.59 \\
    & GA+SML (DPT-BEiT-Large)      & \bfseries 66.14 & \bfseries 126.44 & \bfseries 28.92 & \bfseries 49.85 \\
    & GA+SML (DPT-Hybrid)       &  81.30 & \myuline{146.16} & \myuline{37.35} &  \myuline{60.92} \\
    & GA+SML (MiDaS-small)    &  94.81 & 164.36 & 43.19 &  69.25 \\
    \bottomrule
  \end{tabular}
  \vspace{-12pt}
\end{table}

\mypara{Comparison to related work.} Our evaluation thus far has compared the impact of SML relative to global alignment only. We now compare to related work on VOID. Table \ref{tab:void_comparison} lists VOICED \cite{Wong2020void} and state-of-the-art KBNet \cite{Wong2021kbnet} alongside our approach (GA+SML). Figure~\ref{fig:vis-void-comparison} shows a qualitative comparison.

In addition to using DPT-Hybrid as the depth model in our pipeline, we try DPT-BEiT-Large for its higher accuracy and MiDaS-small for its computational efficiency. With just 150 sparse depth points, our approach GA+SML outperforms KBNet across all metrics, regardless of what depth estimator we use; improvement in iRMSE ranges from 34\% to 55\%. From Table \ref{tab:void_eval}, we see that even with zero-shot transfer, our method outperforms KBNet by 42\% in iRMSE. At a higher density of 500 points, our pipeline with DPT-BEiT-Large continues to outperform KBNet across all metrics.

\vspace{-12pt}
\begin{figure}[h]
\centering
% \footnotesize
  \begin{tabular}{@{}*{6}{c@{\hspace{0.5mm}}}@{}}
    \vspace{-1mm}
    &  &  & {\scriptsize GA+SML} & {\scriptsize GA+SML} & {\scriptsize GA+SML}\\
    {\scriptsize RGB Image} & {\scriptsize GT Depth} & {\scriptsize KBNet~\cite{Wong2021kbnet}} & {\scriptsize DPT-BEiT-L} & {\scriptsize DPT-H} & {\scriptsize MiDaS-s}\\
    \vspace{-0.75mm}
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/kbnet/image/0000000060.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/kbnet/ground_truth/0000000060.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/kbnet/output_depth/0000000060.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/dpt-beit-l/row_1_col_5.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/dpt-hybrid/row_1_col_5.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/midas-small/row_1_col_5.pdf}\\
    \vspace{-0.75mm}
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/kbnet/image/0000000280.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/kbnet/ground_truth/0000000280.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/kbnet/output_depth/0000000280.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/dpt-beit-l/row_4_col_5.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/dpt-hybrid/row_4_col_5.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/midas-small/row_4_col_5.pdf}\\
    % \vspace{-0.75mm}
    % \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/kbnet/image/0000000390.pdf}&
    % \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/kbnet/ground_truth/0000000390.pdf}&
    % \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/kbnet/output_depth/0000000390.pdf}&
    % \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/dpt-beit-l/row_6_col_5.pdf}&
    % \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/dpt-hybrid/row_6_col_5.pdf}&
    % \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/midas-small/row_6_col_5.pdf}\\
    \vspace{-0.75mm}
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/kbnet/image/0000000570.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/kbnet/ground_truth/0000000570.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/kbnet/output_depth/0000000570.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/dpt-beit-l/row_9_col_5.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/dpt-hybrid/row_9_col_5.pdf}&
    \includegraphics[width=0.155\linewidth]{suppl/comparison_void_150/midas-small/row_9_col_5.pdf}\\
    \vspace{-0.75mm}
  \end{tabular}
  \vspace{-8pt}
  \caption{Qualitative comparison of our approach against state-of-the-art KBNet on the VOID 150 dataset. SML is trained only on VOID.}
  \label{fig:vis-void-comparison}
  \vspace{-8pt}
\end{figure}

\subsection{Generalizability and Deployability}

We test zero-shot generalization on NYU Depth v2~\cite{Silberman:ECCV12} and VOID, comparing against NLSPN~\cite{park2020nlspn} (state of the art on NYUv2) and KBNet (state of the art on VOID). These models, having been trained on a single dataset as is commonplace with depth completion tasks, underperform when run on a different dataset. Table~\ref{tab:nyu_and_void} shows that our approach consistently achieves better generalization performance.

\begin{table}
%   \vspace{-8pt}
  \centering
  \footnotesize
    \caption{Testing zero-shot generalizability on NYUv2 and VOID. DPT-Hybrid is used as the depth predictor for GA+SML.}
    \label{tab:nyu_and_void}
    \vspace{-4pt}
    \begin{tabular}{@{}
    c@{\hspace{1mm}}|
    l@{\hspace{4mm}}
    S[table-format=3.1]@{\hspace{2mm}}
    S[table-format=3.1]@{\hspace{2mm}}|
    l@{\hspace{4mm}}
    S[table-format=3.1]@{\hspace{1mm}}
    S[table-format=3.1]@{\hspace{0mm}}
    @{}}
    \toprule
     & \multicolumn{3}{c|}{\textit{NYUv2 (train) $\rightarrow$ VOID (test)}} & \multicolumn{3}{c}{\textit{VOID (train) $\rightarrow$ NYUv2 (test)}} \\
    & Method & {iMAE} & {iRMSE} & Method & {iMAE} & {iRMSE}\\
    \midrule
    150 & NLSPN \cite{park2020nlspn} & 143.0 & 238.1 & KBNet \cite{Wong2021kbnet} & 35.2 & 67.8 \\
    pts &  GA+SML & \bfseries 55.9 & \bfseries 85.2 &  GA+SML & \bfseries 30.2 & \bfseries 48.9\\
    \midrule
    500 & NLSPN \cite{park2020nlspn} & 87.9 & 174.7 & KBNet \cite{Wong2021kbnet} & 28.0 & 57.2\\
    pts &  GA+SML & \bfseries 43.9 & \bfseries 69.5 & GA+SML & \bfseries 26.8 & \bfseries 44.5 \\
    \bottomrule
  \end{tabular}
  \vspace{-6pt}
\end{table}

We also test on rosbags from an entirely new dataset, VCU-RVI~\cite{vcurvi}. Figure~\ref{fig:vcu_samples} shows samples where available sparse metric depth is much lower in quantity than the 150+ points we have so far trained and tested with. Our pipeline still succeeds in resolving metric scale, with SML reducing depth error.

\begin{figure}
\centering
\footnotesize
  \begin{tabular}{@{}*{6}{c@{\hspace{0.5mm}}}c@{}}
    % \vspace{-0.75mm}
    {\scriptsize RGB} & {\scriptsize Sparse} & {\scriptsize Scaffold} & {\scriptsize Regressed} & {\scriptsize GT Depth} & {\scriptsize GA Depth} & {\scriptsize SML Depth}\\
    \vspace{-0.75mm}
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_22_col_0.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_22_col_1.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_22_col_2.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_22_col_3.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_22_col_6.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_22_col_4.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_22_col_5.pdf}\\
    & \scriptsize 66 points & & & \multicolumn{3}{c}{\hspace{8pt}\scriptsize{iRMSE = 177.9 $\rightarrow$ 139.2}} \\
    \vspace{-0.75mm}
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_36_col_0.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_36_col_1.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_36_col_2.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_36_col_3.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_36_col_6.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_36_col_4.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_36_col_5.pdf}\\
    & \scriptsize 52 points & & & \multicolumn{3}{c}{\hspace{5pt}\scriptsize{iRMSE = 118.7 $\rightarrow$ 93.4}} \\
    \vspace{-0.75mm}
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_7_col_0.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_7_col_1.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_7_col_2.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_7_col_3.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_7_col_6.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_7_col_4.pdf}&
    \includegraphics[width=0.135\linewidth]{fig/vis_vcu/row_7_col_5.pdf}\\
    & \scriptsize 51 points & & & \multicolumn{3}{c}{\hspace{8pt}\scriptsize{iRMSE = 51.98 $\rightarrow$ 37.85}} \\
  \end{tabular}
  \vspace{-2pt}
  \caption{Our method tested on lab and corridor samples from the VCU-RVI dataset. RGB and sparse metric depth come from published rosbag data.}
  \label{fig:vcu_samples}
  \vspace{-12pt}
\end{figure}

To demonstrate deployability, we benchmark performance on the NVIDIA Jetson AGX Orin platform and show a breakdown of component runtime in Table~\ref{tab:performance_tx2}. Measurements are averaged over 100 runs after 20 warmup runs. With acceleration via TensorRT, our depth alignment pipeline, in conjunction with a lightweight depth predictor like MiDaS-small, is viable for on-device metric depth estimation. Scale map scaffolding is one bottleneck as interpolation within the convex hull presently runs on the CPU. Data movement between the GPU and host CPU is another bottleneck that we expect can be reduced with additional engineering effort.

% \vspace{-12pt}
\begin{table}[h]
  \centering
  \footnotesize
  \caption{Runtime [ms] on Jetson AGX Orin in MAX-N mode. All pipeline variants are tested with 150 sparse metric depth points.}
  \label{tab:performance_tx2}
  \vspace{-4pt}
  \begin{tabular}{@{}
    l@{\hspace{4mm}}
    S[table-format=3.1]@{\hspace{2mm}}
    S[table-format=3.1]@{\hspace{2mm}}
    S[table-format=3.1]@{\hspace{2mm}}
    S[table-format=3.1]@{\hspace{0mm}}
    @{}}
    \toprule
    Depth predictor & {DPT-BEiT-L} & {DPT-H} & {MiDaS-s} & {MiDaS-s-TRT} \\
    Inference resolution & {384$\times$384} & {384$\times$384} & {256$\times$256} & {256$\times$256}\\
    \midrule
    Depth inference         & 144.8     & 53.9      & 29.2      & 1.5   \\
    D2H copy depth map      & 12.8      & 18.4      & 0.6       & 5.2   \\
    Global alignment        & 2.6       & 2.5       & 1.3       & 1.3   \\
    Scale map scaffolding   & 12.2      & 12.1      & 6.7       & 6.6   \\
    H2D copy SML inputs     & 3.3       & 3.3       & 2.4       & 2.2   \\
    SML-TRT inference       & 2.2       & 2.2       & 1.7       & 1.7   \\
    \midrule
    Total                   & 177.9     & 92.5      & 41.9      & 18.5  \\
    \bottomrule
  \end{tabular}
  \vspace{-6pt}
\end{table}

\input{tab/ablations}

\subsection{Ablations and Analysis}

We experiment with a number of input and regressed data modalities when designing the SML network.

\mypara{Input data modalities.} SML takes in two inputs: \textit{globally-aligned inverse depth} $\tilde{\mathbf{z}}$ and a \textit{scale map scaffolding}. We experiment with four additional inputs: (1) a \textit{confidence map} derived from a binary map pinpointing known sparse depth locations, first dilated with a $7\timess7$ circular kernel and then blurred with a $5\timess5$ Gaussian kernel to mimic confidence spread around a fixed known point; (2) a \textit{gradient map} computed using the Scharr operator; (3) a \textit{grayscale} conversion of the original RGB image; (4) and the RGB image itself. Inputs are concatenated in the channel dimension and fed into SML as a single tensor. Table \ref{tab:ablations} reports the impact of different input combinations on the metric accuracy of SML depth.

Globally-aligned depth alone is not sufficient for the network to learn dense scale regression well. An input scale map scaffolding is necessary. Conceptually, this acts as an initial guess at the dense scale map that the network is learning to regress. Without an accompanying scale map input, the confidence map negligibly improves SML learning; however, using both slightly underperforms compared to using only scale scaffolding. This is surprising, as the confidence map is meant to signal which regions in the input depth and scale scaffolding are more trustworthy. It may be that our representation of confidence is not being parsed well by SML, or that the scale map scaffolding encodes similar information, e.g., boundaries of the convex hull and approximate positions of interpolation anchors corresponding to known sparse metric depth. Incorporating edge representations in the form of gradient maps, grayscale, or RGB images, does not appear to be beneficial. This can be partly attributed to the high quality of depth predictions output by DPT, as those depth maps already exhibit clear edges and surfaces. RGB input actually worsens performance, implying that color cues are not particularly useful in dense metric scale regression.

Since we are also interested in cross-dataset transfer, we evaluate zero-shot performance of every input combination on VOID and report the results in Table \ref{tab:ablations}. Combined depth and scale scaffolding result in noticeably lower error; we therefore select this input combination for SML. 

\vspace{12pt}
\mypara{Regressing scale and shift.} SML learns dense (per-pixel) scale factors by which to multiply input depth estimates $\tilde{\mathbf{z}}$, such that the output depth $\hat{\mathbf{z}}$ achieves higher metric accuracy. The network is allowed to regress negative values as scale residuals $\mathbf{r}$, such that the output depth is $\hat{\mathbf{z}}=\text{ReLU}(1+\mathbf{r})\tilde{\mathbf{z}}$.
Our design choice to regress scale is motivated by scale factors having a more intuitive interpretation in projective geometry. Scaling a depth value at a pixel location can be interpreted as zooming in (pulling closer) or zooming out (pushing further) the object at that location in 3D space. It is more difficult to intuit the impact of shifting depth at individual pixels. We conduct two experiments that involve shift, listed in the bottom two rows of Table \ref{tab:ablations}. We regress only dense shift $\mathbf{t}$, such that the output prediction $\hat{\mathbf{z}}=\tilde{\mathbf{z}}+\mathbf{t}$. We also regress shift $\mathbf{t}$ alongside scale residuals $\mathbf{r}$, where $\hat{\mathbf{z}}=\text{ReLU}(1+\mathbf{r})\tilde{\mathbf{z}}+\mathbf{t}$. For the latter, we add a second output head to the SML network, while the encoder and decoder layers remain common to both regression tasks. When training with shift regression, our default learning rate of $5\timess10^{-4}$ prohibits loss convergence and necessitates a slightly lower one of $4\timess10^{-4}$. Overall, regressing shift does not significantly impact performance on TartanAir, and zero-shot testing on VOID indicates that regressing scale only is the most robust choice for cross-dataset transfer.