\begin{figure*}[t]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.80\linewidth]{fig/methodology_diagram.pdf}
   \caption{Our visual-inertial depth estimation pipeline. There are three stages: (1) input processing, where RGB and IMU data feed into monocular depth estimation alongside visual-inertial odometry, (2) global scale and shift alignment, where monocular depth estimates are fitted to sparse depth from VIO in a least-squares manner, and (3) learning-based dense scale alignment, where globally-aligned depth is locally realigned using a dense scale map regressed by the ScaleMapLearner (SML). The row of images at the bottom illustrate a VOID \cite{Wong2020void} sample being processed through the pipeline; from left to right: the input RGB, ground truth depth, sparse depth from VIO, globally-aligned depth, scale map scaffolding, dense scale map regressed by SML, final depth output.}
   \label{fig:methodology}
  \vspace{-12pt}
\end{figure*}

\section{Method}

We develop a modular three-stage pipeline for visual-inertial depth estimation. Its structure is illustrated in Figure \ref{fig:methodology}.

\mypara{Monocular depth estimation.} The \textit{visual} branch of our pipeline predicts depth from a single image. This is done using a pretrained model that takes in a single RGB image and produces a dense depth map up to some scale.
Monocular processing is appealing as it allows for low-complexity architectures that do not carry large computational costs. 

Our approach is compatible with  traditional convolutional models as well as newer architectures. We select DPT-Hybrid \cite{Ranftl2021} as our depth estimator; this is a transformer-based model trained on a large meta-dataset using scale- and shift-invariant losses. While it achieves high generalizability, its output measures depth relations between pixels, and depth values do not carry metric meaning. Our alignment pipeline aims to recover metric scale for every pixel in this output depth map.

\mypara{Visual-inertial odometry.} The \textit{inertial} branch of our pipeline uses IMU data together with visual data to determine metric scale. Given a sequence of RGB images with synchronized IMU data, we run VINS-Mono \cite{Qin2018} to compute the camera trajectory and yield a set of 3D world coordinates of features tracked throughout the sequence. In a reasonably textured environment, we can expect tens of tracked features per frame. By projecting feature coordinates to image space, we obtain a sequence of sparse maps containing metric depth values. These sparse depth maps serve as inputs to later alignment tasks, thereby propagating metric scale through our pipeline. 

\mypara{Global scale and shift alignment (GA).} Let $\mathbf{z}$ refer to unit-less affine-invariant inverse depth that is output by a monocular depth estimation model such as DPT-Hybrid. To reintroduce metric scale into depth, we align monocular depth estimates to sparse metric depth obtained through VIO. This global alignment is performed in inverse depth space based on a least-squares criterion \cite{Ranftl2020}. The result is a per-frame global scale $s_g$ and global shift $t_g$ that are applied to $\mathbf{z}$ as a linear transformation. Applying global scale can be interpreted as bringing depth values to a correct order of magnitude, while applying global shift can help undo potential bias or offset in the original prediction. The resulting globally-aligned depth estimates are $\tilde{\mathbf{z}} = s_g\mathbf{z}+t_g$. 

\mypara{Dense (local) scale alignment.} Due to its coarse nature, global alignment will not adequately resolve metric scale in all regions of a depth map. To address this, we propose a learning-based approach for determining dense (per-pixel) scale factors that are applied to globally-aligned depth estimates. Using MiDaS-small~\cite{Ranftl2020}, we construct a network that is trained to realign individual pixels in a depth map to improve their metric accuracy. We call this network the ScaleMapLearner (SML) and feed it an input of two concatenated data channels: the \textit{globally-aligned depth} $\tilde{\mathbf{z}}$, and a \textit{scaffolding for a dense scale map}, where $n$ locations of known sparse depth values $\mathbf{v}$ from VIO define $n$ scale anchor points $\mathbf{v}_{i}/\tilde{\mathbf{z}}_{i}$, $i\in\{1...n\}$. The region within the convex hull defined by the anchors is filled via linear interpolation of anchor values. The region outside the convex hull is filled with an identity scale value of 1.

SML regresses a dense scale residual map $\mathbf{r}$ where values are allowed to be negative. We compute the resulting scale map as $\text{ReLU}(1+\mathbf{r})$ and apply it to the input depth $\tilde{\mathbf{z}}$ to produce the output depth $\hat{\mathbf{z}} = \text{ReLU}(1+\mathbf{r})\tilde{\mathbf{z}}$. %Both $\tilde{\mathbf{z}}$ and $\hat{\mathbf{z}}$ are kept in inverse depth space.

\mypara{Loss function.} During training, the SML network is supervised on metric ground truth $\mathbf{z}^{*}$ in inverse depth space. Let $M$ define the number of pixels with valid ground truth. Our loss function comprises two terms: an L1 loss on depth,
\begin{equation}
    \mathcal{L}_{depth}(\hat{\mathbf{z}},\mathbf{z}^{*}) = \frac{1}{M}\sum_{i=1}^{M}|\mathbf{z}^{*}_{i}-\hat{\mathbf{z}}_{i}|
\end{equation}
and a multiscale gradient matching term \cite{MegaDepthLi2018} that biases discontinuities to coincide with discontinuities in ground truth,
\begin{equation}
    \mathcal{L}_{grad}(\hat{\mathbf{z}},\mathbf{z}^{*}) = \frac{1}{K}\sum_{k=1}^{K}\frac{1}{M}\sum_{i=1}^{M}(|\nabla_{x}R_{i}^{k}|+|\nabla_{y}R_{i}^{k}|)
\end{equation}
where $R_{i}=\mathbf{z}^{*}_{i}-\hat{\mathbf{z}}_{i}$ and $R^{k}$ denotes error at different resolutions. We use $K=3$ levels, halving the spatial resolution at each level. Our final loss is $\mathcal{L} = \mathcal{L}_{depth} + 0.5\mathcal{L}_{grad}$. %, with $\alpha=0.5$.

\mypara{Decoupling visual and inertial data.} Our pipeline runs monocular depth estimation and VIO in parallel and independently of each other. The intermediate outputs from these steps are then fused together to generate inputs to SML. This design choice is made to better leverage ongoing advances in monocular depth and VIO systems; newly developed modules can be easily integrated within our pipeline, and SML can be quickly retrained to benefit from the improved performance of those modules. We contrast this with designing a single unified network that learns metric depth directly from a joint RGB-IMU input. A sufficiently large corpus of RGB-D datasets containing IMU data to train such a network and have it generalize well does not exist. We still face a data challenge when training SML; however, by decoupling RGB-to-depth and VIO at the input, we provide SML with an intermediate data representation that simplifies what it needs to learn to perform metric depth alignment. In this setting, a smaller amount of training data is sufficient.