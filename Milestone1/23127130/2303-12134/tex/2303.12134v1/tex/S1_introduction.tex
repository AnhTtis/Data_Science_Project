\section{Introduction}
\label{sec:intro}

Depth perception is fundamental to visual navigation, where correctly estimating distances can help plan motion and avoid obstacles. Accurate depth estimation can also aid scene reconstruction, mapping, and object manipulation. Some applications of estimated depth benefit when it is \textit{metrically accurate}---when every depth value is provided in absolute metric units and represents physical distance.


Algorithms for dense depth estimation can be broadly grouped into several categories. Stereo-based approaches rely on two or more cameras that capture different views. Structure-from-motion (SfM) tries to estimate scene geometry from a sequence of images taken by a moving camera, but it is difficult to recover depth with absolute scale since the relative pose of the camera across images is not known. Monocular approaches require just one camera and try to estimate depth from a single image. Such approaches are appealing since simple RGB cameras are compact and ubiquitous. However, monocular approaches that rely solely on visual data still exhibit scale ambiguity. 


Incorporating inertial data can help resolve scale ambiguity, and most mobile devices already contain inertial measurement units (IMUs). Simultaneous localization and mapping (SLAM) systems \cite{Engel2014LSDSLAMLD,MurArtal2015ORBSLAMAV,MurArtal2017ORBSLAM2AO} use visual or visual-inertial data to track scene landmarks under camera motion, compute the camera trajectory, and map the traversed environment. However, SLAM systems typically only track on the order of hundreds to thousands of sparse feature points, resulting in metric depth measurements that are only semi-dense at best. Our work explores how to use inertial data in conjunction with monocular visual data to produce fully-dense metrically accurate depth predictions as in Figure \ref{fig:intro_teaser}.

\begin{figure}[t]
\centering
% \footnotesize
  \begin{tabular}{@{}*{4}{c@{\hspace{0.5mm}}}c@{}}
    \vspace{-0.75mm}
    \includegraphics[width=0.24\linewidth]{fig/vis_void/row_0_col_0.png}&
    \includegraphics[width=0.24\linewidth]{fig/teaser_visual/depth_3d.png}&
    \includegraphics[width=0.24\linewidth]{fig/teaser_visual/int_depth_3d.png}&
    \includegraphics[width=0.24\linewidth]{fig/teaser_visual/pred_3d.png}\\
    \scriptsize RGB & \scriptsize ground truth & \scriptsize GA output & \scriptsize GA+SML output\\
  \end{tabular}
  \caption{We integrate visual-inertial odometry and monocular depth estimation to produce dense depth with metric scale. Global alignment (GA) determines appropriate global scale, while dense scale alignment (SML) operates locally and pushes or pulls regions towards correct metric depth. Here, with GA+SML, objects are aligned more accurately, the center desk leg is straightened, and the top of the desk is pulled forward.}
  \label{fig:intro_teaser}
  \vspace{-12pt}
\end{figure}


Recent advances in supervised learning-based monocular depth estimation \cite{Ranftl2020,Ranftl2021} provide high generality but do not resolve absolute metric scale and only predict relative depth. Works that use inertial data to inform metric scale typically perform depth completion given a set of known sparse metric depth points and tend to be self-supervised in nature due to a lack of visual-inertial datasets \cite{Wong2020void,Wong2021kbnet}. We seek to bridge these approaches by leveraging monocular depth estimation models trained on diverse datasets and recovering metric scale for individual depth estimates.


Our approach performs least-squares fitting of monocular depth estimates against sparse metric depth, followed by learned local per-pixel adjustment. This combination of global and dense (local) depth alignment successfully rectifies metric scale, with dense alignment consistently outperforming a purely global alignment baseline. Alignment succeeds with just 150 metric depth anchors and is robust to zero-shot cross-dataset transfer. Our pipeline is modular and is agnostic to the monocular depth estimation model and VIO system being used; it should thus benefit from continual improvement in these modules.
