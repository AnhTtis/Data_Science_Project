\section{Related Work}

Monocular depth estimation is inherently an ill-posed problem facing challenges like scale ambiguity. A common approach to handling this in supervised training has been to limit training data to particular datasets with desired environments, e.g., indoor or outdoor scenes. This encourages the supervised network to memorize a metric scale that may be globally inconsistent, results in overfitting to specific depth ranges, and hurts generalizability across environments. Recent work on dataset mixing and training loss construction \cite{Ranftl2020} has enabled robust affine-invariant monocular depth estimation across a variety of datasets. However, recovering absolute metric scale in these depth estimates remains a challenge.

\mypara{Using inertial and pose information.} Incorporating inertial data is being explored as a means of improving metric depth accuracy in self-supervised depth estimation approaches. Fei et al.~\cite{Fei2019geo} propose using global orientation from inertial measurements to regularize depth regression at training time, with an expanded loss function that penalizes planarity deviation based on gravity vectors estimated through VIO. SelfVIO \cite{Almalioglu2019SelfVIOSD} combines learning-based VIO and depth estimation to develop an adversarially trained architecture that jointly estimates ego-motion and dense depth from input RGB and IMU readings. A number of additional works incorporate pose into supervised and unsupervised approaches \cite{Patil2020DontFT,Teed2020DeepV2D,Liu2019NeuralRS,Xie2020VideoDE}, often as part of pose consistency and reprojection terms, or as a pose estimation task that is performed jointly with depth estimation. In the latter case, replacing pose networks with pose estimation from VIO/SLAM is known to improve performance~\cite{Fei2019geo,Wong2020void}.

\mypara{Depth completion from sparse depth.} Sparse depth maps or sparse point clouds, e.g., obtained with LiDAR or through VIO tracking, commonly serve as input to metric depth completion. In VOICED \cite{Wong2020void}, sparse depth from VIO is used as a depth scaffold that is refined to minimize photometric, pose, and depth consistency losses. KBNet~\cite{Wong2021kbnet} adds camera calibration and connects sparse depth and RGB encoders with backprojection layers. Other recent works also explore visual-inertial depth completion~\cite{Sartipi2020DeepDE,Merrill2021RobustMV}, although they rely on depth completion networks that are trained primarily on indoor data, thus limiting generality.

\mypara{Video depth estimation.} In the absence of inertial data, given an ordered sequence of images, temporal correlation can be used to improve scale consistency of monocular depth estimates, though still without absolute scale. CVD \cite{Luo2020cvd} leverages SfM \cite{schoenberger2016sfm} to estimate camera parameters and define geometric constraints that help resolve global scale consistency across per-frame depth maps predicted from monocular video input. Since SfM may fail under challenging motion, Robust CVD \cite{Kopf2021rcvd} replaces it with pose estimation and optimization done jointly with depth scale realignment based on a bilinear spline. In both methods, absolute metric scale remains unknown.

Our work aims to resolve scale ambiguity by performing global and local depth alignment in absolute metric space, given an off-the-shelf monocular depth estimation model and VIO system. Instead of designing novel depth estimation architectures and training procedures, we build upon existing monocular depth models and realign their output depth estimates. We do not perform depth completion~\cite{Wong2020void,Luo2020cvd}, but rather align an already-dense depth map to absolute metric scale. This is a more versatile approach as it can incorporate arbitrary monocular depth estimation models.

