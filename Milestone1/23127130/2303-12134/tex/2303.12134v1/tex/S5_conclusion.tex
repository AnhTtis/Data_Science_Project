\section{Conclusion}

Combining metric accuracy and high generalizability is a key challenge in learning-based depth estimation. We propose incorporating inertial data into the visual depth estimation pipeline---not through sparse-to-dense depth completion, but rather through dense-to-dense depth alignment using estimated and learned scale factors. Inertial measurements inform and propagate metric scale through global and local depth alignment. We show improved error reduction with learning-based local alignment over least-squares global alignment alone, and demonstrate successful zero-shot cross-dataset transfer from synthetic training data to real-world test data. Our modular approach supports direct integration of existing and future monocular depth estimation and visual-inertial odometry systems. It succeeds in resolving metric scale for metrically-ambiguous monocular depth estimates, and we hope that it will assist in the deployment of robust and general monocular depth estimation models.
