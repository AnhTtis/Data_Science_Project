
@InProceedings{Ronneberger2015,
    author="Ronneberger, Olaf
    and Fischer, Philipp
    and Brox, Thomas",
    editor="Navab, Nassir
    and Hornegger, Joachim
    and Wells, William M.
    and Frangi, Alejandro F.",
    title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
    booktitle="MICCAI 2015",
    year="2015",
    publisher="Springer International Publishing",
    pages="234--241",
}
@ARTICLE{Fechter2017,
  title    = "Esophagus segmentation in {CT} via {3D} fully convolutional
              neural network and random walk",
  author   = "Fechter, Tobias and Adebahr, Sonja and Baltas, Dimos and Ben
              Ayed, Ismail and Desrosiers, Christian and Dolz, Jose",
 
  journal  = "Medical Physics",
  volume   =  44,
  number   =  12,
  pages    = "6341--6352",
  month    =  oct,
  year     =  2017,
  address  = "United States",
  keywords = "CT; convolutional neural network; esophagus; image processing;
              segmentation",
  language = "en"
}
@INPROCEEDINGS{Milletari2016,
  author={Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
  booktitle={2016 Fourth International Conference on 3D Vision (3DV)}, 
  title={V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation}, 
  year={2016},
  pages={565-571}
}

  @article{Sinha2019MultiScaleSA,
  title={Multi-Scale Self-Guided Attention for Medical Image Segmentation},
  author={Ashish Sinha and Jos{\'e} Dolz},
  journal={IEEE Journal of Biomedical and Health Informatics},
  year={2019},
  volume={25},
  pages={121-130}
}
@InProceedings{Zhang2019,
author="Zhang, Zhijie
and Fu, Huazhu
and Dai, Hang
and Shen, Jianbing
and Pang, Yanwei
and Shao, Ling",
editor="Shen, Dinggang
and Liu, Tianming
and Peters, Terry M.
and Staib, Lawrence H.
and Essert, Caroline
and Zhou, Sean
and Yap, Pew-Thian
and Khan, Ali",
title="ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation",
booktitle="MICCAI 2019",
year="2019",
pages="442--450",
isbn="978-3-030-32239-7"
}
@article{YangLi2021,
title = {TA-Net: Triple attention network for medical image segmentation},
journal = {Computers in Biology and Medicine},
volume = {137},
year = {2021},
issn = {0010-4825},
author = {Yang Li and Jun Yang and Jiajia Ni and Ahmed Elazab and Jianhuang Wu},
keywords = {Deep learning, Medical image segmentation, Spatial attention, Channel attention, Self-attention}
}

@ARTICLE{Gu2021,
  author={Gu, Ran and Wang, Guotai and Song, Tao and Huang, Rui and Aertsen, Michael and Deprest, Jan and Ourselin, Sébastien and Vercauteren, Tom and Zhang, Shaoting},
  journal={IEEE Transactions on Medical Imaging}, 
  title={CA-Net: Comprehensive Attention Convolutional Neural Networks for Explainable Medical Image Segmentation}, 
  year={2021},
  volume={40},
  number={2},
  pages={699-711}
  }
  
@article{HEINRICH20191,
title = {OBELISK-Net: Fewer layers to solve 3D multi-organ segmentation with sparse deformable convolutions},
journal = {Medical Image Analysis},
volume = {54},
pages = {1-9},
year = {2019},
issn = {1361-8415},
author = {Mattias P. Heinrich and Ozan Oktay and Nassim Bouteldja},
keywords = {Image segmentation, Deep learning, Sparse kernels, Deformable convolutions}
}
@InProceedings{Kakeya2018,
author="Kakeya, Hideki
and Okada, Toshiyuki
and Oshiro, Yukio",
editor="Frangi, Alejandro F.
and Schnabel, Julia A.
and Davatzikos, Christos
and Alberola-L{\'o}pez, Carlos
and Fichtinger, Gabor",
title="3D U-JAPA-Net: Mixture of Convolutional Networks for Abdominal Multi-organ CT Segmentation",
booktitle="MICCAI 2018",
year="2018",

pages="426--433",
isbn="978-3-030-00937-3"
}
@INPROCEEDINGS{Yagi2019,
  author={Yagi, Naomi and Nii, Manabu and Kobashi, Syoji},
  booktitle={2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)}, 
  title={Abdominal Organ Area Segmentation using U-Net for Cancer Radiotherapy Support}, 
  year={2019},
  pages={1210-1214}
  }

@article{Alom2018,
  author    = {Md. Zahangir Alom and
               Mahmudul Hasan and
               Chris Yakopcic and
               Tarek M. Taha and
               Vijayan K. Asari},
  title     = {Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net)
               for Medical Image Segmentation},
  
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.06955},
  eprinttype = {arXiv},
  eprint    = {1802.06955},
  timestamp = {Thu, 27 Oct 2022 07:46:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-06955.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DIAKOGIANNIS202094,
title = {ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {162},
pages = {94-114},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620300149},
author = {Foivos I. Diakogiannis and François Waldner and Peter Caccetta and Chen Wu},
keywords = {Convolutional neural network, Loss function, Architecture, Data augmentation, Very high spatial resolution}
}
  @INPROCEEDINGS{Wang2019,
  author={Wang, Zhao-Hui and Liu, Zhe and Song, Yu-Qing and Zhu, Yan},
  booktitle={2019 IEEE International Conference on Image Processing (ICIP)}, 
  title={Densely connected deep U-Net for abdominal multi-organ segmentation}, 
  year={2019},
  pages={1415-1419}
  }

@article{Ozan2018,
  author    = {Ozan Oktay and
               Jo Schlemper and
               Lo{\"{\i}}c Le Folgoc and
               Matthew C. H. Lee and
               Mattias P. Heinrich and
               Kazunari Misawa and
               Kensaku Mori and
               Steven G. McDonagh and
               Nils Y. Hammerla and
               Bernhard Kainz and
               Ben Glocker and
               Daniel Rueckert},
  title     = {Attention U-Net: Learning Where to Look for the Pancreas},
  journal   = {CoRR},
  volume    = {abs/1804.03999},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.03999},
  eprinttype = {arXiv},
  eprint    = {1804.03999},
  timestamp = {Tue, 17 Sep 2019 14:15:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-03999.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{LCTSC,
  url = {https://wiki.cancerimagingarchive.net/x/e41yAQ},
  author = {Yang, Jinzhong and Sharp, Greg and Veeraraghavan, Harini and Van Elmpt, Wouter and Dekker, Andre and Lustberg, Tim and Gooding, Mark},
  title = {Data from Lung CT Segmentation Challenge 2017 (LCTSC)},
  publisher = {The Cancer Imaging Archive},
  year = {2017},
  copyright = {Creative Commons Attribution 3.0 Unported}
}

@misc{Lambert2019,
  url = {https://arxiv.org/abs/1912.05950},
  author = {Lambert, Z. and Petitjean, C. and Dubray, B. and Ruan, S.},
  title = {SegTHOR: Segmentation of Thoracic Organs at Risk in CT images},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{ouali2020,
  title={Semi-supervised semantic segmentation with cross-consistency training},
  author={Ouali, Yassine and Hudelot, C{\'e}line and Tami, Myriam},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12674--12684},
  year={2020}
}
@article{chen2021,
  title={A deep learning-based auto-segmentation system for organs-at-risk on whole-body computed tomography images for radiation therapy},
  author={Chen, Xuming and Sun, Shanlin and Bai, Narisu and Han, Kun and Liu, Qianqian and Yao, Shengyu and Tang, Hao and Zhang, Chupeng and Lu, Zhipeng and Huang, Qian and others},
  journal={Radiotherapy and Oncology},
  volume={160},
  pages={175--184},
  year={2021},
  publisher={Elsevier}
}

@article{Ibragimov2017,
  title={Segmentation of organs-at-risks in head and neck CT images using convolutional neural networks},
  author={Ibragimov, Bulat and Xing, Lei},
  journal={Medical physics},
  volume={44},
  number={2},
  pages={547--557},
  year={2017},
  publisher={Wiley Online Library}
}

@article{Wang2019b,
  title={Organ at risk segmentation in head and neck ct images using a two-stage segmentation framework based on 3D U-Net},
  author={Wang, Yueyue and Zhao, Liang and Wang, Manning and Song, Zhijian},
  journal={IEEE Access},
  volume={7},
  pages={144591--144602},
  year={2019},
  publisher={IEEE}
}

@article{Men2019,
  title={More accurate and efficient segmentation of organs-at-risk in radiotherapy with convolutional neural networks cascades},
  author={Men, Kuo and Geng, Huaizhi and Cheng, Chingyun and Zhong, Haoyu and Huang, Mi and Fan, Yong and Plastaras, John P and Lin, Alexander and Xiao, Ying},
  journal={Medical physics},
  volume={46},
  number={1},
  pages={286--292},
  year={2019},
  publisher={Wiley Online Library}
}

@article{Henderson2022,
  title={Optimising a 3D convolutional neural network for head and neck computed tomography segmentation with limited training data},
  author={Henderson, Edward GA and Osorio, Eliana M Vasquez and Van Herk, Marcel and Green, Andrew F},
  journal={Physics and Imaging in Radiation Oncology},
  volume={22},
  pages={44--50},
  year={2022},
  publisher={Elsevier}
}




@article{Francis2022,
  title={ThoraxNet: a 3D U-Net based two-stage framework for OAR segmentation on thoracic CT images},
  author={Francis, Seenia and Jayaraj, PB and Pournami, PN and Thomas, Manu and Jose, Ajay Thoomkuzhy and Binu, Allen John and Puzhakkal, Niyas},
  journal={Physical and Engineering Sciences in Medicine},
  volume={45},
  number={1},
  pages={189--203},
  year={2022},
  publisher={Springer}
}

@article{Tanno2021,
  title={Uncertainty modelling in deep learning for safer neuroimage enhancement: demonstration in diffusion MRI},
  author={Tanno, Ryutaro and Worrall, Daniel E and Kaden, Enrico and Ghosh, Aurobrata and Grussu, Francesco and Bizzi, Alberto and Sotiropoulos, Stamatios N and Criminisi, Antonio and Alexander, Daniel C},
  journal={NeuroImage},
  volume={225},
  pages={117366},
  year={2021},
  publisher={Elsevier}
}

@article{Hu2020,
  title={Coarse-to-fine adversarial networks and zone-based uncertainty analysis for NK/T-cell lymphoma segmentation in CT/PET images},
  author={Hu, Xiaobin and Guo, Rui and Chen, Jieneng and Li, Hongwei and Waldmannstetter, Diana and Zhao, Yu and Li, Biao and Shi, Kuangyu and Menze, Bjoern},
  journal={IEEE journal of biomedical and health informatics},
  volume={24},
  number={9},
  pages={2599--2608},
  year={2020},
  publisher={IEEE}
}

@article{Balagopal2021,
  title={A deep learning-based framework for segmenting invisible clinical target volumes with estimated uncertainties for post-operative prostate cancer radiotherapy},
  author={Balagopal, Anjali and Nguyen, Dan and Morgan, Howard and Weng, Yaochung and Dohopolski, Michael and Lin, Mu-Han and Barkousaraie, Azar Sadeghnejad and Gonzalez, Yesenia and Garant, Aurelie and Desai, Neil and others},
  journal={Medical image analysis},
  volume={72},
  pages={102101},
  year={2021},
  publisher={Elsevier}
}

%========================================================



@book{Lamport:Book:1989,
 author = {Lamport, Leslie},
 title = {Latex: A Document Preparation System},
 year = {1986},
 isbn = {0-201-15790-X},
 publisher = {Addison-Wesley Longman Publishing Co., Inc.},
 address = {Boston, MA, USA},
}

@misc{Hinton:arXiv:2015:Distilling,
 title={Distilling the knowledge in a neural network},
 author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
 OPTeprinttype={arXiv},
 OPTeprint={1503.02531},
 howpublished={Eprint \href{http://arxiv.org/abs/1503.02531}{arXiv:1503.02531}},
 OPTurl={http://arxiv.org/abs/1503.02531},
 year=2015
}


%added by riad

%9497733
@ARTICLE{Ma2022,
  author={Ma, Jun and Zhang, Yao and Gu, Song and Zhu, Cheng and Ge, Cheng and Zhang, Yichi and An, Xingle and Wang, Congcong and Wang, Qiyuan and Liu, Xin and Cao, Shucheng and Zhang, Qi and Liu, Shangqing and Wang, Yunpeng and Li, Yuhui and He, Jian and Yang, Xiaoping},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={AbdomenCT-1K: Is Abdominal Organ Segmentation a Solved Problem?}, 
  year={2022},
  volume={44},
  number={10},
  pages={6695-6714},
  doi={10.1109/TPAMI.2021.3100536}}

%DELANEY202252
@article{DELANEY2022,
title = {Surgeon Variation in the Application of Robotic Technique for Abdominal Hernia Repair: A Mixed-Methods Study},
journal = {Journal of Surgical Research},
volume = {279},
pages = {52-61},
year = {2022},
issn = {0022-4804},
doi = {https://doi.org/10.1016/j.jss.2022.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0022480422003250},
author = {Lia D. Delaney and Jyothi Thumma and Ryan Howard and Quintin Solano and Brian Fry and Justin B. Dimick and Dana A. Telem and Anne P. Ehlers},
keywords = {Hernia repair, Mixed methods, Operative technique, Robot, Surgeon decision-making},
abstract = {Introduction
Although the utilization of robotic technique for abdominal hernia repair has increased rapidly, there is no consensus as to when it should be applied for optimal outcomes. High variability exists within surgeon practices regarding how they use this technology, and the factors that drive robotic utilization remain largely unknown. This study aims to explore the motivating factors associated with surgeons’ decisions to utilize a robotic approach for abdominal hernia repair.
Methods
An exploratory mixed-methods approach was utilized. Surgeons who performed abdominal hernia repairs were interviewed to identify impactful themes motivating surgical approach. This informed a retrospective analysis of ventral hernia repairs performed in 2020 within the Michigan Surgical Quality Collaborative. Surgeon robotic utilization rates were calculated. Among selective robotic users, multivariable regression evaluated the patient and hernia factors associated with robotic utilization.
Conclusions
Major drivers of robotic technique for hernia repair were found to be perceived benefits and availability, rather than patient or hernia characteristics. These data will contribute to an understanding of surgeon decision-making and help develop improvements to patient care.}
}
%ALIP202223
@article{ALIP2022,
title = {Future Platforms of Robotic Surgery},
journal = {Urologic Clinics of North America},
volume = {49},
number = {1},
pages = {23-38},
year = {2022},
note = {Minimally Invasive Urology: Past, Present, and Future},
issn = {0094-0143},
doi = {https://doi.org/10.1016/j.ucl.2021.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0094014321018723},
author = {Sylvia L. Alip and Jinu Kim and Koon Ho Rha and Woong Kyu Han},
keywords = {Robotic urology, Robotic surgery, Telemanipulator systems, Robot-assisted laparoscopy, Single-incision surgery, Robotic endoscopy}
}

%jcm11112957
@Article{Fleming2022,
AUTHOR = {Fleming, Christina A. and Fullard, Anna and Croghan, Stefanie and Pellino, Gianluca and Pata, Francesco},
TITLE = {Robotic Abdominal Surgery and COVID-19: A Systematic Review of Published Literature and Peer-Reviewed Guidelines during the SARS-CoV-2 Pandemic},
JOURNAL = {Journal of Clinical Medicine},
VOLUME = {11},
YEAR = {2022},
NUMBER = {11},
ARTICLE-NUMBER = {2957},
URL = {https://www.mdpi.com/2077-0383/11/11/2957},
PubMedID = {35683346},
ISSN = {2077-0383},
ABSTRACT = {Background: Significant concern emerged at the beginning of the SARS-CoV-2 pandemic regarding the safety and practicality of robotic-assisted surgery (RAS). We aimed to review reported surgical practice and peer-reviewed published review recommendations and guidelines relating to RAS during the pandemic. Methods: A systematic review was performed in keeping with PRISMA guidelines. This study was registered on Open Science Framework. Databases were searched using the following search terms: &lsquo;robotic surgery&rsquo;, &lsquo;robotics&rsquo;, &lsquo;COVID-19&rsquo;, and &lsquo;SARS-CoV-2&rsquo;. Firstly, articles describing any outcome from or reference to robotic surgery during the COVID-19/SARS-CoV-2 pandemic were considered for inclusion. Guidelines or review articles that outlined recommendations were included if published in a peer-reviewed journal and incorporating direct reference to RAS practice during the pandemic. The ROBINS-I (Risk of Bias in Non-Randomised Studies of Intervention) tool was used to assess the quality of surgical practice articles and guidelines and recommendation publications were assessed using the AGREE-II reporting tool. Publication trends, median time from submission to acceptance were reported along with clinical outcomes and practice recommendations. Results: Twenty-nine articles were included: 15 reporting RAS practice and 14 comprising peer-reviewed guidelines or review recommendations related to RAS during the pandemic, with multiple specialities (i.e., urology, colorectal, digestive surgery, and general minimally invasive surgery) covered. Included articles were published April 2020&mdash;December 2021, and the median interval from first submission to acceptance was 92 days. All surgical practice studies scored &lsquo;low&rsquo; or &lsquo;moderate&rsquo; risk of bias on the ROBINS-I assessment. All guidelines and recommendations scored &lsquo;moderately well&rsquo; on the AGREE-II assessment; however, all underperformed in the domain of public and patient involvement. Overall, there were no increases in perioperative complication rates or mortalities in patients who underwent RAS compared to that expected in non-COVID practice. RAS was deemed safe, with recommendations for mitigation of risk of viral transmission. Conclusions: Continuation of RAS was feasible and safe during the SARS-CoV-2 pandemic where resources permitted. Post-pandemic reflections upon published robotic data and publication patterns allows us to better prepare for future events and to enhance urgent guideline design processes.},
DOI = {10.3390/jcm11112957}
}

@Article{Rusch2022,
author={Rusch, Ren{\'e}
and Hoffmann, Grischa
and Rusch, Melanie
and Cremer, Jochen
and Berndt, Rouven},
title={Robotic-assisted abdominal aortic surgery: evidence and techniques},
journal={Journal of Robotic Surgery},
year={2022},
month={Mar},
day={04},
abstract={In various disciplines, robotic-assisted surgery is a well-proven routine procedure, but have never been established in vascular surgery so far. This review summarizes the results to date of robotic-assisted abdominal aortic surgery (RAAS) in the treatment of aorto-iliac occlusive disease (AIOD) and abdominal aortic aneurysm (AAA).Web-based literature search of robotic-assisted surgical procedures on the abdominal aorta and iliac arteries between 1990 and 2020 including the Cochrane Library, OVID Medline, Embase, and PubMed medical databases.All studies conducting Robotic-assisted surgery were included in the quantitative analysis regarding operative and cross-clamping times, conversion rates, mortality and morbidity within the first 30 days, and in-hospital stay. Case reports and case studies (<{\thinspace}5 patients) were not included. Twenty-four studies were deemed thematically eligible for inclusion; after exclusion of duplicate publications, nine met the inclusion criteria for further analysis. A total of 850 patients who had either abdominal aortic aneurysm or aorto-iliac occlusive disease underwent RAAS. One study of abdominal aortic aneurysm, three of aorto-iliac occlusive disease, and five studies of both disease entities were analyzed quantitatively. For AAA, conversion rates ranged from 13.1 to 20{\%} and perioperative mortality ranged from 0 to 1.6{\%} with in-hospital stay of 7 days. For aorto-iliac occlusive disease, conversion rates ranged from 0 to 20{\%}, and perioperative mortality ranged from 0 to 3.6{\%} with in-hospital stay of 5--8 days. RAAS has been shown to be technically feasible with acceptable short-term outcomes and questionable benefits in terms of in-hospital stay and complication rates. RAAS is currently considered only an outsider procedure. Randomized-controlled trials are indispensable for regular use in vascular surgery as well as a clear approval situation for the vascular sector.},
issn={1863-2491},
doi={10.1007/s11701-022-01390-0},
url={https://doi.org/10.1007/s11701-022-01390-0}
}

% DIRKS2022106902
@article{DIRKS2022,
title = {Computer-aided detection and segmentation of malignant melanoma lesions on whole-body 18F-FDG PET/CT using an interpretable deep learning approach},
journal = {Computer Methods and Programs in Biomedicine},
volume = {221},
pages = {106902},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.106902},
url = {https://www.sciencedirect.com/science/article/pii/S016926072200284X},
author = {Ine Dirks and Marleen Keyaerts and Bart Neyns and Jef Vandemeulebroucke},
keywords = {Whole-body, F-FDG, PET/CT, Segmentation, Detection},
abstract = {Background and objective: In oncology, 18-fluorodeoxyglucose (18F-FDG) positron emission tomography (PET) / computed tomography (CT) is widely used to identify and analyse metabolically-active tumours. The combination of the high sensitivity and specificity from 18F-FDG PET and the high resolution from CT makes accurate assessment of disease status and treatment response possible. Since cancer is a systemic disease, whole-body imaging is of high interest. Moreover, whole-body metabolic tumour burden is emerging as a promising new biomarker predicting outcome for innovative immunotherapy in different tumour types. However, this comes with certain challenges such as the large amount of data for manual reading, different appearance of lesions across the body and cumbersome reporting, hampering its use in clinical routine. Automation of the reading can facilitate the process, maximise the information retrieved from the images and support clinicians in making treatment decisions. Methods: This work proposes a fully automated system for lesion detection and segmentation on whole-body 18F-FDG PET/CT. The novelty of the method stems from the fact that the same two-step approach used when manually reading the images was adopted, consisting of an intensity-based thresholding on PET followed by a classification that specifies which regions represent normal physiological uptake and which are malignant tissue. The dataset contained 69 patients treated for malignant melanoma. Baseline and follow-up scans together offered 267 images for training and testing. Results: On an unseen dataset of 53 PET/CT images, a median F1-score of 0.7500 was achieved with, on average, 1.566 false positive lesions per scan. Metabolically-active tumours were segmented with a median dice score of 0.8493 and absolute volume difference of 0.2986 ml. Conclusions: The proposed fully automated method for the segmentation and detection of metabolically-active lesions on whole-body 18F-FDG PET/CT achieved competitive results. Moreover, it was compared to a direct segmentation approach which it outperformed for all metrics.}
}

%MORI2022142
@article{Martina2022,
title = {Atlas-based lung segmentation combined with automatic densitometry characterization in COVID-19 patients: Training, validation and first application in a longitudinal study},
journal = {Physica Medica},
volume = {100},
pages = {142-152},
year = {2022},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2022.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S1120179722020117},
author = {Martina Mori and Lisa Alborghetti and Diego Palumbo and Sara Broggi and Davide Raspanti and Patrizia {Rovere Querini} and Antonella {Del Vecchio} and Francesco {De Cobelli} and Claudio Fiorino},
keywords = {Automatic segmentation Atlas-based, Quantitative imaging computed tomography, Lung segmentation, Covid-19},
abstract = {Purpose
To develop and validate an automated segmentation tool for COVID-19 lung CTs. To combine it with densitometry information in identifying Aerated, Intermediate and Consolidated Volumes in admission (CT1) and follow up CT (CT3).
Materials and Methods
An Atlas was trained on manually segmented CT1 of 250 patients and validated on 10 CT1 of the training group, 10 new CT1 and 10 CT3, by comparing DICE index between automatic (AUTO), automatic-corrected (AUTOMAN) and manual (MAN) contours. A previously developed automatic method was applied on HU lung density histograms to quantify Aerated, Intermediate and Consolidated Volumes. Volumes of subregions in validation CT1 and CT3 were quantified for each method.
Conclusions
An Atlas for automatic segmentation of lungs in COVID-19 patients was developed and validated. Combined with a previously developed method for lung densitometry characterization, it provides a fast, operator-independent way to extract relevant quantitative parameters with minimal manual intervention.}
}

%STROSS2020298
@article{William2020,
title = {Atlas based segmentation in prone breast cancer radiation therapy},
journal = {Medical Dosimetry},
volume = {45},
number = {3},
pages = {298-301},
year = {2020},
issn = {0958-3947},
doi = {https://doi.org/10.1016/j.meddos.2020.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0958394720300261},
author = {William C. Stross and Steven M. Herchko and Laura A. Vallow},
keywords = {Atlas based segmentation, Breast cancer, Radiation},
}

%https://doi.org/10.48550/arxiv.2106.09662
@misc{Samei2021,
  doi = {10.48550/ARXIV.2106.09662},
  
  url = {https://arxiv.org/abs/2106.09662},
  
  author = {Samei, Golnoosh and Karimi, Davood and Kesch, Claudia and Salcudean, Septimiu},
  
  keywords = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Automatic Segmentation of the Prostate on 3D Trans-rectal Ultrasound Images using Statistical Shape Models and Convolutional Neural Networks},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

%HEIMANN2009543
@article{Tobias2009,
title = {Statistical shape models for 3D medical image segmentation: A review},
journal = {Medical Image Analysis},
volume = {13},
number = {4},
pages = {543-563},
year = {2009},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2009.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1361841509000425},
author = {Tobias Heimann and Hans-Peter Meinzer},
keywords = {Statistical shape model, Deformable surface, Active Shape model, Active Appearance model},
abstract = {Statistical shape models (SSMs) have by now been firmly established as a robust tool for segmentation of medical images. While 2D models have been in use since the early 1990s, wide-spread utilization of three-dimensional models appeared only in recent years, primarily made possible by breakthroughs in automatic detection of shape correspondences. In this article, we review the techniques required to create and employ these 3D SSMs. While we concentrate on landmark-based shape representations and thoroughly examine the most popular variants of Active Shape and Active Appearance models, we also describe several alternative approaches to statistical shape modeling. Structured into the topics of shape representation, model construction, shape correspondence, local appearance models and search algorithms, we present an overview of the current state of the art in the field. We conclude with a survey of applications in the medical field and a discussion of future developments.}
}

%LUO2022102642
@article{Xiangde2022,
title = {WORD: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image},
journal = {Medical Image Analysis},
volume = {82},
pages = {102642},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102642},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522002705},
author = {Xiangde Luo and Wenjun Liao and Jianghong Xiao and Jieneng Chen and Tao Song and Xiaofan Zhang and Kang Li and Dimitris N. Metaxas and Guotai Wang and Shaoting Zhang},
keywords = {Abdominal organ segmentation, Dataset, Benchmark, Clinical applicable study},
abstract = {Whole abdominal organ segmentation is important in diagnosing abdomen lesions, radiotherapy, and follow-up. However, oncologists’ delineating all abdominal organs from 3D volumes is time-consuming and very expensive. Deep learning-based medical image segmentation has shown the potential to reduce manual delineation efforts, but it still requires a large-scale fine annotated dataset for training, and there is a lack of large-scale datasets covering the whole abdomen region with accurate and detailed annotations for the whole abdominal organ segmentation. In this work, we establish a new large-scale Whole abdominal ORgan Dataset (WORD) for algorithm research and clinical application development. This dataset contains 150 abdominal CT volumes (30495 slices). Each volume has 16 organs with fine pixel-level annotations and scribble-based sparse annotations, which may be the largest dataset with whole abdominal organ annotation. Several state-of-the-art segmentation methods are evaluated on this dataset. And we also invited three experienced oncologists to revise the model predictions to measure the gap between the deep learning method and oncologists. Afterwards, we investigate the inference-efficient learning on the WORD, as the high-resolution image requires large GPU memory and a long inference time in the test stage. We further evaluate the scribble-based annotation-efficient learning on this dataset, as the pixel-wise manual annotation is time-consuming and expensive. The work provided a new benchmark for the abdominal multi-organ segmentation task, and these experiments can serve as the baseline for future research and clinical application development.}
}

@Article{Ilesanmi2022,
author={Ilesanmi, Ademola E.
and Ilesanmi, Taiwo
and Idowu, Oluwagbenga P.
and Torigian, Drew A.
and Udupa, Jayaram K.},
title={Organ segmentation from computed tomography images using the 3D convolutional neural network: a systematic review},
journal={International Journal of Multimedia Information Retrieval},
year={2022},
month={Sep},
day={01},
volume={11},
number={3},
pages={315-331},
abstract={Computed tomography images are scans that combine a series of X-rays with computer processing techniques to display organs in the body. Recently, 3D CNN models have become effective in tasks relating to recognition, delineation, and classification. Therefore we propose a review to summarize different 3D CNN algorithms for segmenting organs in computed tomography images. This work systematically applies a two-stage procedure for review. A thorough screening of abstracts and titles to ascertain their relevance was done. Research papers published in the academic repositories were selected, analyzed, and reviewed. Insight relating to 3D organ segmentation is provided, with content such as database usage, disadvantages, and advantages. A comparison of two accuracies was carried out with a graph depicting database categories. Important insights, limitations, observations, and future directions were elucidated. After careful investigation, we observe that the encoder-decoder network is predominant for segmentation. The encoder-decoder framework provides a seamless procedure to segment CT images. A prediction of future trends with insightful recommendations for researchers is proposed. Finally, findings suggest that CNN algorithms produce good accuracies despite their limitations.},
issn={2192-662X},
doi={10.1007/s13735-022-00242-9},
url={https://doi.org/10.1007/s13735-022-00242-9}
}

@article{KAUR2022,
title = {Evolution of multiorgan segmentation techniques from traditional to deep learning in abdominal CT images – A systematic review},
journal = {Displays},
volume = {73},
pages = {102223},
year = {2022},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2022.102223},
url = {https://www.sciencedirect.com/science/article/pii/S0141938222000567},
author = {Harinder Kaur and Navjot Kaur and Nirvair Neeru},
keywords = {Multiorgan Segmentation (MOS), Atlas Based Segmentation (ABS), Statistical Shape Model (SSM), Deep Learning (DL)},
abstract = {Abdominal organ segmentation is the crucial research direction in computer assisted diagnostic systems. Segmentation of multiple organs in medical images is known as Multiorgan segmentation. It is a widespread subject of research in the realm of medical image analysis. The purpose of this study is to provide the comprehensive systematic literature review on segmentation of multiple organs in abdomen CT scans. This paper focuses on the progression of state-of-art methods from traditional techniques to deep learning models. Firstly, the methods are classified into three categories: atlas based, statistical shape models and deep learning models. Secondly, research is carried out to determine which organs require more attention. The liver, kidney, and spleen are the most often selected organs, whereas the esophagus, duodenum, and portal vein are rarely picked. When medical images are taken into account for research, datasets play a vital role. This paper sheds light on publicly available datasets along with their size, no of organ classes and, related challenges which make the current study more effective and useful for the researchers in the same field. Further, evaluation metrics along with their scope and characteristics are presented. We conclude with a discussion of challenges and future directions which will open pathways for researchers. Based on the surveyed research papers, Dense-Net came out as an optimal choice. Recently, the standard practice in multi organ segmentation is two step deep learning models in sequential manner, which can take leverage of two models.}
}

% 10.1007/978-3-030-01449-0_16
@InProceedings{Javaid2018,
author="Javaid, Umair
and Dasnoy, Damien
and Lee, John A.",
editor="Blanc-Talon, Jacques
and Helbert, David
and Philips, Wilfried
and Popescu, Dan
and Scheunders, Paul",
title="Multi-organ Segmentation of Chest CT Images in Radiation Oncology: Comparison of Standard and Dilated UNet",
booktitle="Advanced Concepts for Intelligent Vision Systems",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="188--199",
abstract="Automatic delineation of organs at risk (OAR) in computed tomography (CT) images is a crucial step for treatment planning in radiation oncology. However, manual delineation of organs is a challenging and time-consuming task subject to inter-observer variabilities. Automatic organ delineation has been relying on non-rigid registrations and atlases. However, lately deep learning appears as a strong competitor with specific architectures dedicated to image segmentation like UNet. In this paper, we first assessed the standard UNet to delineate multiple organs in CT images. Second, we observed the effect of dilated convolutional layers in UNet to better capture the global context from the CT images and effectively learn the anatomy, which results in increased localization of organ delineation. We evaluated the performance of a standard UNet and a dilated UNet (with dilated convolutional layers) on four chest organs (esophagus, left lung, right lung, and spinal cord) from 29 lung image acquisitions and observed that dilated UNet delineates the soft tissues notably esophagus and spinal cord with higher accuracy than the standard UNet. We quantified the segmentation accuracy of both models by computing spatial overlap measures like Dice similarity coefficient, recall {\&} precision, and Hausdorff distance. Compared to the standard UNet, dilated UNet yields the best Dice scores for soft organs whereas for lungs, no significant difference in the Dice score was observed: {\$}{\$}0.84{\backslash}pm 0.07{\$}{\$}0.84{\textpm}0.07vs {\$}{\$}0.71{\backslash}pm 0.10{\$}{\$}0.71{\textpm}0.10for esophagus, {\$}{\$}0.99{\backslash}pm {\{}0.01{\}}{\$}{\$}0.99{\textpm}0.01vs {\$}{\$}0.99{\backslash}pm {\{}0.01{\}}{\$}{\$}0.99{\textpm}0.01for left lung, {\$}{\$}0.99{\backslash}pm {\{}0.01{\}}{\$}{\$}0.99{\textpm}0.01vs {\$}{\$}0.99{\backslash}pm {\{}0.01{\}}{\$}{\$}0.99{\textpm}0.01for right lung and {\$}{\$}0.91{\backslash}pm {\{}0.05{\}}{\$}{\$}0.91{\textpm}0.05vs {\$}{\$}0.88{\backslash}pm {\{}0.04{\}}{\$}{\$}0.88{\textpm}0.04for spinal cord.",
isbn="978-3-030-01449-0"
}

%10.1007/978-3-642-33418-4_52
@InProceedings{Noble2012,
author="Noble, Jack H.
and Gifford, Ren{\'e} H.
and Labadie, Robert F.
and Dawant, Beno{\^i}t M.",
editor="Ayache, Nicholas
and Delingette, Herv{\'e}
and Golland, Polina
and Mori, Kensaku",
title="Statistical Shape Model Segmentation and Frequency Mapping of Cochlear Implant Stimulation Targets in CT",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2012",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="421--428",
abstract="Cochlear implant (CI) surgery is considered standard of care treatment for severe hearing loss. CIs are currently programmed using a one-size-fits-all type approach. Individualized, position-based CI programming schemes have the potential to significantly improve hearing outcomes. This has not been possible because the position of stimulation targets is unknown due to their small size and lack of contrast in CT. In this work, we present a segmentation approach that relies on a weighted active shape model created using microCT scans of the cochlea acquired ex-vivo in which stimulation targets are visible. The model is fitted to the partial information available in the conventional CTs and used to estimate the position of structures not visible in these images. Quantitative evaluation of our method results in Dice scores averaging 0.77 and average surface errors of 0.15 mm. These results suggest that our approach can be used for position-dependent image-guided CI programming methods.",
isbn="978-3-642-33418-4"
}

% 10.1007/978-3-031-16443-9_5
@InProceedings{Yang2022,
author="Yang, Han
and Shen, Lu
and Zhang, Mengke
and Wang, Qiuli",
editor="Wang, Linwei
and Dou, Qi
and Fletcher, P. Thomas
and Speidel, Stefanie
and Li, Shuo",
title="Uncertainty-Guided Lung Nodule Segmentation with Feature-Aware Attention",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="44--54",
abstract="Since radiologists have different training and clinical experiences, they may provide various segmentation annotations for a lung nodule. Conventional studies choose a single annotation as the learning target by default, but they waste valuable information of consensus or disagreements ingrained in the multiple annotations. This paper proposes an Uncertainty-Guided Segmentation Network (UGS-Net), which learns the rich visual features from the regions that may cause segmentation uncertainty and contributes to a better segmentation result. With an Uncertainty-Aware Module, this network can provide a Multi-Confidence Mask (MCM), pointing out regions with different segmentation uncertainty levels. Moreover, this paper introduces a Feature-Aware Attention Module to enhance the learning of the nodule boundary and density differences. Experimental results show that our method can predict the nodule regions with different uncertainty levels and achieve superior performance in the LIDC-IDRI dataset.",
isbn="978-3-031-16443-9"
}