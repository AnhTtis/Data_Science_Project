% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage{comment}
\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage{subcaption}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{adjustbox}
\usepackage{graphicx}


% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\begin{document}
%
\title{Uncertainty Driven Bottleneck Attention U-net for OAR Segmentation}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\authorrunning{Nazib et al.}
\author{Abdullah Nazib\inst{3},
Riad Hassan\inst{1},
Nosin Ibn Mahbub\inst{2},
Zahidul Islam\inst{2},
Clinton Fookes\inst{3}}
%
\authorrunning{A.Nazib et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{
Green University of Bangladesh\\
\email{riad@cse.green.edu.bd}\\
\and Islamic University, Kushtia, Bangladesh\\
\email{zahidimage@gmail.com}\\
\and
Queensland University of Technology,
Brisbane,Australia.\\
\email{nazib@qut.edu.au,c.fookes@qut.edu.au}
}

%
\maketitle     
% typeset the header of the contribution

\begin{abstract}
Organ at risk (OAR) segmentation in computed tomography (CT) imagery is a difficult task for automated segmentation methods and can be crucial for downstream radiation treatment planning. U-net has become a de-facto standard for medical image segmentation and is frequently used as a common baseline in medical image segmentation tasks. In this paper, we develop a multiple decoder U-net architecture where a noisy auxiliary decoder is used
to generate noisy segmentation. The segmentation from the main branch and the noisy segmentation from the auxiliary branch are used together to estimate the attention. Our contribution is the development of a new attention module which derives the attention from the softmax probabilities of two decoder branches. The union and intersection of two segmentation masks from two branches carry the information where both decoders agree and disagree. The softmax probabilities from regions of agreement and disagreement are the indicators of low and high uncertainty. Thus, the probabilities of those selected regions are used as attention in the bottleneck layer of the encoder and passes only through the main decoder for segmentation. For accurate contour segmentation, we also developed a CT intensity integrated regularization loss. We tested our model on two publicly available OAR challenge datasets, Segthor and LCTSC respectively. We trained 12 models on each dataset with and without the proposed attention model and regularization loss to check the effectiveness of the attention module and the regularization loss. The experiments demonstrate a clear accuracy improvement (2\% to 5\% Dice) on both datasets. Code for the experiments will be made available upon the acceptance for publication.  
 
\begin{keywords}
Thoracic CT, Organs-at-Risk, Uncertainty, U-net.
\end{keywords}
\end{abstract}
%
\section{Introduction}
Radiation treatment planing requires meticulous delineation of organs that are adjacent to affected tissues. Delineating organs at risk (OAR) is tedious, time consuming and prone to inter-rater variability leading to unnecessary and dangerous radiation to healthy tissues. Recently, deep learning based methods have achieved remarkable success in image segmentation for both natural and medical images.
Many deep learning-based methods have been developed to simultaneously segment multiple organs. Segmenting multiple organs is a challenging task due to the variation of the organ shape, the low soft tissue contrast in CT and label imbalance. Deep learning models are particularly good at segmenting when organs are relatively large, consistent in shape and their boundary tissues have a high contrast compared to background organs/structures. In the case of smaller organs, models struggle to segment them effectively and sometimes, segmentation completely fails to detect the organ. In chest CT scans, the esophagus is the most challenging organ \cite{Fechter2017} to segment due to its high shape variation and low soft tissue contrast.  

U-net become a standard model for medical image segmentation\cite{Ronneberger2015}. Many different variants of the U-net model have been developed in recent years \cite{Yagi2019,Kakeya2018,HEINRICH20191,Milletari2016,Wang2019,Wang2019b}. Most of these works have concentrated their effort in adding more layers or aggregating robust context information while processing the image in the layers of the U-net. In \cite{Ozan2018}, an attention gate is proposed for U-net, where the attention distribution is generated by processing the features from the deeper layer and shallower layers. Sinha et.al \cite{Sinha2019MultiScaleSA} proposed guided multi resolution attention where attention is generated by aggregating multi-resolution features and is guided by a deep supervision loss. Similar deep feature based works also include \cite{Gu2021},\cite{YangLi2021} and \cite{Zhang2019} where authors proposed different attention mechanisms. 
Despite their high accuracy, adding attention module/gates adds extra layers of complexity. Since neural networks learn to extract optimal features during training, adding attention gates places additional burden on the network to find optimal attention parameters in an unexplainable way. As such, the parameter search space increases exponentially with the added attention type (spatial,channel,recurrent etc). 

To address these issues, this paper proposes two contributions. First, the use of an auxiliary decoder in the U-net architecture allows for the extraction of uncertainties between the main and auxiliary segmentation, which can be used as attention. This approach generates attention that is more explainable and easier to manipulate. Second, to address the inherent complexity of segmenting very close organs, a CT intensity integrated regularization method is proposed, which is tested on two OAR segmentation datasets.

%% Main Figure
\begin{figure}[!htb]
\includegraphics[width=\linewidth,trim={0 20mm 0 20mm},clip]{Main.pdf}
\caption{Overview of the proposed approach.A) The U-net with auxiliary decoder. B) The Uncertainty module, C) Attention module, D)CT intensity integrated regularization.} \label{fig:Main}
\end{figure}

\section{Method}
The proposed method is depicted in Figure \ref{fig:Main}. The approach utilizes a U-net architecture with two decoders. The main decoder sequentially processes bottleneck features, along with skip connections, to produce the segmentation output. Instead of directly using the bottleneck features from the encoder, in auxiliary decoder, a perturbation layer is introduced, similar to the approach presented in \cite{ouali2020}. The perturbation layer employs random noise with a uniform distribution of $U(-0.3, 0.3)$ to modify the auxiliary decoder's output. The segmentation outputs from both decoders are then used in the uncertainty module, which applies union and intersection operations to generate the corresponding masks. This module's concept is derived from \cite{chen2021}, but instead of using ground-truth segmentation from multiple radiologists, the uncertainty is estimated from the agreement and disagreement of the two decoders. The union mask takes all regions where both decoders agree and disagree, while the intersection mask takes only the regions where they agree. Similar to \cite{chen2021}, combining the intersection and union masks provides the multi-confidence mask (MCM) (see Figure \ref{fig:Main}(B)). To extract the probabilities from the regions in the MCM, the maximum of the softmax function is applied to the output of each decoder, and the resulting values are multiplied with the multi-confidence mask. The equations for this procedure are as follows:
\begin{equation}
p_{main} = max(softmax(D_{main},dim=1))
\end{equation}
\begin{equation}
p_{aux} = max(softmax(D_{aux},dim=1))
\end{equation}
\begin{equation}
Attention = max(p_{main},p_{aux}) * MCM
\end{equation}
Here, $D_{main}$ and $D_{aux}$ are the $1\times1$ regression layer output from main and auxiliary decoders. 
The calculated attention is a single channel spatial attention where the probabilities of each pixel is obtained from the region of interest. To apply this attention to the bottleneck features, the spatial dimensions are downsampled and replicated along the channel dimension to match with dimension of the bottleneck. Finally, the attention is multiplied with the bottleneck features to select features that are required for the segmentation (see Figure\ref{fig:Main}(C)).

Dice and cross entropy losses are commonly employed in medical image segmentation. The dice loss evaluates the agreement between the predicted segmentation and the actual segmentation. Thus, it primarily takes into account the shape of the object of interest. In contrast, the cross-entropy loss relies heavily on the distribution of the classes in the dataset. If the classes are evenly distributed, the classification results will be better. Since OAR segmentation requires precise boundary segmentation, the use of mere shape or class distribution sometime classify surrounding the boundary pixels as false positive. 
Based on this observation, we introduce ct-intensity integrated regularization (CTR).In CTR (shown in Figure\ref{fig:Main}(D)), the CT intensities from the ground-truth and predicted regions are extracted using corresponding masks and absolute differences are measured. Thus, the calculated CTR helps the network learn CT intensity distributions more accurately. We also proposed a matrix version of CTR which compares both the intra-class and inter-class CT intensities. \\
Let consider, $Y$ and $Y_{p}$ are the CT-intensities extracted from the the predicted and ground-truth regions and given by: 
\begin{equation}
    Y = \frac{1}{WH} \sum_{i=1}^{W}\sum_{j=1}^{H} CT*GT_{mask}
\end{equation}
\begin{equation}
    Y_p = \frac{1}{WH} \sum_{i=1}^{W}\sum_{j=1}^{H} CT*P_{mask}
\end{equation}
Therefore, the CT-intensity-integrated regularizer is
\begin{equation}
    \mathcal{L_{CTR}} = ||Y - Y_{p}||
        \label{Eq:CTR}
\end{equation}
Let the number of classes are $\mathcal{N}$, the matrix version of CTR is a $\mathcal{N}\times \mathcal{N}$ matrix which is given by:
\begin{equation}
    \mathcal{M_{CTR}} = [\mathcal{CTR}_{i,j}]
    \label{Eq:CTRM}
\end{equation}
Each entry of the matrix is the CTR value between i-th class ground-truth and j-th class prediction, therefore inter-class CTR. When $i=j$ its the intra-class CTR. For simplicity we denote CTR matrix as CTRM. To optimize the network, the mean of the CTRM is taken and the resulting     
regularizer is given by
\begin{equation}
    \mathcal{L_{CTRM}} = \frac{1}{\mathcal{N}\mathcal{N}}\sum_{i=1}^{\mathcal{N}}\sum_{j=1}^{\mathcal{N}}[\mathcal{CTR}_{i,j}]
\end{equation}
The proposed network has two segmentation decoders with slightly different segmentation outputs. Driving both decoders simultaneously require strong gradient computation and also need to maintain consistency between the segmentation outputs mentioned in \cite{ouali2020}. We train the network with
four losses, two for segmentation, one for cross-consistency between the decoders and the last one is the either of the regularizer mentioned in Eq.\ref{Eq:CTR} and \ref{Eq:CTRM}. Therefore, the training loss is
\begin{equation}
    \mathcal{L} = \mathcal{L}_{main} + \mathcal{L}_{aux} + \mathcal{L}_{CCT} + \mathcal{L_{CTR}} or \mathcal{L_{CTRM}}
\end{equation}

\section{Experiments and Results}
\subsection{Dataset and Implementation}
The proposed regularization losses and attention module are evaluated on two datasets, namely SegThor \cite{Lambert2019} and LCTSC \cite{LCTSC}. The SegThor challenge dataset consists of 40 CT images for training and 20 for testing. Unfortunately, we were only able to collect the training dataset, which includes 40 CT images along with corresponding manual annotations of the Esophagus, Heart, Trachea, and Aorta. Since only training set is available, we split the dataset with 35 for training and 5 for testing.
On the other hand, the LCTSC dataset \cite{LCTSC} comprises 36 training and 24 testing CT images with corresponding manual annotations of organs such as the Esophagus, Spinal Cord, Heart, Left and Right Lung.

The model is implemented using PyTorch version 1.12.1 as a 2D model, considering only the axial view of the 3D CT image during training. All the models are trained and tested in a high-performance computing environment with a 64GB RAM, A100 GPU, and a single-core 2.66GHz 64-bit Intel Xeon processor. The network is trained using the ADAM optimizer with a learning rate of 0.01 and batch size of 1 for 200 training epochs.

\subsection{Data Preprocessing}
The same data pre-processing pipeline is applied to both datasets. Specifically, 2D axial slices are extracted from each 3D volume by selecting only the body region. To enhance contrast, a level of 30 and window of 400 of the CT intensities are applied. To facilitate the evaluation of models and fit the data into the GPU memory, the image slices of size $512\times512$ are resized to $256\times256$. The evaluation of the proposed model was primarily based on the Dice coefficient and its standard deviation. Additionally, average surface distance (ASD) and intersection over union (IoU) are also employed for performance comparison between the proposed model and a baseline model. 
\subsection{Experiment Design}
Twelve different experiments are conducted on each dataset. Each loss/loss-combination
is tested with and without attention. In Table-\ref{Tab:segthor} and Table-\ref{Tab:LCTSC}
the first column indicates the loss functions, rest columns are the mean dice $\pm$ standard deviation for each organ. The best performance for each organ is highlighted in bold. The UDBA suffix indicates the use of proposed attention module. 
\subsection{Results on SegThor Dataset}
Table \ref{Tab:segthor} reports the dice accuracy of U-nets on the SegThor dataset for four organs - Esophagus, Heart, Trachea, and Aorta.
It is clear from the Table-\ref{Tab:segthor} data, that the different loss combination performs differently on four different organs. For esophagus, the model with dice and UDBA achieves the top score with 0.81$\pm$0.045. For heart, the cross-entropy and its combinations are in leading position compared to the dice losses. The CE with attention and CE+CTRM with attention achieves the 0.95$\pm$0.008 and 0.95$\pm$0.007 for the heart. Same pattern goes for the trachea, the CE and its combinations are achieving higher dice scores compared to the dice variants. Again, in the trachea, the CE+CTRM with attention achieves the top dice score with 0.91$\pm$0.022. In aorta, the dice scores of dice loss variants fluctuates between 0.90 to 0.92 while the cross-entropy variants are fluctuating from 0.89 to 0.93. Again, in aorta the CE+CTRM(UDBA) scores the highest dice score with 0.93$\pm0.013$.
Additionally, the most significant pattern we find from the Table-\ref{Tab:segthor} is the influence of UDBA module on the performance scores. In most of the cases the use of UDBA module systematically improves the accuracy regardless of loss combinations.

The performance of the proposed model in comparison to another attention based model is reported in Table-\ref{Tab:SOTA_Segthor}. In addition to dice, average surface distance (ASD) and intersection over union (IoU) metrics are also added. To compare, we considered our Dice(UDBA) and CE+CTRM(UDBA) methods since both achieve high scores in different organs. The Dice(UDBA) method achieves the highest Dice scores for the esophagus, as well as the lowest ASD score for the trachea structure. In terms of IoU, both CE+CTRM(UDBA) and AttUnet scores higher than the Dice(UDBA). For heart, CE+CTRM(UDBA) obtains top scores in all three metrics. In trachea and aorta, this method achieves best accuracy in terms of dice and IoU metrics.    
The AttUnet\cite{Ozan2018} achieves lower scores in general than ours with the exception of the ASD, IoU in esophagus, and ASD in Aorta. Since we do not have SegThor test dataset, we are unable to compare side-by-side with methods presented in the challenge ledger board.

\begin{table}[!htb]
    \caption{\small Dice Accuracy of Unets with and without attention module on Segthor Dataset. Combination of different losses and proposed regularizations are also presented.}
    \label{Tab:segthor}
	\centering
    \begin{adjustbox}{width=\textwidth}
            \begin{tabular}{l |{c}| {c}| {c}| {c}}
			\hline
			\textbf{Losses (Unet)} &\textbf{ Esophagus } &\textbf{ Heart } &\textbf{ Trachea } &\textbf{ Aorta}\\
		    \hline
                \hline
			     Dice           &$0.74\pm0.060$            &$0.93\pm0.009$             &$0.85\pm0.025$            &$0.91\pm0.015$ \\
                Dice(UDBA)       &$\textbf{0.81}\pm0.045$   &$0.93\pm0.021$    &$0.84\pm0.033$   &$0.92\pm0.012$ \\
                Dice+CTR         &$0.72\pm0.067$   &$0.92\pm0.010$    &$0.84\pm0.023$    &$0.90\pm0.028$ \\
                Dice+CTR(UDBA)   &$0.76\pm0.082$   &$0.93\pm0.019$   &$0.87\pm0.035$    &$0.92\pm0.013$ \\
                Dice+CTRM        &$0.75\pm0.079$   &$0.93\pm0.017$   &$0.84\pm0.038$    &$0.90\pm0.027$ \\
                Dice+CTRM(UDBA)  &$0.74\pm0.058$   &$0.93\pm0.016$   &$0.87\pm0.021$    &$0.92\pm0.013$ \\
                CE               &$0.71\pm0.068$   &$0.94\pm0.011$   &$0.88\pm0.035$    &$0.89\pm0.028$ \\
                CE(UDBA)         &$0.78\pm0.052$   &$\textbf{0.95}\pm0.008$    &$0.90\pm0.032$  &$0.92\pm0.018$ \\
                CE+CTR		     &$0.70\pm0.129$   &$0.93\pm0.015$    &$0.89\pm0.025$	 &$0.88\pm0.042$\\
                CE+CTR(UDBA)	 &$0.74\pm0.082$	 &$0.94\pm0.013$	  &$0.90\pm0.015$	   &$0.92\pm0.016$\\
                CE+CTRM		     &$0.69\pm0.094$	 &$0.93\pm0.014$	  &$0.88\pm0.031$	   &$0.86\pm0.027$\\
                CE+CTRM(UBDA)	 &$0.74\pm0.092$	 &$\textbf{0.95}\pm0.007$	   &$\textbf{0.91}\pm0.022$	   &$\textbf{0.93}\pm0.013$\\
			\hline
		\end{tabular}
  \end{adjustbox}
\end{table}
\begin{table}[ht]	
		\caption{\small Comparison between other baselines on SegThor Dataset}
		\label{Tab:SOTA_Segthor}
		\centering
    \begin{adjustbox}{width=\textwidth}
		\begin{tabular}{l |{c}| {c}| {c}| {c}|{c}|{c}| {c}| {c} |{c}| {c}| {c}| {c}}
			\hline
			\textbf{Method} &\multicolumn{3}{|c|}{\textbf{ Esophagus }} &\multicolumn{3}{c|}{\textbf{ Heart }} &\multicolumn{3}{c|}{\textbf{ Trachea }} & \multicolumn{3}{c}{\textbf{ Aorta}}\\
		    \cline{2-13}
             &\textbf{Dice}&\textbf{ASD}&\textbf{IoU} &\textbf{Dic}e&\textbf{ASD}&\textbf{IoU} &\textbf{Dice}&\textbf{ASD}&\textbf{IoU} &\textbf{Dice}&\textbf{ASD}&\textbf{IoU}\\
             \hline
			Dice(UDBA)         &\textbf{0.81}&0.82&0.87   &0.92&1.11&0.78    &0.85&\textbf{0.46}&0.84   &0.92&0.73&0.91 \\
            \hline
            CE+CTRM(UDBA)      &0.74&1.45&\textbf{0.89}   &\textbf{0.95}&\textbf{0.94}&\textbf{0.93}  &\textbf{0.91}&0.57&\textbf{0.95}  &\textbf{0.93}&0.66&\textbf{0.93}\\
            \hline
            AttUnet\cite{Ozan2018}     &0.74&\textbf{0.66}&\textbf{0.89} &0.80&1.57&0.61 &0.80&0.50&0.86 &0.88&\textbf{0.59}&0.88\\
			\hline
		\end{tabular}
  \end{adjustbox}
\end{table}

\subsection{Results on LCTSC Dataset}
The Table-\ref{Tab:LCTSC} presents the dice accuracy of U-nets with and without an attention module on the LCTSC dataset. The dataset has five different regions of interest, including Esophagus, Spinal Cord, Heart, left and right Lung. 
In Table-\ref{Tab:LCTSC}, Dice with CTRM and UDBA achieves the top score for esophagus and spinal cord with dice 0.71$\pm$0.063 and 0.89$\pm$0.009, respectively. For heart and both lungs, the cross-entropy with UDBA achieves the highest scores in all three regions with the scores 0.92$\pm$0.033, 0.97$\pm$0.003 and 0.97$\pm$0.003.  The combination CE+CTRM(UDBA) also achieves very similar scores to CE(UDBA) in four organs except the esophagus.\\
In this Table-\ref{Tab:LCTSC}, dice based models are consistently achieved high scores in esophagus and spinal cord compared to their cross-entropy based counterparts. For other three organs, cross-entropy based combinations are performed better than the dice based combinations. Similar to Table-\ref{Tab:segthor}, the use of UDBA module consistently improves the accuracy regardless of loss combinations.
\begin{table}[!htb]	
		\caption{\small Dice Accuracy of Unets with and without attention module on LCTSC  Dataset. Combination of different losses and proposed regularization are also presented.}
		\label{Tab:LCTSC}
		\centering
        \begin{adjustbox}{width=\textwidth}
		\begin{tabular}{l |{c}| {c}| {c}| {c}| {c}}
			\hline
			\textbf{Losses (Unet)} &\textbf{Esophagus} &\textbf{Spine} &\textbf{Heart} &\textbf{Lung(Left)} &\textbf{Lung(Right)}\\
		    \hline
                \hline
			Dice            &$0.66\pm0.019$ &$0.88\pm0.031$ &$0.71\pm0.092$ &$0.94\pm0.043$ &$0.95\pm0.042$ \\
            Dice(UDBA)      &$0.68\pm0.085$ &$0.88\pm0.026$ &$0.82\pm0.034$ &$0.96\pm0.017$ &$0.95\pm0.030$ \\
            Dice+CTR        &$0.63\pm0.097$ &$0.88\pm0.030$ &$0.81\pm0.039$ &$0.93\pm0.045$ &$0.95\pm0.038$ \\
            Dice+CTR(UDBA)  &$0.68\pm0.096$ &$0.88\pm0.027$ &$0.47\pm0.070$ &$0.93\pm0.046$ &$0.94\pm0.053$ \\
            Dice+CTRM       &$0.65\pm0.052$ &$0.88\pm0.009$ &$0.78\pm0.05$  &$0.93\pm0.013$ &$0.94\pm0.004$ \\
            Dice+CTRM(UDBA) &$\textbf{0.71}\pm0.063$ &$\textbf{0.89}\pm0.009$ &$0.88\pm0.029$ &$0.95\pm0.002$ &$0.96\pm0.002$\\
            CE              &$0.61\pm0.038$ &$0.86\pm0.011$ &$0.89\pm0.042$ &$0.96\pm0.005$ &$0.96\pm0.004$ \\
            CE(UDBA)        &$0.63\pm0.044$ &$0.88\pm0.012$ &$\textbf{0.92}\pm0.033$ &$\textbf{0.97}\pm0.003$ &$\textbf{0.97}\pm0.003$ \\
            CE+CTR          &$0.56\pm0.111$ &$0.86\pm0.030$ &$0.90\pm0.038$ &$0.96\pm0.017$ &$0.96\pm0.021$\\
            CE+CTR(UDBA)    &$0.62\pm0.100$  &$0.87\pm0.033$ &$0.91\pm0.037$ &$0.96\pm0.015$
            &$0.97\pm0.021$\\
            CE+CTRM        &$0.52\pm0.134$  &$0.86\pm0.026$ &$0.90\pm0.033$ &$0.96\pm0.018$
            &$0.96\pm0.022$\\
            CE+CTRM(UDBA)   &$0.63\pm0.082$ &$0.88\pm0.025$  &$\textbf{0.92}\pm0.024$ &$0.96\pm0.016$
            &$0.97\pm0.022$\\
			\hline
		\end{tabular}
   \end{adjustbox}
\end{table}
Similar to SegThor data, we compare the performance of two of our models with the attention U-net\cite{Ozan2018}. Table \ref{Tab:SOTA_LCTSC} presenting the comparison data using Dice,ASD and IoU. The models we chose are from best performing dice based model and the best performing cross-entropy based model. In this table, the CE+CTRM(UDBA) method achieves the best performance on four out of five organs (Esophagus, Spine, Heart, and Lung(R)) in terms of Dice, and on all five organs in terms of IoU. The Dice+CTRM(UDBA) method performs the best on esophagus and spine in terms of Dice coefficient, but not performing well on other three organs. The AttUnet method performs better on Esophagus in terms of ASD, but not as well as CE+CTRM(UDBA) and Dice+CTRM(UDBA) in terms of Dice coefficient and IoU. Overall, CE+CTRM(UDBA) appears to be the best performing method based on the evaluation metrics used in this table. For LCTSC dataset, the challenge organizers kept the ledger board private. Hence we are unable to compare side-by-side with the other baseline methods. 

\begin{table}[!htb]	
		\caption{\small Comparison between other baselines on LCTSC Dataset}
		\label{Tab:SOTA_LCTSC}
		\centering
        \begin{adjustbox}{width=\textwidth}
		\begin{tabular}{l |{c}| {c}| {c}| {c}|{c}|{c}| {c}| {c} |{c}| {c}| {c}| {c}| {c}| {c}| {c}}
			\hline
			\multirow{2}{*}{\textbf{Method}} &\multicolumn{3}{|c|}{\textbf{Esophagus}} &\multicolumn{3}{c|}{\textbf{Spine}} &\multicolumn{3}{c|}{\textbf{Heart}} &\multicolumn{3}{c}{\textbf{Lung(L)}} &\multicolumn{3}{|c}{\textbf{Lung(R)}}\\
		    \cline{2-16}
             &\textbf{Dice}&\textbf{ASD}&\textbf{IoU} &\textbf{Dice}&\textbf{ASD}&\textbf{IoU} &\textbf{Dice}&\textbf{ASD}&\textbf{IoU} &\textbf{Dice}&\textbf{ASD}&\textbf{IoU} &\textbf{Dice}&\textbf{ASD}&\textbf{IoU}\\
             \hline
			 CE+CTRM(UDBA)      &0.63&1.76&\textbf{0.80} &0.88&0.71&\textbf{0.91}  &\textbf{0.92}&\textbf{1.35}&\textbf{0.84} &\textbf{0.97}&0.65&\textbf{0.92}  &\textbf{0.97}&0.71&\textbf{0.92}\\
            \hline
            Dice+CTRM(UDBA)     &\textbf{0.71}&1.58&0.75	    &\textbf{0.89}&\textbf{0.66}&0.89	 &0.88&1.78&0.50	    &0.95&\textbf{0.62}&0.85	&0.96&\textbf{0.68}&0.89\\
            				
            \hline
            AttUnet\cite{Ozan2018}     &0.66&\textbf{1.09}&0.70     &\textbf{0.89}&\textbf{0.66}&\textbf{0.91}   &0.69&3.26&0.39   &0.95&0.68&0.86   &0.96&0.72&0.90\\
			\hline
		\end{tabular}
  \end{adjustbox}
\end{table}

%% Visual Results
\begin{figure}[!htb]
    \centering
    \begin{minipage}{\textwidth}
        \begin{tabular}{c}
            \begin{minipage}{0.51\textwidth}
                \includegraphics[width=\textwidth,trim={0 20mm 0 20mm},clip]{seg.pdf}
                \subcaption{Segmentations on SegThor dataset.} %\label{fig:visual}
            \end{minipage}
            \begin{minipage}{0.5\textwidth}
                \includegraphics[width=\linewidth,trim={0 20mm 0 20mm},clip]{LCTSC.pdf}
                \subcaption{Segmentations on LCTSC dataset.} %\label{fig:histogram}
            \end{minipage}\\
            \begin{minipage}{\textwidth}
                \includegraphics[width=\linewidth,trim={0 20mm 0 20mm},clip]{Histogram.pdf}
                \subcaption{3D CT intensity distribution overlap (Frequencies are in log scale)} %\label{fig:histogram}
            \end{minipage}
       \end{tabular}
    \end{minipage}
    \caption{In (a) and (b), top rows are the results from the methods with attention and the bottom rows are from same methods without attention. Red and Green color represents ground-truth and predictions respectively. (c) The intensity histogram overlap of ground-truth and predictions from the Esophagus and Heart regions.Similar to (a) and (b), the top row is from methods with attention and bottom row is without attention methods.}
    \label{fig:images}
\end{figure}


\section{Discussion and Conclusion}
In this work, we presented the utilization of slightly different network segmentation outputs to estimate uncertainty. We also demonstrated that estimated uncertainty can function as attention to enhance segmentation accuracy. Our team created a simple, easy-to-implement 2D U-net architecture that was trained exclusively with axial views. The application of uncertainty-based attention for improved segmentation was validated through experiments on two OAR segmentation datasets. Figure \ref{fig:images} (a) and (b) showcase 2D segmentation outputs using methods with and without attention, respectively. Additionally, 3D CT intensity histogram overlap (c) was presented to provide a more comprehensive view of segmentation accuracy. This histogram further highlights the effectiveness of the proposed attention module.

We also introduced the CT intensity integrated regularization loss to help the network learn texture distributions of organs and their shapes. Experimental evaluation confirmed efficiency of intensity integration for difficult-to-segment organs. Future research will focus on further improving the accuracy of proposed regularizer near the boundary of closely located organs.


\subsubsection{Acknowledgment}
The authors declare no conflict of interest. Technically,
the work is heavily supported by QUT High Performance
Computing (HPC) facility.

\bibliographystyle{splncs04}
\bibliography{references}
%

\end{document}
