{
    "arxiv_id": "2303.14897",
    "paper_title": "Seer: Language Instructed Video Prediction with Latent Diffusion Models",
    "authors": [
        "Xianfan Gu",
        "Chuan Wen",
        "Jiaming Song",
        "Yang Gao"
    ],
    "submission_date": "2023-03-27",
    "revised_dates": [
        "2023-04-13"
    ],
    "latest_version": 2,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Imagining the future trajectory is the key for robots to make sound planning and successfully reach their goals. Therefore, text-conditioned video prediction (TVP) is an essential task to facilitate general robot policy learning, i.e., predicting future video frames with a given language instruction and reference frames. It is a highly challenging task to ground task-level goals specified by instructions and high-fidelity frames together, requiring large-scale data and computation. To tackle this task and empower robots with the ability to foresee the future, we propose a sample and computation-efficient model, named \\textbf{Seer}, by inflating the pretrained text-to-image (T2I) stable diffusion models along the temporal axis. We inflate the denoising U-Net and language conditioning model with two novel techniques, Autoregressive Spatial-Temporal Attention and Frame Sequential Text Decomposer, to propagate the rich prior knowledge in the pretrained T2I models across the frames. With the well-designed architecture, Seer makes it possible to generate high-fidelity, coherent, and instruction-aligned video frames by fine-tuning a few layers on a small amount of data. The experimental results on Something Something V2 (SSv2) and Bridgedata datasets demonstrate our superior video prediction performance with around 210-hour training on 4 RTX 3090 GPUs: decreasing the FVD of the current SOTA model from 290 to 200 on SSv2 and achieving at least 70\\% preference in the human evaluation.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14897v1",
        "http://arxiv.org/pdf/2303.14897v2"
    ],
    "publication_venue": "17 pages, 15 figures"
}