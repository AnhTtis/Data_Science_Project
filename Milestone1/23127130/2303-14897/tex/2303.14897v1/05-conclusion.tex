\section{Conclusion}
In this paper, we propose Seer, a sample and computation efficient model, for the challenging text-conditioned video prediction (TVP) task. We design autoregressive spatial-temporal attention (AST-Attn) and Frame Sequential Text (FSText) Decomposer to inflate the pretrained text-to-image (T2I) stable diffusion models along the temporal axis. With the rich prior knowledge contained in pretrained T2I models and the well-designed architecture, Seer successfully generates high-quality videos by only fine-tuning the AST-Attn and FSText Decomposer, which significantly reduces the data and computation costs. The experiments illustrate our superior performance over all the recent models.



