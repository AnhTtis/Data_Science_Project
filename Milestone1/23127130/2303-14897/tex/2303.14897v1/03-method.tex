
\section{Preliminaries}
\noindent\textbf{Denoising Diffusion Probabilistic Models with classifier-free guidance:}
Diffusion models are probabilistic models that approximate the data distribution by iteratively adding noise and denoising through a forward/reverse Gaussian Diffusion Process~\cite{ddpm,song2020score}. The forward process applies noise at each time step $t\in{0,...,T}$ to the data distribution $\mathbf{x}_{0}$, creating a noisy sample $\mathbf{x}_t$ where $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\bm{\epsilon}$ ($\bm{\epsilon}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})$), and $\bar{\alpha}_t$ is the accumulation of the noise schedule $\alpha_{0:T}$ defined by $\bar{\alpha}_t=\prod^t_{s=1}\alpha_s$. To denoise images, the diffusion process uses a reparameterized variant of Gaussian noise prediction $\bm{\epsilon}_\theta(\mathbf{x}_t,t)$ targeting Gaussian noise $\bm{\epsilon}$. The reverse process $p(\mathbf{x}_{t-1}|\mathbf{x}_{t})$ of the Markov Chain generates new samples from Gaussian noise, which is approximated by Bayes' theorem as $q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)$, where $\mathbf{x}_0$ is derived from the forward process as $\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t-\sqrt{1-\bar{\alpha}_t\bm{\epsilon}_\theta(\mathbf{x}_t,t)})$.

Classifier-free guidance~\cite{clsfree} is introduced for conditional diffusion models to generate images without requiring an extra image classifier. A conditional model with a parameterized reverse process $p(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{c})$ uses a conditional identifier $\mathbf{c}$ through $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t,\mathbf{c})$. To predict an unconditional score, the conditional identifier is replaced with a null token $\O$ and denoted as $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t,\mathbf{c}=\O)$. Classifier-free guidance can then be approximated as a linear combination of conditional and unconditional predictions:
\vspace{-3pt}
\begin{equation}
   \bm{\tilde{\epsilon}}_{\theta}(\mathbf{x}_t,t,\mathbf{c}) = (1+w)\bm{\epsilon}_{\theta}(\mathbf{x}_t,t,\mathbf{c})-w\bm{\epsilon}_{\theta}(\mathbf{x}_t,t,\mathbf{c}=\O),
   \vspace{-3pt}
\end{equation}
where $w$ is the guidance scale. Text-video and text-image-based diffusion models~\cite{ldm,imagen,glide,vdm,makeavideo} use DDPM with classifier-free guidance. This diffusion framework can be adapted to various tasks with flexibility.

\noindent\textbf{Latent Diffusion Models:} 
Compared with image diffusion, video diffusion has significantly higher computation costs because it needs to process multiple frames.
Recent works have explored the computation-efficient version of diffusion modeling, such as latent diffusion model (LDM)~\cite{ldm}. LDM proposes the VAE-based latent diffusion, including a KL-regularized autoencoder for encoding/decoding latent representation $\bm{\varepsilon}(\mathbf{x})$, and a diffusion model to operate on the latent space $\mathbf{z}_t$.
For the conditional generation, LDM introduces a domain-specific encoder $\bm{\tau}_\theta$ to the projection of condition $\mathbf{y}$ for various modality generations. Thus, the objective of LDM is: 
\vspace{-5pt}
\begin{equation}
    \vspace{-10pt}
    L_{\mathrm{LDM}} = \mathbb{E}_{t,\bm{\varepsilon}(\mathbf{x}),\mathbf{y},\bm{\epsilon}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})}\Bigr[\|\bm{\epsilon} - \bm{\epsilon}_\theta(\bm{z}_t,t,\bm{\tau}_\theta(\mathbf{y}))\|^2\Bigr]
\end{equation}


\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{fig/temp_atten.pdf}
\caption{Variants of temporal attention layer, where $P$ is the location of latent vectors relative to the axis of spatial space $s$ and temporal space $n$.}
\vspace{-10pt}
\label{fig:tempattn}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{fig/seq_text_transformer.pdf}
\vspace{-5pt}
\caption{Frame Sequential Text Decomposer is shown in (a). We start by initializing the weight of the transformer network with learnable text tokens that are projected as identity vectors from CLIP text embeddings~\cite{radford2021clip}. We then optimize the generated text tokens via the diffusion process (b), where frame-individual cross-attention is denoted by ``fic-attn."}
\vspace{-10pt}
\label{fig:fseq}
\end{figure*}

\section{Methodology}\label{sec:method}
In this paper, we aim to explore an efficient diffusion method to predict coherent video frames guided by language instructions. 
However, it is challenging to directly apply conventional video diffusion models for TVP due to the following problems: (1) The limited labeled text-video data and computational resources. (2) Low fidelity of frame generation. (3) Lack of fine-grained instruction for each frame in the task-level videos.

Since our diffusion model is a 3D latent-based diffusion model, we address these problems by inflating the 2D latent diffusion model~\cite{ldm} along the temporal dimension. Specifically, we propose an inflated 3D U-Net to extend the prior knowledge contained in Stable Diffusion across the frames to generate high-quality and coherent frames (Sec.~\ref{sec:inflate}). To achieve this, we insert an Autoregressive Spatial-Temporal Attention layer (Sec.~\ref{sec:tempoal}). As for the language conditioning model, we propose a Frame Sequential Text (FSText) Decomposer to adaptively decompose the text instruction into sub-conditions for each frame (Sec.~\ref{sec:fstext}).


\subsection{Video Diffusion via Inflated Model}
\label{sec:inflate}
For the challenging TVP task, learning to parse natural language, understand the scene, and ground the language and scene together requires large-scale text-video datasets, which causes expensive computation and data annotation costs. With the development of conditional diffusion models, the text-to-image (T2I) models capture the underlying data distribution in large-scale datasets and thus get the capability to compose images with unseen objects specified by the text instructions. Motivated by this, we propose to take advantage of the prior knowledge implied in pretrained T2I models by inflating the T2I pre-trained latent diffusion model (LDMs) along the temporal axis, which will significantly reduce the need for large-scale training samples. 


Since the T2I latent diffusion models consist of two main components: the image diffusion module and the language conditioning module~\cite{ldm}. We propose to inflate these two parts to perform the synthesis of high-fidelity frames and the temporal decomposition of text instructions, respectively. Specifically, as shown in Figure~\ref{fig:pipeline} (a), we propose an Inflated 3D U-Net with the autoregressive spatial-temporal attention (AST-Attn) technique for the frame generation module in Section~\ref{sec:tempoal}, and a Frame Sequential Text (FSText) Decomposer for text conditioning module in Section~\ref{sec:fstext}. Overall, we adopt two pathways to implement the conditional diffusion process of language guidance and reference frames. During training, we stack the latent space of the reference frames with the noisy latent space of the remaining frames along the temporal dimension. During inference, we predict future frames by propagating the prior reference frames and Gaussian noise through the Inflated 3D U-Net. For text conditioning, we employ FSText Decomposer to incorporate the text condition into the diffusion model. 


With this design, Seer is able to generate high-quality, coherent and instruction-aligned videos by merely fine-tuning the autoregressive spatial-temporal attention layers and Frame Sequential Text Decomposer module. These two modules are jointly trained by the diffusion objective:
\vspace{-5pt}
\begin{equation}
    L_{\mathrm{diffusion}} = \mathbb{E}_{t,\bm{\varepsilon}(\mathbf{x}),\mathbf{y},\bm{\epsilon}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})}\Bigr[\|\bm{\epsilon} - \bm{\epsilon}_\theta(\mathbf{z}_t,t,f_\theta(\bm{\tau}(\mathbf{y})))\|^2\Bigr],
\end{equation}
where $f_\theta$ is our FSText decomposer, $\bm{\tau}$ is the frozen CLIP text encoder, and $\mathbf{y}$ is the input text.


\subsection{Inflated 3D U-Net with Autoregressive Spatial-Temporal Attention}\label{sec:tempoal}
We inflate the Text-to-Image (T2I) pre-trained 2D U-Net to our Inflated 3D U-Net as illustrated in Figure~\ref{fig:pipeline} (b). A standard 2D U-Net block of LDMs consists of a series of 2D ResNet blocks and Spatial Attention Blocks including spatial self-attention and cross-attention modules. Similar to~\cite{vdm}, we replace the $3 \times 3$ 2D convolution kernel with a $1 \times 3 \times 3$ 3D convolution kernel with an additional axis of video frames. Additionally, to further boost the performance of capturing the inter-frame dependency, we incorporate temporal attention after every spatial cross-attention layer. In Figure~\ref{fig:tempattn}, we explore various types of temporal attention, including: (1) bi-directional temporal attention~\cite{vdm,makeavideo,imagenvideo}, which employs a full self-attention across all tokens along the temporal dimension; (2) directed temporal attention~\cite{magicvideo}, which uses a masked attention mechanism that follows the direction of the video sequence along the temporal dimension; and (3) autoregressive spatial-temporal attention: a novel technique proposed by us, which uses causal attention to autoregressively generates the frames on both spatial and temporal dimensions by flattening the tokens into a long sequence.

We empirically observe that the two existing temporal attention layers cannot achieve promising performance on the TVP task. Bi-directional temporal attention tends to neglect the visual content guidance of the reference frames during the generation process (see Section~\ref{sec:ablate:temp}). 
And the directed temporal attention fails to capture the dependency of nearby spatial regions and thus generates low-quality frames, while it adheres to the temporal sequence constraint.

To handle the limitations of bi-directional and directed temporal attention, we introduce the Autoregressive Spatial-Temporal Attention (AST-Attn) mechanism shown in Figure~\ref{fig:tempattn}.
Given $n$ frames video, a video clip is projected into $n\times s$ latent vectors (where $s$ is the length of a latent vector in each frame) by the pre-trained VAE encoder. We flatten the latent vectors of both temporal and spatial dimensions ($n\times s$) into one dimension. 
Then, AST-Attn performs self-attention on this long sequence with a causal mask that prevents the model from learning from future temporal-spatial tokens. Because it performs in both spatial and temporal spaces, the frame generation will attend to not only the previous frames but also the nearby spatial regions, which results in high-fidelity generation performance.
While the calculation of Autoregressive Spatial-Temporal Attention (AST-Attn) involves both temporal and spatial dimensions, its computational complexity remains manageable due to the design of the Inflated 3D U-Net, which maintains the complexity of spatial compression rates and channel depths within a controllable range. Specifically, in the AST-Attn layers of Inflated 3D U-Net, higher-resolution features have more spatial tokens but are computed in lower embedding dimensions, while lower-resolution features have fewer spatial tokens but are computed in higher embedding dimensions. In practice, we observe that adopting AST-Attn has only a $0.4\%$ computation speed lag compared to directed and bidirectional temporal attention.

By incorporating Autoregressive Spatial-Temporal Attention in the Inflated 3D U-Net, we can generate high-fidelity and coherent video frames with minimal fine-tuning. Specifically, we merely fine-tune the proposed autoregressive spatial-temporal attention layers and freeze the rest of the pre-trained layers in our Inflated 3D U-Net. 

\subsection{Frame Sequential Text Decomposer}\label{sec:fstext}
For the language conditioning module, the existing methods~\cite{magicvideo,makeavideo,tuneavideo} simply encode a single text embedding for the whole video with a CLIP text encoder~\cite{radford2021clip}.
However, the goals specified by the text instructions are usually in the task level, which makes it difficult for the model to figure out the progress at each timestep with a global instruction embedding.
To better capture long-term dependency on both text and reference frames, we propose a Frame Sequential Text (FSText) Decomposer to decompose the global instruction into a sequence of fine-grained sub-instructions corresponding to each frame.

As illustrated in Figure~\ref{fig:fseq}, the FSText Decomposer takes in the global instruction embedding from CLIP text encoder~\cite{radford2021clip} and decomposes them into sub-instructions for each timestep.
Specifically, we introduce a sub-instruction embedding with $l$ tokens for each frame, so there are $n \times l$ learnable tokens in total, where $n$ is the number of frames. 
At the start of the decomposer layer, learnable tokens learn the context dependency of the text by text sequential attention.
Then, the global instruction embedding is decomposed into $n$ timesteps by performing cross-attention with the $n \times l$ learnable tokens. Through the projection of temporal attention and feedforward layers, learnable tokens obtain the fine-grained sub-instruction information, representing the progress of the macro-instruction being completed at each frame. 

After getting $n$ sub-instruction embeddings corresponding to each frame, the next step is to inject this guidance into the diffusion process, which is commonly completed by a cross-attention layer. As shown in Figure~\ref{fig:fseq} (b), different from the existing works that calculate the cross-attention between the global instruction embedding and $n$ frames~\cite{vdm,imagenvideo,magicvideo,tuneavideo}, we propose to perform cross-attention individually between the visual latent vectors and sub-instruction embedding of each frame
and then concatenate $n$ results together, which is called Frame-individual Cross-Attention (\textit{fic-attn}).
Note that: the parameters in \textit{fic-attn} are loaded from the T2I stable diffusion model and are frozen during training; and only the calculation of the attention scores for different frames is changed.


\noindent\textbf{Initialization~~} We find initialization is critical to FSText decomposer. Especially, the random initialization fails to approximate the distribution of text embeddings in the pretrained T2I model and results in poor performance. To guarantee the sub-instruction embeddings become a close approximation of the CLIP text embedding, we employ an initialization strategy by enforcing the FSText decomposer to be an identity function. It can be achieved by this objective:
\vspace{-5pt}
\begin{equation}
    L_{\mathrm{identity}} = \|f_\theta(\bm{\tau}(\mathbf{y})) - \bm{\tau}(\mathbf{y})\|^2
    \vspace{-5pt}
\end{equation}
Note that this initialization step is completed before the diffusion process. We will ablate this design in Section~\ref{sec:ablate:fstext}.
\iffalse
\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig/inflate_unet.pdf}
\caption{The overview of inflated 3D U-Net, we inflate the pre-trained T2I latent diffusion model (LDM) by expanding the 2D Conv kernel to 3D kernels and connecting the cross-attention layer with the trainable causal temporal attention layer.}
\label{fig:inflate3d}
\end{figure}
\fi


