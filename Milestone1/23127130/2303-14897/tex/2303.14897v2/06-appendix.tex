\appendix
\section{Additional Experimental Results}

\subsection{Implementation Details of Baselines}
We compare three baselines in our paper. For Tune-A-Video, to ensure a fair comparison, we use the pre-trained weight of Stable Diffusion-v1.5\footnote{https://github.com/CompVis/stable-diffusion} (same as our model) to initialize the UNet and we fine-tune the model with an image resolution of $256 \times 256$ on the training sets of Something Something-V2 (SSv2) and Bridgedata for 200k training steps. For MCVD, we train the model with an image resolution of $256 \times 256$ for 300k training steps. For TATS, we fine-tune the pre-trained UCF-101 model with an image resolution of $128 \times 128$ on the training sets of SSv2 and Bridgedata for 300k training steps.



\subsection{Evaluation Details and Results of UCF-101}~\label{appendix:sec:ucf101}
Most prior text-conditioned video generation methods~\cite{hong2023cogvideo,vdm,makeavideo,magicvideo} evaluate their performance on the UCF-101~\cite{ucf101} benchmark. However, since our proposed method, Seer, is designed for text-conditioned video prediction (TVP) on task-level video datasets, the UCF-101 benchmark, which evaluates class-conditioned video prediction on random short-horizon video clips, is not an ideal evaluation benchmark for TVP. Nonetheless, in order to fairly compare these baselines, we still evaluate the class-conditioned video prediction performance of Seer on UCF-101. 

\paragraph{Settings}Specifically, we fine-tune our model with a video resolution of $16\times256\times256$ on UCF-101. Following the evaluation protocols of ~\cite{hong2023cogvideo}, Seer predicts the videos conditioned on 5 reference frames during fine-tuning and inference stage. We report FVD and Inception score (IS) metrics on the UCF-101 dataset~\cite{ucf101}. The IS is calculated by a C3D model\cite{c3d} that is pre-trained on the Sports-1M dataset~\cite{sports} and fine-tuned on UCF101. We follow the evaluation code of TGAN-v2~\cite{tganv2} to calculate IS metric. Following ~\cite{hong2023cogvideo,vdm,makeavideo}, we evaluate the FVD metric with 2,048 samples and IS metric with 100k samples in the validation set of UCF-101.

\paragraph{Results} Table~\ref{table:tvp:ucf} presents the class-conditioned video prediction results on UCF-101, demonstrating that Seer outperforms CogVideo~\cite{hong2023cogvideo} and MagicVideo~\cite{magicvideo}, but falls short of Make-A-Video~\cite{makeavideo}. Make-A-Video employs unlabelled video pre-training on temporal layers and achieves the best performance among all other methods. While Make-A-Video shows superior performance on FVD and IS, Seer has the potential to further improve its generation performance by addressing the following two limitations. First, Seer has not been pre-trained on video data. Second, Seer obtains latent vectors via a pre-trained 2D VAE, which has not been fine-tuned on UCF-101 and limits the video generation quality of Seer (with 259.4 FVD and 68.16 IS reconstruction quality). However, as we focus on the text-conditioned video prediction task, addressing the above limitations on UCF-101 is out of the scope of this paper.

\begin{table*}
\begin{threeparttable}
\centering\small
\tablestyle{2pt}{1.1}
\setlength{\tabcolsep}{5pt}
\caption{\textbf{ Class-conditioned video prediction performance on UCF-101} we evaluate the Seer on the UCF-101 with 16-frames-long videos. Ex.data indicates that the model has been pre-trained or fine-tuned on extra datasets.
}
\label{table:tvp:ucf}
\begin{tabular}{cccc|cc}
\specialrule{.1em}{.05em}{.05em} 
 Method & Ex.data & Cond. & Resolution & FVD$\downarrow$ & IS$\uparrow$\\
 \hline
MoCoGAN-HD~\cite{mocogan} & No & Class.  & $256\times 256$ & 700\tiny{$\pm$24} & 33.95\tiny{$\pm$0.25}\\
VideoGPT~\cite{videogpt} & No & No  & $128\times 128$ & - & 24.69\tiny{$\pm$0.30}\\
RaMViD~\cite{ramvid} & No & No  & $128\times 128$ & - & 21.71\tiny{$\pm$0.21}\\
StyleGAN-V~\cite{styleganv} & No & No  & $128\times 128$ & - & 23.94\tiny{$\pm$0.73}\\
DIGAN~\cite{digan} & No & No  & & 577\tiny{$\pm$22} & 32.70\tiny{$\pm$0.35}\\
TGANv2~\cite{tganv2} & No & Class.  & $128\times 128$ & 1431.0 & 26.60\tiny{$\pm$0.47}\\
VDM~\cite{vdm} & No & No  & $64\times 64$ & - & 57.80\tiny{$\pm$1.3}\\
TATS-base~\cite{tats} & No & Class.  & $128\times 128$ & 278\tiny{$\pm$11} & 79.28\tiny{$\pm$0.38}\\
MCVD~\cite{mcvd} & No & No  & $64\times 64$ & 1143.0 & -\\
LVDM~\cite{lvdm} & No & No  & $256\times 256$ & 372\tiny{$\pm$11} & 27\tiny{$\pm$1}\\
MAGVIT-B~\cite{magvit} & No & Class.  & $128\times 128$ & 159\tiny{$\pm$2} & 83.55\tiny{$\pm$0.14}\\
 \hline
CogVideo~\cite{hong2023cogvideo} & txt-img \& txt-video & Class.  & $160\times 160$ & 626 & 50.46\\
Make-A-Video~\cite{makeavideo} & txt-img \& video & Class.  & $256\times 256$ & 81.25 & 82.55\\
MagicVideo~\cite{magicvideo} & txt-img \& txt-video & Class.  & & 699 & -\\
\textbf{Seer(Ours)} & txt-img & Class. & $256\times 256$ & 287.8 & 57.74\\
\specialrule{.1em}{.05em}{.05em} 
\textbf{pre-trained VAE}\tnote{*} & - & - & $256\times 256$ & 259.4 & 68.16\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\begin{tablenotes}\footnotesize
    \item [*] we evaluate the reconstruction quality of pre-trained 2D VAE in this table, the pre-trained 2D VAE is initialized with the pre-trained weight from Stable Diffusion-v1.5 without extra fine-tuning.
\end{tablenotes}
\end{threeparttable}
\end{table*}



\subsection{Evaluation Results of Sampling Steps}
To further investigate the generation effects of sampling steps during evaluation, we conduct a comparison between Seer and Tune-A-Video. We apply a series of DDIM sampling steps (10, 20, 30, 40, 50 DDIM steps), as shown in Figure~\ref{fig:ddimstep}. Seer consistently outperformed Tune-A-Video in terms of both FVD and KVD, with improvements observed from 20 DDIM steps to 50 DDIM steps. Particularly noteworthy is Seer's advantage in video quality (280.7 FVD and 0.73 KVD) compared to Tune-A-Video (419.3 FVD and 1.5 KVD) when using only 10 DDIM steps, demonstrating Seer's ability to sample high-fidelity videos efficiently with minimal denoising steps.
\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/ddim_curve.pdf}
\caption{Evaluation results of Seer and Tune-A-Video with DDIM sampling steps ranging from 10 to 50 on the Something-Something V2 dataset.}
\vspace{-8pt}
\label{fig:ddimstep}
\end{figure}



\subsection{Additional Ablation Results}~\label{appendix:sec:fstext}
\paragraph{FSText layer depth}In this section, we additionally investigate the impact of FSText Decomposer's layer depth in Table~\ref{table:ablation:layer}. Our default setting (8-layer FSText Decomposer) outperforms shallower models (2-layer and 4-layer) in terms of FVD. Though the 4-layer model shows a marginal advantage over the 8-layer model in terms of KVD, our experiments indicate that the 8-layer FSText Decomposer shows a remarkable advantage on FVD metrics and exhibits robustness in text-video alignment. Therefore, we adopt the 8-layer FSText Decomposer as the default setting for Seer.

\begin{table}
\centering\small
\tablestyle{2pt}{1.0}
\setlength{\tabcolsep}{5pt}
\caption{\textbf{Layer depth in FSText Decomposer}.}
\label{table:ablation:layer}
\vspace*{-3mm}
\begin{tabular}{c|cc}
num. layers.& FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 2 & 238.6 & 0.51\\
 4 & 229.7 & 0.23\\
8(Ours) &200.1 & 0.30\\
\end{tabular}
\end{table}

\paragraph{Qualitative results of fine-tuning ablation} We conduct a qualitative analysis of various fine-tune settings. We provide additional visualizations of Fine-tune Setting ablation in Section 5.5 of the main paper. Figure~\ref{fig:ablate:finetune} shows the results of different settings. Among these settings, our default setting \textit{``temp+FSText"} stands out as it preserves a higher-level temporal consistency in video prediction starting from reference frames and also delivers superior text-based video motion compared to the other fine-tune settings. 
\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/ablation_visualization.pdf}
\caption{Additional qualitative results of fine-tuning ablation. \textit{“temp+FSText.”} is our default setting.}
\vspace{-8pt}
\label{fig:ablate:finetune}
\end{figure}


\begin{table}
\centering\small
\tablestyle{2pt}{1.1}
\caption{Hyperparameters and details of Fine Tuning/Inference}
\label{table:hyperparam:finetune}
\begin{tabular}{c|cc}
\hline
param. & value\\
\hline
optim. & AdamW\\
Adam-$\beta_1$ &  0.9\\
Adam-$\beta_2$ &  0.99\\
Adam-$\epsilon$ &  $1e^{-8}$\\
weight decay &  $1e^{-2}$\\
lr &  $1.28e^{-5}$\\
end lr & 0.0\\
lr sche. & cosine\\
noise sche. & cosine\\
train batch size& 1/GPU\\
grad. acc.& 2\\
warmup steps& 10k\\
resolution& $256 \times 256$\\
train. steps & 200k\\
train. hardware & 4 RTX 3090\\
val. batch size& 2/GPU\\
sampler& DDIM\\
sampling steps & 30\\
guidance scale & 7.5\\
\hline
\end{tabular}
\end{table}

\begin{table}
\centering\small
\tablestyle{2pt}{1.1}
\caption{Hyperparameters of 3D U-Net}
\label{table:3dunet}
\begin{tabular}{c|cc}
\hline
hyperparam. & value\\
\hline
input/output channels &  4\\
Base channels & 320\\
Channel multipliers&  1,2,4,4\\
3D Downsample blocks &  4\\
3D Upsample blocks &  4\\
Number of layers (per block) &  2\\
\hline
Modules of layer & 3D ResnetBlock\\
 & Spatial-cross Atten.\\
 & ATS Atten.\\
 & Down./Up. 3D ResnetBlock\\
\hline
Dimension of atten. heads &  8\\
activation function &  SiLU\\
Dimension of cross-atten. &  768\\
\hline
\end{tabular}
\end{table}

\begin{table}
\centering\small
\tablestyle{2pt}{1.1}
\caption{Hyperparameters of FSText Decomposer}
\label{table:hyperparam:fstext}
\begin{tabular}{c|cc}
\hline
hyperparam. & value\\
\hline
learnable tokens channels &  768\\
output channels &  768\\
Base channels & 768\\
Number of layers &  8\\
\hline
Modules of layer & Seq-cross Atten.\\
 & Feedforward\\
 & Directed temporal Atten.\\
 & Feedforward\\
\hline
Number of atten. heads &  8\\
Dimension of cross-atten. &  768\\
\hline
\end{tabular}
\end{table}




\section{Implementation Details}~\label{appendix:sec:impl}

\subsection{Fine-tuning and Sampling}\label{sec:finetuneparam}
 In this section, we list the hyperparameters, fine-tuning details, sampling details, and hardware information of our model in Table~\ref{table:hyperparam:finetune}.
 
\subsection{Architecture information}\label{sec:arch}
In this section, we list the hyperparameters of 3D U-Net in Table~\ref{table:3dunet} and hyperparameters of FSText Decomposer in Table~\ref{table:hyperparam:fstext}.



\section{Visualization} 

\subsection{Additional qualitative results} 
We provide additional visualization on Something-Something v2 (SSv2) of our text-conditioned video prediction in Figure~\ref{fig:ssv2pred}, and text-conditioned video prediction/manipulation results in Figure~\ref{fig:ssv2mani}. Additionally, we provide the visualization on BridgeData of text-conditioned video prediction in Figure~\ref{fig:bridgepred} and text-conditioned video prediction/manipulation in Figure~\ref{fig:bridgemani}.
\section{Human Evaluation Details}~\label{appendix:sec:humaneval} 
To evaluate the quality of video predictions according to human preferences, we conducted a human evaluation with 99 video clips on the validation set of the Something-Something V2 dataset (SSv2), the evaluation process involved 54 anonymous evaluators. To eliminate biases towards specific baselines, we randomly selected 20 questions for each evaluator. Each single-choice question consisted of a ground-truth video as a reference, a manually modified text instruction, and two video prediction results generated by Seer and another baseline method. The evaluators were required to choose the video clip that is more consistent with the text instruction and has higher fidelity from the two options.
To ensure the clarity of the questions, we provided an example to explain the options in each questionnaire. Moreover, we recommended that evaluators prioritize video predictions with strong text-based motions as their first preference and the fidelity of the generated video as their second preference. For reference, Figure~\ref{fig:humanevalexp} provides a screenshot of an example questionnaire.

In total, we collected 342 responses for the Seer vs. TATS comparison, 363 responses for the Seer vs. Tune-A-Video comparison, and 357 responses for the Seer vs. MCVD comparison. And the results in the main paper Figure 7 are calculated based on the collected questionnaires.
\clearpage
\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/sth_predict.pdf}
\caption{Text-conditioned video prediction of Seer on SSv2.}
\vspace{-8pt}
\label{fig:ssv2pred}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/sth_manipulate.pdf}
\caption{Text-conditioned video prediction/manipulation of Seer on SSv2, where ``pred." refers to prediction, ``mani." refers to manipulation.}
\vspace{-8pt}
\label{fig:ssv2mani}
\end{figure}
\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{fig_appendix/bridge_pred.pdf}
\caption{Text-conditioned video prediction of Seer on BridgeData.}
\vspace{-8pt}
\label{fig:bridgepred}
\end{figure*}
\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/bridge_manipulate.pdf}
\caption{Text-conditioned video prediction/manipulation of Seer on BridgeData, where ``pred." refers to prediction, ``mani." refers to manipulation.}
\vspace{-8pt}
\label{fig:bridgemani}
\end{figure*}


\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/screenshot.PNG}
\caption{Screenshot of a questionnaire example shown to human evaluators.}
\vspace{-8pt}
\label{fig:humanevalexp}
\end{figure}