\section{Introduction}

Enabling robots to follow human instructions and complete complex tasks in real environments is the vision for the next generation of robotics learning. However, the instructions are often highly abstract outlines of task goals that require long-horizon operations of the robot to complete.  As a result, learning a mapping between abstract language instructions and specific policy actions at each time step presents a significant challenge. In contrast, humans employ an imagining-planning approach: when presented with a text instruction for a particular task, e.g., ``spinning cube that quickly stops spinning", they can imagine the future trajectories in their minds and plan accordingly. Therefore, generating videos conditioned on a few frames and language instructions, i.e., text-conditioned video prediction (\textbf{TVP}), is a fundamental task for robots to perform complex and long-horizon control and manipulation tasks.
By training large models on large-scale datasets, TVP can address the insufficient training data problem in robotics and facilitate the development of general robot policy learning.


\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig/overview.pdf}
\caption{Seer is an efficient video diffusion model that uses natural language instructions and reference frames (\textit{ref.}) to predict multiple variations of future frames.}
\vspace{-10pt}
\label{fig:overview}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{fig/pipeline.pdf}
\caption{(a) Seer's pipeline includes an Inflated 3D U-Net for diffusion and a Frame Sequential Text Transformer (FSeq Text Transformer) for text conditioning. During training, all video frames are compressed to latent space with a pre-trained VAE encoder. Conditional latent vectors, sampled from reference video frames, are concatenated with noisy latent vectors along the frame axis to form the input latent. During inference, the conditional latent vectors are concatenated with Gaussian noise vectors, and the denoised outputs are decoded to RGB video frames with the pre-trained VAE decoder.
(b) Our Inflated 3D U-Net expands the 2D Conv kernel of a pre-trained T2I latent diffusion model (LDM) to 3D kernels and connects the cross-attention layer with the Autoregressive Spatial-Temporal Attention (AST-Attn) layer.}
\vspace{-10pt}
\label{fig:pipeline}
\end{figure*}

Despite its potential benefits, text-conditioned video prediction (TVP) is a challenging task because it requires a deep understanding of the initial frames, the natural language instruction, and the grounding between language and images, while predicting based upon all the information above. The traditional text-conditioned video generation task~\cite{godiva,nuwa,hong2023cogvideo,vdm,makeavideo} does not condition on initial frames and thus a model could seemingly perform well if it only generates a few prototypical videos corresponding to the input text. The TVP task is much more challenging as the initial frames are given and generating prototypical videos is no longer a solution. 
Besides, the existing text-conditioned video generation task usually aims to generate short horizon video clips with text specifying the general content, such as “a person is skiing”, while our aim in the TVP task is to use the text as a “task descriptor” in robotics, such as “tipping a beer can”, as shown in Figure~\ref{fig:overview}. 

Specifically, there are mainly three problems limiting the performance of the TVP task: 
\textbf{1) Requirement for large-scale labeled text-video datasets and expensive computational cost:} learning to capture the correspondence between two different modalities is non-trivial and needs large amounts of supervised text-video pairs and excessive computation overhead for training. 
\textbf{2) Low fidelity of generated frames:} the frames generated by the models are usually blurry and cannot clearly display the background and objects specified in the reference frames.
\textbf{3) Lack of fine-grained instruction for each frame in the task-level videos:} the goals specified by text instructions are usually in the task level, making it difficult to understand the progress and generate the corresponding frame in each timestep only conditioned on a global text embedding.
To address these issues, we propose \textbf{Seer}: a TVP method capable of generating task-level videos according to the text guidance with high data and training efficiency.


Motivated by the recent progress on generative models~\cite{ldm,ramesh2022dalle2}, we propose to leverage text-to-image (T2I) latent diffusion models~\cite{ldm} for the TVP tasks. T2I models are pretrained with billions of text-image pairs crawled from the internet~\cite{schuhmann2021laion}. They have acquired rich prior knowledge and thus are able to generate high-quality images corresponding to the text descriptions. Therefore, inheriting such prior knowledge by inflating a T2I model along the temporal axis and fine-tuning it with a small text-video dataset is an appealing solution for TVP tasks, which relieves the requirement for extensive labeled data and computational overhead, i.e., Problem 1.

Since the T2I models contain two modalities: image and language, we propose to inflate these two parts to generate high-quality video frames and fine-grained text instruction embeddings for each timestep respectively. For the visual model, we extend the 2D latent diffusion model by inserting an \textbf{Autoregressive Spatial-Temporal Attention} (AST-Attn) layer into the denoising U-Net~\cite{ldm} to model spatial dependencies and the temporal dynamics simultaneously, which is called Inflated 3D U-Net.
By taking advantage of joint modeling of spatial and temporal dimensions, as well as autoregressive generation, we successfully synthesize coherent and high-fidelity frames, which alleviates Problem 2.
As for the language module, in contrast to existing approaches~\cite{zhou2022magicvideo,tuneavideo} that encode one text embedding for the whole video with a text encoder, we propose to decompose the single text instruction into fine-grained guidance embeddings for each time step. We achieve the automatic decomposition by a \textbf{Frame Sequential Text} (FSText) Decomposer based on the causal attention mechanism. By temporally splitting the instruction into different phases, 
Seer improves the guidance embeddings for each frame and thus enables the generation of task-level videos (Problem 3).

We conduct extensive experiments on Something-Something V2~\cite{sthv2} and Bridge Data~\cite{bridge} datasets. 
We outperform all the baselines, such as TATS~\cite{tats}, MCVD~\cite{mcvd} and \textit{Tune-A-Video} (TAV)~\cite{tuneavideo}, and achieve state-of-the-art video prediction performance in terms of FVD and KVD. 
Especially we improve FVD from $291$ to $200$, and KVD from $0.91$ to $0.30$ on Something-Something V2, compared to the SOTA TAV.
Compare to over 480 hours with $13 \times 8$ A100 GPUs in CogVideo~\cite{hong2023cogvideo}, the experiments show the high efficiency of our method: 214 hours with 4 RTX 3090 GPUs. 
The ablation studies illustrate the effectiveness of our two technical contributions: AST-Attn and FSText Decomposer. Furthermore, our method supports video manipulation by modifying the text instructions, and we demonstrate our superior generation quality through a human evaluation study, showing more than 70\% preference for us over TAV and around 90\% preference for us over TATS and MCVD.



