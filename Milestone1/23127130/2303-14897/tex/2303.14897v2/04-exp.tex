\section{Experiments}
In this section, we evaluate the proposed method Seer on the text-conditioned video prediction task. 
We compare against various recent methods and conduct ablation studies on the techniques presented in Section~\ref{sec:method}.

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig/prediction.pdf}
\caption{Visualization results of text-conditioned video prediction (conditioned on first 2 frames) on Something-Something V2. TAV refers to Tune-A-Video.}
\vspace{-10pt}
\label{fig:tvp:sthv2}
\end{figure}
\begin{table*}
\centering\small
\tablestyle{2pt}{1.1}
\setlength{\tabcolsep}{6pt}
{\caption{\textbf{ Text-conditioned video prediction (TVP) results on Something-Something V2 (SSv2) and Bridgedata (Bridge).} We report the FVD and KVD metrics of each method. We can see that our method Seer achieves the lowest FVD and KVD values in both SSv2 and Bridgedata, illustrating our superior performance on the challenging TVP task.
}
\label{table:tvp}}
\vspace*{-3mm}
\begin{tabular}{cccc|cc|cc}
\specialrule{.1em}{.05em}{.05em} 
 \multirow{2}{*}{Method} & \multirow{2}{*}{Pre.-weight} & \multirow{2}{*}{Text} & \multirow{2}{*}{Resolution} & \multicolumn{2}{c|}{\textbf{SSv2} (ref. = 2)} & \multicolumn{2}{c}{\textbf{Bridge} (ref. = 1)}\\
   &  &  &  & FVD$\downarrow$ & KVD$\downarrow$  & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
TATS~\cite{tats} & video & No  & $128\times 128$ & 428.1 & 2177 & 1253 & 6213\\
 MCVD~\cite{mcvd} & No & No  & $256\times 256$ & 1407 & 3.80 & 1427 & 2.50\\
Tune-A-Video~\cite{tuneavideo} & txt-img & Yes & $256\times 256$ & 291.4 & 0.91 & 515.7 & 2.01\\
Seer (Ours) & txt-img & Yes & $256\times 256$ & $\bf 200.1$ & $\bf 0.30$ & $\bf 507.3$ & $\bf 1.37$\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\vspace{-10pt}
\end{table*}
\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig/tvp_bridgedata.pdf}
\caption{Visualization of Text-conditioned Video Prediction on Bridgedata. ``Ref." refers to reference frames and TAV refers to Tune-A-Video.}
\vspace{-10pt}
\label{fig:tvp:bridge}
\end{figure}


\subsection{Datasets}
We conduct experiments on two text-video datasets: Something Something-V2 (SSv2)~\cite{sthv2}, which contains videos of human behaviors involving everyday objects accompanied by language instructions, and BridgeData~\cite{bridge} that is rendered by a photo-realistic kitchen simulator with text prompts.
Because the SSv2 validation set is too large (with over 200k samples), we follow \cite{ucf101} to evaluate the first 2048 samples during evaluation to save testing time.
For BridgeData, we split the dataset into an $80\%$ training set and $20\%$ validation set, and evaluate all validation samples. To reduce complexity, we downsample each video clip to 12 frames for SSv2 and 16 frames for BridgeData during both training and evaluation. Moreover, to provide a fair comparison with recent unreleased video generative model baselines~\cite{vdm,lvdm,magicvideo,makeavideo}, we also included results on the UCF-101 dataset~\cite{ucf101} in Appendix~\ref{appendix:sec:ucf101}.

\subsection{Implementation Details}
We use the pre-trained weights of the Tex-to-Image Latent Diffusion Model (LDM), Stable Diffusion-v1.5~\cite{ldm}
, to initialize the VAE, ResNet Blocks and Spatial-Cross Attention layers of the 3D U-Net. We freeze both the pre-trained VAE and the pre-trained modules of the 3D U-Net, and only fine-tune the Autoregressive Spatial-Temporal Attention Layers. To fine-tune the FSText Decomposer, we initialized it as the identity function of the CLIP text embedding, as described in Section~\ref{sec:fstext}. We train the models on Something Something-V2 and BridgeData with an image resolution of $256 \times 256$ for 20k training steps. In the evaluation stage, we speed up the sampling process with the fast sampler DDIM~\cite{ddim} and denoise the prediction with conditional guidance of 7.5 for 30 timesteps. Please refer to Appendix~\ref{appendix:sec:impl} for more details on hyperparameters.

\subsection{Evaluation Settings}
\noindent\textbf{Baselines.~~}\label{sec:baseline} We compare Seer with three publicly released baselines for video generation (1) conditional video diffusion methods: \textit{Tune-A-Video}~\cite{tuneavideo} and \textit{Masked Conditional Video Diffusion} (MCVD)\cite{mcvd}; and (2) autoregressive-based transformer method: \textit{Time-Agnostic VQGAN and Time-Sensitive Transformer} (TATS)\cite{tats}. Since Tune-A-Video is also the Text-to-Image inflated video diffusion model, and both MCVD and TATS are long video generative models for video prediction, they conform to our benchmark that requires predicting task-level movements. We further fine-tune Tune-A-Video, TATS, and train MCVD on the training sets of SSv2 and Bridgedata for 300k training steps.

\noindent\textbf{Machine Evaluation.~~}\label{sec:exp:tvp} We evaluate the text-conditioned video prediction of several baseline methods on Something Something-V2 (SSv2) (with 2 reference frames) and Bridgedata (with 1 reference frame). Additionally, we conduct several ablation studies of our proposed modules on SSv2. We report the Fréchet Video Distance (FVD) and Kernel Video Distance (KVD) metrics in our evaluation. FVD and KVD are calculated with the Kinetics-400 pre-trained I3D model~\cite{i3d}. We evaluate FVD and KVD on 2,048 SSv2 samples and 5,558 Bridgedata samples in the validation sets. For FVD metrics, we follow the evaluation code of VideoGPT~\cite{videogpt}. We further evaluate the class-conditioned video prediction of our method on the UCF-101 dataset~\cite{ucf101} and present the comparison results in Appendix~\ref{appendix:sec:ucf101}.

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{fig/human_eval.pdf}
\caption{Human evaluation results. Preference percentage for text-conditioned video manipulation on SSv2.}
\vspace{-8pt}
\label{fig:humaneval}
\end{figure}

\noindent\textbf{Human Evaluation.~~}\label{sec:exp:humaneval} Besides evaluating the models on the standard validation sets, we also manually modify the text prompts to provide richer testing results, called text-conditioned video manipulation. Because of the absence of ground-truth frames, we conducted a human evaluation of text-conditioned video manipulation using 99 video clips from the validation set of SSv2. We manually modified partial text prompts and generated 99 predicted videos for each method. Then, we invited 54 anonymous evaluators to rate the quality of the prediction, with a higher priority placed on the semantic contents in the videos and an intermediate priority placed on the fidelity of the video frames. We report the percentage of overall preference choices among the 99 video clips. More details are introduced in Appendix~\ref{appendix:sec:humaneval}.

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig/manipulation.pdf}
\caption{Visualization of Text-conditioned Video Prediction with the original (a) and manually modified (b) text prompts on Something-Something V2. ``Ref." is reference frames and TAV refers to Tune-A-Video.}
\vspace{-10pt}
\label{fig:tvm:sthv2}
\end{figure}

\subsection{Main Results}\label{sec:main-results}
\noindent\textbf{Quantitative Results.~~} Table~\ref{table:tvp} presents the text-condtioned video prediction results on Something Something-V2 (SSv2) and BridgeData. Seer achieves the best performance among all baselines, with the lowest Fréchet Video Distance (FVD) of 200.1 and Kinematic Distance (KVD) of 0.3 in SSv2, and the lowest FVD of 507 and KVD of 1.37 in BridgeData. Notably, Seer and Tune-A-Video both incorporate text conditioning, and the results highlight Seer's superior text-video alignment performance, especially on SSv2.

The results of the human evaluation in the text-conditioned video manipulation experiment are shown in Figure~\ref{fig:humaneval}. Our proposed Seer outperforms the other baselines in terms of both semantic content and fidelity of video, with a preference rate of at least $72.2\%$ in comparison. This indicates that Seer is effective in generating high-quality video clips that are faithful to the input text prompts.

\begin{table}
\centering\small
\tablestyle{2pt}{1.0}
\setlength{\tabcolsep}{4pt}
\floatsetup{floatrowsep=qquad, captionskip=1.5pt}
\begin{floatrow}[2]
\ttabbox
{\begin{tabular}{c|cc}
init. weight & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{random} & 367.9 & 0.75\\
\textit{identity}(Ours) & 200.1 & 0.30\\
\end{tabular}
{\caption[ftext]{\textbf{Init. weight ablation results of FSText}}
\label{table:ablation:weight}
}
}
\hfill
\ttabbox
{\begin{tabular}{c|cc}
temp. attn. & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{bi-direct.} & 258.2 & 0.56\\
 \textit{directed.} & 222.3 & 0.40\\
 \textit{autoreg.}(Ours) &200.1 & 0.30\\
\end{tabular}}
{\caption[temp]{\textbf{Ablation study of temporal attention}}
\label{table:ablation:tempattn}
}
\end{floatrow}

\begin{floatrow}[1]
\ttabbox[1.0\linewidth]
{\begin{tabular}{cc|cc}
\specialrule{.1em}{.05em}{.05em} 
fine-tune& FSText. & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{temp-attn.} & & 328.2 & 1.26\\
 \textit{cross}+\textit{temp-attn.} & & 249.9 & 0.73\\
 \textit{temp-attn.}(Ours) & \checkmark &200.1 & 0.30\\
 \textit{cross}+\textit{temp-attn.} & \checkmark & 1807 & 5.12\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}}
{\caption[fine]{\textbf{Ablation study of Fine-tune settings}}
\label{table:ablation:finetune}
}
\end{floatrow}
\end{table}


\iffalse
\begin{table}
\centering\small
\tablestyle{2pt}{1.0}
\setlength{\tabcolsep}{5pt}
\caption{\textbf{ Fine-tune settings and component design }.}
\label{table:ablation:finetune}
\vspace*{-3mm}
\begin{tabular}{cc|cc}
\specialrule{.1em}{.05em}{.05em} 
fine-tune& FSText. & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{temp-attn.} & & 328.2 & 1.26\\
 \textit{cross}+\textit{temp-attn.} & & 249.9 & 0.73\\
 \textit{temp-attn.}(Ours) & \checkmark &200.1 & 0.30\\
 \textit{cross}+\textit{temp-attn.} & \checkmark & 1807 & 5.12\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\end{table}
\fi


\noindent\textbf{Qualitative Results.~~} Figure~\ref{fig:tvp:sthv2} compares the text-conditioned video prediction (TVP) performance of Seer and Tune-A-Video on Something Something-V2 (SSv2). While Tune-A-Video can align simple text prompts with video in some cases, it struggles to consistently track the spatial appearance of reference frames in later predictions. For instance, in the ``taking glass from desk" samples, Tune-A-Video fails to generate a coherent motion trajectory and corrupts the pixels in the background, generating a new video instead of predicting from the reference frames. In contrast, Seer generates relatively coherent motion and better aligns the predictions with text prompts. Additionally, Seer can generate hidden objects by leveraging the imaging capability of the pretrained text-to-image diffusion model, which flexibly tackles occlusion issues in video prediction. In the ``tearing a piece of paper into two pieces" sample, Seer accurately predicts that a man is hidden behind the paper and generates coherent frames including the man's face. Figure~\ref{fig:tvp:bridge} compares Seer and Tune-A-Video's TVP performance on Bridgedata, illustrating that Seer achieves better text-video alignment, including the alignment of instructed behavior and target objects in future frames, and predicts a more coherent video with higher fidelity.

Figure~\ref{fig:tvm:sthv2} shows a comparison of Seer and Tune-A-Video for text-conditioned video prediction and manipulation on Something Something-V2 (SSv2). Tune-A-Video tends to mainly focus on Text-to-Video alignment, usually ignoring the directional temporal movement of the video. For example, in the case of ``turning the camera left while filming wall mounted fan", Tune-A-Video generates a semantic movement when the word "left" is replaced with "right" in the sentence, but fails to maintain temporal consistency in the video background during the transition from the middle to the last frame. In contrast, Seer performs better in handling the temporal dynamics of the video and achieving more precise text-video alignment in video manipulation.


\subsection{Ablation study}
In this section, we evaluate the effect of different components of our method in the TVP task on the SSv2 dataset.

\noindent\textbf{FSText Decomposer.~~}\label{sec:ablate:fstext}
Table~\ref{table:ablation:weight} compares different weight initialization strategies of FSText decomposer. The results demonstrate that using identity initialization described in Section~\ref{sec:fstext} yields higher prediction quality compared with random initialization. This finding demonstrates that identity initialization is necessary for the temporal-text projection of FSText decomposer. We also provide additional ablation results of FSText decomposer in Appendix~\ref{appendix:sec:fstext}.

\noindent\textbf{Temporal Attention.~~}\label{sec:ablate:temp}
As shown in Table~\ref{table:ablation:tempattn} studies the effectiveness of different types of temporal attention. Our autoregressive spatial-temporal attention (autoreg.) outperforms both bi-directional temporal attention (bi-direct.) and directed temporal attention (directed.), resulting in the lowest FVD and KVD scores. We also find that directed temporal attention further improves video prediction performance compared to bi-directional temporal attention because it utilizes the inductive bias of sequential generation.

\noindent\textbf{Fine-tune Setting.~~}
We compare various fine-tuning settings of 3D Inflated U-Net, and the results are presented in Table~\ref{table:ablation:finetune}. Our default setting involves fine-tuning both FSText decomposer (FSText.) and autoregressive spatial-temporal attention (AST-Attn.) layers (\textit{temp-attn.}), while freezing the remaining modules in 3D U-Net. For the ``\textit{temp-attn.}" setting, we only finetune the AST-Attn. layers and freeze all other components. In the "\textit{cross+temp-attn.}" setting, we jointly update the parameters of Spatial-Cross Attention layers and AST-Attn. layers. We further fine-tune the "\textit{cross+temp-attn.}" together with FSText decomposer. We observe that our default setting achieves the highest quality of video prediction among all these settings, indicating that fine-tuning FSText decomposer is critical. Based on our default setting, further fine-tuning "\textit{cross+temp-attn.}" causes the performance of Seer to drop a lot, even the worst among all fine-tuning settings. These results suggest that the optimization of the FSText decomposer is strongly guided by the frozen conditional diffusion prior, and additional fine-tuning of cross-attention leads to uncontrollable guidance to the FSText decomposer.

\iffalse
\begin{table}
\centering\small
\tablestyle{2pt}{1.0}
\setlength{\tabcolsep}{5pt}
\caption{\textbf{ Type of temporal attention }}
\label{table:ablation:tempattn}
\vspace*{-3mm}
\begin{tabular}{c|cc}
temp. attn. & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{bi-direct.} & 258.2 & 0.56\\
 \textit{directed.} & 222.3 & 0.40\\
 \textit{autoreg.}(Ours) &200.1 & 0.30\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\end{table}
\fi