\section{Introduction}

\begin{wrapfigure}[11]{r}{0.5\textwidth}
\vspace{-20pt}
\centering
\includegraphics[width=1.0\linewidth]{fig/overview.pdf}
\vspace{-20pt}
\caption{Seer is an efficient video diffusion model that uses natural language instructions and reference frames (\textit{ref.}) to predict multiple variations of future frames.}
\label{fig:overview}
\end{wrapfigure}
Text-conditioned Video Prediction (TVP), a task that generates future video frames conditioned on a few frames and language instructions, is crucial for diverse downstream tasks requiring instruction alignment and temporal consistency from an initial environment. For instance, in video editing, TVP empowers the generation of diverse temporal movements from an input video clip, guided by a range of language instructions. Importantly, TVP can selectively extend video segments while preserving temporal consistency from the input video. TVP also plays an important role in the scenario of robot learning.  TVP samples coherent future frames with aligned motion trajectories based on the initial state of a robot, providing task-level visual guidance for long-horizon planning. This overcomes the challenge for a robot to align abstract language instructions with long-horizon operations. Consequently, learning a TVP model is a fundamental task to achieve both temporal consistency in the transition distribution of the initial state and alignment between task-level language instructions and video motion, facilitating the development of video foundation models.

\begin{figure*}
\centering
\vspace{-30pt}
\includegraphics[width=0.85\linewidth]{fig/pipeline.pdf}
\vspace{-10pt}
\caption{(a) Seer's pipeline includes an Inflated 3D U-Net for diffusion and a Frame Sequential Text Transformer for text conditioning. (b) Our Inflated 3D U-Net expands the pre-trained 2D Conv kernel to 3D kernels and connects with the Scaled Autoregressive Window Temporal Attention layer.}
\vspace{-15pt}
\label{fig:pipeline}
\end{figure*}
%(a) Seer's pipeline includes an Inflated 3D U-Net for diffusion and a Frame Sequential Text (FSText) Transformer for text conditioning. During training, all video frames are compressed to latent space with a pre-trained VAE. Conditional latent vectors from reference video frames, are stacked with noisy latent vectors as the input latent. During inference, the conditional latent vectors are stacked with Gaussian noise, and the denoised output is decoded by the pre-trained VAE. (b) Our Inflated 3D U-Net expands the pre-trained 2D Conv kernel to 3D kernels and connects the cross-attention layer with the Scaled Autoregressive Window Temporal Attention (SAWT-Atten) layer.
Despite its potential benefits, text-to-video prediction (TVP) is a challenging task because it requires a deep understanding of the initial frames, the natural language instruction, and the grounding between language and images, while predicting based upon all the information above. In contrast to traditional text-to-video generation tasks \citep{godiva,nuwa,hong2023cogvideo,vdm,makeavideo}, which do not explicitly condition on initial frames, TVP requires a model to synthesize predictions based on the given initial frames and textual instructions. Merely generating a few prototypical videos corresponding to the input text is no longer a viable solution in TVP; the task necessitates a more detailed comprehension of temporal movement. 
Besides, the existing text-to-video generation task usually aims to generate short horizon video clips with text specifying the general content, such as “a person is skiing”, while our aim in the TVP task is to use the text as a task descriptor, such as “tipping a beer can”, as shown in Figure~\ref{fig:overview}.  

Specifically, there are mainly three problems limiting the performance of the TVP task: 
\textbf{1) Requirement for large-scale labeled text-video datasets and expensive computational cost:} learning to capture the correspondence between two different modalities is non-trivial and needs large amounts of supervised text-video pairs and excessive computation overhead for training. 
\textbf{2) Low fidelity of generated frames:} the frames generated by the models are usually blurry and cannot clearly display the background and objects specified in the reference frames.
\textbf{3) Lack of fine-grained instruction for each frame in the task-level videos:} the goals specified by text instructions are usually in the task level, making it difficult to understand the progress and generate the corresponding frame in each timestep only conditioned on a global text embedding.
To address these issues, we propose \textbf{Seer}: a TVP method capable of generating task-level videos according to the text guidance with high data and training efficiency.


Motivated by the recent progress on generative models~\citep{ldm,ramesh2022dalle2}, we propose to leverage text-to-image (T2I) latent diffusion models~\citep{ldm} for the TVP tasks. T2I models are pretrained with billions of text-image pairs crawled from the internet~\citep{schuhmann2021laion}. They have acquired rich prior knowledge and thus are able to generate high-quality images corresponding to the text descriptions. Therefore, inheriting such prior knowledge by inflating a T2I model along the temporal axis and fine-tuning it with a small text-video dataset is an appealing solution for TVP tasks, which relieves the requirement for extensive labeled data and computational overhead, i.e., Problem 1.

Since the T2I models contain two modalities: image and language, we propose to inflate these two parts to generate high-quality video frames and fine-grained text instruction embeddings for each timestep respectively. For the visual model, we extend the 2D latent diffusion model~\citep{ldm} to data and computation-efficient 3D network to model spatial dependencies and the temporal dynamics simultaneously, which is called Inflated 3D U-Net.
By taking advantage of joint modeling of spatial and temporal dimensions, as well as autoregressive generation, we successfully synthesize coherent and high-fidelity frames, which alleviates Problem 2.
As for the language module, in contrast to existing approaches~\citep{magicvideo,tuneavideo} that encode one text embedding for the whole video with a text encoder, we propose to decompose the single text instruction into fine-grained guidance embeddings for each time step. We achieve the automatic decomposition by a \textbf{Frame Sequential Text} (FSText) Decomposer based on the causal attention mechanism. By temporally splitting the instruction into different phases, 
Seer improves the guidance embeddings for each frame and thus enables task-level video generation (Problem 3).

We conduct extensive experiments on Something-Something V2~\citep{sthv2},  Bridge Data~\citep{bridge} and Epic-Kitchens-100~\citep{epickitchen} datasets. 
We outperform all the baselines, such as MCVD~\citep{mcvd}, TATS~\citep{tats} and \textit{Tune-A-Video} (TAV)~\citep{tuneavideo}, and achieve state-of-the-art performance in terms of FVD and KVD. 
Especially we improve FVD from $163$ to $113$ on Something-Something V2, compared to the SOTA TAV.
Compare to over 480 hours with $13 \times 8$ A100 GPUs in CogVideo~\citep{hong2023cogvideo}, the experiments show the high efficiency of our method: 120 hours with 4 RTX 3090 GPUs. 
The ablation studies illustrate the effectiveness of our computation-efficient video-inflated model with the newly proposed FSText Decomposer. Furthermore, our method supports video manipulation by modifying the text instructions, and we demonstrate our superior generation quality through a human evaluation study, showing more than 70\% preference over TAV and around 90\% preference over TATS and MCVD.



