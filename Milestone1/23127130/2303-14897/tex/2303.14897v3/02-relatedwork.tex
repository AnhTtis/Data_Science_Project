
\section{Related Work}

\subsection{Text-to-Image Generation}
Since Scott Reed et al.~\citep{reed2016generative} firstly set up the T2I generation task and proposed a GAN-based method, this multi-modal generation task has attracted the attention of the computer vision community. 
DALL-E~\citep{ramesh2021dalle} makes a breakthrough by modeling the T2I generation task as a sequence-to-sequence translation task with a VQ-VAE~\citep{van2017vqvae} and Transformer~\citep{vaswani2017attention}. Since then, many variants have been proposed with an improved image tokenizer~\citep{yu2022parti}, hierarchical Transformers~\citep{ding2022cogview2} or domain-specific knowledge~\citep{gafni2022make}.
With the recent progress of Denoising Diffusion Probabilistic Models (DDPM)~\citep{ddpm}, the diffusion models have been widely used for T2I generation tasks~\citep{glide,imagen,ramesh2022dalle2}. Specifically, GLIDE~\citep{glide} proposes classifier-free guidance for T2I diffusion models to improve image quality. For a better alignment between text and image, DALL-E 2~\citep{ramesh2022dalle2} proposed to denoise CLIP~\citep{radford2021clip} image embedding conditioned on CLIP text embedding, which integrated high-level semantic information. To reduce the computation cost of the denoising process in pixel space, Latent Diffusion Model (LDM) employs VAE~\citep{Kingma2014vae} to operate in the latent space. 
Seer takes advantage of the prior language-vision knowledge of pretrained LDM and inflates it along the time axis.

\subsection{Text-to-Video Generation}
In contrast to the huge success of Text-to-Image (T2I) generation, Text-to-Video (T2V) generation is still underexplored due to the limitation of the large text-video data annotation and computing resources.  Inspired by the various variants of T2I generation, recent T2V studies have attempted to explore compatible variants for video generation modeling. GODIVA~\citep{godiva} first proposes a VQ-VAE based auto-regressive model with three-dimensional sparse attention for T2V generation. N\"UWA~\citep{nuwa} further improves it by designing a 3D encoder with 3D nearby attention and achieves competitive performance on multi-task generation. Unlike the single frame-rate T2V approaches trained from scratch on large-scale text-video datasets, CogVideo~\citep{hong2023cogvideo} proposes a multi-frame-rate hierarchical model for T2V generation. This approach leverages the pre-trained module of T2I CogView-2~\citep{ding2022cogview2}.

Motivated by the remarkable progress of T2I diffusion models~\citep{ramesh2022dalle2,imagen,ldm}, Make-A-Video~\citep{makeavideo}, MagicVideo~\citep{magicvideo}, Tune-A-Video~\citep{tuneavideo} and Imagen Video~\citep{imagenvideo} transfer the 2D diffusion models to 3D models by incorporating temporal modules in T2V generation. In contrast to Imagen Video, all other three methods utilize the prior knowledge of T2I pre-trained model. Similarly, we use the pre-trained weight of the 2D T2I diffusion model in our 3D T2V model. Varying from the aforementioned methods, our method Seer utilizes autoregressive attention on both spatial and temporal spaces to generate high-fidelity and coherent video frames. And Seer is able to handle the task-level video prediction by decomposing the language condition into fine-grained sub-instruction.
