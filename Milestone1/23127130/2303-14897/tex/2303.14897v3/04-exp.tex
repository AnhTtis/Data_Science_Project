\section{Experiments}
In this section, we evaluate Seer on the text-conditioned video prediction task. 
We compare against various recent methods and conduct ablation studies on the techniques presented in Section~\ref{sec:method}.

\subsection{Datasets}\label{sec:dataset}
We conduct experiments on three text-video datasets: Something Something-V2 (SSv2)~\citep{sthv2}, which contains videos of human daily behaviors with language instructions, BridgeData~\citep{bridge} that is rendered by a photo-realistic kitchen simulator with text prompts, and EpicKitchens-100~\citep{epickitchen} (Epic100), which collects human daily activities in the kitchen in egocentric vision with multi-language narrations. For SSv2, we follow \citep{ucf101} to evaluate the first 2048 samples during evaluation to save testing time.
For BridgeData, we split the dataset into an $80\%$ training set and $20\%$ validation set for evaluation. To reduce complexity, we downsample each video clip to 12 frames for SSv2 and Epic100, and 16 frames for BridgeData during training/evaluation. Besides, We provide the zero-shot evaluation on EGO4D~\citep{ego4d} dataset in section~\ref{sec:generlize}. Moreover, we also included an additional evaluation on the UCF-101 dataset~\citep{ucf101} in Appendix~\ref{appendix:sec:ucf101}.
%, to provide a fair comparison with recent unreleased video generative model baselines~\citep{magicvideo,makeavideo}

\subsection{Implementation Details}\label{sec:impl}
\begin{wrapfigure}[14]{r}{0.45\textwidth}
\centering
\vspace{-35pt}
\includegraphics[width=1.0\linewidth]{fig/manipulation1.pdf}
\vspace{-20pt}
\caption{Visualization of Text-conditioned Video Prediction with the original (a) and manually modified (b) text prompts on SSv2.}
\label{fig:tvm:sthv2}
\end{wrapfigure}
We use the pre-trained weights of Stable Diffusion-v1.5~\citep{ldm} to initialize the VAE, ResNet Blocks and Spatial-Cross Attention layers of the 3D U-Net. We freeze both the pre-trained VAE, and only fine-tune the SAWT-Atten layers in our 3D U-Net. To fine-tune the FSText Decomposer, we initialized it as the identity function of the CLIP text embedding, as described in Section~\ref{sec:fstext}. We train the models with an image resolution of $256 \times 256$ on Something Something-V2 for 200k training steps, EpicKitchens-100 and BridgeData for 80k training steps. In the evaluation stage, we speed up the sampling process with the fast sampler DDIM~\citep{ddim} and conditional guidance of 7.5 for 30 timesteps. See more details in Appendix~\ref{appendix:sec:impl}.

\subsection{Evaluation Settings}
\noindent\textbf{Baselines.~~}\label{sec:baseline} We compare Seer with seven publicly released baselines for video generation (1) conditional video diffusion methods: \textit{Tune-A-Video}~\citep{tuneavideo}, \textit{Masked Conditional Video Diffusion} (MCVD)\citep{mcvd}, \textit{Video Probabilistic Diffusion Models} (PVDM)\citep{pvdm} and VideoFusion\citep{videofusion}; (2) autoregressive-based transformer method: \textit{Time-Agnostic VQGAN and Time-Sensitive Transformer} (TATS)\citep{tats} and \textit{Make It Move} (MAGE)\citep{mage2}; and (3) CNN-based encoder-decoder: SimVP\citep{simvp}.
%Since Tune-A-Video is also the Text-to-Image inflated video diffusion model, \gu{all PVDM, VideoFusion, MAGE, and SimVP are conditioned video models}, and both MCVD and TATS are long video generative models for video prediction, they conform to our benchmark that requires predicting task-level movements. Similarly, we further fine-tune Tune-A-Video and VideoFusion on Something Something-V2 for 200k training steps, Epic-Kitchens-100, and BridgeData for 80k training steps. We also fine-tune TATS, MAGE and train MCVD, SimVP, PVDM on the training sets of SSv2, Bridgedata and Epic-Kitchens-100 for 300k training steps.

\noindent\textbf{Machine Evaluation.~~}\label{sec:exp:tvp} We evaluate the text-conditioned video prediction of several baseline methods on Something Something-V2 (SSv2) (with 2 reference frames), Bridgedata (with 1 reference frame) and Epic-Kitchens-100 (Epic100) (with 1 reference frame). Additionally, we conduct several ablation studies of our proposed modules on SSv2. We report the Fréchet Video Distance (FVD) and Kernel Video Distance (KVD) metrics in our evaluation. FVD and KVD are calculated with the Kinetics-400 pre-trained I3D model~\citep{i3d}. We evaluate FVD and KVD on 2,048 SSv2 samples, 5,558 Bridgedata samples and 9,342 Epic100 samples in the validation sets. For FVD metrics, we follow the evaluation code of VideoGPT~\citep{videogpt}. We further evaluate the class-conditioned video prediction of our method on the UCF-101 dataset and present the comparison results in Appendix~\ref{appendix:sec:ucf101}.

\begin{wrapfigure}[7]{r}{0.5\textwidth}
\centering
\vspace{-15pt}
\includegraphics[width=0.8\linewidth]{fig/human_eval.pdf}
\vspace{-10pt}
\caption{Human evaluation results. Preference percentage for TVM task on SSv2.}
\label{fig:humaneval}
\end{wrapfigure}

\noindent\textbf{Human Evaluation.~~}\label{sec:exp:humaneval} Besides evaluating the models on the standard validation sets, we also manually modify the text prompts to provide richer testing results, called text-conditioned video manipulation. Because of the absence of ground-truth frames, we conducted a human evaluation of text-conditioned video manipulation (TVM) using 99 video clips from the validation set of SSv2. We manually modified partial text prompts and generated 99 predicted videos for each method. Then, we invited 54 anonymous evaluators to rate the quality of the prediction, with a higher priority placed on the semantic contents in the videos and an intermediate priority placed on the fidelity of the video frames. We report the overall preference choices among the 99 video clips. More details are introduced in Appendix~\ref{appendix:sec:humaneval}

\subsection{Main Results}\label{sec:main-results}
\noindent\textbf{Quantitative Results.~~} Table~\ref{table:tvp} presents the text-condtioned video prediction results on Something Something-V2 (SSv2),  BridgeData and Epic-kitchens-100 (Epic100). Seer achieves the best performance among all baselines, with the lowest Fréchet Video Distance (FVD) of 112.9 and Kinematic Distance (KVD) of 0.12 in SSv2, the lowest FVD of 246.3 and KVD of 0.55 in BridgeData, and the lowest FVD of 271.4 in Epic100. Notably, Seer, MAGE, VideoFusion and Tune-A-Video all incorporate text conditioning, and the results highlight Seer's superior text-video alignment performance.

The results of the human evaluation in the text-conditioned video manipulation experiment are shown in Figure~\ref{fig:humaneval}. Our proposed Seer outperforms the other baselines in terms of both semantic content and fidelity of video, with a preference rate of at least $72.2\%$ in comparison. This indicates that Seer is effective in generating high-quality video clips that are faithful to the input text prompts.

\iffalse
\begin{table}
\centering\small
\tablestyle{2pt}{1.0}
\setlength{\tabcolsep}{5pt}
\caption{\textbf{ Fine-tune settings and component design }.}
\label{table:ablation:finetune}
\vspace*{-3mm}
\begin{tabular}{cc|cc}
\specialrule{.1em}{.05em}{.05em} 
fine-tune& FSText. & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{temp-attn.} & & 328.2 & 1.26\\
 \textit{cross}+\textit{temp-attn.} & & 249.9 & 0.73\\
 \textit{temp-attn.}(Ours) & \checkmark &200.1 & 0.30\\
 \textit{cross}+\textit{temp-attn.} & \checkmark & 1807 & 5.12\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\end{table}
\fi

\begin{wrapfigure}[10]{r}{0.47\textwidth}
\centering
\vspace{-10pt}
\includegraphics[width=0.9\linewidth]{fig/tvp_bridgedata.pdf}
\vspace{-10pt}
\caption{Visualization of Text-conditioned Video Prediction on Bridgedata.}
\label{fig:tvp:bridge}
\end{wrapfigure}
\noindent\textbf{Qualitative Results.~~} Figure~\ref{fig:tvm:sthv2} compares the text-conditioned video prediction and manipulation performance of Seer, VideoFusion and Tune-A-Video on Something Something-V2 (SSv2). Seer performs better in handling the temporal dynamics of the video and achieving more precise text-video alignment in video manipulation. 
For instance, consider the task of ``turning the camera left" Seer seamlessly generates coherent movement while preserving the background during the camera view adjustment.  In contrast, both VideoFusion and Tune-A-Video exhibit semantic movement but fail to maintain temporal consistency in the video background. Additionally, Seer can imagine hidden objects by utilizing its text-to-image diffusion prior. This flexibility allows Seer to effectively address occlusion in video prediction. In the ``tearing paper" sample, Seer accurately predicts that a man is hidden behind the paper and generates coherent frames including the man's face. Figure~\ref{fig:tvp:bridge} compares Seer, VideoFusion and Tune-A-Video's TVP performance on Bridgedata, illustrating that Seer achieves better text-video alignment of instructed behavior and target objects in future frames, and predicts a more coherent video with higher fidelity.

%While VideoFusion and Tune-A-Video can align simple text prompts with video in some cases, both these two baselines struggle to consistently track the spatial appearance of reference frames in later predictions. For instance, in the ``taking glass from desk" samples, Tune-A-Video fails to generate a coherent motion trajectory and corrupts the pixels in the background, generating a new video instead of predicting from the reference frames. In contrast, Seer generates relatively coherent motion and better aligns the predictions with text prompts.
\begin{table*}
\centering\small
\vspace{-20pt}
\setlength{\tabcolsep}{1.5pt}
{\caption{\textbf{ Text-conditioned video prediction (TVP) results on Something-Something V2 (SSv2), Bridgedata (Bridge), and Epic-Kitchens-100 (Epic100).} We report the FVD and KVD metrics of each method in SSv2,  Bridgedata, and Epic100.
}
\label{table:tvp}}
\begin{tabular}{cccc|cc|cc|cc}
 \multirow{2}{*}{Method} & \multirow{2}{*}{Pre.-weight} & \multirow{2}{*}{Text} & \multirow{2}{*}{Resolution} & \multicolumn{2}{c|}{\textbf{SSv2}} & \multicolumn{2}{c}{\textbf{Bridge}} & \multicolumn{2}{c}{\textbf{Epic100}}\\
   &  &  &  & FVD$\downarrow$ & KVD$\downarrow$  & FVD$\downarrow$ & KVD$\downarrow$  & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
TATS~\citep{tats} & video & No  & $128\times 128$ & 428.1 & 2177 & 1253 & 6213 & 920.0 & 5065\\
 MCVD~\citep{mcvd} & No & No  & $256\times 256$ & 1407 & 3.80 & 1427 & 2.50 & 4804 & 5.17\\
 SimVP~\citep{simvp} & No & No  & $64\times 64$ & 537.2 & 0.61 & 681.6 & 0.73 & 1991 & 1.34\\
MAGE~\citep{mage2} & video & Yes  & $128\times 128$ & 1201.8 & 1.64 & 2605 & 3.19 & 1358 & 1.61\\
PVDM~\citep{pvdm} & No & No & $256\times 256$ & 502.4 & 61.08 & 490.4 & 122.4 & 482.3 & 104.8\\
VideoFusion~\citep{videofusion} & txt-video & Yes & $256\times 256$ & 163.2 & 0.20 & 501.2 & 1.45 & 349.9 & 1.79\\
Tune-A-Video~\citep{tuneavideo} & txt-img & Yes & $256\times 256$ & 291.4 & 0.91 & 515.7 & 2.01 & 365.0 & 1.98\\
Seer (Ours) & txt-img & Yes & $256\times 256$ & $\bf 112.9$ & $\bf 0.12$ & $\bf 246.3$ & $\bf 0.55$ & $\bf 271.4$ & $ 1.40$\\
\end{tabular}
\vspace{-10pt}
\end{table*}

\subsection{Ablation study}
In this section, we evaluate the effect of different components of our method in the TVP task on the SSv2 dataset. We also evaluate the zero-shot TVP task of different models on the EGO4D dataset.

\begin{wraptable}[7]{r}{0.32\textwidth}
\vspace{-20pt}
\centering\small
\setlength{\tabcolsep}{4pt}
\caption[temp]{\textbf{Ablation study of temporal attention}}
\begin{tabular}{c|cc}
temp. attn. & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{bi-direct.} & 258.2 & 0.56\\
 \textit{directed.} & 222.3 & 0.40\\
 \textit{autoreg.} &200.1 & 0.30\\
 \textit{win-auto.}(Ours) &112.9 & 0.12\\
\end{tabular}
\label{table:ablation:tempattn}
\end{wraptable}
\noindent\textbf{Temporal Attention.~~}\label{sec:ablate:temp}
As shown in Table~\ref{table:ablation:tempattn} studies the effectiveness of different types of temporal attention. Our scaled autoregressive window temporal attention (win-auto.) outperforms autoregressive spatial-temporal attention (autoreg.), bi-directional temporal attention (bi-direct.) and directed temporal attention (directed.), resulting in the lowest FVD and KVD scores. We also find that directed temporal attention further improves video prediction performance compared to bi-directional temporal attention because it utilizes the inductive bias of sequential generation.

\begin{wraptable}[5]{r}{0.32\textwidth}
\vspace{-20pt}
\centering\small
\setlength{\tabcolsep}{4pt}
\caption[ftext]{\textbf{Init. weight ablation results of FSText}}
\begin{tabular}{c|cc}
init. weight & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{random} & 367.9 & 0.75\\
\textit{identity}(Ours) & 112.9 & 0.12\\
\end{tabular}
\label{table:ablation:weight}
\end{wraptable}

\noindent\textbf{FSText Decomposer.~~}\label{sec:ablate:fstext}
Table~\ref{table:ablation:weight} compares different weight initialization strategies of FSText decomposer. The results show that using identity initialization described in Section~\ref{sec:fstext} yields higher prediction quality compared with random initialization.  
This finding demonstrates that identity initialization is necessary for the temporal-text projection of FSText decomposer. See additional ablation results in Appendix~\ref{appendix:sec:fstext}.

\begin{wraptable}[6]{r}{0.47\textwidth}
\vspace{-20pt}
\centering\small
\setlength{\tabcolsep}{4pt}
\caption[fine]{\textbf{Ablation study of Fine-tune settings}}
\begin{tabular}{cc|cc} 
fine-tune& FSText. & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{temp-attn.} & & 328.2 & 1.26\\
 \textit{cross}+\textit{temp-attn.} & & 249.9 & 0.73\\
 \textit{temp-attn.}(Ours) & \checkmark &112.9 & 0.12\\
 \textit{cross}+\textit{temp-attn.} & \checkmark & 1807 & 5.12\\
\end{tabular}
\label{table:ablation:finetune}
\end{wraptable}
\noindent\textbf{Fine-tune Setting.~~}
We compare various fine-tuning settings of 3D Inflated U-Net in Table~\ref{table:ablation:finetune}. Our default setting involves fine-tuning both FSText decomposer (FSText.) and scaled autoregressive window temporal attention (SAWT-Attn.) layers (\textit{temp-attn.}), while freezing the remaining modules in 3D U-Net. For the ``\textit{temp-attn.}" setting, we only finetune the SAWT-Attn. layers and freeze all other components. In the ``\textit{cross+temp-attn.}" setting, we jointly update the parameters of spatial-cross attn. and SAWT-Attn. layers. We observe that our default setting achieves the highest quality of video prediction among all these settings. Based on our default setting, further fine-tuning ``\textit{cross+temp-attn.}" causes the performance of Seer to drop a lot. These results suggest that the optimization of the FSText decomposer is strongly guided by the frozen conditional diffusion prior.

\subsection{Visual Analysis of Instruction Embedding}\label{sec:results:subins}
\begin{wrapfigure}[10]{r}{0.35\textwidth}
\centering
\vspace{-40pt}
\includegraphics[width=0.8\linewidth]{fig/sub_instruct_vis.pdf}
\vspace{-10pt}
\caption{TVP results conditioned on frame-wise sub-instruction and constant clone of sub-instruction.}
\label{fig:sub_vis}
\end{wrapfigure}
To assess the impact of sub-instruction guidance on frame generation and unveil the implicit semantic information contained within a single sub-instruction at specific time steps, we compare text-conditioned video prediction (TVP) results conditioned on default frame-wise sub-instructions with those conditioned on the constant clone of a sub-instruction from the third or twelfth frame along the video axis, as depicted in Figure~\ref{fig:sub_vis}. Unlike the default frame-wise sub-instruction guidance, the sub-instruction clone from the twelfth frame tends to produce a transition from the reference frame to the video's termination state without temporal coherence. In contrast, the sub-instruction clone from the third frame tends to guide the motion until an intermediate state of the video, indicating that temporal sub-instructions provide proximate semantic guidance for motion at each time step. These findings underscore that sub-instructions align with the temporal sequence of the global instruction, offering fine-grained guidance across multiple steps. See more results in the Appendix~\ref{appendix:sec:crossatten}.

%To thoroughly assess Seer's adaptability on smaller datasets such as BridgeData, we conducted an investigation into diverse fine-tuning strategies on this dataset. Influenced by recent advancements in fine-tuning large models, our initial attempt employs LORA module~\cite{lora} into the SSv2 fine-tuned Seer model by integrating the LORA module into each temporal layer of the UNet architecture and throughout the entire FSText decomposer. While the results demonstrated its prowess in aiding domain adaptation, this jeopardize the high-fidelity outcomes from the temporal-spatial layer. To maintain the generation quality of the fine-tuned temporal-spatial module, we proceeded to fine-tune only the temporal block of 3D U-Net, freezing the FSText decomposer. This was carried out on a mixing dataset, blending both the pre-training video dataset and the task-specific downstream dataset. Remarkably, this refined setting showcased success in preserving the model's capacity to generalize effectively within the specific domain of the task. As depicted in Figure~\ref{fig:generlize}, both our proposed setting and the LORA setting have effectively identified the previously unseen "black mug" in the training set of BridgeData. Concurrently, our proposed setting maintains a higher background consistency from the reference frame (frame 0) and generates future frames with superior fidelity when compared to the LORA setting. We also provide additional visualization in Appendix~\ref{appendix:sec:gen}.

\subsection{Zero-shot Evaluation}\label{sec:ablate:gen}
\begin{wraptable}[3]{r}{0.38\textwidth}
\vspace{-45pt}
\centering\small
\setlength{\tabcolsep}{4pt}
\caption[gen]{\textbf{Zero-shot text-conditioned video prediction on EGO4D}}
\begin{tabular}{c|cc}
temp. attn. & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{VideoFusion} & 618.6 & 1.85\\
 \textit{Seer(Ours)} &  301.7 & 0.55\\
\end{tabular}
\label{table:ablation:general}
\end{wraptable}
 %\textit{w/o. frozen. fstext} &458.3 & 0.94\\
 %\textit{lora.} & 310.4 & 0.56\\
To assess the model's generalizability on an unseen dataset, we conducted a comparative analysis between our proposed Seer model and the current state-of-the-art baseline VideoFusion. Both models were fine-tuned on the Something Something-V2 dataset and evaluated on the EGO4D dataset, with the results summarized in Table~\ref{table:ablation:general}. Notably, Seer method outperformed VideoFusion, as evidenced by its superior performance in terms of FVD and KVD metrics. Drawing insights from these results, we can conclude that our default setup demonstrates strong generalizability while effectively adapting to the EGO4D dataset with higher-quality video generation.


%These settings were applied to fine-tune the SSv2 video model on a mixed dataset comprising SSv2 and BridgeData, with a mixing ratio of 1:1 for 800k iterations, employing a learning rate of $4.096e^{-5}$, and subsequently evaluated on the previously unseen EGO4D dataset (sample 15 frames with 1 reference frame).


 
\iffalse
\begin{table}
\centering\small
\tablestyle{2pt}{1.0}
\setlength{\tabcolsep}{5pt}
\caption{\textbf{ Type of temporal attention }}
\label{table:ablation:tempattn}
\vspace*{-3mm}
\begin{tabular}{c|cc}
temp. attn. & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{bi-direct.} & 258.2 & 0.56\\
 \textit{directed.} & 222.3 & 0.40\\
 \textit{autoreg.}(Ours) &200.1 & 0.30\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\end{table}
\fi

%To thoroughly assess Seer's adaptability on smaller datasets such as BridgeData, we conducted an investigation into diverse fine-tuning strategies on this dataset. In our initial attempt, influenced by recent advancements in fine-tuning large models, we introduced the LORA module~\cite{lora} to the Seer temporal layer. Specifically, leveraging a pre-fine-tuned video model from the Something Something-V2 (SSv2) dataset, we integrated the LORA module into each temporal layer of the UNet architecture and throughout the entire FSText decomposer, utilizing a LORA rank of 128. During this fine-tuning phase, all other non-LORA modules were frozen, ensuring the retention of the video model's underlying knowledge. However, this initial trial led to a noticeable decline in the quality of generated videos. This outcome triggered substantial concern regarding the compatibility of LORA with the temporal layers of frozen U-Net. While LORA demonstrated its prowess in aiding domain adaptation for pre-trained models constrained by limited parameters, it appeared to jeopardize the achievement of high-fidelity outcomes from the temporal-spatial layer. This was particularly evident in the context of spatial image generation.
%To maintain the generation quality of the fine-tuned temporal-spatial module, which has been fine-tuned on the SSv2 dataset, we modified our approach. We proceeded to fine-tune only the temporal block of the U-Net architecture, preserving the frozen FSText decomposer. This was carried out on a mixing dataset, blending both the pre-training video dataset and the task-specific downstream dataset with a ratio of nearly 1 to 1. Remarkably, this refined approach showcased success in preserving the model's capacity to generalize effectively within the specific domain of the task. As depicted in Figure~\ref{fig:generlize}, both our proposed setting and the LORA setting have effectively identified the previously unseen "black mug" in the training set of BridgeData. Concurrently, our proposed setting maintains a higher level of background consistency from the reference frame (frame 0) and generates future frames with superior fidelity when compared to the LORA setting. 
%We further explored various settings to validate the generalizability of the model, and the results of these explorations are elaborated in the experiment section.