\section{Additional Experimental Results}

\subsection{Implementation Details of Baselines}\label{sec:appendix:impl_base}
We compare three baselines in our paper. For Tune-A-Video, to ensure a fair comparison, we use the pre-trained weight of Stable Diffusion-v1.5\footnote{https://github.com/CompVis/stable-diffusion} (same as our model) to initialize the UNet and we fine-tune the model with an image resolution of $256 \times 256$ on the training sets of Something Something-V2 (SSv2), Bridgedata and Epic-Kitchens-100 for 200k training steps. Similarly, we further fine-tune pre-trained VideoFusion on Something Something-V2, Epic-Kitchens-100, and BridgeData for 200k training steps. For MCVD, SimVP and PVDM, we train the model with an image resolution of $256 \times 256$, $64\times 64$, $256\times 256$ respectively on the training sets of SSv2, Epic-Kitchens-100, and Bridgedata for 300k training steps. For TATS and MAGE, we fine-tune the pre-trained UCF-101 model with an image resolution of $128 \times 128$ on the training sets of SSv2, Epic-Kitchens-100, and Bridgedata for 300k training steps.

\subsection{Evaluation Details and Results of UCF-101}\label{appendix:sec:ucf101}
Most prior text-conditioned video generation methods~\citep{hong2023cogvideo,vdm,makeavideo,magicvideo} evaluate their performance on the UCF-101~\citep{ucf101} benchmark. However, since our proposed method, Seer, is designed for text-conditioned video prediction (TVP) on task-level video datasets, the UCF-101 benchmark, which evaluates class-conditioned video prediction on random short-horizon video clips, is not an ideal evaluation benchmark for TVP. Nonetheless, in order to fairly compare these baselines, we still evaluate the class-conditioned video prediction performance of Seer on UCF-101. 

\paragraph{Settings}Specifically, we fine-tune our model with a video resolution of $16\times256\times256$ on UCF-101. Following the evaluation protocols of ~\citep{hong2023cogvideo}, Seer predicts the videos conditioned on 5 reference frames during fine-tuning and inference stage. We report FVD and Inception score (IS) metrics on the UCF-101 dataset~\citep{ucf101}. The IS is calculated by a C3D model\citep{c3d} that is pre-trained on the Sports-1M dataset~\citep{sports} and fine-tuned on UCF101. We follow the evaluation code of TGAN-v2~\citep{tganv2} to calculate IS metric. Following ~\citep{hong2023cogvideo,vdm,makeavideo}, we evaluate the FVD metric with 2,048 samples and IS metric with 100k samples in the validation set of UCF-101.

\paragraph{Results} Table~\ref{table:tvp:ucf} presents the class-conditioned video prediction results on UCF-101, demonstrating that Seer outperforms CogVideo~\citep{hong2023cogvideo} and MagicVideo~\citep{magicvideo}, but falls short of Make-A-Video~\citep{makeavideo}. Make-A-Video employs unlabelled video pre-training on temporal layers and achieves the best performance among all other methods. While Make-A-Video shows superior performance on FVD and IS, Seer has the potential to further improve its generation performance by addressing the following two limitations. First, Seer has not been pre-trained on video data. Second, Seer obtains latent vectors via a pre-trained 2D VAE, which has not been fine-tuned on UCF-101 and limits the video generation quality of Seer (with 259.4 FVD and 68.16 IS reconstruction quality). However, as we focus on the text-conditioned video prediction task, addressing the above limitations on UCF-101 is out of the scope of this paper.

\begin{table*}
\centering\small
\setlength{\tabcolsep}{5pt}
\caption{\textbf{ Class-conditioned video prediction performance on UCF-101} we evaluate the Seer on the UCF-101 with 16-frames-long videos. Ex.data indicates that the model has been pre-trained or fine-tuned on extra datasets.
}
\label{table:tvp:ucf}
\begin{tabular}{cccc|cc}
 \hline 
 Method & Ex.data & Cond. & Resolution & FVD$\downarrow$ & IS$\uparrow$\\
 \hline
MoCoGAN-HD~\citep{mocogan} & No & Class.  & $256\times 256$ & 700\tiny{$\pm$24} & 33.95\tiny{$\pm$0.25}\\
VideoGPT~\citep{videogpt} & No & No  & $128\times 128$ & - & 24.69\tiny{$\pm$0.30}\\
RaMViD~\citep{ramvid} & No & No  & $128\times 128$ & - & 21.71\tiny{$\pm$0.21}\\
StyleGAN-V~\citep{stylegan} & No & No  & $128\times 128$ & - & 23.94\tiny{$\pm$0.73}\\
DIGAN~\citep{digan} & No & No  & & 577\tiny{$\pm$22} & 32.70\tiny{$\pm$0.35}\\
TGANv2~\citep{tganv2} & No & Class.  & $128\times 128$ & 1431.0 & 26.60\tiny{$\pm$0.47}\\
VDM~\citep{vdm} & No & No  & $64\times 64$ & - & 57.80\tiny{$\pm$1.3}\\
TATS-base~\citep{tats} & No & Class.  & $128\times 128$ & 278\tiny{$\pm$11} & 79.28\tiny{$\pm$0.38}\\
MCVD~\citep{mcvd} & No & No  & $64\times 64$ & 1143.0 & -\\
LVDM~\citep{lvdm} & No & No  & $256\times 256$ & 372\tiny{$\pm$11} & 27\tiny{$\pm$1}\\
MAGVIT-B~\citep{magvit} & No & Class.  & $128\times 128$ & 159\tiny{$\pm$2} & 83.55\tiny{$\pm$0.14}\\
 \hline
VideoFusion~\citep{videofusion} & txt-video & Class.  & $128\times 128$ & 173 & 80.03\\
CogVideo~\citep{hong2023cogvideo} & txt-img \& txt-video & Class.  & $160\times 160$ & 626 & 50.46\\
Make-A-Video~\citep{makeavideo} & txt-img \& video & Class.  & $256\times 256$ & 81.25 & 82.55\\
MagicVideo~\citep{magicvideo} & txt-img \& txt-video & Class.  & & 699 & -\\
\textbf{Seer(Ours)} & txt-img & Class. & $256\times 256$ & 260.7 & 57.74\\
\hline 
\textbf{pre-trained VAE*} & - & - & $256\times 256$ & 259.4 & 68.16\\
 \hline
\end{tabular}
*we evaluate the reconstruction quality of pre-trained 2D VAE in this table, the pre-trained 2D VAE is initialized with the pre-trained weight from Stable Diffusion-v1.5 without extra fine-tuning.
\end{table*}

\subsection{Evaluation Results of Sampling Steps}
\textbf{Sampling steps:} To assess the influence of sampling steps on the quality of video predictions across varying sequence lengths, we conducted an evaluation on both 12-frame and 16-frame video predictions using a series of DDIM sampling steps (10, 20, 30, 40, 50, 60 DDIM steps). All generated outputs were sampled utilizing a 12-frame SSv2 fine-tuned model. The comparative results are presented in Figure~\ref{fig:ddimvideostep}. Notably, the 16-frame curve exhibits a more rapid decline from DDIM steps 10 to 20 compared to the 12-frame curve. As both curves progress beyond DDIM step 30, they tend to stabilize, showing marginal gains. These findings collectively underscore that increasing DDIM sampling steps notably enhances video quality for longer sequences (10 to 30 DDIM steps). However, the quality improvements of longer videos diminish as DDIM steps exceed 30.

\textbf{Video length:} Furthermore, we present the qualitative outcomes of the 16-frame video in Figure~\ref{fig:vis_16frame}. A comparison with the results of the 12-frame video in Figure~\ref{fig:ssv2mani} reveals that both the 16-frame and 12-frame videos can effectively capture task-described motion, as demonstrated in the example of "moving pen." However, the 16-frame video exhibits a gradual loss of appearance information from the reference frame and a decline in temporal consistency along the temporal axis with the increasing sequence length. Given that 16-frame videos are beyond the anticipated sequence length for a 12-frame model, enhancing the generation quality of the 16-frame video could be achieved through the training of a dedicated 16-frame Seer model.

\textbf{Methods comparison:} In addition, we explore the impact of fast sampling on generation results during evaluation by comparing Seer with Tune-A-Video.We apply a series of DDIM sampling steps (10, 20, 30, 40, 50 DDIM steps), as shown in Figure~\ref{fig:ddimstep}. Seer consistently outperformed Tune-A-Video in terms of both FVD and KVD, with improvements observed from 20 DDIM steps to 50 DDIM steps. Particularly noteworthy is Seer's advantage in video quality (280.7 FVD and 0.73 KVD) compared to Tune-A-Video (419.3 FVD and 1.5 KVD) when using only 10 DDIM steps, demonstrating Seer's ability to sample high-fidelity videos efficiently with minimal denoising steps.
\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/videolen_ddim_curve.pdf}
\caption{Evaluation results of sampling 12-frame video and 16-frame video using 12-frame Seer model with DDIM sampling steps ranging from 10 to 60 on the Something-Something V2 dataset.}
\vspace{-8pt}
\label{fig:ddimvideostep}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/ddim_curve.pdf}
\caption{Evaluation results of Seer and Tune-A-Video with DDIM sampling steps ranging from 10 to 50 on the Something-Something V2 dataset.}
\vspace{-8pt}
\label{fig:ddimstep}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/16frame_visual.pdf}
\vspace{-18pt}
\caption{Visualization of 16-frame text-conditioned video prediction sampled by 12-frame Seer on SSv2.}
\label{fig:vis_16frame}
\end{figure}

\begin{wraptable}[8]{r}{0.35\textwidth}
\centering\small
\setlength{\tabcolsep}{4pt}
\vspace{-20pt}
\caption[temp]{\textbf{Training time of the variants of temporal attention on a 16-frame video}}
\begin{tabular}{c|c}
temp. attn. & (sec./iter.)\\
 \hline
 \textit{bi-direct.} & 2.35\\
 \textit{directed.} & 2.35\\
 \textit{autoreg.} & 5.50\\
 \textit{win-auto.}(Ours) & 2.40\\
\end{tabular}
\vspace{-10pt}
\label{table:speed_temp}
\end{wraptable}
\subsection{Computation Efficiency of Seer} 
To enhance the computational efficiency of Seer, both the inclusion of frozen 2D layers and the thoughtful design of temporal layers contribute significantly. A comprehensive evaluation of the computational cost for different temporal layer types, conducted on a single 24GB NVIDIA 3090 GPU, is presented in Table~\ref{table:speed_temp}. Remarkably, in comparison to plain autoregressive spatial-temporal attention (autoreg.), both bi-directional/directed temporal attention (bi-direct./directed.) and SAWT-Atten (win-auto.) substantially reduce computational overhead. Furthermore, SAWT-Atten demonstrates superior generation quality compared to bi-directional and directed temporal attentions, as evidenced by the ablation results in Section~\ref{sec:ablate:temp} of this paper.
\begin{table}[h]
\begin{minipage}{0.5\textwidth}
\centering\small
\vspace{-8pt}
\setlength{\tabcolsep}{3pt}
\caption[temp]{\textbf{Training time (time.) and GPU memory (Mem.) consumption of the models (16-frame)}}
\vspace{-5pt}
\begin{tabular}{c|c|c|c}
\multirow{2}{*}{model} & \multirow{2}{*}{2D. frozen} & time. & Mem.\\
 &  & (sec./iter.) &(GB)\\
 \hline
 \textbf{\textit{Seer (Ours)}} & Yes & 0.75 & 24.7\\
 \textit{Seer }& No & 0.96 & 39.2\\
 \textit{VideoFusion.}& No & 1.07 & 45.0\\
\end{tabular}
\vspace{-8pt}
\label{table:speed_seer}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\vspace{-10pt}
\centering\small
\setlength{\tabcolsep}{3pt}
\caption[temp]{\textbf{Training time (time.) and GPU memory (Mem.) consumption of the models (16-frame)( $\geq$ 90\% GPU memory usage)}}
\begin{tabular}{c|c|c|c}
\multirow{2}{*}{model} & \multirow{2}{*}{2D. frozen} & time. & Mem.\\
 &  & (sec./iter.) &(GB)\\
 \hline
 \textbf{\textit{Seer (Ours)}} & Yes & 3.10 & 72.9\\
 \textit{Seer }& No & 6.89 & 75.8\\
 \textit{VideoFusion.}& No & 7.63 & 78.7\\
\end{tabular}
\vspace{-8pt}
\label{table:speed_seer_GPU}
\end{minipage}
\end{table}
In addition, a comparison involving our ablated setting and the baseline method VideoFusion~\citep{videofusion}, which shares a network design based on the Stable Diffusion U-Net, is presented. In our proposed Seer setting, the 2D spatial layers of the U-Net are frozen during fine-tuning, while the ablated setting maintains all 2D spatial layers trainable. GPU memory consumption and training time for different models were assessed running with a single 80GB NVIDIA A800 GPU. These results are available in Table~\ref{table:speed_seer}, and highlight the proposed Seer setting with frozen 2D layers exhibiting approximately half the GPU memory consumption (24.7GB) compared to Seer without frozen 2D layers (39.2GB) and VideoFusion (45.0GB). To ensure a fair comparison of training speed among various settings on the single A800 GPU, we conducted additional assessments of models with GPU memory usage exceeding 90\%, as detailed in Table~\ref{table:speed_seer_GPU}. The results in the table reveal a notable advantage in computation efficiency for the proposed Seer setting with frozen 2D layers, exhibiting reduced training time (3.10 sec/iter) compared to Seer without frozen 2D layers (6.89 sec/iter) and VideoFusion (7.63 sec/iter). These findings firmly establish the enhanced computational efficiency of Seer relative to baseline models.


\subsection{Additional Ablation Results}\label{appendix:sec:fstext}
\paragraph{FSText layer depth}In this section, we additionally investigate the impact of FSText Decomposer's layer depth in Table~\ref{table:ablation:layer}. Our default setting (8-layer FSText Decomposer) outperforms shallower models (2-layer and 4-layer) in terms of FVD. Though the 4-layer model shows a marginal advantage over the 8-layer model in terms of KVD, our experiments indicate that the 8-layer FSText Decomposer shows a remarkable advantage on FVD metrics and exhibits robustness in text-video alignment. Therefore, we adopt the 8-layer FSText Decomposer as the default setting for Seer.
\paragraph{FSText componenets}To evaluate the impact of individual attention layers within the FSText decomposer, we conducted an ablation study on the FSText component, as presented in Table 8. In this study, we ablate the temporal attention layer, labeled as "Temp.," and the text-sequential attention layer, denoted as "Seq.," within the FSText network. To maintain consistency in model size across different settings, we replaced the ablated component with a Cross-Attention layer. The results, shown in Table~\ref{table:fstext_component}, highlight the superiority of our proposed setting, which integrates both text-sequential-attention layers and temporal attention layers. Our proposed setting outperforms the other two settings, underscoring the significant attributes of Text-Sequential-Attention layers and Temporal Attention layers to capture text contextual information and model temporal dependencies, collectively enhancing the overall performance of the FSText decomposer.
\paragraph{Additional ablation on BridgeData}To validate the robustness and consistency of Seer ablation results across different datasets, we conducted additional experiments on the BridgeData dataset, extending our analysis from the Something Something-V2 (SSv2) dataset. The corresponding ablation studies on Seer fine-tuning settings are presented in Table~\ref{table:ablate_finetune_bridge}, while the temporal layer settings are detailed in Table~\ref{table:ablate_tempattn_bridge}. These results mirror the ablation outcomes reported in Table~\ref{table:ablation:finetune} and Table~\ref{table:ablation:tempattn} for the SSv2 dataset. Notably, the consistent improvement observed in both Seer fine-tuning and temporal layer ablation across different datasets, as demonstrated in Table~\ref{table:ablate_finetune_bridge} and Table~\ref{table:ablate_tempattn_bridge} on the BridgeData dataset, demonstrates the robustness of the Seer component design.
\begin{table}
\begin{minipage}{0.5\textwidth}
\centering\small
\vspace{-20pt}
\setlength{\tabcolsep}{5pt}
\caption{\textbf{Layer depth in FSText. (SSv2)}.}
\label{table:ablation:layer}
\begin{tabular}{c|cc}
num. layers.& FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 2 & 238.6 & 0.51\\
 4 & 229.7 & 0.23\\
8(Ours) &112.9 & 0.12\\
\end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\vspace{-30pt}
\centering\small
\setlength{\tabcolsep}{4pt}
\caption{\textbf{Components in FSText Decomposer (SSv2)}. Our settings are marked in \colorbox{baselinecolor}{gray}}
\label{table:fstext_component}
\begin{tabular}{cc|cc}
Temp. & Seq. & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \checkmark & & 125.8 & 0.13\\
 & \checkmark & 127.7 & 0.14\\
\baseline{\checkmark} & \baseline{\checkmark} & \baseline{112.9} & \baseline{0.12}\\
\end{tabular}
\vspace{-8pt}
\end{minipage}
\end{table}

\begin{table}
\begin{minipage}{0.5\textwidth}
\centering\small
\vspace{-10pt}
\setlength{\tabcolsep}{5pt}
\caption[temp]{\textbf{Ablation study of temporal attention (BridgeData)}}
\begin{tabular}{c|cc}
temp. attn. & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{bi-direct.} & 284.5 & 0.71\\
 \textit{directed.} & 258.0 & 0.64\\
 \textit{autoreg.} & 261.5 & 0.83\\
 \textit{win-auto.}(Ours) &246.3 & 0.55\\
\end{tabular}
\label{table:ablate_tempattn_bridge}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\vspace{-20pt}
\centering\small
\setlength{\tabcolsep}{4pt}
\caption[fine]{\textbf{Ablation study of Fine-tune settings (BridgeData)}}
\begin{tabular}{cc|cc} 
fine-tune& FSText. & FVD$\downarrow$ & KVD$\downarrow$\\
 \hline
 \textit{temp-attn.} & & 410.7 & 0.97\\
 \textit{cross}+\textit{temp-attn.} & & 319.9 & 1.01\\
 \textit{temp-attn.}(Ours) & \checkmark &246.3 & 0.55\\
 \textit{cross}+\textit{temp-attn.} & \checkmark & 2058.4 & 9.43\\
\end{tabular}
\vspace{-8pt}
\label{table:ablate_finetune_bridge}
\end{minipage}
\end{table}

\paragraph{Qualitative results of fine-tuning ablation} We conduct a qualitative analysis of various fine-tune settings. We provide additional visualizations of Fine-tune Setting ablation in Section 5.5 of the main paper. Figure~\ref{fig:ablate:finetune} shows the results of different settings. Among these settings, our default setting \textit{``temp+FSText"} stands out as it preserves a higher-level temporal consistency in video prediction starting from reference frames and also delivers superior text-based video motion compared to the other fine-tune settings. 
\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/ablation_visualization.pdf}
\caption{Additional qualitative results of fine-tuning ablation. \textit{“temp+FSText.”} is our default setting.}
\vspace{-8pt}
\label{fig:ablate:finetune}
\end{figure}


\subsection{Evaluation of Policy Learning on Seer}\label{appendix:sec:policylr}
To investigate whether Seer can help policy learning, we choose the UniPi~\citep{dai2023unipi} as our baseline in the simulated robotics environment Meta-World~\citep{metaworld}, which generates videos from the initial state and infers actions from the adjacent frames via a pre-trained inverse-dynamics model. Specifically, we distill a policy model from the videos generated by the Seer and the labeled actions from the pretrained inverse-dynamics model. We choose 3 tasks and use 10 in-domain videos for each task to fine-tune the Seer. And we make comparisons between
\textbf{a) Policy a:} Distilled policy from the dataset generated by the Fine-tuned Seer model (1000 generated videos for each task).
\textbf{b) Policy b:} Distilled policy from the 10 in-domain videos for each task. The results are shown in Table~\ref{table:policy}, where we find that compared to utilizing the given 10 in-domain videos to generate policy, fine-tuning the Seer with them and generating more videos can be better because it can acquire more scalable data, which is of comparable quality. Therefore, the videos generated by Seer can help policy learning in simulated robotics tasks in a way. The visualization of Seer within a robot simulation environment is presented in Section~\ref{appendix:sec:robotvision}
\begin{wraptable}[7]{r}{0.45\textwidth}
\centering\small
\setlength{\tabcolsep}{4pt}
\vspace{-8pt}
\caption[policy]{\textbf{Success rate of distilled policy}}
\begin{tabular}{c|c|c}
tasks & \textbf{policy a}& policy b\\
 \hline
 \textit{button-press-topdown-v2} & \textbf{0.45} & 0.4\\
\textit{drawer-close-v2} & \textbf{0.1} & 0.0\\
\textit{drawer-open-v2} & \textbf{0.05} & 0.0\\
\end{tabular}
\label{table:policy}
\end{wraptable}
\clearpage
\section{Implementation Details}\label{appendix:sec:impl}

\subsection{Fine-tuning and Sampling}\label{sec:finetuneparam}
 In this section, we list the hyperparameters, fine-tuning details, sampling details, and hardware information of our model in Table~\ref{table:hyperparam:finetune}.
 
\subsection{Architecture information}\label{sec:arch}
In this section, we list the hyperparameters of 3D U-Net in Table~\ref{table:3dunet} and hyperparameters of FSText Decomposer in Table~\ref{table:hyperparam:fstext}.

\begin{table}[h]
\centering\small
\caption{Hyperparameters and details of Fine Tuning/Inference}
\label{table:hyperparam:finetune}
\begin{tabular}{c|cc}
\hline
param. & value\\
\hline
optim. & AdamW\\
Adam-$\beta_1$ &  0.9\\
Adam-$\beta_2$ &  0.99\\
Adam-$\epsilon$ &  $1e^{-8}$\\
weight decay &  $1e^{-2}$\\
lr &  $1.024e^{-4}$\\
end lr & 0.0\\
lr sche. & cosine\\
noise sche. & cosine\\
train batch size& 1/GPU\\
grad. acc.& 2\\
warmup steps& 10k\\
resolution& $256 \times 256$\\
train. steps & 200k\\
train. hardware & 4 RTX 3090\\
val. batch size& 2/GPU\\
sampler& DDIM\\
sampling steps & 30\\
guidance scale & 7.5\\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering\small
\caption{Hyperparameters of 3D U-Net}
\label{table:3dunet}
\begin{tabular}{c|cc}
\hline
hyperparam. & value\\
\hline
input/output channels &  4\\
Base channels & 320\\
Channel multipliers&  1,2,4,4\\
3D Downsample blocks &  4\\
3D Upsample blocks &  4\\
Number of layers (per block) &  2\\
\hline
Modules of layer & 3D ResnetBlock\\
 & Spatial-cross Atten.\\
 & SAWT-Atten.\\
 & Down./Up. 3D ResnetBlock\\
\hline
Dimension of atten. heads &  8\\
activation function &  SiLU\\
Dimension of cross-atten. &  768\\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering\small
\caption{Hyperparameters of FSText Decomposer}
\label{table:hyperparam:fstext}
\begin{tabular}{c|cc}
\hline
hyperparam. & value\\
\hline
learnable tokens channels &  768\\
output channels &  768\\
Base channels & 768\\
Number of layers &  8\\
\hline
Modules of layer & Seq-cross Atten.\\
 & Feedforward\\
 & Directed temporal Atten.\\
 & Feedforward\\
\hline
Number of atten. heads &  8\\
Dimension of cross-atten. &  768\\
\hline
\end{tabular}
\end{table}
\newpage
\section{Visualization} 

\subsection{Additional qualitative results} 
We provide additional visualization on Something-Something v2 (SSv2) of our text-conditioned video prediction in Figure~\ref{fig:ssv2pred}, and text-conditioned video prediction/manipulation results in Figure~\ref{fig:ssv2mani}. Additionally, we provide the visualization on BridgeData of text-conditioned video prediction in Figure~\ref{fig:bridgepred} and text-conditioned video prediction/manipulation in Figure~\ref{fig:bridgemani}. We also provide the visualization results of text-conditioned video prediction on Epic-Kitchens-100 in Figure~\ref{fig:epicpred}.
\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/visual_1ref12frame_ssv2.pdf}
\vspace{-10pt}
\caption{Additional visualization results of 12-frame Text-conditioned video prediction (reference frame=1) on SSv2 dataset.}
\label{fig:sub_instruct_appendix}
\end{figure}
\begin{wrapfigure}[14]{r}{0.5\textwidth}
\centering
\vspace{10pt}
\includegraphics[width=1.0\linewidth]{fig/generalize.pdf}
\vspace{-20pt}
\caption{The visualization of comparison with different fine-tune settings on BridgeData (given text instructions "pick up black mug", where "black" text prompt is unseen in BridgeData). Compare to other two settings, our default setting successfully recognizes "black mug" and generates RGB frame with higher fidelity}
\label{fig:generlize}
\end{wrapfigure}
\subsection{Generalizability of Seer On Downstream Tasks}\label{sec:generlize}
To thoroughly assess Seer's adaptability on downstream tasks such as video manipulation on BridgeData, we conducted an investigation into diverse fine-tuning strategies on this dataset. Our default setting proceeded to fine-tune only the temporal block of 3D U-Net, freezing the FSText decomposer on a mixed dataset including both the pre-training video dataset and the task-specific downstream dataset. Additionally, we compare our default setting with LORA modules~\citep{lora}, which are integrated into each temporal layer of the UNet architecture and throughout the entire FSText decomposer.  As depicted in Figure~\ref{fig:generlize}, both our proposed setting and the LORA setting have effectively identified the previously unseen "black mug" in the training set of BridgeData. Besides, our proposed setting maintains a higher background consistency from the reference frame (frame 0) and generates future frames with superior fidelity when compared to the LORA setting. We also provide additional visualization in Appendix~\ref{appendix:sec:gen}.
\subsection{Additional Visualization of Sub-Instruction Embedding}\label{appendix:sec:crossatten} 
We present additional visualizations comparing frame-specific sub-instructions to those constant clones from the third or twelfth frame along the temporal axis (Figure~\ref{fig:sub_instruct_appendix}). These visualizations reveal that each frame's sub-instruction represents the motion at its corresponding time step. For example, in the case of "pushing iphone adapter from left to right" the default sub-instruction guides the entire instructed behavior with continuous temporal motion, while the sub-instruction embedding from the third frame directs the motion to a halfway point in the case of pushing the iphone adapter to right. In contrast, utilizing the sub-instruction embedding from the twelfth frame leads to frames that are disconnected from the reference frame's scene, resulting in the absence of transition between frames. This observation demonstrates that sub-instructions closely follow the temporal sequence of the global instruction, providing precise guidance across multiple steps.


\subsection{Additional Visualization of Video Manipulation With Unseen Objects And Zero-shot Video Manipulation}\label{appendix:sec:gen}
We also include visualizations of video manipulation with unseen objects in BridgeData and zero-shot generation on EGO4D. These experiments involved fine-tuning the SSv2 video model on a mixed dataset consisting of SSv2 and BridgeData. The fine-tuning process lasted for 800k iterations, employing a learning rate of $4.096e^{-5}$. Subsequently, the model's performance was evaluated on Bridgedata (as shown in Figure~\ref{fig:generlize_appendix}(a)) and the EGO4D dataset (illustrated on the right side of Figure~\ref{fig:generlize_appendix}(b)), where we sampled 15 frames with one reference frame. In the evaluation on BridgeData, Seer demonstrated the ability to recognize previously unseen objects such as "red plate" and "cabbage" and successfully performed a "pick up" motion, leveraging prior knowledge learned from SSv2. In certain scenarios, such as picking up the plate, where Seer has not encountered the specific action during training, the generated video depicted some limitations in the interaction between the robotic arm and the plate. Regarding the evaluation on EGO4D, although Seer had not been fine-tuned on this dataset, it exhibited the capability to accurately identify objects in the EGO4D environment, such as "laptop" and "book," and execute actions based on prior knowledge acquired from observing human activities. However, Seer still faced challenges in predicting future frames based on the understanding of the scene in the reference frame. For instance, in the case of "closing a book," Seer tended to generate a hand outside the camera view instead of manipulating the object with the main body, such as the hand holding the book, within the scene.


\clearpage
\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{fig_appendix/sub_instruct_appendix.pdf}
\vspace{-10pt}
\caption{Additional visualization results of Seer's Text-conditioned video prediction conditioned on frame-wise sub-instruction and duplicate sub-instruction at the third/twelfth frame.}
\label{fig:sub_instruct_appendix}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/gener_visual.pdf}
\vspace{-20pt}
\caption{Additional visualization of Seer generalizability evaluation on unseen dataset EGO4D (b) and BridgeData (a), where "cabbage" and "red plate" are unseen objects in the training set of BridgeData.}
\label{fig:generlize_appendix}
\end{figure}
\clearpage
\subsection{Visualization of Robot Simulation Environment}\label{appendix:sec:robotvision}
To investigate the applicability of Seer in robot environments, we assess its performance on robot simulation datasets, including Meta-World~\citep{metaworld}, CLIPort~\citep{cliport}, and RLBench~\citep{rlbench}. Beginning with the SSv2-finetuned Seer model, we further fine-tune it on a mixed dataset comprising  Something
Something-V2 (SSv2), Bridgedata, 236 MetaWorld video clips, 785 CliPort video clips, and over 2000 video clips from RLBench for 80,000 steps, using a learning rate of 4e-5. Throughout the fine-tuning process, all 2D layers of the 3D inflated U-Net and the FSText decomposer remain frozen. Figure~\ref{fig:robot_manipulate} presents visualizations of Seer on these robot simulation datasets. In these visualizations, Seer successfully generates coherent videos with continuous motion trajectories aligned with language instructions. In the CLIPort video clips, Seer accurately detects small target objects in complex scenes and places them in the correct positions as instructed by the task descriptors. Moreover, in the RLBench videos, Seer demonstrates the ability to perform multi-step tasks such as "stacking a pyramid with the boxes." These observations highlight Seer's adaptability to a multi-task environment, encompassing human video, robot manipulation, and simulation scenarios, while maintaining robust temporal alignment with task descriptors. We conduct an assessment of Seer performance on policy learning, and a further analysis is presented in Section~\ref{appendix:sec:policylr}.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\linewidth]{fig_appendix/robot_manipulation.pdf}
\caption{Text-conditioned video prediction of Seer (conditioned on first frame) on Meta-World (a1-2), CLIPort (b), and RLBench (c) datasets}
\label{fig:robot_manipulate}
\end{figure}

\subsection{Long Video Prediction}
To evaluate Seer's ability to generate extended video frames during the inference stage, we conducted a qualitative test on long video prediction using a 12-frame SSv2 fine-tuned Seer model. Our video sampling approach involves the sequential generation of video clips, where the first 12 frames are conditioned on 2 reference frames, and the last two frames of the first clip serve as the condition for the subsequent clip. The concatenation of these clips results in a long video. A crucial element for aligning long video frames with language is the presence of a prolonged sequence of sub-instructions. To achieve this, we employ two text conditioning strategies for expanding the sub-instruction embedding along the frame axis, enabling the iterative sampling of longer frames.

The first strategy involves interpolating sub-instruction embedding along the frame axis, while the second strategy entails repeating the sub-instruction embedding from the first video clip to guide the second clip. In Figure~\ref{fig:longvideo}, we compare these two strategies and observe that direct interpolation (Figure~\ref{fig:longvideo} (a)) tends to degrade overall generation quality, introducing unexpected noise. Conversely, utilizing the same sub-instruction (Figure~\ref{fig:longvideo} (b)) can maintain coherent motion, persisting until the target object is no longer present or the current motion is in a terminated state. Although this strategy facilitates the generation of coherent movements, it is distinct from a simple upsampling of video clips along the frame axis. It is noteworthy that all results were obtained using the 12-frame Seer model.

Intuitively, appending an additional upsampler network could potentially enhance Seer's ability to expand video frames, which causes extra computational costs. Observing the results of the evaluation, we believe that expanding the frame length of generated subinstruction embeddings during the training stage represents a promising direction for enabling Seer to generate longer video frames in multi-steps without incurring additional computational overhead.


\begin{figure}[h]
\centering
\includegraphics[width=0.85\linewidth]{fig_appendix/long_frame_gen.pdf}
\caption{22-frame video prediction conditioned on 2 reference frames using a 12-frame Seer model on SSv2. (a) Interpolation of 12-frame subinstruction embeddings to a 22-frame sequence. (b) Repetition of 12-frame subinstruction embeddings for the second video clip prediction from the eleventh frame to the twenty-second frame.}
\label{fig:longvideo}
\end{figure}
\clearpage
\subsection{Failure Case}
In this section, we present instances where Seer encounters challenges in handling environmental motion in human-generated videos. The generated videos highlight situations such as "dropping a card in front of a coin" and "book falling like a rock" (refer to Figure~\ref{fig:fail_case}), where Seer successfully predicts task-descriptive motions like "dropping" and "falling" and correctly identifies text-described objects such as "card" and "book." However, the generated future frames fall short in capturing appearance consistencies, such as the color of the card in the previous frame and the cover of the book in the reference environment. In the scenario of "pouring red wine into a glass," Seer tends to generate a wine glass based on its knowledge of pouring red wine but overlooks the transition distribution from the reference environment.

Notably, in the Epic-Kitchens-100 dataset, where scene transitions are prevalent, Seer exhibits a preference for predicting camera pose movements and generating novel views of the environment, reflecting its imaginative capabilities. However, these outcomes extend beyond the scope of Seer's primary objective, which is to learn human behavior. Consequently, addressing challenges such as filtering out irrelevant background information, including camera pose and object occlusion, and refining Seer's awareness of temporal motion becomes imperative for its adaptation to learning from internet videos. 

\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/failure_case.pdf}
\vspace{-18pt}
\caption{Seer failure cases on Something
Something-V2 (row 1,2,3) and Epic-Kitchens-100 (row 4,5,6)}
\label{fig:fail_case}
\end{figure}
\newpage
\section{Human Evaluation Details}\label{appendix:sec:humaneval} 
To evaluate the quality of video predictions according to human preferences, we conducted a human evaluation with 99 video clips on the validation set of the Something-Something V2 dataset (SSv2), the evaluation process involved 54 anonymous evaluators. To eliminate biases towards specific baselines, we randomly selected 20 questions for each evaluator. Each single-choice question consisted of a ground-truth video as a reference, a manually modified text instruction, and two video prediction results generated by Seer and another baseline method. The evaluators were required to choose the video clip that is more consistent with the text instruction and has higher fidelity from the two options.
To ensure the clarity of the questions, we provided an example to explain the options in each questionnaire. Moreover, we recommended that evaluators prioritize video predictions with strong text-based motions as their first preference and the fidelity of the generated video as their second preference. For reference, Figure~\ref{fig:humanevalexp} provides a screenshot of an example questionnaire.

In total, we collected 342 responses for the Seer vs. TATS comparison, 363 responses for the Seer vs. Tune-A-Video comparison, and 357 responses for the Seer vs. MCVD comparison. And the results in the main paper Figure 7 are calculated based on the collected questionnaires.
\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{fig_appendix/screenshot.PNG}
\caption{Screenshot of a questionnaire example shown to human evaluators.}
\vspace{-18pt}
\label{fig:humanevalexp}
\end{figure}
\clearpage
\begin{figure}
\vspace{-38pt}
\centering
\includegraphics[width=0.65\linewidth]{fig_appendix/sth_predict.pdf}
\vspace{-8pt}
\caption{Text-conditioned video prediction of Seer on SSv2.}
\label{fig:ssv2pred}
\end{figure}
\begin{figure}
\vspace{-38pt}
\centering
\includegraphics[width=0.65\linewidth]{fig_appendix/sth_manipulate.pdf}
\vspace{-8pt}
\caption{Text-conditioned video prediction/manipulation of Seer on SSv2, where ``pred." refers to prediction, ``mani." refers to manipulation.}
\label{fig:ssv2mani}
\end{figure}
\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{fig_appendix/bridge_pred.pdf}
\caption{Text-conditioned video prediction of Seer on BridgeData.}
\vspace{-8pt}
\label{fig:bridgepred}
\end{figure*}
\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/bridge_manipulate.pdf}
\caption{Text-conditioned video prediction/manipulation of Seer on BridgeData, where ``pred." refers to prediction, ``mani." refers to manipulation.}
\vspace{-8pt}
\label{fig:bridgemani}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{fig_appendix/visualization_epic_appendix.pdf}
\vspace{-8pt}
\caption{Text-conditioned video prediction (conditioned on first frame)
on Epic-Kitchens-100. TAV refers to Tune-A-Video, VF indicates VideoFusion.}
\label{fig:epicpred}
\end{figure*}


