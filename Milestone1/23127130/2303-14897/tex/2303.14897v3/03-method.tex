
\section{Preliminaries}
\noindent\textbf{Denoising Diffusion Probabilistic Models with classifier-free guidance:}
Diffusion models are probabilistic models that approximate the data distribution by iteratively adding noise and denoising through a forward/reverse Gaussian Diffusion Process~\citep{ddpm,song2020score}. The forward process applies noise at each time step $t\in{0,...,T}$ to the data distribution $\mathbf{x}_{0}$, creating a noisy sample $\mathbf{x}_t$ where $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\bm{\epsilon}$ ($\bm{\epsilon}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})$), and $\bar{\alpha}_t$ is the accumulation of the noise schedule $\alpha_{0:T}$ defined by $\bar{\alpha}_t=\prod^t_{s=1}\alpha_s$. To denoise images, the diffusion process uses a reparameterized variant of Gaussian noise prediction $\bm{\epsilon}_\theta(\mathbf{x}_t,t)$ targeting Gaussian noise $\bm{\epsilon}$. The reverse process $p(\mathbf{x}_{t-1}|\mathbf{x}_{t})$ of the Markov Chain generates new samples from Gaussian noise, which is approximated by Bayes' theorem as $q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)$, where $\mathbf{x}_0$ is derived from the forward process as $\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t-\sqrt{1-\bar{\alpha}_t\bm{\epsilon}_\theta(\mathbf{x}_t,t)})$.

Classifier-free guidance~\citep{clsfree} is introduced for conditional diffusion models to generate images without requiring an extra image classifier. A conditional model with a parameterized reverse process $p(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{c})$ uses a conditional identifier $\mathbf{c}$ through $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t,\mathbf{c})$. To predict an unconditional score, the conditional identifier is replaced with a null token $\O$ and denoted as $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t,\mathbf{c}=\O)$. Classifier-free guidance can then be approximated as a linear combination of conditional and unconditional predictions:
\vspace{-3pt}
\begin{equation}
   \bm{\tilde{\epsilon}}_{\theta}(\mathbf{x}_t,t,\mathbf{c}) = (1+w)\bm{\epsilon}_{\theta}(\mathbf{x}_t,t,\mathbf{c})-w\bm{\epsilon}_{\theta}(\mathbf{x}_t,t,\mathbf{c}=\O),
   \vspace{-3pt}
\end{equation}
where $w$ is the guidance scale. Text-video and text-image-based diffusion models~\citep{ldm,imagen,glide,vdm,makeavideo} use DDPM with classifier-free guidance. This diffusion method can be adapted to various tasks with flexibility.

\noindent\textbf{Latent Diffusion Models:} 
Compared with image diffusion, video diffusion has significantly higher computation costs because it needs to process multiple frames.
Recent works have explored the computation-efficient version of diffusion modeling, such as latent diffusion model (LDM)~\citep{ldm}. LDM proposes the VAE-based latent diffusion, including a KL-regularized autoencoder for encoding/decoding latent representation $\bm{\varepsilon}(\mathbf{x})$, and a diffusion model to operate on the latent space $\mathbf{z}_t$.
For the conditional generation, LDM introduces a domain-specific encoder $\bm{\tau}_\theta$ to the projection of condition $\mathbf{y}$ for various modality generations. Thus, the objective of LDM is: 
\vspace{-5pt}
\begin{equation}
    \vspace{-10pt}
    L_{\mathrm{LDM}} = \mathbb{E}_{t,\bm{\varepsilon}(\mathbf{x}),\mathbf{y},\bm{\epsilon}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})}\Bigr[\|\bm{\epsilon} - \bm{\epsilon}_\theta(\bm{z}_t,t,\bm{\tau}_\theta(\mathbf{y}))\|^2\Bigr]
\end{equation}

\section{Methodology}\label{sec:method}
In this paper, we aim to explore an efficient diffusion method to predict coherent video frames guided by language instructions, which requires learning to parse natural language, understand the scene, and ground the language and scene together. However, it is challenging to directly apply conventional video diffusion models for TVP due to the following problems: (1) The limited labeled text-video data and computational resources. (2) Low fidelity of frame generation. (3) Lack of fine-grained instruction for each frame in the task-level videos.
 
%\gu{Specifically, inheriting from I3D~\cite{i3d} technique, we build an inflated 3D U-Net to extend the prior knowledge contained in Stable Diffusion across the frames to generate high-quality and coherent frames by inserting computation-efficient spatial-temporal attention layers (Sec.~\ref{sec:efficientnet}).} As for the language conditioning model, we propose a novel Frame Sequential Text (FSText) Decomposer to adaptively decompose the text instruction into sub-conditions for each frame (Sec.~\ref{sec:fstext}).

\subsection{Overview of Seer}
\label{sec:inflate}
Motivated by the robust generative capabilities of text-to-image (T2I) diffusion models, we leverage the prior knowledge implied in pretrained T2I models by inflating the 2D U-Net~\citep{ldm} and incorporating temporally consistent layers. However, the inflated video diffusion model guided solely by coarse global language instruction tends to generate irrelevant T2I outcomes and fails to maintain temporal coherency between video frames. To address this limitation and provide precise and controllable guidance for our inflated model, we introduce a novel temporal decomposition component for language instruction, this component decomposes global instruction as temporally aligned sub-instruction for delicate task-level guidance, which significantly enhances the fidelity and coherency of predicted video.

Our Seer method comprises two main components: the video diffusion and the language conditioning modules. We propose to enhance these two components to facilitate high-fidelity frame synthesis and the temporal alignment of text instructions, respectively. Specifically, as shown in Figure~\ref{fig:pipeline} (a), we utilize two pathways to implement the conditional diffusion process guided by reference frames and language: \textbf{1)} We incorporate the spatial-temporal module discussed in Section~\ref{sec:efficientnet} into the Inflated 3D U-Net. This integration enables the propagation of contextual information from reference frames to future frames within the spatial-temporal space, allowing for coherent motion prediction based on the reference frames. \textbf{2)} To plan continuous motion with fine-grained language guidance, we introduce a Frame Sequential Text (FSText) Decomposer in Section~\ref{sec:fstext}. This module transforms global language instructions into multi-timestep sub-instructions that are synchronized with video. Subsequently, we inject these frame-wise subinstruction tokens into the intermediate latent space of the video frames at each time step.
With this design, we merely train the spatial-temporal layers and FSText module from scratch while freezing the remaining pretrained modules within our 3D inflated U-Net. These two modules are jointly trained by the diffusion objective, where $f_\theta$ is our FSText decomposer, $\bm{\tau}$ is the frozen CLIP text encoder, and $\mathbf{y}$ is the input text:
\begin{equation}
    L_{\mathrm{diffusion}} = \mathbb{E}_{t,\bm{\varepsilon}(\mathbf{x}),\mathbf{y},\bm{\epsilon}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})}\Bigr[\|\bm{\epsilon} - \bm{\epsilon}_\theta(\mathbf{z}_t,t,f_\theta(\bm{\tau}(\mathbf{y})))\|^2\Bigr],
\end{equation}


%\chuan{I find this paragraph is similar to the last paragraph of page 4. Just keep one to reduce redundancy.}
%Since the T2I latent diffusion models consist of two main components: the image diffusion module and the language conditioning module~\cite{ldm}. We propose to inflate these two parts to perform the synthesis of high-fidelity frames and the temporal decomposition of text instructions, respectively. \gu{Specifically, as shown in Figure~\ref{fig:pipeline} (a), we incorporate the computation-efficient spatial-temporal module into the Inflated 3D U-Net for optimizing temporal consistency in Section~\ref{sec:efficientnet}, and we propose a Frame Sequential Text (FSText) Decomposer for text conditioning module in Section~\ref{sec:fstext}.} Overall, we adopt two pathways to implement the conditional diffusion process of language guidance and reference frames. During training, we stack the latent space of the reference frames with the noisy latent space of the remaining frames along the temporal dimension. During inference, we predict future frames by propagating the prior reference frames and Gaussian noise through the Inflated 3D U-Net. For text conditioning, we employ FSText Decomposer to incorporate the text condition into the diffusion model.





\subsection{Data \& Computation-efficient 3D Network}\label{sec:efficientnet}
To design a computation-efficient visual backbone for our video diffusion model,  we refer to some relevant works on lifting 2D to 3D video modeling~\citep{i3d} and efficient attention computation~\citep{swin, videoswin}. In general, we leverage the latent diffusion model (LDMs) pretrained on T2I tasks to build a text-video model. Our inflated 3D U-Net consists of two principal components as illustrated in Figure~\ref{fig:pipeline} (b): \textbf{1)} The 3D spatial layers, where we draw inspiration from I3D~\citep{i3d} and enhance the 2D convolution kernel from ($3 \times 3$) to a 3D counterpart ($1 \times 3 \times 3$)  with an added video frames axis from the pre-trained 2D modules, consisting of a series of 2D ResNet blocks and Spatial Attention Blocks.
\begin{wrapfigure}[17]{r}{0.42\textwidth}
\centering
\vspace{-10pt}
\includegraphics[width=0.8\linewidth]{fig/wintempatten.pdf}
\vspace{-10pt}
\caption{Variants of temporal attention, only the blue tokens attend to the current token in the red box. Red dashed arrows indicate the direction of attention. And the orange boxes indicate the local window region ($2\times 2$ window in this case).}
\label{fig:tempattn}
\end{wrapfigure}
\textbf{2)} The temporal layers, play a crucial role in our visual backbone for propagating contextual information from the reference frame's image prior across the temporal sequence. We investigated various temporal attention and incorporated them into our 3D U-Net architecture. Our empirical observations indicate that bi-directional temporal attention tends to disregard guidance from reference frames, and both bi-directional and directed temporal attention struggle to capture dependencies among spatial regions, as discussed in Section~\ref{sec:ablate:temp}. To address these limitations while reducing complexity, we employ an efficient approach that builds upon the concept of window attention~\citep{swin} in 3D space: the implementation of local window attention in an autoregressive manner across spatial-temporal dimensions. As illustrated in Figure~\ref{fig:tempattn}, we establish fixed local windows for each spatial region with a window size of $m \times m$ relative to the global frame sequence $n$. Within this framework, we compute self-attention using a causal mask, considering both local spatial and global temporal dimensions within the 3D space. This effectively constrains pixel propagation from the future temporal-spatial sequence.

Finally, We maintain the acquired knowledge from the 2D modules by freezing all pretrained weights and exclusively training the spatiotemporal attention layers during fine-tuning. Overall, through a combination of frozen pre-trained spatial layers and lightweight spatiotemporal layers, our inflated 3D U-Net not only retains crucial knowledge but also enhances fine-tuning efficiency.

%Our empirical findings indicate that bi-directional temporal attention tends to disregard visual guidance from reference frames in time sequence (as discussed in Section~\ref{sec:ablate:temp}), and both bi-directional and directed temporal attention also miss out on capturing dependencies among nearby spatial regions, resulting in suboptimal frame quality. To address these limitations and enhance generation while reducing complexity, we employ an efficient approach that builds upon the concept of window attention~\cite{swin} in 3D space: the implementation of local window attention in an autoregressive manner across spatial-temporal dimensions. As illustrated in Figure~\ref{fig:tempattn}, we establish fixed local windows for each spatial region with a window size of $m \times m$ relative to the global frame sequence $n$. Within this framework, we compute self-attention with a causal mask across both local spatial and global temporal space within the 3D space, effectively constraining the pixel propagation from the future temporal-spatial sequence.
%The utilization of text-to-image (T2I) priors enhances the generative and imaginative capabilities of video generative models~\citep{makeavideo}. In this spirit, we leverage the latent diffusion model (LDMs) pretrained on T2I tasks, inflating it along the temporal axis. Our proposed latent diffusion model consists of two principal components as illustrated in Figure~\ref{fig:pipeline} (b): \textbf{1)} The 3D spatial layers inflated from the pre-trained image diffusion module, consisting of a series of 2D ResNet blocks and Spatial Attention Blocks. To adapt these 2D modules for 3D processing, we draw inspiration from I3D~\cite{i3d} to enhance the 2D convolution kernel from ($3 \times 3$) to a 3D counterpart ($1 \times 3 \times 3$)  with an added video frames axis. \textbf{2)} The temporal layers propagate contextual information from the image prior across the temporal sequence. We maintain the learned text-to-image knowledge from 2D modules by freezing all pre-trained weights during fine-tuning. This strategy not only retains crucial knowledge but also enhances fine-tuning efficiency.
%In comparison to plain spatial-temporal attention, the application of local windows to spatial regions significantly reduces computational overhead while delivering high-fidelity generation results. Notably, we observe that adopting SAWT-Atten has only a marginal $2.13\%$ computation speed lag compared to directed and bidirectional temporal attention, as shown in Appendix Table~\ref{table:speed_temp}.
%To address these limitations and enhance generation while reducing complexity, we employ an efficient approach: implementing local window attention in an autoregressive manner on both temporal and spatial spaces. Specifically, in this paper, extending from window attention~\cite{swin} in 3D space, we adopt a fixed window strategy with window sizes of $8\times8$, $4\times4$, $4\times4$, and $4\times4$ at stages 1, 2, 3, and 4 of U-Net encoder, respectively, within the U-Net network which utilizes multi-scale features. This strategy replaces the shifted window technique in Swin-Attention~\cite{swin}. Within the Scaled Autoregressive Window Temporal Attention (SAWT-Attn) layer (illustrated in Figure~\ref{fig:tempattn}), we extend vanilla temporal attention into spatial space through window attention for each spatial area with the local window of size $m \times m$, alongside the global video frame sequence $n$ across this window. The SAWT-Attn layer conducts self-attention on this extended sequence with a causal mask, integrating both spatial and temporal dimensions and restricting the model from learning future temporal-spatial tokens.
%As it operates in both spatial and temporal spaces, frame generation attends not only to prior frames but also to adjacent spatial regions, resulting in high-fidelity generation performance.
%In this context, for a video clip with $n$ frames, each is projected into $n\times H/K \times W/K \times 4$ latent vectors. Here $n$ signifies the frame count, $K$ indicates downsample ratio of VAE encoder, $(H/K, W/K)$ represents spatial dimensions, and $4$ corresponds to the number of latent channels.

\subsection{Frame Sequential Text Decomposer}\label{sec:fstext}
For the language conditioning module, since our 3D inflated U-Net is built upon a pretrained text-to-image model, we noticed that using a text-to-image prior alongside a global instruction tends to provide strong semantic guidance, which can override the scene in reference frames, deviating from the intended guidance for prediction based on the existing scenes.
To address the above limitation and better capture long-term dependencies from both text and reference frames, we introduce the Frame Sequential Text (FSText) Decomposer. This novel approach decomposes the global instruction into fine-grained sub-instructions, aligning with each frame. We further explore the interpretability of sub-instruction embeddings in Section~\ref{sec:results:subins}.
%the existing methods~\citep{magicvideo,makeavideo,tuneavideo} simply encode a single text embedding for the whole video with a CLIP text encoder~\cite{radford2021clip}.However, since text instructions often pertain to the overall task, understanding progress at each time step becomes challenging with a global instruction embedding
\begin{figure*}
\centering
\vspace{-30pt}
\includegraphics[width=0.9\linewidth]{fig/seq_text_transformer.pdf}
\vspace{-10pt}
\caption{Frame Sequential Text Decomposer is shown in (a). We start by initializing the weight of the network to project identity vectors from CLIP text tokens. We then optimize the generated text tokens via the diffusion process (b), where frame-individual cross-attention is denoted by ``fic-attn."}
\label{fig:fseq}
\vspace{-10pt}
\end{figure*}
To derive a sequence of temporally aligned sub-instruction embeddings from the global instruction generated by the CLIP text encoder~\cite{radford2021clip}, we employ a transformer-based temporal network designed to fulfill three essential properties for meaningful sub-instructions: \textbf{1)} Contextual aggregation, which ensures that the inner tokens of each sub-instruction aggregate contextual information within the sentence. \textbf{2)} Semantic inheritance, the semantic information of these sub-instructions is inherited directly from the global instruction \textbf{3)} Temporal consistency ensures alignment between the sub-instructions and the time sequence, thereby facilitating the generation of temporally consistent video. Based on these properties,  our network consists of  three key components: \textbf{a)} To achieve the property of contextual aggregation, we employ Text-Sequential Attention, akin to BERT, a bidirectional self-attention layer~\citep{bert} to capture global dependencies among different positions within text sentences. \textbf{b)} To ensure semantic inheritance, we use Cross-Attention, responsible for projecting the global instruction's textual sequence onto the inner tokens of each sub-instruction, this component ensures that all sub-instructions contain essential global instruction signals for guiding video frame generation. \textbf{c)} To maintain temporal consistency, we adopt temporal Attention, a directed attention layer to capture temporal dependencies along the frame axis, which enhances temporal consistency among the generated sub-instructions throughout the video.
\begin{wrapfigure}[10]{r}{0.4\textwidth}
\centering
\vspace{-10pt}
\includegraphics[width=0.9\linewidth]{fig/fstext_module.pdf}
\vspace{-10pt}
\caption{The FSText attention of sub-instruction tokens.}
\label{fig:fstextpipline}
\end{wrapfigure}
Specifically, as shown in Figure~\ref{fig:fstextpipline}, we start with a global CLIP text embedding, denoted as $(l, C)$, where $l$ signifies the text sentence length and $C$ is the channel size, we initialize learnable tokens with shape $(n, l, C)$ where $n$ denotes the number of frames. The tokens are fed into the text sequential attention layer to perform self-attention along the $l$ axis. Subsequently, the cross-attention layer employs these learnable tokens as queries and the global text embedding as keys and values, resulting in a one-to-multiple projection from the global text into $n$ time steps. This yields $(n, l, C)$ tokens for $n$ frame containing task instruction information. Finally, the temporal attention layer conducts directed attention along the $n$ axis for each token in the textual sequence, transforming the macro-instruction progress into frame-specific guidance.

After getting $n$ sub-instruction embeddings corresponding to each frame, the next step is to inject this guidance into the diffusion process, which is commonly completed by a cross-attention layer. As shown in Figure~\ref{fig:fseq} (b), different from the existing works~\citep{magicvideo,tuneavideo} that calculate the cross-attention between the global instruction embedding and $n$ frames. In our cross-attention layer, where cross-attention is calculated separately between visual latent vectors and sub-instruction embeddings for each frame, and the results from all frames are then concatenated, an attention mechanism we refer to as frame-individual cross-attention (\textit{fic-attn}).

\paragraph{Initialization~~} We find initialization is critical to FSText decomposer. Especially, the random initialization fails to approximate the distribution of text embeddings in the pretrained T2I model and results in poor performance. To guarantee the sub-instruction embeddings become a close approximation of the CLIP text embedding, we employ an initialization strategy by enforcing the FSText decomposer to be an identity function (Note that this initialization step is completed before the diffusion process. We ablate this design in Section~\ref{sec:ablate:fstext}). It can be achieved by this objective:
\begin{equation}
    L_{\mathrm{identity}} = \|f_\theta(\bm{\tau}(\mathbf{y})) - \bm{\tau}(\mathbf{y})\|^2
\end{equation}

%\subsection{Inflated 3D U-Net with Autoregressive Spatial-Temporal Attention}\label{sec:tempoal}
%We inflate the Text-to-Image (T2I) pre-trained 2D U-Net to our Inflated 3D U-Net as illustrated in Figure~\ref{fig:pipeline} (b). A standard 2D U-Net block of LDMs consists of a series of 2D ResNet blocks and Spatial Attention Blocks including spatial self-attention and cross-attention modules. Similar to~\cite{vdm}, we replace the $3 \times 3$ 2D convolution kernel with a $1 \times 3 \times 3$ 3D convolution kernel with an additional axis of video frames. Additionally, to further boost the performance of capturing the inter-frame dependency, we incorporate temporal attention after every spatial cross-attention layer. In Figure~\ref{fig:tempattn}, we explore various types of temporal attention, including: (1) bi-directional temporal attention~\cite{vdm,makeavideo,imagenvideo}, which employs a full self-attention across all tokens along the temporal dimension; (2) directed temporal attention~\cite{magicvideo}, which uses a masked attention mechanism that follows the direction of the video sequence along the temporal dimension; and (3) autoregressive spatial-temporal attention: a novel technique proposed by us, which uses causal attention to autoregressively generates the frames on both spatial and temporal dimensions by flattening the tokens into a long sequence.

%We empirically observe that the two existing temporal attention layers cannot achieve promising performance on the TVP task. Bi-directional temporal attention tends to neglect the visual content guidance of the reference frames during the generation process (see Section~\ref{sec:ablate:temp}). 
%And the directed temporal attention fails to capture the dependency of nearby spatial regions and thus generates low-quality frames, while it adheres to the temporal sequence constraint.

%To handle the limitations of bi-directional and directed temporal attention, we introduce the Autoregressive Spatial-Temporal Attention (AST-Attn) mechanism shown in Figure~\ref{fig:tempattn}.
%Given $n$ frames video, a video clip is projected into $n\times s$ latent vectors (where $s$ is the length of a latent vector in each frame) by the pre-trained VAE encoder. We flatten the latent vectors of both temporal and spatial dimensions ($n\times s$) into one dimension. 
%Then, AST-Attn performs self-attention on this long sequence with a causal mask that prevents the model from learning from future temporal-spatial tokens. Because it performs in both spatial and temporal spaces, the frame generation will attend to not only the previous frames but also the nearby spatial regions, which results in high-fidelity generation performance.
%While the calculation of Autoregressive Spatial-Temporal Attention (AST-Attn) involves both temporal and spatial dimensions, its computational complexity remains manageable due to the design of the Inflated 3D U-Net, which maintains the complexity of spatial compression rates and channel depths within a controllable range. Specifically, in the AST-Attn layers of Inflated 3D U-Net, higher-resolution features have more spatial tokens but are computed in lower embedding dimensions, while lower-resolution features have fewer spatial tokens but are computed in higher embedding dimensions. In practice, we observe that adopting AST-Attn has only a $0.4\%$ computation speed lag compared to directed and bidirectional temporal attention.

%By incorporating Autoregressive Spatial-Temporal Attention in the Inflated 3D U-Net, we can generate high-fidelity and coherent video frames with minimal fine-tuning. Specifically, we merely fine-tune the proposed autoregressive spatial-temporal attention layers and freeze the rest of the pre-trained layers in our Inflated 3D U-Net. 
\iffalse
\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{fig/inflate_unet.pdf}
\caption{The overview of inflated 3D U-Net, we inflate the pre-trained T2I latent diffusion model (LDM) by expanding the 2D Conv kernel to 3D kernels and connecting the cross-attention layer with the trainable causal temporal attention layer.}
\label{fig:inflate3d}
\end{figure}
\fi



