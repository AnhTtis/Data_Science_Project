
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}
\usepackage{graphicx} % add: graphics
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{xcolor, colortbl}
\definecolor{baselinecolor}{gray}{.9}
\newcommand{\baseline}[1]{\cellcolor{baselinecolor}{#1}}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{Seer: Language Instructed Video Prediction with Latent Diffusion Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Xianfan Gu\textsuperscript{\textdagger}\quad  Chuan Wen\textsuperscript{$\ddagger$}\textsuperscript{\textdagger}\textsuperscript{$\diamondsuit$}\quad Weirui Ye \textsuperscript{$\ddagger$}\textsuperscript{\textdagger}\textsuperscript{$\diamondsuit$}\quad Jiaming Song\textsuperscript{$\spadesuit$}\quad Yang Gao\textsuperscript{$\ddagger$}\textsuperscript{\textdagger}\textsuperscript{$\diamondsuit$}\\
\textsuperscript{\textdagger}Shanghai Qi Zhi Institute\quad \textsuperscript{$\ddagger$} IIIS, Tsinghua University\quad \textsuperscript{$\diamondsuit$}Shanghai AI Lab\quad \textsuperscript{$\spadesuit$}NVIDIA\\
\texttt{guxf@sqz.ac.cn}\quad\texttt{\{cwen20,ywr20,gaoyangiiis\}@mails.tsinghua.edu.cn} \\
\texttt{jiamings@nvidia.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\authnote}[2]{$\ll$\textsf{\footnotesize #1 notes: #2}$\gg$}
\newcommand{\gu}[1]{{\color{orange}\authnote{Gu}{#1}}}
\newcommand{\yang}[1]{{\color{blue}\authnote{Yang}{#1}}}
\newcommand{\chuan}[1]{{\color{red}\authnote{Chuan}{#1}}}
\newcommand{\js}[1]{{\color{teal}[Jiaming: #1]}}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Imagining the future trajectory is the key for robots to make sound planning and successfully reach their goals. Therefore, text-conditioned video prediction (TVP) is an essential task to facilitate general robot policy learning.
To tackle this task and empower robots with the ability to foresee the future, we propose a sample and computation-efficient model, named \textbf{Seer}, by inflating the pretrained text-to-image (T2I) stable diffusion models along the temporal axis. We enhance the U-Net and language conditioning model by incorporating computation-efficient spatial-temporal attention. Furthermore, we introduce a novel Frame Sequential Text Decomposer module that dissects a sentence's global instruction into temporally aligned sub-instructions, ensuring precise integration into each frame of generation. Our framework allows us to effectively leverage the extensive prior knowledge embedded in pretrained T2I models across the frames. 
With the adaptable-designed architecture, Seer makes it possible to generate high-fidelity, coherent, and instruction-aligned video frames by fine-tuning a few layers on a small amount of data. The experimental results on Something Something V2 (SSv2), Bridgedata and EpicKitchens-100 datasets demonstrate our superior video prediction performance with around 480-GPU hours versus CogVideo with over 12,480-GPU hours: achieving the 31\% FVD improvement compared to the current SOTA model on SSv2 and 83.7\% average preference in the human evaluation.


%\url{https://seervideodiffusion.github.io/}
%Imagining the future trajectory is the key for robots to make sound planning and successfully reach their goals. Therefore, text-conditioned video prediction (TVP) is an essential task to facilitate general robot policy learning, i.e., predicting future video frames with a given language instruction and reference frames. It is a highly challenging task to ground task-level goals specified by instructions and high-fidelity frames together, requiring large-scale data and computation. 
%Additionally, we introduce the Frame Sequential Text Decomposer, a novel module aimed at representing the global text instruction as a temporally aligned sub-instruction to provide controllable guidance.
\end{abstract}

\input{01-intro}
\input{02-relatedwork}
\input{03-method}
\input{04-exp}
\input{05-conclusion}
\section{Reproducibility Statement}
The main implementations of our proposed method are in Section~\ref{sec:impl} and ~\ref{sec:dataset}. In addition, the settings of the
experiments and hyper-parameters we choose are in Appendix~\ref{appendix:sec:impl}. And the implementation details are in Appendix~\ref{sec:appendix:impl_base}.
\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}
\newpage
\appendix
\section{Appendix}
\input{appendix}

\end{document}
