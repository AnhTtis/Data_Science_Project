\subsection{Data on Exposure}

We define exposure given an LLM or system as whether access to the LLM or system would reduce the time it takes a human to complete the task by half.

To evaluate the exposure of DWAs and Tasks to LLM capabilities, we used the following guidelines:

\begin{outline}[enumerate]
\1 No exposure (E0): Label activities/tasks as E0 if any reduction in the time it would take to complete the activity/task with equivalent quality is minimal and/or using any combination of the capabilities described in the below criteria would decrease the quality of the activity/task output.
\1 Direct exposure (E1): Label activities/tasks E1 if direct access to ChatGPT or OpenAI playground alone can reduce the time it takes to complete the activity/task with equivalent quality by at least half.
\1 Exposure by LLM-powered applications (E2): Label activities/tasks E2 if having access to the LLM alone would not reduce the time it takes to complete the activity/task by at least half, but additional software could be built on too the LLM that could reduce the time it takes to complete the specific activity/task with equivalent quality by at least half. These systems include modular access to image generation systems.\footnote{In practice, as you will see in the taxonomy, we separated out image capabilities during the labeling exercise but then combined these classifications for the analysis.}
\end{outline}

The threshold for exposure was set at a perceived 50\%\ possible reduction in time taken to complete a given task/activity while holding quality constant. While this threshold is somewhat arbitrary, its use was largely motivated by its ease of interpretation for external reviewers as opposed to assigning a continuous value as an exposure score or thinking about more specific classes of potential impacts than a binary designation for either above or below a 50\%\ reduction. 

\subsubsection{Human reviewers}

Labelers included the authors, external reviewers who have significant experience reviewing LLM outputs as part of OpenAI's alignment work \cite{ouyang2022training}, and additional OpenAI researchers. 

We use several measures to score DWA and task-level exposure to LLMs. We develop a rubric to distinguish between levels of exposure to a theoretical LLM, and apply it to each DWA and a subset of all tasks (including those with no associated DWAs) and then aggregate those DWA scores to the task and occupation level. We then sourced human labelers who are themselves quite familiar with the capabilities of LLMs from their work in our human labeling processes \cite{ouyang2022training}. The authors labeled activities that clearly required a high degree of physicality or manual dexterity, the external labelers then labeled the remaining activities, and some tasks as described above. Throughout this paper we refer to this set of labels as the ``Human 2023 Augmentation" labels.

Rewrite this section of my paper to fix anything that is unclear, keep it in latex format though.

To evaluate the exposure of DWAs and tasks, we developed a rubric to distinguish between levels of exposure to a theoretical LLM.  

We refer to this set of labels as the ``Human 2023 Augmentation" labels throughout this paper. 

\begin{outline}[enumerate]
\1 No exposure (E0): Label activities/tasks as E0 if any reduction in the time it would take to complete the activity/task with equivalent quality is minimal and/or using any combination of the capabilities described in the below criteria would decrease the quality of the activity/task output.
\1 Direct exposure (E1): Label activities/tasks E1 if direct access to ChatGPT or OpenAI playground alone can reduce the time it takes to complete the activity/task with equivalent quality by at least half.
\1 Exposure by LLM-powered applications (E2): Label activities/tasks E2 if having access to the LLM alone would not reduce the time it takes to complete the activity/task by at least half, but additional software could be built on too the LLM that could reduce the time it takes to complete the specific activity/task with equivalent quality by at least half. These systems include modular access to image generation systems.\footnote{In practice, as you will see in the taxonomy, we separated out image capabilities during the labeling exercise but then combined these classifications for the analysis.}
\end{outline}

The threshold for exposure was set at a perceived 50\%\ possible reduction in time taken to complete a given task/activity while holding quality constant. While this threshold is somewhat arbitrary, its use was largely motivated by its ease of interpretation for external reviewers as opposed to assigning a continuous value as an exposure score or thinking about more specific classes of potential impacts than a binary designation for either above or below a 50\%\ reduction. 

\subsubsection{Human reviewers}

Labelers included the authors, external reviewers who have significant experience reviewing LLM outputs as part of OpenAI's alignment work \cite{ouyang2022training}, and additional OpenAI researchers. 

We use several measures to score DWA and task-level exposure to LLMs. We develop a rubric to distinguish between levels of exposure to a theoretical LLM, and apply it to each DWA and a subset of all tasks (including those with no associated DWAs) and then aggregate those DWA scores to the task and occupation level. We then sourced human labelers who are themselves quite familiar with the capabilities of LLMs from their work in our human labeling processes \cite{ouyang2022training}. The authors labeled activities that clearly required a high degree of physicality or manual dexterity, the external labelers then labeled the remaining activities, and some tasks as described above. Throughout this paper we refer to this set of labels as the ``Human 2023 Augmentation" labels.


\subsubsection{External Reviewer Scoring of DWA and Task Exposure to a Theoretical LLM}

We aggregate up to task-level exposure scores by collecting human labels on the following:
\begin{itemize}
    \item All DWAs
    \item All tasks that do not have associated DWAs
    \item Some additional tasks to validate our method for aggregating across DWAs and to settle some disagreements
\end{itemize}

As an additional method for scoring potential exposure to a theoretical LLM, we prompted (model) to evaluate each task-occupation pair based on criteria for No exposure (E0), Direct Exposure (E1), Exposure by LLM-powered applications (E2), and Exposure given image capabilities (E3). We sampled at temperature 0 and moderately tuned the model prompt to reach a reasonable agreement level with a set of human labels. In practice we treat E2 and E3 as a single class that points to the modular potential and future of LLMs.

\subsubsection{Reviewer Scoring of DWA and Task Exposure to LLM Capabilities}

While we found separating out E2 and E3 useful for labeling purposes, for most of our analysis we treat E1, E2, and E3 as a single class -- namely LLMs and the software tools we expect to be built on top of them in the next 2 years.




As an additional measure to compare with the external reviewer and LLM-generated exposure scores, we also hand-labeled each unique ONET task with a relevance indicator for two specific LLMs â€“ Codex and GPT-3. Using a simple set of criteria for each model (see Appendix ZZ), we marked a binary indicator of model relevance for each task. Further, we identified a set of keywords associated with tasks that GPT-3 and Codex can be used to perform, marking the task as relevant to the model if any of the keywords appear in the task description. The keywords for codex relevance were all forms of the words: code, program, software, and debug. GPT-3 keywords were all forms of: write, edit, summarize, and report. The results from these labels by task and occupation are presented in the Results section as robustness checks to our other measures.


\textbf{LLM Assessment of Task-Level Automation Potential of Generative Models}

While our core measures of LLM exposure presented in this paper focus on the impact of a theoretical LLM on the time it takes to complete a task, it is also important to understand the future potential for multi-modal generative models to fully automate tasks with high quality. To assess this forward-looking automation potential, we developed a simple 5-point taxonomy to measure the task-level automation potential of generative AI models. We focus here broadly on generative models rather than text-only LLMs in order to include an assessment of the automation potential of models with some level of vision and image generation capabilities. We used a recently trained OpenAI model specially tuned for classification to score each task-occupation pair based on the five-point automation taxonomy below (see Appendix XX for the full prompt):

\begin{outline}[enumerate]
\1 No Automation Exposure (T0): A class of tasks for which a generative model cannot conceivably perform any aspect of the task in any manner.
\1 Low Automation Exposure (T1): A class of tasks where, in most contexts in which this task is currently performed by a human, a generative model could complete 0\%--50\%\ of the components of the task at high quality.
\1 Moderate Automation Exposure (T2): A class of tasks where, in most contexts in which this task is currently performed by a human, a generative model could complete 50\%--90\%\ of the components of the task at high quality.
\1 High Automation Exposure (T3): A class of tasks where, in most contexts in which this task is currently performed by a human, a generative model could complete 90\%--100\%\ of the components of the task when prompted, but the output requires oversight from a human for use in the given task.
\1 Full Automation Exposure (T4): A class of tasks where, in most contexts in which this task is currently performed by a human, a generative model could complete all aspects of this task with high quality when prompted by a human. The output does not normally require oversight by a human for use in this task. 
\end{outline}

As a basic validity check, the authors hand-labeled a set of 100 task-occupation pairs using an identical taxonomy and compared these labels to the model outputs. The overall agreement rate was 65\%\ and the Cohen's kappa was .538. 

\begin{comment}
\textbf{LLM Assessment of Task-Level Skill Requirements}
\end{comment}


To assess consistency within and across the various methods presented above we checked the levels of agreement across multiple labeling sources within a labeling taxonomy by calculating the Cohen's Kappa. Cohen's Kappa is a measure of inter-rater reliability that accounts for chance agreement between two raters and therefore sheds light on the extent to which labeling outputs are driven by the raters themselves as opposed to the labeling criteria. \cite{cohen1960coefficient, mchugh2012interrater}

\textbf{GPT-3 and Codex Relevance Scores by Task}

\textbf{Internal Agreement Across Exposure Scores}

