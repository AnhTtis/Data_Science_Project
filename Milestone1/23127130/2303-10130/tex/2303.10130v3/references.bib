@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}

@techreport{gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  institution={OpenAI},
  year={2023},
  url={https://cdn.openai.com/papers/gpt-4.pdf}
}

@article{argyle2022out,
  title={Out of One, Many: Using Language Models to Simulate Human Samples},
  author={Argyle, Lisa P and Busby, Ethan C and Fulda, Nancy and Gubler, Joshua and Rytting, Christopher and Wingate, David},
  journal={arXiv preprint arXiv:2209.06899},
  year={2022}
}

@article{horton2023large,
  title={Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?},
  author={Horton, John J},
  journal={arXiv preprint arXiv:2301.07543},
  year={2023}
}

@article{brynjolfssonQuantifyingDistributionMachine2023,
  title = {Quantifying the {{Distribution}} of {{Machine Learning}}'s {{Impact}} on {{Work}}},
  author = {Brynjolfsson, Erik and Frank, Morgan R. and Mitchell, Tom and Rahwan, Iyad and Rock, Daniel},
  journal={Forthcoming},
  year = {2023}
}

@book{baumol2012cost,
  title={The cost disease: Why computers get cheaper and health care doesn't},
  author={Baumol, William J},
  year={2012},
  publisher={Yale university press}
}

@incollection{acemoglu2011skills,
  title={Skills, tasks and technologies: Implications for employment and earnings},
  author={Acemoglu, Daron and Autor, David},
  booktitle={Handbook of labor economics},
  volume={4},
  pages={1043--1171},
  year={2011},
  publisher={Elsevier}
}

@misc{bresnahan2019artificial,
  title={Artificial intelligence technologies and aggregate growth prospects},
  author={Bresnahan, Timothy},
  year={2019}
}

@article{bresnahan1995general,
  title={General purpose technologies ‘Engines of growth’?},
  author={Bresnahan, Timothy F and Trajtenberg, Manuel},
  journal={Journal of econometrics},
  volume={65},
  number={1},
  pages={83--108},
  year={1995},
  publisher={Elsevier}
}

@book{lipsey2005economic,
  title={Economic transformations: general purpose technologies and long-term economic growth},
  author={Lipsey, Richard G and Carlaw, Kenneth I and Bekar, Clifford T},
  year={2005},
  publisher={Oup Oxford}
}

@article{katz1992changes,
  title={Changes in relative wages, 1963--1987: supply and demand factors},
  author={Katz, Lawrence F and Murphy, Kevin M},
  journal={The quarterly journal of economics},
  volume={107},
  number={1},
  pages={35--78},
  year={1992},
  publisher={MIT Press}
}

@incollection{korinek2018artificial,
  title={Artificial intelligence and its implications for income distribution and unemployment},
  author={Korinek, Anton and Stiglitz, Joseph E},
  booktitle={The economics of artificial intelligence: An agenda},
  pages={349--390},
  year={2018},
  publisher={University of Chicago Press}
}

@misc{onet272,
title={O*NET 27.2 Database},
author={O*NET},
year={2023},
publisher={National Center for O*NET Development},
url={www.onetcenter.org/database.html},
}

@incollection{aghion2018artificial,
  title={Artificial intelligence and economic growth},
  author={Aghion, Philippe and Jones, Benjamin F and Jones, Charles I},
  booktitle={The economics of artificial intelligence: An agenda},
  pages={237--282},
  year={2018},
  publisher={University of Chicago Press}
}

@article{huang2018artificial,
  title={Artificial intelligence in service},
  author={Huang, Ming-Hui and Rust, Roland T},
  journal={Journal of service research},
  volume={21},
  number={2},
  pages={155--172},
  year={2018},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{manning2022research,
  title={A Research Agenda for Assessing the Economic Impacts of Code Generation Models},
  author={Manning, Sam and Mishkin, Pamela and Hadfield, Gillian and Eloundou, Tyna and Eisner, Emily},
  year={2022}
}

@article{rock2019engineering,
  title={Engineering value: The returns to technological talent and investments in artificial intelligence},
  author={Rock, Daniel},
  journal={Available at SSRN 3427412},
  year={2019}
}

@techreport{autor2022new,
  title={New Frontiers: The Origins and Content of New Work, 1940--2018},
  author={Autor, David and Chin, Caroline and Salomons, Anna M and Seegmiller, Bryan},
  year={2022},
  institution={National Bureau of Economic Research}
}

@techreport{feigenbaum2021organizational,
  title={Organizational Frictions and Increasing Returns to Automation: Lessons from AT\&T in the Twentieth Century},
  author={Feigenbaum, James J and Gross, Daniel P},
  year={2021},
  institution={National Bureau of Economic Research}
}

@incollection{cockburn2018impact,
  title={The impact of artificial intelligence on innovation: An exploratory analysis},
  author={Cockburn, Iain M and Henderson, Rebecca and Stern, Scott},
  booktitle={The economics of artificial intelligence: An agenda},
  pages={115--146},
  year={2018},
  publisher={University of Chicago Press}
}

@article{dixon2021robot,
  title={The robot revolution: Managerial and employment consequences for firms},
  author={Dixon, Jay and Hong, Bryan and Wu, Lynn},
  journal={Management Science},
  volume={67},
  number={9},
  pages={5586--5605},
  year={2021},
  publisher={INFORMS}
}

@incollection{frey2019technology,
  title={The technology trap},
  author={Frey, Carl Benedikt},
  booktitle={The Technology Trap},
  year={2019},
  publisher={Princeton University Press}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{mollick2022new,
  title={New Modes of Learning Enabled by AI Chatbots: Three Methods and Assignments},
  author={Mollick, Ethan R and Mollick, Lilach},
  journal={Available at SSRN},
  year={2022}
}

@techreport{zolas2021advanced,
  title={Advanced technologies adoption and use by US firms: Evidence from the annual business survey},
  author={Zolas, Nikolas and Kroff, Zachary and Brynjolfsson, Erik and McElheran, Kristina and Beede, David N and Buffington, Cathy and Goldschlag, Nathan and Foster, Lucia and Dinlersoz, Emin},
  year={2021},
  institution={National Bureau of Economic Research}
}

@techreport{acemoglu2020ai,
  title={AI and jobs: Evidence from online vacancies},
  author={Acemoglu, Daron and Autor, David and Hazell, Jonathon and Restrepo, Pascual},
  year={2020},
  institution={National Bureau of Economic Research}
}

@article{meindl2021exposure,
  title={Exposure of occupations to technologies of the fourth industrial revolution},
  author={Meindl, Benjamin and Frank, Morgan R and Mendon{\c{c}}a, Joana},
  journal={arXiv preprint arXiv:2110.13317},
  year={2021}
}

@article{frank2019toward,
  title={Toward understanding the impact of artificial intelligence on labor},
  author={Frank, Morgan R and Autor, David and Bessen, James E and Brynjolfsson, Erik and Cebrian, Manuel and Deming, David J and Feldman, Maryann and Groh, Matthew and Lobo, Jos{\'e} and Moro, Esteban and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={14},
  pages={6531--6539},
  year={2019},
  publisher={National Acad Sciences}
}

@incollection{bessen2018artificial,
  title={Artificial intelligence and jobs: The role of demand},
  author={Bessen, James},
  booktitle={The economics of artificial intelligence: an agenda},
  pages={291--307},
  year={2018},
  publisher={University of Chicago Press}
}

@article{peng2023impact,
  title={The Impact of AI on Developer Productivity: Evidence from GitHub Copilot},
  author={Peng, Sida and Kalliamvakou, Eirini and Cihon, Peter and Demirer, Mert},
  journal={arXiv preprint arXiv:2302.06590},
  year={2023}
}

@article{noy2023experimental,
  title={Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence},
  author={Noy, Shakked and Zhang, Whitney},
  journal={Available at SSRN 4375283},
  year={2023}
}

@article{babina2021artificial,
  title={Artificial intelligence, firm growth, and product innovation},
  author={Babina, Tania and Fedyk, Anastassia and He, Alex and Hodson, James},
  journal={Firm Growth, and Product Innovation (November 9, 2021)},
  year={2021}
}

@article{cheng2022innovae,
  title={InnoVAE: Generative AI for Understanding Patents and Innovation},
  author={Cheng, Zhaoqi and Lee, Dokyun and Tambe, Prasanna},
  journal={Available at SSRN},
  year={2022}
}

@article{bresnahan2002information,
  title={Information technology, workplace organization, and the demand for skilled labor: Firm-level evidence},
  author={Bresnahan, Timothy F and Brynjolfsson, Erik and Hitt, Lorin M},
  journal={The quarterly journal of economics},
  volume={117},
  number={1},
  pages={339--376},
  year={2002},
  publisher={MIT Press}
}

@article{david1990dynamo,
  title={The dynamo and the computer: an historical perspective on the modern productivity paradox},
  author={David, Paul A},
  journal={The American Economic Review},
  volume={80},
  number={2},
  pages={355--361},
  year={1990},
  publisher={JSTOR}
}

@article{bresnahan1999computerisation,
  title={Computerisation and wage dispersion: an analytical reinterpretation},
  author={Bresnahan, Timothy F},
  journal={The economic journal},
  volume={109},
  number={456},
  pages={390--415},
  year={1999},
  publisher={Oxford University Press Oxford, UK}
}

@article{bresnahan1996technical,
  title={Technical progress and co-invention in computing and in the uses of computers},
  author={Bresnahan, Timothy and Greenstein, Shane and Brownstone, David and Flamm, Ken},
  journal={Brookings Papers on Economic Activity. Microeconomics},
  volume={1996},
  pages={1--83},
  year={1996},
  publisher={JSTOR}
}

@article{arntz2017revisiting,
  title={Revisiting the risk of automation},
  author={Arntz, Melanie and Gregory, Terry and Zierahn, Ulrich},
  journal={Economics Letters},
  volume={159},
  pages={157--160},
  year={2017},
  publisher={Elsevier}
}

@article{grace2018will,
  title={When will AI exceed human performance? Evidence from AI experts},
  author={Grace, Katja and Salvatier, John and Dafoe, Allan and Zhang, Baobao and Evans, Owain},
  journal={Journal of Artificial Intelligence Research},
  volume={62},
  pages={729--754},
  year={2018}
}

@article{van2011wage,
  title={Wage inequality, technology and trade: 21st century evidence},
  author={Van Reenen, John},
  journal={Labour economics},
  volume={18},
  number={6},
  pages={730--741},
  year={2011},
  publisher={Elsevier}
}

@article{acemoglu2022tasks,
  title={Tasks, automation, and the rise in US wage inequality},
  author={Acemoglu, Daron and Restrepo, Pascual},
  journal={Econometrica},
  volume={90},
  number={5},
  pages={1973--2016},
  year={2022},
  publisher={Wiley Online Library}
}

@article{autor2006polarization,
  title={The polarization of the US labor market},
  author={Autor, David H and Katz, Lawrence F and Kearney, Melissa S},
  journal={American economic review},
  volume={96},
  number={2},
  pages={189--194},
  year={2006},
  publisher={American Economic Association}
}

@article{acemoglu2019automation,
  title={Automation and new tasks: How technology displaces and reinstates labor},
  author={Acemoglu, Daron and Restrepo, Pascual},
  journal={Journal of Economic Perspectives},
  volume={33},
  number={2},
  pages={3--30},
  year={2019},
  publisher={American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203-2418}
}

@article{autor2003skill,
  title={The skill content of recent technological change: An empirical exploration},
  author={Autor, David H and Levy, Frank and Murnane, Richard J},
  journal={The Quarterly journal of economics},
  volume={118},
  number={4},
  pages={1279--1333},
  year={2003},
  publisher={MIT Press}
}

@article{atalay2020evolution,
  title={The evolution of work in the United States},
  author={Atalay, Enghin and Phongthiengtham, Phai and Sotelo, Sebastian and Tannenbaum, Daniel},
  journal={American Economic Journal: Applied Economics},
  volume={12},
  number={2},
  pages={1--34},
  year={2020},
  publisher={American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203-2425}
}

@article{brynjolfsson2021productivity,
  title={The productivity J-curve: How intangibles complement general purpose technologies},
  author={Brynjolfsson, Erik and Rock, Daniel and Syverson, Chad},
  journal={American Economic Journal: Macroeconomics},
  volume={13},
  number={1},
  pages={333--72},
  year={2021}
}

@article{acemoglu2018race,
  title={The race between man and machine: Implications of technology for growth, factor shares, and employment},
  author={Acemoglu, Daron and Restrepo, Pascual},
  journal={American economic review},
  volume={108},
  number={6},
  pages={1488--1542},
  year={2018},
  publisher={American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203}
}

@article{acemoglu2020wrong,
  title={The wrong kind of AI? Artificial intelligence and the future of labour demand},
  author={Acemoglu, Daron and Restrepo, Pascual},
  journal={Cambridge Journal of Regions, Economy and Society},
  volume={13},
  number={1},
  pages={25--35},
  year={2020},
  publisher={Oxford University Press UK}
}

@article{acemoglu2022artificial,
  title={Artificial intelligence and jobs: Evidence from online vacancies},
  author={Acemoglu, Daron and Autor, David and Hazell, Jonathon and Restrepo, Pascual},
  journal={Journal of Labor Economics},
  volume={40},
  number={S1},
  pages={S293--S340},
  year={2022},
  publisher={The University of Chicago Press Chicago, IL}
}


@misc{bls_crosswalk,
title={Occupational Outlook Handbook A-Z index},
author={BLS},
year={2023},
publisher={US Bureau of Labor Statistics},
url={https://www.bls.gov/emp/classifications-crosswalks/ooh_az_index.xlsx}}

@misc{bls_demographics,
title={Demographic Characteristics (CPS)},
author={BLS},
year={2023},
publisher={US Bureau of Labor Statistics},
url={https://www.bls.gov/cps/demographics.htm}}

@article{agrawal2019exploring,
  title={Exploring the impact of artificial intelligence: Prediction versus judgment},
  author={Agrawal, Ajay and Gans, Joshua S and Goldfarb, Avi},
  journal={Information Economics and Policy},
  volume={47},
  pages={1--6},
  year={2019},
  publisher={Elsevier}
}

@misc{bls_employment_occupation,
title={Employment by detailed occupation},
author={BLS},
year={2022},
publisher={US Bureau of Labor Statistics},
url={https://www.bls.gov/emp/tables/emp-by-detailed-occupation.htm}}

@article{goldfarb2023could,
  title={Could machine learning be a general purpose technology? A comparison of emerging technologies using data from online job postings},
  author={Goldfarb, Avi and Taska, Bledi and Teodoridis, Florenta},
  journal={Research Policy},
  volume={52},
  number={1},
  pages={104653},
  year={2023},
  publisher={Elsevier}
}

@techreport{agrawal2021ai,
  title={AI adoption and system-wide change},
  author={Agrawal, Ajay K and Gans, Joshua S and Goldfarb, Avi},
  year={2021},
  institution={National Bureau of Economic Research}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{brynjolfsson2022turing,
  title={The turing trap: The promise \& peril of human-like artificial intelligence},
  author={Brynjolfsson, Erik},
  journal={Daedalus},
  volume={151},
  number={2},
  pages={272--287},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


@article{schick2023toolformer,
  title={Toolformer: Language Models Can Teach Themselves to Use Tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{hernandez2021scaling,
  title={Scaling laws for transfer},
  author={Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},
  journal={arXiv preprint arXiv:2102.01293},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{zhou2022learning,
  title={Learning to prompt for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  journal={International Journal of Computer Vision},
  volume={130},
  number={9},
  pages={2337--2348},
  year={2022},
  publisher={Springer}
}

 @article{somers_2023, title={Whispers of AI's Modular Future}, url={https://www.newyorker.com/tech/annals-of-technology/whispers-of-ais-modular-future}, journal={The New Yorker}, author={Somers, James}, year={2023}, month={Feb}} 

 @article{radford2022robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2212.04356},
  year={2022}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@article{mialon2023augmented,
  title={Augmented Language Models: a Survey},
  author={Mialon, Gr{\'e}goire and Dess{\`\i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and others},
  journal={arXiv preprint arXiv:2302.07842},
  year={2023}
}

@article{agrawal_similarities_nodate,
	title = {Similarities and {Differences} in the {Adoption} of {General} {Purpose} {Technologies}},
	abstract = {Economic models provide little insight into when the next big idea and its associated productivity dividend will come along. Once a general purpose technology (GPT) is identified, the economist’s toolkit does provide an understanding when firms will adopt a new technology and for what purpose. The focus of the literature has been on commonalities across each type of GPT. This focus is natural, given that the goal of the literature has been to identify generalizable insights across technologies. Broadly, this literature emphasizes heterogeneity in co-invention costs across firms. Each GPT, however, provides a distinct benefit. Steam provided a new power source. The internet facilitated communication. The differences between GPTs are important for understanding adoption patterns. Using the examples of the internet and artificial intelligence, we discuss how both co-invention costs and distinct benefits determine the adoption of technology. For both technologies, we demonstrate that discussions of the impact of a GPT on productivity and growth need to emphasize the benefits as well as the costs. The goal of this paper is therefore to link the literature on co-invention costs with an understanding of the distinct benefits of each GPT.},
	language = {en},
	author = {Agrawal, Ajay K and Gans, Joshua S and Goldfarb, Avi},
	file = {Agrawal et al. - Similarities and Differences in the Adoption of Ge.pdf:/Users/sam.manning/Zotero/storage/EAIGKVVV/Agrawal et al. - Similarities and Differences in the Adoption of Ge.pdf:application/pdf},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	language = {en},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:/Users/sam.manning/Zotero/storage/WKALZATM/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf},
}

@misc{smith_using_2022,
	title = {Using {DeepSpeed} and {Megatron} to {Train} {Megatron}-{Turing} {NLG} {530B}, {A} {Large}-{Scale} {Generative} {Language} {Model}},
	url = {http://arxiv.org/abs/2201.11990},
	abstract = {Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and Zhang, Elton and Child, Rewon and Aminabadi, Reza Yazdani and Bernauer, Julie and Song, Xia and Shoeybi, Mohammad and He, Yuxiong and Houston, Michael and Tiwary, Saurabh and Catanzaro, Bryan},
	month = feb,
	year = {2022},
	note = {arXiv:2201.11990 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sam.manning/Zotero/storage/D3FEZ3F9/Smith et al. - 2022 - Using DeepSpeed and Megatron to Train Megatron-Tur.pdf:application/pdf;arXiv.org Snapshot:/Users/sam.manning/Zotero/storage/28IPLD3A/2201.html:text/html},
}

@misc{rae_scaling_2022,
	title = {Scaling {Language} {Models}: {Methods}, {Analysis} \& {Insights} from {Training} {Gopher}},
	shorttitle = {Scaling {Language} {Models}},
	url = {http://arxiv.org/abs/2112.11446},
	abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and Driessche, George van den and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and d'Autume, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
	month = jan,
	year = {2022},
	note = {arXiv:2112.11446 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 120 pages},
	file = {arXiv Fulltext PDF:/Users/sam.manning/Zotero/storage/D4WH4LDH/Rae et al. - 2022 - Scaling Language Models Methods, Analysis & Insig.pdf:application/pdf;arXiv.org Snapshot:/Users/sam.manning/Zotero/storage/2HAJKQMX/2112.html:text/html},
}

@misc{thoppilan_lamda_2022,
	title = {{LaMDA}: {Language} {Models} for {Dialog} {Applications}},
	shorttitle = {{LaMDA}},
	url = {http://arxiv.org/abs/2201.08239},
	abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
	month = feb,
	year = {2022},
	note = {arXiv:2201.08239 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sam.manning/Zotero/storage/PLJAHZZK/Thoppilan et al. - 2022 - LaMDA Language Models for Dialog Applications.pdf:application/pdf;arXiv.org Snapshot:/Users/sam.manning/Zotero/storage/E34A7VZH/2201.html:text/html},
}

@article{bai_constitutional_nodate,
	title = {Constitutional {AI}: {Harmlessness} from {AI} {Feedback}},
	abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through selfimprovement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as ‘Constitutional AI’. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then ﬁnetune the original model on revised responses. In the RL phase, we sample from the ﬁnetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use ‘RL from AI Feedback’ (RLAIF). As a result we are able to train a harmless but nonevasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
	language = {en},
	author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R and Hatﬁeld-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
	file = {Bai et al. - Constitutional AI Harmlessness from AI Feedback.pdf:/Users/sam.manning/Zotero/storage/KN4I6ZFB/Bai et al. - Constitutional AI Harmlessness from AI Feedback.pdf:application/pdf},
}

@article{acemoglu2022demographics,
  title={Demographics and automation},
  author={Acemoglu, Daron and Restrepo, Pascual},
  journal={The Review of Economic Studies},
  volume={89},
  number={1},
  pages={1--44},
  year={2022},
  publisher={Oxford University Press}
}

@misc{bai_training_2022,
	title = {Training a {Helpful} and {Harmless} {Assistant} with {Reinforcement} {Learning} from {Human} {Feedback}},
	url = {http://arxiv.org/abs/2204.05862},
	abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	month = apr,
	year = {2022},
	note = {arXiv:2204.05862 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Data available at https://github.com/anthropics/hh-rlhf},
	file = {arXiv Fulltext PDF:/Users/sam.manning/Zotero/storage/BZBFE3ZY/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf:application/pdf;arXiv.org Snapshot:/Users/sam.manning/Zotero/storage/JUGYSCCY/2204.html:text/html},
}

@misc{ganguli_capacity_2023,
	title = {The {Capacity} for {Moral} {Self}-{Correction} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2302.07459},
	abstract = {We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to "morally self-correct" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Ganguli, Deep and Askell, Amanda and Schiefer, Nicholas and Liao, Thomas I. and Lukošiūtė, Kamilė and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and Olsson, Catherine and Hernandez, Danny and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Landau, Joshua and Ndousse, Kamal and Nguyen, Karina and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Mercado, Noemi and DasSarma, Nova and Rausch, Oliver and Lasenby, Robert and Larson, Robin and Ringer, Sam and Kundu, Sandipan and Kadavath, Saurav and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Olah, Christopher and Clark, Jack and Bowman, Samuel R. and Kaplan, Jared},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07459 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sam.manning/Zotero/storage/AVT72M9W/Ganguli et al. - 2023 - The Capacity for Moral Self-Correction in Large La.pdf:application/pdf;arXiv.org Snapshot:/Users/sam.manning/Zotero/storage/GEBQH3NV/2302.html:text/html},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sam.manning/Zotero/storage/PHQLML3Q/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;arXiv.org Snapshot:/Users/sam.manning/Zotero/storage/LIT7LTCH/2201.html:text/html},
}

@misc{nye_show_2021,
	title = {Show {Your} {Work}: {Scratchpads} for {Intermediate} {Computation} with {Language} {Models}},
	shorttitle = {Show {Your} {Work}},
	url = {http://arxiv.org/abs/2112.00114},
	abstract = {Large pre-trained language models perform remarkably well on tasks that can be done "in one pass", such as generating realistic text or synthesizing computer programs. However, they struggle with tasks that require unbounded multi-step computation, such as adding integers or executing programs. Surprisingly, we find that these same models are able to perform complex multi-step computations -- even in the few-shot regime -- when asked to perform the operation "step by step", showing the results of intermediate computations. In particular, we train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a "scratchpad". On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
	month = nov,
	year = {2021},
	note = {arXiv:2112.00114 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/sam.manning/Zotero/storage/2QL3ZYKS/Nye et al. - 2021 - Show Your Work Scratchpads for Intermediate Compu.pdf:application/pdf;arXiv.org Snapshot:/Users/sam.manning/Zotero/storage/RCG3YD5I/2112.html:text/html},
}

@misc{kojima_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = jan,
	year = {2023},
	note = {arXiv:2205.11916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to NeurIPS2022. Our code is available at https://github.com/kojima-takeshi188/zero\_shot\_cot},
	file = {arXiv Fulltext PDF:/Users/sam.manning/Zotero/storage/M4HHW2WS/Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf:application/pdf;arXiv.org Snapshot:/Users/sam.manning/Zotero/storage/YCWJSJFB/2205.html:text/html},
}

@misc{langchain,
author = {Chase, Harrison},
month = {10},
title = {{LangChain}},
url = {https://github.com/hwchase17/langchain},
year = {2022}
}

@inproceedings{Singla2015LearningTH,
title={Learning to Hire Teams},
author={Adish Kumar Singla and Eric Horvitz and Pushmeet Kohli and Andreas Krause},
booktitle={AAAI Conference on Human Computation \& Crowdsourcing},
year={2015}
}

@misc{chow_chatgpt_2023,
	title = {Why {ChatGPT} {Is} the {Fastest} {Growing} {Web} {Platform} {Ever} {\textbar} {Time}},
        author = {Chow, Andrew. R.},
	month = feb,
	year = {2023},
	url = {https://time.com/6253615/chatgpt-fastest-growing/},
	urldate = {2023-02-21},
	file = {Why ChatGPT Is the Fastest Growing Web Platform Ever | Time:/Users/sam.manning/Zotero/storage/SUEU77NU/chatgpt-fastest-growing.html:text/html},
}

@misc{goldstein_generative_2023,
  doi = {10.48550/ARXIV.2301.04246},
  
  url = {https://arxiv.org/abs/2301.04246},
  
  author = {Goldstein, Josh A. and Sastry, Girish and Musser, Micah and DiResta, Renee and Gentzel, Matthew and Sedova, Katerina},
  
  keywords = {Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{schramowski_large_2022,
	title = {Large pre-trained language models contain human-like biases of what is right and wrong to do},
	volume = {4},
	issn = {2522-5839},
	url = {https://doi.org/10.1038/s42256-022-00458-8},
	doi = {10.1038/s42256-022-00458-8},
	abstract = {Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, GPT-2 and GPT-3. Using them as pre-trained models and fine-tuning them for specific tasks, researchers have extended the state of the art for many natural language processing tasks and shown that they capture not only linguistic knowledge but also retain general knowledge implicitly present in the data. Unfortunately, LMs trained on unfiltered text corpora suffer from degenerated and biased behaviour. While this is well established, we show here that recent LMs also contain human-like biases of what is right and wrong to do, reflecting existing ethical and moral norms of society. We show that these norms can be captured geometrically by a ‘moral direction’ which can be computed, for example, by a PCA, in the embedding space. The computed ‘moral direction’ can rate the normativity (or non-normativity) of arbitrary phrases without explicitly training the LM for this task, reflecting social norms well. We demonstrate that computing the ’moral direction’ can provide a path for attenuating or even preventing toxic degeneration in LMs, showcasing this capability on the RealToxicityPrompts testbed.},
	number = {3},
	journal = {Nature Machine Intelligence},
	author = {Schramowski, Patrick and Turan, Cigdem and Andersen, Nico and Rothkopf, Constantin A. and Kersting, Kristian},
	month = mar,
	year = {2022},
	pages = {258--268},
}

@inproceedings{abid_persistent_2021,
    author = {Abid, Abubakar and Farooqi, Maheen and Zou, James},
    title = {Persistent Anti-Muslim Bias in Large Language Models},
    year = {2021},
    isbn = {9781450384735},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3461702.3462624},
    doi = {10.1145/3461702.3462624},
    booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
    pages = {298–306},
    numpages = {9},
    keywords = {machine learning, bias, stereotypes, ethics, language models},
    location = {Virtual Event, USA},
    series = {AIES '21}
}

@article{van2023algorithmic,
  title={Algorithmic Writing Assistance on Jobseekers' Resumes Increases Hires},
  author={van Inwegen, Emma and Munyikwa, Zanele and Horton, John J},
  journal={arXiv preprint arXiv:2301.08083},
  year={2023}
}
@inproceedings{Klinova2021,
   abstract = {Future advances in AI that automate away human labor may have stark implications for labor markets and inequality. This paper proposes a framework to analyze the effects of specific types of AI systems on the labor market, based on how much labor demand they will create versus displace, while taking into account that productivity gains also make society wealthier and thereby contribute to additional labor demand. This analysis enables ethically-minded companies creating or deploying AI systems as well as researchers and policymakers to take into account the effects of their actions on labor markets and inequality, and therefore to steer progress in AI in a direction that advances shared prosperity and an inclusive economic future for all of humanity.},
   author = {Katya Klinova and Anton Korinek},
   doi = {10.1145/3461702.3462619},
   booktitle = {AIES 2021 - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
   title = {AI and Shared Prosperity},
   year = {2021},
}
@article{Moll2021,
   abstract = {Over the past forty years, economic growth in the United States has been unevenly distributed: income percentiles corresponding to the lower half of the distribution have stagnated while those at the top have sharply increased. At the same time, the aggregate labor share has fallen and wealth inequality has risen. We study technical change as a candidate cause of these trends. To this end, we develop a tractable theory that links technology to the personal income and wealth distributions, and not just the wage distribution as is commonly done in the existing literature.},
   author = {Benjamin Moll and Lukasz Rachel and Pascual Restrepo},
   doi = {10.2139/ssrn.3801089},
   journal = {SSRN Electronic Journal},
   title = {Uneven Growth: Automation’s Impact on Income and Wealth Inequality},
   year = {2021},
}
@article{Acemoglu2002,
   author = {Daron Acemoglu},
   doi = {10.1257/jel.40.1.7},
   issn = {00220515},
   issue = {1},
   journal = {Journal of Economic Literature},
   title = {Technical Change, Inequality, and the Labor Market},
   volume = {40},
   year = {2002},
}
@article{Acemoglu2022,
   abstract = {We argue theoretically and document empirically that aging leads to greater (industrial) automation, because it creates a shortage of middle-aged workers specializing in manual production tasks. We show that demographic change is associated with greater adoption of robots and other automation technologies across countries and with more robotics-related activities across U.S. commuting zones. We also document more automation innovation in countries undergoing faster aging. Our directed technological change model predicts that the response of automation technologies to aging should be more pronounced in industries that rely more on middle-aged workers and those that present greater opportunities for automation and that productivity should improve and the labor share should decline relatively in industries that are more amenable to automation. The evidence supports all four of these predictions.},
   author = {Daron Acemoglu and Pascual Restrepo},
   doi = {10.1093/restud/rdab031},
   issn = {1467937X},
   issue = {1},
   journal = {Review of Economic Studies},
   title = {Demographics and Automation},
   volume = {89},
   year = {2022},
}
@generic{Acemoglu2019,
   author = {Daron Acemoglu and Pascual Restrepo},
   doi = {10.1257/jep.33.2.3},
   issn = {08953309},
   issue = {2},
   journal = {Journal of Economic Perspectives},
   title = {Automation and new tasks: How technology displaces and reinstates labor},
   volume = {33},
   year = {2019},
}

@article{Shahaf2010GeneralizedTM, 
    title={Generalized Task Markets for Human and Machine Computation}, 
    author={Dafna Shahaf and Eric Horvitz}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    year={2010} 
}

@article{Acemoglu2020,
   abstract = {We study the effects of industrial robots on US labor markets. We show theoretically that robots may reduce employment and wages and that their local impacts can be estimated using variation in exposure to robots-defined from industry-level advances in robotics and local industry employment. We estimate robust negative effects of robots on employment and wages across commuting zones. We also show that areas most exposed to robots after 1990 do not exhibit any differential trends before then, and robots’ impact is distinct from other capital and technologies. One more robot per thousand workers reduces the employmentto-population ratio by 0.2 percentage points and wages by 0.42\%.},
   author = {Daron Acemoglu and Pascual Restrepo},
   doi = {10.1086/705716},
   issn = {1537534X},
   issue = {6},
   journal = {Journal of Political Economy},
   title = {Robots and jobs: Evidence from us labor markets},
   volume = {128},
   year = {2020},
}
@generic{Acemoglu2018,
   abstract = {We examine the concerns that new technologies will render labor redundant in a framework in which tasks previously performed by labor can be automated and new versions of existing tasks, in which labor has a comparative advantage, can be created. In a static version where capital is fixed and technology is exogenous, automation reduces employment and the labor share, and may even reduce wages, while the creation of new tasks has the opposite effects. Our full model endogenizes capital accumulation and the direction of research toward automation and the creation of new tasks. If the long-run rental rate of capital relative to the wage is sufficiently low, the long-run equilibrium involves automation of all tasks. Otherwise, there exists a stable balanced growth path in which the two types of innovations go hand-in-hand. Stability is a consequence of the fact that automation reduces the cost of producing using labor, and thus discourages further automation and encourages the creation of new tasks. In an extension with heterogeneous skills, we show that inequality increases during transitions driven both by faster automation and the introduction of new tasks, and characterize the conditions under which inequality stabilizes in the long run.},
   author = {Daron Acemoglu and Pascual Restrepo},
   doi = {10.1257/aer.20160696},
   issn = {00028282},
   issue = {6},
   journal = {American Economic Review},
   title = {The race between man and machine: Implications of technology for growth, factor shares, and employment},
   volume = {108},
   year = {2018},
}
@article{passi2022overreliance,
  title={Overreliance on AI: Literature review},
  author={Passi, Samir and Vorvoreanu, Mihaela},
  year={2022},
  publisher={Technical Report MSR-TR-2022-12. Microsoft. https://www. microsoft. com/en~…}
}

@article{mchugh2012interrater,
  title={Interrater reliability: the kappa statistic},
  author={McHugh, Mary L},
  journal={Biochemia medica: Biochemia medica},
  volume={22},
  number={3},
  pages={276--282},
  year={2012},
  pmid={23092060},
  pmcid={PMC3900052}
}

@article{cohen1960coefficient,
  title={A coefficient of agreement for nominal scales},
  author={Cohen, Jacob},
  journal={Educational and psychological measurement},
  volume={20},
  number={1},
  pages={37--46},
  year={1960},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{felten2023will,
  title={How will Language Modelers like ChatGPT Affect Occupations and Industries?},
  author={Felten, Ed and Raj, Manav and Seamans, Robert},
  journal={arXiv preprint arXiv:2303.01157},
  year={2023}
}

@article{singhal2022large,
  title={Large Language Models Encode Clinical Knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={arXiv preprint arXiv:2212.13138},
  year={2022}
}

@article{brynjolfsson2017can,
  title={What can machine learning do? Workforce implications},
  author={Brynjolfsson, Erik and Mitchell, Tom},
  journal={Science},
  volume={358},
  number={6370},
  pages={1530--1534},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@techreport{NBERw29552,
 title = "Technology, Vintage-Specific Human Capital, and Labor Displacement: Evidence from Linking Patents with Occupations",
 author = "Kogan, Leonid and Papanikolaou, Dimitris and Schmidt, Lawrence D. W and Seegmiller, Bryan",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "29552",
 year = "2021",
 month = "December",
 doi = {10.3386/w29552},
 URL = "http://www.nber.org/papers/w29552",
}

@unpublished{Webb2020,
  title={The Impact of Artificial Intelligence on the Labor Market},
  author={Webb, Michael},
  note={Working paper, Stanford University},
  year={2020}
}

@ARTICLE{FreyOsborne2017,
title = {The future of employment: How susceptible are jobs to computerisation?},
author = {Frey, Carl Benedikt and Osborne, Michael A.},
year = {2017},
journal = {Technological Forecasting and Social Change},
volume = {114},
number = {C},
pages = {254-280},
abstract = {We examine how susceptible jobs are to computerisation. To assess this, we begin by implementing a novel methodology to estimate the probability of computerisation for 702 detailed occupations, using a Gaussian process classifier. Based on these estimates, we examine expected impacts of future computerisation on US labour market outcomes, with the primary objective of analysing the number of jobs at risk and the relationship between an occupations probability of computerisation, wages and educational attainment.},
keywords = {Occupational choice; Technological change; Wage inequality; Employment; Skill demand;},
url = {https://EconPapers.repec.org/RePEc:eee:tefoso:v:114:y:2017:i:c:p:254-280}
}

@article{Tolan2021,
  title={Measuring the occupational impact of AI: tasks, cognitive abilities and AI benchmarks},
  author={Tolan, S. and Pesole, A. and Martínez-Plumed, F. and Fernández-Macías, E. and Hernández-Orallo, J. and Gómez, E.},
  journal={Journal of Artificial Intelligence Research},
  volume={71},
  pages={191--236},
  year={2021}
}

@article{Brynjolfsson2018,
  title={What can machines learn, and what does it mean for occupations and the economy?},
  author={Brynjolfsson, E. and Mitchell, T. and Rock, D.},
  journal={AEA Papers and Proceedings},
  volume={108},
  pages={43--47},
  year={2018},
  doi={10.1257/pandp.20181019}
}

@article{SeamansRajFelten2018,
Author = {Felten, Edward W. and Raj, Manav and Seamans, Robert},
Title = {A Method to Link Advances in Artificial Intelligence to Occupational Abilities},
Journal = {AEA Papers and Proceedings},
Volume = {108},
Year = {2018},
Month = {May},
Pages = {54-57},
DOI = {10.1257/pandp.20181021},
URL = {https://www.aeaweb.org/articles?id=10.1257/pandp.20181021}}

@techreport{korinek2023language,
  title={Language Models and Cognitive Automation for Economic Research},
  author={Korinek, Anton},
  year={2023},
  institution={National Bureau of Economic Research}
}

@ARTICLE{Goodfellow2014-sy,
  title         = "Generative Adversarial Networks",
  author        = "Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi
                   and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and
                   Courville, Aaron and Bengio, Yoshua",
  abstract      = "We propose a new framework for estimating generative models
                   via an adversarial process, in which we simultaneously train
                   two models: a generative model G that captures the data
                   distribution, and a discriminative model D that estimates
                   the probability that a sample came from the training data
                   rather than G. The training procedure for G is to maximize
                   the probability of D making a mistake. This framework
                   corresponds to a minimax two-player game. In the space of
                   arbitrary functions G and D, a unique solution exists, with
                   G recovering the training data distribution and D equal to
                   1/2 everywhere. In the case where G and D are defined by
                   multilayer perceptrons, the entire system can be trained
                   with backpropagation. There is no need for any Markov chains
                   or unrolled approximate inference networks during either
                   training or generation of samples. Experiments demonstrate
                   the potential of the framework through qualitative and
                   quantitative evaluation of the generated samples.",
  month         =  jun,
  year          =  2014,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1406.2661"
}

@ARTICLE{Radford2015-dd,
  title         = "Unsupervised representation learning with deep convolutional
                   generative adversarial networks",
  author        = "Radford, Alec and Metz, Luke and Chintala, Soumith",
  abstract      = "In recent years, supervised learning with convolutional
                   networks (CNNs) has seen huge adoption in computer vision
                   applications. Comparatively, unsupervised learning with CNNs
                   has received less attention. In this work we hope to help
                   bridge the gap between the success of CNNs for supervised
                   learning and unsupervised learning. We introduce a class of
                   CNNs called deep convolutional generative adversarial
                   networks (DCGANs), that have certain architectural
                   constraints, and demonstrate that they are a strong
                   candidate for unsupervised learning. Training on various
                   image datasets, we show convincing evidence that our deep
                   convolutional adversarial pair learns a hierarchy of
                   representations from object parts to scenes in both the
                   generator and discriminator. Additionally, we use the
                   learned features for novel tasks - demonstrating their
                   applicability as general image representations.",
  month         =  nov,
  year          =  2015,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1511.06434"
}

@ARTICLE{Liu2016-uc,
  title         = "Coupled generative adversarial networks",
  author        = "Liu, Ming-Yu and Tuzel, Oncel",
  abstract      = "We propose coupled generative adversarial network (CoGAN)
                   for learning a joint distribution of multi-domain images. In
                   contrast to the existing approaches, which require tuples of
                   corresponding images in different domains in the training
                   set, CoGAN can learn a joint distribution without any tuple
                   of corresponding images. It can learn a joint distribution
                   with just samples drawn from the marginal distributions.
                   This is achieved by enforcing a weight-sharing constraint
                   that limits the network capacity and favors a joint
                   distribution solution over a product of marginal
                   distributions one. We apply CoGAN to several joint
                   distribution learning tasks, including learning a joint
                   distribution of color and depth images, and learning a joint
                   distribution of face images with different attributes. For
                   each task it successfully learns the joint distribution
                   without any tuple of corresponding images. We also
                   demonstrate its applications to domain adaptation and image
                   transformation.",
  month         =  jun,
  year          =  2016,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1606.07536"
}

@ARTICLE{Karras2017-th,
  title         = "Progressive growing of {GANs} for improved quality,
                   stability, and variation",
  author        = "Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen,
                   Jaakko",
  abstract      = "We describe a new training methodology for generative
                   adversarial networks. The key idea is to grow both the
                   generator and discriminator progressively: starting from a
                   low resolution, we add new layers that model increasingly
                   fine details as training progresses. This both speeds the
                   training up and greatly stabilizes it, allowing us to
                   produce images of unprecedented quality, e.g., CelebA images
                   at 1024^2. We also propose a simple way to increase the
                   variation in generated images, and achieve a record
                   inception score of 8.80 in unsupervised CIFAR10.
                   Additionally, we describe several implementation details
                   that are important for discouraging unhealthy competition
                   between the generator and discriminator. Finally, we suggest
                   a new metric for evaluating GAN results, both in terms of
                   image quality and variation. As an additional contribution,
                   we construct a higher-quality version of the CelebA dataset.",
  month         =  oct,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1710.10196"
}

@ARTICLE{Karras2018-gh,
  title         = "A style-based generator architecture for generative
                   adversarial networks",
  author        = "Karras, Tero and Laine, Samuli and Aila, Timo",
  abstract      = "We propose an alternative generator architecture for
                   generative adversarial networks, borrowing from style
                   transfer literature. The new architecture leads to an
                   automatically learned, unsupervised separation of high-level
                   attributes (e.g., pose and identity when trained on human
                   faces) and stochastic variation in the generated images
                   (e.g., freckles, hair), and it enables intuitive,
                   scale-specific control of the synthesis. The new generator
                   improves the state-of-the-art in terms of traditional
                   distribution quality metrics, leads to demonstrably better
                   interpolation properties, and also better disentangles the
                   latent factors of variation. To quantify interpolation
                   quality and disentanglement, we propose two new, automated
                   methods that are applicable to any generator architecture.
                   Finally, we introduce a new, highly varied and high-quality
                   dataset of human faces.",
  month         =  dec,
  year          =  2018,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1812.04948"
}

@misc{dalle2,
  doi = {10.48550/ARXIV.2102.12092},
  
  url = {https://arxiv.org/abs/2102.12092},
  
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}}

@article{Acemoglu2022,
  author = {Acemoglu, D. and Restrepo, P.},
  title = {Tasks, Automation, and the Rise in U.S. Wage Inequality},
  journal = {Econometrica},
  volume = {90},
  pages = {1973--2016},
  year = {2022},
  doi = {10.3982/ECTA19815}
}

@article{hyman2021wage,
  title={Wage Insurance and Labor Market Trajectories},
  author={Hyman, Benjamin G and Kovak, Brian K and Leive, Adam and Naff, Theodore},
  journal={AEA Papers and Proceedings},
  volume={111},
  pages={491--495},
  year={2021}
}

@inproceedings{AIliteracy2020,
author = {Long, Duri and Magerko, Brian},
title = {What is AI Literacy? Competencies and Design Considerations},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376727},
doi = {10.1145/3313831.3376727},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–16},
numpages = {16},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@TechReport{Reed2012,
  author={Debbie Reed and Albert Yung-Hsu Liu and Rebecca Kleinman and Annalisa Mastri and Davin Reed and Samina Sattar and Jessica Ziegler},
  title={{An Effectiveness Assessment and Cost-Benefit Analysis of Registered Apprenticeship in 10 States}},
  year="undated",
  institution={Mathematica Policy Research},
  type={Mathematica Policy Research Reports},
  url={https://ideas.repec.org/p/mpr/mprres/1b5795d01e8a42239b3c98dcc1e1161a.html},
  number={1b5795d01e8a42239b3c98dcc},
  abstract={This study examines Registered Apprenticeships, career-training programs administered by the Employment and Training Administration's Office of Apprenticeship in the U.S. Department of Labor, in conjunction with state apprenticeship agencies.},
  keywords={Cost-Benefit Analysis ; Registered Apprenticeship ; 10 States ; Labor},
  doi={},
}

@misc{noauthor_casetext_2022,
	title = {Casetext - {CoCounsel}},
	url = {https://casetext.com/},
	abstract = {Casetext is an award-winning legal AI company developing cutting-edge tech for 10+ years—including CoCounsel, the first AI legal assistant.},
	language = {en-US},
	urldate = {2023-03-11},
	month = dec,
	year = {2022},
}

@misc{noauthor_jasper_nodate,
	title = {Jasper - {Try} {For} {Free}},
	url = {https://www.jasper.ai/free-trial},
	abstract = {Get 10,000 words written by AI for FREE. Start your free trial at Jasper.ai.},
	urldate = {2023-03-11},
}

@misc{noauthor_github_nodate,
	title = {{GitHub} {Copilot} · {Your} {AI} pair programmer},
	url = {https://github.com/features/copilot},
	abstract = {GitHub Copilot works alongside you directly in your editor, suggesting whole lines or entire functions for you.},
	language = {en},
	urldate = {2023-03-11},
	journal = {GitHub},
}

@article{47751,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}


@url{dalle2,
url={https://openai.com/product/dall-e-2}
}

@misc{milesblog}

@misc{hazard_analysis,
  doi = {10.48550/ARXIV.2207.14157},
  
  url = {https://arxiv.org/abs/2207.14157},
  
  author = {Khlaaf, Heidy and Mishkin, Pamela and Achiam, Joshua and Krueger, Gretchen and Brundage, Miles},
  
  keywords = {Software Engineering (cs.SE), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Hazard Analysis Framework for Code Synthesis Large Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Zero v1.0 Universal}
}

@article{Noy2023,
   author = {Noy, Shakked and Zhang, Whitney},
   title = {Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence},
   journal = {SSRN},
   year = {2023},
   url = {https://ssrn.com/abstract=4375283},
   doi = {10.2139/ssrn.4375283},
   month = {March},
   day = {1},
}

@article{acemoglu_2022_automation_survey,
    title = "Automation and the Workforce: A Firm-Level View from the 2019 Annual Business Survey",
    author = "Acemoglu, Daron and Anderson, Gary W and Beede, David N and Buffington, Cathy and Childress, Eric E and Dinlersoz, Emin and Foster, Lucia S and Goldschlag, Nathan and Haltiwanger, John C and Kroff, Zachary and Restrepo, Pascual and Zolas, Nikolas",
    institution = "National Bureau of Economic Research",
    type = "Working Paper",
    series = "Working Paper Series",
    number = "30659",
    year = "2022",
    month = "November",
    doi = {10.3386/w30659},
    URL = "http://www.nber.org/papers/w30659",
}

@techreport{4systemcard,
  title={GPT-4 System Card},
  author={OpenAI},
  institution={OpenAI},
  year={2023},
  url={https://cdn.openai.com/papers/gpt-4-system-card.pdf}
}


@InCollection{acemoglu_autor_2011,
  author={Acemoglu, Daron and Autor, David},
  editor={O. Ashenfelter and D. Card},
  title={{Skills, Tasks and Technologies: Implications for Employment and Earnings}},
  booktitle={{Handbook of Labor Economics}},
  publisher={Elsevier},
  year=2011,
  month={},
  volume={4},
  number={},
  series={Handbook of Labor Economics},
  edition={},
  chapter={12},
  pages={1043-1171},
  doi={},
  keywords={College premium; Directed technical change; Earnings inequality; Occupations; Returns to schooling; },
  abstract={A central organizing framework of the voluminous recent literature studying changes in the returns to skills and the evolution of earnings inequality is what we refer to as the canonical model, which elegantly and powerfully operationalizes the supply and demand for skills by assuming two distinct skill groups that perform two different and imperfectly substitutable tasks or produce two imperfectly substitutable goods. Technology is assumed to take a factor-augmenting form, which, by complementing either high or low skill workers, can generate skill biased demand shifts. In this paper, we argue that despite its notable successes, the canonical model is largely silent on a number of central empirical developments of the last three decades, including: (1) significant declines in real wages of low skill workers, particularly low skill males; (2) non-monotone changes in wages at different parts of the earnings distribution during different decades; (3) broad-based increases in employment in high skill and low skill occupations relative to middle skilled occupations (i.e., job \&quot;polarization\&quot;); (4) rapid diffusion of new technologies that directly substitute capital for labor in tasks previously performed by moderately skilled workers; and (5) expanding offshoring in opportunities, enabled by technology, which allow foreign labor to substitute for domestic workers specific tasks. Motivated by these patterns, we argue that it is valuable to consider a richer framework for analyzing how recent changes in the earnings and employment distribution in the United States and other advanced economies are shaped by the interactions among worker skills, job tasks, evolving technologies, and shifting trading opportunities. We propose a tractable task-based model in which the assignment of skills to tasks is endogenous and technical change may involve the substitution of machines for certain tasks previously performed by labor. We further consider how the evolution of techno},
  url={https://ideas.repec.org/h/eee/labchp/5-12.html}
}

@article{Zeira1998,
    title={Workers, Machines, and Economic Growth},
    author={Zeira, Joseph},
    journal={Quarterly Journal of Economics},
    volume={113},
    number={4},
    pages={1091--1117},
    year={1998}
}

@article{Weidinger2021,
  author    = {Laura Weidinger and others},
  title     = {Ethical and Social Risks of Harm from Language Models},
  journal   = {arXiv:2112.04359 [cs]},
  year      = {2021},
  month     = {Dec},
  archivePrefix = {arXiv},
  eprint    = {2112.04359},
}

@inproceedings{Weidinger2022,
    author = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
    title = {Taxonomy of Risks Posed by Language Models},
    year = {2022},
    isbn = {9781450393522},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3531146.3533088},
    doi = {10.1145/3531146.3533088},
    booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {214–229},
    numpages = {16},
    keywords = {responsible innovation, responsible AI, technology risks, risk assessment, language models},
    location = {Seoul, Republic of Korea},
    series = {FAccT '22}
}

@book{Autor2022,
  author    = {David Autor and David A. Mindell and Elisabeth B. Reynolds},
  title     = {The Work of the Future: Building Better Jobs in an Age of Intelligent Machines},
  publisher = {The MIT Press},
  year      = {2022},
  isbn      = {978-0-262-36775-2},
  doi       = {10.7551/mitpress/14109.001.0001},
}

@inproceedings{Sorensen_2022,
	doi = {10.18653/v1/2022.acl-long.60},
    
	year = 2022,
	publisher = {Association for Computational Linguistics},
  
	author = {Taylor Sorensen and Joshua Robinson and Christopher Rytting and Alexander Shaw and Kyle Rogers and Alexia Delorey and Mahmoud Khalil and Nancy Fulda and David Wingate},
  
	title = {An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels},
  
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}
}

@misc{constantz_bloomberg,
	title = {Nearly a third of white collar workers have tried ChatGPT or Other AI Programs, According to a New Survey},
        author = {Jo Constantz},
	month = jan,
	year = {2023},
	url = {https://time.com/6248707/survey-chatgpt-ai-use-at-work/},
	urldate = {2023-03-14},
}

@misc{chatgptblog,
	title = {Introducing ChatGPT},
        author = {OpenAI},
	month = nov,
	year = {2022},
	url = {https://openai.com/blog/chatgpt},
	urldate = {2023-03-14},
}

@misc{resumebuildersuvey,
	title = {1 in 4 companies have already replaced workers with ChatGPT},
        author = {ResumeBuilder.com},
	month = feb,
	year = {2023},
	url = {https://www.resumebuilder.com/1-in-4-companies-have-already-replaced-workers-with-chatgpt},
	urldate = {2023-03-14},
}

@techreport{benzell2021,
 title = "Simulating Endogenous Global Automation",
 author = "Benzell, Seth G and Kotlikoff, Laurence J and LaGarda, Guillermo and Ye, Victor Yifan",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "29220",
 year = "2021",
 month = "September",
 doi = {10.3386/w29220},
 URL = "http://www.nber.org/papers/w29220",
 abstract = {This paper develops a 17-region, 3-skill group, overlapping generations, computable general equilibrium model to evaluate the global consequences of automation. Automation, modeled as capital- and high-skill biased technological change, is endogenous with regions adopting new technologies when profitable. Our approach captures and quantifies key macro implications of a range of foundational models of automation. In our baseline scenario, automation has a moderate effect on regional outputs and a small effect on world interest rates. However, it has a major impact on inequality, both wage inequality within regions and per capita GDP inequality across regions. We examine two policy responses to technological change -- mandating use of the advanced technology and providing universal basic income to share gains from automation. The former policy can raise a region's output, but at a welfare cost. The latter policy can transform automation into a win-win for all generations in a region.},
}

@misc{solaiman_release,
  doi = {10.48550/ARXIV.1908.09203},
  
  url = {https://arxiv.org/abs/1908.09203},
  
  author = {Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and McCain, Miles and Newhouse, Alex and Blazakis, Jason and McGuffie, Kris and Wang, Jasmine},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2; I.2.7; K.4},
  
  title = {Release Strategies and the Social Impacts of Language Models},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
