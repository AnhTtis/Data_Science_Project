%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
%%\documentclass[pdflatex,sn-mathphys]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[sn-standardnature]{sn-jnl}% Standard Nature Portfolio Reference Style
%%\documentclass[default]{sn-jnl}% Default
\documentclass[default,iicol]{sn-jnl}% Default with double column layout
    
%%%% Standard Packages
%%<additional latex packages if required can be included here>
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

\jyear{2023}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{A Framework for Real-time Object Detection and Image Restoration}

\author[1]{\fnm{Rui-Yang} \sur{Ju}}\email{jryjry1094791442@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author[1]{\fnm{Chih-Chia} \sur{Chen}}\email{crystal88irene@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author*[1]{\fnm{Jen-Shiun} \sur{Chiang}}\email{jsken.chiang@gmail.com}

\author[1]{\fnm{Yu-Shian} \sur{Lin}}\email{abcpp12383@gmail.com}


\author[1]{\fnm{Wei-Han} \sur{Chen}}\email{kj211378@gmail.com}

\affil[1]{\orgdiv{Department of Electrical and Computer Engineering}, \orgname{Tamkang University} \orgaddress{\street{No.151, Yingzhuan Rd., Tamsui Dist.}, \city{New Taipei City}, \postcode{251301}, \country{Taiwan}}}

\abstract{Object detection and single image super-resolution are classic problems in computer vision (CV). The object detection task aims to recognize the objects in input images, while the image restoration task aims to reconstruct high quality images from given low quality images. In this paper, a two-stage framework for object detection and image restoration is proposed. The first stage uses YOLO series algorithms to complete the object detection and then performs image cropping. In the second stage, this work improves Swin Transformer and uses the new proposed algorithm to connect the Swin Transformer layer to design a new neural network architecture. We name the newly proposed network for image restoration SwinOIR. This work compares the model performance of different versions of YOLO detection algorithms on MS COCO dataset and Pascal VOC dataset, demonstrating the suitability of different YOLO network models for the first stage of the framework in different scenarios. For image super-resolution task, it compares the model performance of using different methods of connecting Swin Transformer layers and design different sizes of SwinOIR for use in different life scenarios. Our implementation code is released at \url{https://github.com/Rubbbbbbbbby/SwinOIR}.}

\keywords{object detection, super-resolution, image restoration, Transformer, YOLO, deep learning}

\maketitle

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{figure_stage.pdf}
  \caption{Overall flowchart of the two-stage framework.}
  \label{fig_stage}
\end{figure*}

\section{Introduction}
Object detection and image restoration are two important topics in computer vision (CV). The former research can be applied to medical image analysis \cite{jaeger2020retina}\cite{li2019clu}, multi-target tracking \cite{zhang2022bytetrack}\cite{zhang2021fairmot}, and automated driving \cite{feng2020deep}\cite{li2019gs3d} scenarios. The latter research can be used for image super-resolution \cite{zhao2019channel}\cite{zhang2021mr}, JPEG artifact removal \cite{ulyanov2018deep}\cite{jiang2021towards}, and image denoising \cite{chen2022simple}\cite{cai2021learning}. Although the applications of these two research topics are very broad, there is currently less research combining these two techniques. In real-world life, there are many situations where objects cannot be recognized because they are blurry in low quality images and videos, which need to be reconstructed to high quality for specific objects, such as real time video surveillance \cite{sultani2018real}\cite{thys2019fooling} and panoramic images \cite{orhan2021efficient}\cite{xu2020state}.

The classic object detection algorithms are mainly based on deep learning and are divided into one-stage and two-stage detection algorithms. R-CNN \cite{girshick2014rich} divides the detection process into two stages. The first stage is to extract several Region Proposals based on the image, and the second stage is to run the state-of-the-art (SOTA) image classification network \cite{tan2019efficientnet}\cite{liu2022convnet} on these proposed regions to obtain the classes of objects. Unlike R-CNN that requires multiple feature extractions, Fast R-CNN \cite{girshick2015fast} only requires one feature extraction for the whole image, and performs classification and regression directly, enabling  optimization for multi-task concatenation. However, the Region of Interest (ROI) of Fast R-CNN is still through Selective Search, which is a less efficient search method. Therefore, Faster R-CNN \cite{ren2015faster} proposes Region Proposal Network (RPN) based on Fast R-CNN to automatically generate ROI, which greatly improves the efficiency of bounding box generation. Since there is no given ground truth image in the test set, all Region Proposals in Faster R-CNN are classified and localized at the end, and the Cascade R-CNN \cite{cai2018cascade} proposes cascade multiple detection heads to solve this problem. Moreover, Mask R-CNN \cite{he2017mask} adds FCN \cite{long2015fully} to Faster R-CNN for segmentation, which greatly improves the model performance.

The one-stage object detection algorithm can directly generate class probabilities and locations of objects, which makes the overall process simpler than the two-stage object detection algorithm that does not require Region Proposals. In the one-stage object detection algorithms, apart from the classic SSD \cite{liu2016ssd}, RetinaNet \cite{lin2017focal}, and CornerNet\cite{law2018cornernet}, the YOLO series detection algorithms \cite{redmon2016you}\cite{redmon2017yolo9000}\cite{redmon2018yolov3}\cite{bochkovskiy2020yolov4}\cite{ge2021yolox}\cite{wang2021you}\cite{glenn2022yolov5}\cite{li2022yolov6}\cite{wang2022yolov7} always strive for the best speed and accuracy trade-off for real-time applications.

With the rise of deep learning, neural networks based on image restoration tasks have started to develop by leaps and bounds \cite{dong2014learning}. Among them, convolutional neural networks (CNN) \cite{lim2017enhanced}\cite{zhang2018image}\cite{zhang2018residual}\cite{dai2019second}\cite{niu2020single} have become the main network models for image super-resolution in the past few years. Most CNNs improve the performance of the model by designing new network architectures, such as using different algorithms to connect the convolutional layers \cite{huang2017densely}\cite{threshnet2022}\cite{ju_efficient_2023}, but this improvement cannot solve the problem of no interactive content between convolutional kernels and images. The same convolution kernel does not perform well in image restoration for different images. Recently, Transformer \cite{vaswani2017attention} has achieved outstanding success in CV by employing self-attention mechanisms to obtain information from contexts. With various Transformer architectures \cite{dosovitskiy2020image}\cite{liu2021swin}\cite{wang2021pyramid} being proposed and achieving SOTA performance in dense prediction tasks, they have also made breakthroughs in the field of image super-resolution \cite{li2021efficient}\cite{liang2021swinir}\cite{zhang2022swinfir}.

We propose a two-stage framework for real-time object detection, cropping, and image super-resolution tasks. Fig. \ref{fig_stage} illustrates the proposed two-stage framework. In the object detection stage we use YOLO series detection algorithms, while in the image super-resolution stage we use the newly proposed SwinOIR.

To evaluate the performance of the proposed two-stage framework, this work trains the models for the first stage object detection task on the MS COCO 2017 dataset and Pascal VOC 2007 and 2012 datasets. In the second stage of image-resolution task, it trains the models on DIV2K and Flickr2K datasets and evaluates them on Set5, Set14, BSD100, and Urban100 datasets.

The contributions of this paper are summarised as follows:
\begin{itemize}
\item [1)]
We propose a two-stage framework for real-time object detection, cropping, and image super-resolution for real-life applications, making it easier to find details of multiple objects in low quality images.
\end{itemize}
\begin{itemize}
\item [2)]
For single image super-resolution, the newly proposed network model improves the model performance by connecting the Swin Transformer layer in a specific way. The experimental results show that our model is suitable for real-time tasks on low-computing platforms.
\end{itemize} 

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{figure_swinoir.pdf}
  \caption{The architecture of the proposed SwinOIR for image restoration.}
  \label{fig_swinoir}
\end{figure*}

\section{Related Work}
\subsection{YOLO}
YOLO series are classic object detection algorithms designed to be lightweight and fast while maintaining accuracy. This makes it ideal for deployment in real-life scenarios where inference time is crucial.

YOLOv1 \cite{redmon2016you} first scales the input image to $448 \times 448 \times 3$, then uses GoogLeNet \cite{szegedy2015going} to extract the feature map, and sends the extracted feature map to two fully connected layers to finally output a $7 \times 7 \times 30$ feature map. Specifically, the input image is divided into a grid cell of \emph{S} $\times$ \emph{S}. When the centre of the object falls in a grid cell, that grid cell is responsible for the detection of it. Each grid cell predicts \emph{B} bounding boxes, and outputs a feature map of size $S \times S \times (B \times 5 \times C)$, where $C$ is the class probabilities. Although YOLOv1 uses direct regression to reduce the computational effort and reduce the inference time, it is not effective when there are multiple objects in close proximity to each other or when there are small objects because each grid cell has only 2 bounding boxes.

YOLOv2 \cite{redmon2017yolo9000} replaces the backbone network with YOLOv1 \cite{redmon2016you} and adds the passthrough layer. It first inputs images into Darknet-19 to extract feature maps, and then outputs related information. YOLOv2 uses passthrough layers to combine high and low level semantic information, which enhances the detection of small objects to a certain extent. It uses a small convolutional kernel instead of a $7 \times 7$ kernel, reducing the amount of computation and improving the strategy of position shifting. However, when multiple objects are in close proximity to each other or small objects are present, the detection results still need to be improved.

YOLOv3 \cite{redmon2018yolov3} proposes Darkent-53 by deepening the network and adding residual learning to Darknet-19. Redmon and Farhadi put forward the idea of feature pyramids \cite{lin2017feature}, by using a multi-scale feature map for detection, multiple independent logistic regression classifiers instead of softmax to predict class classification. It solves the problem and achieves a better balance between inference speed and accuracy.

YOLOv4 \cite{bochkovskiy2020yolov4} adds the Cross Stage Partial (CSP) \cite{wang2020cspnet} architecture to Darknet-53, proposing the new backbone network CSPDarkent-53, which enables the network architecture to obtain richer gradient fusion information and reduce computation. YOLOv4 adopts Spatial Pyramid Pooling (SPP) \cite{he2015spatial} to extend the receptive field and adds Path Aggregation Network (PAN) \cite{liu2018path} architecture to Neck. Experimental results showed that when the FPS is the same, the AP value of YOLOv4 increased by 10\% over YOLOv3 on the MS COCO dataset.

YOLOv5 \cite{glenn2022yolov5} extends the CSP \cite{wang2020cspnet} architecture from the YOLOv4 \cite{bochkovskiy2020yolov4} backbone network to Neck and adds the Focus module, using the Spatial Pyramid Pooling - Fast (SPPF) \cite{he2015spatial} architecture instead of SPP to improve the model performance.

YOLOv6 \cite{li2022yolov6} replaces the backbone network from CSPDarknet to EfficientRep, while Neck is based on Rep \cite{ding2021repvgg} and PAN \cite{liu2018path} to design Rep-PAN. The detection head of YOLOv6 borrows from YOLOX \cite{ge2021yolox} for decoupling operations, which optimizes the time consumption and improves YOLO detection algorithm performance.

YOLOv7 \cite{wang2022yolov7} proposes the planned re-parameterized convolution module, which can be applied to various model architectures. The authors further analyse dynamic label assignment and propose two new label assignment strategies to handle and assign different branches, and propose model scaling and extension to efficiently make use the number of parameters and computation effort.

\subsection{Swin Transformer}
Since the excellent performance of Transformer \cite{vaswani2017attention} model in the field of natural language processing (NLP), researchers started to apply Transformer in CV tasks. With the introduction of Vision Transformer \cite{dosovitskiy2020image}, the attention mechanism has become popular among researchers. Unlike CNN, Transformer learns to focus on important image regions through global interactions, and achieves good performance. Until now Transformer has achieved SOTA model performance in image classification \cite{wu2020visual}\cite{vaswani2021scaling}, object detection \cite{carion2020end}\cite{liu2020deep}, and segmentation \cite{cao2021swin}\cite{zheng2021rethinking} tasks. In the field of image restoration \cite{chen2021pre}\cite{wang2022uformer}, researchers have also started to implement Transformer on image super-resolution tasks.

Swin Transformer \cite{liu2021swin} provides a general Transformer-based backbone network and can be applied to multiple CV tasks, such as image classification, object detection, semantic segmentation, and instance segmentation. Swin Transformer divides the feature map into non-overlapping windows of different sizes, and self-attention is computed only within the given window. This model achieves SOTA performance in multiple CV tasks. Therefore, we improve Swin Transformer and apply the newly proposed network model to image super-resolution tasks.

\section{Proposed Method}
\subsection{Overall Architecture}
As shown in Fig. \ref{fig_stage}, the proposed method consists of a two-stage framework, the first stage accomplishes the object detection task and the second stage accomplishs the image restoration task. In the first stage, the input images are detected by using YOLO series algorithms. As shown in Table \ref{tab_class}, we set up 20 classes for detection, including the common objects in real-world life, such as person, bicycle, bus, cat, and dog. Each detected object has its own bounding box and we crop out the image within that bounding box for the second stage of image restoration.

In general, the output of the first stage would be multiple object images, all of them are part of the original input image. We perform image restoration on each object image separately, using the newly proposed network model to perform $\times$4 super-resolution, making it easier to identify multiple object images and find the details of it.

\begin{table}
\begin{center}
\caption{Target Classes for Object Detection.}
\label{tab_class}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{5}{|c|}{\textbf{Classes for Object Detection Task}} \\
\hline
aeroplane & bicycle & bird & boat & bottle \\
\hline
bus & car & cat & chair & cow \\
\hline
diningtable & dog & horse & motorbike & person \\
\hline
pottedplant & sheep & sofa & train & tvmonitor \\
\hline
\end{tabular}}
\end{center}
\end{table}

\subsection{SwinOIR Architecture}
To perform single image super-resolution, a new network architecture based on Swin Transformer has been designed. The network architecture consists of three modules, as shown in Fig. \ref{fig_swinoir}, which are Pre-Feature Extraction, Main Feature Extraction, and High Quality Image Reconstruction.

\subsubsection{Feature Extraction}
The feature extraction in this work is divided into Pre-Feature Extraction and Main Feature Extraction. Tete \emph{et al.} \cite{xiao2021early} demonstrated that convolution is suitable for early visual processing, and therefore we use a $3\times 3$ convolution layer, $H_{\textit{pre}}(\cdot)$, for pre-feature extraction, $F_{pre}\in\mathbb{R}^{H\times W\times C}$. When the input is the object image, $I_{\textit{O}}\in\mathbb{R}^{H\times W\times C}$ ($H$, $W$ and $C$ are the image height, width, and channel number, respectively), the equation is:

\begin{equation}
F_{pre}=H_{\textit{pre}}(I_{\textit{O}}).
\end{equation} 

After completing pre-feature extraction, we extract main feature $F_{\textit{main}}$ extraction from $F_{\textit{pre}}$:

\begin{equation}
F_{\textit{main}}=H_{\textit{main}}(F_{pre}),
\end{equation} 
where $H_{\textit{main}}(\cdot)$ contains $m$ Interval Dense Swin Transformer Blocks (IDSTB) and a $3 \times 3$ convolution layer. In this paper, we set $m$ to 5 and 7 to design new models. More design details of the IDSTB are described in Section \ref{MFE}.

\subsubsection{Image Reconstruction}
After the feature extraction, we use the image reconstruction module $H_{\textit{IR}}(\cdot)$ to reconstruct the high quality object image $I_{\textit{HQO}}\in\mathbb{R}^{H\times W\times C}$. Since pre-feature extraction is used to extract features from the low-frequency information and main feature extraction focuses on the missing high-frequency information, we use the skip connection to add the information from pre-feature extraction and main feature extraction. The equation is shown as:

\begin{equation}
I_{\textit{HQO}}=H_{\textit{IR}}(F_{\textit{pre}}+F_{\textit{main}}).
\end{equation}

\subsubsection{Loss Function}
In this work, the parameters of the network architecture are optimized using the minimization of $L1$ pixel loss:

\begin{equation}
\mathcal{L}=\| I_{\textit{HQO}}-I_{\textit{HQ}}\|_1,
\end{equation}
where $I_{\textit{HQO}}$ is the image obtained from the input object image by SwinOIR, and $I_{\textit{HQ}}$ is the corresponding high quality ground truth image. For the classical image super-resolution task, we follow the previous work to evaluate the performance of our network model by using only the $L1$ pixel loss. For real-life super-resolution tasks, we combine perceptual loss, GAN loss, and pixel loss \cite{goodfellow2020generative}\cite{johnson2016perceptual}\cite{wang2021real}\cite{wang2018esrgan}\cite{zhang2021designing} to improve the image quality.

\subsection{Main Feature Extraction}
\label{MFE}
\subsubsection{Swin Transformer Layer}
Swin Transformer layer (STL) in SwinOIR is different from that used in the original Swin Transformer. We use multi-head self-attention (MSA) + multilayer perceptron (MLP) instead of window based MSA (W-MSA) + MLP, and shifted window based MSA (SW-MSA) + MLP.

As shown in Fig. \ref{fig_swinoir}, when the size of the input image is ${H\times W\times C}$, Swin Transformer divides the input into $M\times M$ non-overlapping local windows to reshape the image into $\frac{HW}{M^2}\times M^2\times C$ features, where $\frac{HW}{M^2}$ is the total number of windows, and self-attention is computed separately for each window. For feature $X\in\mathbb{R}^{M^2\times C}$ of the local window, the equations for calculating the \textit{query} $Q$, \textit{key} $K$, and \textit{value matrices} $V$ are as follows:

\begin{equation}
\begin{split}
Q=XP_Q,\\
K=XP_K,\\
V=XP_V.
\end{split}
\end{equation}

\begin{algorithm}
\caption{Interval Dense Connection}
\label{alg}
\begin{algorithmic}
\Require {Features $F_{i,1},F_{i,2}, \ldots, F_{i,m}$}
\Ensure {$F_{i,out}=H_{\textit{conv}_i}(F_{i,m})$}
\For{STL $m$ is odd}
\State {STL $m$ connect to STL $2^0$}
\For{$m$ in $2^1$ to $2^5$}
\State {let STL $m$ connect to STL $x$ ($x \leq m$ and $x$ is even)}
\EndFor
\EndFor
\For{STL $m$ is even}
\State {STL $m$ connect to STL $2^0$}
\For{$m$ in $2^1$ to $2^5$}
\State {let STL $m$ connect to STL $x$ ($x \leq m$ and $x$ is odd)}
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

For $Q,K,V\in\mathbb{R}^{M^2\times d}$, the equation of the self-attention  mechanism in the local window is:

\begin{equation}
\text{Attention}(Q,K,V)=\text{SoftMax}(QK^T/\sqrt{d}+B)V,
\end{equation}
where $B$ is the relative position encoding and it implements the attention function multiple times and concatenates the results as MSA.

MLP consists of two fully connected layers and GELU \cite{hendrycks2016gaussian} for feature transformation. We add LayerNorm (LN) \cite{ba2016layer} before MSA and MLP, and the overall equation is as follows:

\begin{equation}
\begin{split}
X&=X+\text{MSA}(\text{LN}(X)),\\
X&=X+\text{MLP}(\text{LN}(X)).
\end{split}
\end{equation}

\subsubsection{Interval Dense Swin Transformer Block}
In this work, main feature extraction consists of multiple IDSTB and a convolutional layer, which separately extract intermediate features $F_{1},F_2, \ldots, F_n$: 

\begin{equation}
F_{i}= H_{\textit{IDSTB}_{i}}(F_{i-1}), \quad i=1, 2, \ldots, n,\\
\end{equation} 
where $H_{\textit{IDSTB}_{i}}(\cdot)$ denotes the $i$-th IDSTB. Since convolution has the effect of inductive bias, adding a convolution layer $H_{\textit{conv}}$ at the end of the feature extraction helps to connect pre-feature extraction and main feature extraction:

\begin{equation}
F_{\textit{final}}= H_{\textit{conv}}(F_n).
\end{equation} 

Even though Transformer has the effect of Spatially-Varying Convolution \cite{vaswani2021scaling}\cite{elsayed2020revisiting}, the convolution layer can increase the translational equivalence of SwinOIR.

\subsubsection{Interval Dense Connection}
As shown in Fig. \ref{fig_swinoir}, each IDSTB consists of multiple Swin Transformer layers (STL) and a convolution layer. We use the newly proposed Algorithm \ref{alg} to connect STL to improve feature reuse. For the input feature $F_{i,0}$ of the $i$-th IDSTB, the extracted features are $F_{i,1},F_{i,2}, \ldots, F_{i,m}$:

\begin{equation}
F_{i,j}=\left\{\begin{array}{c}
\text { if } m \% 2=1\\
H_{\textit{STL}_{i,j}}\left(\left[F_{i,1}, F_{i,2}, F_{i,4}\cdots, F_{i,m-1}\right]\right),\\ \\
\text { if } m \% 2=0\\
H_{\textit{STL}_{i,j}}\left(\left[F_{i,1}, F_{i,3}, F_{i,5} \cdots, F_{i,m-1}\right]\right),
\end{array}\right.
\end{equation}
where $H_{\textit{STL}_{i,j}}(\cdot)$ is the $j$-th STL in the $i$-th IDSTB. The output equation for each IDSTB is as follows:

\begin{equation}
F_{i,out}=H_{\textit{conv}_i}(F_{i,m}).
\end{equation}

To demonstrate the difference between the newly proposed algorithm and the dense connection of all STLs, we perform ablation study to compare the performance of networks designed by these two methods, as described in Section \ref{ablation}. 

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{figure_result_bsds.pdf}
  \caption{Example of experimental results of the two-stage framework on The Berkeley Segmentation Dataset and Benchmark (BSDS300): (a) Image 33, (b) Image 63, (c) Image 84, (d) Image 263.}
  \label{fig_result_bsds}
\end{figure*}

\section{Experiment}
\subsection{Dataset}
\subsubsection{MS COCO dataset}
Common Objects in Context (COCO) dataset \cite{lin2014microsoft} is provided by the Microsoft team, which is a benchmark dataset for image recognition. COCO 2017 dataset is a large scale dataset with 80 categories, including 118,287 images in the training set, 5,000 images in the validation set, and 40,670 images in the test set. Only the training and validation sets are labeled in the dataset, and the test set does not have any label information. Researchers can evaluate the model performance of their proposed neural network on this test set.

\subsubsection{Pascal VOC dataset}
The PASCAL Visual Object Classes (VOC) Challenge \cite{Everingham10} is a benchmark test for the classification recognition and detection of visual objects, providing a standard evaluation system for detection algorithms and network model performance. PASCAL provided datasets from 2005 to 2012. VOC 2007 dataset contains 9,963 labeled images with a total of 24,640 objects labeled. VOC 2012 dataset is an updated version of VOC 2007 dataset, with 11,530 images and 27,450 objects. For detection tasks, VOC 2012 dataset contains all corresponding images from 2008 to 2011.

\subsubsection{SR testing datasets}
Set5 \cite{bevilacqua2012low} and Set14 \cite{zeyde2012single} datasets are public datasets for testing the performance of single image super-resolution models, which Set5 and Set14 contain 5 and 14 color images respectively.

BSD100 dataset \cite{martin2001database} includes 100 images of size 320 $\times$ 480 or 480 $\times$ 320, and these test images have various types of spatial distributions of key visual information.

Urban100 dataset \cite{huang2015single} contains 100 images of urban scenes. It is often used as a test set to evaluate the performance of super-resolution models.

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{figure_result_pirm.pdf}
  \caption{Example of experimental results of the two-stage framework on Perceptual Image Restoration and Manipulation (PIRM) dataset: (a) Image 385039, (b) Image 119082, (c) Image 197017, (d) Image 37073.}
  \label{fig_result_pirm}
\end{figure*}

\subsubsection{DIV2K dataset}
DIVerse 2K resolution high quality images (DIV2K) \cite{Ignatov_2018_ECCV_Workshops} is a dataset provided by 2017 NTIRE competition. It consists of 1,000 high quality images (2K resolution), of which 800 are for training, 100 for verification, and 100 for testing. The training set has both original high-resolution images and corresponding low-resolution images.

\subsubsection{BSDS300}
The Berkeley Segmentation Dataset and Benchmark (BSDS300) \cite{MartinFTM01} is a dataset provided by the computer vision group at the University of California, Berkeley. The dataset contains 200 training images and 100 test images for object detection and segmentation tasks.

\subsubsection{PIRM dataset}
The PIRM Dataset \cite{blau20182018} was provided by Blau \emph{et al.} in 2018 PIRM Challenge on Perceptual Image Super-resolution. The dataset consists of 200 images, which were divided into two equal sets for validation and testing. These images vary in size and cover a diverse range of content, including people, objects, and environments.

\begin{table*}
\begin{center}
\caption{Object detection results for different YOLO models on the VOC 2007 test set.}
\label{tab_voc}
\setlength{\tabcolsep}{4.5mm}{
\begin{tabular}{cccccc}
\hline
\textbf{Method} & \textbf{Backbone} & \textbf{Input Image Size} & \textbf{Trainig Set} & \textbf{mAP 0.5} \\ \hline
YOLOv3 & EfficientNet-B0 & $416\times416$ & VOC 2007 \& 2012 & 81.63 \\
YOLOv3 & EfficientNet-B1 & $416\times416$ & VOC 2007 \& 2012 & 82.70 \\
YOLOv3 & EfficientNet-B2 & $416\times416$ & VOC 2007 \& 2012 & 82.95 \\ \hline
YOLOv4 & MobileNetv1 & $416\times416$ & VOC 2007 \& 2012 & 79.72 \\
YOLOv4 & MobileNetv2 & $416\times416$ & VOC 2007 \& 2012 & 80.12 \\
YOLOv4 & MobileNetv3 & $416\times416$ & VOC 2007 \& 2012 & 79.01 \\
YOLOv4 & GhostNet & $416\times416$ & VOC 2007 \& 2012 & 78.69 \\
YOLOv4 & VGG & $416\times416$ & VOC 2007 \& 2012 & 80.58 \\
YOLOv4 & DenseNet121 & $416\times416$ & VOC 2007 \& 2012 & 83.99 \\
YOLOv4 & ResNet50 & $416\times416$ & VOC 2007 \& 2012 & 84.24 \\ \hline
\end{tabular}}
\end{center}
\end{table*}

\begin{table*}
\begin{center}
\caption{Object detection results for different YOLO models on the COCO 2017 test set.}
\label{tab_coco}
\setlength{\tabcolsep}{3mm}{
\begin{tabular}{cccccc}
\hline
\textbf{Method} & \textbf{Params} & \textbf{FLOPs} & \textbf{Input Image Size} & \textbf{mAP 0.5:0.95} & \textbf{mAP 0.5} \\ \hline
YOLOv5-v5.0-S & 7.3M & 17.0G & $640\times640$ & 35.60 & 53.90 \\
YOLOv5-v5.0-M & 21.4M & 51.3G & $640\times640$ & 43.90 & 62.60 \\
YOLOv5-v5.0-L & 47.0M & 115.4G & $640\times640$ & 47.40 & 66.20 \\
YOLOv5-v5.0-X & 87.7M & 218.8G & $640\times640$ & 49.40 & 67.90 \\ \hline
YOLOv5-v6.1-N & 1.9M & 4.5G & $640\times640$ & 27.60 & 45.00 \\
YOLOv5-v6.1-S & 7.2M & 16.5G & $640\times640$ & 37.00 & 56.20 \\
YOLOv5-v6.1-M & 21.2M & 49.0G & $640\times640$ & 44.70 & 63.40 \\
YOLOv5-v6.1-L & 46.5M & 109.1G & $640\times640$ & 48.40 & 66.60 \\
YOLOv5-v6.1-X & 86.7M & 205.7G & $640\times640$ & 50.10 & 68.30 \\ \hline
YOLOX-Nano & 0.91M & 1.08G & $640\times640$ & 27.40 & 44.50 \\
YOLOX-Tiny & 5.06M & 6.45G & $640\times640$ & 34.70 & 53.60 \\
YOLOX-S & 9.0M & 26.8G & $640\times640$ & 38.20 & 57.70 \\
YOLOX-M & 25.3M & 73.8G & $640\times640$ & 44.80 & 63.90 \\
YOLOX-L & 54.2M & 155.6G & $640\times640$ & 47.90 & 66.60 \\
YOLOX-X & 99.1M & 281.9G & $640\times640$ & 49.00 & 67.70 \\ \hline
YOLOv7-Tiny & 6.2M & 5.8G & $640\times640$ & 36.80 & 54.40 \\
YOLOv7 & 36.9M & 104.7G & $640\times640$ & 50.70 & 69.20 \\
YOLOv7-X & 71.3M & 189.9G & $640\times640$ & 52.40 & 70.50 \\ \hline
\end{tabular}}
\end{center}
\end{table*}

\subsection{Evaluation Metric}
\subsubsection{Mean Average Precision}
Mean Average Precision (mAP) is a metric for evaluating models in object detection tasks, and the equation is shown below:

\begin{equation}
mAP = \frac{1}{\lvert Q_R \rvert} \sum_{r \in Q_{R}} AP(r).
\end{equation}
Precision is the proportion of predicted correct results to all predicted results; Recall is the proportion of predicted correct results to all correct results, and they can be expressed as follows:

\begin{equation}
\frac{}{} \begin{aligned}
\text {Precision}& = \frac{TP}{(TP+FP)}, \\
\text {Recall}& = \frac{TP}{TP+FN},
\end{aligned}    
\end{equation}
where TP is True Positive; TN is True Negativ; FP is False Positive, and FN is False Negative.

Intersection over Union (IoU) is used to measure the extent of the match between the detection result and the true object border. In PASCAL VOC dataset, the prediction is correct while IoU $\geq$ 0.5. In COCO dataset, mAP 0.5:0.95 means the average of all results between 0.5 and 0.95 for IoU.

\subsubsection{Peak Signal-to-Noise Ratio}
Peak Signal-to-Noise Ratio (PSNR) is used to assess the degree of image distortion. When PSNR value is higher, the quality of the distorted image is closer to the original image:

\begin{equation}
PSNR=10 \log _{10}\left(\frac{MAX_{I}^{2}}{MSE}\right),
\end{equation}
where $MSE$ is the mean-square error of the two images, and $MAX_I$ is the maximum value of the image pixels.

In general, while PSNR value exceeds 28dB, the naked eyes cannot tell the difference. When PSNR value exceeds 30dB, the quality of the image is classified as good.

\begin{table*}[]
\begin{center}
\caption{Quantitative comparison (average PSNR/SSIM) of image SR on benchmark datasets using different methods.}
\label{tab_ablation}
\setlength{\tabcolsep}{4.5mm}{
\begin{tabular}{cccccc}
\hline
\multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Connection\\ Method\end{tabular}}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Dense Swin\\ Transformer Block\\ (DSTB)\end{tabular}}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Interval Dense Swin\\ Transformer Block\\ (IDSTB)\end{tabular}}} \\
\hline
\multicolumn{2}{c}{\textbf{Block}} & 5 & 7 & 5 & 7 \\
\hline
\multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Swin Transformer}\\ \textbf{Layer}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}10,12,14,\\ 12,10\end{tabular} & \begin{tabular}[c]{@{}c@{}}8,10,12,14,\\ 12,10,8\end{tabular} & \begin{tabular}[c]{@{}c@{}}10,12,14,\\ 12,10\end{tabular} & \begin{tabular}[c]{@{}c@{}}8,10,12,14,\\ 12,10,8\end{tabular} \\
\hline
\multirow{3}{*}{\textbf{Set5}} & Inference Time & 271.89ms & 272.27ms & \textbf{267.85ms} & 269.86ms \\
 & PSNR & 30.699 & 30.692 & \textbf{30.778} & 30.693 \\
 & SSIM & 0.868 & 0.869 & \textbf{0.871} & 0.868 \\
 \hline
\multirow{3}{*}{\textbf{Set14}} & Inference Time & 511.80ms & 513.84ms & \textbf{504.91ms} & 512.89ms \\
 & PSNR & 27.677 & 27.678 & \textbf{27.738} & 27.662 \\
 & SSIM & 0.757 & 0.758 & \textbf{0.760} & 0.758 \\
 \hline
\multirow{3}{*}{\textbf{BSD100}} & Inference Time & 422.83ms & 439.42ms & \textbf{414.53ms} & 426.69ms \\
 & PSNR & 26.976 & 26.983 & \textbf{27.071} & 26.974 \\
 & SSIM & 0.715 & 0.716 & \textbf{0.717} & 0.716 \\
 \hline
\multirow{3}{*}{\textbf{Urban100}} & Inference Time & 1765.01ms & 1787.84ms & \textbf{1742.43ms} & 1778.38ms \\
 & PSNR & 24.552 & 24.560 & \textbf{24.640} & 24.539 \\
 & SSIM & 0.726 & 0.728 & \textbf{0.732} & 0.727 \\
 \hline
\end{tabular}}
\end{center}
\end{table*}

\subsubsection{Structural Similarity}
Structural Similarity (SSIM) Index is used to compare the similarity of two images in terms of luminance, contrast, and structure.

\begin{equation}
SSIM = L(x,y) \times C(x,y) \times S(x,y).
\end{equation}

\subsection{Experiment Setup}
\label{setup}
We train our proposed model SwinOIR on DIV2K \cite{Ignatov_2018_ECCV_Workshops} and Flickr2K datasets and evaluate them on Set5 \cite{bevilacqua2012low}, Set14 \cite{zeyde2012single}, BSD100 \cite{martin2001database}, and Urban100 \cite{huang2015single} datasets. For the training models, this work chooses optimizer Adam with $\beta 1$ = 0.9 and $\beta 2$ = 0.99, and a default weight decay of 0. Learning rate is  originally set to $10^{-4}$ and decreases by 2 per 500 epochs. We use NVIDIA GeForce RTX 3090 GPU and PyTorch 1.9 to train all models, each with 2000 epochs and batch size is set to 64.

\subsection{Ablation Study}
\label{ablation}
To demonstrate the positive effect of the Interval Dense Connection in the Swin Transformer Block (STB), we conduct ablation experiments on different network models by using Interval Dense Swin Transformer Block (IDSTB) and Dense Swin Transformer Block (DSTB). The Dense Connection was proposed by DenseNet \cite{huang2017densely}, which improves the backpropagation of gradients and makes the network easier to train. As shown in Table \ref{tab_ablation}, this work uses Dense Connection and Interval Dense Connection for the Swin Transformer Block, and designs network models for both the 5-block and 7-block architectures. The training details of all the models are described in Section \ref{setup}.

The results of the evaluation on four benchmark datasets, Set5, Set14, BSD100 and Urban100, show that the model with 5 IDSTB has the best performance. It has shorter inference time per image than the model with DSTB, and its PSNR and SSIM values are the highest of all models. In general, the performance of the deeper network model is better, but the model with 7 IDSTB is not as good as the model with 5 IDSTB, which we consider it is due to overfitting.

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{figure_panoramic.pdf}
  \caption{Example of test results of the two-stage framework on panoramic images.}
  \label{fig_panoramic}
\end{figure*}

\subsection{Experimental Results}
For object detection task, this work evaluates different versions of YOLO algorithms on the Pascal VOC and MS COCO datasets respectively. As shown in Table \ref{tab_voc}, we use YOLOv3 and YOLOv4 detection algorithms in combination with the classical backbone neural networks. All network models are trained on the VOC 2007 and 2012 training sets and evaluate on the VOC 2007 test set. For input images of $416 \times 416$ size, YOLOv4 with ResNet50 achieves the highest mAP value of 84.24. Since the network architecture of ResNet is of simplicity, it is often used in industry projects. Our experiments show that YOLOv4 with ResNet50 model has excellent performance on the object detection tasks and is suitable for application in real-life scenarios.

On the COCO dataset, we compare the detection algorithms of YOLOv5, YOLOX, and YOLOv7, as shown in Table \ref{tab_coco}. The smaller network models are suitable for running on the low computational power platforms, such as YOLOv5-v6.1-N and YOLOX-Nano, which have 1.9M and 0.91M parameters respectively. However, since the models are small, their mAP values are relatively low, only 45.00 and 44.50 respectively. The large model YOLOv7-X has 71.3M parameters with the highest mAP value of 70.50, which is suitable for use in projects where accuracy is required.

We evaluate the performance of the overall framework on the BSD300 and PIRM datasets, as shown in Fig. \ref{fig_result_bsds}. Image 33 is a real-life mountain climbing image and our framework first detects all the people in the image. The cropped image of the person is very blurred and we obtain high clear image of the person by using image super-resolution techniques. Image 63 and Image 84 are images of a dog and a cow respectively. We crop the animals from the background and improve the resolution of the images of animals. The images in the PIRM dataset have a high resolution, as shown in Fig. \ref{fig_result_pirm}, but the details of the small objects are still difficult to observe. Our framework solves this problem well, such as the car in Image 119082 and the person in Image 37073.

In addition, for the panoramic image, we randomly found a bridge view image taken at a distance, as shown in Fig. \ref{fig_panoramic}. The framework first detected all the people and increased the resolution of the image for each person, so that we could observe well that an old woman holding a child, the lady with a sunshade, and the person taking photos. These experiments demonstrate that our framework is well suited to real-life scenarios, including panoramic images.

\section{Conclusion}
This work proposes a two-stage framework consisting object detection task and image restoration task, which uses YOLO series algorithms for object detection, then crops the detected object images, and uses our proposed model to perform image super-resolution. We use interval dense connections for Swin Transformer layer to improve model performance. The newly proposed SwinOIR model performs $ \times$4 image super-resolution on the cropped object image to increase the pixel size of the image and make it easier to recognize the details of the object. The overall framework is suitable for solving real-life problems, such as real-time video surveillance and panoramic images.

\section{Declarations}
\subsection{Funding}
This research work was supported in part by the National Science and Technology Council, Taiwan, under grant number: NSTC 111-2221-E032-021-.

\subsection{Competing interests}
The authors have no financial or proprietary interests in any material discussed in this article.

\subsection{Ethics approval}
This research does not involve human participants and/or animals.

\bibliographystyle{spmpsci}
\bibliography{reference}

\end{document}
