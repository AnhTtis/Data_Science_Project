{
    "arxiv_id": "2303.11552",
    "paper_title": "Boosting Verified Training for Robust Image Classifications via Abstraction",
    "authors": [
        "Zhaodi Zhang",
        "Zhiyi Xue",
        "Yang Chen",
        "Si Liu",
        "Yueling Zhang",
        "Jing Liu",
        "Min Zhang"
    ],
    "submission_date": "2023-03-21",
    "revised_dates": [
        "2023-03-22"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "This paper proposes a novel, abstraction-based, certified training method for robust image classifiers. Via abstraction, all perturbed images are mapped into intervals before feeding into neural networks for training. By training on intervals, all the perturbed images that are mapped to the same interval are classified as the same label, rendering the variance of training sets to be small and the loss landscape of the models to be smooth. Consequently, our approach significantly improves the robustness of trained models. For the abstraction, our training method also enables a sound and complete black-box verification approach, which is orthogonal and scalable to arbitrary types of neural networks regardless of their sizes and architectures. We evaluate our method on a wide range of benchmarks in different scales. The experimental results show that our method outperforms state of the art by (i) reducing the verified errors of trained models up to 95.64%; (ii) totally achieving up to 602.50x speedup; and (iii) scaling up to larger models with up to 138 million trainable parameters. The demo is available at https://github.com/zhangzhaodi233/ABSCERT.git.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11552v1"
    ],
    "publication_venue": "Accepted to CVPR 2023"
}