\renewcommand\arraystretch{1.5}
\section{Experiment}\label{sec:expri}

\begin{table*}[ht!]\Huge
    \centering
    \caption{Verified errors (\%) of models trained by LossLandscapeMatters (LLM), LBP\&Ramp (LBP), AdvIBP (AIBP), and \textsc{AbsCert} (AC) on MNIST and CIFAR-10 datasets. 
    ``--'' means that the publicly available code of LBP does not support the CIFAR-10 dataset.}
    \vspace{-2mm}
    \resizebox{\textwidth}{23mm} {
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
        \hline
        \multirow{2}{*}{\textbf{Dataset}} &
        \multirow{2}{*}{\textbf{$\epsilon$}} &
        \multicolumn{7}{c|}{\textbf{DM-small}} &
        \multicolumn{7}{c|}{\textbf{DM-medium}} &
        \multicolumn{7}{c}{\textbf{DM-large}} \\
        \cline{3-23}
         & & \makebox[0.07\textwidth][c]{AC} & 
         \makebox[0.07\textwidth][c]{LLM} & 
         \makebox[0.07\textwidth][c]{Impr.(\%)} & 
         \makebox[0.07\textwidth][c]{LBP} & 
         \makebox[0.07\textwidth][c]{Impr.(\%)} & 
         \makebox[0.07\textwidth][c]{AIBP} & 
         \makebox[0.07\textwidth][c]{Impr.(\%)} & 
         \makebox[0.07\textwidth][c]{AC} & 
         \makebox[0.07\textwidth][c]{LLM} & 
         \makebox[0.07\textwidth][c]{Impr.(\%)} & 
         \makebox[0.07\textwidth][c]{LBP} & 
         \makebox[0.07\textwidth][c]{Impr.(\%)} &
         \makebox[0.07\textwidth][c]{AIBP} &
         \makebox[0.07\textwidth][c]{Impr.(\%)} &
         \makebox[0.07\textwidth][c]{AC} &
         \makebox[0.07\textwidth][c]{LLM} &
         \makebox[0.07\textwidth][c]{Impr.(\%)} &
         \makebox[0.07\textwidth][c]{LBP} &
         \makebox[0.07\textwidth][c]{Impr.(\%)} &
         \makebox[0.07\textwidth][c]{AIBP} &
         \makebox[0.07\textwidth][c]{Impr.(\%)} \\
        \hline
        \multirow{4}{*}{MNIST}
        & 0.1 & 0.89 & 3.02 & \cellcolor{lo1} 70.53 $\uparrow$
                     & 4.09 & \cellcolor{lo1} 78.24 $\uparrow$
                     & 3.34 & \cellcolor{lo1} 73.35 $\uparrow$
              & 0.69 & 2.67 & \cellcolor{lo1} 74.16 $\uparrow$
                     & 3.47 & \cellcolor{lo2} 80.12 $\uparrow$
                     & 3.62 & \cellcolor{lo2} 80.94 $\uparrow$
              & 0.52 & 2.29 & \cellcolor{lo1} 77.29 $\uparrow$
                     & 2.93 & \cellcolor{lo2} 82.25 $\uparrow$
                     & 3.66 & \cellcolor{lo2} 85.79 $\uparrow$\\
        %\cline{2-23}
        & 0.2 & 0.94 & 6.04 & \cellcolor{lo2} 84.44 $\uparrow$
                     & 5.54 & \cellcolor{lo2} 83.03 $\uparrow$
                     & 5.96 & \cellcolor{lo2} 84.23 $\uparrow$
              & 0.70 & 5.10 & \cellcolor{lo2} 86.27 $\uparrow$
                     & 4.73 & \cellcolor{lo2} 85.20 $\uparrow$
                     & 6.05 & \cellcolor{lo2} 88.43 $\uparrow$
              & 0.61 & 4.38 & \cellcolor{lo2} 86.07 $\uparrow$
                     & 3.96 & \cellcolor{lo2} 84.60 $\uparrow$
                     & 5.89 & \cellcolor{lo2} 89.64 $\uparrow$\\ 
        %\cline{2-23}
        & 0.3 & 1.01 & 12.48 & \cellcolor{lo3} 91.91 $\uparrow$
                     & 8.11  & \cellcolor{lo2} 87.55 $\uparrow$
                     & 12.16 & \cellcolor{lo3} 91.69 $\uparrow$
              & 0.77 & 11.75 & \cellcolor{lo3} 93.45 $\uparrow$
                     & 7.02  & \cellcolor{lo2} 89.03 $\uparrow$
                     & 9.61  & \cellcolor{lo3} 91.99 $\uparrow$
              & 0.64 & 10.38 & \cellcolor{lo3} 93.83 $\uparrow$
                     & 6.14  & \cellcolor{lo2} 89.58 $\uparrow$
                     & 8.76  & \cellcolor{lo3} 92.69 $\uparrow$\\
        %\cline{2-23}
        & 0.4 & 1.22 & 20.51 & \cellcolor{lo3} 94.05 $\uparrow$
                     & 13.03 & \cellcolor{lo3} 90.64 $\uparrow$
                     & 20.69 & \cellcolor{lo3} 94.10 $\uparrow$
              & 0.93 & 19.04 & \cellcolor{lo3} 95.12 $\uparrow$
                     & 11.59 & \cellcolor{lo3} 91.98 $\uparrow$
                     & 17.33 & \cellcolor{lo3} 94.63 $\uparrow$
              & 0.77 & 15.71 & \cellcolor{lo3} 95.10 $\uparrow$
                     & 10.48 & \cellcolor{lo3} 92.65 $\uparrow$
                     & 17.68 & \cellcolor{lo3} 95.64 $\uparrow$\\
        \hline
        \multirow{5}{*}{CIFAR-10}
        & $2/255$ & 25.52 & 50.95 & \cellcolor{lo1} 49.91 $\uparrow$
                                  & --    & -- 
                                  & 57.20 & \cellcolor{lo1} 55.38 $\uparrow$
                          & 16.40 & 49.83 & \cellcolor{lo1} 67.09 $\uparrow$
                                  & --     & -- 
                                  & 54.21 & \cellcolor{lo1} 69.75 $\uparrow$
                          & 13.81 & 48.20 & \cellcolor{lo1} 71.35 $\uparrow$
                                  & -- & --
                                  & 54.39 & \cellcolor{lo1} 74.61 $\uparrow$ \\
        %\cline{2-23}
        & $4/255$ & 25.52 & 61.90 & \cellcolor{lo1} 58.77 $\uparrow$
                                  & --     & -- 
                                  & 65.30     & \cellcolor{lo1} 60.92 $\uparrow$
                          & 16.40 & 61.46 & \cellcolor{lo1} 73.32 $\uparrow$
                                  & --     & -- 
                                  & 62.63     & \cellcolor{lo1} 73.81 $\uparrow$ 
                          & 13.81 & 61.22 & \cellcolor{lo1} 77.44 $\uparrow$
                                  & --     & --
                                  & 61.95 & \cellcolor{lo1} 77.71 $\uparrow$ \\
        %\cline{2-23}
        & $6/255$ & 25.52 & 68.36 & \cellcolor{lo1} 62.67 $\uparrow$
                                  & --    & --   
                                  & 70.20     & \cellcolor{lo1} 63.65 $\uparrow$
                          & 16.70 & 67.28 & \cellcolor{lo1} 75.18 $\uparrow$
                                  & --     & --
                                  & 67.69     & \cellcolor{lo1} 75.33 $\uparrow$ 
                          & 13.88 & 66.99 & \cellcolor{lo1} 79.28 $\uparrow$
                                  & --     & --
                                  & 67.56 & \cellcolor{lo1} 79.46 $\uparrow$ \\
        %\cline{2-23}
        & $8/255$ & 25.52 & 71.92 & \cellcolor{lo1} 64.52 $\uparrow$
                                  & --     & -- 
                                  & 72.50 & \cellcolor{lo1} 64.80 $\uparrow$
                          & 16.93 & 70.54 & \cellcolor{lo1} 76.00 $\uparrow$
                                  & --    & --
                                  & 70.75 & \cellcolor{lo1} 76.07 $\uparrow$
                          & 13.88 & 70.35 & \cellcolor{lo2} 80.27 $\uparrow$
                                  & -- & --
                                  & 70.72 & \cellcolor{lo2} 80.37 $\uparrow$ \\
        %\cline{2-23}
        & $16/255$ & 26.61 & 78.13 & \cellcolor{lo1} 65.94 $\uparrow$
                                   & --     & --  
                                   & 78.90 & \cellcolor{lo1} 66.27 $\uparrow$
                           & 17.16 & 78.27 & \cellcolor{lo1} 78.08 $\uparrow$
                                   & --     & --
                                   & 78.33 & \cellcolor{lo1} 78.09 $\uparrow$
                           & 14.12 & 78.03 & \cellcolor{lo2} 81.90 $\uparrow$
                                   & -- & -- 
                                   & 78.31 & \cellcolor{lo2} 81.97 $\uparrow$ \\
        \hline
        
    \end{tabular}
    }
    \label{tab:exp1}
    \vspace{-2mm}
\end{table*}

\begin{table}
    \centering
    \caption{Accuracy (\%) of models
    trained by LLM, LBP, AIBP, and \textsc{AC} (our method) on MNIST and CIFAR-10.
    % ``--'' means that the publicly available code of LBP does not support  CIFAR-10.
    }
    \vspace{-2mm}
    \setlength{\tabcolsep}{7.6pt}
    \scriptsize
    	\renewcommand{\arraystretch}{1.2}
    % \resizebox{\textwidth}{20mm} {
    \begin{tabular}{c|c|c|c|c|c|c}
        \hline
          &  \textbf{Dataset} & \textbf{$\epsilon$} & AC & LLM & LBP & AIBP  \\
        \hline
        \multirow{9}{*}{\rotatebox{90}{DM-small}}  
		  & \multirow{4}{*}{MNIST} 
		    & 0.1 & \cellcolor{lo1} 99.11 & 98.43 & 96.63 & 98.36 \\
	      & & 0.2 & \cellcolor{lo1} 99.06 & 97.15 & 96.94 & 97.89  \\ 
	      & & 0.3 & \cellcolor{lo1} 98.99 & 94.38 & 96.65 & 96.35  \\
	      & & 0.4 & \cellcolor{lo1} 98.78 & 94.46 & 96.65 & 96.14 \\
		\cline{2-7}
	      & \multirow{5}{*}{CIFAR-10} 
		    & $2/255$ & \cellcolor{lo1} 74.48 & 64.70 & -- & 59.20   \\
	      & & $4/255$ & \cellcolor{lo1} 74.48 & 55.07 & -- & 49.69    \\
	      & & $6/255$ & \cellcolor{lo1} 74.48 & 49.29 & -- & 41.94    \\
	      & & $8/255$ & \cellcolor{lo1} 74.48 & 44.34 & -- & 39.52 \\
	      & & $16/255$ & \cellcolor{lo1} 73.39 & 32.58 & -- & 30.74 \\
	    \hline
		\multirow{9}{*}{\rotatebox{90}{DM-medium}}
		  & \multirow{4}{*}{MNIST}
            & 0.1 & \cellcolor{lo1} 99.31 & 98.76 & 97.37 & 98.65 \\
          & & 0.2 & \cellcolor{lo1} 99.30 & 98.13 & 97.36 & 98.42  \\ 
          & & 0.3 & \cellcolor{lo1} 99.23 & 95.04 & 97.35 & 97.45  \\
          & & 0.4 & \cellcolor{lo1} 99.07 & 93.60 & 97.36 & 97.44 \\
		\cline{2-7}
          & \multirow{5}{*}{CIFAR-10}
            & $2/255$ & \cellcolor{lo1} 83.60 & 66.00 & -- & 62.04   \\
          & & $4/255$ & \cellcolor{lo1} 83.60 & 55.09 & -- & 52.37    \\
          & & $6/255$ & \cellcolor{lo1} 83.30 & 48.38 & -- & 46.26    \\
          & & $8/255$ & \cellcolor{lo1} 83.07 & 41.19 & -- & 40.91 \\
          & & $16/255$ & \cellcolor{lo1} 82.84 & 33.65 & -- & 31.32 \\
        \hline
		\multirow{9}{*}{\rotatebox{90}{DM-large}}
		  & \multirow{4}{*}{MNIST}
            & 0.1 & \cellcolor{lo1} 99.48 & 98.95 & 97.79 & 98.96 \\
          & & 0.2 & \cellcolor{lo1} 99.39 & 98.41 & 97.79 & 98.47  \\ 
          & & 0.3 & \cellcolor{lo1} 99.36 & 95.90 & 97.77 & 98.05  \\
          & & 0.4 & \cellcolor{lo1} 99.23 & 96.14 & 97.79 & 97.78 \\
		\cline{2-7}
          & \multirow{5}{*}{CIFAR-10}
            & $2/255$ & \cellcolor{lo1} 86.19 & 62.50 & -- & 62.33   \\
          & & $4/255$ & \cellcolor{lo1} 86.19 & 56.99 & -- & 52.73    \\
          & & $6/255$ & \cellcolor{lo1} 86.12 & 49.58 & -- & 45.13    \\
          & & $8/255$ & \cellcolor{lo1} 86.12 & 43.26 & -- & 41.54 \\
          & & $16/255$ & \cellcolor{lo1} 85.88 & 33.03 & -- & 30.19 \\
        \hline
    \end{tabular}
    % }
    \label{tab:small}
\end{table}

We have implemented both our training and verification approaches in a tool called \textsc{AbsCert}. 
We evaluate \textsc{AbsCert}, together with existing approaches, on various public benchmarks with respect to both \emph{verified error} and \emph{training and verification time}.

By comparing with the state of the art, our goal is to demonstrate that
%\begin{adjustwidth}{0.5cm}{0.5cm}
    \textsc{AbsCert} can train neural networks with lower verified errors (\textbf{Experiment \uppercase\expandafter{\romannumeral1}}),  incurs less computation overhead in both  training and verification (\textbf{Experiment \uppercase\expandafter{\romannumeral2}}), is applicable to a wide range of neural network architectures (\textbf{Experiment \uppercase\expandafter{\romannumeral3}}), and can scale up to larger models (\textbf{Experiment \uppercase\expandafter{\romannumeral4}}). 
%\end{adjustwidth}

\subsection{Benchmarks and Experimental Setup}
\noindent
\textbf{Competitors.} 
We consider three state-of-the-art provably robust training methods:
LossLandscapeMatters~\cite{DBLP:conf/nips/LeeLPL21}, LBP\&Ramp~\cite{DBLP:conf/cvpr/LyuGWXZL21}, and AdvIBP~\cite{DBLP:conf/aaai/FanL21}. 
All of them rely on linear approximation to train certifiably robust neural networks, which minimizes the upper bound on the worst-case loss against $l_\infty$ perturbations.  
We use their predefined optimal hyper-parameters  to train
neural networks on different perturbations $\epsilon$. 



\begin{table*}[]\Large
    \centering
    \caption{Training time (s/epoch) and verification time (s) of LossLandscapeMatters (LLM), LBP\&Ramp (LBP), AdvIBP (AIBP), and \textsc{AbsCert} (AC). 
    ``--'' means that the publicly available code of  LBP does not support the CIFAR-10 dataset.}
    \vspace{-2mm}
    %\resizebox{\textwidth}{19mm} {
    \resizebox{\textwidth}{!} {
    %\setlength{\tabcolsep}{0.05mm} {
        \begin{tabular}{c|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
            \hline
            \multirow{2}{*}{\textbf{Dataset}} &
            \multirow{2}{*}{\hspace{6mm}\textbf{Model}} &
            \multicolumn{7}{c}{\textbf{Training Time}} & 
            \multicolumn{7}{|c}{\textbf{Verification Time}} & 
            \multicolumn{3}{|c}{\textbf{Total Speedup}}\\
            \cline{3-19}
            & & 
            \makebox[0.08\textwidth][c]{AC} &
            \makebox[0.08\textwidth][c]{LLM} &
            \makebox[0.08\textwidth][c]{SpeedUp} &
            \makebox[0.08\textwidth][c]{LBP} &
            \makebox[0.08\textwidth][c]{SpeedUp} &
            \makebox[0.08\textwidth][c]{AIBP} &
            \makebox[0.08\textwidth][c]{SpeedUp} &
            \makebox[0.08\textwidth][c]{AC} &
            \makebox[0.08\textwidth][c]{LLM} &
            \makebox[0.08\textwidth][c]{SpeedUp} &
            \makebox[0.08\textwidth][c]{LBP} &
            \makebox[0.08\textwidth][c]{SpeedUp} &
            \makebox[0.08\textwidth][c]{AIBP} &
            \makebox[0.08\textwidth][c]{SpeedUp} &
            \makebox[0.08\textwidth][c]{LLM} &
            \makebox[0.08\textwidth][c]{LBP} &
            \makebox[0.08\textwidth][c]{AIBP}\\
            \hline
            \multirow{3}{*}{MNIST} 
            & DM-small & 2.18 & 13.40 & \cellcolor{lo1} 6.14 $\uparrow$
                              & 6.54  & \cellcolor{lo1} 2.99 $\uparrow$
                              & 12.46 & \cellcolor{lo1} 5.72 $\uparrow$ 
                       & 0.54 & 5.77  & \cellcolor{lo2} 10.69 $\uparrow$
                              & 66.39 & \cellcolor{lo3} 122.94 $\uparrow$
                              & 2.35 & \cellcolor{lo1} 4.35 $\uparrow$ 
                              & \cellcolor{lo1} 7.05 $\uparrow$ & \cellcolor{lo2} 26.81 $\uparrow$ & \cellcolor{lo1} 5.44 $\uparrow$\\
            & DM-medium & 2.32 & 35.17 & \cellcolor{lo2} 15.18 $\uparrow$
                               & 11.99 & \cellcolor{lo1} 5.18  $\uparrow$
                               & 34.72 & \cellcolor{lo2} 14.97 $\uparrow$ 
                        & 0.56 & 6.19 & \cellcolor{lo2} 11.05 $\uparrow$ 
                               & 129.18  & \cellcolor{lo3} 230.68 $\uparrow$
                               & 2.63 & \cellcolor{lo1} 4.70 $\uparrow$ 
                               & \cellcolor{lo2} 14.36 $\uparrow$ & \cellcolor{lo2} 49.02 $\uparrow$ & \cellcolor{lo2} 12.97 $\uparrow$\\
            & DM-large & 4.28 & 113.14 & \cellcolor{lo2} 26.43 $\uparrow$
                              & 36.14  & \cellcolor{lo1} 8.44  $\uparrow$
                              & 99.89  & \cellcolor{lo2} 23.34 $\uparrow$ 
                       & 0.56 & 15.11 & \cellcolor{lo2} 26.98 $\uparrow$
                              & 337.40 & \cellcolor{lo3} 602.50 $\uparrow$
                              & 3.79 & \cellcolor{lo1} 6.77 $\uparrow$ 
                              & \cellcolor{lo2} 26.50 $\uparrow$ & \cellcolor{lo2} 77.18 $\uparrow$ & \cellcolor{lo2} 21.42 $\uparrow$\\
            \hline
            \multirow{3}{*}{CIFAR-10}
             & DM-small  & 3.31 & 15.00 & \cellcolor{lo1} 4.54 $\uparrow$
                         & -- & -- 
                         & 13.76 & \cellcolor{lo1} 4.16 $\uparrow$
                         & 0.64 & 7.59 & \cellcolor{lo2} 11.86 $\uparrow$
                         & -- & -- 
                         & 3.53 & \cellcolor{lo1} 5.52 $\uparrow$
                         & \cellcolor{lo1} 5.72 $\uparrow$ & -- & \cellcolor{lo1} 4.38 $\uparrow$\\
             & DM-medium & 3.58 & 31.40 & \cellcolor{lo1} 8.77 $\uparrow$ 
                         & -- & --
                         & 35.84 & \cellcolor{lo2} 10.01 $\uparrow$
                         & 0.66 & 7.68 & \cellcolor{lo2} 11.64 $\uparrow$ 
                         & -- & -- 
                         & 3.99 & \cellcolor{lo1} 6.04 $\uparrow$ 
                         & \cellcolor{lo1} 9.22 $\uparrow$ & -- & \cellcolor{lo1} 9.39 $\uparrow$\\
             & DM-large  & 5.29 & 123.22 & \cellcolor{lo2} 23.29 $\uparrow$ 
                         & -- & --
                         & 163.31 & \cellcolor{lo2} 30.87 $\uparrow$ 
                         & 0.76 & 18.45 & \cellcolor{lo2} 24.28 $\uparrow$
                         & -- & -- 
                         & 6.60 & \cellcolor{lo1} 8.68 $\uparrow$ 
                         & \cellcolor{lo2} 23.42 $\uparrow$ & -- & \cellcolor{lo2} 28.08 $\uparrow$\\
             \hline
        \end{tabular}
    %}
    %}
    }
    \vspace{-2mm}
    \label{tab:exp2}
\end{table*}


\vspace{1ex}
\noindent
\textbf{Datasets and Networks.}
We conduct our experiments on 
MNIST \cite{lecun1998gradient}, CIFAR-10 \cite{krizhevsky2009learning}, and ImageNet~\cite{DBLP:conf/cvpr/DengDSLL009}. 
For  MNIST and CIFAR-10, we train and verify all three convolutional neural networks (CNNs), i.e., DM-small, DM-medium, and DM-large \cite{DBLP:journals/corr/abs-1810-12715, DBLP:conf/iclr/ZhangCXGSLBH20}, and two fully connected neural networks 
 (FNNs) with three and five layers, respectively. 
The batch size is set 128. We use cross-entropy loss function and Adam~\cite{DBLP:journals/corr/KingmaB14} optimizer to update the parameters. The learning rate decreases following the values of the cosine function between $0$ and $\pi$ after a warmup period during which it increases linearly between $0$ and $1$~\cite{DBLP:conf/emnlp/WolfDSCDMCRLFDS20}. 

For ImageNet, we use the  AlexNet \cite{DBLP:journals/cacm/KrizhevskySH17}, 
 VGG11 \cite{DBLP:journals/corr/SimonyanZ14a}, 
 Inception V1 \cite{DBLP:conf/cvpr/SzegedyLJSRAEVR15}, and 
 ResNet18 \cite{DBLP:conf/cvpr/HeZRS16}
 architectures, which are winners of the image classification competition ILSVRC\footnote{https://image-net.org/challenges/LSVRC/index.php}. We use the same super-parameters as in their original experiments for training these networks. 

\vspace{1ex}
\noindent
\textbf{Metrics.}
We use two metrics in our comparisons: (\romannumeral1) \emph{verified error}, which is the percentage of images
that are not verified to be robust. We   quantify the precision improvement by $(e^{\prime}-e)/e^{\prime}$, with $e$ and $e^{\prime}$ the verified errors of neural networks trained by \textsc{AbsCert} and the competitor, respectively; and  
(\romannumeral2) \emph{time}, which includes training and/or verification on  the same neural network architecture with the same dataset. 
We compute speedup by $t^{\prime}/t$, with $t$ and $t^{\prime}$ the execution time \textsc{AbsCert} and the competitors, respectively.
As the time for different perturbations $\epsilon$ is almost the same, we report the average time (in Table \ref{tab:exp2} and Table \ref{tab:exp4}).

\vspace{2mm}
\noindent
\textbf{Experimental Setup.} 
All experiments on MNIST and CIFAR-10, as well as AlexNet were conducted on a workstation running Ubuntu 18.04 with one NVIDIA GeForce RTX 3090 Ti GPU. All experiments on VGG11, Inception V1 and ResNet18 were conducted on a workstation running Windows11 with one NVIDIA GeForce RTX 3090 Ti GPU.


\subsection{Experimental Results}
\label{subsec:exp-results}

\noindent \textbf{Experiment \uppercase\expandafter{\romannumeral1}: Effectiveness.}
Table \ref{tab:exp1} shows the comparison results of verified errors for DM-small, DM-medium, and DM-large. \textsc{AbsCert} achieves lower verified errors than the competitors. On MNIST, we obtain up to $95.12\%$, $92.65\%$, and $95.64\%$ improvements over LLM, LBP, and AdvIBP on three neural network models (with $\epsilon$ = 0.4), respectively. On CIFAR-10, we achieve a $81.9\%$ improvement to LLM with $\epsilon=16/255$, and all the improvements are above $49\%$. Note that the publicly available code of LBP does not support CIFAR-10. 
 
Another observation is that the improvement increases as $\epsilon$ becomes larger. 
This implies that,  under larger perturbations, the verified errors of the models trained by \textsc{AbsCert} 
increase less slowly than those trained by the competing approaches.
This reflects that the models trained by \textsc{AbsCert} are more robust than those trained by the three competitors. 

As for the accuracy of the trained networks, Table \ref{tab:small} shows that our method achieves
\textit{higher accuracy} than the competitors for \textit{all} datasets and models under the same perturbations.
Moreover, the decrease speed is much less than the one of the networks trained in competitors. Namely,  \textsc{AbsCert} can better resist perturbations, and the models trained by \textsc{AbsCert} have stronger robustness guarantees.

\vspace{1ex}
\noindent \textbf{Experiment \uppercase\expandafter{\romannumeral2}: Efficiency.} Table \ref{tab:exp2} shows the average training and verification time.
\textsc{AbsCert} consumes less training time than the competitors for \emph{all} datasets and models; in particular, compared to LLM, our method achieves up to 26.43x speedup on DM-large of MNIST. Additionally, LBP and AdvIBP can hardly be applied to CIFAR-10 (3200 epochs required), while \textsc{AbsCert} runs smoothly (needs only 30 epochs). This indicates less time (and memory) overhead for \textsc{AbsCert}. 


Regarding the verification overhead, \textsc{AbsCert} achieves up to 602.5x speedup and is scalable to large models trained on CIFAR-10. That is mainly because our verification approach treats the networks as black boxes thanks to the abstraction-based training method. 
%as it requires only one forward propagation after the abstraction to obtain the results. 

\begin{table}  
    \centering
    \caption{Verified errors (\%) of the non-ReLU models trained by \textsc{AbsCert}. FC-3 and FC-5 denote FNNs with 3 and 5 hidden layers. DM-s and DM-m refer to DM-small and DM-medium. 
    }
    \vspace{-2mm}
\setlength{\tabcolsep}{6pt}
    \resizebox{0.48\textwidth}{!} 
    {
    \begin{tabular}{c|R{1.1cm}|c|c|c|c|c|c|c}
        \hline
        \multirow{2}{*}{\textbf{D.S.}} &
        \multirow{2}{*}{\textbf{$\epsilon$}\hspace{5mm}} &
        \multicolumn{3}{c|}{\textbf{Sigmoid}} &
        \multicolumn{4}{c}{\textbf{Tanh}} \\
        \cline{3-9}
         & & {FC-3} & 
         {FC-5} & 
         {DM-s} & 
          
         {FC-3} & 
         {FC-5} & 
         {DM-s} & 
         {DM-m} \\
        \hline
        \multirow{4}{*}{\rotatebox{90}{MNIST}}
        & 0.1 & 4.23 & 5.73 & 2.28 
              & 7.95  & 10.50 & 1.56 & 1.04  \\
        %\cline{2-23}
        & 0.2 & 4.23 & 5.87 & 2.66 
              & 8.06 & 10.50 & 1.65 & 1.14  \\
        %\cline{2-23}
        & 0.3 & 4.23 & 5.87 & 2.84 
              & 8.29 & 10.50 & 1.75 & 1.14  \\
        %\cline{2-23}
        & 0.4 & 5.09 & 6.23 & 3.12 
              & 9.39  & 13.16 & 1.97 & 1.24  \\
        \hline
        \multirow{4}{*}{\rotatebox{90}{CIFAR-10}}
        & ${2}/{255}$  & 53.49 & 58.95 & 36.22 
                & 56.62 & 60.07 & 46.69 & 30.02  \\
        %\cline{2-23}
        & ${16}/{255}$ & 53.49 & 58.95 & 39.59 
                & 56.62 & 61.30 & 48.14 & 31.02  \\
        %\cline{2-23}
        & ${32}/{255}$ & 54.41 & 60.05 & 41.39 
                & 57.17 & 61.72 & 49.05 & 32.18  \\
        %\cline{2-23}
       & ${64}/{255}$ & 57.65 & 62.77 & 43.09 
                & 60.99 & 65.73 & 51.22 & 35.62  \\
        \hline
        
    \end{tabular}
    }
    \label{tab:exp3}
    \vspace{-1mm}
\end{table}




\vspace{1ex}

\noindent \textbf{Experiment \uppercase\expandafter{\romannumeral3}: Applicability.}  
We show that our approach is applicable to both CNNs and FNNs with various activation functions, such as Sigmoid and Tanh. 
Table \ref{tab:exp3} shows the verified errors for both types of neural networks trained by our approach. We observe that the 
verified errors of those neural networks trained on MNIST (resp. CIFAR-10) datasets are all below 14\% (resp. 66\%), which are smaller than the benchmark counterparts verified in the work ~\cite{DBLP:journals/pacmpl/SinghGPV19}. 

\begin{table}[]
    \caption{Verified errors (\%) and training time (s/epoch) of the large models trained by \textsc{AbsCert} on ImageNet. 
    }
    \vspace{-2mm}
    \centering
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
         \hline
         \multirow{2}{*}{$\epsilon$} &
         \multicolumn{2}{c}{\textbf{AlexNet}} & 
         \multicolumn{2}{|c}{\textbf{VGG11}} & 
         \multicolumn{2}{|c}{\textbf{Inception V1}} &
         \multicolumn{2}{|c}{\textbf{ResNet18}}\\
         \cline{2-9}
          & Error & Time & Error & Time & Error & Time & Error & Time \\
        %  \hline
        %  $0$ & 40.70 & -- & 29.60 & -- & -- & -- & 27.88 & -- \\
         \hline
         $2/255$ & 44.96 & \multirow{3}{*}{508.2}
         & 36.29 & \multirow{3}{*}{2530.3}
         & 41.67 &  \multirow{3}{*}{2183.7}
         & 32.15 & \multirow{3}{*}{212.6} \\
        \cline{1-2}\cline{4-4}\cline{6-6}\cline{8-8}
        $4/255$ & 44.96 & 
         & 36.35 & 
         & 41.67 &
         & 32.25 &  \\
        \cline{1-2}\cline{4-4}\cline{6-6}\cline{8-8}
        $8/255$ & 44.97 & 
         & 36.93 & 
         & 43.12 &  
         & 32.86 &  \\
        \hline
    \end{tabular}
    %\end{tabularx}
    }
%    \vspace{-2mm}
    \label{tab:exp4}
\end{table}

\vspace{1ex}
\noindent \textbf{Experiment \uppercase\expandafter{\romannumeral4}: Scalability.} 
We show our method is scalable with respect to training four larger neural network architectures: AlexNet, VGG11, Inception V1, and ResNet18 on ImageNet. Table \ref{tab:exp4} shows the verified errors and training times.
The number of trainable parameters varies from 11 million to 138 million.  
Compared with the reported errors and training times on those representative large models \cite{DBLP:journals/cacm/KrizhevskySH17, DBLP:journals/corr/SimonyanZ14a, DBLP:conf/cvpr/HeZRS16},
our approach achieves competitive performance. 
It is worth mentioning that the reported errors are computed on the testing sets but not verified because those models are too large and cannot be verified by existing verification methods~\cite{DBLP:journals/pacmpl/SinghGPV19, zhang2018efficient, DBLP:conf/cav/KatzBDJK17, DBLP:conf/nips/WangZXLJHK21, DBLP:conf/nips/WangPWYJ18, DBLP:conf/nips/SinghGPV19} due to the high computational complexity. In contrast, \textsc{AbsCert} can verify fairly large networks for its black-box feature.
