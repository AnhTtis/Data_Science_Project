\section{Formal Verification and Granularity Tuning} 
\label{4}

\begin{algorithm}[t]
    \renewcommand{\thealgocf}{3}
	\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
	\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\caption{\mbox{Training with Granularity Tuning.}}
	\label{refine_alg}
	\Input{~$\mathbf{X}$: training data; ~$\mathbf{X_{test}}$: test data;\\ ~$\mathbf{Y}$: ground-truth labels of training data;\\ ~$\mathbf{Y_{test}}$: ground-truth labels of test data;\\ ~$\epsilon$: perturbation radius;\\ ~$d_{step}$: step size of abstraction granularity}
	\Output{$f_{out}$: verified robust neural network;\\
	$d_{out}$: the best abstraction granularity}
	Initialize $d,error,e, f_{out}, d_{out}$\tcp*{$e:$ verified error.}
	\While{$d\geq 2*\epsilon$}{
	    $(f,\ell)\leftarrow$  \textsc{AbsTrain}($\mathbf{X}$, $\mathbf{Y}$, $\epsilon$, $d$)\tcp*{$\ell$: training loss.} 
	    
	    $e\leftarrow $ \textsc{Verify}($f$, $\mathbf{X_{test}}$,  $\mathbf{Y_{test}}$, $\epsilon$, $d$)\tcp*{Verification.} 
	    
	    \If{$e < error$}{
	        $error\leftarrow e$; \\
	        $f_{out}\leftarrow f$;\\
	        $d_{out}\leftarrow d$; 
	    }
	    
	    $\mathbb{I}$ $\leftarrow \Phi(\mathbf{X}$, $\epsilon$, $d$) \tcp*{Map to training interval.} 
	    
	    $\overline{G}, \underline{G}\leftarrow\ell'(\overline{\mathbb{I}}),\ell'(\underline{\mathbb{I}}) $\tcp*{Obtain bounds' gradient,}
	    
	    % $\underline{G}\leftarrow \ell'(\underline{\mathbb{I}})$\tcp*{$\underline{G}$: The lower bounds' gradient.}
	    
	    \If{$\overline{G}\leq 0 \wedge \underline{G}\geq 0$}{
	        Break\tcp*{Stop when G guides d to increase} 
	    }
	    \Else{
	        $d \leftarrow  d - d_{step}$\tcp* {Decrease d and continue}
	    }
	}
	Return $f_{out}, d_{out}$;
\end{algorithm}

In this section we introduce a verification-based method for 
tuning the abstraction granularity to train 
% verified 
robust models. Due to the finiteness of $\mathbb{I}^n$, verification procedure can be conducted in a black-box manner, which is both sound and complete. Based on verification results, we can 
tune the abstraction granularity to obtain finer training intervals.
% set for more robust training. 

% \vspace{-3mm}
\subsection{Black-box Robustness Verification}

We propose a black-box verification method $\textsc{Verify}(\cdot)$ for neural networks trained by our approach. Given a neural network $f$, a test set $\mathbf{X_{test}}, \mathbf{Y_{test}}$, a perturbation distance $\epsilon$ and an abstraction granularity  $d$, $\textsc{Verify}(\cdot)$ returns the verified error on the set. 
The verification procedure is straightforward. First, for each $x\in \mathbf{X_{test}}$, we compute the set $\mathbb{I}$ of training interval vectors of $\mathbb{B}(x,\epsilon)$ using the same abstraction function $\phi$ in  Algorithm \ref{abstraction_alg}.  
Then, we feed each interval vector in $\mathbb{I}$ into $f$ and check if the classification result is consistent with the ground-truth label of $x$. The verified error $e$ is the ratio of the inconsistent cases in the test set. 

% \vspace{1ex}
Our verification method is both sound and complete due to the finiteness of $\mathbb{I}$: 
$f$ is robust on $\mathbb{B}(x,\epsilon)$ if and only if $f$ returns the same label on all the interval vectors in   $\mathbb{I}$ as the one of $x$. Another advantage 
% of our method 
is that it treats $f$ as a black box. Therefore, our verification method is orthogonal and scalable to arbitrary models.  

\subsection{Tuning Abstraction Granularity} \label{3.4}

When the verified error of a trained neural network is large, we can reduce it by tuning the abstraction granularity and re-train the model on the refined training set of interval vectors. We propose a gradient descent-based 
algorithm to explore the best $d$ in the abstraction space. 

Algorithm \ref{refine_alg} shows the tuning and re-training process. First, our algorithm initializes abstraction granularity $d$ and verified error $error$ (Line 1). The tuning process repeats until $d < 2\epsilon$, which means that the size of a training interval is not less than that of the perturbed interval (Lines 2-15). Then a neural network and training loss toward training intervals with $d$ are obtained by calling the Abstraction-based Training function (Algorithm \ref{train_alg}) (Line 3). Get the neural network's verified errors on the test dataset (Line 4). If the neural network's verified errors are smaller, we save the neural network and abstraction granularity (Lines 5-8). Next, training intervals are obtained (Line 9). Then the upper and lower bounds' gradient of the training intervals are obtained (Lines 10). 
If $\overline{G} \leq 0$ and $\underline{G} \geq 0$, the algorithm will terminate (Lines 11-12); otherwise, abstraction granularity will be updated to a smaller granularity (Lines 13-14). 
When the algorithm terminates, we obtain the neural network with the lowest verified error.
