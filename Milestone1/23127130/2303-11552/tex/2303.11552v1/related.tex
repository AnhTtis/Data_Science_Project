\vspace{-2mm}
\section{Related Work}\label{sec:rela}
\vspace{-1mm}

This work has been inspired by
earlier  efforts on training and verifying robust models by interval-based abstractions. 

\vspace{1ex}

\noindent
\textbf{Interval Neural Networks (INNs).} A neural network is called an interval neural network if at least one of its input, output, or weight sets are interval-valued \cite{beheshti1998interval}. Interval-valued inputs can capture the uncertainty, inaccuracy, or variability of datasets and thus are used to train prediction models of uncertain systems such as stock markets \cite{roque2007imlp}. Yang and Wu proposed a gradient-based method for smoothing INNs to avoid the oscillation of training \cite{yang2012smoothing}. 
Oala et al. recently proposed to train INNs for image reconstruction with theoretically justified uncertainty scores of predictions   
\cite{oala2020interval}. All these works demonstrate that training on interval-valued data can improve the prediction accuracy under uncertainties. This is consistent with the robustness improvement for image classifications in this work. 

Prabhakar and Afzal proposed to transform regular neural networks to over-approximated INNs for robustness verification \cite{prabhakar2019abstraction}. However, the transformation inevitably introduces overestimation to the models and verification results. Our approach avoids any over-approximation to the trained models by training on interval-valued data. 

\vspace{1ex}
\noindent
\textbf{Verification-in-the-loop Training}. Many approaches on training neural networks with robustness guarantees have been proposed \cite{DBLP:conf/iclr/RaghunathanSL18,DBLP:conf/icml/WongK18,DBLP:conf/nips/WongSMK18,DBLP:conf/icml/MirmanGV18,DBLP:journals/corr/abs-1810-12715,DBLP:conf/iclr/ZhangCXGSLBH20,DBLP:conf/aaai/FanL21}. Most of them are based on linear relaxation \cite{DBLP:conf/icml/WongK18,DBLP:conf/nips/WongSMK18,DBLP:conf/iclr/ZhangCXGSLBH20} or bound propagation \cite{DBLP:conf/icml/MirmanGV18,DBLP:journals/corr/abs-1810-12715,DBLP:conf/iclr/ZhangCXGSLBH20,DBLP:conf/aaai/FanL21}. Linear relaxation-based methods
use linear relaxation
to obtain a convex outer approximation within a norm-bounded perturbation, which results in high time and memory costs \cite{DBLP:conf/icml/WongK18,DBLP:conf/nips/WongSMK18}. In contrast, bound propagation methods are more effective. 
% against $l_{\infty}$ norm bounded input 
 Gowal et al. \cite{DBLP:journals/corr/abs-1810-12715} proposed IBP to train provably robust neural networks on a relatively large scale. However, the bound it produces can be too loose to be put into practice. Zhang et al.  \cite{DBLP:conf/iclr/ZhangCXGSLBH20} improved IBP by combining the fast IBP bounds in
a forward propagation
% bounding pass 
and a tight linear relaxation-based bound, CROWN, in a backward propagation. Lee et al. \cite{DBLP:conf/nips/LeeLPL21} also proposed a training method based on linear approximation but considered another important factor - the smoothness of loss function. However, these methods rely heavily on  verification process, so the time complexity is relatively high. AdvIBP~\cite{DBLP:conf/aaai/FanL21} computes the adversarial loss using FGSM and random initialization and computes the robust loss using IBP. However, the verified errors obtained by AdvIBP are relatively higher. Thanks to the abstraction,  networks trained by our method are more robust.