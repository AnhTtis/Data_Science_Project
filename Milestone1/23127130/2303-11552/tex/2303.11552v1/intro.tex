\section{Introduction}

The robustness of image classifications based on neural networks is attracting more attention than ever due to  their applications to safety-critical domains such as self-driving \cite{Wu_2017_CVPR_Workshops} and medical diagnosis \cite{titano2018automated}. There has been a considerable amount of work on training robust neural networks  against adversarial perturbations \cite{DBLP:conf/icml/AthalyeC018, DBLP:conf/ccs/Carlini017, DBLP:conf/sp/Carlini017, DBLP:journals/corr/GoodfellowSS14, DBLP:conf/iclr/MadryMSTV18, DBLP:conf/sp/PapernotM0JS16, DBLP:conf/cvpr/XiaoYLDL19, DBLP:conf/ijcai/XiaoLZHLS18, DBLP:conf/iclr/XiaoZ0HLS18, DBLP:conf/cvpr/EykholtEF0RXPKS18, DBLP:conf/acl/HsiehYCZC18, DBLP:conf/iclr/XuLZCZFEWL19, DBLP:conf/iclr/ZhangCSBDH19}. Conventional defending approaches  augment the training set with  adversarial examples \cite{DBLP:conf/sp/PapernotM0JS16, DBLP:conf/iclr/GuoRCM18, DBLP:conf/iclr/SongKNEK18, DBLP:conf/iclr/BuckmanRRG18, DBLP:conf/iclr/Ma0WEWSSHB18, DBLP:conf/iclr/SamangoueiKC18, DBLP:conf/eccv/XiaoDLYLS18, DBLP:conf/iccv/XiaoDLLEYSLM19}. 
They target only specific adversaries, depending on how the adversarial samples are generated \cite{DBLP:conf/sp/Carlini017},  but cannot provide robustness guarantees   \cite{DBLP:conf/iclr/BalunovicV20, DBLP:conf/aaai/FanL21, DBLP:conf/cvpr/LyuGWXZL21}. 

Recent approaches attempt to train certifiably robust models with guarantees 
\cite{DBLP:conf/iclr/BalunovicV20,  DBLP:conf/iccv/GowalDSBQUAMK19, DBLP:conf/icml/MirmanGV18, DBLP:conf/icml/WongK18, DBLP:conf/iclr/ZhangCXGSLBH20, DBLP:conf/cvpr/LyuGWXZL21,  DBLP:conf/aaai/FanL21, DBLP:conf/nips/LeeLPL21}. They rely on the robustness verification results of neural networks in the training process. Most of the existing verification approaches are based on symbolic interval propagation (SIP) \cite{DBLP:conf/iccv/GowalDSBQUAMK19,DBLP:conf/iccv/GowalDSBQUAMK19,DBLP:conf/cvpr/LyuGWXZL21,DBLP:conf/aaai/FanL21,DBLP:conf/nips/LeeLPL21}, by which intervals areÂ \textit{symbolically input} into neural networks and propagated on a layer basis.
There are, however, mainly three obstacles for these approaches to be widely adopted:
(i) adding the verification results to the loss function for training brings limited improvement to the robustness of neural networks due to overestimation introduced in the verification phase;
(ii) they are time-consuming due to the high complexity of the verification problem \textit{per se}, e.g., NP-complete for the simplest ReLU-based fully connected feedforward neural networks \cite{DBLP:conf/cav/KatzBDJK17}; and
(iii) the verification is tied to specific types of neural networks in terms of  network architectures and  activation functions.

To overcome  the above obstacles, we propose a novel, abstraction-based approach for training verified robust image classifications whose inputs are 
\textit{numerical intervals}. 
Regarding (i), we abstract each pixel of an image into an interval before inputting it into the neural network. 
The interval is \textit{numerically} input to the neural network by assigning to two input neurons its lower and upper bounds, respectively. This guarantees that all the perturbations to the pixel in this interval do not alter the classification results, thereby improving the robustness of the network. Moreover, this imposes no overestimation in the 
training phase. To address the challenge (ii), we use forward propagation and back propagation only to train the network without extra time overhead.
Regarding (iii),  we treat the neural networks as black boxes since we  deal only with the input layer and do not change other layers. Hence, being agnostic to the actual neural network architectures, our approach can  scale up to fairly large neural networks. Additionally, 
we identify a crucial hyper-parameter, namely \emph{abstraction granularity}, 
which corresponds to the size of intervals used for training the networks. 
We propose a gradient descent-based algorithm to refine the abstraction granularity for training a more robust neural network. 

We  implement our method   in a tool  called \textsc{AbsCert} and assess it, together with existing approaches, on various benchmarks. The experimental results show that our approach 
reduces the verified errors of the trained neural networks up to 
95.64\%. Moreover, it totally achieves up to 
602.50x speedup.
Finally, it  can scale up to larger neural networks with up to 138 million trainable parameters and be applied to a wide range of neural networks.

\vspace{1ex}
\noindent \textbf{Contributions.} Overall, we provide (i) a
novel, 
abstraction-based training method for verified robust neural networks;
(ii) a companion black-box verification method for certifying the robustness of trained neural networks; 
(iii) a tool implementing our 
% abstraction-based training and verification methods
method; and
(iv) an extensive assessment of our 
% training 
method, together with existing approaches, on a wide range of benchmarks, which demonstrates our method's promising achievements.
