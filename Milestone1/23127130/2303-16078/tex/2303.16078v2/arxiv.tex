% CVPR 2024 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
 \usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{makecell}
\usepackage[table]{xcolor}  
\usepackage{animate}
\usepackage{tikz,pgf}
\usepackage{tkz-euclide}
\usepackage{amsthm}

%\setlength{\floatsep}{6pt plus2pt minus2pt}
%\setlength{\textfloatsep}{6pt plus2pt minus2pt}
%\setlength{\dblfloatsep}{6pt plus2pt minus2pt}
%\setlength{\dbltextfloatsep}{6pt plus2pt minus2pt}

% Import additional packages in the preamble file, before hyperref

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{5073} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2024}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{
%Using virtual correspondences in minimal solvers with applications to the four-points-in-three-views problem / 
Relative pose of three calibrated and partially calibrated cameras from four points using virtual correspondences}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Charalambos Tzamos$^\textrm{1}$ \quad
Daniel Barath$^\textrm{2}$ \quad
Torsten Sattler$^\textrm{3}$ \quad
Zuzana Kukelova$^\textrm{1}$\vspace{10pt}\\
$^\textrm{1}$Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague\\
{\tt\small tzamos.charalampos@fel.cvut.cz, kukelova@cmp.felk.cvut.cz} \\
$^\textrm{2}$ETH ZÃ¼rich, Computer Vision and Geometry Group, Switzerland\\
{\tt\small danielbela.barath@inf.ethz.ch} \\
$^\textrm{3}$Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague\\
{\tt\small torsten.sattler@cvut.cz}
}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\newcommand{\PAR}[1]{\vskip4pt \noindent{\bf #1~}}
\newcommand*{\TODO}[1]{\textcolor{red}{[#1]}}
\newcommand*{\TS}[1]{\textcolor{blue}{TS: [#1]}}


\newcommand{\C}{{\mathbb C}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\M}[1]{\mathtt{#1}}
\newcommand{\V}[1]{\mathbf{#1}}
\newcommand{\diag}{\textrm{diag}}
\newcommand{\vnorm}[1]{\left|\left|#1\right|\right|}

\def\sfm{{\texttt{5pt}}\xspace}
\def\ssm{{\texttt{6pt}}\xspace}
\def\sfc{{\texttt{5pt}}\xspace}
\def\ss{{\texttt{5pt}}\xspace}
\def\sft{{\texttt{5pt+P3P}}\xspace}
\def\sst{{\texttt{6pt+P3P}}\xspace}
\def\sftl{{\texttt{4p3v(L)}}\xspace}
\def\sftm{{\texttt{4p3v(M)}}\xspace}
\def\sftmd{{\texttt{4p3v(M$\pm \delta$)}}\xspace}
\def\sstmd{{\texttt{4p3vf(M$\pm \delta$)}}\xspace}
\def\sftld{{\texttt{4p3v(L$\pm \delta$)}}\xspace}
\def\sftlid{{\texttt{4p3v(L$\pm\delta$init)}}\xspace}
\def\sstld{{\texttt{4p3vf(L$\pm \delta$)}}\xspace}
\def\sstlid{{\texttt{4p3vf(L$\pm \delta$init)}}\xspace}
\def\sstl{{\texttt{4p3vf(L)}}\xspace}
\def\sstm{{\texttt{4p3vf(M)}}\xspace}
\def\sfto{{\texttt{4p3v(O)}}\xspace}
\def\ssto{{\texttt{4p3vf(O)}}\xspace}
\def\step{{\texttt{3pt+ep}}\xspace}
\def\sfhc{{\texttt{4p3v(HC)}}\xspace}
\def\sfep{{\texttt{4p3v(N)}}\xspace}
\def\sfepo{{\texttt{4p3v(NO)}}\xspace}

\def\sftp{{\texttt{4p3v}}\xspace}
\def\sstp{{\texttt{4p3vf}}\xspace}

\newtheorem{theorem}{Theorem}
%\linenumbers
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{definition}[theorem]{Definition}
%\crefname{lemma}{Lemma}{Lemmas}

%teaser-tikz
\tikzstyle{bedge}=[line width=1pt,cyan]
\tikzstyle{bsvertex}=[circle, draw=black, fill=cyan, inner sep=0pt, minimum size=2pt]
\tikzstyle{rsvertex}=[circle, draw=black, fill=red, inner sep=0pt, minimum size=2pt]
\newcommand{\iccv}{
\begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.55\columnwidth]{figures/rotunda2.PNG}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
    
        \node[rsvertex] (10) at (0.205, 0.274) {};
        \node[rsvertex] (10) at (0.406, 0.194) {};
        
        \draw[bedge] (0.24, 0.71) -- (0.155, 0.235);
        \draw[bedge] (0.35, 0.56) -- (0.155, 0.235);
        \draw[bedge] (0.66, 0.55) -- (0.155, 0.235);
        \draw[bedge] (0.57, 0.31) -- (0.155, 0.235);

        \draw[bedge] (0.24, 0.71) -- (0.395, 0.155);
        \draw[bedge] (0.35, 0.56) -- (0.395, 0.155);
        \draw[bedge] (0.66, 0.55) -- (0.395, 0.155);
        \draw[bedge] (0.57, 0.31) -- (0.395, 0.155);

        \draw[bedge] (0.24, 0.71) -- (0.848, 0.14);
        \draw[bedge] (0.35, 0.56) -- (0.848, 0.14);
        \draw[bedge] (0.66, 0.55) -- (0.848, 0.14);
        \draw[bedge] (0.57, 0.31) -- (0.848, 0.14);

        \node[bsvertex] (0) at (0.168, 0.31) {};
        \node[bsvertex] (1) at (0.188, 0.29) {};
        \node[bsvertex] (2) at (0.225, 0.279) {};
        \node[bsvertex] (3) at (0.225, 0.247) {};

        \node[bsvertex] (4) at (0.374, 0.228) {};
        \node[bsvertex] (5) at (0.389, 0.207) {};
        \node[bsvertex] (5) at (0.423, 0.197) {};
        \node[bsvertex] (6) at (0.42, 0.176) {};

        \node[bsvertex] (7) at (0.792, 0.174) {};
        \node[bsvertex] (8) at (0.765, 0.21) {};
        \node[bsvertex] (9) at (0.758, 0.225) {};
        \node[bsvertex] (10) at (0.816, 0.21) {};

        

        \draw[->, red, thick, dashed] plot [smooth, tension=1] coordinates {(0.155, 0.21) (0.25, 0.09) (0.395, 0.11)};
        \node at (0.23, 0.05) {$\texttt{\scriptsize 5pt}$};

        \draw[->, orange, thick, dashed] plot [smooth, tension=1] coordinates {(0.87, 0.16) (0.92, 0.4) (0.68, 0.55)};
        \node at (0.97, 0.25) {$\texttt{\scriptsize P3P}$};
        %\clip (0.5,0.5) circle (1cm);
    \end{scope}
\end{tikzpicture}
}

\begin{document}

\maketitle
% Remove page # from the first page of camera-ready.

%%%%%%%%% ABSTRACT
\begin{abstract}
We study challenging problems of estimating the relative pose of three cameras and propose novel efficient solutions to (1) the notoriously difficult configuration of four points in three calibrated views, known as the 4p3v problem, and (2) to the previously unsolved configuration of four points in three cameras with unknown shared focal length, i.e., the 4p3vf problem. Our solutions are based on the simple idea of generating one or two additional virtual point correspondences in two views by using the information from the locations of the four input correspondences in the three views. We generate such correspondences using either a very simple and efficient strategy where the new points are the mean points of three corresponding input points or using a simple neural network. The new solvers are efficient and easy to implement since they are based on existing efficient minimal solvers, i.e., the well-known 5-point and 6-point relative pose solvers and the P3P solver. Our solvers achieve state-of-the-art results on real data. 

\end{abstract}




%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}


%Estimating the camera geometry, \ie, the pose and calibration parameters, 
Camera geometry estimation 
is crucial in many computer vision applications, \eg, visual navigation~\cite{DBLP:journals/ram/ScaramuzzaF11}, structure-from-motion~\cite{Snavely-IJCV-2008},
augmented and mixed reality~\cite{Castle08ISWC}, self-driving cars~\cite{hane20173d}, 
%large-scale 3D reconstruction~\cite{DBLP:conf/cvpr/HeinlySDF15}, 
and 
%image based 
visual localization~\cite{Sattler16PAMI}. 

Due to
noise and outliers in input correspondences, 
the predominant way in camera geometry estimation is to use a hypothesis-and-test framework, \eg,  RANSAC~\cite{Fischler-Bolles-ACM-1981,Chum-2003, DBLP:journals/pami/RaguramCPMF13, barath2017graph}.
 For RANSAC-like methods, it is critical to estimate camera geometry using as few correspondences as possible since the number of RANSAC iterations (and, thus, its run-time)
 depends exponentially on the number of correspondences required for the model estimation.

 \begin{figure}[t]
     \centering
     %\includegraphics[width=0.9\columnwidth]{figures/teaser-4pt-3.png}
     \iccv
     \caption{Visualization of the 4p3v problem and our solution that is based on generating new virtual correspondence(s) (in red) between two views. This is done using coordinates of input point correspondences. 
     %Using the virtual correspondence, 
     Then the 4p3v problem is solved by existing efficient minimal solvers, \ie, the 5pt~\cite{Nister-5pt-PAMI-2004} 
     %relative pose solver~\cite{Nister-5pt-PAMI-2004} 
     and the P3P solvers~\cite{lambda-twist}.}
     \label{fig:teaser}
 \end{figure}

Algorithms that solve camera geometry estimation problems
using all available constraints 
%(equations) 
and the minimum number of correspondences, such that the resulting system of equations has a finite number of solutions, are called \emph{minimal solvers}, and the corresponding problems are called \emph{minimal problems}. 
Minimal problems often result in complex systems of polynomial equations in several variables. After introducing algebraic-based methods~\cite{DBLP:phd/basesearch/Stewenius05,Kukelova-thesis,Kukelova-ECCV-2008,larsson2017efficient,bhayani2019sparse}
for generating efficient polynomial solvers to the computer vision community, 
solutions to many previously unsolved minimal 
problems were proposed

~\cite{Stewenius-CVPR-2005,bujnak_cvpr2008,Bujnak-ICCV-2009,larsson2019revisiting,DBLP:conf/cvpr/KukelovaP07,kukelova2013real,Stewenius-ISPRS-2006,bhayani2023partially}.
Still, some problems
%such as estimating the geometry of rolling shutter cameras, 
result in equations for which state-of-the-art algebraic methods fail to generate a solver that is efficient and/or numerically stable.

In this paper, we study such challenging problems of estimating the relative pose of three cameras. These problems have attracted attention
for a long time~\cite{holt1995,Quan2006,leonardos_cvpr2015,Martyushev16,Aholt2014}. 
However, due to their complexity, they are still not considered fully solved. No efficient and practical solutions exist for most of the minimal configurations of point and line correspondences, as well as for partially calibrated cameras.
One such configuration that is particularly interesting is the notoriously difficult configuration of four points in three views~\cite{Quan2006, Nister-5pt-PAMI-2004}.
%, known as the 4p3v problem.  
%for which an fully efficient and practical solution still does not exist.
This configuration is minimal for three cameras with an unknown shared focal length, \ie, the 4p3vf problem, and provides one more constraint than minimal for calibrated cameras, \ie, the 4p3v problem.

State-of-the-art algebraic and numerical methods are known to fail to generate efficient and numerically stable solutions to both the 4p3v and 4p3vf problems.  
%
The existing methods for solving the 4p3v problem for calibrated cameras are thus only approximate.  By solving only for one (few) solutions from the 272 solutions of the %polynomial 
system that describe the problem~\cite{Hruby_cvpr2022}, and by discretely sampling the space of potential solutions~\cite{DBLP:journals/ijcv/NisterS06}, the existing 4p3v methods can often fail, \ie, the returned solution can be, in general, arbitrarily far from the geometrically correct solution. To decrease the failure rate, both methods~\cite{Hruby_cvpr2022,DBLP:journals/ijcv/NisterS06} require a lot of tuning and are not easy to re-implement.\footnote{For the solver proposed in~\cite{DBLP:journals/ijcv/NisterS06}, there is no publicly available implementation. The publicly available implementation of the solver from~\cite{Hruby_cvpr2022} is quite complex and requires a non-negligible effort to run.}

In this paper, we propose a novel approach for solving both the calibrated 4p3v, as well as, the previously unsolved 4p3vf problem for cameras with an unknown shared focal length.  
Our solutions are based on the simple idea of
generating new approximate point correspondence(s) between two of the three views.\footnote{Note that similarly to the state-of-the-art solutions~\cite{DBLP:journals/ijcv/NisterS06,Hruby_cvpr2022}, our solutions are only approximate. However, as we show in the experiments, they provide good initialization for LO-RANSAC and outperform~\cite{Hruby_cvpr2022}.} Such correspondences are generated using %an information purely from 
 only the locations of the four triplets of input point correspondences. 
Since the generation of new points does not require information from the image itself (\eg, information about appearance or features in the image), new correspondences,  in general, do not need to correspond to any physical 3D points in the scene. 
Thus, we call them  \emph{virtual correspondences}. 
Using virtual correspondences, we can efficiently solve the 4p3v and 4p3vf problems by first estimating the relative pose of the two cameras from five/six correspondences using efficient 5pt/6pt solvers~\cite{Nister-5pt-PAMI-2004,stewenius-etal-omnivis-2005}, and then registering the third camera using the P3P solver~\cite{lambda-twist}. We call this combination of the 5pt/6pt relative pose and the P3P solver the \sft and \sst solvers. 

 Based on this idea, we propose two groups of novel solvers for the 4p3v and 4p3vf problems: 
 (1) M-solvers (\sftm,\sstm): Solvers that use the mean points of three corresponding points detected in two views and potentially points in their vicinity ((M$\pm\delta$)-solvers) as new virtual point correspondences.
 (2) L-solvers (\sftl,\sstl): Solvers that, given four triplets of
corresponding points in three views and a point in the first view, use a network to predict a corresponding point to this point
in the second view, \ie, a virtual correspondence.
While simple to implement, the novel solvers achieve state-of-the-art results for the 4p3v/4p3vf problems on real data.


\noindent The contributions of the paper are as follows: 
%\vspace{-1.3ex}
\begin{itemize}
[leftmargin=0.3cm]
\item For the well-known challenging 4p3v problem for calibrated cameras, we propose two groups of novel solutions \sftm- and \sftl-based solvers. These solvers generate an additional virtual point correspondence(s) in two views by leveraging the locations of four input triplet correspondences. The new solvers achieve state-of-the-art results in terms of accuracy on real data. Compared to state-of-the-art solutions to the 4p3v problem~\cite{Hruby_cvpr2022,DBLP:journals/ijcv/NisterS06}, which are non-trivial and difficult to re-implement such that they are numerically stable and fast, our new solutions, especially the \sftm-based solvers, can be easily implemented using existing efficient implementations of the 5pt solver~\cite{Nister-5pt-PAMI-2004} and the P3P solver~\cite{lambda-twist}.
\item We provide the first efficient solutions to the previously unsolved 4p3vf problem for cameras with unknown equal focal lengths. Our novel \sstm- and \sstl-based solvers generate for each instance two virtual correspondences to solve the problem using the efficient 6pt~\cite{Nister-5pt-PAMI-2004} and the P3P solver~\cite{lambda-twist}. Our novel solvers show the potential of
the proposed idea of \emph{virtual correspondences} to be applied to other camera geometry problems. 
\item We compare the proposed solvers, the state-of-the-art 4p3v solver~\cite{Hruby_cvpr2022}, as well as ``oracle"-based  4p3v/4p3vf solvers, with the baseline minimal \sft and \sst solvers.
%that samples three triplets of points and two additional points visible in two views.
By evaluating all solvers on a large amount of real data as well as data with outliers, we discuss which minimal point configuration is potentially the most useful in which scenario.
%the most relevant in practical applications.

The source code of our solvers will be publicly available.


\end{itemize}

\section{Related work}
Estimating the relative pose of three cameras from a minimal number of point and line  correspondences 
% or a combination of point and line correspondences 
is known as an extremely challenging problem. 

For three uncalibrated cameras, 6 point correspondences are necessary to estimate the trifocal tensor, with a solution known for a long time~\cite{Quan_pami95,Torr97a}. 
 Solutions to three minimal combinations of points and lines are presented in~\cite{Oskarsson_bmvc2004}. 
The minimal configuration using 9 lines is much more challenging and was solved only recently by Larsson et al.~\cite{larsson2017efficient}.  However, the final solver is far from practical as it performs elimination of a huge $16k\times 13k$ matrix and runs 17.8s.

For calibrated cameras, the configuration that attracts most of the attention is the configuration of four points in three views (the 4p3v problem). Note, that this is not a minimal configuration since it generates 12 constraints for 11 degrees-of-freedom (DoF)
%, \ie, it is over-constrained %by one 
(see also Section~\ref{sec:solvers}).
% The four-points-in-tree-views problem (4p3v) 
The 4p3v problem is known as being extremely difficult to solve.
Several papers present mostly theoretical results~\cite{leonardos_cvpr2015,Martyushev16,Aholt2014}.
For four triplets of exact points without noise, it is shown that the 4p3v problem has, in general, a unique solution~\cite{holt1995,Quan2006}.

To the best of our knowledge, there are only two reasonably efficient solutions to the 4p3v problem reported in the literature.
The first solver~\cite{DBLP:journals/ijcv/NisterS06} is based on  one-dimensional exhaustive search.
In the paper, the authors derive several interesting theoretical results and show that the four point correspondences between two calibrated views constrain the epipole in each image to lie on a curve of degree ten.
The solver performs a one-dimensional sweep of the curve of possible epipoles. 
For each potential epipole, it computes the relative pose of two cameras, registers the third camera using three triangulated points, and finally extracts the solution minimizing the reprojection error of the fourth point in the third view. 
The evaluation of the solver on one potential epipole is fast.  
Yet, to obtain reasonable precise and stable results, usually, 1,000 candidates need to be evaluated and even then, %in such a case 
refinement at multiple local minima is required to improve the precision. The reported run-times of this solver were $1-12$ms depending on the number of searched points.\footnote{This is the runtime reported in the paper from 2006, however, implemented by a highly skilled researcher (the author of the fastest version of the well-known 5pt relative pose solver). Therefore, we do not expect significant speedup on recent hardware. 
}  
There is no publicly available implementation for this solver and it is not easy to re-implement. 
As such, the literature does not compare against the solver in experiments. 
% To obtain a
As an upper bound of the performance of~\cite{DBLP:journals/ijcv/NisterS06}, we compare against an oracle version using  the true epipole. % is given. 

The second efficient solver to the 4p3v problem was published only recently~\cite{Hruby_cvpr2022}. In this paper, the authors first transform the 4p3v problem to a minimal problem by considering a line passing through the last correspondence in the third view.
%, \ie by dropping one constraint. 
The resulting system of equations is solved using an efficient Homotopy continuation (HC) method~\cite{Fabbri_CVPR2020,SommeseAndrewJ2005Tnso}. 
To avoid computing large numbers of spurious solutions, an MLP-based classifier is trained. For a given problem $p$, it selects one or several starting problem-solution pairs (so-called anchors), such that the geometrically meaningful/correct solution of $p$ can be obtained by HC starting from this anchor. This strategy is fast, running $16.3\mu s$ on average per solution. 
At the same time, it has a high failure rate. The success rate of the 4p3v solver reported in~\cite{Hruby_cvpr2022} %the paper 
on two test %ing 
datasets and data without noise is $26.3\%$.
%Unfortunately,
% The paper does
\cite{Hruby_cvpr2022} do not show results of their solver in a real scenario, \ie, a RANSAC-like framework with noisy data. 
We provide such an evaluation in this paper. 


Simple solutions to the 4p3v problem for orthographic and scaled orthographic views were presented in~\cite{xu_ortho4p3v,Higgins-4p3v91}. In~\cite{Higgins-4p3v91}, the author suggested an iterative approach for updating to perspective views. 
However, he reported results only on a few synthetic instances. % of points. 
Even after spending a non-negligible amount of work %(adding different local optimizations) 
and time (months) to make this update work, we were not able to get reasonable results on real data with general perspective cameras. % that are further from the scaled orthographic case.


Minimal configurations of points and lines in three calibrated views were  studied 
%from the theoretical perspective 
in~\cite{Duff_PL1P,Kileel2017,duff2019plmp}, aiming to classify %ing 
and derive %ing 
the number of solutions for different configurations. Solutions to two minimal configurations of combinations of points and lines, the so-called Chicago and Cleveland problems, were proposed in~\cite{Fabbri_CVPR2020}. These problems result in 216 respectively 312 solutions and are solved using HC %homotopy continuation 
methods~\cite{SommeseAndrewJ2005Tnso}. Due to their complexity, the resulting solvers are not practical in real applications, with running times ranging from $0.66$s to $1.9$s.

Recently, a very efficient GPU implementation of the homotopy continuation method was used to solve minimal problems of four points in three views and six lines in three views for a generalized (non-central) camera~\cite{Ding_2023_ICCV}.

In our solutions, we generate virtual correspondences. 
Virtual matches are also used in the literature of affine correspondences (AC) %\cite{Perdoch-CVPR2009efficient}. 
\cite{perd2006epipolar,pritts_ivcnz13,pritts-cvpr2018,barath2022relative}. 
There, points are sampled based on the affine feature geometry to generate point correspondences from affine ones. 
% The resulting point correspondences are then used as input to point correspondence-based solvers. 
In our scenario, we are directly given point correspondences, without associated feature geometry, and we predict additional point matches.

\section{Estimating the relative pose of three cameras}
\label{sec:solvers}
Let
%us assume that we are given 
$N$ cameras observe a set of 3D points $\mathcal{P}$ and for each point $\V{P} \in \mathcal{P}$, let $C_{\V{P}}>1$ be the number of cameras that observe it.
A necessary condition for a relative pose problem of $N$ calibrated cameras to be minimal is~\cite{Fabbri_CVPR2020}
\begin{equation}
\label{minimal_con}
     \sum_{\V{P} \in \mathcal{P}} (2C_{\V{P}}-3) =6N - 7 \enspace .
\end{equation}

A configuration of points in $N=3$ views that satisfies the constraint~\eqref{minimal_con} of a minimal problem is three points visible in all three cameras and two additional points visible in two of the three cameras. We will call this configuration %[(2,3),(2,3),(0,3)] 
$(5_3,5_3,3_3)$, where the lower index on the position $i$ indicates how many from the image points sampled in the camera $i$ are visible in all three cameras.

The configuration of four points visible in all three cameras, \ie, the configuration $(4_4,4_4,4_4)$, generates an over-constrained problem. In this case, we have one more constraint than DoF, \ie, in Eq.~\eqref{minimal_con}, we have $12 > 11$. 
A minimal solution would %have needed 
need to drop one constraint, \eg, %e.g., 
by considering only a line passing through one of the points in the third view~\cite{Hruby_cvpr2022} or by considering a ``half" point correspondence.
 Since, in practice, we always have full correspondences and sampling one less point in one view leads to an under-constrained problem, the configuration $(4_4,4_4,4_4)$, is usually considered %a 
 ``minimal".

For cameras with unknown common focal lengths, we have one more DoF. This means that, for $N=3$, the right-hand side of equation~\eqref{minimal_con} equals to 12, resulting in $(6_3,6_3,3_3)$ and $(4_4,4_4,4_4)$ being minimal configurations.


\subsection{Calibrated cameras}
\label{sec:calibrated}
In this section, we describe solutions for three calibrated cameras. We start with one baseline solution for the minimal $(5_3,5_3,3_3)$ configuration followed by novel solutions for the ``minimal" $(4_4,4_4,4_4)$ configuration.

\PAR{5pt+P3P solver:}
First, we review a simple baseline minimal solver for the configuration $(5_3,5_3,3_3)$, \ie, the configuration where three points are visible in all three cameras and two additional points 
%are visible 
in two of the three cameras.
% 

The \sft  solver first estimates the relative pose of two cameras from 5 image point correspondences using the efficient 5pt solver~\cite{Nister-5pt-PAMI-2004}. 
Next, the three points that are visible in all three views are triangulated. Finally, the third camera is registered using the three 2D-3D point correspondences and the well-known efficient P3P solver~\cite{lambda-twist}. 
 
 This solver is straightforward and it is based on efficient existing solvers~\cite{Nister-5pt-PAMI-2004,lambda-twist}.
 The \sft solver is not novel.
 This solver and the $(5_3,5_3,3_3)$ configuration  was, \eg, mentioned in% the paper
 ~\cite{Duff_PL1P}, where the authors present a classification of minimal problems for points and lines (partially) observed  by three calibrated perspective cameras. However, %here 
 the $(5_3,5_3,3_3)$ configuration and the \sft solver were only discussed %only 
 from %the 
 a  theoretical point of view, aiming to report the number of solutions to this problem and not its practical performance.
 %
 The \sft solver is %even 
 used by some researchers~\cite{Nister-5pt-PAMI-2004} and practitioners in their applications~\cite{Rodehorst2017}. 
Nister et al.~\cite{DBLP:journals/ijcv/NisterS06} showed that the \sft solver performs better than their dedicated 4p3v solver. 
 Still, the \sft solver is not even used as a baseline for a comparison in papers that study the relative pose problem for three calibrated cameras~\cite{Fabbri_CVPR2020,Hruby_cvpr2022}. 
 To our knowledge, the performance of this solver on real data and within state-of-the-art RANSAC frameworks in the context of the 4p3v problem has not been extensively studied. % in the computer vision literature. 
 Our paper fills this gap in the literature.

 Motivated by the efficient \sft solver and the minimal $(5_3,5_3,3_3)$ configuration, which, compared to the $(4_4,4_4,4_4)$ configuration, leads to significantly simpler systems of polynomial equations,
 %that can be solved efficiently, 
 we next describe two groups of novel solvers to the calibrated 4p3v problem. These solvers efficiently solve the $(4_4,4_4,4_4)$ configuration by generating a virtual point correspondence in the first two views, and 
 thus solving it using the  \sft solver.
 %and transforming it to $(5_3,5_3,3_3)$ configuration.

 \PAR{4p3v(M) solver:}
 Our first solver is based on a simple observation: 
If we fix the $5^{\text{th}}$ point in the first view to be the mean $\V m^1$ of three points $\left\{\V x_i^1,\V x_j^1,\V x_k^1\right\}, \; i,j,k \in \left\{1,\dots, 4\right\}$ in this view, then the mean point $\V m^2$ of the corresponding three points $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$ in the second view usually has a small epipolar error \wrt the ground truth relative pose. 
Thus 
%is, the mean of three points 
$\V m^1 \leftrightarrow \V m^2$
is usually a good approximation of a correct correspondence.


This can be considered as a surprising fact, since 4 (or actually 3) points in two views define an infinite number of camera poses that can observe these points. However, the reason this mean-point strategy works in practice comes from three simple facts and observations. (1)  To generate a good correspondence, we only require that the point in the second view be reasonably close to the epipolar line defined by the mean point $\V m^1$ in the first view, \ie, the 2D point does not need to correspond to one particular 3D point with a given depth.
(2) The epipolar line defined by 
%the mean point 
$\V m^1$ 
%of three points $\left\{\V x_i^1,\V x_j^1,\V x_k^1\right\}$ in the first image 
passes through the triangle defined by the corresponding three points $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$ in the second image. Therefore, the maximum distance of $\V m^2$ in the second image from the epipolar line is bounded by the maximum distance of $\V m^2$ from $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$.
%of the mean point from one of the vertices of the triangle.
(3) Three 3D points define a plane. If this plane is not observed under very oblique and different angles in two views, then the point on the 3D plane that projects on the mean point in the first image usually projects close to the mean point $\V m^2$ in the second image.
We support this observation by experiments on a large amount of synthetic and real data (\cf Sec.~\ref{sec:experiments}, mean-point strategy). %\ref{sec:mean-point}). 

Motivated by these observations, our \sftm solver uses the mean points of three corresponding points detected in two views as a new $5^\text{th}$ point correspondence. The 4p3v problem is then solved using the \sft solver.

 \PAR{4p3v(M$\pm \delta$) solver:}
While the mean point correspondence used in the \sftm solver can provide a good approximation of a correct correspondence, such a correspondence can be noisy.
In the \sftmd solver, we thus, in addition to the mean point $\V m^2 = \left[x,y\right]$ of 
%the triangle $\mathcal{T_2}$] 
three points $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$
in the second image, generate two additional points. These points are (1) $\V {m}^2_{\delta} = \left[x\pm\delta,y\right]$ if the longest dimension of the triangle  $\mathcal{T}^2 = \Delta\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$ is in the x direction or (2) $\V {m}^2_{\delta} = \left[x,y \pm\delta\right]$ if it is in the y direction.
All three points, \ie, $\V m^2$ and $\V m^2_\delta$ are placed in  correspondence with the mean point $\V m^1$.
%in the first image. 
The \sftmd solver in the first step calls the \sfc solver~\cite{Nister-5pt-PAMI-2004} three times, with the $5^{th}$ correspondence being either $\V m^1 \leftrightarrow \V m^2$ or $\V m^1 \leftrightarrow \V m^2_{\delta}$. The results of these three \sfc solvers are collected to create hypotheses for the relative pose of the first two cameras inside RANSAC. The shift $\delta$ is selected relative to the size of the triangle $\mathcal{T}^2$. 
 
\PAR{4p3v(L) solver:}
In our  learning-based \sftl solver, instead of using the mean point correspondence, we use a network to predict this virtual correspondence.
We use the fact that four triplets of points, in general, define a unique relative pose of three calibrated cameras. 
We train a network that given such four 
%triplets of 
corresponding points in three views and a fixed 5$^\text{th}$ point in the first view, it predicts a corresponding 5$^\text{th}$ 
point
%its corresponding point 
in the second view.\footnote{By fixing a point in one view, we are defining an epipolar line %(an epipolar line) 
in the second view. Any of the points on this line (corresponding to 3D points with different depths) is in  correspondence with the %given 
point in the first view.}

We use a lightweight architecture with a backbone of shared MLP layers, similar to \cite{cavalli2022nefsac}, such that each triplet of correspondences is processed independently. 
The input to our network is a $4 \times 6$ matrix of 4 point correspondences containing the $x$ and $y$ coordinates in three views. 
In estimating the epipolar geometry, the order of the point correspondences does not matter. Thus, our network is designed to be permutation invariant in that input axis.
The input is normalized as follows: We apply a rotation matrix to the points in each view independently, so that the mean point of the first three points is sent to $(0, 0)$, and the fourth point is sent to $(0, y)$. 
Let $\V m^1, \V m^2$, and $\V m^3$ be the mean points of the three corresponding points in three views.
We aim to predict the corresponding point of $\V m^1$ in the second view. Let us denote this predicted point by $\V {\tilde{m}}^2$, \ie, our $5^{th}$ virtual correspondence in the first two views will be $\V m^1 \leftrightarrow \V{\tilde{m}}^2$. 
% 
As suggested by the \sftm solver, $\V{\tilde{m}}^2$ should be close to $\V m^2$. Thus, we use the mean point $\V{m}^2$ as the initialization of our network and  
predict a shift from $\V m^2$. 

Our loss function %of our network 
is the \textit{Sampson error} $\mathcal{L}_S$ of the prediction to the epipolar line of $\V m^1$ in the $2^\text{nd}$ view: 

\begin{equation}
 \mathcal{L}_S = \frac{{\V{\tilde{m}}^{2^\top}} \mathbf{E} \V{m}^1}{\sqrt{\lVert \mathbf{E}\V{m}^1 \rVert^2 + \lVert \mathbf{E}^\top \V{\tilde{m}}^2 \rVert^2}} \enspace .  
\end{equation}

We train and validate our network using synthetic data. 
The  dataset is generated as follows: We generate a scene of 10K random 3D points. 
%$\M{X}_i \in \R^3$. 
To generate each instance,
%/ configuration, 
we randomly select four 3D points and three cameras with random rotations and translations that see the scene, and project those points into the cameras. % get the camera coordinates. 
In total, we use $1$M such instances, which we split 70\% for training and 30\% for validation. %/configurations. 
The network was implemented in PyTorch~\cite{paszke2017automatic} and trained for 
%1,500 
30 epochs using the Adam optimizer~\cite{DBLP:journals/corr/KingmaB14}. 
More details can be found in the supp. material.

Similarly to the \sftm solver, the \sftl solver solves the problem using the efficient \sft solver.


\PAR{4p3v(L$\pm \delta$) and 4p3v(L$\pm \delta$init) solvers:} 
Similar to \sftmd, we try to compensate for potential noise in the predictions $\tilde{\V m}^2$ returned by the network by running the \sfc solver for the first two views three times for three different virtual correspondences. We test two variants: (1) In the \sftld solver, we add a shift $\pm \delta$ to the predicted points $\tilde{\V{m}}^2$, similar to the \sftmd solver. In this way, we generate two more virtual correspondences. (2) In the \sftlid solver, we add a shift $\delta$ to the initialization $\V m^2$ of the network. Thus, we run the network three times with the initializations $\V m^2$ and two points $\V m^2_\delta$ to predict three different virtual correspondences.

\subsection{Partially calibrated cameras}
\label{sec:partial_cam}
Our novel solvers for the \sstp problem of estimating the relative pose of three cameras with an unknown shared focal length from four correspondences follow the idea of the virtual correspondences used in our \sftp solvers for calibrated cameras. 
% In this case
% Similarly, t
The idea is to transform the extremely complex \sstp problem into the \sst problem for which efficient solutions exist.
The \sst solver first estimates the unknown focal length and the relative pose of the first two cameras using the efficient 6pt solver~\cite{stewenius-etal-omnivis-2005} and then registers the third camera using the P3P solver~\cite{lambda-twist}.

This means that, in contrast to the \sftp solvers presented in Section~\ref{sec:calibrated} that generate only one virtual correspondence (+ potentially additional shifted versions of this correspondence), in our novel \sstp solvers, we need to generate two new virtual correspondences to obtain six correspondences in the first two views.

In the \sstm solver, we generate these two new virtual correspondences using the mean points of two different triplets of corresponding points in the first two cameras.

In the \sstl solver, we run the network described in Section~\ref{sec:calibrated} for two different 
initializations of mean points of different triplets in the first camera. %\TODO{Charalambos: please check if this is how you run it} 


\sstmd, \sstld, \sstlid solvers work similarly  to the calibrated $\delta$-solvers
%for the calibrated \sftp problem. 
More details on all \sstp solvers can be found in the supp. mat.   






\section{Experiments}
\label{sec:experiments}
We evaluated the proposed solvers on synthetic and real data to test their robustness to noise and outliers, and assess their performance inside a state-of-the-art RANSAC-framework~\cite{barath2017graph}. 
%
We compare our novel solvers with the following state-of-the-art algorithms:
\begin{itemize}
[leftmargin=0.3cm]
%\vspace{-1.0ex}
       \item \texttt{consecutive 5pt/6pt}: RANSAC-based robust 
       %essential matrix 
       relative pose
       estimation is applied independently for two pairs of views (1-2 and 1-3), without considering triplets. 
    %\vspace{-1.6ex}
    \item \texttt{joint 5pt/6pt}: %model estimation simultaneously returns 
    inside a single RANSAC, from a sample of 5/6 points in 3 views, essential matrices (and the focal length) are estimated independently for two pairs of views (1-2 and 1-3) from the view triplet. 
    The two matrices are jointly validated inside RANSAC.
    %\vspace{-1.6ex}
     \item \sft/\sst: The baseline minimal solvers for the $(5_3,5_3,3_3)$ and the $(6_3,6_3,3_3)$ configurations for calibrated/cameras with unknown focal length.
    %\vspace{-1.6ex}
    \item \sfhc: The homotopy continuation solver~\cite{Hruby_cvpr2022}.
\end{itemize}



\PAR{$\delta$-shift.}
We tested different shifts for our~$\delta$-based solvers. We selected $\delta = 0.15*\texttt{(longest triangle dim.)}$ for M-based solvers and $\delta = 0.1*\texttt{(longest triangle dim.)}$ for L-based solvers. For more details see supp. material. 
\PAR{Mean-point strategy.}
The first experiment aims to support our strategy of selecting a virtual point correspondence as the mean points of three corresponding points in two images.
We validate this strategy on synthetic and real data.

For the synthetic experiment, we generated 10k scenes with known ground truth parameters. 
In each scene, the three 3D points were randomly distributed within a cube of size $10 \times 10 \times 10$. Each 3D point was projected into two cameras.
%with realistic focal lengths. 
The orientations and positions of the cameras were selected at random such that they looked towards the origin from a random distance, varying from $20$ to $50$, from the scene. 
In the first camera, we generated a new point as a mean point of the three projected points. In the second camera, we were sampling points inside the triangle defined by the three 2D points.  To make the sampling invariant of actual coordinates of 2D points, we represented sampling in barycentric coordinates. We uniformly sampled $19 \times 19$ barycentric coordinates $(a,b)\in [0,1]^2$, such that $a+b \leq 1$. The third barycentric coordinate was computed as $c = 1-a-b$. We use these barycentric coordinates to compute the coordinates of the corresponding point in the second image. We then evaluated the symmetric epipolar error of this point and the mean point in the first image.
Fig.~\ref{fig:mean_tests} (left) shows the average symmetric epipolar error as a function of the barycentric coordinates of the point in the second image. 
The 2D Gaussian distribution fitted to these results has mean $\mu = (0.333,0.332)$. In this point the minimum was reached. The point with barycentric coordinates $(0.\bar{3}, 0.\bar{3})$ corresponds to the mean point of the triangle.


 \begin{figure}[t!]
    \centering
	\includegraphics[width=0.45\columnwidth]{figures/synth_symmetric_error.pdf}
	\includegraphics[width=0.45\columnwidth]{figures/real_symmetric_error.pdf}
\caption{Distribution of the average symmetric epipolar error as a function of barycentric coordinates of the point in the second image \wrt the mean point of three points in the first image, for synthetic data (left) and real data (right).  }
\label{fig:mean_tests}
\end{figure}


\begin{figure*}[t]
    \centering
	\begin{subfigure}[t]{0.27\textwidth}
	    \includegraphics[trim=20 0 30 10, clip , width=1.0\textwidth]
     %{figures/outlier_exp/0.1/sc/avg1.png}
     {figures/sacre_coeur0.1_2w_f4.pdf}
		\caption{0.1 Inlier Ratio}
    	\label{fig:outlier0.1}
	\end{subfigure}
	\begin{subfigure}[t]{0.27\textwidth}
	    \includegraphics[trim=10 0 30 10, clip ,width=1.0\textwidth]%{figures/outlier_exp/0.2/sc/avg.png}
       {figures/sacre_coeur0.2_2w_f4.pdf}
		\caption{0.2 Inlier Ratio}
    	\label{fig:outlier0.2}
	\end{subfigure}
	\begin{subfigure}[t]{0.27\textwidth}
	    \includegraphics[trim=20 0 30 10, clip ,width=1.0\textwidth]%{figures/outlier_exp/0.4/sc/avg.png}
       {figures/sacre_coeur0.4_2w_f4.pdf}
		\caption{0.4 Inlier Ratio}
        \label{fig:outlier0.4}
	\end{subfigure}
    \caption{Outlier Experiments: We tested the performance, in terms of average pose error (maximum of rotation and translation errors) of the solvers for different inlier ratios and for different numbers of RANSAC iterations. The result is for the Sacre Coeur scene from Phototourism dataset~\cite{snavely2006photo}. We plot the results for (a) 0.1 inlier ratio, (b) 0.2 inlier ration and (c) 0.4 inlier ratio. }
    \label{fig:synth_outlier}
\end{figure*}

We performed the same experiment on real data. In this  experiment we used the SfM model of the Shop Facade scene from the Cambridge Landmark dataset~\cite{Kendall2015ICCV} to obtain image pairs. 
We extracted 4,525 pairs and used exact projections of 3D points into these pairs to simulate noise-less data. 
In each pair, we sampled 100 different triplets of points and for each, we performed the same experiment as described above. Altogether, we evaluated 452,500 triplets (corresponding triangles). 
Fig.~\ref{fig:mean_tests} (right) again shows the average symmetric epipolar error as a function of barycentric coordinates of the point in the second image. The distribution of errors was very similar to the synthetic data with the minimum at $(0.333,0.333)$, \ie, the mean point. 

\PAR{Noise experiments.}
We tested the performance of our solvers, and the state-of-the-art algorithms \wrt increasing image noise. 
We use the SfM model of the botanical garden scene %(randomly picked from all scenes) 
from the ETH3D dataset~\cite{Schops_2017_CVPR} to obtain instances of points in three views. 
Perfect correspondences with no noise are generated by projecting the 3D points into the images. 
We then add increasing amounts of normally distributed noise to these correspondences. 
Due to the space restrictions, the results of this experiment are presented in the supp. material.

Here we just summarize the main observations from this experiment:
Due to the approximate nature of virtual correspondences, our novel solvers return non-zero errors for zero noise. However, at noise levels $\geq 2px$, these solvers return comparable or even better results than the \sft/\sst solvers. This shows that our predicted virtual correspondences are good approximations of real correspondences.
The recent state-of-the-art solver~\cite{Hruby_cvpr2022} is failing in about 50\% of the instances for noiseless data, even though the solver was trained on the ETH3D dataset. Thus, the median errors are significantly larger than the median errors of the remaining solvers. 


% \subsection{Oracle 4v3p solvers}
\PAR{Oracle 4v3p solvers.}
To obtain upper bounds for the precision that can be achieved by our %of the 
proposed %4p3v solvers, \ie, the  
solvers, as well as the state-of-the art \sfep solver~\cite{DBLP:journals/ijcv/NisterS06}, we consider the following ``oracle'' solvers for real experiments: 
1) 
The \sfto/\ssto solvers use a correct correspondence(s), \ie, correspondences that satisfies the epipolar constraint for the ground truth relative pose of the first two cameras, as the $5^\text{th}$/$6^\text{th}$ virtual correspondence between these cameras. Then the \sft/\sst solver is applied to estimate the relative pose of three cameras. 
The \sfto/\ssto shows the maximum precision that %the  \sftl and \sftm can reach,
our  solvers can reach, if they would have been able to predict or infer a precise %correct 
$5^\text{th}$/$6^\text{th}$ correspondence from the coordinates of four input correspondences.
2) We test an oracle version of the %Nister's 
\sfep solver. In this \sfepo solver, instead of performing a one-dimensional search over the $10^{\text{th}}$ degree curve of possible epipoles, we provide the solver with the ground truth epipole. More details on this solver are in the supp. material. 




\begin{table*}[]
    \centering
    \setlength{\tabcolsep}{4.8pt}
    \resizebox{1.0\linewidth}{!}{\begin{tabular}{r | c c c c c c | c c c c c c || c}
        \toprule
        & \multicolumn{6}{c |}{Sacre Coeur (5000 image triplets)}  & \multicolumn{6}{c}{St.\ Peters' Square (5000 image triplets)} \\
        \midrule
        Estimator & AVG ($^\circ$) $\downarrow$ & MED ($^\circ$) $\downarrow$ & AUC@5$^\circ$ $\uparrow$ & @10$^\circ$ & @20$^\circ$ & Time (s) & AVG ($^\circ$) $\downarrow$ & MED ($^\circ$) $\downarrow$ & AUC@5$^\circ$ $\uparrow$ & @10$^\circ$ & @20$^\circ$ & Time (s) & Time ($\mu$s)\\
        \midrule
        cons. 5pt & 14.24 & \phantom{1}5.06 & 35.20 & 45.82 & 56.76 & \phantom{1}0.15 & 16.46 & \phantom{1}9.12 & 16.49 & 30.27 & 46.39 & \phantom{1}0.04 & \phantom{1}\phantom{1}13.38 \\
        joint 5pt & 24.88 & \phantom{1}4.33 & 38.45 & 47.32 & 55.25 & \phantom{1}0.29 & 26.36 & 10.47 & 15.31 & 28.15 & 43.38 & \phantom{1}0.36 & \phantom{1}\phantom{1}13.38 \\
        \midrule
        %5PC + P3P 
        5pt+P3P & 23.80 & \phantom{1}1.86 & 47.69 & 55.13 & 61.25 & \phantom{1}3.32 & 26.02 & \phantom{1}8.67 & 20.85 & 33.69 & 47.17 & \phantom{1}3.13 & \phantom{1}\phantom{1}77.90\\
        \midrule
        4p3v(HC) \cite{Hruby_cvpr2022} & 24.72 & \phantom{1}3.04 & 42.81 & 50.61 & 57.27 & \phantom{1}\bf{2.60} & 27.06 & \phantom{1}9.73 & \bf{19.78} & 32.03 & 45.10 & \phantom{1}\bf{2.22} & \phantom{1}*66.06 \\
        
        4p3v(M) & 24.77 & \phantom{1}3.08 & 42.62 & 50.58 & 57.52 & \phantom{1}3.12 & 27.44 & \phantom{1}9.88 & 18.59 & 30.99 & 44.61 & \phantom{1}\underline{2.74} & \phantom{1}\phantom{1}83.92\\
        4p3v(M $\pm$ 0.15) & 24.49 & \phantom{1}\underline{2.42} & 44.51 & 52.27 & 58.91 & \phantom{1}\underline{3.10} & \bf{25.96} & \phantom{1}\underline{8.92} & 19.62 & \underline{32.53} & \underline{46.24} & \phantom{1}2.76 & \phantom{1}218.71\\
        4p3v(L) & 24.92 & \phantom{1}2.97 & 42.84 & 50.71 & 57.51 & \phantom{1}3.82 & 26.78 & \phantom{1}9.49 & 18.73 & 31.51 & 45.14 & \phantom{1}2.93 & \phantom{1}450.26 \\
        4p3v(L $\pm$ 0.1) & \underline{24.36} & \phantom{1}\bf{2.35} & \bf{44.89} & \underline{52.48} & \underline{59.00} & \phantom{1}3.76 & 26.12 & \phantom{1}9.06 & \underline{19.77} & 32.52 & 46.06 & \phantom{1}3.39 & \phantom{1}511.28\\
        4p3v(L $\pm$ 0.1 init) & \bf{23.95} & \phantom{1}2.46 & \underline{44.83} & \bf{52.78} & \bf{59.29} & \phantom{1}5.10 & \underline{26.09} & \phantom{1}\bf{8.83} & 19.67 & \bf{32.68} & \bf{46.48} & \phantom{1}4.67 & 1130.31\\
        \midrule
        4p3v(O) & 21.91 & \phantom{1}1.54 & 50.18 & 57.68 & 63.62 & \phantom{1}2.85 & 24.91 & \phantom{1}6.45 & 25.67 & 39.04 & 51.80 & \phantom{1}3.11 & \phantom{1}\phantom{1}80.47\\
        4p3v(NO) & 23.34 & \phantom{1}2.13 & 46.15 & 54.54 & 61.08 & \phantom{1}0.22 & 25.89 & \phantom{1}7.90 & 21.23 & 34.79 & 48.34 & \phantom{1}0.20 & \phantom{1}*73.65\\ 
        
        \midrule
    \end{tabular}
    }
    \caption{The average and median pose errors in degrees, and Area Under the Recall curve (AUC) thresholded at 5$^\circ$, 10$^\circ$ and 20$^\circ$ and the average run-time of RANSAC (in seconds) on a total of 10k image triplets from two scenes of the PhotoTourism dataset~\cite{IMC2020}. The best results among the solvers for the \sftp problem (excluding oracle solvers) are marked in bold, the second best are underlined. The last column contains the average running times of particular solvers in $\mu$s over 1,000 trials. $^{*}$ The time for \sfhc does not account for its high failure rate, the effective time is usually more than $4 \times$ higher~\cite{Hruby_cvpr2022}. Our time for the \sfhc is higher than the average time reported in~\cite{Hruby_cvpr2022}. This can be due to
    %longer run-time of HC on our instances, 
    different instances and hardware, and that time %reported 
    in~\cite{Hruby_cvpr2022} did not involve, \eg, pose extraction from estimated depths. The time for \sfepo is the time to evaluate one search point. Usually 40 -- 1,000 search points have to be evaluated.}
    \label{tab:phototourism_real_Test}
\end{table*}

% \subsection{Timings}
\PAR{Timings.} 
%\paragraph{Timings}
We computed the average processing times of the solvers over 1,000 random problem instances.  
The solvers were implemented in C++ within the GC-RANSAC~\cite{barath2021graph} framework using the Eigen library for performing matrix operations.
We used the \texttt{5pt}~\cite{Nister-5pt-PAMI-2004}, \texttt{6pt}~\cite{Hartley-PAMI-2012} and \texttt{P3P}~\cite{lambda-twist} methods implemented in GC-RANSAC for composing the discussed solvers.
For \sfhc we used the implementation from the authors\footnote{https://github.com/petrhruby97/learning\_minimal} with default settings~\cite{Hruby_cvpr2022}.
The experiments were performed on an Intel(R) Core(TM) i9-10900X CPU @ 3.70GHz.
The results for calibrated solvers are reported in the last column of Tab.~\ref{tab:phototourism_real_Test}.  The discussion of these results is included in Tab.~\ref{tab:phototourism_real_Test} and in Sec.~\ref{sec:discussion}. Timings for solvers for partially calibrated cameras are in the supp. mat.

\PAR{Outlier experiments.}
The advantage of~\sftp/\sstp solvers for the $(4_4,4_4,4_4)$ configuration over the \sft and \sst solvers for the $(5_3,5_3,3_3)$/$(6_3,6_3,3_3)$ configuration should be visible, especially in the presence of higher outlier contamination and a limited running time for RANSAC.
To study this scenario, we evaluate the performance of all proposed solvers, the state-of-the-art \sfhc solver~\cite{Hruby_cvpr2022}, and the \sft/\sst solvers w.r.t.\ decreasing inlier ratio, while reporting errors and metrics on different number of RANSAC iterations. For calibrated case we use the Sacre Coeur and St. Peters' Square scenes from PhotoTourism dataset~\cite{snavely2006photo}. We run the solvers inside RANSAC~\cite{barath2021graph} and measure their performance on a series of iterations, only on triplets of images with a given inlier ratio. We used the uniform sampler inside RANSAC. To obtain triplets of images with the given inlier ratio, for each triplet, we keep a certain amount of inliers depending on the total number of correspondences, and replace the rest by uniformly random outlier w.r.t. ground truth poses, correspondences. We filter out image triplets that do not contain enough inliers. The results of this experiment for calibrated cameras and inlier ratios  $\{0.1, 0.2, 0.4\}$ are presented in Figure~\ref{fig:synth_outlier}. 
The results indicate that the new proposed \sftp solvers reach lower errors faster than the \sft solver that samples one more point in the first two cameras. The  HC~\cite{Hruby_cvpr2022} solver, due to its high failure rate, converges even slower than the \sft solver.

\PAR{Real data experiments.} 
We tested the solvers on two scenes from the validation set of the IMC2020 benchmark~\cite{IMC2020}.
The scenes, Sacre Coeur and St.\ Peter's Square, come from the PhotoTourism dataset~\cite{snavely2006photo} and provide ground truth pose via a COLMAP~\cite{Schoenberger2016CVPR} reconstruction.
To form image triplets, we iterated through the point correspondences provided in~\cite{IMC2020} and selected pairs of image pairs that share one view.
Since the provided correspondences do not contain descriptors, we detected RootSIFT~\cite{lowe1999object,Arandjelovic2012CVPR} features in all three images of each triplet.
We then matched features between the view pairs 1-2 and %th and 
1-3 %th views 
by standard nearest neighbors matching. 
To enforce cycle consistency, we kept only those correspondences that were matched to the same features in all view pair combinations. 
In total, we collected 5,000 image triplets from each scene.

To estimate the relative poses between the images of each triplet, we apply the Graph-Cut RANSAC~\cite{barath2021graph} robust estimator. 
The point-to-model residual function is chosen to be the Sampson error of a triplet correspondence average over image pairs 1-2 %th 
and 1-3. %th. 
%
We calculate the error of the triplet pose by, first, taking the average of the relative rotation and translation errors coming from essential matrices $\textbf{E}_{12}$ and $\textbf{E}_{13}$ given the ground truth relative pose. 
Finally, we take the maximum of the averaged rotation and translation errors (both measured in degrees) as the pose error~\cite{IMC2020}. 

We test the focal length solvers on the KITTI dataset~\cite{geiger2013vision}.
We form triplets by connecting the stereo pair from the $i$-th frame to the left image in the $i+1$-th one.
Similarly as before, we extract and match RootSIFT features.  

Tab.~\ref{tab:phototourism_real_Test} and~\ref{tab:kitti} report the average and median pose error in degrees, the Area Under the recall Curve (AUC) thresholded at $5^\circ$, $10^\circ$, and $20^\circ$, and the average processing time in seconds.
The results 
%of these experiments 
are discussed in Sec.~\ref{sec:discussion}. %the next section.
More results on real data are in the supp. material.

\section{Discussion and limitations}
\label{sec:discussion}
In this paper, we present solvers to the challenging problems of estimating the relative pose of three calibrated and partially calibrated cameras from four correspondences. We tested our solvers on different real-world datasets (Tabs.~\ref{tab:phototourism_real_Test} and~\ref{tab:kitti}) from which several conclusions can be drawn: 


\begin{itemize}[leftmargin=0.3cm]
\item The very simple \sftmd/\sstmd solvers, which use only mean points as virtual ``approximate" correspondences and two more points in their vicinity, achieve state-of-the-art results on the challenging \sftp/\sstp problems.
Compared to state-of-the-art solvers for the \sftp problem, our new \sftmd/\sstmd solvers are extremely simple to implement since they just run existing well-known 5pt/6pt and P3P solvers that are available in many libraries. They do not require any network training, parameter tuning, optimization, or implementation of complex solution techniques as required by the \sfhc and \sfep solvers.
As such, our novel M-solvers should be considered as new baselines for the \sftp/\sstp problems.
\item The \sftmd/\sstmd solvers call the~5pt/6pt solvers %$3\times$ 
three times
and are thus slower than the \sftm/\sstm solvers. However, since 
%\sftmd/\sstmd 
$\delta$-solvers
provide better estimates, the total running times of RANSAC with the \sftmd/\sstmd solvers are almost identical to the running times of RANSAC with the \sftm/\sstm solvers, and comparable to the \sfhc~\cite{Hruby_cvpr2022},
%solver, 
while achieving better precision.
\item Our novel L-based solvers for the \sftp problem show a slight improvement over the M-based solvers (\cf Tab~\ref{tab:phototourism_real_Test}). Although the architecture of our proposed network is very simple and can be considered more as a prototype, it shows the potential to learn virtual correspondences and achieve results closer to the oracle \sfto solver.
\item For the \sstp problem with unknown focal length, the proposed \sstmd solver achieves results very  close to the oracle  \ssto solver (\cf Tab.~\ref{tab:kitti}). For this problem, the \sstl solver achieves smaller average but larger median errors than the \sstm solver. We attribute this to the fact that we use a network trained for the 5pt solver, while the 6pt solver exhibits more degenerate configurations, and the fact that for this dataset, mean point correspondences are most likely very good approximations of correct correspondences.
\item  The state-of-the-art results of our solvers show the potential of the proposed method based on virtual correspondences to be applied to other camera geometry problems. 
A discussion on this can be found in the supp. material.
\item  We are the first to properly compare solvers for two different minimal configurations, \ie, the $(5_3,5_3,5_3)$ and $(4_4,4_4,4_4)$ for three calibrated cameras on real-world datasets within a state-of-the-art RANSAC framework~\cite{barath2021graph}.
Although our novel solvers achieve state-of-the-art results for the $(4_4,4_4,4_4)$  configuration, our results show that in scenarios where we have enough computational time and a small outlier contamination, it is preferable to use the solver \sft
and sample one more point in two views. However, the results of the oracle \sfto solver suggest that there is still room for improving \sftp solvers. Moreover, our outlier experiments (\cf Fig~\ref{fig:synth_outlier}) show that in the presence of larger outlier ratios and limited computational time, the proposed \sftp solvers are preferable.
\end{itemize}

\begin{table}[]
    \centering
    \setlength{\tabcolsep}{4.8pt}
    \resizebox{1.0\linewidth}{!}{\begin{tabular}{r | c c c c c c c }
        \toprule
        Estimator & AVG ($^\circ$) $\downarrow$ & MED ($^\circ$) $\downarrow$ & AUC@1$^\circ$ $\uparrow$ & @2.5$^\circ$ & @5$^\circ$ & @10$^\circ$ & Time (s)\\
        \midrule
        cons. 6pt & \phantom{1}1.69 & \phantom{1}1.14 & 11.77 & 47.34 & 70.30 & 84.57 & \phantom{1}0.05\\
        joint 6pt & \phantom{1}1.59 & \phantom{1}1.09 & 12.85 & 48.77 & 71.05 & 85.00 & \phantom{1}0.05\\ 
        \midrule
        6pt+p3p & \phantom{1}1.83 & \phantom{1}1.00 & 15.79 & 51.14 & 71.73 & 84.89 & \phantom{1}0.11\\
        \midrule
        4p3vf(M) & \phantom{1}1.89 & \phantom{1}1.05 & 14.47 & 49.64 & 70.81 & 84.44 & \phantom{1}\bf{0.07}\\
        4p3vf(M $\pm$ 0.15) & \phantom{1}\underline{1.66} & \phantom{1}\bf{0.99} & \bf{15.74} & \bf{51.36} & \bf{72.05} & \bf{85.27} & \phantom{1}\bf{0.07}\\
        4p3vf(L) & \phantom{1}\bf{1.58} & \phantom{1}1.10 & 12.88 & 48.79 & 71.09 & 85.06 & \phantom{1}\bf{0.07}\\ 
        4p3vf(L $\pm$ 0.1) & \phantom{1}1.69 & \phantom{1}\underline{1.01} & \underline{15.44} & \underline{50.68} & \underline{71.55} & \underline{85.07} & \phantom{1}\underline{0.10}\\
        4p3vf(L $\pm$ 0.1 init) & \phantom{1}1.74 & \phantom{1}\underline{1.01} & 15.30 & 50.30 & 71.34 & 84.84 & \phantom{1}0.18\\
        \midrule
        4p3vf(O) & \phantom{1}1.58 & \phantom{1}0.98 & 16.05 & 51.59 & 72.25 & 85.51 & \phantom{1}0.09 \\
        \midrule
    \end{tabular}
    }
    \caption{Results on KITTI dataset~\cite{geiger2013vision} for partially calibrated cameras. The reported statistics are the same as in Tab.~\ref{tab:phototourism_real_Test}. }
    \label{tab:kitti}
\end{table}

\section{Conclusion}
 In this paper, we consider the highly challenging %4p3v 
problem of relative pose estimation of three calibrated and partially calibrated cameras from four correspondences. 
We propose a novel framework that solves these problems using existing efficient solvers by simply predicting a $5^\text{th}$/$6^\text{th}$ point correspondence(s). 
We propose several solvers based on this framework, one simply using mean coordinates of input points (M-based solvers) and one using a trained predictor (L-based solvers). 
Extensive experiments show that our solvers achieve state-of-the-art performance on real data for the configuration of four points in three views. 
At the same time, our M-based solvers 
%based on using mean points 
are trivial to implement, especially compared to the current state-of-the-art~\cite{Hruby_cvpr2022}. 

\PAR{Acknowledgements.}
Charalambos Tzamos was supported by the Czech Science Foundation (GAÄR) JUNIOR STAR Grant (No. 22-23183M) and by SGS project (No. SGS23/173/OHK3/3T/13). Daniel Barath was supported by the ETH postdoc fellowship. Torsten Sattler was supported by the EU Horizon 2020 project RICAIP (grant agreement No. 857306) and the European Regional Development Fund under project IMPACT (No. CZ.02.1.01/0.0/0.0/15\_003/0000468). Zuzana Kukelova was supported by the Czech Science Foundation (GAÄR) JUNIOR STAR Grant (No. 22-23183M).

{\small
\bibliographystyle{ieeenat_fullname}
\bibliography{bibliography}
}

\section*{Supplementary Material}
This supplementary material provides more details on the architecture, training data, and training of the network used in the \sftl/\sstl solvers presented in Sec.~\ref{sec:calibrated} and~\ref{sec:partial_cam} of the main paper (Sec.~\ref{sec:sm_network}), a discussion of the proposed method for solving camera geometry problems using virtual correspondences (Sec.~\ref{sec:sm_method}), more details on the experiments presented in Sec.~\ref{sec:experiments} of the main paper (Sec.~\ref{sec:sm_experiments}), and potential directions for future work (Sec.~\ref{sec:sm_future_work}). 

 

\section{Details on the network}
\label{sec:sm_network}
As described in the main paper, we use a simple MLP based backbone, similar to \cite{cavalli2022nefsac}. More precisely, our input consists of four 6-vectors, of the $x,$ and $y$ coordinates of four point correspondences in three views. The first part of the model is a shared MLP 3-block of dimensions 6, 32, and 32, exporting a 32-dimensional feature for every correspondence. Then we apply a channel-wise max pooling aggregation, which is the concatenated at the end of each 32-dimensional feature. This results in having an 64-vector for each correspondence, which are passed into another shared MLP 3-block of 64, 64, and 64 dimensions. We aggregate those vector via a max pooling function to get a 64-vector encoding, which eventually passes through an MLP 3-block to reduce the dimension gradually from 64, to 32 and finally to 2, which will be the prediction of the $x$ and $y$ coordinates in the second camera. In all MLPs, in the first 2 layers of the blocks, we use a Leaky RELU activation function \cite{leakyRELU} with slope 0.01. As for the last layer of the MLPs, in the first two blocks, we use a RELU activation function, while in the last MLP we use a $\tanh$ activation, since we want the output to be in the range $(-1, 1)$. For a visualization of the architecture, see Figure~\ref{fig:net}.

For both training and validation, we use synthetic data. Our synthetic dataset contains 1M input instances. 70\% of the dataset is used for training while the rest 30\% is used for validating the performance of the network. We generate 10K 3D points inside a 10$\times$10$\times$10 cube, and to generate each instance, we pick 4 random points and project them to 3 cameras with random rotations and translations that view the cube from a random distance between 20 to 50 units. 

The network is implemented in PyTorch \cite{paszke2017automatic}, and we use the Adam optimizer \cite{DBLP:journals/corr/KingmaB14} for the training. We train it in batches of 1024 input instances, with a fixed learning rate of 1e-5. In our experiments, the network converges in about 30 epochs.

\section{Solving camera geometry problems using virtual correspondences }
\label{sec:sm_method}
The proposed methods generating \emph{virtual correspondences}, \ie, the learning-based and the mean-point-based methods that were used in the novel solvers for the \sftp and \sstp problems, can be, in general, applied to other camera geometry estimation problems. % algorithm that is 
%based on point correspondences. 
%

\PAR{M-based solvers}
While the mean point correspondence used in the M-based solvers can provide a good approximation of a correct correspondence, such a correspondence can be noisy. %\footnote{
Note that all state-of-the-art \sftp solvers (including 
\sfhc and \sfep) rely on certain approximations without establishing theoretical proofs to quantify their accuracy. 
In contrast, 
% On the other hand, 
the error of our virtual correspondence is bounded: %.}. 
% However, a
As mentioned in the main paper, it can be proven that the error of the virtual correspondence $\V m^1 \leftrightarrow\V m^2$ is bounded by the maximum distance of the mean point $\V m^2$ from the vertices of the triangle $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$.
Here we provide a simple proof.

\begin{figure}[t]
     \centering
     \includegraphics[width=1\columnwidth]{figures/4p3v_illustration_3-crop.pdf}
          \caption{Illustration of the considered situation.}
     \label{fig:illustration}
 \end{figure}

\begin{lemma}
Let us assume two cameras with camera centers $\V C^1$ and $\V C^2$ that observe 3D points $X_i, X_j,$ and $X_k$. Let $\left\{\V x_i^1,\V x_j^1,\V x_k^1\right\}$ and $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$ be the projections of these 3D points in camera 1 and camera 2, respectively. Let $\V m^1$ be the mean point of the points $\left\{\V x_i^1,\V x_j^1,\V x_k^1\right\}$ and let $\V E$  be the essential matrix between these two cameras, \ie, a matrix that satisfies $\V x_l^{2^\top} \V E \V x_l^1 = 0, \; \forall l \in \left\{i,j,k\right\}$. Then the epipolar line $\V E \V m^1$ passes through the triangle $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$.
\label{lemma:inter}
\end{lemma}

\begin{proof}
The camera center $\V C^1$ and the 3D points $\V X_i, \V X_j,$ and $\V X_k$ form a tetrahedron $T^1$ (see Figure~\ref{fig:illustration}.). The projections $\left\{\V x_i^1,\V x_j^1,\V x_k^1\right\}$ in the first camera lie at the edges of this tetrahedron $T^1$.
%(see Figure~\ref{fig:illustration}). 
The ray from the camera center $\V C^1$ through the mean point $\V m^1$ thus lies inside the tetrahedron $T^1$ and intersects the plane defined by 3D points $\V X_i, \V X_j,$ and $\V X_k$ in a point $\V M$ that lies inside the triangle defined by $\left\{\V X_i,\V X_j,\V X_k\right\}$. 

The camera center $\V C^2$ and the 3D points $\V X_i, \V X_j,$ and $\V X_k$ form a tetrahedron $T^2$.
Again, the projections $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$ 
%in the second camera 
lie at the edges of the tetrahedron $T^2$.
The ray passing through the camera center $\V C^2$ and the 3D point $\V M$ lies inside the tetrahedron $T^2$ and thus intersects the image plane of the second camera at a point that lies inside the triangle defined by the points $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$. 
By construction, the projection of $\V M$ into the second camera lies on the epipolar line $\V E \V m^1$. 
Therefore, the epipolar line $\V E \V m^1$ which is a line connecting this point and the epipole $\V e^2$, passes through the triangle $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$. 
\end{proof}

Obviously, it follows from Lemma~\ref{lemma:inter} that since the epipolar line $\V E \V m^1$ passes through the triangle $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$, the maximum distance of the mean point $\V m^2$ to the epipolar line $\V E \V m^1$ is equal to the maximum distance of $\V m^2$ to the vertices of the triangle.

The final error of the estimated pose, however, depends on many aspects, \eg, the baseline and the view angles of the cameras \wrt the 3 points used, the depth of these points, 
and the size of the triangle. 
Although in theory, this error could be large, our experiments show that it does not lead to significantly larger errors than the \sft method, which samples an additional fifth correspondence, rather than predicting or approximating one (\eg, for the ShopFacade from the Cambridge Landmarks dataset, avg/med epipolar error of the mean point prediction is 6.01px/2.94px).

A deeper analysis of the error propagation for virtual correspondences is out of the scope of this paper and better fits a follow-up publication dealing with the proposed method as a general method for solving camera geometry problems.
In this paper, we decided to focus on two
important problems of estimating the relative pose of three cameras from four point correspondences, which have been considered as not fully solved for decades.


\PAR{L-based and $\delta$-based solvers}
Our simple L-based and $\delta$-based solvers, that either predict ``a shift" of the mean point in the second image using a simple network or generate two more potential correspondences in the $\delta$-vicinity of the mean-point correspondence, show the potential to deal with noise in the mean-point correspondences. Note that we tested only a simple network architecture and a simple strategy to generate point correspondences in the $\delta$-vicinity of the mean-point correspondence. More advanced network architectures (\eg, using a set transformer architecture \cite{set_transformer}) or a more advanced method to generate additional $\delta$-point correspondences have the potential to improve the results even more and reduce the gap between our proposed \sftp/\sstp solvers and the oracle  \sfto/\ssto solvers.

\begin{figure*}[!h]
\centering  
    \includegraphics[trim=0 350 0 100, clip ,width=1\textwidth]{figures/network_arch.png}
    \caption{The architecture of our network. With LR we denote the Leaky RELU activation function \cite{leakyRELU}. The input consists of four 6-vectors, one for each correspondence. Each 6-vector contains the $x$ and $y$ coordinates of a correspondence in three views. First, the input passes through a shared MLP block, so that each 6-vector is processed independently. Then, the output feature vectors of dimension 32 are aggregated using a channel-wise max pooling function, and the result of it is concatenated at the end of each feature vector. Then, there is one more block of shared MLPs, of which the results are aggregated again by a max pooling, to get a single 64-vector. We reduce the dimensions of this vector by passing it through an MLP block, of which the last layer has 2 nodes and a $\tanh$ activation function.}
    \label{fig:net}
\end{figure*}

\PAR{Use-cases for virtual correspondences}
We believe that the proposed method based on virtual correspondences is general and, due to its simplicity, has the potential to generate efficient and easy-to-implement solvers for other problems. 
Generating one or potentially more \emph{virtual correspondences} using only %an information purely from 
the locations of the input
correspondences can allow to solve complex minimal problems using much simpler non-minimal solvers, while still sampling only a minimal number of points inside RANSAC.


In the first step, our M-based solvers solve \sfm/\ssm relative pose problems of two calibrated/partially calibrated cameras using only a sub-minimal sample of four point correspondences. 
The good performance of these solvers, not only for the pose of the whole triplet of cameras but also for the relative pose of the first two cameras (\cf Sec.~\ref{sec:sm_noise} and  Figure~\ref{fig:noise_uncalibrated}), suggests that the method based on virtual correspondences can also be used to solve minimal problems using sub-minimal samples inside RANSAC in some scenarios.


%Actually, our M-based solvers, in the first step, solve \sfm/\ssm relative pose problems using only a sub-minimal sample of four point correspondences.
%Fig.~\ref{fig:noise} (middle) and (right) 
%show %some 
To better illustrate this use-case, 
%in Sec.~\ref{sec:noise} we show also errors for the first two cameras. Moreover, 
in Sec.~\ref{sec:sm_5pt} we show 
preliminary results for  the well-known 5pt relative pose problem, where we sample only four point correspondences inside RANSAC and generate the $5^\text{th}$ correspondence as the mean point (or points in its $\delta$-vicinity) of three corresponding points in both images. 
Even though four points in two views define an infinite number of camera poses that can observe these points, we show that, in practice, such a mean-point  correspondence is usually a quite good approximation of a correct correspondence.
%and the solver that 
While this $5^\text{th}$ correspondence can be noisy, the resulting \texttt{4pt(M)} and \texttt{4pt(M$\pm \delta$)} solvers show comparable performance to the %minimal 
%well-known 
5pt solver %relative pose problem 
inside RANSAC. 
% 
This shows the potential for applying such a sub-minimal solver in scenarios with lots of outliers and noisy data, where sampling one less point may show some improvement.
%\TODO{maybe point to noise experiment}

While the method based on virtual correspondences can be theoretically applied to any camera geometry problem, we see larger promise in relative pose problems, where it is sufficient to find one 2D point that is sufficiently close to the epipolar line. For %the 
absolute pose solvers, a %good 
virtual correspondence will need to be close to a 2D point instead of an epipolar line.



\section{Experiments}
\label{sec:sm_experiments}
 This section provides more details on the  experiments presented in Sec.~\ref{sec:experiments} of the main paper, 
 %\TODO{will we have this?} 
 as well as results showing the potential of the proposed method based on virtual correspondences to solve minimal problems using sub-minimal samples of points. 

\begin{figure*}[t]
    \centering
	\begin{subfigure}[t]{0.49\textwidth}
	    \includegraphics[width=1.0\textwidth]{figures/plot_pose_error_1.pdf}%{figures/err_avg_R_final.eps}
		% \caption{}    	%\label{fig:ground_rot_error_ori}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
	    \includegraphics[width=1.0\textwidth]
{figures/plot_pose_error_uncalibrated.pdf}
     %{figures/plot_R12_error_1.pdf}
     % {figures/err_avg_t_final.eps}
		% \caption{}
    	\label{fig:ground_tra_error_ori}
	\end{subfigure}\\
%	\begin{subfigure}[t]{0.48\textwidth}
	    %\includegraphics[width=1.0\textwidth]
        %{figures/plot_t12_error_1.pdf}%{figures/err_avg_R12_final.eps}
		% \caption{}
%		\end{subfigure}
    \caption{ Noise experiment showing 
    the pose error measured as $\texttt{max}(\texttt{avg}(e(\M R_{12}),e(\M R_{13})),\texttt{avg}(e(\M t_{12}),e(\M t_{13})))$, for the calibrated \sftp problem (left) and the partial calibrated \sstp problem (right) as a function of the noise scale in pixels. Here, $\M R_{ij}$ and $\M t_{ij}$ are the relative rotation and translation of the $i^{\text{th}}$ and $j^{\text{th}}$ views, respectively.}
    \label{fig:noise}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
	    \includegraphics[width=1.0\textwidth]{figures/plot_R12_error_1.pdf}
       %{figures/err_avg_R_final.eps}
		% \caption{}
%    	\label{fig:ground_rot_error_ori_uncalibrated}
	\end{subfigure}
 \begin{subfigure}[t]{0.48\textwidth}
	    \includegraphics[width=1.0\textwidth]{figures/plot_t12_error_1.pdf}
     %{figures/err_avg_R_final.eps}
		% \caption{}
    %	\label{fig:ground_rot_error_ori_uncalibrated}
	\end{subfigure}
		\begin{subfigure}[t]{0.48\textwidth}
	    \includegraphics[width=1.0\textwidth]{figures/plot_R12_error_uncalibrated.pdf}
     % {figures/err_avg_t_final.eps}
		% \caption{}
    	\label{fig:ground_tra_error_ori_uncalibrated}
	\end{subfigure}
	\begin{subfigure}[t]{0.48\textwidth}
	    \includegraphics[width=1.0\textwidth]
        {figures/plot_t12_error_uncalibrated.pdf}%{figures/err_avg_R12_final.eps}
		% \caption{}
		\end{subfigure}

  
    \caption{  Noise experiment showing $e(\M R_{12})$ 
    (left) and $e(\M t_{12})$ (right) as functions of the noise scale in pixels for the calibrated \sftp problem (top) and the partially calibrated \sstp problem (bottom). Here $\M R_{12}$ and $\M t_{12}$ are the relative rotation and translation between the first two views.}%of the $i^{\text{th}}$ and $j^{\text{th}}$ views, respectively.}
    \label{fig:noise_uncalibrated}
\end{figure*}


\subsection{Oracle solvers}
We first provide more details on implementation of our oracle  version of the %Nister's 
\sfep solver~\cite{DBLP:journals/ijcv/NisterS06}, 
% We test an oracle version of the %Nister's 
\ie the \sfepo solver. 

In the \sfepo solver, instead of performing an one-dimensional search over the $10^{\text{th}}$ degree curve of possible epipoles, we provide the solver with the ground truth epipole. 
To simulate the effect of sampling four points for this solver inside RANSAC, instead of using the second epipole and the epipolar line homography to get the essential matrix $\M{E}$, as suggested in the implementation details of~\cite{DBLP:journals/ijcv/NisterS06}, we use the second suggested way on how to obtain $\M{E}$, \ie, using their \step solver. However, we feed the \step solver %, we however, feed 
with four points and use SVD instead of the null space. The rest of the solver performs the triangulation and registers the last camera using the P3P solver~\cite{lambda-twist}. This is identical to the original \sfep solver. 
However, in this case we do not need to use the fourth point correspondence to select the pose that minimizes the reprojection error. Similarly, we do not need to use refinement. 
Remember that the original %Nister's 
\sfep solver needs to call these evaluations for each search step on the $10^{\text{th}}$ degree curve of possible epipoles (usually $40{\times}-1000{\times}$~\cite{DBLP:journals/ijcv/NisterS06}). Moreover, this solver has several sources of errors, \eg, the $10^{\text{th}}$ degree curve is affected by noise; sparse sampling of the points on the curve introduces additional potentially large noise in the epipole; the reprojections of the fourth image point in the third view, traced out by sweeping through the curve of possible epipoles, generates complex curves in the third view, with the reprojection cost function having a lot of local minima. In the paper~\cite{DBLP:journals/ijcv/NisterS06} it was reported that %, \eg, 
even for exact data and 1000 search points followed by refinement at multiple local minima the failure rate of the solver is $\approx 3\%$. Therefore, we expect the original solver \sfep to perform much worse that the ``oracle" \sfepo solver used in the  experiments presented in the main paper.

 \subsection{Noise experiments}
 \label{sec:sm_noise}
 %\TODO{Update - ZK}
%\PAR{Noise experiments.}
We tested the performance of our solvers and the state-of-the-art algorithms \wrt increasing image noise. 
We used the SfM model of the botanical garden scene (randomly selected from all scenes) from the ETH3D dataset~\cite{Schops_2017_CVPR} to obtain instances of 5/6 points in three views by identifying images in the scene that share 3D points. 
Perfect noise-free correspondences are generated by projecting the 3D points into the images. 
We then add increasing amounts of normally distributed noise to these correspondences. 
We generated more than 9k instances, but show only 1k results per plot to avoid clutter. 
Note that the \sfhc solver was trained on the ETH3D dataset while our %\sftl
L-based solvers were trained on purely synthetic data. 
% Our experimental setup is thus not biased towards our solvers. 
% Since the network used in our \sftl solver was trained on synthetic scenes, here we decided to use real dataset....\TODO{Torste finish}



The results for increasing noise in the image points are shown in Figs.~\ref{fig:noise} and~\ref{fig:noise_uncalibrated}. 
The results are represented by boxplot function which shows the 25\% to 75\% quantiles as a box with a horizontal line at median. Crosses show data beyond 1.5 times the interquartile range.
Let $e(\M R_{ij})$  be the error of the estimated relative rotation between cameras $i$ and $j$, computed as the angle in the axis-angle representation of $\M R_{ij}^{-1}\M R_{ij}^{\M{GT}}$ and let $e(\M t_{ij})$ be the error of the estimated translation computed as the angle between the two unit vectors corresponding to the translations. 
Fig.~\ref{fig:noise} shows boxplots of %
pose errors measured in the same way as in our experiments in the main paper (\cf Sec.~\ref{sec:experiments} in the main paper), \ie as
$\texttt{max}(\texttt{avg}(e(\M R_{12}),e(\M R_{13})),\texttt{avg}(e(\M t_{12}),e(\M t_{13})))$,
for the calibrated \sftp problem (left), and the partially calibrated \sstp problem (right).  
The errors are zoomed into an interesting interval and are shown as functions of varying noise from $0px$ to $4px$. 
%$\texttt{avg}(e(\M R_{12}),e(\M R_{13}))$,  %(b) 
%$\texttt{avg}(e(\M t_{12}),e(\M t_{13}))$, and %(c) 


Due to the approximate nature of the virtual correspondences, our newly proposed M-based and L-based solvers 
%\sftl and \sftm 
return non-zero errors for zero noise. However, at noise levels $\geq 1px$,  
our $\delta$-based solvers (both M and L), and for the calibrated case even the pure \sftm and \sftl solvers, return comparable or even better results than the \sft and \sst solvers. 
%that sample one more point in the first two cameras. 
%
%both proposed solvers return comparable or even better results than the \sft solver, 
Note that the \sft and \sst solvers sample one/two more points (real correspondences) in the first two cameras, and these points are affected only by the considered noise.
%but uses real point correspondences affected only by considered noise. 
This shows that our predicted virtual correspondences are good approximations to real correspondences. 
%The \sftm solver is returning slightly worse rotation but slightly better translation errors than the \sftl solver.
For the calibrated case, the recent state-of-the-art solver~\cite{Hruby_cvpr2022} is failing in about 50\% of the instances for noiseless data, even though the solver was trained on the ETH3D dataset. Thus, the median errors are significantly larger than the median errors of the remaining solvers. 

The rotation and translation errors in the first two views, \ie, $e(\M R_{12})$ and $e(\M t_{12})$, for both the calibrated (top row), and the partially calibrated case (bottom row) are shown in Fig.~\ref{fig:noise_uncalibrated}.
For the partially calibrated case, our new solvers generate two approximate virtual correspondences in the first two views. Therefore, the \sstm and \sstl solvers have slightly larger errors than the \sst solver for all considered noise levels. 
However, similarly to the pose errors in Figure~\ref{fig:noise}, at noise levels $\geq 1px$ 
our $\delta$-based solvers (both M and L), and for the calibrated case even the pure \sftm and \sftl solvers, return comparable or even better results in the first two views than the \texttt{5pt}~\cite{Nister-5pt-PAMI-2004} and \texttt{6pt}~\cite{stewenius-etal-omnivis-2005} solvers, here represented by the results of \sft and \sst solvers.
%\footnote{Notice that we only report errors for the relative pose between the first two views, \ie, the results of the \sft and \sst solvers are identical to the results of the \texttt{5pt}~\cite{Nister-5pt-PAMI-2004} and \texttt{6pt}~\cite{stewenius-etal-omnivis-2005} solvers for these relative poses.}.
This shows an interesting potential of using  our solvers for the two-view relative pose estimation problems by solving these problems  from sub-minimal samples. 

\subsection{Outlier experiments}
\begin{figure*}[h]
    \centering
	\begin{subfigure}[t]{0.32\textwidth}
	    \includegraphics[trim=0 0 30 10, clip , width=1.0\textwidth]{figures/sacre_coeur_inlier_0.1.pdf}
		\caption{0.1 Inlier Ratio}
    	\label{fig:sm_outlier0.1}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
	    \includegraphics[trim=0 0 30 10, clip ,width=1.0\textwidth]{figures/sacre_coeur_inlier_0.2.pdf}
		\caption{0.2 Inlier Ratio}
    	\label{fig:sm_outlier0.2}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
	    \includegraphics[trim=0 0 30 10, clip ,width=1.0\textwidth]{figures/sacre_coeur_inlier_0.4.pdf}
		\caption{0.4 Inlier Ratio}
        \label{fig:sm_outlier0.4}
	\end{subfigure}
    \caption{Outlier Experiments: We tested the performance, in terms of the average percentage of inliers, of the solvers for different inlier ratios and for different numbers of RANSAC iterations. Results are shown for the Sacre Coeur scene from PhotoTourism dataset~\cite{snavely2006photo}. We plot the results for (a) 0.1 inlier ratio, (b) 0.2 inlier ratio, and (c) 0.4 inlier ratio. }
    \label{fig:outlier_sacre_inlier}
\end{figure*}

\begin{figure*}[h]
    \centering
	\begin{subfigure}[t]{0.32\textwidth}
	    \includegraphics[trim=0 0 30 10, clip , width=1.0\textwidth]{figures/peter_inlier_0.1.pdf}
		\caption{0.1 Inlier Ratio}
    	\label{fig:outlier0.1_3}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
	    \includegraphics[trim=0 0 30 10, clip ,width=1.0\textwidth]{figures/peter_inlier_0.2.pdf}
		\caption{0.2 Inlier Ratio}
    	\label{fig:outlier0.2_3}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
	    \includegraphics[trim=0 0 30 10, clip ,width=1.0\textwidth]{figures/peter_inlier_0.4.pdf}
		\caption{0.4 Inlier Ratio}
        \label{fig:outlier0.4_3}
	\end{subfigure}
    \caption{Outlier Experiments: We tested the performance, in terms of the average percentage of inliers, of the solvers for different inlier ratios and for different numbers of RANSAC iterations. Results are shown for the St Peters' Square scene from PhotoTourism dataset~\cite{snavely2006photo}. We plot the results for (a) 0.1 inlier ratio, (b) 0.2 inlier ratio, and (c) 0.4 inlier ratio. }
    \label{fig:outlier_peter_inlier}
\end{figure*}


\begin{figure*}[h]
    \centering
	\begin{subfigure}[t]{0.32\textwidth}
	    \includegraphics[trim=0 0 30 10, clip , width=1.0\textwidth]{figures/peter_0.1.pdf}
		\caption{0.1 Inlier Ratio}
    	\label{fig:outlier0.1_2}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
	    \includegraphics[trim=10 0 30 10, clip ,width=1.0\textwidth]{figures/peter_0.2.pdf}
		\caption{0.2 Inlier Ratio}
    	\label{fig:outlier0.2_2}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
	    \includegraphics[trim=10 0 30 10, clip ,width=1.0\textwidth]{figures/peter_0.4.pdf}
		\caption{0.4 Inlier Ratio}
        \label{fig:outlier0.4_2}
	\end{subfigure}
    \caption{Outlier Experiments: We tested the performance, in terms of the average pose error (maximum of rotation and translation errors), of the solvers for different inlier ratios and for different numbers of RANSAC iterations. Results are shown for the St. Peters' Square scene from the PhotoTourism dataset~\cite{snavely2006photo}. We plot the results for (a) 0.1 inlier ratio, (b) 0.2 inlier ratio, and (c) 0.4 inlier ratio. }
    \label{fig:outlier_peter_pose}
\end{figure*}
The outlier experiments are implemented as described in Section 5 the main paper. Here we provide additional plots for inlier ratios $\{0.1, 0.2, 0.4\}$. In Figures~\ref{fig:outlier_sacre_inlier} and~\ref{fig:outlier_peter_inlier}, we show the performance of each solver w.r.t. the percentage of inliers that they gather, on different RANSAC iterations, for the Sacre Coeur and St. Peters' Square scenes, respectively. In Figure~\ref{fig:outlier_peter_pose}, we show the performance of the solvers w.r.t. average pose error on the St. Peters' Square scene, similar to the one included in the main paper for Sacre Coeur. 
The pose error was measured in the same way as in our experiments in the main paper (\cf Sec.~\ref{sec:experiments} in the main paper), \ie, as
$\texttt{max}(\texttt{avg}(e(\M R_{12}),e(\M R_{13})),\texttt{avg}(e(\M t_{12}),e(\M t_{13})))$.


For low inlier ratios and in early RANSAC iterations, we observe that the $\delta$-based solvers perform the best, slightly better than \sftm and \sftl. \sft and \sfhc solvers show worse performance than our proposed solvers in this experiment. Increasing the inlier ratio up to 0.4, the performance of \sft increases, becoming similar to the performance of \sftm and \sftl. Even for an inlier ratio of 0.4, \sfhc exhibits a slower convergence due to the higher failure rate.


\subsection{Timings}
\begin{table*}
 \begin{center}
 \resizebox{1.0\linewidth}{!}{
 \begin{tabular}{|l| c | c | c | c | c | c |}
    \hline
    & \sst & \sstm & \sstmd & \sstl & \sstld & \sstlid \\
    \hline
    Time ($\mu$s) & 106.67 & 117.28 & 295.87 & 758.59 & 953.34 & 2162.77 \\
    \hline
\end{tabular}
 }
\end{center}
\caption{The average run-time, averaged over more than 10k instances of the Sacre Coeur scene of the PhotoTourism dataset~\cite{snavely2006photo}, of the solvers for the partially calibrated case.}
\label{tab:focal_time}
\end{table*}

In the main paper, we presented run-times of the proposed solvers as well as the state-of-the-art solvers for the relative pose problem of three calibrated cameras (\cf Table 1 (last column) in the main paper). Here we present run-times of solvers  for partially calibrated cameras.
To measure the run-times of the solvers, we calculated the average run-time of each solver on more than 10k instances of the Sacre Coeur scene of the PhotoTourism dataset~\cite{snavely2006photo}.
The run-times are reported in Table~\ref{tab:focal_time}. The experiments were performed on an Intel(R) Core(TM) i9-10900X CPU @ 3.70GHz.
The average run-times of the L-based solvers are higher, because we run the network on the CPU and without batching. In general, the implementations of all proposed solvers are not optimized for speed, and we still see room for speeding them up. 

\subsection{$\delta$-based solvers}
We tested our $\delta$-based solvers for different values of $\delta$ and measured their performance. 
The results of this experiment for the calibrated \sftp problem on the Sacre Coeur and St. Peters' Square scenes from the PhotoTourism dataset~\cite{snavely2006photo} are reported in Table~\ref{tab:phototourism_real_Test_delta}. 

In general, there is no common value of the $\delta$ shift that leads to the best results on all datasets. This is expected since the precision of the mean-point correspondence depends on many different factors, \eg, the viewing angles of the cameras, the type of the motion, the depth and spatial distributions of the 3D points, \etc
In general, choices of $\delta$ between $0.05$ and $0.15$ resulted in  the best results on the tested datasets. 
For the \sftld, and \sftlid solvers a bit smaller shifts are preferable, since these solvers are already shifting the mean-point correspondence by a prediction returned by the network. On the tested datasets a good trade-off between the precision and the running time of RANSAC is achieved using $\delta = 0.15$ for the \sftmd solver, and $\delta = 0.1$ for the \sftld and \sftlid solvers.

%However, based on our experiments, 
%\sftmd, \sftld, and \sftlid solvers seem to have reasonably good performance for various choices of $\delta$, while low values of $\delta$ seem to perform best for all three.

The results for the partially calibrated \sstp problem on the KITTI dataset~\cite{geiger2013vision} are reported in Table~\ref{tab:kitti_delta}. Similar to the calibrated case, similar performance is observed for various choices of $\delta$. However $\delta=0.1$ seems to be the best choice for all three solvers, namely \sstmd, \sstld, and \sstlid.
%Note that in this dataset already pure \sstm solver without any shift was  

Designing a strategy for selecting a suitable $\delta$, \eg, based on the shape of the triangle used to compute the virtual correspondence or based on results obtained using different $\delta$-shifts in early iterations of RANSAC, or from a few images from the given sequence of images (if we \eg have to process a sequence of images that undergo a similar motion) would be an interesting future work.

\begin{table*}[]
    \centering
    \setlength{\tabcolsep}{4.8pt}
    \resizebox{1.0\linewidth}{!}{\begin{tabular}{r | c c c c c c | c c c c c c}
        \toprule
        & \multicolumn{6}{c |}{Sacre Coeur (5000 image triplets)}  & \multicolumn{6}{c}{St.\ Peters' Square (5000 image triplets)} \\
        \midrule
        Estimator & AVG ($^\circ$) $\downarrow$ & MED ($^\circ$) $\downarrow$ & AUC@5$^\circ$ $\uparrow$ & @10$^\circ$ & @20$^\circ$ & Time (s) $\downarrow$ & AVG ($^\circ$) $\downarrow$ & MED ($^\circ$) $\downarrow$ & AUC@5$^\circ$ $\uparrow$ & @10$^\circ$ & @20$^\circ$ & Time (s) $\downarrow$\\
        \midrule
        4p3v(M $\pm$ 0.05) & 24.35 & \phantom{1}\bf{2.39} & \bf{44.83} & \bf{52.43} & \bf{59.07} & \phantom{1}3.14 & 26.28 & \phantom{1}\underline{9.01} & \bf{20.34} & \bf{32.95} & \bf{46.52} & \phantom{1}2.81 \\
        4p3v(M $\pm$ 0.1) & 24.42 &\phantom{1}2.63 & 44.16 & 51.85 & 58.53 & \phantom{1}\underline{3.11} & \underline{26.16} & \phantom{1}9.07 & 19.51 & 32.44 & 46.00 & \phantom{1}3.63\\
        4p3v(M $\pm$ 0.15) & 24.49 & \phantom{1}\underline{2.42} & \underline{44.51} & \underline{52.27} & \underline{58.91} & \phantom{1}\bf{3.10} & \bf{25.96} & \phantom{1}\bf{8.92} & \underline{19.62} & \underline{32.53} & \underline{46.24} & \phantom{1}\underline{2.76} \\
        4p3v(M $\pm$ 0.2) & \underline{24.24} &\phantom{1}2.58 & 44.36 & 52.15 & 58.76 & \phantom{1}\underline{3.11} & 26.32 & \phantom{1}9.20 & 19.24 & 32.15 & 45.92 & \phantom{1}\bf{2.75} \\
        4p3v(M $\pm$ 0.25) & \bf{23.80} & \phantom{1}2.63 & 44.28 & 52.12 & 58.81 & \phantom{1}\underline{3.11} & 26.63 & \phantom{1}9.30 & 19.02 & 31.90 & 45.64 & \phantom{1}2.79 \\
        \midrule
        4p3v(L $\pm$ 0.05) & 24.37 & \phantom{1}\underline{2.43} & \bf{44.90} & \bf{52.49} & 58.94 & \phantom{1}\bf{3.76} & \underline{26.00} & \phantom{1}9.10 & \underline{20.00} & \underline{32.83} & \underline{46.53} & \phantom{1}4.74 \\
        4p3v(L $\pm$ 0.075) & \underline{24.32} & \phantom{1}2.52 & 44.63 & 52.36 & 58.87 & \phantom{1}5.08 & \bf{25.86} & \phantom{1}\bf{8.87} & \bf{20.25} & \bf{33.01} & \bf{46.72} & \phantom{1}\underline{4.66} \\
        4p3v(L $\pm$ 0.1) & 24.36 & \phantom{1}\bf{2.35} & \underline{44.89} & \underline{52.48} & \underline{59.00} & \phantom{1}\bf{3.76} & 26.12 & \phantom{1}\underline{9.06} & 19.77 & 32.52 & 46.06 & \phantom{1}\bf{3.39} \\
        4p3v(L $\pm$ 0.15) & \bf{24.09} & \phantom{1}2.64 & 44.22 & 52.24 & \bf{59.01} & \phantom{1}\underline{3.87} & 26.20 & \phantom{1}9.18 & 19.56 & 32.31 & 46.01 & \phantom{1}\bf{3.39} \\
        \midrule
        4p3v(L $\pm$ 0.05 init) & 24.21 & \phantom{1}\bf{2.42} & \bf{45.04} & \underline{52.64} & \bf{59.30} & \phantom{1}6.30 & \underline{26.09} & \phantom{1}9.07 & \underline{19.89} & 32.57 & 46.16 & \phantom{1}5.78 \\
        4p3v(L $\pm$ 0.075 init) & 24.36 & \phantom{1}2.54 & 44.44 & 52.15 & 58.67 & \phantom{1}6.36 & 26.23 & \phantom{1}\bf{8.81} & \bf{19.97} & \bf{33.03} & \bf{46.56} & \phantom{1}6.06 \\
        4p3v(L $\pm$ 0.1 init) & \bf{23.95} & \phantom{1}\underline{2.46} & \underline{44.83} & \bf{52.78} & \underline{59.29} & \phantom{1}\underline{5.10} & \underline{26.09} & \phantom{1}\underline{8.83} & 19.67 & \underline{32.68} & \underline{46.48} & \phantom{1}\bf{4.22} \\
        4p3v(L $\pm$ 0.15 init) & \underline{24.05} & \phantom{1}2.48 & 44.46 & 52.40 & 59.01 & \phantom{1}\bf{4.90} & \bf{25.80} & \phantom{1}8.96 & 19.48 & 32.43 & 46.00 & \phantom{1}\underline{4.53} \\
        \midrule
    \end{tabular}
    }
    \caption{$\delta$-based solvers: The average and median pose errors in degrees, and Area Under the Recall curve (AUC) thresholded at 5$^\circ$, 10$^\circ$ and 20$^\circ$ on a total of 10k image triplets from two scenes of the PhotoTourism dataset~\cite{IMC2020} for different values of $\delta$ used in the proposed solvers.}
    \label{tab:phototourism_real_Test_delta}
\end{table*}

\begin{table}[]
    \centering
    \setlength{\tabcolsep}{4.8pt}
    \resizebox{1.0\linewidth}{!}{\begin{tabular}{r | c c c c c c c}
        \toprule
        %& \multicolumn{7}{c}{KITTI (step size = 1)} \\
        %\midrule
        Estimator & AVG ($^\circ$) $\downarrow$ & MED ($^\circ$) $\downarrow$ & AUC@1$^\circ$ $\uparrow$ & @2.5$^\circ$ & @5$^\circ$ & @10$^\circ$ & Time (s) $\downarrow$\\
        \midrule
        4p3vf(M $\pm$ 0.05) & \phantom{1}1.65 & \phantom{1}\underline{0.98} & 15.73 & 51.36 & 72.05 & 85.31 & \phantom{1}\underline{0.09}\\
        4p3vf(M $\pm$ 0.075) & \phantom{1}\underline{1.61} & \phantom{1}\bf{0.97} & \underline{16.02} & \bf{51.59} & \bf{72.23} & \bf{85.42} & \phantom{1}\underline{0.09}\\
        4p3vf(M $\pm$ 0.1) & \phantom{1}\bf{1.60} & \phantom{1}\bf{0.97} & \bf{16.11} & \underline{51.52} & \underline{72.12} & \underline{85.40} & \phantom{1}\underline{0.09}\\
        4p3vf(M $\pm$ 0.15) & \phantom{1}1.66 & \phantom{1}0.99 & 15.74 & 51.36 & 72.05 & 85.27 & \phantom{1}\bf{0.07}\\
        4p3vf(M $\pm$ 0.2) & \phantom{1}1.63 & \phantom{1}0.99 & 15.63 & 51.30 & 71.97 & 85.27 & \phantom{1}0.11\\
        4p3vf(M $\pm$ 0.25) & \phantom{1}1.63 & \phantom{1}0.99 & 15.68 & 51.39 & \bf{72.12} & 85.36 & \phantom{1}\underline{0.09}\\
        \midrule
        4p3vf(L $\pm$ 0.05) & \phantom{1}\bf{1.66} & \phantom{1}\underline{1.02} & \underline{15.38} & 50.47 & 71.46 & 84.98 & \phantom{1}\underline{0.15} \\
        4p3vf(L $\pm$ 0.075) & \phantom{1}\bf{1.66} & \phantom{1}\bf{1.01} & 15.27 & 50.53 & \bf{71.58} & \bf{85.09} & \phantom{1}\underline{0.15} \\
        4p3vf(L $\pm$ 0.1) & \phantom{1}\underline{1.69} & \phantom{1}\bf{1.01} & \bf{15.44} & \bf{50.68} & \underline{71.55} & \underline{85.07} & \phantom{1}\bf{0.10}\\
        4p3vf(L $\pm$ 0.15) & \phantom{1}1.72 & \phantom{1}\bf{1.01} & 15.31 & \underline{50.54} & 71.50 & 84.95 & \phantom{1}0.16 \\
        \midrule
        4p3vf(L $\pm$ 0.05 init) & \phantom{1}1.84 & \phantom{1}1.05 & 13.98 & 49.27 & 70.45 & 84.13 & \phantom{1}0.22 \\
        4p3vf(L $\pm$ 0.075 init) & \phantom{1}1.77 & \phantom{1}1.04 & 14.40 & 49.61 & 70.89 & 84.52 & \phantom{1}\underline{0.20} \\
        4p3vf(L $\pm$ 0.1 init) & \phantom{1}\underline{1.74} & \phantom{1}\bf{1.01} & \bf{15.30} & \bf{50.30} & \bf{71.34} & \bf{84.84} & \phantom{1}\bf{0.18}\\
        4p3vf(L $\pm$ 0.15 init) & \phantom{1}\bf{1.73} & \phantom{1}\underline{1.03} & \underline{14.74} & \underline{50.10} & \underline{71.23} & \underline{84.74} & \phantom{1}0.22 \\
        \midrule
    \end{tabular}
    }
    \caption{$\delta$-based solvers: Results on KITTI dataset~\cite{geiger2013vision} for partially calibrated cameras and different values of $\delta$ used in the proposed solvers. The reported statistics are the same as in Tab.~\ref{tab:phototourism_real_Test_delta}. }
    \label{tab:kitti_delta}
\end{table}

\subsection{Essential matrix estimation using a virtual correspondence}
\label{sec:sm_5pt}

\begin{table}[]
    \centering
    \setlength{\tabcolsep}{4.8pt}
    \resizebox{1.0\columnwidth}{!}{\begin{tabular}{r | c c c c c c }
        \toprule
        \midrule
        & AVG ($^\circ$) $\downarrow$ & MED ($^\circ$) $\downarrow$ & AUC@5$^\circ$ $\uparrow$ & @10$^\circ$ & @20$^\circ$ & Time (s) $\downarrow$ \\
        \midrule
        %consecutive 5PC 
        5pt & 5.04 & 0.89 & 63.71 & 74.45 & 83.11 & 0.04 \\
        4p(M) & 5.53 & 0.93 & 61.48 & 72.19 & 81.30 & 0.03 \\
        4p(M $\pm \delta$) & 5.21 & 0.90 & 61.80 & 72.33 & 81.27 & 0.02 \\
        \midrule
        4p(O) & 4.40 & 0.81 & 65.30 & 75.88 & 84.43 & 0.03 \\
        \hline
    \end{tabular}}
    \caption{The average and median pose errors in degrees, and Area Under the recall Curve (AUC), thresholded at 5$^\circ$, 10$^\circ$, and 20$^\circ$, as well as the average run-time (in seconds) on 9,900 image pairs from two scenes of the PhotoTourism dataset~\cite{IMC2020}.}
    \label{tab:phototourism_real_Test_2v}
\end{table}


The proposed M-based solvers do not exploit information from all three views when generating a virtual correspondence. They use only information from three point correspondences in two views and are thus not specific to the 3-view scenario considered in the paper. 
This suggests that the proposed method for generating virtual correspondences 
 can be used to solve other minimal problems using sub-minimal samples inside RANSAC. % in some scenarios.

Here, we
show preliminary results for the well-known 5-point relative pose problem, where we sample only four point correspondences inside RANSAC and generate the $5^\text{th}$ correspondence (a) as a mean point of three corresponding points in both images (the \texttt{4p(M)} solver), (b) by considering two additional points in $\delta$-vicinity of the mean point in the second image, thus generating three different $5^\text{th}$ correspondences. The two additional points are selecting using the same strategy as used in the \sftmd solver presented in Sec.~\ref{sec:calibrated} of the main paper. We call this solver the \texttt{4p(M $\pm \delta$)} solver.
We do not consider the L-based solver since our network was trained on data from all three images. However, a similar network can trained only on four points in two views instead of four points in three views.

In our experiments, we tested the solvers
on the same two scenes (\ie, Sacre Coeur and St.\ Peter's Square) from the PhotoTourism~\cite{snavely2006photo} dataset that we used in the main paper.
The 4,950 image pairs used for each scene were selected by the Image Matching Challenge 2020~\cite{IMC2020}. 
The challenge also provides point correspondences together with the intrinsic calibration matrices and ground truth camera poses obtained from a COLMAP reconstruction~\cite{Schoenberger2016CVPR}.

We run the \texttt{5pt} solver~\cite{Nister-5pt-PAMI-2004}, and the proposed \texttt{4p(M)} and \texttt{4p(M$\pm \delta$)} solvers which, respectively, generate the fifth correspondence by taking the mean of the first three points, or via using the mean point and two points from its $\delta$-vicinity to compensate for noise in the ``mean-point" correspondence.
%the proposed transformer-based neural network. 
We also show the results of the oracle \texttt{4p(O)} solver with a 
% an oracle 
fifth correspondence that is selected ensuring that it is consistent with the ground truth epipolar geometry. 
All minimal solvers are used inside GC-RANSAC~\cite{barath2017graph}.

The results, over all 9,900 image pairs from both scenes, are reported in Table~\ref{tab:phototourism_real_Test_2v}. 
While the virtual $5^\text{th}$ correspondences generated in the proposed \texttt{4p(M)} and \texttt{4p(M$\pm \delta$)} solvers can be noisy, both solvers show only slightly worse results than the %minimal 
%well-known 
\texttt{5pt} solver, which uses a more accurate $5^\text{th}$ correspondence,  
inside GC-RANSAC. Moreover, even though the running time of the \texttt{4p(M$\pm \delta$)} solver is approximately $3\times$ slower than the running time of the \texttt{5pt} and the \texttt{4p(M)} solvers, the running time of the whole GC-RANSAC is almost $2 \times$ faster, showing that the \texttt{4p(M$\pm \delta$)} solver is able to find a good solution in earlier iterations.
This shows the potential for applying such sub-minimal solvers in scenarios with high outlier ratios and noisy data, where sampling one less point may lead to faster run-times and where the higher noise of the $5^\text{th}$ correspondence will only have a limited impact.

The results of the oracle \texttt{4p(O)} solver shows that there is still some space for improvement when predicting the $5^\text{th}$ virtual correspondence, \eg, using a learning based method. However, in this case, the situation is more ``challenging'' than for the 4p3v problem. For the two-view problem, four points define an infinite number of camera poses that can observe these points, \ie, we have an under-constrained problem. This means that while the proposed M-based strategy and the proposed network can return quite good correspondences for some image pairs and scenes, there is no guarantee that it will not result in larger errors for some other image pairs. % they can 

\section{Future Work}
\label{sec:sm_future_work}

Our work opens up several directions of future research. Here we list some of them: 
\begin{itemize}
    \item First of all, a detailed analysis on the noise that the mean point correspondences introduce, to provide theoretical results and bounds through uncertainty propagation analysis, would be important to investigate. 
    \item Designing and training more complex and efficient network architectures for both the \sftl and \sstl solvers can be a path to achieving better performance for the \texttt{4p3v} and \texttt{4p3vf} problems. For \sstl it would be reasonable to train a separate model because the \ssm relative pose problem has many more degenerate configurations than the \sfm relative pose problem.
    \item On the implementation side, one could speed up the $\delta$-based solver by running the multiple \sfm / \ssm solvers in parallel or generating $\delta$-correspondences only if the mean-point correspondence returns a reasonable number of inliers inside RANSAC. This would significantly reduce the run-times of those solvers. One can speed up the learning-based solvers by running the inference on a GPU instead of a CPU.
   \item Designing and testing different sampling techniques inside RANSAC, so that the proposed M-based and L-based solvers would benefit on average from the spatial distribution of points in these samples (\eg, the size and shape of a triangle from which the mean point is computed) might improve both performance and run-time.
    \item Applying the proposed method based on virtual correspondences to new camera geometry problems, \eg, the complex rolling shutter relative pose problem for which minimal solvers do not exist but non-minimal solvers using a non-minimal number of points might be possible to implement reasonably efficiently.
\end{itemize}

\end{document}

