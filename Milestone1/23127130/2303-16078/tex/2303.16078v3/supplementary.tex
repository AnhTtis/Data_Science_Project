% \title{Relative pose of three calibrated and partially calibrated cameras from four points using virtual correspondences \\ - \\ Supplementary Material}

\section*{Supplementary Material}

\author{}

% \maketitle

%%%%%%%%% BODY TEXT
This supplementary material provides additional details and experimental results promised in the main paper: 
Sec.~\ref{sec:method} discusses M-solvers, provides the proof on the bound of the epipolar error mentioned in the main paper, and experiments supporting the choice of mean point correspondences (see Sec.~3.1 in the main paper). 
Sec.~\ref{sec:network} provides details on L-based solvers, namely information about the used network architecture and our training process, as well as about the ${\delta}$-variants of the L-based solvers, 
%\sftld, \sstld, and 
including \sstlidR, used in the paper (see Sec.~3.1 and Tab.~3 of the main paper). 
Sec.~\ref{sec:experiments_sm} provides details on the experiments mentioned in the main paper, namely experiments with different oracle solvers (see Sec.~4 of the main paper), namely ablations for the choice of the number of refinement iterations (Sec.~3.1 of the main paper) and the choice of $\delta$ (see Sec.~4 in the main paper), experiments with an additional evaluation measure (see Sec.~4 of the main paper), noise experiments (see Sec.~4 of the main paper), results with GC-RANSAC (see Sec.~4 of the main paper), and detailed plots over multiple scenes (see Sec.~4 of the main paper).  
% In addition, we discuss 
% 
 % more details on the architecture, training data, and training of the network used in the \sftl/\sstl solvers presented in Sec.~3.1 and ~3.2 of the main paper (Sec.~\ref{sec:network}), a discussion of the proposed method for solving camera geometry problems using virtual correspondences (Sec.~\ref{sec:method}), more details and additional experiments presented in Sec.~4 of the main paper (Sec.~\ref{sec:experiments}), and 
 % potential directions for future work in Sec.~\ref{sec:future_work}. 

% \section{Solving camera geometry problems using virtual correspondences }
\section{Using Mean Point Correspondendes}
\label{sec:method}
\PAR{Proof of the bounds on the epipolar error.} While the mean point correspondence used in the M-based solvers can provide a good approximation of a correct correspondence, such a correspondence can be noisy. %\footnote{
Note that all state-of-the-art \sftp solvers (including 
\sfhc~\cite{Hruby_cvpr2022} and \sfep~\cite{DBLP:journals/ijcv/NisterS06}) rely on certain approximations without establishing theoretical proofs to quantify their accuracy. 
In contrast, 
% On the other hand, 
the error of our virtual correspondence is bounded: %.}. 
% However, a
As mentioned in the main paper, it can be proven that the error of the virtual correspondence $\V m^1 \leftrightarrow\V m^2$ is bounded by the maximum distance of the mean point $\V m^2$ from the vertices of the triangle $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$.
Here we provide a simple proof.

\begin{figure}[t]
     \centering
     \includegraphics[width=1\columnwidth]{figures/4p3v_illustration_3-crop.pdf}
          \caption{Illustration of the considered situation.}
     \label{fig:illustration}
 \end{figure}

\begin{lemma}
Let us assume two cameras with camera centers $\V C^1$ and $\V C^2$ that observe 3D points $X_i, X_j,$ and $X_k$ (see Figure~\ref{fig:illustration} for an illustration). Let $\left\{\V x_i^1,\V x_j^1,\V x_k^1\right\}$ and $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$ be the projections of these 3D points in camera 1 and camera 2, respectively. Let $\V m^1$ be the mean point of the points $\left\{\V x_i^1,\V x_j^1,\V x_k^1\right\}$ and let $\V E$  be the essential matrix between these two cameras, \ie, a matrix that satisfies $\V x_l^{2^\top} \V E \V x_l^1 = 0,\; l \in \left\{i,j,k\right\}$. Then the epipolar line $\V E \V m^1$ passes through the triangle $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$.
\label{lemma:inter}
\end{lemma}

\begin{proof}
The camera center $\V C^1$ and the 3D points $\V X_i, \V X_j,$ and $\V X_k$ form a tetrahedron $T^1$ (see Figure~\ref{fig:illustration}). The projections $\left\{\V x_i^1,\V x_j^1,\V x_k^1\right\}$ in the first camera lie at the edges of this tetrahedron $T^1$.
%(see Figure~\ref{fig:illustration}). 
The ray from the camera center $\V C^1$ through the mean point $\V m^1$ thus lies inside the tetrahedron $T^1$ and intersects the plane defined by 3D points $\V X_i, \V X_j,$ and $\V X_k$ in a point $\V M$ that lies inside the triangle defined by $\left\{\V X_i,\V X_j,\V X_k\right\}$. 

The camera center $\V C^2$ and the 3D points $\V X_i, \V X_j,$ and $\V X_k$ form a tetrahedron $T^2$.
Again, the projections $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$ 
%in the second camera 
lie at the edges of the tetrahedron $T^2$.
The ray passing through the camera center $\V C^2$ and the 3D point $\V M$ lies inside the tetrahedron $T^2$ and thus intersects the image plane of the second camera at a point that lies inside the triangle defined by the points $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$. 
By construction, the projection of $\V M$ into the second camera lies on the epipolar line $\V E \V m^1$. 
Therefore, the epipolar line $\V E \V m^1$ which is a line connecting this point and the epipole $\V e^2$, passes through the triangle $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$. 
\end{proof}

It follows from Lemma~\ref{lemma:inter} that since the epipolar line $\V E \V m^1$ passes through the triangle $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$, the maximum distance of the mean point $\V m^2$ to the epipolar line $\V E \V m^1$ is equal to the maximum distance of $\V m^2$ to the vertices of the triangle.

\begin{figure*}[t!]
    \centering
	%\includegraphics[width=0.593\columnwidth]{assets/tests_distance_5.pdf}\hfill
	%\includegraphics[width=0.593\columnwidth]{assets/tests_distance_10.pdf}\hfill
	%\includegraphics[width=0.593\columnwidth]{assets/tests_distance_20.pdf}\phantom{xxx}\\[0.3cm] 
    \includegraphics[width=0.24\textwidth]{figures/bary_4p/st_peters_bary_sed.png}
    \includegraphics[width=0.24\textwidth]{figures/bary_4p/st_peters_bary_rotation.png}
    \includegraphics[width=0.24\textwidth]{figures/bary_4p/st_peters_bary_translation.png}
    \includegraphics[width=0.24\textwidth]{figures/bary_4p/st_peters_bary_inlier.png}

    \hfill
    
    \includegraphics[width=0.24\textwidth]{figures/bary_4p/temple_nara_japan_bary_sed.png}
    \includegraphics[width=0.24\textwidth]{figures/bary_4p/temple_nara_japan_bary_rotation.png}
    \includegraphics[width=0.24\textwidth]{figures/bary_4p/temple_nara_japan_bary_translation.png}
    \includegraphics[width=0.24\textwidth]{figures/bary_4p/temple_nara_japan_bary_inlier.png}


\caption{Left to right: Distribution of the average symmetric epipolar error (top: 0.3337, 0.3327) (bottom: 0.3355, 0.3290); rotation error (top: 0.3373, 0.3349) (bottom: 0.3261, 0.3496); translation error (top: 0.3336, 0.3417) (bottom: 0.3213, 0.3515); and percentage of inliers gathered (top: 0.3266, 0.3434) (bottom: 0.3198, 0.3552), as a function of the barycentric coordinates of the triangle in the second image \wrt the mean point of the corresponding triangle in the first image %, for (a) Shop Facade from the Cambridge Landmark dataset and (b,c,d) 
on 485k four-tuples of correspondences from scene (top) St. Peter's Square, (bottom) Temple Nara Japan from the PhotoTourism dataset~\cite{IMC2020}. For each metric, we fit a 2D Gaussian distribution and report the mean of the distribution in brackets.}
\label{fig:mean_stats_pt}
\end{figure*}

\PAR{Experiments validating the use of mean point correspondences.} 
The %final 
error of the relative poses estimated with virtual correspondences %, however, 
depends on many aspects, \eg, the baseline and the view angles of the cameras \wrt the three points used to compute the mean points, the depth of these points, the size and shape of the triangles defined by the three points, the type of motion of the cameras, the level of noise in the correspondences, \etc 
Isolating the impact of the individual aspects, \eg, through experiments on synthetic data, is highly non-trivial (\eg, how to generate realistic synthetic scenarios that allow conclusions to generalize to real-world scenarios) and analysing the co-dependencies between different aspects on the overall performance seems to need a paper on its own. 

In the main paper, we thus presented results on real-world scenes, without trying to isolate individual factors (see Figure~2 and Table~1 in the main paper). 
Fig.~2 in the main paper showed results obtained by establishing correspondences between the mean of the triangle in one image and various points in the triangle in the second image. 
We expressed points in the second triangle via their barycentric coordinates and uniformly sample $19 \times 19$ barycentric coordinates $(a,b)\in [0,1]^2$, such that $a+b \leq 1$ (ensuring 
points inside the triangle). 
The %third barycentric 
3rd coordinate is given as $c = 1-a-b$. 
For each correspondence, we measured the symmetric epipolar error \wrt the ground truth %relative 
pose, %the 
translation and rotation errors, and the percentage of inliers consistent with the pose obtained with the \texttt{5pt} solver applied on the virtual and the four real correspondences (denoted as the \texttt{4pt(M)} solver). 
Fig.~2 in the main paper showed results for the Sacre Coeur scene from the PhotoTourism dataset~\cite{IMC2020}. 
Here,  Fig.~\ref{fig:mean_stats_pt} shows the same statistics for two more scenes from the PhotoTourism dataset, St. Peter's Square (top row) and Temple Nara Japan (bottom row). 
As with Fig.~2 in the main paper, for each metric, we fit a 2D Gaussian distribution and report the mean value (in barycentric coordinates) as numbers in brackets in the caption of the figure.
As can be seen, the same conclusion can be drawn from Fig.~\ref{fig:mean_stats_pt} as from Fig.~2 in the main paper: 
The optima of the studied metrics are reached very close to the mean point of the triangles, which has barycentric coordinates $(0.\bar{3}, 0.\bar{3})$. 

  \begin{figure}[t!]
    \centering
	%\includegraphics[width=0.593\columnwidth]{assets/tests_distance_5.pdf}\hfill
	%\includegraphics[width=0.593\columnwidth]{assets/tests_distance_10.pdf}\hfill
	%\includegraphics[width=0.593\columnwidth]{assets/tests_distance_20.pdf}\phantom{xxx}\\[0.3cm] 
	\includegraphics[width=0.45\columnwidth]{figures/synth_symmetric_error-1.pdf}
	\includegraphics[width=0.45\columnwidth]{figures/real_symmetric_error-1.pdf}
\caption{Distribution of the average symmetric epipolar error as a function of barycentric coordinates of the point in the second image \wrt the mean point of three points in the first image, for (left) synthetic data and (right) real data in the form of the Shop Facade scene from the Cambridge Landmarks dataset~\cite{kendall2015cambridge}}
\label{fig:mean_tests_sm}
\end{figure}

In addition to the experiments on scenes from the PhotoTourism dataset, Fig.~\ref{fig:mean_tests_sm} shows results for the symmetric epipolar error on  (left) synthetic data and (right) the Shop Facade scene from the Cambridge Landmarks dataset~\cite{kendall2015cambridge}. 
For the synthetic experiment, we generated 10k scenes with known ground truth parameters. 
In each scene, the three 3D points were randomly distributed within a cube of size $10 \times 10 \times 10$. Each 3D point was projected into two cameras.
%with realistic focal lengths. 
The orientations and positions of the cameras were selected at random such that they looked towards the origin from a random distance, varying from $20$ to $50$, from the scene. 

Fig.~\ref{fig:mean_tests_sm} shows the average symmetric epipolar error as a function of the barycentric coordinates of the point in the second image. 
The 2D Gaussian distribution fitted to the results on synthetic scenes has mean $\mu = (0.3319, 0.3308)$. 
The distribution of errors for the Shop Facade scene is very similar to the synthetic data with the minimum at $(0.333,0.333)$. 
In both cases, the means of the fitted Gaussian distributions are very close to the mean of the triangle (which has 
% The point with 
barycentric coordinates $(0.\bar{3}, 0.\bar{3})$). % corresponds to the mean point of the 
All the above-mentioned experiments clearly validates our approach of using mean point correspondences.

\begin{table}[t!]
    \centering
    \setlength{\tabcolsep}{4.8pt}
    \resizebox{1.0\columnwidth}{!}{\begin{tabular}{r | c c c c c c }
        \toprule
        & AVG ($^\circ$) $\downarrow$ & MED ($^\circ$) $\downarrow$ & AUC@5$^\circ$ $\uparrow$ & @10$^\circ$ & @20$^\circ$ & Time (s) $\downarrow$ \\
        \midrule
        %consecutive 5PC 
        \texttt{5pt} & 5.04 & 0.89 & 63.71 & 74.45 & 83.11 & 0.04 \\
        \texttt{4pt(M)} & 5.53 & 0.93 & 61.48 & 72.19 & 81.30 & 0.03 \\
        \texttt{4pt(M}$\pm \delta$\texttt{)} & 5.21 & 0.90 & 61.80 & 72.33 & 81.27 & 0.02 \\
 %       4p(L) & 5.49 & 0.89 & 61.84 & 72.23 & 81.23 & 3.24 \\
        \midrule
        \texttt{4pt(O)} & 4.40 & 0.81 & 65.30 & 75.88 & 84.43 & 0.03 \\
        \hline
    \end{tabular}}
    \caption{The average and median pose errors in degrees, and Area Under the recall Curve (AUC), thresholded at 5$^\circ$, 10$^\circ$, and 20$^\circ$, as well as the average run-time (in seconds) on 9,900 image pairs from two scenes, Sacre Coeur and St. Peter's Square, from the PhotoTourism dataset~\cite{IMC2020}.}
    \label{tab:phototourism_real_Test}
\end{table}

Tab.~2 in the main paper compares the relative pose accuracy achieved by the \texttt{4pt(M)} solver with the accuracy of the classical \texttt{5pt} relative pose solver. 
As can be seen, the \texttt{4pt(M)} solver is not as accurate as the \texttt{5tp} solver as the 5th correspondence used by the \texttt{4pt(M)} solver (the mean point correspondence) is significantly more noisy than the one used by the \texttt{5pt} solver. 
Tab.~\ref{tab:phototourism_real_Test} shows pose accuracy results obtained by running the \texttt{4pt(M)} (and its variant \texttt{4pt(M$\pm\delta$)}) and the \texttt{5pt} solvers inside GC-RANSAC on a total of 9,900 image pairs from the Sacre Coeur and St. Peter's Square datasets. 
While the individual poses returned by both 4-point solvers are less accurate, RANSAC (and in particular local optimization inside RANSAC) can compensate for this, leading to comparable performance. 
This experiments not only validates the mean point-strategy, but also shows that virtual correspondences can be used to solver minimal problems from sub-minimal samples. 

Tab.~\ref{tab:phototourism_real_Test} also shows results for the oracle variant %The results of the oracle 
\texttt{4pt(O)} of our solvers. %
%solver shows 
As can be seen, %that 
there is still some space for improvement when predicting the 5{th} virtual correspondence, \eg, using a learning-based method. While this can also bring an improvement for two-views, such a learning-based method has a higher potential for improvement for the 4p3v problem, where the information from four points in three views fixes the pose of the input cameras that observe these points.

The method based on virtual correspondences can be theoretically applied to any camera geometry problem, however, we see larger promise in relative pose problems, where it is sufficient to find one 2D point that is sufficiently close to the epipolar line. For %the 
absolute pose solvers, a %good 
virtual correspondence will need to be close to a 2D point instead of an epipolar line.
 

\section{L-based Solvers}
\label{sec:network}

\begin{figure*}[!h]
\centering  
    \includegraphics[trim=0 350 0 100, clip ,width=1\textwidth]{figures/network_arch.png}
    \caption{The architecture of our network. With LR we denote the Leaky RELU activation function \cite{leakyRELU}. The input consists of four 6-vectors, one for each correspondence. Each 6-vector contains the $x$ and $y$ coordinates of a correspondence in three views. First, the input passes through a shared MLP block, so that each 6-vector is processed independently. Then, the output feature vectors of dimension 32 are aggregated using a channel-wise max pooling function, and the result of it is concatenated at the end of each feature vector. Then, there is one more block of shared MLPs, of which the results are aggregated again by a max pooling, to get a single 64-vector. We reduce the dimensions of this vector by passing it through an MLP block, of which the last layer has 2 nodes and a $\tanh$ activation function.}
    \label{fig:net}
\end{figure*}


\PAR{Network architecture and training details.} 
As described in Sec.~3.1 of the main paper, our \sftl/\sstl solvers rely on a neural network to predict a virtual correspondence. %, which shows the potential to deal with noise in the
The following provides details on the network architecture and the training process. 

We use a lightweight architecture with a backbone of shared MLP layers, similar to \cite{cavalli2022nefsac}, so that each triplet of correspondences is processed independently. 
The input to our network is a $4 \times 6$ matrix of 4 point correspondences 
%containing the $x$ and $y$ coordinates in three views. 
where the $i^{th}$ row contains the $x$ and $y$ coordinates of the $i^{th}$ point in three views, \ie, the $i^{th}$ correspondence. 
In estimating the epipolar geometry, the order of the point correspondences does not matter. Thus, our network is designed to be permutation invariant on that input axis.
The input is normalized as follows: 
% Given an initialization for the virtual correspondence, the mean point of the first three points (later we discuss about different initializations), w
We apply a rotation matrix to the points in each view independently, so that the mean point of the first three points is sent to $(0, 0)$, and the fourth point is sent to $(0, y)$. 
Let $\V m^1, \V m^2$, and $\V m^3$ be the mean points of the three corresponding points in three views.
We aim to predict the corresponding point of $\V m^1$ in the second view. Let us denote this predicted point by $\V {\tilde{m}}^2$, \ie, our $5^{th}$ virtual correspondence in the first two views will be $\V m^1 \leftrightarrow \V{\tilde{m}}^2$. 
% 
As suggested by the \sftm solver, $\V{\tilde{m}}^2$ should be close to $\V m^2$. Thus, we use the mean point $\V{m}^2$ as the initialization of our network and predict a shift from $\V m^2$. 

We use a simple MLP-based backbone, similar to \cite{cavalli2022nefsac,Qi_2017_CVPR}. More precisely, our input consists of four 6-vectors, of the $x,$ and $y$ coordinates of four point correspondences in three views. The first part of the model is a shared MLP 3-block of dimensions 6, 32, and 32, exporting a 32-dimensional feature for every correspondence. 
Then we apply a channel-wise max pooling aggregation, which is then concatenated at the end of each 32-dimensional feature. 
This results in having an 64-dimensional vector for each correspondence, which are passed into another shared MLP 3-block of 64, 64, and 64 dimensions. 
We aggregate those vectors via a max pooling function to get a 64-dimensional vector encoding, which eventually passes through an MLP 3-block to reduce the dimension gradually from 64, to 32, and finally to 2, which will be the prediction of the $x$ and $y$ coordinates in the second camera. 
In all MLPs, in the first 2 layers of the blocks, we use a Leaky RELU activation function \cite{leakyRELU} with slope 0.01. 
As for the last layer of the MLPs, in the first two blocks, we use a RELU activation function, while in the last MLP we use a $\tanh$ activation, since we want the output to be in the range $(-1, 1)$. For a visualization of the architecture, see Figure~\ref{fig:net}.

We used a simple network architecture to show the promise of \sftl/\sstl solvers, namely that learning can produce more accurate point correspondences. 
The experiments shown in the paper verify this behavior. 
We believe that 
more advanced network architectures (\eg, using a set transformer architecture \cite{set_transformer}) have the potential to improve the results even more and reduce the gap between the \sftp/\sstp solvers and the oracle \sfto/\ssto solvers.

Our loss function %of our network 
is the \textit{Sampson error} $\mathcal{L}_S$ of the prediction to the epipolar line of $\V m^1$ in the $2^\text{nd}$ view: 
%$$
%\mathcal{L}_S = \frac{{x^\prime_i}^\top \mathbf{E} x_i}{\sqrt{\lVert \mathbf{E}x_i \rVert^2 + \lVert \mathbf{E}^\top x_i^\prime \rVert^2}}
%$$
\begin{equation}
 \mathcal{L}_S = \frac{{\V{\tilde{m}}^{2^\top}} \mathbf{E} \V{m}^1}{\sqrt{\lVert \mathbf{E}\V{m}^1 \rVert^2 + \lVert \mathbf{E}^\top \V{\tilde{m}}^2 \rVert^2}} \enspace .  
\end{equation}
For both training and validation, we use synthetic data. Our synthetic dataset contains 1M input instances. 70\% of the dataset is used for training while the remaining 30\% is used for validating the performance of the network. We generate 10K 3D points inside a 10$\times$10$\times$10 cube, and to generate each instance, we pick 4 random points and project them to 3 cameras with random rotations and translations that view the cube from a random distance between 20 to 50 units. 

The network is implemented in PyTorch \cite{paszke2017automatic}, and we use the Adam optimizer \cite{DBLP:journals/corr/KingmaB14} for the training. We train it in batches of 1024 input instances, with a fixed learning rate of 1e-5. In our experiments, the network converges in about 30 epochs.

% \PAR{$\delta$-variants of L-based solvers.} 

\PAR{4p3v(L$\pm \delta$), 4p3vf(L$\pm \delta$), and 4p3vf(L$\pm \delta$init) solvers.} 
Similar to the \sftmd solver, we try to compensate for potential noise in the prediction $\tilde{\V m}^2$ returned by the network by running the \sfc solver for the first two views three times for three different virtual correspondences. 
We test two variants: 
(1) In the \sftld and \sstld solvers, we add a shift $\pm \delta$ to the predicted point $\tilde{\V{m}}^2$, similar to the \sftmd solver. In this way, we generate two additional virtual correspondences. 
(2) In the \sftlid solver, we add a shift $\delta$ to the initialization $\V m^2$ of the network. Thus, we run the network three times with three initializations, namely $\V m^2$, $\V m^2_{+\delta}$, and $\V m^2_{-\delta}$. Each initialization affects the normalization of the points in the 2nd view, in the sense that a different point (the initialization) is sent to (0, 0), leading to a different input, thus predicting different virtual correspondence.
% and $\V m^2_{-\delta}$, $\V m^2$ and two points $\V m^2_{\pm\delta}$, to predict three different virtual correspondences. 



\section{Experiments}
\label{sec:experiments_sm}
 This section provides more details on the  experiments presented in Sec.~4 of the main paper.  
 %\TODO{will we have this?} 
 % as well as results showing the potential of the proposed method based on virtual correspondences to solve minimal problems using sub-minimal samples of points. 
More precisely, Sec.~\ref{sec:exp:oracles} provides details and experiments 
on the oracle solvers discussed in the main paper. 
Sec.~\ref{sec:exp:ablations} provides ablation studies for the number of iterations of the refinement strategy (\texttt{+R}) and for the choice of $\delta$. 
Sec.~\ref{sec:exp:noise} provides details on the noise experiments summarized in the main paper. 
Sec.~\ref{sec:exp:measure} provides experiments with an evaluation measure taking the relative pose error between the 2nd and 3rd camera in a triplet into account. 
Sec.~\ref{sec:exp:gc_ransac} presents results on the PhotoTourism dataset obtained with GC-RANSAC. 
Sec.~\ref{sec:exp:outliers} presents additional experiments with adding outliers to image triplets (similar to the experiments presented in Fig.~4 in the main paper). 
Sec.~\ref{sec:exp:details} presents the additional details on some experiments promised in Sec.~4 of the main paper. 
Finally, Sec.~\ref{sec:exp:timing} measures the run-times of the different solvers considered in this work. 

\subsection{Ablation studies}
\label{sec:exp:ablations}

\begin{table}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ l | l | c c | c c c}
\toprule
Estimator & \multicolumn{1}{|c|}{$\delta$} & AVG $(^\circ)$ $\downarrow$ & MED $(^\circ)$ $\downarrow$ & AUC@5 $\uparrow$ & @10 $\uparrow$ & @20 $\uparrow$ \\
\midrule

\multirow{13}{*}{\sftmd}
 & 0.2 & 6.33 & 3.89 & 36.88 & 57.46 & 74.74 \\
 & 0.1 & 6.28 & 3.82 & 37.50 & 57.88 & 74.95 \\
 & 0.09 & 6.29 & 3.77 & 37.98 & 58.27 & 75.18 \\
 & 0.08 & 6.31 & 3.72 & 38.32 & 58.53 & 75.36 \\
 & 0.07 & 6.21 & \underline{3.64} & 39.29 & 59.16 & 75.69 \\
 & 0.06 & 6.18 & 3.68 & 39.16 & 59.18 & 75.76 \\
 & 0.05 & 6.07 & 3.65 & \underline{39.47} & \underline{59.37} & \underline{75.93} \\
 & 0.04 & \textbf{5.99} & \textbf{3.62} & \textbf{39.57} & \textbf{59.52} & \textbf{76.01} \\
 & 0.03 & \underline{6.04} & 3.64 & 39.44 & 59.36 & 75.92 \\
 & 0.02 & 6.18 & 3.68 & 38.74 & 58.73 & 75.42 \\
 & 0.01 & 6.30 & 3.81 & 37.89 & 57.94 & 74.90 \\
 & 0.005 & 6.39 & 3.88 & 36.62 & 56.94 & 74.36 \\
 & 0.001 & 6.65 & 4.03 & 35.57 & 55.83 & 73.50 \\
\midrule
\multirow{13}{*}{\sftmdR}
 & 0.2 & 5.89 & 3.40 & 41.40 & 60.73 & 76.71 \\
 & 0.1 & 5.80 & 3.32 & 42.48 & 61.72 & 77.28 \\
 & 0.09 & 5.71 & 3.31 & 42.37 & 61.62 & 77.32 \\
 & 0.08 & 5.68 & 3.29 & 42.86 & 61.95 & 77.42 \\
 & 0.07 & 5.65 & 3.25 & 43.25 & 62.27 & \underline{77.68} \\
 & 0.06 & \textbf{5.57} & 3.28 & 43.12 & 62.07 & 77.56 \\
 & 0.05 & \underline{5.59} & 3.24 & 43.23 & 62.18 & 77.66 \\
 & 0.04 & 5.64 & \textbf{3.19} & \underline{43.60} & \underline{62.43} & 77.66 \\
 & 0.03 & 5.66 & 3.22 & \textbf{43.68} & \textbf{62.49} & \textbf{77.82} \\
 & 0.02 & 5.65 & \underline{3.21} & 43.24 & 62.11 & 77.56 \\
 & 0.01 & 5.79 & 3.29 & 42.54 & 61.75 & 77.18 \\
 & 0.005 & 5.90 & 3.38 & 42.03 & 61.24 & 76.91 \\
 & 0.001 & 6.09 & 3.46 & 40.80 & 59.99 & 75.98 \\
\midrule
\multirow{13}{*}{\sftmdRC}
 & 0.2 & 6.02 & 3.60 & 39.44 & 59.19 & 75.79 \\
 & 0.1 & 6.00 & 3.50 & 40.69 & 60.27 & 76.36 \\
 & 0.09 & 5.92 & 3.43 & 40.83 & 60.40 & 76.51 \\
 & 0.08 & 5.91 & 3.44 & 41.10 & 60.61 & 76.60 \\
 & 0.07 & 5.93 & 3.39 & 41.58 & 60.80 & 76.72 \\
 & 0.06 & 5.79 & 3.38 & 41.85 & 61.01 & 76.91 \\
 & 0.05 & \underline{5.77} & \textbf{3.32} & 42.08 & 61.07 & \underline{76.95} \\
 & 0.04 & 5.79 & 3.35 & 42.00 & \underline{61.13} & 76.94 \\
 & 0.03 & \textbf{5.73} & \underline{3.33} & \textbf{42.46} & \textbf{61.50} & \textbf{77.22} \\
 & 0.02 & 5.81 & 3.33 & \underline{42.12} & 61.01 & 76.80 \\
 & 0.01 & 5.86 & 3.41 & 41.12 & 60.56 & 76.56 \\
 & 0.005 & 5.96 & 3.51 & 40.64 & 60.12 & 76.19 \\
 & 0.001 & 6.20 & 3.53 & 39.95 & 59.08 & 75.52 \\
\midrule
    \end{tabular}}
    \caption{Evaluation of the effects of the scale of the $\delta$ shift on the  \textit{St. Peter's Square} scene from Phototourism~\cite{IMC2020}.}
    \label{tab:delta_ablation}
\end{table}

\PAR{Validation of $\delta$.} We tested our $\delta$-based solvers for different values of $\delta$ and measured their performance. In general, there is no common value of the $\delta$ shift that leads to the best results on all datasets. This is expected since the precision of the mean-point correspondence depends on many different factors, \eg, the viewing angles of the cameras, the type of the motion, the depth and spatial distributions of the 3D points, \etc 
We set the value for $\delta$ and the total number of refinement iterations by evaluating their effects on \textit{St. Peter's Square} scene from the PhotoTourism dataset~\cite{IMC2020} which we used for validation and did not include it in other results for PhotoTourism in the paper.  
Tab.~\ref{tab:delta_ablation} shows how the different settings of the scale of the $\delta$ shift affect the accuracy of the $\delta$-based solvers. 
Based on these experiments we use $\delta = 0.04$ as it provides the best results for \sftmd solver and is also close to the optimal value for its variants.
The choice of optimal $\delta$ parameter may be scene-dependent and could potentially be set by using learning-based approaches.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/poselib_graphs/st_peters_square_refinement_validation.pdf}
    \caption{Evaluation of the effects of the number of inner refinement (\texttt{+R}) iterations within \sftmRC solver on \textit{St. Peter's Square} scene from PhotoTourism~\cite{IMC2020}. Shown is the speed-accuracy evaluation with different number of Poselib RANSAC iterations.}
    \label{fig:r_validation}
\end{figure}

\begin{figure*}[t]
    \centering
	\begin{subfigure}[t]{0.49\textwidth}
	    \includegraphics[width=1.0\textwidth]{figures/plot_pose_error_1.pdf}%{figures/err_avg_R_final.eps}
		% \caption{}    	%\label{fig:ground_rot_error_ori}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
	    \includegraphics[width=1.0\textwidth]
{figures/plot_pose_error_uncalibrated.pdf}
     %{figures/plot_R12_error_1.pdf}
     % {figures/err_avg_t_final.eps}
		% \caption{}
    	\label{fig:ground_tra_error_ori}
	\end{subfigure}\\
%	\begin{subfigure}[t]{0.48\textwidth}
	    %\includegraphics[width=1.0\textwidth]
        %{figures/plot_t12_error_1.pdf}%{figures/err_avg_R12_final.eps}
		% \caption{}
%		\end{subfigure}
    \caption{ Noise experiment showing 
    the pose error measured as $\text{max}\left(0.5 (\M R_{err}^{12} + \M R_{err}^{13}), 0.5 (\V t_{err}^{12} + \V t_{err}^{13})\right)$, for the calibrated \sftp problem (left) and the partial calibrated \sstp problem (right) as a function of the noise scale in pixels. Here, $\M R_{ij}$ and $\M t_{ij}$ are the relative rotation and translation of the $i^{\text{th}}$ and $j^{\text{th}}$ views, respectively.}
    \label{fig:noise}
\end{figure*}

\PAR{Inner refinement validation.} We also perform validation of the total number of LM steps in the inner refinement (\texttt{+R}) shown in Fig.~\ref{fig:r_validation}. We chose the value of 2 for other experiments as it provides the best speed-accuracy trade-off across a range of RANSAC iterations. However, we note that other settings may have very similar performance. 

\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
	    \includegraphics[width=1.0\textwidth]{figures/plot_R12_error_1.pdf}
       %{figures/err_avg_R_final.eps}
		% \caption{}
%    	\label{fig:ground_rot_error_ori_uncalibrated}
	\end{subfigure}
 \begin{subfigure}[t]{0.48\textwidth}
	    \includegraphics[width=1.0\textwidth]{figures/plot_t12_error_1.pdf}
     %{figures/err_avg_R_final.eps}
		% \caption{}
    %	\label{fig:ground_rot_error_ori_uncalibrated}
	\end{subfigure}
		\begin{subfigure}[t]{0.48\textwidth}
	    \includegraphics[width=1.0\textwidth]{figures/plot_R12_error_uncalibrated.pdf}
     % {figures/err_avg_t_final.eps}
		% \caption{}
    	\label{fig:ground_tra_error_ori_uncalibrated}
	\end{subfigure}
	\begin{subfigure}[t]{0.48\textwidth}
	    \includegraphics[width=1.0\textwidth]
        {figures/plot_t12_error_uncalibrated.pdf}%{figures/err_avg_R12_final.eps}
		% \caption{}
		\end{subfigure}

  
    \caption{  Noise experiment showing $\M R_{err}^{12}$ 
    (left) and $\V t_{err}^{12}$ (right) as functions of the noise scale in pixels for the calibrated \sftp problem (top) and the partially calibrated \sstp problem (bottom). Here $\M R_{12}$ and $\M t_{12}$ are the relative rotation and translation between the first two views.}%of the $i^{\text{th}}$ and $j^{\text{th}}$ views, respectively.}
    \label{fig:noise_uncalibrated}
\end{figure*}

\begin{table}[h]
    \centering
    \resizebox{1.0\linewidth}{!}{
\begin{tabular}{ l | c c | c c c | c}
    \toprule
    \multicolumn{7}{c}{Phototourism~\cite{IMC2020}} \\
    \midrule
    Estimator & AVG $(^\circ)$ $\downarrow$ & MED $(^\circ)$ $\downarrow$ & AUC@5 $\uparrow$ & @10 $\uparrow$ & @20 $\uparrow$ & Runtime (ms) $\downarrow$\\

\midrule

\sfhc~\cite{Hruby_cvpr2022} & 11.41 & 3.89 & 37.90 & 53.76 & 68.08 & \phantom{1}76.45 \\
\midrule\sft & \phantom{1}9.88 & 3.41 & 41.36 & 57.36 & 71.13 & 105.50 \\
\midrule\sftm & 11.65 & 4.21 & 35.38 & 51.73 & 66.77 & \phantom{1}76.77 \\
\sftmR & 10.52 & 3.47 & 41.01 & 56.62 & 70.25 & \phantom{1}\underline{74.94} \\
\sftmRC & 10.79 & 3.58 & 40.09 & 55.73 & 69.56 & \phantom{1}\textbf{30.54} \\
\midrule\sftmd & 10.44 & 3.72 & 38.82 & 55.07 & 69.51 & 172.06 \\
\sftmdR & \phantom{1}\textbf{9.84} & \textbf{3.27} & \textbf{42.62} & \textbf{58.30} & \textbf{71.74} & 189.21 \\
\sftmdRC & 10.14 & 3.40 & 41.45 & 57.24 & 70.92 & \phantom{1}75.78 \\
\midrule\sftl & 11.56 & 4.21 & 35.32 & 51.77 & 66.84 & 376.31 \\
\sftlR & 10.48 & 3.47 & 41.02 & 56.60 & 70.25 & 297.88 \\
\sftlidR & \phantom{1}\underline{9.86} & \underline{3.28} & \underline{42.60} & \underline{58.26} & \underline{71.71} & 730.90 \\
\midrule\sfto & \phantom{1}9.64 & 3.29 & 42.60 & 58.54 & 72.09 & \phantom{1}58.22 \\
\sftoR & \phantom{1}9.50 & 3.13 & 44.01 & 59.67 & 72.81 & \phantom{1}86.30 \\
\sftoRC & \phantom{1}9.45 & 3.16 & 43.82 & 59.55 & 72.76 & \phantom{1}36.36 \\
\midrule

\multicolumn{7}{c}{Cambridge Landmarks~\cite{kendall2015cambridge}} \\    
\midrule
    Estimator & AVG $(^\circ)$ $\downarrow$ & MED $(^\circ)$ $\downarrow$ & AUC@5 $\uparrow$ & @10 $\uparrow$ & @20 $\uparrow$ & Runtime (ms) $\downarrow$\\
\midrule

\sfhc~\cite{Hruby_cvpr2022} & 15.12 & 5.51 & 24.50 & 43.42 & 61.10 & \phantom{1}64.49 \\
\midrule\sft & \textbf{13.38} & 5.17 & 25.94 & 45.38 & 63.16 & \phantom{1}48.53 \\
\midrule\sftm & 15.31 & 5.68 & 22.93 & 42.06 & 60.26 & \phantom{1}38.33 \\
\sftmR & 14.28 & 5.29 & 25.37 & 44.57 & 62.33 & \phantom{1}\underline{34.84} \\
\sftmRC & 14.60 & 5.39 & 24.89 & 43.97 & 61.71 & \phantom{1}\textbf{16.51} \\
\midrule\sftmd & 14.18 & 5.36 & 24.72 & 44.12 & 62.15 & \phantom{1}81.21 \\
\sftmdR & 13.67 & \underline{5.11} & \textbf{26.17} & \underline{45.56} & \underline{63.33} & \phantom{1}84.98 \\
\sftmdRC & 13.89 & 5.23 & 25.43 & 44.78 & 62.70 & \phantom{1}38.45 \\
\midrule\sftl & 15.24 & 5.72 & 22.98 & 42.15 & 60.35 & 232.13 \\
\sftlR & 14.23 & 5.27 & 25.33 & 44.49 & 62.30 & 177.06 \\
\sftlidR & \underline{13.63} & \textbf{5.10} & \underline{26.11} & \textbf{45.58} & \textbf{63.35} & 395.43 \\
\midrule\sfto & 13.85 & 5.22 & 25.61 & 45.19 & 63.11 & \phantom{1}30.43 \\
\sftoR & 13.51 & 5.07 & 26.60 & 46.05 & 63.76 & \phantom{1}34.68 \\
\sftoRC & 13.47 & 5.10 & 26.47 & 45.98 & 63.71 & \phantom{1}16.41 \\
\midrule

    \end{tabular}}
    \caption{Results for different solvers implemented in the PoseLib framework~\cite{PoseLib} on all scenes from the PhotoTourism~\cite{IMC2020} and 5 scenes from the Cambridge Landmarks~\cite{kendall2015cambridge} datasets with the alternative definition of pose error \eqref{eq:max_error}.
    We mark the \textbf{best} and \underline{second best} results (excluding oracle solvers). Reported runtimes are for the whole RANSAC.}
    \label{tab:max_error1_sm}
\end{table}

 \subsection{Noise experiments}
 \label{sec:exp:noise}
 %\TODO{Update - ZK}
%\PAR{Noise experiments.}
We tested the performance of our solvers and the state-of-the-art algorithms \wrt increasing image noise. 
We used the SfM model of the botanical garden scene (randomly selected from all scenes) from the ETH3D dataset~\cite{Schops_2017_CVPR} to obtain instances of 5/6 points in three views by identifying images in the scene that share 3D points. 
Perfect noise-free correspondences are generated by projecting the 3D points into the images. 
We then add increasing amounts of normally distributed noise to these correspondences. 
We generated more than 9k instances, but show only 1k results per plot to avoid clutter. 
Note that the \sfhc solver was trained on the ETH3D dataset while our %\sftl
L-based solvers were trained on purely synthetic data. For the noise experiments we also test the \texttt{joint 5PC} solver, which operates on samples of 5 point correspondences, by estimating the essential matrices $\M E_{12}, \M E_{13}$ independently, using the 5pt solver. 
Notice that due to estimating the two essential matrices independently of each other, the scales of both translations are indepent from each other. 
In contrast, the other solvers estimate the scale of the translation of the third camera relative to the scale of the translation between the first two cameras. 
% Our experimental setup is thus not biased towards our solvers. 
% Since the network used in our \sftl solver was trained on synthetic scenes, here we decided to use real dataset....\TODO{Torste finish}



The results for increasing noise in the image points are shown in Figs.~\ref{fig:noise} and~\ref{fig:noise_uncalibrated}. 
The results are represented by the boxplot function which shows the 25\% to 75\% quantiles as a box with a horizontal line at median. Crosses show data beyond 1.5 times the interquartile range.
Let $\M R_{err}^{ij}$  be the error of the estimated relative rotation between cameras $i$ and $j$, computed as the angle in the axis-angle representation of $\M R_{ij}^{-1}\M R_{ij}^{\M{GT}}$ and let $\V t_{err}^{ij}$ be the error of the estimated translation computed as the angle between the two unit vectors corresponding to the translations~\cite{IMC2020}. 
Fig.~\ref{fig:noise} shows boxplots of %
pose errors measured in the same way as in our experiments in the main paper (\cf Sec.~4 in the main paper), \ie, as
$\text{max}\left(0.5 (\M R_{err}^{12} + \M R_{err}^{13}), 0.5 (\V t_{err}^{12} + \V t_{err}^{13})\right)$,
for the calibrated \sftp problem (left), and the partially calibrated \sstp problem (right).  
The errors are zoomed into an interesting interval and are shown as functions of varying noise from $0px$ to $4px$. 
%$\texttt{avg}(e(\M R_{12}),e(\M R_{13}))$,  %(b) 
%$\texttt{avg}(e(\M t_{12}),e(\M t_{13}))$, and %(c) 


Due to the approximate nature of the virtual correspondences, our newly proposed M-based and L-based solvers 
%\sftl and \sftm 
exhibit non-zero errors for zero noise. However, at noise levels $\geq 1px$,  
our $\delta$-based solvers (both M and L), and for the calibrated case even the pure \sftm and \sftl solvers, return comparable or even better results than the \sft and \sst solvers. 
%that sample one more point in the first two cameras. 
%
%both proposed solvers return comparable or even better results than the \sft solver, 
Note that the \sft and \sst solvers sample one/two more points (real correspondences) in the first two cameras, and these points are affected only by the considered noise.
%but uses real point correspondences affected only by considered noise. 
This shows that our predicted virtual correspondences are good approximations to real correspondences. 
%The \sftm solver is returning slightly worse rotation but slightly better translation errors than the \sftl solver.
For the calibrated case, the recent state-of-the-art solver~\cite{Hruby_cvpr2022} is failing in about 50\% of the instances for noiseless data, even though the solver was trained on the ETH3D dataset. Thus, the median errors are significantly larger than the median errors of the remaining solvers. 

The rotation and translation errors in the first two views, \ie, $\M R_{err}^{12}$ and $\V t_{err}^{12}$, for both the calibrated (top row), and the partially calibrated case (bottom row) are shown in Fig.~\ref{fig:noise_uncalibrated}.
For the partially calibrated case, our new solvers generate two approximate virtual correspondences in the first two views. Therefore, the \sstm and \sstl solvers have slightly larger errors than the \sst solver for all considered noise levels. 
However, similarly to the pose errors in Figure~\ref{fig:noise}, at noise levels $\geq 2px$ 
our $\delta$-based solvers (both M and L) return comparable or even better 
results in the first two views than the \texttt{5pt}~\cite{Nister-5pt-PAMI-2004} and \texttt{6pt}~\cite{stewenius-etal-omnivis-2005} solvers, here represented by the results of \sft and \sst solvers. 
For the calibrated case even the pure \sftm and \sftl solvers, without the offset $\delta$, perform comparably well as the \texttt{5pt} solver. 
%\footnote{Notice that we only report errors for the relative pose between the first two views, \ie, the results of the \sft and \sst solvers are identical to the results of the \texttt{5pt}~\cite{Nister-5pt-PAMI-2004} and \texttt{6pt}~\cite{stewenius-etal-omnivis-2005} solvers for these relative poses.}.
This shows an interesting potential of using  our solvers for the two-view relative pose estimation problems by solving these problems  from sub-minimal samples. 
%The mean \sftm solver %, which generates a $5^\text{th}$ correspondences from three points visible in images 1 and 2, %for image pair 1-2 uses only local information from input point coordinates in these two views 
% and samples a sub-minimal sample of four points, 
%achieves comparable results to the minimal 5pt point solver~\cite{Nister-5pt-PAMI-2004}, here represented by the results of \sft, for noise $\geq 3px$. 
%The \sftm solver only uses information from the two individual views.  
%Thus, the results indicate that a potential for %This opens door for investigation of using this simple idea for 
%solving minimal problems from sub-minimal samples.

\begin{figure}
    \centering
    \begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/poselib_graphs/maxerr_pt_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{Phototourism~\cite{IMC2020}}
    \end{subfigure}
    \begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/poselib_graphs/maxerr_cambridge_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{Cambridge Landmarks~\cite{kendall2015cambridge}}
    \end{subfigure}
    
    \caption{Speed-accuracy trade-off on (a) all scenes from Phototourism~\cite{IMC2020} except \textit{St. Peter's Square}, (b) 5  Cambridge Landmarks~\cite{kendall2015cambridge} scenes.
    We report the AUC@10$^\circ$ using the alternative definition of pose error \eqref{eq:max_error}. We vary the number of Poselib RANSAC iterations (100, 200, 500, 1000, 2000, 5000, 10000). Runtimes are averaged over all image triplets. Legend is the same as in Fig.~\ref{fig:poselib_graph_scenes}.}
    \label{fig:max_error}
\end{figure}

\begin{table}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{
\begin{tabular}{ l | c c | c c c | c}
    \toprule
    \multicolumn{7}{c}{Phototourism~\cite{IMC2020}} \\
    \midrule
    Estimator & AVG $(^\circ)$ $\downarrow$ & MED $(^\circ)$ $\downarrow$ & AUC@5 $\uparrow$ & @10 $\uparrow$ & @20 $\uparrow$ & Runtime (s) $\downarrow$\\
    \midrule
\sfhc~\cite{Hruby_cvpr2022} & 5.23 & 1.89 & 43.36 & 62.83 & 76.97 & 2.95 \\ \hline
\sft & 5.00 & 1.85 & 43.99 & 63.35 & 77.39 & 2.78 \\
\midrule
\sftm & 5.11 & 1.94 & 43.03 & 62.87 & 77.15 & 2.23 \\
\sftmR & 5.07 & 1.91 & 43.30 & 63.14 & 77.32 & 2.47 \\ 
\sftmRC & 5.05 & 1.89 & 43.41 & 63.24 & 77.39 & 2.42\\ \hline
\sftmd & 5.02 & 1.92 & 43.24 & 63.15 & 77.44 & 2.25\\
\sftmdR & 4.96 & 1.90 & 43.57 & 63.48 & 77.67 & 2.53\\
\sftmdRC & 5.00 & 1.89 & 43.51 & 63.38 & 77.56 & 2.41\\ \hline
\sftl & 5.46 & 1.93 & 42.82 & 62.07 & 76.25 & 2.88\\
\sftlR &  5.05 & 1.91 & 43.28 & 63.19 & 77.42 & 2.50\\
% \sftlidR & \underline{5.97} & \underline{1.89} & \underline{58.65} & \underline{71.43} & \underline{81.35} & 189.24 \\
\midrule
%\sfto & 4.87 & 1.82 & 44.44 & 63.96 & 77.91 & 2.77\\
\sftoR & 4.70 & 1.81 & 44.73 & 64.50 & 78.44 & 2.38 \\
%\sfepo &  4.19 & 1.69 & 46.55 & 66.37 & 79.99 & 2.40\\
\sfepoR & 4.24 & 1.74 & 46.01 & 65.97 & 79.74 & 2.39\\ 
%5.64 & 2.42 & 37.58 & 58.66 & 74.51 & 0.09\\
\bottomrule

    \end{tabular}}
    \caption{Results for different methods implemented in the GC-RANSAC framework~\cite{barath2017graph} for all scenes from the PhotoTourism~\cite{IMC2020} dataset. We mark the \textbf{best} and \underline{second best} results (excluding oracle solvers).}%Best and second best results (exluding oracle solvers) are highlighted with bold and underline font.}
    \label{tab:gcr_phototourism}
\end{table}
\begin{table}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{
\begin{tabular}{ l | c c | c c c | c}
    \toprule
    \multicolumn{7}{c}{Phototourism~\cite{IMC2020}} \\
    \midrule
    Estimator & AVG $(^\circ)$ $\downarrow$ & MED $(^\circ)$ $\downarrow$ & AUC@5 $\uparrow$ & @10 $\uparrow$ & @20 $\uparrow$ & Runtime (ms) $\downarrow$\\

\midrule

\sfhc~\cite{Hruby_cvpr2022} & 11.41 & 3.89 & 37.90 & 53.76 & 68.08 & \phantom{1}76.45 \\
\midrule\sft & \phantom{1}9.88 & 3.41 & 41.36 & 57.36 & 71.13 & 105.50 \\
\midrule\sftm & 11.65 & 4.21 & 35.38 & 51.73 & 66.77 & \phantom{1}76.77 \\
\sftmR & 10.52 & 3.47 & 41.01 & 56.62 & 70.25 & \phantom{1}\underline{74.94} \\
\sftmRC & 10.79 & 3.58 & 40.09 & 55.73 & 69.56 & \phantom{1}\textbf{30.54} \\
\midrule\sftmd & 10.44 & 3.72 & 38.82 & 55.07 & 69.51 & 172.06 \\
\sftmdR & \phantom{1}\textbf{9.84} & \textbf{3.27} & \textbf{42.62} & \textbf{58.30} & \textbf{71.74} & 189.21 \\
\sftmdRC & 10.14 & 3.40 & 41.45 & 57.24 & 70.92 & \phantom{1}75.78 \\
\midrule\sftl & 11.56 & 4.21 & 35.32 & 51.77 & 66.84 & 376.31 \\
\sftlR & 10.48 & 3.47 & 41.02 & 56.60 & 70.25 & 297.88 \\
\sftlidR & \phantom{1}\underline{9.86} & \underline{3.28} & \underline{42.60} & \underline{58.26} & \underline{71.71} & 730.90 \\
\midrule\sfto & \phantom{1}9.64 & 3.29 & 42.60 & 58.54 & 72.09 & \phantom{1}58.22 \\
\sftoR & \phantom{1}9.50 & 3.13 & 44.01 & 59.67 & 72.81 & \phantom{1}86.30 \\
\sftoRC & \phantom{1}9.45 & 3.16 & 43.82 & 59.55 & 72.76 & \phantom{1}36.36 \\
\midrule

\multicolumn{7}{c}{Cambridge Landmarks~\cite{kendall2015cambridge}} \\    
\midrule
    Estimator & AVG $(^\circ)$ $\downarrow$ & MED $(^\circ)$ $\downarrow$ & AUC@5 $\uparrow$ & @10 $\uparrow$ & @20 $\uparrow$ & Runtime (ms) $\downarrow$\\
\midrule

\sfhc~\cite{Hruby_cvpr2022} & 15.12 & 5.51 & 24.50 & 43.42 & 61.10 & \phantom{1}64.49 \\
\midrule\sft & \textbf{13.38} & 5.17 & 25.94 & 45.38 & 63.16 & \phantom{1}48.53 \\
\midrule\sftm & 15.31 & 5.68 & 22.93 & 42.06 & 60.26 & \phantom{1}38.33 \\
\sftmR & 14.28 & 5.29 & 25.37 & 44.57 & 62.33 & \phantom{1}\underline{34.84} \\
\sftmRC & 14.60 & 5.39 & 24.89 & 43.97 & 61.71 & \phantom{1}\textbf{16.51} \\
\midrule\sftmd & 14.18 & 5.36 & 24.72 & 44.12 & 62.15 & \phantom{1}81.21 \\
\sftmdR & 13.67 & \underline{5.11} & \textbf{26.17} & \underline{45.56} & \underline{63.33} & \phantom{1}84.98 \\
\sftmdRC & 13.89 & 5.23 & 25.43 & 44.78 & 62.70 & \phantom{1}38.45 \\
\midrule\sftl & 15.24 & 5.72 & 22.98 & 42.15 & 60.35 & 232.13 \\
\sftlR & 14.23 & 5.27 & 25.33 & 44.49 & 62.30 & 177.06 \\
\sftlidR & \underline{13.63} & \textbf{5.10} & \underline{26.11} & \textbf{45.58} & \textbf{63.35} & 395.43 \\
\midrule\sfto & 13.85 & 5.22 & 25.61 & 45.19 & 63.11 & \phantom{1}30.43 \\
\sftoR & 13.51 & 5.07 & 26.60 & 46.05 & 63.76 & \phantom{1}34.68 \\
\sftoRC & 13.47 & 5.10 & 26.47 & 45.98 & 63.71 & \phantom{1}16.41 \\
\midrule

    \end{tabular}}
    \caption{Results for different solvers implemented in the PoseLib framework~\cite{PoseLib} on all scenes from the PhotoTourism~\cite{IMC2020} and 5 scenes from the Cambridge Landmarks~\cite{kendall2015cambridge} datasets with the alternative definition of pose error \eqref{eq:max_error}.
    We mark the \textbf{best} and \underline{second best} results (excluding oracle solvers). Reported runtimes are for the whole RANSAC.}
    \label{tab:max_error2_sm}
\end{table}

\subsection{Alternative evaluation measure}
\label{sec:exp:measure}
% \PAR{Alternative Pose Error} 
For the evaluation in the main paper, we defined the pose error as $\text{max}\left(0.5 (\M R_{err}^{12} + \M R_{err}^{13}), 0.5 (\V t_{err}^{12} + \V t_{err}^{13})\right)$, where $\M R_{err}^{ij}$ and $\V t_{err}^{ij}$ are the angular errors of rotation and translation for camera pair $ij$ in degrees. The \sftp problem also includes the estimation of $\M R_{23}$ and $\V t_{23}$ since the relative scale of $\V t_{12}$ and $\V t_{13}$ is recovered. We therefore also presents results for the pose error defined as 
\begin{equation}
P_{err} = \text{max} \left(\M R_{err}^{12}, \M R_{err}^{13}, \M R_{err}^{23}, \V t_{err}^{12}, \V t_{err}^{13}, \V t_{err}^{23}\right) \enspace.    
\label{eq:max_error}
\end{equation}
The results equivalent to Tab.~2 from the main paper using this pose error definition are presented in Tab.~\ref{tab:max_error1_sm}. 
A speed-accuracy comparison equivalent to Fig.~3 in the main is presented in Fig.~\ref{fig:max_error}. The overall comparison of the methods remains the same under both the metric used in the main paper and the alternative described in this section. %even when using the alternative error metric.

\subsection{GC-RANSAC}
\label{sec:exp:gc_ransac}
% \PAR{Experiments on GC-RANSAC} 
We evaluated and compared our proposed solvers against the state-of-the-art solver, also in the GC-RANSAC~\cite{barath2017graph} framework. Results for all scenes of the PhotoTourism dataset~\cite{IMC2020} are presented in Tab.~\ref{tab:gcr_phototourism}.


\begin{figure*}
\resizebox{\textwidth}{!}{
\begin{tikzpicture} 
        \begin{axis}[%
        hide axis, xmin=0,xmax=0,ymin=0,ymax=0,
        legend style={draw=white!15!white, 
        line width = 1pt,
        legend  columns =9, % comment for column display
        /tikz/every even column/.append style={column sep=0.1cm},
        }
        ]
        
        \addlegendimage{Seaborn1}
        \addlegendentry{\scriptsize{\sftm}};
        \addlegendimage{Seaborn2}
        \addlegendentry{\scriptsize{\sftmR}};
        \addlegendimage{Seaborn3}
        \addlegendentry{\scriptsize{\sftmRC}};
        \addlegendimage{Seaborn4}
        \addlegendentry{\scriptsize{\sftmd}}; 
        \addlegendimage{Seaborn5}
        \addlegendentry{\scriptsize{\sftmdR}};   
        \addlegendimage{Seaborn6}
        \addlegendentry{\scriptsize{\sftmdRC}};

        
        \addlegendimage{Seaborn7}
        \addlegendentry{\scriptsize{\sfhc}};
        \addlegendimage{Seaborn10}
        \addlegendentry{\scriptsize{\sft}};
        \addlegendimage{Seaborn9,dash pattern=on 4pt off 2pt on 4pt off 2pt}
        \addlegendentry{\scriptsize{\sftoRC}};        
        % \addlegendimage{white}
        % \addlegendentry{~};
        \end{axis}
    \end{tikzpicture}}

    \centering
    \small{\textit{St. Mary's Church}~\cite{kendall2015cambridge}}\vspace{0.5ex}
    
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/StMarysChurch_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{Original Matches}        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/StMarysChurch_graph-0.6inliers-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{Synth - 0.6 inlier ratio}        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/StMarysChurch_graph-0.4inliers-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{Synth - 0.4 inlier ratio}        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/StMarysChurch_graph-0.2inliers-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{Synth - 0.2 inlier ratio}        
    \end{subfigure}

    \small{\textit{Sacre Coeur}~\cite{IMC2020}}
    \vspace{0.5ex}

    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/sacre_coeur_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{Original Matches}        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/sacre_coeur_graph-0.6inliers-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{Synth - 0.6 inlier ratio}        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/sacre_coeur_graph-0.4inliers-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{Synth - 0.4 inlier ratio}        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/sacre_coeur_graph-0.2inliers-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{Synth - 0.2 inlier ratio}        
    \end{subfigure}

   
    \caption{Outlier experiments for \textit{St. Mary's Church} scene from Cambridge Landmarks~\cite{kendall2015cambridge} (a-d) and \textit{Sacre Coeur} scene from Photorouism~\cite{IMC2020} (e-h). We show the speed-accuracy trade-off evaluation (see Fig.~\ref{fig:poselib_graph_scenes}) for the original matches (a,e) and for synthetic scenario where we kept the matches that are inliers w.r.t. ground truth poses and added randomly distributed noisy matches to obtain desired inlier ratio (b-d,f-h).}
    \label{fig:graph_poselib_outliers}
\end{figure*}

\subsection{Outlier experiments}
\label{sec:exp:outliers}
Fig.~4 in the main paper presented results obtained by %We %that we perform a semi-synthetic experiment. We first show results using unaltered matches for the \textit{Brandenurg Gate} scene from Phototourism. On this scene \sftmRC and \sft have very similar performance. For Fig.~\ref{fig:poselib_outlier_synth} we 
synthetically removing outlier matches based on ground truth pose information and replacing them with outliers distributed uniformly at random to reach a given inlier ratio for all image triplets. 
% The outlier experiments are implemented as described in the main paper. 
Here we provide additional plots for two scenes: \textit{St Mary's Church} from Cambridge Landmarks~\cite{kendall2015cambridge} and \textit{Sacre Coeur} from Phototourism~\cite{IMC2020}. The results for the speed-accuracy evaluation are shown in Fig.~\ref{fig:graph_poselib_outliers}. Consistent with the results in the main paper, these graphs show the potential for our \texttt{M}-based solvers to perform better than the the \sft baseline in scenarios with low inlier ratios.

% inlier ratios $\{0.1, 0.2, 0.4\}$. In Figures~\ref{fig:outlier_sacre_inlier} and~\ref{fig:outlier_peter_inlier}, we show the performance of each solver w.r.t. the percentage of inliers that they gather, on different RANSAC iterations, for the Sacre Coeur and St. Peters' Square scenes, respectively. In Figure~\ref{fig:outlier_peter_pose}, we show the performance of the solvers w.r.t. average pose error on the St. Peters' Square scene, similar to the one included in the main paper for Sacre Coeur. 
% The pose error was measured in the same way as in our experiments in the main paper (\cf Sec.~4 in the main paper), \ie, as
% $\texttt{max}(\texttt{avg}(e(\M R_{12}),e(\M R_{13})),\texttt{avg}(e(\M t_{12}),e(\M t_{13})))$.


% For low inlier ratios and in early RANSAC iterations, we observe that the $\delta$-based solvers perform the best, slightly better than \sftm and \sftl. \sft and \sfhc solvers show worse performance than our proposed solvers in this experiment. Increasing the inlier ratio up to 0.4, the performance of \sft increases, becoming similar to the performance of \sftm and \sftl. Even for an inlier ratio of 0.4, \sfhc exhibits a slower convergence due to the higher failure rate.



\begin{figure*}
    \centering

    \resizebox{\textwidth}{!}{
\begin{tikzpicture} 

        \begin{axis}[%
        hide axis, xmin=0,xmax=0,ymin=0,ymax=0,
        legend style={draw=white!15!white, 
        line width = 1pt,
        legend  columns =9, % comment for column display
        /tikz/every even column/.append style={column sep=0.1cm},
        }
        ]
        
        \addlegendimage{Seaborn1}
        \addlegendentry{\scriptsize{\sftm}};
        \addlegendimage{Seaborn2}
        \addlegendentry{\scriptsize{\sftmR}};
        \addlegendimage{Seaborn3}
        \addlegendentry{\scriptsize{\sftmRC}};
        \addlegendimage{Seaborn4}
        \addlegendentry{\scriptsize{\sftmd}}; 
        \addlegendimage{Seaborn5}
        \addlegendentry{\scriptsize{\sftmdR}};   
        \addlegendimage{Seaborn6}
        \addlegendentry{\scriptsize{\sftmdRC}};

        
        \addlegendimage{Seaborn7}
        \addlegendentry{\scriptsize{\sfhc}};
        \addlegendimage{Seaborn10}
        \addlegendentry{\scriptsize{\sft}};
        \addlegendimage{Seaborn9,dash pattern=on 4pt off 2pt on 4pt off 2pt}
        \addlegendentry{\scriptsize{\sftoRC}};        
        % \addlegendimage{white}
        % \addlegendentry{~};
        \end{axis}
    \end{tikzpicture}}

    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/cambridge_GreatCourt_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{Great Court}}        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/cambridge_ShopFacade_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{Shop Facade}}        
    \end{subfigure}
    \hfill
    % \begin{subfigure}{0.24\linewidth}
    % \includegraphics[width=\linewidth]{figures/poselib_graphs/cambridge_StMarysChurch_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    % \caption{\textit{St. Mary's Church}}        
    % \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/cambridge_OldHospital_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{Old Hospital}}        
    \end{subfigure}
    \hfill    
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/pt_buckingham_palace_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{Buckingham Palace}}        
    \end{subfigure}

    \vspace{0.5ex}

    
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/pt_colosseum_exterior_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{Colosseum Exterior}}        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/pt_grand_place_brussels_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{Grand Place Brussels}}        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/pt_notre_dame_front_facade_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{Notre Dame}}        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/pt_palace_of_westminster_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{Palace of Westminster}}        
    \end{subfigure}

    \vspace{0.5ex}
    
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/pt_pantheon_exterior_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{Pantheon Exterior}}        
    \end{subfigure}
    \hfill
    % \begin{subfigure}{0.24\linewidth}
    % \includegraphics[width=\linewidth]{figures/poselib_graphs/pt_sacre_coeur_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    % \caption{\textit{Sacre Coeur}}        
    % \end{subfigure}
    % \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/pt_taj_mahal_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{Taj Mahal}}        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/pt_temple_nara_japan_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{Temple Nara}}        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/poselib_graphs/pt_trevi_fountain_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{Trevi Fountain}}        
    \end{subfigure}
    
    \caption{Results for individual scenes from the Cambrdige Landmarks~\cite{kendall2015cambridge} (a-c) and Phototourism~\cite{IMC2020} (d-l) datasets which were not presented in the main paper or Fig.~\ref{fig:graph_poselib_outliers}. We report the AUC@10$^\circ$ of the pose error and vary the number of Poselib RANSAC iterations (100, 200, 500, 1000, 2000, 5000, 10000). Runtimes are averaged over all image triplets.}
    \label{fig:poselib_graph_scenes}
\end{figure*}

\subsection{Detailed experiments on Real Data}
\label{sec:exp:details}
\PAR{Results for individual scenes.} 
Fig.~3 in the main paper shows results on all PhotoTourism scenes (except St. Peter's Square), the 5 Cambridge Landmarks scenes we consider, and one individual scene from each dataset. 
In Fig.~\ref{fig:poselib_graph_scenes}, we provide results for the accuracy-speed trade-off evaluation for more evaluation scenes on both PhotoTourism~\cite{IMC2020} and Cambridge Landmarks~\cite{kendall2015cambridge}. 
As discussed and shown in the main paper, the performance of our M-based and L-based solvers is scene-dependent. 
This can also be seen in Fig.~\ref{fig:poselib_graph_scenes}, where for some scenes, the \sftmR and \sftmdR solvers perform worse than the \sft solver (\textit{Shop Facade}, \textit{Palace of Westminster}, \textit{Trevi Fountain}). 
However, for the majority of the scenes, our solvers perform similar to \sft (\textit{Old Hospital}) or even outperform \sft (\textit{Great Court}, \textit{Buckingham Palace}, \textit{Colosseum Exterior}, \textit{Grand Place Brussels}, \textit{Notre Dame}, \textit{Pantheon Exterior}, \textit{Taj Mahal}, \textit{Temple Nara}). 
Overall, the results validate the practical viability of our solvers in a time-constrained setting.

\PAR{Results for L-based solvers.} In Tab.~\ref{tab:poselib_l_table} we provide the same results as shown in Tab.~3 in the main paper, including also more variants for L-based solvers, \ie \ \sftlRC, \sftld, \sftldR, \sftldRC, \sftlid, \sftlidR, and \sftlidRC. %\todo{Move somwhere else? Add discussion}

\begin{table}[!ht]
    \centering
    \resizebox{1.0\linewidth}{!}{
\begin{tabular}{ l | c c | c c c | c}
    \toprule
    \multicolumn{7}{c}{Phototourism~\cite{IMC2020}} \\
    \midrule
    Estimator & AVG $(^\circ)$ $\downarrow$ & MED $(^\circ)$ $\downarrow$ & AUC@5 $\uparrow$ & @10 $\uparrow$ & @20 $\uparrow$ & Runtime (ms) $\downarrow$\\
    \midrule
\sfhc~\cite{Hruby_cvpr2022} & 7.17 & 2.34 & 52.74 & 66.63 & 77.86 & \phantom{1}76.45 \\
\midrule\sft & 5.99 & 2.00 & 57.31 & 70.54 & 80.81 & 105.50 \\
\midrule\sftm & 7.17 & 2.49 & 50.96 & 65.46 & 77.32 & \phantom{1}76.77 \\
\sftmR & 6.39 & 2.00 & 56.92 & 69.90 & 80.17 & \phantom{1}\underline{74.94} \\
\sftmRC & 6.59 & 2.07 & 55.90 & 69.06 & 79.56 & \phantom{1}\textbf{30.54} \\
\midrule\sftmd & 6.39 & 2.19 & 54.70 & 68.58 & 79.59 & 172.06 \\
\sftmdR & \textbf{5.97} & \underline{1.89} & \textbf{58.65} & \textbf{71.43} & \textbf{81.35} & 189.21 \\
\sftmdRC & 6.15 & 1.97 & 57.42 & 70.47 & 80.69 & \phantom{1}75.78 \\
\midrule\sftl & 7.12 & 2.50 & 51.00 & 65.57 & 77.42 & 376.31 \\
\sftlR & 6.35 & 2.00 & 56.88 & 69.88 & 80.15 & 297.88 \\
\sftlRC & 6.55 & 2.07 & 55.88 & 69.08 & 79.55 & 255.93 \\
\midrule\sftld & 6.39 & 2.19 & 54.70 & 68.58 & 79.59 & 172.05 \\
\sftldR & \underline{5.97} & 1.89 & \underline{58.65} & \underline{71.43} & \underline{81.35} & 189.28 \\
\sftldRC & 6.15 & 1.97 & 57.42 & 70.47 & 80.69 & \phantom{1}75.76 \\
\midrule\sftlid & 5.97 & 1.91 & 58.36 & 71.25 & 81.26 & 679.80 \\
\sftlidR & 5.97 & \textbf{1.89} & 58.63 & 71.41 & 81.34 & 730.90 \\
\sftlidRC & 6.04 & 1.92 & 58.11 & 70.97 & 81.02 & 605.86 \\
\midrule\sfto & 5.82 & 1.90 & 58.91 & 71.84 & 81.70 & \phantom{1}58.22 \\
\sftoR & 5.73 & 1.80 & 60.23 & 72.75 & 82.21 & \phantom{1}86.30 \\
\sftoRC & 5.72 & 1.82 & 59.97 & 72.58 & 82.13 & \phantom{1}36.36 \\
\midrule
\multicolumn{7}{c}{Cambridge Landmarks~\cite{kendall2015cambridge}} \\    
\midrule
    Estimator & AVG $(^\circ)$ $\downarrow$ & MED $(^\circ)$ $\downarrow$ & AUC@5 $\uparrow$ & @10 $\uparrow$ & @20 $\uparrow$ & Runtime (ms) $\downarrow$\\
\midrule
\sfhc~\cite{Hruby_cvpr2022} & 9.69 & 3.31 & 40.96 & 58.84 & 72.83 & \phantom{1}64.49 \\
\midrule\sft & \textbf{8.16} & 3.05 & 43.79 & 61.61 & 75.30 & \phantom{1}48.53 \\
\midrule\sftm & 9.61 & 3.42 & 39.71 & 58.08 & 72.54 & \phantom{1}38.33 \\
\sftmR & 8.77 & 3.11 & 42.98 & 60.90 & 74.59 & \phantom{1}\underline{34.84} \\
\sftmRC & 9.03 & 3.17 & 42.31 & 60.18 & 74.03 & \phantom{1}\textbf{16.51} \\
\midrule\sftmd & 8.75 & 3.21 & 42.11 & 60.37 & 74.38 & \phantom{1}81.21 \\
\sftmdR & 8.32 & \underline{3.01} & \underline{44.17} & \underline{62.04} & \underline{75.60} & \phantom{1}84.98 \\
\sftmdRC & 8.47 & 3.08 & 43.21 & 61.22 & 75.03 & \phantom{1}38.45 \\
\midrule\sftl & 9.58 & 3.44 & 39.79 & 58.17 & 72.62 & 232.13 \\
\sftlR & 8.75 & 3.09 & 42.94 & 60.86 & 74.60 & 177.06 \\
\sftlRC & 8.92 & 3.17 & 42.28 & 60.19 & 74.08 & 163.95 \\
\midrule\sftld & 8.75 & 3.21 & 42.11 & 60.37 & 74.38 & \phantom{1}81.20 \\
\sftldR & 8.32 & 3.01 & 44.17 & 62.04 & 75.60 & \phantom{1}84.89 \\
\sftldRC & 8.47 & 3.08 & 43.21 & 61.22 & 75.03 & \phantom{1}38.49 \\
\midrule\sftlid & 8.43 & 3.05 & 43.80 & 61.70 & 75.34 & 379.49 \\
\sftlidR & \underline{8.28} & \textbf{3.00} & \textbf{44.26} & \textbf{62.10} & \textbf{75.62} & 395.43 \\
\sftlidRC & 8.33 & 3.04 & 43.82 & 61.71 & 75.35 & 349.45 \\
\midrule\sfto & 8.62 & 3.07 & 43.58 & 61.65 & 75.30 & \phantom{1}30.43 \\
\sftoR & 8.39 & 2.94 & 44.80 & 62.54 & 75.93 & \phantom{1}34.68 \\
\sftoRC & 8.36 & 2.97 & 44.64 & 62.38 & 75.82 & \phantom{1}16.41 \\
\midrule
    \end{tabular}}
    \caption{Results for different solvers implemented in the PoseLib framework~\cite{PoseLib} on all scenes from the PhotoTourism~\cite{IMC2020} and 5 scenes from the Cambridge Landmarks~\cite{kendall2015cambridge} datasets including all evaluated variants of the proposed solvers. 
    %Results are presented for early termination with $0.9999$ confidence. 
    We mark the \textbf{best} and \underline{second best} results (excluding oracle solvers). Reported runtimes are for the whole RANSAC.}
    \label{tab:poselib_l_table}
\end{table}

\begin{table*}[t!]
 \begin{center}
 \resizebox{0.9\linewidth}{!}{
 \begin{tabular}{|l| c | c | c | c | c | c | c |}
    \hline
    & \sft & \sfhc & \sftm & \sftmd & \sftl & \sftld & \sftlid \\
    \hline
    Time ($\mu$s) & 77.90 & 66.06 & 83.92 & 218.71 & 450.26 & 511.28 & 1130.31 \\
    \hline
\end{tabular}
 }
\end{center}
\caption{The average run-time, averaged over more than 10k instances of the Sacre Coeur scene of the PhotoTourism dataset~\cite{snavely2006photo}, of the solvers for the calibrated case.}
\label{tab:calib_time}
\end{table*}


\begin{table*}[t!]
 \begin{center}
 \resizebox{0.9\linewidth}{!}{
 \begin{tabular}{|l| c | c | c | c | c | c |}
    \hline
    & \sst & \sstm & \sstmd & \sstl & \sstld & \sstlid \\
    \hline
    Time ($\mu$s) & 106.67 & 117.28 & 295.87 & 758.59 & 953.34 & 2162.77 \\
    \hline
\end{tabular}
 }
\end{center}
\caption{The average run-time, averaged over more than 10k instances of the Sacre Coeur scene of the PhotoTourism dataset~\cite{snavely2006photo}, of the solvers for the partially calibrated case.}
\label{tab:focal_time}
\end{table*}

\subsection{Oracle solvers}
\label{sec:exp:oracles}
We first provide more details on implementation of our oracle  version of the %Nister's 
\sfep solver~\cite{DBLP:journals/ijcv/NisterS06}, 
% We test an oracle version of the %Nister's 
\ie the \sfepo solver. 
In the \sfepo solver, instead of performing an one-dimensional search over the $10^{\text{th}}$ degree curve of possible epipoles, we provide the solver with the ground truth epipole. 
To simulate the effect of sampling four points for this solver inside RANSAC, instead of using the second epipole and the epipolar line homography to get the essential matrix $\M{E}$, as suggested in the implementation details of~\cite{DBLP:journals/ijcv/NisterS06}, we use the second suggested way on how to obtain $\M{E}$, \ie, using their \step solver. However, we feed the \step solver %, we however, feed 
with four points and use SVD instead of the null space. The rest of the solver performs the triangulation and registers the last camera using the P3P solver~\cite{lambda-twist}. This is identical to the original \sfep solver. 
% However, in this case we do not need to use the fourth point correspondence to select the pose that minimizes the reprojection error. Similarly, we do not need to use refinement. 
Remember that the original %Nister's 
\sfep solver needs to call these evaluations for each search step on the $10^{\text{th}}$ degree curve of possible epipoles (usually $40{\times}-1000{\times}$~\cite{DBLP:journals/ijcv/NisterS06}). Moreover, this solver has several sources of errors, \eg, the $10^{\text{th}}$ degree curve is affected by noise; sparse sampling of the points on the curve introduces additional potentially large noise in the epipole; the reprojections of the fourth image point in the third view, traced out by sweeping through the curve of possible epipoles, generates complex curves in the third view, with the reprojection cost function having a lot of local minima. In the paper~\cite{DBLP:journals/ijcv/NisterS06}, it was reported that %, \eg, 
even for exact data and 1000 search points followed by refinement at multiple local minima the failure rate of the solver is $\approx 3\%$. Therefore, we expect the original solver \sfep to perform much worse that the ``oracle" \sfepo solver. % used in the  experiments presented in the main paper.

To obtain upper bounds for the precision that can be achieved by our %of the 
proposed %4p3v solvers, \ie, the  
solvers we consider the following ``oracle'' solvers for real experiments: 
The \sfto/\ssto solvers use a correct correspondence(s), \ie, a correspondence(s) that satisfies the epipolar constraint for the ground truth relative pose of the first two cameras, as the $5^\text{th}$/$6^\text{th}$ virtual correspondence between these cameras. Then the \sft/\sst solver is applied to estimate the relative pose of three cameras. 
The \sfto/\ssto solvers thus indicate the maximum precision that %the  \sftl and \sftm can reach,
our  solvers can reach, if they would have been able to predict or infer a precise %correct 
$5^\text{th}$/$6^\text{th}$ correspondence from the coordinates of four input correspondences.

Tab.~\ref{tab:gcr_phototourism} compares the \sfepo with the \sfto solver as well as various variants of our M-based and L-based solvers as well as the \sfhc~\cite{Hruby_cvpr2022} and \sft approaches. 
Results are shown over all scenes from the PhotoTourism dataset using GC-RANSAC. 
The \sfepo solver performs slightly better than %comparable to 
\sfto in terms of pose accuracy.\footnote{We observed that when naively applying \sfepo in RANSAC, RANSAC tends to terminate too early, resulting in reduced pose accuracy. This resulted in the statement in the main paper that the \sfepo oracle performed worse than our oracles. In order to obtain results comparable with \sfto, we had to adapt  RANSAC.} 
While the pose accuracy of \sfepo provides an upper bound on the performance of the \sfep solver~\cite{DBLP:journals/ijcv/NisterS06}, the run-time observed for \sfepo is not indicative of the run-time of \sfep.  
As highlighted above and in the main paper, the \sfep solver needs to evaluate up to thousands of epipole estimates and is thus significantly slower than its oracle variant (which only evaluates a single epipole). 
In contrast, our M-based solvers have a run-time comparable to \sfto. 
In addition, the epipoles used by \sfep can be rather noisy, depending on how densely the curve is sampled. 
Even for a rather dense sampling, which increases the run-time of the \sfep solver, \cite{DBLP:journals/ijcv/NisterS06} report that their solver often has problems with local minima.
% As can be seen, all other methods outperform \sfepo in terms of pose error, justifying our omission of the \sfepo results in the main paper. 
The results of the \sfto solver show that there is room for improvement for our M-based and L-based solvers by developing approaches to generating more accurate virtual correspondences. 


\subsection{Solver run-times}
\label{sec:exp:timing}
In this section, we present run-times of the proposed solvers as well as the state-of-the-art solvers for the relative pose problem of three calibrated/partially calibrated cameras. 
While the main paper reports run-time results for  full RANSAC-based estimation, we now report the run-times of the individual solvers outside of RANSAC. 
To measure the run-times of the solvers, we calculated the average run-time of each solver on more than 10k instances of the Sacre Coeur scene of the PhotoTourism dataset~\cite{snavely2006photo}.
For calibrated cameras, the run-times are reported in Table~\ref{tab:calib_time}, and for partially calibrated cameras in Table~\ref{tab:focal_time}. The experiments were performed on an Intel(R) Core(TM) i9-10900X CPU @ 3.70GHz.
The average run-times of the L-based solvers are higher, because we run the network on the CPU and without batching. In general, the implementations of all proposed solvers are not optimized for speed, and we still see room for speeding them up. 