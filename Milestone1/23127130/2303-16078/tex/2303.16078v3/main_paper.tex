%%%%%%%%% TITLE - PLEASE UPDATE
\title{
%Using virtual correspondences in minimal solvers with applications to the four-points-in-three-views problem / 
Relative pose of three calibrated and partially calibrated cameras from four points using virtual correspondences}

%%%%%%%%% AUTHORS - PLEASE UPDATE
% \author{Charalambos Tzamos\inst{1} \and
% Viktor Kocur\inst{2} \and
% Daniel Barath\inst{3} \and
% Zuzana Berger Haladová\inst{2} \and
% Torsten Sattler\inst{4} \and
% Zuzana Kukelova\inst{1}}

% % TODO FINAL: Replace with an abbreviated list of authors.
% %\authorrunning{C.~Tzamos et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.

% % TODO FINAL: Replace with your institution list.
% \institute{Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague \\
% \email{\{tzamocha, kukelzuz\}@fel.cvut.cz} \and
% Faculty of Mathematics, Physics and Informatics, Comenius University in Bratislava \\
% \email{\{viktor.kocur, haladova\}@fmph.uniba.sk} \and
% ETH Zürich, Computer Vision and Geometry Group, Switzerland \\
% \email{danielbela.barath@inf.ethz.ch} \and
% Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague \\
% \email{torsten.sattler@cvut.cz}
% }

\author{Charalambos Tzamos$^\textrm{1}$ \quad
Viktor Kocur$^\textrm{2}$ \quad
Daniel Barath$^\textrm{3}$ \quad
Zuzana Berger Haladová$^\textrm{2}$\\
Torsten Sattler$^\textrm{4}$ \quad
Zuzana Kukelova$^\textrm{1}$\vspace{10pt}\\
$^\textrm{1}$Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague\\
{\tt\small \{tzamocha, kukelzuz\}@fel.cvut.cz} \\
$^\textrm{2}$Faculty of Mathematics, Physics and Informatics, Comenius University in Bratislava\\
{\tt\small \{viktor.kocur, haladova\}@fmph.uniba.sk} \\
$^\textrm{3}$ETH Zürich, Computer Vision and Geometry Group, Switzerland\\
{\tt\small danielbela.barath@inf.ethz.ch} \\
$^\textrm{4}$Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague\\
{\tt\small torsten.sattler@cvut.cz}
}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


%teaser-tikz
% \tikzstyle{bedge}=[line width=1pt,cyan]
% \tikzstyle{bsvertex}=[circle, draw=black, fill=cyan, inner sep=0pt, minimum size=2pt]
% \tikzstyle{rsvertex}=[circle, draw=black, fill=red, inner sep=0pt, minimum size=2pt]

\tikzstyle{bedge}=[line width=0.4pt, color=blue]
\tikzstyle{redge}=[line width=0.4pt, color=red]
\tikzstyle{gedge}=[line width=0.4pt, color=green]
\tikzstyle{bsvertex}=[circle, draw=black, fill=cyan, inner sep=0pt, minimum size=0.4pt]
\tikzstyle{rsvertex}=[circle, draw=black, fill=red, inner sep=0pt, minimum size=0.2pt]
\newcommand{\iccv}{
\begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.55\columnwidth]{figures/rotunda2.PNG}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
    
        \node[rsvertex] (10) at (0.205, 0.274) {};
        \node[rsvertex] (10) at (0.406, 0.194) {};
        
        \draw[bedge] (0.24, 0.71) -- (0.155, 0.235);
        \draw[bedge] (0.35, 0.56) -- (0.155, 0.235);
        \draw[bedge] (0.66, 0.55) -- (0.155, 0.235);
        \draw[bedge] (0.57, 0.31) -- (0.155, 0.235);

        \draw[bedge] (0.24, 0.71) -- (0.395, 0.155);
        \draw[bedge] (0.35, 0.56) -- (0.395, 0.155);
        \draw[bedge] (0.66, 0.55) -- (0.395, 0.155);
        \draw[bedge] (0.57, 0.31) -- (0.395, 0.155);

        \draw[bedge] (0.24, 0.71) -- (0.848, 0.14);
        \draw[bedge] (0.35, 0.56) -- (0.848, 0.14);
        \draw[bedge] (0.66, 0.55) -- (0.848, 0.14);
        \draw[bedge] (0.57, 0.31) -- (0.848, 0.14);

        \node[bsvertex] (0) at (0.168, 0.31) {};
        \node[bsvertex] (1) at (0.188, 0.29) {};
        \node[bsvertex] (2) at (0.225, 0.279) {};
        \node[bsvertex] (3) at (0.225, 0.247) {};

        \node[bsvertex] (4) at (0.374, 0.228) {};
        \node[bsvertex] (5) at (0.389, 0.207) {};
        \node[bsvertex] (5) at (0.423, 0.197) {};
        \node[bsvertex] (6) at (0.42, 0.176) {};

        \node[bsvertex] (7) at (0.792, 0.174) {};
        \node[bsvertex] (8) at (0.765, 0.21) {};
        \node[bsvertex] (9) at (0.758, 0.225) {};
        \node[bsvertex] (10) at (0.816, 0.21) {};

        

        \draw[->, red, thick, dashed] plot [smooth, tension=1] coordinates {(0.155, 0.21) (0.25, 0.09) (0.395, 0.11)};
        \node at (0.23, 0.05) {$\texttt{\scriptsize 5pt}$};

        \draw[->, orange, thick, dashed] plot [smooth, tension=1] coordinates {(0.87, 0.16) (0.92, 0.4) (0.68, 0.55)};
        \node at (0.97, 0.25) {$\texttt{\scriptsize P3P}$};
        %\clip (0.5,0.5) circle (1cm);
    \end{scope}
\end{tikzpicture}
}

\newcommand{\iccvnew}{
\begin{tikzpicture}
    %\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.47\columnwidth]{figures/new_teaser.png}};
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.9\columnwidth]{figures/old_town.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
    
        % \node[rsvertex] (10) at (0.205, 0.274) {};
        % \node[rsvertex] (10) at (0.406, 0.194) {};
        
        \draw[redge] (0.4521, 0.815) -- (0.06, 0.08);
        \draw[redge] (0.314, 0.627) -- (0.06, 0.08);
        \draw[redge] (0.39, 0.403) -- (0.06, 0.08);
        \draw[redge] (0.3321, 0.885) -- (0.06, 0.08);

        \draw[gedge] (0.4521, 0.815) -- (0.7202, 0.085);
        \draw[gedge] (0.314, 0.627) -- (0.7202, 0.085);
        \draw[gedge] (0.39, 0.403) -- (0.7202, 0.085);
        \draw[gedge] (0.3321, 0.885) -- (0.7202, 0.085);

        \draw[bedge] (0.4521, 0.815) -- (0.59, 0.06);
        \draw[bedge] (0.314, 0.627) -- (0.59, 0.06);
        \draw[bedge] (0.39, 0.403) -- (0.59, 0.06);
        \draw[bedge] (0.3321, 0.885) -- (0.59, 0.06);

        % \draw[redge] (0.4521, 0.815) -- (0.267, 0.22);
        % \draw[redge] (0.434, 0.607) -- (0.267, 0.22);
        % \draw[redge] (0.419, 0.413) -- (0.267, 0.22);
        % \draw[redge] (0.529, 0.46) -- (0.267, 0.22);

        % \draw[bedge] (0.453, 0.815) -- (0.449, 0.201);
        % \draw[bedge] (0.4365, 0.607) -- (0.449, 0.201);
        % \draw[bedge] (0.4225, 0.412) -- (0.449, 0.201);
        % \draw[bedge] (0.5305, 0.459) -- (0.449, 0.201);

        % \draw[gedge] (0.455, 0.816) -- (0.8102, 0.182);
        % \draw[gedge] (0.4395, 0.607) -- (0.8102, 0.182);
        % \draw[gedge] (0.426, 0.414) -- (0.8102, 0.182);
        % \draw[gedge] (0.536, 0.46) -- (0.8102, 0.182);

        % \node[rsvertex] (1) at (0.7595, 0.142) {};
        % \node[rsvertex] (2) at (0.8284, 0.14) {};
        % \node[rsvertex] (3) at (0.8335, 0.252) {};
        % \node[rsvertex] (4) at (0.765, 0.255) {};

        % \draw[gedge] (1) -- (2);
        % \draw[gedge] (1) -- (4);
        % \draw[gedge] (2) -- (3);
        % \draw[gedge] (3) -- (4);

        % \node[rsvertex] (5) at (0.4102, 0.159) {};
        % \node[rsvertex] (6) at (0.48, 0.1557) {};
        % \node[rsvertex] (7) at (0.4837, 0.2707) {};
        % \node[rsvertex] (8) at (0.4135, 0.2735) {};

        % \draw[bedge] (5) -- (6);
        % \draw[bedge] (5) -- (8);
        % \draw[bedge] (6) -- (7);
        % \draw[bedge] (7) -- (8);

        % \node[rsvertex] (5) at (0.4102, 0.159) {};
        % \node[rsvertex] (6) at (0.48, 0.1557) {};
        % \node[rsvertex] (7) at (0.4837, 0.2707) {};
        % \node[rsvertex] (8) at (0.4135, 0.2735) {};

        % \node[bsvertex] (0) at (0.168, 0.31) {};
        % \node[bsvertex] (1) at (0.188, 0.29) {};
        % \node[bsvertex] (2) at (0.225, 0.279) {};
        % \node[bsvertex] (3) at (0.225, 0.247) {};

        % \node[bsvertex] (4) at (0.374, 0.228) {};
        % \node[bsvertex] (5) at (0.389, 0.207) {};
        % \node[bsvertex] (5) at (0.423, 0.197) {};
        % \node[bsvertex] (6) at (0.42, 0.176) {};

        % \node[bsvertex] (7) at (0.792, 0.174) {};
        % \node[bsvertex] (8) at (0.765, 0.21) {};
        % \node[bsvertex] (9) at (0.758, 0.225) {};
        % \node[bsvertex] (10) at (0.816, 0.21) {};

        

        \draw[->, orange, thick, dashed] plot [smooth, tension=1] coordinates {(0.17, 0.05) (0.3, -0.02) (0.49, 0.04)};
        \node at (0.32, 0.05) {$\texttt{\scriptsize 5pt}$};

        \draw[->, orange, thick, dashed] plot [smooth, tension=1] coordinates {(0.7302, 0.085) (0.7502, 0.425) (0.6202, 0.685)};
        \node at (0.7102, 0.675) {$\texttt{\scriptsize P3P}$};

        % \draw[->, orange, thick, dashed] plot [smooth, tension=1] coordinates {(0.24, 0.16) (0.26, 0.1) (0.4, 0.15)};
        % \node at (0.23, 0.07) {$\texttt{\scriptsize 5pt}$};

        % \draw[->, orange, thick, dashed] plot [smooth, tension=1] coordinates {(0.85, 0.18) (0.92, 0.4) (0.68, 0.55)};
        % \node at (0.96, 0.27) {$\texttt{\scriptsize P3P}$};
        %\clip (0.5,0.5) circle (1cm);
    \end{scope}
\end{tikzpicture}
}

%\begin{document}

\maketitle
% Remove page # from the first page of camera-ready.

%%%%%%%%% ABSTRACT
\begin{abstract}
 We study challenging problems of estimating the relative pose of three cameras and propose novel efficient solutions to the configurations (1) of four points in three calibrated cameras (the 4p3v problem), and (2) of four points in three cameras with unknown shared focal length (the 4p3vf problem). Our solutions are based on the simple idea of generating one or two additional virtual point correspondences in two views by using the information from the locations of the input correspondences. We generate such correspondences using a very simple and efficient strategy, where the new points are the mean points of three corresponding input points. The new solvers are efficient and easy to implement, since they are based on existing efficient minimal solvers, i.e., the well-known 5-point and 6-point relative pose solvers and the P3P solver. Extensive experiments on real data show that our solvers achieve state-of-the-art results. We also present a simple network that can improve the precision of the mean-point correspondences, showing the potential to learn better virtual point correspondences.  
\end{abstract}




%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}




%Estimating the camera geometry, \ie, the pose and calibration parameters, 
Camera geometry estimation 
is crucial in many computer vision applications, \eg, visual navigation~\cite{DBLP:journals/ram/ScaramuzzaF11}, Structure-from-Motion~\cite{Snavely-IJCV-2008},
augmented %/ mixed 
reality~\cite{Castle08ISWC}, self-driving cars~\cite{hane20173d}, 
%large-scale 3D reconstruction~\cite{DBLP:conf/cvpr/HeinlySDF15}, 
and 
%image based 
visual localization~\cite{Sattler16PAMI}.
%
Due to
noise and outliers in the input correspondences, 
the predominant way for camera geometry estimation is to use a hypothesis-and-test framework, \eg,  RANSAC~\cite{Fischler-Bolles-ACM-1981,Chum-2003, DBLP:journals/pami/RaguramCPMF13, barath2017graph}. 
For RANSAC-like methods, using as few (ideally the minimal number of) correspondences as possible for estimation %, %it is important to estimate camera geometry using as few correspondences as possible, 
% ideally the minimal number of correspondences, 
is important 
since the number of RANSAC iterations (and, thus, its run-time)
 % depends 
grows exponentially %on 
with the number of correspondences required for the model estimation.
 
 \begin{figure}[t]
     \centering
     %\includegraphics[width=0.9\columnwidth]{figures/teaser-4pt-3.png}
     \iccvnew
     \caption{Visualization of the 4p3v problem and our solution that is based on generating new virtual correspondence(s) between two views. This is done using coordinates of input point correspondences. 
     %Using the virtual correspondence, 
     Then the 4p3v problem is solved by existing efficient minimal solvers, \ie, the 5pt~\cite{Nister-5pt-PAMI-2004} 
     %relative pose solver~\cite{Nister-5pt-PAMI-2004} 
     and the P3P solvers~\cite{lambda-twist}.}
     \label{fig:teaser}
 \end{figure}

Minimal camera geometry problems often result in complex systems of 
polynomial
equations.
Efficient algebraic methods helped to solve many previously unsolved 
%minimal 
problems
%camera relative and absolute problems
~\cite{Stewenius-CVPR-2005,bujnak_cvpr2008,larsson2019revisiting,DBLP:conf/cvpr/KukelovaP07,kukelova2013real,Stewenius-ISPRS-2006}.
%Still, some problems
%result in equations for which state-of-the-art algebraic methods fail to generate a solver that is efficient and/or numerically stable.
Still, %for some problems, 
they fail to generate efficient and/or numerically stable solutions for some problems.
%
%
%
%
%
%
%non-minimal solvers - minimal + 1 or linear solvers with more correspondences (linear trifocal tensor, linear RS solver)
%
%new correspondences usually add linear equations and threfore to solve the problem we can drop some more complex equations describing the model.
%
In this paper, 
we study such challenging problems of estimating the relative pose of three cameras. These problems have received %attracted 
attention
for a long time~\cite{holt1995,Quan2006,leonardos_cvpr2015,Martyushev16,Aholt2014}. 
However, due to their complexity, they are still not considered fully solved. There are no efficient and practical solutions for most of the minimal configurations of point and/or line correspondences~\cite{Kileel2017}.
%, as well as for partially calibrated cameras.
One such configuration that is particularly interesting is the notoriously difficult configuration of four points in three views~\cite{Quan2006, Nister-5pt-PAMI-2004}.
%, known as the 4p3v problem.  
%for which an fully efficient and practical solution still does not exist.
This configuration is minimal for %three
cameras with an unknown shared focal length, \ie, the 4p3vf problem, and provides one more constraint than minimal for calibrated cameras, \ie, the 4p3v problem.








State-of-the-art algebraic and numerical methods are known to fail in generating efficient and numerically stable solutions to the 4p3v and 4p3vf problems.  
%
%The existing methods for solving the 4p3v problem for calibrated cameras are thus only approximate. 
The existing methods for solving these problems are either too slow for practical applications~\cite{Chien_2022_CVPR,Cin_2024_CVPR} or are only approximate~\cite{Hruby_cvpr2022,DBLP:journals/ijcv/NisterS06}.
By solving only for one (a few) solutions from the 272 solutions 
%of the %polynomial system 
of the 4p3v problem~\cite{Hruby_cvpr2022}, and by discretely sampling the space of potential solutions~\cite{DBLP:journals/ijcv/NisterS06}, the existing 4p3v methods can often fail, \ie, the returned solution can be, in general, arbitrarily far from the geometrically correct solution. To decrease the failure rate, both methods~\cite{Hruby_cvpr2022,DBLP:journals/ijcv/NisterS06} require a lot of tuning and are not easy to re-implement.\footnote{For the solver proposed in~\cite{DBLP:journals/ijcv/NisterS06}, there is no publicly available implementation. The publicly available implementation of the solver from~\cite{Hruby_cvpr2022} is quite complex and requires a non-negligible effort to run.}


In this paper, we propose a novel approach for solving 
%both 
the %calibrated 4p3v and the 4p3vf problem for cameras with an unknown shared focal length.
4p3v and 4p3vf problems. 
Our solutions are based on the simple idea of
generating new approximate point correspondence(s) between two of the three views.\footnote{Note that similarly to the state-of-the-art solutions~\cite{DBLP:journals/ijcv/NisterS06,Hruby_cvpr2022}, our solutions are only approximate. However, as we show in the experiments, they provide good initialization for local optimization~\cite{Chum-2003} %LO-RANSAC 
and outperform~\cite{Hruby_cvpr2022}.} Such approximate correspondences are generated using %an information purely from 
 only the locations 
 %of the four triplets 
 of the original input point correspondences, %. 
% Since t
% The generation of new points does not use %require 
without any information about the image itself (\eg, 
%information about 
appearance or local features). %, %features in the image), 
Consequently, the new correspondences %,  in general, 
do not need to correspond to any physical 3D points in the scene. 
Thus, we call them  \emph{virtual correspondences}. 
Using virtual correspondences, we can efficiently solve the 4p3v and 4p3vf problems by first estimating the relative pose of %the 
two 
cameras from five/six correspondences using efficient 5pt/6pt solvers~\cite{Nister-5pt-PAMI-2004,stewenius-etal-omnivis-2005}, and then registering the third camera using a P3P solver~\cite{lambda-twist}. We call these combinations  
%of the 5pt/6pt relative pose and the P3P solver 
the \sft and \sst solvers. 


Based on this idea, we propose a group of novel solvers for the 4p3v and 4p3vf problems. These solvers, called %we call them 
M-based solvers (\sftm,\sstm), use the mean points of three corresponding points detected in two views and, potentially, points in their vicinity ((M$\pm\delta$)-solvers) as new virtual point correspondences.
To compensate for noise in these virtual correspondences, our %these 
solvers refine the solutions on the original four points in three views using just a few iterations of Levenberg-Marquardt refinement.
%  (2) L-solvers (\sftl,\sstl): Solvers that, given four triplets of
% corresponding points in three views and a point in the first view, use a network to predict a corresponding point to this point
% in the second view, \ie, a virtual correspondence.
While conceptually very simple and efficient, the novel solvers achieve state-of-the-art results 
%for the 4p3v/4p3vf problems
on real data.


\noindent The contributions of the paper are as follows: 
\vspace{-1.3ex}
\begin{itemize}
\item For the well-known challenging 4p3v problem for calibrated cameras, 
%we propose two groups of novel solutions \sftm- and \sftl-based solvers. 
we propose novel %\sftm
M-based solvers.
These solvers generate an additional virtual point correspondence(s) in two views 
%by leveraging the locations of four input triplet correspondences. 
as the mean points of three corresponding points and refine the approximate solution on the original four points in three views.
The new solvers achieve state-of-the-art results in terms of accuracy on real data. Compared to state-of-the-art 4p3v solvers~\cite{Hruby_cvpr2022,DBLP:journals/ijcv/NisterS06}, which are non-trivial and difficult to re-implement such that they are numerically stable and fast, our new solutions
%, especially the \sftm-based solvers, 
can be easily implemented using existing efficient implementations of the 5pt solver~\cite{Nister-5pt-PAMI-2004} and the P3P solver~\cite{lambda-twist}. 
The source code of our solvers will be publicly available.
\vspace{-1.3ex}
\item We provide %first 
efficient solutions to the 4p3vf problem for cameras with an unknown shared focal length. Our novel %\sstm
%M-based 
solvers
% and \sstl-based solvers 
generate for each instance two virtual correspondences to solve the problem via the efficient 6pt~\cite{Nister-5pt-PAMI-2004} and the P3P~\cite{lambda-twist} solvers. 
Our solutions are significantly faster than the existing homotopy-continuation solutions~\cite{Chien_2022_CVPR,Cin_2024_CVPR}. 
%Our novel 
Our solvers show the potential of
%the proposed idea of 
%mean-point 
virtual
correspondences to be applied to other camera geometry problems.
\vspace{-1.8ex}
\item We present  preliminary results for a simple network that can improve the precision of the mean-point correspondences. 
While our current versions of the learning-based (L-based) solvers do %sufficiently improve the accuracy of the 
not provide sufficient improvement of 
virtual correspondences to produce a visible improvement after the refinement on all four correspondences, the proposed network shows the potential to learn better virtual point correspondences.
\vspace{-4.1ex}
\item To the best of our knowledge, we are the first to extensively evaluate solutions to the 4p3v and 4p3vf problems on a large variety of real-world scenes and within state-of-the-art RANSAC frameworks, and to compare them to the baseline minimal \sft and \sst solvers on such data. % We provide the first extensive evaluation of the state-of-the-art 4p3v solver~\cite{Hruby_cvpr2022}, the new proposed solvers, 
\end{itemize}
%The source code of our solvers will be publicly available.

\section{Related work}
Estimating the relative pose of three cameras from a minimal number of point and line  correspondences 
% or a combination of point and line correspondences 
is known as an extremely challenging problem. 

For three uncalibrated cameras, 6 point correspondences are necessary to estimate the trifocal tensor, with a solution known for a long time~\cite{Quan_pami95,Torr97a}. 
 Solutions to three minimal combinations of points and lines are presented in~\cite{Oskarsson_bmvc2004}. 
The minimal configuration using 9 lines is much more challenging and was solved only recently 
%by Larsson et al.
~\cite{larsson2017efficient}.  However, the final solver is far from practical
%, as it performs elimination of a huge $16k\times 13k$ matrix and 
since it runs for 17.8s.

For calibrated cameras, the configuration that attracts most of the attention is the configuration of four points in three views (the 4p3v problem). Note that this is not a minimal configuration since it generates 12 constraints for 11 degrees-of-freedom (DoF)
%, \ie, it is over-constrained %by one 
(see also Section~\ref{sec:solvers}).
% The four-points-in-tree-views problem (4p3v) 
The 4p3v problem is known to be extremely difficult to solve.
Several papers present mostly theoretical results~\cite{leonardos_cvpr2015,Martyushev16,Aholt2014}.
For four triplets of exact points without noise, it is shown that the 4p3v problem has, in general, a unique solution~\cite{holt1995,Quan2006}.

To the best of our knowledge, there are only two reasonably efficient solutions to the 4p3v problem reported in the literature.
The first solver~\cite{DBLP:journals/ijcv/NisterS06} is based on a one-dimensional exhaustive search.
%In the paper, the authors derive several interesting theoretical results and show that the four point correspondences between two calibrated views constrain the epipole in each image to lie on a curve of degree ten.
%The solver performs a one-dimensional sweep of the curve of possible epipoles. 
It performs a sweep of a $10^{th}$-degree curve of possible epipoles.
For each potential epipole, it computes the relative pose of two cameras, registers the third camera using three triangulated points, and finally extracts the solution minimizing the reprojection error of the fourth point in the third view. 
Evaluation of the solver on one potential epipole is fast.  
Yet, to obtain reasonable precise and stable results, usually, 1,000 candidates need to be evaluated and even then, %in such a case 
refinement at multiple local minima is required to improve the precision. The runtimes reported for this solver were $1-12ms$ depending on the number of points searched.
There is no publicly available implementation for this solver and it is not easy to re-implement. 
As such, the literature does not compare against the solver in experiments. 
% To obtain a
As an upper bound of the performance of~\cite{DBLP:journals/ijcv/NisterS06}, we compare with an oracle version using the true epipole in the supplementary material (SM). % is given. 

The second efficient solver to the 4p3v problem was published only recently~\cite{Hruby_cvpr2022}. In this paper, the authors first transform the 4p3v problem into a minimal problem by considering a line passing through the last correspondence in the third view.
%, \ie by dropping one constraint. 
The resulting system of equations is solved using an efficient Homotopy continuation (HC) method~\cite{Fabbri_CVPR2020,SommeseAndrewJ2005Tnso}. 
To avoid computing large numbers of spurious solutions, an MLP-based classifier is trained. For a given problem $p$, it selects one or several starting problem-solution pairs (so-called anchors), such that the geometrically meaningful/correct solution of $p$ can be obtained by HC starting from this anchor. This strategy is fast, running $16.3\mu s$ on average per solution. 
% At the same time
However, it has a high failure rate. The success rate of the 4p3v solver reported in~\cite{Hruby_cvpr2022} %the paper 
on two test %ing 
datasets and data without noise is $26.3\%$.  
%Unfortunately,
% The paper does
\cite{Hruby_cvpr2022} 
 do not show results %of their solver in 
for a real scenario, \ie, a RANSAC-like framework with noisy data. 
% We provide such an evaluation in this paper. 
Providing such an evaluation, we show that our much simpler solvers outperform~\cite{Hruby_cvpr2022}. 

% Simple s
Solutions to the 4p3v problem for orthographic and scaled orthographic views were presented in~\cite{xu_ortho4p3v,Higgins-4p3v91}. In~\cite{Higgins-4p3v91}, the author suggested an iterative approach for updating to perspective views, but  
% However, he 
reported results only on a few synthetic instances. % of points. 
According to our own %extensive 
experiments, the update does not work on real data with general perspective cameras, even after spending months on this issue.
% Even after spending a non-negligible amount of work %(adding different local optimizations) 
% and time (months) to make this update work, we were not able to get reasonable results on real data with general perspective cameras. % that are further from the scaled orthographic case.


Minimal configurations of points and lines in three calibrated views were  studied 
%from the theoretical perspective 
in~\cite{Duff_PL1P,Kileel2017,duff2019plmp}, aiming to classify %ing 
and derive %ing 
the number of solutions for different configurations. Solutions to two minimal configurations of combinations of points and lines (%, the so-called 
Chicago and Cleveland), % problems, 
were proposed in~\cite{Fabbri_CVPR2020} %. 
%These problems result in 216 respectively 312 solutions 
and solved using a HC %homotopy continuation 
%
method~\cite{SommeseAndrewJ2005Tnso}. 
Due to their complexity, 
% Taking over a second, 
the solvers are not practical, % in real applications, 
%with running times ranging from $0.66$s to $1.9$s.


Recently, the HC %homotopy continuation 
method was used to solve the 4p3vf problem for cameras with an unknown shared focal length~\cite{Chien_2022_CVPR,Cin_2024_CVPR}. 
The running times of the CPU variants of the proposed solvers range from $250ms$ to $1456ms$. 
Efficient GPU implementations run $16.7ms$ to $154ms$. These times are still too slow for practical applications. %Moreover, it is almost impossible to incorporate the fastest GPU-
Due to slow run-times and their dependency on a GPU, we are not comparing with the GPU solvers on real data.
A GPU HC method
was also used to solve minimal problems of four points/six lines in three views %and six lines in three views 
for a generalized 
%(non-central) 
camera in~\cite{Ding_2023_ICCV}.



In our solutions, we generate virtual correspondences. 
Virtual matches are also used in the literature on affine correspondences (AC) %\cite{Perdoch-CVPR2009efficient}. 
\cite{perd2006epipolar,pritts_ivcnz13,pritts-cvpr2018,barath2022relative}. 
There, points are sampled based on the affine feature geometry to generate point correspondences from affine ones. 
% The resulting point correspondences are then used as input to point correspondence-based solvers. 
In our scenario, we are only given point correspondences, without associated feature geometry, and we predict additional point matches. %are 
% However, they are employed to convert ACs to point correspondences. 
% In our case, 
%instead of reducing the DoF of the estimation problem as in our paper. 




\section{Estimating the relative pose of three cameras}
\label{sec:solvers}
Let
%us assume that we are given 
$N$ cameras observe a set of 3D points $\mathcal{P}$. 
%and 
For each point $\V{P} \in \mathcal{P}$, let $C_{\V{P}}>1$ be the number of cameras that observe it.
%Let us assume that we are given $N$ cameras observing 3D points $\V P_i$ and for each point $\V P_i$, let $C_{\V{P}_i}>1$ be the number of cameras that observe it.
A necessary condition for a relative pose problem of $N$ \textit{calibrated} cameras to be minimal is~\cite{Fabbri_CVPR2020}
\begin{equation}
\label{minimal_con}
     \sum_{\V{P} \in \mathcal{P}} (2C_{\V{P}}-3) =6N - 7 \enspace .
\end{equation}
Let $S_m$ denote a sample of $m$ 3D points and let $S_m^k$ denote a subset of $S_m$ with cardinality $k$.
A configuration of points in $N=3$ views that satisfies the constraint~\eqref{minimal_con} of a minimal problem is three points visible in all three cameras and two additional points visible in two of the three cameras.  We will call this configuration $\confm$.
%where the element on the position $i$ indicates which 3D points from the sampled points are visible in the camera $i$.
%, where the lower index on the position $i$ indicates how many from the image points sampled in the camera $i$ are visible in all three cameras.
%We will call this configuration %[(2,3),(2,3),(0,3)] $(5_3,5_3,3_3)$

The configuration of four points visible in all three cameras, \ie, the configuration $\confc$, generates an over-constrained problem. In this case, we have one more constraint than DoF, \ie, in Eq.~\eqref{minimal_con}, we have $12 > 11$. 
% This means that,
% in general, four point triplets cannot be realized as the projections of four common world points into three calibrated cameras. 
A minimal solution would %have needed 
need to drop one constraint, \eg, %e.g., 
by considering only a line passing through one of the points in the third view~\cite{Hruby_cvpr2022} or by considering a ``half" point correspondence.
 Since, in practice, we always have full correspondences and sampling one less point in one view leads to an under-constrained problem, the configuration $\confc$, is usually considered %a 
 ``minimal".
 %\footnote{Sampling one less point in one view would have led to an under-constrained problem}.

For cameras with an unknown common focal length, we have one more DoF. This means that, for $N=3$, the right-hand side of equation~\eqref{minimal_con} 
% equals 
becomes $18 - 6=12$, resulting in $\confpm$ and $\confc$ being minimal configurations. %for the 4p3vf problem.



\subsection{Calibrated cameras}
\label{sec:calibrated}
In this section, we describe solutions for three calibrated cameras. 
We start with one baseline minimal solution for the minimal $\confm$ configuration,  followed by novel solutions for the ``minimal" $\confc$ configuration.

\PAR{5pt+P3P solver:}
The \sft  solver first estimates the relative pose of two cameras from 5 image point correspondences using the efficient 5pt solver~\cite{Nister-5pt-PAMI-2004}. 
Next, the three points %that are 
visible in all three views are triangulated. Finally, the third camera is registered using the three 2D-3D point correspondences and the well-known efficient P3P solver~\cite{lambda-twist}. 
 
 This solver is straightforward and it is based on efficient existing solvers~\cite{Nister-5pt-PAMI-2004,lambda-twist}.
 This solver appears in the literature~\cite{Duff_PL1P,Nister-5pt-PAMI-2004,Rodehorst2017,DBLP:journals/ijcv/NisterS06}. Nister \etal~\cite{DBLP:journals/ijcv/NisterS06} showed that the \sft solver performs better than their dedicated 4p3v solver.
% Still, the \sft solver is not even used as a baseline for a comparison in papers that study the relative pose problem for three calibrated cameras~\cite{Fabbri_CVPR2020,Hruby_cvpr2022}. 
However, in the most recent works~\cite{Fabbri_CVPR2020,Hruby_cvpr2022} that study the relative pose problem for three calibrated cameras, the \sft solver is not discussed and is not used as a baseline for comparison.  
 To our knowledge, the performance of this solver on real data and within state-of-the-art RANSAC frameworks in the context of the 4p3v problem has not been extensively studied. % in the computer vision literature. 
 Our paper fills this gap in the literature.

 Motivated by the efficient \sft solver and the minimal $\confm$ configuration, which, compared to the $\confc$ configuration, leads to significantly simpler systems of polynomial equations,
 %that can be solved efficiently, 
 we next describe 
 %two groups of 
 novel solvers to the calibrated 4p3v problem. 
 % These solvers 
 They efficiently solve the $\confc$ configuration by generating a virtual point correspondence in the first two views, and 
  %solving it 
 solving the resulting 
 %4p3v 
 $\confm$ problem using the \sft solver.
 %and transforming it to $(5_3,5_3,3_3)$ configuration.

 \PAR{4p3v(M) solver:}
 Our first solver is based on a simple observation: 
If we fix the $5^{\text{th}}$ point in the first view to be the mean $\V m^1$ of three points $\left\{\V x_i^1,\V x_j^1,\V x_k^1\right\}, \; i,j,k \in \left\{1,\dots, 4\right\}$, in this view, then the mean point $\V m^2$ of the corresponding three points $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$ in the second view usually has a small epipolar error \wrt the ground truth relative pose. 
Thus 
%is, the mean of three points 
$\V m^1 \leftrightarrow \V m^2$
is usually a good approximation of a correct correspondence.


This can be considered a surprising observation, %fact, 
since 4 (or actually 3) points in two views define an infinite number of camera poses that can observe these points. However, the reason this mean-point strategy works in practice comes from several simple facts and observations. (1)  To generate a good correspondence, we only require that the point in the second view be reasonably close to the epipolar line defined by the mean point $\V m^1$ in the first view, \ie, the 2D point does not need to correspond to one particular 3D point with a given depth.\footnote{By fixing a point in one view, we are defining an epipolar line %(an epipolar line) 
in the second view. Any of the points on this line (corresponding to 3D points with different depths) is in  correspondence with the %given 
point in the first view.} 
(2) The epipolar line defined by 
%the mean point 
$\V m^1$ 
%of three points $\left\{\V x_i^1,\V x_j^1,\V x_k^1\right\}$ in the first image 
passes through the triangle defined by the corresponding three points $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$ in the second image. 
% Therefore, 
Thus, the maximum distance of $\V m^2$ in the second image from the epipolar line is bounded by the maximum distance of $\V m^2$ from $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$ (for a proof, see SM).
(3) For practical applications, when used in RANSAC, it is not necessary that each triplet $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$ generates a good correspondence $\V m^1 \leftrightarrow \V m^2$. 
Samples with a high level of noise in the mean-point correspondence are filtered inside RANSAC.\footnote{%Note that this property of RANSAC 
This property was also used in the %homotopy-continuation 
HC solver~\cite{Hruby_cvpr2022}, which completely fails for many samples. %However, t
These samples are filtered within RANSAC.} 
On a large number of different scenes, we observed that even if some image pairs have triplets of points that generate very noisy mean-point correspondences, there are usually enough triplets for which the noise in $\V m^2$ is reasonably small to provide good estimates. 
(4) Four point correspondences in two views usually fix the space of possible poses such that the $5^{th}$ correspondence, even if noisy, 
%one, 
often generates  a pose that is not very far from the ground truth pose. 
Such a pose is usually sufficient as an initialization of non-linear optimization on the original four points in three views and subsequent local optimization on detected inliers.
We support our observations by experiments on a large amount of synthetic and real data (see Sec.~\ref{sec:experiments}
%, mean-point strategy
and SM). %\ref{sec:mean-point}). 

Motivated by these observations, our \sftm solver uses the mean points of three corresponding points detected in two views as a new $5^\text{th}$ point correspondence. The 4p3v problem is then solved using the \sft solver.

 \PAR{4p3v(M$\pm \delta$) solver:}
While the mean point correspondence used in the \sftm solver can provide a good approximation of a correct correspondence, % such a correspondence can be noisy.
as mentioned, it can also be noisy.
In the \sftmd solver, we thus, in addition to the mean point $\V m^2 = \left[x,y\right]$ of 
%the triangle $\mathcal{T_2}$] 
three points $\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$
in the second image, generate two additional points. These points are (1) $\V {m}^2_{\pm\delta} = \left[x\pm\delta,y\right]$ if the longest dimension of the triangle  $\mathcal{T}^2 = \Delta\left\{\V x_i^2,\V x_j^2,\V x_k^2\right\}$ is in the x-direction or (2) $\V {m}^2_{\pm\delta} = \left[x,y \pm\delta\right]$ if it is in the y-direction.
All three points, \ie, $\V m^2$, $\V m^2_{-\delta}$, and $\V m^2_{+\delta}$ are placed in  correspondence with the mean point $\V m^1$.
%in the first image. 
The \sftmd solver in the first step calls the \sfc solver~\cite{Nister-5pt-PAMI-2004} three times, with the $5^{th}$ correspondence being either $\V m^1 \leftrightarrow \V m^2$, $\V m^1 \leftrightarrow \V m^2_{-\delta}$, or $\V m^1 \leftrightarrow \V m^2_{+\delta}$. The results of these three \sfc solvers are collected to create hypotheses for the relative pose of the first two cameras inside RANSAC. The shift $\delta$ is selected relative to the size of the triangle $\mathcal{T}^2$. %\TODO{adjust} We select the size of the shift $\delta$ as the shift resulting in the best results on our validation dataset (\cf Sec.~\ref{sec:experiments}). 


\PAR{$\mathbf{4^{th}}$ point in the third view:} 
The 
\sftm and \sftmd solvers are actually solving the configuration $\conft$, \ie, they are not using information from the $4^{th}$ point in third view, \ie,  point $\V{x}^3_4$. 
The information from %for 
%the $4^{th}$ point in third view 
$\V{x}^3_4$
 can be used in the solver for the $\confc$ configuration in two different ways: 
 \textbf{ Filtering (}\texttt{+F}\textbf{):} $\V{x}^3_4$ can be used to filter out geometrically infeasible solutions returned by the \sft solver that is used inside M-based solvers. 
 The \sft solver returns multiple solutions that can be evaluated \wrt $\V{x}^3_4$. 
 Since the returned solutions can be affected by larger noise in the mean-point correspondence, we do not simply select the solution with the smallest error on $\V{x}^3_4$, but we keep all solutions that have an  epipolar error on $\V{x}^3_4$ smaller than twice the threshold used inside RANSAC. 
 % In our experiments, we 
 Our experiments show that this filtering can improve the speed of the proposed solvers. 
 However, as a trade-off, there is a small drop in the precision of the solvers since, in some cases, geometrically correct solutions are filtered. 
 \textbf{Refinement (}\texttt{+R}\textbf{):} $\V{x}^3_4$ can be used to refine the solutions returned by the \sft solver used inside M-based solvers.
 % Instead of having 
 These solutions %that 
 have zero error on the original four points and the mean-point correspondence in the first two views, but can %and can generally 
 have a large error on $\V{x}^3_4$. 
 Rather, we want solutions that minimize the epipolar errors on the original four points in the three views. Note that $\confc$ is an overconstrained configuration and thus for noisy data, there is, in general, no solution with zero error on all four points in the three views. 
 We perform refinement of the poses by minimizing the epipolar errors of all original four points in three views using %the 
 Levengerg-Marquardt optimization (LM), initialized using the solutions from the M-point solvers. 
 % In experiments, we tested 
 Experiments with different numbers of iterations %and observed 
 show that two iterations are usually sufficient to obtain an improvement %for the 4p3v problem 
 (see SM).
 



\PAR{4p3v(L) solvers:}
We also experiment with learning-based \sftl solvers, %in 
which, instead of using the mean point correspondence,  %we 
use a neural network to predict the virtual correspondence.
In the network, we want to directly use the information from all four points in three views. 
We use the fact that four triplets of points, in general, define a unique relative pose of three calibrated cameras. 
We train a network that, given such four 
%triplets of 
corresponding points in three views and a fixed 5$^\text{th}$ point in the first view, %it 
predicts a corresponding 5$^\text{th}$ 
point
%its corresponding point 
in the second view.
% In our network, w
We fix the 5$^\text{th}$ point to the mean point $\V m^1$ as also defined in the M-based solvers. The network actually learns a shift of the mean point $\V m^2$ in the second view to be in correspondence with $\V m^1$. 
We use a lightweight architecture with a backbone of shared MLP layers, similar to \cite{cavalli2022nefsac}, and the Sampson error as a loss function. 
%The details 
Details on the architecture, loss function, and training data are in the SM. 
\sftld solvers can be defined similarly to \sftmd solvers (see SM). 

% In experiments, we 
Our experiments show that the proposed network can, in general, improve the precision of the mean-point correspondence $\V m^1 \leftrightarrow \V m^2$, resulting in better performance than the baseline \sftm. 
However, as shown in Sec.~\ref{sec:experiments}, %we will show in experiments, 
after adding the refinement (\texttt{+R}), the difference between the \sftmR and \sftlR solvers is negligible. 
Still, the proposed network can be seen as a first step towards a method that can learn better virtual point correspondences.

\subsection{Partially calibrated cameras}
To show the potential of the proposed mean-point strategy, we applied this idea to the very challenging \sstp problem of estimating the relative pose of three cameras with an unknown shared focal length from four correspondences. 
Our novel solvers for the \sstp problem  
%of estimating the relative pose of three cameras with an unknown shared focal length from four correspondences 
follow the %idea 
approach of generating virtual correspondences used in our \sftp solvers for calibrated cameras. 
% In this case
% Similarly, t
The idea is to transform the extremely complex \sstp problem into the problem solved by the efficient \sst solver. %problem for which efficient solutions exist.
The \sst solver first estimates the unknown focal length and the relative pose of the first two cameras using the efficient 6pt solver~\cite{stewenius-etal-omnivis-2005} and then registers the third camera using the P3P solver~\cite{lambda-twist}.
%
This means that, in contrast to the \sftp solvers presented in Section~\ref{sec:calibrated} that generate only one virtual correspondence (+ potentially additional shifted versions of this correspondence), %in  
our novel \sstp solvers %, we 
need to 
generate two %new 
virtual correspondences to obtain six correspondences in the first two views.

In the \sstm solver, we generate the two new virtual correspondences using the mean points of two different triplets of corresponding points in the first two cameras. 
Similarly to the calibrated case, we also test %the 
\sstl and (\texttt{+R}), (\texttt{+F}), and  $\delta$-based variants of \sstp solvers.
More details on all \sstp solvers can be found in the SM.   

\section{Experiments}
\label{sec:experiments}
% \PAR{Experimental setup.} 
We extensively evaluated the proposed solvers on a large variety of synthetic and real data to test their robustness to noise and outliers and assess their performance inside state-of-the-art RANSAC-frameworks~\cite{barath2017graph, PoseLib}. 
%
We compare our novel solvers with the homotopy continuation \sfhc solver ~\cite{Hruby_cvpr2022} and the \sft and \sst baseline minimal solvers for the $\confm$ and $\confpm$ configurations for calibrated cameras respectively cameras with an unknown shared focal length. To obtain upper bounds for the precision that can be achieved by our proposed solvers, we also consider  `Oracle' versions (denoted (\texttt{O})), where we use correct correspondence(s), \ie, correspondences that satisfy the epipolar constraint for the ground truth relative pose of the first two cameras, as the $5^\text{th}$/$6^\text{th}$ virtual correspondence between these cameras. 
% Due to the absence of 
Without publicly available code for the %implementation of the 
\sfep solver~% from~
\cite{DBLP:journals/ijcv/NisterS06}, we tested only its `Oracle' version.
%, in which   
%In this \sfepo solver, 
Instead of doing a one-dimensional search over the $10^{\text{th}}$ degree curve of possible epipoles, we use %provide the solver with 
the ground truth epipole. 
Since this solver performs worse than our `Oracles', % solvers, %and thus 
we report it only in the SM. 


\PAR{Experimental setup.} 
To obtain feature correspondences, we use SuperPoint~\cite{detone2018superpoint} features with the  LightGlue~\cite{lindenberger2023lightglue} matcher. 
% Before matching we kept 
We extract at most 2048 features per image. 
We perform matching for all three image pairs and keep only those matches that were consistently matched across all three views. 
%
We performed evaluation within two RANSAC frameworks: PoseLib~\cite{PoseLib} and GC-RANSAC~\cite{barath2017graph}. For the \ss solver, %relative pose problem 
we use~\cite{Nister-5pt-PAMI-2004} and for the \sp solver, %problem 
we use~\cite{lambda-twist}. 
In PoseLib, we perform LO~\cite{Chum-2003} using LM optimization. In GC-RANSAC we perform LO using non-minimal solvers for fitting models to larger-than-minimal samples. 
We tested different shifts for our~$\delta$-based solvers (for the ablation study, see SM) and selected $\delta = 0.04*\texttt{(longest triangle dim.)}$.

\PAR{Evaluation measures.} Inspired by~\cite{IMC2020}, we define the %evaluate the results using the 
pose error %defined 
as $\text{max}\left(0.5 (\M R_{err}^{12} + \M R_{err}^{13}), 0.5 (\V t_{err}^{12} + \V t_{err}^{13})\right)$, where $\M R_{err}^{ij}$ and $\V t_{err}^{ij}$ are the angular errors of rotation and translation for pair $ij$ in degrees~\cite{IMC2020}. We also report AUC values~\cite{IMC2020} at different thresholds for the pose error. 
% The SM shows %
We include 
results for an alternative pose error definition which includes $\M R_{err}^{23}$ and $\V t_{err}^{23}$ %. %$R_{23}$ and $\vec{t}_{23}$ 
in the SM.


 \begin{figure*}[t!]
    \centering
	%\includegraphics[width=0.593\columnwidth]{assets/tests_distance_5.pdf}\hfill
	%\includegraphics[width=0.593\columnwidth]{assets/tests_distance_10.pdf}\hfill
	%\includegraphics[width=0.593\columnwidth]{assets/tests_distance_20.pdf}\phantom{xxx}\\[0.3cm] 
    \includegraphics[width=0.21\textwidth]{figures/bary_4p/sacre_coeur_bary_sed.png}
    \includegraphics[width=0.21\textwidth]{figures/bary_4p/sacre_coeur_bary_rotation.png}
    \includegraphics[width=0.21\textwidth]{figures/bary_4p/sacre_coeur_bary_translation.png}
    \includegraphics[width=0.21\textwidth]{figures/bary_4p/sacre_coeur_bary_inlier.png}

\caption{Left to right: Distribution of the average symmetric epipolar error (0.3319,    0.3308); rotation error (0.3373,    0.3347); translation error (0.3325,    0.3382); and percentage of inliers gathered (0.3377,    0.3354), as a function of the barycentric coordinates of the triangle in the second image \wrt the mean point of the corresponding triangle in the first image %, for (a) Shop Facade from the Cambridge Landmark dataset and (b,c,d) 
on 465k four-tuples of correspondences from scene Sacre Coeur from the PhotoTourism dataset~\cite{IMC2020}. For each metric, we fit a 2D Gaussian distribution and report the mean in brackets.}
\label{fig:mean_tests}
\end{figure*}

% \subsection{Mean-point strategy}
% \label{sec:mean-point}
\PAR{Mean-point strategy.}
The first experiments aim to support our idea of selecting a virtual point correspondence as the mean points of three corresponding points in two images.
%We validate this strategy on synthetic and real data.

\begin{table}
    \resizebox{0.95\columnwidth}{!}{\begin{tabular}{c | c c c}
    Scene & AVG ($^\circ$) & MED ($^\circ$) & $20^{th}$ perc. ($^\circ$)\\
    \midrule
    Brandenburg Gate & 23.1 $\pm$ 20.9 / 19.5 $\pm$ 22.9 & 18.0 / 12.5 & \phantom{1}8.9 / 4.7 \\
Buckingham Palace & 25.7 $\pm$ 22.3 / 22.2 $\pm$ 23.4  & 19.4 / 14.8 & \phantom{1}9.0 / 5.7\\
Colosseum Exterior & 27.5 $\pm$ 22.3 / 20.9 $\pm$ 25.2 & 22.4 / 12.4 & 10.4 / 4.1 \\
Grand Place Brussels & 25.3 $\pm$ 22.0 / 21.7 $\pm$ 23.5 & 19.5 / 15.1 & \phantom{1}9.4 / 5.9\\
Notre Dame Front Facade & 27.9 $\pm$ 23.0 / 20.2 $\pm$ 26.5 & 23.2 / 12.0 & 11.5 / 4.2 \\
Palace of Westminster & 22.5 $\pm$ 21.8 / 19.2 $\pm$ 24.3 & 16.7 / 11.2 & \phantom{1}7.1 / 2.7 \\
Pantheon Exterior & 28.8 $\pm$ 21.2 / 24.5 $\pm$ 21.9 & 24.3 / 19.0  & 13.0 / 7.9\\
Reichstag & 18.5 $\pm$ 23.2 / 17.2 $\pm$ 25.3 & 12.2 / \phantom{1}9.6 & \phantom{1}5.5 / 3.6\\
Sacre Coeur & 23.8 $\pm$ 23.6 / 17.1 $\pm$ 25.3 & 17.1 / \phantom{1}8.1 & \phantom{1}7.5 / 2.2 \\
St Peters Square & 22.8 $\pm$ 21.0 / 21.1 $\pm$ 24.2 & 17.3 / 14.1 & \phantom{1}8.7 / 6.0 \\
Taj Mahal & 18.1 $\pm$ 24.7 / 15.7 $\pm$ 23.7 & 10.5 / \phantom{1}7.8 & \phantom{1}4.4 / 2.4 \\
Temple Nara Japan & 27.0 $\pm$ 25.1 / 23.6 $\pm$ 26.8 & 20.7 / 15.4 & \phantom{1}9.9 / 5.7 \\
Trevi Fountain & 30.3 $\pm$ 22.7 / 20.7 $\pm$ 23.5 & 25.9 / 12.3 & 12.0 / 4.2\\
    \end{tabular}}
    \caption{Comparing the accuracy of the \texttt{4pt(M)} / \texttt{5pt} solvers.}
    \label{tab:4ptM_vs_5pt}
\end{table}

The accuracy of the mean point correspondence depends on a large number of variables, including the depths of the points \wrt the cameras, the angle under which the triangle formed by the three points is observed, the shape and size of the triangle, the type of motion,  \etc 
A detailed analysis of all these factors, \eg, through synthetic experiments, is out of the scope of this paper. 
We thus study the accuracy of the mean point correspondences, and of the resulting estimated poses on real-world data. 
% Note that w
We only consider pairs instead of triplets as we create virtual correspondences in two views. 

For the following experiments, we sample 100 four-tuples of point correspondences, obtained as SuperPoint+LightGlue matches consistent with the ground truth relative pose, \ie inliers, for each image pair in scenes from the PhotoTourism dataset~\cite{IMC2020}. 
We use the first three correspondences to define the triangles in both images. 

In our first experiment, we establish correspondences between the mean of the triangle in one image and various points in the triangle in the second image. 
We express points in the second triangle via their barycentric coordinates and uniformly sample $19 \times 19$ barycentric coordinates $(a,b)\in [0,1]^2$, such that $a+b \leq 1$ (ensuring 
points inside the triangle). 
The %third barycentric 
3rd coordinate is given as $c = 1-a-b$. 
For each correspondence, we measure: 
The symmetric epipolar error \wrt the ground truth %relative 
pose, %the 
translation and rotation errors, and the percentage of inliers consistent with the pose obtained with the \texttt{5pt} solver applied on the virtual and the four real correspondences. 
Note that we are thus considering a 4-point-relative-pose problem. 

Fig.~\ref{fig:mean_tests} shows the results of this experiment on scene Sacre Coeur. 
It can be seen that the optimum of studied metrics is reached around the mean point of the triangles. To suppress the effect of discrete sampling, 
%of points, 
for each metric, we fit a 2D Gaussian distribution and report the mean value (in barycentric coordinates) as numbers in brackets in the caption of the figure. 
%The mean values, \ie points where the minimum or maximum of the given metric is reached, 
The mean values of the 2D Gaussians
are very close to the mean point of the triangles, which has barycentric coordinates $(0.\bar{3}, 0.\bar{3})$. 
This clearly validates our approach of using mean point correspondences. The results for more real scenes and synthetic scenes are in the SM. For all these scenes, we observed a similar behavior.



In our second experiment, we establish the virtual correspondence between the mean points of the triangles. 
We compare the accuracy of the relative poses obtained by applying the \texttt{5pt} solver on one virtual and four real correspondences (denoted as the \texttt{4pt(M)} solver) with the accuracy obtained by the \texttt{5pt} solver on five real correspondences. 
Tab.~\ref{tab:4ptM_vs_5pt} shows the results of this comparison. 
As can be seen, the \texttt{4pt(M)} solver is not as accurate as the \texttt{5pt} solver, which is not surprising given that 
% (without refinement) achieves a similar mean pose error, even though 
the virtual correspondence is inherently noisier than the 5th real correspondence used by the \texttt{5pt} solver. 
While there is a large gap on some scenes (Collosseum, Notre Dame, Trevi), the gap is noticeably smaller on others (Reichstag, St. Peters), showing that the performance of our solver is scene-dependent. 
Overall, the gap is not too large, showing that the idea of using a virtual mean-point correspondence is viable. 
Further, note that the solvers are applied outside of RANSAC and that local optimization inside RANSAC usually compensates for less accurate pose estimates. 
 
\begin{figure*}
    \centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture} 

        \begin{axis}[%
        hide axis, xmin=0,xmax=0,ymin=0,ymax=0,
        legend style={draw=white!15!white, 
        line width = 1pt,
        legend  columns =9, % comment for column display
        /tikz/every even column/.append style={column sep=0.1cm},
        }
        ]
        
        \addlegendimage{Seaborn1}
        \addlegendentry{\scriptsize{\sftm}};
        \addlegendimage{Seaborn2}
        \addlegendentry{\scriptsize{\sftmR}};
        \addlegendimage{Seaborn3}
        \addlegendentry{\scriptsize{\sftmRC}};
        \addlegendimage{Seaborn4}
        \addlegendentry{\scriptsize{\sftmd}}; 
        \addlegendimage{Seaborn5}
        \addlegendentry{\scriptsize{\sftmdR}};   
        \addlegendimage{Seaborn6}
        \addlegendentry{\scriptsize{\sftmdRC}};

        
        \addlegendimage{Seaborn7}
        \addlegendentry{\scriptsize{\sfhc}};
        \addlegendimage{Seaborn10}
        \addlegendentry{\scriptsize{\sft}};
        \addlegendimage{Seaborn9,dash pattern=on 4pt off 2pt on 4pt off 2pt}
        \addlegendentry{\scriptsize{\sftoRC}};        
        % \addlegendimage{white}
        % \addlegendentry{~};
        \end{axis}
    \end{tikzpicture}}
    
    \begin{subfigure}{0.22\textwidth}
    \includegraphics[width=\textwidth]{figures/poselib_graphs/pt_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{Phototourism~\cite{IMC2020}}
    \label{fig:poselib_g_pt_all}
    \end{subfigure}
    \hfill    
    \begin{subfigure}{0.22\textwidth}
    \includegraphics[width=\textwidth]{figures/poselib_graphs/cambridge_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{Cambridge Landmarks~\cite{kendall2015cambridge}}
    \label{fig:poselib_g_cl_all}
    \end{subfigure}  
    \hfill
    \begin{subfigure}{0.22\textwidth}
    \includegraphics[width=\textwidth]{figures/poselib_graphs/pt_reichstag_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}    
    \caption{\textit{Reichstag}~\cite{IMC2020}}
    \label{fig:poselib_g_reichstag}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
    \includegraphics[width=\textwidth]{figures/poselib_graphs/cambridge_KingsCollege_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}
    \caption{\textit{King's College}~\cite{kendall2015cambridge}}
    \label{fig:poselib_g_kc}
    \end{subfigure}
    
    \caption{Speed-accuracy trade-off on (a) all scenes from Phototourism~\cite{IMC2020} except \textit{St. Peter's Square}, (b) 5  Cambridge Landmarks~\cite{kendall2015cambridge} scenes, (c) the \textit{Reichstag} scene from \cite{IMC2020}, and (d)  the \textit{King's College} scene from \cite{kendall2015cambridge}. 
    We report the AUC@10$^\circ$ of the pose error and vary the number of Poselib RANSAC iterations (100, 200, 500, 1000, 2000, 5000, 10000). Runtimes are averaged over all image triplets.}% Results are shown fo all scenes in Phototourism~\cite{IMC2020} except \textit{St. Peter's Square} (a), all scenes in Cambridge Landmarks~\cite{kendall2015cambridge} (b), \textit{Reichstag} scene from \cite{IMC2020} (c) and \textit{King's College} scene from \cite{kendall2015cambridge} (d).}
    \label{fig:poselib_graphs}
\end{figure*}


\begin{figure}
    \centering
    \begin{subfigure}{0.22\textwidth}
    \includegraphics[width=\textwidth]{figures/poselib_graphs/pt_brandenburg_gate_graph-triplets-features_superpoint_noresize_2048-LG_pose.pdf}        
    \caption{original matches}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
    \includegraphics[width=\textwidth]{figures/poselib_graphs/pt_brandenburg_gate_graph-0.4inliers-triplets-features_superpoint_noresize_2048-LG_pose.pdf}        
    \caption{w/ synth. outliers}
    \label{fig:poselib_outlier_synth}
    \end{subfigure}
    \caption{Results on \textit{Brandenburg Gate}. %scene from the Phototourism dataset~\cite{IMC2020}. 
    We show results using (a) only original triplet matches and (b) adding synthetic outliers to reach a 40\% inlier ratio. The legend is provided in Fig.~\ref{fig:poselib_graphs}.}
    \label{fig:poselib_outlier}
\end{figure}

\PAR{Noise experiments.}
We next test the accuracy of our solvers and the state-of-the-art algorithms \wrt increasing image noise. 
% We use the Structure-from-Motion (SfM) model of the botanical garden scene %(randomly picked from all scenes) 
% from the ETH3D dataset~\cite{Schops_2017_CVPR} to obtain instances of points in three views. 
% Perfect correspondences with no noise are generated by projecting the 3D points into the images. 
% We then add increasing amounts of normally distributed noise to these correspondences. 
We establish correspondences by projecting 3D points into the images and then add increasing amounts of normally distributed noise to the projections. 
% Due to the space restrictions, the results of this experiment are presented in the SM. % supp. material.
%Let $e(\M R_{ij})$  be the error of the estimated relative rotation  between cameras $i$ and $j$, computed as the angle in the axis-angle representation of $\M R_{ij}^{-1}\M R_{ij}^{\M{GT}}$ and let $e(\M t_{ij})$ be the error of the estimated translation computed as the angle between the two unit vectors corresponding to the translations. 
% Here we just summarize the main observations from this experiment:
Due to the approximate nature of our virtual correspondences, our novel solvers return non-zero errors for zero noise. However, at noise levels $\geq 2px$, these solvers return comparable or even better results than the \sft/\sst solvers. This again shows that our predicted virtual correspondences are good approximations of real correspondences.
The recent state-of-the-art HC solver~\cite{Hruby_cvpr2022} is failing in about 50\% of the instances for noiseless data. %, even though the solver was trained on the ETH3D dataset. 
Its median errors are thus significantly larger than those %the median errors 
of the other solvers. %remaining solvers. 
See the SM for detailed results of the experiment. 

\PAR{Experiments on real data.} 
We test the solvers on all scenes from the PhotoTourism dataset~\cite{snavely2006photo, IMC2020} which provide ground truth poses and intrinsics via a COLMAP~\cite{Schoenberger2016CVPR} reconstruction. 
In the results, we do not include the \textit{St.~Peter's Square} scene that we used for the validation of $\delta$ and the number of refinement iterations (see SM). We also include results for the Cambridge Landmarks dataset~\cite{kendall2015cambridge} (except the Street scene, which is commonly not used due to issues with its ground truth). 
For PhotoTourism, we use the images in their original resolution. For Cambridge Landmarks, we resize the images so that the larger side is 800~px. 
For each scene, we sample 5,000 random image triplets with at least 10 matches and with at least %satisfy the 
$10\%$ overlap~\cite{IMC2020}. % condition~\cite{IMC2020}. 

\begin{table}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{
\begin{tabular}{ l | c c | c c c | c}
    \toprule
    \multicolumn{7}{c}{Phototourism~\cite{IMC2020}} \\
    \midrule
    Estimator & AVG $(^\circ)$ $\downarrow$ & MED $(^\circ)$ $\downarrow$ & AUC@5 $\uparrow$ & @10 $\uparrow$ & @20 $\uparrow$ & Runtime (ms) $\downarrow$\\
    \midrule
\sfhc~\cite{Hruby_cvpr2022} & 7.17 & 2.34 & 52.74 & 66.63 & 77.86 & \phantom{1}76.45 \\ \hline
\sft & \underline{5.99} & 2.00 & 57.31 & \underline{70.54} & \underline{80.81} & 105.50 \\
\midrule
\sftm & 7.17 & 2.49 & 50.96 & 65.46 & 77.32 & \phantom{1}76.77 \\
\sftmR & 6.39 & 2.00 & 56.92 & 69.90 & 80.17 & \phantom{1}\underline{74.94} \\ 
\sftmRC & 6.59 & 2.07 & 55.90 & 69.06 & 79.56 & \phantom{1}\textbf{30.54} \\ \hline
\sftmd & 6.39 & 2.19 & 54.70 & 68.58 & 79.59 & 172.06 \\
\sftmdR & \textbf{5.97} & \textbf{1.89} & \textbf{58.65} & \textbf{71.43} & \textbf{81.35} & 189.21 \\
\sftmdRC & 6.15 & \underline{1.97} & \underline{57.42} & 70.47 & 80.69 & \phantom{1}75.78 \\ \hline
\sftl & 7.12 & 2.50 & 51.00 & 65.57 & 77.42 & 376.31 \\
\sftlR & 6.35 & 2.00 & 56.88 & 69.88 & 80.15 & 297.88 \\
% \sftlidR & \underline{5.97} & \underline{1.89} & \underline{58.65} & \underline{71.43} & \underline{81.35} & 189.24 \\
\midrule
\sfto & 5.82 & 1.90 & 58.91 & 71.84 & 81.70 & \phantom{1}58.22 \\
\sftoR & 5.73 & 1.80 & 60.23 & 72.75 & 82.21 & \phantom{1}86.30 \\
\sftoRC & 5.72 & 1.82 & 59.97 & 72.58 & 82.13 & \phantom{1}36.36 \\
\midrule

\multicolumn{7}{c}{Cambridge Landmarks~\cite{kendall2015cambridge}} \\
\midrule
    % Estimator & AVG $(^\circ)$ $\downarrow$ & MED $(^\circ)$ $\downarrow$ & AUC@5 $\uparrow$ & @10 $\uparrow$ & @20 $\uparrow$ & Runtime (ms) $\downarrow$\\
    % \midrule
\sfhc~\cite{Hruby_cvpr2022} & 9.69 & 3.31 & 40.96 & 58.84 & 72.83 & \phantom{1}64.49 \\ \hline
\sft & \textbf{8.16} & \underline{3.05} & \underline{43.79} & \underline{61.61} & \underline{75.30} & 
\phantom{1}48.53 \\
\midrule
\sftm & 9.61 & 3.42 & 39.71 & 58.08 & 72.54 & \phantom{1}38.33 \\
\sftmR & 8.77 & 3.11 & 42.98 & 60.90 & 74.59 & \phantom{1}\underline{34.84} \\
\sftmRC & 9.03 & 3.17 & 42.31 & 60.18 & 74.03 & \phantom{1}\textbf{16.51} \\ \hline
\sftmd & 8.75 & 3.21 & 42.11 & 60.37 & 74.38 & \phantom{1}81.21 \\
\sftmdR & \underline{8.32} & \textbf{3.01} & \textbf{44.17} & \textbf{62.04} & \textbf{75.60} & \phantom{1}84.98 \\
\sftmdRC & 8.47 & 3.08 & 43.21 & 61.22 & 75.03 & \phantom{1}38.45 \\ \hline
\sftl & 9.58 & 3.44 & 39.79 & 58.17 & 72.62 & 232.13 \\
\sftlR & 8.75 & 3.09 & 42.94 & 60.86 & 74.60 & 177.06 \\
% \sftlidR & 8.32 & \underline{3.01} & \underline{44.17} & \underline{62.04} & \underline{75.60} & \phantom{1}84.94 \\
\midrule
\sfto & 8.62 & 3.07 & 43.58 & 61.65 & 75.30 & \phantom{1}30.43 \\
\sftoR & 8.39 & 2.94 & 44.80 & 62.54 & 75.93 & \phantom{1}34.68 \\
\sftoRC & 8.36 & 2.97 & 44.64 & 62.38 & 75.82 & \phantom{1}16.41 \\
\midrule

    \end{tabular}}
    \caption{Results for different solvers implemented in the PoseLib framework~\cite{PoseLib} on all scenes from the PhotoTourism~\cite{IMC2020} and 5 scenes from the Cambridge Landmarks~\cite{kendall2015cambridge} datasets. 
    %Results are presented for early termination with $0.9999$ confidence. 
    We mark the \textbf{best} and \underline{second best} results (excluding oracle solvers). Reported runtimes are for the whole RANSAC.}%Best and second best results (exluding oracle solvers) are highlighted with bold and underline font.}
    \label{tab:poselib_both}
\end{table}

\begin{table}
 \resizebox{0.98\linewidth}{!}{
%\resizebox{\linewidth}{!}{
\begin{tabular}{ l | c c | c c c | c}

    \toprule
    Estimator & AVG $(^\circ)$ $\downarrow$ & MED $(^\circ)$ $\downarrow$ & AUC@5 $\uparrow$ & @10 $\uparrow$ & @20 $\uparrow$ & Runtime (ms) $\downarrow$\\
    \midrule
% 3px all Cambridge, R iters = 2
\sst & \textbf{11.16} & \textbf{3.53} & \textbf{38.44} & \textbf{56.79} & \textbf{70.89} & \phantom{1}\textbf{11.72} \\
\midrule\sstm & 18.85 & 4.20 & 33.77 & 49.88 & 62.64 & \phantom{1}\underline{12.67} \\
\sstmR & 19.53 & 4.30 & 33.23 & 49.08 & 61.71 & \phantom{1}14.74 \\
\sstmRC & 19.87 & 4.38 & 32.83 & 48.62 & 61.28 & \phantom{1}12.85 \\
\hline
\sstmd & 18.83 & 4.29 & 33.18 & 49.26 & 62.14 & \phantom{1}20.23 \\
\sstmdR & 18.49 & 4.24 & 33.56 & 49.77 & 62.70 & \phantom{1}21.76 \\
\sstmdRC & 19.09 & 4.33 & 33.04 & 49.07 & 61.91 & \phantom{1}19.98 \\
\hline
\sstl & 20.50 & 4.47 & 32.49 & 47.91 & 60.27 & 138.26 \\
\sstlR & 20.82 & 4.54 & 32.10 & 47.47 & 59.83 & 141.27 \\
\sstlidR & \underline{14.94} & \underline{3.79} & \underline{36.21} & \underline{53.56} & \underline{66.98} & 259.63 \\
\midrule\ssto & 11.72 & 3.50 & 38.63 & 56.91 & 70.86 & \phantom{1}12.31 \\
\sstoR & 11.77 & 3.51 & 38.69 & 56.94 & 70.89 & \phantom{1}14.06 \\
\midrule

\end{tabular}}
\caption{Results for the \sstp problem %on \textit{Great Court}~\cite{kendall2015cambridge}.
on 5 scenes from Cambridge Landmarks~\cite{kendall2015cambridge}.
$\delta$ and (\texttt{+R}) do not always need to improve performance due to the early stopping criterion in RANSAC and being far from the optimum. In this case, the $\delta$-version of the L-based solver performs the best for the  \sstp problem (for more details on the \sstlidR solver, see SM).  
}
\label{tab:4p3vf_cambridge}
\end{table}

Tab.~\ref{tab:poselib_both} shows results for calibrated cameras when using early termination in PoseLib RANSAC at a $0.9999$ confidence threshold. % for PoseLib. % in Tab.~\ref{tab:poselib_both}. 
We provide similar results for GC-RANSAC in the SM. 
As can be seen, with refinement (\texttt{+R}), all of our solvers outperform the state-of-the-art HC-based \sfhc solver~\cite{Hruby_cvpr2022} in terms of accuracy. 
Using filtering (\texttt{+F}) improves the run-time of RANSAC at the cost of a decrease in pose accuracy. 
Still, our \sftmRC solver outperforms \sfhc in terms of both accuracy and run-time. 
The \sftmd and \sftmdR solvers clearly improve upon the \sftm solvers, albeit at an increased run-time without filtering. 
With filtering, \sftmdRC is similarly efficient as \sftmR at a (slightly) higher accuracy. 
The \sftl solvers slightly improve upon the \sftm solvers. 
While they produce more accurate virtual correspondences, refinement in the solvers (\texttt{+R}) %and %steps in both the \texttt{+R} solver variants and 
% in local optimization) 
compensates for the less accurate initial poses provided by the \sftm solvers, resulting in similar results for \sftmR and \sftlR. 
Still, the results for the \sftl solvers show a direction for improvement, namely learning to predict virtual correspondences. 
The results for the \sfto solvers show that there is room for improvement. 
The results for more variants of L-based solvers, ($\delta$, \texttt{+F}, \etc) are in the SM.

Method \cite{Hruby_cvpr2022} and our solvers solve a different configuration for the three-view-pose estimation problem than the \texttt{5pt+P3P}  solver ($\confc$ vs.\ $\confm$). 
Based on limited experiments, \cite{DBLP:journals/ijcv/NisterS06} concluded that \texttt{5pt+P3P} outperforms their solver. % performs worse than the  solver. 
\cite{Hruby_cvpr2022} did not compare to \texttt{5pt+P3P}. 
Tab.~\ref{tab:poselib_both} rectifies this omission, showing that \sfhc is clearly less accurate than \sft, while not being consistently faster. 
In contrast, our \sftmR, \sftmRC, and \sftmdRC perform comparable to \sft at faster run-times. 
To the best of our knowledge, we are the first to show that solvers for the $\confc$ configuration are practically relevant. 

We also investigate the speed-accuracy trade-off of the solvers by %evaluation by 
running PoseLib RANSAC for a set of fixed numbers of iterations. 
Runtimes are reported for 1 core of a 2 GHz Intel Xeon Gold 6338 CPU. 
As shown in Fig.~\ref{fig:poselib_graphs}, % shows results on Phototourism and Cambridge Landmarks datasets for all scenes. O
our %best performing method 
\sftmRC provides a better speed-accuracy trade-off than \sft on Phototourism and a slightly worse trade-off on Cambridge Landmarks. 
On both datasets \sftmRC performs better than \sfhc. 
Fig.~\ref{fig:poselib_g_reichstag} shows results for the \textit{Reichstag} scene,  %scene % from Phototourism 
% for which 
where also other variants of our method provide a better trade-off than \sft. 
Fig.~\ref{fig:poselib_g_kc} shows results for %the 
\textit{King's College}, % scene, %from Cambridge Landmarks 
where \sftmRC outperforms \sft. These results show the practical viability of our solvers in a time-constrained setting. 
Results for more scenes are in the SM. 


% In 
Fig.~\ref{fig:poselib_outlier} shows the potential of our solvers to handle scenarios with low inlier ratios. %are better able to handle higher outlier ratios than \sft. 
We synthetically remove outlier matches based on ground truth pose information and replace them with outliers distributed uniformly at random such that the inlier ratio is fixed to $0.4$ for all image triplets. 
Under the lower inlier ratio, our methods perform better compared to \sft, while \sft performs very similarly to \sftmRC for the higher inlier ratio scenario in the presented Brandenburg Gate scene.  More scenes are in SM. 
% This shows the potential for our method to be especially useful for scenes with low inlier ratios.



\PAR{Shared unknown focal length case.} Tab.~\ref{tab:4p3vf_cambridge} shows results for the three-view-relative-pose problem for cameras with a shared unknown focal length. 
It compares our solvers (solving the $\conft$ configuration) with the \sst solver (solving the $\confpm$ configuration). 
While our \sstp solvers do not reach the accuracy of the \sst solver, the results still show that our approach based on generating two virtual correspondences leads to practically useful solvers. 
To the best of our knowledge, ours are the first practical solvers for the $\conft$ configuration.

\PAR{Limitations.} As shown above, the accuracy of our solvers is scene dependent.\footnote{This weakness also applies to~\cite{Hruby_cvpr2022}, since the scene needs to be similar enough to the training scenes for the MLP-based classifier to work well.} 
In addition, we observed that the performance of our approaches drops when the overlap between the images in a triplet is small. 
In this case, the correspondences form small triangles. This leads to unstable configurations as the distance between the points is relatively small compared to the noise in the points (especially in the virtual points). 
While correspondences found in a small image region cause the same issues for the \texttt{5pt} solver, \sft is more robust in scenarios with small overlap, as its $5^{th}$ correspondence has a chance to be farther away from the other correspondences than the virtual correspondences used in our solvers. 
Properly taking the uncertainty of the virtual points into account, \ie, propagating their uncertainty into the uncertainty of the estimated poses and using this uncertainty during inlier counting~\cite{ForstnerBook}, is a promising direction to handle the fact that our solvers produce less accurate poses. 
Creating more accurate correspondences, \eg, by training better networks, is an alternative. 
At the moment, we use the first three correspondences to create the virtual correspondence. 
More sophisticated selection criteria, \eg, trying to maximize the size of the formed triangles, could also improve performance. 

\section{Conclusion}
 In this paper, we consider the highly challenging %4p3v 
problems of relative pose estimation of three calibrated and partially calibrated cameras from four correspondences. 
We propose a novel and easy-to-implement framework that solves these problems using existing efficient solvers by simply predicting a $5^\text{th}$/$6^\text{th}$ point correspondence(s). 
We propose several solvers based on this framework, one simply using mean coordinates of input points (M-based solvers) and one using a trained predictor (L-based solvers), with multiple variants. 
Extensive experiments show that our solvers achieve state-of-the-art performance on real data for the challenging configuration of four points in three views. 
% At the same time, our M-based solvers 
%based on using mean points 
% are trivial to implement, especially compared to the current state-of-the-art~\cite{Hruby_cvpr2022}. 

\section{Acknowledgements}
C. T. was supported by the Czech Science Foundation (GAČR) JUNIOR STAR Grant (No. 22-23183M). V. K. was supported by the project no. 1/0373/23. and the TERAIS project, a Horizon-Widera-2021 program of the European Union under the Grant agreement number 101079338. (Part of the) Research results was obtained using the computational resources procured in the national project National competence centre for high performance computing (project code: 311070AKF2) funded by European Regional Development Fund, EU Structural Funds Informatization of society, Operational Program Integrated Infrastructure. D. B. was supported by the ETH postdoc fellowship. Z. B. H. was supported by the grant KEGA 004UK-4/2024 “DICH: Digitalization of Cultural Heritage”. T. S. was supported by the EU Horizon 2020 project RICAIP (grant agreement No. 857306) and the European Regional Development Fund under project IMPACT (No. CZ.02.1.01/0.0/0.0/15\_003/000\break0468). Z. K. was supported by the Czech Science Foundation (GAČR) JUNIOR STAR Grant (No. 22-23183M).

% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{bibliography}
% }

%