\documentclass[letterpaper, 10 pt, conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\overrideIEEEmargins                                      % Needed to meet printer requirements.

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{flushend}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\input{defs}
% \usepackage[margin=0.75in]{geometry}

    
\begin{document}

\title{\LARGE \bf Learning a Single Policy for Diverse Behaviors on a Quadrupedal Robot using Scalable Motion Imitation
}

\author{Arnaud Klipfel$^{1}$ Nitish Sontakke$^{1}$ Ren Liu$^{2}$ Sehoon Ha$^{1}$% <-this % stops a space
% \author{Albert Author$^{1}$ and Bernard D. Researcher$^{2}$% <-this % stops a space
% \thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, 30308, USA. {\tt \small aklipfel3@gatech.edu, nitishsontakke@gatech.edu, sehoonha@gatech.edu}}%
\thanks{$^{2}$ Meta Platforms, Inc., USA, 
        {\tt\small renl@meta.com}. Work done while at Georgia Tech.}%
}


\maketitle

\begin{abstract}
Learning various motor skills for quadrupedal robots is a challenging problem that requires careful design of task-specific mathematical models or reward descriptions. In this work, we propose to learn a single capable policy using deep reinforcement learning by imitating a large number of reference motions, including walking, turning, pacing, jumping, sitting, and lying. On top of the existing motion imitation framework, we first carefully design the observation space, the action space, and the reward function to improve the scalability of the learning as well as the robustness of the final policy. In addition, we adopt a novel adaptive motion sampling (AMS) method, which maintains a balance between successful and unsuccessful behaviors. This technique allows the learning algorithm to focus on challenging motor skills and avoid catastrophic forgetting. We demonstrate that the learned policy can exhibit diverse behaviors in simulation by successfully tracking both the training dataset and out-of-distribution trajectories. We also validate the importance of the proposed learning formulation and the adaptive motion sampling scheme by conducting experiments. 
\end{abstract}

% \begin{IEEEkeywords}
% component, formatting, style, styling, insert
% \end{IEEEkeywords}

\section{Introduction}

% Problem of both model-based and learning-based approaches.
Quadrupedal robots can achieve various autonomous missions by overcoming rough terrains that wheeled robots cannot traverse, but the control is not straightforward due to its high-dimensional state space and under-actuated dynamics. Roboticists have studied various approaches for legged robot control, ranging from model-based control~\cite{raibert1986legged,park2017high,bledt2018cheetah,kim2019highly} to learning-based approaches~\cite{hwangbo2019learning,lee2020learning,kumar2021rma,miki2022learning}, which have demonstrated impressive agility and robustness on various quadrupedal robots. However, most of the prior works have focused on the given specific task, such as robust walking, running, or jumping, because they are governed by very different dynamics. These task-specific controllers often require manual engineering based on the expert's prior knowledge, which can be either developing mathematical models for model-based controllers or shaping reward functions for learning-based algorithms. It requires even more effort if the developer wants to improve the naturalness of the behavior.

% Motion imitation - unified, natural.
One interesting approach is to develop a motion imitation controller that can track the given reference motion, which defines the task implicitly. For instance, walking and jumping are two very different tasks, but motion imitation treats them as the same task of tracking the corresponding motion. If the reference is captured by a human or an animal, motion imitation can also allow us to develop natural behaviors from the original actor. One notable early work is DeepMimic proposed by Peng et al.~\cite{deepmimic}, which shows an impressive motion-tracking performance on a simulated humanoid character. Many researchers have extended this work to imitate a wide range of motions on a simulated character by investigating novel policy architectures~\cite{won2020} or introducing adversarial learning~\cite{peng2021amp,peng2022ase}. This motion imitation approach has been investigated in the context of robotics as well to develop natural motions~\cite{peng2020learning}, but it is limited to tracking a single reference motion.

\begin{figure}
    \vspace{5mm}

\centerline{\includegraphics[width=0.45\textwidth]{fig/teaser-run.png}}
\centerline{\includegraphics[width=0.45\textwidth]{fig/teaser-turn.png}}
\centerline{\includegraphics[width=0.45\textwidth]{fig/teaser-jump.png}}
\centerline{\includegraphics[width=0.45\textwidth]{fig/teaser-sit.png}}

\caption{Our scalable motion imitation framework can learn a single policy to execute many motor skills, from running (\textbf{1st}), turning (\textbf{2nd}), jumping (\textbf{3rd}), and sitting (\textbf{4th}).}
\label{fig-teaser}
\end{figure}

This paper investigates a scalable motion imitation framework for a quadrupedal robot to track various motor skills using a single control policy. For the preprocessing, we carefully prepare all the reference motions by retargeting the existing dog's motion database, which results in $701$ motion clips with $15$ different motion types. Then we extend the existing motion imitation framework~\cite{deepmimic} to improve its scalability and robustness. We design a new problem formulation, including a new observation space that includes future and past references and a new reward function that does not incentivize low-level kinematic tracking. In addition, we propose a novel adaptive motion sampling (AMS) scheme to learn all the trajectories without ignoring some outlier motions, such as jumping, and also to avoid catastrophic forgetting about the previously learned motor skills. 

We demonstrate that our framework can learn a single versatile motion imitation policy that can track a large variety of reference motions. Our policy can even track new out-of-distribution trajectories, such as a star-shaped path or a motion with multiple jumps. By conducting an ablation study, we show that adaptive trajectory sampling is necessary to learn all the motor skills in the database. We also demonstrate the robustness of the framework, which is achieved by our novel learning formulation. Our key contributions are summarized as follows:
\begin{itemize}
\item We propose novel techniques for greatly improving the robustness and scalability of motion imitation.
\item We showcase that our framework can learn a single policy to track various challenging trajectories.
\item We validate the proposed components by conducting ablation studies.
\end{itemize}

% Generating gaits for legged systems is a problem that has been tackled from control theory and animation. Recent animation techniques such as the ones using motion  imitation produce incredible results in simulation but have not often be deployed, conversely methods focusing on robust policies have focused on 
% \cite{haarnoja2018learning}

% Our key contributions are:
% \begin{itemize}
%     \item Scalable motion imittation: training one policy on a library of very diverse, and agile skills. Published papers have so far focused on learning an expert for each locomotion skills, i.e. specialized policies, fine-tuning hyperparameters based on the skills they learned. Our framework is locomotion skill agnostic.
%     \item The policy does not use a frame identifier, or a phase variable to encode the motion clip progression.
%     \item And Adaptive Sampling scheme (ADS) is used to sample motion clips in order to obviating the need of motion cluster annotation or training on expert per cluster.
%     \item A different policy design to prevent low-level overfitting on the kinematic reference, thus preventing such policies from being deployed on a real system. Observations comprise the future and past reference information at the present control step, and also the reward that does not incentivize low-level kinematic tracking through error minimization. The reward is entirely positive using exponential formulation.
% \end{itemize}

\section{Related works}

\subsection{Quadrupedal Locomotion}
The control of quadrupedal robots has been thoroughly studied by many robotics researchers. One common approach is to develop a model-based controller that captures the important characteristics of the robot's dynamics using a mathematical model and generates optimal control trajectories~\cite{raibert1986legged,park2017high,bledt2018cheetah,kim2019highly}. While demonstrating impressive robustness and agility on hardware, a model-based approach often requires manual engineering to develop the proper dynamics model for the given task. In recent years, researchers have showcased that it is possible to learn robust locomotion policies using deep reinforcement learning (deep RL)~\cite{hwangbo2019learning,lee2020learning,kumar2021rma,miki2022learning}. However, it is also a well-known challenge that deep RL often requires an extensive amount of reward shaping to obtain the best quality policy that can be effectively transferred to the real world. Therefore, the developed reward functions sometimes have many different terms, up to nine or ten, to guarantee symmetric, energy-efficient, cyclic, and effective gaits~\cite{rudin2022learning,margolis2022rapid}. Therefore, developing a high-quality motion controller for novel tasks still remains a challenging problem for both model-based and learning-based approaches and requires a lot of human effort.

\subsection{Motion Imitation}
Motion imitation is a problem formulation that aims to track the given reference motion. Because the task is implicitly encoded in the reference, this framework allows us to use a unified problem formulation for various tasks, unlike standard task-based problem formulations. The early work of Peng et al.~\cite{deepmimic} demonstrates that it is possible to train a virtual human character to track a single motion in a physics-based simulation. This research is followed by many other works in computer animation~\cite{won2020,won2021control,peng2021amp,peng2022ase} to track a wide range of motions. The robotic community also adopts the same motion imitation framework to develop quadrupedal robot controllers to achieve natural animal-looking motions~\cite{peng2020learning}. Kim et al.~\cite{kim2022human} demonstrate a human motion interface to control a quadrupedal robot by combining motion imitation and motion retargeting. Escontrela et al.~\cite{escontrela2022adversarial} show that adversarial reward formulation of motion imitation can be a good substitute option for complex reward functions. Our work is also closely related to these state-of-the-art contributions in both the computer animation and robotics communities. We extend the motion imitation framework to support a large dataset for quadrupedal locomotion by proposing a novel adaptive sampling and policy design.

% \subsection{Motion synthesis}

% \subsection{Learning locomotion skills in simulation}
% Designing controllers for different locomotion skills (jumping, trotting, galloping, turning) in simulation has been the primary focus of computer animation. Traditional methods generated kinematic motion data. The main issues of these approaches are that they failed to incorporate the dynamics of the system or character. Physics-based simulation was then used to adapt these generated motions to the dynamics. Trajectory-optimization (TO) based techniques were used to adapt kinematic motion data to dynamic ones. But these methods were shown to be very sensitive as they relied on a dynamical model (model-based techniques) of the character, and failed on more agile motions, exhibiting long aerial phases. Recently, Deep Reinforcement Learning (DRL) produced incredible results in motion imitation. Very acrobatic and agile skills were reproduced on a simulated character by imitating kinematic reference data. However, these approaches often use a phase variable, train one policy for each learned skill, use thousands of single motion clips for one skill, and try to track the kinematic reference trajectory too well, which makes these methods sensitive to the dynamics, which is a reason that explains why these controllers have never been deployed as is on real systems. Curriculum learning has also been used in combination with DRL to learn gradually more complex tasks.

% \subsection{Robust locomotion in the real world}

% From the robotics perspective, robust locomotion has been the primary focus of the legged locomotion community. That is designing controllers that can account for environmental perturbations. Model-based approaches have been used, but they fail on more agile motions, and require expert knowledge. As in the computer animation community, DRL has shown incredible results in the past few years. Similar successes have also occurred in other domains, such as in manipulation. Some work also combine traditional optimization-based techniques with DRL. The main challenge when deploying such controllers or policies is the sim-to-real gap. Domain randomization is commonly used to deal with this problem, more accurate simulations such as more accurate motor models. These controllers rely on heavy reward engineering to make the emerge gait look more natural, and on a learning residual joint potision action around a reference trajectory. The state-of-the-art approach is now to train a policy in a two-state fashion. Inspired by Learning-By-Cheating, the first stage of the learning trains a policy with privileged sensor information (teacher training), and the second stage removes the teacher and privileged information to match the robot on-board sensing (student teaching). Other approaches have also attempted to remove the teacher dependence more gradually using a multi-stage learning process, or a reward curriculum, where reward term coefficients would evolve based on the training stage.

% \subsection{Diverse behaviors in the real world}
% Due to the heavy reward engineering that require making a walking gait emerge, policy deployed on hardware exhibit a limited set of locomotion skills, and are not smooth. Diverse behaviors were obtained in simulation. They rely on Mixture-Of-Experts, which consists of training a single policy for each skill (an expert), and learning a gating network to choose which expert to use depending on the situation. Only one work has successfully deployed such policies.


% \subsection{Motion imitation}
% In order to obtain more natural and smooth motions, imitation learning (IL) can be used to imitate a reference trajectory that already contains the hard-to-model transitions between gaits. Some work managed to deploy IL based policies in the real world. These policies were trained on single skills.

% \subsection{Adversarial learning and imitation learning}
% Recently, GANs were used in combination with IL to generate motion clips, and learn locomotion skills both in simulation and on hardware. The results obtained on hardware are impressive, and exhibit the potential of these techniques. These techniques rely on the main assumption that tracking the reference data perfectly is not necessarily the goal, but rather the policy should imitate the style of the motions, and converge to a manifold containing admissible motions.


\section{Scalable Motion Imitation}
In this section, we will describe the proposed scalable motion imitation framework to track more than $700$ motion clips as well as out-of-distribution trajectories with a single policy. We first explain our data generation procedure in Section~\ref{sec:data_generation}. Then we present our novel problem formulation in Section~\ref{sec:problem}, which is designed to improve the robustness of the existing motion imitation framework. Finally, we describe our novel adaptive trajectory sampling method in Section~\ref{sec:adaptive_sampling}, which is necessary to learn a large number of motion trajectories.

\subsection{Data Generation} \label{sec:data_generation}
% - We take the dataset of ...
% - we retarget the motion by selecting the key point
% - It has many hyperparameters that affects the results, such as...
% - call ren about some hyperparmeter selection ....
% - Motion synthesis artifacts: motion clips are not perfect even if we processed them to reduce noise, foot sliding .....
% - Fig.: compare two retargeted motions with different hyperparameters..
% - Give a detail of the different motions that we have
% \sehoon{Ren, please fill this section, if possible.}

\begin{figure}
    \vspace{5mm}

\centerline{\includegraphics[width=0.46\textwidth]{fig/pipeline.png}}
\caption{Motion dataset generation pipeline.}
\label{fig-dataset-gen-pipeline}
\end{figure}

% General description
The motion dataset contains $701$ motion clips with $15$ different motion types. We generate the dataset using our motion generation pipeline (See Fig.~\ref{fig-dataset-gen-pipeline}). Every motion clip in the dataset lasts for $10$ seconds and is of $60$~Hz frame rate. For the random keyboard input commands and their distribution, the pipeline interpolates them into a sequence of 600 to simulate user interaction. Then we infer kinematic data, such as joint angles, with the interactive motion generation framework~\cite{zhang2018mode}, which is trained with a wolf character skeleton. 
% \ren{The motion dataset contains 701 motion clips with 15 different motion types included, and was generated by our self-designed motion generation pipeline (See Fig.\ref{fig-dataset-gen-pipeline}). Every motion clip in the dataset lasts for 10 second and is of 60Hz frame rate. Given a specific set of keyboard input commands and their distribution, the pipeline interpolates the commands into a sequence of length 600 to simulate user interaction, and infers kinematic data (e.g. joint angles) of each frame with the interactive motion generation framework proposed by Zhang et al.\cite{zhang2018mode}. The framework was trained on a public dataset \cite{zhang2018mode} to fit with a wolf skeleton, so the pipeline will then retarget the framework outputs to the A1 [cite A1] robot model. }

\begin{figure}
    \vspace{5mm}

\centerline{\includegraphics[width=0.3\textwidth]{fig/jump.png}}
\caption{Jumping with different scaling factors.(\textbf{Left}:  1.0, \textbf{Right}: 0.825 (ours)).}
\label{fig-dataset-jump-comp}
\end{figure}

\begin{figure}
\centerline{\includegraphics[width=0.3\textwidth]{fig/sit.png}}
\caption{Different retargeted sit motions. \textbf{Left:} IK with hips and feet restriction. \textbf{Right (ours)}: Adjusted IK considering knee positions for sit motions.}
\label{fig-dataset-sit-comp}
\end{figure}


Therefore, we need to retarget the motion of a wolf into our A1 quadrupedal robot~\cite{a1}. We apply the algorithm implemented by Peng et al. \cite{peng2020learning}, which pairs corresponding key points from the source subject's body to the target robot's body, including the positions of the feet and hips, and then performs inverse-kinematics (IK) \cite{gleicher1998retargetting} to fulfill the morphological gap. The pipeline has many robot model-specific hyperparameters that affect the results. For instance, Fig. \ref{fig-dataset-jump-comp} shows that an improper coordinates scaling may cause physically unreasonable front knee joint angles when jumping. We also need to remove all the ground penetration: therefore, our retargeting algorithm is aware of the motion type to handle such special cases (See Fig. \ref{fig-dataset-sit-comp}). We further remove some artifacts, such as foot skating or jitterness, by applying inverse kinematics and smoothing. The content of the generated database is summarized in Table~\ref{tab1-dataset}.


\begin{table}
\caption{Generated Motion Clip Dataset Specification}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{Motion Type} & Stand & Step & Pace & Trot & Gallop & Jump \\
\hline
\textbf{Number of Clips} & 1 & 1 & 200 & 50 & 1 & 200 \\
\hline
\hline
\textbf{Motion Type} & \multicolumn{2}{|c|}{Turn Left} & \multicolumn{2}{|c|}{Turn Right} & Sit & Lie\\
\hline
\textbf{Number of Clips} & \multicolumn{2}{|c|}{51} & \multicolumn{2}{|c|}{51} & 1 & 1 \\
\hline
\hline
\textbf{Motion Type} & \multicolumn{3}{|c|}{Turn Left In-Place} & \multicolumn{3}{|c|}{Turn Right In-Place} \\
\hline
\textbf{Number of Clips} & \multicolumn{3}{|c|}{25} & \multicolumn{3}{|c|}{25} \\
\hline
\hline
\textbf{Motion Type} & \multicolumn{2}{|c|}{Triangle Trace} & \multicolumn{2}{|c|}{Star Trace} & \multicolumn{2}{|c|}{Random Mixed} \\
\hline
\textbf{Number of Clips} & \multicolumn{2}{|c|}{1} & \multicolumn{2}{|c|}{1} & \multicolumn{2}{|c|}{92}  \\
\hline
\end{tabular}
\label{tab1-dataset}
\end{center}
\end{table}

% \ren{We retarget the output wolf motion by applying the algorithm implemented by Peng et al. \cite{peng2020learning}, which pairs corresponding keypoints from source subject's body to target robot's body, including the positions of the feet and hips, and then performs inverse-kinematics (IK) \cite{gleicher1998retargetting} to fulfill the morphological gap. It has many robot model-specific hyperparameters that affects the results, for example the default joint angles when standing steadily, the joint damping coefficients for solving IK and the generalized coordinates scale for retargeting. Fig. \ref{fig-dataset-jump-comp} shows an improper coordinates scaling may cause physically unreasonable front knee joint angles when jumping. Despite of carefully tuning those hyperparameters for A1 robot, it is not sufficient to retarget all types of motion properly by only considering the positions of feet and hips. Sit motion is such a typical outlier: the rear knee joints will fall below the ground if simply estimated by the original IK. Therefore, our retargeting algorithm is made action type-aware to handle with such special cases (See Fig. \ref{fig-dataset-sit-comp}).}



% \ren{The synthesized motion clips can still have artifacts after retargeting and denoising, the foot sliding is one of the most common one we identified. Foot sliding refers to the kind of motion that the foot endpoint keeps contacting with the ground but with noticeable displacement on the ground plane. It can be resulted from multiple factors that are difficult to trace precisely, including unpredictable network inference and inaccurate morphological-retargeting model. To remove foot sliding, we keep the torso motion, fix the sliding foot joint and calculate the other joints' locations by IK. After it stops sliding, we use several extra frames to do motion interpolation to have the fixed endpoint back to the target place. }

% \ren{From the result of manual motion types annotation, the content of the generated dataset are listed as Table \ref{tab1-dataset} below.}




\subsection{Problem Formulation for Scalability and Robustness} \label{sec:problem}
% - Motion imitation is important - proven effective for many applications, and in combination with DRl avoids jerky motions, produces more structures motion - it formulates it as deep Rl - reward is...
% - Type of data used, often Mocap.
Imitating the given reference motion is a popular approach to developing a versatile physics-based controller. While researchers traditionally have approached this problem using model-based control~\cite{li2021fastmimic}, the recent advances in deep reinforcement learning offer an automated approach to learning a tracking policy for a variety of motions. We mostly follow the formulation of Peng et al.~\cite{deepmimic}, while making a few adjustments in the state, action, and reward function designs. We define our problem as a Markov Decision Process with reward function at time $t$, $r_t$, action $\vc{a}_t$, observation $\vc{o}_t$ and state $\vc{s}_t$. 

\subsubsection{Observation Space}
In our formulation, the observation space consists of three components: the robot proprioceptive data, some privileged data, and the reference motion data to track. For each control time step $t\in\mathbb{R}$ (every $0.02s$), the robot proprioception is composed of the joint positions in radians $\vc{q}_t\in\mathbb{R}^{12}$, the joint velocities $\dot{\vc{q}}_t\in\mathbb{R}^{12}$ in rad/s which are given by the encoders, the angular velocity $\omega_t\in\mathbb{R}^{3}$ in rad/s which is given by the robot on-board gyroscope. The policy has also access to some privileged information that is usually not estimated on a robot, or which requires some additional estimation than just pure proprioceptive readings \cite{Ji_2022}. This privileged information is composed of the Center-Of-Mass (CoM) $\vc{x}_t\in\mathbb{R}^{3}$ of the robot with respect to (w.r.t) an origin frame (the same as the reference data), the robot base orientation w.r.t the same origin frame given as the full rotation matrix $\vc{R}_t\in SO(3)$, and the body linear velocity at the CoM $\vc{v}_t\in\mathbb{R}^{3}$, expressed in the inertial frame of the robot. The CoM position could be estimated using a joint or visual odometry, as well as the rotation matrix, and the velocity could also be estimated using the accelerometer data \cite{Ji_2022}. The robot data is written as: $\vc{o}^{robot}_t=(\vc{x}_t^T,(\vc{R}_t)_{i,j}^{i,j\in[1,3]}, \vc{q}_t^T, \vc{v}_t^T,\omega_t^T,\dot{\vc{q}}_t^T)\in \mathbb{R}^{42}$.  $(\vc{A})_{i,j}$ refers to the coefficient $(i,j)$ of the matrix $\vc{A}$.
\\\indent In contrast to other works~\cite{deepmimic, won2020}, the observation space does not include the state of every joint and link of the robot (i.e. twist information, orientation, and relative body position w.r.t. the root joint) and contains only the base full state and the joint angles. The policy has not access to a key frame identifier or marker such as a normalized phase variable \cite{deepmimic} that is used to make the motion learning faster. 
\\\indent The reference motion vector $\bar{\vc{m}}_t \in \mathbb{R}^{24\times8=192}$ is comprised of the target joint positions $\bar{\vc{q}}_t\in\mathbb{R}^{12}$, the rotation matrix $\bar{\vc{R}}_t\in SO(3)$, and the CoM position w.r.t an origin frame $\mathbf{\bar{\vc{x}}}_t\in\mathbb{R}^{3}$. Then our observation $\vc{o}_t \in \mathbb{R}^{234}$ is defined as the concatenation of the robot data and the reference motion data for a short time window, $\vc{o}_t = ({\vc{o}^{robot}_t}^T, \bar{\vc{m}}_{t-1.0}^T, \bar{\vc{m}}_{t-0.5}^T ,  \bar{\vc{m}}_{t-0.2}^T, \bar{\vc{m}}_{t-0.02}^T, \allowbreak \bar{\vc{m}}_{t+0.02}^T, \bar{\vc{m}}_{t+0.2}^T, \bar{\vc{m}}_{t+0.5}^T, \bar{\vc{m}}_{t+1.0}^T\allowbreak)$. Note that we include both past and future reference motions for learning efficiency. We also exclude the current reference frame $\bar{\vc{m}}_t$ to avoid the copy-and-paste behavior of the current frame and promote broader exploration, i.e. adaptation of the low-level joint positions of the reference to the robot and environment dynamics. 

\subsubsection{Action Space}
The action $\vc{a}_t \in \mathbb{R}^{12}$ is defined as the delta to a nominal (i.e independent of the reference and fixed at all time) joint configuration of the robot, which becomes the target position for the proportional-derivative controller at each joint. The generated actions are further smoothed by applying a moving average with a window size of two. The nominal joint configuration is: $\vc{a}_m=(                -0.01, 0.75, -1.5,
                0.01, 0.75, -1.5,
                -0.01, 0.75, -1.5,\allowbreak
                0.01, 0.75, -1.5)$, which corresponds to a standing configuration. Joint positions are bounded. $\theta_{hip}\in[-0.5,0.5] rad$, $\theta_{thigh}\in[-0.1,1.5] rad$, and $\theta_{calf}\in[-2.1,-0.5] rad$. 
\subsubsection{Reward Function} \label{sec:reward}
We design our reward function as follows:
\begin{align} \label{eq:reward}
    r_t &=  w_1 \textit{exp}({-k_1 || \bar{\vc{x}}_t - \vc{x}_t||^2}) + w_2 \textit{exp}({-k_2 || \bar{\mat{R}}_t - \mat{R}_t||^2}) \nonumber \\
    &+ w_3 \textit{exp}({-k_3 || \bar{\vc{e}}_t - \vc{e}_t||^2}),
\end{align}
where $\vc{x}$, $\mat{R}$, and $\vc{e}$ are the root position, the base orientation represented as a rotation matrix, and the end-effector positions expressed in the origin frame. The other terms $\bar{\vc{x}}$, $\bar{\vc{R}}$, and $\bar{\vc{e}}$ are the corresponding desired values from the reference motions. Therefore, each term encourages to track the given reference motion. $w_1$, $w_2$, and $w_3$ are the weight vectors to adjust the importance of each term and $k_1$, $k_2$, and $k_3$ are additional decaying parameters to tune the sensitivity of the reward term. We set the parameters as $w_1=0.7$, $w_2 = 0.5$, $w_3 = 0.15$, $k_1 = 12.5$, $k_2 = 20.0$, and $k_3 = 40.0$ for all the experiments.

As discussed in \ref{sec:reward}, we do not have a low-level tracking reward term as \cite{won2020, deepmimic, peng2020learning}, in order to prevent overfitting on the kinematics, and as such we provide more joint position information in the observation space to passively enforce the reference joint positions. 

\begin{figure*}
    \vspace{5mm}

    \centering
    \setlength{\tabcolsep}{1pt}
    \renewcommand{\arraystretch}{0.7}
    \begin{tabular}{c c c c c}
    \includegraphics[width=0.195\textwidth]{fig/motions/star_0014.png} &
    \includegraphics[width=0.195\textwidth]{fig/motions/star_0024.png} &
    \includegraphics[width=0.195\textwidth]{fig/motions/star_0026.png} &
    \includegraphics[width=0.195\textwidth]{fig/motions/star_0028.png} &
    \includegraphics[width=0.195\textwidth]{fig/motions/star_0034.png} \\
    \includegraphics[width=0.195\textwidth]{fig/motions/jump_0001.png} &
    \includegraphics[width=0.195\textwidth]{fig/motions/jump_0006.png} &
    \includegraphics[width=0.195\textwidth]{fig/motions/jump_0007.png} &
    \includegraphics[width=0.195\textwidth]{fig/motions/jump_0010.png} &
    \includegraphics[width=0.195\textwidth]{fig/motions/jump_0012.png} \\
    \includegraphics[width=0.195\textwidth]{fig/motions/sit_0005.png} &
    \includegraphics[width=0.195\textwidth]{fig/motions/sit_0009.png} &
    \includegraphics[width=0.195\textwidth]{fig/motions/sit_0011.png} &
    \includegraphics[width=0.195\textwidth]{fig/motions/sit_0012.png} &
    \includegraphics[width=0.195\textwidth]{fig/motions/sit_0013.png} \\
    \end{tabular}
    
    \caption{A variety of generated motions using a single policy. 
    \textbf{1st row:} a sharp turn during a star-trajectory tracking task.
    \textbf{2nd row:} multiple jumps in one sequence.
    \textbf{3rd row:} lying down and sit motions.
    Please refer to the supplemental video for the entire sequences.
    }
    \label{fig:motions}
\end{figure*}

\subsubsection{Early Termination}
In contrast to \cite{deepmimic, won2020}, which uses early termination based on the CoM tracking performance \cite{deepmimic}, or the reward function \cite{won2020} in order to speed up the motion tracking learning, and avoid poor performing tracking policies. Our formulation uses simple contact-based termination without penalty. The only allowable contacts are the four feet with the ground. Won et al.~\cite{won2020} pointed out that contact-based terminations prevented them from learning motions which included self-body contacts. However, as our final goal is to deploy learned behaviors on a real robot, self-body contacts or inadmissible contacts are not desirable. Our formulation, without penalty and prior in the action space (joint residuals w.r.t. a nominal joint configuration) allows the policy from learning certain motions where the kinematic reference has inadmissible contacts, the policy tries to satisfy the high-level reference state (CoM, End-effector, body orientation) instead of trying to reproduce the low-level reference data (joint positions) at any cost.

% we also introduce the concept of early termination (ET) for the sake of sample efficiency. However, our formulation is slightly different from the original formulation: we terminate the episode when the center of mass (COM) position deviates more than $XXX$~cm from the reference. \sehoon{please check or delete if unused.I}

\subsection{Adaptive Motion Sampling} \label{sec:adaptive_sampling}
% Motivation
Although many researchers have demonstrated successful motion imitation using deep RL, it is still challenging to learn a single policy for various heterogeneous locomotion skills \cite{deepmimic, peng2020learning, Yang_2020}, including walking, turning, and jumping. One difficulty is that the computation of gradients is highly affected by a stochastic sampling of simulation rollouts, which is impossible to cover the entire range if there are too many reference motions in the database. Even worse, the stochastic nature of deep RL can lead a policy to forget about previously learned motor skills, which is referred to as catastrophic forgetting.
\\\indent Our Adaptive Motion Sampling (AMS) allows us to train our policy from an unlabelled and unbalanced dataset. We found that our policy even performed better at tracking the reference motion data and producing natural joint motions when trained directly on all the motion clips at once. No pre-training is thus required. Having a rich set of motions to train on prevents the policy from overfitting on the dynamics and kinematics of certain locomotion skills, serving as data augmentation, and promoting more general locomotion strategies which can track a rich set of motions. As pointed out in \cite{heess2019}, complex skills emerge when trained on a rich set of tasks.
% Adaptive Sampling
We propose a novel adaptive sampling scheme to overcome these challenges. Our key idea is to maintain two sets of the reference motions: $\mathcal{U}$ and $\mathcal{S}$, which represent unsuccessful and successful motions, respectively. At the beginning of learning, we initially assign all the motions to the unsuccessful group $\mathcal{U}$ and set the successful group $\mathcal{S}=\emptyset$. Sampling from these sets is done following a uniform distribution, and without re-drawing so that after several episodes the policy has been trained on the entirety of the sets. For every $200$ policy iteration, we evaluate the current policy on all the motions and classify them into each group again based on their performance. If the policy is able to track the motion until the end without early termination, we assign the given motion to the successful group $\mathcal{S}$ (resp. $\mathcal{U}$).

% Sampling
Once the motions are classified into two groups, $\mathcal{U}$ and $\mathcal{S}$, we adjust the sampling of the reference motions. We sample $70$~\% of the reference trajectories from $\mathcal{U}$, while taking $30$~\% of trajectories from $\mathcal{S}$. This mechanism allows the policy to majorly focus on difficult reference motions that are yet unsuccessfully learned while not forgetting already learned motions. 

% \begin{algorithmic}[1]
%   \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%  \renewcommand{\algorithmicensure}{\textbf{Output:}}
%  \REQUIRE sets $\mathcal{U}$ of unsuccessfully tracked motions, $\mathcal{S}$ of successfully tracked motions, dataset of diverse kinematic motion clips $\mathcal{D}$ (unlabelled, unsorted)
%  \ENSURE  policy $\Pi_\Theta$, $\Theta$ contains the weights of the policy
%  \\ \textit{Initialisation} : $\mathcal{U}=\emptyset$, $\mathcal{S}=\emptyset$, random weights $\Theta_0$ initialization
%   % \STATE // Training loop.
%   \FOR{ $k$ $\in [0;+\infty]$}
%    \IF{$k$ $\geq$ 200 and $k=0\mod{200}$}
%     % \STATE // Update the sets of motions. 
%     \STATE $\mathcal{U}=\emptyset$, $\mathcal{S}=\emptyset$
%     \FORALL{$d\in\mathcal{D}$}
%         \STATE $early\_termination$ $\leftarrow$ inference($\Pi_\Theta$,$d$)
%         \IF{$early\_termination$}
%             $\mathcal{U}\leftarrow\mathcal{U}\cup\{d\}$
%         \ELSE
%         \STATE$\mathcal{S}\leftarrow\mathcal{S}\cup\{d\}$
%         \ENDIF
%     \ENDFOR
% \ENDIF
% \STATE  
%   \ENDFOR
% \end{algorithmic}

% The algorithm is summarized in Algorithm~XXX \sehoon{work on this later - or skip it if we dont have time}


% - Based on the real robot obs space: buffer for joint torque, joint position (given by encoders), torques, base velocity, base angular velocity. 100, 80, 60, 20 control steps of 0.02s in the past for the robot data.
% - Reference data given for the past, and future. Current reference data is not given to avoid copy/paste behaviors, and incentives the policy to explore more. 100, 80, 60, 20 control steps of 0.02s in future and past.
% - HL reference data : CoM, rotation matrix for orientation
% - LL reference data: join positions

% In this formulation, the state $\vc{s}$ includes the current joint position  $\vc{q}$, joint velocity  $\vc{v}$, and end-effector positions $\vc{e}$.  The action $\vc{a}$ is the target joint angles for the proportional-derivative controller. The reward function is designed as follows:
% \begin{align*}
%     r(\vc{s}_t) &=  w_1 \textit{exp}({-k_1 || \bar{\vc{q}}_t - \vc{q}_t||^2}) + w_2 \textit{exp}({-k_2 || \bar{\mat{R}}_t - \mat{R}_t||^2}) \\
%     &+ w_3 \textit{exp}({-k_3 || \bar{\vc{e}}_t - \vc{e}_t||^2}),
% \end{align*}
% where $\bar{\vc{q}}$, $\bar{\vc{v}}$, and $\bar{\vc{e}}$ are the corresponding desired values from the reference motions. Therefore, each term encourages to track the given reference motion. $w_1$, $w_2$, and $w_3$ are the weight vectors to adjust the important of each term and $k_1$, $k_2$, and $k_3$ are stiffness parameters to tune the shape of each term. Once the problem is formulated, we can optimize a policy using the off-the-shelf deep RL algorithm, such as Proximal Policy Optimization (PPO)~[cite:PPO]. For more details, please refer to the original paper of Peng et al.~[cite:DeepMimic].



% \subsection{Policy design}
% % \subsubsection{Reward}
% % - sum of reward: CoM, end effector, orientation (rotation matrix) tracking. Sum of exponential terms as in Deepmimic, with norm2 errors. Positive reward formulation to prevent policy from overfitting on the kinematic reference data, and account for more adaptation of the behavior based on smoothing terms, torques ....

% % \subsubsection{Observation space}
% % - Based on the real robot obs space: buffer for joint torque, joint position (given by encoders), torques, base velocity, base angular velocity. 100, 80, 60, 20 control steps of 0.02s in the past for the robot data.
% % - Reference data given for the past, and future. Current reference data is not given to avoid copy/paste behaviors, and incentives the policy to explore more. 100, 80, 60, 20 control steps of 0.02s in future and past.
% % - HL reference data : CoM, rotation matrix for orientation
% % - LL reference data: join positions



% % \subsubsection{Architecture}
% % - MLP 256, 256
% % - tanh activations to incentivize smoother actions

% % \subsubsection{Actions}
% % - Joint residuals around a nominal positions, independent of the motion clip/reference data

% \subsection{Training}
% % - PPO, cite paper
% % - Raisim, cite paper
% % - Filtering: moving average window 3.
% - Adaptive sampling in order to deal with a different type of locomotion skills. If some are more represented than others the policy will end up not learning to track some references. Speeds up learning, and avoids forgetting on previously learned skills, while adding new ones. Successfully tracked references are sampled once per episode/or periodically, references that have not been learned successfully are sampled every episode.

% The main challenge when dealing with a single policy and heterogeneous locomotion skills is to make the policy learn all of them. As the naive training method (without ADS) exemplifies, training a policy on a dataset that contains very diverse motion clips can result in the policy rewaching a local minimum that optimizes most of the motions in the dataset, but not all. The motions successfully learned are the motions that are in population pre-dominant in the dataset. Similarly, motions that are not successfully learned are motions that are sparse or in minority in the dataset. To tackle that problem, an adaptive sampling scheme (ADS) was designed based on the performance of the policy on each motion clips of the dataset. Every 200 training iterations/policy update, the policy is tested on all motion clips. Performances metrics on these motion clips are collected, and based on the performance motion clips are divided in two categories. Motion clips successfully tracked and motion clips that result in failure. Early termination to decide whether a motion clip is successfully tracked or not. 100 environments are used for training, and following a drawing without re-drawing, at the beginning of each episode, 70 environments are sampling failure motion clips and 30 successful motion clips.

% -  ADD a pseudo code of the sampling scheme????????


\section{Results}




\subsection{Implementation Details}
We develop the proposed framework using RaiSim~\cite{raisim}. We use the integrated implementation of Proximal Policy Optimization~\cite{schulman2017proximal} for learning. Our neural network policy has two layers of [256, 256] hidden neurons with LeakyReLu activation functions. We select an A1 quadrupedal robot~\cite{unitree} from Unitree as an experimental platform. We conduct all the experiments using a desktop with AMD Ryzen Threadripper 3970X 32 cores CPU, and RTX 3090. Using AMS a policy
trained from scratch on 701 motions takes about 40 hours
with 100 environments, 30 threads, and episodes of length
10 seconds. For the motor gains, we choose a proportional
gain $k_p=50.0$ and a derivative gain $k_d=2.0$ in order
to support more stable learning and smoother motions. The
entropy coefficient is chosen as $\epsilon=0.0001$, the policy is
queried every 0.02s and the motion references are played at
a frequency of 1kHz.


% Using AMS a policy trained from scratch on 701 motions takes about 40 hours with 100 environments, 30 threads, and episodes of length 10 seconds to track the motion clips. After 40 hours the policy exhibits smooth joint position trajectories and tracks the CoM well. End-effector tracking is harder to learn and takes about 80 hours to master. For the motor gains, we choose a proportional gain $k_p=50.0$ and a derivative gain $k_d=2.0$ in order to support more stable learning and smoother motions. The entropy coefficient is chosen as $\epsilon=0.0001$, the policy is queried every $0.02s$ and the motion references are played at a frequency of $1kHz$.

\subsection{Generating Diverse Motions}

Our framework is able to learn a single capable policy that can track a large number of trajectories with great diversity, including walking, turning, jumping, sitting, and lying. Using AMS, the policy can successfully track all 701 motions, and $\approx90\%$ of 47 long random mixed motion clips that are used for validation and to test the ability of our policy to generalize to out-of-distribution motions it has never seen. 
\\\indent An episode length is taken as 10 seconds. Most motions last 10 seconds, but for instance, the star motion lasts 40 seconds. Although the policy is only trained for the first 10 seconds of the motion clip, the policy can successfully track the entire star motion, which supports its generalization capabilities. The policy is able to re-use to some extent the learned locomotion skills to track unseen motion references. This makes the learning faster as training on longer motion clips is not required. Instead, it is possible to train on segments or individual locomotion skills present in a longer motion clip.
\\\indent The policy can track motions that contain a lot of transitions between skills, and sudden changes in yaw, or speed for instance. Indeed, it is able to track a short clip that involves lying down and sitting to demonstrate generalization over non-locomotion tasks. Finally, we demonstrate that our policy can track a very long sequence that involves many different components, including walking, turning, different gaits, speed changes, and more. Note that it will be very difficult to develop a single control policy to execute all the motor tasks included in our testing sequences. Please refer to Fig.~\ref{fig:motions} and the supplemental videos for qualitative evaluation. We will also provide more quantitative analysis in the following section.

% \subsection{Star motion}
% \subsection{Jump}
% \subsection{Speed changes}
% \subsection{Adaptation of the reference motions}
% - Key phases of the reference motions are learned, joint pattern .... but as the policy is learning residual joint positions around a nominal joint position independent from the reference data, the policy adapted slighlty the reference data, removed noise, sliding contacts .... artifacts from the motion synthesis.

% \subsection{Generalization capabilities/zero-shot performances}
% - Would be important to support that training one policy is better than different ones, lead to more generalization.
% - Train one policy on all clusters > test it without training on shape motions.

\section{Analysis} \label{sec:analysis}

% \subsubsection{Orientation information in observations}
% - Rotation matrix form, was shown to work better than a column, quaternion or euler. More robust for orientation tracking
% - Fig: show perf of trained policy on  a turning motion/ jumping that have some yaw, and roll angles variation in the reference data.




\subsection{Influence of Past and Future Target Information in Observations}
\begin{figure}
\centerline{\includegraphics[width=0.47\textwidth]{fig/ref_in_obs/jump/z_ref_in_obs_jump_subplots.pdf}}
\caption{Influence of the past and future trajectory. Our observation space that includes the past and future enables faster learning: even at the 22K-th policy iteration (top right), our design (green) tracks the reference motion better than the baseline design (red). It also results in a better final policy. At the 240K-th policy iteration (bottom right), our design can track all five jumps while the baseline misses the last jump as highlighted in the blue box centered at around 8.0 seconds.}
\label{fig-ref-in-obs}
\end{figure}
Inspired by other works~\cite{won2020, peng2020learning}, the policy is provided with future and past information of the reference to track. The current reference frame is excluded in order to incentivize the agent to interpolate between the different keyframes in order to prevent the policy from overfitting on the low-level kinematic reference data. Fig. \ref{fig-ref-in-obs} shows that this configuration encourages a faster learning and higher quality learned (smoother, more symmetric) joint trajectories. Fig. \ref{fig-ref-in-obs} presents the CoM height tracking for a policy with only the present reference information and a policy with past and future information. First, we find that our design of past and future reference converges faster, as green curves (ours) are closer to the blue-dotted reference motion than red curves (the baseline design with only the current frame) in early policy iterations. Even after a long-enough training with 240k iterations, the policy with the baseline observation design misses the final jump and decides to run through. We hypothesize that past and future trajectory information is critical to plan ahead dynamic jumping motions.

 \begin{figure}
\centerline{\includegraphics[scale=0.36]{fig/ref_in_obs/jump/3-frames.pdf}}
\caption{Selected key frames of a dynamic jump tracking: the fastest jump present in the dataset at a maximum CoM speed of $1.76 m/s$. 
The baseline policy shows reactive and awkward behaviors for balancing (\textbf{top}) while our observation design leads to more natural and smooth jumping motions (\textbf{bottom}) .}
\label{fig-jump-key-frames}
\end{figure}

A qualitative comparison is presented in Fig. \ref{fig-jump-key-frames}. The baseline observation with only the current frame leads to a reactive behavior that uses its rear legs awkwardly in order to balance. In fact, we observe this behavior consistently over all 200 jumping motions the baseline was trained on. On the other hand, our observation space with past and future reference information finds smooth and natural jumping behaviors which can handle even multiple jumps, and even exhibits alterations from the kinematic reference that look as natural as the reference (See Fig. \ref{fig-jump-key-frames}).

% Similarly to \cite{won2020, peng2020learning}, the policy is provided with future information of the reference to track. Past and future information of the reference to track are given, but not the current reference data to incentivize the agent to interpolate between the different keyframes in order to prevent the policy from overfitting on the low-level kinematic reference data. Fig. \ref{fig-ref-in-obs} presents the CoM height tracking for a policy with only the present reference information and a policy with past and future information. Fig. \ref{fig-ref-in-obs} exemplifies that using the past and the future information of the reference data makes the learning faster, as the policy learns to track the CoM height earlier in the training. Around 240K training iterations, the policy which has not access to the past and future information of the reference has still not learned to execute the last jump correctly. Overall, the performance of the two policies is similar when considering the training curves, i.e. an averaged tracking error. However, the policy with the present reference data performs worse on jumping motions, which require more planning given their aerial nature. The contact has to be planned well in order to land at the right location. The initial tacking-off will also influence the entire aerial trajectory, during which the robot will not be able to have contact with the ground to change its course. Both policies performed similarly on the rest of the motions. Adding reference data in the observations was key to learn more aerial motions such as jumping motions.
% \\\indent Adding the future and past information in the observation seems to produce smoother, and more natural joint trajectories while preventing the policy from merely reproducing the joint position of the kinematic reference. It can be hypothesized that the policy that has access to the past and future of the reference data performs some kind of interpolation between the target key frames. The policy does not just track the current key frame, but rather a combination of them, which explains the better tracking of aerial motions such as jumps. Fig. \ref{fig-jump-key-frames} corroborates that providing more information of the reference in the observations will produce policies that are closer to the target joint positions. The policy observing only the current target state has a more reactive control, producing jittery motions, and some unsymmetrical use of its rear legs. Fig. \ref{fig-jump-key-frames} shows that the policy using only the current key frame uses its rear legs awkwardly in order to balance. This reactive strategy was found in all jumping motions.






% \subsection{Pre-training}

% - Using ADS it seems that it is better to train on all the data, and training on more diverse/rich data seems to make nicer joint positions emerge than when training on very reduced motions. The robot tends to adopt a strategy that is not very natural and close from a smooth locomotion behavior.
% - Hence pretraining is not required with our method. It is not needed to add motion cluster successively. Without ADS it is impossible to learn all the motions.

\subsection{Influence of Action Prior}

\begin{figure}
\centerline{\includegraphics[width=0.4\textwidth]{fig/robustness/friction/xy_trajectory_ref_in_obs.pdf}}
\caption{Generalization over unseen frictions of 0.3 (red) and 1.5 (blue). We examine policies with and without action prior. The policies without action prior (ours, solid lines) show better robustness, while the policy with action prior (baseline, dotted lines) shows larger tracking errors and even cannot complete the sequence. Circles represent the location where the episodes end.}
\label{fig-robust-friction}
\end{figure}

There exist two common choices of action spaces in the literature of motion imitation. The first is to define it as the delta to the current frame of the reference motion under the expectation that the desirable PD targets are closer to the reference motion (\emph{Action Prior}). The second is to make it independent from the reference motion, such as the delta to the fixed nominal pose (\emph{No Action Prior}, ours). In our experience, a policy without action prior shows much better robustness, particularly when it is combined with our joint-agnostic reward design (Eq.~(\ref{eq:reward})).

Fig.~\ref{fig-robust-friction} illustrates well the generalization capability of policies over unseen surfaces with low (0.3, red) and high (1.5, blue) lateral friction coefficients ($\mu$), whereas the policy is trained with $\mu=0.8$. In our experience, learning with \emph{action priors} overfits to track the joint motions and does not generalize well when the robot starts to deviate from the desired trajectory. As a result, the policy exhibits high tracking errors (dotted lines) and even terminates early. On the other hand, our learning formulation without both action prior and joint tracking objective allows the policy to show more robust behaviors to complete complex star-shaped trajectories (solid lines).


% \begin{table}
% \caption{Environment and Dynamics randomization on the star motion clip.}
% \begin{center}
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% Policy & Action repeat & Base mass & Terrain Friction\\
% & & multiplicator $\alpha$ & \\
%  & & $\Tilde{m}=\alpha m$ & $\mu$\\
% \hline
% Action prior & $\leq 10\%$ & [0.85, 1.0] & \\
% \hline
% No action prior &  \textbf{max tested 30\%} & \textbf{[0.4, 2]} & \textbf{[0.3, 2]}\\
% \hline
% \end{tabular}
% \label{tab1-dataset}
% \end{center}
% \end{table}

% In order to illustrate that our approach enables to limit overfitting on the joint positions, i.e. the kinematic low-level of the reference motion we test our policy with different environmental, dynamics parameters. Our results show that our formulation which does not use prior in the action space enables the policy to adapt the low-level joint positions in order to track the high-level reference state (CoM, End-effector, base orientation). Overfitting policies try to track the low-level joint positions at the expanse of the high-level tracking. Fig. \ref{fig-robust-friction} points out that when randomizing the friction coefficient of the terrain, the policy with action prior, i.e. learning joint residuals around the reference joint positions, fails to adapt the joint positions to track the high-level of the reference motion clip.



\subsection{Adaptive sampling}

Finally, this subsection analyzes the importance of our adaptive motion sampling (AMS). In our experience, AMS is critical to cover a large number ($\sim700$) motions without missing a few outlier motions, such as jumping, lying, and sitting. For instance, we have $200$ pace motions while having only $10$ of jumping motions. Therefore, naive sampling will likely prioritize pace motions. 

We plot (1) the average episodic reward over time and (2) the number of failed motions in Fig.~\ref{fig-ads-training-curve}. From the perspective of the conventional reward curve (top), it seems that AMS performs suboptimally with slightly lower episodic rewards. However, please note that AMS puts a policy in tougher scenarios by sampling harder tracking problems more often, and we cannot directly compare the reward function. Therefore, we also plot the number of the failed trajectories out of $701$ motions at the bottom of Fig.~\ref{fig-ads-training-curve}, as a more fair comparison criterion. It shows that our AMS fails less over by not ignoring some minority motions.



\begin{figure}[htbp]
\centerline{\includegraphics[width=0.47\textwidth, height=0.3\textwidth]{fig/AMS/average_ll_performance.pdf}}
\centerline{\includegraphics[width=0.47\textwidth,, height=0.3\textwidth]{fig/AMS/AMS_failed_mocap.pdf}}
\caption{Adaptive Motion Sampling (AMS): comparison of training results with a policy trained with AMS and one policy without AMS. AMS looks suboptimal in terms of the episodic reward (\textbf{top}), but it actually successfully tracks a lot more motions than the baseline without AMS (\textbf{bottom}).}
\label{fig-ads-training-curve}
\end{figure}

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=0.47\textwidth,, height=0.3\textwidth]{fig/ADS/ADS_failed_mocap.png}}
% \caption{Adaptive sampling : evolution of the number of motion clip for which the tracking resulted in an early termination (robot falling) over the training.}
% \label{fig-ads-failed-mocaps}
% \end{figure}
% - Show why it is important when you train a policy on a diverse set of motions/ a dataset that might be unbalanced (more walking than jumping for instance)
% - Fig; showing the number of mocap teacked until the end over the training.
% - Fig: to prove the performance of ADS you could compare the tracking performance on some walking motions which are well represensented in the dataset and some more scarce motion such as the the star motion which is only represented once in the dataset.
% - Fig: if time you could add a figure showing how the number of envs 70/100 for now used to sampled motion clips that are not tracked successfully, is influencing the learning
% - Fig: experiment comparing a policy trained on all data with and without ADS.
% - Fig: compare results of a policy trained on walking dataset of 50 motions, 50 trot, and add 10 jumping motions to illustrate that it can deal with dataset imbalance. Or even 1 jump reference.
% - Mention that different pools of motion clips could be defined, more than just the 2 here, and also that other metrics could be used to define them.
% - ADS Seems to also improve the motion quality by avoiding overfitting, constantly adding novelty in the training and balancing it with already learned skills. The learned strategies have to work for all learned motions.



% - Fig : illustrate sim2real capabilities/transferability of a policy that learns to modulate  the reference data and another from scratch.
% - Table: presenting sim2real success for different env param ranges, like friction, motor strength, mass, inertia >>> to prove or discuss possible deployment
% - Maybe discuss deployment challenges: on board sensing, regularization, domain gap ....



% \section{Deployment}
% \subsection{On board sensing}
% - Observation space easier to adapt to hardware than DeepMimin, we don,t have any realtive link Position, orientation ....
% \subsection{Regularization}
% \subsection{Randomization}
% - DR
% - Reference randomization

\section{Conclusion}
% Summary
We present a scalable motion imitation framework to learn a single policy that can track a large variety of motions, including walking, turning, running, jumping, sitting, and lying. Starting from the existing motion imitation framework~\cite{deepmimic} , we carefully design the observation space, action space, and reward function to improve the effectiveness and robustness of the final policy. In addition, we propose an adaptive motion sampling scheme, which is designed to focus on the learning of more challenging trajectories and to avoid catastrophic forgetting of the previously learned skills. We successfully train a very versatile single policy from a large number of trajectories. We demonstrate that it can also  generalize well to novel trajectories to execute a complex, long motion sequence that involves many different motor skills. In addition, we also showcase that the learned policy is robust against the change of environment parameters such as lateral friction. We finally analyze the importance of our problem formulation and adaptive motion sampling by conducting a set of experiments.

% Future work
There exist several interesting future research directions that we want to explore. For instance, we plan to add more reference motions to the database for non-trivial tasks, such as stair climbing, crawling, and walking over rough terrains. However, the current data generation scheme of motion retargeting will have limitations because it relies on the existing public motion capture data set of a real dog. One possible solution is to add more data using the off-the-shelf trajectory optimization framework~\cite{winkler2018gait}, which can generate physically valid trajectories for various environments. Once we increase the size of the database, we may need to even further improve the scalability of the current learning framework. It can be approached by adopting more parallelized reinforcement learning algorithms~\cite{wijmans2019dd}, investigating novel policy architecture~\cite{kumar2022cascaded}, structuring the dataset~\cite{won2020} or adopting the framework of adversarial learning~\cite{peng2021amp}.

Our obvious next step is to deploy the learned policy on the real hardware of the A1 robot. We expect that the learned policy needs to cross a large sim-to-real gap, which can be approached by system identification or domain randomization. However, domain randomization may also increase the difficulty of the problem, and the learning may not converge effectively. In this case, we may want to pretrain a policy without domain randomization and fine-tune the policy in randomized environments.



% \subsection{Maintaining the Integrity of the Specifications}

% The IEEEtran class file is used to format your paper and style the text. All margins, 
% column widths, line spaces, and text fonts are prescribed; please do not 
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}
% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.

% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\gamma\label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Fig. captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Fig.s and Tables}
% \paragraph{Positioning Fig.s and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Fig. captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Fig. Labels: Use 8 point Times New Roman for Fig. labels. Use words 
% rather than symbols or abbreviations when writing Fig. axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.

% \begin{thebibliography}{00}
% % \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% % \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% % \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% % \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% % \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% % \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% % \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}

\bibliographystyle{ieeetr}
\bibliography{ref}

\end{document}
