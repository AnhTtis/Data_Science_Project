\section{Conclusion}
\label{sec:Conclusion}
We have presented a novel 3D renderer highly suited for human reconstruction and synthesis.
By design, our formulation encodes a semantically and physically meaningful latent 3D space of parts while our novel feature rendering approach translates these parts into 2D images giving rise to a robust and easy to optimize representation.
A encoder-decoder architecture allows us to transfer style and move from feature images back into the image space.
We illustrated the versatility of our approach on multiple tasks: semi-supervised learning; novel view synthesis; and style and motion transfer, allowing us to puppet one person's movement from any viewpoint.

Our model allows the synthesis of high quality images of known subjects, and while it can be further fine-tuned on unknown subjects, it is not currently possible to generalise over the space of all human appearances.
An important direction for future work would be to investigate the usage of additional biometric cues in order to improve the degree of realism of the reconstructed images, as well as to provide a better structure of the appearance space, which is a first step towards the generalisation of unseen subjects.

While we trained our framework with in-the-lab data, we think it could be applied with little changes to in-the-wild environments, since the off-the-shelf 2D pose estimator is robust to cluttered environments, and while the appearance extractor might struggle for extracting meaningful features, we should be able to overcome this by removing the complex background, using an instance segmentation tool as a pre-processing step to the input image.

Although we focus on human reconstruction, we believe our approach of rendering a small number of tractable and semantically meaningful primitives as feature images to be useful in a wider scope.
Potential applications include semantic mapping, reconstruction and dynamic estimation of the poses of articulated objects and animals, and novel view synthesis of rigid objects.
