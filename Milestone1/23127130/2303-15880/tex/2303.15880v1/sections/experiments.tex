\section{Experiments}
\label{sec:Experiments}

\subsection*{Implementation}
\label{sec:Experiments:Implementation}

Our framework is implemented in PyTorch and trained end-to-end, with the exception of the 2D detector, which is OpenPose~\cite{cao2017realtime}.
We use a slightly modified version of~\cite{martinez2017simple} to infer the 3D pose from the 2D landmarks and a ResNet-50~\cite{he2016deep} to infer the appearance from the input image.
In both networks, Group Normalization~\cite{wu2018group} replaces Batch Normalization~\cite{ioffe2015batch}.
The renderer presented in \cref{sec:Methodology:Rendering} is simply a differentiable function without any learnt parameters.
To transfer the rendered image into the output image, we design a U-Net encoder-decoder~\cite{ronneberger2015u}, which follows StyleGAN2~\cite{karras2020analyzing} style mixing and design principles.
We use a patch discriminator~\cite{isola2017image} following StyleGAN2 design principles.
Our models are trained with a batch size of $32$, using AdamW~\cite{loshchilov2017decoupled}, with a learning rate of $2 \cdot 10^{-3}$ and a weight decay of $10^{-1}$.
The appearance vectors are 16 dimensional. 
For the differentiable renderer, we set $\alpha = 2.5 \cdot 10^{-2}$, $\beta = 2$ and $b = 0_{A}$.
The segmentation masks used for cropping the background out of the groundtruth target images are obtained with SOLOv2~\cite{wang2020solov2}.
Images are loosely cropped around the subject to a resolution of ${1080}^{2}$ for Panoptic Studio and cropped given a bounding box of the subject for Human3.6M.
Input images are resized to a resolution of ${256}^{2}$, high-dimensional latent images are rendered at a resolution of ${256}^{2}$, and output images are resized to either ${256}^{2}$ or ${512}^{2}$.

\subsection*{Datasets}
\label{sec:Experiments:Datasets}

\subsubsection*{Panoptic Studio}
\label{sec:Experiments:Datasets:Panoptic}

 Joo \etal~\cite{joo2017panoptic} provides marker-less multi-view sequences captured in a studio.
There are over 70 sequences captured from multiple cameras, including 31 HD cameras at 30 Hz.
Some sequences include multiple people engaged in social activities, such as playing music, or playing games, while other sequences focus on single individuals performing diverse tasks.
To avoid problems with matching people across different viewpoints, we restrict ourselves to single person sequences, and use only images recorded by high-definition cameras.
Panoptic Studio presents greater variability in subject clothing, morphology and ethnicity, with the most significant detail being that it was captured in a marker-less fashion, compared to datasets such as Human3.6M~\cite{ionescu2013human3}.
Its high variability in camera poses is beneficial both for novel view synthesis and when demonstrating the 3D understanding and robustness of the model. 

We infer 2D poses by running OpenPose over every view of a sequence, and reconstruct 3D pose following the approach detailed by Faugeras~\cite{faugeras1993three}, which computes a closed-form initialization followed by an iterative refinement.
We remove consistently unreliable 2D estimates and obtain a $117$-point body model. 

Data is partitioned at the subject level into training, validation, and test sets, with each subject in only one of the sets.
Approximately $80\%$ of the frames are used for training, $10\%$ for validation and $10\%$ for testing. This corresponds to 29 subjects for training, 4 for validation, and 4 for testing.
We require pairs of images for training, therefore we have $|\mathcal{E}| = |\mathcal{F}| \times {|\mathcal{V}|}^{2}$ possible pairs, with $\mathcal{F}$ and $\mathcal{V}$, referring to the sets of available frames and views respectively.
This gives us a total of $81.6$M pairs of images for training, $11.7$M pairs for validation and $12.7$M for testing.

\input{tables/ablation_study}
\input{figures/ablation_study}

\subsubsection*{Human3.6M}
\label{sec:Experiments:Datasets:Human36M}

Human3.6M~\cite{ionescu2013human3} is one of the largest publicly available datasets for 3D human pose estimation.
It contains sequences of 11  actors performing 15 different scenarios recorded with 4 high-definition cameras at 50 Hz.
Mo-Cap is used to provide  accurate ground truth.
We use the provided $17$-joint skeletal model.
Following previous work, data is partitioned at the subject level into training (S1, S5, S6, S7 and S8) and validation on two subjects (S9 and S11).
This gives us a total of $6.2$M pairs of images for training, $2.2$M pairs for validation.

\subsection{Ablation Study}
\label{sec:Experiments:AblationStudy}

We evaluate the contribution of each loss for the view synthesis task, reporting the PSNR, SSIM~\cite{wang2004image} and LPIPS~\cite{zhang2018unreasonable}.
For each sequence, we sample two subsequences representing $10\%$ of the whole sequence, for validation and testing, and use the remaining frames for training.
We select models by looking at the validation error, and report the results on the test set.
As expected, \cref{tab:AblationStudy} shows that, the models trained with the LPIPS loss achieve a better LPIPS score, while the models trained with the pixel and VGG losses have lower PSNR and SSIM error.
However, when we look closely at the qualitative examples, we can see on \cref{fig:Experiments:AblationStudy} that the models trained with the LPIPS loss contain  more high-frequency detail, and that the adversarial loss enables finer levels of detail to be reached in the images.
We notice a sharp gap in performance between the models trained on Panoptic Studio and Human3.6M, which we attribute to the granularity of the skeletal model, on Panoptic Studio, the skeletal model provides detailed information about the hands and face, while on Human3.6M, face and hands are represented by a single point.

\input{figures/motion_transfer}

\subsection{Motion Transfer}
\label{sec:Experiments:MotionTransfer}

To demonstrate the versatility of our approach, we apply it to whole body motion transfer.
\Cref{fig:Experiments:MotionTransfer} shows how our approach can be used for motion and view transfer from one individual to another.
Given an unseen person $(a)$ from the test partition, we  estimate their pose from a new viewpoint, and extract the appearance of an individual $(b)$ whose style was learned during training.
Since our framework naturally disentangles pose and appearance, without further training, we can combine them and render the image from a novel viewpoint and obtain $(c)$.
We provide $(d)$ as a visual comparison to show that our network is able to extract and render 3D information faithfully.
Despite small errors in the pose, caused by ambiguities in the pose image $(a)$, we reconstruct a convincing approximation of a different person in the same pose.
As such, this work represents an initial step towards full-actor synthesis from arbitrary input videos.

\input{figures/infer_virtual}

\subsection{Synthesizing Images from Unseen Viewpoints}
\label{sec:Experiments:UnknownViewpoints}

As our model is trained on multiple views, it can generate realistic looking images of a subject in unseen poses from virtual viewpoints, \eg where cameras do not actually exist.
As seen in \cref{fig:Experiments:UnknownViewpoints}, we can create virtual cameras travelling on a spherical orbit around the subject.

\input{figures/appearance_learning}

\subsection{Learning Novel View Synthesis of Unknown Subjects from a Monocular Sequence}
\label{sec:Experiments:UnknownSubjects}

While our model generalizes well to the pose estimation task, the number of subjects in both datasets are insufficient for the model to generalize over the space of all human appearances.
However, using a monocular sequence of an unseen subject, we can quickly retrain both the appearance and image synthesis modules to the new individual.
Our model is able to produce novel views of an unseen individual from a single camera.
As we see in \cref{fig:Experiments:AppearanceLearning} appearance fine-tuning shows a clear visual improvement, however it is more effective on Human3.6M than Panoptic Studio.
This is a consequence of subjects facing in one direction only in Panoptic Studio.
As such, novel view synthesis requires estimating the appearance of completely unseen parts.

\input{figures/fused_inference}

\subsection{Refining View Synthesis by Fusing the Pose and Appearance from Multiple Input Viewpoints}
\label{sec:Experiments:Refining}

Our model is able to synthesize high quality images. However, it is still subject to errors, especially when the 3D pose is incorrect.
Recall that the 3D pose is recovered in a two-step approach, first the 2D pose is inferred from the input image using an off-the-shelf detector~\cite{cao2017realtime}, which is then passed to a 2D-to-3D regression model~\cite{martinez2017simple} (see Fig~\ref{fig:Methodology:BlockDiagram}).
This top-down approach is error-prone, and error in the 2D landmarks are likely to contaminate the 3D skeleton.
Additionally, the appearance extracted from a single viewpoint will not be sufficient unless the change in viewpoint between the input and output view is small.
Therefore a question naturally arises, does fusing the information from multiple input views help enhance the image synthesis quality?
To answer this we propose to infer the pose and appearance from neighbouring viewpoints, rotate the inferred poses back into the original input view, and average the poses and appearances.
We re-write \cref{eq:Methodology:FeatureExtraction:ImageToPose2D,eq:Methodology:FeatureExtraction:Pose2DToPose3D},
\begin{align}
\label{eq:Methodology:FeatureExtraction:ImageToPose3D:Fused}
    \ &
    I_{i}^{*}
    \rightarrow
    p_{i}, c_{i}
    \rightarrow
    \bar{P}_{i}
    \quad
    \forall i = 1 .. N, \\
    \ &
    \bar{P}_{1} = \frac{1}{N} \sum_{i=1}^{N}{R_{i \rightarrow 1} \times \bar{P}_{i}}.
\end{align}
As well as modify \cref{eq:Methodology:FeatureExtraction:ImageToAppearance},
\begin{align}
\label{eq:Methodology:FeatureExtraction:ImageToAppearance:Fused}
    \ &
    I_{i}^{*}
    \rightarrow
    a_{i}
    \quad
    \forall i = 1 .. N, \\
    \ &
    a_{1} = \frac{1}{N} \sum_{i=1}^{N}{a_{i}}.
\end{align}
We evaluate the evolution of both the pose estimation and synthesis quality over the test partition while increasing the number of neighbouring views presented to the model.
We report the LPIPS, PSNR and SSIM metrics for image reconstruction quality, for the model trained on Panoptic Studio~\cite{joo2017panoptic}, we report the MPJPE for the four main body regions, body, left and right hands and face, as the error magnitudes between the various region are very different, and for the model trained on Human3.6M~\cite{ionescu2013human3} we report the MPJPE, N-MPJPE and P-MPJPE.

\cref{fig:Experiments:FusedInference:Panoptic,fig:Experiments:FusedInference:Human36M} show that as the quality of the 3D pose improves, the image synthesis does as well, which is expected as errors and ambiguities in estimating the 3D pose in the input view will in turn contaminate and worsen the output image synthesis process.
Fusing pose information from multiple views yields the largest improvements for the body pose, because it is the least "rigid" region, while hands and face have a much bigger prior.
While all the curves are monotonically increasing, we observe diminishing returns to adding neighbouring views.
Roughly one third of the maximal improvement possible is already reached by adding the information of a single neighbouring view, and two thirds of the improvement in the pose error, and in turn the synthesis error, is realised with only 5 neighbouring views.
Moreover, we notice that the absolute improvement of the reconstruction quality is much less significant with respect to the PSNR and SSIM metrics than for the LPIPS, which we attribute to the shallowness and frailty of these metrics~\cite{zhang2018unreasonable}, since the PSNR relies on the assumption that the pixels forming the image are independent from each others and the SSIM, while overcoming the pixel-wise independence assumption assumes a Gaussian prior on the pixel distribution while operating in the RGB colour space.
