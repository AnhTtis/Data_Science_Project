\section{Related Work}
\label{sec:RelatedWork}

\subsection*{Human Pose Estimation}
\label{sec:RelatedWork:HumanPoseEstimation}
Locating body parts in an image is inherently hard due to variability of
human pose, and ambiguities arising from occlusion, motion blur, and lighting.
Additionally, estimating depth is a
challenging task that is difficult even for humans due to perspective ambiguities.

Approaches to the 2D human pose estimation problem have gradually shifted from energy-based model fitting methods~\cite{felzenszwalb2005pictorial,andriluka2009pictorial} using hand-crafted features to a pattern recognition task that leverages recent advances in deep learning and large amounts of labeled data~\cite{lin2014microsoft}.
Convolutional Pose Machines~\cite{wei2016convolutional}, based on the work of Ramakrishna \etal~\cite{ramakrishna2014pose}, iteratively refine joint location predictions using the previous inference results.
Cao \etal~\cite{cao2017realtime} improved on this using part affinity fields and enabling multi-person scene parsing.

While 2D human pose estimation is robust to in-the-wild conditions, it is not the case for most 3D approaches, mainly due to the limitations of available data.
Large datasets use either Mo-Cap data, such as HumanEva~\cite{sigal2010humaneva} or Human3.6M~\cite{ionescu2013human3}, or rely on a high number of cameras for 3D reconstruction, such as Panoptic Studio~\cite{joo2017panoptic}.
These datasets are captured in tightly controlled environments and lack diversity, limiting the generalization of models.
Inspired by the structure of the human skeleton, Zhou \etal~\cite{zhou2016deep} proposed kinematic layers to produce physically plausible poses, while Sun \etal~\cite{sun2017compositional} designed a compositional loss function to backpropagate a supervision signal aware of human structure.
Chen and Ramanan~\cite{chen20173d} proposed an example-based method, inspired by~\cite{ramakrishna2012reconstructing}, that matched the 2D pose with a set of known 3D poses, rather than estimating pose directly from images.
Bogo \etal~\cite{bogo2016keep} fitted the parametric 3D body model SMPL~\cite{loper2015smpl} by minimizing the re-projection error of the model using 2D landmarks inferred by DeepCut~\cite{pishchulin2016deepcut}.
Some approaches use multiobjective learning to overcome the lack of variability in the data, by either solving jointly for the 2D and 3D pose estimation tasks~\cite{li20143d}, or fusing 2D detection maps with 3D image cues~\cite{tekin2017learning}.
Recent trends revolve around learning 3D human pose with less constrained sources of data.
Inspired by Pose Machines~\cite{ramakrishna2012reconstructing,wei2016convolutional}, Tome \etal~\cite{tome2017lifting} trained a network with only 2D poses, by iteratively predicting 2D landmarks, lifting to 3D, and fusing 2D and 3D cues.
Martinez \etal~\cite{martinez2017simple} presented a simple 2D-to-3D residual densely-connected model that outperformed complex baselines relying on image data.
Exploiting unlabeled images from multiple cameras and some annotated images, Rhodin \etal~\cite{rhodin2018learning} used geometric constraints to show the benefits of augmenting the training with unlabeled data.
Drover \etal~\cite{drover2018can} proposed an adversarial framework, based on~\cite{martinez2017simple}, randomly reprojecting the predicted 3D pose to 2D with a discriminator judging the realism of the pose.
Chen \etal~\cite{chen2019unsupervised} refined~\cite{drover2018can} by adding cycle-consistency constraints.

\subsection*{Image Synthesis}
\label{sec:RelatedWork:ImageSynthesis}
Generative adversarial networks~\cite{goodfellow2014generative} are excellent at synthesising high-quality images, but typically offer little control over the generative process.
Progressively growing adversarial networks~\cite{karras2017progressive} was demonstrated to stabilize training and produce high-resolution photo-realistic faces.
Style modulation~\cite{karras2019style,karras2020analyzing} enabled the synthesis of fine-grained details, as well as a higher variability in the generated faces.
They can be conditioned to generate images from segmentation masks or sketches~\cite{isola2017image,wang2018high}.
Ma \etal~\cite{ma2017pose} proposed an adversarial framework allowing synthesis of humans in arbitrary poses, by conditioning image generation on an existing image and a 2D pose.
Chan \etal~\cite{chan2019everybody}, built on~\cite{wang2018high}, and proposed combining the motion of one human with the appearance of another, while preserving temporal consistency. However, the approach was limited to the generation between two fixed and similar viewpoints, due to the absence of 3D reasoning.
Such works open up the possibility of `deepfaking' somebody's appearance, while another human drives the motion.

\subsection*{Novel View Synthesis}
\label{sec:RelatedWork:NovelViewSynthesis}
Novel view synthesis is the task of generating an image of a scene from a previously unseen perspective.
Most approaches rely on an encoder-decoder architecture, where the encoder solves an image understanding problem via some latent representation, which is subsequently used by the decoder for image synthesis.
Tatarchenko \etal~\cite{tatarchenko2016multi} presented an encoder-decoder using an image and a viewpoint as input to synthesize a novel image with depth information.
Park \etal~\cite{park2017transformation} refined it by adding a second stage hallucinating missing details.
Rather than synthesizing a novel view of images, Zhou \etal~\cite{zhou2016view} learnt the displacement of pixels between views.
Inspired by Grant \etal~\cite{grant2016deep}, Sitzmann \etal~\cite{sitzmann2019deepvoxels} used voxels to represent the 3D structure of objects and to deal with occlusion.
However, these approaches have difficulties dealing with large view changes, due to the unstructured underlying latent representation.

To account for larger view changes, Worrall \etal~\cite{worrall2017interpretable} structured their latent space to be directly parameterized by azimuth and elevation parameters.
Rhodin \etal~\cite{rhodin2018unsupervised} proposed a framework that disentangles pose, as a point cloud, and appearance, as a vector, for novel view synthesis and is later reused for 3D human pose estimation.
By explicitly structuring the latent space to handle geometric transformations, such models are able to handle larger view changes.
However, these approaches learn mappings to project 3D structures onto images, which may be undesirable as the mapping is specific to the nature of the rendered object.

\subsection*{Differentiable Rendering}
\label{sec:RelatedWork:DifferentiableRendering}
Classical rendering pipelines, such as mesh-based renderers, are widely used in graphics.
They are good candidates for projecting information onto an image, however, they suffer from several major limitations.
Firstly, some operations such as rasterization are discrete and therefore not naturally differentiable.
As a substitute, one can use hand-crafted functions to approximate gradients~\cite{kato2018neural,loper2014opendr}.
However, surrogate gradients can add instability to the optimization process.
Liu \etal~\cite{liu2019soft} proposed a natively differentiable formulation by defining a `soft-rasterization' step.
Another problem with classical approaches is that representing objects as polygonal meshes for rendering purposes creates implicit constraints on the inverse graphics problem, as it requires the movement of vertices to preserve local neighbourhoods and importantly preserve nonlocal constraints of what is the \textit{inside} and \textit{outside} of the mesh.
This is challenging to optimize and requires strong regularization.
A polygonal mesh can be understood as a fixed hyper-graph, where each face corresponds to a hyper-edge that may bind any number of vertices together.
Vertices represent points constituting the object in space, whereas edges characterise the relationships between the vertices, and by extension the nature of the object itself.
Therefore, although the location of vertices can be differentiated, the existence of connecting edges, which are discrete by nature, cannot.
Shysheya \etal~\cite{shysheya2019textured} proposed learning texture maps for each body part to render novel views of humans, using 3D skeletal information as input.

\input{figures/block_diagram}

\subsection*{Neural Rendering}
\label{sec:RelatedWork:NeuralRendering}
Embedding a scene implicitly as a Neural Radiance Function (NeRF) is an emerging and promising approach.
Mildenhall \etal~\cite{mildenhall2020nerf} showed that it was possible to optimize an internal volumetric mapping function using a set of input views of the scene.
Using sinusoidal activation functions, Sitzmann \etal~\cite{sitzmann2020implicit} enabled such networks to learn more complex spatial and temporal signals and their derivatives.
By combining neural radiance fields and the SMPL articulated body model, Su \etal~\cite{su2021nerf} proposed a model capable of generating human representations in unseen poses and views.
A range of dynamic NeRF variants have since sprung up \cite{park2020deformable,pumarola2021d,tretschk2020non}, that simultaneously estimate deformation fields alongside the neural radiance function.
These approaches are shape agnostic, each model embeds the appearance of a single individual, and they are designed to run on short sequences and render novel views from a camera position similar to those previously captured.
More recently, Weng \etal~\cite{weng2022humannerf} proposed a framework rendering high quality images from novel viewpoints of dancing humans from monocular videos, by learning inverse linear-blend skinning coefficients mapping the subject's estimated pose back to a canonical pose followed by a learnt volumetric function of the colour and density.
In contrast, we explicitly train the model to learn the appearance of multiple individuals (29), allowing rendering from arbitrary views of a single frame captured separately, or for motion transfer from one individual to another.
