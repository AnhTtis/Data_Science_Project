\IEEEraisesectionheading{
\section{Introduction}
\label{sec:introduction}
}

\input{figures/teaser}

\IEEEPARstart{W}{e} present a new three-step approach for novel view synthesis and motion transfer, that preserves the biometric identity of individuals (see \cref{fig:Introduction:Teaser}).
We first infer the pose and appearance information of a subject from an image.
The pose is then transferred to a novel view along with the appearance, from which we render diffuse primitives, using a realistic camera model, onto a high-dimensional image of the foreground of the scene.
These diffuse Gaussian primitives are semantically meaningful, simplify optimization and explicitly disentangle pose and appearance.
Finally, we use an encoder-decoder architecture to generate novel realistic images.
Leveraging  multi-view data, we train this framework end-to-end, optimizing both image reconstruction quality and pose estimation.
While our approach is generic and can be applied to many tasks, we focus on human pose estimation and novel view synthesis across large view changes.

Scenes in the real world are three-dimensional, yet we observe them as images formed by projecting the world onto a two-dimensional plane.
Generating a new synthetic image of a scene from one or many is termed \emph{novel view synthesis}.
However, novel view synthesis is a fundamentally ill-posed problem, that we decompose into two parts.
The first involves solving an inverse graphics problem, requiring a deep understanding of the scene, while the second relies on image synthesis to generate realistic images using this understanding of the scene.
Differentiable rendering is an exciting area of research and offers a unified solution to these two problems. It allows the fusion of expressive graphical models, that capture the underlying physical logic of systems, with learning using gradient-based optimization.
Current approaches to differentiable rendering try to directly reason about the world by aligning a well-behaved and smooth approximate model with an underlying non-smooth, and highly nonconvex image.
While such approaches guarantee that gradients exist and that a local minimum can be found via continuous optimization, this is a catch-22 situation -- the more detailed the model, the better it can be aligned to represent the image; however, the more non-convex the problem becomes, the harder it is to obtain correct alignment.

Unlike mesh-based renderers, which represent objects as sets of polygons, we treat them as a collection of diffuse three-dimensional shapes.
In the case of human reconstruction, these shapes correspond to semantically meaningful joint and limb locations (see \cref{fig:Methodology:Overview}).
Such a formulation is naturally tractable, and overcomes the aforementioned limitations of mesh-based renderers, as it removes the constraints imposed on the nature of objects, and the discretisation induced by the edges of the mesh.
As a result, it enables a disentanglement of the pose and appearance of the object, and is simpler than mesh-based differentiable renderers.

A well-known property of neural decoders~\cite{chakrabarty2019spectral, ulyanov2018deep} is that they fit to images in a coarse-to-fine manner; first recovering the low-frequency components that coarsely approximate the image and then recovering the fine details.
A consequence of this design choice is that, as the first and second stages remain smooth and easy to optimize, the overall formulation is naturally self-adapting, remaining coarse, approximately convex and easy to optimize when rendering does not closely follow the image.
However, in the latter stages, when it is more detailed and susceptible to the choice of initialization, the primitives are already close to a good location.

Disentangling shape and appearance of an image is fundamental for general 3D understanding and lies at the heart of our idea.
If we focus on synthesizing novel views of humans, the localization of human joints in the three-dimensional space can be seen as a first step towards human body shape estimation.
If we simultaneously estimate the appearance of these body parts, then once we have extracted pose and appearance, we can transfer the information to a novel view and synthesize the output image.

Synthesizing a novel view of the scene is simply the act of projecting the scene from the three-dimensional world into an image.
This generative process, also known as rendering in computer graphics, is crucial, as the degree of realism of the generated image relies on it.
In addition to the objective of photo-realism, another aim of novel view synthesis is to faithfully reflect information contained in the original view.
This requires accurate localization of the human joints, since errors in localization are likely to induce large errors when transferred to other views.

We present a novel rendering primitive that allows us to take full advantage of recent progress in 3D human pose estimation and encoder-decoder networks to gain a higher degree of control over actor rendering, allowing us to synthesize known individuals from arbitrary poses and views. The key insight that allows this to take place is the use of diffuse Gaussian primitives to move from sparse 3D joint locations into a dense 2D feature image via a novel density renderer. An encoder-decoder network then maps from the feature space into RGB images (see \cref{fig:Methodology:Overview}). By exploiting the simplicity of the underlying representation, we can generate novel views while `puppeting' the actors, re-rendering them in novel poses, as shown in \cref{fig:Experiments:MotionTransfer}.      

This manuscript revises and extends~\cite{rochette2021human} adding additional details to methodology including aspects of architectural design and further details on the differentiable rendering. Furthermore we include additional explanation on the relationship between the rendered and synthesised images and provide additional experimental validation.
\input{figures/overview}

\Cref{sec:RelatedWork} gives an overview of the literature related to human pose estimation and novel view synthesis.
\Cref{sec:Methodology} presents our approach for estimating the pose and synthesizing novel views of humans and our novel differentiable renderer.
\Cref{sec:Experiments} contains experiments performed on the Panoptic Studio and Human3.6M datasets, evaluating pose estimation accuracy and image reconstruction quality, as well as motion transfer between humans.
