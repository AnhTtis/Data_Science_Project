%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}
\IEEEoverridecommandlockouts
\overrideIEEEmargins
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
%\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% My own packages etcetera.
\usepackage{graphicx}
\graphicspath{{../figures/}}
\usepackage{hhline}
%\usepackage{algorithmic}
%\usepackage[ruled,vlined,titlenotnumbered]{algorithm2e}
\usepackage{caption,subcaption}
\usepackage{url}
%\usepackage{enumitem}
\usepackage{enumerate}
%\usepackage[shortlabels]{enumitem}

%\newcommand{\depth}{\mathrm{depth}}

% ----------------------- Algorithm things.
\usepackage{algorithm}
\usepackage{algpseudocode}
%\usepackage{algorithmic}
\usepackage{algorithmicx}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
% ----------------------- Algorithm things.

\pdfobjcompresslevel=0

\input{h_com}

% This is to adjust if long or short version should be generated.
\def\longversion{1} % 1 for long version, 0 for short version.

% Tikz things for the block diagrams.
\usepackage{tikz}
\usetikzlibrary{shapes.geometric} % new for flow charts to the platoon.
\usetikzlibrary{calc}
\usetikzlibrary{fit}
\usetikzlibrary{arrows,automata}

\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text badly centered, draw=black]
\tikzstyle{cloud} = [draw, ellipse, node distance=3cm, minimum height=2em]
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=1cm, minimum height=1cm,text centered, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]

\tikzstyle{block} = [draw, rectangle, 
    minimum height=3em, minimum width=2.9cm, text centered]
\tikzstyle{sum} = [draw, fill=blue!20, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

\begin{document}
%Efficient Optimal Planning in \\ Hierarchical Finite State Machines
%Efficient Optimal Planning in Large-scale Systems Modelled as Hierarchical Finite State Machines 
%Efficient Optimal Planning in Hierarchical Finite State Machines Using Modular Reduction
%Optimal Planning in Hierarchical Finite State Machines using a Reduced-solve-expand Approach
%Efficient Optimal Planning in Large-scale Systems Modelled as Hierarchical Finite State Machines

\title{\LARGE \bf
Efficient and Reconfigurable Optimal Planning in Large-Scale Systems Using Hierarchical Finite State Machines
}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Elis Stefansson$^{1}$ and Karl H. Johansson$^{1}$% <-this % stops a space
\thanks{$^{1}$School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Sweden. Email:
        {\tt\small \{elisst,kallej\}@kth.se}. The authors are also affiliated with Digital Futures.}%
\thanks{This work was partially funded by the Swedish Foundation for Strategic Research, the Swedish Research Council, and the Knut och Alice Wallenberg~foundation.}% <-this % stops a space
}

\maketitle

\iffalse
\author{Elis Stefansson$^{1}$ and Karl H. Johansson$^{1}$ % <-this % stops a space
%\thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Sweden. Email: \{elisst, kallej\}@kth.se}%
\thanks{This work was partially funded by the Swedish Foundation for Strategic Research, the Swedish Research Council, and the Knut och Alice Wallenberg foundation.}
}
\fi


%\begin{document}



\maketitle
%\thispagestyle{empty}
%\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
In this paper, we consider a planning problem for a large-scale system modelled as a hierarchical finite state machine (HFSM) and develop a control algorithm for computing optimal plans between any two states. The control algorithm consists of two steps: a preprocessing step computing optimal exit costs for each machine in the HFSM, with time complexity scaling linearly with the number of machines, and a query step that rapidly computes optimal plans, truncating irrelevant parts of the HFSM using the optimal exit costs, with time complexity scaling near-linearly with the depth of the HFSM. The control algorithm is reconfigurable in the sense that a change in the HFSM is efficiently handled, updating only needed parts in the preprocessing step to account for the change, with time complexity linear in the depth of the HFSM. We validate our algorithm on a robotic application, comparing with Dijkstra's algorithm and Contraction Hierarchies. Our algorithm outperforms both.
\end{abstract}

%\begin{IEEEkeywords}
%To be written. % Some keywords.
%\end{IEEEkeywords}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}

\subsection{Motivation}
In today's smart societies, with increased integrability and connectivity, there is an increasing need for efficient and optimal control of large-scale systems. Here, control algorithms should not only compute optimal plans for such systems efficiently, but also be reconfigurable, i.e., be able to handle changes in parts of the system with ease. 

A common approach is to consider modular systems, i.e., systems that can be decomposed into independent entities (modules). For such systems, the idea is to construct control algorithms that can separate the computation into these modules and then combine the results from the modules to obtain an optimal plan for the whole system. Moreover, a change in a modular system typically affects only some of the modules, enabling control algorithms to be reconfigurable by only updating the corresponding affected parts in the~algorithm.

%Such modularity can also naturally account for changes in the system, updating only the modules affected by the change.

In this paper, we consider large-scale systems formalised by hierarchical finite state machines (HFSMs) \cite{harel1987statecharts}. An HFSM is a machine composed of several finite state machines (FSMs) nested into a hierarchical structure. This structure naturally makes an HFSM modular. The key research question is how to exploit the modularity in the HFSM to construct optimal control algorithms that are efficient and~reconfigurable.

\subsection{An Illustrative Example}\label{an_illustrative_example}
Consider an example where a robot is moving between lab houses, schematically depicted in Fig. \ref{fig:robot_example_detailed_overview}. In this environment, the robot is assigned to scan a tube, located in one of the houses, with minimal operational cost. Therefore, the robot must compute a plan that with minimal cost makes it reach the tube and scan it. 

Here, a key insight is that the environment can be naturally modelled as an HFSM with three layers corresponding to the layers in Fig. \ref{fig:robot_example_detailed_overview}, where Layer 1 prescribes how the robot can move between the houses, Layer 2 how the robot moves inside a house, and Layer 3 how the robot can operate the lab desk and scan tubes at a given location inside a house (all formally modelled by FSMs). The first key question is then \emph{how to construct a control algorithm that computes an optimal plan efficiently, by exploiting the hierarchical structure of the~HFSM.}

\begin{figure}
	\centering
  \includegraphics[width=0.4\textwidth]{robot_example_detailed_overview_v2}
  \caption{Robot application example modelled as an HFSM with three layers.} 
  %In each warehouse, the robot can move around to different locations, and at each location it can do different tasks, with decision costs given by some cost~functional.
  \label{fig:robot_example_detailed_overview}
\end{figure}

Suppose now that one of the houses changes, for example, some locations in House 2 are blocked due to maintenance work, as depicted by grey areas in Fig. \ref{fig:robot_example_change_overview}. This changes the HFSM, and may thus affect the optimal plan computed by the robot (e.g., if the computed optimal plan was to go to the bottom-right location in House 2, then the robot has to take a detour now to avoid the blocked locations). However, only a part of the HFSM has been changed and hence, the control algorithm of the robot should not need to reset all its computations, rather, it should only need to update the relevant parts. That is, the control algorithm should be reconfigurable. The second key question is then \emph{how to construct a control algorithm that is reconfigurable given changes in the HFSM.}

%In other words, the robot's control algorithm for computing an optimal plan should be reconfigurable. 

%The MMs in the HFSM, which we call an Hierarchical Mealy Machine (HiMM), also interacts with each other. Here, we follow \cite{} for simplicity, specifying that the robot moves according to the picked action if such a transition exits for the current MM, otherwise the robot quits the current MM moving up to the MM above in the hierarchy, checking if a transition exits there (and so on). For example, a transition for going left at S in house 2 does not exits, but it does for the MM above moving the robot to House 1, more precisely the robot goes to $S$ in House 1.

%the robot is in some state $q$ in some MM $M$, picking an action $a$, then the robot either moves according to the transition in $M$ if it exists,

%noticing that the nested MMs in the HiMM can naturally be arranged into a tree (e.g. the MM of House 2 is a child of the MM corresponding to all houses), 

%(but a bit technical) manner, hence we postpone these details till Section (?). 

%Moreover, the interaction between the MMs in the HFSM can be modelled in several ways. Here, we follow \cite{} noticing first that the nested MMs in the HiMM can naturally be arranged into a tree, where e.g. the MM of House 2 is a child of the MM corresponding to all houses. Intuitively, 

%Consequently, we call the HFSM an Hierarchical Mealy Machine (HiMM). 

%For example, the top layer with the houses is an MM with 10 states and transition function depicted by the arcs.  

% Here I should not only talk about how one can model a system but also how the system might be subject to change, become a part in a larger system, or be modified.

\begin{figure}
	\centering
  \includegraphics[width=0.49\textwidth]{robot_example_change_overview}
  \caption{Change in House 2 in the HiMM given by Fig. \ref{fig:robot_example_detailed_overview}. Gray areas depict blocked locations.} 
  %In each warehouse, the robot can move around to different locations, and at each location it can do different tasks, with decision costs given by some cost~functional.
  \label{fig:robot_example_change_overview}
\end{figure}

\subsection{Contribution}
The main contribution of this paper is to formalise optimal planning in systems modelled as HFSMs, introduce changes in such systems, and provide an efficient and reconfigurable control algorithm computing optimal plans. More precisely, our contribution is~three-fold:

%First, we do awesome modelling
First, we formalise the HFSM systems extending the setup of \cite{biggar2021modular} to machines with outputs, i.e., Mealy machines (MMs) \cite{Mealy1955}, calling the resulting HFSM for a Hierarchical Mealy Machine (HiMM), following the formulation in \cite{stefansson2023ecc}. We then formalise changes in HiMMs, called modifications, which can change any HiMM into any other HiMM (by a series of modifications), and define what we mean for an algorithm to be reconfigurable with respect to modifications.  

%Second, we propose a cool algorithm
Second, we propose a control algorithm that is both efficient and reconfigurable, extending the  algorithm in \cite{stefansson2023ecc} to be reconfigurable. The algorithm is divided into two steps: A preprocessing step where optimal exits costs are computed for all MMs in the given HiMM $Z$ (done only once for a fixed HiMM), with time complexity $O(N)$, where $N$ is the number of MMs in $Z$; and a query step, rapidly computing an optimal plan between any two states in $Z$ using the optimal exit costs, with time complexity $O(\mathrm{depth}(Z) \log ( \mathrm{depth}(Z)))$ for computing the next optimal input (compared to Dijkstra's algorithm \cite{Dijkstra1959, DijkstraFibonacci} with time complexity more than exponential in $\mathrm{depth}(Z)$ in general) and is therefore regarded as efficient. Here, $\mathrm{depth}(Z)$ is the number of layers in the HiMM $Z$. The algorithm is reconfigurable in the sense that each modification of $Z$ only needs an update of the preprocessing step in time $O(\mathrm{depth}(Z))$, instead of recomputing the whole preprocessing step. In all these time complexities, we have, for brevity, assumed bounds on the number of states and inputs in an MM of $Z$, see Sections \ref{control_algorithm_theory} and \ref{more_control_algorithm_theory} for the general expressions without this~assumption.

%For brevity, all these time complexities assumes bounds on the number of states and inputs in an MM in $Z$, see Sections \ref{control_algorithm_theory} and \ref{more_control_algorithm_theory} for detailed expressions without this assumption.

%Third, we valide our algorithm comparing it with other less good stuff.
Third, we validate our control algorithm on the robot application given in the motivation, comparing with Dijkstra's algorithm \cite{Dijkstra1959} and Contraction Hierarchies \cite{geisberger2012exact}. We show that our control algorithm outperforms both.




\subsection{Related Work}
HFSM are typically used to model the control law of an agent \cite{harel1987statecharts,schillinger2016human,millington2018artificial}, prescribing how the agent reacts to external inputs (e.g., ``low battery''), where FSMs in the HFSMs commonly corresponds to subtasks (e.g., ``charge battery''). Here, we instead use an HFSM formalism to model a control system where the agents can choose the inputs, for example, the robot can choose to go left in House 2 in the robot application in the motivation. The aim is to steer the system (the robot) to a desirable state with minimal cost. In discrete-event systems \cite{cassandras2008introduction}, another HFSM formalism, called state tree structures, has been developed to compute safe executions \cite{ma2006nonblocking,wang2020real}. We use a different formalism and focus instead on optimal planning minimising a cost. % (i.e., inputs are decision variables)

There is a wide range of algorithms for optimal path planning on graphs such as Dijkstra's algorithm \cite{Dijkstra1959}, see \cite{bast2016route} for a survey. Here, a common approach is to use a preprocessing step to achieve faster planning at run-time \cite{bast2016route}, similar to our preprocessing step. In particular, hierarchical algorithms \cite{geisberger2012exact,dibbelt2016customizable,mohring2007partitioning,10.1145/1498698.1564502} using a preprocessing step are the ones most reminiscent to our approach. However, these approaches are typically based on heuristics which in the worst-case perform no better than Dijkstra's algorithm. Furthermore, in order to use those methods, one also need to flatten the HFSM to an equivalent FSM. This hides the hierarchical structure making these algorithms less suitable for reconfiguration. In this paper, we instead exploit the hierarchical structure to give formal performance guarantees and make the algorithm reconfigurable to changes. The paper \cite{timo2014reachability} considers a variant of an HFSM formalism and seeks an execution with minimal length between two configurations. In this paper, we have non-negative costs, where minimal length is a special case by letting all costs be unit costs. 

In \cite{biggar2021modular}, HFSMs without outputs are formalised, and used to decompose an FSM into an HFSM. However, planning was not considered. In this paper, we extend their formalism to HFSMs with outputs and consider optimal planning.

Finally, this paper builds on the work \cite{stefansson2023ecc}. We use the same HFSM formalism as in \cite{stefansson2023ecc}, and extend it to formally define changes in such a system, called modifications. We then extend the control algorithm in \cite{stefansson2023ecc} to be reconfigurable, able to handle modifications efficiently.

% Contraction hierarchies are fast but if we get the same amount of speed then we are comparable and our framework should be more flexible and have less preprocessing time in when it comes to modifications. We also have formal guarantees of the time complexity, which is not the case for contraction hierarchies. I should write something like this for the related work concerning contraction hierarchies.
% About modification: You can base your work on https://dl.acm.org/doi/pdf/10.1145/3571282 to a larger extent. In that article, and what I have found in general, the modifiable work seems to deal only with change of weights when it comes to these planning setups. Here instead, we consider reconfigurations where basically a large part of the network can be changed. This seems to not be considered in the route planning literature. Something I should say and then take some references from the article linked above.

\subsection{Outline}
The outline of the paper is as follows. Section \ref{problem_formulation} formalises HiMMs including modifications of HiMMs, and formulates the problem statement. Section \ref{control_algorithm_theory} presents the efficient control algorithm, and Section \ref{more_control_algorithm_theory} extends it to be reconfigurable. Section \ref{numerical_evaluations} then conduct numerical evaluations. Finally, Section \ref{conclusion} concludes the paper with discussion and future directions. \if\longversion0 An extended version of this paper can be found at \cite{stefansson2023cdc} that contains all the proofs in Appendix.\else
All proofs are in Appendix.
\fi

\subsection{Notation}
Let $f: A \rightharpoonup B$ denote a partial function from set $A$ to set $B$, and let $f(a) = \emptyset$ mean that $f$ is not defined for $a \in A$. For a tree $T$, we use the notation $(X \xrightarrow{v} Y) \in T$ meaning that there is an arc from $X$ to $Y$ labelled $v$ in $T$, and denote the depth of $T$ by $\mathrm{depth}(T)$, that is, the maximum number of arcs one can traverse in $T$ starting from the root. Finally, $|A|$ is the cardinality of the set $A$, and $\RR^+ = \{ x \geq 0: x \in \RR\}$.
% Perhaps clarify the tree notation already here. 

\section{Problem Formulation}\label{problem_formulation}

\subsection{Hierarchical Mealy Machines}

In this section, we formalise the notion of an HiMM, also found in \cite{stefansson2023ecc}, extending  the formalism in \cite{biggar2021modular} to machines with outputs. We start with the notion of a Mealy machine.

\begin{definition}[Mealy Machine]
An MM is a tuple $M = (Q,\Sigma,\Lambda,\delta,\gamma,s)$, where $Q$ is the finite set of states, $\Sigma$ the finite set of inputs, $\Lambda$ the finite set of outputs, $\delta: Q \times \Sigma \rightharpoonup Q$ the transition function, $\gamma: Q \times \Sigma \rightarrow \Lambda$ the output function, and $s \in Q$ the start~state. We sometimes use the notation $Q(M)$, $\Sigma(M)$, $\Lambda(M)$, $\delta_M$, $\gamma_M$ and $s(M)$ to stress that e.g., $Q(M)$ is the set of states of $M$.
\end{definition}
The intuition is as follows. The Mealy machine $M = (Q,\Sigma,\Lambda,\delta,\gamma,s)$ starts in $s \in Q$, and transits to the next state $q' = \delta(q,x)$ and outputs $\gamma(q,x)$ given current state $q \in Q$ and input $x \in \Sigma$, or stops if $\delta(q,x) = \emptyset$. Repeating this process, we obtain a trajectory of $M$:%a trajectory $(q_i,x_i)_{i=1}^N$ of $M$, where $q_1 \in Q$, $q_{i+1} = \delta(q_i,x_i) \in Q$ and $x_i \in \Sigma$ for $i \in \{1,\dots,N-1\}$.
\begin{definition}
A sequence $(q_i,x_i)_{i=1}^N \in (Q \times \Sigma)^{N}$ is a trajectory of an MM $M = (Q,\Sigma,\Lambda,\delta,\gamma,s)$ if $q_{i+1} = \delta(q_i,x_i) \in Q$ for $i \in \{1,\dots,N-1\}$.
\end{definition}
In our setting, we assume that an agent (e.g., robot) can choose the inputs. Hence, we also define a plan to be a sequence $u = (x_i)_{i=1}^N \in \Sigma^N$ of inputs, and $z = (q_i,x_i)_{i=1}^N$ to be the induced trajectory of $u$ starting from $q_1$ if $z$ is a~trajectory. We next provide the definition of an HiMM.

 
%Here, the transition function and the output function are partial functions since the transitions might not be supported. 

\begin{definition}[Hierarchical Mealy Machine]\label{HiMM_def}
An HiMM is a tuple $Z = (X,T)$ consisting of a set $X$ of MMs (the machines in $Z$) with a common input set $\Sigma$, and a tree $T$ with nodes given by $X$ (how the MMs are composed in $Z$). More precisely, each node $M \in X$ in $T$ has $|Q(M)|$ outgoing arcs $\{M \xrightarrow{q} N_q \}_{q \in Q(M)}$ where either $N_q \in X$ (meaning that state $q$ in $M$ corresponds to the MM $N_q$ one layer below in the hierarchy of $Z$) or $N_q = \emptyset$ (meaning that state $q$ is simply just a state having no further refinement). % HERE I AM. I NEED TO COME UP WITH GOOD DEFINITIONS FOR STATE AND NODE OF Z, RIGHT NOW I THINK IT IS CONFUSING TO CALL IT NODE SINCE THERE IS ALSO NODES IN T. ARC-LABEL? Extended state? 
%In the former case, we say that $q$ and $N_q$ corresponds to each other.
For brevity, we call $Q_Z := \cup_{X_i \in X} Q(X_i)$ the nodes of $Z$, and $S_Z := Q_Z \cap \{q: N_q = \emptyset \}$ the states of $Z$. We also have corresponding notions for the start state, transition function, and output function:
\begin{enumerate}[(i)]
\item The start function: The start state is a function $\mathrm{start}: X \rightarrow S_Z$ given by 
\begin{equation*}
\mathrm{start}(X_i) =
\begin{cases}
\mathrm{start}(X_j), &  \textrm{$(X_i \xrightarrow{s(X_i)} X_j) \in T$} \\
s(X_i), & \textrm{otherwise.}
\end{cases}
\end{equation*}
\item Hierarchical transition function: Let $q \in Q(X_j)$ with $X_j \in X$ and $v=\delta_{X_j}(q,x)$. The hierarchical transition function $\psi: Q_Z \times \Sigma \rightharpoonup S_Z$ is given by 
\begin{equation*}
\psi(q,x) =
\begin{cases} 
\mathrm{start}(Y), & v \neq \emptyset, (X_j \xrightarrow{v} Y) \in T, Y \in X \\
v, & v \neq \emptyset, \mathrm{otherwise} \\
\psi(w,x), & v = \emptyset, (W \xrightarrow{w} X_j) \in T, W \in X \\
\emptyset, & v = \emptyset, \mathrm{otherwise.}
\end{cases}
\end{equation*}
\item Hierarchical output function: Let $q \in Q(X_j)$ with $X_j \in X$ and $v=\delta_{X_j}(q,x)$. The hierarchical output function $\chi: Q_Z \times \Sigma \rightharpoonup \Lambda$ is given by
\begin{equation*}
\chi(q,x) =
\begin{cases} 
\gamma_{X_j}(q,x), & v \neq \emptyset, (X_j \xrightarrow{v} Y) \in T, Y \in X \\
\gamma_{X_j}(q,x), & v \neq \emptyset, \mathrm{otherwise} \\
\chi(w,x), & v = \emptyset, (W \xrightarrow{w} X_j) \in T, W \in X \\
\emptyset, & v = \emptyset, \mathrm{otherwise.}
\end{cases}
\end{equation*} 
\end{enumerate}
\end{definition}

An HiMM $Z=(X,T)$ works analogously as an MM: The HiMM $Z=(X, T)$ starts in state $\mathrm{start}(M_0)$ with $M_0$ being the root MM of $T$, and for each state $q \in S_Z$ and input $x \in \Sigma$ the machine transits to $q' = \psi(q,x) \in S_Z$ with output $\chi(q,x) \in \Lambda$, or stops if $\psi(q,x) = \emptyset$. A trajectory of an HiMM is also defined analogously as for an MM.

\begin{figure}
	\centering
  \includegraphics[width=0.20\textwidth]{Himm_transition_example_v3}
  \caption{Example of an HiMM depicted in a tree-form.} 
  %In each warehouse, the robot can move around to different locations, and at each location it can do different tasks, with decision costs given by some cost~functional.
  \label{fig:Himm_transition_example_v2}
\end{figure}

\begin{remark}[Intuition] Definition \ref{HiMM_def} is technical, but the intuition is simple. To see it, depict an HiMM $Z = (X,T)$ as a tree similar to $T$ with some extra details, as in Fig. \ref{fig:Himm_transition_example_v2}. Here, $X = \{A,B,C,D\}$ are the MMs of $Z$ with dependency given by lines, e.g., $B$ is a child of $A$ in $T$. States of $Z$ are depicted as circles. We also depict the transitions of each MM given by arrows, where the label of the arrow specifies the input needed to make the transition. We omit writing the outputs explicitly. For example, in MM $A$, one goes from (the state corresponding to) $B$ to (the state corresponding to) $C$ with input $y$. Also, small arrows indicates start states (e.g., $s(D)=4$).
We now provide the intuition of Definition \ref{HiMM_def} with an example. Assume we are at state 5 and apply input $x$. If there were a transition from 5 with $x$ (as from state 6), $Z$ would just follow that transition. However, since this is not the case, $Z$ instead goes up-wards iteratively in the tree $T$ until it finds a node that supports this input (or stops if it does not find one), in this case $C$ in the MM $A$ has a transition with $x$ (to $B$). Thus, $Z$ moves to $B$. After this, $Z$ follows the start states down-wards until it reaches a state of $Z$, in this case it goes to $D$ then 4. Here, the transition is complete and thus $\psi(5,x) = 4$. Furthermore, the output $\chi(5,x)$ corresponds to the output where the transition step occurred, in our case going from $C$ with input $x$. This is to support modularity neatly where we care about that we got from $C$ with input $x$ (e.g., $x$ can encode that we did a task in $C$), and not how we managed to exit $C$ (e.g., how we did the task in $C$). Other setups are considered as future~work.

%If $Z$ is in a state $q$ of an MM $M \in X$ with input $a \in \Sigma$ such that there is no transition in $M$ from $q$ with $a$, then $Z$ will first go iteratively up in the hierarchy until it finds a state that supports this action. For example, if $q = 5$ and $a =x$, no transition (in $C$) exits, hence $Z$ looks if there is a transition in $A$ from (the state corresponding to) $C$. In this case, there is one to $B$, hence $Z$ moves to $B$. After $Z$ has found a transition (or stop if $Z$ did not find one), it will proceed downwards instead in the hierarchy, iteratively following the start states of the MMs until it reaches a start state which is a state of $Z$ (hence, cannot go further down). In the example, $Z$ will go from $B$ to $D$ and from $D$ to 4 (which is a state of $Z$). Here, the transition is complete, e.g., $\psi(5,x) = 4$ in the example. Furthermore, the output $\chi(q,a)$, e.g, $\chi(5,x)$, corresponds to the output where the transition step occurred, in our case the output going from (the state corresponding to) $C$ with input $x$. \elis{motivate this more perhaps}
\end{remark}

\iffalse % Old one.
\begin{remark}[Intuition] At first, Definition \ref{HiMM_def} seems quite technical. However, the intuition is simple. Towards this, it is advantageous to depict an HiMM $Z = (X,T)$ as a tree similar to $T$ with some extra details, as in Fig. \ref{fig:Himm_transition_example_v2}. Here, $X = \{A,B,C,D\}$ are the MMs of $Z$ (the nodes of $T$) with dependency given by lines, e.g., $A$ has two children $B$ and $C$ corresponding to the two states of $A$. States of $Z$ (i.e., states of an MM which does not correspond to any other MM) are depicted as circles, which we have numbered. We also depict the transitions of each MM given by arrows, where the label of the arrow specifies the input needed to make the transition, while  omitting writing out the output explicitly. For example, in A, one can go to (the state corresponding to) $C$ from (the state corresponding to) $B$ with input $y$. Furthermore, small arrows corresponds to start state of each MM (e.g., the start state of $D$ is 4).
The intuition for Definition \ref{HiMM_def} is then as follows. If $Z$ is in a state $q$ of an MM $M \in X$ with input $a \in \Sigma$ such that there is no transition in $M$ from $q$ with $a$, then $Z$ will first go iteratively up in the hierarchy until it finds a state that supports this action. For example, if $q = 5$ and $a =x$, no transition (in $C$) exits, hence $Z$ looks if there is a transition in $A$ from (the state corresponding to) $C$. In this case, there is one to $B$, hence $Z$ moves to $B$. After $Z$ has found a transition (or stop if $Z$ did not find one), it will proceed downwards instead in the hierarchy, iteratively following the start states of the MMs until it reaches a start state which is a state of $Z$ (hence, cannot go further down). In the example, $Z$ will go from $B$ to $D$ and from $D$ to 4 (which is a state of $Z$). Here, the transition is complete, e.g., $\psi(5,x) = 4$ in the example. Furthermore, the output $\chi(q,a)$, e.g, $\chi(5,x)$, corresponds to the output where the transition step occurred, in our case the output going from (the state corresponding to) $C$ with input $x$. \elis{motivate this more perhaps}
\end{remark}
\fi

\subsubsection{Optimal Planning}

In this paper, the output $\chi(q,x)$ for an HiMM $Z$ will exclusively model the cost for applying input $x$ from state $q$, therefore, we henceforth assume $\Lambda \subset \RR^+$. We also consider the cost of a trajectory $z= (q_i,x_i)_{i=1}^N$, called the cumulative cost, defined by $C(z) := \sum_{i=1}^N \chi(q_i,x_i)$ if all $\chi(q_i,x_i) \neq \emptyset$, and $C(z) = + \infty$ otherwise.\footnote{In fact, only $\chi(q_i,x_i)$ may be empty, since $z$ is a trajectory. Also, we put $C(z) = + \infty$ in the latter case since we do not want $Z$ to stop.} In this setting, we want to find a plan taking $Z$ from a state $s_{\mathrm{init}}$ to a state $s_{\mathrm{goal}}$ with minimal cumulative cost. In detail, let $U(s_{\mathrm{init}}, s_{\mathrm{goal}})$ be the set of all plans taking $Z$ from $s_{\mathrm{init}}$ to $s_{\mathrm{goal}}$, i.e., $u \in U(s_{\mathrm{init}}, s_{\mathrm{goal}})$ has an induced trajectory $z =(q_i,x_i)_{i=1}^N$ such that $q_1 =  s_{\mathrm{init}}$ and $q_{N+1} := \psi(q_N,x_N) = s_{\mathrm{goal}}$. Then we want to find an optimal plan $\hat{u} \in \argmin_{u \in U(s_{\mathrm{init}}, s_{\mathrm{goal}})} C(z)$ (where $z$ is the induced trajectory of $u$). We call $\hat{u}$ an optimal plan to the planning objective $(Z,s_{\mathrm{init}},s_{\mathrm{goal}})$ and the corresponding trajectory for an optimal trajectory. In Section \ref{control_algorithm_theory}, we present a control algorithm that efficiently computes optimal plans.



\subsection{Modifications}
In this section, we formalise changes in an HiMM, called modifications. More precisely, we introduce four kinds of modifications: state addition, state subtraction, arc modification and composition, specified by Definitions \ref{def:state_addition} to \ref{def:composition}. These modifications are schematically depicted in Fig.~\ref{fig:all_operations}.

%\elis{Should I have it here? I guess so, then one can really prescribe the problem formulation in a better way. Also use, $\leadsto$ to denote a change, e.g, $Z_1 \leadsto Z_2$.}

\begin{definition}[State Addition]\label{def:state_addition}
Let $Z = (X,T)$ be an HiMM and $M \in X$ be an MM of $Z$. We say that an HiMM $Z_{\mathrm{add}} = (X_{\mathrm{add}},T_{\mathrm{add}})$ is added to $M$ in $Z$ forming a new HiMM $Z'$ with tree $T'$ equal to $T$ except that a new arc $M \xrightarrow{q} N$ has been added from $M$ to the root MM $N$ of $T_{\mathrm{add}}$ (connecting $Z_{\mathrm{add}}$ to $M$) labelled with some (arbitrary but distinct) state $q \notin Q(M)$, and $M$ has been updated its state set to $Q \cup \{q\}$. Adding a state to $M$ in $Z$ is identical except that the arc $M \xrightarrow{q} N$ is instead a terminal arc $M \xrightarrow{q} \emptyset$.
\end{definition}

\begin{figure}
	\centering
  \includegraphics[width=0.45\textwidth]{all_operations}
  \caption{The four modifications of an HiMM depicted.} 
  %In each warehouse, the robot can move around to different locations, and at each location it can do different tasks, with decision costs given by some cost~functional.
  \label{fig:all_operations}
\end{figure}

\begin{definition}[State Subtraction]\label{def:state_subtraction}
Let $Z = (X,T)$ be a HiMM and $M \in X$ be an MM of $Z$. We say that we subtract a state $q \in Q(M)$ and form a new HiMM $Z'$ with tree $T'$ equal to $T$ except that the $q$-arc from $M$ is removed, and $M$ is the modified MM where state $q$ as well as transitions and outputs corresponding to $q$ are removed.\footnote{For simplicity, we forbid the start state to be removed. However, using the arc modification operation, one can change the start state, and then remove the previous start state.}
\end{definition}

\begin{definition}[Arc Modification]\label{def:arc_modification}
Let $Z = (X,T)$ be a HiMM and $M = (Q,\Sigma,\Lambda,\delta,\gamma,s) \in X$ be an MM of $Z$. Let $\delta': Q \times \Sigma \rightharpoonup Q$, $\gamma': Q \times \Sigma \rightarrow \Lambda$ and $s' \in Q$ be new transition function, output function and start state, and form $M' = (Q,\Sigma,\Lambda,\delta',\gamma',s')$. Replace $M$ with $M'$ in $Z$ with new HiMM $Z'$ as a result, called an arc~modification.
\end{definition}

\begin{definition}[Composition]\label{def:composition}
Let $M$ be an MM with states $Q = \{q_1,\dots,q_{|Q|}\}$ and $Z_{\mathrm{seq}} = \{ Z_1,\dots,Z_{n} \} $ be a set of HiMMs such that $1 \leq n \leq |Q|$. The composition of $Z_{\mathrm{seq}}$ with respect $M$ is the HiMM $Z'$ with tree $T'$ having root $M$ and arcs $M \xrightarrow{q_i} R_i$ for $1 \leq i \leq n$, where $R_i$ is the root MM of $T_i$ (connecting $Z_i$ to $M$), and $M \xrightarrow{q_i} \emptyset$ for $i>n$.
\end{definition}

\iffalse
\begin{definition}[Composition]\label{def:composition}
Let $M$ be a Mealy machine with states $Q = \{q_1,\dots,q_{|Q|}\}$ and $Z_{seq} = \{ Z_1,\dots,Z_{n} \} $ be a set of HiMMs such that $n \leq |Q|$. The composition of $Z_{seq}$ with respect $M$ is the HiMM $Z' = (X',T')$ given by:
\begin{enumerate}
\item $X' = \{M\} \cup ( \bigcup_{i=1}^{|Q|} X_i)$, where $X_i$ is the set of MMs of~$Z_i$.
\item $T'$ is the tree with root node $M$ and arcs $M \xrightarrow{q_i} R_i$ for $1 \leq i \leq n$, where $R_i$ is the root node of the tree $T_i$ of $Z_i$, and terminal arcs $M \xrightarrow{q_i} \emptyset$ for $i>n$.
\end{enumerate}
\end{definition}
\fi

\begin{remark}[Intuition]
The composition replaces the first $n$ states in the MM $M$ with corresponding HiMMs from $Z_{\mathrm{seq}}$, while the remaining $|Q|-n$ states in $M$ are left unchanged.
\end{remark}

These four modifications are general enough to account for any change in the sense that any HiMM $Z=(X,T)$ can be changed to any other HiMM $Z'=(X',T')$ using these modifications. To see it, first add all the children of the root of $Z'$ (i.e., the root MM of $T'$) to the root of $Z$ (via repeated state addition), then do an arc modification so that the added part to the root of $Z$ is identical to the root of $Z'$ (including start state), and finally remove the original children of the root of $Z$ (using state subtraction). Then $Z$ is identical to $Z'$. Of course, this is a brute-force change, replacing the whole tree of $Z$ with the whole tree of $Z'$, and there might be other ways that uses much fever modifications to change $Z$ into $Z'$. In particular, the true benefit of using modifications is instead when we have small changes in the system, such as removing just a subtree of $Z$, or changing the transitions in one of the MMs, captured by using just a few modifications.

\subsubsection{Reconfigurability}\label{reconfigurability_definition}
We also make it precise what we mean for a control algorithm to be reconfigurable. More precisely, we consider a control algorithm to be reconfigurable if a change in the HiMM given by any of the four modifications can be handled with low time complexity. We propose such a control algorithm in Section \ref{more_control_algorithm_theory}.

%We also use the modifications to define reconfigurability of a control algorithm. More precisely, assume that a control algorithm has access to a current HiMM $Z$ and can do a computation

%More precisely, a control algorithm is said to be reconfigurable if the time complexity is low for computing an optimal plan for any HiMM $Z$ has been changed by one of the four modifications.

%\begin{definition}[Reconfigurability]
%Consider a control algorithm that computes optimal plans for HiMMs. Then the control algorithm is said to be reconfigurable, if the time complexity is low for computing an optimal plan for any HiMM an $Z$ has been changed by one of the four modifications.
%\end{definition}

%More precisely, a control algorithm that computes optimal plans for a HiMM $Z$ is said to be reconfigurable, if 

%Such small changes are typically efficiently captured by only one or very few modifications, making it ideal for reconfigurable planning, as we will see in Section (?).

%We use the output as the cost and hence $\Lambda \subset \RR$.

%If $Z$ is in a state $q$ of an MM $M \in X$ (e.g., 6 in $C$) with input $a \in \Sigma$ such that there is an transition in $M$ (e.g., $a = x$), then $Z$ transits 

\subsection{Problem Statement}
We now formalise the problem statement. In this paper, we want to find a control algorithm that, for a given HiMM~$Z$:
\begin{enumerate}
\item Efficiently computes an optimal plan ${u}$ to any planning objective $(Z,s_{\mathrm{init}},s_{\mathrm{goal}})$, where efficiency means that the computation has low time complexity.
\item Is reconfigurable in the sense of Section \ref{reconfigurability_definition}.
%\item Efficiently correct itself whenever the HiMM $Z$ is changed by a sequence of modifications, where efficiency 
\end{enumerate}
We present in Section \ref{control_algorithm_theory} a control algorithm from \cite{stefansson2023ecc} that fulfils the first criteria, and extends this control algorithm in Section \ref{more_control_algorithm_theory} to fulfil the second criteria.


\iffalse
We now formalise the problem statement. In this paper, we answer the following questions:
\begin{enumerate}
\item Let $Z$ be a given HiMM. How can a control algorithm compute an optimal plan $\hat{u}$ for any planning objective $(Z,s_{\mathrm{init}},s_{\mathrm{goal}})$ efficiently?
\item Let $Z$ be modified How can a control algorithm efficiently handle 
\end{enumerate}
want to find a control algorithm that computes an optimal plan $\hat{u}$ to any given planning objective $(Z,s_{\mathrm{init}},s_{\mathrm{goal}})$. The control algorithm should effitiently. 
\fi

%We are now in a position to formalise the problem statement. Briefly, given a HiMM $Z$, we want to find a plan taking $Z$ from a state $s_{\mathrm{init}}$ to a state $s_{\mathrm{goal}}$ with minimal cumulative cost. In detail, letting $U(s_{\mathrm{init}}, s_{\mathrm{goal}})$ be the set of all plans taking $Z$ from $s_{\mathrm{init}}$ to $s_{\mathrm{goal}}$, i.e., $u \in U(s_{\mathrm{init}}, s_{\mathrm{goal}})$ has an induced trajectory $z =(q_i,x_i)_{i=1}^N$ such that $q_1 =  s_{\mathrm{init}}$ and $q_{N+1} := \psi(q_N,x_N) = s_{\mathrm{goal}}$, we want to find a $\hat{u} \in \argmin_{u \in U(s_{\mathrm{init}}, s_{\mathrm{goal}})} C(z)$ (where $z$ is the induced trajectory of $u$). We call such a $\hat{u}$ for an optimal plan to the planning objective $(Z,s_{\mathrm{init}},s_{\mathrm{goal}})$ and the corresponding trajectory for an optimal trajectory. \elis{Maybe remove this text to where the cost is introduced? Perhaps call it optimal plans as a sub-subsection. Then, here, one can instead talk about how to find a control algorithm that fulfills the two criteria in the motivation, but here more formalised. This would close the loop much better I think.}

\section{Efficient Hierarchical Planning}\label{control_algorithm_theory}
In this section, we present the control algorithm from \cite{stefansson2023ecc} for efficiently computing an optimal plan to a given and static (i.e., none-changing) HiMM $Z$. Then, in Section \ref{more_control_algorithm_theory}, we extend this algorithm to the reconfigurable case.

%More precisely, we consider the hierarchical planning algorithm from \cite{}, with details in Section (?), and extend this algorithm in Section (??) to the reconfigurable case when the given HiMM can be subject to change given by modifications. 

%\elis{Or should I just present the extended algorithm remarking what has been added?}

%The outline is as follows. In Section \ref{}, we provide the details of the algorithm from \cite{}, which asssumes the HiMM $Z$ is static. In Section, 
%The algorithm from \cite{} is detailed in Section \ref{}, 

\begin{algorithm}[t]
\caption{Static Hierarchical Planning}\label{alg:hierarchical_planning}
\begin{algorithmic}[1]
\Require HiMM$ \; Z =(X,T)$ and states $s_{\mathrm{init}}, s_{\mathrm{goal}}$.
\Ensure Optimal plan $u$ to $(Z,s_{\mathrm{init}}, s_{\mathrm{goal}})$.
\State \textbf{Optimal Exit Computer:}
\State $(c_x^M,z_x^M)_{x \in \Sigma, M \in X} \gets \mathrm{Compute\_optimal\_exits}(Z)$
\State \textbf{Optimal Planner:}
\State $z \gets \mathrm{Reduce\_and\_solve}(Z,s_{\mathrm{init}},s_{\mathrm{goal}},(c_x^M,z_x^M)_{x \in \Sigma, M \in X})$
%\State Construct graph $G$
%\State $\bar{z}_1 \gets \mathrm{Dijkstra}(s_{\mathrm{init}}, \bar{B}, G)$
%\State Compute optimal trajectory $\bar{z}_1$ from $s_{init}$ to $\bar{B}$ in $\bar{Z}$.
%\State Compute optimal trajectory $\bar{z}_2$ from $\bar{B}$ to $s_{\mathrm{goal}}$ in $\bar{Z}$.
%\State $\bar{z} \gets \bar{z}_1 \bar{z}_2$.
\State $u \gets \mathrm{Expand}(z,(z_x^M)_{x \in \Sigma, M \in X}, Z)$
\end{algorithmic}
\end{algorithm}

%We start with an overview of the algorithm, schematically depicted by Figure \ref{fig:algorithm_overview} and summarised by Algorithm \ref{alg:hierarchical_planning_new}. Consider first the case when the given HiMM $Z$ is static, that is, does not change by modifications. In this case, the parts coloured red in Figure \ref{fig:algorithm_overview} and Algorithm \ref{alg:hierarchical_planning_new} can be ignored, and we have an algorithm 

%\subsection{Overview}
We start with an overview of the algorithm, summarised by Algorithm \ref{alg:hierarchical_planning}. Given an HiMM $Z =(X,T)$, the algorithm consists of two steps. In the first step, the Optimal Exit Computer preprocesses the HiMM $Z$, computing optimal exit costs $(c_x^M)_{x \in \Sigma}$ for each MM $M \in X$, and corresponding trajectories $(z_x^M)_{x \in \Sigma}$ (line 2). This step needs to be done only once for the given $Z$. In the second step, the Online Planner considers any two states $s_{\mathrm{init}}$ and $s_{\mathrm{goal}}$, and computes an optimal plan to $(Z,s_{\mathrm{init}}, s_{\mathrm{goal}})$ by first reducing irrelevant parts of the tree of $Z$ (intuitively, parts of the tree of $Z$ not on the path between $s_{\mathrm{init}}$ and $s_{\mathrm{goal}}$) using the result from the Optimal Exit Computer, then obtains an optimal trajectory $z$ for the reduced HiMM (line 4), and finally expands $z$ to get an optimal plan $u$ to the original HiMM $Z$ (line 5).

We provide the details of the Optimal Exit Computer in Section \ref{offline_step}, since it is crucial for the reconfigurable case in Section \ref{more_control_algorithm_theory}. However, the Online Planner is identical in the reconfigurable case. We therefore only highlight the efficiency of it in Section \ref{online_step}, refer to \cite{stefansson2023ecc} for details.

\subsection{Optimal Exit Computer}\label{offline_step}
In this section, we provide the details of the Optimal Exit Computer computing the optimal exit costs. To this end, we first need to define what we mean by optimal exit costs:

\begin{definition}[Optimal Exit Cost]\label{def:optimal_exit_costs} 
Let HiMM~$Z=(X,T)$ be given and consider MM $M \in X$. We say that a a state $q \in Q_Z$ is contained in $M$ if $q_i$ is a descendant of $M$ in $T$ (e.g, 4 is descendant of $B$ in Fig. \ref{fig:Himm_transition_example_v2}), and an $(M,x)$-exit trajectory is a trajectory $z = (q_i,x_i)_{i=1}^N$ such that all $q_i$ are contained in $M$, while $\psi(q_N,x_N)$ is not, and $x_N=x$ (i.e., exits $M$ with $x$). To an $(M,x)$-exit trajectory $z$, let the $(M,x)$-exit cost be given by $\sum_{i=1}^{N-1} \chi(q_i,x_i)$ (we exclude the cost of $(q_N,x_N)$ since the transition is happening outside the subtree with root $M$), and let $c_x^M$ be the minimum over over all $(M,x)$-exit costs. We call $c_x^M$ an optimal exit cost.
%The optimal $(M,x)$-exit cost, denoted $c_x^M$, is then the minimum over all $(M,x)$-exit costs, and we say an optimal exit cost for any of these costs. 
\end{definition}

\iffalse % Old one!
\begin{definition}[Optimal exit cost]\label{def:optimal_exit_costs} % This is quite similar to the old paper.
Let a HiMM $Z$ be given and consider a node $n \in Q_Z$ with corresponding MM $M$. An $(n,x)$-exit trajectory is a trajectory $z = (q_i,x_i)_{i=1}^N$ such that all $q_i$ are contained in $n$ (where $q_i$ is contained in $n$ if there is a path in $T$ from $n$ to $q_i$) while $\psi(q_N,x_N)$ is not. To the $(n,x)$-exit trajectory $z$, we associate a $(n,x)$-exit cost given by $\sum_{i=1}^{N-1} \chi(q_i,x_i)$ (we exclude the cost of $(q_N,x_N)$ since the transition is happening outside the subtree with root $M$ anyway). The optimal $(n,x)$-exit cost, denoted $c_x^n$, is then the minimum over all $(n,x)$-exit costs, and an optimal exit cost is just one of these costs. 

%Finally, an optimal $(n,x)$-exit trajectory is an $(n,x)$-exit trajectory that attains the optimal $(n,x)$-exit cost, and we call any such trajectory for an optimal exit trajectory.

\end{definition} %$\xrightarrow{n} X_1 \xrightarrow{v_2} X_2 \dots X_m \xrightarrow{q}$
\fi

We now provide the details of the Optimal Exit Computer, computing the optimal exit costs $(c_x^M)_{x \in \Sigma}$ for each MM $M \in X$ recursively over $T$, as follows. Let $M \in X$ be given with states $Q(M) = \{q_1,\dots,q_l\}$. For $q \in Q(M)$, define $c_x^{q}$ to be zero if $q \in S_Z$ is a state of $Z$, or $c_x^{q} = c_x^{M_{q}}$ otherwise, where $M_{q}$ is the MM corresponding to $q$. Intuitively, this is the extra cost one needs to exit $q$ with $x$ in $Z$, where, by recursion, we may assume that each $c_x^{q}$ is known. To compute $(c_x^M)_{x \in \Sigma}$, form the augmented MM $\hat{M}$ given by:
\begin{equation*}
\hat{M} := (\{q_1,\dots,q_l\} \cup \{E_x\}_{x \in \Sigma},\Sigma(M),\Lambda(M), \hat{\delta},\hat{\gamma},s(M)).
\end{equation*}
Intuitively, $\hat{M}$ is the same as $M$ except that whenever we are in a state $q_i$ of $M$ that does not support an input $x \in \Sigma$ (i.e., $\delta_M(q_i,x) = \emptyset$), then $\hat{M}$ transits to $E_x$ instead (hence the extra states $\{E_x\}_{x \in \Sigma}$ in $\hat{M}$). More precisely, the transition function $\hat{\delta}$ is given by
\begin{equation*}
\hat{\delta}(q_i,y) :=
\begin{cases}
\delta_M(q_i,y), & \delta_M(q_i,y) \neq \emptyset \\
E_y, & \mathrm{otherwise,}
\end{cases}
\end{equation*}
and $\hat{\delta}(E_x,y)$ is immaterial. Furthermore, the cost is
\begin{equation*}
\hat{\gamma}(q_i,y) := 
\begin{cases}
c_y^{q_i}+\gamma_M(q_i,y), & \delta_M(q_i,y) \neq \emptyset \\
c_y^{q_i}, & \mathrm{otherwise,}
\end{cases}
\end{equation*}
and $\hat{\gamma}(E_x,y)$ is immaterial. With this, we can search in $\hat{M}$ from $s(M)$ using Dijkstra's algorithm to find the minimal cumulative costs to all $(E_x)_{x \in \Sigma}$. Then, $c^M_x$ is equal to the minimal cumulative cost to $E_x$ \cite[Proposition 1]{stefansson2023ecc}. In Dijkstra's algorithm, we also get the corresponding optimal trajectories $(z_x^M)_{x \in \Sigma}$ to $(c_x^M)_{x \in \Sigma}$ for free. The algorithm is summarised by Algorithm \ref{alg:update_offline_step} (also given in \cite{stefansson2023ecc} as Algorithm 2) removing lines 3-5 and 16, with time complexity $O(N [b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|)])$ \cite[Proposition 2]{stefansson2023ecc}. Here, $b_s$ is the maximum number of states in an MM of $Z$, and $N$ is the number of MMs of $Z$. In particular, assuming a bound on the number of states and inputs in an MM of $Z$, we get time complexity $O(N)$. For further details, see \cite{stefansson2023ecc}.

%(also given in \cite{stefansson2023ecc} as Algorithm 2)

\iffalse
\begin{remark} 
Additionally, we get the corresponding optimal trajectories $(z_x^n)_{x \in \Sigma}$ to $(c_x^n)_{x \in \Sigma}$ for free. These trajectories are trajectories in $\hat{M}$ and may not be optimal exit trajectories. However, it is easy to recursively expand these trajectories along the tree of $Z$ to obtain the optimal exit trajectories, see \cite[Proposition 3]{stefansson2023ecc} for details. This is used in the online step.
\end{remark}
\fi

\subsection{Optimal Planner}\label{online_step}
%The Optimal Planner is identical to the online step in \cite{stefansson2023ecc}, with details beyond the scope of this paper. Here, we instead focus on the time complexity of it, referring to \cite{stefansson2023ecc} for details. 
In this section, we highlight the time complexity of the Online Planner, see \cite{stefansson2023ecc} for further details. More precisely, the time complexity of $\mathrm{Reduce\_and\_solve}$ (line 4 in Algorithm \ref{alg:hierarchical_planning}) is
$O \big (|\Sigma|^2 \mathrm{depth}(Z) + |\Sigma| \mathrm{depth}(Z) \log ( |\Sigma| \mathrm{depth}(Z) ) \big ) 
+
O \big ( [b_s |\Sigma | + b_s \log(b_s)] \mathrm{depth}(Z) \big ).$
In particular, assuming a bound on the number of inputs and states in an MM, we get time complexity $O(\mathrm{depth}(Z) \log ( \mathrm{depth}(Z)))$. Moreover, the time complexity for $\mathrm{Expand}$ (line 5 in Algorithm \ref{alg:hierarchical_planning}) is $O(\mathrm{depth}(Z) |u|)$ for obtaining the whole plan $u$ at once (where $|u|$ is the length of $u$), or $O(\mathrm{depth}(Z))$ for obtaining the next input in $u$ (ideal for sequential executions). In particular, the total time complexity for obtaining the next optimal input in $u$ is bounded by $O(\mathrm{depth}(Z) \log ( \mathrm{depth}(Z)))$. This should be compared with Dijkstra's algorithm, having a time complexity lower bounded by $O(V \log (V))$ \cite{DijkstraFibonacci}, where $V$ is the number of states in the HiMM, which in general could be exponential in $\mathrm{depth}(Z)$. The Online Planner therefore potentially saves huge computing time for large HiMMs. %also seen in Section \ref{numerical_evaluations}.

\section{Reconfigurable Hierarchical Planning}\label{more_control_algorithm_theory}

In this section, we extend Algorithm \ref{alg:hierarchical_planning} to be reconfigurable, being able to handle modifications of the HiMM $Z$~efficiently. To first see the need for this, note that the computational bottleneck of Algorithm \ref{alg:hierarchical_planning} typically comes from the Optimal Exit Computer, with time complexity involving the number of MMs $N$ in $Z$, whereas computing the next optimal input using the Optimal Planner involves only $\mathrm{depth}(Z)$. Therefore, the Optimal Planner is typically much faster than the Optimal Exit Computer. This is not a problem for a non-changing HiMM $Z$, since the optimal exit costs need to be computed only once, which can then be used repeatedly by the Optimal Planner to rapidly compute optimal plans. However, if $Z$ changes, then one would need to re-compute the optimal exit costs every time $Z$ is changed, which is inefficient, especially if the changes are small. 
%\footnote{In fact, computing the optimal exit costs is motivated in \cite{stefansson2023ecc} by the fact that this enables rapid computations of optimal plans.}

%The computational bottleneck in this two-step procedure is the Optimal Exit Computer. Fortunately, this step needs to be done only once for a given $Z$, while the Online Planner can then use the result from the Optimal Exit Computer repeatedly to rapidly compute optimal plans between any two states on demand. However, if the HiMM $Z$ changes, then one would naively need to recompute the whole offline step (for the new HiMM), which is very inefficient, especially if the change in $Z$ is small. 

\begin{figure}
	\centering
  \includegraphics[width=0.49\textwidth]{algorithm_overview_new}
  \caption{The control algorithm and its interaction with the changing system $Z$.} 
  %In each warehouse, the robot can move around to different locations, and at each location it can do different tasks, with decision costs given by some cost~functional.
  \label{fig:algorithm_overview}
\end{figure}

To address this issue, we extend Algorithm \ref{alg:hierarchical_planning} so that the Optimal Exit Computer updates only the needed parts of its computation whenever $Z$ changes. The resulting control algorithm and its interaction with the (changing) HiMM $Z$ is depicted in Fig. \ref{fig:algorithm_overview}. Here, a change in $Z$ is given by a sequence of modifications. The control algorithm gets informed of the change and the modifications. By specifying the modifications, only some of the MMs of $Z$ needs to re-computed, marked by the Optimal Exit Computer. After all modifications have been reported, the Optimal Exit Computer recomputes the optimal exits costs, but only for the marked MMs. This potentially saves huge computation time. 

%Those MMs are marked by the Optimal Exit Computer. Then, after all modifications have been reported, the Optimal Exit Computer recomputes the optimal exits costs, but only for the marked MMs. This potentially saves huge computation time. 

%To address this issue, we extend Algorithm \ref{alg:hierarchical_planning} so that the Optimal Exit Computer updates only the needed parts of its computation whenever $Z$ changes. The resulting control algorithm and its interaction with the (changing) HiMM $Z$ is depicted in Fig. \ref{fig:algorithm_overview}. Here, a change in $Z$ is given by a sequence of modifications. The control algorithm gets informed of the change and the modifications that changed it, and updates only the optimal exit costs for the MMs affected by the change.

%The key insight is that by specifying the modifications, only a small part of the MMs of $Z$ needs to re-compute their optimal exits costs, marked by the Optimal Exit Computer. Then, after all modifications have been reported, the Optimal Exit Computer recomputes the optimal exits costs, but only for the marked MMs. This potentially saves huge computation time. 

In more detail, the control algorithm is summarised by Algorithm \ref{alg:hierarchical_planning_new}, where changes with respect to Algorithm \ref{alg:hierarchical_planning} are in red. Here, $m_{\mathrm{seq}}$ is the sequence of modifications, read sequentially (line 2-4): For $m$ in $m_{\mathrm{seq}}$, $Z$ is first changed with respect to $m$ (line 3)\footnote{This line is executed by the system and not the control algorithm, see Fig. \ref{fig:algorithm_overview}. However, the algorithm tracks the change, so we keep it for clarity.}, then, the Optimal Exit Computer marks corresponding MMs (line 4), given by Algorithm \ref{alg:mark}. After $m_{\mathrm{seq}}$ has been read, the optimal exit costs for the marked MMs are updated (line 6), given by Algorithm \ref{alg:update_offline_step} (red lines accounts for the marking). We stress that we only need to execute the Optimal Exit Computer whenever $Z$ changes. The Optimal Planner in Algorithm \ref{alg:hierarchical_planning_new} is identical to The Optimal Planner in Algorithm \ref{alg:hierarchical_planning}, and can be used repeatedly as long as changes are~{up-to-date}.
%to rapidly compute optimal plans, 

\subsection{Reconfigurability}\label{reconfigurability}

We now show that Algorithm \ref{alg:hierarchical_planning_new} is reconfigurable. We first explain the marking procedure in detail. Then, we prove the correctness of the Optimal Exit Computer (Proposition \ref{prop:update_offline_step_correctness}) and show that the time complexity is low for marking and updating the optimal exit costs for each of the four modifications (Proposition \ref{prop:update_offline_step_time_complexity}), thus, Algorithm \ref{alg:hierarchical_planning_new} is reconfigurable.

\begin{algorithm}[t]
\caption{Reconfigurable Hierarchical Planning}\label{alg:hierarchical_planning_new}
\begin{algorithmic}[1]
\Require HiMM$ \; Z$, \textcolor{BrickRed}{modifications $m_{seq}$} and states $s_{\mathrm{init}}, s_{\mathrm{goal}}$.
\Ensure Optimal plan $u$ to $(Z,s_{\mathrm{init}}, s_{\mathrm{goal}})$.
\State \textbf{Optimal Exit Computer:}
\For{\textcolor{BrickRed}{$m$ in $m_{seq}$}}
\State \textcolor{BrickRed}{$\mathrm{Modify}(Z,m)$} \Comment{\textcolor{BrickRed}{This line is executed in System}}
\State \textcolor{BrickRed}{$\mathrm{Mark}(Z,m)$}
\EndFor
\State $(c_x^M,z_x^M)_{x \in \Sigma, M \in X} \gets \mathrm{Compute\_optimal\_exits}(Z)$
\State \textbf{Optimal Planner:}
\State $z \gets \mathrm{Reduce\_and\_solve}(Z,s_{\mathrm{init}},s_{\mathrm{goal}},(c_x^M,z_x^M)_{x \in \Sigma, M \in X})$
%\State Construct graph $G$
%\State $\bar{z}_1 \gets \mathrm{Dijkstra}(s_{\mathrm{init}}, \bar{B}, G)$
%\State Compute optimal trajectory $\bar{z}_1$ from $s_{init}$ to $\bar{B}$ in $\bar{Z}$.
%\State Compute optimal trajectory $\bar{z}_2$ from $\bar{B}$ to $s_{\mathrm{goal}}$ in $\bar{Z}$.
%\State $\bar{z} \gets \bar{z}_1 \bar{z}_2$.
\State $u \gets \mathrm{Expand}(z,(z_x^M)_{x \in \Sigma, M \in X}, Z)$
\end{algorithmic}
\end{algorithm}

\subsubsection{Marking}
% mark_ascestors and mark_all should be with respect to one MM (node). change this.
The marking of $Z=(X,T)$ given a modification is done by Algorithm \ref{alg:mark}. In brief, the procedure is to mark the MM considered in the modification and all ancestors to it.
%In particular, the root MM of the HiMM will always be marked if any MM inside the HiMM is marked.
% Old with confusing terminology:
%To explain the procedure in detail, consider the case when the modification is an arc modification with $Z$, $M$, $M'$, and $Z' =(X',T')$ as in Definition \ref{def:arc_modification}, with marking done by lines 5-8 in Algorithm \ref{alg:mark}. Intuitively, when modifying $M$ to $M'$, the optimal exit costs for $M'$ might not be the same as for $M$. We therefore mark $M'$ (so that its optimal costs will be re-computed) in $Z'$ (line 6). Furthermore, since the optimal exit costs of $M'$ might change, this could affect the optimal exit costs of the MMs above $M'$ in the hierarchy of $Z'$, i.e., the ancestors of $M'$. We therefore mark all the ancestors of $M'$ (line 7). However, note that all other MMs in $Z'$ have unaffected optimal exits costs, and we are therefore done.
To explain the procedure in detail, consider the case when the modification is an arc modification modifying an MM $M$ in $Z$. Since $M$ is modified, the optimal exit costs for $M$ might not be the same as before. We therefore mark $M$ (so that its optimal costs will be re-computed) in $Z$ (line 1). Furthermore, since the optimal exit costs of $M$ might change, this could affect the optimal exit costs of the MMs above $M$ in the hierarchy of $Z$, i.e., the ancestors of $M$. We therefore mark all the ancestors of $M$ (line 3). However, note that all other MMs in $Z$ have unaffected optimal exits costs, and we are therefore done. The case for state subtraction and state addition are analogous, and in the composition case, we only need to mark $M$ since $M$ has no ancestors. 
\begin{convention}\label{marking_convention}
When initialising an HiMM $Z$, all MMs are by default marked, since no optimal exit costs have been computed. This enables us to use Algorithm \ref{alg:update_offline_step} when initialising $Z$ and not only when updating $Z$ (after modifications). Also, whenever we modify an HiMM we always mark it accordingly (as above). Thus, that we do not need to mark $Z_{\mathrm{add}}$ in state addition or $\{Z_1,\dots,Z_n\}$ in composition since their optimal exit costs do not change by the modification, and are already correctly marked (note that some of their optimal exits might have already been computed by Algorithm \ref{alg:update_offline_step}). See the proof of Proposition \ref{prop:update_offline_step_correctness} for details.
%and we assume they have already been appropriately marked. 
\end{convention}
%\footnote{The reason why we do not need to mark $Z_{\mathrm{add}}$ in state addition, is because the optimal exits costs of the MMs in $Z_{\mathrm{add}}$ does not change by the state addition. Hence, assuming they are already appropriately marked, we do not need to mark them. Note also that the marked MMs of $Z_{\mathrm{add}}$ form a (possibly empty) subtree of $T_{\mathrm{add}}$ which when added to $Z$ becomes part of the subtree of marked MMs in $Z$, hence preserving the subtree property vital for Algorithm \ref{alg:update_offline_step}. The argument why we do not mark $\{Z_1,\dots,Z_n\}$ in the composition case is~analogous.}

%In this way, the marked MMs always form a subtree of $T$ with the root MM of $T$ included, which is vital for Algorithm \ref{alg:update_offline_step} when updating the optimal exit costs.

\begin{algorithm}[t]
\caption{Mark}\label{alg:mark}
\begin{algorithmic}[1]
\Require Modification $m$ and (modified) HiMM $Z$
\Ensure Appropriate marking of $Z$
\State $\mathrm{mark\_itself}(M,Z)$
\If{$m$ not composition}
\State $\mathrm{mark\_ancestors}(M,Z)$
\EndIf
\end{algorithmic}
\end{algorithm}

%\begin{remark}
%\end{remark}

%(and therefore trivially needs to be updated)

\subsubsection{Computing optimal exit costs}  
After marking, the optimal exit costs for the marked MMs are computed using Algorithm \ref{alg:update_offline_step}, where the computation of the optimal exit costs for a given MM is as in Section \ref{offline_step}. The correctness of this procedure follows from the following proposition:

%Once $Z=(X,T)$ is appropriately marked as detailed above, 

%The correctness of this procedure is a straightforward consequence of the appropriate marking of $Z$, the fact that Algorithm \ref{alg:update_offline_step} indeed updates all MMs that needs to be updated since the marked MMs forms a subtree, and that the optimal exit costs that does not need to be updated are saved from before. We state this result explicitly:

\begin{proposition}\label{prop:update_offline_step_correctness}
Let $Z$ be a HiMM, modified by the sequence of modifications $m_{\mathrm{seq}}$ (possibly empty, e.g., if $Z$ is only initialised) and marked using Algorithm \ref{alg:mark}. Then Algorithm \ref{alg:update_offline_step} returns the correct optimal exit costs. %and~trajectories. 
%\elis{To do: prove this}
\end{proposition}

\iffalse % The old one.
\begin{algorithm}[h]
\caption{Compute\_optimal\_exits \elis{Perhaps rewrite this one.}}\label{alg:update_offline_step}
\begin{algorithmic}[1]
\Require HiMM $Z' = (X',T')$ with markings.
\Ensure $(c_x^n,z_x^n)_{x \in \Sigma}$ for each node $n$ in $Z'$
\State $\mathrm{Optimal\_exit}(n_0)$ \Comment{$n_0$ corresponding to root MM in $T'$}
\State return $(c_x^n,z_x^n)_{x \in \Sigma}$ for each node $n$ in $Z'$
\State $\mathrm{Optimal\_exit}(n)$: \Comment{Recursive function} % $X \gets x$
\If {$n \in S_Z$} %\Comment{That is, $q$ is a state of $Z$}
\State $(c_x^n)_{x \in \Sigma}$ $\gets$ $0_{| \Sigma |}$ \Comment{Sequence of zeros} 
\State $(z_x^n)_{x \in \Sigma}$ $\gets$ $\Sigma$ \Comment{$\Sigma$ treated as a sequence} 
\Else
\If {\textcolor{BrickRed}{$n$ is marked}} \Comment{\textcolor{BrickRed}{Only update if $n$ is marked}}
%\State $(q_1,x_1),\dots,(q_m,x_m) \gets z^q_x$ \Comment{Optimal $x$-exit from the offline step} 
\For {each child $c$ in $n$.children} %\Comment{Children w.r.t. $T$}
\State $\mathrm{Optimal\_exit}(c)$ \Comment{Update children first}
\EndFor
\State Construct $\hat{M}$
\State $(c_x^n,z_x^n)_{x \in \Sigma}$ $\gets$ Dijkstra($s(M),\{E_x\}_{x \in \Sigma},\hat{M}$)
\State \textcolor{BrickRed}{Unmark $n$}
\EndIf
\EndIf
\end{algorithmic}
\end{algorithm}
\fi

\subsubsection{Time complexity} Finally, we address the time complexity. The time complexity of Algorithm \ref{alg:mark} is $O(\mathrm{depth}(Z))$ since there are at most $\mathrm{depth}(Z)$ MMs to mark (the MM itself and all its ancestors). The time complexity of Algorithm \ref{alg:update_offline_step} varies with the modifications that has been done. In the worst case, all optimal exit costs needs to be computed with time complexity $O(N [b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|)])$ \cite[Proposition 2]{stefansson2023ecc}. However, for just one modification, we get:

%However if only $K$ MMs needs to updated by Algorithm \ref{alg:mark}, then this reduces to $O(K [b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|)])$ (easy to see by following the proof of Proposition 2 in \cite{stefansson2023ecc}). Putting these two observations together, we get: 

\begin{algorithm}[t]
\caption{Compute\_optimal\_exits}\label{alg:update_offline_step}
\begin{algorithmic}[1]
\Require HiMM $Z = (X,T)$ with markings.
\Ensure Computed $(c_x^M,z_x^M)_{x \in \Sigma}$ for each MM $M$ of $Z$
\State $\mathrm{Optimal\_exit}(M_0)$ \Comment{Run from root MM $M_0$ in $T$}
%\State return $(c_x^M,z_x^M)_{x \in \Sigma}$ for each MM of $Z$
\State $\mathrm{Optimal\_exit}(M)$: \Comment{Recursive help function} % $X \gets x$
\If {\textcolor{BrickRed}{$M$ is not marked}}
\State return $(c_x^M,z_x^M)_{x \in \Sigma}$ \Comment{Have already been computed}
\Else
\For {each state $q$ in $Q(M)$}
\If{$q \in S_Z$}
\State $(c_x^q)_{x \in \Sigma} \gets 0_{|\Sigma|}$
\Else
\State Let $M_q$ be the MM corresponding to $q$.
\State $(c_x^{q},z_x^{q})_{x \in \Sigma} \gets \mathrm{Optimal\_exit}(M_q)$
\EndIf
\EndFor
\State Construct $\hat{M}$
\State $(c_x^M,z_x^M)_{x \in \Sigma}$ $\gets$ Dijkstra($s(M),\{E_x\}_{x \in \Sigma},\hat{M}$)
\State \textcolor{BrickRed}{Unmark $M$}
\State return $(c_x^M,z_x^M)_{x \in \Sigma}$
\EndIf
\end{algorithmic}
\end{algorithm}

\begin{proposition}\label{prop:update_offline_step_time_complexity}
Let HiMM $Z$ be given and consider the Optimal Exit Computer with only one modification $m_{\mathrm{seq}} = m$. Assume $Z$ have all optimal exits costs computed before the modification, and that the same holds for $Z_{\mathrm{add}}$ if $m$ is a state addition, or $\{Z_1,\dots,Z_n\}$ if $m$ is a composition. Then, the time complexity for executing the Optimal Exit Computer if $m$ is a state addition, state subtraction, or arc modification is $O( \mathrm{depth}(Z) [b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|)] )$, and $O([b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|)] )$ if $m$ is a composition. In particular, assuming a bound on the number of states and inputs of the MMs in $Z$, the time complexity for any modification is $O(\mathrm{depth}(Z))$. Thus, Algorithm \ref{alg:hierarchical_planning_new} is~reconfigurable.
\end{proposition}

%However, minor modifications results in much lower time complexity, exemplified by Proposition \ref{prop:update_offline_step_complexity}.

\section{Numerical evaluations}\label{numerical_evaluations}
In this section, we present numerical evaluations, validating the proposed control algorithm given by Algorithm \ref{alg:hierarchical_planning_new}. We consider the robot application from the motivation and compare our method with Dijkstra's algorithm and Contraction Hierarchies, \if\longversion0 see \cite{stefansson2023cdc} for implementation details.\else
see Appendix for implementation~details.
\fi

%see Appendix for implementation~details. 

%The latter is a shortest-path algorithm for graphs involving a preprocessing step, adding additional arcs in the graph, and a query step that uses the modified graph to compute optimal plans. The method has several heuristics to optimise performance. We refer to Appendix for details.  

%Contraction Hierarchies is a shortest-path algorithm for graphs involving a preprocessing phase, modifying the graph by adding additional arcs (called shortcuts), and a query phase that uses the modified graph to speed-up the shortest-path search between two nodes using bidirectional search. The preprocessing phase has several heuristics to optimise performance. The heuristics we use are given in Appendix. We also refer to \cite{geisberger2012exact} for details concerning Contraction Hierarchies.

\subsection{Case studies}
We first formalise the robot application from the motivation, also given in \cite{stefansson2023ecc}. The HiMM $Z$ is constructed using three MMs, corresponding to the layers in Fig. \ref{fig:robot_example_detailed_overview}.

The first MM (for Layer 1) has 10 states corresponding to the houses. The houses are ordered in a line where the robot can move to a neighbouring house as seen by the arrows in Fig. \ref{fig:robot_example_detailed_overview}, at a cost of 100. Start state is $s(M_1) = 1$.

The second MM (for Layer 2), $M_2$, corresponds to the locations in a house with $10\cdot 10+1=101$ states in a grid-like formation where the robot can move to any neighbouring grid-point, at a cost of 1, as in Fig. \ref{fig:robot_example_detailed_overview}, and start state $s(M_2)$ marked S, interpreted as the entrance to the house. From $s(M_2)$, the robot can also move left/right. However, these two inputs makes the robot exit $M_2$ and instead, in $Z$, moves it to the corresponding neighbouring house in $M_1$. 

The third MM (for Layer 3), $M_3$, corresponds to the lab desk at a location, with $10 \cdot 9+1 = 91$ states. Here the robot starts at $s(M_3)$, marked S in Fig. \ref{fig:robot_example_detailed_overview}. From $s(M_3)$, the robot can choose to go up, down, left or right, which quits $M_3$ and instead moves to the corresponding location in $M_2$; It can also start to steer a robot arm over a $3 \times 3$ test tube rack analogous to the location-grid in $M_2$, where the robot arm $(i,j)$ ($1 \leq i,j \leq 3$) is initially over tube (1,1). The robot can also scan a tube with the arm. The scanned tube $(i,j)$ is then remembered and no further tubes can be scanned. Finally, the robot can go back to $s(M_3)$ when the arm is again over tube (1,1), depicted in Fig. \ref{fig:robot_example_detailed_overview}. All costs in $M_3$ are set to 0.5, except scanning which costs 10. The hierarchy of $M_1$, $M_2$ and $M_3$ yields $Z$ with $10 \cdot 101 \cdot 91 = 91910$ states.

The case studies we consider are as follows. In Study 1, we consider $Z$ just described. In Study 2, we change $Z$ by adding a house, House 11, being a neighbour house to House 10 (see Fig. \ref{fig:robot_example_detailed_overview}), formally done by a state addition (adding House 11), followed by an arc modification (connecting House 10 and 11). All MMs of House 11 are marked, i.e., no optimal exits costs have been computed yet. In Study 3, we change $Z$ (from Study 1), but now block some locations in House 2 as in Fig. \ref{fig:robot_example_change_overview}, formally removing locations using state subtraction. In all studies, the robot starts in the bottom-right corner of House 1 with arm over tube $(2,2)$, with goal to scan tube $(2,2)$ in the bottom-right corner of House 10, 11 and 2 in Study 1, 2 and 3 respectively.
 %while Study 2 and 3 have the same objective except in House 11 and 2, respectively.

\iffalse % Old divided into subsections.
\subsubsection{Study 1}
Study 1 considers $Z$ just described.
\subsubsection{Study 2}
In Study 2, we change $Z$ by adding a house, House 11, being a neighbour house to House 10 (see Fig. \ref{fig:robot_example_detailed_overview}). This is formally done by a state addition (adding House 11), followed by an arc modification (connecting House 10 and 11). All MMs of House 11 are marked, i.e., no optimal exits costs have been computed yet.
\subsubsection{Study 3}
In Study 3, we again change $Z$ (from Study 1), but now block some locations in House 2 as in Fig. \ref{fig:robot_example_change_overview}. Formally, we remove the locations using state subtraction.
\fi

%From the start state $s(M_3)$, marked S in Figure \ref{fig:robot_example_detailed_overview}, the robot can either start to steer the robot arm at tube $(1,1)$, or go up, down, left or right, which then quits $M_3$ and instead moves in $M_2$ accordingly. Finally, the robot also go to $s(M_3)$ when the robot arm is at tube $(1,1)$.
\subsection{Result}

\iffalse
\begin{table}[]
\centering
\caption{Computing times for Case Study 2.}
\label{table_1}
\begin{tabular}{llll}
\hline
  &  Study 1 & Study 2  & Study 3  \\ \hline
Compute optimal exits               & 2.234 s & 2.2417 s & 1.959 s \\
Update optimal exits            & - & 0.20837 s & 0.002345 s \\
Get optimal plan           & 0.02539 s & 0.02714 & 0.02281 \\
Dijkstra's algorithm                & 3.73 & 4.0134 s & 1.366 s \\
CH - preprocessing               & 1196.4 s  & 1320.99 s & 1174.8 s \\
CH - compute optimal plan               & 0.01655 s & 0.01866 s & 0.015528 s \\
\hline  
\end{tabular}
\end{table}
\fi

\begin{table}[]
\centering
\caption{Computing times for the case studies.}
\label{table_1}
\begin{tabular}{llll}
\hline
  &  Study 1 & Study 2  & Study 3  \\ \hline
Compute optimal plan           & 0.0254 s & 0.0271 s & 0.0228 s \\
Compute optimal exits            & 2.23 s & 0.208 s & 0.00234 s \\
Compute all optimal exits               & 2.23 s & 2.24 s & 1.96 s \\
Dijkstra's algorithm                & 3.73 s & 4.01 s & 1.37 s \\
CH  preprocessing               & 1196 s  & 1320 s & 1175 s \\
CH  compute optimal plan               & 0.0166 s & 0.0187 s & 0.0155 s \\
\hline  
\end{tabular}
\end{table}

The result of Study 1 to 3 are summarised in Table \ref{table_1}. The first row shows the time it takes the online planner in Algoritm \ref{alg:hierarchical_planning_new} to compute an optimal plan. In all three studies, the computation time is around 20-30 ms. This should be compared with Dijkstra's algorithm (row 4) with times around 1-4 s, two order of magnitudes slower. Contraction Hierarchies (row 6) has instead a computation time around 16-19 ms, therefore slightly faster than Algoritm \ref{alg:hierarchical_planning_new}, though still the same order of magnitude. However, the slightly faster times comes at the expense of a slower preprocessing step for the Contraction Hierarchies taking around 1200-1300 s to finish (row 5), compared with significantly faster computation times of Algorithm \ref{alg:update_offline_step} (row 2): 2.23 s, 0.208 s and 0.00234 s for Study 1, 2 and 3, respectively. Here, we also see the importance of reconfigurability, explaining the time differences of Algorithm \ref{alg:update_offline_step}. More precisely, in Study 1, we initialise $Z$ and need to compute all optimal exit costs. This thus takes the same time (2.23 s) as computing all optimal exit costs (row 3). However, in Study 2, we only modify $Z$ from Study 1, hence, only some optimal exit costs needs to be recomputed, in this case all MMs of $Z$ corresponding to House 11 (since they have not been computed yet) and the root MM of $Z$ (since it has been modified). The result is a much faster time (0.208 s), compared to computing all optimal exit costs (2.24 s). In Study 3, the modification results in even fewer MMs that needs to recomputed, in time 2.34 ms. This shows the benefit of reconfigurability, being several order of magnitude faster than needing to compute all optimal exits costs (as in \cite{stefansson2023ecc}).

%The first (second) row is the computation time of Algorithm \ref{alg:update_offline_step} with (without) taking the markings into account. 

%For example, in Study 1, these times are identical since we initialise $Z$ and hence has not computed any optimal exit costs. However, in Study 2, 

%The result for Study 1 to 3 are summarised in Table \ref{table_1}. To highlight the benefit of using markings, we show, in the first row, the time it takes to compute \emph{all} optimal exit costs, ignoring that some MMs are actually already computed (that is, we use Algorithm \ref{alg:update_offline_step} but without the red lines, as in Section \ref{offline_step}). This should be compared to the second row, the computation time for Algorithm \ref{alg:update_offline_step}, computing only necessary optimal exit costs. For Study 1, these two times are the same since we initialise $Z$ (hence need to compute all optimal exit costs). However, for Study 2 and 3, we modify $Z$ from Study 1, hence, Algorithm \ref{alg:update_offline_step} only need to compute some of all optimal exit costs, which results in much lower computation times: 0.208 s compared to 2.24 s in Study 2, and 0.00234 s compared to 1.96 s in Study 3).

%The first row shows the time it takes to compute all optimal exit costs using (i.e., Algorithm \ref{alg:update_offline_step} without the red lines as in Section \ref{}). The second row shows the computation time of Algorithm \ref{alg:update_offline_step}, taking markings into account (hence only needs to update optimal exit costs that has not yet been computed).

%where the first three rows present computation times using Algorithm \ref{alg:hierarchical_planning_new}, and the remaining times are for the comparison methods. To start, consider Study 1. Here, the time for computing all optimal exit costs is 2.23 s, and since $Z$ is initialised, all optimal exit costs needs to be updated, hence the update step using markings 

\section{Conclusion}\label{conclusion}
In this paper, we have considered a planning problem for a large-scale system modelled as an HiMM $Z$ and developed a control algorithm for computing optimal plans between any two states. The control algorithm consists of two steps. A preprocessing step computing optimal exits costs for each MM in the $Z$, and a query step computing an optimal plan between any two states, using the optimal exit costs. Given bounds on the number of states and inputs an MM in $Z$ has, the time complexity for the preprocessing step is $O(N)$ (where $N$ is the number of MMs in $Z$), while the query step has time complexity $O(\mathrm{depth}(Z) \log ( \mathrm{depth}(Z)))$ for obtaining the next optimal input. The latter enables rapid optimal plan computations. The control algorithm is also reconfigurable in the sense that changes, formalised as modifications, can be handled with ease, where a modification in $Z$ only needs a minor update of the preprocessing step in time $O(\mathrm{depth}(Z)$. We validated the control algorithm on a robotic application, comparing it with Dijkstra's algorithm and Contraction Hierarchies. We noted that our control algorithm outperforms Dijkstra's algorithm, computes optimal plans in the same order of magnitude as Contraction Hierarchies but with a significantly faster preprocessing step, and handles changes in the HiMM efficiently.

Future work includes extensions to stochastic hierarchical systems and additional numerical~evaluations.

\bibliographystyle{plain}
\bibliography{Ref3}

\if\longversion1

\section*{Appendix}

\subsection{Proofs}

%We prove the key observation by induction. More precisely, note first that $Z$ is formed by starting with a set of HiMMs that through modifications eventually form $Z$. Let $\mathcal{Z} = \{Z_1,\dots,Z_n\}$ be this set of HiMMs (where we treat $M$ in the composition modification as a HiMM with only one MM, namely $M$). We prove that $Z$ is correctly marked by induction over the cardinality $n$ of $\mathcal{Z}$. Consider the base case $n=1$. 

%Note also that the marked MMs of $Z_{\mathrm{add}}$ form a (possibly empty) subtree of $T_{\mathrm{add}}$ which when added to $Z$ becomes part of the subtree of marked MMs in $Z$, hence preserving the subtree property vital for Algorithm \ref{alg:update_offline_step}. The argument why we do not mark $\{Z_1,\dots,Z_n\}$ in the composition case is~analogous.
We start with the proof of Proposition \ref{prop:update_offline_step_correctness}, followed by the proof of Proposition \ref{prop:update_offline_step_time_complexity}.

\begin{proof}[Proof of Proposition \ref{prop:update_offline_step_correctness}]
Let HiMM $Z =(X,T)$ be given. The key observation for showing Proposition \ref{prop:update_offline_step_correctness} is to note that the marked MMs of $Z$ will always either form a subtree of $T$ that includes the root of $T$, or be empty (where the latter is the case if all MMs in $Z$ have optimal exit costs that are up-to-date). Therefore, Algorithm \ref{alg:update_offline_step} (starting at the root of $T$) will always reach all MMs that are marked and since the computation for a specific MM is correct (see Section \ref{control_algorithm_theory}), we conclude that Algorithm \ref{alg:update_offline_step} indeed returns the correct optimal exit costs and corresponding trajectories. Thus, it remains to show the key observation (i.e., that the marked MMs of $Z$ will always either form a subtree of $T$ that includes the root of $T$, or be empty), which is intuitively clear, but needs a technical proof to account for the HiMMs that one adds to form $Z$. The proof is given below, where we for brevity say that a HiMM $Z$ that fulfils the key observation has the subtree-property.

We prove the key observation by induction. More precisely, note first that $Z$ is formed by starting with a set $\mathcal{Z}$ of initialised HiMMs that through modifications eventually form $Z$. Let $n$ be the number of times we add HiMMs in this procedure using either state addition or composition. We prove that $Z$ has the subtree-property by induction over $n$. 

\subsubsection*{Base case}
Consider first the base case $n=0$. In this case, $\mathcal{Z}$ can only contain one element\footnote{Since if $\mathcal{Z}$ would contain at least two elements, then these HiMMs would eventually need to be composed or added to one another (to form $Z$ in the end), which is not possible if $n=0$.}, call it $Z_1$. Therefore, we only start with $Z_1$, which is first initialised and changed through modifications to eventually form $Z$, where we possibly also compute some of the optimal exit costs using Algorithm \ref{alg:update_offline_step} along the way (see Convention \ref{marking_convention}). Formally, this procedure is nothing but a sequence of changes on the form
\begin{equation}\label{eq:proof_sequence}
Z_1 \xrightarrow{o_1} Z^{(1)} \xrightarrow{o_2} Z^{(2)} \xrightarrow{o_3} \dots \xrightarrow{o_m} Z
\end{equation}
where $o_i$ is either a modification, or $o_i = \star$ specifying that we call Algorithm \ref{alg:update_offline_step}. More precisely, for a modification $o_i$, $Z \xrightarrow{o_i} Z'$ means that we changed $Z$ to $Z'$ through the modification $o_i$, while $Z \xrightarrow{\star} Z'$ means that we called Algorithm \ref{alg:update_offline_step} on $Z$ with $Z'$ as a result (in the latter case, note that $Z'$ only differs by which MMs are marked). We will prove that all HiMMs in \eqref{eq:proof_sequence} fulfil the subtree-property, by induction over the sequence \eqref{eq:proof_sequence}. For the base case, note that $Z_1$ trivially fulfils the subtree-property since all MMs are marked (due to initialisation, see Convention \ref{marking_convention}). For the induction step, assume that $W$ is an HiMM in \eqref{eq:proof_sequence} that fulfils the subtree-property, and changed to $W'$ through $o$. We prove that $W'$ then also fulfils the subtree-property, separating the proof into cases:
\begin{enumerate}[(i)]
\item Consider first the case when $o = \star$. Then we call Algorithm \ref{alg:update_offline_step} on $W$ to get $W'$. Since $W$ has the subtree-property, Algorithm \ref{alg:update_offline_step} will reach all marked MMs and unmark them. Hence, $W'$ has no marked MMs, and therefore fulfils the subtree-property.
\item Consider now the case when $o$ is a modification. If $o$ is a state subtraction or arc modification, on MM $M$ in $W$ say, then we mark $M$ and all the ancestors of $M$ in $W'$ (due to Convention \ref{marking_convention}).\footnote{In particular, we mark the root MM of $W'$.} Therefore, $W'$ fulfils the subtree-property. If $o$ is a state addition, then $o$ cannot add any HiMM $Z_{\mathrm{add}}$ to an MM $M$ (since this would contract $n=0$). Thus, $o$ can only add a state to an MM $M$. In this case, $M$ and all ancestors of are marked, so $W'$ again fulfils the subtree-property. Finally, $o$ cannot be a composition since $n=0$.
\end{enumerate}
Combining the two cases, we conclude that $W'$ fulfils the subtree-property. By induction over the sequence \eqref{eq:proof_sequence}, all HiMMs in \eqref{eq:proof_sequence} fulfil the subtree-property. In particular, $Z$ fulfils the subtree-property. We have therefore proved the base case $n=0$.

\subsubsection*{Induction step}
We continue with the induction step. Towards this, assume $n\geq1$. Let $Z_1$ be the last HiMM that is formed by either adding a HiMM using state addition, or using composition.\footnote{Note that $Z_1$ must indeed exist since we start with the HiMMs in $\mathcal{Z}$ but eventually are left with just $Z$.} Thus, from $Z_1$, $Z$ is formed by a sequence of changes as in \eqref{eq:proof_sequence}.\footnote{This follows from the fact that we cannot add another HiMM from $Z_1$ to $Z$, since $Z_1$ was the last HiMM that was formed by adding a HiMM.} Therefore, by just following the proof for the case $n=0$, we know that $Z$ fulfils the subtree-property if $Z_1$ fulfils the subtree-property. It remains to prove that $Z_1$ fulfils the subtree-property. By assumption, $Z_1$ is formed by adding a HiMM using state addition or composition. We consider the two cases separately:
\begin{enumerate}
\item Assume first that $Z_1$ is formed by state addition, adding an HiMM $Z_{\mathrm{add}}$ to an HiMM $Z_{\mathrm{to}}$. Let $n_{\mathrm{add}}$ ($n_{\mathrm{to}}$) be the number of times we add HiMMs to form $Z_{\mathrm{add}}$ ($Z_{\mathrm{to}}$) using either state addition or composition, similar to $n$. Then, $n_{\mathrm{add}} < n$ and $n_{\mathrm{to}} < n$.\footnote{This is because every time we add an HiMM to form e.g., $Z_{\mathrm{add}}$, this also counts as adding an HiMM to form $Z$, and we add at least one more HiMM to form $Z$, due to the formation of $Z_1$. Hence, $n_{\mathrm{add}} < n$. The argument for $n_{\mathrm{to}} < n$ is analogous.} By induction, $Z_{\mathrm{add}}$ and $Z_{\mathrm{to}}$ fulfils the subtree-property. Let $Z_{\mathrm{add}}$ be added to the MM $M$ in $Z_{\mathrm{to}}$. Then $M$ and its ancestors are marked\footnote{Note that this connects the marked subtree of $Z_{\mathrm{add}}$ with the marked subtree of $Z_{\mathrm{to}}$.} in $Z_1$ and since both $Z_{\mathrm{add}}$ and $Z_{\mathrm{to}}$ fulfils the subtree property, we conclude that $Z_1$ fulfils the~subtree-property.
\item Assume now that $Z_1$ is formed by composition using an MM $M$. By induction, analogous to the state addition case, all the HiMMs in the composition fulfils the subtree-property, and since $M$ (the root of $Z_1$) is marked, we conclude that $Z_1$ also fulfils the~subtree-property.
\end{enumerate}
Combining the two cases, we conclude that $Z_1$ fulfils the subtree-property, hence, $Z$ fulfils the subtree-property. 

We have showed both the base case and the induction step. Therefore, by induction over $n$, $Z$ always fulfils the subtree-property, proving the key observation. Therefore, the proof of Proposition \ref{prop:update_offline_step_correctness} is~complete.
\end{proof}

We continue with the proof of Proposition \ref{prop:update_offline_step_time_complexity}.

\begin{proof}[Proof of Proposition \ref{prop:update_offline_step_time_complexity}]
Let HiMM $Z=(X,T)$ be given. Following the proof of Proposition 2 in \cite{stefansson2023ecc}, it is easy to conclude that if $K$ MMs are marked in $Z$, then the time complexity of Algorithm \ref{alg:update_offline_step} is $O(K [b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|)])$. Since $Z$ has no marked MMs before the modification $m$, the number of MMs in $Z$ after the modification is at most $\mathrm{depth}(Z)$ if $m$ is a state addition, state subtraction, or arc modification, and at most 1 if $m$ is a composition. Hence, marking takes time $O(\mathrm{depth}(Z))$ for state addition, state subtraction, or arc modification, and $O(1)$ composition. Furthermore, using the observation above, running Algorithm \ref{alg:update_offline_step} therefore takes time $O(\mathrm{depth}(Z) [b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|)])$ for state addition, state subtraction, or arc modification, and $O( 1 [b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|)])$ for composition. We conclude that the time complexity for executing the Optimal Exit Computer if $m$ is a state addition, state subtraction, or arc modification is $O( \mathrm{depth}(Z) [b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|)] )$, and $O([b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|)] )$ if $m$ is a composition.
%However if only $K$ MMs needs to updated by Algorithm \ref{alg:mark}, then this reduces to $O(K [b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|)])$ (easy to see by following the proof of Proposition 2 in \cite{stefansson2023ecc}). Putting these two observations together, we get: 
\end{proof}

\subsection{Implementation Details}
In this section, we provide additional details concerning the implementation used in the numerical investigations in Section \ref{numerical_evaluations}, with focus on Contraction Hierarchies. First of all, all implementations are in Python. Moreover, all three methods (Algorithm \ref{alg:hierarchical_planning_new}, Dijkstra's algorithm and Contraction Hierarchies) uses Dijkstra's algorithm in their implementation. Theoretically, Dijkstra's algorithm has the lowest time complexity when using Fibonacci heaps \cite{DijkstraFibonacci}. However, in practice, better performance is typically achieved (at least in our case) with a simple priority queue, hence, in the simulations, we use the latter.

We continue with the details concerning our implementation of Contraction Hierarchies. First of all, Contraction Hierarchies is a shortest-path algorithm for graphs involving a preprocessing step and a query step. In the preprocessing step, one sequentially removes one node at a time in the graph (called a contraction), adding additional arcs (called shortcuts) to account for the removed node (the shortcuts are added so that the remaining nodes have the same shortest-distance to each other). This contraction procedure is then iterated until there is no nodes left to contract. Then, in the query step, an optimal plan is computed using a bidirectional search on the original graph but with the added shortcuts from the preprocessing step, with intuition that the added shortcuts improve the search speed. See \cite{geisberger2012exact} for details.

To get a good performance using Contraction Hierarchies, the node order selection in the preprocessing step is crucial (i.e., how we select the next node to contract). In our implementation, we picked three common heuristics for selecting the next node. Namely, we conduct lazy evaluations, where we pick the next node $n$ based on the edge difference of $n$, and add this number with the number of neighbours to $n$ that has already been contracted (to account for sparcity). The details of these heuristics are beyond the scope of this paper; instead we refer to the lecture notes given by Hannah Bast at \url{https://ad-wiki.informatik.uni-freiburg.de/teaching/EfficientRoutePlanningSS2012} (which our implementation is based on) and the paper \cite{geisberger2012exact}. To speed-up the preprocessing step (following the lecture notes), we also set a maximum number of visited nodes when Dijkstra's algorithm is used in a contraction to locally search if shortcuts should be added, setting the maximum to 20 nodes. Again, see the lecture notes and the paper \cite{geisberger2012exact} for details. Finally, we stress that there might exists other variations of Contraction Hierarchies that might perform better than our implementation (e.g., using other heuristics).
%Here, we followed a standard implementation of Contraction Hierarchies with common heuristics.

%We are also aware that other coding languages might result in a faster algorithm, however, this is true for all algorithms, including our own algorithm.

%modifying the graph by adding additional arcs (called shortcuts), and a query phase that uses the modified graph to speed-up the shortest-path search between two nodes using bidirectional search. The preprocessing phase has several heuristics to optimise performance. In our implementation, we picked three common heuristics for the preprocessing step. Namely in the node order selection in the preprocessing step, we select which node to contract based on lazy evaluations, computing edge difference for neighbours, and accounting for sparcity. We also put a maximum number of visited nodes each time we use Dijkstra's algorithm to check if a shortcut needs to be added, setting the maximum to 20 visited nodes. The latter is used to improve the speed of the preprocessing step. The implementation is based on the lecture notes found in \cite{}.

%Contraction Hierarchies is a shortest-path algorithm for graphs involving a preprocessing phase, modifying the graph by adding additional arcs (called shortcuts), and a query phase that uses the modified graph to speed-up the shortest-path search between two nodes using bidirectional search. The preprocessing phase has several heuristics to optimise performance. The heuristics we use are given in Appendix. We also refer to \cite{geisberger2012exact} for details concerning Contraction Hierarchies.

\fi

\end{document}
\endinput





%%%%%%%%%%%%%%%%%%%%% OLD STUFF %%%%%%%%%%%%%%%%%%%%

\begin{proof}[Proof of Proposition \ref{prop:update_offline_step_correctness}]

To prove Proposition \ref{prop:update_offline_step_correctness}, we first need the following observation:
\begin{lemma}
\end{lemma}
\begin{lemma}
Let $Z=(X,T)$ be a HiMM with all its MMs marked or none marked. Let $m_{seq}$ be a (possible empty) sequence of modifications, which modifies $Z$ to $Z' =(X',T')$. Then the marked MMs $T_m$ of $Z'$ is either empty or forms a subtree of $T'$ that includes the root MM of $T'$.
\end{lemma}
\begin{proof}
The proof of the lemma is by induction over the number of modifications in $m_{seq}$. The base case is trivial, namely, with no modifications $T_m$ is either $T$ or empty. For the induction step, let $Z'$ be the result by modifying $Z$ using the sequence of modifications $m_{seq} = (m_1,m_2)$, where $m_2$ is one modification while $m_1$ is a (possibly empty) sequence of modifications. Consider the intermediate HiMM $Z'' = (X'',T'')$ formed by modifying $Z$ using the sequence of modifications $m_{1}$. By induction, the sub-graph of marked MMs of $Z''$ is either empty, or forms a subtree of $T''$ that includes the root MM of $T''$. 

assume that $Z_i = (X_i,T_i)$ is the HiMM resulting by modifying $Z$ using the sequence of modifications $m_{i}$. By induction, the marked MMs of $Z_i$ is either empty or a subtree of $T_i$ that includes the root MM of $T_i$. 

Furthermore $Z'$ be the result by modifying $Z$ using the sequence of modifications $m_{seq} = (m_{i},m)$. By induction, the marked MMs of $Z_i$ is either empty or 
\end{proof}

Assume that $Z$ has either all MMs marked or none marked. The first is the case when $Z$ is initialised 

We first show that the $T_{m}$ be the sub-graph of $T$ consisting of all marked MMs are either empty

%with either all MMs marked or none MMs marked. Assume that 

Moreover, let $T_{m}$ be the sub-graph of $T$ with all marked MMs. 

We first claim that $T_m$ is either empty or a subtree of $T$ with the root MM of $T$ included. To see it, note first that the statement is trivially true if $Z$ has only been initialised but not modified (since all MMs are then marked, see Remark \ref{}). To show the claim in general, assume by induction that $T_m$

In this way, the marked MMs always form a subtree of $T$ with the root MM of $T$ included, which is vital for Algorithm \ref{alg:update_offline_step} when updating the optimal exit costs.
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop:update_offline_step_complexity}]
\end{proof}

\subsubsection{Offline step for a change in the HiMM $Z$}

Consider now the case when we have already computed the offline step for $Z$, but that $Z$ has afterwards changed through a sequence of modifications. In this case, we might not need to re-compute the whole offline step. More precisely, the key insight is that by specifying the modifications that has happen, only a small part of the HiMM $Z = (X,T)$ might need to be recalculated in the offline step, potentially saving huge computation time. We formalise this bookkeeping with markings, where a node $n$ in $Z$ is marked if its offline step needs to recalculated (due to a change in $Z$). The procedure for state addition, state subtraction, arc modification and composition, including marking, are summarised by Algorithm \ref{alg:state_addition_and_marking} to \ref{alg:composition_and_marking}, which uses Algorithm \ref{alg:mark_ancestors} and \ref{alg:mark_all}. Once all modifications have been marked, an offline step can be executed, updating only the marked parts of the HiMM $Z$. The procedure is summarised by Algorithm \ref{alg:update_offline_step}, with correctness given by Proposition \ref{prop:update_offline_step_correctness}. Note also that Algorithm \ref{alg:update_offline_step} covers the previous case (when we compute optimal exit costs) using the convention that all nodes are initially marked.

\begin{proposition}\label{prop:update_offline_step_correctness}
Let $Z$ be the original HiMM and $Z'$ be the new HiMM after a finite number of modifications with corresponding markings. Then Algorithm \ref{alg:update_offline_step} returns the correct optimal exit costs and trajectories. \elis{To do: prove this}
\end{proposition}

The time complexity of the update of Algorithm \ref{alg:update_offline_step} varies with the modifications that has been done. In the worst case, the whole HiMM has to be updated, with time complexity $O(N [b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|)])$, where $b_s$ is the maximum number of states in an MM of $Z$ and $N$ is the number of MMs in $Z$, following from \cite[Proposition 2]{stefansson2023ecc}. However, minor modifications results in much lower time complexity, exemplified by Proposition \ref{prop:update_offline_step_complexity}.

\begin{proposition}\label{prop:update_offline_step_complexity}
We have the following time complexities: \elis{To do: compute these complexities}
\begin{enumerate}[(i)]
\item Adding a HiMM $Z_{add}$ to $Z$ using Algorithm \ref{alg:state_addition_and_marking} and then updating the resulting HiMM $Z'$ using Algorithm \ref{alg:update_offline_step} has time complexity (??). 
\item Subtracting a state using Algorithm \ref{alg:state_subtraction_and_marking} and then updating the resulting HiMM $Z'$ using Algorithm \ref{alg:update_offline_step} has time complexity (??).
\item Conducting an arc modification using Algorithm \ref{alg:state_subtraction_and_marking} and then updating the resulting HiMM $Z'$ using Algorithm \ref{alg:update_offline_step} has time complexity (??).
\item Composing $Z_{seq} = \{ Z_1,\dots,Z_{n} \} $ with $M$ using Algorithm \ref{alg:arc_modification_and_marking} and then updating the resulting HiMM $Z'$ using Algorithm \ref{alg:update_offline_step} has time complexity (??).
\end{enumerate} 
\end{proposition}

\begin{algorithm}[h]
\caption{State\_addition\_and\_marking}\label{alg:state_addition_and_marking}
\begin{algorithmic}[1]
\Require HiMM $Z = (X,T)$, MM $M \in X$ and $Z_{add} = (X_{add},T_{add})$
\Ensure HiMM $Z' = (X',T')$ as in Definition \ref{def:state_addition} with markings.
\State $Z' \gets \textrm{state\_addition}(Z,M,Z_{add})$
\State $\textrm{mark\_ancestors}(Z_{add},Z')$
\State $\textrm{mark\_all}(Z_{add},Z')$
\State return $Z'$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{State\_subtraction\_and\_marking}\label{alg:state_subtraction_and_marking}
\begin{algorithmic}[1]
\Require HiMM $Z = (X,T)$, MM $M \in X$ and state $q \in Q(M)$.
\Ensure HiMM $Z' = (X',T')$ as in Definition \ref{def:state_subtraction} with markings.
\State $Z' \gets \textrm{state\_subtraction}(Z,M,q)$
\State $\textrm{mark\_ancestors}(M,Z')$
\State return $Z'$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Arc\_modification\_and\_marking}\label{alg:arc_modification_and_marking}
\begin{algorithmic}[1]
\Require $Z_{seq} = \{ Z_1,\dots,Z_{n} \} $ and $M$ 
\Ensure HiMM $Z' = (X',T')$ as in Definition \ref{def:arc_modification} with markings.
\State $Z' \gets \textrm{arc\_modification}(Z,M,\delta',\gamma')$
\State $\textrm{mark\_ancestors}(M,Z')$
\State return $Z'$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Composition\_and\_marking}\label{alg:composition_and_marking}
\begin{algorithmic}[1]
\Require $Z_{seq} = \{ Z_1,\dots,Z_{n} \} $ and $M$ 
\Ensure HiMM $Z' = (X',T')$ as in Definition \ref{def:composition} with markings.
\State $Z' \gets \textrm{composition}(Z_{seq} ,M)$
\For{$Z_i$ in $Z_{seq}$}
\State $\textrm{mark\_all}(Z_i,Z')$
\EndFor
\State mark $M$
\State return $Z'$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{mark\_ancestors}\label{alg:mark_ancestors}
\begin{algorithmic}[1]
\Require Node $n$ and HiMM $Z$.
\Ensure All nodes in $Z$ that are ancestors of $n$ and $n$ itself are marked.
\State $C = n$
\While{}
\If{$C$ is not marked}
\State Return \Comment{Nodes above has already been marked}
\Else
\State Mark $C$
\EndIf
\State $C \gets$ parent node of $C$
\EndWhile
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{mark\_all \elis{Need to check this one}}\label{alg:mark_all}
\begin{algorithmic}[1]
\Require Node $n$ and HiMM $Z$.
\Ensure All nodes in $Z$ that are descendants of $n$ are marked.
\If{$n$ is not marked} \Comment{Computation for all descendants has already been done}
\State Return
\EndIf
\For{node $c$ in children of node $n$}
\If{$c$ is not marked}
\State $\mathrm{mark\_all}(c,Z)$
\Else
\State Return
\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

%One might wonder why we do not mark $Z_{\mathrm{add}}$ in state addition or the components $\{Z_1,\dots,Z_n\}$ in composition. The reason is that the optimal exits costs of these HiMMs are unaffected, hence, they are already appropriately marked. First of all, 

%For example, $Z_{\mathrm{add}}$ might already have computed its optimal exit costs, hence, all MMs are (correctly) unmarked, or it might have some MMs that needs to update their optimal exit costs (e.g., has not yet been computed or has been modified). 

%The marked MMs therefore form a (possibly empty) subtree of $T_{\mathrm{add}}$.  

%To explain it, consider $Z_{\mathrm{add}}$. Either, the optimal exit costs of $Z_{\mathrm{add}}$ has not been computed, then, by default, all MMs of $Z_{\mathrm{add}}$ are marked, hence, we do not need to mark them; or, 

%Note also in the state addition case that we do not need to mark $Z_{add}$ (see Definition \ref{def:state_addition}). The reason is that either $Z_{add}$ has already computed its optimal exit costs (which are unaffected by adding it to $Z'$) and is therefore (correctly) not marked.

%State addition (line 1-4) is also similar to arc modification except that we might also need to mark the added HiMM $Z_{\mathrm{add}}$ in $Z'$ done by 

%To describe the marking, consider first the case when the modification is a state addition with $Z=(X,T)$, $M \in X$, $Z_{\mathrm{add}}$ and $Z'$ as in Definition \ref{def:state_addition}. Intuitively, when adding $Z_{add}$ to $M$ in $Z$, $Z_{add}$ is an isolated subtree of $Z$ that cannot be reached from the rest of $Z$ (see Figure \ref{fig:all_operations}). In particular, it cannot affect the optimal exit costs $(c^{M}_x)_{x \in \Sigma}$ nor the corresponding trajectories $(z^{M}_x)_{x \in \Sigma}$ of $M$. In fact, the same is true for any MM $M \in X$ in the unmodified HiMM $Z$. We are left with the MMs of $Z_{\mathrm{add}}$. The MM of $Z_{\mathrm{add}}$ might have already been. 


%The resulting algorithm is summarised in Algorithm \ref{alg:hierarchical_planning_new}, where we also schematically depict its interaction with the (changing) system in Figure \ref{fig:algorithm_overview}.

%where line 2 has been modified to account for changes in $Z$. 

%We provide the details of the updated offline step in Section \ref{offline_step}. Section \ref{online_step} then highlights results concerning the time complexity of the online step referring to \cite{stefansson2023ecc} for additional details concerning the algorithm, since the online step used in this paper is identical to the one in \cite{stefansson2023ecc}.

\iffalse
To this end, a key insight is that this environment is naturally hierarchical. More precisely, at Layer 1 in Figure \ref{fig:robot_example_detailed_overview}, we have the houses and how the robot can move between them. Next, at Layer 2 in Figure \ref{fig:robot_example_detailed_overview}, inside a given house, we have all the locations the robot can move to, given as a grid-world. Finally, at Layer 3, for a specific location, we have the dynamics for how the robot can control an arm over a lab desk to scan tubes. A key question is then how to compute an optimal plan efficiently, exploting 

assigned tasks to scan different lab tubes in 

To make things concrete, consider the following example. A robot is moving between 10 lab houses scanning different tubes. The houses are ordered linearly where the robot can move from any house to any neighbouring house, as depicted in Figure \ref{fig:robot_example_detailed_overview}. The robot can also move inside a house (up, down, left, right), given as a 10x10 grid-world, where the robot starts outside at $S$ (the entrance) as seen in Figure \ref{fig:robot_example_detailed_overview}. At each location inside the house, the robot can also steer an arm (up, down left, right) over a lab desk with a 3x3 test tube rack, and scan the tube the arm is currently over (the robot starts at S in the lab desk, where it can choose to start steering the arm, or go to another location). Each of these actions (e.g., moving to a house or scan a test tube) are also associated with a cost (not depicted) and the aim for the robot is compute a sequence of actions, i.e., a plan, with minimal cumulated cost such that the robot reaches a prescribed state (e.g., having scanned a specified~tube).
\fi

The key insight is that by specifying the modification that has happen, only a small part of the HiMM $Z = (X,T)$ might need to be recalculated in the offline step, potentially saving huge computation time. We formalise this bookkeeping with markings, where a node $n$ in $Z$ is marked if its offline step needs to recalculated. The procedure for doing state addition, state subtraction, arc modification and composition, including marking, are summarised as Algorithm \ref{alg:state_addition_and_marking} to \ref{alg:composition_and_marking}. Once all modifications have been marked, an offline step can be executed, updating only the marked parts of the HiMM $Z$. The procedure is summarised by Algorithm \ref{alg:update_offline_step}, with correctness given by Proposition \ref{prop:update_offline_step_correctness}.

\begin{proposition}\label{prop:update_offline_step_correctness}
Let $Z$ be the original HiMM and $Z'$ be the new HiMM after a finite number of modifications (given by state addition, state subtraction, arc modification and composition) with corresponding markings. Then Algorithm \ref{alg:update_offline_step} returns the corrected optimal exit costs and trajectories.
\end{proposition}

More precisely, for a given node $n \in Q_Z$ of $Z$, it computes the tuple $(c_x^n)_{x \in \Sigma}$, where $c_x^n$ is the based on values further down in the tree of $Z$. Here, $c_x^n$ is the minimal cost for starting in the MM $M$ corresponding to $n$ and exit $M$ with input $x$. Formally:
\begin{definition}
Let
\end{definition}
We start by presenting how a computation is done for a node of $Z$. Towards this, consider a node $n \in Q_Z$ 
and an online step that computes an optimal plan for a given planning objective $(Z,s_{\mathrm{init}},s_{\mathrm{goal}})$ using the result from the offline step. 
In more detail, the offline step computes optimal exits costs for each MM
\subsubsection{The Offline Step} % I think I need to say this in detail since I will modify this algorithm.
Intuitively, 
the offline step computes the optimal cumulative costs it would take to start in 
Let $n$ be a node of $Z$ with corresponding MM $M \in X$. 
The offline step then computes the minimum cumulative cost it would take to start in $\mathrm{start}$
go from the start state of $Z_{sub}$ to exit (stop) $Z_{sub}$ with a given input. 

\subsection{Composition}
%Here I should talk about how one can compose subsystems to one system and rapidly compute the offline step

\begin{algorithm}[h]
\caption{Composition\_offline\_update}\label{alg:composition_offline_update}
\begin{algorithmic}[1]
\Require $Z_{seq} = \{ Z_1,\dots,Z_{n} \} $ and $M$ 
\Ensure $(c_x^n)_{x \in \Sigma}$ and $(z_x^n)_{x \in \Sigma}$ for each node $n$ in $Z_{comp}$.
\State Construct $\hat{M}$ from $M$
\State Let $n_0$ be the corresponding node of $M$ in $Z_{comp}$
\State $(c_x^{n_0})_{x \in \Sigma}$, $(z_x^{n_0})_{x \in \Sigma}$ $\gets$ Dijkstra($s(M),\{E_x\}_{x \in \Sigma},\hat{M}$)
\State return $(c_x^n)_{x \in \Sigma}$ and $(z_x^n)_{x \in \Sigma}$ for each node $n$ in $Z_{comp}$
\end{algorithmic}
\end{algorithm}
The correctness of Algorithm \ref{alg:composition_offline_update} follows from \cite[Proposition 1]{stefansson2023ecc}, with time complexity $O(b_s |\Sigma|+(b_s+|\Sigma|) \log(b_s+|\Sigma|))$ following from \cite[Proposition 2]{stefansson2023ecc}.

\subsection{Modification}
There are several ways one can modify a HiMM.



\subsection{Replacement}

\begin{definition}[Replacement]\label{def:replacement}
Let $Z = (X,T)$ be a HiMM with node $n \in Q_Z$ and let $Z_{rep} = (X_{rep},T_{rep})$ be another HiMM. The replacement of $n$ by $Z_{rep}$ in $Z$ is the HiMM $Z' = (X',T')$ given by:
\begin{enumerate}
\item $T'$ is the tree given by replacing the subtree with root $n$ in $T$ with $T_{rep}$.
\item $X'$ is the resulting set of MMs corresponding to $T'$.
\end{enumerate}
\end{definition}

\begin{proposition}
Let be $Z' = (X',T')$ be the replacement of $n$ by $Z_{rep}$ in $Z = (X,T)$. Then, for the offline step, one only needs to update the optimal exit costs and the optimal exit trajectories of:
\begin{enumerate}
\item The nodes in $Z_{rep}$; and
\item The ancestors of $Z_{rep}$.
\end{enumerate}

\begin{algorithm}[h]
\caption{Replacement algorithm}\label{}
\begin{algorithmic}[1]
\Require Something
\Ensure Updated $Z' = (X',T')$
\State Update $Z_{rep}$
\State Update the ancestors of $Z_{rep}$
\end{algorithmic}
\end{algorithm}

\end{proposition}

\subsection{Modification}
%Here I should talk about how one can modify a part of a system and how one updates the offline step. 
The HiMM $Z_{rep} = (X_{rep},T_{rep})$ that replaces a subtree in a HiMM $Z$ as in Definition \ref{def:replacement} can also be 
\begin{definition}[Modification]
Let $Z = (X,T)$ be a HiMM and $M \in Q_Z$. 
\end{definition}

\begin{algorithm}[h]
\caption{Update\_offline\_step\_after\_modification}\label{alg:optimal_exit_table}
\begin{algorithmic}[1]
\Require HiMM$ \; Z =(X,T)$.
\Ensure $(c_x^n)_{x \in \Sigma}$ and $(z_x^n)_{x \in \Sigma}$ for each node $n$.
\State Optimal\_exit($n_0$) \Comment{$n_0$ corresponding to root MM in $T$}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Reconfigurable hierarchical planning}\label{alg:hierarchical_planning}
\begin{algorithmic}[1]
\Require HiMM$ \; Z =(X,T)$ and states $s_{\mathrm{init}}, s_{\mathrm{goal}}$.
\Ensure Optimal plan $u$ to $(Z,s_{\mathrm{init}}, s_{\mathrm{goal}})$.
\State \textbf{Offline step:}
\State $(c_x^n,z_x^n)_{x \in \Sigma, n \in Q_Z} \gets \mathrm{Offline\_step}(Z)$
\State \textbf{Online step:}
\State $z \gets \mathrm{Reduce\_and\_solve}(Z,s_{\mathrm{init}},s_{\mathrm{goal}},(c_x^n,z_x^n)_{x \in \Sigma, n \in Q_Z})$
%\State Construct graph $G$
%\State $\bar{z}_1 \gets \mathrm{Dijkstra}(s_{\mathrm{init}}, \bar{B}, G)$
%\State Compute optimal trajectory $\bar{z}_1$ from $s_{init}$ to $\bar{B}$ in $\bar{Z}$.
%\State Compute optimal trajectory $\bar{z}_2$ from $\bar{B}$ to $s_{\mathrm{goal}}$ in $\bar{Z}$.
%\State $\bar{z} \gets \bar{z}_1 \bar{z}_2$.
\State $u \gets \mathrm{Expand}(z,(z_x^n)_{x \in \Sigma, n \in Q_Z}, Z)$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Update\_offline\_step}\label{alg:optimal_exit_table}
\begin{algorithmic}[1]
\Require HiMM$ \; Z =(X,T)$.
\Ensure $(c_x^n)_{x \in \Sigma}$ and $(z_x^n)_{x \in \Sigma}$ for each node $n$.
\State Optimal\_exit($n_0$) \Comment{$n_0$ corresponding to root MM in $T$}
\end{algorithmic}
\end{algorithm}
