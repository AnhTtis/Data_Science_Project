\section{Results}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[h]
\small
\begin{tabular}{p{1cm}p{2cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}}
\toprule
\multicolumn{1}{l}{}                                   & \multicolumn{1}{l}{}                  & \multicolumn{4}{c}{\textbf{Classification}}                                                      & \multicolumn{3}{c}{\textbf{Question Answering}}        \\ \midrule
\multicolumn{1}{l}{}                                   & \multicolumn{1}{l}{}                  & \multicolumn{1}{l}{\textbf{XNLI}}  & \multicolumn{1}{l}{\textbf{Indic-XNLI}} & \multicolumn{1}{l}{\textbf{PAWS-X}} & \textbf{XCOPA}     & \textbf{TyDiQA-GoldP}  & \textbf{XQUAD}         & \textbf{MLQA}          \\ \midrule
\multicolumn{1}{c}{\multirow{4}{*}{\textbf{Fine-Tuned Models}}} & \multicolumn{1}{l}{mBERT}             & \multicolumn{1}{l}{0.654} & \multicolumn{1}{l}{0.547}      & \multicolumn{1}{l}{0.819}  & 0.561     & 0.597 / 0.439 & 0.645 / 0.494 & 0.614 / 0.442 \\ \cline{2-9}
\multicolumn{1}{c}{}                                   & \multicolumn{1}{l}{XLM-R Large} & \multicolumn{1}{l}{0.792} & \multicolumn{1}{l}{0.697}     & \multicolumn{1}{l}{0.864}  & 0.692     & 0.651 / 0.45  & 0.766 / 0.608 & 0.716 / 0.532 \\ \cline{2-9}
\multicolumn{1}{c}{}                                   & MuRIL                                  & NA                         & \textbf{0.763}                          & NA                          & NA        & NA            & NA            & NA            \\ \cline{2-9}
\multicolumn{1}{c}{}                                   & TuLRv6 - XXL                           & \textbf{0.888}                & NA                              & \textbf{0.932}                      & 0.822     &\textbf{0.846} / \textbf{0.738} & \textbf{0.860} / \textbf{0.729}  & \textbf{0.810} / \textbf{0.639}  \\ \midrule
\multirow{4}{*}{\textbf{Prompt Based}}                 & BLOOMZ                                 & 0.542                  &                       NA          & 0.822                      & 0.604 & NA            & NA            & NA            \\ \cline{2-9}
                                                         & DaVinci-003 Zero-Shot Cross Lingual    & 0.599                & 0.449                     & 0.636                      & 0.71      & 0.457 / 0.34  & 0.405 / 0.28  & 0.436 / 0.309 \\ \cline{2-9}
                                                         & DaVinci-003 Monolingual                & 0.593                & 0.496                     & 0.670                     & 0.747  & 0.497 / 0.383 & NA            & 	0.484 / 0.331  \\ \cline{2-9}
                                                         & DaVinci-003 Translate-Test             & 0.67                & 0.624                     & 0.685                      & \textbf{0.838}  & NA            & NA            & 0.549 / 0.346
\\
                                                         \bottomrule
\end{tabular}
\caption{Summary of the results for our benchmarking exercise. For classification tasks the values correspond the accuracy and for QA we have provided the F1-Score / EM values.}
\label{tab:results_summary}
\end{table*}

We analyze the results of MEGA over each task in two parts - first, we present the differences in performance for the DV003 model with the prompting strategies defined earlier. Then, we compare the performance of the best DV003 system to BLOOMZ (if available) and SOTA models such as TULR and MuRIL. In each graph, the Y axis is ordered by language class according to the classes proposed in \cite{joshi2020state}. Class 5 corresponds to very high resource languages such as English, while Class 1 and 2 are extremely under-resourced languages. We include results on English, taken from the English version of each benchmark, for each task we evaluate on to show the gap between performance in English and other languages.

\subsection{Comparison across prompting strategies}

Figure \ref{fig:xnlidv003} shows the performance of DV003 on the XNLI task for the three prompt settings. Translate-test performs best across all languages. For Class 5 languages such as es, fr, de, the performance of translate-test is close to the zero-shot cross-lingual prompt setting, with monolingual prompting not being very far behind. For Class 4 languages, such as tr, ru, vi and hi, there is a large gap between translate-test and the other two settings, with the monolingual setting being slightly better for ru and hi. Similarly, for Class 3 and 4 languages, we see that there is a large gap between translate-test and the other two settings. Importantly, we see a large gap between English performance and the translate-test performance even for high resource languages.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=8cm]{figures/xnli_dv003_eval_settings.pdf}
    \caption{XNLI: DV003}
    \label{fig:xnlidv003}
\end{figure}

Next, we look at results from IndicXNLI presented in Figure \ref{fig:indicxnli}. Once again, the translate-test setting outperforms the other settings by a large margin, and the gap is larger for languages below Class 5. We also see that performance is much better for the monolingual setting compared to zero-shot cross-lingual prompting.

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/IndicXNLI-DV003.png}
    \caption{Indic-XNLI: DV003}
    \label{fig:indicxnli}
\end{figure}

Figure \ref{fig:xcopadv003} shows the performance of DV003 on the XCOPA task. The XCOPA dataset has a couple of extremely low resource languages like qu and ht that belong to Class 1 and 0 respectively. In case of XCOPA, we again find that the translate-test setting works best across most languages. This is especially stark in languages like ta, th and sw, which use their own native scripts. Both monolingual and zero-shot settings perform very poorly for languages in Class 3 and below which are not in the Latin script. Performance on qu and ht in the monolingual setting is slightly better than in the zero-shot cross-lingual setting. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/xcopa_dv003_eval_settings.png}
    \caption{XCOPA: DV003. We do not report numbers for the translate-test setting for qu as it is not supported by Bing translator}
    \label{fig:xcopadv003}
\end{figure}

Figure \ref{fig:pawsxdv003} shows the results on PAWS-X, which has a smaller number of languages all in Class 4 and 5. We see similar trends here that translate-test performs best, except in the case of de, where monolingual prompting works best. The performance of the non-Latin script languages (ja, zh, ko) is much worse than English performance even in the translate-test setting and in the other two settings compared to de, es and fr, even though ja and zh are high resource languages. For all languages, the monolingual setting outperforms the zero-shot cross-lingual setting, which follows the same trend the we observe with XNLI and XCOPA.

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{
    figures/pawsx-DV003-eval-settings.png}
    \caption{PAWS-X: DV003}
    \label{fig:pawsxdv003}
\end{figure}

Next, we look at results for TyDiQA with two metrics, exact match shown in Figure \ref{fig:tydiqaem} and F1 in Figure \ref{fig:tydiqaf1}. As seen before, monolingual prompting outperforms zero-shot cross lingual prompting. We do not test for translate-test as described earlier in Sec 4.2.

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/tydiqa_dv003_em_val_settings.png}
    \caption{TyDiQA: DV003 - Exact Match}
    \label{fig:tydiqaem}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/tydiqa_dv003_f1_val_settings.png}
    \caption{TyDiQA: DV003 - F1}
    \label{fig:tydiqaf1}
\end{figure}

For MLQA shown in Figure \ref{fig:mlqaem} and Figure \ref{fig:mlqaf1}, we find that the results are similar for low resource and non-Latin script languages. For class 5 languages in Latin script (es, de), we find that all three prompting methods work equally well. For zh, the monolingual prompting technique works best in terms of Exact Match, while for both ar and zh, translate-test work much better. For languages in Class 4 (vi and hi), translate-test works best.

The XQUAD dataset does not have test data in the target languages, so we only evaluate the zero-shot cross lingual setup, the results of which can be found in Figure \ref{fig:xquademcomparison} and Figure \ref{fig:xquadf1comparison}. We see similar trends as before, with large drops in performance for low-resource and non-Latin script languages compared to English.

We used the same evaluation setups for the IndicQA task, however, the results were very poor due to the quality of chunks retrieved for each question. We plan to improve the algorithm used for the QA task in future versions of this study. This issue may also be alleviated in future versions of generative models that allow for larger context sizes.

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/mlqa_dv003_em_val_settings.pdf}
    \caption{MLQA: DV003 - Exact Match}
    \label{fig:mlqaem}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/mlqa_dv003_f1_val_settings.pdf}
    \caption{MLQA: DV003 - F1}
    \label{fig:mlqaf1}
\end{figure}



% \myworries{TO ADD: IndicQA results}

\subsection{Comparison across models}

Next, we compare the performance of DV003 with generative and SOTA non-autoregressive models. Since the translate-test setting generally outperforms the other prompting strategies, we compare DV003 translate-test, the next best DV003 prompt strategy (monolingual or cross-lingual), TULRv6 or MuRIL and BLOOMZ (when available). 

Figure \ref{fig:xnlicomparison} shows the comparison of DV003, BLOOMZ and TULRv6 on XNLI. We see that for all languages, the performance of TULRv6 is significantly better than the other models. We also observe that the differences in performance between en and other languages is much less in TULRv6, compared to DV003 and BLOOMZ. BLOOMZ performs worse than DV003 translate-test, but outperforms the next best DV003 strategy for ar, vi, hi.

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/xnli_models_comparison.pdf}
    \caption{Comparison of DV003, BLOOMZ and TULRv6 on XNLI}
    \label{fig:xnlicomparison}
\end{figure}

Figure \ref{fig:indicxnlicomparison} shows the comparison between DV003 (Translate-test) and MuRIL. Similar to the comparisons with TULR, we find that the MuRIL model outperforms DV003 on all languages by a significant margin.

\begin{figure}[!h]
\centering
\includegraphics[width=8cm]{figures/IndicXNLI-DVmonoTT-MuRIL.png}
\caption{Comparison of DV003 (Translate Test), DV003 (Monolingual) and MuRIL on IndicXNLI}
\label{fig:indicxnlicomparison}
\end{figure}
    
Figure \ref{fig:xcopacomparison} shows the comparison of the same models for XCOPA. Here, we find much less difference between the performance of TULRv6 and DV003 translate-test. However, we find that the BLOOMZ performance is very low for all languages except id.

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/xcopa_models_comparison.png}
    \caption{Comparison of DV003, BLOOMZ and TULRv6 on XCOPA}
    \label{fig:xcopacomparison}
\end{figure}

Figure \ref{fig:pawsxcomparison} shows the comparison of the models for PAWS-X. TULRv6 performs the best for all languages, followed by BLOOMZ. There is a considerable difference between performance values of BLOOMZ and DV003 or DV003-TranslateTest, except for zh.

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/pawsx-models-comparison.png}
    \caption{Comparison of DV003, BLOOMZ and TULRv6 on PAWS-X}
    \label{fig:pawsxcomparison}
\end{figure}

Figure \ref{fig:tydiqaemcomparison} and \ref{fig:tydiqaf1comparison} show the comparison of DV003 with TULR for TyDiQA for exact match and F1 respectively. Here too, we see that TULR outperforms DV003 on both metrics, particularly for languages in non-Latin scripts such as ru, ko, bn and te. Recall that we do not test against translate-test for TyDiQA due to the alignment issues mentioned earlier. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/tydiqa_models_comparison_em.png}
    \caption{Comparison of DV003 and TULRv6 on TyDiQA on Exact Match}
    \label{fig:tydiqaemcomparison}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/tydiqa_models_comparison_f1.png}
    \caption{Comparison of DV003 and TULRv6 on TyDiQA on F1}
    \label{fig:tydiqaf1comparison}
\end{figure}

For XQUAD, shown in Figure \ref{fig:xquademcomparison} and \ref{fig:xquadf1comparison} (exact match and F1 respectively), we find that the gap between TULRv6 and DV003 is higher. The gap is highest for languages such as zh and ar in class 5, and all languages in classes 3 and 4.

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/xquad_models_comparison_em.png}
    \caption{Comparison of DV003 and TULRv6 on XQUAD on Exact Match}
    \label{fig:xquademcomparison}
\end{figure}


\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/xquad_models_comparison_f1.png}
    \caption{Comparison of DV003 and TULRv6 on XQUAD on F1}
    \label{fig:xquadf1comparison}
\end{figure}

Comparisons on the MLQA task are shown in Figure \ref{fig:mlqaemcomparison} and Figure \ref{fig:mlqaf1comparison}. Here too, TULRv6 outperforms DV003 in all languages, with the gap being higher in low-resource and non-Latin script languages. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/mlqa_models_comparison_em.pdf}
    \caption{Comparison of DV003 and TULRv6 on MLQA on Exact Match}
    \label{fig:mlqaemcomparison}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/mlqa_models_comparison_f1.pdf}
    \caption{Comparison of DV003 and TULRv6 on MLQA on F1}
    \label{fig:mlqaf1comparison}
\end{figure}

We provide a summary of all the results averaged over all languages in Table \ref{tab:results_summary}. We also include comparisons with Multilingual BERT and XLM-R \cite{conneau2020unsupervised} on all tasks. 
