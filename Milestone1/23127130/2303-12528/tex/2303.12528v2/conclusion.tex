\section{Conclusion}

In this work, we show that generative LLMs perform worse across tasks and languages, when compared to SOTA models, even when the input is translated to English and used as the query to the system. We find that generative models perform better on higher-resource languages and languages that are in the Latin script. We discuss the quality of  tokenization in GPT as one of the potential reasons for this gap.

We also show that the choice of prompting strategy matters - the monolingual prompting setup outperforms the cross-lingual prompting strategy. We may be able to get higher performance from generative models by tuning prompts further. However, tuning prompts for each language and task is challenging to scale and translating prompts automatically can lead to lower performance, which may explain the poor performance of generative models compared to SOTA models in the monolingual setting. Hence, there is a need to scale up multilingual prompt generation with humans-in-the-loop for extracting the best performance from generative models.

While translate-test is currently the best strategy across languages for generative models, it is important to consider the implications of translation carefully as discussed in this paper. Finally, we urge the research and AI community to prioritize automatic benchmarking and human evaluation across as many languages as possible, so that we do not leave behind large populations of the world in the Generative AI wave. We hope that this work spurs research in meeting this goal.