\section{Discussion}

% Some of these can be moved to the intro as appropriate
In this section, we discuss some of our findings during the benchmarking study. Overall, we find that while the translate-test setting works best for DV003 across all tasks, there is a gap between the performance of generative LLMs (both DV003 and BLOOMZ) and SOTA models on these tasks across languages. We also find that this gap is reduced in case of high-resource languages that share the same script as English (the Latin script), and is higher for under-resourced languages. This is consistent with findings in previous benchmarking studies on generative LLMs \cite{bang-etal-2021-assessing, hendy2023good} In this section, we discuss some of the reasons this is the case, along with other implications of this study.

\subsection{Tokenization}

Tokenization is a key component that influences the performance of multilingual models \cite{rust-etal-2021-good}. Figure \ref{fig:tokenizer-comparison} highlights the disparity in DV003's tokenization to the tokenizer in mBERT \cite{bert2019} and a language-specific tokenizer. This points to a clear direction for improvement in generative models for multilingual settings. 

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=16cm]{figures/fertility.pdf}
    \caption{Tokenizer Fertility for GPT and mBERT for different languages}
    \label{fig:tokenizer-fertility}
\end{figure*}


\begin{figure}[!htb]
    \includegraphics[width=6cm]{figures/hi_sentence_length.pdf}
    \caption{Comparing the distribution of the number of tokens produced for different sentences by GPT, mBERT, and reference number of tokens based on Univeral Dependencies in Hindi. The substantial increase in the number of tokens for GPT compared to mBERT and the reference might be indicative of its inability to capture the long-form context in the prompts provided for these languages, resulting in poor performance.}
    \label{fig:tokenizer-comparison}
\end{figure}

Since tokenization is crucial to generating a meaningful representation of the input, its differential behaviour across languages can help explain the model's poor performance across some languages (especially while being prompted in the monolingual setting). Figure \ref{fig:tokenizer-fertility} demonstrates the tokenization of DV003's tokenizer on a set of languages. The poor tokenization for some of the lower-resource languages can lead to issues \textbf{\textit{(a) poor context encapsulation}} the  tokenization limits the length of context that can be provided for a given language which can seriously impede the performance of models for low-resource languages. Additionally, prior work like \cite{Zhang2022HowRI} also shows that degenerate tokenization is strongly related to the generation of \textbf{\textit{(b) poor semantic representations of inputs}}, which affects the performance of such representations on downstream tasks.  

\subsection{Translating prompts to multiple languages}

One key observation of translating prompts (in the monolingual prompting setting) is that the resulting prompts are often not semantically meaningful, including for some moderate-resource languages like Hindi. This is typically due to the difference in the linguistic structure of the pivot language (English) and the target language. This means that \textbf{\textit{human-supervision or post-editing by a native speaker}} is necessary to generate meaningful prompts for such languages. 

For example: For IndicWNLI, a Winograd-scheme variant of NLI on Indian languages, Figure \ref{fig:marathinli} shows a tuple generated through \textit{Translate-Test} for Marathi for a prompt provided in English. Translating the prompt automatically changes the meaning of the instructions, in this case, swapping the order of the hypothesis and premise, making it difficult for the model to perform the task correctly. 

\begin{figure*}[h]
\centering
    \includegraphics[width=16cm]{figures/marathinli.png}
    \caption{Marathi prompt for IndicWNLI translated from English}
    \label{fig:marathinli}
\end{figure*}

Furthermore, the use of translation also appears to favor languages which share scripts with higher-resource languages included in the pre-training corpus of such models. This complies with existing studies like \cite{khemchandani-etal-2021-exploiting}, which show that sharing dominant scripts leads to better cross-lingual transfer amongst languages leading to improved performance on downstream tasks for unseen languages. This improved transfer, combined with the explored gain due to the seen language's better tokenization \cite{Rust2020HowGI} could explain the gap between languages that use the Latin script vs languages that use other scripts. 

\subsection{Translate-Test}

% Translation - translate-test seems to work best overall. comment on gap between en performance and translate-test.
\begin{table*}[h]
\centering 
\begin{tabular}{cccccccccl}
\hline
\multicolumn{10}{c}{\textbf{Performance for M2M MMT on Flores101 Devtest}} \\ \hline
\multicolumn{1}{c}{\textbf{as}} & \multicolumn{1}{c}{\textbf{gu}} & \multicolumn{1}{c}{\textbf{kn}} & \multicolumn{1}{c}{\textbf{ml}} & \multicolumn{1}{c}{\textbf{or}} & \multicolumn{1}{c}{\textbf{te}} & \multicolumn{1}{c}{\textbf{mr}} & \multicolumn{1}{c}{\textbf{pa}} & \multicolumn{1}{c}{\textbf{sw}} & \multicolumn{1}{c}{\textbf{ta}} \\ \hline
\multicolumn{1}{c}{3.76} & \multicolumn{1}{c}{0.51} & \multicolumn{1}{c}{1.56} & \multicolumn{1}{c}{11.15} & \multicolumn{1}{c}{0.14} & \multicolumn{1}{c}{1.86} & \multicolumn{1}{c}{10.35} & \multicolumn{1}{c}{5.65} & \multicolumn{1}{c}{26.95} & \multicolumn{1}{c}{3.35} \\ \hline
\multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{c}{\textbf{bg}} & \multicolumn{1}{c}{\textbf{th}} & \multicolumn{1}{c}{\textbf{ur}} & \multicolumn{1}{c}{\textbf{ru}} & \multicolumn{1}{c}{\textbf{tr}} & \multicolumn{1}{c}{\textbf{vi}} & \multicolumn{1}{c}{\textbf{hi}} & \multicolumn{1}{c}{\textbf{ko}} &  \\ \hline
\multicolumn{1}{c}{} & \multicolumn{1}{c}{37.35} & \multicolumn{1}{c}{11.63} & \multicolumn{1}{c}{15.04} & \multicolumn{1}{c}{27.14} & \multicolumn{1}{c}{24.88} & \multicolumn{1}{c}{35.1} & \multicolumn{1}{c}{27} & \multicolumn{1}{c}{18.51} &  \\ \hline
\multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{c}{\textbf{it}} & \multicolumn{1}{c}{\textbf{fr}} & \multicolumn{1}{c}{\textbf{es}} & \multicolumn{1}{c}{\textbf{de}} & \multicolumn{1}{c}{\textbf{ar}} & \multicolumn{1}{c}{\textbf{zh}} & \multicolumn{1}{c}{\textbf{ja}} & \multicolumn{1}{c}{\textbf{bn}} &  \\ \hline
\multicolumn{1}{c}{} & \multicolumn{1}{c}{27.74} & \multicolumn{1}{c}{41.99} & \multicolumn{1}{c}{25.57} & \multicolumn{1}{c}{32.56} & \multicolumn{1}{c}{17.92} & \multicolumn{1}{c}{19.33} & \multicolumn{1}{c}{22.77} & \multicolumn{1}{c}{22.86} &  \\ \hline
\end{tabular}
\label{tab:translation-analysis}
\caption{Performance of M2M MMT \cite{10.1162/tacl_a_00474} on Flores101: We recall this performance evaluation in order to provide the reader a ballpark of the expected ranges of translation models for the languages we consider}
\end{table*}

Our experiments with DV003 show that the translate-test prompt setting works best overall for all languages and tasks. Translating into English and querying the system (followed by translating into the target language for generative tasks) is feasible for the languages supported by translators available today. To analyze the impact of translation performance, we present the translation performance of M2M \cite{10.1162/tacl_a_00474} in Table 4 on a diverse, domain-agnostic test set, Flores101 \cite{goyal2022flores}. These numbers provide a ballpark for SOTA translation model performance for the languages we consider, in the absence of numbers from the Bing translation service we use. As observed, low-resource languages do not have very competitive translation performance, meaning that translation errors will propagate. Interestingly, since monolingual querying performance for these languages is also poor, translate-test comes out as the best strategy even for them.  We expect, and empirically observe, the efficacy of translate-test varies by the same trend. High-resource languages tend to show stable gains through translate-test, and lower-resource show, variable (though consistent improvements) through translate-test querying. 

However this process of translating into English, sending the query to a generative model, and translating back must be considered carefully. By using this strategy, we forgo the knowledge encoded in each language. The pitfalls of doing so can be similar to when languages die \cite{harrison2007languages} - loss of information relating to local culture, contexts and people. Further, subtleties of language and culture may be lost during translation, and the language produced by translation systems is known to contain artefacts, leading some to dub this kind of language as "translationese" \cite{graham2019translationese}. In addition, Responsible AI guardrails, filters and techniques for reducing bias will need to be rethought and be added at the translation stage if translate-test is the chosen strategy.

\section{Looking Forward}

In this paper, we present our initial work towards a comprehensive benchmarking of generative AI models across languages. In the future, we would like to extend our benchmarking along the following dimensions. Firstly, we would like to expand language coverage to include more low-resource languages and languages that are typologically diverse.  We would also like to include more standard NLP tasks, as well as tasks that are taken from real-world applications, such as the Bing tasks included in XGLUE \cite{liang2020xglue}. In this work, we benchmark DV003 and compare against BLOOMZ - in future work, we would like to expand our evaluation to include more generative models such as GPT4. Additionally, we would also like to include other dimensions such as calibration, fairness, toxicity, bias and disinformation. We also plan to incorporate datasets that represent multilingual communication, such as code-switching. As pointed out by \cite{hendy2023good}, the sufficiency of the metrics and the quality of available datasets especially for less resourced languages, may not adequately capture performance. While this might be true, benchmarking does serve as an effective starting point and highlights important trends. However, any holistic evaluation would consider such metrics in conjunction with human-evaluations by users.
Any real world deployment of the technology, especially for multilingual use-case scenarios, would not be productive and successful in meeting user-requirements without a thorough evaluation, ideally a combination of automatic evaluation and human-feedback.

% Talk about dangers of deploying these models in multilingual settings without proper evaluation. Benchmarking is an ok starting point but need much more robust real-world evaluation and user feedback.