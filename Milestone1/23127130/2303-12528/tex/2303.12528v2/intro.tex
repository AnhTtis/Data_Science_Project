\section{Introduction}
Generative Large Large Models (LLMs) such as ChatGPT have created a lot of interest in the AI community and beyond due to the step jump in their capabilities, such as maintaining context over conversations, fluency of generation and reasoning. Many users have reported having tested these systems on languages other than English, with varying results, and recent demos of these models (such as Bing+GPT) \cite{newbingevent} have been shown in multiple (albeit high-resource) languages. Recently, the GPT-4 model \cite{gpt4techreport} was evaluated on the MMLU multiple choice questions benchmark by automatically translating it into 26 languages, and the results for some low-resource languages in the Latin script was found to be promising. 

GPT-3 contains unlabeled pre-training data in 90 languages \cite{brown-etal-2020-language}, while the BLOOM \cite{scao2022bloom} model created by the BigScience community is trained on 46 natural languages, and Google's PaLM \cite{chowdhery2022palm} model is trained on 122 languages. While these models have been trained on multiple languages with varying distributions in the pre-training data, it is not clear how well they perform across diverse tasks and languages.

Figures \ref{fig:langdistrgpt}, \ref{fig:langdistrbloom} and \ref{fig:langdistrpalm} show the distribution of languages in the pre-training data of GPT3, BLOOM and PaLM respectively. We observe that a significant proportion of the data is in the English language for all three models, with even the BLOOM model, which aims to be a multilingual model, having around 40\% data in English and code. It is unclear how much data is present in the tail languages in these models, and more crucially, what the quality of the data is, which makes it difficult to ascertain whether these models are ready to be used for these languages. \cite{joshi2020state} reviewed the challenges involved in promoting linguistic diversity, such as the lack of resources and infrastructure for developing NLP tools for underrepresented languages. They provided an overview of the efforts being made to promote linguistic diversity and inclusion including the creation of data sets and tools for underrepresented languages. They provide a classification of languages based on the amount of labeled and unlabeled data available for languages, and find that most of the world's population is under-served in terms of availability of data for their languages. This raises an important question about what the advent of generative Large Language Models (LLMs) means for the state of language coverage, and how to ensure that everyone across the world is able to benefit from generative AI. 

\begin{figure*}[h!]
    \centering
    \includegraphics[width=12cm]{figures/Life-Architect-GPT-3-90-languages.png}
    \caption{Language Distribution in LLMs - GPT3}
    \label{fig:langdistrgpt}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=12cm]{figures/Life-Architect-BLOOM-46-languages.png}
    \caption{Language Distribution in LLMs - BLOOM}
    \label{fig:langdistrbloom}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=12cm]{figures/Life-Architect-PaLM-122-languages.png}
    \caption{Language Distribution in LLMs - PALM}
    \label{fig:langdistrpalm}
\end{figure*}

Robust and comprehensive evaluation helps us in understanding the capabilities of AI systems. Evaluation of LLMs has been an active area of research, with benchmarks such as GLUE \cite{wang2018glue} and SuperGLUE \cite{wang2019superglue} proposed a few years ago for English, followed by more recent benchmarks for multilingual evaluation, such as XTREME \cite{hu2020xtreme}, XTREME-R \cite{ruder2021xtreme} and XGLUE \cite{liang2020xglue}. Multilingual benchmarks aim to cover a variety of tasks and languages, so that we can determine whether a foundation model is able to generalize. Benchmarks covering specific language families, such as IndicXTREME \cite{doddapaneni2022indicxtreme} for Indian languages, \cite{wilie2020indonlu} for Indonesian languages and GLUECoS \cite{khanuja2020gluecos} and LinCE \cite{aguilar2020lince} for code-switching have also been proposed.

However, evaluating generative AI models is challenging due to limitations in automatic overlap based metrics such as BLEU and ROUGE and the open-ended nature of conversations that can be had with such systems \cite{Graham2022}. In prior work, we have described the challenges of scaling up multilingual evaluation \cite{ahuja-etal-2022-beyond} due to the lack of representative datasets covering languages from different language families, typological diversity of languages in benchmark datasets and coverage of low-resource languages. We have also pointed out the limitations of using translated datasets for evaluation (such as the one that has been used to evaluate GPT4 in the technical report), due to the loss of local and cultural context in such benchmarks \cite{ahuja-etal-2022-beyond, ahuja2022economics}. However, it is crucial that we make an attempt to improve multilingual evaluation of LLMs, because prior work shows that good performance or gains on some high resource languages do not always result in similar gains in all languages \cite{wu2020all}, with various factors possibly influencing the performance of models on different languages \cite{litmus}.

Recently, there have been attempts at holistic evaluation of generative AI models for English (HELM) \cite{liang2022holistic}, in which several LLMs are evaluated on multiple dimensions such as accuracy, calibration, robustness, fairness, bias etc. \cite{bang2023multitask} conduct an evaluation of ChatGPT across tasks and languages. They evalaute ChatGPT on Sentiment Analysis, Language Identification and Machine Translation and find that it fails to identify low-resource languages, and generate non-Latin script languages. \cite{hendy2023good} evaluate the translation abilities of three GPT models - ChatGPT, GPT3.5 (text-davinci-003) and text-davinci-002 and find that GPT models perform well on translating high-resource languages, and do not perform well on low-resource languages. Recent work \cite{bubeck2023sparks} has pointed out the limitations of using standard NLP benchmarks to evaluate generative models, due to the pace at which these benchmarks become saturated. However, in this work, we show that multilingual NLP benchmarks are far from saturated, and hence, benchmarking is a viable approach to get a sense of the performance of such models on different languages. 

In this work, we carry out a comprehensive Multilingual Evaluation of Generative AI (MEGA). We quantify how well generative LLMs perform across languages across standard multilingual benchmarks covering various NLP tasks such as Natural Language Inference, Commonsense Reasoning, Paraphrase Detection and Question Answering. Our goal is to conduct a comprehensive benchmarking study for many languages of the world, along the lines of HELM \cite{liang2022holistic}, and we plan to benchmark other dimensions of interest and relevance to non-English languages and multilingualism, such as calibration, fairness, bias, toxicity, robustness etc. in future versions of the study. This is very challenging due to the scarcity of datasets available in non-English languages for measuring fairness, toxicity and bias as we discuss in prior work \cite{ramesh2023fairness}, however it is important to evaluate all these dimensions across languages to ensure that we build responsible and accurate systems for all. In our initial study which we present in this paper, we restrict ourselves to the accuracy dimension. 

We aim to answer the following questions: 

\begin{itemize}
    \item How well do generative models understand instructions, perform tasks and generate text in different languages?
    \item Which languages do generative models perform well in?
    \item How well do generative models such as GPT3.5 and BLOOM fare on standard multilingual benchmarks compared to SOTA models, such as Turing ULR on standard NLP tasks? 
    \item  Which prompting strategies should be used for using generative LLMs for non-English languages?
    \item  What are the challenges we foresee in making generative AI work accurately for all languages of the world? 
\end{itemize} 

We evaluate the text-davinci-003 (referred to as DV003 in this paper) model from OpenAI on standard NLP tasks across many languages, shown in Table \ref{tab:datasets} and compare its performance to BLOOMZ \cite{muennighoff2022crosslingual} and SOTA models for specific tasks, such as TULRv6 \cite{patra2022beyond} and MuRIL \cite{khanuja2021muril}. In addition, we compare various prompting strategies for DV003, including the translate-test setting, in which the entire prompt along with the data point is translated into English and sent to the model, followed by back-translation into the target language, if required (eg. for QA).

Our work is the first comprehensive benchmarking of generative AI models across tasks, languages and prompting strategies. In addition to answering the questions we pose above, our study also results in a blueprint of strategies that can be used for building systems using generative AI for multilingual users. Another contribution of this work is the MEGA benchmarking framework that can be used by individual teams for evaluating their system on a specific task across languages, and for the research community to scale up multilingual evaluation of generative models that come in the future. We plan to release the MEGA benchmarking code to facilitate this. We plan to conduct a similar benchmarking of GPT4 in the near future.