\section{Related Work}

% \noindent \textbf{Evaluation of LLMs.} Recently, there has been an increasing interest in evaluating LLMs on a wide range of capabilities, given the surge in their popularity and effectiveness. BIG-bench by \citet{srivastava2023imitation} consists of 204 tasks to evaluate LLMs consisting of a diverse range of problems 
% % from linguistics, commonsense reasoning, math, etc, 
% to evaluate the capabilities of LLMs. While BIG-bench includes tasks in non-English languages as well, they are largely related to translation. \citet{liang2022holistic} proposed Holistic Evaluation of Language Models (HELM) defining a taxonomy of scenarios (tasks, domains, and languages) and metrics (eg. accuracy, calibration, toxicity) that define the space of LLM evaluation, and evaluate 30 language models on 42 scenarios and 7 metrics. However, all the scenarios are focused on datasets in standard English or dialects, and they highlight coverage of languages as an important area for improvement. 

\noindent \textbf{Evaluation of LLMs.} A growing interest in the evaluation of LLMs has harbingered several efforts towards the holistic evaluation of their capabilities. While work like BIG-bench \citet{srivastava2023imitation} cover a diverse range of tasks, the non-English tasks are mostly translation-oriented which limit the more general task based inferences that for such an evaluation. Similarly, \citet{liang2022holistic} propose a taxonomy of scenarios and metrics in  Holistic Evaluation of Language Models (HELM) to define the space of LLM evaluation, and evaluate 30 language models on 42 scenarios and 7 metrics. However, all the scenarios are focused on datasets in standard English or its dialects.
 % highlight coverage of languages as an important area for improvement. 


% Apart from the standard NLP tasks there is also interest in evaluating LLMs for more advanced capabilities like causal reasoning \cite{emre2023causal} and theory of mind (ToM) \cite{kosinski2023theory}. 
% Recent work \cite{bubeck2023sparks}, has pointed out the limitations of using standard NLP benchmarks to evaluate generative models, due to the pace at which these benchmarks become saturated.

\paragraph{Multilingual Benchmarks and Evaluation.} 
% Following the popularity of benchmarks like GLUE \cite{wang2018glue} and Super-GLUE \cite{wang2019superglue}, similar b
Benchmarks for multilingual evaluation, such as XTREME \cite{hu2020xtreme}, XTREME-R \cite{ruder2021xtreme} and XGLUE \cite{liang2020xglue} have been proposed to measure cross-lingual transfer in pre-trained language models. Following their popularity, there has been the development of benchmarks covering specific language families, such as IndicXTREME \cite{doddapaneni2022indicxtreme} for Indian languages, \citet{adelani-etal-2022-masakhaner} for African Languages,  and \citet{wilie2020indonlu} for Indonesian languages, as well. The evaluations on these benchmarks have mainly focused on pre-train then fine-tune kinds of setups.
% Naturally, the evaluations on these benchmarks have mainly focused on pre-train then fine-tune kinds of setups covering models like mBERT \cite{devlin-etal-2019-bert}, XLM-RoBERTa \cite{conneau-etal-2020-unsupervised}, and mT5 \cite{xue-etal-2021-mt5}. 
Particularly for prompting style evaluation, \citet{bang2023multitask} evaluates the multilingual capabilities of ChatGPT and shows that it fails to generalize to low-resource languages with non-latin scripts. However, multilingual evaluation is performed only on a few tasks, 
% Sentiment Analysis, Language Identification, and Translation datasets, 
and a subset of 50-100 examples are used for testing the model. 
\citet{hendy2023good} evaluate the translation abilities of GPT-3.5 models and find that these models, while perform well in translating high-resource languages, their capabilities for low-resource languages are limited. 
Concurrent work BUFFET \cite{asai2023buffet} and \citet{lai2023chatgpt} also perform multilingual benchmarking of large language models, however, they evaluate the performance of ChatGPT and BLOOMZ in their work while our evaluation also spans GPT-4.

\paragraph{Multilingual Prompting:} 
While most work on prompting or in-context learning in LLMs focuses on English data, recently, there has been some interest in prompting them with non-English data. \citet{zhao-schutze-2021-discrete}, for instance, use discrete and soft prompting techniques to evaluate XLM-RoBERTa and show that prompting can be more effective compared to fine-tuning when the amount of labeled data is limited. \citet{lin-etal-2022-shot} show that English prompts perform better than prompts written in the target language (both hand-written and translated). Finally, \cite{shi-etal-2022-language} show chain-of-thought (CoT) prompting results leads to striking multilingual reasoning capabilities in LLMs, even in under-represented languages especially when prompted when English CoT. 
% train an autoregressive language model XGLM (7.5 billion) by up-sampling pre-training data from low-resource languages and show it performs much better compared to GPT-3 (6.7 billion) across languages. They
% and the best performance is always obtained by first translating inputs from other languages to English (Translate-Test)


% \noindent \textbf{Multilingual Prompting.} 
% % While most work on prompting or in-context learning in LLMs focuses on English data, r
% Recently, there has been some interest in prompting LLMs
% % these models 
% with non-English data as well. \citet{zhao-schutze-2021-discrete} use discrete and soft prompting techniques to evaluate XLM-RoBERTa and show that prompting can be more effective compared to fine-tuning when the amount of labeled data is limited. \citet{lin-etal-2022-shot},  also show that English prompts perform better than prompts written in the target language (both hand-written and translated). \cite{shi-etal-2022-language} show chain-of-thought (CoT) prompting results leads to striking multilingual reasoning capabilities in LLMs, even in under-represented languages like Swahili and Bengali. They find that providing CoT in English usually performs better than language-specific CoTs.
% % train an autoregressive language model XGLM (7.5 billion) by up-sampling pre-training data from low-resource languages and show it performs much better compared to GPT-3 (6.7 billion) across languages. They
% % and the best performance is always obtained by first translating inputs from other languages to English (Translate-Test)

% Recently, there have been attempts at holistic evaluation of generative AI models for English (HELM) \cite{liang2022holistic}, in which several LLMs are evaluated on multiple dimensions such as accuracy, calibration, robustness, fairness, bias etc. \citet{bang2023multitask} conduct an evaluation of ChatGPT across tasks and languages. They evaluate ChatGPT on Sentiment Analysis, Language Identification and Machine Translation and find that it fails to identify low-resource languages, and generate non-Latin script languages. \citet{hendy2023good} evaluate the translation abilities of three GPT models - ChatGPT, GPT3.5 (text-davinci-003) and text-davinci-002 and find that GPT models perform well on translating high-resource languages, and do not perform well on low-resource languages. Recent work \cite{bubeck2023sparks}, has pointed out the limitations of using standard NLP benchmarks to evaluate generative models, due to the pace at which these benchmarks become saturated. However, in this work, we show that multilingual NLP benchmarks are far from saturated, and hence, benchmarking is a viable approach to get a sense of the performance of such models in different languages. Concurrent work BUFFET \cite{asai2023buffet} and \citet{lai2023chatgpt} also perform multilingual benchmarking of large language models, however, they evaluate the performance of ChatGPT and BLOOMZ in their work while our evaluation also spans GPT-4 and GPT-3.5 models.

% Evaluating NLP systems has been an active area of research, with benchmarks such as GLUE \cite{wang2018glue} and SuperGLUE \cite{wang2019superglue} proposed a few years ago for English, 

% followed by more recent benchmarks for multilingual evaluation, such as XTREME \cite{hu2020xtreme}, XTREME-R \cite{ruder2021xtreme} and XGLUE \cite{liang2020xglue}. Multilingual benchmarks aim to cover a variety of tasks and languages, so that we can determine whether a foundation model is able to generalize. Benchmarks covering specific language families, such as IndicXTREME \cite{doddapaneni2022indicxtreme} for Indian languages, \citet{adelani-etal-2022-masakhaner} for African Languages,  \citet{wilie2020indonlu} for Indonesian languages and GLUECoS \cite{khanuja2020gluecos} and LinCE \cite{aguilar2020lince} for code-switching have also been proposed.

% However, evaluating generative AI models is challenging due to limitations in automatic overlap based metrics such as BLEU and ROUGE and the open-ended nature of conversations that can be had with such systems \cite{Graham2022}. 
% Prior work has described the challenges of scaling up multilingual evaluation \cite{ahuja-etal-2022-beyond} due to the lack of representative datasets covering languages from different language families, typological diversity of languages in benchmark datasets and coverage of low-resource languages. 
% We have also pointed out the limitations of using translated datasets for evaluation (such as the one that has been used to evaluate GPT4 in the technical report), due to the loss of local and cultural context in such benchmarks \cite{ahuja-etal-2022-beyond, ahuja2022economics}.
% However, it is crucial that we improve multilingual evaluation of LLMs, because prior work shows that good performance or gains on some high resource languages may not always result in similar gains in all languages \cite{wu2020all}, with various factors possibly influencing the performance of models on different languages \cite{litmus}.


% In this work, we carry out a comprehensive Multilingual Evaluation of Generative AI (MEGA). We quantify how well generative LLMs perform across languages across standard multilingual benchmarks covering various standard NLP tasks such as Natural Language Inference, Commonsense Reasoning, Question Answering, Summarization, and Sequence Labelling. Our goal is to conduct a comprehensive benchmarking study for many languages of the world, along the lines of HELM \cite{liang2022holistic}, and we plan to benchmark other dimensions of interest such as calibration, bias, robustness etc. in future versions of the study. This is very challenging due to the scarcity of datasets available in non-English languages for measuring fairness, toxicity and bias as we discuss in prior work \cite{ramesh2023fairness}, however it is important to evaluate all these dimensions across languages to ensure that we build responsible and accurate systems for all. 

% We aim to answer the following questions: 

% \begin{itemize}
%     \item How well do generative models understand instructions, perform tasks and generate text in different languages?
%     \item Which languages do generative models perform well in?
%     \item How well do generative models such as GPT3, GPT4 and BLOOM fare on standard multilingual benchmarks compared to SOTA models, such as Turing ULR on standard NLP tasks? 
%     \item  Which prompting strategies should be used for using generative LLMs for non-English languages?
%     \item  What are the challenges we foresee in making generative AI work accurately for all languages of the world? 
% \end{itemize} 

% We evaluate the following OpenAI models: \texttt{text-davinci-003} (referred to as DV003 in the paper), \texttt{gpt-3.5-turbo} (referred to as GPT-3.5-Turbo in the paper), and \texttt{gpt-4-32k} (referred to as GPT-4 in the paper) on 16 NLP datasets across $\sim70$ languages, shown in Table \ref{tab:datasets} and compare its performance to BLOOMZ \cite{muennighoff2022crosslingual} and SOTA models for specific tasks, such as TULRv6 \cite{patra2022beyond} and MuRIL \cite{khanuja2021muril}. In addition, we compare various prompting strategies, including the translate-test setting (round-tripping through English by translating into English, querying in English and back-translating if applicable).

% Our work is the first comprehensive benchmarking of generative AI models across models, datasets, languages and prompting strategies. In addition to answering the questions we pose above, our study also results in a blueprint of strategies that can be used for building systems using generative AI for multilingual users. Another contribution of this work is the MEGA benchmarking framework that can be used for evaluation of a model or system across languages, and for the research community to scale up multilingual evaluation of generative models that come in the future. We plan to release the MEGA benchmarking code to facilitate this.