\section{Results and Analysis}
\label{sec:results}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table*}[h]
% \small
% \begin{tabular}{p{1cm}p{2cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}}
% \toprule
% \multicolumn{1}{l}{}                                   & \multicolumn{1}{l}{}                  & \multicolumn{4}{c}{\textbf{Classification}}                                                      & \multicolumn{3}{c}{\textbf{Question Answering}}        \\ \midrule
% \multicolumn{1}{l}{}                                   & \multicolumn{1}{l}{}                  & \multicolumn{1}{l}{\textbf{XNLI}}  & \multicolumn{1}{l}{\textbf{Indic-XNLI}} & \multicolumn{1}{l}{\textbf{PAWS-X}} & \textbf{XCOPA}     & \textbf{TyDiQA-GoldP}  & \textbf{XQUAD}         & \textbf{MLQA}          \\ \midrule
% \multicolumn{1}{c}{\multirow{4}{*}{\textbf{Fine-Tuned Models}}} & \multicolumn{1}{l}{mBERT}             & \multicolumn{1}{l}{0.654} & \multicolumn{1}{l}{0.547}      & \multicolumn{1}{l}{0.819}  & 0.561     & 0.597 / 0.439 & 0.645 / 0.494 & 0.614 / 0.442 \\ \cline{2-9}
% \multicolumn{1}{c}{}                                   & \multicolumn{1}{l}{XLM-R Large} & \multicolumn{1}{l}{0.792} & \multicolumn{1}{l}{0.697}     & \multicolumn{1}{l}{0.864}  & 0.692     & 0.651 / 0.45  & 0.766 / 0.608 & 0.716 / 0.532 \\ \cline{2-9}
% \multicolumn{1}{c}{}                                   & MuRIL                                  & NA                         & \textbf{0.763}                          & NA                          & NA        & NA            & NA            & NA            \\ \cline{2-9}
% \multicolumn{1}{c}{}                                   & TuLRv6 - XXL                           & \textbf{0.888}                & NA                              & \textbf{0.932}                      & 0.822     &\textbf{0.846} / \textbf{0.738} & \textbf{0.860} / \textbf{0.729}  & \textbf{0.810} / \textbf{0.639}  \\ \midrule
% \multirow{4}{*}{\textbf{Prompt Based}}                 & BLOOMZ                                 & 0.542                  &                       NA          & 0.822                      & 0.604 & 0.752            & 0.708            & NA            \\ \cline{2-9}
%                                                          & DaVinci-003 Zero-Shot Cross Lingual    & 0.599                & 0.449                     & 0.636                      & 0.71      & 0.457 / 0.34  & 0.405 / 0.28  & 0.436 / 0.309 \\ \cline{2-9}
%                                                          & DaVinci-003 Monolingual                & 0.593                & 0.496                     & 0.670                     & 0.747  & 0.497 / 0.383 & NA            & 	0.484 / 0.331  \\ \cline{2-9}
%                                                          & DaVinci-003 Translate-Test             & 0.67                & 0.624                     & 0.685                      & \textbf{0.838}  & NA            & NA            & 0.549 / 0.346
% \\
%                                                          \bottomrule
% \end{tabular}
% \caption{Summary of the results for our benchmarking exercise. For classification tasks the values correspond the accuracy and for QA we have provided the F1-Score / EM values.}
% \label{tab:results_summary}
% \end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage{graphicx}







% \begin{table*}[]
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{l ccc ccc}
% \toprule
% Model & \multicolumn{3}{c}{IndicXNLI} &  \multicolumn{3}{c}{IndicQA} \\
% \cmidrule(lr){2-4} \cmidrule(lr){5-7}
% & Avg. & Best & Worst & Avg. & Best & Worst \\
% \midrule
% MuRIL & \textbf{72.4} &  \textbf{76.3} (hi) & \textbf{67.6} (ur) & \textbf{47.7} & 56.9 (te) & \textbf{43.2} (as) \\ 

% \texttt{text-davinci-003} & 49.6 & 56.9 (hi) & 45.5 (ta) & 8.45 & 16.8 (hi) & 3.57 (ml)\\

% \texttt{text-davinci-003} (TT) & 62.4 & 66.0 (bn) & 56.0 (as) & - & - & -\\

% \texttt{gpt-3.5-turbo} & 50.7 & 55.5 (hi) & 47.4 (or) & 42.6 & \textbf{67.2} (hi) & 21.7 (or)\\
% \texttt{gpt-3.5-turbo} (TT) & 59.7 & 62.3 (pa) & 54.3 (as) & - & - & - \\
% \midrule
% \end{tabular}}
% \caption{Comparing performance of \texttt{text-davinci-003} and \texttt{gpt-3.5-turbo} with fine-tuned baseline MuRIL \cite{khanuja2021muril} on Indic datasets \cite{doddapaneni2022indicxtreme}. For IndicXNLI we report Accuracy and F1-score for IndicQA.}
% \label{tab:indic_results}
% \end{table*}


% \begin{table}[]
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{l c c}
% \toprule
% Model & NLI En-Hi & Sentiment En-Es \\
% \midrule
% mBERT & 63.1 & 69.31 \\
% \texttt{text-davinci-003} & 72.1 & \textbf{68.8} \\
% \texttt{gpt-3.5-turbo} & \textbf{78.8} & 68.0 \\
% \bottomrule
% \end{tabular}}
% \caption{Performance of GPT-3.5 models on code-mixing datasets from \cite{khanuja2020gluecos}. For both the tasks, the metric reported is accuracy.}
% \label{tab:cm_results}
% \end{table}



% \begin{table*}[h]
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}llllllllll@{}}
% \toprule
%  &
%    &
%   \multicolumn{4}{c}{\textbf{Classification}} &
%   \multicolumn{4}{c}{\textbf{Question Answering}} \\ \midrule
%  &
%    &
%   XNLI &
%   \begin{tabular}[c]{@{}l@{}}Indic\\ XNLI\end{tabular} &
%   PAWS-X &
%   XCOPA &
%   \begin{tabular}[c]{@{}l@{}}TyDiQA-\\ GoldP\end{tabular} &
%   XQUAD &
%   MLQA &
%   IndicQA \\ \midrule
% \multirow{8}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Fine-Tuned \\ Models\end{tabular}}} &
%   \multirow{2}{*}{mBERT} &
%   \multirow{2}{*}{0.654} &
%   \multirow{2}{*}{0.547} &
%   \multirow{2}{*}{0.819} &
%   \multirow{2}{*}{0.561} &
%   0.597 / &
%   0.645 / &
%   0.614 / &
%   0.329 \\
%  &
%    &
%   &
%   &
%   &
%   &
%   0.439 &
%   0.494 &
%   0.442 &
%   - \\ \cmidrule(l){2-10} 
%  &
%   \multirow{2}{*}{XLM-R Large} &
%   \multirow{2}{*}{0.792} &
%   \multirow{2}{*}{0.697} &
%   \multirow{2}{*}{0.864} &
%   0.692 &
%   0.651 / &
%   0.766 / &
%   0.716 / &
%   0.448 \\
%  &
%    &
%   &
%   &
%   &
%   &
%   0.45 &
%   0.608 &
%   0.532 &
%   - \\ \cmidrule(l){2-10} 
%  &
%   \multirow{2}{*}{MuRIL} &
%   - &
%   \textbf{0.763} &
%   - &
%   - &
%   - &
%   - &
%   - &
%   \textbf{0.477} \\
%  &
%    &
%   - &
%   - &
%   - &
%   - &
%   - &
%   - &
%   - &
%   - \\ \cmidrule(l){2-10} 
%  &
%   \multirow{2}{*}{TuLRv6 - XXL} &
%   \textbf{0.888} &
%   - &
%   \textbf{0.932} &
%   \textbf{0.822} &
%   \textbf{0.846 /} &
%   \textbf{0.860 /} &
%   \textbf{0.810 /} &
%   - \\
%  &
%    &
%   - &
%   - &
%   - &
%   - &
%   \textbf{0.738} &
%   \textbf{0.729} &
%   \textbf{0.639} &
%   - \\ \midrule
% \multirow{9}{*}{\textbf{Prompt Based}} &
%   \multirow{2}{*}{BLOOMZ} &
%   0.542 &
%   - &
%   0.822 &
%   0.604 &
%   0.752 / &
%   0.708 / &
%   - &
%   0.366 / \\
%  &
%    &
%   - &
%   - &
%   - &
%   - &
%   0.632 &
%   0.586 &
%   - &
%   0.325 \\ \cmidrule(l){2-10} 
%  &
%   \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}DaVinci-003\\ Zero-Shot\\ Cross Lingual\end{tabular}} &
%   0.599 &
%   0.449 &
%   0.636 &
%   0.71 &
%   0.457 / &
%   0.405 / &
%   0.436 / &
%   0.08 / \\
%  &
%    &
%   \multirow{2}{*}{-} &
%   \multirow{2}{*}{-} &
%   \multirow{2}{*}{-} &
%   \multirow{2}{*}{-} &
%   \multirow{2}{*}{0.34} &
%   \multirow{2}{*}{0.28} &
%   \multirow{2}{*}{0.309} &
%   \multirow{2}{*}{0.04} \\
%  &
%    &
%    &
%    &
%    &
%    &
%    &
%    &
%    &
%    \\ \cmidrule(l){2-10} 
%  &
%   \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}DaVinci-003\\ Monolingual\end{tabular}} &
%   0.593 &
%   0.496 &
%   0.670 &
%   0.747 &
%   0.497 / &
%   - &
%   0.484 / &
%   - \\
%  &
%    &
%   - &
%   - &
%   - &
%   - &
%   0.383 &
%   - &
%   0.331 &
%   - \\ \cmidrule(l){2-10} 
%  &
%   \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}DaVinci-003\\ Translate-Test\end{tabular}} &
%   0.67 &
%   0.624 &
%   0.685 &
%   \textbf{0.838} &
%   - &
%   - &
%   0.549 / &
%   - \\
%  &
%    &
%   - &
%   - &
%   - &
%   - &
%   - &
%   - &
%   0.346 &
%   - \\ \bottomrule
% \end{tabular}%
% }
% \caption{Summary of the results for our benchmarking exercise. For classification tasks the values correspond the accuracy and for QA we have provided the F1-Score / EM values.}
% \label{tab:results_summary}
% \end{table*}


In this section, we analyze the results of our benchmarking exercise across tasks and languages. Broadly, we cover the comparison between the effectivness of various prompting strategies \S\ref{comparison-diff-prompt} followed by the performance comparison of GPT-3.5 and GPT-4 models with appropriate baselines \S\ref{comparison-diff-models}. We conclude with an examination of the factors that affects the performance of these models \S\ref{diff-factors}.

% fine-tuned and prompt-based baselines, on different tasks and languages. 


% We compare the effectiveness of various prompting strategies for OpenAI models in testing their multilingual capabilities. 



% We start by providing an aggregate overview of the results and discussing the general trends that we observe. We then move to a more detailed discussion, going over each dataset and comparing the performance of different models, and prompting strategies for languages included in that dataset.
% \vspace{-2mm}
\subsection{Comparing different prompting strategies}
\label{comparison-diff-prompt}
\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{figures/prompting_strategies.pdf}
\caption{Comparing different prompting strategies discussed in \textsection \ref{sec:prompt_strategies} on DV003 and GPT-3.5-Turbo. The $y$-axis denotes the task-wise performance metric, e.g. Accuracy for XNLI and F1-Score for TyDiQA-GoldP. A list of metrics for all the tasks is provided in Table \ref{tab:agg_results_summary}.}
\label{fig:prompting_comp}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{figures/tt_rel_imp.pdf}
\caption{Relative percentage improvement over Monolingual prompting when using Translate-Test for GPT-3.5-Turbo. The bars are color-coded based on the class taxonomy provided in \cite{joshi-etal-2020-state}}
\label{fig:tt_rel}
\end{figure*}
 In Figure \ref{fig:prompting_comp}, we compare the performance of the three prompting strategies. We find that translate-test often improves the performance over the monolingual strategy, especially so in the case of DV003. We also find that for datasets, which include many low resource and non-latin script languages like IndicXNLI and XStoryCloze, the gains with translate-test are even more substantial for both the models. 
 % More concretely, for languages like Burmese, Tamil, and Telugu we see upto $>30\%$ relative improvement by \ttest{} over \mono{}, while for high-resource languages such as French and Spanish, the two perform similarly (refer to Figure \ref{fig:tt_rel} in Appendix for details).
 % large relative improvements by \ttest{} over \mono{} 
 In Figure \ref{fig:tt_rel}, we present the average (over different tasks) relative improvement by Translate-Test over Monolingual on \turboinf{}  for different languages and observe for languages like Burmese, Tamil, and Telugu the relative improvement can be $> 30\%$! In general, we see that for low-resource languages, the translate-test results in substantial improvement in performance, while for high-resource languages the two perform similarly. While we do not evaluate \gptfourinf{} \ttest{} exhaustively for all tasks, we do run the tests for XStoryCloze and XCOPA datasets. Based on these two, we observe that \gptfourinf{}'s \mono{} prompting performance is often much more on-par with \ttest{} and many times even better. However, for low-resource languages we again see \ttest{} to perform much better. e.g., in XStoryCloze \gptfourinf{}'s accuracy on Burmese is $77.6\%$ vs $93.2\%$ for \mono{} and \ttest{} respectively ( Figures \ref{fig:xstoryclozev2} and \ref{fig:xcopa} in Appendix).

 % Note that the efficacy of translate-test on low-resource languages is not substain

 Note that while \ttest{} substantially improves performance on low-resource languages, compared to the performance of these models in English, the gap even after \ttest{} is significantly high. For example, using translate-test with \turboinf{} for Urdu in XNLI results in $54\%$ accuracy compared to $49.1\%$ for monolingual. However, this contrasts with the $76.2\%$ accuracy that the same model achieves in English. 
 % Note that the efficacy of \ttest{} is also seen on \gptfourinf{} limited evaluation on two tasks: XStoryCloze and XCOPA. Interestingly though, \gptfourinf{}'s \mono{} prompting performance is often much more on-par with \ttest{} here. 
 
 
 
 % We must point out that while \ttest{} can help substantially in improving performance on low-resource languages, when compared to performance in English there is still a huge gap. For example, for Urdu in XNLI, \turboinf{} with translate-test results in $54\%$ accuracy over $49.1\%$ for monolingual, however, the performance is significantly worse compared to English, where the same model achieves $76.2\%$ accuracy. While we do not evaluate \gptfourinf{} \ttest{} exhaustively for all tasks, we do run the tests for XStoryCloze and XCOPA datasets. Based on these two, we observe that \gptfourinf{}'s \mono{} prompting performance is often much more on-par with \ttest{} and many times even better, however, for low-resource languages we again see \ttest{} to perform much better. For e.g., in XStoryCloze \gptfourinf{}'s accuracy on Burmese is $77.6\%$ vs $93.2\%$ for \mono{} and \ttest{} respectively.
 
 \zscl{} prompting for \dvthreeinf{} often performs on par with \mono{} but for \turboinf{}, there is a drop in performance, especially so for tasks like XCOPA which have some extremely low resource languages: Quechua and Haitian Creole. For these languages, we observed that when provided few-shot examples in English, \turboinf{} would often resort to predicting outputs like \textit{"I'm sorry, but the premise is not in a language that I understand."}. However, by providing examples in the language, we are able to ground the model to these languages and we almost never observe such predictions in that case.



% \begin{figure*}
% \includegraphics[width=0.95\textwidth]{figures/tt_rel_imp.pdf}
% \caption{Relative percentage improvement over Monolingual prompting when using Translate-Test for GPT-3.5-Turbo. The bars are color-coded based on the class taxonomy provided in \cite{joshi-etal-2020-state}}
% \label{fig:tt_rel}
% \end{figure*}

\begin{table*}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l cccc ccc cc c}
\toprule
\multirow{2}{*}{Model} & \multicolumn{4}{c}{Classification} & \multicolumn{3}{c}{Question Answering} & \multicolumn{2}{c}{Sequence Labelling} & \multicolumn{1}{c}{Summarization} \\
 & XNLI & PAWS-X & XCOPA & XStoryCloze & XQuAD & TyDiQA-GoldP &  MLQA  & UDPOS & PAN-X & XLSum \\
\midrule
Metrics & Acc. & Acc. & Acc. & Acc. & F1 / EM & F1 / EM & F1 / EM & F1 & F1 & ROUGE-L \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 65.4 & 81.9 & 56.1 & \texttimes & 64.5 / 49.4 & 59.7 / 43.9 & 61.4 / 44.2 & 71.9 & 62.2 & \texttimes \\
mT5-Base & 75.4 & 86.4 & 49.9 & \texttimes & 67.0 / 49.0 & 57.2 / 41.2 & 64.6 / 45.0 & - & 55.7 & \underline{\textbf{28.1}}$^{\dagger}$ \\
XLM-R Large & 79.2 & 86.4 & 69.2 & \texttimes & 76.6 / 60.8 &	65.1 / 45.0	& 71.6 / 53.2 & 76.2 & 65.2 & \texttimes\\
TuLRv6 - XXL & \underline{\textbf{88.8}}$^{\dagger}$ & \underline{\textbf{93.2}}$^{\dagger}$ & \textbf{82.2}$^{\dagger}$ & \texttimes & \underline{\textbf{86 / 72.9}}$^{\dagger}$ & \underline{\textbf{84.6 / 73.8}}$^{\dagger}$ & \underline{\textbf{81 / 63.9}}$^{\dagger}$ &  \underline{\textbf{83.0}}$^{\dagger}$ &  \underline{\textbf{84.7}}$^{\dagger}$ & \texttimes \\
\midrule
\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & 54.2 & \textbf{(82.2)}$^{\ddagger}$ & 60.4 & 76.2 & \textbf{(70.7 / 58.8)}$^{\ddagger}$ & \textbf{(75.2 / 63.2)}$^{\ddagger}$ & - & - & - & -\\
% XGLM & 47.3 & 54.5 & 62.0 & 66.5 &- & - & - & - & - & - \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
% \texttt{text-davinci-003} (CL) & 59.9 & 63.62 & 71.0 & 40.5 / 28.0 & 45.7 / 34.0 & 43.6 / 30.9 & - & - & -\\
% \texttt{text-davinci-003} (M) & 59.27 & 67.08 & 74.7 & - & 49.7 / 38.3 & 44.0 / 28.8 & - & - & -\\
\texttt{text-davinci-003} & 59.27 & 67.08 & 75.2 & 74.7 & 40.5 / 28.0 & 49.7 / 38.3 & 44.0 / 28.8 & - & - & -\\
\texttt{text-davinci-003} (TT) & 67.0 & 68.5 & 83.8 & 94.8 & \texttimes & \texttimes & 54.9 / 34.6 & \texttimes & \texttimes & - \\
\texttt{gpt-3.5-turbo} & 62.1 & 70.0 & 79.1 & 87.7 & 60.4 / 38.2 & 60.1 / 38.4 & 56.1 / 32.8 & \textbf{60.2}$^{\ddagger}$ & 40.3 & 18.8  \\
\texttt{gpt-3.5-turbo} (TT) & 64.3 & 67.2 & 81.9 & 93.8 & \texttimes & \texttimes & 46.3 / 27.0 & \texttimes & \texttimes & 16.0*  \\
\texttt{gpt-4-32k} & \textbf{75.4}$^{\ddagger}$ & 73.0 & \underline{\textbf{89.7}$^{\ddagger}$} & \underline{\textbf{96.5}$^{\ddagger}$} & 68.3 / 46.6 & 71.5 / 50.9 & \textbf{67.2 / 43.3}$^{\ddagger}$ & \textbf{66.6}$^{\ddagger}$ & \textbf{55.5}$^{\ddagger}$ & \textbf{19.7}$^{\ddagger}$ \\
\bottomrule
\end{tabular}}
% \vspace{-3mm}
\caption{Average performance across languages in each of the different datasets included in MEGA. TT suffix refers to the translate-test prompting strategy discussed in Section \ref{sec:prompt_strategies}, without any suffix we refer to the monolingual strategy by default (except for XQuAD and IndicQA where it refers to cross-lingual setup). Numbers in \textbf{bold} with $\dagger$ symbol indicate best performing Fine-tuned model and the ones with $\ddagger$ refer to the best prompt-based generative model. The best overall numbers are \underline{underlined}. For BLOOMZ the values in parenthesis indicate that the model was fine-tuned on the task during multi-task training. Missing values corresponding to the `\texttimes' symbol denote experiments that were not applicable and the ones with `-' were the ones deprioritized due to limited compute. \texttt{gpt-3.5-turbo} (TT) on XL-Sum was only evaluated on 29 languages which are supported by Bing Translator.}
\label{tab:agg_results_summary}
\end{table*}
% \begin{table*}[]
% \small
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{l ccc ccc}
% \toprule
% Model & \multicolumn{3}{c}{IndicXNLI} &  \multicolumn{3}{c}{IndicQA} \\
% \cmidrule(lr){2-4} \cmidrule(lr){5-7}
% & Avg. & Best & Worst & Avg. & Best & Worst \\
% \midrule
% MuRIL & \textbf{72.4} &  \textbf{76.3} (hi) & \textbf{67.6} (ur) & \textbf{47.7} & \textbf{56.9 (te)} & \textbf{43.2} (as) \\ 

% \texttt{text-davinci-003} & 49.6 & 56.9 (hi) & 45.5 (ta) & 8.45 & 16.8 (hi) & 3.57 (ml)\\

% \texttt{text-davinci-003} (TT) & 62.4 & 66.0 (bn) & 56.0 (as) & - & - & -\\

% \texttt{gpt-3.5-turbo} & 50.7 & 55.5 (hi) & 47.4 (or) & 38.6 & 55.9 (hi) & 22.1 (or)\\
% \texttt{gpt-3.5-turbo} (TT) & 59.7 & 62.3 (pa) & 54.3 (as) & - & - & - \\
% \midrule
% \end{tabular}}
% \caption{Comparing performance of \texttt{text-davinci-003} and \texttt{gpt-3.5-turbo} with fine-tuned baseline MuRIL \cite{khanuja2021muril} on Indic datasets \cite{doddapaneni2022indicxtreme}. For IndicXNLI we report Accuracy and F1-score for IndicQA.}
% \label{tab:indic_results}
% \end{table*}



% \vspace{-2mm}
\subsection{Comparing different models}
\label{comparison-diff-models}
The aggregated results comparing different models and prompting strategies are provided in Table \ref{tab:agg_results_summary} and Table \ref{tab:indic_results} (for Indic Datasets). Excluding the commonsense reasoning tasks XCOPA and XStoryCloze, the OpenAI models generally lag behind the fine-tuned baseline TULRv6 for most tasks often by a significant margin and often are only slightly better than some of the smaller fine-tuned multilingual models i.e. mBERT and mT5-base. Between OpenAI models and BLOOMZ, the former models tend to outperform the latter (despite having a larger proportion of multilingual pre-training data), except for datasets like PAWS-X, XQUAD, and TyDiQA-GoldP, where BLOOMZ performs better. However, it must be noted that all these three datasets were present in the multi-task fine-tuning stage for BLOOMZ, especially for XQUAD and TyDiQA-GoldP for which the validation data that we use for evaluation is also likely to be included in the fine-tuning data\footnote{Note that this can be a possibility for OpenAI models as well and we discuss this in more detail in \textsection \ref{sec:contam}.}.

Between the OpenAI models, generally \dvthreeinf{} and \turboinf{} perform on par, with \ttest{} performance of \dvthreeinf{} being generally better than \turboinf{}, and the other way around for \mono{} performance. However, we do observe a notable exception to this, which is for the QA tasks where \turboinf{} performs substantially better than \dvthreeinf{}, especially so for IndicQA. We attribute this to the fact that in order to fit the prompt in the 4096 context size for \dvthreeinf{}, we had to resort to retrive-then prompt strategy and imperfect retrieval for low-resource languages leads to worse performance. Please check \textsection \ref{sec:retrieval} of Appendix for more details on this. For \gptfourinf{} on the other hand, we consistently observe substantial improvements, with it being \textit{Pareto Optimal} \cite{choudhury2021how} compared to the two GPT-3.5 models for all datasets with an exception of XL-Sum, where for some languages \turboinf{} performs better. For the detailed results spanning all models, tasks, and languages, please refer to Appendix \S\ref{sec:appendix_res}.



% \begin{figure*}
% \centering

% \begin{subfigure}[t]{0.8\textwidth}
% \includegraphics[width=0.95\textwidth]{figures/turbo-lang-family.pdf}
%     \caption{Language-Family wise performance of \texttt{gpt-3.5-turbo} on various tasks.}
%     \label{fig:turbo_lang_fam}
% \end{subfigure}
% \begin{subfigure}[t]{0.8\textwidth}
% \includegraphics[width=0.95\textwidth]{figures/turbo-script.pdf}
%     \caption{Script wise performance of \texttt{gpt-3.5-turbo} on various tasks.}
%     \label{fig:turbo_script}
% \end{subfigure}
% \caption{Language family and script wise performance of GPT-3.5-Turbo across tasks}
% \end{figure*}

% \subsection{Overall results and general trends}
% \noindent \textbf{Comparing different models}: The aggregated results comparing different models and prompting strategies are provided in Tables \ref{tab:agg_results_summary} and \ref{tab:indic_results} (for Indic Datasets). Excluding the commonsense reasoning task XCOPA, the OpenAI models generally lag behind the fine-tuned baseline TULRv6 for most tasks often by a significant margin and often are only slightly better than some of the smaller fine-tuned multilingual models that we study i.e. mBERT and mT5-base. Between OpenAI models and the other prompt-based baselines, the former models tend to outperform the latter (despite having a larger proportion of multilingual pre-training data), except for tasks like PAWS-X, XQUAD, and TyDiQA-GoldP, where BLOOMZ performs better. However, it must be noted that all these 3 tasks were present in the multi-task fine-tuning stage for BLOOMZ. Between the OpenAI models, the general trend includes GPT-3.5-Turbo outperforming DV003, and GPT-4  often obtaining significant improvements over both of the models (for the tasks where we evaluate it).



% \noindent \textbf{Linguistic Comparison}: In Figures \ref{fig:turbo_lang_fam} and \ref{fig:turbo_script}, we group and aggregate the performance of different tasks on language families and language scripts respectively for GPT-3.5-Turbo. As can be observed across tasks, the model's performance is generally best on the Germanic language family and Latin scripts. We have similar observations for DV003 and GPT-4 models that we provide in Figures \ref{fig:dv003_famscript} and \ref{fig:gpt4_famscript} of the Appendix. 

% We analyze the results of MEGA over each task in three ways. For example, first, we present the differences in performance for the DV003 model with the prompting strategies defined earlier and compare the performance of the best DV003 system to BLOOMZ (if available) and SOTA models such as TULR and MuRIL. 
% In each graph, the Y axis is ordered by language class according to the classes proposed in \cite{joshi2020state}.
% We include results on English, taken from the English version of each benchmark, for each task we evaluate on, to show the gap between performance in English and other languages. 

% Class 5 corresponds to very high resource languages such as English, while Class 1 and 2 are extremely under-resourced languages. 

 % Second, we compare the results of DV003 performance with Turbo performance for the translate-test and monolingual prompting setups. 
 
 % Finally, we compare the performance of all models with GPT4's performance using the monolingual prompting setup on select datasets through which we can represent the performance of these models across our broad classification of tasks: inference and generation.

% \vspace{-2mm}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fertility.pdf}
    \vspace{-3mm}
    \caption{Tokenizer Fertility for OpenAI models, mBERT, and BLOOM for different languages}
    \vspace{-3mm}
    \label{fig:tokenizer-fertility}
\end{figure*}


\begin{figure}
\centering
    \includegraphics[width=0.33\textwidth]{figures/fertility_score_corr.pdf}
    \caption{Correlation between the performance of GPT-3.5-Turbo with the tokenizer fertility. We report the curves for the cases where the person coefficient $|\rho| > 0.7$ with a p-value of 0.05. We have combined Indic-XNLI and XNLI for a better coverage of languages. Similar plots for GPT-4 can be found in Figure \ref{fig:fertility_score_corr_gpt4} of Appendix.}
    \label{fig:fertility_perf_corr}
\end{figure}

\subsection{Factors Explaining Performance Trends}
\label{diff-factors}

In this section, we try to understand what factors influence our observed trends in   multilingual LLM capabilities. We begin by investigating the \textit{Fertility} of the tokenizers used by different models, which is defined as the average number of sub-words produced per tokenized word (higher means worse quality), as that has been shown to critically impact the downstream task performance of pre-trained multilingual models \cite{rust-etal-2021-good}. In Figure \ref{fig:tokenizer-fertility}, we plot the tokenizer fertility of different models. We observe that the tokenizers for the OpenAI models are substantially worse for low-resource, non-latin script languages: where the fertility for languages like Malayalam and Tamil is so high ($\sim 10$) that the tokenizer essentially operates as a byte-level tokenizer for these languages. Note that this means that for low-resource languages, substantially larger number of tokens are needed to encode the inputs as well as for generation, which results in a significant additional API costs. \citet{Ahia2023DoAL} discusses how this phenomenon leads to large socio-economic disparities for speakers of underrepresented languages.
% We expect this degenerate tokenization to adversely impact the (a) context we can provide to the model as in-context examples across tasks and the (b) the fluency of construction in generation tasks \cite{10.1145/3578707}.  
We study if these discrepancies in the tokenizer's quality across languages have any effect on the performance. 
As can be seen in Figure \ref{fig:fertility_perf_corr}, for six tasks we observe statistically significant (negative) correlations between the tokenizer's fertility and dataset-specific performance i.e. the models obtain worse performance on languages for which the tokenizer is of poor quality, and vice-versa. 


% In this section, we try to understand what factors influence the trends in the multilingual capabilities of LLMs that we observe above. Tokenizer quality is one of the factors that has been shown to play an important role in the downstream task performance of pre-trained multilingual models \cite{rust-etal-2021-good}. We measure \textit{Fertility} of the tokenizers of different models that we study and observe that the tokenizers for the OpenAI models are substantially worse in quality for low-resource non-latin script languages, where the fertility for languages like Malayalam and Tamil is so high ($\sim 10$) that the tokenizer essentially operates as a byte-level tokenizer for these languages (refer to Appendix Figure  \ref{fig:tokenizer-fertility} for detailed numbers).


% which is defined as the average number of sub-words produced per tokenized word (higher means worse quality) and is shown in Figure \ref{fig:tokenizer-fertility}. As can be seen, the tokenizers for the OpenAI models are substantially worse in quality for low-resource non-latin script languages, where the fertility for languages like Malayalam and Tamil is so high that the tokenizer essentially operates as a byte-level tokenizer for these languages. 

We also study the effect that the amount of data available for each language during pre-training \cite{wu-dredze-2020-languages, lauscher-etal-2020-zero} has on the multilingual performance of these models. We measure the correlations between the language-wise number of tokens present in the pre-training data with language-wise performance on each dataset. While the exact language-wise pre-training data distribution for GPT-3.5 and GPT-4 models is not available, we use the GPT-3's language-wise pretraining distribution as a proxy. We observe that for four tasks (PAWS-X, XNLI, XCOPA, and XQuAD) statistically significant positive correlations between the pre-training data size and performance. Note that, the amount of pre-training data and tokenizer fertility are highly likely to be correlated with each other. However, we do see that using pre-training data we are able to explain some trends that are not explained by tokenizer fertility alone.
For example, even though the OpenAI models have similar tokenizer fertilities for both French and Japanese, these models perform much better in French than they do for Japanese (72.1\% accuracy vs 67\% accuracy for \turboinf{}) for PAWS-X. However, when we take into consideration the amount of pre-training data for these languages: roughly 3.5 B French tokens in the pre-training data versus 214M for Japanese, we can partially explain this discrepancy.

However, we must note that these two factors correlate well with only a subset of the tasks and what we are measuring is the correlation which might not imply causation. Investigating different factors that together more holistically explain multilingual capabilities is an important direction that we leave for future work. Please check Appendix \textsection \ref{sec:corrs_dets} for detailed results from this section.


% Note that neither of these factors can independently explain performance trends; In fact, our results indicate some relation between the tokenizer quality and the amount of pre-training data towards the multilingual performance of these models. For example, even though the OpenAI models have similar tokenizer fertilities for both French and Japanese these models perform much better in French than they do for Japanese (72.1\% accuracy vs 67\% accuracy for \turboinf{}) for PAWS-X. However, when we take into consideration the amount of pre-training data for these languages: roughly 3.5 B French tokens in the pre-training data versus 214M for Japanese, we can partially explain this discrepancy. We also point out that these two factors correlate well with only a subset of the tasks that we have, and this measurement might not imply causation. Overall, investigating such conjunctive effects is an important part of our future work. 

% However, we must note that fertility doesn't always explain performance in its entirety. As an example, t


% However, we must note that these two factors correlate well with only a subset of the tasks that we have, and what we are measuring is the correlation which might not imply causation. Investigating different factors that together more holistically explain multilingual capabilities is an important direction that we leave for future work. Please check Appendix \textsection \kabir{add} for detailed results from this section.

% \begin{figure*}[!htb]
%     \centering
%     \includegraphics[width=16cm]{figures/fertility.pdf}
%     \vspace{-3mm}
%     \caption{Tokenizer Fertility for OpenAI models, mBERT, and BLOOM for different languages}
%     \vspace{-3mm}
%     \label{fig:tokenizer-fertility}
% \end{figure*}






% \subsection{Detailed Results}
% We now provide more in-depth coverage of language-wise results for all tasks and models.
% \subsubsection{Comparison across prompting strategies}
% The performance of different models and prompting strategies for DV003 on all the tasks is summarized in Table \ref{tab:results_summary}. We observe that excluding a few cases, fine-tuned models outperform prompt-based models by a significant margin. Out of the three prompt setups, the best performance on DV003 is always obtained using the Translate-Test setup while the Monolingual setup often performs better than the Zero-Shot Cross-Lingual strategy. Below we discuss the language-specific performance for each task in detail.

% \noindent \textbf{Classification Tasks:} Figure \ref{fig:xnliv2} shows the performance of DV003 on the XNLI task for the three prompt settings. Translate-test performs best across all languages. For Class 5 languages such as es, fr, de, the performance of translate-test is close to the zero-shot cross-lingual prompt setting, with monolingual prompting not being very far behind. For Class 4 languages, such as tr, ru, vi and hi, there is a large gap between translate-test and the other two settings, with the monolingual setting being slightly better for ru and hi. Similarly, for Class 3 and 4 languages, we see that there is a large gap between translate-test and the other two settings. Importantly, we see a large gap between English performance and the translate-test performance even for high resource languages.

% % \begin{figure}[!htb]
% %     \centering
% %     \includegraphics[width=8cm]{figures/xnli_dv003_eval_settings.pdf}
% %     \caption{XNLI: DV003}
% %     \label{fig:xnlidv003}
% % \end{figure}

% Next, we look at results from IndicXNLI presented in Figure \ref{fig:indicxnliv2}. Once again, the translate-test setting outperforms the other settings by a large margin, and the gap is larger for languages below Class 5. We also see that performance is much better for the monolingual setting compared to zero-shot cross-lingual prompting.

% \begin{figure*}
% \centering

% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/xnli_dv003_eval_settings.pdf}
%  \caption{XNLI}
% \label{fig:xnlidv003}
% \end{subfigure}%
% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/indicxnli_dv003.pdf}%IndicXNLI-DV003.png}
%  \caption{Indic-XNLI}
% \label{fig:indicxnli}
% \end{subfigure}

% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/xcopa_dv003_eval_settings.png}
%     \caption{XCOPA. We do not report numbers for the translate-test setting for qu as it is not supported by Bing translator}
%     \label{fig:xcopadv003}
% \end{subfigure}%
% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/pawsx-dv003-eval-settings.pdf}
%  \caption{PAWS-X}
% \label{fig:pawsxdv003}
% \end{subfigure}

% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/tydiqa_dv003_f1_val_settings.png}
%     \caption{TyDiQA}
%     \label{fig:tydiqaf1}
% \end{subfigure}%
% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/mlqa_dv003_f1_val_settings.pdf}
%     \caption{MLQA}
%     \label{fig:mlqaf1}
% \end{subfigure}

% % \begin{subfigure}[t]{0.45\textwidth}
% %  \includegraphics[width=0.95\textwidth]{figures/tydiqa_dv003_f1_val_settings.png}
% %     \caption{TyDiQA: DV003 - F1}
% %     \label{fig:tydiqaf1}
% % \end{subfigure}%
% % \begin{subfigure}[t]{0.45\textwidth}
% %  \includegraphics[width=0.95\textwidth]{figures/mlqa_dv003_f1_val_settings.pdf}
% %     \caption{MLQA: DV003 - F1}
% %     \label{fig:mlqaf1}
% % \end{subfigure}
% \caption{Comparison between the performance of DV003 using the three prompting strategies for different tasks and languages.}
% \label{fig:compare_prompt_strat}
% \end{figure*}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/IndicXNLI-DV003.png}
%     \caption{Indic-XNLI: DV003}
%     \label{fig:indicxnli}
% \end{figure}

% Figure \ref{fig:xcopav2} shows the performance of DV003 on the XCOPA task. The XCOPA dataset has a couple of extremely low resource languages like qu and ht that belong to Class 1 and 0 respectively. In case of XCOPA, we again find that the translate-test setting works best across most languages. This is especially stark in languages like ta, th and sw, which use their own native scripts. Both monolingual and zero-shot settings perform very poorly for languages in Class 3 and below which are not in the Latin script. Performance on qu and ht in the monolingual setting is slightly better than in the zero-shot cross-lingual setting. 

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/xcopa_dv003_eval_settings.png}
%     \caption{XCOPA: DV003. We do not report numbers for the translate-test setting for qu as it is not supported by Bing translator}
%     \label{fig:xcopadv003}
% \end{figure}

% Figure \ref{fig:pawsxv2} shows the results on PAWS-X, which has a smaller number of languages all in Class 4 and 5. We see similar trends here that translate-test performs best, except in the case of de, where monolingual prompting works best. The performance of the non-Latin script languages (ja, zh, ko) is much worse than English performance even in the translate-test setting and in the other two settings compared to de, es and fr, even though ja and zh are high resource languages. For all languages, the monolingual setting outperforms the zero-shot cross-lingual setting, which follows the same trend the we observe with XNLI and XCOPA.

% \noindent \textbf{Choice of the language of the prompt template}: For the monolingual prompting strategy, we evaluate the models with both English-Template prompts and Native-Language-Template prompts. As can be seen in Table \ref{tab:en_v_translated_prompts}, for all the classification tasks that we study, English-Template prompts perform much better than using Native-Language-Template prompts. We conjecture two possible reasons for the poor performance of native language prompts. First, since these prompts were obtained by using Machine Translation, the errors in the translation might propagate in the evaluation. Secondly, we suspect that the instruction-tuning procedure in the InstructGPT models \cite{ouyang2022training} largely relies on instructions written in English which might help explain poor performance with native-language templates (also observed for BLOOMZ in \citet{muennighoff2022crosslingual}). Following these results, we only evaluate QA tasks with English-Template prompts to reduce the computational overhead.

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{
%     figures/pawsx-DV003-eval-settings.png}
%     \caption{PAWS-X: DV003}
%     \label{fig:pawsxdv003}
% \end{figure}

% \noindent \textbf{Question Answering Tasks:} Next, we look at results for TyDiQA 
% % with two metrics, exact match shown in Figure \ref{fig:tydiqaem} and F1 
% in Figure \ref{fig:tydiqav2}. As seen before, monolingual prompting outperforms zero-shot cross-lingual prompting. We do not test for the translate-test as described earlier.

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/tydiqa_dv003_em_val_settings.png}
%     \caption{TyDiQA: DV003 - Exact Match}
%     \label{fig:tydiqaem}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/tydiqa_dv003_f1_val_settings.png}
%     \caption{TyDiQA: DV003 - F1}
%     \label{fig:tydiqaf1}
% \end{figure}

% For MLQA shown in 
% % Figure \ref{fig:mlqaem} and 
% Figure \ref{fig:mlqav2}, we find that the results are similar for low resource and non-Latin script languages. For class 5 languages in Latin script (es, de), we find that all three prompting methods work equally well. For zh, the monolingual prompting technique works best in terms of Exact Match, while for both ar and zh, translate-test work much better. For languages in Class 4 (vi and hi), translate-test works best.

% The XQUAD dataset does not have test data in the target languages, so we only evaluate the zero-shot cross lingual setup, the results of which can be found in 
% % Figure \ref{fig:xquademcomparison} and
% Figure \ref{fig:xquadv2}. We see similar trends as before, with large drops in performance for low-resource and non-Latin script languages compared to English.

% We used the same evaluation setups for the IndicQA task, however, the results were very poor due to the quality of chunks retrieved for each question. We plan to improve the algorithm used for the QA task in future versions of this study. This issue may also be alleviated in future versions of generative models that allow for larger context sizes.

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/mlqa_dv003_em_val_settings.pdf}
%     \caption{MLQA: DV003 - Exact Match}
%     \label{fig:mlqaem}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/mlqa_dv003_f1_val_settings.pdf}
%     \caption{MLQA: DV003 - F1}
%     \label{fig:mlqaf1}
% \end{figure}



% \myworries{TO ADD: IndicQA results}

% \subsubsection{Comparison across models}

% \begin{figure*}
% \centering

% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/xnli_models_comparison.pdf}
% \caption{XNLI}
% \label{fig:xnlicomparison}
% \end{subfigure}%
% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/IndicXNLI-DVmonoTT-MuRIL.png}
% \caption{IndicXNLI}
% \label{fig:indicxnlicomparison}
% \end{subfigure}

% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/xcopa_models_comparison.png}
%     \caption{XCOPA}
%     \label{fig:xcopacomparison}
% \end{subfigure}%
% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/pawsx-models-comparison.pdf}
%     \caption{PAWS-X}
%     \label{fig:pawsxcomparison}
% \end{subfigure}

% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/tydiqa_models_comparison_f1.png}
%     \caption{TyDiQA}
%     \label{fig:tydiqaf1comparison}
% \end{subfigure}%
% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/xquad_models_comparison_f1.png}
%     \caption{XQUAD}
%     \label{fig:xquadf1comparison}
% \end{subfigure}

% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/mlqa_models_comparison_f1.pdf}
%     \caption{MLQA}
%     \label{fig:mlqaf1comparison}
% \end{subfigure}

% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/tydiqa_dv003_f1_val_settings.png}
%     \caption{TyDiQA: DV003 - F1}
%     \label{fig:tydiqaf1}
% \end{subfigure}%
% \begin{subfigure}[t]{0.45\textwidth}
%  \includegraphics[width=0.95\textwidth]{figures/mlqa_dv003_f1_val_settings.pdf}
%     \caption{MLQA: DV003 - F1}
%     \label{fig:mlqaf1}
% \end{subfigure}
% \caption{Comparison between the performance of different fine-tuned and prompt-based models.}
% \label{fig:compare_models}
% \end{figure*}

% We compare the performance of the GPT models across all tasks with SOTA models. Since the translate-test and monolingual setting generally outperforms zero-shot cross-lingual prompting strategies, we only compare results for these two for the DV003 and GPT-3.5-Turbo models. Further, we only run monolingual prompting experiments with GPT4, to make the best use of computing resources. 

% %%%%%%%%SS DONE TILL HERE %%%%

% \paragraph{Classification tasks} Figure \ref{fig:xnliv2} shows the comparison of GPT models, including GPT4 on XNLI. We see that for all languages, the performance of TULRv6 is significantly better than the other models, however, the performance of GPT4 comes close to it, outperforming the Turbo model across all languages. We also observe that the differences in performance between English and other languages is much less in TULRv6, compared to DV003 and BLOOMZ. BLOOMZ performs worse than DV003 translate-test, but outperforms the next best DV003 strategy for Arabic, Vietnamese, Hindi. DV003 translate-test outperforms Turbo, but the gap is small for many languages. Surprisingly, Turbo translate-test does not outperform Turbo monolingual prompting, suggesting that monolingual prompting may be the best prompting strategy for GPT models from Turbo onwards.

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/xnli_models_comparison.pdf}
%     \caption{Comparison of DV003, BLOOMZ and TULRv6 on XNLI}
%     \label{fig:xnlicomparison}
% \end{figure}

% Figure \ref{fig:indicxnliv2} shows the comparison between the GPT models and MuRIL on IndicXNLI. Similar to the comparisons with TULR, we find that the MuRIL model outperforms the GPT models on all languages by a significant margin (we do not test GPT4 in this dataset). We also find that in this task, the performance of translate-test prompting is better than monolingual for both DV003 and Turbo. Surprisingly, the performance of DV003 is better than Turbo for translate-test.

% Figure \ref{fig:pawsxv2} shows the comparison of the models for PAWS-X. TULRv6 performs the best for all languages, followed by BLOOMZ which performs close to TULR for all languages except Japanese and Chinese. We do not observe a significant difference in scores between the translate-test and monolingual setting, or between DV003 and Turbo.

% % \begin{figure}[!h]
% % \centering
% % \includegraphics[width=8cm]{figures/IndicXNLI-DVmonoTT-MuRIL.png}
% % \caption{Comparison of DV003 (Translate Test), DV003 (Monolingual) and MuRIL on IndicXNLI}
% % \label{fig:indicxnlicomparison}
% % \end{figure}
    
% Figure \ref{fig:xcopav2} shows the comparison across models for XCOPA. Here, we find much less difference between the performance of TULRv6 and DV003 translate-test. However, we find that the BLOOMZ performance is very low for all languages except Indonesian. We also find that the translate-test prompting strategy outperforms monolingual for some low-resource languages like Tamil and Thai.

% Figure \ref{fig:xstoryclozev2} shows the comparison across GPT models with BLOOMZ for the XStoryCloze task. Once again, we see that BLOOMZ performs worse than the GPT models, with translate-test prompting for both DV003 and Turbo performing equally well. Turbo monolingual performance is worse in some languages such as Basque, Burmese and Telugu but is similar to translate-test performance in other languages, once again suggesting that for some languages there is no need to translate to English. 

% \paragraph{Question Answering Tasks}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[width=8cm]{figures/pawsx-models-comparison.png}
% %     \caption{Comparison of DV003, BLOOMZ and TULRv6 on PAWS-X}
% %     \label{fig:pawsxcomparison}
% % \end{figure}



% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[width=8cm]{figures/xcopa_models_comparison.png}
% %     \caption{Comparison of DV003, BLOOMZ and TULRv6 on XCOPA}
% %     \label{fig:xcopacomparison}
% % \end{figure}

% Figure 
% % \ref{fig:tydiqaemcomparison} and 
% \ref{fig:tydiqav2} show the comparison of the GPT models including GPT4 with TULR and BLOOMZ for TyDiQA.
% Here too, we see that TULR outperforms the GPT models, with BLOOMZ outperforming GPT4 in all languages except Finnish and Korean. Recall that we do not run the translate-test experiment for TyDiQA due to the alignment issues mentioned earlier. While GPT4 outperforms Turbo and DV003 across all languages, Turbo does not always outperform DV003.

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[width=8cm]{figures/tydiqa_models_comparison_em.png}
% %     \caption{Comparison of DV003 and TULRv6 on TyDiQA on Exact Match}
% %     \label{fig:tydiqaemcomparison}
% % \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[width=8cm]{figures/tydiqa_models_comparison_f1.png}
% %     \caption{Comparison of DV003 and TULRv6 on TyDiQA on F1}
% %     \label{fig:tydiqaf1comparison}
% % \end{figure}

% For XQUAD, shown in Figure 
% % \ref{fig:xquademcomparison} and 
% \ref{fig:xquadv2}
% , we observe similar trends. TULR performs best followed by BLOOMZ, with GPT4 performance being better than DV003 and Turbo. Turbo outperforms DV003 across all languages by a significant margin. 

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[width=8cm]{figures/xquad_models_comparison_em.png}
% %     \caption{Comparison of DV003 and TULRv6 on XQUAD on Exact Match}
% %     \label{fig:xquademcomparison}
% % \end{figure}


% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[width=8cm]{figures/xquad_models_comparison_f1.png}
% %     \caption{Comparison of DV003 and TULRv6 on XQUAD on F1}
% %     \label{fig:xquadf1comparison}
% % \end{figure}

% Comparisons on the MLQA task are shown in Figure 
% % \ref{fig:mlqaemcomparison} and Figure 
% \ref{fig:mlqav2}. Here too, TULRv6 outperforms the GPT models in all languages, with the gap being higher in low-resource and non-Latin script languages. Turbo monolingual outperforms Turbo translate-test and the DV003 model on both prompt settings, once again suggesting that there is no need to translate into English for this task with Turbo.

% Next, we look at IndicQA results in Figure \ref{fig:indicqav2} which shows the comparison between the GPT models and BLOOMZ. We find that Turbo is significantly better than DV003, which performs very poorly on this task. Turbo outperforms BLOOMZ on the higher-resource Indic languages, and underperforms on the lower-resource languages. 

% \paragraph{Sequence labeling tasks}

% Next, we look at the results of the UDPOS task in Figure \ref{fig:udposb1}, \ref{fig:udposb2} and \ref{fig:udposb3} respectively. We compare the results of Turbo monolingual with XLMR and find that XLMR outperforms Turbo across all languages in all classes. We also observe that the performance on non-Latin script languages is lower across both models, especially for Turbo. Similarly, we also compare the performance of these two models on the PANX NER task, shown in Figure \ref{fig:panx_all_results}. Again, we find that Turbo is significantly worse than XLMR, suggesting that sequence labeling tasks are challenging for the GPT models, particularly for low-resource and non-Latin script languages. 

% \paragraph{Summarization Tasks}

% The results of XLSUM are shown in Figures \ref{fig:xlsum_results}. For this task, we compare the performance of Turbo, GPT4 and mT5, where we use mT5 as. 

% mT5 outperforms the GPT models across all languages, however GPT4 outperforms Turbo only in some languages. 

% For example, our evaluation's sensitivity to the decoding length significantly affects the performance of the GPT-class models. Specifically, for the lower-resource languages, we observe an unnatural delimitation of inferences (upon spot-checking with at least five languages), which naturally leads to the degradation of the model's performance in these languages. Since decoding length is so sensitive to each language's tokenization (which varies significantly and consistently shows higher fertility for lower-resource language), we wish to highlight this as a non-trivial problem while carrying out large-scale evaluation across multiple languages.  


% Also note that the alternative of passing a large decoding length (maximum possible for the lowest-resource languages) not only induces a cost overhead in terms of latency of inference and API-consumption, but also demonstrates a scope of hallucination - something that we empirically observe in English. This may also partially explain mt5's superior performance on English (despite GPT's expected proclivity to a fundamental generation like summarization, especially in a data-rich language like English).

% \subsubsection{Code-switching tasks}

% We compare DV003 and Turbo with mBERT for both CS tasks in Table \ref{tab:cm_results} and find no significant difference in the performance of all models suggesting that the GPT models are able to perform as well as SOTA models on these tasks. 

% \subsubsection{RAI tasks}

% We compare Turbo with BLOOMZ and two commercial MT systems - Bing Translator and Google Translate in Figure \ref{fig:winomtv2} for the WinoMT gender bias task. We find that Turbo outperforms all the models across all languages except German, while BLOOMZ performs poorly on many languages. 

% As reflected in the results from Table \ref{tab:jigsaw_results_summary} and Figure \ref{fig:jigsawv2}, we observe that, on an average, DV003 outperforms Turbo across all languages in the Jigsaw Multilingual Toxicity Classification task. However, the baselines for the PALM models over the same dataset are higher as compared to the OpenAI models, with few-shot monolingual and zero-shot prompting strategies being adopted to evaluate the PALM models as compared to the few-shot crosslingual and translate-test strategies which are used for the OpenAI models.


% \begin{figure*}
%     % \centering
%     % \begin{subfigure}[t]{0.95\linewidth}
%     % \includegraphics[width=0.95\textwidth]{figures/XQuAD_lang_nums.pdf}
%     % \caption{}
%     % \end{subfigure}
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/TyDiQA-GoldP_lang_nums.pdf}
%     % \caption{}
%     % \label{fig:tydiqav2}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/MLQA_lang_nums.pdf}
%     % \caption{}
%     % \label{fig:mlqav2}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/IndicQA_lang_nums.pdf}
%     % \caption{}
%     % \label{fig:indicqav2}
%     % \end{subfigure}\\
%     \caption{Comparing performance of different models on the XQuAD dataset.}
%     \label{fig:xquadv2}
    
%     % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
% \end{figure*}



% \begin{figure*}
%     \centering
%     \begin{subfigure}[t]{0.95\linewidth}
%     \includegraphics[width=0.95\textwidth]{emnlp2023-latex/figures/PAN-X_lang_nums.pdf}
%     \caption{}
%     \end{subfigure}
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/panx_24.pdf}
%     % \caption{}
%     % \label{fig:panxb2}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/panx_36.pdf}
%     % \caption{}
%     % \label{fig:panxb3}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/panx_48.pdf}
%     % \caption{}
%     % \label{fig:panxb4}
%     % \end{subfigure}\\
%     % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
%     \caption{Comparing performance of different models on PAN-X on 15 representative languages. (Results for all languages available in Appendix).}
%     \label{fig:panx_all_results}
% \end{figure*}


% \begin{figure*}
%     \centering
%     \begin{subfigure}[t]{0.95\textwidth}
%     \includegraphics[width=0.95\textwidth]{emnlp2023-latex/figures/XLSum_lang_nums.pdf}
%     \caption{}
%     \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/xlsum_22.pdf}
%     % \caption{}
%     % \label{fig:xlsumb2}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/xlsum_33.pdf}
%     % \caption{}
%     % \label{fig:xlsumb3}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/xlsum_44.pdf}
%     % \caption{}
%     % \label{fig:xlsumb4}
%     % \end{subfigure}\\
%     % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
%     \caption{Comparing performance of different models on XLSUM on 14 representative languages (top-7 and bottom-7 performing language according to GPT-3.5-Turbo). (Results for all languages available in Appendix).}
%     \label{fig:xlsumb1}
    
%     \label{fig:xlsum_results}
% \end{figure*}


% \begin{figure*}
%     \centering
%     % \begin{subfigure}[t]{0.95\textwidth}
%     \includegraphics[width=0.95\textwidth]{figures/xlsum_11.pdf}
%     % \caption{}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/xlsum_22.pdf}
%     % \caption{}
%     % \label{fig:xlsumb2}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/xlsum_33.pdf}
%     % \caption{}
%     % \label{fig:xlsumb3}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/xlsum_44.pdf}
%     % \caption{}
%     % \label{fig:xlsumb4}
%     % \end{subfigure}\\
%     % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
%     \caption{Comparing performance of different models on XLSUM on 10 representative languages. (Results for all languages available in Appendix).}
%     \label{fig:xlsumb1}
    
%     \label{fig:xlsum_results}
% \end{figure*}

% \begin{figure*}
%     \centering
%     % \begin{subfigure}[t]{0.95\linewidth}
%     \includegraphics[width=0.95\textwidth]{figures/Jigsaw_lang_nums.pdf}
%     % \caption{}
%     % \end{subfigure}
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/WinoMT_lang_nums.pdf}
%     % \caption{}
%     % \label{fig:winomtv2}
%     % \end{subfigure}\\
%     \caption{Comparing performance of different models on the Jigsaw dataset.}
%     \label{fig:jigsawv2}
    
%     % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
% \end{figure*}


% \begin{figure*}
%     \centering
%     % \begin{subfigure}[t]{0.95\linewidth}
%     \includegraphics[width=0.95\textwidth]{figures/udpos_13.pdf}
%     % \caption{}
%     % \end{subfigure}
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/udpos_26.pdf}
%     % \caption{}
%     % \label{fig:udposb2}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/udpos_39.pdf}
%     % \caption{}
%     % \label{fig:udposb3}

%     % \end{subfigure}\\
%     % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
%     \caption{Comparing performance of different models on UDPOS on 10 representative languages. (Results for all languages available in Appendix).}
%     \label{fig:udposb1}
    
% \end{figure*}




% \begin{figure*}
%     \centering
%     % \begin{subfigure}[t]{0.95\linewidth}
%     \includegraphics[width=0.95\textwidth]{figures/panx_12.pdf}
%     % \caption{}
%     % \end{subfigure}
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/panx_24.pdf}
%     % \caption{}
%     % \label{fig:panxb2}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/panx_36.pdf}
%     % \caption{}
%     % \label{fig:panxb3}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/panx_48.pdf}
%     % \caption{}
%     % \label{fig:panxb4}
%     % \end{subfigure}\\
%     % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
%     \caption{Comparing performance of different models on PAN-X on 10 representative languages. (Results for all languages available in Appendix).}
%     \label{fig:panx_all_results}
% \end{figure*}







% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/XNLIV2.png}
%     \caption{XNLI results across all models. Metric reported is Accuracy, higher is better}
%     \label{fig:xnliv2}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/IndicXNLIV2.png}
%     \caption{IndicXNLI results across all models. Metric reported is Accuracy, higher is better}
%     \label{fig:indicxnliv2}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/XCOPAV2.png}
%     \caption{XCOPA results across all models. Metric reported is Accuracy, higher is better}
%     \label{fig:xcopav2}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/XStoryClozeV2.png}
%     \caption{XStoryCloze results across all models. Metric reported is Accuracy, higher is better}
%     \label{fig:xstoryclozev2}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/PAWSXV2.png}
%     \caption{PAWS-X results across all models. Metric reported is Accuracy, higher is better}
%     \label{fig:pawsxv2}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/XQUADV2.png}
%     \caption{XQUAD results across all models. Metric reported is F1, higher is better}
%     \label{fig:xquadv2}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/TyDiQAV2.png}
%     \caption{TyDiQA-GoldP results across all models. Metric reported is F1, higher is better}
%     \label{fig:tydiqav2}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/MLQAV2.png}
%     \caption{MLQA results across all models. Metric reported is F1, higher is better}
%     \label{fig:mlqav2}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/IndicQAV2.png}
%     \caption{IndicQA results across all models. Metric reported is F1, higher is better}
%     \label{fig:indicqav2}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/UDPOSV2cl54.png}
%     \caption{UDPOS results for language classes 5 and 4 across all models. Metric reported is F1, higher is better}
%     \label{fig:udposv2cl54}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/UDPOSV2cl321.png}
%     \caption{UDPOS results for language classes 3, 2, 1 across all models. Metric reported is F1, higher is better}
%     \label{fig:udposv2cl321}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/PANXV2cl54.png}
%     \caption{PANX results for language classes 5 and 4 across all models. Metric reported is F1, higher is better}
%     \label{fig:panxv2cl54}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/PANXV2cl321.png}
%     \caption{PANX results for language classes 3, 2, 1 across all models. Metric reported is F1, higher is better}
%     \label{fig:panxv2cl321}
% \end{figure}





% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/XLSUMV2cl543.png}
%     \caption{XLSum results across all models for language classes 5, 4, 3. Metric reported is Rouge-L, higher is better}
%     \label{fig:xlsumv2cl543}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/XLSUMV2cl210.png}
%     \caption{XLSum results across all models for language classes 2, 1, 0. Metric reported is Rouge-L, higher is better}
%     \label{fig:xlsumv2cl210}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/CSV2.png}
%     \caption{Results across all models for the two code switching tasks - NLI in Hindi-English and Sentiment Analysis in Spanish-English. Metric reported is Accuracy, higher is better}
%     \label{fig:csv2}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/JigsawV2.png}
%     \caption{Jigsaw results across all models. Metric reported is Accuracy, higher is better}
%     \label{fig:jigsawv2}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/WinoMTV2.png}
%     \caption{WinoMT results across all models. Metric reported is Accuracy, higher is better}
%     \label{fig:winomtv2}
% \end{figure}


% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/mlqa_models_comparison_em.pdf}
%     \caption{Comparison of DV003 and TULRv6 on MLQA on Exact Match}
%     \label{fig:mlqaemcomparison}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=8cm]{figures/mlqa_models_comparison_f1.pdf}
%     \caption{Comparison of DV003 and TULRv6 on MLQA on F1}
%     \label{fig:mlqaf1comparison}
% \end{figure}


% \hfill \break
% We provide a summary of all the results averaged over all languages in Table \ref{tab:agg_results_summary}. 
% We also include comparisons with Multilingual BERT and XLM-R \cite{conneau2020unsupervised} on all tasks. 
% Performance values for all models across different tasks are included in the Appendix \ref{sec:appendix_res}.

% We also analyze the script and language family-wise performance of Turbo in Figures \ref{fig:turbo_lang_fam} and \ref{fig:turbo_script} and find that it performs best on Indo European languages and languages that use the Latin script. 


