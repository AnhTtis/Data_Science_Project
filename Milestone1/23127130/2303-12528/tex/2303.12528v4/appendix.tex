\clearpage
\section{Appendix}
\label{sec:appendix}

\subsection{Tasks and Datasets}
\label{sec:datasets_all}

In our experiments, we consider 16 tasks spanning the following task types - classification, sequence to sequence labeling and generation. Below we review the experimental setups and datasets used for benchmarking for these two tasks. A list of all the datasets with the languages covered by them can be found in Table \ref{tab:datasets}.

\subsubsection{Classification}
These tasks involve classifying a single sentence or a group of sentences into a finite number of discrete labels. For each dataset, we measure the performance of different models in terms of classification accuracy. For prompt-based models in particular, since we add no constraint on the output space of the LLM we compute the exact match between the generated output and a verbalized label to determine if the example was classified correctly. We run experiments for all the prompting strategies that we discussed in the previous sections for each dataset. The details of each dataset that we use for benchmarking are given below:

 \begin{table}[h]
     \centering
\resizebox{\linewidth}{!}{%
     \begin{tabular}{lll}
     \toprule
    Dataset&Task&Languages\\
    \midrule
    XNLI&Natural Language Inference& 15\\
    Indic-XNLI&Natural Language Inference&11\\
    GLUECoS&Natural Language Inference&2\\
    %Indic-WNLI&Natural Language Inference&gu, hi, mr\\
    %GLUECoS-NLI&Natural Language Inference&hi-en\\
    %GLUECoS-En-Es-Sentiment&Sentiment Analysis&es-en\\
    PAWS-X&Paraphrase Identification&7\\
    XCOPA&Commonsense Reasoning& 10\\
    XStoryCloze&Commonsense Reasoning& 11 \\
    TyDiQA-GoldP&Question Answering& 9 \\
    MLQA&Question Answering&6\\
    XQuAD&Question Answering& 11\\
    IndicQA&Question Answering& 10\\
    UDPOS&Part of Speech Tagging& 38\\
    PANX& NER & 48\\ 
    WinoMT&Gender Bias& 8 \\
    GLUECoS&Sentiment Analysis& 2 \\
    Jigsaw&Toxicity Classification& 6\\
    XLSum&Summarization&  44\\
    
  \bottomrule
     \end{tabular}}
     \caption{Datasets and Language coverage of the datasets that MEGA presents evaluation for.}
     \label{tab:datasets}
     \vspace{-0.4cm}
 \end{table}

\noindent{\textbf{1. Natural Language Inference}}: XNLI \cite{Conneau2018xnli} is a dataset for cross-lingual Natural Language Inference, which consists of professional translations of the MNLI \cite{wang2018glue} corpus into 14 languages. We also consider IndicXNLI \cite{aggarwal2022indicxnli} that translates the XNLI dataset into 11 Indic languages by using Machine Translation, followed by validation by native speakers.


% Indic-WNLI \cite{doddapaneni2022indicxtreme} is a translation of the Winograd NLI dataset \cite{wang2018glue}, an NLI version of the Winograd Schema Challenge into three Indic languages. 

%remove gluecos-nli if not done

% GLUECoS-NLI \cite{khanuja2020new} is a code-mixed NLI dataset in Hindi-English, consisting of Bollywood (Hindi) movie conversations as premises, with manually created hypotheses. 

% \subsubsection{Sentiment Analysis}

% %remove section if not done

% The EN-ES-CS Sentiment Analysis dataset \cite{vilares2016cs}, part of the GLUECoS benchmark \cite{khanuja2020gluecos} is a code-mixed dataset consisting of English-Spanish Tweets annotated with SentiStrength \cite{thelwall2017heart} scores. 

\noindent{\textbf{2. Paraphrase Identification}}: PAWS-X \cite{yang2019paws} is a paraphrase identification dataset professionally translated from the PAWS \cite{zhang2019paws} dataset into six typologically diverse languages. 

\noindent{\textbf{3. Commonsense Reasoning}}: XCOPA \cite{ponti2020xcopa} is a commonsense reasoning dataset, which is a translation of the COPA \cite{roemmele2011choice} dataset into 11 typologically diverse languages, including very low-resource languages such as Eastern Apurímac Quechua and Haitian Creole.
 
XStoryCloze \cite{lin2022few} is created by translating the English StoryCloze \cite{mostafazadeh2017lsdsem} dataset using professional translators into 10 typologically diverse languages.

\subsubsection{Question Answering}
We focus on Span Prediction type of Question Answering (QA) tasks in our experiments, where given a context and a question the task is to predict the answer within the context. One major challenge that we come across for multilingual evaluation of QA tasks is that for many languages we often cannot fit the context and question pairs for the few-shot and text examples in the maximum context size of 4096 for the DV003 model. This is mainly attributed to the poor performance of GPT's tokenizer on many non-latin script languages which results in over-tokenizing the words in these languages.

To overcome this issue we follow two steps. First, for the few-shot examples we only provide the line within the paragraph containing the answer as the context. Second, for the test example, we index the chunks of the context using the embeddings from the \texttt{text-embedding-ada-002} model. Given the question, the closest chunk in the full context is retrieved and used in the prompt for the test example. We use a maximum chunk size of 100 in our experiments and use the implementation for retrieval provided in the \textbf{LangChain}\footnote{\url{https://github.com/hwchase17/langchain}} library. By doing this,we minimize the space taken by the context tokens in our prompt.

Note that, for newer GPT models i.e. GPT-3.5-Turbo and GPT-4 which support longer context lengths, we do not use this retrieval strategy for QA tasks and prompt the models to obtain the answers directly. For each task, we calculate the Exact Match and F1 score as defined in \citet{rajpurkar-etal-2016-squad}.  For our experiments we 
 consider the following four tasks:

\noindent \textbf{1. TyDiQA} \cite{clark2020tydi} is a QA dataset covering 11 typologically diverse languages. The task consists of two sub-tasks - passage selection and minimum answer span (Gold-P). For our experiments, we consider the Gold-P task and evaluate Monolingual and Zero-Shot Cross-Lingual prompting strategies. Since the labels do not directly transfer one-to-one across translation for QA tasks as they do for classification and require the use of alignment algorithms, we skip translate-test prompting for this task.

\noindent \textbf{2. MLQA} \cite{lewis2020mlqa} is an extractive QA dataset translated into 7 languages by professional translators. The task has two variants, the first where the question, context, and answer are all in the same language; and the second, where the question is in a different language than the context and answer. We consider the former variant of the task in our experiments. For MLQA, translate-test splits are also available, where each language's test data has been translated into English with answers aligned using the attention scores. There is no training data available for MLQA, and we use SQuAD's\citet{rajpurkar-etal-2016-squad} training data for selecting few-shot examples in English and validation data for MLQA in other languages to get their few-shot examples. This way, we are able to evaluate for all three prompting setups.

\noindent \textbf{3. XQuAD} \cite{artetxe2020cross} consists of professional translations of a subset of the SQuaD dataset \cite{rajpurkar2016squad} into 10 languages. XQuAD only has validation datasets available publicly, hence we evaluate the models on them. Like MLQA we use English SQuAD data for few-shot examples and since we cannot use validation data in other languages for few-shot, we only evaluate for zero-shot cross-lingual setup for this task.

\noindent \textbf{4. IndicQA} \cite{doddapaneni2022indicxtreme} is a manually curated cloze-style reading comprehension dataset that can be used for evaluating question-answering models in 11 Indic languages. The context paragraphs are chosen from Wikipedia articles whose topics are closely related to Indic culture, history,etc. The publicly available test set has about 2000 sentences that we carry out our evaluation on. 

\subsection{Sequences Labeling}

In the sequence labeling task, a sequence of tokens (such as words) to be labeled are provided to the system. 

\subsubsection{Part of Speech Tagging}

UDPOS \cite{zeman2020universal} is a dataset for Part of Speech Tagging taken from the Universal Dependencies 2.5 from the XTREME \cite{hu2020xtreme} benchmark. We benchmark a subset of the languages available in UDPOS.

\subsubsection{Named Entity Recognition}

PANX \cite{pan2017cross} or WikiANN is a Named Entity Recognition dataset consisting of Wikipedia sentences tagged with Person, Organization and Location.

For both tasks we use the linguistic structure prompting approach of \citet{blevins2022prompting} to define the prompts. The exact prompts used can be found in \textsection \ref{sec:appendix_propmts}. Given the nature of both tasks, which would involve token alignment across the translation, we do not evaluate the translate-test prompting strategies for these setups. Also, since both tasks involve $> 30$ languages, to make the best use of the compute resources we only evaluate GPT-3.5-Turbo in a monolingual setup for these two tasks. Finally, we evaluate the first 1000 examples for each language for these datasets given the large number of languages. We have recomputed all baselines with this specification as well.

\subsection{Generation}

\subsubsection{Summarization}

The XLSum \cite{hasan2021xl} dataset contains article-summary pairs across 44 typologically diverse languages, ranging from high to very low-resource. 

For a similar reason as the tagging datasets, we only evaluate on first 1000 examples of the test sets in different languages and recompute the baselines on the same testset using the weights of the XLSUM pretrained model, opensourced by the authors \cite{hasan-etal-2021-xl}.  

\subsubsection{Code-switching datasets}

All the datasets we consider so far are monolingual, however, a majority of the world's population speaks more than one language, leading to language contact phenomena such as code-switching \cite{dogruoz-etal-2021-survey, sitaram2019survey}. We include two code-switching datasets in MEGA to benchmark the performance of generative models.

GLUECoS-NLI \cite{khanuja2020new} is a code-mixed NLI dataset in Hindi-English, consisting of Bollywood (Hindi) movie conversations as premises, with manually created hypotheses. 

The EN-ES-CS Sentiment Analysis dataset \cite{vilares2016cs}, part of the GLUECoS benchmark \cite{khanuja2020gluecos} is a code-mixed dataset consisting of English-Spanish Tweets annotated with SentiStrength \cite{thelwall2017heart} scores. 



\subsubsection{RAI datasets}

We include two datasets that measure the Responsible AI (RAI) dimensions of fairness and toxicity - Jigsaw\footnote{https://www.kaggle.com/competitions/jigsaw-multilingual-toxic-comment-classification/data} for toxic comment classification and WinoMT for gender bias. 

The Jigsaw dataset contains online comments sourced from Wikipedia. The training data, which is in English, contains labels pertaining to the toxicity of the comment and any relevant identity mentions contained in the comment. We use the test dataset, which contains these comments for 6 languages as illustrated in Table \ref{tab:datasets} for evaluation. The test dataset contains a binary label indicating whether or not the comment is toxic. Our objective is to assess the performance of these models across multiple languages and observe the disparity in this performance that could arise due to a number of factors, a prominent one being the source data that these models are trained on. Using English prompts from PromptSource for the original monolingual Jigsaw task, we task the model with classifying a comment as toxic or non-toxic. We perform crosslingual few-shot prompting and translate-test experiments for the test sets of all 6 languages, and report the results excluding content violations in Table \ref{tab:jigsaw_results_summary}.

The WinoMT dataset \cite{stanovsky2019evaluating} is created by concatenating the WinoGender \cite{rudinger-etal-2018-gender} and WinoBias \cite{zhao-etal-2018-gender} datasets. WinoMT dataset consists of 3888 English sentences with equal distribution of Male and Female genders. It is also equally balanced between stereotypical and non-stereotypical gender role assignments. We follow the method as reported by \cite{stanovsky2019evaluating} in their paper. We perform zero-shot monolingual prompting of all sentences in the dataset to translate them in 8 target languages. Further using \textit {fast\_align} we map the English entity to its translation. Finally, we extract the target-side entity’s using off the shelf tools for each target language. The extracted translated gender can be finally compared against the gold annotations for English.


\subsection{Prompts}
\label{sec:appendix_propmts}
\subsubsection{XNLI, IndicXNLI, GLUECoS NLI}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$:
You are an NLP assistant whose purpose is to solve Natural Language Inference (NLI) problems.  NLI is the task of determining the inference relation between two (short, ordered) texts: entailment, contradiction, or neutral.  Answer as concisely as possible in the same format as the examples below:\\[5pt]
\noindent Template $f_{temp}$: \\
    \texttt{\{premise\}}\\ Question: \texttt{\{hypothesis\}} \\ True, False, or Neither?\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
Entailment : True, \\ Contradiction: False,\\ Neutral: Neither

\paragraph{Models}: DV003\\[5pt]
\noindent Template $f_{temp}$: \\
\texttt{\{premise\}} Based on previous passage is it true that \texttt{\{hypothesis\}} ? Yes, No, or Maybe?\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
Entailment : Yes, \\ Contradiction: No,\\ Neutral: Maybe

\subsubsection{PAWS-X}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to perform Paraphrase Identification. The goal of Paraphrase Identification is to  determine whether a pair of sentences have the same meaning. Answer as concisely as possible in the same format as the examples below:\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{sentence1\}} \\ Question: \texttt{\{sentence2\}} \\ True or False?

\paragraph{Models}: DV003 \\[5pt]
\noindent Template $f_{temp}$:\\
Sentence 1: \texttt{\{sentence1\}} Sentence 2: \texttt{\{sentence2\}} Question: Does Sentence 1 paraphrase Sentence 2 ? Yes or No?\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
\textit{Positive}: Yes \\ \textit{Negative}: No

\subsubsection{XCOPA}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an AI assistant whose purpose is to perform open-domain commonsense causal reasoning. You will be provided a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. Answer as concisely as possible in the same format as the examples below:\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{ premise \}} \\ \texttt{\{\% if question == ``cause" \%\}} This happened because... \\ \texttt{\{\% else \%\}} As a consequence... \texttt{\{\% endif \%\}} \\ Help me pick the more plausible option: - \texttt{\{choice1\}} - \texttt{\{choice2\}}

\paragraph{Models}: DV003 \\[5pt]
\noindent Template $f_{temp}$:\\
\texttt{\{ premise \}} \\ \texttt{\{\% if question == ``cause" \%\}} This happened because... \\ \texttt{\{\% else \%\}} As a consequence... \texttt{\{\% endif \%\}} \\ Help me pick the more plausible option: - choice1: \texttt{\{choice1\}}, choice2: \texttt{\{choice2\}}\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
choice1: \texttt{\{choice1\}} \\ choice2: \texttt{\{choice2\}}

\subsubsection{XQUAD, TyDiQA, MLQA}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to solve reading comprehension problems. You will be provided questions on a set of passages and you will need to provide the answer as it appears in the passage. The answer should be in the same language as the question and the passage.\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{context\}} \\ Q: \texttt{\{question\}}\\ Referring to the passage above,  the correct answer to the given question is: \texttt{\{answer\}}

\paragraph{Models}: DV003\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{context\}} \\ Q: \texttt{\{question\}}\\ Referring to the passage above,  the correct answer to the given question is: \texttt{\{answer\}}

\subsubsection{IndicQA}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to solve reading comprehension problems. You will be provided questions on a set of passages and you will need to provide the answer as it appears in the passage. The answer should be in the same language as the question and the passage.\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{context\}} \\ Q: \texttt{\{question\}}\\ Referring to the passage above,  the correct answer to the given question is? If you can't find the answer, please respond "unanswerable". \texttt{\{answer\}}

\paragraph{Models}: DV003\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{context\}} \\ Q: \texttt{\{question\}}\\ Referring to the passage above,  the correct answer to the given question is: \texttt{\{answer\}}

\subsubsection{XStoryCloze}

\paragraph{Models}: DV003, GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{input\_sentence\_1\} \{input\_sentence\_2\} \{input\_sentence\_3\} \{input\_sentence\_4\}}\\ What is a possible continuation for the story given the following options ?\\ Option1: \texttt{\{sentence\_quiz1\}} Option2: \texttt{\{sentence\_quiz2\}}\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
\texttt{\{sentence\_quiz1\}}: Option1, \\ \texttt{\{sentence\_quiz2\}}: Option2

\subsubsection{PANX}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$:  You are an NLP assistant whose purpose is to perform Named Entity Recognition (NER). NER involves identifying and classifying named entities in a text into predefined categories such as person names, organizations, locations, and others. You will need to use the tags defined below: O means the word doesn’t correspond to any entity. B-PER/I-PER means the word corresponds to the beginning of/is inside a person entity. B-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity. B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity. Do not try to answer the question! Just tag each token in the sentence.\\[5pt]
\noindent Template $f_{temp}$: \texttt{\{token\_1 token\_2 ... token\_n\}}\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
\texttt{\{tag\_1\} \{tag\_2\} ... \{tag\_n\}}: \\ \texttt{\{token\_1\}\_\{tag\_1\} \{token\_2\}\_\{tag\_2\}}\\  ... \texttt{\{token\_n\}\_\{tag\_n\}}

\subsubsection{UDPOS}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to perform Part of Speech (PoS) Tagging. PoS tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context. You will need to use the tags defined below:
\begin{enumerate}
\itemsep0em 
    \item ADJ: adjective
    \item ADP: adposition
    \item ADV: adverb
    \item AUX: auxiliary
    \item CCONJ: coordinating-conjunction
    \item DET: determiner
    \item INTJ: interjection
    \item NOUN: noun
    \item NUM: numeral
    \item PART: particle
    \item PRON: pronoun
    \item PROPN: proper-noun
    \item PUNCT: punctuation
    \item SCONJ: subordinating-conjunction
    \item SYM: symbol
    \item VERB: verb
    \item X: other
\end{enumerate}
\noindent Template $f_{temp}$: \texttt{\{token\_1 token\_2 ... token\_n\}}\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
\texttt{\{tag\_1\} \{tag\_2\} ... \{tag\_n\}}: \\ \texttt{\{token\_1\}\_\{tag\_1\} \{token\_2\}\_\{tag\_2\} ...} \\ \texttt{\{token\_n\}\_\{tag\_n\}}

\subsubsection{GLUECoS Sentiment Analysis}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to solve Sentiment Analysis problems. Sentiment Analysis is the task of determining whether the sentiment, opinion or emotion expressed in a textual data is:  positive, negative, or neutral. Answer as concisely as possible in the same format as the examples below:\\[5pt]
\noindent Template $f_{temp}$: Does the following sentence have a positive, negative or neutral sentiment? \texttt{\{text\}}

\paragraph{Models}: DV003\\[5pt]
\noindent Template $f_{temp}$: Does the following sentence have a positive, negative or neutral sentiment? \texttt{\{text\}}

\subsubsection{XLSum}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to summarize any given article. You should summarize all important information concisely in the same language in which you have been provided the document. Following the examples provided below:\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{document\}} \\ === \\ Write a summary of the text above :

\subsubsection{Jigsaw}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to solve the Jigsaw Toxicity Prompt problem. Toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. The objective is to identify toxic comments. Answer as concisely as possible in the same format as the examples below:\\[5pt]
\noindent Template $f_{temp}$: \texttt{\{text\}} Should this online comment be removed for its toxicity? Yes or No?

\paragraph{Models}: DV003\\[5pt]
\noindent Template $f_{temp}$: \texttt{\{text\}} Someone posted this comment online. Do you find it toxic? Yes or No?

\subsubsection{WinoMT}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Template $f_{temp}$: Translate the following English text to \texttt{\{target\_language\}}: \texttt{\{sentence\}} \\

\subsection{Handling Long Contexts}
\label{sec:retrieval}
As discussed in \textsection \ref{sec:models}, the models we study have limited context lengths and for QA tasks in particular, fitting the entire prompt containing the few-shot examples is often not feasible for low-resource languages where the tokenizers of these models are found to over-tokenize the text (nearly resulting in byte level tokens). To overcome this issue we follow two steps. First, for the few-shot examples we only provide the line within the paragraph containing the answer as the context. Second, for the test example, we index the chunks of the context using the embeddings from the \texttt{text-embedding-ada-002} model. Given the question, the closest chunk in the full context is retrieved and used in the prompt for the test example. We use a maximum chunk size of 100 in our experiments and use the implementation for retrieval provided in the \textbf{LangChain}\footnote{\url{https://github.com/hwchase17/langchain}} library. By doing this,we minimize the space taken by the context tokens in our prompt. Note that, for newer GPT models i.e. \turboinf{} and \gptfourinf{} which support longer context lengths, hence we only use this retrieval strategy for \dvthreeinf{} on QA tasks.

We attribute the significantly worse performance of \dvthreeinf{} on IndicQA to imperfect retrieval in the case of \dvthreeinf{}, while for \turboinf{} we do not rely on retrieval due to the larger context size. We provide the retrieval accuracies for \dvthreeinf{} (i.e. if the retrieved chunk contains the answer) in Appendix Table \ref{tab:ret_acc}
% \kabir{Add the table in appendix and refer}
, where we clearly see for low-resource languages like Telugu, the accuracies can be as low as $5\%$. While beyond the scope of this work, alternate retrieval strategies like using better embeddings from multilingual models for retrieval can be explored to close this gap \cite{nambi2023breaking}.

\begin{table}[t!]
% \resizebox{\textwidth}{!}{%
\centering
{\small
\begin{tabular}{@{}cc@{}}
\toprule
Language & Retrieval Acc. \\ \midrule
en       & 0.858          \\
ar       & 0.492          \\
bn       & 0.141          \\
fi       & 0.756          \\
id       & 0.680          \\
sw       & 0.760          \\
ko       & 0.453          \\
te       & 0.056          \\
ru       & 0.421          \\ \bottomrule
\end{tabular}%
}
\caption{Retrieval accuracy on TyDiQA dataset for chunk size = 100. }
\label{tab:ret_acc}
\end{table}

\subsection{Factors Explaining Multilingual Capabilities of LLMs}
\label{sec:corrs_dets}
We provide correlation plots in Figures \ref{fig:gpt3_gpt4_fertility_score} (between performance and fertility) and  \ref{fig:gpt3_gpt4_ps_score_corr} (between performance and pre-training size) for both \turboinf{} and \gptfourinf{}. The exact values of the correlations for all tasks and the two models is provided in Table \ref{tab:corrs_all}.


\begin{figure*}
\centering
\begin{subfigure}[t]{0.8\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{figures/fertility_score_corr.pdf}
    \caption{Correlation between tokenizer fertility and performance for GPT-3.5-Turbo.}
    \label{fig:fertility_score_corr_gpt3}
\end{subfigure}
\begin{subfigure}[t]{0.8\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{figures/fertility_score_corr_gpt4.pdf}
    \caption{Correlation between tokenizer fertility and performance for GPT-4}
    \label{fig:fertility_score_corr_gpt4}
\end{subfigure}
\caption{Correlation between the performance of GPT-3.5-Turbo and GPT-4 with the tokenizer fertility.}
\label{fig:gpt3_gpt4_fertility_score}
\end{figure*}

\begin{figure*}
\centering
\begin{subfigure}[t]{0.8\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{figures/ps_score_corr.pdf}
    \caption{Correlation between pre-training size and performance for GPT-3.5-Turbo.}
    \label{fig:gpt3_ps_score_corr}
\end{subfigure}
\begin{subfigure}[t]{0.8\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{figures/ps_score_corr_gpt4.pdf}
    \caption{Correlation between pre-training size and performance for GPT-4}
    \label{fig:gpt4_ps_score_corr}
\end{subfigure}
\caption{Correlation between the performance of GPT-3.5-Turbo and GPT-4 with the pre-training size.}
\label{fig:gpt3_gpt4_ps_score_corr}
\end{figure*}


%%Corr
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccccclccccc}
\toprule
 &
  \multicolumn{4}{c}{Tokenizer Fertility} &
  \multicolumn{4}{c}{Pre-training Size} & \\
 &
  \multicolumn{2}{c}{GPT-3.5-Turbo} &
  \multicolumn{2}{c}{GPT-4} & 
  \multicolumn{2}{c}{GPT-3.5-Turbo} &
  \multicolumn{2}{c}{GPT-4} & 
  \\
\midrule
  Task
 &
  $\rho$ &
  P-value &
  $\rho$ &
  P-value &
  $\rho$ &
  P-value & 
  $\rho$ &
  P-value
  \\ 
  \midrule
XNLI+IndicXNLI & $-0.784$ & $6.9e-05$ & $-0.803$ & $3.4e-05$ & 0.893 & $4.1e-09$ & 0.836 & $3.5e-07$ \\
XCOPA & $-0.982$ & $7.9e-05$ & $-0.957$ & 0.00 & 0.70 & 0.035 & 0.489 & 0.181 \\
XstoryCloze & $-0.745$ & 0.033 & $-0.918$ & 0.001 & 0.603 & 0.064 & 0.407 & 0.242 \\
PAWS-X & $-0.587$ & 0.219 & $-0.61$ & 0.198 & 0.85 & 0.031 & 0.94 & 0.005 \\
\midrule
MLQA & $-0.451$ & 0.368 & $-0.674$ & 0.141 & 0.71 & 0.085 & 0.808 & 0.051 \\
TyDiQA-GoldP & 0.543 & 0.163 & 0.049 & 0.907 & $-0.464$ & 0.207 & $-0.159$ & 0.682 \\
XQuAD & $-0.865$ & 0.00 & $-0.818$ & 0.002 & 0.782 & 0.004 & 0.736 & 0.009 \\
IndicQA & $-0.960$ & 0.002 & $-0.856$ & 0.029 & 0.628 & 0.051 & 0.690 & 0.027 \\
\midrule
WinoMT & $-0.36$ & 0.379 & - & - & 0.249 & 0.589 & - & - \\
Jigsaw & 0.306 & 0.554 & - & - & $-0.674$ & 0.141 & - & - \\
\midrule
PAN-X & $-0.456$ & 0.003 & $-0.442$ & 0.004 & 0.41 & 0.006 & 0.326 & 0.032 \\
UDPOS & $-0.216$ & 0.198 & $-0.304$ & 0.066 & 0.29 & 0.095 & 0.359 & 0.036 \\
\midrule
XLSum & $-0.821$ & $4.8e-07$ & $-0.578$ & 0.002 & 0.448 & 0.011 & 0.49 & 0.005 \\
\bottomrule
\end{tabular}%
}
\caption{{Pearson Correlation coefficient $\rho$ between performance and tokenizer fertility and performance and pre-training data size for different datasets and models. We also provide the p-values, to see which correlations are statistically significant.}}
\label{tab:corrs_all}
\end{table*}


\subsection{Challenges in Multilingual Evaluation}
\label{sec:challenges_full}
\noindent \textbf{Effect of number of in-context examples $k$.}
Our main experiments were conducted with $k = 8$ or $k = 4$, depending on the task. Here, we evaluate what effect different numbers of in-context examples have on XNLI and XCOPA for three languages in Figures \ref{fig:xnli_icl} and \ref{fig:xcopa_icl}. We observe while the performance increases sharply while moving from 0 to 2-4 examples, it is fairly stable after $k \geq 8$, with the exception of Haitian Creole in XCOPA, where it continues to improve.

\noindent \textbf{Effect of language-specific prompt tuning.}
As discussed in \textsection \ref{sec:prompt_strategies}, we use English validation data for prompt selection in each dataset that we use for all languages. Here, we explore whether separately tuning the prompts for each language helps. For XNLI, we run this experiment on Urdu and Swahili, tuning over ten different prompt templates from Prompt-Source, but find that the same prompt that was tuned for English gets picked up for these two languages as well. For XCOPA however, different prompts are chosen when tuned on Haitian Creole and Tamil. This leads to an improvement in the test performance for Haitian Creole (from 72\% to 75.6\%, see Figure \ref{fig:xcopa_ptune}). Interestingly for Tamil, we see the test performance actually drops slightly compared to the accuracy obtained with prompt selected on English data, which we conjecture might be due to the fact that the validation sets in XCOPA have only 100 examples that may not be sufficient for selecting optimal prompts.

\noindent \textbf{Effect of Explanations.} \citet{yeDurrett2022Explanations}, showed for \texttt{text-davinci-002}, that prompting the model with explanations before the outputs (Explain-then-Predict) in the in-context examples can help improve few-shot performance substantially on English language datasets. Hence, here we evaluate if they help improve the multilingual performance of the \turboinf{} model as well. We perform experiments on XStoryCloze and XCOPA datasets and use the explanations available in Super-NaturalInstructions (SNI)\cite{wang-etal-2022-super}\footnote{At the time of writing this paper, XStoryCloze wasn't included in SNI, hence we use the few-shot examples and explanations available for StoryCloze dataset\cite{mostafazadeh-etal-2016-corpus}, making the prompting setup \zscl{}.}. All the explanations that we used were written in English. For XStoryCloze, the results are plotted in Figure \ref{fig:xstory_expl}, and we observe that while there is a slight gain upon using explanations for Telugu, for all other languages the performance remains largely unchanged if not slightly worse. Interestingly, upon manual inspection of the model's prediction, we observe that the model often first translates the problem to English and then proceeds with the explanation, without having prompted to do so. We have similar observations for the XCOPA dataset as well, where adding explanations doesn't help improve performance and ends up hurting the performance by a slight margin (Figure \ref{fig:xcopa_expl})

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/analysis/xcopa_expl.pdf}
    \caption{Effect of using explanations in XCOPA dataset. Blue bars mean no explanations in the prompt and orange bars correspond to prompting with explanations.}
    \label{fig:xcopa_expl}
\end{figure}

\noindent \textbf{Effect of the language of the prompt templates.} While all our experiments were run using prompt templates written in English, we initially evaluated \dvthreeinf{} on Native-Language-Templates as well, which were obtained by translating English templates using Bing-Translator. As can be seen in Table \ref{tab:en_v_translated_prompts}, the performance is much worse when using templates in the native language compared to English. This is consistent with the results in \citet{muennighoff2022crosslingual} for BLOOMZ and \citet{lin-etal-2022-shot} for XGLM, which also show better performance when using prompt templates in English. 
\begin{table}
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{l c c}
    \toprule
     Task & English-Template & Native-Language-Template  \\
     \midrule
     XNLI & \textbf{58.3} & 54.4 \\
     Indic-XNLI & \textbf{49.6} & 38.7 \\
     PAWS-X & \textbf{67.1} & 64.2 \\
     XCOPA & \textbf{77.6} & 73.1 \\
     \bottomrule
    \end{tabular}}
    \vspace{-3mm}
    \caption{Average performance on non-English languages with the Monolingual Prompting strategy using English-Template and Native-Language-Template prompts for the classification tasks for DV003.}
    \vspace{-3mm}
    \label{tab:en_v_translated_prompts}
\end{table}



\subsection{Detailed Results}
\label{sec:appendix_res}

The results for across all tasks, languages and models included in our benchmarking exercise can are provided in Figures \ref{fig:lang_nums_cls} (for Classification tasks), \ref{fig:lang_nums_qa} (for QA Tasks), \ref{fig:lang_nums_xlsum} (for XLSum), \ref{fig:lang_nums_panx} (for PAN-X), \ref{fig:lang_nums_udpos} (for UDPOS),   \ref{fig:lang_nums_jigsawv2} (for Jigsaw), and finally \ref{fig:lang_nums_winomtv2} (for Wino-MT). The results for the Indic Datasets and the two code-mixed datasets GLUECoS NLI and En-ES-CS are provided in Tables \ref{tab:indic_results} \ref{tab:cm_results} respectively.

\begin{table}[]
\small
\resizebox{\linewidth}{!}{%
\begin{tabular}{l c c}
\toprule
Model & IndicXNLI & IndicQA \\
% \cmidrule(lr){2-4} \cmidrule(lr){5-7}
% Accuracy & F1-Score \\
\midrule
MuRIL & \textbf{72.4} & 47.7 \\ 
\texttt{text-davinci-003} & 49.6 & 8.45\\

\texttt{text-davinci-003} (TT) & 62.4 & \texttimes \\

\texttt{gpt-3.5-turbo} & 50.7  & 38.6 \\
\texttt{gpt-3.5-turbo} (TT) & 59.7 & \texttimes \\
\texttt{gpt-4-32k} & 66.8 & \textbf{55.0} \\
\midrule
\end{tabular}}
% \vspace{-3mm}
\caption{Comparing performance of \texttt{text-davinci-003}, \texttt{gpt-3.5-turbo}, and \texttt{gpt-4-32k} with fine-tuned baseline MuRIL on Indic datasets \cite{doddapaneni2022indicxtreme}. For IndicXNLI we report Accuracy and F1-score for IndicQA.}
\label{tab:indic_results}
\end{table}


\begin{table}[]
\resizebox{\linewidth}{!}{%
\begin{tabular}{l c c}
\toprule
Model & NLI En-Hi & Sentiment En-Es \\
\midrule
mBERT & 63.1 & 69.31 \\
\texttt{text-davinci-003} & 72.1 & \textbf{68.8} \\
\texttt{gpt-3.5-turbo} & \textbf{78.8} & 68.0 \\
\bottomrule
\end{tabular}}
\caption{Performance of GPT-3.5 models on code-mixing datasets from \cite{khanuja2020gluecos}. For both the tasks, the metric reported is accuracy.}
\label{tab:cm_results}
\end{table}

\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.95\linewidth}
    \includegraphics[width=0.95\textwidth]{figures/XNLI_lang_nums.pdf}
    \caption{}
    \label{fig:xnliv2}
    \end{subfigure}
    % \begin{subfigure}[t]{0.95\textwidth}
    % \includegraphics[width=0.95\textwidth]{figures/IndicXNLI_lang_nums.pdf}
    % \caption{}
    % \label{fig:indicxnliv2}
    % \end{subfigure}\\
    % \begin{subfigure}[t]{0.95\textwidth}
    % \includegraphics[width=0.95\textwidth]{figures/PAWS-X_lang_nums.pdf}
    % \caption{}
    % \label{fig:pawsxv2}
    % \end{subfigure}\\
    % \begin{subfigure}[t]{0.95\textwidth}
    % \includegraphics[width=0.95\textwidth]{figures/XCOPA_lang_nums.pdf}
    % \caption{}
    % \label{fig:xcopav2}
    % \end{subfigure}\\
    \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/XstoryCloze_lang_nums.pdf}
    \caption{}
    \label{fig:xstoryclozev2}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.95\linewidth}
    \includegraphics[width=0.95\textwidth]{figures/IndicXNLI_lang_nums.pdf}
    \caption{}
    \label{fig:indicxnli}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.95\linewidth}
    \includegraphics[width=0.95\textwidth]{figures/XCOPA_lang_nums.pdf}
    \caption{}
     \label{fig:xcopa}
    \end{subfigure}\\
    

    % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
    % \caption{Comparing performance of different models on XNLI dataset.}
    \caption{Comparing the language-wise performance of different models on the classification tasks.}
    \label{fig:lang_nums_cls}
    % \caption{Comparing language-wise performance of different models on the 5 representative tasks from our benchmarking exercise. For PAN-X and XLSum, we report performance on a subset of languages (top 5 and bottom 5 performing on average).}
\end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.95\linewidth}
    \includegraphics[width=0.95\textwidth]{figures/XQuAD_lang_nums.pdf}
    \caption{}
    \label{fig:xquad}
    \end{subfigure}
    \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/MLQA_lang_nums.pdf}
    \caption{}
    \label{fig:mlqa}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.95\linewidth}
    \includegraphics[width=0.95\textwidth]{figures/TyDiQA-GoldP_lang_nums.pdf}
    \caption{}
    \label{fig:tydiqa}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.95\linewidth}
    \includegraphics[width=0.95\textwidth]{figures/IndicQA_lang_nums.pdf}
    \caption{}
     \label{fig:indicqa}
    \end{subfigure}\\
    

    % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
    % \caption{Comparing performance of different models on XNLI dataset.}
    \caption{Comparing the language-wise performance of different models on the QA tasks.}
    \label{fig:lang_nums_qa}

    % \caption{Comparing language-wise performance of different models on the 5 representative tasks from our benchmarking exercise. For PAN-X and XLSum, we report performance on a subset of languages (top 5 and bottom 5 performing on average).}
\end{figure*}


% \begin{figure*}
% \begin{subfigure}[t]{0.8\textwidth}
% \includegraphics[width=0.95\textwidth]{figures/dv003-lang-family.pdf}
%     \caption{Language-Family wise performance of \texttt{text-davinci-003} on various tasks.}
%     \label{fig:dv003_lang_fam}
% \end{subfigure}
% \begin{subfigure}[t]{0.8\textwidth}
% \includegraphics[width=0.95\textwidth]{figures/turbo-script.pdf}
%     \caption{Script wise performance of \texttt{text-davinci-003} on various tasks.}
%     \label{fig:dv003_script}
% \end{subfigure}
% \caption{Language family and script wise performance of DV003 across tasks}
% \label{fig:dv003_famscript}
% \end{figure*}

% \begin{figure*}
% \begin{subfigure}[t]{0.8\textwidth}
% \includegraphics[width=0.95\textwidth]{figures/gpt4-lang-family.pdf}
%     \caption{Language-Family wise performance of \texttt{gpt-4-32k} on various tasks.}
%     \label{fig:gpt4_lang_fam}
% \end{subfigure}
% \begin{subfigure}[t]{0.8\textwidth}
% \includegraphics[width=0.95\textwidth]{figures/gpt4-script.pdf}
%     \caption{Script wise performance of \texttt{gpt-4-32k} on various tasks.}
%     \label{fig:gpt4_script}
% \end{subfigure}
% \caption{Language family and script-wise performance of GPT-4 across tasks}
% \label{fig:gpt4_famscript}
% \end{figure*}



\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/XLSum_lang_nums_0.pdf}
    \caption{}
    \label{fig:xlsumb1_1}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/XLSum_lang_nums_12.pdf}
    \caption{}
    \label{fig:xlsumb2}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/XLSum_lang_nums_24.pdf}
    \caption{}
    \label{fig:xlsumb3}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/XLSum_lang_nums_36.pdf}
    \caption{}
    \label{fig:xlsumb4}
    \end{subfigure}\\
    % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
    \caption{Comparing performance of different models on XLSUM.}
    \label{fig:xlsum_results}
     \label{fig:lang_nums_xlsum}
\end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.95\linewidth}
    \includegraphics[width=0.95\textwidth]{figures/UDPOS_lang_nums_0.pdf}
    \caption{}
    \label{fig:udposb1_1}
    \end{subfigure}
    \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/UDPOS_lang_nums_12.pdf}
    \caption{}
    \label{fig:udposb2}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/UDPOS_lang_nums_24.pdf}
    \caption{}
    \label{fig:udposb3}

    \end{subfigure}\\
    \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/UDPOS_lang_nums_36.pdf}
    \caption{}
    \label{fig:udposb4}

    \end{subfigure}\\
    % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
    \caption{Comparing performance of different models on UDPOS}
     \label{fig:lang_nums_udpos}
\end{figure*}




\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.95\linewidth}
    \includegraphics[width=0.95\textwidth]{figures/PAN-X_lang_nums_0.pdf}
    \caption{}
    \label{fig:panxb1_1}
    \end{subfigure}
    \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/PAN-X_lang_nums_12.pdf}
    \caption{}
    \label{fig:panxb2}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/PAN-X_lang_nums_24.pdf}
    \caption{}
    \label{fig:panxb3}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/PAN-X_lang_nums_36.pdf}
    \caption{}
    \label{fig:panxb4}
    \end{subfigure}\\
    % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
    \caption{Comparing performance of different models on PAN-X}
    \label{fig:lang_nums_panx}
\end{figure*}

\begin{figure*}
    \centering
    % \begin{subfigure}[t]{0.95\linewidth}
    \includegraphics[width=0.95\textwidth]{figures/Jigsaw_lang_nums.pdf}
    % \caption{}
    % \end{subfigure}
    % \begin{subfigure}[t]{0.95\textwidth}
    % \includegraphics[width=0.95\textwidth]{figures/WinoMT_lang_nums.pdf}
    % \caption{}
    % \label{fig:winomtv2}
    % \end{subfigure}\\
    \caption{Comparing performance of different models on the Jigsaw dataset.}
    \label{fig:lang_nums_jigsawv2}
    
    % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
\end{figure*}

\begin{figure*}
    \centering
    % \begin{subfigure}[t]{0.95\linewidth}
    % \includegraphics[width=0.95\textwidth]{figures/Jigsaw_lang_nums.pdf}
    % \caption{}
    % \label{fig:jigsawv2}
    % \end{subfigure}
    % \begin{subfigure}[t]{0.95\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/WinoMT_lang_nums.pdf}
    % \caption{}
    % \end{subfigure}\\
    \caption{Comparing performance of different models on the WinoMT dataset.}
    % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
    \label{fig:lang_nums_winomtv2}
    
\end{figure*}




%Tables comparing performance of models per datasets
% XNLI
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c c c c}
\toprule
Model & en & ar & bg & de & el & es & fr & hi & ru & sw & th & tr & ur & vi & zh & \textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 80.8 & 64.3 & 68.0 & 70.0 & 65.3 & 73.5 & 73.4 & 58.9 & 67.8 & 49.7 & 54.1 & 60.9 & 57.2 & 69.3 & 67.8 & 65.4 \\
mT5-Base & 84.7 & 73.3 & 78.6 & 77.4 & 77.1 & 80.3 & 79.1 & 70.8 & 77.1 & 69.4 & 73.2 & 72.8 & 68.3 & 74.2 & 74.1 & 75.4\\
XLM-R Large & 88.7 & 77.2 & 83.0 & 82.5 & 80.8 & 83.7 & 82.2 & 75.6 & 79.1 & 71.2 & 77.4 & 78.0 & 71.7 & 79.3 & 78.2 & 79.2 \\
TuLRv6 - XXL & \textbf{93.3} & \textbf{89.0} & \textbf{90.6} & \textbf{90.0} & \textbf{90.2} & \textbf{91.1} & \textbf{90.7} & \textbf{86.2} & \textbf{89.2} & \textbf{85.5} & \textbf{87.5} & \textbf{88.4} & \textbf{82.7} & \textbf{89.0} & \textbf{88.4} & \textbf{88.8} \\
\midrule
\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & 67.5 & 60.7 & 46.5 & 54.0 & 47.4 & 61.2 & 61.4 & 56.8 & 53.3 & 50.4 & 43.8 & 42.7 & 50.0 & 61.0 & 56.7 & 54.2\\
XGLM & 52.6 & 46.4 & 48.9 & 45.6 & 48.7 & 45.8 & 49.4 & 46.8 & 48.6 & 44.5 & 46.6 & 45.4 & 43.4 & 48.5 & 48.8 & 47.3\\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 76.2 & 59.0 & 63.5 & 67.3 & 65.1 & 70.3 & 67.7 & 55.5 & 62.5 & 56.3 & 54.0 & 62.6 & 49.1 & 60.9 & 62.1 & 62.1\\
\texttt{gpt-3.5-turbo} (TT) &  76.2 & 62.7 & 67.3 & 69.4 & 67.2 & 69.6 & 69.0 & 59.9 & 63.7 & 55.8 & 59.6 & 63.8 & 54.0 & 63.9 & 62.6 & 64.3\\
\texttt{text-davinci-003} & 79.5 & 52.2 & 61.8 & 65.8 & 59.7 & 71.0 & 65.7 & 47.6 & 62.2 & 50.2 & 51.1 & 57.9 & 50.0 & 56.4 & 58.0 & 59.3 \\
\texttt{text-davinci-003} (TT) & 79.5 & 65.1 & 70.8 & 71.7 & 69.3 & 72.2 & 71.8 & 63.3 & 67.3 & 57.3 & 62.0 & 67.6 & 55.1 & 66.9 & 65.8 & 67.1\\
\texttt{gpt-4-32k} & 84.9 & 73.1 & 77.3 & 78.8 & 79.0 & 78.8 & 79.5 & 72.0 & 74.3 & 70.9 & 68.8 & 76.3 & 68.1 & 74.3 & 74.6 & 75.4 \\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in XNLI. Metric: Accuracy.}
\label{tab:results_summary}
\end{table*}

% IndicXNLI
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c }
\toprule
Model & as & bn & gu & hi & kn & ml & mr & or & pa & ta & te & \textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
MuRIL & \textbf{76.0} & \textbf{75.0} & \textbf{77.0} & \textbf{77.0} & \textbf{77.0} & \textbf{79.0} & \textbf{74.0} & \textbf{76.0} & \textbf{77.0} & \textbf{77.0} & \textbf{74.0} & \textbf{76.0}\\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 49.5 & 53.6 & 50.6 & 55.5 & 53.9 & 48.4 & 49.9 & 47.4 & 53.6 & 48.2 & 47.4 & 50.7 \\
\texttt{gpt-3.5-turbo} (TT) & 54.3 & 61.6 & 61.8 & 59.6 & 60.8 & 59.9 & 58.7 & 58.5 & 62.3 & 58.3 & 60.8 & 59.7 \\
\texttt{text-davinci-003} & 48.6 & 52.6 & 51.2 & 56.9 & 49.1 & 48.2 & 49.4 & 46.4 & 50.4 & 45.5 & 47.2 & 49.6 \\
\texttt{text-davinci-003} (TT) & 56.0 & 66.0 & 64.7 & 62.6 & 63.9 & 61.8 & 60.9 & 60.8 & 64.7 & 61.8 & 63.1 & 62.4 \\
\texttt{gpt-4-32k} & 63.5 & 72.2 & 66.9 & 71.7 & 69.0 & 64.3 & 66.2 & 61.1 & 71.1 & 63.7 & 64.8 & 66.8
\\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in IndicXNLI. Metric: Accuracy.}
\label{tab:results_summary}
\end{table*}



% PAWS-X
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c}
\toprule
Model & en & de & es & fr & ja & ko & zh & \textbf{avg} \\ \midrule
\multicolumn{9}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 94.0 & 85.7 & 87.4 & 87.0 & 73.0 & 69.6 & 77.0 & 81.9 \\
mT5-Base & 95.4 & 89.4 & 89.6 & 91.2 & 79.8 & 78.5 & 81.1 & 86.4 \\
XLM-R Large & 94.7 & 89.7 & 90.1 & 90.4 & 78.7 & 79.0 & 82.3 & 86.4 \\
TuLRv6 - XXL & \textbf{97.2} & \textbf{95.1} & \textbf{94.8} & \textbf{95.6} & \textbf{89.4} & \textbf{90.4} & \textbf{90.4} & \textbf{93.2} \\
\midrule
\multicolumn{9}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & 89.8 & 84.3 & 88.9 & 87.5 & 74.4 & 85.8 & 65.2 & 82.3\\
\midrule
\multicolumn{9}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 72.4 & 70.6 & 72.0 & 72.1 & 67.2 & 66.5 & 69.2 & 70.0 \\
\texttt{gpt-3.5-turbo} (TT) & 72.4 & 70.8 & 69.7 & 70.1 & 61.9 & 62.5 & 63.1 & 67.2 \\
\texttt{text-davinci-003} & 72.5 & 70.6 & 72.7 & 70.7 & 60.6 & 61.8 & 60.8 & 67.1 \\
\texttt{text-davinci-003} (TT) & 72.5 & 69.8 & 70.1 & 71.3 & 65.4 & 65.8 & 65.2 & 68.6 \\
\texttt{gpt-4-32k} & 76.2 & 74.0 & 74.1 & 72.6 & 71.5 & 69.9 & 72.6 & 73.0\\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in PAWS-X. Metric: Accuracy.}
\label{tab:results_summary}
\end{table*}

% XCOPA
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c}
\toprule
Model & en & et & ht & id & it & qu & sw & ta & th & tr &\textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
%mBERT & \\
mT5-Base & - & 50.3 & 49.9 & 49.2 & 49.6 & 50.5 & 50.4 & 49.2 & 50.7 & 49.5 & 49.9
\\
%XLM-R Large & \\
TuLRv6 - XXL & - & 77.4 & 78.0 & 92.6 & 96.0 & 61.0 & 69.4 & 85.4 & 87.2 & 92.8 & 74.0 \\
\midrule
\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & 88.0 & 48.0 & 55.0 & 86.0 & 74.0 & 50.0 & 60.0 & 67.0 & 50.0 & 54.0 & 63.2 \\
XGLM & - & 65.9 & 58.9 & 68.9 & 69.2 & 47.1 & 62.9 & 56.3 & 62.0 & 58.5 & 61.1\\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 97.8 & 90.6 & 72.0 & 90.4 & 95.2 & 54.6 & 82.0 & 59.0 & 77.6 & 91.0 & 81.0 \\
\texttt{gpt-3.5-turbo} (TT) & 97.8 & 88.2 & 79.4 & 90.8 & 94.4 & 50.0 & 77.6 & 87.0 & 82.2 & 87.8 & 83.5 \\
\texttt{text-davinci-003} & 98.2 & 87.8 & 75.0 & 91.4 & 96.0 & 54.8 & 63.6 & 53.8 & 66.6 & 87.8 & 77.5 \\
\texttt{text-davinci-003} (TT) & 98.2 & 89.6 & 82.8 & 93.0 & 94.6 & 50.0 & 82.8 & 87.0 & 84.8 & 89.8 & 85.3 \\
\texttt{gpt-4-32k} & \textbf{99.6} & \textbf{98.8} & \textbf{93.2} & \textbf{97.6} & \textbf{99.8} & 58.6 & \textbf{94.4} & 79.6 & \textbf{87.8} & \textbf{97.4} & \textbf{90.7} \\
\texttt{gpt-4-32k} (TT) & \textbf{99.6} & 94.4 & 85.8 & 96.0 & 98.2 & \textbf{85.8} & 83.4 & \textbf{91.4} & \textbf{87.8} & 92.2 & 90.6
\\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in XCOPA. Metric: Accuracy.}
\label{tab:results_summary}
\end{table*}


% XQuAD
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c}
\toprule
Model & en & ar & de & el & es & hi & ru & th & tr & vi & zh & \textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 83.5 / 72.2 & 61.5 / 45.1 & 70.6 / 54.0 & 62.6 / 44.9 & 75.5 / 56.9 & 59.2 / 46.0 & 71.3 / 53.3 & 42.7 / 33.5 & 55.4 / 40.1 & 69.5 / 49.6 & 58.0 / 48.3 & 64.5 / 49.4 \\
mT5-Base & 84.6 / 71.7 & 63.8 / 44.3 & 73.8 / 54.5 & 59.6 / 35.6 & 74.8 / 56.1 & 60.3 / 43.4 & 57.8 / 34.7 & 57.6 / 45.7 & 67.9 / 48.2 & 70.7 / 50.3 & 66.1 / 54.1 &  67.0 / 49.0\\
XLM-R Large & 86.5 / 75.7 & 68.6 / 49.0 & 80.4 / 63.4 & 79.8 / 61.7 & 82.0 / 63.9 & 76.7 / 59.7 & 80.1 / 64.3 & 74.2 / 62.8 & 75.9 / 59.3 & 79.1 / 59.0 & 59.3 / 50.0 & 76.6 / 60.8 \\
TuLRv6 - XXL & 90.1 / 80.6 & \textbf{85.4 / 69.6} & \textbf{86.1 / 70.4} & \textbf{86.3 / 70.4} & \textbf{87.6 / 71.0} & \textbf{85.9 / 70.5} & \textbf{86.8 / 73.2} & \textbf{87.0 / 81.1} & \textbf{84.3 / 71.0} & \textbf{87.6 / 71.3} & 79.2 / 73.2 & \textbf{86.0 / 72.9} \\
\midrule
\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & \textbf{92.1 / 83.8} & 82.8 / 69.7 & 76.3 / 60.4 & 49.7 / 37.6 & 86.8 / 71.4 & 83.4 / 72.9 & 65.7 / 47.2 & 20.5 / 15.5 & 51.4 / 37.2 & 86.9 / 72.7 & \textbf{82.4 / 78.6} & 70.7 / 58.8 \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 79.3 / 58.7 & 59.6 / 35.1 & 70.6 / 46.6 & 49.0 / 22.8 & 70.3 / 40.8 & 54.0 / 29.0 & 58.0 / 31.3 & 41.9 / 30.4 & 61.8 / 35.0 & 69.1 / 42.4 & 50.4 / 48.3 & 60.4 / 38.2 \\
%\texttt{gpt-3.5-turbo} (TT) & \\
\texttt{text-davinci-003} & 77.2 / 61.8 & 36.8 / 22.5 & 55.2 / 39.7 & 31.8 / 19.7 & 61.8 / 41.3 & 19.9 / 10.0 & 29.4 / 17.6 & 11.5 / 8.7 & 44.8 / 29.2 & 41.7 / 25.4 & 35.6 / 32.8 & 40.5 / 28.1 \\
%\texttt{text-davinci-003} (TT) & \\
\texttt{gpt-4-32k} & 83.2 / 65.6 & 67.8 / 42.4 & 71.9 / 48.7 & 62.3 / 36.6 & 77.5 / 50.7 & 63.9 / 36.7 & 63.8 / 35.8 & 54.6 / 42.0 & 70.8 / 46.6 & 75.8 / 49.7 & 60.0 / 57.5 & 68.3 / 46.6 \\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in XQuAD. Metric: F1 Score / Exact Match.}
\label{tab:results_summary}
\end{table*}

% TyDiQA-GoldP
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c}
\toprule
Model & en & ar & bn & fi & id & ko & ru & sw & te & \textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 75.3 / 63.6 & 62.2 / 42.8 & 49.3 / 32.7 & 59.7 / 45.3 & 64.8 / 45.8 & 58.8 / 50.0 & 60.0 / 38.8 & 57.5 / 37.9 & 49.6 / 38.4 & 59.7 / 43.9 \\
mT5-Base & 71.8 / 60.9 & 67.1 / 50.4 & 40.7 / 22.1 & 67.0 / 52.2 & 71.3 / 54.5 & 49.5 / 37.7 & 54.9 / 32.6 & 60.4 / 43.9 & 40.6 / 31.1 & 58.1 / 42.8 \\
XLM-R Large & 71.5 / 56.8 & 67.6 / 40.4 & 64.0 / 47.8 & 70.5 / 53.2 & 77.4 / 61.9 & 31.9 / 10.9 & 67.0 / 42.1 & 66.1 / 48.1 & 70.1 / 43.6 & 65.1 / 45.0 \\
TuLRv6 - XXL & \textbf{85.4 / 76.4} & \textbf{84.1 / 70.4} & 86.9 / 79.6 & \textbf{83.8 / 72.8} & \textbf{88.8 / 77.9} & \textbf{78.5 / 67.8} & \textbf{81.9 / 68.6} & \textbf{87.2 / 79.6} & 85.2 / 71.6 & \textbf{84.6 / 73.8} \\
\midrule
\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & 82.4 / 70.9 & 81.9 / 62.2 & \textbf{87.8 / 82.3} & 43.6 / 28.6 & 85.0 / 71.0 & 52.3 / 43.1 & 67.4 / 51.5 & 86.0 / 77.2 & \textbf{90.3 / 81.6} & 75.2 / 63.2 \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 54.8 / 30.7 & 50.9 / 24.2 & 60.7 / 32.7 & 66.6 / 49.0 & 67.2 / 43.4 & 59.7 / 45.3 & 45.8 / 20.0 & 64.3 / 47.7 & 70.9 / 53.1 & 60.1 / 38.4 \\
\texttt{text-davinci-003} & 73.7 / 59.1 & 56.2 / 38.7 & 16.1 / 10.6 & 70.3 / 58.8 & 68.6 / 51.2 & 40.6 / 32.2 & 42.3 / 28.9 & 74.1 / 62.3 & 5.8 / 3.0 & 49.8 / 38.3 \\
\texttt{gpt-4-32k} & 72.9 / 51.4 & 60.8 / 32.7 & 68.0 / 42.5 & 75.4 / 57.7 & 80.8 / 61.1 & 69.7 / 58.5 & 61.4 / 30.5 & 81.8 / 68.7 & 72.5 / 54.9 & 71.5 / 50.9 \\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in TyDiQA. Metric: F1 Score / Exact Match.}
\label{tab:results_summary}
\end{table*}

% MLQA
\begin{table*}[h]
\resizebox{\textwidth}{!}{ %
\begin{tabular}{l c c c c c c c c }
\toprule
Model & en & ar & de & es & hi & vi & zh & \textbf{avg} \\ \midrule
\multicolumn{9}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 80.2 / 67.0 & 52.3 / 34.6 & 59.0 / 43.8 & 67.4 / 49.2 & 50.2 / 35.3 & 61.2 / 40.7 & 59.6 / 38.6 & 61.4 / 44.2 \\
mT5-Base & 81.7 / 66.9 & 57.1 / 36.9 & 62.1 / 43.2 & 67.1 / 47.2 & 55.4 / 37.9 & 65.9 / 44.1 & 61.6 / 38.6 & 64.4 / 45.0 \\
XLM-R Large & 83.5 / 70.6 & 66.6 / 47.1 & 70.1 / 54.9 & 74.1 / 56.6 & 70.6 / 53.1 & 74.0 / 52.9 & 62.1 / 37.0 & 71.6 / 53.2 \\
TuLRv6 - XXL & \textbf{86.6 / 74.4} & \textbf{76.2 / 56.5} & \textbf{80.2 / 67.0} & \textbf{81.7 / 65.1} & \textbf{82.2 / 64.8} & \textbf{82.3 / 63.2} & \textbf{78.1 / 56.5} & \textbf{81.0 / 63.9} \\
%\midrule
%\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
%\midrule
%BLOOMZ & \\
\midrule
\multicolumn{9}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 72.8 / 53.2 & 48.5 / 23.9 & 51.0 / 29.6 & 53.8 / 29.4 & 50.7 / 28.9 & 58.9 / 35.1 & 56.7 / 29.4 & 56.1 / 32.8 \\
\texttt{gpt-3.5-turbo} (TT) & 72.8 / 53.2 & 37.8 / 18.4 & 44.3 / 26.2 & 54.1 / 31.8 & 37.3 / 20.0 & 41.6 / 22.5 & 36.5 / 17.2 & 46.4 / 27.0 \\
\texttt{text-davinci-003} & 74.8 / 59.0 & 38.4 / 21.7 & 57.7 / 38.1 & 62.9 / 37.8 & 24.9 / 14.1 & 47.7 / 29.7 & 32.3 / 31.7 & 48.4 / 33.1 \\
\texttt{text-davinci-003} (TT) & 74.8 / 59.0 & 48.2 / 25.6 & 53.5 / 33.9 & 62.9 / 40.9 & 49.2 / 28.7 & 51.0 / 30.4 & 45.2 / 24.1 & 55.0 / 34.7
\\
\texttt{gpt-4-32k} & 80.3 / 62.8 & 59.1 / 33.5 & 64.7 / 44.4 & 70.0 / 45.9 & 57.3 / 35.6 & 72.2 / 49.0 & 67.1 / 38.4 & 67.2 / 44.2
\\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in MLQA. Metric: F1 Score / Exact Match.}
\label{tab:results_summary}
\end{table*}

% IndicQA
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c }
\toprule
Model & as & bn & gu & hi & kn & ml & mr & or & pa & ta & te & \textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
BLOOMZ & 40.6 / 31.7 & 42.9 / 36.6 & 37.2 / 29.9 & 44.0 / 45.1 & 37.8 / 26.6 & 30.5 / 28.4 & 39.2 / 33.0 & 25.4 / 22.0 & 26.4 / 33.5 & 39.7 / 35.9 & 38.9 / 34.7 & 36.6 / 32.5 \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 35.3 / 21.4 & 49.5 / 30.2 & 40.5 / 25.5 & 55.9 / 39.3 & 35.3 / 20.4 & 30.0 / 19.2 & 50.0 / 32.0 & 22.1 / 12.7 & 35.8 / 15.1 & 32.7 / 21.6 & 32.9 / 19.7 & 38.2 / 23.4 \\
\texttt{text-davinci-003} & 6.7 / 3.2 & 10.3 / 5.8 & 5.4 / 3.5 & 16.8 / 11.8 & 7.1 / 3.9 & 3.6 / 2.3 & 14.6 / 8.5 & 6.9 / 3.4 & 10.7 / 4.1 & 4.2 / 2.5 & 6.8 / 3.6 & 8.4 / 4.8 \\
\texttt{gpt-4-32k} & \textbf{58.8 / 40.4} & \textbf{67.1 / 47.4} & \textbf{59.4 / 42.4} & \textbf{75.2 / 62.2} & \textbf{47.1 / 31.6} & \textbf{48.3 / 33.7} & \textbf{60.7 / 43.1} & \textbf{29.9 / 16.7} & \textbf{56.1 / 34.1} & \textbf{54.0 / 39.7} & \textbf{47.9 / 27.8} & \textbf{55.0 / 38.1}
\\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in IndicQA. Metric: F1 Score / Exact Match.}
\label{tab:results_summary}
\end{table*}

% UDPOS
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c c c c c c c c}
\toprule
Model & en & af & ar & bg & de & el & es & et & eu & fa & fi & fr & he & hi & hu & id & it & ja  & kk \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 96.4 & 86.7 & 50.0 & 84.7 & 88.7 & 80.9 & 86.6 & 79.9 & 62.1 & 65.5 & 73.3 & 81.2 & 55.5 & 66.0 & 78.6 & 74.2 & 87.8 & 47.2 & 70.4 \\
XLM-R Large  & \textbf{97.0} & \textbf{89.2} & \textbf{63.0} & \textbf{88.3} & \textbf{91.2} & \textbf{86.5} & \textbf{89.2} & \textbf{87.3} & 74.9 & \textbf{70.8} & \textbf{82.7} & \textbf{86.7} & \textbf{67.5} & \textbf{75.2} & \textbf{83.4} & \textbf{75.7} & \textbf{89.2} & 29.3 & \textbf{78.3} \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo}  & 78.5 & 74.3 & 38.3 & 79.1 & 80.7 & 47.1 & 34.8 & 76.0 & 72.0 & 46.7 & 79.5 & 78.0 & 53.8 & 50.7 & 65.4 & 63.6 & 75.4 & 47.4 & 64.8\\
\texttt{gpt-4-32k} & 84.1 & 77.6 & 42.0 & 83.1 & 86.3 & 49.8 & 68.4 & 80.2 & \textbf{79.3} & 46.4 & \textbf{82.7} & 85.4 & 60.4 & 52.2 & 68.3 & 68.6 & 84.1 & \textbf{60.2} & 71.8
\\
\midrule
& ko & lt & mr & nl & pl & pt & ro & ru & ta & te & th & tl & tr & uk & ur & vi & wo & yo & zh & \textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT &  51.7 & 78.8 & 68.7 & 88.6 & 80.7 & 88.0 & 71.5 & 82.4 & 58.5 & 75.2 & 41.3 & 80.5 & 70.5 & 80.6 & 56.6 & 55.4 & 0.0 & \textbf{56.6} & \textbf{59.6} & 71.9\\
XLM-R Large & \textbf{57.1} & \textbf{84.2} & \textbf{81.8} & \textbf{89.5} & \textbf{86.8} & \textbf{90.2} & \textbf{82.6} & \textbf{87.3} & \textbf{64.0} & 84.2 & \textbf{48.5} & \textbf{92.4} & \textbf{81.2} & \textbf{85.8} & \textbf{70.8} & 58.5 & 0.0 & 24.8 & 44.1 & \textbf{76.2} \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 39.0 & 71.3 & 57.9 & 78.3 & 81.7 & 76.7 & 66.7 & 69.9 & 32.6 & 79.8 & 25.5 & 54.3 & 77.2 & 58.9 & 39.9 & 57.7 & 50.4 & 7.0 & 57.2 & 60.2\\
\texttt{gpt-4-32k} & 51.2 & 73.7 & 79.1 & 81.8$^{\dagger}$ & 80.7 & 81.0 & 66.3$^{\dagger}$ & 74.7 & 34.7 & \textbf{84.6} & 31.2$^{\dagger}$ & 58.4$^{\dagger}$ & 77.0 & 61.9 & 41.3 & \textbf{64.7} & \textbf{59.1} & 33.8$\dagger$ & 63.5 & 66.6
\\
\midrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in POS. Metric: F1 Score. (All numbers are Monolingual results except the ones marked with $\dagger$ symbol which indicate Zero-Shot Cross-Lingual results (due to the absence of training data in those languages)}
\label{tab:results_summary}
\end{table*} 

% NER
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c c c c c c c c c c c c c}
\toprule
Model & en & af & ar & az & bg & bn & de & el & es & et & eu & fa & fi & fr & gu & he & hi & hu & id & it & ja & jv & ka & kk \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & \textbf{86.4} & 76.1 & 42.9 & 65.5 & 76.7 & 69.7 & 79.5 & 70.9 & \textbf{75.3} & 75.8 & \textbf{64.4} & 40.0 & 76.6 & 79.6 & 51.3 & \textbf{56.2} & 65.9 & 76.1 & 61.0 & \textbf{81.3} & \textbf{29.2} & 62.4 & 65.1 & 50.3\\
XLM-R Large & 85.4 & \textbf{78.6} & 47.3 & \textbf{69.4} & \textbf{80.9} & \textbf{74.7} & \textbf{80.7} & \textbf{79.2} & 71.8 & \textbf{78.7} & 61.6 & 55.2 & \textbf{79.6} & \textbf{79.8} & \textbf{62.7} & 55.5 & \textbf{70.9} & \textbf{80.2} & 51.8 & 80.3 & 18.5 & 61.9 & \textbf{70.9} & \textbf{54.4} \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 43.2 & 43.8 & 45.4 & 42.1 & 51.6 & 40.3 & 52.7 & 41.0 & 60.2 & 58.7 & 31.5 & 39.3 & 59.1 & 50.7 & 18.4 & 34.3 & 45.5 & 53.7 & 58.4 & 60.0 & 7.4 & 57.7 & 25.1 & 30.9 \\
\texttt{gpt-4-32k} & 49.7 & 55.9 & \textbf{59.4} & 59.6 & 62.6 & 52.7 & 69.2 & 54.4 & 68.6 & 74.4 & 57.8 & \textbf{67.6} & 71.1 & 68.5 & 23.8 & 48.0 & 59.4 & 71.9 & \textbf{72.7} & 72.8 & 9.2 & \textbf{68.8} & 31.6 & 45.3\\
\midrule 
& ko & lt & ml & mr & ms & my & nl & pa & pl & pt & qu & ro & ru & sw & ta & te & th & tl & tr & uk & ur & vi & yo & zh & \textbf{avg}\\\midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & \textbf{59.5} & \textbf{75.8} & 53.0 & 57.0 & 67.1 & 45.7 & 81.0 & 30.5 & 79.2 & \textbf{80.4} & 58.5 & 74.0 & 63.9 & \textbf{71.4} & 50.7 & 48.9 & 0.4 & 72.6 & 73.4 & 69.7 & 35.4 & 74.5 & 45.8 & \textbf{42.5} & 62.3\\
XLM-R Large & 59.2 & \textbf{75.8} & \textbf{60.2} & \textbf{63.4} & \textbf{68.5} & \textbf{55.2} & \textbf{83.2} & 49.4 & \textbf{79.3} & 79.9 & 58.5 & \textbf{78.7} & \textbf{71.9} & 68.9 & \textbf{58.4} & \textbf{53.8} & 0.7 & \textbf{74.7} & \textbf{80.3} & \textbf{78.0} & 60.3 & \textbf{78.3} & 37.0 & 26.6 & \textbf{65.2}\\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 27.9 & 51.9 & 25.2 & 34.4 & 52.0 & 8.7 & 59.4 & 36.7 & 58.4 & 48.9 & 41.9 & 42.7 & 29.4 & 57.7 & 26.0 & 22.0 & 1.7 & 36.5 & 50.5 & 34.4 & 35.7 & 33.5 & 56.9 & 13.3 & 40.3\\
\texttt{gpt-4-32k} & 51.4 & 71.3 & 35.6 & 47.4 & 64.1 & 16.3 & 67.9 & \textbf{49.8} & 70.3 & 64.5 & \textbf{69.8} & 59.6 & 64.8 & 68.9 & 36.9 & 33.0 & \textbf{2.5} & 61.9 & 72.9 & 58.4 & \textbf{69.6} & 58.4 & \textbf{73.9} & 18.5 & 55.5\\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in PAN-X. Metric: F1 Score.}
\label{tab:results_summary}
\end{table*}

% XStoryCloze
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c }
\toprule
Model & ar & en & es & eu & hi & id & my & ru & sw & te & zh &
\textbf{avg} \\ \midrule
%\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
%\midrule
%mBERT & \\
%mT5-Base & \\
%XLM-R Large &  \\
%\midrule
\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & 79.7 & 95.7 & 87.3 & 70.5 & 79.9 & 85.6 & 49.9 & 67.3 & 65.3 & 67.4 & 90.0 & 76.2 \\
XGLM & 59.8 & 75.9 & 69.2 & 63.8 & 62.5 & 70.8 & 61.2 & 72.4 & 65.2 & 63.4 & 67.7 & 66.5 \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 92.5 & 96.8 & 95.8 & 78.4 & 91.1 & 95.0 & 57.2 & 96.6 & 92.3 & 73.1 & 95.6 & 87.7 \\
\texttt{gpt-3.5-turbo} (TT) & 94.3 & 96.8 & 96.1 & 92.5 & 94.7 & 95.2 & 88.6 & 96.2 & 88.7 & 93.6 & 95.6 & 93.9 \\
\texttt{text-davinci-003} & 87.4 & 98.3 & 97.6 & 78.1 & 77.8 & 96.4 & 47.4 & 94.2 & 78.1 & 57.6 & 95.0 & 82.5 \\
\texttt{text-davinci-003} (TT) & 95.0 & 98.3 & 96.2 & 94.1 & 95.1 & 95.9 & 90.1 & 96.9 & 90.7 & 94.3 & 96.2 & 94.8 \\
\texttt{gpt-4-32k} & \textbf{99.1} & \textbf{99.6} & \textbf{99.5} & \textbf{97.6} & \textbf{98.8} & \textbf{99.0} & 77.6 & 99.1 & \textbf{98.4} & 93.4 & \textbf{99.2} & 96.5 \\
\texttt{gpt-4-32k} (TT) & 97.7 & \textbf{99.6} & 98.7 & 96.8 & 97.9 & 98.1 & \textbf{93.2} & \textbf{99.2} & 93.6 & \textbf{96.4} & 98.3 & \textbf{97.0}
\\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in XStoryCloze. Metric: Accuracy.}
\label{tab:results_summary}
\end{table*}


% % XLSum
% \begin{table*}[h]
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{l c c c c c c c c c c c c c c c c c c c c c c c }
% \toprule
% Model & en & am & ar & az & bn & my & zh & zh & fr & gu & ha & hi & ig & in & ja & rn & ko & ky & mr & ne & om & ps\\ \midrule
% \multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
% \midrule
% mT5-RL & 28.6 & 26.1 & 19.3 & 22.9 & 18.2 & 22.4 & 16.2 & 28.2 & 24.1 & 24.2 & 29.3 & 24.5 & 30.8 & 29.9 & 30.9 & 26.1 & 25.8 & 29.9 & 25.1 & 14.2 & 31.7 & 16.5 \\
% \midrule
% \multicolumn{10}{l}{\textit{Open AI Models}} \\
% \midrule
% \texttt{gpt-3.5-turbo} & 26.0 & 6.0 & 7.1 & 24.0 & 0.0 & 7.1 & 8.0 & 8.0 & 26.0 & 0.0 & 24.0 & 4.0 & 21.0 & 27.0 & 22.0 & 21.0 & 21.0 & 8.0 & 6.0 & 0.0 & 14.2 & 0.0 \\
% \texttt{gpt-4-32k} & 27.4 & 16.9 & 23.1 & 15.6 & 0.0 & 6.3 & 27.2 & 9.7 & 23.8 & 6.6 & 24.5 & 24.0 & 20.0 & 25.4 & 24.1 & 21.9 & 21.2 & 13.3 & 7.0 & 18.5 & 14.7 & 23.9\\
% \midrule 
% & fa & pidgin & pt & pa & gd & sr & sr & si & so & es & sw & te & th & ti & tr & uk & ur & uz & vi & cy & yo & \textbf{avg}\\\midrule
% \multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
% \midrule
% mT5-RL & 20.9 & 15.4 & 18.1 & 28.9 & 20.1 & 19.9 & 37.4 & 22.1 & 17.6 & 19.9 & 25.5 & 21.2 & 29.2 & 23.5 & 31.6 & 24.3 & 25.1 & 33.4 & 32.0 & 32.8 & 30.1 & 25.0\\
% \midrule
% \multicolumn{10}{l}{\textit{Open AI Models}} \\
% \midrule
% \texttt{gpt-3.5-turbo} & 0.0 & 24.0 & 26.0 & 5.0 & 23.0 & 7.1 & 17.0 & 0.0 & 21.0 & 21.0 & 27.0 & 0.0 & 16.0 & 0.0 & 26.0 & 11.0 & 3.0 & 5.0 & 35.0 & 22.0 & 19.0 & 13.7 \\
% \texttt{gpt-4-32k} & 23.7 & 27.1 & 24.0 & 5.9 & 24.6 & 18.1 & 16.3 & 15.4 & 17.4 & 20.0 & 24.8 & 5.6 & 9.7 & 18.8 & 17.8 & 18.2 & 25.5 & 12.3 & 21.6 & 25.5 & 16.3 & 18.2 \\
% \bottomrule
% \end{tabular}}
% \caption{Comparing performance of different models on all languages in XLSum. Metric: Rouge - L}
% \label{tab:results_summary}
% \end{table*}
% %%%%%%%%%



\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccccclccccc}
\hline
 &
  \multicolumn{3}{c}{Google} &
  \multicolumn{3}{c}{Microsoft} &
  \multicolumn{3}{c}{Amazon} &
  \multicolumn{3}{c}{Systran} &
  \multicolumn{3}{c}{GPT Turbo 3.5} &
  \multicolumn{3}{c}{Bloomz} \\
 &
  Acc &
  $\Delta_G$\ &
  $\Delta_S$\ &
  Acc &
  $\Delta_G$\ &
  $\Delta_S$\ &
  Acc &
  $\Delta_G$\ &
  $\Delta_S$\ &
  Acc &
  $\Delta_G$\ &
  $\Delta_S$\ &
  \multicolumn{1}{c}{Acc} &
  $\Delta_G$\ &
  $\Delta_S$\ &
  Acc &
  $\Delta_G$\ &
  $\Delta_S$\ \\ \hline
es &
  50.9 &
  23.2 &
  20.9 &
  45 &
  36.5 &
  22.9 &
  \textbf{57.2} &
  15.3 &
  21.7 &
  42.5 &
  46.2 &
  15.6 &
  54.9 &
  22.7 &
  26.2 &
  55.6 &
  17.2 &
  32.5 \\
fr &
  \textbf{61.6} &
  6.1 &
  22.3 &
  44.5 &
  34.2 &
  15.8 &
  54.2 &
  16.4 &
  15 &
  43.4 &
  41.8 &
  -0.1 &
  52.7 &
  21.4 &
  26.1 &
  52 &
  17.8 &
  24.6 \\
it &
  38.6 &
  32.9 &
  18.6 &
  38.8 &
  41.8 &
  10.5 &
  40.2 &
  26.8 &
  14.7 &
  38.1 &
  47.3 &
  6.3 &
  45.1 &
  21.9 &
  26.7 &
  \textbf{45.7} &
  9 &
  18.5 \\ \hline
ru &
  37.8 &
  36.7 &
  11.4 &
  36.9 &
  42 &
  8.4 &
  39.8 &
  34.8 &
  9.4 &
  37.3 &
  44.1 &
  9.2 &
  \textbf{41} &
  31.6 &
  10.2 &
  5.9 &
  INV &
  0 \\
uk &
  38.4 &
  43.5 &
  10.7 &
  41.3 &
  46.8 &
  11.9 &
  - &
  - &
  - &
  28.9 &
  22.4 &
  12.9 &
  \textbf{42.9} &
  34.2 &
  12.1 &
  16.8 &
  22.7 &
  2.2 \\ \hline
he &
  50.8 &
  11.7 &
  35.5 &
  44 &
  22 &
  29.8 &
  48 &
  13.6 &
  45.9 &
  43.1 &
  26.9 &
  23.1 &
  \textbf{57.5} &
  7.6 &
  40.8 &
  27.5 &
  31.4 &
  5 \\
ar &
  45.8 &
  42.5 &
  16.2 &
  45 &
  47.1 &
  14.2 &
  48.3 &
  37.8 &
  18.8 &
  45.6 &
  49.4 &
  -4.1 &
  \textbf{61.1} &
  13.9 &
  27.9 &
  48.1 &
  23 &
  25.6 \\ \hline
de &
  59.4 &
  12.5 &
  12.6 &
  \textbf{74.1} &
  0 &
  8.8 &
  62.4 &
  12 &
  16.7 &
  48.5 &
  34.5 &
  10 &
  57.5 &
  19.5 &
  14.2 &
  47.6 &
  56.2 &
  6.6 \\ \hline
\end{tabular}%
}
\caption{Performance of commercial MT systems and LLMs on the WinoMT corpus on 8 target languages. Results are categorized by language family. Acc indicates overall gender accuracy (\% of instances the translation had the correct gender), $\Delta_G$ denotes the difference in performance (F1 score) between masculine and feminine scores, and $\Delta_S$ is the difference in performance (F1 score) between pro-stereotypical and anti-stereotypical gender role assignments (higher numbers in the two latter metrics indicate stronger biases). Numbers in bold indicate best accuracy for the language across all systems. \footnotesize {Notes: [1. For Google, Microsoft, Amazon, and Systran we use the translations provided by \cite{stanovsky2019evaluating}. Some values differ from the original paper due to updated Spcay modules. 2. For Ru in Bloomz, Precision in male predictions is 0 leading to Invalid (INV) in $\Delta_G$]}}
\label{tab:wino-mt}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{lcccccccl} 
\hline
Model                           & es                                        & fr                                        & it                                        & pt                                        & ru                                        & tr                                        & \textbf{avg} &   \\ 
\hline
\multicolumn{9}{l}{\textit{LLM Baselines}}                                                                                                                                                                                                                                                                          \\ 
\hline
PALM (0-Shot)                   & 79.83                                     & 78.99                                     & -                                         & 77.58                                     & 80.35                                     & 84.1                                      & 80.17        &   \\
PALM (10-Shot Monolingual)      & \textbf{91.23}                                     & 86.16                                     & -                                         & 90.99                                     & 92.47                                     & 84.5                                      & 89.07        &   \\
PALM-2 (0-Shot)                 & 88.6                                      & 84.11                                     & -                                         & 87.68                                     & 90.5                                      & 93.42                                     & 88.86        &   \\
PALM-2 (10-Shot Monolingual)    & 89.68                                     & \textbf{87.94}                                     & -                                         & \textbf{92.05}                                     & \textbf{94.25}                                     & \textbf{94.34}                                     & \textbf{91.65}        &   \\ 
\hline
\multicolumn{9}{l}{\textit{OpenAI Models}}                                                                                                                                                                                                                                                                                 \\ 
\hline
gpt-3.5-turbo (Crosslingual)    & \textcolor[rgb]{0.129,0.129,0.133}{77.27} & \textcolor[rgb]{0.129,0.129,0.133}{73.64} & \textcolor[rgb]{0.129,0.129,0.133}{80.05} & \textcolor[rgb]{0.129,0.129,0.133}{81.16} & \textcolor[rgb]{0.129,0.129,0.133}{74.99} & \textcolor[rgb]{0.129,0.129,0.133}{85.65} & 78.79        &   \\
gpt-3.5-turbo (TT)              & 74.20                                     & 70.09                                     & 76.67                                     & 72.66                                     & 73.68                                     & 82.99                                     & 75.05        &   \\
text-davinci-003 (Crosslingual) & 79                                        & 74.55                                     & \textbf{81.11}                                     & 81.63                                     & 79.13                                     & 93.55                                     & 81.50        &   \\
text-davinci-003 (TT)           & 79.06                                     & 72.93                                     & 78.93                                     & 75.18                                     & 80.48                                     & 93.22                                     & 79.97        &   \\
\hline
\end{tabular}
\caption{Comparing performance of different models on all languages in Jigsaw. Metric: Accuracy.}
\label{tab:jigsaw_results_summary}
\end{table*}





% \begin{figure*}
%     \centering
%     % \begin{subfigure}[t]{0.95\linewidth}
%     \includegraphics[width=0.95\textwidth]{figures/udpos_13.pdf}
%     % \caption{}
%     % \end{subfigure}
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/udpos_26.pdf}
%     % \caption{}
%     % \label{fig:udposb2}
%     % \end{subfigure}\\
%     % \begin{subfigure}[t]{0.95\textwidth}
%     % \includegraphics[width=0.95\textwidth]{figures/udpos_39.pdf}
%     % \caption{}
%     % \label{fig:udposb3}

%     % \end{subfigure}\\
%     % \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
%     \caption{Comparing performance of different models on UDPOS on 10 representative languages. (Results for all languages available in Appendix).}
%     \label{fig:udposb1}
    
% \end{figure*}




