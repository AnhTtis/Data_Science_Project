\section{Discussion}

\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.24\linewidth}
    \includegraphics[width=0.95\textwidth]{emnlp2023-latex/figures/analysis/XNLI_icl.pdf}
    \caption{}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\linewidth}
    \includegraphics[width=0.95\textwidth]{emnlp2023-latex/figures/analysis/XCOPA_icl.pdf}
    \caption{}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\linewidth}
    \includegraphics[width=0.95\textwidth]{emnlp2023-latex/figures/analysis/xcopa_prompt_tuning.pdf}
    \caption{}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\linewidth}
    \includegraphics[width=0.95\textwidth]{emnlp2023-latex/figures/analysis/xstorycloze_expl.pdf}
    \caption{}
    \end{subfigure}
    % \begin{subfigure}[t]{0.95\textwidth}
    \caption{Analysing the effect on GPT-3.5-Turbo's performance given different factors.}
    \label{fig:analysis_sum}
\end{figure*}

% Some of these can be moved to the intro as appropriate
In this section, we discuss some of our findings during the benchmarking study. Overall, we find that there is a gap between the performance of generative LLMs (GPT models and BLOOMZ) and SOTA models on the  tasks across languages.  We also find that this gap is reduced in case of high-resource languages that share the same script as English (the Latin script), and is higher for under-resourced languages. This is consistent with findings in previous benchmarking studies on generative LLMs \cite{bang-etal-2021-assessing, hendy2023good}. This observation is also consistent with literature which shows that the efficacy of multilingual transfer improve due to shared scripts \cite{fujinuma2022match, khemchandani-etal-2021-exploiting}. Next, we discuss some of the issues that still remain to be addressed that could potentially improve the multilingual performance of generative models.

\subsection{Tokenization}
\label{sec:tokenization}

Tokenization is a key component that influences the performance of multilingual models \cite{rust-etal-2021-good}. Figure \ref{fig:tokenizer-comparison} highlights the disparity in DV003's tokenization to the tokenizer in mBERT \cite{bert2019} and a language-specific tokenizer. This points to a clear direction for improvement in generative models for multilingual settings. 

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=16cm]{figures/fertility.pdf}
    \caption{Tokenizer Fertility for OpenAI models, mBERT, and BLOOM for different languages}
    \label{fig:tokenizer-fertility}
\end{figure*}


\begin{figure}[!htb]
    \includegraphics[width=6cm]{figures/fertility_score_corr.pdf}
    \caption{Correlation between the performance of GPT-3.5-Turbo with the tokenizer fertility. We report the curves for the cases where the person coefficient $|\rho| > 0.7$ with a p-value of 0.05. We have combined Indic-XNLI and XNLI for a better coverage of languages.}
    \label{fig:fertility_perf_corr}
\end{figure}

% \begin{figure}[!htb]
%     \includegraphics[width=6cm]{figures/hi_sentence_length.pdf}
%     \caption{Comparing the distribution of the number of tokens produced for different sentences by GPT, mBERT, and reference number of tokens based on Univeral Dependencies in Hindi. The substantial increase in the number of tokens for GPT compared to mBERT and the reference might be indicative of its inability to capture the long-form context in the prompts provided for these languages, resulting in poor performance.}
%     \label{fig:tokenizer-comparison}
% \end{figure}

Since tokenization is crucial to generating a meaningful representation of the input, its differential behaviour across languages can help explain the model's poor performance across some languages (especially while being prompted in the monolingual setting). Figure \ref{fig:tokenizer-fertility} demonstrates the tokenization of GPT's tokenizer on a set of languages. The poor tokenization for some of the lower-resource languages can lead to issues \textbf{\textit{(a) poor context encapsulation}} the  tokenization limits the length of context that can be provided for a given language which can seriously impede the performance of models for low-resource languages. Additionally, prior work like \cite{Zhang2022HowRI} also shows that degenerate tokenization is strongly related to the generation of \textbf{\textit{(b) poor semantic representations of inputs}}, which affects the performance of such representations on downstream tasks. Recent work like \cite{Ahia2023DoAL} also highlight an interesting fiscal disparity - the degenerate tokenization in low-resource languages often leads to higher costs of generation for much poorer quality of generation. This disparity exacerbates the adverse impact of inadequate tokenization for low-resource languages.

\subsection{Translating prompts to multiple languages}

One key observation of translating prompt instructions (in the monolingual prompting setting) is that the resulting prompts are often not semantically meaningful, including for some moderate-resource languages like Hindi. This is typically due to the difference in the linguistic structure of the pivot language (English) and the target language. This means that \textbf{\textit{human-supervision or post-editing by a native speaker}} is necessary to generate meaningful prompts for such languages. 

For example: For IndicWNLI, a Winograd-scheme variant of NLI on Indian languages, Figure \ref{fig:marathinli} shows a tuple generated through \textit{Translate-Test} for Marathi for a prompt provided in English. Translating the prompt automatically changes the meaning of the instructions, in this case, swapping the order of the hypothesis and premise, making it difficult for the model to perform the task correctly. 

% \begin{figure*}[h]
% \centering
%     \includegraphics[width=16cm]{figures/marathinli.png}
%     \caption{Marathi prompt for IndicWNLI translated from English}
%     \label{fig:marathinli}
% \end{figure*}

Furthermore, the use of translation also appears to favor languages which share scripts with higher-resource languages included in the pre-training corpus of such models. This complies with existing studies like \cite{khemchandani-etal-2021-exploiting}, which show that sharing dominant scripts leads to better cross-lingual transfer amongst languages leading to improved performance on downstream tasks for unseen languages. This improved transfer, combined with the explored gain due to the seen language's better tokenization \cite{Rust2020HowGI} could explain the gap between languages that use the Latin script vs languages that use other scripts. 

\subsection{Translate-Test}

For the GPT Turbo and GPT4 models, we find that translation may not be necessary for many languages, because the gap between monolingual and translate-test is reduced or monolingual prompting outperforms translate-test in some tasks. This indicates that the models starting from Turbo (for some languages and some tasks) probably have good enough monolingual capabilities to be queried directly in the language. For older models like DV003, translate-test still outperforms monolingual prompting by a large margin.

% Translation - translate-test seems to work best overall. comment on gap between en performance and translate-test.
% \begin{table*}[h]
% \centering 
% \begin{tabular}{cccccccccl}
% \hline
% \multicolumn{10}{c}{\textbf{Performance for M2M MMT on Flores101 Devtest}} \\ \hline
% \multicolumn{1}{c}{\textbf{as}} & \multicolumn{1}{c}{\textbf{gu}} & \multicolumn{1}{c}{\textbf{kn}} & \multicolumn{1}{c}{\textbf{ml}} & \multicolumn{1}{c}{\textbf{or}} & \multicolumn{1}{c}{\textbf{te}} & \multicolumn{1}{c}{\textbf{mr}} & \multicolumn{1}{c}{\textbf{pa}} & \multicolumn{1}{c}{\textbf{sw}} & \multicolumn{1}{c}{\textbf{ta}} \\ \hline
% \multicolumn{1}{c}{3.76} & \multicolumn{1}{c}{0.51} & \multicolumn{1}{c}{1.56} & \multicolumn{1}{c}{11.15} & \multicolumn{1}{c}{0.14} & \multicolumn{1}{c}{1.86} & \multicolumn{1}{c}{10.35} & \multicolumn{1}{c}{5.65} & \multicolumn{1}{c}{26.95} & \multicolumn{1}{c}{3.35} \\ \hline
% \multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{c}{\textbf{bg}} & \multicolumn{1}{c}{\textbf{th}} & \multicolumn{1}{c}{\textbf{ur}} & \multicolumn{1}{c}{\textbf{ru}} & \multicolumn{1}{c}{\textbf{tr}} & \multicolumn{1}{c}{\textbf{vi}} & \multicolumn{1}{c}{\textbf{hi}} & \multicolumn{1}{c}{\textbf{ko}} &  \\ \hline
% \multicolumn{1}{c}{} & \multicolumn{1}{c}{37.35} & \multicolumn{1}{c}{11.63} & \multicolumn{1}{c}{15.04} & \multicolumn{1}{c}{27.14} & \multicolumn{1}{c}{24.88} & \multicolumn{1}{c}{35.1} & \multicolumn{1}{c}{27} & \multicolumn{1}{c}{18.51} &  \\ \hline
% \multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{c}{\textbf{it}} & \multicolumn{1}{c}{\textbf{fr}} & \multicolumn{1}{c}{\textbf{es}} & \multicolumn{1}{c}{\textbf{de}} & \multicolumn{1}{c}{\textbf{ar}} & \multicolumn{1}{c}{\textbf{zh}} & \multicolumn{1}{c}{\textbf{ja}} & \multicolumn{1}{c}{\textbf{bn}} &  \\ \hline
% \multicolumn{1}{c}{} & \multicolumn{1}{c}{27.74} & \multicolumn{1}{c}{41.99} & \multicolumn{1}{c}{25.57} & \multicolumn{1}{c}{32.56} & \multicolumn{1}{c}{17.92} & \multicolumn{1}{c}{19.33} & \multicolumn{1}{c}{22.77} & \multicolumn{1}{c}{22.86} &  \\ \hline
% \end{tabular}
% \label{tab:translation-analysis}
% \caption{Performance of M2M MMT \cite{10.1162/tacl_a_00474} on Flores101: We recall this performance evaluation in order to provide the reader a ballpark of the expected ranges of translation models for the languages we consider}
% \end{table*}

% Our experiments with DV003 show that the translate-test prompt setting works best overall for all languages and tasks. Translating into English and querying the system (followed by translating into the target language for generative tasks) is feasible for the languages supported by translators available today. To analyze the impact of translation performance, we present the translation performance of M2M \cite{10.1162/tacl_a_00474} in Table 4 on a diverse, domain-agnostic test set, Flores101 \cite{goyal2022flores}. These numbers provide a ballpark for SOTA translation model performance for the languages we consider, in the absence of numbers from the Bing translation service we use. As observed, low-resource languages do not have very competitive translation performance, meaning that translation errors will propagate. Interestingly, since monolingual querying performance for these languages is also poor, translate-test comes out as the best strategy even for them.  We expect, and empirically observe, the efficacy of translate-test varies by the same trend. High-resource languages tend to show stable gains through translate-test, and lower-resource show, variable (though consistent improvements) through translate-test querying. 

However this process of translating into English, sending the query to a generative model, and translating back must be considered carefully. Firstly, translation errors propagate making the performance of the model worse for languages that do not have accurate translation. Second, by translating to and from English, we forgo the knowledge encoded in each language. The pitfalls of doing so can be similar to when languages die \cite{harrison2007languages} - loss of information relating to local culture, contexts and people. Further, subtleties of language and culture may be lost during translation, and the language produced by translation systems is known to contain artefacts, leading some to dub this kind of language as "translationese" \cite{graham2019translationese}. In addition, Responsible AI guardrails, filters and techniques for reducing bias will need to be rethought and be added at the translation stage if translate-test is the chosen strategy. 

However, it is encouraging to see the improved performance of models such as GPT4 on many languages beyond English. This opens up the possibility of translating in and out of languages other than English (in other words, choosing the pivot language) which may help alleviate some of the issues mentioned above.

\subsection{Test data contamination}
\label{sec:contam}
Datasets that are available on the Internet may have been ingested during model training, which is referred to as test data contamination. GPT4 shows improved performance across most languages and tasks in our study and it is not known whether this is because it has already been exposed to the datasets we use for benchmarking. In future versions of this study, we plan to conduct experiments to check for data contamination. An important direction for future research is to create multilingual benchmarks (which are already few in number) that cannot be accessed by crawling the web.

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lccl}
    \toprule
    Dataset & Card Fill & Data Acc. w/o Down. & Release Date \\
    \midrule
    XNLI & \cellcolor{red!25}Full & \cellcolor{green!25} No & \cellcolor{red!25} September 2019  \\
    Indic-XNLI & \cellcolor{red!25}Full & \cellcolor{green!25} No &  \cellcolor{green!25} April 2022 \\
    PAWS-X & \cellcolor{red!25}Full  & \cellcolor{green!25} No & \cellcolor{red!25} August 2019 \\
    XCOPA &  \cellcolor{yellow!25}Partial & \cellcolor{red!25} Yes & \cellcolor{red!25} April 2020 \\
    XStoryCloze &  \cellcolor{yellow!25}Partial &  \cellcolor{green!25} No & \cellcolor{green!25} May 2023\\
    XQuAD & \cellcolor{red!25}Full & \cellcolor{red!25} Yes & \cellcolor{red!25} October 2019 \\
    MLQA & \cellcolor{red!25}Full & \cellcolor{green!25} No & \cellcolor{red!25} October 2019 \\
    TyDiQA-GoldP & \cellcolor{red!25}Full & \cellcolor{green!25} No & \cellcolor{red!25} February 2020 \\
    IndicQA &  \cellcolor{yellow!25}Partial & \cellcolor{green!25} No & \cellcolor{green!25} September 2022 \\
    PAN-X & \cellcolor{red!25}Full & \cellcolor{green!25} No & \cellcolor{red!25} July 2017 \\
    UDPOS &  \cellcolor{red!25}Full & \cellcolor{green!25} No & \cellcolor{red!25} March 2020 \\
    XLSum & \cellcolor{yellow!25}Partial & \cellcolor{green!25} No & \cellcolor{yellow!25} June 2021 \\
    \bottomrule
    \end{tabular}}
    \caption{Contamination analysis for the datasets that we consider in MEGA. Card Fill denotes whether the model could fill the dataset card for a given dataset. Data Acc. w/o Down. refers to if the evaluation datasets as released by the authors are available to access online without download. For the release date, we check if it was after September 2021. We use \colorbox{red!25}{red color} when there is a strong suspicion of contamination based on these three metrics, \colorbox{green!25}{green} for no suspicion, and \colorbox{yellow!25}{yellow} for partial evidence.}
    \label{tab:contam}
\end{table}

%  \begin{table}[h]
%      \centering
% \resizebox{\linewidth}{!}{%
%      \begin{tabular}{lll}
%      \toprule
%     Dataset&Task&Languages\\
%     \midrule
%     XNLI&Natural Language Inference& 15\\
%     Indic-XNLI&Natural Language Inference&11\\
%     GLUECoS&Natural Language Inference&2\\
%     %Indic-WNLI&Natural Language Inference&gu, hi, mr\\
%     %GLUECoS-NLI&Natural Language Inference&hi-en\\
%     %GLUECoS-En-Es-Sentiment&Sentiment Analysis&es-en\\
%     PAWS-X&Paraphrase Identification&7\\
%     XCOPA&Commonsense Reasoning& 10\\
%     XStoryCloze&Commonsense Reasoning& 11 \\
%     TyDiQA-GoldP&Question Answering& 9 \\
%     MLQA&Question Answering&6\\
%     XQuAD&Question Answering& 11\\
%     IndicQA&Question Answering& 10\\
%     UDPOS&Part of Speech Tagging& 38\\
%     PANX& NER & 48\\ 
%     WinoMT&Gender Bias& 8 \\
%     GLUECoS&Sentiment Analysis& 2 \\
%     Jigsaw&Toxicity Classification& 6\\
%     XLSum&Summarization&  44\\
    
%   \bottomrule
%      \end{tabular}}
%      \caption{Datasets and Language coverage of the datasets that MEGA presents evaluation for.}
%      \label{tab:datasets}
%      \vspace{-0.4cm}
%  \end{table}

\section{Looking Forward}

In this paper, we present our work towards a comprehensive benchmarking of generative AI models across languages. In the future, we would like to extend our benchmarking along the following dimensions. Firstly, we would like to expand language coverage to include more low-resource languages and languages that are typologically diverse.  We would also like to include more standard NLP tasks, as well as tasks that are taken from real-world applications, such as the Bing tasks included in XGLUE \cite{liang2020xglue}. Additionally, we would also like to include other dimensions such as calibration, bias and disinformation. As pointed out by \cite{hendy2023good}, the sufficiency of the metrics and the quality of available datasets especially for less resourced languages, may not adequately capture performance. While this might be true, benchmarking does serve as an effective starting point and highlights important trends. However, any holistic evaluation would consider such metrics in conjunction with human-evaluations by users.
Any real world deployment of the technology, especially for multilingual use-case scenarios, would not be productive and successful in meeting user-requirements without a thorough evaluation, ideally a combination of automatic evaluation and human-feedback.

% Talk about dangers of deploying these models in multilingual settings without proper evaluation. Benchmarking is an ok starting point but need much more robust real-world evaluation and user feedback.