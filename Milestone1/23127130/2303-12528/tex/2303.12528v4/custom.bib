@article{nivre2018universal,
  title={Universal Dependencies 2.2},
  author={Nivre, Joakim and Abrams, Mitchell and Agi{\'c}, {\v{Z}}eljko and Ahrenberg, Lars and Antonsen, Lene and Aranzabe, Maria Jesus and Arutie, Gashaw and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and others},
  year={2018}
}

@article{fujinuma2022match,
  title={Match the script, adapt if multilingual: Analyzing the effect of multilingual pretraining on cross-lingual transferability},
  author={Fujinuma, Yoshinari and Boyd-Graber, Jordan and Kann, Katharina},
  journal={arXiv preprint arXiv:2203.10753},
  year={2022}
}

@article{Ahia2023DoAL,
  title={Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models},
  author={Orevaoghene Ahia and Sachin Kumar and Hila Gonen and Jungo Kasai and David R. Mortensen and Noah A. Smith and Yulia Tsvetkov},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.13707},
  url={https://api.semanticscholar.org/CorpusID:258841465}
}

@inproceedings{ponti2020xcopa,
  title={XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning},
  author={Ponti, Edoardo Maria and Glava{\v{s}}, Goran and Majewska, Olga and Liu, Qianchu and Vuli{\'c}, Ivan and Korhonen, Anna},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2362--2376},
  year={2020}
}

@article{10.1145/3578707,
author = {Toraman, Cagri and Yilmaz, Eyup Halit and \c{S}ahi̇nu\c{c}, Furkan and Ozcelik, Oguzhan},
title = {Impact of Tokenization on Language Models: An Analysis for Turkish},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3578707},
doi = {10.1145/3578707},
abstract = {Tokenization is an important text preprocessing step to prepare input tokens for deep language models. WordPiece and BPE are de facto methods employed by important models, such as BERT and GPT. However, the impact of tokenization can be different for morphologically rich languages, such as Turkic languages, in which many words can be generated by adding prefixes and suffixes. We compare five tokenizers at different granularity levels, that is, their outputs vary from the smallest pieces of characters to the surface form of words, including a Morphological-level tokenizer. We train these tokenizers and pretrain medium-sized language models using the RoBERTa pretraining procedure on the Turkish split of the OSCAR corpus. We then fine-tune our models on six downstream tasks. Our experiments, supported by statistical tests, reveal that the morphological-level tokenizer delivers a challenging performance with de facto tokenizers. Furthermore, we find that increasing the vocabulary size improves the performance of Morphological- and Word-level tokenizers more than that of de facto tokenizers. The ratio of the number of vocabulary parameters to the total number of model parameters can be empirically chosen as 20\% for de facto tokenizers and 40\% for other tokenizers to obtain a reasonable trade-off between model size and performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {116},
numpages = {21},
keywords = {Language model, vocabulary size, morphological analysis, tokenization}
}

@inproceedings{adelani-etal-2022-masakhaner,
    title = "{M}asakha{NER} 2.0: {A}frica-centric Transfer Learning for Named Entity Recognition",
    author = "Adelani, David  and
      Neubig, Graham  and
      Ruder, Sebastian  and
      Rijhwani, Shruti  and
      Beukman, Michael  and
      Palen-Michel, Chester  and
      Lignos, Constantine  and
      Alabi, Jesujoba  and
      Muhammad, Shamsuddeen  and
      Nabende, Peter  and
      Dione, Cheikh M. Bamba  and
      Bukula, Andiswa  and
      Mabuya, Rooweither  and
      Dossou, Bonaventure F. P.  and
      Sibanda, Blessing  and
      Buzaaba, Happy  and
      Mukiibi, Jonathan  and
      Kalipe, Godson  and
      Mbaye, Derguene  and
      Taylor, Amelia  and
      Kabore, Fatoumata  and
      Emezue, Chris Chinenye  and
      Aremu, Anuoluwapo  and
      Ogayo, Perez  and
      Gitau, Catherine  and
      Munkoh-Buabeng, Edwin  and
      Memdjokam Koagne, Victoire  and
      Tapo, Allahsera Auguste  and
      Macucwa, Tebogo  and
      Marivate, Vukosi  and
      Elvis, Mboning Tchiaze  and
      Gwadabe, Tajuddeen  and
      Adewumi, Tosin  and
      Ahia, Orevaoghene  and
      Nakatumba-Nabende, Joyce  and
      Mokono, Neo Lerato  and
      Ezeani, Ignatius  and
      Chukwuneke, Chiamaka  and
      Oluwaseun Adeyemi, Mofetoluwa  and
      Hacheme, Gilles Quentin  and
      Abdulmumin, Idris  and
      Ogundepo, Odunayo  and
      Yousuf, Oreen  and
      Moteu, Tatiana  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.298",
    pages = "4488--4508",
    abstract = "African languages are spoken by over a billion people, but they are under-represented in NLP research and development. Multiple challenges exist, including the limited availability of annotated training and evaluation datasets as well as the lack of understanding of which settings, languages, and recently proposed methods like cross-lingual transfer will be effective. In this paper, we aim to move towards solutions for these challenges, focusing on the task of named entity recognition (NER). We present the creation of the largest to-date human-annotated NER dataset for 20 African languages. We study the behaviour of state-of-the-art cross-lingual transfer methods in an Africa-centric setting, empirically demonstrating that the choice of source transfer language significantly affects performance. While much previous work defaults to using English as the source language, our results show that choosing the best transfer language improves zero-shot F1 scores by an average of 14{\%} over 20 languages as compared to using English.",
}


@inproceedings{litmus,
  title={Litmus predictor: An ai assistant for building reliable, high-performing and fair multilingual nlp systems},
  author={Srinivasan, Anirudh and Kholkar, Gauri and Kejriwal, Rahul and Ganu, Tanuja and Dandapat, Sandipan and Sitaram, Sunayana and Santhanam, Balakrishnan and Aditya, Somak and Bali, Kalika and Choudhury, Monojit},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={11},
  pages={13227--13229},
  year={2022}
}

@inproceedings{wu2020all,
  title={Are All Languages Created Equal in Multilingual BERT?},
  author={Wu, Shijie and Dredze, Mark},
  booktitle={Proceedings of the 5th Workshop on Representation Learning for NLP},
  pages={120--130},
  year={2020}
}

@inproceedings{ahuja2022economics,
  title={On the Economics of Multilingual Few-shot Learning: Modeling the Cost-Performance Trade-offs of Machine Translated and Manual Data},
  author={Ahuja, Kabir and Choudhury, Monojit and Dandapat, Sandipan},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1369--1384},
  year={2022}
}

@inproceedings{hasan2021xl,
  title={XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages},
  author={Hasan, Tahmid and Bhattacharjee, Abhik and Islam, Md Saiful and Mubasshir, Kazi and Li, Yuan-Fang and Kang, Yong-Bin and Rahman, M Sohel and Shahriyar, Rifat},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={4693--4703},
  year={2021}
}

@inproceedings{pan2017cross,
  title={Cross-lingual name tagging and linking for 282 languages},
  author={Pan, Xiaoman and Zhang, Boliang and May, Jonathan and Nothman, Joel and Knight, Kevin and Ji, Heng},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1946--1958},
  year={2017}
}

@article{zeman2020universal,
  title={Universal dependencies 2.5},
  author={Zeman, Daniel and Nivre, Joakim and Abrams, Mitchell and Ackermann, Elia and Aepli, No{\"e}mi and Aghaei, Hamid and Ziane, R},
  journal={LINDAT/CLARIAHCZ digital library at the Institute of Formal and Applied Linguistics (UFAL), Faculty of Mathematics and Physics, Charles University. url: http://hdl. handle. net/11234/1-3226},
  year={2020}
}

@inproceedings{mostafazadeh2017lsdsem,
  title={Lsdsem 2017 shared task: The story cloze test},
  author={Mostafazadeh, Nasrin and Roth, Michael and Louis, Annie and Chambers, Nathanael and Allen, James},
  booktitle={Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics},
  pages={46--51},
  year={2017}
}

@inproceedings{lin2022few,
  title={Few-shot Learning with Multilingual Generative Language Models},
  author={Lin, Xi Victoria and Mihaylov, Todor and Artetxe, Mikel and Wang, Tianlu and Chen, Shuohui and Simig, Daniel and Ott, Myle and Goyal, Naman and Bhosale, Shruti and Du, Jingfei and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={9019--9052},
  year={2022}
}

@inproceedings{stanovsky2019evaluating,
  title={Evaluating Gender Bias in Machine Translation},
  author={Stanovsky, Gabriel and Smith, Noah A and Zettlemoyer, Luke},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1679--1684},
  year={2019}
}

@misc{bubeck2023sparks,
      title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, 
      author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
      year={2023},
      eprint={2303.12712},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{goyal2022flores,
  title={The flores-101 evaluation benchmark for low-resource and multilingual machine translation},
  author={Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc’Aurelio and Guzm{\'a}n, Francisco and Fan, Angela},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={522--538},
  year={2022},
  publisher={MIT Press}
}



@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{gpt4techreport,
  title={GPT4 Technical Report},
  author={OpenAI},
  url={https://arxiv.org/pdf/2303.08774.pdf},
  year={2023}
}

@inproceedings{palm2techreport,
  title={PALM-2 Technical Report},
  author={Google},
  url={https://ai.google/static/documents/palm2techreport.pdf},
  year={2023}
}

@inproceedings{conneau2020unsupervised,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, {\'E}douard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8440--8451},
  year={2020}
}

@misc{muennighoff2022crosslingual,
      title={Crosslingual Generalization through Multitask Finetuning}, 
      author={Niklas Muennighoff and Thomas Wang and Lintang Sutawika and Adam Roberts and Stella Biderman and Teven Le Scao and M Saiful Bari and Sheng Shen and Zheng-Xin Yong and Hailey Schoelkopf and Xiangru Tang and Dragomir Radev and Alham Fikri Aji and Khalid Almubarak and Samuel Albanie and Zaid Alyafeai and Albert Webson and Edward Raff and Colin Raffel},
      year={2022},
      eprint={2211.01786},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{bert2019,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of naacL-HLT},
  pages={4171--4186},
  year={2019}
}

@article{sitaram2019survey,
  title={A survey of code-switched speech and language processing},
  author={Sitaram, Sunayana and Chandu, Khyathi Raghavi and Rallabandi, Sai Krishna and Black, Alan W},
  journal={arXiv preprint arXiv:1904.00784},
  year={2019}
}

@inproceedings{xue2021mt5,
  title={mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={483--498},
  year={2021}
}

@article{graham2019translationese,
  title={Translationese in machine translation evaluation},
  author={Graham, Yvette and Haddow, Barry and Koehn, Philipp},
  journal={arXiv preprint arXiv:1906.09833},
  year={2019}
}

@misc{newbingevent,
        author={Warren, Tom},
      title={Microsoft’s ChatGPT event live blog},
      year={2023},
      url={"https://www.theverge.com/2023/2/7/23588249/microsoft-event-ai-live-blog-openai-chatgpt-bing-announcements-news"},
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}



@article{patra2022beyond,
  title={Beyond english-centric bitexts for better multilingual language representation learning},
  author={Patra, Barun and Singhal, Saksham and Huang, Shaohan and Chi, Zewen and Dong, Li and Wei, Furu and Chaudhary, Vishrav and Song, Xia},
  journal={arXiv preprint arXiv:2210.14867},
  year={2022}
}

@inproceedings{khanuja2020gluecos,
  title={GLUECoS: An Evaluation Benchmark for Code-Switched NLP},
  author={Khanuja, Simran and Dandapat, Sandipan and Srinivasan, Anirudh and Sitaram, Sunayana and Choudhury, Monojit},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={3575--3585},
  year={2020}
}

@article{bang2023multitask,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv preprint arXiv:2302.04023},
  year={2023}
}

@article{hendy2023good,
  title={How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation},
  author={Hendy, Amr and Abdelrehim, Mohamed and Sharaf, Amr and Raunak, Vikas and Gabr, Mohamed and Matsushita, Hitokazu and Kim, Young Jin and Afify, Mohamed and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2302.09210},
  year={2023}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@article{ramesh2023fairness,
  title={Fairness in Language Models Beyond English: Gaps and Challenges},
  author={Ramesh, Krithika and Sitaram, Sunayana and Choudhury, Monojit},
  journal={arXiv preprint arXiv:2302.12578},
  year={2023}
}

@article{wilie2020indonlu,
  title={IndoNLU: Benchmark and resources for evaluating Indonesian natural language understanding},
  author={Wilie, Bryan and Vincentio, Karissa and Winata, Genta Indra and Cahyawijaya, Samuel and Li, Xiaohong and Lim, Zhi Yuan and Soleman, Sidik and Mahendra, Rahmad and Fung, Pascale and Bahar, Syafri and others},
  journal={arXiv preprint arXiv:2009.05387},
  year={2020}
}

@inproceedings{aguilar2020lince,
  title={LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation},
  author={Aguilar, Gustavo and Kar, Sudipta and Solorio, Thamar},
  booktitle={Proceedings of the Twelfth Language Resources and Evaluation Conference},
  pages={1803--1813},
  year={2020}
}


@book{harrison2007languages,
  title={When languages die: The extinction of the world's languages and the erosion of human knowledge},
  author={Harrison, K David},
  year={2007},
  publisher={Oxford University Press}
}

@article{liang2020xglue,
  title={Xglue: A new benchmark dataset for cross-lingual pre-training, understanding and generation},
  author={Liang, Yaobo and Duan, Nan and Gong, Yeyun and Wu, Ning and Guo, Fenfei and Qi, Weizhen and Gong, Ming and Shou, Linjun and Jiang, Daxin and Cao, Guihong and others},
  journal={arXiv preprint arXiv:2004.01401},
  year={2020}
}

@inproceedings{ruder2021xtreme,
  title={XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation},
  author={Ruder, Sebastian and Constant, Noah and Botha, Jan and Siddhant, Aditya and Firat, Orhan and Fu, Jinlan and Liu, Pengfei and Hu, Junjie and Garrette, Dan and Neubig, Graham and others},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={10215--10245},
  year={2021}
}

@inproceedings{hu2020xtreme,
  title={Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation},
  author={Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  booktitle={International Conference on Machine Learning},
  pages={4411--4421},
  year={2020},
  organization={PMLR}
}

@inproceedings{Zhang2022HowRI,
  title={How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?},
  author={Shiyue Zhang and Vishrav Chaudhary and Naman Goyal and James Cross and Guillaume Wenzek and Mohit Bansal and Francisco Guzm{\'a}n},
  booktitle={Conference of the Association for Machine Translation in the Americas},
  year={2022}
}

@inproceedings{Khemchandani2021ExploitingLR,
  title={Exploiting Language Relatedness for Low Web-Resource Language Model Adaptation: An Indic Languages Study},
  author={Yash Khemchandani and Sarvesh Mehtani and Vaidehi Patil and Abhijeet Awasthi and Partha Pratim Talukdar and Sunita Sarawagi},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

@article{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={EMNLP 2018},
  pages={353},
  year={2018}
}

@inproceedings{vilares2016cs,
  title={En-es-cs: An english-spanish code-switching twitter corpus for multilingual sentiment analysis},
  author={Vilares, David and Alonso, Miguel A and G{\'o}mez-Rodr{\'\i}guez, Carlos},
  booktitle={Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)},
  pages={4149--4153},
  year={2016}
}

@inproceedings{artetxe2020cross,
  title={On the Cross-lingual Transferability of Monolingual Representations},
  author={Artetxe, Mikel and Ruder, Sebastian and Yogatama, Dani},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4623--4637},
  year={2020}
}

@inproceedings{yang2019paws,
  title={PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification},
  author={Yang, Yinfei and Zhang, Yuan and Tar, Chris and Baldridge, Jason},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3687--3692},
  year={2019}
}

@article{doddapaneni2022indicxtreme,
  title={IndicXTREME: A Multi-Task Benchmark For Evaluating Indic Languages},
  author={Doddapaneni, Sumanth and Aralikatte, Rahul and Ramesh, Gowtham and Goyal, Shreya and Khapra, Mitesh M and Kunchukuttan, Anoop and Kumar, Pratyush},
  journal={arXiv preprint arXiv:2212.05409},
  year={2022}
}

@article{aggarwal2022indicxnli,
  title={IndicXNLI: Evaluating multilingual inference for Indian languages},
  author={Aggarwal, Divyanshu and Gupta, Vivek and Kunchukuttan, Anoop},
  journal={arXiv preprint arXiv:2204.08776},
  year={2022}
}

@inproceedings{lewis2020mlqa,
  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},
  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7315--7330},
  year={2020}
}

@article{10.1162/tacl_a_00474,
    author = {Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc’Aurelio and Guzmán, Francisco and Fan, Angela},
    title = "{The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {522-538},
    year = {2022},
    month = {05},
    abstract = "{One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the Flores-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are fully aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00474},
    url = {https://doi.org/10.1162/tacl\_a\_00474},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00474/2020699/tacl\_a\_00474.pdf},
}





@article{clark2020tydi,
  title={TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages},
  author={Clark, Jonathan H and Choi, Eunsol and Collins, Michael and Garrette, Dan and Kwiatkowski, Tom and Nikolaev, Vitaly and Palomaki, Jennimaria},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={454--470},
  year={2020},
  publisher={MIT Press}
}

@inproceedings{khanuja2020new,
  title={A New Dataset for Natural Language Inference from Code-mixed Conversations},
  author={Khanuja, Simran and Dandapat, Sandipan and Sitaram, Sunayana and Choudhury, Monojit},
  booktitle={Proceedings of the The 4th Workshop on Computational Approaches to Code Switching},
  pages={9--16},
  year={2020}
}

@inproceedings{Pan2017,
author = {Pan, Xiaoman and Zhang, Boliang and May, Jonathan and Nothman, Joel and Knight, Kevin and Ji, Heng},
booktitle = {Proceedings of ACL 2017},
pages = {1946--1958},
title = {{Cross-lingual name tagging and linking for 282 languages}},
year = {2017}
}



@inproceedings{Conneau2018xnli,
    title = "{XNLI}: Evaluating Cross-lingual Sentence Representations",
    author = "Conneau, Alexis  and
      Rinott, Ruty  and
      Lample, Guillaume  and
      Williams, Adina  and
      Bowman, Samuel  and
      Schwenk, Holger  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of EMNLP 2018",
    year = "2018",
    pages = "2475--2485",
}

@inproceedings{Yang2019paws-x,
    title = "{PAWS-X}: A Cross-lingual Adversarial Dataset for Paraphrase Identification",
    author = "Yang, Yinfei  and
      Zhang, Yuan  and
      Tar, Chris  and
      Baldridge, Jason",
    booktitle = "Proceedings of EMNLP 2019",
    year = "2019",
    pages = "3685--3690",
}

@inproceedings{Rust2020HowGI,
  title={How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models},
  author={Phillip Rust and Jonas Pfeiffer and Ivan Vulic and Sebastian Ruder and Iryna Gurevych},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2020}
}

@inproceedings{DavidsonWMW17,
  author={Thomas Davidson and Dana Warmsley and Michael W. Macy and Ingmar Weber},
  title={Automated Hate Speech Detection and the Problem of Offensive Language},
  year={2017},
  cdate={1483228800000},
  pages={512-515},
  url={https://aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/view/15665},
  booktitle={ICWSM},
}


@inproceedings{joshi2020state,
  title={The State and Fate of Linguistic Diversity and Inclusion in the NLP World},
  author={Joshi, Pratik and Santy, Sebastin and Budhiraja, Amar and Bali, Kalika and Choudhury, Monojit},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={6282--6293},
  year={2020}
}
@inproceedings{rajpurkar2016squad,
  title={SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={2383--2392},
  year={2016}
}

@article{Sarkar_KhudaBukhsh_2021, title={Are Chess Discussions Racist? An Adversarial Hate Speech Data Set (Student Abstract)}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17937}, abstractNote={On June 28, 2020, while presenting a chess podcast on Grandmaster Hikaru Nakamura, Antonio Radic’s YouTube handle got blocked because it contained ``harmful and dangerous’’ content. YouTube did not give further specific reason, and the channel got reinstated within 24 hours. However, Radic speculated that given the current political situation, a referral to ``black against white’’, albeit in the context of chess, earned him this temporary ban. In this paper, via a substantial corpus of 681,995 comments, on 8,818 YouTube videos hosted by five highly popular chess-focused YouTube channels, we ask the following research question: \emph{how robust are off-the-shelf hate-speech classifiers to out-of-domain adversarial examples?} We release a data set of 1,000 annotated comments where existing hate speech classifiers misclassified benign chess discussions as hate speech. We conclude with an intriguing analogy result on racial bias with our findings pointing out to the broader challenge of color polysemy.}, number={18}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Sarkar, Rupak and KhudaBukhsh, Ashiqur R.}, year={2021}, month={May}, pages={15881-15882} }
  title = {Are Chess Discussions Racist? An Adversarial Hate Speech Data Set},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{deshpande-2022-highly,
  doi = {10.48550/ARXIV.2201.11294},
  
  url = {https://arxiv.org/abs/2201.11294},
  
  author = {Deshpande, Neha and Farris, Nicholas and Kumar, Vidhur},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Highly Generalizable Models for Multilingual Hate Speech Detection},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@InProceedings{guo-2017-on,
  title = 	 {On Calibration of Modern Neural Networks},
  author =       {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1321--1330},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/guo17a/guo17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/guo17a.html},
  abstract = 	 {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.}
}

@INPROCEEDINGS{he-et-al-2016-deep,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}
  
  @ARTICLE{lecun-et-al-1998-gradient,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}
  
  @misc{liu-et-al-2019-roberta,
  doi = {10.48550/ARXIV.1907.11692},
  
  url = {https://arxiv.org/abs/1907.11692},
  
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{aviral-sarawagi-2019-calibration,
  author    = {Aviral Kumar and
               Sunita Sarawagi},
  title     = {Calibration of Encoder Decoder Models for Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1903.00802},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.00802},
  eprinttype = {arXiv},
  eprint    = {1903.00802},
  timestamp = {Sat, 30 Mar 2019 19:27:21 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-00802.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{Zhao-et-al-2021-calibrate,
  title = 	 {Calibrate Before Use: Improving Few-shot Performance of Language Models},
  author =       {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12697--12706},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zhao21c.html},
  abstract = 	 {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model’s bias towards each answer by asking for its prediction when given a training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2’s accuracy (up to 30.0% absolute) across different choices of the prompt, while also making learning considerably more stable.}
}

@misc{pereyra-et-al-2017,
  doi = {10.48550/ARXIV.1701.06548},
  
  url = {https://arxiv.org/abs/1701.06548},
  
  author = {Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, Łukasz and Hinton, Geoffrey},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Regularizing Neural Networks by Penalizing Confident Output Distributions},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{Platt99probabilisticoutputs,
    author = {John C. Platt},
    title = {Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods},
    booktitle = {ADVANCES IN LARGE MARGIN CLASSIFIERS},
    year = {1999},
    pages = {61--74},
    publisher = {MIT Press}
}

@inproceedings{rafael-et-al-2019-when,
 author = {M\"{u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {When does label smoothing help?},
 url = {https://proceedings.neurips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{
wald2021on,
title={On Calibration and Out-of-Domain Generalization},
author={Yoav Wald and Amir Feder and Daniel Greenfeld and Uri Shalit},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=XWYJ25-yTRS}
}

@article{keung-et-al-2020-marc,
  author    = {Phillip Keung and
               Yichao Lu and
               Gy{\"{o}}rgy Szarvas and
               Noah A. Smith},
  title     = {The Multilingual Amazon Reviews Corpus},
  journal   = {CoRR},
  volume    = {abs/2010.02573},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.02573},
  eprinttype = {arXiv},
  eprint    = {2010.02573},
  timestamp = {Mon, 12 Oct 2020 17:53:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-02573.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
K2020Cross-Lingual,
title={Cross-Lingual Ability of Multilingual BERT: An Empirical Study},
author={Karthikeyan K and Zihan Wang and Stephen Mayhew and Dan Roth},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJeT3yrtDr}
}


@InProceedings{pmlr-v108-park20b,
  title = 	 {Calibrated Prediction with Covariate Shift via Unsupervised Domain Adaptation},
  author =       {Park, Sangdon and Bastani, Osbert and Weimer, James and Lee, Insup},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3219--3229},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/park20b/park20b.pdf},
  url = 	 {https://proceedings.mlr.press/v108/park20b.html},
  abstract = 	 {Reliable uncertainty estimates are an important tool for helping autonomous agents or human decision makers understand and lever-age predictive models. However, existing approaches to estimating uncertainty largely ignore the possibility of covariate shift—i.e.,where the real-world data distribution may differ from the training distribution.  As a consequence, existing algorithms can overestimate certainty, possibly yielding a false sense of confidence in the predictive model. We pro-pose an algorithm for calibrating predictions that accounts for the possibility of covariate shift, given labeled examples from the train-ing distribution and unlabeled examples from the real-world distribution. Our algorithm uses importance weighting to correct for the shift from the training to the real-world distribution. However, importance weighting relies on the training and real-world distributions to be sufficiently close. Building on ideas from domain adaptation, we additionally learn a feature map that tries to equalize these two distributions. In an empirical evaluation, we show that our proposed approach outperforms existing approaches to calibrated prediction when there is covariate shift.}
}


@article{pampari-ermon-2020-unsupervised,
  author    = {Anusri Pampari and
               Stefano Ermon},
  title     = {Unsupervised Calibration under Covariate Shift},
  journal   = {CoRR},
  volume    = {abs/2006.16405},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.16405},
  eprinttype = {arXiv},
  eprint    = {2006.16405},
  timestamp = {Thu, 02 Jul 2020 14:42:48 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-16405.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{
braverman2020calibration,
title={Calibration, Entropy Rates, and Memory in Language Models},
author={Mark Braverman and Xinyi Chen and Sham Kakade and Karthik Narasimhan and Cyril Zhang and Yi Zhang},
year={2020},
url={https://openreview.net/forum?id=B1eQcCEtDB}
}

@inproceedings{roemmele2011choice,
  title={Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning.},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={AAAI spring symposium: logical formalizations of commonsense reasoning},
  pages={90--95},
  year={2011}
}

@inproceedings{kingma-ba-2015-adam,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  cdate={1420070400000},
  url={http://arxiv.org/abs/1412.6980},
  booktitle={ICLR (Poster)},
}

@article{metz-satariano-2020-alogrithm,
author={Cade Metz and Adam Satariano},
title={An Algorithm That Grants Freedom, or Takes It Away.},
url={https://www.nytimes.com/2020/02/06/technology/predictive-algorithms-crime.html},
journal={The New York Times},
year={2020},
ISSN={0362-4331}
}

@article{cuthbertson-2021-AI,
author={Anthony Cuthbertson},
title={AI mistakes ‘black and white’ chess chat for racism},
journal={The Independent},
url={https://www.independent.co.uk/tech/ai-chess-racism-youtube-agadmator-b1804160.html},
year={2021},
ISSN={1741-9743}
}

@misc{srinivasan-et-al-2021-predicting,
  doi = {10.48550/ARXIV.2110.08875},
  
  url = {https://arxiv.org/abs/2110.08875},
  
  author = {Srinivasan, Anirudh and Sitaram, Sunayana and Ganu, Tanuja and Dandapat, Sandipan and Bali, Kalika and Choudhury, Monojit},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Predicting the Performance of Multilingual NLP Models},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{brown-etal-2020-language,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  journal   = {CoRR},
  volume    = {abs/2005.14165},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.14165},
  eprinttype = {arXiv},
  eprint    = {2005.14165},
  timestamp = {Wed, 03 Jun 2020 11:36:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhang2019paws,
  title={PAWS: Paraphrase Adversaries from Word Scrambling},
  author={Zhang, Yuan and Baldridge, Jason and He, Luheng},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={1298--1308},
  year={2019}
}



@article{thelwall2017heart,
  title={The Heart and soul of the web? Sentiment strength detection in the social web with SentiStrength},
  author={Thelwall, Mike},
  journal={Cyberemotions: Collective emotions in cyberspace},
  pages={119--134},
  year={2017},
  publisher={Springer}
}
@article{khanuja2021muril,
  title={Muril: Multilingual representations for indian languages},
  author={Khanuja, Simran and Bansal, Diksha and Mehtani, Sarvesh and Khosla, Savya and Dey, Atreyee and Gopalan, Balaji and Margam, Dilip Kumar and Aggarwal, Pooja and Nagipogu, Rajiv Teja and Dave, Shachi and others},
  journal={arXiv preprint arXiv:2103.10730},
  year={2021}
}

@article{shi-etal-2022-language,
  author    = {Freda Shi and
               Mirac Suzgun and
               Markus Freitag and
               Xuezhi Wang and
               Suraj Srivats and
               Soroush Vosoughi and
               Hyung Won Chung and
               Yi Tay and
               Sebastian Ruder and
               Denny Zhou and
               Dipanjan Das and
               Jason Wei},
  title     = {Language Models are Multilingual Chain-of-Thought Reasoners},
  journal   = {CoRR},
  volume    = {abs/2210.03057},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2210.03057},
  doi       = {10.48550/arXiv.2210.03057},
  eprinttype = {arXiv},
  eprint    = {2210.03057},
  timestamp = {Fri, 07 Oct 2022 16:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2210-03057.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{ponti-etal-2021-modelling,
  doi = {10.48550/ARXIV.2107.11353},
  
  url = {https://arxiv.org/abs/2107.11353},
  
  author = {Ponti, Edoardo Maria and Kreutzer, Julia and Vulić, Ivan and Reddy, Siva},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Modelling Latent Translations for Cross-Lingual Transfer},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{lin-etal-2022-shot,
    title = "Few-shot Learning with Multilingual Generative Language Models",
    author = "Lin, Xi Victoria  and
      Mihaylov, Todor  and
      Artetxe, Mikel  and
      Wang, Tianlu  and
      Chen, Shuohui  and
      Simig, Daniel  and
      Ott, Myle  and
      Goyal, Naman  and
      Bhosale, Shruti  and
      Du, Jingfei  and
      Pasunuru, Ramakanth  and
      Shleifer, Sam  and
      Koura, Punit Singh  and
      Chaudhary, Vishrav  and
      O{'}Horo, Brian  and
      Wang, Jeff  and
      Zettlemoyer, Luke  and
      Kozareva, Zornitsa  and
      Diab, Mona  and
      Stoyanov, Veselin  and
      Li, Xian",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.616",
    pages = "9019--9052",
    abstract = "Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4{\%} absolute accuracy improvement in 0-shot settings and +9.4{\%} in 4-shot settings) and natural language inference (+5.4{\%} in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples.",
}

@misc{Graham2022,
  author = {Neubig, Graham},
  title = {Is My NLP Model Working?},
  year="2022",
   url = {http://www.phontron.com/slides/neubig23ismynlpmodelworking.pdf},
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{blevins2022prompting,
      title={Prompting Language Models for Linguistic Structure}, 
      author={Terra Blevins and Hila Gonen and Luke Zettlemoyer},
      year={2022},
      journal={arXiv cs.CL 2211.07830},
      primaryClass={cs.CL}
}

@article{asai2023buffet,
      title={BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer}, 
      author={Akari Asai and Sneha Kudugunta and Xinyan Velocity Yu and Terra Blevins and Hila Gonen and Machel Reid and Yulia Tsvetkov and Sebastian Ruder and Hannaneh Hajishirzi},
      year={2023},
      journal={arXiv cs.CL 2305.14857},
}

@misc{lai2023chatgpt,
      title={ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning}, 
      author={Viet Dac Lai and Nghia Trung Ngo and Amir Pouran Ben Veyseh and Hieu Man and Franck Dernoncourt and Trung Bui and Thien Huu Nguyen},
      journal={arXiv cs.CL 2304.05613},
      year={2023},
}

@article{wei-etal-2021-finetuned,
  author       = {Jason Wei and
                  Maarten Bosma and
                  Vincent Y. Zhao and
                  Kelvin Guu and
                  Adams Wei Yu and
                  Brian Lester and
                  Nan Du and
                  Andrew M. Dai and
                  Quoc V. Le},
  title        = {Finetuned Language Models Are Zero-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2109.01652},
  year         = {2021},
  url          = {https://arxiv.org/abs/2109.01652},
  eprinttype    = {arXiv},
  eprint       = {2109.01652},
  timestamp    = {Mon, 20 Sep 2021 16:29:41 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-01652.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{nguyen2023incontext,
      title={In-context Example Selection with Influences}, 
      author={Tai Nguyen and Eric Wong},
      year={2023},
      journal={arXiv cs.CL 2302.11042},

}

@inproceedings{zhang-etal-2022-active,
    title = "Active Example Selection for In-Context Learning",
    author = "Zhang, Yiming  and
      Feng, Shi  and
      Tan, Chenhao",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.622",
    pages = "9134--9148",
    abstract = "With a handful of demonstration examples, large-scale language models demonstrate strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a 5.8{\%} improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.",
}

@article{nambi2023breaking,
      title={Breaking Language Barriers with a LEAP: Learning Strategies for Polyglot LLMs}, 
      author={Akshay Nambi and Vaibhav Balloli and Mercy Ranjit and Tanuja Ganu and Kabir Ahuja and Sunayana Sitaram and Kalika Bali},
      year={2023},
      journal={arXiv cs.CL 2305.17740},

}

@inproceedings{choudhury2021how,
author = {Choudhury, Monojit and Deshpande, Amit},
title = {How Linguistically Fair are Multilingual Pre-Trained Language Models?},
organization = {AAAI},
booktitle = {AAAI-21},
year = {2021},
month = {February},
abstract = {Massively multilingual pre-trained language models, such as mBERT and XLM-RoBERTa, have received significant attention in the recent NLP literature for their excellent capability towards crosslingual zero-shot transfer of NLP tasks. This is especially promising because a large number of languages have no or very little labeled data for supervised learning. Moreover, a substantially improved performance on low resource languages without any significant degradation of accuracy for high resource languages lead us to believe that these models will help attain a fairer distribution of language technologies despite the prevalent unfair and extremely skewed distribution of resources across the world’s languages.
Nevertheless, these models, and the experimental approaches adopted by the researchers to arrive at those, have been criticised by some for lacking a nuanced and thorough comparison of benefits across languages and tasks. A related and important question that has received little attention is how to choose from a set of models, when no single model significantly outperforms the others on all tasks and languages. As we discuss in this paper, this is often the case, and the choices are usually made without a clear articulation of reasons or underlying fairness assumptions. In this work, we scrutinize the choices made in previous work, and propose a few different strategies for fair and efficient model selection based on the principles of fairness in economics and social choice theory. In particular, we emphasize Rawlsian fairness, which provides an appropriate framework for making fair (with respect to languages, or tasks, or both) choices while selecting multilingual pre-trained language models for a practical or scientific set-up.},
publisher = {AAAI},
url = {https://www.microsoft.com/en-us/research/publication/how-linguistically-fair-are-multilingual-pre-trained-language-models/},
}

@article{wei2023chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      journal={arXiv 2201.11903 cs.CL},
}

@article{maxwell-etal-2021-scratchpad,
  author       = {Maxwell I. Nye and
                  Anders Johan Andreassen and
                  Guy Gur{-}Ari and
                  Henryk Michalewski and
                  Jacob Austin and
                  David Bieber and
                  David Dohan and
                  Aitor Lewkowycz and
                  Maarten Bosma and
                  David Luan and
                  Charles Sutton and
                  Augustus Odena},
  title        = {Show Your Work: Scratchpads for Intermediate Computation with Language
                  Models},
  journal      = {CoRR},
  volume       = {abs/2112.00114},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.00114},
  eprinttype    = {arXiv},
  eprint       = {2112.00114},
  timestamp    = {Fri, 29 Apr 2022 17:42:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-00114.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{shih2023long,
      title={Long Horizon Temperature Scaling}, 
      author={Andy Shih and Dorsa Sadigh and Stefano Ermon},
      year={2023},
      eprint={2302.03686},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{schick2023toolformer,
      title={Toolformer: Language Models Can Teach Themselves to Use Tools}, 
      author={Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
      year={2023},
      eprint={2302.04761},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}




@inproceedings{wang-etal-2022-super,
    title = "Super-{N}atural{I}nstructions: Generalization via Declarative Instructions on 1600+ {NLP} Tasks",
    author = "Wang, Yizhong  and
      Mishra, Swaroop  and
      Alipoormolabashi, Pegah  and
      Kordi, Yeganeh  and
      Mirzaei, Amirreza  and
      Naik, Atharva  and
      Ashok, Arjun  and
      Dhanasekaran, Arut Selvan  and
      Arunkumar, Anjana  and
      Stap, David  and
      Pathak, Eshaan  and
      Karamanolakis, Giannis  and
      Lai, Haizhi  and
      Purohit, Ishan  and
      Mondal, Ishani  and
      Anderson, Jacob  and
      Kuznia, Kirby  and
      Doshi, Krima  and
      Pal, Kuntal Kumar  and
      Patel, Maitreya  and
      Moradshahi, Mehrad  and
      Parmar, Mihir  and
      Purohit, Mirali  and
      Varshney, Neeraj  and
      Kaza, Phani Rohitha  and
      Verma, Pulkit  and
      Puri, Ravsehaj Singh  and
      Karia, Rushang  and
      Doshi, Savan  and
      Sampat, Shailaja Keyur  and
      Mishra, Siddhartha  and
      Reddy A, Sujan  and
      Patro, Sumanta  and
      Dixit, Tanay  and
      Shen, Xudong",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.340",
    pages = "5085--5109",
    abstract = "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions{---}training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9{\%} on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.",
}

@article{sainz-etal-2023-chatgpt,
author={ Sainz, Oscar and Campos, Jon Ander and García-Ferrero, Iker and Etxaniz, Julen and Agirre, Eneko},
title={Did ChatGPT cheat on your test?},
url={https://hitz-zentroa.github.io/lm-contamination/blog/},
year={2023},
}

@inproceedings{blevins-zettlemoyer-2022-language,
    title = "Language Contamination Helps Explains the Cross-lingual Capabilities of {E}nglish Pretrained Models",
    author = "Blevins, Terra  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.233",
    pages = "3563--3574",
    abstract = "English pretrained language models, which make up the backbone of many modern NLP systems, require huge amounts of unlabeled training data. These models are generally presented as being trained only on English text but have been found to transfer surprisingly well to other languages. We investigate this phenomenon and find that common English pretraining corpora actually contain significant amounts of non-English text: even when less than 1{\%} of data is not English (well within the error rate of strong language classifiers), this leads to hundreds of millions of foreign language tokens in large-scale datasets. We then demonstrate that even these small percentages of non-English data facilitate cross-lingual transfer for models trained on them, with target language performance strongly correlated to the amount of in-language data seen during pretraining. In light of these findings, we argue that no model is truly monolingual when pretrained at scale, which should be considered when evaluating cross-lingual transfer.",
}

@misc{emre2023causal,
      title={Causal Reasoning and Large Language Models: Opening a New Frontier for Causality}, 
      author={Emre Kıcıman and Robert Ness and Amit Sharma and Chenhao Tan},
      year={2023},
      eprint={2305.00050},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{kosinski2023theory,
      title={Theory of Mind May Have Spontaneously Emerged in Large Language Models}, 
      author={Michal Kosinski},
      year={2023},
      eprint={2302.02083},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{srivastava2023imitation,
      title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, 
      author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmüller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakaş and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bartłomiej Bojanowski and Batuhan Özyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and César Ferri Ramírez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Moseguí González and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Martínez-Plumed and Francesca Happé and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-López and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Schütze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fernández Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Kocoń and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jörg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Colón and Luke Metz and Lütfi Kerem Şenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ramírez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and Mátyás Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Michał Swędrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Miłkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphaël Millière and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Théo Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
      year={2023},
      eprint={2206.04615},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{yeDurrett2022Explanations,
 author = {Ye, Xi and Durrett, Greg},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {30378--30392},
 publisher = {Curran Associates, Inc.},
 title = {The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c402501846f9fe03e2cac015b3f0e6b1-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{jigsaw-multilingual-toxic-comment-classification,
    author = {Kivlichan, Ian and Sorensen, Jeffrey and Elliott, Julia and Vasserman, Lucy and Görner, Martin and Culliton, Phil},
    title = {Jigsaw Multilingual Toxic Comment Classification},
    publisher = {Kaggle},
    year = {2020},
    url = {https://kaggle.com/competitions/jigsaw-multilingual-toxic-comment-classification}
}