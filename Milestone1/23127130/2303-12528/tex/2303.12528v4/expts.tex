\section{MEGA}

In this section, we discuss different components of our benchmarking exercise to measure the multilingual capabilities of LLMs. We start by discussing different NLP tasks and datasets that we evaluate these models on, along with their linguistic diversity. We provide an overview of the models we evaluate, baselines for comparison, and describe our evaluation scheme and prompting strategies.

% In this section, we describe how we adapt various NLP tasks to the in-context learning setting. We describe the prompting strategies we use for the benchmark and the models, tasks and datasets included in our study.
% \vspace{-2mm}

\subsection{Datasets and Languages}

We broadly consider five families of NLP tasks in our experiments covering 16 different datasets:

\noindent \textbf{Classification Tasks.} Here, we further have four different sub-tasks, i) \textit{Natural Language Inference} (classify if a hypothesis is entailed in the premise, contradicts it or neither), which includes XNLI \cite{Conneau2018xnli} , Indic-XNLI \cite{aggarwal2022indicxnli} (version of XNLI translated to 11 Indian languages), and GLUECos NLI\cite{khanuja2020gluecos} for English-Hindi code-mixed data; ii) \textit{Commonsense Reasoning} datasets including causal commonsense reasoning benchmark XCOPA \cite{ponti2020xcopa} and XStoryCloze \cite{lin-etal-2022-shot}, where the correct ending of a story with four sentences is to be predicted; iii) \textit{Paraphrase Identification} task PAWS-X \cite{Yang2019paws-x}, where given two sentences, the model must predict if the two have the same meaning; iv)  EN-ES-CS dataset for \textit{Sentiment Analysis} on English-Spanish code-mixed tweets.

\noindent \textbf{Question Answering (QA).} For QA we consider \textit{Span-Prediction} tasks, where the answer to a question is to be predicted within a piece of context provided. We evaluate on XQuAD \cite{artetxe2020cross}, MLQA \cite{lewis2020mlqa}, TyDiQA-GoldP \cite{clark2020tydi}, and IndicQA \cite{doddapaneni2022indicxtreme}.

\noindent \textbf{Sequence Labeling.} This task involves classifying each token in a piece of text and we consider \textit{Named Entity Recognition} dataset PAN-X \cite{pan2017cross} (also called WikiANN) and UDPOS \cite{nivre2018universal} for \textit{Part of Speech Tagging}.

\noindent \textbf{Natural Language Generation (NLG).} For NLG we consider the multilingual \textit{Abstractive Summarization} dataset XL-Sum.

\noindent \textbf{Responsible AI (RAI).} We consider the multilingual \textit{Toxicity Prediction} dataset Jigsaw\cite{jigsaw-multilingual-toxic-comment-classification}, and Wino-MT to measure \textit{Gender Bias} in MT systems.

All the datasets with the number of languages they include are listed in Figure \ref{fig:mega_tasks}. These 16 datasets encompass a total of 70 languages covering 21 different language families, with Indo-Aryan and Afro-Asiatic languages in the majority (see Figure \ref{fig:mega_lang_fam}). Note that for tasks with $>30$ languages i.e. UDPOS, PAN-X, and XL-Sum, we run evaluations on the first 1000 examples of the test sets. For tasks where no public test sets are available (like XQUAD, TyDiQA-GoldP, and IndicQA), we evaluate on validation data. 
% Few-shot examples are chosen from the training data, except for datasets with no language-specific training data, where we use validation data.
Refer to Appendix \textsection \ref{sec:datasets_all} for a detailed description of all the datasets.
% \vspace{-2mm}

\subsection{Models}
\label{sec:models}

\noindent \textbf{OpenAI Models.} We conduct all benchmarking experiments on the GPT-3.5 models \dvthree{}  (denoted as \dvthreeinf{} in the paper) and \turbo{} \cite{ouyang2022training} (\turboinf{}) as well on the GPT-4 model \gptfour{} \cite{gpt4techreport}. The \texttt{text-davinci-003} model has a maximum context size of 4096 tokens, while \texttt{gpt-3.5-turbo} and \texttt{gpt-4-32k} support context sizes of 16k and 32k respectively. 
% In the paper we use \dvthreeinf{}, \turboinf{}, and  \gptfourinf{} to denote \dvthree{}, \turbo{}, and \gptfour{} respectively.
% We do not conduct any fine-tuning of the model for our benchmarking experiments or carry out hyperparameter tuning for temperature or other settings. 
% We use the suffix \texttt{TT} to indicate if the models are evaluated using the translate-test prompting strategy (e.g. \texttt{text-davinci-003} (TT)), and by default, the model names without any suffix denote the model evaluated using monolingual prompting strategy \footnote{There are some exceptions to this. For XQUAD and IndicQA there is no available non-English training data and we can't use the validation dataset as it is used for evaluation. Hence, we report numbers with zero-shot cross-lingual prompting strategy for these two datasets by default.}. We use Azure OpenAI services to access the APIs.

\noindent \textbf{Baselines.} We compare the performance of OpenAI models with two classes of baselines, i) \textit{Prompt-Based baselines}, which like the OpenAI models are evaluated by prompting the model directly for solving a task, and ii) \textit{Fine-tuned Baselines}, which are fine-tuned on task-specific training data. For the former we consider BLOOMZ \cite{muennighoff2022crosslingual}, a multi-task fine-tuned version of the BLOOM \cite{scao2022bloom} model, which is a 176 billion parameter model trained on 46 natural languages and 13 programming languages. For fine-tuned baselines, we consider TULRv6 \cite{patra2022beyond} (the current SoTA on XTREME benchmark), XLMR \cite{conneau2020unsupervised}, multilingual BERT \cite{bert2019}, and mT5 \cite{xue2021mt5}. For Indic-datasets we also compare with MuRIL\cite{khanuja2021muril}, a multilingual BERT model trained on 16 Indic languages that obtains SOTA performance on many Indic benchmarks. All of these models (excluding mT5 for the XL-Sum and XCOPA), were fine-tuned with English data and then evaluated in a zero-cross-lingual fashion on other target languages.
% Generative models are known to be few-shot learners \cite{brown2020language}, meaning that they are capable of learning through in-context few-shot examples (which we provide in our experiments) and do not need to be fine-tuned on language-specific or task-specific data. Since we foresee generative AI models replacing many of the task-specific fine-tuned models in the future, we believe it is important to do a comprehensive multilingual evaluation comparing all of these models. 
% \vspace{-3mm}
\subsection{Evaluation Methodology}

LLMs exhibit two remarkable properties that make them effective at solving a variety of NLP tasks. The first is in-context learning \cite{brown-etal-2020-language}, where the model learns to solve a task through the few input-output examples provided as part of the context without any weight updates. Secondly, the ability to follow instructions \cite{mishra-etal-2022-cross, wei-etal-2021-finetuned, ouyang2022training} which is a property of instruction-tuned LLMs, where the models can be prompted to solve new-tasks based on the textual instructions provided in context.  

We adopt these two techniques together to test the capabilities of LLMs to solve a variety of tasks in different languages. We define five main components to define the prompts: i) a \textbf{test example} $\xtest$ for which the predictions are to be made; ii) $k$ \textbf{few-shot exemplars} $\{(x_i, y_i)\}_{i = 1}^{k}$, that are used to provide in-context supervision to the model; iii) a \textbf{task instruction} $\inst$ which describes the instruction in text for the task to LLM; iv) a \textbf{prompt template} $\temp(x)$ which turns a dataset input example into a text format that can be used for prompting; and v) an \textbf{answer verbalizer} $\verbz(y)$ that maps the label $y$ to a textual representation. In our evaluation framework we often consider the instruction, template, and verbalizer as a single entity, and from now on will denote the template to encapsulate the three unless specified separately. 
 
 % Some examples of $f_{temp}$, $\mathcal{I}$ and $f_{verb}$ are given in Table \ref{tab:prompt_template}.


Given these components, the final prompt $\prompt(\xtest; \{(x_i, y_i)\}_{i = 1}^{K}, \inst, \temp, \verbz)$ or $\prompt(\xtest)$ for short for a test input $\xtest$ can be defined as:
% \vspace{-3mm}
\begin{align*}
\prompt(\xtest) =\inst \mathbin\Vert_{i = 1}^{K} \big\{\temp(x_i)&\mathbin\Vert \verbz(y_i)\big\}\\
&\mathbin\Vert \temp(\xtest)
\end{align*}
where $\mathbin\Vert$ denotes the string concatenation operator. 
% An example prompt obtained is provided in Figure \ref{fig:megamain} (\textit{bottom-right}). 
The prompt can then be provided as input to the LLM $P(.;\theta)$ to obtain the prediction $\ztest=\argmax_{z \in \mathcal{Z}} P(z | \prompt(\xtest); \theta)$, where $\mathcal{Z}$ is the space of possible answers, which in all of our experiments is taken to be the entirety of the language as modeled by the LLM. We approximate the $\argmax$ by sampling from the probability distribution predicted by the LLM.

% The predicted answer $\ztest$ is compared with the verbalized label using $\metric(\ztest, \verbz(\ytest)) \in [0, 1]$ that measures the extent of similarity between the ground truth and predicted answer. For our experiments, we use the exact-match score to determine accuracy for classification tasks, use the exact-match and F1-score for QA tasks, F1-Score for Sequence Labeling, and ROUGE-L for generation tasks. 
% Formally, the evaluation score $s$ for an LLM $P(.;\theta)$ on a task $\mathcal{T}$ can be defined as: $s = {\E_{(\xtest, \ytest) \in \mathcal{T}}}[\metric(\ztest, \verbz(\ytest))]$


\subsubsection{Multilingual Prompting Strategies}
\label{sec:prompt_strategies}
The choice of prompt significantly influences the performance of LLMs and these models have been shown to be brittle to simple prompting variations, such as the choice of prompt template and the training examples or even the ordering of examples \cite{Zhao-et-al-2021-calibrate}. For multilingual setups as highlighted in \citet{lin-etal-2022-shot} and \citet{shi-etal-2022-language}, some additional variations to consider include, {the choice of the language of the few-shot examples}, {the language of the prompt template}, and {the language of the test examples}.

In this work, we evaluate models using three types of prompting strategies: \textbf{\mono{} Prompting}: In this setup, the $k$ randomly selected examples are of the same language as the test examples.  \textbf{\zscl{}}: Here, we evaluate generative models' zero-shot cross-lingual transfer ability during in-context learning. We use $k$-shot examples from a pivot language (always English in our experiments) which is different from the language of the test example. \textbf{\ttest{}}: In this setup also, the few-shot examples are sampled from English data. However, the test example itself is modified by translating it into English.   We use Bing Translator to translate the test examples into English. We do not perform evaluations with \ttest{} prompting for QA and Sequence Labelling tasks where there is no trivial alignment between the labels in the translated text with native language text. To preserve costs, for GPT-4 we only run evaluations with the Monolingual prompting strategy except for a couple of datasets, which we explicitly discuss later in \textsection \ref{sec:results}. Irrespective of the prompting strategy, we use the prompt templates written in English (see Appendix \textsection  \ref{sec:challenges_full} for the impact of this choice).


% Translate-test has been shown to be often better than cross-lingual transfer for both fine-tuning \cite{ponti-etal-2021-modelling} and in-context learning \cite{shi-etal-2022-language}, hence we explore its effectiveness for our benchmarking as well.
% We also consider different choices for the language of prompt templates: \textbf{English-Templates}, where the prompt templates are written in English, irrespective of the language of the few-shot and test examples;
% % As has been shown in \citet{shi-etal-2022-language}, English instructions can often perform on par or even better than providing them in the native language; 
% \textbf{Native-Language-Templates}, where the prompt templates are written in the language of the test example $(\xtest, \ytest)$. For our experiments, we use Bing Translator to translate the prompts from English to the native language. In our initial experiments, we found that English-Templates often perform much better (consistent with \citet{muennighoff2022crosslingual}; see Table \ref{tab:en_v_translated_prompts}). Hence, unless specified explicitly, the main results are obtained using the English-Template setting.
\paragraph{Prompt Tuning.} 
% For each task that we consider in our benchmarking study, we need to come up with optimal prompts to appropriately measure the capabilities of these models. 
We use PromptSource \cite{bach-etal-2022-promptsource} for a database of existing prompts to use for our experiments. In order to select the best prompt for a dataset (to appropriately measure the capabilities of these models), we evaluate the performance of available English templates on PromptSource on the English validation set and select the prompt that gives the best performance. This prompt template is then used to evaluate models on the test sets for all languages and prompt strategies. While it would be ideal to tune the prompts separately for each language, the scale of our experiments and the computational costs of these models make it prohibitive. We investigate the impact this choice has on our results in \textsection \ref{sec:choices}. We perform separate prompt-tuning for \dvthreeinf{} and \turboinf{} models, and to keep the costs in check, we use the prompts obtained for the latter for \gptfourinf{} as well. Final prompts selected are included in Appendix \textsection \ref{sec:appendix_propmts}.

\paragraph{Choice of Few-Shot Examples.}  In all our experiments, we choose few-shot examples randomly from the training or validation set (depending on what's available) of a dataset. For most datasets, we use 8 few-shot examples, excluding tasks with longer contexts like QA and summarization tasks where we use $k = 4$.

% It is possible that more sophisticated choices of few-shot \cite{nguyen2023incontext, liu-etal-2022-makes} examples for the tasks can lead to better performance, however, we leave an in-depth study of the same for future work.


% We also translate the selected English prompt to the corresponding target language using the Bing translator, and we present the results of these experiments on select tasks in Table \ref{tab:en_v_translated_prompts}. Using the English template outperforms translating to the native language in all four classification tasks. We observe that translation errors often propagate in the form of incorrect syntax and semantics in the prompts, which may influence task performance negatively. To avoid this, we recommend that all automatically translated prompt templates should be verified by a native speaker.


% For the choice of language for the prompt template, we consider the following two setups:
% \noindent
% \begin{itemize}
%     \item \textbf{English-Template}: Here the prompt templates are written in English, irrespective of the language of the few-shot and test examples. As has been shown in \citet{shi-etal-2022-language}, English instructions can often perform on par or even better than providing them in the native language. 
%     \item \textbf{Native-Language-Template}: Here the prompt templates are written in the language of the test example $(x_{test}, y_{test})$. For our experiments, we use Bing Translator to translate the prompts from English to the native language.
% \end{itemize}
% In our initial experiments, we used different languages for task templates by automatically translating templates but found that it performs poorly due to translation errors changing the instructions leading to a loss in accuracy. We find English templates to perform better than native language prompts in most cases (see Table \ref{tab:en_v_translated_prompts}). Hence, unless specified explicitly, the main results are obtained using the English-Template setting.

% \iffalse
% \begin{table}[]
%     \centering
%     \begin{tabular}{p{2.5cm}p{2cm}p{2cm}}
%         \toprule
%          &  \textbf{English-Template} & \textbf{Native-Language-Template}\\
%          \midrule
%          Monolingual & \checkmark & \checkmark\\
%          Zero-Shot Cross-Lingual & \checkmark & \xmark \\
%          Translate-Test & \checkmark & \xmark\\
%          \bottomrule
%     \end{tabular}
%     \caption{Combinations of the two higher level prompting setups.}
%     \label{tab:prompting_setups}
% \end{table}

% Table \ref{tab:prompting_setups} provides the possible combinations of the two classes of prompting strategies. We only allow Native-Language templates for Monolingual setup, as in the other two setups the few-shot examples are in English. In our experiments, we tried using different language templates for few-shot examples and the test example but found that it performs poorly, as it would often lead to the model being confused about which language to generate the predictions in.
% \fi

% We believe that it is important to compare the performance of generative models to other multilingual models for the following reasons: multilingual models such as TULR are typically fine-tuned in a zero-shot cross-lingual manner, meaning that they are not fine-tuned on multilingual task-specific data, giving them no language-specific advantage over generative models. 



% IndicQA \cite{doddapaneni2022indicxtreme} is a cloze-style reading comprehension dataset with context taken from Wikipedia articles on Indian culture and history, manually created in 11 Indic languages. 


% the few-shot examples are selected to belong to the same language as the test example to be evaluated. An example of the same for Hindi is shown in Figure \ref{fig:monoprompting}.

% Figure \ref{fig:zsprompting} illustrates the zero-shot prompting technique, in which the few-shot examples are in English, while the test example is in the target language. In this case, the model learns how to perform the task using few-shot examples in English and generates a response for the task in the target language.

% As shown in Figure \ref{fig:monoprompting}, in the monolingual prompting strategy, the entire prompt is in the target language, with the few-shot examples also coming from the target language. 

% The choice of prompt can greatly influence the performance of generative models, and models have been shown in the past to be brittle to prompting variations such as the words used in the prompt, number of few-shot examples, ordering of examples etc CITE. We used four prompting strategies for all our experiments - translate-test, zero-shot prompting, cross-lingual translated prompting and monolingual prompting. To illustrate the differences between the prompting methods, we use the following terminology: the \textit{instructions} part of the prompt contains the instructions on how to perform the task, along with \textit{few-shot examples}. The \textit{test example}
 % part of the prompt contains the data point for which we want the response from the model. We used the Bing Translator to perform the automatic translation in all our experiments. 

% \subsubsection{Translate-test}

% As shown in Figure \ref{fig:translate_test}, the Translate-test setting translates test example into English and uses the English instructions along with few-shot examples from English data.

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=18cm]{figures/translate_test.jpg}
%     \caption{Translate-test}
%     \label{fig:translate_test}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \begin{subfigure}[t]{0.32\textwidth}
%     \includegraphics[width=0.95\textwidth]{figures/monolingual_prompting.jpg}
%     \caption{Monolingual Prompting}
%     \label{fig:monoprompting}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.32\textwidth}
%     \includegraphics[width=0.95\textwidth]{figures/zero_shot_prompting.jpg}
%     \caption{Zero-Shot Cross-Lingual Prompting}
%     \label{fig:zsprompting}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.32\textwidth}
%     \includegraphics[width=0.95\textwidth]{figures/translate_test.jpg}
%     \caption{Translate-Test Prompting:}
%     \label{fig:translate_test}
%     \end{subfigure}
% \end{figure*}


% \subsubsection{Zero-shot prompting}

% Figure \ref{fig:zsprompting} illustrates the zero-shot prompting technique, in which the few-shot examples are in English, while the test example is in the target language. In this case, the model learns how to perform the task using few-shot examples in English and generates a response for the task in the target language.

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=18cm]{figures/zero_shot_prompting.jpg}
%     \caption{Zero-shot prompting}
%     \label{fig:zsprompting}
% \end{figure*}

% \subsubsection{Monolingual translated prompting}

 

% \subsubsection{Monolingual prompting}

% As shown in Figure \ref{fig:monoprompting}, in the monolingual prompting strategy, the entire prompt is in the target language, with the few-shot examples also coming from the target language. 

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=18cm]{figures/monolingual_prompting.jpg}
%     \caption{Monolingual prompting}
%     \label{fig:monoprompting}
% \end{figure*}


% \subsection{Few-shot examples}

% In all our experiments, we choose few-shot examples randomly from the development set available in the dataset, unless specified. Better choices of few-shot examples for the tasks can lead to higher performance, which we leave for future work.

% how we chose the few shot examples and the different design choices that can be made here

% Issues with prompt length - cannot stuff more few shot examples in some languages. just mention here and can go into more detail in the tokenizer discussion

% \subsection{Choice of prompts}

% % Task-specific choice of prompts

% % how we went about choosing the prompt for each task, using promptsource, optimizing for english, can add the potential drawbacks of doing so here or in discussion.

% For each task that we consider in our benchmarking study, we need to come up with a prompt that specifies the instruction that the model should follow. We use PromptScource\cite{bach-etal-2022-promptsource} from the BigScience community to use the existing prompts or to create new prompts for the tasks \footnote{Hosted version: \url{https://huggingface.co/spaces/bigscience/promptsource}}. PromptSource is a toolkit for creating, sharing, and using natural language prompts. Prompts are saved in standalone structured files and written in a simple templating language called Jinja. 

% For all datasets, we evaluate the performance of available English prompts on PromptSource on the English validation set and select the prompt that gives the best performance. This prompt is then used to evaluate the entire test sets for all languages and prompt strategies as described in \ref{sec:prompt_strategies}. 
% % 
%  Prompts for all of the datasets are included in Appendix \textsection \ref{sec:appendix_propmts}.  There is a possibility that the best prompt for English is not necessarily the best prompt for other or all languages. However, given the computational overhead of prompt selection for each language for these models, we leave this for future work.

% We also translate the selected English prompt to the corresponding target language using the Bing translator, and we present the results of these experiments on select tasks in Table \ref{tab:en_v_translated_prompts}. Using the English template outperforms translating to the native language in all four classification tasks. We observe that translation errors often propagate in the form of incorrect syntax and semantics in the prompts, which may influence task performance negatively. To avoid this, we recommend that all automatically translated prompt templates should be verified by a native speaker.

% \myworries{ToDo: Discuss drawbacks of English finetuning here?}

% \begin{table*}[t!]
% \centering
% \resizebox{\textwidth}{!}{%
% \setlength{\tabcolsep}{4pt}
% \begin{tabular}{c | l | c}
% \toprule
% Dataset(s) & Template $f_{temp}$ & Verbalizer $f_{verb}$ \\ \midrule
% \multirow{3}{*}{NLI} & \texttt{\{premise\}} Based on previous passage,   & \textit{Entailment}: Yes  \\
% & is it true that \texttt{\{hypothesis\}} ?& \textit{Contradiction}: No\\
% & Yes, No, or Maybe? & \textit{Neutral}: Maybe \\
% \midrule
% \multirow{3}{*}{PAWS-X}                                         & Sentence 1: \texttt{\{sentence1\}} Sentence 2: \texttt{\{sentence2\}} & \textit{Positive}: Yes \\
% & Question: Does Sentence 1 paraphrase Sentence 2 ? &  \textit{Negative}: No\\
% & Yes or No? &\\
% \midrule
% \multirow{5}{*}{XCOPA}     &
% \texttt{\{ premise \}} &        \\ 
% & \texttt{\{\% if question == "cause" \%\}} This happened  because & \texttt{\{choice1\}}: choice1 \\
% &\texttt{\{\% else \%\}} As a consequence... \texttt{\{\% endif \%\}} & \texttt{\{choice2\}}: choice2  \\
% &  Help  me pick the more plausible option: &   \\
% & choice1: \texttt{\{choice1\}}, choice2: \texttt{\{choice2\}} & \\
% \midrule
% \multirow{3}{*}{QA} & \texttt{\{context\}}  & \\
% & Q: \texttt{\{question\}} & Identity \\
% & Referring to the passage above, the correct answer to the given question is& \\
%   \bottomrule
% \end{tabular}}
% \caption{Prompt type and prompt used for each dataset.}
% \label{tab:promptsource}
% \end{table*}

% \begin{table*}[]
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}llll@{}}
% \toprule
% Dataset(s) &
%   \multicolumn{2}{c}{Template  $f_{temp}$}&
%   Verbalizer $f_{verb}$\\
%  &
%   System &
%   User &
%    \\ \midrule

%    \begin{tabular}[c]{@{}l@{}}XNLI, IndicXNLI,\\  GLUECoS NLI\end{tabular}
%  &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to solve Natural Language Inference (NLI) problems. \\ NLI is the task of determining the inference relation between two (short, ordered) texts: entailment, \\ contradiction, or neutral.  Answer as concisely as possible in the same format as the examples below:\end{tabular} &
%   \texttt{\{premise\}} Question: \texttt{\{hypothesis\}} True, False, or Neither? &
%   \begin{tabular}[c]{@{}l@{}}Entailment : True, \\ Contradiction: False,\\ Neutral: Neither\end{tabular} \\
% PAWS-X &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to perform Paraphrase Identification. The goal of \\ Paraphrase Identification is to  determine whether a pair of sentences have the same meaning. \\ Answer as concisely as possible in the same format as the examples below:\end{tabular} &
%   \{sentence1\} Question: \{sentence2\} True or False? &
%   - \\
% XCOPA &
%   \begin{tabular}[c]{@{}l@{}}You are an AI assistant whose purpose is to perform open-domain commonsense causal reasoning. \\ You will be provided a premise and two alternatives, where the task is to select the alternative that more \\ plausibly has a causal relation with the premise. Answer as concisely as possible in the same format as\\  the examples below:\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}\{ premise \} \\ \{\% if question == "cause" \%\} This happened because... \\ \{\% else \%\} As a consequence... \{\% endif \%\} \\ Help me pick the more plausible option: - \{choice1\} - \{choice2\}\end{tabular} &
%   - \\
% \begin{tabular}[c]{@{}l@{}}XQUAD, TyDiQA,\\ MLQA, IndicQA\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to solve reading comprehension problems. \\ You will be provided questions on a set of passages and you will need to provide the answer as it\\  appears in the passage. The answer should be in the same language as the question and the passage.\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}\{context\} Q: \{question\}Referring to the passage above, \\ the correct answer to the given question is: \{answer\}\end{tabular} &
%   - \\
% XStoryCloze &
%   - &
%   \begin{tabular}[c]{@{}l@{}}\{input\_sentence\_1\} \{input\_sentence\_2\} \{input\_sentence\_3\} \{input\_sentence\_4\}\\ What is a possible continuation for the story given the following options ?\\ Option1: \{sentence\_quiz1\}\textbackslash{}n-Option2: \{sentence\_quiz2\}\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}\{sentence\_quiz1\}: Option1, \\ \{sentence\_quiz2\}: Option2\end{tabular} \\
% PANX &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to perform Named Entity Recognition (NER). \\ NER involves identifying and classifying named entities in a text into predefined categories such as \\ person names, organizations, locations, and others. You will need to use the tags defined below:\\ \textbackslash{}nO means the word doesn’t correspond to any entity.\textbackslash{}nB-PER/I-PER means the word corresponds \\ to the beginning of/is inside a person entity.\textbackslash{}nB-ORG/I-ORG means the word corresponds to the \\ beginning of/is inside an organization entity.\textbackslash{}nB-LOC/I-LOC means the word corresponds to the\\  beginning of/is inside a location entity.\textbackslash{}nDo not try to answer the question! Just tag each token in the sentence.\end{tabular} &
%   \{token\_1 token\_2 ... token\_n\} &
%   \begin{tabular}[c]{@{}l@{}}\{tag\_1\} \{tag\_2\} ... \{tag\_n\}: \\ \{token\_1\}\_\{tag\_1\} \{token\_2\}\_\{tag\_2\}\\  ... \{token\_n\}\_\{tag\_n\}\end{tabular} \\
% UDPOS &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to perform Part of Speech (PoS) Tagging. PoS tagging is the \\ process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on\\  both its definition and its context. You will need to use the tags defined below:\end{tabular} &
%   \{token\_1 token\_2 ... token\_n\} &
%   \begin{tabular}[c]{@{}l@{}}\{tag\_1\} \{tag\_2\} ... \{tag\_n\}: \\ \{token\_1\}\_\{tag\_1\} \{token\_2\}\_\{tag\_2\} ... \\ \{token\_n\}\_\{tag\_n\}\end{tabular} \\

% \begin{tabular}[c]{@{}l@{}}GLUECoS\\ Sentiment Analysis\end{tabular}&

%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to solve Sentiment Analysis problems. Sentiment Analysis \\ is the task of determining whether the sentiment, opinion or emotion expressed in a textual data is: \\ positive, negative, or neutral. Answer as concisely as possible in the same format as the examples below:\end{tabular} &
%   Does the following sentence have a positive, negative or neutral sentiment? \{text\} &
%   - \\
% XLSum &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to summarize any given article. You should summarize all \\ important information concisely in the same language in which you have been provided the document. \\ Following the examples provided below:\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}\{document\} \\ === \\ Write a summary of the text above :\end{tabular} &
%   - \\
% Jigsaw & \begin{tabular}[c]{@{}l@{}} You are an NLP assistant whose purpose is to solve the Jigsaw Toxicity Prompt problem. Toxicity is defined \\ as anything rude, disrespectful or otherwise likely to make someone leave a discussion. The objective is to \\ identify toxic comments. Answer as concisely as possible in the same format as the examples below: \end{tabular}
%    & \begin{tabular}[c]{@{}l@{}} Should this online comment be removed for its toxicity \{answer\_choices[1]\} \\ or \{answer\_choices[0]\}? \end{tabular}
%    &
%    \\
% WinoMT & -
%    & Translate the following English text to \{target\_language\}: \{sentence\}
%    & -
%    \\ \bottomrule
% \end{tabular}%
% }
% \caption{Prompt type and prompt used for each task.}
% \label{tab:prompt_template}
% \end{table*}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \begin{table*}[]
% \renewcommand{\arraystretch}{1.3}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}llll@{}}
% \toprule
% Dataset(s) & Instruction $\mathcal{I}$ & Template $f_{temp}$ & Verbalizer $f_{verb}$ \\
%    \\ \midrule
% \begin{tabular}[c]{@{}l@{}}XNLI, IndicXNLI, \\ GLUECoS NLI\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to solve Natural Language Inference (NLI) problems. \\ NLI is the task of determining the inference relation between two (short, ordered) texts: entailment, \\ contradiction, or neutral.  Answer as concisely as possible in the same format as the examples below:\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}} \texttt{\{premise\}}\\ Question: \texttt{\{hypothesis\}} \\ True, False, or Neither?\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}Entailment : True, \\ Contradiction: False,\\ Neutral: Neither\end{tabular} \\ \midrule
% % PAWS-X &
% %   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to perform Paraphrase Identification. The goal of \\ Paraphrase Identification is to  determine whether a pair of sentences have the same meaning. \\ Answer as concisely as possible in the same format as the examples below:\end{tabular} &
% %   \begin{tabular}[c]{@{}l@{}} \texttt{\{sentence1\}} \\ Question: \texttt{\{sentence2\}} \\ True or False?\end{tabular} &
% %   - \\ \midrule
% % XCOPA &
% %   \begin{tabular}[c]{@{}l@{}}You are an AI assistant whose purpose is to perform open-domain commonsense causal reasoning. \\ You will be provided a premise and two alternatives, where the task is to select the alternative that more \\ plausibly has a causal relation with the premise. Answer as concisely as possible in the same format as\\  the examples below:\end{tabular} &
% %   \begin{tabular}[c]{@{}l@{}} \texttt{\{ premise \}} \\ \texttt{\{\% if question == "cause" \%\}} This happened because... \\ \texttt{\{\% else \%\}} As a consequence... \texttt{\{\% endif \%\}} \\ Help me pick the more plausible option: - \texttt{\{choice1\}} - \texttt{\{choice2\}} \end{tabular} &
% %   - \\ \midrule
% \begin{tabular}[c]{@{}l@{}}XQUAD, TyDiQA,\\ MLQA\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to solve reading comprehension problems. \\ You will be provided questions on a set of passages and you will need to provide the answer as it\\  appears in the passage. The answer should be in the same language as the question and the passage.\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}} \texttt{\{context\}} \\ Q: \texttt{\{question\}}\\ Referring to the passage above,  the correct answer to the given question is: \texttt{\{answer\}}\end{tabular} &
%   \textit{Identity} \\ \midrule
% % XStoryCloze &
% %   - &
% %   \begin{tabular}[c]{@{}l@{}}\texttt{\{input\_sentence\_1\} \{input\_sentence\_2\} \{input\_sentence\_3\} \{input\_sentence\_4\}}\\ What is a possible continuation for the story given the following options ?\\ Option1: \texttt{\{sentence\_quiz1\}} Option2: \texttt{\{sentence\_quiz2\}}\end{tabular} &
% %   \begin{tabular}[c]{@{}l@{}}\texttt{\{sentence\_quiz1\}}: Option1, \\ \texttt{\{sentence\_quiz2\}}: Option2\end{tabular} \\ \midrule
% % PANX &
% %   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to perform Named Entity Recognition (NER). \\ NER involves identifying and classifying named entities in a text into predefined categories such as \\ person names, organizations, locations, and others. You will need to use the tags defined below:\\ O means the word doesn’t correspond to any entity. B-PER/I-PER means the word corresponds \\ to the beginning of/is inside a person entity. B-ORG/I-ORG means the word corresponds to the \\ beginning of/is inside an organization entity. B-LOC/I-LOC means the word corresponds to the\\  beginning of/is inside a location entity. Do not try to answer the question! Just tag each token in the sentence.\end{tabular} &
% %   \texttt{\{token\_1 token\_2 ... token\_n\}} &
% %   \begin{tabular}[c]{@{}l@{}}\texttt{\{tag\_1\} \{tag\_2\} ... \{tag\_n\}}: \\ \texttt{\{token\_1\}\_\{tag\_1\} \{token\_2\}\_\{tag\_2\}}\\  ... \texttt{\{token\_n\}\_\{tag\_n\}}\end{tabular} \\ \midrule
% % UDPOS &
% %   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to perform Part of Speech (PoS) Tagging. PoS tagging is the \\ process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on\\  both its definition and its context. You will need to use the tags defined below:\end{tabular} &
% %   \texttt{\{token\_1 token\_2 ... token\_n\}} &
% %   \begin{tabular}[c]{@{}l@{}}\texttt{\{tag\_1\} \{tag\_2\} ... \{tag\_n\}}: \\ \texttt{\{token\_1\}\_\{tag\_1\} \{token\_2\}\_\{tag\_2\} ...} \\ \texttt{\{token\_n\}\_\{tag\_n\}}\end{tabular} \\ \midrule
% % \begin{tabular}[c]{@{}l@{}}GLUECoS\\ Sentiment Analysis\end{tabular} &
% %   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to solve Sentiment Analysis problems. Sentiment Analysis \\ is the task of determining whether the sentiment, opinion or emotion expressed in a textual data is: \\ positive, negative, or neutral. Answer as concisely as possible in the same format as the examples below:\end{tabular} &
% %   Does the following sentence have a positive, negative or neutral sentiment? \texttt{\{text\}} &
% %   - \\ \midrule
% XLSum &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to summarize any given article. You should summarize all \\ important information concisely in the same language in which you have been provided the document. \\ Following the examples provided below:\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}\texttt{\{document\}} \\ === \\ Write a summary of the text above :\end{tabular} &
%   \textit{Identity} \\ \midrule
% Jigsaw & \begin{tabular}[c]{@{}l@{}} You are an NLP assistant whose purpose is to solve the Jigsaw Toxicity Prompt problem. Toxicity is defined \\ as anything rude, disrespectful or otherwise likely to make someone leave a discussion. The objective is to \\ identify toxic comments. Answer as concisely as possible in the same format as the examples below: \end{tabular}
%   \textit{Identity} &
%   \begin{tabular}[c]{@{}l@{}}Should this online comment be removed for its toxicity? \\ Yes or No?\end{tabular} &
%   Toxic: Yes, Non-Toxic: No \\ \bottomrule
% % WinoMT &
% %   - &
% %   Translate the following English text to \texttt{\{target\_language\}}: \texttt{\{sentence\}} &
%   % - \\ \bottomrule
% \end{tabular}%
% }
% \caption{Prompt template, instruction, and the verbalizers used for a set of representative tasks for evaluating GPT-3.5-Turbo and GPT-4. For details on all the prompts that we use for our evaluation, please refer to \textsection \ref{sec:appendix_propmts} in Appendix.}
% \label{tab:prompt_template}
% \end{table*}