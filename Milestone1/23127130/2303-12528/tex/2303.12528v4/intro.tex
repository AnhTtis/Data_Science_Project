\section{Introduction}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/mega_main_v2.png}
%     \caption{An overview of our benchmarking exercise: Multilingual Evaluation of Generative AI (MEGA).} 
%     %\textbf{Top:} We evaluate on 16 different multilingual datasets spanning 70 languages. \textbf{Bottom Left}: The 70 languages in our study cover 21 different language families. \textbf{Bottom Right}: An example of instructions plus in-context learning approach that we use to evaluate LLMs.}
%     \vspace{-5mm}
%     \label{fig:megamain}
% \end{figure}

\begin{figure*}
    \centering
    \begin{subfigure}[c]{0.35\textwidth}
    \includegraphics[width=0.9\textwidth]{figures/mega_tasks.png}
    \caption{Tasks and Datasets included in MEGA.}
    \label{fig:mega_tasks}%
    \end{subfigure}
    \begin{subfigure}[c]{0.3\textwidth}
    \includegraphics[width=0.9\textwidth]{figures/fam_dist.png}
    \caption{Language Family\\ Distribution}
    \label{fig:mega_lang_fam}
    \end{subfigure}%
    \begin{subfigure}[c]{0.33\textwidth}
    \includegraphics[width=0.9\textwidth]{figures/mega_icl_ex.png}
    \caption{Example of\\ multilingual prompting}
    \label{fig:mega_icl_ex}
    \end{subfigure}
    \caption{An overview of our benchmarking exercise: Multilingual Evaluation of Generative AI (MEGA). Numbers in parentheses in Figure \ref{fig:mega_tasks} contain the number of languages supported in the dataset.}
    \label{fig:megamain}
\end{figure*}

% \begin{figure*}
%     \centering
%     \begin{subfigure}[t]{0.32\textwidth}
%     \includegraphics[width=0.95\textwidth]{figures/lang_comp_gpt.pdf}
%     \caption{}
%     \label{fig:langdistrgpt}
%     \end{subfigure}%
%     \begin{subfigure}[t]{0.32\textwidth}
%     \includegraphics[width=0.95\textwidth]{figures/lang_comp_bloom.pdf}
%     \caption{}
%     \label{fig:langdistrbloom}
%     \end{subfigure}%
%     \begin{subfigure}[t]{0.32\textwidth}
%     \includegraphics[width=0.95\textwidth]{figures/lang_comp_palm.pdf}
%     \caption{}
%     \label{fig:langdistrpalm}
%     \end{subfigure}
%     \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
% \end{figure*}


Large Large Models (LLMs) such as ChatGPT and GPT-4 have created a lot of interest in the AI community and beyond, due to the step jump in their capabilities, such as maintaining context over conversations, fluency of generation, and reasoning. Many users have reported having tested these systems on languages other than English, with varying results, and recent demos of these models \cite{newbingevent} have been shown in multiple (albeit high-resource) languages. Recently, the GPT-4 model \cite{gpt4techreport} was evaluated on the MMLU multiple choice questions benchmark by automatically translating it into 26 languages, and the results for some low-resource languages in the Latin script were found to be quite promising. 

The multilingual capabilities of these models can be traced to their pre-training data, where even the predominantly English large-scale corpora contain hundreds of millions of non-Engish tokens \cite{blevins-zettlemoyer-2022-language}. For GPT-3 unlabeled pre-training data has been documented to contain 119 languages \cite{brown-etal-2020-language}, where roughly ~93\% of the tokens are in English\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}. Other LLMs like BLOOM \cite{scao2022bloom} and PaLM \cite{chowdhery2022palm} have a better multilingual representation with 60\% and 18\% non-English data respectively for pre-training. While these models have been trained on multiple languages with varying distributions in the pre-training data, it is not clear how well they perform relative to each other across diverse tasks and languages due to a lack of comprehensive analysis across all models with the same experimental setup. 
% Further, the datasets used for evaluation are often obtained by machine translation of English test sets, leading to a shortage of evaluation results on more natural multilingual data.

% , while the BLOOM \cite{scao2022bloom} model created by the BigScience community is trained on 46 natural languages, and Google's PaLM \cite{chowdhery2022palm} model is trained on 122 languages, with the latest PaLM 2 model \cite{palm2techreport} being trained on larger multilingual corpora and parallel data. While these models have been trained on multiple languages with varying distributions in the pre-training data, it is not clear how well they perform relative to each other across diverse tasks and languages due to a lack of comprehensive analysis across all models with the same experimental setup. Further, the datasets used for evaluation are often obtained by machine translation of English test sets, leading to a shortage of evaluation results on more natural multilingual data.

% Figures \ref{fig:langdistrgpt}, \ref{fig:langdistrbloom} and \ref{fig:langdistrpalm} show the distribution of languages in the pre-training data of GPT3, BLOOM and PaLM respectively. We observe that a significant proportion of the data is in the English language for all three models, with even the BLOOM model, which aims to be a multilingual model, having around 40\% data in English and code. It is unclear how much data is present in the tail languages in these models, and more crucially, what the quality of the data is, which makes it difficult to ascertain whether these models are ready to be deployed for these languages. \citet{joshi2020state} reviewed the challenges involved in promoting linguistic diversity, such as the lack of resources and infrastructure for developing NLP tools for underrepresented languages. They provided an overview of the efforts being made to promote linguistic diversity and inclusion including the creation of data sets and tools for underrepresented language and provide a classification of languages based on the amount of labeled and unlabeled data available for languages. This study finds that most of the world's population is under-served in terms of availability of data for their languages, raising important questions about what the advent of generative Large Language Models (LLMs) means for the state of language coverage and inclusivity. 




% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=12cm]{figures/Life-Architect-GPT-3-90-languages.png}
%     \caption{Language Distribution in LLMs - GPT3}
%     \label{fig:langdistrgpt}
% \end{figure*}

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=12cm]{figures/Life-Architect-BLOOM-46-languages.png}
%     \caption{Language Distribution in LLMs - BLOOM}
%     \label{fig:langdistrbloom}
% \end{figure*}

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=12cm]{figures/Life-Architect-PaLM-122-languages.png}
%     \caption{Language Distribution in LLMs - PALM}
%     \label{fig:langdistrpalm}
% \end{figure*}

% Robust and comprehensive evaluation helps us in understanding the capabilities of AI systems. 
Recently, there has been a lot of interest in evaluating the different capabilities of LLMs, with comprehensive studies like HELM \cite{liang2022holistic} that evaluate these models on a wide variety of capabilities.
% like Reasoning, Question Answering, Efficiency, and Toxicity. 
However, such studies are largely performed on English language data and there is a lack of such large-scale evaluation of LLMs for their multilingual capabilities. Given the current pace at which new language technologies are being developed that use LLMs, the importance of such an evaluation cannot be understated as the cases of inequalities in the performance of previous-generation models across languages have been well-documented \cite{blasi-etal-2022-systematic}.

In our work, we present the first large-scale Multilingual Evaluation of Generative AI models (MEGA), spanning 16 different datasets, 
$70$ topologically diverse languages, and four LLMs i.e. GPT-3.5 models \dvthree{} and \turbo{}, GPT-4 (\gptfour{}) and BLOOMZ \cite{muennighoff2022crosslingual}. We also compare these models with the models fine-tuned on these datasets like TULRv6 \cite{patra2022beyond} and MuRIL \cite{khanuja2021muril}, which are SoTA on different multilingual benchmarks. 
% In addition, we explore strategies to prompt these models with multilingual data.

Through our evaluation, we aim to answer three research questions. \textit{(1)}, how well do LLMs fare on multilingual benchmarks compared to fine-tuned SOTA models? \textit{(2)}, what languages do these models perform well in, and can we explain the trends in performance for these models across languages? \textit{(3)}, what prompting strategies should be used for using LLMs for non-English languages? 
% And \textit{(4)}, what are the challenges we foresee in making generative AI work fairly across all languages of the world?

Our study highlights that there is a significant disparity between the performance of LLMs in English vs non-English languages, especially low-resource languages with non-Latin scripts for which fine-tuned models perform significantly better. While GPT-4 bridges this gap to some extent, the discrepancy still exists. Further, we find that for these languages it is often difficult to do better than simply machine translating the input in a target language to English and then sending it to the LLM for prediction (\textit{translate-test}). We also discuss how different prompt-design choices like prompt-tuning, use of explanations, and number of few-shot examples impact multilingual performance. Finally, we perform some initial analysis to the test the possibility of test data contamination in LLMs that we evaluate and discuss its implications on our findings. Our work provides a blueprint for strategies that can be used for building systems using generative AI for multilingual users. We also release our code \footnote{\url{https://aka.ms/MEGA}} for the community to scale up the multilingual evaluation of generative models.

% Another contribution of this work is the MEGA benchmarking framework that can be used for the evaluation of a model or system across languages, and for the research community to scale up multilingual evaluation of generative models that come in the future. We plan to release the MEGA benchmarking code to facilitate this.

% the translate-test prompting strategy, i.e. where the input in a target language is machine translated to English and then sent 

% \begin{itemize}
%     \item How well do generative models understand instructions, perform tasks and generate text in different languages?
%     \item Which languages do generative models perform well in?
%     \item How well do generative models such as GPT3, GPT4 and BLOOM fare on standard multilingual benchmarks compared to SOTA models, such as Turing ULR on standard NLP tasks? 
%     \item  Which prompting strategies should be used for using generative LLMs for non-English languages?
%     \item  What are the challenges we foresee in making generative AI work accurately for all languages of the world? 
% \end{itemize}


% Evaluation of LLMs has been an active area of research, with benchmarks such as GLUE \cite{wang2018glue} and SuperGLUE \cite{wang2019superglue} proposed a few years ago for English, followed by more recent benchmarks for multilingual evaluation, such as XTREME \cite{hu2020xtreme}, XTREME-R \cite{ruder2021xtreme} and XGLUE \cite{liang2020xglue}. Multilingual benchmarks aim to cover a variety of tasks and languages, so that we can determine whether a foundation model is able to generalize. Benchmarks covering specific language families, such as IndicXTREME \cite{doddapaneni2022indicxtreme} for Indian languages, \citet{adelani-etal-2022-masakhaner} for African Languages,  \citet{wilie2020indonlu} for Indonesian languages and GLUECoS \cite{khanuja2020gluecos} and LinCE \cite{aguilar2020lince} for code-switching have also been proposed.

% However, evaluating generative AI models is challenging due to limitations in automatic overlap based metrics such as BLEU and ROUGE and the open-ended nature of conversations that can be had with such systems \cite{Graham2022}. 
% Prior work has described the challenges of scaling up multilingual evaluation \cite{ahuja-etal-2022-beyond} due to the lack of representative datasets covering languages from different language families, typological diversity of languages in benchmark datasets and coverage of low-resource languages. 
% We have also pointed out the limitations of using translated datasets for evaluation (such as the one that has been used to evaluate GPT4 in the technical report), due to the loss of local and cultural context in such benchmarks \cite{ahuja-etal-2022-beyond, ahuja2022economics}.
% However, it is crucial that we improve multilingual evaluation of LLMs, because prior work shows that good performance or gains on some high resource languages may not always result in similar gains in all languages \cite{wu2020all}, with various factors possibly influencing the performance of models on different languages \cite{litmus}.

% Recently, there have been attempts at holistic evaluation of generative AI models for English (HELM) \cite{liang2022holistic}, in which several LLMs are evaluated on multiple dimensions such as accuracy, calibration, robustness, fairness, bias etc. \citet{bang2023multitask} conduct an evaluation of ChatGPT across tasks and languages. They evaluate ChatGPT on Sentiment Analysis, Language Identification and Machine Translation and find that it fails to identify low-resource languages, and generate non-Latin script languages. \citet{hendy2023good} evaluate the translation abilities of three GPT models - ChatGPT, GPT3.5 (text-davinci-003) and text-davinci-002 and find that GPT models perform well on translating high-resource languages, and do not perform well on low-resource languages. Recent work \cite{bubeck2023sparks}, has pointed out the limitations of using standard NLP benchmarks to evaluate generative models, due to the pace at which these benchmarks become saturated. However, in this work, we show that multilingual NLP benchmarks are far from saturated, and hence, benchmarking is a viable approach to get a sense of the performance of such models in different languages. Concurrent work BUFFET \cite{asai2023buffet} and \citet{lai2023chatgpt} also perform multilingual benchmarking of large language models, however, they evaluate the performance of ChatGPT and BLOOMZ in their work while our evaluation also spans GPT-4 and GPT-3.5 models.

% In this work, we carry out a comprehensive Multilingual Evaluation of Generative AI (MEGA). We quantify how well generative LLMs perform across languages across standard multilingual benchmarks covering various standard NLP tasks such as Natural Language Inference, Commonsense Reasoning, Question Answering, Summarization, and Sequence Labelling. Our goal is to conduct a comprehensive benchmarking study for many languages of the world, along the lines of HELM \cite{liang2022holistic}, and we plan to benchmark other dimensions of interest such as calibration, bias, robustness etc. in future versions of the study. This is very challenging due to the scarcity of datasets available in non-English languages for measuring fairness, toxicity and bias as we discuss in prior work \cite{ramesh2023fairness}, however it is important to evaluate all these dimensions across languages to ensure that we build responsible and accurate systems for all. 

% We aim to answer the following questions: 

% \begin{itemize}
%     \item How well do generative models understand instructions, perform tasks and generate text in different languages?
%     \item Which languages do generative models perform well in?
%     \item How well do generative models such as GPT3, GPT4 and BLOOM fare on standard multilingual benchmarks compared to SOTA models, such as Turing ULR on standard NLP tasks? 
%     \item  Which prompting strategies should be used for using generative LLMs for non-English languages?
%     \item  What are the challenges we foresee in making generative AI work accurately for all languages of the world? 
% \end{itemize} 

% We evaluate the following OpenAI models: \texttt{text-davinci-003} (referred to as DV003 in the paper), \texttt{gpt-3.5-turbo} (referred to as GPT-3.5-Turbo in the paper), and \texttt{gpt-4-32k} (referred to as GPT-4 in the paper) on 16 NLP datasets across $\sim70$ languages, shown in Table \ref{tab:datasets} and compare its performance to BLOOMZ \cite{muennighoff2022crosslingual} and SOTA models for specific tasks, such as TULRv6 \cite{patra2022beyond} and MuRIL \cite{khanuja2021muril}. In addition, we compare various prompting strategies, including the translate-test setting (round-tripping through English by translating into English, querying in English and back-translating if applicable).

% Our work is the first comprehensive benchmarking of generative AI models across models, datasets, languages and prompting strategies. In addition to answering the questions we pose above, our study also results in a blueprint of strategies that can be used for building systems using generative AI for multilingual users. Another contribution of this work is the MEGA benchmarking framework that can be used for evaluation of a model or system across languages, and for the research community to scale up multilingual evaluation of generative models that come in the future. We plan to release the MEGA benchmarking code to facilitate this.