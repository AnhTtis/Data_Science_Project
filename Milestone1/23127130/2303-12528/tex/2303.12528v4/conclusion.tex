\section{Conclusion}
In this work, we conduct an evaluation across different prompting strategies, models, tasks, and languages to investigate the multilingual capabilities of LLMs. We also investigate underlying properties like tokenizer quality and size of pretraining data to explain the trends in performance that we observe. Our investigation shows the consistent performance gap between high-resource, Latin script, and under-resourced languages in addition to highlighting the efficacy, yet limited sufficiency of methods like translate-test prompting. Through our evaluation, we present evidence of the need to prioritize automatic benchmarking and human evaluation across as many languages as possible.  We hope that this work spurs research in meeting this goal. 


% Hence, there is a need to scale up multilingual prompt generation with humans-in-the-loop for extracting the best performance from generative models.


% We find that generative models perform better on higher-resource languages and languages that are in the Latin script. We discuss the quality of  tokenization in GPT as one of the potential reasons for this gap.

% In this work, we show that generative LLMs perform worse across tasks and languages, when compared to SOTA models, even when the input is translated to English and used as the query to the system. We find that generative models perform better on higher-resource languages and languages that are in the Latin script. We discuss the quality of  tokenization in GPT as one of the potential reasons for this gap.

% We also show that the choice of prompting strategy matters - the monolingual prompting setup outperforms the cross-lingual prompting strategy. We may be able to get higher performance from generative models by tuning prompts further. However, tuning prompts for each language and task is challenging to scale and translating prompts automatically can lead to lower performance, which may explain the poor performance of generative models compared to SOTA models in the monolingual setting.

% While translate-test is currently the best strategy across languages for generative models such as DV003, it is important to consider the implications of translation carefully as discussed in this paper.


