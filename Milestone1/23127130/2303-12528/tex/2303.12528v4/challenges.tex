
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.2\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/analysis/XNLI_icl.pdf}
    \caption{}
    \label{fig:xnli_icl}
    \end{subfigure}
    \begin{subfigure}[t]{0.2\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/analysis/XCOPA_icl.pdf}
    \caption{}
    \label{fig:xcopa_icl}
    \end{subfigure}%
    
    \begin{subfigure}[t]{0.2\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/analysis/xcopa_prompt_tuning.pdf}
    \caption{}
    \label{fig:xcopa_ptune}
    \end{subfigure}
    \begin{subfigure}[t]{0.2\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/analysis/xstorycloze_expl.pdf}
    \caption{}
    \label{fig:xstory_expl}
    \end{subfigure}
    % \begin{subfigure}[t]{0.95\textwidth}
    \caption{Analysing the effect on GPT-3.5-Turbo's performance given different evaluation factors. To obtain explanations we use Super-Natural Instructions \cite{wang-etal-2022-super}.\vspace{-3mm}}
    \label{fig:analysis_sum}
\end{figure}
\section{Challenges in Multilingual Evaluation}
\label{sec:challenges}

In this section, we examine some of the challenges and consequent limitations of a large-scale multilingual evaluation like ours.
% Particularly, we enlist a few of the design decisions we take during our evaluation and investigate how that impacts the observations that we make. 

% We mainly discuss different decisions that have to be made to evaluate LLMs for their multilingual capabilities and their impact on the observations that we make. We also briefly touch upon the possibility of the test dataset contamination in the training data of these models that we evaluate.

\subsection{A Kaleidoscope of Choices.}
\label{sec:choices}
% \subsection{Evaluation Parameters: A Kaleidoscope of Choices}
% \vspace{-2mm}

There are various moving parts when evaluating LLMs using prompting-based approaches, including the choice of prompt templates, instructions, and few-shot examples \cite{liu-etal-2022-makes, lu-etal-2022-fantastically, Zhao-et-al-2021-calibrate}, different prompting strategies \cite{wei2023chainofthought, maxwell-etal-2021-scratchpad, ye-durrett-2022-explanations}, using external tools \cite{schick2023toolformer}, the language of prompts  \cite{shi-etal-2022-language, lin-etal-2022-shot},  as well as different decoding specific hyper-parameters \cite{shih2023long}, which can have varying degrees of impact on the performance, sometimes in unexpected ways. 
% When dealing with multilingual data, the choice of the language of templates, instructions, explanations, and few-shot examples adds another level of choices to be made for evaluations \cite{shi-etal-2022-language, lin-etal-2022-shot}.
Holistically exploring these choices for all the datasets and languages can quickly get out of hand, especially given the excessive computational cost of running these models. In order to understand the sensitivity of our observations to the choices we make in \textsection \ref{sec:results}, we re-evaluate our setups on a subset of datasets and languages for a varying set of parameters. Our findings are summarized in Figure \ref{fig:analysis_sum}, where we see that having a large few-shot size generally helps improve performance, however, the performance is often stable beyond $k = 8$. Further, language-specific fine-tuning can help improve the performance like we see for Haitian Creole in XCOPA, but for Tamil we actually observe the accuracy to go down which might be attributed to the small size of the validation set (100 in the case of XCOPA). Finally, on XStoryCloze dataset (also for XCOPA), we see using explanations to prompt the models have negligible impact on the performance. Overall, these experiments indicate that the existing prompting approaches might not be sufficient to address the performance gap that exists for non-English languages (especially mid-to-low resource languages) and there is an imminent need to propose new methods as well as improve the representation of different languages in these model's pre-training (and instruction-tuning) data.
 % and check their impact on our original observations in 



% \noindent \textbf{Effect of number of in-context examples $k$.}
% Our main experiments were conducted with $k = 8$ or $k = 4$, depending on the task. Here, we evaluate what effect different numbers of in-context examples have on XNLI and XCOPA for three languages in Figures \ref{fig:xnli_icl} and \ref{fig:xcopa_icl}. We observe while the performance increases sharply while moving from 0 to 2-4 examples, it is fairly stable after $k \geq 8$, with the exception of Haitian Creole in XCOPA, where it continues to improve.

% \noindent \textbf{Effect of language-specific prompt tuning.}
% As discussed in \textsection \ref{sec:prompt_strategies}, we use English validation data for prompt selection in each dataset that we use for all languages. Here, we explore whether separately tuning the prompts for each language helps. For XNLI, we run this experiment on Urdu and Swahili, tuning over ten different prompt templates from Prompt-Source, but find that the same prompt that was tuned for English gets picked up for these two languages as well. For XCOPA however, different prompts are chosen when tuned on Haitian Creole and Tamil. This leads to an improvement in the test performance for Haitian Creole (from 72\% to 75.6\%, see Figure \ref{fig:xcopa_ptune}). Interestingly for Tamil, we see the test performance actually drops slightly compared to the accuracy obtained with prompt selected on English data, which we conjecture might be due to the fact that the validation sets in XCOPA have only 100 examples that may not be sufficient for selecting optimal prompts.

% \noindent \textbf{Effect of Explanations.} \citet{yeDurrett2022Explanations}, showed for \texttt{text-davinci-002}, that prompting the model with explanations before the outputs (Explain-then-Predict) in the in-context examples can help improve few-shot performance substantially on English language datasets. Hence, here we evaluate if they help improve the multilingual performance of the \turboinf{} model as well. We perform experiments on XStoryCloze and XCOPA datasets and use the explanations available in Super-NaturalInstructions (SNI)\cite{wang-etal-2022-super}\footnote{At the time of writing this paper, XStoryCloze wasn't included in SNI, hence we use the few-shot examples and explanations available for StoryCloze dataset\cite{mostafazadeh-etal-2016-corpus}, making the prompting setup \zscl{}.}. All the explanations that we used were written in English. For XStoryCloze, the results are plotted in Figure \ref{fig:xstory_expl}, and we observe that while there is a slight gain upon using explanations for Telugu, for all other languages the performance remains largely unchanged if not slightly worse. Interestingly, upon manual inspection of the model's prediction, we observe that the model often first translates the problem to English and then proceeds with the explanation, without having prompted to do so. We have similar observations for the XCOPA dataset as well, where adding explanations doesn't help improve performance and ends up hurting the performance by a slight margin \kabir{Add plot in appendix and refer here.}.

% Overall, these experiments indicate that the existing prompting approaches might not be sufficient to address the performance gap that exists for non-English languages (especially mid-to-low resource languages) and there is an imminent need to propose new methods as well as improve the representation of different languages in these model's pre-training (or even instruction-tuning) data as well in their tokenizers. 
% \vspace{-2mm}
\subsection{Test data contamination}
\label{sec:contam}
Given the massive amount of online data that LLMs are trained with, it is critical to factor in the possibility of contamination of test datasets \cite{sainz-etal-2023-chatgpt}. Accordingly, we attempt to verify if the performances we observed are in fact, representative of the capabilities of these models or merely a result of memorization. Given the lack of transparency in the training distribution of recent models like GPT-4, we perform some preliminary investigations against this phenomenon. Specifically, we consider three factors: i) LLM's knowledge of the dataset, ii) availability of test datasets on the internet, and iii) dataset release date.


% LLMs are trained with trillions of tokens encompassing the entirety of text data available on the internet as well on human-generated data and feedback during the instruction fine-tuning stage. The possibility of test data contamination during the model training is hard to ignore. Hence, it is important to verify if the observed performance for these models from our evaluation represents the capabilities of these models or is merely a result of memorization. However, the lack of transparency for the recent models (ChatGPT and GPT-4), makes it very hard to assess the degree of contamination.% We perform some preliminary checks to gauge some indications of this phenomenon in our benchmarking exercise by considering three factors: i) LLM's knowledge of the dataset, ii) The availability of test datasets on the internet, and iii) the Dataset release date.


To measure the LLM's (we do this for GPT-4) memory of the dataset, we prompt it to fill the dataset cards for each of MEGA's  datasets (denoted as Card Fill). This involves filling templatic information like the task's supported languages, input-output structure and description. If the model fills a dataset card correctly (Full), we note this as suspicion of contamination. If it fills the card partially correct (Partial) i.e. detecting either the correct task structure or correct set of languages, we mark it as partial evidence, and if it succeeds in neither, we mark it as no suspicion (None). 
 % To measure the LLM's (we do this for GPT-4) understanding of the dataset, we prompt it to fill the dataset cards for each of the datasets that we have in MEGA (denoted as Card Fill). These dataset cards involve filling out things such as the supported languages, input-output structure of the task, and description. If the model fills a dataset card correctly (Full), we note this as suspicion of contamination. If it fills the card partially correct (Partial) i.e. detecting either the correct task structure or correct set of languages, we mark it as partial evidence, and if it succeeds in neither, we mark it as no suspicion (None). 
 For test dataset availability, we check if the test dataset can be accessed online directly without downloading either as part of the official release from the authors or via other sources such as Hugging Face dataset viewer (Data Acc. w/o Down.). For release date, we check if the dataset was made public after the cut-off date of September 2021.

The overall results from this analysis are provided in Table \ref{tab:contam}. We see that for a majority of datasets, GPT-4 can fill in the dataset card correctly; On the more recent datasets like XLSum and XStoryCloze it is only partially successful, while on Jigsaw and code-mixing datasets it fails to correctly fill the cards. Note that except XStoryCloze, Jigsaw and the Code-mixing datasets, evaluation sets for all other datasets are directly accessible online. Collectively, this connotes that for tasks like XStoryCloze and IndicQA there is a weak suspicion against contamination. While all other tasks are highly likely contaminated (except Jigsaw, and Code-Mixed datasets). 




% We also tried prompting the LLMs to generate the dataset splits following \citet{sainz-etal-2023-chatgpt}, but had limited success (we document our efforts for the same in Appendix \kabir{add in appendix and refer here.})


 
 % The overall results from this analysis are provided in Table \ref{tab:contam}. We see for a majority of datasets GPT-4 can fill in the dataset cards correctly, on the more recent datasets like XLSum and XStoryCloze it is only partially successful, while on Jigsaw and code-mixing datasets it fails to correctly fill the cards. Also, excluding XStoryCloze, Jigsaw and Code-mixing datasets, evaluation sets for all other datasets can be accessed online directly. Overall, these three metrics suggest the weakest suspicion for test set contamination for XStoryCloze, mild for IndicQA, Jigsaw, and Code-Mixed datasets, while for the remaining ones, there remains a high possibility of the same. We also tried prompting the LLMs to generate the dataset splits following \citet{sainz-etal-2023-chatgpt}, but had limited success (we document our efforts for the same in Appendix \kabir{add in appendix and refer here.})
 %  Following \citet{sainz-etal-2023-chatgpt}, we prompt \turboinf{} and \gptfourinf{} to generate the examples from the train, validation, and test splits for all the datasets in their corresponding supported languages. We observe that the two models are often able to generate correct-looking examples from these datasets, but the generations were not found to be part of any of the dataset splits (we document our efforts for the same in Appendix \kabir{add in appendix and refer here.}). However, we do not use this as evidence to rule out the possibility of contamination. 

% Card Fill: For each of the datasets we ask the GPT-4 and GPT-3.5-Turbo, to fill the dataset cards (Dataset Cards (huggingface.co)), which include things like languages present in the dataset, structure of the dataset i.e. different type of features and labels that are there, and description of the task. If the model fills the dataset card directly, we mark it as red, if it predicts the correct sets of languages but the wrong structure or otherwise then we mark it as yellow, and if it doesn't do both then it is marked as green (which never happens for GPT-4).
% Data Acc w/o Down. : This is short for test dataset accessible without downloads (as released by the authors). We mark it as green if it is not accessible without downloading and as red if it is.
% Release Date: Here we check if the dataset was released before (red) or after (green) September 2021, which is the cutoff date that Chat-GPT and GPT-4 says when we prompt it to answer that. I have also kept XL-Sum's release date which is June 2021 as yellow instead of red, since it is close to the supposed cut-off date but I am not entirely sure to keep that or just mark it as red


% GPT4 shows improved performance across most languages and tasks in our study and it is not known whether this is because it has already been exposed to the datasets we use for benchmarking. In future versions of this study, we plan to conduct experiments to check for data contamination. An important direction for future research is to create multilingual benchmarks (which are already few in number) that cannot be accessed by crawling the web.

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lccl}
    \toprule
    Dataset & Card Fill & Data Acc. w/o Down. & Release Date \\
    \midrule
    XNLI & \cellcolor{red!25}Full & \cellcolor{red!25} Yes & \cellcolor{red!25} September 2019  \\
    Indic-XNLI & \cellcolor{red!25}Full & \cellcolor{red!25} Yes &  \cellcolor{green!25} April 2022 \\
    PAWS-X & \cellcolor{red!25}Full  & \cellcolor{red!25} Yes & \cellcolor{red!25} August 2019 \\
    XCOPA &  \cellcolor{yellow!25}Partial & \cellcolor{red!25} Yes & \cellcolor{red!25} April 2020 \\
    XStoryCloze &  \cellcolor{yellow!25}Partial &  \cellcolor{green!25} No & \cellcolor{green!25} May 2023\\
    XQuAD & \cellcolor{red!25}Full & \cellcolor{red!25} Yes & \cellcolor{red!25} October 2019 \\
    MLQA & \cellcolor{red!25}Full & \cellcolor{red!25} Yes & \cellcolor{red!25} October 2019 \\
    TyDiQA-GoldP & \cellcolor{red!25}Full & \cellcolor{red!25} Yes & \cellcolor{red!25} February 2020 \\
    IndicQA &  \cellcolor{yellow!25}Partial & \cellcolor{red!25} Yes & \cellcolor{green!25} September 2022 \\
    PAN-X & \cellcolor{red!25}Full & \cellcolor{red!25} Yes & \cellcolor{red!25} July 2017 \\
    UDPOS &  \cellcolor{red!25}Full & \cellcolor{red!25} Yes & \cellcolor{red!25} March 2020 \\
    XLSum & \cellcolor{yellow!25}Partial & \cellcolor{red!25} Yes & \cellcolor{red!25} June 2021 \\
    Jigsaw & \cellcolor{green!25} None & \cellcolor{green!25} No & \cellcolor{red!25}{February 2020}\\
    GLUECos NLI & \cellcolor{green!25} None & \cellcolor{green!25} No & \cellcolor{red!25}{June 2020}\\
    EN-ES-CS & \cellcolor{green!25} None & \cellcolor{green!25} No & \cellcolor{red!25}{May 2016}\\
    \bottomrule
    \end{tabular}}
    % \vspace{-3mm}
    \caption{Contamination analysis for the datasets that we consider in MEGA. 
    % Card Fill denotes whether the model could fill the dataset card for a given dataset. Data Acc. w/o Down. refers to if the evaluation datasets as released by the authors are available to access online without download. For the release date, we check if it was after September 2021. 
    We use {red color} when there is a strong suspicion of contamination based on these three metrics, {green} for no suspicion, and {yellow} for partial.}
    % \vspace{-3mm}
    \label{tab:contam}
\end{table}

\paragraph{Implications.} 
% Given the lack of transparency about the training details of the LLMs that we evaluate, finding concrete evidence of memorization or data contamination becomes very difficult \footnote{We also tried prompting LLMs to generate the dataset splits following \citet{sainz-etal-2023-chatgpt}, but had limited success}. What we have in our work are some indicators like the modelâ€™s understanding of the task structure and accessibility and release dates of the test datasets, 
Our analysis implies a notable chance of the test data appearing in the training datasets of these LLMs. The contamination of test datasets is a serious problem for works centered around LLM evaluation (including ours), as they might lead to an overestimation of the capabilities of these models. However, we would like to highlight that despite the possibility of contamination, LLMs still vastly underperform on (especially low-resource) non-English languages . These observations about data contamination indicate that the disparity in performance between English and non-English languages might be even greater than what we observe in our work.  