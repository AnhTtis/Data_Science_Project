\clearpage
\section{Appendix}
\label{sec:appendix}

\subsection{Prompts}
\label{sec:appendix_propmts}
\subsubsection{XNLI, IndicXNLI, GLUECoS NLI}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$:
You are an NLP assistant whose purpose is to solve Natural Language Inference (NLI) problems.  NLI is the task of determining the inference relation between two (short, ordered) texts: entailment, contradiction, or neutral.  Answer as concisely as possible in the same format as the examples below:\\[5pt]
\noindent Template $f_{temp}$: \\
    \texttt{\{premise\}}\\ Question: \texttt{\{hypothesis\}} \\ True, False, or Neither?\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
Entailment : True, \\ Contradiction: False,\\ Neutral: Neither

\paragraph{Models}: DV003\\[5pt]
\noindent Template $f_{temp}$: \\
\texttt{\{premise\}} Based on previous passage is it true that \texttt{\{hypothesis\}} ? Yes, No, or Maybe?\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
Entailment : Yes, \\ Contradiction: No,\\ Neutral: Maybe

\subsubsection{PAWS-X}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to perform Paraphrase Identification. The goal of Paraphrase Identification is to  determine whether a pair of sentences have the same meaning. Answer as concisely as possible in the same format as the examples below:\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{sentence1\}} \\ Question: \texttt{\{sentence2\}} \\ True or False?

\paragraph{Models}: DV003 \\[5pt]
\noindent Template $f_{temp}$:\\
Sentence 1: \texttt{\{sentence1\}} Sentence 2: \texttt{\{sentence2\}} Question: Does Sentence 1 paraphrase Sentence 2 ? Yes or No?\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
\textit{Positive}: Yes \\ \textit{Negative}: No

\subsubsection{XCOPA}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an AI assistant whose purpose is to perform open-domain commonsense causal reasoning. You will be provided a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. Answer as concisely as possible in the same format as the examples below:\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{ premise \}} \\ \texttt{\{\% if question == ``cause" \%\}} This happened because... \\ \texttt{\{\% else \%\}} As a consequence... \texttt{\{\% endif \%\}} \\ Help me pick the more plausible option: - \texttt{\{choice1\}} - \texttt{\{choice2\}}

\paragraph{Models}: DV003 \\[5pt]
\noindent Template $f_{temp}$:\\
\texttt{\{ premise \}} \\ \texttt{\{\% if question == ``cause" \%\}} This happened because... \\ \texttt{\{\% else \%\}} As a consequence... \texttt{\{\% endif \%\}} \\ Help me pick the more plausible option: - choice1: \texttt{\{choice1\}}, choice2: \texttt{\{choice2\}}\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
choice1: \texttt{\{choice1\}} \\ choice2: \texttt{\{choice2\}}

\subsubsection{XQUAD, TyDiQA, MLQA}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to solve reading comprehension problems. You will be provided questions on a set of passages and you will need to provide the answer as it appears in the passage. The answer should be in the same language as the question and the passage.\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{context\}} \\ Q: \texttt{\{question\}}\\ Referring to the passage above,  the correct answer to the given question is: \texttt{\{answer\}}

\paragraph{Models}: DV003\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{context\}} \\ Q: \texttt{\{question\}}\\ Referring to the passage above,  the correct answer to the given question is: \texttt{\{answer\}}

\subsubsection{IndicQA}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to solve reading comprehension problems. You will be provided questions on a set of passages and you will need to provide the answer as it appears in the passage. The answer should be in the same language as the question and the passage.\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{context\}} \\ Q: \texttt{\{question\}}\\ Referring to the passage above,  the correct answer to the given question is? If you can't find the answer, please respond "unanswerable". \texttt{\{answer\}}

\paragraph{Models}: DV003\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{context\}} \\ Q: \texttt{\{question\}}\\ Referring to the passage above,  the correct answer to the given question is: \texttt{\{answer\}}

\subsubsection{XStoryCloze}

\paragraph{Models}: DV003, GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{input\_sentence\_1\} \{input\_sentence\_2\} \{input\_sentence\_3\} \{input\_sentence\_4\}}\\ What is a possible continuation for the story given the following options ?\\ Option1: \texttt{\{sentence\_quiz1\}} Option2: \texttt{\{sentence\_quiz2\}}\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
\texttt{\{sentence\_quiz1\}}: Option1, \\ \texttt{\{sentence\_quiz2\}}: Option2

\subsubsection{PANX}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$:  You are an NLP assistant whose purpose is to perform Named Entity Recognition (NER). NER involves identifying and classifying named entities in a text into predefined categories such as person names, organizations, locations, and others. You will need to use the tags defined below: O means the word doesnâ€™t correspond to any entity. B-PER/I-PER means the word corresponds to the beginning of/is inside a person entity. B-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity. B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity. Do not try to answer the question! Just tag each token in the sentence.\\[5pt]
\noindent Template $f_{temp}$: \texttt{\{token\_1 token\_2 ... token\_n\}}\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
\texttt{\{tag\_1\} \{tag\_2\} ... \{tag\_n\}}: \\ \texttt{\{token\_1\}\_\{tag\_1\} \{token\_2\}\_\{tag\_2\}}\\  ... \texttt{\{token\_n\}\_\{tag\_n\}}

\subsubsection{UDPOS}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to perform Part of Speech (PoS) Tagging. PoS tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context. You will need to use the tags defined below:
\begin{enumerate}
\itemsep0em 
    \item ADJ: adjective
    \item ADP: adposition
    \item ADV: adverb
    \item AUX: auxiliary
    \item CCONJ: coordinating-conjunction
    \item DET: determiner
    \item INTJ: interjection
    \item NOUN: noun
    \item NUM: numeral
    \item PART: particle
    \item PRON: pronoun
    \item PROPN: proper-noun
    \item PUNCT: punctuation
    \item SCONJ: subordinating-conjunction
    \item SYM: symbol
    \item VERB: verb
    \item X: other
\end{enumerate}
\noindent Template $f_{temp}$: \texttt{\{token\_1 token\_2 ... token\_n\}}\\[5pt]
\noindent Verbalizer $f_{verb}$:\\
\texttt{\{tag\_1\} \{tag\_2\} ... \{tag\_n\}}: \\ \texttt{\{token\_1\}\_\{tag\_1\} \{token\_2\}\_\{tag\_2\} ...} \\ \texttt{\{token\_n\}\_\{tag\_n\}}

\subsubsection{GLUECoS Sentiment Analysis}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to solve Sentiment Analysis problems. Sentiment Analysis is the task of determining whether the sentiment, opinion or emotion expressed in a textual data is:  positive, negative, or neutral. Answer as concisely as possible in the same format as the examples below:\\[5pt]
\noindent Template $f_{temp}$: Does the following sentence have a positive, negative or neutral sentiment? \texttt{\{text\}}

\paragraph{Models}: DV003\\[5pt]
\noindent Template $f_{temp}$: Does the following sentence have a positive, negative or neutral sentiment? \texttt{\{text\}}

\subsubsection{XLSum}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to summarize any given article. You should summarize all important information concisely in the same language in which you have been provided the document. Following the examples provided below:\\[5pt]
\noindent Template $f_{temp}$:\\
    \texttt{\{document\}} \\ === \\ Write a summary of the text above :

\subsubsection{Jigsaw}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Task Instruction $\mathcal{I}$: You are an NLP assistant whose purpose is to solve the Jigsaw Toxicity Prompt problem. Toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. The objective is to identify toxic comments. Answer as concisely as possible in the same format as the examples below:\\[5pt]
\noindent Template $f_{temp}$: \texttt{\{text\}} Should this online comment be removed for its toxicity? Yes or No?

\paragraph{Models}: DV003\\[5pt]
\noindent Template $f_{temp}$: \texttt{\{text\}} Someone posted this comment online. Do you find it toxic? Yes or No?

\subsubsection{WinoMT}

\paragraph{Models}: GPT-3.5-Turbo, GPT-4\\[5pt]
\noindent Template $f_{temp}$: Translate the following English text to \texttt{\{target\_language\}}: \texttt{\{sentence\}} \\


\subsection{Results}
\label{sec:appendix_res}
%Tables comparing performance of models per datasets
% XNLI
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c c c c}
\toprule
Model & en & ar & bg & de & el & es & fr & hi & ru & sw & th & tr & ur & vi & zh & \textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 80.8 & 64.3 & 68.0 & 70.0 & 65.3 & 73.5 & 73.4 & 58.9 & 67.8 & 49.7 & 54.1 & 60.9 & 57.2 & 69.3 & 67.8 & 65.4 \\
mT5-Base & 84.7 & 73.3 & 78.6 & 77.4 & 77.1 & 80.3 & 79.1 & 70.8 & 77.1 & 69.4 & 73.2 & 72.8 & 68.3 & 74.2 & 74.1 & 75.4\\
XLM-R Large & 88.7 & 77.2 & 83.0 & 82.5 & 80.8 & 83.7 & 82.2 & 75.6 & 79.1 & 71.2 & 77.4 & 78.0 & 71.7 & 79.3 & 78.2 & 79.2 \\
TuLRv6 - XXL & \textbf{93.3} & \textbf{89.0} & \textbf{90.6} & \textbf{90.0} & \textbf{90.2} & \textbf{91.1} & \textbf{90.7} & \textbf{86.2} & \textbf{89.2} & \textbf{85.5} & \textbf{87.5} & \textbf{88.4} & \textbf{82.7} & \textbf{89.0} & \textbf{88.4} & \textbf{88.8} \\
\midrule
\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & 67.5 & 60.7 & 46.5 & 54.0 & 47.4 & 61.2 & 61.4 & 56.8 & 53.3 & 50.4 & 43.8 & 42.7 & 50.0 & 61.0 & 56.7 & 54.2\\
XGLM & 52.6 & 46.4 & 48.9 & 45.6 & 48.7 & 45.8 & 49.4 & 46.8 & 48.6 & 44.5 & 46.6 & 45.4 & 43.4 & 48.5 & 48.8 & 47.3\\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 76.2 & 59.0 & 63.5 & 67.3 & 65.1 & 70.3 & 67.7 & 55.5 & 62.5 & 56.3 & 54.0 & 62.6 & 49.1 & 60.9 & 62.1 & 62.1\\
\texttt{gpt-3.5-turbo} (TT) &  76.2 & 62.7 & 67.3 & 69.4 & 67.2 & 69.6 & 69.0 & 59.9 & 63.7 & 55.8 & 59.6 & 63.8 & 54.0 & 63.9 & 62.6 & 64.3\\
\texttt{text-davinci-003} & 79.5 & 52.2 & 61.8 & 65.8 & 59.7 & 71.0 & 65.7 & 47.6 & 62.2 & 50.2 & 51.1 & 57.9 & 50.0 & 56.4 & 58.0 & 59.3 \\
\texttt{text-davinci-003} (TT) & 79.5 & 65.1 & 70.8 & 71.7 & 69.3 & 72.2 & 71.8 & 63.3 & 67.3 & 57.3 & 62.0 & 67.6 & 55.1 & 66.9 & 65.8 & 67.1\\
\texttt{gpt-4-32k} & 84.9 & 73.1 & 77.3 & 78.8 & 79.0 & 78.8 & 79.5 & 72.0 & 74.3 & 70.9 & 68.8 & 76.3 & 68.1 & 74.3 & 74.6 & 75.4 \\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in XNLI. Metric: Accuracy.}
\label{tab:results_summary}
\end{table*}
% IndicXNLI
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c }
\toprule
Model & as & bn & gu & hi & kn & ml & mr & or & pa & ta & te & \textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
MuRIL & \textbf{76.0} & \textbf{75.0} & \textbf{77.0} & \textbf{77.0} & \textbf{77.0} & \textbf{79.0} & \textbf{74.0} & \textbf{76.0} & \textbf{77.0} & \textbf{77.0} & \textbf{74.0} & \textbf{76.0}\\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 49.5 & 53.6 & 50.6 & 55.5 & 53.9 & 48.4 & 49.9 & 47.4 & 53.6 & 48.2 & 47.4 & 50.7 \\
\texttt{gpt-3.5-turbo} (TT) & 54.3 & 61.6 & 61.8 & 59.6 & 60.8 & 59.9 & 58.7 & 58.5 & 62.3 & 58.3 & 60.8 & 59.7 \\
\texttt{text-davinci-003} & 48.6 & 52.6 & 51.2 & 56.9 & 49.1 & 48.2 & 49.4 & 46.4 & 50.4 & 45.5 & 47.2 & 49.6 \\
\texttt{text-davinci-003} (TT) & 56.0 & 66.0 & 64.7 & 62.6 & 63.9 & 61.8 & 60.9 & 60.8 & 64.7 & 61.8 & 63.1 & 62.4 \\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in IndicXNLI. Metric: Accuracy.}
\label{tab:results_summary}
\end{table*}



% PAWS-X
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c}
\toprule
Model & en & de & es & fr & ja & ko & zh & \textbf{avg} \\ \midrule
\multicolumn{9}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 94.0 & 85.7 & 87.4 & 87.0 & 73.0 & 69.6 & 77.0 & 81.9 \\
mT5-Base & 95.4 & 89.4 & 89.6 & 91.2 & 79.8 & 78.5 & 81.1 & 86.4 \\
XLM-R Large & 94.7 & 89.7 & 90.1 & 90.4 & 78.7 & 79.0 & 82.3 & 86.4 \\
TuLRv6 - XXL & \textbf{97.2} & \textbf{95.1} & \textbf{94.8} & \textbf{95.6} & \textbf{89.4} & \textbf{90.4} & \textbf{90.4} & \textbf{93.2} \\
\midrule
\multicolumn{9}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & 89.8 & 84.3 & 88.9 & 87.5 & 74.4 & 85.8 & 65.2 & 82.3\\
\midrule
\multicolumn{9}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 72.4 & 70.6 & 72.0 & 72.1 & 67.2 & 66.5 & 69.2 & 70.0 \\
\texttt{gpt-3.5-turbo} (TT) & 72.4 & 70.8 & 69.7 & 70.1 & 61.9 & 62.5 & 63.1 & 67.2 \\
\texttt{text-davinci-003} & 72.5 & 70.6 & 72.7 & 70.7 & 60.6 & 61.8 & 60.8 & 67.1 \\
\texttt{text-davinci-003} (TT) & 72.5 & 69.8 & 70.1 & 71.3 & 65.4 & 65.8 & 65.2 & 68.6 \\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in PAWS-X. Metric: Accuracy.}
\label{tab:results_summary}
\end{table*}

% XCOPA
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c}
\toprule
Model & en & et & ht & id & it & qu & sw & ta & th & tr &\textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
%mBERT & \\
%mT5-Base &  \\
%XLM-R Large & \\
TuLRv6 - XXL & - & 77.4 & 78.0 & 92.6 & \textbf{96.0} & \textbf{61.0} & 69.4 & 85.4 & \textbf{87.2} & \textbf{92.8} & 74.0 \\
\midrule
\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & 88.0 & 48.0 & 55.0 & 86.0 & 74.0 & 50.0 & 60.0 & 67.0 & 50.0 & 54.0 & 63.2 \\
XGLM & - & 65.9 & 58.9 & 68.9 & 69.2 & 47.1 & 62.9 & 56.3 & 62.0 & 58.5 & 61.1\\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 97.8 & \textbf{90.6} & 72.0 & 90.4 & 95.2 & 54.6 & 82.0 & 59.0 & 77.6 & 91.0 & 81.0 \\
\texttt{gpt-3.5-turbo} (TT) & 97.8 & 88.2 & 79.4 & 90.8 & 94.4 & 50.0 & 77.6 & \textbf{87.0} & 82.2 & 87.8 & 83.5 \\
\texttt{text-davinci-003} & \textbf{98.2} & 87.8 & 75.0 & 91.4 & \textbf{96.0} & 54.8 & 63.6 & 53.8 & 66.6 & 87.8 & 77.5 \\
\texttt{text-davinci-003} (TT) & \textbf{98.2} & 89.6 & \textbf{82.8} & \textbf{93.0} & 94.6 & 50.0 & \textbf{82.8} & \textbf{87.0} & 84.8 & 89.8 & \textbf{85.3} \\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in XCOPA. Metric: Accuracy.}
\label{tab:results_summary}
\end{table*}
% XQuAD
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c}
\toprule
Model & en & ar & de & el & es & hi & ru & th & tr & vi & zh & \textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 83.5 / 72.2 & 61.5 / 45.1 & 70.6 / 54.0 & 62.6 / 44.9 & 75.5 / 56.9 & 59.2 / 46.0 & 71.3 / 53.3 & 42.7 / 33.5 & 55.4 / 40.1 & 69.5 / 49.6 & 58.0 / 48.3 & 64.5 / 49.4 \\
mT5-Base & 84.6 / 71.7 & 63.8 / 44.3 & 73.8 / 54.5 & 59.6 / 35.6 & 74.8 / 56.1 & 60.3 / 43.4 & 57.8 / 34.7 & 57.6 / 45.7 & 67.9 / 48.2 & 70.7 / 50.3 & 66.1 / 54.1 &  67.0 / 49.0\\
XLM-R Large & 86.5 / 75.7 & 68.6 / 49.0 & 80.4 / 63.4 & 79.8 / 61.7 & 82.0 / 63.9 & 76.7 / 59.7 & 80.1 / 64.3 & 74.2 / 62.8 & 75.9 / 59.3 & 79.1 / 59.0 & 59.3 / 50.0 & 76.6 / 60.8 \\
TuLRv6 - XXL & 90.1 / 80.6 & \textbf{85.4 / 69.6} & \textbf{86.1 / 70.4} & \textbf{86.3 / 70.4} & \textbf{87.6 / 71.0} & \textbf{85.9 / 70.5} & \textbf{86.8 / 73.2} & \textbf{87.0 / 81.1} & \textbf{84.3 / 71.0} & \textbf{87.6 / 71.3} & 79.2 / 73.2 & \textbf{86.0 / 72.9} \\
\midrule
\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & \textbf{92.1 / 83.8} & 82.8 / 69.7 & 76.3 / 60.4 & 49.7 / 37.6 & 86.8 / 71.4 & 83.4 / 72.9 & 65.7 / 47.2 & 20.5 / 15.5 & 51.4 / 37.2 & 86.9 / 72.7 & \textbf{82.4 / 78.6} & 70.7 / 58.8 \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 79.3 / 58.7 & 59.6 / 35.1 & 70.6 / 46.6 & 49.0 / 22.8 & 70.3 / 40.8 & 54.0 / 29.0 & 58.0 / 31.3 & 41.9 / 30.4 & 61.8 / 35.0 & 69.1 / 42.4 & 50.4 / 48.3 & 60.4 / 38.2 \\
%\texttt{gpt-3.5-turbo} (TT) & \\
\texttt{text-davinci-003} & 77.2 / 61.8 & 36.8 / 22.5 & 55.2 / 39.7 & 31.8 / 19.7 & 61.8 / 41.3 & 19.9 / 10.0 & 29.4 / 17.6 & 11.5 / 8.7 & 44.8 / 29.2 & 41.7 / 25.4 & 35.6 / 32.8 & 40.5 / 28.1 \\
%\texttt{text-davinci-003} (TT) & \\
\texttt{gpt-4-32k} & 83.2 / 65.6 & 67.8 / 42.4 & 71.9 / 48.7 & 62.3 / 36.6 & 77.5 / 50.7 & 63.9 / 36.7 & 63.8 / 35.8 & 54.6 / 42.0 & 70.8 / 46.6 & 75.8 / 49.7 & 60.0 / 57.5 & 68.3 / 46.6 \\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in XQuAD. Metric: F1 Score / Exact Match.}
\label{tab:results_summary}
\end{table*}

% TyDiQA-GoldP
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c}
\toprule
Model & en & ar & bn & fi & id & ko & ru & sw & te & \textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 75.3 / 63.6 & 62.2 / 42.8 & 49.3 / 32.7 & 59.7 / 45.3 & 64.8 / 45.8 & 58.8 / 50.0 & 60.0 / 38.8 & 57.5 / 37.9 & 49.6 / 38.4 & 59.7 / 43.9 \\
mT5-Base & 71.8 / 60.9 & 67.1 / 50.4 & 40.7 / 22.1 & 67.0 / 52.2 & 71.3 / 54.5 & 49.5 / 37.7 & 54.9 / 32.6 & 60.4 / 43.9 & 40.6 / 31.1 & 58.1 / 42.8 \\
XLM-R Large & 71.5 / 56.8 & 67.6 / 40.4 & 64.0 / 47.8 & 70.5 / 53.2 & 77.4 / 61.9 & 31.9 / 10.9 & 67.0 / 42.1 & 66.1 / 48.1 & 70.1 / 43.6 & 65.1 / 45.0 \\
TuLRv6 - XXL & \textbf{85.4 / 76.4} & \textbf{84.1 / 70.4} & 86.9 / 79.6 & \textbf{83.8 / 72.8} & \textbf{88.8 / 77.9} & \textbf{78.5 / 67.8} & \textbf{81.9 / 68.6} & \textbf{87.2 / 79.6} & 85.2 / 71.6 & \textbf{84.6 / 73.8} \\
\midrule
\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & 82.4 / 70.9 & 81.9 / 62.2 & \textbf{87.8 / 82.3} & 43.6 / 28.6 & 85.0 / 71.0 & 52.3 / 43.1 & 67.4 / 51.5 & 86.0 / 77.2 & \textbf{90.3 / 81.6} & 75.2 / 63.2 \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 54.8 / 30.7 & 50.9 / 24.2 & 60.7 / 32.7 & 66.6 / 49.0 & 67.2 / 43.4 & 59.7 / 45.3 & 45.8 / 20.0 & 64.3 / 47.7 & 70.9 / 53.1 & 60.1 / 38.4 \\
\texttt{text-davinci-003} & 73.7 / 59.1 & 56.2 / 38.7 & 16.1 / 10.6 & 70.3 / 58.8 & 68.6 / 51.2 & 40.6 / 32.2 & 42.3 / 28.9 & 74.1 / 62.3 & 5.8 / 3.0 & 49.8 / 38.3 \\
\texttt{gpt-4-32k} & 72.9 / 51.4 & 60.8 / 32.7 & 68.0 / 42.5 & 75.4 / 57.7 & 80.8 / 61.1 & 69.7 / 58.5 & 61.4 / 30.5 & 81.8 / 68.7 & 72.5 / 54.9 & 71.5 / 50.9 \\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in TyDiQA. Metric: F1 Score / Exact Match.}
\label{tab:results_summary}
\end{table*}

% MLQA
\begin{table*}[h]
\resizebox{\textwidth}{!}{ %
\begin{tabular}{l c c c c c c c c }
\toprule
Model & en & ar & de & es & hi & vi & zh & \textbf{avg} \\ \midrule
\multicolumn{9}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 80.2 / 67.0 & 52.3 / 34.6 & 59.0 / 43.8 & 67.4 / 49.2 & 50.2 / 35.3 & 61.2 / 40.7 & 59.6 / 38.6 & 61.4 / 44.2 \\
mT5-Base & 81.7 / 66.9 & 57.1 / 36.9 & 62.1 / 43.2 & 67.1 / 47.2 & 55.4 / 37.9 & 65.9 / 44.1 & 61.6 / 38.6 & 64.4 / 45.0 \\
XLM-R Large & 83.5 / 70.6 & 66.6 / 47.1 & 70.1 / 54.9 & 74.1 / 56.6 & 70.6 / 53.1 & 74.0 / 52.9 & 62.1 / 37.0 & 71.6 / 53.2 \\
TuLRv6 - XXL & \textbf{86.6 / 74.4} & \textbf{76.2 / 56.5} & \textbf{80.2 / 67.0} & \textbf{81.7 / 65.1} & \textbf{82.2 / 64.8} & \textbf{82.3 / 63.2} & \textbf{78.1 / 56.5} & \textbf{81.0 / 63.9} \\
%\midrule
%\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
%\midrule
%BLOOMZ & \\
\midrule
\multicolumn{9}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 72.8 / 53.2 & 48.5 / 23.9 & 51.0 / 29.6 & 53.8 / 29.4 & 50.7 / 28.9 & 58.9 / 35.1 & 56.7 / 29.4 & 56.1 / 32.8 \\
\texttt{gpt-3.5-turbo} (TT) & 72.8 / 53.2 & 37.8 / 18.4 & 44.3 / 26.2 & 54.1 / 31.8 & 37.3 / 20.0 & 41.6 / 22.5 & 36.5 / 17.2 & 46.4 / 27.0 \\
\texttt{text-davinci-003} & 74.8 / 59.0 & 38.4 / 21.7 & 57.7 / 38.1 & 62.9 / 37.8 & 24.9 / 14.1 & 47.7 / 29.7 & 32.3 / 31.7 & 48.4 / 33.1 \\
\texttt{text-davinci-003} (TT) & 74.8 / 59.0 & 48.2 / 25.6 & 53.5 / 33.9 & 62.9 / 40.9 & 49.2 / 28.7 & 51.0 / 30.4 & 45.2 / 24.1 & 55.0 / 34.7
\\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in MLQA. Metric: F1 Score / Exact Match.}
\label{tab:results_summary}
\end{table*}

% IndicXNLI
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c }
\toprule
Model & as & bn & gu & hi & kn & ml & mr & or & pa & ta & te & \textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
BLOOMZ & \textbf{40.6 / 31.7} & 42.9 / 36.6 & 37.2 / 29.9 & 44.0 / 45.1 & \textbf{37.8 / 26.6} & \textbf{30.5 / 28.4} & 39.2 / 33.0 & \textbf{25.4 / 22.0} & 26.4 / 33.5 & \textbf{39.7 / 35.9} & \textbf{38.9 / 34.7} & 36.6 / 32.5 \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 35.3 / 21.4 & \textbf{49.5 / 30.2} & \textbf{40.5 / 25.5} & \textbf{55.9 / 39.3} & 35.3 / 20.4 & 30.0 / 19.2 & \textbf{50.0 / 32.0} & 22.1 / 12.7 & \textbf{35.8 / 15.1} & 32.7 / 21.6 & 32.9 / 19.7 & \textbf{38.2 / 23.4} \\
\texttt{text-davinci-003} & 6.7 / 3.2 & 10.3 / 5.8 & 5.4 / 3.5 & 16.8 / 11.8 & 7.1 / 3.9 & 3.6 / 2.3 & 14.6 / 8.5 & 6.9 / 3.4 & 10.7 / 4.1 & 4.2 / 2.5 & 6.8 / 3.6 & 8.4 / 4.8 \\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in IndicQA. Metric: F1 Score / Exact Match.}
\label{tab:results_summary}
\end{table*}
% UDPOS
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c c c c c c c c}
\toprule
Model & en & af & ar & bg & de & el & es & et & eu & fa & fi & fr & he & hi & hu & id & it & ja  & kk \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & 96.4 & 86.7 & 50.0 & 84.7 & 88.7 & 80.9 & 86.6 & 79.9 & 62.1 & 65.5 & 73.3 & 81.2 & 55.5 & 66.0 & 78.6 & 74.2 & 87.8 & 47.2 & 70.4 \\
XLM-R Large  & \textbf{97.0} & \textbf{89.2} & \textbf{63.0} & \textbf{88.3} & \textbf{91.2} & \textbf{86.5} & \textbf{89.2} & \textbf{87.3} & \textbf{74.9} & \textbf{70.8} & \textbf{82.7} & \textbf{86.7} & \textbf{67.5} & \textbf{75.2} & \textbf{83.4} & \textbf{75.7} & \textbf{89.2} & 29.3 & \textbf{78.3} \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo}  & 78.5 & 74.3 & 38.3 & 79.1 & 80.7 & 47.1 & 34.8 & 76.0 & 72.0 & 46.7 & 79.5 & 78.0 & 53.8 & 50.7 & 65.4 & 63.6 & 75.4 & \textbf{47.4} & 64.8
\\
\midrule
& ko & lt & mr & nl & pl & pt & ro & ru & ta & te & th & tl & tr & uk & ur & vi & wo & yo & zh & \textbf{avg} \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT &  51.7 & 78.8 & 68.7 & 88.6 & 80.7 & 88.0 & 71.5 & 82.4 & 58.5 & 75.2 & 41.3 & 80.5 & 70.5 & 80.6 & 56.6 & 55.4 & 0.0 & \textbf{56.6} & \textbf{59.6} & 71.9\\
XLM-R Large & \textbf{57.1} & \textbf{84.2} & \textbf{81.8} & \textbf{89.5} & \textbf{86.8} & \textbf{90.2} & \textbf{82.6} & \textbf{87.3} & \textbf{64.0} & \textbf{84.2} & \textbf{48.5} & \textbf{92.4} & \textbf{81.2} & \textbf{85.8} & \textbf{70.8} & \textbf{58.5} & 0.0 & 24.8 & 44.1 & \textbf{76.2} \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 39.0 & 71.3 & 57.9 & 78.3 & 81.7 & 76.7 & 66.7 & 69.9 & 32.6 & 79.8 & 25.5 & 54.3 & 77.2 & 58.9 & 39.9 & 57.7 & \textbf{50.4} & 7.0 & 57.2 & 60.2\\
\midrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in POS. Metric: F1 Score.}
\label{tab:results_summary}
\end{table*} 
% NER
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c c c c c c c c c c c c c}
\toprule
Model & en & af & ar & az & bg & bn & de & el & es & et & eu & fa & fi & fr & gu & he & hi & hu & id & it & ja & jv & ka & kk \\ \midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & \textbf{86.4} & 76.1 & 42.9 & 65.5 & 76.7 & 69.7 & 79.5 & 70.9 & \textbf{75.3} & 75.8 & \textbf{64.4} & 40.0 & 76.6 & 79.6 & 51.3 & \textbf{56.2} & 65.9 & 76.1 & \textbf{61.0} & \textbf{81.3} & \textbf{29.2} & \textbf{62.4} & 65.1 & 50.3\\
XLM-R Large & 85.4 & \textbf{78.6} & \textbf{47.3} & \textbf{69.4} & \textbf{80.9} & \textbf{74.7} & \textbf{80.7} & \textbf{79.2} & 71.8 & \textbf{78.7} & 61.6 & \textbf{55.2} & \textbf{79.6} & \textbf{79.8} & \textbf{62.7} & 55.5 & \textbf{70.9} & \textbf{80.2} & 51.8 & 80.3 & 18.5 & 61.9 & \textbf{70.9} & \textbf{54.4} \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 43.2 & 43.8 & 45.4 & 42.1 & 51.6 & 40.3 & 52.7 & 41.0 & 60.2 & 58.7 & 31.5 & 39.3 & 59.1 & 50.7 & 18.4 & 34.3 & 45.5 & 53.7 & 58.4 & 60.0 & 7.4 & 57.7 & 25.1 & 30.9 \\
\midrule 
& ko & lt & ml & mr & ms & my & nl & pa & pl & pt & qu & ro & ru & sw & ta & te & th & tl & tr & uk & ur & vi & yo & zh & \textbf{avg}\\\midrule
\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
\midrule
mBERT & \textbf{59.5} & \textbf{75.8} & 53.0 & 57.0 & 67.1 & 45.7 & 81.0 & 30.5 & 79.2 & \textbf{80.4} & \textbf{58.5} & 74.0 & 63.9 & \textbf{71.4} & 50.7 & 48.9 & 0.4 & 72.6 & 73.4 & 69.7 & 35.4 & 74.5 & 45.8 & \textbf{42.5} & 62.3\\
XLM-R Large & 59.2 & \textbf{75.8} & \textbf{60.2} & \textbf{63.4} & \textbf{68.5} & \textbf{55.2} & \textbf{83.2} & \textbf{49.4} & \textbf{79.3} & 79.9 & \textbf{58.5} & \textbf{78.7} & \textbf{71.9} & 68.9 & \textbf{58.4} & \textbf{53.8} & 0.7 & \textbf{74.7} & \textbf{80.3} & \textbf{78.0} & \textbf{60.3} & \textbf{78.3} & 37.0 & 26.6 & \textbf{65.2}\\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 27.9 & 51.9 & 25.2 & 34.4 & 52.0 & 8.7 & 59.4 & 36.7 & 58.4 & 48.9 & 41.9 & 42.7 & 29.4 & 57.7 & 26.0 & 22.0 & \textbf{1.7} & 36.5 & 50.5 & 34.4 & 35.7 & 33.5 & \textbf{56.9} & 13.3 & 40.3\\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in NER. Metric: F1 Score.}
\label{tab:results_summary}
\end{table*}
% XStoryCloze
\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c }
\toprule
Model & ar & en & es & eu & hi & id & my & ru & sw & te & zh &
\textbf{avg} \\ \midrule
%\multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
%\midrule
%mBERT & \\
%mT5-Base & \\
%XLM-R Large &  \\
%\midrule
\multicolumn{10}{l}{\textit{Prompt-Based Baselines}} \\
\midrule
BLOOMZ & 79.7 & 95.7 & 87.3 & 70.5 & 79.9 & 85.6 & 49.9 & 67.3 & 65.3 & 67.4 & 90.0 & 76.2 \\
XGLM & 59.8 & 75.9 & 69.2 & 63.8 & 62.5 & 70.8 & 61.2 & 72.4 & 65.2 & 63.4 & 67.7 & 66.5 \\
\midrule
\multicolumn{10}{l}{\textit{Open AI Models}} \\
\midrule
\texttt{gpt-3.5-turbo} & 92.5 & 96.8 & 95.8 & 78.4 & 91.1 & 95.0 & 57.2 & 96.6 & \textbf{92.3} & 73.1 & 95.6 & 87.7 \\
\texttt{gpt-3.5-turbo} (TT) & 94.3 & 96.8 & 96.1 & 92.5 & 94.7 & 95.2 & 88.6 & 96.2 & 88.7 & 93.6 & 95.6 & 93.9 \\
\texttt{text-davinci-003} & 87.4 & \textbf{98.3} & \textbf{97.6} & 78.1 & 77.8 & \textbf{96.4} & 47.4 & 94.2 & 78.1 & 57.6 & 95.0 & 82.5 \\
\texttt{text-davinci-003} (TT) & \textbf{95.0} & \textbf{98.3} & 96.2 & \textbf{94.1} & \textbf{95.1} & 95.9 & \textbf{90.1} & \textbf{96.9} & 90.7 & \textbf{94.3} & \textbf{96.2} & \textbf{94.8} \\
\bottomrule
\end{tabular}}
\caption{Comparing performance of different models on all languages in XStoryCloze. Metric: Accuracy.}
\label{tab:results_summary}
\end{table*}


% % XLSum
% \begin{table*}[h]
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{l c c c c c c c c c c c c c c c c c c c c c c c }
% \toprule
% Model & en & am & ar & az & bn & my & zh & zh & fr & gu & ha & hi & ig & in & ja & rn & ko & ky & mr & ne & om & ps\\ \midrule
% \multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
% \midrule
% mT5-RL & 28.6 & 26.1 & 19.3 & 22.9 & 18.2 & 22.4 & 16.2 & 28.2 & 24.1 & 24.2 & 29.3 & 24.5 & 30.8 & 29.9 & 30.9 & 26.1 & 25.8 & 29.9 & 25.1 & 14.2 & 31.7 & 16.5 \\
% \midrule
% \multicolumn{10}{l}{\textit{Open AI Models}} \\
% \midrule
% \texttt{gpt-3.5-turbo} & 26.0 & 6.0 & 7.1 & 24.0 & 0.0 & 7.1 & 8.0 & 8.0 & 26.0 & 0.0 & 24.0 & 4.0 & 21.0 & 27.0 & 22.0 & 21.0 & 21.0 & 8.0 & 6.0 & 0.0 & 14.2 & 0.0 \\
% \texttt{gpt-4-32k} & 27.4 & 16.9 & 23.1 & 15.6 & 0.0 & 6.3 & 27.2 & 9.7 & 23.8 & 6.6 & 24.5 & 24.0 & 20.0 & 25.4 & 24.1 & 21.9 & 21.2 & 13.3 & 7.0 & 18.5 & 14.7 & 23.9\\
% \midrule 
% & fa & pidgin & pt & pa & gd & sr & sr & si & so & es & sw & te & th & ti & tr & uk & ur & uz & vi & cy & yo & \textbf{avg}\\\midrule
% \multicolumn{10}{l}{\textit{Fine-tuned Baselines}} \\
% \midrule
% mT5-RL & 20.9 & 15.4 & 18.1 & 28.9 & 20.1 & 19.9 & 37.4 & 22.1 & 17.6 & 19.9 & 25.5 & 21.2 & 29.2 & 23.5 & 31.6 & 24.3 & 25.1 & 33.4 & 32.0 & 32.8 & 30.1 & 25.0\\
% \midrule
% \multicolumn{10}{l}{\textit{Open AI Models}} \\
% \midrule
% \texttt{gpt-3.5-turbo} & 0.0 & 24.0 & 26.0 & 5.0 & 23.0 & 7.1 & 17.0 & 0.0 & 21.0 & 21.0 & 27.0 & 0.0 & 16.0 & 0.0 & 26.0 & 11.0 & 3.0 & 5.0 & 35.0 & 22.0 & 19.0 & 13.7 \\
% \texttt{gpt-4-32k} & 23.7 & 27.1 & 24.0 & 5.9 & 24.6 & 18.1 & 16.3 & 15.4 & 17.4 & 20.0 & 24.8 & 5.6 & 9.7 & 18.8 & 17.8 & 18.2 & 25.5 & 12.3 & 21.6 & 25.5 & 16.3 & 18.2 \\
% \bottomrule
% \end{tabular}}
% \caption{Comparing performance of different models on all languages in XLSum. Metric: Rouge - L}
% \label{tab:results_summary}
% \end{table*}
% %%%%%%%%%



\begin{table*}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccccclccccc}
\hline
 &
  \multicolumn{3}{c}{Google} &
  \multicolumn{3}{c}{Microsoft} &
  \multicolumn{3}{c}{Amazon} &
  \multicolumn{3}{c}{Systran} &
  \multicolumn{3}{c}{GPT Turbo 3.5} &
  \multicolumn{3}{c}{Bloomz} \\
 &
  Acc &
  $\Delta_G$\ &
  $\Delta_S$\ &
  Acc &
  $\Delta_G$\ &
  $\Delta_S$\ &
  Acc &
  $\Delta_G$\ &
  $\Delta_S$\ &
  Acc &
  $\Delta_G$\ &
  $\Delta_S$\ &
  \multicolumn{1}{c}{Acc} &
  $\Delta_G$\ &
  $\Delta_S$\ &
  Acc &
  $\Delta_G$\ &
  $\Delta_S$\ \\ \hline
es &
  50.9 &
  23.2 &
  20.9 &
  45 &
  36.5 &
  22.9 &
  \textbf{57.2} &
  15.3 &
  21.7 &
  42.5 &
  46.2 &
  15.6 &
  54.9 &
  22.7 &
  26.2 &
  55.6 &
  17.2 &
  32.5 \\
fr &
  \textbf{61.6} &
  6.1 &
  22.3 &
  44.5 &
  34.2 &
  15.8 &
  54.2 &
  16.4 &
  15 &
  43.4 &
  41.8 &
  -0.1 &
  52.7 &
  21.4 &
  26.1 &
  52 &
  17.8 &
  24.6 \\
it &
  38.6 &
  32.9 &
  18.6 &
  38.8 &
  41.8 &
  10.5 &
  40.2 &
  26.8 &
  14.7 &
  38.1 &
  47.3 &
  6.3 &
  45.1 &
  21.9 &
  26.7 &
  \textbf{45.7} &
  9 &
  18.5 \\ \hline
ru &
  37.8 &
  36.7 &
  11.4 &
  36.9 &
  42 &
  8.4 &
  39.8 &
  34.8 &
  9.4 &
  37.3 &
  44.1 &
  9.2 &
  \textbf{41} &
  31.6 &
  10.2 &
  5.9 &
  INV &
  0 \\
uk &
  38.4 &
  43.5 &
  10.7 &
  41.3 &
  46.8 &
  11.9 &
  - &
  - &
  - &
  28.9 &
  22.4 &
  12.9 &
  \textbf{42.9} &
  34.2 &
  12.1 &
  16.8 &
  22.7 &
  2.2 \\ \hline
he &
  50.8 &
  11.7 &
  35.5 &
  44 &
  22 &
  29.8 &
  48 &
  13.6 &
  45.9 &
  43.1 &
  26.9 &
  23.1 &
  \textbf{57.5} &
  7.6 &
  40.8 &
  27.5 &
  31.4 &
  5 \\
ar &
  45.8 &
  42.5 &
  16.2 &
  45 &
  47.1 &
  14.2 &
  48.3 &
  37.8 &
  18.8 &
  45.6 &
  49.4 &
  -4.1 &
  \textbf{61.1} &
  13.9 &
  27.9 &
  48.1 &
  23 &
  25.6 \\ \hline
de &
  59.4 &
  12.5 &
  12.6 &
  \textbf{74.1} &
  0 &
  8.8 &
  62.4 &
  12 &
  16.7 &
  48.5 &
  34.5 &
  10 &
  57.5 &
  19.5 &
  14.2 &
  47.6 &
  56.2 &
  6.6 \\ \hline
\end{tabular}%
}
\caption{Performance of commercial MT systems and LLMs on the WinoMT corpus on 8 target languages. Results are categorized by language family. Acc indicates overall gender accuracy (\% of instances the translation had the correct gender), $\Delta_G$ denotes the difference in performance (F1 score) between masculine and feminine scores, and $\Delta_S$ is the difference in performance (F1 score) between pro-stereotypical and anti-stereotypical gender role assignments (higher numbers in the two latter metrics indicate stronger biases). Numbers in bold indicate best accuracy for the language across all systems. \footnotesize {Notes: [1. For Google, Microsoft, Amazon, and Systran we use the translations provided by \cite{stanovsky2019evaluating}. Some values differ from the original paper due to updated Spcay modules. 2. For Ru in Bloomz, Precision in male predictions is 0 leading to Invalid (INV) in $\Delta_G$]}}
\label{tab:wino-mt}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{lcccccccl} 
\hline
Model                           & es                                        & fr                                        & it                                        & pt                                        & ru                                        & tr                                        & \textbf{avg} &   \\ 
\hline
\multicolumn{9}{l}{\textit{LLM Baselines}}                                                                                                                                                                                                                                                                          \\ 
\hline
PALM (0-Shot)                   & 79.83                                     & 78.99                                     & -                                         & 77.58                                     & 80.35                                     & 84.1                                      & 80.17        &   \\
PALM (10-Shot Monolingual)      & \textbf{91.23}                                     & 86.16                                     & -                                         & 90.99                                     & 92.47                                     & 84.5                                      & 89.07        &   \\
PALM-2 (0-Shot)                 & 88.6                                      & 84.11                                     & -                                         & 87.68                                     & 90.5                                      & 93.42                                     & 88.86        &   \\
PALM-2 (10-Shot Monolingual)    & 89.68                                     & \textbf{87.94}                                     & -                                         & \textbf{92.05}                                     & \textbf{94.25}                                     & \textbf{94.34}                                     & \textbf{91.65}        &   \\ 
\hline
\multicolumn{9}{l}{\textit{OpenAI Models}}                                                                                                                                                                                                                                                                                 \\ 
\hline
gpt-3.5-turbo (Crosslingual)    & \textcolor[rgb]{0.129,0.129,0.133}{77.27} & \textcolor[rgb]{0.129,0.129,0.133}{73.64} & \textcolor[rgb]{0.129,0.129,0.133}{80.05} & \textcolor[rgb]{0.129,0.129,0.133}{81.16} & \textcolor[rgb]{0.129,0.129,0.133}{74.99} & \textcolor[rgb]{0.129,0.129,0.133}{85.65} & 78.79        &   \\
gpt-3.5-turbo (TT)              & 74.20                                     & 70.09                                     & 76.67                                     & 72.66                                     & 73.68                                     & 82.99                                     & 75.05        &   \\
text-davinci-003 (Crosslingual) & 79                                        & 74.55                                     & \textbf{81.11}                                     & 81.63                                     & 79.13                                     & 93.55                                     & 81.50        &   \\
text-davinci-003 (TT)           & 79.06                                     & 72.93                                     & 78.93                                     & 75.18                                     & 80.48                                     & 93.22                                     & 79.97        &   \\
\hline
\end{tabular}
\caption{Comparing performance of different models on all languages in Jigsaw. Metric: Accuracy.}
\label{tab:jigsaw_results_summary}
\end{table*}


\begin{figure*}
\begin{subfigure}[t]{0.8\textwidth}
\includegraphics[width=0.95\textwidth]{figures/dv003-lang-family.pdf}
    \caption{Language-Family wise performance of \texttt{text-davinci-003} on various tasks.}
    \label{fig:dv003_lang_fam}
\end{subfigure}
\begin{subfigure}[t]{0.8\textwidth}
\includegraphics[width=0.95\textwidth]{figures/turbo-script.pdf}
    \caption{Script wise performance of \texttt{text-davinci-003} on various tasks.}
    \label{fig:dv003_script}
\end{subfigure}
\caption{Language family and script wise performance of DV003 across tasks}
\label{fig:dv003_famscript}
\end{figure*}

\begin{figure*}
\begin{subfigure}[t]{0.8\textwidth}
\includegraphics[width=0.95\textwidth]{figures/gpt4-lang-family.pdf}
    \caption{Language-Family wise performance of \texttt{gpt-4-32k} on various tasks.}
    \label{fig:gpt4_lang_fam}
\end{subfigure}
\begin{subfigure}[t]{0.8\textwidth}
\includegraphics[width=0.95\textwidth]{figures/gpt4-script.pdf}
    \caption{Script wise performance of \texttt{gpt-4-32k} on various tasks.}
    \label{fig:gpt4_script}
\end{subfigure}
\caption{Language family and script-wise performance of GPT-4 across tasks}
\label{fig:gpt4_famscript}
\end{figure*}