\section{Experiments}

In this section, we describe how we adapt various NLP tasks to the in-context learning setting. We describe the prompting strategies we use for the benchmark and the models, tasks and datasets included in our study.

\subsection{Problem Formulation}

In order to solve different tasks via in-context learning we adopt the prompt-based few-shot learning strategy as defined in \cite{brown-etal-2020-language}. We define five main components of the prompts that we use in our experiments as follows: i) a \textbf{test example} $x_{test}$ for which the predictions are to be made; ii) $K$ \textbf{few-shot exemplars} $\{(x_i, y_i)\}_{i = 1}^{K}$, that are used to provide in-context supervision to the model; iii) a \textbf{task instruction} $\mathcal{I}$ which describes the instruction in text for the task to LLM; iv) a \textbf{prompt template} $f_{temp}(x)$ which turns a dataset input example into a text format that can 
 be used for prompting, containing the task description; and v) an \textbf{answer verbalizer} $f_{verb}(y)$ that maps the label $y$ to a textual representation. In our evaluation framework we often consider the instruction, template and verbalizer as a single entity, and from now on will denote the template to encapsulate the three unless specified separately. 
 
 % Some examples of $f_{temp}$, $\mathcal{I}$ and $f_{verb}$ are given in Table \ref{tab:prompt_template}.


Given these components, the final prompt $f_{prompt}(x_{test}; \{(x_i, y_i)\}_{i = 1}^{K}, \mathcal{I}, f_{temp}, f_{verb})$ or $f_{prompt}(x_{test})$ for short for a test input $x_{test}$ can be defined as:
\begin{align*}
f_{prompt}(x_{test}) =\mathcal{I} \mathbin\Vert_{i = 1}^{K} \big\{f_{temp}(x_i)&\mathbin\Vert f_{verb}(y_i)\big\}\\
&\mathbin\Vert f_{temp}(x_{test})
\end{align*}

where $\mathbin\Vert$ denotes the string concatenation operator.

The prompt can then be provided as input to the LLM $P(.;\theta)$ to obtain the prediction $z_{test}$

\begin{align*}
    z_{test} = \argmax_{z \in \mathcal{Z}} P(z | f_{prompt}(x_{test}); \theta)
\end{align*}

where $\mathcal{Z}$ is the space of possible answers, which in all of our experiments is taken to be the entirety of the language as modeled by the LLM. We approximate the $\argmax$ by sampling from the probability distribution predicted by the LLM.

The predicted answer $z_{test}$ is compared with the verbalized label using $f_{metric}(z_{test}, f_{verb}(y_{test})) \in [0, 1]$ that measures the extent of similarity between the ground truth and predicted answer. For our experiments, we use the exact-match score to determine accuracy for classification tasks and use the exact-match and F1-score for QA tasks. Formally, the evaluation score $s$ for an LLM $P(.;\theta)$ on a task $\mathcal{T}$ can be defined as:

\begin{equation*}
    s = {\E_{(x_{test}, y_{test}) \in \mathcal{T}}}[f_{metric}(z_{test}, f_{verb}(y_{test}))]
\end{equation*}


\subsection{Prompting Strategies}
\label{sec:prompt_strategies}
The choice of prompt significantly influences the performance of Large Language Models. Generative models have been shown to be brittle to simple prompting variations, such as the choice of prompt template and the training examples or even the ordering of examples \cite{Zhao-et-al-2021-calibrate}. For multilingual setups as highlighted in \citet{lin-etal-2022-shot} and \citet{shi-etal-2022-language}, some additional variations to consider include, the choice of the language of the prompt examples, the language of the prompt template (and instruction and verbalizer), and the language of the test examples. In this work, we evaluate models using (up to) three types of prompting strategies:

\iffalse
The choice of prompt can greatly influence the performance of generative models, and models have been shown in the past to be brittle to prompting variations such as the words used in the prompt, number of few-shot examples, ordering of examples etc CITE. Our setup in particular involves the choice of the few-shot examples $\{(x_i, y_i)\}_{i = 1}^{K}$ as well as  choice of different template $f_{temp}$ and verbalizer $f_{verb}$ functions, for defining the prompt. For our evaluation framework we consider two higher level decisions which stem from the choice of language in which the few-shot examples and the text examples are represented and the language in which the templates are written. For the former in particular we consider three setups:

\fi
\noindent
\begin{itemize}
    \item \textbf{Monolingual Prompting}: In this setup, the $k$\footnote{$k$=8, unless specified} randomly selected examples are of the same language as the test examples. 
    % Figure \ref{fig:monoprompting} illustrates an example of monolingual prompting in hindi.
    \item \textbf{Zero-Shot Cross-Lingual Prompting}: Pre-trained multilingual models are effective at zero-shot cross-lingual transfer \cite{pires-etal-2019-multilingual, wu-dredze-2019-beto}, that is on fine-tuning them for a task in one language leads to reasonable performance on unseen languages. In this section, we evaluate generative models' zero-shot cross-lingual transfer ability after in-context learning. We use $k$-shot examples from a pivot language\footnote{We use English as the pivot language in this paper} which is different from the language of the test example. 
    % Figure \ref{fig:zsprompting} illustrates the setup for a hindi test query.
    \item \textbf{Translate-Test Prompting}: This setup is similar to the Zero-Shot Cross-Lingual setup in the fact that the few-shot examples are sampled from English data. However, here we modify the test example itself by translating it to English.  Translate-test has been shown to be often better than cross-lingual transfer for both fine-tuning \cite{ponti-etal-2021-modelling} and in-context learning \cite{lin-etal-2022-shot, shi-etal-2022-language} and hence we explore its effectiveness for our benchmarking exercise as well. We use Bing Translator to translate the test examples to English.
    % An example for this setup for Hindi is given in Figure \ref{fig:translate_test}, we use Bing Translator to translate the test examples to English.
\end{itemize}

\iffalse
\noindent
\textbf{2. Zero-Shot Cross-Lingual Prompting}: Pre-trained multilingual models have been shown to be surprisingly effective at zero-shot cross lingual transfer \cite{pires-etal-2019-multilingual, wu-dredze-2019-beto}, where fine-tuning them for a task in one language leads to reasonable performance on unseen languages. In this setup, we try to probe to what extent LLMs can exhibit this behavior via in-context learning. Hence, the few-shot examples here are selected from a language (we call it pivot language) different from the language of the test example and for the purposes of our experiments we use English as the pivot language. Refer to Figure \ref{fig:zsprompting} for an example of this setup for Hindi.

\noindent
\textbf{3. Translate-Test Prompting}: This setup is similar to the Zero-Shot Cross-Lingual setup in the fact that the few-shot examples are sampled from English data. However, here we modify the test example itself by translating it to English.  Translate-test has been shown to be often better than cross-lingual transfer for both fine-tuning \cite{ponti-etal-2021-modelling} and in-context learning \cite{lin-etal-2022-shot, shi-etal-2022-language} and hence we explore its effectiveness for our benchmarking exercise as well. An example for this setup for Hindi is given in Figure \ref{fig:translate_test}. We use Bing Translator to translate the test examples to English in our experiments.
\fi

For the choice of language for the prompt template, we consider the following two setups:
\noindent
\begin{itemize}
    \item \textbf{English-Template}: Here the prompt templates are written in English, irrespective of the language of the few-shot and test examples. As has been shown in \citet{shi-etal-2022-language}, English instructions can often perform on par or even better than providing them in the native language. 
    \item \textbf{Native-Language-Template}: Here the prompt templates are written in the language of the test example $(x_{test}, y_{test})$. For our experiments, we use Bing Translator to translate the prompts from English to the native language.
\end{itemize}

In our initial experiments, we used different languages for task templates by automatically translating templates but found that it performs poorly due to translation errors changing the instructions leading to a loss in accuracy. We find English templates to perform better than native language prompts in most cases (see Table \ref{tab:en_v_translated_prompts}). Hence, unless specified explicitly, the main results are obtained using the English-Template setting.

\iffalse
\begin{table}[]
    \centering
    \begin{tabular}{p{2.5cm}p{2cm}p{2cm}}
        \toprule
         &  \textbf{English-Template} & \textbf{Native-Language-Template}\\
         \midrule
         Monolingual & \checkmark & \checkmark\\
         Zero-Shot Cross-Lingual & \checkmark & \xmark \\
         Translate-Test & \checkmark & \xmark\\
         \bottomrule
    \end{tabular}
    \caption{Combinations of the two higher level prompting setups.}
    \label{tab:prompting_setups}
\end{table}

Table \ref{tab:prompting_setups} provides the possible combinations of the two classes of prompting strategies. We only allow Native-Language templates for Monolingual setup, as in the other two setups the few-shot examples are in English. In our experiments, we tried using different language templates for few-shot examples and the test example but found that it performs poorly, as it would often lead to the model being confused about which language to generate the predictions in.
\fi
\subsection{Models}

In this section, we discuss details of the OpenAI models that we evaluate our benchmarking experiments as well as the fine-tuned as well prompt-based baselines that we compare these models against.

\noindent \textbf{OpenAI Models}: We conduct all benchmarking experiments on the GPT-3.5 models \texttt{text-davinci-003} and \texttt{gpt-3.5-turbo} \cite{ouyang2022training} as well on the GPT-4 model \texttt{gpt-4-32k}\cite{gpt4techreport}. The \texttt{text-davinci-003} model has a maximum context size of 4096 tokens, while \texttt{gpt-3.5-turbo} and \texttt{gpt-4-32k} support context sizes of 16k and 32k respectively. We do not conduct any fine-tuning of the model for our benchmarking experiments or carry out hyperparameter tuning for temperature or other settings. We use the suffix \texttt{TT} to indicate if the models are evaluated using translate-test prompting strategy (e.g. \texttt{text-davinci-003} (TT)), and by default, the model names without any suffix denote the model evaluated using monolingual prompting strategy \footnote{There are some exceptions to this. For XQUAD and IndicQA there is no available non-English training or validation datasets. Hence, we report numbers with zero-shot cross-lingual prompting strategy for these two datasets.}.

\noindent \textbf{Prompt-based Baselines}: We compare the performance of OpenAI models with other multilingual LLMs including:
\begin{itemize}
    \item BLOOMZ \cite{muennighoff2022crosslingual}, a multi-task fine-tuned version of the BLOOM \cite{scao2022bloom} model, which is a 176 billion parameter model created by the BigScience community trained on 46 natural languages and 13 programming languages.
    \item XGLM \cite{lin-etal-2022-shot}: It is a 7.5 billion parameter model with a decoder-only Transformer architecture. To improve multilingual performance, the pre-training data was upsampled for medium to low-resource languages.
\end{itemize}

\noindent \textbf{Fine-tuned Baselines}: We also compare generative models' performance against SOTA models fine-tuned for specific tasks that we evaluate, such as TULRv6 \cite{patra2022beyond} and MuRIL \cite{khanuja2021muril} for the Indic benchmarks. The TULRv6 model is trained with a novel sampling strategy and bitexts in multiple languages and is currently at the top position on the XTREME \cite{hu2020xtreme} benchmark as of writing this paper. MuRIL is a multilingual BERT model trained on 16 Indic languages and obtains SOTA performance on some Indic benchmarks. We also report the performance of other models such as XLMR \cite{conneau2020unsupervised}, multilingual BERT \cite{bert2019}, and mT5 \cite{xue2021mt5} on individual tasks. All of these models (excluding mT5 for the summarization task), were fine-tuned with English data in different tasks and then evaluated in a zero-cross-lingual fashion on other target languages. Generative models are known to be few-shot learners \cite{brown2020language}, meaning that they are capable of learning through in-context few-shot examples (which we provide in our experiments) and do not need to be fine-tuned on language-specific or task-specific data. Since we foresee generative AI models replacing many of the task-specific fine-tuned models in the future, we believe it is important to do a comprehensive multilingual evaluation comparing all of these models. 

% We believe that it is important to compare the performance of generative models to other multilingual models for the following reasons: multilingual models such as TULR are typically fine-tuned in a zero-shot cross-lingual manner, meaning that they are not fine-tuned on multilingual task-specific data, giving them no language-specific advantage over generative models. 


\subsection{Tasks and Datasets}

In our experiments, we consider 16 tasks spanning the following task types - classification, sequence to sequence labeling and generation. Below we review the experimental setups and datasets used for benchmarking for these two tasks. A list of all the datasets with the languages covered by them can be found in Table \ref{tab:datasets}.

\subsubsection{Classification}
These tasks involve classifying a single sentence or a group of sentences into a finite number of discrete labels. For each dataset, we measure the performance of different models in terms of classification accuracy. For prompt-based models in particular, since we add no constraint on the output space of the LLM we compute the exact match between the generated output and a verbalized label to determine if the example was classified correctly. We run experiments for all the prompting strategies that we discussed in the previous sections for each dataset. The details of each dataset that we use for benchmarking are given below:

 \begin{table}[h]
     \centering
\resizebox{\linewidth}{!}{%
     \begin{tabular}{lll}
     \toprule
    Dataset&Task&Languages\\
    \midrule
    XNLI&Natural Language Inference& 15\\
    Indic-XNLI&Natural Language Inference&11\\
    GLUECoS&Natural Language Inference&2\\
    %Indic-WNLI&Natural Language Inference&gu, hi, mr\\
    %GLUECoS-NLI&Natural Language Inference&hi-en\\
    %GLUECoS-En-Es-Sentiment&Sentiment Analysis&es-en\\
    PAWS-X&Paraphrase Identification&7\\
    XCOPA&Commonsense Reasoning& 10\\
    XStoryCloze&Commonsense Reasoning& 11 \\
    TyDiQA-GoldP&Question Answering& 9 \\
    MLQA&Question Answering&6\\
    XQuAD&Question Answering& 11\\
    IndicQA&Question Answering& 10\\
    UDPOS&Part of Speech Tagging& 38\\
    PANX& NER & 48\\ 
    WinoMT&Gender Bias& 8 \\
    GLUECoS&Sentiment Analysis& 2 \\
    Jigsaw&Toxicity Classification& 6\\
    XLSum&Summarization&  44\\
    
  \bottomrule
     \end{tabular}}
     \caption{Datasets and Language coverage of the datasets that MEGA presents evaluation for.}
     \label{tab:datasets}
     \vspace{-0.4cm}
 \end{table}

\noindent{\textbf{1. Natural Language Inference}}: XNLI \cite{Conneau2018xnli} is a dataset for cross-lingual Natural Language Inference, which consists of professional translations of the MNLI \cite{wang2018glue} corpus into 14 languages. We also consider IndicXNLI \cite{aggarwal2022indicxnli} that translates the XNLI dataset into 11 Indic languages by using Machine Translation, followed by validation by native speakers.


% Indic-WNLI \cite{doddapaneni2022indicxtreme} is a translation of the Winograd NLI dataset \cite{wang2018glue}, an NLI version of the Winograd Schema Challenge into three Indic languages. 

%remove gluecos-nli if not done

% GLUECoS-NLI \cite{khanuja2020new} is a code-mixed NLI dataset in Hindi-English, consisting of Bollywood (Hindi) movie conversations as premises, with manually created hypotheses. 

% \subsubsection{Sentiment Analysis}

% %remove section if not done

% The EN-ES-CS Sentiment Analysis dataset \cite{vilares2016cs}, part of the GLUECoS benchmark \cite{khanuja2020gluecos} is a code-mixed dataset consisting of English-Spanish Tweets annotated with SentiStrength \cite{thelwall2017heart} scores. 

\noindent{\textbf{2. Paraphrase Identification}}: PAWS-X \cite{yang2019paws} is a paraphrase identification dataset professionally translated from the PAWS \cite{zhang2019paws} dataset into six typologically diverse languages. 

\noindent{\textbf{3. Commonsense Reasoning}}: XCOPA \cite{ponti2020xcopa} is a commonsense reasoning dataset, which is a translation of the COPA \cite{roemmele2011choice} dataset into 11 typologically diverse languages, including very low-resource languages such as Eastern Apurímac Quechua and Haitian Creole.
 
XStoryCloze \cite{lin2022few} is created by translating the English StoryCloze \cite{mostafazadeh2017lsdsem} dataset using professional translators into 10 typologically diverse languages.

\subsubsection{Question Answering}
We focus on Span Prediction type of Question Answering (QA) tasks in our experiments, where given a context and a question the task is to predict the answer within the context. One major challenge that we come across for multilingual evaluation of QA tasks is that for many languages we often cannot fit the context and question pairs for the few-shot and text examples in the maximum context size of 4096 for the DV003 model. This is mainly attributed to the poor performance of GPT's tokenizer on many non-latin script languages which results in over-tokenizing the words in these languages (See \textsection \ref{sec:tokenization} for details).

To overcome this issue we follow two steps. First, for the few-shot examples we only provide the line within the paragraph containing the answer as the context. Second, for the test example, we index the chunks of the context using the embeddings from the \texttt{text-embedding-ada-002} model. Given the question, the closest chunk in the full context is retrieved and used in the prompt for the test example. We use a maximum chunk size of 100 in our experiments and use the implementation for retrieval provided in the \textbf{LangChain}\footnote{\url{https://github.com/hwchase17/langchain}} library. By doing this,we minimize the space taken by the context tokens in our prompt.

Note that, for newer GPT models i.e. GPT-3.5-Turbo and GPT-4 which support longer context lengths, we do not use this retrieval strategy for QA tasks and prompt the models to obtain the answers directly. For each task, we calculate the Exact Match and F1 score as defined in \citet{rajpurkar-etal-2016-squad}.  For our experiments we 
 consider the following four tasks:

\noindent \textbf{1. TyDiQA} \cite{clark2020tydi} is a QA dataset covering 11 typologically diverse languages. The task consists of two sub-tasks - passage selection and minimum answer span (Gold-P). For our experiments, we consider the Gold-P task and evaluate Monolingual and Zero-Shot Cross-Lingual prompting strategies. Since the labels do not directly transfer one-to-one across translation for QA tasks as they do for classification and require the use of alignment algorithms, we skip translate-test prompting for this task.

\noindent \textbf{2. MLQA} \cite{lewis2020mlqa} is an extractive QA dataset translated into 7 languages by professional translators. The task has two variants, the first where the question, context, and answer are all in the same language; and the second, where the question is in a different language than the context and answer. We consider the former variant of the task in our experiments. For MLQA, translate-test splits are also available, where each language's test data has been translated into English with answers aligned using the attention scores. There is no training data available for MLQA, and we use SQuAD's\citet{rajpurkar-etal-2016-squad} training data for selecting few-shot examples in English and validation data for MLQA in other languages to get their few-shot examples. This way, we are able to evaluate for all three prompting setups.

\noindent \textbf{3. XQuAD} \cite{artetxe2020cross} consists of professional translations of a subset of the SQuaD dataset \cite{rajpurkar2016squad} into 10 languages. XQuAD only has validation datasets available publicly, hence we evaluate the models on them. Like MLQA we use English SQuAD data for few-shot examples and since we cannot use validation data in other languages for few-shot, we only evaluate for zero-shot cross-lingual setup for this task.

\noindent \textbf{4. IndicQA} \cite{doddapaneni2022indicxtreme} is a manually curated cloze-style reading comprehension dataset that can be used for evaluating question-answering models in 11 Indic languages. The context paragraphs are chosen from Wikipedia articles whose topics are closely related to Indic culture, history,etc. The publicly available test set has about 2000 sentences that we carry out our evaluation on. 

\subsection{Sequences Labeling}

In the sequence labeling task, a sequence of tokens (such as words) to be labeled are provided to the system. 

\subsubsection{Part of Speech Tagging}

UDPOS \cite{zeman2020universal} is a dataset for Part of Speech Tagging taken from the Universal Dependencies 2.5 from the XTREME \cite{hu2020xtreme} benchmark. We benchmark a subset of the languages available in UDPOS.

\subsubsection{Named Entity Recognition}

PANX \cite{pan2017cross} or WikiANN is a Named Entity Recognition dataset consisting of Wikipedia sentences tagged with Person, Organization and Location.

For both tasks we use the linguistic structure prompting approach of \citet{blevins2022prompting} to define the prompts. The exact prompts used can be found in \textsection \ref{sec:appendix_propmts}. Given the nature of both tasks, which would involve token alignment across the translation, we do not evaluate the translate-test prompting strategies for these setups. Also, since both tasks involve $> 30$ languages, to make the best use of the compute resources we only evaluate GPT-3.5-Turbo in a monolingual setup for these two tasks. Finally, we evaluate the first 1000 examples for each language for these datasets given the large number of languages. We have recomputed all baselines with this specification as well.

\subsection{Generation}

\subsubsection{Summarization}

The XLSum \cite{hasan2021xl} dataset contains article-summary pairs across 44 typologically diverse languages, ranging from high to very low-resource. 

For a similar reason as the tagging datasets, we only evaluate on first 1000 examples of the test sets in different languages and recompute the baselines on the same testset using the weights of the XLSUM pretrained model, opensourced by the authors \cite{hasan-etal-2021-xl}.  

\subsubsection{Code-switching datasets}

All the datasets we consider so far are monolingual, however, a majority of the world's population speaks more than one language, leading to language contact phenomena such as code-switching \cite{dogruoz-etal-2021-survey, sitaram2019survey}. We include two code-switching datasets in MEGA to benchmark the performance of generative models.

GLUECoS-NLI \cite{khanuja2020new} is a code-mixed NLI dataset in Hindi-English, consisting of Bollywood (Hindi) movie conversations as premises, with manually created hypotheses. 

The EN-ES-CS Sentiment Analysis dataset \cite{vilares2016cs}, part of the GLUECoS benchmark \cite{khanuja2020gluecos} is a code-mixed dataset consisting of English-Spanish Tweets annotated with SentiStrength \cite{thelwall2017heart} scores. 



\subsubsection{RAI datasets}

We include two datasets that measure the Responsible AI (RAI) dimensions of fairness and toxicity - Jigsaw\footnote{https://www.kaggle.com/competitions/jigsaw-multilingual-toxic-comment-classification/data} for toxic comment classification and WinoMT for gender bias. 

The Jigsaw dataset contains online comments sourced from Wikipedia. The training data, which is in English, contains labels pertaining to the toxicity of the comment and any relevant identity mentions contained in the comment. We use the test dataset, which contains these comments for 6 languages as illustrated in Table \ref{tab:datasets} for evaluation. The test dataset contains a binary label indicating whether or not the comment is toxic. Our objective is to assess the performance of these models across multiple languages and observe the disparity in this performance that could arise due to a number of factors, a prominent one being the source data that these models are trained on. Using English prompts from PromptSource for the original monolingual Jigsaw task, we task the model with classifying a comment as toxic or non-toxic. We perform crosslingual few-shot prompting and translate-test experiments for the test sets of all 6 languages, and report the results excluding content violations in Table \ref{tab:jigsaw_results_summary}.

The WinoMT dataset \cite{stanovsky2019evaluating} is created by concatenating the WinoGender \cite{rudinger-etal-2018-gender} and WinoBias \cite{zhao-etal-2018-gender} datasets. WinoMT dataset consists of 3888 English sentences with equal distribution of Male and Female genders. It is also equally balanced between stereotypical and non-stereotypical gender role assignments. We follow the method as reported by \cite{stanovsky2019evaluating} in their paper. We perform zero-shot monolingual prompting of all sentences in the dataset to translate them in 8 target languages. Further using \textit {fast\_align} we map the English entity to its translation. Finally, we extract the target-side entity’s using off the shelf tools for each target language. The extracted translated gender can be finally compared against the gold annotations for English.

% IndicQA \cite{doddapaneni2022indicxtreme} is a cloze-style reading comprehension dataset with context taken from Wikipedia articles on Indian culture and history, manually created in 11 Indic languages. 


% the few-shot examples are selected to belong to the same language as the test example to be evaluated. An example of the same for Hindi is shown in Figure \ref{fig:monoprompting}.

% Figure \ref{fig:zsprompting} illustrates the zero-shot prompting technique, in which the few-shot examples are in English, while the test example is in the target language. In this case, the model learns how to perform the task using few-shot examples in English and generates a response for the task in the target language.

% As shown in Figure \ref{fig:monoprompting}, in the monolingual prompting strategy, the entire prompt is in the target language, with the few-shot examples also coming from the target language. 

% The choice of prompt can greatly influence the performance of generative models, and models have been shown in the past to be brittle to prompting variations such as the words used in the prompt, number of few-shot examples, ordering of examples etc CITE. We used four prompting strategies for all our experiments - translate-test, zero-shot prompting, cross-lingual translated prompting and monolingual prompting. To illustrate the differences between the prompting methods, we use the following terminology: the \textit{instructions} part of the prompt contains the instructions on how to perform the task, along with \textit{few-shot examples}. The \textit{test example}
 % part of the prompt contains the data point for which we want the response from the model. We used the Bing Translator to perform the automatic translation in all our experiments. 

% \subsubsection{Translate-test}

% As shown in Figure \ref{fig:translate_test}, the Translate-test setting translates test example into English and uses the English instructions along with few-shot examples from English data.

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=18cm]{figures/translate_test.jpg}
%     \caption{Translate-test}
%     \label{fig:translate_test}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \begin{subfigure}[t]{0.32\textwidth}
%     \includegraphics[width=0.95\textwidth]{figures/monolingual_prompting.jpg}
%     \caption{Monolingual Prompting}
%     \label{fig:monoprompting}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.32\textwidth}
%     \includegraphics[width=0.95\textwidth]{figures/zero_shot_prompting.jpg}
%     \caption{Zero-Shot Cross-Lingual Prompting}
%     \label{fig:zsprompting}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.32\textwidth}
%     \includegraphics[width=0.95\textwidth]{figures/translate_test.jpg}
%     \caption{Translate-Test Prompting:}
%     \label{fig:translate_test}
%     \end{subfigure}
% \end{figure*}


% \subsubsection{Zero-shot prompting}

% Figure \ref{fig:zsprompting} illustrates the zero-shot prompting technique, in which the few-shot examples are in English, while the test example is in the target language. In this case, the model learns how to perform the task using few-shot examples in English and generates a response for the task in the target language.

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=18cm]{figures/zero_shot_prompting.jpg}
%     \caption{Zero-shot prompting}
%     \label{fig:zsprompting}
% \end{figure*}

% \subsubsection{Monolingual translated prompting}

 

% \subsubsection{Monolingual prompting}

% As shown in Figure \ref{fig:monoprompting}, in the monolingual prompting strategy, the entire prompt is in the target language, with the few-shot examples also coming from the target language. 

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=18cm]{figures/monolingual_prompting.jpg}
%     \caption{Monolingual prompting}
%     \label{fig:monoprompting}
% \end{figure*}


\subsection{Few-shot examples}

In all our experiments, we choose few-shot examples randomly from the development set available in the dataset, unless specified. Better choices of few-shot examples for the tasks can lead to higher performance, which we leave for future work.

% how we chose the few shot examples and the different design choices that can be made here

% Issues with prompt length - cannot stuff more few shot examples in some languages. just mention here and can go into more detail in the tokenizer discussion

\subsection{Choice of prompts}

% Task-specific choice of prompts

% how we went about choosing the prompt for each task, using promptsource, optimizing for english, can add the potential drawbacks of doing so here or in discussion.

For each task that we consider in our benchmarking study, we need to come up with a prompt that specifies the instruction that the model should follow. We use PromptScource\cite{bach-etal-2022-promptsource} from the BigScience community to use the existing prompts or to create new prompts for the tasks \footnote{Hosted version: \url{https://huggingface.co/spaces/bigscience/promptsource}}. PromptSource is a toolkit for creating, sharing, and using natural language prompts. Prompts are saved in standalone structured files and written in a simple templating language called Jinja. 

For all datasets, we evaluate the performance of available English prompts on PromptSource on the English validation set and select the prompt that gives the best performance. This prompt is then used to evaluate the entire test sets for all languages and prompt strategies as described in \ref{sec:prompt_strategies}. 
% 
 Prompts for all of the datasets are included in Appendix \textsection \ref{sec:appendix_propmts}.  There is a possibility that the best prompt for English is not necessarily the best prompt for other or all languages. However, given the computational overhead of prompt selection for each language for these models, we leave this for future work.

We also translate the selected English prompt to the corresponding target language using the Bing translator, and we present the results of these experiments on select tasks in Table \ref{tab:en_v_translated_prompts}. Using the English template outperforms translating to the native language in all four classification tasks. We observe that translation errors often propagate in the form of incorrect syntax and semantics in the prompts, which may influence task performance negatively. To avoid this, we recommend that all automatically translated prompt templates should be verified by a native speaker.

\begin{table}
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{l c c}
    \toprule
     Task & English-Template & Native-Language-Template  \\
     \midrule
     XNLI & \textbf{0.583} & 0.544 \\
     Indic-XNLI & \textbf{0.496} & 0.387 \\
     PAWS-X & \textbf{0.671} & 0.642 \\
     XCOPA & \textbf{0.776} & 0.731 \\
     \bottomrule
    \end{tabular}}
    \caption{Average performance on non-English languages with the Monolingual Prompting strategy using English-Template and Native-Language-Template prompts for the classification tasks for DV003.}
    \label{tab:en_v_translated_prompts}
\end{table}

% \myworries{ToDo: Discuss drawbacks of English finetuning here?}

% \begin{table*}[t!]
% \centering
% \resizebox{\textwidth}{!}{%
% \setlength{\tabcolsep}{4pt}
% \begin{tabular}{c | l | c}
% \toprule
% Dataset(s) & Template $f_{temp}$ & Verbalizer $f_{verb}$ \\ \midrule
% \multirow{3}{*}{NLI} & \texttt{\{premise\}} Based on previous passage,   & \textit{Entailment}: Yes  \\
% & is it true that \texttt{\{hypothesis\}} ?& \textit{Contradiction}: No\\
% & Yes, No, or Maybe? & \textit{Neutral}: Maybe \\
% \midrule
% \multirow{3}{*}{PAWS-X}                                         & Sentence 1: \texttt{\{sentence1\}} Sentence 2: \texttt{\{sentence2\}} & \textit{Positive}: Yes \\
% & Question: Does Sentence 1 paraphrase Sentence 2 ? &  \textit{Negative}: No\\
% & Yes or No? &\\
% \midrule
% \multirow{5}{*}{XCOPA}     &
% \texttt{\{ premise \}} &        \\ 
% & \texttt{\{\% if question == "cause" \%\}} This happened  because & \texttt{\{choice1\}}: choice1 \\
% &\texttt{\{\% else \%\}} As a consequence... \texttt{\{\% endif \%\}} & \texttt{\{choice2\}}: choice2  \\
% &  Help  me pick the more plausible option: &   \\
% & choice1: \texttt{\{choice1\}}, choice2: \texttt{\{choice2\}} & \\
% \midrule
% \multirow{3}{*}{QA} & \texttt{\{context\}}  & \\
% & Q: \texttt{\{question\}} & Identity \\
% & Referring to the passage above, the correct answer to the given question is& \\
%   \bottomrule
% \end{tabular}}
% \caption{Prompt type and prompt used for each dataset.}
% \label{tab:promptsource}
% \end{table*}

% \begin{table*}[]
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}llll@{}}
% \toprule
% Dataset(s) &
%   \multicolumn{2}{c}{Template  $f_{temp}$}&
%   Verbalizer $f_{verb}$\\
%  &
%   System &
%   User &
%    \\ \midrule

%    \begin{tabular}[c]{@{}l@{}}XNLI, IndicXNLI,\\  GLUECoS NLI\end{tabular}
%  &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to solve Natural Language Inference (NLI) problems. \\ NLI is the task of determining the inference relation between two (short, ordered) texts: entailment, \\ contradiction, or neutral.  Answer as concisely as possible in the same format as the examples below:\end{tabular} &
%   \texttt{\{premise\}} Question: \texttt{\{hypothesis\}} True, False, or Neither? &
%   \begin{tabular}[c]{@{}l@{}}Entailment : True, \\ Contradiction: False,\\ Neutral: Neither\end{tabular} \\
% PAWS-X &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to perform Paraphrase Identification. The goal of \\ Paraphrase Identification is to  determine whether a pair of sentences have the same meaning. \\ Answer as concisely as possible in the same format as the examples below:\end{tabular} &
%   \{sentence1\} Question: \{sentence2\} True or False? &
%   - \\
% XCOPA &
%   \begin{tabular}[c]{@{}l@{}}You are an AI assistant whose purpose is to perform open-domain commonsense causal reasoning. \\ You will be provided a premise and two alternatives, where the task is to select the alternative that more \\ plausibly has a causal relation with the premise. Answer as concisely as possible in the same format as\\  the examples below:\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}\{ premise \} \\ \{\% if question == "cause" \%\} This happened because... \\ \{\% else \%\} As a consequence... \{\% endif \%\} \\ Help me pick the more plausible option: - \{choice1\} - \{choice2\}\end{tabular} &
%   - \\
% \begin{tabular}[c]{@{}l@{}}XQUAD, TyDiQA,\\ MLQA, IndicQA\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to solve reading comprehension problems. \\ You will be provided questions on a set of passages and you will need to provide the answer as it\\  appears in the passage. The answer should be in the same language as the question and the passage.\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}\{context\} Q: \{question\}Referring to the passage above, \\ the correct answer to the given question is: \{answer\}\end{tabular} &
%   - \\
% XStoryCloze &
%   - &
%   \begin{tabular}[c]{@{}l@{}}\{input\_sentence\_1\} \{input\_sentence\_2\} \{input\_sentence\_3\} \{input\_sentence\_4\}\\ What is a possible continuation for the story given the following options ?\\ Option1: \{sentence\_quiz1\}\textbackslash{}n-Option2: \{sentence\_quiz2\}\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}\{sentence\_quiz1\}: Option1, \\ \{sentence\_quiz2\}: Option2\end{tabular} \\
% PANX &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to perform Named Entity Recognition (NER). \\ NER involves identifying and classifying named entities in a text into predefined categories such as \\ person names, organizations, locations, and others. You will need to use the tags defined below:\\ \textbackslash{}nO means the word doesn’t correspond to any entity.\textbackslash{}nB-PER/I-PER means the word corresponds \\ to the beginning of/is inside a person entity.\textbackslash{}nB-ORG/I-ORG means the word corresponds to the \\ beginning of/is inside an organization entity.\textbackslash{}nB-LOC/I-LOC means the word corresponds to the\\  beginning of/is inside a location entity.\textbackslash{}nDo not try to answer the question! Just tag each token in the sentence.\end{tabular} &
%   \{token\_1 token\_2 ... token\_n\} &
%   \begin{tabular}[c]{@{}l@{}}\{tag\_1\} \{tag\_2\} ... \{tag\_n\}: \\ \{token\_1\}\_\{tag\_1\} \{token\_2\}\_\{tag\_2\}\\  ... \{token\_n\}\_\{tag\_n\}\end{tabular} \\
% UDPOS &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to perform Part of Speech (PoS) Tagging. PoS tagging is the \\ process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on\\  both its definition and its context. You will need to use the tags defined below:\end{tabular} &
%   \{token\_1 token\_2 ... token\_n\} &
%   \begin{tabular}[c]{@{}l@{}}\{tag\_1\} \{tag\_2\} ... \{tag\_n\}: \\ \{token\_1\}\_\{tag\_1\} \{token\_2\}\_\{tag\_2\} ... \\ \{token\_n\}\_\{tag\_n\}\end{tabular} \\

% \begin{tabular}[c]{@{}l@{}}GLUECoS\\ Sentiment Analysis\end{tabular}&

%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to solve Sentiment Analysis problems. Sentiment Analysis \\ is the task of determining whether the sentiment, opinion or emotion expressed in a textual data is: \\ positive, negative, or neutral. Answer as concisely as possible in the same format as the examples below:\end{tabular} &
%   Does the following sentence have a positive, negative or neutral sentiment? \{text\} &
%   - \\
% XLSum &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to summarize any given article. You should summarize all \\ important information concisely in the same language in which you have been provided the document. \\ Following the examples provided below:\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}\{document\} \\ === \\ Write a summary of the text above :\end{tabular} &
%   - \\
% Jigsaw & \begin{tabular}[c]{@{}l@{}} You are an NLP assistant whose purpose is to solve the Jigsaw Toxicity Prompt problem. Toxicity is defined \\ as anything rude, disrespectful or otherwise likely to make someone leave a discussion. The objective is to \\ identify toxic comments. Answer as concisely as possible in the same format as the examples below: \end{tabular}
%    & \begin{tabular}[c]{@{}l@{}} Should this online comment be removed for its toxicity \{answer\_choices[1]\} \\ or \{answer\_choices[0]\}? \end{tabular}
%    &
%    \\
% WinoMT & -
%    & Translate the following English text to \{target\_language\}: \{sentence\}
%    & -
%    \\ \bottomrule
% \end{tabular}%
% }
% \caption{Prompt type and prompt used for each task.}
% \label{tab:prompt_template}
% \end{table*}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \begin{table*}[]
% \renewcommand{\arraystretch}{1.3}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}llll@{}}
% \toprule
% Dataset(s) & Instruction $\mathcal{I}$ & Template $f_{temp}$ & Verbalizer $f_{verb}$ \\
%    \\ \midrule
% \begin{tabular}[c]{@{}l@{}}XNLI, IndicXNLI, \\ GLUECoS NLI\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to solve Natural Language Inference (NLI) problems. \\ NLI is the task of determining the inference relation between two (short, ordered) texts: entailment, \\ contradiction, or neutral.  Answer as concisely as possible in the same format as the examples below:\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}} \texttt{\{premise\}}\\ Question: \texttt{\{hypothesis\}} \\ True, False, or Neither?\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}Entailment : True, \\ Contradiction: False,\\ Neutral: Neither\end{tabular} \\ \midrule
% % PAWS-X &
% %   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to perform Paraphrase Identification. The goal of \\ Paraphrase Identification is to  determine whether a pair of sentences have the same meaning. \\ Answer as concisely as possible in the same format as the examples below:\end{tabular} &
% %   \begin{tabular}[c]{@{}l@{}} \texttt{\{sentence1\}} \\ Question: \texttt{\{sentence2\}} \\ True or False?\end{tabular} &
% %   - \\ \midrule
% % XCOPA &
% %   \begin{tabular}[c]{@{}l@{}}You are an AI assistant whose purpose is to perform open-domain commonsense causal reasoning. \\ You will be provided a premise and two alternatives, where the task is to select the alternative that more \\ plausibly has a causal relation with the premise. Answer as concisely as possible in the same format as\\  the examples below:\end{tabular} &
% %   \begin{tabular}[c]{@{}l@{}} \texttt{\{ premise \}} \\ \texttt{\{\% if question == "cause" \%\}} This happened because... \\ \texttt{\{\% else \%\}} As a consequence... \texttt{\{\% endif \%\}} \\ Help me pick the more plausible option: - \texttt{\{choice1\}} - \texttt{\{choice2\}} \end{tabular} &
% %   - \\ \midrule
% \begin{tabular}[c]{@{}l@{}}XQUAD, TyDiQA,\\ MLQA\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to solve reading comprehension problems. \\ You will be provided questions on a set of passages and you will need to provide the answer as it\\  appears in the passage. The answer should be in the same language as the question and the passage.\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}} \texttt{\{context\}} \\ Q: \texttt{\{question\}}\\ Referring to the passage above,  the correct answer to the given question is: \texttt{\{answer\}}\end{tabular} &
%   \textit{Identity} \\ \midrule
% % XStoryCloze &
% %   - &
% %   \begin{tabular}[c]{@{}l@{}}\texttt{\{input\_sentence\_1\} \{input\_sentence\_2\} \{input\_sentence\_3\} \{input\_sentence\_4\}}\\ What is a possible continuation for the story given the following options ?\\ Option1: \texttt{\{sentence\_quiz1\}} Option2: \texttt{\{sentence\_quiz2\}}\end{tabular} &
% %   \begin{tabular}[c]{@{}l@{}}\texttt{\{sentence\_quiz1\}}: Option1, \\ \texttt{\{sentence\_quiz2\}}: Option2\end{tabular} \\ \midrule
% % PANX &
% %   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to perform Named Entity Recognition (NER). \\ NER involves identifying and classifying named entities in a text into predefined categories such as \\ person names, organizations, locations, and others. You will need to use the tags defined below:\\ O means the word doesn’t correspond to any entity. B-PER/I-PER means the word corresponds \\ to the beginning of/is inside a person entity. B-ORG/I-ORG means the word corresponds to the \\ beginning of/is inside an organization entity. B-LOC/I-LOC means the word corresponds to the\\  beginning of/is inside a location entity. Do not try to answer the question! Just tag each token in the sentence.\end{tabular} &
% %   \texttt{\{token\_1 token\_2 ... token\_n\}} &
% %   \begin{tabular}[c]{@{}l@{}}\texttt{\{tag\_1\} \{tag\_2\} ... \{tag\_n\}}: \\ \texttt{\{token\_1\}\_\{tag\_1\} \{token\_2\}\_\{tag\_2\}}\\  ... \texttt{\{token\_n\}\_\{tag\_n\}}\end{tabular} \\ \midrule
% % UDPOS &
% %   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to perform Part of Speech (PoS) Tagging. PoS tagging is the \\ process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on\\  both its definition and its context. You will need to use the tags defined below:\end{tabular} &
% %   \texttt{\{token\_1 token\_2 ... token\_n\}} &
% %   \begin{tabular}[c]{@{}l@{}}\texttt{\{tag\_1\} \{tag\_2\} ... \{tag\_n\}}: \\ \texttt{\{token\_1\}\_\{tag\_1\} \{token\_2\}\_\{tag\_2\} ...} \\ \texttt{\{token\_n\}\_\{tag\_n\}}\end{tabular} \\ \midrule
% % \begin{tabular}[c]{@{}l@{}}GLUECoS\\ Sentiment Analysis\end{tabular} &
% %   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to solve Sentiment Analysis problems. Sentiment Analysis \\ is the task of determining whether the sentiment, opinion or emotion expressed in a textual data is: \\ positive, negative, or neutral. Answer as concisely as possible in the same format as the examples below:\end{tabular} &
% %   Does the following sentence have a positive, negative or neutral sentiment? \texttt{\{text\}} &
% %   - \\ \midrule
% XLSum &
%   \begin{tabular}[c]{@{}l@{}}You are an NLP assistant whose purpose is to summarize any given article. You should summarize all \\ important information concisely in the same language in which you have been provided the document. \\ Following the examples provided below:\end{tabular} &
%   \begin{tabular}[c]{@{}l@{}}\texttt{\{document\}} \\ === \\ Write a summary of the text above :\end{tabular} &
%   \textit{Identity} \\ \midrule
% Jigsaw & \begin{tabular}[c]{@{}l@{}} You are an NLP assistant whose purpose is to solve the Jigsaw Toxicity Prompt problem. Toxicity is defined \\ as anything rude, disrespectful or otherwise likely to make someone leave a discussion. The objective is to \\ identify toxic comments. Answer as concisely as possible in the same format as the examples below: \end{tabular}
%   \textit{Identity} &
%   \begin{tabular}[c]{@{}l@{}}Should this online comment be removed for its toxicity? \\ Yes or No?\end{tabular} &
%   Toxic: Yes, Non-Toxic: No \\ \bottomrule
% % WinoMT &
% %   - &
% %   Translate the following English text to \texttt{\{target\_language\}}: \texttt{\{sentence\}} &
%   % - \\ \bottomrule
% \end{tabular}%
% }
% \caption{Prompt template, instruction, and the verbalizers used for a set of representative tasks for evaluating GPT-3.5-Turbo and GPT-4. For details on all the prompts that we use for our evaluation, please refer to \textsection \ref{sec:appendix_propmts} in Appendix.}
% \label{tab:prompt_template}
% \end{table*}