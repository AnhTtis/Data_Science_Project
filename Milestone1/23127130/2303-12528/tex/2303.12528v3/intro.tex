\section{Introduction}
Generative Large Large Models (LLMs) such as ChatGPT have created a lot of interest in the AI community and beyond due to the step jump in their capabilities, such as maintaining context over conversations, fluency of generation and reasoning. Many users have reported having tested these systems on languages other than English, with varying results, and recent demos of these models (such as Bing+GPT) \cite{newbingevent} have been shown in multiple (albeit high-resource) languages. Recently, the GPT-4 model \cite{gpt4techreport} was evaluated on the MMLU multiple choice questions benchmark by automatically translating it into 26 languages, and the results for some low-resource languages in the Latin script was found to be promising. 

GPT-3 contains unlabeled pre-training data in 90 languages \cite{brown-etal-2020-language}, while the BLOOM \cite{scao2022bloom} model created by the BigScience community is trained on 46 natural languages, and Google's PaLM \cite{chowdhery2022palm} model is trained on 122 languages, with the latest PaLM 2 model \cite{palm2techreport} being trained on larger multilingual corpora and parallel data. While these models have been trained on multiple languages with varying distributions in the pre-training data, it is not clear how well they perform relative to each other across diverse tasks and languages due to a lack of comprehensive analysis across all models with the same experimental setup. Further, the datasets used for evaluation are often obtained by machine translation of English test sets, leading to a shortage of evaluation results on more natural multilingual data.

Figures \ref{fig:langdistrgpt}, \ref{fig:langdistrbloom} and \ref{fig:langdistrpalm} show the distribution of languages in the pre-training data of GPT3, BLOOM and PaLM respectively. We observe that a significant proportion of the data is in the English language for all three models, with even the BLOOM model, which aims to be a multilingual model, having around 40\% data in English and code. It is unclear how much data is present in the tail languages in these models, and more crucially, what the quality of the data is, which makes it difficult to ascertain whether these models are ready to be deployed for these languages. \citet{joshi2020state} reviewed the challenges involved in promoting linguistic diversity, such as the lack of resources and infrastructure for developing NLP tools for underrepresented languages. They provided an overview of the efforts being made to promote linguistic diversity and inclusion including the creation of data sets and tools for underrepresented language and provide a classification of languages based on the amount of labeled and unlabeled data available for languages. This study finds that most of the world's population is under-served in terms of availability of data for their languages, raising important questions about what the advent of generative Large Language Models (LLMs) means for the state of language coverage and inclusivity. 

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/lang_comp_gpt.pdf}
    \caption{}
    \label{fig:langdistrgpt}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.32\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/lang_comp_bloom.pdf}
    \caption{}
    \label{fig:langdistrbloom}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.32\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/lang_comp_palm.pdf}
    \caption{}
    \label{fig:langdistrpalm}
    \end{subfigure}
    \caption{Language distribution of pre-training data for different LLMs. For GPT-3 we report the percentage of the total words of a language in the pre-training corpus\footnote{\url{https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv}}, disk-size for BLOOM, and number of tokens for PALM. We label top-6 represented languages for each model.}
\end{figure}

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=12cm]{figures/Life-Architect-GPT-3-90-languages.png}
%     \caption{Language Distribution in LLMs - GPT3}
%     \label{fig:langdistrgpt}
% \end{figure*}

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=12cm]{figures/Life-Architect-BLOOM-46-languages.png}
%     \caption{Language Distribution in LLMs - BLOOM}
%     \label{fig:langdistrbloom}
% \end{figure*}

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=12cm]{figures/Life-Architect-PaLM-122-languages.png}
%     \caption{Language Distribution in LLMs - PALM}
%     \label{fig:langdistrpalm}
% \end{figure*}

Robust and comprehensive evaluation helps us in understanding the capabilities of AI systems. Evaluation of LLMs has been an active area of research, with benchmarks such as GLUE \cite{wang2018glue} and SuperGLUE \cite{wang2019superglue} proposed a few years ago for English, followed by more recent benchmarks for multilingual evaluation, such as XTREME \cite{hu2020xtreme}, XTREME-R \cite{ruder2021xtreme} and XGLUE \cite{liang2020xglue}. Multilingual benchmarks aim to cover a variety of tasks and languages, so that we can determine whether a foundation model is able to generalize. Benchmarks covering specific language families, such as IndicXTREME \cite{doddapaneni2022indicxtreme} for Indian languages, \citet{adelani-etal-2022-masakhaner} for African Languages,  \citet{wilie2020indonlu} for Indonesian languages and GLUECoS \cite{khanuja2020gluecos} and LinCE \cite{aguilar2020lince} for code-switching have also been proposed.

% However, evaluating generative AI models is challenging due to limitations in automatic overlap based metrics such as BLEU and ROUGE and the open-ended nature of conversations that can be had with such systems \cite{Graham2022}. 
In prior work, we have described the challenges of scaling up multilingual evaluation \cite{ahuja-etal-2022-beyond} due to the lack of representative datasets covering languages from different language families, typological diversity of languages in benchmark datasets and coverage of low-resource languages. We have also pointed out the limitations of using translated datasets for evaluation (such as the one that has been used to evaluate GPT4 in the technical report), due to the loss of local and cultural context in such benchmarks \cite{ahuja-etal-2022-beyond, ahuja2022economics}. However, it is crucial that we improve multilingual evaluation of LLMs, because prior work shows that good performance or gains on some high resource languages may not always result in similar gains in all languages \cite{wu2020all}, with various factors possibly influencing the performance of models on different languages \cite{litmus}.

Recently, there have been attempts at holistic evaluation of generative AI models for English (HELM) \cite{liang2022holistic}, in which several LLMs are evaluated on multiple dimensions such as accuracy, calibration, robustness, fairness, bias etc. \citet{bang2023multitask} conduct an evaluation of ChatGPT across tasks and languages. They evaluate ChatGPT on Sentiment Analysis, Language Identification and Machine Translation and find that it fails to identify low-resource languages, and generate non-Latin script languages. \citet{hendy2023good} evaluate the translation abilities of three GPT models - ChatGPT, GPT3.5 (text-davinci-003) and text-davinci-002 and find that GPT models perform well on translating high-resource languages, and do not perform well on low-resource languages. Recent work \cite{bubeck2023sparks}, has pointed out the limitations of using standard NLP benchmarks to evaluate generative models, due to the pace at which these benchmarks become saturated. However, in this work, we show that multilingual NLP benchmarks are far from saturated, and hence, benchmarking is a viable approach to get a sense of the performance of such models on different languages. 

In this work, we carry out a comprehensive Multilingual Evaluation of Generative AI (MEGA). We quantify how well generative LLMs perform across languages across standard multilingual benchmarks covering various standard NLP tasks such as Natural Language Inference, Commonsense Reasoning, Question Answering, Summarization, and Sequence Labelling. Our goal is to conduct a comprehensive benchmarking study for many languages of the world, along the lines of HELM \cite{liang2022holistic}, and we plan to benchmark other dimensions of interest such as calibration, bias, robustness etc. in future versions of the study. This is very challenging due to the scarcity of datasets available in non-English languages for measuring fairness, toxicity and bias as we discuss in prior work \cite{ramesh2023fairness}, however it is important to evaluate all these dimensions across languages to ensure that we build responsible and accurate systems for all.

We aim to answer the following questions: 

\begin{itemize}
    \item How well do generative models understand instructions, perform tasks and generate text in different languages?
    \item Which languages do generative models perform well in?
    \item How well do generative models such as GPT3, GPT4 and BLOOM fare on standard multilingual benchmarks compared to SOTA models, such as Turing ULR on standard NLP tasks? 
    \item  Which prompting strategies should be used for using generative LLMs for non-English languages?
    \item  What are the challenges we foresee in making generative AI work accurately for all languages of the world? 
\end{itemize} 

We evaluate the following OpenAI models: \texttt{text-davinci-003} (referred to as DV003 in the paper), \texttt{gpt-3.5-turbo} (referred to as GPT-3.5-Turbo in the paper), and \texttt{gpt-4-32k} (referred to as GPT-4 in the paper) on 16 NLP datasets across $\sim70$ languages, shown in Table \ref{tab:datasets} and compare its performance to BLOOMZ \cite{muennighoff2022crosslingual} and SOTA models for specific tasks, such as TULRv6 \cite{patra2022beyond} and MuRIL \cite{khanuja2021muril}. In addition, we compare various prompting strategies, including the translate-test setting (round-tripping through English by translating into English, querying in English and back-translating if applicable).

Our work is the first comprehensive benchmarking of generative AI models across models, datasets, languages and prompting strategies. In addition to answering the questions we pose above, our study also results in a blueprint of strategies that can be used for building systems using generative AI for multilingual users. Another contribution of this work is the MEGA benchmarking framework that can be used for evaluation of a model or system across languages, and for the research community to scale up multilingual evaluation of generative models that come in the future. We plan to release the MEGA benchmarking code to facilitate this.