% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2022}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{subfloat}
\usepackage{booktabs} % To thicken table lines
\usepackage{multirow}
\usepackage{amsmath,amssymb}
\usepackage{cuted}
\usepackage{flushend}
\usepackage{tablefootnote}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{subfloat}
\usepackage{wrapfig}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{pifont}
% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{xcolor}
\newcommand\myworries[1]{\textcolor{red}{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\E}{\mathbb{E}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% If the title and author information does not fit in the area allocated, uncomment the following
%
\setlength\titlebox{6cm}
%
% and set <dim> to something 5cm or larger.

\title{MEGA: Multilingual Evaluation of Generative AI}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%will change author order later
\author{Kabir Ahuja \quad Harshita Diddee \quad Rishav Hada \quad Millicent Ochieng  
\AND  Krithika Ramesh \quad Prachi Jain \quad Akshay Nambi  \quad Tanuja Ganu \\
 \AND Sameer Segal \quad Maxamed Axmed \quad Kalika Bali \quad Sunayana Sitaram \\ 
 \\
 Microsoft Corporation \\ \\
 {\tt Contact: sunayana.sitaram@microsoft.com}}

%Outline
%1. Intro:
    % - Motivate the problem, how cross lingual transfer has been studied w.r.t to accuracy but not calibration
    % - why calibration is important
    % - some related work on calibration
    % - contributions
    %     - 1. We show that the models are badly calibrated for langauges other than english in zero-shot setting specially for low resource languages
    %     - 2. Investigate factors influencing the calibration, rersourcefulness, typological diversity, choice of model, choice of task, number of epochs, choice of pivot language.
    %     - 3. Standard Calibration techniques leads to substantial improvement in calibration errors, and collecting a few examples in the target can not only improve the performance but substantially improve calibration as well.
% 2 Calibration of Neural Network based Classifiers:
%     - Define what calibration is,
%     - Formulas for ECE and CACE
%     2.1 Experimental Setup
%         - Tasks and Datasets
%         - Models
%         - Calibration Methods
%     2.2 
        

\begin{document}
\maketitle
\begin{abstract}
Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field. 

% Next, we show that standard calibration methods like Temperature Scaling and Label Smoothing can be used to substantially improve calibration in the zero-shot scenario which can be even further improved by collecting few-shot examples in those languages. Overall, our work provides a step towards building more reliable multilingual models by taking into account their calibration in addition to their performance across languages. %Overall, our work contributes towards understanding and building more reliable multilingual models by highlighting calibration as an issue in these models, which should be considered along with performance while deploying them into production.


% Overall, our work provides a step towards building more reliable multilingual models by taking into account their calibration in addition to their performance across languages.

%Overall, our work provides a step towards building more reliable multilingual models and draw attention of the community to consider calibration of these models along with their performance while deploying them into production

\end{abstract}

\input{intro}
\input{expts}
\input{results}
\input{discussion}
% \input{v2}
\input{conclusion}
\input{limitations}


% \section*{Ethics Statement}



% Scientific work published at EMNLP 2022 must comply with the \href{https://www.aclweb.org/portal/content/acl-code-ethics}{ACL Ethics Policy}. We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

% \section*{Acknowledgements}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\input{appendix}

\end{document}
