\section{Experiments}

In this section, we describe how we adapt various NLP tasks to the in-context learning setting. We describe the prompting strategies we use for the benchmark and the models, tasks and datasets included in our initial study.

\subsection{Problem Formulation}

In order to solve different tasks via in-context learning we adopt the prompt-based few-shot learning strategy as defined in \cite{brown-etal-2020-language}. We define four main components of the prompts that we use in our experiments as follows: i) a \textbf{test example} $x_{test}$ for which the predictions are to be made; ii) $K$ \textbf{few-shot exemplars} $\{(x_i, y_i)\}_{i = 1}^{K}$, that are used to provide in-context supervision to the model; iii) a \textbf{prompt template} $f_{temp}(x)$ which turns a dataset input example into a text format that can 
 be used for prompting, containing the task description; and iv) an \textbf{answer verbalizer} $f_{verb}(y)$ that maps the label $y$ to a textual representation. In our evaluation framework we often consider the template and verbalizer as a single entity, and from now on will denote the template to encapsulate both the template and verbalizer unless specified separately. Some examples of $f_{temp}$ and $f_{verb}$ are given in table \ref{tab:promptsource}.



Given these components, the final prompt $f_{prompt}(x_{test}; \{(x_i, y_i)\}_{i = 1}^{K}, f_{temp}, f_{verb})$ or $f_{prompt}(x_{test})$ for short for a test input $x_{test}$ can be defined as:
\begin{align*}
f_{prompt}(x_{test}) =\mathbin\Vert_{i = 1}^{K} \big\{f_{temp}(x_i)&\mathbin\Vert f_{verb}(y_i)\big\}\\
&\mathbin\Vert f_{temp}(x_{test})
\end{align*}

where $\mathbin\Vert$ denotes the string concatenation operator.

The prompt can then be provided as input to the LLM $P(.;\theta)$ to obtain the prediction $z_{test}$

\begin{align*}
    z_{test} = \argmax_{z \in \mathcal{Z}} P(z | f_{prompt}(x_{test}); \theta)
\end{align*}

where $\mathcal{Z}$ is the space of possible answers, which in all of our experiments is taken to be the entirety of the language as modeled by the LLM. We approximate the $\argmax$ by sampling from the probability distribution predicted by the LLM.

The predicted answer $z_{test}$ is compared with the verbalized label using $f_{metric}(z_{test}, f_{verb}(y_{test})) \in [0, 1]$ that measures the extent of similarity between the ground truth and predicted answer. For our experiments, we use the exact-match score to determine accuracy for classification tasks and use the exact-match and F1-score for QA tasks. Formally, the evaluation score $s$ for an LLM $P(.;\theta)$ on a task $\mathcal{T}$ can be defined as:

\begin{equation*}
    s = {\E_{(x_{test}, y_{test}) \in \mathcal{T}}}[f_{metric}(z_{test}, f_{verb}(y_{test}))]
\end{equation*}


\subsection{Prompting Strategies}
\label{sec:prompt_strategies}
The choice of prompt significantly influences the performance of Large Language Models. Generative models have been shown to be brittle to simple prompting variations, such as ordering of examples, number of few-shot examples and the choice of words in the prompt. There are many variations to consider for our setup: the choice of the language of the prompt examples, the language of the prompt template, and the language of the test examples. In this work we evaluate all the models using three types of prompts:

\iffalse
The choice of prompt can greatly influence the performance of generative models, and models have been shown in the past to be brittle to prompting variations such as the words used in the prompt, number of few-shot examples, ordering of examples etc CITE. Our setup in particular involves the choice of the few-shot examples $\{(x_i, y_i)\}_{i = 1}^{K}$ as well as  choice of different template $f_{temp}$ and verbalizer $f_{verb}$ functions, for defining the prompt. For our evaluation framework we consider two higher level decisions which stem from the choice of language in which the few-shot examples and the text examples are represented and the language in which the templates are written. For the former in particular we consider three setups:

\fi
\noindent
\begin{itemize}
    \item \textbf{Monolingual Prompting}: In this setup, the k\footnote{k=8, unless specified} randomly selected examples are of the same language as the test examples. Figure \ref{fig:monoprompting} illustrates an example of monolingual prompting in hindi.
    \item \textbf{Zero-Shot Cross-Lingual Prompting}: Pre-trained multilingual models are effective at zero-shot cross-lingual transfer \cite{pires-etal-2019-multilingual, wu-dredze-2019-beto}, that is on fine-tuning them for a task in one language leads to reasonable performance on unseen languages. In this section, we evaluate the model's zero-shot cross-lingual transfer ability after in-context learning. In this experiment, we use k-shot examples from a pivot language\footnote{We use english as the pivot language in this paper} which is different from the language of the test example. Figure \ref{fig:zsprompting} illustrates the setup for a hindi test query.
    \item \textbf{Translate-Test Prompting}: This setup is similar to the Zero-Shot Cross-Lingual setup in the fact that the few-shot examples are sampled from English data. However, here we modify the test example itself by translating it to English.  Translate-test has been shown to be often better than cross-lingual transfer for both fine-tuning \cite{ponti-etal-2021-modelling} and in-context learning \cite{lin-etal-2022-shot, shi-etal-2022-language} and hence we explore its effectiveness for our benchmarking exercise as well. An example for this setup for Hindi is given in Figure \ref{fig:translate_test}, we use Bing Translator to translate the test examples to English.
\end{itemize}

\iffalse
\noindent
\textbf{2. Zero-Shot Cross-Lingual Prompting}: Pre-trained multilingual models have been shown to be surprisingly effective at zero-shot cross lingual transfer \cite{pires-etal-2019-multilingual, wu-dredze-2019-beto}, where fine-tuning them for a task in one language leads to reasonable performance on unseen languages. In this setup, we try to probe to what extent LLMs can exhibit this behavior via in-context learning. Hence, the few-shot examples here are selected from a language (we call it pivot language) different from the language of the test example and for the purposes of our experiments we use English as the pivot language. Refer to Figure \ref{fig:zsprompting} for an example of this setup for Hindi.

\noindent
\textbf{3. Translate-Test Prompting}: This setup is similar to the Zero-Shot Cross-Lingual setup in the fact that the few-shot examples are sampled from English data. However, here we modify the test example itself by translating it to English.  Translate-test has been shown to be often better than cross-lingual transfer for both fine-tuning \cite{ponti-etal-2021-modelling} and in-context learning \cite{lin-etal-2022-shot, shi-etal-2022-language} and hence we explore its effectiveness for our benchmarking exercise as well. An example for this setup for Hindi is given in Figure \ref{fig:translate_test}. We use Bing Translator to translate the test examples to English in our experiments.
\fi

For the choice of language for the prompt template, we consider the following two setups:
\noindent
\begin{itemize}
    \item \textbf{English-Template}: Here the prompt templates are written in English, irrespective of the language of the few-shot and test examples. As has been shown in \citet{shi-etal-2022-language}, English instructions can often perform on par or even better than providing them in the native language. 
    \item \textbf{Native-Language-Template}: Here the prompt templates are written in the language of the test example $(x_{test}, y_{test})$. For our experiments, we use Bing Translator to translate the prompts from English to the native language.
\end{itemize}

In our initial experiments, we used different languages for task templates in few-shot examples and the test example but it performs poorly. We speculate such a poor performance of the model, as it may be getting confused about which language to generate the predictions in. We also found English templates to perform better than native language prompts. Hence we use English prompts for all our experiments.

\iffalse
\begin{table}[]
    \centering
    \begin{tabular}{p{2.5cm}p{2cm}p{2cm}}
        \toprule
         &  \textbf{English-Template} & \textbf{Native-Language-Template}\\
         \midrule
         Monolingual & \checkmark & \checkmark\\
         Zero-Shot Cross-Lingual & \checkmark & \xmark \\
         Translate-Test & \checkmark & \xmark\\
         \bottomrule
    \end{tabular}
    \caption{Combinations of the two higher level prompting setups.}
    \label{tab:prompting_setups}
\end{table}

Table \ref{tab:prompting_setups} provides the possible combinations of the two classes of prompting strategies. We only allow Native-Language templates for Monolingual setup, as in the other two setups the few-shot examples are in English. In our initial experiments, we tried using different language templates for few-shot examples and the test example but found that it performs poorly, as it would often lead to the model being confused about which language to generate the predictions in.
\fi
\subsection{Models}

We conduct all benchmarking experiments on OpenAI's GPT text-davinci-003 \cite{brown-etal-2020-language} model, which is available via API access. We do not conduct any fine-tuning of the model for our benchmarking experiments or carry out hyperparameter tuning for temperature or other settings.

We compare the performance of DV003 with the following models: BLOOMZ \cite{muennighoff2022crosslingual}, a fine-tuned version of the BLOOM \cite{scao2022bloom} model, which is a 176 parameter model created by the BigScience community trained on 46 natural languages and 13 programming languages. We also compare DV003's performance against SOTA non-autoregressive models such as TULRv6 \cite{patra2022beyond} and MuRIL \cite{khanuja2021muril} for the Indic benchmarks. The TULRv6 model is trained with a novel sampling strategy and bitexts in multiple languages and is currently at the top position on the XTREME \cite{hu2020xtreme} benchmark as of writing this paper. MuRIL is multilingual BERT model trained on 16 Indic languages and obtains SOTA performance on some Indic benchmarks. 

\subsection{Tasks and Datasets}

In our experiments, we consider two broad families of NLU tasks, i) Classification and ii) Question Answering. Below we review the experimental setups and datasets used for benchmarking for these two tasks. A list of all the datasets with the languages covered by them can be found in Table \ref{tab:datasets}.

\subsubsection{Classification}
These tasks involve classifying a single sentence or a group of sentences into a finite number of discrete labels. For each dataset, we measure the performance of different models in terms of classification accuracy. For prompt-based models in particular, since we add no constraint on the output space of the LLM we compute the exact match between the generated output and a verbalized label to determine if the example was classified correctly. We run experiments for all the prompting strategies that we discussed in the previous sections for each dataset. The details of each dataset that we use for benchmarking are given below:

 \begin{table*}[h]
 \small
     \centering
     \begin{tabular}{ccc}
     \toprule
    Dataset&Task&Languages\\
    \midrule
    XNLI&Natural Language Inference&en, fr, es, de, el, bg, ru, tr, ar, vi, th, zh, hi, sw, ur\\
    Indic-XNLI&Natural Language Inference&as, bn, gu, hi, kn, ml, mr, or, pa, ta, te\\
    %Indic-WNLI&Natural Language Inference&gu, hi, mr\\
    %GLUECoS-NLI&Natural Language Inference&hi-en\\
    %GLUECoS-En-Es-Sentiment&Sentiment Analysis&es-en\\
    PAWS-X&Paraphrase Identification&zh, fr, de, ko, ja, es\\
    XCOPA&Commonsense Reasoning&et, ht, id, it, qu, sw, ta, th, tr, vi, zh\\
    TyDiQA&Question Answering&en, ar, bn, fi, id, ja, sw, ko, ru, te, th\\
    MLQA&Question Answering&de, es, ar, zh, vi, hi\\
    XQUAD&Question Answering&en, es, de, el, ru, tr, ar, vi, th, zh, hi\\
    IndicQA&Question Answering&as, bn, gu, hi, kn, ml, mr, or, pa, ta, te\\
  \bottomrule
     \end{tabular}
     \caption{Datasets and languages}
     \label{tab:datasets}
     \vspace{-0.4cm}
 \end{table*}

\noindent{\textbf{1. Natural Language Inference}}: XNLI \cite{Conneau2018xnli} is a dataset for cross-lingual Natural Language Inference, which consists of professional translations of the MNLI \cite{wang2018glue} corpus into 14 languages. We also consider IndicXNLI \cite{aggarwal2022indicxnli} that translates the XNLI dataset into 11 Indic languages by using Machine Translation, followed by validation by native speakers.

% Indic-WNLI \cite{doddapaneni2022indicxtreme} is a translation of the Winograd NLI dataset \cite{wang2018glue}, an NLI version of the Winograd Schema Challenge into three Indic languages. 

%remove gluecos-nli if not done

% GLUECoS-NLI \cite{khanuja2020new} is a code-mixed NLI dataset in Hindi-English, consisting of Bollywood (Hindi) movie conversations as premises, with manually created hypotheses. 

% \subsubsection{Sentiment Analysis}

% %remove section if not done

% The EN-ES-CS Sentiment Analysis dataset \cite{vilares2016cs}, part of the GLUECoS benchmark \cite{khanuja2020gluecos} is a code-mixed dataset consisting of English-Spanish Tweets annotated with SentiStrength \cite{thelwall2017heart} scores. 

\noindent{\textbf{2. Paraphrase Identification}}: PAWS-X \cite{yang2019paws} is a paraphrase identification dataset professionally translated from the PAWS \cite{zhang2019paws} dataset into six typologically diverse languages. 

\noindent{\textbf{3. Commonsense Reasoning}}: XCOPA \cite{ponti2020xcopa} is a commonsense reasoning dataset, which is a translation of the COPA \cite{roemmele2011choice} dataset into 11 typologically diverse languages, including very low-resource languages such as Eastern ApurÃ­mac Quechua and Haitian Creole.

\subsubsection{Question Answering}
We focus on the Span Prediction type of Question Answering (QA) tasks in our experiments, where given a context and a question the task is to predict the answer within the context. One major challenge that we come across for multilingual evaluation of QA tasks is that for many languages we often cannot fit the context and question pairs for the few-shot and text examples in the maximum context size of 4096 for the DV003 model.

To overcome this issue we follow two steps. First, for the few-shot examples we only provide the line within the paragraph containing the answer as the context. Second, for the test example, we index the chunks of the context using the embeddings from the \texttt{text-embedding-ada-002} model. Given the question, the closest chunk in the full context is retrieved and used in the prompt for the test example. We use a maximum chunk size of 100 in our experiments and use the implementation for retrieval provided in the \textbf{LangChain}\footnote{\url{https://github.com/hwchase17/langchain}} library. By doing this,we minimize the space taken by the context tokens in our prompt.

For each task, we calculate the Exact Match and F1 score as defined in \citet{rajpurkar-etal-2016-squad}.  For our experiments we 
 consider the following three tasks:

\noindent \textbf{1. TyDiQA} \cite{clark2020tydi} is a QA dataset covering 11 typologically diverse languages. The task consists of two sub-tasks - passage selection and minimum answer span (Gold-P). For our experiments, we consider the Gold-P task and evaluate Monolingual and Zero-Shot Cross-Lingual prompting strategies. Since the labels do not directly transfer one-to-one across translation for QA tasks as they do for classification and require the use of alignment algorithms, we skip translate-test prompting for this task.

\noindent \textbf{2. MLQA} \cite{lewis2020mlqa} is an extractive QA dataset translated into 7 languages by professional translators. The task has two variants, the first where the question, context, and answer are all in the same language; and the second, where the question is in a different language than the context and answer. We consider the former variant of the task in our experiments. For MLQA, translate-test splits are also available, where each language's test data has been translated into English with answers aligned using the attention scores. There is no training data available for MLQA, and we use SQuAD's\citet{rajpurkar-etal-2016-squad} training data for selecting few-shot examples in English and validation data for MLQA in other languages to get their few-shot examples. This way, we are able to evaluate for all three prompting setups.

\noindent \textbf{3. XQuAD} \cite{artetxe2020cross} consists of professional translations of a subset of the SQuaD dataset \cite{rajpurkar2016squad} into 10 languages. XQuAD only has validation datasets available publicly, hence we evaluate the models on them. Like MLQA we use English SQuAD data for few-shot examples and since we cannot use validation data in other languages for few-shot, we only evaluate for zero-shot cross-lingual setup for this task.


\noindent \textbf{4. IndicQA} \cite{doddapaneni2022indicxtreme} is a manually curated cloze-style reading comprehension dataset that can be used for evaluating question-answering models in 11 Indic languages. The context paragraphs are chosen from Wikipedia articles whose topics are closely related to Indic culture, history,etc. The publicly available test set has about 2000 sentences that we carry out our evaluation on. 

% IndicQA \cite{doddapaneni2022indicxtreme} is a cloze-style reading comprehension dataset with context taken from Wikipedia articles on Indian culture and history, manually created in 11 Indic languages. 


% the few-shot examples are selected to belong to the same language as the test example to be evaluated. An example of the same for Hindi is shown in Figure \ref{fig:monoprompting}.

% Figure \ref{fig:zsprompting} illustrates the zero-shot prompting technique, in which the few-shot examples are in English, while the test example is in the target language. In this case, the model learns how to perform the task using few-shot examples in English and generates a response for the task in the target language.

% As shown in Figure \ref{fig:monoprompting}, in the monolingual prompting strategy, the entire prompt is in the target language, with the few-shot examples also coming from the target language. 

% The choice of prompt can greatly influence the performance of generative models, and models have been shown in the past to be brittle to prompting variations such as the words used in the prompt, number of few-shot examples, ordering of examples etc CITE. We used four prompting strategies for all our experiments - translate-test, zero-shot prompting, cross-lingual translated prompting and monolingual prompting. To illustrate the differences between the prompting methods, we use the following terminology: the \textit{instructions} part of the prompt contains the instructions on how to perform the task, along with \textit{few-shot examples}. The \textit{test example}
 % part of the prompt contains the data point for which we want the response from the model. We used the Bing Translator to perform the automatic translation in all our experiments. 

% \subsubsection{Translate-test}

% As shown in Figure \ref{fig:translate_test}, the Translate-test setting translates test example into English and uses the English instructions along with few-shot examples from English data.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=18cm]{figures/translate_test.jpg}
    \caption{Translate-test}
    \label{fig:translate_test}
\end{figure*}

% \subsubsection{Zero-shot prompting}

% Figure \ref{fig:zsprompting} illustrates the zero-shot prompting technique, in which the few-shot examples are in English, while the test example is in the target language. In this case, the model learns how to perform the task using few-shot examples in English and generates a response for the task in the target language.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=18cm]{figures/zero_shot_prompting.jpg}
    \caption{Zero-shot prompting}
    \label{fig:zsprompting}
\end{figure*}

% \subsubsection{Monolingual translated prompting}

 

% \subsubsection{Monolingual prompting}

% As shown in Figure \ref{fig:monoprompting}, in the monolingual prompting strategy, the entire prompt is in the target language, with the few-shot examples also coming from the target language. 

\begin{figure*}[h!]
    \centering
    \includegraphics[width=18cm]{figures/monolingual_prompting.jpg}
    \caption{Monolingual prompting}
    \label{fig:monoprompting}
\end{figure*}


\subsection{Few-shot examples}

In all our experiments, we choose few-shot examples randomly from the development set available in the dataset, unless specified. Better choices of few-shot examples for the tasks can lead to higher performance, which we leave for future work.

% how we chose the few shot examples and the different design choices that can be made here

% Issues with prompt length - cannot stuff more few shot examples in some languages. just mention here and can go into more detail in the tokenizer discussion

\subsection{Choice of prompts}

% Task-specific choice of prompts

% how we went about choosing the prompt for each task, using promptsource, optimizing for english, can add the potential drawbacks of doing so here or in discussion.

For each task that we consider in our benchmarking study, we need to come up with a prompt that specifies the instruction that the model should follow. We use PromptScource from the BigScience community to use the existing prompts or to create new prompts for the tasks \footnote{Hosted version: \url{https://huggingface.co/spaces/bigscience/promptsource}}. PromptSource is a toolkit for creating, sharing, and using natural language prompts. Prompts are saved in standalone structured files and written in a simple templating language called Jinja. 

For all datasets, we evaluate the performance of all English prompts on 10\% of the English test set. The 10\% of the test set is sampled randomly. We select the prompt that gives best performance on English. This prompt is then used for the entire test set for all prompt strategies as described in \ref{sec:prompt_strategies}. The selected English prompt is also translated to the corresponding target language using the Bing translator. Table \ref{tab:promptsource} shows the final English prompt for each dataset. 

There is a possibility that the best prompt for English is not necessarily the best prompt for other or all languages. Additionally, translation errors may propagate in the form of incorrect syntax and semantics in the prompts, which may influence task performance negatively. To avoid this, we manually inspect and edit prompts for languages that we know (mainly Indian languages and Swahili). We recommend that all translated prompt templates should be verified by a native speaker and plan to do so in future work.

% \myworries{ToDo: Discuss drawbacks of English finetuning here?}

\begin{table*}[h]
\begin{tabular}{p{3cm}p{2cm}p{5cm}p{3cm}}
\toprule
Dataset                 & Prompt Name & Template $f_{temp}$ & Verbalizer $f_{verb}$ \\ \midrule
XNLI, Indic-XNLI                    & Based on previous passage & \{premise\} Based on previous passage, is it true that \{hypothesis\} ? Yes, No, or Maybe? & Entailment -> Yes, Contradiction -> No, Neutral -> Maybe \\ \midrule
PAWS-X                  & Concatenation                                          & Sentence 1: \{sentence1\} Sentence 2: \{sentence2\} Question: Does Sentence 1 paraphrase Sentence 2? Yes or No? & Positive -> Yes, Negative -> No  \\ \midrule
XCOPA                   & Discrete version of plausible alternatives prompt &
\{ premise \} \{\% if question == "cause" \%\} This happened  because \{\% else \%\} As a consequence... \{\% endif \%\} Help  me pick the more plausible option:- choice1: \{choice1\}, choice2: \{choice2\} & \{choice1\} -> choice1 , \{choice2\} -> choice2       \\ \midrule
TyDiQA, MLQA, XQUAD, IndicQA &
  Answer given context and question &
  \{context\} Q: \{question\} Referring to the passage above, the correct answer to the given question is & Identity\\
  \bottomrule
\end{tabular}
\caption{Prompt type and prompt used for each dataset.}
\label{tab:promptsource}
\end{table*}
