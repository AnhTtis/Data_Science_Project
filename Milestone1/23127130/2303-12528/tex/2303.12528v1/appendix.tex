\clearpage
\section{Appendix}
\label{sec:appendix}

\subsection{Measuring Calibration}
\label{sec:cal_measure}
\paragraph{Reliability Diagrams} are an effective way of visualizing the calibration of a classifier. To plot these, we first predict the (maximum) confidence for each example in the test set and group the data points into $M$ equally sized bins based on the predicted confidence. For each bin, accuracy is computed (by considering label with maximum confidence as prediction) and plotted on the y axis with confidence being on the x-axis, as can be seen in Figure \ref{fig:cal_en_sw} (blue bars). For a perfectly calibrated classifier the confidence of a bin should match with the classifier's accuracy on that bin i.e. the accuracy vs confidence curve should lie on the line $y = x$ (red-dotted line in Figure \ref{fig:cal_en_sw}). The gap between the two is plotted (as red bars in Figure \ref{fig:cal_en_sw}) to represent the calibration error of the classifier. 

\paragraph{Expected Calibration Error (ECE)} is defined as expected value of the difference between the confidence and accuracy of the classifier's predictions. In practice ECE of a classifier is computed using a binning strategy as described in \citet{guo-2017-on} 
where the confidence predictions (corresponding to the maximum confidence class) on the $n$ test examples are grouped into $M$ uniform-sized bins, such that set of examples belonging to the $m^{th}$ bin are denoted by $B_m$. Accuracy ($\text{acc}(B_m)$) and average confidence ($\text{conf}(B_m)$) for each bin is computed and a weighted average of the differences between the two is taken to obtain ECE.
\begin{equation*}
    \texttt{ECE} =  \sum_{m = 1}^M \frac{|B_m|}{n} |\text{acc}(B_m) - \text{conf}(B_m)|
\end{equation*}

\subsection{Datasets Description}

\noindent\textbf{1. XNLI}\footnote{\url{https://github.com/facebookresearch/XNLI}} \cite{Conneau2018xnli}  is a Natural Langauge Inference task where a premise and hypothesis are given and the task is to predict if the hypothesis is \textit{entailed} in premise,  \textit{contradicts} the premise, or is \textit{neutral} towards it, hence being a three way classification problem. MultiNLI \cite{williams-etal-2018-broad} corpus which is available in English is used as training set, and dev and validation sets are obtained by manually translating crowd sourced English sentences for the task into 14 other languages (Arabic, Bulgarian, German, Greek, Spanish, French, Hindi, Russian, Swahili, Thai, Turkish, Urdu, Vietnamese and Chinese.)

\noindent\textbf{2. XCOPA}\footnote{\url{https://huggingface.co/datasets/xcopa}} \cite{ponti-etal-2020-xcopa} is a multilingual benchmark for causal commonsense reasoning. It was obtained by extending the dev and test sets of Choice of Plausible Alternatives (COPA)\footnote{\url{https://huggingface.co/datasets/super_glue}} \cite{roemmele2011choice} dataset to  11 typologically diverse languages. The task here is, given a premise and two alternatives, predict which alternative has a causal relationship with the premise. The original COPA dataset has only 400 training examples in English, hence it is common to first train the model on Social-IQA (SIQA)\footnote{\url{https://huggingface.co/datasets/social_i_qa}} \cite{sap-etal-2019-social} dataset (which has around 33k training examples), and then fine-tune it on COPA, and that's the strategy we adopt in our experiments. SIQA is similar to COPA but the questions or premise are defined in such a way that the possible answers have social implications, and instead of two alternatives it has three. Out of the 11 supported languages we experiment with 9 languages, ignoring Quechua and Haitian Creole as both mBERT and XLM-R were not pre-trained on these languages, leaving us with Greek, Indonesian, Italian, Swahili, Tamil, Thai, Turkish, Vietnamese and Chinese.

\noindent\textbf{3. MARC}\footnote{\url{https://registry.opendata.aws/amazon-reviews-ml/}} \cite{keung-et-al-2020-marc} is the multilingual Amazon product reviews corpus. We are given title, body and category of the review and the task is to predict the corresponding rating from 1 to 5. The corpus contains train, dev and test sets in six high resource languages: English, German, Spanish, French, Japanese and Chinese. In our experiments we only fine-tune using title and body text and ignore the category information.

\noindent\textbf{4. PAWS-X}\footnote{\url{https://huggingface.co/datasets/paws-x}} \cite{Yang2019paws-x} is an adversarial dataset for multilingual paraphrase detection. It was adapted from PAWS dataset \cite{zhang-etal-2019-paws} by manually translating the dev and test sets in English to six high resource languages (French, German, Spanish, Chinese, Japanese and Korean). The task is, given a pair of sentences predict whether the two are paraphrases of each other. For training the original English PAWS dataset is used. 

The statistics of all these datasets are provided in Table \ref{tab:data_stats}

\subsection{Detailed Experimental Setup}
\label{sec:expt_set_detail}
For fine-tuning the model we do a grid search over the learning rate($[1\text{e-}5, 3\text{e-}5, 5\text{e-}6, 7\text{e-}6]$) and the number of epochs ($[1, 3, 4, 5]$), run each setting for 3 random seeds (1, 11 and 22) and select the best hyper-parameter set corresponding to the average dev accuracy on English data \footnote{For XCOPA we did use dev data in all languages for model selection as we saw performance in English data to not necessarily correlate well with performance on other languages.}. 
% For evaluating on XCOPA dataset we first fine-tune the MMLMs on SocialIQA (SIQA) \cite{sap-etal-2019-social} dataset and then do continued fine-tuning on Choice of Plausible Alternatives (COPA) \cite{roemmele2011choice} dataset.
The final set of hyperparameters used for each dataset and MMLM are provided in Table \ref{tab:hyps}. Apart from these we use a batch size of 8 in all experiments. We use Adam optimizer \cite{kingma-ba-2015-adam} to train all of our models and LBFGS to learn the temperature parameter while performing temperature scaling. 

For computing ECE values for different tasks and languages we use $M = 10$ i.e. 10 buckets. For label smoothing we set the smoothing parameter $\alpha = 0.1$ in all experiments and initialize the temperature $T = 1.5$ while doing temperature scaling. For few-shot cases we use $\min(2500, |\mathcal{D}_{val}(\mathfrak{t}, l)|)$ examples, where $ |\mathcal{D}_{val}(\mathfrak{t}, l)|$ denotes the number of dev examples available in task $\mathfrak{t}$ for language $l$, and use that to perform continued fine-tuning or temperature scaling.

All the experiments were run on NVIDIA V100 and P100 GPUs with 32GB and 16GB memory respectively. We use pre-trained models available in Hugging Face's Transformers library \cite{wolf-etal-2020-transformers}. For computing the calibration errors as well as plotting the reliability diagrams we use the open source tool Calibration Framework\footnote{\url{https://github.com/fabiankueppers/calibration-framework}}. To encourage the research in this area we will make our code public.


\begin{table*}[]
    \small
    \centering
    \begin{tabular}{p{1cm}p{1.5cm}p{2.5cm}p{2cm}p{1.4cm}p{1.4cm}}
         \toprule
         Dataset & MMLM & Learning Rate & Epochs & Few-Shot Learning Rate & Few-Shot Epochs  \\
         \midrule
         \multirow{ 2}{*}{XNLI} & XLM-R & 7e-6 & 3 & 5e-06 & 2 \\
         & mBERT & 3e-5 & 3 & 3e-5 & 1\\
         \midrule
         \multirow{ 2}{*}{XCOPA} & XLM-R & 5e-6 (for SIQA) \& 5e-6 (for COPA)  & 4 (for S-IQA) \& 10 (for COPA) & 1e-5 & 1 \\
         & mBERT & 1e-5 (for S-IQA) \& 3e-5 (for COPA) & 3 (for SIQA) \& 10 (for COPA) & 3e-5 & 10\\
         \midrule
         \multirow{ 2}{*}{MARC} & XLM-R & 5e-6 & 3 & 5e-6 & 1 \\
         & mBERT & 5e-6 & 3 & 5e-6 & 1\\
         \midrule
         \multirow{ 2}{*}{PAWS-X} & XLM-R & 7e-6 & 3 & 3e-5 & 1 \\
         & mBERT & 1e-5 & 3 & 3e-5 & 1\\
         \bottomrule
    \end{tabular}
    \caption{Final list of hyperparameters used for reporting results.}
    \label{tab:hyps}
\end{table*}




% For calibrating the models with label smoothing, we chose $\alpha = 0.1$ and accordingly modify the fine-tuning objective. On the other hand, methods like temperature scaling and few-shot learning are applied post-hoc after fine-tuning on English is done and dev data in English (for TS) or the target languages is used to perform calibration (for Self-TS and FSL).  We also consider combinations of different calibration methods in our experiments, including Temperature Scaling + Label Smoothing (TS + LS or Self-TS + LS) and Few-Shot Learning + Label Smoothing (FSL + LS) in addition to using these methods in isolation.
% \footnote{We experimented with Few-Shot Learning + Temperature Scaling as well i.e. FSL + TS but found this combination to be highly unstable.} in addition to using these methods in isolation.

% Details about the final set of hyper-parameters for each task, MMLM and calibration method are given in Appendix \myworries{Add hyperparameter table in appendix and put a reference here}.

% \paragraph{Calibration Settings}  We also consider combinations of different calibration methods in our experiments, including Temperature Scaling + Label Smoothing (TS + LS or Self-TS + LS) and Few-Shot Learning + Label Smoothing (FSL + LS) \footnote{We experimented with Few-Shot Learning + Temperature Scaling as well i.e. FSL + TS but found this combination to be highly unstable.} in addition to using these methods in isolation.



\begin{table*}[]
    \small
    \centering
    \begin{tabular}{p{3cm}p{1.5cm}p{1.2cm}p{2cm}p{1.4cm}p{1.4cm}}
         \toprule
         Dataset & Number of Languages & Number of Labels & Training Size & Dev Size & Test Size  \\
         \midrule
         XNLI (sub-sampled) & 15 & 3 & 40000 & 2500 & 2500\\
         XCOPA & 9 & 2 & 33410 (S-IQA) + 500 (COPA) & 100 & 400\\
         MARC (sub-sampled) & 6 & 5 & 40000 & 2500 & 5000\\
         PAWS-X & 7 & 2 & 50000 & 2000 & 2000\\
         \bottomrule
    \end{tabular}
    \caption{Dataset statistics for the 4 multilingual classification tasks that we study in our experiments. We use sub-sampled versions of XNLI and MARC and only use the first 40k examples in their training sets to reduce the compute overhead and making the scale of training data consistent across all the tasks. For completeness we also run some preliminary experiments with using the entire data for fine-tuning XNLI and present the calibration errors in Figure \ref{fig:full_v_sample}.}
    \label{tab:data_stats}
\end{table*}


\begin{table}[]
    \centering
    \small
    \begin{tabular}{p{2.5cm}p{1cm}p{1cm}p{1cm}}
        \toprule
         Dataset & SIZE & SYN & SWO \\
         \midrule
         XNLI & -0.4 & -0.38 & \textbf{-0.41} \\
         XNLI (wo th) & -0.86 & \textbf{-0.88} & -0.78 \\
         XCOPA & -0.14 & \textbf{-0.22} & 0.07 \\
         MARC & -0.5 & -0.69 & \textbf{-0.86} \\
         PAWS-X & -0.91 & -0.91 & \textbf{-0.98} \\
         \bottomrule
    \end{tabular}
    \caption{Pearson correlation coefficient between the
Expected Calibration Error (ECE) and SIZE, SYN, and
SWO features of different languages in the test set for
mBERT}
    \label{tab:cal_factors_mbert}
\end{table}

\begin{table*}[]
\small
\centering
\begin{tabular}{p{1cm}p{1.2cm}p{0.9cm}p{0.9cm}p{0.9cm}p{1.2cm}p{1.2cm}p{1.7cm}p{0.9cm}p{1.2cm}}
    \toprule
    \multirow{ 2}{*}{Dataset} & \multirow{ 2}{*}{MMLM} & \multicolumn{4}{c}{Zero-Shot Calibration} & \multicolumn{4}{c}{Few-Shot Calibration}\\
    \cmidrule(l){3-6}  \cmidrule(l){7-10}
    & & OOB & TS & LS & TS + LS & Self-TS & Self-TS + LS & FSL & FSL + LS\\
    \cmidrule(l){3-10}
    \multirow{ 2}{*}{MARC} & XLM-R & 9.65 & \textbf{4.22}$^{\dagger}$ & 7.93 & 4.45 & 3.55 & 3.36 & \textbf{2.51}$^{\ddag}$ & 2.58 \\
    & mBERT & 11.11 & 6.14 & 5.96 & \textbf{4.46}$^{\dagger}$ & 4.62 & 4.71 & \textbf{2.12}$^{\ddag}$ & 3.01\\
    \midrule
    \multirow{ 2}{*}{PAWS-X} & XLM-R & 4.28 & \textbf{2.29}$^{\dagger}$ & 3.37 & 4.44 & - & - & - & - \\
    & mBERT & 10.33 & 5.64 & 5.58 & \textbf{5.36}$^{\dagger}$ & - & - & - & -\\
    \bottomrule



\end{tabular}
\caption{Calibration Errors for MARC and PAWS-X tasks for XLM-R and mBERT on using different methods for calibration. For PAWS-X we only report numbers for Zero-Shot methods as the dev data and test data of the benchmark have sentences in common (even though the pairs are unique), hence we avoid using dev examples as few-shot in this case.}
\label{tab:cal_impr_mbert}
\end{table*}


\begin{table*}[]
\small
\centering
\begin{tabular}{p{1cm}p{1.2cm}p{0.9cm}p{0.9cm}p{0.9cm}p{1.2cm}p{1.2cm}p{1.7cm}p{0.9cm}p{1.2cm}}
    \toprule
    \multirow{ 2}{*}{Dataset} & \multirow{ 2}{*}{MMLM} & \multicolumn{4}{c}{Zero-Shot Calibration} & \multicolumn{4}{c}{Few-Shot Calibration}\\
    \cmidrule(l){3-6}  \cmidrule(l){7-10}
    & & OOB & TS & LS & TS + LS & Self-TS & Self-TS + LS & FSL & FSL + LS\\
    \cmidrule(l){3-10}
    \multirow{ 2}{*}{XNLI} & XLM-R & 74.9 & 74.9 &  74.6 & 74.6 & 74.9 & 74.6 & 79.4 & 79.3\\
    & mBERT & 56.7 & 56.7 &  57.5 & 57.5 & 56.7 & 57.5 & 60.6 & {61.2}\\
    \midrule
    \multirow{ 2}{*}{XCOPA} & XLM-R & 74.3 & 74.3 & {75.1} & {75.1} & 74.3 & {75.1} & 67.2 & 69.5 \\
    & mBERT & 54.5 & 54.5 & 54.4 & 54.4 & 54.5 & 54.4 & 53.0 & 52.9\\
    \midrule
    \multirow{ 2}{*}{MARC} & XLM-R & 57.7 & 57.7 & 57.6 & 57.6 & 57.7 & 57.6 & {59.4} & 59 \\
    & mBERT & 42.9 & 42.9 & 42.6 & 42.6 & 42.9 & 42.6 & {49.6} & 49.2\\
    \midrule
    \multirow{ 2}{*}{PAWS-X} & XLM-R & 76.1 & 76.1 & 75.6 & 75.6 & - & - & - & - \\
    & mBERT & 80.4 & 80.4 & 81.1 & 81.1 & - & - & - & -\\
    \bottomrule



\end{tabular}
\vspace*{-3mm}
\caption{Accuracy ($ \mathop{\mathbb{E}}_{l \in \mathcal{L}'}[\texttt{Accuracy}(l)]$) for XLM-R and mBERT on using different methods for calibration. Similar to Table \ref{tab:cal_impr_mbert}, here again we skip few-shot calibration for PAWS-X due to the possible data leakage.}
\vspace*{-3mm}
\label{tab:acc}
\end{table*}

\begin{table*}[h]
    \centering
    \small
    \begin{subtable}[h]{0.45\textwidth}
        \begin{tabular}{p{1.7cm}p{1cm}p{1.5cm}p{1.5cm}}
            \toprule
             Method & $\texttt{ECE}(en)$ & $\displaystyle \mathop{\mathbb{E}}_{l \in \mathcal{L}'}[\texttt{ECE}(l)]$ & $\displaystyle \max_{l \in \mathcal{L}'}\texttt{ECE}(l)$\\
            \midrule
            \multicolumn{4}{c}{Zero-Shot Calibration}\\
            \midrule
             OOB & 7.32 & 13.34 & 19.07 \\
             TS & 2.02 & 6.74 & 11.81 \\
             LS & 3.2 & 6.93 & 12.1 \\
             TS + LS & 4.1 & 4.9 & 9.35\\
            \midrule
            \multicolumn{4}{c}{Few-Shot Calibration}\\
            \midrule
             Self-TS & 2.02 & 5.41 & 9.7 \\
             Self-TS + LS & 4.1 & 4.05 & 4.64 \\
             FSL & 7.32 & 7.67 & 9.23 \\
             FSL + LS & 3.2 & 4.37 & 5.73\\
             \bottomrule
        \end{tabular}
        \caption{Detailed results on XNLI with XLMR}
         \vspace*{3mm}
    \end{subtable}
    \hfill
    \begin{subtable}[h]{0.45\textwidth}
        \begin{tabular}{p{1.7cm}p{1cm}p{1.5cm}p{1.5cm}}
            \toprule
             Method & $\texttt{ECE}(en)$ & $\displaystyle \mathop{\mathbb{E}}_{l \in \mathcal{L}'}[\texttt{ECE}(l)]$ & $\displaystyle \max_{l \in \mathcal{L}'}\texttt{ECE}(l)$\\
            \midrule
            \multicolumn{4}{c}{Zero-Shot Calibration}\\
            \midrule
             OOB & 5.44 & 12.34 & 45.15 \\
             TS & 2.51 & 6.29 & 37.25 \\
             LS & 4.51 & 10.42 & 38.36 \\
             TS + LS & 2.61 & 6.71 & 30.51\\
            \midrule
            \multicolumn{4}{c}{Few-Shot Calibration}\\
            \midrule
             Self-TS & 2.51 & 4.77 & 29.82 \\
             Self-TS + LS & 2.61 & 4.7 & 23.23 \\
             FSL & 5.44 & 3.14 & 4.28 \\
             FSL + LS & 2.61 & 2.55 & 4.26\\
             \bottomrule
        \end{tabular}
        \caption{Detailed results on XNLI with mBERT}
         \vspace*{3mm}
    \end{subtable}
    
    \begin{subtable}[h]{0.45\textwidth}
        \begin{tabular}{p{1.7cm}p{1cm}p{1.5cm}p{1.5cm}}
            \toprule
             Method & $\texttt{ECE}(en)$ & $\displaystyle \mathop{\mathbb{E}}_{l \in \mathcal{L}'}[\texttt{ECE}(l)]$ & $\displaystyle \max_{l \in \mathcal{L}'}\texttt{ECE}(l)$\\
            \midrule
            \multicolumn{4}{c}{Zero-Shot Calibration}\\
            \midrule
             OOB & 14.54 & 20.01 & 29.33 \\
             TS & 12.31 & 15.95 & 24.04 \\
             LS & 9.66 & 5.47 & 9.42 \\
             TS + LS & 8.98 & 4.52 & 7.2\\
            \midrule
            \multicolumn{4}{c}{Few-Shot Calibration}\\
            \midrule
             Self-TS & 12.31 & 16.02 & 24.02 \\
             Self-TS + LS & 8.98 & 4.06 & 5.36 \\
             FSL & 14.54 & 8.93 & 14.92 \\
             FSL + LS & 9.66 & 4.4 & 5.74\\
             \bottomrule
        \end{tabular}
        \caption{Detailed results on XCOPA with XLMR}
         \vspace*{3mm}
    \end{subtable}
    \hfill
    \begin{subtable}[h]{0.45\textwidth}
        \begin{tabular}{p{1.7cm}p{1cm}p{1.5cm}p{1.5cm}}
            \toprule
             Method & $\texttt{ECE}(en)$ & $\displaystyle \mathop{\mathbb{E}}_{l \in \mathcal{L}'}[\texttt{ECE}(l)]$ & $\displaystyle \max_{l \in \mathcal{L}'}\texttt{ECE}(l)$\\
            \midrule
            \multicolumn{4}{c}{Zero-Shot Calibration}\\
            \midrule
             OOB & 23.4 & 23.51 & 29.02 \\
             TS & 21.76 & 20.02 & 23.99 \\
             LS & 15.85 & 12.41 & 15.87 \\
             TS + LS & 11.28 & 6.77 & 8.93\\
            \midrule
            \multicolumn{4}{c}{Few-Shot Calibration}\\
            \midrule
             Self-TS & 21.76 & 20.1 & 24.12 \\
             Self-TS + LS & 11.28 & 6.89 & 9.01 \\
             FSL & 23.4 & 3.75 & 10.5 \\
             FSL + LS & 15.85 & 3.54 & 6.65\\
             \bottomrule
        \end{tabular}
        \caption{Detailed results on XCOPA with mBERT}
         \vspace*{3mm}
    \end{subtable}

    \begin{subtable}[h]{0.45\textwidth}
        \begin{tabular}{p{1.7cm}p{1cm}p{1.5cm}p{1.5cm}}
            \toprule
             Method & $\texttt{ECE}(en)$ & $\displaystyle \mathop{\mathbb{E}}_{l \in \mathcal{L}'}[\texttt{ECE}(l)]$ & $\displaystyle \max_{l \in \mathcal{L}'}\texttt{ECE}(l)$\\
            \midrule
            \multicolumn{4}{c}{Zero-Shot Calibration}\\
            \midrule
             OOB & 7.15 & 9.65 & 13.45 \\
             TS & 2.75 & 4.22 & 5.8 \\
             LS & 5.36 & 7.93 & 10.66 \\
             TS + LS & 3.39 & 4.55 & 6.33\\
            \midrule
            \multicolumn{4}{c}{Few-Shot Calibration}\\
            \midrule
             Self-TS & 2.75 & 3.56 & 4.1 \\
             Self-TS + LS & 3.39 & 3.36 & 3.64 \\
             FSL & 7.15 & 2.51 & 3.51 \\
             FSL + LS & 5.36 & 2.59 & 3.28\\
             \bottomrule
        \end{tabular}
        \caption{Detailed results on MARC with XLMR}
         \vspace*{3mm}
    \end{subtable}
    \hfill
    \begin{subtable}[h]{0.45\textwidth}
        \begin{tabular}{p{1.7cm}p{1cm}p{1.5cm}p{1.5cm}}
            \toprule
             Method & $\texttt{ECE}(en)$ & $\displaystyle \mathop{\mathbb{E}}_{l \in \mathcal{L}'}[\texttt{ECE}(l)]$ & $\displaystyle \max_{l \in \mathcal{L}'}\texttt{ECE}(l)$\\
            \midrule
            \multicolumn{4}{c}{Zero-Shot Calibration}\\
            \midrule
             OOB & 9.38 & 11.1 & 17.3 \\
             TS & 3.56 & 6.14 & 10.9 \\
             LS & 5.19 & 5.96 & 12.9 \\
             TS + LS & 3.70 & 4.47 & 10.1\\
            \midrule
            \multicolumn{4}{c}{Few-Shot Calibration}\\
            \midrule
             Self-TS & 3.56 & 4.62 & 8.34 \\
             Self-TS + LS & 3.70 & 4.71 & 7.50 \\
             FSL & 9.38 & 2.12 & 3.45 \\
             FSL + LS & 5.19 & 3.02 & 4.24\\
             \bottomrule
        \end{tabular}
        \caption{Detailed results on MARC with mBERT}
         \vspace*{3mm}
    \end{subtable}
    
    
    \begin{subtable}[h]{0.45\textwidth}
        \begin{tabular}{p{1.7cm}p{1cm}p{1.5cm}p{1.5cm}}
            \toprule
             Method & $\texttt{ECE}(en)$ & $\displaystyle \mathop{\mathbb{E}}_{l \in \mathcal{L}'}[\texttt{ECE}(l)]$ & $\displaystyle \max_{l \in \mathcal{L}'}\texttt{ECE}(l)$\\
            \midrule
            \multicolumn{4}{c}{Zero-Shot Calibration}\\
            \midrule
             OOB & 1.93 & 4.28 & 5.88 \\
             TS & 0.81 & 2.29 & 3.39 \\
             LS & 4.83 & 3.37 & 3.85 \\
             TS + LS & 6.30 & 4.44 & 5.07\\
             \bottomrule
        \end{tabular}
        \caption{Detailed results on PAWS-X with XLMR}
         \vspace*{3mm}
    \end{subtable}
    \hfill
    \begin{subtable}[h]{0.45\textwidth}
        \begin{tabular}{p{1.7cm}p{1cm}p{1.5cm}p{1.5cm}}
            \toprule
             Method & $\texttt{ECE}(en)$ & $\displaystyle \mathop{\mathbb{E}}_{l \in \mathcal{L}'}[\texttt{ECE}(l)]$ & $\displaystyle \max_{l \in \mathcal{L}'}\texttt{ECE}(l)$\\
            \midrule
            \multicolumn{4}{c}{Zero-Shot Calibration}\\
            \midrule
             OOB & 3.57 & 10.3 & 15.6 \\
             TS & 0.99 & 5.64 & 9.80 \\
             LS & 3.35 & 5.57 & 8.94 \\
             TS + LS & 5.27 & 5.36 & 7.40\\
             \bottomrule
        \end{tabular}
        \caption{Detailed results on PAWS-X with mBERT}
         \vspace*{3mm}
    \end{subtable}
\caption{Detailed results on improving calibration for the 4 datasets and 2 MMLMs considered in our experiments}
\label{tab:det_results}
\end{table*}


% \begin{figure*}
%     \centering
%     % \captionsetup[subfloat]{margin=0.5em}
%     \begin{subfigure}{.2\textwidth}
%     \centering
%     \captionsetup{justification=centering}
%     \includegraphics[width=.99\textwidth]{EMNLP 2022/figures/en_oob.pdf}
%     \vspace*{-7mm}
%     \caption{}%Out of box calibration on English}
%     \label{fig:en_oob_full}
%     \end{subfigure}
%     \begin{subfigure}{.2\textwidth}
%     \centering
%     \includegraphics[width=.99\textwidth]{EMNLP 2022/figures/sw_oob.pdf}
%     \vspace*{-7mm}
%     \caption{}%Out of box calibration on Swahili}
%     \label{fig:sw_oob_full}
%     \end{subfigure}
%     \begin{subfigure}{.2\textwidth}
%     \centering
%     \includegraphics[width=.99\textwidth]{EMNLP 2022/figures/sw_zs_cali.pdf}
%     \vspace*{-7mm}
%     \caption{}%Calibrating using zero-shot methods}
%     \label{fig:sw_zs_full}
%     \end{subfigure}
%     \begin{subfigure}{.2\textwidth}
%     \centering
%     \includegraphics[width=.99\textwidth]{EMNLP 2022/figures/sw_fs_cali.pdf}
%     \vspace*{-7mm}
%     \caption{}%Calibrating using few-shot methods}
%     \label{fig:sw_fs_full}
%     \end{subfigure}
%     \vspace*{-3mm}
%     \caption{Full version of the reliability diagrams shown in Figure \ref{fig:cal_en_sw} also containing a comparison between the average confidence and accuracy of the model.}
%     \vspace*{-3mm}
%     \label{fig:cal_en_sw_det}
% \end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=.9\textwidth]{EMNLP 2022/figures/full_vs_sampled_data.pdf}
    \caption{Out-of-Box Calibration Errors (ECE) for XLMR and mBERT trained on XNLI with 40k samples (sub-sampled) and 392k samples(full data). Even though the calibration is better with using the entire data for training, the observed patterns about the models being mis-calibrated on languages other than English, especially low resource languages like Swahili, Thai and Urdu still hold true.}
    \label{fig:full_v_sample}
\end{figure*}

\begin{figure}
    \centering
    \begin{subfigure}{.49\textwidth}
        \includegraphics[width=.99\textwidth]{EMNLP 2022/figures/xlmr_ece.pdf}
    \caption{}
    \label{fig:xlmr_size_vary}
    \end{subfigure}
    \begin{subfigure}{.49\textwidth}
        \includegraphics[width=.99\textwidth]{EMNLP 2022/figures/mbert_ece.pdf}
    \caption{}
    \label{fig:mbert_size_vary}
    \end{subfigure}
\caption{Variation in ECE as we use more and more data for calibrating using Self-TS method across languages for XNLI dataset. As can be seen 500 samples are sufficient to obtain low calibration errors.}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{.49\textwidth}
        \includegraphics[width=.99\textwidth]{EMNLP 2022/figures/size.pdf}
    \caption{}
    \label{fig:size_corr}
    \end{subfigure}
    \begin{subfigure}{.49\textwidth}
        \includegraphics[width=.99\textwidth]{EMNLP 2022/figures/syn.pdf}
    \caption{}
    \label{fig:syn_corr}
    \end{subfigure}
    \begin{subfigure}{.49\textwidth}
        \includegraphics[width=.99\textwidth]{EMNLP 2022/figures/swo.pdf}
    \caption{}
    \label{fig:swo_corr}
    \end{subfigure}
\caption{Visualizing the correlations of ECE with SIZE, SYN and SWO for XLMR fine-tuned on XNLI.}
\end{figure}











