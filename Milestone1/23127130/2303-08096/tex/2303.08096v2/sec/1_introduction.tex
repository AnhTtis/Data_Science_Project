\section{Introduction}
\label{sec:intro}

Neural rendering methods have demonstrated wide ranging application from novel-view synthesis~\cite{Mildenhall20eccv_nerf}, to avatar generation for virtual reality ~\cite{Gafni21cvpr_DNRF}, to 3D reconstruction of molecules from microscope images ~\cite{Levy2022cryoai}.
Neural radiance fields (NeRFs)~\cite{Mildenhall20eccv_nerf} represent radiance with a neural field that reproduces the geometric structure and appearance of a scene, allowing the use of gradient descent to reconstruct a set of input images.
NeRFs have gained wide acceptance for their robust ability to generate novel-views of a 3D scene, but must be trained with known camera poses, limiting their use to cases where camera pose is known or can be inferred from other methods.
Our method seeks to estimate camera pose and radiance field simultaneously, eliminating the requirement of known camera poses.

Classical structure-from-motion algorithms (SfM), such as COLMAP~\cite{Schonberger2016sfmrevisited}, compute the relative camera pose between  views by detecting and matching salient points in the scene and minimizing re-projection error.
Despite being widely used, SfM commonly fails on scenes containing many view-dependent effects or few textures and edges.
Other methods \cite{Lin21iccv_BARF, Wang21arxiv_NeRFminusminus, Chng2022garf} jointly estimate a neural radiance field and camera parameters  using gradient descent, but require an approximate initialization of camera parameters, and none work in a $360^\circ$ object centered configuration with random initialization. Adversarial approaches \cite{Henzler2019platonicgan, NguyenPhuoc2019hologan, Niemeyer2021giraffe, Niemeyer21cvpr_GIRAFFE, Chan21cvpr_piGAN, Meng21iccv_GNeRF} jointly train a 3D generator representing the scene, and a discriminator that attempts to differentiate input images from generated images.
These methods, however, suffer from large input data requirements, hyper parameter sensitivity, complex training schedules, and high computation times.

\input{figref/teaser}
We introduce \methodname (Modulo-Equivalent Latent Optimization of NeRF), an encoder-decoder-based method that infers a neural representation of a scene from unposed views (Fig.~\ref{fig:teaser}).
Our method simultaneously trains a CNN encoder that maps images to camera poses in $\sothree$, and a neural radiance field of the scene.
\methodname does not require CNN pre-training, and is able to infer camera pose in object centered configurations entirely ``\emph{ab-initio}'', \textit{i.e.} without any pose initialization whatsoever.
To cope with the presence of local minima in the low-dimensional latent space $\sothree$, we introduce a novel \textit{\lossname} that replicates the encoder output following a pre-defined structure.
We formalize this replication with an equivalence relation defined in the space of camera parameters, and show this enables the encoder to operate in a quotient set of the camera space.
We introduce a one-dimensional toy problem that mimics the challenges posed by 3D inverse rendering to better illustrate our method.

We identify the following contributions:
\begin{itemize}
%  \setlength \itemsep{0em}
\item We introduce a gradient-descent-based algorithm to train a neural radiance field from entirely unposed images, demonstrating competitive reconstruction metrics on a variety of synthetic and real datasets.
\item We show that \methodname is robust to noise and can perform pose estimation and novel-view synthesis from as few as six unposed images.
\item We introduce 1D and 3D datasets of almost symmetric objects which can be used as challenging examples for pose estimation and inverse rendering.
\end{itemize}

Our code will be released upon publication.
