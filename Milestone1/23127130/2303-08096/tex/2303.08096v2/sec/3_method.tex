\input{figref/method_1d_labeled}

\section{Methods}

We are given a set of input images $\{\image\}_{i=1 \ldots n}$ which we assume to be renderings of an unknown density $\density^*$ and view-dependent radiance $\radiance^*$ field from unknown cameras $\{\pose^*\}_{i=1,\ldots,n}$, following~\eqref{eqn:nerf}:
\begin{equation}
    \forall i,\forall\pixel\in\plane,\quad\image(\pixel)=\nerfcolor(\pixel~; \shared^*,\pose^*)
\label{eqn:three-d-forward-model}
\end{equation}
where $\plane$ is a discrete set of pixel coordinates and $\shared^*=(\radiance^*,\density^*)$.
We assume that all cameras have known intrinsics and point towards the origin of the scene from a known distance.
Each camera orientation $R_i\in\sothree$ can therefore be described with three numbers: azimuth $\azim\in\sotwo$, elevation $\elev\in[-\pi/2,\pi/2]$ and roll $\roll\in\sotwo$. We use the symbol $\sim$ to represent an element of $\sothree$ in this coordinate system.
We define (external) additive and multiplicative operations such that, for $\theta_0\in\sotwo$, $\lambda\in\mathbb{R}$ and $R\sim(\azim,\elev,\roll)$,
\begin{equation}
    \theta_0+R\sim(\theta_0+\azim,\elev,\roll)\quad;\quad\lambda R\sim(\lambda \azim,\elev,\roll).
\end{equation}

We use a neural radiance field parameterized by $\weightsshared$ to represent the predicted function $\shared_\weightsshared=(\radiance_\weightsshared,\density_\weightsshared)$. The goal of \textit{3D inverse rendering with unknown poses} is to approximate $f^*$ with $\shared_\weightsshared$ and $R_i^*$ with $R_i$, up to a global rotation.
%minimize
% \begin{equation}
%     \loss_\text{3D}(\weightsshared,\{\pose\})=\sum_{i=1}^n\norm\big{\nerfcolor(.~;\shared_\weightsshared,\pose)-\image(.)}_{\plane,2}^2
% \label{eqn:three-d-loss}
% \end{equation}
% over the poses $\{\pose\}_i$ and parameters $\weightsshared$.
While $\weightsshared$ is shared, each image is associated with a unique $\pose$, called a \textit{latent variable} living in the \textit{latent space} $\sothree$. The low dimensionality of this space induces the presence of local minima~\cite{bray2007statistics, dauphin2014identifying} and makes pose estimation difficult to solve by gradient descent.
% Note that this loss remains constant if the same rigid rotation is applied to the poses and to the field.

\subsection{One-dimensional Toy Problem}

We propose a simpler 1D version of the above problem to aid us in understanding the challenges existing in 3D space (Fig.~\ref{fig:method-1d-labeled}). 
Assume we are given $n$ crops $\{\image\}_{i=1,\ldots,n}$ of width $\pi$ of an unknown one-dimensional function $\torus^*$ mapping $\sotwo$ to $\mathbb{R}$. Each crop $\image$ is centered around an unknown angle $\ang^*$:
\begin{equation}
    \forall i,\forall\pix\in\segment,\quad\image(\pix)=\nerfcolor(\pix~;\torus^*,\ang^*)=\torus^*(\ang^*+\pix)
\label{eqn:one-d-forward-model}
\end{equation}
where $\segment=[-\pi/2,\pi/2]\subset\sotwo$. Our objective is to recover the function $f^*$ given the crops $\image$, without knowing the set of angles $\{\theta_i^*\}$.

The objective of \textit{1D inverse rendering with unknown angles} is to approximate $\torus^*$ with a coordinate-based neural network $\torus_\weightsshared$ and $\theta_i^*$ with $\theta_i$, up to a global rotation.
% minimize
% \begin{equation}
%     \loss_{\text{1D}}(\weightsshared,\{\ang\})=\sum_{i=1}^n\norm\big{\nerfcolor(.~;\torus_\weightsshared,\ang)-\image(.)}_{\segment,2}^2
% \label{eqn:one-d-loss}
% \end{equation}
% over $\{\ang\}_i$ and $\weightsshared$.
In this case, the latent variables are the angles $\ang$ belonging to the latent space $\sotwo$.
% As before, this problem has infinitely many global optima that are equivalent by rotation.

\subsection{Amortized Inference of Latent Variables}

In both problems, a set of latent variables $\latent$ must be estimated ($\pose$ in 3D, $\ang$ in 1D) jointly with a parameter $\weightsshared$. Instead of minimizing
$\norm\big{\nerfcolor(.~;\torus_\weightsshared,z_i)-\image(.)}_{2}^2$ over each $\latent$ independently, we aim to find an \textit{encoder} $\enc$ mapping observations $\image$ (images or crops) to their associated latent variables $\latent^*$. The encoder is parameterized by a neural network with weights $\weightsenc$ (see details on the architecture in the supplements). The objective function of our optimization is therefore
\begin{equation}
    \loss(\weightsshared,\xi)=\sum_{i=1}^n\norm\big{ \nerfcolor(.~;\shared_\weightsshared,\enc_\xi(\image)) - \image(.)}_2^2,
\label{eqn:generic-loss}
\end{equation}
where $\Vert.\Vert_2$ is the L2 norm on the plane $\plane$ or segment $\segment$. We say that the inference of the latent variables is \textit{amortized} over the size of the dataset, meaning that the estimation of the latent variables for the dataset is seen as a unique inference problem instead of $n$ independent problems. In particular, if $\enc$ is accurate on a given observation $\image$, it will likely be accurate on similar observations provided that it is likely to be \textit{smooth} in observation space.

\subsection{Self-Similarity Map}

\input{figref/method_3d}

Let $\latentspace$ be the latent space ($\sotwo$ or $\sothree$) and $\nerfcolor$ our rendering model. In both cases, we can provide $\Omega$ with an internal binary operation $\cdot$ and see $(\Omega,\cdot)$ as a group.
In order to focus our analysis on the pose estimation problem, we will make the simplifying assumption that $\shared_\weightsshared$ approximates $\shared^*$, up to a global \textit{alignment} $\alignment\in\latentspace$, \textit{i.e.}
\begin{equation}
    \forall z\in\latentspace,\quad\nerfcolor(.~;\shared^*,\alignment\cdot z) = \nerfcolor(.~;\shared_\weightsshared,z).
\label{eq:approx}
\end{equation}

We define the \textit{self-similarity map} of $\shared^*$ as
\begin{equation}
    \degen_{\shared^*}:\begin{cases}
    \latentspace^2 & \to\mathbb{R}_+\\
    (z,z^*) & \mapsto \norm\big{ \nerfcolor(.~;\shared^*,z) - \nerfcolor(.~;\shared^*,z^*)}_2^2
    \end{cases},
\label{eq:self-similarity}
\end{equation}
Given \eqref{eq:approx}, the loss~\eqref{eqn:generic-loss} can be re-written
\begin{equation}
    \mathcal{L}(\xi) = \sum_{i=1}^n \degen_{\shared^*}(\alignment\cdot\enc_\xi(\image), z^*_i).
\label{eq:loss-poses}
\end{equation}
By the chain rule, the computation of $\nabla_\weightsenc\mathcal{L}$ involves the gradients of $\degen_{\shared^*}(., z^*_i)$. As the loss is minimized by gradient descent, the model can converge to the optimal solution only if it does not need to cross ``bariers'' in the energy landscape to reach a global minimum\def\thefootnote{1}\footnote{for a an infinitely small learning rate}, \textit{i.e.} if there exists a continuous path in $\Omega$ between the initial value $\alignment\cdot h_\weightsenc(\image)$ and $z^*_i$ where $\degen_{\shared^*}(.,z^*_i)$ is strictly decreasing. For $z^*\in\Omega$, we write $\conv_{f^*}(z^*)$ the set of initial values satisfying the above condition and call it the \textit{region of attraction} of $z^*$ for $f^*$.
The difficulty of the pose estimation problem with random initialization can therefore be related to the fractional size of the regions of attraction of $f^*$:
\begin{equation}
    0\leq D(f^*)=\mathbb{E}_{z^*\sim p^*,z\sim p}[z\in\conv_{f^*}(z^*)]\leq 1,
\label{eq:difficulty}
\end{equation}
where $p=p^*$ is the uniform distribution over the latent space $\latentspace$. The closer this value to $1$, the easier pose estimation will be.

\subsection{\LossName: From Latent to Quotient Space}
\label{sec:modulo-loss}

Our core idea is to let the encoder work in a smaller space than the latent space, namely in the quotient set of the latent space for some equivalence relation. Given an equivalence relation $\relation$ (reflexive, transitive and symmetric), we map each latent $\enc_\weightsenc(\image)$ predicted by the encoder to its equivalence class
\begin{equation}
    [\enc_\weightsenc(\image)]_\relation = \{z~|~z\relation\enc_\weightsenc(\image)\}.
\end{equation}
All the latents belonging to this class are passed to the rendering model to generate a set of synthetic observations
\begin{equation}
    \mathcal{C}_i^\relation(\weightsshared,\weightsenc)=\{C(.~;f_\weightsshared,z), z\in[\enc_\weightsenc(\image)]_\relation\}.
\end{equation}
The L2 distances are computed between the ground truth observation and all the generated views.
The function we minimize, called the \textit{\lossname}, is defined as
\begin{equation}
    \mathcal{L}_\relation(\weightsshared,\weightsenc)=\sum_{i=1}^n\text{min}\{\Vert C - I_i \Vert_2^2, C\in\mathcal{C}_i^\relation(\weightsshared,\weightsenc)\}.
\label{eqn:modulo-loss}
\end{equation}
and is a lower bound of~\eqref{eqn:generic-loss} due to the reflexivity of $\relation$.
Given our simplifying assumption~\eqref{eq:approx}, the \lossname can be re-written
\begin{equation}
    \mathcal{L}_\relation(\weightsenc)=\sum_{i=1}^n\degen_{\shared^*}^\relation([\alignment\cdot \enc_\xi(\image)]_\relation,z^*_i)\leq \mathcal{L}(\weightsenc)
\label{eqn:modulo-loss-approx}
\end{equation}
where the \textit{quotient self-similarity map} $\degen_{\shared^*}^\relation$ is defined by
\begin{equation}
    \degen_{\shared^*}^\relation:\begin{cases}
    \latentspace/\relation\times\latentspace & \to\mathbb{R}_+\\
    (Z,z^*) & \mapsto \text{min}\{\degen_{\shared^*}(z^\prime,z^*),z^\prime\in Z\}
    \end{cases}.
\end{equation}
This loss is minimized when $z_i^*\in[\alignment\cdot\enc_\weightsenc(\image)]_\relation$ for all $i$. Given an optimal encoder, $\enc_{\weightsenc^*}(\image)$ is an estimate of $z_i$ \textit{modulo $\relation$} and the true estimate is given by
\begin{equation}
    z_i=\text{argmin}\{\Vert C(.~;f_\weightsshared,z)-\image(.)\Vert_2^2, z\in[\enc_{\weightsenc^*}(\image)]\}.
\label{eqn:predicted-latents}
\end{equation}

As previously, we introduce $\conv^\relation_{f^*}(z^*)$, the region of attraction of $z^*$ for $f^*$ \textit{modulo} $\relation$, as the set of $Z\in\Omega/\relation$ such that there exists a continuous path in $\Omega/\relation$ between $Z$ and $[z^*]_\relation$ where $\degen_{\shared^*}^\relation(.,z^*)$ is strictly decreasing. The difficulty of the pose estimation problem in the quotient space is inversely correlated to the expected fractional covering of  $\conv^\relation_{f^*}(z^*)$,
\begin{equation}
    0\leq D_\relation(f^*)=\mathbb{E}_{z^*\sim p^*,z\sim p}[z\in\conv^\relation_{f^*}(z^*)]\leq 1,
\label{eq:difficulty-rep}
\end{equation}
where $p$ is the uniform distribution over $\latentspace/\relation$ and $p^*$ the uniform distribution over $\latentspace$. Fig.~\ref{fig:method-3d} illustrates the notions of modulo loss, self-similarity map and region of attraction (for a fixed reference $z^*$) in 1D and 3D.

Although $\conv^\relation_{f^*}$ cannot be computed when $\shared^*$ is unknown, the relation $\relation$ can be chosen in a way that matches the general structure of $\degen_{\shared^*}$. For both inverse rendering problems, we introduce an equivalence relation $\relation_{\reporder}$, parameterized by $\reporder\in\mathbb{N}^*$ and defined by
\begin{equation}
    z\relation_{\reporder} z^\prime \quad \text{iff} \quad \exists k\in\mathbb{Z}, z= \frac{2k\pi}{\reporder} + z^\prime.
\end{equation}
$\reporder$ corresponds to the number of distinct elements in $[z]$ and will be called the ``replication order''. In 3D, $\relation_\reporder$ induces a replication of the cameras along the azimuthal dimension and is well-chosen for natural objects, which tend to be pseudo-symmetric by rotation around the vertical axis. For all $\reporder\in\mathbb{N}^*$, a natural homoemorphism $F:\latentspace\to\latentspace/\relation$ is defined by $F(z)=[z/\reporder]$. The quotient space $\latentspace/\relation$ can be seen as ``$\reporder$ times smaller'' than $\latentspace$.

\input{figref/ab_initio_qual}
