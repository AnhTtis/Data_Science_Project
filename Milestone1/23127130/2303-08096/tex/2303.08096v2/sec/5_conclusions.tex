\section{Discussion}

\methodname solves the problem of inverse rendering from unposed images using amortized inference and a novel loss function.
Using an equivalence relation that matches the distribution of local minima in a given object's self-similarity map, the \lossname reduces the camera space to a smaller space (its quotient set), in which gradient descent is more likely to converge.
We demonstrated that \methodname could perform inverse rendering on a variety of synthetic and real unposed datasets with state-of-the-art accuracy and that \methodname could cope with small or noisy datasets.

Characterizing the full loss landscape of 3D inverse rendering with unknown poses remains an open research question.
Our theoretical analysis focused on the pose estimation problem only, making the assumption that the reconstructed function $f$ was close to $f^*$, up to a global alignment.
Although the loss~\eqref{eqn:generic-loss} simplified to~\eqref{eq:loss-poses}, no inequality generally holds between the two. The full inverse rendering problem contains many more unknown parameters and ambiguities than pose estimation alone.
Furthermore, the assumption of a perfectly object-centered setup is too restrictive for reconstructing real world datasets and the prediction of camera extrinsics in $\text{SE}(3)$ remains an open challenge.

We demonstrated the possibility of optimizing a neural radiance field from unposed images. However, other volumetric representations are conceivable, such as tensorial products~\cite{Chan2021eg3d, Chen2022tensorf} hash tables~\cite{Muller2022ingp}, or explicit representations~\cite{fridovich2022plenoxels}. Exploring the strengths of other encodings within the context of our method is a promising future direction.

As real objects often contain rotational symmetries around the vertical axis, we focused on the use of equivalence classes that keep the elevation and roll of the cameras fixed. An interesting extension of this work could be to broaden the scope of equivalence relations used to mod out the latent space $\sothree$. 

% Other rigid symmetries, like planar reflections, could be addressed in a similar manner but would require the orientation of the symmetry plane to be known or learned. (an object that is symmetric by a planar reflection is NOT photometrically self-similar since a mirror operation in the camera would flip the rendered images too)