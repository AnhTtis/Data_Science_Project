\appendix

\setcounter{page}{1}

\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\setcounter{figure}{0}  
\setcounter{table}{0}  

\twocolumn[
\centering
\Large
\textbf{\titlemethod} \\
\vspace{0.5em}Supplementary Material \\
\vspace{1.0em}
] %< twocolumn





\section{Datasets}

\subsection{1D Dataset}

Fig.~\ref{fig:1d-dataset} shows an example of a ground truth 1D function $f^*$, its self-similarity maps and its regions of attraction. We show visual results for the ablation study of Fig.~5. Details on the generation process are given in the main paper (4.1). A jupyter notebook for generating these datasets and running our experiments will be provided in an open source repository.

\input{figref/1d_dataset}

\subsection{3D Datasets}

Table~\ref{tab:datasets} summarizes the properties of the datasets used for 3D inverse rendering. Fig.~\ref{fig:sph-dataset} shows sample views of the RGB-MELON datasets. These datasets and the script for generating them in Blender \cite{blender} will be provided upon publication.

\input{tab/datasets.tex}

\input{figref/sph_dataset}

\section{Baselines}

\subsection{COLMAP}

We ran COLMAP with the default configuration for performance evaluations. To ensure fair comparison, we swept parameters, running with different patch window radii (\verb|window_radius|) of 5, 3, 7, 9, 11, and different filtering thresholds of photometric consistency cost (\verb|filter_min_ncc|) of 0.1, 0.01, 0.2, 0.5, and 1.0. We tested all combinations of each.

COLMAP fails when it can not find enough corresponding points between frames. When it finds enough points, it does not always solve all the poses. We report the mean angular error over solved poses in the main text. For fair comparison to MELON and GNeRF, which always predict pose, we also compute a mean over all poses where the error of an unsolved pose is counted as a random guess (\textit{i.e.} $73^\circ$ when the elevation range is $[0^\circ, 90^\circ]$ and $90^\circ$ when it is $[-90^\circ, 90^\circ]$). Results are reported in Table~\ref{tab:colmap-means}.

In general, we note that the high mean angular error of COLMAP is due to large errors on few images. The distribution of angular errors (Fig.~\ref{fig:histo-colmap-errors}) shows a large discrepancy between the median and the mean.

\input{figref/histo_error_colmap}

\input{tab/colmap_means}

\subsection{GNeRF}

We run the public version of GNeRF, available at \url{https://github.com/quan-meng/gnerf} with the default parameters. When optimizing the poses in $\text{SO}(3)$, we use $6$-dimensional embeddings to parameterize the camera-to-world rotation matrices $R_i$ in the training and test sets. Based on the value of $R_i$ we set the camera location $t_i$ to
\begin{equation}
    t_i = -R_i\cdot T_i,
\label{eqn:translation-camera-to-world}
\end{equation}
where $T_i$ is the published 3D vector representing the location of the origin in the reference frame of camera $i$. The 3x4 matrix ($R_i$, $t_i$) represents an element of $\text{SE}(3)$.

\subsection{VMRF}
\input{tab/ab_initio_vmrf_comparison}
We compare \methodname to VMRF's reported results~\cite{Zhang2022vmrf} by running with the same configuration as in our GNeRF comparison, but with an an expanded elevation range of $[-30^\circ,90^\circ]$, the smallest range reported by VMRF. Results are show in Table \ref{tab:ab_initio_vmrf_comparison}.

\subsection{SAMURAI}

We run the public implementation of SAMURAI, available at \url{https://github.com/google/samurai}. In the fixed initialization scheme, we set all the initial directions to [Center, Above, Center] (North pole). When optimizing the poses in $\text{SO}(3)$, we overwrite the predicted camera locations using~\eqref{eqn:translation-camera-to-world}.

With a manual initialization, each view direction is specified with a triplet [Left/Center/Right, Above/Center/Below, Front/Center/Back]. [Center, Center, Center] is disallowed, giving a total of 26 possible initial directions.

\section{Architecture}

\subsection{Encoder}

Fig.~\ref{fig:encoder} summarizes the architecture of the encoder mapping 2D images to poses. For the 1D experiments, the encoder contains five 1D convolution layers of size $5$ interlaced with SiLU activation functions~\cite{hendrycks2016gaussian}, max-pooling layers and group normalization layers~\cite{wu2018group}.

\input{figref/encoder}

\subsection{Encoder Ablations}
\label{sec:encoder-ablation}
We perform a series of ablations to explore encoder architecture. We run \methodname on all training views of ``lego'', ``drum'', ``ficus'' and ``ship'' at $400\times400$ resolution and $\reporder=2$. We observe in our primary experiments that when poses converge, they tend to do so very quickly, often in the first thousand steps. We therefore train each configuration for 10k steps, but repeat 10 times to find min, max and mean metrics. We compute metrics as in other experiments over all test views. We choose a primary configuration of $L=64$, $F=32$, $d=4$ varying one parameter at a time in our experiments.

\input{figref/encoder_ablation}

Fig.~\ref{fig:encoder-ablation} shows the result of sweeping feature count $F$ from 8 to 256 and encoder depth $d$ from 2 to 6 layers. We observe optimal values at 4 layers, and a feature count of 32. 

% \input{figref/encoder_resolution_sweep}
Fig.~\ref{fig:encoder-ablation} also shows performance sweeping encoder input resolutions $L$ from $16\times16$ to $256\times256$. We observe that certain datasets appear to work better at different resolutions, with optimal values typically ranging from $32\times32$ to $128\times128$. We therefore choose per-scene values in our primary experiments. 


\subsection{Replication Order}
\input{figref/replication_order}
To determine the effect of varying the replication order, we run with the same base configuration as Sec.~\ref{sec:encoder-ablation}, but varying replication order over $\reporder = \{1, 2, 4\}$. We run \methodname 10 times to 10k steps on five scenes and plot the results in Fig.~\ref{fig:replication-order}. We observe a clear bi-modal behavior of convergence / non-convergence, and likelier convergence with higher $\reporder$, consistent with our analysis of Sec.~3.4. The scenes ``ficus'' and ``ship'' typically take longer than 10k steps to converge, and thus are not well illustrated by this study.

\subsection{Neural Radiance Field}
We use the standard NeRF architecture of \cite{Mildenhall20eccv_nerf}, an MLP with 8 layers of 256 features, and one skip connection to layer 4. Our density and RGB branches use a single layer of 128 channels. We use a single MLP, sampled at 96 stratified points, plus 32 additional importance sampled points, guided by the initial stratified samples. We keep the $t_f - t_n$ span fixed at 3.0 for baseline NeRF runs with ground truth cameras. For \methodname runs, we linearly interpolate from 1.0 to 3.0 during the first 10k steps, which we found to aid convergence. We ignore view dependence for the first 50k steps by providing random view directions to the network. After 50k steps, we anneal a maximum of 4 frequency bands of directional encoding, until training completion at 100k steps. We use the Adam optimizer \cite{kingma2014adam} with a learning rate of $10^{-4}$, $\beta_1=0.9, \beta_2=0.999$ and $128$ pixels per batch.

\section{Additional Results}

\subsection{Lego Scene}

We show the self-similarity map and the regions of attraction of the Lego scene for a fixed elevation and a variable reference pose $z^*$ in Fig.~\ref{fig:ssm-roa-lego-fixed-elevation}.

\input{figref/ssm_roa_lego_fixed_elevation}

\subsection{Comparison to VMRF}

In our comparison to VMRF, the predicted elevation is constrained to the range $[-30^\circ, 90^\circ]$, as done in~\cite{Zhang2022vmrf}. \methodname achieves higher reconstruction metrics on all the ``NeRF-Synthetic'' scenes (Table~\ref{tab:ab_initio_vmrf_comparison}).

\subsection{Number of Views}

We show qualitative reconstructions obtained on the ``lego'' dataset with a varying number of views in the training set in Fig.~\ref{fig:num-views-qual}.

\subsection{NeRF in the Wild}

We report quantitative metrics obtains on the real datasets in Table~\ref{tab:nerf_in_the_wild_qualitative} and qualitative reconstructions in Fig.~\ref{fig:nerf_in_the_wild_so3_se3}.

\input{tab/nerf_in_the_wild_quantitative}

\subsection{Noise Sensitivity}

We compare \methodname to COLMAP and GNeRF on noisy datasets. We add pixel-independent Gaussian noise of variance $\sigma^2$ to the training images and evaluate the mean angular error and novel view synthesis accuracy of competing methods in Table~\ref{tab:ab_initio_vmrf_comparison}. We show a qualitative comparison between \methodname and GNeRF in Fig.~\ref{fig:noise-sweep-gnerf}.

\input{tab/image_noise_sweep_small}

\input{figref/noise_sweep_gnerf}

\input{figref/num_views}

\input{figref/nerf_in_the_wild_so3_se3}
