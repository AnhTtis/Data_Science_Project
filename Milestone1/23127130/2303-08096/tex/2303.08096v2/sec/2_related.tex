\section{Related Work}
\hyphenation{iNeRF}

% \TODO{cite SRT and RUST?}

% https://geometry.stanford.edu//papers/ompg-smvqs-13/ompg-smvqs-13.pdf
% https://geometry.stanford.edu//papers/ohg-cnsm-11/ohg-cnsm-11.pdf
% \TODO{cite "Shape Matching via Quotient Spaces", "A Condition Number for Non-Rigid Shape Matching"}

\label{sec:relatedwork}
\paragraph{Neural Radiance Fields~(NeRF)}~\cite{Mildenhall20eccv_nerf} use classical volume rendering~\cite{Kajiya1984rtv} to compute the RGB values $\nerfcolor$ for each pixel~$\pixel$ from samples taken at points $\position$ along a ray of direction $\viewdir$. The ray direction is determined by the camera parameters $R$ and the pixel location.
These samples are computed using learned radiance ($\radiance$) and density ($\density$) fields.
The volume rendering equation takes the form of an integral along a ray between near and far view boundaries $(t_n,t_f)$:
\begin{equation}
\nerfcolor(\pixel~;\radiance,\density,R) = \int_{t_n}^{t_f} T(t)\density(\position(t))\radiance(\position(t),\viewdir)\diff t
\label{eqn:nerf}
\end{equation}
where the transmittance is given by
\begin{equation}
\label{eqn:sample_weight}
T(t)= \exp\left(-\int_{t_n}^t\density(\position(s))\diff s\right).
\end{equation}

Neural Radiance Fields have proven capable of high-quality novel view synthesis and have been a recent focus in the research community.
They have been extended in many ways including training from noisy HDR images \cite{Mildenhall2022rawnerf}, grid-based representations for faster training and inference~\cite{Sun2022dvgo, Muller2022ingp, Wang2022plenoctrees, Reiser21iccv_KiloNeRF}, training with limited or single views \cite{Yu21cvpr_pixelNeRF, Jain21iccv_DietNeRF, Xu2022sinnerf}, decomposition into sub-fields~\cite{Rebain20arxiv_derf, Reiser21iccv_KiloNeRF}, unconstrained images in the wild \cite{MartinBrualla21cvpr_nerfw}, deformable fields \cite{Park21iccv_nerfies, Noguchi21iccv_NARF, Raj2021pva}, and video~\cite{Li2022dynerf}. 
Most of these techniques however assume camera poses to be given, and automatically inferring them during training remains an open research question.


\textbf{Structure-from-Motion.}
Reconstruction from unposed images has been a major thrust of research in the structure from motion community for many years~\cite{Ozyesil2017sfmsurvey, Iglhhaut2019sfmreview}. 
COLMAP~\cite{Schonberger2016sfmrevisited} is currently the most commonly used method to infer pose when unavailable.
Though robustly usable for a number of cases, COLMAP relies on SIFT feature matching and can fail in cases where SIFT features cannot be found, such as scenes with limited edges, corners and texture, or view-dependent effects like reflections, refractions or specular surfaces.

\textbf{Neural Representations without Poses.}
Pose estimation of neural scenes can be divided into those which assume a known scene, and those which simultaneously reconstruct a scene along with poses. 
Of those assuming a known scene, iNeRF~\cite{YenChen20iros_iNeRF}, further extended by~\cite{Lin2022pin}, demonstrated that pose could be recovered with stochastic gradient descent and ``interest region sampling'', when initialized within $\pm40^\circ$ of ground truth orientation.

NeRF$\shortminus\shortminus$~\cite{Wang21arxiv_NeRFminusminus} was the first to co-optimize a neural field and 6-DOF camera poses simultaneously.
To avoid local minima, the neural field was re-initialized mid-training, and results were only shown for forward-facing scenes.
BARF~\cite{Lin21iccv_BARF} proposed a coarse-to-fine positional encoding annealing strategy~\cite{Park21iccv_nerfies} and GARF~\cite{Chng2022garf} argued that Gaussian activation functions were more robust for pose estimation. SCNeRF~\cite{Jeong2021scnerf} extended the parameterization to include camera instrinsics and, like SPARF~\cite{truong2022sparf}, incorporated multi-view geometric constraints. These methods avoid the need to re-initialize the field and demonstrated results for non forward-facing scenes, but require a rough initialization of the camera poses. SAMURAI~\cite{Boss2022samurai} simultaneously optimizes several cameras with a novel camera parameterization and tackles non forward-facing scenes but requires poses to be initialized to one of 26 possible directions. NoPe-NeRF~\cite{Bian2022nopenerf} introduces additional supervision from an off-the-shelf monocular depth estimation network and novel losses that encourage 3D consistency between the scene and successive frames in a video sequence.
All these techniques require initialization within some fixed bound of ground truth pose, or a series of views known to be close in pose to one another.

Approaches like iMAP~\cite{Sucar21iccv_iMAP} or NICE-SLAM~\cite{Zhu2022niceslam} perform reconstruction from hand-held camera, but requires depth information and ignore view-dependence in the neural field.

\textbf{Adversarial Approaches }\cite{Henzler2019platonicgan, NguyenPhuoc2019hologan, Niemeyer2021giraffe, Chan21cvpr_piGAN} jointly supervise a generator containing a differentiable neural field and a discriminator.
Such techniques enable the reconstruction of a radiance field while bypassing the pose estimation problem.
GNeRF~\cite{Meng21iccv_GNeRF} trains on a single scene by supplying image crops to the discriminator, increasing the effective data set size. It co-trains a pose encoder supervised with renderings of the neural scene, which is then used to estimate ground truth image poses.
Using a complex training schedule GNeRF, is able to reconstruct a single scene from completely unposed images.
VMRF~\cite{Zhang2022vmrf} refined this work by using a pre-trained feature detector, differentiable unbalanced optimal transport solver and relative pose predictor to provide extra supervision achieving better reconstruction and pose estimation than GNeRF.

Although such techniques enable the reconstruction of a photo-realistic 3D field from unposed images, they require large datasets for training the discriminator, complex training schedules, and suffer the inherent instabilities and hyper-parameter sensitivities of GANs.

\textbf{Supervised Pose Regression} methods approach the pose estimation (or ``camera calibration'') problem as a regression problem, where a mapping from image space to pose space needs to be approximated \cite{Hu2020sss, lian2022end, kendall2015posenet, kendall2017geometric, mahendran20173d, su2015render, li2019monocular, Lopez2019dsi} and \cite{sattler2019understanding} analyzed the limits of CNN-based methods.
Focusing on the degenerate case of symmetric objects, Implicit-PDF~\cite{murphy2021implicit} showed that an image can be mapped to a multimodal posterior distributions stored in a coordinate-based representation and RelPose~\cite{zhang2022relpose} introduced an energy-based approach to estimate joint distribution over relative viewpoints in $\sothree$. SparsePose~\cite{sinha2022sparsepose} trains a coarse pose regression model on a large dataset and refines camera poses in $\sethree$ in an auto-regressive manner.
However, these approaches require training over a large data set with known poses and do not provide a neural field of the scene.

Previous works proposed ways to build rotation-equivariant networks \cite{Deng2021vectorneurons, elesedy2021provably, batzner20223, fuchs2020se, chen2021equivariant, esteves2018learning, esteves2019equivariant, nasiri2022unsupervised}.
Such symmetry-aware networks showed improved generalization properties for processing point clouds or spherical images but do not ensure $\text{SO}(3)$-equivariance for pose estimation since no group action for the group $\text{SO}(3)$ can be defined on the set of 2D images~\cite{Klee2022iip}.

\textbf{Unsupervised Approaches }use an encoder--decoder architecture mapping the image space to an $\text{SO}(3)$-valued latent space and mapping this space back to the image space with a group-equivariant generative model \cite{Falorsi2018ehv}.
Spatial-VAE \cite{bepler2019explicitly} demonstrated translation and in-plane rotation estimation for 2D images using a neural-based 2D generative model in the decoder, while TARGET-VAE \cite{nasiri2022unsupervised} proposed to use a translation and in-plane rotation equivariant encoder.
SaNeRF \cite{Chen2022san} runs classical COLMAP SIFT feature detection and mapping, but regresses relative poses from a triplet of images fed to a CNN.
As it uses SIFT feature matching, this method is not suitable for objects containing large texture-less regions or repeated structures.

Approaches from the Cryo-Electron Microscopy (Cryo-EM) community \cite{Levy2022aih, Levy2022cryoai} recently showed the possibility of using an auto-encoding approach to reconstruct a 3D neural field in Fourier domain~\cite{zhong2021cryodrgn, shekarforoush2022residual} from unposed 2D projections.
These methods do not require any initialization for the poses and have been shown to work on datasets with high levels of noise.
The image formation model in cryo-EM is orthographic and does not involve opacity, allowing faster computation with the Fourier Slice Theorem.
Building on these approaches, our method copes with a more complex rendering equation that incorporates perspective and opacity.
