\section{Results}

\subsection{Datasets}
\label{subsec:datasets}

\input{tab/ab-initio_comparison}

\textbf{1D Dataset}
We generate random functions $f^*$ that map $\sotwo$ to $\mathbb{R}$ by randomly drawing their $512$ first Fourier coefficients $c_k$ following a centered Gaussian distribution. The expected magnitudes of these Fourier coefficients is $\exp(-|k|/5)$ and we make the functions almost symmetric by setting $c_{\pm 2}=100$. We generate ten one-dimensional functions containing $256$ crops of $65$ uniformly spaced samples of $f^*$. We show an example of ground truth function and its self-similarity map in the supplements.

\textbf{RGB-MELON} 
We build a pathological dataset containing centered views of 3D spheres on which we map almost-symmetric textures. We use spherical harmonics to generate red--green textures that are invariant by translation of $2\pi/K$ along the azimuthal direction, with a tunable \textit{symmetry order} parameter $K\in\{1,2,3,4\}$. In order to break the perfect symmetry of these scenes, we add three red/green/blue squares along the azimuthal direction. We generate $116$ views from a fixed and known distance, with all the cameras located in the equator plane. We hold out $16$ test images in each dataset. Sample views of these datasets are provided in the supplements.

\textbf{``NeRF-Synthetic'' Dataset} We use the eight object-centric scenes provided by \cite{Mildenhall20eccv_nerf}, which consist of $300$ rendered images (downscaled to $400\times 400$ pixels) and ground truth camera poses for each scene. All the methods evaluated on these datasets assume the object-to-camera distance and the ``up'' direction to be known (fixed roll $\alpha$). We hold out the $200$ ``test'' views, using them only for evaluation.

\textbf{Real-World Datasets} We evaluate our method using three real-world scenes: the Gold Cape~\cite{goldcape} and the Ethiopian Head~\cite{ethiopianhead} from the British Museum's photogrammetry dataset, and a Toytruck from the Common Object in 3D (CO3D) dataset~\cite{reizenstein2021common}. These scenes are approximately object-centered and were recorded with fixed illumination conditions. We consider the published camera poses as ground truth. We use ground truth masks to remove the background before computing the photometric loss.

\subsection{1D Inverse Rendering}
\label{subsec:1dinverse}
In our 1D study we map crops to predicted angles with an encoder, trivially "render" with the identity following \eqref{eqn:one-d-forward-model}, use the modulo loss and reconstruct the function $f_\weightsshared$ with a neural network (positional encoding \cite{Mildenhall20eccv_nerf} and four fully-connected layers of size 64 with ReLU activation functions). We perform an ablation study and separately (1) replace the neural representation $f_\weightsshared$ with an explicit representation, (2) replace the modulo loss with an L2 loss and (3) replace the encoder with a set of independently optimized angles. We use the Adam optimizer \cite{kingma2014adam} with a learning rate of $10^{-4}$ and $256$ crops per batch. We align the predicted angles (as defined by~\eqref{eqn:predicted-latents}) and the predicted function to the ground truth before computing mean square errors. As shown in Fig.~\ref{fig:torus-results}, the neural representation, the modulo loss and the encoder are all necessary for the method to work, in this order of importance. We hypothesize that explicit grids are not well-suited for this task because they do not have any inductive bias encouraging smoothness. Since the derivative of the loss w.r.t. $\theta_i$ involves $f_\weightsshared^\prime$, gradient-descent gets stuck at an early stage (visual reconstructions in the supplements). The modulo loss mods out the latent space, making it more convex. The encoder leverages the fact that similar images should be mapped to similar poses.

\input{figref/torus_results}

\subsection{NeRF with Unposed Images}

Using the ``NeRF-Synthetic'' dataset, we compare \methodname to COLMAP~\cite{Schonberger2016sfmrevisited} and GNeRF~\cite{Meng21iccv_GNeRF} for pose estimation and novel view synthesis. In Table~\ref{tab:ab-initio_comparison}, we quantify the accuracy of pose estimation by reporting the mean angular error of the predicted view directions on the training set. We evaluate novel view synthesis quality by computing the Peak Signal Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM)~\cite{ssim} and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{lpips} against all 200 test views. The pose accuracy is reported after applying a global alignment matrix minimizing the mean angular error over $\sothree$. A qualitative comparison is shown on Fig.~\ref{fig:ab-initio-qual}. \methodname outperforms GNeRF on novel view synthesis, thanks to more accurate estimation of the camera poses. COLMAP fails on scenes where it does not find enough corresponding points (``ficus'') while a small number of outliers significantly increases the mean angular error on ``chair''.

\input{figref/noise_sweep}

\subsection{Noise and Number of Views}

We illustrate the sensitivity of \methodname to noise in Fig.~\ref{fig:noise-sweep}. We add white Gaussian noise of variance $\sigma^2$ to the training images of the ``lego'' dataset. For this experiment, we use a view independent neural field.
\methodname accurately estimates the poses on the training set and generates noise-free novel views. In this context, our method can be seen as a ``3D-consistent'' unposed image denoiser.
A comparison to COLMAP is provided in the supplements.

Unlike adversarial approaches, \methodname does not need to train a model to discriminate real from synthetic images, and can therefore perform \textit{ab initio} inverse rendering with datasets containing few images. We artificially reduce the size of the training set in the ``lego'' scene and report the results of \methodname with a replication order $\reporder=2$ in Table~\ref{tab:num_views_sweep}. We compare our method to the adversarial approach GNeRF, and to COLMAP. Novel views are shown in the supplements.

COLMAP does not find enough corresponding points when given 16 or less views. In comparison, \methodname accurately estimates the poses from only six views while GNeRF's accuracy significantly degrades with the number of views. We hypothesize that supervising the field with slightly wrong cameras at an early stage of training can act as a regularizer and could explain why \methodname sometimes gets a marginally higher PSNR than NeRF with ground truth poses.

\input{tab/num_views_sweep}

\subsection{Replication Order}

\input{tab/sph_results}

We run \methodname with a replication order $\reporder\in\{1,2,3,4\}$ to reconstruct the RGB-MELON scenes. In Table~\ref{tab:sph-results}, we report, for each value of $\reporder$ and each symmetry order $K$, the fractional size $D_\relation(f^*)$ of the regions of attraction for $\relation=\relation_\reporder$. This number is closer to $1$ when the replication order ``matches'' the symmetry order of the dataset, \textit{i.e.} when $N$ is a multiple of $K$.

We report the quality of novel view synthesis on the test set and pose estimation on the training set. COLMAP fails at estimating the poses due to the presence of repeating patterns and the absence of sharp corners. In comparison, our method converges when the replication order matches the symmetries of the texture. As $\reporder$ increases, our method can cope with a broader range of datasets at the cost of more computation.

\subsection{Towards NeRF in the Wild}

\methodname assumes that all the cameras point towards the center of the scene from a known distance. In a real capture setup, the object-to-camera distance, $r$, is unknown and the cameras do not all point towards a unique point. We parameterize each camera with a camera-to-world rotation matrix $R_i\in\sothree$ and a three-dimensional vector $T_i\in\mathbb{R}^3$ that specifies the location of the origin in the camera frame. This vector is constant and equal to $(0, 0, r)$ in an ideal object-centered setup.

In Fig.~\ref{fig:nerf_in_the_wild}, we compare our results to those obtained with GNeRF~\cite{Meng21iccv_GNeRF} and SAMURAI~\cite{Boss2022samurai}, for the estimation of the rotation matrix $R_i$ only. For all methods, we use the published values for $T_i$ and parameterize the elevations in $[0^\circ,90^\circ]$. Since SAMURAI requires the user to manually initialize the camera view directions to one of 26 possible directions, we show the results obtained with a fixed initialization at the North pole for fair comparison. Quantitative results and additional comparison to a manual initialization of SAMURAI are shown in the supplements.

When both the object-to-camera distances and the in-plane camera translations are unknown, the latent space becomes $\text{SE}(3)$ and contains twice as many degrees of freedom as $\text{SO}(3)$. Although it can accurately estimate the rotation matrix $R_i$, we show in Fig.~\ref{fig:limitations} that \methodname fails at jointly inferring $R_i$ and $T_i$. In this setup, SAMURAI is the only effective reconstruction method but requires a rough initialization of the camera view directions (see supplements).

\input{figref/nerf_in_the_wild.tex}
\input{figref/limitations}
