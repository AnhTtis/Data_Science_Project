% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{4940} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Supplementary Materials:   

3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process
}

\author{Yuhan Li\textsuperscript{1}\hspace{2mm}
Yishun Dou\textsuperscript{2}\hspace{2mm}
Xuanhong Chen\textsuperscript{1}\hspace{2mm}
Bingbing Ni\textsuperscript{1, 2$\dagger$}\hspace{2mm}
Yilin Sun\textsuperscript{1}\hspace{2mm}
Yutian Liu\textsuperscript{1}\hspace{2mm}
Fuzhen Wang\textsuperscript{1}\\
\textsuperscript{1}Shanghai Jiao Tong University, Shanghai 200240, China \qquad \textsuperscript{2}Huawei Hisilicon \\
{\tt\small \{melodious, nibingbing\}@sjtu.edu.cn}\\
{\small \url{https://github.com/colorful-liyu/3DQD}}
}

\maketitle

\newcommand{\customfootnotetext}[2]{{% Group to localize change to footnote
  \renewcommand{\thefootnote}{#1}% Update footnote counter representation
  \footnotetext[0]{#2}}}% Print footnote text
\customfootnotetext{${\dagger}$}{Corresponding author: Bingbing Ni.}



%%%%%%%%% BODY TEXT

%-----------------------------------------------------------------------------------

We first provide the implementation details of the P-VQ-VAE, discrete diffusion generator and condition pipeline in~\cref{sec:imple}. More ablation study about important settings is reported in~\cref{sec:more_able}. Technical details about experiments are given in~\cref{sec:expe}, with more visual results in~\cref{sec:visual}.

\section{Implementation} 
\label{sec:imple}
\subsection{P-VQ-VAE Backbone} 

\paragraph{Architecture details.} 
Our P-VQ-VAE backbone consists of three components: an encoder $E$, a decoder $D$ and a vector quantizer $VQ$ with convolutions. Following AutoSDF~\cite{mittal2022autosdf}, we adapt the VQ-VAE from the VAE backbone of LDM~\cite{rombach2021ldm}. We show the details of encoder in \cref{tab:encoder}, the decoder in \cref{tab:decoder}, and the vector quantizer in \cref{tab:vq}. 

\paragraph{Dataset details.} 
We train the P-VQ-VAE using the objects from 13 categories of ShapeNet~\cite{chang2015shapenet} data, including [\textit{airplane, bench, cabinet, car, chair, display, lamp, speaker, rifle, sofa, table, phone, watercraft}]. We first extract the Truncated-SDF (T-SDF) following pre-processing steps in DISN~\cite{xu2019disn} and PixelTransformer~\cite{tulsiani2021pixeltransformer}. The shapes are normalized to lie in an origin-centered cube in $[-1, 1]^3$, while most shape T-SDFs' absolute values are less than $0.5$. The signed distance function is evaluated at locations in a uniformly sampled $64^3$ grid. Following AutoSDF~\cite{mittal2022autosdf}, we use $0.2$ as the threshold to further obtain
the T-SDFs representations.


\paragraph{Training details.} 
Then the whole 3D shape in the format of T-SDF $X \in \mathbb{R }^{64\times 64\times 64}$ is divided into 512 partial regions $X'\in \mathbb{R }^{N\times 8\times 8\times 8}$, and $N=512$ is the number of non-overlap regions, for directly working on whole shape is computationally unaffordable with cubic increase with resolution. Afterward, all regions are vectorized by the encoder as $Z = E(X) \in \mathbb{R }^{N\times n_z}$, while each patch is treated independently and equally. 



Next, we obtain a spatial collection of quantized shape tokens $Z_q=\left \{ z_{q}^0, \dots, z_{q}^{N-1} \right \}$ by a further quantization step as $z_{q}^i = VQ(z^{i}) \in \mathbb{R }^{n_z}$, and their corresponding codebook indexes $S=\left \{ s^{0}, \dots, s^{N-1} \right \}$, where $s^i\in \left \{ 0, \dots, K-1 \right \}$. $VQ$ is a spatial-wise quantizer which maps each partial shape $z^i$ into its closest codebook entry in $\mathcal{Z} $. Complete latent space $Z$ and $Z_q$ is produced by gathering $z^i$ and $z_q^i$ for all location $i$ into grids. Finally, decoder $D$ decodes quantized feature $Z_q$ to output the reconstruction $X'$. The training objective consists of three parts: reconstruction loss, VQ loss and commitment loss:

\begin{equation}
L=\underset{Reconstruction~loss}{ \underbrace{\log P(X|Z_q)}} + \underset{VQ~loss}{ \underbrace{\left \| sg\left [ Z \right ] -Z_q \right \| ^2}} + \beta \underset{Commitment~loss}{ \underbrace{\left \|  Z -sg\left [ Z_q \right ]  \right \| ^2}},
  \label{vqvaeloss}
\end{equation}
where $sg[Z]$ means stopping the gradient of Z, so the optimization will only impact the other item. The learning rate is set to be $1e-4$, which halves every 30 epochs. Adam optimizer is used with betas = [0.5, 0.9].


\begin{table*}[]
  \centering{
\begin{tabular}{lccc}
  \toprule
$E$ Layer name & Parameters          & Input size                            & Output size \\
\midrule
Conv3D     & kernel size 3  & $Bs\times1\times8\times8\times8$ & $Bs\times64\times8\times8\times8$            \\
3D ResNet Block  & kernel size 3  & $Bs\times64\times8\times8\times8$   & $Bs\times64\times8\times8\times8$    \\
Down-sample  & kernel size 3, stride 2, padding 0 & $Bs\times64\times8\times8\times8$   & $Bs\times64\times4\times4\times4$    \\    
3D ResNet Block  & kernel size 3  & $Bs\times64\times4\times4\times4$   & $Bs\times128\times4\times4\times4$    \\
Down-sample  & kernel size 3, stride 2, padding 0 & $Bs\times128\times4\times4\times4$   & $Bs\times128\times2\times2\times2$    \\  
3D ResNet Block  & kernel size 3  & $Bs\times128\times2\times2\times2$   & $Bs\times128\times2\times2\times2$    \\
Down-sample  & kernel size 3, stride 2, padding 0 & $Bs\times128\times2\times2\times2$   & $Bs\times128\times1\times1\times1$    \\  
3D ResNet Block  & kernel size 3  & $Bs\times128\times1\times1\times1$   & $Bs\times256\times1\times1\times1$    \\
3D ResNet Block  & kernel size 3  & $Bs\times256\times1\times1\times1$   & $Bs\times256\times1\times1\times1$    \\
3D Attention Block  & Multi-head 1 & $Bs\times256\times1\times1\times1$   & $Bs\times256\times1\times1\times1$    \\
3D ResNet Block  & kernel size 3  & $Bs\times256\times1\times1\times1$   & $Bs\times256\times1\times1\times1$    \\
Group Norm   & number of groups 32   &    $Bs\times256\times1\times1\times1$   & $Bs\times256\times1\times1\times1$     \\
Swish     &       -       &    $Bs\times256\times1\times1\times1$     &  $Bs\times256\times1\times1\times1$    \\
Conv3D    &kernel size 3    &  $Bs\times256\times1\times1\times1$     &  $Bs\times256\times1\times1\times1$      \\
\bottomrule
\end{tabular}}
\caption{Architecture of encoder $E$ of P-VQ-VAE. The default stride is set to 1.  The default padding is set to 1.}
  \label{tab:encoder}
\end{table*}


\begin{table*}[]
  \centering{
\begin{tabular}{lccc}
  \toprule
$VQ$ Layer name & Parameters          & Input size                            & Output size \\
\midrule
Conv3D     & kernel size 3  & $Bs\times256\times1\times1\times1$ & $Bs\times256\times1\times1\times1$  \\Quantizer     & codebook $K \times 256$  & $Bs\times256\times1\times1\times1$ & $Bs\times256\times1\times1\times1$  \\
Conv3D    &kernel size 3 & $Bs\times256\times1\times1\times1$  &$Bs\times256\times1\times1\times1$  \\
\bottomrule
\end{tabular}}
\caption{Architecture of vector quantizer $VQ$ of P-VQ-VAE. The default stride is set to 1.  The default padding is set to 1.}
  \label{tab:vq}
\end{table*}


\begin{table*}[]
  \centering{
\begin{tabular}{lccc}
  \toprule
$D$ Layer name & Parameters          & Input size                            & Output size \\
\midrule
Conv3D     & kernel size 3  & $Bs\times256\times1\times1\times1$ & $Bs\times256\times1\times1\times1$            \\
3D ResNet Block  & kernel size 3  & $Bs\times256\times1\times1\times1$   & $Bs\times256\times1\times1\times1$    \\
3D Attention Block  & Multi-head 1 & $Bs\times256\times1\times1\times1$   & $Bs\times256\times1\times1\times1$    \\
3D ResNet Block  & kernel size 3  & $Bs\times256\times1\times1\times1$   & $Bs\times256\times1\times1\times1$    \\
Up-sample  & kernel size 3, stride 2, padding 0 & $Bs\times256\times1\times1\times1$   & $Bs\times256\times2\times2\times2$    \\  
3D ResNet Block  & kernel size 3  & $Bs\times256\times2\times2\times2$   & $Bs\times128\times2\times2\times2$    \\
3D Attention Block  & Multi-head 1 & $Bs\times128\times2\times2\times2$   & $Bs\times128\times2\times2\times2$    \\
Up-sample  & kernel size 3, stride 2, padding 0 & $Bs\times128\times2\times2\times2$   & $Bs\times128\times4\times4\times4$    \\  
3D ResNet Block  & kernel size 3  & $Bs\times128\times4\times4\times4$   & $Bs\times64\times4\times4\times4$    \\
Up-sample  & kernel size 3, stride 2, padding 0 & $Bs\times64\times4\times4\times4$   & $Bs\times64\times8\times8\times8$    \\  
3D ResNet Block  & kernel size 3  & $Bs\times64\times8\times8\times8$   & $Bs\times64\times8\times8\times8$     \\
3D ResNet Block  & kernel size 3  & $Bs\times64\times8\times8\times8$   & $Bs\times64\times8\times8\times8$     \\
Group Norm   & number of groups 32   &    $Bs\times64\times8\times8\times8$   & $Bs\times64\times8\times8\times8$     \\
Swish     &       -       &    $Bs\times64\times8\times8\times8$     &   $Bs\times64\times8\times8\times8$    \\
Conv3D    &kernel size 3    &  $Bs\times64\times8\times8\times8$     &  $Bs\times1\times8\times8\times8$     \\
\bottomrule
\end{tabular}}
\caption{Architecture of decoder $D$ of P-VQ-VAE. The default stride is set to 1.  The default padding is set to 1.}
  \label{tab:decoder}
\end{table*}


%-----------------------------------------------------------------------------------

\subsection{Discrete Diffusion Generator} 


\paragraph{Architecture details.} 
We adopt a transformer-based architecture to model the joint probabilistic distribution prior over 3D shapes. The transformer consists of 16 Ordinary Blocks, 3 Multi-frequency Modules (MFM), and a Final Block with the same setting as Ordinary Blocks, as shown in~\cref{diff_pipeline}.

We show the details of the Ordinary Block in \cref{ordinary}. The default dimension of latent feature is $C=256$. All activation function is \textit{GELU2()}. AdaLayerNorm layers are used to synthesize timestep and class-label information and modulate the whole generation process, since timestep $t$ is an important parameter to control the degree of denoising of outputs, while MLP has a boosting bottleneck of $4\times C$ channels. The MFM doubles the branches of information flowing in Ordinary Blocks with intra- and inter-communication. 

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.9\linewidth]{cvpr2023-author_kit-v1_1-1/img/pipline.png}
   \caption{The complete denoising transformer framework which is used in reverse process.}
   \label{diff_pipeline}
\end{figure*}

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.8\linewidth]{cvpr2023-author_kit-v1_1-1/img/ordinary.png}
   \caption{Ordinary block with a boosting bottleneck MLP.}
   \label{ordinary}
\end{figure*}

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.8\linewidth]{cvpr2023-author_kit-v1_1-1/img/ca_ordinary.png}
   \caption{Ordinary block with cross-attention to modulate multi-modality information.}
   \label{ca_ordinary}
\end{figure*}


\paragraph{Training details.} 
After the traning of P-VQ-VAE, each shape is encoded into quantized shape tokens $Z_q$ with their codebook index map $S=\left \{ s^{0}, \dots, s^{N-1} \right \}$, where $s^i\in \left \{ 0, \dots, K-1 \right \}$. The index map is sent to the forward diffusing process followed by reverse learnable denoising chain. In this stage, the encoder and the decoder are frozen, and only the diffusion generator network is trained using the objectives with uniformly sampled timestep $t$ from $\left \{ 1, 2, \dots, 100 \right \} $:
\begin{equation}
L=-\mathbb{E} _{q(s_0)q(s_t|s_0)}\left [\underset{L_{main}}{\underbrace{\log p_{\theta}(s_{t-1}|s_t)}} +\underset{L_{aux}}{\lambda \underbrace{\log p_{\theta}(\hat{s}_0|s_t)}} \right ],
  \label{Loss}
  \vspace{-2mm}
\end{equation}
where $\lambda $ is $1e-3$.


Following minGPT~\cite{radford2019language}, we separate out all parameters of the model into two buckets: some will experience weight decay for regularization and the others won't (\ie biases, and layernorm / embedding weights). The learning rate is set to be $1e-4$, which multiplies 0.9 every 30 epochs. Besides, to stabilize the training, we convert one-hot row vector $s$ to log one-hot vector with a minimum value of $-69.07$ ($ln(-30)$) according to Argmax Flow~\cite{hoogeboom2021argmax}.

%-----------------------------------------------------------------------------------

\subsection{Condition Injection Module} 
\label{sec:condition}
In face of the challenge of various conditional generation tasks like
shape completion, or generation based on different modalities like images and language, considering a single way to solve all problems is not possible. We design two ways to inject the different modality information which are concisely introduced in main body of this paper. More details are given here.

\paragraph{Shape initialization as conditions.}
Generally speaking, in the inference stage, the diffusion generator starts the reverse denoising process from completely corrupted or masked shape tokens $S_T$ and recovers the token maps $S_T$ towards the desired data distribution $S_0$. In this case, the token maps $S_T$ are from the prior setting (\ie, fully masked matrices). However, if the shapes or parts of shapes $\tilde{s}_0$ (\eg the editing task and completion task with given shapes) are provided, we will go on the forward diffusing process for $k$ timesteps on $\tilde{s}_0$ and obtain partially corrupted conditional feature maps $\tilde{s}_k$, where some original shape structure information still remains. Initialized with $\tilde{s}_k$ rather than fully masked tokens $S_T$, the learned reverse process can be derived as:
\begin{equation}
p_{\theta }(\tilde{s}_{0:k})=p(\tilde{s}_k) {\textstyle \prod_{t=1}^{k}p_{\theta}(\tilde{s}_{t-1}|\tilde{s}_t)},
  \label{cond1}
\end{equation}
gradually removing the added noise and generating a 3D shape from condition. Smaller $k$ value means corrupting the condition sightly, so the freedom to denoise and reorganize conditionally initialized token maps is quite restricted, leaning towards less diversity and limited rebuilding capability (\ie, worse TMD and MMD). While a large $k$ value means hurting the condition seriously, it always leads to low-fidelity and distorted generated results, violating the condition (\ie, bad MMD).

We use this kind of condition injection pipeline for shape completion task ($k=0.5\times T=50$), shape denoising task ($k=0.5\times T=50$), and text-driven shape editing ($k=0.98\times T=98$).

\paragraph{Cross-attention for conditions.} Although shape initialization solves some problems, not all conditions can be expressed as token maps initialization, especially condition $c$ with different modalities (\eg, images, texts). As a consequence, inspired by ~\cite{rombach2021ldm}, we use cross-attention modules to inject conditions. At first, conditions with different modalities are encoded into discrete and quantized tokens embedding with its own encoder. For texts, we utilize pre-trained CLIP~\cite{radford2021clip} model (ViT-B) to project text prompts to 77 tokens $c_q$. As for single-view images, VQ-VAE from ~\cite{mittal2022autosdf} is used to encode images to 512 tokens $c_q$, which is specifically trained on image data on Pix3D dataset~\cite{sun2018pix3d} and aligned with shape tokens. Next, only the indexes $c_I$ of these tokens among codebook entries are preserved, while $c_q$ is discarded. The wide use of indexes is beneficial to allow the model to receive uniform input (\ie, conditions and shapes are all expressed with integers), and eliminate the inherent differences in data distribution among different modalities. Then the conditional token indexes are projected to tensors with learnable embedding modules, followed by fully connected layers to encode them to $key$ and $value$ into cross-attention. The Ordinary Block and MFM are modified with a LayerNorm and a cross-attention module behind self-attention, as shown in \cref{ca_ordinary}.


%-----------------------------------------------------------------------------------
%-----------------------------------------------------------------------------------

\section{More Ablation Study} 
\label{sec:more_able}

\subsection{Shape novelty analysis.} 
To statistically analyze the novelty of the generated shapes, we visualize the distribution of LFDs between our generated shapes and retrieved shapes in \cref{lfd}. Note that our 3DQD can not only learn the distribution of training data (low LFD), but also generate novel shapes (high LFD) that are different from the training data.

\begin{figure}[t]
\vspace{-5mm}
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/ourlfd.pdf}
   \vspace{-3mm}
   \caption{We generate 500 random chairs using 3DQD, then retrieve the most similar shape in training set for each samples with LFD metric. The distribution of LFDs between samples (in green) and retrieved shapes (in blue) is plotted here.}
   \label{lfd}
\vspace{-6mm}
\end{figure}


%-----------------------------------------------------------------------------------
\subsection{Multi-frequency Fusion Module Settings.} 
In the main body of this paper, we have analyzed the frequency components of the generation to validate our design of Multi-frequency Fusion Module (MFM) with quantitative and visual comparisons. In this section, to verify our designed MFM settings, we compare 4 configurations about MFM on the shape completion task:
\begin{itemize}
\item 3DQD model with different MFM layers. It still utilizes residual add as Pair-wise Fusion Operator (PFO) with the complete MFM pipeline.
\end{itemize}

\begin{itemize}
\item 3DQD model with cross-attention to fuse two branches of frequency components. In this case, adaptive rescaling is no longer required, for cross-attention is able to align features with different spatial dimensions with $QKV$.
It implements 3 complete MFM layers.
\end{itemize}

\begin{itemize}
\item 3DQD model with single-fusion MFM layers as shown in \cref{Fig: MFM_abla}. In this case, the inter-communication between $y^L$ and $x^L$ is absent, so the fusion only occurs in the other side. It uses residual add as fusion operator with 3 MFM layers.
\end{itemize}

\begin{itemize}
\item 3DQD model with single-attention MFM layers as shown in \cref{Fig: MFM_abla}. In this case, the intra-communication within $y^L$ is absent. It uses residual add as fusion operator with 3 MFM layers.
\end{itemize}

Quantitative results are shown in \cref{tab:abla_MFM}, where we compare our full MFM pipeline with various ablated cases. The best choice for MFM is utilizing residual add as PFO, with 3 full MFM pipeline layers.

\begin{table}
\resizebox{\linewidth}{!}{
  \centering
  %\resizebox{\linewidth}{!}
  {
  \begin{tabular}{ccccc}
  \toprule
MFM layers & Fusion types & Dual fusion & Dual-attn & MMD$\downarrow $   \\
\midrule
3          & residual add & \ding{52}   & \ding{52} & \textbf{0.2934} \\ \hline \hline

0          &              &             &           & 0.3205 \\
1          &              &             &           & 0.3083 \\
2          &              &             &           & 0.3001 \\
4          &              &             &           & 0.2942 \\
5          &              &             &           & 0.2937 \\ 
           & cross-attn   &             &           & 0.4173 \\
           &              & \ding{56}   &           & 0.2981 \\
           &              &             & \ding{56}   & 0.3059 \\

\bottomrule

  \end{tabular}}}
  \caption{Comparing our full MFM pipeline (the first row) with various ablated cases on the shape completion task. We use the hyperparameters on the first line as default hyperparameters for blanks. MMD is multiplied by $1\times 10^{2}$.}
  \label{tab:abla_MFM}
\end{table}

\begin{figure}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/MFMpipeline_abla.png}
   \caption{Two MFM communication frameworks in ablation cases (single-fusion and single-attention).}
   \label{Fig: MFM_abla}
\end{figure}


%-----------------------------------------------------------------------------------
%-----------------------------------------------------------------------------------


\section{Experiment Detail} 
\label{sec:expe}

\subsection{Details for Unconditional Generation} 
\paragraph{Evaluation metrics details.}
Although the outputs of our model are T-SDFs, we choose to evaluate them by sampling point clouds on their surfaces. The community has proposed different metrics to quantitatively evaluate the generation performance of point cloud generative models, but some of them suffer from certain drawbacks. Given a generated set of point clouds $S_g$ and a reference set $S_r$, the most popular metrics are (following Lion~\cite{zeng2022lion}):

\begin{itemize}
    \item \textbf{Coverage(COV)}:
\begin{equation}
COV(S_g, S_r)=\frac{\left | \left \{ \underset{Y\in S_r}{\text{arg min}}D(X,Y)|X\in S_g \right \}  \right | }{\left | S_r \right | } ,
\end{equation}
where $D(\cdot ,\cdot)$ is distance measurement (\ie Chamfer distance or earth mover distance). COV measures the number of reference point clouds that are matched to at least one generated shape. COV can quantify diversity, but low-quality and diverse generated point clouds can achieve high coverage scores.
\end{itemize}


\begin{itemize}
    \item \textbf{Minimum matching distance(MMD)}:
\begin{equation}
MMD(S_g, S_r)=\frac{1}{|S_r|}\sum_{Y\in S_r}\underset{X\in S_g}{\text{min}}D(X,Y). 
\end{equation}
MMD calculates the average distance between the point clouds in the reference set and their closest neighbors in the generated set. However, MMD is not sensitive to low-quality point clouds which will never be matched during calculation. Moreover, MMD varies depending on implementation and is not robust on large scale unconditional shape generation.
\end{itemize}


\begin{itemize}
    \item \textbf{1-nearest neighbor accuracy (1-NNA)}: To overcome the drawbacks of COV and MMD, PointFlow~\cite{yang2019pointflow} proposes to use 1-NNA as a metric to evaluate point cloud generative models:


\begin{equation}
\begin{split}
&1\text{-}NNA(S_g, S_r)=\\
&~~~~~~~~~~~~\frac{\sum_{X\in S_g}\mathbb{I}\left [ N_X\in S_g \right ] + \sum_{Y\in S_r}\mathbb{I}\left [ N_Y\in S_r \right ]}{|S_r|+|S_g|},
\end{split}
\label{1nn}
\end{equation}
where $\mathbb{I}[\cdot ] $ is the indicator function and $N_X$ is the nearest neighbor of X in the set $S_r\cup  S_g- \left \{  X  \right  \}$. Hence, 1-NNA represents the leave-one-out accuracy of the 1-NN classifier defined in \cref{1nn}. More specifically, this 1-NN classifier classifies each sample as belonging to either $S_r$ or $S_g$ based on its nearest neighbor sample within $N_X$. As a consequence, 1-NNA directly quantifies distribution similarity between $S_r$ and $S_g$ and measures both quality and diversity.
\end{itemize}

In conclusion, we are following PointFlow~\cite{yang2019pointflow}, PVD~\cite{zhou2021pvd} and use 1-NNA as our main evaluation metric to quantify shape generation performance. with both CD and EMD distances.

\paragraph{More quantitative results.}
More quantitative results including COV and MMD are reported in~\cref{tab:full_uncond} for an additional comparison, although they may be unreliable metrics for unconditional generation quality. 3DQD outperforms all baselines in experiment for the more reasonable 1-NNA metric.

\paragraph{About normalization method.}
Almost all of the baselines in~\cref{tab:full_uncond} preprocess meshes into point clouds following PointFlow, while we preprocess them into T-SDF following DISN~\cite{xu2019disn}, which causes \textbf{different scales of shapes} in two kinds of datasets. Moreover, the most popular method (PointFlow~\cite{yang2019pointflow}, PVD~\cite{zhou2021pvd}, Lion~\cite{zeng2022lion}, \etc) count the mean and variance of the shapes in the training set, and apply \textbf{dataset-wise normalization} to keep the same scales with the training set during evaluation. For a fair comparison on MMD, dataset-wise normalization is also performed on our generated shapes and our test sets to keep the same scales with the training set of PointFlow~\cite{yang2019pointflow} and PVD~\cite{zhou2021pvd}. There is still a gap on MMD to some extent, which mainly comes from the loss caused by multiple sampling during data preprocessing and evaluation. So we strongly recommend referring to relative distance metrics, such as 1-NNA metric.

\paragraph{Improvement on airplane.}
The 20\% performance improvement mainly comes from a large number of special shapes in \textit{airplane} category, such as flat fighters and planes with complex propellers. Our 3DQD generates shapes with exact and clean details in \cref{air_rebut}, because of sufficient and accurate local shape tokens stored in codebook, while previous works are highly possible to produce failure cases with low-quality shape details.


\input{cvpr2023-author_kit-v1_1-1/table/full_uncond}

%-------------------------------------------------------------------------
\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/airplane_rebut.pdf}
   
   \caption{We sample shapes with 3DQD model, then we select samples generated by baselines with minimum CD from ours for a fair comparison. Our method outperforms baselines significantly in special shape cases in \textit{Airplane} due to exact shape tokens.}
   \label{air_rebut}
\end{figure}


%-------------------------------------------------------------------------

%-----------------------------------------------------------------------------------
\subsection{Details for Shape Completion} 

\paragraph{Evaluation setting details.}
For PoinTr~\cite{yu2021pointr} and SeedFormer~\cite{zhou2022seedformer}, we use the pre-trained model released on ShapeNet-55 benchmark, which fully covers the 13 categories we used to evaluate. Following PoinTr~\cite{yu2021pointr}, for each object, we randomly sample 8,192 points from the surface to obtain the ground truth point cloud. During evaluation, the points clouds in partial regions (\ie, half and octant) are preserved, while others are discarded. Then the amount of points preserved is repeated or deleted to 4096 (moderate difficulty degree in their benchmark~\cite{yu2021pointr}) as inputs. The models' outputs are 8192 complete points. As for AutoSDF~\cite{mittal2022autosdf} and 3DQD, the partial T-SDFs regions are inputs, and 8192 points are sampled on surfaces of generated shapes. Before computing metrics, all point clouds are normalized with method in SeedFormer~\cite{zhou2022seedformer}.

\paragraph{Evaluation metrics details.}
\textit{Unidirectional Hausdorff Distance} (UHD) is used to measure the completion fidelity to
the input partial input. MPC~\cite{wu2020mpc} and AutoSDF~\cite{mittal2022autosdf} calculate the average Hausdorff distance from the input
partial shape sets $S _p$ to each of the $k$ completion results in $S_g$:
\begin{equation}
UHD(S _p, S_g)=\frac{1}{|S _p|} \sum _{Z\in S_p, X\in S_g}\left ( \frac{1}{k}\sum_{i=1}^k D^{HD}(X^{i}, Z) \right ).
\label{UHD}
\end{equation}
However, UHD only measures the distortion of generated shapes from partial inputs, rather than the quality of completion with ground truths. Low-quality shapes with inputs unchanged will achieve good scores. As a result, we use MMD and Average Matching Distance (AMD) to evaluate completion quality. MMD and AMD for diverse shape completion can be derived as:
\begin{equation}
\begin{split}
& MMD(S_g, S_r)=\frac{1}{|S_r|}\sum_{Y\in S_r, X\in S_g} \left \{ \underset{X_i\in X}{\text{min}}D(X^i,Y)\right \} .\\ 
& AMD(S_g, S_r)=\frac{1}{|S_r|}\sum_{Y\in S_r, X\in S_g}\left \{ \frac{1}{|X|}\sum_{i=0}^{|X|}D(X^i,Y)\right \} . 
\end{split}
\label{MMDAMD}
\end{equation}

%-----------------------------------------------------------------------------------
\subsection{Details for Language-guided Generation} 
Training and inference details are reported in \cref{sec:condition}. As for metrics, \textit{Pairwise Minimum Matching Distance} (PMMD) has the same expression as MMD in~\cref{MMDAMD}. \textit{Frechet Pointcloud Distance} (FPD) is first proposed in TreeGAN to calculate the distance between features of point clouds with a pre-trained PointNet backbone. We select the shapes with minimum PMMD from ground truths in each $k$ generation to build a group of samples, and then measure FPD between the generated data distribution and ground truth distribution on test set, to check if the model can simulate the real shape distribution. Before computing metrics, all point clouds are normalized with method in SeedFormer~\cite{zhou2022seedformer}.

%-----------------------------------------------------------------------------------
\subsection{Details Denoising Conditional Generation} 
The pre-trained single-class 3DQD model trained on \textit{chair} category of ShapeNet data~\cite{chang2015shapenet} is evaluated about its denoising capability in this section to validate the prior on extended applications. We first sample 500 pure T-SDFs inputs $X \in \mathbb{R }^{64\times 64\times 64}$ from the dataset. Then different types of noise $\epsilon \in \mathbb{R }^{64\times 64\times 64}$ (\ie standard Gaussian distribution and uniform distribution) with same spatial resolutions scaled by noise level $\alpha $ are added to pure inputs:
\begin{equation}
X^{noisy} = X + \alpha \epsilon .
\end{equation}
Afterward, the noisy input $X^{noisy}$ is encoded by P-VQ-VAE and set as shape token initialization $\tilde{s}_k$ following the same procedure in \textit{Shape initialization as conditions} part. $k=0.5\times T=50$. At last reverse process starts from $\tilde{s}_k$ to recover the clean samples and remove the noise $\epsilon $.

Quantitative results with different noise levels and types are reported in \cref{tab:denoise}, and visual results are in \cref{sec:visual}.

\input{cvpr2023-author_kit-v1_1-1/table/denoise_tab}
%-----------------------------------------------------------------------------------
\subsection{Details for Single-view Reconstruction} 

\paragraph{Dataset details.}
We evaluate our proposed method on the real-world benchmark Pix3D~\cite{sun2018pix3d}, using cropped and segmented images as inputs for reconstruction (\ie, the background is removed). Since official train/test splits are only provided for the chair category, we test single-view reconstruction on chair category.

\paragraph{Architecture details.}
AutoSDF~\cite{mittal2022autosdf} has released a VQ-VAE to encode images to 512 tokens $c_q$, which is aligned with shape tokens. Based on it, we finish single-view reconstruction with pre-trained 3DQD model trained on language-guided generation task as the backbone, for it has cross-attention modules to fuse cross-modality information. Almost all parts of pre-trained model are preserved, except the conditional embedding modules. 

\paragraph{Training and inference details.}
During reconstruction, we use two ways of condition injection (\ie, shape token initialization and  cross-attention). Namely, we first encode each image into 512 token indexes $s_0$ with VQ-VAE. Then the token indexes $s_0$ after $k$ timestep corruption are initialized as the start of reverse denoising chain $\tilde{s}_k$, where $k=0.5\times T=50$. Afterward, the image indexes $s_0$ are also injected into 3DQD with new learnable embedding layers and cross-attention modules, which is the same as text-driven shape generation. Note that this image-condition 3DQD model is fine-tuned on Pix3D~\cite{sun2018pix3d} with masked and cropped image-shape pairs for 10 hours, to train the new image embedding and refine the whole model well.


%-----------------------------------------------------------------------------------
%-----------------------------------------------------------------------------------
\section{Additional Experimental Results} 
\label{sec:visual}
More visual results are presented in this section, including unconditional shape generation in~\cref{Fig: supp_uncond}, shape completion comparison in~\cref{Fig: supp_half} and~\cref{Fig: supp_octant}, denoising results in~\cref{Fig: supp_denoise}, single-view reconstruction in~\cref{Fig: supp_img}.

\begin{figure*}[]
  \centering
   \begin{center}
   \includegraphics[width=0.9\linewidth]{cvpr2023-author_kit-v1_1-1/img/supp_uncond.png}
   \end{center}
   \caption{Visual comparison about unconditional shape generation. We randomly sample shapes with our 3DQD model, then we select samples generated by baselines with minimum Chamfer distances from ours, to present a fair comparison.}
   \label{Fig: supp_uncond}
\end{figure*}

\begin{figure*}[]
  \centering
   \includegraphics[width=0.9\linewidth]{cvpr2023-author_kit-v1_1-1/img/supp_completion2.png}
    \vspace{-10mm}
   \caption{Visual comparison about shape completion given \textit{half} shapes.}
   \label{Fig: supp_half}
\end{figure*}


\begin{figure*}[]
  \centering
   \includegraphics[width=0.9\linewidth]{cvpr2023-author_kit-v1_1-1/img/supp_completion_octant2.png}
    \vspace{-2mm}
   \caption{Visual comparison about shape completion given \textit{octant} shapes.}
   \label{Fig: supp_octant}
\end{figure*}

\begin{figure*}[]
  \centering
   \includegraphics[width=0.9\linewidth]{cvpr2023-author_kit-v1_1-1/img/supp_denoise.png}
    \vspace{-7mm}
   \caption{Visual results about denoising with various noise types and levels. \textit{Upper}: Gaussian noise. \textit{Lower}: Uniform noise.}
   \label{Fig: supp_denoise}
\end{figure*}

\begin{figure*}[]
  \centering
   \includegraphics[width=0.8\linewidth]{cvpr2023-author_kit-v1_1-1/img/supp_img_3d2.png}
    \vspace{-7mm}
   \caption{Visual results about single-view reconstruction.}
   \label{Fig: supp_img}
\end{figure*}



%%%%%%%%% REFERENCES
\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
