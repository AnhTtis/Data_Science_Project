% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.



% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{4940} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process}
\author{Yuhan Li\textsuperscript{1}\hspace{2mm}
Yishun Dou\textsuperscript{2}\hspace{2mm}
Xuanhong Chen\textsuperscript{1}\hspace{2mm}
Bingbing Ni\textsuperscript{1, 2$\dagger$}\hspace{2mm}
Yilin Sun\textsuperscript{1}\hspace{2mm}
Yutian Liu\textsuperscript{1}\hspace{2mm}
Fuzhen Wang\textsuperscript{1}\\
\textsuperscript{1}Shanghai Jiao Tong University, Shanghai 200240, China \qquad \textsuperscript{2}Huawei Hisilicon \\
{\tt\small \{melodious, nibingbing\}@sjtu.edu.cn}\\
{\small \url{https://github.com/colorful-liyu/3DQD}}
}

\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}
 \vspace{-3mm}
We develop a generalized 3D shape generation prior model, tailored for multiple 3D tasks including unconditional shape generation, point cloud completion, and cross-modality shape generation, \etc. On one hand, to precisely capture local fine detailed shape information, a vector quantized variational autoencoder (VQ-VAE) is utilized to index local geometry from a compactly learned codebook based on a broad set of task training data. On the other hand, a discrete diffusion generator is introduced to model the inherent structural dependencies among different tokens. In the meantime, a multi-frequency fusion module (MFM) is developed to suppress high-frequency shape feature fluctuations, guided by multi-frequency contextual information.
The above designs jointly equip our proposed 3D shape prior model with high-fidelity, diverse features as well as the capability of cross-modality alignment, and extensive experiments have demonstrated superior performances on various 3D shape generation tasks.
\end{abstract}

\newcommand{\customfootnotetext}[2]{{% Group to localize change to footnote
  \renewcommand{\thefootnote}{#1}% Update footnote counter representation
  \footnotetext[0]{#2}}}% Print footnote text
\customfootnotetext{${\dagger}$}{Corresponding author: Bingbing Ni.}

%%%%%%%%% BODY TEXT
 \vspace{-5mm}
\section{Introduction}
 \vspace{-2mm}
%%background
While pre-trained 2D prior models~\cite{karras2019style, rombach2021ldm} have shown great power in various downstream vision tasks such as image classification, editing and cross-modality generation, \etc., their counterpart 3D prior models  which are generally beneficial for three-dimensional shape generation tasks have NOT been well developed, unfortunately.
On the contrary, the graphics community has developed a number of task-specific pre-trained models, tailored for unary tasks such as 3D shape generation~\cite{yang2019pointflow, luo2021dpm, hui2022neural}, points cloud completion~\cite{zhou2021pvd, yu2021pointr, zhou2022seedformer} and conditional shape prediction~\cite{fu2022shapecrafter, liu2022towards, mittal2022autosdf, wang2018pixel2mesh}.  Since above individual 3D generative representations do NOT share common knowledge among tasks, to migrate a trained 3D shape network from one task to another related one requires troublesome end-to-end model re-work and training resources are also wasted.
For instance, a good shape encoding of ``chairs" based on a general prior 3D model could benefit shape completion of a given partial chair and text-guided novel chair generation.

\begin{figure}[t]
 \vspace{-5mm}
  \centering
   \includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/moti3.png}
   \caption{Our shape generation model is a unified and efficient prior model to produce high-fidelity, diverse results on multiple tasks, while most previous approaches are task-specific.}
   \label{motivation}
    \vspace{-5mm}
\end{figure}

%% chanlenge
This work aims at designing a unified 3D shape generative prior model, which serves as a generalized backbone for multiple downstream tasks, \ie, with few requirements for painstaking adaptation of the prior model itself.
Such a general 3D prior model should possess the following \emph{good} properties. On one hand, it should be \emph{expressive} enough to generate \textbf{high-fidelity} shape generation results with fine-grained local details. On the other hand, this general prior should cover a large probabilistic support region so that it could sample \textbf{diverse} shape prototypes in both conditional and unconditional generation tasks. 
Moreover, to well support \textbf{cross-modal} generation, \eg, text-to-shape, the sampled representation from the prior model should achieve good semantic consistency between different modalities, making it easy to encode and match partial shapes, images and text prompts.

%%problem
The above criteria, however, are rarely satisfied by existing 3D shape modeling approaches. Encoder-decoder based structures~\cite{yu2021pointr, zhou2022seedformer} usually focus on dedicated tasks and fail to capture diverse shape samples because the generation process mostly relies on features sampled from deterministic encoders. On the contrary, probabilistic models such as GAN~\cite{shu2019treegan, li2021sp}, Flow~\cite{yang2019pointflow} and diffusion models~\cite{luo2021dpm, hui2022neural, zhou2021pvd}, cannot either adapt to multiple tasks flexibly or generate high-quality shapes without artifacts~\cite{zeng2022lion}.
Note that recent models such as AutoSDF~\cite{mittal2022autosdf} and Lion~\cite{zeng2022lion} explore multiple conditional generative frameworks. However, AutoSDF~\cite{mittal2022autosdf} is defective in diversity and shows mode collapse in unconditional and text-guided generation, while training and inference of Lion~\cite{zeng2022lion} are costly.


\begin{figure*}[t]
 \vspace{-5mm}
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/framework4.png}
   \caption{Overall framework of our method. VQ-VAE encodes shapes into geometry tokens with compact representation. Then diffusion generator models the joint distribution of tokens by reversing a forward diffusion process that corrupts the inputs via a Markov chain.}
   \label{framework}
    \vspace{-5mm}
\end{figure*}

%% My innovation
In view of above limitations, this work proposes an efficient \textbf{\emph{3D-Disassemble-Quantization-and-Diffusion (3DQD)}} prior model to simultaneously address above challenges in 3D shape generation: high-fidelity, diversity, and good cross-modality alignment ability, as shown in \cref{motivation}.
This 3DQD prior is a unified, probabilistic and powerful backbone for both unconditional shape generation and multiple conditional shape completion applications.
On one hand, instead of using holistic codes for whole-shape, we adopt a vector quantized variational autoencoder~\cite{van2017vqvae} (VQ-VAE) that learns a compact representation for \emph{disassembled} local parts of the shape, which is expected to well represent diverse types of local geometry information out of a broad set of training shapes from shared tasks, forming a generalized part-level codebook.
Note that this disassembled and quantized representation effectively eliminates the intrinsic structural bias between different modalities, and thus is able to perform good cross-modality data alignment on conditional generation tasks.
On the other hand, a discrete diffusion generator~\cite{austin2021structured, gu2022vector} with reverse Markov chain is introduced to model the inherent semantic dependencies among geometric tokens. Namely, the forward process corrupts the latent variables with progressively increasing noise, transferring parts of variables into random or masked ones; and the reverse process gradually recovers the variables towards the desired data distribution by learning the structural connections among geometry tokens.
It is worth mentioning that random corruption in the forward process facilitates diverse samples, while discrete embedding results in a dramatic cost-down in computational budget, with stable and iterative sampling. 
%It is worth noting that the above discrete diffusion generator gradually [estimates the probability density of geometry tokens step-by-step, where the random corruption leads to diverse results.
Furthermore, during shape generation we introduce \textbf{Multi-frequency Fusion Module} (MFM) to suppress high-frequency outliers, encouraging smoother and high-fidelity samples.


%%experiment
Comprehensive and various downstream experiments on shape generation tasks demonstrate that the proposed 3DQD prior is capable of helping synthesize high-fidelity, diverse shapes efficiently, outperforming multiple state-of-the-art methods. Furthermore, our 3DQD prior model can serve as a generalized backbone with highly competitive samples for extended applications and cross-domain tasks requiring NO or little tuning.


 \vspace{-2mm}
\section{Related Works}
 \vspace{-2mm}
\noindent
\textbf{Diffusion models.}
Recently, the denoising diffusion probabilistic model (DDPM), has attracted great attention in the community~\cite{ho2020denoising, nichol2021glide, rombach2021ldm}, which utilizes a Markov chain to convert the noise distribution to the data distribution. Models under this family have achieved strong results on image generation~\cite{ho2020denoising, nichol2021improved,  ho2022cascaded, dhariwal2021diffusion, ashual2022knn, saharia2022palette} and multiple conditional generation tasks~\cite{saharia2022image, rombach2021ldm, santos2022face, choi2021ilvr, meng2021sdedit, preechakul2021diffusion, chen2020wavegrad}. Meanwhile, other researchers investigated the discrete diffusion models. Argmax Flow~\cite{hoogeboom2021argmax} first introduces a diffusion model directly defined on discrete categorical variables, while D3PM~\cite{austin2021structured} generalizes it by going beyond corruption processes with uniform transition probabilities. VQ-Diffusion~\cite{gu2022vector, tang2022improved} encodes images into discrete feature maps to train the diffusion, presenting comparable results in text-to-image synthesis with the state-of-the-art models~\cite{nichol2021glide, ramesh2022hierarchical}. 


\noindent
\textbf{3D Shape generation.} 
Traditional shape generation methods~\cite{yang2018foldingnet, groueix2018papier, li2021sp} mitigate generation process by mapping a primitive matrix to a point cloud with transformation. Some recent works~\cite{yang2019pointflow, cai2020learning, luo2021dpm, zhou2021pvd} consider point clouds as samples from a distribution by different probabilistic models. Almost all of them cannot cope with conditional input and multi-modality data. Some conditional works~\cite{chen2019unpaired, tchapmi2019topnet, yuan2018pcn, zhang2021unsupervised, yu2021pointr, zhou2022seedformer} focus on completing full shapes from partial inputs on point clouds with task-specific architectures to encode inputs and decode the global features. Others explore single-view reconstruction~\cite{mandikal20183d, wu2020pq, wang2018pixel2mesh, wang20193dn, wu2018learning, shi2021geometric} and text-driven generation~\cite{chen2018text2shape, mittal2022autosdf, liu2022towards, fu2022shapecrafter} to learn a joint condition-shape distribution with deterministic process. However, most conditional generation methods fail in capturing the multiple output modes~\cite{mittal2022autosdf} and adapting to various tasks.

Most related to our work, PVD~\cite{zhou2021pvd} and Lion~\cite{zeng2022lion} both use a diffusion model for diverse generation. Their models work on latent embedding space, leading to either difficulty in multi-modality applications (\ie, denosing)~\cite{zeng2022lion} or time-consuming training and inference. Inspired by discrete approaches~\cite{esser2021taming}, we introduce VQ-VAE~\cite{van2017vqvae} to learn a compact representation, followed by a discrete diffusion generator, to reduce computational overhead and align cross-domain data. Our model is a superior and efficient prior model for shape generation compared with PVD~\cite{zhou2021pvd} and Lion~\cite{zeng2022lion}, and shows more diversity than AutoSDF~\cite{mittal2022autosdf}.
 


 \vspace{-3mm}
\section{Methodology}
 \vspace{-2mm}
 
In this section, we introduce in detail of our proposed general 3D shape prior model (3DQD) for various 3D generation tasks. The architecture is visualized in \cref{framework}.
To begin with, we present our shape encoding scheme based on part-level discretization using VQ-VAE~\cite{van2017vqvae} style methods, for its advantages in representation compactness and consistency among different tasks.
Then, a novel diffusion process based prior is developed according to this discrete part based shape representation with expressive and diversified object structural modeling, forming a good basis for 3D generation downstream applications.
Moreover, a novel multi-frequency fusion module is introduced for enhancing fine-grained 3D surface modeling.

\subsection{Discrete Part-based Geometry Encoding}
 \vspace{-2mm}


A proper representation (\ie, shape encoding) is essential for effective shape distribution model design. Previous coding schemes~\cite{zhou2021pvd, luo2021dpm} based on continuous embedding representation learning suffer from drawbacks of large computational consumption and heavy task dependence, rendering difficulty in 3D conditional shape generation, especially for multi-task settings. While most encoder-based methods (\ie Lion~\cite{zeng2022lion}) choose to embed shapes into continuous space via VAE~\cite{kingma2013vae}, in this work, we propose to circumvent this drawback by introducing Patch-level VQ-VAE (P-VQ-VAE)~\cite{van2017vqvae, mittal2022autosdf} on Truncated-Signed Distance Field~\cite{xu2019disn, jiang2020sdfdiff} (T-SDF) to learn a compact and efficient representation. The motivation is obvious. All 3D objects are composed of geometrically similar local components, and thus a good local atomic representation can be shared among different objects and diverse tasks.% As a consequence
Besides, using discrete codebook to index each local part results in a dramatic cost-down in searching discrete feature space. As it's flexible to encode inter-dependencies between parts, this encoding scheme provides a compact way for expressing highly diverse intrinsic structural variations. Moreover, local atomic indexing potentially offers unified and well-aligned local representation among different modalities, \eg images and text prompts, which greatly facilitate cross-domain conditional shape generation.



Formally, the proposed P-VQ-VAE consists of an encoder $E$, a decoder $D$ and codebook entries $\mathcal{Z}  \in \mathbb{R }^{K\times n_z} $, which contains a finite number of embedding vectors, where $K$ is the size of codebook and $n_z$ is the dimension of vectors. Following AutoSDF~\cite{mittal2022autosdf}, given a T-SDF input $X \in \mathbb{R }^{H\times W\times D}$, the whole 3D shape is divided into partial regions $X'=\left \{ x'^{0}, \dots, x'^{N-1} \right \}$, where $x'^i\in \mathbb{R }^{h\times w\times d}$ and $N$ is the number of regions.  Afterwards, each part is vectorized by the encoder as $z^{i} = E(x'^{i}) \in \mathbb{R }^{n_z}$. Superscript $i$ is its spatial location.
Finally, we obtain a spatial collection of quantized shape tokens $Z_q=\left \{ z_{q}^0, \dots, z_{q}^{N-1} \right \}$ by a further quantization step as $z_{q}^i = VQ(z^{i}) \in \mathbb{R }^{n_z}$, and their corresponding codebook indexes $S=\left \{ s^{0}, \dots, s^{N-1} \right \}$, where $s^i\in \left \{ 0, \dots, K-1 \right \}$. $VQ$ is a spatial-wise quantizer which maps partial shape $z^i$ into its closest codebook entry in $\mathcal{Z} $.


\subsection{3D Shape Prior: Discrete Diffusion Model}
 \vspace{-2mm}
 
Given the above P-VQ-VAE encoding (\emph{i.e.}, discrete part geometry tokens $Z_q\in \mathbb{R }^{N\times n_z}$ and corresponding indexes $S=\left \{ s^0, \dots, s^{N-1} \right \}$), to build the 3D shape prior is equivalent to modeling the \textbf{joint probability distribution} of all local shape codes in the latent space. In essence, a sampling from this prior distribution reveals a certain intrinsic structure of a 3D shape, by considering the inter-relationship geometric organization among local shape codes, \emph{i.e.}, how to spatially combine local parts into various 3D objects.
In this sense, a good prior model, should not only provide a probabilistic support region as wide as possible (\ie possessing sufficient shape diversity), but also be general enough to deal with different downstream 3D shape tasks. For example, a chair with 4 legs can be transformed into a chair with wheels easily by the knowledge learned in generation, without training on target editing tasks.


The joint distribution of partial components and conditions are usually learned with autoregressive models in previous works, \eg, DALL-E~\cite{ramesh2021dalle}, Taming~\cite{esser2021taming}, ShapeFormer~\cite{yan2022shapeformer} and AutoSDF~\cite{mittal2022autosdf}, along with sequential non-Markov-chain styled generation process: $ {\textstyle \prod_{i=1}^{N}p_{\theta }(s^i|s^1, \dots ,s^{i-1})} $. However, this scheme has several drawbacks. 1) \textbf{Error Accumulation}: Geometry tokens are predicted one by one, therefore errors induced in the earlier sampling timesteps will never be corrected and contaminate the subsequent generation steps; 2) \textbf{Un-optimal Generation Order}: Most autoregressive models perform an unidirectional prediction process, \eg left-to-right, bottom-to-top, front-to-back or random orders, which obviously ignores the underlying complex 3D structure; and 3) \textbf{Lack of Diversity}: Deterministic transformers are usually instantiated as backbones of autoregressive models~\cite{yan2022shapeformer, mittal2022autosdf}, and without sufficient randomness injection they easily lean towards mode collapse (\eg, highly similar completion results given a partial shape), especially in condition-driven tasks.

In view of these limitations, we develop a discrete diffusion generator~\cite{tang2022improved, gu2022vector} to iteratively sample in the time domain with all local part codes updated simultaneously at each timestep (\ie , instead of sampling each spatial location one by one). In this way, diffusion generator is able to get rid of fixed unidirectional generation and update all partial geometry with long-range dependencies simultaneously, which enhances structural expressivity of the learned shape distribution.
Within this framework, earlier samples are rechecked multiple times during iterations, reducing the likelihood of being confused. In addition, random corruption on shapes in diffusing process also leads to a great diversity of generated results.
The forward sampling process for shape generation and the backward sampling process for training the proposed discrete diffusion model are introduced in detail as follows.


%-----------------------------------------------------------------------------------
 \vspace{-2mm}
\subsubsection{Forward Diffusing Process and Corruption.}
 \vspace{-2mm}
 
The forward diffusion process gradually adds noise to tokenized 3D geometric input $S_0=\left \{ s_0^{0}, \dots, s_0^{N-1} \right \}$ (superscripts are locations and subscripts are timesteps) via a Markov chain $q(s^i_{1:T}|s^i_0)={\textstyle \prod_{t=1}^{T}q(s^i_t|s^i_{t-1})} $, where each token is randomly replaced into noisy index $s^i_{1:T}=s^i_1,s^i_2,\dots ,s^i_T$. Without introducing confusion, we omit superscripts $i$ in the following description.  After $T$ timesteps, each token in the entire map will be completely corrupted into $s_T$, \ie, a non-sense index. The learned reverse process $p_{\theta }(s_{0:T})=p(s_T) {\textstyle \prod_{t=1}^{T}p_{\theta}(s_{t-1}|s_t)} $ gradually removes the added noise on the random variables $s_t$, namely to generate a 3D shape from random noise.

While continuous diffusion process models~\cite{ho2020denoising, zhou2021pvd, zeng2022lion} employ Gaussian noise in the forward process; in discrete diffusion process, we use transition matrices $\left [ Q_t \right ] _{i,j}=q(s_t=j|s_{t-1}=i)$ characterized by uniform transition probabilities to describe the corruption from $s_{t-1}$ to $s_{t}$. As a result, all local part codes can be transformed to any other shape codes with the same transition probability, producing complex 3D structure representations. With transition matrix $\left [ Q_t \right ]$ and one-hot encoding of $s_t$, we define the forward Markov chain as:
\begin{small}
\vspace{-2mm}
\begin{equation}
q(s_t|s_{t-1}) = \mathbf{\Psi} (s_t;p=s_{t-1}Q_t),
\label{q(st|st-1)}
\end{equation}
\end{small}
where $\mathbf{\Psi} (s_t;p)$ is a categorical distribution over the one-hot row vector $s_t$ sampled with probability $p$, and $s_{t-1}Q_t$ is computed by a row vector-matrix product. 
Accordingly, we derive the posterior by Markov chain iteratively from $s_0$ as:

\begin{small}
\vspace{-5mm}
\begin{equation}
q(s_t|s_0) = \mathbf{\Psi} (s_t;p=s_{0}\overline{Q_t})\text{,~~with }\overline{Q_t}=Q_1\cdots Q_t,
  \label{q(st|s0)}
\end{equation}
\begin{equation}
\begin{split}
q(s_{t-1}|s_t, s_0) &= \frac{q(s_t|s_{t-1}, s_0)q(s_{t-1}|s_0)}{q(s_t|s_0)} \\
&= \mathbf{\Psi} \left (  s_{t-1};p=\frac{s_tQ_t^\top \odot s_0\overline{Q}_{t-1} }{s_0\overline{Q}_t s_t^\top }\right ).
\end{split}
  \label{posterior}
\end{equation}
\end{small}

The transition matrix $Q_t$ reflects how to spatially re-organize 3D local primitives and it determines the degree of freedom in deformation. In order to assist the network to locate what needs to be fixed quickly, we introduce an additional special token, [\emph{MASK}] token~\cite{gu2022vector}. So the codebook entries are composed of $K+1$ states: $K$ geometry tokens and one [\emph{MASK}] token. $Q_t\in \mathbb{R}^{(K+1) \times (K+1)}$ can be formulated as:

\begin{small}
\vspace{-5mm}
\begin{equation}
  \left [ Q_t \right ] = \
  \begin{bmatrix}\alpha _t-\frac{K-1}{K}\beta _t & \frac{\beta _t}{K} & \frac{\beta _t}{K} & \cdots & \gamma_t
 \\\frac{\beta _t}{K} & \alpha _t-\frac{K-1}{K}\beta _t & \frac{\beta _t}{K} & \cdots & \gamma_t
 \\\frac{\beta _t}{K} & \frac{\beta _t}{K} & \alpha _t-\frac{K-1}{K}\beta _t & \cdots & \gamma_t
 \\\vdots  & \vdots  & \vdots  & \ddots  & \vdots 
 \\0 & 0 & 0 & \cdots & 1
\end{bmatrix},
\label{qt_ij}
\end{equation}
\end{small}
with $\alpha _t = 1 - \gamma _t$, which means each token has a probability of $1-\gamma _t-\frac{K-1}{K}\beta _t$ to remain unchanged, while with a probability of $\frac{\beta _t}{K}$ being transited equally into other $K-1$ geometry categories and $\gamma _t$ into [\emph{MASK}] state. 



%-----------------------------------------------------------------------------------

\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/MFM.pdf}
   \caption{Information from detailed shapes and condensed features are intra- and inter-communicated in MFM to remove high-frequency components from uniform noise, where timestep $t$ and classifier-free guidance are modulated with AdaLayerNorm.}
   \label{MFM}
    \vspace{-5mm}
\end{figure}

 \vspace{-2mm}
\subsubsection{Reverse Sampling Process and Model Learning}
 \vspace{-2mm}
 
To recover original 3D grids of tokens from $s_t$, we follow Ho \etal~\cite{ho2020denoising} and Hoogeboom \etal~\cite{hoogeboom2021argmax}, and train a denoising transformer $T_{\theta }$ to directly estimate the noise-removed shape distribution $p_{\theta }(\hat{s}_0 |s_t)$. Then the posterior transition distribution can be obtained with $q(s_{t-1}|s_t, s_0)$ as:

\begin{small}
\vspace{-2mm}
\begin{equation}
p_{\theta }(s_{t-1}|s_t) = \sum_{\hat{s}_0} q(s_{t-1}|s_t, \hat{s}_0)p_{\theta}(\hat{s}_0|s_t).
  \label{p(st-1|st)}
\end{equation}
\end{small}

The evidence lower bound (ELBO) associated with this model then decomposes over the discrete timesteps as:
\begin{small}
 \vspace{-1mm}
\begin{equation}
\begin{split}
-\log &p_{\theta }(\hat{s_0}) \le \mathbb{KL}(q(s_T|s_0)|p(s_T))\\
&+\sum_{t=1}^{T}\mathbb{E}_{q(s_t|s_0)}\mathbb{KL}(q(s_{t-1}|s_t, s_0)|p_{\theta}(s_{t-1}|s_t)).
\end{split}
  \label{ELBO}
  \vspace{-3mm}
\end{equation}
\end{small}

For the first term of ELBO has no relation with the learnable network ($p(s_T)$ is an initial noise distribution), the common way to minimize ELBO is to optimize the posterior $p_{\theta}(s_{t-1}|s_t)$ in terms of~\cref{p(st-1|st)}. Due to the direct prediction of neural network is original shapes $p_{\theta}(\hat{s}_0|s_t)$, we set an auxiliary training objective with the distance of recovered original shapes apart from the posterior, yielding the following loss function:
\begin{small}
 \vspace{-3mm}
\begin{equation}
L=-\mathbb{E} _{q(s_0)q(s_t|s_0)}\left [\underset{L_{main}}{\underbrace{\log p_{\theta}(s_{t-1}|s_t)}} +\underset{L_{aux}}{\lambda \underbrace{\log p_{\theta}(\hat{s}_0|s_t)}} \right ],
  \label{Loss}
  \vspace{-2mm}
\end{equation}
\end{small}
where $\lambda $ balances the weight of the auxiliary loss.

\noindent
\textbf{Classifier-free Guidance.} To trade off between precision and diversity, Ho \& Nichol \etal~\cite{ho2022classifier, nichol2021glide} propose classifier-free guidance, which does not require a separately trained classifier model. During training, the class label $y$ in diffusion model has a fixed probability (drop out rate $p$ ) to be replaced with the empty label $\emptyset$. During inference, the model prediction is thus adjusted in the direction of $p_{\theta}(\hat{s}_0|s_t, y)$ away from $p_{\theta}(\hat{s}_0|s_t, \emptyset)$ as:

\begin{small}
 \vspace{-4mm}
\begin{equation}
p_{\theta}(\hat{s}_0|s_t) = (1+w)\cdot p_{\theta}(\hat{s}_0|s_t, y)-w\cdot p_{\theta}(\hat{s}_0|s_t, \emptyset),
  \label{cf-guide}
\end{equation}
\end{small}
with $w\ge 0$ controlling the guidance scale.

In this work, we also employ classifier-free guidance on multi-category generation tasks ( \eg, shape completion), which is proved to enrich diversity of samples without apparently affecting the fidelity. We set drop out rate $p=0.5$ and guidance weight $w=0.5$.

 \vspace{-2mm}
\subsubsection{Multi-frequency Fusion Module}
\label{mainbodyMFM}
 \vspace{-2mm}
 
In the meantime, we observe noisy surfaces with outliers from generated shapes mainly introduced by \emph{categorical corruption} in discrete diffusion models. Namely, our \emph{categorical corruption} has equal probability to transit a token into any completely irrelevant shape tokens~\cite{austin2021structured}, and it is hard to recover the correct category of this shape token without looking into its contextual information, \eg, its adjacent tokens.
Note that in continuous diffusion models, the added Gaussian noise only brings up \emph{soft} transition which still preserves part of the original shape information. Therefore, our transition matrix $Q_t$ with uniform noise imports more high-frequency noise and outliers than Gaussian diffusion~\cite{ho2020denoising} and autoregressive model~\cite{van2017vqvae, esser2021taming}.

To remedy this issue, inspired by the observation that corrupted tokens' neighbors are still possible to remain unchanged and provide dependencies, we develop a Multi-frequency Fusion Module (MFM) which looks into local contextual regions and extracts low-frequency components by downsampling to suppress high-frequency outliers, encouraging smoother and high-fidelity samples.
As shown in~\cref{MFM}, we split the detailed shape token embedding $x^H$ explicitly with the condensed features $y^L$ from its neighbors, which is obtained by down-sampling within its local receptive field. Each component is further sent into intra- and inter-relationship part: $[x^{H\longrightarrow H\longrightarrow L} ]$ and $[y^{L\longrightarrow L\longrightarrow H} ]$, respectively, and communication of information is realized. Specifically, we employ self-attention for intra-frequency update, and Pair-wise Fusion Operator (PFO) for communication between two components. We examine two ways of communication ( \ie cross-attention~\cite{chen2021crossvit}, residual add~\cite{he2016deep}). Note that residual add 
\begin{small}
\vspace{-1mm}
\begin{equation}
x^H+f(x^H+\mathcal{P}(y^L)),
  \label{res_add}
\end{equation}
\vspace{-1mm}
\end{small}
is set as default Pair-wise Fusion Operator in experiment, where $f$ is fully connected layers, and $\mathcal{P}$ means pair-wise alignment. We employ 3 MFM layers cascaded at the end of the denoising transformer, as filters to suppress high-frequency outliers, as validated in experiment.

\begin{figure*}[t]
 \vspace{-3mm}
  \centering
   \includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/uncond_denoise3.png}
   \caption{Our approach synthesizes high-quality and diverse shapes with smooth surfaces and complex structures on multiple tasks. \textit{Left}: Unconditional shape generation. \textit{Right}: Shape denoising without fine-tuning.}
   \label{uncond_denoise}
    \vspace{-5mm}
\end{figure*}

\begin{figure*}[t]
\centering
\begin{minipage}[t]{0.65\linewidth}
\centering
\includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/completion_comp4.png}
\caption{Shape completion comparison. Our method yields most high-fidelity and diverse generation by the powerful and iterative modeling of joint distribution from diffusion, as well as generalized prior, even though only a small part is given in the last two rows.}
\label{completion_comp}
\end{minipage}
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/tsne.png}
\caption{We visualize the prior with T-SNE projections. Observe intra-class coherence and inter-class divergence.}
\label{tsne}
\end{minipage}
 \vspace{-5mm}
\end{figure*}


\section{Experiments}
\label{sec:Experiments}
 \vspace{-2mm}
In this section, we quantitatively and qualitatively evaluate our proposed unified 3DQD prior on three mainstream 3D shape generation tasks. We also discuss how 3DQD is extended for different relevant applications.

\subsection{3D Shape Generation}
\label{sec:uncond}
 \vspace{-2mm}
\noindent
\textbf{Data and evaluation.} We select three most popular categories from ShapeNet~\cite{chang2015shapenet}: \textit{Airplane}, \textit{Chair}, \textit{Car} as our main datasets for training. For input, we prepare the same T-SDF files as in DISN~\cite{xu2019disn} and follow the train-test split. Following previous works~\cite{zhou2021pvd, zeng2022lion}, we use 1-NNA as our main metric with both Chamfer distance (CD) and earth mover distance (EMD), measuring both shape generation quality and diversity. Limitations of other metrics are also discussed in our supplementary material.

\noindent
\textbf{Baselines and results.} We present the visual samples in~\cref{uncond_denoise} and quantitative  results in~\cref{tab:uncondition}. In comparison with baseline methods, we follow the same data processing and evaluation procedure as in PVD~\cite{zhou2021pvd}. Since our data is in the format of volumetric T-SDFs, for a fair comparison, we first transform T-SDFs into meshes, and then sample 2048 points for each mesh. It is noted that 3DQD outperforms all baselines with the best shape generation quality, due to the stable iterative generation and powerful joint distribution modeling capabilities of discrete diffusion generator, while MFM layers successfully suppress high-frequency outliers and improve the smoothness of results.

\begin{table}
  \centering
  \resizebox{\linewidth}{!}{
  \begin{tabular}{lcccccc}
  \toprule
               & \multicolumn{2}{c}{Airplane}    & \multicolumn{2}{c}{Chair}       & \multicolumn{2}{c}{Car}  \\  \cline{2-7} 
Method       & CD$\downarrow $             & EMD$\downarrow $            & CD$\downarrow $             & EMD$\downarrow $            & CD$\downarrow $             & EMD$\downarrow $     \\
\midrule
r-GAN~\cite{achlioptas2018learning}         & 99.84          & 96.79          & 83.69          & 99.70          & 94.46          & 99.01    \\
PointFlow~\cite{yang2019pointflow}     & 75.68          & 70.74          & 62.84          & 60.57          & 58.10          & 56.25    \\
SoftFlow~\cite{kim2020softflow}      & 76.05          & 65.80          & 59.21          & 60.05          & 64.77          & 60.09    \\
SetVAE~\cite{kim2021setvae}        & 76.54          & 67.65          & 55.84          & 60.57          & 59.94          & 59.94    \\
DPF-Net~\cite{klokov2020discrete}       & 75.18          & 65.55          & 62.00          & 58.53          & 62.35          & 54.48    \\
DPM~\cite{luo2021dpm}           & 76.42          & 86.91          & 60.05          & 74.77          & 68.89          & 79.97    \\
PVD~\cite{zhou2021pvd}          & 73.82          & 64.81          & 56.26          & 53.32          & \textbf{54.55} & 53.83   \\
\midrule
Ours  & \textbf{56.29} & \textbf{54.78} & \textbf{55.61} & \textbf{52.94} & 55.75         & \textbf{52.80}\\

\bottomrule

  \end{tabular}}
  \caption{Generation results on \textit{Airplane}, \textit{Chair}, \textit{Car} categories from ShapeNet~\cite{chang2015shapenet} using 1-NN$\downarrow $ as the metric.}
  \label{tab:uncondition}
   \vspace{-5mm}
\end{table}



\subsection{Shape Completion}
\label{sec:completion}
 \vspace{-2mm}
 
\noindent
\textbf{Data and evaluation.} We evaluate our method on the ShapeNet dataset with 13 categories using the train/test splits provided by DISN~\cite{xu2019disn}. As partial input is in the form of T-SDFs, we use the benchmark from AutoSDF~\cite{mittal2022autosdf}, which contains two different settings of observed shapes: 1) \textit{Bottom half} of ground truth as partial shape, and 2) \textit{Octant} with front, left and bottom half of ground truth as partial shape. For evaluation, we compute \textit{Total Mutual Difference} (TMD) of $N=10$ generated shapes for each input. We also report \textit{Minimum Matching Distance} (MMD) and \textit{Average Matching Distance} (AMD) which measure the minimum and average Chamfer distance from ground truth to the $N$ generated shapes.


\noindent
\textbf{Condition injection.} 
It's vital for us to explore how to infer missing shapes in terms of the partial observations (out of distribution) with a model trained with noisy complete shape tokens. To this end, we encode the inputs into codebook tokens and diffuse them at $k$ timestep into $\widetilde{s}_k$, to replace the full masked $s_T$ for iterative generation. So that the reverse process starts at $k$ timestep rather than $T$ timestep, with partial information kept in incompletely corrupted token maps. We set $k=\frac{1}{2}T$, as smaller $k$ value damages diversity in shape generation and larger $k$ reduces the fidelity of generation shapes.

 
 \begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/text_gen_color_comp2.png}
   \caption{Comparative results for language-guided generation. Our model captures the reasonable variations from text prompts with smooth surfaces, while others produce unsatisfying results.}
   \label{text_3d}
    \vspace{-5mm}
\end{figure}



\noindent
\textbf{Baselines and results.} 
We compare 3DQD with three state-of-the-art shapes completion methods, including PoinTr~\cite{yu2021pointr}, SeedFormer~\cite{zhou2022seedformer} and AutoSDF~\cite{mittal2022autosdf}. The former two methods perform accurate supervised completion, while the latter is able to sample diverse shapes from a partial observation. A set of random held-out shapes from ShapeNet dataset with 13 categories are used to compare the performance. The number of objects in the set is $M=500$. Quantitative results are presented in~\cref{tab:completion} and several visualized results are shown in~\cref{completion_comp}. 
It is observed that our method surpasses all models in both diversity and fidelity of generation, thanks to the under-determined, multi-modality nature of diffusing process. 
It is also worth mentioning that although our model is trained only on completely noisy shape tokens, it successfully performs inference conditioned on local noisy tokens with rest regions fully masked (which it has never seen). In contrast, models which are specifically trained only for 3D completion fail this task, as illustrated in the right column of~\cref{completion_comp}. It also indicates that our model has learned prior knowledge about the structural relations between local shapes, and this joint prior can be extended to more applications as discussed in~\cref{sec:Extensions}.


\noindent
\textbf{Prior Analysis.} We further visualize our learned prior distribution to provide an in-depth analysis about how the underlying shape structure distributions for various object types are well modeled by 3DQD so that they could easily facilitate various tasks. For this purpose, we sample $N=100$ shape completion results from only an octant input for each object category, and then perform feature encoding for each appended completion with a pre-trained PointNet~\cite{qi2017pointnet}. We use t-SNE~\cite{van2008visualizing} to project each feature representation into a point in~\cref{tsne}. From~\cref{tsne}, we see that 1) the visualized 3D representations completed from a same partial input are clustered, which shows that our learned prior model is able to capture common structural features for the same category; and 2) results from different inputs are obviously scattered, which demonstrates that the learned prior distribution is diverse enough (in other words, the coverage of the learned prior is sufficiently wide), so that it can differentiate different object categories. The above observations confirm that our learned prior is able to well model the underlying dependencies between local shape components, thus it serves as a general object structure probabilistic distribution, supporting a wide range of related tasks.


\begin{table}
 \vspace{-3mm}
  \centering
  \resizebox{\linewidth}{!}
  {
  \begin{tabular}{lcccccc}
  \toprule
               & \multicolumn{3}{c}{Bottom Half}    & \multicolumn{3}{c}{Octant}     \\  \cline{2-7} 
Method       & MMD$\downarrow $  & AMD$\downarrow $        & TMD$\uparrow $        & MMD$\downarrow $  & AMD$\downarrow $     & TMD$\uparrow $       \\
\midrule
PoinTr~\cite{yu2021pointr}     &  0.5316    & N/A    & N/A      &   2.1567       & N/A    & N/A    \\
SeedFormer~\cite{zhou2022seedformer}     & 0.4972  & N/A  & N/A    &2.3990     &N/A           & N/A        \\
AutoSDF~\cite{mittal2022autosdf}    & 0.3510   & 0.8200     & 0.0466  & 0.5720  & 1.279   & 0.0826      \\
\midrule
Ours  & \textbf{0.2933} & \textbf{0.6302} & \textbf{0.0478} & \textbf{0.4690}  & \textbf{1.093}  & \textbf{0.0960}\\

\bottomrule

  \end{tabular}}
  \caption{Quantitative completion results on ShapeNet. MMD and AMD is multiplied by $10^2$. TMD is multiplied by $10$.}
  \label{tab:completion}
   \vspace{-3mm}
\end{table}


\begin{table}
  \centering
  \resizebox{\linewidth}{!}
  {
  \begin{tabular}{lcccc}
  \toprule
Method       & PMMD$\downarrow $        & CLIP-S$\uparrow $        & FPD$\downarrow $     & TMD$\uparrow $       \\
\midrule
Shape-IMLE~\cite{liu2022towards}     & 1.681          & 31.42         & 82.34          & 0.0539        \\
AutoSDF~\cite{mittal2022autosdf}    & 1.961        & 31.65         & 141.87          & 0.1302        \\
\midrule
Ours  & \textbf{1.492} & \textbf{32.11} & \textbf{59.00} & \textbf{0.2795} \\

\bottomrule

  \end{tabular}}
  \caption{Quantitative results for text-guided generation. PMMD and CLIP-S is multiplied by $10^2$. TMD is multiplied by $10$.}
  \label{tab:text}
   \vspace{-5mm}
\end{table}




\subsection{Language-guided Generation}
\label{sec:text}
 \vspace{-2mm}
 
\noindent
\textbf{Data and evaluation.} We reorganize the dataset released by ShapeGlot~\cite{achlioptas2019shapeglot} into text-shape pairs to train a text-driven conditional generative model for 3D shapes, using the train/test splits provided by AutoSDF~\cite{mittal2022autosdf}.
Since most existing metrics cannot well measure the similarity between text and shape modalities, we propose new evaluation metrics for text-driven shape generation task. \textit{CLIP-S} computes the maximum score of cosine similarity between $N=9$ generated shapes and their text prompts by a pre-trained CLIP~\cite{radford2021clip}. Since CLIP cannot handle 3D shape inputs, we render each generated shape into 20 2D images from different views to compute \textit{CLIP-S}. In addition, We deploy \textit{Frechet Pointcloud Distance} (FPD) and \textit{Pairwise Minimum Matching Distance} (PMMD) to calculate the distance between ground truth and samples.

 \begin{figure}[t]
 \vspace{-5mm}
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/img-3d3.png}
   \caption{Single-view reconstruction results. Our model synthesizes high-quality single-view reconstruction with learned prior and cross-domain alignment.}
   \label{img_3d}
   \vspace{-5mm}
\end{figure}

\noindent
\textbf{Condition injection.} We first use the CLIP~\cite{radford2021clip} pre-trained model (ViT-B) to encode text prompts to 77 tokens. Then parts of self-attention modules in the denoising network are replaced by cross-attention~\cite{chen2021crossvit}, for its natural compatibility with multi-modality data. As a result, the text token embeddings constantly influence the generation through cross-attention modules.

\noindent
\textbf{Baselines and results.} We quantitatively compare our results with two state-of-the-art methods, \ie, AutoSDF~\cite{mittal2022autosdf} and Shape-IMLE~\cite{liu2022towards}. We only use the generated shapes from Shape-IMLE~\cite{liu2022towards} and discard their colors, to only evaluate 3D geometry quality. Quantitative results are reported in~\cref{tab:text} and a visual comparison is shown in~\cref{text_3d}. We observe a lack of diversity from results of baselines, while our method is diversified due to randomness in diffusing process. The unified discrete local representation of codebook indexing among texts and shapes achieves good alignment between two modalities.


\subsection{Extended Applications}
\label{sec:Extensions}
 \vspace{-2mm}
 
In addition to the above major tasks, we show that 3DQD can be extended for a wide range of downstream applications, serving as a generalized prior \textbf{with little or even no tuning}. Note that in contrast, previous 3D shape priors~\cite{zhou2021pvd, mittal2022autosdf} DO NOT possess this generalization capability. 


\noindent
\textbf{Denoising conditonal generation.}
In practice, 3D shapes captured from real scenes often have rough surfaces and noise points due to precision limitation of the capture device. It thus requires extra adaption for downstream models to accommodate noisy data, \eg point clouds denoising~\cite{rakotosaona2020pointcleannet}. To demonstrate our model's ability in dealing with noisy inputs, we add different levels of Gaussian and uniform noise to T-SDFs to simulate the noisy mesh surfaces in real world. Then noisy T-SDFs are encoded into shape tokens as $\widetilde{s}_k$ to replace the fully masked $s_T$ into our pre-trained 3DQD, where $k=\frac{1}{2}T$. Then reverse Markov process starts from $\widetilde{s}_k$. Visual results are shown in~\cref{uncond_denoise}, and quantitative evaluations with different noise levels and types are detailed in our supplementary material.
From~\cref{uncond_denoise}, it is noted that our model successfully recovers noisy inputs into clean samples \textbf{without any tuning}, demonstrating the noise immunity ability of our pre-trained 3DQD, which comes from the noise compatibility from diffusing training and quantized noise-free vectors in codebook.


\noindent
\textbf{Text-driven shape editing.}
We also conduct an experiment on text-guided shape editing \textbf{without any tuning}. We initialize standard chair token maps with text prompts \textit{``A chair"} with 3DQD. Then we initialize the former token maps as the start of new reverse process $\widetilde{s}_k$ for sequentially generating new shapes, with new text inputs one by one. $k$ is set to $0.98T$ to encourage novel structure generation. The results are visualized in~\cref{text_edit}. Generated shapes are apparently high-quality and realistic, due to the structure prior memorized by diffusion generator.


\begin{figure}[t]
\vspace{-7mm}
  \centering
   \includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/text_edit2.png}
   \caption{Text-driven shape editing results. We start from initial phrase at left and gradually edit them with new prompts. They present continuous and smooth evolution.}
   \label{text_edit}
    \vspace{-4mm}
\end{figure}

\begin{table}
%\vspace{-7mm}
  \centering
  \resizebox{\linewidth}{!}{
  \begin{tabular}{lccc}
  \toprule
Method       & VAE Training       & Diffusion Training        & Inference Time / Shape     \\
\midrule
Lion~\cite{zeng2022lion}     & 110 hours          & 440 hours         & 27.12 seconds                \\
Ours    & \textbf{9 hours}       & \textbf{60 hours}        & \textbf{1.61 seconds}           \\

\bottomrule
%\vspace{-5mm}
  \end{tabular}}
  \caption{Quantitative comparison of computational cost on a single V100 NVIDIA GPU. Our VAE is fine-tuned on pre-trained model released in ~\cite{mittal2022autosdf} for 9 hours.}
  \label{tab:efficiency}
   \vspace{-5mm}
\end{table}


\noindent
\textbf{Single-view reconstruction.}
Inferring the 3D shape from a single image is always non-trivial due to the lack of information. We show that with a well pre-trained VQ-VAE to encode and quantize images, our model achieves singe-view 3D reconstruction with a little fine-tuning. Specifically, we obtain the index coding of images with VQ-VAE released by Mittal \etal~\cite{mittal2022autosdf} on Pix3D dataset~\cite{sun2018pix3d}, and modify the condition embedding module of language-guided 3DQD model from discrete text tokens to discrete image tokens, with rest parts of the net remaining unchanged. We then fine-tune this conditional 3DQD model on Pix3D with masked image-shape pairs for 10 hours. For inference, we set index codes of images from VQ-VAE at the beginning of reverse process $\widetilde{s}_k$. Note that image embeddings modulate the generation process via cross-attention. Visual results are shown in~\cref{img_3d} as accurate and realistic reconstructions with learned prior and cross-domain alignment. More results are provided in our supplementary material.




 
\begin{figure}[t]
 \vspace{-5mm}
  \centering
   \includegraphics[width=\linewidth]{cvpr2023-author_kit-v1_1-1/img/freq.png}
   \caption{Power spectral density of different frequencies on 1000 generated shapes along with visual comparison. Our MFM layers successfully suppress high-frequency components.}
   \label{freq}
 \vspace{-5mm}
\end{figure}



 \vspace{-2mm}
\subsection{Ablation Study}
\label{sec:Ablation}

 \vspace{-2mm}
\subsubsection{Frequency Analysis}
\label{sec:freq}
 \vspace{-2mm}
We experimentally analyze the frequency components of the generation to validate our design of Multi-frequency Fusion Module (MFM). We sample 1000 completion samples with three methods: our 3DQD, our 3DQD without MFM, and AutoSDF. The T-SDF results are fed to Discrete Cosine Transform to calculate its average power spectral density and obtain frequency components within each spatial volume. The power spectral density of sampled volumes is plotted in~\cref{freq}. It is noted that our MFM layers successfully suppress high-frequency noise and help generate shapes with smooth surfaces. More ablation studies about the architecture of MFM is in our supplementary material.

 \vspace{-3mm}
\subsubsection{Efficiency Analysis}
\label{sec:Efficiency}
 \vspace{-2mm}
 
We compare our model with the latest 3D diffusion work Lion~\cite{zeng2022lion} (with no open source codes) about training and inference complexity, evaluated on single-class shape generation task as shown in~\cref{tab:efficiency}. It is observed that our model only requires 69 (9+60) hours for training totally while Lion~\cite{zeng2022lion} spends as much as 550 (110+440) hours. Also note that our reported results are all generated with 100-step DDPM-based sampling, much more efficient than that of Lion (\ie 1000 steps), with visually competitive performance compared to Lion~\cite{zeng2022lion}.





 \vspace{-3mm}
\section{Conclusion} 
 \vspace{-2mm}

We have introduced 3DQD, a unified and efficient shape generation prior model for multiple 3D tasks. Our model first learns a compact representation with P-VQ-VAE for its advantages in computational saving and consistency among different tasks. Then a novel discrete diffusion generator is trained with accurate, expressive and diversified object structural modeling. Multi-frequency fusion modules are developed to suppress high-frequency outliers. Solid experiments and rich analysis have demonstrated our approach possesses superior generative power and impressive shape generation quality on various 3D shape generation tasks. Furthermore, our prior model can serve as a generalized backbone for multiple downstream tasks with no or little tuning, while no architectural change is needed.

 \vspace{-2mm}
\section{Acknowledge} 
 \vspace{-2mm}
This work was supported by National Science Foundation of China (U20B2072, 61976137). This work was also partly supported by SJTU Medical Engineering Cross Research Grant YG2021ZD18.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
