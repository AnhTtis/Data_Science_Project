\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{graphicx}
%\usepackage{caption}
\usepackage{float}
\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{mathtools}
\usepackage{cite}
\usepackage{colortbl}
\usepackage{subfigure}
\allowdisplaybreaks

\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{problem}{Problem}

\theoremstyle{plain}
\newtheorem{proposition}{Proposition}
\newtheorem{struct_result}{Structural Result}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\let\inf\relax 
\DeclareMathOperator*\inf{\vphantom{p}inf}
\newcommand{\indep}{\perp \!\!\! \perp}

\pagestyle{plain}
\pagenumbering{arabic}

\title{\LARGE \bf Worst-Case Control and Learning Using Partial Observations Over an Infinite Time-Horizon}

\author{Aditya Dave, Ioannis Faros, Nishanth Venkatesh, {\itshape{Student Members, IEEE,}} \\ Andreas A. Malikopoulos, {\itshape{Senior Member, IEEE}} 
	\thanks{This research was supported by NSF under Grants CNS-2149520 and CMMI-2219761.} %
	\thanks{The authors are with the Department of Mechanical Engineering, University of Delaware, Newark, DE 19716 USA (email: \texttt{adidave@udel.edu; nish@udel.edu; ifaros@udel.edu; andreas@udel.edu).}} }

\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}
Safety-critical cyber-physical systems require control strategies whose worst-case performance is robust against adversarial disturbances and modeling uncertainties.
In this paper, we present a framework for approximate control and learning in partially observed systems to minimize the worst-case discounted cost over an infinite time-horizon. 
We model disturbances to the system as finite-valued uncertain variables with unknown probability distributions. For problems with known system dynamics, we construct a dynamic programming (DP) decomposition to compute the optimal control strategy. 
Our first contribution is to define information states that improve the computational tractability of this DP without loss of optimality.
Then, we describe a simplification for a class of problems where the incurred cost is observable at each time-instance. 
Our second contribution is a definition of approximate information states that can be constructed or learned directly from observed data for problems with observable costs. We derive bounds on the performance loss of the resulting approximate control strategy. %and illustrate the effectiveness of our approach using a numerical example.
%A key feature of these approximate information states is that they can be learned directly from observed data, without knowledge of system dynamics. 
%We illustrate the effectiveness of our approach to learning approximate information states and controlling partially observed systems using a numerical example.
\end{abstract}

\section{Introduction}
\label{section:Introduction}

Cyber-physical systems, such as connected and automated vehicles \cite{Malikopoulos2020}, power systems \cite{zhu2011robust}, and social media platforms \cite{Dave2020SocialMedia}, often require decision-making in uncertain environments with limited knowledge of the system model \cite{Malikopoulos2022a} and partial knowledge of the dynamics \cite{sutton2018reinforcement} over a long time horizon. This decision-making challenge is typically modeled with a \textit{stochastic formulation}, where an agent can access a prior probability distribution for all uncertainties and computes a control strategy to minimize the expected value of a discounted total cost across an infinite time-horizon \cite{Dave2021nestedaccess}. %This approach has also been utilized in reinforcement learning \cite{subramanian2022approximate} and decentralized systems \cite{Dave2021nestedaccess}.
However, the actual performance of a such a control strategy is poor when the assumed prior distribution is different from the actual underlying distribution \cite{mannor2007bias}. To mitigate this drawback, research efforts have proposed alternatives, including: \textit{(1) robust stochastic formulations:} where an agent minimizes the worst-case expected cost given a set of feasible probability distributions \cite{iyengar2005robust, wiesemann2013robust}, and \textit{(2) risk-sensitive formulations:} where an agent minimizes a combination of both the expected cost and the cost variance \cite{mihatsch2002risk, baauerle2017partially}. 
While these formulations reduce performance degradation from distribution mismatch, many safety-critical applications require further guarantees on the worst-case performance of a control strategy against either adversarial attacks or system failure, e.g., cyber-security \cite{rasouli2018scalable} and cyber-physical systems \cite{shoukry2013minimax}. %, and power systems \cite{zhu2011robust}. 
For such applications, it is appropriate to consider a \textit{non-stochastic formulation}, where the agent has knowledge only of the feasible set of all uncertainties and no knowledge of their distributions \cite{bertsekas1973sufficiently}. Then, the control strategy minimizes the \textit{maximum} possible discounted cost across a time horizon \cite{bernhard2000max, moon2015minimax, gagrani2017decentralized, Dave2021minimax, Dave2023approximate}. This non-stochastic formulation is both maximally robust and most risk-averse under any feasible prior distribution \cite{james1994risk, coraluppi1999risk}.

%In either of these formulations, either the risk-sensitivity or robustness need to be maximized in applications that require guarantees on a system's worst-case performance, for example: (1) control of systems under attack from an adversary, like cyber-security systems \cite{rasouli2018scalable}, and (2) control of systems where a single event of failure can be damaging, like water reservoirs \cite{giuliani2021state}. 
%The limiting formulation in both cases yields a non-stochastic worst-case formulation \cite{james1994risk, coraluppi1999risk}, where the agent aims to minimize the worst-case discounted cost without knowledge of distributions \cite{bernhard2000max, bacsar2008h, moon2015minimax, gagrani2017decentralized, Dave2021minimax, Dave2023approximate}. 

In this paper, we analyze a non-stochastic problem over an infinite time-horizon with an agent that can access only output data and may not know the underlying state-space model. In such partially observed problems, an optimal strategy can be computed using a memory-based dynamic program (DP) when the time-horizon is finite. However, as time tends to infinity, the agent's memory grows to an infinite dimensional vector. This makes a memory-based approach computationally intractable and necessitates an alternate solution. When the state-space model is known, this challenge is overcome using the \textit{maximum cost-to-come} as an information state to construct a DP decomposition, both in finite-time problems \cite{bacsar2008h} and infinite-time discounted problems \cite{baras1998robust, bernhard2003minimax}. The computational tractability of this DP has been further improved in finite-time problems using approximate information states \cite{Dave2022approx, dave2022additive}. Meanwhile, general notions of information states and their approximations have been developed for stochastic problems over an infinite time-horizon without knowledge of state-space models \cite{subramanian2022approximate}. 
In contrast, to the best of our knowledge, no such general notions exist for infinite-time non-stochastic problems. 

%Meanwhile, for stochastic decision-making problems with an unknown state-space model, a general notion of an information state and its approximation has been defined in \cite{subramanian2022approximate}.

The main contributions of this paper are: (1) we introduce general information states and a time-invariant DP to compute an optimal strategy in non-stochastic problems over an infinite time-horizon; and (2) we specialize information states for problems with observable costs and define approximate information states to compute a strategy with a bounded performance loss. %; and (3) using a numerical example, we show that approximate information states can be learned directly from output data and subsequently, used in a deep Q-learning algorithm to compute an approximate strategy.

The remainder of the paper proceeds as follows. In Section \ref{section:problem}, we present our model. In Section \ref{section:DP_and_info}, we define information states and the corresponding DP. In Section \ref{section:perfectly_observed}, we consider problems with observable costs, define approximate information states, and derive performance bounds. 
%In Section \ref{section:example}, we present a numerical example to illustrate our results.
Finally, in Section \ref{section:conclusion}, we draw concluding remarks and discuss ongoing work.


\subsection{Notation and Preliminaries}
\label{subsection:Notation}

In our formulation, we use the mathematical framework of uncertain variables \cite{nair2013nonstochastic} and cost distributions \cite{bernhard2003minimax,dave2022additive}:

\textit{1) Cost Measures:}
Consider a sample space $\Omega$ with a sigma algebra $\mathcal{B}(\Omega)$. A \textit{cost measure} is the non-stochastic analogue of a probability measure. Specifically, it is a function $q: \mathcal{B}(\Omega) \to \{-\infty\} \cup (-\infty,0]$ satisfying the properties: (1) $q(\Omega) = 0$, (2) $q(\emptyset) = - \infty$, and (3) $q(B) = \sup_{\omega \in B} q(\omega)$ for all $B \in \mathcal{B}(\Omega)$, where, by convention, the supremum over an empty set is $- \infty$. Furthermore, for two sets $B^1, B^2 \in \mathcal{B}(\Omega)$ with $q(B^2) > - \infty$, the conditional cost measure of $B^1$ given $B^2$ is 
$q(B^1|B^2) := q(B^1, B^2) - q(B^2),$ where $q(B^1, B^2) = \sup_{\omega \in B^1 \cap B^2} q(\omega).$ 

\textit{2) Uncertain Variables:}
For a set $\mathscr{X}$, an uncertain variable is a mapping $\mathsf{X}: \Omega \to \mathscr{X}$ and is compactly denoted by $\mathsf{X} \in \mathscr{X}$. This is the non-stochastic equivalent of a random variable. For any $\omega \in \Omega$, its realization is $\mathsf{X}(\omega) = \mathsf{x} \in \mathscr{X}$. 
Its \textit{marginal range} is the set of feasible realizations $[[\mathsf{X}]] \hspace{-1pt} := \hspace{-1pt} \{\mathsf{X}(\omega) \; | \; \omega \in \Omega\} \subseteq \mathscr{X}$.
The \textit{cost distribution} is
$q(\mathsf{x}) := \sup_{\omega \in \{\Omega|\mathsf{X}(\omega) = \mathsf{x}\}} q(\omega)$ for all $\mathsf{x} \in [[\mathsf{X}]]$.
The \textit{joint range} of two uncertain variables $\mathsf{X} \in \mathscr{X}$ and $\mathsf{Y} \in \mathscr{Y}$ is the set of feasible simultaneous realizations $[[\mathsf{X},\mathsf{Y}]] \hspace{-1pt} := \hspace{-1pt} \big\{ \big(\mathsf{X}(\omega), \mathsf{Y}(\omega) \big) \; | \; \omega \in \Omega \big\} \subseteq \mathscr{X} \times \mathscr{Y}$. The two uncertain variables are \textit{independent} if $[[\mathsf{X},\mathsf{Y}]] = [[\mathsf{X}]] \times [[\mathsf{Y}]]$. The \textit{conditional range} of $\mathsf{X}$ given a realization $\mathsf{y}$ of $\mathsf{Y}$ is the set $[[\mathsf{X}|\mathsf{y}]] \hspace{-1pt} := \hspace{-1pt} \big\{ \mathsf{X}(\omega) \; | \; \mathsf{Y}(\omega) = \mathsf{y}, \; \omega \in \Omega \big\}$. %, and in general, $[[\mathsf{X}|\mathsf{Y}]] \hspace{-1pt} := \hspace{-1pt} \big\{ [[\mathsf{X}|\mathsf{y}]]\;| \; \mathsf{y} \in [[\mathsf{Y}]] \big\}$.
The cost distribution of any $\mathsf{x} \in [[\mathsf{X}]]$ given a realization $\mathsf{y} \in [[\mathsf{Y}]]$ with $q(\mathsf{y}) > - \infty$ is $q(\mathsf{x}|\mathsf{y}) = q(\mathsf{x},\mathsf{y}) - q(\mathsf{y})$, where $q(x,y) = \sup_{\omega \in \{\Omega|X(\omega)=x, Y(\omega) = y\}} q(\omega)$.

\textit{3) Hausdorff Distance:}
Consider two bounded, non-empty subsets $\mathscr{X}, \mathscr{Y}$ of a metric space $(\mathscr{S}, \eta)$, where $\eta: \mathscr{X} \times \mathscr{Y} \to \mathbb{R}_{\geq0}$ is the metric. The Hausdorff distance between $\mathscr{X}$ and $\mathscr{Y}$ is the pseudo-metric \cite[Chapter 1.12]{barnsley2006superfractals}:
\begin{align} \label{H_met_def}
    \hspace{-4pt} \mathcal{H}(\mathscr{X}, \mathscr{Y}) \hspace{-2pt} := \hspace{-2pt} \max \hspace{-1pt} \big\{ \hspace{-1pt} \sup_{\mathsf{x} \in \mathscr{X}} \hspace{-1pt} \inf_{\mathsf{y} \in \mathscr{Y}}  d(\mathsf{x}, \mathsf{y}), \sup_{\mathsf{y} \in \mathscr{Y}} \hspace{-1pt} \inf_{\mathsf{x} \in \mathscr{X}}  d(\mathsf{x}, \mathsf{y}) \hspace{-2pt} \big\}. \hspace{-4pt}
\end{align}
Furthermore, if $f: \mathscr{S} \to \mathbb{R}$ is a function with a global Lipschitz constant $L_f \in \mathbb{R}_{\geq0}$, then \cite[Lemma 5]{Dave2023approximate}:
\begin{align} \label{H_met_property}
    \Big|\sup_{x \in \mathscr{X}}f(x) - \sup_{y \in \mathscr{Y}}f(y)\Big| \leq L_f \cdot \mathcal{H}(\mathscr{X}, \mathscr{Y}).
\end{align}

\section{Problem Formulation} \label{section:problem}

We consider the control of an uncertain system which evolves in discrete time steps. At each time $t \in \mathbb{N} = \{0,1,2,\dots\}$ an agent collects an observation on the system as the uncertain variable $Y_t \in \mathcal{Y}$ and generates a control action denoted by the uncertain variable $U_t \in \mathcal{U}$. After generating the action at each $t$, the agent incurs a cost denoted by the uncertain variable $C_t \in \mathcal{C} \subset \mathbb{R}_{\geq0}$. The set $\mathcal{C}$ is bounded by $\min\{\mathcal{C}\} = c^{\min}$ and $\max\{\mathcal{C}\} = c^{\max}$. We formulate our problem for a general case where the agent may not have knowledge of a state-space model for the system. Thus, we use an \textit{input-output model} to describe the evolution of the system, as follows. At each ${{t \in \mathbb{N}}}$, the system receives two inputs: the action $U_t$, and an uncontrolled disturbance $W_t \in \mathcal{W}$. The disturbances $\{W_t:{{t \in \mathbb{N}}}\}$ constitute a sequence of independent uncertain variables.
After receiving the inputs at each $t$, the system generates two outputs, the observation $Y_{t+1} = h_{t+1}(W_{0:t}, U_{0:t})$ using an observation function $h_{t+1}: \mathcal{W}^t \times \mathcal{U}^t \to \mathcal{Y}$, and the cost $C_t = d_t(W_{0:t}, U_{0:t})$ using a cost function $d_t: \mathcal{W}^t \times \mathcal{U}^t \to \mathcal{C}$. The initial observation is generated as $Y_0 = h_0(W_0)$.

The agent has perfect recall of the history of observations and control actions. At each ${{t \in \mathbb{N}}}$, we denote the agent's memory by the uncertain variable $M_t := (Y_{0:t}, U_{0:t-1})$ taking values in $\mathcal{M}_t := \mathcal{Y}^t \times \mathcal{U}^{t-1}$. The agent uses a control law $g_t: \mathcal{M}_t \to \mathcal{U}$ to generate the control action $U_t = g_t(M_t)$ as a function of the memory. The control strategy is the collection of control laws $\boldsymbol{g} := (g_0, g_1,\dots)$ with a feasible set $\mathcal{G}$. We measure the performance of a strategy $\boldsymbol{g} \in \mathcal{G}$ using the \textit{worst-case discounted cost}:
    \begin{align} \label{eq_instantaneous_criterion}
    \mathcal{J}(\boldsymbol{g}) := \lim_{T \to \infty} \sup_{c_{0:T} \in [[C_{0:T}]]^{\boldsymbol{g}}} \sum_{t=0}^T \gamma^t {\cdot} c_t,
    \end{align}
where $\gamma \in (0,1)$ is the discount parameter, the marginal range $[[C_{0:T}]]^{\boldsymbol{g}}$ is the set of all feasible costs consistent with the strategy $\boldsymbol{g}$ and with the set of feasible disturbances $\mathcal{W}$. Note that this limit exists because $C_t \leq c^{\max}$ for all $t$. Next, we set up the agent's control problem with known dynamics.

\begin{problem} \label{problem_1}
The optimization problem is to derive the optimal value $\inf_{\boldsymbol{g} \in \mathcal{G}} \mathcal{J}(\boldsymbol{g}),$
given the feasible sets $\{\mathcal{U}, \mathcal{W}, \mathcal{Y}, \mathcal{C}\}$ and the functions $\{h_t, d_t ~|~ {{t \in \mathbb{N}}} \}$.
\end{problem}

If the minimum value is achieved in Problem \ref{problem_1}, the minimizing argument $\boldsymbol{g}^* = \arg \min_{\boldsymbol{g} \in \mathcal{G}} \mathcal{J}(\boldsymbol{g})$ in Problem \ref{problem_1} is called an optimal control strategy. Our aim is to tractably compute the optimal value and an optimal strategy if one exists. We impose the following assumption on our analysis.

\begin{assumption} \label{assumption_1}
We consider that the sets $\{\mathcal{U},\mathcal{W},\mathcal{Y}\}$ are bounded subsets of a metric space $(\mathscr{S}, \eta)$ and $\mathcal{C}$ is a bounded subset of $\mathbb{R}_{\geq0}$.
\end{assumption}

Assumption \ref{assumption_1} ensures that all uncertain variables take values in bounded sets and that we can use the Hausdorff pseudo-metric \eqref{H_met_def} as a distance measure between them. 

%\begin{assumption} \label{assumption_2}
%The observation functions $\{h_t, d_t~|~ t=0,\dots,T\}$ of the system are both Lipschitz continuous.
%\end{assumption}

%Assumption \ref{assumption_2} is satisfied by a large class of observation functions, including: (1) all functions with compact domains and finite co-domains; and (2) bi-Lipschitz functions, like linear functions, with compact domains and compact co-domains (see Appendix A). We will require both assumptions in Section \ref{section:approx} when deriving the main results.

%\begin{remark}
%In our exposition, we also consider a special case of \eqref{eq_instantaneous_criterion}, called the \textit{maximum terminal cost criterion}, given by
%\begin{align} \label{eq_terminal_criterion}
%    \mathcal{J}^\text{tm}(\boldsymbol{g}) := \sup_{w_{0:T} \in [[W_{0:T}]]} C_T.
%\end{align}
%In addition to the general results for Problem \ref{problem_1}, we often present results specifically for systems which utilize \eqref{eq_terminal_criterion} as the performance measure. This serves two purposes: (1) the results are often easier to interpret for a terminal cost problem; and (2) these results can be extended to \textit{additive cost problems}. We explicitly present this extension in Subsection \ref{subsection:info_examples}.
%\end{remark}

%\begin{remark}
%We first derive our results for Problem \ref{problem_1} with known dynamics. However, our main results in Section \ref{section:perfectly_observed} are also suitable for reinforcement learning problems with unknown dynamics. We illustrate this application with an example in Section \ref{section:example}.
%\end{remark}

%\begin{remark}
%For some dynamics, no control strategy $\boldsymbol{g}^* \in \mathcal{G}$ may satisfy $\mathcal{J}(\boldsymbol{g}^*) = \inf_{\boldsymbol{g} \in \mathcal{G}} \mathcal{J}(\boldsymbol{g})$. In these problems, we can generally find an $\varepsilon$-optimal control strategy $\boldsymbol{g}^*(\varepsilon)$ instead, which satisfies $\mathcal{J}(\boldsymbol{g}^*(\varepsilon)) -  \inf_{\boldsymbol{g} \in \mathcal{G}} \mathcal{J}(\boldsymbol{g}) \leq \varepsilon$ for an arbitrarily small $\varepsilon \in \mathbb{R}_{\geq0}$ \cite{moitie2002optimal}. Note that the results derived in subsequent sections will also be applicable to the computation of $\boldsymbol{g}^*(\varepsilon)$.
%\end{remark}

\section{Dynamic Program and Information States} \label{section:DP_and_info}

In this section, we first present value functions to evaluate the performance of any strategy $\boldsymbol{g} \in \mathcal{G}$. Next, we present a memory-based DP decomposition of Problem \ref{problem_1} that approximately computes the value functions with arbitrary precision. However, because the memory grows in size with time, this DP suffers from exponentially increasing computation with an increase in precision. To alleviate this computational challenge, we present the notion of information states in Subsection \ref{subsection:basic_info_states} and use them to construct a time-invariant DP decomposition which converges to the optimal value of Problem \ref{problem_1}.
%Note that a memory-based DP cannot compute the optimal value exactly because the memory grows to an infinite-dimensional vector as $t \to \infty$. Then, we define time-invariant approximate information states for this problem and show how they can be used to to simplify the DP decomposition. 
To construct value functions, we first define the \textit{accrued cost} at each ${{t \in \mathbb{N}}}$ as the sum of past incurred costs:
\begin{gather}
    A_t := \sum_{\ell=0}^{t-1} \gamma^\ell {\cdot} C_{\ell},
\end{gather}
which satisfies $A_{t+1} = A_t + \gamma^t{\cdot} C_t$ with $A_0 := 0$. 
This is well defined in the limit $t \to \infty$ because $\lim_{t \to \infty} A_t \leq \lim_{t \to \infty} \sum_{\ell=0}^{t-1} \gamma^{\ell} {\cdot} c^{\max} = \frac{c^{\max}}{1-\gamma} =: a^{\max}$. Thus, $A_t \in [0, a^{\max}]$ for all ${{t \in \mathbb{N}}}$. Similarly, the \textit{cost-to-go} at any ${{t \in \mathbb{N}}}$ is the sum of future all costs still to be incurred:
\begin{gather}
    C^{\infty}_t := \sum_{\ell = t}^{\infty} \gamma^{\ell-t} {\cdot} C_{\ell}.
\end{gather}
Note that $C^{\infty}_t \in [0, a^{\max}]$ for all $t$ and that starting with $\lim_{t\to \infty} C^{\infty}_t = 0$, the cost-to-go follows the dynamics $C^{\infty}_{t} = C_t +  \gamma {\cdot} C^{\infty}_{t+1}$. Then, for all ${{t \in \mathbb{N}}}$, we can define a value function for any strategy $\boldsymbol{g} \in \mathcal{G}$ as 
    \begin{gather} \label{value_function_t_g}
        V_t^{\boldsymbol{g}}(m_t) := \sup_{a_t, c^{\infty}_{t} \in [[A_t, C^{\infty}_{t}|m_t]]^{\boldsymbol{g}}} \left(a_t + \gamma^t {\cdot} c^{\infty}_t \right),
    \end{gather}
where $[[A_t, C^{\infty}_{t}|m_t]]^{\boldsymbol{g}}$ is the conditional range induced by the choice of strategy $\boldsymbol{g}$. 
From the definition of the value functions, at $t=0$ it holds that $\sup_{y_0 \in \mathcal{Y}}V_0^{\boldsymbol{g}}(y_0) = \mathcal{J}({\boldsymbol{g}})$, where $m_0 = y_0$. 
Thus, the value function $V_0^{\boldsymbol{g}}(y_0)$ evaluates the performance of any strategy $\boldsymbol{g}$ for an initial observation $y_0$. Similarly, the optimal value function for each ${{t \in \mathbb{N}}}$ is:
\begin{gather} \label{value_function_t_opt}
    V_t(m_t) := \inf_{\boldsymbol{g} \in \mathcal{G}} V_t^{\boldsymbol{g}}(m_t),
\end{gather}
and the optimal value in Problem \ref{problem_1} is $\sup_{y_0 \in \mathcal{Y}} V_0(y_0) = \inf_{\boldsymbol{g} \in \mathcal{G}} \mathcal{J}(\boldsymbol{g})$. 

Given the value functions in \eqref{value_function_t_g} and \eqref{value_function_t_opt}, we can evaluate the performance of a strategy and compare it with the optimal performance. However, there is no natural DP decomposition to compute these value functions in an infinite-horizon system with no terminal time. 
Thus, we construct a memory-based DP that assumes a finite horizon $T \in \mathbb{N}$ and use it to recursively compute approximations of the value functions. For any $\boldsymbol{g} \in \mathcal{G}$, we define finite-horizon evaluation functions for all $m_t \in \mathcal{M}_t$ and $t=0,\dots,T-1$ as
    \begin{gather}
        J_{t}^{\boldsymbol{g}}(m_t; T) := \sup_{m_{t+1} \in [[M_{t+1}|m_t]]^{\boldsymbol{g}}} J_{t+1}^{\boldsymbol{g}}(m_{t+1}; T),
    \end{gather}
where, $J_{T}^{\boldsymbol{g}}(m_T;T) \hspace{-1pt} := \hspace{-1pt} \sup_{a_T, c_{T} \in [[A_T, C_{T}|m_T]]^{\boldsymbol{g}}} \hspace{-1pt} (a_T + \gamma^T {\cdot} c_T)$. Similarly, we define approximately optimal finite-horizon functions for all $m_t \in \mathcal{M}_t$ and $t=0,\dots, T-1$ as
    \begin{gather} \label{eq_DP_memory_opt}
        J_{t}(m_t; T) := \inf_{u_t \in \mathcal{U}}\sup_{m_{t+1} \in [[M_{t+1}|m_t, u_t]]} J_{t+1}(m_{t+1}; T),
    \end{gather}
where, $J_{T}(m_T;T) := \inf_{u_T \in \mathcal{U}} \sup_{a_T, c_{T} \in [[A_T, C_{T}|m_T, u_T]]}$ $(a_T + \gamma^T {\cdot} c_T)$. If the minimum is achieved in the RHS of \eqref{eq_DP_memory_opt}, the minimizing argument gives an approximately optimal control law $g_t:\mathcal{M}_t \to \mathcal{U}$ for all $t=0,\dots,T$. Note that the finite-horizon functions $J_t^{\boldsymbol{g}}(m_t; T)$ and $J_t(m_t; T)$ at any $t=0,\dots,T$ are parameterized by the choice of horizon $T \in \mathbb{N}$. Next, we bound the approximation error between the value functions and their finite-horizon counterparts.

% minimizing argument $u_t^*$ in the RHS of \eqref{eq_DP_memory_opt} exists for all $t=0,\dots,T$, then the corresponding approximate control law is $g_t(m_t) = u_t^*$. %Here, note that the the presence of the control action in the conditioning ensures that the conditional ranges are independent of the choice of strategy $\boldsymbol{g}$. 
%Next, we show that by increasing the horizon $T \in \mathbb{N}$, we can estimate the optimal value functions with arbitrary accuracy using the finite-horizon functions.

\begin{lemma} \label{finite_DP_memory}
For any finite horizon $T \in \mathbb{N}$ and all $t=0,\dots,T$, it holds that:
    \begin{align} 
        \hspace{-8pt} \textbf{a) }\dfrac{\gamma^{T+1} \hspace{-1pt} {\cdot} c^{\min}}{1-\gamma} \hspace{-2pt} + \hspace{-2pt} J_{t}^{\boldsymbol{g}}(m_t; \hspace{-1pt} T) &\leq \hspace{-1pt} V_t^{\boldsymbol{g}}(m_t) \nonumber \\
        &\leq \hspace{-1pt} J_{t}^{\boldsymbol{g}}(m_t; \hspace{-1pt} T) \hspace{-2pt} + \hspace{-2pt}\dfrac{\gamma^{T+1} \hspace{-1pt} {\cdot} c^{\max}}{1-\gamma}, \label{eq_finite_DP_memory_g} \\
        \hspace{-10pt} \textbf{b) } \dfrac{\gamma^{T+1} \hspace{-1pt}  {\cdot} c^{\min}}{1-\gamma} \hspace{-1pt} + \hspace{-1pt} J_{t}(m_t ; \hspace{-1pt} T) \hspace{-1pt}  &\leq \hspace{-1pt}  V_t(m_t) \nonumber \\
        &\leq \hspace{-1pt}  J_{t}(m_t; \hspace{-1pt} T) \hspace{-1pt}  + \hspace{-1pt} \dfrac{\gamma^{T+1} \hspace{-1pt}  {\cdot} c^{\max}}{1-\gamma}. \hspace{-4pt} \label{eq_finite_DP_memory_opt}
    \end{align}
\end{lemma}

\begin{proof}
\textit{a)} We prove each inequality in \eqref{eq_finite_DP_memory_g} using backward mathematical induction. For the upper bound at time $T$, we use the dynamics of the accrued cost and cost-to-go to write that $V^{\boldsymbol{g}}_T(m_T) = \sup_{a_T, c_T, c_{T+1}^{\infty} \in [[A_T, C_T, C_{T+1}^{\infty}|m_T]]^{\boldsymbol{g}}} (a_T + \gamma^T {\cdot} c_T + \gamma^{T+1} {\cdot} c_{T+1}^\infty)$ $ \leq \sup_{a_T, c_T \in [[A_T, C_T|m_T]]^{\boldsymbol{g}}} \left(a_T + \gamma^T {\cdot} c_T\right) + \gamma^{T+1} {\cdot} a^{\max} \leq J_{t}^{\boldsymbol{g}}(m_t;T) + \frac{\gamma^{T+1}{\cdot} c^{\max}}{1-\gamma}$. The lower bound at time $T$ follows from $\frac{c^{\min}}{1-\gamma} \leq c_{T+1}^{\infty}$ using the same sequence of arguments as before. This forms the basis of our induction.
Next, consider the hypothesis that \eqref{eq_finite_DP_memory_g} holds at time $t+1$. For the upper bound at time $t$, by definition $V_t^{\boldsymbol{g}}(m_t) 
= \sup_{a_t, c_t, c^{\infty}_{t+1} \in [[A_t, C_t, C_{t+1}^{\infty}|m_t]]^{\boldsymbol{g}}} \left(a_t + \gamma^t {\cdot} c_t + \gamma^{t+1} {\cdot} c^{\infty}_{t+1} \right) 
= \sup_{a_{t+1}, c^{\infty}_{t+1} \in [[A_{t+1}, C^{\infty}_{t+1}|m_t]]^{\boldsymbol{g}}}(a_{t+1} + \gamma^{t+1} {\cdot} c^{\infty}_{t+1}) 
= \sup_{m_{t+1} \in [[M_{t+1}|m_t]]^{\boldsymbol{g}}} \sup_{a_{t+1}, c^{\infty}_{t+1} \in [[A_{t+1}, C^{\infty}_{t+1}|m_{t+1}]]^{\boldsymbol{g}}}(a_{t+1} $ $+ \gamma^{t+1} {\cdot} c^{\infty}_{t+1}) 
= \sup_{m_{t+1} \in [[M_{t+1}|m_t]]^{\boldsymbol{g}}}V^{\boldsymbol{g}}_{t+1}(m_{t+1}) 
\leq \sup_{m_{t+1} \in [[M_{t+1}|m_t]]^{\boldsymbol{g}}} J^{\boldsymbol{g}}_{t+1}(m_{t+1}; T) + \frac{\gamma^{T+1}{\cdot} c^{\max}}{1-\gamma}
= J^{\boldsymbol{g}}_{t}(m_{t}; T) + \frac{\gamma^{T+1}{\cdot} c^{\max}}{1-\gamma}$, where, in the fourth equality, we use the definition of the value function at $t+1$; and in the inequality, we use the induction hypothesis. The lower bound at time $t$ follows from the same sequence of arguments. Thus, \eqref{eq_finite_DP_memory_g} holds using mathematical induction.

\textit{b)} We can prove the lower bound in \eqref{eq_finite_DP_memory_opt} by taking the infimum on both sides of the lower bound in \eqref{eq_finite_DP_memory_g}. To prove the upper bound in \eqref{eq_finite_DP_memory_opt}, we first note that $J_t(m_t; T) = \inf_{\boldsymbol{g} \in \mathcal{G}} J_t^{\boldsymbol{g}}(m_t; T)$ for all $t=0,\dots,T$ using standard arguments for the DP decomposition of terminal-cost problems \cite{bertsekas1973sufficiently}. Then, for the upper bound at time $T$, by definition $V_T(m_T) = \inf_{\boldsymbol{g} \in \mathcal{G}}V^{\boldsymbol{g}}_T(m_T) \leq \inf_{\boldsymbol{g} \in \mathcal{G}} J^{\boldsymbol{g}}_T(m_T; T) + \frac{\gamma^{T+1}{\cdot} c^{\max}}{1-\gamma} = J_T(m_T; T) + \frac{\gamma^{T+1}{\cdot} c^{\max}}{1-\gamma}$. Using this as the basis, the result follows for all $t=0,\dots,T$ using the same backward mathematical induction arguments as in \eqref{eq_finite_DP_memory_g}. 
\end{proof}

%\begin{remark}
From Lemma \ref{finite_DP_memory}, the approximation error between finite-horizon functions and corresponding value functions decreases with an increase in $T$. As a direct consequence of \eqref{eq_finite_DP_memory_opt}, it holds that $\lim_{T \to \infty} J_0(y_0; T) = V_0(y_0)$ for all $y_0 \in \mathcal{Y}$. However, note that the domain of $J_{T}(m_T; T)$ is $\mathcal{M}_T = \mathcal{Y}^T \times \mathcal{U}^{T-1}$ for a given $T$. The dimension of this domain grows with $T$ and in the limit $T \to \infty$, the set $\mathcal{M}_T$ is infinite dimensional. Thus, it is computationally intractable to achieve close approximations of the value using \eqref{eq_DP_memory_opt}. We alleviate this challenge in the next subsection using \textit{information states} which take values in time-invariant spaces.

% involves computing value functions for each $m_t \in \mathcal{M}_t$ for all $t=0,\dots,T$. As $T \to \infty$, note that the memory becomes an infinite dimensional vector rendering this a computationally intractable problem for large horizons. In the next subsection, we define \textit{information states} which can address this concern.
%\end{remark}

%    \begin{gather}
%        Q_t^{\boldsymbol{g}}(m_t, u_t) := \sup_{a_t, c_{t:\infty} \in [[A_t, C_{t:\infty}|m_t, u_t]]^g} \left(a_t + \sum_{\ell=t}^\infty \gamma^\ell {\cdot} c_\ell \right).
%    \end{gather}
%    Then $\sup_{y_0}V_0^g(y_0) = \mathcal{J}(g)$. Then, we can prove that, at each ${{t \in \mathbb{N}}}$:
%    \begin{gather*}
%        Q_t^{\boldsymbol{g}}(m_t, u_t) = \sup_{m_{t+1} \in [[M_{t+1}|m_t,u_t]]} V_{t+1}^g(m_{t+1}).
%    \end{gather*}
%Similarly, consider the optimal value functions $V_t(m_t) := \inf_{g} V_t^g(m_t)$ and 
%    \begin{gather*}
%        Q_t(m_t, u_t) := \sup_{m_{t+1} \in [[M_{t+1}|m_t,u_t]]} V_{t+1}(m_{t+1}).
%    \end{gather*}

\subsection{Information States} \label{subsection:basic_info_states}

In this subsection, we present the notion of information states which take values in time-invariant spaces and thus, lead to a fixed-point equation to compute the optimal value in Problem \ref{problem_1}.
%and use them to construct a time-invariant fixed-point operator which can compute the optimal value and optimal control strategy. 
To begin, recall from Section \ref{subsection:Notation} that a \textit{cost distribution} is the non-stochastic equivalent of a probability distribution for uncertain variables. Two specific cost distributions are used in our construction of information states.

\begin{definition} \label{def_ind}
Let $\mathsf{X} \in \mathscr{X}$ and $\mathsf{Y} \in \mathscr{Y}$ be two uncertain variables. The \textit{indicator function} $\mathbb{I}: \mathscr{X} \to \{\infty, 0\}$ is
\begin{gather}
    \mathbb{I}(\mathsf{x}) :=
    \begin{aligned}
    \begin{cases}
        0, &\text{ if } \mathsf{x} \in [[\mathsf{X}]], \\
        - \infty, &\text{ if } \mathsf{x} \not\in [[\mathsf{X}]],
    \end{cases}
    \end{aligned}
\end{gather}
and the conditional indicator function $\mathbb{I}: \mathcal{X} \times \mathcal{Y} \to  \{\infty, 0\}$ for any $\mathsf{x} \in \mathscr{X}$ given a realization $\mathsf{y} \in [[\mathsf{Y}]]$ is given by
\begin{gather} \label{indicator_def}
    \mathbb{I}(\mathsf{x}|\mathsf{y}) :=
    \begin{aligned}
    \begin{cases}
        0, &\text{ if } \mathsf{x} \in [[\mathsf{X}|\mathsf{y}]], \\
        - \infty, &\text{ if } \mathsf{x} \not\in [[\mathsf{X}|\mathsf{y}]].
    \end{cases}
    \end{aligned}
\end{gather}
\end{definition}

The indicator function checks whether the input takes values within the conditional range of an uncertain variable and it satisfies the properties of a cost distribution from Subsection \ref{subsection:Notation}. Next, we use it to define the accrued distribution.

\begin{definition} \label{def_accrued_cost}
Let $\mathsf{X} \in \mathscr{X}$ and $\mathsf{Y} \in \mathscr{Y}$ be two uncertain variables and let $A_t \in \mathcal{A}_t$ be the accrued cost at any $t=0,\dots,T$. An \textit{accrued distribution} for any $\mathsf{x} \in \mathscr{X}$ at any $t$ is a function $r_t: \mathscr{X} \to \{-\infty\} \cup [-a^{\max}, 0]$, given by
\begin{gather}
    r_t(\mathsf{x}) := \sup_{a_t \in \mathcal{A}}\big(a_t + \mathbb{I}(\mathsf{x}, a_t)\big) - \sup_{a_t \in \mathcal{A}} \big(a_t + \mathbb{I}(a_t) \big), 
\end{gather}
and the accrued distribution for $\mathsf{x} \in \mathscr{X}$ given a realization $\mathsf{y} \in [[\mathsf{Y}]]$ is a function $r_t: \mathscr{X} \times \mathscr{Y} \to \{-\infty\} \cup [-a^{\max}, 0]$, given by
\begin{align}
    \hspace{-5pt} r_t(\mathsf{x}|\mathsf{y}) \hspace{-2pt} := \hspace{-2pt} \sup_{a_t \in \mathcal{A}} \hspace{-2pt} \big(a_t + \mathbb{I}(\mathsf{x}, a_t|\mathsf{y}) \big) \hspace{-2pt}
    - \hspace{-2pt} \sup_{a_t \in \mathcal{A}} \hspace{-2pt} \big(a_t + \mathbb{I}(a_t|\mathsf{y}) \big). \hspace{-4pt} \label{def_r}
\end{align}
\end{definition}

The accrued distribution returns $-\infty$ when the input is not within the range of an uncertain variable and it returns an output from $[-a^{\max}, 0]$ otherwise. It is also satisfies all properties of a cost distribution as defined in Subsection \ref{subsection:Notation}. Note that, at each ${{t \in \mathbb{N}}}$, given the realization of memory $m_t \in \mathcal{M}_t$, action $u_t \in \mathcal{U}$ and the dynamics, we can compute the indicator $\mathbb{I}(c_t, m_{t+1}|m_t, u_t)$, and subsequently, the accrued distribution $r_t(c_t, m_{t+1}|m_t, u_t)$ for all possible realizations $c_t \in \mathcal{C}$ and $m_{t+1} \in \mathcal{M}_{t+1}$. We use accrued distributions to define information states for Problem \ref{problem_1}.

\begin{definition} \label{def_information_state}
An \textit{information state} at any $t \in \mathbb{N}$ is an uncertain variable $S_t= \sigma_t(M_t)$ taking values in a bounded, time-invariant subset $\mathcal{S}$ of a metric space $(\mathscr{S}, \eta)$. %, where $\sigma_t: \mathcal{M}_t \to \mathcal{S}$. 
Furthermore, there exists a time-invariant cost distribution $\rho: \mathcal{C} \times \mathcal{S} \times \mathcal{S} \times \mathcal{U} \to \{-\infty\} \cup [-a^{\max}, 0]$ such that for all $t$, for all $m_t \in \mathcal{M}_t$, $u_t \in \mathcal{U}$, $c_t \in \mathcal{C}$ and ${{{{s}}}}_{t+1} \in \mathcal{S}$:
    \begin{align}
        r_t(c_t, {{{{s}}}}_{t+1}~|~m_t, u_t) &= \rho(c_t, {{{{s}}}}_{t+1}~|~\sigma_t(m_t), u_t).
    \end{align}
    %where $r_t$ is the accrued distribution at time $t$ and
    %\begin{multline}
    %    \rho(c_t, {{{{s}}}}_{t+1}|\sigma_t(m_t), u_t) = \sup_{a_t \in \mathcal{A}}(a_t + \mathbb{I}(c_t, {{{{s}}}}_{t+1}, a_t|{{{{s}}}}_t, u_t) ) \\
    %    - \sup_{a_t \in \mathcal{A}}(a_t + \mathbb{I}(a_t|{{{{s}}}}_t, u_t)),
    %\end{multline}
    %and $\mathbb{I}(a_t|{{{{s}}}}_t, u_t) = \mathbb{I}(a_t|{{{{s}}}}_t)$ is time-invariant for all ${{t \in \mathbb{N}}}$ and ${{{{s}}}}_t \in \mathcal{S}$ and $u_t \in \mathcal{U}$.
\end{definition}

%We call such an information state time-invariant because its feasible set, its effect on the incurred cost and its accrued distribution on its evolution are unchanging with time. Note that for all ${{t \in \mathbb{N}}}$, an information state takes values in a time-invariant space $\mathcal{S}$ and induces a time-invariant accrued distribution $\rho$.
Next, we use the information state from Definition \ref{def_information_state} to construct a time-invariant operator $\mathcal{T}$ that yields a fixed-point equation to recursively compute the optimal value in Problem \ref{problem_1}. 
We begin by defining the \textit{cumulative discount} at any ${{t \in \mathbb{N}}}$ as the deterministic variable $z_t := \gamma^t \in (0,1]$ that starts at $z_0 = 1$ and evolves as $z_{t+1} = \gamma {\cdot} z_t$. 
Then, for any uniformly bounded function $\Lambda:\mathcal{S} \times (0,1] \to \mathbb{R}$ we define $\mathcal{T}:[\mathcal{S} \times (0,1] \to \mathbb{R}] \to [\mathcal{S} \times (0,1] \to \mathbb{R}]$, such that:
    \begin{multline} \label{eq_general_value_operator}
        \hspace{-10pt} [\mathcal{T} \Lambda]({{{{s}}}}, z) \\
        \hspace{-1pt} := \underset{u \in \mathcal{U}}{\inf} \; \underset{c \in \mathcal{C}, \; s' \in \mathcal{S}}{\sup} \Bigg(c + \gamma {\cdot} \Lambda(s', \gamma {\cdot} z)
            + \frac{\rho(c, s' ~|~ s, u)}{z}\Bigg), \hspace{-5pt}
    \end{multline}
where recall that $\gamma \in (0,1)$ is the discount factor.

\begin{remark}
    The RHS of \eqref{eq_general_value_operator} is finite in the limit $z \to 0$ because $c$ has a finite upper bound, $\Lambda(\cdot,\cdot)$ is uniformly bounded, and $\sup_{c \in \mathcal{C}, s' \in \mathcal{S}} \rho(c, s'|s,u)= 0$ from the definition of cost distributions.
\end{remark}

Due to discounting, $\mathcal{T}$ is a contraction mapping (see Appendix A) and therefore, using the Banach fixed point theorem, the equation $\Lambda = \mathcal{T} \Lambda$ admits a unique solution $\Lambda^{{{\infty}}} = \mathcal{T}\Lambda^{\infty}$. Starting at $\Lambda^{{{0}}}(s, z) := 0$, the fixed-point iteration around $\mathcal{T}$ recursively generates a sequence of functions
    \begin{align} \label{n_iterated_function}
        \Lambda^{{{n+1}}}(s, z) = [\mathcal{T} \Lambda^{{{n}}}](s, z)
        = [\mathcal{T}^n \Lambda^{{{0}}}](s,z),
    \end{align}
for all $n=1,2,\dots$, such that $\lim_{n \to \infty} \mathcal{T}^n V^{{{0}}} = \Lambda^{{{\infty}}}$. Next, we establish that $\Lambda^n(\sigma_t(m_t), z_t)$, for any ${{n \in \mathbb{N}}}$, can be used to estimate the value function $V_t(m_t)$ of Problem \ref{problem_1} at any ${{t \in \mathbb{N}}}$, with estimation error that decreases in $n$.
%error bounds for any ${{t \in \mathbb{N}}}$, when $\Lambda^n(\sigma_t(m_t), z_t)$, $z_t = \gamma^t$, for any ${{n \in \mathbb{N}}}$, is used to estimate the value function $V_t(m_t)$ of Problem \ref{problem_1} from \eqref{value_function_t_opt}.



%\begin{remark}
%    Note that in the limit $z \to 0$, the RHS of \eqref{eq_general_value_operator} has the limit $\inf_{u \in \mathcal{U}}\sup \big\{c + \gamma {\cdot} \Lambda(s', 0)~|~\rho(c, s'|s,u)=0\big\}$. This limit is well defined because from the definition of cost distributions $\sup_{s' \in \mathcal{S}, c \in \mathcal{C}} \rho(c, s'|s,u)= 0$. 
%\end{remark}




%Thus, the sequence of iterations $\mathcal{T}^n\Lambda^{{{0}}}({{{{s}}}}, z)$ for $n = 1,2,\dots$ converges to a fixed point given by
%    \begin{gather}
%       V^{{{\infty}}}({{{{s}}}}, z) := \min_{u \in \mathcal{U}} \sup_{a_t, c_{0:T} \in [[A_t, C_{0:T}|{{{{s}}}}, u]]} \left(a_t + z {\cdot} \sum_{\ell=0}^T \gamma^\ell {\cdot} c_{\ell} \right),
%    \end{gather}
%    for all ${{{{s}}}}_t \in \mathcal{S}$ and ${{t \in \mathbb{N}}}$.


\begin{theorem} \label{thm_g_opt}
    Consider the function $\Lambda^n$, for any ${{n \in \mathbb{N}}}$, generated using \eqref{n_iterated_function}. Then, for all ${{t \in \mathbb{N}}}$, it holds that:
    \begin{multline} \label{eq_gen_info_bounds}
        \hspace{-10pt} \frac{\gamma^{n+t}{\cdot} c^{\min}}{1-\gamma} + \gamma^t {\cdot} \Lambda^{{{n}}}(\sigma_t(m_t), \gamma^t) + \hspace{-2pt} \sup_{a_t \in [[A_t|m_t]]} \hspace{-4pt} a_t \; \leq \; V_t(m_t) \\ 
        \hspace{-10pt}\leq \sup_{a_t \in [[A_t|m_t]]}a_t + \gamma^t {\cdot} \Lambda^{{{n}}}(\sigma_t(m_t), \gamma^t)  + \frac{\gamma^{n+t} {\cdot} c^{\max}}{1-\gamma}.
    \end{multline}
\end{theorem}

\begin{proof}
We develop the proof for \eqref{eq_gen_info_bounds} by relying on \eqref{eq_finite_DP_memory_opt} from Lemma \ref{finite_DP_memory}. For this purpose, we first show that for any finite horizon $T \in \mathbb{N}$, for all $t=0,\dots,T,$ it holds that 
    \begin{gather} \label{interim_gen_info_bounds_1}
        J_{t}(m_t; T) = \gamma^t {\cdot} \Lambda^{T-t+1}\big(\sigma_t(m_t), \gamma^t\big) + \hspace{-4pt} \sup_{a_t \in [[A_t|m_t]]}a_t,
    \end{gather}
where $\Lambda^{T-t+1}$ is the $(T-t+1)$-th iterated function in \eqref{n_iterated_function}. We can prove \eqref{interim_gen_info_bounds_1} by reverse mathematical induction. At time $T$, recall that $J_{T}(m_T; T) = \inf_{u_T \in \mathcal{U}} \sup_{a_T, c_T \in [[A_T, C_T|m_T, u_T]]}(a_T + \gamma^T {\cdot} c_T)$. Using the indicator function in the RHS, it holds that $\sup_{a_T, c_T \in [[A_T, C_T|m_T, u_T]]}(a_T + \gamma^T {\cdot} c_T)$ 
$= \sup_{a_T \in \mathcal{A}, c_T \in \mathcal{C}} (a_T + \gamma^T {\cdot} c_T + \mathbb{I}(a_T, c_T| m_T, u_T))$ 
$= \sup_{a_T \in \mathcal{A}, c_T \in \mathcal{C}} \big(a_T + \gamma^T {\cdot} c_T + \mathbb{I}(a_T, c_T| m_T, u_T)$ 
$- \sup_{a_T \in \mathcal{A}}(a_T + \mathbb{I}(a_T | m_T, u_T) ) \big) + \sup_{a_T \in [[A_T|m_T]]}$ 
$= \gamma^T {\cdot} \sup_{c_T \in \mathcal{C}} \big( c_T + \gamma^{-T} {\cdot} r_T(c_T|m_T, u_T) \big) + \sup_{a_T \in [[A_T|m_T]]}a_T,$
where, in the second equality, we add and subtract the term $\sup_{a_T \in \mathcal{A}}(a_T + \mathbb{I}(a_T | m_T, u_T) )$; and in the last equality, we use the definition of the accrued cost. Thus, using we can write that $J_T(m_T; T) 
= \inf_{u_T \in \mathcal{U}} \sup_{c_T \in \mathcal{C}, s_{T+1} \in \mathcal{S}} \gamma^T {\cdot} (c_T + \gamma {\cdot} \Lambda^{{{0}}}(s_{T+1}, \gamma^{T+1}) + \gamma^{-T} {\cdot} r_T(c_T, s_{T+1}|m_T, u_T) ) + \sup_{a_T \in [[A_T|m_T]]} a_T 
= \gamma^T {\cdot} \Lambda^1(\sigma_T(m_T), \gamma^T) + \sup_{a_T \in [[A_T|m_T]]}a_T$,
where, in the first equality, we use the fact that $\Lambda^0(s_{T+1}, \gamma^{T+1}) :=0$ identically; % and that $\sup_{s_{T+1} \in \mathcal{S}}r_T(c_T, s_{T+1}|m_T, u_T) = r_T(c_T|m_T, u_T)$; 
and in the second equality, we use $r_T(c_T, s_{T+1}|m_T, u_T) = \rho(c_T, s_{T+1}|m_T, u_T)$ and the definition of $\Lambda^1(\sigma_T(m_T), \gamma^T)$. This forms the basis of our induction. Next, consider the hypothesis that \eqref{interim_gen_info_bounds_1} holds for time $t+1$. Using the definition of the memory-based function at time $t$ and the induction hypothesis, $J_{t}(m_t;T) 
= \inf_{u_t \in \mathcal{U}} \sup_{m_{t+1} \in [[M_{t+1}|m_t, u_t]]} J_{t+1}(m_{t+1};T) 
= \inf_{u_t \in \mathcal{U}} \sup_{m_{t+1} \in [[M_{t+1}|m_t, u_t]]} \sup_{a_{t+1} \in [[A_{t+1}|m_{t+1}]]}\big(\gamma^{t+1}$ ${\cdot} \Lambda^{T-t}(\sigma_{t+1}(m_{t+1}), \gamma^{t+1}) + a_{t+1}) 
= \inf_{u_t \in \mathcal{U}} \sup_{m_{t+1}, a_{t+1} \in [[M_{t+1}, A_{t+1}|m_t, u_t]]}(\gamma^{t+1} {\cdot} \Lambda^{T-t}(\sigma_{t+1}($ $m_{t+1}), \gamma^{t+1})+ \gamma^t {\cdot} c_t + a_t) 
= \gamma^t {\cdot} V^{T-t+1}(\sigma_t(m_t), \gamma^t) +  \sup_{a_t \in [[A_t|m_t]]} a_t$, where, in the third equality, we use the dynamics of accrued cost and rearrange the terms; and the fourth equality follows using the same sequence of arguments as in the terminal time step $T$. 
This proves \eqref{interim_gen_info_bounds_1} by mathematical induction.
Then, \eqref{eq_gen_info_bounds} follows directly for all ${{t \in \mathbb{N}}}$ and all ${{n \in \mathbb{N}}}$ by substituting \eqref{interim_gen_info_bounds_1} into \eqref{eq_finite_DP_memory_opt} with the time horizon $T = t + n -1$.
%\begin{multline}
%    = \min_{u_t \in \mathcal{U}} \sup_{m_{t+1} \in \mathcal{M}_{t+1}, a_{t} \in \mathcal{A}, c_t \in \mathcal{C}_t}(\gamma^{t+1} {\cdot} \Lambda^{T-t}(\sigma_{t+1}( \\m_{t+1}))
%    + \gamma^t {\cdot} c_t + a_t + \mathbb{I}(c_t, \sigma_{t+1}(m_{t+1}), a_t|m_t, u_t)) \\
%    = \min_{u_t \in \mathcal{U}} \sup_{m_{t+1} \in \mathcal{M}_{t+1}, c_t \in \mathcal{C}_t}(\gamma^{t+1} {\cdot} \Lambda^{T-t}(\sigma_{t+1}( \\ m_{t+1}))
%    + \gamma^t {\cdot} c_t + r_t(c_t, \sigma_{t+1}(m_{t+1})|m_t, u_t)) + \sup_{a_t \in [[A_t|m_t]]} a_t \\
%    = \gamma^{t} {\cdot}\min_{u_t \in \mathcal{U}} \sup_{\sigma_{t+1}(m_{t+1}) \in \mathcal{S}, c_t \in \mathcal{C}_t}(\gamma {\cdot} \Lambda^{T-t}(\sigma_{t+1}(m_{t+1})) \\
%    + c_t + \frac{r_t(c_t, \sigma_{t+1}(m_{t+1})|\sigma_t(m_t), u_t)}{\gamma^t} ) + \sup_{a_t \in [[A_t|m_t]]} a_t \\
%    = \gamma^t {\cdot} V^{(T-t+1)}(\sigma_t(m_t), \gamma^t) +  \sup_{a_t \in [[A_t|m_t]]} a_t.
%\end{multline}
\end{proof}

%Then, we can prove the following bounds for any ${{t \in \mathbb{N}}}$, it holds that $\frac{\gamma^{T+1}{\cdot} c_{\min}}{1-\gamma} + \gamma^t {\cdot} \Lambda^{(T-t+1)}(\sigma_t(m_t), \gamma^t) + \sup_{a_t \in [[A_t|m_t]]}a_t \leq V_t(m_t) 
%        \leq \gamma^t {\cdot} \Lambda^{(T-t+1)}(\sigma_t(m_t), \gamma^t) + \sup_{a_t \in [[A_t|m_t]]}a_t + \frac{\gamma^{T+1}{\cdot} c_{\max}}{1-\gamma}.$
%This directly implies that 
%\begin{multline}
%    \gamma^t {\cdot} \lim_{T \to \infty}[\mathcal{T}^{T-t+1} \Lambda](\sigma_t(m_t), \gamma^t) + \sup_{a_t \in [[A_t|m_t]]}a_t  \\
%    = \gamma^t {\cdot} \Lambda^{{{\infty}}}(\sigma_t(m_t), \gamma^t) + \sup_{a_t \in [[A_t|m_t]]}a_t = V_t(m_t),
%\end{multline}
%or in words, the fixed point of the value iteration equations computes the optimal value functions of Problem \ref{problem_1}. Furthermore, it holds by substituting $t=0$ that $\sup_{y_0 \in \mathcal{Y}} \Lambda^{{{\infty}}}(\sigma_0(y_0), 1)  = \sup_{y_0 \in \mathcal{Y}} V_0(y_0) = \inf_{\boldsymbol{g} \in \mathcal{G}}\mathcal{J}(\boldsymbol{g})$. Furthermore, the optimal control strategy which minimizes the time-invariant fixed point is also an optimal solution to the original problem at each $t$.

Theorem \ref{thm_g_opt} allows us to characterize the error when we estimate the optimal value $V_0(y_0)$ in Problem \ref{problem_1} for a given $y_0 \in \mathcal{Y}$ using $\Lambda^n(\sigma_0(y_0), 1)$, where $z_0 = \gamma^0 = 1$. On substituting $t=0$ in Theorem \ref{thm_g_opt}, we have  
$\frac{\gamma^{n} {\cdot} c^{\min}}{1-\gamma} + \Lambda^{{{n}}}(\sigma_0(y_0), 1) \leq V_0(y_0) \leq \Lambda^{{{n}}}(\sigma_0(y_0), 1)  + \frac{\gamma^{n} {\cdot} c^{\max}}{1-\gamma}$, where recall that $a_0 = 0$. Furthermore, this implies that as $n \to \infty$, the fixed point $\Lambda^{\infty}$ exactly computes the value as 
\begin{gather} \label{eq_optimality_of_infor_state}
    \Lambda^{\infty}(\sigma_0(y_0), 1) = V_0(y_0).
\end{gather}

Next, consider that the infimum is achieved in the RHS of $\mathcal{T} \Lambda^n$, i.e., in the RHS of \eqref{eq_general_value_operator}, for all ${{n \in \mathbb{N}}}$. Then, we have a time-invariant control strategy $\boldsymbol{\pi}^* = (\pi^*, \pi^*,\dots)$, where we omit the subscripts because the control laws are the same for each $t \in \mathbb{N}$. Specifically, the control law $\pi^*: \mathcal{S} \times (0,1) \to \mathcal{U}$ at each time is the minimizing argument $\pi^*(s, z) := \arg \min_{u \in \mathcal{U}} \sup_{c \in \mathcal{C}, s' \in \mathcal{S}}(c + \gamma {\cdot} \Lambda^{\infty}(s', \gamma {\cdot}z) + \frac{\rho(c, s'|s, u)}{z})$ for all $s \in \mathcal{S}$ and $z \in (0,1]$. 
A corresponding memory-based strategy is given by $\boldsymbol{g}^* = (g_0^*, g_1^*, \dots)$ with $g_t^*(m_t) := \pi^*(\sigma_t(m_t), \gamma^t)$ for all $m_t \in \mathcal{M}_t$ and all $t$. Then, $\boldsymbol{g}^*$ is an optimal solution to Problem \ref{problem_1} and $V^{\boldsymbol{g}^*}_0(y_0) = V_0(y_0)$ (see Appendix B). Thus, we can compute an optimal control strategy using information states and the time-invariant DP.

\subsection{Examples of Information States} \label{subsection:info_state_examples}

In this subsection, we consider a system with a known state-space model to present examples of information states. Consider a system with a state $X_t \in \mathcal{X}$ which starts at $X_0$ and evolves as $X_{t+1} = f_t(X_t, U_t, N_t)$ at each $t$, where $N_t \in \mathcal{N}$ is an uncontrolled disturbance. The agent's observation is  $Y_t = h_t(X_t, W_t)$ and the incurred cost is $C_t = d_t(X_t,U_t)$ at each $t$, where the uncontrolled variables $\{X_0, N_t, W_t: {{t \in \mathbb{N}}}\}$ take realizations independently. Next, we gives examples of information states for different cases:

\textit{1) Perfectly observed systems:} Consider that $Y_t = X_t$ for all $t$. An information state for such a system is $S_t = X_t \in \mathcal{X}$ at each $t$, i.e, the state itself.

\textit{2) Perfectly observed systems with deep dynamics:} Consider for all $t$ that $Y_t = X_t$ and %the state evolves as a function of $k + 1 \in \mathbb{N}$ previous states, i.e., 
$X_{t+1} = f(X_{t:t-k}, U_t, W_t)$. Then, an information state for such a system at each $t$ is $S_t = (X_{t-k}, \dots, X_t) \in \mathcal{X}^{k+1}$. 

\textit{3) Partially observed systems:} Consider a generic partially observed system. An information state at each $t$ is the function-valued variable $S_t: \mathcal{X} \to \{-\infty\} \cup [-a^{\max}, 0]$. %Note that this takes place in a time-homogenous function space. 
At each $t$, for a given $m_t \in \mathcal{M}_t$, its realization is a function $s_t(x_t) := r_t(x_t|m_t)$, where $r_t(\cdot)$ is an accrued distribution. This is a normalization of the results in \cite{james1994risk,  bernhard2003minimax}.

\textit{4) Systems with action dependent costs:} Consider a partially observed system where the cost has a form $c_t(U_t) \in \mathbb{R}_{\geq0}$ for all $t$. An information state for this system at each $t$ is the conditional range $S_t = [[X_t|M_t]] \in \mathcal{B}(\mathcal{X})$, where $\mathcal{B}(\mathcal{X})$ is the set of all subsets of $\mathcal{X}$. 

\begin{remark}
    The conditions in Definition \ref{def_information_state} help us identify information states for systems with known dynamics. However, often such a representation needs to be learned purely from observation and cost data, with incomplete knowledge of system dynamics. Thus, in the next section, we specialize the notion of information states to systems with observable costs and define approximate information states that can be learned from output data.
\end{remark}

\section{Systems with Observable Costs} \label{section:perfectly_observed}

In this section, we consider Problem \ref{problem_1} when the agent can observe the incurred cost at each instance of time. Thus, at each ${{t \in \mathbb{N}}}$, the agent receives a realization of $(Y_t, C_t)$ and the memory is $M_t = (Y_{0:t}, C_{0:t-1}, U_{0:t-1})$. We first prove that for such a system, the accrued distribution in Definition \ref{def_information_state} at each ${{t \in \mathbb{N}}}$ reduces to simply an indicator function.

\begin{lemma} \label{lemma_specialization}
    Consider Problem \ref{problem_1} with observable costs. At each ${{t \in \mathbb{N}}}$, for any given realizations $c_t \in \mathcal{C}$, $m_{t+1} \in \mathcal{M}_{t+1}$, $m_t \in \mathcal{M}_t$, and $u_t \in \mathcal{U}$, it holds that
    \begin{gather} \label{eq_lemma_specialization}
        r_t(c_t, m_{t+1}|m_t, u_t) = \mathbb{I}(c_t, m_{t+1}|m_t, u_t).
    \end{gather}
\end{lemma}

\begin{proof}
Let the given realization of the memory at time $t$ be $m_t = (\Tilde{y}_{0:t}, \Tilde{c}_{0:t-1}, \Tilde{u}_{0:t-1})$. Then, we simplify the terms in the definition of the accrued cost distribution at any $t$ as $r_t(c_t, m_{t+1}|m_t, u_t) 
= \sup_{a_t \in \mathcal{A}} \big( a_t + \mathbb{I}(a_t, c_t, m_{t+1}|m_t, u_t) \big)$ $- \sup_{a_t \in \mathcal{A}} \big( a_t + \mathbb{I}(a_t|m_t)\big)$ 
$= \sup_{a_t \in \mathcal{A}} \big( a_t + \mathbb{I}(a_t|m_t, u_t, c_t, m_{t+1}) + \mathbb{I}(c_t, m_{t+1}|m_t, u_t) \big) - \sum_{\ell=0}^{t-1} \gamma^{\ell} {\cdot} \Tilde{c}_{\ell}$ 
$= \sum_{\ell=0}^{t-1} \gamma^{\ell} {\cdot} \Tilde{c}_{\ell} + \mathbb{I}(c_t, m_{t+1}|m_t, u_t) - \sum_{\ell=0}^{t-1} \gamma^{\ell} {\cdot} \Tilde{c}_{\ell}$ $= \mathbb{I}(c_t, m_{t+1}|m_t, u_t)$, where, in the second equality, we note that the realization of $A_t$ is completely determined as $\tilde{a}_t = \sum_{\ell=0}^{t-1} \gamma^{\ell} {\cdot} \tilde{c}_{\ell}$ given $m_t$; and in the third equality, we note that $\mathbb{I}(a_t|m_t, u_t, c_t, m_{t+1}) = 0$ only if $a_t = \tilde{a}_t$.
%    \begin{multline}
%        r_t(c_t, m_{t+1}|m_t, u_t) = \sup_{a_t \in \mathcal{A}} \big( a_t + \mathbb{I}(a_t, c_t, m_{t+1}|m_t, u_t) \big) \\
%        - \sup_{a_t \in \mathcal{A}} \big( a_t + \mathbb{I}(a_t|m_t)\big)\\
%        = \sup_{a_t \in \mathcal{A}} \big( a_t + \mathbb{I}(a_t|m_t, u_t, c_t, m_{t+1}) \\
%        + \mathbb{I}(c_t, m_{t+1}|m_t, u_t) \big) - \sum_{\ell=0}^{t-1} \gamma^{\ell} {\cdot} \Tilde{c}_{\ell} \\
%        = \sum_{\ell=0}^{t-1} \gamma^{\ell} {\cdot} \Tilde{c}_{\ell} + \mathbb{I}(c_t, m_{t+1}|m_t, u_t) - \sum_{\ell=0}^{t-1} \gamma^{\ell} {\cdot} \Tilde{c}_{\ell} \\
%        = \mathbb{I}(c_t, m_{t+1}|m_t, u_t).
%    \end{multline}
\end{proof}

Motivated by Lemma \ref{lemma_specialization}, we present a simpler definition for information states in systems with observable costs.

\begin{definition} \label{def_info_specialized}
    An \textit{information state} for Problem \ref{problem_1} with observable costs at any ${{t \in \mathbb{N}}}$ is an uncertain variable $\bar{S}_t = \bar{\sigma}_t(M_t)$ taking values in a bounded, time-invariant set $\bar{\mathcal{S}}$. For all $t \in \mathbb{N}$, for all $m_t \in \mathcal{M}_t$ and $u_t \in \mathcal{U}_t$, it satisfies:
    \begin{gather} \label{eq_def_info_specialized}
        [[C_t, \bar{S}_{t+1}~|~m_t, u_t]] = [[C_t, \bar{S}_{t+1}~|~\bar{\sigma}_t(m_t), u_t]]. 
    \end{gather}
\end{definition}

Next, we use the information state from Definition \ref{def_info_specialized} to construct a time-invariant operator $\bar{\mathcal{T}} : [\bar{\mathcal{S}} \to \mathbb{R}] \to [\bar{\mathcal{S}} \to \mathbb{R}]$, such that, for any uniformly bounded function $\bar{\Lambda}: \bar{\mathcal{S}} \to \mathbb{R}$,
    \begin{gather} \label{eq_info_value_operator}
        [\bar{\mathcal{T}} \bar{\Lambda}](\bar{s}) := \inf_{u \in \mathcal{U}} \sup_{c, \bar{s}' \in [[C, \bar{S}'|\bar{s}, u]]} \big(c + \gamma {\cdot} \bar{\Lambda}(\bar{s}') \big).
    \end{gather}
The discounting in the RHS of \eqref{eq_info_value_operator} ensures that $\bar{\mathcal{T}}$ is a contraction mapping and thus, $\bar{\Lambda} = \bar{\mathcal{T}} \bar{\Lambda}$ admits a unique solution $\bar{\Lambda}^{\infty} = \bar{\mathcal{T}} \bar{\Lambda}^{\infty}$. Starting with $\bar{\Lambda}^0(\bar{s}) := 0$, the fixed-point iteration around $\bar{\mathcal{T}}$ generates a sequence of functions
    \begin{gather} \label{eq_specialized_iteration}
        \bar{\Lambda}^{n+1}(\bar{s}) = [\bar{\mathcal{T}} \bar{\Lambda}^n](\bar{s}) = [\bar{\mathcal{T}}^n \bar{\Lambda}^0](\bar{s}),
    \end{gather}
for all $n=1,2,\dots$, such that $\lim_{n \to \infty} \bar{\mathcal{T}}^n \bar{\Lambda}^0 = \bar{\Lambda}^{\infty}$. Next, we establish error bounds for using $\bar{\Lambda}^n(\bar{\sigma}_t(m_t))$, ${{n \in \mathbb{N}}}$, to estimate $V_t(m_t)$ for all $t$ in Problem \ref{problem_1} with observable costs.

\begin{theorem} \label{thm_specialized_bounds}
    Consider the function $\bar{\Lambda}^n$ generated using \eqref{eq_specialized_iteration} for any $n \in \mathbb{N}$. Then, for all ${{t \in \mathbb{N}}}$, it holds that:
    \begin{multline} \label{eq_thm_specialized_bounds}
        \frac{\gamma^{n+t}{\cdot} c^{\min}}{1-\gamma} + \gamma^t {\cdot} \bar{\Lambda}^{{{n}}}(\bar{\sigma}_t(m_t)) + \sup_{a_t \in [[A_t|m_t]]}a_t  \; \leq \; V_t(m_t) \\ 
        \leq \sup_{a_t \in [[A_t|m_t]]}a_t + \gamma^t {\cdot} \bar{\Lambda}^{{{n}}}(\bar{\sigma}_t(m_t))  + \frac{\gamma^{n+t} {\cdot} c^{\max}}{1-\gamma}.
    \end{multline}
\end{theorem}

\begin{proof}
    We show \eqref{eq_thm_specialized_bounds} by combining the arguments in Theorem \ref{thm_g_opt} with \eqref{eq_lemma_specialization} from Lemma \ref{lemma_specialization}. Thus, we first show that for any horizon $T \in \mathbb{N}$, we have for all $t=0,\dots,T$:
    \begin{gather} \label{second_interim}
        J_{t}(m_t; T) = \gamma^t {\cdot} \bar{\Lambda}^{T-t+1}\big(\bar{\sigma}_t(m_t)\big) + \sup_{a_t \in [[A_t|m_t]]}a_t.
    \end{gather}
    We can prove \eqref{second_interim} by induction. At time $T$, using the definition of the finite-time function $J_T(m_T; T) = \inf_{u_T \in \mathcal{U}} \sup_{a_T, c_T \in [[A_T, C_T|m_T, u_T]]}(a_T + \gamma^T {\cdot} c_T) = \inf_{u_T \in \mathcal{U}} \sup_{c_T \in [[C_T|m_T, u_T]]}c_T + \sup_{a_T \in [[A_T|m_T]]} a_T = \inf_{u_T \in \mathcal{U}} \sup_{c_T, \bar{\sigma}_{T+1}(m_{T+1}) \in [[C_T, S_{T+1}|m_T, u_T]]}(c_T + \gamma^T {\cdot} \bar{\Lambda}^0(\bar{\sigma}_{T+1}(m_{T+1}))) + \sup_{a_T \in [[A_T|m_T]]} a_T = \bar{\Lambda}^1(\sigma_T(m_T)) + \sup_{a_T \in [[A_T|m_T]]} a_T$, where, in the second equality, we use the fact that the realization of $A_T$ is completely determined given $M_T$ as in Lemma \ref{lemma_specialization}; in the third equality, we note that $\bar{\Lambda}^0(\bar{\sigma}_{T+1}(m_{T+1})) = 0$ identically; and in the fourth equality, we use the definition of $\bar{\Lambda}^1$. 
    This forms the basis of our induction. Next, consider the hypothesis that \eqref{second_interim} holds at time $t+1$. Using the definition of the finite-time function at time $t$, $J_t(m_t; T) = \inf_{u_t \in \mathcal{U}} \sup_{m_{t+1} \in [[M_{t+1}|m_t, u_t]]}\sup_{c_t, a_{t}\in [[C_t, A_{t}|m_{t+1}]]}(\gamma^{t+1} {\cdot}$ $ \bar{\Lambda}^{T-t}(\bar{\sigma}_{t+1}(m_{t+1})) + \gamma^t {\cdot} c_t + a_t) = \inf_{u_t \in \mathcal{U}}$ $ \sup_{c_t, a_t, m_{t+1} \in [[C_t, A_t, M_{t+1}|m_t, u_t]]}(\gamma^{t+1} {\cdot} \bar{\Lambda}^{T-t}(\bar{\sigma}_{t+1}(m_{t+1}))$ $ + \gamma^t {\cdot} c_t + a_t) = \inf_{u_t \in \mathcal{U}}\sup_{c_t, \bar{\sigma}_{t+1}(m_{t+1}) \in [[C_t, \bar{S}_{t+1}|m_t, u_t]]}$ $(\gamma^{t+1} {\cdot} \bar{\Lambda}^{T-t}(\bar{\sigma}_{t+1}(m_{t+1})) +\gamma^t{\cdot}c_t ) + \sup_{a_t \in [[A_t|m_t]]}a_t$ $= \inf_{u_t \in \mathcal{U}}\sup_{c_t, \bar{\sigma}_{t+1}(m_{t+1}) \in [[C_t, \bar{S}_{t+1}|\bar{\sigma}_t(m_t), u_t]]}(\gamma^{t+1} {\cdot} \bar{\Lambda}^{T-t}($ $\bar{\sigma}_{t+1}(m_{t+1})) + \gamma^t {\cdot} c_t ) + \sup_{a_t \in [[A_t|m_t]]}a_t = \gamma^t \cdot \bar{\Lambda}^{T-t+1}(\bar{\sigma}_{t}(m_t)) + \sup_{a_t \in [[A_t|m_t]]}a_t$, where, in the third equality, we use the same arguments as in Lemma \ref{lemma_specialization}; in the fourth equality, we use \eqref{eq_def_info_specialized} from Definition \ref{def_info_specialized}; and in the last equality, we use the definition of $\bar{\Lambda}^{T-t+1}$ from \eqref{eq_specialized_iteration}. This proves the \eqref{second_interim} using induction.
    Then, \eqref{eq_thm_specialized_bounds} follows directly for all ${{t \in \mathbb{N}}}$ and all ${{n \in \mathbb{N}}}$ by substituting \eqref{second_interim} into \eqref{eq_finite_DP_memory_opt} and setting $T = t + n -1$.
\end{proof}

As a direct consequence of Theorem \ref{thm_specialized_bounds}, we select $t=0$ in \eqref{eq_thm_specialized_bounds} and let $n \to \infty$ to establish that 
\begin{gather} \label{eq_optimality_of_specialized}
    \bar{\Lambda}^{\infty}(\bar{\sigma}_0(y_0)) = V_0(y_0).
\end{gather}
Thus, the fixed point $\bar{\Lambda}^{\infty}$ computes the optimal value function $V_0$ for Problem \ref{problem_1} with observable costs. Next, consider that the infimum is achieved in the RHS of $\bar{\mathcal{T}}\bar{\Lambda}^{n}$ for all ${{n \in \mathbb{N}}}$. We define a time-invariant control strategy $\boldsymbol{\pi}^* = (\pi^*, \pi^*, \dots)$, where $\bar{\pi}^*: \bar{\mathcal{S}} \to \mathcal{U}$ is the minimizing argument in RHS of $\bar{\mathcal{T}}\bar{\Lambda}^{\infty}$ expanded using \eqref{eq_info_value_operator}. Then, from the same arguments as in Appendix B, it holds that the memory-based strategy $\bar{\boldsymbol{g}}^* = (\bar{g}_0^*, \bar{g}_1^*, \dots)$, where $\bar{g}_t^* := \bar{\pi}^*(\sigma_t(m_t))$, is an optimal solution to Problem \ref{problem_1} with observable costs.

\begin{remark}
    Consider a partially observed system with observable costs that has the state-space model described in Subsection \ref{subsection:info_state_examples}. An information state at each $t$ is the conditional range $\bar{S}_t = [[X_t|M_t]] \in \mathcal{B}(\mathcal{X})$. This is simpler than the accrued cost function in Subsection \ref{subsection:info_state_examples}.
\end{remark}

\begin{remark}
    When attempting to learn an information state that satisfies Definition \ref{def_info_specialized} using only output data, we may not be able to satisfy \eqref{eq_def_info_specialized} exactly. Thus, in Subsection \ref{subsection:approx}, we relax this definition for approximate information states.
\end{remark}

\subsection{Approximate Information States} \label{subsection:approx}

In this subsection, we define approximate information states by relaxing the condition \eqref{eq_def_info_specialized} and construct a corresponding time-invariant approximate dynamic programming decomposition of Problem \ref{problem_1}. Then, we bound the error in the computation of the approximate value and bound the performance loss in the resulting approximate strategy.

\begin{definition} \label{def_approximate_info}
    An \textit{approximate information state} for Problem \ref{problem_1} with observable costs at any ${{t \in \mathbb{N}}}$ is an uncertain variable $\hat{S}_t = \hat{\sigma}_t(M_t)$ taking values in a bounded, time-invariant set $\hat{\mathcal{S}}$. Furthermore, for all $t$, there exists a parameter $\epsilon \in \mathbb{R}_{\geq0}$ such that it satisfies for all $m_t \in \mathcal{M}_t$ and $u_t \in \mathcal{U}$:
    \begin{gather} \label{eq_def_approximate_info}
        \mathcal{H}\big( [[C_t, \hat{S}_{t+1}|m_t, u_t]], [[C_t, \hat{S}_{t+1}|\hat{\sigma}_t(m_t), u_t]] \big) \leq \epsilon,
    \end{gather}
    where recall that $\mathcal{H}$ is the Hausdorff distance defined in \eqref{H_met_def}.
\end{definition}

Using the approximate information state from Definition \ref{def_approximate_info}, we construct a time-invariant operator $\hat{\mathcal{T}}: [\hat{\mathcal{S}} \to \mathbb{R}] \to [\hat{\mathcal{S}} \to \mathbb{R}]$, such that for any uniformly bounded function $\hat{\Lambda}: \hat{\mathcal{S}} \to \mathbb{R}$:
\begin{multline} \label{eq_DP_approx}
    [\hat{\mathcal{T}} \hat{\Lambda}](\hat{s}) := \inf_{u \in \mathcal{U}} \sup_{c, \hat{s}' \in [[C, \hat{S}'|\hat{s}, u]]}\big(c + \gamma {\cdot} \hat{\Lambda}(\hat{s}')\big).
\end{multline}
Due to discounting in the RHS of \eqref{eq_DP_approx}, $\hat{\mathcal{T}}$ is a contraction mapping and thus, the equation $\hat{\Lambda} = \hat{\mathcal{T}} \hat{\Lambda}$ admits a unique solution $\hat{\Lambda}^{\infty} = \hat{\mathcal{T}} \bar{\Lambda}^{\infty}$. Starting with $\hat{\Lambda}^0(\hat{s}):= 0$ the fixed-point iteration around $\hat{\mathcal{T}}$ recursively generates the functions
    \begin{gather} \label{eq_approx_iteration}
        \hat{\Lambda}^{n+1}(\hat{s}) = [\hat{\mathcal{T}} \hat{\Lambda}^n](\hat{s}) = [\hat{\mathcal{T}}^n \hat{\Lambda}^0](\hat{s}),
    \end{gather}
for all $n=1,2,\dots$, such that $\lim_{n \to \infty} \hat{\mathcal{T}}^n \hat{\Lambda}^0 = \hat{\Lambda}^{\infty}$. Furthermore, consider that the minimum is achieved in the RHS of $[\hat{\mathcal{T}} \bar{\Lambda}^{n}](s)$ for all $s \in \mathcal{S}$ and all $n$. We define a time-invariant approximate strategy  $\hat{\boldsymbol{\pi}}^* = (\hat{\pi}^*, \hat{\pi}^*, \dots)$, where $\hat{\pi}^*: \hat{S} \to \mathcal{U}$ is the minimizing argument in the RHS of $\hat{\mathcal{T}}\hat{\Lambda}^{\infty}$ expanded using \eqref{eq_DP_approx}. Then, a corresponding memory-based strategy is $\hat{\boldsymbol{g}}^* := (\hat{g}_0^*, \hat{g}_1^*, \dots)$ with $\hat{g}_t^* := \pi^*(\sigma_t(m_t))$ for all $t\in\mathbb{N}$. Next, we bound the approximation error between the optimal value $V_0(y_0)$ and $\hat{\Lambda}^{\infty}(\hat{\sigma}_0(y_0))$, and the performance loss when control actions are generated using $\hat{\boldsymbol{g}}^*$.

%After bounding the value approximation error, we also seek to bound the maximum performance loss in the implementation of an approximately optimal strategy. Consider an approximate strategy $\boldsymbol{\pi}^* = (\pi^*, \pi^*, \dots)$, where $\pi^*$ is the argument that minimizes the RHS in \eqref{eq_DP_approx}. Then, a corresponding control strategy is $\boldsymbol{g}^* = (g^*_0, g^*_1, \dots)$ such that $g^*_t(m_t) = \pi^*(\hat{\sigma}_t(m_t))$. 

\begin{theorem} \label{thm_approx}
    Let the functions $\hat{\Lambda}^n$ be Lipschitz continuous for all ${{n \in \mathbb{N}}}$ with constant $L_{\hat{\Lambda}} \in \mathbb{R}_{\geq0}$. Then, we have:
    \begin{align} \label{eq_thm_approx}
        \textbf{a) } \; &|V_0(y_0) - \hat{\Lambda}^{\infty}(\hat{\sigma}_0(y_0))| \leq \frac{\hat{L} {\cdot} \epsilon}{1-\gamma}, \\
        \textbf{b) } \; &|V_0(y_0) - V_0^{\hat{\boldsymbol{g}}^*}(y_0)| \leq \dfrac{2{\cdot}\hat{L}{\cdot}\epsilon}{1-\gamma}, \label{eq_thm_approx_g}
    \end{align}
    where $\hat{L} = \max\{\gamma {\cdot} L_{\hat{\Lambda}},1\}$. 
\end{theorem}

\begin{proof}
We show \eqref{eq_thm_approx} using \eqref{eq_finite_DP_memory_opt} from Lemma \ref{finite_DP_memory}. For this purpose, we first show that for any finite horizon $T \in \mathbb{N}$, for all $t=0,\dots,T$, it holds that
    \begin{gather} \label{eq_interim_third}
         \hspace{-5pt} |J_{t}(m_t; \hspace{-1pt} T) \hspace{-2pt} - \hspace{-1pt} \gamma^t {\cdot} \hat{\Lambda}^{T-t+1} \hspace{-1pt} \big(\hat{\sigma}_t(m_t)\big) \hspace{-2pt}  - \hspace{-7pt} \sup_{a_t \in [[A_t|m_t]]} \hspace{-7pt} a_t| \hspace{-1pt} \leq \hspace{-1pt} \beta_{t}(T), \hspace{-2pt}
    \end{gather}
where $\beta_{t}(T) = \beta_{t+1}(T) + \gamma^t {\cdot} \hat{L} {\cdot} \epsilon$ and $\beta_{T}(T) = \gamma^T {\cdot} \hat{L} {\cdot} \epsilon$. with $\hat{L} = \max\{\gamma {\cdot} L_{\hat{\Lambda}},1\}$. We can prove \eqref{eq_interim_third} by reverse mathematical induction. At time $T$, recall that $J_{T}(m_T; T) = \inf_{u_T \in \mathcal{U}}\sup_{c_T \in [[C_T|m_T, u_T]]} \gamma^T {\cdot}c_T + \sup_{a_T \in [[A_T|m_T]]} a_T$ using the same arguments as in Lemma \ref{lemma_specialization}. This implies that $|J_{T}(m_T; T)- \gamma^T {\cdot} \hat{\Lambda}^1\big(\hat{\sigma}_T(m_T)\big) - \sup_{a_T \in [[A_T|m_T]]} a_T| 
= |\inf_{u_T \in \mathcal{U}} \sup_{c_T \in [[C_T|m_T, u_T]]} \gamma^T {\cdot}c_T - \gamma^T {\cdot} \hat{\Lambda}^1\big(\hat{\sigma}_T(m_T)\big)| 
= \gamma^T {\cdot} |\inf_{u_T \in \mathcal{U}} \sup_{c_T, \hat{s}_{T+1} \in [[C_T, \hat{S}_{T+1}|m_T, u_T]]} (c_T + \gamma {\cdot} \hat{\Lambda}^0(\hat{s}_{T+1}))- \inf_{u_T \in \mathcal{U}} \sup_{c_T, \hat{s}_{T+1} \in [[C_T, \hat{S}_{T+1}|\hat{\sigma}_T(m_T), u_T]]}$ 
$(c_T + \gamma {\cdot} \hat{\Lambda}^0 \hspace{-1pt} (\hat{s}_{T+1} \hspace{-1pt}) \hspace{-1pt}) \hspace{-1pt}| \hspace{-1pt} \leq \hspace{-1pt} \gamma^T {\cdot} \hat{L} {\cdot} \sup_{u_T \in \mathcal{U}} \hspace{-2pt} \mathcal{H}([[C_T, \hat{S}_{T+1}|m_T, u_T]],$ $ [[C_T, \hat{S}_{T+1}|\hat{\sigma}_T(m_T), u_T]])
\leq \gamma^T {\cdot} \hat{L} {\cdot} \epsilon$, where, in the second equality, we use \eqref{eq_approx_iteration} and the fact that $\Lambda^0(\hat{s}_{T+1}) = 0$ identically; in the first inequality, we note that $\hat{L} = \max\{\gamma {\cdot} L_{\hat{\Lambda}}, 1\}$ is the Lipschitz constant of $(c_T + \gamma {\cdot} \hat{\Lambda}^0(\hat{s}_{T+1}) )$ with respect to $(c_T, \hat{s}_{T+1})$ and use \eqref{H_met_property}; and in the second inequality, we use \eqref{eq_def_approximate_info} from Definition \ref{def_approximate_info}. This forms the basis of our induction. Next, we consider the induction hypothesis that \eqref{eq_thm_approx} holds at time $t+1$. Using the hypothesis and rearranging terms, it holds that
$J_{t+1}(m_{t+1};T) \leq \beta_{t+1}(T) + \gamma^{t+1} {\cdot} \hat{\Lambda}^{T-t}(\hat{\sigma}_{t+1}(m_{t+1})) + \sup_{a_{t+1} \in [[A_{t+1}|m_{t+1}]]} a_{t+1}$. Then, at time $t$ we use the definition of the finite time functions and the induction hypothesis to write that $|J_t(m_t; T) - \gamma^t {\cdot} \hat{\Lambda}^{T-t+1}\big(\hat{\sigma}_{t}(m_t)\big) - \sup_{a_t \in [[A_t|m_t]]} a_t| \leq\beta_{t+1}(T) + |\inf_{u_t \in \mathcal{U}} \sup_{m_{t+1} \in [[M_{t+1}|m_t, u_t]]}(\gamma^{t+1} {\cdot} \hat{\Lambda}^{T-t}(\hat{\sigma}_{t+1}(m_{t+1}))$ $ + \sup_{a_t, c_t \in [[A_t, C_t|m_{t+1}]]}(a_t + \gamma^t {\cdot} c_t) ) - \gamma^t {\cdot} \hat{\Lambda}^{T-t+1}(\hat{\sigma}_{t}($ $m_t)) - \sup_{a_t \in [[A_t|m_t]]}a_t | \leq \beta_{t+1}(T) + \gamma^t {\cdot} \sup_{u_t \in \mathcal{U}}$ 
$|\sup_{c_t, \hat{\sigma}_{t+1}(m_t) \in [[C_t, \hat{S}_{t+1}|m_t, u_t]]} (c_t + \gamma {\cdot} \hat{\Lambda}^{T-t}(\hat{\sigma}_{t+1}(m_{t+1})))$ $- \sup_{c_t, \hat{s}_{t+1} \in [[C_t, \hat{S}_{t+1}|\hat{\sigma}_t(m_t), u_t]]}(c_t + \gamma {\cdot} \hat{\Lambda}^{T-t} (\hat{s}_{t+1}))| \leq \beta_{t+1}(T) + \gamma^t {\cdot} \hat{L} {\cdot} \epsilon$, where, in the second inequality, we use the same arguments as in Lemma \ref{lemma_specialization} to write that $[[A_t, C_t|m_{t+1}]] = [[A_t|m_{t+1}]] \times [[C_t|m_{t+1}]]$ and subsequently use \eqref{eq_approx_iteration}; and in the third inequality we use \eqref{H_met_property} and \eqref{eq_def_approximate_info} from Definition \ref{def_approximate_info}. This proves \eqref{eq_interim_third} for all $t$ using mathematical induction. 

Next, for the iterated function $\hat{\Lambda}^n$, we select a horizon $T = n-1$ and set $t=0$ in \eqref{eq_interim_third}, to write that
\begin{gather}
    |J_0(y_0; T) - \hat{\Lambda}^n(\hat{\sigma}_0(y_0))| \leq \beta_0(T),    
\end{gather}
where $\beta_0(T) = \sum_{\ell=0}^{n-1} \gamma^{\ell} {\cdot} \hat{L} {\cdot} \epsilon$. As $n \to \infty$ with $T=n-1$, note that $\lim_{T\to \infty} J_0(y_0; T) = V_0(y_0)$, $\lim_{n \to \infty} \hat{\Lambda}^n(\hat{\sigma}_0(y_0)) = \hat{\Lambda}^\infty(\hat{\sigma}_0(y_0))$, and $\lim_{T \to \infty} \beta_0(T) = \frac{\hat{L} {\cdot} \epsilon}{1-\gamma}$. This completes the proof for \eqref{eq_thm_approx}. The proof for \eqref{eq_thm_approx_g} follows from a similar series of arguments.
\end{proof}

\subsection{Alternate Characterization} \label{subsection:alternate}

When exploring whether an uncertain variable is a valid candidate to be considered an approximate information state, it may be difficult to verify \eqref{eq_def_approximate_info} in Definition \ref{def_approximate_info}. In this subsection, we present two \textit{stronger} conditions which are easier to verify and sufficient to establish \eqref{eq_def_approximate_info} (see Appendix C). At each $t \in \mathbb{N}$, to establish that $\hat{S}_t = \hat{\sigma}_t(M_t)$ is a valid information state, it is sufficient to satisfy:

\textit{1) State-like evolution:} There exists a Lipschitz continuous function $\hat{f}:\hat{\mathcal{S}} \times \mathcal{U} \times \mathcal{Y} \to \mathcal{S}$, such that
\begin{gather}
    \hat{\sigma}_{t+1}(M_{t+1}) = \hat{f}(\hat{\sigma}_t(M_t), U_t, Y_{t+1}). \label{ap2a}
\end{gather}

\textit{2) Sufficient to approximate outputs:} For all $m_t \in \mathcal{M}_t$ and $u_t \in \mathcal{U}$, there exists a constant $\delta \in \mathbb{R}_{\geq0}$ such that
\begin{gather}
    \mathcal{H}([[C_t, Y_{t+1}|m_t, u_t]]], [[C_t, Y_{t+1}|\hat{\sigma}_{t}(m_t), u_t]]) \leq \delta. \label{ap2b}
\end{gather}

%\section{Numerical Example} \label{section:example}

\section{Conclusions} \label{section:conclusion}

In this paper, we defined a general notion of information states for worst-case decision-making problems over an infinite time-horizon without a known state-space model. We showed that these information states yield a time-invariant DP decomposition to compute the optimal control strategy, and that many information states for systems with known state-space models are special cases of our general definition. Then, we specialized the notion of information states and extended it to define approximate information states for problems with observable costs. We proved that approximate information states yield a time-invariant DP decomposition that computes an approximate control strategy and the resulting loss in worst-case performance is bounded. Future work should consider the application of these results in worst-case reinforcement learning problems.

%and approximate information states to tractably compute control strategies in non-stochastic additive cost problems. We used the theoretical framework of cost distributions to present a general definition for information states that compute an optimal control strategy. We showed that specific information states proposed in previous research efforts emerge as special cases of our definition. Then, we extended this definition to approximate information states which can be used to compute approximate control strategies which admit a bounded worst-case performance loss. Finally, using a numerical simulation, we illustrated the trade-off between computational tractability and performance loss inherent in the application of approximate information states. Future work should consider the use of this theory in non-stochastic reinforcement learning problems.


\bibliographystyle{ieeetr}
\bibliography{References,Latest_IDS}

\section*{Appendix A - Proof that the Dynamic Programming Operator is a Contraction Mapping}

In this appendix, we prove that the operator $\mathcal{T}$ defined in \eqref{eq_general_value_operator} is a contraction mapping.

\begin{lemma}
    Consider the operator $\mathcal{T}$ defined in \eqref{eq_general_value_operator}. There exists a constant $\alpha \in [0,1)$ such that for two functions $\Lambda:\mathcal{S} \times (0,1] \to \mathbb{R}$ and $\tilde{\Lambda}:\mathcal{S} \times (0,1] \to \mathbb{R}$:
    \begin{gather} \label{eq_contraction_map}
        ||\mathcal{T}\Lambda - \mathcal{T}\tilde{\Lambda}||_{\infty} \leq \alpha {\cdot} ||\Lambda - \tilde{\Lambda}||_{\infty}.
    \end{gather}    
\end{lemma}

\begin{proof}
    Using the definition of $\mathcal{T}$, we expand the left hand side (LHS) of \eqref{eq_contraction_map} as $||\mathcal{T}\Lambda - \mathcal{T}\tilde{\Lambda}||_{\infty} 
        = \sup_{s \in \mathcal{S}, z \in (0,1]}$ $\big|\inf_{u \in \mathcal{U}} \sup_{s' \in \mathcal{S}, c \in \mathcal{C}}( c + \gamma {\cdot} \Lambda(s', \gamma {\cdot} z) + \frac{\rho(c, s' ~|~ s, u)}{z})$
        $- \inf_{u \in \mathcal{U}} \sup_{s' \in \mathcal{S}, c \in \mathcal{C}}( c + \gamma {\cdot} \tilde{\Lambda}(s', \gamma {\cdot} z) + \frac{\rho(c, s' ~|~ s, u)}{z})\big|$
        $\leq \gamma {\cdot} \sup_{s' \in \mathcal{S}, \gamma {\cdot} z \in (0,\gamma]} |\Lambda(s', \gamma {\cdot} z) - \tilde{\Lambda}(s', \gamma {\cdot} z) | \leq \gamma {\cdot} \sup_{s \in \mathcal{S}, z \in (0,1]} |\Lambda(s, z) - \tilde{\Lambda}(s, z)|$
        $= \gamma {\cdot} ||\Lambda - \tilde{\Lambda}||_{\infty}$,
    where, in the first inequality, we upper bound the difference between supremum values of two functions by the supremum difference between the two functions; and in the second inequality, we use $(0, \gamma] \subset (0,1]$ in the argument of the supremum. This proves that the operator $\mathcal{T}$ is a contraction mapping by setting $\alpha = \gamma$.
\end{proof}

\section*{Appendix B - Proof that Information States Yield an Optimal Control Strategy}

In this appendix, we prove that the information-state based control strategy $\boldsymbol{\pi}^* = (\pi^*, \pi^*, \dots)$ and corresponding memory-based control strategy $\boldsymbol{g}^* = (g_0^*, g_1^*, \dots)$ defined in Subsection \ref{subsection:basic_info_states} are optimal solutions to Problem \ref{problem_1}. Recall from Subsection \ref{subsection:basic_info_states} that the information-based control law is $\pi^*(s, z) = \arg \min_{u \in \mathcal{U}} \sup_{c \in \mathcal{C}, s' \in \mathcal{S}}(c + \gamma {\cdot} \Lambda^{\infty}(s', \gamma {\cdot}z) + \frac{\rho(c, s'|s, u)}{z})$ for all $s \in \mathcal{S}$ and $z \in (0,1]$. Furthermore, recall that the memory-based control law is $g_t^*(m_t) = \pi^*(\sigma_t(m_t), \gamma^t)$ for all $m_t \in \mathcal{M}_t$ and $t \in \mathbb{N}$.
To begin, for any time-invariant control law $\pi: \mathcal{S} \times (0,1] \to \mathcal{U}$, we define a law-dependent operator $\mathcal{T}(\pi):[\mathcal{S} \times (0,1] \to \mathbb{R}] \to [\mathcal{S} \times (0,1] \to \mathbb{R}]$, such that for any uniformly bounded function $\Lambda:\mathcal{S} \times (0,1] \to \mathbb{R}$, we have:
\begin{multline} \label{eq_pi_dependent_operator}
    [\mathcal{T}(\pi) \Lambda](s, z) \\
    := \sup_{c \in \mathcal{C}, s' \in \mathcal{S}} \Bigg(c + \gamma {\cdot} \Lambda (s', \gamma {\cdot} z)
    + \frac{\rho(c, s'|s, \pi(s,z) )}{z} \Bigg). 
\end{multline}
Note that the control action in the RHS of \eqref{eq_pi_dependent_operator} is selected using as $u = \pi(s, z)$. Furthermore, by definition of $\pi^*$ it holds that for all $s \in \mathcal{S}$ and $z \in (0,1]$:
\begin{gather} \label{eq_appendix_b_pi_star}
    [\mathcal{T}(\pi^*) \Lambda^{\infty}](s, z) = [\mathcal{T} \Lambda^{\infty}](s,z) = \Lambda^{\infty}(s,z).
\end{gather}
Then, we can construct a corresponding memory-based control law at each ${{t \in \mathbb{N}}}$ as $g_t(m_t) := \pi(\sigma_t(m_t), \gamma^t)$ and a memory-based control strategy $\boldsymbol{g} = (g_0, g_1, \dots)$. Next, we establish that we can use $[\mathcal{T}(\pi)^n \Lambda^0](\sigma_t(m_t), \gamma^t)$ for any ${{n \in \mathbb{N}}}$ to estimate the strategy-dependent value $V^{\boldsymbol{g}}_t(m_t)$ at each ${{t \in \mathbb{N}}}$.

\begin{lemma} \label{lem_appendix_b}
    For all ${{t \in \mathbb{N}}}$ and all ${{n \in \mathbb{N}}}$, it holds that:
    \begin{multline} \label{eq_lem_appendix_b}
        \frac{\gamma^{n+t}{\cdot} c^{\min}}{1-\gamma} + \gamma^t {\cdot} [\mathcal{T}(\pi)^n \Lambda^0](\sigma_t(m_t), \gamma^t) + \sup_{a_t \in [[A_t|m_t]]}a_t \\
        \leq V^{\boldsymbol{g}}_t(m_t) \\ 
        \leq \sup_{a_t \in [[A_t|m_t]]}a_t + \gamma^t {\cdot} [\mathcal{T}(\pi)^n \Lambda^0](\sigma_t(m_t), \gamma^t)  + \frac{\gamma^{n+t} {\cdot} c^{\max}}{1-\gamma}.
    \end{multline}
\end{lemma}

\begin{proof}
    The proof follows using the same sequence of arguments as the proof for Theorem \ref{thm_g_opt}, but by using \eqref{eq_finite_DP_memory_g} from Lemma \ref{finite_DP_memory} along with $u_t = g_t(m_t) := \pi(\sigma_t(m_t), \gamma^t)$.
\end{proof}

We can set $t=0$ in \eqref{eq_lem_appendix_b} and note that
\begin{multline}
    \frac{\gamma^{n}{\cdot} c^{\min}}{1-\gamma} + [\mathcal{T}(\pi)^n \Lambda^0](\sigma_0(y_0), 1) \; \leq \; V^{\boldsymbol{g}}_t(m_t) \\ 
    \leq [\mathcal{T}(\pi)^n \Lambda^0](\sigma_t(m_t), 1) + \frac{\gamma^{n} {\cdot} c^{\max}}{1-\gamma},
\end{multline}
where recall that $A_0 = 0$ always. Thus, as a direct consequence of Lemma \ref{lem_appendix_b}, it holds that
$[\lim_{n \to \infty} \mathcal{T}(\pi)^n] \Lambda^0(\sigma_0(y_0), 1) = V_0^{\boldsymbol{g}}(y_0)$.
Next, we prove that $\lim_{n \to \infty}\mathcal{T}(\pi^*)^n = \Lambda^{\infty}$. 

\begin{lemma} \label{lem_appendix_b_2}
    For all $s \in \mathcal{S}$ and $z \in (0,1]$, it holds that
    \begin{gather} \label{eq_appendex_b_2}
        [\lim_{n \to \infty}\mathcal{T}(\pi^*)^n](s,z) = \Lambda^{\infty}(s,z).
    \end{gather}
\end{lemma}

\begin{proof}
    We begin by showing that the LHS of \eqref{eq_appendex_b_2} forms an upper bound on the RHS. By definition, note that $[\mathcal{T}(\pi)\Lambda](s, z) \geq [\mathcal{T}\Lambda](s, z)$ for any control law $\pi$ and for all $s \in \mathcal{S}$, $z \in (0,1]$. Taking the limit on both sides with $\pi = \pi^*$, this implies for all $s \in \mathcal{S}$, $z \in (0,1]$ that
    \begin{gather} \label{eq_b_2_1}
        [\lim_{n \to \infty} \mathcal{T}(\pi^*)^n \Lambda^{{{0}}}](s,z) \geq [\lim_{n \to \infty}\mathcal{T}^n \Lambda^{{{0}}}](s,z) = \Lambda^{\infty}(s,z).
    \end{gather} 
    Next, we prove that the LHS of  \eqref{eq_appendex_b_2} also forms a lower bound on the RHS. From \eqref{eq_appendix_b_pi_star}, it holds that $\Lambda^{\infty} = \mathcal{T}(\pi^*) \Lambda^{\infty} = \lim_{n \to \infty} \mathcal{T}(\pi^*)^n \Lambda^{\infty}$. Then, using $\Lambda^\infty \geq \Lambda^0 = 0$, we write for all $s \in \mathcal{S}$ and $z \in (0,1]$ that
    \begin{gather} \label{eq_b_2_2}
        \Lambda^{\infty}(s,z) = \lim_{n \to \infty} \mathcal{T}(\pi^*)^n \Lambda^{\infty} \geq [\lim_{n \to \infty} \mathcal{T}(\pi^*)^n \Lambda^{{{0}}}](s,z).
    \end{gather}
    Using \eqref{eq_b_2_1} and \eqref{eq_b_2_2} simultaneously establishes \eqref{eq_appendex_b_2}.
\end{proof}

Then, as a direct consequence of both Lemmas \ref{lem_appendix_b} and \ref{lem_appendix_b_2}, we can conclude that
\begin{multline}
    V_t^{\boldsymbol{g}^*}(y_0) = [\lim_{n \to \infty} \mathcal{T}(\pi)^n] \Lambda^0(\sigma_0(y_0), 1) \\
    = \Lambda^{\infty}(\sigma_0(y_0), 1) = V_0(y_0),
\end{multline}
where, in the last equality we use \eqref{eq_optimality_of_infor_state}. This proves that the control strategies $\boldsymbol{g}^*$ and $\boldsymbol{\pi}^*$ are optimal solutions to Problem \ref{problem_1}.

\section*{Appendix C - Proof that the Alternate Characterization Defines an Approximate Information State}

In this appendix, we prove that the alternate characterization presented in Subsection \ref{subsection:alternate} using the properties \eqref{ap2a} and \eqref{ap2b} is sufficient to establish \eqref{eq_def_approximate_info} in Definition \ref{def_approximate_info}.

\begin{lemma}
    For all ${{t \in \mathbb{N}}}$, if an uncertain variable $\hat{S}_t = \hat{\sigma}_t(M_t)$ satisfies \eqref{ap2a} - \eqref{ap2b}, it also satisfies \eqref{eq_def_approximate_info}.
\end{lemma}

\begin{proof}
Let $m_t \in \mathcal{M}_t$ be a given realization of $M_t$ and let $\hat{s}_t = \hat{\sigma}_t(s_t)$ satisfy \eqref{ap2a} - \eqref{ap2b}, for all $t$. Let $\mathcal{K}^\text{ob}_{t} :=[[C_t, Y_{t+1}|m_t,u_t]]$ and $\hat{\mathcal{K}}^\text{ob}_{t} :=[[C_t, Y_{t+1}|\hat{\sigma}_t(m_t),u_t]]$. 
Then, using \eqref{ap2a}, we can write the LHS in \eqref{eq_def_approximate_info} as
$\mathcal{H}\big([[C_t, \hat{f}_t(\hat{\sigma}_t(m_t), u_t, Y_{t+1})|m_t, u_t]], [[C_t, \hat{f}_t(\hat{\sigma}_t(m_t), u_t, $ $Y_{t+1})|\hat{\sigma}_t(m_t), u_t]] \big) = \max \big\{ \sup_{(c_t, y_{t+1}) \in \mathcal{K}^\text{ob}_t}  \inf_{(\hat{c}_t,\hat{y}_{t+1}) \in \hat{\mathcal{K}}^\text{ob}_{t}}$ $\big(\eta(c_t, \hat{c}_t) + \eta\big(\hat{f}_t(\hat{\sigma}_t(m_t),u_t,y_{t+1}), \hat{f}_t(\hat{\sigma}_t(m_t),u_t,\hat{y}_{t+1})\big)\big),$ $ \sup_{(\hat{c}_t,\hat{y}_{t+1}) \in \hat{\mathcal{K}}^\text{ob}_{t}}$ $\inf_{(c_t, y_{t+1}) \in \mathcal{K}^{\text{ob}}_{t}} \big(\eta(c_t, \hat{c}_t) + \eta\big(\hat{f}_t(\hat{\sigma}_t(m_t),u_t,$ $y_{t+1}), \hat{f}_t(\hat{\sigma}_t(m_t),u_t,\hat{y}_{t+1})\big)\big)\big\},$ where, in the second equality, we use the definition of the Hausdorff distance from \eqref{H_met_def}. Note that $\hat{f}_t$ is globally Lipschitz from the alternate characterization of the approximate information state. This implies that $\eta\big(\hat{f}_t(\hat{\sigma}_t(m_t),u_t,y_{t+1}),$ $\hat{f}_t(\hat{\sigma}_t(m_t),u_t,\hat{y}_{t+1})\big) \leq L_{\hat{f}_t} {\cdot} \eta(y_{t+1},$ $\hat{y}_{t+1})$, and thus 
$\mathcal{H}\big([[C_t, \hat{f}_t(\hat{\sigma}_t(m_t), u_t, Y_{t+1})|m_t, u_t]], [[C_t, \hat{f}_t(\hat{\sigma}_t(m_t), u_t, $ $Y_{t+1})|\hat{\sigma}_t(m_t), u_t]] \big) \leq L_{\hat{f}_t} \max\big\{ \sup_{(c_t,y_{t+1}) \in \mathcal{K}^{\text{ob}}_{t}}$ $\inf_{(\hat{c}_t,\hat{y}_{t+1}) \in \hat{\mathcal{K}}^\text{ob}_{t}} \big(\eta(c_t, \hat{c}_t) + \eta(y_{t+1}, \hat{y}_{t+1}\big), 
\sup_{(\hat{c}_t,\hat{y}_{t+1}) \in \hat{\mathcal{K}}^\text{ob}_{t}}$ $\inf_{(c_t,y_{t+1}) \in \mathcal{K}^{\text{ob}}_{t}}$ $\big(\eta(c_t, \hat{c}_t) + \eta(y_{t+1}, \hat{y}_{t+1}\big)\big\}
= L_{\hat{f}_t} {\cdot} \mathcal{H}(\mathcal{K}^\text{ob}_{t},\hat{\mathcal{K}}^\text{ob}_{t}) \leq L_{\hat{f}_t} {\cdot} \delta$. Thus, \eqref{eq_def_approximate_info} is satisfied by selecting $\epsilon = L_{\hat{f}_t} {\cdot} \delta$.
\end{proof}

\begin{comment}
\begin{lemma}
    Consider the finite-time value functions $J_{0,T}, J_{1,T}, \dots, J_{T,T}$ for any time horizon $T \in \mathbb{N}$. Then, for all $t=0,\dots,T$ it holds that
    \begin{gather}
         J_{t,T}(m_t) = \gamma^t {\cdot} \bar{\Lambda}^{(T-t+1)}\big(\sigma_t(m_t)\big) + \sup_{a_t \in [[A_t|m_t]]}a_t.
    \end{gather}
\end{lemma}

\begin{proof}
    We first begin by writing the value functions at each time $t$ using the cost distributions. To do this at time $T$, note that
    \begin{multline}
        J_{T,T}(m_T) = \min_{u_T \in \mathcal{U}} \sup_{a_T, c_T \in [[A_T, C_T|m_T, u_T]]}(a_T + \gamma^T {\cdot} c_T) \\
        =\min_{u_T \in \mathcal{U}} \sup_{a_T \in \mathcal{A}, c_T \in \mathcal{C}} (a_T + \gamma^T {\cdot} c_T + \mathbb{I}(a_T, c_T| m_T, u_T)) \\
        = \min_{u_T \in \mathcal{U}} \sup_{a_T \in \mathcal{A}, c_T \in \mathcal{C}} (a_T + \gamma^T {\cdot} c_T + \mathbb{I}(a_T, c_T| m_T, u_T) \\
        - \sup_{a_T \in \mathcal{A}}(a_T + \mathbb{I}(a_T | m_T, u_T) ) ) + \sup_{a_T \in [[A_T|m_T]]} a_T \\
        = \gamma^T {\cdot} \min_{u_T \in \mathcal{U}} \sup_{c_T \in \mathcal{C}} ( c_T + \frac{r_T(c_T~|~m_T, u_T)}{\gamma^T}) + \sup_{a_T \in [[A_T|m_T]]} a_T \\
        = \gamma^T {\cdot} \min_{u_T \in \mathcal{U}} \sup_{c_T \in \mathcal{C}} ( c_T + \mathbb{I}(c_T~|~m_T, u_T)) + \sup_{a_T \in [[A_T|m_T]]} a_T \\
        = \gamma^T {\cdot} \min_{u_T \in \mathcal{U}} \sup_{c_T, s_{T+1} \in [[C_T, S_{T+1}|m_T, u_T]]} (c_T + \gamma {\cdot} \bar{\Lambda}^0(s_{T+1})) \\
        + \sup_{a_T \in [[A_T|m_T]]} a_T \\
        = \gamma^T {\cdot} \min_{u_T \in \mathcal{U}} \sup_{c_T, s_{T+1} \in [[C_T, S_{T+1}| \sigma_T(m_T), u_T]]} (c_T + \gamma {\cdot} \bar{\Lambda}^0(s_{T+1})) \\
        + \sup_{a_T \in [[A_T|m_T]]} a_T \\
        = \gamma^T {\cdot} \bar{\Lambda}^1(\sigma_T(m_T)) + \sup_{a_T \in [[A_T|m_T]]}a_T,
    \end{multline}
where, in the fourth equality, we use the fact that $\bar{\Lambda}^{{{0}}}({{{{s}}}}_{T+1}, \gamma^{T+1}) :=0$ identically and that $\sup_{{{{{s}}}}_{T+1} \in \mathcal{S}}r_T(c_T, {{{{s}}}}_{T+1}|m_T, u_T) = r_T(c_T|m_T, u_T)$; in the fifth equality, we use Lemma \ref{lemma_specialization}, and in the last equality, we use the definition of the iterated value function $\bar{\Lambda}^1(\sigma_T(m_T))$. 
Next, we assume that the result holds at time $t+1$ and prove it at time $t$. Then, at time $t$,
\begin{multline}
    J_{t,T}(m_t) = \min_{u_t \in \mathcal{U}}\sup_{m_{t+1} \in [[M_{t+1}|m_t, u_t]]} J_{t+1, T}(m_{t+1}) \\
    = \min_{u_t \in \mathcal{U}}\sup_{m_{t+1} \in [[M_{t+1}|m_t, u_t]]}(\gamma^{t+1} {\cdot} \bar{\Lambda}^{T-t}(\sigma_{t+1}(m_{t+1})) \\
    + \min_{a_t \in [[A_{t+1}|m_{t+1}]]} a_{t+1}) \\
    = \min_{u_t \in \mathcal{U}} \sup_{m_{t+1}, a_{t+1} \in [[M_{t+1}, A_{t+1}|m_t, u_t]]}(\gamma^{t+1} {\cdot} \bar{\Lambda}^{T-t}(\sigma_{t+1}( \\
    m_{t+1}))
    + \gamma^t {\cdot} c_t + a_t) \\
    = \min_{u_t \in \mathcal{U}} \sup_{m_{t+1} \in \mathcal{M}_{t+1}, a_{t} \in \mathcal{A}, c_t \in \mathcal{C}_t}(\gamma^{t+1} {\cdot} \bar{\Lambda}^{T-t}(\sigma_{t+1}( \\m_{t+1}))
    + \gamma^t {\cdot} c_t + a_t + \mathbb{I}(c_t, \sigma_{t+1}(m_{t+1}), a_t|m_t, u_t)) \\
    = \min_{u_t \in \mathcal{U}} \sup_{m_{t+1} \in \mathcal{M}_{t+1}, c_t \in \mathcal{C}_t}(\gamma^{t+1} {\cdot} \bar{\Lambda}^{T-t}(\sigma_{t+1}( \\ m_{t+1}))
    + \gamma^t {\cdot} c_t + r_t(c_t, \sigma_{t+1}(m_{t+1})|m_t, u_t)) + \sup_{a_t \in [[A_t|m_t]]} a_t \\
    = \gamma^{t} {\cdot}\min_{u_t \in \mathcal{U}} \sup_{\sigma_{t+1}(m_{t+1}) \in \mathcal{S}, c_t \in \mathcal{C}_t}(\gamma {\cdot} \bar{\Lambda}^{T-t}(\sigma_{t+1}(m_{t+1})) \\
    + c_t + \frac{r_t(c_t, \sigma_{t+1}(m_{t+1})|\sigma_t(m_t), u_t)}{\gamma^t} ) + \sup_{a_t \in [[A_t|m_t]]} a_t \\
    = \gamma^{t} {\cdot}\min_{u_t \in \mathcal{U}} \sup_{\sigma_{t+1}(m_{t+1}) \in \mathcal{S}, c_t \in \mathcal{C}_t}(\gamma {\cdot} \bar{\Lambda}^{T-t}(\sigma_{t+1}(m_{t+1})) \\
    + c_t + \mathbb{I}(c_t, \sigma_{t+1}(m_{t+1})|\sigma_t(m_t), u_t) ) + \sup_{a_t \in [[A_t|m_t]]} a_t \\
    = \gamma^{t} {\cdot}\min_{u_t \in \mathcal{U}} \sup_{\sigma_{t+1}(m_{t+1}), c_t \in [[S_{t+1}, C_t|\sigma_t(m_t), u_t]]}(\gamma {\cdot} \bar{\Lambda}^{T-t}(\sigma_{t+1}( \\
    m_{t+1})) + c_t ) + \sup_{a_t \in [[A_t|m_t]]} a_t \\
    = \gamma^t {\cdot} V^{(T-t+1)}(\sigma_t(m_t)) +  \sup_{a_t \in [[A_t|m_t]]} a_t.
\end{multline}
\end{proof}
\end{comment}

\end{document}
