\section{Introduction}
\label{sec:into}
% Background (VB, competition) -> Research Gap -> Motivation -> Our work description
\iffalse
\textcolor{red}{Introduction through speech is not appropriate? How about directly introducing vocalization emotion recognition} Speech emotion recognition (SER), which aims to determine the emotional state of a given speech input, is an essential technique that supports the interactions between humans and machines. Emotive speech signals 
%are usually highly personalized, but 
can be generally categorized into two categories, verbal and nonverbal.
The verbal speech signals contain rich emotion information manifested as acoustic and linguistic features.
%, such as prosody and semantic information. 
While the nonverbal vocalizations, such as vocal bursts (e.g., laughs, sighs, and shouts), are also critical indicators of affect, and oftentimes even more informative than verbal one for the recognition of emotions.
\fi

Recognition of emotions conveyed by non-linguistic vocalizations, e.g., affective bursts, has attracted increasing research attention, as vocalizations can reliably express certain emotions and the meanings of vocal bursts are generally preserved across diverse cultures \cite{cowen2019mapping}. This lays the theoretical foundation for using affective vocalization information to more robustly and holistically understand emotional reactions.
Despite the fact that affect vocalizations and speech-embedded prosody both utilize the same expressive (vocal) apparatus, it is also found that the accuracy of emotion decoding for non-linguistic affect vocalizations is higher than the accuracy for speech-embedded vocal prosody~\cite{hawk2009worth}. 
% \cite{scherer1986vocal,hawk2009worth,simon2009voice}.
% necessary of research on AVB
Much research has been conducted in speech emotion recognition (SER) with verbal speech recently, such as feature exploration~\cite{liang2021multibench,li2022context} and multilingual generalization~\cite{singh2022systematic}.
% \textcolor{red}{related works on tradition vocal features, and multilingual ER}
On the other hand, nonverbal vocalizations have received less attention.


Due to the scarcity of vocal burst data and lack of understanding about mechanisms of emotion signaling via vocal bursts, developing computational models for such emotion signaling remains a challenging task.   
Therefore, the recent ICML Expressive Vocalisations Workshop \& Competition 2022 (ExVo) and the ACII Affective Vocal Bursts Workshop \& Challenge 2022 (A-VB) introduce the large-scale Hume Vocal Bursts Competition Dataset (HUME-VB) for exploring various computational approaches \cite{baird2022icml,baird2022acii}. The corpus contains about 37 hours of self-recorded data by speakers in 4 countries spanning 3 native languages, which can support investigation of affective vocal bursts from diverse perspectives. Multi-task approaches have been demonstrated to be effective in previous works, e.g., by integrating various losses \cite{jing2022redundancy}, jointly modeling auxiliary prediction tasks of culture and age \cite{song2022dynamic}. However, it is desirable to explicitly model the relationship between emotion classes in vocalization signaling and the relationship between the different related tasks.
% Cowen2022HumeVB,BairdA-VB2022,
To this end, we propose a hierarchical framework based on chain regression models, which generate predictions for one task that is conditioned on the prediction from the other related tasks.

% related works: features, methods (multitask)
With recent advancements in self-supervised learning (SSL), the adopted speech representations for emotion recognition are shifting from hand-crafted features, e.g., acoustic pitch and energy, to high-level embeddings extracted by pretrained models, such as Wav2vec 2.0 \cite{baevski2020wav2vec}. The large, Transformer-based SSL models trained on large-scale data can learn representations for various downstream tasks, including automatic speech recognition (ASR) \cite{baevski2020wav2vec} and SER \cite{sharma2022multi}. 
As affective vocal burst (AVB) data is generally lacking, it is important to borrow data from other speech domains to improve the AVB modeling. 
%better and more generalizable representations that are critical for extracting the affective cues. 
Purohit \textit{et al.}~\cite{purohit2022comparing} compared supervised and self-supervised embeddings for the affective vocal burst recognition (AVBR), and showed that SSL-based representations typically yield better performance than supervised embeddings learned by pretrained task-dependent neural networks.
To further leverage these high-level features, various network architectures have been explored in the latest SER research, such as layer-wise aggregation~\cite{pepino2021emotion}, temporal attention~\cite{liu2020temporal} and dynamic convolution~\cite{wen2021crossmodal}.
Following \cite{pepino2021emotion}, we leverage representations from different layers of pretrained models with trainable weights.
% xin2022exploring

In this work, we investigate AVBR based on a hierarchical framework using chain regression models and pretrained representations. The relationships between emotional states and diverse cultures, between low-dimension and high-dimension emotion spaces, and between various emotion classes within the high-dimension space, are explicitly modeled. 
%We model the emotion labels' dependency with the hierarchical bi-chain regression framework.
% Multiple classifiers and regressors are \textcolor{red}{XXX}. 
Our system participated in the ACII A-VB challenge and ranked first in the task of the ``TWO'' and ``CULTURE'' tasks, and second in the ``HIGH'' task.
The effectiveness of the regression models and the weighted aggregation of pretrained representations is also demonstrated by further experiments we conduct on the challenge dataset.
% The rest of this paper is organized as follows: 
% The rest of the paper is organized as follows. Section \ref{sec:method} introduces the proposed methods. Then, Section \ref{sec:exp} describes the experimental setups and results, as well as further analysis of the proposed methods and comparison with baseline systems. Finally, Section \ref{sec:conclusion} concludes the paper and presents possible future research directions.



%The multitask training paradigm also benefits the SER task by considering gender, culture or other cues. \textcolor{red}{related works in ExVo}
\iffalse
% gap -> motivation
\textcolor{red}{Motivation, proposed methods, results, contribution}
\fi


% proposed methods, contribution
% As the emotion recognition tasks are heavily interdependent in A-VB 2022.


