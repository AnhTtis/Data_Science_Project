\section{Introduction}
\label{sec:into}
% Background (VB, competition) -> Research Gap -> Motivation -> Our work description

Recognition of emotions conveyed by non-linguistic vocalizations, e.g., affective bursts, has attracted increasing research attention, as vocalizations can reliably express certain emotions and the meanings of vocal bursts are generally preserved across diverse cultures \cite{cowen2019mapping}.
Similar to facial expressions, the affective vocalization information lays the foundation of more robustly and holistically understanding emotional reactions.
Despite the fact that affect vocalizations and speech-embedded prosody both utilize the same expressive (vocal) apparatus, it is also found that the accuracy of emotion decoding for non-linguistic affect vocalizations is higher than the accuracy for speech-embedded vocal prosody~\cite{hawk2009worth}. 
% \cite{scherer1986vocal,hawk2009worth,simon2009voice}.
% necessary of research on AVB
Much research has been conducted in speech emotion recognition (SER) with verbal speech recently, such as feature exploration~\cite{liang2021multibench,li2022context} and multilingual generalization~\cite{singh2022systematic}.
% \textcolor{red}{related works on tradition vocal features, and multilingual ER}
Although some research investigated the combination of verbal and non-verbal speech~\cite{huang2019speech,hsu2021speech}, emotion recognition with nonverbal vocalizations only have still been received less attention.


Due to the scarcity of vocal burst data and lack of understanding about mechanisms of emotion signaling via vocal bursts, developing computational models for such emotion signaling remains a challenging task.   
Therefore, the recent ICML Expressive Vocalisations Workshop \& Competition 2022 (ExVo) and the ACII Affective Vocal Bursts Workshop \& Challenge 2022 (A-VB) introduce the large-scale Hume Vocal Bursts Competition Dataset (HUME-VB) for exploring various computational approaches \cite{baird2022icml,baird2022acii}. The corpus contains about 37 hours of self-recorded data by speakers in 4 countries spanning 3 native languages, which can support investigation of affective vocal bursts from diverse perspectives. Multi-task approaches have been demonstrated to be effective in previous works, e.g., by integrating various losses \cite{jing2022redundancy}, jointly modeling auxiliary prediction tasks of culture and age \cite{song2022dynamic}. However, it is desirable to explicitly model the relationship between emotion classes in vocalization signaling and the relationship between the different related tasks.
% Cowen2022HumeVB,BairdA-VB2022,
To this end, we propose a hierarchical framework based on chain regression models, which generate predictions for one task that is conditioned on the prediction from the other related tasks.

% related works: features, methods (multitask)
With recent advancements in self-supervised learning (SSL), the adopted speech representations for emotion recognition are shifting from hand-crafted features, e.g., acoustic pitch and energy, to high-level embeddings extracted by pretrained models, such as Wav2vec 2.0 \cite{baevski2020wav2vec}. The large, Transformer-based SSL models trained on large-scale data can learn representations for various downstream tasks, including automatic speech recognition (ASR) \cite{baevski2020wav2vec} and SER \cite{sharma2022multi}. 
As affective vocal burst (AVB) data is generally lacking, it is important to borrow data from other speech domains to improve the AVB modeling. 
%better and more generalizable representations that are critical for extracting the affective cues. 
Purohit \textit{et al.}~\cite{purohit2022comparing} compared supervised and self-supervised embeddings for the affective vocal burst recognition (AVBR), and showed that SSL-based representations typically yield better performance than supervised embeddings learned by pretrained task-dependent neural networks.
To further leverage these high-level features, various network architectures have been explored in the latest SER research, such as layer-wise aggregation~\cite{pepino2021emotion}, temporal attention~\cite{liu2020temporal}, dynamic convolution~\cite{wen2021crossmodal}, and multi-labeling~\cite{chochlakis2022leveraging}.
Following \cite{pepino2021emotion}, we leverage representations from different layers of pretrained models with trainable weights.
% xin2022exploring

In this work, we investigate AVBR based on a hierarchical framework using chain regression models and pretrained representations. The relationships between emotional states and diverse cultures, between low-dimension and high-dimension emotion spaces, and between various emotion classes within the high-dimension space, are explicitly modeled. 
% Multiple classifiers and regressors are \textcolor{red}{XXX}. 
Our system participated in the ACII A-VB challenge and ranked first in the task of the ``TWO'' and ``CULTURE'' tasks, and second in the ``HIGH'' task.
The effectiveness of the regression models and the weighted aggregation of pretrained representations is also demonstrated by further experiments we conduct on the challenge dataset.

