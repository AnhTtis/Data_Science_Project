@InProceedings{smartglass_yuan_2020,
author="Yuan, Baojie
and Han, Yetong
and Dai, Jialu
and Zou, Yongpan
and Liu, Ye
and Wu, Kaishun",
editor="Qiu, Meikang",
title="I am Smartglasses, and I Can Assist Your Reading",
booktitle="Algorithms and Architectures for Parallel Processing",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="383--397",
abstract="Human reading states reflect people's mental activities and are closely related with learning behaviors. Consequently, it is of great value to identify reading states in many aspects. For example, it can provide timely assistance in learning processes and boost the learning efficiency of learners in a personalized way. Conventionally, researchers usually make use of EEG and fMRI to recognize human reading states. However, these methods have shortcomings of high device cost, low portability and bad user experience. In this paper, we design a real-time reading states detection system named ETist with commodity wearable glasses that can identify four reading states including attention, browsing, mind wandering and drowsiness via tracking eye movement. Through our experiments, we demonstrate that this system can recognize four fine-grained states with an average accuracy of 84.0{\%}, which is applicable in a wide area of applications.",
isbn="978-3-030-60239-0"
}

@inproceedings{note-taking_khan_2022,
author = {Khan, Anam Ahmad and Newn, Joshua and Bailey, James and Velloso, Eduardo},
title = {Integrating Gaze and Speech for Enabling Implicit Interactions},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502134},
doi = {10.1145/3491102.3502134},
abstract = {Gaze and speech are rich contextual sources of information that, when combined, can result in effective and rich multimodal interactions. This paper proposes a machine learning-based pipeline that leverages and combines users’ natural gaze activity, the semantic knowledge from their vocal utterances and the synchronicity between gaze and speech data to facilitate users’ interaction. We evaluated our proposed approach on an existing dataset, which involved 32 participants recording voice notes while reading an academic paper. Using a Logistic Regression classifier, we demonstrate that our proposed multimodal approach maps voice notes with accurate text passages with an average F1-Score of 0.90. Our proposed pipeline motivates the design of multimodal interfaces that combines natural gaze and speech patterns to enable robust interactions.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {349},
numpages = {14},
keywords = {implicit annotation, natural gaze, voice interfaces, semantic similarity, natural language processing},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{distraction_vidhya_2011,
author = {Navalpakkam, Vidhya and Rao, Justin and Slaney, Malcolm},
title = {Using Gaze Patterns to Study and Predict Reading Struggles Due to Distraction},
year = {2011},
isbn = {9781450302685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1979742.1979832},
doi = {10.1145/1979742.1979832},
abstract = {We analyze gaze patterns to study how users in online reading environments cope with visual distraction, and we report gaze markers that identify reading difficulties due to distraction. The amount of visual distraction is varied from none, medium to high by presenting irrelevant graphics beside the reading content in one of 3 conditions: no graphic, static or animated graphics. We find that under highly-distracting conditions, a struggling reader puts more effort into the text -- she takes a longer time to comprehend the text, performs more fixations on the text and frequently revisits previously read content. Furthermore, she reports an unpleasant reading experience. Interestingly, we find that whether the user is distracted and struggles or not can be predicted from gaze patterns alone with up to 80\% accuracy and up to 15\% better than with non-gaze based features. This suggests that gaze patterns can be used to detect key events such as user strugglefrustration while reading.},
booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
pages = {1705–1710},
numpages = {6},
keywords = {reading, user experience, animated graphics, visual distraction, eye-tracking, gaze patterns},
location = {Vancouver, BC, Canada},
series = {CHI EA '11}
}

@inproceedings{difficulties_lunte_2020,
author = {Lunte, Tobias and Boll, Susanne},
title = {Towards a Gaze-Contingent Reading Assistance for Children with Difficulties in Reading},
year = {2020},
isbn = {9781450371032},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373625.3418014},
doi = {10.1145/3373625.3418014},
abstract = {Reading is a key skill in learning, working and participating in society on all levels. However, in 2018, 20\% of German school students possessed only insufficient levels of reading proficiency. Frustration associated with these difficulties results in avoidance of reading, such that struggling readers will often not overcome them on their own. We present a first approach towards an assistance system that recognizes reading difficulties by analyzing the user's gaze behavior and offers dynamic adaptation of the text presentation. In a formative study, including 34 fifth-grade students, letter- and syllabication-based assistance significantly and substantially increased children's motivation to read. Based on these findings, gaze-contingent assistance presents a promising approach in improving struggling readers’ reading experience and motivation to read.},
booktitle = {Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {83},
numpages = {4},
keywords = {Reading difficulties, assistance systems, augmented reading, gaze tracking, learning to read, gaze-aware applications},
location = {Virtual Event, Greece},
series = {ASSETS '20}
}

@inproceedings{unknown-word_hiraoka_2016,
author = {Hiraoka, Rui and Tanaka, Hiroki and Sakti, Sakriani and Neubig, Graham and Nakamura, Satoshi},
title = {Personalized Unknown Word Detection in Non-Native Language Reading Using Eye Gaze},
year = {2016},
isbn = {9781450345569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993148.2993167},
doi = {10.1145/2993148.2993167},
abstract = {This paper proposes a method to detect unknown words during natural reading of non-native language text by using eye-tracking features. A previous approach utilizes gaze duration and word rarity features to perform this detection. However, while this system can be used by trained users, its performance is not sufficient during natural reading by untrained users. In this paper, we 1) apply support vector machines (SVM) with novel eye movement features that were not considered in the previous work and 2) examine the effect of personalization. The experimental results demonstrate that learning using SVMs and proposed eye movement features improves detection performance as measured by F-measure and that personalization further improves results.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimodal Interaction},
pages = {66–70},
numpages = {5},
keywords = {unknown word detection, natural reading, Eye movement},
location = {Tokyo, Japan},
series = {ICMI '16}
}

@inproceedings{summary_dubey_2020,
author = {Dubey, Neeru and Setia, Simran and Verma, Amit Arjun and Iyengar, S. R.S.},
title = {WikiGaze: Gaze-Based Personalized Summarization of Wikipedia Reading Session},
year = {2020},
isbn = {9781450380584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406853.3432662},
doi = {10.1145/3406853.3432662},
abstract = {Wikipedia is an open-content encyclopedia that receives billions of page views per month. It has been observed that in a single reading session, Wikipedia users visit multiple articles. To reduce the problems of overload and loss of information, there has been a growing interest in the research community to develop new approaches to present the only necessary information to the users. Automatically generation of personalized summaries is a proven remedy for the information overload problem. In this paper, we propose a technique to generate personalized summaries for Wikipedia articles by analyzing the reading patterns of users. To perform reading pattern analysis, we track eye gaze during the article reading session. Eye gaze analysis helps in identifying the attention distribution of a reader over an article. We extend the proposed approach to generate a summary for multiple articles visited during a user's Wikipedia reading session. We capture a dataset representing the reading pattern of Wikipedia users. We make this dataset publicly available for research community1.},
booktitle = {Proceedings of the 3rd Workshop on Human Factors in Hypertext},
articleno = {4},
numpages = {9},
keywords = {Gaze detection, Wikipedia, Summary dataset, Natural Language Processing, Multi-document Summarization},
location = {Virtual Event, USA},
series = {HUMAN'20}
}

@inproceedings{eval-webcam_hutt_2022,
author = {Hutt, Stephen and D'Mello, Sidney K.},
title = {Evaluating Calibration-Free Webcam-Based Eye Tracking for Gaze-Based User Modeling},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536221.3556580},
doi = {10.1145/3536221.3556580},
abstract = {Eye tracking has been a research tool for decades, providing insights into interactions, usability, and, more recently, gaze-enabled interfaces. Recent work has utilized consumer-grade and webcam-based eye tracking, but is limited by the need to repeatedly calibrate the tracker, which becomes cumbersome for use outside the lab. To address this limitation, we developed an unsupervised algorithm that maps gaze vectors from a webcam to fixation features used for user modeling, bypassing the need for screen-based gaze coordinates, which require a calibration process. We evaluated our approach using three datasets (N=377) encompassing different UIs (computerized reading, an Intelligent Tutoring System), environments (laboratory or the classroom), and a traditional gaze tracker used for comparison. Our research shows that webcam-based gaze features correlate moderately with eye-tracker-based features and can model user engagement and comprehension as accurately as the latter. We discuss applications for research and gaze-enabled user interfaces for long-term use in the wild.},
booktitle = {Proceedings of the 2022 International Conference on Multimodal Interaction},
pages = {224–235},
numpages = {12},
keywords = {Eye Tracking, Webcam, Mind Wandering, DBSCAN, Gaze-enabled interfaces, Comprehension},
location = {Bengaluru, India},
series = {ICMI '22}
}

@inproceedings{searchGazer_papoutsaki_2017,
author = {Papoutsaki, Alexandra and Laskey, James and Huang, Jeff},
title = {SearchGazer: Webcam Eye Tracking for Remote Studies of Web Search},
year = {2017},
isbn = {9781450346771},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3020165.3020170},
doi = {10.1145/3020165.3020170},
abstract = {We introduce SearchGazer, a web-based eye tracker for remote web search studies using common webcams already present in laptops and some desktop computers. SearchGazer is a pure JavaScript library that infers the gaze behavior of searchers in real time. The eye tracking model self-calibrates by watching searchers interact with the search pages and trains a mapping of eye features to gaze locations and search page elements on the screen. Contrary to typical eye tracking studies in information retrieval, this approach does not require the purchase of any additional specialized equipment, and can be done remotely in a user's natural environment, leading to cheaper and easier visual attention studies.While SearchGazer is not intended to be as accurate as specialized eye trackers, it is able to replicate many of the research findings of three seminal information retrieval papers: two that used eye tracking devices, and one that used the mouse cursor as a restricted focus viewer. Charts and heatmaps from those original papers are plotted side-by-side with SearchGazer results. While the main results are similar, there are some notable differences, which we hypothesize derive from improvements in the latest ranking technologies used by current versions of search engines and diligence by remote users. As part of this paper, we also release SearchGazer as a library that can be integrated into any search page.},
booktitle = {Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval},
pages = {17–26},
numpages = {10},
keywords = {gaze prediction, web search behavior, user interactions, remote user studies, online eye tracking},
location = {Oslo, Norway},
series = {CHIIR '17}
}

@ARTICLE{sentiment_long_21,
  author={Long, Yunfei and Xiang, Rong and Lu, Qin and Huang, Chu-Ren and Li, Minglei},
  journal={IEEE Transactions on Affective Computing}, 
  title={Improving Attention Model Based on Cognition Grounded Data for Sentiment Analysis}, 
  year={2021},
  volume={12},
  number={4},
  pages={900-912},
  doi={10.1109/TAFFC.2019.2903056}}

@inproceedings{barrett-etal-2018-sequence,
    title = "Sequence Classification with Human Attention",
    author = "Barrett, Maria  and
      Bingel, Joachim  and
      Hollenstein, Nora  and
      Rei, Marek  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the 22nd Conference on Computational Natural Language Learning",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K18-1030",
    doi = "10.18653/v1/K18-1030",
    pages = "302--312",
    abstract = "Learning attention functions requires large volumes of data, but many NLP tasks simulate human behavior, and in this paper, we show that human attention really does provide a good inductive bias on many attention functions in NLP. Specifically, we use estimated human attention derived from eye-tracking corpora to regularize attention functions in recurrent neural networks. We show substantial improvements across a range of tasks, including sentiment analysis, grammatical error detection, and detection of abusive language.",
}

@article{barrett2020sequence,
  title={Sequence labelling and sequence classification with gaze: Novel uses of eye-tracking data for Natural Language Processing},
  author={Barrett, Maria and Hollenstein, Nora},
  journal={Language and Linguistics Compass},
  volume={14},
  number={11},
  pages={1--16},
  year={2020},
  publisher={Wiley Online Library}
}


@inproceedings{ner_Hollenstein_2019,
  title={Entity Recognition at First Sight: Improving NER with Eye Movement Information},
  author={Nora Hollenstein and Ce Zhang},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019}
}

@article{mokhtar2012guessing,
  title={Guessing word meaning from context has its limit: Why},
  author={Mokhtar, Ahmad Azman and Rawian, Rafizah Mohd},
  journal={International Journal of Linguistics},
  volume={4},
  number={2},
  pages={288--305},
  year={2012}
}

@article{rigg1991whole,
  title={Whole language in TESOL},
  author={Rigg, Pat},
  journal={Tesol Quarterly},
  volume={25},
  number={3},
  pages={521--542},
  year={1991},
  publisher={Wiley Online Library}
}

@article{posner1980orienting,
  title={Orienting of attention},
  author={Posner, Michael I},
  journal={Quarterly journal of experimental psychology},
  volume={32},
  number={1},
  pages={3--25},
  year={1980},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{rayner1998eye,
  title={Eye movements in reading and information processing: 20 years of research.},
  author={Rayner, Keith},
  journal={Psychological bulletin},
  volume={124},
  number={3},
  pages={372},
  year={1998},
  publisher={American Psychological Association}
}

@article{uttal1968recognition,
  title={Recognition of alphabetic characters during voluntary eye movements},
  author={Uttal, William R and Smith, Pamela},
  journal={Perception \& Psychophysics},
  volume={3},
  number={4},
  pages={257--264},
  year={1968},
  publisher={Springer}
}

@article{jacobs1987spatial,
  title={Spatial and/or temporal adjustments of scanning behavior to visibility changes},
  author={Jacobs, Arthur M and O'Regan, J Kevin},
  journal={Acta Psychologica},
  volume={65},
  number={2},
  pages={133--146},
  year={1987},
  publisher={Elsevier}
}

@article{jacobs1986eye,
  title={Eye-movement control in visual search: How direct is visual span control?},
  author={Jacobs, Arthur M},
  journal={Perception \& Psychophysics},
  volume={39},
  number={1},
  pages={47--58},
  year={1986},
  publisher={Springer}
}

@article{rayner1981masking,
  title={Masking of foveal and parafoveal vision during eye fixations in reading.},
  author={Rayner, Keith and Inhoff, Albrecht Werner and Morrison, Robert E and Slowiaczek, Maria L and Bertera, James H},
  journal={Journal of Experimental Psychology: Human perception and performance},
  volume={7},
  number={1},
  pages={167},
  year={1981},
  publisher={American Psychological Association}
}

@article{clifton2007eye,
  title={Eye movements in reading words and sentences},
  author={Clifton Jr, Charles and Staub, Adrian and Rayner, Keith},
  journal={Eye movements},
  pages={341--371},
  year={2007},
  publisher={Elsevier}
}

@article{rayner1977visual,
  title={Visual attention in reading: Eye movements reflect cognitive processes},
  author={Rayner, Keith},
  journal={Memory \& cognition},
  volume={5},
  number={4},
  pages={443--448},
  year={1977},
  publisher={Springer}
}

@article{just1980theory,
  title={A theory of reading: from eye fixations to comprehension.},
  author={Just, Marcel A and Carpenter, Patricia A},
  journal={Psychological review},
  volume={87},
  number={4},
  pages={329},
  year={1980},
  publisher={American Psychological Association}
}

@inproceedings{web_ehara_2010,
author = {Ehara, Yo and Shimizu, Nobuyuki and Ninomiya, Takashi and Nakagawa, Hiroshi},
title = {Personalized Reading Support for Second-Language Web Documents by Collective Intelligence},
year = {2010},
isbn = {9781605585154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1719970.1719978},
doi = {10.1145/1719970.1719978},
abstract = {Novel intelligent interface eases the browsing of Web documents written in the second languages of users. It automatically predicts words unfamiliar to the user by collective intelligence and glosses them with their meaning in advance. If the prediction succeeds, the user does not need to consult a dictionary; even if it fails, the user can correct the prediction. The correction data are collected and used to improve the accuracy of further predictions. The prediction is personalized in that every user's language ability is estimated by a state-of-the-art language testing model, which is trained in a practical response time with only a small sacrifice of prediction accuracy. Evaluation results for the system in terms of prediction accuracy are encouraging.},
booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
pages = {51–60},
numpages = {10},
keywords = {reading support, item response theory, web page, collective intelligence, glossing system},
location = {Hong Kong, China},
series = {IUI '10}
}

@inproceedings{imu_higa_2022,
author = {Higashimura, Riku and Vargo, Andrew and Iwata, Motoi and Kise, Koichi},
title = {Helping Mobile Learners Know Unknown Words through Their Reading Behavior},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519620},
doi = {10.1145/3491101.3519620},
abstract = {Vocabulary acquisition is a fundamental part of learning a new language. In order to acquire new vocabulary, words with meanings that are unknown to the learner must be added to the language learning process. When searching for material in the target language, it is useful to know how much of a document is made up of currently unknown words. One simple way to estimate the unknown words in a document is to use the frequency of occurrence, which indicates the difficulty of the word. However, this approach can lead to missed unknown words. In this study, we aim to improve the accuracy of unknown word estimation by using reading activity data obtained from smartphone sensors and taking into account the individual learner’s English reading behavior. We apply a novel user interface which allows us to improve estimation through reading behavior, without the use of eye-trackers.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {249},
numpages = {5},
keywords = {mobile learning, language learning, education},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@INPROCEEDINGS{gaze-text_garain_2017,
  author={Garain, Utpal and Pandit, Onkar and Augereau, Olivier and Okoso, Ayano and Kise, Koichi},
  booktitle={2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)}, 
  title={Identification of Reader Specific Difficult Words by Analyzing Eye Gaze and Document Content}, 
  year={2017},
  volume={01},
  number={},
  pages={1346-1351},
  doi={10.1109/ICDAR.2017.221}}

@book{hyrskykari2006eyes,
  title={Eyes in attentive interfaces: Experiences from creating iDict, a gaze-aware reading aid},
  author={Hyrskykari, Aulikki},
  year={2006},
  publisher={Tampere University Press}
}

@inproceedings{papoutsaki2016webgazer,
  author = {Alexandra Papoutsaki and Patsorn Sangkloy and James Laskey and Nediyana Daskalova and Jeff Huang and James Hays},
  title = {WebGazer: Scalable Webcam Eye Tracking Using User Interactions},
  booktitle = {Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI)},
  pages = {3839--3845},
  year = {2016},
  organization={AAAI}
}

@article{chujo2009many,
  title={How many words do you need to know to understand TOEIC, TOEFL \& EIKEN? An examination of text coverage and high frequency vocabulary},
  author={Chujo, Kiyomi and Oghigian, Kathryn},
  journal={Journal of Asia TEFL},
  volume={6},
  number={2},
  year={2009},
  publisher={Asia TEFL}
}

@article{rayner1986lexical,
  title={Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity},
  author={Rayner, Keith and Duffy, Susan A},
  journal={Memory \& cognition},
  volume={14},
  number={3},
  pages={191--201},
  year={1986},
  publisher={Springer}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@article{honnibal2020spacy,
  title={spaCy: Industrial-strength natural language processing in python},
  author={Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  year={2020},
  publisher={Zenodo, Honolulu, HI, USA}
}

@incollection{nation1990teaching,
  title={Teaching and learning vocabulary},
  author={Nation, Paul},
  booktitle={Handbook of Practical Second Language Teaching and Learning},
  pages={397--408},
  year={1990},
  publisher={Routledge}
}

@article{schmitt2001developing,
  title={Developing and exploring the behaviour of two new versions of the Vocabulary Levels Test},
  author={Schmitt, Norbert and Schmitt, Diane and Clapham, Caroline},
  journal={Language testing},
  volume={18},
  number={1},
  pages={55--88},
  year={2001},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@book{read2000assessing,
  title={Assessing vocabulary},
  author={Read, John},
  year={2000},
  publisher={Cambridge university press}
}

@article{read1988measuring,
  title={Measuring the vocabulary knowledge of second langauge learners},
  author={Read, John},
  journal={RELC journal},
  volume={19},
  number={2},
  pages={12--25},
  year={1988},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@inproceedings{faceori,
author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun},
title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517698},
doi = {10.1145/3491102.3517698},
abstract = {Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {290},
numpages = {12},
keywords = {head pose estimation., Acoustic ranging, earphone, head orientation},
location = {New Orleans, LA, USA},
series = {CHI '22}
}
@inproceedings{reflectrack,
author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun},
title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474805},
doi = {10.1145/3472749.3474805},
abstract = {3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm \texttimes{} 60cm \texttimes{} 60cm space and 22.1 mm in the 30cm \texttimes{} 30cm \texttimes{} 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {1050–1062},
numpages = {13},
keywords = {Acoustic tracking, FMCW, smartphones, sound reflection},
location = {Virtual Event, USA},
series = {UIST '21}
}