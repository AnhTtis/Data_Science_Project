\section{Results and Findings}
\label{sec:results}


\subsection{Prediction Accuracy}

\begin{table}[b]
\begin{tabular}{lrr|rrr}
\hline
PLM Backbone & $n_p$ & $n_k$ & Precision & Recall & F-1 Score \\ \hline
RoBERTa      & 16             & 16           &68.31     &79.20    &73.97           \\
RoBERTa      & 16             & 32                  &\textbf{71.21}           &80.70        &\textbf{75.73}           \\
RoBERTa      & 32             & 16                  &67.29           &84.81        &75.11           \\
RoBERTa      & 32             & 32                  &65.50           &\textbf{86.55}        &74.97           \\ \hline
\end{tabular}
\caption{Unknown word detection results of \projectname.}
\label{tab:main-results}
% \vspace{-8mm}
\end{table}

The main results of our model are shown in Table~\ref{tab:main-results}. We use precision, recall, and F-1 score as metrics for our unknown word detection task. We trained our models for 5 epochs, with the learning rate of 8e-5 for parameters in the RoBERTa and linear layers, and 0.1 for LSTM layers. Our best model can achieve the accuracy of 98.09\% with 75.73\% F-1 score. Moreover, we tried out different hidden dimensions for gaze-text attention $n_p$ and knowledge embedding $n_k$. It shows that by increasing the knowledge dimension $n_k$, the overall performance of our model can be improved while increasing the gaze-document attention dimension $n_p$ helps promote the recall.

\subsection{Transfer Learning Analysis}

\subsubsection{Transfer Learning Setting}
Table~\ref{tab:transfer-learning} indicates our model's transferability between users and reading materials. For the cross-user transfer learning experiment, we trained the model 12 times in total, and for each running, we selected one user's data as the test set and took other users' data as the training set. For the cross-document experiment, we also conducted 12 pieces of training and evaluations. Under each running, we selected the data generated from three documents as the test set, and the rest of the data were combined as the training set. Therefore, our relative data sizes between training and testing data are similar under cross-user and cross-document settings.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{fig/jaccard.png}
    \caption{Jaccard similarity of unknown words between users.}
    \label{fig:jaccard}
\end{figure}

\subsubsection{Cross-User Transferability}
We find out that when the model is applied to new users' data, its performance is still on par with its original setting. Therefore, our model can be effectively adapted to new user groups without specific calibrations or tuning, which successfully fits our goal of ubiquitous unknown word detection. We further analyze the unknown words of different users, and their Jaccard similarity matrix is shown in Fig~\ref{fig:jaccard}. The average of cross-user unknown words Jaccard similarity score is 0.23 and most of their similarities are below 0.3, meaning that different users' unknown words are distinct even given the same article. It further proves that our model does not simply remember the unknown words of users, but predicts the unknown words based on the user's behaviors. Meanwhile, the model's performance can become significantly better on some users, and a possible reason is that these users' behaviors are more general, i.e., their language skills might be the average among all people.

\begin{table*}[]
\begin{tabular}{ll|rrr}
\hline
User  dependency & Document dependency & Precision & Recall & F-1 Score \\ \hline
Dependent        & Dependent           &71.21           &80.70        &75.73           \\
Independent      & Dependent           &$73.65 \pm 5.83$           &$82.77\pm 4.41$       &$78.26\pm 4.53$           \\
Dependent        & Independent         &$46.56\pm 4.26$           &$68.39\pm 5.80$        &$56.31 \pm 3.38$           \\ \hline
\end{tabular}
\caption{Transferability analysis of unknown word detection by \projectname{}.}
\label{tab:transfer-learning}
\end{table*}

\subsubsection{Cross-Document Transferability}
In order to test whether our model only memorizes the difficult words from the text, we conducted the experiment to see how the model would perform if the words in the test data were not presented for training. We find that the model's performance dropped in this setting, while the recall is still acceptable. 

\subsection{Ablation Study}

\begin{table*}[]
\begin{tabular}{l|rrr}
\hline
Model                         & Precision & Recall & F-1 Score \\ \hline
\projectname{}                           &\textbf{71.21}           &80.70        &\textbf{75.73}           \\
\projectname{}~w/o context-awareness     &4.78           &59.03        &10.00           \\
\projectname{}~w/o gaze                  &66.35           &\textbf{86.67}        &75.59           \\
\projectname{}~w/o knowledge enhancement &69.96          &80.32        &74.93           \\ \hline
\end{tabular}
\caption{Ablation study of \projectname{}.}
\label{tab:ablation-study}
% \vspace{-5mm}
\end{table*}

We tried to exclude three different modalities of data presented in our model respectively, which are text information, gazing data, and word-level knowledge. After the text information is removed, the model's performance drops significantly, which proves that the use of a pre-trained language model can effectively promote the model's ability to detect unknown words of users. 
%Meanwhile, after removing the gazing data, the model's performance slightly decreases since, in this case, the model cannot determine which specific span of text the user is reading. Therefore, we can see that the model's precision has dropped because it tend to recognize difficult words that the user is not reading as predicted unknown words. 
Then we removed the gaze data, in which case the model cannot obtain the gaze behavior related to the text. The model's precision drops and the recall increases. It indicates that both the true positive and the false positive increase. In other words, without gaze, the model tends to predict a word as an unknown word. The gaze may help identify whether a difficult word is an unknown word or not.
Finally, the result of ablating knowledge enhancement proves that infusing word-level knowledge can promote the model's performance.

\subsection{Design Guidance}

Our results of the 5000-level VLT show that users can be divided into a high-level group and a low-level group. A Mann-Whitney U test was run to determine if there were differences in the test score between them. Scores for the high-level group (mean rank = 9.5) and low-level group (mean rank = 3.5) were statistically significantly different, \textit{U} = .000, z = -2.887, \textit{p} = .002, using an exact sampling distribution for U. The results demonstrate that our users covered learners of different levels.

As for factors affecting the reading of academic texts, compared to a neutral attitude score of 3, the scores were 3.67 for new words (\textit{SD} = 0.98, \textit{t}(11) = 2.345, \textit{p} < .05), 3.83 for the ambiguous meaning of words (\textit{SD} = 0.72, \textit{t}(11) = 4.022, \textit{p} < .01), 4.25 for long and complex sentences (\textit{SD} = 0.87, \textit{t}(11) = 5.000, \textit{p} < .001), all of which were perceived by users as having a significant effect on comprehension of the text. This result suggests the importance of contextualizing the interpretation of unknown words in reading.

The scores of the two proposed features are 4.35 for eye-tracking reading assistance (\textit{SD} = 0.64, \textit{t}(11) = 7.244, \textit{p} < .001) and 3.93 for vocabulary management (\textit{SD} = 0.99, \textit{t}(11) = 3.260, \textit{p} < .01), respectively. Both are significantly higher than a neutral score of 3, demonstrating that users hold a positive attitude toward our proposed features. The results show that none of the privacy and power consumption is rated significantly, which indicates the possibility of future tool design.

By analyzing words users wrote about expecting features, we found that the three most frequently mentioned features are translating proper nouns, translating long and complex sentences, and accurately identifying multi-meaning words. Two other users also mentioned the need to avoid pop-ups from affecting their reading. We believe these goals can all be achieved using our webcam-based tool and NLP model.

Based on the results mentioned above, applying the webcam to unknown words detecting and reading assistance by our approach of eye tracking and NLP model has theoretical and practical value. Interaction design strategies should also be addressed in developing applications to improve the user experience.