
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption} %[labelformat=empty]
\usepackage{subcaption} % [labelformat=empty]
\usepackage{wrapfig}
\usepackage{xcolor}  
\usepackage{booktabs}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{mathtools}
\newcommand{\GK}[1]{\textcolor{blue}{[Gabriel: #1]}}


%\usepackage[algo2e]{algorithm2e}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage[noend]{algpseudocode}
\usepackage[algo2e]{algorithm2e} 


\usepackage[toc,page]{appendix}
\usepackage{minitoc}
\usepackage{etoc}
\newcommand{\nocontentsline}[3]{}
\newcommand{\tocless}[2]{\bgroup\let\addcontentsline=\nocontentsline#1{#2}\egroup}
\renewcommand{\contentsname}{}
\renewcommand \thepart{}
\renewcommand \partname{}

\title{Sparse Distributed Memory is a Continual Learner}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{
Trenton Bricken\thanks{Correspondence to \texttt{trentonbricken@g.harvard.edu}} \\
  Systems, Synthetic, and Quantitative Biology\\
  Harvard University\\
  \And 
  Xander Davies\\
  Computer Science\\
  Harvard College\\
  \And 
  Deepak Singh\\
  Computer Science\\
  Harvard College\\
  \And 
  Dmitry Krotov\\
  MIT-IBM Watson AI Lab\\
  IBM Research\\
  \And 
  Gabriel Kreiman\\
  Programs in Biophysics and Neuroscience\\
  Harvard Medical School\\
}
% \thanks{ Correspondence .}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\doparttoc % Tell to minitoc to generate a toc for the parts
\faketableofcontents % Run a fake tableofcontents command for the partocs
%\part{} % Start the document part
%\parttoc % Insert the document TOC


\maketitle
\begin{abstract}
Continual learning is a problem for artificial neural networks that their biological counterparts are adept at solving. Building on work using Sparse Distributed Memory (SDM) to connect a core neural circuit with the powerful Transformer model, we create a modified Multi-Layered Perceptron (MLP) that is a strong continual learner. We find that every component of our MLP variant translated from biology is necessary for continual learning. Our solution is also free from any memory replay or task information, and introduces novel methods to train sparse networks that may be broadly applicable.
\end{abstract}

\section{Introduction}
\label{sec:Introduction}

%Using biological inspiration to improve continual learning in artificial neural networks is a well trodden and successful route. However, past work has typically taken a single feature of the brain, such as synaptic plasticity, and only loosely approximated its underlying function. Our work is unique in both the number of biological features that we leverage and the extent to which we implement their functionality.

Biological networks tend to thrive in continually learning novel tasks, a problem that remains daunting for artificial neural networks. Here, we use Sparse Distributed Memory (SDM) to modify a Multi-Layered Perceptron (MLP) with features from a cerebellum-like neural circuit that are shared across organisms as diverse as humans, fruit flies, and electric fish \citep{ReviewMushroomBody, Xie2022TaskdependentOR}. These modifications result in a new MLP variant (referred to as SDMLP) that uses a Top-K\footnote{Also called ``k Winner Takes All'' in related literature.} activation function (keeping only the $k$ most excited neurons in a layer on), no bias terms, and enforces both $L^2$ normalization and non-negativity constraints on its weights and data. All of these SDM-derived components are necessary for our model to avoid catastrophic forgetting. 

We encounter challenges when training the SDMLP that we leverage additional neurobiology to solve, resulting in better continual learning performance. Our first problem is with ``dead neurons'' that are never active for any input and which are caused by the Top-K activation function \citep{Frey, SparseFFNumenta}. Having fewer neurons participating in learning results in more of them being overwritten by any new continual learning task, increasing catastrophic forgetting. Our solution imitates the ``GABA Switch'' phenomenon where inhibitory interneurons that implement Top-K will \emph{excite} rather than \emph{inhibit} early in development \citep{GabaSwitch}. 

The second problem is with optimizers that use momentum, which becomes ``stale'' when training highly sparse networks. This staleness refers to the optimizer continuing to update inactive neurons with an out-of-date moving average, killing neurons and again harming continual learning. To our knowledge, we are the first to formally identify this problem that will in theory affect any sparse model, including recent Mixtures of Experts \citep{SwitchTransformer, MixtureExpertsHinton}.
% As a result, we resort to using SGD without momentum. 

Our SDMLP is a strong continual learner, especially when combined with complementary approaches. Using our solution in conjunction with Elastic Weight Consolidation (EWC) \citep{EWC}, we obtain, to the best of our knowledge, state-of-the-art performance for CIFAR-10 in the class incremental setting when memory replay is not allowed. Another variant of SDM, developed independently of our work, appears to be state-of-the-art for CIFAR-100, MNIST, and FashionMNIST, with our SDMLP as a close second \citep{FruitFlyContLearning}. %TALK ABOUT MNIST AND FASHION MNIST HERE TOO? 

Excitingly, our continual learning success is ``organic'' in resulting from the underlying model architecture and does not require any task labels, task boundaries, or memory replay. Abstractly, SDM learns its subnetworks responsible for continual learning using two core model components. First, the Top-K activation function causes the $k$ neurons most activated by an input to specialize towards this input, resulting in the formation of specialized subnetworks. Second, the $L^2$ normalization and absence of a bias term together constrain neurons to the data manifold, ensuring that all neurons democratically participate in learning. When these two components are combined, a new learning task only activates and trains a small subset of neurons, leaving the rest of the network intact to remember previous tasks without being overwritten

As a roadmap of the paper, we first discuss related work (Section \ref{sec:RelatedWork}). We then provide a short introduction to SDM (Section \ref{sec:Background}), before translating it into our MLP (Section \ref{sec:SDMtoMLP}). Next we present our results, comparing the organic continual learning capabilities of our SDMLP against relevant benchmarks (Section \ref{sec:SDMCatastrophicForgetting}). Finally, we conclude with a discussion on the limitations of our work, sparse models more broadly, and how SDM relates MLPs to Transformer Attention (Section \ref{sec:Discussion}).

%SDM cannot effectively model real-world data that exists on a correlated, low dimensional manifold of its vector space. This is because SDM assumed that the ``neurons'' used to store memories are randomly distributed throughout the full vector space. With real-world data, this means many neurons are distant from the lower dimensional data manifold and never utilized.\footnote{Having the neurons be randomly distributed was not a problem when the patterns being stored by the system were also assumed to be randomly distributed. This assumption was made in order to make analysis of memory capacity and convergence dynamics of the memory system analytically tractable.}  

%The solution, fully explained in Section 3 after a short introduction to SDM in Section 2, utilizes the Golgi inhibitory interneuron found in the cerebellum to enforce competitive learning between neurons in the form of a Top-K activation function \citep{SDMvsHopfield}. We leverage additional neurobiology, specifically the fact that the Golgi interneuron will \emph{excite} rather than \emph{inhibit} early in development, to improve upon the naive Top-K activation function and resolve the ``Dead Neuron problem'', crucial for continual learning (Section 4) \citep{GabaSwitch}. 
%This Golgi interneuron, when combined with the continuous version of SDM developed in \citep{SDMAttention} and made differentiable for backpropagation, closely relates to a Multi-Layered Perceptron (MLP) with one hidden layer (Section 5). We investigate the unique features of our SDMLP against related benchmarks (Section 6).

%In summary, our unique contributions are:
%\begin{itemize}
%\itemsep0em 
%    \item Creating a highly competitive new model for continual learning that is ``organic'' in %its avoidance of task labels, task boundaries, and memory replay. 
%    \item Widening the initial bridge SDM has established between deep learning and neurobiology %by implementing an MLP as SDM by using the Top-K activation function.
%    \item Solving the ``Dead Neuron'' problem in a new and biologically plausible fashion %beneficial to continual learning.
%    \item Identifying the ``Stale Momentum'' problem for training highly sparse models with %optimizers that use momentum.
%    %\item Improving the representational capacity of SDM by allowing it to model real-world %data.
%\end{itemize}

\section{Related Work}
\label{sec:RelatedWork}

\textbf{Continual Learning -} The techniques developed for continual learning can be broadly divided into three categories: architectural \citep{GoodfellowContLearning, Goodfellow2013MaxoutN}, regularization \citep{RehersalFreeContLearning, EWC, SISynapticIntelligence, MAS}, and rehearsal \citep{ContLearningSurvey, ContinualLearningBaselines}. Many of these approaches have used the formation of sparse subnetworks for continual learning \citep{BadSparseContLearning, LocalInhibContLearning, ContLearningPretraining, NumentaCatastrophicForgetting, ParamIsolationMask, PackNetContLearning, Schwarz2021PowerpropagationAS, RehersalFreeContLearning, NeuroCoder1, NeuroCoder2}.\footnote{Even without sparsity or the Top-K activation function, pretraining models can still lead to the formation of subnetworks, which translates into better continual learning performance as found in \cite{ContLearningPretraining}.} However, in contrast to our ``organic'' approach, these methods employ complex algorithms and additional memory consumption to explicitly protect model weights important for previous tasks from overwrites.\footnote{Memory replay methods indirectly determine and protect weights by deciding what memories to replay.}

Works applying the Top-K activation to continual learning include \citep{LocalInhibContLearning, GabaSwitch, NumentaCatastrophicForgetting}. \cite{lWTAforForgetting} used a local version of Top-K, defining disjoint subsets of neurons in each layer and applying Top-K locally to each. However, this was only used on a simple task-incremental two-split MNIST task and without any of the additional SDM modifications that we found crucial to our strong performance (Table 2).

The Top-K activation function has also been applied more broadly in deep learning \citep{Frey, SparseFFNumenta, SimilarityMatchingManifold, GabaSwitch, LocalInhibContLearning}. Top-K converges not only with the connectivity of many brain regions that utilize inhibitory interneurons but also with results showing advantages beyond continual learning, including: greater interpretability \citep{Frey, Krotov2019UnsupervisedLB, KrotovImageAnalysis}, robustness to adversarial attacks \citep{TopKRobustness, KrotovHopfieldRobust, NumentaCatastrophicForgetting}, efficient sparse computations \citep{SparseFFNumenta}, tiling of the data manifold \citep{SimilarityMatchingManifold}, and implementation with local Hebbian learning rules \citep{GabaSwitch,SimilarityMatchingManifold, Krotov2019UnsupervisedLB, ryali2020bio, liang2020can}.

%Of particular interest is \citep{SimilarityMatchingManifold}, which shows that a similarity matching objective function can be written as an online Hebbian learning rule that utilizes an inhibitory interneuron and thus approximates the Top-K operation. The work introduces the same positive weight and $L^2$ normalization constraints that we use and even analytically proves for tractable datasets that its neurons will tile the data manifold in proportion to its density. This manifold tiling result has been shown empirically in a number of works using Top-K activations and is a result that we also find for our model as evidenced by Fig. 6 \citep{GabaSwitch, Frey, KrotovHopfieldRobust, KrotovImageAnalysis}.
% that obtains strong continual learning performance
% . The only modification to the original SDM algorithm is a. that uses random, fixed neuron addresses that are highly sparse.

\textbf{FlyModel -} The most closely related method to ours is a model of the \emph{Drosophila} Mushroom Body circuitry \citep{FruitFlyContLearning}. This model, referred to as ``FlyModel'', unknowingly implements the SDM algorithm, specifically the Hyperplane variant \citep{Jaeckel1989Hyperplane} with a Top-K activation function that we also use and will justify \citep{SDMvsHopfield}.  The FlyModel shows strong continual learning performance, trading off the position of best performer with our SDMLP across tasks and bolstering the title of our paper that SDM is a continual learner. 

While both models are derived from SDM, our work both extends the theory of SDM by allowing it to successfully learn data manifolds (App. \ref{appendix:AddressDecoding}), and reconciles the differences between SDM and MLPs, such that SDM can be trained in the deep learning framework (this includes having no fixed neuron weights and using backpropagation). Both of these contributions are novel and may inspire future work beyond continual learning. For example, learning the data manifold preserves similarity in the data and leads to more specialized, interpretable neurons (e.g., Fig. \ref{fig:ReceptiveFields} of App. \ref{appendix:InvestigateContLearning}). 

Training in the deep learning framework also allows us to combine SDMLP with other gradient-based methods like Elastic Weight Consolidation (EWC) that the FlyModel is incompatible with \citep{EWC}. Additionally, demonstrating how MLPs can be implemented as a cerebellar circuit serves as an example for how ideas from neuroscience, like the GABA switch, can be leveraged to the benefit of deep learning. %allowing us to achieve SOTA results across benchmarks that the FlyModel is incompatible with.

%While both models are derived from SDM, our work extensively modifies the underlying SDM theory to not only enable modelling correlated, real world data manifolds, but also recast it in the deep learning framework. SDM originally made a simplifying assumption that the data points were random, limiting its ability to model real world tasks (App. \ref{appendix:AddressDecoding}). The FlyModel uses careful data preprocessing combined with the Top-K activation function to perform well on the continual learning tasks tested, however, it still uses the original SDM approach of having a fixed weight matrix and relying upon random projections. By contrast, we implement insights from \cite{SDMvsHopfield} that extend SDM, allowing our SDMLP to successfully learn data manifolds. This approach is more robust to data distribution, preserves similarity between data points, and creates interpretable neurons.\footnote{Data points that are not well separated will repeatedly activate the same small subset of FlyModel neurons, over utilizing them while under utilizing others. This is because they will not learn the tile the data manifold in proportion to data density \citep{SimilarityMatchingManifold}.}

% By contrast, the FlyModel does not learn data manifolds, using random projections and careful data preprocessing instead. While remaining empirically very effective in the continual learning tasks tested, the FlyModel random projection fails to 
%Other works are related to SDMLP in learning the data manifold structure with a Mushroom Body inspired architecture include \citep{ryali2020bio,liang2020can}; however, they have not been applied to continual learning.

%The FlyModel is also deep learning independent, avoiding relations to deep learning architectures and backpropagation.\footnote{The FlyModel does use deep learning to generate low dimensional image embeddings with an AlexNet model but this is a separate data preprocessing step \citep{FruitFlyContLearning}.} Meanwhile, our work explicitly connects SDM and its underlying cerebellum-like neural circuitry with the MLP. This effort makes SDMLP compatible with other deep learning approaches to continual learning such as Elastic Weight Consolidation (EWC) that rely upon backpropagation \citep{EWC}. This demonstrates how MLPs can be implemented as a cerebellar circuit and serves as an example for how ideas from neuroscience, like the GABA switch, can be leveraged to the benefit of deep learning.

% by reconciling some of the many differences between deep learning and neuroscience.

% \footnote{The Drosophila Mushroom Body has a close mapping to the circuitry of the cerebellum that SDM derives its biological plausibility from, making this connection unsurprising in hindsight \citep{ReviewMushroomBody}. However, the degree to which the algorithm used in \citep{FruitFlyContLearning} converges with SDM remains striking.}

%The FlyModel and our approach differ in their underlying motivations: the FlyModel uses the cerebellar circuitry and neuroscience as a starting point to implement a statistical model; our work does the inverse, using an MLP and deep learning as a starting point and modifying it to model the cerebellar circuit. Concretely, this means that 
%A promising Combining these approaches through the use of sparse weights and Hebbian associative learning instead of backpropagation is a promising direction for future work.
% We also test our method in the more challenging class incremental setting.

\section{Background on Sparse Distributed Memory}
\label{sec:Background}

Sparse Distributed Memory (SDM) is an associative memory model that tries to solve the problem of how patterns (memories) could be stored in the brain \citep{SDM, SDMBookChapter1993} and has close connections to Hopfield networks, the circuitry of the cerebellum, and Transformer Attention \citep{SDM, SDMAttention, OGHopfield, hopfield1984neurons, modernHopfield, tyulmankov2021biological, UniversalHopfieldNetworks}. We briefly provide background on SDM and notation sufficient to relate SDM to MLPs. For a summary of how SDM relates to the cerebellum, see App. \ref{appendix:SDMBioPlausibility}. We use the continuous version of SDM, where all neurons and patterns exist on the $L^2$ unit norm hypersphere and cosine similarity is our distance metric \citep{SDMAttention}. 

\begin{figure}[h]
    \centering
    \begin{center}
            \includegraphics[width = 0.9\linewidth]{figures/SDMOverview.pdf} 
    \end{center}
    \caption{\textbf{Graphical Summary of the SDM Write and Read Operations.} \textbf{Top Row -} Three patterns being written into nearby neurons. \underline{1.} First write operation; \underline{2.} Patterns are stored inside neurons and the original pattern address is shown; \underline{3.} Writing a second pattern; \underline{4.} Writing a third pattern and neurons storing a superposition of multiple patterns. \textbf{Bottom Row -} The SDM read operation. The query reads from nearby neurons with the inset showing the number of times each pattern is read. Blue is read the most as its original pattern location (shown by the hollow circle) is closest to the query. Fig. adapted with permission from \cite{SDMAttention}. }
\label{fig:SDMSummary}
\end{figure}

SDM randomly initializes the addresses of $r$ neurons on the $L^2$ unit hypersphere in an $n$ dimensional space. These neurons have addresses that each occupy a column in our address matrix $X_a \in (L^2)^{n\times r}$, where $L^2$ is shorthand for all $n$-dimensional vectors existing on the $L^2$ unit norm hypersphere. Each neuron also has a storage vector used to store patterns represented in the matrix $X_v \in \mathbb{R}^{o\times r}$, where $o$ is the output dimension. Patterns also have addresses constrained on the $n$-dimensional $L^2$ hypersphere determined by their encoding; encodings can be as simple as flattening an image into a vector or as complex as preprocessing with a deep learning model. 

Patterns are stored by activating all nearby neurons within a cosine similarity threshold $c$, and performing an elementwise summation with the activated neurons' storage vector. Depending on the task at hand, patterns write themselves into the storage vector (e.g., during a reconstruction task) or write another pattern, possibly of different dimension (e.g., writing in their one hot label for a classification task). This write operation and the ensuing read operation are summarized in Fig. \ref{fig:SDMSummary}.

%Patterns need to write themselves into neurons because we assume, like in the real-world, that they are ephemeral, while neurons persist across time. 
Because in most cases we have fewer neurons than patterns, the same neuron will be activated by multiple different patterns. This is handled by storing the pattern values in superposition via the aforementioned elementwise summation operation. The fidelity of each pattern stored in this superposition is a function of the vector orthogonality and dimensionality $n$. 

Using $m$ to denote the number of patterns, matrix $P_a \in (L^2)^{n\times m}$ for the pattern addresses, and matrix $P_v \in \mathbb{R}^{o\times m}$ for the values patterns want to write, the SDM write operation is: 
%
\begin{align}
    \label{eq:SDMWriteMatrix}
    X_v = P_v b \big ( P_a^T X_a \big ), \qquad
    b(e)=
    \begin{cases}
      1, & \text{if}\ e \geq c \\
      0, & \text{else},
    \end{cases}
\end{align}
%
where $b(e)$ performs an element-wise binarization of its input to determine which pattern and neuron addresses are within the cosine similarity threshold $c$ of each other. 

Having written patterns into our neurons, we read from the system by inputting a query $\boldsymbol{\xi}$, that again activates nearby neurons. Each activated neuron outputs its storage vector and they are all summed elementwise to give a final output $\mathbf{y}$. The output $\mathbf{y}$ can be interpreted as an updated query and optionally $L^2$ normalized again as a post processing step: 
%
\begin{align}
    \label{eq:SDMReadMatrix}
    \mathbf{y} = X_v b \big ( X_a^T \boldsymbol{\xi} \big ).
\end{align}
%
Intuitively, SDM's query will update towards the values of the patterns with the closest addresses. This is because the patterns with the closest addresses will have written their values into more neurons that the query reads from than any competing patterns. For example, in Fig. \ref{fig:SDMSummary}, the blue pattern address is the closest to the query meaning that it appears the most in those nearby neurons the query reads from. SDM is closely related to modern Hopfield networks \citep{modernHopfield, hopfieldallyouneed, HopfieldBioKrotov} if they are restricted to a single step update of the recurrent dynamics \citep{tyulmankov2021biological, SDMAttention, UniversalHopfieldNetworks}. 

\section{Translating SDM into MLPs for continual learning}
\label{sec:SDMtoMLP}

A one hidden layer MLP transforms an input $\boldsymbol{\xi}$ to an output $\mathbf{y}$. Using notation compatible with SDM and representing unnormalized continuous vectors with a tilde, we can write the MLP as: 
%
\begin{align}
    \label{eq:MLPdef}
    \mathbf{y} = \tilde{X}_v f( \tilde{X}_a^T \tilde{\boldsymbol{\xi}} + \tilde{\mathbf{b}}_a ) + \tilde{\mathbf{b}}_v,
\end{align}
%
where $\tilde{X}_a \in \mathbb{R}^{n \times r}$ and $\tilde{X}_v \in \mathbb{R}^{o \times r}$ are weight matrices corresponding to our SDM neuron addresses and values, respectively. Meanwhile, $\tilde{\mathbf{b}}_a \in \mathbb{R}^r$, $\tilde{\mathbf{b}}_v \in \mathbb{R}^o$ are the bias parameters and $f(\cdot)$ is the activation function used by the MLP such as ReLU \citep{ReLUBengio}. 

Using this SDM notation for the MLP, it is trivial to see the similarity between the SDM read Eq. \ref{eq:SDMReadMatrix} and Eq. \ref{eq:MLPdef} for single hidden layer MLPs that was first established in \citep{SDMBookChapter1993}. However, the fixed random neuron address of SDM makes it unable to effectively model real-world data. Using ideas from \cite{SDMvsHopfield}, we resolve this inability with a biologically plausible inhibitory interneuron that is approximated by a Top-K activation function. App. \ref{appendix:SDMTopK} explains how SDM is modified and the actual Top-K activation function used is presented shortly in Eq. \ref{eq:KSubtract}. This modification makes SDM compatible with an MLP that has no fixed weights. As for the SDM write operation of Eq. \ref{eq:SDMWriteMatrix}, this is related to an MLP trained with backprop as outlined in App. \ref{appendix:SDMWriteOperationMLP}. 

%\begin{figure}[h]
%    \begin{center}
%            \includegraphics[scale=0.15]{figures/Frame_67.pdf}
%    \caption{ SDM implemented as a single hidden layer MLP. The only differences between our improved SDM %network and an MLP are: $L^2$ normalization of the input and weights, no bias term, positive weights, %and Top-K activation across all of the hidden layer units as the activation function. Fig. and the %initial connection to MLPs adapted from \citep{SDMBookChapter1993}.}
%    \label{fig:SDMFFNInterpretation}
%    \end{center}
%\end{figure}

%A core assumption of SDM that deviates from MLPs is that its neuron addresses are fixed. This limits SDM's representational capacity by making it ineffective at learning the manifolds of correlated, real-world data. Motivated by ideas from \citep{SDMvsHopfield}, we set out to resolve this limitation in a biologically plausible way that leverages the existence of inhibitory interneurons.

\textbf{The Dead Neuron Problem -} An issue with the Top-K activation function is that it creates dead neurons \citep{SparseFFNumenta, competitiveLearning, Frey, SwitchTransformer}. Only a subset of the randomly initialized neurons will exist closest to the data manifold and be in the top $k$ most active, leaving all remaining neurons to never be activated. In the continual learning setting, when a new task is introduced, the model has fewer neurons that can learn and must overwrite more that were used for the previous task(s), resulting in catastrophic forgetting.

Rather than pre-initializing our neuron weights using a decomposition of the data manifold \citep{competitiveLearning, UMAP, Strang1993IntroductionTL} we ensure that all neurons are active at the start of training so that they update onto the manifold. This approach has been used before but in biologically implausible ways (see App. \ref{appendix:ImplausibleDeadNeuronSolutions}) \citep{Frey, SparseFFNumenta, VQVAE}. We instead extend the elegant solution of \cite{GabaSwitch} by leveraging the inhibitory interneuron we have already introduced. After neurogenesis, when neuronal dendrites are randomly initialized, neurons are excited by the inhibitory GABA neurotransmitter (see App. \ref{appendix:GABASwitchBiology}). This means that the same inhibitory interneuron used to enforce competition via Top-K inhibition at first creates cooperation by exciting every neuron, allowing them all to learn. As a result, every neuron first converges onto the data manifold at which point it is inhibited by GABA and Top-K competition begins, forcing each neuron to specialize and tile the data manifold.

We present the full GABA switch implementation in App. \ref{appendix:GABASwitchConsiderations} that is the most biologically plausible. However, using the positive weight constraint that makes all activations positive, we found a simpler approximation that gives the same performance. This is by linearly annealing our $k$ value from the maximum number of neurons in the layer down to our desired $k$. A similar approach without the biological justification is used by \cite{Frey} but they zero out all values that are not the $k$ most active and keep the remaining $k$ untouched. Motivated by our inhibitory interneuron, we instead subtract the $k+1$-th activation from all neurons that remain on. This change leads to a significant boost in continual learning (see Table 2 and App. \ref{appendix:GABASwitchConsiderations}). Formally, letting the neuron activations pre inhibition be $\mathbf{a}\coloneqq X_a^T \boldsymbol{\xi}$ and $\mathbf{a}^*$ post inhibition:
%
\begin{align}
\label{eq:KSubtract}
    \mathbf{a}^* &= [\mathbf{a} - I]_+ \\
    I &= \text{descending-sort}([\mathbf{a}]_+)_{(k_t+1)} \nonumber \\
    k_t &= \max \big (  k_{\text{target}}, \floor{k_{\text{max}} -E_t (k_{\text{max}} - k_{\text{target}})/s}  \big ), \nonumber
\end{align}
%
where $[\cdot]_+$ is the ReLU operation, $\floor{\cdot}$ is the floor operator to ensure $k_t$ is an integer, and descending-sort($\cdot$) sorts the neuron activations in descending order to find the $k+1$-th largest activation. $E_t$ is an integer representing the current training epoch and subscript $t$ denotes that $k_t$ and $E_t$ change over time. Hyperparameter $s$ sets the number of epochs for $k$ to go from its starting value $k_\text{max}$, to its target value $k_\text{target}$. When $k_t = k_\text{target}$ our approximated GABA switch is fully inhibitory for every neuron. An algorithm box in App. \ref{appendix:SDMTrainAlgo} summarizes how each SDMLP component functions. 

\textbf{The Stale Momentum Problem -} 
We discovered that even when using the GABA switch, some optimizers continue killing off a large fraction of neurons, to the detriment of continual learning. Investigating why, we have coined the ``stale momentum'' problem where optimizers that utilize some form of momentum (especially Adam and RMSProp) fail to compute an accurate moving average of previous gradients in the sparse activation setting \citep{Kingma2015AdamAM, RMSProp}. Not only will these momentum optimizers update inactive neurons not in the Top-K, but also explode gradient magnitudes when neurons become activated after a period of quiescence. We explain and investigate stale momenta further in App. \ref{appendix:StaleMomentum} and use SGD without momentum as our solution. 

\textbf{Additional Modifications -} There are five smaller discrepancies (expanded upon in App. \ref{appendix:SDMAdditionalModifications}) between SDM and MLPs: (i) using rate codes to avoid non differentiable binary activations; (ii) using $L^2$ normalization of inputs and weights as an approximation to contrast encoding and heterosynapticity \citep{PrinciplesNeuralDesign, competitiveLearning, SleepHeterosynapticity}; (iii) removing bias terms which assume a tonic baseline firing rate not present in cerebellar granule cells \citep{GranuleCellsNoTonic, WangCerebellarGranuleCells}; (iv) using only positive (excitatory) weights that respect Dale's Principle \citep{Dale1935PharmacologyAN}; (v) using backpropagation as a more efficient (although possibly non-biological \citep{BrainBackpropLilicrap}) implementation of Hebbian learning rules \citep{Krotov2019UnsupervisedLB, SimilarityMatchingManifold, GabaSwitch}.

%\textbf{Modifications Summary -} To summarize, we have shown how an MLP can be modified in biologically plausible ways to align with a version of SDM that can learn its neuronal addresses. This is accomplished with an inhibitory interneuron that implements the Top-K operation \citep{SDMvsHopfield}, using both the GABA switch and avoidance of stale momentum to resolve the dead neuron problem \citep{GabaSwitch}.

\section{SDM Avoids Catastrophic Forgetting}
\label{sec:SDMCatastrophicForgetting}

Here we show that the proposed SDMLP architecture results in strong and organic continual learning. In modifying only the model architecture, our approach is highly compatible with other continual learning strategies such as regularization of important weights \citep{EWC, MAS, SISynapticIntelligence} and memory replay \citep{HypothesisDrivenStreamLearningKreiman, ContinualLearningBaselines}, both of which the brain is likely to use in some fashion. We show that combinations of SDMLP with weight regularization are positive sum and do not explore memory replay.  
%What is compelling about these results is that continual learning naturally emerges without any additional modifications to the model such as memory replay or using any task information (neither task labels nor boundaries are given or inferred). The model is also able to outperform elastic weight consolidation, which uses task information but these training approaches are not mutually exclusive and the best performance is achieved when they are combined.

\textbf{Experimental Setup -} Trying to make the continual learning setting as realistic as possible, we use Split CIFAR10 in the class incremental setting with pretraining on ImageNet \citep{Russakovsky2015ImageNetLS}. This splits CIFAR10 into disjoint subsets that each contain two of the classes. For example the first data split contains classes 5 and 2 the second split contains classes 7 and 9, etc. CIFAR is more complex than MNIST and captures real-world statistical properties of images. The class incremental setting is more difficult than incremental task learning because predictions are made for every CIFAR class instead of just between the two classes in the current task \citep{ContinualLearningBaselines, YarinGalRobustContL}. Pretraining on ImageNet enables learning general image statistics and is when the GABA switch happens, allowing neurons to specialize and spread across the data manifold. In the main text, we present results where our ImageNet32 and CIFAR datasets have been compressed into 256 dimensional latent embeddings taken from the last layer of a frozen ConvMixer that was pre-trained on ImageNet32 (step \#1 of our training regime in Fig. \ref{fig:TrainingRegimeAndResults}) \citep{PatchesAllYouNeed, Russakovsky2015ImageNetLS}.\footnote{This version of ImageNet has been re-scaled to be 32 by 32 dimensions and is trained for 300 epochs \citep{Russakovsky2015ImageNetLS}.}
%


\begin{figure}[h]

\begin{subfigure}[b]{0.45\textwidth}
  \centering
    \includegraphics[width=0.9\linewidth]{figures/Frame_rename114.pdf}
    %\caption*{\textbf{Three-step Training Regime.} Green outlines denote the module currently being trained. Gray hatches on the ConvMixer denote when it is frozen.}
  \label{fig:TrainingRegimeAndResults.}
\end{subfigure}
\hspace{0.5em}    
\begin{subtable}[b]{0.45\textwidth}
%-----------
  \centering
  %\caption{Split CIFAR10 Performance}
   \begin{tabular}[b]{llll}%[t]
        \toprule
        Method     & Neurons & $k$ & Val. Acc. \\
        \midrule
        SDMLP & 10K  & 10 &  \underline{0.71}  \\
        FlyModel & 10K & 32 &  \dashuline{0.82}   \\
        MAS & 10K  & NA &  0.67   \\
        SI & 10K  & NA &  0.44   \\
        ReLU & 10K  & NA &  0.21   \\
        EWC & 10K  & NA &  0.65   \\
        SDMLP+MAS & 10K  & 10 &  0.84   \\
        SDMLP+EWC & 10K  & 10 &  \textbf{0.86}   \\
        \midrule
        Oracle & 10K  & NA &  0.93   \\
        \bottomrule
      \end{tabular}

  %\caption*{\textbf{Split CIFAR10 Final Validation Accuracy.} We highlight the best performing \underline{SDMLP}, \dashuline{baseline}, and \textbf{overall performer} in the 10K neuron setting. Oracle was trained on the full CIFAR10 dataset. All results are the average of 5 random task splits.}
  \label{table:CIFAR10Results}
  \end{subtable}
\caption{\textbf{Left}, the three-step training regime. Green outlines denote the module currently being trained. Gray hatches on the ConvMixer denote when it is frozen. Table 1: \textbf{Right}, Split CIFAR10 final validation accuracy. We highlight the best performing \underline{SDMLP}, \dashuline{baseline}, and \textbf{overall performer} in the 10K neuron setting. Oracle was trained on the full CIFAR10 dataset. All results are the average of 5 random task splits. }
\label{fig:TrainingRegimeAndResults}
\end{figure}




Using the image embeddings, we pre-train our models on ImageNet (step \#2 of Fig. \ref{fig:TrainingRegimeAndResults}). We then reset $X_v$ that learns class labels and switch to continual learning on Split CIFAR10, training every model for 2,000 epochs on each of the five data splits (step \#3 of Fig. \ref{fig:TrainingRegimeAndResults}). We use such a large number of epochs to encourage forgetting of previous tasks. We test model performance on both the current data split and every data split seen previously to assess forgetting. The CIFAR dataset is split into disjoint sets using five different random seeds to ensure our results are independent of both data split ordering and the classes each split contains.

We use preprocessed image embeddings because single hidden layer MLPs struggle to learn directly from image pixels. This preprocessing is also representative of the visual processing stream that compresses images used by deeper brain regions \citep{Wang_Pisano2020HomologousOO, DrosophilaVisualInputMB, DiCarloCNN}. However, we consider two ablations that shift the validation accuracies but not the rank ordering of the models or conclusions drawn: (i) Removing the ConvMixer embeddings and instead training directly on image pixels (removing step \#1 of Fig. \ref{fig:TrainingRegimeAndResults}, App. \ref{appendix:CIFAR10Raw}); (ii) Testing continual learning without any ImageNet32 pre-training (removing step \#2 of Fig. \ref{fig:TrainingRegimeAndResults}, App. \ref{appendix:NoPretraining}). We also run experiments for CIFAR100 (App. \ref{appendix:CIFAR100}), MNIST (App. \ref{appendix:SplitMNIST}) and FashionMNIST (App. \ref{appendix:SplitFashionMNIST}).


%(iii) Training the ConvMixer with an SDM output layer from scratch on ImageNet32 pixels jointly by combining steps 1 and 2 of Fig. 2 and then finetuning the whole model on Split CIFAR10 in step 3 (App. \ref{appendix:SDM_ConvMixer}).

\textbf{Model Parameters and Baselines -} All models are MLPs with 1,000 neurons in a single hidden layer unless otherwise indicated. When using the Top-K activation function, we set $k_\text{target}=10$ and also present $k_\text{target}=1$. We tested additional $k$ values and suggest how to choose the best $k_\text{target}$ value in App. \ref{appendix:OptimizedTopK}. Because the $k$ values considered are highly sparse -- saving on FLOPs and memory consumption -- we also evaluate the 10,000 neuron setting which improves SDM and the FlyModel continual learning abilities in particular. 

The simplest baselines we implement are the ReLU activation function, L2 regularization, and Dropout, that are all equally organic in not using any task information \citep{GoodfellowContLearning}. We also compare to the popular regularization based approaches Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Synaptic Intelligence (SI) \citep{EWC, MAS, SISynapticIntelligence, RehersalFreeContLearning}. These methods infer model weights that are important for each task and penalize updating them by using a regularization term in the loss. To track each parameter, this requires at least doubling memory consumption plus giving task boundaries and requiring a coefficient for the loss term.\footnote{We acknowledge that task boundaries can be inferred in an unsupervised fashion but this requires further complexity and we expect the best performance to come from using true task boundaries \citep{Aljundi2019TaskFreeCL}.}  %This regularization term needs a hyperparameter that trades off stability of performance on the current task with plasticity for learning future tasks. 

In the class incremental learning setting, it has been shown that these regularization methods catastrophically forget \citep{ContinualLearningBaselines, Gurbuz2022NISPANS}. However, we found these results to be inaccurate and get better performance through careful hyperparameter tuning. This included adding a $\beta$ coefficient to the softmax outputs of EWC and SI that alleviates the problem of vanishing gradients (see App. \ref{appendix:betaEWC}).We are unaware of this simple modification being used in prior literature but use it to make our baselines more challenging.

% of \cite{FruitFlyContLearning} that implements the original SDM algorithm 
We also compare to related work that emphasizes biological plausibility and sparsity. The FlyModel performs very well and also requires no task information \citep{FruitFlyContLearning}. Specific training parameters used for this model can be found in App. \ref{appendix:FlyModelParams}. Additionally, we test Active Dendrites \citep{NumentaCatastrophicForgetting} and NISPA (Neuro-Inspired Stability-Plasticity Adaptation) \citep{Gurbuz2022NISPANS}. However, these models both use the easier task incremental setting and failed to generalize well to the class incremental setting, especially Active Dendrites, on even the simplest Split MNIST dataset (App. \ref{appendix:SplitMNIST}).\footnote{Because we do not test memory replay approaches, we ignore the modified version of NISPA developed for the class incremental setting \citep{Gurbuz2022NISPANS}.} The failure for task incremental methods to apply in a class incremental setting has been documented before \citep{YarinGalRobustContL}. These algorithms are also not organic, needing task labels and have significantly more complex, less biologically plausible implementations. We considered a number of additional baselines but found they were all were lacking existing results for the class incremental setting and often lacked publicly available codebases.\footnote{All of our code and training parameters can be found in our publicly available codebase: \url{https://github.com/TrentBrick/SDMContinualLearner}.}

%\begin{figure}[h]
%    \begin{center}
%            \includegraphics[scale=0.07]{figures/Frame_72.pdf}
%    \caption{\textbf{SDMLP outperforms regularization methods and the combination %is positive sum.}. We plot best performing methods and baselines on Split %CIFAR10, showing the average of 5 random seed with standard error of the %mean. SDMLP+EWC (green line) does the best, then SDMLP (yellow) with MAS %(blue) \citep{MAS} close behind. SDMLP has a different learning curve than the %regularization methods whereby it is more plastic, better learning the new %task, but also more forgetful, gradually forgetting components of previous %tasks. Top-K (orange) without the additional SDM modifications does poorly %as does ReLU model (purple).}
%    \label{fig:FullChart}
%    \end{center}
%\end{figure}
%  SDM has a different learning curve than the regularization methods whereby it is more plastic, better learning the new task, but also more forgetful, gradually forgetting components of previous tasks. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/Frame_90.pdf} 
    \caption{\textbf{SDM outperforms regularization methods and their combination is positive sum.} We plot validation accuracy across tasks for the best performing methods and baselines on Split CIFAR10. SDMLP+EWC (green line) does the best, then the FlyModel (magenta), then SDMLP (yellow) with MAS (blue) close behind. The Top-K (orange) without SDM modifications and ReLU (purple) baselines do poorly. The FlyModel was only trained for one epoch on each task as per \cite{FruitFlyContLearning} but we visually extend the validation accuracy on each task to make method comparison easier. App. \ref{appendix:CIFAR10Extras} visualizes how SDMLP gradually forgets each task compared to catastrophic forgetting of the ReLU baseline. We use the average of 5 random seeds and error bars to show standard error of the mean but the variance is small making them hard to see.}
    \label{fig:FullChartAndForgetting}
\end{figure}

\textbf{CIFAR10 Results -} We present the best continual learning results of each method in Fig. \ref{fig:FullChartAndForgetting} and its corresponding Table 1. SDMLP organically outperforms all regularization baselines, where its best validation accuracy is 71\% compared to 67\% for MAS. In the 1K neuron setting (shown in Table 5 of App. \ref{appendix:CIFAR10Extras}) SDMLP with $k=1$ ties with the FlyModel at 70\%. In the 10K neuron setting, the FlyModel does much better achieving 82\% versus 71\% for SDMLP. However, the combination of SDMLP and EWC leads to the strongest performance of 86\%, barely forgetting any information compared to an oracle that gets 93\% when trained on all of CIFAR10 simultaneously.\footnote{We found that SGDM for the combined SDMLP+EWC does slightly better than SGD and is used here while SGD worked the best for all other approaches.} 

SDMLP and the regularization based methods are complimentary and operate at different levels of abstraction. SDMLP learns what subset of neurons are important and the regularization method learns what subset of weights are important. Because the FlyModel combines fixed weights with Top-K, it can be viewed as a more rigid form of the SDMLP and regularization method combined. The FlyModel also benefits from sparse weights making neurons more orthogonal to respond uniquely to different tasks. We attempted to integrate this weight sparsity into our SDMLP so that our neurons could learn the data manifold but be more constrained to a data subset, hypothetically producing less cross-talk and forgetting between tasks. Interestingly, initial attempts to prune weights either randomly or just using the smallest weights from the ImageNet pretrained models resulted in catastrophic forgetting. We leave more in-depth explorations of weight sparsity to future work. 

\textbf{Ablations -} We ablate components of our SDMLP to determine which contribute to continual learning in Table 2. Deeper analysis of why each ablation fails is provided in App. \ref{appendix:InvestigateContLearning} but we highlight the two most important here: First, the Top-K activation function prevents more than $k$ neurons from updating for any given input, restricting the number of neurons that can forget information. A small subset of the neurons will naturally specialize to the new data classes and continue updating towards it, protecting the remainder of the network from being overwritten. This theory is supported by results in App. \ref{appendix:OptimizedTopK} where the smaller $k$ is, the less catastrophic forgetting occurs. We provide further support for the importance of $k$ by showing the number of neurons activated when learning each task in App. \ref{appendix:InvestigateContLearning}. 

Second, the $L^2$ normalization constraint and absence of a hidden layer bias term are crucial to keeping all neurons on the $L^2$ hypersphere, ensuring they all participate democratically. Without both these constraints, but still using the Top-K activation function, we found that a small subset of the neurons become ``greedy'' in having a larger weight norm or bias term and always out-compete the other neurons. This results in a small number of neurons updating for every new task and catastrophically forgetting the previous one as shown in the Table 2 ablations. Evidence of manifold tiling is shown in Fig. 6 with a UMAP \citep{UMAP} plot fit on the SDM weights (orange) that were pre-trained on ImageNet32 embeddings. We project the embedded CIFAR10 training data (blue) to show that pre-training has given SDM subnetworks that cover the manifold of general image statistics contained in ImageNet32. This manifold tiling is in sharp contrast to the other methods shown in App. \ref{appendix:InvestigateContLearning} and enables SDM to avoid catastrophic forgetting. As further evidence of manifold tiling, we show in the same appendix that SDM weights are often highly interpretable when trained directly on CIFAR10 images.

As an additional ablation to our SDMLP defined by Eq. \ref{eq:SDMReadMatrix}, we find that introducing a bias term in the output layer breaks continual learning. This is because the model assigns large bias values to the classes within the current task over any classes in previous tasks, giving the appearance of catastrophic forgetting. Interestingly, this effect only applies to SDM; modifying all of our other baselines by removing the output layer bias term fails to affect their continual learning performance.%\footnote{These ablations were done on Split MNIST using the experimental setup described in App. F.1.}

% many neurons never specializing or tiling the data manifold and as a result there were no subnetworks present to specialize to each continual learning task. Instead the same
%Table 2 shows how SDM needs all of its components to perform so well. Most notably, if SDM is missing either the $L^2$ normalization of its weights and inputs, or has a bias term, then the number of dead neurons increases dramatically and results in having fewer neurons to tile the manifold and support continual learning (see App. \ref{appendix:InvestigateContLearning}). This is because without both constraints, a subset of neurons can dominate by having larger weight norms or a large bias term, resulting in large activations for a more inputs, thus failing to exist directly on the data manifold and specialize. 
%Finally, the other ablations The Stale momenta of particularly Adam and RMSProp also kill off neurons, leaving fewer to learn distinct subnetworks that are robust to being overwritten (App. \ref{appendix:StaleMomentum}). We show our analysis that supports these conclusions, including comparisons to ReLU and the default Top-K models in App. \ref{appendix:InvestigateContLearning}. 
  
  
\begin{figure}[h]
\begin{subtable}[b]{0.45\textwidth}
%-----------
  \centering
  %\caption{Split CIFAR10 Performance}
  \begin{tabular}{ll}
    \toprule
    %\cmidrule(r){1-2}
    Name & Val. Acc. \\
    \midrule
    SGD & 0.63 \\
    SGDM & 0.54 \\
    Adam & 0.23 \\
    RMSProp & 0.20 \\
    Linear Subtract & 0.63 \\
    Linear Mask & 0.38 \\
    No Linear Anneal & 0.35 \\
    Negative Weights &  0.57 \\
    No $L^2$ Norm & 0.20 \\
    Hidden Layer Bias Term & 0.20 \\
    Output Layer Bias Term & 0.20 \\
    \bottomrule
  \end{tabular}
  \caption*{Table 2: \textbf{SDMLP Ablations.} Validation accuracy for ablated versions of SDMLP on CIFAR10 embeddings and $k=10$ with 1,000 neurons (the same task as Table 1). See main text for details.}
  \label{table:SDMAblations}
  \end{subtable}
  \hspace{0.5em}
%---------
  \begin{subfigure}[b]{0.45\textwidth}
  \centering
    \includegraphics[width=1.0\linewidth]{figures/EmbeddingDataOnWeightsSDM.pdf}
    \caption*{Fig. 6: \textbf{SDM learns the data manifold.} UMAP representation of CIFAR10 embeddings (blue) and SDM weights (orange) pretrained on ImageNet32. The overlap between weights and data is lacking in other methods (see App. \ref{appendix:InvestigateContLearning}).}
    \label{fig:SDM_ManifoldTilingDataOnWeights}
    \end{subfigure}
%----------
%\centering
%  \begin{subfigure}[b]{0.3\textwidth}
%  %\caption{Training Regime}
%  \centering
%  \includegraphics[scale=0.062]{figures/Neurons_1K.pdf}  
%  \caption*{Fig. 5: \textbf{SDM neurons are highly interpretable}.}
%  \label{fig:Interpretability}
%  \end{subfigure}
%--------
%\caption{ }
\label{fig:InterpretabilityAndSDMAblations}
\end{figure}
% We take the trained weights of randomly chosen neurons from an SDMLP with 1,000 neurons directly trained on CIFAR10 pixels, and reshape them into their image dimensions to observe that they correspond to not only specific image classes but even specific examples within the class, analogous to the hypothetical ``grandmother neuron'' \citep{Grandma, JenniferAniston}. Here a bird, frog, and multiple deer or horses are all visible.

\section{Discussion}
\label{sec:Discussion}

Setting out to implement SDM as an MLP, we introduce a number of modifications resulting in a model capable of continual learning. These modifications are biologically inspired, incorporating components of cerebellar neurobiology that are collectively necessary for strong continual learning. Our new solution to the dead neuron problem and identification of the stale momentum problem are key to our results and may further generalize to other sparse neural network architectures. 
%The Hardware Lottery \citep{HardwareLottery} documents instances across the history of AI where hardware, rather than ideas, set research agendas. It is possible that the inability of our hardware accelerators to leverage unstructured sparsity has caused not only neglect of sparsity's advantages but also path dependencies in our deep learning best practices that favour dense models \citep{GPUSparsity, GPUSparsity2}. This is because our sparse models are currently constrained to use the same number of weights as fully dense models, but with a lower dimensional representation that can reduce validation accuracy on the current training task. Moreover, our most popular deep learning benchmarks, such as ImageNet, often neglect metrics beyond validation accuracy where advantages of sparsity have begun to appear \citep{TopKRobustness, SimilarityMatchingManifold, Krotov2019UnsupervisedLB}. The stale momentum problem affecting popular optimizers like Adam is an example of this path dependency favouring dense models \citep{Kingma2015AdamAM}. 

Being able to write SDM as an MLP with minor modifications is interesting in light of SDM's connection to Transformer Attention \citep{SDMAttention}. This link converges with work showing that Transformer MLP layers perform associative memory-like operations that approximate Top-K by showing up to 90\% activation sparsity in later layers \citep{FeedForwardMemoryDeepAnalysis, FeedForwardMemoryEmpirical, elhage2022solu}. Viewing both Attention and MLPs through the lens of SDM presents their tradeoffs: Attention operates on patterns in the model's current receptive field. This increases the fidelity of both the write and read operations because patterns do not need to rely on neurons that poorly approximate their original address location and store values in a noisy superposition. However, in the MLP setting, where patterns are stored in and read from neurons, an increase in noise is traded for being able to store patterns beyond the current receptive field. This can allow for learning statistical regularities in the patterns' superpositions to represent useful prototypes, likely benefiting generalization \citep{SchmidMemoryMLPAttention}.

%PROBABLY REMOVE THIS PARAGRAPH? Beyond Transformers, there is ample evidence of sparse weights and activations in neural networks. It is interesting that ReLU as the only truly sparse activation function, while at risk of the dead neuron problem, continues to perform incredibly well in state of the art models and against ablations like GeLU \citep{MLPLinearConv, FeedForwardMemoryDeepAnalysis, ReLUBengio, ConvMixer}. We leave it to future work to better formalize the extent to which progress in deep learning methods, from the ReLU, to Dropout and newer Mixture of Experts models, is inadvertently converging with the Top-K activation function \citep{ReLUBengio, DropOut, SwitchTransformer, TransformerHashLayers, MixtureExpertsHinton, Ba2020DeeplySparseSR}. 
% In fact, the ReLU nonlinearity even arises as the optimal solution to an $L^1$ regularized regression problem \citep{Ba2020DeeplySparseSR}.
%SDM shows gradual memory decay when training on new tasks that in the limit of many tasks leads to forgetting. This is reminiscent of the Ebbinghaus forgetting curve used to quantify the rate that human forget information over time \citep{Ebbinghaus}. While human forgetting is a function of not only learning new tasks but also the decay of previous ones, the gradual memory decay displayed by SDM, while shared by EWC, is distinct from other sparsity based methods like PackNet that explicitly enforce a subnetwork and result in absolutely no forgetting while training on other tasks \citep{ EWC, ContLearningSurvey, PackNetContLearning}.

%Our efforts to cast SDM in the deep learning framework as a modified MLP that learns its neuron addresses is a very different approach to that of the FlyModel: learning the data manifold and randomly projecting it represent opposite ends of the data modelling spectrum. The fact that both approaches are so successful at continual learning highlights SDM is a strong continual learner because of both sparse neuron activations and the democratic participation of all neurons. In the FlyModel case, this is enforced by all neurons having the same number and weight strength. In our case, by enforcing L2 normalization and absence of a bias term. We hope by introducing our version of SDM capable of learning the data manifold, that we have set the scene for future work to harnesses the pattern separation abilities of random projections while preserving useful information contained in the data manifold structure.
% Allowing for the neuron addresses to be updated in our model enables it to learn the data manifold instead of creating random projections. This has benefits including making the neuron receptive fields interpretable and learning an approximate distribution of the data density (App. \ref{appendix:InvestigateContLearning}) \citep{SimilarityMatchingManifold}. Additionally, the fact we cast our model in the deep learning framework, including using backpropagation (instead of the FlyModel's biological learning rule), is an attempt to create a bridge between MLPs and SDM, in turn enabling us to make connections back to Transformer Attention \citep{SDMAttention} and the cerebellum \citep{SDM}.  

\textbf{Limitations -} Our biggest limitation remains ensuring that SDM successfully avoids the dead neuron problem and tiles the data manifold in order to continually learn. Setting $k_{\text{target}}$ and $s$ decay rate remain difficult and data manifold dependent (App. \ref{appendix:OptimizedTopK}). Avoiding dead neurons currently requires training SDM on a static data manifold, whether it is image pixels or a fixed embedding. Initial experiments jointly training SDM modules either interleaved throughout a ConvMixer or placed at the end resulted in many dead neurons and failure to continually learn. We believe this is in large part due to the manifold continuing to change over time.%Interestingly, there is some evidence that the brain calibrates the GABA switch progressively along the hierarchy of brain regions \citep{GABASwitchEverywhere}. Earlier regions switch GABA and become static sooner than deeper ones, potentially helping stabilize later manifolds.

%Another hyperparameter that needs tuning is the learning rate of SGD. Momentum based optimizers require less tuning but need to be avoided (App. \ref{appendix:StaleMomentum}) \citep{DLTextbook, Kingma2015AdamAM, RMSProp}. Future work should determine how sparse versions of these optimizers affect stale momenta \citep{SparseAdam}.

Constraining neurons to exist on the data manifold and only allowing a highly sparse subset to fire is also suboptimal for maximizing classification accuracy in non continual learning settings. Making $k$ sufficiently large can alleviate this problem but at the cost of continual learning abilities. However, the ability for hardware accelerators to leverage sparsity should allow for networks to become wider (increasing $r$), rather than decreasing $k$, and help alleviate this issue \citep{GPUSparsity, GPUSparsity2}. Finally, while our improved version of SDM assigns functionality to five different cell types in the cerebellar circuit, there is more of the circuitry to be mapped such as Basket cells and Deep Cerebellar Nuclei \citep{DendriticGating}.

% Finally, there is more work to be done investigating why Top-K subtraction in comparison to masking helps with continual learning but harms accuracy in non continual learning settings.

\textbf{Conclusion -} We have shown that SDM, when given the ability to model correlated data manifolds in ways that respect cerebellar neurobiology, is a strong continual learner. This result is striking because every component of the SDMLP, including $L^2$ normalization, the GABA switch, and momentum free optimization are necessary for these continual learning results. Beyond setting a new ``organic'' baseline for continual learning, we help establish how to train sparse models in the deep learning setting through solutions to the dead neuron and stale momentum problems. More broadly, this work expands the neurobiological mapping of SDM to cerebellar circuitry and relates it to MLPs, deepening links to the brain and deep learning.

\section*{Acknowledgements} 
Thanks to Dr. Beren Millidge, Joe Choo-Choy, Miles Turpin, Blake Bordelon, Stephen Casper, Dr. Mengmi Zhang, Dr. Tomaso Poggio, and Dr. Pentti Kanerva for providing invaluable inspiration, discussions, and feedback. We would also like to thank the open source software contributors that helped make this research possible, including but not limited to: Numpy, Pandas, Scipy, Matplotlib, PyTorch, and Anaconda. This work was supported by NIH Grant R01EY026025, NSF Grant CCF-1231216 and the NSF Graduate Research Fellowships Program. 

\section*{Author Contributions}
\begin{itemize}
    \item Trenton Bricken conceived of the project and theory, conducted all experiments, and wrote the paper with suggestions from co-authors.
    \item Xander Davies co-discovered Stale Momentum with T.B, conducted early investigations of Dead Neurons, reviewed related work, and participated in discussions.
    \item Deepak Singh reviewed robustness and Top-K related work and participated in discussions.
    \item Dmitry Krotov advised on the continual learning experiments, Top-K learning theory, and relations to associative memory models.
    \item Gabriel Kreiman supervised the project providing guidance throughout on theory and experiments.
\end{itemize}


\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}


\appendix
\addcontentsline{toc}{section}{App.} % Add the appendix text to the document TOC
\part{Appendix} % Start the appendix part
\parttoc % Insert the appendix TOC


%\section*{App.}
% https://tex.stackexchange.com/questions/419249/table-of-contents-only-for-the-appendix
%\vspace*{-10mm}
%\tableofcontents


\section{SDM}

\subsection{SDM Training Algorithm}
\label{appendix:SDMTrainAlgo}

Here we present in full the training loop used for the SDMLP.

\begin{algorithm}[H]
    \DontPrintSemicolon
    \SetAlgoLined
    \KwIn{Model weights: $(X_a, X_v)$, Tasks: $(T_1, \dots, T_n)$, Top-K params: $k_\text{max}$, $k_\text{target}$, Train steps: $s$}
    \KwOut{ Trained model weights: $(X_a, X_v)$ }
    $X_v \gets \text{LayerInit}(X_v)$ \tcp*[r]{Reset neuron output weights}
    \For{i in (1,…,n)}{
        $\mathbf{X}, \mathbf{Y} \gets T_i$ \tcp*[r]{Obtain data and class labels for current task}
        \For{j in (1,…,s)}{
	 $\mathbf{x}, \mathbf{y} \sim \mathbf{X}, \mathbf{Y}$ \tcp*[r]{Sample a data point from the task}
	 $\mathbf{x} \gets \mathbf{x}/||\mathbf{x}||$  \tcp*[r]{L2 normalize the data}
            $\mathbf{a} \gets X_a^T \mathbf{x}$  \tcp*[r]{Get neuron activations}
	 $\mathbf{a} \gets \text{Top-K}(\mathbf{a} ) $  \tcp*[r]{Apply Top-K Eq.4}
	 $\hat{\mathbf{y}} \gets X_v^T \mathbf{a}$  \tcp*[r]{Get model predictions}
	 $\nabla_\theta X_a, \nabla_\theta X_v  \gets \text{ComputeGradients}(\hat{\mathbf{y}}, \mathbf{y})$  \tcp*[r]{Compute Loss and Gradients}
	 $X_a, X_v \gets \text{GradientStep}(\nabla_\theta X_a, \nabla_\theta X_v)$ \tcp*[r]{Update model weights}
      $X_a, X_v \gets [X_a]_+, [X_v]_+$ \tcp*[r]{Clamp all weights to be positive}
	 $X_a \gets X_a/||X_a||$ \tcp*[r]{L2 normalize the neuron addresses}
    }
    }
    \Return{$X_a, X_v$}
    \caption{SDMLP Training Algorithm}
    \label{alg:SDMAlgorithm}
\end{algorithm}

\subsection{SDM Biological Plausibility}
\label{appendix:SDMBioPlausibility}

Here we summarize the biological foundations of SDM.\footnote{A more extensive treatment of how SDM may be implemented by the cerebellum can be found in Chapter 9 of the SDM book \citep{SDM} and \citep{SDMBookChapter1993}.} The biological plausibility of our new modifications to SDM that allow it to learn the data manifold and be implemented as a deep learning model can be found in App. \ref{appendix:SDMAdditionalModifications}. The biological plausibility of the GABA switch can be found in App. \ref{appendix:GABASwitchBiology}.

Fig. \ref{fig:SDMBioSummary} overlays SDM notation and operations on the cerebellar circuitry. Mossy Fibers represent incoming queries $\boldsymbol{\xi}$ and pattern addresses $\mathbf{p}_a$ through their firing activity. Each Granule cell represents an SDM neuron $\mathbf{x}$, with its dendrites as the neuron address vector $\mathbf{x}_a$ and post-synaptic connections with Purkinje cells as the storage vector $\mathbf{x}_v$. In branching perpendicularly, each Granule cells axon (called a Parallel Fiber), forms contacts with thousands of Purkinje cells with many granule cells \citep{ParallelFibersandPurkinjeCells}. The strength of each contact represents a counter recording the value of a particular element in the neuron storage vector $(\mathbf{x}_v)_i$, where $i$ indexes this vector element. 

\begin{figure}[h]
    \centering
    \begin{center}
            \includegraphics[width = 0.7\linewidth]{figures/Frame_104.pdf} 
    \end{center}
    \caption{\textbf{SDM Mapping to the Cerebellum.} The operations are as follows: \underline{\#1.} A pattern $\mathbf{p}_a$ (blue) or query $\boldsymbol{\xi}$ (red) enters via firing of the Mossy Fibers (Mo). \underline{\#2.} Mossy fibers activate Granule Cells (Gr), with the Gr dendrites representing $\mathbf{x}_a$. Here we show two different Granule cells and their addresses $\mathbf{x}_a^1$ and $\mathbf{x}_a^2$. These Granule Cells project their axons (called Parallel Fibers (Pa)) upwards and across Purkinje Cells (Pu) where their synaptic connections encode their storage vectors. Each contact with a Purkinje Cell encodes a single element of the storage vector. Here we show one of these elements with the first Purkinje Cell with the subscript 1, $[\mathbf{x}_v^1]_1$ and $[\mathbf{x}_v^2]_1$. We will come back to these. \underline{\#3.} A Golgi (Go) inhbitory interneuron gets input from a number of neuron types including Granule cells, it inhibits these Granule cells with a value of $-I$, implementing an approximate Top-K. \underline{\#4.} If and only if we are writing in a pattern $\mathbf{p}_v$, it comes through a climbing fiber (Cl) that wraps around a single Purkinje Cell to produce large action potentials that induce long term potentiation/depression (LTP/LTD). Here we show the Climbing fiber that writes to the Purkinje Cell representing the first element of the storage vectors with $[\mathbf{p}_v]_1$. \underline{\#5.} The Granule Cells that are firing excite the Purkinje cells in proportion to the strength of their synaptic connections that represent their storage vectors. In the case of writing, this updates these synapses. \underline{\#6.} If we are reading from the system, the Purkinje Cell integrates signals from all activate neurons' storage vectors and determines whether or not to fire, implementing a non-linear activation function that outputs $[\mathbf{y}]_1$ (the first element of the vector in this case). The neuron types not mentioned previously are, going clockwise starting in the top right: St=Stellate cell (inhibitory interneuron) and Ba=Basket cell (inhibitory interneuron). Displayed originally without the SDM notation overlayed as Fig. 3.11 of \citep{SDMBookChapter1993}, reprinted with permission from Pentti Kanerva to whom all rights belong.}
\label{fig:SDMBioSummary}
\end{figure}

During SDM read operations, this Purkinje cell can thus sum over index $i$ of all activated neuron storage vectors and use its firing rate to represent the value of $\mathbf{y}_i$. During SDM write operations, the Climbing Fibers are used to update the Purkinje cell counter of each active neuron with the pattern value at this index position $(\mathbf{p}_v)_i$. The most demanding part of SDM's implementation in a biological circuit that is strikingly satisfied by the cerebellum is the three way interface and specificity that exists between Purkinje cells, Climbing Fibers, and Parallel Fibers. This allows for Granule cells to have storage vectors that are precisely written to and read from. We also provide Fig. \ref{fig:BioLabelledAndCircuitryNeuronBasedReadWriteOperations} to give an additional perspective on the biological mapping. 

While our improved version of SDM assigns functionality to five different cell types in the cerebellar circuit, there is more of the circuitry to be mapped such as the Basket cells and Deep Cerebellar Nuclei \citep{DendriticGating}. There is also additional neuroscientific evidence required to confirm that the cerebellum operates as SDM predicts, for example that granule cells fire sparsely \citep{SDM, SDMAttention, WangCerebellarGranuleCells}. Within the scope of our model, the weakest biological plausibility is how well Top-K actually approximates the Golgi inhibitory interneuron (App. \ref{appendix:ApproxTopK}) and how $L^2$ normalization is implemented (App. \ref{appendix:SDMAdditionalModifications}). 

\begin{figure}[h]
    \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/Frame_8.pdf}
    \caption{}
    \label{fig:NeuronBasedReadWriteOperations}
    \end{subfigure}
    % ----------
    \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/Frame_11.pdf}
    \caption{}
    \label{fig:BioLabelledNeuronBasedReadWriteOperations}
    \end{subfigure}
    
    \caption{ \textbf{Another perspective on the mapping from SDM to the cerebellar circuitry.} \textbf{(a)} The computational circuitry necessary to implement SDM. \textbf{(b)} The biological implementation of SDM where the labels of (a) have been replaced by their biological components. The Purkinje cell is shown as an oval to represent the synaptic contacts with the granule cells. See the text for explanations of the architectural features.}
    \label{fig:BioLabelledAndCircuitryNeuronBasedReadWriteOperations}
    
\end{figure}

\clearpage

\subsection{Learning SDM Neuron Addresses with the Top-K Activation Function}
\label{appendix:SDMTopK}
SDM was built on the assumption that its neuron addresses, $X_a$, are randomly initialized and fixed in place \citep{SDM}. This was for reasons of both biological plausibility and analytical tractability when determining the maximum memory capacity and convergence dynamics of the model (see App. \ref{appendix:AddressDecoding}). However, these random and fixed neuron addresses will often not be within cosine similarity $c$ of the real-world patterns existing on some lower dimensional manifold in the vector space. As a result, these neurons will often never be written to or read from. 

\cite{SDMvsHopfield} outlined how SDM could both retain its theoretical results and allow for neuron addresses to learn the data manifold. This is by replacing the fixed cosine similarity activation threshold $c$ that activates a variable number of neurons, with a variable threshold that activates a fixed $k$ neurons. Intuitively, with random neuron and pattern addresses, a fixed $c$ would result in a pattern or query activating some $k$ neurons in expectation, keeping neuron utilization constant. However, using the same fixed $c$ with non-random addresses would vary the number of neurons being activated and utilized. Over-utilized neurons would store too many pattern values in their storage vector superposition, harming pattern fidelity. Instead, we can achieve the same constant neuron utilization as in the random address setting by varying $c$ such that only $k$ neurons are always activated. %By tiling the data manifold we mean having neuron addresses that are spaced along the manifold defined by the pattern addresses (dataset) in proportion to their density \citep{SimilarityMatchingManifold}. 

%Is the following paragraph a subtopic within SDM as MLP? Should this be a separate subheading. %This section: 
%a. SDM can be written as MLP
%b. Top-K can be implemented via dynamically adjustin c
%c. And there is a way to deal with the dead-neuron problem that is pervasive in sparse models.
%(d. stale momentum?)
%Just trying to keep the same ideas and organize them in a more readable fashion.

How can $c$ be dynamically adjusted to keep a constant $k$ neurons active in a biologically plausible fashion? Via an inhibitory interneuron that creates a negative feedback loop: the more neurons that are active, the more activated the interneuron becomes, and the more it inhibits, keeping only $k$ active at convergence.\footnote{App. \ref{appendix:ApproxTopK} discusses of how well inhibitory interneurons can be approximated by Top-K.} Sparsity inducing interneurons are ubiquitious across layers of cortical processing in the brain, particularly relevant to SDM is the cerebellar Golgi interneuron \citep{Marr1969ATO, SDMvsHopfield, TopKRobustness}. 

\subsection{Why SDM Originally Required Fixed Neuron Addresses}
\label{appendix:AddressDecoding}

In attempting to be biologically plausible, SDM was designed to respect Dale's Principle, whereby a synaptic connection can be either excitatory or inhibitory but not transition between them \citep{Dale1935PharmacologyAN}. Using the original binary vector formulation, neurons could compute if they were near enough to an incoming pattern or query to be activated (within some Hamming distance threshold) through the following steps: 
\begin{enumerate}
    \item Converting the neuron's binary address to bipolar weights $\{+1, -1\}$ corresponding to being excitatory or inhibitory, respectively.
    \item Activating those weights where the incoming query has a 1 in its address and summing over them. This corresponds to how many 1s in the query and neuron address vectors agree -- if the addresses match then an excitatory weight is activated, giving a +1, if they disagree then a negative weight is activated, giving a -1.
    \item Rescaling the binary Hamming distance threshold if the neuron is activated by the total number of 1s in the neuron address. See \citep{SDM, SDMBookChapter1993} for details beyond this summary.
\end{enumerate} 

Formally, the binary neuron address can be converted into weights $w_i = \pm1$ with:
%
\begin{equation}
    \label{eq:dendriticWeight}
    w_i = \big (2*[\mathbf{x}_a^\tau]_i \big )-1, \forall i \in \{1,...,n\}.
\end{equation}
%
Where $\tau$ is used to denote a specific neuron address, the query vector is $\boldsymbol{\xi}$, and the function to determine if the neuron should be activated is $a(\cdot, \cdot)$ that returns a binary action potential. We can write: 
%
$$a(\boldsymbol{\xi}, \mathbf{x}^\tau_a) = I\bigg[ \Big ( \sum_{i=1}^n w_i \xi_i \Big ) > d \bigg]  = I \Bigg[ \bigg ( \sum_{i=1}^n \Big ((2*[\mathbf{x}^\tau_a]_i)-1 \Big ) \xi_i \bigg ) > d \Bigg], $$
%
where there is the action potential threshold $d$ for the neuron to fire and $I[\cdot]$ is the indicator function. The interval of values for address decoding is the number of 1s in the address, $[-\sum_i^n[\mathbf{x}_a^\tau]_i,\sum_i^n[\mathbf{x}_a^\tau]_i]$. By adjusting our $d$ value to account for the possible number of matches, we can implement the Hamming distance threshold for each neuron. 

The potential for changing the magnitude of neuron weights was considered but a change of their sign was banned due to Dale's Principle. It was acknowledged in \citep{SDM} that changing the magnitudes of the neuron weights results in a weighted sum where the matching of some bits in the addresses matters more than others for calculating if the neuron is activated. However, the crucial feature is the sign of the weights, determining if there is a match or mismatch. This sign change is what was banned by Dale's Principle, making the neuron addresses fixed both to their sign and to their specific value by the use of binary vectors that cannot represent nuances in weight magnitude \citep{SDM, SDMBookChapter1993}.

% However, there is an issue with the biological plausibility of the original SDM formulation that when resolved makes learning neuron addresses possible. First, the original SDM assumed that the neuron addresses are dense vectors; this is not the case for the granule cells that they are mapped onto in the cerebellum where each has only $\sim 4$ dendrites \citep{OptimalSynapticConnectivity} (see \citep{Jaeckel1989Hyperplane} for a solution that makes the weights of SDM sparse). 

However, the original SDM approach still in fact violated Dale's Principle by mapping the binary neuron address to bipolar because of what this means for the pre-synaptic input neurons. Consider the 3rd element of the input vector (either a pattern or a query) being on (a 1 value). The neuron that is active and represents this value will not have a mixture of both excitatory and inhibitory efferent connections with the SDM neuron addresses. In the cerebellar mapping, mossy fibers represent the pattern/query inputs and release glutamate. This means they are always excitatory to granule cells and the 0s in a neuron address should correspond to having no weight (no dendritic connection) rather than a negative weight (inhibitory connection). 
%Additionally, non sparse, equal distribution of 1s and 0s allowed in the original binary vector representation implies that these synaptic connections are equally excitatory and inhibitory. 

An additional issue with the original SDM formulation is that the neuron addresses are dense vectors; this is not the case for the granule cells that they are mapped onto in the cerebellum where each has only $\sim 4$ dendrites \citep{OptimalSynapticConnectivity} (see \citep{Jaeckel1989Hyperplane} for a solution that makes the weights of SDM sparse).

Taken together, these issues can be resolved by having the binary neuron addresses be sparse and staying binary rather than becoming bipolar. We can then allow our neuron addresses to use positive real values, simultaneously allowing for changes in weight strengths and respecting Dale's Principle. In this work, we use positive real values but do allow for our weights to be dense, leaving high degrees of weight sparsity to future exploration \citep{Jaeckel1989Hyperplane, Jaeckel1989SCD}.

\subsection{Additional Modifications to SDM}
\label{appendix:SDMAdditionalModifications}

The five modifications made to SDM for it to be implemented in a deep learning framework and that differentiate it from a vanilla MLP are explained in full here. These modifications are: 
\begin{enumerate}
    \item{Using continuous instead of binary neuron activations}
    \item{$L^2$ normalization of inputs and weights}
    \item{No bias term}
    \item{Only positive weights}
    \item{Backpropagation}
\end{enumerate}

\textbf{1. Continuous Activations -} SDM originally modelled neurons as having binary action potentials by using a Heaviside step function. However, this is non-differentiable and we want to use backpropagation for training our model. This can be resolved by viewing a neuron's action potentials over a time interval, referred to in the neuroscience literature as a rate code \citep{TheoreticalNeuro}. We believe this is compatible with SDM, whereby neurons with addresses closer to an incoming pattern or query will receive stronger stimulation and fire more action potentials by having more dendritic connections stimulated by excitatory neurotransmitters.

We can represent neuronal activation as an expected firing rate with a real positive number $a \in [0,t]$ where $t$ is some maximum firing rate. This may look like a rectified tanh function where, due to refractory periods, there are diminishing returns to more stimulation \citep{ReLUBengio}. However, because our neurons are already constrained by $L^2$ normalization to be in the cosine similarity bounds of $[-1,1]$ we simply use a ReLU activation function.  

Implementing weighted read and write operations in our original SDM Eqs. \ref{eq:SDMWriteMatrix} and \ref{eq:SDMReadMatrix}, we would replace our binarizing function $b(\cdot)$ with a weight coefficient proportional to the distance between the input and neuron addresses. We show in App. \ref{appendix:StillSoftmax} that this modification has minimal impact on how SDM weights different patterns. This means it should have minimal effect on memory capacity and is still approximately exponential, maintaining its relationship to Transformer Attention \citep{SDMAttention}.

\textbf{2. $L^2$ Normalization -} SDM requires a valid distance metric to compute if neurons and patterns/queries are sufficiently close to each other to produce an activation. We make SDM continuous so that it is differentiable by following \citep{SDMAttention} and replacing its original Hamming distance with cosine similarity.

In SDM's mapping to the cerebellum (outlined in App. \ref{appendix:SDMBioPlausibility}), mossy fibers represent input patterns/queries and granule cell dendrites represent neuron addresses. It remains to be experimentally established if and how these specific cell types enforce $L^2$ normalization. However, contrast encoding and heterosynapticity are both ubiquitous mechanisms that can approximate $L^2$ normalization for the mossy fiber activations and granule dendrites, respectively \citep{PrinciplesNeuralDesign, competitiveLearning, SleepHeterosynapticity}. From a deep learning perspective, LayerNorm and BatchNorm are both used ubiquitously and can also be viewed as approximations to $L^2$ normalization \citep{BatchNorm, LayerNorm}. 
% We are also aware of evidence that granule cells will regulate the strength of their inputs with one of its four dendritic claws being the primary driver for activations \citep{DendriticClawDriverBias}

\textbf{3. No Bias Term -} In order to have our neuron activations represent cosine similarities, we must remove the bias term. We also remove the bias term from the output layer so that outputs represent only a summation of the neuron storage vectors that are activated. The absence of both of these bias terms is clear in the SDM Eq. \ref{eq:SDMReadMatrix} compared to the MLP Eq. \ref{eq:MLPdef}. 

Bias terms can be viewed biologically as representing a neuron activation threshold (if negative) or a baseline tonic firing rate (if positive). In SDM's cerebellar mapping, granule cells that represent the neurons do not have a tonic baseline firing rate meaning that positive bias terms should not be allowed \citep{GranuleCellsNoTonic, WangCerebellarGranuleCells}. Purkinje cell firing that represents the output layer is much more complex such that keeping or removing the bias term is hard to justify biologically \citep{UpBoundDownBound} but we follow the SDM equation in also removing it. %the removal of the bias term is more difficult to justify \citep{UpBoundDownBound}

While removing positive bias terms from the granule cells fits neurobiology, removing negative bias terms corresponding to an activation threshold is less justified. However, as it relates to learning dynamics and the ability for Top-K to still be approximated, the ordinary differential equations of \citep{GabaSwitch} (summarized in \ref{appendix:ApproxTopK}), still maintain approximately $k$ neurons firing while using activation thresholds. This is because the fewer neurons that are firing, the less the inhibitory interneuron is activated, keeping more neurons active. As a result, and in line with SDM Eq. \ref{eq:SDMReadMatrix}, we make the simplifying assumption of not allowing for negative bias terms either, thus removing bias terms entirely. 

An additional benefit of SDM having no bias term, in conjunction with positive weights, is that all neurons will have positive activations by default. This allows for $k$ annealing instead of the GABA switch that is otherwise needed to inject positive current into each neuron when learning the data manifold (also discussed in App. \ref{appendix:GABASwitchConsiderations}). Having all neurons active also guarantees that there will always be at least $k$ active neurons to use in the Top-K.

It is noteworthy that the removal of the bias terms is seeing a resurgence in state of the art models such as the 540B parameter PaLM Transformer language model \citep{Chowdhery2022PaLMSL}, which noted that removal of bias terms resulted in more stable training.

\textbf{4. Positive Weights -} The connection between mossy fibers and granule cells is excitatory so the weights should be positive. Allowing for both positive and negative weights would violate Dale's Principle \citep{Dale1935PharmacologyAN}. As discussed in the last section, the combination of only positive weights and no bias term gives the added benefit of ensuring that all neuronal activations are positive, allowing for the use of the simpler $k$ annealing algorithm. 

Coincidentally, having only positive weights also gives better continual learning performance as shown in the ablations of Table 2.
%, however, we leave an investigation of why to future work. 
A final benefit is the creation of sparse weights but this advantage has its limitations. In models trained on our ConvMixer embeddings, we found only $\sim$20\% weight sparsity. Meanwhile, when trying to jointly train a ConvMixer with SDM on top, too many weights were set to 0, resulting in failed training runs.

\textbf{5. Backpropagation -} Gradient descent via backpropagation used to train MLPs is likely to be biologically implausible \citep{BrainBackpropLilicrap, DLTextbook}. However, there exist a number of local Hebbian learning rules associated with inhibitory interneurons and manifold tiling that we see as viable alternatives \citep{SimilarityMatchingManifold, Krotov2019UnsupervisedLB, GabaSwitch}. In addition, the cerebellum receives supervised learning signals, via climbing fibers\footnote{It is an open question if the climbing fiber signals encode errors or the target to be learnt but we use ``supervised'' here as a superset of the two and distinct from unsupervised learning without any ``teacher'' signal.}, that are known to be capable of encoding heteroassociative relationships such as an eyeblink response to a tone. This circuitry is considered in \citep{DendriticGating} and presents yet another alternative to backpropagation for training the cerebellum. 
%We leave it to future work to integrate our work with a Hebbian learning algorithm.



\subsection{SDM Write Operation Relation to MLP Backpropagation}
\label{appendix:SDMWriteOperationMLP}

The original SDM write operation directly updates the neuron value vector with the pattern value $\mathbf{x}_v = \mathbf{x}_v + \alpha \mathbf{p}_v$ where $\alpha$ weights the pattern by the amount each neuron was activated. Meanwhile, the backpropgation write operation updates $\mathbf{x}_v$ with the error between the model output and the true class one-hot (using cross entropy loss). During model training all inputs are considered write operations. After training, during inference, all inputs are considered queries that perform the SDM read operation. $\mathbf{p}_a$ corresponds to a CIFAR image and $\mathbf{p}_v$ is a one hot label of its encoding. 

There are a few reasons why this difference is compatible with SDM: 

First, an optimal solution for $\mathbf{x}_v$ that will result in zero error is the one hot encoding used by the original SDM write operation.

Second, the original SDM write is only appropriate when the neuron addresses are fixed and $\mathbf{x}_v$ is the only thing learnt. Otherwise, as neurons update their address $\mathbf{x}_a$, this changes the patterns they are activated by and what $\mathbf{p}_v$ they should store in $\mathbf{x}_v$. Using the backpropagation approach to continuously update $\mathbf{x}_v$ as a function of the patterns it is currently activated by is a viable solution.

Finally, from a biological perspective, the error signal used by backpropagation is a closer approximation to how the cerebellar circuit that SDM maps to updates $\mathbf{x}_v$ (Bidirectional learning in upbound and downbound microzones of the cerebellum, De Zeeuw, 2021). While backpropagation through multiple layers of a deep network has been argued to be biologically implausible, this update to the output layer is directly connected to the error computation making it possible.

In summary, SDM will explicitly write in $\mathbf{p}_v$ while the MLP with backprop will compute a delta between $\mathbf{p}_v$ and the network output but this approach is compatible with the same solution, works better when also learning neuron addresses, and is likely to be more biologically plausible.



\subsection{Rate Code Activations Maintain SDM's Approximation to Transformer Attention}
\label{appendix:StillSoftmax}

It was shown in \citep{SDMAttention} that the Attention operation of Transformers closely approximates the SDM read operation \citep{AttentionAllYouNeed}. This is because the weighting assigned to each pattern in SDM is approximately exponential, resulting in the softmax rule. 

However, this result is derived where SDM has binary activations of neurons when reading and writing, not weighting each pattern by its distance. Here, we show that with linear or exponential pattern weightings, SDM remains a close approximation to Attention. This makes the results of previous work continue to hold in our case where SDM is written as an MLP, giving an interesting relation between MLPs and Attention (see Section \ref{sec:Discussion}).

The algorithm derived in \citep{SDMAttention} for the size of each circle intersection is summarized before showing how it can implement the weighting coefficient and the effects of this weighting. 

As in the original SDM formulation, we are using $n$ dimensional binary vectors with a Hamming activation threshold of $d$ and the distance between the addresses of a query $\boldsymbol{\xi}$ and pattern $\mathbf{p}_a$ is $d_v \coloneqq d(\boldsymbol{\xi},\mathbf{p}_a)$. We can group the elements of $\boldsymbol{\xi}$ and $\mathbf{p}_a$ into two disjoint sets: the $n-d_v$ elements where they agree and the $d_v$ elements where they disagree\footnote{This formulation was first inspired by \citep{Jaeckel1989SCD, Jaeckel1989Hyperplane}.}: 
%
\begin{align}
    \label{eq:demoForIntersection}
    \boldsymbol{\xi} &= [\overbrace{ {1,\dots ,1} \: | \: {0,\dots, 0} }^{n-d_v} | \overbrace{1,\dots ,1  \: | \:  0,\dots, 0}^{d_v}] \\
    \mathbf{p}_a &= [\underbrace{1,\dots ,1 \: | \: 0,\dots, 0}_{a+z} | \underbrace{0,\dots, 0 \: | \: 1,\dots, 1}_{b+c}] \nonumber
\end{align}

Now imagine a third vector, representing a potential neuron address. This neuron has four possible groups that the elements of its vector can fall into: 
\begin{itemize}
    \item $a$ - agree with \emph{both} $\boldsymbol{\xi}$ and $\mathbf{p}_a$
    \item $z$ - disagree with \emph{both} $\boldsymbol{\xi}$ and $\mathbf{p}_a$
    \item $b$ - agree with $\boldsymbol{\xi}$ and disagree with $\mathbf{p}_a$
    \item $c$ - agree with $\mathbf{p}_a$ and disagree with $\boldsymbol{\xi}$
\end{itemize}
We want to constraint the values of $a$, $z$, $b$ and $c$ such that the neuron address exists inside the circle intersection between $\boldsymbol{\xi}$ and $\mathbf{p}_a$. This produces the following constraints:
%
\begin{align*}
    a+b+c+z &=n \\
    a+b &\geq n-d\\
    a+c &\geq n-d\\
    a+z&=n-d_v\\
    b+c&=d_v
\end{align*}
%
Using the notation of \citep{AttentionAllYouNeed}, we can write the total number of neurons that exist in the intersection of the read and write circles as: 
%
\begin{equation}
    \label{eq:SDMCircleIntersectEquationMainText}
     \sum_{a=n-d-\floor{\frac{d_v}{2}}}^{n-d_v} \sum_{c=\max(0,n-d-a)}^{d_v-(n-d-a)} w_{\text{Type}}(a, c, d_v, n) \Bigg ( {{n-d_v} \choose a} \cdot {d_v \choose c} \Bigg ), \nonumber
\end{equation}
%
where introduce the weight coefficient $w_{\text{Type}}()$ that can be: 
\begin{align}
    w_{\text{Binary}}(a, c, d_v, n) &= 1\\
    w_{\text{Linear}}(a, c, d_v, n) &= \frac{a+c}{n}\cdot \frac{a+(d_v-c)}{n}\\
    w_{\text{Exp}}(a, c, d_v, n) &= \exp \Big (-\beta \big (n-(a+c) \big ) \Big ) \cdot \exp \Big (-\beta \big (n-(a+(d_v-c) ) \big ) \Big ),
\end{align}
%
where $w_{\text{Binary}}$ is the original weighting of 1 for everything; $w_{\text{Linear}}$ applies a linear decay from 1 for a perfect vector match to 0 for the maximum distance allowed between vectors; and $w_{\text{Exp}}$ is an exponential decay weighting that uses a $\beta$ coefficient for its decay rate. 

As we will now show, the linear weighting applies the most weight to patterns right in the middle of the pattern and query with a gradual, symmetric decline around this point. Meanwhile, the exponential weighting cancels out to apply a constant weight re-scaling to everything. Letting $x$ represent the distance of a neuron to the read and write vectors it is weighted by, $x \coloneqq (n-z)-(a+c)=b-z$. Because we want to know the weighting coefficient that applies to neurons at all possible distances from the pattern and query, without loss of generality we can set $z=0$ for the analysis that follows:
% care about the weights at all possible vector distances (not the number of possible vectors at each distance),
%
\begin{align}
    w_{\text{Linear}}(a, c, d_v, n) &= \frac{n-x}{n}\cdot \frac{n-((d_v+z)-x)}{n} \nonumber \\
    &= \frac{n-x}{n}\cdot \frac{n-(\mathcal{C}-x)}{n} \nonumber \\
    &= \frac{n^2-x^2-n\mathcal{C}+x\mathcal{C}}{n^2} \nonumber \\
    &= Z+\frac{-x(x-\mathcal{C})}{n^2} \label{eq:WLinearResult}
\end{align}
%
where $\mathcal{C}=(d_v+z)$ is a constant as is $Z=\frac{n^2-n\mathcal{C}}{n^2}$. We can take the first and second derivatives of Eq. \ref{eq:WLinearResult} to know that the most weight is applied at the distance right between the read and write vectors, $\mathcal{C}/2$. This linear weighting applies different weights to neurons at different distances. As a result, the patterns stored in superposition will also have different weightings. However,  empirically, this has a negligible effect on the SDM exponential approximation, as shown in Fig. \ref{fig:SDMApproxAttentionEmpirical}. We hypothesize that this is due to two factors: (i) the difference in weight values is not particularly large; (ii) neurons receiving the largest weights are the most numerous. Therefore, the weighting is correlated with the approximately exponential decay in the number of neurons that exist in the circle intersection as vector distance changes. 

As for the exponential, it cancels to a constant term that depends upon the choice of $\beta$:
%
\begin{align}
    \label{eq:ExpoWeightAnalysis}
    w_{\text{Exp}}(a, c, d_v, n) &= \exp \big (-\beta x \big ) \cdot \exp \big (-\beta (\mathcal{C} - x) \big ) \nonumber \\
    &= \exp \big (-\beta \mathcal{C} \big ).
\end{align}
%
This constant term modification to the weighting of all patterns is then removed by the normalization term in the softmax operation, resulting in no effect on the output. 

We confirm our results empirically for the three optimal SDM hamming distances $d^*$ and $n=64$ dimensional canonical Transformer Attention setting used throughout \citep{SDMAttention} in Fig. \ref{fig:SDMApproxAttentionEmpirical}. 

\begin{figure}[h]
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/5_NormalizedCircleInterWeights_logged=False.pdf}  
    \caption{}
    \label{fig:subim1}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/15_NormalizedCircleInterWeights_logged=False.pdf}
    \caption{}
    \label{fig:subim2}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
    \begin{center}
    \includegraphics[width=0.5\linewidth]{figures/25_NormalizedCircleInterWeights_logged=False.pdf}
    \caption{}
    \end{center}
    \label{fig:subim5}
    \end{subfigure}
    \caption{\textbf{Weighting neuron read and write operations has a negligible effect on SDM.} The Exponential weighting (green) as expected gives the same result as if there was a Binary weighting (blue) making it overlap perfectly. The Linear weighting (orange) results in a negligible difference that is barely visible due to high overlap.}
    \label{fig:SDMApproxAttentionEmpirical}
\end{figure}

\clearpage

\section{GABA Switch}

\subsection{Biologically Implausible Solutions to the Dead Neuron Problem}
\label{appendix:ImplausibleDeadNeuronSolutions}

The dead neuron problem is where neurons are never activated by any input data and are a waste of resources. This is particularly a problem with the Top-K activation function, especially at initialization where it is possible for a small subset of neurons that happen to already exist closest to the data manifold to always be activated and in the Top-K. This means that none of the other neurons are ever activated, and thus never receive weight updates, preventing them from learning the data manifold and becoming useful.

The first set of strategies to solve the dead neuron problem initializes neurons immediately on or near the data manifold, for example, using PCA or SVD \citep{competitiveLearning, UMAP}. Biologically, this would assume that priors on the location of the data manifold are encoded in our genome and pre-determine the structure of neuronal dendrites. However, dendrites often form in a highly dynamic and stochastic way \citep{GABADendriteDevelopment, Purves}.

In addition, this solution requires the genome to store a large amount of information and would limit an organism's adaptability to learn new data manifolds during its development, for example by moving to a new environmental niche. It is popular in deep learning to initialize weights not using the data directly but with optimized strategies such as Xavier \citep{Xavier} that empirically work well, even for the sparse ReLU activation function. However, as evidenced by the continued development of new initialization strategies, this approach remains heuristic and does not generalize across datasets, networks, and training techniques \citep{CarefulWeightInit, CarefulWeightInit2, DLTextbook}. 

The second set of approaches to solve the dead neuron problem ensures that all neurons, independent of their initialization, are active and update to learn the data manifold. This is a major cited reason why ReLU is often avoided \citep{DLTextbook, GeLU}, even though in practice it often does not seem to significantly harm performance \citep{Pedamonti2018ComparisonON, PatchesAllYouNeed}. The Top-K model of \citep{SparseFFNumenta}, VQ-VAE \citep{VQVAE}, and Switch Transformer \citep{SwitchTransformer} all address their dead neuron problems in biologically implausible ways. When using the Top-K activation function, the work of \citep{SparseFFNumenta} artificially ``boosted'' the ranking for each neuron to be in the Top-K, with more inactive neurons getting higher rankings. This solution makes the biologically implausible assumption that neurons fire as a function of how inactive they are and undergo ubiquitous anti-Hebbian plasticity. Meanwhile, the VQ-VAE and Switch Transformer both used terms in their loss function to increase the utilization of ``dead'' code vectors and expert modules, respectively.

\subsection{GABA Switch Biology}
\label{appendix:GABASwitchBiology}

We summarize how the GABA switch works biologically in Fig. \ref{fig:GABASwitchBioSummary} \citep{GabaSwitch}. Neurons are excited by GABA early in development before being inhibited by it due to changes in intracellular $\text{CL}^-$ concentration. After neurogenesis, neurons express NKCC1 which imports $\text{CL}^-$ \emph{into} the cell causing the $\text{CL}^-$ reversal potential to be more positive than resting potential \citep{GabaSwitch, Heigele2016BidirectionalGC}. When GABA is present, $\text{CL}^-$ flows towards its reversal potential, resulting in depolarization. Over time, this GABA activation indirectly results in increased KCC2, which instead pumps $\text{CL}^-$ \emph{out} of the cell \citep{GABASwitchesItself, Connor1987DepolarizationAT}. This makes the $\text{CL}^-$ reversal potential more negative than the resting potential and hyper-polarization during GABA activation. 

\begin{figure}[h]
    \centering
    \begin{center}
            \includegraphics[width = 0.9\linewidth]{figures/Frame_105.pdf} 
    \end{center}
    \caption{\textbf{Summary of GABA Switch Biology.} Here we visualize the ion transporters inside young versus adult neurons and how they change over time. The left side of the figure shows how as a neuron gets activated more times it swaps out the concentration of its ion transporters from NKCC1 (blue) to KCC2 (red). This causes the neuron to go from importing chloride ions (Cl-) to exporting them. The right side of the figure shows the consequence of this transition where the reversal potential of chloride goes from being more positive than the neuron's resting potential -- resulting in GABA being excitatory -- to more negative -- resulting in GABA being inhibitory. }
\label{fig:GABASwitchBioSummary}
\end{figure}

This biological mechanism suggests that our GABA switch implementation in Eq. \ref{eq:GabaSwitch} could be made more sophisticated. Rather than counting up the number of times the neuron is simply active (a binary outcome), it could switch as a function of the actual activation amount that the neuron experiences or size of gradient updates. 

\clearpage

\subsection{GABA Switch Practical Considerations}
\label{appendix:GABASwitchConsiderations}

There are three conclusions from this section on when and how to use the GABA switch to avoid dead neurons, improving continual learning: 
\begin{itemize}
    \item If the positive weight constraint is being used then annealing $k$ gives the same performance as the full GABA switch. If you allow for negative weights then the GABA switch should be used. 
    \item The subtraction operation works much better for continual learning than masking. However, the inverse is true in a non continual learning setting. We provide some initial analysis of why this is the case. 
    \item The value of $s$ for the GABA switch should consider the complexity of the data manifold, optimizer, and learning rate. When in doubt there is no harm in setting $s$ larger aside from requiring more training epochs.  
\end{itemize}

We first give the full GABA switch algorithm that was developed and is used for the SDMLP throughout the paper. We then outline why the $k$ annealing approximation works just as well when we constrain our weights and inputs to be only positive. Next, we discuss why subtracting to enforce Top-K instead of masking leads to better continual learning performance. We conclude with other considerations on how to avoid dead neurons with interactions between the data manifold, learning rate, and hyperparameter $s$.

\textbf{The GABA Switch Algorithm} 

Formally, the GABA switch algorithm is implemented as: 
%
\begin{align}
\label{eq:GabaSwitch}
    a_i^* &= [a_i -\lambda_i I]_+ \nonumber \\
    I &= \text{descending-sort}([\mathbf{a}]_+)_{(k_{\text{target}}+1)} \nonumber  \\
    \lambda_i &= \min \Big(1, \max \big (-1, -1 + 2C_i/s \big ) \Big )
\end{align}
%
This algorithm is slightly more complex than the K annealing one presented in Eq. \ref{eq:KSubtract} of the main text. $C_i$ is a counter for each neuron, recording the number of times that it has been activated since the start of training. $\lambda_i$ linearly increases from -1 to 1 as a function of $C_i$, and $[\cdot]_+$ is the ReLU operation. $s$ is a hyperparameter that determines the number of activations required for this neuron to \emph{switch} from being excitated by GABA to being inhibited. When training on top of the ConvMixer embeddings, we found that $s=250,000$ ensured all neurons could move onto the data manifold, resulting in none being dead. As an example, if we assume all neurons are activated before the GABA switch, with this $s$ value the switch occurs after just 2.5 epochs of training on CIFAR10 (there are 50,000 training examples). However, this value depends significantly upon the learning rate and complexity of the manifold where in the case of training on raw CIFAR10 images, to ensure there are no dead neurons we set $s=5,000,000$ requiring 100 epochs for the GABA switch to occur. However, in this case we did not try to find the lower bound on $s$.

\textbf{Why $k$ annealing works just as well as the GABA switch when weights are positive}

Because of Dale's Principle, we implement SDM with positive weights and inputs. Empirically, this also boosts continual learning performance and introduces weight sparsity\footnote{We found that $\sim 20$\% of weights in two models checked (trained on the ImageNet32 ConvMixer embedding and directly on pixels) had values less than 0.01 meaning they could likely be pruned, however, we leave further investigations of sparsity including the introduction of $L^1$ losses, etc to future work.} that would increase computational efficiency if unstructured sparsity can be taken advantage of \citep{GPUSparsity}.

\begin{figure}[h]
    \begin{center}
            \includegraphics[scale=0.06]{figures/AnnealVsGABA_DeadNeurons.pdf}
    \caption{\textbf{Linear $k$ Annealing needs Positive Weights}. All lines overlap at the bottom of the plot aside from Linear Annealing when negative weights are allowed (green). The GABA Switch excites neurons that can otherwise be inactivated due to their negative weights. This ensures all neurons update onto the data manifold and are not dead. However, linear $k$ annealing presented in Eq. \ref{eq:KSubtract} does not excite leaving these neurons dead. Training is directly on the CIFAR10 manifold which is more difficult to learn than the ConvMixer embeddings.}
    \label{fig:LinearAnnealNeedsPositive}
    \end{center}
\end{figure}

When positive weights are used, we found no change in the number of dead neurons or continual learning performance using Eq. \ref{eq:KSubtract} linear annealing. This was true for both the ImageNet32 ConvMixer embeddings and raw pixel datasets. However, when negative weights are allowed this was no longer the case and only the full GABA switch algorithm avoided dead neurons. We speculate that this is because when GABA is excitatory, the full algorithm will inject positive activation into neurons that may otherwise have negative activity and not fire thus failing to get a gradient update. Meanwhile, with positive weights and inputs the activation for every neuron is always positive resulting in every neuron being active unless Top-K is enforced. Thus the positive weights and inputs ensure that every neuron receives gradient updates and moves onto the data manifold. In Fig. \ref{fig:LinearAnnealNeedsPositive}, we present this result training directly on CIFAR10 pixels, a more difficult manifold to learn than the ConvMixer embeddings. 

Note that while positive weights allow for linear $k$ annealing, it can also result in all weights be set to 0, leading to dead neurons and model training to fail.


\textbf{Why \emph{subtracting} works better than \emph{masking} for Continual Learning}

Empirically, masking results in much higher validation accuracies during pretraining than subtraction (Fig. \ref{fig:MvsSsubim1}). However, during continual learning, subtraction works better as shown in the ablations of Table 2 and Fig. \ref{fig:MvsSsubim0}. To work out why subtracting is better for continual learning we analyzed the learning dynamics of both in App. \ref{appendix:InvestigateContLearning} and discuss them here. We leave an analysis of why masking works better during pretraining to future work as we do not care about maximizing validation accuracy for our pretraining task here. 

\begin{figure}[h]
    \begin{subfigure}{1.0\textwidth}
    \begin{center}
    \includegraphics[width=0.7\linewidth]{figures/MaskVsSubtractTesting.pdf} 
    \caption{}
    \label{fig:MvsSsubim0}
    \end{center}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/Frame_81.pdf}
    \caption{}
    \label{fig:MvsSsubim1}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/Frame_82.pdf}
    \caption{}
    \label{fig:MvsSsubim2}
    \end{subfigure}
    \caption{\textbf{(a)} Subtraction beats Masking for Continual Learning. However, as shown in \textbf{(b)} it is worse for pretraining. During pretraining, neither method creates any dead neurons. \textbf{(c)} During continual learning, across each task, there are fewer dead neurons when subtracting that may explain the better memory retention of previous tasks. Because these plots are not crowded, error bars show the min and max range across the five random continual learning data splits instead of the standard error of the mean.}
    \label{fig:MaskVsSubTest}
\end{figure}

Top-K with subtraction utilizes more neurons during continual learning as shown in Fig. \ref{fig:MvsSsubim2}. This result is supported by App. \ref{appendix:InvestigateContLearning} Fig. \ref{fig:InvestigateEntropies} where there are fewer neurons that are consistently activated and they are more polysemantic, being used for multiple tasks. 

Motivated by the stale momentum results of App. \ref{appendix:StaleMomentum}, we wondered if the larger activation values preserved by masking may lead to more dead neurons when using SGDM as our momentum based optimizer. However, using SGD did not lead to any change in results and in hindsight our gradients being less than 1 means that the results in App. \ref{appendix:StaleMomentum} hold independent of the gradient values.  

This means that the gradients must actually be smaller for subtraction and lead to slower updating. At first glance, the subtraction operation just reduces the activity of the $k$ neurons that remain active by a constant term. This will scale the size of each gradient, equivalent to modifying the learning rate. 

However, the situation is more complex. The activity of the $k$ neurons and their learning rate is conditioned upon the activity of the $k+1$th neuron. If this neuron is almost as active as those in the $k$ subset, all of their activities will be very close to zero. Meanwhile, if the $k+1$th neuron is much less active than the $k$ subset, their activities will remain large and they will receive a larger gradient update. These two situations correspond to the input being in densely and sparsely tiled regions of the data manifold, respectively. 

We can think of this situation as approximating a Bayesian one where we have a mixture of Gaussians defining a posterior distribution over the data manifold. The number of neurons in a region is inversely proportional to the variance in the distribution at that location and thus changes the amount the likelihood of our current data point updates the distribution. In other words, the subtraction operation introduces a dynamic learning rate. 

During continual learning when the task changes and a new subregion of the data manifold is presented, we want those neurons closest to the subregion and only those neurons to update and move onto it. In our stochastic optimization setting, it is possible that neurons not necessarily closest to new subregion are updated towards it instead, leading to an over-allocation of neurons to the new task. We hypothesize that the subtraction operation helps avoid this problem by making the learning rate both lower and dynamic. However, we believe the dynamic learning effect is more influential than stochasticity because even when increasing the batch size from 128 to 10,000, we see that subtraction is still more robust to catastrophic forgetting. 

It is likely that the reduced and dynamic learning rate is a doubled edged sword where while it is better at remembering, it is also worse at learning new information. Both in pretraining and within each data split, the subtraction achieves worse training and validation accuracies. Why the subtraction operation does worse during the pretraining on a single task we leave to future work. 

As an interesting aside, the Top-K subtraction operation implemented by an inhibitory interneuron when $k=1$ is the same as a Vickrey second price auction used in \citep{Chang2020DecentralizedRL} to model neurons as individual agents with their own utility functions. The winning neuron places a ``bid'' represented by its activity amount. It has its activity subtracted by that of the second most active neuron (thus paying the second highest price) and its remaining activity determines its exposure to the next gradient update.

\textbf{Other GABA Switch Considerations - Data Manifolds, Optimizers, and Learning Rates}

Note that for the ConvMixer embedding, dead neurons are not as much of a problem and either the GABA switch or $k$ annealing can avoid any dead neurons within just a few epochs and with or without positive weights. This is because the manifold is much easier to learn as it is lower dimensional both in the size of vectors (256 versus 3072) and the data manifold being far more meaningfully correlated. 

Fig. \ref{fig:SGDLearningRate} shows how a learning rate that is too low will cause SGD to produce dead neurons. The number of neurons dying being inversely proportional to the learning rate and the sudden jump in dead neurons before it plateaus just after the GABA switch both support the theory that the learning rate is too low. This means that neurons do not update quickly enough onto the data manifold before the GABA switch occurs. This sudden jump and plateau in dead neurons is in contrast to Adam and RMSProp optimizers shown in Fig. \ref{fig:StaleGradsSummary} where neurons will continue dying over time due to the stale momentum problems \citep{Kingma2015AdamAM, RMSProp}. Meanwhile, SGDM does not suffer from Stale Momentum problems to the same extent and is much more robust to choice of learning rate making it a good choice for pretraining. However, it is still suffers somewhat from the stale momentum problem in the continual learning setting as shown in the ablations of Table 2. 
% \footnote{We ran this analysis of dead neurons using the validation data which is an upper bound on the number of dead neurons because a number may be activated by training data. However, this shifts the number of dead neurons up by a constant amount and our result presented holds.}

To avoid dead neurons when using SGD either the learning rate can be tuned, the GABA switch value $s$ can be increased, or a sparse optimizer that avoids the stale momentum problem can be used (however, the success of these sparse optimizers remains to be validated in future work). Also keep in mind that the learning rate chosen will be affected by the $L^2$ normalization. 

\begin{figure}[h]
    \begin{center}
            \includegraphics[scale=0.06]{figures/SGDLearningRate.pdf}
    \caption{\textbf{The GABA switch needs to account for the learning rate}. We train SDM directly on CIFAR10 pixels to test its ability to learn the manifold and avoid dead neurons. The GABA switch occurs at epoch $\sim$150 and each line denotes a different learning rate ``lr''. For reasons outlined in App. \ref{appendix:StaleMomentum}, SGD should result in the fewest dead neurons. However, the learning rate needs to be carefully set in relation to the GABA switch threshold, $s$. If the learning rate is too small, neurons won't update onto the data manifold quickly enough and will die instead.}
    \label{fig:SGDLearningRate}
    \end{center}
\end{figure}


\section{Top-K}

\subsection{Inhibitory Interneurons Approximate Top-K}
\label{appendix:ApproxTopK}

At a high level, inhibitory interneurons exist to regulate the firing of excitatory neurons, introducing sparse firing and keeping only the most active on.\footnote{Inhibitory interneurons can also inhibit each other resulting in disinhibition but this is still used to perform more sophisticated forms of excitatory neuron regulation.} This makes the brain likely to be highly sparse in the number of neurons firing at any given time. Estimates suggest that ``an average neuron in the human brain transmits a spike about 0.1-2 times per second.'' \citep{AIImpactsNeuronFiring}, while action potentials and refractory periods happen on a time interval of roughly 10ms \citep{PrinciplesNeuralDesign}. Assuming neurons fire randomly within this 10ms time interval, this gives a back of the envelope calculation that a maximum of 10ms/100ms = 10\%  neurons will fire within the time interval. This aligns well with the prediction of 15\% by \citep{AP1}. Sparse firing also makes sense from the perspective of metabolic costs where action potentials are expensive, consuming $\sim$20\% of the brain's energy \citep{PrinciplesNeuralDesign, AP1, AP2}.

However, in practice, it is unrealistic to assume that an inhibitory neuron can keep exactly $k$ neurons active for any given input because its afferent and efferent synaptic connections are heterogeneous. The inhibitory interneuron sums together the activations from all neurons, weighted by their presynaptic strengths and then outputs an inhibitory value that is  scaled by the strengths of post synaptic connections. The weighted summation of inputs to the interneuron removes information on how many neurons are firing and at what rate. The heterogenous post synaptic connections will weight the effects of inhibitory on each neuron differently.

Keeping these concerns in mind, there are a few reasons that we draw from for optimism that Top-K can be approximated. First, inhibitory horizontal cells in the retina compute the mean activation value of thousands of nearby cone photoreceptors and inhbits them so that they encode contrast (Chapter 13 of \citep{PrinciplesNeuralDesign}). Not only do horizontal cells have carefully tuned weights for their gap junction connections to compute this mean, but also will dynamically change its pre and post synaptic connectivity strengths to accurately compute mean activity under different lighting conditions. In a low light setting, the horizontal cell will average over a larger number of neurons to reduce the variance in its estimate of the mean (page 252 of \citep{PrinciplesNeuralDesign}). If inhibitory interneurons in general come close to the level of sophistication shown by horizontal cells in calibrating their synaptic connectivity, then it is possible the interneuron can compute how much it should inhibit to implement Top-K reliably.

Second, looking at biological evidence of Top-K from cerebellum-like structures, there is strong evidence of Top-K being approximated in the Drosophila Mushroom Body by its Golgi interneuron analog \citep{MBTopK}. In mammals, the evidence is more complicated with some papers finding dense granule cell activations \citep{WangCerebellarGranuleCells}. However, other experimental evidence and theory suggests that granule cell sparsity in the mammalian cerebellum may be a function of task complexity \citep{SilverGranuleDimensions, Xie2022TaskdependentOR}. The lower complexity the task, the more dense the representations can afford to be as there are fewer stimuli that must be unique encoded. Meanwhile, high complexity tasks require a larger number of orthogonal codes resulting in the need for sparser representations that lose fidelity as a result \citep{Xie2022TaskdependentOR}. Ultimately, the density of granule cell firing remains an open question in need of lower latency voltage indicators and the ability to record from more neurons simultaneously across a more diverse range of tasks \citep{SilverGranuleDimensions}. 

\begin{figure}[h]
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/Frame_84.pdf} 
    \caption{}
    \label{fig:subim1PercNeuronsActive}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/Frame_83.pdf}
    \caption{}
    \label{fig:subim2ActivityConvergence}
    \end{subfigure}
    \caption{\textbf{Inhibitory Interneuron Approximates Top-K}. We reproduce the Hebbian learning rule of \citep{GabaSwitch}. \textbf{(a)} We show that it approximates having $k \approx 200$ of the 1000 neurons active for 50 different MNIST inputs. \textbf{(b)} We show an example of the average neuron activity running the ODEs given in Eq. \ref{eq:GABASwitchODE} until convergence.}
    \label{fig:GABASwitchApproxTopK}
\end{figure}

Finally, we tried to model the ability for an inhibitory interneuron to approximate Top-K under the simplifying assumption that its pre and post synaptic weights are fixed and homogenous. We first analyzed the $k$ neurons still on using the Hebbian learning dynamics of \citep{GabaSwitch}. We then looked at our SDMLP during learning and the relationship between the total sum of excitatory activations and the $k+1$-th inhibitory value used in Eq. \ref{eq:KSubtract} to keep only $k$ neurons still on.

The work that introduces the GABA switch \citep{GabaSwitch} uses a Hebbian learning rule with an inhibitory interneuron. An MNIST digit is presented for the network to learn and it obeys the following ordinary differential equations to determine the activities of the excitatory and inhibitory neurons: 

\begin{align}
\label{eq:GABASwitchODE}
\mathbf{e}^0 &= W_\text{inp}^T \mathbf{x} \nonumber \\
\mathbf{i}^0 &= 0 \nonumber \\
\nonumber \\
\mathbf{p}_a^t &= \mathbf{e}^0 + W_{IE} \mathbf{i}^{t-1} \nonumber \\
\mathbf{p}_i^t &= \mathbf{e}^0 + W_{EI} \mathbf{e}^{t-1} \nonumber \\
\nonumber \\
\mathbf{g}_a &= \frac{1}{\tau_a} \big ( -\mathbf{e}^{t-1}+ \tanh{( [(\mathbf{p}_a^t-\mathbf{b})/L ]_+  )} \big ) \nonumber \\
\mathbf{g}_i &= \frac{1}{\tau_i} \big ( -\mathbf{i}^{t-1} +  [(\mathbf{p}_i^t - b_i) ]_+    \big ) \nonumber \\
\nonumber \\
\mathbf{e}^t &= \mathbf{e}^{t-1} + \Delta \mathbf{g}_a \nonumber \\
\mathbf{i}^t &= \mathbf{i}^{t-1} + \Delta \mathbf{g}_i,
\end{align}

where $\mathbf{e}$ is a vector describing excitatory neuron activities, $\mathbf{i}$ the inhibitory neurons, and $\Delta$ is the learning rate. The excitation and inhibition time constants are $\tau_a$ and $\tau_i$, respectively. $L$ is a smoothing term, $\mathbf{b}$ is a firing threshold learnt for each neuron. $W_{IE}$ and $W_{EI}$ are the weight matrices from excitatory to inhibition neurons and vice versa; they are fixed and have homogenous weights. $W_\text{inp}$ are the weights that respond to the input $\mathbf{x}$. $b_i$ is the firing threshold for the inhibitory interneurons that a fixed scalar and can be thought of as resulting in a noisy approximate $k$ value. During training, we use a Hebbian learning rule (not given) to train $W_\text{inp}$ and run the ODEs shown to converge to approximately $k$ neurons remaining on.

These dynamics are simplifications because the weights between excitatory and inhbitory neurons are fixed and homogenous. $W_{EI}$ is initialized where every excitatory neuron is connected to each inhibitory with a  probability of 90\% and strength of 1. $W_{IE}$ is initialized with the same connectivity probability but with a strength of $1/(0.9 r_I)$ where $r_I$ is the number of inhibitory neurons. This makes the synaptic weights connecting inhibitory to excitatory neurons sum to 1 in expectation.

Putting aside these simplifying assumptions, when we run the model and analyse the number of neurons that remain active, it is approximately a constant $k$ neurons as shown in Fig. \ref{fig:subim1PercNeuronsActive}. Given the inhibitory activation threshold $b_i$, approximately 20\% of the neurons remain on for each input giving $k \approx 200$. We also show how the average activity of the excitatory neurons evolves over time in Fig. \ref{fig:subim2ActivityConvergence} to emphasize that the inhibitory interneuron does not have to make a single guess for how to correctly inhibit all but $k$ neurons. Instead, this is a dynamic process defined by the ODEs of Eq. \ref{eq:GABASwitchODE}.

We also looked for our SDMLP at the $k+1$-th highest activity value used by the inhibitory neuron to implement Top-K and how this relates to the total unweighted sum of neuron activations entering the inhibitory interneuron. After the GABA switch when neurons tile the data manifold and form subnetworks, this relationship is largely linear where the $k+1$-th highest activity value is the total sum of all neuron activations before inhibition, divided by the total number of neurons. This is again a simplification by assuming all input and output weights are homogenous and equal to one. However, again, if the interneuron can modify its pre and post synaptic strengths with the same degree of precision displayed by horizontal cells \citep{PrinciplesNeuralDesign} then these results support the possibility of the interneuron approximating Top-K, using a simple function to respond to the sum of its inputs.  
%leverage a combination of Hebbian and anti-Hebbian plasticity rules. 

\subsection{Optimized Top-K}
\label{appendix:OptimizedTopK}

The $k$ value is a parameter which must be chosen in our model and can significantly affect performance. We present empirical results for ablating $k$ on both pretraining and continual learning across a number of different data manifolds. 

To summarize our findings: 
\begin{itemize}
    \item A larger $k$ gives better pretraining but worse continual learning and vice versa.
    \item $k$ depends upon the complexity of the data manifold and number of neurons, $r$.
    \item SDM will approximately tile whatever data manifold it is given. Joint training of SDM with other model components such as a CNN will improve pretraining. However, learning a non static manifold in this joint training setting is difficult for SDM. 
\end{itemize}

\textbf{Optimal $k$ values for Pretraining or Continual Learning?} 

Sparsity in the SDM setting presents a fundamental tradeoff between performing well in continual learning (CL) and non continual learning (NCL) settings. This tradeoff exists because sparsity limits the representational capacity of the model and should be expected to reduce NCL accuracy. Meanwhile, the more sparse the model is, the better it is able to form subnetworks that are not overwritten by future CL tasks. Additionally, SDM will tile the data manifold it is given, which depending on the manifold, can be poorly correlated with maximizing classification accuracy. 

\begin{figure}[h]
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/kValueAblate.pdf} 
    \caption{}
    \label{fig:kValsAblateAbsPerformances}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/kValuePareto.pdf}
    \caption{}
    \label{fig:subim2}
    \end{subfigure}
    \caption{Two perspectives on how the $k$ value influences the ability to learn the ImageNet32 pretraining task and the Split CIFAR10 continual learning task. \textbf{(a)} Validation Accuracies for final NCL ImageNet pretraining (blue) and final CL Split-CIFAR10 (orange) over all tasks as a function of $k$ values. \textbf{(b)} The pareto frontier between ImageNet pretraining accuracy (x-axis) and CIFAR10 continual learning (y-axis). Each blue dot is a different k value. This plot makes evident the better pretraining performance for $k<8$. }
    \label{fig:kValsAblate}
\end{figure}

We see this tradeoff in Fig. \ref{fig:kValsAblate} that summarizes ablating $k$ during pretraining on ImageNet32 embeddings from the ConvMixer and testing on Split CIFAR10. As the $k$ value decreases, continual learning accuracy improves while pretraining accuracy declines. However, it is unclear why performance on the original ImageNet32 training is parabolic, slightly improving for $k<8$. While we are indifferent to the NCL pretraining task here, using it as a way for the neurons to specialize across an arbitrary manifold of real-world images, it is worth acknowledging the performance is worse. That this performance decline is caused by sparsity is even more evident during a single training run as GABA switches and $k$ is slowly annealed. Fig. \ref{fig:subim1TrainDrop} shows this for smaller $k$ values where performance declines. 


To emphasize how the optimal $k$ value changes as a function of the number of neurons and data manifold, we describe a number of additional results: 

For the same ConvMixer embeddings of ImageNet32 pretraining and Split CIFAR10 testing, if we use 10K neurons instead of 1K then there is the same linear decline in pretraining performance as in Fig. \ref{fig:kValsAblate} but $k=5$ outperforms all $k \leq 50$, it is also the best for continual learning. Meanwhile, $k=1$ becomes the worst for pretraining and the second best for continual learning. 

With the same setup again but operating directly on pixels without using the ConvMixer embeddings, $k=10$ is the best for pretraining but $k=1$ is the best at continual learning as shown in Fig. \ref{fig:RawCIFAR10} of App. \ref{appendix:CIFAR10Raw}.

In all of the above cases we are given a static manifold for SDM to learn. This always results in worse pretraining accuracy than an equivalent ReLU MLP, even when using large $k$ values. This is because the ReLU MLP is not constrained to the data manifold, having a bias term and no $L^2$ normalization. This allows it to learn weights and biases that better maximize NCL validation accuracy. 

\begin{figure}[h]
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/ConvSDM_kAblate.pdf} 
    \caption{}
    \label{fig:subim1TrainDrop}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/ConvSDM_DeadNeurons.pdf}
    \caption{}
    \label{fig:subim2DeadNeuronManifoldJointTrain}
    \end{subfigure}
    \caption{\textbf{Sufficiently small $k$ values harm network training}. ConvMixers with an SDM layer at the end trained on CIFAR10. All networks perform the same for the first 20 epochs at which point Top-K is approximately fully implemented (the GABA switch occurs around epoch 10). \textbf{(a)} Validation accuracy for all models. The smaller the $k$ value, the worse accuracy is. \textbf{(b)} The fraction of neurons that are not active for any of the validation inputs (this is an upper bound on the true number of dead neurons that may be inactive for the training data). Training the CNN portion of the network along with the SDM neurons leads to many dead neurons. We think this is because the manifold is still updating. As more neurons die, the representational capacity of the model declines.}
    \label{fig:TrainingConvSDM}
\end{figure}


If we perform joint training of the ConvMixer and SDM module then the ConvMixer can learn to create a manifold for SDM to tile that does maximize validation accuracy, performing on par with the ReLU MLP. This is what happens in a test where we train on the whole CIFAR10 dataset in the NCL setting, as long as $k\geq 250$, but independent of if there are 1,000 or 10,000 neurons in the SDM layer. This result is shown in Fig. \ref{fig:subim1TrainDrop} and suggests that the ReLU network only needs at least 250 neurons in its final layer to backpropagate gradients and achieve high prediction accuracy. There is evidence that artificial neural networks are overparameterized at the start of training and functionally sparse, as supported by results from network pruning, most notably the Lottery Ticket Hypothesis \citep{LotteryTicketHypothesis}.

However, this result is again manifold dependent whereby training instead on ImageNet32, even with $k=2,500$ and 10,000 neurons still harms performance compared to a ReLU network. We believe this is because ImageNet32 has a dramatically more complex data manifold with $\sim $ 1.2M images from 1,000 different classes, compared to 50,000 images in 10 classes for CIFAR10. This means that even a $k$ of 2500 is too small and harms the model's representational capacity. 

An additional difficulty with joint training is the timing of the GABA switch to avoid dead neurons. This is because the manifold SDM learns is dynamically changing over time and we believe it explains the neuron death that is inversely proportional to $k$ shown in Fig. \ref{fig:subim2DeadNeuronManifoldJointTrain}.

We leave it to future work to further investigate optimal $k$ values and joint training versus using frozen, pretrained models. We are optimistic that the best way to resolve the tradeoff from sparsity limiting representational capacity is to make the network layers wider rather than the number of neurons that can be active, $k$, fewer. This is feasible because sparse activations are computationally cheap -- while computing the $k$ most active neurons requires multiplying the input with each neuron (this can be parallelized) and then sorting them, a constant $k<<r$ neurons produce outputs and receive gradient updates. Computing which neurons are active would also be cheaper with sparse weights \citep{Jaeckel1989Hyperplane, SparseFFNumenta}. 


%Interestingly, using Top-K with the subtraction operation instead of masking also sigificantly harms NCL performance but again only on ImageNet32 training. 

%One consideration around joint training of the CNN with SDM versus using a frozen model is that SDM will tile whatever manifold it is given.\footnote{This is always true in a Hebbian learning setting and true when using backpropagation if we assume that all inputs generate a loss that produce gradients to drive neuronal specialization.} With joint training where the manifold that SDM is trying to model is constantly moving, this can result in dead neurons. If the CNN model is frozen, there is no issue with moving manifolds and dead neurons. However, this is where accurate tiling the data manifold and forming subnewtorks benefitial to CL does not maximize validation accuracy in a NCL setting. In this way, joint pretraining can be advantageous as the rest of the network can produce a manifold for SDM to model that also maximizes validation accuracy. We see exactly this result with CIFAR10 and NCL validation accuracy where when we use the ConvMixer embeddings SDM with any $k$ underperforms ReLU. However, as mentioned in the previous paragraph, when the ConvMixer is trained jointly, as long as $k \geq 250$ SDM performs equally.

%Illustrating this, we tried to train ConvMixers on CIFAR10 directly where the last output layer used SDM with various $k$ annealing approaches to solve the dead neuron problem (see Fig. \ref{fig:TrainingConvSDM}). With 1,000 neurons, when $k<250$ it harmed training performance, we believe this may be because the sparse activations introduce a gradient bottleneck making it harder for the network to train. Even with 10,000 neurons, $k<250$ harmed training in the same way supporting the idea of a gradient bottleneck where a certain number of neurons must be active to propagate information to the rest of the network. Moreover, a large number of neurons died which we believe is due to the fact that their input latent embedding is changing over time rather than being frozen and once the GABA switch occurs they no longer have the chance to be re-activated and learn the new manifold that has moved. The rebuttal to this issue is that if the sparsity in computation was taken advantage of, we should be able to have much wider layers and larger $k$ values that retain high levels of sparsity. For example, having $k=250$ with 25,000 neurons, therefore still retaining 99\% sparsity. 


%in that as the ratio between the number of neurons $r$ and number of random patterns $m$ increases, $k$ drops.
% In experiments outlined later where we perform exactly these tests, with a low $\frac{r}{A}$ ratio the specialization bound constrains neurons not to generic digits but generic strokes that make up these digits. However, the theory holds in that combining a larger $k$ number of these sub strokes out performs activating a small $k$ number of generic digits.

\textbf{SDM Theory to derive optimal $k$ values} 

Before concluding this section, we want to flag to readers that SDM, with a few simplifying assumptions, is able to analytically derive optimal $k$ values under a few different conditions of optimality: (i) Optimising the signal to noise ratio of every pattern; (ii) Optimizing for the maximum number of patterns that can be stored, within a certain retrieval probability; (iii) Maximizing the distance a query can be from its target pattern while still converging correctly. These derivations and further discussion be found in \citep{SDMBookChapter1993, SDMAttention}. 

While flagging that these analytical results exist and are interesting, there are all concerned with maximizing the information content of the patterns stored that is most relevant to a reconstruction task. In this paper, we care about two different objective functions somewhat uncorrelated with maximizing information content: (i) Classification accuracy, where better performance can be achieved without trying to model the data manifold; (ii) Continual learning, where while we care about modelling the data manifold, we also want the formation of unique subnetworks that can output the correct classification label. In addition, SDM's analytical results assume that the patterns are random. For correlated real-world datasets, we need ways to quantify the complexity of the data manifold and a full exploration of this problem and solutions is beyond the scope of this work.

\section{Stale Momentum}
\label{appendix:StaleMomentum}

Imagine a neuron is activated for three training batches, then inactive for the next 10 batches, and finally activated two more times. During the first set of activations there are no problems, the neuron will update its weights and its momentum term, using up to date gradient information. During inactivity, problems begin as the inactive neuron will still update its weights and do so using an increasingly out of date momentum term. Problems continue when this neuron is activated again because the momentum term decays slowly and will significantly boost the gradient in an out of date direction. %mention tldr of large batch size and SGD or SGDM? also sparse Adam? 

These stale momenta are especially harmful in the Top-K competitive activation setting where a neuron needs to be the $k$ most active for at least one input to continue receiving gradient updates, otherwise it will permanently die. Using Stochastic Gradient Descent (SGD) that is free from any momentum term removes this problem. SGD with momentum works worse than SGD without momentum, but still better than Adam or RMSProp \citep{Kingma2015AdamAM, RMSProp, DLTextbook}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/DeadNeurons.pdf} 
    \caption{The GABA switch occurs by epoch $\sim$100 and Top-K is fully implemented by $\sim$200 epochs. At this point, the RMS and Adam optimizers begin continuously killing off neurons such that they are never activated for any training or validation data. These results are for training directly on CIFAR10 images. See the text for a discussion of training on ConvMixer embeddings.}
    \label{fig:StaleGradsSummary}
\end{figure}

We discovered the Stale Momentum effect when training our models directly on CIFAR10 images that have a much more complex data manifold than the ConvMixer embeddings. Fig. \ref{fig:StaleGradsSummary} shows that after the GABA switch, when Top-K is fully implemented around epoch 200, Adam and RMSProp continue killing off neurons \citep{Kingma2015AdamAM, RMSProp}. These neurons are dead in that they are never activated by any training or validation data. Our results are robust across learning rates for the different optimizers. 

While these dead neurons from Adam and RMSProp don't appear when pretraining on ImageNet32 embeddings, the Stale Momentum problem still leads to incorrect gradient updates that harm continual learning as shown in the ablations of Table 2. This is because fewer neurons are covering the data manifold and able to retain memories of separate tasks.
%We believe that training on images directly is the ``canary in the coalmine'' for stale momenta because the data manifold for image pixels is far more complex. This is supported by the fact the GABA switch $s$ needs to be much larger e.g. going from 250,000 and occurring in 2.5 epochs to 10,000,000 and occurring in 100 epochs. 
%Maximizing validation accuracy can be independent from traits beneficial to continual learning and the number of dead neurons often has little effect on the pretraining classification task. This is a theme also noted in App. \ref{appendix:OptimizedTopK} with regards to tiling the data manifold. 

Notably, a ReLU network trained with Adam or RMSProp also kills off up to 95\% of its neurons with only minor effects on train and validation accuracy when trained directly on CIFAR10 pixels. SGDM and SGD again do not kill off any neurons for this network.  

\begin{figure}[h]
    %----------
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/DeadNeuronsL2GradMag.pdf}
    \caption{}
    \label{fig:StillUpdating}
    \end{subfigure}
    %----------
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/SpecificNeuron.pdf} 
    \caption{}
    \label{fig:SpecificStaleNeuronsubim1}
    \end{subfigure}
    %----------
    \caption{\textbf{Empirical Stale Momenta Seen During Training.} \textbf{(a)} We take all neurons that are dead during a batch and record the mean $L^2$ magnitude that its weights experience during the update. Adam (teal) has the largest updates to dead neurons. Note that these magnitudes are still 5-10x smaller than the average for alive neurons. SGDM (purple) also updates dead neurons but to a lesser degree. SGD (yellow) and RMSProp (orange) are not visible because they do not apply any updates to dead neurons. We start our x-axis at training step 300k because this is when there are enough dead neurons for their gradient updates to be meaningfully calculated. \textbf{(b)} We track all gradient updates applied to ten random neurons during the course of training and pick a representative to display here. We track gradient updates starting right as the GABA switch occurs around epoch 100 and all training steps from this point on are shown along the x-axis. RMSProp (orange) has the highest variance and magnitude in gradient updates followed by Adam (teal). }
    \label{fig:StaleGradsSpecific}
\end{figure}

We logged the gradients of neuron weights during training to confirm that neurons are receiving gradient updates even while they are dead as shown in Fig. \ref{fig:StillUpdating}. This figure is initially confusing because, while Adam and RMSProp kill off the most neurons in Fig. \ref{fig:StaleGradsSummary}, it is Adam and SGDM that keep updating dead neurons in Fig. \ref{fig:StillUpdating} \citep{Kingma2015AdamAM, RMSProp}. Looking at every gradient update applied to a single neuron right after GABA switches in Fig. \ref{fig:SpecificStaleNeuronsubim1} provides a different perspective. Here, the highest variance and largest gradient magnitudes (as quantified by their $L^2$ norm) are produced by RMSProp and Adam. These gradient spikes appear particularly in cases where the neuron is dead for some time and then activated again. 

To investigate further, we implemented each optimizer in a toy environment tracking a single weight that receives sparse gradient updates to see how the moving averages respond.\footnote{Further details for this toy experiment can be found and reproduced in our github repository in the Jupyter Notebook in the notebooks/ directory with the filename StaleGradients.ipynb} We were able to reproduce the much larger gradient spikes displayed by Adam and RMSProp upon the first few gradient updates after a period of quiescence. These results are shown in Fig. \ref{fig:SGsubim0}, where we show the $\Delta_{\text{Optimizer}}$ term for each optimizer, independent of the learning rate. Initializing their respective terms shown in Eqs. \ref{eq:OptSection} with zeros, we first introduce two gradient updates and then have periods of quiescence before injecting gradient updates. 

In Fig. \ref{fig:SGsubim0} we show four different gradient injection periods and that RMSProp has the largest response, followed by Adam and then SGDM, reproducing the gradient spikes seen in Fig. \ref{fig:SpecificStaleNeuronsubim1}. Fig. \ref{fig:SGsubim1} explains these larger gradients by plotting the Adam and RMSProp numerator and denominators for only two gradient injection periods. Note in particular that for Adam, its numerator $m_t$ (dark orange) declines somewhat quickly, having a decay of $\beta=0.9$, however, its denominator $\sqrt{v_t}$ (light orange) has a much slower decay of $\beta_2=0.999$, staying large for much longer and causing the new gradient inputs to explode in size. 

Finally, Fig. \ref{fig:SGsubim2} quantifies how large the gaps in gradient update magnitude are between the three optimizers and the actual gradient when we vary the time between gradient injections. 

To summarize, we believe it is more the gradient explosions after quiescence rather than updating of neurons currently dead that causes Adam and RMSProp to kill off neurons as in Fig. \ref{fig:StaleGradsSummary}. The denominators present in these optimizers, given in Eqs. \ref{eq:Adam} and \ref{eq:RMSProp} are to blame because they are such slow moving averages and inflate the sparse gradients.

The update rules for SGDM, Adam, and RMSProp are as follows where we let $\lambda$ be the learning rate, $\theta_t$ are the parameters at time $t$, $g_t$ is the gradient, and $\gamma=0.9$ $\beta_1=0.9$, $\beta_2=0.999$, $\alpha=0.99$ are hyperparameters for the various algorithms with their standard values. 

\begin{align}
    \theta_{t} &= \theta_{t-1} - \lambda \Delta_{\text{Optimizer}} \label{eq:OptSection} \\
    \nonumber \\
    \Delta_\text{SGD} &= g_t \nonumber \\
    \nonumber \\
    \Delta_\text{SGDM} &= \gamma \theta_{t-1} + g_t \label{eq:SGDM} \\
    \nonumber \\
    \Delta_\text{Adam} &= \frac{\hat{m}_t}{\sqrt{\hat{v}_t}} \label{eq:Adam} \\
    \hat{m}_t &= \frac{m_t}{1-\beta_1} \nonumber \\
    \hat{v}_t &= \frac{v_t}{1-\beta_2} \nonumber \\
    m_t &= \beta_1 m_{t-1} + (1-\beta_1)g_t \nonumber \\
    v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \nonumber \\
    \nonumber \\
    \Delta_\text{RMSProp} &= \frac{g_t}{\sqrt{v_t}} \label{eq:RMSProp} \\
    v_t &= \alpha v_{t-1} + (1-\alpha)g_t^2 \nonumber
\end{align}

\begin{figure}[h]
    \begin{subfigure}{1.0\textwidth}
    \begin{center}
    \includegraphics[width=0.7\linewidth]{figures/StaleGradSpikes.pdf} 
    \caption{}
    \label{fig:SGsubim0}
    \end{center}
    \end{subfigure}
    %---------------
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/AdamNRMSPropComponents.pdf}
    \caption{}
    \label{fig:SGsubim1}
    
    \end{subfigure}
    %---------------
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/Log_StaleGradDeltas_VsBaseline.pdf}
    \caption{}
    \label{fig:SGsubim2}
    \end{subfigure}
    %---------------
    \caption{ \textbf{Toy experiment of how Adam, RMSProp and SGDM introduce stale momenta.} \textbf{(a)} Gradient updates of 0.2 are injected at four different time points visible by the large RMSProp green dots. We first apply two gradient updates, then four, then one and one. Note how RMSProp amplifies the gradient magnitudes but does not apply any gradient update when the neuron is inactive. Meanwhile, Adam (orange) and SGDM (blue) will keep applying gradient updates that slowly decay over time. Also note that the Adam gradient magnitudes are larger than those for SGDM. \textbf{(b)} We inject gradients at single time points twice and observe how the numerators (darker color) and denominators (lighter color) for Adam and RMSProp change. The denominators decay very slowly. \textbf{(c)} We see how the delta between the actual gradient value of 0.2 and the gradient value applied by the different optimizers changes depending on the interval between the last gradient update. Note that the y-axis is $\log_{10}$. RMSProp has a $10/0.2=$50x amplified gradient and Adam is a $1/0.2=$ 5x. Note that SGDM without a denominator term is the only optimizer that over longer periods of time stops amplifying the gradient value. All results generalize for gradient values that are $<1$.  }
    \label{fig:StaleGradsToyModelSpecific}
\end{figure}

Note that these results are robust for any gradient updates $g_t<1$, otherwise SGDM has larger gradient spikes than Adam. However, this is never the case in our training regime because of the $L^2$ normalization term for our weights and inputs which means that no weight or neuron activation value is ever larger than 1. If this were not the case then we predict that SGDM would have sufficiently large jumps in its gradients to also result in dead neurons. 

\textbf{Practical Takeaways} Below we give our reasoning for why SGD is the best solution but needs careful hyperparameter tuning of the learning rate and GABA switch $s$. Sparse optimizers \citep{SparseAdam} are another alternative that should be investigated in future work. 

Because SGD has no momentum term it does not suffer from stale momenta and is the most principled solution. However, lacking a momentum term, SGD is also the most sensitive to learning rate choice and can introduce either slow or poor convergence if it is too high or low, respectively. Setting the learning rate too low is particularly problematic in our setting with the excitatory period of the GABA switch because neurons can fail to receive enough gradient updates to move onto the data manifold, resulting in dead neurons. In Fig. \ref{fig:SGDLearningRate} of App. \ref{appendix:GABASwitchConsiderations} we show how a learning rate that is too low will result in dead neurons that never learn the manifold before the GABA switch occurs.  

Given these hyperparameter tuning requirements for SGD, we recommend using SGDM to pretrain SDMLP while using SGD for continual learning. SGDM pretraining does not result in dead neurons as shown in Fig. \ref{fig:StaleGradsSummary} and is more robust to choice of learning rate. For example in this instance of SGDM on CIFAR10 pixels, varying the learning rate from 0.01 to 0.09 did not affect either convergence speed or kill off any additional neurons.  Meanwhile, as shown in the ablations of Table 2, in the continual learning setting, SGD is the best choice. 

Another alternative are sparse optimizers like ``sparse Adam'' which does not apply gradient updates to any neuron that is dead \citep{SparseAdam}. However, failing to update the momentum term in a novel way will theoretically still result in what we believe to be the larger problem of exploding re-activated neuron gradients. The only way to use sparse Adam in Pytorch is to implement sparse weight layers and we leave it to future work to empirically test sparse momentum based optimizers. 

This section lacks many citations because we are unaware of existing literature around other models in the sparse activation regime, using either Top-K or Mixture of Experts \citep{SwitchTransformer, TransformerHashLayers, MixtureExpertsHinton} that discuss the stale momentum problem and issues with momentum based optimizers.\footnote{Interestingly, \citep{MixtureExpertsHinton} modifies Adam to reduce its parameter count by setting $\beta_1=0$ and taking averages over $v_t$, both of which may have inadvertently helped avoid stale momenta.} We also could not find an academic citation for the sparse Adam implementation provided in PyTorch or the motivations for implementing it. Suggestively, in TensorFlow the algorithm is called ``LazyAdam" and advertised as being advantageous for computational efficiency reasons without any mention of dead neurons. This is unsurprising in light of the fact that the dead neuron problem is only an issue in the sparse regime and even in this case can go unnoticed if validation accuracy is the only metric of interest. For example, training on CIFAR10 pixels with a ReLU network and Adam results in 95\% dead neurons with minor effects to validation accuracy. %Moreover, even in this context there are , when we care about continual learning settings, not for achieving high validation accuracy and when catastrophic forgetting is not considered. 

\section{Training Regime Ablations}

\subsection{Training Directly on Image Pixels}
\label{appendix:CIFAR10Raw}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RawCIFAR10Results.pdf} 
    \caption{\textbf{Rank ordering of continual learning algorithms remains approximately the same for training directly on CIFAR10.} Here SDMLP with 10K neurons (yellow) actually does better than SDMLP+EWC (green) that was the best performer in the main text Fig. \ref{fig:FullChartAndForgetting}(a). However, we did not tune the hyperparameters for the EWC loss coefficient or the $\beta$ parameter for training directly on pixels. SDMLP+EWC with 10K neurons fails to terminate within the 15 hours of GPU time allocated however it is clear that SDM outperforms it. Results are for the 5 random seed splits of Split CIFAR 10.}
    \label{fig:RawCIFAR10}
\end{figure}

We remove the ConvMixer preprocessing and train models directly on ImageNet32 pixels before testing their continual learning abilities on Split CIFAR10 pixels. This is a way to address concerns that the ConvMixer preprocessing was manipulating the CIFAR10 data manifold to make the continual learning task too easy. Fig. \ref{fig:RawCIFAR10} shows the results and Table 3. We only tested the best performing models with reference to Table 1 and chose EWC over MAS because it gives the best model overall and otherwise performs similarly. We did not test the FlyModel because it assumes the presence of preprocessed latent embeddings rather than the 3072 dimensional CIFAR10 flattened image vectors. All training settings and parameters are kept the same as in the Table 1 tests. 

Interestingly, all EWC methods fail to perform as well as they did before. This is likely because we did not tune our loss coefficient or $\beta$ parameter for training directly on pixels. As a result, the best performing model is now SDM with 10K neurons (yellow) instead of SDMLP+EWC (green). Also note that while SDMLP with $k=1$ (pink) does worse, it fails to learn each task within the 2,000 epochs, e.g. at the end of 2,000 epochs the validation accuracy is $\sim$10\% while the other methods maximize the accuracy within task of $\sim$20\%. This means that SDM with $k=1$ could potentially do better with more epochs or a higher learning rate. 

We use the same 5 random seeds and 2,000 epochs per task. Training takes longer because the images are 3x32x32=3072 pixels rather than the 256 ConvMixer embeddings and the most complicated model, SDMLP+EWC with 10K neurons fails to terminate in the 15 hours of GPU time allocated to each run but makes it to the last learning task where it's performance can be inferrred. 

\begin{table}[h]
    \centering
    \caption*{\textbf{Table 3: Pixel Training - Split CIFAR10 Validation Accuracy}}
    \begin{tabular}{llll}
        \toprule
        %\cmidrule(r){1-3}
        Method     & Neurons & $k$ & Val. Acc. \\
        \midrule
        SDMLP & 1K  & 1 &   0.33  \\
        SDMLP & 10K  & 10 &   \textbf{0.42}  \\
        EWC & 1K  & NA &  0.19   \\
        SDMLP+EWC & 1K  & 10 &  0.28   \\
        SDMLP+EWC* & 10K  & 10 &  0.32   \\
        \midrule
        Oracle & 1K  & NA &  0.53   \\
        Oracle & 10K  & NA &  0.53   \\
        \bottomrule
      \end{tabular}
      \caption*{The most competitive models, pretrained on ImageNet32 and tested on Split CIFAR10 without any ConvMixer embedding. SDMLP+EWC with 10K neurons has a * to denote that it did not finish training. Average of 5 random task splits.}
  \label{table:NoPretrainResults}
\end{table}


\clearpage
\newpage

\subsection{No Pretraining}
\label{appendix:NoPretraining}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/NoPretrain.pdf} 
    \caption{\textbf{Rank ordering of continual learning algorithms remains for no ImageNet32 pretraining.} Solid lines denote different algorithms. Dotted lines denote variants of an algorithm. SDMLP+EWC (green) with 10K neurons does the best then SDMLP with 10K neurons (yellow). The 1K algorithms all do approximately the same including EWC (purple), SDMLP+EWC (dotted brown), and SDMLP k=1 (pink). Results are for the 5 random seed splits of Split CIFAR 10. TopK and ReLU benchmarks both do poorly and were not run for the full set of random seeds or shown as a result.}
    \label{fig:NoPretrainingResults}
\end{figure}

Fig. \ref{fig:NoPretrainingResults} shows the results for the most competitive continual learning algorithms without any ImageNet32 pretraining and tested on the ConvMixer embedded Split CIFAR10. See Table 4 for the final validation results.

We train for 2,000 epochs on each task and 10,000 epochs in total. The GABA switch occurs well within the first task where $s=5,000,000$ with GABA switching at epoch $\sim 50$ and Top-K being fully implemented by epoch $\sim 100$. Performances are lower than with the ImageNet32 pretraining, for example the best performing SDMLP+EWC is 84\% versus the 86\% in Table 1. Most notable is the worse performance for the 1K neuron models that lack sufficient capacity to learn the data manifolds of new tasks without the benefits of pretraining.  

\begin{table}[h]
    \centering
    \caption*{\textbf{Table 4: No Pretraining - Embedded Split CIFAR10 Validation Accuracy}}
    \begin{tabular}{llll}
        \toprule
        %\cmidrule(r){1-3}
        Method     & Neurons & $k$ & Val. Acc. \\
        \midrule
        SDMLP & 1K  & 1 &   0.56  \\
        SDMLP & 10K  & 10 &   0.77  \\
        FlyModel & 1K & 32 &  0.69   \\
        FlyModel & 10K & 32 &  0.82   \\
        EWC & 1K  & NA &  0.52   \\
        SDMLP+EWC & 1K  & 10 &  0.54   \\
        SDMLP+EWC & 10K  & 10 &  \textbf{0.84}   \\
        \midrule
        Oracle & 1K  & NA &  0.93   \\
        Oracle & 10K  & NA &  0.93   \\
        \bottomrule
      \end{tabular}
  \label{table:NoPretrainResults}
\end{table}

\clearpage
\newpage

\section{Additional Datasets}
\subsection{CIFAR10 Extra Fig. and Table}
\label{appendix:CIFAR10Extras}

Table 5 shows the Split CIFAR10 results for the 1K neuron setting. Fig. \ref{fig:ExtraCIFAR10Fig} shows the extent to which the SDMLP forgets previous tasks when compared to the baseline ReLU model. This figure gives extra context to the headline continual learning results of Fig. \ref{fig:FullChartAndForgetting}.

\begin{table}[h]
    \centering
    \caption*{\textbf{Table 5: Split CIFAR10 - 1K Neurons - Validation Accuracy}}
    %\caption{\textbf{Split CIFAR10 Validation Accuracy}}
    \begin{tabular}[t]{llll}%[t]
    \toprule
        Method     & Neurons & $k$ & Val. Acc. \\
        \midrule
        SDMLP & 1K  & 1 &   \underline{0.70}  \\
        SDMLP & 1K  & 10 &   0.63  \\
        Top-K & 1K  & 10 &  0.29   \\
        FlyModel & 1K & 64 &  \dashuline{0.70}   \\
        MAS & 1K  & NA &  0.69   \\
        EWC & 1K  & NA &  0.67   \\
        SI* & 1K  & NA &  0.34   \\
        NISPA & $\sim$1K & NA &  0.19   \\
        L2 & 1K  & NA &  0.23   \\
        Dropout & 1K  & NA &  0.21   \\
        SDMLP+MAS & 1K  & 10 &  0.83   \\
        SDMLP+EWC & 1K  & 10 &  \textbf{0.83}   \\
        \midrule
        Oracle & 1K  & NA &  0.93   \\
        \bottomrule
      \end{tabular}
      \vspace{1.0em}
      \caption*{\textbf{Split CIFAR10 Final Validation Accuracy} - We highlight the best performing \underline{SDMLP}, \dashuline{baseline}, and \textbf{overall performer} in the 1K neuron setting. Oracle was trained on the full CIFAR10 dataset. SI has a * denoting some of its runs failed. NISPA uses a three hidden layer network with 400 hidden units per layer but this is close in parameter count to one hidden layer with 1K neurons. All results are the average of 5 random task splits.}
  \label{table:CIFAR101KResults}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/Frame_115.pdf}
    %\caption{}
    %\label{fig:ForgettingChart}
    \caption{\textbf{SDMLP gradually forgets previous tasks.} Solid lines along the top show SDMLP forgetting previous tasks over time in comparison to ReLU that catastrophically forgets (dashed lines along the bottom). Validation accuracy is now computed within tasks and to avoid clutter we only show the learning curves for the first three tasks. Both plots use the average of 5 random seeds and error bars show standard error of the mean.
    }
    \label{fig:ExtraCIFAR10Fig}
\end{figure}

\subsection{CIFAR100}
\label{appendix:CIFAR100}

To better assess the true memory capacity of our models, we use the same pretraining on ImageNet32 and then test continual learning on 50 splits of CIFAR100, shown in Fig. \ref{fig:CIFAR100FullChartAndForgetting} and Table 6. We allow for 500 epochs per task and 25,000 epochs in total. We used three random seeds instead of five, the same hyperparameters found for CIFAR10, and only tested the best performers from Table 1. 

It is with CIFAR100 that the SDM models including the FlyModel and SDMLP+EWC really shine over the regularization baselines. In the 1K neuron setting, SDMLP+EWC is the best performer with 42\%. Meanwhile, in the 10K setting the FlyModel takes the lead getting 58\% and SDMLP+EWC behind it at 51\% against the oracle of 72\%. The next closest baseline is 40\% for MAS. The large performance jumps in the FlyModel going from 1K to 10K neurons for CIFAR10 and CIFAR100 highlight the importance of dimensionality for the fixed neuron addresses that perform a random projection. Interestingly, when looking at the validation accuracy on Task 1 for each of the models, which shows how it forgets the task over time, SDMLP+EWC performs better than the FlyModel, however the FlyModel retains later tasks better. 

\begin{figure}[h]
    \begin{subfigure}{1.0\textwidth}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/Frame_88.pdf} 
    %\caption{}
    %\label{fig:CIFAR100FullChart}
    \end{subfigure}
    
    \begin{subfigure}{1.0\textwidth}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/Frame_89.pdf}
    %\caption{}
    %\label{fig:CIFAR100ForgettingChart}
    \end{subfigure}
    
    \caption{\textbf{Top - Rank ordering of continual learning algorithms remains similar for CIFAR100.} The FlyModel with neurons (magenta) now outperforms SDMLP+EWC (green). The SDM algorithm does the next best (yellow) with MAS (blue) again close behind and then EWC (purple) and Top-K (orange) performing poorly.
    %Top-K (orange) and ReLU (purple) both perform poorly
    \textbf{Bottom - SDM methods are robust to forgetting.} The first task of CIFAR100 and its validation accuracy for this task over the entire course of training on 49 other tasks is shown for each method. SDMLP+EWC is the most robust to forgetting (green), then the FlyModel (magenta) and SDMLP (yellow). It is interesting that the SDMLP+EWC forgets less for this first task than the FlyModel. Top-K shows some retained memory but only a small amount while MAS and EWC both catastrophically forget over time within $\sim 5$ tasks (2,500 epochs). 
    %Top-K (orange) shows jumps in its performance. This may be because neurons used for Task 1 are overwritten for a new task that is later forgotten again, restoring some degree of performance.}
    }
    \label{fig:CIFAR100FullChartAndForgetting}
\end{figure}

\begin{table}[h]
    \centering
    \caption*{\textbf{Table 6: Split CIFAR100 Validation Accuracy} }
    %\caption{\textbf{Split CIFAR100 Validation Accuracy}}
    \begin{tabular}[t]{llll}%[t]
        \toprule
        Method     & Neurons & $k$ & Val. Acc. \\
        \midrule
        SDMLP & 1K  & 1 &  0.32  \\
        SDMLP & 1K  & 10 &  0.39  \\
        Top-K & 1K  & 10 &  0.11  \\
        FlyModel & 1K  & 32 &  0.36   \\
        MAS & 1K  & NA &  0.24   \\
        EWC & 1K  & NA &  0.12   \\
        SDMLP+EWC & 1K  & 10 &  \textbf{0.42}   \\
        \bottomrule
      \end{tabular}
     \hspace{0.2em}
    \begin{tabular}[t]{llll}%[t]
        \toprule
        Method     & Neurons & $k$ & Val. Acc. \\
        \midrule
        SDMLP & 10K  & 10 &  0.43  \\
        FlyModel & 10K  & 32 &  \textbf{0.58}   \\
        MAS & 10K  & NA &  0.40   \\
        EWC & 10K  & NA &  0.16   \\
        SDMLP+EWC & 10K  & 10 &  0.51   \\
        \midrule
        Oracle & 10K  & NA &  0.72   \\
        \bottomrule
      \end{tabular}
      \vspace{1.0em}
      \caption*{We bold the best performing model within the 1K and 10K neuron settings. Oracle was trained on the full CIFAR10 dataset. Average of 3 random task splits.}
  \label{table:CIFAR100Results}
\end{table}

\clearpage

\subsection{Split MNIST}
\label{appendix:SplitMNIST}

For the sake of completeness and providing an easier benchmark for other methods, we use the Split MNIST dataset but keep the class incremental setting. Results are presented in Table 7 and approximate the general rank ordering of performance whereby the FlyModel with 10K neurons does the best, SDMLP+EWC comes second, then SDMLP on its own and then the parameter importance regularization methods. We did extensive hyperparameter tuning of the EWC and MAS regularization coefficients and $\beta$ parameter along with the FlyModel parameters (App. \ref{appendix:FlyModelParams}). 

We do not use any pretraining and train directly on MNIST pixels. We train for 500 epochs on each task meaning 2,500 epochs in total. We use just three random seeds to initialize model weights but always use naive split where task 1 contains digits 0 and 1, task 2 contains digits 2 and 3, etc.  

As one of our benchmarks we used Active Dendrites \citep{NumentaCatastrophicForgetting}. We used the code provided but found that it failed to generalize beyond the easier Permuted MNIST task incremental benchmark used in the paper.
% Note that in Table 7 of App. \ref{appendix:betaEWC} we pre500 epochs, this is more involved than the shorter runs that were done to compare the continual learning baselines paper. These ones have performance drops showing the importance of having many epochs per task. 

NISPA has a * in the number of neurons column of Table 7 because it used three hidden layers of 400 neurons each as in its original implementation. This is approximately the same number of parameters as the 1K neurons in a single layer (637600 vs 794000 ignoring bias terms).\footnote{NISPA is 80\% weight sparse here but this aids its continual learning and we do not consider the activation sparsity of the Top-K models in our parameter counts either. It also takes many more FLOPs to train because of the iterative weight growth and pruning.}

%0.37 when 5 epochs

\begin{table}[h]
    \centering
    \caption*{\textbf{Table 7: Split MNIST Validation Accuracy}}
    \begin{tabular}{llll}
        \toprule
        %\cmidrule(r){1-3}
        Method     & Neurons & $k$ & Val. Acc. \\
        \midrule
        SDMLP & 1K  & 1 &  0.69  \\
        SDMLP & 1K  & 10 &  0.53  \\
        SDMLP & 10K  & 10 &   0.53  \\
        FlyModel & 1K  & 64 &  0.77   \\
        FlyModel & 10K  & 32 &  \textbf{0.91}  \\
        EWC & 1K  & NA &  0.61   \\
        EWC & 10K  & NA &  0.67   \\
        MAS & 1K  & NA &  0.49   \\
        MAS & 10K  & NA &  0.58   \\
        SI & 1K  & NA &  0.36   \\
        Top-K & 1K  & 10 &   0.22  \\
        NISPA & * & NA & 0.40 \\ 
        Active Dendrites & 1K & NA & 0.20 \\
        ReLU & 1K  & NA &  0.21   \\
        SDMLP+EWC & 1K  & 10 &  0.83   \\
        SDMLP+EWC & 10K  & 10 &  0.86   \\
        \midrule
        Oracle & 1K  & NA &  0.98   \\
        Oracle & 10K  & NA &  0.99   \\
        \bottomrule
      \end{tabular}
     \vspace{1em}
  \caption*{Models trained directly on MNIST pixels and without any pretraining. We bold the best performing method. We run these results on three random seeds and using 500 epochs for each split (this is much higher than other baselines \citep{ContinualLearningBaselines}).}
  \label{table:Split_MNIST}
\end{table}

Interestingly, the SDMLP with 1K neurons does better than with 10K neurons. We also found that the SDMMLP+EWC models were still learning the last task at the end of training. However, increasing training times did not result in the last task being learnt better, suggesting that the model has run out of new neurons to avoid overwriting, or that the regularization coefficient for EWC should be reduced for the combination with SDM.\footnote{This effect was also observed for the FashionMNIST results in App. \ref{appendix:SplitFashionMNIST}.}

\clearpage
\newpage


\subsection{Split FashionMNIST}
\label{appendix:SplitFashionMNIST}

We also evaluate our most successful models and baselines on the FashionMNIST dataset that has ten different classes of fashion items as grayscale 28x28 images. This task is slightly harder than MNIST with our oracles getting 90\% instead of 99\% for MNIST. Table 8 shows these results as the average of 3 random seeds used to initalize the model weights. The rank ordering of results agrees with that of Split MNIST Table 7. 

\begin{table}[h]
    \centering
    \caption*{\textbf{Table 8: Split FashionMNIST Validation Accuracy}}
    \begin{tabular}{llll}
        \toprule
        %\cmidrule(r){1-3}
        Method     & Neurons & $k$ & Val. Acc. \\
        \midrule
        SDMLP & 1K  & 1 &  0.73  \\
        SDMLP & 1K  & 10 &  0.53  \\
        SDMLP & 10K  & 10 &   0.52  \\
        FlyModel & 1K  & 64 &  0.67   \\
        FlyModel & 10K  & 64 &  \textbf{0.76}  \\
        EWC & 1K  & NA &  0.68   \\
        EWC & 10K  & NA &  0.72   \\
        MAS & 1K  & NA &  0.33   \\
        MAS & 10K  & NA &  0.34   \\
        Top-K & 1K  & 10 &   0.23  \\
        ReLU & 1K  & NA &  0.21   \\
        SDMLP+EWC & 1K  & 10 &  0.74   \\
        SDMLP+EWC & 10K  & 10 &  0.72   \\
        \midrule
        Oracle & 1K  & NA &  0.90   \\
        Oracle & 10K  & NA &  0.90   \\
        \bottomrule
      \end{tabular}
     \vspace{1em}
  \caption*{Models trained directly on FashionMNIST pixels and without any pretraining. We bold the best performing method.}
  \label{table:Split_FashionMNIST}
\end{table}

\clearpage
\newpage

\section{Baseline Implementations}

\subsection{Beta Coefficient for Elastic Weight Consolidation and Synaptic Intelligence}
\label{appendix:betaEWC}

The paper \citep{ContinualLearningBaselines} considers a number of baseline continual learning algorithms with different continual learning settings. This includes the most realistic class incremental setting that we use. The paper also open sourced their implementations of these different algorithms and found that the regularization methods of EWC, MAS and SI \citep{EWC, MAS, SISynapticIntelligence} all catastrophically forget Split MNIST. 

However, in our hands by more carefully tuning the loss coefficient for the regularization term in the loss function we were able to improve performance. Tuning the loss coefficient for EWC and SI did not increase their performance. But when we looked at the EWC and SI learning dynamics, we found that because they were getting 100\% accuracy on each task split, there was no gradient from the loss that could be used to infer the importance of each weight to be regularized for future tasks. In order to give the model gradient information, we modified its cross entropy loss, introducing a $\beta<1$ coefficient that made the model less confident in its prediction. Formally: 
\begin{align}
    p_i = \frac{\exp{(\beta l_i)}}{ \sum_{i=1}^{o} \exp{(\beta l_i)} },
\end{align}
where $l_i \in \mathbb{R}$ are the real valued outputs (logits) for each of the $o$ output classes, indexed by $i$. And when put through the softmax distribution this gives a probability distribution where $\sum_{i=1}^{o} p_i =1$. In the original algorithm of \citep{ContinualLearningBaselines}, $\beta=1$ by not being a hyperparameter to tune and this $\beta=1$ is large enough that it results in the model output for the correct class getting a probability of $\sim 1$ with values of 0 for all other classes, giving a loss of 0 and no gradient. By having $\beta<1$, we force our model to be less confident in its prediction of the correct class, creating a loss and gradient information to infer which parameters are important. Note that this $\beta$ term is only used when inferring parameter importance, not when training the model. By tuning the $\beta$ value in conjuction with the regularization loss coefficient, we were able to avoid catastrophic forgetting and again exceed the baseline reported in \citep{ContinualLearningBaselines} for EWC and SI. While this is another hyperparameter that must be tuned, and is not a modification mentioned in the original algorithms of  \citep{EWC, SISynapticIntelligence}, or other literature that we are aware of, it is a straightforward modification that boosts performance and retains the original essence of the algorithm. Therefore, we feeling that it is an appropriate modification to create more meaningful baselines. 

%\footnote{Ironically, the paper \citep{ContinualLearningBaselines} calls for stronger continual learning baselines. Concerningly, its misleading baseline results have been cited over 150 times.}

\begin{table}[h]
    \centering
    \caption*{\textbf{Table 9: Improved Regularization Baselines on Split MNIST}}
    \begin{tabular}{lllll}
    \toprule
    %\cmidrule(r){1-2}
    Name & Loss Coef. & $\beta$ & New Val. Acc. & Original \\
    \midrule
    EWC \citep{EWC} & 200 & 0.005 & 63.23 & 19.80\\
    SI \citep{SISynapticIntelligence} & 1500 & 0.005 & 35.76 & 19.67\\
    MAS \citep{MAS} & 0.5 & NA & 24.99 &  19.52 \\
    L2 \citep{GoodfellowContLearning} & 10 & NA & 36.77 & 22.52 \\
    \bottomrule
  \end{tabular}
  
  \label{table:SDMAblations}
\end{table}

Testing a 1,000 neuron single hidden layer MLP on Split MNIST with 10 epochs per split we get the final validation accuracies after hand trying a few different hyperparameters shown in Table 9.\footnote{For our full experiments using Split CIFAR10 in the pretraining setting shown in Table 1, we do more extensive Bayesian hyperparameter searches to maximize the performance of each baseline.} We show the hyperparameters used and present results from the class incremental setting of Table 2 in \citep{ContinualLearningBaselines} for comparison. 

Note that the number of epochs here is fewer than in the full Split MNIST analysis of App. \ref{appendix:SplitMNIST}. This is to relate our results to other baselines that use only a small number of epochs per task \citep{ContinualLearningBaselines}. If you compare Table 9 with Table 7 of  \ref{appendix:SplitMNIST} you will see that training for more epochs does affect the performance of EWC and MAS. 

\clearpage

\subsection{FlyModel Parameters}
\label{appendix:FlyModelParams}

The FlyModel is unique in being trained for only one epoch and not using backpropagation. The authors outlined a method for the model to be trained for more than one epoch on each task, however, this introduces synaptic decay that would likely reduce performance via forgetting. It was never implemented in \citep{FruitFlyContLearning} and so we present the strongest version of the algorithm trained for one epoch on each task here. 

We used the parameters outlined in \citep{FruitFlyContLearning} as a starting point. Because the dimensionality of our ConvMixer embeddings is 256, half of the 512 used in the original paper, we were able to cut in half the number of Kenyon cells from 20,000 to 10,000, conveniently fitting the number of neurons considered in our other experiments. We also varied the number of projection neuron connections between 64 and 3, finding that 32 performed better in both the 10,000 and 1,000 neuron settings. We experimented with the learning rate and found that the value of 0.005 worked best for the MNIST and CIFAR10 experiments. For CIFAR100, we got better performance using a learning rate of 0.2 but only for the 10,000 Kenyon cell model. The best results for each run across the random seeds is what is presented in the text. 

\section{Investigating Differences in Continual Learning Abilities}
\label{appendix:InvestigateContLearning}

We investigated why SDM is robust to catastrophic forgetting while baseline models and SDMLP models that ablate specific features all fail. 

To summarize our findings: 
\begin{itemize}
    \item ReLU - neurons never learn to tile the data manifold and every neuron is activated by almost every input. 
    \item Top-K - the lack of $L^2$ normalization and the use of a bias term means neurons don't tile the data manifold. This means they are not only activated by more tasks, forgetting previous ones. Additionally, there are more dead neurons reducing memory capacity.  
    \item SDM Top-K Mask - masking leads to more dead neurons and the neurons that are alive being more polysemantic, failing to have subnetworks that avoid being overwritten by future tasks.
    \item SDM No $L^2$ Norm - there are few neurons with massive weight norms that are active for up to 50\% of all inputs. These neurons are active for all tasks, resulting in memory overwriting and catastrophic forgetting. 
    %\item SDM with Bias
\end{itemize}

During Split CIFAR10 continual learning we checkpoint our model every 100 epochs, training on each data split for 300 epochs for a total of 1,500. We also track the number of times that each neuron is active across the entirety of training and visualize this on the $\log_{10}$ plots shown in Fig. \ref{fig:InvestigateSummary}. 

Fig. \ref{fig:subim1InvestSummary} compares SDM to Top-K and ReLU where it is clear that ReLU neurons (green) are all activated many times without the Top-K activation function. While Top-K has fewer activations (orange), its neurons lack the bimodal distribution of SDM (blue) that we believe corresponds to the unique subnetworks only activated for a specific task. Fig. \ref{fig:subim2InvestSummary} looks at SDMLP ablations that use a Top-K Mask instead of subtraction and no $L^2$ normalization. Note the small blip of neurons for no $L^2$ norm (green) that corresponds to the greedy neurons always in the Top-K for all tasks. Also note that the Top-K mask (orange) is slightly shifted towards larger activation values.

For all of our analysis that follow we do not show dead neurons. These dead neurons are only really a problem for Top-K that has 55\% dead neurons and 18\% for SDM without an $L^2$ norm.  

\begin{figure}[h]
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/Baselines_TotalNeuronActs.pdf} 
    \caption{}
    \label{fig:subim1InvestSummary}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/SDMAblates_TotalNeuronActs.pdf}
    \caption{}
    \label{fig:subim2InvestSummary}
    \end{subfigure}
    \caption{\textbf{SDM Neurons are more specialized than Top-K or ReLU.} We track the number of times every neuron is activated during continual learning and present this as a histogram with a $\log_{10}$ scale. \textbf{(a)} We compare SDM (blue) to the Top-K (orange) and ReLU (green) baselines. \textbf{(a)} We compare SDM (blue) to two ablations, SDM with a Top-K mask (orange) and SDM without $L^2$ normalization (green).}
    \label{fig:InvestigateSummary}
\end{figure}

To look at how specialized each neuron is to specific tasks and data classes, we take the final models after Split CIFAR10 continual learning and pass all of the CIFAR10 training data through them, recording which neurons are in the Top-K for each input. For each neuron, we count the number of times it is active for each of the 10 input classes and use this to create a probability distribution over the classes the neuron is activated by. We take the entropy of this distribution as a metric for how polysemantic each neuron is. We plot the entropy of each neuron against the percentage of time it is in the Top-K in Fig. \ref{fig:InvestigateEntropies}. We also combine these two metrics to weight each neurons' entropy by the amount of time it is in the Top-K to compute the average entropy of activated neurons and present this in Table 10 alongside the continual learning performance of each method. There is a clear inverse correlation between the average entropy of activated neurons and continual learning.\footnote{Note that in these experiments we used the SGDM optimizer instead of SGD. This means the validation accuracies for SDM and the linear mask are lower than they otherwise would be, however, we believe the insights we drawn here are unaffected and SGD would have just resulted in fewer dead neurons.} 

\begin{figure}[h]
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/EntropyScatter_SDM.pdf} 
    \caption{}
    \label{fig:subim1Ent}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/EntropyScatter_SDM-LinearDecayMask.pdf}
    \caption{}
    \label{fig:subim2Ent}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/EntropyScatter_SDM-NoL2Norm.pdf} 
    \caption{}
    \label{fig:subim3Ent}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/EntropyScatter_Top-K.pdf}
    \caption{}
    \label{fig:subim4Ent}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
    \begin{center}
    \includegraphics[width=0.5\linewidth]{figures/EntropyScatter_ReLU.pdf}
    \caption{}
    \end{center}
    \label{fig:subim5Ent}
    \end{subfigure}
    \caption{ \textbf{SDM Neurons are specialized and participate democratically in learning.} Plotting the entropy of each neuron (the distribution of data classes that the neuron is activated by) against the percentage of time that it is in the Top-K for all inputs.}
    \label{fig:InvestigateEntropies}
\end{figure}

\begin{table}[h]
  \caption*{\textbf{Table 10: Mean Neuron Entropy Weighted by Top-K Presence}}
  \label{table:InvestigateMeanEntropies}
  \centering
  \begin{tabular}{lll}
    \toprule
    \cmidrule(r){1-3}
    Name & Mean Weighted Entropy & Val. Accuracy \\
    \midrule
    SDMLP & 0.99 & 0.54 \\
    SDMLP Linear Mask & 1.36 & 0.35 \\
    SDMLP No $L^2$ Norm & 1.96 & 0.20 \\
    Top-K & 1.48 & 0.29 \\
    ReLU & 2.25 & 0.21 \\
    \bottomrule
  \end{tabular}
\end{table}

Fig. \ref{fig:InvestigateEntropies} and the summary in Table 10 effectively convey the subnetwork formation by SDM that enables strong continual learning performance. SDM (Fig. \ref{fig:subim1Ent}) has the lowest mean entropy of activated neurons followed by SDM with the Top-K mask (Fig. \ref{fig:subim2Ent}). The SDM with Top-K Mask Fig. \ref{fig:subim2Ent} shows how the most polysemantic (highest entropy) neurons are also the most active which will result in forgetting across tasks. Fig. \ref{fig:subim3Ent} strikingly shows 13 ``greedy'' neurons in the top right that are not only highly polysemantic but active for many inputs (compare the y-axis going up to 6\% compared to 1.4\% for SDM). Top-K in Fig. \ref{fig:subim4Ent} looks like the SDM with Top-K Mask but the average polysemantism of each neuron is much higher. Finally, the ReLU network in Fig. \ref{fig:subim5Ent}, while haved a very even distribution of neuron activations, having the lowest y-axis range of 0.16\%, the neurons are all highly polysemantic. 

Notably, the fact that SDM in Fig. \ref{fig:subim1Ent} does not have every neuron activated and in the Top-K the same percentage of the time means that it's neurons do not perfectly tile the data manifold in proportion to the density of the data. However, this may be because we are optimizing the network for classification performance instead of reconstruction loss.

\begin{figure}[h]
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/EmbeddingDataOnWeightsSDM.pdf}
    \caption{}
    \label{fig:subim1}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/EmbeddingDataOnWeightsSDM-LinearDecayMask.pdf}
    \caption{}
    \label{fig:subim2}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/EmbeddingDataOnWeightsSDM-NoL2Norm.pdf}
    \caption{}
    \label{fig:subim3}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/EmbeddingDataOnWeightsTop-K.pdf} 
    \caption{}
    \label{fig:subim4}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
    \begin{center}
    \includegraphics[width=0.5\linewidth]{figures/EmbeddingDataOnWeightsReLU.pdf}
    \caption{}
    \end{center}
    \label{fig:subim5}
    \end{subfigure}
    \caption{ \textbf{CIFAR10 projected onto a UMAP embedding of the SDM weights tiles the manifold the best.} \textbf{(a)} is shown as Fig. 6 of the main text and the only approach that manages to tile the manifold such that the UMAP plot has meaningful structure. }
    \label{fig:WeightsOnManifold}
\end{figure}

\begin{figure}[h]
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/EmbeddingWeightsOnDataSDM.pdf} 
    \caption{}
    \label{fig:subim1}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/EmbeddingWeightsOnDataSDM-LinearDecayMask.pdf}
    \caption{}
    \label{fig:subim2}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/EmbeddingWeightsOnDataSDM-NoL2Norm.pdf}
    \caption{}
    \label{fig:subim3}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/EmbeddingWeightsOnDataTop-K.pdf}
    \caption{}
    \label{fig:subim4}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
    \begin{center}
    \includegraphics[width=0.5\linewidth]{figures/EmbeddingWeightsOnDataReLU.pdf}
    \caption{}
    \end{center}
    \label{fig:subim5}
    \end{subfigure}
    \caption{\textbf{SDM Weights Tile the CIFAR10 UMAP Embedding the Best.} The two SDMLP models in the top row (\textbf{(a)} and \textbf{(b)}) do the best tiling of the manifold. This is unlike the previous Fig. \ref{fig:WeightsOnManifold} where only SDM in \textbf{(a)} showed good tililng. The CIFAR10 data manifold looks different here for the top row, forming three distinct blobs because of the $L^2$ normalization operation.  }
    \label{fig:ManifoldTiling}
\end{figure}

\clearpage

We present a number of additional plots that show the manifold tiling abilities of SDM in comparison to the other methods. Fig. \ref{fig:WeightsOnManifold} fits the UMAP projection on the weights of the pretrained on ImageNet32 models and then uses this projection for the CIFAR10 data. Fig. \ref{fig:ManifoldTiling} does the inverse where it fits a UMAP \citep{UMAP} projection to ConvMixer embedded CIFAR10 training data and uses this projection for the trained model weights. Both Fig.s show that SDM learns to tile the data manifold most effectively with the full SDMLP being the only method to tile the manifold in both figures. This shows that upon pretraining, the neurons of SDM have learnt to differentiate across the manifold of general image statistics, forming subnetworks that will be useful for continual learning.\footnote{The fact that the neurons are unique and dispersed across the data manifold, when combined with the Top-K activation function, ensures that subnetworks of neurons of will be active for different input classes.}

We emphasize that while there is manifold tiling and subnetwork formation, the ImageNet32 pretraining does not result in SDM already knowing the CIFAR10 data or perfectly tiling the CIFAR10 manifold. Fig. \ref{fig:CIFAR10Frogs} shows a UMAP plot with the projection fit to the SDMLP trained directly on CIFAR10 pixels and projecting the same CIFAR10 data, note how much tighter the manifold tiling is here. Further evidence of manifold tiling is evident in this pixel based training where in Fig. \ref{fig:ReceptiveFields} we take the weights of ten random SDMLP neurons and reshape them into 3x32x32 dimensions to reveal the neurons have specialized to specific classes in the data. This is in stark contrast to the weights of the ReLU neurons shown adjacently. SDM neurons specialize to not only specific image classes but even specific examples within the class, analogous to the hypothetical ``grandmother neuron'' \citep{Grandma, JenniferAniston}. This figure shows the trained weights of randomly chosen neurons from an SDMLP trained on CIFAR10 pixels and reshaped into their image dimensions. A bird, frog, and multiple deer and horses are visible. 

Note that if we directly visualize these $L^2$ normalized weights and our $L^2$ normalized CIFAR10 images, they all look black because the pixel values denoting RGB colors are between 0 and 1 while our normalization makes them much smaller. We rescale our weights back into pixel values by first subtracting the minimum pixel value $\textbf{x} - \min{(\textbf{x})}$ and then multiplying $\textbf{x} * 1/( \max{(\textbf{x})})$ that is standard practice. 

\begin{figure}[h]
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/Neurons_1K.pdf}
    \caption{}
    \label{fig:subim1}
    \end{subfigure}
    % -----
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/Frame_80.pdf}
    \caption{}
    \label{fig:subim1}
    \end{subfigure}
    \caption{\textbf{Learned Neuron Weights for SDM (a) versus ReLU (b).} We train our models directly on CIFAR10 and visualize the weights of nine randomly chosen neurons by reshaping them into their image dimensions. SDM results in significantly more interpretable receptive fields for the neurons.}
    \label{fig:ReceptiveFields}
\end{figure}


\begin{figure}[h]
    \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/wind=999EmbeddingDataOnWeights-SDM.pdf}
    \caption{}
    \label{fig:subim0Frogs}
    \end{subfigure}
    %-----
    \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/Frame_74.pdf}
    \caption{}
    \label{fig:subim1Frogs}
    \end{subfigure}
    \caption{\textbf{Analyzing neuron receptive fields of SDM in relation to the data manifold.} \textbf{(a)} A UMAP plot fit on neuron weights trained on CIFAR10 pixels (orange) with the CIFAR10 data (blue). The black dot in the top left shows the location of the neuron weights that give the frog image displayed as an inset. \textbf{(b)} The neuron weights are shown in the center, in the top row above are the three CIFAR10 images that maximally activate this neuron showing that the neuron weights learn a superposition of similar frog images. In the bottom row are the three closest images determined by euclidian distance on the UMAP embedding. This embedding will be less precise but still shows that the manifold largely captures similar classes, aside from the horse.}
    
    \label{fig:CIFAR10Frogs}
\end{figure}

To fully emphasize that these neurons and their interpretable receptive fields are on meaningful parts of the manifold, we take a neuron that has a frog receptive field and show where it is located on the manifold. We then scan the CIFAR10 training data and take the 3 images that maximally activate this neuron, plotting them in the top row of Fig. \ref{fig:CIFAR10Frogs}. We also look at the UMAP embedded images that have the smallest euclidian distance to the UMAP embedding of our neuron weights and show the closest three images. This UMAP apporach does contain a horse but this approach will be less precise due to the UMAP embedding mapping the 3072 dimensional images into 2 dimensions. 

We use our neuron UMAP embeddings to visualize the number of times that each neuron is activated in Fig. \ref{fig:ManifoldActivationAmounts}. This uses the same overall activation values across continual learning presented in the histograms of Fig. \ref{fig:InvestigateSummary} with yellow indicating the most activations and purple the fewest. 

\begin{figure}[h]
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/NeuronManifold_NeuronActivationMapSDM.pdf}
    \caption{}
    \label{fig:subim1}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/NeuronManifold_NeuronActivationMapSDM-LinearDecayMask.pdf}
    \caption{}
    \label{fig:subim2}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/NeuronManifold_NeuronActivationMapSDM-NoL2Norm.pdf}
    \caption{}
    \label{fig:subim3}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/NeuronManifold_NeuronActivationMapTop-K.pdf}
    \caption{}
    \label{fig:subim4}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
    \begin{center}
    \includegraphics[width=0.5\linewidth]{figures/NeuronManifold_NeuronActivationMapReLU.pdf}
    \caption{}
    \end{center}
    \label{fig:subim5}
    \end{subfigure}
    \caption{\textbf{Neurons colored by the number of times they have been activated.} This plot makes it clear how SDM with no $L^2$ norm \textbf{(c)} only activates a few neurons to do all of its learning, resulting in catastrophic forgetting. Meanwhile, ReLU in \textbf{(e)} activates almost all of its neurons. All models that use Top-K: SDM \textbf{(a)}, SDM with Top-K masking \textbf{(b)} and Top-K \textbf{(d)} activate subsets of neurons making it harder to distinguish their learning dynamics without the additional analyses presented in this section.}
    \label{fig:ManifoldActivationAmounts}
\end{figure}

Finally, we show the data manifold tiling of SDM in Fig. \ref{fig:ClassBasedTiling} where we assign each CIFAR10 class a different color in the left plot and then show the positions of each neuron assigning the color that activates this neuron the most. 

\begin{figure}[h]
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/NeuronManifold_TrainingDataSDM.pdf}
    \caption{}
    \label{fig:subim1}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/NeuronManifold_NeuronClassMapSDM.pdf}
    \caption{}
    \label{fig:subim2}
    \end{subfigure}
    \caption{ \textbf{SDM neurons tile the regions of the manifold that they are most activated by.} We use the same UMAP plot as in Fig. \ref{fig:ManifoldTiling} but color the data by its class in \textbf{(a)} and the neurons by the class that activates each the most in \textbf{(b)}.}
    \label{fig:ClassBasedTiling}
\end{figure}

\end{document}