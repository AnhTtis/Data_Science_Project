
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              %

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bigstrut}
\usepackage{array}
\usepackage{colortbl}
\usepackage[T1]{fontenc}
\usepackage{wrapfig,lipsum}
\usepackage{booktabs}
\usepackage{bigstrut}
\usepackage{multirow}
\usepackage{placeins}
\usepackage{gensymb}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{makecell}
\usepackage{colortbl}
\makeatletter  %
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{tikz}
\usepackage{yfonts}
\usepackage{graphicx}
\usepackage{import}
\usepackage{appendix}
\usepackage[accsupp]{axessibility}  %
\usepackage{hhline}
\usepackage{stfloats}
\usepackage{setspace}
\usepackage{bigstrut}
\usepackage{atbegshi,picture}   
\usepackage{mathtools}
\usepackage{cite} 
\usepackage[font={footnotesize}]{caption}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\tableborder}[0]{\Xhline{8\arrayrulewidth}}
\newcommand{\R}{\mathbb{R}}  %

\DeclareMathOperator*{\argminB}{argmin}   
\DeclareMathOperator*{\minB}{min} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\bowen}[1]{\textcolor{blue}{[\textbf{Bowen:} #1]}}
\newcommand{\stan}[1]{\textcolor{red}{[\textbf{Stan:} #1]}}
\newcommand{\thomas}[1]{\textcolor{green}{[\textbf{Thomas:} #1]}}
\newcommand{\JK}[1]{\textcolor{orange}{[\textbf{Jan:} #1]}}
\newcommand{\link}[1]{{\color{blue}\href{#1}{#1}}}

\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}

\newenvironment{myitem}{\begin{list}{$\bullet$}
{\setlength{\itemsep}{-0pt}
\setlength{\topsep}{0pt}
\setlength{\labelwidth}{5pt}
\setlength{\leftmargin}{10pt}
\setlength{\parsep}{-0pt}
\setlength{\itemsep}{0pt}
\setlength{\partopsep}{0pt}}}%
{\end{list}}


\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\setlength{\belowdisplayskip}{2pt} 
\setlength{\abovedisplayskip}{2pt} 
\setlength{\belowdisplayshortskip}{2pt} 
\setlength{\abovedisplayshortskip}{2pt} 

\title{BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects}


\author{Bowen Wen\affmark[] \quad
Jonathan Tremblay\affmark[] \quad Valts Blukis\affmark[] \quad    Stephen Tyree\affmark[] \quad Thomas MÃ¼ller\affmark[] \and Alex Evans\affmark[] \quad Dieter Fox\affmark[] \quad Jan Kautz\affmark[] \quad Stan Birchfield\affmark[]\\ 
{\affmark[]NVIDIA}
}




\twocolumn[{
\renewcommand\twocolumn[1][]{#1}%
\maketitle

\begin{center}
\vspace{-0.3in}
    \centering
     \includegraphics[width = 0.95\textwidth]{figures/intro_horizontal_c.pdf}
      \vspace{-0.15in}
    \captionof{figure}{Given a monocular RGBD sequence and 2D object mask (in the first frame only), our method performs causal 6-DoF tracking and 3D reconstruction of an unknown object. Without any prior knowledge of the object or interaction agent, our method generalizes well, handling flat and untextured surfaces, specular highlights, thin structures, severe occlusion, and a variety of interaction agents (human hand / body / robotic arm). The visualized meshes are directly output by the method.}\label{fig:intro}
    \vspace{-.1in}
\end{center}%
}]


\begin{abstract}
\vspace{-0.15in}
We present a near real-time (10Hz) method for 6-DoF tracking of an unknown object from a monocular RGBD video sequence, while simultaneously performing neural 3D reconstruction of the object. Our method works for arbitrary rigid objects, even when visual texture is largely absent. The object is assumed to be segmented in the first frame only.  No additional information is required, and no assumption is made about the interaction agent. Key to our method is a Neural Object Field that is learned concurrently with a pose graph optimization process in order to robustly accumulate information into a consistent 3D representation capturing both geometry and appearance. A dynamic pool of posed memory frames is automatically maintained to facilitate communication between these threads. Our approach handles challenging sequences with large pose changes, partial and full occlusion, untextured surfaces, and specular highlights. We show results on HO3D, YCBInEOAT, and BEHAVE datasets, demonstrating that our method significantly outperforms existing approaches. Project page: \link{https://bundlesdf.github.io/}
\vspace{-0.2in}
\end{abstract}

\section{Introduction}
\vspace{-0.1in}

Two fundamental (and closely related) problems in computer vision are 6-DoF (``degree of freedom'') pose tracking and 3D reconstruction of an unknown object from a monocular RGBD video.
Solving these problems will unlock a wide range of applications in areas such as augmented reality~\cite{marchand2015pose}, robotic manipulation~\cite{kappler2018real,wen2022catgrasp}, learning-from-demonstration~\cite{wen2022you}, and sim-to-real transfer~\cite{andrychowicz2020learning,handa2022dextreme}.


Prior efforts often consider these two problems separately.
For example, neural scene representations have achieved great success in creating high quality 3D object models from real data~\cite{munkberg2022extracting,wang2021neus,oechsle2021unisurf,azinovic2022neural,sun2021neuralrecon,yariv2021volsdf}.  These approaches, however, assume known camera poses and/or ground-truth object masks. Furthermore, capturing a static object by a dynamically moving camera prevents full 3D reconstruction (\eg, the bottom of the object is never seen if resting on a table).
On the other hand, instance-level 6-DoF object pose estimation and tracking methods often require a textured 3D model of the test object beforehand~\cite{li2018deepim,labbe2020cosypose,wang2021gdr,wen2020se,wen2020robust} for pre-training and/or online template matching. While category-level methods enable generalization to new object instances within the same  category~\cite{chen2020learning,tian2020shape,wang2019normalized,li2021leveraging,weng2021captra}, they struggle with out-of-distribution object instances and unseen object categories.

To overcome these limitations, in this paper we propose to solve these two problems jointly.
Our method assumes that the object is rigid, and it requires a 2D object mask in the first frame of the video.
Apart from these two requirements, the object can be moved freely throughout the video, even undergoing severe occlusion.
Our approach is similar in spirit to prior work in object-level SLAM~\cite{Zhu2022CVPR,mccormac2018fusion,sharma2021compositional,merrill2022symmetry,runz2018maskfusion,salasmoreno2013slam,wada2020morefusion}, but we relax many common assumptions, allowing us to handle occlusion, specularity, lack of visual texture and geometric cues, and abrupt object motion.
Key to our method is an online pose graph optimization process, a concurrent Neural Object Field to reconstruct the 3D shape and appearance, and a memory pool to facilitate communication between the two processes.
The robustness of our method is highlighted in Fig.~\ref{fig:intro}.

Our contributions can be summarized as follows: 
\begin{myitem}
    \item A novel method for causal 6-DoF pose tracking and 3D reconstruction of a novel unknown dynamic object.  This method leverages a novel co-design of concurrent tracking and neural reconstruction processes that run online in near real-time while largely reducing tracking drift.
    \item We introduce a hybrid SDF representation to deal with uncertain free space caused by the unique challenges in a dynamic object-centric setting, such as noisy segmentation and external occlusions from interaction.
    \item Experiments on three public benchmarks demonstrate state-of-the-art performance against leading methods.
\end{myitem}

\section{Related Work}

\vspace{0pt}\noindent \textbf{6-DoF Object Pose Estimation and Tracking.} 6-DoF object pose estimation infers the 3D translation and 3D rotation of a target object in the camera's frame. 
State-of-the-art methods often require instance- or category-level object CAD models for offline training or online template matching~\cite{sundermeyer2018implicit,wang2019normalized,labbe2020cosypose,labbemegapose}, which prevents their application to novel unknown objects. Although several recent works \cite{sun2022onepose,liu2022gen6d,park2020latentfusion} relax the assumption and aim to quickly generalize to novel unseen objects, they still require pre-capturing posed reference views of the test object, which is not assumed in our setting. Aside from single-frame pose estimation, 6-DoF object pose tracking leverages temporal information to estimate per-frame object poses throughout the video.
Similar to their single-frame counterparts, these methods make various levels of assumptions, such as training and testing on the same objects~\cite{tjaden2017real,li2018deepim,wen2020se,bundle2021wen,stoiber2022iterative,muller2021seeing} or pretraining on the same category of objects~\cite{lin2022keypoint,wang20206,muller2021seeing}. 
BundleTrack~\cite{bundle2021wen} shares the closest setting to ours, generalizing pose tracking instantly to novel unknown objects. 
Differently, however, our co-design of tracking and reconstruction with a novel neural representation not only results in more robust tracking as validated in experiments (Sec.~\ref{sec:exp}), but also enables an additional shape output, which is not possible with \cite{bundle2021wen}.



\vspace{2pt}\noindent \textbf{Simultaneous Localization and Mapping.}  
 SLAM solves a similar problem to the one addressed in this work, but focuses on tracking the camera pose w.r.t.\ a large static environment \cite{teed2021droid,Zhu2022CVPR,sucar2021imap,mur2015orb}.
Dynamic-SLAM methods usually track  dynamic objects by frame-model Iterative Closest Point (ICP) combined with color \cite{xu2019mid,runz2018maskfusion,ma2015simultaneous,runz2017co}, probabilistic data association \cite{strecke2019fusion}, or 3D level-set likelihood maximization \cite{yuheng2013star3d}. 
Models are simultaneously reconstructed on-the-fly by aggregating the observed RGBD data with the newly tracked pose. In contrast, our method leverages a novel Neural Object Field representation that allows for automatic on-the-fly fusion~\cite{dai2017bundlefusion}, while 
dynamically rectifying historically tracked poses to maintain multi-view consistency. We focus on the object-centric setting including dynamic scenarios, in which there is often a lack of texture or geometric cues, and severe occlusions are frequently introduced by the interaction agent---difficulties that rarely happen in traditional SLAM. Compared to static scenes studied in object-level SLAM~\cite{salasmoreno2013slam,merrill2022symmetry,sharma2021compositional,wada2020morefusion,mccormac2018fusion}, dynamic interaction also allows observing different faces of the object for more complete 3D reconstruction.


\vspace{2pt}\noindent \textbf{Object Reconstruction.}
Retrieving a 3D mesh from images has been extensively studied using learning based 
methods~\cite{lei2020pix2surf,yang2022fvor,munkberg2022extracting}. 
With recent advances in neural scene representation, high quality 3D models can be reconstructed~\cite{munkberg2022extracting,wang2021neus,oechsle2021unisurf,azinovic2022neural,sun2021neuralrecon,yariv2021volsdf}, 
though most of these methods assume known camera poses or ground-truth segmentation and often focus on static scenes with rich texture or geometric cues. In particular, \cite{patten2021object} presents a semi-automatic method with a similar goal but uses manual object pose annotations to retrieve a textured model of the object. In contrast, our method is fully automatic and operates over the video stream causally. Another line of research leverages human hand or body priors to resolve object scale ambiguity or refine object pose estimations via contact/collision constraints~\cite{jiang2022neuralhofusion,zhang2020phosa,xie2022chore,bhatnagar22behave,huang2022intercap,liu2021semi,cao2021reconstructing,yang2021cpf,hasson2020leveraging,krainin2011manipulator}. In contrast, we do not assume specific knowledge of the interaction agent, which allows us to generalize to drastically different forms of interactions and scenarios, ranging from human hand, human body to robot arms, as shown in the experiments. This also eliminates another possible source of error from imperfect human hand/body pose estimation.

\section{Approach}
\vspace{-0.1in}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/pipeline.pdf}
    \vspace{-0.15in}
    \caption{Framework overview. First, features are matched between consecutive segmented images, to obtain a coarse pose estimate (Sec.~\ref{sec:coarse_pose}).
Some of these posed frames are stored in a memory pool, to be used and refined later (Sec.~\ref{sec:memory_pool}).
A pose graph is dynamically created from a subset of the memory pool (Sec.~\ref{sec:pose_graph}); online optimization refines all the poses in the graph jointly with the current pose.  These updated poses are then stored back in the memory pool.
Finally, all the posed frames in the memory pool are used to learn  a Neural Object Field (in a separate thread) that models both geometry and visual texture (Sec.~\ref{sec:neural_sdf}) of the object, while adjusting their previously estimated poses.}
    \label{fig:pipeline}
    \vspace{-0.25in}
\end{figure*}

An overview of our method is depicted in Fig.~\ref{fig:pipeline}.
Given a monocular RGBD input video, along with a segmentation mask of the object of interest \emph{in the first frame only}, our method tracks the 6-DoF pose of the object through subsequent frames and reconstructs a textured 3D model of the object.
All processing is causal (no access to future frames) The object is assumed to be rigid, but no specific amount of texture is required---our method works well with untextured objects.
In addition, no instance-level CAD model of the object, nor category-level prior (\eg, training on the same object category beforehand), is needed. 


\subsection{Coarse Pose Initialization}\label{sec:coarse_pose}
\vspace{-0.1in}
To provide a good initial guess for the subsequent online pose graph optimization, we compute a coarse object pose estimate $\tilde{\xi_t} \in \mathrm{SE}(3)$ between the current frame $\mathcal{F}_t$ and the previous frame $\mathcal{F}_{t-1}$. 
First, the object region is segmented in $\mathcal{F}_t$ by leveraging an object-agnostic video segmentation network~\cite{cheng2022xmem}. 
This segmentation method was chosen because it does not require any knowledge of the object or the interaction agent (\eg, a human hand), thus allowing our framework to be applied to a wide range of scenarios and objects.

Feature correspondences in RGB between $\mathcal{F}_t$ and $\mathcal{F}_{t-1}$ are established via a transformer-based feature matching network~\cite{sun2021loftr}, which was pretrained on a large collection of internet photos~\cite{li2018megadepth}. 
Together with depth, the identified correspondences are  filtered by a RANSAC-based pose estimator~\cite{fischler1981random} using least squares~\cite{arun1987least}. 
The pose hypothesis that maximizes the number of inliers is then selected as the current frame's coarse pose estimation $\tilde{\xi}_t$. 

\subsection{Memory Pool}\label{sec:memory_pool}
\vspace{-0.1in}
To alleviate catastrophic forgetting, which can cause long-term tracking drift, it is important to retain information about past frames.
A common approach exploited by prior work is to fuse each posed observation into an explicit global model~\cite{slavcheva2018sdf,runz2018maskfusion,newcombe2011kinectfusion}. 
The fused global model is then used to compare against the subsequent new frames for their pose estimation (frame-to-model matching).  
However, such an approach is too brittle for the challenging scenarios considered in this work, for at least two reasons.
First, any imperfections in the pose estimates will be accumulated when fusing into the global model, causing additional errors when estimating the pose of subsequent  frames. 
Such errors frequently occur when there is insufficient texture or geometric cues on the object, or this information is not visible in the frame.
Such errors accumulate over time and are irreversible. 
Second, in the case of long-term complete occlusion, large motion changes make registration between the global model and the reappearing frame observation difficult and suboptimal.

Instead, we introduce a keyframe memory pool $\mathcal{P}$ that stores the most informative historical observations. 
To build the memory pool, the first frame $\mathcal{F}_0$ is automatically added, thus setting the canonical coordinate system for the novel unknown object. 
For each new frame, its coarse pose $\tilde{\xi}_t$ is updated by comparing to the existing frames in the memory pool, as described in Sec.~\ref{sec:pose_graph}, to yield an updated pose $\xi_t$.
The frame is only added to $\mathcal{P}$ when its viewpoint (described by $\xi_t$) is deemed to sufficiently enrich the multi-view diversity in the pool while keeping the pool compact.

More specifically, $\xi_t$ is compared with the poses of all existing memory frames in the pool.
Since in-plane object rotation does not provide additional information, this comparison takes into account rotational geodesic distance while ignoring rotation around the camera's optical axis. 
Ignoring this difference allows the system to allocate memory frames more sparsely in the space while maintaining a similar amount of multi-view consistency information. 
This trick enables jointly optimizing a wider range of poses, compared to previous work~(\eg, \cite{bundle2021wen}),
when selecting the same number of memory frames to participate in the online pose graph optimization.

\subsection{Online Pose Graph Optimization} \label{sec:pose_graph}
\vspace{-0.1in}
Given a new frame $\mathcal{F}_t$ with its coarse pose estimation $\tilde{\xi}_t$ (Sec. \ref{sec:coarse_pose}), we select a subset of (no more than) $K$ memory frames from the memory pool to participate in online pose graph optimization. 
The optimized pose corresponding to the new frame becomes the output estimated pose $\xi_t$. 
This step is implemented in CUDA for near real-time processing, making it sufficiently fast to be applied to every new frame, thus resulting in more accurate pose estimations as the object is tracked throughout the video. 

As described below (Sec.~\ref{sec:neural_sdf}), the Neural Object Field is also used to assist in this optimization process.  
Every frame in the memory pool has associated with it a binary flag $b(\mathcal{F})$ indicating whether the pose of this particular frame has had the benefit of being updated by the Neural Object Field.
When a frame is first added to the memory pool, $b(\mathcal{F}) =$~{\sc False}.  This flag remains unchanged through subsequent online updates until the frame's pose has been updated by the Neural Object Field, at which point it is forever set to {\sc True}.

Concurrent with updating the pose of the new frame $\mathcal{F}_t$, all the poses of the subset of frames selected for the online pose graph optimization are also updated to the memory pool, as long as their flag is set to {\sc False}.  
Those frames whose flag is set to {\sc True} continue to be updated by the more reliable Neural Object Field process, but they cease being modified by the online pose graph optimization.

\vspace{0pt}\noindent\textbf{Selecting Subset of Memory Frames}. We constrain the number of memory frames participating in the pose graph optimization to be no more than $K$ for efficiency.
Early in the video, when $|\mathcal{P}|\leqslant K$,  no selection is  needed, and all frames in the memory pool are used. 
When the size of the memory pool grows to be larger than $K$, a selection process is applied with the goal of maximizing the multi-view consistency information. 
Prior efforts select keyframes by exhaustively searching pair-wise feature correspondences and solving a spanning tree~\cite{mur2015orb}, which is either too time-consuming for real-time processing, or simply based on a fixed time interval~\cite{slavcheva2018sdf}, which is less effective in our object-centric setting. 
Therefore, we propose instead to efficiently select the subset $\mathcal{P}_{pg} \subset \mathcal{P}$ of memory frames by leveraging the current frame's coarse pose estimation $\tilde{\xi_t}$ (obtained in Sec.~\ref{sec:coarse_pose}). 
Specifically, for each frame $\mathcal{F}^{(k)}$ in the memory pool, we first compute the point normal map and compute the dot product between these normals and  the ray direction in the new frame's camera view to test their visibility. 
If the point cloud visibility ratio in the new frame $\mathcal{F}_t$ is above a threshold (0.1 for all experiments), we further measure the viewing overlap with $\mathcal{F}_t$ by computing the rotation geodesic distance between $\xi^{(k)}$ and $\tilde{\xi}_t$ while ignoring the in-plane rotation (as described above). 
Finally we select the $K$ memory frames with the maximum viewing overlap (smallest distance) to participate in the pose graph optimization along with $\mathcal{F}_t$.
Therefore, $|\mathcal{P}_{pg}|=K$.

\vspace{0pt}\noindent\textbf{Optimization.} In the pose graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, the nodes consist of $\mathcal{F}_t$ and the above selected subset of memory frames:  $\mathcal{V}=\mathcal{F}_t \cup \mathcal{P}_{pg}$, so $|\mathcal{V}|=K+1$.
The objective is to find the optimal poses that minimize the total loss of the pose graph:
\vspace{-0.05in}
\begin{align} 
\mathcal{L}_{pg} = w_{s}\mathcal{L}_s(t)+\!\!\!\!\!\!\!\sum_{i\in \mathcal{V},j\in \mathcal{V},i\neq j}\!\!\!\!\! \left[w_{f}\mathcal{L}_f(i,j) + w_{p}\mathcal{L}_p(i,j)\right], \label{eq:L_pg}
\end{align} 
where $\mathcal{L}_f$ and $\mathcal{L}_p$ are pairwise edge losses~\cite{bundle2021wen}, and $\mathcal{L}_s$ is an additional unary loss.
The scalar factors $w_f, w_p, w_s$ are all set to 1 empirically.
The loss
\begin{align} 
\mathcal{L}_f(i,j)=\sum\limits_{(p_m,p_n) \in C_{i,j}} \rho \left( \left \| \xi_{i}^{-1}p_{m} - \xi_{j}^{-1}p_{n} \right \|_{2} \right)
\end{align} 
measures the Euclidean distance of the RGBD feature correspondences $p_m,p_n \in \R^3$, where $\xi_{i}$ denotes the object pose in frame $\mathcal{F}^{(i)}$, and $\rho$ is the Huber loss~\cite{huber1992robust} for robustness. 
The set of correspondences $C_{i,j}$ between frames $\mathcal{F}^{(i)}$ and $\mathcal{F}^{(j)}$ is detected by the same network introduced in Sec.~\ref{sec:coarse_pose}, where we run batch inference in parallel for efficiency. 
The loss
\begin{align} 
\!\!\!\!\mathcal{L}_p(i,\!j)=\!\!\sum\limits_{p\in I_i}\rho \left( \left|  n_i(p) \cdot \left( T_{ij}^{-1}\pi^{-1}_{D_j} (\pi_j({T_{ij}}       p)) - p \right)    \right| \right)
\end{align} 
measures the pixel-wise point-to-plane distance via re-projective association,
where ${T_{ij}} \equiv \xi_{j}\xi_{i}^{-1}$ transforms from $\mathcal{F}^{(i)}$ to $\mathcal{F}^{(j)}$,
$\pi_j$ denotes the perspective projection mapping onto image $I_j$ associated with $\mathcal{F}^{(j)}$, ${\pi^{-1}_{D_j}}$ represents the inverse projection mapping via looking-up the depth image $D_j$ at the pixel location, $n_i(p)$ denotes the normal via looking-up the normal map of $\mathcal{F}^{(i)}$ at pixel location $p\in I_i$ associated. Lastly, the unary loss
\begin{align} 
\mathcal{L}_s(t)=\sum\limits_{p\in I_t}\rho \big( \left|  \Omega(\xi_t^{-1}(\pi_{D}^{-1}(p))) \right| \big)
\end{align} 
measures the point-wise distance to the neural implicit shape using the current frame, where $\Omega(\cdot)$ denotes the signed distance function from the Neural Object Field as will be discussed in Sec.~\ref{sec:neural_sdf}. 
The Neural Object Field weights are frozen in this step. 
This unary loss is taken into account only after the initial training of the Neural Object Field has converged. 

The poses are represented as inversions of camera poses w.r.t.\ the object, parametrized using Lie Algebra, fixing the coordinate frame of the initial frame as the anchor point. 
We solve the entire pose graph optimization via the Gauss-Newton algorithm with iterative re-weighting. 
The optimized pose corresponding to $\mathcal{F}_t$ becomes its updated pose $\xi_t$. 
For the rest of the selected memory frames, their optimized poses in the memory pool are also updated to rectify possible errors computed earlier in the video, unless $b(\mathcal{F})=$~{\sc True}, as mentioned earlier.

\subsection{Neural Object Field}\label{sec:neural_sdf}
\vspace{-0.1in}

A key to our approach is learning an object-centric neural signed distance field that learns multi-view consistent 3D shape and appearance of the object while adjusting memory frames' poses. It is learned per-video and does not require pre-training in order to generalize to novel unknown objects. This Neural Object Field trains in a separate thread parallel to the online pose tracking. At the start of each training period, the Neural Object Field consumes all the memory frames (along with their poses) from the pool and begins learning. When  training converges, the optimized poses are updated to the memory pool to aid subsequent online pose graph optimization, which fetches these updated memory frame poses each time to alleviate tracking drift. The learned SDF is also updated to the subsequent online pose graph to compute the unary loss $\mathcal{L}_s$ described in Sec.~\ref{sec:pose_graph}. 
The Neural Object Field training process is then repeated by grabbing new memory frames from the pool.

\vspace{0pt}\noindent \textbf{Object Field Representation.} Inspired by 
 \cite{yariv2020multiview}, we represent the object by two functions.  First, the geometry function $\Omega: x \mapsto  s$ takes as input a 3D point $x \in \R^3$ and outputs a signed distance value $s \in \mathbb{R}$.  Second, the appearance function $\Phi: (f_{\Omega(x)}, n, d) \mapsto c$ takes the intermediate feature vector $f_{\Omega(x)} \in \mathbb{R}^3$ from the geometry network, a point normal $n \in \mathbb{R}^3$, and a view direction $d \in \mathbb{R}^3$, and outputs the color  $c \in \mathbb{R}^3_+$. 
In practice, we apply multi-resolution hash encoding \cite{mueller2022instant} to $x$ before forwarding to the network. 
The normal of a point in the object field can be derived by taking the first-order derivative on the signed distance field: $n(x)=\frac{\partial \Omega(x)}{\partial x}$, which we implement by leveraging automatic differentiation in PyTorch~\cite{paszke2019pytorch}. 
For both directions $n$ and $d$, we embed them by a fixed set of low-order spherical harmonic coefficients (order 2 in our case) to prevent over-fitting that could discourage the object pose update (represented as inversion of camera poses w.r.t.\ the object, as mentioned above), in particular the rotations.

The implicit object surface is obtained by taking the zero level set of the signed distance field: 
$S=\left \{ x\in \mathbb{R}^3 \mid \Omega(x)=0  \right \}$. The SDF object representation $\Omega$ has two major benefits compared to \cite{mildenhall2021nerf} in our setting. First, when combined with our efficient ray sampling with depth guided truncation (described below), it enables the training to converge quickly within seconds for online tracking. 
Second, implicit regularization guided by the normals encourages smooth and accurate surface extraction.  This not only provides a satisfactory object shape reconstruction as one of our final goals, but also in return provides more accurate frame-to-model loss $\mathcal{L}_s$ for the online pose graph optimization.

\vspace{0pt}\noindent \textbf{Rendering.} 
Given the object pose $\xi$ of a memory frame, an image is rendered by emitting rays through the pixels.  3D points are sampled at different locations along the ray: 
\begin{align}
    x_i(r) = o(r)+t_i d(r),
\end{align}
where $o(r)$ and $d(r)$ are the ray origin (camera focal point) and ray direction, respectively, both of which depend on $\xi$; and $t_i \in \R_+$ governs the position along the ray. 

The color $c$ of a ray $r$ is integrated by near-surface regions:
\vspace{-0.1in}
\begin{equation}
    c(r)=\int_{z(r)-\lambda}^{z(r)+0.5\lambda} w(x_i)\Phi(f_{\Omega(x_i)},n(x_i),d(x_i))\,dt, \label{eq:render} 
\end{equation}
\begin{equation}
w(x_i)= \frac{1}{1+e^{-\alpha\Omega(x_i)}}\frac{1}{1+e^{\alpha\Omega(x_i)}},
\end{equation}
where $w(x_i)$ is the bell-shaped probability density function \cite{wang2021neus} that depends on the distance from the point to the implicit object surface, \ie, the signed distance $\Omega(x_i)$. $\alpha$ (set to a constant) adjusts the softness of the probability density distribution. The probability reaches a local maximum at the surface intersection. $z(r)$ is the depth value of the ray from the depth image. $\lambda$ is the truncation distance. In Eq.~\eqref{eq:render}, we ignore the contribution from  empty space that is more than $\lambda$ away from the surface to reduce over-fitting from the empty space in the neural field in order to improve pose updates. 
We then only integrate up to a $0.5\lambda$ penetrating distance to model self-occlusion \cite{wang2021neus}. 
An alternative to directly using the depth reading $z(r)$ to guide the integration would be to infer the zero-crossing surface from $\Omega(x_i)$. 
However, we found this requires denser point sampling and slower training convergence compared to using the depth.

\begin{figure}[h]
    \centering
    \definecolor{blue}{RGB}{67, 143, 196}
    \definecolor{orange}{RGB}{255, 192, 0}
    \definecolor{red}{RGB}{255, 0, 0}
    \vspace{-0.15in}
    {\includegraphics[width=0.48\textwidth]{figures/ray_sample.pdf}} 
    \vspace{-0.28in}
    \caption{\textbf{Left:} Octree-voxel representation for efficient ray tracing, using the predicted binary mask from the video segmentation network (Sec.~\ref{sec:coarse_pose}), which contains errors.  Rays can land inside the mask (shown as red) or outside (yellow). 
    \textbf{Right:} 2D top-down illustration of the neural volume and point sampling along the rays with hybrid SDF modeling.  Blue samples are near the surface.
    }    \label{fig:ray_sample}
    \vspace{-0.15in}
\end{figure}

\noindent \textbf{Efficient Hierarchical Ray Sampling.} 
For efficient rendering, we construct an Octree representation~\cite{KaolinLibrary} before training by naively merging the point clouds of the posed memory frames. 
We then perform hierarchical sampling along the rays. Specifically, we first uniformly sample $N$ points bounded by the occupancy voxels (gray boxes in Fig.~\ref{fig:ray_sample}), terminating at $z(r)+0.5\lambda$. 
A custom CUDA kernel was implemented to skip the sampling of intermediate unoccupied voxels. 
Additional samples are allocated around the surface for higher quality reconstruction: Instead of importance sampling based on the SDF predictions, which requires multiple forward  passes through the network~\cite{wang2021neus,mildenhall2021nerf}, we draw $N'$ point samples from a normal distribution centered around the depth reading $\mathcal{N}(z(r), \lambda^2)$. This results in $N+N'$ total samples,  without querying the more expensive multi-resolution hash encoding or the networks.

\vspace{0pt}\noindent \textbf{Hybrid SDF Modeling.} Due to the imperfect segmentation and external occlusions, we propose a hybrid signed distance model. Specifically, we divide the space into three regions to learn the SDF (see Fig.~\ref{fig:ray_sample}): 
\begin{myitem}
    \item \textit{Uncertain free space:} These points (yellow in the figure) correspond to the background in the segmentation mask or to pixels with missing depth values, for which the observation is unreliable. For instance, at ray $r_1$'s pixel location in the binary mask, the finger's occlusion results in background prediction, even though it actually corresponds to the pitcher handle. Naively ignoring the background for emitting the ray would lose the contour information, causing bias. Therefore, instead of fully trusting or ignoring \textit{uncertain free space}, we assign a small positive value $\epsilon$ to be potentially external to the object surface so that it can quickly adapt when a more reliable observation is available later:
    \begin{equation}
        \mathcal{L}_{\textit{u}}=\frac{1}{|\mathcal{X}_{\textit{u}}|}\sum_{x\in \mathcal{X}_{\textit{u}}} (\Omega(x)-\epsilon )^2.
    \end{equation}
    \item \textit{Empty space:} These points (red in the figure) are in front of the depth reading up to a truncation distance, making them almost certainly external to the object surface. We apply $L_1$ loss to the truncated signed distance to encourage sparsity:
    \begin{equation}
        \mathcal{L}_{\textit{e}}=\frac{1}{|\mathcal{X}_{\textit{e}}|}\sum_{x\in \mathcal{X}_{\textit{e}}} | \Omega(x)-\lambda |.
    \end{equation}
    \item \textit{Near-surface space:} These points (blue in the figure) are near the surface, no more than $z(r)+0.5\lambda$ distance behind the depth reading to model self-occlusion. This space is critical for learning the sign flipping in SDF and the zero level set. We approximate the near-surface SDF by projective approximation for efficiency:
    \begin{align}
        \mathcal{L}_{\textit{surf}}=\frac{1}{|\mathcal{X}_{\textit{surf}}|}\sum_{x\in \mathcal{X}_{\textit{surf}}}\left(\Omega(x)
        +d_x - d_D \right)^2,
    \end{align}
    where $d_x=\left\|x-o(r)\right\|_2$ and $d_D=\left\|\pi^{-1}(z(r))\right\|_2$ are the distance from ray origin to the sample point and the observed depth point, respectively.
    
\end{myitem}

\noindent \textbf{Training.} The trainable parameters include the multi-resolution hash encoder, $\Omega$, $\Phi$, and the object pose updates in the tangent space parametrized in Lie Algebra $\Delta\overline{\xi}\in \R^{(|\mathcal{P}|-1)\times 6}$, wherein we freeze the first memory frame's pose to be the anchor point. The training loss is:
\begin{equation}
\begin{aligned}
\mathcal{L}=&w_{\textit{u}}\mathcal{L}_{\textit{u}}\!+\!w_{\textit{e}}\mathcal{L}_{\textit{e}}\!+\!w_{\textit{surf}}\mathcal{L}_{\textit{surf}}\!+\!w_{c}\mathcal{L}_{c}\!+\!w_{\textit{eik}}\mathcal{L}_{\textit{eik}},
\end{aligned}
\end{equation}
where $\mathcal{L}_{c}$ denotes the $L_2$ loss over the foreground color for appearance network supervision:
\begin{align}
    \mathcal{L}_{c}=\frac{1}{|\mathcal{R}|}\sum_{r\in \mathcal{R}}\left \| \Phi(f_{\Omega(x)},n(x),d(r))-\bar{c}(r) \right \|_2,
\end{align}
and $\mathcal{L}_{\textit{eik}}$ is the Eikonal regularization \cite{gropp2020implicit} over the SDF in \textit{near-surface space}:
\begin{align}
\mathcal{L}_{\textit{eik}}=\frac{1}{|\mathcal{X}_{\textit{surface}}|}\sum_{x\in \mathcal{X}_{\textit{surface}}} ( \left\|\nabla  \Omega(x)\right\|_2-1 )^2.
\end{align}
Unlike \cite{wang2021neus} which requires ground-truth mask as input, we do not perform mask supervision, since the predicted mask is often noisy from the network.


\vspace{-0.1in}
\section{Experiments}\label{sec:exp}
\vspace{-0.1in}


\subsection{Datasets}
\vspace{-0.1in}
To evaluate our method, we consider three real-world datasets with drastically different forms of interactions and dynamic scenarios. For results on wild application and static scenes, see \href{https://bundlesdf.github.io/}{project page}.

\noindent \textbf{HO3D \cite{hampali2020honnotate}:} This dataset contains the RGBD video of a human hand interacting with YCB objects \cite{calli2015benchmarking}, captured by Intel RealSense camera at close range. Ground truth is automatically generated from multi-view registration. We adopt the most recent version HO-3D\_{v3} and test on the official evaluation set. This results in 4 different objects, 13 video sequences, and 20428 frames in total.

\noindent \textbf{YCBInEOAT \cite{wen2020se}:} This dataset contains the ego-centric RGBD videos of a dual-arm robot manipulating the YCB objects \cite{calli2015benchmarking} captured by Azure Kinect camera at mid range. There are three types of manipulation: (1) single arm pick-and-place, (2) within-hand manipulation, and (3) pick-and-place with handoff between arms. Although this dataset was originally developed to evaluate pose estimation approaches relying on CAD models, we do not provide any object prior knowledge to the evaluated methods. There are 5 different objects, 9 videos, and 7449 frames in total. 


\noindent \textbf{BEHAVE \cite{bhatnagar22behave}:} This dataset contains the RGBD video of a human body interacting with the objects, captured at far range by a pre-calibrated multi-view system with Azure Kinect cameras. However, we constrain our evaluation to the single-view setting, where severe occlusions frequently occur. We evaluate on  the official test split excluding the deformable objects. This results in 16 different objects, 70 videos/scenes, and 107982 frames in total.


\subsection{Metrics}
\vspace{-0.1in}
 We separately evaluate pose estimation and shape reconstruction. For 6-DoF object pose, we compute the area under the curve (AUC) percentage of \textit{ADD} and \textit{ADD-S} metrics \cite{xiang2018posecnn,he2022fs6d,bundle2021wen} using ground-truth object geometry. For 3D shape reconstruction, we compute the chamfer distance between the final reconstructed mesh and ground-truth mesh in the canonical coordinate frame defined by the first image of each video. More details can be found in the appendix.

\subsection{Baselines}
\vspace{-0.1in}
We compare against DROID-SLAM (RGBD) \cite{teed2021droid}, NICE-SLAM \cite{Zhu2022CVPR}, KinectFusion \cite{newcombe2011kinectfusion},  BundleTrack \cite{bundle2021wen} and SDF-2-SDF \cite{slavcheva2018sdf} using their open-source implementations with the best tuned parameters.
We additionally include the baseline results from their leaderboard.  Note that methods such as \cite{newcombe2015dynamicfusion,innmann2016volumedeform} focus on deformable objects and the root 6-DoF tracking and fusion are often based on ~\cite{newcombe2011kinectfusion}, whereas we focus on rigid objects that are dynamically moving. We thus omit their comparisons. The inputs to each evaluated method are the RGBD video and the first frame's mask indicating the object of interest. We augment the comparison methods with the same video segmentation masks used in our framework for fair comparison, to focus on 6-DoF object pose tracking and 3D reconstruction performance. In the case of tracking failure, no re-initialization is performed to test long-term tracking robustness.  

DROID-SLAM \cite{teed2021droid}, NICE-SLAM \cite{Zhu2022CVPR} and KinectFusion \cite{newcombe2011kinectfusion} were originally proposed for camera pose tracking and scene reconstruction. When given the segmented images, they run in an object-centric setting. Since DROID-SLAM \cite{teed2021droid} and BundleTrack \cite{bundle2021wen} cannot reconstruct an object mesh, we augment these methods with TSDF Fusion \cite{zeng20163dmatch,curless1996volumetric} for shape reconstruction evaluation. For NICE-SLAM~\cite{Zhu2022CVPR} and our method, we initialize the neural volume's bound   using only the first frame's point cloud (to preserve causal processing, we cannot access future frames).


\begin{figure*}[tbh]
    \centering
    \definecolor{cyan}{RGB}{0, 255, 255}
    \includegraphics[width=0.95\textwidth]{figures/qual.pdf}
    \vspace{-0.15in}
    \caption{Qualitative comparison of the three most competitive methods on HO3D Dataset. \textbf{Left:} 6-DoF pose tracking visualization, where the  contour (\textcolor{cyan}{cyan}) is rendered with the estimated pose. Note, as shown in the 2nd column, that our predicted pose sometimes corrects errors in the ground truth. \textbf{Right:} Front and back view of the final reconstructed shape output by each method. Due to hand occlusions, some parts of the object are never visible in the video.  Meshes are rendered from the same viewpoint, though significant drift of DROID-SLAM and BundleTrack results in erroneously rotated meshes.}
    \label{fig:qual}
    \vspace{-0.25in}
\end{figure*}

\subsection{Comparison Results on HO3D}
\vspace{-0.1in}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/ho3d_err_time.pdf}
    \vspace{-0.15in}
    \caption{Pose tracking error against time on HO3D Dataset. Each time stamp's result is averaged across all videos.  \textbf{Left:} Rotation error measured by geodesic distance. \textbf{Right:} Translation error. }
    \label{fig:ho3d_err_time}
    \vspace{-0.2in}
\end{figure}



Quantitative results on HO3D are shown in Tab.~\ref{tab:ho3d} and Fig.~\ref{fig:ho3d_err_time}.  Our method outperforms the comparison methods by a large margin on both 6-DoF pose tracking and 3D reconstruction. For DROID-SLAM \cite{teed2021droid}, NICE-SLAM \cite{Zhu2022CVPR} and KinectFusion \cite{newcombe2011kinectfusion}, when working in an object-centric setting, significantly less texture or geometric (purely planar or cylindrical object surfaces) cues can be leveraged for tracking, leading to poor performance. Fig.~\ref{fig:ho3d_err_time} presents the tracking error against time to study the long-term tracking drift. While BundleTrack \cite{bundle2021wen} achieves similarly low translation error as our approach, it struggles on the rotation estimation.  In contrast, our method maintains a low tracking error throughout the video.  We provide per-video quantitative results in the appendix. 

Fig.~\ref{fig:qual} shows example qualitative results of the three most competitive methods. Despite multiple challenges such as severe hand occlusions, self-occlusions, little texture cues in intermediate observations and strong lighting reflections, our method keeps tracking accurately along the video and obtains dramatically higher quality  3D object reconstruction. Notably, our predicted pose is sometimes more accurate than ground-truth, which was annotated by multi-camera multi-view registration leveraging hand priors. 


\input{tables/ho3d.tex}

\vspace{-0.2in}
\subsection{Comparison Results on YCBInEOAT}
\vspace{-0.1in}

\input{tables/ycbineoat.tex}

Quantitative results on YCBInEOAT are shown in Tab.~\ref{tab:ycbineoat}. 
This dataset captures the interaction between the robot arms and the object from an ego-centric view, which leads to challenges due to the constrained camera view and severe occlusions by the robot arms.  For completeness, in this table we also include additional baseline methods from \cite{bundle2021wen}.\footnote{For fair comparison, we only include baselines from \cite{bundle2021wen} that---like our method---do not require instance- or category-level object knowledge.}
The results from these methods, indicated by asterisk ($^*$), are simply copied from \cite{bundle2021wen}.
Note that, in the case of (non-asterisk) BundleTrack, we re-run the algorithm with the same segmentation masks as ours for fair comparison, and we augment with TSDF Fusion for reconstruction evaluation (same as Tab.~\ref{tab:ho3d}).
We omit the re-running for MaskFusion*~\cite{runz2018maskfusion} and TEASER++*~\cite{Yang20troteaser} due to their relatively poorer performance. 


Our approach sets a new benchmark record on ADD-S metric and chamfer distance in 3D reconstruction, while obtaining comparable performance with the previous state-art-art method on ADD metric. In particular, while BundleTrack \cite{bundle2021wen} achieves competitive object pose tracking, it does not obtain satisfactory 3D reconstruction results. This demonstrates the benefits of our co-design of tracking and reconstruction.




\vspace{-0.1in}
\subsection{Comparison Results on BEHAVE}


\input{tables/behave.tex}


Quantitative results on BEHAVE are shown in Tab.~\ref{tab:behave}. We refer to the supplemental material for more detailed results. In our setting of single-view and zero-shot transfer without leveraging human body priors, this dataset exhibits extreme challenges. For instance, (i) there are long-term complete occlusions when the human carries the object and faces away from the camera; (ii) severe motion blur and abrupt displacement frequently occur due to the human freely swinging the object; (iii) the objects are of diverse properties and vary greatly in size; (iv) the video is captured at a distance from the camera, making it difficult for depth sensing. Therefore, evaluation on this benchmark pushes the boundary to a more difficult setting. Despite these challenges, our method is still able to perform long-term robust tracking in most scenarios and performs significantly better than previous methods. 


\vspace{-0.1in}
\subsection{Ablation Study}

\input{tables/ablations.tex}

We investigate the effectiveness of our design choices on HO3D dataset given its more accurate pose annotations. The results are shown in Tab.~\ref{tab:ablations}. \textit{Ours w/o memory} achieves dramatically worse performance as there is no mechanism to alleviate tracking drift. For \textit{Ours-GPG}, even with similar amount of computation, it struggles on  objects or observations with little texture or geometric cues due to hand-crafted losses. Aside from object pose tracking, \textit{Ours w/o memory}, \textit{Ours w/o NOF} and \textit{Ours-GPG} lack the module for 3D object reconstruction.  \textit{Ours w/o hybrid SDF} ignores the contour information and can be biased by false positive segmentation when rectifying the memory frames' pose. These lead to less stable pose tracking and more noisy final 3D reconstruction. \textit{Ours w/o compact mem pool}, when under the same computational budget, leads to insufficient pose coverage during pose graph optimization and Neural Object Field learning, as mentioned in Sec.~\ref{sec:memory_pool}.

\vspace{-0.1in}
\section{Conclusion}
\vspace{-0.05in}
We presented a novel method for 6-DoF object tracking and 3D reconstruction from a monocular RGBD video.  Our method only requires segmentation of the object in the initial frame. Leveraging two parallel threads that perform online graph pose optimization and Neural Object Field representation respectively, our method is able to handle challenging scenarios, such as fast motion, partial and compete occlusion, lack of texture, and specular highlights.  On several datasets we have demonstrated state-of-the-art results compared with existing methods.  Future work will be aimed at leveraging shape priors to reconstruct unseen parts.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{ref}
}


\clearpage



\appendix
\appendixpage
\input{supplemental}

\end{document}
