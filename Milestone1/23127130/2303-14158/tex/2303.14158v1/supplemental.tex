


























\section{Implementation Details}

During coarse pose initialization, if there is no immediate previous frame to compare with (\eg, missing detection by the segmentation, or object reappearing after complete occlusion), the current frame will instead be compared with the memory frames. The memory frame which has more than 10  feature correspondences with the current frame is selected as the new reference frame for the coarse pose initialization. The following steps remain the same.

For online pose graph optimization, we constrain the maximum number of participating memory frames $K=10$ for efficiency. When computing $\mathcal{L}_p$ we reject corresponding points whose distance is larger than 1 cm, or their normal angle is larger than 20\degree. The Gauss-Newton optimization iterates for 7 steps.

For Neural Object Field learning, we normalize the object into the neural volume bound of $[-1,1]$, where the scale is computed as 1.5 times of the initial frame's point cloud dimension. The neural volume's coordinate system is based on the  first frame's centered point cloud. The geometry network $\Omega$ consists of two-layer MLP with hidden dimension 64 and ReLU activation except for the last layer. The intermediate geometric feature $f_{\Omega(\cdot)}$ has dimension 16. The bias of the last layer is initialized to 0.1 for a small positive SDF prediction at the start of training. The appearance network $\Phi$ consists of three-layer MLP with hidden dimension 64 and ReLU activation except for the last layer, where we apply sigmoid activation to map the color prediction to $[0,1]$. For Octree ray-tracing, the finest voxel size is set to 2 cm. We simplify the multi-resolution hash encoder~\cite{mueller2022instant} to 4 levels, with number of feature vectors from 16 to 128 for efficiency. Each level's feature dimension is set to 2. The hash table size is set to $2^{22}$. In each iteration the ray batch size is 2048. For hierarchical point sampling, $N$ and $N'$ are set to 128 and 64, respectively.  The truncation distance $\lambda$ is set to 1 cm. For \textit{uncertain free space}, $\epsilon$ is set to 0.001. In the training loss, $w_u=100, w_e=1, w_{\textit{surf}}=1000, w_c=100, w_{\textit{eik}}=0.1$. We implement in PyTorch~\cite{paszke2019pytorch} with Adam optimizer. The initial learning rate is 0.01 with linear decay rate 0.1. The Neural Object Field training runs in a separate thread concurrently and interchanges data with the memory pool periodically after each training convergence (300 steps), which leads to sufficient pose refinement. The first training period starts when there are 10 memory frames in the pool. Upon training convergence, it returns the data to the memory pool and grabs memory frames newly added to the pool during its last training period, to repeat the training process. The next training reuses the latest updated frames' poses. But for the other trainable parameters, reusing their weights tend to get stuck in local minima if there is any sub-optimum in the previous training period, particularly due to noisy pose. Therefore, we re-initialize the network weights for the new training periods. This takes similar number of steps to refine the newly added memory frames' poses, compared to reusing the previous network weights. 

 
\section{Computation Time}

All experiments were conducted on a standard
desktop with Intel i9-10980XE CPU
and a single NVIDIA RTX 3090 GPU. Our method consists of two threads running concurrently. The online tracking thread processes frames at around 10.2 Hz, where video segmentation takes 18 ms, coarse matching takes 24 ms, pose graph takes 56 ms
on average. Concurrently, the neural object field
thread runs in the background and takes 6.7 s averagely for each training round, at the end of which it exchanges data with the main thread. On the same hardware, competitive methods DROID-SLAM~\cite{teed2021droid} and BundleTrack~\cite{bundle2021wen} run at 6.1 Hz and 11.2 Hz respectively. 

\section{Metrics}

For evaluation, we decouple the pose estimation and shape reconstruction, so that they can be treated separately. For 6-DoF object pose evaluation, we compute the area under the curve (AUC) percentage of \textit{ADD} and \textit{ADD-S} metric: 
 \begin{equation}
     \text{ADD}=\frac{1}{|\mathcal{M}|}\sum_{x\in \mathcal{M}}^{}\left\| (Rx+t)-(\tilde{R}x+\tilde{t})  \right\|_2
 \end{equation}
  \begin{equation}
     \text{ADD-S}=\frac{1}{|\mathcal{M}|}\sum_{x_1\in \mathcal{M}} \minB_{x_2\in \mathcal{M}}\left\| (Rx_1+t)-(\tilde{R}x_2+\tilde{t})  \right\|_2,
 \end{equation}
 where $\mathcal{M}$ is the object model.
Since the novel unknown object's CAD model is inaccessible to the methods to define the coordinate system, we use the ground-truth pose in the first image to define the canonical coordinate frame of each video to evaluate the pose.

For 3D shape reconstruction evaluation, we report the results of chamfer distance between the final reconstructed mesh and the ground-truth mesh, using the following symmetric formulation:
\begin{align}
d_{CD}=&\frac{1}{2|\mathcal{M}_1|}\sum\limits_{x_1\in \mathcal{M}_1}^{}\min\limits_{x_2\in \mathcal{M}_2}\left\| x_1-x_2\right\|_2 + \\
&\frac{1}{2|\mathcal{M}_2|}\sum\limits_{x_2\in \mathcal{M}_2}^{}\min\limits_{x_1\in \mathcal{M}_1}\left\| x_1-x_2\right\|_2
\end{align}
In our method, the mesh can be extracted by applying Marching Cubes over the zero level set in the Neural Object Field. For all methods, we use the same resolution (5~mm) to sample points for evaluation. Since most videos do not cover the complete surrounding view of the object, we cull the ground-truth mesh faces that are never visible in the video by a rendering test, given by the ground-truth mesh and pose. 

\section{Detailed Results}

\textbf{Recall curves} for ADD-S and ADD for all three datasets are presented in Fig.~\ref{fig:ho3d_auc} (HO3D), Fig.~\ref{fig:ycb_auc} (YCBInEOAT), and Fig.~\ref{fig:behave_auc} (BEHAVE). 
Each plot shows the results for all videos of the respective dataset.
As can be seen, the area-under-the-curve (AUC) for our method exceeds that of other methods for almost all datasets.

\textbf{Per-video quantitative results} for all three datasets are presented in Tab.~\ref{tab:ho3d_detail} (HO3D), Tab.~\ref{tab:ycbineoat_detail} (YCBInEOAT), and Tabs.~\ref{tab:behave_detailA}-\ref{tab:behave_detailD} (BEHAVE).  
As can be seen, our method performs best on almost all videos of HO3D, more than half the videos of YCBInEOAT, and a large majority of videos of BEHAVE.
Note that the last row of each table (``Mean'') is included in the main paper.

\textbf{Qualitative results} are demonstrated in Figs.~\ref{fig:ho3d_AP_13_comparison} and~\ref{fig:ho3d_MPM13_comparison} (HO3D), 
Fig.~\ref{fig:ycb_sugar_box_comparison} (YCBInEOAT),
and
Figs.~\ref{fig:behave_chair_comparison} and \ref{fig:behave_table_comparison} (BEHAVE). 
We encourage the reader to watch the supplemental video.





\vspace{8pt}\noindent \textbf{Details Regarding the Single-View Setup of BEHAVE.} As mentioned in the paper, the BEHAVE Dataset was captured by a pre-calibrated multi-camera system with four cameras. 
Since our method only requires a monocular input, for fair evaluation, we run all methods on a single monocular input.  
That is, for each scene, we input only one of the cameras' captured video to the methods. 

Although in theory we could run each method four times, once per video camera, this would be excessively time consuming for the little insight that it might bring.
Moreover, since there are only four cameras placed at each corner around the scene, it is often the case that the object is severely occluded by the human in several cameras' views (including at the beginning of the video).  Using such cameras would not lead to meaningful results for tracking evaluation, due to the very limited object visibility at initialization. 

Instead, we decided to automatically select one of the four cameras from each scene for evaluation.
More specifically, we select the video with the least amount of occlusion in each scene over the entire sequence. To do so, we compute the average visibility ratio of the object in each camera's video by comparing the ground-truth object mask against the rendered object mask using the ground-truth information. This is performed offline for all videos before evaluation.  The selected single-view video is then used by all methods for evaluation, even though severe occlusions still occur frequently which exhibit challenges, as shown in Fig.~\ref{fig:behave_chair_comparison}, \ref{fig:behave_table_comparison}. 


\section{Robustness Analysis}

In the following we discuss our approach's robustness under various challenges.  We encourage the reader to watch our supplemental video for more complete appreciation of the system.


\vspace{6pt}\noindent \textbf{Dearth of Texture or Geometric Cues.} In the case of dynamic object-centric setting, dearth of texture or geometric cues frequently occur given by the object itself. For instance, in Fig.~\ref{fig:ho3d_AP_13_comparison}, large areas on the blue pitcher lack texture, which challenge those methods heavily relying on optical flow (DROID-SLAM~\cite{teed2021droid}), or keypoint matching (BundleTrack~\cite{bundle2021wen}), or photometric loss (NICE-SLAM~\cite{Zhu2022CVPR}). Additionally, large areas of cylindrical surface also exhibit few geometric cues to leverage and can cause rotational ambiguity to those methods relying on point-to-surface matching (SDF2SDF~\cite{slavcheva2018sdf}, BundleTrack~\cite{bundle2021wen}, KinectFusion~\cite{newcombe2011kinectfusion}). In contrast, our method is robust to these challenges due to the synergy of  pose graph optimization and Neural Object Field. More examples of such challenges can be found in Fig.~\ref{fig:ho3d_MPM13_comparison}, \ref{fig:behave_chair_comparison},  \ref{fig:behave_table_comparison}. 



\vspace{6pt}\noindent \textbf{Occlusions.} In the dynamic object setting, occlusions  include self-occlusions and external occlusions introduced by the interaction agent (\eg, human hand, human body, robotic arm). For instance, in Fig.~\ref{fig:ho3d_MPM13_comparison}, there are moments when the ``meat can'' only exhibits a single flat face (2nd column) after extreme rotations, causing severe self-occlusion. In other observations, external occlusion introduced by the human hand (4th column) also challenges the comparison methods. More examples of such challenges can be found in Fig.~\ref{fig:ho3d_AP_13_comparison}, \ref{fig:behave_chair_comparison}, \ref{fig:behave_table_comparison},
\ref{fig:ycb_sugar_box_comparison}. As can be observed, our method is robust to either case and keeps tracking accurately throughout the video thanks to the memory mechanism, whereas the comparison methods struggle. 

\vspace{6pt}\noindent \textbf{Specularity.} Due to the object's surface smoothness, material and complex environmental lighting, specularity could happen, introducing challenges for those methods heavily replying on optical flow (DROID-SLAM~\cite{teed2021droid}), keypoint matching (BundleTrack~\cite{bundle2021wen}) or photometric loss (NICE-SLAM~\cite{Zhu2022CVPR}). As shown in Fig.~\ref{fig:ho3d_AP_13_comparison}, \ref{fig:ho3d_MPM13_comparison}, \ref{fig:behave_chair_comparison}, \ref{fig:ycb_sugar_box_comparison}, despite the specularity on metalic or highly smooth surfaces, our method keeps tracking accurately throughout the video, whereas the comparison methods become brittle. 

\vspace{6pt}\noindent \textbf{Abrupt Motion and Motion Blur.} Fig.~\ref{fig:behave_motion_blur} illustrates an example of abrupt object motion due to the human freely swinging the box. Aside from challenges for 6-DoF pose tracking under large displacement, it causes motion blur in RGB, leading to additional challenge for keypoint matching and Neural Object Field learning. However, our method has shown robustness under these adverse conditions and even yields more accurate pose than ground-truth.

\vspace{6pt}\noindent \textbf{Noisy Segmentation.} Figs.~\ref{fig:noisy_segA} and \ref{fig:noisy_segB} demonstrate examples of noisy  masks (purple) from the video segmentation network, including both false positive and false negative predictions. The false negative segmentation leads to ignorance of the texture-rich areas, intensifying the issue of dearth of texture. The false positive segmentation introduces deformable part from the interaction agent or undesired scene background, causing inconsistency in multi-view. However, our downstream modules are robust to the segmentation noise and maintain accurate tracking.

\vspace{6pt}\noindent \textbf{Noisy Depth.} As shown in Fig.~\ref{fig:noisy_depth}, in our setting, the noisy depth comes from two sources. First, the consumer-level RGBD camera has observable sensing noise. This is especially the case for BEHAVE~\cite{bhatnagar22behave} and YCBInEOAT~\cite{wen2020se} Dataset, where the images are captured at a distance from the camera, which challenges depth sensing. Second, due to the noisy segmentation, false positive predictions include undesired background areas in the depth point cloud. In Fig.~\ref{fig:noisy_depth} (left), when naively fusing the per-frame depth point cloud using ground-truth pose, the result is highly cluttered, which implies the noisy depth sensing and segmentation. However, despite such noise, our simultaneous pose tracking and reconstruction produce high quality mesh, as shown on the right.


\section{Limitation and Failure Modes}

While our method is robust to a variety of challenging conditions, it fails when multiple types of challenges appear together. For instance, in Fig.~\ref{fig:failure}, the occurrence of severe occlusion, segmentation error, dearth of texture and geometric cues together lead to tracking failure. When the object re-appears, the recovered pose is affected by symmetric geometry. Besides, our method requires depth modality which limits its application to certain types of objects where depth sensing fails, such as transparent objects. Finally, our method assumes the object to be rigid. In future work, generalizing to both rigid and non-rigid objects at the same time would be of interest.



\begin{figure*}[hbt]
    \centering
    {\includegraphics[width=0.95\textwidth]{figures/ho3d_auc.pdf}}
    \vspace{-0.1in}
    \caption{Recall curve of ADD-S (left) and ADD (right) metric  including all videos on HO3D Dataset.} \label{fig:ho3d_auc} 
\end{figure*}

\begin{figure*}[h]
    \centering
    {\includegraphics[width=0.95\textwidth]{figures/ycb_auc.pdf}}
    \vspace{-0.1in}
    \caption{Recall curve of ADD-S (left) and ADD (right) metric  including all videos on YCBInEOAT Dataset.}  \label{fig:ycb_auc}
\end{figure*}

\begin{figure*}[h]
    \centering
    {\includegraphics[width=0.95\textwidth]{figures/behave_auc.pdf}}
    \vspace{-0.1in}
    \caption{Recall curve of ADD-S (left) and ADD (right) metric  including all videos on BEHAVE Dataset.} \label{fig:behave_auc} 
\end{figure*}



\input{tables/ho3d_detailed_quant.tex}

\input{tables/ycbineoat_detailed_quant.tex}

\input{tables/behave_detailed_quantA.tex}
\input{tables/behave_detailed_quantB.tex}
\input{tables/behave_detailed_quantC.tex}
\input{tables/behave_detailed_quantD.tex}



\begin{figure*}[h]
    \centering
    {\includegraphics[width=0.9\textwidth]{figures/ho3d_AP13_track_comparison.pdf}}
    \vspace{-0.1in}
    \caption{Qualitative comparison on HO3D video ``AP13''. Our method is robust to observations with little texture or geometric cues (large area of cylindrical surface), whereas comparison methods struggle.} \label{fig:ho3d_AP_13_comparison}
\end{figure*}
\clearpage

\begin{figure*}[h]
    \centering
    {\includegraphics[width=0.9\textwidth]{figures/ho3d_MPM13_track_comparison.pdf}}
    \vspace{-0.1in}
    \caption{Qualitative comparison on HO3D video ``MPM13''. Note that our pose tracking at times appears to be slightly more accurate than the ground-truth as shown in the rightmost column.}\label{fig:ho3d_MPM13_comparison}
\end{figure*}
\clearpage





\begin{figure*}[h]
    \centering
    {\includegraphics[width=0.9\textwidth]{figures/ycb_sugar_box1_track_comparison.pdf}}
    \vspace{-0.1in}
    \caption{Qualitative comparison on YCBInEOAT video ``sugar\_box1".}\label{fig:ycb_sugar_box_comparison}
\end{figure*}
\clearpage





\begin{figure*}[h]
    \centering
    {\includegraphics[width=0.9\textwidth]{figures/behave_Date03_Sub03_chairblack_hand.3.color_track_comparison.pdf}}
    \vspace{-0.1in}
    \caption{Qualitative comparison on BEHAVE video ``Date03\_Sub03\_chairblack\_hand.3''. Our method is robust to severe and even complete occlusions (3rd and last column).}\label{fig:behave_chair_comparison}
\end{figure*}
\clearpage

\begin{figure*}[h]
    \centering
    {\includegraphics[width=0.9\textwidth]{figures/behave_Date03_Sub04_tablesquare_lift.3_track_comparison.pdf}}
    \vspace{-0.1in}
    \caption{Qualitative comparison on BEHAVE video ``Date03\_Sub04\_tablesquare\_lift.3''. Our method is sometimes even more accurate than ground-truth (3rd and last column). It is also robust to severe occlusions (4th column).}\label{fig:behave_table_comparison}
\end{figure*}
\clearpage


\begin{figure*}[h]
    \centering
    {\includegraphics[width=0.95\textwidth]{figures/behave_Date03_Sub05_boxsmall.3.color_motion_blur.pdf}}
    \vspace{-0.1in}
    \caption{Despite fast object pose change and motion blur, our approach  produces even more accurate pose than ground-truth. Image is best viewed by zooming in.} \label{fig:behave_motion_blur} 
\end{figure*}


\begin{figure*}[h]
    \centering
    {\includegraphics[width=0.95\textwidth]{figures/noisy_segA.pdf}}
    \vspace{-0.1in}
    \caption{Example of noisy masks (purple) from the video segmentation network, showing both false positive and false negative predictions.  The first column visualizes the first frame's mask that initializes tracking.  Our method is robust to noisy segmentation and maintains accurate tracking despite such noise.  Figure is continued on the next page.  (Part 1 of 2.)} \label{fig:noisy_segA} 
\end{figure*}


\begin{figure*}[h]
    \centering
    {\includegraphics[width=0.95\textwidth]{figures/noisy_segB.pdf}}
    \vspace{-0.1in}
    \caption{Example of noisy masks (purple) from the video segmentation network.  Continued from previous figure.  (Part 2 of 2.)} \label{fig:noisy_segB} 
\end{figure*}


\begin{figure*}[h]
    \centering
    {\includegraphics[width=0.95\textwidth]{figures/noisy_depth.pdf}}
    \vspace{-0.1in}
    \caption{Example of noisy depth from BEHAVE video ``Date03\_Sub04\_tablesquare\_lift.3''. \textbf{Left:} Fused point cloud using ground-truth pose and masks from the video segmentation network. \textbf{Right:} Final reconstruction from our approach without any trimming.} \label{fig:noisy_depth} 
\end{figure*}

\begin{figure*}[h]
    \centering
    {\includegraphics[width=0.95\textwidth]{figures/failure.pdf}}
    \vspace{-0.1in}
    \caption{Failure case. The occurrence of severe occlusion, segmentation error, dearth of texture or geometric cues together lead to tracking failure. When the object re-appears, the recovered pose is affected by symmetric geometry.} \label{fig:failure} 
\end{figure*}











