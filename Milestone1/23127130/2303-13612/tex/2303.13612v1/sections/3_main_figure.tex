\begin{figure*}[!t]
    \begin{center}
    \includegraphics[width=1\linewidth]{figures/method/framework.png}
    \end{center}
    \caption{
        \label{fig:framework}
        {\bf Overview.} During training, we train a U-Net to predict the embedding of a novel view of an object, given a reference image of the object and a relative pose. The U-Net is conditioned on an embedding of the relative pose computed using an MLP, which we train jointly with the U-Net. 
        At inference, our method first takes as input a reference image of a new object and predicts the embeddings of views of the object under many relative poses. This inference takes around 1 second on a single GPU V100.
        Then, given a query image of the object, we first compute its embedding and match it against the set of predicted embeddings, which takes about 0.2s.  This gives us a distribution over the possible relative poses between the reference and query images. The maximum of the distribution corresponds to the predicted pose. 
        \vspace{-5pt} 
        }
        
        % {\bf At training time}, we use pairs made of a reference image $\imageRef$, a query image  $\imageQuery$ and a relative pose $\relativeR$ to optimizing our network U-Net. {\bf At testing time}, we apply this network to images of objects not seen during training to compute their local features at different pre-defined viewpoints from a single reference image. We can then retrieve the object pose by matching the image against the database of templates.}
\end{figure*} 
