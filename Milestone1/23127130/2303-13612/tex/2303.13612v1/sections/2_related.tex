In this section, we first review various approaches to novel view synthesis. We then shift our focus to pose estimation techniques that aim to achieve generalization.

\subsection{Novel view synthesis from a single image} 


% Novel views --> Nerfs
Our method generates discriminative feature views, which are conditioned on a reference view and the relative pose between the views. This relates to the pioneering work of NeRFs~\cite{mildenhall2020nerf} since it performs novel-view synthesis. While recent advancements have improved the speed of NeRFs~\cite{muller_instant_2022, sparsevoxelgrid, plenoxel}, our approach is still orders of magnitude faster as it does not require the creation of a full 3D volumetric model. Furthermore, our approach only requires a single input view, whereas a typical NeRF setup necessitates around 50 views. Reducing the number of views required for NeRF reconstruction remains an active research area, especially in the single-view scenario~\cite{pixelNerf, realfusion}.


% NERFs via diffusion models enable novel views from sparse views. 
Recent works~\cite{sparsefusion, realfusion}  have had successes generating novel views via NERFs  using a sparse set of views as input by leveraging 2D diffusion models. For images, the breakthrough in diffusion models~\cite{ddpm,song2020denoising} have unlocked several workflows~\cite{ramesh10hierarchical, saharia2022photorealistic,ruiz2022dreambooth}. For 3D applications, DreamFusion~\cite{poole2022dreamfusion} pioneered a score-distillation sampling that allows for the use of a 2D diffusion model as an image-based loss, leveraged by 3D applications via differentiable rendering. This has resulted in significant improvements for tasks previously trained with a CLIP-based image loss~\cite{clip, dreamfields, CLIPmesh, magic3d, Clipasso, alainjay}.  By building on top of score-distillation sampling, SparseFusion~\cite{sparsefusion} reconstructs a NeRF scene with as few as two views with relative pose, while the concurrent work RealFusion~\cite{realfusion} does it from a single input view, although the reconstruction time is impractical for real-time applications. Our approach is much faster as we do not create a 3D representation of the object.

% diffusion model conditionned on 2d pose
Closest to our work, 3DiM~\cite{3dim} generates novel views of an object by conditioning a diffusion model on the pose. Instead of leveraging foundation diffusion models in 2D like DreamFusion does, they retrain a diffusion model specifically for this task. While they have not applied their approach to template-based pose estimation, we design such a baseline and compare against it. We find that the diffusion model tends to always generate sharp images, even at the cost of sometimes changing the texture or hallucinating wrong details which hinders the performance of the template-based approach. In contrast, our approach generates novel views directly in an embedding space instead of pixel space, which is much more efficient as we will demonstrate in Section~\ref{sec:motivation}.  

% regression-based model
Finally, several approaches~\cite{mariotti_semi-supervised_2020,mariotti_viewnerf_2022} generate novel views by conditioning a feed-forward neural network on the 3D pose, which we also do with a U-Net. We share with those approaches an advantage in speed : such feed-forward neural network are one or two orders of magnitude faster than current diffusion model. However, the way we perform pose estimation is fundamentally different. We use novel-view synthesis in a template-based matching approach~\cite{nguyen2022templates}, while they use it in a regression-based optimization. In practice, we found these methods to work well on a limited number of object categories, and we observed their performance to deteriorate significantly when evaluating on novel categories.

% ControlNet conditionned on pose is a better baseline than 3DiM.
%Concurrently with our work, ControlNet~\cite{controlnet} introduces a technique for adding conditioning to Stable Diffusion, though no prior work has shown the possibility of conditioning Stable Diffusion on relative pose at the time of our submission. While this could be an interesting approach, it would be considerably slower than our U-Net, making it unsuitable for real-time applications.
%\nguyenrmk{I don't know whether this paragraph is necessary? }



% Many conditional generation approaches, based on different paradigms, such as semi-supervised~\cite{mariotti_semi-supervised_2020} and unsupervised learning~\cite{mariotti_viewnet_2021}, as well as neural radiance fields~\cite{mariotti_viewnerf_2022}, have already shown promising results in generating novel views of the target from a single reference view. In principle, these generated novel views can be used to obtain 3D viewpoint information when compared with the query, making it possible to estimate a 3D viewpoint from a single image. However, 
% we found that this not to perform well~(see our experiment in Section~\ref{sec:motivation}). Moreover, these methods can only work on a limited number of object categories, and we observed their performance to deteriorate significantly when evaluating on novel categories.

% , which we believe is mainly caused by the not-easy-to-generalize global features of \cite{mariotti_semi-supervised_2020, mariotti_viewnet_2021}. Indeed, \cite{nguyen2022templates} showed that global feature-based representations tend to generalize much worse than local feature-based representations.


%\yinlinrmk{Correct?}\nguyenrmk{I would remove "which we believe ..." to be safe } \yinlinrmk{While, we need to explain somehow.}\nguyenrmk{Is Nerf a global feature?} \yinlinrmk{Emm, hard to say. we'd better to remove this for safe, as your suggested. A little tricky.} \nguyenrmk{I removed the ViewNerf, we can keep these two others (I don't compare with ViewNerf anyway since it takes so much time} \yinlinrmk{Yeah, fair enough}


%, which improves the generalization ability to novel objects.
% However, it reconstructs novel views in pixel space and requires the canonical pose of the reference view, which is difficult to obtain in practice. By contrast, we propose a pose-conditioned image-to-embedding approach that works in feature embedding space, which is much more efficient as we will demonstrate in Section~\ref{sec:motivation}. 

% Additionally, we use template matching to obtain the relative pose between the embedding of the reference and input view, making our method independent of the canonical pose of the reference view.


% \nguyen{Closest to our work, 3DiM \cite{3dim} introduces an image-to-image translation approach for novel view synthesis. Unlike our work, this method reconstructs objects in pixel space rather than embedding space, and requires the canonical pose of the reference view, which is not feasible in real-world scenarios. Our experimental results also demonstrate that our approach outperforms 3DiM \cite{3dim} while being significantly faster.}



%In this work, we use a simple U-Net 

\subsection{Generalizable object pose estimation}

% Estimating the 3D pose of an object in an image often relies on establishing 3D-to-2D correspondences between the 3D object points and 2D image locations, followed by some geometry solvers~\cite{1,2,3}. 

Although many recent methods for estimating the 3D pose of an object from an image have shown significant advances in both efficiency and accuracy~\cite{wohlhart-cvpr15-learningdescriptors, balntas-iccv17-poseguidedrgbdfeaturelearning, labbe-eccv20-cosypose, sundermeyer-cvpr20-multipathlearning, sundermeyer-eccv18-implicit3dorientationlearning, peng-cvpr19-pvnet, zebrapose}, most of them can only work for known objects, which means they need to be retrained on every new object, limiting their application range.


Several techniques have been explored to generalize better to unseen object pose estimation, such as generic 2D-3D correspondences~\cite{Pitteri20203DOD}, an energy-based strategy~\cite{relpose}, keypoint matching~\cite{onepose}, or template matching~\cite{nguyen2022templates, shugurov_osop_2022, chen_fusion, liu2022gen6d, megapose}. Despite significant progress, these methods either need an accurate 3D model of the target or they  rely on multiple annotated reference images from different viewpoints. These 3D annotations are challenging to obtain in practice. By contrast, we propose a strategy that works with neither the 3D model of the target nor the annotation of multiple views. More importantly, our method predicts accurate poses with only a single reference image, and generalizes to novel objects without retraining.


% \nguyen{Traditional 3D object pose estimation methods are typically designed for known objects\cite{wohlhart-cvpr15-learningdescriptors, balntas-iccv17-poseguidedrgbdfeaturelearning, labbe-eccv20-cosypose, sundermeyer-cvpr20-multipathlearning, sundermeyer-eccv18-implicit3dorientationlearning}. However, recent works have attempted to address the challenge of estimating pose for unseen objects using approaches such as generic 2D-3D correspondences \cite{Pitteri20203DOD}, energy-based methods \cite{relpose}, keypoint matching \cite{sun2022onepose, he_onepose_nodate}, or template matching \cite{nguyen_templates_2022, slobodan_cvpr22, liu2022gen6d, megapose}. Despite these advancements, these methods still rely on a 3D model of the target object or reference images captured from annotated multiple viewpoints. 

% Various conditional generation approaches, including semi-supervised \cite{mariotti_semi-supervised_2020} and unsupervised learning \cite{mariotti_viewnet_2021}, as well as neural radiance fields \cite{mariotti_viewnerf_2022}, have shown promising results in generating conditional views of objects and estimating 3D viewpoints from a single image, similar to our method. However, these methods have only been evaluated on a limited number of object categories, and we have experimentally demonstrated that they struggle to generalize to unseen categories. In contrast, our method builds upon the recent architecture of \cite{stable-diffusion} for injecting the condition into latent space and outperforms all of these approaches in terms of performance on unseen categories.}


% Recently, denoising diffusion models~\cite{ho2020denoising, song2020denoising} have unlocked several workflows, such as generating images from text \cite{ramesh10hierarchical, saharia2022photorealistic} and placing a foreground object in different backgrounds \cite{ruiz2022dreambooth}. In this work, we leverage these class of models for (probabilistic) novel view synthesis while using geometry-aware features as conditioning. Inspired by the impressive results in DreamFusion ~\cite{poole2022dreamfusion} which optimized 3D scenes using text-conditioned diffusion models, we propose a view-conditioned diffusion distillation mechanism to similarly extract 3D modes in the sparse view reconstruction task.
