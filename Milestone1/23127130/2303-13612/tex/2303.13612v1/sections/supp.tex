\section{Supplementary Material}

\subsection{Denoising Diffusion Probabilistic Models (DDPM)}
\paragraph{Diffusion model.}
Diffusion models~\cite{sohl2015deep, ho2020denoising, song2019generative, song2021denoising} are a classes of likelihood-based models inspired by nonequilibrium thermodynamics~\cite{song2019generative, song2020improved}. These models define a Markovian chain of diffusion forward process by gradually adding noise to sample data. The forward noise process is defined as

\begin{equation}
\label{eq:noise_process}
    q(\bm{z}_t | \bm{z}_0) = \mathcal{N}(\bm{z}_t | \sqrt{\bar{\alpha}_t} \bm{z}_0, (1 - \bar{\alpha}_t) \bm{I}),
\end{equation}
which transforms data sample $\bm{z}_0$ to a latent noisy sample $\bm{z}_t$ for $t\in\{0, 1, ...,T\}$ by adding noise to $\bm{z}_0$.

$\bar{\alpha}_t \coloneqq \prod_{s=0}^{t} \alpha_s = \prod_{s=0}^{t} (1 - \beta_s)$ and $\beta_s$ represents the noise variance schedule~\cite{ ho2020denoising}.
During training, a neural network $f_\theta(\bm{z}_t, t)$ is trained to predict $\bm{z}_0$ from $\bm{z}_t$ by minimizing the training objective with $\ell_2$ loss~\cite{ho2020denoising}:
\begin{equation}
    \mathcal{L}_\text{train} =  \frac{1}{2}|| f_\theta(\bm{z}_t, t) - \bm{z}_0 ||^2.
\end{equation}