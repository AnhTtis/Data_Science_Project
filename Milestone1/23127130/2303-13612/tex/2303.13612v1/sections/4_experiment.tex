In this section, we first describe our experimental setup in Section~\ref{sec:experimental setup}, and then compare our method to others~(\cite{nguyen_pizza_2022, mariotti_semi-supervised_2020, mariotti_viewnet_2021, 3dim} on both seen and unseen object categories in Section~\ref{sec:main_results}. Section~\ref{sec:occlusions} reports an evaluation of the robustness to partial occlusions. In Section~\ref{sec:ablation}, we conduct an ablation study to investigate the effectiveness of our method in different settings, and finally discuss failure cases of our method in Section~\ref{sec:failureCases}.

\subsection{Experimental setup}
\label{sec:experimental setup}

\input{sections/4_dataset}
\input{tables/forge}

\paragraph{Dataset.} To the best of our knowledge, we are the first method addressing the problem of object pose estimation from a single image when the object belongs to a category not seen during training: PIZZA~\cite{nguyen_pizza_2022} evaluated on the DeepIM refinement benchmark, which is made of pairs of images with a small relative pose; SSVE~\cite{mariotti_semi-supervised_2020} and ViewNet~\cite{mariotti_viewnet_2021} evaluated only on objects from categories seen during training. 

We therefore  had to create a new benchmark to evaluate our method. We created a dataset using the same object categories from ShapeNet~\cite{chang2015shapenet} as in FORGE~\cite{forge_jiang}. For the training set, we randomly select 1000 object instances from each of the 13 categories as done in FORGE~(\textit{airplane, bench, cabinet, car, chair, display, lamp, loudspeaker, rifle, sofa, table, telephone, and vessel}), resulting in a total of 13,000 instances. We build two separate test sets for evaluation. The first test set is the ``novel instances'' set, which contains 50 new instances that do not appear during training but are still from the object categories used for training. The second test set is the ``novel category'' set, which includes 100 models from the 10 unseen categories selected by FORGE~(\textit{bus, guitar, clock, bottle, train, mug, washer, skateboard, dishwasher, and pistol}). For each 3D model, we randomly select camera poses to produce five reference images and five query images. We use BlenderProc~\cite{denninger2019blenderproc} as our synthetic rendering engine. 

Figure~\ref{fig:shapeNet} illustrates the categories used for training our architecture and the categories used for testing it. The shapes and appearances of the categories in the test set are very different from the shapes and appearances of the categories in the training set and thus constitute a good test \nguyen{set} for generalization to unseen categories.


\paragraph{Metrics.}
We report two different metrics based on relative camera pose error as done in \cite{mariotti_semi-supervised_2020}. Specifically, we provide the median pose error across instances for each category in the test set and the accuracy $\bm{\AccThirty}$ for which a prediction is treated as correct when the pose error is $\le 30$ $^{\circ}$. Additionally, we present the results of our method for the top 3 and 5 nearest neighbors retrieved by template matching.


\paragraph{Baselines.}

We compare our work with all previous methods~(to the best of our knowledge) that aim to predict a pose from a single view: PIZZA~\cite{nguyen_pizza_2022}, a regression-based approach that directly predicts the relative pose, and SSVE~\cite{mariotti_semi-supervised_2020} and ViewNet~\cite{mariotti_viewnet_2021}, which employ semi-supervised and self-supervised techniques to treat viewpoint estimation as an image reconstruction problem using conditional generation. 

We also compare our method with the recent diffusion-based method 3DiM~\cite{3dim}, which generates pixel-level view synthesis. Since 3DiM originally only targets view-synthesis and is not designed for 3D object pose, we use it to generate templates and perform nearest neighbor search to estimate a 3D object pose, as mentioned in Section~\ref{sec:motivation}. To make 3DiM work in the same setting as us, we retrain it relative pose conditioning instead of canonical pose conditioning.

Only the code of PIZZA is available. The other methods did not release their code at the time of writing, however we re-implemented them. Our numerical results for the ``novel instances'' cases are similar to the ones reported in the respective papers, which validates our implementations. We use a ResNet18 backbone  as in~\cite{nguyen_pizza_2022} for PIZZA, SSVE, and ViewNet. 

We train all models on input images with a resolution of 256$\times$256 except for 3DiM for which we use a resolution of 128$\times$128 since 3DiM performs view synthesis in pixel space, which takes much more memory. Our re-implementations achieve similar performance as the original papers when evaluated on the same data for seen categories, as shown in Table~\ref{tab:shapeNet}, which validates our re-implementation. 
Our method also uses the frozen encoder from \cite{nguyen2022templates} to encode the input images into embeddings of size 32$\times$32$\times$8. 
For all methods, we use AdamW~\cite{loshchilov_decoupled_2017} with an initial learning rate of 1e-4. Training takes 10 hours on 4GPUs V100 for each method on average. We refer the reader to the supplementary material for more implementation details.


\subsection{Comparison with the state of the art}
\label{sec:main_results}

Table~\ref{tab:shapeNet} summarizes the results of our method compared with the baselines discussed above.
%other methods~\cite{mariotti_semi-supervised_2020, mariotti_viewnet_2021, nguyen_pizza_2022, 3dim}. 
Under both the Acc30 and Median metrics, our method consistently achieves the best overall performance, outperforming the baselines by more than 10\% in Acc30 and 10$^o$ in Median. 
In particular, while other works produce reasonable results on unseen instances of training categories, they often struggle to estimate the 3D pose of objects from unseen categories. By contrast, our method works well in this case, demonstrating better generalization ability on unseen categories.

% thanks its efficient novel synthesis in latent space.

Figure~\ref{fig:qualitative} shows some visualization results of our method on unseen categories with and without symmetries. Our method  produces more accurate 3D poses than the baselines when a symmetry axis exists.




\input{sections/4_qualitative}


\subsection{Robustness to occlusions}
\label{sec:occlusions}

To evaluate the robustness of our method against occlusions, we added random rectangle filled with Gaussian noise to the query images over the objects, in a  similar way to  Random Erasing~\cite{zhong2020random}. We vary the size of the rectangles to cover a range betwen 0\% to 25\% of the bounding box of the object.  Figures~\ref{fig:teaser} and \ref{fig:qualitative} show several examples.

Table~\ref{tab:occlusion} compares the performance of PIZZA, the best second performing method in our previous evaluation, to our method for different occlusion rates. Our method remains robust even under large occlusions, thanks to the robust discriminative embeddings used to match the images. In Figure~\ref{fig:qualitative}, note that our pose probabilities remain peaked on the correct maximum and still shows clear symmetries when they exist.


% \input{sections/4_occluded_test_sets}

\begin{table}[!t]
    \addtolength{\tabcolsep}{-2pt}
    \centering
    \scalebox{0.85}{
    \begin{tabular}{@{}l l c c  c  c  c  c }
    \toprule
    \parbox[t]{2mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{$\bm{\AccThirty}\uparrow$}}} & Method &  0\% & 5\% & 10\% & 15\%  & 20\% & 25\% \\
    \cmidrule{2-8}
    & PIZZA \cite{nguyen_pizza_2022} & 55.92 &  51.63 &  40.32 &  31.49 &  25.21 &  16.58 \\%&  33.05\\
    & NOPE (ours) & \bf 66.85 & \bf 61.35 &  \bf 55.42 &  \bf 52.10 &  \bf 50.71 &  \bf 47.52  \\% \bf 53.42\\
    \bottomrule
    \end{tabular}}
    \caption{{\bf Robustness to partial occlusions. } We add rectangles of Gaussian noise to the query image, and vary the ratio between the area of the rectangle and the area of the object's 2D bounding box. We compare with PIZZA, the method with the second best performance in Table~\ref{tab:shapeNet}. 
    Our method remains robust under large occlusions, while PIZZA's performance decreases significantly.
    }
    \vspace{-2pt}
    \label{tab:occlusion}
\end{table}


\subsection{Ablation study}
\label{sec:ablation}

\paragraph{Pose conditioning. } 
We experimented with two different pose representations. 
3DiM first creates camera rays from a given 3D pose and uses the positional encoding of \cite{mildenhall2020nerf} to obtain a pixel-wise pose condition.  This representation is then processed by an MLP before being integrated into the feature map of the U-Net as in our method.
We experimented with this method but discovered it does not improve the performance for our problem compared to applying an MLP directly to a representation of the rotation. We favored the second approach for its simplicity.

% conditioning strategy and discovered that it doesn't enhance accuracy. Our current pose conditioning, on the other hand, is much more straightforward.

\paragraph{Rotation representation for the relative pose.} 
Similarly, we experimented with different representations for the rotation:  axis-angles, quaternions, and rotation-6D~\cite{zhou2019continuity}. We observed all these representations yield similar performances, so we use rotation-6D~\cite{zhou2019continuity} in all of our experiments.


\subsection{Runtime analysis} We report the running time of NOPE and 3DiM in Table~\ref{tab:runtime}. Our method is significantly faster than 3DiM, thanks to our strategy of predicting the embedding of novel viewpoints with a single step instead of multiple diffusion steps.


\begin{table}[t]
	\centering
	\resizebox{0.88\linewidth}{!}{
	\begin{tabular}{@{}l r c c@{}}
	\toprule
    \multirow{2}{*}{\bf Method} 
    &\multirow{2}{*}{\bf \parbox{1.3cm}{Memory}}
     & 
    \multicolumn{2}{c}{\textbf{Run-time}}\\
	\cmidrule(lr){3-4}                                
	& & \textbf{Processing} & \textbf{Neighbors search} \\
	\midrule
	3DiM \cite{3dim} & 358.6 MB &  13 mins &   0.31 s\\
	NOPE (ours) &  22.4 MB &  1.01 s &  0.18 s\\
    \bottomrule
	\end{tabular}}
	\caption{{\bf Average run-time}  of our method and 3DiM \cite{3dim} on a single GPU V100. We report the memory used for storing novel views, the time taken to generate novel views, and the time taken for nearest neighbor search to obtain the final prediction. Our code is written in plain Pytorch for simplicity and could be further optimized with AITemplate or TensorRT.}
    \label{tab:runtime}
\end{table}



\subsection{Failure cases}
\label{sec:failureCases}

\input{sections/4_failure_cases}

All the methods fail to yield accurate results when evaluated on the ``bus'' and ``guitar'' categories, as indicated by the high median errors. 
After visual inspection~(see Figure~\ref{fig:failureCases}), it appears that 3D models from the guitar category can be very thin under some viewpoints. Moreover, both the guitar and bus categories are `almost symmetric', in the sense that only small details make the pose non-ambiguous. 

To check if this is really the reason for the poor performance on these two categories, we re-run the evaluation by taking the reference view for the ``guitar" category to be the view with the largest silhouette and by treating both categories as having a 180-degree symmetry. Table~\ref{tab:guitar_and_bus} presents the results of this new evaluation; the metrics are significantly better. This shows that the failures are indeed caused by views where the objects appear to be very thin, and by `quasi-symmetries'.

\begin{table}[!t]
    \addtolength{\tabcolsep}{-2pt}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{@{}l l c  c  c  c  c@{}}
    \toprule
    & Category & ViewNet \cite{mariotti_viewnet_2021} & SSVE \cite{mariotti_semi-supervised_2020} & PIZZA \cite{nguyen_pizza_2022}  & 3DiM \cite{3dim} & Ours \\
    \midrule
    \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{$\bm{\MedErr}\downarrow$}}} & guitar & 70.43 & 67.52 & 60.12 & 55.09 & \bf 51.52\\
    & bus    & 75.02 & 84.42 & 65.64 & 59.02 & \bf 57.64\\
    \cmidrule{2-7}
    & \textdagger{guitar} & 31.05 & 25.46 & 20.87 & 20.05 & \bf 18.65\\
    & \textdagger{bus}    & 19.26 & 23.45 & 17.03 & 13.25 & \bf 12.51\\
    \bottomrule
    \end{tabular}}
    \caption{{\bf Additional experiments on the ``guitar'' and ``bus'' categories. } We compare the base results from Table~\ref{tab:shapeNet} (\textbf{top})  with an evaluation in which we fix the reference viewpoints of the objects from the ``guitar'' category, and we allow a 180$^{\circ}$ symmetry in the error metric  (\textbf{bottom} with \textdagger{}). We report the median of the errors for each method. The values are significantly better than the values reported in Table~\ref{tab:shapeNet}, which validate the reasons for the poor results on these two categories in the general case.
    }
    \label{tab:guitar_and_bus}
    \vspace{-2pt}
\end{table}




% \begin{table}[!t]
%     \addtolength{\tabcolsep}{-2pt}
%     \centering
%     \scalebox{0.8}{
%     \begin{tabular}{@{}l  c  c  c  c  c@{}}
%     \toprule
%     Category & ViewNet \cite{mariotti_viewnet_2021} & SSVE \cite{mariotti_semi-supervised_2020} & PIZZA \cite{nguyen_pizza_2022}  & 3DiM \cite{3dim} & Ours \\
%     \midrule
%     guitar & 31.05 & 25.46 & 20.87 & 20.05 & \bf 18.65\\
%     bus    & 19.26 & 23.45 & 17.03 & 13.25 & \bf 12.51\\
%     \bottomrule
%     \end{tabular}}
%     \caption{{\bf Additional experiments on the ``guitar'' and "bus" categories. } Here, we fix the reference viewpoints of the objects from the ``guitar'' category, and we consider these two categories as have a 180$^{\circ}$ symmetry. We report the median of the errors for each method. The values are significantly better than the values reported in Table~\ref{tab:shapeNet}, which validate the reasons for the poor results on these two categories in the general case.
%     }
%     \label{tab:guitar_and_bus}
% \end{table}
