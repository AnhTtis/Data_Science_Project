Estimating the 3D pose of objects has seen significant progress in the past decade with regard to both robustness and accuracy~\cite{kehl-iccv17-ssd6d,rad-iccv17-bb8,tekin-cvpr18-realtimeseamlesssingleshot,li-eccv18-deepim,zakharov-iccv19-dpod}. Specifically, there has been a considerable increase in robustness to partial occlusions~\cite{peng-cvpr19-pvnet,hu-cvpr19-segmentationdriven6dobjectposeestimation,oberweger-eccv18-makingdeepheatmapsrobust}, and the need for large amounts of real annotated training images has been relaxed through the use of domain transfer~\cite{baek-cvpr20-weaklysuperviseddomainadaptation}, domain randomization~\cite{tremblay-18-deepobjectposeestimation,labbe-eccv20-cosypose,sundermeyer-cvpr20-multipathlearning}, and self-supervised learning techniques~\cite{sundermeyer-ijcv20-augmentedautoencoders} that leverage synthetic images for training.



Unfortunately, the practicality of 3D object pose estimation remains limited for many applications, including robotics and augmented reality. Typically, existing approaches require a 3D model~\cite{megapose}, a video sequence~\cite{onepose,oneposepp}, or sparse multiple images of the target object~\cite{relpose}~(to reference only the most recent works), and a training stage. Several techniques aim to prevent the need for retraining by assuming that new objects fall into a recognized category~\cite{Grabner_CVPR18,Wang_2019_NOCS}, share similarities with the previously trained examples as in the T-LESS dataset~\cite{sundermeyer-cvpr20-multipathlearning}, or exhibit noticeable corners~\cite{Pitteri20203DOD}.



In this paper, we introduce an approach, which we call {\bf NOPE} for {\bf N}ovel {\bf O}bject {\bf P}ose {\bf E}stimation, that only requires a single image of the new object to predict the relative pose of this object in any new images, without 
the need for the object's 3D model and without training on the new object.
This is a very challenging task, as, by contrast with the multiple views used in \cite{onepose,relpose} for example, a single view only provides limited information about the object's geometry.



To achieve this, we train NOPE to generate novel views of an object. These novel views can be used as `templates' annotated with the corresponding poses.  Matching these templates with new input views lets us estimate the object relative pose with respect to the initial view. Our approach relates to the recent development in novel view synthesis. However, we take this one step further: Motivated by the good performance of recent template matching work for pose estimation~\cite{nguyen2022templates, shugurov_osop_2022}, instead of predicting plain color images, we directly predict discriminative embeddings of the views. Specifically, our embeddings are extracted by passing the input image through a U-Net architecture with attention and conditioned on the desired pose for the new view.


In essence, in contrast to existing novel view synthesis works~\cite{realfusion, poole2022dreamfusion}, for our pose estimation task, we do not need to create a 3D model of the object. This saves significant computation time, as optimizing a 3D model using a single view takes at least one hour~\cite{sparsefusion} or longer~\cite{neuralLift}. 
By contrast, our approach based on direct inference of the view embeddings is extremely fast, processing an image in less than 2 seconds. Yet, it yields accurate pose estimates with more than 65\% accuracy on average on unseen instances or categories. Our work is also related to motion prediction between 2 images, such as~\cite{ummenhofer-cvpr17-demon,nguyen_pizza_2022}, which predict the camera displacement between two images. Our comparisons to the closest work to ours~\cite{nguyen_pizza_2022} show that we significantly outperform this approach.



Furthermore, we show that our approach can identify the pose ambiguities due, for example, to symmetries~\cite{tombari}, even if we do not have access to the object 3D model but only to a single view. To this end, we estimate the distribution over all poses for the query, which becomes increasingly less peaked as the pose suffers from increasingly many ambiguities. Figure~\ref{fig:teaser} depicts a variety of ambiguous and unambiguous cases with their pose distributions.


In summary, our main contribution is to show that we can  efficiently and reliably recover the relative pose of an unseen object in novel views given only a single view of that object. To the best of our knowledge, our approach is the first to predict ambiguities due to symmetries and partial occlusions of unseen objects from only a single view.


