% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[review]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage{graphicx}
% Include other packages here, before hyperref.
\usepackage{stfloats}

\usepackage{mathtools} % \coloneqq :=
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\input{command}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{6994} % *** Enter the CVPR Paper ID here
\def\confName{ICCV}
\def\confYear{2023}
\title{Unseen Object Pose Estimation from a Single Image}
\title{NOPE: Novel Object Pose Estimation from a Single Image \\


Supplementary Material
} 

\author{Van Nguyen Nguyen$^{1}$,  Thibault Groueix$^{2}$, Yinlin Hu$^{3}$, Mathieu Salzmann$^{4}$,  Vincent Lepetit$^{1}$\\ % To be defined later
{$^{1}$LIGM, Ecole des Ponts}, {$^{2}$Adobe}, {$^{3}$MagicLeap}, {$^{4}$EPFL}\\
}

\begin{document}
\definecolor{DarkMagenta}{rgb}{0.7, 0.0, 0.7}
\newcommand{\nguyen}[1]{{\color{DarkMagenta}#1}}
\newcommand{\nguyenrmk}[1]{{\color{DarkMagenta} {\bf [VN: #1]}}}

\definecolor{DarkBlue}{rgb}{0.0, 0.0, 0.8}
\newcommand{\thibault}[1]{{\color{DarkBlue} #1}}
\newcommand{\thibaultrmk}[1]{{\color{DarkBlue} {\bf [TG: #1]}}}

\definecolor{DarkOrange}{rgb}{1.0, 0.55, 0.0}
\newcommand{\ms}[1]{{\color{DarkOrange}#1}}
\newcommand{\mathieurmk}[1]{{\color{DarkOrange} {\bf [MS: #1]}}}

\definecolor{DarkGreen}{rgb}{0.0, 0.5, 0.0}
\newcommand{\vincent}[1]{{\color{DarkGreen} #1}}
\newcommand{\vincentrmk}[1]{{\color{DarkGreen} {\bf [VL: #1]}}}

\newcommand{\was}[1]{}

\definecolor{Random}{rgb}{0.1, 0.5, 0.9}
\newcommand{\yinlin}[1]{{\color{Random} #1}}
\newcommand{\yinlinrmk}[1]{{\color{Random} {\bf [YH: #1]}}}


% Uncomment for submitted version:
\renewcommand{\nguyen}[1]{#1}
\renewcommand{\nguyenrmk}[1]{}
\renewcommand{\thibault}[1]{#1}
\renewcommand{\thibaultrmk}[1]{}
\renewcommand{\yinlin}[1]{#1}
\renewcommand{\yinlinrmk}[1]{#1}
\renewcommand{\ms}[1]{#1}
\renewcommand{\mathieurmk}[1]{}
\renewcommand{\vincent}[1]{#1}
\renewcommand{\vincentrmk}[1]{}

\maketitle

\input{tables/shapeNet_std}

\section{Implementation details}
Our implementation of U-Net is mainly based on denoising-diffusion-pytorch \cite{lucidrains2022}. 
%The U-Net architecture is composed of two primary components: the encoder block and the decoder block. 
The encoder receives as input an image embedding of size $8\times32\times32$ and encodes it into a $1024\times4\times4$ representation. The decoder takes as input  a this $1024\times4\times4$ representation and intermediate representations from the encoder through skip connections, as inputs and decodes them into an embedding for a new view of the original size $8\times32\times32$.

Both the encoder and decoder consist of four layers, each of which comprises a residual block, a cross-attention layer that facilitates the injection of pose embeddings into feature maps, and a convolution followed by group normalization and the SiLU activation function.

\begin{figure}[!t]
    \begin{center}
    \includegraphics[width=1.0\linewidth]{figures/supplementary/u_net_architecture.png}
    \end{center}
    \caption{
        \label{fig:uNetArchitecture}
        {Architecture of our U-Net.} 
        \vspace{-5pt} 
        }
\end{figure}

We show in Figure~\ref{fig:uNetArchitecture} an overview of the U-Net architecture we use to generate the novel views' embeddings.


\section{Additional results}

We provide in Table~\ref{tab:resultStd} the 3D pose estimation results of the baselines \cite{nguyen_pizza_2022, mariotti_semi-supervised_2020, mariotti_viewnet_2021, 3dim} and our method on the ShapeNet dataset with standard deviation computed on 5 runs with 5 different reference images.

We also show in Figures~\ref{fig:resultsNoOcclusion} and \ref{fig:resultsOcclusion} some additional results on \textbf{unseen categories} without and with occlusions.



\input{sections/6_supp_qualitative}

\input{sections/6_supp_qualitative_occlusion}


{\small
\bibliographystyle{ieee_fullname}
% \bibliography{references, zotero}
\bibliography{cleaned_refs}
}
%%%%%%%%% REFERENCES


\end{document}
