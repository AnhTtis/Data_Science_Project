\vspace{-6pt}
In this section, we first motivate our approach with an experiment based on novel view generation.
We then describe our architecture, how we train it, and how we use it for pose prediction and for identifying pose ambiguities.

\input{sections/3a_motivation}

\input{sections/3_main_figure.tex}

\label{sec:motivation}

\subsection{Framework}
\label{sec:framework}
Figure~\ref{fig:framework} gives an overview of our approach. 
We train a deep architecture to generate embeddings of novel views of an object using pairs of images of objects from a first set of object categories. In practice, we consider embeddings computed from the pretrained VAE of~\cite{stable-diffusion}, as it was shown to yield a robust representation for template matching. To generate these embeddings, we use a U-Net-like network with a pose conditioning mechanism that is very close to the one introduced in 3DiM~\cite{3dim} and related to the text-to-image mechanism in \cite{stable-diffusion}. 

More precisely, we first use a Multilayer Perceptron~(MLP) to convert the desired relative viewpoint $\Delta R$ with respect to the object pose in the reference view to a pose embedding. We then integrate this pose embedding into the feature map at every stage of our U-Net using cross-attention, as described in \cite{stable-diffusion}. 

\vspace*{-15pt}
\paragraph{Training.} At each iteration, we build a batch composed of $N$ pairs of images, a reference image and another image of the same object with a known relative pose. The U-Net model takes as input the embedding of the reference image and as conditioning the embedding of the relative pose to predict an embedding for the second image. We jointly optimize the U-Net and the MLP by minimizing the Euclidean distance between this predicted embedding and the embedding of the query image. \nguyen{Note that we freeze the pretrained VAE network of \cite{stable-diffusion} during the training.}

By training it on a diverse dataset of objects, this architecture generalizes well to  novel unseen object categories. \nguyen{Interestingly, our method does not explicitly learn any symmetries during training, but it is able to detect pose ambiguities during testing as discussed below.}


\subsection{Pose prediction}
\label{sec:templateMatching}

\vspace{-4pt}
\paragraph{Template matching.} 
Once our architecture is trained, we can use it to generate the embeddings for novel views: Given a reference image $\imageRef$ and a set of $N$ relative viewpoints $\mathcal{P}=\left(\relativeR_1, \relativeR_2, \dots, \relativeR_N \right)$, we can obtain a corresponding set of predicted embeddings $(\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_N)$. To define these viewpoints, we follow the approach used in \cite{nguyen2022templates}: We start with a regular icosahedron and subdivide each triangle recursively into four smaller triangles twice to get 342 final viewpoints. Finally, we simply perform a nearest neighbor search to determine the reference point that has the embedding closest to the embedding of the query image. 
%Note that this can be done efficiently~\cite{nguyen2022templates}.

\vspace{-8pt}
\paragraph{Detecting pose ambiguities.} Pose ambiguities arise when the object has symmetries or when an object part that could remove the ambiguity is not visible, as for the mug in Figure~\ref{fig:teaser}. By considering the distance between the embedding of the query image and the generated embeddings, we not only can predict a single pose but also identify all the other poses that are possible given the reference and query views.

\newcommand{\be}{{\bf e}}

More formally, we estimate the probability of a relative pose $\Delta R$ given reference image $I_r$ and query image $I_q$ as
%
\begin{equation}
p(\Delta R \; | \; I_r, I_q) = \frac{1}{Z} \frac{\be_{\Delta R} \cdot \be_q}{\| \be_{\Delta R} \| \| \be_q \|} \> ,
\end{equation}
%
where $\be_{\Delta R}$ is the embedding predicted by the U-Net from reference image $I_r$ and relative pose $\Delta R$, $\be_q$ is the embedding for query image $I_q$, and $Z$ is a normalization factor. The operator $\cdot$ denotes the dot product.

\input{sections/3_symmetry}

To illustrate this, we show in Figure~\ref{fig:symmetry} three distinct types of symmetry and visualize the pose distribution for corresponding pairs of  reference and query images~(not shown). The number of regions with high similarity scores is consistent with the number of symmetries and pose ambiguities:  If an object has no symmetry, the probability distribution has a clear mode. The probability distribution for objects with symmetries have typically several modes or even a continuous high-probability region in case of rotational symmetry. We provide additional qualitative results in Section~\ref{sec:experiments}.

