In this section, we first review various approaches to novel view synthesis. We then shift our focus to pose estimation techniques that aim to achieve generalization.

\subsection{Novel view synthesis from a single image} 


% Novel views --> Nerfs
Our method generates discriminative feature views, which are conditioned on a reference view and the relative pose between the views. This relates to the pioneering work of NeRFs~\cite{mildenhall2020nerf} since it performs novel-view synthesis. While recent advancements have improved the speed of NeRFs~\cite{muller_instant_2022, sparsevoxelgrid, plenoxel}, our approach is still orders of magnitude faster as it does not require the creation of a full 3D volumetric model. Furthermore, our approach only requires a single input view, whereas a typical NeRF setup necessitates around 50 views. Reducing the number of views required for NeRF reconstruction remains an active research area, especially in the single-view scenario~\cite{pixelNerf, realfusion}.


% NERFs via diffusion models enable novel views from sparse views. 
Recent works~\cite{sparsefusion, realfusion}  have had successes generating novel views via NERFs  using a sparse set of views as input by leveraging 2D diffusion models. For images, the breakthrough in diffusion models~\cite{ddpm,song2020denoising} have unlocked several workflows~\cite{ramesh10hierarchical, saharia2022photorealistic,ruiz2022dreambooth}. For 3D applications, DreamFusion~\cite{poole2022dreamfusion} pioneered a score-distillation sampling that allows for the use of a 2D diffusion model as an image-based loss, leveraged by 3D applications via differentiable rendering. This has resulted in significant improvements for tasks previously trained with a CLIP-based image loss~\cite{clip, dreamfields, CLIPmesh, magic3d, Clipasso, alainjay}.  By building on top of score-distillation sampling, SparseFusion~\cite{sparsefusion} reconstructs a NeRF scene with as few as two views with relative pose, while the concurrent work RealFusion~\cite{realfusion} does it from a single input view, although the reconstruction time is impractical for real-time applications. Our approach is much faster as we do not create a 3D representation of the object.

% diffusion model conditionned on 2d pose
Closest to us, 3DiM~\cite{3dim} and Zero-1-to-3~\cite{liu2023zero} generate novel views of an object by conditioning a diffusion model on the pose. Instead of leveraging foundation diffusion models in 2D like DreamFusion~\cite{poole2022dreamfusion} does, they retrain a diffusion model specifically for this task. While they have not applied their approach to template-based pose estimation, we design such a baseline and compare against it. We find that the diffusion model tends to change the texture or invent wrong details which hinders the performance of the template-based approach. In contrast, our approach generates average novel views directly in an embedding space instead of a pixel space, which is much more efficient~\cite{nguyen2022templates}.  

% regression-based model
Finally, several methods~\cite{mariotti_semi-supervised_2020,mariotti_viewnerf_2022} generate novel views by conditioning a feed-forward neural network on the 3D pose, which we also do with a U-Net. We share with these methods an advantage in speed: such feed-forward neural network are one or two orders of magnitude faster than current diffusion models. However, the way we perform pose estimation is fundamentally different. We use novel-view synthesis in a template-based matching approach~\cite{nguyen2022templates}, while they use it in a regression-based optimization. In practice, we found these methods to work well on a limited number of object categories, and we observed their performance to deteriorate significantly when \nguyen{testing} on novel categories.

\vspace{-4pt}
\subsection{Generalizable object pose estimation}
\vspace{-5pt}
% Although many recent methods for estimating the 3D pose of an object from an image have shown significant advances in both efficiency and accuracy~\cite{wohlhart-cvpr15-learningdescriptors, balntas-iccv17-poseguidedrgbdfeaturelearning, labbe-eccv20-cosypose, sundermeyer-cvpr20-multipathlearning, sundermeyer-eccv18-implicit3dorientationlearning, peng-cvpr19-pvnet, zebrapose}, most of them can only work for known objects, which means they need to be retrained on every new object, limiting their application range.


Several techniques have been explored to generalize better to unseen object pose estimation, such as generic 2D-3D correspondences~\cite{Pitteri20203DOD}, an energy-based strategy~\cite{relpose}, keypoint matching~\cite{onepose}, or template matching~\cite{nguyen2022templates, shugurov_osop_2022, chen_fusion, liu2022gen6d, megapose, nguyen2024gigaPose}. Despite significant progress, these methods either need an accurate 3D model of the target or they  rely on multiple annotated reference images from different viewpoints. These 3D annotations are challenging to obtain in practice. By contrast, we propose a strategy that works with neither the 3D model of the target nor the annotation of multiple views. More importantly, our method predicts accurate poses with only a single reference image, and generalizes to novel objects without retraining.


% \nguyen{Traditional 3D object pose estimation methods are typically designed for known objects\cite{wohlhart-cvpr15-learningdescriptors, balntas-iccv17-poseguidedrgbdfeaturelearning, labbe-eccv20-cosypose, sundermeyer-cvpr20-multipathlearning, sundermeyer-eccv18-implicit3dorientationlearning}. However, recent works have attempted to address the challenge of estimating pose for unseen objects using approaches such as generic 2D-3D correspondences \cite{Pitteri20203DOD}, energy-based methods \cite{relpose}, keypoint matching \cite{sun2022onepose, he_onepose_nodate}, or template matching \cite{nguyen_templates_2022, slobodan_cvpr22, liu2022gen6d, megapose}. Despite these advancements, these methods still rely on a 3D model of the target object or reference images captured from annotated multiple viewpoints. 

% Various conditional generation approaches, including semi-supervised \cite{mariotti_semi-supervised_2020} and unsupervised learning \cite{mariotti_viewnet_2021}, as well as neural radiance fields \cite{mariotti_viewnerf_2022}, have shown promising results in generating conditional views of objects and estimating 3D viewpoints from a single image, similar to our method. However, these methods have only been evaluated on a limited number of object categories, and we have experimentally demonstrated that they struggle to generalize to unseen categories. In contrast, our method builds upon the recent architecture of \cite{stable-diffusion} for injecting the condition into latent space and outperforms all of these approaches in terms of performance on unseen categories.}


% Recently, denoising diffusion models~\cite{ho2020denoising, song2020denoising} have unlocked several workflows, such as generating images from text \cite{ramesh10hierarchical, saharia2022photorealistic} and placing a foreground object in different backgrounds \cite{ruiz2022dreambooth}. In this work, we leverage these class of models for (probabilistic) novel view synthesis while using geometry-aware features as conditioning. Inspired by the impressive results in DreamFusion ~\cite{poole2022dreamfusion} which optimized 3D scenes using text-conditioned diffusion models, we propose a view-conditioned diffusion distillation mechanism to similarly extract 3D modes in the sparse view reconstruction task.
