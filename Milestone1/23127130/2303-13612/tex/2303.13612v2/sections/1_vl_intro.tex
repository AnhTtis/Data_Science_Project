Estimating the 3D pose of objects has seen significant progress in the past decade with regard to both robustness and accuracy~\cite{kehl-iccv17-ssd6d,rad-iccv17-bb8,tekin-cvpr18-realtimeseamlesssingleshot,li-eccv18-deepim,zakharov-iccv19-dpod}. Specifically, there has been a considerable increase in robustness to partial occlusions~\cite{peng-cvpr19-pvnet,hu-cvpr19-segmentationdriven6dobjectposeestimation,oberweger-eccv18-makingdeepheatmapsrobust}, and the need for large amounts of real annotated training images has been relaxed through the use of domain transfer~\cite{baek-cvpr20-weaklysuperviseddomainadaptation}, domain randomization~\cite{tremblay-18-deepobjectposeestimation,loing-ijcv18-virtual,labbe-eccv20-cosypose,sundermeyer-cvpr20-multipathlearning}, and self-supervised learning techniques~\cite{sundermeyer-ijcv20-augmentedautoencoders} that leverage synthetic images for training.

Unfortunately, the practicality of 3D object pose estimation remains limited for many applications, including robotics and augmented reality. Typically, existing approaches require a 3D model~\cite{xiao-bmvc19-posefromshape,megapose,nguyen2022templates,nguyen2024gigaPose}, a video sequence~\cite{onepose,oneposepp}, or sparse multiple images of the target object~\cite{relpose}, and a training stage. Several techniques aim to prevent the need for retraining by assuming that new objects fall into a recognized category~\cite{Grabner_CVPR18,Wang_2019_NOCS}, share similarities with the previously trained examples as in the T-LESS dataset~\cite{sundermeyer-cvpr20-multipathlearning}, or exhibit noticeable corners~\cite{Pitteri20203DOD}.



In this paper, we introduce an approach, which we call {\bf NOPE} for {\bf N}ovel {\bf O}bject {\bf P}ose {\bf E}stimation, that only requires a single image of the new object to predict the relative pose of this object in any new images, without the need for the object's 3D model and without training on the new object. This is a very challenging task, as, by contrast with the multiple views used in \cite{onepose,relpose} for example, a single view only provides limited information about the object's geometry.


\vincent{
To achieve this, we train NOPE to predict the appearance of the object under novel views. We use these predictions as `templates' annotated with the corresponding poses.  Matching these templates with new input views lets us estimate the object relative pose with respect to the initial view. This approach is motivated by the good performance of recent related work~\cite{nguyen2022templates, shugurov_osop_2022}. In particular, \cite{nguyen2022templates} showed that template matching can be extremely fast and robust to partial occlusions. This contrasts with methods that rely on a deep network to predict the probability of a pose~\cite{relpose}.

Since our method relies on predicting the appearance of the target object, it relates to recent developments in novel view synthesis. However, it has two critical differences: The first difference is that instead of predicting color images, we directly predict discriminative embeddings of the views. These embeddings are extracted by passing the input image through a U-Net architecture with attention and conditioned on the desired pose for the new view. 

The second main difference of our approach with novel view synthesis is more fundamental. We first note that generating novel views given a single view of an object is ambiguous. Novel view synthesis usually focuses on generating a single possible image for a given point of view. This is however not suitable for our purpose: The view synthesis method will ``invent'' the parts that were not visible in the input view. As illustrated in Figure~\ref{fig:motivation}, these invented parts create a plausible novel view but there is no guarantee this view actually corresponds to the actual view. For our goal of pose estimation, the invented parts will not match in general the query view and this will result in incorrect pose estimation. The limitations of using novel view synthesis for pose estimation will further be quantitatively demonstrated in our experiments~(see Table~\ref{tab:shapeNet}).

Our approach to handling the ambiguities in novel view synthesis for template matching is to consider the \emph{distribution} of all the possible appearances of the object for the target viewpoint. More exactly, we train NOPE to predict the average of all the possible appearances of the object. We then treat the predicted average as a template: Under some simple assumptions, the distance between this template and the query view is directly related to the probability of the query view to be a sample from the distribution of the possible appearances of the object. This approach allows us to deal with the ambiguities of novel view prediction in a robust and efficient way:  Predicting the average views is just a direct inference of NOPE and is thus very fast,  and robust to partial occlusions thank to template-matching. 

Furthermore, our approach can identify the pose ambiguities due, for example, to symmetries~\cite{tombari}, even if we do not have access to the object 3D model but only to a single view. To this end, we estimate the distribution over all poses for the query, which becomes increasingly less peaked as the pose suffers from increasingly many ambiguities. Figure~\ref{fig:teaser} depicts a variety of ambiguous and unambiguous cases with their pose distributions.
}
%computing the distances between the average views and query view is also very fast  thanks to partial occlusions

% To motivate our approach, we evaluate the use of the state-of-the-art 3D model-based image generation method (3DiM)~\cite{3dim} for pose estimation. 3DiM is a diffusion-based method for view synthesis in pixel space. To apply it to pose prediction given a reference image of an object, we experimented with using it to generate a set of novel views from many different viewpoints and match a query image of the object to these views in pixel space. Since the generated views are annotated with the corresponding pose, this gives us a pose estimate. 

% As shown in Figure~\ref{fig:motivation}, the images generated by 3DiM look very realistic. However, the recovered poses are not very accurate. This can be explained in part by the fact that diffusion can ``invent'' details that disturb image matching. The limitations of this approach will further be quantitatively demonstrated in our experiments (Table~\ref{tab:shapeNet}). This motivates us to learn to directly generate a discriminative representation of the views.


\begin{figure}[t]
\newlength{\plotheight}
\setlength\plotheight{1.5cm}
\centering
\setlength\lineskip{2.0pt}
\setlength\tabcolsep{2.0pt} 
{\small
\begin{tabular}{
% 
>{\centering\arraybackslash}m{\plotheight}
>{\centering\arraybackslash}m{\plotheight}
>{\centering\arraybackslash}m{\plotheight}%|
>{\centering\arraybackslash}m{\plotheight}%|
>{\centering\arraybackslash}m{\plotheight}
}
 & & Generated & Recovered &  \\
 & & view from & pose by & Estimated \\
 & & the query & template &  pose \\
Reference & Query & GT pose & matching & distribution \\
\frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample9/ref.png}}&
\frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample9/query.png}}&
\frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample9/wonder3d.png}}&
\frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample9/pose.png}}&
\frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample9/proba.png}}\\% [0em]

\frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample8/ref.png}}&
\frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample8/query.png}}&
\frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample8/wonder3d.png}}&
\frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample8/pose.png}}&
\frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample8/proba.png}}\\
% \frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample4/ref.png}}&
% \frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample4/query.png}}&
% \frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample4/3dim.png}}&
% \frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample4/pose.png}}&
% \frame{\includegraphics[height=\plotheight, ]{figures/method/motivation/sample2/proba.png}}\\
\end{tabular}
% \begin{tabular}{
% >{\centering\arraybackslash}m{0.3\teaserheight}
% }
% \includegraphics[height=3\teaserheight, ]{figures/teaser/colorbar.png}\\
% \end{tabular}
}
\vspace*{-10pt}
\caption{
\vincent{
\textbf{The limit of novel view synthesis for pose prediction.} While the images generated by Wonder3D~\cite{long2023wonder3d} \vincentrmk{Can we have Wonder3D here?} \nguyenrmk{I put two samples of Wonder3d} look very realistic, they have to invent unseen parts, impairing the similarity computation between the query image and the generated view, and hence the pose estimation: The probability distributions computed by template matching do not peak on the right pose but show many wrong local maxima. This is not a limitation of Wonder3D but of view synthesis from a single view in general.
}
}
\label{fig:motivation}
\end{figure}



% In essence, in contrast to existing novel view synthesis works~\cite{realfusion, poole2022dreamfusion}, for our pose estimation task, we do not need to create a 3D model of the object. This saves significant computation time, as optimizing a 3D model using a single view takes at least one hour~\cite{sparsefusion} or longer~\cite{neuralLift}. 
% By contrast, our approach based on direct inference of the view embeddings is extremely fast, processing an image in less than 2 seconds. Yet, it yields accurate pose estimates with more than 59 accuracy on average on unseen instances or categories. Our work is also related to motion prediction between 2 images, such as~\cite{ummenhofer-cvpr17-demon,nguyen_pizza_2022}, which predict the camera displacement between two images. Our comparisons to the closest work to ours~\cite{nguyen_pizza_2022} show that we significantly outperform this approach.




In summary, our main contribution is to show we can  efficiently and reliably recover the relative pose of an unseen object in novel views given only a single view of that object \renaud{as reference}. To the best of our knowledge, our approach is the first to predict ambiguities due to symmetries and partial occlusions of unseen objects from only a single view.
