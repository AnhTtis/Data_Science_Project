\vspace{-6pt}

In this section, we first describe our experimental setup in Section~\ref{sec:experimental setup}. We then compare our method to others \cite{nguyen_pizza_2022, mariotti_semi-supervised_2020, mariotti_viewnet_2021, 3dim, sundermeyer-cvpr20-multipathlearning, nguyen2022templates} on both synthetic and real-world datasets in Section~\ref{sec:main_results}. Section~\ref{sec:occlusions} reports an evaluation of the robustness to partial occlusions. We  provide the run-time in Section~\ref{sec:run_time}. Finally, we discuss  failure cases in Section~\ref{sec:failureCases}. An ablation study is provided in the supp.~mat.

 % We then conduct an ablation study to investigate the effectiveness of our method in different settings in Section~\ref{sec:ablation}, and provide the run-time in Section~\ref{sec:run_time}. Finally, we discuss  failure cases in Section~\ref{sec:failureCases}.




\subsection{Experimental setup}
\label{sec:experimental setup}

\input{sections/4_dataset}
\input{tables/forge}
To the best of our knowledge, we are the first method addressing the problem of object pose estimation from a single image when the object belongs to a category not seen during training: PIZZA~\cite{nguyen_pizza_2022} evaluated on the DeepIM refinement benchmark, which is made of pairs of images with a small relative pose; SSVE~\cite{mariotti_semi-supervised_2020} and ViewNet~\cite{mariotti_viewnet_2021} evaluated only on objects from categories seen during training.  We therefore  had to create a new benchmark to evaluate our method. 

\vspace{-12pt}
\paragraph{Synthetic dataset.} 
We created a dataset as in FORGE~\cite{forge_jiang} using the same ShapeNet~\cite{chang2015shapenet} object categories.
For the training set, we randomly select 1000 object instances from each of the 13 categories as done in FORGE~(\textit{airplane, bench, cabinet, car, chair, display, lamp, loudspeaker, rifle, sofa, table, telephone, and vessel}), resulting in a total of 13,000 instances. We build two separate test sets for evaluation. The first test set is the ``novel instances'' set, which contains \nguyen{50 new instances for each training category}. The second test set is the ``novel category'' set, which includes \nguyen{100 models per category for} the 10 unseen categories selected by FORGE~(\textit{bus, guitar, clock, bottle, train, mug, washer, skateboard, dishwasher, and pistol}). For each 3D model, we randomly select camera poses to produce five reference images and five query images. We use BlenderProc~\cite{denninger2019blenderproc} as rendering engine. 

Figure~\ref{fig:shapeNet} illustrates the categories used for training our architecture and the categories used for testing it. The shapes and appearances of the categories in the test set are very different from the shapes and appearances of the categories in the training set, and thus constitute a good test set for generalization to unseen categories.

\vspace{-12pt}
\paragraph{Real-world dataset.}
We evaluate on the T-LESS dataset \cite{hodan-wacv17-tless} following the evaluation protocol of \cite{sundermeyer-cvpr20-multipathlearning}: we train only on objects 1-18 and test on the full PrimeSense test set using the ground-truth masks. At inference, we randomly sample a non-occluded reference image either from \emph{all views} or only from \emph{front views}~(-45\textdegree $\leq$ azimuth $\leq$ 45\textdegree), which often offers more information on the object and illustrates the influence of the reference view.

%The visualization for the shape and appearance difference between training and testing objects can be found in \cite{hodan2018bop}.}

\vspace{-12pt}
\paragraph{Metrics.}
For the ShapeNet dataset, we report two different metrics based on relative camera pose error as done in \cite{mariotti_semi-supervised_2020}. Specifically, we provide the median pose error across instances for each category in the test set, and the accuracy $\bm{\AccThirty}$ for which a prediction is treated as correct when the pose error is $\le 30^{\circ}$. Additionally, we present the results of our method for the top 3 and 5 nearest neighbors retrieved by template matching.

For the T-LESS dataset, as most objects are symmetric, we report the recall VSD metric as done in \cite{sundermeyer-cvpr20-multipathlearning}. Please note that for the evaluation on the T-LESS dataset, we also predict the translation by using the same formula ``projective distance estimation'' as SSD-6D~\cite{kehl-iccv17-ssd6d}, as done in~\cite{sundermeyer-eccv18-implicit3dorientationlearning, sundermeyer-cvpr20-multipathlearning}. This translation is deduced from the retrieved template and the relative scale factor between the two input images, as detailed in Section~8 of~\cite{nguyen2022templates}.

% \vincentrmk{In table 2, you say that you treat bottle as a symmetric category. What does it mean? How do you change the metrics for bottle, exactly? } \nguyenrmk{It means I change the metric for bottle: The error is only the difference of elevation angle}

\vspace{-12pt}
\paragraph{Baselines.}

We compare our work with all previous methods that aim to predict a pose from a single view: PIZZA \cite{nguyen_pizza_2022}, a regression-based approach that directly predicts the relative pose, as well as SSVE~\cite{mariotti_semi-supervised_2020} and ViewNet~\cite{mariotti_viewnet_2021}, which employ semi-supervised and self-supervised techniques to treat viewpoint estimation as an image reconstruction problem using conditional generation. We also compare our method with the recent diffusion-based method 3DiM~\cite{3dim}, which generates pixel-level view synthesis. Since 3DiM originally only targets view-synthesis and is not designed for 3D object pose, we use it to generate templates and perform nearest neighbor search to estimate a 3D object pose. To make 3DiM work in the same setting as us, we retrain it using relative pose conditioning instead of canonical pose conditioning.

\vspace{-12pt}
\paragraph{Implementation.}
Only the code of PIZZA is available. The other methods did not release their code at the time of writing, however we re-implemented them. We use a ResNet18 backbone  as in~\cite{nguyen_pizza_2022} for PIZZA, SSVE, and ViewNet. We train all models on input images with a resolution of 256$\times$256 except for 3DiM for which we use a resolution of 128$\times$128 since 3DiM performs view synthesis in pixel space, which takes much more memory. Our re-implementations achieve similar performance as the original papers when evaluated on the same data for seen categories, as shown in Table~\ref{tab:shapeNet}, which validates our comparisons. 
Our method also uses the frozen encoder from \cite{stable-diffusion} to encode the input images into embeddings of size 32$\times$32$\times$8. \nguyen{In all settings, we train the baselines and our method using the same training set and  AdamW~\cite{loshchilov_decoupled_2017} with an initial learning rate of $5\,{\times}\,10^{-5}$. % 5e-5
Training takes about 20 hours on 4 V100 GPUs for each method.

} %We refer the reader to the supplementary material for more implementation details.


\input{tables/t_less}

\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
    \footnotesize{Seen objects: \#4, \#14} & \footnotesize{Novel objects: \#20, \#22} \\
    \includegraphics[width=0.46\linewidth]{rebuttal/obj4.png} & \includegraphics[width=0.46\linewidth]{rebuttal/obj20.png}\\
    \includegraphics[width=0.46\linewidth]{rebuttal/obj14.png} & \includegraphics[width=0.46\linewidth]{rebuttal/obj22.png} \\[-1mm]
    \footnotesize{Reference$\;\;\;\;$Query$\;\;\;\;$Prediction} & \footnotesize{Reference$\;\;\;\;$Query$\;\;\;\;$Prediction}\\
    \end{tabular}
    % \vspace*{-10pt}
    \caption{{\bf Qualitative results on real images of T-LESS.} For each sample, we show in the last column the predicted poses. }
    % \textbf{We visualize the predicted poses by rendering the object from these poses, but the 3D model is only used for visualization purposes, not as input to our method.}
    % \vspace*{-4mm}
    \label{fig:tless}
\end{figure}

\input{sections/4_failure_cases}

\input{sections/4_qualitative}

% \vspace{-4pt}
\subsection{Comparison with the state of the art}
\label{sec:main_results}
% \vspace{-4pt}
\subsubsection{Results on ShapeNet}
Table~\ref{tab:shapeNet} summarizes the results of our method compared with the baselines discussed above.
%other methods~\cite{mariotti_semi-supervised_2020, mariotti_viewnet_2021, nguyen_pizza_2022, 3dim}. 
Under both the Acc30 and Median metrics, our method consistently achieves the best overall performance, outperforming the baselines by more than 10\% in Acc30 and 10$^o$ in Median. 
In particular, while other works produce reasonable results on unseen instances of seen training categories, they often struggle to estimate the 3D pose of objects from unseen categories. By contrast, our method works well in this case, demonstrating a better generalization ability on unseen categories.

% thanks its efficient novel synthesis in latent space.

Figure~\ref{fig:qualitative} shows some visualization results of our method on unseen categories, with and without symmetries. Our method  produces more accurate 3D poses than the baselines when there is a symmetry axis.

% \vspace*{-10pt}
\subsubsection{Results on T-LESS}

% \vspace*{-4pt}
Table~\ref{tab:tless} shows our comparison with~\cite{nguyen_pizza_2022,sundermeyer-cvpr20-multipathlearning,nguyen2022templates} on real images of T-LESS. While our method focuses on the more challenging case of using \textit{a single reference image}, \cite{nguyen2022templates,sundermeyer-cvpr20-multipathlearning} rely on ground-truth CAD models. Our method consistently outperforms the baseline PIZZA by a large margin. Interestingly, although there is still a gap compared to the SOTA~\cite{nguyen2022templates}, our method outperforms MultiPath~\cite{sundermeyer-cvpr20-multipathlearning}. Figure~\ref{fig:tless} shows results on seen and unseen objects of T-LESS.


% \vspace*{-7pt}
\subsection{Robustness to occlusions}
\label{sec:occlusions}
% \vspace*{-6pt}
To evaluate the robustness of our method against occlusions, we added random rectangle filled with Gaussian noise to the query images over the objects, in a  similar way to  Random Erasing~\cite{zhong2020random}. We vary the size of the rectangles to cover a range betwen 0\% to 25\% of the bounding box of the object.  Figures~\ref{fig:teaser} and \ref{fig:qualitative} show several examples.



\begin{table}[!t]
    \addtolength{\tabcolsep}{-2pt}
    \centering
    \scalebox{0.85}{
    \begin{tabular}{@{}l l c c  c  c  c  c }
    \toprule
    \parbox[t]{2mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{$\bm{\AccThirty}\uparrow$}}} & Method &  0\% & 5\% & 10\% & 15\%  & 20\% & 25\% \\
    \cmidrule{2-8}
    & PIZZA \cite{nguyen_pizza_2022} & 48.9 &  44.6 &  33.3 &  24.5 &  18.2 &  14.6 \\%&  33.05\\
    & NOPE (ours) & \bf 59.8 & \bf 54.3 &  \bf 48.4 &  \bf 45.1 &  \bf 43.7 &  \bf 40.5  \\% \bf 53.42\\
    \bottomrule
    \end{tabular}}
    % \vspace{-8pt}
    \caption{{\bf Robustness to partial occlusions. } We add rectangles of Gaussian noise to the query image, and vary the ratio between the area of the rectangle and the area of the object's 2D bounding box. 
    % We compare with PIZZA, the method with the second best performance in Table~\ref{tab:shapeNet}. 
    Our method remains robust under large occlusions, while PIZZA's performance decreases significantly.
    }
    \label{tab:occlusion}
\end{table}

Table~\ref{tab:occlusion} compares PIZZA, the best second performing method in our previous evaluation, to our method for different occlusion rates. Our method remains robust even under large occlusions, thanks to embedding matching. Figure~\ref{fig:qualitative} shows that our pose probabilities remain peaked on the correct maximum and shows clearly the symmetries.

\begin{table}[t]
	\centering
	\resizebox{0.88\linewidth}{!}{
	\begin{tabular}{@{}l r c c@{}}
	\toprule
    \multirow{2}{*}{\bf Method} 
    &\multirow{2}{*}{\bf \parbox{1.3cm}{Memory}}
     & 
    \multicolumn{2}{c}{\textbf{Run-time}}\\
	\cmidrule(lr){3-4}                                
	& & \textbf{Processing} & \textbf{Neighbors search} \\
	\midrule
	3DiM \cite{3dim} & 358.6 MB &  13 min &   0.31 s\\
	NOPE (ours) &  22.4 MB &  1.01 s &  0.18 s\\
    \bottomrule
	\end{tabular}}
 % \vspace{-10pt}
\caption{{\bf Average run-time}  of our method and 3DiM \cite{3dim} on a single GPU V100. We report the memory used for storing novel views, the time taken to generate novel views, and the time taken for nearest neighbor search to obtain the final prediction. }
    \label{tab:runtime}
\end{table}

\subsection{Runtime analysis} 
\label{sec:run_time}
% \vspace*{-5pt}
We report the running time of NOPE and 3DiM in Table~\ref{tab:runtime}. Our method is significantly faster than 3DiM, thanks to our strategy of predicting the embedding of novel viewpoints with a single step instead of multiple diffusion steps.


\subsection{Failure cases}
\label{sec:failureCases}

% \vspace*{-4pt}

All the methods fail to yield accurate results when evaluated on ``clock'', ``dishwasher'', ``guitar'', and ``mug'' categories, as indicated by the high median errors. As shown in Figure~\ref{fig:failureCases}, these categories except ``guitar'' are ``almost symmetric'', in the sense that only small details make the pose non-ambiguous. Our predictions using the top-3 and top-5 nearest neighbors significantly improves median errors for 90-symmetrical, 180-symmetrical objects, but not circular-symmetrical as mug objects. Additionally, guitar objects  can appear very thin under certain viewpoints.

%After visual inspection~(see Figure~\ref{fig:failureCases}), it appears that 3D models from the guitar category can be very thin under some viewpoints. 

% To check if this is really the reason for the poor performance on these two categories, we re-run the evaluation by taking the reference view for the ``guitar" category to be the view with the largest silhouette and by treating both categories as having a 180-degree symmetry. Table~\ref{tab:guitar_and_bus} presents the results of this new evaluation; the metrics are significantly better. This shows that the failures are indeed caused by views where the objects appear to be very thin, and by `quasi-symmetries'.

% \begin{table}[!t]
%     \addtolength{\tabcolsep}{-2pt}
%     \centering
%     \scalebox{0.8}{
%     \begin{tabular}{@{}l l c  c  c  c  c@{}}
%     \toprule
%     & Category & ViewNet \cite{mariotti_viewnet_2021} & SSVE \cite{mariotti_semi-supervised_2020} & PIZZA \cite{nguyen_pizza_2022}  & 3DiM \cite{3dim} & Ours \\
%     \midrule
%     \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{$\bm{\MedErr}\downarrow$}}} & guitar & 70.43 & 67.52 & 60.12 & 55.09 & \bf 51.52\\
%     & bus    & 75.02 & 84.42 & 65.64 & 59.02 & \bf 57.64\\
%     \cmidrule{2-7}
%     & \textdagger{guitar} & 31.05 & 25.46 & 20.87 & 20.05 & \bf 18.65\\
%     & \textdagger{bus}    & 19.26 & 23.45 & 17.03 & 13.25 & \bf 12.51\\
%     \bottomrule
%     \end{tabular}}
%     \caption{{\bf Additional experiments on the ``guitar'' and ``bus'' categories. } We compare the base results from Table~\ref{tab:shapeNet} (\textbf{top})  with an evaluation in which we fix the reference viewpoints of the objects from the ``guitar'' category, and we allow a 180$^{\circ}$ symmetry in the error metric  (\textbf{bottom} with \textdagger{}). We report the median of the errors for each method. The values are significantly better than the values reported in Table~\ref{tab:shapeNet}, which validate the reasons for the poor results on these two categories in the general case.
%     }
%     \label{tab:guitar_and_bus}
%     \vspace{-2pt}
% \end{table}
