\vspace{-6pt}
\vincent{
In this section, we first introduce our formalism, then describe our architecture and how we train it, and finally how we use it for pose prediction and for identifying pose ambiguities.
}

% \input{sections/3a_motivation}

\input{sections/3_main_figure.tex}

% \label{sec:motivation}

\subsection{Formalization}

Given a reference image $I_r$ of a target object and a query image $I_q$ of the same object, we would like to estimate the probability $p(\Delta R \;|\; I_r, I_q)$ that the relative motion between $I_r$ and $I_q$ is a certain discretized relative pose $\Delta R$. We assume that this probability follows a normal distribution in the embedding space of the images:
%
\begin{equation}
p(\Delta R \;|\; I_r, I_q) = \mathcal{N}(\rve_q \;|\; \rve(\rve_r, \Delta R), \Sigma(\rve_r, \Delta R)) \> ,
\label{eq:N}
\end{equation}
%
where $\rve_q$ and $\rve_r$ are the embeddings for query image $I_q$ and reference image $I_r$ respectively, $\rve(\rve_r, \Delta R)$ is the mean of the normal distribution, and $\Sigma(\rve_r, \Delta R)$ its covariance. This approach allows us to handle the fact that the object can have various appearances from viewpoint $\Delta R$ given the reference image, as discussed in the introduction.

We take the mean $\rve(\rve_r, \Delta R)$ as the average embedding for the appearance of the object from pose $\Delta R$ over the possible 3D shapes for the object:
%
\begin{equation}
\rve(\rve_r, \Delta R) = \int_{\mathcal{M}} \rve(\Delta R, \mathcal{M}) p(\mathcal{M} | \rve_r)d\mathcal{M} \> ,
\end{equation}
%
with $\mathcal{M}$ a 3D model of testing object and $\rve(\Delta R, \mathcal{M})$ the image embedding of same object under pose $\Delta R$. $\rve(\rve_r, \Delta R)$ may look complicated to compute, but it is in fact easy to train a deep network to predict it using the L2 loss:
%
\begin{equation}
\sum_{(\rve_1, \rve_2, \Delta R)} \|F(\rve_r, \Delta R) - \rve_2 \|^2 \> .
\end{equation}
%
$F$ denotes the network, $(\rve_1, \rve_2, \Delta R)$ is a training sample where $\rve_1$ is the embedding for a view of a training object and $\rve_2$ the embedding for the view of the same object after pose change $\Delta R$. During training, given enough  samples, $F(\rve_r, \Delta R)$ will converge naturally towards $\rve(\rve_r, \Delta R)$. 



\subsection{Framework}
\label{sec:framework}

Figure~\ref{fig:framework} gives an overview of our approach. 
We train a deep architecture to predict the average embeddings of novel views of an object using pairs of images of objects and the corresponding pose changes from a first set of object categories. In practice, we consider embeddings computed from the pretrained VAE of~\cite{stable-diffusion}, as it was shown to be robust for template matching. To generate these embeddings, we use a U-Net-like network with a pose conditioning mechanism that is very close to the one of 3DiM~\cite{3dim}. 
%and related to the text-to-image mechanism in \cite{stable-diffusion}. 

More precisely, we first use an MLP to convert the desired relative viewpoint $\Delta R$ with respect to the object pose in the reference view to a pose embedding. We then integrate this pose embedding into the feature map at every stage of our U-Net using cross-attention, as  in \cite{stable-diffusion}. 

\vspace*{-15pt}
\paragraph{Training.} At each iteration, we build a batch composed of $N$ pairs of images, a reference image and another image of the same object with a known relative pose. The U-Net model takes as input the embedding of the reference image and as conditioning the embedding of the relative pose to predict an embedding for the second image. We jointly optimize the U-Net and the MLP by minimizing the Euclidean distance between this predicted embedding and the embedding of the query image. Note that we freeze the pretrained VAE network of \cite{stable-diffusion} during the training.

By training it on a dataset of diverse objects, this architecture generalizes well to  novel unseen object categories. Interestingly, our method does not explicitly learn any symmetries during training, but it is able to detect pose ambiguities during testing as discussed below.


\subsection{Pose prediction}
\label{sec:templateMatching}

\vspace{-4pt}
\paragraph{Template matching.} 
Once our architecture is trained, we can use it to generate the embeddings for novel views: Given a reference image $I_r$ and a set of $N$ relative viewpoints $\mathcal{P}=\left(\relativeR_1, \relativeR_2, \dots, \relativeR_N \right)$, we can obtain a corresponding set of predicted embeddings $(\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_N)$. To define these viewpoints, we follow the approach used in \cite{nguyen2022templates}: We start with a regular icosahedron and subdivide each triangle recursively into four smaller triangles twice to get 342 final viewpoints. Finally, we simply perform a nearest neighbor search to determine the reference point that has the embedding closest to the embedding of the query image. 
%Note that this can be done efficiently~\cite{nguyen2022templates}.

\vspace{-8pt}
\paragraph{Detecting pose ambiguities.} Pose ambiguities arise when the object has symmetries or when an object part that could remove the ambiguity is not visible, as for the mug in Figure~\ref{fig:teaser}. By considering the distance between the embedding of the query image and the generated embeddings, we not only can predict a single pose but also identify all the other poses that are possible given the reference and query views.

\newcommand{\be}{{\bf e}}

This can be done simply by relying on the normal distribution introduced in Eq.~(\ref{eq:N}):
%
\begin{equation}
\log p(\Delta R \; | \; I_r, I_q) \propto \|F(\rve_r, \Delta R) - \rve_q\|^2 \> .
\end{equation}

% More formally, we estimate the probability of a relative pose $\Delta R$ given reference image $I_r$ and query image $I_q$ as
% %
% \begin{equation}
% p(\Delta R \; | \; I_r, I_q) = \frac{1}{Z} \frac{\be_{\Delta R} \cdot \be_q}{\| \be_{\Delta R} \| \| \be_q \|} \> ,
% \end{equation}
% %
% where $\be_{\Delta R}$ is the embedding predicted by the U-Net from reference image $I_r$ and relative pose $\Delta R$, $\be_q$ is the embedding for query image $I_q$, and $Z$ is a normalization factor. The operator $\cdot$ denotes the dot product.



\input{sections/3_symmetry}

To illustrate this, we show in Figure~\ref{fig:symmetry} three distinct types of symmetry and visualize the pose distribution for corresponding pairs of  reference and query images~(not shown). The number of regions with high similarity scores is consistent with the number of symmetries and pose ambiguities:  If an object has no symmetry, the probability distribution has a clear mode. The probability distribution for objects with symmetries have typically several modes or even a continuous high-probability region in case of rotational symmetry. We provide additional qualitative results in Section~\ref{sec:experiments}.

