% \documentclass[10pt,twocolumn,letterpaper]{article}
% \usepackage[rebuttal]{cvpr}

% % Include other packages here, before hyperref.
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{booktabs}


% % If you comment hyperref and then uncomment it, you should delete
% % egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% % run, let it finish, and you should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% % Support for easy cross-referencing
% \usepackage[capitalize]{cleveref}
% \crefname{section}{Sec.}{Secs.}
% \Crefname{section}{Section}{Sections}
% \Crefname{table}{Table}{Tables}
% \crefname{table}{Tab.}{Tabs.}

% \usepackage[dvipsnames,table,xcdraw]{xcolor}

% \renewcommand{\thetable}{A\arabic{table}}

% % If you wish to avoid re-using figure, table, and equation numbers from
% % the main paper, please uncomment the following and change the numbers
% % appropriately.
% %\setcounter{figure}{2}
% %\setcounter{table}{1}
% %\setcounter{equation}{2}

% % If you wish to avoid re-using reference numbers from the main paper,
% % please uncomment the following and change the counter for `enumiv' to
% % the number of references you have in the main paper (here, 6).
% %\let\oldthebibliography=\thebibliography
% %\let\oldendthebibliography=\endthebibliography
% %\renewenvironment{thebibliography}[1]{%
% %     \oldthebibliography{#1}%
% %     \setcounter{enumiv}{6}%
% %}{\oldendthebibliography}


% %%%%%%%%% PAPER ID  - PLEASE UPDATE
% \def\cvprPaperID{8067} % *** Enter the CVPR Paper ID here
% \def\confName{CVPR}
% \def\confYear{2023}

% \begin{document}

% %%%%%%%%% TITLE - PLEASE UPDATE
% %\title{Author Response of Paper 8067}  % **** Enter the paper title here
% % \title{HOICLIP: Efficient Knowledge Transfer for HOI Detection with Visual Linguistic Model}
% %\maketitle
% \thispagestyle{empty}
% \appendix

% %%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW

% We sincerely thank the reviewers for their feedback.
% We are encouraged that the reviewers find our method \textit{novel and effective} (\textcolor{Blue}{R1},\textcolor{Green}{R3}), \textit{paper well written} (\textcolor{Blue}{R1},\textcolor{Orange}{R2},\textcolor{Green}{R3}), and \textit{experiments extensive} (\textcolor{Blue}{R1},\textcolor{Green}{R3}) with \textit{promising gains} (\textcolor{Orange}{R2},\textcolor{Green}{R3}). We address the reviewers' concerns below.

% % HOICLIP目标是挖掘CLIP里面的信息来增强HOI Detection，因此我们提出了从两个方向实现这个目的，一个是CLIP knowledge integration，将CLIP的visual信息直接融入整体检测框架，第二个是用visual semantic arithmetic提取CLIP对抽象概念的理解，实现是在verb adapter里面。
% \noindent \textbf{\textcolor{blue}{@R1-Q1:} Motivation for model design.} 
% %The purpose of HOICLIP is to fully exploit knowledge in CLIP for HOI detection. To achieve the goal, we choose the strategy of directly integrating CLIP knowledge into model instead of distillation. Besides, we explore the CLIP's ability of understanding abstract verb concept through visual semantic arithmetic. These designs works collaborative to benefit HOI detection with CLIP.
% The overall design aims to fully exploit CLIP-based knowledge for a base transformer-based HOI detector from two aspects: 1) In order to preserve the CLIP representations for few/zero-shot HOI classification, we introduce a cross-attention module to directly fuse the features from the base HOI detector and CLIP encoder (Sec.~3.2); 2) To cope with the long-tailed verb distribution (Fig.~1b), we develop a verb representation based on visual semantic arithmetic to improve the verb classification under low-data regime (also see \textbf{\textcolor{Orange}{R2-Q1}}).        

% % 在UC中，这几篇工作使用的作为unseen的HOI类别与本文不同，不方便比较；UO的比较附在表中，HOICLIP的表现更为优异；reviewer提及的几篇工作的其中一篇ATL在原paper中Table 2已经比较过
% \noindent \textbf{\textcolor{blue}{@R1-Q2:} Comparison with other zero-shot methods.} 1) We have compared with ATL in the Table~2 of the main paper. 
% 2) Those prior works adopt a different unseen combination setting (UC) than recent literature (also used in our work). To compare with them, we conduct experiments under the same setting in Table~\ref{tab:zs_results_add}. 3) We also present the comparison under unseen object setting (UO) in Table~\ref{tab:zs_results_add}. We note that those prior methods utilize additional object category information from a pretrained detector. However, our HOICLIP still achieve superior overall performance. 

% % Comparison with other works under zero-shot settings.} We compare the performance of ATL in main paper Table 2. Other works adopt a slightly different zero-shot setting, which is the choice of unseen categories. To make a fair comparison with Shen et al., Bansal et al. and ConsNet, we conduct experiments under same setting in Table~\ref{tab:zs_results_add}. Under unseen object setting, previous methods adopt a stronger setting where object category is provided by detector. However, HOICLIP achieve a better overall performance.

% \begin{table}[h]
% \vspace{-0.3cm}
% \small
% \centering
% \scalebox{0.8}{
% \begin{tabular}{lcccc}
% \hline
% Method                  & Setting & Unseen         & Seen           & Full           \\
% \hline
% Shen et al.                    & UC          & 10.06           & 24.28          & 21.43          \\
% Bansal et al.                     & UC          & 9.18           & 24.67          & 21.57          \\
% ConsNet                    & UC          & 13.16          & 24.23          & 22.01          \\
% \rowcolor{blue!6}HOICLIP                  & UC          & \textbf{25.53} & \textbf{34.85} & \textbf{32.99} \\
% \hline

% Bansal et al.             & UO             & 11.22          & 14.36          & 13.84         \\
% ConsNet                & UO             & \textbf{19.27}           & 20.99         & 20.71          \\
% \rowcolor{blue!6}HOICLIP                  & UO             & 16.20 & \textbf{30.99} & \textbf{28.53} \\
% \hline
% \end{tabular}}
% \captionsetup{font={footnotesize}}
% \setlength{\abovecaptionskip}{0.5mm}
% \caption{Zero-shot performance on HICO-DET.}
% \label{tab:zs_results_add}
% \vspace{-0.4cm}
% \end{table}
% % \vspace{-1.0cm}

% % 1)说明Visual Semantic Arithmetic和之前取union region的embedding来生成verb representation的方式差别在，Visual Semantic Arithmetic提取的verb representation排除了object信息的干扰（已经被减掉了），之前的方式是按instance-level的方式提取一整个场景的embedding来作为verb representation。实验中我们的方法取得了更好的效果
% % 2) 声明我们之前用的是从全部数据中提取的verb repreentation，实验证明使用部分数据生成的verb reprentation效果和全部的差不多。
% \noindent \textbf{\textcolor{Orange}{@R2-Q1:} Justification for Visual Semantic Arithmetic.} 1)
% % The proposed Visual Semantic Arithmetic extract the verb representation which is not affected by object information. 
% In contrast to previous interpretation, our VSA design aims to reduce the noisy cues from the object regions due to the variation in object classes. 
% We verify the effectiveness of this design in Table \ref{tab:verb_result}, which shows our method \textbf{Frac} outperforms previous representation \textbf{Union}.
% 2) We use a verb representation extracted from whole dataset in main paper experiments. To investigate the impact of using partial data, we conduct experiments in Table \ref{tab:verb_result} where \textbf{Frac} indicates verb representation extracted from only partial data settings. By comparing with {HOICLIP}, we can see that our model is robust w.r.t different amount of data for VSA.

% %Previous methods consider the union region of object and human as the verb representation where leads to ambiguity because the unrelated object information is included as part of verb concept. Our proposed methods alleviate the issue through Visual Semantic Arithmetic. We conduct experiments to verify the effectiveness of different extraction methods in Table \ref{tab:verb_result} where \textbf{Union} indicates extraction methods from previous researches. 2) We use the verb representation extracted from whole dataset in main paper experiments. To verify the impact of the amount of data for verb representation extraction, we conduct experiments in Table \ref{tab:verb_result} where \textbf{Frac} indicates verb representation extracted from fractional data. The results show that the performance is not sensitive to the amount of data.


% \noindent \textbf{\textcolor{Orange}{@R2-Q2:} Low-data Regime.} We thank R2 for the suggestion and conduct ablation study under low-data settings in Table \ref{tab:verb_result}. The result shows that the model relies more on the CLIP features with fewer data. However, all the modules work collaboratively to achieve the best performance. %all the components still contributes to the performance improvements while their relative importance changes in the low-data settings. %the \textbf{+CLIP} is critical to low-data performance while the importance of \textbf{+integration} degrades under fewer training data. 

% % \begin{table}[]
% % \centering
% % \small
% % \scalebox{0.8}{
% % \begin{tabular}{llll}
% % \hline
% % Method           & 100\% & 50\%      & 15\%   \\
% % \hline\textit{Base}            & 32.09 & 26.68 & 33.71   \\\hline
% % \textit{+CLIP}  & 32.72   & 28.74 & 33.92     \\ 
% % \textit{+integration}  & 34.13   & 30.54 & 35.20     \\ 
% % \textit{+verb}  & 34.54   & 30.71 & 35.70     \\ 
% % \rowcolor{blue!6}\textit{+free}  & 34.69   & 31.12 & 35.74     \\
% % \hline
% % \end{tabular}}
% % \caption{xxxxxxxx}
% % \label{tab:ablation}
% % \vspace{-0.3cm}
% % \end{table}

% % CLIP作为一个powerful的模型，如何利用CLIP来做HOI detection还是没有被探索过的。因此我们设计了多个module来探索CLIP在HOI上的性能，但是都是为了同一个目的。
% \noindent \textbf{\textcolor{Orange}{@R2-Q3:} Main contribution.} We design a strategy to exploit the CLIP knowledge from three aspects tailored for HOI detection, including representations for better \textit{spatial localization}, \textit{verb representation} and \textit{linguistic semantics}.   
% %CLIP is a powerful cross-modality model and it remains unclear how to leverage its full power. Our designed modules serve the same purpose of fully exploit knowledge in CLIP for HOI detection. 

% \noindent \textbf{\textcolor{Orange}{@R2-Q4:} Computational cost.} We evaluate HOICLIP and the prior art GEN-VLKT on a single NVIDIA A100 GPU. The inference time of HOICLIP (\textbf{55.52} ms/img) is comparable to GEN-VLKT (\textbf{52.80} ms/img), and the small gap is from the additional CLIP encoder. Given the performance improvement, such a trade-off seems worthwhile. 

% % 用imagenet pretrained vit替换 CLIP vit会导致performance下降，由于CLIP的text和visual encoder之间的联系被破坏了。实验结果能证明上述推理。
% \noindent \textbf{\textcolor{Orange}{@R2-Q5:} The necessity of CLIP.} 
% The strong performance of HOICLIP is achieved by leveraging the alignment between 
% CLIP's text and visual encoders instead of using more parameters. As shown in Table \ref{tab:verb_result}, replacing CLIP ViT with imagenet pretrained ViT (denoted as \textbf{HOICLIP-ViT}) breaks the alignment and the performance degrades.

% % To verify the importance of CLIP, we replace CLIP ViT with imagenet pretrained ViT and conduct experiments. The results of \textbf{HOICLIP-ViT} in Table \ref{tab:verb_result} shows performance degrades since the alignment between CLIP text and visual encoder are broken by the modification.

% %大概总结一下RLIP的内容。说明HOICLIP是通过训练把CLIP信息融入HOI detector因此finetune必不可少。
% \noindent \textbf{\textcolor{Orange}{@R2-Q6:} Discussion about recent work RLIP.}
% RLIP is an inspiring cocurrent work focusing on pre-training for HOI detection, which will be compared in our revised version. However, its contribution is largely orthogonal to ours as we focus on better integrating CLIP into HOI detection model where training is necessary and typically achieve better results (\textit{34.69 vs 32.84} in the regular setting). % for detection. Indeed, adopting a pretrained detector to generate bounding boxes and leveraging CLIP to classify HOI categories requires no fine-tuning to perform zero-shot HOI detection but it is not the contribution of HOICLIP. 

% % One of our main contribution is to integrates CLIP knowledge into HOI detection model through training which makes fine-tuning inevitable for HOICLIP.

% % 在inference阶段，HOICLIP中加入的CLIP模块确实导致了参数量相比于蒸馏方法GEN-VLKT有所增加，并且使得inference用时变长，但参数量的增多与inference用时的增多均不显著。同时，GEN-VLKT在常规训练（zero-shot除外）还需要额外fine-tune CLIP的参数，训练时需要训练的参数量反而更多。综上所述，这样的trade-off是可接受的
% \noindent \textbf{\textcolor{Green}{@R3-Q1:} Model efficiency.} The introduced parameters in HOICLIP only leads to a slight increase in inference cost. Please kindly refer to \textbf{\textcolor{Orange}{R2-Q4}} for details. Moreover, for the model training, the number of trainable parameters in GEN-VLKT is \textbf{129.8M} under its regular setting as its CLIP module is fine-tuned while our HOICLIP has \textbf{66.1M} trainable parameters due to the \textit{fixed CLIP} visual encoder.

% %in its official implementation. In this case, we believe such trade-off is bearable. 
% %For inference stage, the number of parameters in the distillation method GEN-VLKT is \textbf{42.0M}, and  since HOICLIP adopt query-based retrieval from  \textbf{fixed CLIP} visual encoder. 

% % HOICLIP integrates \textbf{fixed CLIP} visual encoder into pipeline. The number of parameters to be trained of HOICLIP is \textbf{66.1M} while GEN-VLKT is \textbf{42.0M} under zero-shot setting and \textbf{129.8M} under regular setting (CLIP is fine-tuned during training in official implementation). For inference time, please kindly refer to \textbf{\textcolor{Orange}{R2-Q4}}. We believe such trade-off is bearable.

% % The introduced CLIP encoder increases the number of parameters. However, CLIP encoder are fixed during training. In this case, the number of parameters of HOICLIP is 66.1M. 
% % The number of parameters of GEN-VLKT is 42.0M under zero-shot setting,  The added model complexity lead to a slight decrease of efficiency during inference. 
% % We show the inference time of HOICLIP compared with GEN-VLKT in \textbf{\textcolor{Orange}{R2-Q4}}. We believe such trade-off is bearable.

% % 在inference阶段移除本模型的CLIP encoder是违背我们的设计初衷的。我们的目标是设计出一种策略，能够学习到如何将CLIP中隐藏的信息挖掘出来，并与模型自己得到的信息进行整合，而非仅仅使用CLIP辅助特征学习。最终我们的策略具备更好的表现，可以证明这样的策略对于利用CLIP信息是更高效的
% \noindent \textbf{\textcolor{Green}{@R3-Q2:} Removing CLIP during inference.} The CLIP module is an integrate part of our model and as such, removing CLIP feature during inference is infeasible. We note that our representation differs from the distillation-based model in which we augment the CLIP representations (which are expensive to train) with a smaller task-specific integration module learned from data. %Therefore, CLIP feature is an indispensable information source in HOICLIP.
% % where CLIP is integrated into the pipeline to provide prior knowledge. Besides, the model is designed to mix the knowledge with information extracted by its own to make better prediction. 
% %This strategy is different with strategy used in distillation model, which leverage CLIP for assisting representation learning. 
% Our method achieves superior performance, which indicates this strategy is able to better utilize the prior knowledge of CLIP.
% % Discussion about removing CLIP during inference.} Removing CLIP feature during inference is infeasible and contrary to our design intuition that CLIP is integrated into pipeline instead of leveraging CLIP for representation learning exclusively.

% \begin{table}[t]
% \vspace{+0.2em}
% \small
% \centering
% \scalebox{0.8}{
% \begin{tabular}{lcccc}
% \hline
% Method                  & 100\%        & 50\%           & 15\%          \\
% \hline
% Union             & 33.03          & 30.79           & 26.80               \\
% Frac                  & 34.69          & 31.11          & 26.84        \\
% \hline
% \hline\textit{Base}            & 32.09 & 25.54 & 21.57   \\
% \textit{+CLIP}  & 32.72   & 29.80 & 25.20     \\ 
% \textit{+integration}  & 34.13   & 30.28 & 25.63     \\ 
% \textit{+verb}  & 34.54   & 30.33 & 26.25     \\
% \hline
% \hline
% HOICLIP-ViT & 33.03   & 28.28 & 23.91     \\ 
% \hline
% \hline
% \rowcolor{blue!6}HOICLIP & \textbf{34.69}   & \textbf{30.88} & \textbf{27.07}     \\ 
% \hline
% \end{tabular}}
% \captionsetup{font={footnotesize}}
% \setlength{\abovecaptionskip}{0.5mm}
% \caption{Fractional data performance on HICO-DET.}
% \label{tab:verb_result}
% \vspace{-0.7cm}
% \end{table}
% % \vspace{-0.6cm}

% % . We design a remix learning strategy, where the model learn to mine and utilize the prior knowledge from CLIP, instead of using CLIP for representation learning. 

% % V-COCO数据集规模小，类别少，比较难展示HOICLIP的特性和优势。
% \noindent \textbf{\textcolor{Green}{@R3-Q3:} V-COCO.} Our comparison on V-COCO is under the regular settings, which is used as a sanity check for model performance. While our model also achieves a competitive result, it is secondary for our contribution.  

% %V-COCO provides 10,346 images and defines 29 verbs, while HICO-DET provides 47,776 images and defines 117 verbs. Therefore, V-COCO is not enough to demonstrate the characteristic of HOICLIP due to its relatively small scale.

% % 提及的两篇工作目标与HOICLIP不同，其中一篇使用额外的body part level标注，试图利用姿势信息进行HOI判断，另一篇寻找一种可以附加在one-stage detectors的方法，利用hard-positive HOI queries辅助HOI判断。我们的方法的目标是在少数据与zero-shot的情况下找到更好地挖掘利用CLIP的方法。我们会在related work中讨论这两篇工作。
% \noindent \textbf{\textcolor{Green}{@R3-Q4:} Related works.} These recent works enhance HOI detection from different perspectives, leveraging additional body part level label or mining hard-positive HOI queries in one-stage detectors. By contrast, our goal is to leverage CLIP knowledge for low-data and zero-shot HOI detection. We will discuss these works in revised paper.


% % adopt additional human body part level atomic action labels and  

% % to assist model training. The method proposed in the second paper is a additional module to improve the performance of other models, which focus on a different perspective.


% % \begin{table}[]
% % \small
% % \centering
% % \scalebox{0.8}{
% % \begin{tabular}{lcccc}
% % \hline
% % Method                  & 100\%        & 50\%           & 15\%          \\
% % \hline
% % HOICLIP                     & RF-UC          & 10.06           & 24.28       \\
% % Union                     & RF-UC          & 9.18           & 24.67                \\
% % \hline
% % \end{tabular}}
% % \caption{Zero-shot performance on HICO-DET.}
% % \label{tab:vit}
% % \vspace{-0.3cm}
% % \end{table}

% %%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% % \bibliography{egbib}
% }

% \end{document}
