% % CVPR 2023 Paper Template
% % based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% % modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

% \documentclass[10pt,twocolumn,letterpaper]{article}

% %%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% % \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
% %\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% % Include other packages here, before hyperref.
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{booktabs}
% \usepackage[table,xcdraw]{xcolor}
% \usepackage{stfloats}
% \usepackage{physics}
% % \usepackage{natbib}
% % \bibliographystyle{ieee_fullname}


% % It is strongly recommended to use hyperref, especially for the review version.
% % hyperref with option pagebackref eases the reviewers' job.
% % Please disable hyperref *only* if you encounter grave issues, e.g. with the
% % file validation for the camera-ready version.
% %
% % If you comment hyperref and then uncomment it, you should delete
% % ReviewTempalte.aux before re-running LaTeX.
% % (Or just hit 'q' on the first LaTeX run, let it finish, and you
% %  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% \newcommand{\rev}[1]{\textcolor{violet}{#1}}

% % Support for easy cross-referencing
% \usepackage[capitalize]{cleveref}
% \crefname{section}{Sec.}{Secs.}
% \Crefname{section}{Section}{Sections}
% \Crefname{table}{Table}{Tables}
% \crefname{table}{Tab.}{Tabs.}


% %%%%%%%%% PAPER ID  - PLEASE UPDATE
% \def\cvprPaperID{8067} % *** Enter the CVPR Paper ID here
% \def\confName{CVPR}
% \def\confYear{2023}


% \begin{document}

% %%%%%%%%% TITLE - PLEASE UPDATE
% %\title{HOICLIP: Efficient Knowledge Transfer from Visual Linguistic Model for HOI Detection}
% \title{HOICLIP: Efficient Knowledge Transfer for HOI Detection with Vision-Language Models}

% \author{Supplementary Material}

% % \author{First Author\\
% % Institution1\\
% % Institution1 address\\
% % {\tt\small firstauthor@i1.org}
% % % For a paper whose authors are all at the same institution,
% % % omit the following lines up until the closing ``}''.
% % % Additional authors and addresses can be added with ``\and'',
% % % just like the second author.
% % % To save space, use either the email address or home page, not both
% % \and
% % Second Author\\
% % Institution2\\
% % First line of institution2 address\\
% % {\tt\small secondauthor@i2.org}
% % }
% \maketitle

% % interaction decoder attention map比较GEN 

% \section{Validation Set Generation}
% % 写下validation set 怎么选出来的，显得正规一点的写法，e.g. randomly split, balabalabala
% The HICO-DET doesn't provide a official validation set.
% When deciding our hyperparameters, we split a validation set from the training set, and also generate a new training set specially for hyperpatameter selection from the remaining examples. We evaluate the performances of each hyperparameters choice on the separated validation set, to ensure fair and reliable results. We train models for hyperparameter selection on the new training set, thus model will not see the training example of validation set during training stage. To guarantee a proper function of the validation set and the new training set, we generate the the two set following the criterion: 1) all training instances are randomly picked from the training set; 2) there is at least one training instance for any class included in the training set. The final validation set contains 12892 images and the new training set contains 18250 images.

% \section{Supplementary Analysis}

% \textbf{Zero-shot HOI Enhancement for GEN-VLKT}
% We conduct experiments to validate the effectiveness of zero-shot HOI enhancement in previous HOI detector e.g. GEN-VLKT. We report the result of different top $k$ selection on test set in Table \ref{tab:gen_zs}. The training-free enhancement also works for GEN-VLKT and achieve a improvement of +0.56 mAP gain when $k$ equals to 5. We also provide the result of HOICLIP with different $k$ on test set to explore the upper bound for zero-shot HOI enhancement in Table \ref{tab:hoi_zs}. We observe the maximum improvement is achieved when $k$ equals to 20 and we report result with $k$ equals to 5, which is selected by validation set.

% \textbf{Visualization of Improvement}
% We visualize the performance of HOICLIP with zero-shot HOI enhancement and without zero-shot HOI enhancement. The result is showed in Figure~\ref{fig:enhancement}. We observe the enhancement benefit the tail classes more compared with head classes. We conclude the enhancement is a complement of CLIP knowledge to learned knowledge from HOI detection training data and model with worse performance benefits more from enhancement.


% \begin{figure}
%     \centering
%     \includegraphics[width=0.3\textwidth]{enhancement.png}
%     \caption{\textbf{Enhancement Analysis}: We split all 600 HOI categories into 5 part by the number of training examples, then evaluate the performance of model with/without the training free enhancement method on each part and show the results. The performance gain is distinct especially on the classes with fewer training example, i.e. the tail classes.}
%     \label{fig:enhancement}
% \end{figure}


% \begin{table}[]
% \centering
% \begin{tabular}{lccc}
% \hline
% K &  Full     & Rare    & Non-Rare\\
% \hline
% 0  & 33.75    & 29.25   & 35.10          \\
% \textbf{5}  &\textbf{34.31}         & \textbf{30.50}          & \textbf{35.44}         \\
% 10 & 34.15 & 29.95 & 35.41 \\
% 15 & 34.16          & 30.01          & 35.40         \\
% 20 & 34.17          & 29.94         & 35.43          \\
% 25 & 34.13          & 29.88         & 35.40       \\ 
% \hline
% \end{tabular}
% \caption{\textbf{GEN-VLKT with zero-shot HOI enhancement on HICO-DET.}}
% \label{tab:gen_zs}
% \vspace{-0.3cm}
% \end{table}

% \begin{table}[]
% \centering
% \begin{tabular}{lccc}
% \hline
% K &  Full     & Rare    & Non-Rare\\
% \hline
% 0  & 34.55    & 30.71   & 35.70          \\
% 5  & 34.54    & 30.71   & 35.70         \\
% 10 & 34.69 & 31.18 & 35.74 \\
% 15 & 34.71          & 31.23          & 35.74         \\
% \textbf{20} & \textbf{34.75}          & \textbf{31.30}         & \textbf{35.79}         \\
% 25 & 34.70         & 31.11         & 35.78       \\ 
% \hline
% \end{tabular}
% \caption{\textbf{HOICLIP with zero-shot HOI enhancement on HICO-DET.}}
% \label{tab:hoi_zs}
% \vspace{-0.3cm}
% \end{table}


% % \textbf{T-SNE for Interaction Representation}
% % % 真的，调不出来


% \begin{figure*}[!t]
% 	\centering  %图片全局居中
% 	% \subfigbottomskip=2pt %两行子图之间的行间距
% 	% \subfigcapskip=-5pt %设置子图与子标题之间的距离
%     \begin{subfigure}[t]{0.2\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{sup_vis1.jpg}
%          \caption{}
%      \end{subfigure}
%      \begin{subfigure}[t]{0.2\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{sup_vis4.jpg}
%          \caption{}
%      \end{subfigure}
%      \begin{subfigure}[t]{0.2\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{sup_vis2.jpg}
%          \caption{}
%      \end{subfigure}
%      \begin{subfigure}[t]{0.2\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{sup_vis3.jpg}
%          \caption{}
%      \end{subfigure}
%      \vspace{-3mm}
% 	\caption{\textbf{Fail cases of conventional methods.} In (a) and (b), we present the fail cases of GEN-VLKT and corresponding attention map from interaction decoder. In (c) and (d), we visualize the prediction from HOICLIP and corresponding attention map from CLIP spatial feature in interaction decoder.}
%     \label{fig:visualization}
% \end{figure*}


% \textbf{Additional Qualitative Analysis}
% We present the fail cases of conventional HOI detector in Figure \ref{fig:visualization}. The ground truth interaction category for first row is training a dog and flipping a skateboard for second row. In the first row, conventional methods wrongly predict that human is interacting with a ball instead of the dog. Meanwhile, in the second row, GEN-VLKT wrongly predict the interaction category where a man is flipping the skateboard instead of the wield a baseball bat. As discussed in the main manuscript, we conclude the difference lies in the focus point of GEN-VLKT and HOICLIP. We observe the attention map of HOICLIP covers more informative regions and aggregate more accurate interaction information. In the other hand, GEN-VLKT simply focus on the object region which is inconsistent with region required for interaction prediction.

% \textbf{Justification for Visual Semantic Arithmetic.}
% In contrast to previous interpretation where the verb representation is interpreted as features of the union region minus the object features, our VSA design aims to reduce the noisy cues from the object regions due to the variation in object classes. 
% We verify the effectiveness of this design in Table \ref{tab:verb_result}, which shows our method \textbf{Frac} outperforms previous representation \textbf{Union.}
% 2) We use a verb representation extracted from whole dataset in main paper experiments. To investigate the impact of using partial data, we conduct experiments in Table \ref{tab:verb_result} where \textbf{Frac} indicates verb representation extracted from only partial data settings. By comparing with {HOICLIP}, we can see that our model is robust w.r.t different amount of data for VSA.

% \textbf{Ablation Study under Low-data Regime.} We conduct ablation study under low-data settings in Table \ref{tab:verb_result} to better demonstrate the characteristic of proposed methods. The result shows that the model relies more on the \textbf{CLIP} features with fewer data. However, all the modules work collaboratively to achieve the best performance. 

% % 用imagenet pretrained vit替换 CLIP vit会导致performance下降，由于CLIP的text和visual encoder之间的联系被破坏了。实验结果能证明上述推理。
% \textbf{The necessity of CLIP.} 
% To verify the necessity of CLIP, we replace the CLIP visual encoder with imagenet pre-trained ViT.
% The strong performance of HOICLIP is achieved by leveraging the alignment between 
% CLIP's text and visual encoders instead of using more parameters. As shown in Table \ref{tab:verb_result}, replacing CLIP ViT with imagenet pretrained ViT (denoted as \textbf{HOICLIP-ViT}) breaks the alignment and the performance degrades.

% \begin{table}[t]
% \vspace{+0.2em}
% \centering
% \scalebox{0.8}{
% \begin{tabular}{lcccc}
% \hline
% Method                  & 100\%        & 50\%           & 15\%          \\
% \hline
% Union             & 33.03          & 30.79           & 26.80               \\
% Frac                  & 34.69          & 31.11          & 26.84        \\
% \hline
% \hline\textit{Base}            & 32.09 & 25.54 & 21.57   \\
% \textit{+CLIP}  & 32.72   & 29.80 & 25.20     \\ 
% \textit{+integration}  & 34.13   & 30.28 & 25.63     \\ 
% \textit{+verb}  & 34.54   & 30.33 & 26.25     \\
% \hline
% \hline
% HOICLIP-ViT & 33.03   & 28.28 & 23.91     \\ 
% \hline
% \hline
% \rowcolor{blue!6}HOICLIP & \textbf{34.69}   & \textbf{30.88} & \textbf{27.07}     \\ 
% \hline
% \end{tabular}}
% \captionsetup{font={footnotesize}}
% \setlength{\abovecaptionskip}{0.5mm}
% \caption{Fractional data performance on HICO-DET.}
% \label{tab:verb_result}
% \vspace{-0.7cm}
% \end{table}


% \section{Limitation Discussion}
% We notice the training parameter number for conventional methods are different with HOICLIP since HOICLIP include CLIP visual encoder as a indivisible part of its network architecture. Specifically, during training under default setting, GEN-VLKT  leverage CLIP as a teacher model for knowledge distillation and finetune CLIP with a smaller learning rate while HOICLIP freeze all of CLIP parameters during training and inference. However, during inference, HOICLIP require CLIP spatial feature which leads to additional  cost in CLIP visual encoder. In summary, HOICLIP require less training cost but more inference cost compared with GEN-VLKT. We regard the cost in inference is inevitable for better integrating CLIP knowledge and achieving more generalized and data efficient HOI detectors.
% \clearpage

% %%%%%%%%% REFERENCES
% % {\small
% % \bibliographystyle{ieee_fullname}
% % \bibliography{egbib}
% % }

% \end{document}
