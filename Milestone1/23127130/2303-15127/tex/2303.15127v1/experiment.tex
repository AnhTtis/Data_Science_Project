\section{Experimental Setup \& Results}

% \subsection{Setup}
\textbf{Datasets.}
We select four popular datasets for the
evaluation of \Method{}, namely,
CIFAR-10~\cite{krizhevsky2009learning},
CIFAR-100~\cite{krizhevsky2009learning},
SVHN~\cite{netzer2011reading},
and an ImageNet~\cite{deng2009imagenet} subset.
Following the setup in EM~\cite{huangunlearnable},
we use the first 100 classes of the
full dataset as the ImageNet-subset,
and resize all images to \( 32 \times 32 \).
We evaluate the effectiveness of \Method{}
by examining the accuracies of the trained models
on clean test examples,
\ie{}, the higher the clean test accuracy,
the greater its effectiveness.
By default,
all target and surrogate models
use ResNet-18~\cite{he2016deep}
if not otherwise specified.
We explore the effect of \Method{}
on partial poisoning (\Cref{sec:results:ablation:partial}),
larger perturbation budgets (\Cref{sec:results:ablation:larger_perturbation}),
different network architectures
(\Cref{sec:results:ablation:architectures}),
transfer learning (\Cref{sec:results:ablation:transfer}),
and perform ablation analyses
in~\Cref{sec:results:ablation:options,sec:results:ablation:repeat}.
Finally,
\Cref{app:sensitivity}
provides additional sensitivity analyses.

\textbf{Training settings.}
We train the CIFAR-10 model for 200 epochs,
SVHN model for 150 epochs,
and CIFAR-100 and ImageNet-subset models
for 300 epochs due to the use
of strong augmentation policies.
We adopt standard random cropping and flipping
for all experiments by default
as standard training baselines
and introduce additional augmentations
as required by the compared methods.
For the optimizer,
we use SGD with a momentum of 0.9
and a weight decay of \( 5 \times 10^{-4} \).
By default,
the learning rate is fixed at 0.01.
We follow the dataset setup in respective attacks,
where all unlearning perturbations
are bounded within \( \ell_\infty = 8/255 \)
or \( \ell_2 = 1.0 \).
% we use the kornia\footnote{} package.
% Specifically, we use
% We use
% \definecolor{mygray}{rgb}{0.92,0.92,0.92}
% {\footnotesize \colorbox{mygray}{\texttt{RandomPlasmaBrightness}}},
% {\footnotesize \colorbox{mygray}{\texttt{RandomPlasmaContrast}}},
% {\footnotesize \colorbox{mygray}{\texttt{RandomChannelShuffle}}},
% and {\footnotesize \colorbox{mygray}{\texttt{TrivialAugment}}}
% to implement \Method{}.
For \Method{},
we divided the training process
into two parts for speed:
the adversarial augmentation process,
and the standard training process.
In the first stage,
we used the loss-maximizing augmentations for training,
with a default number of repeated samples
\( K = 5 \) (as the input to~\Cref{alg:method}).
In the second stage,
we used the \MethodLite{} process
which sets \( K = 1 \).
This approach allows us
to keep a balance
between suppressing the emergence of unlearning shortcuts
and training speed.
% to the maximum extent
% during the early stage of training.
% To speed up
% training convergence,
% the subsequent training process is consistent
% with the \MethodLite{}.
% If the error-maximizing augmentation
% is always active during training,
% the clean accuracy of the unlearnable
% CIFAR-10 is 92.14\(\%\) under
% the same training epochs (200),
% which is comparable to that of \Method{}.
We further explore
full training with loss-maximizing augmentation
in~\Cref{sec:results:ablation:repeat},
which attains the highest known test accuracies.
Finally, \Cref{tab:main}
shows the effect of \Method{}
on five different unlearnable methods.
Additional information
regarding the training setup and hyperparameters
can be found in~\Cref{app:setup}.
The details of the attack and defense baselines
are available in~\Cref{app:baselines}.
\input{tables/tabmain}

\subsection{Main Evaluation}

% We first evaluate \Method{}
% against five representative
% unlearned poisoning methods.
To demonstrate the effectiveness of \Method{},
we select five SOTA unlearnable example attacks:
Error-Minimization (EM)~\cite{huangunlearnable},
Hypocritical Perturbations (HYPO)~\cite{tao2021better},
Robust Error-Minimization (REM)~\cite{furobust},
Linear-separable Synthetic Perturbations (LSP)~\cite{yu2022availability},
and Autoregressive Poisoning (AR)~\cite{sandoval2022autoregressive}.
Experimental results show that \Method{}
has achieved better results
than the SOTA defense methods:
Image shortcut squeezing (ISS)~\cite{liu2023image}
and adversarial training~\cite{madry2017towards}.
For EM, REM, and HYPO,
We use the same model (ResNet-18)
as the surrogate and target model.
All unlearnable methods
have a poisoning rate of \( 100\% \).
From the experimental results of \Method{}
on CIFAR-10 dataset
shown in~\Cref{tab:main},
we can conclude
that \Method{} can achieve better defensive results
than ISS and adversarial training in most cases.
Note that the adversarial training experiments
use the same type of perturbation,
which is equal to half the size
of the unlearned perturbation.
Specifically,
the perturbation radius is \( \ell_\infty = 4/255 \)
for EM, REM, and HYPO,
and \( \ell_2 = 0.5 \)
for LSP and AR.
We consider sample-wise perturbations
for all experiments.
\MethodLite{} contains a series of augmentations
that effectively break through unlearned perturbation
while also slightly affecting the clean accuracy.
Therefore,
applying the \MethodLite{}
augmentation on clean data
may even lead to accuracy degradation,
and this degradation is further increased
when applying the \Method{}.  % ???
When error-maximizing augmentation
is used throughout the training phase,
the model requires more epochs to converge
but achieves a higher accuracy rate (\(95.24\%\)).

In~\Cref{tab:extra},
We further validate the effect of \Method{}
on CIFAR-100, SVHN and ImageNet-subset.
We select the popular method (EM)
and the latest attacks for the experiments (LSP and AR).
Note that since \Method{}
increases the diversity of the data
with strong augmentations,
it requires more training epochs
to achieve converged accuracies.
All results are thus evaluated
after 300 training epochs
for CIFAR-100 and ImageNet-subset
and 150 training epochs for SVHN\@.
Experimental results
show that \Method{} achieves SOTA results
on all three datasets.
\input{tables/aug}
\begin{figure*}[ht]
\centering
\includegraphics[width=0.8\linewidth]{augvis.pdf}
\caption{%
    The Visualization of \Method{} augmentations.
}\label{fig:augvis}
\end{figure*}
\input{tables/extra}
