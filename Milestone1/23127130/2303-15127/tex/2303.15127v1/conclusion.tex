\section{Conclusion}
Using the intuition
of disrupting the unlearning perturbation
with perturbations beyond the \( \ell_p \) budgets,
we propose a simple yet effective
defense method called \Method{},
which can mitigate unlearning poisons
and restore clean accuracies.
\Method{} achieves robust defenses
on unlearning poisons
with simple data augmentations
and adversarial augmentation policies.
Similar to adversarial training,
it employs error-maximizing augmentation
to further eliminate the impact
of unlearning poisons.
Our comprehensive experiments
on five state-of-the-art unlearnable example attacks
demonstrate that \Method{}
outperforms existing countermeasures
such as adversarial training~\cite{%
    huangunlearnable,furobust,tao2021better}.
We also evaluate adaptive poisons
and transfer learning on \Method{}.
Our results suggest
that existing unlearning perturbations
are tragically inadequate
in making data unlearnable.
By understanding the weaknesses of existing attacks,
we can anticipate how malicious actors
may attempt to exploit them,
and prepare stronger safeguards against such threats.
We hope \Method{}
can help facilitate the advancement
of research in these attacks and defenses.
Our code is open source
and available to the deep learning community
for scrutiny\footnote{\repourl}.
