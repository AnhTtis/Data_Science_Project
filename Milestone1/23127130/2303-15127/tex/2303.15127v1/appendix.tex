\section{Experimental Setup}\label{app:setup}

\subsection{Datasets}\label{app:setup:datasets}

\textbf{CIFAR-10}
consists of 60,000 \( 32\times32 \)
resolution images,
of which 50,000 images are the training set
and 10,000 are the test set.
This dataset contains 10 classes,
each with 6000 images.

\textbf{CIFAR-100}
is similar to CIFAR-10.
It has 100 classes,
Each class has 600 images of size \( 32\times32 \),
of which 500 are used as
the training set and 100 as the test set.

\textbf{SVHN},
derived from Google Street View door numbers,
is a dataset of cropped images containing sets of
Arabic numerals `0-9'.
The dataset consists of 73,257 digit images
in the training set and 26,032 digit images in the test set.

\textbf{ImageNet-subset}
refers to a dataset constructed
with the first 100 classes from ImageNet
resized to \( 32 \times 32 \),
following the setup in~\cite{huangunlearnable}
for fair comparisons.
The training set comprises approximately 120,000 images,
and the test set contains 5,000 images.
% We conduct experiments
% using only the training and validation set
% of this dataset.

\Cref{tab:dataset}
shows the detail specifications of these datasets.
\begin{table}[h]
\centering
\caption{%
    Overview of the specifications of datasets used in this paper.
}\label{tab:dataset}
\begin{tabular}{l|cccc}
\toprule
Dataset
    & Input size & Train-set & Test-set & Classes \\
\midrule
CIFAR-10
    & \( 32 \times 32 \times 3 \) & 50,000 & 10,000 & 10 \\
CIFAR-100
    & \( 32 \times 32 \times 3 \) & 50,000 & 10,000 & 100 \\
SVHN
    & \( 32 \times 32 \times 3 \) & 73,257  & 26,032  & 10 \\
ImageNet-subset
    & \( 32 \times 32 \times 3 \) & 127,091 & 5,000 & 100 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Models and Hyperparameters}

We evaluate \Method{}
using a standard ResNet-18~\cite{he2016deep} architecture by default,
and extend experiments
to standard ResNet-50, DenseNet-121,
and MobileNet-v2 in~\Cref{tab:arch}.
In all the experiments,
we used a stochastic gradient descent (SGD) optimizer
with a momentum of 0.9.
\Cref{%
    tab:hyperparameters:u,%
    tab:hyperparameters:uem}
provide the default hyperparameters
used to evaluate \Method{} and \emph{}
on unlearnable examples.
\begin{table}[h]
\centering\caption{%
   Default hyperparameters for \MethodLite{}.
}\label{tab:hyperparameters:u}
\begin{tabular}{l|cccc}
\toprule
Hyperparameters & CIFAR-10 & CIFAR-100 &SVHN & ImageNet-subset \\
\midrule
Learning rate \( \alpha \)
    & 0.01 & 0.01 & 0.01 & 0.01\\
Weight decay
    & 5e-4 & 5e-4 & 5e-4 & 5e-4\\
Epochs \( E \)
    & 200 & 300 & 150 & 300\\
Batch size \( B \)
    & 128 & 128 & 128 & 128\\
\bottomrule
\end{tabular}
\end{table}
\begin{table}[h]
\centering
\caption{%
   Default hyperparameters for \Method{}.
   \MethodMax{} uses the same hyperparameters
   except \( W \) equals \( E \).
}\label{tab:hyperparameters:uem}
\begin{tabular}{l|cccc}
\toprule
Hyperparameters & CIFAR-10 & CIFAR-100 & SVHN & ImageNet-subset \\
\midrule
Learning rate \( \alpha \)
    & 0.01 & 0.01 & 0.01 & 0.01 \\
Weight decay
    & 5e-4 & 5e-4 & 5e-4 & 5e-4 \\
Epochs \( E \)
    & 200 & 300 & 150 & 300 \\
Batch size \( B \)
    & 128 & 128 & 128 & 128 \\
Number of repeated sampling \( K \)
    & 5  & 5  & 5  & 5 \\
Number of error-maximizing augmentation epochs \( W \)
    & 50  & 30  & 30  & 30 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Standard Augmentation}

For CIFAR-10, SVHN, and CIFAR-100 baselines
to compare against,
we perform data augmentation
via random flipping,
and random cropping to \( 32 \times 32 \) images
on each image.
For the ImageNet-subset,
we perform data augmentation with a 0.875 center cropping,
followed with a resize to \( 32 \times 32 \),
and random flipping for each image.

\subsection{Unlearning Perturbution Budgets}\label{app:perturbation}

The attacks,
EM~\cite{huangunlearnable}, REM~\cite{furobust},
and HYPO~\cite{tao2021better},
all have a permitted perturbation bound
of \(\ell_\infty = 8/255\) for each image.
Additionally,
the LSP~\cite{yu2022availability}
and AR~\cite{sandoval2022autoregressive} attacks
permit \(\ell_2 = 1.0\).

\subsection{Adversarial Training}

For comparison,
the baseline defenses against the five methods
(EM, REM, HYPO, LSP, and AR)
on CIFAR-10
employ PGD-7 adversarial training~\cite{madry2017towards},
following the evaluation of~\cite{furobust}.
The adversarial training perturbation bounds used
were \( \ell_\infty = 4/255 \) for EM, REM and HYPO,
and \( \ell_2 = 0.5 \) for LSP and AR as baseline defenses.


\subsection{Hyperparameters for \Method{}}
\Method{} comprises three composite augmentations
(PlasmaTransform, ChannelShuffle, TrivialAugment).
We implement augmentations using Kornia%
\footnote{Documentation: \url{https://kornia.readthedocs.io/en/latest/augmentation.module.html}.},
specifically, with {\texttt{kornia.augmentation.RandomPlasmaBrightness}},
\texttt{kornia.augmentation.RandomChannelShuffle},
and \texttt{kornia.augmentation.auto.TrivialAugment}.
% {\texttt{kornia.augmentation.RandomPlasmaContrast}},
Here,
TrivialAugment uses the default hyperparameters,
\Cref{tab:hyperparameters:ua}
shows the hyperparameter settings
for the remaining augmentations.
Finally, \Cref{fig:augs}
provides the visualization of the augmentation effects
on different dataset examples.
\begin{table}[h]
\centering\caption{%
    Default hyperparameters
    for \Method{} augmentations.
}\label{tab:hyperparameters:ua}
\small
\begin{tabular}{l|ccc}
\toprule
Augmentations & PlasmaBrightness & PlasmaContrast  & ChannelShuffle \\
\midrule
Probability of use \( p \)
    & 0.5 & 0.5 & 0.5 \\
Roughness
    & (0.1, 0.7) & (0.1, 0.7) & - \\
Intensity
    & (0.0, 1.0) & - & - \\
% Keepdim
%     & True & True & True \\
Same on batch
    & False & False & False \\

\bottomrule
\end{tabular}
\end{table}
\begin{figure}[ht]
    \centering
    \newcommand{\augsubfig}[2]{%
        \begin{subfigure}{0.7\linewidth}
            \centering
            \includegraphics[width=\linewidth]{#2.pdf}
            \caption{Visualization of augmented images of {#1}.}
        \end{subfigure}
    }
    \augsubfig{CIFAR-10}{cifar10} \\
    \augsubfig{CIFAR-100}{cifar100} \\
    \augsubfig{SVHN}{svhn} \\
    \augsubfig{ImageNet}{imagenet} \\
    \caption{%
        The visualization of various augmentations
        on different datasets.
    }\label{fig:augs}
\end{figure}

\section{Sensitivity Analysis}\label{app:sensitivity}

Here, we provide a sensitivity analysis
of the number of repeated sampling \(K\)
using in adversarial augmentation.
We also explore the effect
of increasing the number of training epochs
under \Method{}.
Taking the example of unlearnable CIFAR-10
produced with EM~\cite{huangunlearnable},
\Cref{tab:sensitivity}
shows the results of \Method{}
with various combinations
of different numbers of repeated augmentation sampling \( K \)
and the total number of training epochs \( E \).
Higher \( K \) values can effectively
improve the defense performance of \Method{},
with a training cost increasing proportionally with \( K \).
More training epochs
can also improve the performance of \Method{},
and even matches the test accuracy
of training with clean data.
Finally,
\Cref{fig:additional_sensitivity}
provides the train and test accuracy curves
\wrt{} the number of training epochs
for different \( \parens{E, K} \) configurations.
% This validates the great strength
% of the \Method{} consisting of
% data augmentation policies.
\begin{table}[h]
\centering
\caption{%
    Clean test accuracies (\%)
    of different numbers of repeated augmentation sampling
    \( K \) for \Method{}.
    Note that \( K = 1\) denotes \MethodLite{},
    and the number of error-maximizing epochs \( W = 50 \),
    ``---'' means \( K = 1 \) is not applicable for \MethodMax{}.
    The unlearnable training data
    is generated with EM on CIFAR-10,
    and a standard ResNet-18
    trained on this data attains a test accuracy
    of \( 21.21\% \).
}\label{tab:sensitivity}
\begin{tabular}{c|cc|cc}
\toprule
    \multirow{2}{*}{\( K \)}
    & \multicolumn{2}{c}{\Method{}} & \multicolumn{2}{c}{\MethodMax{}}\\
    & \( E = 200 \) & \( E = 300 \)
    & \( E = 200 \) & \( E = 300 \) \\
    \midrule
    1  & 90.78 & 94.19 &   --- &   --- \\
    2  & 92.76 & 94.79 & 91.32 & 95.25 \\
    3  & 92.91 & 95.01 & 91.40 & 95.85 \\
    4  & 93.02 & 95.14 & 91.26 & 95.59 \\
    5  & 93.38 & 94.83 & 92.12 & 95.24 \\
    6  & 93.24 & 94.78 & 93.17 & 95.36 \\
    7  & 93.16 & 94.50 & 93.58 & 94.79 \\
    8  & 93.23 & 94.87 & 93.95 & 94.84 \\
    9  & 92.86 & 94.58 & 93.92 & 94.66 \\
    10 & 93.03 & 94.83 & 94.06 & 94.57 \\
\bottomrule
\end{tabular}
\end{table}
\begin{figure}[ht]
    \centering
    \newcommand{\senssubfig}[2]{%
        \begin{subfigure}{0.31\linewidth}
            \includegraphics[width=\linewidth]{#2.pdf}
            \caption{{#1}.}
        \end{subfigure}
    }
    \senssubfig{\Method{} with \( K = 1\) (\MethodLite{})}{fast}
    \senssubfig{\Method{} with \( K = 5 \)}{UEk=5}
    \senssubfig{\Method{} with \( K = 10 \)}{UEk=5}
    \\
    \senssubfig{\Method{} with \( K = 3 \)}{EM=3}
    \senssubfig{\Method{} with \( K = 5 \)}{EM=5}
    \senssubfig{\Method{} with \( K = 10 \)}{EM=10}
    \caption{%
        Train and test accuracy curves
        \wrt{} the number of training epochs.
        The subfigures correspond
        to \Method{} under \( K \in \braces{1, 5, 10} \)
        and \MethodMax{} under \( K \in \braces{3, 5, 10} \).
        Recall that \( K \) is the number
        of repeated augmentation sampling.
    }\label{fig:additional_sensitivity}
\end{figure}

% \subsection{Hardware Details}
% The experiments on CIFAR-10 are
% conducted on NVIDIA Tesla V100 (32GB),
% and CIFAR-100, SVHN, and
% ImageNet-subset are conducted on NVIDIA Tesla A100 (40GB).

% \subsection{Time cost for \Method{}}
% We calculate the time costs
% for using \Method{} to train different
% unlearned datasets. The
% results are reported in the~\Cref{tab:cost}.

\section{Attack and Defense Baselines}\label{app:baselines}

We use five baseline attacks
and two exisiting SOTA defenses
for evaluation and comparisons
in our experiments (\Cref{tab:source_code}).
Each attack method is implemented
from their respective official source code
for a fair comparison.
We adopt experimental setup
identical to the original publications,
and use perturbation budgets
described in~\Cref{app:perturbation}.
For defenses,
we compare \Method{} variants
against the current SOTA techniques,
image shortcut squeezing~\cite{liu2023image}
and adversarial training~\cite{madry2017towards}.
The compared defenses (ISS and adversarial training)
respectively follow the original source code
and PGD-7 adversarial training~\cite{madry2017towards}.
\begin{table}[t]
    \centering
    \caption{%
        Attack and defense methods
        and respective links to open source repositories.
    }\label{tab:source_code}
    \adjustbox{max width=\linewidth}{%
    \begin{tabular}{ll}
        \toprule
        Name & Open Source Repository \\
        \midrule
        \multicolumn{2}{c}{Attacks} \\
        \midrule
        Error-minimizing attack (EM)~\cite{huangunlearnable}
        & \url{https://github.com/HanxunH/Unlearnable-Examples/} \\
        Robust error-minimizing attack (REM)~\cite{furobust}
        & \url{https://github.com/fshp971/robust-unlearnable-examples} \\
        Hypocritical perturbations (HYPO)~\cite{tao2021better}
        & \url{https://github.com/TLMichael/Delusive-Adversary} \\
        Linear-separable synthetic perturbations (LSP)~\cite{yu2022availability}
        & \url{https://github.com/dayu11/Availability-Attacks-Create-Shortcuts} \\
        Autoregressive poisoning~\cite{sandoval2022autoregressive}
        & \url{https://github.com/psandovalsegura/autoregressive-poisoning} \\
        \midrule
        \multicolumn{2}{c}{Defenses} \\
        \midrule
        Image shortcut squeezing~\cite{liu2023image}
        & \url{https://github.com/liuzrcc/ImageShortcutSqueezing} \\
        Adversarial Training~\cite{madry2017towards}
        & Example implementation from \url{https://github.com/fshp971/robust-unlearnable-examples} \\
        \bottomrule
    \end{tabular}}
\end{table}

% \section{Limitations}
% \Method{} is composed of a series of
% augmentations whose goal is to
% find augmentations that maximize the error.
% In other words,
% currently \Method{} just enables
% the model to go through the gradient
% to pick the appropriate augmentations.
% We cannot guarantee that \Method{}'s portfolio
% of augmentation policies is the most effective,
% but the unlearning effect can be
% effectively removed by error maximization.
% Although \Method{}/\MethodMax{} can recover
% clean test accuracy comparable to clean data,
% the error-maximizing augmentation
% requires high computational overhead.
% In addition,
% the number of repetitions (\(K\))
% per epoch also influences the effect of \Method{}.
% When K is set too large, it can lead to
% slower training or even gradient explosion.

% \section{Ethics Statement}
% Unlearnable examples can potentially
% serve as a protective mechanism for private data,
% but malicious exploitation of such examples
% may lead to data poisoning.
% Our study focuses on thwarting data poisoning attacks
% that limit the generalization ability
% of a model through malicious poisoning.
% Moreover, our proposed defense method presents
% potential means of circumventing such attacks,
% thereby inspiring more efficacious
% privacy protection techniques for defenders.

% \begin{table}[h]
% \centering
% \caption{
%    Time costs per epoch for \Method{} training.
%    Note that the step of \MethodMax{} is 10
%    for CIFAR-10 and 5 for CIFAR-100, SHVN,
%    and ImageNet-subset.
% }\label{tab:cost}
% \small
% \begin{tabular}{c||ccc}
% \toprule
% Datesets & Normal & \MethodLite{}  & \Method{} \(\&\) \MethodMax{} (\(K=5\)) \\
% \midrule
% CIFAR-10
%     & 21s  & 32s & 2min 32s \\
% CIFAR-100
%     & 29s & 40s  & 2min 07s \\
% SHVN
%     & 23s & 44s  & 2min 31s \\
% ImageNet-subset
%     & 3min 41s & 4min 49s & 9min 37s \\
% \bottomrule
% \end{tabular}
% \end{table}
