\section{Discussion}\label{limit}

% With adversarial augmentation policies,
% \Method{} achieves much improved results
% than other countermeasures to unlearnable attacks.
% Currently,
% \Method{} outperforms all existing SOTA methods
% in terms of accuracy on clean data,
% achieving result that can even exceed training
% on clean data.

In this section, we investigate
limitations of \Method{}
and the potential use cases of the provided options.
% However,
% the computational overhead
% of \Method{} cannot be neglected.
We compared \MethodLite{} training
with normal training
and found that \MethodLite{}
requires more training rounds
to converge due to data augmentation policies.
For instance,
CIFAR-10 typically converges
with 60 epochs of normal training,
whereas it requires more than
150 epochs of training to reach
full convergence with \MethodLite{}.
% Moreover, the training time per epoch
% is approximately \( 20\% \) longer than
% that of normal training.
With respect to \MethodMax{}
and \Method{},
the error-maximizing augmentations
consumes the same amount
of floating-point operations (FLOPs)
as adversarial training,
if \( K \) is the same as the adversarial attack steps.
However,
we note that
our approach is easily parallelizable,
but adversarial training approaches are not,
with compute dependences between adversarial iterations.
One can choose between \MethodMax{},
\MethodLite{} and \Method{} depending
on their requirements.
\MethodMax{}
can be preferred if seeking higher clean accuracy,
whereas \Method{} or \MethodLite{}
can be a practical option
for faster training
with nearly the same accuracies.

Despite acknowledging
that a malicious party
may exploit the approach suggested in this paper,
we believe that the ethical approach
for the open-source deep learning community
is not to withhold information
but rather to increase awareness of these risks.
