\section{Related Work}

\textbf{Adversarial examples and adversarial training.}
Adversarial examples deceive machine learning
models by adding adversarial perturbations,
often imperceptible to human,
to source images,
leading to incorrect classification
results~\cite{goodfellow2014explaining,szegedy2013intriguing}.
White-box adversarial attacks~\cite{szegedy2013intriguing}
maximize the loss of a source image
with gradient descent on the defending model
to add adversarial perturbations
onto an image to maximize its loss
on the model.
% When the input
% is slightly perturbed
% in the right direction,
% the optimizer effectively finds these directions
% through gradients to update perturbation
% and create adversarial perturbations.
Effective methods to gain adversarial robustness
usually involve adversarial training~\cite{madry2017towards},
which leverages adversarial examples to train models.
Adversarial training algorithms
thus solve the min-max problem
of minimizing the loss function
for most adversarial examples
within a perturbation budget,
typically bounded in \( \ell_p \).
Recent years have thus observed an arms race
between adversarial attack strategies
and defense mechanisms~\cite{%
    croce2020robustbench,croce2020reliable,yu2021lafeat,yu2022mora}.

\textbf{Data poisoning.}
Data poisoning attacks
manipulate the training of a deep learning model
by injecting malicious and poisoned examples
into its training set~\cite{biggio2012poisoning,shafahi2018poison}.
Data poisoning methods~\cite{chen2017targeted,nguyen2021wanet}
achieve their malicious objectives
by stealthily replacing a portion
of training data,
and successful attacks can be triggered
with specially-crafted
prescribed inputs.
Effective data poisoning attacks
typically perform well on clean data
and fail on data that contains triggers~\cite{li2022backdoor}.

\textbf{Unlearnable examples.}
Unlearnable examples attacks
are a type of data poisoning methods
with bounded perturbation
that aims to make learning
from such examples difficult.
Unlike traditional data poisoning methods,
unlearnable examples methods usually
require adding imperceptible perturbations
to all examples~\cite{
    furobust,huangunlearnable,sandoval2022autoregressive,
    tao2021better,yu2022availability}.
Error-minimizing (EM)~\cite{huangunlearnable} poison
generates imperceptible perturbations
with a min-min objective,
which minimizes the errors of training examples
on a trained model,
making them difficult to learn by deep learning models.
By introducing noise
that minimizes the error of all training examples,
the model instead learns ``shortcut'' of such perturbations,
resulting in inability to learn from such data.
Hypocritical perturbations (HYPO)~\cite{tao2021better}
follows a similar idea
but uses a pretrained surrogate
rather than the above min-min optimization.
As the above method cannot defend
against adversarial training,
Robust Error-Minimizing (REM)~\cite{furobust}
uses an adversarially-trained model
as an adaptively attack
to generate stronger unlearnable examples.
INF~\cite{wen2023adversarial}
enables samples from different classes
to share non-discriminatory features
to improve resistance to adversarial training.
Linear-separable Synthetic Perturbations
(LSP)~\cite{yu2022availability}
reveals that if the perturbations
of unlearnable samples
are assigned to the corresponding target label,
they are linearly separable.
It thus proposes
linearly separable perturbations
in response to this characteristic
and show great effectiveness.
Autoregressive poisoning
(AR)~\cite{sandoval2022autoregressive}
proposes a generic perturbation
that can be applied to different
datasets and architectures.
The perturbations of AR
are generated from dataset-independent processes.

\textbf{Data augmentations.}
Data augmentation techniques
increase the diversity
of training data by applying
random transformations~\cite{krizhevsky2012alexnet}
(such as rotation, flipping, cropping, \etc{})
to images,
thereby improving the model's generalization ability.
Currently,
automatic search-based
augmentation techniques
such as TrivialAugment~\cite{muller2021trivialaugment}
and AutoAugment~\cite{cubuk2019autoaugment},
can further improve the performance of trained
DNNs by using a diverse set of augmentation policies.
TorMentor~\cite{nicolaou2022tormentor},
an image-augmentation framework,
proposes fractal-based data augmentation
to improve model generalization.
Current unlearnable example methods~\cite{
    furobust,huangunlearnable,sandoval2022autoregressive,
    tao2021better,yu2022availability}
demonstrate strong results
under an extensive range of data augmentation methods.
Despite prevailing beliefs
on their ineffectiveness against unlearnable examples,
\Method{} challenges this preconception,
as it searches for adversarial policies
with error-maximizing augmentations
and achieves the state-of-art defense performance
against existing unlearnable example attacks.
