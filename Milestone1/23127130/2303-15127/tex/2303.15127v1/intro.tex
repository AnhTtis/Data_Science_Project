\section{Introduction}\label{sec:intro}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/overview.pdf}
    % \vspace{-5pt}
    \caption{%
        A high-level overview of \Method{} for
        countering unlearning poisons.
        Note that \Method{} recovers the clean
        accuracy of unlearnable examples
        by data augmentations.
        The reported results
        are for EM~\cite{huangunlearnable}
        unlearnable CIFAR-10
        with an \( \ell_\infty \) perturbation budget
        of \( 8/255 \).
    }\label{fig:overview}
\end{figure*}
Deep learning has achieved great success in fields
such as computer vision~\cite{he2016deep}
and natural language processing~\cite{devlin2018bert},
and the development of various fields
now relies on large-scale datasets.
While these datasets
have undoubtedly contributed significantly
to the progress of deep learning,
the collection of unauthorized
private data for training these models
now presents an emerging concern.
Recently, numerous poisoning methods~\cite{
    furobust, huangunlearnable,
    sandoval2022autoregressive,
    tao2021better,
    yu2022availability}
have been proposed
to add imperceptible perturbations to images.
These perturbations
can form ``shortcuts''~\cite{%
    geirhos2020shortcut, huangunlearnable}
in the training data
to prevent training
and thus make the data unlearnable
in order to preserve privacy.
It is commonly perceived
that the only effective defense against unlearnable examples
are adversarial training algorithms~\cite{huangunlearnable,tao2021better,furobust}.
Popular data augmentation methods
such as CutOut~\cite{devries2017improved},
MixUp~\cite{zhang2017mixup},
and AutoAugment~\cite{cubuk2019autoaugment},
however,
have all been demonstrated to be ineffective defenses.
% Adversarial training~\cite{madry2017towards}
% is a powerful countermeasure
% against most unlearnable examples,
% but its high computational cost
% and noticeable accuracy loss
% could make it impractical~\cite{furobust}.
% Recently,
% a simple yet effective defense,
% Matrix-Completion~\cite{he2022indiscriminate},
% shows great effectiveness
% against unlearnable examples.
% It simply drops pixels
% and reconstructs the missing pixels
% through the universal singular value thresholding
% (USVT)~\cite{chatterjee2015matrix} algorithm,
% which allows it to obtain much better test accuracies
% than adversarial training
% when trained with unlearnable datasets.

% This paper thus considers the most commonly used method
% for data pre-processing,
% \ie{}, data augmentation~\cite{maharana2022review}.
% Current research suggests that there
% is no effective augmentation method~\cite{furobust,huangunlearnable}
% for unlearnable examples.
% FIXME ^ 跟上面的CutOut MixUp意思重复了，建议精炼
% We thus combine adversarial training
% and data augmentation
% to enable the deep learning model to find
% suitable augmentation policies
% by learning with adversarial augmentation policies,
% thus suppressing data ``shortcuts''~\cite{
%     geirhos2020shortcut}
% and achieving defense against
% unlearnable examples.

Current methods of unlearnable attacks
involves the specification
of an \( \ell_p \) perturbation budget,
where \( p \in \braces{2, \infty} \) in general.
Essentially,
they constrain the added perturbation
to a small \( \epsilon \)-ball of \( \ell_p \)-distance
from the source image,
in order to ensure stealthiness of these attacks.
Adversarial training defenses~\cite{madry2017towards,furobust}
represent a defense mechanism
that seeks to counteract the bounded perturbations
from such unlearnable attacks.
However, large defensive perturbations
comes with significant accuracy degradations.
This prompts the inquiry of the existence
of effective defense mechanisms
that leverage threat models
that are outside the purview of attackers.
Specifically,
\emph{can we devise effective adversarial policies
for training models
that extend beyond the confines
of the \( \ell_p \) perturbation budgets?}

In this paper,
we thus propose \Method{},
which performs error-maximizing data augmentation,
to defense against unlearning poisons.
\Method{} challenges
the preconception that data augmentation
is not an effective defense against unlearning poisons.
\Method{} expands the perturbation distance
far beyond traditional adversarial training,
as data augmentation policies
do not confine themselves
to the \( \ell_p \)
perturbation constraints.
It can therefore effectively disrupt ``unlearning shortcuts''
formed by attacks within narrow \( \ell_p \) constraints.
Yet, the augmentations employed by \Method{}
are natural and realistic transformations
extensively utilized by existing works
to improve the models' ability to generalize.
This, in turn, helps in avoiding accuracy loss
due to perturbations used by adversarial training
that could potentially be out-of-distribution.
Finally,
traditional adversarial training
is not effective in mitigating unlearning poisons
produced by adaptive attacks~\cite{furobust},
while \Method{} is highly resiliant
against adaptive attacks
with significantly lower accuracy reduction.

In summary, our work has three main contributions:
\begin{itemize}
    \item
    It extends adversarial training
    beyond the confines
    of the \( \ell_p \) perturbation budgets
    commonly imposed by attackers
    into data augmentation policies.

    \item
    We propose \Method{},
    which introduces an effective adversarial augmentation
    to wipe out unlearning perturbations.
    It defends against the unlearnable attacks
    by maximizing the error of the augmented samples.

    \item
    \Method{} is highly effective
    in wiping out the unlearning effect
    on five state-of-the-art (SOTA) unlearning attacks,
    outperforming existing SOTA defense methods.

    \item
    We explore the adaptive attacks
    on \Method{}
    and explored additional combinations
    of augmentation policies.
    It lays a fresh foundation for future competitions
    among unlearnable example attack and defense
    strategies.
\end{itemize}

Unlearnable example attacks bear great significance,
not just from the standpoint of privacy preservation,
but also as a form of data poisoning attack.
It is thus of great significance
to highlight the shortcomings
of current attack methods.
Perhaps most surprisingly,
even a well-known unlearnable attack
such as EM~\cite{huangunlearnable}
is unable to impede the effectiveness of \Method{}.
By training a ResNet-18 model from scratch
using exclusively CIFAR-10 unlearnable data
produced with EM
(with an \( \ell_\infty \) budget of \( 8/255 \)),
\Method{} achieves exceptional accuracy of \( 95.24\% \)
on the clean test set,
which closely matches the accuracy achievable
by standard training on a clean training set.
This suggests that existing unlearning perturbations
are tragically inadequate
in making data unlearnable,
even with adaptive attacks
that employs \Method{}.
% Additional comparisons
% can be found in~\Cref{tab:ablation:emaug}.
By understanding their weaknesses,
we can anticipate how malicious actors
may attempt to exploit them,
and prepare stronger safeguards against such threats.
We hope \Method{}
can help facilitate the advancement
of research in these attacks and defenses.
% Our code is open source and available to the deep
% learning community\footnote{\repourl}.
