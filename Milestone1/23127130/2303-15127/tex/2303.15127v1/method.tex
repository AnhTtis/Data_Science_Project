\section{The \Method{} Defense}

\subsection{Preliminaries on Unlearnable Example Attacks and Defenses}

\textbf{Attacker.}
We assume the attacker has access
to the original data
they want to make unlearnable,
but cannot alter the training process~\cite{li2022backdoor}.
Typically,
the attacker attempts
to make the data unlearnable
by adding perturbations to the images
to prevent trainers
from using them to learn a
classifier that generalize well
to the original data distribution.
Formally,
suppose we have a dataset
consisting of original clean examples
\(
    \cleanset = \braces*{
        \parens*{\bx_{1}, y_{1}},
        \ldots,\parens*{\bx_{n}, y_{n}}
    }
\)
drawn from a distribution \( \sampledist \),
where \( \bx_i \in \inputset \) is an input image
and \( y_i \in \outputset \) is its label.
The attacker thus aims
to construct a set
of sample-specific unlearning perturbations
\( \bdelta = \{ \bdelta_\bx | \bx \in \inputset \} \),
in order to make the model
\( f_\btheta \colon \inputset \rightarrow \outputset \)
trained on the \emph{unlearnable examples} set \(
    \ueset\parens{\bdelta} = \braces*{
        \parens{\bx + \bdelta_\bx, y} \mid
        \parens{\bx, y} \in \cleanset
    }
\)
perform poorly on a test set \( \testset \)
sampled from \( \sampledist \):
\begin{equation}
    \max_{\bdelta}
    \expect_{
        (\bx_i, y_i) \sim \testset
    }\bracks{
        \loss\parens{
            f_{\btheta^\star\parens{\bdelta}} \parens{\bx_i},
            y_i
        }
    },
    \text{ s.t. }
    \btheta^\star\parens{\bdelta}
        = \underset{\btheta}{\operatorname{argmin}}
    \sum_{
        \qquad
        \mathclap{\parens*{\bx_{i}, y_{i}}
        \in \ueset \parens{\bdelta}}
        \qquad
    }
    \loss \parens*{
        f_{\btheta} \parens*{ \hat\bx_{i} },
        y_{i}
    },
    \label{eq:attacker}
\end{equation}
where \( \loss \) is the loss function,
typically the softmax cross-entropy loss.
For each image,
the noise \( \bdelta_i \) is bounded
by \( \norm{\bdelta_i}_p \leq \epsilon \),
where \( \epsilon \) is a small perturbation budget
such that it may not affect the intended utility of the image,
and \( \norm{\cdot}_p \) denotes the \( \ell_p \) norm.
\Cref{tab:visualization}
provides samples generated
by unlearnable example attacks
and their corresponding perturbations
(amplified with normalization).
% five unlearnable methods
% EM~\cite{huangunlearnable},
% REM~\cite{furobust},
% HYPO~\cite{tao2021better},
% LSP~\cite{yu2022availability}, and
% AR~\cite{sandoval2022autoregressive}.
% Here, EM and REM generate error-minimizing
% perturbations through a bi-level
% optimization process.
% The main idea of HYPO is
% similar to EM,
% but it replaces the bi-level
% optimization process
% with a pretrained surrogate.
% LSP and AR synthesize unlearnable
% examples by sampling
% from Gaussian distributions.
% So recent research~\cite{liu2023image}
% divided unlearnable
% methods into three categories:
% Slightly-trained surrogate (EM, REM);
% Fully-trained surrogate (HYPO);
% Surrogate-free (LSP, AR).
% We will verify the effectiveness
% of \Method{} on the five unlearnable
% methods above.
\input{tables/uep}

\textbf{Defender.}
The goal of the defender
is to ensure that the trained model
learns from the poisoned training data,
allowing the model to be generalized
to the original clean data distribution \( \cleanset \).
The attacker assumes full control
of its training process.
In our context,
we thus assume that the attacker's
policy is to perform poison removal on the image,
in order to ensure the trained model generalizes
even when trained on poisoned data \( \ueset \).
It has been shown
in~\cite{huangunlearnable,tao2021better,furobust}
that Adversarial training~\cite{madry2017towards}
is effective against unlearnable examples,
which optimizes the following objective:
\begin{equation}
    \arg\min_{\btheta}
    \expect_{\parens{\hat\bx, y} \sim \ueset} \bracks*{
        \max_{
            \norm{\bdelta_\textrm{adv}}_p \leq \epsilon
        }
        \loss\parens*{f_\btheta\parens{\hat\bx + \bdelta_\textrm{adv}}, y}
    }.
    \label{eq:adversarial_training}
\end{equation}
Specifically for each image \( \hat\bx \in \ueset \),
it finds an adversarial perturbation \( \bdelta_\textrm{adv} \)
that maximizes the classifier loss.
It then performs gradient descent on the maximal loss
to optimize for the model parameters \( \btheta \).
A model trained on the unlearnable set \( \ueset \) in this manner
thus demonstrates robustness
to perturbations in the input,
and can generalize to clean images.

\subsection{Adversarial Augmentations}

% As Matrix-Completion~\cite{he2022indiscriminate}
% method is proposed,
% it is demonstrated that destroying
% the partial perturbation pattern
% can wipe out the unlearning effect.
% Image Shortcut Squeezing
% (ISS)~\cite{liu2023image}
% attains SOTA metrics
% by utilizing simple techniques
% such as image compression and grayscale filters.
% Undoubtedly, this simple and direct
% image preprocessing completely
% eliminates unlearning perturbations,
% but it also results in a
% non-negligible accuracy loss.
% From LSP~\cite{yu2022availability},
% we know that the unlearning perturbations
% under supervised learning are
% linearly differentiable.
% Then is there an approach
% that breaks this strong
% linear correlation
% without any loss of accuracy?
Geirhos~\etal{}~\cite{geirhos2020shortcut}
reveal that models tend to learn ``shortcuts'',
\ie{}, unintended features in the training images.
These shortcuts negatively
impact the model's generalization ability.
Intuitively,
unlearnable example attacks
thus leverage such shortcuts
to generate effective perturbations
to impede learning from the poisoned examples.
Subsequently,
existing adversarial training defenses~\cite{%
    huangunlearnable,tao2021better,furobust}
attempt to remove these shortcuts
from training images
with adversarial perturbations.
This is done to counter the effects
of the unlearning perturbations.

It is natural to think
that augmentation policies may be a dead end
against unlearnable attacks,
as none of the existing strong data augmentation methods
show significant effectiveness (\Cref{tab:ueaug}).
Adversarial training
can also be viewed as a practical
data augmentation policy,
which presents an interesting perspective
as it allows the model
to choose its own policy in the form
of \( \ell_p \)-bounded perturbations adaptively.
However, it poses a considerable challenge
due to its use of large defensive perturbations,
often resulting in reduced accuracy.
This begs the question
of whether new defense mechanisms
can leverage \emph{unseen} threat models
that unlearnable attacks
may be unable to account for.

Inspired by this,
we introduce \Method{},
which performs adversarial augmentations polices
that preserves to the semantic information
of the images
rather than adding \( \ell_p \)-bounded adversarial noise.
Our objective
is a bi-level optimization,
where the inner level
samples image transformation policies
\( \transform(\cdot) \)
from a set of all possible augmentations \( \augdist \),
in order to maximize the loss,
and the outer level performs model training
with adversarial polices:
\begin{equation}
    \arg\min_{\btheta} \expect_{
        (\bx, y) \sim \ueset
    } \bracks*{
        \max_{\transform \sim \augdist}
        \loss\parens*{
            f_{\btheta}\parens{
                \transform\parens*{\bx}
            }, y
        }
    }.
    \label{eq:method}
\end{equation}
Intuitively,
\Method{} finds the most ``adversarial''
augmentation policies
for the current images,
and use that to train the model
in order to prevent unlearnable ``shortcuts''
from emerging during model training.
Compared to adversarial training methods
that confine the defensive perturbations
within a small \( \epsilon \)-ball
of \( \ell_p \) distance,
here we adopt a different approach
that allows for a more aggressive distortion.
Moreover,
augmentation policies
also effectively preserve the original semantics
in the image.
By maximizing the adversarial loss in this manner,
the model can thus avoid training from the unlearning ``shortcuts''
and instead learn from the original features.

To generate augmentation policies
with high-level of distortions,
we select PlasmaTransform~\cite{nicolaou2022tormentor},
and TrivialAugment~\cite{muller2021trivialaugment},
two modern suites of data augmentation policies,
and ChannelShuffle in sequence,
to form a strong pipeline of data augmentations polices.
PlasmaTransform performs image distortion
with fractal-based transformations.
TrivialAugment
provide a suite of natural augmentations
which shows great generalization abilities
that can train models with SOTA accuracies.
Finally,
ChannelShuffle swaps the color channels randomly,
this is added to further increase the aggressiveness
of adversarial augmentation policies.
Interestingly,
using this pipeline
without the error-maximization augmentation sampling
can also significantly
reduce the effect of unlearning perturbations.
We denote this method as \MethodLite{},
as it requires only 1 augmentation sample
per training image.
Compared to \Method{},
although \MethodLite{}
may not perform as well as \Method{}
on most datasets,
it is more practical
than both \MethodLite{} and adversarial training
due to its faster training speed.

Finally,
we provide an algorithmic overview
of \Method{} in~\Cref{alg:method}.
It accepts as input
the poisoned training dataset \( \ueset \),
batch size \( B \),
randomly initialized model \( f_\btheta \),
number of training epochs \( N \),
number of error-maximizing augmentation epochs \( W \),
learning rate \( \alpha \),
number of repeated sampling \( K \),
and a suite of augmentation policies \( \augdist \).
For each sampled mini-batch \( \bx, \by \)
of data points from the dataset,
it applies \( K \) different
random augmentation policies
for each image in \( \bx \),
and compute the corresponding loss values
for all augmented images.
It then selects
for each image in \( \bx \),
the maximum loss produced by its \( K \) augmented variants.
The algorithm then uses the averaged loss
across the same mini-batch
to perform gradient descent
on the model parameters.
Finally,
the algorithm
returns the trained model parameters \( \btheta \)
after completing the training process.
\input{algor}
