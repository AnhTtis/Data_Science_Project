\subsection{Partial Poisoning}\label{sec:results:ablation:partial}

In practical scenarios,
attackers may only have partial control
over the training data~\cite{huangunlearnable},
thus it is more practical
to consider the scenario
where only a part of the data is poisoned.
We adopt EM~\cite{huangunlearnable}
and LSP~\cite{yu2022availability}
on the CIFAR-10 dataset
as an example for our discussions.
Following the same setup,
we split varying percentages
from the clean data to carry out unlearnable poisoning
and mix it with the rest of the clean training data
for the target model training.
\Method{} is applied during model training
to explore its effectiveness
against partial poisoning.
\Cref{fig:sensitivity:em,fig:sensitivity:lsp},
show that when the poisoning ratio is low (\( <40\% \)),
the effect of the poisoning is negligible.
% From~\Cref{limit},
% we know \Method{}
% makes model convergence
% slower due to its multiple augmentation policies.
% In this case,
% we need to be more realistic in considering
% whether to use \Method{}.
Another type of partial dataset attack scenario
is the selection of a targeted class to poison.
We thus poison all training
samples of the \ordinal{9} label (``truck''),
and \Cref{fig:sensitivity:standard,fig:sensitivity:ueraser}
shows the prediction confusion matrices
of ResNet-18 trained on CIFAR-10.
In summary,
\Method{} demonstrates significant efficacy
in partial poisoning scenarios.
\begin{figure*}[htbp]
\centering
\begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{EM-p.pdf}
    \caption{Partial poisoning of EM.}\label{fig:sensitivity:em}
\end{subfigure}
\hfill
\begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{LSP-p.pdf}
    \caption{Partial poisoning of LSP.}\label{fig:sensitivity:lsp}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth]{unlearnconf.pdf}
    \caption{%
        Prediction matrix.
    }\label{fig:sensitivity:standard}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth]{ueraserconf.pdf}
    \caption{%
        With \Method{}.
    }\label{fig:sensitivity:ueraser}
\end{subfigure}
% \vspace{8pt}
\caption{%
    The defensive efficacy of
    \Method{} against partial poisoning.
    (\subref{fig:sensitivity:em})
    EM with different poisoning ratios;
    (\subref{fig:sensitivity:lsp})
    LSP with different poisoning ratios.
    (\subref{fig:sensitivity:standard}),
    (\subref{fig:sensitivity:ueraser})
    Prediction confusion matrices
    on the clean test set
    of ResNet-18 trained on CIFAR-10
    with an unlearnable class
    (\emph{the \ordinal{9} label `truck'}).
    (\subref{fig:sensitivity:standard}) Standard training;
    (\subref{fig:sensitivity:ueraser}) \Method{} training.
}\label{fig:sensitivity}
\end{figure*}

\subsection{Adaptive Poisoning}\label{sec:results:ablation:adaptive}

Since \Method{} is composed
of multiple data augmentations,
we should consider possible adaptive
unlearnable example attacks
which may leverage \Method{}
to craft poisons against it.
We therefore evaluate \Method{}
in a worst-case scenario
where the adversary is fully aware
of our defense mechanism,
in order to reliably assess the resilience of \Method{}
against potential adaptive attacks.
Specifically,
we design an adaptive unlearning poisoning attack
by introducing an additional data augmentation
during the training,
We adopt the error-minimization
(EM) attack~\cite{huangunlearnable}
as an example.
The EM unlearning objective
solves the following min-min optimization:
\begin{equation}
    \arg\min_{\bdelta} \min_{\btheta}
    \expect_{(\bx, y) \sim \cleanset}\bracks*{
        \loss\parens*{
            f_\btheta \parens{\bx+\bdelta_{\bx}}, y
        }
    },
    \label{eq:attack:em}
\end{equation}
where \( \|\bdelta\|_{p} \leq \epsilon \).
Similar to the REM~\cite{furobust}
that generates adaptive unlearnable examples
under adversarial training,
each image \( \bx \)
optimizes its unlearning perturbation \( \bdelta_\bx \)
before performing adversarial augmentations:
\begin{equation}
    \arg\min_{\bdelta} \min_{\btheta}
    \expect_{(\bx, y) \sim \cleanset} \bracks*{
         \loss\parens*{
            f_\btheta \parens*{
                \transform_{\textrm{adv}}\parens{\bx + \bdelta_\bx}
            }, y
        }
    },
    \label{eq:attack:adaptive}
\end{equation}
where \( \transform_{\textrm{adv}}\parens{\cdot} \)
denotes the adversarial augmentations with \Method{}.
We select different combinations of augmentations
for our experiments,
and the results are shown in~\Cref{tab:adaptive}.
The hyperparameters of the augmentations
employed in all experiments
are kept consistent
with those of \Method{}.
We observe
that the adaptive augmentation of unlearning poisons
do not significantly reduce the effectiveness
of \Method{}.
As it encompasses a diverse set of augmentation policies
and augmentation intensities,
along with loss-maximizing augmentation sampling,
adaptive poisons are hardly effective.
Moreover,
we speculate that the affine and cropping transformations
in TrivialAugment can cause unlearning perturbations
to be confined to a portion of the images,
which also limits the effectiveness
of unlearning poisons.
Because of the aggressiveness of the augmentation
in image transformations
extend beyond the \( \ell_p \) bounds,
adaptive poisons do not perform as well under \Method{}
as they do against REM\@.
To summarize,
it is challenging for the attacker
to achieve successful poisoning
against \Method{}
even if it observes the possible transformations
taken by the augmentations.
\input{tables/adaptive}

\subsection{%
    Larger Perturbation Scales
}\label{sec:results:ablation:larger_perturbation}

Will the performance of \Method{}
affected by large unlearnable perturbations?
To verify,
we evaluate the performance of \Method{}
on unlearnable CIFAR-10 dataset
with even larger perturbations.
We use the example
of error-maximizing (EM) attack
and increase the \( \ell_{\infty}\) perturbation
from \(8/255\) to \(24/255\)
to examine the efficacy of \Method{}
on a more severe unlearning perturbation scenario.
We also include adversarial training (AT)
as a defense baseline
with a perturbation bound of \( 8/255 \).
The experimental results in~\Cref{tab:scale}
confirm the effectiveness of \Method{}
under large unlearning noises.
\input{tables/scale}

\subsection{%
    Resilience against Architecture Choices
}\label{sec:results:ablation:architectures}

Can \Method{} show resilience
against architecture choices?
In a realistic scenario,
we need to train the data
with different network architectures.
We thus explore whether \Method{}
can wipe out the unlearning effect
under different architectures.
\Cref{tab:arch} shows the corresponding results.
It is clear that \Method{}
is capable of effectively wiping out unlearning poisons,
across various network architectures.
\input{tables/arch}

\subsection{Augmentation Options}\label{sec:results:ablation:options}

In this section,
we investigate the impact
of the \Method{} policy composition
on the mitigation of unlearning effects.
The visualization of the three policies
included is shown in~\Cref{fig:augvis}.
We conduct experiments
with the unlearnable examples from CIFAR-10
generated by the EM~\cite{huangunlearnable} method,
and~\Cref{tab:policy}
explore effectiveness of the various combinations
of augmentation policies.
ISS~\cite{liu2023image}
discovered that for the unlearnable examples
generated by the EM attack,
a grayscale filter easily
removes the unlearning poisons.
Additionally,
setting the value of each channel
to the average of all color channels
or to the value of any color channel
also considerately achieves the same effect.
However,
we show that using only ChannelShuffle
does not yield satisfactory results.
We have also discovered
an interesting phenomenon:
PlasmaTransform and ChannelShuffle
are essential for mostly restoring the accuracies,
whereas TrivialAugment,
can be substituted
with a similar policy,
\ie{}~AutoAugment~\cite{cubuk2019autoaugment}.
Only when the three policies
are employed together
can the effect of unlearning poisons
be effectively wiped out.
This also proves that the \Method{} policies
are effective and reasonable.
Recall that we also found the adoption
of error-maximizing augmentation
results in an overall improvement
on all five unlearning poisons.
Hence,
the utilization of error-maximizing
augmentation during training
serves as an effective means
to mitigate the challenges of training with unlearning examples
and improve the model's clean accuracy.
% We believe that \Method{}
% can effectively advance future research on unlearnable examples.
\input{tables/policy}

\subsection{%
    Adversarial Augmentations and Error-Maximizing Epochs
}\label{sec:results:ablation:repeat}

From~\Cref{alg:method},
the training of \Method{}
is affected by two hyperparameters, namely,
the numbers of repeated augmentation samples \( K \)
per image
and the epochs of error-maximizing augmentations \( W \).
For CIFAR-10,
The clean accuracy of the unlearnable examples
can be improved to around \( 80\% \)
after 50 epochs of training
using error-maximizing augmentation.
We explored \MethodMax{}
which applies error-maximizing augmentations
throughout the entire training phase,
and it attains best known accuracies.
The results are shown in~\Cref{tab:ablation:emaug}.
Alternatively,
one can continue the training with \MethodLite{}
can improve the clean accuracy to \( \geq 93\% \)
to save computational costs.
Although \MethodMax{}
achieves the highest clean accuracy,
we mainly focus on \Method{} in this paper,
due to its high computational cost per epoch
and longer training epochs.
\input{tables/emaug}

Regarding the number of samples \( K \)
(by default \( K = 5 \)),
increasing it
further enhances the suppression
of unlearning shortcuts
during model training,
but also more likely to lead to gradient explosions
at the beginning of model training.
Therefore,
it may be necessary
to apply learning rate warmup
or gradient clipping
with increased number of repeated sampling.
Larger \( K \) can also results
in higher computational costs,
as it result in more samples per image
for training.
We provide a sensitivity analysis
of the number of repeated sampling \( K \)
in~\Cref{app:sensitivity}.

\subsection{Transfer Learning}\label{sec:results:ablation:transfer}

In this section,
we aim to explore the impact
of transfer learning~\cite{dai2009eigentransfer,torrey2010transfer}
on the efficacy of unlearnable examples.
We hypothesize that pretrained models
may learn certain in-distribution features
of the unaltered target distribution,
it may be able to gain accuracy
even if the training set contains unlearning poisons.

To this end,
we adopt a simple transfer learning setup,
where we use the pretrained ResNet-18
model available in the torchvision repository~\cite{torchvision}.
To fit the expected input shape
of the feature extractor,
we upsampled the input images
to \( 224 \times 224 \).
The final fully-connected classification layer
of the pretrained model
was replaced with a randomly initialized one
with 10 logit outputs.
We then fine-tune the model with
unlearnable CIFAR-10 training data.
We also further explored fine-tuning
on unlearnable data with our defenses.
For control references,
we fine-tuned a model
with clean training data,
and also trained a randomly initialized model from scratch
with poisoned training data.
\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.24\linewidth}
        \includegraphics[width=\linewidth]{transfer_clean.pdf}%
        \caption{%
            Fine-tuning on a clean training set.
        }\label{fig:transfer:clean}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \includegraphics[width=\linewidth]{transfer_unlearn.pdf}%
        \caption{%
            Fine-tuning
            on an unlearnable training set.
        }\label{fig:transfer:unlearn}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \includegraphics[width=0.93\linewidth]{transfer_test.pdf}%
        \caption{%
            Different input dimensions
            on an unlearnable training set.
        }\label{fig:transfer:test}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \includegraphics[width=\linewidth]{transfer_uefast.pdf}%
        \caption{%
            Train and test accuracies of transfer learning
            with \Method{}.
        }\label{fig:transfer:ueem}
    \end{subfigure}
    % \subfigure[
    %     Fine-tuning with \MethodLite{}.
    % ]{%
    %     \includegraphics[scale=0.5]{transfer_uefast.pdf}%
    %     \label{fig:transfer:uefast}
    % }
    % \quad
    \caption{%
        Accuracies \wrt{} the number of training / fine-tuning epochs
        for randomly initialized / pretrained ResNet-18
        on different CIFAR-10 datasets.
        (\subref{fig:transfer:clean})
        Fine-tuning the pretrained model
        on a clean training set.
        (\subref{fig:transfer:unlearn})
        Fine-tuning the pretrained model
        with unlearnable training set
        generated with EM~\cite{huangunlearnable}.
        (\subref{fig:transfer:test})
        Comparing the test accuracies
        by training from scratch
        with either \(32 \times 32\)
        or upsampled \(224 \times 224\)
        unlearnable examples.
        (\subref{fig:transfer:ueem})
        Fine-tuning on unlearnable data
        with \Method{}.
    }\label{fig:transfer}
\end{figure}

The results of the experiments
are shown in~\Cref{fig:transfer}.
\Cref{fig:transfer:unlearn}
shows that fine-tuning
with unlearnable examples
can improve the clean test accuracy
from \(22\%\) to \(66\%\).
Additionally in~\Cref{fig:transfer:test},
we find that simply upsampling
the unlearnable samples
to use more compute and have larger feature maps
does not significantly weaken
the unlearning attack
(test accuracy increased to \(34\%\)).
Most importantly,
\Method{} successfully
eliminates the negative impact
of unlearning poisons,
which enables the model
to utilize pretrained knowledge effectively.
This enables the fine-tuned model
to achieve a test accuracy
of approximately \( 95\% \)
as shown in~\Cref{fig:transfer:ueem}.
