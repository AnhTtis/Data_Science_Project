%
% \documentclass[10pt,peerreview,compsoc]{IEEEtran}
\documentclass[10pt,journal,compsoc]{IEEEtran}


% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % The IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{bm}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{makecell}

\usepackage{amsthm}

\begin{document}


\title{Discovering Predictable Latent Factors for Time Series Forecasting}

\author{Jingyi~Hou,
        Zhen~Dong,
        Jiayu~Zhou,
        and~Zhijie~Liu,~\IEEEmembership{Member,~IEEE}
        % and~Wei~He,~\IEEEmembership{Senior Member,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem 
J. Hou and Z. Liu are with the School of Intelligence Science and Technology, University of Science and Technology Beijing, Beijing 100083, China, and the Institute of Artificial Intelligence, University of Science and Technology Beijing, Beijing 100083, China.
% J. Hou, Z. Liu, and W. He are with the School of Intelligence Science and Technology (Institute of Artificial Intelligence), University of Science and Technology Beijing, Beijing 100083, China.
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: \{houjingyi, liuzhijie\}@ustb.edu.cn.\protect\\
% E-mail: \{houjingyi, liuzhijie\}@ustb.edu.cn; weihe@ieee.org.\protect\\
\IEEEcompsocthanksitem Z. Dong is with the College of Mathematics and Computer Science, Yan’An University, Yan’an 716000, China.
E-mail: cndongzhen@gmail.com\protect\\
\IEEEcompsocthanksitem J. Zhou is with the Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48823. 
E-mail: jiayuz@msu.edu.
}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}




% The paper headers
\markboth{ }%
{ }
%{Hou \MakeLowercase{\textit{et al.}}: Discovering Predictable Latent Factors for Financial Time Series Forecasting}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



\IEEEtitleabstractindextext{%
\begin{abstract}
  Modern time series forecasting methods, such as Transformer and its variants, have shown strong ability in sequential data modeling.
  To achieve high performance, they usually rely on redundant or unexplainable structures to model complex relations between variables and tune the parameters with large-scale data.
  Many real-world data mining tasks, however, lack sufficient variables for relation reasoning, and therefore these methods may not properly handle such forecasting problems.
  With insufficient data, time series appear to be affected by many exogenous variables, and thus, the modeling becomes unstable and unpredictable.
  To tackle this critical issue, in this paper, we develop a novel algorithmic framework for inferring the intrinsic latent factors implied by the observable time series. 
  The inferred factors are used to form multiple independent and predictable signal components that enable not only sparse relation reasoning for long-term efficiency but also reconstructing the future temporal data for accurate prediction.
  To achieve this, we introduce three characteristics, i.e., predictability, sufficiency, and identifiability, and model these characteristics via the powerful deep latent dynamics models to infer the predictable signal components.
  Empirical results on multiple real datasets show the efficiency of our method for different kinds of time series forecasting.
  The statistical analysis validates the predictability of the learned latent factors.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Financial time series, sequence modeling, latent factor model.
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when compsoc mode
% is not selected <OR> if conference mode is selected - because compsoc
% conference papers position the abstract like regular (non-compsoc)
% papers do!
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc under a non-conference mode.


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

% \IEEEPARstart{F}{inancial} time series are hardly predictable characterized by the uncertainty of the asset volatility. % which might not be directly observable.
\IEEEPARstart{T}{ime} series forecasting is an essential task of data science and machine learning with a wide range of applications, such as financial management, health informatics, weather forecasting, and epidemic prediction.
Deep learning has shown its strong ability to model various complex sequential information in many fields, especially for vision and language tasks \cite{DBLP:conf/iccv/Feichtenhofer0M19,DBLP:conf/emnlp/0001DL16,DBLP:conf/nips/VaswaniSPUJGKP17}. 
Even though there are many efforts applying deep learning in time series forecasting, unlike the data in successful deep learning applications, most real-world times series neither contain sufficient information with high-dimensional data nor represent explicit semantic information that can guide the data analysis. 
Therefore, deep learning methods still face critical challenges in time-series forecasting tasks.
Especially, the regularities of time series could be affected by many exogenous factors that are hardly observable.
For example, in the financial domain, asset volatility describes the price changes of an asset (e.g., a stock), and its prediction is important in many quantitative strategies. The volatility of an asset is affected by massive factors, e.g.,  the historical price information of the asset, economic factors, and also various events in the markets (that can sometimes be captured by the news), but it is virtually impossible to collect all the relevant events and news.


Discovering and modeling these unobserved factors and their relations can empower time series forecasting with good interpretability and thus greatly improve the prediction performance of prediction models.
Prior work has explored the use of factors and their relations to predict time series.
It is achieved by either explicitly exploiting additional knowledge \cite{DBLP:conf/acl/CohenX18,DBLP:journals/tkde/ShiTWZB19,DBLP:journals/pr/ChengYXL22,DBLP:journals/corr/abs-2110-13716} or discovering hidden representations from the historical data of time series \cite{duan2022factorvae}.
These approaches may have limited applicability and are only feasible for tasks in certain domains with the need for a sophisticated process of domain knowledge.
The method \cite{DBLP:conf/kdd/WuPL0CZ20} uses the values of timelines as auxiliary nodes and models the relationships between both observed and auxiliary nodes for predicting multiple types of time series.
It has a limited ability to discover more explicable factors contributing to the prediction of future data. 
Moreover, the relations among different factors in the real world are too complex for practical computation that involves time, and some of the factors are even unobservable, which further increases the difficulty of precise modeling.




% This work attempts to address the aforementioned problems 
Fortunately, since the observable data is dependent on these unobserved factors, it is possible to infer the trails of the unobservable factors from the observation.
%Given large-scale data nowadays, we might as well infer the latent factors from data via generative models. 
Given large-scale data nowadays, we might as well learn a graphical model to infer the latent factors from data generatively.
Through some straightforward and general prior knowledge, we can make reasonable independence assumptions on these latent factors to encourage the sparsity of the relations between factors, so as to reduce the computational complexity.
Therefore, we propose to explain the generation procedure of the original time series for the forecasting tasks by inferring latent factors varying along time from all the observed historical time series.
%To conquer the problems caused by the hidden relations of factors in both time and concept spaces, our work infers different latent factors from the time series to form several simple and intrinsic temporal signals that are easy to predict, where the ``intrinsic'' denotes factors in the same signal component are self-descriptive and do not affect by factors from other signal components.
To conquer the problems caused by the hidden relations of different factors along time, our work infers different latent factors from the time series to form several simple and intrinsic temporal signals that are easy to predict, where the ``intrinsic'' denotes factors in the same signal component are self-descriptive and do not affect by factors from other signal components.
Our approach is motivated by the general non–stationary signal decomposition methods, i.e., empirical mode decomposition (EMD) \cite{huang1998empirical} and its variations \cite{huang2014hilbert,DBLP:journals/tsp/DragomiretskiyZ14}.
Since the derived temporal signals are simple and intrinsic, it is easy to predict the future factors of each signal, and we can easily predict future data given the predicted factors.
Under this premise, we can assume that factors of different derived temporal signals are pairwise independent to ensure the intrinsicality of the signals for prediction. The assumption also disentangles the factors to obtain more sparse relations for efficient calculation.
%to avoid too much computations of their complex relations.
Accordingly, our aim is to discover factors to form multiple signal components that are simple and intrinsic for prediction (i.e., predictability), informative for reconstruction (i.e., sufficiency), and feasible for learning (i.e., identifiability), instead of the delicately predefined intrinsic modes that should be oscillatory and add up to be the original signal. 

Based on the above motivations, we propose a novel approach for inferring predictable factors for forecasting time series.
The proposed framework is constructed based on variational inference and sequence-based models by approximating the joint distribution over observed time series and latent factors following the aforementioned key characteristics of predictability, sufficiency, and identifiability.
The framework can also be decomposed into multiple latent dynamics models that are conditionally pairwise independent for alleviating the error accumulating of the long-term series.
We conduct empirical studies on two typical time series forecasting tasks, i.e., long-term series forecasting and stock trend forecasting, to show the effectiveness of our method, and we also conduct a statistical analysis to gain insights into the predictability of the inferred signal components.
The contributions of this work are summarized as follows:


\begin{itemize}
  \item We consider inferring latent factors varying along the time from the observation to form multiple signal components with characteristics of predictability, sufficiency, and identifiability. The factors we inferred can be used to easily construct future signals for time series forecasting.
  \item We design a novel method that disentangles the latent factors and models the relations of the factors according to the signal components to draw the joint distribution for more efficient approximation.
  \item The proposed method achieves better performance compared with the state-of-the-art for long-term series forecasting and stock trend forecasting, respectively.
\end{itemize}





\section{Related Work}

\subsection{Time Series Forecasting}

Time series forecasting is challenging and has long been investigated in many research areas. 
Several classical methods are proposed for industries, such as the autoregressive methods \cite{sims1980macroeconomics,tauchen1986finite}, moving average methods \cite{box2015time}, filtering models \cite{morrison1977kalman,joo2015time}, and factor models \cite{sharpe1964capital,fama1992cross}.
Although these methods are limited in their ability to handle data available nowadays that become increasingly large-scale and high-dimensional, they still provide theoretical guidance for time series forecasting methods.

With the notable rise of deep learning, sequence-based neural networks are getting increasingly involved.
One typical structure is the recurrent neural network (RNN) which is able to process theoretically arbitrary long-term input sequences with only a few parameters \cite{DBLP:conf/nips/RangapuramSGSWJ18,DBLP:journals/corr/FlunkertSG17}.
Despite these advantages, the recursive mechanism is not really conducive to information propagation.
Attention-based RNNs \cite{DBLP:conf/ijcai/QinSCCJC17,DBLP:conf/aaai/SongRTS18,DBLP:conf/ijcai/FengC0DSC19} address this problem by encoding long-term series with the adaptive weighted summation for generating the prediction.
Another structure is the convolutional network that captures various local temporal patterns with different convolutional filters \cite{borovykh2017conditional,DBLP:conf/sigir/LaiCYL18,DBLP:conf/nips/SenYD19}.
Recently, Transformer-based models \cite{DBLP:conf/ijcai/FengC0DSC19,DBLP:conf/iclr/KitaevKL20,DBLP:conf/aaai/ZhouZPZLXZ21,DBLP:conf/nips/WuXWL21,liunon,DBLP:conf/icml/ZhouMWW0022} are developed for time series forecasting.
These methods regard each time series data as an undirected graph, where each node represents the feature of a time step, and the edges are calculated as the attention weights between nodes. 
Although the Transformer-based methods have a strong model-fitting ability, they still suffer from high computation complexity.
In order to save the number of model parameters, RNN-based models, convolutional networks, and the variants of Transformer have to make some assumptions (e.g., sparsity and locality) that might impair the modeling capability of long-term series data.
Differently, our method focuses on inferring easy-to-predict factors from the original time series, so that we can use these relatively simple predictors (without strong fitting ability) to process the factors and then calculate the future data according to the predicted factors.
% 看autoformer的related work
%lstm或cnn为了减少参数量，加入了很多假设，减弱了复杂时间建模能力，本方法将学习复杂时序数据用多个简单时序数据表达，从而可以利用简化模型进行处理
The structures of our methods can be regarded as deep latent dynamics models \cite{DBLP:conf/icml/HafnerLFVHLD19,DBLP:conf/icml/BeckerPGZTN19,DBLP:conf/nips/SaxenaBH21} by considering the original time series as the observation and the prediction of each factorized signal component as the decision-making process.
From this perspective, the problem of accumulating errors could be alleviated, so that the accuracy of the predictability estimation in the long term could be improved.
There are methods \cite{DBLP:conf/aistats/KhemakhemKMH20,DBLP:conf/nips/HalvaCLSZGH21} studying the non-linear independent component analysis (ICA) to disentangle the latent factors of dynamic models. 
The disentanglement can be effectively learned by these models thanks to their theoretical guarantees of the models' identifiability.
Nevertheless, these methods mainly focus on designing generalized models without being constrained by too many restrictive assumptions. 
Differently, our method focuses on using the disentanglement of the factors to model reasonable and sparse relations for more efficient approximation.



% The relations of different assets in the whole market are also important and should be considered in the prediction of each time series.
There are methods using GNN \cite{DBLP:conf/cikm/ChenWH18,DBLP:journals/corr/abs-1908-07999,DBLP:conf/kdd/WuPL0CZ20,DBLP:conf/iclr/ChenSCG22} or MRF \cite{DBLP:conf/kdd/LiST19} to model the spatiotemporal relations between different variables of time series to improve the prediction performance.
However, reasoning and propagating information through  global relations would cause computational insufficient and introduce redundant information when the data scale and dimension get larger.
We propose to use variational inference to draw the distribution of hidden factors of time series and learn the relations between these factors to generate and predict the time series.
Duan \emph{et al.}~\cite{duan2022factorvae} leveraged VAE to learn the multi-dynamic-factor model \cite{ng1992multi} for predicting cross-sectional stock return, which means they ignored the temporal variation of the factors.
Differently, our method takes the temporal relations into account for more comprehensive modeling and adapts the classic theory to be more powerful with fewer constraints by means of modern deep learning techniques.


There are methods using extra knowledge of the market to improve the performance of times series forecasting, such as the event-driven approaches \cite{DBLP:conf/acl/CohenX18,DBLP:journals/tkde/ShiTWZB19,DBLP:journals/pr/ChengYXL22} and the method exploiting additional enterprise information \cite{DBLP:journals/corr/abs-2110-13716} for stock trend forecasting.
Although these methods utilize most information from the environment, our method has the advantage of discovering complementary factors by analyzing the data along the temporal dimension to further boost the prediction performance, as evidenced by our experiments.


\subsection{Decomposition of Time Series}

Decomposing non-stationary observed time series into simple and predictable components is quite an intuitive way for time series analysis, and can be traced back to the blind source separation \cite{jutten1991blind}.
Empirical mode decomposition methods \cite{huang1998empirical,huang2014hilbert,DBLP:journals/tsp/DragomiretskiyZ14} iteratively decompose time series into stationary oscillating components, known as intrinsic mode functions (IMFs).
The IMFs are simple and nearly orthogonal signals and are defined to have the similar number of extrema and zero crossings as well as symmetric envelopes to guarantee the conduct of the Hilbert transform on them.
With the help of deep learning techniques, our method generalizes the data processing from the frequency domain to the concept domain by preserving the instincts of the simplicity and orthogonality of the intrinsic modes that are in accordance with our instincts.


% in the concept domain where the data can be processed in a more general way.

% womende fangfa gengxiang shi jianmo latent dynamics 
Researchers began constructing deep networks for time series decomposition in the past few years.
Several approaches \cite{asadi2020spatio,DBLP:conf/iclr/OreshkinCCB20,DBLP:conf/nips/WuXWL21} explicitly decompose time series into trend and seasonality and predict future signals according to the characteristics of different components.
Sen \emph{et al.}~\cite{DBLP:conf/nips/SenYD19} use temporal convolution networks to conduct matrix factorization for prediction time series.
Wang \emph{et al.}~\cite{wangmicn} use isometric and multi-scaled convolutions to capture local-global interactions in time series data.
The methods above mainly focus on leveraging the existing classic signal processing theories to increase the interpretability of their models, while our method takes full use of the strong ability of information abstraction of deep learning to extract more intrinsic representations as predictable latent factors for time series forecasting.
The neural basis expansion analysis approaches, N-BEATS \cite{DBLP:conf/iclr/OreshkinCCB20} and N-HITS \cite{DBLP:journals/corr/abs-2201-12886}, use residual mechanism to hierarchically decompose the prediction into interpretable outputs besides the seasonality-trend decomposition.
Unlike gradually decomposing the original signal, our method infers intrinsic and predictable latent factors from the input data with the guidance of semantics, which is more flexible for relation modeling.









% $\bm{x}(T+1)$

% $\bm{x}(t)|^{T}_{t=1}$


% $\bm{h}^{i}(t)|^{K,T}_{i=1,t=1}$


% $\bm{h}^{i}(T+1)|^{K}_{i=1}$


% $y(T+1)$



\section{Predictable Latent Factor Inference Modeling}
\label{sec:plfi}

% \subsection{Overview}

\begin{figure*}[tbp]
\centerline{\includegraphics[width=1.85\columnwidth]{fig/framework.pdf}}
\caption{The graphical model for learning predictable latent factors. We present a model with 2 signal components for illustration here.}
\label{fig:framework}
\end{figure*}


We aim to discover predictable latent factors of time series data for efficient forecasting.
To achieve this, we first model the relationships among the latent factors and the observed time series, and then learn the latent factors from the time series based on the modeled relationships.
Specially, we assume that all the latent factors can be divided into several independent signal components, and that factors in each component are conditionally independent for computational efficiency.
%Since the inferred latent factors drive the trends of the times series, the original low-dimensional time series data can be analyzed in a latent semantic domain. 
Since the inferred latent factors drive the trends of the times series, the original time series data can be analyzed in a latent semantic domain. 
To effectively learn the latent factors for feasible and effective prediction, we enforce the factors in each signal component with the characteristics of predictability, sufficiency, and identifiability, which will be elaborated in Section \ref{sec:PLFl}.

%which makes it possible to learn latent factors from original final time series.
%Specially, the latent factors constitute several independent signal component to simplify the relations of the factors for computational efficiency.

Figure~\ref{fig:framework} shows the graphical model of the proposed predictable factor learning method.
% Just take the task of predicting stock price return as an example to elaborate our factor inferring model, and we will show applications of forecasting on other tasks in the experiment section. 
Let $\bm{X} = [\bm{x}(1),\dots,\bm{x}(T)]\in \textbf{R}^{D\times T}$ be the matrix of input time series of past $T$ time steps with $D$ representing the number of features, and $\bm{H} = [\bm{h}(1),\dots,\bm{h}(T)]\in \textbf{R}^{L \times K \times T}$ are the factorized $K$ signal components each of which is composed of $L$ predictable latent factors.
The observed time series $\bm{X}$, e.g., the price signal of stocks, are usually non-stationary time series that cannot be predicted, and we assume that $\bm{X}$ depends on the hidden predictable $\bm{H}$ which can be inferred from $\bm{X}$ to accomplish time forecasting tasks.
With the predicted $\bm{h}(T+1)$, we can make a specific prediction $\bm{y}(T+1)$ according to different kinds of time series forecasting.
% As shown in the figure, both of the $\bm{X}(T+1)$ and the final prediction $y(T+1)$ are able to be reconstructed by the predicted $\bm{h}(T+1)$, while $y(T+1)$ is often paid more attentions than the price itself in real investment practice.
%Furthermore, since latent factors affects the trends of different stocks in varying degrees, 



%a probability distribution of each factor is expected, so a variational auto-encoder (VAE) based model is proposed to extract the potential factors from the original time series.

%We believe that the price time series can be factorized into several predictable latent  factors, and these latent factors drive the tendency of the observed time series to some certain extent.
%Since these factors unequally impact different stocks in varying degrees, a probability distribution of each factor is expected, so a variational auto-encoder (VAE) based model is proposed to extract the potential factors from the original time series.



%Furthermore, the financial time series natively has the hierarchical structure, \emph{e.g.}, candle charts of one day, one week, or one month, so potential signals are supposed to aim at various of frequencies in our network.
%Since time series with low frequency can be calculated by using ones with high frequency, the network is able to work well only given the price time series with low frequency.

\subsection{Multi-Scale Convolutional Encoder}
Computational efficiency is among the main challenges in time series analysis. To improve the computational efficiency, we thereby make several independent assumptions to encourage the sparsity of the relations between the inferred factors.
The first independent assumption is that factors representing different time-scale semantics are merely related.
Taking the financial time series data as an example, the market sentiments and the business scopes are both causal factors of the stock prices while they are independent.
Market sentiments might have weekly seasonality, e.g., hesitant on Mondays and active on Wednesdays.
And the profits of companies with specific business scopes usually have yearly seasonality, e.g., relatively high in summer for a company selling air-conditioners.
More supportive statements are claimed by \cite{muller1993fractals}.
The second assumption we make is that factors in the same time scale are conditionally independent.
This assumption leverages temporal information to clearly describe the directed relations between latent factors.  
If two factors are related to each other, the factor at the previous time step determines the current time step, and these two factors are independent given the observations at the current time step.


With the two independence assumptions above, we now discuss the proposed multi-scale convolutional encoding network, which can discover various predictable latent factors and directly divide the factors into different independent signal components according to the time scales.
Concretely, we sample the observed time series data in several specific time scales corresponding to different semantics, such as ``day'' and ``week''.
 % In order to discover various predictable latent factors capturing long-term and short-term dependencies, the time series are sampled at multiple scales.
% We directly divide the factors into different independent signal components according to the sampling scales, given the prior knowledge that long-term and short-term financial behaviors are merely related \cite{muller1993fractals}.
% involved in the input time series, the time series are sampled at multiple frequencies.
Let $\mathcal{R}=\{r_{1},\dots,r_{K}\}$ be the sets of $K$ sampling rates, the sampled time series are $s(\bm{X}, \mathcal{R})=\{s(\bm{X},r_{i})|i=1,\dots,K\}$, where $s(\bm{X},r)$ describes the sampling operation on $\bm{X}$ with sampling rate of $r$. 
The proposed encoder $\phi$ learns latent factors at each time scale individually, and the factors are combined during the decoding procedure.
In particular, the $\phi$ is composed of $K$ sub-encoders $\phi = \{\phi^{(i)}\}|_{i=1}^{K}$, and these sub-encoders are used to learn the distributions of latent factors:
\begin{equation}
\begin{aligned}
\label{eq:encoder}
  &[\bm{\mu}^{i},\bm{\sigma}^{i}] = \phi^{(i)}(\bm{X}^{(i)}), \ \bm{X}^{(i)} = s(\bm{X},r_i),\\
  & \bm{h}^{i}_j(t) \sim \mathcal{N}(\bm{\mu}^{i}_j(t),\bm{\sigma}^{i}_j(t)), \ j = 1,2,\cdots,L,
\end{aligned}
\end{equation}
where $\bm{h}^{i}_j(t)$ represents the $j$-th latent factor learned by the input time series with a sampling rate of $r_i$ (i.e., the $j$-th latent factor in the $i$-the signal component) at time step $t$.
We use a dilation convolution operation to directly implement the function composition $\phi^{(i)} \circ s$ with appropriate padding to hold the same size between $\bm{X}^{(i)}$ and $\bm{X}$. 
% The $s(\cdot,\cdot)$ is implemented via a dilation convolution with appropriate padding to hold same size between $\bm{X}^{(i)}$ and $\bm{X}$. 
The $\bm{\mu}^{i}_j(t)$ and $\bm{\sigma}^{i}_j(t)$ represent the $j$-th element of $\bm{\mu}^{i} \in \textbf{R}^{L\times T}$ and $\bm{\sigma}^{i} \in \textbf{R}^{L\times T}$ at time step $t$, respectively.
So 
$\mathcal{N}(\bm{\mu}^{i}_j(t),\bm{\sigma}^{i}_j(t))$ in Eq.(\ref{eq:encoder}) shows that the learned posterior distribution of the latent factor is a $1$-d Gaussian distribution with a mean value of $\bm{\mu}^{i}_j(t)$ and standard deviation of $\bm{\sigma}^{i}_j(t)$.

\subsection{Co-Attention-Weighted Decoder}

After encoding the historical observations into $K$ signal components, we present a co-attention-weighted decoder that includes two sets of sub-decoders with the same weight coefficients for interpolating the sub-decoders of each set.
The two sets aim to reconstruct the observation $\bm{X}$ and the prediction $\bm{Y}$, respectively.
The weights are set to be shared because both of them represent the importance of the signal components contributing to the formation of the observed data.


First, a linear transformation $f^{(i)}_{d}$ is used to reconstruct $\bm{\hat{X}}^{(i)}$ for each time scale separately, i.e., $\bm{\hat{X}}^{(i)} = f^{(i)}_{d}(\bm{h}^{i})$.
We calculate the weights of the sub-decoders according to the reconstruction of the original historical data rather than the predictions to avoid unnecessary error accumulation.
An attention mechanism is used to combine the reconstructed signals at different time scales, which is formulated as:
\begin{equation}
\begin{aligned}
\label{eq:decoder}
  & \alpha_{i} = \frac{\exp(-\|\bm{X}-\bm{\hat{X}}^{(i)}\|_{F}^{1/2})}{\sum_{i=1}^{K}\exp(-{\|\bm{X}-\bm{\hat{X}}^{(i)}\|_{F}^{1/2}})}, \\
  & \bm{\hat{X}} = \sum_{i=1}^{K}\alpha_i \bm{\hat{X}}^{(i)},
\end{aligned}
\end{equation}
where $\alpha_i$ is the attention-based weight coefficient, and $\|\cdot\|_{F}$ is the Frobenious norm of a matrix.
Gathering the linear transformation in each signal component, the decoder set for reconstructing the observation is described as $\psi_d=\{f^{(i)}_d\}|_{i=1}^{K}$, and we further have $\bm{\hat{X}} = \psi_d(\bm{H})$.

Second, for each signal component, an RNN-based sequence model $g^{(i)}$ is introduced to derive the next hidden state $\bm{\hat{h}}^{i}(t+1) = g^{(i)}(\bm{h}^{i}(j) |_{j=t-\epsilon+1}^{t})$ given the historical time series before time step $t+1$, where $\epsilon \in \mathbb{N}^{+}$ denotes the length of time steps needed to predict the former latent factors.
By using $\bm{\hat{h}}^{i}(t+1)$ and the shared weight coefficient $\alpha_i$, the final task-specified prediction can be calculated as
\begin{equation}
\begin{aligned}
\label{eq:predictor}
  \bm{\hat{Y}}(t+1) = \sum_{i=1}^{K}\alpha_i f^{(i)}_{y}(\bm{\hat{h}}^{i}(t+1)),
\end{aligned}
\end{equation}
where $f^{(i)}_{y}$ is designed as an arbitrary neural network for the downstream prediction task.
The decoder set for reconstructing the final prediction is thus formulated as $\psi_f = \{f^{(i)}_y\}|_{i=1}^{K}$, and we further have $\bm{\hat{Y}} = \psi_f(\bm{\hat{H}})$.

\section{Model Learning}

\label{sec:PLFl}

The proposed model encodes $\bm{X}$ into probability distributions of predictable latent factors $\bm{H}$, and $\bm{H}$ is used to generate the predictions of downstream tasks.
The encoded factors are encouraged to have the following three characteristics:

\begin{itemize}
    \item{Predictability.} As our presumption, there exist predictable latent factors underneath the unpredictable non-stationary time series. % The predictability of the latent factors is implemented by a sequence-based model.
    \item{Sufficiency.} The sufficiency means the encoded factors contain enough information for the reconstruction and prediction.
    \item{Identifiability.} An identifiable model guarantees the correctness of drawing the joint distribution of the observations and latent signals, where the latent factors determine the trend of the observed time series.
    % An identifiable model can indeed generate latent factors which determine the trend of the observed time series by learning the correct joint distribution of observed and hidden signals. The identifiability is achieved by imposing the conditionally independent constraint on the encoded factors according to \cite{DBLP:conf/aistats/KhemakhemKMH20}.
\end{itemize}

\subsection{Learning Objective}
To achieve this goal, we propose a learning strategy that jointly maximizes the marginal probability of the proposed graphical model via variational inference and minimizes the task-specific objective.
Beforehand, an extra observed variable $\bm{E}$ is introduced to hold the identifiability of the whole model and disentangle the learned latent factors.
We define $\bm{E} = [\bm{e}_{1},\cdots,\bm{e}_{T}] \in \bm{R}^{(D+1)\times T}$ with $\bm{e}_{t}$ describing the joint representation of the time index $t$ and the corresponding observed signal $\bm{x}_{t}$.
With the help of $\bm{E}$, the prior distribution $p(\bm{H}|\bm{E})$ is assumed to be conditionally factorial, where all the latent factors $\bm{h}_j$ are independent each other.
In our model, all of the $\{\phi^{(i)}\}|_{i=1}^{K}$ jointly model the learned posterior distribution $q_\phi(\bm{H}|\bm{X},\bm{E})$ with $\phi$ as a parameter, which is used as a variation approximation to the true posterior distribution $p_\psi(\bm{H}|\bm{X},\bm{E})$ calculated by taking a marginal distribution of the joint probability $p_\psi(\bm{H},\bm{X}|\bm{E})$.
$\psi$ represents the total parameters of the decoder $\psi_d$ and the parameters of prior distributions $\psi_p$, so the joint distribution is described as:
\begin{equation}
\begin{aligned}
\label{eq:joint_distr}
  p_\psi(\bm{H},\bm{X}|\bm{E}) = p_{\psi_d}(\bm{X}|\bm{H}) p_{\psi_p}(\bm{H}|\bm{E}).
\end{aligned}
\end{equation}

Our model optimizes the parameters by maximizing the log-likelihood function:
\begin{equation}
\begin{aligned}
\label{eq:likelihood_abs}
  \max  &\sum_{(\bm{X},\bm{Y},\bm{E})\in \mathcal{D}}  \log p(\bm{X}|\bm{E})+\lambda \log p(\bm{Y}|\bm{X}),
  %\ \ \ \Leftrightarrow \\
\end{aligned}
\end{equation}
where $\mathcal{D}$ is the training set, and $\bm{Y}$ varies with the prediction task, \emph{i.e.}, the future observed data in the long-term series forecasting task, and the price change rate in the stock trend forecasting task.
The two terms in Eq.(\ref{eq:likelihood_abs}) focus on different aspects: $\log p(\bm{X}|\bm{E})$ aims to reconstruct $\bm{X}$ so that an informative representation $\bm{H}$ can be learned, and $\log p(\bm{Y}|\bm{X})$ serves for predications to encourage $\bm{H}$ to be predictable and discriminative, and $\lambda$ is a hyper-parameter to tune the relative importance of the two terms.
In this way, our method gains the advantages of both the generative and discriminative models to learn good representations for the specific forecasting task.

Eq.(\ref{eq:likelihood_abs}) is equivalent to
\begin{equation}
\begin{aligned}
\label{eq:likelihood-x}
  \max_{\phi,\psi,\upsilon,\psi_f} & \sum_{(\bm{X},\bm{Y},\bm{E})\in \mathcal{D}}  \mathbb{E}_{q_{\phi}(\bm{H}|\bm{X},\bm{E})} \Big[ \lambda \log  p_{\upsilon, \psi_f}(\bm{Y}|\bm{H}) + \\
  & \log p_{\psi}(\bm{H},\bm{X}|\bm{E})- \log q_{\phi}(\bm{H}|\bm{X},\bm{E}) \Big],
\end{aligned}
\end{equation}
where $\upsilon = \{g^{(i)}\}|_{i=1}^K$ denotes the total parameters of the predictive sequence models for hidden states, and $\upsilon$ together with $\psi_f$ form the task-specific prediction networks. 
The deduction from Eq.(\ref{eq:likelihood_abs}) to Eq.(\ref{eq:likelihood-x}) is elaborated in the Appendix.

Under the assumption of conditionally factorial, we show that Eq.(\ref{eq:likelihood-x}) can further be expanded as
\begin{equation}
\begin{aligned}
\label{eq:likelihood}
  \max_{\phi,\psi,\upsilon,\psi_f}   &\sum_{(\bm{X},\bm{E})\in \mathcal{D}} \big[\lambda \mathbb{E}_{q_{\phi}(\bm{H}|\bm{X},\bm{E})}\log  p_{\upsilon,\psi_f}(\bm{Y}|\bm{H})\\
  &+\sum_{i=1}^{K} \mathbb{E}_{q_{\phi^{(i)}}(\bm{h}^{i}|\bm{X}^{(i)},\bm{E})} \big( \log   p_{\psi^{(i)}}(\bm{h}^{i},\bm{X}^{(i)}|\bm{E})\\
  &- \log q_{\phi^{(i)}}(\bm{h}^{i}|\bm{X}^{(i)},\bm{E}) \big)\big].
  % +\log p_{\psi^{(i)}}(\bm{X}^{(i)}|\bm{\hat{h}}^{i})
\end{aligned}
\end{equation}
We provide details of the transformation procedure of Eq.~(\ref{eq:likelihood-x}) and Eq.~(\ref{eq:likelihood}) in the Appendix.
% Eq.(\ref{eq:likelihood}) can further be decomposed into a reconstruction term and a KL divergence between prior and posterior distributions, and the importance of the KL divergence is gradually improved to disentangle the latent factors by tuning hyper-parameters during the training procedure of our model.
In the following, we give further explanation of Eq.~(\ref{eq:likelihood}) with respect to the three characteristics.


\subsection{Predictability}
\label{sec:predic}

%In practical, the financial time series is often hardly to predict, but the latent factors learned from the original series can be predicted.
Given a 1-D sequence $\bm{s} = \{\bm{s}(1),\bm{s}(2),\cdots,\bm{s}(t),\cdots\}$ where $t \in \mathbb{N}^{+}$ is the time index and $\bm{x}(t) \in \mathbb{R}$, the predictability of $\bm{x}$ is intuitively defined as

\textit{Definition 1} For any $\tau>0$, there exist $\epsilon, \delta \in \mathbb{N}^{+}$ and a function $g: \mathbb{R}^{\epsilon} \rightarrow \mathbb{R}$ which satisfies that the set $\{\bm{s}\in \mathbb{R}^{ \epsilon}|\lim_{\bm{s}\rightarrow \bm{s}^{'}} g(\bm{s})\neq g(\bm{s}^{'}) \}$ has measure zero, and for any $t > \epsilon$, 
\begin{equation}
\begin{aligned}
\label{eq:predicability}
  |g(\bm{s}(i)|_{i=t-\epsilon+1}^{t}) - \bm{s}(t+\delta)|<\tau,
\end{aligned}
\end{equation}
where $\tau$ is a small error tolerance when the sequence $\bm{s}$ is predictable.

According to the definition, the function $g$ in Eq.~(\ref{eq:predicability}) is essential to the predictability of $\bm{x}$, but it is often difficult to find or design.
As described in Eq.~(\ref{eq:predictor}), we propose to use a sequential neural network to approximate $g$ for endowing the latent factors with predictability. 
For each signal component, an RNN-based model $g^{(i)} (i=1,2,\cdots,K)$ is introduced and $|\bm{\hat{h}}^{i}(t+1) - \bm{h}^{i}(t+1)|<\tau$ is expected where $\bm{\hat{h}}^{i}(t+1)$ is the output of the model.
% $\bm{\hat{h}}^{i}(t+1) = g^{(i)}(\bm{h}^{i}(j) |_{j=t-\epsilon+1}^{t})$
In general supervised learning, $\bm{h}^{i}(t+1)$ is treated as the ground-truth for training $g^{(i)}$.
However, $\bm{h}^{i}(t+1)$ is calculated by the encoder $\phi^{(i)}$, and it is undetermined before the end-to-end training, which might degenerates the leaning of $g$.
Instead, both of $\bm{h}^{i}(t+1)$ learned by $\phi^{(i)}$ and $\bm{\hat{h}}^{i}(t+1)$ predicted by $g^{(i)}$ are used to reconstruct the original historical and future signals $\bm{X}$, respectively, that are determinate and common.
We show that feasibility is warranted in the following result. %, and the detail of the theorem's proof is in Appendix A.2.

% a function $g$ satisfying the same condition as \textit{Definition 1},
\textit{Theorem 1} For any $\tau_1, \tau_2>0$, there exists $\epsilon, \delta \in \mathbb{N}^{+}$, a function $g: \mathbb{R}^{\epsilon} \rightarrow \mathbb{R}$ which satisfies that the set $\{\bm{s}\in \mathbb{R}^{\epsilon}|\lim_{\bm{s}\rightarrow \bm{s}^{'}} g(\bm{s})\neq g(\bm{s}^{'}) \}$ has measure zero, and a continuous and derivable function $l: \mathbb {R} \rightarrow \mathbb{R}$, for any $t > \epsilon$,
\begin{equation}
\begin{aligned}
\label{eq:predicability_theo}
  |l(g(\bm{s}(i)|_{i=t-\epsilon+1}^{t}))| < \tau_1,  \ |l(\bm{s}(t+\delta))|<\tau_2
\end{aligned}
\end{equation}
holds, then $\bm{s}$ must be predictable.

% We elaborate on the theorem's proof as follows:



\begin{proof}
% \textbf{Proof:} 
We rewrite $g(\bm{s}(i)|_{i=t-\epsilon+1}^{t})$ as $\hat{s}$ for simplicity.
Without loss of generality, $\hat{s}$ is assumed to be less than $\bm{s}(t+\delta)$.
Since that $l$ is continuous and derivable, there must exist $\hat{s}<\xi<\bm{s}(t+\delta)$ such that
\begin{equation}
\begin{aligned}
\label{eq:lmvt}
  l^{'}(\xi)(\bm{s}(t+\delta)-\hat{s}) = l(\bm{s}(t+\delta))-l(\hat{s}),
\end{aligned}
\end{equation}
according to the Lagrange mean value theorem.
For any $\tau > 0$, let $\tau_1=\tau_2= \tau*|l{'}(\xi)|/2$, then
\begin{equation}
\begin{aligned}
\label{eq:uneq_01}
  |l(\hat{s})| < \tau_1,  \ |l(\bm{s}(t+\delta))|<\tau_2.
\end{aligned}
\end{equation}
Accordingly,
\begin{equation}
\begin{aligned}
\label{eq:uneq_02}
  |l(\hat{s}) - l(\bm{s}(t+\delta))| &\leq |l(\hat{s})| + |l(\bm{s}(t+\delta))| \\
  &< \tau_1+\tau_2 = \tau*|l{'}(\xi)|.
\end{aligned}
\end{equation}
Substituting Eq.(\ref{eq:lmvt}) into (\ref{eq:uneq_02}), we thus have that
\begin{equation}
\begin{aligned}
\label{eq:uneq_03}
  |l^{'}(\xi)||(\bm{s}(t+\delta)-\hat{s})| < \tau*|l{'}(\xi)|,
\end{aligned}
\end{equation}
and further $|g(\bm{s}(i)|_{i=t-\epsilon+1}^{t}) - \bm{s}(t+\delta)|<\tau$.
Therefore, $\bm{s}$ is predictable.
% $\qedsymbol$
\end{proof}

% By reconstructing $\bm{X}$ with $\bm{\hat{H}}$, a reconstruction error should be minimized:
% \begin{equation}
% \begin{aligned}
% \label{eq:pred_recon}
%   \min \sum_{(\bm{X},\bm{E})\in \mathcal{D}}\sum_{i=1}^{K} -\mathbb{E}_{q_{\phi^{(i)}}(\bm{h}^{i}|\bm{X}^{(i)},\bm{E})} 
%   \log  p_{\psi_d^{(i)}}(\bm{X}^{(i)}|\bm{\hat{h}}^{i}).
% \end{aligned}
% \end{equation}


\subsection{Sufficiency}
\label{sec:suff}

The learned components should be informative enough for both the forecasting task and reconstructing $\bm{X}$, and it is implemented by learning the proposed co-attention-weighted decoder that should be designed according to specific time series forecasting tasks.
In this paper, we present two typical tasks as examples to show the prediction learning ability of our model, i.e., long-term series forecasting and stock trend forecasting.

\noindent{\textbf{Long-term series forecasting.}}
This task is to predict the future $H$-horizon data $\{\bm{x}(T+1),\bm{x}(T+2),\dots,\bm{x}(T+H)\}$ given historical data before time step $T+1$, i.e., the goal of forecasting $\bm{Y}=[\bm{x}(T+1),\bm{x}(T+2),\dots,\bm{x}(T+H)] \in \mathbb{R}^{D \times H}$.
We apply the non-dynamic decoder to our method for generating $\bm{Y}$.
To be specific, an MLP is used as $f_y^{(i)}$ to calculate all the $H$-horizon prediction of the $i$-th sampled time series given the predicted hidden state $\bm{\hat{h}}^{i}(T+1)$.
We use the MSE (Mean Squared Error) loss to guide the learning of this characteristic with respect to the reconstruction of future data (i.e., forecasting). 
In this task, the output prediction is just the future data of the original time series, which is known as the auto-regressive generation task, so this objective is equivalent to learning the characteristic of predictability.

\noindent{\textbf{Stock trend forecasting.}}
Different from the aforementioned auto-regressive generation task, stock trend forecasting aims to predict the stock price change rate given some stock-related variables (e.g., the stock price at specific times and the trading volume).
The prediction $\bm{Y}$ is defined as $\bm{Y}(T+1) = (P(T+1)-P(T))/P(T)$ where $P$ is the real price.
It is obvious that $\bm{Y}$ is defined in two consecutive time steps, but sampling for multiple signal components would break the consecutiveness of $\bm{h}^{i}$.
Fortunately, the designed function $g$ ensures $\bm{h}^{i}$ holding the predictability, so the predicted $\hat{\bm{h}}^{(i)}$ is just capable to fill the time gaps.
Suppose that the latent components $\bm{h}^{i}$ learned by using the sampled $\bm{X}^{(i)}$ are represented as $\{\cdots,\bm{h}^{i}(t_{1}), \bm{h}^{i}(t_{2}), \dots,\bm{h}^{i}(T)\}$ where $t_{k}-t_{k-1} = \eta$ is a constant, then we use $g^{(i)}$ to predict $\{\cdots,\hat{\bm{h}}^{(i)}(t_{1}+1), \hat{\bm{h}}^{(i)}(t_{2}+1), \dots,\hat{\bm{h}}^{(i)}(T+1)\}$, and combine them together to learn representations for the forecasting task, i.e., 
\begin{equation}
\begin{aligned}
\label{eq:combine}
  \{&\cdots,(\bm{h}^{i}(t_{1}),\hat{\bm{h}}^{(i)}(t_{1}+1)), (\bm{h}^{i}(t_{2}),\hat{\bm{h}}^{(i)}(t_{2}+1)), \dots,\\
  &(\bm{h}^{i}(T), \hat{\bm{h}}^{(i)}(T+1)) \}.
\end{aligned}
\end{equation}
Using representations of Eq.(\ref{eq:combine})  as inputs, another RNN-based model is used as $f_y^{(i)}$ to predict $\bm{Y}$.
As shown in Eq.(\ref{eq:predictor}), the $\alpha_{i}$ in Eq.(\ref{eq:decoder}) is used to integrate the outputs of sequence models in various signal components.
An MSE regression loss between the predicted price return and the ground-truth is calculated as the guidance of factor learning for keeping information sufficiency.

Furthermore, for both tasks, the sub-decoders composed of linear functions for reconstruction are designed as simple as possible to enforce $\bm{H}$ informative enough for reconstructing $\bm{X}$.
However, the predictability and identifiability require an injective decoder which is often approximated by a complex neural network.
The contradiction is alleviated by introducing the sum-injective theorem \cite{DBLP:conf/iclr/XuHLJ19} as shown in Sec.~\ref{sec:identifi}.

\subsection{Identifiability}
\label{sec:identifi}

The identifiability of our model is ensured by Theorem 1 of \cite{DBLP:conf/aistats/KhemakhemKMH20}.
In order to fit the theorem, the prior distribution should be elaborated.
Different from the standard normal distribution prior used in the classical VAE, the Gaussian location-scale family is chosen as the prior distribution, which is rewritten in the form of exponential family distribution as 
\begin{equation}
\begin{aligned}
\label{eq:exp_prior}
  &p_{\psi_p}(\bm{h^{(i)}_j}|\bm{E}) \\
  = &\frac{1}{\sqrt{2\pi}\sigma} \exp\big(-\frac{\mu^2}{2\sigma^2}\big) \exp\big( \begin{bmatrix} \bm{h}^{i}_j & (\bm{h}^{i}_j)^{2} \end{bmatrix}^{\top} \begin{bmatrix} \frac{\mu}{\sigma^2} \\ -\frac{1}{2\sigma^2} \end{bmatrix}\big), \\
\end{aligned}
\end{equation}
where $\mathcal{T}^{(i)}(\bm{h}^{i}_j) = (\bm{h}^{i}_j, (\bm{h}^{i}_j)^{2})$ are sufficient statistics, and the corresponding parameters $\lambda^{(i)}(\bm{E}) = (\frac{\mu}{\sigma^2}, -\frac{1}{2\sigma^2})$ and the normalizing constant $\frac{1}{\sqrt{2\pi}\sigma} \exp\big(-\frac{\mu^2}{2\sigma^2}\big)$ are dependent on $\bm{E}$.
In our model, the $\lambda^{i}$ are independently and randomly sampled according to $\bm{E}$ to achieve the aforementioned characteristic of conditionally independent of the latent factors, and $\mathcal{T}^{(i)}$ are same for different components. 
Keeping consistent with \cite{DBLP:conf/aistats/KhemakhemKMH20}, the parameters of the joint distribution are $ \psi = \{\psi_d,\psi_p\}$ where $\psi_d=\{f_c^{(i)}\}$ and $\psi_{p} = \{\mathcal{T}^{(i)}, \lambda^{(i)}\}$ when gathering the parameters of all components.
For the deep latent variable model, the $\sim_A$ identifiable is introduced as

\textit{Definition 2} Define $\sim$ as the equivalence relation as follows: 
\begin{equation}
\begin{aligned}
\label{eq:equivalence}
  (f, \mathcal{T}, \lambda) &\sim (\tilde{f},\widetilde{\mathcal{T}},\tilde{\lambda}) \Leftrightarrow \\
  \exists \bm{A}, c | \mathcal{T}(f^{-1}(\bm{x})) &= \bm{A}\widetilde{\mathcal{T}}(\tilde{f}^{-1}(\bm{x}))+\bm{c}, \forall x
\end{aligned}
\end{equation}
where $\bm{A}$ is an $LK \times LK$ matrix, $\bm{c}$ is a vector.
If $\bm{A}$ is invertible, this relation is denoted as $\sim_A$.

Using the theorem, our deep latent factor model is $\sim_A$ identifiable since four conditions are satisfied:
\begin{enumerate}
    \item The decoder $f_d$ used to reconstruct $\bm{x}$ is with zero measure error.
    \item The $f_d$ is composed of fully connected layers and non-linear attention units, and thus not injective. Fortunately, we can assert that our attention-based structure is learned to approximate an injective according to the researches \cite{DBLP:conf/iclr/XuHLJ19} and \cite{wijesinghe2021new} on the aggregation operation of graph attention networks. According to \cite{wijesinghe2021new}, the sum aggregation in Eq.(\ref{eq:decoder}) is adjusted by adding $1$ to $\alpha$ to ensure the injective.
    \item Obviously, $\mathcal{T}$ is differentiable everywhere, and $\bm{h}_{j}^{(i)}$ and $(\bm{h}^{i}_j)^{2}$ are linearly independent.
    \item There exist $LK+1$ points $\bm{E}_i|_{i=1}^{LK+1}$ such that the matrix $[\lambda(\bm{E}_2)-\lambda(\bm{E}_1),\cdots,\lambda(\bm{E}_{LK+1})-\lambda(\bm{E}_{LK})]$ is invertible, which is achieved by randomly and independently sampling $\mu$ and $\sigma$ according to $\bm{E}$.
\end{enumerate}
Furthermore, similar to \cite{DBLP:conf/aistats/KhemakhemKMH20} and \cite{DBLP:conf/nips/HalvaCLSZGH21}, the identifiability of our model makes it feasible to disentangle the intricate relations of the latent factors, which provides sparse connections in our model and thus reduces the model's complexity for more efficient learning.



% \begin{table*}[tbp]
%   \centering
%   \caption{Test results of the existing and the proposed methods on the Exchange dataset. Values marked in bold represent the best results.}
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc}
%     \toprule
% \multicolumn{1}{c|}{\multirow{2}{*}{Methods}} & \multicolumn{2}{c|}{ARIMA \cite{anderson1976time}}      & \multicolumn{2}{c|}{LogTrans \cite{DBLP:conf/nips/LiJXZCWY19}} & \multicolumn{2}{c|}{N-BEATS \cite{DBLP:conf/iclr/OreshkinCCB20}} & \multicolumn{2}{c|}{DeepAR \cite{DBLP:journals/corr/FlunkertSG17}} & \multicolumn{2}{c|}{Reformer \cite{DBLP:conf/iclr/KitaevKL20}} & \multicolumn{2}{c|}{Informer \cite{DBLP:conf/aaai/ZhouZPZLXZ21}} & \multicolumn{2}{c|}{Autoformer \cite{DBLP:conf/nips/WuXWL21}} & \multicolumn{2}{c}{Ours}        \\ \cline{2-17} 
% \multicolumn{1}{c|}{}                         & MSE            & MAE            & MSE           & MAE           & MSE           & MAE          & MSE          & MAE          & MSE           & MAE           & MSE           & MAE           & MSE            & MAE            & MSE            & MAE            \\ \midrule
% $t= 96$                                         & \textbf{0.112} & \textbf{0.245} & 0.279         & 0.441         & 0.156         & 0.299        & 0.417        & 0.515        & 1.327         & 0.944         & 0.591         & 0.615         & 0.241          & 0.387          & 0.157          & 0.322          \\
% $t = 192$                                       & 0.304          & 0.404          & 1.95          & 1.048         & 0.669         & 0.665        & 0.813        & 0.735        & 1.258         & 0.924         & 1.183         & 0.912         & 0.273          & 0.403          & \textbf{0.243} & \textbf{0.402} \\
% $t= 336$                                        & 0.736          & 0.598          & 2.438         & 1.262         & 0.611         & 0.605        & 1.331        & 0.962        & 2.179         & 1.296         & 1.367         & 0.984         & 0.508          & 0.539          & \textbf{0.368} & \textbf{0.503} \\
% $t=720$                                         & 1.871          & 0.935          & 2.01          & 1.247         & 1.111         & 0.86         & 1.894        & 1.181        & 1.28          & 0.953         & 1.872         & 1.072         & 0.991          & 0.768          & \textbf{0.687} & \textbf{0.693} \\
% \bottomrule    
%     \end{tabular}}%
%   \label{tab:main_result2}%
% \end{table*}%



\begin{table*}[tbp]
  \centering
  \caption{Comparisons on the long-term series forecasting performances of the existing and the proposed methods. Values marked in bold and with underlines represent the highest scores with and without using extra information, respectively.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ll|cccccccccccccccccc}
\toprule
\multirow{2}{*}{}                         &     & \multicolumn{2}{c}{Ours}                                                & \multicolumn{2}{c}{\makecell[c]{N-HITS\\ \cite{DBLP:journals/corr/abs-2201-12886} }}      &  
\multicolumn{2}{c}{\makecell[c]{NS Transformer\\ \cite{liunon} }}&
\multicolumn{2}{c}{\makecell[c]{FEDformer\\ \cite{DBLP:conf/icml/ZhouMWW0022} }} & 
\multicolumn{2}{c}{\makecell[c]{Autoformer\\ \cite{DBLP:conf/nips/WuXWL21} }} & \multicolumn{2}{c}{\makecell[c]{N-BEATS\\ \cite{DBLP:conf/iclr/OreshkinCCB20,DBLP:journals/corr/abs-2201-12886} }}  & \multicolumn{2}{c}{\makecell[c]{LSTNet\\ \cite{DBLP:conf/sigir/LaiCYL18} }}  & \multicolumn{2}{c}{\makecell[c]{DilRNN\\ \cite{DBLP:conf/nips/ChangZHYGTCWHH17} }}  & \multicolumn{2}{c}{\makecell[c]{ARIMA\\ \cite{anderson1976time} }} \\
                                          & H   & MSE                                & MAE                                & MSE            & MAE            & MSE              & MAE             & MSE        & MAE              & MSE            & MAE           & MSE            & MAE            & MSE          & MAE         & MSE          & MAE         & MSE          & MAE        \\ \midrule
\multirow{4}{*}{\rotatebox{90}{ETTm$_2$}} & 96  & \textbf{0.170}                     & \textbf{0.254}                     & \underline{0.176}    & \underline{0.255}    & 0.192            & 0.274           & 0.203      & 0.287            & 0.255          & 0.339         & 0.184          & 0.263          & 3.142        & 1.365       & 0.343        & 0.401       & 0.225        & 0.301      \\
                                          & 192 & \textbf{0.215}                     & \textbf{0.296}                     & \underline{0.245}    & \underline{0.305}    & 0.280            & 0.339           & 0.269      & 0.328            & 0.281          & 0.340         & 0.273          & 0.337          & 3.154        & 1.369       & 0.424        & 0.468       & 0.298        & 0.345      \\
                                          & 336 & \textbf{0.278}                     & \underline{0.354}                        & \underline{0.295}    & \textbf{0.346} & 0.334            & 0.361           & 0.325      & 0.366            & 0.339          & 0.372         & 0.309          & 0.355          & 3.160        & 1.369       & 0.632        & 1.083       & 0.370        & 0.386      \\
                                          & 720 & \textbf{0.374}                     & \underline{0.415}                        & \underline{0.401}    & \textbf{0.413} & 0.417            & \textbf{0.413}  & 0.421      & \underline{0.415}      & 0.422          & 0.419         & 0.411          & 0.425          & 3.171        & 1.368       & 0.634        & 0.594       & 0.478        & 0.445      \\ \midrule
\multirow{4}{*}{\rotatebox{90}{ECL}}      & 96  & \textbf{0.144}                     & \underline{0.249}                        & 0.147          & \underline{0.249}    & 0.169            & 0.273           & 0.183      & 0.297            & 0.201          & 0.317         & \underline{0.145}    & \textbf{0.247} & 0.680        & 0.645       & 0.233        & 0.927       & 1.220        & 0.814      \\
                                          & 192 & \underline{0.178}                        & \underline{0.278}                        & \textbf{0.167} & \textbf{0.269} & 0.182            & 0.286           & 0.195      & 0.308            & 0.222          & 0.334         & 0.180          & 0.283          & 0.725        & 0.676       & 0.265        & 0.921       & 1.264        & 0.842      \\
                                          & 336 & \textbf{0.185}                        & \textbf{0.287}                     & \underline{0.186} & \underline{0.290}    & 0.200            & 0.304           & 0.212      & 0.313            & 0.231          & 0.338         & 0.200          & 0.308          & 0.828        & 0.727       & 0.235        & 0.896       & 1.311        & 0.866      \\
                                          & 720 & \underline{0.228}                        & \underline{0.331}                        & 0.243          & 0.340          & \textbf{0.222}   & \textbf{0.321}  & 0.231      & 0.343            & 0.254          & 0.361         & 0.266          & 0.362          & 0.957        & 0.811       & 0.322        & 0.890       & 1.364        & 0.891      \\ \midrule
\multirow{4}{*}{\rotatebox{90}{Exchange}} & 96  & \multicolumn{1}{l}{\textbf{0.079}} & \textbf{0.202}                        & \underline{0.092}    & \textbf{0.202} & 0.111            & 0.237           & 0.139      & 0.276            & 0.197          & 0.323         & 0.098          & \underline{0.206}    & 1.551        & 1.058       & 0.383        & 0.450       & 0.296        & 0.214      \\
                                          & 192 & \textbf{0.164}                     & \textbf{0.301}                     & \underline{0.208}    & \underline{0.322}    & 0.219            & 0.335           & 0.256      & 0.369            & 0.300          & 0.369         & 0.225          & 0.329          & 1.477        & 1.028       & 1.123        & 0.834       & 1.056        & 0.326      \\
                                          & 336 & \textbf{0.247}                     & \textbf{0.371}                     & \underline{0.301}    & \underline{0.403}    & 0.421            & 0.476           & 0.426      & 0.464            & 0.509          & 0.524         & 0.493          & 0.482          & 1.507        & 1.031       & 1.612        & 1.051       & 2.298        & 0.467      \\
                                          & 720 & \textbf{0.636}                     & \underline{0.658}                        & \underline{0.798}    & \textbf{0.596} & 1.092            & 0.769           & 1.090      & 0.800            & 1.447          & 0.941         & 1.108          & 0.804          & 2.285        & 1.243       & 1.827        & 1.131       & 20.666       & 0.864      \\ \midrule
\multirow{4}{*}{\rotatebox{90}{TrafficL}} & 96  & \underline{0.401}                        & \textbf{0.281}                     & 0.402          & \underline{0.282}    & 0.612            & 0.338           & 0.562      & 0.349            & 0.613          & 0.388         & \textbf{0.398} & \underline{0.282}    & 1.107        & 0.685       & 0.580        & 0.308       & 1.997        & 0.924      \\
                                          & 192 & \textbf{0.408}                              & \underline{0.296}                              & {0.420}    & {0.297}    & 0.613            & 0.340           & 0.562      & 0.346            & 0.616          & 0.382         & \underline{0.409} & \textbf{0.293} & 1.157        & 0.706       & 0.739        & 0.383       & 2.044        & 0.944      \\
                                          & 336 & \textbf{0.434}                     & \textbf{0.311}                     & \underline{0.448}    & \underline{0.313}    & 0.618            & 0.328           & 0.570      & 0.323            & 0.622          & 0.337         & 0.449          & 0.318          & 1.216        & 0.730       & 0.804        & 0.419       & 2.096        & 0.960      \\
                                          & 720 & \textbf{0.536}                        & \textbf{0.349}                              & \underline{0.539} & \underline{0.353} & 0.653            & {0.355}     & 0.596      & 0.368            & 0.660          & 0.408         & 0.589          & 0.391          & 1.481        & 0.805       & 0.695        & 0.372       & 2.138        & 0.971      \\ \midrule
\multirow{4}{*}{\rotatebox{90}{Weather}}  & 96  & \textbf{0.154}                     & \textbf{0.195}                     & \underline{0.158}    & \textbf{0.195} & 0.173            & 0.223           & 0.217      & 0.296            & 0.266          & 0.336         & 0.167          & \underline{0.203}    & 0.594        & 0.587       & 0.193        & 0.245       & 0.217        & 0.258      \\
                                          & 192 & \underline{0.213}                        & \textbf{0.246}                     & \textbf{0.211} & \underline{0.247}    & 0.245            & 0.285           & 0.276      & 0.336            & 0.307          & 0.367         & 0.229          & 0.261          & 0.560        & 0.565       & 0.255        & 0.306       & 0.263        & 0.299      \\
                                          & 336 & \textbf{0.271}                     & 0.307                              & \underline{0.274}    & \textbf{0.300} & 0.321            & 0.338           & 0.339      & 0.380            & 0.359          & 0.395         & 0.287          & \underline{0.304}    & 0.597        & 0.587       & 0.329        & 0.360       & 0.330        & 0.347      \\
                                          & 720 & \textbf{0.349}                     & \underline{0.354}                        & \underline{0.351}    & \textbf{0.353} & 0.414            & 0.410           & 0.403      & 0.428            & 0.419          & 0.428         & 0.368          & 0.359          & 0.618        & 0.599       & 0.521        & 0.495       & 0.425        & 0.405      \\ \midrule
\multirow{4}{*}{\rotatebox{90}{ILI}}      & 24  & \multicolumn{1}{l}{\textbf{1.837}} & \multicolumn{1}{l}{\textbf{0.848}} & \underline{1.862}    & \underline{0.869}    & 2.294            & 0.945           & 2.203      & 0.963            & 3.483          & 1.287         & 1.879          & 0.886          & 6.026        & 1.770       & 4.538        & 1.449       & 5.554        & 1.434      \\
                                          & 36  & \textbf{1.748}                     & \underline{0.862}                        & 2.071          & 0.934          & \underline{1.825}      & \textbf{0.848}  & 2.272      & 0.976            & 3.103          & 1.148         & 2.210          & 1.018          & 5.340        & 1.668       & 3.709        & 1.273       & 6.940        & 1.676      \\
                                          & 48  & \textbf{1.654}                     & \textbf{0.839}                     & \underline{2.134}    & 0.932          & 2.010            & \underline{0.900}     & 2.209      & 0.981            & 2.669          & 1.085         & 2.440          & 1.088          & 6.080        & 1.787       & 3.436        & 1.238       & 7.192        & 1.736      \\
                                          & 60  & \multicolumn{1}{l}{\textbf{1.756}} & \textbf{0.878}                     & \underline{2.137}    & 0.968          & 2.178            & \underline{0.963}     & 2.545      & 1.061            & 2.770          & 1.125         & 2.547          & 1.057          & 5.548        & 1.720       & 3.703        & 1.272       & 6.648        & 1.656      \\ \bottomrule
\end{tabular}}
  \label{tab:main_result2}%
\end{table*}%



\section{Experiments}



\subsection{Settings}
\subsubsection{Datasets}

The proposed method \footnote{https://github.com/houjingyi-ustb/discover\_PLF} is evaluated on two typical time series forecasting tasks, i.e., long-term series forecasting and stock trend forecasting.


For long-term series forecasting, we conduct experiments on 6 public datasets:
(1) The ETTm$_2$ \cite{DBLP:conf/aaai/ZhouZPZLXZ21} dataset records a 7-dimensional feature including oil temperature and loads of an electricity transformer every 15 minutes.
(2) The Exchange \cite{DBLP:conf/sigir/LaiCYL18} dataset includes the daily exchange rates of 8 countries.
(3) The \href{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}{ECL} dataset collects the hourly electricity consumption from 321 customers.
(4) The \href{http://pems.dot.ca.gov}{TrafficL} dataset includes hourly transportation data describing the road occupancy rates collected from 862 sensors.
(5) The \href{https://www.bgc-jena.mpg.de/wetter}{Weather} dataset reports 21 meteorological indicators (i.e., air temperature, humidity, etc.) every 10 minutes from a weather station.
(6) The \href{https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html}{ILI} dataset contains records of ratios of patients who get influenza-like illness every week.
We follow the standard protocol that chronologically splits the ETTm$_2$ dataset into training, validation, and test sets by the ratio of $7:1:2$ and other datasets by the ratio of $6:2:2$.
The horizon lengths are set to $H \in \{96, 192, 336, 720\}$ for the first 5 datasets, and $H \in \{24,36,48,60\}$.
For the evaluation, we use the mean absolute error (MAE) and the mean squared error (MSE) metrics.


% univariate data in the Exchanged dataset \cite{DBLP:conf/sigir/LaiCYL18}.
% The dataset contains the daily exchange rate records of 8 countries during 1990-2016. 
% Following the standard data split protocol, we chronologically split the dataset into training, validation, and test sets by the ratio of 6:2:2. 
% Different from the stock trend forecasting, long-term financial time series forecasting is an autoregressive task the input length of the data is 96 and the prediction lengths $t$, where $t \in \{96, 192, 336, 720\}$.


%For the stock trend forecasting, we conduct experiments on CSI 100 and CSI 300 in the publicly available stock dataset, Alpha360, from the Qlib platform \cite{DBLP:journals/corr/abs-2009-11189}. %, where CSI 100 and CSI 300 contain 100 and 300 largest and most liquid stocks in the China A-share market, respectively.
For stock trend forecasting, we conduct experiments on CSI 100 and CSI 300 in the publicly available stock dataset, Alpha360 \cite{DBLP:journals/corr/abs-2009-11189}.
The input data of this task are the sequences of 6-dimensional stock prices in 60 trading days and the output data are the price returns of the next day.
% The dataset provides the 6 values, i.e., opening price, closing price, highest price, lowest price, volume weighted average price (VWAP), and trading volume of each stock per day.
For fair comparisons, we follow the experimental settings of \cite{DBLP:journals/corr/abs-2110-13716}.  
We split the data from both sets according to temporal order, where the training, validation, and test data are sampled in the range of 01/01/2007 to 12/31/2014, 01/01/2015 to 12/31/2016, and 01/01/2017 to 12/31/2020, respectively.
As for the evaluation metrics, the popular Information Coefficient (IC), Rank IC, and Precision@$N$ (P@$N$) with $N \in \{3,5,10,30\}$ for quantitative investment are employed.
%As for the evaluation metrics, the popular Information Coefficient (IC) and Rank IC for financial prediction are employed. 
%We also provide the results using Precision@$N$ (P@$N$) \cite{DBLP:journals/corr/abs-2110-13716} with $N \in \{3,5,10,30\}$. %, each of which calculates the precision of the top $N$ predictions and is more practical for real-world scenarios than other metrics. 
We calculate the mean and standard deviation of the results by repeating each experiment 10 times.

%\begin{table*}[htbp]
\begin{table*}[tbp]
  %\footnotesize
  %\small
  \centering
  \caption{Test results (and the corresponding standard deviations) of the existing and the proposed methods on CSI100 and CSI300. The ``*'' indicates using extra knowledge. Values marked in bold and with underline represent the highest scores with and without using extra information, respectively.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccccc|cccccc}
    \toprule
    \multirow{2}{*}{Methods} & \multicolumn{6}{c|}{CSI100}     & \multicolumn{6}{c}{CSI300} \\
\cmidrule{2-13}          & IC    & Rank IC  & P@3 & P@5 & P@10    & P@30   & IC    & Rank IC  & P@3 & P@5 & P@10    & P@30   \\
    \midrule
    \multirow{2}{*}{MLP}&0.071&0.067&56.53&56.17&55.49&53.55&0.082&0.079&57.21&57.10&56.75&55.56\\
    &(4.8e-3)&(5.2e-3)&(0.91)&(0.48)&(0.30)&(0.36)&(6e-4)&(3e-4)&(0.39)&(0.33)&(0.34)&(0.14)\\
    \midrule
    \multirow{2}{*}{LSTM \cite{DBLP:journals/neco/HochreiterS97}}&0.097&0.091&60.12&59.49&59.04&54.77&0.104&0.098&59.51&59.27&58.40&56.98\\
    &(2.2e-3)&(2.0e-3)&(0.52)&(0.19)&(0.15)&(0.11)&(1.5e-3)&(1.6e-3)&(0.46)&(0.34)&(0.30)&(0.11)\\
    \midrule
    \multirow{2}{*}{GRU \cite{DBLP:journals/corr/ChungGCB14}}&0.103&0.097&59.97&58.99&58.37&55.09&0.113&0.108&59.95&59.28&58.59&57.43\\
    &(1.7e-3)&(1.6e-3)&(0.63)&(0.42)&(0.29)&(0.15)&(1.0e-3)&(8e-4)&(0.62)&(0.35)&(0.40)&(0.28)\\
    \midrule
    \multirow{2}{*}{ALSTM \cite{DBLP:conf/uksim/AriyoAA14}}&0.102&0.097&\underline{60.79}&\underline{59.76}&58.13&55.00&0.115&0.109&59.51&59.33&58.92&57.47\\
    &(1.8e-3)&(1.9e-3)&(0.23)&(0.42)&(0.13)&(0.12)&(1.4e-3)&(1.4e-3)&(0.20)&(0.51)&(0.29)&(0.16)\\
    \midrule
    \multirow{2}{*}{SFM \cite{DBLP:conf/kdd/ZhangAQ17}}&0.081&0.074&57.79&56.96&55.92&53.88&0.102&0.096&59.84&58.28&57.89&56.82\\
    &(7.0e-3)&(8.0e-3)&(0.76)&(1.04)&(0.60)&(0.47)&(3.0e-3)&(2.7e-3)&(0.91)&(0.42)&(0.45)&(0.39)\\
    \midrule
    \multirow{2}{*}{GAT \cite{DBLP:conf/iclr/VelickovicCCRLB18}}&0.096&0.090&59.17&58.71&57.48&54.59&0.111&0.105&60.49&59.96&59.02&57.41\\
    &(4.5e-3)&(4.4e-3)&(0.68)&(0.52)&(0.30)&(0.24)&(1.9e-3)&(1.9e-3)&(0.39)&(0.23)&(0.14)&(0.30)\\
    \midrule
    \multirow{2}{*}{Transformer \cite{DBLP:conf/ijcai/DingWSGG20}}&0.089&0.090&59.62&59.20&57.94&54.80&0.106&0.104&60.76&60.06&59.48&57.71\\
    &(4.7e-3)&(5.1e-3)&(1.20)&(0.84)&(0.61)&(0.33)&(3.3e-3)&(2.5e-3)&(0.35)&(0.20)&(0.16)&(0.12)\\
    \midrule
    \multirow{2}{*}{TRA \cite{DBLP:conf/kdd/LinZL021}}&0.107&0.102&60.27&59.09&57.66&55.16&0.119&0.112&60.45&59.52&59.16&\underline{58.24}\\
    &(2.0e-3)&(1.8e-3)&(0.43)&(0.42)&(0.33)&(0.22)&(1.9e-3)&(1.7e-3)&(0.53)&(0.58)&(0.43)&(0.32)\\
    \midrule
    \multirow{2}{*}{FactorVAE \cite{duan2022factorvae}}&0.107&0.102&60.23&{59.51}&\underline{58.39}&55.21&0.118&0.110&61.04&60.73&59.26&57.90\\
    &(2.1e-3)&(1.5e-3)&(0.78)&(0.45)&(0.45)&(0.76)&(2.3e-3)&(3.1e-3)&(0.48)&(0.62)&(0.17)&(0.51)\\
    \midrule
    \multirow{2}{*}{HIST \cite{DBLP:journals/corr/abs-2110-13716}*}&0.120&0.115&61.87&60.82&59.38&56.04&0.131&0.126&61.60&61.08&60.51&58.79\\
    &(1.7e-3)&(1.6e-3)&(0.47)&(0.43)&(0.24)&(0.19)&(2.2e-3)&(2.2e-3)&(0.59)&(0.56)&(0.40)&(0.31)\\
    \midrule
    \multirow{2}{*}{\textbf{Ours+LR}}&\underline{0.111}&\underline{0.105}&{60.52}&59.60&58.09&\underline{55.31}&\underline{0.120}&\underline{0.113}&\underline{62.18}&\underline{61.09}&\underline{59.71}&{58.10}\\
    &(1.8e-3)&(1.5e-3)&(0.63)&(0.33)&(0.25)&(0.18)&(2.6e-3)&(2.4e-3)&(0.98)&(0.76)&(0.47)&(0.24)\\
    \midrule
    \multirow{2}{*}{\textbf{Ours+HIST}*}&\textbf{0.128}&\textbf{0.122}&\textbf{62.41}&\textbf{61.41}&\textbf{60.06}&\textbf{56.46}&\textbf{0.136}&\textbf{0.131}&\textbf{63.08}&\textbf{62.39}&\textbf{61.59}&\textbf{59.45}\\
    &(2.2e-3)&(1.9e-3)&(0.50)&(0.53)&(0.15)&(0.14)&(3.1e-3)&(3.2e-3)&(0.58)&(0.60)&(0.28)&(0.18)\\

    \bottomrule
    \end{tabular}}%
  \label{tab:main_result}%
\end{table*}%


\subsubsection{Implementation Details}


%The input stock features for prediction are extracted from the last 60 trading days.
%The output prediction is the stock price change rate of the next day, following \cite{DBLP:conf/wsdm/HuLBLL18,DBLP:conf/www/XuL000L21,DBLP:journals/corr/abs-2110-13716}.
For the encoders with different sampling rates, we use 1D convolutional operations with different dilation rates to capture the information of the local context on the temporal sequences.
The kernel size of the 1D dilated convolution operations is 3.
For the input time series of CSI 100 and CSI 300, we pad $(k-1)\times r$ dimensional zero vectors before the very first of the input features to keep the temporal length the same.
For the long-term series forecasting tasks, there is no limitation rule about the input length, so we directly clip the data and only preserve the complete calculated signal components.
The residual mechanism is applied to the convolution layers by adding a linear projection as the identity shortcut for better optimization.
The coefficient of the task-specific prediction loss is set to 1, the KL loss is $5e-1$ and others are $1$ selected from $\{1,1e1,1,5e-1,1e-1\}$ according to the corresponding performances.
We emphasize the prediction loss for long-term series forecasting to enable the model to concentrate more on the generation of the relatively hard-to-predict long-term data by multiplying the losses with $1e-3$ except for the prediction loss.
% Since we focus on learning representations of the financial time series, any regression model can be used for the prediction.



For long-term series forecasting, we follow the auto-tuning strategy of \cite{DBLP:journals/corr/abs-2201-12886}.
The learning rate is sampled between $[1e-4, 5e-4]$, and the input size is sampled from $\{k\times H|1\leq k \leq 10\}$.
Actually, from our experiments, the longer $H$ the better, but for computational sufficiency, the experimental results reported in this paper are done by limiting $H<2000$.
The  sampling rates (dilation rates) of different datasets contain 1, and others are chosen from basic time units, such as hour, day, week, and year.
The sequence model is selected from $\{$LSTMs, GRUs$\}$ for different tasks. 
The task-specific decoder is a 2-layer MLP, where the top layer is for the prediction generation and the bottom layer has 128-dimensional nodes with the ReLU activation.
Because different variables at the same time step often represent homogeneous semantics in many datasets, we directly conduct univariate prediction for this task.
Note that the state-of-the-art method N-HITS \cite{DBLP:journals/corr/abs-2201-12886} also treats the task as the univariate prediction.
However, our method is more suitable for multivariate prediction, we embed each 1-dimensional time series into $\{8,16,32\}$-dimensional sequential data using 3-kernel 1D convolution without the bias term.
To ensure a stable optimization, we conduct data normalization within each mini-batch.
Specifically, before feeding the data into the proposed network, we calculate the mean and variance of all the data in the mini-batch and normalize the data with the two statistics.
After generating the prediction via the model, we reverse the normalization of the predicted data by multiplying the variance and then adding the mean value calculated at the very beginning to obtain the final prediction.


% we replace the stock return prediction module with the Autoformer \cite{DBLP:conf/nips/WuXWL21} for forecasting the long-term future time series. 
% More specifically, we directly use the decomposed signal components as the input of different sequence based models without sharing weights and predict the long-term future signal components, separately. 
% Thus, the predicted signal components are linearly combined to generate the final prediction. 
% Here we apply the Autoformer [2] as the sequence based model to our model, and the hyperparameter settings are almost the same as [2] except that the node dimension of the layers (d_model) of the Autoformer is set to 32. 
%We conduct our experiments on a single NVIDIA Geforce RTX 3090 GPU.

% Detailed hyper-parameter settings can be found in Appendix B.1.

For stock trend forecasting, we simply apply a linear regression model to our method for end-to-end training (Ours+LR). 
We also cascade the knowledge-driven model, HIST \cite{DBLP:journals/corr/abs-2110-13716}, into our model to further improve the performance with the guidance of extra information from the market (Ours+HIST).
Considering a large sampling rate leads to too much padding and the input length is limited for fair comparisons, we set $r<10$.
After comparing the validation results, we get the optimal setting, that is, factorizing the original time series into 3 signal components with sampling rates, $r=1,2,5$, which is further analyzed in section \ref{sub:ablation}.
%Figure~\ref{fig:sample} shows an example of the sampling operation in the proposed model.
The hidden size of each latent component is set to 128.
The number of the hidden units is empirically set to be 128 according to the prior related work.
We use 1-layer GRUs as the sequence models in subsections \ref{sec:predic} and \ref{sec:suff}.
%The GRUs used in the subsections \ref{sec:predic} and \ref{sec:suff} are both 1-layer.
For the optimization of our model, the learning rate is $2e-4$.
In the experiments of this task, we further improve the model performance by applying the disentanglement trick.
Eq.~(\ref{eq:likelihood}) can further be decomposed into a reconstruction term and a KL divergence between prior and posterior distributions, and the importance of the KL divergence can be gradually improved to disentangle the latent factors by tuning hyper-parameters during the training procedure of our model.
%the losses of VAE are calculated by the mean operation instead of the general summation for  steady optimization, because the batch sizes of different back propagation are the numbers of the stocks at the corresponding time steps and are not equal.
Here, the loss of the KL divergence is multiplied by a coefficient $\beta$ \cite{DBLP:conf/iclr/HigginsMPBGBML17} in addition to $5e-1$ according to the $\beta$-VAE for disentangling the predictive latent factors.
We gradually increase the value of $\beta$ in stages, which is
\begin{equation}
\label{eq6}
\beta=\left\{
\begin{aligned}
&0.1, \ \ \ \mathrm{epoch}<20,\\
&0.5, \ \ \ 20\leq \mathrm{epoch}<30,\\
&1, \ \ \ \mathrm{epoch}\geq 30.\\
\end{aligned}
\right.
\end{equation}
We observe that the optimization converges within 50 epochs.




%\begin{table*}[htbp]
\begin{table*}[tbp]
  %\footnotesize
  %\small
  \centering
  \caption{ Test results of the ablation studies. The MP indicates the mean precision of Precision@$N$, where $N \in \{3,5,10,30\}$. The \checkmark and $\times$ represent conducting experiments with and without using the corresponding module, respectively.}
    \scalebox{0.9}{
    % \resizebox{\textwidth}{!}{
    \begin{tabular}{cccc|ccc|ccc|cc|cc|cc}
    \toprule
    %\multirow{2}{*}{Decomposition} &\multirow{2}{*}{Disentanglement} &\multirow{2}{*}{Reconstruction} & \multicolumn{3}{c|}{CSI100}     & \multicolumn{3}{c}{CSI300} \\
    \multirow{2}{*}{Decomp.} &\multirow{2}{*}{Disent.} &\multirow{2}{*}{Reconst.}&\multirow{2}{*}{Ind.} & \multicolumn{3}{c|}{CSI100}     & \multicolumn{3}{c|}{CSI300} & \multicolumn{2}{c|}{Exchange} & \multicolumn{2}{c|}{Weather}  & \multicolumn{2}{c}{ILI} \\
\cmidrule{5-16}   &  & &     & IC    & Rank IC  & MP  & IC    & Rank IC  & MP & MSE  & MAE  & MSE  & MAE  & MSE  & MAE  \\
    \midrule
    $\times$&$\times$&$\times$&$\times$&0.103&0.098&58.15&0.113&0.108&58.92&0.293&0.432&0.272&0.312&3.143&1.145\\
    \midrule
    $\times$&\checkmark&\checkmark&$\times$&0.105&0.100&58.26&0.116&0.110&58.96&0.289&0.421&0.273&0.306&2.836&1.132\\
    \midrule
    \checkmark&$\times$&\checkmark&\checkmark&0.108&0.103&58.23&0.119&0.111&60.23&0.283&0.381&0.259&0.290&2.682&1.046\\
    \midrule
    \checkmark&$\times$&$\times$&\checkmark&0.106&0.102&58.42&0.117&0.111&60.03&0.283&0.383&0.258&0.292&2.732&1.065\\
    \midrule
    \checkmark&\checkmark&\checkmark&$\times$&0.108&0.103&58.34&0.118&0.112&59.46&0.286&0.387&0.254&0.286&2.543&1.031\\
    \midrule
    \checkmark&\checkmark&\checkmark&\checkmark&0.111&0.105&58.38&0.120&0.113&60.27&0.282&0.383&0.247&0.276&1.749&0.857\\

    \bottomrule
    \end{tabular}}%
  \label{tab:ablation}%
\end{table*}%




%\subsubsection{Baselines}
%We include the follow baseline methods for stock trend forecasting:
%MLP, LSTM \cite{DBLP:journals/neco/HochreiterS97}, GRU \cite{DBLP:journals/corr/ChungGCB14}, ALSTM \cite{DBLP:conf/uksim/AriyoAA14}, SFM \cite{DBLP:conf/kdd/ZhangAQ17}, GAT \cite{DBLP:conf/iclr/VelickovicCCRLB18}, Transformer \cite{DBLP:conf/ijcai/DingWSGG20}, TRA \cite{DBLP:conf/kdd/LinZL021}, FactorVAE \cite{duan2022factorvae}, and HIST \cite{DBLP:journals/corr/abs-2110-13716}.

%As for long-term financial time series forecasting, we use the following methods as the baselines:
%ARIMA \cite{anderson1976time}, LogTrans \cite{DBLP:conf/nips/LiJXZCWY19}, 


\subsection{Main Results}





% \cite{DBLP:journals/asc/SezerGO20}, and researchers began to focus on designing specific deep models according to the characteristics of financial data.
% Zhang et al. \cite{DBLP:conf/kdd/ZhangAQ17} adapt the cell of RNN, called state frequency memory (SFM), to capture  multi-frequency trading patterns for stock price prediction.
% %Feng et al. \cite{DBLP:conf/ijcai/FengC0DSC19} introduce the adversarial learning to train an attentive LSTM that is robust to  perturbations for stock movement prediction.
% Lin et al. \cite{DBLP:conf/kdd/LinZL021} propose the temporal routing adaptor (TRA) with a memory mechanism to select the best predictor at each time step for the stock ranking task.
% The above methods focus on processing different financial time series individually, without considering the impacts of each other time series.

\noindent{\textbf{Long-term series forecasting.}}
Table~\ref{tab:main_result2} shows the comparison results of the proposed and the commonly used methods on 6 long-term series forecasting datasets. 
From the tables, our method achieves the best or the second best performances among the existing methods on all the datasets.
We can observe that the proposed method especially outperforms the state-of-the-art on ETTm$_2$, Exchange, and ILI which are relatively small-scale datasets with less than one million observation data each (the other 3 datasets contains more data than one million) and fewer variables at each time steps (7 or 8 variables).
It validates that our method can deal well with more realistic situations, i.e., not available to sufficient information.
When going deeper into the results, we can find that the MAE scores are not always better than other methods compared to the MSE scores on the first 5 datasets, where most MAE scores are lower than 0.5.
It can be caused by the reason that the method performs much better on relatively hard-to-predict samples (i.e., with larger error scores than 0.5) but slightly worse on some easy-to-predict samples (i.e., with lower error scores than 0.5).
Or another reason could be the number of easy-to-predict samples with lower error scores is much less than hard-to-predict samples with higher error scores, because it needs more accurate predictions on easy-to-predict samples to lower the scores caused by the relatively inaccurate predictions of hard-to-predict ones.
Our method is probably because of the former reason as can be observed from that it outperforms other methods on the ILI dataset with more hard-to-predict samples but the superiority is not obvious on relatively easy-to-predict datasets, i.e., ECL and Weather.  
 
% It indicates that the performances of our method have relatively large variances because of introducing uncertainty.
% But the MSE performs much better results when the values are lower than 0.5, which might illustrate that our method is more confident on less noisy data and vice versa.

% we can observe that our method outperforms almost all the methods, and especially for the longer-term prediction, our method shows better performance. 
% The traditional method, ARIMA \cite{anderson1976time}, performs the best among all the methods for predicting the data of the future 96 days. 
% It is because the data might be quite unstable and the ARIMA model is specially designed for handling the non-stationarity. 

\noindent{\textbf{Stock trend forecasting.}}
Table~\ref{tab:main_result} shows the comparison results of the proposed and the commonly used methods on CSI100 and CSI300 for stock trend forecasting.
As can be observed from the table, our method outperforms the state-of-the-art in all benchmarks.
Compared with HIST which exploits explicit and implicit enterprise information, our method (Ours+HIST) still performs better, which indicates that our method might have the ability to discover more latent factors for the time series or the relations of the factors are modeled more efficiently for calculation.
Without using extra information, our method (Our+LR) also shows competitive results.
When compared with SFM which captures multi-frequency trading patterns inside the RNN cells, our method achieves better performance, which demonstrates the superiority of processing data in the latent concept domain.
Our method performs better than FactorVAE on most of the evaluation metrics, which verifies the capability of our method on discovering more useful information from temporal variations.
% We visualize the backtest on portfolio investment based on the predictions of our method and other related methods, as can be seen in Appendix B.4.











\subsection{Ablation Studies}
\label{sub:ablation}

%\begin{table*}[htbp]
\begin{table*}[htbp]
  %\footnotesize
  %\small
  \centering
  \caption{Test results of different settings of signal components. The set marked with an underline is selected for our method.}
    %\resizebox{\textwidth}{!}{
    % \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccccccccc}
    \toprule
    \multicolumn{1}{l}{Sets of sampling rates}&\{1\}&\{1,2\}&\{1,3\}&\{1,5\}&\{1,2,3\}&\{\underline{1,2,5}\}&\{1,3,5\}&\{1,2,3,5\}&\{1,1,1\}\\
    \midrule
    IC&0.105&0.109&0.109&0.111&0.109&0.111&0.112&0.111&0.110\\
    Rank IC&0.100&0.103&0.103&0.104&0.102&0.105&0.105&0.106&0.104\\
    MP&58.26&58.32&58.27&58.41&58.32&58.38&58.34&58.36&58.31\\
    \bottomrule
    \end{tabular}%}
  \label{tab:component}%
\end{table*}%


We conduct ablation experiments to study the effectiveness of different modules in our method.
The main modules of the proposed method are as follows:


\textbf{Decomposition (Decomp.)} represents that the input signal is decomposed into multiple signal components. The model without the decomposition denotes the input signal is just mapped into a single signal component.

\textbf{Disentanglement (Disent.)} represents the mechanism of using the KL divergence term for disentangling the inferred factors. The model without disentanglement is achieved by removing the KL divergence term in the objective function Eq.(4) during training.

\textbf{Reconstruction (Recons.)} represents the objective of forcing the inferred factors to reconstruct the input signals at the next time step.  The model without the reconstruction is achieved by removing the reconstruction error, i.e., Eq. (7), during training.

\textbf{Independence (Ind.)} represents that the factors in a signal component only depend on the previous factors in the same signal component. The model without independence is implemented by concatenating all the latent factors in different signal components to predict the next factors in a signal component.

% \begin{itemize}
%     \item \textbf{Decomposition (Decomp.)} represents whether the learned factors are arranged to form multiple independent signal components.
%     \item \textbf{Disentanglement (Disent.)} represents whether we use the KL loss for disentangling the inferred factors. 
%     \item \textbf{Reconstruction (Recons.)} represents whether we force the inferred factors to reconstruct the input signals at the next time step.
%     \item \textbf{Independence (Ind.)} represents whether the prediction of factors in a signal component is independent of the factors in other components.
% \end{itemize}

Table~\ref{tab:ablation} shows the experimental results of the ablation study of our method on CSI100, CSI300, Exchange, Weather, and ILI.
For the 3 long-term series forecasting datasets, we report the average MAE and MSE scores of the 4 prediction horizons.
As can be found in the 1-st and 2-nd rows of the results, the module of decomposition is of great importance for the performance on this task.
Without this model, the method achieves little improvement, which implies that the predictable and independent assumptions play a critical role in computational efficiency.
From the 3-rd row of the results, we can observe that disentanglement learning is useful for improving performance.
It is probably because the semantics of the latent factors could be clarified for better representation with the disentangling operation.
Moreover, we also tried other disentanglement learning methods for handling the information bottleneck problem but got no improvement.
It might be because the reconstruction module helps to constrain the representation to be more informative.
This viewpoint can be verified from the 4-th row of the results, without the guidance of the reconstruction module, the learning performance will degrade.
From the 5-th row of the results, we can safely conclude that there is no need to consider other signal components for the prediction of each signal component.


%More detailed experiments of different number and settings of the decomposed components as well as applying different disentangling methods are in Appendix B.2 and B.3.

We investigate the settings of the signal components. 
We use different subsets $s\subseteq \{1,2,3,5\}$ of sampling rates for investigating the settings of the signal components on CSI100.
Table~\ref{tab:component} shows the comparison results.
We observe that models with sampling rate 5 have relatively good performance, because they explicitly present weekly signals (5 trading days).
Models with sampling rate 2 perform slightly better than those with sampling rate 3, and it might be because they can present monthly signals (20 trading days).
Factorizing signal components only using sampling rate 1 can theoretically present all the signals with different periodic trends, however, the performance degradation shows that explicit settings can improve the efficiency of the optimization.
The results indicate that the model would perform better with more diverse sampling rates, but we finally choose the setting $\{1,2,5\}$ for computational efficiency.
For the long-term series forecasting, we directly show the optimal sampling rates in Table~\ref{tab:samplingrate}.
Recall that the sampling rates are chosen from 1 and basic time units (i.e., hour, day, week, and month).
The sampling rate of 1 can be regarded as a residual operation.
For the ETTm${_2}$, Exchange and ILI datasets, all the time units are meaningful. 
Too large sampling rates are not chosen, because the sampled signals are too short to be modeled.
The hourly sampling is not used on the Weather dataset, which is rational because the changes of weather in the first minute and last minute in each hour do not contain any salient features.
Daily variants of different hours in ECL and TrafficL are redundant for prediction according to the experiments.
It indicates that the daily scale contains no extra information than the hourly and weekly scales, and even introduces noise for the computation of the model.









\begin{table}[htbp]
\centering
  \caption{Optimal sampling rates of different datasets.}
\begin{tabular}{lcc}
\toprule
Dataset  & Frequency & Sampling rates \\
\midrule
ETTm$_2$    & 15min     & \{1,4,96\}     \\
Exchange & 1day      & \{1,7,30\}     \\
ECL      & 1h        & \{1,168\}   \\
TrafficL & 1h        & \{1,168\}   \\
Weather  & 10min     & \{1,144\}      \\
ILI      & 1week     & \{1,2,4\}     \\
\bottomrule
\end{tabular}
\label{tab:samplingrate}%
\end{table}



We also conduct experiments of using different disentanglement operations on our model on CSI100, and the results are shown in Table~\ref{tab:disent}.
The hyper-parameters of the $\beta$-VAE$_{\textrm{B}}$ \cite{DBLP:journals/corr/abs-1804-03599} are set as follows.
The starting and final annealed capacities are 0 and 0.5, respectively.
%The weight of the KL loss is 1.
The hyper-parameter, i.e., the coefficient of the TC term of the Factor-VAE \cite{DBLP:conf/icml/KimM18} is set to $0.6$.
Note that the Factor-VAE \cite{DBLP:conf/icml/KimM18} here is a totally different method from the Factor-VAE in the paper.
Although the $\beta$-VAE has the problem of information bottleneck when disentangling, and the  $\beta$-VAE$_{\textrm{B}}$  and the FactorVAE aim to solve the problem, our method is still effective just using a simple disentanglement operation, i.e., increasing the value of $\beta$.
It might be because the sufficiency of our method can alleviate the bottleneck problem.
%https://github.com/YannDubs/disentangling-vae/tree/7b8285baa19d591cf34c652049884aca5d8acbca
Although FactorVAE yields the best performance, we still choose the simple strategy of increasing the $\beta$ for lower time complexity.

\begin{table*}[htbp]
  %\footnotesize
  %\small
  \centering
  \caption{Test results of different disentanglement operations. The set marked with an underline is selected for our method.}
    \resizebox{\textwidth}{!}{
    % \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccccc}
    \toprule
    \multicolumn{1}{l}{disentanglement operations}&$\beta$-VAE ($\beta$=0.1) \cite{DBLP:conf/iclr/HigginsMPBGBML17}&$\beta$-VAE ($\beta$=1) \cite{DBLP:conf/iclr/HigginsMPBGBML17}&\underline{$\beta$-VAE (increasing $\beta$) \cite{DBLP:conf/iclr/HigginsMPBGBML17}}&$\beta$-VAE$_{\textrm{B}}$ \cite{DBLP:journals/corr/abs-1804-03599}&FactorVAE \cite{DBLP:conf/icml/KimM18}\\
    \midrule
    IC&0.107&0.110&0.111&0.110&0.112\\
    Rank IC&0.102&0.104&0.105&0.105&0.106\\
    MP&58.43&58.32&58.38&58.28&58.39\\
    \bottomrule
    \end{tabular}}%
  \label{tab:disent}%
\end{table*}%


\begin{figure}[htbp]
\centerline{\includegraphics[width=1\columnwidth]{fig/app-invest-30.png}}
\caption{Cumulative return of portfolios on CSI100 when $k=30$.}
\label{fig:invest}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=1\columnwidth]{fig/app-invest-50.png}}
\caption{Cumulative return of portfolios on CSI100 when $k=50$.}
\label{fig:invest_50}
\end{figure}

\begin{figure*}[tbp]
\centerline{\includegraphics[width=2\columnwidth]{fig/sa.pdf}}
\caption{Examples of the ADF tests on the original signal and learned factors. Each column presents the visualization results of a randomly selected sample.}
\label{fig:sa}
\end{figure*}


\subsection{Investment Analysis}

In order to further evaluate the effectiveness of the proposed method, we construct stock portfolios based on models' predictions and compare the backtest performance on portfolio investment with other related methods.
Specifically, the \emph{TopK-Drop} strategy is adopted to maintain a stock portfolio size of $k$ every day.
The stocks of one day are ranked according to the corresponding predicted returns from high to low, and the top $k$ stocks are selected to be invested.
The stocks that are in the portfolio of the last day but not involved in the top $k$ stocks are sold.
When simulating the investment strategy, the initial account capital is set to $1e8$, and the positions of invested stocks are equally assigned to be $2e6$.
Furthermore, a transaction cost of $5$BP for buying stocks and $15$BP for selling stocks are assumed in consistency with HIST, and the stock suspension and price limit are also taken into account.
The backtest is conducted from 01/01/2007 to 12/31/2020 on the CSI100 set which consists $100$ largest and most liquid stocks of China A-Shares market.
The cumulative return (CR), i.e., the aggregated amount that is gained or lost by the strategy over time, is introduced to evaluate the investment performance, which is calculated by
\begin{equation}
\begin{aligned}
\label{eq:cr}
  CR = (\frac{\text{current capital}}{\text{initial capital}} - 1) \times 100\%.
\end{aligned}
\end{equation}





To verify the performances of methods under various selections of $k$, both $k=30$ and $k=50$ are used, and Figure~\ref{fig:invest} and Figure~\ref{fig:invest_50} show the CR curves over time of our method compared with other related methods when $k=30$ and $k=50$, respectively.
Our method significantly outperforms other related methods such as HIST, and Factor VAE, which is unrelated to the choice of $k$.
Even when facing the stock market crash in 2018, our method can still achieve $70\%$ excess return from 2017 to 2010 without less maximum drawdown than other methods.
The performances of the investment simulation demonstrate the effectiveness of our method on the task of financial time series forecasting.







\subsection{Statistical Analysis}


The predictability of the latent factor is encouraged in our model.
According to Eq.(\ref{eq:predicability}), stationarity is a very important necessary condition of a predictable time series, since stationarity ensures that the function $g$ is able to output similar values for similar inputs.
It's difficult to design appropriate statistics for testing predictability, but lots of hypothesis test methods for stationarity are proposed.
Here, the classical Augmented Dickey-Fuller unit root test (ADF test) is used on both the input original price signal and the learned latent factors, and Figure \ref{fig:sa} depicts the test results of 4 randomly selected samples.
The first row shows the original price paths along time (we neglect the VWAP, because the values are almost zero).
The other rows show the signal components with each color representing a representative latent factor with the topmost statistic values (SVs).
As expected, the SVs of the learned factors are commonly less than the critical value under the significance level of $1\%$, $5\%$ or $10\%$, i.e., $-3.46$, $-2.87$, $-2.57$, indicating that the latent factor series are stationary.
The ADF tests the linear reliability contained in the time series, and it is interesting to note that our factors can still refuse the null hypothesis even though the $g$ in the proposed method is implemented via GRU which is a non-linear neural network.
The probable reason is that the learned $g$ actually approximates an injective to fit the complex data for predictability, which is more strict than the stationarity.
%In addition, some price paths of the same stock always overlap, while the patterns of the component paths are diverse.
%It indicates that 


% \subsection{Investment Analysis}

% we conduct the backtest on portfolio investment based on the predictions of our method and other related methods to simulate the performance on the real-word trading.
% We apply the TopK-Drop strategy to selection of the CSI100 test set, and the results of the cumulative return are shown in Fig.? 


\section{Conclusions}


We have introduced a simple but efficient method of inferring predictable latent factors for time series forecasting.
The inferred factors can form multiple independent signal components with characteristics of predictability, sufficiency, and identifiability to guarantee that our model effectively reconstructs the future values with these components in the latent concept domain.
Thanks to the independence and predictability of the discovered signal components, our method enables sparse relation modeling of the inferred factors for long-term efficiency and the easy reconstruction of the prediction.
Experiments validate the effectiveness of the proposed method and the predictability of the inferred latent factors.

The limitation of our work is that we empirically sample data of different time scales for inferring the latent factors and the signal components with specific inductive bias.
% according to the characteristics of the financial market. 
% This might prevent our method from being applied to other time series forecasting tasks.
In the future, we plan to explore a more general method that enables the model to learn the optimal encoding structure by adding a meta-learning algorithm.

\section*{Acknowledgement}
This work was supported in part by the National Key Research and Development Program of China under grant No. 2020YFC1523200, and the Natural Science Foundation of China under grants No. 62106021 and No. U20A20225.



\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{ref}



% that's all folks
\end{document}


