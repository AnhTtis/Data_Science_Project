\section*{Limitations}

Although SUPMER performs superbly in a variety of problem scenarios, there still exist some limitations in our work: 1) We did not conduct any data filtering or cleaning operations to the meta-training data, which could potentially result in the inclusion of some biased content. 2) Our experiments are solely conducted on English tasks, and also do not involve some kinds of NLP tasks (\textit{e.g.}, language generation~\citealp{li2022pretrained}) or vision-language tasks~\citep{zhang2022magic, li2022compositional, zhang2019frame, li2021adaptive}.

To address these limitations, in the future we plan to conduct further cleansing and filtering on the current meta-training data. Besides, we intend to evaluate the few-shot performance of our framework in the multilingual setting and also broaden the scope of tasks, including retrieval~\citep{pan2023controlretriever}, language generation~\citep{li2022pretrained} and vision-language tasks~\citep{li2023finetuning, chen2023improving, li2022fine, zhang2022boss}. Furthermore, we hope our work could pave the way for future research on better leveraging parameter-efficient methods under few-shot settings. 
