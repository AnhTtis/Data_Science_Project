\section*{Appendices}
% sec A
\section{Additional Information for SUPMER}
% sec A.1
\subsection{Complete Analysis of SUPMER}
\label{app:math_analysis}
\newcommand{\og}{\overline{g}}
\newcommand{\oh}{\overline{H}}
In this section, we provide a more comprehensive and complete analysis of SUPMER. We will show that during meta-training, the optimization of soft prompt embeddings $\theta$ and the meta-gradient regularization parameters $\phi$ tends to maximize the inner product of gradients obtained from the support set after regulation and gradients from the query set.

Specifically, to update the parameters $\theta$ and $\phi$, we should evaluate their gradients at first, denoting them as $g^\theta$ and $g^\phi$. Considering the original algorithm of MAML, each task consists of a support set and a query set. And only one step of gradient descent is applied in the inner-loop optimization. To make our statement more direct, we denote the loss function based on the support set and the query set as $\mathcal{L}_0$ and $\mathcal{L}_1$. In SUPMER, ignoring the regularized loss, only $\mathcal{L}_1$ is directly utilized to optimize $\phi$, while $\theta$ is optimized in a bi-level meta-optimization paradigm. Here we define the following terms related to $\theta$ similar to \citet{reptile}: 
\begin{equation}
\small
    \begin{aligned}
        &g^{\theta}_{i} = \frac{\partial \mathcal{L}_i(\theta_i)}{\partial \theta_i}     &&\quad\text{(gradient obtained during SGD)}   \\
        &\og^{\theta}_{i} = \frac{\partial \mathcal{L}_i(\theta_0)}{\partial \theta_0}   &&\quad\text{(gradient at initial point)}      \\
        &\oh^{\theta}_{i} = \frac{\partial^2 \mathcal{L}_i(\theta_0)}{\partial \theta_0^2} &&\quad\text{(Hessian at initial point)} \\
        &\theta_1 = \theta_0 - \alpha_1 \psi_{\phi}(g^{\theta}_0)  &&\quad\text{(gradient descent in the inner-loop)}
    \end{aligned}
\end{equation}
For each definition $i \in \{0,1\}$ and $\psi_\phi(\cdot)$ is the meta-gradient regularization operation. $\theta_0$ denotes the initial soft prompt embeddings for each step, and $\theta_1$ denotes the embeddings after the inner-loop optimization. Obviously we have $g^{\theta}_{0} = \og^{\theta}_{0}$. Firstly we perform a Taylor series expansion to approximate the SGD gradients $g^{\theta}_1$ obtained from the query set as follows:
\begin{equation}
\label{eq:proof1}
\small
    \begin{aligned}
        g^{\theta}_{1} &= \frac{\partial \mathcal{L}_1(\theta_1)}{\partial \theta_1} \\
        &= \frac{\partial \mathcal{L}_1(\theta_0)}{\partial \theta_0} + \frac{\partial^2 \mathcal{L}_1(\theta_0)}{\partial \theta_0^2}(\theta_1-\theta_0) + \underbrace{O(||\theta_1 - \theta_0||^2)}_{=O(\alpha_1^2)} \\
        &= \og^{\theta}_{1} - \alpha_1 \oh^{\theta}_{1} \psi_{\phi}(\og^{\theta}_0) + O(\alpha_1^2)
    \end{aligned}
\end{equation}
Then we analysis the gradient descent operation in the inner-loop optimization based on the support set. Define $U$ as the gradient descent and we have $U(\theta_0) = \theta_0 - \alpha_1 \psi_{\phi}(\frac{\partial \mathcal{L}_0(\theta_0)}{\partial \theta_0})$. So we can get $\frac{\partial U(\theta_0)}{\partial \theta_0}$ and $\frac{\partial U(\theta_0)}{\partial \phi}$ as follows:
\begin{equation}
\label{eq:proof2}
\small
\begin{aligned}
    \frac{\partial U(\theta_0)}{\partial \theta_0} &= \frac{\partial}{\partial \theta_0} (\theta_0 - \alpha_1 \psi_{\phi}(\frac{\partial \mathcal{L}_0}{\partial \theta_0})) \\
    &= I - \alpha_1 \frac{\partial \psi_\phi(g^\theta_0)}{\partial g^\theta_0}\cdot \frac{\partial g^\theta_0}{\partial \theta_0} \\
    &= I - \alpha_1 \frac{\partial \psi_\phi(g^\theta_0)}{\partial g^\theta_0}\cdot \oh_0^\theta
\end{aligned}
\end{equation}

\begin{equation}
\label{eq:proof3}
\small
\begin{aligned}
    \frac{\partial U(\theta_0)}{\partial \phi} &= \frac{\partial}{\partial \phi} (\theta_0 - \alpha_1 \psi_{\phi}(\frac{\partial \mathcal{L}_0}{\partial \theta_0})) \\
    &= - \alpha_1 \frac{\partial \psi_\phi(g^\theta_0)}{\partial \phi}
\end{aligned}
\end{equation}
So based on Eq.~(\ref{eq:proof1}, \ref{eq:proof2}, \ref{eq:proof3}), we can finally approximate the gradients $g^\theta$ and $g^\phi$ as:
\begin{equation}
\small
\begin{aligned}
    g^{\theta} &= \frac{\partial \mathcal{L}_1(\theta_1)}{\partial \theta_0} \\
    &= \frac{\partial \mathcal{L}_1(U(\theta_0))}{\partial \theta_0} \\
    &= \frac{\partial \mathcal{L}_1}{\partial \theta_1} \cdot \frac{\partial U(\theta_0)}{\partial \theta_0} \\
    &= \og^\theta_{1} - \alpha_1 \oh^\theta_{1} \psi_{\phi}(\og^\theta_0) - \alpha_1 \og^\theta_{1}\cdot \frac{\partial \psi_\phi(\og^\theta_0)}{\partial \og^\theta_0}\cdot \oh^\theta_0 + O(\alpha_1^2) \\
    &= \og^\theta_{1} - \alpha_1\frac{\partial}{\partial \theta_0}(\og^\theta_1 \psi_\phi(\og^\theta_0)) + O(\alpha_1^2) \\
    \\
    g^{\phi} &= \frac{\partial \mathcal{L}_1(\theta_1)}{\partial \phi} \\
    &= \frac{\partial \mathcal{L}_1(U(\theta_0)}{\partial \phi} \\
    &= \frac{\partial \mathcal{L}_1}{\partial \theta_1} \cdot \frac{\partial U(\theta_0)}{\partial \phi} \\
    &= -\alpha_1 \og^\theta_{1} \frac{\partial \psi_\phi(\og^\theta_0)}{\partial \phi} + O(\alpha_1^2) \\
    &= -\alpha_1 \frac{\partial}{\partial \phi} (\og^\theta_1 \psi_\phi(\og^\theta_0)) + O(\alpha_1^2)
\end{aligned}
\end{equation}
 Thus, $ -\frac{\partial}{\partial \theta_0} (\og^\theta_1 \psi_\phi(\og^\theta_0)) $ and $-\frac{\partial}{\partial \phi} (\og^\theta_1 \psi_\phi(\og^\theta_0))$ indicate the optimization direction, which increases the inner product between gradients from the query set and gradients from the support set after transformation. To further consolidate our analysis, we also track the normalized gradient inner product in the first 5000 steps during meta-training. As shown in Figure~\ref{fig:innerproduct}, it is clear that the normalized gradient inner product gradually increases during meta-training.
 
 On this basis, since there exists distribution shift between the support set and the query set after task augmentation, our method aligns the gradient directions across different distributions, which helps enhance model generalization. In other words, the trainable parameters descend in a coordinated manner such that the input-output correspondence is as close as possible across two distributions with deviation. Besides, the meta-gradient regularization parameters $\phi$ also retain some domain-invariant information of the meta-training data in the above process. Considering that $\phi$ is fixed in downstream tasks, $\phi$ can be applied to encourage the alignment between the domain-specific gradients and avoid prompt-tuning overfitting to some domain-specific correlations.

\begin{figure}[t]
\centerline{\includegraphics[width=0.6\linewidth]{figures/inner_product.pdf}}
    \caption{Normalized gradient inner products in the first 5000 steps during meta-training.}\label{fig:innerproduct}
% \vspace{-2em}
\vspace{-1em}
\end{figure}

% sec A.2
\subsection{Constructing Anchor Meta Tasks}
\label{app:anchor_task}
Given a sentence $x$ from unlabeled corpora, we can derive semantically meaningful sentence embedding $\bm{H}=f^{enc}_{\theta}(x)$ with PLMs, e.g., T5 encoder. And we apply K-means to cluster these unlabeled sentences according to their embeddings:
\begin{equation}
\small
    \mathcal{P}, \{\bm{\mu}_c\} = \arg\min_{\{\mathcal{C}_c\},\{\bm{\mu}_c\}} \sum_{c=1}^K \sum_{\bm{H}\in \mathcal{C}_c} \lVert \bm{H} - \bm{\mu}_c \rVert^2
\end{equation}
where $\bm{\mu}_c$ indicates the learned centroid of cluster $\mathcal{C}_c$ and $\mathcal{P}$ indicates the partitions of all sentences. K-means clustering leads to more abundant formats and objectives of meta-training tasks. Based on the results of K-means, we design three formats of anchor self-supervised meta-training tasks: sentence-pair classification, multi-choice classification, and single-text classification. Here we introduce each of them in detail.

\paragraph{Sentence-pair Classification.} Sentence-pair classification takes a pair of sentences $(x_0,x_1)$ as input, and $x_0$ is the anchor sentence. We carry on next sentence prediction task and sentence similarity task in sentence-pair classification with the label list $\mathcal{Y}=[0,1,2]$. For the former one, following \citet{ppt}, we set two sentences next to each other as label 0, those from the same document but not adjacent as label 2, and those from different documents as label 1. And for sentence similarity task, we set two sentences coming from the same cluster as label 0, and those from different clusters as label 1. In this way, the prompt template and verbalizer are designed as:
\begin{equation}
\small
\begin{aligned}
    &P = \text{``}s_1 \left\langle \text{X} \right\rangle . s_2\text{''} \\
    &\mathcal{V} = \{0\rightarrow \text{yes}, 1\rightarrow \text{no}, 2\rightarrow \text{maybe}\}
\end{aligned}
\end{equation}

\paragraph{Multi-choice Classification.} Multi-choice classification takes an anchor sentence $x_0$ as the query and we should find the correct one in several answer candidates. Here we also set two different tasks. The first one aims to select the sentence next to $s_0$ and the second one aims to select the sentence which belongs to the same cluster as $s_0$. In each task we will set four candidates, and only one of them is correct. We design the prompt template and verbalizer as follows:
\begin{equation}
\small
\begin{aligned}
    &P = \text{``}s_0\text{? A.}s_1 \cdots \text{D.} s_4. \text{ Answer: }\left\langle \text{X} \right\rangle\text{''} \\
    &\mathcal{V} = \{0\rightarrow \text{A}, 1\rightarrow \text{B}, 2\rightarrow \text{C}, 3\rightarrow \text{D}\}
\end{aligned}
\end{equation}

% alg.1
\begin{algorithm}[tp]
\caption{Meta-training Process of SUPMER}\label{alg:supmer}
\begin{algorithmic}[1]
\State $p(\mathcal{T})$ : Distribution over anchor tasks
\State $f_\theta$ : PLM with soft prompt embeddings $\theta$
\State $\psi_\phi$ : Meta-gradient regularization
\State $\alpha_1, \beta_1, \beta_2$ : Learning rate 
\State $\mathbf{TA}$ : Task augmentation in Algorithm 2
\Statex 

\State $s \gets -1$
\State Randomly initialize $\theta, \phi$
\While{not done}
    \State Sample a batch of task \begin{small}$\{\tau_i\}_{i=1}^n$\end{small} from $p(\mathcal{T})$
    \State $\{\tau_i\}_{i=1}^n = \mathbf{TA}( \{\tau_i\}_{i=1}^n, p(\mathcal{T}), s)$
    \For {\textbf{all} $\tau_i = \{\mathcal{D}_{\tau_i}^s , \mathcal{D}_{\tau_i}^q \}$}
        \State Evaluate $\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta)$ with $\mathcal{D}_{\tau_i}^{s}$ 
        \State Evaluate $\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}(f_\theta)$ with $\mathcal{D}_{\tau_i}^{q}$
        \State Transform $\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta)$ via $\psi_\phi(\cdot)$
        % \State Transform $\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta)$ via $\psi_\phi(\cdot)$
        \State $s_i = \frac{\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}(f_\theta) \cdot \psi_\phi(\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta))}{\Vert \nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}(f_\theta) \Vert\cdot\Vert \psi_\phi(\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta))\Vert}$
        \State $\theta_i^{\prime} = \theta - \alpha_1 \psi_\phi(\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta))$
    \EndFor
    \State $s \gets \sum_i s_i / \sum_i 1$
    \State $\theta \gets \theta - \beta_1 \nabla_{\theta} \sum_{\tau_i} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}} (f_{\theta^{\prime}_i})$
    \State $\phi \gets \phi - \beta_2 \nabla_{\phi} \big( \sum_{\tau_i} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}} (f_{\theta^{\prime}_i}) + \mathcal{L}_{reg}\big)$
\EndWhile
\State \textbf{return} $\theta, \phi$
\end{algorithmic}
\end{algorithm}

\paragraph{Single-Sentence Classification.} Through K-means clustering, each sentence is associated with a cluster label $r_i$ in $\{0,1\}^K$ where $r_{ic}=1$ if $c=k$ and $y_{ic}=0$ if $c \neq k$. Here $k$ represents the cluster to which the sentence belongs. We simply use $r_i$ as the pseudo label for meta-training and construct 4-way classification tasks. As for the designing of the verbalizer, we transform the single-sentence classification into the format of multi-choice classification. We insert the centroid of cluster $\bm{\mu}_c$ into the template and use it to represent the corresponding cluster. So that we have:
\begin{equation}
\small
\begin{aligned}
    &P = \text{``}s_0\text{? A.}\left\langle \mu_{c_1} \right\rangle \cdots \text{D.}\left\langle \mu_{c_4} \right\rangle. \text{ Answer: }\left\langle \text{X} \right\rangle\text{''} \\
    &\mathcal{V} = \{0\rightarrow \text{A}, 1\rightarrow \text{B}, 2\rightarrow \text{C}, 3\rightarrow \text{D}\}
\end{aligned}
\end{equation}
On this basis, for each task format, we separate all data into different tasks to construct anchor meta-training tasks with good task distributions. Through K-means, sentences with similar embeddings are clustered into the same group. So in sentence-pair classification and multi-choice classification, we group samples whose anchor sentence comes from the same cluster into the same meta-training task. And in single-sentence classification, for each meta-training task, we randomly select $N$ clusters as $N$ classes and then sample $k$ sentences for each cluster to construct a $N$-way $k$-shot classification task ($N=4$). In this way, we completely construct all anchor meta-training tasks.

\subsection{Additional Loss to Train Meta-Gradient Regularization Parameters}
In the meta-training stage, we optimize the meta-gradient regularization parameters $\phi$ via Eq.~(\ref{eq:para_upt_2}), utilizing the same loss which optimizes the soft prompt embeddings. Here we introduce a regularized loss to attach some additional restrictions when updating the meta-gradient regularization parameters. Notably, a higher value of $b_k$ in Eq.~(\ref{eq:dynamic_mixup}) indicates a higher probability of a larger distribution deviation between the support set and the query set. Furthermore, in Eq.~(\ref{eq:gradient_trans}) we also tend to increase $z$ to achieve a more pronounced gradient transformation with a more noticeable distribution deviation. From this perspective, $z$ has a similar monotonicity with $b_k$, and they both range between 0 and 1. Thus we further add a regularized loss $\mathcal{L}_{reg} = \Vert z - b_k \Vert^2$ to constrain the value of $z$ and finally modify Eq.~(\ref{eq:para_upt_2}) into:
\begin{equation}
\small
\begin{aligned}
\phi \gets \phi - \beta_2\nabla_{\phi} \big( \sum_{\tau_i\sim p(\mathcal{T})} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}(f_{\theta_i^\prime})  +\lambda \mathcal{L}_{reg}\big)
\end{aligned}
\end{equation}

% sec A.3
\subsection{Pseudo-Codes of SUPMER}
\label{app:pseudo_code}
We show the pseudo-codes for the meta-training process of SUPMER in Alg.~\ref{alg:supmer}. And the process of curriculum-based task augmentation is described in Alg.~\ref{alg:ta}.

% alg.2
\begin{algorithm}[tp]
\caption{$\mathbf{TA}$ : Curriculum-based Task Augmentation} \label{alg:ta}
\begin{algorithmic}[1]
\State $\{\tau_i\}_{i=1}^n$ : A batch of anchor tasks
\State $p(\mathcal{T})$ : Distribution over anchor tasks
\State $s \in [-1,1]$ : Avg cos-sim between gradients  
\State $\alpha$, $m$ : hyper-parameters 
\Statex 
\State $s \gets (1+s)/2$
\State $b \gets (m^s-1)/(m-1)$
\For {\textbf{all} $\tau_i = \{\mathcal{D}_{\tau_i}^s , \mathcal{D}_{\tau_i}^q \}$}
    \State Sample task $\tau_j=\{\mathcal{D}_{\tau_j}^s , \mathcal{D}_{\tau_j}^q \}$ from $p(\mathcal{T})$
    \State Draw $\lambda$ from $Beta(\alpha, b\alpha)$
    \Statex \quad \textcolor{teal}{// $\mathcal{D}_{\tau_i}^q=(\bm{H}^{q}_i, \bm{Y}^{q}_i),\mathcal{D}_{\tau_j}^q=(\bm{H}^{q}_j, \bm{Y}^{q}_j)$}
    \Statex \quad \textcolor{teal}{// $\bm{H}$: the hidden representations of samples} 
    \State $\tilde{\bm{H}}_{i}^{q} = (1-\lambda) \bm{H}^{q}_i + \lambda \bm{H}^{q}_j$
    \State $\tilde{\bm{Y}}_{i}^{q} = (1-\lambda) \bm{Y}^{q}_i + \lambda \bm{Y}^{q}_j$
    \State $\mathcal{D}_{\tau_i}^q \gets (\tilde{\bm{H}}_{i}^{q}, \tilde{\bm{Y}}_{i}^{q})$
\EndFor
\State \textbf{return} $\{\tau_i\}_{i=1}^n$
\end{algorithmic}
\end{algorithm}

% table-statics
\begin{table}[t]
    \centering
    \small
    \begin{adjustbox}{width=0.48\textwidth}
    \begin{tabular}{llccc}
    \toprule
    \textbf{Dataset} & \textbf{Task} & \textbf{\#Train} & \textbf{\#Test} & \textbf{K} \\ \midrule
    SST-2 & Sentiment analysis & 6920 & 872 & 2 \\
    SST-5 & Sentiment analysis & 8544 & 2210 & 5 \\
    MR & Sentiment analysis & 8662 & 2000 & 2 \\
    CR & Sentiment analysis & 1774 & 2000 & 2 \\ \midrule
    SUBJ & Subjectivity classification & 8000 & 2000 & 2 \\ \midrule
    TREC & Question classification & 5452 & 500 & 6 \\ \midrule
    CB & Natural language inference & 250 & 56 & 3 \\
    RTE & Natural language inference & 2490 & 277 & 2 \\ \midrule
    QNLI & Question answering & 104743 & 5463 & 2 \\ \midrule
    WiC & Word sense disambiguation & 5428 & 638 & 2 \\ \midrule
    MRPC & Paraphrase detection & 3668 & 408 & 2 \\
    QQP & Paraphrase detection & 363846 & 40430 & 2 \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{\label{tab:data}Statistics of all 12 datasets for few-shot learning. K is the number of labels. We sample $N \times \text{K}$ instances from the original training set to construct the few-shot training and validation sets. And \emph{\#Test} shows the size of the test set.}
    \vspace{-2em}
\end{table}

% sec B
\section{Dataset \& Baseline Details}
\label{app:data_info}
\paragraph{Few-shot Learning.} We conduct experiments of few-shot learning on 6 different downstream English tasks with 12 datasets. Since some of the test sets of the datasets are not publicly available, following \citet{perfect}, we leverage the original validation sets of SST-2, CB, RTE, QNLI, WiC, MRPC, and QQP\footnote{\url{https://quoradata.quora.com/}}  as substitutes for the unavailable test sets. And the validation sets for few-shot learning are sampled from the original training set, ensuring no overlap with our designated test sets. Besides, we download the datasets of SST-2, SST-5, MR, CR, and SUBJ from \citet{better-learner}. And the rest of the datasets are obtained from the HuggingFace Datasets library~\citep{hugging-face-dataset}. CB, RTE, BoolQ, and Wic are from SuperGLUE Benchmark~\citep{superglue}, while QNLI, MRPC, and QQP are from GLUE Benchmark~\citep{glue} with Creative Commons license (CC BY 4.0). We give the statistics of all these datasets in Table \ref{tab:data}.  

\paragraph{Domain Generalization.} Similar to \citet{docogen}, We evaluate on the sentiment analysis task including 6 different domains: Airlines (A), Books (B), DVDs (D), Electronics (E), Kitchen appliances (K), and Restaurants (R). Each domain has totally 2,000 manually labeled data of binary categories for testing, including 1000 positive and 1000 negative. We choose A as the source domain and the other five (B, D, E, K, R) constitute the target domains. We sample 16 instances per label from the training set of the source domain for prompt tuning and then evaluate on the test sets of all 6 domains. 

\paragraph{Baselines. } We first compare with baseline methods with the same number of parameters as SUPMER. These methods utilize prompt tuning~\citep{prompt_tuning} to handle downstream tasks, with the key distinction lying in the initialization of the soft prompts. Vallina prompt tuning (\textbf{PT}~\citealp{prompt_tuning}) directly tunes the soft prompts in the downstream task, which are randomly initialized from a normal distribution. \textbf{PPT}~\citep{ppt} pre-trains soft prompts in a self-supervised way with 3 formats of pre-training tasks: sentence-pair classification, multiple-choice classification and single-text classification. \textbf{Unified-PPT}~\citep{ppt} formulate all these three formats into a unified task form. \textbf{MetaPT}~\citep{metapt} using a supervised sentiment analysis dataset Yelp5 as the meta-training data and directly leveraging MAML to initialize soft prompts. 

To further demonstrate the effectiveness of our method, we also consider baseline methods with more tunable parameters, including \textbf{Prefix-Tuning}~\citep{prefix_tuning} and \textbf{P-tuning-v2}~\citep{ptuning-v2}, which add prompts at each layer of PLM. We also compare with full-model tuning (\textbf{FT}) that fine-tunes all parameters of the PLM.

Given that FLAN-T5-XL was also designed with few-shot inference in mind, we newly compare with two baseline methods on FLAN-T5-XL: \textbf{zero-shot inference} and \textbf{few-shot inference}. For both of them, we directly employ Flan-T5-XL for downstream evaluation, coupled with carefully designed task instructions for each dataset. Furthermore, in few-shot inference, we also provide an appropriate number of few-shot examples to form a demonstration context. 


\section{Training Details}
\label{app:training_detail}
We apply the T5 base model~\citep{t5} (220M parameters) and Flan-T5-XL model~\citep{flant5} (3B parameters) as the underlying PLM, and use the HuggingFace Pytorch implementation~\citep{transformers}. We run experiments with 8 GeForce RTX 3090 24G GPUs. And the meta-training process of SUPMER takes about 140 GPU hours. Next we will describe the details of training hyper-parameters in the case of leveraging T5-base as the PLM.
% sec C.1
\input{tables/hyper}
\input{tables/ablation_detail}
\subsection{Training Hyper-parameters for Downstream Tasks}
In our experiments, we leverage full-model tuning and prompt tuning to solve downstream tasks, including few-shot learning and domain generalization. In few-shot learning, following some prior work~\citep{small_model_fewshot,perfect}, we set the maximum sequence length of each example to 256 for CR, SUBJ, CB, RTE and WiC, and 128 for other datasets. While in domain generalization, the maximum sequence length of each example is set to 256. 

We run each experiment 5 times on the random seed [10, 20, 30, 40, 50] and report the average accuracy as well as the standard deviation. For both full-model tuning and prompt tuning, We implement AdamW as the optimizer. We use a batch size of 32 and train the model for 200 epochs, meanwhile evaluating the model every 10 steps. And we report the results for hyper-parameters performing the best on the validation set for each task.

Besides, for full-model tuning, all parameters of PLM are fine-tuned without adding soft prompts. We use the learning rate of [1e-5, 2e-5, 3e-5] and choose the one obtaining the highest validation performance. Moreover, to fine-tune the Flan-T5-XL model, 
we use ZeRO~\citep{rajbhandari2020zero} stage-2 provided in DeepSpeed~\citep{rasley2020deepspeed} to reduce GPU memory usage.

For prompt tuning, we freeze all PLM parameters and only tune soft prompts composed of 100 soft tokens. As a result, the tunable parameters of prompt tuning are only 77\text{K} with T5-base and 205K with Flan-T5-XL, updating around 3000 and 15000 times fewer parameters on T5-base and Flan-T5-Xl, respectively, compared to full-model tuning. And we find that prompt tuning requires a much larger learning rate than full-model tuning. We search for the learning rate in [1e-1, 2e-1, 3e-1] and also choose the model with the best performance on the validation set. 


\input{tables/dg_mix}
% sec C.2
\subsection{Training Hyper-parameters for Prompt Initialization}
\paragraph{Pre-training for prompt initialization.} \citet{ppt} proposes two frameworks for unsupervised prompt pre-training, named PPT and Unified PPT. PPT designs three formats of unsupervised pre-training tasks (sentence-pair classification, multiple-choice classification and single-text classification), and Unified-PPT further formulate them into a unified task form. We implement PPT and Unified-PPT following the hyper-parameters provided in \citet{ppt} and reset the pre-trained language model to T5-base and Flan-T5-XL. Specifically, for both PPT and Unified-PPT, we sample 10GB of unlabeled data from OpenWebText to construct pre-training tasks for each task format. And 5\% data are split for validation. We apply the “inverse square root” learning rate scheduler with no warm-up steps and set the learning rate as 0.1. We set the batch size to 256 with the max sequence length as 512, and train soft prompts for at most 200,000 steps. We evaluate the performance on the validation set every 2,000 steps and choose prompts with the lowest validation loss.


\paragraph{Meta-training for prompt initialization.} In our SUPMER framework, we sample 10GB of unlabeled data from OpenWebText to construct self-supervised meta-training tasks. We split 5\% data to construct tasks for validation. And for each task format, we first set the number of clusters to 250. We sample 4 meta-training tasks in a batch, and train the prompt embeddings $\theta$ and the meta-gradient regularization parameters $\phi$ for at most 100,000 steps. We also evaluate the performance on the validation set every 2,000 steps, choosing $\theta$ and $\phi$ with the lowest validation loss for downstream tasks. Table \ref{tab:hyper-parameters} lists all training hyper-parameters for SUPMER. It is worth noting that for most hyper-parameters in Table \ref{tab:hyper-parameters}, we just set a default value by experience without tuning them. we tune the hyper-parameters which are also tuned in other baselines (\textit{e.g.}, learning rate), ensuring all methods have the same number of tunable hyper-parameters in our experiments.

Moreover, to illustrate the superiority of self-supervised meta-learning, we also imitate MetaPT\citep{metapt} to initialize soft prompts via supervised meta-learning. MetaPT uses a supervised sentiment analysis dataset Yelp5 as the meta-training data, which has 650,000 training samples only covering the domain of restaurants. Following \citet{metapt}, We group all labeled data into 10 clusters through K-means. And we set the inner loop learning rate to 0.08, the outer loop learning rate to 0.025 with the early stop patience as 6. Other hyper-parameters are consistent with those in SUPMER. 


% sec D
\section{Full Results of Ablation Study}
\label{app:ablation_result}

In this section, we first give detailed experimental results of the ablation study to illustrate the effect of individual components. We evaluate each ablation model over all 12 datasets of few-shot learning and all 6 domains of domain generalization, with T5-base as the underlying PLM. We run each experiment 5 times on the random seed [10, 20, 30, 40, 50] and report the average performances as well as the standard deviation. The detailed results of few-shot learning and domain generalization are shown in Table \ref{abla_table_1} and Table \ref{abla_table_2}. We can see each component is critical in our framework.

Besides, in \S \ref{sec: 4.4}, to explore the superiority of self-supervised meta-learning and the impact of integrating additional labeled data for soft prompt initialization, we conduct experiments of few-shot learning on T5-base, considering different data and methods for soft prompt initialization. We also carry out experiments of domain generation leveraging different data with various prompt initialization methods, with the results presented in Table \ref{dg_mix}. From Table \ref{fewshot_mix} and \ref{dg_mix}, it is evident that self-supervised meta-learning utilizing unlabeled data exhibits enhanced adaptability to unseen tasks in comparison to its supervised counterparts. And amalgamating both labeled and unlabeled data for the construction of meta-training tasks emerges as a more advantageous strategy. When it comes to employing both labeled and unlabeled data for prompt initialization, SUPMER continues to showcase markedly superior results in contrast to baseline methods in the realms of both few-shot learning and domain generalization.
