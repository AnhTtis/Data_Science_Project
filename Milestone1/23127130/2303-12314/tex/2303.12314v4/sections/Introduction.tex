\section{Introduction}

Recent NLP accomplishments witnessed the rapid development of pre-trained language models (PLMs) (\textit{e.g.}, BERT~\citealp{bert}; T5~\citealp{t5}; GPT3~\citealp{gpt3}). Fine-tuning, which tunes the entire PLM parameters, has achieved outstanding performances in various NLP tasks. However, as the pre-trained model scale increases, tuning the entire set of parameters would be sometimes unaffordable. More recently, prompt-based methods, which simply insert a piece of carefully designed text to the input (\textit{e.g.}, ``\textit{It was $\left\langle \text{X} \right\rangle$.}'') and predict target words (\textit{e.g.}, ``great'' or ``terrible'') at the mask position with frozen PLMs, have demonstrated remarkable effectiveness. But it has been observed that the performance of prompt-based methods is greatly affected by the design of prompts. In light of this, prompt tuning (PT~\citealp{prompt_tuning}), as a parameter-efficient tuning method, is proposed to only prepend some additional learnable tokens called soft prompts to the input text, with all PLM parameters freezing.


\begin{figure}[t]
    \includegraphics[width=\linewidth]{figures/PtAcc.pdf}
    \caption{(a) Performance of PT with different prompt initialization. (b) Performance after different training steps for vanilla PT and SUPMER.}\label{fig:ptvsft}
\vspace{-1.45em}
\end{figure}


Though prompt tuning is an efficient and effective paradigm, \citet{ppt} shows it performs much worse than fine-tuning under few-shot settings. We argue that the performance is not satisfactory mainly due to two limitations: 1) The performance of PT is highly \textbf{sensitive to the soft prompt initialization}, especially for few-shot tasks. As shown in Figure~\ref{fig:ptvsft} (a), different soft prompt initialization leads to significant performance variations. 2) Few-shot PT risks \textbf{overfitting to some spurious correlations} as soft prompts are tuned on limited training samples, thus undermining the generalizability of PLMs. As shown in Figure~\ref{fig:ptvsft} (b), the performance of few-shot vanilla PT degrades significantly in the final training steps. 


Recent research mainly focused on the first limitation, leveraging pre-training or supervised meta-learning for soft prompt initialization. A pre-trained prompt tuning method (PPT)~\citep{ppt} is proposed from the beginning, which utilizes self-supervised tasks to pre-train soft prompts and then applies them in the few-shot scenario. However, without explicitly optimizing the fast adaptation ability of the model, PPT suffers from a train-test mismatch between the pre-training data and the downstream data. So it limits generalization to unseen few-shot tasks, especially when there is a significant disparity in task domains or formats. MetaPrompting \citep{metaprompting}, as another effort, seeks assistance from model-agnostic meta-learning (MAML~\citealp{maml}) for fast adaptation in few-shot settings. 
However, in each task, MetaPrompting requires plenty of labeled data within certain classes to perform supervised meta-learning for prompt initialization, which is often inaccessible in practical few-shot scenarios. And the learned initialization can only generalize to the remaining classes of the same task in a few-shot manner, exhibiting weak task transferability. Furthermore, all these existing works ignore the second limitation, \textit{i.e.}, the propensity for few-shot prompt tuning to lead to overfitting.

 % THIS SENTENCE MAY NEED IMPROVE!

To address the shortcomings of existing works, we propose ~\textbf{SUPMER}, a \textbf{\underline{S}}elf-s\textbf{\underline{U}}pervised meta-\textbf{\underline{P}}rompt learning framework with \textbf{\underline{ME}}ta-gradient \textbf{\underline{R}}egularization. It leverages self-supervised meta-learning to universally learn an efficient soft prompt initialization, also with a meta-gradient regularization function to mitigate overfitting. This comprehensive process only \textbf{\textit{requires a one-time execution}} and enables seamless adaptation to different downstream few-shot tasks, while also facilitating faster convergence for downstream prompt tuning.

Specifically, to address the first limitation, we design a novel self-supervised meta-learning method for prompt initialization, which automatically generates a diverse set of meta-training tasks from large-scale unlabeled corpora and explicitly learns to fast adapt across these tasks. To ensure task diversity, we initially design a collection of anchor self-supervised meta-training tasks with different formats. And then a curriculum-based task augmentation method is further proposed to enrich the task distribution dynamically in terms of the current model capability.

For the second issue, we integrate a meta-gradient regularization function into meta-prompt learning. As we simulate distribution shift through task augmentation, the meta-gradient regularization parameters are jointly optimized to align gradient directions across different distributions within our proposed meta-prompt learning paradigm. Consequently, in downstream tasks, these optimized parameters can be directly utilized to transform raw gradients over few-shot samples into a domain-generalizable direction, preventing prompt tuning overfitting to some domain-specific correlations.

Overall, our contributions are mainly three-fold:

(1) We propose a novel self-supervised meta-prompt learning framework to better initialize soft prompts, where only unlabeled pre-training data are used to construct different meta-training tasks with curriculum-based task augmentation for further task enrichment.

(2) We incorporate a novel meta-gradient regularization function into our meta-prompt learning framework, which meta-learns to transform the raw gradient during few-shot learning into a domain-generalizable direction, thus preventing prompt tuning overfitting to domain-specific correlations.

(3) Comprehensive experiments on few-shot learning and domain generalization validate the superiority of our method, which even outperforms full-model tuning in few-shot learning. It also exhibits a stronger domain generalization ability.