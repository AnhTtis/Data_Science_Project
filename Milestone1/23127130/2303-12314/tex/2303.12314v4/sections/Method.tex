\section{Method}
In this section, we describe the whole framework of SUPMER (shown in Figure~\ref{fig:main}). With pre-defined preliminaries, we first introduce the way to construct anchor self-supervised meta tasks and the foundation of task augmentation to densify task distributions. Then we elaborate on the SUPMER model, including the meta-gradient regularization function. Finally, we upgrade the original task augmentation method into a curriculum-based one. Besides, we formalize all tasks in a text-to-text format following the T5 fashion~\citep{t5}.

% sec 3.1
\subsection{Preliminaries}
\label{sec: 3.1}
% prompt tuning
\paragraph{Prompt Tuning.} In prompt tuning~\citep{prompt_tuning}, given a training sample $(x_i, y_i)$  from task $\mathcal{D}_{\tau}$, we apply a prompt template $P$ converting $x_i$ into a new sequence $P(x_i)$ and then concatenate a set of soft prompts $\theta$ to the beginning of $P(x_i)$. And verbalizer $\mathcal{V}$ plays a role in mapping $y_i$ to some corresponding label tokens $\mathcal{V}(y_i)$ in the vocabulary of PLMs. So the objective of prompt tuning can be formulated as follows:
\begin{equation}
\small
\begin{aligned}
    &\arg\min_{\theta} \mathcal{L}_{\mathcal{D}_{\tau}}(\theta) \\
    &= \arg\max_{\theta}\sum_{(x_i, y_i) \in {D}_{\tau}} \log p\big(\left\langle \text{X} \right\rangle = \mathcal{V}(y_i) | [\theta; P(x_i)]; \theta \big)
\end{aligned}
\end{equation}
where $\theta$ denotes the soft prompt embedding (the only tunable parameters in prompt tuning). $\left\langle \text{X} \right\rangle$ let PLMs predict target tokens at the masked positions and $[\cdot;\cdot]$ is the concatenation operation.

% maml
\paragraph{Model-Agnostic Meta-Learning.} Assuming access to a task distribution $p(\mathcal{T})$, the goal of meta-learning is to utilize tasks $\tau_i \sim p(\mathcal{T})$, referred to as meta-training tasks or meta tasks, to train a learning procedure that generalizes to unseen tasks from the distribution. Model-Agnostic Meta-Learning (MAML)~\citep{maml} is a gradient-based bi-level optimization meta-learning method, which consists of an inner loop task-specific learning and outer loop fast adaptation across tasks.

Specifically, a task $\tau$ is composed of the support set $\mathcal{D}^s_{\tau}$ and the query set $\mathcal{D}^q_{\tau}$.
In the inner loop of MAML, a model learns to adapt to a new task $\tau_i$ using its support set in the following way:
\begin{equation}
\label{eq:inner_loop}
\small
\theta_i^\prime = \theta - \alpha_1 \nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(\theta)
\end{equation}
where $\alpha_1$ is the inner loop learning rate and $\theta$ is the model's parameters. And the optimized parameters $\theta_i^\prime$ is then evaluated on the query set of task $\tau_i$ with the loss function $\mathcal{L}_{\mathcal{D}_{\tau_i}^q}$. In the outer loop, this loss across meta-training tasks is treated as the final training loss to update $\theta$:
\begin{equation}
\label{eq:outer_loop}
\small
\theta \gets \theta - \beta_1\nabla_{\theta} \sum_{\tau_i\sim p(\mathcal{T})} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}({\theta_i^\prime})
\end{equation}
where $\beta_1$ is the outer loop learning rate.

% sec 3.2
\subsection{Constructing Anchor Meta Tasks}
\label{sec: 3.2}
Supervised datasets with a large amount of labeled data are often unavailable in many NLP tasks. While unlabeled data is more easily accessible and generally covers broader semantic concepts. So we utilize the unlabeled data from a large corpus to create anchor self-supervised meta-training tasks. 

The unlabeled data are first grouped into different clusters. We utilize PLMs to derive semantically meaningful embeddings for sentences in the corpus, and then apply unsupervised K-means to cluster these unlabeled sentences. Based on the results of K-means, we design three different formats of self-supervised meta-training tasks: sentence-pair classification, multi-choice classification, and single-sentence classification.

Specifically, \textbf{sentence-pair classification} involves predicting whether two sentences are adjacent in the same document or from the same cluster after K-means clustering. \textbf{Multi-choice classification} identifies the correct sentence among several candidates, which is either adjacent to a query sentence or from its same cluster. And \textbf{Single-sentence classification} aims to associate each sentence with its correct cluster label, as determined by K-means. On this basis, for each task format, we distribute meta-training data into different tasks to construct anchor meta-training tasks with well-balanced task distributions. We group samples with similar embeddings into the same task based on the results of K-means. And we give a more detailed description of anchor meta-training task construction in Appendix \ref{app:anchor_task}.

% Specifically, \textbf{sentence-pair classification}, which takes a pair of sentences as input, includes next sentence prediction task and sentence similarity task. The former predicts whether two sentences are adjacent or from the same document, while the latter predicts whether two sentences source from the same cluster after K-means clustering. And \textbf{multi-choice classification} takes an anchor sentence as the query alongside several candidate sentences. Among all answer candidates,  the correct sentence needs to be selected, either adjacent to the query or originating from the same cluster as the query. Finally in \textbf{single-sentence classification}, the objective is to associate each sentence with the correct cluster label, determined by K-means clustering.

% On this basis, for each task format, we separate all data into different tasks to construct anchor meta-training tasks with well-balanced task distributions. We group samples with similar embeddings into the same task based on K-means. And we give a more detailed description of anchor meta-training task construction in Appendix \ref{app:anchor_task}.

% sec 3.3
\subsection{Vanilla Task Augmentation}
\label{sec: 3.3}
With a set of anchor meta-training tasks, in this section we first introduce the vanilla task augmentation to densify the task distribution. Extending the idea of mixup~\citep{mixup}, we augment the task set through task interpolation, which linearly combines features and corresponding labels of samples from the query set in different tasks. In \S \ref{sec: 3.5} we further upgrade the vanilla task augmentation method into a curriculum-based one, which dynamically controls the task interpolation in terms of the current model capability.

% After getting a set of anchor meta-training tasks, to further improve generalization, we densify the task distribution through task augmentation. Following the idea of mixup~\citep{mixup}, we augment the task set through task interpolation, which linearly combines features and corresponding labels of samples from the query set of different tasks. 

Specifically, for a task composed of the support set and the query set, we denote the hidden representations of the query set samples in task $\tau_k$ as $\bm{H}^{q}$. Given an anchor task $\tau_i$, first we randomly select another task $\tau_j$. While retaining the support set of $\tau_i$, we reconstruct its query set by interpolating on the hidden representations $(\bm{H}^{q}_i, \bm{H}^{q}_j)$ and corresponding labels $(\bm{Y}^{q}_i, \bm{Y}^{q}_j)$ from the query sets in $\tau_i$ and $\tau_j$, which can be accomplished using mixup:
\begin{equation}
\label{eq:mixup}
\small
\begin{aligned}
    &\tilde{\bm{H}}_{i}^{q} = (1-\lambda) \bm{H}^{q}_i + \lambda \bm{H}^{q}_j,\ \tilde{\bm{Y}}_{i}^{q} = (1-\lambda) \bm{Y}^{q}_i + \lambda \bm{Y}^{q}_j
\end{aligned}
\end{equation}
where the mixing ratio $\lambda \in [0,1]$ is drawn from a Beta distribution $Beta(\alpha, \alpha)$, and $\alpha$ is a hyper-parameter. The process of task augmentation not only enriches the task distribution, but also simulates the distribution shift between the support set and the query set within one task, as we only leverage interpolation between the query sets of different anchor meta-training tasks. And in \S \ref{sec: 3.4} we will show the effect of this distribution deviation.
% and In \S \ref{sec: 3.3} we will further upgrade the task augmentation method into a curriculum-based one. 

% sec 3.4
\subsection{Meta-Prompt Learning with Meta-Gradient Regularization}
\label{sec: 3.4}
In this section we introduce the algorithm of our meta-prompt learning framework, which is a bi-level meta-learning paradigm learning a task-universal soft prompt initialization $\theta$ for efficient adaptation. And it jointly meta-learns a meta-gradient regularization function $\psi_\phi$ that transforms raw gradients into a domain-generalizable direction to prevent prompt tuning from overfitting.

% with a simple but effective meta-gradient regularization method to avoid overfitting. 

Specifically, considering that the inner loop update of MAML (\textit{i.e.}, Eq.~(\ref{eq:inner_loop})) over limited samples might overfit to some domain-specific correlations, we propose to learn a gradient regularization function $\psi_\phi(\cdot)$, making a direct transformation to the raw gradients obtained from the support set $\mathcal{D}_{\tau_i}^{s}$. The function first performs affine transformation $h(\cdot)$ (\textit{e.g.}, rotation) to modulate the raw gradients $g$, and then an update gate vector $z$ is employed to combine $g$ and $h(g)$ into the final gradients:
\begin{equation}
\label{eq:gradient_trans}
\small
\psi_{\phi}(g) = z\cdot h(g) + (1-z) \cdot g
\end{equation}
Obviously, the value of $z$ can be used to control how much the transformed gradients $h(g)$ contribute to the output of $\psi_{\phi}(g)$. We hope to determine this weight based on the input samples themselves, setting $z = \sigma(W\bm{H}+b)$, where $\bm{H}$ is the hidden representations of input samples. Formally, now we transform Eq.~(\ref{eq:inner_loop}) into:
\begin{equation}
\label{eq:new_inner_loop}
\small
\theta_i^\prime = \theta - \alpha_1 \psi_\phi(\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(\theta))
\end{equation}

After adapting the soft prompt embeddings to the support set $\mathcal{D}_{\tau_i}^{s}$, in the outer loop we optimize the prompt initialization $\theta$ based on these adapted embeddings $\theta^\prime$ via Eq.~(\ref{eq:outer_loop}). Besides, meta-gradient regularization parameters $\phi$ are also optimized using the same loss to learn a better gradient transformation, with $\beta_2$ as the learning rate:
\begin{equation}
\label{eq:para_upt_2}
\small
\begin{aligned}
\phi \gets \phi - \beta_2\nabla_{\phi}  \sum_{\tau_i\sim p(\mathcal{T})} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}({\theta_i^\prime})   
\end{aligned}
\end{equation}

Overall, the total meta-prompt learning obejective can be formulated as follows:
\begin{equation}
\label{eq:obj}
\small
\arg\min_{\theta,\phi} \sum_{\tau_i\sim p(\mathcal{T})} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}\big(\theta-\alpha_1 \psi_\phi (\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(\theta))\big)
\end{equation}

\paragraph{Downstream Prompt Tuning. } The above meta-prompt learning framework only requires a one-time execution. \textit{The optimized prompt initialization $\theta^*$ and meta-gradient regularization parameters $\phi^*$ are then universal for different downstream tasks.} During downstream prompt tuning, we fix $\phi^*$ and further adapt $\theta^*$ to testing tasks as Eq.~(\ref{eq:new_inner_loop}).

\paragraph{Analysis of SUPMER.} Here we give some analysis of how SUPMER could enhance generalizability, with more complete proof in Appendix \ref{app:math_analysis}. Given that $x=\theta-\alpha_1 \psi_\phi (\nabla_{\theta} \mathcal{L}_{\mathcal{D}^{s}}(\theta))$ and $x_0=\theta$, focusing on a single meta-training task, we can apply a first-order Taylor expansion around the point $x_0$ to reformulate Eq.~(\ref{eq:obj}) as:
\begin{equation}
\small
\begin{aligned}
\because \ &\mathcal{L}_{\mathcal{D}^{q}}(x) = \mathcal{L}_{\mathcal{D}^{q}}(x_0) + \mathcal{L}_{\mathcal{D}^{q}}^{\prime}(x_0)(x-x_0) \\
\therefore \ &\arg\min_{\theta,\phi} \mathcal{L}_{\mathcal{D}^{q}}\big(\theta-\alpha_1 \psi_\phi (\nabla_{\theta} \mathcal{L}_{\mathcal{D}^{s}}(\theta))\big) \\
= &\arg\min_{\theta,\phi} \mathcal{L}_{\mathcal{D}^{q}}(\theta) - \alpha_1 \nabla_{\theta} \mathcal{L}_{\mathcal{D}^{q}}(\theta) \cdot  \psi_\phi \big(\nabla_{\theta} \mathcal{L}_{\mathcal{D}^{s}}(\theta)\big)
\end{aligned}
\end{equation}
Based on the aforementioned discussion, we can reach the following conclusions: (1) The update of $\theta$ minimizes the expected loss on the query set. (2) The optimization of both $\theta$ and $\phi$ maximizes the inner product between the regulated gradients from the support set and the gradients from the query set. The inner product of two vectors is larger if they are in a similar direction. 
Recalling that we simulate the distribution shift between the support set and the query set, the optimization of $\theta$ and $\phi$ tries to align the gradient directions across different distributions. To improve the alignment between the domain-specific gradients, the gradient regularization parameters $\phi$ are optimized to retain some domain-invariant information of meta-training data and then can be utilized to regulate raw gradients obtained from few-shot samples into a domain-generalizable direction in downstream prompt tuning, thus avoiding overfitting to some spurious correlations.


\input{tables/fewshot_main}
%  sec 3.5
\subsection{Curriculum-based Task Augmentation}
\label{sec: 3.5}
In \S \ref{sec: 3.4} we show that SUPMER can help align the optimization direction across two distributions with deviation, which is simulated by performing task augmentation exclusively on the support sets. From Eq.~(\ref{eq:mixup}) it is evident that the mixing ratio $\lambda$ of mixup controls the extent of the distribution deviation, with a larger $\lambda$ resulting in a more noticeable deviation. However, in the previously discussed method, $\lambda$ is sampled from a fixed Beta distribution. In this section, we propose a more flexible sampling approach, which upgrades the original task augmentation method into a curriculum-based one, gradually increasing the task difficulty and achieving a more reasonable distribution shift.

The curriculum-based task augmentation dynamically adjusts the parameters of the Beta distribution, from which we sample the mixing ratio $\lambda$. Specifically, a batch of meta tasks is sampled in each training epoch. For each task, we can obtain gradients on the support set $g_i^s$ and gradients on the query set $g_i^q$, along with their cosine similarity. We leverage the average cosine similarity $s_{k-1}$ of all tasks in a batch during the last epoch to derive the mixing ratio $\lambda_k$ for the current epoch $k$:
\begin{equation}
\label{eq:dynamic_mixup}
\small
\begin{gathered}
    \lambda_k = Beta(\alpha, b_{k} \alpha) \\
    b_k = \frac{m^{\frac{1+s_{k-1}}{2}}-1}{m-1}, \\ \text{where}\ s_{k-1} = \frac{1}{\lvert \mathcal{B} \rvert}\cdot \sum_{i=1}^{\lvert \mathcal{B} \rvert} \frac{ g_i^s \cdot g_i^q }{\Vert g_i^s \Vert \cdot \Vert g_i^q \Vert}
\end{gathered}
\end{equation}
where $m$ is the curve parameter. In this way, when our model is not capable of aligning the optimization directions across different distributions at the beginning, a smaller $\lambda$ is preferable to create a smaller distribution deviation. Then $\lambda$ tends to gradually increase as the model's capability improves, resulting in a larger distribution deviation and a corresponding increase in task difficulty. 

We present the pseudo-codes of SUPMER in Appendix \ref{app:pseudo_code}.







