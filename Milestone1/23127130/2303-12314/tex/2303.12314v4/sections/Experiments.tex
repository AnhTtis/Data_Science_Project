%  experiments
\section{Experiments}

%  sec 4.1
\subsection{Experimental Setup}
\label{sec: 4.1}
We evaluate our approach in two problem settings: 1) Few-shot learning with different NLP downstream tasks; 2) domain generalization. 

\paragraph{Few-shot Learning.} We consider 6 downstream tasks with 12 datasets: 1) the sentiment analysis datasets SST-2, SST-5~\citep{sst}, MR~\citep{mr} and CR~\citep{cr}; 2) the subjectivity classification dataset SUBJ~\citep{subj}; 3) the question classification dataset TREC~\citep{trec}; 4) the natural language inference datasets CB~\citep{cb} and RTE~\citep{superglue}; 5) the question answering dataset QNLI~\citep{qnli}; 6) the word sense disambiguation dataset WiC~\citep{wic}; 7) the paraphrase detection datasets MRPC~\citep{mrpc} and QQP. Following \citet{perfect}, for each dataset we sample 16 instances per label from the original training set to form training and validation sets for few-shot learning.

\paragraph{Domain Generalization.} Then we design a more challenging problem about zero-shot domain generalization in the sentiment analysis task. Our experiments include 6 domains across 3 datasets: 1) the Amazon review dataset~\citep{amazon} containing reviews about Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K); 2) the airline review dataset (A)~\citep{airline1,airline2}; 3) the restaurant (R) domain obtained from the Yelp dataset~\citep{yelp}. 

We choose A as the source domain and the other five (B, D, E, K, R) constitute the target domains. On this basis, we sample 16 instances per label from the training set of the source domain to tune soft prompts. And then we directly use the soft prompts learned from the source domain to evaluate performance on the test set of each domain.


% sec 4.2
\subsection{Experimental Details}

\paragraph{Baselines.} Our experiments are built on a smaller-scale model, \textbf{T5-base}~\citep{t5}, and then on a larger-scale instruction-tuned model, \textbf{Flan-T5-XL}~\citep{flant5}. For both two backbone models, we use the following baselines: (1) prompt tuning methods with the same number of tunable parameters as SUPMER: vanilla prompt tuning (\textbf{PT}~\citealp{prompt_tuning}), \textbf{PPT}~\citep{ppt}, \textbf{Unified-PPT}~\citep{ppt}, and \textbf{MetaPT}~\citep{metapt}. (2) methods with more tunable parameters: \textbf{Prefix-Tuning}~\citep{prefix_tuning}, \textbf{P-tuning-v2}~\citep{ptuning-v2}, full-model tuning (\textbf{FT}). Furthermore, Given that FLAN-T5-XL was also designed with few-shot inference in mind, we additionally compare with two baseline methods on FLAN-T5-XL, \textit{i.e.,} \textbf{zero-shot inference} and \textbf{few-shot inference}, which directly employ Flan-T5-XL for downstream evaluation. We list the details of baselines in Appendix \ref{app:data_info}.

\input{tables/dg_main}


\paragraph{Implementation Details.} We solve all downstream tasks in a text-to-text format and run each experiment with 5 different random seeds. For all prompt tuning methods, we follow \citet{prompt_tuning} to design soft prompts composed of 100 soft tokens, with tunable parameters far less than full-model tuning. For our SUPMER, following PPT~\citep{ppt} we sample 10GB data from OpenWebText~\citep{openwebtext}, a large-scale unlabeled corpus, to construct self-supervised meta-training tasks. The meta-training stage only requires a one-time execution. In downstream prompt-tuning, we freeze the meta-gradient regularization parameters and the soft prompts are the only tunable parameters. We give more details of training hyper-parameters in Appendix \ref{app:training_detail}.



%  sec 4.3
\input{tables/fewshot_mix}
\subsection{Main Result}
Table \ref{tab:main_table_1} and Table \ref{tab:main_table_2} show the main results of few-shot learning and domain generalization. From the results, we have the following observations.

\begin{figure}[t]
    \includegraphics[width=\linewidth]{figures/diffstep.pdf}
    \caption{The performance after different training steps on CB and MRPC.}\label{fig:diffstep}
\vspace{-1em}
\end{figure}

\begin{figure}[t]
    \includegraphics[width=\linewidth]{figures/diffshot.pdf}
    \caption{The performance on SST-5 and SUBJ when different numbers of training samples are available.}\label{fig:diffshot}
\vspace{-1.2em}
\end{figure}

First, in few-shot learning, SUPMER achieves better performance than all baselines on 10 of 12 datasets, whether using T5-base or Flan-T5-XL as the backbone. And the average accuracy of SUPMER over all datasets reaches 71.3\% on T5-base, significantly outperforming other baselines (\textit{e.g.}, improving the performance by +1.3 points compared to FT). Notably, when utilizing the larger Flan-T5-XL as the backbone, SUPMER demonstrates even more substantial performance gains (\textit{e.g.}, improving the average performance by +2.5 points compared to FT), which indicates that our approach unlocks greater capabilities for stronger models that have undergone instruction-tuning with a higher number of parameters.

Specifically, SUPMER consistently outperforms all other prompt tuning methods with the same number of tunable parameters across all datasets. This indicates that our method offers soft prompts with better few-shot generalization ability. And it is noteworthy to highlight that SUPMER utilizes exactly the same unlabelled data as PPT and Unified-PPT for soft prompt initialization. Yet it considerably outperforms these two baselines, demonstrating that \textit{the performance improvement is primarily attributable to our methodology rather than the meta-training data itself}. Additionally, SUPMER outperforms baseline methods with more tunable parameters (\textit{e.g.}, full-model tuning) on the majority of datasets, achieving superior performance with fewer parameters.

Second, SUPMER is superior to all baselines in almost all domain-generalization setups. For example, compared to MetaPT which meta-trains soft prompts with a supervised sentiment analysis dataset, SUPMER exhibits average gains of 1.1\% on T5-base and 1.4\% on Flan-T5-XL. So it can be inferred that SUPMER shows stronger robustness to domain shifts, exhibiting better generalization to unseen tasks or domains.

Third, for both few-shot learning and domain generalization on Flan-T5-XL, SUPMER demonstrates superior performance across almost all datasets and domains in contrast to few-shot inference. It provides further evidence that for LMs such as Flan-T5-XL with inherent few-shot inference capabilities, our approach can significantly enhance their abilities in a parameter-efficient tuning strategy, without providing any in-context examples during inference. 

Fourth, SUPMER also results in lower variances on most datasets. Few-shot learning is often notorious for its instability. And in our method we keep few-shot prompt tuning more stable.


\input{tables/ablation_avg}

%  sec 4.4
\subsection{Ablation Study}
\label{sec: 4.4}
\paragraph{Analysis of Generalization. } Figure~\ref{fig:diffstep} shows the performance trend for each method after different training steps on datasets CB and MRPC with T5-base model. It illustrates that few-shot prompt tuning converges slowly with its performance typically showing an overall decline during the final training steps because they may easily result in overfitting. In comparison, SUPMER achieves faster, stronger, and more enduring few-shot generalization. \textit{It not only accelerates the convergence to the optimal performance realizing fast adaptation, but also consistently maintains its optimal performance across prolonged training periods}.

\paragraph{Effect of Sample Size. } We also discuss how the performance of SUPMER and other baselines varies when the number of training samples increases on SST-5 and SUBJ. As shown in Figure~\ref{fig:diffshot}, with T5-base as the underlying PLM, when the number of training samples per label grows from 4 to 64, SUPMER is consistently better than other prompt tuning methods. And the performance gap between these methods is gradually reduced as the number of training data increases. 

\paragraph{Self-Supervised v.s. Supervised.} To illustrate that self-supervised meta-learning can better generalize to unseen tasks compared to supervised meta-learning, we also collect a set of labeled datasets (ensuring no overlap with downstream testing datasets) to formulate meta-training tasks for soft prompt initialization and conduct the experiments of few-shot learning on T5-base. The results are displayed in Table \ref{fewshot_mix} (rows 1 and 2). As our collected labeled data contains lots of sentiment analysis datasets (\textit{e.g.,} Yelp5), SUPMER (only labeled) and SUPMER (only unlabeled) reveal proximity in their performance on sentiment analysis tasks (\textit{i.e.}, SST-2, SST-5, MR, CR). But in other tasks, using unlabeled data consistently achieves better results than utilizing only labeled data, also with a higher average accuracy over all datasets, which validates the superiority of self-supervised meta-learning.

\paragraph{Effect of integrating Labeled Data.} To further explore the impact of integrating labeled data and substantiate the efficacy of SUPMER following this integration, we amalgamate the original unlabeled meta-training data with our collected labeled data mentioned above, with a mixing ratio of labeled to unlabeled as 1:2. The amalgamated data is employed for constructing meta-training tasks to meta-train SUPMER. Moreover, following PPT~\citep{ppt} and MetaPT~\citep{metapt}, We also leverage pre-training and vanilla MAML to initialize soft prompts using the same amalgamated data. The experimental results of few-shot learning on T5-base are shown in Table \ref{fewshot_mix} (rows 3-5). First, we can see that SUPMER (labeled+unlabeled) outperforms SUPMER (unlabeled) and SUPMER (labeled) as it allows us to harness the high-quality advantages of labeled data while also exploiting the broader semantic concepts encapsulated by unlabeled data. Second, After the integration of labeled data, SUPMER still consistently demonstrates significantly superior performance compared to baseline methods employing the same data for prompt initialization, which further underscores the effectiveness of SUPMER.

\paragraph{Effect of Individual Components.} We train the following ablation models. 1) only sp / mc / ss: we retain sentence-pair classification / multi-choice classification / single-sentence classification as the only anchor meta-training task format. 2) w/o ta: we entirely remove the task augmentation method. 3) w/o curriculum: we only retain the vanilla task augmentation without the curriculum-based idea. 4) w/o mgr: we remove the meta-gradient regularization function. All experiments follow the settings in \S \ref{sec: 4.1} and are conducted on T5-base. We report the average accuracy of few-shot learning and domain generalization in Table \ref{tab:ablation}. More detailed results are in Appendix \ref{app:ablation_result}.

The results of Row 1-3 indicate that considering diversified task formats during meta-training helps efficiently generalize to different tasks as downstream tasks often contain various task formats. Row 4 and Row 5 highlight that task augmentation plays an essential role in our framework, with curriculum-based augmentation further enriching the task distribution and realistically simulating the distribution shift. Moreover, Row 6 validates the superiority of meta-gradient regularization in avoiding overfitting to some domain-specific correlations, thus achieving better performance.

