\section*{Appendices}
% sec A
\section{Additional Information for SUPMER}
% sec A.1
\subsection{Complete Analysis of SUPMER}
\label{app:math_analysis}
\newcommand{\og}{\overline{g}}
\newcommand{\oh}{\overline{H}}
In this section, we provide a more comprehensive and complete analysis of SUPMER. We will show that during meta-training, the optimization of soft prompt embeddings $\theta$ and the meta-gradient regularization parameters $\phi$ tends to maximize the inner product of gradients obtained from the support set after transformation and gradients from the query set.

Specifically, to update the parameters $\theta$ and $\phi$, we should evaluate their gradients at first, denoting them as $g^\theta$ and $g^\phi$. Considering the original algorithm of MAML, each task consists of a support set and a query set. And only one step of gradient descent is applied in the inner-loop optimization. To make our statement more direct, we denote the loss function based on the support set and the query set as $\mathcal{L}_0$ and $\mathcal{L}_1$. In SUPMER, ignoring the regularized loss, only $\mathcal{L}_1$ is directly utilized to optimize $\phi$, while $\theta$ is optimized in a bi-level meta-optimization paradigm. Here we define the following terms related to $\theta$ similar to \citet{reptile}: 
\begin{equation}
\small
    \begin{aligned}
        &g^{\theta}_{i} = \frac{\partial \mathcal{L}_i(\theta_i)}{\partial \theta_i}     &&\quad\text{(gradient obtained during SGD)}   \\
        &\og^{\theta}_{i} = \frac{\partial \mathcal{L}_i(\theta_0)}{\partial \theta_0}   &&\quad\text{(gradient at initial point)}      \\
        &\oh^{\theta}_{i} = \frac{\partial^2 \mathcal{L}_i(\theta_0)}{\partial \theta_0^2} &&\quad\text{(Hessian at initial point)} \\
        &\theta_1 = \theta_0 - \alpha_1 \psi_{\phi}(g^{\theta}_0)  &&\quad\text{(gradient descent in the inner-loop)}
    \end{aligned}
\end{equation}
For each definition $i \in \{0,1\}$ and $\psi_\phi(\cdot)$ is the meta-gradient regularization operation. $\theta_0$ denotes the initial soft prompt embeddings for each step, and $\theta_1$ denotes the embeddings after the inner-loop optimization. Obviously we have $g^{\theta}_{0} = \og^{\theta}_{0}$. Firstly we perform a Taylor series expansion to approximate the SGD gradients $g^{\theta}_1$ obtained from the query set as follows:
\begin{equation}
\label{eq:proof1}
\small
    \begin{aligned}
        g^{\theta}_{1} &= \frac{\partial \mathcal{L}_1(\theta_1)}{\partial \theta_1} \\
        &= \frac{\partial \mathcal{L}_1(\theta_0)}{\partial \theta_0} + \frac{\partial^2 \mathcal{L}_1(\theta_0)}{\partial \theta_0^2}(\theta_1-\theta_0) + \underbrace{O(||\theta_1 - \theta_0||^2)}_{=O(\alpha_1^2)} \\
        &= \og^{\theta}_{1} - \alpha_1 \oh^{\theta}_{1} \psi_{\phi}(\og^{\theta}_0) + O(\alpha_1^2)
    \end{aligned}
\end{equation}
Then we analysis the gradient descent operation in the inner-loop optimization based on the support set. Define $U$ as the gradient descent and we have $U(\theta_0) = \theta_0 - \alpha_1 \psi_{\phi}(\frac{\partial \mathcal{L}_0(\theta_0)}{\partial \theta_0})$. So we can get $\frac{\partial U(\theta_0)}{\partial \theta_0}$ and $\frac{\partial U(\theta_0)}{\partial \phi}$ as follows:
\begin{equation}
\label{eq:proof2}
\small
\begin{aligned}
    \frac{\partial U(\theta_0)}{\partial \theta_0} &= \frac{\partial}{\partial \theta_0} (\theta_0 - \alpha_1 \psi_{\phi}(\frac{\partial \mathcal{L}_0}{\partial \theta_0})) \\
    &= I - \alpha_1 \frac{\partial \psi_\phi(g^\theta_0)}{\partial g^\theta_0}\cdot \frac{\partial g^\theta_0}{\partial \theta_0} \\
    &= I - \alpha_1 \frac{\partial \psi_\phi(g^\theta_0)}{\partial g^\theta_0}\cdot \oh_0^\theta
\end{aligned}
\end{equation}

\begin{equation}
\label{eq:proof3}
\small
\begin{aligned}
    \frac{\partial U(\theta_0)}{\partial \phi} &= \frac{\partial}{\partial \phi} (\theta_0 - \alpha_1 \psi_{\phi}(\frac{\partial \mathcal{L}_0}{\partial \theta_0})) \\
    &= - \alpha_1 \frac{\partial \psi_\phi(g^\theta_0)}{\partial \phi}
\end{aligned}
\end{equation}
So based on Eq.~(\ref{eq:proof1}, \ref{eq:proof2}, \ref{eq:proof3}), we can finally approximate the gradients $g^\theta$ and $g^\phi$ as:
\begin{equation}
\small
\begin{aligned}
    g^{\theta} &= \frac{\partial \mathcal{L}_1(\theta_1)}{\partial \theta_0} \\
    &= \frac{\partial \mathcal{L}_1(U(\theta_0))}{\partial \theta_0} \\
    &= \frac{\partial \mathcal{L}_1}{\partial \theta_1} \cdot \frac{\partial U(\theta_0)}{\partial \theta_0} \\
    &= \og^\theta_{1} - \alpha_1 \oh^\theta_{1} \psi_{\phi}(\og^\theta_0) - \alpha_1 \og^\theta_{1}\cdot \frac{\partial \psi_\phi(\og^\theta_0)}{\partial \og^\theta_0}\cdot \oh^\theta_0 + O(\alpha_1^2) \\
    &= \og^\theta_{1} - \alpha_1\frac{\partial}{\partial \theta_0}(\og^\theta_1 \psi_\phi(\og^\theta_0)) + O(\alpha_1^2) \\
    \\
    g^{\phi} &= \frac{\partial \mathcal{L}_1(\theta_1)}{\partial \phi} \\
    &= \frac{\partial \mathcal{L}_1(U(\theta_0)}{\partial \phi} \\
    &= \frac{\partial \mathcal{L}_1}{\partial \theta_1} \cdot \frac{\partial U(\theta_0)}{\partial \phi} \\
    &= -\alpha_1 \og^\theta_{1} \frac{\partial \psi_\phi(\og^\theta_0)}{\partial \phi} + O(\alpha_1^2) \\
    &= -\alpha_1 \frac{\partial}{\partial \phi} (\og^\theta_1 \psi_\phi(\og^\theta_0)) + O(\alpha_1^2)
\end{aligned}
\end{equation}
 Thus, $ -\frac{\partial}{\partial \theta_0} (\og^\theta_1 \psi_\phi(\og^\theta_0)) $ and $-\frac{\partial}{\partial \phi} (\og^\theta_1 \psi_\phi(\og^\theta_0))$ indicate the optimization direction, which increases the inner product between gradients from the query set and gradients from the support set after transformation. To further consolidate our analysis, we also track the normalized gradient inner product in the first 5000 steps during meta-training. As shown in Figure~\ref{fig:innerproduct}, it is clear that the normalized gradient inner product gradually increases during meta-training.
 
 On this basis, since there exists distribution shift between the support set and the query set after task augmentation, our method aligns the gradient directions across different distributions, which helps enhance model generalization. Besides, the meta-gradient regularization parameters $\phi$ also retain some domain-invariant information of the meta-training data in the above process. Considering that $\phi$ is fixed in downstream tasks, $\phi$ can be applied to encourage the alignment between the domain-specific gradients and avoid prompt-tuning overfitting to some domain-specific correlations.

\begin{figure}[t]
    \centerline{\includegraphics[width=0.6\linewidth]{figures/inner_product.pdf}}
    \caption{Normalized gradient inner products in the first 5000 steps during meta-training.}\label{fig:innerproduct}
% \vspace{-2em}
\vspace{-1em}
\end{figure}

% sec A.2
\subsection{Constructing Anchor Meta Tasks}
\label{app:anchor_task}
Given a sentence $x$ from unlabeled corpora, we can derive semantically meaningful sentence embedding $\bm{H}=f^{enc}_{\theta}(x)$ with PLMs, e.g., T5 encoder. And we apply K-means to cluster these unlabeled sentences according to their embeddings:
\begin{equation}
\small
    \mathcal{P}, \{\bm{\mu}_c\} = \arg\min_{\{\mathcal{C}_c\},\{\bm{\mu}_c\}} \sum_{c=1}^K \sum_{\bm{H}\in \mathcal{C}_c} \lVert \bm{H} - \bm{\mu}_c \rVert^2
\end{equation}
where $\bm{\mu}_c$ indicates the learned centroid of cluster $\mathcal{C}_c$ and $\mathcal{P}$ indicates the partitions of all sentences. K-means clustering leads to more abundant formats and objectives of meta-training tasks. Based on the results of K-means, we design three formats of anchor self-supervised meta-training tasks: sentence-pair classification, multi-choice classification, and single-text classification. Here we introduce each of them in detail.

\paragraph{Sentence-pair Classification.} Sentence-pair classification takes a pair of sentences $(x_0,x_1)$ as input, and $x_0$ is the anchor sentence. We carry on next sentence prediction task and sentence similarity task in sentence-pair classification with the label list $\mathcal{Y}=[0,1,2]$. For the former one, following \citet{ppt}, we set two sentences next to each other as label 0, those from the same document but not adjacent as label 2, and those from different documents as label 1. And for sentence similarity task, we set two sentences coming from the same cluster as label 0, and those from different clusters as label 1. In this way, the prompt template and verbalizer are designed as:
\begin{equation}
\small
\begin{aligned}
    &P = \text{``}s_1 \left\langle \text{X} \right\rangle . s_2\text{''} \\
    &\mathcal{V} = \{0\rightarrow \text{yes}, 1\rightarrow \text{no}, 2\rightarrow \text{maybe}\}
\end{aligned}
\end{equation}

\paragraph{Multi-choice Classification.} Multi-choice classification takes an anchor sentence $x_0$ as the query and we should find the correct one in several answer candidates. Here we also set two different tasks. The first one aims to select the sentence next to $s_0$ and the second one aims to select the sentence which belongs to the same cluster as $s_0$. In each task we will set four candidates, and only one of them is correct. We design the prompt template and verbalizer as follows:
\begin{equation}
\small
\begin{aligned}
    &P = \text{``}s_0\text{? A.}s_1 \cdots \text{D.} s_4. \text{ Answer: }\left\langle \text{X} \right\rangle\text{''} \\
    &\mathcal{V} = \{0\rightarrow \text{A}, 1\rightarrow \text{B}, 2\rightarrow \text{C}, 3\rightarrow \text{D}\}
\end{aligned}
\end{equation}

% alg.1
\begin{algorithm}[tp]
\caption{Meta-training Process of SUPMER}\label{alg:supmer}
\begin{algorithmic}[1]
\State $p(\mathcal{T})$ : Distribution over anchor tasks
\State $f_\theta$ : PLM with soft prompt embeddings $\theta$
\State $\psi_\phi$ : Meta-gradient regularization
\State $\alpha_1, \beta_1, \beta_2$ : Learning rate 
\State $\mathbf{TA}$ : Task augmentation in Algorithm 2
\Statex 

\State $s \gets -1$
\State Randomly initialize $\theta, \phi$
\While{not done}
    \State Sample a batch of task \begin{small}$\{\tau_i\}_{i=1}^n$\end{small} from $p(\mathcal{T})$
    \State $\{\tau_i\}_{i=1}^n = \mathbf{TA}( \{\tau_i\}_{i=1}^n, p(\mathcal{T}), s)$
    \For {\textbf{all} $\tau_i = \{\mathcal{D}_{\tau_i}^s , \mathcal{D}_{\tau_i}^q \}$}
        \State Evaluate $\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta)$ with $\mathcal{D}_{\tau_i}^{s}$ 
        \State Evaluate $\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}(f_\theta)$ with $\mathcal{D}_{\tau_i}^{q}$
        \State Transform $\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta)$ via $\psi_\phi(\cdot)$
        % \State Transform $\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta)$ via $\psi_\phi(\cdot)$
        \State $s_i = \frac{\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}(f_\theta) \cdot \psi_\phi(\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta))}{\Vert \nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}(f_\theta) \Vert\cdot\Vert \psi_\phi(\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta))\Vert}$
        \State $\theta_i^{\prime} = \theta - \alpha_1 \psi_\phi(\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta))$
    \EndFor
    \State $s \gets \sum_i s_i / \sum_i 1$
    \State $\theta \gets \theta - \beta_1 \nabla_{\theta} \sum_{\tau_i} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}} (f_{\theta^{\prime}_i})$
    \State $\phi \gets \phi - \beta_2 \nabla_{\phi} \big( \sum_{\tau_i} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}} (f_{\theta^{\prime}_i}) + \mathcal{L}_{reg}\big)$
\EndWhile
\State \textbf{return} $\theta, \phi$
\end{algorithmic}
\end{algorithm}

\paragraph{Single-Sentence Classification.} Through K-means clustering, each sentence is associated with a cluster label $r_i$ in $\{0,1\}^K$ where $r_{ic}=1$ if $c=k$ and $y_{ic}=0$ if $c \neq k$. Here $k$ represents the cluster to which the sentence belongs. We simply use $r_i$ as the pseudo label for meta-training and construct 4-way classification tasks. As for the designing of the verbalizer, we transform the single-sentence classification into the format of multi-choice classification. We insert the centroid of cluster $\bm{\mu}_c$ into the template and use it to represent the corresponding cluster. So that we have:
\begin{equation}
\small
\begin{aligned}
    &P = \text{``}s_0\text{? A.}\left\langle \mu_{c_1} \right\rangle \cdots \text{D.}\left\langle \mu_{c_4} \right\rangle. \text{ Answer: }\left\langle \text{X} \right\rangle\text{''} \\
    &\mathcal{V} = \{0\rightarrow \text{A}, 1\rightarrow \text{B}, 2\rightarrow \text{C}, 3\rightarrow \text{D}\}
\end{aligned}
\end{equation}
On this basis, for each task format, we separate all data into different tasks to construct anchor meta-training tasks with good task distributions. Through K-means, sentences with similar embeddings are clustered into the same group. So in sentence-pair classification and multi-choice classification, we group samples whose anchor sentence comes from the same cluster into the same meta-training task. And in single-sentence classification, for each meta-training task, we randomly select $N$ clusters as $N$ classes and then sample $k$ sentences for each cluster to construct a $N$-way $k$-shot classification task ($N=4$). In this way, we completely construct all anchor meta-training tasks.

% sec A.3
\subsection{Pseudo-Codes of SUPMER}
\label{app:pseudo_code}
We show the pseudo-codes for the meta-training process of SUPMER in Alg.~\ref{alg:supmer}. And the process of curriculum-based task augmentation is described in Alg.~\ref{alg:ta}.

% alg.2
\begin{algorithm}[tp]
\caption{$\mathbf{TA}$ : Curriculum-based Task Augmentation} \label{alg:ta}
\begin{algorithmic}[1]
\State $\{\tau_i\}_{i=1}^n$ : A batch of anchor tasks
\State $p(\mathcal{T})$ : Distribution over anchor tasks
\State $s \in [-1,1]$ : Avg cos-sim between gradients  
\State $\alpha$, $m$ : hyper-parameters 
\Statex 
\State $s \gets (1+s)/2$
\State $b \gets (m^s-1)/(m-1)$
\For {\textbf{all} $\tau_i = \{\mathcal{D}_{\tau_i}^s , \mathcal{D}_{\tau_i}^q \}$}
    \State Sample task $\tau_j=\{\mathcal{D}_{\tau_j}^s , \mathcal{D}_{\tau_j}^q \}$ from $p(\mathcal{T})$
    \State Draw $\lambda$ from $Beta(\alpha, b\alpha)$
    \Statex \quad \textcolor{teal}{// $\mathcal{D}_{\tau_i}^q=(\bm{H}^{q}_i, \bm{Y}^{q}_i),\mathcal{D}_{\tau_j}^q=(\bm{H}^{q}_j, \bm{Y}^{q}_j)$}
    \Statex \quad \textcolor{teal}{// $\bm{H}$: the hidden representations of samples} 
    \State $\tilde{\bm{H}}_{i}^{q} = (1-\lambda) \bm{H}^{q}_i + \lambda \bm{H}^{q}_j$
    \State $\tilde{\bm{Y}}_{i}^{q} = (1-\lambda) \bm{Y}^{q}_i + \lambda \bm{Y}^{q}_j$
    \State $\mathcal{D}_{\tau_i}^q \gets (\tilde{\bm{H}}_{i}^{q}, \tilde{\bm{Y}}_{i}^{q})$
\EndFor
\State \textbf{return} $\{\tau_i\}_{i=1}^n$
\end{algorithmic}
\end{algorithm}

% table-statics
\begin{table}[t]
    \centering
    \small
    \begin{adjustbox}{width=0.48\textwidth}
    \begin{tabular}{llccc}
    \toprule
    \textbf{Dataset} & \textbf{Task} & \textbf{\#Train} & \textbf{\#Test} & \textbf{K} \\ \midrule
    SST-2 & Sentiment analysis & 6920 & 872 & 2 \\
    SST-5 & Sentiment analysis & 8544 & 2210 & 5 \\
    MR & Sentiment analysis & 8662 & 2000 & 2 \\
    CR & Sentiment analysis & 1774 & 2000 & 2 \\ \midrule
    SUBJ & Subjectivity classification & 8000 & 2000 & 2 \\ \midrule
    CB & Natural language inference & 250 & 56 & 3 \\
    RTE & Natural language inference & 2490 & 277 & 2 \\ \midrule
    QNLI & Question answering & 104743 & 5463 & 2 \\
    BoolQ & Question answering & 9427 & 3270 & 2 \\ \midrule
    WiC & Word sense disambiguation & 5428 & 638 & 2 \\ \midrule
    MRPC & Paraphrase detection & 3668 & 408 & 2 \\
    QQP & Paraphrase detection & 363846 & 40430 & 2 \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{\label{tab:data}Statistics of all 12 datasets for few-shot learning. K is the number of labels. We sample $N \times \text{K}$ instances from the original training set to construct the few-shot training and validation sets. And \emph{\#Test} shows the size of the test set.}
    \vspace{-1em}
\end{table}

% sec B
\section{Dataset Information}
\label{app:data_info}
\paragraph{Few-shot Learning.} We conduct experiments of few-shot learning on 6 different downstream English tasks with 12 datasets. Since some of the test sets of the datasets are not publicly available, following \citet{perfect}, we test on the original validation sets for SST-2, CB, RTE, QNLI, BoolQ, WiC, MRPC, and QQP. Besides, we download the datasets of SST-2, SST-5, MR, CR, and SUBJ from \citet{better-learner}. And the rest of the datasets are obtained from the HuggingFace Datasets library~\citep{hugging-face-dataset}. CB, RTE, BoolQ, and Wic are from SuperGLUE Benchmark~\citep{superglue}, while QNLI, MRPC, and QQP are from GLUE Benchmark~\citep{glue} with Creative Commons license (CC BY 4.0). We give the statistics of all these datasets in Table \ref{tab:data}.  

\paragraph{Domain Generalization.} We evaluate on the sentiment analysis task including 6 different domains: Books (B), DVDs (D), Electronics (E), Kitchen appliances (K), Airlines (A) and Restaurants (R). Each domain has totally 2,000 manually labeled data of binary categories for testing, including 1000 positive and 1000 negative. We choose B as the source domain and the other five (D, E, K, A, R) constitute the target domains. We sample 16 instances per label from the training set of the source domain for prompt tuning and then evaluate on the test sets of all 6 domains. 

%  sec C
% table-hyperparameters

\section{Training Details}
\label{app:training_detail}
We apply the T5 base model~\citep{t5} (220M parameters) as the underlying PLM, and use the HuggingFace Pytorch implementation~\citep{transformers}. We run experiments with six GeForce RTX 3090 24G GPUs. And the meta-training process of SUPMER takes about 140 GPU hours. Next we will describe the details of training hyper-parameters.
% sec C.1
\subsection{Training Hyper-parameters for Downstream Tasks}
In our experiments, we leverage full-model tuning and prompt tuning to solve downstream tasks, including few-shot learning and domain generalization. In few-shot learning, following some prior work~\citep{small_model_fewshot, perfect}, we set the maximum sequence length of each example to 256 for CR, SUBJ, CB, RTE and WiC, and 128 for other datasets. While in domain generalization, the maximum sequence length of each example is set to 256. 

We run each experiment 5 times on the random seed [10, 20, 30, 40, 50] and report the average accuracy as well as the standard deviation. For both full-model tuning and prompt tuning, We implement AdamW as the optimizer. We use a batch size of 32 and train the model for 200 epochs, meanwhile evaluating the model every 10 steps. And we report the results for hyper-parameters performing the best on the validation set for each task.

Besides, for full-model tuning, all parameters of PLM are fine-tuned without adding soft prompts. We use the learning rate of [1e-5, 2e-5, 3e-5] and choose the one obtaining the highest validation performance.

For prompt tuning, we freeze all PLM parameters and only tune soft prompts composed of 100 soft tokens. And we find that prompt tuning requires a much larger learning rate than full-model tuning. We search for the learning rate in [1e-1, 2e-1, 3e-1] and also choose the model with the best performance on the validation set. 
\begin{table}[t]
    \centering
    \small
    \begin{adjustbox}{width=0.48\textwidth}
    \begin{tabular}{@{}cc@{}}
    \toprule
    \textbf{Hyper-parameter} & \textbf{Value} \\ \midrule
    Number of clusters for each task format & 250 \\
    Tasks per batch & 4  \\
    Size of support set per task & 32 \\
    Size of query set per task & 32 \\
    Optimizer & Adam \\
    Inner loop learning rate & 0.1 \\
    Outer loop learning rate & 0.1 \\
    Learning rate for $\phi$ & 1e-4 \\
    Scheduler & Linear scheduler  \\
    Warm-up steps & 0 \\  
    Max training steps & 100,000 \\
    Validation steps & 2,000 \\
    Max sequence length & 512 \\
    $\lambda$ & 1.0 \\
    $m$ & 2.0 \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{\label{tab:hyper-parameters}Hyper-parameters for SUPMER. $\phi$ denotes the meta-gradient regularization parameters. $\lambda$ is the coefficient of the regularized loss. And $m$ is the curve parameter in the curriculum-based task augmentation.}
    \vspace{-1em}
\end{table}

 \begin{table*}[t]
    \centering
    \small
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
    \toprule
    \textbf{Methods} & \textbf{SST-2} & \textbf{SST-5} & \textbf{MR} & \textbf{CR} & \textbf{SUBJ} & \textbf{CB} & \textbf{RTE} & \textbf{QNLI} & \textbf{BoolQ} & \textbf{WiC} & \textbf{MRPC} & \textbf{QQP}\\
    \midrule
    only sp & $83.6_{1.5}$ & $42.6_{2.2}$ & $81.7_{1.8}$ & $86.0_{0.6}$ & $65.8_{2.8}$ & $64.6_{2.1}$ & $57.0_{2.5}$ & $58.4_{3.3}$ & $\bm{61.8_{0.8}}$ & $53.6_{1.5}$ & $69.9_{1.3}$ & $66.0_{1.0}$ \\  
    only mc & $83.4_{1.4}$ & $44.5_{1.9}$ & $79.3_{5.1}$ & $88.3_{0.5}$ & $70.5_{4.7}$ & $65.9_{3.1}$ & $54.9_{1.3}$ & $58.7_{1.7}$ & $61.6_{1.1}$ & $54.2_{1.8}$ & $68.8_{0.8}$ & $67.6_{1.3}$ \\
    only ss & $84.5_{1.5}$ & $45.0_{2.0}$ & $81.5_{0.7}$ & $88.4_{0.5}$ & $73.3_{3.1}$ & $62.1_{2.6}$ & $53.9_{1.0}$ & $56.5_{1.4}$ & $58.9_{1.9}$ & $53.3_{1.3}$ & $67.7_{1.3}$ & $63.7_{1.7}$ \\ \midrule
    w/o ta & $84.7_{1.0}$ & $40.1_{3.3}$ & $81.9_{1.8}$ & $87.2_{0.8}$ & $73.6_{2.8}$ & $66.4_{1.9}$ & $56.6_{0.9}$ & $59.4_{1.8}$ & $59.8_{2.6}$ & $54.3_{2.4}$ & $69.5_{1.1}$ & $70.2_{1.1}$ \\
    w/o curriculum & $86.8_{0.8}$ & $40.8_{2.2}$ & $82.3_{1.3}$ & $88.4_{0.9}$ & $74.8_{3.1}$ & $71.0_{2.1}$ & $56.5_{0.8}$ & $\bm{62.6_{1.4}}$ & $59.9_{2.1}$ & $\bm{55.4_{1.1}}$ & $69.7_{0.8}$ & $\bm{71.3_{1.2}}$ \\ \midrule
    w/o mgr & $85.0_{1.3}$ & $44.5_{1.1}$ & $82.8_{0.7}$ & $88.0_{0.5}$ & $76.0_{1.7}$ & $67.1_{1.6}$ & $56.8_{0.8}$ & $58.9_{2.4}$ & $60.6_{1.3}$ & $54.4_{2.0}$ & $70.0_{1.0}$ & $70.3_{0.9}$ \\
    \midrule
    SUPMER & $\bm{87.3_{0.5}}$ & $\bm{46.7_{0.6}}$ & $\bm{84.0_{0.6}}$ & $\bm{89.3_{0.3}}$ & $\bm{79.6_{2.2}}$ & $\bm{72.4_{1.4}}$ & $\bm{57.3_{1.0}}$ & $61.7_{1.0}$ & $61.1_{1.2}$ & $54.8_{1.2}$ & $\bm{71.3_{0.5}}$ & $70.5_{1.0}$ \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{\label{abla_table_1}Results of ablation study for few-shot learning. In the first three rows we keep only one anchor task format during meta-training, and sp stands for sentence-pair classification, mc for multi-choice classification, ss for single-sentence classification. And w/o ta means entirely removing task augmentation, w/o curriculum only retains the vanilla task augmentation without the curriculum-based idea. w/o mgr means removing the meta-gradient regularization method. We can see each component is critical in our method.}
\end{table*}

\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{l cccccc}
    \toprule
    \multirow{2}*{\textbf{Method}} & \multicolumn{1}{c}{Source} & \multicolumn{5}{c}{Target} \\
    \cmidrule(lr){2-2} \cmidrule(lr){3-7} & \textbf{B} & \textbf{D} & \textbf{E} & \textbf{K} & \textbf{A} & \textbf{R}\\
    \midrule
    only sp & $83.4_{1.1}$ & $82.1_{1.4}$ & $83.0_{0.7}$ & $88.5_{1.2}$ & $88.9_{0.8}$ & $88.1_{0.7}$\\  
    only mc & $84.0_{0.6}$ & $82.3_{1.2}$ & $81.5_{0.9}$ & $88.5_{1.0}$ & $89.3_{0.7}$ & $88.8_{0.7}$\\
    only ss & $83.6_{0.7}$ & $84.7_{0.8}$ & $84.2_{0.6}$ & $88.9_{0.9}$ & $89.7_{0.9}$ & $89.0_{0.3}$\\ \midrule
    w/o ta & $83.4_{0.8}$ & $82.0_{1.4}$ & $81.7_{1.5}$ & $87.8_{0.8}$ & $88.2_{0.6}$ & $88.6_{0.6}$\\
    w/o curriculum & $84.0_{0.5}$ & $84.7_{0.8}$ & $83.9_{0.6}$ & $89.6_{0.5}$ & $90.3_{0.8}$ & $89.7_{1.1}$\\ \midrule
    w/o mgr & $83.8_{0.4}$ & $83.4_{0.5}$ & $83.3_{0.5}$ & $88.1_{0.6}$ & $89.2_{0.8}$ & $88.9_{0.4}$\\
    \midrule
    SUPMER & $\bm{85.7_{0.5}}$ & $\bm{85.3_{0.6}}$ & $\bm{85.1_{0.4}}$ & $\bm{90.3_{0.7}}$ & $\bm{91.1_{0.5}}$ & $\bm{90.4_{0.4}}$\\
    \bottomrule
    \end{tabular}
    \caption{\label{abla_table_2}Results of ablation study for domain generalization. }
\end{table*}

% sec C.2
\subsection{Training Hyper-parameters for Prompt Initialization}
\paragraph{Pre-training for prompt initialization.} \citet{ppt} proposes two frameworks for unsupervised prompt pre-training, named PPT and Unified PPT. PPT designs three formats of unsupervised pre-training tasks (sentence-pair classification, multiple-choice classification and single-text classification), and Unified-PPT further formulate them into a unified task form. We implement PPT and Unified-PPT following the hyper-parameters provided in \citet{ppt} and reset the pre-trained language model to T5-base. Specifically, for both PPT and Unified-PPT, we sample 10GB of unlabeled data from OpenWebText to construct pre-training tasks for each task format. And 5\% data are split for validation. We apply the “inverse square root” learning rate scheduler with no warm-up steps and set the learning rate as 0.1. We set the batch size to 256 with the max sequence length as 512, and train soft prompts for at most 200,000 steps. We evaluate the performance on the validation set every 2,000 steps and choose prompts with the lowest validation loss.


\paragraph{Meta-training for prompt initialization.} In our SUPMER framework, we sample 10GB of unlabeled data from OpenWebText to construct self-supervised meta-training tasks. We split 5\% data to construct tasks for validation. And for each task format, we first set the number of clusters to 250. We sample 4 meta-training tasks in a batch, and train the prompt embeddings $\theta$ and the meta-gradient regularization parameters $\phi$ for at most 100,000 steps. We also evaluate the performance on the validation set every 2,000 steps, choosing $\theta$ and $\phi$ with the lowest validation loss for downstream tasks. Table \ref{tab:hyper-parameters} lists all training hyper-parameters for SUPMER.

To illustrate the superiority of self-supervised meta-learning, we also imitate the method of \citet{metapt} to initialize soft prompts via supervised meta-learning and name it as Sup-MetaPT. Sup-MetaPT uses a supervised sentiment analysis dataset Yelp5 as the meta-training data, which has 650,000 training samples only covering the domain of restaurants. Following \citet{metapt}, We group all labeled data into 10 clusters through K-means. And we set the inner loop learning rate to 0.08, the outer loop learning rate to 0.025 with the early stop patience as 6. Other hyper-parameters are consistent with those in SUPMER. 

% sec D
\section{Full Results of Ablation Study}
\label{app:ablation_result}

\begin{table}[t]
\small
\centering
% \begin{adjustbox}{width=0.48\textwidth}
\begin{tabular}{@{}l|cc@{}}
\toprule
\textbf{Dataset} & \textbf{self-supervised} & \textbf{supervised} \\
\midrule
SST-2  & $\mathbf{87.3_{0.5}}$ & $87.0_{0.6}$\\
SST-5  & $46.7_{0.6}$ & $\mathbf{46.9_{0.6}}$\\
MR & $\mathbf{84.0_{0.6}}$ & $83.6_{0.8}$\\
CR & $89.3_{0.3}$ & $\mathbf{90.1_{0.2}}$\\
SUBJ & $\mathbf{79.6_{2.2}}$ & $75.0_{2.6}$\\
CB & $\mathbf{72.4_{1.4}}$ & $67.7_{1.9}$\\
RTE & $\mathbf{57.3_{1.0}}$ & $56.8_{1.3}$\\
QNLI & $\mathbf{61.7_{1.0}}$ & $58.8_{2.1}$\\
BoolQ & $\mathbf{61.1_{1.2}}$ & $57.7_{3.9}$\\
WiC & $\mathbf{54.8_{1.2}}$ & $54.4_{1.3}$\\
MRPC & $\mathbf{71.3_{0.5}}$ & $69.3_{0.9}$\\
QQP & $\mathbf{70.5_{1.0}}$ & $69.7_{1.2}$\\ \midrule
Avg & $\mathbf{69.7_{1.0}}$ & $68.1_{1.5}$ \\ \bottomrule
\end{tabular}
% \end{adjustbox}
\caption{\label{abla_table_3}Results of ablation study to illustrate the superiority of self-supervised meta-learning. ``self-supervised'' denotes our SUPMER framework while ``supervised'' denotes the ablation model.}
\vspace{-1em}
\end{table}

Here we first give detailed experimental results of ablation study in \S \ref{sec: 4.4}. We evaluate each ablation model over all 12 datasets of few-shot learning and all 6 domains of domain generalization. We run each experiment 5 times on the random seed [10, 20, 30, 40, 50] and report the average performances as well as the standard deviation. The detailed results of few-shot learning and domain generalization are shown in Table \ref{abla_table_1} and Table \ref{abla_table_2}.

Besides, to further illustrate that self-supervised meta-learning can better generalize to unseen tasks compared to supervised meta-learning, we design a new ablation model based on Sup-MetaPT. Specifically, Sup-MetaPT directly leverages MAML to initialize soft prompts after constructing meta-training tasks with Yelp5. Here, we also migrate the curriculum-based task augmentation and meta-gradient regularization in SUPMER to Sup-MetaPT. The only difference between SUPMER and this ablation model is the way to create anchor meta-training tasks (self-supervised or supervised). 

We conduct the corresponding experiments in few-shot learning. Experimental results are shown in Table \ref{abla_table_3}. As the ablation model is trained with a supervised sentiment analysis dataset Yelp5, we can see that there is little difference between the performance of these two models under sentiment analysis tasks (i.e., SST-2, SST-5, MR, CR). But in other tasks, SUPMER consistently achieves better results also with higher average performance over all datasets. The results further validate the superiority of self-supervised meta-learning. 



