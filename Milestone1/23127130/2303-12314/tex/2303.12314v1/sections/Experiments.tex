% main table 1
\begin{table*}[t]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
    \toprule
    \textbf{Methods} & SST-2 & SST-5 & MR & CR & SUBJ & CB & RTE & QNLI & BoolQ & WiC & MRPC & QQP\\
    \midrule
    FT & $83.6_{1.7}$ & $41.2_{2.6}$ & $81.7_{0.9}$ & $88.3_{0.9}$ & $\bm{80.0_{1.4}}$ & $71.9_{1.5}$ & $56.9_{2.1}$ & $\bm{62.3_{0.6}}$ & $61.7_{1.6}$ & $54.6_{1.6}$ & $70.2_{0.7}$ & $69.5_{1.0}$ \\  
    PT & $71.9_{3.4}$ & $37.3_{2.8}$ & $73.2_{4.5}$ & $84.4_{3.5}$ & $61.5_{6.0}$ & $58.9_{6.1}$ & $53.2_{2.8}$ & $55.2_{4.8}$ & $56.4_{6.3}$ & $53.1_{2.4}$ & $66.6_{2.6}$ & $63.0_{2.6}$ \\
    PPT & $81.2_{2.0}$ & $40.2_{5.4}$ & $81.2_{0.7}$ & $83.6_{7.3}$ & $66.8_{3.7}$ & $60.7_{7.7}$ & $55.4_{1.2}$ & $60.4_{3.9}$ & $\bm{61.8_{0.7}}$ & $53.6_{1.3}$ & $68.0_{0.8}$ & $63.1_{0.7}$ \\
    Unified-PPT & $76.8_{7.7}$ & $44.7_{1.7}$ & $79.0_{3.3}$ & $87.7_{0.6}$ & $64.2_{5.8}$ & $63.9_{2.9}$ & $54.4_{1.1}$ & $56.7_{2.4}$ & $61.4_{0.8}$ & $54.5_{1.6}$ & $67.8_{1.1}$ & $67.6_{2.9}$ \\
    Sup-MetaPT & $85.7_{1.3}$ & $45.3_{1.1}$ & $82.5_{4.5}$ & $88.5_{0.3}$ & $73.2_{3.7}$ & $65.4_{2.4}$ & $56.1_{1.7}$ & $58.2_{3.1}$ & $54.0_{4.2}$ & $54.1_{1.3}$ & $69.6_{0.6}$ & $68.8_{0.9}$ \\
    \midrule
    SUPMER & $\bm{87.3_{0.5}}$ & $\bm{46.7_{0.6}}$ & $\bm{84.0_{0.6}}$ & $\bm{89.3_{0.3}}$ & $79.6_{2.2}$ & $\bm{72.4_{1.4}}$ & $\bm{57.3_{1.0}}$ & $61.7_{1.0}$ & $61.1_{1.2}$ & $\bm{54.8_{1.2}}$ & $\bm{71.3_{0.5}}$ & $\bm{70.5_{1.0}}$ \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{\label{tab:main_table_1}Results of few-shot learning. We report the average accuracy and standard deviation over five random seeds. \textbf{Bold fonts} indicate the best results. We can see SUPMER achieves better performance.}
    \vspace{-1em}
\end{table*}

% main table 2
\begin{table}[t]
    \centering
    \footnotesize
    \begin{adjustbox}{width=0.48\textwidth}
    \begin{tabular}{l cccccc}
    \toprule
    \multirow{2}*{\textbf{Method}} & \multicolumn{1}{c}{Source} & \multicolumn{5}{c}{Target} \\
    \cmidrule(lr){2-2} \cmidrule(lr){3-7} & \textbf{B} & \textbf{D} & \textbf{E} & \textbf{K} & \textbf{A} & \textbf{R}\\
    \midrule
    FT & $84.4_{0.2}$ & $83.9_{0.6}$ & $81.0_{0.6}$ & $84.1_{0.6}$ & $85.0_{0.7}$ & $85.3_{0.6}$\\  
    PT & $79.8_{2.5}$ & $75.3_{2.2}$ & $76.0_{3.2}$ & $79.6_{2.0}$ & $79.8_{1.9}$ & $83.0_{1.8}$\\
    PPT & $81.9_{1.4}$ & $77.9_{3.5}$ & $83.6_{1.3}$ & $88.4_{1.0}$ & $89.1_{1.6}$ & $87.8_{1.5}$\\
    Unified-PPT & $80.9_{3.0}$ & $82.1_{2.0}$ & $76.8_{4.0}$ & $81.0_{4.5}$ & $82.2_{3.9}$ & $84.9_{3.3}$\\
    Sup-MetaPT & $\bm{86.1_{0.3}}$ & $84.3_{0.8}$ & $84.2_{0.7}$ & $89.4_{0.9}$ & $90.3_{0.6}$ & -\\
    \midrule
    SUPMER & $85.7_{0.5}$ & $\bm{85.3_{0.6}}$ & $\bm{85.1_{0.4}}$ & $\bm{90.3_{0.6}}$ & $\bm{91.1_{0.5}}$ & $\bm{90.4_{0.4}}$\\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{\label{tab:main_table_2}Results of domain generalization. }
    \vspace{-1.5em}
\end{table}

%  experiments
\section{Experiments}

%  sec 4.1
\subsection{Experimental Setup}
\label{sec: 4.1}
We evaluate our approach in two problem settings: 1) Few-shot learning with different NLP downstream tasks; 2) domain generalization. 

\paragraph{Few-shot Learning.} We consider 6 downstream tasks with 12 datasets: 1) the sentiment analysis datasets SST-2~\citep{sst}, SST-5~\citep{sst}, MR~\citep{mr} and CR~\citep{cr}; 2) the subjectivity classification dataset SUBJ~\citep{subj}; 3) the natural language inference datasets CB~\citep{cb} and RTE~\citep{superglue}; 4) the question answering dataset QNLI~\citep{qnli} and BoolQ~\citep{boolq}; 5) the word sense disambiguation dataset WiC~\citep{wic}; 6) the paraphrase detection datasets MRPC~\citep{mrpc} and QQP.\footnote{\url{https://quoradata.quora.com/}}

Following \citet{perfect}, for each dataset we sample 16 instances per label from the original training set to form training and validation sets. For SST-5, MR, CR, and SUBJ, we test on the original test sets, while for others, we test on the original validation set.

\paragraph{Domain Generalization.} And then we design a more challenging problem about zero-shot domain generalization in the sentiment analysis task. Our experiments include 6 domains across 3 datasets similar to \citet{docogen}: 1) the Amazon review dataset~\citep{amazon} containing reviews from four domains - Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K); 2) the airline review dataset (A)~\citep{airline1,airline2}; 3) the restaurant (R) domain obtained from the Yelp dataset~\citep{yelp}. 
% , hoping to show that SUPMER can well generalize to out-of-distribution data. 

We choose B as the source domain and the other five (D, E, K, A, R) constitute the target domains. On this basis, we sample 16 instances per label from the training set of the source domain to tune soft prompts. And then we use the soft prompts learned from the source domain to evaluate performance on the test set of each domain.

% sec 4.2
\subsection{Experimental Details}
\paragraph{Our method.} In the meta-training stage, we sample 10GB data from OpenWebText~\citep{openwebtext}, a large-scale unlabeled corpus, to construct self-supervised meta-training tasks and tune the soft prompts together with the meta-gradient regularization parameters. While in the prompt-tuning stage for downstream tasks, the soft prompts are the only tunable parameters.

\paragraph{Baselines.} We mainly compare with full-model tuning (FT), vanilla prompt tuning (PT), pre-trained prompt tuning (PPT), unified pre-trained prompt tuning (Unified-PPT), and supervised meta-learned prompt tuning (Sup-MetaPT).

\textbf{FT} is the standard task-oriented fine-tuning without adding soft prompts, which fine-tunes all parameters of the pre-trained language model.

\textbf{PT}~\citep{prompt_tuning} directly tunes the soft prompts in the downstream task, which are randomly initialized from a normal distribution.
% , without a well-designed initialization process.

\textbf{PPT}~\citep{ppt} pre-trains soft prompts in a self-supervised way with 3 formats of pre-training tasks: sentence-pair classification, multiple-choice classification and single-text classification.

\textbf{Unified-PPT}~\citep{ppt} formulate all three formats of pre-training tasks into a unified task form on the basis of PPT.

\textbf{Sup-MetaPT} imitates the idea of ~\citet{metapt}, using a supervised sentiment
analysis dataset Yelp5 as the meta-training data and directly leveraging MAML to initialize soft prompts.

All experiments are built on the T5 base model from HuggingFace~\citep{transformers} so that we solve all downstream tasks in a text-to-text format. And we run each experiment with 5 different random seeds. For prompt tuning, we follow \citet{prompt_tuning} to design soft prompts composed of 100 soft tokens. As a result, the tunable parameters are only $100\times768\approx77\text{K}$, far less than full-model tuning (220M). Besides, following \citet{ppt}, we also combine some carefully designed hard prompts with soft prompts during prompt tuning. We give more details of the training hyper-parameters in the Appendix \ref{app:training_detail}.

%  sec 4.3
\subsection{Main Result}
Table \ref{tab:main_table_1} and Table \ref{tab:main_table_2} show the main results of few-shot learning and domain generalization. We have the following observations from the results.

First, in few-shot learning, SUPMER achieves better performance than all baselines on 9 of 12 datasets. And the average accuracy of SUPMER over all datasets reaches 69.7\%, improving the performance by +8.5 points compared to vanilla PT, +5.0 points compared to PPT, +4.8 points compared to Unified-PPT, +2.9 points compared to Sup-MetaPT and +1.2 points to FT.

Specifically, SUPMER outperforms all other prompt tuning methods (i.e., vanilla prompt tuning, pre-trained prompt tuning, and supervised meta-learned prompt tuning) over almost all datasets significantly. This means our method provides soft prompts with a good few-shot generalization ability. And SUPMER also outperforms full-model tuning on most datasets besides SubJ, QNLI and BoolQ. It indicates that there remains a gap between the objectives of PLMs and downstream tasks. And our method bridges the gap equally well.

Second, SUPMER is superior to all baselines in almost all domain-generalization setups. It outperforms vanilla / pre-trained prompt tuning, also achieving better results than full-model tuning on average (88.0\% v.s. 84.0\%). SUPMER even exhibits average gains of 0.6\% compared to Sup-MetaPT, which meta-trains soft prompts with a supervised sentiment analysis dataset. So we can say that SUPMER shows stronger robustness to domain shifts, and self-supervised meta-learning can help better generalize to unseen tasks or domains compared to supervised meta-learning.

Third, SUPMER also results in lower variances on most of the datasets. Few-shot learning is often notorious for its instability, especially when tuning smaller parameters such as prompt tuning. And in our method, we reduce the variance and keep few-shot prompt tuning more stable.

Furthermore, we discuss how the performance of SUPMER and other baselines varies when the number of training samples increases on SST-5 and SUBJ. As shown in Figure~\ref{fig:diffshot}, when the number of training samples per label grows from 4 to 64, SUPMER is consistently better than other prompt tuning methods. And the performance gap between these methods is gradually reduced as the number of training data increases. 

We also show the performance trend for each method after different training steps on CB and MRPC. Figure~\ref{fig:diffstep} illustrates that the performance of few-shot prompt tuning varies considerably because it may easily result in overfitting. While SUPMER not only accelerates the convergence to the optimal performance realizing fast adaptation, but also alleviates overfitting and holds its ideal performance with the increase in training steps.

\begin{figure}[t]
    \includegraphics[width=\linewidth]{figures/diffshot.pdf}
    \caption{The performance on SST-5 and SUBJ when different numbers of training samples are available.}\label{fig:diffshot}
\vspace{-1em}
\end{figure}

\begin{figure}[t]
    \includegraphics[width=\linewidth]{figures/diffstep.pdf}
    \caption{The performance after different training steps on CB and MRPC.}\label{fig:diffstep}
% \vspace{-2em}
\vspace{-1em}
\end{figure}

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{@{}llcc@{}}
        \toprule
        & Methods & Few-shot Learning        & DG  \\ \midrule
        1 & only sp                   & $65.9$ & $85.7$ \\
        2 & only mc                   & $66.5$ & $85.7$ \\
        3 & only ss              & $65.7$ & $86.7$ \\ \midrule
        4 & w/o ta           & $67.0$ & $85.3$ \\ 
        5 & w/o curriculum  & $68.3$ & $87.0$ \\ \midrule
        6 & w/o mgr   & $67.9$ & $86.1$ \\ \midrule
        7 & SUPMER  & $\bm{69.7}$ & $\bm{88.0}$ \\ \bottomrule
    \end{tabular}
    \caption{Results of ablation study. We report the average accuracy over all 12 datasets in few-shot learning and all 6 domains of domain generalization (Denoted as DG).}
    \label{tab:ablation}
\vspace{-1em}
\end{table}

%  sec 4.4
\subsection{Ablation Study}
\label{sec: 4.4}
In this section we conduct an ablation study to explore the impact of each component. We train the following ablation models. 1) only sp / mc / ss: we retain sentence-pair classification / multi-choice classification / single-sentence classification as the only anchor meta-training task format. 2) w/o ta: we entirely remove the task augmentation method. 3) w/o curriculum: we only retain the vanilla task augmentation without the curriculum-based idea. 4) w/o mgr: We remove the meta-gradient regularization method. All experiment follows the settings in \S \ref{sec: 4.1}. We report the average accuracy over all 12 datasets of few-shot learning and all 6 domains of domain generalization in Table \ref{tab:ablation}. More detailed results and analysis of ablation study can be found in Appendix \ref{app:ablation_result}.

The results of Row 1, Row 2 and Row 3 indicate that considering diversified task formats during meta-training helps efficiently generalize to different tasks, because downstream tasks often contain various task formats. Also, the results of Row 4 and Row 5 illustrate that task augmentation plays an essential role in our framework. And curriculum-based task augmentation further enriches the task distribution and simulates the distribution shift more reasonably. Moreover, the result of Row 6 validates the superiority of meta-gradient regularization. It helps prevent prompt tuning overfitting to some domain-specific correlations, thus achieving better performance.

