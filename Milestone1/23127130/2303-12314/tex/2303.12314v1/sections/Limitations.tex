\section*{Limitations}

Although SUPMER performs superbly in a variety of problem scenarios, there still exist some limitations in our work: 1) Our experiments are only conducted on English tasks, and also do not involve some kinds of NLP tasks such as language generation. 2) SUPMER mainly focuses on improving the few-shot performance and the generalization ability of prompt tuning, while there exist various of other parameter-efficient tuning methods, e.g., Adapter~\citep{adapter1}, LoRA~\citep{lora} and BitFit~\citep{bitfit}. These approaches have not been mentioned in our work so far.

To address these limitations, in the future we plan to evaluate the few-shot performance of our framework in the multilingual setting and also consider some other kinds of tasks, such as language generation and relation extraction. And another direction is to migrate the critical ideas of SUPMER to other parameter-efficient tuning methods. In summary, we hope our work could pave the way for future research on better leveraging parameter-efficient methods under few-shot settings. 
