\section{Introduction}

Recent NLP accomplishments witnessed the rapid development of pre-trained language models (PLMs) (e.g., BERT~\citealp{bert}; T5~\citealp{t5}; GPT3~\citealp{gpt3}). Fine-tuning methods, leveraging these PLMs, which tune the entire model parameters, have achieved outstanding performances in various NLP tasks. There are two mainstream fine-tuning methods: task-oriented fine-tuning and prompt-oriented fine-tuning~\citep{prompt_oriented}. The former adapts PLMs to downstream tasks with task-specific training objects, while the latter inserts a piece of text to the input and makes fine-tuning more similar to the pre-training objectives. For example, in the sentiment analysis task we could append the template ``\textit{It was $\left\langle \text{X} \right\rangle$.}'' to the input sentence and predict sentiment words, e.g., ``great'' or ``terrible'', at the mask position with PLMs, formulating the downstream task as a masked language modeling problem.

\begin{figure}[t]
    \includegraphics[width=\linewidth]{figures/PtAcc.pdf}
    \caption{Few-shot performance of prompt tuning on dataset CB (a) with different random seeds (b) after different training steps.}\label{fig:ptvsft}
\vspace{-1em}
\end{figure}

However, as the pre-trained model scale increases, tuning the entire model parameters would be unaffordable. To address this problem, some recent works~\citep{adapter1, lora} focus on parameter-efficient tuning. And \citet{prompt_tuning} proposes prompt tuning (PT), which freezes all PLM parameters and only prepends some additional tunable tokens called soft prompts to the input text. So we can efficiently adapt PLMs to downstream tasks. Though PT is an efficient and effective paradigm, \citet{ppt} shows it performs much worse than fine-tuning under few-shot settings. We argue that the performance is not satisfactory mainly for two reasons: 1) The performance of PT is quite sensitive to the soft prompt initialization especially for few-shot downstream tasks. As shown in Figure~\ref{fig:ptvsft} (a), initializing soft prompts with different random seeds leads to significant performance variations. 2) Few-shot prompt tuning risks learning spurious correlations and may easily result in overfitting based on the biased distribution triggered by a few training examples \citep{freelunch}. As shown in Figure~\ref{fig:ptvsft} (b), the increase in training steps causes a sharp fall in the performance of few-shot PT. To better leverage PT under few-shot settings, efforts need to be made to address these challenges.

Recent research mainly focused on the first challenge, leveraging pre-training or supervised meta-learning for soft prompt initialization. A pre-trained prompt tuning method (PPT)~\citep{ppt} is proposed from the beginning, which uses some self-supervised tasks to pre-train soft prompts and then applies them in the few-shot scenario. However, PPT suffers from a train-test mismatch between the pre-training data and the downstream data, without explicitly optimizing the fast adaptation ability of the model. So it limits generalization to unseen few-shot tasks, especially when significant differences exist in the task format or task domain between the pre-training task and the downstream task. MetaPrompting~\citep{metaprompting}, as another effort, seeks assistance from model-agnostic meta-learning (MAML)~\citep{maml} for fast adaptation in few-shot settings. It initializes soft prompts in a supervised way and splits each supervised target dataset into disjoint parts. One part is used for prompt initialization while another is considered as the downstream task. But in the practical few-shot scenario, it is nearly impossible to capture enough labeled data relevant to downstream tasks, e.g., MetaPrompting also needs to reinitialize soft prompts for different downstream tasks with weak task transferability in such a supervised way. Furthermore, all these existing works ignore the second challenge, that is, few-shot prompt tuning may easily result in overfitting.

To address the shortcomings of existing works, we propose ~\textbf{SUMMER}, a \textbf{\underline{S}}elf-s\textbf{\underline{U}}pervised \textbf{\underline{M}}eta-prompt learning framework with \textbf{\underline{ME}}ta-gradient \textbf{\underline{R}}egularization for few-shot generalization. To address the first challenge, we design a novel self-supervised meta-learning method for prompt initialization, which automatically generates a diverse set of meta-training tasks with different task formats from large-scale unlabeled corpora and explicitly learns to fast adapt across these tasks. Specifically, we first design some self-supervised anchor meta-training tasks with three task formats (i.e., sentence-pair classification, multi-choice classification, and single-sentence classification). Then we propose a curriculum-based task augmentation method to enhance model generalization, which can further enrich the task distribution dynamically in terms of the current model capability.

For the second issue, we integrate a meta-gradient regularization method into meta-prompt learning. As we simulate distribution shift through task augmentation, the meta-gradient regularization parameters are jointly optimized to align gradient directions across different distributions during the proposed meta-optimization paradigm. Then it can be applied to transform the raw gradients during few-shot learning into a domain-generalizable direction, preventing prompt tuning overfitting to some domain-specific correlations.

Overall, our contributions are mainly three-fold:

(1) We design a novel self-supervised meta-prompt learning method, where various anchor meta-training tasks with different formats are constructed and curriculum-based task augmentation is further proposed to enrich the task distribution. 

(2) We incorporate a novel meta-gradient regularization method into our meta-prompt learning framework, which meta-learns to transform the raw gradient during few-shot learning into a domain-generalizable direction, preventing prompt tuning overfitting to domain-specific correlations.

(3) Comprehensive experiments on few-shot learning and domain generalization validate the superiority of our method, which even outperforms full-model tuning in few-shot learning. It also exhibits a stronger domain generalization ability.