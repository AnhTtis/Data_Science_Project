\section{Conclusion}

In this paper, we present SUMMER, a self-supervised meta-prompt learning framework with meta-gradient regularization for few-shot generalization. SUMMER leverages a novel self-supervised meta-learning method for prompt initialization, which contains a diverse set of anchor meta-training tasks with different formats and further enriches the task distribution with curriculum-based task augmentation. Then a meta-gradient regularization method is integrated into meta-prompt learning, preventing prompt tuning overfitting to domain-specific correlations. Extensive experiments on few-shot learning and domain generalization show that ~{SUMMER} outperforms other prompt tuning baselines and full-model tuning, achieving state-of-the-art performance. 