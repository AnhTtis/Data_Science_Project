\section{Method}
In this section, we describe the whole framework of SUMMER (shown in Figure~\ref{fig:main}). With pre-defined preliminaries, we first introduce the way to construct anchor self-supervised meta tasks and the foundation of task augmentation to densify task distributions. Then we elaborate the SUMMER model, including the meta-gradient regularization method. Finally, we upgrade the original task augmentation method into a curriculum-based one. Besides, we formalize all tasks in a text-to-text format following the T5 fashion~\citep{t5}.

% sec 3.1
\subsection{Preliminaries}
\label{sec: 3.1}
% prompt tuning
\paragraph{Prompt Tuning.} In prompt tuning~\citep{prompt_tuning}, given a training sample $(x_i, y_i)$  from task $\mathcal{D}_{\tau}$, we apply a prompt template $P$ converting $x_i$ into a new sequence $P(x_i)$ and then concatenate a set of soft prompts $\theta$ to the beginning of $P(x_i)$. And verbalizer $\mathcal{V}$ plays a role in mapping $y_i$ to some corresponding label tokens $\mathcal{V}(y_i)$ in the vocabulary of PLMs. So the objective of prompt tuning can be formulated as follows:
\begin{equation}
\small
\begin{aligned}
    &\arg\min_{\theta} \mathcal{L}_{\mathcal{D}_{\tau}}(f_\theta) \\
    &= \arg\max_{\theta}\sum_{(x_i, y_i) \in {D}_{\tau}} \log p\big(\left\langle \text{X} \right\rangle = \mathcal{V}(y_i) | [\theta; P(x_i)]; \theta \big)
\end{aligned}
\end{equation}
where $f_\theta$ is the PLM with soft prompt embedding $\theta$ as the only tunable parameters. $\left\langle \text{X} \right\rangle$ let PLMs predict target tokens at the masked positions and $[\cdot;\cdot]$ is the concatenation operation.

% maml
\paragraph{Model-Agnostic Meta-Learning.} Assuming access to a task distribution $p(\mathcal{T})$, the goal of meta-learning is to utilize tasks $\tau_i \sim p(\mathcal{T})$, referred to as meta-training tasks or meta tasks, to train a learning procedure that generalizes to unseen tasks from the distribution. Model-Agnostic Meta-Learning (MAML)~\citep{maml} is a gradient-based bi-level optimization meta-learning method, which consists of an inner loop task-specific learning and outer loop fast adaptation across tasks.

Specifically, a task $\tau$ is composed of the support set $\mathcal{D}^s_{\tau}$ and the query set $\mathcal{D}^q_{\tau}$.
In the inner loop of MAML, a model $f_\theta$ learns to adapt to a new task $\tau_i$ using its support set in the following way:
\begin{equation}
\label{eq:inner_loop}
\small
\theta_i^\prime = \theta - \alpha_1 \nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta)
\end{equation}
where $\alpha_1$ is the inner loop learning rate and $\theta$ is the model's parameters. And the optimized parameters $\theta_i^\prime$ is then evaluated on the query set of task $\tau_i$ with the loss function $\mathcal{L}_{\mathcal{D}_{\tau_i}^q}$. In the outer loop, this loss across meta-training tasks is treated as the final training loss to update $\theta$:
\begin{equation}
\label{eq:outer_loop}
\small
\theta \gets \theta - \beta_1\nabla_{\theta} \sum_{\tau_i\sim p(\mathcal{T})} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}(f_{\theta_i^\prime})
\end{equation}
where $\beta_1$ is the outer loop learning rate.

% sec 3.2
\subsection{Constructing Anchor Meta Tasks}
\label{sec: 3.2}
Supervised datasets with a large amount of labeled data are often unavailable in many NLP tasks. While unlabeled data is more easily accessible and usually covers broader semantic concepts. So we utilize the unlabeled data from a large corpus to create anchor self-supervised meta-training tasks. 

The unlabeled data are first grouped into different clusters. We can derive semantically meaningful sentence embeddings with PLMs from sentences in the corpus. And we apply unsupervised K-means to cluster these unlabeled sentences according to their embeddings. Based on the results of K-means and considering the diversity of downstream tasks, we design three different formats of self-supervised meta-training tasks: sentence-pair classification, multi-choice classification, and single-sentence classification.

Specifically, \textbf{sentence-pair classification}, which takes a pair of sentences as input, includes next sentence prediction task and sentence similarity task. The former predicts whether two sentences are adjacent or from the same document, and the latter predicts whether two sentences source from the same cluster after K-means clustering. And \textbf{multi-choice classification} takes an anchor sentence as the query together with several candidate sentences. From all answer candidates, we need to select the correct sentence adjacent to the query or coming from the same cluster as the query. Finally in \textbf{single-sentence classification}, we are required to associate each sentence with the correct cluster label after K-means clustering.

On this basis, for each task format, we separate all data into different tasks to construct anchor meta-training tasks with good task distributions. We group samples containing similar embeddings into the same task based on K-means. And we give a more detailed description of anchor meta-training task construction in Appendix A.2.

% sec 3.3
\subsection{Vanilla Task Augmentation}
\label{sec: 3.3}
After getting a set of anchor meta-training tasks, in this section we first introduce the vanilla task augmentation to densify the task distribution. Extending the idea of mixup~\citep{mixup}, we augment the task set through task interpolation, which linearly combines features and corresponding labels of samples from the query set in different tasks. In \S \ref{sec: 3.5} we further upgrade the vanilla task augmentation method into a curriculum-based one, which dynamically controls the task interpolation in terms of the current model capability.

Specifically, for a task composed of the support set and the query set, we denote the hidden representations of samples from the query set in task $\tau_k$ as $\bm{H}^{q}_k = f_\theta^{enc}(x^{q}_k)$, where $f_\theta^{enc}$ indicates the T5 encoder. Given an anchor task $\tau_i$, first we randomly select another task $\tau_j$. We keep the support set of $\tau_i$ and reconstruct the query set by interpolating on the hidden representations $(\bm{H}^{q}_i, \bm{H}^{q}_j)$ and corresponding labels $(\bm{Y}^{q}_i, \bm{Y}^{q}_j)$ of the query sets in $\tau_i$ and $\tau_j$. Because we solve all tasks in a text-to-text format via T5, the labels of all tasks share the same label space and task interpolation can be directly performed with mixup:
\begin{equation}
\label{eq:mixup}
\small
\begin{aligned}
    &\tilde{\bm{H}}_{i}^{q} = (1-\lambda) \bm{H}^{q}_i + \lambda \bm{H}^{q}_j,\ \tilde{\bm{Y}}_{i}^{q} = (1-\lambda) \bm{Y}^{q}_i + \lambda \bm{Y}^{q}_j
\end{aligned}
\end{equation}
where the mixing ratio $\lambda \in [0,1]$ is drawn from the Beta distribution $Beta(\alpha, \alpha)$, and $\alpha$ is the hyper-parameter. The process of task augmentation not only enriches the task distribution, but also simulates the distribution shift between the support set and the query set within one task, as we only leverage interpolation between the query sets of different anchor meta-training tasks. And in \S \ref{sec: 3.4} we will show the effect of the distribution deviation.

% sec 3.4
\subsection{Meta-Prompt Learning with Meta-Gradient Regularization}
\label{sec: 3.4}
With a set of meta-training tasks after task augmentation, in this section we introduce the algorithm of our meta-prompt learning framework with a simple but effective meta-gradient regularization method to avoid overfitting. 

Specifically, before the gradient descent update in the inner loop of MAML, the learnable gradient regularization method makes a dynamic and direct transformation $\psi_\phi(\cdot)$ to the original gradients obtained from the support set $\mathcal{D}_{\tau_i}^{s}$. Taking the gradients $g$ as input, it first applies the affine transformation (such as gradient rotation) to modify $g$ into $h(g)$. And then an update gate vector $z$ is leveraged to combine $g$ and $h(g)$ into the final gradient:
\begin{equation}
\label{eq:gradient_trans}
\small
\psi_{\phi}(g) = z\cdot h(g) + (1-z) \cdot g
\end{equation}
Obviously, the value of $z$ can be used to control how much the modified gradients $h(g)$ contribute to the output of $\psi_{\phi}(g)$. We hope to let the input samples determine this weight and set $z = \sigma(W\bm{H}+b)$, where $\bm{H}$ is the hidden representations of input samples. And then we can transform Eq.~(\ref{eq:inner_loop}) into:
\begin{equation}
\small
\theta_i^\prime = \theta - \alpha_1 \psi_\phi(\nabla_{\theta} \mathcal{L}_{\mathcal{D}_{\tau_i}^{s}}(f_\theta))
\end{equation}
After adapting the soft prompt embeddings $\theta$ to $\mathcal{D}_{\tau_i}^{s}$, in the outer loop we make a real optimization to $\theta$ with the query sets via Eq.~(\ref{eq:outer_loop}). Besides, we should also optimize the meta-gradient regularization parameters $\phi$ to make a better gradient transformation. Here we temporarily utilize the same loss which optimizes the soft prompt embeddings to update $\phi$, with $\beta_2$ as the learning rate:  
\begin{equation}
\label{eq:para_upt_2}
\small
\begin{aligned}
\phi \gets \phi - \beta_2\nabla_{\phi}  \sum_{\tau_i\sim p(\mathcal{T})} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}(f_{\theta_i^\prime})   
\end{aligned}
\end{equation}

\paragraph{Analysis of SUMMER.} Here we give some analysis to help better understand SUMMER, especially the meta-gradient regularization method. Focusing on one meta-training task, we can denote the objective of SUMMER as follows for convenience:
\begin{equation}
\label{eq:obj}
\small
\arg\min_{\theta,\phi} \mathcal{L}_1\big(\theta-\alpha_1 \psi_\phi (\nabla_{\theta} \mathcal{L}_{0}(\theta))\big)
\end{equation}
where $\mathcal{L}_0 (\cdot)$ is the loss from the support set and $\mathcal{L}_1 (\cdot)$ is that from the query set. Assuming we have $x=\theta-\alpha_1 \psi_\phi (\nabla_{\theta} \mathcal{L}_{0}(\theta))$ and $x_0=\theta$, we apply a Taylor series expansion to Eq.~(\ref{eq:obj}):
\begin{equation}
\small
\begin{aligned}
\because \ &\mathcal{L}_1(x) = \mathcal{L}_1(x_0) + \mathcal{L}_1^{\prime}(x_0)(x-x_0) \\
\therefore \ &\arg\min_{\theta,\phi} \mathcal{L}_1\big(\theta-\alpha_1 \psi_\phi (\nabla_{\theta} \mathcal{L}_{0}(\theta))\big) \\
= &\arg\min_{\theta,\phi} \mathcal{L}_1(\theta) + \nabla_{\theta} \mathcal{L}_{1}(\theta) \cdot \big(-\alpha_1 \psi_\phi (\nabla_{\theta} \mathcal{L}_{0}(\theta))\big) \\
= &\arg\min_{\theta,\phi} \mathcal{L}_1(\theta) - \alpha_1 \nabla_{\theta} \mathcal{L}_{1}(\theta) \cdot  \psi_\phi \big(\nabla_{\theta} \mathcal{L}_{0}(\theta)\big)
\end{aligned}
\end{equation}
From what has been mentioned above, we can reach the following conclusions: (1) The update of $\theta$ minimizes the expected loss on the query set. (2) The optimization of both $\theta$ and $\phi$ maximizes the inner product between the gradients from the support set after transformation and the gradients from the query set. The inner product of two vectors is larger if they are in a similar direction. Since we simulate the distribution shift between the support set and the query set, the optimization of $\theta$ and $\phi$ tries to align the gradient directions across different distributions. Or we can say, the parameters descend in a coordinated way such that the input-output correspondence is as close as possible across two distributions with deviation. We give a more complete analysis and conduct gradient inner product tracking experiments in Appendix A.1.

On this basis, through meta-training, the meta-gradient regularization parameters can retain some domain-invariant information of meta-training data, and learned to encourage the alignment between the domain-specific gradients. In the downstream prompt tuning stage, the meta-gradient regularization parameters are fixed. We use them to transform the gradients obtained from the few-shot downstream samples and then tune soft prompts via gradient descent, which better avoids overfitting.

%  sec 3.5
\subsection{Curriculum-based Task Augmentation}
\label{sec: 3.5}
In \S \ref{sec: 3.4} we show that SUMMER can help align the optimization direction across two distributions with deviation, which is simulated via performing task augmentation only for support sets. From Eq.~(\ref{eq:mixup}) it isn't hard to notice that the mixing ratio $\lambda$ of mixup controls the degree of the distribution deviation: the deviation is more notable with a larger $\lambda$. However, in the method discussed so far, $\lambda$ is drawn from a fixed Beta distribution. We hope to sample it in a more flexible way to make the distribution shift more reasonable.

In this section we upgrade the original task augmentation method into a curriculum-based one, gradually increasing the task difficulty. It dynamically adjusts the parameters of the Beta distribution from which we sample the mixing ratio $\lambda$. Specifically, a batch of meta tasks is sampled in each training epoch. For each task, we can obtain gradients on the support set $g_i^s$ and gradients on the query set $g_i^q$ together with their cosine similarity. We leverage the average cosine similarity $s_{k-1}$ of all tasks in a batch during the last epoch to construct the mixing ratio $\lambda_k$ of the current epoch $k$:

\begin{equation}
\label{eq:dynamic_mixup}
\small
\begin{gathered}
    \lambda_k = Beta(\alpha, b_{k} \alpha) \\
    b_k = \frac{m^{\frac{1+s_{k-1}}{2}}-1}{m-1}, \\ \text{where}\ s_{k-1} = \frac{1}{\lvert \mathcal{B} \rvert}\cdot \sum_{i=1}^{\lvert \mathcal{B} \rvert} \frac{ g_i^s \cdot g_i^q }{\Vert g_i^s \Vert \cdot \Vert g_i^q \Vert}
\end{gathered}
\end{equation}
where $m$ is the curve parameter. In this way, when our model is not capable of aligning the optimization directions across different distributions at the beginning, a smaller $\lambda$ is preferable to create a smaller distribution deviation. Then the value of $\lambda$ tends to gradually increase with the improvement of the model capability, producing a larger distribution deviation so that raising the task difficulty. 

On this basis, we also attach some additional restrictions when updating the meta-gradient regularization parameters. Note that the greater the value of $b_k$ in Eq.~(\ref{eq:dynamic_mixup}), the higher the probability that there exists a larger distribution deviation between the support set and the query set. At the same time, In Eq.~(\ref{eq:gradient_trans}) we also tend to enlarge $z$ to make a larger gradient modification with a more notable distribution deviation. From this perspective, $z$ has a similar monotonicity with $b_k$, and they both vary between 0 and 1. Thus we further add a regularized loss $\mathcal{L}_{reg} = \Vert z - b_k \Vert^2$ to constrain the value of $z$ and modify Eq.~(\ref{eq:para_upt_2}) into:
\begin{equation}
\small
\begin{aligned}
\phi \gets \phi - \beta_2\nabla_{\phi} \big( \sum_{\tau_i\sim p(\mathcal{T})} \mathcal{L}_{\mathcal{D}_{\tau_i}^{q}}(f_{\theta_i^\prime})  +\lambda \mathcal{L}_{reg}\big)
\end{aligned}
\end{equation}
So far, we have completed the description of the whole SUMMER framework. And we give the pseudo-codes of SUMMER in Appendix A.3.







