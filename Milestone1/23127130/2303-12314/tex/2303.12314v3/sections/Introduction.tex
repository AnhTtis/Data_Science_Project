\section{Introduction}

Recent NLP accomplishments witnessed the rapid development of pre-trained language models (PLMs) (e.g., BERT~\citealp{bert}; T5~\citealp{t5}; GPT3~\citealp{gpt3}). Fine-tuning, which tunes the entire PLM parameters, has achieved outstanding performances in various NLP tasks. However, as the pre-trained model scale increases, tuning the entire set of parameters would be sometimes unaffordable. More recently, prompt-based methods, which simply insert a piece of carefully designed text to the input (\textit{e.g.}, ``\textit{It was $\left\langle \text{X} \right\rangle$.}'') and predict target words (\textit{e.g.}, ``great'' or ``terrible'') at the mask position with frozen PLMs, have demonstrated remarkable effectiveness. But it has been observed that the performance of prompt-based methods is greatly affected by the design of prompts. In light of this, prompt tuning (PT~\citealp{prompt_tuning}), as a parameter-efficient tuning method, is proposed to only prepends some additional learnable tokens called soft prompts to the input text, with all PLM parameters freezing.

\begin{figure}[t]
    \includegraphics[width=\linewidth]{figures/PtAcc.pdf}
    \caption{Few-shot performance of prompt tuning on dataset CB (a) with different random seeds (b) after different training steps.}\label{fig:ptvsft}
\vspace{-1em}
\end{figure}

Though prompt tuning is an efficient and effective paradigm, \citet{ppt} shows it performs much worse than fine-tuning under few-shot settings. We argue that the performance is not satisfactory mainly due to two limitations: 1) The performance of PT is highly sensitive to the soft prompt initialization especially for few-shot tasks. As shown in Figure~\ref{fig:ptvsft} (a), initializing soft prompts with different random seeds leads to significant performance variations. 2) Few-shot PT risks learning spurious correlations and is prone to overfitting based on the biased distribution triggered by a few training examples~\citep{freelunch}. As shown in Figure~\ref{fig:ptvsft} (b), the performance of few-shot vanilla PT degrades in the final training steps while the training accuracy still continues to improve. 

Recent research mainly focused on the first limitation, leveraging pre-training or supervised meta-learning for soft prompt initialization. A pre-trained prompt tuning method (PPT)~\citep{ppt} is proposed from the beginning, which utilizes self-supervised tasks to pre-train soft prompts and then applies them in the few-shot scenario. However, without explicitly optimizing the fast adaptation ability of the model, PPT suffers from a train-test mismatch between the pre-training data and the downstream data. So it limits generalization to unseen few-shot tasks, especially in cases where there is a significant disparity in task domains or formats. MetaPrompting~\citep{metaprompting}, as another effort, seeks assistance from model-agnostic meta-learning (MAML)~\citep{maml} for fast adaptation in few-shot settings. It initializes soft prompts specifically for different downstream tasks with relevant supervised data. But in practical few-shot scenarios, acquiring the required supervised data poses significant challenges and the resulting task-specific prompt initialization has weak task transferability. Furthermore, all these existing works ignore the second limitation, i.e., the propensity for few-shot prompt tuning to lead to overfitting.

To address the shortcomings of existing works, we propose ~\textbf{SUPMER}, a \textbf{\underline{S}}elf-s\textbf{\underline{U}}pervised meta-\textbf{\underline{P}}rompt learning framework with \textbf{\underline{ME}}ta-gradient \textbf{\underline{R}}egularization. It leverages self-supervised meta-learning to universally learn an efficient soft prompt initialization, also with a meta-gradient regularization function to mitigate overfitting. This comprehensive process only requires a one-time execution and enables seamless adaptation to different downstream few-shot tasks, while also facilitating faster convergence for downstream prompt tuning.

Specifically, to address the first limitation, we design a novel self-supervised meta-learning method for prompt initialization, which automatically generates a diverse set of meta-training tasks from large-scale unlabeled corpora and explicitly learns to fast adapt across these tasks. To ensure meta-training task diversity, we initially design a collection of anchor self-supervised meta-training tasks with different formats. And then a curriculum-based task augmentation method is further proposed to enrich the task distribution dynamically in terms of the current model capability.

For the second issue, we integrate a meta-gradient regularization method into meta-prompt learning. As we simulate distribution shift through task augmentation, the meta-gradient regularization parameters are jointly optimized to align gradient directions across different distributions within our proposed meta-prompt learning paradigm. Consequently, in downstream tasks, these optimized parameters can be directly utilized to transform raw gradients over few-shot samples into a domain-generalizable direction, preventing prompt tuning overfitting to some domain-specific correlations.

Overall, our contributions are mainly three-fold:

(1) We propose a novel self-supervised meta-prompt learning framework to better initialize soft prompts, where only unlabeled pre-training data are used to construct different meta-training tasks with curriculum-based task augmentation for further task enrichment.

(2) We incorporate a novel meta-gradient regularization method into our meta-prompt learning framework, which meta-learns to transform the raw gradient during few-shot learning into a domain-generalizable direction, thus preventing prompt tuning overfitting to domain-specific correlations.

(3) Comprehensive experiments on few-shot learning and domain generalization validate the superiority of our method, which even outperforms full-model tuning in few-shot learning. It also exhibits a stronger domain generalization ability.