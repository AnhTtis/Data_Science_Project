{
    "arxiv_id": "2303.12314",
    "paper_title": "Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization",
    "authors": [
        "Kaihang Pan",
        "Juncheng Li",
        "Hongye Song",
        "Jun Lin",
        "Xiaozhong Liu",
        "Siliang Tang"
    ],
    "submission_date": "2023-03-22",
    "revised_dates": [
        "2023-05-23"
    ],
    "latest_version": 3,
    "categories": [
        "cs.CL",
        "cs.LG"
    ],
    "abstract": "Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily overfit to few-shot training samples, thereby undermining generalizability. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they fail to data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-supervised meta-prompt learning framework with meta-gradient regularization for few-shot generalization (SUPMER). SUPMER leverages self-supervised meta-learning with a diverse set of well-designed meta-tasks to learn a universal prompt initialization for efficient adaptation using only unlabeled data. Additionally, it jointly meta-learns a gradient regularization function to transform raw gradients into a domain-generalizable direction, thus alleviating the problem of overfitting. Extensive experiments show that SUPMER achieves better performance for different few-shot downstream tasks, and also exhibits a stronger domain generalization ability.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12314v1",
        "http://arxiv.org/pdf/2303.12314v2",
        "http://arxiv.org/pdf/2303.12314v3"
    ],
    "publication_venue": null
}