\section{Introduction}

The increasing availability of commodity RGB-D sensors, now widely available on iPhones as well as with the Microsoft Kinect or Intel RealSense, has enabled consumer-level capture of 3D geometry of real-world environments.
To enable applications in robotics, autonomous navigation, and mixed reality in such scenes, semantic 3D scene understanding is necessary. In particular, 3D instance segmentation is critical to 3D perception, providing both object localization and dense instance mask predictions, thus enabling physical and geometric reasoning about objects in an environment.
While various 3D deep learning approaches have been developed for 3D instance segmentation \cite{qi2017pointnet++,wu2019pointconv,wang2019dynamic,rethage2018fully,wang2018sgpn,hu2020randla,fan2021scf,occuseg, chen2021hierarchical,hou20193d,rozenberszki2022language,liang2021instance,vu2022softgroup,vu2022softgroup++,hui2022graphcut,kolodiazhnyi2023topdown,sun2022spformer,Schult23mask3d}, they require full supervision from expensive, manual, dense annotations on 3D scenes.

We thus propose \OURS{}, method for class-agnostic 3D instance segmentation, without requiring any human labels. 
That is, for a geometric reconstruction of a real-world 3D scene, we aim to identify objects in the environment by estimating their dense instance masks, without being constrained to a limited set of class categories.

\OURS{} comprises two essential elements.
First, we observe that for RGB-D scan data, self-supervised representation learning methods \cite{xie2020pointcontrast,hou2021exploring} can provide an innate signal indicating object-ness through feature similarity. 
We thus generate pseudo masks on 3D segment primitives, based on multimodal analysis of self-supervised color and geometry features from the RGB-D data.
By considering 3D segments rather than voxels or points, our approach efficiently scales with high-dimensional 3D data in large scene environments while providing inherent regularization for pseudo annotations.
As we require strong features for these initial coarse estimates, we fuse information from both geometric and color features in a complementary fashion.
Second, following the pseudo mask generation, we train our model through iterative self-training on both the initial pseudo masks and the current confident model predictions.
A noise robust loss enables multiple rounds of self-training to achieve improved object recognition and segmentation.
At inference time, we do not require any color signal and can produce class-agnostic 3D instance segmentation for a new geometric observation of a 3D environment.
Experiments on challenging, cluttered indoor environments from the ScanNet~\cite{dai2017scannet} and ARKit~\cite{dehghan2021arkitscenes} datasets show that \OURS{} improves significantly over the unsupervised state of the art. \\
%
In summary, our contributions are:
\begin{itemize}
	\item We propose the first unsupervised 3D instance segmentation approach for indoor RGB-D scans, without requiring any human annotation.
	\item We generate sparse 3D pseudo masks for unsupervised training based on a multi-modal fusion of color and geometric signal from RGB-D scan data. We achieve robustness and efficiency through a geometry-aware scene coarsening. 
	\item Our generated pseudo masks are iteratively refined by self-training with a noise robust loss for 3D instances to improve 3D instance segmentation performance.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{figures/pipelines/simple_pipeline.pdf}
    \vspace{-0.7cm}
    \caption[]{\OURS{} follows a two stage method, by first generating the coarse pseudo masks, then iteratively refine them with multiple cycles of self-training. } 
    \label{fig:simple_pipeline}
\end{figure}