UnScene3D Video Narration

Slide 1 (title):

We introduce UnScene3D, which predicts unsupervised 3D instance segmentation for indoor scenes

Slide 2 (Flythrough video):

% Longer alternative
Instance Segmentation is an essential task in 3D understanding.
Current methods rely on densely annotated 3D scenes for supervision.
We propose UnScene3D, the first fully unsupervised 3D instance segmentation method for cluttered indoor scenes. 

% Shorter alternative
Unlike current methods, that rely on dense annotation for instance segmentation, we propose UnScene3D, the first fully unsupervised 3D instance segmentation method for cluttered indoor scenes. 

% mention the comparison as the animations are coming  can edit it later
In comparison to classical oversegmentation, clustering, and graphcut-based refinement, our approach robustly identifies 3D object instances

Slide 3 (simple pipeline): 

We follow a two stage approach; we first generate a coarse pseudo mask dataset, and then iteratively refine predictions through a series of self-training cycles. 

Slide 4 (features):

We combine both self-supervised 3D and 2D features, and aggregate them on geometric scene segments.
This helps to characterize high-dimensional 3d data while regularizing feature information.

Slide 5:
From the aggregated features, we analyze their similarity matrix to extract pseudo masks for potential objects.

Slide 6 (show iteration of bipartitions):
We iteratively mask out the already predicted foregrounds and apply the same cut function to predict multiple pseudo instances of the scene. 

Slide 7 (simple pipeline again):
While these pseudo masks can already provide training signal, they are too sparse for reliable instance segmentation.

We thus employ a self-training loop to produce more dense, complete instance predictions.

Slide 8 (full pipeline):
We use a transformer-based 3D backbone, and train it with the pseudo masks using noise robust mask losses to predict a denser set of masks.

Slide 9 (full pipeline + self-training):
Then, we combine the new mask predictions with the initial pseudo masks to update the supervision set for another cycle of self training, and iterate.

Slide 10-13 ( self train cycle slides)
After multiple self-training cycles, we obtain a final set of clean, dense, class-agnostic 3D instances.

Slide 14 (modalities):
Our use of both 2D and 3D self-supervised features for pseudo mask generation is complementary. 
Both provide good results independently, with the best performance together.
Note that for self-training, we rely scene meshes as input. 

Limited data slides:
Our unsupervised training produces strong learned features that can be used as pre-training for downstream tasks with limited data available. 
When fine-tuned on ScanNet with limited annotated scenes, our method improves over training from scratch by 19\% average precision on only 1\% of data annotation, as well as improving notably over state-of-the-art 3d pre-training.

ScanNet result:
Our approach significantly outperforms traditional clustering and refinement methods.

ArKitScene slide:
As our method is unsupervised, we also demonstrate its performance on the ArkitScnes dataset, illustrating generality to various indoor datasets. 

Final flythrough slide:
We compare with geometric and graphcut-based baselines on challenging indoor scenarios. 

End:
Thanks for watching!

