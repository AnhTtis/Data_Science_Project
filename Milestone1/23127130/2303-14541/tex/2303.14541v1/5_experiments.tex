\section{Experiments}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{figures/results/full_results.jpg}
    \caption[]{
    Qualitative comparison on ScanNet~\cite{dai2017scannet} scenes with projected predictions from the 2D method CutLER~\cite{wang2023cut}, traditional clustering-based methods Felzenszwalb~\cite{felzenszwalb2004efficient} and HDBSCAN~\cite{mcinnes2017accelerated_hdbscan}, and the GraphCut-based cluster refinement method \cite{nunes2022unsupervised}.
    Our approach leverages strong pseudo mask prediction and a self-training strategy to produce cleaner, more accurate instance segmentation.} 
    \label{fig:full_results}
\end{figure*}

We demonstrate the effectiveness of \OURS{} for unsupervised class-agnostic 3D instance segmentation on challenging real-world 3D scan datasets containing a large diversity of objects and significant clutter.
We train our method and all learned baselines on ScanNet~\cite{dai2017scannet}, using the official train split.
Note that no semantic annotation data is used for training, only the RGB-D reconstructions.
Additionally, we show that our approach trained on ScanNet data can effectively transfer to class-agnostic 3D instance segmentation on ARKitScenes~\cite{dehghan2021arkitscenes} data.

\noindent \paragraph{Datasets.}
We train and evaluate \OURS{} on RGB-D scan data from ScanNet~\cite{dai2017scannet}, using the official train split.
We use the raw captured RGB images, and registered camera poses for training our approach, while the semantic annotations are used only for evaluation.
We use the official ScanNet train split for both the pre-trained 3D features from \cite{hou2021exploring} and our self-training iterations.
%
We additionally evaluate our method on ARKitScenes~\cite{dehghan2021arkitscenes}, on an 884/120 train/test split of indoor LIDAR scans. 
For ARKitScenes, we use 3D pre-trained features from ScanNet, followed by pseudo mask generation and self-training on the ARKitScenes train scenes.
We convert the LIDAR scan data to meshes with Poisson Surface Reconstruction~\cite{kazhdan2006poisson,kazhdan2013screened} prior to our graph coarsening.
Note that all baselines using learned features are trained on the same ScanNet data as ours.

\noindent \paragraph{Evaluation metrics.}
We evaluate class-agnostic 3D instance segmentation performance with the widely-used Average Precision score on the full-resolution mesh vertices. Following the strategy of the supervised benchmark \cite{dai2017scannet} we report scores at IoU scores of 25\% and 50\% (AP@25, AP@50) and averaged over all overlaps between [50\% and 95\%] at 5\% steps (AP). 
Note that since predictions are class agnostic, all methods evaluate only instance mask AP values without considering any semantic class labels.
For ScanNet, we evaluate against ground truth instance masks from the established 20-class benchmark.
Since ARKitScenes does not contain any ground truth instance mask annotations, we evaluate all methods qualitatively.

\paragraph{Comparison to the state of the art.}
We evaluate our approach in comparison to state-of-the-art traditional clustering methods HDBSCAN~\cite{mcinnes2017accelerated_hdbscan} and Felzenszwalb's algorithm~\cite{felzenszwalb2004efficient}, in addition to the unsupervised approach of Nunes et. al.~\cite{nunes2022unsupervised} leveraging learned feature clustering and refinement.
All baselines are provided with input mesh vertices, colors, and normals, while our approach and Nunes et. al. also operate on sparse voxel scene representations.
Table~\ref{tab:scannet_results} and Figure~\ref{fig:full_results} show comparisons on ScanNet data; our \OURS{} approach improves significantly over state of the art by effectively leveraging signal from self-supervised 3D features to guide our model through self-training.
Note that since Nunes et. al. has been designed for outdoor applications, even while leveraging ScanNet-trained features, it uses ground removal and relies on physical object separation, making segmentation difficult in cluttered scenes.

Additionally, we demonstrate the importance of reasoning in 3D, and compare with a state-of-the-art unsupervised 2D instance segmentation approach CutLER~\cite{wang2023cut} run on the RGB frames of the scans, and projected to 3D using the corresponding camera poses.
Here, the difficulty lies in resolving view inconsistencies, occlusions, and lack of knowledge of geometric structure resulting in poor 3D segmentation performance despite plausible 2D proposals.

\vspace{-0.3cm}
\paragraph{Evaluation on ARKitScenes.} We additionally compare with state of the art on ARKitScenes~\cite{dehghan2021arkitscenes} data in Figure~\ref{fig:arkitscenes_results}.
We show only qualitative results due to the absence of ground truth instance mask annotations.
\OURS{} effectively produces cleaner, more accurate segmentations in these complex environments.

%%% MAIN TABLE %%%
\begin{table}
\centering
\begin{tabular}{lccccc}\toprule
  \textit{ScanNet}        & AP@25  & AP@50 & AP \\\midrule
HDBSCAN \cite{mcinnes2017accelerated_hdbscan}   & 32.1 & 5.5 & 1.6 \\
Nunes et al. \cite{nunes2022unsupervised}   & 30.5 & 7.3 & 2.3 \\
Felzenswalb \cite{felzenszwalb2004efficient}    & 38.9 & 12.7 & 5.0 \\
CutLER Projection \cite{wang2023cut}    & 7.0 & 0.2 & 0.3 \\
Ours    & \textbf{58.5} & \textbf{32.2} & \textbf{15.9} \\ \bottomrule
\end{tabular}
\caption{
Unsupervised class-agnostic 3D instance segmentation on ScanNet~\cite{dai2017scannet}. 
Our approach improves significantly over baselines (3x improvement in AP) due to our pseudo mask generation and self-training strategy.
}
\label{tab:scannet_results}
\end{table}

\vspace{-0.3cm}
\paragraph{What is the effect of multi-modal signal for pseudo mask generation?} 
We evaluate the effect self-supervised color and geometry signals for generating pseudo annotations in Table~\ref{tab:pseudo_performance}. 
We consider using only self-supervised geometric features (3D), only self-supervised color features (2D) that are projected to the 3D scans, and both together (both).
We find that the color and geometry provide complementary signals.
We also note that color features are only used for the initial pseudo mask generation, during self-training iterations and test time only 3D features were used. 

%%% FreeMask vs NCut %%%
\vspace{-0.28cm}
\paragraph{What is the effect of pseudo annotations?}
We also evaluate the effect of our pseudo mask generation in  Table \ref{tab:pseudo_performance} and Figure \ref{fig:freemask_vs_ncut}, in comparison to the 3D adaptation of the FreeMask~\cite{wang2022freesolo} approach operating on our geometric segments. 
FreeMask tends to estimate a larger but noisier set of initial pseudo masks, while our approach is focusing on a sparser set of more reliable pseudo masks and produces significantly better performance. 
The strong difference in performance can be explained by the nature of the samples. While a sparser set of examples can be extended with multiple iterations of self-training, noisy samples will propagate through the full pipeline, and thus directly degrade the final performance. Further details of our adaptations of the FreeMask 3D method can be found in our supplemental.

\begin{table}
\centering
\small
\begin{tabular}{lcccccc}\toprule
           & Modality  & AP@25  & AP@50 & AP & AP Final\\\midrule
FreeMask   &  3D  &  14.4  & 3.6  &  1.3 &  2.0 \\
Ours    &  3D  &  45.4  &  16.7 &   9.2 &  13.3 \\  \midrule
FreeMask   &  2D  &  31.1  & 15.1 &  6.8 &  13.8 \\
Ours    &  2D  &  51.3  & 21.8 &   9.4  & 15.7 
\\ \midrule
FreeMask   &  both  &  23.7  &  10.1 &  5.7 & 12.1 \\
Ours    &  both  &  \textbf{52.9}  & \textbf{23.2}  &  \textbf{10.4} & \textbf{15.9} \\ \bottomrule
\end{tabular}
\caption{We compare pseudo mask generation from 3D-only features (3D), color-only features (2D), and both color and geometry (both) signal, as well as with pseudo annotation generation algorithm FreeMask~\cite{wang2022freesolo}. In this table we report method performances after a single iteration of self-training initialized from the different pseudo annotation methods and the final AP scores after 4 self-training iterations.}
\label{tab:pseudo_performance}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{figures/results/ncut_vs_freemask_simple.jpeg}
    \caption[]{
    Initial pseudo masks generated by \OURS{} in comparison with a 3D-lifted FreeMask~\cite{wang2022freesolo}.
    FreeMask tends to produce a larger set of noisier pseudo masks, while we rely on a cleaner but sparser set for our self-training.
    } 
    \label{fig:freemask_vs_ncut}
\end{figure}

%%% NOISE Robust LOSSES
\vspace{-0.2cm}
\paragraph{The effect of noise robust losses.} 
We evaluate the effect of different noise robust losses for self-training in Table~\ref{tab:noise_robust_ablation}. 
We compare our baseline losses explained in \ref{par:losses} with a 3D extension of the projection loss of \cite{wang2022freesolo}, and our adaptation of  DropLoss from \cite{wang2023cut}.
Our approach does not penalize for missing pseudo masks, which enables more effective self-training to discover previously missed instances.

\begin{table}
\centering
\small
\begin{tabular}{lcccc}\toprule
     & AP@25  & AP@50 & AP & AP Final\\\midrule
Initial Pseudo Masks &  19.9  &  10.0  & 5.9  & -\\
Baseline losses \cite{Schult23mask3d} &  42.3  &  16.9 &  7.2 &  14.2 \\
Projection loss \cite{wang2022freesolo} &  35.7  &  12.1  &  4.7 & 7.2 \\
%DropLoss@0.01 &  45.4  &  18.2  &  7.5  \\
%DropLoss@0.05 &  48.7  &  21.8  &  9.5  \\
%DropLoss@0.1 &  \textbf{52.8}  &  \textbf{23.4 } &  \textbf{10.3}  \\ \bottomrule
DropLoss \cite{wang2023cut} &  \textbf{52.9}  &  \textbf{23.2} &  \textbf{10.4}  & \textbf{15.9} \\ \bottomrule
\end{tabular}
\caption{
A 3D projection loss struggles with under-determined associations, while DropLoss helps \OURS{} to discover parts of the scene that were missed by the source supervision. We report all metrics after a single iteration and the AP scores after 4 iterations of self-training.}
\label{tab:noise_robust_ablation}
\end{table}

\vspace{-0.2cm}
\paragraph{What is the impact of self-training?}
We observe that while self-training iterations are always improving the qualitative performance, their effective added information value is saturating after a limited number of cycles. We report on Table \ref{tab:self_train_iteration} through the first 4 steps, and observe a significant relative improvement in both modalities.  

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{figures/results/self_training_vertical.jpg}
    %\vspace{-0.7cm}
    \caption[]{
    \OURS{} employs self-training to refine the initial sparse set of  proposals. We can see consistent improvement over both the number of predicted instances and the quality of the instance masks. Here we show results using the pseudo annotations obtained from both modalities. 
    } 
    \label{fig:self_training_refinement}
\end{figure}

%%% THE NUMBER OF CYCLES in SELF TRAINING %%%
\begin{table}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccc}\toprule
& \multicolumn{3}{c}{3D Only} & \multicolumn{3}{c}{3D \& 2D}
\\\cmidrule(lr){2-4}\cmidrule(lr){5-7}
           & AP@25  & AP@50 & AP   & AP@25  & AP@50 & AP \\\midrule
$S^0$ pseudo masks  & 13.8 & 4.7  &  2 & 19.9 & 10.0 & 5.9 \\
$1^\textrm{st}$ Self-train  & 45.4 & 16.7  & 9.2  & 52.9 & 23.2 & 10.4 \\
$2^\textrm{nd}$ Self-train & 50.0 & 24.1  & 12.0  & 56.5 & 29.8 & 15.0 \\
$3^\textrm{rd}$ Self-train  & 52.2 &  25.8 & 12.8  & \textbf{58.8} & 31.9 & 15.9 \\
$4^\textrm{st}$ Self-train  & \textbf{52.7} &  \textbf{26.2} & \textbf{13.3}  & 58.5 & \textbf{32.2} & \textbf{15.9} \\ \bottomrule
\end{tabular}
}
%\vspace{-0.4cm}
\caption{Multiple iterations of self-training significantly improve performance, saturating around 4 iterations.}
\label{tab:self_train_iteration}
\end{table}


\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{figures/results/arkit_results.jpg}
    %\vspace{-0.7cm}
    \caption[]{As \OURS{} does not require any human annotation, so we can also train and test our method on the ARKitScenes~\cite{dehghan2021arkitscenes} dataset. We leverages 3D features followed by a series of self-training iterations for cleaner, more accurate instance segmentation. Qualitative results shows consistently better results than our baselines. 
    } 
    \label{fig:arkitscenes_results}
\end{figure}

\vspace{-0.4cm}
\paragraph{Limitations}
While \OURS{} offers a promising step towards unsupervised 3D instance segmentation, various limitations remain.
We rely on a mesh representation for graph coarsening, but believe this could be extended to alternative representations through neighborhood reasoning.
Additionally, our graph coarsening step may cause very small objects (e.g., pens, cell phones) to be missed in the pseudo annotation generation.
Finally, employing a fixed set of pseudo masks from the initial stage that are used through self-training could reinforce noisy predictions.
