\section{Method}\label{sec:method}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{figures/pipelines/pipeline_figure_full.pdf}
    %\vspace{-0.7cm}
    \caption[]{
    \OURS{} first generates a set of pseudo masks (top) to initiate self-training (bottom) for unsupervised 3D instance segmentation.
    We leverage features from 3D self-supervised pre-training in combination with 2D self-supervised features on an input mesh.
    These multi-modal features are then aggregated on geometric segment primitives, integrating low- and high-level signals for pseudo mask segmentation.
    These initial pseudo masks are then used as supervision for a 3D transformer-based model to produce updated instance masks that are integrated into the supervision of multiple self-training cycles.
    Finally, we obtain clean and dense instance segmentation without using any manual annotations.
    } 
    \label{fig:full_pipeline}
\end{figure*}

\paragraph{Problem definition}
We propose an unsupervised learning-based method for 3D instance segmentation. Formally, we assume a set of training 3D scenes $\{X_i\}_{i=1}^{n_t}$, represented as meshes, where each scene $X_i$ contains an unknown set of $n_i$ objects. We aim to train a model that can predict for a previously unseen input scene $X$, a set of 3D masks representing the different object instances in that scene. 

\paragraph{Method overview}
We employ a two-stage scheme for unsupervised 3D instance segmentation.
First, we group scene points into contiguous regions based on geometric primitives, and aggregate self-supervised features in these regions to generate an initial set of pseudo masks using Normalized Cut.  This grouping technique enables efficient handling of high-dimensional 3D data.
%
We then follow a series of self-training cycles to refine the pseudo annotations.
An overview of our approach is shown in Figure~\ref{fig:full_pipeline}.

\subsection{Geometric oversegmentation}\label{sec:oversegmentation}
Normalized Cut~\cite{shi2000normalized_cut} (NCut) with deep features has been successfully applied in the 2D domain on dense graphs generated from image patches \cite{wang2022tokencut,lis2022attentropy,wang2023cut}. However, adopting this directly to 3D would be computationally infeasible due to the cubic growth with dimensionality.

We thus propose a solution in the form of geometric oversegmentation through graph coarsening. We begin by creating a graph where each node represents a mesh vertex. Then, we aggregate nodes with similar normal direction and color values and cluster them into contiguous mesh segments using the efficient method proposed by \cite{felzenszwalb2004efficient}. This process reduces the graph size by multiple orders of magnitude. 
%
Our geometry aware segments, as opposed to voxels or points, additionally provide regularization to the subsequent stage of feature aggregation for generating pseudo masks, which we will describe in the following section.

\subsection{Initial pseudo mask generation}\label{sec:dataset_gen}

We first predict an initial set of pseudo masks.  Additional masks will be added during the self-training phase described in Section~\ref{sec:self_train}. 
Thus, we favor generating a reliable set of masks at the cost of restricting to a sparse initial set (i.e., missing potential instances rather than generating noisy masks for them).

\paragraph{Feature aggregation}

We aim to employ a strong set of features for pseudo mask generation and thus consider complementary geometric and color signals from RGB-D scan data.
We leverage geometric 3D self-supervised features from a state-of-the-art 3D pre-training approach, Contrastive Scene Contexts (CSC)~\cite{hou2021exploring}.
We additionally consider 2D self-supervised features from DINO~\cite{caron2021emerging_dino}, extracted from the RGB images and projected to 3D using the corresponding camera poses.
Both the 3D and 2D features are aggregated within each of our geometry-aware segments. 

\paragraph{Masked foreground separation}
We apply NCut to our aggregated features to extract foreground regions $M$ as our initial pseudo masks. 
%
Starting with an empty set $M^0=\{\}$, we iteratively compute the adjacency matrix and retrieve the masks. 
%
That is, we start from $N$ geometric segments with their corresponding $D$-dimensional features $\mathcal{F} \in \mathcal{R}^{N\times D}$, and construct the similarity matrix $A = sim(\mathcal{F})$, where $sim$ denotes cosine similarity. 
Additionally, for the multi-modal setup we calculate similarity matrices $A_{2D}$ and $A_{3D}$ independently and take their weighted average to obtain the final scores. 
Empirically, we found this to be more robust than direct feature fusion of the different modalities, due to their different statistical characteristics.
%
\indent We obtain $W_j$ for Equation~\ref{eq:general_eigenval} by thresholding $A$ at $\tau_{cut}$, where $j$ denotes the $j^{th}$ NCut iteration. 
Using $W_j$, we solve for the second eigenvector $v_j$ and threshold it to retrieve the partition $m_j$. 
We keep all separated foregrounds in $M^0$, where for each upcoming iteration, we mask out the row and column vectors from $W_i$, where $m_i \in M^0$ was already accepted as a foreground instance and $i$ being the segment ids. 
This allows greedy separation of instances in order of confidence  in every cut iteration.
Examples of our generated pseudo masks are visualized in Figures \ref{fig:freemask_vs_ncut} and \ref{fig:self_training_refinement}. \\
%
\indent As the adjacency graph is unaware of the mesh connectivity, NCut often results in masks that span  spatially separated scene regions. 
In 3D, we can leverage knowledge of physical distance to constrain masks to be contiguous in the coarsened scene connectivity graph. We thus filter masks that have separated components, keeping only the ones that contain the item with the maximum absolute value in  $v_j$. Separation is performed before saving $m_j$ into $M^0$, thus allowing for repeated separation of every component. 
We iterate until the maximum number of instances $M^0 = \{m_i\}_{i=1}^{N_m}$ are obtained, or there are no segments left in the scene. 

\subsection{Self-Training}\label{sec:self_train}

Our initial pseudo masks can provide a set of proposed instances $M^0$; however, these pseudo masks are quite sparse in the scenes and sometimes over- or under-split nearby instances.
We thus refine the pseudo mask data through an iterative self-training strategy, producing final instance segmentation predictions $M'$ with more dense and complete instance proposals.

We leverage a state-of-the-art 3D transformer-based backbone~\cite{Schult23mask3d} for our self-training from pseudo mask data as supervision. 
Through multiple training cycles we save the proposals of the $t^{th}$ iteration into $M^{t}$, from the self-trained model, and save these masks as an extension to the original pseudo dataset obtaining $M^t \supseteq M^0$. 
From the second training iteration, we can extract the most confident $K$ predictions and sample these new instance proposals as an addition to the pseudo annotations. 
Further, we only accept new instances if the added information value is larger than a minimum threshold, which we measure by simple segment IoU scores. This way, we can effectively densify the originally sparse annotations, but without limiting the quality of the originally clean pseudo masks. 
%
\paragraph{Loss} \label{par:losses}
We adapt DropLoss \cite{wang2023cut} for our self-training cycles, which is robust to sparse data and missing annotations. 
In particular, we use a weighted combination of cross-entropy and Dice \cite{sudre2017generalised_diceloss} losses for bipartite-matching with pseudo annotations.
We then drop losses for backpropagation which do not have at least $\tau_{drop}$ overlap with the annotations from the previous cycle.

\subsection{Implementation Details}\label{sec:implementation}
%
\paragraph{Backbones.} 
We use a Res16UNet34C sparse-voxel UNet implemented in the MinkowskiEngine~\cite{choy20194d} for 3D pre-trained feature extraction as well as for the 3D transformer during self-training. 

\paragraph{Self-training.} We employ the 3D transformer architecture of \cite{Schult23mask3d}, initialized from scratch. 
The first self-training cycle is trained for 600 epochs with a batch size of 8 until convergence, which takes $\approx 3$ days on a single NVIDIA RTX A6000 GPU. 
Further self-training cycles are all initialized from the previous state and finetuned for an additional 50 epochs in $\approx 4$ hours and for a total of 4 training cycles to produce the final set of instance predictions $S$. 
For the Hungarian assignment, we take the original weighted combination of dice and binary cross-entropy losses and only apply the DropLoss condition in the backpropagation phase.