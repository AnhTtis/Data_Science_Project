\section{Method}\label{sec:method}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{figures/pipelines/pipeline_figure_full.pdf}
    \vspace{-0.7cm}
    \caption[]{
    \OURS{} first generates a set of pseudo masks (top) to initiate self-training (bottom) for unsupervised 3D instance segmentation.
    We leverage features from 3D self-supervised pre-training in combination with 2D self-supervised features on an input mesh.
    These multi-modal features are then aggregated on geometric primitives, integrating low- and high-level signals for pseudo mask segmentation.
    These initial pseudo masks are then used as supervision for a 3D transformer-based model to produce updated instance masks that are integrated into the supervision of multiple self-training cycles.
    Finally, we obtain clean and dense instance segmentation without using any manual annotations.} 
    \label{fig:full_pipeline}
    \vspace{-0.4cm}
\end{figure*}

\paragraph{Problem definition}
We propose an unsupervised learning-based method for 3D instance segmentation.
We operate on a set of training 3D scenes $\{X_i\}_{i=1}^{n_t}$, represented as mesh graphs $G = (V, E)$, of vertices $V$ and triangular face edges $E$, where each scene $X_i$ contains an unknown set of $n_i$ objects in the $i^{th}$ scene. We aim to train a model that can predict for a previously unseen input scene $X$, a set of 3D masks representing the different object instances in that scene. 

\vspace{-0.3cm}
\paragraph{Method overview}
In order to achieve unsupervised 3D instance segmentation we first break down the scenes into $N$ geometric primitives $S_N$, which we use to initialize an adjacency matrix $W$ to extract an initial set of pseudo masks $M^0$, representing instance hypotheses based on combining 2D and 3D inputs $\mathcal{F}_{2D}$ / $\mathcal{F}_{3D}$ $\in R^{N \times D_{2D/3D}}$, where $D_{2D}, D_{3D}$ are the dimensions of the $2D/3D$ self-supervised features. We regularize the per-segment similarities over geometric primitives for mitigating noise and enabling efficient 3D reasoning.
We then employ a series of self-training cycles, updating pseudo mask supervision with new predicted masks, in order to produce final 3D instances.
An overview of our approach is shown in Figure~\ref{fig:full_pipeline}.


\subsection{Initial pseudo mask generation}\label{sec:dataset_gen}

In order to initiate self-training, we first generate an initial set of pseudo masks, leveraging complementary information from 2D and 3D signal in $\{X_i\}$.


\subsubsection{Feature aggregation}

To encourage effective initial pseudo mask generation, we employ joint reasoning across both self-supervised color and geometry features, as they can provide complementary information regarding objects.
%For a clean set of initial masks, we aim to employ a strong set of features and thus consider complementary geometric and color signals from RGB-D scan data.
As RGB-D scans often contain color image information and reconstructed 3D scan geometry, we can associate both 2D and 3D features in 3D by back-projecting the 2D extracted features using the corresponding depth and camera pose information for each image. 
Both 2D and 3D features are extracted through state-of-the-art self-supervised feature learning methods \cite{hou2021exploring, caron2021emerging_dino}.
As real-world camera estimation often contains small misalignment errors and noise or oversmoothing in reconstructed scan geometry, these self-supervised features can often also contain high-frequency noise, which we address in Sec.~\ref{sec:ncut_primitives} when reasoning over these features.
Note that while we employ both 2D and 3D signal when available for training, we do not require any aligned color image inputs for inference, enabling more general applicability.
%We leverage geometric 3D self-supervised features from a state-of-the-art 3D pre-training approach, Contrastive Scene Contexts (CSC)~\cite{hou2021exploring}.
%We additionally consider 2D self-supervised features from DINO~\cite{caron2021emerging_dino}, extracted from the RGB images and projected to 3D using the corresponding camera matrices.
%Both the 3D and 2D features are aggregated independently within each of our geometry-aware segments to remove high-frequency noise typical to self-supervised features~\cite{hou2021exploring}.
%\orl{here we say they are aggregated but later we say that we use their similarity scores separately and take their weighted average } 


\subsubsection{3D Graph Cut}\label{sec:ncut_primitives}
To generate pseudo masks from the 2D and 3D self-supervised features, we employ graph cut to estimate class-agnostic instances from the background. 
More precisely, we leverage the principle of Normalized Cut~\cite{shi2000normalized_cut} (NCut), which employs eigenvalue decomposition from an adjacency matrix $W\in R^{N \times N}$ over a graph to identify self-similar regions potentially representing semantic instances, where a set of potential instances can be extracted iteratively. 
Given a graph representing the 3D scene, we build an adjacency matrix $W$ and self-supervised features with a corresponding degree matrix $D\in R^{N \times N}$, where $D(i,i) = \Sigma_jW(i,j)$ and $(D-W)v = \lambda D v$. 
In this system, finding the second smallest eigenvalue $\lambda$  and its corresponding eigenvector $v$ is a close approximation for the minimized cost.
From $v$, we obtain foreground separation by taking all node activations where the eigenvector components were larger than their mean.
To identify multiple foreground objects, this process is repeated iteratively.

Unfortunately, applying this approach directly to the 3D scenes $\{X_i\}$ in common 3D representations such as voxels or points is not only computationally infeasible, but unreliable due to the noise in camera pose estimation and geometric reconstruction of 3D scan data. 
Thus, we propose to regularize the graph cut across geometric primitives.
%, but lift it to support high-resolution 3D segmentation by operating on geometric primitives from self-supervised 2D and 3D features. This graph cut algorithm separates the most self-similar regions representing semantic entities and providing an effective initialization for the subsequent stages. 

%  In practice this is formalized through an approximation as a generalized eigenvalue problem. 
%NCut maximizes similarities within partitions and dissimilarities across partitions by minimizing the cost of a graph cut.  We initialize an adjacency matrix $W$ from 3D primitives detailed in Section \ref{sec:oversegmentation} and self-supervised features with a corresponding degree matrix $D$, where $D(i,i) = \Sigma_jW(i,j)$ and $(D-W)v = \lambda D v$.  In this system finding the second smallest eigenvalue $\lambda$  and its corresponding eigenvector $v$ is a close approximation for the minimized cost. From $v$, we obtain foreground separation by taking all node activations where the eigenvector components were larger than their mean.   While multiple foreground objects could be directly predicted with a single pass by taking the eigenvectors in order, it was shown in \cite{wang2022tokencut} that a greedy iterative approach produces better results. While NCut could be used directly with color and normal features as well, combining it with deep features intuitively helps to scale across instance parts.

\subsubsection{Geometric Primitives}
%, and has been successfully applied in the 2D domain on dense graphs generated from image patches \cite{wang2022tokencut,lis2022attentropy,wang2023cut}. 
%However, adopting this directly to a volumetric grid would be computationally infeasible due to the cubic growth with dimensionality.

%\NEW{However, adopting NCut directly to scenes involve additional challenges unique to 3D.  Firstly, calculating the dense $W$ and $D$ in a volumetric grid would be computationally infeasible due to the cubic growth with dimensionality. }

To employ efficient reasoning across high-dimensional 3D data and enable robust 3D regularization of noisy features, we propose to operate on geometric primitives acquired through a graph coarsening process.
%We construct a graph for each $X_i$ where each node represents a mesh vertex. Then, nodes with similar normals and colors are aggregated and clustered based on the mesh topology of $X_i$ following \cite{felzenszwalb2004efficient}. 
For a 3D scene $X_i$ we construct the graph $G = (V, E)$ where $V$ and $E$ being the mesh vertices and face edges. Then, nodes with similar normals and colors are aggregated and clustered based on the mesh topology following \cite{felzenszwalb2004efficient} and resulting in a set $S_N = \{C_1 \dots C_N \}$ and $\bigcup (S_N) = V$ where $C_n$ represent a single primitive. 
This reduces the graph size by multiple orders of magnitude, and enables effective regularization of noise in the used self-supervised 2D and 3D features.

\subsubsection{NCut on Geometric Primitives}

%\ANGIE{this section needs transition and motivation - feels like a non-sequitur}
%
%As the Normalized Cut algorithm is able to provide a clean partitioning of scene graphs, and with the employed geometric coarsening of the scene we obtained a manageable resolution and regularized features we repeatedly apply NCut to our aggregated features to extract foreground regions $M$ as our initial pseudo masks. 
After addressing the challenge of dimensionality reduction and effectively mitigating speckle noise in our features using geometric primitives, we can leverage the capabilities of the Normalized Cut algorithm to achieve a clean partitioning of scene graphs. For this, we iteratively apply NCut to our aggregated features for the extraction of initial pseudo masks denoted as $M$.
%
Starting with an empty set $M^0=\{\}$, we iteratively compute the adjacency matrix over $S_N$ and retrieve the masks $m \subset S_N$. 
%
We start from $N$ geometric segments with their corresponding $D$-dimensional features $\mathcal{F} \in \mathcal{R}^{N\times D}$, and construct the similarity matrix $A = sim(\mathcal{F})$, where $sim$ denotes cosine similarity. 
Additionally, for the multi-modal setup we calculate similarity matrices $A_{2D}$ and $A_{3D}$ independently and take their weighted average to obtain the final scores. 
Empirically, we found this to be more robust than direct feature fusion of the different modalities, due to their different statistical characteristics.
%
\indent We obtain $W_j$ introduced in Section \ref{sec:ncut_primitives} by thresholding $A$ at $\tau_{cut}$, where $j$ denotes the $j^{th}$ NCut iteration. 
Using $W_j$, we solve for the second eigenvector $v_j$ and threshold it to retrieve the partition $m_j$. 
We keep all separated foregrounds in $M^0$, where for each upcoming iteration, we mask out the row and column vectors from $W_j$, where $m_i \in M^0$ was already accepted as a foreground instance and $i$ being the previous segment ids.
This allows greedy separation of instances in order of confidence  in every cut iteration.
Examples of our generated pseudo masks are visualized in Figures \ref{fig:freemask_vs_ncut} and \ref{fig:self_training_refinement}. \\
%
\indent As the adjacency graph is unaware of the mesh connectivity, NCut often results in masks that span  spatially separated scene regions. 
In 3D, we can leverage knowledge of physical distance and connectivity of $G$ to constrain masks to be contiguous in the coarsened scene connectivity graph. 
We thus filter masks $m_j$ that have separated components, keeping only the parts $\tilde{m_j}$ that contain the item with the maximum absolute value in $v_j$. 
Separation based on connectivity is performed before saving $\tilde{m_j}$ into $M^0$, thus allowing for repeated detection of the dropped part over the next NCut iterations. 
Finally, we iterate until the maximum number of instances $M^0 = \{m_i\}_{i=1}^{N_m}$ are obtained, or there are no segments left in the scene. 
%This simple separation idea makes a difference of around 2x improvement in the final AP scores, but for more qualitative details we refer to our supplemental.
Moreover, we favor generating a reliable set of masks at the cost of restricting to a sparse initial set (i.e., missing potential instances rather than generating noisy masks for them) through a stricter $\tau_{cut}$ or lower number of instances.

\subsection{Self-Training}\label{sec:self_train}

Our initial pseudo masks can provide a set of proposed instances $M^0$; however, these pseudo masks are quite sparse in the scenes and sometimes over- or under-split nearby instances.
We thus refine the pseudo mask data through an iterative self-training strategy, producing final instance segmentation predictions $M'$ with more dense and complete instance proposals.

We leverage a state-of-the-art 3D transformer-based backbone~\cite{Schult23mask3d} for our self-training from pseudo mask data as mask-head supervision, while the class-head is collapsed to \textit{foreground} and \textit{background} classes.
Through multiple training cycles we save the proposals of the $t^{th}$ iteration into $M^{t}$, from the self-trained model, and save these masks as an extension to the original pseudo dataset obtaining $M^t \supseteq M^0$. 
From the second training iteration, we can extract the most confident $K$ predictions and sample these new instance proposals as an addition to the pseudo annotations. 
Further, we only accept new instances if the added information value is larger than the minimum threshold, measured by simple segment IoU scores. This way, we can effectively densify the originally sparse annotations, but without limiting the quality of the originally clean pseudo masks. 
%
\subsection{Implementation Details}\label{sec:implementation}
%
\paragraph{Backbones.} 
We use a Res16UNet34C sparse-voxel UNet implemented in the MinkowskiEngine~\cite{choy20194d} for 3D pre-trained feature extraction as well as for the 3D transformer during self-training. For the pretrained features we use our own trained weights of \cite{hou2021exploring} for compatibility reasons.

\paragraph{Self-training.} We employ the 3D transformer architecture of \cite{Schult23mask3d}, initialized from scratch. 
The first self-training cycle is trained for 600 epochs with a batch size of 8 until convergence, which takes $\approx 3$ days on a single NVIDIA RTX A6000 GPU. 
Further self-training cycles are all initialized from the previous state and finetuned for an additional 50 epochs in $\approx 4$ hours and for a total of 4 training cycles to produce the final set of instance predictions $S$. 
For the Hungarian assignment, we take the original weighted combination of dice and binary cross-entropy losses and only apply the DropLoss condition in the backpropagation phase.