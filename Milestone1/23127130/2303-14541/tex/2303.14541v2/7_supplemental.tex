\section{Appendix}

%% Qualitative figures leading
%
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.75\linewidth, keepaspectratio]{figures/supplement/arkit_supplemental_scenes.jpg}
    \caption[]{Additional results on the ARKitScenes dataset \cite{dehghan2021arkitscenes}, compared to geometric clustering and oversegmentation-based baselines.} 
    \label{fig:arkit_supplement}
\end{figure*}
%
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.85\linewidth,keepaspectratio]{figures/supplement/scannet_supplemental_scenes.jpg}
    \caption[]{Additional results on the ScanNet dataset \cite{dai2017scannet}, compared to geometric clustering and oversegmentation-based baselines.} 
    \label{fig:scannet_supplemental}
\end{figure*}

\subsection{\OURS{} as Data Efficient Pretraining}

We report additional qualitative details on the data efficient pretraining performance of \OURS{} in Table~\ref{tab:data_efficient}.

We also note that the 3D contrastive pre-training of CSC, similar to other 3D pre-training methods developed for non-transformer backbones \cite{xie2020pointcontrast, hou2021exploring, zhang_depth_contrast,nunes2022segcontrast}, was not beneficial for a transformer-based model. A similar observation was also reported in a recent pretraining method \cite{hou2023mask3d}. 
We thus also compare with CSC pretraining on their original 3D backbone (which demonstrated improvement over training from scratch on the same backbone).
Our approach can improves notably over both alternatives.
%
\begin{table*}[!ht]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccccccccccc}\toprule
& & \multicolumn{3}{c}{1\%} &  \multicolumn{3}{c}{5\%} &  \multicolumn{3}{c}{10\%} &  \multicolumn{3}{c}{20\%} & \multicolumn{3}{c}{50\%}
\\\cmidrule(lr){3-5}\cmidrule(lr){6-8}\cmidrule(lr){9-11}\cmidrule(lr){12-14}\cmidrule(lr){15-17}
  Model & Backbone  & AP@25  & AP@50 & AP   & AP@25  & AP@50 & AP & AP@25  & AP@50 & AP & AP@25  & AP@50 & AP & AP@25  & AP@50 & AP \\\midrule
Scratch & Bottom-up & 22.6 &14.1 &6.8 &45.5 &33.3 &18.1 &54.8 &39.2 &21.9 & 61.0 &43.4 &25.5 &67.0 &51.4 &30.3 \\
CSC \cite{hou2021exploring} & Bottom-up &35.6 &22.1 &12.5 &52.7 &39.9 &23.3 &59.8 & 43.8& 25.0& 63.8& 48.9& 29.6& 70.5& 56.0 & 33.6 \\
Scratch & Transformer & 24.7 & 9.3 & 4.6 & 48.1 & 27.6 & 16.3 & 59.2 & 39.1& 23.4& 66.4 & 49.6 & 33.1& \textbf{78.9}& 67.5& \textbf{49.8} \\
CSC  & Transformer & 17.0& 6.8& 3.8& 44.2& 22.7& 13.1& 55.2& 32.3& 19.1& 62.0& 41.2& 26.0& 73.7& 58.2&40.0 \\
Ours & Transformer & \textbf{43.5} & \textbf{28.4} & \textbf{15.8} & \textbf{63.2} & \textbf{46.8} & \textbf{28.3} & \textbf{70.3} & \textbf{55.7} & \textbf{36.7} & \textbf{72.4} & \textbf{60.7}& \textbf{41.5} & \textbf{78.9}& \textbf{68.0} & 48.2 \\ \bottomrule
\end{tabular}
}
\caption{Unsupervised class-agnostic pretraining with our method can also act as a powerful pretraining strategy, advancing over state of the art. 
We report pretraining with CSC \cite{hou2021exploring} and \OURS{}, and evaluate the downstream weakly-supervised instance segmentation performance on ScanNet with percentage of limited annoated scenes used denoted in the top row. 
As we found that CSC degraded performance when using a transformer-based backbone, we also report the performance of training from scratch and CSC on their originally proposed backbone of a sparse UNet with bottom-up voting.}
\label{tab:data_efficient}
\end{table*}

%%% NOISE Robust LOSSES
\subsection{The effect of noise robust losses.} 
We adopt DropLoss \cite{wang2023cut} for our self-training cycles, which is robust to sparse data and missing annotations. 
In particular, we use a weighted combination of cross-entropy and Dice \cite{sudre2017generalised_diceloss} losses for bipartite-matching with pseudo annotations.
We then drop losses for backpropagation which do not have at least $\tau_{drop}$ overlap with the annotations from the previous cycle.
We evaluate the effect of different noise robust losses for self-training in Table~\ref{tab:noise_robust_ablation}. 
We compare our baseline losses with a 3D extension of the projection loss of \cite{wang2022freesolo}, and our adaptation of  DropLoss from \cite{wang2023cut}.
Our approach does not penalize for missing pseudo masks, which enables more effective self-training to discover previously missed instances.

\begin{table}[!ht]
\centering
\small
\begin{tabular}{lcccc}\toprule
     & AP@25  & AP@50 & AP & AP Final\\\midrule
Initial Pseudo Masks &  19.9  &  10.0  & 5.9  & -\\
Baseline losses \cite{Schult23mask3d} &  42.3  &  16.9 &  7.2 &  14.2 \\
Projection loss \cite{wang2022freesolo} &  35.7  &  12.1  &  4.7 & 7.2 \\
%DropLoss@0.01 &  45.4  &  18.2  &  7.5  \\
%DropLoss@0.05 &  48.7  &  21.8  &  9.5  \\
%DropLoss@0.1 &  \textbf{52.8}  &  \textbf{23.4 } &  \textbf{10.3}  \\ \bottomrule
DropLoss \cite{wang2023cut} &  \textbf{52.9}  &  \textbf{23.2} &  \textbf{10.4}  & \textbf{15.9} \\ \bottomrule
\end{tabular}
\caption{
A 3D projection loss struggles with under-determined associations, while DropLoss helps \OURS{} to discover parts of the scene that were missed by the source supervision. We report all metrics after a single iteration and the AP scores after 4 iterations of self-training.}
\label{tab:noise_robust_ablation}
\end{table}

\subsection{Additional Qualitative Results}

We show more qualitative results from our method trained on ARKitScenes \cite{dehghan2021arkitscenes} in Figure \ref{fig:arkit_supplement} and on ScanNet \cite{dai2017scannet} in Figure \ref{fig:scannet_supplemental}. 

\subsection{Pseudo Mask Generation Ablations}

We also ablate the saliency threshold, oversegmentation parameters, and separation strategy  in our pseudo mask generation. If not explicitly stated otherwise in Table \ref{tab:freemask_vs_ncut}, we use both 2D and 3D modality features for the pseudo mask generation. 

\paragraph{What is the effect of the saliency threshold in pseudo mask generation?} 

 We threshold the saliency matrix $A$ with $\tau_{cut}=0.55$ for geometric-only features and $\tau_{cut}=0.65$ for combined modalities. Table~\ref{tab:cutler3d_tau} shows that our approach maintains robust performance across a large range of $\tau_{cut}$ thresholds used to estimate salient areas for pseudo masks. In this table we report results using features from combined modalities, but similar behaviour can be observed for the other scenarios as well.

\begin{comment}
    \begin{table}
\centering
\small
\begin{tabular}{lccc}\toprule
$\tau_{cut}$ & AP@25  & AP@50 & AP \\\midrule
0.40 &  10.9  &  3.9  &  1.7  \\
0.50 &  12.5  &  4.7  &  1.9  \\
\textbf{0.55} &  \textbf{13.8}  &  \textbf{4.7}  &  \textbf{2.0}  \\
0.60 &  13.4  &  4.5  &  1.8  \\
0.70 &  13.3  &  4.5  &  1.8  \\
0.80 &  8.7  &  3  &  1.2  \\ \bottomrule
\end{tabular}
\caption{
Our pseudo mask generation quality, as measured by AP metrics, maintains robustness to a large range of $\tau$ thresholds that extract saliency.
Note that this measures the quality of only the pseudo masks; our full approach with self-training produces significantly improved results. In this table we show results from 3D only features. 
}
\label{tab:cutler3d_tau}
\end{table}
\end{comment}

\begin{table}
\centering
\small
\begin{tabular}{lccc}\toprule
$\tau_{cut}$ & AP@25  & AP@50 & AP \\\midrule
0.40 & 16.7 & 9.0 & 5.2 \\
0.50 & 20.8 & 10.7 & 5.7 \\
0.55 & 21.0 & 10.8 & 5.7 \\
0.60 & 21.3 & 11.3 & 5.8 \\
\textbf{0.65} & \textbf{19.9} & \textbf{10.0} & \textbf{5.9} \\
0.70 & 18.2 & 9.9 & 5.6 \\
0.80 & 11.8 & 5.0 & 2.6 \\ \bottomrule
\end{tabular}
\caption{
Our pseudo mask generation quality, as measured by AP metrics, maintains robustness to a large range of $\tau$ thresholds that extract saliency.
Note that this measures the quality of only the pseudo masks; our full approach with self-training produces significantly improved results. In this table we show results and parameters used by our method in bold and report pseudo mask performance generated from both modalities.}
\label{tab:cutler3d_tau}
\end{table}

\paragraph{The effect of iterative mask densification.}

We designed a strategy to leverage a sparse set of relatively clean initial pseudo masks, which are progressively extended with confident self-predictions during later iterations. This leads to a 3x improvement over state of the art in the Average Precision Metric. We could also consider different mask refinement strategies using a mixture of segments, initial masks or self-trained instances.
Tab.~\ref{tab:mask_refinement} ablates a mask refinement strategy of discarding previous masks and retaining current predictions. We also consider using Felzenswalb segments directly instead of feature-based pseudo labels. Both these strategies lead to lower performance due to the increased presence of noisy labels, which dominate the training signal.

\vspace{-0.3cm}
\begin{table}[!ht]
\centering
\footnotesize
\begin{tabular}{lccc}
                               & AP@25 & AP@50 & AP    \\ \hline \hline
\multicolumn{1}{l|}{Felzenswalb Masks}   & 35.5   & 20.6  &  10.3  \\ \hline
\multicolumn{1}{l|}{Mask Refinement}   &   43.7 & 24.4  &  12.4  \\ \hline
\multicolumn{1}{l|}{Mask Addition (Ours)}   &  58.6  & 32.0  &  16.0  \\ \hline
\end{tabular}
%\vspace{-0.2cm}
\caption{Instead of using masks from previous iteration directly it is the best to keep the initial masks fixed, and iteratively sample plausible predictions to enrich the pseudo dataset during self-training. This method strikes a balance between relatively clean, but sparse labels and increasing number of confident samples. Finally, even though Felzenswalb oversegmentation yields to higher precision, then our initial mask prediction algorithm, it also includes more background into the training, and this way plateauing at a lower self-training performance.}
\vspace{-0.3cm}
\label{tab:mask_refinement}
\end{table}


\paragraph{Robustness to oversegmentation parameters.}
Table~\ref{tab:pseudomaskablation} shows that our approach maintains strong robustness to a wide range of oversegmentation parameters for our geometric segments (our used parameters denoted in bold).

\paragraph{Additional pseudo mask generation hyperparameters.} 
Additionally, we also test the effect of other hyperparameters in out \textit{NCut}-based pseudo mask generation module, including  used distance metrics in the similarity matrix and different methods to separate unconnected patches in the predicted foregrounds. 
During the foreground separation in the Normalized Cut algorithm, we had an additional condition for the minimum number of foreground segments for the bipartitions. This conditions was able effectively filter out suboptimal partitioning of the full graph leading to separated parts from the full instances. Reducing the size of this parameter can directly lead to a more dense set of initial pseudo masks, with the cost of higher false positive rate. In Table \ref{tab:pseudomaskablation} we report a sparser and denser version of the datasets with a minimum number of foregorund segments of 8 and 2 accordingly, and show the initial higher scores of the pseudo annotation doesn't necessarily propagate to better downstream self-trained performance. 

Finally, we also ablate the effect of our physical connectivity-based foreground separation introduced in Section 3.1. In our main method we separate all set of connected components in the foreground, but only keep the component with the highest eigenvector activation (\textit{Max}). As an alternative we also test a method where we calculate the highest average activation in the connected component (\textit{Avg.}), a method where we keep the component with the largest surface value (\textit{Largest}) and finally, to test the effect of this module, without any kind of connectivity-based separation (\textit{No Sep.}). 

\begin{table*}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccccccccc}\toprule
\multicolumn{4}{c}{Generation Params.} & \multicolumn{4}{c}{Initial Pseudo Mask} & \multicolumn{3}{c}{1 Iteration of Self-Training}  & \multicolumn{3}{c}{4 Iterations of Self-Training}
\\\cmidrule(lr){1-4}\cmidrule(lr){5-8}\cmidrule(lr){9-11}\cmidrule(lr){12-14}
  Segment Size & Metric  & Separation & Min. \# of Foreground & \# of Instances & AP@25  & AP@50 & AP  & AP@25  & AP@50 & AP & AP@25  & AP@50 & AP  \\\midrule
30 & Cos & Max & 8 & 2169 &  21.9 & 11.5 & 6.3  & 53.7 & 26.2 & 12.4 & 55.4 & 30.3 & 15.3\\
 \textbf{50} & \textbf{Cos} & \textbf{Max} & \textbf{8} & 1414 & \textbf{19.9} & \textbf{10.0} & \textbf{5.9}  & \textbf{52.9} & \textbf{23.2} & \textbf{10.4} & \textbf{58.5} & \textbf{32.2} & \textbf{15.9} \\
 100 & Cos & Max & 8 & 1090 &  17.4 & 8.0 & 4.2 & 33.1 & 10.2 & 3.9 & 39.6 & 13.7 & 5.3 \\
 200 & Cos & Max & 8 & 584 &  11.0 & 3.7 & 1.8  & 24.3 & 8.7 & 2.1 & 26.1 & 9.7 & 2.4\\
 400 & Cos & Max & 8 & 319  & 6.4 & 2.5 & 1.1  & 19.1 & 3.9 & 1.2 & 19.9 & 3.2 & 1.0 \\ \midrule
 50 & L2 & Max & 8 & 1539 &  20.1 & 10.6 & 5.4 &  49.0 & 21.7  & 9.8 & 55.3 & 38.4 & 14.3 \\
 100 & L2 & Max & 8 & 805 & 13.3 & 5.3 & 2.6 & 30.8 & 8.3 & 2.8  & 39.0 & 12.7 & 5.0\\ \midrule
 50 & Cos & No Sep. & 8 & 125 &  4.3 & 0.3 &  0.1  & 4.3 & 0.5 & 0.2 & 4.9 & 0.6 & 0.2 \\ 
 50 & Cos & Largest & 8 & 620 &  11.5 & 4.9 & 2.5 &  11.5 & 1.5 & 0.4 & 12.9 & 2.2 & 12.9\\
 50 & Cos & Avg.  & 8 & 1078 & 16.8 & 9.1 & 5.1  & 36.4 & 12.5 & 4.9 & 43.8 & 17.8 & 7.5 \\ \midrule \midrule
 30 & Cos & Max & 2 & 2909 & 29.0 & 15.6 & 8.7 & 53.6 & 28.6 & 14.2 & 54.2 & 29.8& 15.4\\
 50 & Cos & Max & 2 & 2512 &24.9 & 12.4 & 7.2 & 56.5 & 29.8 & 15.0  & 51.3 & 26.2& 12.6\\
 100 & Cos & Max & 2 & 2317 & 23.1 & 12.3 & 6.8 & 51.8 & 24.4 & 11.6  & 57.1 & 31.3& 15.6\\
 200 & Cos & Max & 2 & 2181 & 28.4 & 15.5 & 8.9 & 54.6 & 28.7 &  13.7 & 56.6&31.4 & 15.6\\
 400 & Cos & Max & 2 & 1373 & 20.6 & 11.1 & 6.3 & 51.0 & 24.8 &  11.8 & 55.8 & 30.3& 15.2\\ \midrule
 50 & L2 & Max & 2 & 2496 & 28.6 & 15.8 & 9.0 & 55.8  & 29.6 & 14.6 & 54.8& 30.3& 15.3\\
 100 & L2 & Max & 2 & 1668 & 23.4 & 12.7 & 7.3 & 53.1 & 25.0 & 11.3 & 56.3&27.7 &12.9 \\ \midrule
 50 & Cos & No Sep. & 2 & 159 & 0.2 & 0.5 & 3.6 &  5.4 & 0.6 & 0.3 & 3.9& 0.4& 0.2\\ 
 50 & Cos & Largest & 2 &1026 & 14.1 & 7.2 & 3.9 & 11.5 & 1.8 & 0.5  &14.5 & 2.5& 0.7\\
 50 & Cos & Avg.  & 2 & 2053 & 23.3 & 12.0 & 6.8 & 52.5 & 27.4 & 12.7 & 54.9& 29.9& 14.9\\ \bottomrule
\end{tabular}
}
%\vspace{-0.4cm}
\caption{
We denote the parameters used by our method in bold. 
We show that our method is robust to a wide range of numbers regarding segments sizes and different similarity metrics, and only degrades somewhat in performance when segments are constrained to be too large. 
We also show that the separation of physically distant foreground patches is important and it is beneficial to use the activation of the eigenvector for the best results. 
Finally, we show that denser initial mask predictions lead to quantitatively better initial pseudo annotations, and even better self-training performance after a single iteration, but underperforming in their final scores. This behaviour can be explained by the larger false positive ratio in the denser initial predictions, which is propagating through all iterations, but thanks to the noise robust losses and iterative refinement of predictions the sparse set of labels can be effectively used. In this table we report results using both modalities for the initial pseudo mask generation, and number predicted pseudo instances in the official validation split of the ScanNet dataset.}
\label{tab:pseudomaskablation}
\end{table*}

\begin{comment}
\begin{table}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccccccc}\toprule
\multicolumn{3}{c}{Generation Params.} & \multicolumn{3}{c}{Pseudo Mask Scores} \\\cmidrule(lr){1-3}\cmidrule(lr){4-6}
  Segment Size & Metric  & Separation & AP@25  & AP@50 & AP  \\\midrule
 30 & Cos & Max & 21.9 & 11.5& 6.3 \\
 \textbf{50} & \textbf{Cos} & \textbf{Max} & \textbf{19.9}& \textbf{10.0}& \textbf{5.9} \\
 100 & Cos & Max  & 17.4& 8.0& 4.2 \\
 200 & Cos & Max  & 11.0& 3.7& 1.8 \\
 400 & Cos & Max  & 6.4& 2.5& 1.1 \\ \midrule
 50 & L2 & Max  & 20.1& 10.6& 5.4 \\
 100 & L2 & Max  & 13.3& 5.3& 2.6\\ \midrule
 50 & Cos & No Sep.  & 4.3& 0.3&  0.1\\ 
 50 & Cos & Largest  & 11.5& 4.9& 2.5 \\
 50 & Cos & Avg.  & 16.8& 9.1& 5.1 \\ \bottomrule
\end{tabular}
}
%\vspace{-0.4cm}
\caption{
We denote the parameters used by our method in bold. 
We show that the method is robust to distance metrics, the smaller segment sizes work the best. Additionally we show that the separation of physically distant foreground patches is important and it is beneficial to use the activation of the eigenvector for the best results. }
\label{tab:pseudomaskablation}
\end{table}  
\end{comment}




\subsection{Comparison with methods from the 2D domain}

To ensure a fair evaluation of methods operating on different input domains in Table 1. we followed the established procedure of well-known baselines \cite{Dai20183DMVJ3,hou20193d,jaritz2019multi}. This involves using depth information to project 2D predictions into 3D such that all methods are evaluated in the same 3D domain and aggregate multiple predictions through consensus by majority voting or accepting the maximum confidence scores for every voxel location. 
We also show results evaluated against 2D ScanNet images by projecting our method's predictions into 2D in Tab.~\ref{tab:evaluation_2d}, and comparing it to the current state of the art 2D unsupervised segmentation method \cite{wang2023cut} which demonstrates the usefulness of 3D reasoning.

\vspace{-0.3cm}s
\begin{table}[!ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
                               & AP@25 (2D) & AP@50 (2D) & AP (2D)    \\ \hline \hline
\multicolumn{1}{l|}{CutLER (2D)}   &  7.8  & 2.8  &  0.7  \\ \hline
\multicolumn{1}{l|}{Ours (projected)}   &  60.0  & 38.1  &  21.1  \\ \hline
\end{tabular}}
\caption{2D evaluation on ScanNet images.}
\vspace{-0.3cm}
\label{tab:evaluation_2d}
\end{table}

We also compare to weakly-supervised instance segmentation method SAM3D \cite{yang2023sam3d}, where powerful class-agnostic 2D masks are extracted by the powerful SAM model \cite{kirillov2023segment}. Here the projected 2D masks are merged into 3D masks iteratively with a bottom-up bidirectional merging approach to achieved cleaner and more view-independent 3D instances. A qualitative comparison on ScanNet can be seen in Table \ref{tab:sam3d_results}, with qualitative comparisons in Figure \ref{fig:sam3d_preds}.

\vspace{-0.3cm}
\begin{table}[!h]
\centering
\footnotesize
\begin{tabular}{lccc}
                               & AP@25 & AP@50 & AP    \\ \hline \hline
\multicolumn{1}{l|}{SAM3D}   & 37.2   &  11.8 &   3.7 \\ \hline
\multicolumn{1}{l|}{SAM3D with GT Segments}   & 47.6   & 24.1  & 10.8   \\ \hline
\multicolumn{1}{l|}{Ours}   & \textbf{58.5}   & \textbf{32.2}  &  \textbf{15.9}  \\ \hline
\end{tabular}
\vspace{-0.3cm}
\caption{\OURS{} achieves significantly better performance on ScanNet than SAM3D through our strong multi-modal reasoning. 
}
\vspace{-0.7cm}
\label{tab:sam3d_results}
\end{table}

\begin{figure}[!h]
    \centering
    \centering
    \includegraphics[width=\linewidth]{figures/rebuttal/sam3d_enebled_vs_ours_scene0645_00_cropped.png}
    \vspace{-0.8cm}
    \caption{ While SAM has powerful capabilities in crisp 2D mask generation, when aggregated on 3D, SAM3D tends to over-segment object instances. }
    \label{fig:sam3d_preds}
    \vspace{-0.5cm}
\end{figure}

SAM3D must resolve view inconsistencies and SAMâ€™s tendency to over-segment objects, which results in SAM3D splitting instances, while \OURS{} is able to achieve complete masks through multi-modal reasoning.
We believe integrating SAM or other (weakly-) supervised 2D models into our pipeline to enable multi-modal reasoning is an interesting avenue for future work.

\subsection{Additional Implementation Details}

Here, we further explain the implementation details of our pseudo mask generation. 

\paragraph{Pseudo code for masked NCut}

We show the pseudo code-style implementation for the masked normalized cut algorithm generating multiple instances as pseudo masks. The full algorithm can be seen in \ref{alg:masked_ncut}. 

\begin{algorithm}[ht!]
\DontPrintSemicolon
\caption{Masked NCut on 3D segments} \label{alg:masked_ncut}
\KwData{$\mathcal{S} = \{s_i,  \dots, s_N\}$, $\mathcal{F} \in \mathcal{R}^{NxD}$, 
$\mathcal{C} = \{(s_1, s_k), (s_1, s_l), \dots \}$}
\KwResult{$\mathcal{M} = \{m_j,  \dots, m_M\}$}
$\mathcal{M} \gets \{\}$ \\
\While{$j \le max\_inst\_num$}{
  $\mathcal{F}' \gets \mathcal{F}$ \\
  $\mathcal{F}'[\mathcal{M}] \gets 0.$ \tcp*{Mask out previous insts.}
  $\mathcal{W} \gets \mathcal{F} \times \mathcal{F}^T$ \tcp*{Feature similarity}
  \tcp*[l]{Saliency with connected graph}
  $\mathcal{W}_{i,k} = \left\{\begin{array}{cl}
           1. & \text{if $\mathcal{W}_{i,k} \geq \tau_{cut}$} \\
           \epsilon               & \text{if $\mathcal{W}_{i,k} < \tau_{cut}$} \\
           \end{array}\right.$ \\  \label{algoline:saliency}
  $\mathcal{D}_{i,i} = \sum_{k} W_{i,k}$ \\
  \tcp*[l]{Get $2^{nd}$ smallest eigenvector}
  $\lambda, \mathbf{v} \gets eigh(\mathcal{D} - \mathcal{W}, \mathcal{D}, -2)$ \\
  $m_i = \left\{\begin{array}{cl}
           1 & \text{if $ v_i \geq mean( \mathbf{v} )$} \\
           0 & \text{if $ v_i < mean( \mathbf{v} )$} \\
           \end{array}\right.$ \\   \label{algoline:segment_foreground}
  \tcp*[l]{Invert bipartition if too large}
  \If{$sum(\mathbf{m}) > D / 2$}{
    $ \mathbf{m} = 1 - \mathbf{m} $ \\
    $ \mathbf{v} = -1. * \mathbf{v} $ \\ 
    }
  \tcp*[l]{Separate unconnected components}
  $ v_{max} = max(\mathbf{v}) $ \\
  $ \tilde{\mathbf{m}} = sep(\mathbf{v}, v_{max}, \mathcal{C})$ \\
  $M \gets M \cup \{\tilde{\mathbf{m}}\}$
}
\end{algorithm}

\paragraph{3D Adaptation of FreeMask}

We also evaluate an alternative pseudo mask segmentation algorithm besides the masked \textit{NCut} method. 
In the 2D domain FreeSOLO \cite{wang2022freesolo} also followed a two stage pipeline first generating the pseudo annotations, and then refine those predictions through a series of self-training cycles. We followed their intuition to take a self-supervised pretrained backbone and extract it's deep features at multiple levels of the decoder. 
While in standard pretrained UNet-style models early features represent global context, final features and local semantic meaning, intermediate features can act as an useful proxy to extract self-similar regions in the input samples. 
In our implementation we used the same backbone features of \cite{hou2021exploring,caron2021emerging_dino} for the same 2D-3D setup and extracted the penultimate layer features for the self-similarity calculation. 
Then sampled the feature space with the Furthest Point Sampling \cite{qi2017pointnet++} strategy to get a more limited set of anchor points, later used to extract self-similar regions. 
For every seed point we took similarity scores with the other features of the full scene and thresholded it to extract salient regions. 
Finally, we used the efficient Non Maximum Suppression implementation from \cite{wang2022freesolo} to sort the predicted salient areas and filter out overlapping regions. 
We also used average similarity score combined with the salient region area to get \textit{maskness scores} for every salient region, directly following the original implementation. 
We report comparative results of the masked \textit{NCut} algorithm and our FreeMask 3D adaptation after self-training in Table 3. of the main paper and in Table \ref{tab:freemask_vs_ncut} of the initial pseudo mask scores. 

\begin{table}[!ht]
\centering
\small
\begin{tabular}{lccccc}\toprule
           & Modality  & AP@25  & AP@50 & AP \\\midrule
FreeMask   &  3D  &  13.7  & 7,2  &  3.7   \\
Ours    &  3D & 13.8 & 4.7 & 2.0  \\  \midrule
FreeMask   &  2D  & 15.3 & 6.6 & 2.9  \\
Ours    &  2D  & 15.6 & 7.2 & 3.6   \\ \midrule
FreeMask   &  both  & 17.9 & 7.5 & 3.7  \\
Ours    &  both  & 19.9 & 10.0 &  5.9 \\ \bottomrule
\end{tabular}
\caption{We compare pseudo mask generation from 3D-only features (3D), color-only features (2D), and both color and geometry (both) signal, as well as with pseudo annotation generation algorithm FreeMask.
We compare the quality of the initial pseudo mask dataset using our masked \textit{NCut} algorithm and the adaptation of FreeMask~\cite{wang2022freesolo} to 3D. We see that the normalized cut-based method is superior for both modalities.}
\label{tab:freemask_vs_ncut}
\end{table}

We also note here that while there is a difference in the initial pseudo mask qualities for the different methods, the downstream performance is way more significant. This can explained by the nature of the pseudo masks. \textit{NCut} provides a clean and sparse set of annotation, which is easy to densify for following iterations. On the other hand, the more dense, but noisy FreeMask predictions remain in the training for the duration of the whole training, hindering the performance of the self-trained model with noisy supervision. 
