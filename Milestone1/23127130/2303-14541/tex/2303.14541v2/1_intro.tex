\section{Introduction}

The increasing availability of commodity RGB-D sensors, now widely available on iPhones as well as with the Microsoft Kinect or Intel RealSense, has enabled consumer-level capture of 3D geometry of real-world environments.
To enable applications in robotics, autonomous navigation, and mixed reality in such scenes, semantic 3D scene understanding is necessary. In particular, 3D instance segmentation is critical to 3D perception, providing dense instance mask predictions, thus enabling physical and geometric reasoning about objects in an environment.
While various 3D deep learning approaches have been developed for 3D instance segmentation \cite{qi2017pointnet++,wu2019pointconv,wang2019dynamic,rethage2018fully,wang2018sgpn,hu2020randla,fan2021scf,occuseg, chen2021hierarchical,hou20193d,rozenberszki2022language,liang2021instance,vu2022softgroup,vu2022softgroup++,hui2022graphcut,kolodiazhnyi2023topdown,sun2022spformer,Schult23mask3d}, they require full supervision from expensive, manual, dense annotations on 3D scenes.

% We thus propose \OURS{}, a method for class-agnostic 3D instance segmentation. Our objective is to identify objects in real-world 3D scenes by estimating their dense instance masks. Importantly, our approach is not constrained to a predefined set of class categories. Moreover, our method operates without the need for human annotations, relying solely on self-supervised features in either 2D or 3D.
We introduce \OURS{}, a novel approach designed for class-agnostic 3D instance segmentation. 
Our aim is to identify objects in real-world 3D scans by predicting their dense instance masks, without any constraints to a predefined set of class categories. 
Moreover, we avoid expensive data annotation requirements by operating in an unsupervised fashion, instead leveraging self-supervised 2D and 3D features for segmentation. 


%\NEW{For this task we don't need any ground truth data, but provide an effective segmentation of class-agnostic instances relying only on 2D or 3D self-supervised features. }
%\orl{we say "that is" but only expaliun the category-agnostic, not the unsupervised nature of the task?}

\OURS{} comprises two essential elements.
First, we observe that for RGB-D scan data, self-supervised representation learning methods \cite{xie2020pointcontrast,hou2021exploring} can provide an innate signal indicating object-ness through feature similarity. 
We thus generate pseudo masks over 3D segment primitives, based on multimodal analysis of self-supervised color and geometry features from the RGB-D data.
By considering mesh segments rather than voxels or points, our approach efficiently scales with high-resolution 3D data in large scene environments while inherently promoting contiguous segmentation masks. %providing inherent regularization for pseudo annotations.
As we require strong features for these initial coarse estimates, we fuse information from both geometric and 2D color features in a complementary fashion.
Second, following the pseudo mask generation, we train our model through iterative self-training on both the initial pseudo masks and the current confident model predictions. Through multiple rounds of self-training with noise robust losses achieve improved object recognition and segmentation.
At inference time, we do not require any 2D color signal and can produce class-agnostic 3D instance segmentation for a new geometric observation of a 3D environment.
Experiments on challenging, cluttered indoor environments from the ScanNet~\cite{dai2017scannet}, S3DIS \cite{armeni_s3dis} and ARKit~\cite{dehghan2021arkitscenes} datasets show that \OURS{} improves significantly over unsupervised, clustering-based state of the art. 
% \orl{let's rephrase? given that we claim to be first its weird to call previous work state of the art no?} 
% \DAVID{removed stating that we are first - as that is easier to attack - or should we keep first and replace this part to clustering-based baselines} \\
%
In summary, our contributions are:
\begin{itemize}
	\item We propose an unsupervised 3D instance segmentation approach for indoor RGB-D scans, without requiring any human annotation.
	\item We generate sparse 3D pseudo masks for unsupervised training based on a multi-modal fusion of color and geometric signal from RGB-D scan data. We achieve robustness and efficiency through a geometry-aware scene coarsening. 
	\item Our generated pseudo masks are iteratively refined by self-training for 3D instances to improve 3D instance segmentation performance.
\end{itemize}

\begin{comment}
    
\begin{figure}
    \centering
    \includegraphics[width=\linewidth,keepaspectratio]{figures/pipelines/simple_pipeline.pdf}
    \vspace{-0.7cm}
    \caption[]{\OURS{} follows a two stage method, by first generating the coarse pseudo masks, then iteratively refine them with multiple cycles of self-training. \DAVID{Is this redundant to the main pipeline figure?}} 
    \label{fig:simple_pipeline}
\end{figure}

\end{comment}