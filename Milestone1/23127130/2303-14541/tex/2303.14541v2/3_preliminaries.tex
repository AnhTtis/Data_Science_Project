\section{Preliminaries}\label{sec:preliminaries}

%We leverage the principle of the normalized cut algorithm~\cite{shi2000normalized_cut} (NCut) to identify potential instances for 3D pseudo masks, lifted to the high-dimensional 3D scenario by using a geometric primitive basis, as discussed in Section~\ref{sec:oversegmentation}.
\NEW{We employ the principle of the NCut approach for pseudo-generation similarly to \cite{wang2023cut}, but lift it to support high-resolution 3D segmentation by operating on segment-level geometric primitives from self-supervised 2D and 3D features.}

NCut maximizes similarities within partitions and dissimilarities across partitions by minimizing the cost of a graph cut. This is formalized as: 
%
\begin{equation}
    NCut(A,B) = \frac{cut(A,B)}{assoc(A,V)} + \frac{cut(A,B)}{assoc(B,V)},
\end{equation}
where $A$ and $B$ are disjoint bipartitions from a full graph $V$, $cut$ measures the degree of dissimilarity computed as the total weight of edges that have been removed, and  $assoc$ represents the total connection within the partition.
Normalizing the cut cost function with the size of the partitions can solve the problem of single outlier node removal, which is one of the biggest difficulties of other graph cut algorithms~\cite{244673}.

While the minimum solution of this problem is intractable for practical applications, it can be rewritten as a generalized eigenvalue problem with adjacency matrix $W$ and degree matrix $D$, where $D(i,i) = \Sigma_jW(i,j)$:
%
\begin{equation} \label{eq:general_eigenval}
    (D-W)v = \lambda D v,
\end{equation}
%
Finding the second smallest eigenvalue $\lambda$  and its corresponding eigenvector $v$ is a close approximation for the minimized cost. 
From $v$, we obtain foreground separation by taking all node activations where the eigenvector components were larger than their mean. 
This method has been shown to be effective on intensity images, but combining it with deep features has demonstrated even stronger potential in the image domain \cite{wang2022tokencut,lis2022attentropy,wang2023cut}. 
While this multiple foreground objects could be directly predicted with a single pass by taking the eigenvectors in order, it was shown in \cite{wang2022tokencut} that a greedy iterative approach produces better results.



