%
We present DiffuScene for indoor 3D scene synthesis based on a novel scene graph denoising diffusion probabilistic model, which generates 3D instance properties stored in a fully-connected scene graph and then retrieves the most similar object geometry for each graph node \ie object instance which is characterized as a concatenation of different attributes, including location, size, orientation, semantic, and geometry features.
%
%Recently, we have seen immense progress in the field of automatic indoor scene synthesis by data-driven methods however, state-of-the-art methods that rely on VAE, GAN, or autoregressive models still struggle to represent the distribution of 3D indoor scenes well.
%
%Inspired by the recent success of diffusion models, we present a novel diffusion generative model for 3D indoor scene synthesis, which generates 3D instance properties stored in a fully-connected scene graph and then retrieves the most similar object geometry for each node.
%
%Each graph node \ie object instance is characterized as a concatenation of different attributes, including location, size, orientation, semantic, and geometry features.
%
Based on this scene graph, we designed a diffusion model to determine the placements and types of 3D instances.
%
Our method can facilitate many downstream applications, including scene completion, scene arrangement, and text-conditioned scene synthesis.
%
Experiments on the 3D-FRONT dataset show that our method can synthesize more physically plausible and diverse indoor scenes than state-of-the-art methods. 
%
Extensive ablation studies verify the effectiveness of our design choice in scene diffusion models.
%

\vspace{-0.5cm}

\begin{comment}
In this work, we address the task of automatic indoor scene synthesis.
Traditional scene modeling methods utilize optimization strategies based on hand-crafted room design rules.
Recent learning-based methods try to utilize different generative models, like VAE, GAN, or autoregressive models, to solve this problem.
While varying degrees of success have been achieved, they are still inferior to accurately approximating the complicated data distribution of 3D indoor scenes.
%
Inspired by the recent success of diffusion models, we present a novel diffusion generative model for 3D indoor scene synthesis, by firstly generating 3D instance properties stored in a full-connected scene graph and then retrieving the most similar object geometry for each node. 
Concretely, each graph node~\ie object instance is characterized as a concatenation of different attributes, including location, size, orientation, semantic, and geometry features. 
Then, we design a denoising diffusion probabilistic model to determine the placements of 3D instances. With the nice properties of diffusion models, our method can facilitate many downstream applications, including scene completion, scene arrangement, and text-conditioned scene synthesis, with few modifications.
Experiments on the 3D-FRONT dataset show that our method can synthesize more physically plausible and diverse indoor scenes against state-of-the-art methods. 
Extensive ablation studies verify the effectiveness of our design choice in scene diffusion models.

\end{comment}    
