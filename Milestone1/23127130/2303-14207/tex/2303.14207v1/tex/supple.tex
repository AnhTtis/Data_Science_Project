In this supplemental material, we provide details for our implementation in Sec.~\ref{SecImple}, dataset pre-processing and text prompt generation in Sec.~\ref{SecData}, baseline implementations in Sec.~\ref{SecBaseline}, additional results in Sec.~\ref{SecAddRes}, and user studies in Sec.~\ref{SecUser}.

\section{Implementations}
\label{SecImple}

\subsection{Shape Auto-Encoder}
\label{SubSecShapeAE}

We adopt a pre-trained shape auto-encoder to extract a set of latent shape codes for CAD models from the 3D-FUTURE~\cite{fu20213dm} dataset. The network architecture of the shape auto-encoder is shown in Fig.~\ref{fig:shapeae}. It is a variational auto-encoder, similar to FoldingNet~\cite{yang2018foldingnet}.
Specifically, a point cloud $\mathbf{P}_{in}$ of size 2,048 is fed into a graph encoder based on PointNet~\cite{qi2017pointnet} with graph convolutions~\cite{wang2019dynamic} to extract a global latent code of dimension 512, which is used to predict the mean $\mathbf{\mu}$ and variance $\mathbf{\sigma}$ of a low-dimensional latent space of size 64.
Subsequently, a compressed latent is sampled from $\mathcal{N}(\mathbf{\mu}, \mathbf{\sigma})$.
%\TODO{maybe stupid question, but what is reparametrization sampling. we should explain that}
Finally, the compressed latent is mapped back to the original space and passed to the FoldingNet decoder to recover a point cloud $\mathbf{P}_{rec}$ of size 2,025.
The used training objective is a weighted combination of Chamfer distance (\ie CD) and KL divergence.
\begin{equation}
    \label{EquaShapeAE}
    L_{vae} = \CD(\mathbf{P}_{in}, \mathbf{P}_{rec}) + \omega_{kl} *\KL(\mathcal{N}(\mathbf{\mu}, \mathbf{\sigma}) || \mathcal{N}(\mathbf{0}, \mathbf{I})) ,
\end{equation}
where $\omega_{kl}$ is set to 0.001.
The latent compression and KL regularization leads to a compact and structured latent space, focusing on global shape structures.
The shape autoencoder is trained on a single RTX 2080 with a batch size of 16 for 1,000 epochs.
The learning rate is initialized to $lr=\expnumber{1}{-4}$ and then gradually decreases with the decay rate of 0.1 in every 400 epochs.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./figs/shapeautoencoder.pdf}
    \caption{\textbf{Shape Auto-encoder.}}
    \label{fig:shapeae}
\end{figure}

\subsection{Shape Code Diffusion}
\label{SubSecShapeDiffu}

We use the extracted latent codes to train shape code diffusion.
While we apply KL regularization, the value range of latent codes is still unbound.
To make it easier to diffuse, we scale the latent codes to $[-1, 1]$ by using the statistical minimum and maximum feature values over the whole set.
During inference, we rescale generated shape codes.

\subsection{Shape Retrieval}
\label{SubSecRetrieval}

During inference, we use shape retrieval as the post-processing procedure to acquire object surface geometries for generated scene graphs.
Concretely, for each graph node, we perform the nearest neighbor search in the 3D-FUTURE~\cite{fu20213dm} dataset to find the CAD model with the same class label, the closest bounding box size, and the closest geometry feature.
Previous works~\cite{wang2021sceneformer, paschalidou2021atiss} only use object semantics and bounding box sizes during shape retrieval, we consider the similarity of geometry descriptors. Thus, our method can retrieve more accurate shape geometries. After the object retrieval, we place the retrieved CAD models into the scene based on the predicted locations, orientation angles, and sizes.  

% \subsection{Loss function}
% \label{SubSecLoss}

\section{Dataset}
\label{SecData}

\paragraph{Preprocessing}
The dataset preprocessing is based on the setting of ATISS~\cite{paschalidou2021atiss}.
We start by filtering out those scenes with problematic object arrangements such as severe object intersections or incorrect object class labels, e.g., beds are misclassified as wardrobes in some scenes.
Then, we remove those scenes with unnatural sizes. The floor size of a natural room is within $6m \times 6m$ and its height is less than $4m$. Subsequently, we ignore scenes that have too few or many objects.
The number of objects in valid bedrooms is between 3 and 13. As for dining and living rooms, the minimum and maximum numbers are set to 3 and 21 respectively. Thus, the number of scene graph nodes is $N=13$ in bedrooms and $N=21$ in dining and living rooms. In addition, we delete scenes that have objects out of pre-defined categories. After pre-processing, we obtained 4,041 bedrooms, 900 dining rooms, and 813 living rooms.

For the semantic class diffusion, we have an additional class of  `empty' to define the existence of an object. Combining with the object categories that appeared in each room type, we have $L=22$ object categories for bedrooms, and
$L=25$ object categories for dining and living rooms in total. The category labels 
are listed as follows.

\begin{python}
# 22 3D-Front bedroom categories
['empty', 'armchair', 'bookshelf', 'cabinet',
'ceiling_lamp', 'chair', 'children_cabinet',
'coffee_table', 'desk', 'double_bed',
'dressing_chair', 'dressing_table', 'kids_bed',
'nightstand', 'pendant_lamp', 'shelf',
'single_bed', 'sofa', 'stool', 'table',
'tv_stand', 'wardrobe']

# 25 3D-Front dining or living room categories
['empty', 'armchair', 'bookshelf', 'cabinet', 
'ceiling_lamp', 'chaise_longue_sofa', 
'chinese_chair', 'coffee_table', 'console_table',  
'corner_side_table',  'desk', 'dining_chair', 
'dining_table', 'l_shaped_sofa', 'lazy_sofa', 
'lounge_chair', 'loveseat_sofa', 
'multi_seat_sofa', 'pendant_lamp', 
'round_end_table', 'shelf', 'stool', 
'tv_stand', 'wardrobe', 'wine_cabinet']
\end{python}

\paragraph{Text Prompt Generation}
We follow the SceneFormer~\cite{wang2021sceneformer} to generate text prompts describing partial scene configurations. Each text prompt contains one to three sentences. We explain the details of text formulation process by using the text prompt 'The room has a dining table, a pendant lamp, and a lounge chair. The pendant lamp is above the dining table. There is a stool to the right of the lounge chair.` as an example. First, we randomly select three objects from a scene, get their class labels, and then count the number of appearances of each selected object category. As such, we can get the first sentence. Then, we find all valid object pairs associated with the selected three objects. An object pair is valid only if the distance between two objects is less than a certain threshold that is set to 1.5 in our method. Next, we calculate the relative orientations and translations, from which we can determine the relationship type of the valid object pair from the candidate pool: 'is above to`, 'is next to`, 'is left of`, 'is right of`, ' surrounding`, 'inside`, 'behind`, 'in front of`, and 'on`. In this way, we can acquire some relation-describing sentences like the second and third sentences in the example. Finally, we randomly sampled zero to two relation-describing sentences.

\section{Baselines}
\label{SecBaseline}

\paragraph{DepthGAN} 
DepthGAN~\cite{yang2021indoor} adopts a generative adversary network to train 3D scene synthesis using both semantic maps and depth images. The generator network is built with 3D convolution layers, which decode a volumetric scene with semantic labels. A differentiable projection layer is applied to project the semantic scene volume into depth images and semantic maps under different views, where a multi-view discriminator is designed to distinguish the synthesized views from ground-truth semantic maps and depth images during the adversarial training.


\paragraph{Sync2Gen} 
Sync2Gen~\cite{yang2021scene} represents a scene arrangement as a sequence of 3D objects characterized by different attributes (e.g., bounding box, class category, shape code). The generative ability of their method relies on a variational auto-encoder network, where they learn objects' relative attributes. Besides, a Bayesian optimization stage is used as a post-processing step to refine object arrangements based on the learned relative attribute priors.

\paragraph{ATISS}
ATISS~\cite{paschalidou2021atiss} considers a scene as an unordered set of objects and then designs a novel autoregressive transformer architecture to model the scene synthesis process. During training, based on the previously known object attributes, ATISS utilizes a permutation-invariant transformer to aggregate their features and  predicts the location, size, orientation, and class category of the next possible object conditioned on the fused feature. 
The original version of ATISS~\cite{paschalidou2021atiss} is conditioned on a 2D room mask from the top-down orthographic projection of the 3D floor plane of a scene. To ensure fair comparisons, we train an unconditional ATISS without using a 2D room mask as input, following the same training strategies and hyperparameters as the original ATISS.


% \section{Evaluation Metrics}
% \label{SecEval}

% \paragraph{Fr{\'e}chet Inception Distance}

% \paragraph{Kernel Inception Distance}

% \paragraph{Scene Classification Accuracy}

% \paragraph{Category KL Divergenece}



\section{Additional Results}
\label{SecAddRes}

\paragraph{Unconditional Scene Synthesis}
\input{./figs/uncond_gallery.tex}
 In Fig.~\ref{fig:uncond_gallery}, we provide more visualization results of our unconditional scene synthesis model. 

\paragraph{Scene Completion}
\input{./figs/complete_supple.tex}
We present more qualitative comparisons on the task of scene completion in Fig.~\ref{fig:completion_supple}.

\paragraph{Scene Arrangement}
\input{./figs/arrangement_supple.tex}
\input{./tabs/arrangement.tex}
\input{./tabs/text.tex}
We visualize additional qualitative comparisons on the task of scene arrangement in Fig.~\ref{fig:arrangement_supple}. Also, the quantitative results are shown in Tab.~\ref{tab:arrange}. 

\paragraph{Text-conditioned Scene Synthesis}
\input{./figs/text2scene_supple.tex}

We provide additional qualitative comparisons on the text-conditioned scene synthesis in Fig.~\ref{fig:text2scene_supple}. 
As observed, in the first and third rows, ATISS has object intersection issues while ours does not. In the second row, our method can correctly generate a corner side table on the left of the armchair. However, ATISS generates a corner side table on the right of the armchair.
 In the fourth row, our method can generate four dining chairs that are consistent with the text description, but ATISS can only generate two dining chairs.
The quantitative results evaluated by FID, KID, and SCA are reported in Tab.~\ref{tab:text}. Our method consistently outperforms ATISS in all used metrics.

\section{User Study}
\label{SecUser}

We conducted a perceptual user study to evaluate the quality of our method against ATISS on the application of text-conditioned scene synthesis.
As shown in Fig.~\ref{fig:user_study}, we provide the visualization of a ground-truth scene used to generate a text prompt as a reference. For each pair of results, a user needs to answer ``which of the generated scene can better match the text prompt?" and ``Which of the generated scene is more reasonable and realistic?".
%needs to decide which of the generated scene can better match the text prompt and judge which of the synthesized scene is more plausibly realistic than the other.
%\TODO{what is plausibly realistic? write down the question that you asked in the study}
We collect the answers of 225 scenes from 45 users and calculate the statistics. 62$\%$ of the user answers prefer our method to ATISS in realism.  55$\%$ of answers think our method is more consistent with the text prompt.

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{./figs/experiments/user_study/question_reference.jpg}
    
    \includegraphics[width=\linewidth]{./figs/experiments/user_study/question_match.jpg}

    \includegraphics[width=\linewidth]{./figs/experiments/user_study/question_realism.jpg}

    \caption{\textbf{User Study UI}. Based on the reference scene used to generate text prompts, users are asked which of the synthesized scene is more matched with the text prompt and more realistic. Note that the results from ATISS and our method are randomly shuffled to avoid bias.}
    \label{fig:user_study}
    
\end{figure*}
