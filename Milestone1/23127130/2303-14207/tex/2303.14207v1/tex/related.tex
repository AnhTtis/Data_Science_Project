\begin{figure*}
    \centering
    \includegraphics[width=.95\textwidth]{./figs/overview_new.pdf} %overview.jpg
    \caption{\textbf{Overview.} Given a 3D scene $\mathcal{S}$ of $N$ objects, we obtain its fully-connected scene graph $\Vec{x}_0=\{ \Vec{o}_i \}_{i=1}^{N}$, by parametrizing each object $\Vec{o}_i$ as a graph node storing all object attributes~\ie, location $\Vec{l}_i$, size $\Vec{s}_i$, orientation $\theta_i$, class label $\Vec{c}_i$, and latent shape code $\Vec{f}_i$. Based on a set of all possible $\Vec{x}_0$, we propose \emph{DiffuScene}, a denoising diffusion probabilistic model for 3D scene graph generation. In the forward process, we gradually add noise to $\Vec{x}_0$ until we obtain a standard Gaussian noise $\Vec{x}_T$. In the reverse process i.e. generative process, a denoising network iteratively cleans the noisy graph using ancestral sampling. Finally, we use the denoised object features to perform shape retrieval for realistic scene synthesis.} 
    %until the maximal step $T$ is reached
    \label{fig:pipeline}
\end{figure*}

\section{Related work}
\label{SecRelate}
%
\paragraph{Traditional Scene Modeling and Synthesis} 
%Generating realistic 3D indoor scenes has seen impressive progress in recent decades following the increasing popularity of 3D indoor scene datasets. Many approaches have been proposed to synthesize realistic object arrangements by learning from scene samples in datasets.
%
%\noindent \textbf{Traditional Scene Modeling and Synthesis}
Traditional methods usually formulate this problem into a data-driven optimization task, which consists of three key modules: scene formulation, prior representation, and optimization strategy. To extract the spatial relationship between objects, many approaches formulate objects in a scene as a graph~\cite{chang2014learning,chang2017sceneseer,qi2018human,zhu2018modeling}, and connect them with human bodies to build a human-centric graph~\cite{fisher2015activity,fu2017adaptive,ma2016action,qi2018human,nie2022pose2room}. To synthesize plausible 3D scene graphs, prior knowledge of reasonable scenes is required to drive scene optimization. Traditional scene priors are often inferred by following guidelines of interior design~\cite{merrell2011interactive,yeh2012synthesizing}, object frequency distributions (e.g., co-occurrence map of object categories)~\cite{chang2014learning,chang2017sceneseer,fisher2010context}, affordance maps from human motions~\cite{fisher2015activity,fu2017adaptive,jiang2012learning}, scene arrangement examples~\cite{fisher2012example,fu2017adaptive}. 
%
%Beyond scene priors, some methods synthesize scenes with an initial condition to involve extra constraints (e.g., an incomplete scene~\cite{fu2017adaptive,merrell2011interactive,yeh2012synthesizing}, a text~\cite{chang2014learning,chang2017sceneseer}, or a 2D sketch~\cite{xu2013sketch2scene}).
%
Constrained by scene priors, a new scene can be sampled with the above graph formulation using different optimization methods, e.g.,
iterative methods~\cite{fisher2015activity,fu2017adaptive}, non-linear optimization~\cite{chang2014learning,qi2018human,xu2013sketch2scene,yeh2012synthesizing,yu2011make,fisher2012example}, or
manual interaction~\cite{chang2017sceneseer,merrell2011interactive,savva2017scenesuggest}. Different from them, we adopt scene graph representation to learn complicated scene composition patterns from datasets, avoiding human-defined constraints and iterative optimization processes.
%
\paragraph{Learning-based Generative Scene Synthesis}
%\noindent \textbf{Learning-based Generative Scene Synthesis}
3D deep learning reforms this task by learning scene priors in a fully automatic, end-to-end, and differentiable manner. The capacity to process large-scale datasets dramatically increases the inference ability in synthesizing diverse and reasonable object arrangements.
%
Existing generative models for 3D scene synthesis are usually based on feed-forward networks~\cite{zhang2020deep}, VAEs~\cite{purkait2020sg,yang2021scene} , GANs~\cite{yang2021indoor}, Autoregressive models~\cite{paschalidou2021atiss,nie2022learning,wang2021sceneformer}. %which predict all objects in a single forward pass, where the scene context information is implicitly learned in neural networks. 
GAN methods produce high-quality results with fast sampling however, they often suffer from poor mode coverage and limited diversity. VAEs show better mode coverage but struggle to generate faithful samples~\cite{xiao2021tackling}.
%Recurrent networks~\cite{li2019grains,paschalidou2021atiss,nie2022learning,ritchie2019fast,wang2019planit,wang2018deep,wang2021sceneformer} generate objects sequentially in an autoregressive manner. Predicting each new object is conditioned on the generation of previous objects, where the object relations are learned in a more explicit way.
Recurrent networks~\cite{li2019grains,paschalidou2021atiss,nie2022learning,ritchie2019fast,wang2019planit,wang2018deep,wang2021sceneformer} including autoregressive models predict each new object conditioned on the previously generated objects. Thus, they are still limited by capturing complicated spatial relationships between objects.
In contrast, we formulate scene generation as a scene graph diffusion process. A scene configuration is learned by denoising a set of noisy vectors with random object properties defined on graph nodes, which explicitly encodes diverse scene configuration modes into the denoising process and presents faithful object configurations. 
%
\paragraph{3D Diffusion Models}
%After the pioneering work by~\cite{sohl2015deep} on using the diffusion process for data distribution learning, diffusion models have shown impressive generation quality in generative tasks, especially in 2D image synthesis~\cite{song2020denoising,ho2020denoising}.
%%%Compared with GANs and VAEs, diffusion models have shown promising capability in maintaining high generation quality and wide mode coverage~\cite{xiao2021tackling}.
%Several follow-up works improved its performance by designing different sampling strategies~\cite{karras2022elucidating}, denoising objectives~\cite{daras2022soft}, exploring denoising strategies~\cite{hoogeboom2022blurring}, or improving efficiency\cite{xiao2021tackling}. Other works extended diffusion models to more application tasks, e.g., text-to-image or guided image synthesis~\cite{meng2021sdedit,nichol2021glide,rombach2022high} and video generation~\cite{ho2022video,yang2022diffusion}
%
%\noindent \textbf{3D Diffusion Models}
After the pioneering work by~\cite{sohl2015deep} on using the diffusion process for data distribution learning, diffusion models~\cite{song2019generative,song2020improved,song2020denoising,ho2020denoising} have shown impressive visual quality in generative tasks, especially in various applications of 2D image synthesis, including
image inpainting~\cite{meng2021sdedit}, super-resolution~\cite{saharia2022image, ho2022cascaded}, editing~\cite{meng2021sdedit, avrahami2022blended}, text-to-image synthesis~\cite{nichol2021glide,rombach2022high, kim2022diffusionclip}, and video generation~\cite{ho2022video,yang2022diffusion}.
Unlike 2D applications, diffusion models in the 3D domain receive much less attention, especially in 3D scenes. Most existing works focus on single object generation~\cite{luo2021diffusion,zeng2022lion,zhou20213d,hui2022neural,zhang20233dshape2vecset, poole2022dreamfusion}. 
%
In contrast to single objects, 3D scene synthesis shows much higher semantics and geometry complexity and a larger spatial extent. A concurrent work of LegoNet~\cite{yu2022legonet} uses a diffusion model to rearrange indoor furnitures into reasonable placements. However, it requires users to provide an initial scene with known furniture categories and shapes. Our method can support unconditional generation by synthesizing all 3D object properties characterized by their class categories, oriented 3D bounding boxes, and geometry features. 
In addition to scene re-arrangements, our method can be applied to scene completion and text-to-scene synthesis while LegoNet cannot.
%With a CLIP encoder~\cite{radford2021learning}, our method enables text-to-scene synthesis to generate reasonable scene configurations following the text descriptions.