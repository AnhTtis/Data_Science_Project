\section{Introduction}
\label{SecIntro}

Synthesizing 3D indoor scenes that are realistic, semantically meaningful, and diverse is a long-standing problem in computer graphics.
%
Scene synthesis allows for drastically reduced budgets for game development or CGI in movies and will become even more relevant with the emergence of socializing in virtual reality.
%
Furthermore, scene synthesis can also be applied to existing apartments and houses to virtually re-arrange the scene, plan new furniture based on existing furniture or textual descriptions of a user.
%Furthermore, there is a huge potential for the application of scene synthesis for more than just entertainment, but actual planning of real homes might be performed this way in the future without relying on professional designers or planners.
%For DIY home designers, text-conditioned scene synthesis opens a whole world of opportunities for a very low cost. In addition, a customer could upload a photo of a room to a service by a furniture business and obtain a scene completion output and recommendations for furniture purchases that would fit to the room.
%
Automatic scene synthesis can also be a foundation of data-driven approaches for 3D scene understanding and reconstruction that require large-scale 3D datasets with ground-truth labels for supervision.

Traditional scene modeling and synthesis formulate this as an optimization problem. With pre-defined scene prior constraints defined by room design rules such as layout guidelines ~\cite{merrell2011interactive,yeh2012synthesizing}, object category frequency distributions \cite{chang2014learning,chang2017sceneseer,fisher2010context}, affordance maps from human-object interactions~\cite{fisher2015activity,fu2017adaptive,jiang2012learning}, or scene arrangement examples~\cite{fisher2012example,fu2017adaptive}, this line of work firstly sampled an initial scene, and then iteratively optimized the scene configurations. However, creating well-defined rules is extremely time-consuming and requires lots of effort from skillful and experienced artists. And the scene optimization stage is often tedious and computationally inefficient. Moreover, the pre-defined design rules can only manifest simple and intuitive scene composition patterns and cannot express all possible scenes. 
% increasing the time of scene formation

To capture more complicated scene arrangement patterns for diverse scene synthesis, some approaches~\cite{wang2018deep,li2019grains,ritchie2019fast,wang2019planit,zhang2020deep, purkait2020sg, wang2021sceneformer,yang2021indoor,yang2021scene,paschalidou2021atiss,nie2022learning} resort to deep generative models to learn scene priors from large-scale datasets. Among them, GAN-based methods~\cite{yang2021indoor} implicitly fit the scene distribution via adversarial training. Although they can generate high-quality results, they are always restricted with limited diversity due to poor mode coverage and easily suffer from mode collapse issues.
VAE-based methods~\cite{purkait2020sg,yang2021scene} explicitly approximate the scene distribution, enabling better mode coverage for generative diversity but with low-fidelity generation results. Recent auto-regressive models~\cite{wang2021sceneformer,paschalidou2021atiss,nie2022learning} progressively predict the next object conditioned on the previously known objects. However, the characteristic of the sequential prediction process cannot effectively exploit the relative attributes between objects and can accumulate prediction errors. 
%Nevertheless, there is still a long way to explore how t design a deep scene generative model that can accurately approximate the complicated scene distributions.

Inspired by the success of diffusion generative models in image synthesis~\cite{ho2020denoising, meng2021sdedit, kim2022diffusionclip, avrahami2022blended, saharia2022image, ho2022cascaded, dhariwal2021diffusion, rombach2022high,lugmayr2022repaint} and shape generation~\cite{luo2021diffusion,zhou20213d,zeng2022lion,zhang20233dshape2vecset,hui2022neural,tang2019skeleton,tang2022neural}, we strive to design a diffusion model for 3D scene synthesis.
Compared to other generative models~\cite{kingma2013auto,goodfellow2020generative,graves2013generating, rezende2015variational, chen2018neural,van2016conditional, van2017neural, razavi2019generating, esser2021taming}, diffusion models are easier to train while achieving a good balance between diversity and realism. %However, it is difficult to marry  diffusion models with scenes, as 3D scenes have irregular data structures. 
%
In this work, we propose the usage of a fully-connected scene graph to represent a scene.
%
A scene graph characterizes a scene as a union of object instances, where each graph node stores attributes of an object, including location, size, orientation, class label, and geometry feature.
%
Compared to other scene representations like multi-view images~\cite{dai20183dmv,han2019deep}, voxel grids~\cite{choy20163d,wu2016learning}, and neural fields~\cite{park2019deepsdf, mescheder2019occupancy, chen2019learning, mildenhall2021nerf,tang2021sa}, a scene graph is very compact and lightweight.
%
Based on the scene graph representation, we design a denoising diffusion probabilistic model~\cite{ho2020denoising, song2020score, ho2022classifier} to estimate these object attributes to determine the placements and types of 3D
instances, and then perform shape retrieval to obtain final surface geometries.
%
The scene diffusion priors are learned in the iterative transitions of noisy and clean scene graphs and, thus, can be used to sample physically plausible scenes with a wide variety.
%
Note that in the denoising process, we jointly predict the object properties of all objects in a scene and, thus, explicitly exploit the spatial relationships between objects via an attention mechanism~\cite{vaswani2017attention}.
%
Different from previous works~\cite{wang2021sceneformer, yang2021scene, paschalidou2021atiss} that only predict object bounding boxes, we diffuse semantics, oriented bounding boxes, and geometry features together to promote a holistic understanding of composition structure and surface geometries.
%
%
We show compelling results in the unconditional and conditional settings against state-of-the-art scene generation models, and provide extensive ablation studies to verify the design choices of our method.

\medskip
\noindent
Our contributions can be summarized as follows.
\begin{itemize}
    \item  we introduce 3D scene graph denoising diffusion probabilistic models for diverse indoor scene synthesis which learn holistic scene configurations of object concurrences, placements, and geometries.
    \item  based on this proposed model we facilitate completion from partial scenes, object re-arrangement in an existing scene, as well as text-conditioned scene synthesis.
\end{itemize}