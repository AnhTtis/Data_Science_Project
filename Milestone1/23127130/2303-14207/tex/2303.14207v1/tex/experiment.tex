\input{./figs/uncond_comparison.tex}
\section{Experiments}
\label{SecExp}
\paragraph{Datasets}
%\noindent \textbf{Datasets}
For experimental comparisons, we use the large-scale 3D indoor scene dataset 3D-FRONT~\cite{fu20213d} as the benchmark. 3D-FRONT is a synthetic dataset composed of 6,813 houses with 14,629 rooms, where each room is arranged by a collection of high-quality 3D furniture objects from the 3D-FUTURE dataset~\cite{fu20213dm}.  Following ATISS~\cite{paschalidou2021atiss}, we use three types of indoor rooms for training and evaluation, including 4,041 bedrooms, 900 dining rooms, and 813 living rooms.
For each room type, we use $80\%$ of rooms as the training sets, while the remaining are for testing.
%
\paragraph{Baselines}
%\noindent \textbf{Baselines}
We compare against state-of-the-art scene synthesis approaches using various generative models, including: 
1)  DepthGAN~\cite{yang2021indoor}, learning a volumetric generative adversarial network from multi-view semantic-segmented depth maps;
2) Sync2Gen~\cite{yang2021scene}, learning a latent space through a variational auto-encoder of scene object arrangements represented by a sequence of 3D object attributes; A Bayesian optimization stage based on the relative attributes prior model further regularized and refined the results.
3) ATISS~\cite{paschalidou2021atiss}, an autoregressive model to sequentially predict the 3D object bounding box attributes.
%
\paragraph{Implementation}
%\noindent \textbf{Implementations}
We train our scene diffusion models on different types of indoor rooms respectively.
They are trained on a single RTX 3090 with a batch size of 128 for $T=100,000$ epochs. The learning rate is initialized to $lr=\expnumber{2}{-4}$ and then gradually decreases with the decay rate of 0.5 in every 15,000 epochs.
For the diffusion processes, we use the default settings from the denoising diffusion probabilistic models (DDPM)~\cite{ho2020denoising}, where the noise intensity is linearly increased from 0.0001 to 0.02 with 1,000-time steps. During inference, we first use the ancestral sampling strategy to obtain a scene graph and then retrieve the most similar CAD model in the 3D-FUTURE~\cite{fu20213dm} for each object based on generated shape codes and sizes.
%
\input{./tabs/uncond.tex}
%
\paragraph{Evaluation Metrics}
%\noindent \textbf{Evaluation Metrics}
Following previous works~\cite{wang2021sceneformer, yang2021scene, paschalidou2021atiss}, we use Fr\'echet inception distance (FID )~\cite{heusel2017gans}, Kernel inception distance~\cite{binkowski2018demystifying} (KID $\times$ 0.001), scene classification accuracy (SCA), and category KL divergence (CKL $\times$ 0.01) to measure the plausibility and diversity of 1,000 synthesized scenes. For FID, KID, and SCA, we render the generated and ground-truth scenes into 256$\times$256 semantic maps through top-down orthographic projections, where the texture of each object is uniquely determined by the associate color of its semantic class. We use a unified camera and rendering setting for all methods to ensure fair comparisons.  For CKL, we calculate the KL divergence between the semantic class distributions of synthesized scenes and ground-truth scenes. For FID, KID, and CKL, the lower number denotes a better approximation of the data distribution. FID and KID can also manifest the result diversity. For the SCA, a score close to $50\%$ represents that the generated scenes are indistinguishable from real scenes. 
%
%
\subsection{Unconditional Scene Synthesis}
\label{SubSecExpSceSyn}
Fig.~\ref{fig:uncond_comparison} visualizes the qualitative comparisons of different scene synthesis methods. We observe that both DepthGAN~\cite{yang2021indoor} and Sync2Gen~\cite{yang2021scene} are vulnerable to object intersections. While ATISS~\cite{paschalidou2021atiss} can alleviate the penetration issue by autoregressive scene priors, it cannot always generate reasonable scene results. However, our scene graph diffusion can synthesize natural and diverse scene arrangements. Tab.~\ref{tab:uncond} presents the quantitative comparisons under various evaluation metrics. Our method consistently outperforms others in all metrics, which clearly demonstrates that our method can generate more diverse and plausible scenes.
%
% \subsection{Ablation Studies}
% \label{SubSecExpSceAbla}
%
\input{./tabs/ablation.tex}
\input{./tabs/completion.tex}
We conduct detailed ablation studies to verify the effectiveness of each design in our scene graph diffusion models. The quantitative results are provided in Tab.~\ref{tab:ablation}.
%

%\paragraph{Plane vs Grid vs Scene graph.} 
%\noindent \textbf{Plane vs Grid vs Scene graph.}
% we use paragraph for camera-ready 
\paragraph{What is the effect of scene graphs?}
%\noindent \textbf{What is the effect of scene graphs?}
Instead of using scene graphs, we can store object properties in regular 2D plane images or 3D volumetric grids. Then, we can use conventional 2D U-Net or 3D U-Net~\cite{ronneberger2015u} with attention layers to learn the diffusion object properties. However, the performances are inferior to our proposed scene graph diffusion.
%

%\paragraph{PVCNN vs Transformer vs MLP+Atten.}
%\noindent \textbf{PVCNN vs Transformer vs MLP+Atten.}
% we use paragraph for camera-ready 
\paragraph{What is the effect of MLP+Attention as the denoiser?}
%\noindent \textbf{What is the effect of MLP+Attention as the denoiser?}
We investigate the different choices of denoising networks. The performances degrade when we use the Point-Voxel CNN~\cite{zhou20213d} or the transformer in DALLE-2~\cite{ramesh2022hierarchical}.
%

%\paragraph{Single vs Multiple prediction heads.}
%\noindent \textbf{Single vs Multiple prediction heads.}
% we use paragraph for camera-ready 
\paragraph{The effect of multiple prediction heads in the denoiser.}
%\noindent \textbf{The effect of multiple prediction heads in the denoiser.}
In the denoiser, we use three different encoding and prediction heads for respective object properties,~\eg bounding box parameters, semantic class labels, and geometry codes. We found that a single head to process and predict all attributes together is less effective than multiple heads.
%

%\paragraph{With vs without geometry code diffusion}
%\noindent \textbf{With vs without geometry code diffusion}
% we use paragraph for camera-ready 
\paragraph{What is the effect of geometry code diffusion?}
%\noindent \textbf{What is the effect of geometry code diffusion?}
With the geometry code diffusion, the model can acquire a more accurate and holistic understanding of indoor scenes, facilitating more realistic scene synthesis. This can be verified in the improvement of SCA. Besides, the decrease in CKL can manifest that the joint diffusion of geometry code and object layout can promote the model to learn more accurate object class distribution.
%
% \paragraph{With vs without positional instance embedding}
%
%\paragraph{Autoregressive vs joint diffusion}
%
% \paragraph{Denoising process visualization} for supple 
%
\input{./figs/completion.tex}
\input{./figs/arrangement.tex}
%
\subsection{Applications}
%\subsection{Scene Completion}
%\label{SubSecExpSceComp}
\paragraph{Scene Completion}
We compare against ATISS~\cite{paschalidou2021atiss} on the task of scene completion. As shown in Fig.~\ref{fig:completion}, our method can produce more diverse completion results with high fidelity. This can also be verified by the quantitative comparisons in Tab.~\ref{tab:completion}.
%
% \subsection{Scene Re-arrangement}
% \label{SubSecExpSceArrang}
\paragraph{Scene Re-arrangement}
%
%\input{./tabs/arrangement.tex}
We also conduct comparisons with ATISS~\cite{paschalidou2021atiss} on the application of scene re-arrangement. As depicted in Fig.~\ref{fig:arrangement}, our method is generating various object placements with better plausibility compared to ATISS.
%
\paragraph{Text-conditioned Scene Synthesis}
%\subsection{Text-conditioned Scene Synthesis}
%\label{SubSecExpText2Sce}
\input{./figs/text2scene}
%\input{./tabs/text_user.tex}
%\TODO{I don't understand that sentence}
%
Given a text prompt describing a partial scene configuration, we aim to synthesize a full scene satisfying the input.
%
We conduct a perceptual user study for the text-conditioned scene synthesis.
%by randomly sampling 50 pairs of text and scenes.  
Given a text prompt and a ground-truth scene as a reference, we ask the attendance two questions for each pair of results from ATISS and ours: which of the synthesized scenes is closely matched with the input text, and which one is more realistic and reasonable. We collect the answers of 225 scenes from 45 users. 
%
Please refer to the supplementary material for the details of user study.
%
62$\%$ of users prefer our method to ATISS in realism. 
55$\%$ of users are in favor of us in the matching score. This illustrates that our text-conditioned model generates more realistic scenes while capturing more accurate object relationships described in the text prompt.
%
\label{SubSecExpTexSyn}



\subsection{Limitations}
%\TODO{talk about the limitations}
\blue{ Although we have shown impressive scene synthesis results, there are still some limitations in our method. Firstly, we only consider single-room generation and train our model on a specific room type. Thus, our method cannot synthesize large-scale scenes with multiple rooms. Secondly, the object textures are from the provided 3D CAD model dataset via shape retrieval. An interesting direction is to integrate texture diffusion into our model. %Thirdly, we have not considered human-object interactions in the generation process. It would be interesting to synthesize indoor scenes with humans.  
Lastly, we rely on 3D labeled scenes to drive the learning of scene diffusion. Leveraging scene datasets with only 2D labels to learn scene diffusion priors is also a promising direction. We leave these mentioned limitations as our future efforts.}