%
We present DiffuScene for indoor 3D scene synthesis based on a novel scene configuration denoising diffusion model. It generates 3D instance properties stored in an unordered object set and retrieves the most similar geometry for each object configuration, which is characterized as a concatenation of different attributes, including location, size, orientation, semantics, and geometry features.
%, \ie object instance 
%
%Recently, we have seen immense progress in the field of automatic indoor scene synthesis by data-driven methods however, state-of-the-art methods that rely on VAE, GAN, or autoregressive models still struggle to represent the distribution of 3D indoor scenes well.
%
%Inspired by the recent success of diffusion models, we present a novel diffusion generative model for 3D indoor scene synthesis, which generates 3D instance properties stored in a fully-connected scene graph and then retrieves the most similar object geometry for each node.
%
%Each graph node \ie object instance is characterized as a concatenation of different attributes, including location, size, orientation, semantic, and geometry features.
%
We introduce a diffusion network to synthesize a collection of 3D indoor objects by denoising a set of unordered object attributes.
Unordered parametrization simplifies and eases the joint distribution approximation. The shape feature diffusion facilitates natural object placements, including symmetries.
%
Our method enables many downstream applications, including scene completion, scene arrangement, and text-conditioned scene synthesis.
%
Experiments on the 3D-FRONT dataset show that our method can synthesize more physically plausible and diverse indoor scenes than state-of-the-art methods. 
%
Extensive ablation studies verify the effectiveness of our design choice in scene diffusion models.
%

\vspace{-0.5cm}

\begin{comment}
In this work, we address the task of automatic indoor scene synthesis.
Traditional scene modeling methods utilize optimization strategies based on hand-crafted room design rules.
Recent learning-based methods try to utilize different generative models, like VAE, GAN, or autoregressive models, to solve this problem.
While varying degrees of success have been achieved, they are still inferior to accurately approximating the complicated data distribution of 3D indoor scenes.
%
Inspired by the recent success of diffusion models, we present a novel diffusion generative model for 3D indoor scene synthesis, by firstly generating 3D instance properties stored in a full-connected scene graph and then retrieving the most similar object geometry for each node. 
Concretely, each graph node~\ie object instance is characterized as a concatenation of different attributes, including location, size, orientation, semantic, and geometry features. 
Then, we design a denoising diffusion probabilistic model to determine the placements of 3D instances. With the nice properties of diffusion models, our method can facilitate many downstream applications, including scene completion, scene arrangement, and text-conditioned scene synthesis, with few modifications.
Experiments on the 3D-FRONT dataset show that our method can synthesize more physically plausible and diverse indoor scenes against state-of-the-art methods. 
Extensive ablation studies verify the effectiveness of our design choice in scene diffusion models.

\end{comment}    
