\section{Introduction}
\label{SecIntro}
%
% Synthesizing 3D indoor scenes that are realistic, semantically meaningful, and diverse is a long-standing problem in computer graphics.
% %
% Scene synthesis allows for drastically reduced budgets for game development or CGI in movies and will become even more relevant with the emergence of socializing in virtual reality.
% %
% Furthermore, scene synthesis can be applied to existing apartments and houses to virtually re-arrange the scene and plan new furniture based on existing furniture or textual descriptions of a user.
% %
% Automatic scene synthesis can also be a foundation of data-driven approaches for 3D scene understanding and reconstruction that require large-scale 3D datasets with ground-truth labels for supervision.
% %
Synthesizing 3D indoor scenes that are realistic, semantically meaningful, and diverse is a long-standing problem in computer graphics.  It can significantly reduce costs in game development, CGI for films, and virtual reality. Furthermore, scene synthesis has practical applications in virtual interior design, enabling virtual rearrangement based on existing furniture or textual descriptions.  It also serves as a fundamental component in data-driven approaches for 3D scene understanding and reconstruction, necessitating large-scale 3D datasets with ground-truth labels.

Traditional scene modeling and synthesis formulate this as an optimization problem. With pre-defined scene prior constraints defined by room design rules such as layout guidelines ~\cite{merrell2011interactive,yeh2012synthesizing}, object category frequency distributions \cite{chang2014learning,chang2017sceneseer,fisher2010context}, affordance maps from human-object interactions~\cite{fisher2015activity,fu2017adaptive,jiang2012learning}, or scene arrangement examples~\cite{fisher2012example,fu2017adaptive}, they initially sample an initial scene and subsequently refine scene configurations through iterative optimization. However, defining precise rules is time-consuming and demands significant artistic expertise. The scene optimization stage is often laborious and computationally inefficient. Additionally, predefined design rules may limit the expression of complex and diverse scene compositions.

To automate the scene synthesis, some approaches~\cite{wang2018deep,li2019grains,ritchie2019fast,wang2019planit,zhang2020deep, purkait2020sg, wang2021sceneformer,yang2021indoor,yang2021scene,paschalidou2021atiss,nie2022learning} resort to deep generative models to learn scene priors from large-scale datasets. GAN-based methods~\cite{yang2021indoor} implicitly fit the scene distribution via adversarial training, yielding favorable results. However, they often lack diversity due to limited mode coverage and are prone to mode collapse. 
% Although they can generate high-quality results, they are always restricted with limited diversity due to poor mode coverage and easily suffer from mode collapse issues.
VAE-based methods~\cite{purkait2020sg,yang2021scene} explicitly approximate the scene distribution, offering better generative diversity but with lower-fidelity results. Recent auto-regressive models~\cite{wang2021sceneformer,paschalidou2021atiss,nie2022learning} progressively predict object properties sequentially. However, the sequential process may not accurately capture inter-object relationships and can accumulate prediction errors.
%predict the next based on the previously known objects. 
%However, the sequential prediction process cannot learn accurate inter-object relationships and can accumulate prediction errors.

%However, the characteristic of the sequential prediction process cannot effectively exploit the relative attributes between objects and can accumulate prediction errors. 
%
To capture more complicated scene configuration patterns for diverse scene synthesis, we strive to design a diffusion model for 3D scene synthesis. Diffusion models offer a compelling balance between diversity and realism and are relatively easier to train compared to other generative models~\cite{kingma2013auto,goodfellow2020generative,graves2013generating, rezende2015variational, chen2018neural,van2016conditional, van2017neural, razavi2019generating, esser2021taming}.
%
%inspired by the success of diffusion generative models in image synthesis~\cite{ho2020denoising, meng2021sdedit, kim2022diffusionclip, nichol2021glide, avrahami2022blended, saharia2022image, ho2022cascaded,  dhariwal2021diffusion, rombach2022high,lugmayr2022repaint} and shape generation~\cite{luo2021diffusion,zhou20213d,zeng2022lion,zhang20233dshape2vecset,hui2022neural,tang2019skeleton,tang2022neural}
%However, it is difficult to marry diffusion models with scenes, as 3D scenes have irregular data structures. 
%
%In this work, we propose the usage of a fully connected scene graph to represent a scene.
%A scene graph characterizes a scene as a union of object instances, where each graph node stores attributes of an object, including location, size, orientation, class label, and geometry feature.
In this work, we represent a scene as a set of unordered objects, with each element comprising a concatenation of various attributes, including location, size, orientation, semantics, and geometry features.
%
Compared to other scene representations like multi-view images~\cite{dai20183dmv,han2019deep}, voxel grids~\cite{choy20163d,wu2016learning}, and neural fields~\cite{park2019deepsdf, mescheder2019occupancy, chen2019learning, mildenhall2021nerf,tang2021sa}, our representation is more compact and lightweight, making it suitable for learning through diffusion models.
Rather than representing a scene as an ordered object sequence and diffusing them sequentially~\cite{wang2021sceneformer, paschalidou2021atiss}, unordered set diffusion simplifies and eases the approximation of joint distribution of object instances. 
%Based on the unordered object set
To this end, we design a denoising diffusion model~\cite{ho2020denoising, song2020score, ho2022classifier} to estimate object attributes to determine the placements and types of 3D instances and then perform shape retrieval to obtain final surface geometries.
%
%The scene diffusion priors are learned in the iterative transitions of noisy and clean object sets and, thus, can be used to sample physically plausible scenes with a wide variety.
%
%Note that in the denoising process, we jointly predict the object properties of all objects in a scene and, thus, explicitly exploit the spatial relationships between objects via an attention mechanism~\cite{vaswani2017attention}.
%
The scene diffusion priors are learned through iterative transitions between noisy and clean object sets, allowing for generating a diverse range of physically plausible scenes. During denoising, we simultaneously refine the properties of all objects within a scene, explicitly leveraging spatial relationships through an attention mechanism~\cite{vaswani2017attention}.
%
Different from previous works~\cite{wang2021sceneformer, yang2021scene, paschalidou2021atiss} that only predict object bounding boxes, we diffuse semantics, oriented bounding boxes, and geometry features together to promote a holistic understanding of composition structure and surface geometries. The synthesized shape codes for geometry retrieval can produce more natural object arrangements, such as symmetric relations commonly seen in the real world.
%
We show compelling results in the unconditional and conditional settings against state-of-the-art scene generation models and provide extensive ablation studies to verify the design choices of our method.

\medskip
\noindent
Our contributions can be summarized as follows.
\begin{itemize}
    \item We introduce 3D scene denoising diffusion models for diverse indoor scene synthesis, which learn holistic scene configurations of object semantics, placements, and geometries.
    \item We introduce shape latent feature diffusion for geometry retrieval, which exploits accurate inter-object relationships for symmetry formation.
    \item based on this proposed model we facilitate completion from partial scenes, object re-arrangement in an existing scene, as well as text-conditioned scene synthesis.
\end{itemize}