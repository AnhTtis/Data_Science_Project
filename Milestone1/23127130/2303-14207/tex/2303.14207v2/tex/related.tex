\begin{figure*}
    \centering
    \includegraphics[width=.92\textwidth]{./figs/overview_new.pdf} %overview.jpg
    \caption{\textbf{Overview.} Given a 3D scene $\mathcal{S}$ of $N$ objects, we represent it as an unordered set $\Vec{x}_0=\{ \Vec{o}_i \}_{i=1}^{N}$, by parametrizing each object $\Vec{o}_i$ as a vector storing all object attributes~\ie, location $\Vec{l}_i$, size $\Vec{s}_i$, orientation $\theta_i$, class label $\Vec{c}_i$, and latent shape code $\Vec{f}_i$. Based on a set of all possible $\Vec{x}_0$, we propose \emph{DiffuScene}, a denoising diffusion probabilistic model for 3D scene generation. In the forward process, we gradually add noise to $\Vec{x}_0$ until we obtain a standard Gaussian noise $\Vec{x}_T$. In the reverse process i.e. generative process, a denoising network iteratively cleans the noisy scene using ancestral sampling. Finally, we use the denoised class labels and shape latent codes to perform shape retrieval, and place object geometries through denoised locations, sizes, and orientations.} 
    %until the maximal step $T$ is reached
    \label{fig:pipeline}
\end{figure*}

\section{Related work}
\label{SecRelate}
%
\paragraph{Traditional Scene Modeling and Synthesis} 
%Generating realistic 3D indoor scenes has seen impressive progress in recent decades following the increasing popularity of 3D indoor scene datasets. Many approaches have been proposed to synthesize realistic object arrangements by learning from scene samples in datasets.
%
%\noindent \textbf{Traditional Scene Modeling and Synthesis}
Traditional methods usually formulate this problem into a data-driven optimization task.
%consisting of three key modules: scene formulation, prior representation, and optimization strategy. 
%To extract the spatial relationship between objects, many approaches formulate objects in a scene as a graph~\cite{chang2014learning,chang2017sceneseer,qi2018human,zhu2018modeling}, and connect them with human bodies to build a human-centric graph~\cite{fisher2015activity,fu2017adaptive,ma2016action,qi2018human,nie2022pose2room}.
To synthesize plausible 3D scenes, prior knowledge of reasonable configurations is required to drive scene optimization. Scene priors were often defined by following guidelines of interior design~\cite{merrell2011interactive,yeh2012synthesizing}, object frequency distributions (e.g., co-occurrence map of object categories)~\cite{chang2014learning,chang2017sceneseer,fisher2010context}, affordance maps from human motions~\cite{fisher2015activity,fu2017adaptive,jiang2012learning,ma2016action,qi2018human}, or scene arrangement examples~\cite{fisher2012example,fu2017adaptive}.
%
%Beyond scene priors, some methods synthesize scenes with an initial condition to involve extra constraints (e.g., an incomplete scene~\cite{fu2017adaptive,merrell2011interactive,yeh2012synthesizing}, a text~\cite{chang2014learning,chang2017sceneseer}, or a 2D sketch~\cite{xu2013sketch2scene}).
%
%Constrained by scene priors, a new scene can be sampled with the scene graph formulation using different optimization methods, 
Constrained by scene priors, a new scene can be sampled from the formulation using different optimization methods, 
e.g., iterative methods~\cite{fisher2015activity,fu2017adaptive}, non-linear optimization~\cite{chang2014learning,qi2018human,xu2013sketch2scene,yeh2012synthesizing,yu2011make,fisher2012example}, or
manual interaction~\cite{chang2017sceneseer,merrell2011interactive,savva2017scenesuggest}. Unlike them, we learn complicated scene composition patterns from datasets, avoiding human-defined constraints and iterative optimization processes.
%
\paragraph{Learning-based Generative Scene Synthesis}
%\noindent \textbf{Learning-based Generative Scene Synthesis}
3D deep learning reforms this task by learning scene priors in a fully automatic, end-to-end, and differentiable manner. The capacity to process large-scale datasets dramatically increases the inference ability in synthesizing diverse object arrangements.
%
Existing generative models for 3D scene synthesis are usually based on feed-forward networks~\cite{zhang2020deep,wu2022targf}, VAEs~\cite{purkait2020sg,yang2021scene} , GANs~\cite{yang2021indoor}, or Autoregressive models~\cite{paschalidou2021atiss,nie2022learning,wang2021sceneformer}. %which predict all objects in a single forward pass, where the scene context information is implicitly learned in neural networks. 
GAN methods generate high-quality results rapidly but often lack mode coverage and diversity. VAEs offer better mode coverage but face challenges in generating faithful samples~\cite{xiao2021tackling}.
%Recurrent networks~\cite{li2019grains,paschalidou2021atiss,nie2022learning,ritchie2019fast,wang2019planit,wang2018deep,wang2021sceneformer} generate objects sequentially in an autoregressive manner. Predicting each new object is conditioned on the generation of previous objects, where the object relations are learned in a more explicit way.
Recurrent networks~\cite{li2019grains,paschalidou2021atiss,nie2022learning,ritchie2019fast,wang2019planit,wang2018deep,wang2021sceneformer} including autoregressive models predict each new object conditioned on the previously generated objects. 
%Thus, they are still limited by capturing complicated spatial relationships between objects.
In contrast, we approach scene generation as an unordered object-set diffusion process where we explicitly model the joint distribution of object compositions. Multiple object properties are denoised synchronously, enhancing inter-object relationships and object composition plausibility.
%A scene configuration is learned by denoising a set of noisy vectors with random object properties, which explicitly encodes diverse scene configuration modes into the denoising process and presents faithful object configurations. 
%
\paragraph{3D Diffusion Models}
%After the pioneering work by~\cite{sohl2015deep} on using the diffusion process for data distribution learning, diffusion models have shown impressive generation quality in generative tasks, especially in 2D image synthesis~\cite{song2020denoising,ho2020denoising}.
%%%Compared with GANs and VAEs, diffusion models have shown promising capability in maintaining high generation quality and wide mode coverage~\cite{xiao2021tackling}.
%Several follow-up works improved its performance by designing different sampling strategies~\cite{karras2022elucidating}, denoising objectives~\cite{daras2022soft}, exploring denoising strategies~\cite{hoogeboom2022blurring}, or improving efficiency\cite{xiao2021tackling}. Other works extended diffusion models to more application tasks, e.g., text-to-image or guided image synthesis~\cite{meng2021sdedit,nichol2021glide,rombach2022high} and video generation~\cite{ho2022video,yang2022diffusion}
%
%\noindent \textbf{3D Diffusion Models}
%After the pioneering work by~\cite{sohl2015deep} on using the diffusion process for data distribution learning, 
%
Recently, diffusion models~\cite{sohl2015deep,song2019generative,song2020improved,song2020denoising,ho2020denoising} have shown impressive visual quality in generative tasks, especially in various applications of 2D image synthesis~\cite{ho2020denoising, meng2021sdedit, kim2022diffusionclip, nichol2021glide, avrahami2022blended, saharia2022image, ho2022cascaded, dhariwal2021diffusion, rombach2022high,lugmayr2022repaint, ho2022imagen, cong2024flatten}  and single shape generation~\cite{luo2021diffusion,zhou20213d,zeng2022lion,zhang20233dshape2vecset,hui2022neural,tang2019skeleton,tang2022neural,tang2024dphms, cao2024motion2vecsets,zhang2023functional}
%including image inpainting~\cite{meng2021sdedit}, super-resolution~\cite{saharia2022image, ho2022cascaded}, editing~\cite{meng2021sdedit, avrahami2022blended} and text-to-image synthesis~\cite{nichol2021glide,rombach2022high, kim2022diffusionclip}.
%
%and video generation~\cite{ho2022video,yang2022diffusion}.
%Unlike 2D applications, diffusion models in the 3D domain receive much less attention, especially in 3D scenes. Most existing works focus on single object generation~\cite{luo2021diffusion,zeng2022lion,zhou20213d,hui2022neural,zhang20233dshape2vecset, poole2022dreamfusion}. 
%
However, diffusion models in the 3D scene receive much less attention.
%  In contrast to single objects, 3D scene synthesis shows much higher semantics and geometry complexity and a larger spatial extent. 
%
%A concurrent work of LegoNet~\cite{wei2023lego} uses a diffusion model to rearrange indoor furniture into reasonable placements. 
%However, users must provide an initial scene with known furniture categories and shapes. Our method can support unconditional generation by synthesizing all 3D object properties characterized by their class categories, oriented 3D bounding boxes, and geometry features. 
%In addition to scene re-arrangements, our method can be applied to scene completion and text-to-scene synthesis, while LegoNet cannot.
%
A concurrent work of LEGO-Net~\cite{wei2023lego} aims to predict 2D object locations and orientations, taking the input of a floor plane, object semantics, and geometries. 
Meanwhile, CommonScene~\cite{zhai2024commonscenes} generates 3D indoor scenes conditioned on scene graphs.
%It does not handle objects like lamps that could hang from the ceiling. 
%
In contrast, DiffuScene is a scene-generative model that predicts 3D instance properties from random noise, including 3D locations and orientations, semantics, and geometries. 
%
Our method is more generic and versatile, which can benefit scene completion and conditioned scene synthesis from multi-modal signals like texts. 
%
In terms of implementation, our approach is based on a denoising diffusion model~\cite{ho2020denoising}, while LEGO-Net uses a Langevin Dynamics scheme based on a score-based method~\cite{song2019generative}.
We use a UNet-1D with attention as a denoiser rather than a transformer in LEGO-Net. 
%These differences in implementation contribute to our model's ability to acquire more natural scene arrangement patterns. More symmetric pairs discovered in our approach evidences this.
These implementation differences contribute to our model's ability to acquire more natural scene arrangements, as evidenced by the discovery of more symmetric pairs in our method.