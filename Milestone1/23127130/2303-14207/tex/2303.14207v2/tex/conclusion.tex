\section{Conclusion}
\label{SecConclu}
In this work, we introduced DiffuScene, a novel method for generative indoor scene synthesis based on a denoising diffusion probabilistic model that learns holistic scene configuration priors in the full set diffusion process of object semantics, bounding boxes, and geometry features.% In the scene graph diffusion, inter-object relationships are explicitly explored via attention layers. After scene graph diffusion, we obtained 3D object properties attached to the graph nodes and then retrieved the most similar object geometry for each node. 
%
%\red{We investigate various design choices in the scene diffusion models.}
We applied our method to several downstream applications, namely scene completion, scene re-arrangement, and text-conditioned scene synthesis. Compared to prior state-of-the-art methods. Our approach can synthesize more plausible and diverse indoor scenes as has been measured by different metrics and confirmed in a user study.
Our method is an important piece in the puzzle of 3D generative modeling and we hope that it will inspire research in denoising diffusion-based 3D synthesis.

\paragraph{Acknowledgement.}
This work is supported by a TUM-IAS Rudolf M{\"{o}}{\ss}bauer Fellowship, the ERC
Starting Grant Scan2CAD (804724), and Sony Semiconductor Solutions.