In this supplemental material, we provide details for our implementation in Sec.~\ref{SecImple}, dataset pre-processing and text prompt generation in Sec.~\ref{SecData}, baseline implementations in Sec.~\ref{SecBaseline}, additional results in Sec.~\ref{SecAddRes}, and user studies in Sec.~\ref{SecUser}.

\section{Implementations}
\label{SecImple}

\subsection{Shape Auto-Encoder}
\label{SubSecShapeAE}

We adopt a pre-trained shape auto-encoder to extract a set of latent shape codes for CAD models from the 3D-FUTURE~\cite{fu20213dm} dataset. The network architecture of the shape auto-encoder is shown in Fig.~\ref{fig:shapeae}. It is a variational auto-encoder, similar to FoldingNet~\cite{yang2018foldingnet}.
Specifically, a point cloud $\mathbf{P}_{in}$ of size 2,048 is fed into a graph encoder based on PointNet~\cite{qi2017pointnet} with graph convolutions~\cite{wang2019dynamic} to extract a global latent code of dimension 512, which is used to predict the mean $\mathbf{\mu}$ and variance $\mathbf{\sigma}$ of a low-dimensional latent space of size 32.
Subsequently, a compressed latent is sampled from $\mathcal{N}(\mathbf{\mu}, \mathbf{\sigma})$.
%\TODO{maybe stupid question, but what is reparametrization sampling. we should explain that}
Finally, the compressed latent is mapped back to the original space and passed to the FoldingNet decoder to recover a point cloud $\mathbf{P}_{rec}$ of size 2,025.
The used training objective is a weighted combination of Chamfer distance (\ie CD) and KL divergence.
\begin{equation}
    \label{EquaShapeAE}
        L_{vae} = \CD(\mathbf{P}_{in}, \mathbf{P}_{rec}) + \omega_{kl} *\KL(\mathcal{N}(\mathbf{\mu}, \mathbf{\sigma}) || \mathcal{N}(\mathbf{0}, \mathbf{I})) ,
\end{equation}
where $\omega_{kl}$ is set to 0.001.
The latent compression and KL regularization leads to a compact and structured latent space, focusing on global shape structures.
The shape autoencoder is trained on a single RTX 2080 with a batch size of 16 for 1,000 epochs.
The learning rate is initialized to $lr=\expnumber{1}{-4}$ and then gradually decreases with the decay rate of 0.1 in every 400 epochs.
\begin{figure}[!htp]
    \centering
    \includegraphics[width=\linewidth]{./figs/shapeautoencoder.pdf}
    \caption{\textbf{Shape Auto-encoder.}}
    \label{fig:shapeae}
\end{figure}

\subsection{Shape Code Diffusion}
\label{SubSecShapeDiffu}

We use the extracted latent codes to train shape code diffusion.
While we apply KL regularization, the value range of latent codes is still unbound.
To make it easier to diffuse, we scale the latent codes to $[-1, 1]$ by using the statistical minimum and maximum feature values over the whole set.
During inference, we rescale generated shape codes.

\subsection{Shape Retrieval}
\label{SubSecRetrieval}

During inference, we use shape retrieval as the post-processing procedure to acquire object surface geometries for generated scenes.
Concretely, for each instance, we perform the nearest neighbor search in the 3D-FUTURE~\cite{fu20213dm} dataset to find the CAD model with the same class label and the closest geometry feature.

% \subsection{Loss function}
% \label{SubSecLoss}

\section{Dataset}
\label{SecData}

\paragraph{Preprocessing}
The dataset preprocessing is based on the setting of ATISS~\cite{paschalidou2021atiss}.
We start by filtering out those scenes with problematic object arrangements such as severe object intersections or incorrect object class labels, e.g., beds are misclassified as wardrobes in some scenes.
Then, we remove those scenes with unnatural sizes. The floor size of a natural room is within $6m \times 6m$ and its height is less than $4m$. Subsequently, we ignore scenes that have too few or many objects.
The number of objects in valid bedrooms is between 3 and 13. As for dining and living rooms, the minimum and maximum numbers are set to 3 and 21 respectively. Thus, the number of objects is $N=13$ in bedrooms and $N=21$ in dining and living rooms. In addition, we delete scenes that have objects out of pre-defined categories. After pre-processing, we obtained 4,041 bedrooms, 900 dining rooms, and 813 living rooms.

For the semantic class diffusion, we have an additional class of  `empty' to define the existence of an object. Combining with the object categories that appeared in each room type, we have $L=22$ object categories for bedrooms, and
$L=25$ object categories for dining and living rooms in total. The category labels 
are listed as follows.

\begin{python}
# 22 3D-Front bedroom categories
['empty', 'armchair', 'bookshelf', 'cabinet',
'ceiling_lamp', 'chair', 'children_cabinet',
'coffee_table', 'desk', 'double_bed',
'dressing_chair', 'dressing_table', 'kids_bed',
'nightstand', 'pendant_lamp', 'shelf',
'single_bed', 'sofa', 'stool', 'table',
'tv_stand', 'wardrobe']

# 25 3D-Front dining or living room categories
['empty', 'armchair', 'bookshelf', 'cabinet', 
'ceiling_lamp', 'chaise_longue_sofa', 
'chinese_chair', 'coffee_table', 'console_table',  
'corner_side_table',  'desk', 'dining_chair', 
'dining_table', 'l_shaped_sofa', 'lazy_sofa', 
'lounge_chair', 'loveseat_sofa', 
'multi_seat_sofa', 'pendant_lamp', 
'round_end_table', 'shelf', 'stool', 
'tv_stand', 'wardrobe', 'wine_cabinet']
\end{python}

\paragraph{Text Prompt Generation}
We follow the SceneFormer~\cite{wang2021sceneformer} to generate text prompts describing partial scene configurations. Each text prompt contains one to three sentences. We explain the details of text formulation process by using the text prompt 'The room has a dining table, a pendant lamp, and a lounge chair. The pendant lamp is above the dining table. There is a stool to the right of the lounge chair.` as an example. First, we randomly select three objects from a scene, get their class labels, and then count the number of appearances of each selected object category. As such, we can get the first sentence. Then, we find all valid object pairs associated with the selected three objects. An object pair is valid only if the distance between two objects is less than a certain threshold that is set to 1.5 in our method. Next, we calculate the relative orientations and translations, from which we can determine the relationship type of the valid object pair from the candidate pool: 'is above to`, 'is next to`, 'is left of`, 'is right of`, ' surrounding`, 'inside`, 'behind`, 'in front of`, and 'on`. In this way, we can acquire some relation-describing sentences like the second and third sentences in the example. Finally, we randomly sampled zero to two relation-describing sentences.

\section{Baselines}
\label{SecBaseline}

\paragraph{DepthGAN} 
DepthGAN~\cite{yang2021indoor} adopts a generative adversary network to train 3D scene synthesis using both semantic maps and depth images. The generator network is built with 3D convolution layers, which decode a volumetric scene with semantic labels. A differentiable projection layer is applied to project the semantic scene volume into depth images and semantic maps under different views, where a multi-view discriminator is designed to distinguish the synthesized views from ground-truth semantic maps and depth images during the adversarial training.


\paragraph{Sync2Gen} 
Sync2Gen~\cite{yang2021scene} represents a scene arrangement as a sequence of 3D objects characterized by different attributes (e.g., bounding box, class category, shape code). The generative ability of their method relies on a variational auto-encoder network, where they learn objects' relative attributes. Besides, a Bayesian optimization stage is used as a post-processing step to refine object arrangements based on the learned relative attribute priors.

\paragraph{ATISS}
ATISS~\cite{paschalidou2021atiss} considers a scene as an unordered set of objects and then designs a novel autoregressive transformer architecture to model the scene synthesis process. During training, based on the previously known object attributes, ATISS utilizes a permutation-invariant transformer to aggregate their features and  predicts the location, size, orientation, and class category of the next possible object conditioned on the fused feature. 
The original version of ATISS~\cite{paschalidou2021atiss} is conditioned on a 2D room mask from the top-down orthographic projection of the 3D floor plane of a scene. To ensure fair comparisons, we train an unconditional ATISS without using a 2D room mask as input, following the same training strategies and hyperparameters as the original ATISS.


\section{Ablation Studies}
\label{SecAbla}
 %We provide more detailed explanations for ablation studies.
In main paper, we investigated the effectiveness of each design in our DiffuScene, including network architecture, loss function, and geometry feature diffusion. We present more implementation details of each method variant.

\noindent \textbf{What is the effect of UNet-1D+Attention as the denoiser? } 
We advocate the use of UNet-1D with attention layers as the denoising network. 
The self-attention layers within this architecture effectively aggregate all object features and explore inter-object relationships, facilitating the learning of a global context that aids in distinguishing different objects within the scene.
An alternative choice is to use a pure transformer network, like the one adopted in DALLE-2~\cite{ramesh2022hierarchical}. However, our comparisons revealed a marginal degradation in performance metrics such as FID, KID, SCA, and CKL. It demonstrates that UNet-1D with attention layers is more adept at capturing accurate scene distributions than networks solely composed of transformation layers.

\noindent \textbf{What is the effect of multiple prediction heads in the denoiser?} 
In our denoiser architecture, we employ three distinct encoding and prediction heads tailored for specific object properties, including bounding box parameters, semantic class labels, and geometry codes. By utilizing multiple diffusion heads with individual loss functions for each attribute (e.g., bbox, class, geometry), we mitigate the risk of bias towards any single attribute within a single encoding and prediction head. This approach ensures that our denoiser effectively captures and processes diverse object properties without favoring one over the others. The consistent improvement in each evaluation metric verifies the effectiveness of multiple prediction heads.

\noindent \textbf{What is the effect of the IoU loss? }
In scene diffusion models, we employ noise prediction loss as the primary supervision, focusing on attribute denoising of individual object instances. However, this loss does not address object intersections within a scene. To alleviate the issue, we augment it with pair-wise bounding box IoU loss. Quantitative comparisons indicate that incorporating IoU loss results in the synthesis of scenes with improved symmetry and enhanced plausibility, as evidenced by lower FID, KID, SCA, PIoU and higher Sym.

\noindent \textbf{What is the effect of geometry feature diffusion?}
To evaluate our method's performance without geometry feature diffusion, we eliminate the geometry feature encoding and prediction heads from our denoiser network. Consequently, this method only produces bounding boxes and class labels for objects within a scene.
During inference, 
 for each generated object, we conduct shape retrieval in the 3D-FUTURE~\cite{fu20213dm} dataset to find the CAD model with the same class label and the closest 3D bounding box sizes.
 %
Fig. 5 of the main paper shows that our model can find symmetric nightstands by beds due to the geometry awareness of the diffusion process and shape retrieval. Table 3 in the main paper presents the comparison in the formation of symmetric pairs: 0.72 (w/ shape diffusion) vs. 0.50 (w/o shape diffusion).
This highlights the effectiveness of geometry feature diffusion in achieving symmetric placements and semantically coherent arrangements. Improved plausibility in synthesis results is reflected in lower FID, KID, and SCA evaluations. Additionally, the decrease in CKL suggests that the joint diffusion of geometry code and object layout facilitates learning more similar object class distributions.
% \section{Evaluation Metrics}
% \label{SecEval}

% \paragraph{Fr{\'e}chet Inception Distance}

% \paragraph{Kernel Inception Distance}

% \paragraph{Scene Classification Accuracy}

% \paragraph{Category KL Divergenece}



\section{Additional Results}
\label{SecAddRes}

\paragraph{Diversity Analysis.} 
%
The qualitative comparisons in Fig. 7 of the main paper and Fig.~\ref{fig:completion_supple} %of the supplementary 
illustrate that our diffusion-based method can produce more diverse results than the baseline methods.
%
Following ATISS and LEGO, we use FID and KID to quantitatively evaluate the result diversity.
%
We compare both the mean and covariance of generated and reference scene distribution.
%
Additionally, we include Precision / Recall commonly used to evaluate generative models~\cite{kynkaanniemi2019improved}. 
%
Precision is the probability that a randomly generated scene falls within the support of real scene distribution.
%
Recall is the probability that a random scene from the datasets falls within the generated scene distribution.
%
Tab.~\ref{tab:pre_rec} shows that our approach outperfoms all baselines in both metrics, which demonstrates better diversity, plausiblity, and mode coverage. 
%
\input{./tabs/diversity.tex}

%
\begin{figure}[!htp]
    %\vspace{-4mm}
    \centering
\includegraphics[width=.86\linewidth]{./figs/experiments/real_completion/real_completion.pdf}
    %\vspace{-3mm}
    \caption{
    Scene completion of a real scene. We select an sofa and perform CAD retrieval to obtain a partial scene as input.
    }
    \label{fig:real_world_completion}
    %\vspace{-3mm}
\end{figure}
%
\begin{figure}[!htp]
    %\vspace{-0.6cm}
    \centering
\includegraphics[width=.9\linewidth]{./figs/experiments/text_edit/text_edit.pdf}
%\vspace{-0.5cm}
    \caption{
    Text-guided (a) object suggestion (b) scene editing.
    }
    \label{fig:text_editing}
%\vspace{-0.6cm}
\end{figure}
%

\paragraph{Unconditional Scene Synthesis}
%\subsection{Unconditional Scene Synthesis}
\input{./figs/uncond_comparison_supple.tex}
\input{./figs/uncond_gallery.tex}
\input{./figs/complete_supple.tex}
In Fig.~\ref{fig:uncond_comparison_supple}, we provide additional qualitative comparisons against state-of-the-art methods on the unconditional scene synthesis model. Also,  more visualization results of our unconditional scene synthesis model are presented in Fig.~\ref{fig:uncond_gallery}.

\paragraph{Scene Arrangement}
%\subsection{Scene Arrangement}
%\input{./tabs/text.tex}
We visualize additional qualitative comparisons on the task of scene arrangement in Fig.~\ref{fig:arrangement_lego_supple}.  
%
LEGO~\cite{wei2023lego} aims to predict 2D object locations and orientations, taking the input of a floor plane, object semantics and geometries. It does not handle objects like lamps that could hang from the ceiling. 
%
In contrast, DiffuScene is a scene-generative model that predicts 3D instance properties from random noise, including 3D locations and orientations, semantics, and geometries.  
%
Compared to ATISS and LEGO, our method generates various object placement options with better plausibility and more symmetries.
\input{./figs/arrangement_lego_supple.tex}

\paragraph{Scene Completion}
%\subsection{Scene Completion}
We present more qualitative comparisons on the task of scene completion in Fig.~\ref{fig:completion_supple}.
Also, the quantitative results are shown in Tab.~\ref{tab:completion}.  Compared to ATISS, our method produced more diverse completion results with higher fidelity. Our method can consistently outperform ATISS  in all listed metrics.
\input{./tabs/completion_new.tex}

\paragraph{Real-world Scene Generalization}
%\subsection{Real-world Scene Generalization}
%
While trained on synthetic dataset, our method can be evaluated on real-world scenes without finetuning, e.g. for scene completion as shown in Fig.~\ref{fig:real_world_completion}.
%
Compared to ATISS, our method produces a more favourable scene.

\paragraph{Text-conditioned Scene Synthesis}
%\subsection{Text-conditioned Scene Synthesis}
\input{./figs/text2scene_supple.tex}
We provide additional qualitative comparisons on the text-conditioned scene synthesis in Fig.~\ref{fig:text2scene_supple}. 
As observed, in the first and third rows, ATISS has object intersection issues while ours does not. In the second row, our method can correctly generate a corner side table on the left of the armchair. However, ATISS generates a corner side table on the right of the armchair.
 In the fourth row, our method can generate four dining chairs that are consistent with the text description, but ATISS can only generate two dining chairs.
% The quantitative results evaluated by FID, KID, and SCA are reported in Tab.~\ref{tab:text}. Our method consistently outperforms ATISS in all used metrics.
%
\paragraph{Scene editing via texts.} 
In Fig.~\ref{fig:text_editing}, we show that our method can support text-guided object suggestion and scene editing, without changing the attributes of other objects.

\section{User Study}
\label{SecUser}

We conducted a perceptual user study to evaluate the quality of our method against ATISS on the application of text-conditioned scene synthesis.
As shown in Fig.~\ref{fig:user_study}, we provide the visualization of a ground-truth scene used to generate a text prompt as a reference. For each pair of results, a user needs to answer ``which of the generated scene can better match the text prompt?" and ``Which of the generated scene is more reasonable and realistic?".
%needs to decide which of the generated scene can better match the text prompt and judge which of the synthesized scene is more plausibly realistic than the other.
%\TODO{what is plausibly realistic? write down the question that you asked in the study}
We collect the answers of 225 scenes from 45 users and calculate the statistics. 62$\%$ of the user answers prefer our method to ATISS in realism.  55$\%$ of answers think our method is more consistent with the text prompt.

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{./figs/experiments/user_study/question_reference.jpg}
    
    \includegraphics[width=\linewidth]{./figs/experiments/user_study/question_match.jpg}

    \includegraphics[width=\linewidth]{./figs/experiments/user_study/question_realism.jpg}

    \caption{\textbf{User Study UI}. Based on the reference scene used to generate text prompts, users are asked which of the synthesized scene is more matched with the text prompt and more realistic. Note that the results from ATISS and our method are randomly shuffled to avoid bias.}
    \label{fig:user_study}
    
\end{figure*}
