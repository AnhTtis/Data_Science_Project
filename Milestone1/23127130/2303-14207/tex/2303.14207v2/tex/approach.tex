\section{DiffuScene}
\label{SecApp}
%
We introduce DiffuScene, a scene denoising diffusion model aiming at learning the distribution of 3D indoor scenes which includes semantic classes, surface geometries, and placements of multiple objects.
%
%
Specifically, we assume indoor scenes to be located in a world coordinate system with the origin at the floor center, and each scene $\mathcal{S}$ is a composition of at most $N$ objects $\{ \Vec{o} \}_{i=1}^{N}$.
%
%Let $C$ be the number of semantic categories and $F$ be the number of object feature channels.
%
We represent each scene as an unordered set with $N$ objects, each object in a scene set is defined by its class category $\Vec{c} \in \mathbb{R}^C$, object size $\Vec{s}\in \mathbb{R}^3$, location $\Vec{\ell}\in \mathbb{R}^3$, rotation angle around the vertical axis $\Vec{\theta}\in \mathbb{R}$, and shape code $\Vec{f}\in \mathbb{R}^F$ extracted from object surfaces in the canonical system through a pre-trained shape auto-encoder~\cite{yang2018foldingnet}.
%
Since the number of objects varies across different scenes, we define an additional `empty' object and pad it into scenes to have a fixed number of objects across scenes.
%
%Concretely, we denote the semantic label as an additional class category `empty', representing the existence of an object and we assign zero vectors to other properties of empty objects.
%
As proposed in~\cite{yin2021center}, we represent the object rotation angle by parametrizing a 2-d vector of cosine and sine values.
%
In summary, each object $\Vec{o}_i$ is characterized by the concatenation of all attributes,~\ie $\Vec{o}_i = [ \Vec{\ell}_i , \Vec{s}_i , \cos\Vec{\theta}_i  , \sin\Vec{\theta}_i , \Vec{c}_i , \Vec{f}_i ] \in \mathbb{R}^D$, where $D$ is the dimension of concatenated attributes.
%
%$\Vec{o}_i = \Vec{\ell}_i \oplus \Vec{s}_i  \oplus \cos\Vec{\theta}_i  \oplus \sin\Vec{\theta}_i \oplus \Vec{c}_i \oplus \Vec{f}_i \in \mathbb{R}^D$
%
%
Based on this representation, we design our denoising diffusion model in Sec.~\ref{SubSecSceDiff}, which supports many different downstream applications like scene completion, scene re-arrangement, and text-conditioned scene synthesis in Sec.~\ref{SubSecApp}.
%
%\subsection{Multi-object Scene Configuration Denosing}
\subsection{Object Set Diffusion}
\label{SubSecSceDiff}
An overview of our approach is shown in Fig.~\ref{fig:pipeline}.
%Following recent success in denoising diffusion-based generative models~\cite{ho2020denoising, meng2021sdedit}, 
We design a denoising diffusion model that employs Gaussian noise corruptions and removals on object attributes to transition between noisy and clean scene distributions.
%\cite{ho2020denoising, meng2021sdedit, avrahami2022blended, kim2022diffusionclip, avrahami2022blended, saharia2022image,ho2022cascaded, dhariwal2021diffusion, rombach2022high,lugmayr2022repaint}
%
\paragraph{Diffusion process.} 
The (forward) diffusion process is a pre-defined discrete-time Markov chain in the data space $\mathcal{X}$ spanning all possible scene configurations represented as 2D tensors of fixed size $\Vec{x} \in \mathbb{R}^{N \times D}$, which are the concatenations of $N$ object properties $\{ \Vec{o}_i \}_{i=1}^{N}$ within a scene $\mathcal{S}$.
%
Given a clean scene configuration $\Vec{x}_0$ from the underlying distribution  $q(\Vec{x}_0)$, we gradually add Gaussian noise to $\Vec{x}_0$, obtaining a series of intermediate scene variables $\Vec{x}_1, ..., \Vec{x}_T$ with the same dimensionality as $\Vec{x}_0$, according to a pre-defined, linearly increased noise variance schedule $\beta_1, ..., \beta_T$ (where $\beta_1 < ... < \beta_T$).
%
The joint distribution $q (\Vec{x}_{1:T} | \Vec{x}_{0} )$ of the diffusion process can be expressed as:
\vspace{-3mm}
\begin{equation}
	\label{Equadiffusion}
	q (\Vec{x}_{1:T} | \Vec{x}_{0} ) :=  \prod_{t=1}^{T} q(\Vec{x}_{t} | \Vec{x}_{t-1}),
	\vspace{-3mm}
\end{equation}
where the diffusion step at time $t$ is defined as:
\vspace{-1mm}
\begin{equation}
	\label{Equadiffusion_each}
	q (\Vec{x}_{t} | \Vec{x}_{t-1} ) :=  \mathcal{N}(\Vec{x}_{t}; \sqrt{ 1-\beta_{t} } \Vec{x}_{t-1}, \beta_{t} \Vec{I} ).
	\vspace{-1mm}
\end{equation}
A helpful property of diffusion processes is that we can directly sample $\Vec{x}_t$ from $\Vec{x}_0$ via the conditional distribution:
\vspace{-1mm}
\begin{equation}
	\label{Equadiffusion_02t}
	q( \Vec{x}_{t} | \Vec{x}_{0} ) :=  \mathcal{N}( \Vec{x}_{t}; \sqrt{\bar{\alpha_t}}\Vec{x}_{0}, (1-\bar{\alpha_{t}}) \Vec{I} ) ,
\end{equation}
where $\Vec{x}_t = \sqrt{\bar{\alpha}_t} \Vec{x}_0 + \sqrt{1-\bar{\alpha}_t} \Vec{\epsilon}$ where $\alpha_t := 1 - \beta_t$  , $\bar{\alpha}_t := \prod_{r=1}^{t} \alpha_s$, and $\Vec{\epsilon}$ is the noise used to corrupt $\Vec{x}_t$. 
%
%
\paragraph{Generative process.}
%
The generative (\ie denoising) process is parameterized as a Markov chain of learnable reverse Gaussian transitions.
%
Given a noisy scene from a standard multivariate Gaussian distribution $\Vec{x}_T\sim\mathcal{N}(\mathbf{0}, \Vec{I})$ as the initial state, it corrects $\Vec{x}_{t}$ to obtain a cleaner version $\Vec{x}_{t-1}$ at each time step by using a learned Gaussian transition $p_\Vec{\phi} (\Vec{x}_{t-1} | \Vec{x}_{t} )$ which is parameterized by a learnable network $\Vec{\phi}$.
%
By repeating this reverse process until the maximum number of steps $T$, we can reach the final state $\Vec{x}_0$, the clean scene configuration we aim to obtain.
%
Specifically, the joint distribution of the generative process $p_\Vec{\phi} (\Vec{x}_{0:T} )$ is formulated as:
\vspace{-3mm}
\begin{equation}
	\label{Equadenoise}
	p_\Vec{\phi} (\Vec{x}_{0:T} ) :=  p(\Vec{X}_T) \prod_{t=1}^{T} p_\Vec{\phi}( \Vec{x}_{t-1} | \Vec{x}_{t} ).
	\vspace{-1mm}
\end{equation}
% $p_\Vec{\phi} (\Vec{x}_{t-1} | \Vec{x}_{t} )$ 
%
\begin{equation}
	\label{Equadenoise_each}
	p_\Vec{\phi} (\Vec{x}_{t-1} | \Vec{x}_{t} ) :=  \mathcal{N}(\Vec{x}_{t-1}; \Vec{\mu}_{\Vec{\phi}}(\Vec{x}_{t}, t), \Vec{\Sigma}_{\Vec{\phi}}(\Vec{x}_{t}, t)),
\end{equation}
where $\Vec{\mu_{\phi}}(\Vec{x}_{t})$ and $\Vec{\Sigma}_\Vec{\phi}(\Vec{x}_{t})$ are the predicted mean and covariance of the Gaussian $\Vec{x}_{t-1}$ by feeding $\Vec{x}_{t}$ into the denoising network $\Vec{\phi}$.
%
For simplicity, we pre-define the constants of $\Sigma_{\Vec{\phi}}(\Vec{x}_{t}) := \sigma_t := \frac{1-\bar{\alpha}_{t-1}}{ 1-\bar{\alpha}_t } \beta_t$, although Song et al. has shown that learnable covariances can increase generation quality in DDIM~\cite{song2020improved}.
%
Ho et al. empirically found in DDPM~\cite{ho2020denoising} that rather than directly predicting $\Vec{\mu}_{\Vec{\phi}}(\Vec{x}_{t}, t)$, we can synthesize more high-frequent details by estimating the noise $\Vec{\epsilon}_{\Vec{\phi}}(\Vec{x}_{t}, t)$ applied to perturb $\Vec{x}_{t}$.
%
Then $\Vec{\mu}_{\Vec{\phi}}(\Vec{x}_{t})$ can be re-parametrized by subtracting the predicted noise according to Bayes's theorem:
%
\vspace{-2mm}
\begin{equation}
	\label{Equareparam}
	\Vec{\mu}_{\Vec\phi}(\Vec{x}_{t}, t) :=  \frac{1}{\sqrt{\alpha_{t}}} (\Vec{x}_{t} - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \Vec{\epsilon}_{\Vec{\phi}}(\Vec{x}_{t}, t)).
	\vspace{-8mm}
\end{equation}
%
\begin{figure}[t]
	\centering
	\includegraphics[width=.5\textwidth]{./figs/Figure3.jpg}
	%\vspace{-2mm}
	\caption{The denoising network architecture takes the attributes of multiple objects (bounding box, object class, geometry code) as input and denoises them using 1D convolutions with skip connections and attention blocks.}
	\label{fig:denoise_net}
	\vspace{-5mm}
\end{figure}
%
\paragraph{Denoising network.}
%
As shown in Fig.~\ref{fig:denoise_net}, the denoiser in our method is based on 1D convolution with skip connections, where convolution blocks are interleaved with attention blocks~\cite{vaswani2017attention} to aggregate the features of different objects, exploiting the inter-object relationships and capturing the global scene context.
%
% As MLPs are shared with each object, it would be difficult for the denoising network to distinguish objects without position information.
%
% To this end, we also use the positional embeddings of instance IDs and inject them into the network to guide the denoising process.
%
\paragraph{Training objective.}
%
The goal of training the reverse diffusion process is to find optimal denoising network parameters $\Vec{\phi}$ that can generate natural and plausible scenes.
%
Our training objective is composed of two parts:
i) A loss $L_{\text{sce}}$ to constrain that the generated object set can approximate the underlying data distribution, 
and ii) a regularization term $L_{\text{iou}}$ to penalize the object intersections.
%
The $L_{\text{sce}}$ is derived by maximizing the negative log-likelihood of the last denoised scene $\mathbb{E}[ -\log p_{\Vec{\phi}}(\Vec{x}_0) ]$, which is yet not intractable to optimize directly.
%
Thus, we can instead choose to maximize its variational upper bound:
%
\vspace{-2mm}
\begin{equation}
	\label{equaNLL}
	L_{\text{sce}} :=  \mathbb{E}_q [ -\log \frac{p_{\phi}(\Vec{x}_{0:T})} {q(\Vec{x}_{1:T}|\Vec{x}_0)} ] \ge \mathbb{E}[ -\log p_{\Vec{\phi}}(\Vec{x}_0) ].
	\vspace{-1mm}
\end{equation}
%Kullbackâ€“Leibler (KL)
%$L_{\text{sce}}^t$
By surrogating variables, we can further simplify $L_{\text{sce}}$ as the sum of KL divergence between posterior $p_{\Vec{\phi}} (\Vec{x}_{t-1}  | \Vec{x}_{t}, \Vec{x}_{0})$ and conditional distribution $q(\Vec{x}_{t} | \Vec{x}_{t-1})$ at each $t$ :
\vspace{-3mm}
\begin{equation}
	\label{equaDecompose}
	\begin{aligned}
		L_{\text{sce}} 
		%:= \mathbb{E}_q [ -\log p(\Vec{x}_T) - \sum_{t=1}^{T} L_{\text{sce}}^t ]\quad \quad \quad \quad \quad \quad  \\ 
		:=  \mathbb{E}_q [ -\log p(\Vec{x}_T) - \sum_{t=1}^{T} \log \frac{ p_{\Vec{\phi}} (\Vec{x}_{t-1} | \Vec{x}_{t}, \Vec{x}_{0}) } {q(\Vec{x}_{t} | \Vec{x}_{t-1})} ] ,
		\vspace{-5mm}
	\end{aligned}
\end{equation}
where $-\log p(\Vec{x}_T)$ is a fixed constant since $\Vec{x}_T \sim \mathcal{N}(0, \Vec{I})$.
%
Here, we refer to DDPM~\cite{ho2020denoising} for the details of the derivation process.
%
Moreover, we can re-write $L_{\text{sce}}$ into a simple and intuitive version that constrains the correct prediction of the corrupted noise on $\Vec{x}_t$:
%
\vspace{-2mm}
\begin{equation}
	\label{equaNoisypred}
	\begin{aligned}
		L_{\text{sce}} := \mathbb{E}_{\Vec{x}_0, \Vec{\epsilon}, t} [ \| \Vec{\epsilon} - \Vec{\epsilon}_{\Vec{\phi}} (\Vec{x}_t, t) \|^2 ] \quad \quad \quad \quad \quad \quad \\
		:= \mathbb{E}_{\Vec{\phi}} [\| \Vec{\epsilon} - \Vec{\epsilon}_{\Vec{\phi}} ( \sqrt{\bar{\alpha}_t} \Vec{x}_0 +  \sqrt{1-\bar{\alpha}_t} \Vec{\epsilon}, t) \|^2] .
	\end{aligned}
	\vspace{-2mm}
\end{equation}
%
Based on Eq.~\ref{Equareparam}, we can obtain the approximation of clean scene $\tilde{\Vec{x}}_0^t$.
%
Thus, we can compute $L_{\text{iou}}$ as the IoU summation of arbitrary two bounding boxes:
\vspace{-2mm}
\begin{equation}
	\label{equaIoU}
	L_{\text{iou}} :=  \sum_{t=1}^{T} 0.1 * \bar{\alpha}_t  * \sum_{\Vec{o}_i, \Vec{o}_j \in \tilde{\Vec{x}}_0^t} \IoU(\Vec{o}_i, \Vec{o}_j).
	\vspace{-2mm}
\end{equation}
%where the hyperparamter $\omega_t$ is set to $\bar{\alpha}_t * 0.1$.
%
%\paragraph{Sampling.} In the inference phase, we adopt the ancestral sampling strategy to progressively decorrupt the 3D scene graph via $\Vec{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} (\Vec{x}_t-\frac{1-\alpha_t} {\sqrt{1-\bar{{\alpha}_t}}} \Vec{\epsilon}(\Vec{x}_t, t) ) + \sqrt{\beta_t} \Vec{z}$ where $\Vec{z} \sim \mathcal{N}(0, \Vec{I})$ until the final state $\Vec{x}_{0}$.
%
\subsection{Applications}
\label{SubSecApp}
Based on our diffusion model above, we can support various downstream tasks (see Fig.~\ref{fig:teaser}) with few modifications. 
%
\vspace{-2mm}
\paragraph{Scene completion.}
%
Assuming a partial scene with $M (\le N)$ objects, \ie $\Vec{y} \in \mathbb{R}^{M \times D}$, we utilize the learned scene priors from diffusion models to complement novel  $\hat{\Vec{x}}_0)$ into $\Vec{y}_0$ to obtain a complete object set  $\Vec{x}_0 = (\Vec{y}, \hat{\Vec{x}}_0)$.
%Similar to image in-painting~\cite{ lugmayr2022repaint, rombach2022high} and shape completion~\cite{luo2021diffusion,zhou20213d,zeng2022lion,hui2022neural,zhang20233dshape2vecset} , the completion process is almost the same as unconditional scene generation, except that 
% We keep the already known elements in the forward Gaussian transitions $q$, and only hallucinate the missing ones through learnable reverse Gaussian transitions $q_\Vec{\phi}$.
% 
We keep the already known elements and only hallucinate the missing ones through learnable reverse Gaussian transitions $q_\Vec{\phi}$ conditioning on $\Vec{y}$.
%
% Concretely, the intermediate scene during the diffusion process can be revised as $\Vec{x}_t = (\Vec{y}_t, \hat{\Vec{x}}_t)$, where the partial scene $\Vec{y}_t$ at time step $t$ is obtained by the forward diffusions.
%
The complemented scene $\hat{\Vec{x}}_t$ at time step $t$ is generated by:
\vspace{-1mm}
\begin{equation}
	\begin{aligned}
		\label{EquanCompletion}
		%q(\Vec{y}_t | \Vec{y}_{t-1}) := \mathcal{N}(\Vec{y}_t; \sqrt{\bar{\alpha}_t}\Vec{y_0},(1-\bar{\alpha}_t\Vec{I})), \\ 
		p_{\Vec{\phi}}( \hat{\Vec{x}}_{t-1} | \hat{\Vec{x}}_t  ) := \mathcal{N}(\mu_{\Vec{\phi}}(\Vec{x}_t, t, \Vec{y}), \sigma_t^2\Vec{I}) . \quad \quad \quad 
	\end{aligned}
	\vspace{-2mm}
\end{equation}
%
\paragraph{Scene re-arrangement.}
Given a set of objects with random spatial positions, we can leverage the priors of our diffusion model to rearrange reasonable object placements by estimating their locations and orientations.
%
We denote the noisy scene initialization as $\hat{\Vec{x}}_0 = [\hat{\Vec{u}}_0, \Vec{v}]$, where $\hat{\Vec{u}}_0 = \{  [\Vec{l}_i, \cos\theta_i, \sin\theta_i] \}_{i=1}^N$ is the concatenation of $N$ objects' locations and orientations, and $\Vec{v} = \{ [\Vec{s}_i, \Vec{c}_i, \Vec{f}] \}_{i=1}^N$ is the concatenation of $N$ objects' sizes, category classes, and shape codes.
%
The intermediate scenes during the arrangement diffusion process can be expressed as:
\vspace{-1mm}
\begin{equation}
	\begin{aligned}
		\label{EquanRearrange}
		%q(\Vec{v}_t | \Vec{v}_{t-1}) := \mathcal{N}(\Vec{v}_t; \sqrt{\bar{\alpha}_t}\Vec{v_0}, (1-\bar{\alpha}_t\Vec{I})), \\ 
		p_{\Vec{\phi}}( \hat{\Vec{u}}_{t-1} | \hat{\Vec{u}}_t ) := \mathcal{N}(\mu_{\Vec{\phi}}(\hat{\Vec{u}}_t , t, \Vec{v}), \sigma_t^2\Vec{I}) , \quad \quad \quad
	\end{aligned}
\end{equation}
where we iteratively update the object locations and orientations $\Vec{u}_t$ via $p_\Vec{\phi}$ conditioned on $\Vec{v}$.
%
\vspace{-2mm}
\paragraph{Text-conditioned scene synthesis.}
Given a list of sentences describing the desired object classes and inter-object spatial relationship as conditional inputs, we can employ a pre-trained BERT encoder~\cite{devlin2018bert} to extract word embeddings $\Vec{z} \in \mathbb{R}^{48 \times 768}$, then we utilize cross attention layers to inject the language guidance into the denoising network that predicts out noise via $\Vec{\epsilon}_\Vec{\phi}(\Vec{x}_t, t, \Vec{z})$, as depicted in Fig.~\ref{fig:denoise_net}.


