{
    "arxiv_id": "2303.14207",
    "paper_title": "DiffuScene: Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis",
    "authors": [
        "Jiapeng Tang",
        "Yinyu Nie",
        "Lev Markhasin",
        "Angela Dai",
        "Justus Thies",
        "Matthias Nie√üner"
    ],
    "submission_date": "2023-03-24",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "We present DiffuScene for indoor 3D scene synthesis based on a novel scene graph denoising diffusion probabilistic model, which generates 3D instance properties stored in a fully-connected scene graph and then retrieves the most similar object geometry for each graph node i.e. object instance which is characterized as a concatenation of different attributes, including location, size, orientation, semantic, and geometry features. Based on this scene graph, we designed a diffusion model to determine the placements and types of 3D instances. Our method can facilitate many downstream applications, including scene completion, scene arrangement, and text-conditioned scene synthesis. Experiments on the 3D-FRONT dataset show that our method can synthesize more physically plausible and diverse indoor scenes than state-of-the-art methods. Extensive ablation studies verify the effectiveness of our design choice in scene diffusion models.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14207v1"
    ],
    "publication_venue": "13 figures, 5 tables"
}