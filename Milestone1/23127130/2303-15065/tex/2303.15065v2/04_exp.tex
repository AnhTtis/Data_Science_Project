\section{Experiments and Results}

\noindent{\textbf{Datasets.}}
To enable fair evaluation between our predictions and the reference HR ground truths, the in-plane SNR between the LR input scan and corresponding ground truth has to match. To synthetically create 2D LR images, it is necessary to downsample out-of-plane in the image domain anisotropically \cite{zhao2020smore} while preserving in-plane resolution. Consequently, to mimic realistic 2D clinical protocol, which often has higher in-plane details than that of 3D scans, we use spline interpolation to model partial volume and downsampling. We demonstrate our network's modeling capabilities for different contrasts (T1w, T2w, FLAIR, DIR), views (axial, coronal, sagittal), and pathologies (MS, brain tumor). We conduct experiments on two public datasets, BraTS \cite{menze2014multimodal}, and MSSEG \cite{commowick2016msseg}, and an in-house clinical MS dataset (cMS). In each dataset, we select 25 patients that fulfill the isotropic acquisition criteria for both ground truth HR scans. 
Note that we only use the ground truth HR for evaluation, not anywhere in training. We optimize separate INRs for each subject with supervision from only its two LR scans. 
If required, we employ skull-stripping \cite{isensee2019automated} and rigid registration to the MNI152 (MSSEG, cMS) or SRI24 (BraTS) templates. For details, we refer to Table 2 in the supplementary.\\

\noindent{\textbf{Metrics.}}
We evaluate our results by employing common SR \cite{dong2015image,lyu2020multi,wu2021irem} quality metrics, namely PSNR and SSIM. To showcase perceptual image quality, we additionally compute the Learned Perceptual Image Patch Similarity (LPIPS) \cite{zhang2018unreasonable} and measure the absolute error $\epsilon_{MI}$ in mutual information of two upsampled images to their ground truth counterparts as follows:
\begin{equation}
\small
    \varepsilon_{MI}^{C_i} = \frac{1}{N}\sum_{k=1}^N|\mathrm{MI}(\hat{I}_i^k, I_j^k)-\mathrm{MI}(I_i^k, I_j^k)| ;~~
    \hat{\varepsilon}_{MI} = \frac{1}{N}\sum_{k=1}^N|\mathrm{MI}(\hat{I}_1^k, \hat{I}_2^k)-\mathrm{MI}(I_1^k, I_2^k)|\nonumber
\end{equation}

\noindent{\textbf{Baselines and Ablation.}}
To the best of our knowledge, there are no prior data-driven methods that can perform MCSR on a single-subject basis. Hence, we provide single-subject baselines that operate solely on single contrast and demonstrate the benefit of information transfer from other contrasts with our proposed models. In addition, we show ablations of our proposed split head model compared to our vanilla INR. Precisely, our experiments include:

\noindent{\textbf{\emph{{Baseline 1}}:} Cubic-spline interpolation is applied on each contrast separately. \\
\noindent{\textbf{\emph{{Baseline 2}}:} LRTV \cite{shi2015lrtv} applied on each contrast separately. \\
\noindent{\textbf{\emph{{Baseline 3}}:} SMORE (v3.1.2) \cite{zhao2020smore} applied on each contrast separately. \\
\noindent{\textbf{\emph{{Baseline 4}}:} Two single-contrast INRs with one output channel each.

\noindent{\textbf{\emph{{Our vanilla INR (ablation)}}}}: Single INR with two output channels that jointly predicts the two contrast intensities.

\noindent{\textbf{\emph{{Our proposed split-head INR}}}}: Single INR with two separate heads that jointly predicts the two contrast intensities (cf. Fig. \ref{fig:approach}).\\


\begin{table}[t!]
\caption{Quantitative results for MCSR on two public and one in-house datasets. All metrics consistently show that our split-head INR performs the best for MCSR.}\label{tab1}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{l|l|r|r|r|r|r|r|r|r|r}
\toprule
\multirow{8}*{\rotatebox[origin=c]{90}{BraTS 2019}} & Contrasts &  \multicolumn{3}{c|}{T1w} & \multicolumn{3}{c|}{T2w} & \multicolumn{3}{c}{T1w \& T2w}\\
\cline{2-11}
& Methods & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & $\varepsilon_{MI}^{T1}$ $\downarrow$ & $\varepsilon_{MI}^{T2}$ $\downarrow$ & $\hat{\varepsilon}_{MI}$ $\downarrow$\\
\cmidrule{2-11}
& Cubic Spline & 21.201 & 0.896 & 0.098 & 26.201 & 0.932 & 0.058 & 0.096 & 0.087 & 0.145 \\
& LRTV & 21.328 & 0.919 & 0.052 & 24.206 & 0.915 & 0.053 & 0.126 & 0.127 & 0.203 \\
& SMORE & 26.266 & 0.942 & 0.030 & 28.466 & 0.942 & 0.030 & 0.157 & 0.127 & 0.225 \\

& Single Contrast INR & 26.168& 0.952& 0.036& 29.920 &0.957 & 0.028& 0.051& 0.030& 0.079\\
\cmidrule{2-11}
& Our vanilla INR & 26.196 & 0.960 & 0.032 & 29.777 & 0.962 & 0.026& 0.008& 0.015& 0.065\\
& Our split-head INR & \textbf{28.746}& \textbf{0.965}& \textbf{0.028}& \textbf{31.802} & \textbf{0.966} & \textbf{0.024}&\textbf{0.007} &\textbf{0.014} &\textbf{0.062}\\


\toprule
\multirow{8}*{\rotatebox[origin=c]{90}{MSSEG 2016}} & Contrasts &  \multicolumn{3}{c|}{T1w} & \multicolumn{3}{c|}{Flair} & \multicolumn{3}{c}{T1w \& Flair}\\
\cline{2-11}
& Methods & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & $\varepsilon_{MI}^{T1}$ $\downarrow$ & $\varepsilon_{MI}^{Flair}$ $\downarrow$ & $\hat{\varepsilon}_{MI}$ $\downarrow$\\
\cmidrule{2-11}
& Cubic Spline & 30.102 & 0.953 & 0.051 & 28.724 & 0.945 & 0.054 & 0.062 & 0.087 & 0.115 \\
& LRTV & 22.848 & 0.860 & 0.050 & 23.920 & 0.872 & 0.044 & 0.068 & 0.052 & 0.095 \\
& SMORE & 25.729 & 0.937 & 0.030 & 27.430 & 0.940 & 0.029 & 0.138 & 0.100 & 0.183 \\
& Single Contrast INR & 30.852  & 0.956 & 0.029 &31.156 & 0.955 & 0.030 & 0.047&0.074  & 0.095\\
\cmidrule{2-11}
& Our vanilla INR & 31.599 & 0.966 & \textbf{0.019} & 32.312 & 0.969 & 0.019 & \textbf{0.008} & 0.025 & \textbf{0.024}\\
& Our split-head INR & \textbf{31.769} & \textbf{0.967} & \textbf{0.019} & \textbf{32.514} & \textbf{0.970} & \textbf{0.018} & \textbf{0.008}& \textbf{0.023}& \textbf{0.024}\\


\toprule
\multirow{8}*{\rotatebox[origin=c]{90}{cMS}} & Contrasts &  \multicolumn{3}{c|}{DIR} & \multicolumn{3}{c|}{Flair} & \multicolumn{3}{c}{DIR \& Flair}\\
\cline{2-11}
& Methods & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & $\varepsilon_{MI}^{DIR}$ $\downarrow$ & $\varepsilon_{MI}^{Flair}$ $\downarrow$ & $\hat{\varepsilon}_{MI}$ $\downarrow$\\
\cmidrule{2-11}
& Cubic Spline & 28.106 & 0.929 & 0.065 & 26.545 & 0.923 & 0.079 & 0.083 & 0.096 & 0.136 \\
& LRTV & 28.725 & 0.904 & 0.033 & 22.766 & 0.835 & 0.057 & 0.269 & 0.088 & 0.312 \\
& SMORE & 28.933 & 0.926 & 0.040 & 25.336 & 0.921 & 0.039 & 0.124 & 0.079 & 0.139 \\
& Single Contrast INR & 29.941&0.937 & 0.037 & 28.655 & 0.936 & 0.041& 0.063& 0.072& 0.096\\
\cmidrule{2-11}
& Our vanilla INR & 30.816& \textbf{0.956}&0.024 &29.749 & 0.950 & 0.029 & 0.022 & \textbf{0.033} & \textbf{0.009}\\
& Our split-head INR & \textbf{31.686}& \textbf{0.956}& \textbf{0.023} & \textbf{30.246} & \textbf{0.952} & \textbf{0.028} & \textbf{0.021} & \textbf{0.033} & \textbf{0.009}\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


\noindent{\textbf{Quantitative Analysis.}} Table \ref{tab1} demonstrates that our proposed framework poses a trustworthy candidate for the task of MCSR.
As observed in \cite{zhao2020smore}, LRTV struggles for anisotropic up-sampling while SMORE's overall performance is better than cubic-spline, but slightly worse to single-contrast INR. However, the benefit of single-contrast INR may be limited if not complemented by additional views as in~\cite{wu2021irem}.
For \textit{MCSR} from single-subject scans, we achieve encouraging results across all metrics for all datasets, contrasts, and views.
Since T1w and T2w both encode anatomical structures, the consistent improvement in BraTS for both sequences serves as a proof-of-concept for our approach.
As FLAIR is the go-to-sequence for MS lesions, and T1w does not encode such information, the results are in line with the expectation that there could be a relatively higher transfer of anatomical information to pathologically more relevant FLAIR than vice-versa.
Lastly, given their similar physical acquisition and lesion sensitivity, we note that DIR/FLAIR benefit to the same degree in the cMS dataset.\\

\begin{figure}[t!]
    \centering
    \includegraphics[clip, trim=7cm 4.0cm 3cm 4.0cm, scale=0.38]{figures/test.pdf}
    \caption{Qualitative results for MCSR for cMS. The predictions of the split-head INR demonstrate the transfer of anatomical and lesion knowledge from complementing views and sequences. Yellow boxes highlight details recovered by the split-head INR in the out-of-plane reconstructions, where others struggle.}
    \label{fig:visual}
\end{figure}

\noindent{\textbf{Qualitative Analysis.}}
Fig. \ref{fig:visual} shows the typical behavior of our models on cMS dataset, where one can qualitatively observe that the split-head INR preserves the lesions and anatomical structures shown in the yellow boxes, which other models fail to capture.
While our reconstruction is not identical to the GT HR, the coronal view confirms anatomically faithful reconstructions despite not receiving any in-plane supervision from any contrast during training.
We refer to Fig. 4 in the supplementary for similar observations on BraTS and MSSEG.\\

\section{Discussion and Conclusion}
Given the importance and abundance of large multi-parametric retrospective cohorts \cite{menze2014multimodal,commowick2016msseg}, our proposed approach will allow the upscaling of LR scans with the help of other sequences.
Deployment of such a model in clinical routine would likely reduce acquisition time for multi-parametric MRI protocols maintaining an acceptable level of image fidelity. Importantly, our model exhibits trustworthiness in its clinical applicability being 1) subject-specific, and 2) as its gain in information via super-resolution is validated by MI preservation and is not prone to hallucinations that often occur in a typical generative model.

In conclusion, we propose the first subject-specific deep learning solution for isotropic 3D super-resolution from anisotropic 2D scans of two different contrasts of complementary views. Our experiments provide evidence of inter-contrast information transfer with the help of INR. Given the supervision of only single subject data and trained within minutes on a single GPU, we believe our framework to be potentially suited for broad clinical applications. Future research will focus on prospectively acquired data, including other anatomies.

\subsection*{Acknowledgement} 
JM, MM and JSK are supported by Bavarian State Ministry for Science and Art (Collaborative Bilateral Research Program Bavaria – Québec: AI in medicine, grant F.4-V0134.K5.1/86/34). SS, RG and JSK are supported by European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (101045128-iBack-epic-ERC2021-COG). MD and DR are supported by ERC (Deep4MI - 884622) and ERA-NET NEURON Cofund (MULTI-FACT - 8810003808). HBL is supported by an Nvidia GPU grant. 