\subsubsection{Related Work.}
Single-image super-resolution (SISR) \cite{bhowmik2017training} aims at restoring a high-resolution (HR) image from a low-resolution (LR) input from a single sequence and targets applications such as low-field MR upsampling or optimization of MRI acquisition \cite{chen2018brain}.
Recent methods \cite{chen2018brain,georgescu2020convolutional} incorporate priors learned from a training set \cite{chen2018brain}, which is later combined with generative models \cite{chen2020mri}.
On the other hand, multi-image super-resolution (MISR) relies on the information from complementary views of the same sequence \cite{wu2021irem} and is especially relevant to capturing temporal redundancy in motion-corrupted low-resolution MRI \cite{gholipour2010robust,wesarg2010combining}.

Multi-contrast Super-resolution (MCSR) targets using inter-contrast priors \cite{rousseau2010non}. In conventional settings \cite{manjon2010mri}, an isotropic HR image of another contrast is used to guide the reconstruction of an anisotropic LR image.
Zeng et al. \cite{zeng2018simultaneous} use a two-stage architecture for both SISR and MCSR.
Utilizing a feature extraction network, Lyu et al. \cite{lyu2020multi} learn multi-contrast information in a joint feature space.
Later, multi-stage integration networks \cite{feng2021multi}, separatable attention \cite{feng2021exploring} and transformers \cite{li2022transformer} have been used to enhance joint feature space learning.
However, all current MCSR approaches are limited by their need for a large training dataset. Consequently, this constrains their usage to specific resolutions and further harbors the danger of hallucination of features (e.g., lesions, artifacts) present in the training set and does not generalize well to unseen data.

Originating from shape reconstruction \cite{park2019deepsdf} and multi-view scene representations \cite{mildenhall2020nerf}, Implicit Neural Representations (INR) have achieved state-of-the-art results by modeling a continuous function on a space from discrete measurements. Key reasons behind INR's success can be attributed to overcoming the low-frequency bias of Multi-Layer Perceptrons (MLP) \cite{tancik2020fourier,sitzmann2020implicit,saragadam2023wire}. 
Although MRI is a discrete measurement, the underlying anatomy is a continuous space. We find INR to be a good fit to model a continuous intensity function on the anatomical space. Once learned, it can be sampled at an arbitrary resolution to obtain the super-resolved MRI.
Following this spirit, INRs have recently been successfully employed in medical imaging applications ranging from k-space reconstruction \cite{huang2023neural} to SISR \cite{wu2021irem}.
Unlike \cite{wu2021irem,shen2022nerp}, which learn anatomical priors in single contrasts, and \cite{wu2022ASSR,amiranashvili2022learning}, which leverage INR with latent embeddings learned over a cohort, we focus on employing INR in subject-specific, multi-contrast settings.