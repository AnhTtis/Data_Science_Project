\section{Methods}
\vspace{-0.3em}
\begin{figure}[tbh]
    \centering
    \includegraphics[width=\linewidth,trim=40 10 0 0, clip]{figures/overview_miccai.pdf}
    \caption{Overview of our proposed approach (best viewed in full screen). a) Given a realistic clinical scenario, two MRI contrasts are acquired in complementary 2D views. b) Our proposed INR models both contrast from the supervision available in the 2D scans and, by doing so, learn to transfer knowledge from in-plane measurements to out-of-plane of the other contrast. Although our model is trained on MSELoss only for the observed coordinates, it constructs a continuous function space, converging to an optimum state of mutual information between the contrasts on the global space of $\Omega$. c) Once learned, we can sample an isotropic grid and obtain the anatomically faithful and pathology-preserving super-resolution.}
    \label{fig:approach}
\end{figure}

In this section, we first formally introduce the problem of joint super-resolution of multi-contrast MRI from only one image per contrast per patient. Next, we describe strategies for embedding information from two contrasts in a shared space. Subsequently, we detail our model architecture and training configuration.\\

\noindent{\textbf{Problem Statement.}}
Let us consider two imaging contrasts, $C_1$ and $C_2$.
The image intensities are a function of the underlying anatomical structure.
We denote the collection of all 3D coordinates of interest in this anatomical space as $\Omega=\{(x,y,z)\}$ with anatomical properties $q(\Omega)$.
Individual contrasts can be scanned in a low-resolution subspace $\Omega_1,\Omega_2 \subset \Omega$.
Let us also consider two functions $g_1$ and $g_2$ that map from anatomical properties $q$ to their corresponding contrast intensities $I_1$ and $I_2$; given sparse observations $I_1=g_1(q(\Omega_1))=f_1(\Omega_1)$ and $I_2=g_2(q(\Omega_2))=f_2(\Omega_2)$, where $f_i$ is composition of $g_i$ and $q$.
However, one can easily obtain the global anatomical space $\Omega$ by knowing $\Omega_1$ and $\Omega_2$, for example, by rigid image registration between the two spaces.
In this paper, we aim to estimate $f_1(\Omega) $ and $f_2(\Omega)$ given $I_1=f_1(\Omega_1)$ and $I_2=f_2(\Omega_2)$.\\


\noindent{\textbf{Joint Multi-contrast Modelling.}}
Since both component-functions $f_1$ and $f_2$ operate on a subset of the same input space, we argue that it is beneficial to model them jointly as a single function $f$ on $\Omega$ and optimize it based on their estimation error incurred in their respective subsets.  
This will enable information transfer from one contrast to another, thus improving the estimation and preventing over-fitting in single contrasts, bringing consistency to the prediction.
To this end, we propose to leverage INR to model a continuous multi-contrast function $f$ from discretely sampled sparse observations $I_1$ and $I_2$.\\


\noindent{\textbf{MCSR Setup.}}
Without loss of generalization, let us consider two LR input contrasts scanned in two orthogonal planes $p_1$ and $p_2$, where $p_1, p_2\in$ \{axial, sagittal, coronal\}. We assume they are aligned by rigid registration requiring no coordinate transformation.
Their corresponding in-plane resolutions are $(s_1\times s_1)$ and $(s_2\times s_2)$ and slice thickness is $t_1$ and $t_2$, respectively. Note that $s_1<t_1$ and $s_2<t_2$ imply high in-plane and low out-of-plane resolution. In the end, we aim to sample an isotropic $(s\times s \times s)$ grid for both contrasts where $s\leq s_1,s_2$.\\

\noindent{\textbf{Implict Neural Representations for MCSR.}}
We intend to project the information available in one contrast into another by embedding both in the shared weight space of a neural network.
However, a high degree of weight sharing could hinder contrast-specific feature learning.
Based on this reasoning, we aim to hit the sweet spot where maximum information exchange can be encouraged without impeding contrast-specific expressiveness.
We propose a split-head architecture, as shown in Fig. \ref{fig:approach}, where the initial layers jointly learn the common anatomical features, and subsequently, two heads specialize in contrast-specific information. The model takes Fourier \cite{tancik2020fourier} Features $\boldsymbol{v} = [cos(2 \pi B\boldsymbol{x}), sin(2 \pi B\boldsymbol{x})] ^{T}$ as input and predicts $[\hat{I}_1, \hat{I}_2] = f(\boldsymbol{v})$, where $\boldsymbol{x}=(x,y,z)$ and $B$ is sampled from a Gaussian distribution $\mathcal{N}(\mu, \sigma^2)$. We use mean-squared error loss, $\mathcal{L}_{{MSE}}$, for training.
\begin{equation}
    \mathcal{L}_{{MSE}}=\alpha\sum_{\boldsymbol{x}\in\Omega_1}(\hat{I}_1(\boldsymbol{v})-I_1(\boldsymbol{x}))^2+\beta\sum_{\boldsymbol{x}\in\Omega_2}(\hat{I}_2(\boldsymbol{v})-I_2(\boldsymbol{x}))^2
\end{equation}
where $\alpha$ and $\beta$ are coefficients for the reconstruction loss of two contrasts. Note that for points $\{(x,y,z)\}\in \Omega_{2} \setminus \Omega_{1}$, there is no explicit supervision coming from low resolution $C_1$. For these points, one can interpret learning $C_1$ from the loss in $C_2$ to be a weakly supervised task, and vice versa.\\


\noindent{\textbf{Implementation and Training.}}
Given the rigidly registered LR images, we compute $\Omega_{1}$, $\Omega_{2} \in \Omega$ in the scanner reference space using their affine matrices. Subsequently, we normalize $\Omega$ to the interval $[-1,1]^3$ and independently normalize each contrast's intensities to $[0,1]$. We use 512-dimensional Fourier Features in the input. Our model consists of a four-layer MLP with a hidden dimension of 1024 for the shared layers and two layers with a hidden dimension of 512 for the heads. We use ReLU activation and Adam optimizer with a learning rate of 4e-4 and a Cosine annealing rate scheduler with a batch size of 1000. For the multi-contrast INR models, we use MI as in Eq. \ref{eq:mi} for early stopping. Implemented in PyTorch \footnote{Code: \url{https://github.com/jqmcginnis/multi_contrast_inr/}}, we train our model on a single A6000 GPU. Please refer to Tab. \ref{tab:parameters} in supplementary for an exhaustive hyper-parameter search. \\

\noindent{\textbf{Model Selection and Inference.}}
Since our model is trained on sparse sets of coordinates, it is prone to overfitting them and has little incentive to generalize in out-of-plane predictions for single contrast settings. A remedy to this is to hold random points as a validation set. However, this will reduce the number of training samples and hinder the reconstruction of fine details. For multi-contrast settings, one can exploit the agreement between the two predicted contrasts. Ideally, the network should reach an equilibrium between the contrasts over the training period, where both contrasts optimally benefit from each other. We empirically show that Mutual Information (MI) \cite{wells1996multi} is a good candidate to capture such an equilibrium point without the need for ground truth data in its computation. For two predicted contrasts $\hat{I}_1$ and $\hat{I}_2$, MI can be expressed as:

\begin{equation}
\mathrm{MI}(\hat{I}_1, \hat{I}_2)=\sum_{y \in \hat{I}_2} \sum_{x \in \hat{I}_1} P_{(\hat{I}_1, \hat{I}_2)}(x, y) \log \left(\frac{P_{(\hat{I}_1, \hat{I}_2)}(x, y)}{P_{\hat{I}_1}(x) P_{\hat{I}_2}(y)}\right)\label{eq:mi}
\end{equation}
Compared to image registration, we do not use MI as a loss for aligning two images; instead, we use it as a quantitative assessment metric. Given two ground truth HR images for a subject, one can compute the optimum state of MI. We observe that the MI between our model predictions converges close to such an optimum state over the training period without any explicit knowledge about it, c.f. Fig. \ref{fig:qual} in the supplementary. This observation motivates us to detect a plateau in MI between the predicted contrasts and use it as a stopping criterion for model selection in multi-contrast INR.
