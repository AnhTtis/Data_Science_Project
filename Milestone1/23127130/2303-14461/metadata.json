{
    "arxiv_id": "2303.14461",
    "paper_title": "Indian Language Summarization using Pretrained Sequence-to-Sequence Models",
    "authors": [
        "Ashok Urlana",
        "Sahil Manoj Bhatt",
        "Nirmal Surange",
        "Manish Shrivastava"
    ],
    "submission_date": "2023-03-25",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL"
    ],
    "abstract": "The ILSUM shared task focuses on text summarization for two major Indian languages- Hindi and Gujarati, along with English. In this task, we experiment with various pretrained sequence-to-sequence models to find out the best model for each of the languages. We present a detailed overview of the models and our approaches in this paper. We secure the first rank across all three sub-tasks (English, Hindi and Gujarati). This paper also extensively analyzes the impact of k-fold cross-validation while experimenting with limited data size, and we also perform various experiments with a combination of the original and a filtered version of the data to determine the efficacy of the pretrained models.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14461v1"
    ],
    "publication_venue": "Accepted at FIRE-2022, Indian Language Summarization (ILSUM) track"
}