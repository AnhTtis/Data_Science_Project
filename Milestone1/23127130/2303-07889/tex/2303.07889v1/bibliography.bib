@inproceedings{albrecht1997bayesian,
	title        = {{Towards a Bayesian Model for Keyhole Plan Recognition in Large Domains}},
	author       = {Albrecht, David W. and Zukerman, Ingrid and Nicholson, Ann E. and Bud, Ariel},
	year         = 1997,
	booktitle    = {User Modeling},
	publisher    = {Springer},
	address      = {Vienna},
	series       = {International Centre for Mechanical Sciences},
	pages        = {365--376},
	doi          = {10.1007/978-3-7091-2670-7_37},
	isbn         = {978-3-7091-2670-7},
	langid       = {english}
}
@article{atkeson1997robot,
	title        = {{Robot Learning From Demonstration}},
	author       = {Atkeson, Christopher G and Schaal, Stefan},
	year         = 1997,
	abstract     = {The goal of robot learning from demonstration is to have a robot learn from watching a demonstration of the task to be performed. In our approach to learning from demonstration the robot learns a reward function from the demonstration and a task model from repeated attempts to perform the task. A policy is computed based on the learned reward function and task model. Lessons learned from an implementation on an anthropomorphic robot arm using a pendulum swing up task include 1) simply mimicking demonstrated motions is not adequate to perform this task, 2) a task planner can use a learned model and reward function to compute an appropriate policy, 3) this modelbased planning process supports rapid learning, 4) both parametric and nonparametric models can be learned and used, and 5) incorporating a task level direct learning component, which is non-model-based, in addition to the model-based planner, is useful in compensating for structural modeling errors and slow model learning.},
	langid       = {english}
}
@inproceedings{augustsson2014how,
	title        = {{How to Transfer Information between Collaborating Human Operators and Industrial Robots in an Assembly}},
	author       = {Augustsson, Svante and Olsson, Jonas and Christiernin, Linn Gustavsson and Bolmsj{\"o}, Gunnar},
	year         = 2014,
	month        = oct,
	booktitle    = {Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{{NordiCHI}} '14},
	pages        = {286--294},
	doi          = {10.1145/2639189.2639243},
	isbn         = {978-1-4503-2542-4},
	keywords     = {coproduction,human-robot interaction,industrial assembly,information transfer}
}
@article{bland2012different,
	title        = {{Different Varieties of Uncertainty in Human Decision-Making}},
	author       = {Bland, Amy and {Schaefer}, Alexandre},
	year         = 2012,
	journal      = {Frontiers in Neuroscience},
	volume       = 6,
	issn         = {1662-453X}
}
@article{borras-comes2011perceiving,
	title        = {{Perceiving Uncertainty: Facial Gestures, Intonation, and Lexical Choice}},
	author       = {{Borr{\`a}s-Comes}, Joan and Roseano, Paolo},
	year         = 2011,
	abstract     = {Languages rely on many verbal and nonverbal sources for the expression of uncertainty, and these linguistic markers are used by hearers to detect degrees of uncertainty in natural communication. An important question is whether the perception of uncertainty is better chracterized by lexical marking, prosody or facial gestures. To test this, a group of Catalan speakers were presented with two perception experiments containing a set of audiovisual materials in which lexical, prosodic and facial gestural cues presented congruent and incongruent combinations for the expression of uncertainty. The results from the two experiments demonstrate that, even though lexical choice is important for conveying pragmatic meanings like uncertainty, it can be easily overridden by prosodic and gestural patterns. Moreover, when gesture and prosody are in conflict, gesture is a more salient and powerful cue.},
	langid       = {english}
}
@article{brennan1995feeling,
	title        = {{The Feeling of Another$'$s Knowing: Prosody and Filled Pauses as Cues to Listeners about the Metacognitive States of Speakers}},
	shorttitle   = {The {{Feeling}} of {{Another}}{${'}$}s {{Knowing}}},
	author       = {Brennan, Susan E. and Williams, Maurice},
	year         = 1995,
	month        = jun,
	journal      = {Journal of Memory and Language},
	volume       = 34,
	number       = 3,
	pages        = {383--398},
	doi          = {10.1006/jmla.1995.1017},
	issn         = {0749596X},
	abstract     = {Abstract In question-answering, speakers display their metacognitive states using filled pauses and prosody (Smith \& Clark, 1993). We examined whether listeners are actually sensitive to this information. Experiment 1 replicated Smith and Clark{${'}$}s study; respondents were tested on general knowledge questions, surveyed about their FOK (feeling-of-knowing) for these questions, and tested for recognition of answers. In Experiment 2, listeners heard spontaneous verbal responses from Experiment 1 and were tested on their feeling-of- another{${'}$}s -knowing (FOAK) to see if metacognitive information was reliably conveyed by the surface form of responses. For answers, rising intonation and longer latencies led to fewer FOAK ratings by listeners. For nonanswers, longer latencies led to higher FOAK ratings. In Experiment 3, electronically edited responses with 1-s latencies led to higher FOAK ratings for answers and lower FOAK ratings for nonanswers than those with 5-s latencies. Filled pauses led to lower ratings for answers and higher ratings for nonanswers than did unfilled pauses. There was no support for a filler-as-morpheme hypothesis, that "um" and "uh" contrast in meaning. We conclude that listeners can interpret the metacognitive information that speakers display about their states of knowledge in question-answering.},
	langid       = {english}
}
@article{bui2002policy,
	title        = {{Policy Recognition in the Abstract Hidden Markov Model}},
	author       = {Bui, Hung H. and Venkatesh, Svetha and West, Geoff},
	year         = 2002,
	month        = dec,
	journal      = {Journal of Artificial Intelligence Research},
	volume       = 17,
	pages        = {451--499},
	doi          = {10.1613/jair.839},
	issn         = {1076-9757},
	eprint       = {1106.0672},
	eprinttype   = {arxiv},
	primaryclass = {cs},
	archiveprefix = {arXiv},
	keywords     = {Computer Science - Artificial Intelligence}
}
@inproceedings{chadalavada2015that,
	title        = {{That's on My Mind! Robot to Human Intention Communication through on-Board Projection on Shared Floor Space}},
	author       = {Chadalavada, Ravi Teja and Andreasson, Henrik and Krug, Robert and Lilienthal, Achim J.},
	year         = 2015,
	month        = sep,
	booktitle    = {European Conference on Mobile Robots},
	publisher    = {IEEE},
	address      = {New York, NY, USA},
	series       = {ECMR},
	pages        = {1--6},
	doi          = {10.1109/ECMR.2015.7403771},
	keywords     = {Calibration,Cameras,Service robots,Standards,Three-dimensional displays,Vehicles}
}
@inproceedings{cumbal2020Uncertainty,
	title        = {{Uncertainty in Robot Assisted Second Language Conversation Practice}},
	author       = {Cumbal, Ronald and Lopes, Jos{\'e} and Engwall, Olov},
	year         = 2020,
	month        = apr,
	booktitle    = {Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {HRI '20},
	pages        = {171--173},
	doi          = {10.1145/3371382.3378306},
	isbn         = {978-1-4503-7057-8},
	keywords     = {affective states,human-robot interaction,robot assisted language learning,uncertainty}
}
@inproceedings{dragan2013legibility,
	title        = {{Legibility and Predictability of Robot Motion}},
	author       = {Dragan, Anca D. and Lee, Kenton C.T. and Srinivasa, Siddhartha S.},
	year         = 2013,
	month        = mar,
	booktitle    = {2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
	publisher    = {IEEE},
	address      = {Tokyo, Japan},
	pages        = {301--308},
	doi          = {10.1109/HRI.2013.6483603},
	isbn         = {978-1-4673-3101-2 978-1-4673-3099-2 978-1-4673-3100-5},
	abstract     = {A key requirement for seamless human-robot collaboration is for the robot to make its intentions clear to its human collaborator. A collaborative robot's motion must be legible, or intent-expressive. Legibility is often described in the literature as and effect of predictable, unsurprising, or expected motion. Our central insight is that predictability and legibility are fundamentally different and often contradictory properties of motion. We develop a formalism to mathematically define and distinguish predictability and legibility of motion. We formalize the two based on inferences between trajectories and goals in opposing directions, drawing the analogy to action interpretation in psychology. We then propose mathematical models for these inferences based on optimizing cost, drawing the analogy to the principle of rational action. Our experiments validate our formalism's prediction that predictability and legibility can contradict, and provide support for our models. Our findings indicate that for robots to seamlessly collaborate with humans, they must change the way they plan their motion.},
	langid       = {english}
}
@inproceedings{duarte2020human,
	title        = {{From Human Action Understanding to Robot Action Execution: How the Physical Properties of Handled Objects Modulate Non-Verbal Cues}},
	shorttitle   = {From Human Action Understanding to Robot Action Execution},
	author       = {Duarte, Nuno Ferreira and Chatzilygeroudis, Konstantinos and {Santos-Victor}, Jose and Billard, Aude},
	year         = 2020,
	month        = oct,
	booktitle    = {2020 Joint IEEE 10th International Conference on Development and Learning and Epigenetic Robotics},
	location     = {Valparaiso, Chile},
	publisher    = {IEEE},
	address      = {New York, NY, USA},
	series       = {ICDL-EpiRob},
	pages        = {1--6},
	doi          = {10.1109/ICDL-EpiRob48136.2020.9278084},
	isbn         = {978-1-72817-306-1},
	langid       = {english}
}
@article{fox2021Relationship,
	title        = {{Relationship Development with Humanoid Social Robots: Applying Interpersonal Theories to Human\textendash{{Robot Interaction}}}},
	shorttitle   = {Relationship {{Development}} with {{Humanoid Social Robots}}},
	author       = {Fox, Jesse and Gambino, Andrew},
	year         = 2021,
	month        = may,
	journal      = {Cyberpsychology, Behavior, and Social Networking},
	volume       = 24,
	number       = 5,
	pages        = {294--299},
	doi          = {10.1089/cyber.2020.0181},
	issn         = {2152-2715, 2152-2723},
	abstract     = {Humanoid social robots (HSRs) are human-made technologies that can take physical or digital form, resemble people in form or behavior to some degree, and are designed to interact with people. A common assumption is that social robots can and should mimic humans, such that human\textendash robot interaction (HRI) closely resembles humanhuman (i.e., interpersonal) interaction. Research is often framed from the assumption that rules and theories that apply to interpersonal interaction should apply to HRI (e.g., the computers are social actors framework). Here, we challenge these assumptions and consider more deeply the relevance and applicability of our knowledge about personal relationships to relationships with social robots. First, we describe the typical characteristics of HSRs available to consumers currently, elaborating characteristics relevant to understanding social interactions with robots such as form anthropomorphism and behavioral anthropomorphism. We also consider common social affordances of modern HSRs (persistence, personalization, responsiveness, contingency, and conversational control) and how these align with human capacities and expectations. Next, we present predominant interpersonal theories whose primary claims are foundational to our understanding of human relationship development (social exchange theories, including resource theory, interdependence theory, equity theory, and social penetration theory). We consider whether interpersonal theories are viable frameworks for studying HRI and human\textendash robot relationships given their theoretical assumptions and claims. We conclude by providing suggestions for researchers and designers, including alternatives to equating human\textendash robot relationships to human-human relationships.},
	langid       = {english}
}
@article{giger2019humanization,
	title        = {{Humanization of Robots: Is It Really Such a Good Idea?}},
	shorttitle   = {Humanization of Robots},
	author       = {Giger, Jean-Christophe and Pi{\c c}arra, Nuno and Alves-Oliveira, Patr{\'i}cia and Oliveira, Raquel and Arriaga, Patr{\'i}cia},
	year         = 2019,
	month        = apr,
	journal      = {Human Behavior and Emerging Technologies},
	volume       = 1,
	number       = 2,
	pages        = {111--123},
	doi          = {10.1002/hbe2.147},
	issn         = {2578-1863, 2578-1863},
	abstract     = {The aim of this review was to examine the pros and cons of humanizing social robots following a psychological perspective. As such, we had six goals. First, we defined what social robots are. Second, we clarified the meaning of humanizing social robots. Third, we presented the theoretical backgrounds for promoting humanization. Fourth, we conducted a review of empirical results of the positive effects and the negative effects of humanization on human\textendash robot interaction (HRI). Fifth, we presented some of the political and ethical problems raised by the humanization of social robots. Lastly, we discussed the overall effects of the humanization of robots in HRI and suggested new avenues of research and development.},
	langid       = {english}
}
@book{givens2006nonverbal,
	title        = {{The Nonverbal Dictionary of Gestures, Signs \& {{Body Language Cues}}}},
	author       = {Givens, David B.},
	year         = 2006,
	publisher    = {David B. Givens},
	abstract     = {Dictionary of gestures, signs, and body language cues from the Center for Nonverbal Studies which aims to advance the study of human communication in all forms excepting oral means. Draws on the work of anthropologists, archaeologists, biologists, linguists, psychiatrists, psychologists, and semioticians to provide a compendium of brief essays on the way people say things without speaking.}
}
@inproceedings{grigore2013joint,
	title        = {{Joint Action Understanding Improves Robot-to-Human Object Handover}},
	author       = {Grigore, Elena Corina and Eder, Kerstin and Pipe, Anthony G. and Melhuish, Chris and Leonards, Ute},
	year         = 2013,
	month        = nov,
	booktitle    = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
	publisher    = {IEEE},
	address      = {Tokyo},
	pages        = {4622--4629},
	doi          = {10.1109/IROS.2013.6697021},
	isbn         = {978-1-4673-6358-7 978-1-4673-6357-0},
	langid       = {english}
}
@inproceedings{grubov2022frequencyspace,
	title        = {{Frequency-Space Features of EEG Activity during Decision-Making Task with Uncertainty}},
	author       = {Grubov, Vadim V. and Kuc, Alexander A. and Maksimenko, Vladimir A.},
	year         = 2022,
	month        = apr,
	booktitle    = {Computational Biophysics and Nanobiophotonics},
	publisher    = {SPIE},
	address      = {Saratov, Russian Federation},
	pages        = 39,
	doi          = {10.1117/12.2626570},
	isbn         = {978-1-5106-5369-6 978-1-5106-5370-2},
	editor       = {Khlebtsov, Boris N. and Postnov, Dmitry E.},
	langid       = {english}
}
@article{hellstrom2018Understandable,
	title        = {{Understandable Robots - What, Why, and How}},
	author       = {Hellstr{\"o}m, Thomas and Bensch, Suna},
	year         = 2018,
	month        = jul,
	journal      = {Paladyn, Journal of Behavioral Robotics},
	publisher    = {De Gruyter Open Access},
	volume       = 9,
	number       = 1,
	pages        = {110--123},
	doi          = {10.1515/pjbr-2018-0009},
	issn         = {2081-4836},
	langid       = {english},
	keywords     = {communication,explainable,human-robot interaction,predictable}
}
@inproceedings{hough2017It,
	title        = {{It's Not What You Do, It's How You Do It: Grounding Uncertainty for a Simple Robot}},
	shorttitle   = {It's {{Not What You Do}}, {{It}}'s {{How You Do It}}},
	author       = {Hough, Julian and Schlangen, David},
	year         = 2017,
	month        = mar,
	booktitle    = {Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {HRI '17},
	pages        = {274--282},
	doi          = {10.1145/2909824.3020214},
	isbn         = {978-1-4503-4336-7},
	keywords     = {communicative grounding,incrementality,uncertainty}
}
@article{hramov2018artificial,
	title        = {{Artificial Neural Network Detects Human Uncertainty}},
	author       = {Hramov, Alexander E. and Frolov, Nikita S. and Maksimenko, Vladimir A. and Makarov, Vladimir V. and Koronovskii, Alexey A. and {Garcia-Prieto}, Juan and {Ant{\'o}n-Toro}, Luis Fernando and Maest{\'u}, Fernando and Pisarchik, Alexander N.},
	year         = 2018,
	month        = mar,
	journal      = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	volume       = 28,
	number       = 3,
	pages        = {033607},
	doi          = {10.1063/1.5002892},
	issn         = {1054-1500, 1089-7682},
	langid       = {english}
}
@inproceedings{huang2017enabling,
	title        = {{Enabling Robots to Communicate Their Objectives}},
	author       = {Huang, Sandy H. and Held, David and Abbeel, Pieter and Dragan, Anca D.},
	year         = 2017,
	month        = jul,
	booktitle    = {Robotics: Science and Systems XIII},
	doi          = {10.15607/RSS.2017.XIII.059},
	note         = {Comment: RSS 2017},
	eprint       = {1702.03465},
	eprinttype   = {arxiv},
	primaryclass = {cs},
	archiveprefix = {arXiv},
	keywords     = {Computer Science - Machine Learning,Computer Science - Robotics}
}
@inproceedings{lee2005human,
	title        = {{Human Mental Models of Humanoid Robots}},
	author       = {Lee, Sau-lai and Lau, Ivy Yee-man and Kiesler, S. and Chiu, Chi-Yue},
	year         = 2005,
	month        = apr,
	booktitle    = {Proceedings of the 2005 IEEE International Conference on Robotics and Automation},
	publisher    = {IEEE},
	address      = {New York, NY, USA},
	pages        = {2767--2772},
	doi          = {10.1109/ROBOT.2005.1570532},
	issn         = {1050-4729},
	keywords     = {Animals,Cognitive science,Communication effectiveness,Computer applications,dialogue,Human computer interaction,Human robot interaction,human-robot interaction,Humanoid robots,humanoids,Interactive systems,perception,Psychology,Robotics and automation,social robots}
}
@article{mavridis2015review,
	title        = {{A Review of Verbal and Non-Verbal Human\textendash Robot Interactive Communication}},
	author       = {Mavridis, Nikolaos},
	year         = 2015,
	month        = jan,
	journal      = {Robotics and Autonomous Systems},
	volume       = 63,
	pages        = {22--35},
	doi          = {10.1016/j.robot.2014.09.031},
	issn         = {0921-8890},
	langid       = {english},
	keywords     = {Human–robot communication,Human–robot interaction,Non-verbal,Survey,Verbal}
}
@article{milburn1977decision,
	title        = {{Decision-making perspectives from psychology: Dealing with risk and uncertainty.}},
	author       = {Milburn, Thomas W. and Billings, Robert S.},
	year         = 1976,
	journal      = {American Behavioral Scientist},
	publisher    = {Sage Publications},
	address      = {US},
	volume       = 20,
	pages        = {111--126},
	doi          = {10.1177/000276427602000107},
	url          = {https://doi.org/10.1177/000276427602000107},
	keywords     = {*Decision Making; *Government Policy Making; *Political Processes; Risk Taking},
	abstract     = {Argues that psychological decision-making models are inadequate because of oversimplistic constructs of risk and uncertainty. An analysis of the classical decision-making model which assumes individuals make rational optimal choices in clearly defined contexts leads to the assertion that an expanded multistage process model is more appropriate. In this model, problems would first be identified and placed on an individual or social agenda, alternatives sought and consequences identified, and a choice made and implemented, followed by an evaluation to determine if an actual solution was reached. While uncertainty in the classical paradigm depends on the probability that a particular alternative leads to a particular consequence, uncertainty in the expanded model depends on each stage of decision making and has a complex effect on the final decision. Similarly, while risk in the classical paradigm depends on the level of probability of a specific outcome, risk in the expanded model depends on the individual's perceptions of utilities and probabilities and is a broad and ambiguous factor. Risk is discussed with respect to individual and situational determinants, the subjective expected utility model, and the influence of time, action, and other variables. Emphasis is placed on the inevitable interrelationships among various decisions that an individual makes. (27 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}
@article{moon2010using,
	title        = {{Using Hesitation Gestures for Safe and Ethical Human-Robot Interaction}},
	author       = {Moon, AJung and Panton, Boyd},
	year         = 2010,
	pages        = 3,
	abstract     = {Safe interaction with non-expert users is increasingly important in the development of robotic assistants. Ethical ``codes'' can serve as a guide as to how this interaction should take place with lay users in non-structured environments. Such codes suggest that robots should behave in a way that is intuitive to users. Previous research has demonstrated that the implicit channel is useful for intuitive human-robot interaction. Our work described in this position paper investigates how a robot should behave when it is uncertain of its human partner's intentions. In this context, uncertainties arising in human-robot shared-tasks should be made transparent to a human user. We posit that hesitant hand motion used by people and animals is a natural modality for a robot to communicate uncertainty. To test our hypothesis we propose to characterize and implement human hesitation gestures onto a robot, and investigate its ability to communicate uncertainty.},
	langid       = {english}
}
@article{mori2012Uncanny,
	title        = {{The Uncanny Valley [From the Field]}},
	author       = {Mori, Masahiro and MacDorman, Karl F. and Kageki, Norri},
	year         = 2012,
	month        = jun,
	journal      = {IEEE Robotics \& Automation Magazine},
	volume       = 19,
	number       = 2,
	pages        = {98--100},
	doi          = {10.1109/MRA.2012.2192811},
	issn         = {1558-223X},
	abstract     = {More than 40 years ago, Masahiro Mori, a robotics professor at the Tokyo Institute of Technology, wrote an essay [1] on how he envisioned people's reactions to robots that looked and acted almost like a human. In particular, he hypothesized that a person's response to a humanlike robot would abruptly shift from empathy to revulsion as it approached, but failed to attain, a lifelike appearance. This descent into eeriness is known as the uncanny valley. The essay appeared in an obscure Japanese journal called Energy in 1970, and in subsequent years, it received almost no attention. However, more recently, the concept of the uncanny valley has rapidly attracted interest in robotics and other scientific circles as well as in popular culture. Some researchers have explored its implications for human-robot interaction and computer-graphics animation, whereas others have investigated its biological and social roots. Now interest in the uncanny valley should only intensify, as technology evolves and researchers build robots that look human. Although copies of Mori's essay have circulated among researchers, a complete version hasn't been widely available. The following is the first publication of an English translation that has been authorized and reviewed by Mori. (See ``Turning Point'' in this issue for an interview with Mori.).}
}
@article{navajas2017idiosyncratic,
	title        = {{The Idiosyncratic Nature of Confidence}},
	author       = {Navajas, Joaquin and Hindocha, Chandni and Foda, Hebah and Keramati, Mehdi and Latham, Peter E. and Bahrami, Bahador},
	year         = 2017,
	month        = nov,
	journal      = {Nature Human Behaviour},
	publisher    = {Nature Publishing Group},
	volume       = 1,
	number       = 11,
	pages        = {810--818},
	doi          = {10.1038/s41562-017-0215-1},
	issn         = {2397-3374},
	copyright    = {2017 The Author(s)},
	abstract     = {Confidence is the `feeling of knowing' that accompanies decision-making. Bayesian theory proposes that confidence is a function solely of the perceived probability of being correct. Empirical research has suggested, however, that different individuals may perform different computations to estimate confidence from uncertain evidence. To test this hypothesis, we collected confidence reports in a task in which subjects made categorical decisions about the mean of a sequence. We found that for most individuals, confidence did indeed reflect the perceived probability of being correct. However, in approximately half of them, confidence also reflected a different probabilistic quantity: the perceived uncertainty in the estimated variable. We found that the contribution of both quantities was stable over weeks. We also observed that the influence of the perceived probability of being correct was stable across two tasks, one perceptual and one cognitive. Overall, our findings provide a computational interpretation of individual differences in human confidence.},
	langid       = {english},
	keywords     = {Computational neuroscience,Decision}
}
@inproceedings{oechsner2022challenges,
	title        = {{Challenges and Opportunities of Cooperative Robots as Cooking Appliances}},
	author       = {Oechsner, Carl and Mayer, Sven and Butz, Andreas},
	year         = 2022,
	booktitle    = {Engaging with Automation - Understanding and Designing for Operation, Appropriation, and Behaviour Change},
	publisher    = {ceur-ws.org},
	series       = {AutomationXP 2022},
	pages        = 7,
	url          = {https://ceur-ws.org/Vol-3154/paper14.pdf}
}
@inproceedings{scherf2022learning,
	title        = {{Learning from Unreliable Human Action Advice in Interactive Reinforcement Learning}},
	author       = {Scherf, Lisa and Turan, Cigdem and Koert, Dorothea},
	year         = 2022,
	month        = nov,
	booktitle    = {2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids)},
	publisher    = {IEEE},
	address      = {Ginowan, Japan},
	pages        = {895--902},
	doi          = {10.1109/Humanoids53995.2022.10000078},
	isbn         = 9798350309799,
	langid       = {english}
}
@book{szekely2017Synthesising,
	title        = {{Synthesising Uncertainty: The Interplay of Vocal Effort and Hesitation Disfluencies}},
	shorttitle   = {Synthesising {{Uncertainty}}},
	author       = {Szekely, Eva and Mendelson, Joseph and Gustafson, Joakim},
	year         = 2017,
	month        = aug,
	pages        = 808,
	doi          = {10.21437/Interspeech.2017-1507}
}
@article{thepsoonthorn2021Exploration,
	title        = {{The Exploration of the Uncanny Valley from the Viewpoint of the Robot's Nonverbal Behaviour}},
	author       = {Thepsoonthorn, Chidchanok and Ogawa, Ken-ichiro and Miyake, Yoshihiro},
	year         = 2021,
	month        = sep,
	journal      = {International Journal of Social Robotics},
	volume       = 13,
	number       = 6,
	pages        = {1443--1455},
	doi          = {10.1007/s12369-020-00726-w},
	issn         = {1875-4805},
	abstract     = {Many studies have been conducted to find approaches to overcome the Uncanny Valley. However, the focus on the influence of the robot's appearance leaves a big missing part: the influence of the robot's nonverbal behaviour. This impedes the complete exploration of the Uncanny Valley. In this study, we explored the Uncanny Valley from the viewpoint of the robot's nonverbal behaviour in regard to the Uncanny Valley hypothesis. We observed a relationship between the participants' ratings on human-likeness of the robot's nonverbal behavior and affinity toward the robot's nonverbal behavior, and define the point where the affinity toward the robot's nonverbal behavior significantly drops down as the Uncanny Valley. In this study, an experiment of human\textendash robot interaction was conducted. The participants were asked to interact with a robot with different nonverbal behaviours, ranging from 0 (no nonverbal behavior, speaking only) to 3 (gaze, head nodding, and gestures) combinations and to rate the perceived human-likeness and affinity toward the robot's nonverbal behavior by using a questionnaire. Additionally, the participants' fixation duration was measured during the experiment. The result showed a biphasic relationship between human-likeness and affinity rating results. A curve resembling the Uncanny Valley is found. The result was also supported by participants' fixation duration. It showed that the participants had the longest fixation at the robot when the robot expressed the nonverbal behaviours that fall into the Uncanny Valley. This exploratory study provides evidence suggesting the existence of the Uncanny Valley from the viewpoint of the robot's nonverbal behaviour.},
	langid       = {english}
}
@inproceedings{trick2019multimodal,
	title        = {{Multimodal Uncertainty Reduction for Intention Recognition in Human-Robot Interaction}},
	author       = {Trick, Susanne and Koert, Dorothea and Peters, Jan and Rothkopf, Constantin A.},
	year         = 2019,
	month        = jul,
	booktitle    = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	publisher    = {IEEE},
	pages        = {7009--7016},
	doi          = {10.1109/IROS40897.2019.8968171},
	eprint       = {1907.02426},
	eprinttype   = {arxiv},
	primaryclass = {cs, stat},
	archiveprefix = {arXiv}
}
@inproceedings{wang2023explainable,
	title        = {{Explainable Human-Robot Training and Cooperation with Augmented Reality}},
	author       = {Wang, Chao and Belardinelli, Anna and Hasler, Stephan and Stouraitis, Theodoros and Tanneberg, Daniel and Gienger, Michael},
	booktitle    = {Proceedings of the 42st ACM Conference on Human Factors in Computing Systems Extended Abstact},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI EA'23},
	doi          = {10.1145/3544549.3583889}
}
@article{washburn2020robot,
	title        = {{Robot Errors in Proximate HRI: How Functionality Framing Affects Perceived Reliability and Trust}},
	shorttitle   = {Robot Errors in Proximate HRI},
	author       = {Washburn, Auriel and Adeleye, Akanimoh and An, Thomas and Riek, Laurel D.},
	year         = 2020,
	month        = may,
	journal      = {ACM Transactions on Human-Robot Interaction},
	volume       = 9,
	number       = 3,
	pages        = {19:1--19:21},
	doi          = {10.1145/3380783},
	keywords     = {Human robot teaming,joint action,proximate interaction,robot reliability,trust}
}
@inproceedings{xu2020gtad,
	title        = {{G-TAD: Sub-Graph Localization for Temporal Action Detection}},
	author       = {Xu, Mengmeng and Zhao, Chen and Rojas, David S. and Thabet, Ali and Ghanem, Bernard},
	year         = 2020,
	month        = jun,
	booktitle    = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	publisher    = {IEEE},
	address      = {Seattle, WA, USA},
	pages        = {10153--10162},
	doi          = {10.1109/CVPR42600.2020.01017},
	isbn         = {978-1-72817-168-5},
	langid       = {english}
}
@inproceedings{yu2015fast,
	title        = {{Fast Action Proposals for Human Action Detection and Search}},
	author       = {Yu, Gang and Yuan, Junsong},
	year         = 2015,
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages        = {1302--1311}
}
@article{zhang2019comprehensive,
	title        = {{A Comprehensive Survey of Vision-Based Human Action Recognition Methods}},
	author       = {Zhang, Hong-Bo and Zhang, Yi-Xiang and Zhong, Bineng and Lei, Qing and Yang, Lijie and Du, Ji-Xiang and Chen, Duan-Sheng},
	year         = 2019,
	month        = jan,
	journal      = {Sensors},
	publisher    = {Multidisciplinary Digital Publishing Institute},
	volume       = 19,
	number       = 5,
	pages        = 1005,
	doi          = {10.3390/s19051005},
	issn         = {1424-8220},
	copyright    = {http://creativecommons.org/licenses/by/3.0/},
	langid       = {english},
	keywords     = {action detection,action feature,human action recognition,human–object interaction recognition,systematic survey}
}
@inproceedings{cooper2021social,
	title        = {{Social robotic application to support active and healthy ageing}},
	author       = {Cooper, S. and Di Fava, A. and Villacañas, Ó. and Silva, T. and Fernandez-Carbajales, V. and Unzueta, L. and Serras, M. and Marchionni, L. and Ferro, F.},
	year         = 2021,
	booktitle    = {2021 30th IEEE International Conference on Robot \& Human Interactive Communication (RO-MAN)},
	volume       = {},
	number       = {},
	pages        = {1074--1080},
	doi          = {10.1109/RO-MAN50785.2021.9515432}
}
@article{PAPADOPOULOS2020103924,
	title        = {{A systematic review of the literature regarding socially assistive robots in pre-tertiary education}},
	author       = {Irena Papadopoulos and Runa Lazzarino and Syed Miah and Tim Weaver and Bernadette Thomas and Christina Koulouglioti},
	year         = 2020,
	journal      = {Computers \& Education},
	volume       = 155,
	pages        = 103924,
	doi          = {https://doi.org/10.1016/j.compedu.2020.103924},
	issn         = {0360-1315},
	url          = {https://www.sciencedirect.com/science/article/pii/S0360131520301238},
	keywords     = {Early years education, Elementary education, Secondary education, Improving classroom teaching, Socially assistive robots}
}
@inproceedings{feil2009socially,
	title        = {{Toward Socially Assistive Robotics for Augmenting Interventions for Children with Autism Spectrum Disorders}},
	author       = {{Feil-Seifer}, David and Matari{\'c}, Maja J.},
	year         = 2009,
	booktitle    = {Experimental {{Robotics}}},
	publisher    = {{Springer}},
	address      = {{Berlin, Heidelberg}},
	series       = {Springer {{Tracts}} in {{Advanced Robotics}}},
	pages        = {201--210},
	doi          = {10.1007/978-3-642-00196-3_24},
	isbn         = {978-3-642-00196-3},
	editor       = {Khatib, Oussama and Kumar, Vijay and Pappas, George J.},
	langid       = {english},
	keywords     = {Autism Spectrum Disorder,Autism Spectrum Disorder Child,Joint Attention,Social Orienting}
}
