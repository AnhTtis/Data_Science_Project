@inproceedings{reason1,
title={Language Models Can (kind of) Reason: A Systematic Formal Analysis of Chain-of-Thought},
author={Abulhair Saparov and He He},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=qFVVBzXxR2V}
}


@article{reason2,
author = {Philip N. Johnson-Laird },
title = {Mental models and human reasoning},
journal = {Proceedings of the National Academy of Sciences},
volume = {107},
number = {43},
pages = {18243-18250},
year = {2010},
doi = {10.1073/pnas.1012933107},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1012933107},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1012933107},
abstract = {To be rational is to be able to reason. Thirty years ago psychologists believed that human reasoning depended on formal rules of inference akin to those of a logical calculus. This hypothesis ran into difficulties, which led to an alternative view: reasoning depends on envisaging the possibilities consistent with the starting pointâ€”a perception of the world, a set of assertions, a memory, or some mixture of them. We construct mental models of each distinct possibility and derive a conclusion from them. The theory predicts systematic errors in our reasoning, and the evidence corroborates this prediction. Yet, our ability to use counterexamples to refute invalid inferences provides a foundation for rationality. On this account, reasoning is a simulation of the world fleshed out with our knowledge, not a formal rearrangement of the logical skeletons of sentences.}}



@misc{bert,
  doi = {10.48550/ARXIV.1810.04805},
  url = {https://arxiv.org/abs/1810.04805},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}}



@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{gpt3,
  doi = {10.48550/ARXIV.2005.14165},
  url = {https://arxiv.org/abs/2005.14165},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language Models are Few-Shot Learners},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}}





@inproceedings{spartqa,
    title = "{SPARTQA}: A Textual Question Answering Benchmark for Spatial Reasoning",
    author = "Mirzaee, Roshanak  and
      Rajaby Faghihi, Hossein  and
      Ning, Qiang  and
      Kordjamshidi, Parisa",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.364",
    doi = "10.18653/v1/2021.naacl-main.364",
    pages = "4582--4598",
    abstract = "This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs{'} capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.",
}


@inproceedings{bats,
    title = "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn{'}t.",
    author = "Gladkova, Anna  and
      Drozd, Aleksandr  and
      Matsuoka, Satoshi",
    booktitle = "Proceedings of the {NAACL} Student Research Workshop",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-2002",
    doi = "10.18653/v1/N16-2002",
    pages = "8--15"}

@article{analogical1,
  title={Analogical reasoning and cognitive development},
  author={Goswami, Usha},
  journal={Advances in child development and behavior},
  volume={26},
  pages={91--138},
  year={1996},
  publisher={ACADEMIC PRESS, INC.}
}