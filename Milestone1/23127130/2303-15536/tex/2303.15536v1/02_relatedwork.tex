\section{Terminology}
 Already in 1936, Merton~\cite{merton1936unanticipated} coined the term \emph{unanticipated  consequences} to describe unforeseen, desirable or undesirable outcomes of one's action. Today, most researchers refer to unforeseen outcomes (the results of policies, technologies, or other ``purposive social actions''~\cite{merton1936unanticipated}) as \emph{unintended consequences}, though some have suggested that this term conflation has caused a loss of nuance~\cite{huntington1971change, parvin2020unintended}. As we show in ~\autoref{fig:term}, Merton's original term of \emph{unanticipated consequences} suggests that such consequences are always unintended. In contrast, \emph{unintended consequences} can be either unanticipated or anticipated. Parvin and Pollock have therefore argued that this lack of precision in terminology may lead people ``to abdicate responsibility for the perfectly foreseeable consequences of particular decisions''~\cite[p.323]{parvin2020unintended}. Hence, the use of the term \emph{unintended consequences} may reduce accountability: ``Phenomena described as unintended consequences are deemed too difficult, too out of scope, too out of reach, or too messy to have been dealt with at any point in time before they created problems for someone else. The descriptive approach works as a defensive and dismissive strategy''~\cite[p.322]{parvin2020unintended}. 
 

 \label{terminology}
 \begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/terminology.pdf}
        \caption{Terminology of anticipated, intended, unintended, and unanticipated consequences. \rr{Unintended consequences can be either unanticipated or anticipated. }}
    \label{fig:term}
    \Description{A Venn diagram with two intersecting circles labeled "unanticipated" and "intended." The intersection is labeled "anticipated". The area including the circle labeled "unanticipated" but excluding the area of the "anticipated" intersection is labeled "unintended."}
\end{figure}
 
 In this paper, we use the term \emph{unintended consequences (UCs)} to purposefully broaden the discussion to include both anticipated and unanticipated, positive or negative unintended side effects of technology on society. Our definition includes consequences that the instigators of an action (i.e., researchers and/or technology innovators) may not have addressed but could have foreseen. While these consequences can be positive or negative in nature (and oftentimes have different effects on a population), our work is inherently oriented towards considering negative UCs more than positive ones. In the remainder of this paper, we use ``technology'' for digital technology, such as hardware devices or software systems. We broadly refer to ``society''  at a regional, national, or international level.
 

\section{Related Work}
 
 Our work draws upon prior work studying values and ethics in digital technology~\cite{Shilton2018ValuesAE} and is informed by discussions about the effects of digital technology on society in fields such as philosophy~\cite{moor1985computer, johnson1985computer}, STS~\cite{winner2017artifacts, winner2010whale, Klein2002TheSC}, social informatics~\cite{kling1996computerization}, feminism~\cite{Bardzell2010FeministHT,haimson2016constructing} and postcolonial theories~\cite{irani2010postcolonial}. We start by showing how researchers in these fields have long discussed various societal effects of technology before outlining the methods and approaches researchers and practitioners have developed for mitigating unintended consequences. 
 
 \paragraph{Critiques of Technology}
 
 Prior work in STS, and later in HCI, has provided critical analyses of the risks and benefits of technology in society since at least the 1960s~\cite{sveiby2009unintended, Shilton2018ValuesAE}. According to STS scholar Winner, technology ``embodies specific forms of power and authority''~\cite{winner2017artifacts} and technologists should ``pay attention not only to the making of physical instruments and processes [...], but also the production of psychological, social, and political conditions as a part of any significant technical change''~\cite{winner2010whale}. 

 \rr{Work on the risks of technology has been published on a broad range of topics, including the Internet ~\cite{kraut1998internet}, health care information technologies~\cite{harrison2007unintended,ash2007categorizing}, mobile phones~\cite{reyns2013unintended,Moser:2016}, smart technologies~\cite{machidon2018societal}, machine learning~\cite{cabitza2017unintended}, and social media ~\cite{del2016spreading, starbird2019disinformation,starbird2017examining}.} 
 

 The examples above provide a critical lens of the role of technology in society and caution about the unknown and differential effects on societies. Historically, however, most innovation research has focused on desirable and intended consequences~\cite{Rogers1976NewPA, sveiby2009unintended}. In 2009, Sveiby and colleagues suggested that this focus could potentially be due to a ``pro-innovation bias among researchers and vested interests of funding agencies''~\cite{sveiby2009unintended}. Our literature review did not reveal whether this bias has changed in the years since. 

 However, we found many recent calls for more accountability for research innovations~\cite{metcalf2019owning,friedman2019value,Hecht2021ItsTT}. Several prominent computing conferences --- such as the Conference on Neural Information Processing System (NeurIPS ~\cite{NeurIps2021}), Annual Meetings of the Association for Computational Linguistics (ACL) ~\cite{ACL2022}, and the ACM Conference on Intelligent User Interfaces (IUI)~\cite{IUI2022} --- have begun to experiment with ways to encourage or even require researchers to state both the positive and negative potential implications of their work in all paper submissions. \rr{Recent work has made several suggestions for such broader impact statements based on an analysis of these statements in NeurIPS conference proceedings~\cite{nanayakkara2021unpacking, Liu2022ExaminingRA, Ashurst2022AIES}}. After requiring all submissions to contain a section describing the impact of the work, NeurIPS has since transitioned towards a checklist system that offers additional guidance and adaptability ~\cite{NeurIps2021}. 
%
% In 2018, the ACM Code of Ethics and Professional Conduct was revised for the first time since 1992 to address the significant advances in computing technology and the growing pervasiveness of computing in all aspects of society~\cite{Gotterbarn2018ACMCO}. There are also calls for researchers within different computing communities to accurately report the design considerations of their datasets and models~\cite{gebru2021datasheets,mitchell2019model,bender2018data,rogers2021changing} as well as the tasks that they are working on~\cite{Lindley2017ImplicationsFA,mohammad2021ethics}. 

 The HCI community has been raising awareness of UCs of computing research through dedicated publication tracks (e.g., Critical Computing at CHI) and workshops~\cite{conseuqences, unethically}. In particular, a CHI 2021 workshop explored how HCI researchers might think about and report potential negative consequences stemming from their research~\cite{conseuqences}. HCI researchers have also advocated for changes to the peer review process to reduce negative impacts of research innovations, suggesting that reviewers should routinely require that papers and proposals discuss potential adverse effects~\cite{Hecht2021ItsTT}. 
%
 Given these calls for examining the societal impacts of technology, our work explores whether researchers adopt any methods for anticipating the UCs of their own work. 

\paragraph{Anticipating and Mitigating Unintended Consequences of Technology}
 UCs are often dismissed as unavoidable because anticipating what may happen in the future can be hard~\cite{parvin2020unintended} \rr{and uncertain~\cite{Nanayakkara2020AnticipatoryEA}.} However, HCI researchers have developed ethics-focused design methods to ensure the inclusion of various stakeholders in the design process (for an overview  see~\cite{chivukula2021surveying}). One prominent example is the value-sensitive design (VSD) approach by Friedman, Kahn, and Borning~\cite{friedman2008value}, which can aid in understanding technology, its human value, and its context of use. The process aims to help product teams and researchers identify alternative approaches that better uphold their chosen values while accommodating the same constraints. 
 A number of recent proposals have sought to bridge the gap between theory and implementation by creating toolkits meant for brainstorming about a product's potential societal impacts. For example, the Envisioning Cards~\cite{EnvisioningCards} present the VSD concepts in a clear and modular fashion~\cite{Nathan2008}. In addition, stakeholder tokens also support a VSD stakeholder analysis~\cite{Yoo:2018}. Prior work in HCI has also recommended the use of Tarot Cards of Tech~\cite{Menking:2019} and the Value Cards~\cite{shen2021value} for anticipating potential UCs of specific design choices. 
 
 \rrr{Another approach for considering possible societal impacts is through design fiction~\cite{bleecker2022design, Baumer2018WhatWY}. As a form of speculative design~\cite{Lindley2016PushingTL, Dunne2013SpeculativeED},  design fiction creates a fictional future world to think through sociotechnical issues that have relevance and implications for the present~\cite{wong2017eliciting, lindley2017implications}. This practice has been used to reflect on potential downsides of public data~\cite{fiesler2019ethical}, technology design~\cite{Harrington2022AllTY}, and research prototypes~\cite{soden2019chi4evil}. More recent work developed the design fiction memos method to explore how UX practitioners engage with ethical issues and social impact in their work~\cite{wong2021using}.} 
 
 \rrr{The existing approaches to consider societal implications, however, were often assumed to be effective in practice~\cite{gray2019ethical} and might be difficult to evaluate ~\cite{Baumer2018WhatWY}. While the toolkits often target designers and practitioners as users~\cite{gray2019ethical}, applying them for research projects may pose additional complexities. We extend this line of work by inquiring into whether computer science researchers are aware of and proactively incorporate these tools in their research process. Our work also explores future design implications to support researchers to consider UCs in their research process.
 }
 %These existing approaches are not without shortcomings.} For one, they are primarily designed for practitioners; applying them to research projects \rr{in practice} may be challenging~\cite{gray2019ethical}. Nevertheless, they could serve as important tools for considering UCs. In the present work, we explore whether computer science researchers are aware of and incorporate any of these tools in their research process.}  
 
\paragraph{Reacting to Unintended Consequences of Technology}
 Not much work has investigated how practitioners and researchers react to UCs in practice. Kling's book on ``Computerization and Controversy'' shows how the power dynamics between programmers and their employers can prevent discussions of potential ethical issues in the products they work on~\cite{kling1996computerization}. As a result, computer science professionals may feel discouraged when reacting to potential or known UCs. 
 \rr{Recently, an interview study showed that the Deepfake open source contributors felt unable to control downstream uses of their software, given the core principle of open source~\cite{widder}.}
 %More recently, some researchers were found to be ``nonchalant'' toward the broader impact statement and perceive it as ``burden''~\cite{Abuhamad2020LikeAR}. 
%
 %\rr{It could well be that similar power dynamics prevent anticipating and reacting to UCs in academic settings.} 
%
 Researchers have occasionally written public posts in response to public backlash or negative press after deploying a research project~\cite{jiang_2021, openai_2022}, but it is unclear whether they also do so when an incident is less public or when it has only been anticipated (but has not materialized). We fill this gap in prior work by studying whether and how academic computer scientists react if they discover that their work may have UCs. 
