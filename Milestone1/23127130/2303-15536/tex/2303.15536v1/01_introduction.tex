\section{Introduction}

 From smart glasses that invoke fears of surveillance ~\cite{eveleth_2018} to chatbots that use racist language ~\cite{blog_2016}, our society must increasingly protect itself against the harmful effects of our own technological advancements --- commonly referred to as unanticipated or unintended consequences~\cite{merton1936unanticipated,parvin2020unintended}. While industry is seen as the main offender due to its large user base and broad product impact, computer science research has experienced its own fair share of cases in which innovations have gone awry. For example, AI innovations enabling deepfakes have fostered the spread of disinformation~\cite{kertysova2018artificial}; deep learning models that predict a person's sexual orientation have caused fierce backlash from the LGBTQ community~\cite{wang2018deep}; language models have been shown to exacerbate inequalities~\cite{vieira2021understanding}, propagate social bias~\cite{sap2019risk, koenecke2020racial}, and intentionally produce discriminatory content~\cite{kurenkov_2022}; and innovations in interaction design to improve usability have simultaneously widened disparities between the experiences of the demographic groups included or omitted from the research and development process~\cite{toyama2015geek}. 

 Research in Science and Technology Studies (STS) has documented the hopes and challenges of technological utopianism for decades, but commonly focuses on industry and computer science practitioners, such as software engineers~\cite{kling1991computerization,parker1995all}. Computer scientists in academia may face different and in some ways more complex challenges than practitioners when considering how to anticipate unintended consequences. For example, while their research can result in widely-adopted, non-commercial or commercial products, academic researchers commonly generate ideas and artifacts that are primarily used or extended by other researchers within academia~\cite{Koya2020MeasuringIO}. 
 
 The recent flurry of negative media about the adverse effects of technologies is spurring researchers in various computer science disciplines to more commonly examine ethical implications of their work. Human-Computer Interaction (HCI) researchers have explored the ethics of research and technology in workshops (e.g., ~\cite{bates2019towards,waycott2016ethical}) and various publications (e.g., ~\cite{mackay1995ethics,branham2014co,fiesler2015ethics,mcmillan2013categorised, brown2016five,grimpe2014towards}). Not too long ago, ethics was proposed as one of seven grand challenges for HCI~\cite{stephanidis2019seven}. The SIGCHI research ethics committee has been facilitating open conversations about ethical challenges in our communities through research ethics town halls and panels at CHI ~\cite{frauenberger2017research, munteanu2019sigchi}, CSCW~\cite{bruckman2017cscw, fiesler2021sigchi}, and GROUP~\cite{bjorn2018research}. Similar discussions for more ethical research have been pushed in disciplines such as natural language processing ~\cite{ws-2017-acl, acl}, computer vision~\cite{cvpr2022}, virtual reality ~\cite{Behr2005, Jonathan2021, Ramirez2018}, robotics~\cite{vanderelst2018, ingram2010}, data management~\cite{stoyanovich2022responsible, Abiteboul2019}, and data mining~\cite{hajian2016}. 
 
 Most of the discussions on unintended consequences in computer science research have centered on issues that researchers face during the research process, such as when conducting user studies~\cite{frauenberger2017research}, using online data~\cite{vitak2017ethics,vitak2016beyond}, and crowdsourcing data collection~\cite{Barbosa2019}. What remains unknown is whether and how computer science researchers consider any potential unintended consequences of their innovations on society. How do they incorporate such considerations into their research practice, if at all, and what barriers do they encounter? 

 In this paper, we explored these questions through semi-structured interviews with 20 computer science researchers in various academic positions and sub-disciplines, including Accessibility, Augmented and Virtual Reality (AR/VR), CS Education, Computer Vision (CV), Fabrication, HCI, Machine Learning (ML), Natural Language Processing (NLP), Security, Social Computing, and Robotics. Our findings revealed researchers' current attitudes and practices surrounding the unintended consequences of their research innovations (Section ~\ref{5.1}). Concretely, we observed that researchers recognize the importance of this topic, but do not proactively anticipate potential unintended consequences in practice. In fact, thinking about possible unintended consequences of their research innovations is not an integral part of their research process. To further unpack the reasons for their (in)actions, we identified two main barriers to anticipating unintended consequences: (1) the lack of formal methods and guidelines to anticipate them (Section~\ref{5.2}), and (2) academic practices that promote rapid progress and publications (Section~\ref{5.3}). 

 Overall, \rr{this work presents an in-depth qualitative investigation of applied computer science researchers' current practices and challenges in dealing with unintended consequences.} As a step towards more routinely considering these consequences in computer science research, we discuss directions for future research and provide actionable recommendations to the research community and individual researchers. 
