\section{Discussion}

Our findings demonstrate that computer science researchers in our study do not formally consider any potential societal impacts of their research innovations, despite perceiving it as important. 
%
Participants contemplated potential societal impacts of their innovations only in hindsight, such as when they were required to write a broader impact statement for a conference. They responded to individual incidents, such as when receiving participant feedback or getting bad press, instead of actively considering the topic. We contend that this observation is troublesome, as argued by Pillai et al.~\cite[p.2]{g2021co}: ``Unless ethics is integrated in every aspect of the design process and educational curriculum, it is bound to be an afterthought and thus inadequate in identifying and addressing ethical issues.'' 

%%%%% Barriers %%%%%%%%%%
While our participants generally suggested the need to proactively consider UCs, we identified various knowledge and structural barriers that currently prevent them from doing so.
%
First, participants felt that the lack of considering UCs is due to academic practices promoting fast progress and publications. In their eyes, the pressure to publish encourages researchers to de-prioritize and resist any form of stepping back and considering long-term effects. Several interviewees felt that spending time evaluating potential UCs slowed their momentum when conducting actual research. 

Second, we identified that researchers lack guidelines, tools,  and methods for considering UCs. None of our  participants reported using any of the existing tools for brainstorming about potential societal impacts, such as the Envisioning Cards~\cite{EnvisioningCards} or the Tarot Cards of Tech~\cite{tarotcards}. They also suggested that there are no  approaches or processes for thinking through the societal implications of their research that they felt genuinely satisfied with. 

 \begin{table}[t] 
    \footnotesize
    \centering
    \caption{Five Causes of Unanticipated Consequences by Robert Merton ~\cite{merton1936unanticipated}}
    \label{table:cause_merton}
    \centering
    \begin{tabular}{p{0.95\columnwidth}}
        \toprule
        \textbf{Ignorance}: Lack of knowledge, experience, expertise, and prudent investigation of a problem. \\
        \textbf{Errors}: Incorrect reasoning, analysis techniques, and interpretation of a problem. \\
        \textbf{Imperious immediacy of interest}: Actor's paramount concern with the foreseen immediate consequences excludes the consideration of further or other consequences of the same act. \\
        \textbf{Basic Values}: No consideration of further consequences because of the felt necessity of certain actions enjoined by certain fundamental values. \\
        \textbf{Self-defeating prophecies}: Predictions are frequently not sustained precisely because the prediction has become a new element in the concrete situation. \\
        \bottomrule
    \end{tabular}
\end{table}

Third, participants mentioned the lack of demographic diversity in collaborators and other academics, which they felt could enrich different viewpoints and experiences. This is a known structural issue in computer science, an occupational group that is heavily skewed towards white males~\cite{echeverri2018unintended}. Although the U.S. student population is becoming more diverse, faculty members and general academics remain predominantly white and male~\cite{bourabain2021everyday}. Furthermore, in a number of fields, most research published at high-profile venues is conducted in Western countries, with only Western participants~\cite{Linxen2021}. This means that research and innovation processes are distanced from the lives and experiences of many of their future users. Additionally, innovations can affect different kinds of people in unpredictable ways. This lack of diverse viewpoints and experiences characterized innovations that are biased against minority populations~\cite{pereidadiversity,hankerson2016does}. 
 
 Our exploration of the barriers to researchers' (in)actions resonates with Merton's five causes of UCs (see Table~\ref{table:cause_merton})~\cite{merton1936unanticipated} and enables us to place his theory in the academic context today. For example, Merton described how ignorance (i.e., a lack of knowledge and experience) can lead to UCs. We saw ignorance being part of the issue in the results of our study, with many researchers lacking the know-how to anticipate UCs and the experience of thinking about potential impacts from diverse perspectives. Merton also suggested that UCs can be caused by an ``imperious immediacy of interest,'' with people being driven to focus on the foreseen, desired consequences. As our participants mentioned, academic pressures to publish could exacerbate the desire to meet \emph{intended} consequences, such as developing a technology for the purpose of publication and/or getting a degree or promotion. They may therefore unconsciously or consciously ignore unintended effects. 

%%%%%% Responsibility %%%%%%%%%%%%%%%


 Our results indicate that researchers would more routinely consider UCs if they felt that their research outcomes had a future impact on society. As it is, many deflect responsibility to those whose research they believe is more likely to influence products. The insight surprised us given that all interviewees were selected based on their work in applied research areas, and all had previously worked for research spin-offs, open-source projects, or companies. It suggests that there may be a gap between the impact academics think their work will have and the actual risk of UCs. However, the finding is in line with recent work~\cite{nanayakkara2021unpacking, Ashurst2022AIES} analyzing the text included in the broader impact statements for NeurIPS conference papers. Their results show that authors rarely state who is responsible for preventing negative impacts, and if they do, it is often a call for action rather than a statement of adopting personal responsibility.

 Several participants mentioned that they relied on their IRBs to alert them to potential UCs, wrongfully assuming that this ``falls under the same jurisdiction.'' While providing such structure or access could certainly help, it may risk replacing researchers' responsibilities with ``legalistic bureaucracy,'' a known criticism of the IRB~\cite{brown2016five}. This is the case for ethics checklists as well, such as those designed to guide practitioners' development of AI systems, which have been found to obfuscate responsibility while additionally being too abstract or ignored~\cite{madaio2020co}. Nevertheless, the finding emphasizes the need for computer science researchers to receive more guidance in thinking through societal implications, be it through an IRB-like structure or access to ethic experts who can provide guidance. 


 The deflection of responsibility could be attributed to insufficient prior experiences with computing ethics and awareness of cautionary tales of innovations that had negative societal impacts. In fact, many participants cited their limited experience as a barrier to thinking about potential ethics pitfalls.  This may be slowly changing as more universities now offer ethics classes as part of their computer science curriculum~\cite{fiesler2020we}, which, as  several senior faculty interviewees noted, is how their students gain experience with considering these consequences.
 With an increase in ethics education and a generally heightened mindfulness of these issues due to media attention and public outcries, we may look back and be surprised that computer science researchers do not regularly consider ethical consequences in the research process. We remain hopeful, but our findings also show that overcoming barriers  will require a holistic approach to restructuring academia. In the following, we discuss how to better support researchers anticipating UCs. 

\section{Supporting Consideration of Potential UCs of Research Innovations}
 
 Our analysis suggests that there is still a long way to go before UCs are fully taken into account by researchers, despite recent calls to do so. Our work, however, does not contend that all computer science researchers should be ethicists to address the issue. Instead,
 by properly anticipating these incidents of varying severity~\cite{Greene2022AtomistOH}, researchers might also avoid suffering from their own reputation tarnishing and unexpectedly developing harmful technologies. 
 Releasing technology innovations into the wild without fully considering their effects on society is arguably a very large human-subject experiment with unknown outcomes. Our work reveals that considering societal impacts, at this point, is primarily an individual responsibility and that some form of oversight should be in place. 
 %
 While structural changes may be needed to overcome these issues, we offer concrete suggestions for tangible next steps as follows.  \\
 
 \textbf{Collect and disseminate case studies of UCs that were the result of computer science research innovations.} Our findings suggest that participants sometimes felt their own research to be too prototypical and too removed from public release to cause UCs. Others believed that certain research, especially if the goal is to address a societal issue, was less likely to result in UCs. These findings suggest the need for information that could raise awareness of UCs that resulted from varied research innovations. Concretely, we suggest collecting and disseminating case studies of the societal impacts of computer science research, which can serve as an informational resource for teaching new and experienced researchers about prior work that has had (differential) negative effects on society. It is crucial that such case studies and other relevant resources demonstrate different aspects of UCs on innovations that might be otherwise overlooked. \rr{Moreover, collecting such resources should be an invitation to collectively learn from past mistakes instead of finger-pointing, and their submission should be encouraged, such as by providing specific awards.}  \\ 
 
 \textbf{Develop tools that support learning and brainstorming about UCs.} Our study shows that researchers need more structured guidance to think through potential societal impacts. While some tools already exist, such as the Tarot Cards of Tech or Envisioning Cards, these often target practitioners and are not always suitable for research artifacts. They also require much time and cognitive effort to brainstorm about UCs. Moreover, anticipating the downstream uses and effects of research innovations is a notoriously difficult problem~\cite{prunkl2021institutionalizing}. Since a majority of our participants relied on their peers' knowledge of prior UC experiences, creativity support tools, such as those that cluster ideas from previous users to support the generation of more diverse ideas~\cite{siangliulue2015toward}, could promote sharing of UCs within communities. Such collaborative ideation tools could also serve as one approach for engaging citizen scientists in the research process. 
 \\ 
 
 \textbf{Increase access to input from diverse people in research design and development.} Our results show the benefits of more demographic diversity among researchers, so their projects can be conceived and evolve with input from people with varying values and experiences. \rr{In particular, several participants mentioned how, especially when attending alongside a diverse panel of researchers, participating in ethics workshops at conferences supported conversations and considerations about ethics in their research.} This suggestion also reflects discussions by Kling ~\cite{kling1991computerization}, who encouraged computer science practitioners to apply diverse perspectives from other fields (e.g., the social sciences) to recognize potential societal implications, and Hankerson et al.~\cite{hankerson2016does}, who suggested that hiring more diverse people at every level could help mitigate racial biases in technology. 

While these suggestions are long-term goals, a more immediate suggestion, inspired by human-centered design practices, is to engage with diverse participants of varying backgrounds, skills, and characteristics to ideate and test new technologies. For example, researchers have found that unrepresentative participant sampling may lead to the creation of non-generalizable technologies that amplify existing inequities \cite{robertson2018, Linxen2021}. Similarly, to prevent further social inequity in technology, we also propose the use of participatory and co-design methods, which have been shown to more equitably and effectively inform technologies by directly identifying user needs~\cite{Walsh2018, Tojo2022}.

Second, we suggest inviting the public to routinely shape research ideas and ongoing projects with comments and feedback. For example, Johnson and Crivellaro designed a community panel to create dialogic spaces that foster critical engagements with technologies and social issues for the purpose of reviewing research proposals on HCI~\cite{johnsonopen}. Many citizen science projects witnessed radical improvements due to public contributions~\cite{hand2010citizen}. 
These projects offer a give and take, with researchers receiving help collecting, annotating, and analyzing data in exchange for an educational gain for citizen scientists~\cite{miller2020introducing, labinthewild}. A similar involvement of citizen scientists in evaluating the societal impacts of research innovations (at all stages of the research process) could benefit both researchers and the public. 

Of course, such involvement requires precautions. \rr{For example, participatory methods might bring unequal power dynamics and risk over-reliance on citizen scientists who may not be experienced and knowledgeable in examining these issues~\cite{Birhane2022PowerTT}.} Researchers may also be hesitant to openly discuss their ideas before they are published. This may be mitigated by allowing the preregistration of ideas (similar to what has been encouraged for research studies~\cite{cockburn2018hark, registration}). Overall, we believe that allowing the public to have a voice in evaluating the societal implications of research projects on them will be a challenging but rewarding endeavor. \\


%These tools could make use of hindsight bias 

\textbf{Increase incentives to investigate UCs at various stages of the research process.} 
Our findings suggest that academics may have a lower incentive to investigate UCs than companies do, because they often do not perceive their research as immediately affecting large numbers of users and because they are less likely to face direct public backlash. While any additional task will be perceived as burdensome, we can learn from several efforts to improve scientific practices. For example, conferences have included tracks that auto-accept papers if researchers preregister their studies~\cite{icsme}. Several venues have also encouraged the replication of studies to improve the quality of science~\cite{wilson2011replichi}. Similarly, computer science venues could encourage the publication of auditing research artifacts~\cite{raji2020closing} by providing special tracks. To support such early audits, these venues could additionally allow for submissions of experience reports that describe how a research project was modified or even canceled due to the discovery of unanticipated societal impacts. According to our participants, researchers would greatly benefit from learning about such experiences to prevent UCs in the future. 

Additionally, conferences, research methods classes, and advisors should reframe finding negative UCs as an opportunity. Oftentimes, analyzing a project for disparate outcomes can ``serve as a starting point for another research project''~\cite{IUI2022}. For example, prior work on language models has suggested that discussing the potential harms of this technology can ``stimulate efforts to study and mitigate them''~\cite[p. 34]{brown2020language}. Similarly, research communities should consider rewarding researchers for uncovering and addressing societal consequences and provide venues for openly debating whether research efforts should continue, be abandoned, or change direction.

Finally, as our participants suggested, adding ``lessons learned'' to existing research publications should be made possible in digital libraries to allow others to learn from these experiences. Collecting and sharing these experiences with researchers may also help overcome the common ``not me'' attitude that we have seen in our study by showing researchers that no matter how well intentioned, almost all research can impose negative consequences that must be considered. \\

\textbf{Front-load considerations of societal impacts.} % by require submissions of UCs analysis reports along with funding proposals} 
 Recently, conferences require or nudge authors to consider UCs, e.g., by including broader impact statements in papers. However, critics have raised concerns that these statements contain speculative fiction and are published too late in the research process~\cite{Adar2021}. 
 Front-loading considerations of societal impacts in the research process can avoid the point of no return, as some of our participants described it. One possibility is encouraging pilot studies to confirm ethical practices before conducting large-scale human-subjects experiments~\cite{Truong2017}. Another possibility is to require submitting an analysis of potential UCs to funding agencies along with research proposals. Funding agencies commonly request broader impacts statements, but these are mostly used to advertise the positive impacts of a research project rather than to reflect on potential negative consequences.
 
 Two challenges must be addressed to make this happen: First, authors of a grant proposal would need adequate guidelines and tools to think through societal impacts. Second, funding agencies and their reviewers would need to learn how to evaluate such societal impact statements. A similar process was successfully tested at an academic institution~\cite{Bernstein2021EthicsAS}, though it was cautioned that it can be perceived as ''burden`` without additional scaffolding. 
 
 Therefore, one immediate next step is to invest in research efforts that can provide guidance to authors and reviewers. Institutions could further assist researchers with regular consultation by hiring an ethics advisor similar to the ethics consultation service in most medical programs~\cite {bioethics, medical}.  With the onset of more advanced technology applications, having a technology ethics advisor on staff to answer researcher questions and review proposals and papers might alleviate burdens for researchers on their own.\\ 


\textbf{Provide guidelines for reacting to societal impacts.}
 In addition to the lack of guidelines for \emph{considering} UCs, our participants frequently mentioned the lack of guidelines for \emph{reacting} to them once they were discovered. Their hesitation to consider societal impacts may come from knowledge barriers regarding anticipating and reacting to UCs. In line with Knight's recommendation to addressing UCs, we offer two suggestions that aim to ``increase knowledge'' about UCs and combine uncertainties within a large-scale community ~\cite{knight1921risk}. First, we advocate creating opportunities where researchers with similar research topics can share their experiences with considering and addressing UCs. These opportunities, which could be digital platforms (e.g., online forums) or physical venues (e.g., conferences and workshops), would not only support researchers by recognizing common UCs within their sub-disciplines but would also create community and dialogue around UCs. Second, funding agencies could provide resources similar to the IUI 2022 program chairs, who have put together a list of links to papers and tools for authors to audit their projects~\cite{IUI2022}. Providing them with checklists and tools to evaluate these analysis reports will be essential to avoid ethics washing~\cite{prunkl2021institutionalizing}. Altogether, providing guidelines for such decisions is desirable; we believe that HCI researchers, given the field's interdisciplinary nature and focus on societal impact, are uniquely equipped to drive research in this direction. 