%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Submitted to DEEM '23]{}{June 18,
  2023}{Seattle, WA, USA}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{svg}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}
% \usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage[capitalize,noabbrev]{cleveref}
\input{mathcommands.tex} 
\mathtoolsset{centercolon}
\newcommand{\proc}[1]{#1}
\newtheorem{assumption}{Assumption}
\usepackage{xcolor}
\usepackage{color, colortbl}
\usepackage{tabularx}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{breqn}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Model Cascades for Efficient Image Search}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Robert Hönig}
\authornote{Both authors contributed equally to this research.}
\email{robhoenig@gmail.com}
\affiliation{%
  \institution{ETH Zürich}
  % \streetaddress{P.O. Box 1212}
  % \city{Zürich}
  \country{Switzerland}
  % \postcode{43017-6221}
}
\author{Mingyuan Chi}
\authornotemark[1]
\email{minchi@ethz.ch}
\affiliation{%
  \institution{ETH Zürich}
  % \streetaddress{P.O. Box 1212}
  % \city{Zürich}
  \country{Switzerland}
  % \postcode{43017-6221}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
    % \renewcommand{\shortauthors}{Hönig and Chi}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Modern neural encoders offer unprecedented text-image retrieval (TIR) accuracy. 
However, their high computational cost impedes an adoption to large-scale image searches.
We propose a novel image ranking algorithm that uses
a cascade of increasingly powerful neural encoders
to progressively filter images by how well they match a given text.
Our algorithm reduces lifetime TIR costs by over 3x.
\end{abstract}


%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003338</concept_id>
       <concept_desc>Information systems~Retrieval models and ranking</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003338.10003346</concept_id>
       <concept_desc>Information systems~Top-k retrieval in databases</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003338.10003344</concept_id>
       <concept_desc>Information systems~Combination, fusion and federated search</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Information systems~Retrieval models and ranking}
\ccsdesc[300]{Information systems~Top-k retrieval in databases}
\ccsdesc[500]{Information systems~Combination, fusion and federated search}
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{neural networks, text-image retrieval, cascaded models}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}
Search engines are the most widely used tool for information retrieval (IR) on the internet --- Google alone processes over 8.5 billion searches a day \cite{googlesearches}. A search engine takes as input some query $q$ and returns a list of documents $\gD$ ranked by how well they match $q$. Keyword-based search ranks results by naively matching query keywords with documents. Semantic search tries to improve on keyword-based search by matching queries to documents based on their meaning.
A fruitful domain for semantic search is TIR, where documents are images and queries are texts. New semantic search engines for TIR leverage recent advances in deep learning for processing images and natural language \cite{evertrove, Vespa, jain2017image, mishra2021deep}. Typically,
these engines use neural networks to construct an image encoder $I$ and a text encoder $T$ that process text $q$ and each image $d\in\gD$
into embeddings $\vv_q = \f{T}{q}$ and $\mV_{\gD} = \cbrace{\vv_d = \f{I}{d} : d\in\gD}$ that capture their semantics. Then, the engines rank images in $\gD$
by some similarity measure of $\vv_d$ and $\vv_q$.
In large-scale search scenarios, $\gD$ may contain several million documents. This makes it
computationally expensive to compute embeddings for all documents.

\begin{figure}
    \centering
    % \vspace*{-3em}
    \resizebox{0.45\textwidth}{!}{%
        \input{images/casCLIP_architecture_detailed_1.tikz}
    }
    % \vspace{-1em}
    \caption{Schematic of our algorithm for a 2-level cascade $\bracket{I_s, I_l}$. In this example, encoder $I_s$ computes embeddings $\mV_{\gD}$ of all four images (leftmost four squares) at build time. At runtime, the images that correspond to two the highest-ranking embeddings (green) are processed by encoder $I_l$ that produces embeddings $\mV_{\gD_2}$ of higher quality. Finally, we rerank the top-2 images with  $\mV_{\gD_2}$ to output the highest-ranking image. }
    % \vspace{-1em}
    \label{fig:arch_detail}
\end{figure}

We seek to lower this computational cost while preserving search quality. To this end, we measure search quality as Recall@$k$, which denotes the fraction of searches that include the desired result in the top-$k$ results. Even small increases in $k$ can
 significantly improve the Recall@$k$  \cite{xvlm, clip}. Hence, for $g\gg k$, the top-$k$ results 
  of a large and expensive encoder $I_l$ are likely included in the top-$g$ results of a small and cheap encoder $I_s$. This observation leads to our main idea: \emph{At build time, pre-compute $\mV_{\gD}$ with $I_s$. Then, at runtime, to handle a query $q$, retrieve the top-$m$ results 
$\gD_m \subset \gD$ for some $m\gg k$, recompute $\mV_{\gD_m}$ with $I_l$ and return the top-$k$ results.}
This idea, illustrated in  \Cref{fig:arch_detail}), naturally extends to a cascade of $r$ progressively larger encoders that compute progressively smaller sets $\mV_1\ldots,\mV_r$.

In practice, it is possible for over 90\% of all documents in $\gD$ to never be included in any search result over the lifetime of a large-scale search engine \cite{googlesearchstats}. This means that our technique would evaluate $I_l$ on less than 10\% of $\gD$, resulting in significant lifetime computational savings. In this work we make the following contributions:
\begin{itemize}% [leftmargin=4.5mm,noitemsep]
    \item We introduce a novel cascading algorithm for fast TIR.
    \item We show that our algorithm speeds up TIR on standard benchmarks by over 3x at no reduction in search quality.
    \item We investigate the benefits of deep cascades and
    demonstrate a 2x reduction in query latency.
\end{itemize}




\section{Related Work}
Model cascading is a recurrent theme in the literature on efficient machine learning (ML) systems.
FrugalML \cite{frugalml} minimizes access costs of ML APIs by cascading two calls to a cheap API and to an expensive API.
NoScope \cite{noscope} speeds up object detection in videos by splitting a reference model into a squence of two specialized models.
Model cascades have also been applied to facial key point estimation \cite{facialest}, pedestrian detection \cite{pedest} and other domains.

Recent work on encoders for TIR is dominated by transformer-based bi-encoders (BEs) \cite{clip, xvlm, vse++} and
cross-encoders (CEs) \cite{x2lvm, lxmert, uvl}. BEs process images and texts with separate encoders, whereas CEs also add cross-connections between the encoders. Hence, CEs are more powerful, but need to recompute $\mV_{\gD}$ for new queries. This makes them impractical for large-scale searches and unsuitable for our idea. Therefore, we focus on BEs.

Several methods for fast TIR with CEs have been developed:  VLDeformer \cite{vldeformer} trains a decomposable CE that can be used as a BE at inference time with minimal loss in quality.
CrispSearch \cite{crispsearch}, LightningDot \cite{lightningdot} and ``Retrieve Fast, Rerank Smart'' \cite{rfrs} all introduce two-level sequences of a BE whose results can be cached for approximate inference and a CE for precise inference on a subset of the BE results. This is similar to our idea but differs in two key ways:
First, we consider arbitrarily deep model cascades, whereas these approaches are fundamentally limited to two models.
Second, we target BE inference instead of CE inference. In fact, this suggests that our approach could complement these existing techniques as the BE model in their first stage for even faster TIR.


\section{Models and Methods}


\subsection{Cascaded Search}
\label{sec:cascadedsearch}
Let $\gD$ be a collection of $n$ images 
that we want to query with a cascade of BEs. Consider a cascade of image encoders
$I = \bracket{I_s, I_1, \ldots, I_r}$ that all use the same text encoder $T$. We propose \Cref{alg:cascadedsearch} to query $\gD$ by ranking all images with $I_s$ and subsequently the top $m_j$ images with $I_j$. Note that with $r=0$, \Cref{alg:cascadedsearch} reduces to a standard BE search. 


\paragraph{Computational cost}
Assume that function Query in \Cref{alg:cascadedsearch} invoked $q$ times and denote the computational cost of \Cref{alg:cascadedsearch} with $\f{C}{I, q}$.
We want to minimize the lifetime computational cost of \Cref{alg:cascadedsearch}, that is $\f{C}{I, q}$ as $q\rightarrow\infty$. We can decompose $\f{C}{I, q}$ into the sum of the lifetime image encoding cost $\f{a}{I, q}$ and some term $\f{b}{q}$ that is independent of $I$ and thus irrelevant for optimization over $I$.
Next, we formalize our introductory observation on the set of a search engine's lifetime search results  into the following key assumption:

\begin{figure}
  % \begin{minipage}{0.45\textwidth}
    % \vspace*{-2.7em}
    \begin{algorithm}[H]
     \caption{Cascaded Search. Here, $\f{\mathrm{Rank}}{\gI, \mV, \vt}$ sorts the images in $\gI$ by the cosine similarity of their encodings $\mV$ with text encoding $\vt$.}
     \label{alg:cascadedsearch}
  \begin{algorithmic}[1]
    \STATE {\bfseries Input:} $\bracket{I_s, I_1,\ldots,I_r}$, $m_1 > \ldots > m_r \in \sN$, $\gD$
    \STATE {\bfseries Init:} \algorithmicfor\,\,$c \in \gD$ \algorithmicdo\,\,\,$\mV_s\!\bracket{c} \longleftarrow \f{I_s}{c}$
      \vspace{0.2em}
    \STATE \textbf{Function} \proc{Query}($\mathrm{text}$)
      \STATE $\mathrm{Top} \longleftarrow \f{\mathrm{Rank}}{\gD,\mV_s,\f{T}{\mathrm{text}}}$
      \FOR {$j=1$ {\bfseries to} $r$}
      \vspace{-0.5em}
      \STATE \algorithmicfor\,\,$c\in\mathrm{Top}\!\bracket{1\ldots m_j}$\,\,\algorithmicdo\,\, $\mV_j\!\bracket{c} \xleftarrow{\text{if empty}} \f{I_j}{c}$ 
      % \vspace{-1.3em}
      \STATE $\mathrm{Top} \longleftarrow \f{\mathrm{Rank}}{ \mathrm{Top}\!\bracket{1\ldots m_j},\mV_j,\f{T}{\mathrm{text}}}$
      \ENDFOR
      \STATE {\bfseries return} $\mathrm{Top}\!\bracket{1}$
    \STATE \textbf{EndFunction}
  \end{algorithmic}
  \end{algorithm}
% \end{minipage}
% \vspace*{-2em}
\end{figure}

\begin{assumption}
\label{ass:search}
For $q\in\sN$, let $S_q \subset \gD$ be the set of all images pushed to $\mathrm{Top}$ in query $q$. Then, $\frac{1}{n}\abs{\bigcup_{q\in\sN}S_q} =: f \ll 1$.
\end{assumption}
If $I_s, I_1, \ldots, I_r$ have costs $t_s < t_1 < \ldots < t_r$, then
\Cref{ass:search} implies that $$\f{a}{I, q} = nt_s + fn\sum_{i=1}^rt_i\text.$$ Hence, the 2-level cascade $\bracket{I_s, I_1}$ is cheaper than the 1-level cascade $\bracket{I_1}$ if the speedup factor $\brace{t_s + ft_1}/{t_1}$ exceeds 1.

We note that \Cref{ass:search} implies no computational advantage of the $(r\!+\!1)$-level cascade $I$ with $r > 1$ over the equally powerful $2$-level cascade $I' = \bracket{I_s,I_r}$ with $m'_r = m_1$. However, if $q$ is low enough that $\mV$ is not hit, then the $(r\!+\!1)$-level cascade $I$ speeds up individual queries by a factor of
% \leavevmode
\begin{equation} % \displaywidth=\parshapelength\numexpr\prevgraf+2\relax
  \label{eq:queryspeedup}
  m_1t_r/\sum_{i=1}^rm_it_i
\end{equation} This is useful, because unlike an uncascaded model $[I_r]$ that executes the expensive image encoder $I_r$ only  during build time, the 2-level cascade $I'$ has a $m_1 t_r$ runtime overhead when $\mV$ is not hit. Hence, deep cascades can mitigate the increased latency of early queries in 2-level cascades.


\subsection{Creating the Cascade}

\begin{table}[h]
  % \small
  % \vspace*{-1.8em}
  \caption{Recall@$k$ in \% and lifetime speedup of the 2-level cascade $\bracket{\textrm{ViT-B/32}, \textrm{ViT-B/16}}$ over the uncascaded baseline $\bracket{\textrm{ViT-B/16}}$.}
  % \vspace*{-1.5em}
  \label{tab:2lvlmscoco}
% \vskip 0.1in
\begin{tabular}{llllll}
\hline
Dataset & Method & R@$1$ & R@$5$ & R@$10$ & Speedup \\ \hline
\multicolumn{1}{c}{\multirow{2}{*}{MSCOCO}} & No Cascade & 30.1 & 54.2 & 64.6 & 1x \\
\multicolumn{1}{c}{} & Cascade & +0.2 & +0.4 & +0.5 & 3.2x \\ \hline
\multirow{2}{*}{Flickr30k} & No Cascade & 29.9 & 52.0 & 61.3 & 1x \\
  & Cascade & +0.8 & +2.0 & +2.4 & 3.2x
\end{tabular}
\end{table}

We apply our proposed methods to CLIP \cite{clip}, a powerful transformer-based text-image BE. CLIP uses the GPT-2 architecture \cite{gpt2} for the text encoder, the vision transformer (ViT) \cite{vit} architecture
for the image encoder and matches images to texts by the cosine similarity of their embeddings.
% Several pre-trained ViT encoders of different sizes are publicly available. 
We create a cascade $\bracket{I_s, I_1, \ldots, I_r}$ from publicly available trained CLIP image encoders of different sizes.


\section{Experiments}

\subsection{Experimental Setup}
% \setlength{\parskip}{0pt}
% \paragraph{Models} We use CLIP with the ViT-B/16\footnote{ViT-B/$n$ tiles an image into $n\times n$ patches.} image encoder as our baseline 1-level cascade $\bracket{I_l}$. For retraining, we set $\f{M}{I_l}$ to
% the four times faster ViT-B/32 image encoder.

\begin{description}
  \item[Metrics]  Given a dataset $\gD$ of image-caption pairs we measure the Recall@$k$ (R@$k$) metric as the fraction of captions in $\gD$ whose corresponding image is among the top-$k$ search results. 
  In line with the IR literature, we report the Recall@$k$ for $k\in\cbrace{1,5,10}$. In addition, we report for 2-level cascades the lifetime speedup and for deeper cascades the query speedup as discussed in \cref{sec:cascadedsearch}. We run all experiments on an Intel i7-11800H CPU at 2.30 GHz with turboboost disabled and compute speedups by measuring the total CPU time of queries. 
  \item[Datasets] We evaluate our algorithm on the MSCOCO validation dataset with 5k samples and on the Flickr30k dataset with 32k samples.
  \item[Parameters] We set the top-$m$ value of encoder $I_1$ to $m_1 = 50$ and assume a lifetime return fraction of $f = 0.1$.
\end{description}




\subsection{2-level cascades}
\label{sec:2lvlcascades}
% \begin{wraptable}{}{0.5\textwidth}
% \small
% \caption{Top-$k$ accuracies in \% and lifetime speedups of 2-level cascades $\bracket{\f{M}{I_l}, I_l}$ over the uncascaded baseline $\bracket{I_l}$ on MSCOCO.}
% \vskip 0.1in
%  \setlength{\tabcolsep}{3pt}
% \begin{tabular}{lllll}
% \hline
% Method & Top-1 & Top-5 & Top-10 & Speedup \\ \hline
% No Cascade & 30.1 & 54.2 & 64.6 & 1x \\
% \rowcolor[HTML]{EFEFEF} 
% Retraining & +0.2 & +0.4 & +0.5 & 3.2x \\
% \end{tabular}
% \label{tab:2lvlmscoco}
% \end{wraptable}

We use the Huggingface \cite{huggingface} CLIP implementation with a ViT-B/16 image encoder as our uncascaded baseline $\bracket{I_1}$. We use the faster ViT-B/32 image encoder as $I_s$ to create the 2-level cascade $\bracket{I_s, I_1}$. 
\Cref{tab:2lvlmscoco} shows empirical results. The cascaded model reduces lifetime computational costs threefold. Surprisingly, the cascaded model achieves at the same time consistently higher Recall@$k$ than the uncascaded model. One explanation may be that ViT-B/32 initially processes input images into 32x32 tiles. Since this tiling is more coarse-grained than the 16x16 tiling used by ViT-B/16, it may offer superior approximate filtering of search results. Hence, $I_s$ could determine the top $m_1$ images more effectively than $I_1$. Further research is needed to explain why 2-level cascades show superior Recall@k.




\subsection{3-level cascades}
\begin{table}[h]
  % \small
  % \vspace*{-1.7em}
  \caption{Recall@$k$ in \% and query speedup of the 3-level cascade $\bracket{\textrm{ViT-B/32}, \textrm{ViT-B/16}, \textrm{ViT-L/14}}$ with $m_2=10$ over the 2-level cascade $\bracket{\textrm{ViT-B/32}, \textrm{ViT-L/14}}$.}
  % \vskip 0.1in
  \begin{tabular}{llllll}
    \hline
    Dataset & Method & R@$1$ & R@$5$ & R@$10$ & Speedup \\ \hline
    \multicolumn{1}{c}{\multirow{2}{*}{MSCOCO}} & No Cascade & 32.5 & 57.2 & 68.1 & 1x \\
    \multicolumn{1}{c}{} & Cascade & +0.5 & +0.2 & -3.0 & 2.0x \\ \hline
    \multirow{2}{*}{Flickr30k} & No Cascade & 35.3 & 58.5 & 67.4 & 1x \\
     & Cascade & +1.0 & +0.0 & -3.7 & 2.0x
    \end{tabular}
  \label{tab:3lvlmscoco}
\end{table}
  
  

As noted in \cref{sec:cascadedsearch}, $n$-level cascades offer no reduced lifetime costs over $2$-level cascades, but may speed up individual queries. This is important for large image encoders that slow down queries, such as the ViT-L/14 encoder that is 3.3x slower than ViT-B/16. Therefore, we introduce the 2-level cascade $\bracket{\textrm{ViT-B/32}, \textrm{ViT-L/14}}$ and compare it against the 3-level cascade $[\textrm{ViT-B/32}, \textrm{ViT-B/16},\allowbreak \textrm{ViT-L/14}]$. Concretely, we set a target speedup of 2x and use \Cref{eq:queryspeedup} to determine the corresponding number $m_2$ of top ranked images on which \Cref{alg:cascadedsearch} should execute ViT-L/14. This yields $m_2 = m_1\brace{\frac{1}{2} - \frac{t_1}{t_2}}  = 50\brace{\frac{1}{2} - \frac{1}{3.3}}\approx 10$. \Cref{tab:3lvlmscoco} reports the empirically measured query speedups and the change in Recall@$k$ of the 3-level cascade. Similarly to \Cref{sec:2lvlcascades}, the deeper cascade offers superior predictions. However, for Recall@10 the predictions become significantly worse. This is because \Cref{alg:cascadedsearch} only uses ViT-L/14 to rerank the top $m_2 = 10$ images, so the set of the top 10 images stays unchanged. Hence, for $m_2 = 10$, the cascade $\bracket{\textrm{ViT-B/32}, \textrm{ViT-B/16}, \textrm{ViT-L/14}}$ is equivalent to the less powerful cascade $\bracket{\textrm{ViT-B/32}, \textrm{ViT-B/16}}$ with respect to the Recall@10 metric.

\section{Conclusion}

Our experiments show that \Cref{alg:cascadedsearch} can lower lifetime computational search costs by over 3x at no reduction in search quality. At the same time,  we show that deeper model cascades can mitigate the increase in latency of early queries.

However, single-digit speedups may not sufficiently reduce computational costs to economically rank large-scale image databases with expensive transformer-based BEs. Instead, a practitioner may use traditional search engines to retrieve the top-$k$ images and apply a neural search cascade on top of it. This heterogeneous cascade may offer a viable path towards the integration of state-of-the-art neural networks with established image search platforms.

It is important to note that all our observations rely on \cref{ass:search}. While we have provided anecdotal evidence to support our choice of the lifetime return fraction as $f = 10\%$,
different search scenarios likely
vary in $f$ and achieve accordingly different speedups.



 
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
