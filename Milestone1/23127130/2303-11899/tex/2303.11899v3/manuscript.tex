\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{xcolor}
\usepackage{cite}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{graphicx}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\graphicspath{{fig/}}
\begin{document}
\title{Large-Scale Regional Traffic Signal Control Using Dynamic Deep Reinforcement Learning}
% \title{Deep Reinforcement Learning for Regional Signal Control in Large-scale Traffic Grid}
\author{Hankang Gu, Shangbo Wang, Member, \textit{IEEE}}

\maketitle
\begin{abstract}
Multi-agent Reinforcement Learning (MARL) based traffic signal control becomes a popular research topic in recent years. Most existing MARL approaches tend to learn the optimum control strategies in a decentralised manner by considering communication among neighbouring intersections. However, the non-stationary property in MARL may lead to extremely slow or even failure of convergence, especially when the number of intersections becomes large. One of the existing methods is to partition the whole network into several regions, each of which utilizes a centralized RL framework to speed up the convergence rate. However, there are two challenges for this strategy: the first one is how to get a flexible partition and the second one is how to search for the optimal joint actions for a region of intersections. In this paper, we propose a novel training framework where our region partitioning rule is based on the adjacency between the intersections and propose Dynamic Branching Dueling Q-Network (DBDQ) to search for optimal joint action efficiently and to maximize the regional reward. The experimental results with both real datasets and synthetic datasets demonstrate the superiority of our framework over other existing frameworks.


\end{abstract}
\begin{IEEEkeywords}
Adaptive traffic signal control, Multi-agent Reinforcement Learning, Regional control
\end{IEEEkeywords}
\section{Introduction}
\IEEEPARstart{T}{raffic} congestion is becoming a significant problem that brings both financial costs and environmental damage. 
According to a recent study, in 2021, traffic congestion costs \pounds 595 and 73 hours per driver in the UK \cite{inrix2021uk} while drivers in the USA spent \$ 564 each and wasted 3.4 billion hours a year in total \cite{inrix2021us}.
Meanwhile, the gas emission caused by traffic congestion is now an unignorable contributor to the pollutants responsible for air pollution \cite{who}.
Therefore, there is an urgent demand to apply certain strategies to relieve urban traffic congestion.  
Traffic signal control (TSC) is an efficient and direct way to manage and regulate the movements of vehicles so that the flow is smooth.
With advanced technology and more data nowadays, it is promising to find a better way to utilise traffic signals to reduce congestion.

By controlling the timing and phase sequence of signals, the use of available approach capacity can be optimised and priority can be given to more congested approaches. TSC strategies can be generally classified into two categories: classical methods and AI-based methods. Classical methods take rule-based signal plans such as Webster  \cite{webster1958traffic,koonce2008traffic} and MaxBand \cite{little1981maxband}. 
These two methods compute optimised signal plans based on traffic parameters such as traffic demand and saturation rate. However, since these two methods assume that traffic flow is uniform and intersections share the same cycle length, they hardly adapt to more complex traffic dynamics in real scenarios.

AI-based methods such as Deep reinforcement learning (DRL) become very popular in adaptive traffic signal control (ATSC) due to the huge achievement and success in both reinforcement learning (RL) and deep neural network (DNN).  
RL, a method where an agent learns how to act by interacting with the environment and getting feedback from the environment, has shown success in making sequential decisions and has been proved with lots of theoretical convergence properties \cite{sutton2018reinforcement}.
Tabular RL which stores the value of states in a table requires huge computational resources if the dimension of the state is large. Meanwhile, DNN has been examined successfully to approximate the value of high-dimension input in many subjects such as machine translation and image classification during the last few decades \cite{lecun2015deep}. As a consequence, DRL has been examined with better generalization ability in high dimension states \cite{mnih2015human,kober2013reinforcement,vinyals2019alphastar}.

Researchers have applied SARSA \cite{rummery1994line,sutton1995generalization} and tabular Q-learning \cite{watkins1992q} to both single intersection and multiple intersections \cite{thorpe1996tra,wen2007stochastic,el2010agent} successfully.
However, the fully centralised setting where a single agent controls all intersections costs outrageous computational resources and suffers from scalability issues and the curse of dimension in large-scale traffic networks. To utilise DRL in a larger traffic network, the fully decentralised setting where a single agent controls only one intersection holds better scalability and naturally leads us to consider multi-agent deep reinforcement learning (MADRL) where agents not only interact with the environment but also interact with other agents.
The simplest strategy of MRDRL is independent agents(IA) where each agent only considers its individual situation and maximizes its own reward \cite{Ault2020LearningAI,calvo2018heterogeneous,abdoos2011traffic}. 
Although IA can be applied to large-scale traffic networks conveniently, the non-stationary property and unstable convergence property still limit the performance of IA. 

To improve scalability and alleviate non-stationary property, researchers have proposed cooperative agents by either modelling optimised joint action or sharing certain information \cite{van2016coordinated,tan2019cooperative,wang2020large,wei2019colight,xie2020iedqn}.  
In \cite{van2016coordinated}, the max-plus algorithm is used to search the optimal joint action over agents. 
Tan et al. adopt iterative action search to evaluate the Q-values of different sets of joint actions\cite{tan2019cooperative}.
Wang et al. \cite{wang2020large} proposed a cooperative agent called Co-DQL in which the observation and reward of one agent are aggregated with the average value of its neighbour agents' observations, actions and rewards. 
Wei et al.\cite{wei2019colight} proposed CoLight which uses a graph attention network to embed the observations of its neighbourhood agent.   
In \cite{xie2020iedqn}, a Mess-Net is designed to embed a global state into a global message and each agent takes a part of the global message as a local message. Since states and Q-values are shared implicitly, agents cooperate to reach better performance. 

Apart from the fully centralised setting and the fully decentralised setting, a compromised setting where each agent controls a small region of intersections has the potential to achieve better scalability and alleviate the non-stationary issue by reducing the number of agents. 
To the authors' best knowledge, the first paper that adopts this setting is \cite{chu2016large}.
The traffic grid in \cite{chu2016large} was partitioned into sub-networks based on normalized cut in network theory and the RL agent was trained with a centralised approximated Q-learning framework. In \cite{tan2019cooperative}, a grid traffic network was partitioned into several $ 2 \times 3$ regions and controlled in a decentralised-to-centralized manner by regional DRL(R-DRL). However, in this setting, we further discuss the following challenges:
\begin{itemize}
    \item An ideal traffic network is a grid where intersections, except those on boundaries, have exactly four neighbourhoods. However, in real life, not all traffic networks are grid-like. In heterogeneous networks, it might be impossible to partition the network into desired regions with a fixed size. For example, a $4 \times 6$ traffic grid was partitioned into four $2 \times 3$ grid regions in \cite{tan2019cooperative}. If we change the size of the traffic network to  $4 \times 4$, $2 \times 3$ fixed-size regions fail to cover the new traffic network perfectly. 
    In other words, the flexibility to adapt to different traffic networks is also a practical property. Here, if all intersections in that region exist in the network, we define that a region is fully loaded. Otherwise, the region is partially loaded.
    \item The performance of a region is evaluated by all intersections in this region. How to credit the contribution of each intersection, especially not existing intersections in a partially loaded region is not explored.
    \item As the number of intersections in a region increases, the cardinality of action space grows exponentially. In our experiments, the simple DQN failed to converge when the cardinality reaches one thousand. Thus, how to search for the optimal action for a region of intersections is critical. 
\end{itemize}

To tackle the above challenges, we propose a new design of a partitioning rule and extend Branching Dueling Q-Network(BDQ) with dynamic target value computing. Experiments show that, although there is no coordination or communication between our agents, the performance of our agents is better than cooperative agents. Our main contributions are listed as followings.
\begin{enumerate}
\item We propose a shape-free region partition rule. The region under our partition rule includes a center intersection and its adjacent intersections. Subsequently, our region has no fixed shape and can be applied to heterogeneous scenarios. Due to the property of our region, in the grid traffic network, the size of a region is five and some regions are inevitably not fully loaded. To improve convergence and generalised ability, agents are trained with centralized learning with decentralized execution(CLDE) paradigm where agents share parameters and experience buffer but execute independently.
\item To solve the credit issues that arise with partially loaded regions, we extend BDQ to Dynamic-BDQ(D-BDQ) in which target value and loss are computed only among activated action branches. Since each region might have different activated action branches, experiment results show that our agents under our extension have the potential to control a dynamic number of intersections and regional traffic signal control in non-grid traffic networks.
\item we further examined the robustness of this partition rule by employing different partition settings and applied DBDQ to $2 \times 3$ regions. Experimental results show that DBDQ can search for more optimal actions more efficiently. 

\end{enumerate}
 The structure of the rest paper is arranged as follows: Section \ref{sec: related work} discusses the related work of this paper. Section \ref{sec: background} introduces the background and notations of traffic signal control and MARL. Section \ref{sec: ATSC formulation} formulates the 
 and section \ref{sec: regional control formulation} presents the formulation of our region agents. Section \ref{sec: sec experiments and results} describes the setting of experiments and discusses the results. Section \ref{sec: conclusion} summarises this paper.


 
\section{Related Work}
\label{sec: related work}
In this section, we revise papers about cooperative agents in traffic signal control and joint action searches in high-dimension action space. 
In the early stages, most researchers calculated the optimised signal plan based on strong assumptions and considering only local information. 
Webster \cite{webster1958traffic,koonce2008traffic} considered one isolated intersection with uniformed traffic flow and calculates a fixed signal plan based on local information such as saturation rate and volume-to-capacity ratio. GreenWave \cite{roess2004traffic} further considered the offsets between phases over neighbour intersections. Although GreenWave computes the offsets between intersections with the ratio of lane length and expected velocity, the ratio is mostly static and this method is hard to adapt to traffic flow that is not uniform.

In recent decades, more researchers have realised that only local information is not enough to design intelligent controllers in a system of intersections and have started to utilise information from other intersections.
The most naive way is to use a fully centralised agent to control all signals with the global information of one system\cite{thorpe1996tra,wen2007stochastic,el2010agent}. Although this strategy shows a convergence advantage in small-size traffic networks, it suffers scalability issues and poor convergence properties in large-scale traffic networks. Areal et al.\cite{arel2010reinforcement} proposed Neighbour RL in which fully decentralised agents with linear Q-approximation controls each intersection based on a concatenation of the state of the neighbourhood intersection. To now, lots of novel algorithms based on the fully decentralised setting have been proposed to coordinate agents and share information among agents either explicitly or implicitly. 

Van der Pol et al.\cite{van2016coordinated} modelled the traffic network as a linear combination of each intersection and apply the max-plus algorithm\cite{kok2005using} to select joint actions. In \cite{tan2018large}, according to the congestion level between the intersection and its neighbourhoods, the agent can either choose actions by greedy algorithm or Neighbourhood Approximate Q-Learning.
Tan et al.\cite{tan2019cooperative} proposed an iterative action search approach to search optimal joint actions. In their approach, each agent first proposes its local best action. Then, from this initial joint action search point, each agent iteratively chooses whether to deviate from its local best action based on the value computed by a global function. Although the iterative search approach offers sufficient trials for different joint actions, the effectiveness depends on the performance of the global function and the assumption that the global function is well-learned is still too strong for large-scale traffic networks.

Meanwhile, communications between intersections have been studied by lots of researchers and communications mainly bring two benefits: the first one is that an agent has more information about the transition of environment and the second one is that an agent can predict the policies of other agents.
When designing information sharing, most papers have drawn their attention to the neighbouring intersections.
Varaiya \cite{varaiya2013max} proposed Max-pressure which considers the difference between the number of waiting vehicles of upstream intersections and downstream intersections and this idea was further applied in an MDRL framework---PressLight\cite{wei2019presslight}. In CoLight\cite{wei2019colight}, a Graph Attention Network (GAN) was applied to compute the attention score between intersections and hidden layers are combined with their respective importance to generate graph-level attention layers. Although the observation of each agent only contains individual information, the hidden layer contains embedded information of all intersections as graph attention layers get deeper.
Wand et al. \cite{wang2020large} proposed Co-DQL in which the Q-values of agents converge to Nash equilibrium. In their work, the state of one intersection is concatenated with the average value of its neighbourhood intersections while the reward of one intersection is summed with a weight-average of rewards of its neighbourhood intersections.
Zhang et al.\cite{zhang2022neighborhood} proposed neighbourhood cooperative hysteretic DQN (NC-HDQN) which studies the correlations between neighbourhood intersections.
In their work, two methods are designed to calculate the correlation degrees of intersection. The first method named empirical NC-HDQN (ENC-HDQN) assumes that the correlation degrees of two intersections are positively related to the number of vehicles between two intersections. In ENC-HDQN, the correlation degrees are always positive and the threshold is manually defined according to the demands of traffic flows. The other method named Pearson NC-HDQN (PNC-HDQN) stores reward trajectories of each intersection and computes Pearson correlation coefficients based on those trajectories. Unlike ENC-HDQN, PNC-HDQN allows negative correlation degrees between intersections to represent the negative effects between intersections. With correlation degrees of neighbourhood intersections, the reward of one intersection is the rewards of its neighbourhood intersections multiplied with correlation degrees respectively. In ENC-HDQN, the calculation of rewards can be interpreted as a weighted average of several rewards since all coefficients are non-negative. However, in PNC-HDQN, negative coefficients might break the interpretation of the reward. 

As one agent controls more intersections, the size of its action space grows exponentially. 
If the agent controls $n$ intersections and each intersection offers $a$ actions, then the cardinality of its action space is $a^n$. This brings problems both to exploration and sampling. To increase the efficiency of joint action searching, Lee et al. \cite{lee2019reinforcement} proposed a weight fixing procedure and reduced the size of the output of the neural network from $a^n$ to $a\times n$. In their method, although the size of the output of the neural network is linear, the Q-values of all actions in joint action space are still computed and the Q-values of each intersection are considered independent.  
Tan et al\cite{tan2019cooperative} applied a policy architecture named Wolpertinger Architecture(WA), proposed by G. Dulac-Arnold et al.\cite{dulac2015deep}, which searches for the optimal action based on a proto-action and applied Deep Deterministic Policy Gradient(DDPG) to perform training. In WA, the actor in DDPG generates a protocol action in continuous space. Then this protocol action is embedded and mapped to $k$ proposed actions by the k-NN algorithm. Finally, WA chooses the action with the highest state-action value evaluated by the critic in DDPG. However, during training, DDPG computes the state-action value of the next state directly by the continuous protocol action generated by the actor. This might introduce bias during evaluating loss. The indexing of action might influence the embedding of actions and further affect policy search. Tavakol et al. proposed a new neural architecture named Branching Dueling Q-Network(BDQ) that features a shared hidden representation of state and predicts state-action value for each sub-action branching\cite{tavakoli2018action}. BDQ embeds the state of the agents into a shared representation. Then, the shared representation is further used to evaluate state value and compute the action advantages for each action dimension. In their architecture, the size of the outputs grows linearly and each sub-action could receive the specific loss propagation. 


\section{Background}
\label{sec: background}
In this section, we revise the background knowledge of traffic signal control and MARL to let readers better understand our approaches.
\subsection{Traffic Signal Control Definition}
\label{sec: Traffic Signal Control Definition}
In a traffic network $\mathcal{G}=(\mathcal{V},\mathcal{E})$ where $v_i \in \mathcal{V}$ represents an intersection and $e_{vu}=(v,u)\in  E$ represents the connection between two intersections. 
The entering approach is the approach on which vehicles enter the intersection and the leaving approach is the approach on which vehicles leave the intersection. Each approach consists of a number of lanes and there are entering lanes and leaving lanes. The set of incoming lanes of intersection $v_i$ is denoted as $L_i$. A traffic movement is defined as a pair of one entering lane and one leaving lane. A phase is a combination of traffic movements. As illustrated right side in Figure \ref{fig:Traffic network}, one intersection has four phases $p$ which are North-South Straight(NS), North-South Left-turn(NSL), East-West Straight(EW) and East-West Left(EWL). A signal plan $SP=\{(p_1,t_1),(p_2,t_2),...\}$ is a sequence of tuples which indicates the starting time of the specific phase.  
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{fig/Traffic_network.png}
    \caption{Traffic network example}
    \label{fig:Traffic network}
\end{figure}

\begin{table}[]
    \centering
    \caption{Notion Table}
    \begin{tabular}{|c|c|}
    \hline
        $\mathcal{V}$ & set of all intersections \\
        \hline
        $\mathcal{E}$ & set of all approaches\\
        \hline
        $Lane[v]$& entering lane of intersection $v$\\ 
        \hline
        $NB_{v_i}$ & neighbourhood intersections of $v_i$\\
        \hline
        $wait[l]$& number of waiting vehicles on lane $l$\\
        \hline
        $wave[l]$& number of vehicles on lane $l$\\
        \hline
        $s_i$ & state of intersection $v_i$\\
        \hline
        $o_i$& observation of agent $i$\\

         \hline
    \end{tabular}

    \label{tab:Notion Table}
\end{table}
\subsection{Markov Game Framework}
Multi-agents system is usually modelled as a Markov Game(MG) \cite{littman1994markov} which is defined as a tuple$\langle \mathcal{N},\mathcal{S},\mathcal{O},\mathcal{A}, R, P,\gamma \rangle$ where $\mathcal{N}$ is the agent space, $\mathcal{S}$ is the state space, $\mathcal{O}=\{\mathcal{O}_1,...,\mathcal{O}_{|\mathcal{N}|}\}$ is the observation space of all agents and $\mathcal{O}_i$ of agent $i$ is observed partially from the state of the system,  $\mathcal{A}=\{\mathcal{A}_1,...,\mathcal{A}_{|\mathcal{N}|}\}$ is the joint action space of all agents, $r_i \in R: \mathcal{O}_i \times \mathcal{A}_1\times \cdots \times \mathcal{A}_{|\mathcal{N|}}\rightarrow \mathbb{R}$ maps a observation-action pair to a real number, $P: \mathcal{S}\times \mathcal{A}_1 \times \cdots \times \mathcal{A}_{|\mathcal{N}|} \times\mathcal{S}\rightarrow [0,1] $ is the transition probability space that assigns a probability to each state-action-state transition and $\gamma$ is the reward discounted factor.

The goal of MG is to find a joint optimal policy $\pi^*=\{\pi^*_1,...,\pi^*_{|\mathcal{N}|}\}$ under which each agent $i$ maximizes its own expected cumulative reward

\begin{equation}
    \mathbb{E}_{\pi_i}[\sum_{k=0}^\infty \gamma^k r_{i,t+k} |\mathcal{S}_t]
\end{equation}
where $\pi_i$ : $\mathcal{O}_i \times \mathcal{A}_i \rightarrow [0,1]$ maps the observation of agent $i$ to the probability distribution of its action. The action-value (Q-value) of agent $i$ is defined as $ Q_i^{ \pi_i }(\mathcal{O}_i,\mathcal{A}_i)=\mathbb{E}_{\pi_i}[\sum_{k=0}^\infty \gamma^k r_{i,t+k} |\mathcal{S}_t,\pi_i(\mathcal{S}_t)]$. 
Tabular Q-learning is a classic algorithm to learn and store action-value \cite{watkins1992q}. 
The update rule is formulated as
\begin{equation}
Q_{i,t+1}(\mathcal{O}_{i,t},\mathcal{A}_{i,t})=Q_{i,t}(\mathcal{O}_{i,t},\mathcal{A}_{i,t})+\alpha(y_t-Q_{i,t}(\mathcal{O}_{i,t},\mathcal{A}_{i,t}))
\end{equation}
where $\alpha$ is the learning rate and
\begin{equation}
    y_t=r_{i,t}+\gamma \max_{a \in \mathcal{A}_i}  Q_t(\mathcal{O}_{i,t},\mathcal{A}_{i,t})
\end{equation}

\subsection{Branching Dueling Q-Network}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/DBDQ.png}
    \caption{Structure of BDQ. Firstly, the states of intersections are concatenated and embedded through fully-connected layers. Then the last hidden layer of shared representation is used for both calculations of advantage values and the state value. Next, through the aggregation layer, Q-values of all action branches are calculated with advantage values and the state value based on Equation \ref{eq: aggregation }. The size of each layer is annotated below.}
    \label{figStructure of Double-BDQ}
\end{figure*}
In some complex tasks in real life such as robotic control, one task might be divided into several sub-tasks and each sub-tasks contain a different number of actions. 
Consequently, the size of the action space grows exponentially and the credit to each sub-action becomes complex. To improve the efficiency of searching and better guide sub-actions to reach a global goal, 
Travakoli et al. proposed a novel agent BDQ \cite{tavakoli2018action} to reduce the output size of the neural network while holding a good convergence property. Suppose an agent controls $K$ intersection and each intersection has $|A_k|$ actions, then the cardinality of the action space of this agent is $|A_d|^k$. The size of the output of DQN grows exponentially while that of BDQ grows linearly(In Figure \ref{figStructure of Double-BDQ}).

As illustrated in Figure \ref{figStructure of Double-BDQ}, the last shared representation layer is first used to compute common state value and then embedded to get advantage values of each action branch independently. 
Then the advantage values of each action branch are further aggregated with state value to calculate the Q-values of each action branch. In \cite{tavakoli2018action}, three ways are proposed to aggregate Q-values and we select the mean advantage aggregation because it has better stability. Formally, suppose there are $K$ action branches and each action branch has $|A_k|$ sub-actions, The Q-value of one sub-action is calculated by the common state value and the advantage of this sub-action over the average performance of all sub-actions in this action branch. That is 
\begin{equation}
    Q_k(s,a_k)=V(s)+(A_k(s,a_k)-\frac{1}{n} \sum_{ a^{'}_{k} \in A_k } (A_k(s,a'_k)))
    \label{eq: aggregation }
\end{equation}

To compute the temporal difference target, we choose the mean operator as they suggested. That is 
\begin{equation}
    y=r+\gamma \frac{1}{N} \sum_d Q^{-}_d(s',\underset{a^{'}_d \in A_d}{\text{arg max}} Q_d(s',a_d'))
    \label{eq: target value compute}
\end{equation}
The loss function is 
\begin{equation}
    L=\mathbb{E}_{(s,a,r,s')\sim D}[\frac{1}{N}\sum_d(y_d-Q_d(s,a_d)^2]
    \label{eq:loss}
\end{equation}

\section{ATSC Formulation}
\label{sec: ATSC formulation}
In this section, we define the formulation of state, action and reward of single intersections with the above notations. In a complete episode with a length of $\mathcal{T}$ time steps, an agent observes the environment and makes actions at every time step $t$. 
\subsection{State Representation}
There are lots of types of state representations in literature such as queue length, waiting time and delay\cite{wei2019colight,wei2018intellilight,arel2010reinforcement}. 
In \cite{wei2019presslight}, vehicle wave on each lane is justified with the ability to fully describe the system while the most commonly used state representation is queue length on each lane. In this paper, we combine the state representation in \cite{chu2019multi,wang2020large} with the signal phase. The formulation of the state of a single intersection $v$ at time step $t$ is defined as
\begin{equation}
s_{v}^t=\{\{wait^{t}[l]\}_{l \in Lane[v]},\{wave^{t}[l]\}_{l \in Lane[v]},phase^t[v]\}
\end{equation} 

\subsection{Action space}
As defined in Section \ref{sec: Traffic Signal Control Definition}, each intersection has four phases. In TSC, two common settings of action for one intersection are "Switch" or "Choose Phase". In the "Switch" setting, one intersection chooses whether to switch to the next predefined phase or to hold the current phase. Therefore, the phase sequence $P=\{p_1,p_2,..\}$ in this setting follows fixed order and only starting time steps are allowed to deviate. In the "Choose Phase" setting, one intersection chooses which exact phase to run in the next time period. Therefore, both phase sequence and starting time steps vary and this setting offers more flexibility. Moreover, the phase sequence in "Switch" setting is a subset of that in "Choose Phase" setting. Thus, the intersection with "Choose Phase" should perform better than the "Switch" setting.
To both improve the travel efficiency and demonstrate the strength of our model in high dimensional action space, the action setting for a single intersection is "Choose Phase" and the action space for intersection $v$ is $a_v=\{NS,NSL,EW,EWL\}$.

\subsection{Reward Design}
The goal of TSC is to improve traffic conditions in a network such as reducing average travel time. However, average travel time is a delayed measurement that can be calculated only after the vehicle completes its travel. In \cite{zheng2019diagnosing}, for a single intersection, using the queue length as the reward is equivalent to minimizing average travel time. In this paper, the reward of a single intersection is defined as
\begin{equation}
    r_v^t=-\sum_{l \in Lane[v]}wait[l]^t
\end{equation}

\section{Regional Control Agent in TSC}
\label{sec: regional control formulation}
In this section, we present our partition rules of region and MADRL training framework. Previous research has demonstrated the benefits of observing either partial or complete states of neighbouring intersections.
In \cite{arel2010reinforcement}, the central intersection has direct access to the states of its neighbourhood intersection. In \cite{zhang2022neighborhood}, the states of adjacent intersections are further weighted with the degree of correlation between corresponding intersections and a similar augmentation method is applied to the calculation of rewards. In \cite{wang2020large}, the states of adjacent intersections are averaged before augmentation. Although agents in the above works cooperate by either implicit or explicit communication, they only maximize their own rewards and non-stationary issue between agents might still limit the performance of agents. Inspired by these works, we want to utilise the best of adjacent intersections. In our work, an agent observes the state of all intersections in the corresponding region and tries to maximise the reward of a region of intersections. 
\subsection{Region Partition Rule}
 A region $I_{v}=\{v \cup NB_{v}\}$ centred at $v$ is a set of intersections including $v$ and its adjacent intersections. One intersection is partitioned into several regions such that $\cup_{v}I_{v}=\mathcal{V}$ and for any two regions $I_{v}$ and $ I_{v} $,$I_{v}\cap I_{v}=\phi$. 
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{fig/partitionexample.png}
    \caption{A four-by-four grid with rows and columns indexed. To best utilize all branches of BDQ, only one imaginary intersection is allowed in one region. The four-by-four grid traffic network is divided into four regions---$I_{13}, I_{21}, I_{34}, I_{42}$.}
    \label{fig:partition example}
\end{figure}

\subsection{Imaginary Intersection}
Our partition rule only involves the adjacency of intersections and we assume each intersection has four adjacent intersections. However, intersections at the boundary of grid have less than four neighbourhoods. As a consequence, regions under our partition rule can not cover a grid traffic network perfectly. To solve this issue, we introduce the imaginary intersection to fill the absence of adjacent intersections. One partition example in a four-by-four grid traffic network is illustrated in Figure \ref{fig:partition example}.
 
\subsection{RL Formulation of Regional agents}
\subsubsection{Observation Representation}
An agent can observe the states of all intersections in its regions. So the observation of an agent $i$ is a concatenation of states of intersections. Formally,
\begin{equation}
    O^t_i=\{s^t_v\}_{v\in I_{i}}
\end{equation}
The state of an imaginary intersection is a vector of zeros.
\subsubsection{Joint Action}
Our agent controls signals of all intersections. So the joint action space of a regional agent $i$ is
\begin{equation}
    A_i=\{NS,NSL,EW,EWL\}^{|I_i|}
\end{equation}
The size of the joint action space grows exponentially so does the size of the output layer in simple DQN or Actor-Critic architectures. With BDQ, the size of the output layer grows linearly. And each action branch corresponds to one particular intersection. If the corresponding intersection of one branch is imaginary, then this action branch is idle. Otherwise, the branch is activated.
\subsubsection{Reward Design}
Similar to \cite{tan2019cooperative}, we assume the reward of a region is the summation of the rewards of its intersections. Regional agents tempt to minimize the waiting queue length of all intersections in the region
\begin{equation}
    R^t_i=\sum_{v \in I_{i}} r^t_v
\end{equation}

\subsection{Dynamic-BDQ in regional signal control}
BDQ was proposed to solve multi-task problems where the control is partially distributed and multi branches of actions work together such as robotic control. Similarly, we can model regional signal control as a multi-task control problem. In a region of $|I|$ intersections, there are $|I|$ action branches and each branch corresponds to one specific intersection. For agent $i$, the Q-value of intersection $k$ and phase $p$ at time step $t$ is $Q_k(o^t_i,p)$.

Since some action branches are not activated, the original Equation  \ref{eq: target value compute} for computing target value using an average of all branches might mislead the estimate of the target value and further deteriorate the performance of agents.

Next, we propose Dynamic-BDQ(DBDQ) in which the computation of the target values in Equation \ref{eq: target value compute} is further modified. Instead of calculating the average of all branches $d$, the Q-value of the next state is the average of activated action branches $\Tilde{d}$
 \begin{equation}
    y_{\Tilde{d}}=r+\gamma \frac{1}{\Tilde{N}} \sum_{\Tilde{d}} Q^{-}_{\Tilde{d}}(s',\underset{a^{'}_{\Tilde{d}} \in A_{\Tilde{d}}}{\text{arg max}} Q_{\Tilde{d}}(s',a_{\Tilde{d}}'))
    \label{eq: target value compute modi}
\end{equation}
Where $\Tilde{N}$ is the number of activated action branches. In the loss function, only errors of activated branches are involved
\begin{equation}
    L=\mathbb{E}_{(s,a,r,s')\sim D}[\frac{1}{\Tilde{N}}\sum_{\Tilde{d}}(y_{\Tilde{d}}-Q_{\Tilde{d}}(s,a_{\Tilde{d}})^2]
    \label{eq:loss modi}
\end{equation}
\marginpar{\textcolor{red}{\scriptsize adaption of training procedure}}
\subsection{Components and Pipeline of Training framework }
In RL, there are two major components-- Environment and agent. Agent receives observations from the environment and returns actions. The environment then moves to the next step and passes transition tuples and the next observation to agents. The architecture of our training is illustrated in  Figure \ref{fig:training framework}. Since the state of the simulator needs further augmented into observation, a Pipeline class is introduced to process data from the simulator and agents. The procedure of Pipeline is listed in Algorithm \ref{alg:pipeline}. As shown in Figure\ref{fig:partition example}, the position of imaginary intersection varies in different regions. Thus, corresponding components of observations are zeros and different action branches are idle. To accelerate convergence and improve generalised ability, we adopt the CLDE paradigm where agents share network parameters and experience memory.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{fig/Training_Framework.png}
    \caption{The training framework of a regional agent. Two major components communicate and exchange data through Pipeline Class. The working flow is separated into two parts. The first part is the interaction between the agent and the environment (Yellow arrows) and the second part is the learning and updating of the agent (Red arrows). In the simulation step of the interaction part, the simulator passes the state to the pipeline and the pipeline then generates observations for agents. Based on observation, the agent chooses joint action $a$ under $\epsilon$-greedy policy and passes the joint action to the pipeline. Finally, the pipeline passes signal phases to simulation and moves to the next simulation step.}
    \label{fig:training framework}
\end{figure}
\begin{algorithm}
    

\caption{Algorithm for Pipeline}\label{alg:pipeline}
\begin{algorithmic}[1]
\State Initialise DBDQ $\theta_i$ and target DBDQ $\theta^-_i\leftarrow\theta_i$
\State Initialise Replay Memory $M_i$, Region Configuration $I$
\While{$i < episode$}
    \State $s\leftarrow$ environment reset
    \State $o_i \leftarrow$ generate observation based on $s,I$
    \While{$t<T$}
        \If{$rand<\epsilon$}
            \State $a_i\leftarrow$ random joint action
        \Else
            \State $a_i\leftarrow \cup_{\Tilde{d}} \underset{a^{'}_{\Tilde{d}} \in A_{\Tilde{d}}}{\text{arg max}}Q_{\Tilde{d}}(o_i,a_{\Tilde{d}}')$ 
        \EndIf
        \State $r,s'\leftarrow$ env step after all agents choose actions
        \State calculate$R_i$ and generate $o_i'$ based on $s',I$
        \State store transition$(o_i,a,R_i,o_i')$ to $M_i$
        \State Update $\theta$ by Eq. \ref{eq:loss} with target value by Eq. \ref{eq: target value compute modi}
        \State $\theta^-\leftarrow (1-\tau)\theta^-+\tau \theta$ for certain step
        \State $o_i\leftarrow o_i'$
        
    \EndWhile

\EndWhile
\end{algorithmic}
\end{algorithm}
\section{Experiments and Results}
\label{sec: sec experiments and results}
In this section, we test our method in both real and synthetic grid and compare it with other novel MADRL frameworks. To show the robustness of our region design, we adopt two region configurations. To evaluate the improvement of DBDQ in dealing with a region with imaginary intersections, we test DBDQ on both our region with one imaginary intersection and $2\times 3$ region with two imaginary intersections.
\subsection{Experiment Scenario}
The traffic simulator CityFlow we used in this paper is an open-source simulator \cite{zhang2019cityflow}. In our experiment, two four-by-four grid---one real(Hangzhou) and one synthetic are used. Their major difference is the distance between adjacent intersections. The distance between two adjacent intersections in the Hangzhou network is 800 meters and that in the synthetic network is 300 meters.
In the Hangzhou network, two traffic flows are applied---one is in flat hours and the other is in peak hours. The data is based on the camera data in Hangzhou and is further simplified. 
In the synthetic network, the flow is generated according to Gaussian distribution. The turning ratio for all flow is distributed as 10\%(left turn), 60\%(straight) and 30\%(right turn). The summarised statistics are listed in Table \ref{tab:Flow statistics}. In the table, the arrival rate of synthetic flow is the highest and both flat flow and peak flow in the real network can test the performance of RL agents in the real world. All these datasets are open-sourced\footnote{https://traffic-signal-control.github.io/\#open-datasets}.

\begin{table}[]
    \centering
        \caption{Flow statistics}
    \begin{tabular}{|c|c|c|}
        \hline
        \multirow{2}*{Flow} & 
        \multicolumn{2}{c|}{Arrival Rate(vehicles/s)} \\ \cline{2-3} 
        \multicolumn{1}{|c|}{} & 
        \multicolumn{1}{|c|}{Mean}&Std  \\ \hline
        (Hangzhou) Flat & 0.83 & 1.33  \\ \hline
        (Hangzhou) Peak & 1.82 & 2.15  \\ \hline
        Synthetic & 3.12 & 4.08  \\ \hline
    \end{tabular}
    \label{tab:Flow statistics}
\end{table}
The overall time span for all flows is 3600s and some vehicles arrive near the end of the time span. To obtain a complete performance, we set the simulator to simulate 4000s to allow more vehicles to arrive at their destinations.
To avoid signals flicking too frequently, all agents perform actions for every $\Delta t=$10s and no yellow phase is inserted between different phases. Afterall, the length $\mathcal{T}$ of one episode is 400. 
\begin{table}[]
    \centering
    \caption{Hyperparameter Summary}
    \begin{tabular}{|c|c|c|}
     \hline
        Component&Hyperparameter &Value  \\
        \hline
         \multirow{7}*{DBDQ}& $\gamma$ &0.99 \\ \cline{2-3} &
                                Learning rate $\alpha$ &0.0001\\\cline{2-3}&
                                Replay Buffer Size&200000\\\cline{2-3}&
                                Network optimizer& Adam\\\cline{2-3}&
                                Activation Function &Relu\\\cline{2-3}&
                                $\tau$ &0.001\\\cline{2-3}&
                                Batch Size &32 \\
                                     \hline
         \multirow{3}*{R-DRL}&$k$ & 128\\\cline{2-3}&
                              $\alpha_{\text{critic}}$&0.0001\\\cline{2-3}&
                               $\alpha_{\text{actor}}$&0.00001\\    
         \hline
         \multirow{3}*{$\epsilon$-greedy Policy}& $\epsilon_{max}$ &1 \\ \cline{2-3}&
                                                  $\epsilon_{min}$ &0.001\\\cline{2-3}&
                                                  decay steps & 20000\\
                          
         \hline
    \end{tabular}
    \label{tab:Hyperparameter}
\end{table}
We compare our agent with the following baselines---Fixed time, NeighbourRL, R-DRL, CoLight\footnote{https://github.com/wingsweihua/colight}, PNC-HDQN and ENC-HDQN\footnote{https://github.com/RL-DLMU/PNC-HDQN}. For DBDQ, the network structure follows the original paper \cite{tavakoli2018action} and is demonstrated in Figure \ref{figStructure of Double-BDQ}. The size of hidden layers of NeighbourRL is the same as DBDQ. For R-DRL, the structures of actor and critic follow the description in \cite{tan2019cooperative}. Although a global critic is proposed to coordinate R-DRL, the convergence of this global critic is not guaranteed. So only independent R-DRL is compared. The hyperparameters for our agent, NeighbourRL and R-DRL are listed in Table \ref{tab:Hyperparameter}. For other baselines, we run the source code for fairness.
\subsection{Metric}
Similarly to \cite{zhang2022neighborhood}, we choose below three metrics to evaluate the performance of agents.
\begin{itemize}
    \item Average Travel Time(ATT): average of all vehicles' travel time. Since the time step of our simulation is large than the arrival time span, all vehicles can travel in the network for enough time and the computation of ATT is more complete and less affected by vehicles which just start their trip
    \item Average Queue Length(AQL): average queue length on each lane of all intersection. The definition of the reward of our agent is the sum of queue length. So AQL is a direct numerical interpretation of reward.
    \item Throughput(TP): number of vehicles which arrive at the destination. While reducing ATT, we also want to increase throughput so that benefits are maximized
\end{itemize}


\subsection{Overall Performance}
\begin{figure*}[htb]
    \centering
    \subfloat[(Hangzhou)Flat]{
    \includegraphics[width=0.3\textwidth]{fig/Hangzhou_low_Episode_Reward_Curve.png}
}
    \hfill
    \subfloat[(Hangzhou)Peak]{
    \includegraphics[width=0.3\textwidth]{fig/Hangzhou_High_Episode_Reward_Curve.png}
    }
    \hfill
      \subfloat[Synthetic]{
    \includegraphics[width=0.3\textwidth]{fig/Synthetic_Episode_Reward_Curve.png}
    }

        \subfloat[Flat after Convergence]{
    \includegraphics[width=0.3\textwidth]{fig/Hangzhou_low_Episode_Reward_Curve_converge.png}
}
    \hfill
    \subfloat[Peak after Convergence]{
    \includegraphics[width=0.3\textwidth]{fig/Hangzhou_High_Episode_Reward_Curve_converge.png}
    }
    \hfill
      \subfloat[Synthetic after Convergence]{
    \includegraphics[width=0.3\textwidth]{fig/Synthetic_Episode_Reward_Curve_converge.png}
    }
    \caption{Learning curve of RL agents. The top three pictures are the overall performance during 2000 episodes and the bottom three pictures are the overall performance after episode 1250}
    \label{fig:reward curve}
\end{figure*}
\begin{table*}
    \centering
        \caption{Numerical Statistics}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
         Flow&Metric&Fixed&NeighbourRL&R-DRL&CoLight&PNC&ENC&OURS \\
         \hline
         \multirow{3}{*}{Flat}&
         ATT&482.19&379.83$\pm$8.37&335.29$\pm$1.65&334.01$\pm$0.95&328.13$\pm$0.49&326.23$\pm$0.59&319.14$\pm$0.43\\&
         AQL&0.57&0.29$\pm$0.031&0.13$\pm$0.01&0.28$\pm$0.34&0.10$\pm$0.0020&0.09$\pm$0.0021&0.07$\pm$0.0016\\
        &TP&2810&2930.79$\pm$7.19&2950.75$\pm$3.81&2899.92$\pm$144.7&2942.9$\pm$3.09&2959.776$\pm$1.60&2963.27$\pm$0.89\\

         \hline
         \multirow{3}{*}{Peak}&
         ATT&803.78&675.01$\pm$60.26&415.75$\pm$4.94&463$\pm$7.11&437.09$\pm$5.84&479.25$\pm$26.14&402.02$\pm$3.00\\
        &AQL&1.8&2.17$\pm$0.21&0.58$\pm$0.04&1.31$\pm$0.37&0.75$\pm$0.03&1.12$\pm$0.23&0.44$\pm$0.01\\
        &TP&5105&5669.4$\pm$226.92&6340.18$\pm$18.28&6034$\pm$370.6&6296.85$\pm$29.96&6222.2$\pm$136.89&6382.19$\pm$10.9\\

         \hline
         \multirow{3}{*}{Synthetic}&
         ATT&548.77&-&216.03$\pm$7.28&246.14$\pm$18.19&228.24$\pm$2.0&224.6$\pm$1.6&206.6$\pm$1.3\\
        &AQL&3.32&-&0.99$\pm$0.1&1.27$\pm$0.2&1.08$\pm$0.02&1.05$\pm$0.02&0.85$\pm$0.016\\
        &TP&9553&-&11173.29$\pm$63.63&11179.2$\pm$201.3&11179.8$\pm$15.67&11215.41$\pm$4.96&11227.93$\pm$1.07\\

         \hline
    \end{tabular}

    \label{tab:performance evaluation}
\end{table*}
In Figure \ref{fig:reward curve}, the reward curves of all RL agents are plotted. Since the definitions of the reward of other baselines involve augmentations, we computed the episode reward as the below equation for consistency.
\begin{equation}
    R= \frac{1}{|\mathcal{V}|} \sum_{t=1}^\mathcal{T}\sum_v r_v^t
\end{equation}
As illustrated in Figure \ref{fig:reward curve}, our agent converged in all three scenarios and the convergence speed was faster than other RL agents except CoLight because CoLight adopted the Monte Carlo training procedure. NeighbourRL failed to converge in the synthetic scenarios and the corresponding curves and numerical data were omitted. From the bottom plots in Figure \ref{fig:reward curve}, our agent achieved the highest reward after convergence. As the volume of vehicles increases, the curves started to oscillate and the curve in synthetic is the most unstable. However, our agent was the most stable one and with the smallest fluctuation. Numerical results are listed in Table \ref{tab:performance evaluation}. 
In the Hangzhou scenario, as the flow density increases, the average travel time becomes larger and the network become crowded. Interestingly, R-DRL performed better during peak hours than other baselines. This is likely because the observation of agents in the flat hour is a sparse vector while that in peak hour contains more complex information on traffic dynamics.
In the synthetic scenario, the average travel time of vehicles is the shortest and the average queue length is the highest because the intersections in the synthetic network are much closer than those in the Hangzhou network and the arrival rate is the highest. 
The average queue length of regional agents in the synthetic scenario almost doubled that in the Hangzhou peak scenario while those of CoLight and ENC even decreased. This is probably caused by two factors: The first is the distance between intersections.
If intersections are close, the travel time between intersections becomes much shorter and it is harder to decrease the queue length. The second is the strategy of agents. Regional agents try to minimise the queue length of a region and some intersections might have to sacrifice individual rewards to achieve better regional rewards. 
The difference in throughput between agents is not significant because simulation steps are large enough for most vehicles to arrive at their destinations. Overall, our agent achieved the best results and standard deviation among all metrics. 
\subsection{Robustness of Our Region}
\begin{figure*}[htb]
    \centering
    \subfloat[(Hangzhou)Flat]{
    \includegraphics[width=0.3\textwidth]{fig/Hangzhou_low_Episode_Reward_Curve_converge_two_config.png}
}
    \hfill
    \subfloat[(Hangzhou)Peak]{
    \includegraphics[width=0.3\textwidth]{fig/Hangzhou_High_Episode_Reward_Curve_converge_two_config.png}
    }
    \hfill
      \subfloat[Synthetic]{
    \includegraphics[width=0.3\textwidth]{fig/Synthetic_Episode_Reward_Curve_converge_two_config.png}
    }
    \caption{Learning curve of two configurations.}
    \label{fig:reward curve two config}
\end{figure*}
As illustrated in Figure \ref{fig:partition example}, we balanced the number of activated branches as much as we can. There are two configurations of regions---one is $I_{13}, I_{21}, I_{34}, I_{42}$ as it is shown in Figure \ref{fig:partition example} and the other configuration is $I_{12}, I_{24}, I_{31}, I_{43}$. To ensure the completeness of the experiment, we compare the performance of agents under both configurations and the learning curves are plotted in Figure \ref{fig:reward curve two config}. Agents under both configurations converge but Config2 has a slower convergence speed in all scenarios. These training curves indicate that the configuration of regions can affect the training process of agents. In different configurations, agents observe different sets of intersections and traffic flows of these intersections are different. The intersections of regions in configuration 1 might share higher correlations. In Table \ref{tab:performance evaluation two config}, although the numerical results of configuration 1 are better than those of configuration 2, there is no significant difference and the numerical results of configuration 2 are still better than the results of other baselines.
\begin{table}[h]
    \centering
        \caption{Numerical Statistics of Two configurations}
    \begin{tabular}{|c|c|c|c|}
    \hline
         Flow&Metric&Config1&Config2 \\
         \hline
         \multirow{3}{*}{Flat}&
         ATT&319.14$\pm$0.43&320.62$\pm$0.48\\&
         AQL&0.07$\pm$0.0016&0.07$\pm$0.0019\\&
         TP&2963.27$\pm$0.89&2963.03$\pm$0.81\\

         \hline
         \multirow{3}{*}{Peak}&
         ATT&402.02$\pm$3.00&406.99$\pm$4.67\\
        &AQL&0.438$\pm$0.01&0.443$\pm$0.009\\
        &TP&6382.19$\pm$10.9&6369.724$\pm$15.2\\

         \hline
         \multirow{3}{*}{Synthetic}&
         ATT&206.6$\pm$1.3&208.27$\pm$1.27\\
        &AQL&0.85$\pm$0.0157&0.86$\pm$0.0162\\
        &TP&11227.93$\pm$1.07&11227.41$\pm$ 1.34\\

         \hline
    \end{tabular}

    \label{tab:performance evaluation two config}
\end{table}
\subsection{Improvement of DBDQ over BDQ and DDPG+WA}
The target value of DBDQ only involves activated branches to remove the influence of imaginary intersections.
\begin{table}[h]
    \centering
        \caption{Improvement by DBDQ}
    \begin{tabular}{|c|c|c|c|}
    \hline
         Flow&Metric&Config1&grid\\
         \hline
         \multirow{3}{*}{Flat}&
         ATT&2.8\%&3.5\%\\&
         AQL&9.7\%&24.1\%\\&
         TP&0.14\%&0.27\% \\
         \hline
         \multirow{3}{*}{Peak}&
         ATT&1.1\%&1.8\%\\
        &AQL&7.8\%&19.3\%\\
        &TP&0.12\%&0.42\%\\
         \hline
         \multirow{3}{*}{Synthetic}&
         ATT&1.7\%&2.4 \%\\
        &AQL&8.5\%&11.8\%\\
        &TP&0.27\%&0.34\%\\
         \hline
    \end{tabular}
    \label{tab:Improvement by DBDQ}
\end{table}
 To show the advantage of DBDQ, we combined BDQ with "Config1" and DBDQ with a $2 \times 3$ grid region in R-DRL. "Config1" has one imaginary intersection and the grid region has two imaginary intersections. Both regions have four activated branches in BDQ and DBDQ. In Table \ref{tab:Improvement by DBDQ}, the performance of both region configurations is improved by DBDQ. For different scenarios, the improvement in the Hangzhou flat scenario is more significant than that in other scenarios. For different region configurations, the improvement of the grid region is more significant than "Config1". It is likely that there are more imaginary intersections in grid regions and DBDQ can estimate a less biased target value.  

\section{Conclusion and Future Work}
In this paper, we proposed a new regional control agent for ATSC in which the region is partitioned by the adjacency of intersections. Meanwhile, we applied DBDQ to search for joint action and to release the negative influence of imaginary intersections. Our region design has the potential to be applied in heterogeneous networks. There are two major advantages of DBDQ. One is that the size of joint action space grows linearly as the number of intersections in the region increase so the search for optimal actions is more efficient. The other is that DBDQ alleviates the influence of imaginary intersections by calculating target values based on activated branches.

We have carried out thorough experiments to evaluate the performance and robustness of our agent. Experiments show that our agent achieves the best performance in both real and synthetic scenarios, especially in scenarios with a high-density flow. Also, we observed that the distance between intersections can limit more or less the learning of agents and their potential. It is interesting that, although the performance of different configurations is very similar, a different configuration can influence the convergence of agents. 
One limitation of our agent is that we modelled agents as independent learners and no explicit cooperation is designed between agents.

In the future, we will focus on the maximum capacity of our region by increasing hops when defining neighbourhoods and it is worthwhile to investigate the influence of the distance between intersections on the same traffic flow. 
\label{sec: conclusion}
\bibliographystyle{IEEEtran}
\bibliography{reference}
\end{document}