

\noindent
Visual perception is important for autonomous driving and decision-making for smarter and sustainable cities. Real-time efficient perception is critical to accelerate these advances. For instance, a single traffic camera captures half a million frames every day or a commuter bus acting as a city sensor captures one million frames every day to monitor road conditions~\cite{christensen2019towards} or to inform public services~\cite{hull2006cartel}. There are thousands of traffic cameras~\cite{vox2015trafficcams} and nearly a million commuter buses~\cite{manybuses} in the United States. It is infeasible to transmit and process visual data on the cloud, leading to the rise of edge architectures~\cite{satyanarayanan2017emergence}. However, edge devices are severely resource constrained and real-time inference requires down-sampling images to fit both latency and memory constraints severely impacting accuracy. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/Geo-Zoom-Teaser.pdf}
    \caption{Geometric cues (black dashed lines) are implicitly present in scenes. Our Perspective based prior exploits this geometry. Our method (a) takes an image and (b) warps them, and performs detection on warped images. Small objects which are (d) not detected when naively downsampled but (e) are detected when enlarged with our geometric prior. Our method (f) uses a geometric model to construct a saliency prior to focus on relevant areas and (g) enables sensing on resource-constrained edge devices.}
    \label{fig:teaser}
    \vspace{-0.2in}
\end{figure}

\noindent
On the other hand, humans take visual shortcuts~\cite{hayes2019scene} to recognize objects efficiently and employ high-level semantics~\cite{hayes2019scene, torralba2006contextual} rooted in scene geometry to focus on relevant parts. Consider the scene in Figure~\ref{fig:teaser} (c), humans can recognize the distant car despite its small appearance (Figure~\ref{fig:teaser} (d)). We are able to contextualize the car in the 3D scene, namely (1) it's on the road and (2) is of the right size we'd expect at that distance. Inspired by these observations, can we incorporate semantic priors about scene geometry in our neural networks to improve detection? 

\noindent
In this work, we develop an approach that enables object detectors to ``zoom'' into relevant image regions (Figure~\ref{fig:teaser} (d) and (e)) guided by the geometry of the scene.  Our approach considers that most objects of interests are present within two planar regions, either on the ground plane or within another plane above the ground, and their size in the image follow a geometric relationship. Instead of uniformly downsampling, we sample the image to enlarge far away regions more and detect those smaller objects.

\noindent
While methods like quantization~\cite{gupta2015deep}, pruning~\cite{han2015learning}, distillation~\cite{chen2017learning} and runtime-optimization~\cite{ghosh2021adaptive} improve model efficiency (and are complementary), approaches exploiting spatial and temporal sampling are key for enabling efficient real-time perception~\cite{huang2017speed, li2020towards}. Neural warping mechanisms~\cite{jaderberg2015spatial, recasens2018learning} have been employed for image classification and regression, and recently, detection for self-driving~\cite{thavamani2021fovea}. Prior work~\cite{thavamani2021fovea} observes that end-to-end trained saliency networks fail for object detection. They instead turn to heuristics such as dataset-wide priors and object locations from previous frames, which are suboptimal. We show that formulation of learnable geometric priors is critical for learning end-to-end trained saliency networks for detection.

\noindent
We validate our approach in a variety of scenarios to showcase the generalizability of geometric priors for detection in self-driving on Argoverse-HD~\cite{li2020towards} and BDD100K~\cite{yu2020bdd100k} datasets, and for traffic-cameras on  WALT~\cite{reddy2022walt} dataset. 
\begin{itemize} \itemsep -0.25em
    \item On Argoverse-HD, our learned geometric prior improves performance over naive downsampling by \green{+6.6 $AP$} and \green{+2.7 $AP$} over SOTA using the same detection architecture. Gains from our approach are achieved by detecting small far-away objects, improving by \green{9.6 $AP_S$} (or $195\%$) over naive down-sampling and \green{4.2 $AP_S$} (or $63\%$) over SOTA.
    \item On WALT, our method detects small objects at image scales where other methods perform poorly. Further, it significantly improves detection rates by \green{10.7 $AP_{S}$} over naive down-sampling and \green{3 $AP_{S}$} over SOTA.
    \item Our approach improves object tracking  (+$4.8\%$ MOTA) compared to baseline. It also improves tracking quality, showing increase of \green{+7.6\% $MT\%$} and reduction of \green{-6.7\% $ML\%$}. 
    \item  Our approach can be deployed in resource constrained edge devices like Jetson AGX to detect $42\%$ more rare instances while being \green{2.2X} faster to enable real-time sensing from buses.
\end{itemize}

