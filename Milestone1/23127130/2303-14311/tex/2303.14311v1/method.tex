\noindent
We describe how a geometric model rooted in the interpretation of a 3D scene can be derived from the image. We then describe how to employ this rough 3D model to construct saliency for warping images and improving detection. 

\subsection{Overview}
\label{sec:overview}

\noindent
Object sizes in the image are determined by the 3D geometry of the world. Let us devise a geometric inductive prior considering a camera mounted on a vehicle. Without loss of generality, assume the vehicle is moving in direction of the dominant vanishing point.

\noindent
We are interested in objects that are present in a planar region (See Figure~\ref{fig:parameterization}) of width $P_{1}P_{2}$ corresponding to the camera view, of length $P_{1}P_{3}$ defined in the direction of the vanishing point. This is the planar region on the ground on which most of the objects of interest are placed (vehicles, pedestrians, etc) and another planar region $Q_{1} ... Q_{4}$ parallel to this ground plane above horizon line, such that all the objects are within this region (e.g., traffic lights).

\noindent
From this simple geometry model, we shall incorporate relationships derived from perspective geometry about objects, i.e., the scale of objects on ground plane is inversely proportional to their depth w.r.t camera~\cite{hoiem2008putting}. 

\subsection{3D Plane parameterization from 2D images}
\label{sec:3dreasoning}

\noindent
We parameterize the planes of our inductive geometric prior. We represent 2D pixel projections $u_{1} ... u_{4}$ of 3D points $P_{1} ... P_{4}$. Assume that the dominant vanishing point in the image is $v = (v_{x}, v_{y})$ and let the image size be $(w, h)$. Consider $u_{1}$ (Figure~\ref{fig:parameterization} (b)). We can define a point on the edge of the image plane,

\vspace{-0.1in}
\begin{equation}
    \label{eq:pleft}
    u_{L} = (0, v_{y} + v_{x}\tan{\theta_{1}})   
\end{equation}

\noindent
$u_{1}$ can expressed as a linear combination of $v$ and $u_{L}$,

\vspace{-0.1in}
\begin{equation}
    \label{eq:p1}
    u_{1} = \alpha_{1} u_{L} + (1 - \alpha_{1}) v
\end{equation}

\noindent
Similarly, for $u_{2}$, we can define $u_{R}$ in terms of $v$ and $\theta_{2}$ and $\alpha_{2}$ while $u_{3}$ and $u_{4}$ are defined like Equation~\ref{eq:pleft} to represent any arbitrary plane in this viewing direction. However, for simplicity, for ground plane we fix them as $(0, h)$ and $(w, h)$ respectively. Consider the planar region $Q_{1} ... Q_{4}$ at height $H$ above the horizon line.  We can similarly define $\theta_{3}$ and $\theta_{4}$ to represent the angles from the horizon in the opposite direction and define $q_{1}$ and $q_{2}$. Again, we set $q_{3}$ as $(0, 0)$ and $q_{4}$ as $(w, 0)$. We now have $4$ points to calculate homographies $H_{plane}$ for both  planes. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/PlaneParameterization.pdf}
    \caption{\textbf{Geometry Of The Two Plane Perspective Prior:}  (a) describes the single view geometry of the proposed two plane prior. Region on the ground plane defined by $P_{1}, ... P_{4}$, and rays emanating from camera $C$ to $P_{i}$ intersect at $u_{1} ... u_{4}$ on the image plane. The vanishing point $v$ maps to $P_{\infty}$. This planar region accounts for small objects on the ground plane. To account for objects that are tall or do not lie on the ground plane, we consider another plane $Q_{1} .. Q_{4}$ above the horizon line. These two planes encapsulate all the relevant objects in the scene. (b) depicts the re-parameterization of the two planes in the 2D image. Instead of representing the planar points $u_{1} ... u_{4}$ as pixel coordinates, we instead parameterize them in terms of the vanishing point $v$, $\theta$'s and $\alpha$ to ease learning.}
    \label{fig:parameterization}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/sideways-top-view.pdf}
    \caption{\textbf{Two-Plane Perspective Prior based Image Resampling:} Consider the scene of car, bus and traffic light from (a) camera view and (b) (simplified) bird's eye view. (c) Saliency function that captures the inverse relationship between object size (in camera view) and depth (bird's eye view is looking at $XZ$ plane from above) can be transferred to the camera view (d), by mapping row $z$ using $H$ (marked by blue arrows). (e) and (f) shows that ground plane severely distorts nearby tall objects while squishing traffic light. (g) and (h) shows that additional plane reduces distortion for both tall objects and objects not on ground plane.}
    \label{fig:saliency-explanation}
\end{figure*}

\noindent
For now, assume $v$ is known. However, we still do not know the values for $\theta$'s and $\alpha$, and we shall learn these parameters end-to-end from task loss. These parameters are learned and fixed for a given scenario in our learning paradigm. Our re-parameterization aims to ease learning of these parameters as we clamp the values of $\alpha$'s to $[0, 1]$ and $\theta$'s to $[-\frac{\pi}{2}, \frac{\pi}{2} ]$. It should be noted that all the operations are differentiable.


\subsection{From Planes to Saliency}
\label{sec:avoiding-cropping}


\noindent
We leverage geometry to focus on relevant image regions through saliency guided warping~\cite{jaderberg2015spatial, recasens2018learning}, and create a saliency map from a parameterized homography using $u_{1} .. u_{4}$ defined earlier. Looking at ground plane from two viewpoints (Figure~\ref{fig:saliency-explanation} (a) and (b)), object size decreases by their distance from the camera~\cite{hoiem2008putting}. We shall establish a relationship to counter this effect and ``sample'' far-away objects on the ground plane more than nearby objects.

\noindent
The saliency guided warping proposed by~\cite{recasens2018learning} operates using an inverse transformation $\mathcal{T}_{S}^{-1}$ parameterized by a saliency map $S$ as follows,

\vspace{-0.1in}
\begin{equation}
    I'(x, y) = W_{\mathcal{T}}(I) = I(\mathcal{T}_{S}^{-1}(x, y))
\end{equation}

\noindent
where the warp $W_{\mathcal{T}}$ implies iterating over output pixel coordinates, using \textbf{$\mathcal{T}_{S}^{-1}$} to find corresponding input coordinates (non-integral), and bilinearly interpolating output color from neighbouring input pixel grid points. For each input pixel $(x, y)$, pixel coordinates with higher $S(x, y)$ values (i.e. salient regions) would be sampled more. 

\noindent
We construct a saliency $S$ respecting the geometric properties that we desire. Let $H_{plane}$ be the homography between the camera view (using coordinates $u_{1} ... u_{4}$) and a bird's eye view of the ground plane assuming plane size to be the original image size $(w, h)$. In bird's eye view, we propose saliency function for a row of pixels $z$ (assuming bottom-left of this rectangle as (0, 0)) as,

\vspace{-0.1in}
\begin{equation}
    \label{eq:saliency}
    S_{bev}(z) = e^{\nu(\frac{z}{h} - 1)}
\end{equation}

\noindent
with a learnable parameter $\nu$ ($> 1$). $\nu$ defines the extent of sampling with respect to depth. 

\noindent
To map this saliency to camera view, we warp $S_{bev}$ via perspective transform $W_{p}$ and $H_{plane}$ (Figure~\ref{fig:saliency-explanation} (c)),

\vspace{-0.1in}
\begin{equation}
    \label{eq:saliency-warps}
    S_{plane} = W_{p}(H^{-1}_{plane}, S_{bev})
\end{equation} 

\noindent
We have defined saliency $S_{plane}$ given $H_{plane}$ in a differentiable manner. Our saliency ensures that objects on the ground plane separated by depth $Z$ are sampled by the factor $e^{\nu \frac{Z}{h}}$ in the image.

\subsection{Two-Plane Perspective Prior}
\label{sec:complete-prior}

\noindent
Ground Plane saliency focuses on objects that are geometrically constrained to be on this plane and reasonably models objects far away on the plane. However, nearby and tall objects, and small objects far above the ground plane are not modelled well. In Fig~\ref{fig:saliency-explanation} (f), \textit{nearby objects above ground plane} (traffic lights), they are highly distorted. Critically, these \textit{same objects when further away} are rendered small in size and appear close to ground (and thus modelled well). Objects we should \textbf{focus} more on are thus the former compared to the latter. Thus, another plane is needed, and direction of the saliency function is reversed to $\hat{S}_{bev}(z) = e^{\hat{\nu}(((h - z)/h) - 1)}$ to account for these objects that would otherwise be severely distorted.

\noindent
To represent the Two-Plane Prior, we represented the planar regions as saliencies. The overall saliency is,

\vspace{-0.1in}
\begin{equation}
    S = S_{ground\_plane} + \lambda S_{top\_plane}
\end{equation}

\noindent
where $\lambda$ is a learned parameter.

\subsection{Additional Considerations}

\noindent
Warping via a piecewise saliency function imposes additional considerations. The choice of deformation method is critical, saliency sampler~\cite{recasens2018learning} implicitly avoids drastic transformations common in other appraoches. For e.g., Thin-plate spline performs worse~\cite{recasens2018learning}, produces extreme transformations and requires regularization~\cite{ehteshami2022salisa}.

\noindent
Fovea~\cite{thavamani2021fovea} observes that restricting the space of allowable warps such that axis alignment is preserved improves accuracy, we adopt the separable formulation of $\mathcal{T}^{-1}$,    

\vspace{-0.1in}
\begin{equation}
    \mathcal{T}_{x}^{-1}(x) = \frac{\int_{x'} S_{x}(x') k(x', x) x'}{\int_{x'} S_{x}(x') k(x, x')}
\end{equation}

\begin{equation}
    \mathcal{T}_{y}^{-1}(y) = \frac{\int_{y'} S_{y}(y') k(y', y) y'}{\int_{y'} S_{y} (y') k(y, y')}
\end{equation}

\noindent
where $k$ is a Gaussian kernel. To convert a saliency map $S$ to $S_{x}$ and $S_{y}$ we marginalize it along the two axes. Thus entire rows or columns are ``stretched'' or ``compressed''.

\noindent
Two-plane prior is learnt end to end as a learnable image warp. For object detection, labels need to be warped too, and~\cite{recasens2018learning}'s warp is invertible. Like~\cite{thavamani2021fovea}, We employ the loss $\mathcal{L}(\mathcal{T}^{-1}(f_{\phi}(W_{\mathcal{T}}(I)), L)$  where $(I, L)$ is the image-label pair and omit the use of delta encoding for training RPN~\cite{ren2015faster} (which requires the existence of a closed form $\mathcal{T}$), instead adopting GIoU loss~\cite{rezatofighi2019generalized}. This ensures $W_{\mathcal{T}}$ is learnable, as $\mathcal{T}^{-1}$ is differentiable.


\noindent
We \textit{did not} assume that the vanishing point is within the field of view of our image, and our approach places no restrictions on the vanishing point. Thus far, we explained our formulation while considering a single dominant vanishing point, however, multiple vanishing points can be also considered. Please see supplementary for more details.


\subsection{Obtaining the Vanishing Point}

\noindent
We now describe how we obtain the vanishing point. Many methods exist with trade-offs in accuracy, latency and memory which which inform our design to perform warping efficiently with minimal overheads.

\noindent
\textbf{Fixed Cameras:} In settings like traffic cameras, the camera is fixed. Thus, the vanishing point is fixed, and we can cache the corresponding saliency $S$, as all the parameters, once learnt, are fixed. We can define the vanishing point for a camera manually by annotating two parallel lines or any accurate automated approach. Saliency caching renders our approach extremely efficient.

\noindent
\textbf{Autonomous Navigation:} 
Multiple assumptions simplify the problem. We assume that there is one dominant vanishing point and a navigating car is often moving in the viewing direction. Thus, we assume that this vanishing point lies inside the image, and directly regress $v$ from image features using a modified coordinate regression module akin to YOLO~\cite{redmon2016you, liu2020d}. This approach appears to be memory and latency efficient. Other approaches, say, using parallel lane lines~\cite{abualsaud2021laneaf} or inertial measurements~\cite{camposeco2015using} might also be very efficient. An even simpler assumption is to employ the average vanishing point, as vanishing points are highly local, we observe this is a good approximation.

\noindent
\textbf{Temporal Redundancies:} In videos, we exploit temporal redundancies, the vanishing point is computed every $n_{v}$ frames and saliency is cached to amortize latency cost.

\noindent
\textbf{General Case:} This is the most difficult case, and many approaches have been explored in literature to find all vanishing points. Classical approaches~\cite{tardif2009non} while fast are not robust, while deep-learned approaches~\cite{zhou2019neurvps, lin2022deep, liu2021vapid} are accurate yet expensive (either in latency or memory). 


\subsection{Learning Geometric Prior from Pseudo-Labels}
\label{sec:pseudo-labels}

\noindent
Prior work~\cite{thavamani2021fovea} have shown improvements in performance on pre-trained models via heuristics, which didn't require any training. However, their method \textit{still employs domain-specific labels} (say, from Argoverse-HD) to generate the prior. Our method can't be used directly with pre-trained models as it learns geometrically inspired parameters end-to-end. However, domain-specific images without labels can be exploited to learn the parameters.

\noindent
We propose a simple alternative to learn the proposed prior using pre-trained model without requiring additional domain-specific labels. We generate pseudo-labels from our pre-trained model inferred at 1x scale. We fine-tune and learn our warp function (to 0.5x scale) end-to-end using these ``free'' labels. Our geometric prior shows improvements without access to ground truth labels. 

