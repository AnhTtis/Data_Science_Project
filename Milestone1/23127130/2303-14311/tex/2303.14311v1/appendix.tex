\section{Implementation, Training and Evaluation Details}

\subsection{Implementation Details}

\noindent
We implemented our approach using Pytorch~\cite{pytorch} and mmdetection~\cite{mmdetection}. The two-plane perspective prior is implemented as a neural network layer with learnable parameters that are global (fixed warps parameterized by vanishing point; Figure~\ref{fig:implementation}). We employ differentiable versions of Direct Linear Transform and $warp\_perspective$ from Kornia~\cite{kornia}, while we reuse implementation of separable neural warps from~\cite{thavamani2021fovea}. To detect vanishing points, we employ NeurVPS~\cite{zhou2019neurvps} for fixed cameras. We use VPNet~\cite{liu2020d} with ResNet18 backbone for autonomous navigation.

\noindent
\textbf{Parameter Initialization} We selected one representative image, and initialized the learnable parameters (i.e. $\theta$'s and $\alpha$'s) via visual inspection. The guiding principal was to enlarge far objects while trying to distort the close-by objects as less as possible. The same initial parameters are used for all the datasets. Please look at our code for the initial parameters.

\subsection{Evaluation Details}

\noindent
\textbf{Detection Model Choice} We experiment with Faster R-CNN as our base detection model as prior work has shown it occupies the optimal sweet-spot~\cite{li2020towards} w.r.t latency and accuracy on modern GPUs. However, our approach is agnostic to the choice of detector and our results generalize to other detectors.

\noindent
\textbf{Latency:} We capture end-to-end latency in milliseconds, that includes image pre-processing, network inference and post-processing, following protocol from prior work~\cite{thavamani2021fovea}.

\noindent
\textbf{Scale} The image down-sampling factor is equal in both spatial dimensions. So an image originally $1920\times1200$ (1x scale) when down-sampled to 0.25x is a $480\times300$ image. 


\subsection{Training Details}

\noindent
For training our proposed approaches, to train the Faster R-CNN model we use the Adam optimizer with a learning rate of $3\times10^{-4}$. For training any methods by Fovea~\cite{thavamani2021fovea} we follow their protocol. We follow the protocol mentioned by~\cite{ghosh2021adaptive, chin2019adascale} for training their approach. 

\noindent
\textbf{Argoverse-HD} We considered the same base architecture (Faster R-CNN) for all the methods. We compare with SOTA~\cite{thavamani2021fovea} using models provided by their public code release, and follow the training protocol prescribed in their work, training our models for 3 epochs.

\noindent
\textbf{WALT} We considered the same base architecture (Faster R-CNN) for all the methods. We trained the models (and learnt the warping function parameters, if applicable) using the Adam optimizer with the same learning rate and other parameters for 6 epochs.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{suppl/MultiVP.pdf}    
    \caption{\textbf{Multiple Vanishing Points:} Saliency from \red{First Vanishing Point} parallel to sidewalk ``compresses'' far away cars on perpendicular road. \blue{Second Vanishing Point} ensures those cars are not compressed. Please zoom in to observe the vanishing points and deformation.}
    \label{fig:multivps}
\end{figure*}

\noindent
\textbf{Vanishing Point Estimation} For NeurVPS, we directly employ the pre-trained model trained on Natural Scenes (TMM17) dataset~\cite{zhou2017detecting} part of their public code release. While for VPNet~\cite{liu2020d}, as there is no public code release, we implement this architecture employing a ResNet18 backbone attached to a modified YOLO head. We omit the upsampling  refinement procedure described in~\cite{liu2020d}, as model's median error in vanishing point prediction is around 10 pixels with an average latency of 28 ms, which is sufficient for our method to work. The off-the-shelf model is executed at $n_{v} = 30$ to amortize the cost of executing this model. We also tried using LaneAF~\cite{abualsaud2021laneaf} to obtain lane lines (similar latency), however, we observed the method was prone to errors while clustering lines and obtaining the vanishing point.

\section{Multiple Vanishing Points}

\noindent Our method can consider additional planes that correspond to lines meeting at a different vanishing point. For example, a traffic camera with a wide field of view that is placed at an intersection observing two roads simultaneously would benefit from this. Assuming $N$ vanishing points, considering Saliency $S_{v_{i}}$ corresponding to vanishing point $v_{i}$,

\begin{equation}
    S = \sum_{i = 0}^{N} \lambda_{i} S_{v_{i}}
\end{equation}

where $\lambda_{i}$'s are learnable, initialized as $\frac{1}{N}$. Please observe the case of $N=2$ in an image from the commuter bus dataset in Figure~\ref{fig:multivps}, wherein combining saliencies from two vanishing points (obtained from~\cite{lin2022deep}) ensures far away objects of interest are sampled more. 

We observed in the datasets we considered, multiple vanishing points were rare as it generally requires a camera with a large field of view. Thus, we employed models~\cite{zhou2019neurvps, liu2020d} trained on Natural Scenes dataset, which predict only one vanishing point. However, vanishing points can be estimated from other methods~\cite{zhou2019neurvps, lin2022deep, liu2021vapid} which do predict all the vanishing points, but incur higher overheads.

\section{Results on Another Detector}

\noindent
Adaptive spatial sampling mechanisms leverage and exploit priors corresponding to the input images in a way that is agnostic to the detection method. We expect our approach to generalize across detectors, similar to observations by such warping mechanisms and saliency priors proposed earlier~\cite{thavamani2021fovea}. We choose RetinaNet~\cite{lin2017focal}, a popular single-stage object detector as our archetypal example (Faster R-CNN~\cite{ren2015faster} is the two-stage archetype). Results can be viewed in Table~\ref{table:appendix-alternate-detector}. Our approach improves upon both the baseline Faster R-CNN and SOTA, specially for small and medium sized objects, following the trends observed in the main manuscript.

\begin{table}[]
{\footnotesize
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{llllllll}
\toprule
Method                                    & Scale & $AP$ & $AP_S$ & $AP_M$ & $AP_L$ \\
\midrule
RetinaNet & 0.5x & 22.6 & 4.0 & 22.0 & 53.1 \\
Fovea ($S_{I}$)~\cite{thavamani2021fovea}                               & 0.5x & 24.9 & 7.1 & 27.7 & 50.6 \\
Two-Plane Prior & 0.5x & \textbf{26.3} & \textbf{10.1} & \textbf{29.2} & 50.5 \\
\midrule
\multicolumn{6}{c}{Baseline at higher scales} \\
\midrule
RetinaNet & 0.75x & 29.9 & 9.7 & 32.5 & 54.2 \\
\bottomrule
\end{tabular}
\caption{\textbf{Alternate Detector:} We replace Faster R-CNN with RetinaNet (archetypal one-stage detector), and observe considerable improvements over Baseline (RetinaNet with uniform downsampling) and SOTA trained on Argoverse-HD dataset.} 
\label{table:appendix-alternate-detector}
}
\end{table}


\begin{table}[]
{\footnotesize
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{llllllll}
\toprule
Method                                    & Scale & Model & $AP$ & $AP_S$ & $AP_M$ & $AP_L$ \\
\midrule
Faster R-CNN                               & 0.5x & COCO  & 15.3 & 1.1 & 12.5 & 40.5 \\
Faster R-CNN                               & 0.5x & AVHD  & 15.1 & 1.0 & 10.6 & 39.0 \\
\midrule
Fovea ($S_{D}$)~\cite{thavamani2021fovea}  & 0.5x & AVHD  & 13.7 & 1.3 & 10.0 & 34.7 \\
Fovea ($S_{I}$)~\cite{thavamani2021fovea}  & 0.5x & AVHD  & 16.4 & 2.1 & 12.8 & 38.6 \\
% \midrule
% Fovea ($S_{D}$)~\cite{thavamani2021fovea} (BDD Sal.)  & 0.5x & AVHD &  & & &  \\ 
\midrule
Two-Plane Prior (Psuedo.)                  & 0.5x & AVHD  & 16.2 & \textbf{4.7} & \textbf{15.9} & 33.3 \\
Two-Plane Prior (Psuedo.)                  & 0.5x & COCO  & \textbf{20.9} & \textbf{5.8} & \textbf{19.4} & \textbf{44.2} \\
\midrule
\multicolumn{7}{c}{Baseline at higher scales} \\
\midrule
Faster R-CNN  & 0.75x                      & AVHD & 19.7 & 3.0 & 16.1 & 44.2 \\
Faster R-CNN & 0.75x                       & COCO  & 20.3 & 3.7 & 18.2 & 45.3 \\
Faster R-CNN  & 1x                         & AVHD & 22.6 & 5.7 & 20.1 & 45.7 \\
Faster R-CNN  & 1x                         & COCO  & 23.1 & 6.5 & 21.7 & 46.1 \\

\bottomrule
\end{tabular}
\caption{\textbf{Generalization to BDD100K:} Scale in this case is fixed to 0.5x, AVHD refers to Argoverse-HD and COCO datasets respectively. AVHD models are finetuned from the pre-trained COCO model. We compare generalization on the BDD100K dataset. Our method assumes availability of training set images of BDD100K \textbf{and not labels}, we generate pseudo-labels from the available model (Section 3.6) to learn the Two Plane prior.} 
\label{table:appendix-generalization-BDD100K}
}
\end{table}

% \section{Results with another backbone}

% \noindent
% \blue{Changing the backbone makes things more accurate.}

% \section{Some Modern Detectors}

% \noindent
% \blue{The method also improves accuracy of modern heavy detectors.}


\begin{table}[]
{\footnotesize
\centering
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{lllll}
\toprule
Method                              & $sAP$ & $sAP_S$ & $sAP_M$ & $sAP_L$ \\
\midrule
StreamYOLO-L~\cite{yang2022streamyolo} & 25.9 & 8.6 & 24.2 & 40.9 \\
StreamYOLO-M~\cite{yang2022streamyolo} & 25.9 & 9.2 & 24.8 & 41.0 \\
StreamYOLO-S~\cite{yang2022streamyolo} & 29.6 & 11.0 & 30.9 & 51.6 \\
\midrule
 Ours      & \textbf{30.0}       &  \textbf{13.7}        &  \textbf{31.5}     &   \textbf{52.2}     \\
\bottomrule
\end{tabular}
\caption{{\textbf{``Real-Time'' Detectors}:} Streaming Comparison on Argoverse-HD on Titan X. StreamYOLO-M and StreamYOLO-L single-frame latency is \red{45.8 ms} and \red{62.9 ms} respectively, is greater than 33ms, violating~\cite{yang2022streamyolo}'s ``real-time'' restriction. StreamYOLO-S satisfies (\green{20.8 ms}), hence has better performance.}
\label{table:appendix-streamyolo}
}
\end{table}

\section{Additional Results on Autonomous Driving}

\begin{table*}[t!]
{\footnotesize
\centering
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{lllllllllllllllll}
\toprule
& Method            & Scale   & $AP$   & $AP_{50}$ & $AP_{75}$ & $AP_S$ & $AP_M$ & $AP_L$ & person & mbike & tffclight & bike & bus  & stop & car  & truck \\
\midrule
& Faster R-CNN (Pre.) & 0.5x    & 21.5 & 35.8    & 22.3    & 2.8 & 22.4 & 50.6 & 20.8 & 9.1 & 13.9 & 7.1 & 48.0 & 16.1 & 37.2 & 20.2 \\
& Faster R-CNN               & 0.5x    & 24.2 & 38.9    & 26.1    & 4.9    & 29.0   & 50.9   & 22.8   & 7.5   & 23.3      & 5.9  & 44.6 & 19.3 & 43.7 & 26.6 \\
\midrule
% Fovea ($S_{D}$)~\cite{thavamani2021fovea} (Pre.)   & 0.5x &  23.3 & 40.0 & 22.9 & 5.4 & 25.5 & 48.9 & 20.9 & 13.7 & 12.2 & 9.3 & 50.6 & 20.1 & 40.0 & 19.5 \\
% Fovea ($S_{I}$)~\cite{thavamani2021fovea} (Pre.)  & 0.5x  & 24.1 & 40.7 & 24.3 & 8.5 & 24.5 & 48.3 & 23.0 & 17.7 & 15.1 & 10.0 & 49.5 & 17.5 & 41.0 & 19.4 \\
% Fovea (L:$S_{I}$)~\cite{thavamani2021fovea} (Pre.) & 0.5x & 24.0 & 40.5 & 24.3 & 7.4 & 26.0 & 48.2 & 22.5 & 14.9 & 14.0 & 9.5 & 49.7 & 20.6 & 41.0 & 19.9 \\
& Fovea (Learned Nonsep.)~\cite{thavamani2021fovea} & 0.5x & 25.9  & 42.9  & 26.5  & 10.0  & 28.4 & 48.5  & 25.2  & 11.9  & 20.9  & 7.1  & 39.5  & 25.1 & 49.4  & 28.1 \\
& Fovea (Learned Sep.)~\cite{thavamani2021fovea} & 0.5x & 27.2 & 44.8 & 28.3 & 12.2 & 29.1 & 46.6 & 24.2 & 14.0 & 22.6 & 7.7 & 39.5 & 31.8 & 50.0 & 27.8 \\
\midrule
\parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{SOTA}}} & Fovea ($S_{D}$)~\cite{thavamani2021fovea}   & 0.5x          & 26.7 & 43.3    & 27.8    & 8.2    & 29.7   & 54.1   & 25.4   & 13.5  & 22.0      & 8.0  & 45.9 & 21.3 & 48.1 & 29.3 \\
& Fovea ($S_{I}$)~\cite{thavamani2021fovea}   & 0.5x            & 28.0 & 45.5    & 29.2    & 10.4   & 31.0   & \textbf{54.5}   & 27.3   & 16.9  & \textbf{24.3}      & 9.0  & 44.5 & 23.2 & 50.5 & 28.4 \\
& Fovea (L:$S_{I}$)~\cite{thavamani2021fovea}   & 0.5x           & 28.1 & 45.9    & 28.9    & 10.3   & 30.9   & 54.1   & 27.5   & \textbf{17.9}  & 23.6      & 8.1  & 45.4 & 23.1 & 50.2 & 28.7 \\
\midrule
\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{Ours}}}
 & Ground Plane Prior  & 0.5x       & \textbf{29.1} & \textbf{46.2}    & \textbf{30.5}    & \textbf{15.5}   & 27.5   & 48.3   & \textbf{28.6}   & 15.0  & 10.4      & \textbf{10.5} & 33.7 & \textbf{46.2} & \textbf{55.4} & \textbf{32.9} \\
& Two-Plane Pr. (Psuedo.) & 0.5x & 27.1 & 43.4    & 28.4    & 9.8    & 28.9   & 50.2   & \textbf{28.2}   & 16.2  & 20.6      & 8.1  & \textbf{50.8} & 26.1 & 45.2 & 21.7 \\
& Two-Plane Pr. (Avg VP) & 0.5x & \textbf{29.6} & \textbf{45.9}    & \textbf{31.6}    & \textbf{12.7}   & \textbf{30.7}   & 52.7   & \textbf{28.4}   & 14.3  & \textbf{24.3}      & \textbf{11.9} & 38.5 & \textbf{31.6} & \textbf{53.9} & \textbf{34.2} \\
& Two-Plane Prior   & 0.5x          & \textbf{30.8} & \textbf{47.2}    & \textbf{33.2}    & \textbf{14.5}   & \textbf{31.6}   & 52.9   & \textbf{30.0}   & 16.7  & 24.1      & \textbf{13.7} & 35.9 & \textbf{35.9} & \textbf{55.3} & \textbf{35.3} \\
\midrule
\multicolumn{17}{c}{Baseline at higher scales} \\
\midrule
& Faster R-CNN  & 0.75x         & 29.2 & 47.6    & 31.1    & 11.6   & 32.1   & 53.3   & 29.6   & 12.7  & 30.8      & 7.9  & 44.1 & 29.8 & 48.8 & 30.1 \\
& Faster R-CNN & 1.0x         & 33.3 & 53.9    & 35.0    & 16.8   & 34.8   & 53.6   & 33.1   & 20.9  & 38.7      & 6.7  & 44.7 & 36.7 & 52.7 & 32.7 \\
\bottomrule
\end{tabular}
\caption{\textbf{Evaluation on the Argoverse-HD dataset:} This is an expanded version of the Table present in the manuscript. We see improvements for most of the objects that are on the ground, with much better overall performance for both small and medium sized objects along with improvements in $AP_{50}$ and $AP_{75}$. Notice that Two-Plane prior performs at par with SOTA on ``traffic-light" category. Pre. denotes Pretrained model, while Psuedo. denotes model trained with Psuedo-Labels from pretrained model (no access to Argoverse-HD labels) as described in Section 3.6.  \textit{We have bolded all of our method variations that perform better than SOTA.} }
\label{table:appendix-baselinekiller}
}
\end{table*}


\noindent We shall consider the comparisons made in Section 5 on Argoverse-HD in the main manuscript and present some additional results (specially across different categories present in the dataset).

\noindent
\textbf{Comparison with Learned Fovea~\cite{thavamani2021fovea}:} Fovea~\cite{thavamani2021fovea} also proposed end-to-end global, dataset-wide saliency map $S$ learned via backpropagation (Learned Seperable and Learned Nonseperable). However, they observed worse performance compared to their bounding box priors ($S_{D}$ and $S_{I}$), see Table~\ref{table:appendix-baselinekiller}. We show that end-to-end learned saliency is better, with careful geometric parameterization.

\noindent
\textbf{Improved Performance on ground plane:} We observe improved performance over state-of-the-art on every object category for objects on the ground plane (person, traffic light, bike, stop-sign, car, truck) apart from motorbike (See Table~\ref{table:appendix-baselinekiller}). On further observation, this might be an artifact of the label skew of the Argoverse-HD dataset ($mbike$ has the least number of instances).  For objects not on the ground plane, like traffic-light, we observe performance as good as SOTA.

\noindent
\textbf{Generalization to BDD100K:} We compare generalization from approaches trained on Argoverse-HD and COCO datasets to BDD100K MOT dataset (See Table~\ref{table:appendix-generalization-BDD100K}). Our approach assumes access to training set images from BDD100K dataset, but not it’s ground truth labels. As our priors can be learnt without access to ground truth data, we employ the method detailed in Section 3.6 to generate pseudo labels to learn and adapt the geometric parameters on this dataset by training on these pseudo-labels for 1 epoch only. 

We observe that our method nearly matches SOTA when adapted starting from a model finetuned on Argoverse-HD, however, dramatically exceeds it’s performance when adapted from a model solely trained on COCO. We believe the reason for this mismatch is due to catastrophic forgetting~\cite{kemker2018measuring} observed in finetuned models when evaluated on out-of-distribution data. Lastly, the results indicate the benefits of learnability of our perspective prior, we observe increase in performance for ``free’’ even when images are available without access to ground truth.

\noindent 
\textbf{Comparison with ``Real-Time'' Detectors:} Real-time detectors like~\cite{yang2022streamyolo} have been recently proposed which predict boxes $G_{t+1}$ at time $t$ (of frame $F_{t+1}$; available solely during training and not testing) given $F_{t}$ to satisfy sAP. Approaches like these constraint the detector to perform the computation within a latency budget ($<33$ms or 30 FPS). Our methods are complementary to such detectors, as long as \textit{their} constraint is satisfied.

However, real-time detectors (termed as ``fast'' strategy) might be suboptimal~\cite{li2020towards}. Satisfying the real-time detector constraint may not be optimal for every hardware platform, specially on slower edge devices. Such methods~\cite{yang2022streamyolo} are \textbf{not} hardware-agnostic, and model architecture choices are optimized for specific hardware (in their case, for a V100 GPU). On Titan X (See Table~\ref{table:appendix-streamyolo}), their streaming performance (which is hardware dependent) is worse.

\section{Tracking Smaller Objects for Longer}

\noindent We provide an analysis of our approach observing how it improves object tracking. We wish to observe if the gains from our method translates to detecting far-away objects for longer period of time. We employ Argoverse-HD dataset for our experiments which have ground truth object IDs.

\noindent
\textbf{Setup:} We employ a Faster R-CNN as our baseline and the tracker is fixed to IOU Tracker~\cite{bochinski2017high}. We additionally pair the priors proposed by Fovea~\cite{thavamani2021fovea} for comparison. All the detectors are executed at 0.5x scale for fair comparison.

\begin{figure*}
    \centering
    \includegraphics[width=0.49\linewidth]{suppl/track_viz/2_baseline.png}
    \includegraphics[width=0.49\linewidth]{suppl/track_viz/2_ours.png}
\caption{\textbf{Tracking Visualization:} To visualize the impact of our two-plane prior, we visualize tracks of length greater than 150 frames tracked by both the methods for a given sequence. We plot object size w.r.t frame numbers (which denotes length). We can observe that some objects are detected earlier and are tracked for a longer time.}
    \label{fig:appendix-track-viz-1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.49\linewidth]{suppl/track_viz/6_baseline.png}
    \includegraphics[width=0.49\linewidth]{suppl/track_viz/6_ours.png}
\caption{\textbf{Tracking Visualization:} To visualize the impact of our two-plane prior, we visualize tracks of length greater than 150 frames tracked by both the methods for a given sequence. We plot object size w.r.t frame numbers (which denotes length). The severe drops of the object sizes for some tracks correspond to nearby object overtaken by our vehicle. We can observe that some objects are detected earlier and are tracked for a longer time.}
    \label{fig:appendix-track-viz-2}
\end{figure*}

\noindent
\textbf{Tracking Visualizations:} We present some tracking visualization in Figures~\ref{fig:appendix-track-viz-1} and~\ref{fig:appendix-track-viz-2}. These visualizations motivate us to define the following metrics.

\noindent
\textbf{Detecting and Tracking for Longer:} We wish to understand if Two-Plane Prior is able to detect an object for a longer lifespan. This is important in autonomous driving situations, wherein we want to detect far-away objects as quickly as possible or any object moving away from us. 

\noindent
Prior tracking quality metrics such as MT\% and ML\% check the ratio of tracks that are mostly tracked or mostly lost. However, this does not capture the track length improvements. We propose to compare the average extension of a track (\textbf{ATE}) compared to the baseline detection method. Given a track $\tau$, $E_{\tau}$ can be positive or negative, and is given by,

\begin{equation}
    E_{\tau}(m, b, gt) = (L_{m} - L_{b}) / L_{gt}
\end{equation}

where $m$ is the method, $b$ is baseline and $gt$ is the ground truth track, while $L$ denotes track length. \textbf{ATE} is the average over tracks across all sequences. However, as the metric weighs all tracks equally, which is unfair for extremely small track lengths, thus, we only consider ground truth tracks which are atleast 5 seconds or 150 frames long.

\noindent
\textbf{Detecting and Tracking Smaller Objects:} Given a track, we wish to observe if Two-Plane Prior is able to detect an object when it's ``smaller'' compared to other methods. This is important in autonomous driving situations, wherein we would like to detect further away objects, which would appear smaller. We wish to compute the minimum object size tracked (\textbf{MOS}). We employ a proxy for object size, $size(x) = log(area(x))$ where $x$ denotes an object bounding box, as the area quadratically increases. For a given ground truth track $\tau$, let $o_\tau$ denote minimum object size of an object, while $O_\tau$ denotes maximum object size. Let $c_\tau$ denote the minimum object size in the predicted track currently considered. We can write,

\begin{equation}
    M_\tau = \frac{c_\tau - o_\tau}{O_\tau - o_\tau}
\end{equation}

$M_\tau$ is averaged over all tracks across all sequences to obtain \textbf{MOS}.


\section{Detection on the Commuter Bus}

\noindent
The bus is equipped with a Jetson AGX edge device. The edge device communicates with a modified onboard-NVR recording bus data from 7 cameras, two inside the bus and five on the outside of the bus. The cameras record data at $5 FPS$ at 720P resolution for $8$ working hours of the bus, totalling 1.08 million frames everyday. It is not feasible to transmit and process this data on the cloud due to bandwidth and compute limitations, and privacy concerns. Thus, the edge device and the NVR are part of a distributed edge-cloud infrastructure wherein the edge device is employed to process these simultaneous streams, only relevant frames are transmitted to cloud machines where we do further offline analysis.

\noindent
We analyze bus streams to build an actionable map of public infrastructure, for instance, which areas need a trash pickup or where does snow needs to be shovelled. We also provide real-time feedback to the bus driver, informing them of people who may need assistance (say, on wheelchairs, or with a stroller or service animal) getting on the bus. Thus we employ an object detector to detect trash cans, garbage bags and people with an assistive device. Our system has to operate at near real-time on all streams simultaneously, rendering cloud-transmission-turn-around infeasible.

\begin{table}[]
{\footnotesize
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lllllllll}
\toprule
Method                                    & Scale & $AP_{50}$ & $AR$ & $AR_S$ & $AR_M$ & $AR_L$ & Latency (ms) \\
\midrule
Faster R-CNN                               & 0.5x & 45.0 & 31.7 & 0.5 & 37.3 & 46.9 & $154 \pm 8.5$ \\
Two-Plane Prior                  & 0.5x & \textbf{77.2} & \textbf{61.7} & \textbf{16.4} & \textbf{69.9} & \textbf{68.7} & $158 \pm 7.5$\\
\midrule
Faster R-CNN                               & 0.75x & 58.6 & 41.1 & 10.5 & 45.3 & 54.7 & $240 \pm 8.5$ \\
Two-Plane Prior                  & 0.75x & \textbf{84.5} & \textbf{68.3} & \textbf{38.8} & \textbf{72.9} & \textbf{73.3} & $245 \pm 10$ \\
\midrule
\multicolumn{8}{c}{Baseline at higher scales} \\
\midrule
Faster R-CNN  & 1x                         & 68.2 & 41.5 & 16.9 & 47.5 & 57.1 &  $350 \pm 15$ \\
\bottomrule
\end{tabular}
\caption{\textbf{Rare Object Detection on the Commuter Bus:} We compare our approach with a baseline Faster R-CNN. We observe improved precision and recall over the baseline, specially for small and medium sized objects. Do note, for $\approx$1FPS throughput over five simultaneous streams, average latency of $200$ms should be achieved (however, this is not an enforced latency budget for streaming perception~\cite{li2020towards}). } 
\label{table:appendix-bus-data}
}
\end{table}

\noindent
As we employ the edge device to filter out relevant frames, detecting all the objects in the scene is more important than the precision and localization accuracy (a frame once, marked ``relevant'', is sent to cloud where we employ larger models at higher resolutions without constraints).

\noindent
\textbf{Dataset Acquisition:} For research purposes, we do record all the data\footnote{\href{https://what-if.xkcd.com/31/}{Transmission is infeasible, HDD's swapped physically.} (\href{https://en.wikipedia.org/wiki/Sneakernet}{Sneakernet})}, which is humongous ($\approx$30 Terabytes till now) and the instances are rare, we were able to identify 3.5K such frames (temporally subsampled to 750) through a semi-automatic method. Firstly, we only sampled frames from the camera that is facing the sidewalk (people entering the bus are visible). We then geo-fenced images from bus-stop locations and major intersections on the bus route reducing the set to 780K images. Then, we employ off-the-shelf Detic Swin-B Large Faster R-CNN with CLIP (for custom vocabulary)~\cite{zhou2022detecting} and find images with "wheelchair", "stroller", "walker", "crutches", "cane", "dog", "animal", "trolley", "cart", "trash can", "garbage bin", "garbage", "garbage bag" categories with a confidence threshold of 0.25. This model has a high false positive rate for these rare classes, and we were able to automatically filter a set of 21K images, and manually filtered these to yield 3.5K images. As many of these images were part of dense temporal sequences, we further sub-sampled temporally within each sequence yielding 750 samples. We manually annotated these images with object bounding boxes and categories ("trash-can", "garbage-bag" and "person-requiring-assistance"; labels from Detic~\cite{zhou2022detecting} were not accurate). As the data is recorded over the course of a year, we split the train and test test (70\% - 30\%) using the date stamp (images taken on the same day are in the same split) so that the model doesn't overfit. 

\noindent
\textbf{Hardware Platform:} We set the Jetson AGX to consume 30+ Watts (MAXN configuration; no power budget). Memory is measured using the \texttt{tegrastats} utility, while we use Jetpack 4.6.1 and pytorch 1.6, mmdetection 2.7 (+ mmcv 1.15) compiled for Jetson AGX to measure latency consistently across methods (models can be compiled with TensorRT and trained with mixed precision for additional orthogonal improvements).  

\noindent
\textbf{Results:} In this case, just like autonomous driving, we observe that the vanishing point is highly local. Due to overheads of vanishing point estimate on our edge device, we instead employ the average vanishing point, and cache saliency $S$, considerably reducing our approach's latency and memory while maximizing accuracy. From Table~\ref{table:appendix-bus-data}, we observe $AR$ and $mAP_{50}$ for the baseline (Faster R-CNN) and our approach at 0.5x and 0.75x scales. Our method consistently outperforms the baseline method at the same scale, showing both better precision and recall while incurring only \textbf{4ms} additional latency and \textbf{22 MB} memory overheads.


\section{Qualitative Results}

% \blue{Add temporal + geometric}

% \blue{How it works in different Situations: Bus Data on Highways, BDD etc}

\noindent
We present the variations of our proposed Two-Plane Perspective Prior across different datasets and scenarios in Figure~\ref{fig:appendix-two-plane}. We also show case of the major failure mode of just employing Ground Plane Prior in Fig~\ref{fig:appendix-ground-plane}. We also show a qualitative comparison with prior work in Figures~\ref{fig:appendix-warp-compare-1},~\ref{fig:appendix-warp-compare-2} and ~\ref{fig:appendix-warp-compare-3}. Lastly, we take a closer look at some of the far away objects that were detected in Figures~\ref{fig:appendix-far-away-1} and ~\ref{fig:appendix-far-away-2}. The accompanying website further illustrates some of the aspects of our method.


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{suppl/Warped-Saliency.pdf}
    \caption{\textbf{Two-Plane Prior Based Warping:} Two-Plane Prior is defined by a few parameters that describe two planar regions in the direction of the vanishing point in the 3D scene (See Section 3.1 and 3.2 in manuscript). Firstly, we can observe the Two-Plane Prior's explicit dependence on the vanishing point $v$ in the saliency maps. Next, as we can observe from grid lines (equidistant in the original image) overlaid on top of the warped images, the extent of spatial warping varies across datasets (WALT, Argoverse-HD and Commuter Bus), showing us the need for learnable parameter $\nu$ over prior work which do not directly model this relationship. Lastly, notice the second plane's effect in sampling. The second plane acts as a "counter-balance" to reduce distortion, and the plane is  \textbf{faintly observable} (contrast adjusted for better visibility).}
    \label{fig:appendix-two-plane}    
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{suppl/GroundPlane-Vs-TwoPlane.pdf}
    \caption{\textbf{Ground Plane Prior vs Two-Plane Prior:} This figure demonstrates how crucial it is to model the second plane. Learning is difficult with Ground Plane Prior (Section 5.1 in the manuscript) and causes heavy distortion of non-ground-plane regions. \\
    \textbf{\textit{Scene 1:}} Detector with Ground Plane Prior misses nearby tall objects because of heavy distortion. Turquoise colored bus on the right (blue box) is detected when Two-Plane prior is used and missed with Ground Plane prior.
    \\ \textbf{\textit{Scene 2:}} Objects not on the ground plane are missed as they are squished by the Ground Plane Prior. Yellow boxes denote the traffic lights. All 6 traffic lights in the scene were detected when Two-Plane prior is used while Ground Plane prior missed 4 traffic lights.}
    \label{fig:appendix-ground-plane}    
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{suppl/WarpComparison-1.pdf}
    \caption{\textbf{Qualitative Comparison with Fovea Warps on Argoverse-HD:} We observe that the reliance on the vanishing point $v$ allows the warp to sample in the direction of the road even while making turns. Far ahead on the road, a $truck$ (dark-blue) is not detected by Fovea ($S_{D}$ or $S_{I}$), but correctly detected by our approach.}
    \label{fig:appendix-warp-compare-1}    
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{suppl/WarpComparison-2.pdf}
    \caption{\textbf{Qualitative Comparison with Fovea Warps on Argoverse-HD:} We observe that scale factor $\nu$ models the extent of sampling better. Fovea ($S_D$ or $S_I$) misses the $stop-sign$ (a magenta box in the middle of the image) which our method is able to detect (as it's larger in the warped image). Fovea ($S_{I}$) notes that in their method, regions immediately adjacent to magnified regions are often contracted which is noticed in this case.}
    \label{fig:appendix-warp-compare-2}    
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{suppl/WarpComparison-3.pdf}
    \caption{\textbf{Qualitative Comparison with Fovea Warps on Argoverse-HD:} \textit{Failure Case:} The model has misclassified a pedestrian as car in the image warped by the Two-Plane Prior while correctly classified by Fovea ($S_{D}$ or $S_{I}$) (red), likely due to the presence of bicycle and heavier distortion.}
    \label{fig:appendix-warp-compare-3}    
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{suppl/far_away_examples-1.pdf}
    \caption{\textbf{Detection of Far Away Objects:} Our Two-Plane Prior boosts the detection of small far-away objects at lower resolutions (depicted image from Argoverse-HD dataset). The cropped green region in the (a) original image is (c) zoomed in while (b) shows all the detections in the warped image. Our method detects far-away pedestrian and car.}
    \label{fig:appendix-far-away-1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{suppl/far_away_examples-2.pdf}
    \caption{\textbf{Detection of Far Away Objects:} Our Two-Plane Prior boosts the detection of small far-away objects at lower resolutions (depicted image from Argoverse-HD dataset). The cropped green region in the (a) original image is (c) zoomed in while (b) shows all the detections in the warped image. Our method is able to detect the occluded car.}
    \label{fig:appendix-far-away-2}
\end{figure*}