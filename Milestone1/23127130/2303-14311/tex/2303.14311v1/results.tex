\noindent
\textbf{Accuracy-Latency Comparisons:} On Argoverse-HD, we compare with Faster R-CNN with naive downsampling (Baseline) and Faster R-CNN paired with adaptive sampling from Fovea~\cite{thavamani2021fovea} which proposed two priors, a dataset-wide prior ($S_{D}$) and frame-specific temporal priors ($S_{I}$ \& $L: S_{I}$; from previous frame detections). 

\noindent
Two-Plane Prior improves (Table~\ref{table:baselinekiller}) upon baseline at the same scale by \green{+6.6 $AP$} and over SOTA by \green{+2.7 $AP$}. For small objects, the improvements are even more dramatic, our method improves accuracy by \green{+9.6 $AP_{S}$} or \green{195\%} over baseline and \green{+4.2 $AP_{S}$} or \green{45\%} over SOTA respectively. Surprisingly, our method at 0.5x scale improves upon Faster R-CNN at 0.75x scale by \green{+1.6 $AP$} having latency improvement of 35\%. Our Two-Plane prior trained via pseudo-labels comes very close to SOTA which employ ground truth labels, the gap is only \maroon{-1 $AP$} and improves upon Faster R-CNN model trained on ground truth by \green{+2.9 AP} inferred at the same scale.

\begin{table}[t]
{\footnotesize
\centering
\setlength{\tabcolsep}{2.6pt}
\begin{tabular}{lllllll}
\toprule
Method            & Scale   & $AP$ & $AP_S$ & $AP_M$ & $AP_L$ & Latency (ms) \\
\midrule
Faster R-CNN               & 0.5x    & 24.2  & 4.9    & 29.0   & 50.9  & $78.4 \pm 1.8$ \\
\midrule
Fovea ($S_{D}$)~\cite{thavamani2021fovea}   & 0.5x          & 26.7  & 8.2    & 29.7   & 54.1 & $83 \pm 2.5$ \\
Fovea ($S_{I}$)~\cite{thavamani2021fovea}   & 0.5x            & 28.0 & 10.4   & 31.0   & \textbf{54.5} & $85 \pm 2.7$ \\
Fovea (L:$S_{I}$)~\cite{thavamani2021fovea}   & 0.5x           & 28.1    & 10.3   & 30.9   & 54.1  &  $85.4 \pm 2.7$ \\
\midrule
Two-Plane Pr. (Pseudo.) & 0.5x & 27.1   & 9.8    & 28.9   & 50.2 & $104.5 \pm 8.5$ \\
Two-Plane Prior   & 0.5x          & \textbf{30.8} & \textbf{14.5}   & \textbf{31.6}   & 52.9 & $105 \pm 8.5$ \\
\midrule
\multicolumn{7}{c}{Baseline at higher scales} \\
\midrule
Faster R-CNN  & 0.75x         & 29.2 & 11.6   & 32.1   & 53.3 & $142 \pm 2.5$ \\
Faster R-CNN & 1.0x         & 33.3  & 16.8   & 34.8   & 53.6 & $220 \pm 1.7 $ \\
\bottomrule
\end{tabular}
\caption{\textbf{Evaluation on Argoverse-HD:} Two-Plane Prior outperforms both SOTA's dataset-wide and temporal priors in overall accuracy. Our method improves small object detection by \green{$+4.1 AP_{S}$} or 39\% over SOTA.}
\label{table:baselinekiller}
}
\vspace{-0.1in}
\end{table}

\noindent
WALT dataset~\cite{reddy2022walt} comprises images (only images with notable changes are saved) and not videos, we compare with Fovea~\cite{thavamani2021fovea} paired with the dataset-wide prior. We observe similar trends on both splits (Table~\ref{table:viewpoint-generalization} and Fig~\ref{fig:walt-scale-fig}) and note large improvements over baseline and a consistent improvement over Fovea~\cite{thavamani2021fovea}, specially for small objects.

\begin{table}[]
{\footnotesize
\centering
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{lllllll}
\toprule
ID & Method                              & Scale(s) & $sAP$ & $sAP_S$ & $sAP_M$ & $sAP_L$ \\
\midrule
1 & Streamer~\cite{li2020towards}   & 0.5x &  18.3           &  3.1      &  15.0      & 40.9       \\
\midrule
2 & 1 + Adascale~\cite{chin2019adascale} & 0.2x-0.6x & 13.4   & 0.2    & 8.6  & 37.4 \\
3 & Adaptive Streamer~\cite{ghosh2021adaptive}  & 0.2x-0.6x  & 21.3  & 4.2    & 18.8  & 47.0       \\
4 & 1 + FOVEA ($S_I$)~\cite{thavamani2021fovea}     & 0.5x                & 24.1  & 8.4    & 24.7   & 48.7   \\
\midrule
5 & 1 + Ours (Avg VP)             & 0.5x  & \textbf{29.9}   & \textbf{13.7}   & \textbf{31.3}   & \textbf{52.2}   \\
6 & 1 + Ours       & 0.5x  & \textbf{30.0}       &  \textbf{13.7}        &  \textbf{31.5}     &   \textbf{52.2}     \\
\midrule
\midrule
7 & 1 + Ours (VP Oracle)        & 0.5x  & 30.7   & 14.5   & 31.6   & 52.9   \\
\bottomrule
\end{tabular}
\caption{\textbf{Streaming Evaluation on Argoverse-HD:} Ours denotes Two-Plane Prior. Every frame's prediction (streamed at 30FPS) must be emitted \textbf{before} frame is observed~\cite{li2020towards} (via forecasting). All methods evaluated on Titan X GPU. Underlying detector (Faster R-CNN) is constant across approaches, improvements are solely from spatial sampling mechanisms. Notice improved detection of small objects by \green{$+5.3 sAP_{S}$} or 63\% over SOTA.} 
\label{table:streaming-eval}
}
\end{table}

\noindent
\textbf{Real-time/Streaming Comparisons:} We use Argoverse-HD dataset, and compare using the \textbf{sAP} metric (Described in Section~\ref{sec:considerations}). Algorithms may choose any scale and frames as long as real-time latency constraint is satisfied. 

\noindent
All compared methods use Faster R-CNN, and we adopt their reported scales (and other parameters). Streamer~\cite{li2020towards} converts any single frame detector for streaming by scheduling which frames to process and interpolating predictions between processed frames. AdaScale~\cite{chin2019adascale} regresses optimal scale from image features to minimize single-frame latency while Adaptive Streamer~\cite{ghosh2021adaptive} learns scale choice in the streaming setting. Both these methods employ naive-downsampling. State-of-the-art, Fovea~\cite{thavamani2021fovea} employs the temporal prior ($S_{I}$). From Table~\ref{table:streaming-eval}, Two-Plane prior outperforms the above approaches by \green{+16.5 $sAP$}, \green{+8.6 $sAP$} and \green{+5.9 $sAP$} respectively. Comparison with~\cite{chin2019adascale, li2020towards, ghosh2021adaptive} shows the limitations of naive downsampling, even when ``optimal'' scale is chosen. Our geometric prior greatly improves small object detection performance by \green{63\%} or \green{+5.3 $sAP_S$} over SOTA. To consider dependence on accurate Vanishing Point detection (and its overheads), we use NeurVPS~\cite{zhou2019neurvps} as oracle (we simulate accurate prediction with zero delay) to obtain an upper bound, we observe even average vanishing point location's performance is within 0.8 $sAP$.


\begin{figure}
    \centering
    \includegraphics[width=0.49\linewidth]{images/detscale-fixed.pdf}
    \includegraphics[width=0.49\linewidth]{images/detscale-small-fixed.pdf}
\caption{\textbf{WALT All-Viewpoints Split:} Our approach (Two-Plane Prior) shows improved overall performance over naive downsampling and state-of-the-art adaptive sampling technique, specially for small objects at all scales (starting from 0.5x). Horizontal line (orange) indicates performance at maximum possible scale (0.6x) the base detector was trained at (memory constraints).}
    \label{fig:walt-scale-fig}
\end{figure}

\noindent
\textbf{Accuracy-Scale Tradeoffs:}  We experiment with WALT \textbf{All-Viewpoints} split to observe accuracy-scale trade-offs. The native resolution ($4K$) of the dataset is extremely large and the gradients don't fit within 12 GB memory of our Titan X GPU, thus we cropped the skies and other static regions to reduce input scale (1x) to $1500 \times 2000$. Still, the highest scale we were able to train our baseline Faster R-CNN model is 0.6x. So, we use aggressive downsampling scales $\{0.5, 0.4375, 0.375, 0.3125, 0.25, 0.125 \}$. The results are presented in Figure~\ref{fig:walt-scale-fig}. We observe a large and consistent improvement over baseline and Fovea~\cite{thavamani2021fovea}, specially for small objects. For instance, considering performance at 0.375x scale, our approach is better than baseline by \green{+13.1 $AP$} and Fovea by \green{+1.4 $AP$} for all objects.

\noindent
For small objects, we observe dramatic improvement, at scales smaller than 0.375x, other approaches are unable to detect any small objects while our approach does so until 0.125x scale, showing that our approach degrades more gracefully. At 0.375x scale, our approach improves upon Faster R-CNN by \green{+10.7 $AP_S$} and Fovea by \green{+3.0 $AP_S$}.


\begin{table}[t]
{\footnotesize
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lllllll}
\toprule
Method              & Scale   & $AP$ &  $AP_S$ & $AP_M$ & $AP_L$ \\
\midrule
Faster R-CNN & 0.25x & 16.7  & 0	& 7.2 & 42.3 \\
FOVEA ($S_{D}$)~\cite{thavamani2021fovea} & 0.25x & 23.2 &	0	& 13.9 &  54.1 \\
\midrule
Two-Plane Prior & 0.25x & \textbf{25.2}	& \textbf{1.0} &	\textbf{18.2} &	\textbf{54.5} \\
\midrule
\midrule
Faster R-CNN & 0.5x        & 29.2    & 4.9    & 24.7   & 55.5   \\
FOVEA ($S_{D}$)~\cite{thavamani2021fovea} & 0.5x        & 34.4    & 8.7    & 30.5   & \textbf{59.4}   \\
\midrule
Two-Plane Prior & 0.5x  & \textbf{36.4}  & \textbf{11.6}   & \textbf{32.3}   & 59.0   \\
\midrule
\multicolumn{6}{c}{Baseline at higher scales} \\
\midrule
Faster R-CNN & 0.6x       &   33.2      &  9.3      &  28.6      &  56.7  \\   
Faster R-CNN\textbf{*} & 1.0x       &   34.3         &  12.6      &  30.7      &  54.3     \\
\bottomrule
\end{tabular}
\caption{\textbf{WALT Camera-Split:} The viewpoints on the test set were not seen, and Two-Plane Prior shows better performance over both naive downsampling and state-of-the-art adaptive sampling as it generalizes better to unseen scenes and viewpoints. \textit{*Not trained at that scale due to memory constraints on Titan X.}}
\label{table:viewpoint-generalization}
}
\vspace{-0.2in}
\end{table}

\noindent
\textbf{Generalization to new viewpoints:} We use WALT \textbf{Camera-Split}, the test scenes and viewpoint are unseen in training. E.g., the vanishing point of one of the held-out cameras is beyond the image's field of view. We operate on the same scale factors in the earlier experiments, and results are presented in Table~\ref{table:viewpoint-generalization}. We note lower overall performance levels due to scene/viewpoint novelty in the test sets. Our approach generalizes better due to the explicit modelling of the viewpoint via the vanishing point (See Section~\ref{sec:3dreasoning}). We note trends similar to previous experiment, we demonstrate improvements of \green{+8.5 $AP$} over naive-downsampling and \green{+2.0 $AP$} over Fovea~\cite{thavamani2021fovea} at 0.25x scale. 

\noindent
\textbf{Tracking Improvements}: We follow tracking-by-detection and pair IOUTracker~\cite{bochinski2017high} with detectors on Argoverse-HD dataset. MOTA and MOTP evaluate overall tracking performance. From Table~\ref{table:tracking-improvements}, Our method improves over baseline by \green{+4.8\%} and \green{+0.7\%}. We also focus on tracking quality metrics, Mostly Tracked \% (\textbf{MT\%}) evaluates the percentage of objects tracked for atleast 80\% of their lifespan while Mostly Lost \% (\textbf{ML\%}) evaluates percentage of objects for less than 20\% of their lifespan. In both these cases, our approach improves upon the baseline by \green{+7.6\%} and \green{-6.7\%} respectively. To  autonomous navigation, we define two relevant metrics, namely, average lifespan extension \textbf{(ALE)} and minimum object size tracked \textbf{(MOS)}, whose motivation and definitions can be viewed in supplementary.  We observe that our improvements are better than both Fovea ($S_{D}$) and ($S_{I}$). As we observe, our method improves tracking lifespan and also helps track smaller objects.

\noindent
\textbf{Efficient City-Scale Sensing:} We detect objects on Commuter Bus equipped with a Jetson AGX. Identifying (Recall) relevant frames is key on the edge. Recall of Faster R-CNN with at 1x scale is $43.3 AR$ ($16.9 AR_{S}$) (Latency: 350ms; infeasible for real-time execution) but drops to $31.7 AR$ ($0.5 AR_{S}$) at 0.5x scale when naively down-sampled (Latency: 154ms). Whereas our approach at 0.5x scale improves recall by 42\% over full resolution execution to \green{61.7 $AR$} (\green{16.4 $AR_{S}$}) with latency of 158 ms (+4 ms). 

\noindent
In supplementary, we perform additional comparisons, provide details on handling multiple vanishing points, more tracking results and edge sensing results. We also present some qualitative results and comparisons.

\begin{table}[]
{\footnotesize
\centering
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{lllllll}
\toprule
Method                              & MOTA $\uparrow$ & MOTP $\uparrow$ & MT\% $\uparrow$ & ML\% $\downarrow$ & MOS $\downarrow$ & ALE\% $\uparrow$ \\
\midrule
Faster RCNN & 39.8 & 82.3 & 30.7 & 35.6 & 37.1 & 59.3 \% \\
Fovea ($S_{D}$)~\cite{thavamani2021fovea} & 43.9 & 81.9 & 34.1 & 31.9 & 34.8 & +5.4 \% \\
Fovea ($S_{I}$)~\cite{thavamani2021fovea} & 44.3 & 81.8 & 36.7 & \textbf{28.4} & 33.8 & +8.4 \% \\
\midrule
 Two-Plane Prior      & \textbf{44.6}       &  \textbf{83.0}        &  \textbf{38.3}     &   28.9  & \textbf{31.6} & \textbf{+9.9 \%} \\
\bottomrule
\end{tabular}
\caption{{\textbf{Tracking Improvements}:} We setup a tracking by detection pipeline and replace the underlying detection method and observe improvements if any. All the detectors employ the Faster R-CNN architecture and are executed at 0.5x scale. We observe improvements in tracking metrics due to Two-Plane Prior.}
\label{table:tracking-improvements}
}
\end{table}

\subsection{Ablation Studies}

\noindent
We discuss some of the considerations of our approach through experiments on the Argoverse-HD dataset.


\noindent
\textbf{Ground Plane vs Two-Plane Prior:} We discussed the rationale of employing multiple planes in Fig~\ref{fig:saliency-explanation}, and our results are consistent. From Table~\ref{table:design-considerations}, Two-Plane Prior outperforms Ground Plane prior considerably (\green{+1.8 $AP$}). Ground Plane Prior outperforms Two-Plane Prior on small objects by \green{+1 $AP_S$} but is heavily penalized on medium (\maroon{-4.1 $AP_M$}) and large objects (\maroon{-4.6 $AP_L$}). This is attributed to heavy distortion of tall and nearby objects, and objects that are not on the plane (Figure~\ref{fig:saliency-explanation}). Lastly, this prior was difficult to learn, the parameter space severely distorted the images (we tuned initialization and learning rate). Thus we did not consider this prior further. The second plane acts as a counter-balance and that warping space is learnable.

\noindent
\textbf{Vanishing Point Estimate Dependence:} From Table~\ref{table:design-considerations}, dominant vanishing point in autonomous navigation is highly local in nature, and estimating VP improves the result by \green{+1.2 AP}. Estimating the vanishing point is a design choice, it's important for safety critical applications like autonomous navigation (performance while navigating turns) however might be omitted for sensing applications.

\begin{table}[t]
{\footnotesize
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lllllll}
\toprule
Method              & Scale   & $AP$ &  $AP_S$ & $AP_M$ & $AP_L$ \\
\midrule
Ground Plane Prior  & 0.5x & 29.1  & 15.5 & 27.5 & 48.3 \\
Two-Plane Prior (Psuedo.) & 0.5x & 27.1 & 9.8 & 28.9 & 50.2 \\
Two-Plane Prior (Avg VP) & 0.5x & 29.6 & 12.7 & 30.7 & 52.7 \\
Two-Plane Prior & 0.5x  & 30.8 & 14.5 & 31.6 & 52.9 \\
\bottomrule
\end{tabular}
\caption{\textbf{Ablation Study on Argoverse-HD} to justify our design choice of using two planes, dependence on accurate vanishing point detection and choice of pseudo labels vs ground truth.}
\label{table:design-considerations}
}
\vspace{-0.2in}
\end{table}

\noindent
\textbf{Using Pseudo Labels vs Ground Truth:} Table~\ref{table:design-considerations} shows there is still considerable gap (\maroon{-3.7 $AP$}) between the Two-Plane Prior trained from pseudo labels and ground truth. We observe that the model under-performs on $stopsign$, $bike$ and $truck$ classes, which are under-represented in the COCO dataset~\cite{lin2014microsoft} compared to $person$ and $car$ classes. Performance of the pre-trained model on these classes is low even at 1x scale. Hence, we believe that the performance difference is an artifact of this domain gap.
