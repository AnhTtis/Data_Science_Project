\subsection{Datasets}
\label{sec:datasets}

\noindent
\textbf{Argoverse-HD~\cite{li2020towards}:} We employ Argoverse-HD dataset for evaluation in the autonomous navigation scenario. This dataset consists of 30fps video sequences from a car collected at $1920\times1200$ resolution, and dense box annotations are provided for common objects of interest such as vehicles, pedestrians, signboards and traffic lights. 

\noindent
\textbf{WALT~\cite{reddy2022walt}:} We employ images from $8$ 4K cameras that overlook public urban settings to analyze the flow of traffic vehicles. The data is captured for 3-second bursts every few minutes and only images with notable changes are stored. We annotated a set of 4738 images with vehicles collected over the time period of a year covering a variety of day/night/dawn settings, seasonal changes and camera viewpoints. We show our results on two splits (approximately 80\% training and 20\% testing). The first, \textbf{All-Viewpoints}, images from all the cameras are equally represented in the train and test sets. Alternatively, we split by camera, \textbf{Split-by-Camera}, images from 6 cameras are part of the training set and 2 cameras are held out for testing. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{images/bus-data.pdf}
    \caption{\textbf{Commuter Bus Dataset:} The captured data has a unique viewpoint, the average size of objects is small and captured under a wide variety of lighting conditions. Top-left and bottom-right images depict the same bus-stop and trashcan at different time of day and season.}
    \label{fig:busedge-imagery}
\end{figure}

\noindent
\textbf{Commuter Bus Dataset:} We curated this dataset from a commuter bus running on an urban route. The 720p camera of interest has a unique viewpoint and scene geometry (See Figure~\ref{fig:busedge-imagery}). Annotated categories are trashcans and garbage bags (to help inform public services) and people with special needs (using wheelchairs or strollers), which are a rare occurrence. The dataset size is small with only 750 annotated images (split into 70\% training and 30\% testing). This is an extremely challenging dataset due to it's unique viewpoint, small object size, rare categories, along with variations in lighting and seasons. 

\subsection{Evaluation Details}
\label{sec:considerations}

\noindent
We perform apples-to-apples comparisons on the same detector trained using the datasets with identical training schedule and hardware.

\noindent
\textbf{Data:} We compare to methods that were trained on fixed training data. In contrast, sAP leaderboards~\cite{li2020towards} don't restrict data and evaluate on different hardware. We compare with~\cite{ghosh2021adaptive, li2020towards} from leaderboard, which follow the same protocols. Other methods on the leaderboard use additional training data to train off-the-self detectors. Our detectors would see similar improvements with additional data.

\noindent
\textbf{Real-Time Evaluation:} We evaluate using \textbf{Streaming AP (sAP)} metric proposed by~\cite{li2020towards}, which integrates latency and accuracy into a single metric. Instead of considering models via accuracy-latency tradeoffs~\cite{huang2017speed}, real-time performance can be evaluated by applying real-time constraints on the predictions~\cite{li2020towards}. Frames of the video are observed every 33 milliseconds (30 fps) and predictions for every frame must be emitted \textbf{before} the frame is observed (forecasting is necessary). For a fair comparison, \textbf{sAP} requires evaluation on the same hardware. Streaming results are not directly comparable with other work~\cite{thavamani2021fovea, yang2022streamyolo, li2020towards} as they use other hardware (say, V100 or 2080Ti), thus we run the evaluation on our hardware (Titan X).

\noindent
{\it Additional details are in the Supplementary.}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{images/Implementation.pdf}
    \caption{\textbf{Two-Plane Prior as a Neural Layer:} We implemented our approach as a global prior that is learned end-to-end from labelled data. Our prior is dependent on a vanishing point estimate to specify the viewing direction of the camera.}
    \label{fig:implementation}
\end{figure}
