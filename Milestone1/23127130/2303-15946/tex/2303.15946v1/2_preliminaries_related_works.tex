\section{Preliminaries and Related Work}
In this paper we consider the extreme setting for inductive recommendation in which user preferences are estimated by leveraging only past user-item interactions without any additional source of information. We focus on implicit user feedback \cite{rendle2012bpr}, with the understanding that explicit interactions are becoming increasingly scarce in real-world contexts. More formally, denoting with $\mathcal{U}$ and $\mathcal{I}$ the sets of users and items, and with $U = |\mathcal{U}|$ and $I = |\mathcal{I}|$ their respective cardinalities, we define the user-item interaction matrix $\textbf{R}_{U \times I}$, where cell $r_{ui} = 1$ if user $u$ has interacted with item $i$, and $0$ otherwise, as the only source of information.

%Modern recommender systems \edoinline{maybe talk about CF isntead?} aim to learn user preferences by leveraging knowledge about past user-item interactions. In this paper, we focus on implicit user feedback \cite{rendle2012bpr}, considering that explicit interactions are becoming increasingly scarce in real-world contexts. More formally, denoting with $\mathcal{U}$ and $\mathcal{I}$ the sets of users and items, and with $U = |\mathcal{U}|$ and $I = |\mathcal{I}|$ their respective cardinalities, we define the user-item interaction matrix $\textbf{R}_{U \times I}$, where cell $r_{ui} = 1$ if user $u$ has interacted with item $i$, and $0$ otherwise, as only source of information.

%Considering a graph $G = \{\mathcal{V}, \mathcal{E}\}$, where $\mathcal{V}$ indicates the set of nodes with cardinality $|\mathcal{V}| = N$, and $\mathcal{E}$ the set of its edges, we indicate with $\textbf{A}_{N\times N}$ its adjacency matrix, where cell $a_{ij}$ assumes the value $1$ if there is a directed edge between node $i$ and node $j$, and $0$ otherwise, and with $\textbf{D}_{N\times N}$ as its diagonal degree matrix, where cell $d_{ii}$ stores the number of edges connected to node $i$. Furthermore we indicate with $\Tilde{\textbf{A}} = \textbf{A} + \textbf{I}_N$ the adjacency matrix of the graph with added self-loop and with $\Tilde{\textbf{D}} = \textbf{D}+\textbf{I}_N$ its degree matrix. We denote with $\mathcal{N}_{i} = \{j \in \mathcal{V}| a_{ij} = 1\}$ the neighbor set of node $i$. %

\subsection{GCN-based Recommender}
%Modern recommender systems aim to learn user and item representations by leveraging knowledge about past user-item interactions. Here, we focus on implicit user feedback \cite{rendle2012bpr}, considering that explicit interactions are becoming increasingly scarce in real-world contexts. 
%\edoinline{don't know if keep this first two sentences} I think keep it. Is there a suitable reference for lack of explicit feedback
GCN-based models have recently been applied to recommender system models, by virtue of the fact that historical user-item interactions can be interpreted as the edges of a graph.  It is possible to define the adjacency matrix $\textbf{A}$, associated with an undirected bipartite graph, exploiting the user-item interaction matrix $\textbf{R}_{U \times I}$, as:
\begin{align*}
    \textbf{A} = 
    \begin{bmatrix}
    \textbf{0}_{U \times I} & \textbf{R} \\
    \textbf{R}^T & \textbf{0}_{I \times U}
    \end{bmatrix}
\end{align*}
The set of the graph's nodes is $\mathcal{V} = \mathcal{U} \bigcup \mathcal{I}$ and there exists an edge between a user $u$ and an item $i$ if the corresponding cell of the interaction matrix $r_{ui} = 1$.
He et al. \cite{wang2019neural}, first applied graph convolution in a setting where no side information was available, and proposed to initialise the node representations with free parameters. This formulation is a variant of the one proposed in \cite{kipf2016semisupervised} but includes information about the affinity of two nodes, computed as the dot product between embeddings. 
%Subsequently Chen et al. in \cite{chen2020revisiting} \chl{is shown how the affinity information as well as the non-linearities are only making more complex the training process and degrading the performance. Finally He et al.} 
Subsequently, Chen et al.~\cite{Chen_2020} have shown how the affinity information as well as the non-linearities tend to complicate the training process as well as degrade the overall performance.
%Finally He et al. \cite{He_2020}, confirm what introduced by \cite{wu2019simplifying} \chl{showing how the benefit of graph convolution arises only from the embedding smoothing and that better performance can be achieved by removing all the weight matrices. In this final formulation users and items embeddings at depth $k$ can be simply computed as the linear combination of the embeddings of the step before with a weight assigned from a suitable propagation matrix \textbf{P} as}:
Finally, He et al. \cite{He_2020}, confirmed the results of \cite{wu2019simplifying} by showing how the benefits of graph convolution derive from smoothing the embeddings and that better performance can be achieved by removing all the intermediary weight matrices. In this formulation, the embeddings of users and items at depth $k$ can be simply computed as the linear combination of the embeddings of the previous step with weights assigned from a suitably chosen propagation matrix \textbf{P}.
%
% 
%but He et al. in \cite{wang2019neural}, have shown their great capabilities to enrich user and items embeddings, with high-order realtionships mined from the user-item graph in a collaborative filtering setting in which no side information is available. The first application of graph convolution to collaborative filtering is presented by Wang et al. in \cite{wang2019neural}. Here it is proposed to use node features consisting of free parameter vectors that can be optimized during the training process and to include inside the embeddings propagation information related to the affinity between the nodes embeddings through a dot product.  Following this, in \cite{chen2020revisiting},  the graph convolution process is simplified by removing the affinity between embeddings and also through the use of non-linearity, making each embedding a linear combination of its neighborhood at each convolution step. Finally, in \cite{He_2020} it is proved that the benefit of graph convolution arises only from the embedding smoothing and that better performance can be achieved by removing all the weight matrices. In this final formulation users and items embeddings at depth $k$ can be simply computed as the linear combination of the embeddings of the step before with a weight assigned from a suitable propagation matrix \textbf{S}:

% \subsection{Inductive Recommendations through side information}
% To tackle the inductive recommendation problem several works \cite{volkovs2017dropoutnet, hamilton2017inductive, ying2018graph} propose to leverage item and user features such as age, occupation etc. to learn a mapping function capable of projecting user and item feature vectors into a shared latent space. 
% The benefit of these approaches is that when a new user joins the system, we just need to access the new user's associated features and then we can generate embeddings for them.
% However, such models are limited by the quantity and quality of the features available.

% Another line of research proposes inductive models for the matrix completion problem \cite{zhang2019inductive,jain2013provable,pmlr-v80-hartford18a}. These methods do not rely on pre-existing features but instead assume that explicit feedback (i.e. ratings) is available as input to the model, and the target is then to learn a function which estimates the user's explicit preferences.
% %suppose to  have explicit feedbacks (i.e. ratings) as input to the model, and target to learn a function which estimate explicit user preferences. 
% The main drawback is that explicit user feedback is usually difficult to collect in many modern recommender systems settings and is often very scarce in real-world scenarios and this considerably narrows the possible applications of these models.
% Both of these solutions are intrinsically different from our proposed model IGCCF. With IGCCF we aim to achieve inductive recommendations for a new user by relying solely on \emph{implicit} user-item interaction data without the need for features or explicit user feedback.

% \begin{comment}
% since  associated to them. that since the feature space is shared among users 
% a model trained on a group of user can
% map new user to the learnt latent space with the only need of their features.
% However such models are often limited by the quality of the features available, 
% furthermore the harvesting of features, especially user ones, can be very hard in practice.
% Another well studied category of models are the matrix compeltion models. Here no content information are used to learn embeddings but the models suppose to have explicit feedbacks (i.e. ratings) as input from which to learn predictions.
% the problem is based on the fact the user explicit feedbacks such as rating are availble for the prediction.
% cite igmc, f-eae.

% We are studying a different setting from the two introduced above, since our proposed method doesn't use features to initialise emebeddings and furthermore is based on pure \textit{implicit feedbacks}, since the objective is not to predict user item ratings.

% \edoinline{move that}, \hl{Graph convolutional algorithms have proven their ability to deal well with situations in which there are abundant content features available to initialise the node's representations} \cite{ying2018graph}. However, for most real-world recommender systems, high quality side information is often hard to extract, or simply not available.
% \end{comment}


\subsection{Item-based Recommender}
%A category of models related to the proposed methodology are the item-based recommendation models. 
Item-based models aim to learn item embeddings which are subsequently used to infer user representations. As a result, this model category is capable of providing recommendations to new users who join the system post training.
%Instead of mapping users to a fixed representation, only the item embeddings are learned, and then used to infer user representations, introducing the possibility of directly create representations for new users who join the system.
Cremonesi \etal \cite{Cremonesi2010Performance}, proposed PureSVD which uses singular value decomposition to retrieve item representations from the user-item interaction matrix, and subsequently compute the user embeddings as a weighted combination of item representations.
Later, Kabbur \etal in \cite{kabbur2013fism} also propose 
to compute users as a weighted combination of items, but instead of 
computing them after the creation of the item embeddings, they are jointly used together with the item representation as part of an optimisation process. 

Our proposed IGCCF model inherits from the item-based model the core idea of inferring user embeddings from items, but it is also capable of leveraging the information contained in the graph-structure during the item representation learning phase through graph convolution.
% but at the same time leverages the nature of the graph-structured data in the user-item interaction matrix to build richer embeddings.

\begin{comment}
derive the item embeddings through a closed form solution, these are lea

somewhat similar where some term have been added to correct a user prediction but the idea is always to compute user embeddings from the weighted combination of the item but this time user embedding are actively used during the training procedure.

The paradigm rel idea is to use the item embe

These category of model learn item embeddings 

Instead of mapping users to a fixed embedding to optimise during the training process, these are projected in the item
The core idea is to infer map users 


Users embeddings are
Users are not directly mapped to a fixed embedding 

The main idea is to learn the embeddings only for the items
The key concept of these category of model is to compute the representation of a given user through
The main idea is to learn emebeddings only for the items and use them to retrieve the user ones. One of the first work proposing that is \textbf{Cremonesi}, proposing PureSVD which uses singular value decomposition to decompose the user-item interaction matrix, and compute the user embeddings as a weighted combination of the item, key thing to notice is that user embeddings are computed in a second moment and not used during the learning process. Succesively \textbf{FISM} propose again somewhat similar where some term have been added to correct a user prediction but the idea is always to compute user embeddings from the weighted combination of the item but this time user embedding are actively used during the training procedure.
Our model is similar from those category of model but at the same time differs since we are incorporating high-order associations mining them from the user-item graph.
\end{comment}




