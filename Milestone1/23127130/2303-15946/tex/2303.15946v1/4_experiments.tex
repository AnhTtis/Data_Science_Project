\section{Experiments}
We perform experiments on four real-world datasets to evaluate the proposed model. We answer to the following research questions. \textbf{[RQ1]}: How does IGCCF perform against transductive graph convolutional algorithms?
\textbf{[RQ2]}: How well does IGCCF generalise to unseen users?
\textbf{[RQ3]}: How do the hyperparameters of the algorithm affect its performance?

\subsection{Datasets}
% \input{tables/datasets_stats}
To evaluate the performance of the proposed methodology we perform experiments on four real world datasets gathered in different domains. \textbf{LastFM}: Implicit interactions from the Last.fm music website. In particular, the user \textit{listened} artist relation expressed as listening counts \cite{cantador2011second}. We consider a positive interaction as one where the user has listened to an artist. \textbf{Movielens1M}: User ratings of movies from the MovieLens website \cite{movielens}. Rating values range from $1$ to $5$, we consider ratings $\geq$ 3 as positive interactions. \textbf{Amazon Electronics}: User ratings of electronic products from the Amazon platform \cite{He_2016,mcauley2015image}. The rating values also range from $1$ to $5$, so we consider ratings $\geq$ 3 as positive interactions. \textbf{Gowalla} User \textit{check-ins} in key locations from Gowalla \cite{liang2016modeling}.
Here, we consider a positive interaction between a user and a location, if the user has checked-in at least once. To ensure the integrity of the datasets, following \cite{He_2020,wang2019neural}, we perform a \textit{k}-core preprocessing step setting $k_{core} = 10$, meaning we discard all users and items with less than ten interactions.
\input{tables/table-perf}
\subsection{Baselines}
To demonstrate the benefit of our approach we compare it against the following baselines: \textbf{BPRMF} \cite{rendle2012bpr} Matrix factorisation optimised by the BPR loss function. \textbf{iALS} \cite{hu2008collaborative} matrix factorization learned by implicit alternating least squares. \textbf{PureSVD}\cite{Cremonesi2010Performance}Compute item embeddings through a singular value decomposition of the user-item interaction matrix, which will be then used to infer user representations.  \textbf{FISM}\cite{kabbur2013fism} Learn item embeddings through optimisation process creating user representations as a weighted combination of items in their profile. Additional user and item biases as well as an agreement term are considered in the score estimation. \textbf{NGCF} \cite{wang2019neural} Work that introduces graph convolution to the collaborative filtering scenario, it uses dense layer and inner product to enrich the knowledge injected in the user item embeddings during the convolution process.
\textbf{LightGCN} \cite{He_2020} Simplified version of graph convolution applied to collaborative filtering directly smooth user and item embeddings onto the user-item bipartite graph. We follow the original paper \cite{He_2020} and use $a_k = 1/(k+1)$.

For each baseline, an exhaustive grid-search has been carried out to ensure optimal performance. Following \cite{He_2020}, for all adopted algorithms the batch size has been  set to $1024$ and embedding size to $64$.
Further details on the ranges of the hyperparameter search as well as the data used for the experiments are available in the code repository \footnote{\href{https://github.com/damicoedoardo/IGCCF}{https://github.com/damicoedoardo/IGCCF}}.

% \input{tex_figures/performance-upl}
% \input{tex_figures/1fig_performance_upl}
\subsection{Transductive performance}
\label{sec:transductive_perf}
In this section we evaluate the performance of IGCCF against the proposed baselines in a transductive setting, meaning considering only users present at training time.
To evaluate every model, following \cite{He_2020,wang2019neural}, for each user, we randomly sample  $80\%$ of his interactions to constitute the training set, $10\%$ to be the test set, while the remaining $10\%$ are used as a validation set to tune the algorithm hyper-parameters. Subsequently, validation and training data are merged together and used to retrain the model, which is then evaluated on the test set.
In order to asses the quality of the recommendations produced by our system, we follow the approach outlined in
\cite{wang2019neural,wu2019simplifying,Chen_2020}. For each user in the test data, we generate a ranking of items and calculate the average \textit{Recall@N} and \textit{NDCG@N} scores across all users, considering two different cutoff values $N = 5$ and $N = 20$. The final results of this analysis are presented in \autoref{table:performance_comparison}. 

Based on the results obtained, we can establish that IGCCF outperforms NGCF and LightGCN on all four datasets examined for each metric and cutoff. This confirms that explicitly parametrizing the user embeddings is not necessary to get the optimum performance; on the contrary, it might result in an increase in the number of parameters of the model, which is detrimental to both training time and spatial complexity of the model.
Furthermore, IGCCF shows superior performance with respect to the item-based baseline models. This demonstrates that interpreting user-item interaction as graph-structured data introduces relevant knowledge into the algorithm learning process, leading to improved model performance.
    % \item LightGCN \cite{He_2020} performs better with respect to NGCF on all datasets. This confirms the intuition that in the absence of node features, the key component of the graph convolution operation is the smoothing process, and that the dense layer and non-linearity in NGCF only increase the complexity of the model, while at the same time reducing its actual performance.
\subsection{Inductive performance}
\input{tables/inductive_perf_table}
\label{sec:inductiveperf}
%However it has to be noted that, in the case of LastFM, the proposed model performs poorly on longer user profiles with respect to the baselines, though this is just one dataset out of four \diarmuidinline{Can we think of an explanation for why it performs worse here?}. 
% have I got this experiment right?
% split the users into seen / unseen
% then split each group seen[90/10] unseen[90/10]
% then evaluate ndcg@20
% the 'train user percentage' reduces the size of the training data, but keeps the relative numbers of seen/unseen the same
% is that right?
A key feature of the proposed IGCCF algorithm, is the ability to create embeddings and consequently retrieve recommendations for \emph{unseen} users who are not present at training time. IGCCF does not require an additional learning phase to create the embeddings. As soon as a new user begins interacting with the items in the catalogue, we may construct its embedding employing \autoref{eqn:userembedding}.
%Once the item embeddings have been computed, as soon as 
%he model can construct representations for unseen users, requiring only the interaction profiles of these users.
%Our experiments are designed to verify that IGCCF, beyond achieving excellent performance in the transductive setting, can also be successfully applied in the inductive setting and produce high quality recommendations for users presented only at inference time.

To assess the inductive performance of the algorithm we hold out $10\%$ of the users, using the remaining $90\%$ as training data.
For every unseen user we use $90\%$ of their profile interactions to create their embedding (Eq. \ref{eqn:userembedding}) and we evaluate the performance on the remaining $10\%$ of interactions.
%with respect to the performance on \emph{seen} users whose profiles have been used as training data.
%For both partitions, we split the user profiles removing $10\%$ of the interactions to constitute the test data while the remaining $90\%$ are used to build the user embeddings following \autoref{eqn:userembedding}. The model is trained using only the interactions contained in the \emph{seen} user profiles.
We compare the performance of our model against the inductive baselines corresponding to the item-based models (PureSVD and FISM) since the transductive models are not able to make predictions for users who are not present at training time without an additional learning phase.
Recommendation performance is evaluated using the same metrics and cutoffs reported in \autoref{sec:transductive_perf}. The overall results are reported in \autoref{table:inductive_performance_comparison}.
IGCCF outperforms the item-based baselines on all the datasets. These results strongly confirm our insight that the knowledge extracted from the constructed item-item graph is beneficial to the item-embedding learning phase, even when making predictions for unseen users.

%using the first to train the algorithm and the second to evaluate the performance on unseen users. The process is illustrated in ~\autoref{fig:seen_unseen_split}. 

%Additionally we seek to establish the extent to which the proposed method can maintain comparable recommendation performance on seen and unseen users as we train the model with even less data. 
% removed this sentence- doesn't seem to work
%We investigate the recommendation performance using \emph{NDCG@20} from a seen percentage of 90\%, where almost all users are present at training time, down to 50\%, which is a large fraction of unseen users to accommodate, but not likely where the graph structure is not likely ttoo disconnected.
%\hl{This due to the possibility of losing generalisation as the training data starts to be less dense, since implies the break of the underlying graph structure}.
%For this experiment, we increasingly reduce the percentage of \textit{seen} users which are used to train the model and consequently increase the percentage of \textit{unseen} users which are presented for inductive inference. Each algorithm is trained $5$ times on each different split used and we report the average performance.
%For this purpose we perform an iterative evaluation reducing the percentage of \textit{seen} users used to train the algorithm, consequently increasing the size of the one presented at inference time. We train each algorithm $5$ times on each different split used, reporting the average performance.
%We report a comparison with respect to the inductive baselines (PureSVD, FISM), corresponding to the item-based models. Recommendation performance is evaluated using the same metrics and cutoff reported in \autoref{sec:transductive_perf}, since there are no significant differences between metrics and cutoffs, for conciseness, we report only the $NDCG@20$. 
%The results are shown in  %\autoref{fig:Inductive_performance}.

% The overall results are reported in Table 3.
% \begin{itemize}[leftmargin=*]
    % \item IGCCF outperforms the item-based baselines on all the datasets. These results strongly confirm our insight that the knowledge extracted from the constructed item-item graph is beneficial to the item-embedding learning phase, even when making predictions for unseen users. %Futhermore result demonstrates that the patterns learned from the training data can easily generalise for the creation of representations of unseen users.
    %\item IGCCF exhibits comparable performance on both \textit{seen} and \textit{unseen} user groups for all the splits analysed, showing how the inductive performance of IGCCF is robust with respect to the amount of training data available. This does not hold for the baseline models, FISM and PureSVD, which show important differences between their transductive and inductive performance on the Gowalla and Movielens1M data sets respectively. 
    % \item The results also show how IGCCF handles different distributions of data, achieving the best performance across all the four datasets while the performance of the item-based baseline models is seen to be highly dependent on the dataset. For example, we find that FISM outperforms PureSVD on LastFM and AmazonElectronics data sets, whereas the situation is reversed for Movielens1M and Gowalla. We attribute this performance difference to the different distributions of user profile length in these datasets which is shown in \autoref{fig:Performance_upl}. When users with many interactions are present in the dataset, PureSVD performs better the FISM, whereas if the distribution is skewed towards shorter profiles, FISM gets the upper hand over PureSVD.
% \end{itemize}
%The proposed method outperform the item-based baselines on all the datasets metrics and cutoff analysed. The results obtained strongly confirm how the knowledge extracted from the constructed item-item graph can be beneficial to the embedding learning phase, proving how recommendation data can benefit from being interpreted as a graph structure instead of uncorrelated pointwise data.

%Additionally we seek to establish the extent to which we can maintain recommendation performance as we train the model with fewer seen users. retrain the model with varying percentages of seen and unseen users.

%As expected the performance of IGCCF are proportional to the amount of data available at training time, but the algorithm shows its ability to achieve comparable performance on both seen and unseen groups even when a very small amount of interaction data is available, exhibiting robust inductive performance with respect to the training data available.

\begin{comment}
For all datasets, the overall performance on seen users is usually a little higher than for unseen users, which is justified by the fact that the interaction patterns of these users are learned by the model in the training phase. But, it is clear that that \uigccf can achieve comparable performance on the unseen user group for all datasets.
As we reduce the amount of training data, we expect the performance of IGCCF to drop, and indeed it does degrade a little on both groups
%\edoinline{degrades a little, it is expected to see this degradation using less trianing data}. 
Even so, the algorithm maintains its ability to achieve comparable performance on both seen and unseen groups even when a very small amount of interaction data is available. The \uigccf model has excellent inductive performance, even when we train with very sparse quantities of data.
\end{comment}


% We can observe that:
% \edoinline{I have to rewrite this}
% \begin{itemize}[leftmargin=*]
%     \item The algorithm achieves comparable performance on both seen and unseen users. It must be noted that the overall performance on seen users is often better than that on unseen users. This can be justified by the fact that the patterns learnt from users on the training data are trivially right pattern to predict for themselves while not guaranteed right for new users \diarmuidinline{The last sentence here confuses me. What do we mean by "trivially right" patern?}.
%     \item The percentage of data used to train the model does not impact the ability of the algorithm to achieve comparable performance on seen and unseen users. However, as expected, the overall performance of the algorithm degrades. This means that useful patterns can be mined even when only a small amount of interaction data is available. \diarmuidinline{These are pretty good results! But I think we need a more thorough explanation. Why would I expect the performance gap between seen and unseen users to widen as the amount of training data is reduced?} \khalilinline{This section is good, but need to be proofread and edit}
% \end{itemize}

\subsubsection{Robustness of inductive performance}
\input{tex_figures/1fig_inductive_performance}
We are interested in the extent to which IGCCF can maintain comparable recommendation performance between \emph{seen} and \emph{unseen} users as we train the model with less data. For this experiment, we increasingly reduce the percentage of seen users which are used to train the model and consequently increase the percentage of unseen users which are presented for inductive inference. We train the model 5 times on each different split used and we report the average performance (NDCG@20). From the results in \autoref{fig:Inductive_performance} we can observe:
IGCCF exhibits comparable performance on both seen and unseen user groups for all the splits analysed, showing how the inductive performance of IGCCF is robust with respect to the amount of training data available. As expected, reducing the amount of available training data results in a lower $NDCG@20$, anyway is interesting to notice how the drop in performance is minimal even when the model is trained with half of the data available.

%\input{tex_figures/ablation-cdepth}
\input{tex_figures/1fig_ablation_cdepth}
\subsection{Ablation study}
% In this section we investigate the sensitivity of 
% the proposed model to the choice of hyper-parameters.
%\begin{itemize}[leftmargin=*]
%\item The results are reported in  \autoref{fig:ablation_cdepth}.
\subsubsection{Convolution Depth}
The convolution operation applied during the learning phase of the item embeddings, is beneficial in all the studied datasets, the results are reported in  \autoref{fig:ablation_cdepth}. It is interesting to consider the relationship between the dataset density and the effect of the convolution operation. We can see that the largest improvement of $31\%$ is found on Gowalla, which is the least dense dataset ($0.08\%$). As the density increases, the benefit introduced by the convolution operation decreases. We have an improvement of $26\%$ and $6\%$ on Amazon Electronics ($0.21\%$) and LastFM ($2.30\%$) respectively
%There is a decrease in Amazon Electronics () and LastFM (), 
while there is a very small increase of $1.5\%$ on Movielens1M ($4.43\%$). 
The results obtained suggest an inverse correlation between the dataset density and the benefit introduced by the convolution operation.
% This is explained by the fact that the dataset's density is proportional to the length of the user profiles. Having many interactions available during the mapping of a user into the item space mitigates the contribution of noisy interactions producing representations that directly reflect the real user preference. Whereas in circumstances in which users exhibit a short profile introducing additional information mined from the item-item graph is more helpful thanks to the fact that their representations will also indirectly benefit from representations of items directly associated to the ones in their profiles. As a consequence of this additional information, the influence of noisy interactions during the embeddings construction phase will be reduced, improving the final recommendation performance. 
%\input{tex_figures/ablation-dropout}

%\input{tex_figures/ablation-topk}


\begin{comment}
% figure correlation sparsity and performance
~\autoref{fig:convdepthsparsity}.
\input{fig-convdepthsparsity}
\end{comment}
%Our experiment show how the benefit introduced by the convolution is inversely proportional to the density of the data.
%This indicates that rich user profiles are, by themselves, informative enough so that the smoothing process from the convolution operation degrades prospective embeddings by introducing unnecessary information from the neighborhood. 

%\edoinline{I'm not sure about this paragraph} 
%With respect to other GCN models, for which the optimal convolution depth is commonly $3$ or $4$ \cite{wang2019neural, He_2020}, the proposed algorithm is achieving the best performance, on the studied datasets, using only a single convolution operation. This has to be attributed to the graph projection operation, which halves the graph diameter.

%As reported for other GCN models \cite{wang2019neural, He_2020}, the optimal convolution depth is commonly found to be $3$ or $4$. However, \uigccf reaches its optimal performance on the datasets we studied with a single convolutional operation. We attribute this result to the shrink effect received by the graph during the graph projection. %operation which reduces the graph diameter by half.
%\end{itemize}

%\input{fig-convergence}

%The results are reported in  \autoref{fig:ablation_dropout}
\subsubsection{User-profile dropout}
\input{tex_figures/1fig_ablation_dropout}
From the analysis reported in \autoref{fig:ablation_dropout}, it is clearly visible that user profile dropout regularisation have a strong impact on the performance of the proposed method. In all four datasets, the utilisation of the suggested regularisation technique 
enhance the quality of the recommendation performance, resulting in a gain over the \textit{NDCG@20} metric of 4.4\%, 3.0\%, 10.5\%, 1.5\% for LastFM, Movielens1M, Amazon Electronics and Gowalla respectively. Dropping a portion of the user profiles during the embeddings creation phase, force the algorithm to not heavily rely on information coming from specific items.

\subsubsection{Top-K pruning}
\input{tex_figures/1fig_ablation_topk}
\label{sec:exp_topk_pruning}
To prevent the well-known oversmoothing issue caused by graph convolution, we trim the edges of the item-item graph to maintain only the most strong connections between items. \autoref{fig:ablation_top-k} illustrates the results of the ablation study. In all of the datasets investigated, utilising at most $20$ neighbours for each item node yields the highest performance; this demonstrates how retaining edges associated with weak item links can worsen model performance while also increasing the algorithm training time.
% To prevent the well-known oversmoothing problem introduced by the graph convolution operations, after the creation of the item-item graph from the weighted projection of the user-item bipartite network, we prune the edges of it to maintain only the most informative relationships between items.
% The benefit of this procedure are visible from the result of the ablation study reported in \autoref{fig:ablation_top-k}.
% We can see that the best performance are reached using at most $20$ neighbours for each item node in all the datasets analysed, this confirms how keeping edges associated to weak item associations can degrade the performance due to the oversmoothing phenomenon introduced in the item embedding creation phase and at the same time reduce the time needed to train the algorithm as visible on the barplot in the background. 
% Beyond increasing the algorithm performance, as shown in the previous complexity analysis, the top-K pruning is also capable of bounding the time complexity associated to the model learning phase. We report the time required for the completion of an epoch in seconds, on the background of \autoref{fig:ablation_top-k}. From the empirical results we can confirm the linear impact of the parameter $k$ on the time complexity of the proposed method.
