
\section{Optimistic Online Learning for Reservation} \label{sec:algo}

\subsection{Algorithm}

Our approach is inspired from the \emph{Follow-the-Regularized-Leader} (FTRL) policy, whereby the learner aims to minimize the loss on all past slots plus a regularization term:
\begin{flalign}
\forall t, \bm z_{t+1} = \arg\min_{\bm z \in \Delta^2} \sum_{i=1}^t f_i(\bm z) + R(\bm z)
\end{flalign}
Due to the convexity of $f_t$, the following property holds:
\begin{flalign}
f_t(\bm z_t) - f_t(\bm z^*) \leq \grad f_t(\bm z_t)^\top (\bm z_t - \bm z^*)
\end{flalign}
which means that the regret against the functions $\{f_t\}$ is upper-bounded by the regret against their linearized form $\bar f_t(\bm z) = \grad f_t(\bm z_t)^\top \bm z$ \cite{mcmahan}. Consequently, the FTRL algorithm simplifies to:
\begin{flalign} \label{eq:ftrl}
\forall t, \bm z_{t+1} = \arg\min_{\bm z \in \Delta^2} \sum_{i=1}^t \grad f_i(\bm z_i)^\top \bm z + R(\bm z)
\end{flalign}

In our approach, we consider an additional gradient term, which is the \emph{optimistic} next slot gradient prediction $\grad \hat f_{t+1}( \hat{\boldsymbol{z}}_{t+1})$. 
In the FTRL, the regularization function is quadratic $R(\bm z) = \frac{1}{2\eta}||\bm z||^2$. In contrast, we design a sequence of proximal regularizers:
\begin{align}
    \forall t=1\ldots T, \quad r_t(\bm z) = \frac{\sigma_t}{2}||\bm z -\bm z_t||^2, \label{eq:reg}
\end{align}
with $||.||$ the Euclidean norm. The regularizer parameters are:
\begin{align}
    \sigma_t &= \sigma\Big( \sqrt{h_{1:t}} - \sqrt{h_{1:t-1}}\Big), \label{eq:acc_error}\\
    h_t &= ||\grad f_t(\bm z_t) - \grad \hat f_t(\hat{\boldsymbol{z}_t})  ||^2, \label{eq:quadratic_error}
\end{align}
where $\sigma\geq 0$, and $h_{1:t}=\sum_{i=1}^t h_i$.

All the above lead to the final form of our algorithm decision step:
\begin{align}
    \bm z_{t+1} = \arg& \min_{\bm z \in \Delta^2} \Big\{ r_{1:t}(\bm z) + \notag\\
    &\Big(\sum_{i=1}^t \grad f_i(\bm z_i) + \grad \hat f_{t+1}(\hat{\boldsymbol{z}}_{t+1})\Big)\top \bm z\Big\} \label{eq:stepOOLR}
\end{align}

\begin{algorithm}[t]
\SetAlgoRefName{OOLR}
\caption{Optimistic Online Learning for Reservation}
\DontPrintSemicolon
\KwInitialize{ \; $\bm z_1 \in \Delta^2, \sigma = 1$, $a_1$, $\bm q_1$, $f_1(\bm z_1)$ } 
	%
	%
\For{ $t=1,\ldots, T-1$ } 
{
Observe the new prediction of the gradient $\grad \hat{f}_{t+1}(\hat{\boldsymbol{z}}_{t+1})$ \;
%
Decide $\bm z_{t+1}$ by solving \eqref{eq:stepOOLR} \;
%
Observe the demand $a_{t+1}$, the reservation price $\bm p_{t+1}$, the spot price $\bm q_{t+1}$, the contributions $\bm \theta_{t+1}$ \;
%
Calculate $f_{t+1}(\bm z_{t+1})$ and $\grad f_{t+1}(\bm z_{t+1})$\;
%
Update $r_{1:t+1}(\bm z)$ according to \eqref{eq:reg} and \eqref{eq:acc_error} \;
}
%
\end{algorithm}


\subsection{Performance analysis}
We start with the necessary assumptions.
\begin{assumption}
The sets $\Gamma_i$, $i=1\ldots m$, are convex and compact, and it holds $|x|\leq D_i$, for any $x \in \Gamma_i$\footnote{Note that we can rename the set $\Gamma_1\times\ldots\times\Gamma_m$ as $\Delta$ and simply assume $\Delta$ is a compact convex set with diameter $D$. The design of the different sets $\Gamma_i$ allows us to choose a different reservation restriction for each resource type.}.
\end{assumption}

\begin{assumption}
The function $f_t$ is convex.
\end{assumption}

\begin{assumption}
$\{r_t\}_{t=1}^T$ is a sequence of proximal non-negative functions.
\end{assumption}

\begin{assumption}
Prediction $\grad \hat{f}_{t+1}(\hat{\boldsymbol{z}}_{t+1})$ is known at $t$.
\end{assumption}

%\begin{assumption}
%The function $r_{1:t}(\bm z)$ is $1$-strongly convex with respect to some norm $||.||_{(t)}$.%, that we will define in the sequel.
%\end{assumption}
%For simplicity, we denote:
%$$\left\{ \begin{array}{ll}
%     \grad f_t(\bm z_t) &= c_t  \\
%     \grad \hat{f}_t(\hat{\boldsymbol{z}}_t) &= \tilde c_t 
%\end{array}
%\right.$$


\begin{corollary} \label{corollary-oolr}
Under Assumptions 1-4, we derive from \cite[Theorem~1]{mohri} and \cite[Theorem~1]{mhaisen} the following regret bound:
\begin{align}
    \boxed{
    R(T) \leq \sqrt{\sum_{t=1}^T ||\grad f_t(\bm z_t)-\grad \hat{f}_t(\hat{\boldsymbol{z}}_t)||^2}(\frac{2}{\sigma}+\frac{\sigma}{2}2D^2)} \label{eq:regret_bound}
    \end{align}
\end{corollary}




\begin{proof}
First let's remark that the function $h_{0:t}:\bm z \rightarrow r_{0:t}(\bm z) + (c_{1:t}+\tilde c_{t+1})^\top \bm z$ is $1$-strongly convex, with respect to the norm $||.||_{(t)}$. It allows us to use \cite[Theorem~1]{mohri}, which yields regret:
\begin{align}
    R(T) \leq r_{1:T}(\bm z^*) + \sum_{t=1}^T ||c_t-\tilde c_t||^2_{(t),*} \quad \forall \bm z^* \in \Delta^2 \label{lemma}
\end{align}


Now, we define the norm $||x||_{(t)} = \sqrt{\sigma_{1:t}}||x||$, which has dual norm $||x||_{(t),*} = ||x||/\sqrt{\sigma_{1:t}}$.
We remark that $\sigma_{1:t}=\sigma\sqrt{h_{1:t}}$, and starting from \eqref{lemma}, we get:
\begin{align}
    R(T) &\leq \frac{\sigma}{2}\sum_{t=1}^T (\sqrt{h_{1:t}}-\sqrt{h}_{1:t-1})||\bm z^* - \bm z_t||^2
    + \sum_{t=1}^T \frac{h_t}{\sigma\sqrt{h_{1:t}}} \\ \notag
     &\leq \frac{\sigma}{2}\sum_{t=1}^T (\sqrt{h_{1:t}}-\sqrt{h}_{1:t-1})2D^2
    + \sum_{t=1}^T \frac{h_t}{\sigma\sqrt{h_{1:t}}} \notag
\end{align}

We use the first order definition of convexity on the square root function to get:
\begin{align}
    \sqrt{h_{1:t}} - \sqrt{h_{1:t-1}} &\leq    \notag \frac{1}{2\sqrt{h_{1:t}}}(h_{1:t} - h_{1:t-1})\\
    &= \frac{h_t}{2\sqrt{h_{1:t}}} \notag
\end{align}
Thus,
\begin{align}
    R(T) \leq \frac{\sigma}{4}\sum_{t=1}^T \frac{h_t}{\sqrt{h_{1:t}}}2D^2 + \sum_{t=1}^T \frac{h_t}{\sigma\sqrt{h_{1:t}}} \label{intermed}
\end{align}

From \cite[Lemma~3.5]{cesa}, we have:

\begin{align}
    \sum_{t=1}^T \frac{h_t}{\sqrt{h_{1:t}}}  \leq 2\sqrt{h_{1:t}}
\end{align}

Plugging this result into \eqref{intermed}, it yields:

\begin{align}
    R(T) \leq \sqrt{h_{1:t}} (\frac{2}{\sigma} + \frac{\sigma}{2}2D^2)
\end{align}

%We finish the proof by finding the following bound:
%%\begin{align}
 %   \sum_{t=1}^T ||\bm z^* - \bm z_t||^2 \leq \sum_{t=1}^T 2(D_1^2+\ldots+D_m^2) = 2D^2 T \notag
%%\end{align}
\end{proof}


\emph{Remark 1.} We observe that a certain value of $\sigma$ can minimize the upper-bound on the regret, but one has to know the diameter of the decision set $\sqrt{2}D$. The very value of $\sigma$ which minimizes the upper-bound is:
\begin{align}
    \sigma = \frac{\sqrt{2}}{D} \label{eq:sig}
\end{align}

We re-write the upper bound:

\begin{align}
    \boxed{
    R(T) \leq 2\sqrt{2}D\sqrt{\sum_{t=1}^T ||\grad f_t(\bm z_t)-\grad \hat{f}_t(\hat{\boldsymbol{z}}_t)||^2}
   } \label{eq:regret_bound2}
\end{align}

\emph{Remark 2.} The regret bound is in $\mathcal{O}(\sqrt{T})$ if predictions are arbitrarily bad i.e. $\sum_{t=1}^T ||\grad f_t(\bm z_t)-\grad \hat{f}_t(\hat{\boldsymbol{z}}_t)||^2 = \mathcal{O}(T)$, and becomes \emph{null} when the predictions are perfect, i.e. when $\forall t, \quad  \grad \hat{f}_t(\hat{\boldsymbol{z}}_t) = \grad f_t(\boldsymbol{z}_t)$.

\emph{Remark 3.} We implement an online learning prediction method that learns how to predict the gradient with the regret $\mathcal{O}(2mGM\sqrt{T})$, where $G$ and $M$ are key constant in \cite{anava}. % The integration of this algorithm into our main OOLR solution leads to the regret of $\mathcal{O}(T^{1/4})$. 
Other prediction methods could be applied to the prediction of the gradient; however, this online learning method offers sublinear regret guarantees against all types of traces, even non-stationary.

\emph{Conclusion.} We conclude that our OOLR algorithm brings the best of both worlds. Given arbitrarily bad predictions, it provides the same guarantee of sublinear regret as the FTRL algorithm, i.e. $\mathcal{O}(\sqrt{T})$. Associated with an accurate prediction model, it provides tighter guarantee of performance down to $\mathcal{O}(1)$ in the ideal case, i.e. when predictions are perfect $\sum_{t=1}^T ||\grad f_t(\bm z_t)-\grad \hat{f}_t(\hat{\boldsymbol{z}}_t)||^2 = \mathcal{O}(1)$.

\subsection{Complexity analysis}
We stress there that the computational cost and memory requirements of the OOLR algorithm are fairly low. We need to solve at each slot $t$, the problem \eqref{eq:stepOOLR}. We add the term $r_t(\bm z)$ to the previous regularizer $r_{1:t-1}(\bm z)$. We can just replace $r_{1:t-1}(\bm z)$ by $r_{1:t}(\bm z)$ in the same variable to limit storage cost. 
The gradient terms $\grad f_i(\bm z_i)$ are equal to:
\begin{align}
    \grad f_i(\bm z_i) &= \begin{bmatrix}
          -V \frac{a_i \theta_{i,1}}{1+ (\bm x_i + \bm y_i)^\top \bm \theta_t  } + p_{t,1} \\
            -V \frac{a_i \theta_{i,2}}{1+ (\bm x_i + \bm y_i)^\top \bm \theta_i  } + p_{i,2} \\
           \vdots \\
           -V \frac{a_i \theta_{i,m}}{1+ (\bm x_i + \bm y_i)^\top \bm \theta_i  } + p_{i,m} \\
           -V \frac{a_i \theta_{i,1}}{1+ (\bm x_i + \bm y_i)^\top \bm \theta_i  } + q_{i,1} \\
           -V \frac{a_i \theta_{i,2}}{1+ (\bm x_i + \bm y_i)^\top \bm \theta_i  } + q_{i,2} \\
           \vdots \\
           -V \frac{a_i \theta_{i,m}}{1+ (\bm x_i + \bm y_i)^\top \bm \theta_i  } + q_{i,m}
         \end{bmatrix}. \label{eq:gradient}
 \end{align}
 Thus, at each slot $i$, we need to store the vectors $\bm p_i$, $\bm q_i$, $\bm \theta_i$ of length $m$ and the scalar $a_i$. Therefore, memory requirements are of $3m+1 = \mathcal{O}(m)$. We can just replace the gradient term $\sum_{i=1}^{t-1} \grad f_i(\bm z_i)$ by $\sum_{i=1}^{t} \grad f_i(\bm z_i)$ in the same variable to limit storage cost. 
The computation of the gradient \eqref{eq:gradient} runs in $2m(m+4)$ operations. The computation of the regularizer term \eqref{eq:reg} runs in $4m$ operations for $||\bm z - \bm z_t||^2$ and $4m+2$ operations for $\sigma_t$. Therefore the running time for \eqref{eq:stepOOLR} is in $2m(m+4)+8m+2=\mathcal{O}(m^2)$.

The complexity of the OOLRgrad algorithm is higher as we must take account of the complexity of the prediction module ARMA-OGD from \cite{anava}. We apply the ARMA-OGD to each gradient item separately. For one item, the prediction consists of an online gradient descent update of the $q$ lag coefficients. Then, the prediction is the linear combination of the $q$ previous real values of the gradient weighted by the $q$ lag coefficients. Thus, the running time of ARMA-OGD applied to our specific case is in $\mathcal{O}(mq)$. The memory requirements of the ARMA-OGD are $2m2q$ as we must store the last $q$ observations of the real gradient and the $q$ lag coefficients, for each item of the gradient. Therefore, memory requirements are of $\mathcal{O}(mq)$.

We conclude that both OOLR and OOLRgrad have fairly low running time and memory requirements, given that the number of resources $m$ composing one slice is not too high, which is practically the case.


%\emph{Remark 4.} We cannot set the value of $\sigma$ like we did in \eqref{eq:sig} while ignoring the horizon $T$. If we ignore the latter, which is possible in some settings, we must set the value of $\sigma$ according to the doubling trick. This consists in dividing the horizon $T$ in $\log_2(T)$ periods, then iterate over the periods $k=0,1,\ldots,log_2(T)$, and set for the slots of period $k$ which are $t=2^k,2^{k}+1,\ldots,2^{k+1}-1$ the value:
%$$\sigma = \frac{\sqrt{2}}{D\sqrt{2^{k+1}}}.$$
%This method will conserve the $\mathcal{O}(T^{3/4})$ regret bound, which will worsen only by a constant multiplicative factor. % of $\sqrt{2}/(\sqrt{2}-1)$. 
%We refer the reader to the proof \cite[Section~2.3.1]{shalev-book}.



