vggnet
This review covers a number of network models in deep learning. Some of the more frequent, novel and promising network models that appear in this review are described next.
vggnet
This network model, proposed by xxx, won second place in the ImageNet Large Scale Visual Recognition Competition in 2014, and is often used to extract features from images. This network model demonstrates that increasing the depth of the convolutional neural network and the use of small convolutional kernels can have a significant effect on the final recognition results of the network.
Nowadays, vgg-16 and vgg-19 are more commonly used. vgg-16 has 13 convolutional layers with 3*3 kernels, 5 maximum pooling layers and 3 fully connected layers, while vgg-19 has only 3 more convolutional layers.
The vgg network requires a sufficiently large training data set, and it is not possible to train a vgg network adequately if the training set does not have enough samples.

fcn

The main difference between a fully convolutional network and a CNN network is that the FCN replaces the fully-connected layer in the CNN with a convolutional operation.
With the change to full convolutional operation, the input to the convolutional layer can accept images of different sizes because there is no limitation on the number of neurons in the input layer of the fully connected layer, and there is no need to require the training image and the test image to be the same size. However, because the window of perceptual field mapped to the input image is fixed for each value of the output result, i.e. the detection window is fixed, resulting in less effective detection. cite8


\section{summaries of paper}\label{sec4}

\subsection{Iris Recognition}

\subsubsection{Recognition based on VGGNet}

\subsubsection{Recodnition based on DBN}

\subsubsection{Recognition based on GAN}

\subsubsection{Recogniton based on Convolutional Auto-Encoder (CAE)}

\subsubsection{Recognition based on other deep CNN}

\subsection{Segmentation}

\subsubsection{Segmentation based on U-Net} 

\subsubsection{Segmentation based on other deep CNN}

\subsection{Presentation attack detection}

\subsection{Other tasks}


\subsubsection{Support vector machine}

SVM~\cite{38} is popular as a machine learning algorithm for classification in iris recognition~\cite{7, 33, 34, 35, 36, 37, 39, 40, 41}. It is usually combined with a variety of network models used to extract features to perform iris recognition tasks.

\cite{35} uses a pre-trained AlexNet for feature extraction and then uses an SVM to classify these features. AlexNet is the first convolutional neural network to succeed in image classification and won the ILSVRC2012 championship~\cite{13}. The experimental results of this method not only validate the powerful feature extraction capability of this classical deep neural network but also illustrate that the segmented iris images can achieve better classification performance than normal images on public datasets. Again for feature extraction, \cite{37} introduce a novel network that is an adaption of DenseNet-161, called \emph{Densely Connected Contact Lens Detection Network} (DCLNet). Unlike the original DenseNet-161~\cite{43}, the DCLNet keeps only two dense blocks for feature extraction, which effectively reduces the complexity of the network. Finally, SVM is used for classification. The model of DCLNet with SVM achieves a \emph{Correct Classification Rate} (CCR) of 99.10\% on IIITD.

In addition to simply using a deep neural network for feature extraction, \cite{36, 41} perform dimensionality reduction using \emph{Principal Component Analysis} (PCA)~\cite{42} on the features extracted by the network to reduce the complexity of the features. PCA is a representative dimensionality reduction method that removes redundant information from the original data and retains important features, thus reducing the complexity of the original data. The models used for feature extraction in these two studies are ResNet-50 and VGG-16, respectively. And they both use SVM to perform the classification task. The experimental results show that these two studies obtain the \emph{False Match Rate} (FMR) of 97.93\% on CASIA-Iris-Thousand and the recognition rate of 99.4\% on IIT.

To further validate the satisfactory classification capability of SVM, \cite{34} compares the classification performance of \emph{K-Nearest Neighbor} (KNN) and SVM. These two algorithms classify the features extracted by a CNN. The experimental results show that SVM has superior classification performance than KNN. This is due to the SVM being able to perform classification in the higher dimensional space than KNN. In contrast, KNN is disabled to handle large datasets and it is more sensitive to the noise introduced due to image enhancement.

%PAD
%\textcolor{blue}{To enhances the identify verification performance,}  \textcolor{blue}{The method of}~\cite{39} \textcolor{blue}{fuses local features and global features extracted from the iris image for verification, and uses SVM to classify these features. The experimental results show that the accuracy of the feature-level fusion and score-level fusion of this method reaches xx.xx\% and xx.xx\%, respectively. }

%这篇文献放端到端
%SVM classifiers are advantageous, and~\cite{33} confirms that CNN-based methods perform better in the same sensor experiments compared with SVM classifiers, while generalising slightly less well to unknown sensors.

%这篇文章是PAD的
%The SVM classifier also performs contact lens detection in~\cite{7}. The fusion of the hand-crafted feature extraction method and the data-driven feature extraction method is performed on the classifier after each feature extractor is associated with a dedicated SVM classifier to produce local classification scores, and the final results are obtained by fusion of the top k-features at the score-level. The results demonstrate the fusion of handcrafted feature descriptors with their data-driven counterparts combined with SVM classifiers to improve iris PAD and contact lens discrimination. In~\cite{7}, after classification by SVM, the final result of the output is obtained by score-level fusion of the top k-features. 

\subsubsection{Deep neural network}

\cite{64} extracts optimised ordinal measures (OMs) features , and then the similarity score of HD is obtained by calculating the Hamming distance (HD) of the OMs features. Pairs of features are then automatically learned to measure the correlation between two irises using a CNN, which in this thesis is an end-to-end method that learns non-linear expressions for pairs of iris images directly and gives a final similarity score. To explore whether the selected OMs features are complementary to the paired features learned by the CNN, the thesis performs a fraction-level fusion of the two features.The experimental results show that the OMs features and the paired features are highly complementary and effective for iris recognition on mobile devices.
 
\cite{65} is to calculate the Hamming distance while considering the mask of an iris image pair. A given iris image is input, and after iris segmentation and normalisation, a fixed size normalised image is obtained containing only the iris region and a binary mask image. The normalised image was then fed into the network for training. The output is used to represent the iris texture during the validation process. On the feature map of the last convolutional layer, ordinal measures are computed to model the sequential relationship of feature pairs before iris matching is performed.
 
\cite{66} proposes a deep learning method for iris recognition based on a capsule network structure. The thesis adapts the structure of the network. In order to improve the applicability of the capsule structure and make the network converge easily when processing large size input images, the thesis proposes an improved routing algorithm based on dynamic routing between the two capsule layers, called the DRDL algorithm. The direction and length of the capsules are used as evaluation metrics for the similarity of the two vectors, instead of the original single cosine similarity.
Migration learning is also used, making the method usable even when the number of samples is limited. In addition, the paper divides the three networks, VGG16, InceptionV3 and ResNet50, into a series of sub-network structures based on the number of their main constituent blocks. They are used as the convolutional part of the extracted main features, rather than the individual convolutional layers in the capsule network. Experiments were conducted on three iris datasets, JluIrisV3.1, JluIrisV4 and CASIA-V4 Lamp, to analyse the performance of the different network structures.

The model input for~\cite{67} is captured from the original eye images. The iris-specific Mask R-CNN was then used to detect the position of the iris and segment the iris region pixels. A normalisation layer is created and applied to fit the circle and normalise the iris. The spatially corresponding feature maps are then extracted by FeatNet for feature learning or matching.

An "FMnet" algorithm for iris recognition using FCN and Multi-scale Convolutional Neural Networks (MCNN) is proposed in~\cite{68}. It extracts the iris recognition frame using FCN segmentation, then normalises it, and then extracts the features using MCNN. Recognition accuracy of 99.41\% was achieved on UBIRIS.v2.

\cite{69} segments and normalises the noise-free iris regions of the learning set and manually annotated pairs of regions with corresponding iris patches of the same bioregion. These correspondences are remapped to Cartesian coordinates and immediately adjacent to a pseudo-polar coordinate system that does not require image segmentation, where the corresponding regions are learned using VGG-19.

\cite{70} uses a segmentation network with an light U-Net structure to extract pupil and iris features from the video images to perform the Pixel-level classification of eye images. The extracted feature maps are then used to guide the decision network to estimate the openness of the eyes. Finally, the detection method is tested using the National Tsing Hua University drowsy driver detection video dataset, with a fatigue detection accuracy of 96.72\%.

\cite{71} uses black hat processing, median processing and gamma correction to improve the quality of the input images in the Pre-processing of the images. The Hough circle transform model is then applied to the region of interest for effective localisation of the iris. A convolutional neural network (Inception v2 model R-CNN) is then used for plausible iris recognition and segmentation, To validate the results of this model, detailed simulations are performed using the benchmark CASIA-Iris thousand dataset and intervals of detection accuracy are verified. The simulation results show that the projection technique has a maximum recognition accuracy of 99.14\%.

\cite{72} proposes a biometric authentication system with cancelable biometrics. A multi-instance cancellable iris system (MICBTDL) is proposed. MICBTDL uses a CNN trained using triple loss for feature extraction and stores the feature vectors as cancellable templates. The system uses an artificial neural network as a comparator module rather than a similarity metric. Experiments were conducted on the IITD and MMU iris databases to verify the effectiveness of MICBTDL.

DenseNet-201 is used in~\cite{73} to perform iris recognition on the open set. A CNN trains on the closed set is first used as the feature extractor. For each identity, a separate binary classifier is trained and then the same number of classifiers are generated. This step allows the model to use a simple similarity metric for classification, speeding up the recognition phase and simplifying the registration process. Experimental results are 97.3\% recognition accuracy in closed set recognition and 98.5\% recognition accuracy in open set recognition.

An effective defence mechanism against adversarial attacks is proposed in~\cite{74} for detecting examples of adversarial iris. The proposed defence mechanism is based on the discrete wavelet transform (DWT), which investigates the high and medium spectrum of wavelet subbands. The model then reconstructs various denoised versions of the iris image based on the DWT. A deep convolutional architecture based on U-net is used for further classification. Experimental analysis of a benchmark iris image database, IITD, yielded good results with an average accuracy of 94\%. The experimental results show that the strategy outperforms other advanced defence models in detecting adversarial attacks.

\cite{78,75,76,77} all use DBN for classification. 
\cite{78} uses contour-based feature vectors to distinguish the difference between iris and pupil contours. In addition, the contours of the border are extracted using a radius vector function. A modified feed-forward neural network based DBN is used in the classification stage.

\cite{75} proposes an adaptive Gabor filter selection strategy and a deep learning classification architecture. The architecture uses adaptive Gabor filters determined by particle swarm optimization and binary version binary particle swarm optimization rules to fit the band involving the richest iris information. The iris Gabor code is then placed into the deep learning architecture DBN to detect potential learning features of the iris in a data-driven manner. The Gabor codes of all registrants and their corresponding labels are assigned to train the DBN model. Finally, the converged DBN algorithm is used to implement the feature extraction and classification of the test iris.

An effective superimposed convolutional DBN combined with DBN is proposed in~\cite{76} as a neural network model for multi-source heterogeneous iris recognition. The model first uses a region extraction method to locate effective local texture feature structures by locating convolutional kernels through the offset of hidden layers. Secondly, the model uses DBNs as classifiers to reduce reconstruction errors through the negative feedback mechanism of the autoencoder.

\cite{77} also uses DBN for classification. The image is first pre-processed. The iris region is then extracted using the Hough transform, and then the iris region is segmented and normalised using the Dagerman rubber sheet model. Once segmentation is performed, features are generated by a combination of the Scattering Transform, Tetrolet Transform, Local Gradient Pattern and Local Optimal Orientation Pattern. Finally, the iris is classified using the steepest gradient-based DBN.

\subsubsection{Other classifier}
\cite{79} creates two CNN-based models, the first being unsupervised, fusing the scores of two external feature vectors and a baseline root SIFT model. The second is a supervised model that uses only images from the MICHE-II dataset and uses a cosine similarity measure on the derived feature vectors to measure the similarity between image pairs. Experimental results show that the second model outperforms the first model and that the combined model outperforms the two independent models.

\cite{80} also uses cosine distance for classification, but the difference is that the model uses segmentation and normalisation techniques in the first stage. The VGG and ResNet-50 networks are used for the feature extraction phase.

SVM is more commonly used, but~\cite{81} compares the performance of three recognition algorithms, k-nearest neighbour (KNN), SVM and kernel-based extreme learning machine (KELM). The experimental results show that KELM performs better in comparison with the other two for the case of CNN iris and CNN contextual eye image feature fusion recognition. The model utilises descriptors such as Log-Gabor, Contourlet transform, gradient local autocorrelation and CNN to extract features. Then, iris classification is performed using each of the three algorithms mentioned above. The results also show that KELM has good iris recognition accuracy for all the above feature descriptors.

\cite{82,83} use multiclass perceptrons in the classification phase. \cite{82} uses Multi-Layer Perceptrons (MLPs) and CNNs as classifiers to compare features produced using CNNs and handcrafted features.
When using the MLPs, handcrafted features like Local Binary Pattern produce better prediction accuracy than features extracted by CNNs. However, in a similar experiment using an entire eye image, CNNs and MLPs had the same performance using the learned features.
\cite{83} proposes a new periocular assisted iris recognition framework using MLP for less constrained iris recognition. Iris images in a less constrained imaging environment tend to present different regions of valid iris pixels for iris matching. This difference in the effective number of available iris pixels can be used to dynamically enhance the periocular information obtained from the iris image simultaneously.

The identification of~\cite{84} is a matter about variable selection and regularisation. The model uses sparse linear regression techniques to infer the probability of matching with respect to each gallery recognition. The strategy is robust to outlier matching scores, which are a major source of error in biometric recognition.\cite{85} proposes to join a supervised discrete hashing scheme to significantly reduce the size of iris templates. It also uses the Hamming distance of the CNN-based extracted features to improve the template matching speed.
\cite{86} uses a multi-layer similar convolutional structure to reduce the computational complexity, reduce the dimensionality of the iris texture information and obtain the main texture information in the iris. Finally, the iris textures are extracted and classified using a collaborative representation approach.



\subsection{end-to-end}

Definition, role, importance of iris recognition, etc.
With the development of deep learning techniques, researchers have combined iris recognition with deep learning techniques to come up with a series of iris recognition algorithms, such as.
1.
2.
3.
.
.
.
During the development of deep learning based iris recognition, a number of representative algorithms have been proposed which have achieved good results. (then list, methods, data, correct recognition rates, etc.)
Table
With the rise of cnn, iris recognition has increasingly used deep learning techniques. During this period, xxx, xxx and xxx proposed xxx were relatively successful models in the field of iris recognition. In particular, xxx, (kudos), is of great scientific significance and has greatly contributed to the development of iris recognition technology. Although, in recent years iris recognition technology has achieved good results in various aspects, its shortcomings are still quite obvious, for example, (pose a challenge). What has been done by whom in this area to.
1.
2.
.
.
However, there are still some challenges, (for example) In short, solving the above problems is the direction of further research in iris recognition at the moment.


The parameters of traditional iris feature extraction methods are usually determined manually, which is often time-consuming and sub-optimal~\cite {97}. Such as~\cite{77}, it extracts iris features using a combination method based on scattering transforms, Tetrolet transforms, local gradient patterns and local optimal orientation patterns. The traditional machine learing methods like this are costly. Deep learning methods, which can learn filters automatically from massive amounts of data, have recently shown explosive popularity, particularly CNNs, which have been successfully applied to pattern recognition and have achieved superior results over most traditional methods.

In~\cite{64}, the feature extraction method contains both manual and data-driven extraction. It extracts \emph{Oridinal Measures} (OMs) feature from local iris texture while also automatically extracting paired features from iris images using CNN. The CNN obtains the paired features to measure the similarity between two input images. Finally, fuses the two features at the score-level method. The experiment results demonstrate the OMs features and the paired features by CNN automatically acquire are complementary and effective in the iris recognition tasks. Similarly, \cite{81} compares the effectiveness of some manual feature extraction methods with CNN feature extraction. The manual schemes include Log-Gabor, Contourlet Transform, and Gradient Local Auto-Correlation. Specifically, feature extraction is performed on segmented normalized iris image and contextual eye image, not on the original iris image. The experimental results show that the CNN method of extracting features outperforms all the manual methods mentioned above. 

As DL research continues to evolve, manual feature extraction methods are no longer favored by researchers. The use of deep neural networks to extract features is becoming a trend in the field of iris recognition. \cite{11} comp ares the feature extraction capabilities of some popular deep CNNs. These networks include AlexNet, VGGNet, GooLeNet, ResNet, and DenseNet.
These deep CNNs have tens or even hundreds of layers, and millions of parameters, they are very good at capturing and encoding the complex features of iris images. Because each layer models different levels of visual content in an image. Earlier layers retain coarser information and later layers encode finer and more abstract information. This study uses the output of each layer as a feature descriptor and reports the corresponding recognition accuracy. The experimental results show that DenseNet performed the best out of the five CNNs, with the highest peak recognition accuracy of 98.7\% in layer 6 of the LG2200 dataset and 98.8\% in layer 5 of the CASIA-Iris-1000 dataset. Meanwhile, these preliminary results show that existing CNN features can be successfully transferred to the iris recognition problem, resulting in the effective extraction of discriminative visual features from iris images and eliminating laborious feature engineering tasks. In subsequent studies, more and more researchers have started to use CNNs for feature extraction. \cite{35} uses a pre-trained AlexNet to extract features and uses the SVM to perform classification. \cite{70} uses a light U-Net to extract pupil and iris features from video images to perform \textcolor{red}{pixel-level classification of eye images.}
The ResNet-101\cite{98} and DenseNet-201\cite{99} architectures were used in\cite{73}. ResNet introduces the concept of skip or residual connectivity, where the output skips one layer and is fed to the next. DenseNet makes use of the concept of densely connected layers. Each layer receives additional input from all previous layers, and each layer also passes its feature mapping to subsequent layers. The inputs to the layers are connected to the passed feature mappings. This makes the network more compact and improves computational and memory efficiency. Both models performed well, achieving an accuracy of 96\%.
\cite{72} proposes a \emph{Multi-Instance Cancelable Iris System} (MICBTDL). MICBTDL uses a CNN trained using triple loss for feature extraction and stores the feature vectors as cancelable templates

\cite{66} \textcolor{red}{proposes a deep learning method for iris recognition based on a capsule network structure. The thesis adapts the structure of the network. In order to improve the applicability of the capsule structure and make the network converge easily when processing large size input images, the thesis proposes an improved routing algorithm based on dynamic routing between the two capsule layers, called the DRDL algorithm. The direction and length of the capsules are used as evaluation metrics for the similarity of the two vectors, instead of the original single cosine similarity.}

\begin{sidewaystable}[htbp]
    \centering
    \caption{Summary of the FVR based on deep neural networks.}
    \label{tab3}
    \resizebox{\linewidth}{!}{\begin{tabular}{ccllll}
        \toprule
        \textbf{Year} & \textbf{Reference} & \textbf{Task} & \textbf{Network} & \textbf{Dataset} & \textbf{Performance} \\
        \specialrule{0em}{3pt}{3pt}
        \hline
        \specialrule{0em}{4pt}{4pt}
        2015 & \cite{126} & Verification & DBN & Private (IN:6000 SN:64) & ACC = 96.9\% \; EER = 1.5\% \\
        \specialrule{0em}{4pt}{4pt}
        2015 & \cite{245} & Image Q-Assessment & Custom & \makecell[l]{HKPU \\ FV-USM} & \makecell[l]{ACC = 88.89\% (HI) \; ACC = 88.18\% (LI) \\ ACC = 74.98\% (HI) \; ACC = 70.07\% (LI)} \\
        \specialrule{0em}{4pt}{4pt}
        2016 & \cite{130} & Verification & LeNet-5 & Private & ACC = 96.78\% \\
        \specialrule{0em}{4pt}{4pt}
        2016 & \cite{159} & Verification & Constom & UTFVP & ACC = 100\% \\
        \specialrule{0em}{4pt}{4pt}
        2017 & \cite{99} & Verification & AlexNet & Private & ACC = 99.4\% \; EER = 0.21\% \\
        \specialrule{0em}{4pt}{4pt}
        2017 & \cite{113} & Verification & VGG-16 & \makecell[l]{SDUMLA-HMT \\ Private1 (IN:1200 SN:20) \\ Private2 (IN:1980 SN:33)} & \makecell[l]{EER = 0.396\% \\ EER = 1.275\% \\ EER = 3.906\%} \\
        \specialrule{0em}{4pt}{4pt}
        2017 & \cite{114} & Verification & VGG-16 & \makecell[l]{Private1 (Outdoor continuous acquisition)  \\ Private2 (Indoor intermittent acquisition)  \\ Private3 (Indoor continuous acquisition ) \\ } & \makecell[l]{EER = 0.42\% \\ EER = 1.41\% \\ EER = 2.14\%} \\
        \specialrule{0em}{4pt}{4pt}
        2017 & \cite{229} & PAD & Custom & \makecell[l]{IDIAP \\ SCUT} & \makecell[l]{ACER = 0.00\% \\ ACER = 0.00\%} \\
        \specialrule{0em}{4pt}{4pt}
        2017 & \cite{101} & Verification & AlexNet & \makecell[l]{SDUMLA-HMT \\ Private (IN:2970 \; SN:198)} & \makecell[l]{EER = 0.80\% \\ EER = 0.079\%} \\
        \specialrule{0em}{4pt}{4pt}
        2017 & \cite{127} & Verification & DBN Custom & Private (IN:960 SN:15) & ACC = 99.6\% \\
        \specialrule{0em}{4pt}{4pt}
        2017 & \cite{246} & Image Q-Assessment & Custom & \makecell[l]{FV-USM \\ HKPU} & \makecell[l]{ACC = 71.01\% (HI) \; ACC = 73.57\% (LI) \\ ACC = 87.08\% (HI) \; ACC = 86.36\%} \\
        \specialrule{0em}{4pt}{4pt}
        2017 & \cite{258} & Image enhancement & Custom & \makecell[l]{HKPU \\ FV-USM} & \makecell[l]{EER = 2.70\% \\ EER = 1.42\%} \\
        \bottomrule
    \end{tabular}}
\end{sidewaystable}

\cite{53} proposes HCNN and MFCN models. The common advantage of these two iris segmentations is that they can be used for long-distance iris images and noisy iris images acquired while moving. And both models can automatically locate iris pixels without the need for hand-crafted functions or rules, reducing labor costs. 
Compared with HCNN, MFCN accepts input of any size and generates corresponding size output without sliding window prediction, which makes MFCNs more effective. In MFCNs, shallow, thin, and deep, global layers are combined to capture the texture details and global structure of the iris pattern. The experimental results show that MFCNs have stronger robustness than HCNNs, which is greatly improved by 25.62\% and 13.24\% on UBIRIS.v2 and CASIA.v4-distance databases, respectively.

In addition to normal IR tasks in vivo, the recognition of mortem iris images is likewise a task of interest to researchers. Because this approach is able to help for forensic identifiction in some specific scenarios, is also has medical pathology research value. So~\cite{20} summaries the existing methods about the post-mortem IR, and these methods are discussed from the classical techniques represented by Gabor filtering to the modern techniques represented by deep learning, and the performance of some representative state-of-the-art methods are evaluated.

\textcolor{green}{In contrast,~\cite{15,17} are reviews of some of the more specific aspects of IR research, such as~\cite{15} discussing current iris segmentation methods. As can be seen from the literature surveyed in this total, many techniques have been proposed to implement iris segmentation based on edge details. The article discusses these segmentation methods according to Edge based techniques, Histogram and thresholding techniques, Clustering techniques and Contour evolution methods. Segmentation using clustering, thresholding and deformable models all yielded relatively good results. Contour evolution methods based on level sets are under development. However, no single edge-based method addresses all the challenges encountered in non-ideal iris databases. \cite{17} reviews various approaches to IR that rely on neural networks within 2011-2019. The review proposes the use of neural networks for IR, obtaining many effective algorithms and good results. In addition, the review summarises the background of many different fields of iris image recognition techniques and compares these techniques.
\cite{32}  focuses on the analysis of IR articles conducted in a timeline. Most of the iris datasets used are also presented.}


\cite{52} presents the \emph{Fully Convolutional Neural Network} (FCN) structure, which is originally proposed for semantic object segmentation tasks, but the network structure accepts arbitrarily sized inputs and produces outputs of corresponding sizes through efficient inference and learning, and it has been shown that this network structure improves the segmentation performance of iris images.

To make the model better adapt to iris images segmentation tasks, some studies uses the original FCN as the backbone to optimize the model. \cite{54} improves the up-sampling method in the original FCN by combining the current feature channels with the corresponding feature channels, enabling down-sampling layers to require iris image with high resolution. \cite{55} adds dilated convolution to the FCN, which enables the network to extract the more globalized features from the iris image than normal convolution. Inspired by DenseNet, \cite{60} adds dense block to the network,and each dense block contains three convolution layers. This dense block composites the feature values from the bottom layer to the top layer, which can effectively suppress overfitting and gradient vanishing. In contrast to the above methods, \cite{62} does not changes the network structure, but sets the segmentation targets to multi-classes instead of just iris and non-iris regions. The training process of semantic segmentation is more complex than the segmentation based on two-classes, but captures more abundant semantic information from the original iris image.

In order to extract more detailed features from the iris image, \cite{53} proposes two different iris segmentation models that are called \emph{Hierarchical Convolutional Neural Network} (HCNN) and \emph{Multiscale Fully Convolutional Network} (MFCN), respectively, and both models can automatically locate iris locations instead of rely on manual production. In HCNN, the same iris image is resized into three patches of different size, which are fed to each of the three CNN channels to learn features. Finally, a fully connected layer is used to label the every pixel on the image. However, this computational volume of this model is complex since some patches are repeatedly computed, and the fixed size input size limits the receptive fields of conlvolution layers. To overcome these problems, this study ten proposes MFCN without the fully connected layer, where images can be input at arbitrary sizes. Besides, The structure of MFCN is designed with six layers as shown in Fig.~\ref{HCNNMFCN}, and each layer has its own output. This design allows the MFCN to accurately capture rough and detailed features in te iris image. The experimental results show that MFCN has more noise immunity than HCNN, and MFCN obtains the EER of 0.29\% on UBIRIS.v2 and 0.20\% on CASIA-V4-Distance.

\begin{figure}[htbp]
    \centering
    \includegraphics[width = 1.0\textwidth]{HCNNMFCN.pdf}
    \caption{The structure of HCNN and MFCN~\cite{53}. \textbf{a} HCNN. \textbf{b} MFCN.}
    \label{HCNNMFCN}
\end{figure}

The fully convolution encoder-decoder structure is widely used in image segmentation. \cite{55, 56, 58, 63} use encoder-decoder network to perform the iris image segmentation task. The encoder and decoder of~\cite{55} are both 44 layers, and~\cite{55} introduce the dropout to inhibit overfitting and Monte-Carlo sampling to output the model uncertainty for each class. The experimental results show that the network with the new strategy has better segmentation performance than the original encoder-decoder structure. \cite{56} uses the same encoder-decoder network as~\cite{55} for iris image segmentation. Besides, to solve the problem of lack of labeled data in the IR task, this study designs two domain adaptation methods based on linear mapping and non-linear mapping, respectively. These domain adaptation methods transfer the data distribution from the source domain to the target domain to generate a new sample distribution, then train the encoder-decoder network in the new sample distribution, and finally use the trained model to segment the iris images on the target domain. The F score is the harmonic mean of the fraction of relevant instances among the retrieved instances and the fraction of relevant instances that have been retrieved over total relevant instances. The experimental results show that this method obtains the F score of 0.925, 0.937, and 0.951 on CASIA5a, CASIA4i, and IITD, respectively. In~\cite{58}, the encoder structure consists of the first 13 layers of the VGG-16, and the feature maps generated by the fifth pooling layer are used as the input to the decoder. The decoder uses three transposed convolution layers for up-sampling, and adds the skips layers to extract features of high resolution. Experiments on various public datasets show that the \emph{Average Segmentation Error}(ASE) of the network is satisfactory. The model of \cite{63}emulates the training process of GAN and not just an segmentation network based on encoder-decoder. The overall framework consists of a generator composed of a encoder-decoder network and a CNN-based discriminator, and a skip connection is added between the encoder and decoder to fuse shallow and deep feature. In the training of this model, the ground truth image and the segmented image generated by encoder-decoder are fed to the discriminator, and the loss is back propagated into encoder-decoder to opeimize this network. This training approach further improves the segmentation performance of this model. The detailed design of this framework is shown in Fig.xx. In the encoder, three residual blocks are densely connected to enhance the ability of feature extraction. Meanwhile, this study employs the multi-supervised training of multi-scale image to decrease the influence of image size. The experiments results show that this framework acieves the \emph{Mean Intersection over Union}(mIoU) of 95.35\% on UBIRIS.v2 and 96.75\% on CASIA-Iris-Thousand.

%\begin{figure}[htbp]
%    \centering
%    \includegraphics{}
%    \caption{Caption}
%    \label{SegGAN}
%\end{figure}

%Later, \cite{54} researchers combined a new FCN iris segmentation model with an improved resnet-18 iris matching model in order to test the impact of FCNs for iris segmentation on IR. In particular, the proposed FCNs model has better iris segmentation results considering existing IR methods in some complex cases. The segmentation model allows arbitrary size inputs and produces iris images of the same size. The structure of the segmentation network consists of an iteratively varying FCN that contains a systolic layer path that captures features and a symmetric upsampling path to provide accurate pixel-to-pixel localisation. In iris validation experiments on the CASIA-Iris-Interval dataset, the method showed good performance, obtaining an intra-class accuracy of 90.85\% and an inter-class accuracy of 99.59\%. With the appropriate combination of these two networks, the method can achieve 95.26\% intra-class and 99.33\% inter-class accuracy.

%In order to make the acquisition conditions more unconstrained, \cite{55, 56} are both based on fully convolutional encoder-decoder networks and both have redesigned the softmax layer to segment only the iris and non-iris regions.\cite{55} used a non-CNN approach to the iris segmentation algorithm on the same dataset in order to perform a realistic evaluation of the network and evaluated its performance in a similar way. The results obtained were compared with those obtained using a fully convolved coder-decoder network. The results show that the network achieved better performance than the non-CNN segmentation algorithm on all the datasets used.

%\cite{56} proposes training models for adaptive iris segmentation of pixel-level domains. The method can effectively transfer the source dataset domain to the target domain, generating new adaptive datasets. The adaptive datasets are then used to train CNN to segment the iris texture in the target dataset, eliminating the need for target marker data. It is also noted that training a particular CNN for a new iris segmentation task, maintaining the best segmentation score, is possible using only a very low number of training samples.

%\cite{57} proposes an end-to-end \emph{Fully Convolutional Deep Neural Network} (FCDNN) using network that is FCN with different depths and kernel sizes, each designed to extract different levels of detail on low quality iris images for the iris segmentation task. The network is initially trained on NIR images and then tuned to additional datasets obtained from visible images. Finally, the optimal tuning of the network is compared with some other state-of-the-art iris segmentation algorithms. The results show that this FCDNN design has excellent performance compared with the techniques applied to the same low quality dataset.

%\cite{58} researchers introduce two methods for robust iris segmentation based on FCNs and \emph{Generative Adversarial Networks} (GANs) in order to further improve the segmentation performance of iris. Similar to a common convolutional network, but without fully connected layers (i.e. classification layers), an FCN ends up using a combination of pooling layers from different convolutional layers. Based on game theory, the GAN is designed as two networks competing with each other to generate the best segmentation.

%The researchers in~\cite{59} choose to combine FCN with dilated convolution, with reduced downsampling factors, to extract more global features in order to make the segmentation network model better able to handle details and it does not require any pre-processing and post-processing, such as resizing the input image to a fixed size, in a non-cooperative environment where the quality of the iris image cannot be guaranteed. The experimental results of~\cite{59} show that the network model reduces the error rate by 79\%, 84\%, and 79\% on the CASIA-V4-Interval, IITD dataset and UBIRIS v2 datasets respectively.

%The researchers in~\cite{60}, on the other hand, propose a CNN combined with dense block based iris segmentation architecture, called \emph{Dense-Full Convolutional Network} (DFCN), which adaptively segments out the iris region of the iris image and employs some popular optimisation methods such as \emph{Batch Normalisation} (BN) and dropout. Experimental results based on CASIA-V4-Interval, IITD, and UBIRIS.v2 iris dataset capture under different conditions show that the iris segmentation network proposes in this paper outperforms all conventional and most cnn-based iris segmentation algorithms.

%The researchers of~\cite{61} have redesigned the internal blocks of the FCN in a way that combines the latest results in the fields of classification and semantic segmentation. The design removes all maximum pooling layers, which are replaced with layered convolutions. The researchers of~\cite{61} state that this design helps to slightly improve accuracy and reduces the tuning of the learning curve at the beginning of training. Additionally, the researchers of~\cite{61} believe that this allows for a more lightweight network to be shipped to a variety of embedded platforms, allowing for successful operation in less controlled environmental conditions. To simulate realistic environmental scenarios and test the generalisation capabilities of the developed network, the researchers of~\cite{61} modifies the CASIA-Iris-Lamp dataset by adding more challenging contrast and luminance variations, obtaining IoUs of 0.93 and 0.92 on both the original and modified CASIA-Iris-Lamp datasets, a significant improvement. Both architectures show higher accuracy results, maintaining reasonable memory consumption and allowing them to be used in a wide range of embedded solutions.

% In addition to changing the structure of the network itself improving the training process of the network to capture rich semantic information from images can also improves the segmentation performance of the model.Most of the existing segmentation methods are only two-class about iris and non-iris region. Contrary to this, \cite{62} introduce a segmentation approach based on multi-class to obtain more abundant semantic information from iris image. In this segmentaton scheme, more biological information is distinguished such as pupil or sclera, instead of just splitting the image into iris region and non-iris region. The results of experiments conduct on two publicly available datasets show that the accuracy of iris segmentation was improved using the proposed multi-class approach.

%There is a trend to make network models as lightweight as possible, making them embeddable in mobile devices, and the researchers in~\cite{63} propose an optical iris segmentation method base on a fully convolutional network to develop a lightweight fully convolutional iris segmentation network. Secondly, the researchers in~\cite{63} use weighted loss, multi-level feature-dense fusion modules, multi-supervised training of multi-scale images and generative adversarial networks to improve segmentation performance. Experiments in~\cite{63} show that the method achieved 99.30\% PA, 95.35\% mIoU on UBIRIS.v2, 99.66\% PA on the CASIA-Iris-Thousand dataset, and 96.75\% mIoU, which is relatively encouraging for optical iris segmentation networks. Segmentation of images from the UBIRIS.v2 and CASIA-Iris-ik000 datasets require 41.56 ms and 63.03 ms, respectively.

\subsection{Segmentation based on U-shaped neural network structure}
The U-Net is a fully convolutional neural network for segmentation, that was first used in the field of medical images. However, benefiting of its special U-shaped structure, U-Net has shown excellent performance on many segmentation tasks. Therefore, many researchers use U-Net or modified U-shaped network to perform the segmentation task of IR.

\cite{44} investigates the effect of different hyper parameters on the segmentation performance of a trained U-Net model. \cite{44} researchers propose that the CNN-based segmentation model is very successful in segmenting irises and also outperforms all the baseline methods considered. The model does not require a large amount of training data and works well without the use of data augmentation during training. In conclusion, the use of deep learning methods in IR can improve performance in the field of biometrics compared with traditional methods. The model is a trainable end-to-end model that avoids the need for hand-designed segmentation procedures.

\cite{45} also comes to the same conclusions as~\cite{44}. Firstly this U-Net architecture can work with a relatively small amount of training image data while producing accurate segmentation regions, and secondly~\cite{45} tests their model on several iris datasets and the results obtained prove that their model outperforms other state-of-the-art deep learning based methods in solving the iris segmentation problem.

\cite{10} researchers have found that existing methods typically use the entire image of the eye as the input to the network learning, which does not take into account take into account the geometric constraint that the iris only appears in specific regions of the eye. The researchers in~\cite{10} then regress a bounding box of potential iris regions on top of the original U-Net network to generate an attention mask. The mask was then merged as a weighting function with the discriminative feature map in the model to make the segmentation model more focused on the iris region. The experimental results in~\cite{10} show that its \emph{Mean Error Rate} (MER) achieves better results. Thus, it can be seen that considering the geometric constraint that the iris appears only in a specific region of the eye can be of great help in improving the iris segmentation performance.

Difference from the improved scheme of~\cite{10}, the segmentation performance was improved by changing the internal network structure in~\cite{46}.
\cite{46} is based on the U-Net network model, retaining all pool sampling and upsampling operations, and replacing all $3\times 3$ unfilled convolutions with two-dilated convolutions except for the $1\times 1$ convolution in the last layer. \cite{46} proposes a new network model combining U-Net and diluted convolution of a novel network model for iris image segmentation. Combining the advantages of diluted convolution, which can extract more information about image features and improve segmentation accuracy, the researchers in~\cite{46} propose a \emph{Fully Diluted Convolution Combined with U-Net} (FD-UNet) using diluted convolution instead of original convolution to extract more global features for better processing of image details.

The above mentioned are all research methods proposed based on segmentation of iris images under better conditions, but in real life, segmentation performance is susceptible to non-ideal conditions caused by ambient light noise and user non-cooperation. Existing segmentation methods based on local features are unable to find the true iris boundary under these non-ideal conditions, and errors generated in the segmentation stage will carry over to all subsequent stages, resulting in reduced accuracy and reliability of IR. To address these issues and improve the accuracy of iris segmentation, \cite{47} combines Densenet with U-Net to propose a new network structure that not only reduces the network parameters but also exploits the advantages of U-Net in semantic segmentation. Dense U-Net integrates dense connections into the contraction and expansion paths of U-Net. Compared with traditional CNNS, dense connections reduce learning redundancy, enhance information flow and reduce the number of parameters required to achieve similar or better performance. Experimental results demonstrate that the proposed model can improve accuracy and reduce error rates.

With the aim of improving the robustness of iris segmentation algorithms in non-cooperative environments, the researchers in~\cite{48} propose a new iris segmentation algorithm that locates the inner and outer iris boundaries of iris images. They propose a neural network model called Interleaved Residual UNet for iris mask synthesis and semantic segmentation. They also used K-means clustering to select salient points to recover the outer boundary of the iris and another set of salient points on the inner side of the mask to recover the inner boundary. The experimental results in~\cite{48} showed that the proposed iris segmentation algorithm achieved mIoU values of 98.9\% and 97.7\% for inner and outer boundary estimation, respectively, on the challenging CASIA-Iris-Thousand dataset, respectively.

\cite{49} similarly find that the quality of iris images varied considerably under different shooting conditions and that these factors seriously affected the accuracy of iris segmentation. However, conventional algorithms are not sufficiently adaptable and CNNs based algorithms are not efficient enough. An end-to-end encoder-decoder model based on an improved UNet++ for iris segmentation is therefore proposed, referred to as the attention mechanism UNet++. Firstly, the researchers in~\cite{49} choose efficient netv2 as the convolutional block for UNet++ in order to improve the training speed and reduce the number of network parameters. Secondly, to make full use of the captured positional information, enhance the semantic information on the channels to suppress irrelevant noise interference and enhance the learning of the discriminability of iris regions to improve the discriminability of iris features, the researchers embedded an attention module in the downsampling process of UNet++. Finally, the algorithm uses a pruning scheme to obtain four different performance networks, just so that it can meet the needs of IR in multiple application scenarios. 

\cite{51} proposes an efficient lightweight U-Net architecture, which is novel in that although this U-shaped network structure consists of 36 layers and uses 148 parameters, its values are an order of magnitude lower than those of other existing networks used for similar applications. The model is dimensionally reduced, but the network is experimented on images of varying resolution and quality from five standard open source benchmarks: BioSec, CasiaI4, CasiaT4, IITD, and UBIRIS. The experimental results show that the network still maintains high iris segmentation accuracy and iris segmentation efficiency.

\subsection{Segmentation based on other neural network structure}
\cite{153} proposes a new real-time biometric model based on deep learning architecture called HCNN. The proposed model involves several stages. At the initial level, boundaries are obtained from a given image to provide RoI to subsequent levels. Next, the resulting images include areas of the upper and lower eyelids and remaining areas such as skin, eyelashes and sclera. In the next layer, within RoI, HCNN is used to provide the boundaries of the actual iris through learned features. The CASIA-V3-Interval dataset experiment results are better, the accuracy rate is 99.5\%.

 Using CNN-based segmentation as noise masking improves the traditional method.Since semantic segmentation is more difficult than iris segmentation~ \cite{154}, the CNN model selected in this paper is RefineNet~\cite{155}, which shows good performance in semantic segmentation. RefineNet is a multi-path refinement network, which uses a cascade architecture with four refinement network units. Each unit is directly connected to the output of a residual network~\cite{98} and the previous refinement network block in the cascade. Each RefineNet unit is composed of two residual convolution units, and its output is fused into a high-resolution feature map, and then a chain residual pool block is input. The last part of each refined block is another residual convolution unit. RefineNet refers to improved FCEDN\cite{55}. After selecting the appropriate network model, the parameterization process of this paper includes the following steps: Firstly, the edge is smoothed by median blurring. Then the circular Hough transform is used to generate candidate segmentation. Finally, the best candidate segmentation is selected as the final parameterization. The traditional iris segmentation method is superior to CNN segmentation and parameterization in high quality dataset, especially in NIR images of open eyelid and frontal lobe. However, CNN segmentation and parameterization perform better on lower quality datasets. \cite{154} is a parametric CNN-based segmentation method to bridge the gap between CNN-based segmentation and rubber sheet transformation. Parameterization makes CNN segmentation a complete segmentation step in any conventional iris biometric system, and can also be used as a noise mask for other segmentation methods. Overall, the CNN-based segmentation model and the proposed parameterization are improved over traditional segmentation methods, except for very high quality biometric records.

In order to keep the input size constant throughout the network and make the network lightweight, inspired by ~\cite{157,158}, the researchers of ~\cite{156} have adapted the original hourglass network. It is a graphical boundary representation of binary fill masks with iris pupil or edge boundaries introduced by ~\cite{156}. It can be referred to as an iris map. In this way, the prediction of iris boundaries and segmentation masks can be modeled as a multi-label semantic segmentation problem that can be solved by an adaptive lightweight overlay hourglass network. Furthermore, the final parametric localization results can be obtained by a small number of simple post-operations. Similar to U-Net, the hourglass network is a symmetric fully convolutional architecture in which feature mappings are downsampled by pooling operations and then upsampled by nearest neighbor upsampling. At each scale level, the remaining skip connections are applied to connect the corresponding layers on both sides of the hourglass. This bottom-up and top-down structure is stacked multiple times to capture remote contextual information, so that predictions can even be made by capturing the spatial relationships between pupil boundaries, iris boundaries, and edge boundaries. \cite{156} not only improves the segmentation and recognition performance, but also reduces the complexity of the model and facilitates its deployment on mobile devices. Smaller \emph{Mean normalized Hausdorff distance}(mHdis) indicates higher detection accuracy. The mHdis of~\cite{156} achieves a better value.


\cite{159} proposes a two-stage iris segmentation method based on CNN. This method can find the real iris boundary stably under limited conditions. The iris segmentation scheme can be used for low quality noisy images, even in the visible light environment where the first stage includes a bottom cap filter, noise removal, Canny edge detector, contrast enhancement, and an improved HT to segment the approximate iris boundary. In the second stage, a $21\times 21$ pixel deep CNN is used to fit the real iris boundary. By applying the second-stage segmentation only in the RoI defined by the approximate iris boundary detected in the first stage, we can reduce the processing time and error of iris segmentation. In order to reduce the effect of bright specular reflection on the performance of iris segmentation, the specular reflection  region in the image input to CNN is normalized with the average RGB value of the iris region.The ASE is only 0.345\% in MICHE Dataset. 

It is precisely because of the idea of~\cite{159} that the ROI around the iris is manually detected by edge detection and binarization techniques. \cite{160} attempts to treat it as a learning problem without the need for a specific transformation. The assembly and classification of bounding boxes has been regarded as a separate problem~\cite{161}. Unifying these issues is more straightforward, so that each project can be solved in parallel by sharing features, increasing speed and reducing redundancy. Mask reasoning, which determines which bits are iris bits, can now be done more efficiently. Once the bounding box class is predicted, the search space is reduced. A loss function is used, which is a cumulative of bounding box, classification, and mask inference losses. The model is based on the Mask R-CNN framework~\cite{162}. The Mask R-CNN framework enhances the new value of region-based convolution, and the experimental results show that the method is faster than the previous method including mask R-CNN network segmentation. Its average segmentation rate on the Ubiris dataset is 94.8\%.

\cite{163} is modified by DeepLab model~\cite{164}. The DeepLab model for semantic segmentation needs to obtain more texture details in the image. But the texture of the human eye is simple, so the model simplifies the structure of DeepLab for iris segmentation tasks. The network has five groups, including five Conv or pooling layers and three FC layers. The ReLU activation function is used by neurons. Dropout is also used to allow online learning to have a more robust relationship. To generate a mask of equal size, an interpolation layer is added before the softmax layer. This study was conducted on the visible spectral iris dataset \cite{165}. The data set was captured by researchers using a MEIZU4 pro phone and a Sony nex5c camera. The dataset contains 350 eye images. In the experiment, 270 images were randomly selected as the training set and 80 images as the testing set. The final segmentation accuracy is 97.5\%.

IrisDenseNet proposed by~\cite {166} is an end-to-end segmentation network that uses complete images without pre-processing or the best information gradient flow of other traditional image processing techniques to prevent network overfitting and vanishing gradient problems. To solve the challenging iris segmentation problem under visible and near-infrared camera sensors, the tightly connected IrisDenseNet can leverage the information gradient flow between better dense blocks to determine the true iris boundary under low-quality images. This study validates the power of dense connectivity, the visual difference between the output feature maps from convolutional layers of dense connectivity and normal connectivity. IrisDenseNet is more effective in accurately segmenting high-frequency regions of iris regions, such as eyelashes and ghost regions. 


The algorithm in~\cite{167} can segment iris patterns from eye images before and after cataract surgery. Therefore, the focus of this study is to design a segmentation algorithm for extracting iris regions from cataract patients and after cataract surgery. We propose a segmentation algorithm based on deep learning, named SegDenseNet, which uses four dense blocks to learn the specific iris shape of the model, even if there are irregular iris shapes. The average segmentation error rate on the IIITD Cataract Surgery dataset was 0.98\%.

\cite{168} is based on deep residual network~\cite{98}. VGG-16 and extended convolution \cite{169}.and SegNet~\cite {170} are three ready-made CNNs to train iris segmentation tasks. In the Biosec dataset, the best result is DRN, its \emph{Intersection over Union} ( IoU ) = 87.29\%. In the ND-IRIS-0405 dataset, the best result is the SegNet model, IoU = 89.75\%. Finally, for the most challenging UBIRIS dataset, Context-100k model, the result of the model is the best, IoU = 56.28\%. This paper studies the possibility of using irregular iris segmentation masks returned by deep learning-based models in traditional Gabor-based IR. The results show that these models are superior to the traditional circle-based iris segmentation.

\cite{171} proposes an end-to-end semantic segmentation network called FRED-Net. First, it does not use the traditional image processing scheme and has no preprocessing overhead. It is a standalone network in which eyelid, eyelash and flicker detection does not require acquiring real iris boundaries. Secondly, the proposed FRED-Net is the final composite structure developed step by step. In each step, considering the detailed description of the network, a new complete variant network is created for semantic segmentation. Thirdly, FRED-Net uses the residual shortcut of encoder and decoder to realize the residual connectivity between convolutional layers, so that high-frequency components can flow in the network and obtain higher accuracy on few layers. The real iris region is determined by connecting the high frequency information flow from the previous layer through the residual skip. Using CASIA-V4-Distance dataset, the \emph{Average Error}(AE) is 0.26\%. In the MICHE-I dataset. The AE is 0.18\%. Experimental results show that the FRED-Net performs well on 7 iris and road scene segmentation datasets.

\cite{172} proposes a robust and fast iris segmentation algorithm based on fast R-CNN. The main advantage of the proposed algorithm is that it has a smaller model size to segment the iris image faster, which is crucial for real-time IR systems and even implements it on mobile devices. This paper presents a combination method of iris segmentation algorithm based on learning and edge. A well-designed faster R-CNN has only six layers to establish positioning and classification eyes. Using the boundary box found by fast R-CNN, the Gaussian mixture model is used to locate the pupil area. The circular boundary of the pupil area is fitted according to five key boundary points. The boundary point selection algorithm is used to find the boundary points of the edge, and the circular boundary of the edge is constructed by using these boundary points. Experimental results show that the iris segmentation method achieves 95.49\% accuracy on challenging CASIA-Iris-Thousand dataset.

For the volume segmentation of iris video, \cite{173} proposes the \emph{Flexible Learning-Free Reconstruct of Neural Volumes} (FLoRIN) framework. The FLoRIN framework is a multi-stage pipeline with flexible image processing steps at each stage. The framework is an open source segmentation and reconstruction framework originally designed for neuromicroscope volume. The test bed of this framework is a near-infrared iris video dataset in which the pupil of each subject changes rapidly in size due to visible light stimulation. 
Through the algorithm, FLoRIN can improve the signal of interest features without any machine learning. In the segmentation phase, the image is loaded into the FLoRIN and processed selectively to improve contrast. Volume data is incorporated into the segmentation process, e.g. via histogram equalization, Weiner filter, etc. Then use \emph{N-Dimensional Neighborhood Thresholding} (NDNT) ~\cite{174} to perform two-dimensional or three-dimensional threshold and parameterized threshold on the image. Neighborhood size can specify a neighborhood in two-dimensional or three-dimensional space, which allows volume data to be merged into threshold segmentation. The binarized NDNT output is then passed to the next stage. Compared with SegNet~ \cite {170} and OSIRIS~\cite{175}, FLoRIN achieves an increase of 3.6 times to an order of magnitude in output by combining volume information. The topic matching performance only decreased slightly. In addition, researchers~\cite{173} iris segmentation based on FLoRIN maintains this acceleration on low-resource hardware, making it suitable for embedded biometric systems.

A real-time and accurate 3D eye gaze tracking method for monocular RGB camera is proposed by~\cite{176}. The key idea is to train a  DCNN  that automatically extracts the iris and pupil pixels of each eye from the input image. Can be used for iris and pupil classification and eye closure detection. It combines U-Net~\cite{9} and compressor~\cite{112} to train a compact convolutional neural network suitable for real-time mobile applications. The researchers integrated iris and pupil segmentation and eye closure detection into a 3D eye tracking framework to achieve real-time 3D eye tracking results. The results of evaluating the system on live videos and web videos show that the system is robust and accurate for various genders, races, lighting conditions, postures, shapes and facial expressions. The technical level of 3D eye tracking using a single RGB camera is improved.

 IR method of~\cite{177} specifically designed for postmortem samples. Compared with the solution described in~\cite{179}, this paper uses a retrained off-site SegNet model~\cite{170} and provides a much improved method than the Osiris segmentation method. The study fine-tuned the cadaver iris image dataset and its corresponding ground true binary mask. The neural network is trained and tested with an iris image set that is completely disjoint with the topic. Overfitting is minimized in a standard way by applying cross-validation to the training subset. The network can well summarize the images collected from unknown subjects during training. The mask used for training is annotated in a fine-grained manner. The purpose is to teach the network to only locate iris regions that are not affected by post-mortem changes and represent clearly visible iris textures.
 Overall, postmortem IR is more accurate than Osirisand IriCore 's method of implementing only premortem iris design. In the samples collected 10 hours after death, the EER of this method was less than 1\%, while the EER of Osiris and IriCore~\cite{178} were 16.89\% and 5.37\%, respectively. For samples collected 369 hours after death, the EER of this method reached 21.45\%, while the EER of Osiris and IriCore were 33.59\% and 25.38\%, respectively. The specific method is based on DCNN after death iris image segmentation method, using different ground facts. such as coarse iris approximation and fine-grained areas affected by tissue decay.The first stage of the scheme includes a data-driven segmentation model based on DCNN for locating the iris in the image. 

For segmentation tasks, \cite{180} uses a FCN inspired by~\cite{57}, consisting of 10 layers. The network starts with a 3 $\times$ 3 core and maps the input ( 1 channel ) to the first convolutional hidden layer, which consists of 32 channels and uses a correction linear unit as the activation function. Throughout the hidden convolution layer, the size of the kernel remains unchanged, while the number of channels and their activation functions remain unchanged. Finally, in the output layer (1 channel), the kernel size is 3 $\times$ 3, but in this layer, the s-type activation function is used. The pool layer is not used because the performance of the network output is observed to be declining. Compared with other deep learning iris segmentation techniques, its main contribution is a neural network design with low complexity, which reduces memory requirements and computational requirements. The complexity of the network is at least an order of magnitude smaller than other CNNs designed specifically for iris segmentation tasks. In addition, the network 's segmentation performance on frontal iris samples on several public datasets is comparable to the SPDNN network of iris segmentation methods proposed by~\cite{57}.The proposed network is much smaller in size and complexity and is trained for off-axis iris sample segmentation tasks. Due to its lightweight design and high-performance segmentation off-axis and frontal iris samples and processing a series of input image quality, the proposed network is very suitable for general deployment on AR or VR devices.

A new CNN iris RoI segmentation model proposed by~\cite{181}. No pre-training is required. Its main contribution is to show a multi-loss function, including content loss and cross entropy. Content loss takes into account advanced features and therefore runs at different abstract scales, which complements cross entropy loss, which considers pixel-to-pixel classification loss. Used for split tasks to promote fast and better convergence of losses. The proposed PixISegNet consists of an autoencoder using mainly long-hop and short-hop connections and a stacked hourglass network between the encoder and decoder. There is a continuous up and down amplification in the stacked hourglass network, which helps to extract features on multiple scales. The researchers deployed a very rigorous strategy of all training one test. The ASE scores of the proposed method on UBIRIS.v2, IITD dataset and CASIA-V3-Interval datasets are 0.00672, 0.00916 and 0.00117, respectively.

\emph{Dual attention dense connection network}(DADCNet) proposed by~\cite{182} to achieve more accurate real iris region segmentation. 
Based on typical encoding and decoding structures, DADCNet introduces an improved skip connection structure to extract more sufficient and effective iris information. In this paper, the double attention module is used to embed the semantic information of the high-level features into the low-level features, and the spatial information of the low-level features into the high-level features. By connecting the above two embedded features to form an effective fusion feature, the accurate positioning of the real iris region is realized. A mask image segmented by DADCNet is proposed to replace the corresponding GT image. Secondly, logic and operations are performed on the original image for the mask image after DADCNet segmentation and the corresponding ground truth image. Finally, these results are treated as two input classes of the same identity.The results show that the introduction of an improved skip connection can indeed improve the performance of segmented DADCNet. Although the improved results are not particularly obvious, the purpose of this paper is to design an iris segmentation network that can completely segment the real iris region as much as possible. Therefore, the suggestion of improving jump connections is meaningful. In order to verify the iris segmentation performance of the improved jump connection proposed in this paper, a general attention network \emph{Normal Attention Network}(NANet) is designed to replace the improved jump connection with the ordinary jump connection without other changes. Since the \emph{spatial attention}(SA) value in DADCNet is one of the input values of the improved skip connection, the SA value will not appear in NANet. The recognition error rates of CASIA-Iris-Interval dataset, IITD dataset, UBIRIS.v2 dataset, and MICHE.I iris dataset were reduced.

\subsection{Summary}
Whether based on FCN, U-Net or other networks, studies have been conducted to obtain good performance on iris segmentation tasks. The methods and results of each study in the specific iris segmentation task are shown in Table.~\ref{tab5}.