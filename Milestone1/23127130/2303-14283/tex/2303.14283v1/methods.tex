\section{Proposed Approach}
Let $\rbtVars_t:=(\rbtVar_0,\rbtVar_1,\ldots,\rbtVar_t)$ be robot pose variables up to time step $t$ where $\rbtVar_i \in \SE(d)$. Landmark variables are denoted by $\lmkVars_n:=(\lmkVar_1,\lmkVar_2,\dots,\lmkVar_n)$. For SLAM problems, we consider two types of measurements: i) odometry and ii) landmark measurements. Let $\odomVals_t:=(\odomVal_1,\odomVal_2,\ldots,\odomVal_{t})$ be odometry measurements of which each is modeled by the likelihood function $p(\odomVal_i|\rbtVar_{i-1},\rbtVar_{i})$.  Without loss of generality, for the ease of notation in the method formulation, we assume the robot makes at most a landmark measurement at a time. Let $\measVals_t:=(\measVal_0,\measVal_1,\ldots, \measVal_t)$ be all landmark measurements in which each is modeled by the likelihood function $p(\measVal_i|\rbtVar_{i},\lmkVar_{d_i})$, where $\daVal_i \in \{1,2,\ldots,n\}$ is the landmark index associated with measurement $\measVal_i$. Let $\daVals_t:=(\daVal_0, \daVal_1, \ldots, \daVal_t)$ be data associations for all landmark measurements thus the posterior of robot and landmark variables at time step $t$ can be formulated by
\begin{align}
& p(\rbtVars_t, \lmkVars_n|\measVals_t,\odomVals_t, \daVals_t) \\
& \stackrel{\text{Bayes}}{\propto} \prod_{i=0}^{t} p(\measVal_i|\rbtVar_{i},\lmkVar_{d_i})\prod_{i=1}^{t} p(\odomVal_i|\rbtVar_{i-1},\rbtVar_{i})p(\rbtVars, \lmkVars),
\end{align}
where $p(\rbtVars, \lmkVars)$ denotes the prior.

Our goal is to infer marginal posteriors. Our method represents marginals either by samples or parametric models, depending on the uncertainty of the marginal. This hybrid density modeling aims to achieve a balance between computational efficiency and expressiveness. Our method is inspired by a typical observation about the posterior: oftentimes, only a few of the landmarks have very uncertain marginal posteriors, while marginals of most variables can be well described by Gaussian distributions. For example, a noisy range-only \emph{or} bearing-only measurement between a robot pose and a landmark can simulate either spherical or conic landmark distributions in 3D, while accumulated measurements may well constrain the landmark to a concentrated and (almost) unimodal distribution.

Specifically, we use particle filters to estimate very uncertain landmarks for the expressiveness of sample-based density representations. Meanwhile, we maintain a Gaussian approximation of the posterior for computation efficiency. Combining these two techniques leads to a win-win situation over time: i) the Gaussian approximation provides particle filters with a parametric model of the smoothed robot path, reducing the sampling complexity to landmarks, and ii) particle filters afford the Gaussian approximation statistically probable values of landmarks, which can be used to explicitly reset linearization points in nonlinear optimization solvers, thereby benefiting the pursuit of global optima. We describe the details of our method in the following sections.

\subsection{Gaussian Approximation}
We denote the Gaussian approximation (GA) of the posterior at time step $t$ as $g(\rbtVars_t, \lmkVars_n|\measVals_t,\odomVals_t, \daVals_t)$. We categorize all landmarks into a set of non-Gaussian landmarks $\lmkVar_{\uct_t}$ and a set of Gaussian landmarks $\lmkVar_{\gaus_t}= \lmkVars_n \setminus \lmkVar_{\uct_t}$, where $\uct_t \cup \gaus_t=\{1,2,\ldots,n\}$ and $\uct_t \cap \gaus_t=\emptyset$ (see Sec. \ref{sec:update-new-lmk-meas} for the approach to identify Gaussian landmarks). We use the Gaussian approximation to represent the posterior of robot poses and the Gaussian landmarks. That said,
\begin{align}
& p(\rbtVars_t, \lmkVars_n|\measVals_t,\odomVals_t, \daVals_t)\\
& \stackrel{\text{GA}}{\approx} p(\lmkVar_{\uct_t}|\rbtVars_t, \lmkVar_{\gaus_t},\measVals_t,\odomVals_t, \daVals_t)g(\rbtVars_t,  \lmkVar_{\gaus_t}|\measVals_t,\odomVals_t, \daVals_t), \label{eqn:ga}\\
& \stackrel{\text{CIR}}{=} p(\lmkVar_{\uct_t}|\rbtVars_t, \measVals_t,\odomVals_t, \daVals_t)g(\rbtVars_t, \lmkVar_{\gaus_t}|\measVals_t,\odomVals_t, \daVals_t), \label{eqn:ci-reduction} \\
& \stackrel{\text{CIR}}{=} \prod_{i \in \uct_t} p(\lmkVar_i|\rbtVar_{\idx_i}, \measVal_{\idx_i},\daVal_{\idx_i})g(\rbtVars_t, \lmkVar_{\gaus_t}|\measVals_t,\odomVals_t, \daVals_t) \label{eqn:post-fac}
\end{align}
where $\rbtVar_{\idx_i}$ denotes the set of robot poses that observed landmark $\lmkVar_i$ (i.e., $\idx_i=\{j \in [0,t]| \daVal_j = i\}$ denotes the time steps observing landmark $\lmkVar_i$). The reduction from \eqref{eqn:ga} to \eqref{eqn:ci-reduction} exploits the conditional independence relation (CIR) that landmarks are conditionally independent given a robot path. Note that while we use the CIR in a similar way to typical Rao-Blackwellized particle filters (RBPFs) for SLAM (e.g., FastSLAM 2.0 \cite{montemerlo2003fastslam}), our probabilistic modeling and inference methods differ from these RBPFs. The Gaussian approximation can be computed via the Laplace approximation \cite{bishop2006gaussian}: 1) finding the MAP estimate as the mean and 2) estimating the Hessian of the negative logarithm of the posterior at the MAP estimate as the covariance. In practice, many nonlinear local optimization solvers, such as GTSAM, are capable of providing efficient out-of-the-box solutions to the Gaussian approximation, although they are subject to local optima of the MAP estimate. In this paper, we refer to them as Gaussian solvers.
%

\subsection{Marginal Posterior of non-Gaussian Landmarks}
\label{sec:marginal-posterior}
We will use particles to represent the marginal posterior of any non-Gaussian landmark $\lmkVar_i \in \lmkVar_{\uct_t}$. Alg. \ref{algo:lmksample} summarizes how we draw the particles. Integrating out all variables except $\lmkVar_i$ in \eqref{eqn:post-fac}, we can formulate the marginal posterior by
\begin{align}
& p(\lmkVar_i|\measVals_t,\odomVals_t, \daVals_t)\\
& \stackrel{\text{GA}}{\approx} \int_{\rbtVar_{\idx_i}}  p(\lmkVar_i|\rbtVar_{\idx_i}, \measVal_{\idx_i},\daVal_{\idx_i})g(\rbtVar_{\idx_i}|\measVals_t,\odomVals_t, \daVals_t) \label{eqn:marginal}\\
& \stackrel{\text{MC}}{\approx} \frac{1}{K} \sum_{k=1}^{K} p(\lmkVar_i|\rbtVar_{\idx_i}=\rbtVal_{\idx_i}^{(k)}, \measVal_{\idx_i},\daVal_{\idx_i}), \label{eqn:MC}
\end{align}
where $\{\rbtVal_{\idx_i}^{(k)}\}_{k=1}^K$ are $K$ i.i.d. samples of part of the robot path drawn from the Gaussian marginal $g(\rbtVar_{\idx_i}|\measVals_t,\odomVals_t, \daVals_t)$ (line \ref{line:gs-samples} in Alg. \ref{algo:lmksample}). Monte Carlo (MC) integration is applied in \eqref{eqn:MC}. The conditional of $\lmkVar_i$ in \eqref{eqn:MC} is a result of eliminating $\lmkVar_i$ from factors adjacent to $\lmkVar_i$ (line \ref{line:lmk-factors} in Alg. \ref{algo:lmksample}), thus
\begin{align}
p(\lmkVar_i|\rbtVar_{\idx_i}, \measVal_{\idx_i},\daVal_{\idx_i}) =  \frac{\potential(\lmkVar_i, \rbtVar_{\idx_i})}{\int_{\lmkVar_i} \potential(\lmkVar_i, \rbtVar_{\idx_i}) },
\end{align}
where
\begin{align}
\potential(\lmkVar_i, \rbtVar_{\idx_i})=\prod_{j \in \idx_i} p(\measVal_j|\rbtVar_{j},\lmkVar_i)p(\lmkVar_i).\label{eq:lmk-potential}
\end{align}
We use importance sampling to draw samples representing $p(\lmkVar_i|\rbtVar_{\idx_i}=\rbtVal_{\idx_i}^{(k)}, \measVal_{\idx_i},\daVal_{\idx_i})$. We design the proposal distribution of landmark $\lmkVar_i$ as the sum-mixture of binary factors, as seen in
\begin{align}
q(\lmkVar_i; \rbtVal_{\idx_i}^{(k)}) = \frac{1}{|\idx_{i}|} \sum_{j \in \idx_i} p(\lmkVar_i|\rbtVar_{j} = \rbtVal_j^{(k)},\measVal_j). \label{eqn:proposal}
\end{align}
This proposal\footnote{Although we sacrificed sample diversity, we adopted a lighter-weight proposal in our experiments to ensure computational efficiency. Specifically, we limited the number of components in the sum-mixture to 5 and chose these components randomly.} was chosen for two reasons: i) covering more area in the space of the landmark variable and ii) it is efficient to draw landmark samples $\{\lmkVal_i^{(m,k)}\}_{m=1}^{M}$ from the proposal $q(\lmkVar_i)$ by sampling a multinomial distribution and binary factors independently.
With proposal samples $\{\lmkVal_i^{(m,k)}\}_{m=1}^{M}$ in hand, the normalized weight of each sample can be computed by
\begin{align}
w^{(m,k)} \propto \frac{p(\lmkVal_i^{(m,k)}|\rbtVal_{\idx_i}^{(k)}, \measVal_{\idx_i},\daVal_{\idx_i})}{q(\lmkVal_i^{(m,k)}; \rbtVal_{\idx_i}^{(k)})} \propto \frac{\potential(\lmkVal_i^{(m,k)}, \rbtVal_{\idx_i}^{(k)})}{q(\lmkVal_i^{(m,k)}; \rbtVal_{\idx_i}^{(k)})},\label{eqn:weights}
\end{align}
where $\sum_{m=1}^M w^{(m,k)}= 1$ (line \ref{line:weights} in Alg. \ref{algo:lmksample}). Finally we apply the re-sampling and regularization steps in \cite[Alg. 6]{arulampalam2002tutorial} to generate equally weighted samples of $\lmkVar_i$ (line \ref{line:resampling} in Alg. \ref{algo:lmksample}).

\setlength\floatsep{2pt}
\begin{algorithm}[t]
  \fontsize{9pt}{9pt}\selectfont
  \DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
  \KwIn{Landmark $l$, Gaussian solver $g$}
	Draw robot path samples $\mathcal{X}$ from the Gaussian approx. \label{line:gs-samples}
	
	$\mathcal{F}=\{f(l,\mathbf{x})\} \gets$ All factors adjacent to $l$ \label{line:lmk-factors} \tcp*[f]{robot vars $\mathbf{x}$} 

	$\psi(l,\mathbf{x}) \gets$ Product of all factors in $\mathcal{F}$ \tcp*[f]{potential func. \eqref{eq:lmk-potential}}

	$q(l,\mathbf{x}) \gets$ Sum of elements in $\mathcal{F}$ \tcp*[f]{proposal density \eqref{eqn:proposal}}

	Initialize a set $\mathcal{S}$ for storing landmark samples

	\For{\text{\normalfont sample $\hat{\mathbf{x}}$ in $\mathcal{X}$}}{
		Draw landmark samples $\mathcal{L}$ from density $q(l,\mathbf{x}=\hat{\mathbf{x}})$

		Weights $\mathcal{W}$ of landmark samples by computing $\psi/q$ \label{line:weights}

		$\mathcal{L} \gets$ Resampling using $\mathcal{L}$ and $\mathcal{W}$ \label{line:resampling}
		
		Add $\mathcal{L}$ to $\mathcal{S}$
	}
	\Return{$\mathcal{S}$} \tcp*[f]{samples of $l$}
  \caption{LandmarkSampler}
  \label{algo:lmksample}
\end{algorithm}
\setlength\floatsep{2pt}
\begin{algorithm}[!t]
  \fontsize{9pt}{9pt}\selectfont
  \DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
  \KwIn{New factor $f$ of landmark $l$, Gaussian solver $g$, dictionary $\mathcal{D}$ of non-Gaussian landmark samples}    
	\If(\tcp*[f]{new landmark}){l \textup{not in the Gaussian solver}}
	{
		Add $l$ to non-Gaussian landmarks $\mathcal{D}$ as a new key \label{line:new-lmk}
	}

	Add $f$ to the Gaussian solver \label{line:add-to-gs}

	\If{l \textup{in non-Gaussian landmarks} $\mathcal{D}$}
	{
		$\mathsf{LandmarkReinitializer}$($l$,$\ g$,$\ \mathcal{D}[l]$) \label{line:re-init} \tcp*[f]{re-init. in the solver}
	}
	Update the MAP estimate in the Gaussian solver \label{line:update-gs}
		
	\If{l \textup{in non-Gaussian landmarks} $\mathcal{D}$}
	{
		$\mathcal{D}[l]=\mathsf{LandmarkSampler}$($l$,$\ g$) \label{line:update-samples} \tcp*[f]{update samples}

		Compute empirical covariance matrix $C$ of $\mathcal{D}[l]$

		\If{\textup{the largest eigenvalue of $C$ is small}}
		{
			Delete $l$ from $\mathcal{D}$ \label{line:unimodal-lmk} \tcp*[f]{no longer non-Gaussian lmks}
		}
	}
	
	\Return{$g$ \textup{and} $\mathcal{D}$} \tcp*[f]{updated solver and dictionary}
  \caption{GAPSLAM}
  \label{algo:gapslam}
\end{algorithm}

\setlength\floatsep{2pt}
\setlength\textfloatsep{5pt}
\begin{algorithm}[!t]
  \fontsize{9pt}{9pt}\selectfont
  \DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
  \KwIn{Landmark $l$, Gaussian solver $g$, samples $\mathcal{S}$ of landmark $l$}
	$\psi(l,\mathbf{x}) \gets$ Product of all factors adjacent to $l$ \tcp*[f]{robot vars $\mathbf{x}$}

	$\hat{l},\hat{\mathbf{x}} \gets$ Current estimate in the Gaussian solver $g$

	$l^{*} \gets$ Value in $\{\mathcal{S}, \hat{l}\}$ maximizes function $\psi(l,\mathbf{x}=\hat{\mathbf{x}})$ \label{line:maximize-posterior}

	Reset the value of $l$ in the Gaussian solver to $l^{*}$ \label{line:reset-value}
	
	\Return{$g$} \tcp*[f]{updated solver}
  \caption{LandmarkReinitializer}
  \label{algo:reinit}
\end{algorithm}

Thus the marginal posterior in $\eqref{eqn:MC}$ can be approximated by samples, as seen in
\begin{align}
& p(\lmkVar_i|\measVals_t,\odomVals_t, \daVals_t) \stackrel{\text{MC}}{\approx} \tilde{p}(\lmkVar_i|\measVals_t,\odomVals_t, \daVals_t) \\
& = \frac{1}{K} \sum_{k=1}^{K} \sum_{m=1}^{M} \delta(\lmkVar_i-\lmkVal_i^{(m,k)}). \label{eqn:estimate-marginal}
\end{align}
To summarize, for each of the $K$ robot path samples, we draw $M$ landmark samples. The delta functions (or other kernels) of all $KM$ landmark samples can be used to approximate the marginal posterior of the landmark.

\subsection{Updates with new odometry and landmark measurements}\label{sec:update-new-lmk-meas}
The previous sections describe how the Gaussian approximation helps compute the particle-based belief of landmarks. Now we discuss how these particles provide the Gaussian approximation with linearization points, thus leading to the uncertainty-aware re-initialization in the Gaussian approximation. We will begin by analyzing the incremental update by one time step.


From time step $t$ to time step $t+1$, the robot receives a new odometry reading $o_{t+1}$ and a new landmark measurement $z_{t+1}$. It is easy to create a new robot pose $\rbtVar_{t+1}$ and absorb the odometry $\odomVal_{t+1}$ to form the intermediate Gaussian $g(\rbtVars_{t+1}, \lmkVars_n|\measVals_t,\odomVals_{t+1}, \daVals_t)$. However, fusing the measurement $\measVal_{t+1}$ is a more involved process. Alg. \ref{algo:gapslam} summarizes steps for tackling the landmark measurement. Assuming that the measurement $z_{t+1}$ comes from a landmark with index $d_{t+1}$\footnote{Data association and creation of new landmarks are left to the specific application in Sec. \ref{sec:da}.}, several cases are considered:

\begin{enumerate}
\item If the observed landmark $\lmkVar_{d_{t+1}}$ is not new or a non-Gaussian landmark (lines \ref{line:add-to-gs} and \ref{line:update-gs} in Alg. \ref{algo:gapslam}), we simply add the likelihood model of $\measVal_{t+1}$ (i.e., factor in the algorithm) to the Gaussian solver to update the Gaussian approximation without making any further changes.

\item If the observed landmark $\lmkVar_{d_{t+1}}$ is new (line \ref{line:new-lmk} in Alg. \ref{algo:gapslam}), i.e. $d_{t+1}=n+1$, we update the landmark set to $\lmkVars_{n+1} = \lmkVars_n \cup \{\lmkVar_{n+1}\}$ and indices of non-Gaussian landmarks to ${\uct_{t+1}}={\uct_{t}}\cup \{{n+1}\}$. We then add the likelihood model of measurement $\measVal_{t+1}$ to the Gaussian solver. Finally, we can simulate equally-weighted samples of the landmark using this single measurement and randomly select a point from the samples as the initial guess of the landmark.

\item If the observed landmark $\lmkVar_{d_{t+1}}$ is not new and is in non-Gaussian landmarks $\lmkVar_{\uct_t}$, this is the challenging case we will focus on.
\end{enumerate}

In the third case, our goal is to use the new measurement $\measVal_{t+1}$ to determine whether we should explicitly re-initialize the non-Gaussian landmark in the Gaussian solver (line \ref{line:re-init} in Alg. \ref{algo:gapslam}). Furthermore, if our belief of the landmark converges to a less uncertain situation after fusing measurement $\measVal_{t+1}$, we remove the landmark from the non-Gaussian set $\lmkVar_{\uct_t}$ (line \ref{line:unimodal-lmk} in Alg. \ref{algo:gapslam}).

Algorithm \ref{algo:reinit} summarizes how we re-initialize a non-Gaussian landmark $\lmkVar_i$. As described in Sec. \ref{sec:marginal-posterior} and Alg. \ref{algo:lmksample}, we already have samples of the landmark at hand, which can be passed to Alg. \ref{algo:reinit} as input. The union of the samples and the current estimate of the landmark forms a set of candidate points for re-initialization. The point in the set that maximizes the posterior will be used to re-initialize the landmark (lines \ref{line:maximize-posterior} and \ref{line:reset-value} in Alg. \ref{algo:reinit}). During the evaluation of the posterior, all variables other than the landmark are fixed by the current mean of the Gaussian approximation. This reduces the evaluation to only the product of factors that are adjacent to the landmark, simplifying the computation. The re-initialization can be formulated as

\begin{align}
l_i^* = \argmax_{\lmkVal_i \in \{\mathcal{S}, \hat{\lmkVal}_i \} } \potential(\lmkVar_i=\lmkVal_i, \rbtVar_{\idx_i}=\hat{\rbtVal}_{\idx_i}),\label{eq:reinit}
\end{align}
where the potential $\potential(\cdot)$ is the same as \eqref{eq:lmk-potential}, $\mathcal{S}=\{ l_i^{(j)}\}_{j=1}^{M}$ denotes current samples of the landmark, and $\hat{l}_i$ and $\hat{\rbtVal}_{\idx_i}$ denote current estimates of the landmark and robot poses in the Gaussian solver.

After performing the re-initialization, we compute the Gaussian approximation for time step $t+1$ (line \ref{line:update-gs} in Alg. \ref{algo:gapslam}), $g(\rbtVars_{t+1},\lmkVars_n|\measVals_{t+1},\odomVals_{t+1}, \daVals_{t+1})$,
%\begin{equation*}
%     g(\rbtVars_{t+1},\lmkVars_n|\measVals_{t+1},\odomVals_{t+1}, \daVals_{t+1}),
%\end{equation*}
where the new measurement $\measVal_{t+1}$ has been incorporated. Given the new Gaussian approximation, we update samples of the marginal posterior of $\lmkVar_i$ (line \ref{line:update-samples} in Alg. \ref{algo:gapslam}), which are cached for potential re-initialization in the future. Following \cite{blanco2008pure}, we compute an empirical covariance matrix of these samples as well as its largest eigenvalue. When the largest eigenvalue falls below a threshold, it indicates that the belief of landmark $\lmkVar_i$ has become sufficiently certain, and $\lmkVar_i$ is removed from the non-Gaussian landmark set $\lmkVar_{\uct_{t+1}}$ (line \ref{line:unimodal-lmk} in Alg. \ref{algo:gapslam}). As a result, $\lmkVar_i$ will no longer be estimated by particle filters and will only be involved in updates in the Gaussian solver.

\subsection{Summary of the Approach}
Algorithms \ref{algo:lmksample}-\ref{algo:reinit} summarize our approach. Algorithm \ref{algo:gapslam} is the main process where we update the Gaussian solver/approximation and a dictionary that maps non-Gaussian landmarks to their samples.