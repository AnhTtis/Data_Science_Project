\def\secstatus{finished}

\section{Relavitistic electron drift-kinetic solver based on PETSc and p4est}
\label{sec:solver}

DMBF (Data Management for Block-structured Forest-of-trees) is a new data
management object in PETSc that we have developed and implemented along with a
new relativistic electron drift-kinetic solver.
We first outline the capabilities of the implementation that are incorporated
into PETSc;  subsequently, we give a high-level description of the new
application code for relativistic electrons.

%---------------------------------------

\subsection{DMBF: New PETSc data management for p4est-based AMR solvers}
\label{sec:dmbf}

Our goal is to discretize the computational domain with locally adaptively
refined quadrilateral meshes in two dimensions.  Extreme local refinement is
critical for resolving the rapidly changing solution, which can vary 15--20
orders of magnitude, while over regions where the solution is relatively flat,
coarser meshes can help reduce the computational cost.  Therefore, a key
requirement of our solver is adaptive mesh refinement.

%%% P4EST
The dynamic mesh adaptivity in parallel is, at its lowest level, enabled by the
p4est library \cite{BursteddeWilcoxGhattas11, IsaacBursteddeWilcoxEtAl15}.
This library implements hierarchically refined quadrilateral and hexahedral meshes
utilizing forest-of-octree algorithms and space-filling curves
\cite{sundar2008bottom}.
Mesh refinement and coarsening are efficient in parallel because p4est performs
these tasks locally on each compute core without communication.  The
representation of the mesh as a quadtree/octree topology enables efficient 2:1
mesh balancing in parallel as well as repartitioning of the mesh cells
across compute cores, for both of which communication is necessary.
Space-filling curves transform a two- or three-dimensional space that is subdivided in cells
into a (one-dimensional) sequence of cells.  This sequence is used for an efficient
partitioning of mesh cells in parallel with the desirable property of
keeping spatially neighboring cells nearby each other in the sequence;
hence exhibiting memory locality.
This property of space-filling curves is responsible for keeping the amount of
communication low.
Overall, the algorithms of p4est have demonstrated scalability up to $O(100,000)$
of processes on distributed-memory CPU-based systems
\cite{BursteddeGhattasGurnisEtAl10, SundarBirosBursteddeEtAl12}; moreover,
scalability to $O(1,000,000)$ of CPU cores have been achived for complex
implicit PDE solvers \cite{rudi2015extreme, RudiStadlerGhattas17,
RudiShihStadler20}.
Generally, tree-based AMR algorithms have shown impressive scalability
results \cite{weinzierl2019peano, FernandoNeilsenLimEtAl19}.

%%% DMBF
DMBF is a new type of data management in PETSc.
%it is thus a particular kind of DM in PETSc.
It provides interfaces for block-structured forest-of-trees meshes.
The block structure is understood in the sense that leaf elements of the trees
can be equipped with blocks of uniformly refined cells.  DMBF implements
PETSc functions that give direct access to the functionality of p4est with the
least amount of overhead compared with existing PETSc data management approaches.
In addition, DMBF provides cellwise data management for storing any kind
of data that is mapped to the cells of the p4est mesh.
Because DMBF associates cellwise data directly with the p4est mesh, the cell
data managed by DMBF is created and destroyed when p4est cells are refined
and coarsened; and DMBF-managed cell data migrates across processes in parallel
as the p4est mesh is repartitioned.  In the context of discretizations of PDEs,
it is essential that data from neighboring cells be shared, which in a
distributed-memory setting is handled via point-to-point communication between
processes in order to communicate data of a ghost layer.  DMBF supports the
communication of DMBF-managed cell data within a ghost layer.

The focus of DMBF in managing general data objects enables scientific
applications that use the DMBF interface to store any kind of cell-dependent
data, for instance, the data required for finite element, finite volume, or
finite difference discretizations.  The shape and size of the cell data are left
entirely up to the application using DMBF and are not imposed by DMBF's
interface.  This approach allows for greater flexibility, because more
applications can utilize DMBF.  At the same time, it requires the
application to implement its own discretization routines (or rely on other
libraries).
We have taken this approach of separating concerns.  We have implemented a new
relativistic electron drift-kinetic solver that is supported by the DMBF
interfaces (see Section~\ref{ref:rfp-solver}).

To perform computations with the DMBF-managed cell data, the DMBF
interface provides iterator functions that call a user-specified function for
each cell.  Additional functions are capable of iterating over the edges/faces
of cells.  The adaptation of a mesh is handled with similar user-provided
functions that act on the data of each cell individually.
%
The DMBF interface is completed by allowing the generation of PETSc vectors and
matrices. Therefore, calculations that utilize DMBF leverage the linear and
nonlinear solvers of PETSc as well as PETSc's time-stepping functionality.

%---------------------------------------

\subsection{Relativistic electron drift-kinetic solver based on DMBF}
\label{ref:rfp-solver}

This section gives a brief overview of the relativistic drift-kinetic Fokker--Planck solver
that is built on the DMBF data structure.

The spatial discretization chooses a finite volume scheme, MUSCL (Monotonic
Upstream-centered Scheme for Conservation Law) \cite{LeVeque02, Toro09},
or a conservative finite difference scheme, QUICK (Quadratic Upstream
Interpolation for Convective Kinematics) \cite{Leonard79}, for the advection
operators.  Both schemes are second-order accurate in space
on a uniform mesh for time-dependent advection problems.
The collision operator is discretized by a central scheme using a three-point stencil that
also gives second-order accuracy. An adaptive quadtree-based
quadrilateral mesh is used. Thus our solver supports the handling of hanging nodes and can be nonsymmetric in a single cell.
More details on the AMR and the implementation can be found
in Section~\ref{sec:dmbf}.

Numerical solutions are always assumed to live on cell centers.  Each cell of
the adapted mesh is further subdiveded into four uniform cells, which
constitute the degrees of freedom (DOFs) of a solution's discretization.
To distinguish between the two notions of cells, we use the term \emph{mesh cells} for the
cells of the adapted p4est-based mesh and the term \emph{FV cells} for the four uniform
cells inside a mesh cell.
%
Additional to the FV cells inside each mesh cell, the numerical schemes need to
access DOFs from neighboring FV cells.  To this end, the DMBF cell data includes
a layer, also referred to as guard layer, of two FV cells around a mesh cell.
This outer layer of DOFs can require point-to-point communication to be filled
with data from other processes.
%
The spatial discretization is coupled with explicit or implicit time stepping.
To ease the mesh adaptivity, we choose single-step integrators.
For the explicit case a SSP Runge--Kutta is chosen, while for the implicit case
a fully implicit DIRK or ESDIRK time integrator is chosen.
Here the explicit integrators are chosen only for testing purposes.
All the production runs use implicit integrators because we are interested in
physics of the collisional time scale.

%- quadrilateral mesh cells\\
%- cell-centered location of degrees of freedom\\
%- faces around a cell-centered node can be non-symmetric, because of mesh adaptivity\\
%- finite volume scheme with piecewise linear reconstruction\\
%- pw lin local reconstruction of q\\
%- total variation diminishing (TVD) [in time] property\\
%- slope limiting with MINMOD\\
%- for advection part of the PDE, this gives up to second-order accuracy\\
%- diffusion flux is handled with a three-point finite difference stencil

%solver
Depending on the choice of the advection scheme, the problem can be nonlinear
(in the case of MUSCL) or linear (in the case of QUICK).  For nonlinear problems, the
JFNK algorithm is the primary solver, and it is further preconditioned with an
approximate Jacobian.  The Jacobian is computed through finite difference
coloring provided by PETSc.  A GMRES iterative solver is used to invert the
linearized system (and also the linear system arising from QUICK).
A hypre \cite{FalgoutYang02} algebraic multigirid (BoomerAMG
\cite{yang2002boomeramg}) solver is used as the preconditioner.  Those choices
of the algorithm are state of the art for large-scale nonlinear simulations.

Two commonly used operators on adaptive meshes are interpolations and
integrations, both of which need to support handling of hanging nodes/edges.  For
interpolation from coarse to fine and reverse, we use tensor product
interpolation matrices because of their lower memory usage per generated Flops.
When needed, a postprocessing step is performed after interpolation to
preserve the positivity and mass conservation of the algorithm.
Computations of fluxes across hanging edges require tailored algorithms for
filling the guard cells; these are derived from the interpolation algorithms
and inherit their properties (tensor product formulation, positivity, and mass
conservation).
%
A line integral along $\xi$ direction is performed to compute runaway
electrons.  A simple quadrature rule is used in each cell, and the positivity
can be enforced at the price of reducing the spatial accuracy to
first-order.

%\noindent
%for interpolation from coarse to fine and reverse, we use:\\
%- tensor product interpolation matrices (low memory, high Flops)\\
%- postprocessing to enforce positivity and mass conservation\\
%-computations for fluxes across haning edges are derived from interpolation
% and inherit their properties (tensor product, positivity, mass conservation)\\

