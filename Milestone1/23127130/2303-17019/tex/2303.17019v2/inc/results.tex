\def\secstatus{finished}

\section{Numerical results}
\label{sec:results}

Numerical experiments were performed in order to demonstrate the convergence of the
AMR-based numerical schemes and solvers, the efficiency gained by AMR with
prediction, the robustness of the solvers for practical physics regimes, and
the algorithmic as well as parallel scalability of RFP simulations.

%

\subsection{Manufactured solution results: Convergence of algorithms}
\label{sec:manufactured-results}

This section employs the manufactured solution derived in
Section~\ref{sec:solutions}  to demonstrate a convergence analysis of
the overall solver, including the numerical scheme and handling of the
{finite volume discretization} %
at hanging faces of neighboring cells with different
levels of refinement.

The manufactured solution is approximated in the domain $[0.3, 60]\times[-1, 1]$ for
$[p_\mathrm{min},p_\mathrm{max}]\times[\xi_\mathrm{min},\xi_\mathrm{max}]$.
\editOne{The RFP model parameters used in}%
{The parameter $E=0.5$ is used in the model}~\eqref{eqn:exactCollisionMMS};
\editOne{are $E=0.5$, $\alpha=0.05$}{}
the simulation of the manufactured solution takes place in the
time interval $t=0,\ldots,10$; and the MUSCL scheme is used.  We  utilize
the 3rd-order explicit Runge--Kutta time integrator of PETSc with a constant
time step length \editAll{}{(varying sizes determined by spatial refinement)}.
The range of levels of mesh refinement, in between which our AMR
refinement criteria from Section~\ref{sec:amr-pred} are adapting the mesh
dynamically, is varied.  We prescribe the range of levels to be three and shift the
minimum and maximum permitted levels up by one in each simulation setup.
Consequently, the coarsest setup has refinement levels $(2,3,4)$; the next finer
setup has levels $(3,4,5)$,
\editBOne{then $(4,5,6)$, and finally $(5,6,7)$}
{etc.; and finally we reach levels $(7,8,9)$}.
Table~\ref{tab:manufactured} lists the number of
\editOne{FV cells}{cells}
that are generated by the AMR algorithm with
prediction (see Section~\ref{sec:amr-pred}) at each refinement level.

The increasingly finer mesh resolution requires a shorter time step length that
is selected to satisfy the CFL condition; see column $\Delta t$ in
Table~\ref{tab:manufactured}.  Because of the changing time step length, we
also are adjusting the frequency of time steps at which mesh refinement is
performed; the corresponding column in Table~\ref{tab:manufactured} is called
\emph{Refine freq.}  The refinement frequency determines after how many time
steps the mesh is updated to track the dynamically changing solution.
\editBOne{}{We increase the refinement frequency proportionally to the number of
time steps, hence the total number of AMR operations stays constant across all
experiments.}

\begin{table}\centering
  \caption{Numerical results pertaining to the manufactured
    solution~\eqref{eqn:exactCollisionMMS}%
    \editBOne{.  Four simulations are shown with increasing levels of refinement, with the
    total number of dynamically adapted cells increasing by factors of $\sim$3
    and the relative error to manufactured solution decreasing by a factor of
    $\sim$1/2.}{, where rows show simulations with
    increasing levels of refinement. An order of convergence of $\sim$2 is
    asymptotically reached.} }
  \label{tab:manufactured}
  \centering
  \scriptsize
  \input{inc/tab_manufactured_advdiff}
\end{table}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

\editBOne{The convergence of the numerical solution, expressed in the error to
the known exact solution~\eqref{eqn:exactCollisionMMS}, is listed in the
rightmost column of Table~\ref{tab:manufactured} denoted \emph{Error}.
The errors decrease with each additional level of refinement by a factor of 1/2
or less.  Simultaneously the number of total cells is increasing by a factor of
$\sim$3, as opposed to a factor of 4 if the refinement was performed uniformly.}
{The errors of the numerical solution relative to the known exact
solution~\eqref{eqn:exactCollisionMMS} and the order of convergence of these
errors are listed in the two rightmost columns of Table~\ref{tab:manufactured}.
The error initially decreases with an order that is around 0.6 at coarser levels
of refinement (first three rows of Table~\ref{tab:manufactured}).  As the levels
of refinement increase, the order of convergence also increases, and it reaches
a value of around two.  From this we observe an asymptotic order of convergence
of $\sim$2 for our numerical scheme with adaptive meshes.  The lower order of
error reduction that is present at coarser meshes is likely due to the specific
function that represents the manufactured solution.  A sufficiently refined mesh
may be necessary to capture certain oscillations of that sinusoidal function
before the desired asymptotic order of convergence can be observed.
%
%
}

%

\subsection{AMR prediction results: Higher accuracy with low computational overhead}
\label{sec:pred0-vs-pred1}

This section demonstrates numerically the benefit of adding the proposed
prediction capabilities to AMR as described in Section~\ref{sec:amr-pred}.  The
benefit is that the solution function is resolved with increased accuracy with
a comparatively small increase in computational costs.
%
We focus on an experimental setup where the frequency of mesh
refinement (and coarsening) varies such that AMR is performed each 32, 16, 8,
and 4 time step iterations, where the time step length is adapted based on a
local truncation error.  For brevity, we refer to these varying frequencies of
mesh refinement as RF32, RF16, RF8, and RF4, respectively.  These refinement
frequencies are used in experiments with standard mesh refinement without
prediction and AMR with prediction. The adaptive levels for mesh refinement
range from 2 to 6, hence capturing a level contrast of 4, where a uniform mesh
of level 2 would contain 192
\editOne{FV cells}{cells}
and each additional level subdivides a parent cell into 4 child cells.

The RFP model parameters for this study are $E=0.5$ and $\alpha=0.1$; the time
interval of the simulation is $t=0,\ldots,4$; and the MUSCL scheme is used.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

The AMR indicator LogDR~\eqref{eq:ldr} is a metric based on features of the numerical
solution that quantifies the smoothness of the solution with nondimensional
values.  When the solution exhibits artifacts or oscillations arising from too
coarse meshes, the metric increases to values larger than one.
Therefore, we use this metric to evaluate how well the solution is resolved by
a dynamically adapted mesh, and we compare the differences as the refinement
frequency varies and for AMR without and with prediction.

Initially, we observe how the mean of the AMR indicator  evolves in time. In
Figure~\ref{fig:pred0-vs-pred1-mean} the top graphs correspond to
standard AMR (without prediction), and the bottom graphs show AMR with
prediction.  The colors of each graph indicate different refinement
frequencies.
The top of the figure
%
shows how more frequent mesh
adaptation is resulting in a reduction of the spatial mean of the metric and
how the metric is approaching unity.  These results stand in contrast to the bottom of
%
the figure, where the AMR indicators are propagated
ahead of the solution, which results in the metric being significantly lower,
by about 10--40\%
Additionally, the curves corresponding to different refinement frequencies are
more clustered together and below unity, showing that less frequent mesh
adaptation must not result in poorer-resolved solutions as in the case without
AMR prediction.
The trend as refinement frequency increases (RF32-pred \ldots RF4-pred) is
reversed compared with the top curves (RF32 \ldots RF4), because the metric
associated with RF4-pred (pink color) is above the one for RF32-pred (green
color).  The reason  is that the prediction intervals are longer for RF32-pred and
therefore more cells of the mesh are refined along the predicted path of the
solution.

Complementary to the evolution of the mean of the metric,
Figure~\ref{fig:pred0-vs-pred1-mesh} plots the evolution of the mesh size.
This figure shows how many more mesh cells are being generated due to the
prediction of AMR indicators.  The increase in the number of cells is
$<$10\%
$\sim$20\%
RF32-pred compared with RF4-pred also support the above observations about the
mean of the metric (i.e., RF32-pred's metric is below RF4-pred).
While the discussion has been highlighting the extreme cases RF32-pred and
RF4-pred, the intermediate setups RF16-pred and RF8-pred show only moderate
increases in mesh sizes (orange and purple curves in
Figure~\ref{fig:pred0-vs-pred1-mesh}) while at the same time keeping the
metric uniformly bounded below one (orange and purple curves in
Figure~\ref{fig:pred0-vs-pred1-mean}).  This shows that a balance between mesh
size, implying computational cost, and boundedness of the metric, implying a
well-resolved solution, is possible in practice.

\begin{figure}\centering
  \includegraphics[width=0.7\columnwidth]{figs/pred0_vs_pred1/mean}
  \vspace{-0.5\baselineskip}
  \caption{Spatial mean of the AMR indicator LogDR~\eqref{eq:ldr} evolving
    in time: AMR without prediction \emph{(top)} and AMR with prediction
    \emph{(bottom)}.
    Decreasing refinement frequencies (RF32, RF16, RF8, RF4) are lowering the
    LogDR metric, whereas with prediction the metric remains at low levels for
    any refinement frequency.}
  \label{fig:pred0-vs-pred1-mean}
%
  \vskip 4ex
%
  \includegraphics[width=0.7\columnwidth]{figs/pred0_vs_pred1/mesh_size}
  \vspace{-0.5\baselineskip}
  \caption{Temporal evolution of the mesh size reported as number of
    \editOne{FV cells}{cells}.
    The \emph{top} and \emph{bottom} graphs and different \emph{colors}
    correspond to the metric shown in Figure~\ref{fig:pred0-vs-pred1-mean}.}
  \label{fig:pred0-vs-pred1-mesh}
\end{figure}

Going beyond observations of the spatial mean of the metric as it evolves in
time, we also give a sense of the distribution of the metric at time instances.
Figure~\ref{fig:pred0-vs-pred1-dist} shows the envelope of one
standard deviation about the mean for each of the experiments discussed
previously.
The bottom of the figure
%
shows how AMR with
prediction is keeping steady control of the distribution of the AMR indicators,
because the curves are staying predominantly flat.
AMR without prediction, however, shown at the top of the figure,
%
demonstrates that large values of the
metric are being created for lower frequencies of refinement (see RF32, green
color).  To reduce the spread of values, one has to refine more
frequently (see RF4, pink color), which consequently comes at an increased
computational cost (see Table~\ref{tab:pred0-vs-pred1} discussed below).

The most extreme discrepancy between AMR with and without prediction is
demonstrated in Figure~\ref{fig:pred0-vs-pred1-contrast}.  The graphs in this
figure show envelopes between the spatial mean and maximum values of the metric
as it evolves in time.  That is, these graphs are showing a contrast of the
metric between mean and maximum (note that the spatial minimum of the metric is
zero for all experiments and hence cannot be used to define a contrast).  The
differences between AMR with and without prediction are most pronounced in
these figures, with a factor of $\sim$3 difference between the
top and bottom graphs of Figure~\ref{fig:pred0-vs-pred1-contrast}.
Note that in Figure~\ref{fig:pred0-vs-pred1-contrast} the initial mesh at
$t=0$ is the same for all the cases.  Thus  a few mesh adaptation steps
are needed until a more steady state of the maximum metric is reached, and it takes longer
for RF32-pred, where AMR is performed less frequently.

\begin{figure}\centering
  \includegraphics[width=0.7\columnwidth]{figs/pred0_vs_pred1/dist}
  \vspace{-0.5\baselineskip}
  \caption{Spatial distribution of AMR indicator around the mean from
    Figure~\ref{fig:pred0-vs-pred1-mean} that is shown as an envelope of one
    standard deviation above and below the mean curve. (Note: The vertical axis
    is clipped at zero because the AMR indicator is nonnegative.)
    AMR without prediction is shown in the \emph{top} and AMR with prediction
    in the \emph{bottom} graph.}
  \label{fig:pred0-vs-pred1-dist}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=0.7\columnwidth]{figs/pred0_vs_pred1/contrast}
  \vspace{-0.5\baselineskip}
  \caption{Maximum value of AMR indicator relative to the mean from
    Figure~\ref{fig:pred0-vs-pred1-mean}.
    that is shown as one standard deviation
    above and below the mean curve.
    AMR without prediction is shown in the \emph{top} and AMR with prediction
    in the \emph{bottom} graph.}
  \label{fig:pred0-vs-pred1-contrast}
\end{figure}

We summarize the preceding observations in
Table~\ref{tab:pred0-vs-pred1}.  This table lists summary statistics of the AMR
indicator by taking the time average of the spatial mean, denoted by
$\langle\text{mean}\rangle_t$, and the time averages of the standard deviation
and the maximum, denoted by $\langle\text{std}\rangle_t$ and $\langle\text{max}\rangle_t$,
respectively.  Further, the table lists a time-averaged mesh size and the
maximum mesh size over all time steps.
Complementing the earlier figures, the table presents data
about the computational costs in terms of
the time steps used until final time (these values are closely clustered), the
total number of Newton iterations across all time steps (these increase with
frequency of AMR), and the total number of GMRES iterations used (these follow
a  trend similar to the Newton iterations).
The last column of the table gives the run time of the
simulation overall, which was obtained on one node with 56 CPU cores of the
Frontera supercomputer (details in Table~\ref{tab:frontera}).
The run times show, as expected, that higher frequencies of mesh refinement
result in larger run times.
%
Additionally the run times for AMR with prediction are slightly larger than
for AMR without prediction, while AMR frequency is the same.  However,
we have demonstrated above that the accuracy of the solution is significantly
improved by AMR prediction.  The table shows that the run times are, for
instance, $\sim$15\%
RF8-pred).  This is a modest increase in computational cost but a dramatic
increase in accuracy as measured by the AMR indicator (shown in the second
column).

\begin{table}\centering
  \caption{Comparison of AMR without prediction \emph{(top four rows)} and AMR
    with prediction \emph{(bottom four rows)}.
    The table summarizes
    Figures~\ref{fig:pred0-vs-pred1-mean}--\ref{fig:pred0-vs-pred1-contrast} by
    reporting temporal averages of the AMR indicator and mesh sizes, where
    $\langle\cdot\rangle_t$ denotes a temporal average for the time interval
    $0.5<t\le4$.
    The computational cost is given in terms of iterations of Newton's method,
    the iterations of GMRES for the linearized systems of Newton, right-hand
    side (RHS) evaluations, and run time.}
  \label{tab:pred0-vs-pred1}
  \centering
  \scriptsize
  \input{inc/tab_pred0_vs_pred1}
\end{table}

We further want to illustrate the  quantitative observations with
qualitative figures of the numerical solution.
%
The effects of the different AMR settings can be observed qualitatively by
visualizing the solution and the associated mesh. This is done in Figure~\ref{fig:pred0-vs-pred1-t1.99},
where the plots visualize the solution at time $t=1.99$.
Looking at the edges of the solution's distribution, we can clearly see
artifacts for nonpredictive AMR in the left column of the figure, when the
refinement lags behind the solution.  Frequency RF32 (top left) shows the most
severe artifacts.
These artifacts can be ameliorated by faster refinement frequencies, RF32,
RF16, RF8, and RF4 (top left to bottom left, respectively).
%
On the other hand, in the right column of Figure~\ref{fig:pred0-vs-pred1-t1.99}
when AMR prediction is active, the solution's features are resolved as well as or better
than was the case for RF4 (bottom left) before.  This result holds for all
plots: RF32-pred, RF16-pred, RF8-pred, and RF4-pred (top right to bottom right,
respectively).
The additional mesh cells that AMR prediction generates are most pronounced for
RF32-pred (top right in Figure~\ref{fig:pred0-vs-pred1-t1.99}), where the
prediction window is the longest.  This illustrates the additional
computational cost that was observed before (e.g., in
Table~\ref{tab:pred0-vs-pred1}).  Reducing the prediction window to RF16-pred
or RF8-pred results in a better balance between extra generated cells and
resolving the numerical solution.

\begin{figure}\centering
  \includegraphics[width=0.98\columnwidth]{figs/pred0_vs_pred1/pred0_vs_pred1_t1.99}
  \caption{Visualization of the refinement levels of the dynamically adapted
    mesh \emph{(white lines)} at $t=1.99$, without prediction \emph{(left
    column)} vs.\ AMR with prediction \emph{(right column)}, while showing the
    numerical solution in \emph{colors}.  The four \emph{rows} of plots
    correspond, from top to bottom, to RF32, RF16, RF8, and RF4.}
  \label{fig:pred0-vs-pred1-t1.99}
\end{figure}

%

\subsection{Algorithmic robustness under different damping coefficients and electric fields}
\label{sec:robustness-results}

The first practical study focuses on the impact of two important coefficients, the damping coefficient $\alpha$, and the electric field $E$.
In this study, the Fokker--Planck collision is turned on as usual while the knock-on source is turned off.
The runs here all use the same initial condition, which is a Maxwellian with a small perturbation in the tail region.
We vary two coefficients in a range close to practice.
The AMR algorithm in this example uses a base mesh of $48\times 8$ for a domain of $[0.3, 60]\times[-1, 1]$ and a total of 7 levels of refinement.

{
\newcommand{\trimfig}[2]{\trimFig{#1}{#2}{.0}{.0}{0}{0}}
\begin{figure}[htb]
\begin{center}
\begin{tikzpicture}[scale=1]
\useasboundingbox (0,-.5) rectangle (12,2.5);
\draw(-.5,-.1) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/compare/alpha_0p1}{4.35cm}};
\draw(3.9,-.1) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/compare/alpha_0p01}{4.35cm}};
\draw(8.3,-.1) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/compare/alpha_0p001}{4.35cm}};
\draw(0,-1.2) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/compare/range_log}{12cm}};
%
%
\end{tikzpicture}
\end{center}
  \caption{Comparison of RFP solutions under different damping coefficients $\alpha$. The field is fixed as $E=0.5$, and the final time is $T=1$.
  The knock-on source is turned off. Note that the solutions are presented in the log scale and spread over more than 15 orders of magnitude.
  }
  \label{fig:compare_alpha}
\end{figure}
}


Distribution functions at $T=1$ with different damping coefficients are presented in Figure~\ref{fig:compare_alpha}.
One can see that a larger damping term leads to the initial high-energy perturbation moving close to the low-energy region (low $p$).
Meanwhile, Maxwellian bulks at the low-energy region are  similar under different $\alpha$, as expected.
Note that solutions in Figure~\ref{fig:compare_alpha} are presented in  log scale, which shows the distribution function spreads more than 15 orders of magnitude.


{
\newcommand{\trimfig}[2]{\trimFig{#1}{#2}{.0}{.0}{0}{0}}
\begin{figure}[htb]
\begin{center}
\begin{tikzpicture}[scale=1]
\useasboundingbox (0,-.5) rectangle (12,2.5);
\draw(-.5,-.1) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/compare/E_10}{4.35cm}};
\draw(3.9,-.1) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/compare/E_5}{4.35cm}};
\draw(8.3,-.1) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/compare/E_1}{4.35cm}};
\draw(0,-1.2) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/compare/range_log}{12cm}};
%
%
\end{tikzpicture}
\end{center}
  \caption{Comparison of RFP solutions under different electric fields $E$. The damping coefficient is fixed as $\alpha=0.01$, and the final time is $T=1$.
  The knock-on source is turned off. Note that the solutions are presented in  log scale and spread over more than 15 orders of magnitude.
  }
  \label{fig:compare_e}
\end{figure}
}


Distribution functions at $T=1$ with different electric fields are presented in Figure~\ref{fig:compare_e}. We find that a larger $E$ field will push the entire distribution to the high-energy region, as expected.
The expansion of the Maxwellian is found to be faster in a larger $E$ case. In practice, however, the actual distribution will be a result of the interplay of many factors such as the damping force, the electric field, and the Fokker--Planck collision.
Therefore, the distribution will not  move all the way to the high $p$ region but instead form an interesting fat tail in a moderate high $p$ region. This effect will be studied more carefully later.
In addition, we  find that AMR can capture both the Maxwellian expansion and the movement of the tail very well (not presented here).


The second focus of this example is to study the performance of the numerical algorithms under different coefficients.
Such a study for our solver is not easy because the time steps and  details of adaptive meshes change dynamically throughout the simulations.
(Here we use ESDIRK and estimate the time step  using local truncation error estimators.) In addition, when the coefficients change, the numerical difficulty of the model also changes.
To have a fair comparison, we run all the simulations with the same final time $T=1$ and average all the important algorithm quantities over the number of time steps.

\begin{table}[htb]
\caption{Comparison of RFP solver performance under different damping coefficients $\alpha$. The field is fixed as $E=2$, and the final time is $T=1$.
  \label{table:compare_alpha}}
\centering
\vskip 1ex
\footnotesize
\begin{tabular}{ccccc}
\toprule
  \thead{$\alpha$} & \thead{Averaged $\Delta t$} & \thead{Averaged DOFs} &  \thead{RHS eval./solve} & \thead{GMRES it./solve} \\
\midrule
  \num{1}{-1} & 0.00351 & 132358 & 42.85 & 4.49 \\
  \num{5}{-2} & 0.00420 & 120592 & 43.02 & 4.52 \\
  \num{2}{-2} & 0.00441 & 114166 & 43.17 & 4.67 \\
  \num{1}{-2} & 0.00457 & 111694 & 43.22 & 4.66 \\
  \num{5}{-3} & 0.00463 & 110459 & 43.02 & 4.71 \\
\bottomrule
\end{tabular}
\end{table}

We start with testing the damping coefficients. Recall that the practical range of $\alpha$ is 0.001 to 0.3. We test several $\alpha$ and present the corresponding results in Table~\ref{table:compare_alpha}.
The electric field is fixed as $E=2$, which is above the so-called avalanche threshold (see Figure 7 in~\cite{mcdevitt2018relation}).
Here we use hypre's algebraic multigrid solver (BoomerAMG) as the preconditioner for the linearized system.
We further choose hypre's parallel ILU solver as the multigrid smoother (Euclid) and use default values  in the rest hypre options.
Table~\ref{table:compare_alpha} shows that the algorithm performs  well for different $\alpha$.
We also note that as $\alpha$ increases, the averaged time step becomes smaller, and the averaged AMR degrees of freedom become larger,
which all indicate that the problem becomes harder for large $\alpha$.
Nevertheless, the  solver still performs well. %
We also present the averaged RHS evaluation per solve. The majority of those evaluations come from evaluating the Jacobian through finite difference coloring.

\begin{table}[htb]
\caption{Comparison of RFP solver performance under different electric fields $E$. The damping coefficient is fixed as $\alpha=0.1$, and the final time is $T=1$. \label{table:compare_e}}
\centering
\vskip 1ex
\footnotesize
\begin{tabular}{ccccc}
\toprule
  \thead{$E$} & \thead{Averaged $\Delta t$} & \thead{Averaged DOFs} &   \thead{RHS eval./solve} & \thead{GMRES it./solve} \\
\midrule
 2  & 0.00351 & 132358 & 42.85 & 4.49 \\
 5  & 0.00244 & 130881 & 43.02 & 4.50 \\
 10 & 0.00192 & 131288 & 43.12 & 3.96 \\
 15 & 0.00183 & 132139 & 41.82 & 3.70 \\
 20 & 0.00179 & 132657 & 41.77 & 3.51 \\
\bottomrule
\end{tabular}
\end{table}

We then consider the impact of the electric field. A reasonable region for the normalized electric field from practice would be the interval $[0, 20]$. We test several $E$ and present the corresponding results  in Table~\ref{table:compare_e}.
We find that the linear and nonlinear solvers all perform well. We also note that the averaged time steps become smaller for larger $E$, which is expected as the problem becomes  stiffer.

%

\subsection{Algorithmic and parallel scalability}
\label{sec:scalability-results}

This section presents the \editOne{}{algorithmic as well as} parallel
scalability of the overall numerical simulations.
\editOne{Included are}
{One aspect of scalability is algorithmic scalability, which is the dependence
of Newton and/or Krylov iterations on the spatial and temporal resolutions.  The
second aspect is parallel scalability of the implementation, which is the
runtime measured on increasing numbers of compute cores.  Studying both aspects
is required to fully assess the performance of a solver at scale.
The runtimes measuring parallel scalability include}
the AMR algorithms, the numerical scheme, the linear solver, and the setup of
the AMG-based preconditioner.  The computations are carried out on the Frontera
system,  a CPU-based platform detailed in Section~\ref{sec:hw-sw}.

\editBOne{The setup of the scalability runs is challenging because during a
simulation the mesh is adaptively refined in a dynamic fashion, where the
refinement and coarsening depend on properties of the solution (see
Section~\ref{sec:amr}).  Therefore, it is difficult to increase the problem size
proportionally with the number of compute cores.  Because of these difficulties
we choose to present a sequence of strong scalability results only, meaning we
omit the demonstration of weak scalability.}{}
In order to obtain strong scalability results, the parameters of the problem,
and the problem size specifically, remain the same for each run while the number
of compute cores increases by a factor of 2.
\editBOne{}{The setup of weak scalability runs entails the challenge to control
the increase of the problem size proportional to the number of compute cores,
because the mesh is adaptively refined in a dynamic fashion depending on
properties of the solution.}

We generate different experiment configurations that exhibit increasing problem
sizes.  Each of these configurations provides a new baseline for showing strong
scalability, where the baseline core counts are increasing with larger problem
sizes:
(i) problem size with 1.4 million (time-averaged) \editOne{FV cells}{cells} is scaled from 112 to 7,168 cores;
(ii) problem size with 2.9 million (time-averaged) \editOne{FV cells}{cells} is scaled from 224 to 14,336 cores; and
(iii) problem size with 4.4 million (time-averaged) \editOne{FV cells}{cells} is scaled from 448 to 28,672 cores.
These three different mesh sizes feature increasingly aggressive mesh adaptivity,
which is measured by the difference between finest mesh level and coarsest mesh
level of refinement;  we call this the \emph{mesh level contrast}.
The mesh level contrast for the above described problem sizes is
(i) 8 levels contrast,
(ii) 9 levels contrast, and
(iii) 10 levels contrast.
%
In addition to the different mesh configurations, we alter the frequency
of mesh adaptation analogous to Section~\ref{sec:pred0-vs-pred1}.  As before,
the label RF32 stands for performing mesh adaptivity after every 32 time steps,
and additionally reported refinement frequencies are RF64, RF128, and RF256.
The AMR indicators are predicted, as proposed in Section~\ref{sec:amr-pred},
for all of these frequencies.

The RFP model parameters for this study are $E=5$ and $\alpha=0.1$, which
correspond to a realistic parameterization (see
Section~\ref{sec:robustness-results}). The Fokker--Planck collision is
turned on, while the knock-on source is turned off.
%
To demonstrate scalability, we prescribe 512 time
steps of the implicit second-order ESDIRK
time integrator of PETSc with a constant time step length that decreases along
with finer mesh resolutions. We use the QUICK scheme.

\editOne{}{
Table~\ref{tab:alg-scalability} demonstrates the algorithmic scalability of the
overall solver.  It shows along the columns the three increasing problem sizes
from 1.4 million to 4.4 million cells; and it also lists the varying refinement
frequencies, RF512 to RF32, in each row.  The number of GMRES iterations per
time step is given either as a range, to present its variation with increasing
core counts, or as a single number if there are no variations.  Recall that the
number of time steps is prescribed and constant. Moreover, the stopping
criterion for GMRES is the tolerance $10^{-6}$ for relative residual reduction.
We can see from the table that the number of GMRES iterations per time step is
around 14 for the smallest problem size (1.4~M cells), and it is six for the
larger problem sizes (2.9 and 4.4~M cells).  This demonstrates optimal
algorithmic scalability, because the number of iterations is not increasing as
the mesh is refined.  The larger GMRES iteration count of $\sim$14 for 1.4~M
cells can be explained by slightly worse conditioning of the disretized PDE
operator, which can be caused by large variations in the coefficients relative
to the resolution of the mesh.
}

\begin{table}\centering
  \caption{\editOne{}{Algorithmic scalability results show the number of GMRES
  iterations per time step either as a range (\emph{min \ldots\ max} value) over
  runs with increasing core counts or as a single number (when min is equal to
  max).  Columns show increasing problem sizes as number of cells, and rows show
  decreasing refinement frequencies.
  The number of GMRES iterations needed per solve is not increasing as the mesh
  becomes finer, which means the solver achieves optimal algorithmic scalability.
  }}
  \label{tab:alg-scalability}
  \centering
  \footnotesize
  \input{inc/tab_algorithmic_scalability}
\end{table}

Implicit solvers with variable coefficients and discretized on adaptive meshes
are well known to be challenging to scale strongly in parallel requiring
significant efforts (e.g., see \cite{rudi2015extreme} for variable coefficient
Poisson and Stokes solvers).
%
\editBOne{Each of the three figures shows strong scalability for one of the
defined problem configurations: Figures~\ref{fig:strong-scalability-1}
and~\ref{fig:strong-scalability-3}
correspond to the strong scalability results for the problem sizes
(i) 1.4 million,
(ii) 2.9 million, and
(iii) 4.4 million \editOne{FV cells}{cells}, respectively.}
{The following two figures show strong scalability for the previously defined
problem configurations:
Figures~\ref{fig:strong-scalability-1} corresponds to problem size (i) 1.4 million and
Figures~\ref{fig:strong-scalability-3} corresponds to problem size (iii) 4.4 million
(we omit the presentation for problem size (ii) 2.9 million for brevity).}
Each figure also shows four differently colored curves, which we use to
distinguish between refinement frequencies.
%
%
Every figure shows the number of cores on the horizontal axis.  Below the core
count, the average number \editOne{FV cells}{cells} per core is reported in
parentheses, which tells how large the portion of the distributed problem is for
each core.
%
Mainly because of the reduction in problem size per core, the
communication starts to dominate the simulation's run time, because the fewer
degrees of freedom that reside at a compute unit's local memory, the more
communication needs to be performed via MPI.
%
The vertical axis shows the speedup in run time, which is calculated as the
quotient of the baseline run time over the run time with associated core count.
We report normalized speedup, where the run time is normalized with respect to
the number of GMRES iterations per time step, because we document parallel
scalability separately from algorithmic scalability.
The speedup depicted by the gray dashed line is the idealized speedup,
%
which, for for implicit solvers, is well known to be out of reach.

\begin{figure}\centering
  \includegraphics[width=0.9\columnwidth]{figs/frontera_scalability/strong_scalability_0002-0128nodes}
  \vspace{-0.5\baselineskip}
  \caption{Strong scalability results on Frontera from 2 to 128 nodes, where
    the baseline problem has 1.4 million
    \editOne{FV cells}{cells}
    and a range of 8 levels of refinement; $\Delta t=10^{-5}$.}
  \label{fig:strong-scalability-1}
%
  \vskip 4ex
%
%
%
%
%
%
%
%
%
  \vskip 4ex
%
  \includegraphics[width=0.9\columnwidth]{figs/frontera_scalability/strong_scalability_0008-0512nodes}
  \vspace{-0.5\baselineskip}
  \caption{Strong scalability results on Frontera from 8 to 512 nodes, where
    the baseline problem has 4.4 million
    \editOne{FV cells}{cells}
    and a range of 10 levels of refinement; $\Delta t=10^{-6}$.}
  \label{fig:strong-scalability-3}
%
  \vskip 4ex
%
  \includegraphics[width=0.9\columnwidth]{figs/frontera_scalability/weak_scalability_0008-0512nodes}
  \vspace{-0.5\baselineskip}
  \caption{Weak scalability results on Frontera from 8 to 512 nodes, where
    the problem sizes range from 112 thousand (at 448 cores) to 5.4 million
    cells (at 28,672 cores).}
  \label{fig:weak-scalability}
\end{figure}

Each of the Figures~\ref{fig:strong-scalability-1} and~\ref{fig:strong-scalability-3}
shows a general trend of speedup that is unavoidable when strongly scaling an
implicit method: At first, the speedup stays close to the ideal speedup with
initial increases in cores; this is where the run time is dominated by
computations and/or communication is overlapped with computations.  Speedup
then gradually deviates from ideal and starts to flatten up to a certain
increase in cores.  After the flattening, the speedup decreases because the
time used for communication dominates compared with the time used for
computations.
Our strong scalability results follow this general trend. Specifically,
they demonstrate that the highest achievable speedup is 12.2 (green curve at
3,584 cores in Figure~\ref{fig:strong-scalability-1})
The highest speedup is slightly lower in
Figure~\ref{fig:strong-scalability-3} (green curve reaches speedup of 8).
We show the scalability of different refinement frequencies because this work
focuses on the AMR algorithms as well as AMR-induced overheads.
The more often the mesh is refined dynamically (i.e., the lower the refinement
frequency), the wider the gap is between the case with only two refinements during
the whole simulation (green curve for RF256) and the frequencies RF128
(orange), RF64 (purple), and RF32 (pink).
This implies that to achieve better scalability in our solver, mesh adaptation would
need to happen less often.
Note that as discussed in Section~\ref{sec:dmbf}, the AMR algorithm based on p4est
is highly scalable. The inefficiency comes from the overhead in rebuilding various
operators when the mesh is updated.
 However, less frequent adaptation of the mesh is
detrimental for the accuracy of the solution, as demonstrated in
Section~\ref{sec:pred0-vs-pred1}.
These conflicting objectives need to be balanced, which was the main motivation
in proposing indicator prediction for AMR, which achieves control over higher
accuracies while keeping the impact on computational overheads, and hence
on scalability, at a minimum.
With the demonstrated improvements in accuracy from
Section~\ref{sec:pred0-vs-pred1} due to AMR prediction, we can expect that each
of the refinement frequencies in Figures~\ref{fig:strong-scalability-1}
and~\ref{fig:strong-scalability-3}
resolves the solution similarly well.  Therefore, one can choose the
frequency that is most advantageous for scalability without sacrificing
accuracy.

\editBOne{}{The strong scalability results of
Figure~\ref{fig:strong-scalability-3} are complemented by weak scalability
results in Figure~\ref{fig:weak-scalability}.
We observe a nearly ideal speedup up to 7,168 cores and see a slight deviation
from the ideal for the maximum number of cores. Because of the dynamic
adaptivity, the problem sizes are not perfectly proportionally increasing with
core counts; see the fewer cells per core shown below the numbers of cores in
brackets.  This is a contributing reason that the weak scalability at 28
thousand cores performs slightly worse.
%
The trend of fewer refinements with RF256 (green curve) performing better than
the other refinement frequencies is similarly observed for weak scalability as
it was for strong scalability.
%
Note that we also observe better than ideal speedups for 1,792 cores, which is
a known phenomena for time dependent solvers.
}

%

\subsection{Benchmark with nonlinear PDE}
\label{sec:nlpde}

\editBOne{}{%
One issue of interest is how our scalable implicit solver and
dynamic AMR methods would perform for nonlinear PDEs as opposed to
the linear relativistic kinetic equation shown in this paper.  In the
context of runaway electron dynamics in a magnetized plasma, the
nonlinear coupling is through the inductive parallel electric field,
which is described by a modified Ohm's law in which the plasma current
density is replaced by the difference between total current density
and the runaway current density.  The leading order physics has a
dynamically changing parallel inductive electric field that is
independent of spatial position, so the nonlinear coupling via the
inductive electric field is numerically straightforward, and the
implementation of which is trivially simple compared with the AMR
kinetic solver in momentum space. To give a
  more reasonable test of the AMR machinery for nonlinear problems, we
  design new numerical experiments and
  implement them within our solver codes.
To this end we take
\editCOne{the two-dimensional viscous Burgers' equation}
{a two-dimensional nonlinear convection--diffusion equation}
as a test problem because of its common use for numerical benchmarks.

The domain supporting the PDE is a channel of extensions $x \in [0,6]$
horizontally and $y \in [-1,1]$ vertically.  We seek the solution $f(t,x,y)$ of
\editCOne{Burgers' equation}{the following PDE}
written in conservative form
\begin{equation}
  \label{eq:burgers}
  \frac{\partial f}{\partial t} +
  n_x \frac{\partial}{\partial x} \left( \frac12 f^2 \right) +
  n_y \frac{\partial}{\partial y} \left( \frac12 f^2 \right)
  =
  \nu \frac{\partial^2 f}{\partial x^2} +
  \nu \frac{\partial^2 f}{\partial y^2},
\end{equation}
for $t>0, x \in (0,6), y \in (-1,1)$, where $\mathbf{n}=(n_x,n_y)$ is a
given unit vector, and $\nu>0$ is a given diffusion coefficient.
Equation~\ref{eq:burgers} is complemented with Neumann boundary conditions and we
propagate an initial Gaussian distribution
$f(0,x,y) = \exp\left( -(x - 5)^2/0.16 \right) + \exp\left( -y^2/0.16 \right)$
with the direction of advection $\mathbf{n} = (-1,0)$ and viscosity $\nu =
10^{-4}$ until final time $T_\mathrm{final}=20$.
%
\editCOne{}{A one-dimensional version of \eqref{eq:burgers} was discussed in
\cite{Burgers1948}, and in the case $\nu=0$, the PDE is commonly referred to as
Burgers' equation.}
Due to the nonlinear nature of the PDE \eqref{eq:burgers}, the changing
advective speed results in the formation of discontinuous solutions.  This
behavior is similar to shock waves, and the shock dissipates as the solution
travels because of the viscosity.

Dynamic AMR is performed every 100th time step with prediction of adaptive
refinement using a linear approximation to \eqref{eq:burgers} and $\nu=0$, such
that a computationally cheap explicit scheme can be employed for propagation of
AMR indicators.  The overall levels of mesh refinement ranges from two to
eight, resulting in a mesh level contrast of six.
%
Figure~\ref{fig:burgers} presents the snapshots of the numerical solution at
times $t=0,5,10,20$.  We observe the formation of a shock discontinuity at the
left tip of the solution.  AMR is facilitating a higher resolution of the
solution's sharp features as is depicted with a mesh wireframe overlaying the
solution function at final time $t=20$.
}

\begin{figure}\centering
  \includegraphics[width=0.98\columnwidth]{figs/burgers/solution_evolution}
  \caption{\editBOne{}{%
    The numerical solution of the \editCOne{viscous Burgers' equation}{nonlinear
    PDE \eqref{eq:burgers}} shown as
    snapshots over time. At final time $t=20$, the solution is overlaid with the
    mesh wireframe to show the adaptive refinement resolving sharp features of
    the solution.}}
  \label{fig:burgers}
\end{figure}

%

\subsection{Compatible boundary condition at $p=0$ with AMR}
\label{sec:p-eq-zero}

{
\newcommand{\figWidth}{2.4cm}
\newcommand{\trimfig}[2]{\trimFig{#1}{#2}{.0}{1}{0}{0}}
\newcommand{\trimfigb}[2]{\trimFig{#1}{#2}{.0}{0}{0}{0}}
\begin{figure}[tb]
\begin{center}
%
\begin{tikzpicture}[scale=1]
\useasboundingbox (0.0,0) rectangle (12,5);  %
\draw(0,   -.5) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/primary/t0}{\figWidth}};
\draw(2.5,-.5) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/primary/t2}{\figWidth}};
\draw(5,   -.5) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/primary/t4}{\figWidth}};
\draw(7.45,-.5) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfigb{figs/primary/t4mesh}{4.7cm}};
%
%
%
\end{tikzpicture}
%
\end{center}
  \caption{\editBOne{}{%
    Left: Distribution functions at $t=0$, 2 and 4. The top row uses the
    Dirichlet boundary condition at $p=3\hat{v}_t$ and the bottom row uses the
    compatible boundary condition at $p=0$.  Right: The adaptive mesh at the
    final time.}}
  \label{fig:pzero}
\end{figure}
}

\editBOne{}{
In this test, we provide an extension of the boundary up to $p=0$. This study uses a primary runaway electron test, in which the initial condition is given as
\[
f_0(p,\xi) = \frac{1}{\hat{v}_t^3 \pi^{\frac{3}{2}} } \exp{\left(\frac{1-\sqrt{1+p^2}}{\hat{v}_t^2/2}\right)},
\]
where the normalized thermal velocity is taken as $\hat{v}_t = 0.1$.
The runaway electron in the tail region will grow exponentially due to the driving electric field. We select an electric field of $E=3$, which is well above the runaway electron threshold.
We extended the idea given in~\cite{mohseni2000numerical} to handle the boundary
condition at $p=0$ with AMR.
Using the expansion of $f(p, \xi)$ with azimuthal symmetry, it can be derived that the compatible boundary condition for distribution at $p=0$ should be
\begin{equation}
  \label{eq:p-zero-bc}
  f(-p,-\xi) = f(p,\xi).
\end{equation}
This condition is used to fill in the ghost point at the negative $p$ location.
%
We utilize the connectivity of trees of the p4est library to connect two coarse
quadrants in $\xi$-direction along the $p=0$ quadrant face; and we also flip the
orientation of this connectivity to account for the alternating sign of the
$\xi$ argument in \eqref{eq:p-zero-bc}.  Therefore, the mesh adaptivity
along the $p=0$ boundary can be treated consistently with all other cell
boundaries with varying levels of refinement.
The results of two different types of boundary conditions are presented in Figure~\ref{fig:pzero}.
The physical locations are perfectly aligned in the figure for easy comparison.
It is clear to see that the boundary layer of $p=0$ is thicker than that of $p=3\hat{v}_t$, but they match well starting from $p=3\hat{v}_t $,
despite different types of boundary conditions being used.
It is also observed that the shapes of runaway tails match well throughout the entire run.
At the final time, it is observed that the details of runaway tails are slightly different,
which is due to the impact of different boundary conditions.
In our forthcoming physics study, we will document the specifics of runaway growth rates in the two different setups.
}



%

\subsection{Interaction between a Maxwellian and a runaway tail}

We next present a physics-motivated  example to demonstrate the AMR capability for resolving both  of a bulk Maxwellian and a runaway distribution tail.
Our aim is to study the algorithm performance in a practical setting, and thus we design a test that is still relatively simple.
The problem is initialized with a Maxwellian uniformly in $\xi$  under a small perturbation in the tail, which is centered around $(p, \xi) = (40, -0.9)$,
\[
f_0(p,\xi) = \frac{1}{\hat{v}_t^3 \pi^{\frac{3}{2}} } \exp\editBOne{}{\left(\frac{1-\sqrt{1+p^2}}{\hat{v}_t^2/2}\right)} + 10^{-15} \exp\left(\frac{(p-40)^2}{25} \right) \exp\left(\frac{-(\xi+0.9)^2}{0.0025} \right),
\]
where the normalized thermal velocity is taken as $\hat{v}_t = 0.1$. A Neumann boundary condition is applied at the right boundary, while the left boundary uses a Dirichlet boundary condition from
the initial Maxwellian. The computational domain is $[0.3, 60]\times[-1, 1]$ as we let $p_{\min} = 3 \, \hat{v}_t$ for which the Dirichlet is still a good approximation.  We choose a numerical scheme based on the QUICK finite difference scheme and an  implicit second-order ESDIRK integrator through PETSc's TS interface.

The proposed AMR algorithm and indicators are tested using this problem.
The coarsest mesh in the AMR algorithm is chosen to be $48\times 8$, and we use a maximum of 6 more levels of refinement.
Initially, we enforce the AMR algorithm to refine around the left and bottom boundaries for better resolving the Maxwellian bulk and the large portion of the perturbation (see the top row in Figure~\ref{fig:chiu1}).
After the initial indicator is computed from the given solution, the proposed AMR indicator prediction is used to evolve those indicator values over time.
The mesh is checked and updated every 6 time steps. The averaged number of DOFs
in the whole simulation is only \editOne{287,684}{71,156, which amounts to
a reduction in DOFs of $\sim$4.5\%
with the same finest level as the adaptive mesh.}
The previous work~\cite{GuoMcDevittTang2017} needed \editOne{several}{0.62}
million DOFs through a structured stretched grid to have a comparable
resolution, \editOne{}{which again highlights the computational savings obtained
with AMR.}

{
\newcommand{\figWidth}{12cm}
\newcommand{\trimfig}[2]{\trimFig{#1}{#2}{.0}{.0}{0}{0}}
\begin{figure}[tb]
\begin{center}
%
\begin{tikzpicture}[scale=1]
\useasboundingbox (0.0,0) rectangle (12,11.7);  %
\draw(0,9.2) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/chiu2/full0}{\figWidth}};
\draw(0,6.9) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/chiu2/full1}{\figWidth}};
\draw(0,4.6) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/chiu2/full2}{\figWidth}};
\draw(0,2.3) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/chiu2/full3}{\figWidth}};
\draw(0,-.62) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/chiu2/full_range}{\figWidth}};
\draw(0,0) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/chiu2/full4}{\figWidth}};
%
%
%
%
\end{tikzpicture}
%
\end{center}
  \caption{Distribution functions (left), mesh and AMR levels (middle), and MPI ranks (right) over time.}
  \label{fig:chiu1}
\end{figure}
}

The numerical results are presented in Figure~\ref{fig:chiu1}. The distribution solutions along with the adaptive meshes are shown.
We note that the Maxwellian bulk and the tail perturbation are slowly merging
into each other due to the interaction between the driven electric field, the
relativistic Fokker--Planck collision, synchrotron radiation,
and the secondary knock-on source. Here we let the damping strength be $\alpha=0.1$ and turn on the partial screening effects. We note that our AMR algorithm captures the interesting features in the solution very well.
The finest mesh is around the region where it has a large gradient, which is either near the Maxwellian boundary layer or near the runaway tail.
It is also critical for this test that the solver be capable of resolving a large variation of the solution. The solution varies from $10^{-2}$ (the bulk Maxwellian boundary layer) to $10^{-20}$ (the runaway tail).
In order to demonstrate the large variation, the distribution function in Figure~\ref{fig:chiu1} is presented in the log scale, which shows that our adaptive solver is capable of resolving both regions.
To demonstrate the dynamic load-balancing, we include the MPI ranks  in the figure.
Note that this test is designed to be relatively simple so that 32 processors are sufficient.




{
\newcommand{\drawPlot}[4]{%
\begin{scope}[#1]
\draw(0,0) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/chiu2/int#2}{\figWidth}};
\draw(3.5,.7) node[draw,fill=white,anchor=east,xshift=2pt,yshift=0pt,scale=0.8]{\scriptsize #3};
\end{scope}
}

\newcommand{\figWidth}{4.cm}
\newcommand{\trimfig}[2]{\trimFig{#1}{#2}{.0}{.0}{0}{0}}
\newcommand{\trimfigb}[2]{\trimFig{#1}{#2}{.0}{.0}{0}{0.1}}
\begin{figure}[htb]
\begin{center}
%
\begin{tikzpicture}[scale=1]
\useasboundingbox (0.0,-.5) rectangle (12,5.4);  %
\drawPlot{xshift= -.1cm,yshift=2.2cm}{0}{$t=0$}{};
\drawPlot{xshift= 4cm,yshift=2.2cm}{1}{$t=0.269$ }{};
\drawPlot{xshift= 8.1cm,yshift=2.2cm}{2}{ $t=0.669$}{};
\drawPlot{xshift= 1.95cm,yshift=-1cm}{3}{ $t=1.769$}{};
\drawPlot{xshift= 6.05cm,yshift=-1cm}{4}{ $t=2.669$}{};
%
%
%
\end{tikzpicture}
%
\end{center}
  \caption{Runaway electron distributions in $p$ over time.}
  \label{fig:chiu_runaway}
\end{figure}
}


In addition to the distribution, we evaluate the so-called runaway electron population $R(p, t)$,
\begin{align*}
R(p, t) :=  \int_{\xi=-1}^{\xi=1} f(p,\xi, t)  v_\parallel 2\pi p^2  \, d\xi,
\end{align*}
where the parallel velocity is defined as  $v_\parallel = {p \, \xi}/{\gamma \, m_e}$.
This is a distribution in the $p$ direction to measure the runaway electron strength, which is a good indicator of the number of high-energy runaway electrons in the system.
Figure~\ref{fig:chiu_runaway} shows that initially there is a large portion of runaway electrons due to the perturbation and that they are slowly reduced and merged into the bulk.

{
\newcommand{\trimfig}[2]{\trimFig{#1}{#2}{.0}{.0}{0}{0}}
\begin{figure}[htb]
\begin{center}
\begin{tikzpicture}[scale=1]
\useasboundingbox (0,-.5) rectangle (12,3.8);
\draw(0,-.9) node[anchor=south west,xshift=-4pt,yshift=+0pt] {\trimfig{figs/chiu2/knock_on.png}{12cm}};
%
%
\end{tikzpicture}
\end{center}
  \caption{Knock-on electron source at the first checkpoint.}
  \label{fig:chiu2}
\end{figure}
}

We further present the knock-on electron source term in Figure~\ref{fig:chiu2}. We note that the knock-on source is exactly in the range of $\xi \in [-\sqrt{\gamma/(\gamma+1)}, -p/(\gamma+1)]$ as we indicated in Section~\ref{sec:knock_on}.
The mesh plot of Figure~\ref{fig:chiu2} shows that the AMR algorithm is able to capture the source term in this narrow region through adjusting the mesh on the fly.


