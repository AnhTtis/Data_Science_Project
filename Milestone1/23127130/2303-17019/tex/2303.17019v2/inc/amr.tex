\def\secstatus{finished}

\section{Adaptive mesh refinement: Indicators and their prediction in time}
\label{sec:amr}

%
%
\editAll{Indicators are flags that correspond to each grid point and determine}
{Indicators are spatial fields that determine at each grid point}
whether to refine or coarsen the mesh.
This section presents the indicators for adaptivity and shows how indicators are
propagated forward in time ahead of the simulation in order to predict the
%
 regions of the mesh that need to be refined and derefined.
We consider an abstract setting; hence the discussion is general and applicable
beyond the RFP equation, which is otherwise the focus of this work.
First, we present how the indicators are computed, and then we propose an approach for
the prediction of indicators.

%
%

%

\subsection{AMR indicators for coarsening and refinement}
\label{sec:amr-indicators}

%
We define an indicator as a functional, $\indfn$, that maps a function, say
$f$, to a single positive scalar value for each mesh cell $\Omega_h$
within the entire domain $\Omega$.  Hence we write
\[
  \indfn(f;\Omega_h)\in\R_+ \quad\text{for each }\Omega_h\subset\Omega.
\]
In the following, the dependence on $\Omega_h$ is assumed implicitly such
that we can use the brief notation $\indfn(f) = \indfn(f;\Omega_h)$.
The result is then used to evaluate the criteria for AMR.  A mesh cell is coarsened if the indicator is below a threshold
\[
  \indfn(f) < \indmin,
\]
and it is refined if the indicator is above a threshold
\[
  \indmax < \indfn(f),
\]
where $0\le\indmin<\indmax$ are (prescribed) constants that stay fixed during
the simulation.  These criteria for coarsening and refinement are carried
out uniformly for every cell of the mesh.  This process yields the requirement
that $\indfn(f)$ has to produce values that are independent of any scale,
because the function $f$ is likely to vary significantly.  Indeed, large
variations of $f$ are the reason to utilize AMR in the first place.

%
To proceed to defining AMR indicators, we introduce the following
preliminary definitions and observations.
Let $f:\Omega\rightarrow\R_+$ be a positive\footnote{%
  To simplify the
  notation, we assume the function $f$ to have positive values. One could, however, remove the assumption on positivity and, in place
  of $f$, use $\abs{f}+\varepsilon$ with a chosen $0 < \varepsilon \ll 1$.
} and continuously differentiable
function, $f\in C^1(\Omega)$, over an open domain $\Omega\in\R^d$.  We define
the gradient scale of a function $f>0$ as the nondimensionalized quantity
$(\gradient f)/f$, and as a result the scale of $f$ is neutralized.
It satisfies the \emph{gradient-scale identity}
\begin{equation}
\label{eq:gs-identity}
  \frac{\gradient f(x)}{f(x)} = \gradient\left(\log f(x)\right),
  \quad\text{for }x\in\Omega.
\end{equation}
%
%
%
%
Since we are concerned with meshes of a finite resolution, we denote
$\Omega_h\subset\Omega$ to be a mesh cell with a \emph{characteristic
size} $h>0$, and we let $\overline{\Omega}_h$ be its closure.  Then we define a
\emph{discrete local gradient magnitude}, which is motivated by finite
difference gradients,
\begin{equation}
\label{eq:discrete-grad}
  G_h[f](x) \coloneqq
    \max_{y\in\overline{\Omega}_h} \frac{\abs{f(x)-f(y)}}{h},
  \quad\text{for }x\in\Omega_h,
\end{equation}
where $\abs{\cdot}$ denotes the absolute value.  Note that for
\eqref{eq:discrete-grad} to be well defined, it is only required
for $f$ to be Lipschitz continuous.

We define two AMR indicators based on the gradient scale (GS) in two different
ways, therefore making use of both sides of the identity \eqref{eq:gs-identity}.

\begin{definition}[Gradient-scale indicator]
\label{def:gs-ind}
Let $f>0$ be Lipschitz continuous, and let $\Omega_h\subset\Omega$ be a
cell of the mesh.
The first version of the GS indicator is
%
\begin{equation}
\label{eq:gs}
  \indfn_{GS}(f) \coloneqq
    h \max_{x\in\overline{\Omega}_h} \left( \frac{G_h[f](x)}{f(x)} \right).
\end{equation}
The second version is
%
\begin{equation}
\label{eq:lgs}
  \indfn_{LGS}(f) \coloneqq
    h \max_{x\in\overline{\Omega}_h} \Bigl( G_h[\log f](x) \Bigr).
\end{equation}
\end{definition}

The indicators in Definition~\ref{def:gs-ind} are derived from a discrete
version of the gradient-scale identity with an additional multiplication by the
characteristic size $h$ of the cell.  The multiplication by $h$ reduces
the value of the indicator as the mesh is refined, and it also neutralizes
division by $h$ appearing in the discrete gradient \eqref{eq:discrete-grad}.
Hence, the indicator becomes nondimensionalized regarding the cell
size.  Overall, because we eliminated the scale of the function $f$ and the
scale of the discrete gradient $h$, the resulting indicators $\indfn_{GS}$ and
$\indfn_{LGS}$ are nondimensional.

An alternative indicator based on the dynamic ratio (i.e., maximum value
divided by minimum value of $f$ in $\overline{\Omega}_h$) is defined next.
Subsequently, a relation between the two definitions of indicators is
established in Proposition~\ref{prop:lgs-vs-ldr}.
%

\begin{definition}[Log-DR indicator]
\label{def:ldr-ind}
We define an AMR indicator based on the logarithm of the dynamic ratio (DR)
within a cell $\Omega_h$
\begin{equation}
\label{eq:ldr}
  \indfn_{LDR}(f) \coloneqq
    \log\left( \frac{\max_{x\in\overline{\Omega}_h} f(x)}
                    {\min_{x\in\overline{\Omega}_h} f(x)} \right).
\end{equation}
\end{definition}

\begin{proposition}[Equivalence between gradient scale and log-DR indicators]
\label{prop:lgs-vs-ldr}
Given a Lipschitz continuous function $f>0$, let $\indfn_{LGS}$ be the gradient-scale AMR indicator defined in \eqref{eq:lgs} and $\indfn_{LDR}$ be the log-DR
indicator defined in \eqref{eq:ldr}.  Then the two indicators satisfy
\begin{equation}
\label{eq:lgs-vs-ldr}
  \indfn_{LGS}(f) = \indfn_{LDR}(f).
\end{equation}
\end{proposition}
\begin{proof}
Starting from the definition \eqref{eq:lgs}, we obtain
\begin{align*}
  \indfn_{LGS}(f)
  &=
    h \max_{x\in\overline{\Omega}_h} \Bigl( G_h[\log f](x) \Bigr)
  \\&=
    h \max_{x,y\in\overline{\Omega}_h}
      \left( \frac{\abs{\log f(x) - \log f(y)}}{h} \right)
  \\&=
    \max_{x,y\in\overline{\Omega}_h} \abs{\log\left(\frac{f(x)}{f(y)}\right)}.
\end{align*}
Utilizing the monotonicity of the logarithm and $f>0$, we continue
\begin{align*}
  \max_{x,y\in\overline{\Omega}_h} \abs{\log\left(\frac{f(x)}{f(y)}\right)}
  &=
    \max\left\{
      \max_{x,y\in\overline{\Omega}_h}  \log \left(\frac{f(x)}{f(y)}\right),
      -\min_{x,y\in\overline{\Omega}_h}  \log \left(\frac{f(x)}{f(y)}\right)
    \right\}
  \\&=
    \max\left\{
       \log \left(\max_{x,y\in\overline{\Omega}_h} \frac{f(x)}{f(y)}\right),
      -\log \left(\min_{x,y\in\overline{\Omega}_h} \frac{f(x)}{f(y)}\right)
    \right\}
  \\&=
    \max\left\{
       \log \left( \frac{\max_{x\in\overline{\Omega}_h} f(x)}
                        {\min_{y\in\overline{\Omega}_h} f(y)} \right),
      -\log \left( \frac{\min_{x\in\overline{\Omega}_h} f(x)}
                        {\max_{y\in\overline{\Omega}_h} f(y)} \right)
    \right\}.
\end{align*}
Additionally, the second argument in the outermost maximum is
\[
  -\log \left( \frac{\min_{x\in\overline{\Omega}_h} f(x)}
                    {\max_{y\in\overline{\Omega}_h} f(y)} \right)
  =
  \log \left( \frac{\min_{x\in\overline{\Omega}_h} f(x)}
                   {\max_{y\in\overline{\Omega}_h} f(y)} \right)^{-1}.
\]
Therefore we can simplify
\[
  \max_{x,y\in\overline{\Omega}_h} \abs{\log\left(\frac{f(x)}{f(y)}\right)}
  =
  \log \left( \frac{\max_{x\in\overline{\Omega}_h} f(x)}
                   {\min_{y\in\overline{\Omega}_h} f(y)} \right),
\]
which shows the claim of the proposition.
\end{proof}

\begin{remark}
The indicator $\indfn_{LGS}$ can be seen in relation to the Lipschitz constant.
Let us assume that $K_h$ is a discrete version of the Lipschitz constant such
that
  $\abs{\log f(x) - \log f(y)} \le K_h \abs{x - y}$
for all $x,y\in\overline{\Omega}_h$ with $h\le\abs{x-y}$.
Then the following estimate holds:
\[
    \max_{\substack{x,y\in\overline{\Omega}_h \\ h\le\abs{x-y}}}
    \frac{\abs{\log f(x) - \log f(y)}}{\abs{x - y}}
  \le
    \max_{x,y\in\overline{\Omega}_h}
    \frac{\abs{\log f(x) - \log f(y)}}{h}
  =
    \frac{\indfn_{LGS}(f)}{h}.
\]
Thus the indicator computes an upper bound on the discrete Lipschitz constant scaled by the characteristic cell size:
  $h K_h \le \indfn_{LGS}(f)$.
\end{remark}

\begin{remark}
In order to compute the indicators \eqref{eq:gs}, \eqref{eq:lgs}, and
\eqref{eq:ldr}, a minimum and/or maximum needs to be computed over a
mesh cell.  In practice this can be done by looping over the nodes or
degrees of freedom of the particular discretization.  Furthermore, it is
sufficient to approximate the discrete local gradient magnitude
\eqref{eq:discrete-grad} by utilizing the routines for gradient computation
that are native to the discretization.

For computing the log-DR indicator in \eqref{eq:ldr} from a discretized $f$
that has been polluted by errors due to finite precision arithmetic or solver
truncations, which are unavoidable in practice, we recommend  computing
$\indfn_{LDR}(f+\epsilon)$ with a small constant $\epsilon>0$.  This ensures
numerically that the argument of $\indfn_{LDR}$ is positive.
\end{remark}

\editTwo{}{
Proposition~\ref{prop:lgs-vs-ldr} shows that the gradient-scale and the log-DR
indicators can be used interchangeably.  Specifically, the simple-to-implement
and computationally cheaper log-DR indicator quantifies features in the solution
equally well as the (slightly) more complex gradient-scale indicator.
To decide which formulation of indicator to use depends on a particular
discretization method and the quantities that are computed in the
implementation.  If, for example, a gradient already needs to be computed, then
the gradient-scale indicator is a cheap byproduct of this computation.  For the
finite difference-based discretization that we consider in this work, the log-DR
indicator presents the computationally cheapest option.  This is why we will
utilize log-DR for our adaptivity criterion in the numerical experiments that
follow in Section~\ref{sec:results}.
}

%

\subsection{Dynamic AMR and indicator prediction in time}
\label{sec:amr-pred}

This section is concerned with adapting a mesh dynamically to a function
$f=f(t)$ that is evolving in time.
Dynamic mesh adaptivity is a requirement that is imposed by the PDEs we aim to
solve (see Section~\ref{sec:governing-eqns}).  These PDEs are
advection-dominated in some parts of the domain and diffusion-dominated in
other parts.

%
The mesh can be adapted in between time steps of the implicit time integration
scheme.  To track the variations of the solution as accurately as possible, one
would ideally adapt the mesh in between every time step.  However, this will
increase the computational cost of a parallel implicit solver, because adapting
the mesh induces additional computational costs, such as
redistributing the mesh cells across compute cores, interpolation
between coarse and fine cells, and reallocating and setting up of
solver components.  If the mesh remains unchanged for too many time steps, on
the other hand, accuracy may suffer, and fine-scale features can become
insufficiently resolved because they will migrate from fine mesh cells to
coarse ones.  Therefore dynamic AMR faces two competing constraints:
(i) how long can the time intervals be between adapting the mesh in order to
adequately track the quantity of interest and
(ii) how large is the additional computational cost that each change of the
mesh induces.

%
The goal in this section is to introduce a new technique to balance these two
constraints.  We want to reduce the frequency of changing the
mesh and at the same time provide sufficient resolution where it is needed
(within a certain time interval).  We also want to avoid overly fine meshes
where resolution is not needed.  It therefore is desired to ``predict''  AMR
indicators in a time evolution simulation.  We will show that such a prediction
is possible in an advection-dominated setting.
%

We denote $\Delta t$ as the length of the time step of the time evolution
scheme.  We define a number $n_\mathrm{adapt}\in\N_+$ (i.e., positive integer)
that determines after how many time steps $\Delta t$ adaptivity is triggered;
therefore the mesh remains fixed during the interval
  $\Delta t_\mathrm{adapt} \coloneqq n_\mathrm{adapt} \Delta t$.
This implies that within the $n_\mathrm{adapt}$ time steps the features of a
function $f(t)$, which we want to resolve,
%
will migrate.  This is what we want to address with AMR prediction.
The superscript notation $f^{(A)}(t)$ indicates that $f(t)$ is discretized (in
space) on a particular mesh $A$.

The diagram in Figure~\ref{fig:diagram-amr} illustrates how dynamic AMR is
carried out between time steps without AMR prediction.
For simplicity of the diagram, we set $n_\mathrm{adapt}=1$, and hence
  $\Delta t_\mathrm{adapt} = \Delta t$.
The diagram shows the sequence of mesh adaptivity, where interpolation from
mesh $A$ to mesh $B$ is performed, alternating with the time evolution scheme,
where $f^{(B)}(t)$ evolves to $f^{(B)}(t + \Delta t)$.

With AMR prediction, we aim to avoid a loss of accuracy when fine-scale
features of $f(t)$ migrate away from fine to coarser regions of the mesh.
We propose to predict the ``path of refinement'' by evolving the AMR indicators
``ahead'' of the quantity of interest, $f(t)$, by intervals of at most
$\Delta t_\mathrm{adapt}$.  In effect, we are leapfrogging the AMR
indicators forward in time relative to the (regular) simulation.
%
Dynamic AMR with prediction is illustrated in the diagram in
Figure~\ref{fig:diagram-amr-prediction}.  For simplicity of the diagram, we set
$n_\mathrm{adapt}=n_\mathrm{pred}=1$, and hence
  $\Delta t_\mathrm{adapt} = \Delta t_\mathrm{pred} = \Delta t$.
First, the indicator $\indfn$ is computed as in the previous case without
prediction; however, $\indfn$ is subsequently evolved forward in time by
$\Delta t_\textrm{pred}$ ahead of the simulation.  To keep the
computational costs low, we propose to use an explicit time integration scheme,
which should be significantly faster to carry out compared with the implicit
scheme used to evolve $f$.  As $\indfn$ evolves, it is simultaneously reduced
into a time-independent indicator $\indfn_\textrm{pred}$.  To adapt
the mesh, we then use
$\indfn_\textrm{pred}$, which predicts the path of adaptivity.

\begin{figure}\centering
  \resizebox{0.98\columnwidth}{!}{
    \input{inc/fig_amr_without_prediction}
  }
  \caption{Diagram of \textbf{dynamic AMR without prediction}.
    In the ``interp.'' (interpolation) step, the indicator $\indfn$ is computed
    from $f$, and this indicator is used to adapt the mesh (e.g., from mesh
    $A$ to mesh $B$).
    %
    The ``evolve'' step advances $f$ forward in time by $\Delta
    t_\mathrm{adapt} = \Delta t$.}
  \label{fig:diagram-amr}
  %
  \vskip 8ex
  %
  \resizebox{0.98\columnwidth}{!}{
    \input{inc/fig_amr_with_prediction}
  }
  \caption{Diagram of \textbf{dynamic AMR with prediction}.
    The indicator $\indfn$ is first computed from $f$ and subsequently evolved
    forward in time by $\Delta t_\textrm{pred} = \Delta t$ ahead of the
    simulation, thus predicting the path of AMR.  The evolution of
    $\indfn$ is reduced into one time-independent indicator
    $\indfn_\textrm{pred}$.  In the ``interp.'' (interpolation) step,
    $\indfn_\textrm{pred}$ is used to adapt the mesh (e.g., from mesh $A$ to
    mesh $B$).
    %
    The ``evolve'' step advances $f$ forward in time by $\Delta
    t_\mathrm{adapt} = \Delta t$.}
  \label{fig:diagram-amr-prediction}
\end{figure}

%
%
%
%
%
%
%
%

Algorithm~\ref{alg:amr-prediction} describes in more detail how dynamic AMR
with prediction is carried out during the time evolution of a function $f$.
Specifically, the reduction of the leapfrogged indicator $\indfn$ is performed
by taking the maximum of the evolving indicator across all time steps.

\begin{algorithm}
\caption{AMR prediction by leapfrogging AMR indicator.}
\label{alg:amr-prediction}
\input{inc/alg_amr_with_prediction}
\end{algorithm}

In the general setting, as it has been considered in this section, we have not
made assumptions about the time-stepping methods used to evolve $f(t)$ and to
leap-frog $\indfn$.  The choice of the pairing of these two time-stepping
methods is important when considering the computational complexity of the
overall simulation with dynamic AMR and AMR prediction.
In the context of the governing equations we are aiming to solve
(Section~\ref{sec:governing-eqns}), we are dealing with an advection--diffusion PDE.  The PDE
will require implicit time-stepping schemes, which have a significantly higher
computational complexity when compared with explicit schemes.
The leapfrogging of the indicators $\indfn$, on the other hand, has to be
performed only locally in time: for short time intervals and with a new initial
condition at each invocation of AMR prediction.
Therefore we can relax the requirement for accuracy, and we can approximate
local short-time behavior by eliminating the diffusion term of the PDE.  Hence,
the equation for AMR prediction will be an advection-only PDE, and we will use
the same advection coefficient as for the governing equations of the physical
models (see Section~\ref{sec:governing-eqns})

In
Section~\ref{sec:pred0-vs-pred1} we discuss the increase in accuracy of the numerical solution while keeping the
computational costs low, which is achieved due to AMR prediction.

