\def\secstatus{finished}

\section{Experimental setup}
\label{sec:setup}

This section focuses on
  the derivation of manufactured solutions,
  the setup of numerical experiments of relativistic electron drift-kinetic simulations, and
  the experimental hardware and software environment.
The goal of this section is to ensure the reproducibility of the work and to
introduce the computational studies in the numerical results of
Section~\ref{sec:results}.


\subsection{Manufactured solution}\label{sec:solutions}

To verify the accuracy of our implementation, we rely on the method of manufactured solutions. %
Several exact solutions are constructed for the RFP equation~\eqref{eqn:rfp}.
Consider the original RFP model for the distribution function $f$,
\begin{align}
  \frac{\partial f}{\partial t} - E \xi\frac{\partial f}{\partial p}
   - E \frac{1-\xi^2}{p} \frac{\partial f}{\partial \xi} = C(f),
\end{align}
%
where $C(f)$ represents a general collision operator. To determine an exact solution,
we convert the RFP equation back to the coordinates of
$(p_\parallel,\mathbf{p}_\perp)$, giving
\begin{align}
  \frac{\partial f}{\partial t} - E \frac{\partial f}{\partial p_\parallel} = C(f).
  \label{eqn:rfp2}
\end{align}
Note that without the collision operator the exact solution to~\eqref{eqn:rfp2} satisfies the following identity  between $t=T$ and $t=0$:
\begin{align}
 f(T, \editOne{\textit{p}}{p_\parallel}, \mathbf{p}_\perp) = f(0, p_\parallel + ET, \mathbf{p}_\perp).
\end{align}
Therefore, exact solutions can be constructed by following characteristics. For instance, the exact solutions to the pure advection equation can be
\begin{itemize}
\item exponential solutions, such as
\begin{align*}
 f(t, p, \xi) &= \exp\Big[-p_\perp^2-(p_\parallel+Et)^2\Big] \nonumber\\
 &=\exp\Big[-p^2-2p\xi \,E t -(Et)^2\Big].
%
\end{align*}
 (we note that this exact solution decays properly at $p_{\max}$), or
\item sinusoidal solutions, such as
\begin{align*}
 f(t, p, \xi) =\sin(p\xi +Et), \qquad
\text{or}
\qquad
 f(t, p, \xi) = \cos(p\xi +Et)^2.
\end{align*}
\end{itemize}
Now we consider the RFP equation with a simplified collision operator:
\begin{align*}
  \frac{\partial f}{\partial t} - E \xi\frac{\partial f}{\partial p}
   - E \frac{1-\xi^2}{p} \frac{\partial f}{\partial \xi} =  \frac{\epsilon}{p^2} \frac{\partial}{\partial p}\Big[p^2 \frac{\partial f}{\partial p} \Big] + \frac{\epsilon}{p^2}\frac{\partial }{\partial \xi}\Big[(1-\xi^2)\frac{\partial f }{\partial \xi}\Big].
\end{align*}
One nontrivial exact solution can be derived as
\begin{align}
 f(t, p, \xi)=\sin(p\xi+Et) \exp(-\epsilon t)
\label{eqn:exactCollisionMMS}
\end{align}
through separation of variables~\cite{banks2017stable}.
Note that this collision operator corresponds to a special case of $C_F=0$, $C_A=C_B=\epsilon$ in the general Fokker--Planck collision operator.
These exact solutions can be used to verify numerical solutions with or without collision operators.

\paragraph{Boundary conditions associated with the manufactured solutions}
For practical problems, the RFP equation is discretized in the domain of $[p_{\min}, p_{\max}]\times[-1, 1]$ with $p_{\min}>0$ to avoid the singularity of the factor $1/p$.
We note, however, that the cited exact solutions are valid in the entire domain of $[0, +\infty)\times[-1, 1]$.
Therefore, the time-dependent Dirichlet boundary condition is used in the $p$ direction at both $p_{\min}$ and $p_{\max}$.
%
Because of the coefficients involving the term $1-\xi^2$, no boundary conditions are needed at $\xi=\pm 1$. Alternatively, one can enforce a Dirichlet boundary condition at $\xi=\pm 1$ for testing purposes.


\subsection{Setup of relativistic electron drift-kinetic simulations}

The RFP model considered in the current work has rich physics that are
often used to describe runaway electrons, which are one of the major sources of
tokamak disruptions.  The current work focuses on examining the capabilities
and performance of the proposed RFP solver;  further advancement of
physics studies will be carried out in follow-up work.
This section lays out details of the RFP model in order to introduce the
numerical results in the next section. It is of interest to numerically analyze
the linear and nonlinear solvers in practical physics regimes, examine the
efficiency gained by AMR, and demonstrate the parallel scalability of RFP
simulations.

The RFP model is considered in a domain of $[0.3, 60]\times[-1, 1]$ for
$[p_\mathrm{min},p_\mathrm{max}]\times[\xi_\mathrm{min},\xi_\mathrm{max}]$.
The model's dynamics are  dominated primarily by the interplay of the electric
field, the damping term, and the Fokker--Planck collision operator.
We aim to demonstrate in which regime the solvers and the preconditioners
converge robustly. It is well known that an algebraic multigrid preconditioner
deteriorates in efficacy when the advection--diffusion operator becomes
increasingly nonsymmetric~\cite{manteuffel2018nonsymmetric}.  In the RFP model,
this is possible when the electric field $E$ becomes large.  On the other hand,
there is a well-known threshold that $E$ has to surpass for a runaway
avalanche to happen~\cite{mcdevitt2018relation}.
The solver is desired to be robust at or above this threshold for $E$.

Additionally, we are concerned with the numerical performance in the presence
of the radiation damping term, $R(f)$ in~\eqref{eqn:rfp}.  This  term is close to a friction force, which is effectively another
advection term, and it mainly impacts the high-energy region (large $p$).
This operator interplays with the electric field in the moderately high energy
region, and thus it is critical for the forming of the runaway tail.
We are also interested in determining the range of $\alpha$, which is the
scaling factor of $R(f)$ in~\eqref{eqn:rfp}, where the solver exhibits a robust
performance.

Another important value of interest is the runaway electron population along the $p$-direction. This is computed through a line integration of the distribution along the $\xi$ direction with a metric that will be defined explicitly in the numerical section.
Note that computing a line integral over an adaptively refined mesh becomes a nontrivial task.
In this work we  focus only on the time evolution of this important population.
One immediate next goal would be to couple this population with the field solver so that it leads to a self-consistent model that is capable of describing the impact of the runaway electrons on the tokamak field.

Furthermore, we are interested in evolving the runaway electron distribution under the impact of the secondary knock-on source as described in Section~\ref{sec:knock_on}. Recall that we have shown
that the dominant term of the Chiu knock-on source~\eqref{eqn:chiu_full} is nonzero in a very thin region.
The major difficulty in evaluating~\eqref{eqn:chiu_full} is thus twofold:
(i) the thin region needs to be sufficiently well resolved; and
(ii) the evaluation of the source terms requires another line integral along
  $\xi$ direction, and in a parallel context the resulting integral needs to
  be communicated to each process participating in the distributed parallel
  simulation.
To address (i), we rely on AMR to resolve such a region dynamically in
time.  Therefore, this case demonstrates another aspect for the need of an
adaptive solver.
To address (ii), we design a positivity-preserving interpolation
approach for performing the line integral on an octree-based adaptively refined
mesh, and we implement MPI communication routines to distribute the integral
information across processes.

Unless otherwise noted, adaptivity criteria for mesh refinement and coarsening
are carried out with AMR indicator prediction as proposed in
Section~\ref{sec:amr-pred}.  Other details of AMR will be described in each
numerical example.


\subsection{Hardware and software environments}
\label{sec:hw-sw}

The hardware and software environments are presented in this section for the
purpose of reproducibility of numerical experiments.
The development of the solvers and the studies of the algorithmic performance
were carried out on three different HPC platforms:  Argonne
National Laboratory's Bebop cluster, Los Alamos National Laboratory's Chicoma system, and the
Cori system at the National Energy Research Scientific Computing Center.
The computations pertaining to performance, specifically to demonstrate the
parallel scalability of the solvers, were carried out on the Frontera system at
the Texas Advanced Computing Center (TACC).  Table~\ref{tab:frontera} lists the
hardware specifications of Frontera.
The software environment consists of Intel C/C++ compilers, the Intel MPI
(Message Passing Interface) library, and Intel MKL, as well as the libraries PETSc,
p4est, and hypre.  The versions of the software  utilized for parallel
scalability studies are listed at the bottom of Table~\ref{tab:frontera}.
The PETSc library has been extended with additional code that implements the
data management functionalities for adaptive meshes based on p4est. This work
relies on these extensions (currently residing in a public branch of the PETSc
repository\footnote{%
  \editTwo{}{Link to DMBF code (branch in PETSc repository):}
  \url{https://gitlab.com/petsc/petsc/-/tree/johann/jcp2023/}
}), and they will be integrated into a future release version of PETSc.

\begin{table}\centering
  \caption{Hardware specifications of the TACC Frontera system based on the CPU
    Intel Xeon Platinum 8280 (``Cascade Lake'') and the Mellanox HDR
    interconnect technology. The bottom rows list the software environment
    utilized for our parallel scalability studies.}
  \label{tab:frontera}
  \centering
  \footnotesize
  \input{inc/tab_frontera}
\end{table}

%
%
%
%
%
%
%
%
%
%
%
%

%
