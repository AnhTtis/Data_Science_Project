\def\secstatus{finished}

\section{Relavitistic electron drift-kinetic solver based on PETSc and p4est}
\label{sec:solver}

DMBF (Data Management for Block-structured Forest-of-trees) is a new data
management object in PETSc that we have developed and implemented along with a
new relativistic electron drift-kinetic solver.
We first outline the capabilities of the implementation that are incorporated
into PETSc;  subsequently, we give a high-level description of the new
application code for relativistic electrons.

%

\subsection{DMBF: New PETSc data management for p4est-based AMR solvers}
\label{sec:dmbf}

Our \editAll{}{overall} goal is to discretize a computational domain with locally adaptively
refined quadrilateral meshes in two dimensions.
\editAll{}{In particular for relativistic electrons,} extreme local refinement is
critical for resolving the rapidly changing solution, which can vary 15--20
orders of magnitude, while over regions where the solution is relatively flat,
coarser meshes can help reduce the computational cost.  Therefore, a key
requirement of our solver is adaptive mesh refinement.

%
The dynamic mesh adaptivity in parallel is, at its lowest level, enabled by the
p4est library \cite{BursteddeWilcoxGhattas11, IsaacBursteddeWilcoxEtAl15}.
This library implements hierarchically refined quadrilateral and hexahedral meshes
utilizing forest-of-octree algorithms and space-filling curves
\cite{sundar2008bottom}.
Mesh refinement and coarsening are efficient in parallel because p4est performs
these tasks locally on each compute core without communication.  The
representation of the mesh as a quadtree/octree topology enables efficient 2:1
mesh balancing in parallel as well as repartitioning of the mesh cells
across compute cores, for both of which communication is necessary.
Space-filling curves transform a two- or three-dimensional space that is subdivided in cells
into a (one-dimensional) sequence of cells.  This sequence is used for an efficient
partitioning of mesh cells in parallel with the desirable property of
keeping spatially neighboring cells nearby each other in the sequence;
hence exhibiting memory locality.
This property of space-filling curves is responsible for keeping the amount of
communication low.
Overall, the algorithms of p4est have demonstrated scalability up to
$O$(100,000)
of processes on distributed-memory CPU-based systems
\cite{BursteddeGhattasGurnisEtAl10, SundarBirosBursteddeEtAl12}; moreover,
scalability to $O$(1,000,000) of CPU cores have been achived for complex
implicit PDE solvers \cite{rudi2015extreme, RudiStadlerGhattas17,
RudiShihStadler20}.
Generally, tree-based AMR algorithms have shown impressive scalability
results \cite{weinzierl2019peano, FernandoNeilsenLimEtAl19}.

%
DMBF is a new type of data management in PETSc.
%
It provides interfaces for block-structured forest-of-trees meshes.
The block structure is understood in the sense that leaf elements of the trees
can be equipped with blocks of uniformly refined cells.  DMBF implements
PETSc functions that give direct access to the functionality of p4est with the
least amount of overhead compared with existing PETSc data management approaches.
In addition, DMBF provides cellwise data management for storing any kind
of data that is mapped to the cells of the p4est mesh.
Because DMBF associates cellwise data directly with the p4est mesh, the cell
data managed by DMBF is created and destroyed when p4est cells are refined
and coarsened; and DMBF-managed cell data migrates across processes in parallel
as the p4est mesh is repartitioned.  In the context of discretizations of PDEs,
it is essential that data from neighboring cells be shared, which in a
distributed-memory setting is handled via point-to-point communication between
processes in order to communicate data of a ghost layer.  DMBF supports the
communication of DMBF-managed cell data within a ghost layer.

The focus of DMBF in managing general data objects enables scientific
applications that use the DMBF interface to store any kind of cell-dependent
data, for instance, the data required for finite element, finite volume, or
finite difference discretizations.  The shape and size of the cell data are left
entirely up to the application using DMBF and are not imposed by DMBF's
interface.  This approach allows for greater flexibility, because more
applications can utilize DMBF.  At the same time, it requires the
application to implement its own discretization routines (or rely on other
libraries).
We have taken this approach of separating concerns.  We have implemented a new
relativistic electron drift-kinetic solver that is supported by the DMBF
interfaces (see Section~\ref{ref:rfp-solver}).

To perform computations with the DMBF-managed cell data, the DMBF
interface provides iterator functions that call a user-specified function for
each cell.  Additional functions are capable of iterating over the edges/faces
of cells.  The adaptation of a mesh is handled with similar user-provided
functions that act on the data of each cell individually.
%
The DMBF interface is completed by allowing the generation of PETSc vectors and
matrices. Therefore, calculations that utilize DMBF leverage the linear and
nonlinear solvers of PETSc as well as PETSc's time-stepping functionality.

%

\subsection{Relativistic electron drift-kinetic solver based on DMBF}
\label{ref:rfp-solver}

This section gives a brief overview of the relativistic drift-kinetic Fokker--Planck solver
that is
\editAll{built}{a new application code building}
on the DMBF data structure.

The spatial discretization chooses
a finite volume scheme, MUSCL (Monotonic Upstream-centered Scheme for
Conservation Law) [20,21], or a conservative finite difference scheme, QUICK (Quadratic Upstream Interpolation for Convective Kinematics)
[22],
%
%
%
for the advection
operators.  Both schemes are second-order accurate in space
on a uniform mesh for time-dependent advection problems.
The collision operator is discretized by a central scheme using a three-point stencil that
also gives second-order accuracy.
\editOne{}{Note that the second-order central scheme for the collision operator can be interpreted as finite difference or finite volume~\cite{LeVeque02}.
We support both finite difference and finite volume schemes through associating corresponding degrees of freedom to each cell.
}
An adaptive quadtree-based
quadrilateral mesh is used. Thus our solver supports the handling of hanging nodes and can be nonsymmetric in a single cell.
More details on the AMR and the implementation can be found
in Section~\ref{sec:dmbf}.

Numerical solutions
\editAll{are always assumed to live on cell centers.}
{throughout the current paper are supported on nodes at cell centers.}
Each cell of
the adapted mesh is further subdiveded into four uniform cells, which
constitute the degrees of freedom (DOFs) of a solution's discretization.
To distinguish between the two notions of cells, we use the term \emph{mesh
cells},
\editAll{for}{denoting}
the cells of the adapted p4est-based mesh, and we use the term
\editOne{\emph{FV cells}}{\emph{cells}}
for the four uniformly refined cells inside a mesh cell.
%
Additional to the
\editOne{FV cells}{cells}
inside each mesh cell, the numerical schemes need to
access DOFs from neighboring
\editOne{FV cells}{cells}.
To this end, the DMBF cell data includes
a layer---also referred to as guard layer---of two
\editOne{FV cells}{cells}
around a mesh cell.
This outer layer of DOFs \editAll{can require}{requires}
point-to-point communication%
\editAll{ to be filled with data from other processes.}
{, if neighboring cells reside on different parallel processes.}

\editOne{}{
The size of the guard layer is determined from the required DOFs of a numerical
scheme.  Here, we employ the second-order QUICK and MUSCL schemes, which need
DOFs from up to two neighboring cells.  Hence, each mesh cell has a guard layer
of two (discretization) cells in each of the horizontal $p$- and vertical
$\xi$-directions.
Since we choose the adaptive mesh to be block-structured by incorporating four
cells of uniform refinement into each mesh cell, the parallel communication of
ghost cells is feasible with standard point-to-point communication of adjacent
neighbors of mesh cells (as opposed to needing ghost layers over two mesh
cells).
Filling a guard layer, if the neighboring mesh cells have the same level of
refinement, is straightforward:  assuming a ghost layer communication has taken
place, the values of the DOFs from neighbors can be copied into the guard layer
of a mesh cell; this is depicted in Figure~\ref{fig:guard-layer-uniform}.  When
the level of refinement differs, then it will differ only by one level, because
we enforce 2:1-balancing supported by p4est.  This leaves two cases to be
discussed: when a guard layer needs to be filled with DOFs from one coarser mesh
cell or from two mesh cells of one level finer.  We will present the example of
the left edge of a mesh cell, while all other edges are treated analogously.
}

\editOne{}{
First, the interpolation from one coarse mesh cell to the guard layers of two
fine mesh cells involves the DOFs shown in Figure~\ref{fig:guard-layer-adaptive},
left, where the figure depicts the coarse--fine interface along the left edge of
two fine mesh cells.  The guard layers' (fine) DOFs have coordinates that are
horizontally aligned with the coarse DOFs; this allows to interpolate only along
the vertical $\xi$-dimension.
We employ linear projections, because they are more stable than higher-order
projections with respect to extrapolation, which affects the fine DOFs at the
top and bottom of the guard layer (see Figure~\ref{fig:guard-layer-adaptive},
left); and we enforce nonnegativity after projection.
Second, the interpolation from two fine mesh cells to the guard layer of one
coarse mesh cell is depicted in Figure~\ref{fig:guard-layer-adaptive}, right, for
the fine--coarse interface along the left edge of a mesh cell.  The guard
layer's (coarse) DOFs are horizontally aligned with the fine DOFs (i.e.,
similarly to the previously presented fine guard layers).  A linear
interpolation only along the $\xi$-direction is necessary; and no extrapolation
is performed, because coarse DOFs will always be surrounded by fine DOFs.
}

\begin{figure}\centering
%
    \input{inc/fig_guard_layer_uniform}
%
  \caption{\editOne{}{%
    A mesh cell \emph{(blue square)} is subdivided into four cells with one
    cell-centered DOF \emph{(back filled circle)}.  The guard layer of this mesh
    cell is comprised of four adjacent mesh cells \emph{(brown squares)}, if the
    neighboring cells have the same level of refinement.
    Filling the guard layer's DOFs \emph{(black unfilled circles)} of the
    \emph{(blue)} mesh cell is done by copying the DOFs from adjacent cells
    \emph{(brown filled circles)} in the case of uniform level of refinement.
    }}
  \label{fig:guard-layer-uniform}
\end{figure}

\begin{figure}\centering
%
    \input{inc/fig_guard_layer_adaptive}
%
  \caption{\editOne{}{%
    Filling a guard layer's DOFs \emph{(black unfilled circles)} in the case of
    adaptive refinement requires interpolating the DOFs of the adjacent mesh
    cell(s) \emph{(brown filled circles)} to the locations at the guard layer's
    DOFs, while the latter are positioned to resemble a stretched, uniformly
    refined grid for the DOF of the mesh cell(s) \emph{(black filled circles)}.
    }}
  \label{fig:guard-layer-adaptive}
\end{figure}

The spatial discretization is coupled with explicit or implicit time stepping.
To ease the mesh adaptivity, we choose single-step integrators.
For the explicit case a SSP Runge--Kutta is chosen, while for the implicit case
a fully implicit DIRK or ESDIRK time integrator is chosen.
Here the explicit integrators are chosen only for testing purposes.
All the production runs use implicit integrators because we are interested in
physics of the collisional time scale.

%
%
%
%
%
%
%
%
%

%
Depending on the choice of the advection scheme, the problem can be nonlinear
(in the case of MUSCL) or linear (in the case of QUICK).  For nonlinear problems, the
JFNK algorithm is the primary solver, and it is further preconditioned with an
approximate Jacobian.  The Jacobian is computed through finite difference
coloring provided by PETSc.  A GMRES iterative solver is used to invert the
linearized system (and also the linear system arising from QUICK).
A hypre \cite{FalgoutYang02} algebraic multigirid (BoomerAMG
\cite{yang2002boomeramg}) solver is used as the preconditioner.  Those choices
of the algorithm are state of the art for large-scale nonlinear simulations.

Two commonly used operators on adaptive meshes are interpolations and
integrations, both of which need to support handling of hanging nodes/edges.  For
interpolation from coarse to fine and reverse, we use tensor product
interpolation matrices because of their lower memory usage per generated Flops.
When needed, a postprocessing step is performed after interpolation to
preserve the positivity and mass conservation of the algorithm.
Computations of fluxes across hanging edges require tailored algorithms for
filling the guard cells; these are derived from the interpolation algorithms
and inherit their properties (tensor product formulation, positivity, and mass
conservation).
%
A line integral along $\xi$ direction is performed to compute runaway
electrons.  A simple quadrature rule is used in each cell, and the positivity
can be enforced at the price of reducing the spatial accuracy to
first-order.

%
%
%
%
%
%

