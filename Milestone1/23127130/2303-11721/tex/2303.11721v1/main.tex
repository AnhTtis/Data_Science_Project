\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{enumitem}
\usepackage{tablefootnote}
\usepackage{ragged2e}
\usepackage{chngcntr}
\counterwithin{figure}{section}
\interfootnotelinepenalty=10000
%\usepackage{mathptmx} % times new roman
\usepackage[section]{placeins}
\usepackage{lscape}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[hidelinks]{hyperref}

%\setlength\footnotemargin{1em} 

\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{float}
\usepackage{caption}

\hypersetup{
linkcolor=blue,
citecolor=blue,
urlcolor=black,
colorlinks=true
}
\usepackage{amsmath,amsfonts,amssymb,mathrsfs}
\usepackage{amsthm}
\usepackage{cases}

\usepackage{setspace}
\usepackage{geometry}
 \geometry{
 letterpaper,
 left=1in,
 top=0.95in,
 right=1in,
 bottom=1in
 }
\usepackage{hyperref}

\title{\LARGE{\textbf{Using Forests in Multivariate Regression Discontinuity Designs\footnote{We thank Francesca Molinari for her guidance and support throughout this project. We also thank Yanqin Fan, José Luis Montiel Olea, Chen Qiu, Hongyuan Xia, and the faculty instructors, Levon Barseghyan and Douglas Miller, as well as student participants at the Third Year Research Seminar at Cornell for valuable discussion and helpful comments. }\\}}}
\author{Yiqi Liu\\
\href{mailto:yl3467@cornell.edu}{\texttt{yl3467@cornell.edu}}
\and Yuan Qi \\
\href{mailto:ayqi@uw.edu}{\texttt{ayqi@uw.edu}}}
\date{{January 2023}}

\begin{document}
\maketitle

\begin{abstract}
    We discuss estimating conditional treatment effects in regression discontinuity designs with multiple scores. While local linear regressions have been popular in settings where the treatment status is completely described by one running variable, they do not easily generalize to empirical applications involving multiple treatment assignment rules. In practice, the multivariate problem is usually reduced to a univariate one where using local linear regressions is suitable. Instead, we propose a forest-based estimator that can flexibly model multivariate scores, where we build two honest forests in the sense of \hyperref[ref:13]{Wager and Athey (2018)} on both sides of the treatment boundary. This estimator is asymptotically normal and sidesteps the pitfalls of running local linear regressions in higher dimensions. In simulations, we find our proposed estimator outperforms local linear regressions in multivariate designs and is competitive against the minimax-optimal estimator of \hyperref[IW2019]{Imbens and Wager (2019)}. The implementation of this estimator is simple, can readily accommodate any (fixed) number of running variables, and does not require estimating any nuisance parameters of the data generating process.
\end{abstract}

\section{Introduction}
First introduced by \hyperref[ref:1]{Thistlethwaite and Campbell (1960)} to study the effect of scholarship programs on subsequent academic outcomes, regression discontinuity (RD) designs are widely used to estimate causal effects in non-experimental settings. It explores a particular treatment assignment mechanism in which individuals receive the treatment if an observed assignment or running variable scores above some known threshold. Under the potential outcomes framework, \hyperref[ref:2]{Hahn, Todd and van der Klaauw (2001)} show that the treatment is as good as randomly assigned in the population arbitrarily close to the assignment threshold if the potential outcomes with and without the treatment are continuous at the cutoff.

Estimation of treatment effects in this context is, therefore, a local problem in the neighborhood of the assignment cutoff. The standard practice in empirical RD research is to run local linear regressions on each side of the cutoff, where the neighborhood is pinned down by a deterministic kernel with a fixed bandwidth selected based on some asymptotic optimality criteria that may have poor performance in finite samples. Kernel-based methods are also known to be sensitive to the curse of dimensionality and become less useful when the dimension exceeds two or three (\hyperref[ref:21]{Hastie, Tibshirani and Friedman, 2016}). Moreover, in higher dimensions standard bias correction techniques for local linear regressions are both algebraically involved to compute and computationally difficult to implement. To our knowledge, there has been little research trying to establish valid inference tools for multivariate local regressions.

For these reasons, in empirical research with multivariate scores, the common practice is to reduce the problem to a univariate one. For example, unemployment insurance programs usually set multiple eligibility criteria, such as age \textit{and} prior labor force history in determining the maximum duration of benefits, in which case \hyperref[SWB2012]{Schmieder, von Wachter and Bender (2012)} subset the data to consider only individuals with eligible employment histories within their respective age groups, and use age as the effective running variable. In a similar setting, to evaluate education policies where the treatment eligibility is determined by standardized test scores of \textit{multiple} subjects, \hyperref[JL2004]{Jacob and Lefgren (2004)} and \hyperref[ref:22]{Matsudaira (2008)} divide the data to subsets where using only a single test score as the assignment variable is appropriate. An alternative approach is to construct a univariate “measure” of the multivariate score, especially in cases where the treatment boundary takes arbitrary shapes and finding a suitable subset of the data where a univariate score applies is infeasible. For example, to exploit geographic discontinuities, \hyperref[B1999]{Black (1999)} considers the shortest distance to the boundary of a geographic region and \hyperref[ref:20]{Keele and Titiunik (2015)} transform bivariate coordinates to scalars by taking their rescaled Euclidean distance to a given boundary point, and then run local linear regressions. Other examples include \hyperref[BBRW2009]{Battistin et al. (2009)} and \hyperref[CM2014]{Clark and Martorell (2014)} that use the minimum of multiple scores as the effective running variable in designs where the treatment status is determined by \textit{any} of the scores crossing the threshold.

These approaches, however, have several drawbacks. Considering only subsets of the data leaves unexplored a rich set of treatment effects that could be identified under a valid multivariate RD design, where the conditional average treatment effect as a multivariate function of any point along the treatment boundary is identified; see Section \ref{sec2}. Heterogeneous treatment effects of this kind can be important to answer policy questions.  In addition, subsetting does not fully exploit signals from multiple dimensions and could incur a loss of precision (\hyperref[IW2019]{Imbens and Wager, 2019}). The latter case where observations with the same distance to the cutoff of interest are collapsed to the same point on the real line is subject to similar issues. Transforming a multivariate score could also incur unexpected behaviors of the resulting density of the effective univariate score, such as discontinuity and zero density at the cutoff, violating key assumptions for local linear regressions to work. These issues have not been discussed in empirical RD research and are difficult to detect in practice without knowing the true data generating process (DGP); see Appendix \hyperref[appendix:A.6]{A.6}.\footnote{Appendix \hyperref[appendix:A.6]{A.6} gives a characterization of bivariate densities that have a zero univariate density at the cutoff once the scores are collapsed to a univariate variable by taking the Euclidean distance to the cutoff. This characterization depends on the unobserved population joint density of the scores. We demonstrate how such problem can affect local linear regressions in a simple simulation in Appendix \hyperref[DGPA1]{A.1}.}

To sidestep these issues, we extend nonparametric estimators from the machine learning literature that have shown empirical success in the presence of many covariates to sharp RD designs. We consider ensemble-tree methods that systematically construct a set of neighborhoods near the treatment assignment threshold and return a weighted average of “neighbors,” where the weights are obtained from counting the number of times each observation falls into the same leaf as the target point across the ensemble. In this sense, tree ensembles, including random forests (\hyperref[ref:23]{Breiman, 2001}), gradient boosted trees (\hyperref[ref:5]{Friedman, 2001}), and their variants (e.g., \hyperref[ref:12]{Athey, Tibshirani and Wager, 2019}; \hyperref[ref:7240-2]{Ghosal and Hooker, 2021}; \hyperref[ref:13]{Wager and Athey, 2018}) are analogous to kernel functions. As a motivating example, we demonstrate the potential of ensemble trees as adaptive multivariate kernels by running a simple simulation in Appendix \hyperref[DGPA1]{A.1}.

In this paper, we consider the variant of forests proposed by \hyperref[ref:13]{Wager and Athey (2018)} and \hyperref[ref:12]{Athey, Tibshirani and Wager (2019)} that, instead of using the bootstrap aggregation originally proposed by \hyperref[ref:23]{Breiman (2001)}, averages trees grown on random subsamples of the data. As shown by \hyperref[ref:13]{Wager and Athey (2018)}, forests based on subsampling admit a $U$-statistics representation, allowing classical analysis following \hyperref[H1968]{Hájek (1986)} and \hyperref[H1948]{Hoeffding (1948)} to establish asymptotic normality. Under the potential outcomes framework (\hyperref[N1923]{Neyman, 1923}; \hyperref[R1974]{Rubin, 1974}), we extend the causal forests in \hyperref[ref:13]{Wager and Athey (2018)} that rely on the unconfoundedness assumption to RD designs that depend on a different identification assumption. To estimate the conditional treatment effect at the cutoff, we mimic the standard estimation procedure by building two regression forests using treated and control observations, respectively, and obtain the estimated conditional treatment effect by subtracting the the control forest estimate from the treated forest estimate, both evaluated at the cutoff. This estimator is easy to implement, allows valid inference, and can accommodate any fixed number of scores.

We note, however, that theoretical analysis comparing the convergence rates of local linear regressions and forests is complicated by the fact that existing rates for forests are based on upper bounds, whereas the rate for local linear regressions is exact. But the heuristic behind the competitive performance of forests is two-fold. First, unlike local linear regressions, forests do not have to trade off bias for variance or vice versa conditional on the samples used to fit each tree, as forests average a large number of noisy but already low-bias trees. Second, as noted in \hyperref[ref:13]{Wager and Athey (2018)}, when there are multiple scores, regression trees split more frequently along dimensions with more signals for predicting the conditional expectation function (CEF), leading to potential gain in power. In simulations, we find forests outperform local linear regressions with a univariate score in terms of mean squared error (MSE) when the underlying running variable is in fact multivariate, both when forests effectively use all the dimensions and when we collapse the dimensions to one. In addition, forests are competitive against an alternative estimator proposed by \hyperref[IW2019]{Imbens and Wager (2019)} that also works directly with multiple scores but requires estimating additional nuisance parameters of the population DGP. 

\subsection{Related Work}\label{sec:relatedwork}
\hyperref[ref:7]{Imbens and Lemieux (2008)}, \hyperref[ref:3]{Lee and Lemieux (2010)}, and \hyperref[ref:8]{Cattaneo and Escanciano (2017)} provide comprehensive reviews on RD designs. Following \hyperref[ref:2]{Hahn, Todd and van der Klaauw (2001)} and \hyperref[ref:9]{Porter (2003)}, local polynomial regressions remain the most popular estimation strategy in the empirical RD literature. At the heart of these local methods is bandwidth selection for the weighting kernel. In designs with a univariate running variable, \hyperref[ref:4]{Imbens and Kalyanaraman (2012)} propose a data-driven algorithm that yields an MSE-optimal bandwidth by minimizing the asymptotic MSE evaluated at the score cutoff. However, this leads to a non-negligible bias term that does not vanish in the limit. \hyperref[ref:7240-1]{Calonico, Cattaneo and Titiunik (2014)} propose a bias-corrected estimator that first subtracts from the point estimate an estimated bias term and then adjusts the standard error accordingly so that valid inference can be conducted.

In many situations, the treatment status is determined by more than just one criterion.
Univariate local linear regressions, however, do not readily generalize to the case with multiple scores, as the choice of an optimal bandwidth now becomes a multivariate problem. Existing nonparametric estimators that work directly with multiple scores include methods proposed by \hyperref[PWM2011]{Papay, Willett and Murnane (2011)} and \hyperref[CL2018]{Choi and Lee (2018)} that employ local linear regressions with multiple bandwidths selected by cross-validation and \hyperref[Z2012]{Zajonc (2012)} that extends the MSE-optimal bandwidth of \hyperref[ref:4]{Imbens and Kalyanaraman (2012)} to the bivariate case. As pointed out by \hyperref[IW2019]{Imbens and Wager (2019)}, these methods yield oversmoothing bandwidths and biased estimates, an issue also highlighted by \hyperref[ref:7240-1]{Calonico, Cattaneo and Titiunik (2014)}.\footnote{\hyperref[CL2018]{Choi and Lee (2018)} also consider rule-of-thumb bandwidths that scale with $n^{-1/6}$, which are again oversmoothing in the sense that $nh^5\not\to0$.} To avoid the problem of selecting a multivariate bandwidth, \hyperref[IW2019]{Imbens and Wager (2019)} propose an estimator based on numerical optimization that, given a second derivative bound $\mathcal{B}$ on the CEF of potential outcomes, is minimax-optimal among all estimators that are linear in the outcome variable over a class of CEF with second derivatives bounded by $\mathcal{B}$. This method, however, requires estimating the second derivative bound $\mathcal{B}$, and, as we demonstrate in Section \ref{sec3}, its performance depends crucially on accurate estimates of the second derivative bound. Similar approaches that allow uniform inference over a prespecified class of functions have also been considered by, for example, \hyperref[AK2018]{Armstrong and Kolesár (2018)} in the univariate design, where the class of CEF they consider has bounded Taylor approximation errors. In contrast, our proposed method allows pointwise inference and does not require estimating any nuisance parameters of the DGP.

There has been growing popularity in using classification and regression trees (CART) as an alternative to kernel functions, especially in higher dimensions. Under the potential outcomes framework with unconfoundedness (\hyperref[N1923]{Neyman, 1923}; \hyperref[R1974]{Rubin, 1974}), \hyperref[ref:11]{Athey and Imbens (2016)} use CART to partition the sample into sub-populations heterogeneous in treatment effects and construct valid confidence intervals. Building on this work, \hyperref[ref:13]{Wager and Athey (2018)} develop a point-wise consistent, asymptotically normal estimator for heterogeneous treatment effects using random forests (\hyperref[ref:23]{Breiman, 2001}) that outperforms conventional methods such as nearest-neighbor matching. \hyperref[ref:12]{Athey, Tibshirani and Wager (2019)} highlight the adaptive nearest-neighbor matching perspective of random forests and estimate parameters identified by local moment conditions using forest weights. \hyperref[ref:14]{Gu (2018)} extends the aforementioned works to estimating heterogeneous treatment effects in RD designs by running local linear regressions with the MSE-optimal bandwidth of \hyperref[ref:4]{Imbens and Kalyanaraman (2012)} on samples weighted by forest weights.

Our paper is most closely related to \hyperref[ref:13]{Wager and Athey (2018)} and \hyperref[ref:12]{Athey, Tibshirani and Wager (2019)}. Instead of building a single forest, we mimic the standard estimation procedure and build two separate forests for the observations that fall into the treatment and control regions, respectively. The estimate for the conditional treatment effect at any cutoff of interest is the difference between the two fitted forests evaluated at that cutoff. As in the local linear regression case, conditional on an independently and identically drawn sample, the covariance between the two fitted forests is zero, and hence the variance of our proposed estimator is simply the sum of the variances of the two forests, which can be consistently estimated using existing methods, including infinitesimal jackknife (\hyperref[E2014]{Efron, 2014}; \hyperref[WHE2014]{Wager, Hastie and Efron, 2014}; \hyperref[ref:13]{Wager and Athey, 2018}) and bag of little bootstraps (\hyperref[SL2009]{Sexton and Laake, 2009}; \hyperref[ref:12]{Athey, Tibshirani and Wager, 2019}). To our knowledge, this is the first paper that uses tree-based methods to directly estimate treatment effects in multivariate RD designs. We demonstrate the advantage of our proposed estimator over local linear regressions and show its competitiveness against the minimax-optimal estimator of \hyperref[IW2019]{Imbens and Wager (2019)} in simulation studies based on data from \hyperref[L2008]{Lee (2008)} and \hyperref[ref:20]{Keele and Titiunik (2015)}.

In Section \hyperref[sec2]{2}, we begin with identification of treatment effects in multivariate RD designs and a discussion on the current estimation practice, and then detail our proposed estimator. We present simulation results in Section \hyperref[sec3]{3}. Section \hyperref[sec4]{4} concludes. All replication files can be accessed at \href{https://github.com/yqi3/GRF-RD}{\texttt{github.com/yqi3/Replication-GRF-RD}}.

\section{Building Forests for RD Designs}
\label{sec2}
\subsection{Identification in Sharp RD Designs with Multiple Scores}
Suppose we have a sample of independent and identically distributed (i.i.d.) observations $\mathcal{S} =\{(Y_i, X_i)\}_{i=1}^n$, where $Y_i \in \mathbb{R}$ is the outcome of interest and $X_i \in \mathcal{X} \subseteq \mathbb{R}^d$ is the $d$-dimensional running variable that determines the treatment status of the $i$-th unit according to some exogenous treatment assignment rule, $A:\mathcal{X} \to \{0,1\}$. If $d=1$, then unit $i$ is treated if and only if $A(X_i)=\mathbf{1}\{X_i\geq x^c\}=1$, where $x^c$ is the running variable cutoff as in the standard sharp RD design with a univariate score. In higher dimensions, the treatment assignment rule can be more complex. For example,  \hyperref[ref:20]{Keele and Titiunik (2015)} study the effect of television advertising on election turnout, where the treatment status is determined by a geographic media market boundary. Following \hyperref[Z2012]{Zajonc (2012)}, let $\mathcal{T}:=\{x \in \mathcal{X}: A(x)=1\}$ denote the set of scores assigned to the treatment and $\mathscr{B}:=bd(\mathcal{T})$ denote the treatment boundary, i.e., the set of scores belonging to the closure but not the interior of $\mathcal{T}$.

Let $\left(Y_i(1), Y_i(0)\right)$ be the pair of potential outcomes with and without the treatment. We are interested in the following conditional treatment effect for some $x^c\in \mathscr{B}$,
\begin{align*}
    \tau(x^c):=\mathbb{E}[Y_i(1)-Y_i(0)|X_i=x^c].
\end{align*}
For each unit $i$, we only ever observe one of the potential outcomes. However, if we assume (i) the CEF $\mathbb{E}\left[Y_i(1)|X_i=x^c\right]$ and $\mathbb{E}\left[Y_i(0)|X_i=x^c\right]$ are continuous at all $x^c \in \mathscr{B}$, and (ii) $X$ has a density, $f_X(\cdot)$, that is bounded away from 0 in some neighborhood around $x^c$ for all $x^c \in \mathscr{B}$. Then we can identify $\tau(x^c)$ using neighboring observations around $x^c$.
\begin{align*}
    \text{For all $x^c \in \mathscr{B}$,}\quad\tau(x^c)=\lim_{x \to x^c}\mathbb{E}\left[Y_i|X_i=x, X_i\in\mathcal{T}\right] - \lim_{x \to x^c}\mathbb{E}\left[Y_i|X_i=x, X_i\notin \mathcal{T}\right].
\end{align*}
The proof follows from a generalization of the argument in \hyperref[ref:2]{Hahn, Todd, and van der Klaauw (2001)} and can be found in \hyperref[Z2012]{Zajonc (2012, p.56)}. Thus under a multivariate RD design, we can identify the conditional treatment effect as a function of scores along the treatment boundary. Such heterogeneity may be of interest to the researcher. Alternatively, \hyperref[Z2012]{Zajonc (2012)} and \hyperref[ref:20]{Keele and Titiunik (2015)} discuss a summary measure of the average treatment effect along the boundary, i.e., $\mathbb{E}[Y_i(1)-Y_i(0)|X_i \in \mathscr{B}]=\int_{x\in\mathscr{B}}\tau(x)f(x|\mathscr{B})dx$, which can be estimated given a sample of boundary points by averaging estimated conditional treatment effects at those sample boundary points, weighted by the empirical distribution of boundary points.

\subsubsection{Estimation via Local Linear Regressions}
Given a cutoff $x^c \in \mathscr{B}$, estimation of $\tau(x^c)$ boils down to estimating two CEFs,
\begin{align*}
    \mu^+(x^c)&:=\mathbb{E}\left[Y_i(1)|X_i=x^c\right]=\lim_{x \to x^c}\mathbb{E}\left[Y_i|X_i=x, X_i\in\mathcal{T}\right],\\
    \mu^-(x^c)&:=\mathbb{E}\left[Y_i(0)|X_i=x^c\right]=\lim_{x \to x^c}\mathbb{E}\left[Y_i|X_i=x, X_i\notin \mathcal{T}\right].
\end{align*}
In the univariate case, $\mathcal{T}=\{x \in \mathbb{R}: x \geq x^c\}$ and $\mathscr{B}=\{x^c\}$, a singleton. Assume without loss of generality that $x^c$ is normalized to $0$. The standard practice is to run local linear regressions on each side of the cutoff using some kernel $K_h(\cdot)$ with bandwidth $h$,
\begin{align}
     \hat{\tau}_{ll}(x^c;h)&:=\hat{\mu}_{ll}^+(x^c;h)-\hat{\mu}_{ll}^-(x^c;h),\label{eqn:1}\\
     \hat{\mu}_{ll}^+(x^c;h) &= e'\left(\arg\min_{(\beta_0,\beta_1)'}\sum_{i=1}^n\mathbf{1}\{X_i\geq x^c\}K_h(X_i-x^c)\left(Y_i-\beta_0-\beta_1(X_i-x^c)\right)^2\right),\notag\\
     &= e'\left(X_+'W(X_+;h)X_+\right)^{-1}X_+'W(X_+;h)Y_+,\label{eqn:2}\\
     \hat{\mu}_{ll}^-(x^c;h) &= e'\left(\arg\min_{(\beta_0,\beta_1)'}\sum_{i=1}^n\mathbf{1}\{X_i< x^c\}K_h(X_i-x^c)\left(Y_i-\beta_0-\beta_1(X_i-x^c)\right)^2\right),\notag\\
     &= e'\left(X_-'W(X_-;h)X_-\right)^{-1}X_-'W(X_-;h)Y_- ,\label{eqn:3}
\end{align}
where the subscript “$ll$” denotes local linear, $e=[\,1\quad 0\,]^\intercal$ is the standard basis in $\mathbb{R}^2$ that picks out the first element of the minimizer (i.e., the fitted intercept term $\beta_0$); $X_+$ is the $n_1\times 2$ design matrix, where $n_1$ is the number of treated observations. The first column of $X_+$ contains all ones and the second column stacks all $\{X_i\geq x^c\}$. $W(X_+;h)$ is the $n_1\times n_1$ diagonal weighting matrix with the $i$-th diagonal element being the kernel weight $K_h(X_i-x^c)$ for $X_i\geq x^c$. $Y_+$ is the $n_1\times 1$ outcome vector that stacks the outcomes for all observations that score above the cutoff. Notations in line (\ref{eqn:3}) are defined analogously.

Following \hyperref[ref:4]{Imbens and Kalyanaraman (2012)}, the choice of the bandwidth $h$ is usually obtained by minimizing the asymptotic expansion of 
\begin{align}
    \mathbb{E}\left[\left(\hat{\tau}_{ll}(x^c;h)-\tau(x^c)\right)^2\right]\label{mse-opt}
\end{align}
with respect to $h$. However, \hyperref[ref:7240-1]{Calonico, Cattaneo, and Titiunik (2014)} note that the MSE-optimal bandwidth from optimizing (\ref{mse-opt}), $h^*$, is oversmoothing in the sense that $n(h^{{*}})^{5}$ does not converge to 0, leading to a bias term that must be corrected in order to conduct valid inference. They propose subtracting from $\hat{\tau}_{ll}(x^c;h^*)$ an estimated bias term and then adjust the standard error to account for the variation in the bias estimate. This procedure, however, does not generalize easily to multivariate RD designs. Although \hyperref[Z2012]{Zajonc (2012)} proposes a similar MSE-optimal bandwidth selector in the bivariate case, valid inference cannot be performed without bias correction due to oversmoothing. However, in higher dimensions standard bias correction techniques for local linear regressions are both algebraically involved to compute and computationally difficult to implement, and to our knowledge there has been little research trying to establish valid inference tools for multivariate local regressions. In empirical research involving multiple scores, the problem is usually simplified so that univariate methods can apply. An example is \hyperref[ref:20]{Keele and Titiunik (2015)}, where a two-dimensional score is first collapsed to a scalar using rescaled Euclidean distance and then estimation follows by using local linear regression with robust bias correction as detailed in \hyperref[ref:7240-1]{Calonico, Cattaneo, and Titiunik (2014)}.

\subsubsection{Estimation via Numerical Optimization}
To avoid the problem of choosing a multivariate bandwidth, \hyperref[IW2019]{Imbens and Wager (2019)} propose an estimator based on numerical optimization that is minimax-optimal among all estimators that are linear in the outcome variable over all problems with $\sigma_i^2:=Var[Y_i|X_i]$ and a second derivative bound $\mathcal{B}$ on the CEF of potential outcomes. Specifically, they seek to find an estimator for $\tau(x^c)$ that takes the form
\begin{align}
    \hat{\tau}_{mm}(x^c;\mathcal{B}):=\sum_{i=1}^n\hat{\gamma}_i(x^c;\mathcal{B})Y_i, \label{minimax}
\end{align}
where
\begin{align}
    \hat{\gamma}(x^c;\mathcal{B})&=\arg\min_{\gamma}\left\{\sum_{i=1}^n\gamma_i^2\sigma_i^2+b^2(\gamma;\mathcal{B})\right\},\\
    b(\gamma;\mathcal{B})&:=\sup_{\mu^+,\mu^-}\left\{\sum_{i=1}^n\gamma_i\mu_{A_i}(X_i)-(\mu^+(x^c)-\mu^-(x^c)): \forall\,x,\lVert\nabla^2\mu^+(x)\rVert \leq \mathcal{B}, \lVert\nabla^2\mu^-(x)\rVert\leq \mathcal{B}\right\},\notag
\end{align}
for $\mu_{A_i}(X_i)=\mu^+(X_i)$ if $A_i:=A(X_i)=1$ and $\mu_{A_i}(X_i)=\mu^-(X_i)$ otherwise. The norm $\lVert\cdot\rVert$ denotes the Euclidean norm when $d=1$ and the operator norm when $d>1$. In other words, the estimator (\ref{minimax}) is a weighted average of the outcomes, where the weights $\hat{\gamma}(x^c;\mathcal{B})$ are constructed by minimizing the resulting variance plus the worst-case bias over a class of CEF with second derivatives bounded by $\mathcal{B}$. Inference follows from obtaining confidence intervals $\mathcal{I}_{\alpha}$ that achieve uniform coverage over this class of CEF:
\begin{align}
\mathcal{I}_{\alpha}:\liminf_{n\to\infty}\inf\left\{\mathbb{P}[\tau(x^c)\in\mathcal{I}_{\alpha}]:\forall\,x,\lVert\nabla^2\mu^+(x)\rVert \leq \mathcal{B}\text{ and } \lVert\nabla^2\mu^-(x)\rVert\leq \mathcal{B}\right\}\geq 1-\alpha.
\end{align}

Implementing the minimax-optimal estimator (\ref{minimax}) requires estimates for $\sigma_i^2$ and $\mathcal{B}$. \hyperref[IW2019]{Imbens and Wager (2019)} recommend estimating $\sigma_i^2$ by averaging the residuals from an ordinary least-squares regression of $Y_i$ on the interaction of $X_i$ and $A(X_i)$. Estimating $\mathcal{B}$, however, is problem specific, and \hyperref[IW2019]{Imbens and Wager (2019)} recommend estimating the CEF globally using, for example, second-order polynomials, and multiply the maximal estimated curvature by 2 or 4. In our first simulation using the \hyperref[L2008]{Lee (2008)} data, we find that using second-order polynomials gives too small estimates for $\mathcal{B}$, and multiplying by $4$ does not help; only after multiplying the maximal estimated curvature by $30$ do we get an estimate that is close to the true (least upper) bound $\mathcal{B}$.

\subsection{Estimating Conditional Treatment Effects Using Forests}
Our goal is to propose an estimator that works directly with multiple dimensions, allows valid inference, and does not require estimating any nuisance parameters of the DGP. We focus on a variant of random forests originally proposed by \hyperref[ref:23]{Breiman (2001)}. A random forest evaluated at some feature $x \in \mathcal{X}$, $RF(x)$, is an average of $B$ trees, $\{T_b(x)\}_{b=1}^B$, each fitted using a bootstrapped sample or random subsample of $\{(Y_i, X_i)\}_{i=1}^n$. Following \hyperref[ref:13]{Wager and Athey (2018)}, we focus on forests built using random subsamples as they admit a $U$-statistic representation that allows classical statistical analysis following \hyperref[H1968]{Hájek (1986)} and \hyperref[H1948]{Hoeffding (1948)}. Given a subsample of size $s$, $(Y_{i_k}, X_{i_k})_{k=1}^s$, a regression tree recursively divides the sample into mutually exclusive and collectively exhaustive subsets called leaves. Given a point $x \in \mathcal{X}$, denote $L(x)$ the leaf that contains $x$. A tree $T$ estimates the CEF, $\mathbb{E}[Y_i|X_i=x]$, by averaging the observations that fall into the same leaf as $x$:
\begin{align*}
    T(x) = \frac{1}{|L(x)|}\sum_{i_k: X_{i_k} \in L(x)} Y_{i_k}.
\end{align*}
Similar to selecting an MSE-optimal bandwidth, we seek to find partitions of the sample space that are MSE-optimal. Unlike local linear regressions, trees do not have an analytical form for the asymptotic expansion of the MSE,\footnote{This is also commonly referred to as the mean squared prediction error in the machine learning literature.}
\begin{align}
    \mathbb{E}\left[(Y_i-T(X_i))^2\right],\label{MSPE}
\end{align}
and searching for all possible partitions is unfortunately impossible. Instead, standard splitting rules rely on greedy algorithms that immediately improve the in-sample fit after each split. 

Estimates from a single tree can be highly unstable without regularization. In the extreme case, one can achieve perfect in-sample fit by partitioning the sample data so that each leaf contains only one observation. Random forests stabilize predictions from individual trees by averaging many of them fitted using the same process: For a total number of $b=1,...,B$ trees, we fit each tree $T_{b}(x)$ using a random subsample of size $s$, and obtain our forest estimate by averaging across all $B$ trees, $\frac{1}{B}\sum_{b=1}^B T_{b}(x)$. To achieve a sizable reduction in variance, we may want to decorrelate the trees as much as possible by, for example, randomly selecting a dimension to split at each node. We assume throughout that $B$ is large enough so that the following definition of forests from \hyperref[ref:13]{Wager and Athey (2018)} given training sample $\mathcal{S}=\{(Y_i,X_i)\}_{i=1}^n$ and subsample size $s$ applies:
\begin{align}
    RF\left(x;s, \{(Y_i,X_i)\}_{i=1}^n
    \right) := {n \choose s}^{-1}\sum_{1\leq i_1 < ... < i_s \leq n} \mathbb{E}_{\xi}\left[T(x;\xi, \{(Y_{i_k},X_{i_k})\}_{k=1}^s)\right],\label{eqn:4}
\end{align}
which averages across trees fitted using all possible draws of a size-$s$ subsample, marginalized over an auxiliary random term $\xi$ that serves as a decorrelation tool encoding, for example, the random split information.  

We now define our forest-based estimator for the conditional treatment effect at some cutoff point $x^c \in \mathscr{B}$ along the treatment boundary, $\tau(x^c)$:
\begin{align}
    \hat{\tau}_{rf}(x^c)&:=\hat{\mu}_{rf}^+(x^c)-\hat{\mu}_{rf}^-(x^c),\label{eqn:5}\\
    \hat{\mu}_{rf}^+(x^c) &= RF(x^c; s_1, \{(Y_i,X_i) \in \mathcal{S}: X_i \in \mathcal{T}\}),\label{eqn:6}\\
    \hat{\mu}_{rf}^-(x^c) &= RF(x^c; s_0, \{(Y_i,X_i) \in \mathcal{S}: X_i \notin \mathcal{T}\}).\label{eqn:7}
\end{align}
In words, $\hat{\tau}_{rf}(x^c)$ takes the difference between a forest fitted using observations that fall into the treated region and a forest fitted using observations that fall into the control region, both evaluated at the cutoff $x^c$.

\vspace{.3cm}

\noindent\textbf{Remark 1.} Although it seems that the local linear estimator (\ref{eqn:1}) depends on one more parameter, $h$, than the forest estimator (\ref{eqn:5}), we note that this is not the case. Each tree $T$ in the forest implicitly defines a neighborhood $L(x^c)$ for the cutoff point $x^c$. These neighborhoods are then aggregated, producing a weight for each observation that is proportional to the fraction of time it falls into the same leaf as $x^c$ across all trees in the forest. These forest weights are analogous to the weights generated by a kernel with bandwidth $h$. Thus (\ref{eqn:5}) implicitly depends on some “pseudo-bandwidth” generated by tree partitions across the forest. Analogously, the $\gamma$ weights of the minimax-optimal estimator (\ref{minimax}) play the role of bandwidth.

The connection is more obvious if we rewrite (\ref{eqn:6})
and (\ref{eqn:7}) as weighted least squares solutions,
\begin{align}
    \hat{\mu}_{rf}^+(x^c) &= \arg\min_{\theta}\sum_{i=1}^nA(X_i)W_{rf}(X_i,x^c)\left(Y_i-\theta\right)^2,\notag\\
    &= \left(X_+'W_{rf}(X_+,x^c)X_+\right)^{-1}X_+'W_{rf}(X_+,x^c)Y_+,\label{eqn:8}\\
    \hat{\mu}_{rf}^-(x^c) &= \arg\min_{\theta}\sum_{i=1}^n(1-A(X_i))W_{rf}(X_i,x^c)\left(Y_i-\theta\right)^2,\notag\\
    &= \left(X_-'W_{rf}(X_-,x^c)X_-\right)^{-1}X_-
    'W_{rf}(X_-,x^c)Y_-,\label{eqn:9}
\end{align}
where the elements in (\ref{eqn:8}) and (\ref{eqn:9}) are analogous to those in (\ref{eqn:2}) and (\ref{eqn:3}), except $X_+$ and $X_-$ are now $n_1\times 1$ and $n_0\times 1$ vectors of ones and we replace the kernel weights with forest weights, 
\begin{align*}
    W_{rf}(X_i,x^c):=\frac{1}{B}\sum_{b=1}^B\frac{\mathbf{1}\{X_i\in L_b(x^c)\}}{|L_b(x^c)|},
\end{align*}
where $L_b(x^c)$ denotes the leaf containing $x^c$ in the $b$-th tree. More generally, the least-square representation of forests in (\ref{eqn:8}) and (\ref{eqn:9}) effectively views $\hat{\mu}_{rf}^+(x^c)$ and $\hat{\mu}_{rf}^-(x^c)$ as solutions to the local moment conditions, 
\begin{align*}
    \mathbb{E}\left[(Y_i-\theta)^2|X_i=x^c, X_i \in \mathcal{T}\right]=0\quad \text{and} \quad
    \mathbb{E}\left[(Y_i-\theta)^2|X_i=x^c, X_i \notin \mathcal{T}\right]=0,
\end{align*}
and solves their empirical analogues using weights that reflect the importance of each observation $i$ in terms of minimizing the objective function. \hyperref[ref:12]{Athey, Tibshirani and Wager (2019)} use forests as weighting kernels to solve parameters identified by local moments in a more general setting.

\vspace{.3cm}

\noindent\textbf{Remark 2.} It may appear appealing to run local linear regressions, instead of constant regressions as in (\ref{eqn:8}) and (\ref{eqn:9}), with forest weights to better capture smooth signals. This is exactly the idea of \hyperref[FTAW2021]{Friedberg et al. (2021)}, who propose local linear forests and show improved convergence rates over regression forests under smooth signals. However, their asymptotic results apply to points in the interior of the feature space only, whereas in RD designs we necessarily focus on points along the boundary. Although boundary problems have also been noted for regression forests in \hyperref[ref:13]{Wager and Athey (2018)}, their theoretical result guarantees that this issue goes away in the limit.

\subsection{Asymptotic Normality for the Difference of Two Forests}

Given that the estimator (\ref{eqn:5}) is simply the difference between two forests, Theorem 1 of \hyperref[ref:13]{Wager and Athey (2018)} that establishes asymptotic normality of a single forest directly applies under the following conditions on the tree-building procedure. In particular, it relies on honesty to reduce bias, which requires that a tree does not use the same information to make the splits and estimate the within-leaf conditional means. 

\vspace{.3cm}

\noindent \textbf{Assumption 1.\label{asp1}} All trees are \textit{honest} in that the sample used to place the splits is independent of the sample used for estimation; make \textit{random splits} in that at each split, marginalizing over $\xi$, the probability that any dimension gets split is at least $\frac{\pi}{d}$ for $\pi \in (0,1]$; \textit{$\alpha$-regular} for some $\alpha \in (0,0.2]$ in that each split leaves at least $\alpha$ fraction of the observations in the parent node to each child node, and there are between $k$ and $2k-1$ observations in all leaves for some $k\in\mathbb{N}$; \textit{symmetric} in that the order in which the training sample is indexed does not matter.

\vspace{.3cm}

Assumption \hyperref[asp1]{1} summarizes Definitions $2$-$5$ of \hyperref[ref:13]{Wager and Athey (2018)} and can be enforced by the algorithm. To establish asymptotic normality, they also impose the following assumptions on the DGP.

\vspace{.3cm}

\noindent \textbf{Assumption 2.\label{asp2}} The density of the running variable $f_X(x)$ is bounded away from $0$ and infinity. Further, 
\begin{enumerate}[label=(\alph*)]
    \item $\mathbb{E}[Y_i(1)|X=x]$, $\mathbb{E}[Y_i(0)|X=x]$, $\mathbb{E}[Y_i(1)^2|X=x]$ and $\mathbb{E}[Y_i(0)^2|X=x]$ are Lipschitz-continuous, and $Var[Y_i(1)|X=x], Var[Y_i(0)|X=x]>0$.
    \item $\mathbb{E}\left[\left|Y_i-\mathbb{E}[Y_i|X_i=x]\right|^{2+\delta}|X_i=x\right]\leq M$ for some constants $\delta$, $M >0$ uniformly over all $x \in \mathcal{X}$.
\end{enumerate}

\vspace{.3cm}

\noindent\textbf{Remark 3.} We highlight several major differences in the DGP assumptions required by forests, local linear regressions, and the minimax-optimal estimator. First, unlike local linear regressions where these assumptions only need to hold in some neighborhood around a particular boundary point $x^c\in\mathscr{B}$, here we require Assumption \hyperref[asp2]{2} to hold for all $x \in \mathcal{X}$ because trees and forests are not minimizing the MSE (\ref{MSPE}) at a particular point only. The gain is that once we estimate (\ref{eqn:5}), it can simultaneously return the conditional treatment effects at all available boundary points, whereas local linear regressions require repeated estimations, one for each boundary point. On the other hand, the minimax-optimal estimator (\ref{minimax}) requires the CEF to have bounded second derivatives over all $x\in\mathcal{X}$, yet still needs repeated estimations for different boundary points.\footnote{\hyperref[IW2019]{Imbens and Wager (2019)} also consider estimating some weighted average of $\tau(x)$ over the treated sample. But unlike the average treatment effect along the boundary, $\mathbb{E}[Y_i(1)-Y_i(0)|X_i \in \mathscr{B}]$, discussed in Section \ref{sec2}, it is unclear what the corresponding estimand is.}

Another major difference is the continuity assumptions on the moments. Here Lipschitz continuity in Assumption \hyperref[asp2]{2(a)} is a standard assumption in the regression forest literature, whereas in the local linear regression literature, $\mathbb{E}[Y_i(1)|X=x]$ and $\mathbb{E}[Y_i(0)|X=x]$ are required to be thrice continuously differentiable to allow Taylor expansion of the MSE (\ref{mse-opt}). Finally, the proof of \hyperref[ref:13]{Wager and Athey (2018)} invokes the Lyapunov central limit theorem, and Assumption \hyperref[asp2]{2(b)} ensures the Lyapunov condition holds. A similar Lyapunov-type condition is required for the minimax-optimal estimator.

\vspace{.4cm}

The following result is a corollary of Theorem 1 of \hyperref[ref:13]{Wager and Athey (2018)}. Although in their formulation, $X_i$ is drawn from $Uniform([0,1]^d)$, they note that this is for simplicity only and the result applies for any density $f_X(x)$ bounded away from 0 and infinity.

\vspace{.5cm}

\noindent\textbf{Corollary 1.\label{cor1}} \textit{Suppose we have n i.i.d. observations $\{(Y_i, X_i)\}_{i=1}^n$, of which $n_1$ observations fall into the treated region and $n_0$ observations fall into the control region. If Assumptions \hyperref[asp1]{1}-\hyperref[asp2]{2} hold, $n_1$ and $n_0$ grow at the same rate as $n$, and the subsample sizes $s_1$ and $s_0$ satisfy
\begin{align}
    s_1,s_2 \asymp n^{\beta}, \,\,\,\text{for some $\beta$ such that}\quad 1-\left(1+\frac{d}{\pi}\frac{\log(\alpha)}{\log(1-\alpha)}\right)^{-1}<\beta<1.\label{eqn:10}
\end{align}
Then, for any $x^c \in \mathscr{B}$, the difference of two random forests, $\hat{\tau}_{rf}(x^c):=\hat{\mu}_{rf}^+(x^c)-\hat{\mu}_{rf}^-(x^c)$, is asymptotically normal,
\begin{align*}
    \frac{\hat{\tau}_{rf}(x^c)-\tau(x^c)}{\sigma_n(x^c)}\to \mathcal{N}(0,1)\quad \text{for a sequence $\sigma_n(x^c)\to 0$}.
\end{align*}
Further, conditional on $\mathcal{X}_n:=\{X_i\}_{i=1}^n$, $\hat{\mu}_{rf}^+(x^c)$ and $\hat{\mu}_{rf}^-(x^c)$ are independent, and the variance $\sigma_n(x^c)$ can be consistently estimated by the sum of any consistent estimators for $Var[\hat{\mu}_{rf}^+(x^c)]$ and $Var[\hat{\mu}_{rf}^-(x^c)]$.
}

\vspace{.3cm}

\begin{proof}
    The normality result follows directly from Theorem 1 of \hyperref[ref:13]{Wager and Athey (2018)} and that the difference of two normally distributed random variables is normal. To show that  conditional on $\mathcal{X}_n$, $\hat{\mu}_{rf}^+(x^c)$ and $\hat{\mu}_{rf}^-(x^c)$ are independent, we use their least squares representation in (\ref{eqn:8}) and (\ref{eqn:9}) and let $C_+=\left(X_+'W_{rf}(X_+,x^c)X_+\right)^{-1}X_+'W_{rf}(X_+,x^c)$, $C_-=\left(X_+'W_{rf}(X_-,x^c)X_-\right)^{-1}X_-'W_{rf}(X_-,x^c)$. Both $C_+$ and $C_-$ are constant vectors once conditional on $\mathcal{X}_n$. Then
\begin{align*}
    Cov\left(\left.\hat{\mu}_{rf}^+(x^c),\hat{\mu}_{rf}^-(x^c)\right|\mathcal{X}_n\right)&=Cov\left.\left(C_+Y_+, C_-Y_-\right|\mathcal{X}_n\right)=0
\end{align*}
because $Cov(Y_i,Y_j)=0$ for all $i \neq j$ given the data is $i.i.d.$ sampled and there is no overlap of indices between $Y_+$ and $Y_-$.
\end{proof}

\vspace{.3cm}

\noindent\textbf{Remark 4.} Corollary \hyperref[cor1]{1} assumes $\hat{\mu}_{rf}^+(x^c)$ and $\hat{\mu}_{rf}^-(x^c)$ use the same $\pi$ and $\alpha$ for simplicity. The result still holds if we use different tuning parameters for $\hat{\mu}_{rf}^+(x^c)$ and $\hat{\mu}_{rf}^-(x^c)$, but the lower bound for $\beta$ needs to be adjusted accordingly. In practice, using different tuning parameters tailored to the treated and control samples respectively may improve performance. We provide a detailed discussion on parameter tuning in Appendix \hyperref[appendix:A.4]{A.4}.

\vspace{.3cm}

\noindent\textbf{Remark 5.} In terms of estimating the asymptotic variance, we note that both infinitesimal jackknife (IJ) (\hyperref[E2014]{Efron, 2014}; \hyperref[WHE2014]{Wager, Hastie and Efron, 2014}) and bag of little bootstraps (BLB) (\hyperref[SL2009]{Sexton and Laake, 2009}) have been proved to be consistent. See Theorem 9 of \hyperref[ref:13]{Wager and Athey (2018)} for consistency of IJ and Theorem 6 of \hyperref[ref:12]{Athey, Tibshirani and Wager (2019)} for consistency of BLB. In our simulations we implement BLB, which is readily available in the \texttt{grf} package of \hyperref[ref:12]{Athey, Tibshirani and Wager (2019)}.

\section{Simulations}
\label{sec3}
In this section, we present two simulations based on data from \hyperref[L2008]{Lee (2008)} and \hyperref[ref:20]{Keele and Titiunik (2015)} to assess the performance of our proposed estimator (\ref{eqn:5}) against local linear regressions (\ref{eqn:1}) and the alternative minimax-optimal estimator (\ref{minimax}).

For each simulation, we conduct 1000 Monte Carlo replications using a sample of size $n\in\{1000,5000,10000\}$ drawn i.i.d. from the corresponding DGP. We implement all simulations in \texttt{R}, where we use the \texttt{rdrobust} package of \hyperref[ref:7240-1]{Calonico, Cattaneo and Titiunik (2014)} for local linear regressions, the \texttt{optrdd} package of \hyperref[IW2019]{Imbens and Wager (2019)} for the minimax-optimal estimator, and the \texttt{grf} package of \hyperref[ref:12]{Athey, Tibshirani and Wager (2019)} for forests. For multivariate designs, we focus on two scores since the \texttt{optrdd} package accepts at most two scores. We also collapse the bivariate scores to a one-dimensional variable as in \hyperref[ref:20]{Keele and Titiunik (2015)} to implement local linear regressions with multiple scores, since multivariate local linear regressions are in general not asymptotically centered; see the discussion in Section \hyperref[sec:relatedwork]{1.1}. In this section we only consider methods that allow valid inference. For each simulation, we report the average computation time in seconds per Monte Carlo iteration for each estimator and sample size combination in Table \hyperref[tbl:A.5]{A.5} in the Appendix.

We note that, unlike local linear regressions where the optimal choice of the tuning parameter, i.e., the bandwidth, can be easily obtained from minimizing a closed-form optimality criteria, forests, like many other machine learning type of nonparametric estimators, must rely on empirical risk minimization for selecting tuning parameters. We provide a detailed discussion in Appendix \hyperref[appendix:A.4]{A.4} on how the \texttt{grf} package implements parameter tuning, as well as how we modify the process so that the tuned parameters are in accordance with the assumptions in Corollary \hyperref[cor1]{1}. All replication files can be accessed at \href{https://github.com/yqi3/GRF-RD}{\texttt{github.com/yqi3/Replication-GRF-RD}}.

\subsection{A Univariate Design}
\hyperref[L2008]{Lee (2008)} studies the effect of being the current incumbent party in a district on the votes obtained in the district’s next election. The running variable is univariate and measures the difference in vote share between the Democratic candidate and her strongest opponent in the current election, and the outcome variable is the Democratic vote share in the next election. This univariate design normalizes the assignment cutoff to $0$, and units with $X_i\geq 0$ are treated (i.e., winning the current election).

The \hyperref[L2008]{Lee (2008)} data is a popular choice to conduct simulations in the RD literature. \hyperref[ref:4]{Imbens and Kalyanaraman (2012)}, \hyperref[ref:7240-1]{Calonico, Cattaneo and Titiunik (2014)}, and \hyperref[CCFT2019]{Calonico et al. (2019)} all consider the DGP generated by using the \hyperref[L2008]{Lee (2008)} data to fit two $5$-th order global polynomials separately for $X_i \geq 0$ and $X_i < 0$; see Appendix \hyperref[appdix:A.2]{A.2} for details. Figure \hyperref[fig:1]{1} plots the observed data (Panel A) alongside the model data (Panel B), and Table \hyperref[tbl:1]{1} shows the simulation results.

In this univariate example, local linear regressions have better performance than the minimax-optimal estimator and random forests, largely due to a smaller bias. To understand this, we note that this simulation DGP is close to linear in the neighborhood around the cutoff, whereas curvature starts to kick in as we move away from the cutoff (see Panel B of Figure \hyperref[fig:1]{1}). This favors local linear regressions and the minimax-optimal estimator, whose objective functions target only the neighborhood around the cutoff. In contrast, forests have a global target and will thus make efforts to fit the curvature away from the cutoff. In other cases where the CEF does not exhibit strong curvature, there may be gains from forests; see a coin-flip example in Appendix \hyperref[DGPA1]{A.1}. 

\begin{table}[H] \centering 
  \label{tbl:1}
  \scriptsize{
\begin{tabular}
{@{\extracolsep{5pt}}lccccccccc} 
 & \multicolumn{3}{c}{Local Linear\tablefootnote{We note that the slight undercoverage of local linear regressions is consistent with the simulation result ($90.6\%$ coverage) of \hyperref[CCFT2019]{Calonico et al. (2019)}, where they use the same DGP but with $n=1000$ for a total of $5000$ replications. See Table SA-1, Row 2 and Column 3 in their supplemental appendix. We have done our simulation using their \texttt{STATA} code and obtained similar results to those reported in Table \hyperref[tbl:1]{1}.}}
 & \multicolumn{3}{c}{Minimax-Optimal} & \multicolumn{3}{c}{Random Forests}\\
\cline{2-4} \cline{5-7} \cline{8-10}
\\[-1.2ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9)
\vspace{.2cm}
\\
\multicolumn{1}{r}{Sample size $n=$} & $1,000$ & $5,000$ & $10,000$ & $1,000$ & $5,000$ & $10,000$ & $1,000$ & $5,000$ & $10,000$
\vspace{.2cm}
\\[-1.2ex]
\hline \\[-1.2ex] 
\multicolumn{1}{l}{Mean} & 0.0561 & 0.0510 & 0.0478 & 0.0778 & 0.0652 & 0.0607 & 0.0854 & 0.0616 & 0.0526\\
\multicolumn{1}{l}{Variance} &  0.0021 & 0.0005 & 0.0003 & 0.0008 & 0.0002 & 0.0001 & 0.0012	& 0.0006 & 0.0005\\
\multicolumn{1}{l}{MSE} &  0.0023 & 0.0006 & 0.0003 & 0.0023 & 0.0009 & 0.0006 & 0.0033 & 0.0010 & 0.0007\\
\multicolumn{1}{l}{95\% coverage} & 91.1\% & 88.3\% & 90.6\% & 80.5\% & 70.9\% & 66.3\% & 68.7\% & 82.5\%	& 89.3\%\\
\\[-1.2ex]\hline 
\end{tabular}
}
\\
\vspace{.2cm}
{\noindent \justifying \scriptsize \textbf{Table 1.} Lee (2008): Estimating incumbency advantage on vote share in the next election. The true conditional treatment effect at the 0 cutoff is 0.04. Columns 1-3 correspond to the local linear regression estimator (\ref{eqn:1}) with robust bias correction and is implemented by the \texttt{rdrobust} package with default options; Columns 4-6 correspond to the minimax-optimal estimator (\ref{minimax}) implemented by the \texttt{optrdd} package, where $\mathcal{B}$ is estimated by fitting a global quadratic and then multiplying the maximal curvature of the fitted model by 4 and $\sigma_i^2$ is estimated by averaging the residuals from an ordinary least-squares model of $Y_i$ on the interaction of $X_i$ and $A(X_i)$; Columns 7-9 correspond to the forest estimator in (\ref{eqn:5}) with $B=5000$ trees and is implemented by the \texttt{grf} package with parameter tuning as discussed in Appendix \hyperref[appendix:A.4]{A.4}. The 95\% coverage rate records the Monte Carlo coverage probability of the robust bias-corrected confidence intervals for local linear regressions, the uniform confidence intervals for the minimax-optimal estimator, and confidence intervals obtained from the the bag of little bootstraps variance estimator for random forests.\par}
\end{table} 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/Lee_vis.png}
    \label{fig:1}\\
    {\noindent \justifying \scriptsize\textbf{Figure 1.} Lee (2008): Democratic victory margin in the current election against the vote share in the next election. Panel A shows the observed data. Panel B plots the model data fitted using a 5-th order polynomial with different coefficients for the treated and control observations. The cutoff is normalized to 0. The figure is generated using the \texttt{rdrobust} package (\hyperref[ref:7240-1]{Calonico, Cattaneo and Titiunik, 2014}) with default smoothing. \par}
\end{figure}

Table \hyperref[tbl:1]{1} also reports the Monte Carlo average MSE (i.e., bias squared plus variance) of each estimator. Although the minimax-optimal estimator targets the neighborhood around the cutoff and has slightly lower MSE than forests, it is highly biased and has poor coverage rate. The reason is that estimating the second derivative bound $\mathcal{B}$ by fitting a global quadratic and then multiplying the maximal curvature of the fitted model by 4 can lead to unreasonable estimates of $\mathcal{B}$, upon which the performance of the minimax-optimal estimator crucially depends. Table \hyperref[tbl:A.2.1]{A.2.1} in the Appendix shows how estimates of $\mathcal{B}$ and the corresponding simulation results change with the constant we multiply, and it is only after multiplying the maximal estimated curvature by $30$ do we get an estimate that is close to the true (least upper) bound $\mathcal{B}$. Of course, one could argue that a global second-order polynomial is a bad model to start with; but without knowledge about the true DGP, it is difficult to choose a “good” model. With more dimensions, accurately estimating second derivatives can be even more difficult.

We therefore recommend local linear regressions in the univariate design, unless the researcher has \textit{a priori} knowledge about the underlying DGP.


\subsection{A Bivariate Design}
The more interesting example is \hyperref[ref:20]{Keele and Titiunik (2015)}, who study the effect of television advertising on election turnout, where the running variable is the two-dimensional geographic coordinate of a voter's location and the outcome of interest is election turnout and takes binary values. Treatment status is determined by a geographic media market boundary. Their data also contains several other outcome variables for placebo analysis. We include age and housing price per square foot in our simulation in addition to election turnout to cover both discrete and continuous outcome variables. 

We generate DGP for the continuous outcomes by fitting a CEF of the form ${Y_i}_j = \mu_j(X_i) +\epsilon_{ij}$, where $\epsilon_{ij} \sim N(0,\sigma_j^2)$ and $j \in \{\text{housing price}, \text{age}\}$. We fit a $3$-rd order interacted polynomials for $\mu_j(X_i)$, with $\sigma_j^2$ estimated from the data. To generate the coordinates, $X_i$, in a way that preserve their spatial correlations to some extent, we first randomly draw a coordinate from the original data with replacement, and perturb it slightly with a noise term drawn from $\mathcal{N}(0,0.01^2)$.\footnote{We also make sure this noise term is bounded by discarding draws with absolute value exceeding $10$, so that the CEFs indeed have bounded second derivatives.} We then classify the treatment status of $X_i$ based on its location with respect to the treatment boundary and fit the coordinates to the models above to generate the outcomes.

For the discrete turnout variable, we consider a logit model, $P(Y_i = 1 | X_i) = \frac{e^{poly(X_i)}}{1+e^{poly(X_i)}}$, where $poly(X_i)$ is again a $3$-rd order interacted polynomial. To generate binary outcomes for turnout, we avoid simply rounding the predicted probabilities. The reason is that the sample distributions of the two classes (i.e., voted or did not vote) are highly mixed in the sense that there is little discernible geographic pattern. In addition, since the sample contains more ones than zeros, after rounding the logit model will mostly predict ones (i.e., voted). Instead, we consider coin flips. At every location $X_i$, its corresponding outcome $Y_i$ is drawn from a Bernoulli distribution with the predicted probability $P(Y_i = 1 | X_i)$ from the logit model. This coin flip procedure adds desirable randomness to the predicted outcomes while achieving an accuracy rate of around $58\%$ when compared with the observed data. Figure \hyperref[fig:2]{2} visualizes how we generate the DGP for turnout. We leave further explanations for age and housing price in Appendix \hyperref[appdix:A.3]{A.3}.

\begin{figure}[H]
    \centering
    \captionsetup{labelformat=empty}
    \vspace{0.2cm}
    \includegraphics[width=1\textwidth]{KT_Turnout_Vis.jpg}
    \label{fig:2}\\
{\noindent \justifying \scriptsize
\textbf{Figure 2.} Fitting the DGP for election turnout. The black curve represents the treatment boundary, where coordinates below the boundary are treated, and the green circle shows the boundary point at which we estimate the treatment effect. The first plot shows the observed data. The second plot shows the predicted probabilities from a logit model evaluated at the observed geographic coordinates. Notice that almost all points have predicted probabilities larger than $0.5$, making rounding problematic. The third plot resolves this issue by the coin flip method. The last plot shows the simulated data with the coordinates sampled with replacement from the original sample plus a noise term $N(0,0.01^2)$; to make sure the CEFs have bounded second derivatives, we discard noise draws with absolute value exceeding 10. \par}
\end{figure}

In Figure \hyperref[fig:2]{2}, the black curve represents the media market boundary, and coordinates below the curve are treated (i.e., exposed to the advertising). \hyperref[ref:20]{Keele and Titiunik (2015)} select three equally spaced boundary points, depicted as black squares on the treatment boundary. Without loss of generality, we pick the middle point, circled in green, to be the cutoff point of interest in our simulation. We report in Table \hyperref[tbl:2]{2} the simulation results, where for local linear regressions, following \hyperref[ref:20]{Keele and Titiunik (2015)} we collapse the bivariate score using chordal distance, which is a rescaling of Euclidean distance. For the minimax-optimal estimator, as noted by \hyperref[IW2019]{Imbens and Wager (2019)}, using a global quadratic model to estimate $\mathcal{B}$ is not sensitive enough to the strong local structure of the \hyperref[ref:20]{Keele and Titiunik (2015)} data.\footnote{The visualization in Figure \hyperref[fig:A2]{A.2} for age and housing price confirms the strong local structure observed in Figure 7 of the working paper version of \hyperref[IW2019]{Imbens and Wager (2019)}; however in our DGP, turnout does not exhibit strong local structure due to polynomial smoothing.} We therefore follow \hyperref[IW2019]{Imbens and Wager (2019)}  and estimate $\mathcal{B}$ by taking the $95$-th percentile (over all sample points) of the operator norm curvature estimated by a cross-validated ridge regression with interacted $7$-th order natural splines, which is able to adequately account for the curvature in our example given that the true CEF are 3-rd order polynomials.

\begin{table} \centering 
  \label{tbl:2}
  \scriptsize{
\begin{tabular}
{@{\extracolsep{5pt}}lccccccccc} 
 & \multicolumn{3}{c}{Local Linear}
 & \multicolumn{3}{c}{Minimax-Optimal} & \multicolumn{3}{c}{Random Forests}\\
\cline{2-4} \cline{5-7} \cline{8-10}
\\[-1.2ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9)
\vspace{.2cm}
\\
\multicolumn{1}{r}{Sample size $n=$} & $1,000$ & $5,000$ & $10,000$ & $1,000$ & $5,000$ & $10,000$ & $1,000$ & $5,000$ & $10,000$
\vspace{.2cm}
\\[-1.2ex]
\hline \\[-1.2ex] 
\multicolumn{4}{l}{\textit{Panel A: Turnout Probability (truth=0.209)}}\\
\multicolumn{1}{l}{Mean} & 0.167 & 0.153 & 0.156 & 0.201 &	0.208 & 0.208 & 0.288 & 0.253 & 0.241\\
\multicolumn{1}{l}{Variance} & 0.090 & 0.014 & 0.006 & 0.031 & 0.010 & 0.007 & 0.010 & 0.007 & 0.005\\
\multicolumn{1}{l}{MSE} & 0.091 & 0.017 & 0.009 & 0.031	& 0.010	& 0.007 & 0.016 & 0.008 & 0.006\\
\multicolumn{1}{l}{95\% coverage} &  93.7\% & 95.5\% & 95.8\% & 96.1\% & 97.5\% & 97.4\% & 86.1\% & 90.5\%	& 92.7\%\\
\\[-1.2ex] \hline 
\multicolumn{4}{l}{\textit{Panel B: Housing Price (truth=11.600)}}\\
\multicolumn{1}{l}{Mean} & -3.643 & -1.356 & -0.095 & 6.514 & 6.355 & 6.390 & 19.828 & 14.346 & 12.987 \\
\multicolumn{1}{l}{Variance} & 634.685 & 87.831 & 49.477 & 337.703 & 105.899 & 66.337 & 80.199 & 53.818 & 42.557\\
\multicolumn{1}{l}{MSE} & 866.792 & 255.481 & 186.047 & 363.486 & 133.322 & 93.390 & 148.031 & 61.407 & 44.505\\
\multicolumn{1}{l}{95\% coverage} & 91.2\% & 85\% & 77.1\% & 98.1\% & 97.1\%	& 95.0\% & 85.6\% & 93.7\% & 94.2\%\\
\\[-1.2ex]\hline 
\multicolumn{4}{l}{\textit{Panel C: Age (truth=7.730)}}\\
\multicolumn{1}{l}{Mean} & 6.300 & 6.182 & 6.236 & 7.577 & 7.661 & 7.771 & 7.859 & 7.433 &	7.484\\
\multicolumn{1}{l}{Variance} & 112.144 & 16.615 & 7.575 & 25.643 & 9.303 & 6.152 & 10.438	& 8.154 & 6.632\\
\multicolumn{1}{l}{MSE} &  114.189 & 19.012 & 9.808 & 25.666 & 9.307 & 6.154 & 10.455 & 8.243	& 6.693\\
\multicolumn{1}{l}{95\% coverage} & 93.4\% & 94.8\% & 95.2\% & 98.0\% & 98.9\% & 98.0\% & 95.5\% & 94.1\% & 95.2\%\\
\\[-1.2ex]\hline
\end{tabular}
}
\\
\vspace{.2cm}
{\noindent \justifying \scriptsize \textbf{Table 2.} Keele and Titiunik (2015): Estimating the effect of political television campaign. The true treatment effects in parentheses are calculated from the DGP models. Columns 1-3 correspond to the local linear regression estimator (\ref{eqn:1}) with robust bias correction and is implemented by the \texttt{rdrobust} package with default options; the effective running variable is univariate, measured by the rescaled Euclidean (chordal) distance between the coordinate and the cutoff point. Columns 4-6 correspond to the minimax-optimal estimator (\ref{minimax}) implemented by the \texttt{optrdd} package, where $\mathcal{B}$ is the $95$-th percentile (over all sample points) of the operator norm curvature estimated by a cross-validated ridge regression with interacted $7$-th order natural splines and $\sigma_i^2$ is estimated by averaging the residuals from an ordinary least-squares model of $Y_i$ on the interaction of $X_i$ and $A(X_i)$. Columns 7-9 correspond to the forest estimator in (\ref{eqn:5}) with $B=5000$ trees and is implemented by the \texttt{grf} package with parameter tuning as discussed in Appendix \hyperref[appendix:A.4]{A.4}. The 95\% coverage rate records the Monte Carlo coverage probability of the robust bias-corrected confidence intervals for local linear regressions, the uniform confidence intervals for the minimax-optimal estimator, and confidence intervals obtained from the the bag of little bootstraps variance estimator for random forests.\par}
\end{table} 

In this bivariate design, overall both the minimax-optimal estimator and random forests have a clear advantage over local linear regressions with a collapsed univariate score. This advantage seems not solely due to the fact that the minimax-optimal estimator and forests effectively explore both dimensions of the running variable; in
Appendix \hyperref[appdix:A.3.1]{A.3.1}, we run the same simulations for the minimax-optimal estimator and forests but use the collapsed univariate score, and the same argument still holds, except now the coverage rates seem to suffer due to bias once we collapse the dimensions. This suggests that the minimax-optimal estimator and random forests have more stable performance than local linear regressions, which seem to be sensitive to transformations of the running variable.

In this example, forests outperform the minimax-optimal estimator in terms of MSE when the sample size is small to moderate, but the latter is able to catch up with the performance of forests as the sample size grows. However, we caution that direct comparison between the minimax-optimal estimator and forests is not fair, as the former is designed for minimizing the \textit{worst-case} MSE and the construction of confidence intervals aims at achieving \textit{uniform} coverage over a class of CEF with second derivatives bounded by $\mathcal{B}$. This is reflected in the overcoverage in Columns 4-6 of Table \hyperref[tbl:2]{2}. We also note that when the sample size is small, forests tend to suffer from bias and thus undercoverage. In practice, researchers may consider both forests and the minimax-optimal estimator in multivariate RD designs; the estimates of forests could be used to guide the selection of $\mathcal{B}$.

Overall, these results highlight the potential of local estimation using random forests in RD designs with multiple scores and open doors to flexible estimation of conditional treatment effect as a multivariate function of boundary points.


\section{Discussion}
\label{sec4}
In this paper, we propose using forests to estimate conditional treatment effects identified by the difference between two conditional expectation functions in sharp RD designs, where there can be more than one treatment assignment rule and the standard estimation strategy using local linear regressions cannot be directly applied. Asymptotic normality of this forest-based estimator follows directly from \hyperref[ref:13]{Wager and Athey (2018)}, with readily available consistent variance estimators to conduct valid inference. In our simulations with multiple scores, forests outperform local linear regressions. In comparison with the alternative minimax-optimal estimator, forests do not require estimating any nuisance parameters of the DGP while achieving competitive performance. Extensions to more than two scores using forests are straightforward and require no modification of the existing algorithm.

We note, however, that theoretical analysis comparing the convergence rate of forests with that of other nonparametric estimators is complicated by the fact that existing rates for forests are based on upper bounds, whereas the rate for local linear regressions is exact. Theorem 1 of \hyperref[ref:13]{Wager and Athey (2018)} implies the MSE for forests is of the order $O\left(n^{-(1-\beta)}\right)$. This rate depends on the lower bound for $\beta$ in (\ref{eqn:10}), which in turn depends on the upper bound for bias. It will be helpful for future research to investigate conditions under which tighter bounds could be established to allow meaningful comparisons across nonparametric estimators. In addition, we choose the scaling of the subsample size via the tuning procedure described in Appendix \hyperref[appendix:A.4]{A.4}. As mentioned by \hyperref[ref:13]{Wager and Athey (2018)}, deriving the optimal scaling of the subsample fraction for minimizing MSE can be of considerable interest.

\input{ref}

\newpage
\normalsize{
\input{appendix.tex}
}

\end{document}