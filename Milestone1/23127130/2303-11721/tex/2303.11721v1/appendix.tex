\section*{Appendix}
\subsection*{A.1 \hspace{.3cm} A Motivating Toy Example}
As a motivating example demonstrating the potential of ensemble trees as adaptive multivariate kernel functions, we present a simple simulation. In addition to random forests, we also present results for gradient boosted regression trees (GBRT) (\hyperref[ref:5]{Friedman, 2001}) and one-step boosted forests (\hyperref[ref:7240-2]{Ghosal and Hooker, 2021}). These methods are based on gradient boosting to reduce the bias of a class of already low-variance estimators.

\vspace{.5cm}

We consider a simple data generating process:
\label{DGPA1}
\begin{align*}
     \text{\textbf{DGP A.1.}} && &X_i \sim Uniform[-1,1]^d, &\\
    && &A(X_i) = \mathbf{1}\{X_i \geq 0\}\quad \text{where $\geq$ is element-wise}, &\\
    && &Y_i \sim Bernoulli\left(p=0.2+0.3A(X_i)\right),
\end{align*}
so that the true treatment effect at the $0$ cutoff is $0.3$. We consider up to 3 running variables and estimate the mean, variance and MSE of the estimated ${\tau}(0)$ using each method in Monte Carlo simulations. Figure \hyperref[fig:A.1]{A.1} visualizes the simulation DGP \hyperref[DGPA1]{A.1}. The results are reported in Table \hyperref[tbl:A.1]{A.1}.

 Surprisingly, even in this simple toy example, in Table \hyperref[tbl:A.1]{A.1}, Panel A where we only have a univariate score, local linear regression performs worse than the tree-based methods in terms of MSE. When we increase the dimension of the running variable, the performance of local linear estimator worsens, and this is partly due to the fact that in the multivariate setting we follow \hyperref[ref:20]{Keele and Titiunik (2015)} to transform the running variable by calculating its Euclidean distance to the 0 cutoff. The effective running variable that goes into the local linear regression is therefore a scalar $S_i = sign(2A(X_i)-1)||X_i-0||_2$. It can be verified that $S_i$ has a zero density evaluated at the cutoff, violating a key assumption in employing local linear regressions; see Appendix \hyperref[appendix:A.6]{A.6}. 

 The ensemble tree-based methods, on the other hand, are robust to the increase in the dimension of the running variable. Random forests and GBRT are competitive in terms of performance, whereas boosted forests seem to be slightly noisier. Based on empirical applications of forests and GBRT, they tend to have similar performance. But  in addition to theoretical guarantees, forests are easier and faster to tune, as they rely on out-of-bag error instead of the more time consuming cross-validation, and are less sensitive to tuning parameters and more resilient against over-fitting problems. For GBRT, however, we manually create a grid of 30 possible combinations of tuning parameters and perform a $5$-fold cross validation. We therefore focus on random forests throughout the paper.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{DGPplots_nocov.jpg}
    \captionsetup{labelformat=empty}
    \label{fig:A.1}\\
{\noindent \justifying \footnotesize \textbf{Figure A.1.} Visualization of DGP \hyperref[DGPA1]{A.1}. The top left panel shows the DGP with a univariate running variable and plots the outcome against the running variable; the top right panel shows the DGP with a bivariate score, each represented by an axis, and the color represents the value of the outcome, where red is the treated region and corresponds to an average of 0.5; the bottom panel shows the DGP with a trivariate score, again each represented by an axis, and the treated region is the red cube. All plots are generated using $500,000$ draws from the DGP.\par}
\end{figure}

\begin{table}[H] \centering 
  \label{tbl:A.1} 
  \footnotesize{
\begin{tabular}{@{\extracolsep{5pt}}lcccc} 
\\
 & \multicolumn{1}{c}{Local Linear} & \multicolumn{1}{c}{Random Forests} & \multicolumn{1}{c}{Boosted Forests} & \multicolumn{1}{c}{GBRT} \\
\\[-1.8ex] & (1) & (2) & (3) & (4) \\ 
\hline \\[-1.8ex] 
\multicolumn{5}{l}{\textit{Panel A: Univariate running variable}}\\
\multicolumn{1}{l}{Average estimate} & 0.307 & 0.302 & 0.301 & 0.291 \\
\multicolumn{1}{l}{Variance} & 0.019 &  0.008 & 0.024 & 0.003 \\
\multicolumn{1}{l}{MSE} & 0.019 &  0.008 & 0.024 & 0.003 \\
\multicolumn{1}{l}{95\% coverage} & 93.2\% & 94.5\% & - & - \\
\\
\multicolumn{5}{l}{\textit{Panel B: Bivariate running variable}}\\
\multicolumn{1}{l}{Average estimate} & 0.283 & 0.301 & 0.302 & 0.287\\
\multicolumn{1}{l}{Variance} & 0.219 &  0.007 & 0.009 & 0.004\\
\multicolumn{1}{l}{MSE} & 0.219 & 0.007 & 0.009 & 0.004\\
\multicolumn{1}{l}{95\% coverage} & 88.6\% & 94.0\% & - & - \\
\\
\multicolumn{5}{l}{\textit{Panel C: Trivariate running variable}}\\
\multicolumn{1}{l}{Average estimate} & 0.283 & 0.299 & 0.300 & 0.292\\
\multicolumn{1}{l}{Variance} & 0.494 &  0.006 & 0.008 & 0.006 \\
\multicolumn{1}{l}{MSE} & 0.494 & 0.006 & 0.008 & 0.006\\
\multicolumn{1}{l}{95\% coverage} & 85.3\% & 93.5\% &  - & -\\
\\[-1.8ex]\hline 
\end{tabular}
}
\\
\vspace{.2cm}
{\noindent \justifying \scriptsize \textbf{Table A.1.} Estimating the conditional treatment effect at the 0 cutoff under DGP \hyperref[DGPA1]{A.1}. The true conditional treatment effect at the 0 cutoff is 0.3. The results are based on 1000 Monte Carlo iterations, each using a sample size of 1000 from DGP \hyperref[DGPA1]{A.1}. We implement the simulation in \texttt{R}. Column 1 uses local linear regressions with bias correction and is implemented by the \texttt{rdrobust} package (\hyperref[ref:7240-1]{Calonico, Cattaneo and Titiunik, 2014}) with default options. Columns 2 and 3 use honest forests and boosted forests with one boosting iteration (\hyperref[ref:7240-2]{Ghosal and Hooker, 2021}), respectively, and are implemented by the \texttt{grf} package (\hyperref[ref:12]{Athey, Tibshirani and Wager, 2019}) with $B=5000$ trees. Column 2 implements parameter tuning as discussed in Appendix \hyperref[appendix:A.4]{A.4}. Column 4 uses GBRT (\hyperref[ref:5]{Friedman, 2001}) implemented by the \texttt{xgboost} package. For local linear regressions, we record the 95\% coverage rate using robust bias-corrected confidence intervals. For random forests, we record the 95\% coverage rate with confidence intervals built using the bag of little bootstraps variance estimator in the \texttt{grf} package. \par}
\end{table}

\subsection*{A.2 \hspace{.3cm} Lee (2008): Estimating Incumbency Advantage on Vote Share in the Next Election \label{appdix:A.2}}
This section provides details on the DGP employed in our simulation study using the \hyperref[L2008]{Lee (2008)} dataset. Following \hyperref[ref:4]{Imbens and Kalyanaraman (2012)}, \hyperref[ref:7240-1]{Calonico, Cattaneo and Titiunik (2014)} and \hyperref[CCFT2019]{Calonico, Cattaneo, Farrell and Titiunik (2019)}, we generate a DGP as follows,
    \begin{align*}
        Y_i = \mu(X_i) +\epsilon_i, \quad X_i \sim (2{Beta}(2,4)-1), \quad \epsilon \sim N(0,\sigma^2)
    \end{align*}
    where $\mu(X_i)$ is constructed by fitting a $5$-th order global polynomial with different coefficients for the treated and untreated groups, ${Beta}$ denotes the Beta distribution, and $\sigma$ is estimated from the residuals of the regression.

\begin{align}\label{DGP:Lee}
  \mu(x) =
  \begin{cases}
  0.48+1.27x+7.18x^2+20.21x^3+21.54x^4+7.33x^5 & \text{if $x<0$} \\
  0.52+0.84x-3.00x^2+7.99x^3-9.01x^4+3.56x^5 & \text{if $x\geq0$}
  \end{cases}    
\end{align}

and $\sigma=0.1295$. Hence the true treatment effect at the $0$ cutoff is $0.04$.

\subsubsection*{A.2.1 \hspace{.3cm} Estimating the Second Derivative Bound for the Conditional Mean Functions \label{appdix:A.2.1}}
In this section we consider estimating the second derivative bound $\mathcal{B}$ for the DGP based on \hyperref[L2008]{Lee (2008)} by fitting a global quadratic function and multiplying the maximal estimated curvature by different constants. The results are reported in Table \hyperref[tbl:A.2.1]{A.2.1}, along with the results using the infeasible least upper bound $\mathcal{B}$ calculated from the true DGP. \hyperref[IW2019]{Imbens and Wager (2019)} recommend multiplying by 2 or 4, but notice that in this example these choices do not yield reasonable bounds for the second derivative and thus lead to biased estimates and poor coverage rates; multiplying the maximal estimated curvature by 30 gives estimates that are (on average over the Monte Carlo replications) close to the true $\mathcal{B}$.

\begin{table}[H] \centering 
  \label{tbl:A.2.1}
  \scriptsize{
\begin{tabular}
{@{\extracolsep{5pt}}lccccccccc} 
\hline
\vspace{-.28cm}
\\
 & \multicolumn{3}{c}{True $\mathcal{B}=14.36$}
 & \multicolumn{3}{c}{$\hat{\mathcal{B}}=2\times$max.curvature} & \multicolumn{3}{c}{$\hat{\mathcal{B}}=4\times$max.curvature}\\
\cline{2-4} \cline{5-7} \cline{8-10}
\vspace{-.2cm}
\\
\multicolumn{1}{r}{Sample size $n=$} & $1,000$ & $5,000$ & $10,000$ & $1,000$ & $5,000$ & $10,000$ & $1,000$ & $5,000$ & $10,000$
\vspace{.2cm}
\\[-1.2ex]
\hline \\[-1.2ex] 
\multicolumn{1}{l}{Mean} & 0.0538 & 0.0471 & 0.0453 & 0.0832 & 0.0740 & 0.0693 & 0.0778 & 0.0652 & 0.0607\\
\multicolumn{1}{l}{Variance} & 0.0019 & 0.0006 & 0.0003 & 0.0006 & 0.0002 & 0.0001 & 0.0008 & 0.0002 & 0.0001\\
\multicolumn{1}{l}{MSE} & 0.0021 & 0.0006 & 0.0003 & 0.0025 & 0.0013 & 0.0010 & 0.0023 & 0.0009 & 0.0006\\
\multicolumn{1}{l}{95\% coverage} & 96.7\% & 96.7\% & 97.0\% & 67.6\% & 37.4\% & 25.3\% & 80.5\% & 70.9\%	& 66.3\%\\
\multicolumn{1}{l}{Average $\hat{\mathcal{B}}$} & 14.360 & 14.360 & 14.360 & 0.8372 & 0.8382 & 0.8408 & 1.6745 & 1.6765 & 1.6816\\
\\[-1.2ex]\hline 
\vspace{-.28cm}
\\
 & \multicolumn{3}{c}{$\hat{\mathcal{B}}=6\times$max.curvature}
 & \multicolumn{3}{c}{$\hat{\mathcal{B}}=10\times$max.curvature} & \multicolumn{3}{c}{$\hat{\mathcal{B}}=30\times$max.curvature}\\
\cline{2-4} \cline{5-7} \cline{8-10}
\vspace{-.2cm}
\\
\multicolumn{1}{r}{Sample size $n=$} & $1,000$ & $5,000$ & $10,000$ & $1,000$ & $5,000$ & $10,000$ & $1,000$ & $5,000$ & $10,000$
\vspace{.2cm}
\\[-1.2ex]
\hline \\[-1.2ex] 
\multicolumn{1}{l}{Mean} & 0.0728 & 0.0604 & 0.0564 & 0.0661 & 0.0552 & 0.052 & 0.0550 & 0.0477 & 0.0458\\
\multicolumn{1}{l}{Variance} & 0.0010 & 0.0003 & 0.0002 & 0.0012 & 0.0003 & 0.0002 & 0.0018 & 0.0005 & 0.0003\\
\multicolumn{1}{l}{MSE} & 0.0021 & 0.0007 & 0.0004 & 0.0019 & 0.0006 & 0.0003 & 0.0020 & 0.0006 & 0.0003\\
\multicolumn{1}{l}{95\% coverage} & 87.7\% & 83.3\% & 82.2\% & 93.4\% & 91.2\% & 92.6\%	& 96.5\%	& 96.6\% & 96.7\%\\
\multicolumn{1}{l}{Average $\hat{\mathcal{B}}$} & 2.5117 & 2.5147 & 2.5224 & 4.1861 & 4.1912 & 4.2040 & 12.5584 & 12.5736 & 12.6121\\
\\[-1.2ex]\hline 
\end{tabular}
}
\\
\vspace{.2cm}
{\noindent \justifying \scriptsize \textbf{Table A.2.1.} Simulation results based on different estimates of the second derivative bound. The true least upper bound on the second derivative of the DGP (\ref{DGP:Lee}) is $\mathcal{B}=14.36$. The table reports the average point estimate, variance, MSE, and $95\%$ coverage rate over the $1000$ Monte Carlo replications using the true bound as well as estimated bounds by fitting a global quadratic and then multiplying the maximal curvature of the fitted model by different constants $\in\{2,4,6,10,30\}$. Also reported is the Monte Carlo average of estimated bounds. \hyperref[IW2019]{Imbens and Wager (2019)} recommend multiplying by 2 or 4, but notice that in this example these choices do not yield reasonable bounds for the second derivative and thus lead to biased estimates and poor coverage rates; multiplying the maximal estimated curvature by 30 gives $\hat{\mathcal{B}}$ close to the true $\mathcal{B}$.\par}
\end{table} 

\subsection*{A.3 \hspace{.3cm} Keele and Titiunik (2015): Estimating the Effect of Political Television Campaign \label{appdix:A.3}}
This section provides details on the DGP employed in our simulation study regarding the \hyperref[ref:20]{Keele and Titiunik (2015)} dataset. For the continuous outcome variables, housing price and age, the models are of the form 
\[{Y_i}_j = \mu_j(X_i) +\epsilon_{ij}, \text{ where } j \in \{\text{price}, \text{age}\} \text{ and } \epsilon_{ij} \sim N(0,\sigma_j^2).\]
Following \hyperref[IW2019]{Imbens and Wager (2019)}, we fit $3$-rd order interacted polynomials for $\mu_j(X_i)$. Below are the model specifications, where $\Tilde{x}_{i1}, \Tilde{x}_{i2}$ are demeaned versions of $x_{i1}, x_{i2}$ respectively: 
\[\mu_{price}(x_i) =
  \begin{cases}
  77307.53-1026.24{x}_{i1}+60847.12\Tilde{x}_{i1}^2-749930.08\Tilde{x}_{i1}^3\\
  \quad+481.10{x}_{i2} -13224.51\Tilde{x}_{i2}^2+185693.53\Tilde{x}_{i2}^3 -16725.49 \Tilde{x}_{i1}\Tilde{x}_{i2}& \text{if $i$ is treated,} \\
  230407.6-9713.5{x}_{i1}+537644.5\Tilde{x}_{i1}^2-8356369.6\Tilde{x}_{i1}^3\\
  \quad-2164.6{x}_{i2} -9998.4\Tilde{x}_{i2}^2+748352.9\Tilde{x}_{i2}^3 +55631.1\Tilde{x}_{i1}\Tilde{x}_{i2}& \text{if $i$ is control,}
  \end{cases}
\]
and $\sigma_{price} = 32.63344$.
\[\mu_{age}(x_i) =
  \begin{cases}
  13544.2+150.2{x}_{i1}-11847.5\Tilde{x}_{i1}^2-29821.7\Tilde{x}_{i1}^3\\
  \quad+259.3{x}_{i2} -15479.8\Tilde{x}_{i2}^2-207469.6\Tilde{x}_{i2}^3 +24446.8\Tilde{x}_{i1}\Tilde{x}_{i2}& \text{if $i$ is treated,} \\
  77307.53-1026.24{x}_{i1}+60847.12\Tilde{x}_{i1}^2-749930.08\Tilde{x}_{i1}^3\\
  \quad+481.10{x}_{i2} -13224.51\Tilde{x}_{i2}^2+185693.53\Tilde{x}_{i2}^3 -16725.49 \Tilde{x}_{i1}\Tilde{x}_{i2}& \text{if $i$ is control,}
  \end{cases}
\]
and $\sigma_{age} = 15.94957$.

\vspace{0.5cm}
For the discrete outcome, election turnout, we consider a logit conditional expectation function $P(Y_i = 1 | X_i) = \frac{e^{poly(X_i)}}{1+e^{poly(X_i)}}$, where $poly(X_i)$ is a third-order interacted polynomial as before. Below are the specifications for $poly(X_i)$: 
\[poly(x_i) =
  \begin{cases}
  -200.4079+3.71458{x}_{i1}+224.367\Tilde{x}_{i1}^2+1028.082\Tilde{x}_{i1}^3\\
  \quad-0.69173{x}_{i2} +57.06678\Tilde{x}_{i2}^2+3778.132\Tilde{x}_{i2}^3 -127.5027\Tilde{x}_{i1}\Tilde{x}_{i2}& \text{if $i$ is treated,} \\
  10399.59239-286.507{x}_{i1}+10515.95017\Tilde{x}_{i1}^2-114884.76485\Tilde{x}_{i1}^3\\
  \quad-15.44891{x}_{i2} -423.756\Tilde{x}_{i2}^2+12700.05737\Tilde{x}_{i2}^3 +228.6636\Tilde{x}_{i1}\Tilde{x}_{i2}& \text{if $i$ is control,}
  \end{cases}
\]

Figure \hyperref[fig:A2]{A.2} visually presents how we generate these DGP.

\vspace{0.5cm}
\begin{figure}[H]
    \centering
    \captionsetup{labelformat=empty}
    \caption{Panel A: Turnout Probability}
    \vspace{0.2cm}
    \includegraphics[width=1\textwidth]{KT_Turnout_Vis.jpg} \\
    \vspace{0.2cm}
    \caption{Panel B: Housing Price}
    \vspace{0.1cm}
    \includegraphics[width=1\textwidth]{KT_Price_Vis.png} \\
    \vspace{0.2cm}
    \caption{Panel C: Age}
    \vspace{0.1cm}
    \includegraphics[width=1\textwidth]{KT_Age_Vis.png}
    \label{fig:A2}\\
{\noindent \justifying \footnotesize \textbf{Figure A.2.} Visualization of the simulated DGP based on \hyperref[ref:20]{Keele and Titiunik (2014)}. The black curve represents the treatment boundary, where coordinates below the boundary are treated, and the green circle shows the boundary point at which we estimate the treatment effect. Housing price uses a different dataset, so there are fewer observations than the other two cases. For turnout in Panel A, the second plot shows the predicted probabilities of a logit model evaluated at the observed geographic coordinates. Notice that almost all points have predicted probabilities larger than $0.5$, making rounding problematic. The third plot resolves this issue by the coin flip method. For housing price and age in panels B and C, the plots in the middle are the model predictions using the actual geographic coordinates. In every panel, the last plot presents the 2500 simulated points for better visualization, with the coordinates sampled with replacement from the original sample plus a noise term $N(0,0.01^2)$; to make sure the CEFs have bounded second derivatives, we discard noise draws with absolute value exceeding 10. \par}
\end{figure}


\subsubsection*{A.3.1 \hspace{.3cm} Using the Transformed Univariate Score for All Estimators
\label{appdix:A.3.1}}
In this section we assess the performance of the minimax-optimal estimator and random forests when the bivariate score is collapsed to the rescaled Euclidean (chordal) distance. Table \hyperref[tbl:A.3.1]{A.3.1} reports the results. Overall the minimax-optimal estimator and random forests still outperform local linear regressions, except now the coverage rates seem to suffer due to bias once we collapse the dimensions. This suggests that the minimax-optimal estimator and random forests have more stable performance than local linear regressions, which seem to be sensitive to transformations of the running variable.

\begin{table}[H] \centering 
  \label{tbl:A.3.1}
  \scriptsize{
\begin{tabular}
{@{\extracolsep{5pt}}lccccccccc} 
 & \multicolumn{3}{c}{Local Linear}
 & \multicolumn{3}{c}{Minimax-Optimal} & \multicolumn{3}{c}{Random Forests}\\
\cline{2-4} \cline{5-7} \cline{8-10}
\\[-1.2ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9)
\vspace{.2cm}
\\
\multicolumn{1}{r}{Sample size $n=$} & $1,000$ & $5,000$ & $10,000$ & $1,000$ & $5,000$ & $10,000$ & $1,000$ & $5,000$ & $10,000$
\vspace{.2cm}
\\[-1.2ex]
\hline \\[-1.2ex] 
\multicolumn{4}{l}{\textit{Panel A: Turnout Probability (truth=0.209)}}\\
\multicolumn{1}{l}{Mean} & 0.167 & 0.153 & 0.156 & 0.243 & 0.205 & 0.193 & 0.303	& 0.254	& 0.243  \\
\multicolumn{1}{l}{Variance} & 0.090 & 0.014 & 0.006 & 0.021 & 0.006 & 0.003 & 0.013 & 0.007 & 0.006\\
\multicolumn{1}{l}{MSE} & 0.091 & 0.017 & 0.009 & 0.022 & 0.006 & 0.004 & 0.022	& 0.009	& 0.007\\
\multicolumn{1}{l}{95\% coverage} &  93.7\% & 95.5\% & 95.8\% & 91.3\% & 95.8\% & 97.4\% & 80.3\%	& 88.5\% & 91.7\%\\
\\[-1.2ex] \hline 
\multicolumn{4}{l}{\textit{Panel B: Housing Price (truth=11.600)}}\\
\multicolumn{1}{l}{Mean} & -3.643 & -1.356 & -0.095 & 2.345 & 1.271 & 2.299 & 27.219	& 16.954 & 13.733\\
\multicolumn{1}{l}{Variance} & 634.685 & 87.831 & 49.477 & 235.655 & 65.504 & 43.304 &99.338	& 42.286 & 39.999 \\
\multicolumn{1}{l}{MSE} & 866.792 & 255.481 & 186.047 & 321.161 & 172.013 & 129.658 & 343.553	& 71.044 & 44.583\\
\multicolumn{1}{l}{95\% coverage} & 91.2\% & 85\% & 77.1\% & 97.8\% & 92.1\% & 88.9\% & 57.2\% & 85.3\%	& 91.0\%\\
\\[-1.2ex]\hline 
\multicolumn{4}{l}{\textit{Panel C: Age (truth=7.730)}}\\
\multicolumn{1}{l}{Mean} & 6.300 & 6.182 & 6.236 & 8.894 & 8.544 & 8.158 & 8.396 & 7.851 & 7.797\\
\multicolumn{1}{l}{Variance} & 112.144 & 16.615 & 7.575 & 17.390 & 4.026 & 2.455 & 13.738	& 8.758 & 8.001\\
\multicolumn{1}{l}{MSE} &  114.189 & 19.012 & 9.808 & 18.743 & 4.688 & 2.637 & 14.182 &	8.772	& 8.006\\
\multicolumn{1}{l}{95\% coverage} & 93.4\% & 94.8\% & 95.2\% & 96.8\% & 93.2\% & 89.7\% & 93.2\% & 93.2\%	& 94.1\%\\
\\[-1.2ex]\hline 
\end{tabular}
}
\\
\vspace{.2cm}
{\noindent \justifying \scriptsize \textbf{Table A.3.1.} Keele and Titiunik (2015): Estimating the effect of political television campaign using the transformed univariate score for all estimators. The effective running variable is univariate, measured by the rescaled Euclidean (chordal) distance between the coordinate and the cutoff point. The true treatment effects in parentheses are calculated from the DGP models. Columns 1-3 correspond to the local linear regression estimator (\ref{eqn:1}) with robust bias correction and is implemented by the \texttt{rdrobust} package with default options. Columns 4-6 correspond to the minimax-optimal estimator (\ref{minimax}) implemented by the \texttt{optrdd} package, where $\mathcal{B}$ is the $95$-th percentile (over all sample points) of the operator norm curvature estimated by a cross-validated ridge regression with interacted $7$-th order natural splines and $\sigma_i^2$ is estimated by averaging the residuals from an ordinary least-squares model of $Y_i$ on the interaction of $X_i$ and $A(X_i)$. Columns 7-9 correspond to the forest estimator in (\ref{eqn:5}) with $B=5000$ trees and is implemented by the \texttt{grf} package with parameter tuning as discussed in Appendix \hyperref[appendix:A.4]{A.4}. The 95\% coverage rate records the Monte Carlo coverage probability of the robust bias-corrected confidence intervals for local linear regressions, the uniform confidence intervals for the minimax-optimal estimator, and confidence intervals obtained from the the bag of little bootstraps variance estimator for random forests.\par}
\end{table} 

\subsection*{A.4 \hspace{.3cm} Parameter Tuning for Forests \label{appendix:A.4}}
In this section, we discuss how the \texttt{grf} package implements parameter tuning, as well as how we modify the process so that the tuned parameters are in alignment with the conditions in Corollary \hyperref[cor1]{1}.

Our simulations use the \texttt{regression\char`_forest()} function in the \texttt{grf} package, where the tunable parameters include the subsample fraction (i.e., $s/n$), $\alpha$, and the number of variables tried for each split (i.e., $\pi$), as well as other parameters that do not affect the asymptotics, including the minimum node size, imbalance penalty (i.e., how harshly imbalanced splits are penalized), fraction of data used for honesty, and whether we prune the estimation sample trees such that no leaves are empty (recall that honesty requires using independent samples for placing splits and for estimation). The \texttt{regression\char`_forest()} function sets default values for these parameters, e.g., the default subsample fraction is $0.5$ and the default $\alpha$ is $0.05$, which serve as the rule of thumb and generate decent average predictions over our Monte Carlo replications. However, in practice, using the default parameters does not fully reveal the potential of random forests, which makes parameter tuning an essential step in training a forest. Figure \hyperref[fig:A.4]{A.4} illustrates the potential gain from tuning parameters.

The \texttt{grf} package tunes the above parameters using a grid search, followed by a kriging model. First, the tuning function draws a small number (default$\ =100$) of random values from $Uniform([0,1])$ for each parameter to be tuned. These randomly drawn numbers are then mapped to values that fall into the practical range of each parameter, forming 100 combinations of parameter values. For example, the mapping function for subsample fraction, $s/n$, ensures that the randomly drawn values of subsample fractions are within the range $[0.05, 0.5]$, where the upper bound, $0.5$, is required for estimating the variance using BLB via half-sampling (see Section 4 of \hyperref[ref:12]{Athey, Tibshirani and Wager (2019)}). Then, a small forest of $50$ trees is built with each of the $100$ combinations of random parameter values, and the average debiased out-of-bag error of each combination is recorded. These average errors, along with the original set of random values drawn from $Uniform([0,1])$, are used to build the kriging model, which predicts errors for a larger set of $1000$ combinations of newly drawn random values from $Uniform([0,1])$ and picks out the combination that achieves the lowest average error. Employing a kriging model is clearly time-efficient in that we only need to build a small number of forests to compare a large number of random parameter combinations. Finally, the tuning function re-trains a moderately-sized forest of $200$ trees using the optimal parameter values chosen by the kriging model (after mapping), and compares the average error of this forest to that of a forest with default parameters to make sure the tuned forest does not do worse than the default; otherwise default parameters are used. 

From the parameter tuning process described above, we notice that the \texttt{grf} package draws values for subsample fractions and $\alpha$ separately, whereas in Corollary \hyperref[cor1]{1}, the subsample sizes $s_1$ and $s_0$ should satisfy $s_1 \asymp n_1^{\beta}\,\,\,\text{and} \,\,\, s_0 \asymp n_0^{\beta}$, for some $\beta$ lower bounded by $\beta_{min}:=\left(1+ \frac{\pi\log(1-\alpha)}{d\log(\alpha)}\right)^{-1}$. This condition implies that the subsample fractions for the treated and control forests, $\frac{s_1}{n_1}$ and $\frac{s_0}{n_0}$, should not be randomly drawn, but rather depend on the draws of $\alpha$ and $\pi$. Since we only have up to 2 running variables in our simulations, we set $\pi=1$ for simplicity, i.e., each dimension has an equal probability of being split; when there are many running variables, tuning $\pi$ can make the forest split more frequently along dimensions that contain more signals for predicting the CEF and thus improve performance. For $\alpha$, we find that the subsample fractions tuned using the \texttt{grf} package tend to be very small in our simulations, often around $0.1$. This leads to too few observations used in each tree and can cause a severe bias problem and therefore undercoverage -- recall that the tuning function aims at selecting parameters that minimize the average out-of-bag error across all observations, but in the RD setting we are only concerned with predictions at boundary points, where the bias problem is pronounced and a larger subsample fraction is usually a safer choice. However, a larger subsample fraction leads to more correlated trees and thus less reduction in variance via forest averaging, so it is not ideal to fix subsample fraction at its default, which is $0.5$.

We thus modify the tuning function in the \texttt{grf} package in the following way. Instead of randomly drawing from $Uniform([0,1])$ for subsample fraction, for each random draw of $\alpha$, we set the corresponding subsample fraction to $ceil[n_j^{\beta_{min}}]/n_j$ times a constant $c\in[0.05,0.5]$ for $j \in \{0,1\}$.\footnote{$c$ is bounded between 0.05 and 0.5 because $\beta_{min}$ is close to $1$, so the tuned subsample fraction (bounded between 0.05 and 0.5) will be very close to the value of this scaling constant.} The tuning process of other parameters remains unchanged. The kriging model design takes in the original random values from $Uniform([0,1])$ for all parameters to be tuned except subsample fraction, but since the responses of the kriging model, i.e., average errors, still depend on subsample fraction, which is in turn determined by $\alpha$ in the kriging model design, we are now ``indirectly" tuning subsample fraction, but now in accordance with the scaling condition as required by Corollary \hyperref[cor1]{1}. The default subsample fraction (used when the tuned parameters are suboptimal) is modified in the same way using $\alpha_{default}=0.05$, but it is now a function of the dimension of the problem ($d$) and the number of observations used to train a forest ($n_1$ or $n_0$). 

Figure \hyperref[fig:A.4]{A.4} shows, for each DGP we consider with sample size $n=5000$, how the average squared bias, variance, MSE and the $95\%$ coverage rate across the 1000 Monte Carlo replications change with the scaling constant we use. As $c$ gets smaller, the bias problem and undercoverage become more pronounced, whereas the variance decreases. We recommend setting $c$ in the range of $[0.4,0.5]$ for small to moderate data sets; smaller values of $c$ can be considered with larger sample sizes. All the simulation results for random forests reported in this paper are based on the modified tuning procedure above with $c=0.4$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/Figure_A.4.png}
    \label{fig:A.4}\\
    {\noindent \justifying \footnotesize \textbf{Figure A.4.} Parameter tuning for forests. Each panel is generated using $1000$ Monte Carlo replications with sample size $n=5000$. The horizontal axis corresponds to different scaling constants $c \in \{0.2, 0.3, 0.4, 0.5\}$ as well as the a “Default” option, in which case none of the parameters are tuned. The left vertical axis corresponds to values of the Monte Carlo average of squared bias (red), variance (blue), and MSE (green); the right vertical axis corresponds to values of the empirical $95\%$ coverage rate (black).  \par}
\end{figure}

\subsection*{A.5\hspace{.3cm} Computation Time\label{appendix:A.5}
}
Table \hyperref[tbl:A.5]{A.5} reports the average computation time in seconds per Monte Carlo iteration for each estimator and sample size combination used in the simulation studies in Section \ref{sec3}.

We note that forests in general require a longer computation time, as they target a global criterion and have a more involved tuning procedure as described in Appendix \hyperref[appendix:A.4]{A.4}. But the gain is that, in multivariate RD designs, if the researcher is interested in heterogeneous treatment effects evaluated at different boundary points, they only need to fit forests once, whereas local linear regressions and the minimax-optimal estimator require repeated estimations, one for each boundary point, which may be more computationally costly.

\begin{table}[H] \centering 
  \label{tbl:A.5}
  \scriptsize{
\begin{tabular}
{@{\extracolsep{5pt}}lccccccccc} 
 & \multicolumn{3}{c}{Local Linear}
 & \multicolumn{3}{c}{Minimax-Optimal} & \multicolumn{3}{c}{Random Forests}\\
\cline{2-4} \cline{5-7} \cline{8-10}
\\[-1.2ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9)
\vspace{.2cm}
\\
\multicolumn{1}{r}{Sample size $n=$} & $1,000$ & $5,000$ & $10,000$ & $1,000$ & $5,000$ & $10,000$ & $1,000$ & $5,000$ & $10,000$
\vspace{.2cm}
\\[-1.2ex]
\hline \\[-1.2ex] 
\multicolumn{4}{l}{\textbf{Table 
 \hyperref[tbl:1]{1}.} Lee (2008)}\\
& 0.026s & 0.101s & 0.193s & 0.086s & 0.105s & 0.113s & 5.265s & 8.204s & 14.345s\\
\\[-1.2ex]\hline \\[-1.2ex] 
\multicolumn{8}{l}{\textbf{Table \hyperref[tbl:1]{2}.} Keele and Titiunik (2015)}\\\\[-1.2ex]
\multicolumn{8}{l}{\textit{\quad Panel A: Turnout Probability}}\\
& 0.090s & 0.405s & 0.874s & 4.904s & 5.074s & 5.721s & 3.434s & 10.350s &  19.720s\\
\\[-1.2ex] 
\multicolumn{8}{l}{\textit{\quad Panel B: Housing Price}}\\
& 0.068s & 0.324s & 0.834s & 5.102s & 4.888s & 5.716s & 4.914s & 13.811s & 24.580s\\
\\[-1.2ex]
\multicolumn{4}{l}{\textit{\quad Panel C: Age}}\\
& 0.085s & 0.394s & 0.877s & 4.993s & 4.434s & 4.597s & 9.070s & 13.255s & 41.713s\\
\\[-1.2ex]
\hline 
\end{tabular}
}
\\
\vspace{.2cm}
{\noindent \justifying \scriptsize \textbf{Table A.5.} Computation time per iteration. This table reports the average computation time in seconds per Monte Carlo iteration. The rows correspond to the samples used in the simulations reported in Table \hyperref[tbl:1]{1} and Table \hyperref[tbl:2]{2}. See platform information at \href{https://github.com/yqi3/GRF-RD}{\texttt{github.com/yqi3/Replication-GRF-RD}}.\par}
\end{table} 


\subsection*{A.6 \hspace{.3cm} Characterization of Bivariate Densities with A Zero Transformed Univariate Density at the Cutoff \label{appendix:A.6}}

\input{zerodensity.tex}

