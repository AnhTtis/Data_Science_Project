\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{makecell}
\usepackage{rotating}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[table,xcdraw]{xcolor}

\usepackage{arydshln}
\usepackage{authblk}

\let\svthefootnote\thefootnote
\newcommand\blankfootnote[1]{%
  \let\thefootnote\relax\footnotetext{#1}%
  \let\thefootnote\svthefootnote%
}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\definecolor{Gray}{gray}{0.95}
\newcolumntype{g}{>{\columncolor{Gray}}c}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{LiDARFormer: A Unified Transformer-based Multi-task Network for LiDAR Perception}

\author[1,2]{\textbf{Zixiang~Zhou}\thanks{Work done during an internship at TuSimple.}\thanks{Contributed equally.}}
\newcommand\CoAuthorMark{\footnotemark[\arabic{footnote}]} % get the current value
\author[1]{\textbf{Dongqiangzi~Ye}\protect\CoAuthorMark}
\author[1]{\textbf{Weijia~Chen}}
\author[1]{\textbf{Yufei~Xie}}
\author[1]{\par\textbf{Yu~Wang}}
\author[1]{\textbf{Panqu~Wang}}
\author[2]{\textbf{Hassan~Foroosh}}
\affil[1]{TuSimple}
\affil[2]{University of Central Florida}


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
    There is a recent trend in the LiDAR perception field towards unifying multiple tasks in a single strong network with improved performance, as opposed to using separate networks for each task. In this paper, we introduce a new LiDAR multi-task learning paradigm based on the transformer. The proposed \textbf{LiDARFormer} utilizes cross-space global contextual feature information and exploits cross-task synergy to boost the performance of LiDAR perception tasks across multiple large-scale datasets and benchmarks. Our novel transformer-based framework includes a cross-space transformer module that learns attentive features between the 2D dense Bird's Eye View (BEV) and 3D sparse voxel feature maps. Additionally, we propose a transformer decoder for the segmentation task to dynamically adjust the learned features by leveraging the categorical feature representations. Furthermore, we combine the segmentation and detection features in a shared transformer decoder with cross-task attention layers to enhance and integrate the object-level and class-level features. LiDARFormer is evaluated on the large-scale nuScenes and the Waymo Open datasets for both 3D detection and semantic segmentation tasks, and it outperforms all previously published methods on both tasks. Notably, LiDARFormer achieves the state-of-the-art performance of $76.4\%$ L2 mAPH and $74.3\%$ NDS on the challenging Waymo and nuScenes detection benchmarks for a single model LiDAR-only method.
\end{abstract}

\input{intro}
\input{relate}
\input{method}
\input{experiment}
\input{conclusion}

\appendix
\input{supp}

% \newpage
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{citation}
}
\end{document}