\section{Introduction}

\begin{figure}[th]
  \begin{center}
    \includegraphics[width=0.95\linewidth]{figs/fig1.pdf}
  \end{center}
  \caption{\textbf{LiDAR Perception Network Designs.} LiDAR detection (a) and segmentation (b) networks typically extract feature representations on distinct feature maps. While a recent multi-task network~\cite{ye2022lidarmultinet} (c) integrates these tasks into a single network, it often overlooks differences among feature maps and the higher-level connections between tasks. Our network (d) utilizes transformer attention to establish more effectively the transformations between 3D sparse and 2D dense features. Moreover, the cross-task information is further shared through class-level and object-level feature embeddings in the multi-task transformer decoder.}
  \label{fig:teaser}
  \vspace{-10pt}
\end{figure}

LiDAR point cloud detection and semantic segmentation tasks aim to predict the object-level 3D bounding boxes and point-level semantic labels, which are among the most fundamental tasks in autonomous vehicle perception. With the recent release of the large-scale LiDAR point cloud datasets~\cite{caesar2020nuscenes,sun2020scalability}, there has been a surge of interest in integrating these tasks into a single framework. Current methods~\cite{lidarmtl2021,ye2022lidarmultinet} rely on voxel-based networks with sparse convolution~\cite{yan2018second,choy20194d} for leading performance. However, different tasks are only connected through sharing the same low-level features without considering the high-level contextual information that is highly related among those tasks. On the other hand, more recent works~\cite{shi2020pv,tang2020searching,xu2021rpvnet} try to fuse features from multiple views that contain both voxel-level and point-level information. These approaches focus more on exploiting local point geometric relations to recover fine-grained details. The problem of efficiently extracting and sharing global contextual information in LiDAR perception tasks is still by and large underexplored.

Meanwhile, transformer-based network structures~\cite{carion2020end,wang2021max,cheng2021maskformer,xie2021segformer,zhang2022dino} start to exhibit an outstanding performance on 2D image detection and segmentation tasks. Apart from directly replacing the conventional CNN with the transformer encoder~\cite{dosovitskiy2020vit,Liu2021SwinTH}, various methods~\cite{carion2020end,zhu2021deformable,cheng2021maskformer,li2022mask} explore using the transformer decoder to extract objects or class-level feature representations, which are served as strong contextual information for feature learning. This transformer decoder design is then adopted in recent LiDAR perception methods~\cite{Zhou_centerformer,bai2022transfusion,marcuzzi2023mask}. However, the transformer decoders used for LiDAR detection and segmentation tasks are performed independently on different feature maps and are not yet unified.

Is it possible to develop a unified transformer-based multi-task LiDAR perception network with the ability to learn global context information? To accomplish this goal, we introduce three novel components in a voxel-based framework. The first component is a cross-space transformer module that enhances the feature mapping between the 3D sparse voxel space and the 2D dense BEV space. These two spaces are frequently used to obtain feature representations for segmentation and detection tasks, respectively. Second, we propose a transformer-based refinement module as the segmentation decoder. The module uses a transformer to extract class feature embeddings and refine voxel features through bidirectional cross-attention. Lastly, we propose a multi-task learning structure that combines segmentation and detection transformer decoders into a unified transformer decoder. By doing so, the network can transfer high-level features through cross-task attention, as depicted in Figure~\ref{fig:teaser}. These three innovative components result in a powerful network, named \textbf{LiDARFormer}, for the next generation of LiDAR perception.

We evaluate our method on two challenging large-scale LiDAR datasets: the nuScenes dataset~\cite{caesar2020nuscenes} and the Waymo Open Dataset~\cite{sun2020scalability}. Our method sets new state-of-the-art standards both in detection and semantic segmentation, by achieving $74.3\%$ NDS on the nuScenes 3D detection and $81.5\%$ mIoU on the nuScenes semantic segmentation. LiDARFormer also achieves $76.4\%$ mAPH in the Waymo Open Dataset detection set, surpassing thus all previous methods. 

Our main contributions are summarized as follows:

\begin{itemize}
    \item We propose a cross-space transformer module to improve feature learning when transferring features between sparse voxel features and dense BEV features in the multi-task network.
    \item We present the first LiDAR cross-task transformer decoder that bridges the information learned across object-level and class-level feature embedding. 
    \item We introduce a transformer-based coarse-to-fine network that utilizes a transformer decoder to extract class-level global contextual information for the LiDAR semantic segmentation task. 
    \item Our network achieves state-of-the-art 3D detection and semantic segmentation performances on two popular large-scale LiDAR benchmarks.
\end{itemize}