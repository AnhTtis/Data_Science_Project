\section{Related Work}
\textbf{Voxel-based LiDAR Point Cloud Perception}
Unlike most point cloud networks~\cite{qi2017pointnet,qi2017pointnet++,li2018pointcnn,wu2019pointconv,hu2019randla,xu2020grid,thomas2019KPConv,zhao2021point} that directly learn point-level features in outdoor or indoor point cloud data, LiDAR point cloud perception usually requires transforming the large-scale sparse point cloud into either a 3D voxel map~\cite{zhou2018voxelnet,zhu2021cylindrical}, 2D BEV~\cite{yang2018pixor,lang2019pointpillars,Zhang_2020_CVPR}, or range-view map~\cite{sun2021rsn,fan2021rangedet,wu2018squeezeseg,wu2019squeezesegv2,milioto2019rangenet++,cortinhal2020salsanext}. Thanks to the development of the 3D sparse convolution layer~\cite{yan2018second,choy20194d} in point cloud processing, voxel-based methods are becoming dominant in terms of both high performance and efficient runtime. CenterPoint~\cite{yin2021center} and AFDet~\cite{ge2020afdet} adopted the anchor-free design that detects objects through heatmap classification. Cylinder3D~\cite{zhu2021cylindrical} utilized the cylindrical voxel partition to extract the voxel-level features. LargeKernel3D~\cite{chen2022scaling} showed that the long-range information from a bigger receptive field can significantly improve the performance. LidarMultiNet~\cite{ye2022lidarmultinet} presented a multi-task learning network that unifies different LiDAR perception tasks.

\begin{figure*}[t]
  \begin{center}
    \includegraphics[width=1.0\textwidth]{figs/LiDARFormer_architecture.pdf}
  \end{center}
  \caption{\textbf{The architecture of LiDARFormer.} Our network first transforms the point cloud into a sparse voxel map. Next, sparse 3D CNN is used to extract voxel feature representation. Between the encoder and the decoder, we use a Cross-space Transformer (\textbf{XSF}) module to learn long-range information in the BEV map. Additionally, we use a cross-task transformer decoder (\textbf{XTF}) to extract class-level and object-level feature representations, which are fed into task-specific heads to generate the detection and segmentation predictions.}
  \label{fig:architecture}
\end{figure*}

Voxel-based methods have to make a trade-off between accuracy and complexity due to the information loss introduced during the projection or voxelization. To alleviate the quantization error, some recent methods~\cite{tang2020searching,shi2020pv,ye2021drinet,xu2021rpvnet} propose to fuse features from multi-view feature maps, combining point-level information with 2D BEV/range-view and 3D voxel features. PVRCNN~\cite{shi2020pv} and SPVNAS~\cite{tang2020searching} used two concurrent point-level and voxel-level feature encoding branches, where these two features were connected at each network block. RPVNet~\cite{xu2021rpvnet} further combined all point, voxel, and range image features in an encoder-decoder segmentation network through a gated fusion module. In contrast to these methods that focus on fine-grained features for details, our method aims to enhance global feature learning in the voxel-based network.

\textbf{Segmentation Refinement} 
In the image domain, various methods~\cite{li2016iterative,zhu20183d,chen20182,zhang2019acfnet,yuan2020object} use multiple stages to refine the segmentation prediction from coarse to fine. ACFNet~\cite{zhang2019acfnet} proposed an attentional class feature module to refine the pixel-wise features based on a coarse segmentation map. OCR~\cite{yuan2020object} further advanced the idea to use a bidirectional connection between pixel-wise features and object-contextual representations to enrich the features. In comparison, refinement modules have been rarely used in point cloud semantic segmentation.

\textbf{Transformer Decoder}
Transformer~\cite{vaswani2017attention} structure has gained huge popularity in recent years. Built on the development of 2D transformer backbones~\cite{dosovitskiy2020vit,Liu2021SwinTH}, various methods~\cite{zhu2021deformable,SETR,wang2021max,cheng2021maskformer,xie2021segformer,zhang2022dino} are proposed to tackle the 2D detection and segmentation problems. Depending on the source of the input, the vision transformers can be categorized into encoder~\cite{Liu2021SwinTH,SETR,xie2021segformer} and decoder~\cite{carion2020end,wang2021max,cheng2021maskformer,yuan2020object,zhang2022dino,li2022mask}. A transformer encoder usually serves as a feature encoding network to replace the conventional neural networks, while a transformer decoder is used to extract class-level or instance-level feature representations for the downstream tasks. In the LiDAR domain, several detection methods~\cite{misra2021end,yang20213d,liu2021,Sheng2021ICCV,bai2022transfusion,nguyen2021boxer,li2022bevformer,Zhou_centerformer} have started to integrate the transformer decoder structure into the previous frameworks. Besides the performance improvement, the transformer decoder demonstrates great potential for an end-to-end training~\cite{misra2021end} and multi-frame~\cite{yang20213d,Zhou_centerformer} / modality~\cite{bai2022transfusion,li2022bevformer} feature fusion. However, studying effective methods of using a transformer decoder in LiDAR segmentation is still an underexplored area. In this paper, we propose a novel class-aware global contextual refinement module for LiDAR segmentation based on the transformer decoder, while exploiting the synergy between detection and segmentation decoders. 

