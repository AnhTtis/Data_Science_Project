\section{Experiments}
In this section, we present the experimental results of our proposed method on two large-scale public LiDAR point cloud datasets: the nuScenes dataset~\cite{caesar2020nuscenes} and the Waymo Open Dataset\cite{sun2020scalability}, both of which have 3D object bounding boxes and pixel-wise semantic label annotations. We also provide a detailed ablation study of the improvements and in-depth analysis of our model. More details and visualization are included in the supplementary materials.

\subsection{Datasets}
The \textbf{NuScenes} dataset is a large-scale autonomous driving dataset developed by Motional. It contains 1000 scenes of 20s video data, each of them captured by a 20Hz Velodyne HDL-32E Lidar sensor with 32 vertical beams. NuScenes provides annotations of object bounding boxes and pixel-wise semantic labels at each keyframe sampled at 2 Hz. 16 classes are used for the semantic segmentation evaluation. 10 foreground object (``thing'') classes with ground truth bounding box labels are used for the object detection task. For the nuScenes detection task, mean Average Precision (mAP) and NuScenes Detection Score (NDS) are used as the metrics. For semantic segmentation, mean Intersection over Union (mIoU) is used as the metric.

The \textbf{Waymo Open Dataset (WOD)} contains around 2000 scenes of 20s video data that is collected at 10Hz by a 64-line LiDAR sensor. Even though WOD provides object bounding box annotation for every frame, it only has the semantic annotation in some key frames sampled at 2Hz. WOD has semantic labels for 23 classes and uses the standard mIoU as the evaluation metric. For the object bounding box annotation, 3 classes of vehicles, pedestrians, and cyclists are calculated into the 3D detection metrics. Average Precision Weighted by Heading (APH) is used as the main detection evaluation metric. The ground truth objects are categorized into two levels of difficulty, LEVEL\_1 (L1) is assigned to the examples that have more than 5 LiDAR points and not in the L2 category, while LEVEL\_2 (L2) is assigned to examples that have at least 1 LiDAR point and at most 5 points or are manually labeled as hard. The primary metric mAPH L2 is computed by considering both L1 and L2 examples.

\subsection{Experiment Setup}
We used the AdamW optimizer with the one-cycle scheduler to train our model for 20 epochs. Most experiments are conducted on 8 Nvidia A100 GPUs with batch size 16. For the multi-task training experiments on WOD, we used batch size 8 because of the GPU memory limits. We used the voxel size of $[0.1,0.1,0.2]$ for nuScenes datasets, and $[0.1,0.1,0.15]$ for Waymo Open Dataset. For the segmentation task, we used a combination of cross-entropy loss and Lovasz loss~\cite{berman2018lovasz} to optimize our network. For the detection task, we followed \cite{yin2021center} to use the common center heatmap classification loss and bounding box regression loss. We added an auxiliary loss on the output voxel features or BEV features to supervise the segmentation prediction, which is used to initialize the class feature embedding. All losses are fused by multi-task uncertainty weighting strategy~\cite{kendall2018multi}. We concatenated the points from the previous 9 scans to the current point cloud in nuScenes, and 2 scans in WOD. Standard data augmentation strategy~\cite{wang2021pointaugmenting,ye2022lidarmultinet} were applied when training the model. More network and training details are included in the supplementary materials.

\begin{table*}[t]
\begin{minipage}{.56\textwidth}
\centering
\caption{Detection L2 mAPH results on the \texttt{test} split of WOD. ``L" and ``CL" denote LiDAR-only and camera \& LiDAR fusion methods. Second best results are underlined.}
\label{tab:WOD_det_test}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|ccc|>{\columncolor[gray]{0.95}}c}
\Xhline{4\arrayrulewidth}
    Model & Ref & Modal & Frame & Veh. & Ped. & Cyc. & Mean \\
    \Xhline{2\arrayrulewidth}
    M3DETR~\cite{guan2022m3detr} & WACV 2022 & L & 1 & 70.0  & 52.0 & 63.8 & 61.9\\
    PV-RCNN++~\cite{shi2021pv} & arXiv 2022 & L & 1 & 73.5 & 69.0 & 68.2 & 70.2\\
    CenterPoint++~\cite{yin2021center} & CVPR 2021 & L & 3 & 75.1 & 72.4 & 71.0 & 72.8\\
    SST\_3f~\cite{fan2022embracing} & CVPR 2022 & L & 3 & 72.7 & 73.5 & 72.2 & 72.8\\
    AFDetV2~\cite{hu2022afdetv2} & AAAI 2022 & L & 2 & 73.9  & 72.4 & 73.0 & 73.1 \\
    DeepFusion \cite{li2022deepfusion} & CVPR 2022 & CL & 5 &  75.7 & 76.4 & 74.5 & 75.5  \\
    MPPNet~\cite{chen2022mppnet} & ECCV 2022 & L & 16 & 76.9  & 75.9 & 74.2 & 75.7 \\
    CenterFormer~\cite{Zhou_centerformer} & ECCV 2022 & L & 16 & \textbf{78.3}  & \textbf{77.4} & 73.2 & \underline{76.3} \\
    BEVFusion~\cite{liu2022bevfusion} & ICRA 2023 & CL  & 3 & \underline{77.5}  & 76.4 & \textbf{75.1} & \underline{76.3}  \\
    \Xhline{2\arrayrulewidth}
    LiDARFormer    & & L & 3  & \underline{77.5} & \underline{77.2} & \underline{74.6} & \textbf{76.4}\\
    \Xhline{4\arrayrulewidth}
\end{tabular}
}
\end{minipage}
\hfill
\begin{minipage}{.42\textwidth}
\centering
	\caption{Results on \texttt{val} split of WOD. *: From our reproduction.}
	\label{tab:waymo_val}
	\resizebox{\linewidth}{!}{%
	\begin{tabular}{l|c|c|>{\columncolor[gray]{0.95}}c>{\columncolor[gray]{0.95}}c}
		\Xhline{4\arrayrulewidth}
		Model & Modal & Frame & mIoU & L2 mAPH \\
		\Xhline{2\arrayrulewidth}
		PolarNet~\cite{Zhang_2020_CVPR} & L & 1 &  61.6* & - \\
		Cylinder3D~\cite{zhu2021cylindrical} & L & 1 &  66.6* & - \\
		\Xhline{2\arrayrulewidth}
		PointAugmenting~\cite{wang2021pointaugmenting}& CL & 1 & - & 66.7 \\
		PV-RCNN++~\cite{shi2021pv}& L & 1 & - & 68.6 \\
		AFDetV2-Lite~\cite{hu2022afdetv2}& L & 1 & - & 68.8 \\
		CenterPoint++~\cite{yin2021center}& L & 3 & - & 71.6 \\
		SST\_3f~\cite{fan2022embracing}& L & 3 & - & 72.4 \\
		CenterFormer~\cite{Zhou_centerformer}& L & 8 & - & 73.7 \\
		MPPNet~\cite{chen2022mppnet}& L & 16 & - &  74.9 \\
		\Xhline{2\arrayrulewidth}
		LidarMultiNet~\cite{ye2022lidarmultinet}& L & 3 & 71.9 & 75.2\\
		\Xhline{2\arrayrulewidth}
		LiDARFormer seg only & L & 3 &  71.3 & - \\
		LiDARFormer & L & 3 &  \textbf{72.2} & \textbf{76.2}\\
		
		\Xhline{4\arrayrulewidth}
	\end{tabular}
	}
\end{minipage}
\end{table*}

\begin{table*}[t]
\begin{minipage}{.49\textwidth}
\centering
\caption{The ablation of mIoU improvement of each component on the nuScenes and WOD \texttt{val} split when trained only for the segmentation task. XSF and STD stand for cross-space transformer and segmentation transformer decoder.}
\label{table:ablation_seg_only}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc|cc}
\Xhline{4\arrayrulewidth}
Baseline (\ref{section:basline}) & STD & Multi-frame & XSF & nuScenes & WOD \\ \Xhline{2\arrayrulewidth}
$\checkmark$ &              &              &              & 76.6\phantom{ (+1.7)}   &  70.3\phantom{ (+1.7)} \\
$\checkmark$ & $\checkmark$ &              &              & 78.3 (\textcolor{red}{+1.7})   &  70.6 (\textcolor{red}{+0.3}) \\
$\checkmark$ & $\checkmark$ & $\checkmark$ &              & 80.8 (\textcolor{red}{+4.2})   &  71.2 (\textcolor{red}{+0.9}) \\
$\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{81.7} (\textcolor{red}{+5.1})   &  \textbf{71.3} (\textcolor{red}{+1.0})
 \\\Xhline{4\arrayrulewidth}
\end{tabular}
}
\end{minipage}
\hfill
\begin{minipage}{.49\textwidth}
\centering
\caption{The ablation of the improvement of shared transformer decoder on the nuScenes \texttt{val} split when jointly trained with detection task.}
\label{table:ablation}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc|>{\columncolor[gray]{0.95}}cc>{\columncolor[gray]{0.95}}c}
\Xhline{4\arrayrulewidth}
& \multicolumn{2}{c}{XTF} & & & & \\
\multirow{-2}{*}{Baseline~\cite{ye2022lidarmultinet}} &{Seg} &{Det} & \multirow{-2}{*}{XSF} & \multirow{-2}{*}{mIoU} & \multirow{-2}{*}{mAP}  & \multirow{-2}{*}{NDS}\\ \Xhline{2\arrayrulewidth}
$\checkmark$ &              &              &              & 81.8\phantom{ (+1.7)}  &  65.2\phantom{ (+1.7)} & 70.0\phantom{ (+1.7)} \\
$\checkmark$ & $\checkmark$ &              &              & 82.1 (\textcolor{red}{+0.3})  &  65.4 (\textcolor{red}{+0.2}) & 70.2 (\textcolor{red}{+0.2}) \\
$\checkmark$ &              & $\checkmark$ &              & 82.4 (\textcolor{red}{+0.6})  &  65.9 (\textcolor{red}{+0.7}) & 70.3 (\textcolor{red}{+0.3})\\
$\checkmark$ & $\checkmark$ & $\checkmark$ &              & 82.6 (\textcolor{red}{+0.8})   &  66.0 (\textcolor{red}{+0.8}) & 70.2 (\textcolor{red}{+0.2}) \\
$\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{82.7} (\textcolor{red}{+0.9})  & \textbf{66.6} (\textcolor{red}{+1.4}) & \textbf{70.8} (\textcolor{red}{+0.8})\\\Xhline{4\arrayrulewidth}
\end{tabular}
}
\end{minipage}
\end{table*}

\subsection{Main Results}
We present the detection and segmentation benchmark results on both nuScenes and WOD. All results of other methods in the test set are from the literature, where most of them apply test-time augmentation (TTA) or an ensemble method to increase the performance. In addition to our multi-task network, we also provide the results of the segmentation-only variation of our model, which is trained only with the segmentation transformer decoder.

\textbf{NuScenes} In Table~\ref{tab:nusc_det_test} and Table~\ref{tab:nusc_seg_test}, we compare LiDARFormer with other state-of-the-art methods on the test set of nuScenes. LiDARFormer reaches the top performance of $81.5\%$ mIoU, $71.5\%$ mAP, and $74.3\%$ NDS for a single model result. Notably, the results of the detection task outperform all previous methods by a large margin, especially for the mAP metric. Although the segmentation performance of LiDARFormer is only $0.1\%$ higher than LidarMultiNet, LiDARFormer does not require a second stage and can be trained end-to-end by comparison. To fairly compare with other methods without the effect of test-time augmentation, we also demonstrate the performance on the validation set of nuScenes in Table~\ref{tab:nusc_val}. Our segmentation-only LiDARFormer achieves a $81.7\%$ mIoU performance while full LiDARFormer further improves the mIoU to $82.7\%$ with the SOTA detection performance NDS $70.8\%$. Our method surpasses all previous state-of-the-art methods, which matches our result in the test set.

\textbf{Waymo Open Dataset} Table~\ref{tab:WOD_det_test} shows the detection results of LiDARFormer on the test set of WOD. LiDARFormer achieves the state-of-the-art performance of $76.4\%$ L2 mAPH, outperforming even the camera-LiDAR fusion methods and methods that use a much greater number of frames. Lastly, we report the validation results on Waymo Open Dataset in Table~\ref{tab:waymo_val}. We reproduce the result of PolarNet and Cylinder3D based on their released code for comparison. Our segmentation-only LiDARFormer achieves a $71.3\%$ mIoU performance on the validation set. Our multi-task model also outperforms the previous best multi-task network by $0.3\%$ on the segmentation task. For the more competitive detection task, our method reaches the best L2 mAPH result of $76.2\%$.

\subsection{Ablation Study}

\textbf{Effect of Transformer Structure on Segmentation Task}  Table~\ref{table:ablation_seg_only} shows the effectiveness of each proposed component in our method when trained only for the segmentation task. We use the network described in \ref{section:basline} as our baseline model. This simple design already can achieve competitive performance compared to other current state-of-the-art methods. After adding the segmentation transformer decoder, the mIoU increases by $1.7\%$ and $0.3\%$ in nuScenes and WOD, respectively. By concatenating points from previous frames to the current frame, the result further increases by $2.5\%$ and $0.6\%$. The cross-space transformer also can improve the mIoU by $0.9\%$ and $0.1\%$, respectively.

\textbf{Effect of the Unified Multi-task Transformer Decoder}
Table~\ref{table:ablation} demonstrates the improvements achieved by our proposed transformer decoder in the multi-task network. We use the 1st-stage results of LidarMultiNet~\cite{ye2022lidarmultinet} as our baseline. Adding an individual transformer decoder to either the detection or segmentation branch results in improved performance in both tasks, as our multi-task network has a shared backbone, allowing improvement in one task to contribute to feature representation learning. Our proposed shared transformer decoder yields superior overall performance by introducing cross-task attention learning. The cross-space transformer module further improves performance, particularly for the detection task. We also evaluate the panoptic segmentation performance of our multi-task network in Table~\ref{table:pano}. Even without a second stage dedicated to panoptic segmentation, our model achieves competitive results compared to the previous best method, LidarMultiNet. This demonstrates the ability of our multi-task transformer decoder to generate more compatible results for both tasks.
\begin{table}[t]
    \centering
    \caption{Panoptic segmentation result on nuScenes \texttt{val} split.}
    \label{table:pano}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|c|cccc}
    \Xhline{4\arrayrulewidth}
         & stage & PQ & SQ & RQ & mIoU\\ 
        \Xhline{2\arrayrulewidth}
        LidarMultiNet~\cite{ye2022lidarmultinet} & 2-stage & \textbf{81.8} & \textbf{90.8} & 89.7 & 83.6 \\
        LiDARFormer & 1-stage & \textbf{81.8} & 90.7 & \textbf{89.9} & \textbf{84.1} \\
    \Xhline{4\arrayrulewidth}
    \end{tabular}
    }
\end{table}

\subsection{Analysis}

\textbf{Analysis of the Segmentation Decoder} We compare the segmentation-only performance of our method using different transformer designs in Table~\ref{table:ccr}. Removing either way of the cross-attention leads to an inferior result. The dynamic kernel design outperforms the traditional segmentation head by $0.8\%$. Furthermore, the performance is $0.3\%$ lower without using an auxiliary segmentation head to initialize the class embedding.

\begin{table}[t]
\centering
    \caption{Design choice of the segmentation decoder on the nuScenes \texttt{val} split.}

    \label{table:ccr}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{l|l}
    \Xhline{4\arrayrulewidth}
    LiDARFormer seg only result without XSF (mIoU) & 80.8 \\
    \Xhline{2\arrayrulewidth}
    w/o voxel to class attention & 80.4 (\textcolor{green}{-0.4}) \\
    w/o class to voxel attention  & 80.1 (\textcolor{green}{-0.7}) \\
    w/o dynamic kernel  &  80.3 (\textcolor{green}{-0.5})   \\
    w/o class embedding initialization & 80.5 (\textcolor{green}{-0.3}) \\
    \Xhline{4\arrayrulewidth}
    \end{tabular}
    }

\end{table}

\begin{table}[t]
\centering
    \caption{The ablation of XSF on the nuScenes \texttt{val} split. S$\rightarrow$D and D$\rightarrow$S denote sparse-to-dense (\ref{fig:xst_b}) and dense-to-sparse (\ref{fig:xst_a}) XSFs.}
    \label{table:xsf}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{ccc|ccc}
    \Xhline{4\arrayrulewidth}
    S$\rightarrow$D & D$\rightarrow$S & Add Convs & mIoU & mAP & NDS \\
    \Xhline{2\arrayrulewidth}
    \multicolumn{6}{c}{Segmentation Only}\\
    \Xhline{2\arrayrulewidth}
    $\checkmark$ & $\checkmark$ & & 81.7\phantom{ (-1.7)} & - & - \\
     & & $\checkmark$ &  80.9 (\textcolor{green}{-0.8}) & - & - \\
    \Xhline{2\arrayrulewidth}
    \multicolumn{6}{c}{Multi-task}\\
    \Xhline{2\arrayrulewidth}
    $\checkmark$ & $\checkmark$ & & 82.7\phantom{ (+1.7)}  & 66.6\phantom{ (-1.7)} & 70.8\phantom{ (-1.7)}\\
     & $\checkmark$ & $\checkmark$ & 82.8 (\textcolor{red}{+0.1}) & 66.0 (\textcolor{green}{-0.6}) & 70.5 (\textcolor{green}{-0.3}) \\
    \Xhline{4\arrayrulewidth}
    
    \end{tabular}
    }
\end{table}

\textbf{Analysis of Cross-space Transformer} Table~\ref{table:xsf} illustrates the effectiveness of the Cross-Space Transformer (XSF) module in both detection and segmentation tasks, as compared to the direct mapping method. If we replace XSF with additional convolution layers of similar parameter size, the segmentation performance decreases by 0.8\%. However, when we only replace the sparse-to-dense XSF in the multi-task model, the segmentation performance remains largely unaffected, while detection performance shows a significant decline. This finding suggests that the dense-to-sparse and sparse-to-dense XSFs contribute differently to the detection and segmentation tasks.

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=1\linewidth]{figs/XSF_offset.pdf}
  \end{center}
  \caption{\textbf{Visualization of the learned offsets.} We showcase the features of a car's 3D voxels (\textcolor{blue}{blue}) and their corresponding deformable offsets (\textcolor{red}{red}) that were learned in our XSF module. For a better visual representation, we only highlight the offsets with high attention scores.}
  \label{fig:offsets}
\end{figure}

In Figure~\ref{fig:offsets}, we provide a visualization of the deformable offsets in our cross-space transformer. When using the previous direct mapping method for sparse voxels, only the features in the same position are used for transferring features between 3D and 2D space. This method may not utilize some useful features learned in the dense 2D BEV map. In contrast, our method is capable of aggregating related features across a wider range.

