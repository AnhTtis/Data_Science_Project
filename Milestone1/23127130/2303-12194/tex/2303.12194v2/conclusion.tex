\section{Conclusion}

In this paper, we present \textbf{LiDARFormer}, a novel and effective paradigm for multi-task LiDAR perception. 
Our method offers a novel way of strengthening voxel feature representation and enables joint learning of detection and segmentation tasks in a more elegant and effective manner. 
Although we have designed LiDARFormer for LiDAR-only input, our transformer XSF and XTF can extend to learn multi-modality and temporal features simply through cross-attention layers. Similarly, XSF can apply multi-scale feature maps in the deformable attention module to further extract the contextual information with larger receptive fields.
LiDARFormer sets a new state-of-the-art performance on the competitive nuScenes and Waymo detection and segmentation benchmarks. We believe that our work will inspire more innovative future research in this field. 